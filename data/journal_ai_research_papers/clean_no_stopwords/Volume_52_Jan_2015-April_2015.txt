Journal Artificial Intelligence Research 52 (2015) 601-713Submitted 08/14; published 04/15Compositional Framework Grounding LanguageInference, Generation, Acquisition VideoHaonan YuN. SiddharthAndrei BarbuJerey Mark Siskindhaonan@haonanyu.comsiddharth@iffsid.comandrei@0xab.comqobi@purdue.eduSchool Electrical Computer EngineeringPurdue University465 Northwestern AvenueWest Lafayette, 47907-2035 USAAbstractpresent approach simultaneously reasoning video clip entirenatural-language sentence. compositional nature language exploited constructmodels represent meanings entire sentences composed meaningswords sentences mediated grammar encodes predicate-argumentrelations. demonstrate models faithfully represent meanings sentencessensitive roles played participants (nouns), characteristics (adjectives), actions performed (verbs), manner actions (adverbs), changingspatial relations participants (prepositions) affect meaning sentencegrounded video. exploit methodology three ways. first, videoclip along sentence taken input participants event describedsentence highlighted, even clip depicts multiple similar simultaneousevents. second, video clip taken input without sentence sentencegenerated describes event clip. third, corpus video clips pairedsentences describe events clips meaningswords sentences learned. learn meanings without needing specifyattribute video clips word given sentence refers to. learnedmeaning representations shown intelligible humans.1. IntroductionPeople use knowledge language make sense world around them,describe observations communicate others. work, presentapproach able describe video clips natural language simultaneouslyusing capacity reason content clips. earlier approachesdetect individual features video (Laptev, 2005; Kuehne, Jhuang, Garrote, Poggio, &Serre, 2011), objects events, show knowledge language integrateinformation different feature detectors order improve performancesupport novel functionality. this, exploit compositional nature languageconstruct models entire sentences individual word models, use modelsdetermine entire sentence describes video clip. call mechanism determiningwell video clip depicts sentence, alternatively well sentence describesc2015AI Access Foundation. rights reserved.fiYu, Siddharth, Barbu, & Siskindvideo clip, sentence tracker (Yu & Siskind, 2013; Siddharth, Barbu, & Siskind, 2014),simultaneously performs multi-object tracking recognition events describedsentences. ability score video-sentence pairs also allows us perform anotherimportant task humans naturally engage in: learning word meanings. showsentence tracker perform task using kind informationavailable children, namely, video paired entire sentences describeevents depicted. general-purpose inference mechanism combining bottom-upinformation low-level video-feature detectors top-down information naturallanguage semantics allows us perform three novel tasks: tracking objectsengaged specific event described sentence, generating sentence describevideo clip, learning word meaning video clips paired entire sentences.Fundamentally, approach relies solving two separate problems simultaneously:tracking participants event recognizing occurrence event.formulate combination two measures: measure well video clipdepicts track collection well track collection depicts event. Notemean event complex state affairs described entire sentence,common definition used computer vision community, refers single verblabel attached video clip. order solve problems simultaneously, showsimilarity tracking event recognition facilities common inferencealgorithm. perform single-object tracking combining output unreliabledetection source, object detector, estimate motion present video,optical flow. tracks produced consist strong detections motion agreesmotion present video. perform single-word recognition representingmeaning word terms gross motion object tracks. Finally, showsingle-object tracking single-word recognition combine perform multi-object trackingwhole-sentence recognition exploiting compositionality language combineword models sentence models formulating tasks way amenabledynamic programming.ability perform tasks simultaneouslyin words, score videosentence pair well video clip depicts sentenceis crucial attaininggood performance. integrating top-down bottom-up information, corrects errorsobject-detector output. important object detectors highly unreliable,achieving 40%-50% average precision PASCAL Visual Object Classes (VOC)challenge (Everingham, Van Gool, Williams, Winn, & Zisserman, 2010). Barbu, Siddharth,Michaux, Siskind (2012b) showed reliability object tracking singleword recognition (typically verb) improved performing simultaneously.build earlier work extend track multiple objects recognize wholesentences. extend work novel approach sentence generationlearning word meanings.Following Yamoto, Ohya, Ishii (1992), Siskind Morris (1996), Starner,Weaver, Pentland (1998), represent word meanings fashion groundedvideo multi-state time-series classifiers, either hidden Markov models (HMMs)finite-state machines (FSMs), features extracted object tracks video.example, model approach might use three states encode event distance two tracked objects initially high, time decreases, finally ends602fiGrounding Language Inference, Generation, Acquisition Videosmall. earlier approaches confined representing meaningverbs, employ representation words lexicon regardlesspart speech. allows us combine word models together sentencemodels, essence, creating large factorial models. Unlike earlier work (Kulkarni, Premraj, Dhar, Li, Choi, Berg, & Berg, 2011; Hanckmann, Schutte, & Burghouts, 2012; Barbu,Bridge, Burchill, Coroian, Dickinson, Fidler, Michaux, Mussman, Siddharth, Salvi, Schmidt,Shangguan, Siskind, Waggoner, Wang, Wei, Yin, & Zhang, 2012a; Krishnamoorthy, Malkarnenkar, Mooney, Saenko, & Guadarrama, 2013), exploit linguistics, namely conceptlinking, construct particular factorial model encodes predicate-argumentstructure specific sentence, sentences happen share words.example sentence, person picked backpack different meaningsentence backpack picked person, despite sharing words,method encodes distinctions.overview operation sentence tracker shown Figure 1. Informationextracted video using object detectors optical flow, discussed Section 2.1.Independently, sentence parsed number participants determined, togetherlinking function, discussed Sections 3. word sentenceassociated model, discussed Section 2.2. information extracted sentencecombines per-word models form model entire sentence, discussedSections 2.3 2.4. model takes, input, data extracted video clipcomputes well clip depicts given sentence, video-sentence score shownEquation 10.order formally articulate approach applications, representmeasure well video clip depicts sentence function : (B, s, ) 7 ( , J),B represents information extracted video clip, represents sentence,represents word meanings, video-sentence score, J collection tracks,one participant event described sentence, corresponding optimalvideo-sentence score. use SJ refer two components produced S.function internally makes use number L event participants , linking function.linking function maps arguments words sentence event participants.make use linking process, function : 7 (L, ), described Section 3,derive number L participants linking function . elaboratethree applications approach demonstrate: language inference, languagegeneration, language acquisition.language inference, one apply sentence tracker video clip B,depicts multiple simultaneous events taking place field view, two different sentences s1 s2 . words, one compute J1 = SJ (B, s1 , ) J2 = SJ (B, s2 , )yield two different track collections J1 J2 corresponding different setsparticipants different events described s1 s2 . demonstrate Section 5.3. Specifically, show language inference, unlike many approachesevent recognition, deals video depicts multiple simultaneous events,also sensitive subtle changes sentence meaning. present experimentconstruct minimal pairs sentences, given grammar, differ single lexicalconstituent, varying lexical constituent vary among parts speechsentential positions. example two sentences603fiYu, Siddharth, Barbu, & SiskindSection 3Section 2.1sentencevideo Blinking processnumberparticipants Lobjectdetectionslinking functionoptical flowlexiconsentence trackerword modelsSections 2.3 2.4Section 4Section 2.2track collection Jvideo-sentence scoreEquation 10Figure 1: overview approach presented roadmap presentation. Section 5.3 demonstrates language inference. Section 5.4 demonstrates language generation.Section 5.5 demonstrates language acquisition.person left trash put object.person right trash put object.minimal pairs differ preposition attached subject noun phrase.construct video corpus sentences minimal pairs occur simultaneouslyvideo clip demonstrate language inference sensitive changessentential meaning producing two distinct semantically appropriate sets tracksgiven two sentences input. conduct thorough1 evaluation, employvocabulary 17 lexical items (5 nouns, 2 adjectives, 4 verbs, 2 adverbs, 2 spatial-relationprepositions, 2 motion prepositions) video corpus 94 clips.1. thorough mean following:1. evaluate three applications general method: inference, generation, acquisition.2. show performance entire corpus, without cherry picking.3. illustrate deep semantic grounding way minimal pairs vary lexical itemssentential positions.4. demonstrate deep semantic grounding rendering thematic-role assignments sentencesassociated videos, illustrating correct assignment event participants roles predicatearguments.5. compare learned models ground-truth meaning representations precisely measureKL divergence Table 10.604fiGrounding Language Inference, Generation, Acquisition Videolanguage generation, take video clip B input systematically searchspace possible sentences s, generated context-free grammar, findsentence maximal video-sentence score:argmax (B, s, )generates sentence best describes input video clip B. demonstrateSection 5.4. Unlike previous approaches sentence generation videolargely ad hoc(Barbu et al., 2012a; Hanckmann et al., 2012; Krishnamoorthy et al., 2013),present approach optimal, sense generated sentenceproduce highest video-sentence score. evaluation language generationuses video corpus, grammar, lexicon used language inference.language acquisition, exploit fact simultaneously reasonpresence motion participants video clip meaning sentencedescribing clip compute models word meaning training set video clipspaired sentences. words, given training set {(B1 , s1 ), . . . , (BM , sM )}video-sentence pairs word meanings unknown, computeargmaxX(Bm , sm , )m=1finds word meanings maximize aggregate score video-sentencepairs training set. demonstrate Section 5.5. learn word meaningswithout needing annotate word refers attribute video withoutannotating tracks objects participate event described trainingsentences. conduct thorough evaluation, employ vocabulary 16 lexical items(6 nouns, 4 verbs, 2 adverbs, 2 spatial-relation prepositions, 2 motion prepositions)video corpus 94 clips total 276 video-sentence pairs constructed.central contribution work sentence tracker, precise mathematicalcomputational framework performing simultaneous object detection, multi-object tracking, action recognition, recognition multiple predicates assigned different subsetsparticipants, culminating Equation 10, implemented efficient algorithmillustrated Figures 11 12, implements exact inference joint model, alongmethod training solely videos paired sentential annotation.current focus computational linguistics community large-scale unrestricted text processing long time now. computer vision community currentlyundergoing similar transition towards processing large-scale unrestricted image videocorpora. sentence tracker intended process unrestricted text video.intended produce natural-sounding text descriptions video. concernedsemantics, reflected truth text descriptions accuracylearned meaning representations. Moreover, evaluate corpus considerablysmaller currently used computational linguistics computervision communities. intend work address orthogonal setconcerns:1. provide unified framework supports inference, generation, acquisition.605fiYu, Siddharth, Barbu, & Siskind2. demonstrate learns correct meanings words, prior meaningswords, video paired whole sentences, manual guidancewords correspond components video.3. demonstrate deep understanding sentential semantics, groundedvideo, derived systematic computational process deepword meanings grounded video.4. deep understanding allows framework distinguish subtle semanticdistinctions manifest two sentences differ single word wordorder, i.e., understands mapping objects detected videoparticular semantic roles play sentences.mean greater lesser limitations current work computational linguistics computer vision. Different research different limitations.four points highlight limitations current work exhibits absentwork presented here.2. Joint Tracking Event Recognitionrepresent word meanings, ultimately sentence meanings, constraintstime-varying spatial relations event participants: relative and/or absolutepositions, velocities, and/or accelerations. requires track positionsevent participants course video clip. ideal world, would ableaccurately determine object classes present video frameare, precisely determine positions instances classes fieldview. Unfortunately, current state art object detection far ideal.Object detectors achieve 3.8% 65% average precision PASCALVOC benchmark (Everingham et al., 2010). means that, practice, sufferfalse positives false negatives, illustrated Figure 2. wish producesingle detection person backpack, shown Figure 2(a), practice,often get spurious detections (false positives), happens person detectorFigure 2(b), fail obtain desired detection (false negatives), happensbackpack detector Figure 2(c).2.1 Detection-Based Trackinggeneral approach resolving problem overgenerate. lower acceptancethreshold detector, trading higher false-positive rate lower false-negativerate, Figure 2(d). attempt lower threshold sufficiently completely eliminate false negatives, biasing preponderance false positives. trackingproblem reduces problem selecting detections frames video clipassemble coherent tracks.Let us assume, moment, wish track single instance specified objectclass known present field view throughout video clip. track objectselecting single detection frame pool detections objectclass. sequence top-scoring detection frame might temporallycoherent, shown Figure 3(a). Likewise, temporally-coherent sequencedetections might consist low-scoring misdetections, shown Figure 3(b). Thus606fiGrounding Language Inference, Generation, Acquisition Video(a)(b)(c)(d)Figure 2: State-of-the-art object detectors imperfect. wish single detectionperson backpack, (a), practice often get spurious detections (falsepositives), (b), fail obtain desired detection (false negatives), (c).Reducing acceptance threshold biases detector trade higher false-positiverate lower false-negative rate, (d).approach balance two extremes incorporating detection scoretemporal-coherence score selection criterion. often yield desired track,shown Figure 3(c).adopt objective function linearly combines sum detectionscores video frames sum temporal-coherence score applied pairsadjacent video frames. formally, video clip B frames, J detections bt1 , . . . , btJ frame t, seek track j, namely sequence j 1 , . . . , j detectionindices, maximizes sum detection scores f (btj ) temporal-coherencescores g(bt1, bt ):j t1 jmaxjXt=1f (btj )!+Xt=2607g(bt1, bt )j t1 j!(1)fiYu, Siddharth, Barbu, & Siskind(a)(b)(c)Figure 3: Assembling track single detection per frame selected poolovergenerated detections. Selecting top-scoring detection frame videoclip yield incoherent track, shown (a). Selecting tracks maximize temporalcoherence lead tracks incorporating solely low-scoring misdetections, shown (b).Selecting tracks maximize appropriate combination detection score temporalcoherence score lead desired track, shown (c).objective function Equation 1 constitutes measure well video clip B depictstrack j. employ particular objective function optimized efficientlydynamic programming (Bellman, 1957), namely Viterbi (1967) algorithm.leads lattice, shown Figure 4. columns lattice correspond videoframes, detections frame constitute columns, track constitutes pathlattice.general approach tracking overgenerating detections selecting amongyield track known detection-based tracking (Han, Sethi, Hua, & Gong,2004; Avidan, 2004; Wu & Nevatia, 2007). approach using Viterbi algorithmpurpose first explored Wolf, Viterbi, Dixon (1989) track radar detections.relies analogy:... detections correspond HMM states, detection score correspondsHMM output probability, temporal-coherence score correspondsHMM state-transition probability, finding optimal track correspondsfinding maximum posteriori probability (MAP) estimate HMMstate sequence (where computation MAP estimate performed logspace).crucially rely analogy entire remainder paper.trivially modified denote MAP estimate log space suitablenormalization constant factor. purposes, however, relevantoptimizes linear combination two score components: sum state-based scoressum transition-based scores. particular, Viterbi algorithm appliedEquation 1, without constraint permissible values scores f (b) g(b , b).608fiGrounding Language Inference, Generation, Acquisition Videot=1t=2t=3j=1b11b21b31...bT1j=2b12b22b32...bT2j=3b13b23b33...bT3.........b1J 1b2J 2b3J 3j = Jtt=T......bTJgfdetection temporalscore coherencescoreFigure 4: lattice constructed Viterbi algorithm detection-based tracking.columns correspond video frames = 1, . . . , . column contains overgenerated collection bt1 , . . . , btJ detections frame. rows correspond detectionindices j. track j, namely sequence j 1 , . . . , j detection indices, correspondspath lattice. Viterbi algorithm finds path optimizes Equation 1,among exponentially many potential tracks, time O(T J 2 ), J maximumJ 1 , . . . , J .detection-based tracking framework general. use detectionsource(s), method f (b) scoring detections b, method g(b , b) scoring temporal coherence detections b b adjacent frames. work reported here, use deformable part model (DPM) detector (Felzenszwalb, Girshick,McAllester, & Ramanan, 2010a; Felzenszwalb, Girshick, & McAllester, 2010b) detection source, yields detections represented axis-aligned rectangles usescores provided DPM basis f (b). raw DPM score ranges. Nominally, Equation 1 Viterbi algorithm support scores. However,raw DPM scores, unfortunately, incomparable across object classes. reasonsdiscussed Section 2.3, joint tracking multiple objects requires detectionscores comparable across object classes. Moreover, reasons discussedSection 4, language acquisition requires moderately accurate indication objectclasses present field view, could ascertained detection scorescomparable across object classes. address above, normalize detectionscores f (b) within object class using sigmoid11 + exp((f (b) ))609fiYu, Siddharth, Barbu, & Siskindparameters empirically determined per object class detectionscore correlates probability detection true positive. convert this,values discussed later sections, log space, protect underflowfloating-point calculations. Choosing parameters fashion per-classbasis allows resulting detection scores comparable across classes. Noteresulting values f (b) range (, 0], take represent logprobabilities.use optical flow compute adjacent-frame temporal-coherence score. employFlowLib optical-flow library (Werlberger, Pock, & Bischof, 2010) onehighest-performing methods optical-flow benchmarks (Baker, Scharstein, Lewis, Roth,Black, & Szeliski, 2011). specifically, compute g(bt1, bt ), compute opticalj t1 jflow frame 1, compute average flow vector v inside axis-aligned rectangledetection bt1, forward project detection one frame translating rectanglej t1along v, compute square Euclidean distance centertranslated rectangle center corresponding rectangle btj . yieldsvalue measures well local detection displacement matches local estimatevelocity ranges 0 fashion inversely related temporalcoherence. wish value comparable detection score f (b) temporalcoherence neither overpowers overpowered detection score. Thus normalizetemporal coherence sigmoid well, using negative invert polarity,convert log space. Unlike detection score, single set sigmoid parametersused across object classes, temporal-coherence score dependsdetection centers. Note again, resulting values g(b , b) range(, 0], take represent log probabilities. Moreover, even thoughvalues f (b) g(b , b) range (, 0], values produced Equation 1also lie range, represent log probabilities.2.2 Event Recognition Based Motion Prole using HMMsGiven particular track collection, one determine whether tracks depict givenevent measuring time-varying properties tracks. properties couldrelative and/or absolute object positions, velocities, and/or accelerations. time-varyingproperties represented abstractly time-series feature vectors computedtracks. view, event recognition formulated time-series classification.classification performed hidden Markov models (HMMs), either computing likelihood MAP estimate. Let us limit consideration, moment, eventssingle participant. case, abstractly take HMM consistK states, state-transition function a(k , k) log space, output model h(k, b)denotes log probability generating detection b state k. Let us refercollection K, a, h event model . log space, MAP estimateparticular track j!!XXt1maxh(k , bj ) +a(k , k )(2)kt=1t=2k1 , . . . , kTk sequencestates. Let Bj denote detection sequence b1j 1 , . . . , bTjTselected video clip B track j. Equation 2 constitutes measure610fiGrounding Language Inference, Generation, Acquisition Videowell detection sequence Bj selected video clip B track j depicts eventmodel . Higher MAP estimates result tracks better depict event model.MAP estimates computed efficiently using Viterbi algorithm time O(T K 2 ).Note similarity Equations 2 1. due aforementioned analogy.Momentarily, crucially avail fact computedViterbi algorithm. first need address several subtleties formulation.use HMMs encode probability distributions time-series feature vectorsextracted object tracks. turn serve represent meanings verbsdescribe motion participant objects. example, meaning wordbounce might represented HMM, like Figure 5, places high probability track exhibits alternating downward upward motion.representations tolerant noisy input learned using Baum-Welch (Baum,Petrie, Soules, & Weiss, 1970; Baum, 1972), HMMs many states, many features,non-sparsely populated state-transition functions output models difficult humans understand create. Sections 5.3 5.4, conduct experimentshuman-generated meaning representations. While, Section 5.5, conduct experimentsmachine-learned meaning representations, also compare human-generatedmeaning representations. facilitate perspicuity human-generated meaning representations, adopt regular-expression notation, following representationmeaning word bounce:bounce = (movingDown+ movingUp+ )+above, movingDown(b) movingUp(b) predicates detections bused construct output model h(k, b) regular expression used determinenumber K states, state-transition function a(k , k), predicate employoutput model given state. straightforwardly converted finitestate machines (FSMs) can, turn, viewed special case HMMs 0/1state-transition functions output models (/0 log space).Equation 2 formulated abstractly around single state-transition function a(k , k).also must include distributions initial final states. Traditional HMM formulationsincorporate initial-state distributions final-state distributions. HMMsmight recognize prefix event specification constrained matchentire event specification. (Without initial-state distribution, might recognizesubinterval event specification.) actual formulations include initial-final-state distributions omit presentation sake expositoryclarity.Formulating output model h(k, b) depend detections singletrack allows HMM encode time-varying constraints single track.used represent meaning intransitive verb describes motion singleparticipant. wish, however, also able represent meanings transitive verbsdescribe motion pairs participants. accomplish extendingoutput model h(k, b1 , b2 ) depend pairs detections, one track.two distinct tracks j1 = (j11 , . . . , j1T ) j2 = (j21 , . . . , j2T ) two distinct participants,think deriving detection pool. allows extending611fiYu, Siddharth, Barbu, & Siskind0.010.990.990.980.010.010.0101.0upwardrightwardleftwarddownwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryupwardrightwardleftwarddownwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryupwardrightwardleftwarddownwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudevelocity orientationfirst argument first argumentFigure 5: HMM represents meaning word bounce track exhibitsalternating downward upward motion.Equation 2maxkXt=1h(k , btj , btj )12!+Xt=2a(k t1 , k )!(3)support this.HMMs susceptible short-term noise input signal. oneevent model, Figure 6(a), intended match time seriesinterval velocity zero, followed interval upwardmotion, followed interval velocity zero, may unintentionallymatch time series interval upward motion single framespurious result noisy tracking feature extraction. thing mighthappen FSM representationrest(b1 , b2 ) = stationary(b1 ) stationary(b2 ) close(b1 , b2 )action(b1 , b2 ) = stationary(b1 ) movingUp(b2 ) close(b1 , b2 )pick = rest+ action+ rest+intended model meaning pick period time agentstationary close patient subdivided three sequential intervals612fiGrounding Language Inference, Generation, Acquisition Video0.990.990.011.000.0101.0moving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationary0.99moving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudefirst argument(a)0.990.990.990.010.010.011.000.0101.0moving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudefirst argument(b)Figure 6: (a) HMM susceptible short-term noise input signal.central state might admit noisy impulse lasting single frame. (b) variant (a)constrains central interval hold least 3 frames.patient first stationary, moves up, stationary again.unintentionally match time series patient continually stationary exceptsingle frame spurious result noisy tracking feature extraction.address issue requiring central interval minimum duration.indicate regular-expression operator R{n,} = R. . R} R indicate| .{zR must repeated least n times. definitionnpick = rest+ action{3,} rest+reduced FSM within framework. Similarly, one add minimum stateduration requirement HMM, Figure 6(a), recoding Figure 6(b).handles short-term false positives, namely presence short-term spuriously true signal. also need handle short-term false negatives, namely intended613fiYu, Siddharth, Barbu, & Siskindlonger interval signal must meet specified condition fails dueshort-term failure meet condition. use new regular-expression operatorR[n,] = (R [true]){n,} indicate R must repeated least n times optionally single frame noise repetition. One extend HMMssimilar fashion though found need output modelsalready tolerate noise.Nominally, detections btj axis-aligned rectangles represented image coordinates. allows output models h(k, b) depend quantities computedsuch, e.g., position detection center, size detection, aspectratio detection, indicate notions like big, small, tall, wide. also allowstwo-track output models h(k, b1 , b2 ) depend quantities like distance detection centers orientation line centers, indicate notionslike close, far, above, below. Without information, possible outputmodels depend relative absolute velocity, would needed encode notionslike fast, slow, stationary, moving, upwards, downwards, towards, away from. One wayachieve would extend output models depend detections adjacent frames, h(k, b , b) h(k, b1 , b1 , b2 , b2 ). accomplish variantEquation 2 sums pairs adjacent detections.!Xt1t1maxh(k , bj t1 , bj ) + a(k , k )kt=2generalized extending sums three adjacent frames acceleration, even frames longer-term velocity acceleration. However,multiple-point estimates, e.g., two-point velocity estimates three-point acceleration estimates, suffer noise due inaccurate tracking. Moreover, extensions wouldsupport desired features could extracted image, color.Thus instead extend notion detection include information mightextracted image location detection, average hue opticalflow inside detection, retain initial formulation output models h(k, b)h(k, b1 , b2 ) depends detections single frame.2.3 Event Trackeraforementioned method operates feed-forward pipeline. Equation 1 produces tracksevent participants, time series feature vectors extracted tracks,time series classified HMMs detect verb/event occurrences. approach,however, brittle. Failure earlier pipeline necessarily leads failure laterpipeline. particularly concern, since pipeline starts object detectionsand, mentioned before, state-of-the-art object detection unreliable.Barbu et al. (2012b) presented novel approach addressing brittleness calledevent tracker. approach originates observation Equations 1 2share structure due aforementioned analogy, thus share analogousalgorithmic framework performing optimization analogous lattices.feed-forward pipeline essentially cascades algorithms lattices, shown Figure 7(a). independently optimizes Equation 1, measure well video clip B614fiGrounding Language Inference, Generation, Acquisition Videodepicts track j, Equation 2, measure well detection sequence Bjselected video clip B track j depicts event model , performing former latter, constructing latter optimization problem around track jproduced former. takes Equation 2 sole measure well videoclip B depicts event model . precisely, performs following optimization:!!XXmaxh(k , btt ) +a(k t1 , k )kt=1t=2!!(4)XXt1= argmaxf (btj ) +g(bj t1 , btj )jt=1t=2measure well detection sequence Bj selected video clip Btrack j depicts event model , might measure well video clip Bdepicts event model fails incorporate measure wellvideo clip B depicts track j. Thus, might instead take sum Equations 1 2measure well video clip B depicts event model . precisely,could adopt following measure involves optimization Equation 4:!# ""!!!#XXXXt1t1maxh(k , bt ) +f (bj ) +a(k , k )g(bj t1 , bj )+ maxjkt=1t=2t=1t=2!!XXg(bt1, bt )f (btj ) += argmaxj t1 jjt=2t=1(5)still independently optimizes track j Equation 1 state sequence kEquation 2. could, however, attempt jointly optimize track j statesequence k. could done lifting maximizations track jstate sequence k outside summation measures well video clip Bdepicts track j well detection sequence Bj selected video clip Btrack j depicts event model . leads following optimization problem:!!!!XXXXmaxa(k t1 , k )(6)f (btj ) +g(bt1, bt ) +h(k , btj ) +j t1 jj,kt=1t=2t=1t=2crucial observation Barbu et al. (2012b) Equation 6 structureEquations 1 2 optimized using Viterbi algorithm formingcross-product tracker HMM lattices, shown Figure 7(b), noderesulting lattice combines detection HMM state, shown Figure 7(c).Since width cross-product lattice O(JK), applying Viterbi algorithmcross-product lattice finds path optimizes Equation 6, among exponentiallymany potential paths, time O(T (JK)2 ).Like before, Equation 6 trivially modified denote MAP estimate log spacesuitable normalization. However, need constant factorintroduced normalization would change result joint optimizationtrack j state sequence k. Like before, Viterbi algorithm applied615fiYu, Siddharth, Barbu, & Siskind(a)t=1t=2t=3j=1b11b21b31...bT1j=2b12b22b32...bT2j=3b13b23b33...bT3j = Jt.........b1J 1b2J 2b3J 3t=1t=2t=3k=1111...1k=2222...2k=3333...3.........KKKt=T......bTJk=Kfgdetection temporalscore coherencescoremaxj(b)Xt=1tracker lattice!!Xt1f (bj ) +g(bj t1 , bj )maxkt=2t=1t=2t=3b11b21b31...bT1j=2b12b22b32...bT2j=3b13b23b33...bT3.........b1J 1b2J 2b3J 3Xt=1...Kverb lattice!!Xt1h(k , bj ) +a(k , k )t=2t=1t=2t=3k=1111...1k=2222...2k=3333...3.........KKKt=T......bTJk=Kfgdetection temporalscore coherencescoret=T......Khoutput statemodel transitionfunctiontracker lattice!!XXt1maxf (bj ) +g(bj t1 , bj ) +j,k...houtput statemodel transitionfunctionj=1j = Jtt=Tt=1t=2j=1j = JtXt=1verb lattice!!Xt1h(k , bj ) +a(k , k )t=2t=1t=2t=3b11 , 1b21 , 1b31 , 1t=T.........b1J 1 , 1b2J 2 , 1b3J 3 , 1...bTJ , 1b11 , 2b21 , 2b31 , 2...bT1 , 2.........b2J 2 , Kb3J 3 , K...bT1 , 1...(c)b1J 1 , Kg,......bTJ , Kf, hFigure 7: (a) pipeline consisting cascade tracker lattice followed HMM latticeused verb/event recognition. (a), finding track j optimizes measure wellvideo clip B depicts track, Equation 1, happens independently prior findingstate sequence k optimizes measure well detection sequence Bj selectedvideo clip B track j depicts event model , Equation 2, latter dependingtrack j produced former. Since portion Equation 2 used measurewell video clip B depicts event model , corresponds optimizing scoring functionEquation 4. Taking measure well video clip B depicts event modelcombination measures well video clip B depicts track j well detectionsequence Bj selected video clip B track j depicts event model viewedoptimizing scoring function Equation 5, sum two measures. (b) variant (a)jointly optimizes two measures corresponding optimization Equation 6 migratesoptimization outside sum. (c) method performing joint optimization (b)forming cross-product lattice.616fiGrounding Language Inference, Generation, Acquisition VideoEquation 6 without constraint permissible values detection score f (b),temporal-coherence score g(b , b), output model h(k, b), state-transition functiona(k , k) However, constraining lie range empirically allows servegood scoring function.event tracker ameliorates brittleness feed-forward pipeline allowing topdown information event influence tracking. Using HMMs event recognizersaccomplished selecting event model best fits event. involves runningevent model independently data. context running particular eventmodel data, event model could influence tracking top-down fashion.example, context evaluating well event model walk fits data,tracker would biased produce tracks move normal walking pace. Stationarytracks, move quickly, would depict target event wouldfiltered Equation 6 Equations 1, 4, 5, tracks comprisedhigh-scoring detections temporally coherent.Equation 6 jointly optimizes single tracker single event model. such,recognize events single participant, described intransitiveverbs. Events two participants, described transitive verbs,encoded using methods Section 2.2, using Equation 3 instead Equation 2forming cross product two trackers instead one.!!!!XXXXt1t1(7)g(bj t1 , bj )f (bj ) +g(bj t1 , bj ) +f (bj ) +max2211j1 ,j2 ,k21t=2t=1t=2!t=1!XXa(k t1 , k )h(k , btj , btj ) ++t=112t=2generalized two participants L participants.!#" L!X XXt1g(bj t1 , bj )maxf (bj ) +llJ,klt=2t=1l=1!!XXa(k t1 , k )+h(k , btj , . . . , btj ) +t=1L1(8)t=2above, J denotes track collection j1 , . . . , jL which, turn, comprises detectionindices jlt . Equations 7 8 also optimized Viterbi algorithm formingcross-product lattice. Since width cross-product lattice O(J L K), applyingViterbi algorithm cross-product lattice finds path optimizes Equation 8,among exponentially many potential paths, time O(T (J L K)2 ). Noteexponential number L participants. practice, however, arity semanticpredicate underlying events limited, three case ditransitive verbs.Let BJ denote detection-sequence collection b1j 1 , . . . , bTjT , . . . , b1j 1 , . . . , bTjT selected1L1Lvideo clip B track collection J. Equation 8 jointly optimizes measurewell video clip B depicts event model combination measureswell video clip B depicts track collection J well detectionsequence collection BJ selected video clip B track collection J depicts617fiYu, Siddharth, Barbu, & Siskindp2b1 b2b1p1p1b2p2b1p2b2 p1Figure 8: Example showing necessity normalization detection scores across different object classes. (left) Image depicting two pairs detections personbackpack object classes. (right top) Distribution raw detection scores two objectclasses. Indicated scores corresponding detections image f (b1 ) = 4,f (b2 ) = 6, f (p2 ) = 11, f (p1 ) = 14. (right bottom) Distribution detection scorestwo object classes cross-object-class normalization f (b1 ) = 5, f (b2 ) = 12,f (p2 ) = 9, f (p1 ) = 14.event model . Note Equation 8 involves summation multiple detection-scorecomponents f , one L participants. fact raw detection scoresincomparable across object class means detection scores different participantscontribute different extents final score. Figure 8 shows example differencesvariance detection scores person trash result better scoreEquation 8 spurious set detections. values p1 p2 indicate detectionsperson object class values b1 b2 indicate detections backpack objectclass. Let us assume pairs detections, (p1 , b1 ) (p2 , b2 ), match eventmodel, carry, equally well. case, raw detections scores would yield(p1 , b1 ) best match f (p1 ) + f (b1 ) > f (p2 ) + f (b2 ). reasonemploy normalization detection scores discussed Section 2.1.results selection correct pair detections, (p2 , b2 ), since normalization,f (p2 ) + f (b2 ) > f (p1 ) + f (b1 ).Figure 9 illustrates power event tracker. objective track person.However, due poor performance state-of-the-art person detector, producesstrong false-positive detections bench background. Even overgeneratingdetections, shown Figure 9(a), selecting track optimizes Equation 1,shown Figure 9(b), tracks bench background portion video clip,618fiGrounding Language Inference, Generation, Acquisition Video(a)(b)(c)Figure 9: Keyframes video clip demonstrates advantages eventtracker. (a) Overgenerated person detections. (b) Detections selected detection-basedtracking Equation 1. Note selects strong false-positive detection benchbackground able rule detections exception singlelarge jump, rest track happens temporally coherent. (c) Detections selectedevent tracker top-down information, form model transitiveverb carry, constraining detections fill role agent event, contextbackpack, patient, carried person bench.instead person. happens track largely temporally coherent withinsegments, combination strong false-positive detections background,overpowers adverse effect single large jump, thus yielding high score Equation 1.However, top-down information form event model transitive verb carry,linked two trackers, one agent one patient, selects track agent,comprising true-positive person detections, accurately reflects role playedperson event, shown Figure 9(c), backpack, patient, carriedperson bench background.2.4 Sentence Trackerevent tracker previous section, generally HMM-based event recognizers, model events varying numbers participants (one, two, L participants619fiYu, Siddharth, Barbu, & Siskindevent trackers Equations 6, 7, 8 one two participants HMM-basedevent recognizers Equations 2 3). Nominally, think eventsdescribed verbs: one-participant events intransitive verbs, two-participant eventstransitive verbs, three-participant events ditransitive verbs. Figures 25 28Appendix B gives examples HMMs represent meanings verbs. However,nothing framework formally restricts us so. meanings wordsparts speech often also represented HMMs. example, meaningnoun describes object class represented single-state one-participantHMM whose output model serves classifier object class. Figure 23 Appendix B gives examples HMMs represent meanings nouns. Similarly,meaning adjective describes object characteristics represented singlestate one-participant HMM whose output model serves select detections exhibitdesired characteristics reflected adjective. example, meanings adjectiveslike big tall could represented output models areas aspect ratiosparticipant detections. Likewise, meaning preposition describes spatialrelation two objects represented single-state two-participant HMMwhose output model serves select collection features encode relation.example, meaning preposition left could represented outputmodel relative x-coordinates detections participants. Figure 24Appendix B gives examples HMMs represent meanings spatial-relation prepositions. generally, static property either single participant, collectionparticipants, encoded single-state HMM.Multiple-state HMMs encode dynamic properties either single participantcollection participants. reflect meanings adverbs prepositionsaddition verbs. example, meaning adverb quickly describeschanging characteristics motion single participant could representedthree-state HMM describing transition motion, motion high velocity,back motion. Figure 29 Appendix B gives examples HMMs representmeanings adverbs. Similarly, meaning preposition towards describeschanging relative motion pair participants could represented threestate HMM describing transition agent distant goal, perioddistance agent goal decreases goal stationary,ending agent close goal. Figure 30 Appendix B gives examplesHMMs represent meanings motion prepositions.thus see distinction different parts speech primarily syntactic,semantic, i.e., word use reflected grammar, potential meaning.may coarse-grained trends, canonical structure realizations(CSRs) proposed Grimshaw (1979, 1981) Pinker (1984), nouns typically describe object class, adjectives typically describe object properties, verbs typically describeevent class, adverbs typically describe event properties, prepositions typically describespatial relations, universally case. intransitive verbs like sleep describe static object property, transitive verbs like hold describe staticspatial relation pairs objects, nouns like wedding describe event.might seem like overkill represent static classifiers single-state HMMs,several advantages adopting single uniform meaning representation form620fiGrounding Language Inference, Generation, Acquisition VideoHMMs. First, capacity multiple states affords ability encode resiliencetemporal noise. Thus practice, even static properties might robustly encodedmultiple states. Second, adopting single uniform representation simplifies overallframework associated algorithms.event tracker previous section could influence detection-based trackingtop-down information event model. event model could represent meaningindividual word. could constrain single track single-participant words likeintransitive verbs (Equation 6), pair tracks two-participant words like transitiveverbs (Equation 7), even collection L tracks L-participant words (Equation 8).possible take cross products multiple trackers single eventmodel, one extend framework take cross products multiple trackersmultiple event models, thereby constraining track collection jointly satisfycollection event models words s1 , . . . , sW sentence s.!#" L!X XXt1(9)maxf (bj ) +g(bj t1 , bj )llJ,Klt=2l=1" W t=1!!#X XXt1+hsw (kw, btj , . . . , btj ) +asw (kw, kw )w=1L1t=1t=2above, K denotes state-sequence collection k1 , . . . , kW which, turn, comprises. L distinct trackers distinct detection indices j selectstate indices kwloptimal detection participant l frame t.distinguish words lexicon occurrences sentences.refer former lexical entries e latter words w. given lexical entrymay appear one word sentence. lexicon contains E event models1 , . . . , E , one event model e lexical entry e. sentence formulatedsequence s1 , . . . , sW W lexical entries sw , one word w. Equation 9 W distinctevent models sw , one word w sentence s, taken event modellexical entry sw word w. event model sw distinct numbers Kswstates, state-transition functions asw , output models hsw . Note statetransition functions asw output models hsw vary word w, detection score ftemporal-coherence score g vary participant l., bt , . . . , bt ) word wformulated Equation 9, output model hsw (kwjLj1tdepends detections frame selected tracks j1 , . . . , jL L participants.practice, meaning individual word applies subset participants,illustrated Figure 10. Here, sentence person left stool carriedtraffic cone towards trash describes event four participants:agent, referent, patient, goal. nouns person, stool, traffic cone trashrefer agent, referent, patient, goal respectively. verb carried describessemantic relation agent patient. preposition leftdescribes semantic relation agent referent. prepositiontowards describes semantic relation agent goal. employindicate participant fills argument event modellinking function wword w. Let Bhs, t, w, Ji denote btj , . . . , btj , collection detections selectedw w1w621fiYu, Siddharth, Barbu, & Siskindframe track collection J assigned Isw arguments event modelword w linking function . incorporate arity event model , alongnumber K states, state-transition function a, output model h.allows reformulating Equation 9!!#" LXX Xg(bt1, bt )f (btj ) +(10)maxjlt1 jllJ,Kt=2t=1l=1"W!!#X XXt1+hsw (kw , Bhs, t, w, Ji) +asw (kw , kw )w=1t=1t=2refer Equation 10 sentence tracker. remainder paper, Isw 2.Equation 10 also optimized Viterbi algorithm forming cross-productlattice. Since width cross-product lattice O(J L K W ), K maximum Ks1 , . . . , KsW , applying Viterbi algorithm cross-product lattice findspath optimizes Equation 10, among exponentially many potential paths, timeO(T (J L K W )2 ). Note exponential number L participantssentence length W . practice, however, natural-language sentences bounded lengthtypically short. Moreover, quadratic time complexity mitigated somewhatWfact K W approximationKsw . practice, nouns, adjectives,w=1spatial-relation prepositions describe static properties tracks thus word models Ksw = 1. Even longer sentences comprised predominantly wordmodels contain relatively verbs, adverbs, motion prepositions.Modeling meaning sentence collection words whose meaningsmodeled HMMs defines factorial HMM sentence, overall Markovprocess sentence factored independent component processes (Brand, Oliver,& Pentland, 1997; Zhong & Ghosh, 2001) individual words. view, K denotesstate sequence combined factorial HMM kw denotes factor statesequence word w. Figure 11 illustrates formation cross product two trackerlattices (Equation 1) three word lattices (Equation 2), linked together appropriatelinking function implement sentence tracker (Equation 10) sentenceperson carried backpack. Figure 12 illustrates resulting cross-product latticenode lattice consists combination two detections, one trackerlattice, three HMM states, one word lattice. state thus representednode cross-product lattice factored collection states writteninside node separated commas.Equation 10 constitutes : (B, s, ) 7 ( , J). scores video-sentence pairmeasure well given video clip B depicts given sentence s, interpreted givenlexicon . Alternatively, score measures well given sentence s, interpretedgiven lexicon , describes given video clip B. J 1 , . . . , J determined B,W determined s, arities Isw , numbers Ksw states, state-transitionfunctions asw output models hsw taken words models sw ,number L participants linking function computed sentencelinking process : 7 (L, ) described Section 3. result Equation 10 constitutesvideo-sentence score . track collection yields score constitutes J.622fiGrounding Language Inference, Generation, Acquisition Videoperson left stool carried trac cone towards trash can.wagentpatientreferentgoaljltdetection 0detection 1detection 2detection 33012Figure 10: illustration linking function used sentence tracker.word sentence one arguments. (When words two arguments,first argument indicated solid line second dashed line.)argument word filled participant event described sentence.given participant fill arguments one words. participant trackedtracker selects detections pool detections produced multiple objectarguments words w participants determineddetectors. upper mapping wparsing sentence. lower mapping jlt participants l frames detectionsdetermined automatically Equation 10. figure shows possible (but erroneous)interpretation sentence lower mapping, indicated darker lines, is:agent 7 detection 3, referent 7 detection 0, patient 7 detection 1, goal 7detection 2.623fiYu, Siddharth, Barbu, & Siskindt=1t=2t=3t=1t=2t=3j1 = 1b11b21b31...bT1j1 = 1b11b21b31...bT1j1 = 2b12b22b32...bT2j1 = 2b12b22b32...bT2j1 = 3b13b23b33...bT3j1 = 3b13b23b33...bT3..................b1J 1b2J 2b3J 3b1J 1b2J 2b3J 3...bTJj1 = Jfat=T......bTJj1 = Jfpgat=1t=2t=3111...1k wp = 2222...2k wp = 3333...3.........KswpKswpKswpkwp = Kswppatient-trackert=1t=2t=3k wc = 1111...1k wc = 2222...2k wc = 3333...3.........KswcKswcKswct=T......Kswph wp wp...gpagent-trackerk wp = 1t=Tkwc = Kswct=1t=2t=3k wb = 1111...1k wb = 2222...2k wb = 3333...3.........KswbK swbK wb...K wbt=T......Kswck w b = K wb...h wb wbhs wc wcpersont=TbackpackcarriedFigure 11: Forming cross product two tracker lattices (Equation 1) three wordlattices (Equation 2) implement sentence tracker (Equation 10) sentenceperson carried backpack. connections tracker lattices wordlattices denote linking function .t=1t=2t=3b11 ,b111,1,1b21 ,b211,1,1b31 ,b311,1,1.........j1 =J ,j2 =Jkwp =1,kwc =1,kwb =1b1 1 ,b1 1JJ1,1,1b2 2 ,b2 2JJ1,1,1b3 3 ,b3 3JJ1,1,1...bT ,bTJJ1,1,1j1 =1,j2 =1kwp =1,kwc =1,kwb =2b11 ,b111,1,2b21 ,b211,1,2b31 ,b311,1,2...bT1 ,b11,1,2.........j1 =1,j2 =1kwp =1,kwc =1,kwb =1j1 =J ,j2 =Jkwp =Kswp ,kwc =Kswc ,kwb =Kswb1 1 ,b1 1JJKswp ,Kswc ,Kswbbga , gp , aswp , aswc , aswbb2 2 ,b2 2JJKswp ,Kswc ,Kswbb3 3 ,b3 3JJKswp ,Kswc ,Kswt=T...bT1 ,b11,1,1.........bbT ,bTJJKswp ,Kswc ,Kswbfa , fp , hswp , hswc , hswbFigure 12: actual cross-product lattice produced example Figure 11. Notenode lattice consists combination two detections, onetracker lattice, three HMM states, one word lattice.624fiGrounding Language Inference, Generation, Acquisition Video3. Linking Processsentence tracker requires specification number L participants linkingindicates participant fills argument word w argumentfunction wword sentence. Often, participant (i.e., tracker) fill multiplearguments multiple words. sentence likeperson right chair picked backpack| {z } || {z } | {z }| {z }{z}(11)11 = 1 21 = 1 22 = 2 31 = 2 41 = 1 42 = 3 51 = 3(12)123453 participants requires linking function likeassigns argument person first argument rightpicked first participant, argument chair second argumentright second participant, argument backpack second argumentpicked third participant. number L participants sentence s,corresponding linking function , produced linking process : 7 (L, ).use particular linking process described details Appendix A. process makes use techniques mainstream linguistics, namely X-bar theory (Jackendoff,1977) government relations (Chomsky, 1982; Aoun & Sportiche, 1983; Haegeman, 1992;Chomsky, 2002). such, limited small hand-built grammar (Figure 11a)small lexicon (Figure 11b). purposes, restrictive. state artcomputer vision limits number distinct object classes reliably detectednumber distinct action classes reliably detected. restrictsnumber nouns verbs supported method, ours, attempts ground language computer vision methods detect objects actions.restricts class utterances constructed small setnouns verbs. this, small hand-constructed grammar suffices. one couldconceivably use methods support larger grammars vocabularies processlarger space unrestricted text, would possible ground currentstate-of-the art computer vision techniques. discuss detail Sections 67.linking process employ uses well-known techniques mainstream linguistics. central contribution work. Rather, central contributionsentence tracker (Section 2.4). sentence tracker requires linking process : 7 (L, ) maps sentence number L participants linkingfunction . need restricted particular grammar lexicon Figure 11. Indeed, employ one plethora well-known well-understoodtechniques common computational linguistics community. need evenrestricted particular grammar lexicon. possible construct linkingprocess standard mechanisms, dependency relations produced parsingdependency grammar. example, Stanford Parser (Klein & Manning, 2003)produces dependencies right sentence Equation 11, alsoused determine requisite number participants construct requisite linkingfunction. output correctly identifies three participants, person-2, chair-8,625fiYu, Siddharth, Barbu, & Siskinddet(person-2, The-1)nsubj(picked-9, person-2)det(right-5, the-4)prep_to(person-2, right-5)det(chair-8, the-7)prep_of(right-5, chair-8)root(ROOT-0, picked-9)prt(picked-9, up-10)det(backpack-12, the-11)dobj(picked-9, backpack-12)backpack-12. Note transitive verb picked-9 distinguishes two arguments,identifying person-2 first argument nsubj dependency backpack-12second argument dobj dependency. Also note spatial relationright-5 distinguishes two arguments, identifying person-2 first argumentprep dependency chair-8 second argument prepdependency.4. Language Acquisition Sentence TrackerChildren learn language exposure rich perceptual context. observe eventshearing descriptions events. correlating many events correspondingdescriptions, learn map words, phrases, sentences meaning representationsrefer world. come know noun chair refers object classtypically back four legs. also come know verb approachrefers dynamic process one object moves towards another. learnedconcepts purely symbolic; used decide presence absenceintended reference perceptual input. Thus concepts perceptually grounded.children learn language, usually given informationwords sentence correspond concepts see. example, child hearsdog chased cat seeing dog chase cat, prior knowledgemeaning word sentence, might entertain least two possible correspondencesmappings: (i) dog 7 dog cat 7 cat (ii) dog 7 cat cat 7 dog. first,child might assume chased means ran second child mightassume means ran before. Thus child hears description contextobserved event need disambiguate among several possible interpretationsmeanings words description. Things get worse process exhibitsreferential uncertainty (Siskind, 1996): multiple simultaneous descriptions contextmultiple simultaneous events.situation faced children motivates formulation shown Figure 13,video clips represent children see textual sentences represent hear. Notegiven video clip paired one sentence given sentencepaired one video clip. Siskind (1996, 2001) showed even referentialuncertainty noise, system based cross-situational learning (Smith, Smith, Blythe, &Vogt, 2006; Smith, Smith, & Blythe, 2011) robustly acquire lexicon, mapping wordsword-level meanings sentences paired sentence-level meanings. However,symbolic representations word- sentence-level meanings626fiGrounding Language Inference, Generation, Acquisition Videoperson picked traffic cone left stool.person picked traffic cone.person carried chair.chair approached backpack.chair approached traffic cone slowly.person carried chair away backpack.Figure 13: Video-sentence pairs language-acquisition problem. video clippaired multiple sentences sentence paired multiple video clips.perceptually grounded. ideal system would require detailed word-level labelingsacquire word meanings video rather could learn language largely unsupervisedfashion, child does, video paired sentences. algorithm presentedsection resolve ambiguity inherent referential uncertainty yieldlexicon intended meaning word. algorithm solve problemreminiscent faced children, make psychological neurophysiologicalclaims.One view language-acquisition task constraint-satisfaction problem (CSP),depicted Figure 14. treats words variables, initially unknownmeaning. video-sentence pair viewed constraint imposed wordssentence: words sentence mutually constrained requirementcollection word meanings allow sentence describe video clip. constraintformulated using variant sentence tracker Section 2. Sinceword may appear different sentences, sufficient number video-sentence pairsform connected network. two types inference network. First, oneperform inference across different words sentence. Suppose knowmeanings words sentence except one. case, meaningunknown word inferred applying video-sentence constraint. example,Figure 14, know meaning backpack person, meaning picked could627fiYu, Siddharth, Barbu, & Siskindchairpickedperson picked chair.(b)personchair approached backpack.person picked backpack.(a)approachedbackpackFigure 14: Viewing language acquisition constraint-satisfaction problem (CSP)solved propagating information word meanings around network. Word meaningsgreen used learn word meanings orange used learnword meanings red. performs inference across different wordssentence, shown (a), word different sentences, shown (b).inferred constraint (a), process occurredperson backpack. Second, one perform inference across worddifferent sentences. meaning given word shared exploited multiplesentences inferring meanings words sentences. example,learning meaning picked up, constraint (b), meaning chair alsoinferred. Thus, information word meanings propagate network.result, word meanings mutually constrained learned. Siskind (1996) referslearning mechanism cross-situational learning. practice, process startsinformation word meanings. formulation using EM (Dempster,Laird, & Rubin, 1977) propagate partial information word meanings. Thusstarting initial guess meaning word iterating process,converge intended lexicon.discussed earlier, sentence tracker supports representing word meanings HMMsFSMS, special case HMMs state-transition functions output mod628fiGrounding Language Inference, Generation, Acquisition Videoels 0/1 (/0 log space). Section 5.2, formulate output modelsmanually-constructed FSMs regular expressions Boolean features computeddetections using predicates shown Table 6. procedure learning wordmeanings employs HMMs state-transition functions output models0/1. case, output models derived features shown Table 8.use denote computation produces feature vectors detections Ndenote length feature vectors. Word models extended incorporate N.employ discrete distributions output models h. Further, assumedistributions factorial features, i.e., distributions featuresfeature vector independent. end, quantize feature bins.particular binning process described Section 5.5. means output modelstake formNX(k, b1 , . . . , bIe ) =hne (k, ne (b1 , . . . , bIe ))n=1ne (b1 , . . . , bIe ) {ne,1 , . . . , ne,Zen }Zen indicates number bins feature n lexical entry e ne,z indicatesquantized value bin z feature n lexical entry e.learning procedure makes five assumptions.1. training set contains samples, pairing short video clip Bmsentence sm describes clip. procedure able determinealignment multiple sentences longer video segments. Noterequirement clip depict sentence. objects may presentevents may occur. fact, nothing precludes training set multiplecopies clip, paired different sentence describing differentaspect clip. Similarly, nothing precludes training set multiple copiessentence, paired different clip depicts sentence.Moreover, procedure potentially handle small amount noise, clippaired incorrect sentence describe clip.2. already (pre-trained) low-level object detectors capable detecting instancestarget event participants individual frames video. allow detections unreliable; method handle moderate amount false positivesfalse negatives using techniques Section 2. need knowmapping object-detection classes nouns; procedure determines that.words, detectors locate classify objects symbolic labels likechair, labels distinct lexical entries like chair. procedure learnsmapping lexical entries object-class labels. mapping needone-to-one noisy. Learning mapping, however, requiresobject classes present times, would provide constraintrequired learn mappinga lexical entry could correspond object class.case, additionally need identify object classespresent video clip. made possible fact detection scoresrendered comparable, using normalization process described Section 2.1,629fiYu, Siddharth, Barbu, & Siskindthus use normalized scores indicator object presencevideo clip.3. know part speech ce associated lexical entry e. particularmapping lexical entry part speech used experiments Section 5.5given Table 11(a).4. word models lexical entries part speecharity I, number K states, feature-vector length N ,computation produces feature vectors, together associatedbinning process quantizing features. values known learned.particular values parameters used experiments Section 5.5given Table 8.5. know linking process grammar lexicon portion neededdetermine number L participants linking function trainingsentence. particular linking process used experiments Section 5.5described Section 3 using grammar lexicon portion Table 11.know track collection J chosen training sample. determinedautomatically methods Section 2.grammar, portions lexicon , namely components I, K, N , ,linking process prespecified learned. state-transition functionsoutput models hn learned. One imagine learning grammar,nonlearned portions lexicon, perhaps even linking process ,done Kwiatkowski, Goldwater, Zettlemoyer, Steedman (2012). leavefuture work.4.1 General Approachgiven grammar, portions lexicon , namely components I, K, N , ,linking process . lexicon contains E word models e lexical entries e.given training set samples, video clip Bm paired sentence sm . Let Bdenote B1 , . . . , BM denote s1 , . . . , sM . use grammar, nonlearned portionslexicon , linking process determine number L participantslinking function training sentence. state-transition functions aeoutput models hne word models e lexicon , could instantiatesentence tracker Equation 10 training sample compute video-sentencescore sample. side effect would compute track collection Jyielded video-sentence score. Moreover, could compute aggregate scoreentire training set summing per-sample scores. However, dont knowstate-transition functions ae output models hne . constitute unknownmeanings words training set wish learn. jointly learn aehne lexical entries e searching maximize aggregate score.4.2 Learning Procedureperform search Baum-Welch. Equation 10 constitutes score potentially could maximized, easier adapt scoring function likelikelihood calculation, Equation 10, like MAP estimate, EM630fiGrounding Language Inference, Generation, Acquisition Videoframework. PThus convert Equation 10 log space linear space replacemaxredefine scoring function follows:XJ,K"Lf (btj )lt=1"l=1Ww=1!t=2, bt )g(bt1jlt1 jlhsw (kw, Bhs, t, w, Ji)t=1!!#(13)t1, kw )asw (kwt=2!#f , g, h, linear space. Recall Equation 6 jointly maximizes summeasure well video clip B depicts track j measure well detectionsequence Bj selected video clip B track j depicts event model . Similarly,Equation 10 jointly maximizes sum measure well video clip B depictstrack collection J measure well detection-sequence collection BJ selectedvideo clip B track collection J depicts given sentence s, interpretedgiven lexicon . One maximize first component latter sum.!#!" LXX Xt1(14)maxg(bj t1 , bj )f (bj ) +Jl=1t=1lt=2llvariant Equation 1 track collection. OneP similarly convert Equation 14log space linear space replace maxyield:XJ"Ll=1t=1f (btj )l!t=2, bt )g(bt1jlt1 jl!#(15)suitable normalization constant factor, Equation 15 used obtainprobability particular track collection J relative distribution possibletrack collections probability given track collection proportionalsummand. Let us denote probability given track collection J P (J|B).given track collection J, one similarly maximize measure welldetection-sequence collection BJ selected video clip B track collection Jdepicts sentence s, interpreted given lexicon ."W!!#X XXt1max(16)hsw (kw , Bhs, t, w, Ji) +asw (kw , kw )Kw=1t=1t=2variant Equation 2 factorial HMM multiple words. OneP similarlyconvert Equation 16 log space linear space replace maxyield:XK"Ww=1, Bhs, t, w, Ji)hsw (kwt=1!t=2t1, kw )asw (kw!#(17)summand Equation 17 joint probability state sequence K BJ depictingsentence s, interpreted given lexicon : P (K, BJ |s, ) = P (BJ |K, s, )P (K|s, ).631fiYu, Siddharth, Barbu, & SiskindEquation 17 (marginal) probability BJ depicting sentence s, interpretedgiven lexicon : P (BJ |s, ). divide Equation 13 Equation 15 obtain:L(B; s, ) =XP (J|B)P (BJ |s, )Jexpected probability BJ depicting sentence s, interpreted givenlexicon , track collection distribution underlying P (J|B). Equations 13 15computed efficiently forward algorithm (Baum & Petrie, 1966).allows us take L(B; s, ) sample score adoptL(B; S, ) =L(Bm ; sm , )m=1training-set score. seek h maximize L(B; S, ). Notesample training-set scores [0, 1].find local maximum objective function using techniques usedBaum-Welch. reestimation formulas derived auxiliary functionsanalogous used HMMs (Bilmes, 1998). Let us first define J = J1 , . . . , JMK = K1 , . . . , KM track collections state-sequence collections entire trainingset. let us define L(B, J , K; S, ) product summand Equation 13training set divided product Equation 15 training set. Thus have:XL(B; S, ) =L(B, J , K; S, )J ,Kadopt following auxiliary function:XF (, ) =L(B, J , K; S, ) log L(B, J , K; S, )J ,Kcurrent lexicon potential new lexicon. One showF (, ) F ( , ) implies L(B; S, ) L(B; S, ).X L(B, J , K; S, )L(B, J , K; S, )F (, ) F ( , ) = L(B; S, )logL(B; S, )L(B, J , K; S, )J ,KXL(B, J , K; S, )P (J , K|B, S, ) logL(B, J , K; S, )J ,KXL(B, J , K; S, )P (J , K|B, S, )logL(B, J , K; S, )J ,K= logX L(B, J , K; S, )J ,K= logL(B; S, )L(B; S, )L(B; S, )632fiGrounding Language Inference, Generation, Acquisition Videosecond step holds training-set score L(B; S, ) nonnegative.third step holds due Jensens (1906) inequality. Thus given current lexicon ,find new lexicon F (, ) F ( , ), one iterate process, increasingtraining-set score local maximum. done maximizing F (, )respect . Since L(B, J , K; S, ) proportional product summandsEquation 13 training set, product two terms, latterdepends , following holds:X L(B, J , K; S, )F (, )J ,KL(B; S, )X L(B, J , K; S, )J ,KL(B; S, )log L(B, J , K; S, )WmXXm=1 w=1TmX, Bm hsm , t, w, Jm i)log hsm,w (km,w|{z}t=1hTmXt1), km,wlog asm,w (km,w+{z}|t=2Tm number frames video clip Bm training sample , Wmnumber words sentence sm training sample m, sm,w lexical entrystateword w sentence sm training sample m, km,wstate kwsequence collection Km training sample m. above, Bm hsm , t, w, Jm extended, collection detections selected frame videodenote btj , . . . , btj1m,wIsm,wm,wclip Bm track collection Jm assigned Ism,w arguments word modelword w sentence sm linking function m,wproduced sm determinesparticipant argument word w sentence sm . Thus F (, ) comprises two terms,one which, H, weighted sum terms h which, A, weightedsum terms a. One maximize F (, ) maximizing H independently.lead reestimation procedures output models h state-transition functions a.First consider A. Rewrite term explicitly sum lexical entries e pairsstates k k.A===XXXt1 = k , kL(B, km,wm,w = k; S, )log ae (k , k)L(B; S, )t1 = k , kL(Bm , km,wm,w = k; sm , )L(Bm 6=m ; Sm 6=m , )log ae (k , k)L(Bm ; sm , )L(Bm 6=m ; Sm 6=m , )t1 = k , kL(Bm , km,wm,w = k; sm , )log ae (k , k)L(Bm ; sm , )633(18)fiYu, Siddharth, Barbu, & SiskindXdenotesTmWm XKe XKe XXE XXe=1 k =1 k=1 m=1 w=1 t=2sm,w =et1 = k , kL(B, km,wm,w = k; S, ) =X XL(B, J , K; S, )X XL(Bm , Jm , Km ; sm , )Jt1 = k , kL(Bm , km,wm,w = k; sm , ) =JmL(Bm 6=m ; Sm 6=m, )=Kt1km,w=kkm,w =kKmt1km,w=kkm,w =kL(Bm ; sm , )=16=msecond step Equation 18 holds assumption training samplesi.i.d. Taking derivative respect ae (k , k), get reestimationformula state-transition function:ae (k , k) := e (k )Wm XTmXt1 = k , kXL(Bm , km,wm,w = k; sm , )L(Bm ; sm , )m=1 w=1 t=2 |{z}sm,w =e(m,w,k ,k,t)coefficient e (k ) chosen normalize distribution sums one.reestimation formula output model derived similarly H.make use fact output model factorial model factors discretedistributions. linear space:(k, b1 , . . . , bIe ) =Nehne (k, ne (b1 , . . . , bIe ))n=1Again, rewrite H explicitly sum lexical entries e, states k, features n, bins z.H===XXXL(B, km,w= k, ne (Bm hsm , t, w, Jm i) = ne,z ; S, )log hne (k, ne,z )L(B; S, )L(Bm , km,w= k, ne (Bm hsm , t, w, Jm i) = ne,z ; sm , )L(Bm 6=m ; Sm 6=m , )log hne (k, ne,z )L(Bm ; sm , )L(Bm 6=m ; Sm 6=m , )L(Bm , km,w= k, ne (Bm hsm , t, w, Jm i) = ne,z ; sm , )log hne (k, ne,z )L(Bm ; sm , )634fiGrounding Language Inference, Generation, Acquisition VideoXndenotesZ e Wm TmKe XNe XE XX XXXe=1 k=1 n=1 z=1 m=1 w=1 t=1sm,w =e) =L(B, km,w= k, ne (Bm hsm , t, w, JmXi) = ne,z ; S, XL(B, J , K; S, )KJnne (Bm hsm ,t,w,Jm i)=e,z km,w =knL(Bm , km,w= k, ne (Bm hsm , t, w, JX, ) =i) = e,z ; smXL(Bm , Jm , Km ; sm , )KmJmnne (Bm hsm ,t,w,Jm i)=e,z km,w =kTaking derivative H respect hne (k, ne,z ), get reestimation formulaoutput model:hne (k, ) := en (k)TmWm XXXL(Bm , km,w= k, ne (Bm hsm , t, w, Jm i) = ; sm , )L(Bm ; sm , )m=1 w=1 t=1 |{z}sm,w =e(m,w,n,k,,t)coefficient en (k) chosen normalize distribution sums one.reestimation formulas involve occurrence counting. Since use factorial HMMsinvolve cross-product lattice use scoring function derived Equation 13incorporates tracking (Equation 1) word models (Equation 2), need count occurrences whole cross-product lattice. example cross-product occurrencecounting, counting transitions state k k word w frame t1sample m, i.e., (m, w, k , k, t), need count possible paths adjacentt1t1t1t1, . . . , jtfactorial states, i.e., jm,1, . . . , jm,L, km,1, . . . , km,Wjm,1m,L , km,1 , . . . , km,Wt1 = k kkm,wm,w = k. Similarly, counting frequencystate k observing value feature n frame sample word w,i.e., (m, w, n, k, , t), need count possible paths factorial state, . . . , jtnjm,1m,L , km,1 , . . . , km,W km,w = k e (Bm hsm , t, w, Jm i) = .reestimation one word model depend previous estimate wordmodels. dependence happens linking function assign participant arguments different words sentence lexical entry appeardifferent training sentences. precisely dependence leads cross-situationallearning: former performs inference across different words sentencelatter performs inference across word different sentences.5. Experimentssentence tracker implements function : (B, s, ) 7 ( , J) takes video clip Binput, along sentence lexicon , produces, output, video-sentencescore , together track collection J depicts sentence interpretedlexicon . ability produce score track collection allows sentencetracker used variety ways, among them:635fiYu, Siddharth, Barbu, & Siskindlanguage inference Using track collection produces, take sentenceinput focus attention event described sentence. allowsprocessing video clip depicts many participants, various subsetsengaged different events, track particular participants engagedparticular event specified sentence.language generation Using score produces, generate sentential descriptions video clips efficiently searching space possible sentencesfind one best describes given clip.language acquisition Using score produces, learn word meaningstraining set video clips paired sentences describe clips, searchingspace potential word meanings find collectively allows sentencesbest describe associated clips.evaluate first use Section 5.3, second use Section 5.4, third useSection 5.5.5.1 Corporaconduct evaluation, filmed two different corpora, containing 94 video clips.One corpus used experiments Sections 5.3 5.4 usedexperiments Section 5.5. corpora filmed 640480 resolution 30 fps.contained clips varied length 3 5 seconds. filmedvariety outdoor environments, first varying three different environmentssecond varying four. camera moved filming clipvarying background precluded unanticipated confounds.video clips filmed variety actors objects. clips firstcorpus contain one two people collection three actors clipssecond corpus contain single person collection four actors. first corpusfilmed three objects, backpack, chair, trash can,present field view clips. second corpus filmed five objects,backpack, chair, traffic cone, trash can, stool, either two three presentfield view given clip. whole dataset counterbalanced avoidartifactual correlation. object class combination object classes appears clipsnearly equal frequency.four different environments second corpus used construct three different cross-validation folds. 29 video clips filmed one environment always containexactly two objects 23, 22, 20 clips filmed three environments respectively always contain exactly three objects. test set given foldcomprised clips filmed one latter three environments. Thus test setsthree folds contained 23, 22, 20 clips respectively. training set givenfold comprised clips except test set fold. Thus training setsthree folds contained 71, 72, 74 clips respectively.video clips depict multiple simultaneous events. depiction, clip clip,varied scene layout actor(s) performing event. clips first corpusdepicted one 21 sentences Table 1. clips second corpusdepicted one 187 sentences Tables 2 3. sentences636fiGrounding Language Inference, Generation, Acquisition Video1 a.b.2 a.b.3 a.b.4 a.b.5 a.b.6 a.b.7 a.b.8 a.b.9 a.b.1 0.1 1.1 2.backpack approached trash can.chair approached trash can.red object approached trash can.blue object approached trash can.person left trash put object.person right trash put object.person put trash can.person put backpack.person carried red object.person carried blue object.person picked object left trash can.person picked object right trash can.person picked object.person put object.person picked object quickly.person picked object slowly.person carried object towards trash can.person carried object away trash can.backpack approached chair.red object approached chair.person put chair.Table 1: selection sentences drawn grammar Table 11(a) basedcollected multiple video clips first corpus. Note sentence pairs 1 9constitute minimal pairs, single constituent varies two lexical entriespair. varying constituent ranges parts speech sentential positions.constrained conform grammar Table 11(a). 187 sentencessecond corpus divided two groups, one consisting 175 sentences usedexclusively training one consisting 12 sentences used exclusivelytest. delineation indicated horizontal line Table 3.corpora carefully constructed number ways. First, many video clipsdepict one sentence. particular, many clips depict simultaneous distinct events.Second, sentence describes multiple clips. Third, first corpus constructedminimal pairs: clips described pair sentences differ exactly one lexical item.minimal pairs help evaluate language inference indicated bvariants sentences 19 Table 1. varying lexical item carefully chosen spanparts speech sentential positions: sentence 1 varies subject noun, sentence 2varies subject adjective, sentence 3 varies subject preposition, sentence 4 varies object noun,sentence 5 varies object adjective, sentence 6 varies object preposition, sentence 7 variesverb, sentence 8 varies adverb, sentence 9 varies motion preposition. Fourth, clipsecond corpus contains subset objects used corpus. Withoutasymmetry would difficult (but impossible) determine correspondence637fiYu, Siddharth, Barbu, & Siskindchair approached stool.chair right backpack approached stool.chair left stool approached stool.person picked stool.person picked stool left backpack.person carried trash can.person carried trash left backpack.person put trash can.person put trash quickly.person put trash left stool.person left backpack put trash can.person picked chair.person picked chair quickly.person picked chair left traffic cone.person picked chair left backpack.person put chair.person put chair quickly.person left traffic cone put chair.person carried traffic cone.person left backpack carried traffic cone.person carried traffic cone away trash can.backpack approached traffic cone.backpack right chair approached traffic cone.backpack left traffic cone approached traffic cone.person put traffic cone.person put traffic cone left stool.person left chair put traffic cone.person carried backpack.person left chair carried backpack.person carried backpack away stool.person put stool left trash can.person approached trash can.stool approached trash can.person carried stool.person carried stool towards trash can.stool approached trash left traffic cone.backpack left traffic cone approached trash can.backpack right trash approached trash can.traffic cone approached stool left trash can.trash approached chair.trash left chair approached chair.trash approached chair left backpack.person approached chair.person picked trash left stool.person approached traffic cone.chair approached traffic cone.person left backpack approached traffic cone.person carried chair towards traffic cone.person put chair right backpack.person right traffic cone put chair.person right trash put traffic cone.person left backpack put traffic cone.person put traffic cone slowly.person picked chair right backpack.person right trash picked chair.stool approached traffic cone right chair.stool approached traffic cone left person.person picked traffic cone quickly.person picked traffic cone left stool.person left chair picked traffic cone.person picked backpack.person left chair picked backpack.person put backpack.person put backpack slowly.person right chair put backpack.person put backpack right trash can.traffic cone approached stool.traffic cone left trash approached stool.traffic cone right stool approached stool.backpack approached trash can.backpack approached trash right stool.backpack right stool approached trash can.person carried chair.person left stool carried chair.person carried chair left traffic cone.person picked trash can.person picked trash quickly.person picked trash right stool.person picked traffic cone.person picked traffic cone slowly.person left stool picked traffic cone.person picked traffic cone right trash can.stool approached traffic cone.stool left traffic cone approached traffic cone.stool right chair approached traffic cone.chair approached trash can.chair left traffic cone approached trash can.chair left trash approached trash can.person put stool.person left traffic cone put stool.traffic cone approached chair left stool.traffic cone approached chair.person carried traffic cone towards chair.person left stool carried traffic cone.person carried traffic cone away chair.person left stool put backpack.person put backpack right chair.person picked stool slowly.person right trash put stool.traffic cone approached trash can.traffic cone right stool approached trash can.chair approached stool left traffic cone.chair right stool approached stool.person left stool put trash can.person put trash left traffic cone.person approached stool.backpack approached stool.person carried backpack towards stool.backpack approached chair.backpack right chair approached chair.backpack right traffic cone approached chair.person carried stool away traffic cone.person left traffic cone picked backpack.stool approached backpack.stool approached backpack right trash can.stool left backpack approached backpack.person left chair approached stool.person carried stool towards chair.person left chair put stool.person put stool slowly.Table 2: selection sentences (first part) drawn grammar Table 11(a)used annotate clips second corpus.638fiGrounding Language Inference, Generation, Acquisition Videoperson right trash approached chair.person right trash carried chair.person right trash put chair.person put chair slowly.person right trash approached stool.person picked stool right trash can.person put stool right trash can.person left stool approached chair.person picked chair left stool.person carried chair towards stool.person left stool put chair.person right chair approached trash can.person picked trash right chair.person carried trash away chair.person put trash right chair.person picked stool quickly.person put stool quickly.person approached chair left stool.person put chair left stool.trash approached traffic cone.trash right backpack approached traffic cone.trash approached traffic cone right backpack.person right chair put trash can.person carried chair towards backpack.chair approached backpack.chair approached backpack left stool.person carried trash towards traffic cone.personpersonpersonpersonpersonpersonpicked stool right traffic cone.left stool picked trash can.put stool left chair.left trash carried stool.put backpack quickly.left backpack put chair.person right backpack picked stool.person right backpack picked traffic cone.person left trash picked traffic cone.trash approached stool.trash left stool approached stool.trash right chair approached stool.person picked trash left chair.person carried backpack away chair.person left traffic cone carried backpack.person carried stool away chair.person right chair picked backpack.person left trash picked backpack.person picked backpack quickly.traffic cone approached backpack.traffic cone left backpack approached backpack.traffic cone approached backpack left stool.person left traffic cone picked chair.person left trash put traffic cone.person put traffic cone right stool.person carried traffic cone towards trash can.person carried traffic cone away stool.stool approached chair.stool approached chair right traffic cone.stool right traffic cone approached chair.person left traffic cone put backpack.person right trash put backpack.chair left backpack approached backpack.chair approached backpack left trash can.trash left backpack approached chair.person carried trash towards chair.person picked chair slowly.person picked stool left chair.person picked backpack right trash can.person carried trash away backpack.Table 3: selection sentences (second part) drawn grammar Table 11(a).sentences horizontal line used annotate clips second corpusused test.nouns object classes. Note, however, since training clips containone object, task learning noun meanings still challenging. filmedcorpora unaware existing corpora exhibit properties.2annotated 94 video clips corpus human judgments.first corpus annotated 21 sentences Table 1, indicating whethergiven clip depicted given sentence. Table 4 provides statistics annotation.resulting set 94 21 = 1974 judgments associated statistics usedcompare contrast machine-generated results human judgmentsanalyses Sections 5.3 5.4. clip second corpus used either trainingtest, depending cross-validation fold, described earlier. includedtraining set, paired 1 5 sentences selected 1752. video clips, sentential annotation described below, code needed replicate experiments section available http://upplysingaoflun.ecn.purdue.edu/~qobi/cccp/grounding-language-in-video.html.639fiYu, Siddharth, Barbu, & Siskind#Clips depict given sentence#Sentences describe given clip12.332.766.481.22Table 4: Annotation statistics first corpus.#Clips depict given sentence#Sentences describe given clip2.000.370.580.61Table 5: Annotation statistics second corpus.training sentences Tables 2 3 deemed describe associated trainingclip human judge. average, training clip paired 2.94 sentences.Collectively, corpus contains 276 video-sentence pairs used training. threetraining folds contained 213, 208, 204 video-sentence pairs respectively. givenclip included test set, paired 12 test sentences Table 3. Thus94 29 = 65 potential test clips second corpus annotated12 test sentences Table 3, indicating whether given clip depicted givensentence. Table 5 provides statistics annotation. resulting set 65 12 = 780judgments associated statistics used compare contrast machinegenerated results human judgments analyses Section 5.5.experiments use off-the-shelf object detector (Felzenszwalb et al., 2010a,2010b) outputs detections form scored axis-aligned rectangles. particular, used implementation described Song, Zickler, Althoff, Girshick, Fritz, Geyer,Felzenszwalb, Darrell (2012). Using off-the-shelf software, trained six object detectors, one six object classes corpora: person, backpack, chair, trafficcone, trash can, stool. compensate false negatives, described Section 2.1,lowered acceptance threshold models produced automatic training.per-part thresholds uniformly reduced 1.2, model thresholds uniformlyreduced 2.0, non-maxima suppression set 0.6 first corpus 0.55second. applied person, backpack, chair, trash detectors uniformlyframes video clips first corpus six detectors framesclips second corpus. first corpus, selected five highest-scoring detections produced object detector frame pooled results yieldingtwenty detections per frame. second corpus, selected two highest-scoringdetections produced object detector frame pooled results yieldingtwelve detections per frame. larger pool detections per frame bettercompensate false negatives object detection potentially yield smoother tracks,increases size lattice concomitant running time leadappreciably better performance corpora.5.2 Manually-Constructed Lexiconsexperiments Sections 5.3 5.4 use manually-constructed FSMs represent wordmeanings evaluating language inference language generation. hand-writtenrepresentations word meaning clearly encode pretheoretic human intuition make640fiGrounding Language Inference, Generation, Acquisition Videointuition perspicuous. experiments, formulate word models lexicalentries Table 11(a) appear sentences Table 1. experiments Section 5.5learn word models represented HMMs. evaluated learned word models, part,comparison manually-constructed HMMs. manually-constructed HMMsdiscussed Section 5.5.formulate FSMs regular expressions predicates computed detections.particular set regular expressions associated predicates usedexperiments Sections 5.3 5.4 given Table 6. predicates formulatedaround number primitive functions. function avgFlow(b) computes vectorrepresents average optical flow inside detection b. function model(b) returnsobject class b. function x(b) returns x-coordinate center b. functionhue(b) returns average hue pixels inside b. function angleSep determinesangular distance two angular arguments. function fwdProj(b) displaces baverage optical flow inside b. function 6 determines angular component givenvector. function computes normal unit vector given vector. argument vnoJitter denotes specified direction represented 2D unit vector direction.Predicates take single detection b sole argument serve 0/1 outputmodels h(k, b) (/0 log space) single-participant word models. Predicatestake pair detections b1 b2 sole arguments serve 0/1 output modelsh(k, b1 , b2 ) (/0 log space) two-participant word models. Regular expressionsformulated around predicates atoms. given regular expression must formed solelyoutput models arity denotes word model 0/1 state-transitionfunction (/0 log space) output models associated appropriatestates.5.3 Experiment 1: Language InferenceTracking traditionally performed using cues motion, object detection, and/or manualinitialization object interest (Yilmaz, Javed, & Shah, 2006). However,case cluttered scene involving multiple events occurring simultaneously,many moving objects, many instances object class, perhaps even multiplesimultaneously occurring instances event class. illustrate oneuse sentential description guide tracking objects based ones participatetarget event.sentence tracker focus attention objects participateevent specified sentential description. description differentiatedifferent simultaneous events taking place many moving objects scene usingdescriptions constructed variety parts speech. Using nouns specify objectclass, one could differentiateperson picked backpackperson picked chair.Using adjectives specify object properties, one could differentiateperson picked red objectperson picked blue object.641fiYu, Siddharth, Barbu, & SiskindConstantsxBoundary = 300pxnextTo = 50pxstatic = 6pxslow = 30pxangle = 30closing = 10pxjump = 30pxquick = 80pxhue = 30Simple PredicatesnoJitter(b, v) = kavgFlow(b) vk jumpalike(b1 , b2 ) = model(b1 ) = model(b2 )close(b1 , b2 ) = |x(b1 ) x(b2 )| < xBoundaryfar(b1 , b2 ) = |x(b1 ) x(b2 )| xBoundaryleft(b1 , b2 ) = 0 < x(b2 ) x(b1 ) nextToright(b1 , b2 ) = 0 < x(b1 ) x(b2 ) nextTohasColor(b, hue) = angleSep(hue(b), hue) huestationary(b) = kavgFlow(b)k staticquick(b) = kavgFlow(b)k quickslow(b) = kavgFlow(b)k slowperson(b) = model(b) = personbackpack(b) = model(b) = backpackchair(b) = model(b) = chairtrashcan(b) = model(b) = trashcanblue(b) = hasColor(b, 225 )red(b) = hasColor(b, 0 )Complex PredicatesstationaryClose(b1 , b2 ) = stationary(b1 ) stationary(b2 ) alike(b1 , b2 ) close(b1 , b2 )stationaryFar(b1 , b2 ) = stationary(b1 ) stationary(b2 ) alike(b1 , b2 ) far(b1 , b2 )closer(b1 , b2 ) = |x(b1 ) x(b2 )| > |x(fwdProj(b1 )) x(b2 )| + closingfarther(b1 , b2 ) = |x(b1 ) x(b2 )| < |x(fwdProj(b1 )) x(b2 )| + closingmoveCloser(b1 , b2 ) = noJitter(b1 , (0, 1)) noJitter(b2 , (0, 1)) closer(b1 , b2 )moveFarther(b1 , b2 ) = noJitter(b1 , (0, 1)) noJitter(b2 , (0, 1)) farther(b1 , b2 )inDirection(b, v) = noJitter(b, (v)) stationary(b) angleSep(6 avgFlow(b), 6 v) < angleapproaching(b1 , b2 ) = alike(b1 , b2 ) stationary(b2 ) moveCloser(b1 , b2 )departing(b1 , b2 ) = alike(b1 , b2 ) stationary(b2 ) moveFarther(b1 , b2 )carry(b1 , b2 , v) = person(b1 ) alike(b1 , b2 ) inDirection(b1 , v) inDirection(b2 , v)carrying(b1 , b2 ) = carry(b1 , b2 , (0, 1)) carry(b1 , b2 , (0, 1))pickingUp(b1 , b2 ) = person(b1 ) alike(b1 , b2 ) stationary(b1 ) inDirection(b2 , (0, 1))puttingDown(b1 , b2 ) = person(b1 ) alike(b1 , b2 ) stationary(b1 ) inDirection(b2 , (0, 1))Regular Expressionsperson = person+trash = trashcan+blue = blue+backpack = backpack+object = (backpack | chair | trashcan)+quickly = true+ quick[3,] true+red = red+right = right+left = left+chair = chair+slowly = true+ slow[3,] true+approached = stationaryFar+ approaching[3,] stationaryClose+carried = stationaryClose+ carrying[3,] stationaryClose+picked = stationaryClose+ pickingUp[3,] stationaryClose+put = stationaryClose+ puttingDown[3,] stationaryClose+towards = stationaryFar+ approaching[3,] stationaryClose+away = stationaryClose+ departing[3,] stationaryFar+Table 6: FSMs representing meanings lexical entries Table 11(a)appear sentences Table 1 used experiments Sections 5.3 5.4.642fiGrounding Language Inference, Generation, Acquisition VideoUsing verbs specify events, one could differentiateperson picked red objectperson put red object.Using adverbs specify motion properties, one could differentiateperson quickly picked red objectperson slowly picked red object.Using prepositions specify (changing) spatial relations objects, one could differentiateperson right chair picked objectperson left chair picked object.Furthermore, sentential description even differentiate objects trackbased role play event: agent, patient, source, goal, referent.example, sentence person picked backpack left chair differsperson picked chair left backpack roles backpackchair exchanged. Although objects involved describedevents, roles events differ, distinguished tracker. Figure 15demonstrates ability: different tracks produced video depictsmultiple simultaneous events focused different sentences. figure, wellFigure 16 Figures 21 22 Appendix B, boxes around participantscolor coded indicate semantic role: agent red, patient blue, source violet, goalturquoise, referent green. particularly illustrates system understandsimage regions correspond participants particular mappingargument positions predicates denote meanings lexical items sententialdescription. illustrates deep semantic understanding.Figure 15 evaluates ability sentential position. Figure 21 Appendix Bevaluates ability 9 minimal pairs, indicated b variantssentences 19 Table 1, collectively applied 25 suitable video clips first corpus.discard two clips original set 9 3 = 27 video clips due factinvolve adjective (grey), corresponding chair, cannot reliably extractedvideo. 18 25, sentences minimal pair yielded trackcollections deemed correct depictions. determine error subjective humanjudgment whether track collection system produces matches desireddescription. errors encountered task fall one two categories. Onecategory deals use color adjective along generic word objectpresence entity video intended object incidentallysimilar color. sole error category involves tracker selecting detectionspersons red shirt instead red backpack, one three instances minimalpair 2 Table 1: red object approached chair blue object approachedchair. correct result obtained instance minimal pairassociated different video clips. category largely due deficienciesdetectors, particularly trash can. least four instances, paucity643fiYu, Siddharth, Barbu, & SiskindDifferentiateverbperson picked object.person put object.backpack approached trash can.chair approached trash can.red object approached chair.blue object approached chair.person left trash put object.person right trash put object.person put trash can.person put backpack.person carried red object.person carried blue object.person picked object left trash can.person picked object right trash can.person carried object towards trash can.person carried object away trash can.subjectnounadjectivesubjectNPprepositionsubjectNPobjectnounadjectiveobjectNPprepositionobjectNPprepositionadjunctFigure 15: Language inference: two different track collections video clip produced guidance two different sentences. clip processed minimal pair,sentence varies single lexical item highlighted red vs. green. varying lexicalitem varies among sentential positions across eight examples. Resultsvideo clips processed minimal pairs sentences 19 Table 1 includedFigure 21 Appendix B. figure, well Figures 16, 21, 22, indicatethematic role participants color bounding box: red box denotesagent, blue box denotes patient, violet box denotes source, turquoisebox denotes goal, green box denotes referent. roles determinedautomatically using techniques Appendix A.644fiGrounding Language Inference, Generation, Acquisition VideoContraction ThresholdAccuracy0.9567.02%0.9071.27%0.8564.89%Table 7: Accuracy function contraction threshold.detections trash detector results either poor tracks complete failuresatisfy FSMs corresponding word models. exacerbatedcase adverbs. Since adverbs modify verbs, verbs vary mannerexecution, tight bounds would constitute quickly slowly difficult obtain.bounds able impose sufficiently noisy sometimes distinctionaction happening quickly slowly lost. Two errors occur here, namelytwo instances minimal pair 8 Table 1: person picked object quicklyperson picked object slowly. correct result obtained remaininginstance minimal pair associated different video clip.5.4 Experiment 2: Language Generationuse ability sentence tracker score video-sentence pair generatesentence describes given video clip searching highest-scoring sentenceclip. However, problem. Recall f , g, h, values logspace range (, 0] increasing value denotes higher score, i.e., better fitmodel. Since sentence-tracker scoring function (Equation 10) sums these, scoresdecrease longer word strings greater numbers participants resultlonger word strings. dont actually search highest-scoring sentence,would bias process towards short sentences. Instead seek complex sentencesdescribe clip informative.Nominally, search process would intractable since space possible sentenceshuge even infinite. However, use beam search get approximateanswer. possible sentence tracker score word sequence,complete sentences, long one construct linking function . selecttop-scoring single-word sequences repeatedly extend top-scoring W -wordsequences, one word, select top-scoring W + 1-word sequences, subjectconstraint linking function exists W + 1 words W + 1-wordsequences extended grammatical sentences insertion additional words.terminate search process contraction threshold, ratio scoresequence score sequence expanding it, drops specifiedvalue sequence expanded complete sentence. contraction thresholdcontrols complexity generated sentence.restricted FSMs, h 0/1 become /0 log space. Thusincrease number words decrease score , meaning sequencewords no-longer describes video clip. Since seek sentences do, terminatebeam-search process score goes . case,approximation: beam search maintaining W -word sequences finite score yieldshighest-scoring sentence contraction threshold met.645fiYu, Siddharth, Barbu, & Siskindevaluate approach, searched space sentences generated grammarTable 11(a) find top-scoring sentence 94 video clips first corpus.Note grammar generates infinite number sentences due recursion NP.Even restricting grammar eliminate NP recursion yields space 147,123,874,800sentences. Despite restricting grammar fashion, able effectivelyfind good descriptions video clips.evaluated accuracy sentence tracker generating descriptions 94video clips first corpus multiple contraction thresholds. Accuracy computedpercentage 94 clips sentence tracker produced descriptionsdeemed describe video human judges. resulting accuracydifferent contraction thresholds shown Table 7. Figure 16 shows highest-scoringsentence generated approach several clips first corpus contractionthreshold 0.90. Figure 22 Appendix B shows highest-scoring sentence generatedapproach 94 clips first corpus. illustrate effectcontraction threshold, show below, generated sentence correspondingcontraction thresholds first video clip Figure 16.0.950.900.85backpack approached trash can.backpack left chair approached trash can.backpack left chair approached trash can.important distinction approach state art generatingsentential video description generativity labeling domain. existing work(Kulkarni et al., 2011; Gupta, Verma, & Jawahar, 2012), process labeling eventsvideo involves searching phrases sentences best match video using trainedset classifiers. process usually involves extracting correspondences labelsvideo features training corpus. training corpus labels video wordphrase sentence-generation process unseen video labels video eitherexisting label training corpus simple concatenation labels.contrast, approach label unseen video grammatical utterance admittedgrammar lexicon, potentially unbounded set, even ones neverappeared, whole part, training set.sentence-tracker framework also generate sentential video descriptionfixed set sentential labels, simply scoring potential label unseen videoclip selecting top-scoring label. evaluate ability labeling 94video clips first corpus fixed label set 21 sentences shown Table 1comparing human judgments. performed three analyses. First, measuredpercentage clips depict top-scoring sentence determined human judges.determined 94.68%. Chance performance 13.12%, since average, 2.76sentences deemed describe given clip, shown Table 4. Second, relaxselection criterion slightly, consider percentage clips described least onetop-three sentences, obtain 100% accuracy. Chance performance 1 (1 0.1312)334.42%. Finally, threshold video-sentence score, yielding binary machinejudgment whether given sentence describes given clip, alternatively whethergiven clip depicts given sentence. ask well machine judgmentsmatch human judgment 94 21 = 1974 video-sentence pairs first corpus.646fiGrounding Language Inference, Generation, Acquisition Videobackpack left chair approached trash can.person right backpack carried chair.person right trash approached trash can.chair right person approached trash can.backpack left trash approached trash can.Figure 16: Sentential descriptions generated several video clips first corpus subjectcontraction threshold 0.90. highest-scoring sentence clip generated,among sentences generated grammar Table 11(a), means beamsearch. sentences deemed human judges describe associated clips indicatedgreen, ones indicated red. Sentential descriptions generated94 video clips first corpus shown Figure 22 Appendix B.647fiYu, Siddharth, Barbu, & SiskindSearching threshold maximizes accuracy yields accuracy 86.88%.Chance performance 13.12%, since 259 1974 human judgments positive.Thus sentence tracker performs significantly chance three analyses.5.5 Experiment 3: Language Acquisitionsentence tracker, wrapped EM, learn lexicon maps wordsmeanings training set video clips paired sentences. crucial distinctionapproach prior state art learning object event recognizersvideo that, approach, training videos paired entire sentences,individual class labels. sentential labels generative; set possible labelsinfinite generated context-free grammar contains recursion. Thusvast majority potential labels never appear training set. Yet methodlearn describe previously unseen videos previously unseen sentential labelscomposed words likely occur single training sample instead requirecomposing words learned exposure distinct training samples.evaluate use sentence tracker perform language acquisition, employsecond corpus described Section 5.1, particular Tables 2 3, togethergrammar lexicon Table 11. language fragment contains 17 lexical entries6 parts speech (1 determiner, 6 nouns, 2 spatial-relation prepositions, 4 verbs,2 adverbs, 2 motion prepositions). model learn meanings contentwords lexicon. Table 8 specifies arity I, number K states, feature-vectorlength N , number Z bins fore feature, feature computationword models part speech c. specify different subset featurespart speech, presume that, principle, enough training data, could includefeatures parts speech automatically learn ones noninformativelead uniform distributions.compute continuous features, velocity, distance, size ratio, x-positiondetections quantize features bins follows:velocity reduce noise, compute velocity participant averaging opticalflow detection. velocity magnitude quantized 5 levels. expository clarity, refer levels mnemonically absolutely stationary, mostlystationary, moving slowly, moving quickly, moving quickly. velocity orientation quantized 4 directions: leftward, upward, rightward, downward.distance compute Euclidean distance detection centers two participants, quantized 3 levels: near, moderate distance, far.size ratio compute ratio detection area first participant detection area second participant, quantized 2 levels: larger smallerthan.x-position compute difference x-coordinates participants, quantized 2 levels: left right of.binning process determined preprocessing step clustered subsettraining data. addition continuous features need quantization, alsoincorporate index detector produced detection discrete feature.648fiGrounding Language Inference, Generation, Acquisition VideocK N ZN1116detector indexvelocity magnitude first argumentvelocity orientation first argumentvelocity magnitude second argumentvelocity orientation second argumentdistance first second argumentssize first argument / size second argumentV236545432P2112difference x-positions first second argumentsAdv 1315velocity magnitudePM3253velocity magnitude first argumentdistance first second arguments2Table 8: Characteristics HMMs used model word meanings various partsspeech c. denotes arity, K denotes number states, N denotes number featuresoutput model, Z denotes number bins particular feature, denotesfeature computation.detector index mainly used identifying detection learning nouns.particular features computed part speech given Table 8.Note use English phrases, like left of, refer particular binsparticular features, object detectors train samples particularobject class backpack, phrases mnemonic clustering objectdetector training process. fixed correspondence lexical entriesparticular feature value. Moreover, correspondence need one-to-one:given lexical entry may correspond (time variant) constellation feature valuesgiven feature value may participate meaning multiple lexical entries.performed three-fold cross validation using partitioning described Section 5.1.important stress fold, test set disjoint training set,video clips sentential labels. crucially allowed us evaluate generativenature sentential labels: ability learn generate previously unseen labelspreviously unseen video.fold, trained lexicon training set fold using procedureSection 4. evaluated trained lexicon test set foldperforming three distinct analyses:1. comparing F1 score test set variety baselines2. comparing ROC curve test set variety baselines3. inspection learned models comparison hand-constructed modelsfirst two analyses require scoring unseen video-sentence pairs. could scoredEquation 10. However, score depend sentence length W , lengthvideo clip, number L participants, collective numbers states Kfeature-vector lengths N word models words sentence. One remove649fiYu, Siddharth, Barbu, & SiskindFoldBaselinesmethodChanceBlindHand1230.060.070.040.100.120.080.730.650.500.560.500.31average0.060.100.620.46Table 9: comparison F1 scores test sets method varietybaselines.dependence number L participants using L(B; s, ) score. However,remove dependence factors.render scores comparable across variation, apply sentence-length prior(s) average per-frame score computed whole-video score L(B; s, ):1[L(B; s, )] (s)(s) = expWXw=1(Z) =ZXz=1Ns w(Ksw ) +Xn=111log = log ZZZ(Zsnw )above, (Z) entropy uniform distribution Z bins. prior preferslonger sentences descriptive video.resulting scores thresholded decide hits, together manualannotation, generate True Positive (TP), True Negative (TN), False Positive (FP),False Negative (FN) counts. conduct first analysis, fold, selectedthreshold led maximal F1 score training set, used thresholdcompute F1 score test set. Table 9 reports per-fold F1 scores alongaverage across folds.comparison, also report F1 scores three baselines: Chance, Blind,Hand. Chance baseline randomly classifies video-sentence pair hit probability 0.5. Blind baseline determines hits potentially looking sentencenever looking video. strategy make decision video-sentencepairs pairs contain sentence. find upper bound F1score blind method could test sets solving 0/1 fractionalprogramming problem follows. optimal blind baseline try find decision dmtest sentences sm maximizes F1 score. Suppose, comparisonground-truth yields FPm false positives TPm true positives test set650fiGrounding Language Inference, Generation, Acquisition Videodm = 1. Also suppose setting dm = 0 yields FNm false negatives. F1 score then:11+Xdm FPm + (1 dm )FNmm=1X2dm TPmm=1|{z}Thus maximize F1 seek minimize term . instance 0/1 fractionalprogramming problem solved binary search Dinkelbachs (1967) algorithm. yields best possible F1 score blind algorithm produce.Hand baseline determines hits hand-crafted HMMs described below.carefully designed yield believe near-optimal performance. seenTable 9, trained word models perform substantially better ChanceBlind baselines approach performance Hand baseline. corpuscounterbalanced, Chance Blind baselines exhibit similar poor performance.conduct second analysis, varied threshold used decide hits produceROC curves. Figure 17 shows curves folds along average acrossfolds, comparing trained word models various baselines. Again, trainedword models significantly outperform baselines essentially match performancehand-crafted word models.Good F1 scores ROC curves necessary sufficient demonstrate successful learning. possible trained word models reflect artifactual propertiescorpus dont encode natural pretheoretic intended meaning. example,dataset spurious unintended correlations, whenever approach happens,agent always larger goal, learned word model may reflect correlationcorrelation may primary factor leading good performance test set.artifactual correlation overly strong, could even overpower correlationsrelevant features allow learning meanings rely features would fail generalize corpora exhibit artifactualproperties.evaluate whether occurs experiments, conducted third analysiscompared trained word models (for fold 2) hand-crafted ones illustrated Figures 23 30 Appendix B. qualitative comparison, render hand-craftedtrained word models side side lexical entry, graphically illustratingoutput distributions textually illustrating initial-state state-transition-functiondistributions. Qualitative inspection indicates corresponding word models indeed quite similar except noise learned word models. crucial qualitativeobservation large extent initial-state state-transition-function distributions place bulk probability mass state relevant outputdistributions exhibit peaks bins. example, word person, twoword models peak first bin denotes object-detector class person.Similarly, word models verb approached describe qualitative motion profile.depict initial state which:651fiYu, Siddharth, Barbu, & Siskind(a)(b)(c)(d)Figure 17: ROC curves comparing performance trained models variousbaselines three folds (a-c) averaged across fold (d).1. agent goal stationary2. agent far goalfollowed intermediate state which:1. agent moving horizontally,2. goal stationary,3. distance participants decreasingfollowed final state which:1. agent goal stationary2. agent close goal.two primary qualitative differences learned hand-crafted distributions. first noise. second hand-crafted distributions irrelevantfeatures intentionally uniform learned distributions features sometimes encode artifactual properties corpus small extent. example,second state trained word model picked indicates first argument652fiGrounding Language Inference, Generation, Acquisition Videotrained word modelspersonbackpackchairtraffic conetrashstoolleftrightapproachedcarriedpickedputtowardsawayrandom word models123average123average0.000.000.000.000.000.000.000.0012.6315.899.408.731.713.210.000.000.000.000.000.000.000.0015.4310.609.4413.094.696.720.000.000.000.000.000.000.000.0012.4411.7410.9710.053.142.860.000.000.000.000.000.000.000.0013.5012.749.9410.623.184.271.115.434.172.090.821.121.190.5011.3214.4212.8616.593.9710.911.091.721.441.471.351.330.260.0918.9211.978.4911.873.885.323.451.141.641.781.094.100.590.5318.1015.1014.4414.024.659.811.882.762.421.781.092.180.680.3716.1113.8311.9314.164.178.68Table 10: upper bound KL-divergence hand-crafted trainedword models fold averaged across folds. (left) KL-divergence trainedword models hand-crafted word models. (right) KL-divergence random wordmodels hand-crafted word models.moving upward hand-crafted word model contains uniform distributionvelocity orientation first argument. Similarly, second third statestrained word model carried appear, first glance, quite different handwritten one. However, closer inspection reveals encode similar information.second state hand-written word model actually corresponds last two statestrained word model, collectively encode mixture distribution. mixture distribution encodes fact carried bidirectional involve leftwardrightward motion. hand-written word model encodes single statebimodal output distribution trained word model encodes two statesunimodal output distributions. lack additional state forces trainedword model merge output distributions velocity features last statehand-crafted word model two states code mixture distribution.expect differences eliminated larger training set accurate featureextraction.augmented qualitative analysis similarity hand-craftedtrained word models quantitative analysis. computed KL-divergenceoutput distributions corresponding word models. true KL-divergencetwo word models, ignores initial-state distributions state-transitionfunctions, provides loose lower bound actual KL-divergence. Table 10 reportsword lexicon. Across board, trained word models muchcloser hand-trained ones random word models.653fiYu, Siddharth, Barbu, & Siskind6. Related Worklanguage-inference task discussed Section 5.3 requires mechanism focus attentionparticular activity video depicts multiple simultaneous activities. Obtainingcapability extension state-of-the-art methods identify activityvideo trivial. large portion work, recently done Kuehne et al.(2011) Sadanand Corso (2012), identify either single activity given videorank ordering possible activities. videos depicted multiple simultaneous identicalactivities, methods would identify single instance activity.partly due fact matching features, say STIP (Laptev, 2005), providesscore, means localization. method, hand, so. existtwo instances activity, say pick up, occurring simultaneously, specifyone focus attention means elements video, characteristicsparticipants (adjectives), manner action (adverbs), relationsparticipants unrelated objects scene (prepositions). discussed previouslySection 5.4, much prior work generating sentences describe images (Jie,Caputo, & Ferrari, 2009; Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, &Forsyth, 2010; Kulkarni et al., 2011; Li & Ma, 2011; Yang, Teo, Daume III, & Aloimonos,2011; Gupta et al., 2012; Mitchell, Dodge, Goyal, Yamaguchi, Stratos, Han, Mensch, Berg,Berg, & III, 2012) video (Kojima, Tamura, & Fukunaga, 2002; Fernandez Tena, Baiget,Roca, & Gonzalez, 2007; Barbu et al., 2012a; Hanckmann et al., 2012; Khan & Gotoh, 2012;Krishnamoorthy et al., 2013; Wang, Guan, Qiu, Zhuo, & Feng, 2013) uses special-purposenatural-language-generation methods. method, contrast, systematically searcheshighest-scoring sentence generated grammar using video-sentence scoringfunction used language inference language acquisition. generativitylabeling domain allows us label unseen video sentence, potentiallyunbounded set, including never appeared, whole part, formtraining.active research grounded language learning computationallinguistics community. research employs approaches directly map wordsperceptual features extracted external world. Roy (2002) paired training sentences vectors real-valued features extracted synthesized images depict2D blocks-world scenes, learn specific set features adjectives, nouns, adjuncts. Roy Pentland (2002) presented computational model acquires wordmeanings directly multimodal sensory input. Yu Ballard (2004) paired trainingimages containing multiple objects spoken name candidates objects findcorrespondence lexical items visual features. Marocco, Cangelosi, Fischer,Belpaeme (2010) grounded meanings action words link robotsaction effects behavior observed manipulated objectsaction. approaches directly learn word meanings associated features,robustly understand limited set sentential fragments lack capability deal complex syntactic structures, since resulting word meaningsneither generative compositional.work within computational linguistics community focused learningsymbolic representations word meanings corpora sentences paired sym654fiGrounding Language Inference, Generation, Acquisition Videobolic representations sentential meaning, illustrated Figure 18(a). ThompsonMooney (2003) described system called Wolfie acquires semantic lexicon phrasemeaning pairs corpus sentences paired semantic representations. Zettlemoyer Collins (2005) presented method learning sentence meanings formlambda-calculus encodings. Dominey Boucher (2005) paired narrated sentencessymbolic representations meanings, automatically extracted video, learnobject names, spatial-relation terms, event names mappings grammaticalstructure sentential fragments semantic structure associated meaning representation. Piantadosi, Goodman, Ellis, Tenenbaum (2008) employed unsupervised,cross-situational Bayesian learning model acquisition compositional semantics,solve problem referential uncertainty. Chen Mooney (2008) Kim Mooney(2010) learned language sportscasting determining alignment gamecommentaries meaning representations output rule-based simulationgame. later reduced task learning Probabilistic Context-Free Grammar (PCFG) Borschinger, Jones, Johnson (2011). subsequent work (Chen& Mooney, 2011; Kim & Mooney, 2012, 2013) proposed techniques learning follownavigation instructions observation given weak, ambiguous supervision. Kwiatkowski,Zettlemoyer, Goldwater, Steedman (2010) Kwiatkowski et al. (2012) presentedapproach learns Montague-grammar representations word meanings togethercombinatory categorial grammar (CCG) child-directed sentences paired firstorder formulas represent meaning. Although methods succeed learningword meanings sentential descriptions, symbolic representationsmight extracted simple synthesized visual input; fail bridge gaplanguage computer vision, i.e., extract meaning representationscomplex visual scenes.recent work computational linguistics robotics communities attempted learn grounded word meanings richer perceptual input paired multiword phrases. Krishnamurthy Kollar (2013) introduced Logical SemanticsPerception (LSP) framework grounded language acquisition learns map naturallanguage statements referents physical environment. However,nouns spatial-relation prepositions small set static images. Tellex,Thaker, Joseph, Roy (2013) learned mapping specific phrases aspectsexternal world robotic system, assumed ideal scene: perfect object classification, 3D coordinate system, unambiguous demonstration robotcorrectly executing action environment.also research training object event models large corporacomplex images video computer vision community (Feng, Manmatha, &Lavrenko, 2004; Yao, Yang, Lin, Lee, & Zhu, 2010; Kulkarni et al., 2011; Ordonez, Kulkarni, & Berg, 2011; Kuznetsova, Ordonez, Berg, Berg, & Choi, 2012; Sadanand & Corso,2012; Chen & Grauman, 2013; Everts, van Gemert, & Gevers, 2013; Song, Morency, &Davis, 2013; Tian, Sukthankar, & Shah, 2013a), illustrated Figure 18(b).viewed learning meanings nouns verbs. However, work requirestraining data labels individual concepts individual words (i.e., objects delineatedvia bounding boxes images nouns events occur short video clips verbs).words, specify correspondence concepts data655fiYu, Siddharth, Barbu, & Siskindtraining sentencetraining meaninglearned representationsperson picked chair.cause(person, go(chair, up))personpickedchairpersoncause(x,go(y,up))chair(a)nounstraining wordstrainingimages/videospersonverbschairpicked......upwardrightwarddownwardleftwardvelocity orientationsecond argumentmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudesecond argumentlearnedrepresentations(b)Figure 18: illustration dominant paradigms prior work. (a) workcomputational linguistics community learns symbolic representations word meaningssentences paired symbolic representations sentential meanings. (b) workcomputer vision community learns word independently, training dataannotates image video portion corresponds object event label, distinctrepresentations part speech.words trained. attempt model phrasal sentential meaning,let alone acquire object event models training data labeled phrasalsentential annotation. result, learned word meanings neither generativecompositional. Descriptions new images video produced mosaicing togetherpreviously learned sentence fragments. Moreover, unlike methods presented here,approaches use distinct representations different parts speech; i.e., object eventrecognizers use different representations.method differs prior work three ways. First, input consists realisticvideo filmed outdoor environment. Second, learn entire lexicon, includingnouns, verbs, adverbs, prepositions, simultaneously video described whole656fiGrounding Language Inference, Generation, Acquisition Videosentences. Third, adopt uniform representation meanings wordsparts speech, namely hidden Markov models (HMMs) whose states distributionsallow multiple possible interpretations word sentence ambiguous perceptualcontext.work presented similar three recent papers (Das, Xu, Doell, &Corso, 2013; Rohrbach, Qin, Titov, Thater, Pinkal, & Schiele, 2013; Guadarrama, Krishnamoorthy, Malkarnenkar, Venugopalan, Mooney, Darrell, & Saenko, 2013) generatetext descriptions video. surface, papers appear describe approacheshandle unrestricted text video. However, deeper analysis revealscase. Indeed, analysis demonstrates space text supported systemsfar restrictive present here. discuss prior work depthalong analysis.Das et al. (2013) generate text descriptions cooking videos garnered YouTube.using shallow vision features unseen video index trainingcorpus videos paired text annotations find similar videos stitching togetherfragments text associated indexed videos obtain new text annotationunseen video.1. model word sentence meanings. doesnt knowwords sentences annotations refer video. One cant pointcomponent system say definition particular word.precisely Table 6 Figures 5, 6, 2330 (in Appendix B).Moreover, one cant analyze portions meanings correctwrong, page 651. system generates incorrect annotation,nothing much one say so.2. (1) cant call inference. cant process videosimultaneous actions taking place different subsets actors objectsvideo two different sentences highlight different sets participantsdifferent sentences. precisely demonstrate Figure 15 Figure 21Appendix B. demonstrates deep understanding. factminimal pairs, pairs sentences differ single word, vary wordlexical entries sentential positions demonstrates semantic modelreflects deep understanding every word.3. work Das et al. (2013) lacks such. causes method generatehuge number erroneous descriptions. Das et al. (2013, Figure 6) show five samplesentences generated five sample videos. sample videos,three four generated sentences false video. completelyincorrect objects actions. examples picked showcase system.Presumably, performs worse examples. contrast, conduct presentresults thorough evaluation: Figure 21 Appendix B presents resultsexamples, without exception.4. nouns adjectives generated sentences work Das et al. (2013,Figure 6) describe objects far beyond ability state-of-the-art objectdetection systems detect (e.g., knob, pliers pieces metal, glass bowl, porcelainbowl, sponge, old food, dish towel, hand held brush, vacuum, panel, health care reform,. . .), particularly size field view. Ditto verbs denoting657fiYu, Siddharth, Barbu, & Siskindactions (e.g., clean, speak, sit, stand, open, renovate install, bend, cook, mix, . . .).system really grounding meanings words video. Ratherindexing based surface features. mean saysystem uses linguistics. system may use techniques prevalentnatural-language-processing community, one might even call computationallinguistics, one would call linguistics. denigrate system.simply incomparable work.5. Das et al. (2013) report measured alignment words textportions video. Thus one unable determine whether sentencegeneration really based video features convey meanings wordssentences generated whether based accidental correlationfeatures background reflective true meanings.Rohrbach et al. (2013) generate text annotations videos two-step process.first translate video x intermediate representation (SR) translateSR sentence z. SR five discrete random variables (activity, tool, object,source, target). 66 possible activities, 43 possible tools, 109 possible objects,51 possible sources, 35 possible targets. mapping video SR mediatedjoint probability model implemented conditional random field (CRF) mutuallyconstrains five random variables. CRF trained supervised fashion.training data contains videos paired human annotated SRs. mapping processvideo SR yields quantized SR returning SR training setlowest Hamming (1950) distance SR estimated CRF.text-generation process involves second step maps SR sentence.However, process use information video alreadyabstracted SR. purposes comparing work, process relevant.discrete quantized SR component work Rohrbach et al. (2013)analogous individual words generate. case, mappingwords sentences done deterministic grammar neither introduces errorscontains joint-distribution information filter errors. mappingSR sentences introduce errors also constitutes additional level jointconstrained-distribution information filter errors. Thus compare systemfirst step work.Due Hamming-distance post processing, Rohrbach et al. (2013) outputone 5,609 possible SRs total 66431095135=552,175,470 onesnominally possible. Thus generate 5,609 possible sentences. contrast,system generate 235,575 possible sentences three objectsfirst corpus, 406,296 possible sentences three objects secondcorpus, 6,614,325 possible sentences four objects first corpus,13,633,272 possible sentences four objects second corpus.Thus surface, appears system Rohrbach et al. (2013) handlesunrestricted text, reality handles space sentences four five ordersmagnitude smaller do. Thus solving immensely easier problemmuch smaller space possible outputs. Yet, Rohrbach et al. (2013, Table 1) indicateobtain correct SR 21.6% time. obtain true sentence658fiGrounding Language Inference, Generation, Acquisition Video64% time. Moreover, system learns solely videos paired sentences.system requires additional human annotation SRs associated video.Points 15 comparison work Das et al. (2013) also applywork Rohrbach et al. (2013). particular, vast majority object classes wellbeyond state-of-the-art ability support recognition CRF (e.g.,avocado, egg, cucumber, bag chilies, cutting board, loaf bread, lime, knife, plate, butter,carrot, (half ) kiwi, package beans, orange, saucer, . . .), particularly sizefield view. Similarly, vast majority verbs well beyond stateart support action recognition, CRF (e.g., slice, crack, takeout, rinses, put away, select, split, . . .). Rohrbach et al. (2013) derive successhighly constrained set possible SRs distribution encoded CRF.means cannot describe videos exhibit person taking kiwi fridgenever occurred training corpus, even though might perfectly reasonablevideo. Surely vastly 5,609 552,175,470 possible SRs plausibleperhaps even likely. Yet even constraint, Rohrbach et al. (2013, Table 2) reporthuman judges evaluated truth generated sentences, average report3.1 scale 1 5, 3 7080% good. Moreover, limitedparticular representation employed SRs. encode sentence meaningsformulated terms particular five random variables (activity, tool, object,source, target). contrast, approach formulate sentence meanings termsarbitrary conjunctions predicates applied subset event participants longpredicates formulated HMMs arbitrary output distributionsfeatures extracted video.Guadarrama et al. (2013) describe method outputs three-word sentences summarize video activity. Like Rohrbach et al. (2013), encoded three variables:actor (subject), action (verb), object (object). 45 possible subjects,218 possible verbs, 241 possible objects. Given training corpus comprising videoclips paired annotated SVO triples, method first builds three semantic hierarchies,represented trees, one subject, verb, object, indicate similarityrelationships among meanings words occur training corpus. wordappears training corpus constitutes leaf node one hierarchy trees.internal nodes represent sets dominated leaf nodes, generalized concept lessspecificity leaf nodes.visual classifier associated leaf nodes individual subject, verb,object. leaf classifier uses1. Dense Trajectories (Wang, Klaser, Schmid, & Liu, 2011, 2013; Wang & Schmid, 2013),encoded using pre-trained codebook,2. vector object-detector scores, entry denoting maximal scoreobject class,3. multi-channel approach combines two features classifiesnon-linear SVM.classifiers trained, probability estimates nodes hierarchy treesobtained unseen video clip. nodes three hierarchies representingwords generated unseen video clip predicted optimizing cost functiontrades specificity accuracy. internal node predicted, represen659fiYu, Siddharth, Barbu, & Siskindtative leaf word selected set leaves dominated node leafhighest cumulative WUP WordNet (Miller, 1995; Fellbaum, 1998). Guadarramaet al. (2013) also introduce zero-shot approach generate verbs appeartraining corpus thus absent verb hierarchy. that, verbdetermined text-mined likelihoods fit detected subject object.paint virtue, view deficit. Essentially, guessing verb.celebrated cases objects predict verbs vice versa (e.g., hammer ),believe accounts far less actual video. one sees dog cat,still plethora possible verbs: approach, leave, run away from, fight with, ignore,chase, flee from,bite, lick, . . . easy pick examples showcase workssays little well approach works general.approach taken Guadarrama et al. (2013) similar takenRohrbach et al. (2013), construct joint probability models collectionrandom variables, five case latter three case former. Rohrbachet al. add quantization Hamming distance absent work Guadarramaet al., Guadarrama et al. add zero-shot approach along hierarchiesbalance accuracy specificity absent work Rohrbach et al.. Without zero-shot extension, Guadarrama et al. output one 45218241=2,364,210possible sentences. number roughly equivalent number sentencesmethod produce.work Guadarrama et al. exhibits shortcomings workRohrbach et al. Das et al. (2013). Points 15 comparison workDas et al. also apply work Guadarrama et al.. case workRohrbach et al., vast majority object classes well beyond state-of-theart ability support recognition (e.g., chef, cook, microphone, flute, flour, music, pasta,spaghetti, . . .), particularly size field view. Similarly, vastmajority verbs well beyond state art support action recognition(e.g., slice, cut, chop, prepare, make, . . .). almost certain visual-feature spacewould separate verbs chop cut, nouns pasta spaghetti.words also similar semantic meanings even quite difficulthumans distinguish short video clips. Thus number words appeargenerated sentences increased considering similar lexical items, difficultgeneration task increase much expected evaluation lax (e.g.,considering chop correct even though slice actually happens video).hand, specificity-accuracy tradeoff, generated sentencessometimes uninformative, e.g., animal plays something animal something instrument (Guadarrama et al., 2013, Table 4). Also zero-shot approachseems override actual activity recognition quite easily, seen fourthrow (Guadarrama et al., 2013, Table 4): car rides vehicle. Finally, Guadarrama et al.evaluate truth sentences generated. Instead, calculate WUPsimilarity generated annotated subjects, verbs, objects, independently.estimates, seven eleven generated sentences false corresponding video clip. examples picked showcase system. Presumably,performs worse examples. unclear actual truth accuracygenerated sentences entire corpus.660fiGrounding Language Inference, Generation, Acquisition Videoalso something deeply unsettling general approach takenRohrbach et al. Guadarrama et al. using joint probability model derived textmining influence activity recognition. Suppose corpus text much higherfrequency occurrence dog chases cat dog is-bigger-than cat dog eats-with cat.says nothing actual prior truth probability underlying propositions,let alone actual posterior truth probability conditioned particular video.sense, Rohrbach et al. Guadarrama et al. actually grounding languagevideo rather generating natural-language utterances using information obtainedungrounded language.7. Discussioncomputational linguistics community become accustomed employing large lexicons grammars trained large text corpora process unrestricted text. Similarly,computer vision community become accustomed employing methodstrained large image video corpora process unrestricted images video. Onemay wonder would take extend methods exploredapply large-scale unrestricted text video corpora. One might assumesimply matter employing better state-of-the-art methods computationallinguistics computer vision. This, however, case. usestate-of-the-art methods computational linguistics, computer vision methodsstate art. use deformable part model (DPM) object detector (Felzenszwalbet al., 2010a, 2010b) action detector exhibits state-of-the-art performance.approach limited computer vision, computational linguistics. stateart object detection reflected ongoing Pascal Visual Object Category (VOC)Challenge (Everingham et al., 2010). currently 20 classes current state-of-the-artperformance 40-50% best classes, far worse classes. stateart action recognition reflected standard corpora used community, e.g., Weizmann (9 classes; Blank, Gorelick, Shechtman, Irani, & Basri, 2005), KTH(6 classes; Schuldt, Laptev, & Caputo, 2004a), UCF Sports (10 classes; Rodriguez, Ahmed,& Shah, 2008), UCF YouTube (11 classes; Liu, Luo, & Shah, 2009), Olympic Sports (16classes; Niebles, Chen, & Fei-Fei, 2010). best reported performance corpora(Weizmann 100%: Tian, Sukthankar, & Shah, 2013b; KTH 95.49%: Yuan, Li, Hu, Ling, &Maybank, 2013; UCF Sports 95%: Sadanand & Corso, 2012; UCF YouTube 89.4%: Zhu,Wang, Yang, Zhang, & Tu, 2013; Olympic Sports 85%: Gaidon, Harchaoui, & Schmid,2014) might lead one mistaken conclusion action classification solved smallnumbers classes. However, Barbu, Barrett, Chen, Siddharth, Xiong, Corso, Fellbaum,Hanson, Hanson, Helie, Malaia, Pearlmutter, Siskind, Talavage, Wilbur (2014) illustrate false, 6-class corpus state-of-the-art methods get52.34%. Moreover, largest corpora actively used action recognition contain 50classes (UCF50, Reddy & Shah, 2013 HMDB51, Kuehne et al., 2011). best reportedperformance UCF50 91.2% HMDB51 57.2%, (Wang & Schmid, 2013). Thusapproach, ours, grounds meaning individual word stateof-the-art computer vision object detectors, trackers, action recognizers inherentlylimited small number concepts. space natural-language utterances661fiYu, Siddharth, Barbu, & Siskindone erect around thus limited effectively captured small fixedunambiguous context-free grammar. Thus employing state-of-the-art methods computational linguistics would improve generality approach given limitedstate art computer vision.paper we, thus, employ state-of-the-art methods computationallinguistics. employ small fixed lexicon grammar. make claimlexicon grammar general. particular lexicon grammar focuswork. serve illustrate framework capability frameworksupporting concerns outlined end Section 1. One change lexicongrammar still use framework. Indeed, done report this.Table 11(a) reports two slightly different grammars. experiments employ two different video corpora two different sets sentential annotations use differentgrammars reported Tables 13. video corpora use different sets objectsassociated object-detector models. Barbu, Siddharth, Siskind (2014) employ yet another corpus, different set objects object-detector models, different lexicon,different grammar, different set word-meaning representations. demonstratesframework adapted variety such. beyond this, Barbu et al.(2014) demonstrate yet another whole different application framework, namelyvideo retrieval. corpus ten full-length Hollywood movies.corpus far toy. framework support large-scale real-world videowild. Yet concept vocabulary still small natural-language fragment stillrestricted. one could employ state-of-the-art methods computational linguistics,supported concept set thus supported language fragment would still small.Thus one would using state-of-the-art methods potentialdesigned for.two general approaches towards action recognition computer vision. Oneemploys methods detect track people objects participate action,classifying action properties derived detected objects tracks.extracts classifies features video without detecting tracking people objects. latter methods generally employ bag spatio-temporal visual-words approach(BOW). generally extract feature vectors, spatio-temporal interest points(STIP; Schuldt, Laptev, & Caputo, 2004b), subset space-time points, build codebook pooling such, vector quantize feature vectors codebook, computehistogram codebook-entry occurrences pooled frames video, classifyhistograms temporally invariant models. Early approaches action recognition generally employed former method (e.g., Siskind & Morris, 1996; Mann, Jepson, & Siskind,1996, 1997; Siskind, 1999, 2000, 2001; Fern, Givan, & Siskind, 2002a; Fern, Siskind, & Givan, 2002c; Fern, Givan, & Siskind, 2002b; Siskind, 2003). approach eschewedrecent work, favor latter method, difficulty detectingtracking people objects reliably (e.g., Schuldt et al., 2004b; Liu et al., 2009; IkizlerCinbis & Sclaroff, 2010; Kuehne et al., 2011; Reddy & Shah, 2013). However, BOWapproach suffers severe limitation: localize event participants.may able generate verbs describe classified actions, cannot generate nounsdescribe object class event participants, adjectives describe propertiesevent participants, spatial-relation prepositions describe relative position event662fiGrounding Language Inference, Generation, Acquisition Videoparticipants, adverbs describe event properties, motion prepositions describepath taken event participants. distinguishing, novel, unique aspectapproach. Moreover, systems, one proposed Guadarrama et al.(2013), employ object detector addition STIP-based event detector,link objects arguments event predicates. system using similar approachlike would1. fail distinguish dog approached person cat approached persondog cat present field view2. fail distinguish dog approached person person approached dog.approach correctly makes distinctions. One design principles behindcorpus multiple people appear most, all, videos, most, all, objectsappear every video. Beyond this, videos depict simultaneous different actionsdifferent subsets participants. renders minimal-pairs experiment(Section 5.3) acquisition experiment (Section 5.5) far trivial.make claim particular features employ Tables 6 8sufficient represent semantics possible words utterances. servesupport experimental evaluation conducted Section 5. One could employsentence-tracker approach discussed different set features. Indeed,done (Barbu et al., 2014). Moreover, make claim one employ HMMsform core sentence tracker represent semantics possible wordsutterances. limitation HMM-based approach requiresobject detectors trackers; BOW approaches suffer well. BOW approachcannot represent verb approach. neither BOW HMM approach representverbs liberate, contemplate, discuss, help, finish, . . . Representing semanticsentire space verbs, let alone natural language, even non-grounded fashion,even so, grounded video, central unsolved problem computationallinguistics, AI, cognitive science.surface, may appear BOW approaches robust recognizingcertain action classes like play instrument approaches involve detectingtracking objects. However, none standard datasets (Weizmann, KTH, UCF Sports,UCF YouTube, Olympic Sports, UCF50, HMDB51) class playing instrument(in general). one, UCF50, classes playing small number specific instruments: drumming, playing guitar, playing piano, playing violin. unawarepublished action-recognition systems perform well dataset. One bestperforming methods dataset Action Bank (Sadanand & Corso, 2012),use BOW. performance method enlightening current stateart. gets roughly 80% accuracy classes. Moreover confusion matrix enlightening: drumming confused biking yoyo, playing piano confusedbasketball, drumming, golf swing, tennis swing, soccer, juggling, playing violinconfused drumming, rope-climbing, taichi, tennis, yoyo, rock climbing.confusions indicate lacks deep understanding characteristic actionsquestion appears triggering spurious correlations particulardataset. particular, dataset contain people sitting next drum setpiano, holding guitar violin without playing it. way know whetheractually recognizing playing activity simply recognizing gross image statistics663fiYu, Siddharth, Barbu, & Siskindindicate instruments present field view. one getsmotions air guitar, banging piano keys elbow, simply wavingviolin air constitute activity instrument question dont constitute playing said instrument. Beyond this, see little ability generalizeplaying specific instrument playing instrument general.BOW-based systems often encode true semantics actions question.often trigger spurious correlations dataset. acknowledgedauthors systems themselves.instance, v spiking normally happens crowd people, divinghappens pool. common professional sport actions takeplace highly structured environments (Liu et al., 2009, p. 2002)Basketball shooting volleyball actions also confused cases:largely time, basketball volleyball sports usesimilar courts (Ikizler-Cinbis & Sclarof, 2010, p. 505)One may desire, even expect, form characterization space possiblewords videos approach support. Unfortunately, know way provide such. know way, general, formally characterizing space words,images, video supported action-recognition system, matter object-recognition system or, generally, computer vision, computationallinguistics, AI system.current corpus lacks camera motion. restriction approach.restriction appear mathematical algorithmic formulationsSection 2 4, even implementation. sentence tracker extensionprior work detection-based tracking (Barbu et al., 2012b) employed performaction recognition sentence generation videos involve camera motion (Barbuet al., 2012a). Barbu et al. (2014) apply sentence tracker perform video retrievalcorpus ten full-length Hollywood movies, vast majority involve cameramotion.framework expressly restricted using verbs represent events.current linking process particular grammar used support process restrictedsuch. nothing turns that. discussed above, sentence tracker uselinking process construct factorial utterance-level HMM constituent wordlevel HMMs. expedience, limit set features entertained learningpart-of-speech basis. restriction could lifted change algorithmimplementation. introduced allow convergence smaller training set.know reason method Section 4 would work without restriction.would require larger corpus would unwieldy perform experiments with.method represents word meanings parts speech simply predicatesone tracks sentential meanings conjunctions such. Presumably, different linking process could construct logical form man(x) pause(x)sentences like man made pause well could man paused.beauty approach, employing unified representation meanings664fiGrounding Language Inference, Generation, Acquisition Videowords parts speech, common cost function, common algorithm, commonimplementation.State-of-the-art object-recognition systems highly unreliable. image datasets,trained object model, say person chair, may succeed one image fail another, even chair person pose wearingclothing background. video, even happens adjacentframes video. State-of-the-art object detection suffers immense false positives negatives. Moreover, reliable object detection imply reliableaction recognition, state-of-the-art action recognizers similarly highly unreliable. Stateof-the-art recognizers bend wave trained one dataset yield chance performancedifferent datasets. Even dataset, action recognizers mysteriouslysucceed fail similar samples, background, actors,manner performance action, etc. central novel contribution worksentence tracker Equation 10, method overcoming severe limitationsobject detectors action detectors formulating joint model object detection,tracking (temporal coherence), sentential semantics.video corpora may appear simpler typically used currentaction-recognition work computer vision community (e.g., Weizmann, KTH, UCFSports, UCF YouTube, Olympic Sports, UCF50, HMDB51) apparent simplicitymisleading. Several aspects video corpora far complex usedvast majority related work.1. videos contain many, all, objects repertoire. makeslanguage acquisition difficult. One needs determine objects referredtraining sentences ignore extraneous ones field view.done automatically without human annotation.2. videos contain least two simultaneous actions, often performed differentpeople different objects. One needs determine action referredtraining sentence associated video, pay attention particularsubset people objects participate action, ignore extraneousactivity occurs field view. done automatically withouthuman annotation.3. system process complex natural-language sentences contain many participants, e.g., something complex person left chair carriedbackpack right traffic cone towards stool left person. even support multiple instances noun sentence referdistinct instances object class video (as person above).determine semantic-role assignment, nouns argumentswords correspond regions video frames. assignment determinedautomatically without human annotation change small subtlechanges sentence. Moreover, learn solely complex sentential annotation, without human annotation words correspondregions video frames.novel central technical contribution formulation sentence trackerEquation 10 observation optimized using standard well-knowntechniques adapted HMMs, namely Viterbi algorithm (1967) Baum-Welch665fiYu, Siddharth, Barbu, & Siskind(1970, 1972). key understanding Equation 10 jointly optimizes cost function incorporates multiple detection-based trackers, one event participant,multiple factorial event models, one lexical item sentence, judiciously linkingdetection-based trackers factors sentential model way consistentpredicate-argument structure sentence, model truth-conditional semanticssentence derived constituent words. Formulating truth-conditionalsentential semantics way allows exiting algorithms like Viterbi Baum-Welchground semantics natural language video perform novel applicationslanguage inference (Section 5.3), language generation (Section 5.4, language acquisition(Section 5.5), particularly minimal-pair experiment Figure 15 acquisitionvideos labeled whole sentences human annotation.significant prior work multi-object tracking (e.g., Berclaz, Fleuret,Turetken, & Fua, 2011; Pirsiavash, Ramanan, & Fowlkes, 2011). novel aspect eventtracker particular formulation detection-based tracking cost functionoptimized Viterbi algorithm allows forming joint model HMM-basedevent detector also optimized Viterbi algorithm cross-productlattice. might possible trackers event models. Beyondthis, sentence tracker forms joint model multiple trackers factorial HMM,linking particular factors particular trackers, way again, also optimizedViterbi algorithm cross-product lattice. also might possiblemulti-object trackers event models.video corpora filmed giving actors instructions actions perform. such, staged. computational linguistics community attempteduse unsolicited samples natural language fear solicited samples might introduce bias. One might wonder whether desirable, even possible, videocorpora well. However, appears infeasible gather unsolicited video corpora exceptsurveillance situations. Surveillance video tends highly uniform sparse:event classes occur occur infrequently. renders ill suited actionrecognition. Almost situations video recorded, even recordedexplicitly computer vision use, solicited. amateur video form uploadedYouTube similarly staged level usually records activity elicited specificallyfilming. Indeed, prominent video corpora used computer vision communityevaluate action recognition filmed specifically purpose constructingcorpus: Weizmann, KTH, Activities Daily Living corpus (Messing, Pal, & Kautz,2009), DARPA Minds Eye corpus (both year 1 year 2), TaCOS corpusused Rohrbach et al. (2013), name few. YouCook corpus usedDas et al. (2013) culled YouTube, videos appear staged,above.related work generating sentences describe video evaluates generatedsentences comparison human-elicited sentences video. oftendone computing BLEU scores (Rohrbach et al., 2013) measuring fractionwords common machine-generated human-elicited descriptions (Khan,Zhang, & Gotoh, 2011). might evaluate degree machine-generatedsentences natural sounding, fails evaluate truth machine-generatedsentences, central objective work. Indeed, machine-generated sentences666fiGrounding Language Inference, Generation, Acquisition Videohigh BLEU scores high commonality human-elicited descriptions often falsevideo even human-elicited descriptions true.current linking process would fail ambiguous sentence parse. linkingprocess might also fail yield unambiguous role assignment unique linking function. Further, current lexicon contains lexical ambiguity current linkingprocess would support such. myriad approaches parsing constructinglogical form presence ambiguity could brought bear problem.beyond this, current approach offers novel possibility existing approachsupport. One imagine using video disambiguate parsing constructionlogical form. One could imagine evaluating truth various word senses, sentence fragments, attachment alternatives, alternate logical forms video using sentencetracker.sentence tracker general-purpose inference mechanism combining information multiple frames video using language vision.presented particular instantiation sentence tracker, particular detectors, particular temporal-coherence scores, particular event models operating 2D, generalapproach could instantiated numerous ways. employed object detectors detection sources, method selects image regions could usedapproach presented. need rectangular: one imagine variants sentence tracker employ general-purpose foreground-background segmentation insteadobject detection. also need two-dimensional: one imagine variantssentence tracker employ projection models reconstruct temporally-coherent tracks3D 2D images also satisfy 3D event models. could even pool detectionsvariety sources scale scores prefer reliable ones possible.Moreover, temporal-coherence score uses optical flow, could employ appearance model order alleviate situations tracks converge imagelocation swapped two tracked objects divergelocation. one employ human-pose detector, one could incorporate coherencehuman-pose variation temporal-coherence model. One could similarly incorporate changing human pose event model. event tracker wouldallow event model influence improve recovered human pose estimatedtop-down fashion, much way event model influence improve recovered tracks. Finally, event models formulated HMMs,general frameworks possible. Even nongenerative frameworks, like maximum-entropyMarkov models, could accommodated long inference could performed usinglattice dynamic programming. One even imagine forgoing lattice dynamicprogramming integrate complex models object detection, temporal coherence,events using message-passing inference.sentence tracker also learn word meanings video paired sentences.Unlike prior work, method deals video labeled whole sentences, insteadindividual words. Moreover, method successfully learns without prior delineationcorrespondence words sentence labels visual features associatedvideo used object and/or event recognition. experiments show correctlylearn meaning representations terms HMM parameters lexical entries,highly ambiguous training data, training video clip depicts one667fiYu, Siddharth, Barbu, & Siskindsentence sentence describes one clip. performinginter- intra-sentential inference: determining meaning word cross-situationallycollection training samples appears well spreadingsentential meaning across words sentence way consistent acrosstraining set.method amenable extension. First, due nature Markovmodels, state depends immediate predecessor. discussed Section 2.2,property implies output model employ features computed singleframes two adjacent frames. features may prove inadequate larger lexicons.example, models often exhibit difficulty differentiating picked putdown, since difference encoded second-argument velocity orientationsecond state. current implementation computes orientation using optical flownoisy. One could reliably differentiate two event classesone could encode model overall displacement second argument, alongdirection displacement, event proceeds: picked involves significantupward displacement put involves significant downward displacementpossible encode multiple-frame feature HMM, possiblecomplex graphical models conditional random fields (CRFs). Oneimagine employing CRFs event model, together object detection temporalcoherence, variant sentence tracker.Another possible extension employing state-duration models HMMs. Without explicit state-duration models, implicit state-duration model exponential:probability staying state k frames a(k, k)t . exponential stateduration model encode minimum duration event, filter short-term noisesignal, discussed Section 2.2, cannot bias event detector towards typicalduration performing event. experiments, manifest difficultydistinguishing picked put similar initial finalstates differ short transition period. Employing explicit state-duration models,hidden semi-Markov models (HSMMs; Yu, 2010) event models withinsentence-tracker framework could potentially improve alleviate difficulty.third possible extension employ discriminative training instead maximumlikelihood training. Maximum-likelihood training makes use positive sentential labelstraining data. Discriminative training also make use negative sentential labels.could reduce amount training data required also could yield better resultstrains models competitively. would require method obtaining negativesentence labels. One could manual annotation, positive sentencelabels. However, discriminative training works well number negative labels farexceeds number positive ones. Thus rather manual annotation, one imagineform sentential inference automatically generate negative sentential labelscould possibly true video associated positive sentential label. mayallow learning larger lexicons complex video without excessive training data.668fiGrounding Language Inference, Generation, Acquisition Video8. Conclusionpresented novel framework utilizes compositional structure eventscompositional structure language drive semantically meaningful targetedapproach towards event recognition. multimodal framework integrates low-level visualcomponents, object detectors, high-level semantic information, formsentential descriptions natural language. integration facilitated sharedstructure detection-based tracking, encodes low-level visual features,event models, form HMMs, encode sentential semantics.demonstrated utility expressiveness framework performing threeseparate tasks video corpora, simply leveraging framework different manners. first, language inference, showcases ability focus attentiontracker event described sentence, demonstrating capability correctlyidentify subtle distinctions person picked chair lefttrash person picked chair right trash can. second,language generation, showcases ability produce complex sentential descriptionvideo clip, involving multiple parts speech, performing efficient searchbest description though space possible descriptions. third, languageacquisition, showcases ability learn lexicon corpus video clips annotatedsentential descriptions searching among possible lexicons find one allowssentences best collectively describe associated video clips.Acknowledgmentsresearch sponsored Army Research Laboratory accomplishedCooperative Agreement Number W911NF-10-2-0060. views conclusions containeddocument authors interpreted representingofficial policies, either express implied, Army Research Laboratory U.S.Government. U.S. Government authorized reproduce distribute reprintsGovernment purposes, notwithstanding copyright notation herein.Appendix A. Linking Processuse linking process mediated grammar portions lexicon .lexicon portion specifies arity permissible roles individual lexical entries.grammar used experiments Section 5 shown Table 11(a). portionlexicon specifies arity permissible roles used experiments shownTable 11(b). grammar lexicon portion, linking process describeddetermine sentence Equation 11 3 participants producelinking function Equation 12.linking process operates first constructing parse tree sentencegiven grammar. means recursive-descent parser. lexical-categoryheads parse tree map words used sentence tracker. Nominally, lexicalcategories, e.g., noun (N), adjective (A), verb (V), adverb (Adv), preposition (P),serve heads corresponding phrasal categories NP, AP, VP, AdvP, PP.669fiYu, Siddharth, Barbu, & Siskind(a)NP VPNP [A] N [PP]|blue | redN person | backpack | chair |trash | traffic cone | stool | objectPP P NPP left | rightVP V NP [Adv] [PPM ]V approached | carried | picked | putAdv quickly | slowlyPPM PM NPPM towards | away(b)left : {agent, patient, source, goal, referent}, {referent}right : {agent, patient, source, goal, referent}, {referent}approached : {agent}, {goal}carried : {agent}, {patient}picked up: {agent}, {patient}put down: {agent}, {patient}towards: {agent, patient}, {goal}away from: {agent, patient}, {source}other: {agent, patient, source, goal, referent}Table 11: (a) grammar used experiments Section 5. Terminals nonterminals red used experiments Sections 5.3 5.4 first corpus.Terminals nonterminals green used experiments Section 5.5second corpus. Terminals nonterminals black used experimentscorpora. first corpus uses 19 lexical entries 7 parts speech (2 determiners, 2 adjectives, 5 nouns, 2 spatial-relation prepositions, 4 verbs, 2 adverbs, 2 motionprepositions). second corpus uses 17 lexical entries 6 parts speech (1 determiner,6 nouns, 2 spatial-relation prepositions, 4 verbs, 2 adverbs, 2 motion prepositions). Notegrammar allows infinite recursion noun phrase. (b) portionlexicon specifies arity permissible roles experiments Section 5.structure parse tree encodes linking function different wordsform government relations (Chomsky, 1982; Aoun & Sportiche, 1983; Haegeman, 1992;Chomsky, 2002). government relation defined formally Figure 19.example, determine Figure 20, N person governs P rightN chair, P right governs N chair.government relation, coupled lexicon portion, determines number Lparticipants linking function . construct word w head.lexicon portion specifies arity lexical entry, namely fact person, chair,670fiGrounding Language Inference, Generation, Acquisition Videolexical categories N, A, V, Adv, P heads.Parse-tree nodes labeled heads governors.parse-tree node dominates parse-tree node iff subtree .X-bar theory (Jackendoff, 1977), parse-tree node maximal projectionparse-tree node ifflabeled lexical category X,labeled corresponding phrasal category XP,dominates ,parse-tree node existslabeled XP,dominates ,dominates .parse-tree node m-commands parse-tree node iff dominatemaximal projection dominates .parse-tree node c-commands parse-tree node iff dominateimmediate parent dominates .parse-tree node governs parse-tree node iffgovernor,m-commands ,parse-tree node existsgovernor,m-commands ,c-commands ,c-command .Figure 19: government relation underlying linking process .backpack unary right picked binary. sole argumentword associated head noun filled distinct participant.3 soleargument word associated unary non-noun head filled soleargument word associated head noun governs . first argumentword associated binary non-noun head also filled sole argumentword associated head noun governs . second argument wordassociated binary non-noun head filled sole argument wordassociated head noun governed . example Figure 20, solearguments words associated nouns person, chair, backpack assigneddistinct participants 1, 2, 3 respectively. arguments word associatedpreposition right assigned participants 1 2, since N persongoverns P right turn governs N chair. Similarly, argumentsword associated verb picked assigned participants 1 3, sinceN person governs V picked turn governs N backpack.3. document concern anaphora, thus omit discussion support potentialcoreference. implementation, fact, support mediates analysisdeterminers.671fiYu, Siddharth, Barbu, & SiskindVPNPpersonpickedNPPrightNPVPPNNNbackpackchairFigure 20: parse tree example sentence person right chair pickedbackpack. highlighted portion indicates government relations Pright used determine arguments. N person governs Pright of, N chair, P right governs N chair.determine consistent assignment roles, one agent, patient, source,goal, referent, participants. allowed roles argument wordspecified lexicon portion. specification arity permissible roles usedexperiments Section 5 given Table 11(b). specification e : {r11 , . . .}, . . . , {r1Ie , . . .}means arity lexical entry e Ie r1i , . . . constitute permissible rolesargument i. participant constrained assigned role intersectionsets permissible roles argument word participant appears.constrain role assignment assign role one participant.example sentence Equation 11, role assignment computed follows:role(1) {agent, patient, source, goal, referent} {agent, patient} {agent}role(2) {agent, patient, source, goal, referent} {referent}role(3) {agent, patient, source, goal, referent} {patient}leading to:role(1) = agent role(2) = referent role(3) = patient672fiGrounding Language Inference, Generation, Acquisition VideoAppendix B. Complete Experimental Resultsbackpack approached trash can.chair approached trash can.person left trash put object.person right trash put object.Figure 21: Language inference: two different track collections video clip produced guidance two different sentences. minimal pairs sentences correspondsentences 19 Table 1 differences (a) (b) variants highlighted. track collections deemed human judges depict given sentencesindicated green, ones indicated red.673fiYu, Siddharth, Barbu, & Siskindperson put trash can.person put backpack.person carried red object.person carried blue object.person picked object left trash can.person picked object right trash can.Figure 21: Language-inference examples continued.674fiGrounding Language Inference, Generation, Acquisition Videoperson picked object.person put object.person picked object quickly.person picked object slowly.person carried object towards trash can.person carried object away trash can.Figure 21: Language-inference examples continued.675fiYu, Siddharth, Barbu, & Siskindbackpack approached trash can.chair approached trash can.red object approached chair.blue object approached chair.person left trash put object.person right trash put object.Figure 21: Language-inference examples continued.676fiGrounding Language Inference, Generation, Acquisition Videoperson put trash can.person put backpack.person picked object left trash can.person picked object right trash can.person picked object left trash can.person picked object right trash can.Figure 21: Language-inference examples continued.677fiYu, Siddharth, Barbu, & Siskindperson picked object.person put object.person picked object quickly.person picked object slowly.person carried object towards trash can.person carried object away trash can.Figure 21: Language-inference examples continued.678fiGrounding Language Inference, Generation, Acquisition Videobackpack approached trash can.chair approached trash can.red object approached chair.blue object approached chair.person put chair.person put backpack.Figure 21: Language-inference examples continued.679fiYu, Siddharth, Barbu, & Siskindperson carried red object.person carried blue object.person picked object left trash can.person picked object right trash can.person picked object.person put object.Figure 21: Language-inference examples continued.680fiGrounding Language Inference, Generation, Acquisition Videoperson picked object quickly.person picked object slowly.person carried object towards trash can.person carried object away trash can.Figure 21: Language-inference examples continued.681fiYu, Siddharth, Barbu, & Siskindbackpack left chair approached trash can.person right backpack carried chair.person right trash approached trash can.chair right person approached trash can.backpack left trash approached trash can.Figure 22: Sentential descriptions generated 94 video clips firstcorpus subject contraction threshold 0.90. highest-scoring sentence clipgenerated, among sentences generated grammar Table 11(a),means beam search. sentences deemed human judges describe associatedclips indicated green, ones indicated red.682fiGrounding Language Inference, Generation, Acquisition Videochair left trash approached trash can.backpack right trash approached trash can.backpack right trash approached trash can.person left trash put chair.backpack right person approached trash can.person right chair put backpack.Figure 22: Sentential-description examples continued.683fiYu, Siddharth, Barbu, & Siskindchair left trash approached backpack.trash right person approached chair.person right chair put trash can.person right chair put trash can.person right chair approached trash can.trash right chair approached chair.Figure 22: Sentential-description examples continued.684fiGrounding Language Inference, Generation, Acquisition Videobackpack right chair approached chair.person left trash picked chair.person right chair picked backpack.person right trash picked backpack.person left chair picked backpack.trash right person approached chair.Figure 22: Sentential-description examples continued.685fiYu, Siddharth, Barbu, & Siskindbackpack left trash approached trash can.person right chair put chair.trash right chair approached person.person right chair picked trash can.person left trash picked chair.backpack right chair approached trash can.Figure 22: Sentential-description examples continued.686fiGrounding Language Inference, Generation, Acquisition Videoperson right chair carried backpack.chair left trash approached trash can.person right chair approached chair.backpack right person approached trash can.person left trash approached trash can.backpack right trash approached trash can.Figure 22: Sentential-description examples continued.687fiYu, Siddharth, Barbu, & Siskindbackpack left chair approached chair.trash right backpack approached chair.trash right chair approached chair.person right trash put chair.person left chair put backpack.chair right trash approached trash can.Figure 22: Sentential-description examples continued.688fiGrounding Language Inference, Generation, Acquisition Videotrash left person approached backpack.trash left chair approached backpack.person left chair put backpack.person left chair put backpack.person left chair put backpack.chair right backpack approached trash can.Figure 22: Sentential-description examples continued.689fiYu, Siddharth, Barbu, & Siskindtrash left chair approached backpack.trash left backpack approached backpack.backpack left chair approached trash can.person right trash picked chair.person left trash picked trash can.person left trash picked backpack.Figure 22: Sentential-description examples continued.690fiGrounding Language Inference, Generation, Acquisition Videoperson right chair picked backpack.person right trash put backpack.person left chair approached chair.person right chair picked backpack.person right chair picked backpack.person right trash picked backpack.Figure 22: Sentential-description examples continued.691fiYu, Siddharth, Barbu, & Siskindperson left backpack picked chair.trash right chair approached chair.person right trash carried backpack.chair left trash approached trash can.person left backpack approached trash can.chair left backpack approached trash can.Figure 22: Sentential-description examples continued.692fiGrounding Language Inference, Generation, Acquisition Videotrash right person approached chair.backpack right trash approached trash can.backpack left chair approached chair.trash right backpack approached chair.backpack left chair approached chair.person right trash put chair.Figure 22: Sentential-description examples continued.693fiYu, Siddharth, Barbu, & Siskindperson left chair put backpack.person left trash put backpack.person right chair put backpack.person right chair put chair.person right trash put chair.backpack right trash approached chair.Figure 22: Sentential-description examples continued.694fiGrounding Language Inference, Generation, Acquisition Videoperson left backpack carried trash can.backpack right chair approached chair.person right chair approached trash can.person right chair picked backpack.person right trash picked backpack.person left backpack picked backpack.Figure 22: Sentential-description examples continued.695fiYu, Siddharth, Barbu, & Siskindtrash right chair approached person.person right chair put backpack.trash left person approached person.person right chair picked backpack.person left chair put trash can.person left trash picked trash can.Figure 22: Sentential-description examples continued.696fiGrounding Language Inference, Generation, Acquisition Videoperson left trash picked backpack.person left trash picked trash can.backpack right chair approached trash can.person right trash carried backpack.chair left trash approached trash can.Figure 22: Sentential-description examples continued.697fiYu, Siddharth, Barbu, & Siskindstooltraffic conetrashchairbackpackpersonstooltraffic conetrashchairbackpackpersondetector indexfirst argumentstooltraffic conetrashchairbackpackpersonstooltraffic conetrashchairbackpackpersondetector indexfirst argumentstooltraffic conetrashchairbackpackpersonstooltraffic conetrashchairbackpackpersondetector indexfirst argumentstooltraffic conetrashchairbackpackpersonstooltraffic conetrashchairbackpackpersondetector indexfirst argumentstooltraffic conetrashchairbackpackpersonstooltraffic conetrashchairbackpackpersondetector indexfirst argumentstooltraffic conetrashchairbackpackpersonstooltraffic conetrashchairbackpackpersondetector indexfirst argument6981.000001.001.001.001.001.01.01.0stooltraffic conetrainedhand-craftedtrainedhand-crafted1.000001.001.001.001.00trainedhand-craftedtrainedhand-crafted1.01.01.0trashchair1.000001.001.001.001.00trainedhand-craftedtrainedhand-crafted1.01.01.0backpackpersonFigure 23: Comparison hand-crafted trained models nouns.fiGrounding Language Inference, Generation, Acquisition Videolefthand-craftedrighttrained1.00hand-crafted1.001.00001.0001.01.0trained01.01.0rightleftrightleftdifferencex-position firstsecond argumentsrightleftrightleftdifferencex-position firstsecond argumentsFigure 24: Comparison hand-crafted trained models spatial-relation prepositions.699fiYu, Siddharth, Barbu, & Siskindapproached0.050.010.012440moving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardleftwardleftwardleftwardleftwardupwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardvelocity orientationfirst argumentdownwardrightwardupwarddownwardrightwardupwarddownwardrightwardupwarddownwardrightwardupwarddownwardrightwardupwarddownwardrightwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudefirst argumentupwardleftwardupwardleftwardupwardleftwardnearfarnearfarnearfarnearsmallerlargersmallerlargersmallerlargersmallerlargersmallermoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardfarlargerfarmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryupwardnearsmallerupwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardfardownwardrightwarddownwardrightwarddownwardrightwarddownwardrightwarddownwardrightwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryupwardrightwarddownwardnearmoderate distancemoderate distancemoderate distancemoderate distancemoderate distancemoderate distancemoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardratio sizevelocity orientation distance firstfirst argumentsecond argument second arguments size second argumentlargermoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudesecond argument7000.00.40.51.01.000.950.911.000.990.99trained0.09hand-craftedFigure 25: Comparison hand-crafted trained models verb approached.fiGrounding Language Inference, Generation, Acquisition Videocarried460moving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardleftwardleftwardleftwardleftwardupwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardvelocity orientationfirst argumentdownwardrightwardupwarddownwardrightwardupwarddownwardrightwardupwarddownwardrightwardupwarddownwardrightwardupwarddownwardrightwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudefirst argumentupwardleftwardupwardleftwardupwardleftwardnearfarnearfarnearfarnearsmallerlargersmallerlargersmallerlargersmallerlargersmallermoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardfarlargerfarmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryupwardnearsmallerupwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardfardownwardrightwarddownwardrightwarddownwardrightwarddownwardrightwarddownwardrightwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryupwardrightwarddownwardnearmoderate distancemoderate distancemoderate distancemoderate distancemoderate distancemoderate distancemoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardratio sizevelocity orientation distance firstfirst argumentsecond argument second arguments size second argumentlargermoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudesecond argument7010.00.91.00.020.080.010.011.000.980.861.000.990.99trained0.05hand-craftedFigure 26: Comparison hand-crafted trained models verb carried.fiYu, Siddharth, Barbu, & Siskindpickedmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardleftwardleftwardleftwardleftwardupwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardvelocity orientationfirst argumentdownwardrightwardupwarddownwardrightwardupwarddownwardrightwardupwarddownwardrightwardupwarddownwardrightwardupwarddownwardrightwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudefirst argumentupwardleftwardupwardleftwardupwardleftwardnearfarnearfarnearfarnearsmallerlargersmallerlargersmallerlargersmallerlargersmallermoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardfarlargerfarmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryupwardnearsmallerupwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardfardownwardrightwarddownwardrightwarddownwardrightwarddownwardrightwarddownwardrightwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryupwardrightwarddownwardnearmoderate distancemoderate distancemoderate distancemoderate distancemoderate distancemoderate distancemoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardratio sizevelocity orientation distance firstfirst argumentsecond argument second arguments size second argumentlargermoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudesecond argument7027010.040.040.010.010.961.000.990.0.93.001.00trained0.96hand-crafted0.99Figure 27: Comparison hand-crafted trained models verb picked up.fiGrounding Language Inference, Generation, Acquisition Videoput1.001.00moving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardleftwardleftwardleftwardleftwardupwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardvelocity orientationfirst argumentdownwardrightwardupwarddownwardrightwardupwarddownwardrightwardupwarddownwardrightwardupwarddownwardrightwardupwarddownwardrightwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudefirst argumentupwardleftwardupwardleftwardupwardleftwardnearfarnearfarnearfarnearsmallerlargersmallerlargersmallerlargersmallerlargersmallermoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardfarlargerfarmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryupwardnearsmallerupwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardfardownwardrightwarddownwardrightwarddownwardrightwarddownwardrightwarddownwardrightwardmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryupwardrightwarddownwardnearmoderate distancemoderate distancemoderate distancemoderate distancemoderate distancemoderate distancemoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryleftwardratio sizevelocity orientation distance firstfirst argumentsecond argument second arguments size second argumentlargermoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudesecond argument7030.060.010.010.011.000.940.971.000.990.99trained0.02hand-craftedFigure 28: Comparison hand-crafted trained models verb put down.fiYu, Siddharth, Barbu, & Siskindslowly.001.001moving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudefirst argument704moving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationary00moving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudefirst argument1.01.00.050.080.010.011.00trained0.950.921.00hand-crafted0.990.990.040.020.010.011.00trained0.960.981.00hand-crafted0.990.99quicklyFigure 29: Comparison hand-crafted trained models adverbs.fiGrounding Language Inference, Generation, Acquisition Videotowards1.001.00nearfarmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarynearmoderate distancefarmoderate distancemoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarynearnearnearfarmoderate distancefarmoderate distancefarmoderate distancemoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarynearnearfarmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarynearmoderate distancefarmoderate distancefarmoderate distancemoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarymoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarynearnearfarmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarynearmoderate distancefarmoderate distancefarmoderate distancemoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudedistancefirst argument first second arguments705farmoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationarynear110.070.070.010.01moderate distancemoving quicklymoving quicklymoving slowlymostly stationaryabsolutely stationaryvelocity magnitudedistancefirst argument first second arguments.00.001.00trained0.930.931.00hand-crafted0.990.991.000.010.011.000.941.000.990.99trained0.06hand-craftedawayFigure 30: Comparison hand-crafted trained models motion prepositions.fiYu, Siddharth, Barbu, & SiskindReferencesAoun, J., & Sportiche, D. (1983). formal theory government. Linguistic Review,2 (3), 211236.Avidan, S. (2004). Support vector tracking. IEEE Transactions Pattern AnalysisMachine Intelligence, 26 (8), 10641072.Baker, S., Scharstein, D., Lewis, J., Roth, S., Black, M. J., & Szeliski, R. (2011). databaseevaluation methodology optical flow. International Journal Computer Vision, 92 (1), 131.Barbu, A., Barrett, D. P., Chen, W., Siddharth, N., Xiong, C., Corso, J. J., Fellbaum,C. D., Hanson, C., Hanson, S. J., Helie, S., Malaia, E., Pearlmutter, B. A., Siskind,J. M., Talavage, T. M., & Wilbur, R. B. (2014). Seeing worse believing:Reading peoples minds better computer-vision methods recognize actions.Proceedings European Conference Computer Vision, pp. 612627.Barbu, A., Bridge, A., Burchill, Z., Coroian, D., Dickinson, S., Fidler, S., Michaux, A.,Mussman, S., Siddharth, N., Salvi, D., Schmidt, L., Shangguan, J., Siskind, J. M.,Waggoner, J., Wang, S., Wei, J., Yin, Y., & Zhang, Z. (2012a). Video sentencesout. Proceedings Conference Uncertainty Artificial Intelligence, pp.102112.Barbu, A., Siddharth, N., Michaux, A., & Siskind, J. M. (2012b). Simultaneous objectdetection, tracking, event recognition. Advances Cognitive Systems, 2, 203220.Barbu, A., Siddharth, N., & Siskind, J. M. (2014). Language-driven video retrieval.Proceedings IEEE Conference Computer Vision Pattern RecognitionWorkshop Vision Meets Cognition.Baum, L. E. (1972). inequality associated maximization technique statisticalestimation probabilistic functions Markov process. Inequalities, 3, 18.Baum, L. E., & Petrie, T. (1966). Statistical inference probabilistic functions finitestate Markov chains. Annals Mathematical Statistics, 37 (6), 15541563.Baum, L. E., Petrie, T., Soules, G., & Weiss, N. (1970). maximization technique occuringstatistical analysis probabilistic functions Markov chains. AnnalsMathematical Statistics, 41 (1), 164171.Bellman, R. (1957). Dynamic Programming. Princeton University Press.Berclaz, J., Fleuret, F., Turetken, E., & Fua, P. (2011). Multiple object tracking usingK-shortest paths optimization. IEEE Transactions Pattern Analysis MachineIntelligence, 33 (9), 18061819.Bilmes, J. (1998). gentle tutorial EM algorithm application parameterestimation Gaussian mixture hidden Markov models. Tech. rep. TR-97-021,ICSI.Blank, M., Gorelick, L., Shechtman, E., Irani, M., & Basri, R. (2005). Actions space-timeshapes. Proceedings IEEE International Conference Computer Vision,pp. 13951402.706fiGrounding Language Inference, Generation, Acquisition VideoBorschinger, B., Jones, B. K., & Johnson, M. (2011). Reducing grounded learning tasksgrammatical inference. Proceedings Conference Empirical MethodsNatural Language Processing, pp. 14161425.Brand, M., Oliver, N., & Pentland, A. (1997). Coupled hidden Markov models complexaction recognition. Proceedings IEEE Conference Computer VisionPattern Recognition, pp. 994999.Chen, C.-Y., & Grauman, K. (2013). Watching unlabeled videos helps learn new humanactions labeled snapshots. Proceedings IEEE ConferenceComputer Vision Pattern Recognition, pp. 572579.Chen, D. L., & Mooney, R. J. (2008). Learning sportscast: test grounded languageacquisition. Proceedings International Conference Machine Learning, pp.128135.Chen, D. L., & Mooney, R. J. (2011). Learning interpret natural language navigationinstructions observations. Proceedings Conference Artificial Intelligence, pp. 859865.Chomsky, N. (1982). Concepts Consequences Theory GovernmentBinding. MIT Press.Chomsky, N. (2002). Syntactic Structures (Second edition). Walter de Gruyter.Das, P., Xu, C., Doell, R. F., & Corso, J. J. (2013). thousand frames words:Lingual description videos latent topics sparse object stitching.Proceedings IEEE Conference Computer Vision Pattern Recognition,pp. 26342641.Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incompletedata via EM algorithm (with discussion). Journal Royal Statistical SocietyB, 39 (1), 138.Dinkelbach, W. (1967). nonlinear fractional programming. Management Science, 13 (7),492498.Dominey, P. F., & Boucher, J.-D. (2005). Learning talk events narrated videoconstruction grammar framework. Artificial Intelligence, 167 (1-2), 3161.Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserman, A. (2010).PASCAL Visual Object Classes (VOC) challenge. International Journal ComputerVision, 88 (2), 303338.Everts, I., van Gemert, J. C., & Gevers, T. (2013). Evaluation color stips humanaction recognition. Proceedings IEEE Conference Computer VisionPattern Recognition, pp. 28502857.Farhadi, A., Hejrati, M., Sadeghi, M., Young, P., Rashtchian, C., Hockenmaier, J., &Forsyth, D. (2010). Every picture tells story: Generating sentences images.Proceedings European Conference Computer Vision, pp. 1529.Fellbaum, C. (1998). WordNet: electronic lexical database. MIT Press.707fiYu, Siddharth, Barbu, & SiskindFelzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010a). Objectdetection discriminatively trained part-based models. IEEE TransactionsPattern Analysis Machine Intelligence, 32 (9), 16271645.Felzenszwalb, P. F., Girshick, R. B., & McAllester, D. A. (2010b). Cascade object detectiondeformable part models. Proceedings IEEE Conference ComputerVision Pattern Recognition, pp. 22412248.Feng, S. L., Manmatha, R., & Lavrenko, V. (2004). Multiple Bernoulli relevance modelsimage video annotation. Proceedings IEEE Conference ComputerVision Pattern Recognition, pp. 10021009.Fern, A. P., Givan, R. L., & Siskind, J. M. (2002a). Specific-to-general learning temporalevents. Proceedings Conference Artificial Intelligence, pp. 152158.Fern, A. P., Givan, R. L., & Siskind, J. M. (2002b). Specific-to-general learning temporalevents application learning event definitions video. Journal ArtificialIntelligence Research, 17, 379449.Fern, A. P., Siskind, J. M., & Givan, R. L. (2002c). Learning temporal, relational, forcedynamic event definitions video. Proceedings Conference ArtificialIntelligence, pp. 159166.Fernandez Tena, C., Baiget, P., Roca, X., & Gonzalez, J. (2007). Natural language descriptions human behavior video sequences. Advances Artificial Intelligence,pp. 279292.Gaidon, A., Harchaoui, Z., & Schmid, C. (2014). Activity representation motionhierarchies. International Journal Computer Vision, 107 (3), 219238.Grimshaw, J. (1979). Complement selection lexicon. Linguistic Inquiry, 10 (2),279326.Grimshaw, J. (1981). Form, function, language acquisition device. Baker, C. L.,& McCarthy, J. J. (Eds.), Logical Problem Language Acquisition, pp. 165182.MIT Press.Guadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S., Mooney, R., Darrell, T., & Saenko, K. (2013). Youtube2text: Recognizing describing arbitraryactivities using semantic hierarchies zero-shot recognition. ProceedingsIEEE International Conference Computer Vision, pp. 27122719.Gupta, A., Verma, Y., & Jawahar, C. (2012). Choosing linguistics vision describeimages. Proceedings Conference Artificial Intelligence, pp. 606612.Haegeman, L. (1992). Introduction government binding theory. Blackwell.Hamming, R. W. (1950). Error detecting error correcting codes. Bell System TechnicalJournal, 29 (2), 147160.Han, M., Sethi, A., Hua, W., & Gong, Y. (2004). detection-based multiple object trackingmethod. Proceedings IEEE International Conference Image Processing,pp. 30653068.708fiGrounding Language Inference, Generation, Acquisition VideoHanckmann, P., Schutte, K., & Burghouts, G. J. (2012). Automated textual descriptionswide range video events 48 human actions. Proceedings EuropeanConference Computer Vision (Workshops Demonstrations), pp. 372380.Ikizler-Cinbis, N., & Sclaroff, S. (2010). Object, scene actions: Combining multiplefeatures human action recognition. Proceedings European ConferenceComputer Vision, pp. 494507.Jackendoff, R. (1977). X-bar-syntax: study phrase structure. MIT Press.Jensen, J. L. W. V. (1906). Sur les fonctions convexes et les inegalites entre les valeursmoyennes. Acta Mathematica, 30 (1), 175193.Jie, L., Caputo, B., & Ferrari, V. (2009). Whos what: Joint modeling namesverbs simultaneous face pose annotation. Proceedings NeuralInformation Processing Systems Conference, pp. 11681176.Khan, M. U. G., & Gotoh, Y. (2012). Describing video contents natural language.Proceedings Workshop Innovative Hybrid Approaches ProcessingTextual Data, pp. 2735.Khan, M. U. G., Zhang, L., & Gotoh, Y. (2011). Human focused video description.Proceedings IEEE International Conference Computer Vision (Workshops),pp. 14801487.Kim, J., & Mooney, R. J. (2010). Generative alignment semantic parsing learning ambiguous supervision. Proceedings International ConferenceComputational Linguistics, pp. 543551.Kim, J., & Mooney, R. J. (2012). Unsupervised PCFG induction grounded languagelearning highly ambiguous supervision. Proceedings ConferenceEmpirical Methods Natural Language Processing, pp. 433444.Kim, J., & Mooney, R. J. (2013). Adapting discriminative reranking grounded languagelearning. Proceedings Annual Meeting Association ComputationalLinguistics, pp. 218227.Klein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. ProceedingsAnnual Meeting Association Computational Linguistics, pp. 423430.Kojima, A., Tamura, T., & Fukunaga, K. (2002). Natural language description humanactivities video images based concept hierarchy actions. InternationalJournal Computer Vision, 50 (2), 171184.Krishnamoorthy, N., Malkarnenkar, G., Mooney, R. J., Saenko, K., & Guadarrama, S.(2013). Generating natural-language video descriptions using text-mined knowledge.Proceedings NAACL HLT Workshop Vision Language, pp. 1019.Krishnamurthy, J., & Kollar, T. (2013). Jointly learning parse perceive: Connectingnatural language physical world. Transactions Association Computational Linguistics, 1, 193206.Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., & Serre, T. (2011). HMDB: large videodatabase human motion recognition. Proceedings IEEE InternationalConference Computer Vision, pp. 25562563.709fiYu, Siddharth, Barbu, & SiskindKulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A. C., & Berg, T. L. (2011).Baby talk: Understanding generating simple image descriptions. ProceedingsIEEE Conference Computer Vision Pattern Recognition, pp. 16011608.Kuznetsova, P., Ordonez, V., Berg, A. C., Berg, T. L., & Choi, Y. (2012). Collectivegeneration natural image descriptions. Proceedings Annual MeetingAssociation Computational Linguistics, pp. 359368.Kwiatkowski, T., Goldwater, S., Zettlemoyer, L., & Steedman, M. (2012). probabilisticmodel syntactic semantic acquisition child-directed utterancesmeanings. Proceedings Conference European Chapter AssociationComputational Linguistics, pp. 234244.Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., & Steedman, M. (2010). Inducing probabilistic CCG grammars logical form higher-order unification. ProceedingsConference Empirical Methods Natural Language Processing, pp. 12231233.Laptev, I. (2005). space-time interest points. International Journal Computer Vision,64 (2/3), 107123.Li, P., & Ma, J. (2011). happening still picture?. Proceedings AsianConference Pattern Recognition, pp. 3236.Liu, J., Luo, J., & Shah, M. (2009). Recognizing realistic actions videos wild.Proceedings IEEE Conference Computer Vision Pattern Recognition,pp. 19962003.Mann, R., Jepson, A. D., & Siskind, J. M. (1996). computational perception scenedynamics. Proceedings European Conference Computer Vision, pp. 528539.Mann, R., Jepson, A. D., & Siskind, J. M. (1997). computational perception scenedynamics. Computer Vision Image Understanding, 65 (2), 113128.Marocco, D., Cangelosi, A., Fischer, K., & Belpaeme, T. (2010). Grounding action wordssensorimotor interaction world: experiments simulated iCubhumanoid robot. Frontiers Neurorobotics, 4 (7), 115.Messing, R., Pal, C., & Kautz, H. (2009). Activity recognition using velocity historiestracked keypoints. Proceedings IEEE International Conference ComputerVision, pp. 104111.Miller, G. A. (1995). WordNet: lexical database English. CommunicationsACM, 38 (11), 3941.Mitchell, M., Dodge, J., Goyal, A., Yamaguchi, K., Stratos, K., Han, X., Mensch, A., Berg,A. C., Berg, T. L., & III, H. D. (2012). Midge: Generating image descriptionscomputer vision detections. Proceedings Conference European ChapterAssociation Computational Linguistics, pp. 747756.Niebles, J. C., Chen, C.-W., & Fei-Fei, L. (2010). Modeling temporal structure decomposable motion segments activity classification. Proceedings EuropeanConference Computer Vision, pp. 392405.710fiGrounding Language Inference, Generation, Acquisition VideoOrdonez, V., Kulkarni, G., & Berg, T. L. (2011). Im2text: Describing images using 1 millioncaptioned photographs. Proceedings Neural Information Processing SystemsConference, pp. 11431151.Piantadosi, S. T., Goodman, N. D., Ellis, B. A., & Tenenbaum, J. B. (2008). Bayesianmodel acquisition compositional semantics. Proceedings AnnualConference Cognitive Science Society, pp. 16201625.Pinker, S. (1984). Language Learnability Language Development. Harvard UniversityPress.Pirsiavash, H., Ramanan, D., & Fowlkes, C. C. (2011). Globally-optimal greedy algorithmstracking variable number objects. Proceedings IEEE ConferenceComputer Vision Pattern Recognition, pp. 12011208.Reddy, K. K., & Shah, M. (2013). Recognizing 50 human action categories web videos.Machine Vision Applications, 24 (5), 971981.Rodriguez, M. D., Ahmed, J., & Shah, M. (2008). Action MACH: spatio-temporal maximum average correlation height filter action recognition. ProceedingsIEEE Conference Computer Vision Pattern Recognition, pp. 18.Rohrbach, M., Qin, W., Titov, I., Thater, S., Pinkal, M., & Schiele, B. (2013). Translating video content natural language descriptions. Proceedings IEEEInternational Conference Computer Vision, pp. 433440.Roy, D. (2002). Learning visually-grounded words syntax scene description task.Computer Speech Language, 16 (3-4), 353385.Roy, D. K., & Pentland, A. P. (2002). Learning words sights sounds: computational model. Cognitive Science, 26 (1), 113146.Sadanand, S., & Corso, J. J. (2012). Action bank: high-level representation activityvideo. Proceedings IEEE Conference Computer Vision PatternRecognition, pp. 12341241.Schuldt, C., Laptev, I., & Caputo, B. (2004a). Recognizing human actions: local SVMapproach. Proceedings International Conference Pattern Recognition, pp.3236.Schuldt, C., Laptev, I., & Caputo, B. (2004b). Recognizing human actions: local svmapproach. Proceedings International Conference Pattern Recognition, pp.3236.Siddharth, N., Barbu, A., & Siskind, J. M. (2014). Seeing youre told: Sentence-guidedactivity recognition video. Proceedings IEEE Conference ComputerVision Pattern Recognition, pp. 732739.Siskind, J. M. (1996). computational study cross-situational techniques learningword-to-meaning mappings. Cognition, 61 (1-2), 3991.Siskind, J. M. (2001). Grounding lexical semantics verbs visual perception usingforce dynamics event logic. Journal Artificial Intelligence Research, 15, 3190.Siskind, J. M., & Morris, Q. (1996). maximum-likelihood approach visual event classification. Proceedings European Conference Computer Vision, pp. 347360.711fiYu, Siddharth, Barbu, & SiskindSiskind, J. M. (1999). Visual event perception. Proceedings NEC Research Symposium, pp. 91154.Siskind, J. M. (2000). Visual event classification via force dynamics. ProceedingsConference Artificial Intelligence, pp. 149155.Siskind, J. M. (2003). Reconstructing force-dynamic models video sequences. ArtificialIntelligence, 151 (1-2), 91154.Siskind, J. M., & Morris, Q. (1996). maximum-likelihood approach visual event classification. Proceedings European Conference Computer Vision, pp. 347360.Smith, K., Smith, A. D. M., & Blythe, R. A. (2011). Cross-situational learning: experimental study word-learning mechanisms. Cognitive Science, 35 (3), 480498.Smith, K., Smith, A. D. M., Blythe, R. A., & Vogt, P. (2006). Cross-situational learning: mathematical approach. Proceedings International WorkshopEmergence Evolution Linguistic Communication, pp. 3144.Song, H. O., Zickler, S., Althoff, T., Girshick, R., Fritz, M., Geyer, C., Felzenszwalb, P.,& Darrell, T. (2012). Sparselet models efficient multiclass object detection.Proceedings European Conference Computer Vision, pp. 802815.Song, Y., Morency, L.-P., & Davis, R. (2013). Action recognition hierarchical sequencesummarization. Proceedings IEEE Conference Computer VisionPattern Recognition, pp. 35623569.Starner, T., Weaver, J., & Pentland, A. (1998). Real-time American Sign Language recognition using desk wearable computer based video. IEEE Transactions PatternAnalysis Machine Intelligence, 20 (12), 13711375.Tellex, S., Thaker, P., Joseph, J., & Roy, N. (2013). Learning perceptually grounded wordmeanings unaligned parallel data. Machine Learning, 0, 117.Thompson, C. A., & Mooney, R. J. (2003). Acquiring word-meaning mappings naturallanguage interfaces. Journal Artificial Intelligence Research, 18, 144.Tian, Y., Sukthankar, R., & Shah, M. (2013a). Spatiotemporal deformable part modelsaction detection. Proceedings IEEE Conference Computer VisionPattern Recognition, pp. 26422649.Tian, Y., Sukthankar, R., & Shah, M. (2013b). Spatiotemporal deformable part modelsaction detection. Proceedings IEEE Conference Computer VisionPattern Recognition, pp. 26422649.Viterbi, A. J. (1967). Error bounds convolutional codes asymtotically optimumdecoding algorithm. IEEE Transactions Information Theory, 13 (2), 260267.Wang, H., Klaser, A., Schmid, C., & Liu, C.-L. (2011). Action recognition dense trajectories. Proceedings IEEE Conference Computer Vision PatternRecognition, pp. 31693176.Wang, H., Klaser, A., Schmid, C., & Liu, C.-L. (2013). Dense trajectories motionboundary descriptors action recognition. International Journal Computer Vision, 103 (1), 6079.712fiGrounding Language Inference, Generation, Acquisition VideoWang, H., & Schmid, C. (2013). Action recognition improved trajectories. Proceedings IEEE International Conference Computer Vision, pp. 35513558.Wang, Z., Guan, G., Qiu, Y., Zhuo, L., & Feng, D. (2013). Semantic context based refinement news video annotation. Multimedia Tools Applications, 67 (3), 607627.Werlberger, M., Pock, T., & Bischof, H. (2010). Motion estimation non-local totalvariation regularization. Proceedings IEEE Conference Computer VisionPattern Recognition, pp. 24642471.Wolf, J. K., Viterbi, A. M., & Dixon, G. S. (1989). Finding best set K pathstrellis application multitarget tracking. IEEE Transactions AerospaceElectronic Systems, 25 (2), 287296.Wu, B., & Nevatia, R. (2007). Detection tracking multiple, partially occluded humansBayesian combination edgelet based part detectors. International JournalComputer Vision, 75 (2), 247266.Yamoto, J., Ohya, J., & Ishii, K. (1992). Recognizing human action time-sequential images using hidden Markov model. Proceedings IEEE Conference ComputerVision Pattern Recognition, pp. 379385.Yang, Y., Teo, C. L., Daume III, H., & Aloimonos, Y. (2011). Corpus-guided sentencegeneration natural images. Proceedings Conference Empirical MethodsNatural Language Processing, pp. 444454.Yao, B. Z., Yang, X., Lin, L., Lee, M. W., & Zhu, S.-C. (2010). I2T: Image parsing textdescription. Proceedings IEEE, 98 (8), 14851508.Yilmaz, A., Javed, O., & Shah, M. (2006). Object tracking: survey. ACM ComputingSurveys, 38 (4), 145.Yu, C., & Ballard, D. H. (2004). integration grounding language learningobjects. Proceedings Conference Artificial Intelligence, pp. 488493.Yu, H., & Siskind, J. M. (2013). Grounded language learning video describedsentences. Proceedings Annual Meeting Association ComputationalLinguistics, pp. 5363.Yu, S.-Z. (2010). Hidden semi-Markov models. Artificial Intelligence, 174 (2), 215243.Yuan, C., Li, X., Hu, W., Ling, H., & Maybank, S. (2013). 3D R transform spatiotemporal interest points action recognition. Proceedings IEEE ConferenceComputer Vision Pattern Recognition, pp. 724730.Zettlemoyer, L. S., & Collins, M. (2005). Learning map sentences logical form: Structured classification probabilistic categorial grammars. Proceedings Conference Uncertainty Artificial Intelligence, pp. 658666.Zhong, S., & Ghosh, J. (2001). new formulation coupled hidden Markov models. Tech.rep., Department Electrical Computer Engineering, University TexasAustin.Zhu, J., Wang, B., Yang, X., Zhang, W., & Tu, Z. (2013). Action recognition Actons.Proceedings IEEE International Conference Computer Vision, pp. 35593566.713fiJournal Articial Intelligence Research 52 (2015) 235-286Submitted 10/14; published 02/15Lazy Model Expansion:Interleaving Grounding Searchbroes.decat@gmail.comBroes De CatOM Partners, BelgiumMarc.Denecker@cs.kuleuven.beMarc DeneckerDept. Computer Science, KULeuven, Belgiumpstuckey@unimelb.edu.auPeter StuckeyNational ICT AustraliaDept. Computing Information SystemsUniversity Melbourne, AustraliaMaurice BruynoogheDept. Computer Science, KULeuven, BelgiumMaurice.Bruynooghe@cs.kuleuven.beAbstractFinding satisfying assignments variables involved set constraintscast (bounded) model generation problem: search (bounded) models theorylogic. state-of-the-art approach bounded model generation rich knowledge representation languages like Answer Set Programming (ASP) FO() CSPmodeling language Zinc, ground-and-solve : reduce theory groundpropositional one apply search algorithm resulting theory.important bottleneck blow-up size theory caused groundingphase. Lazily grounding theory search way overcome bottleneck.present theoretical framework implementation context FO()knowledge representation language. Instead grounding parts theory, justicationsderived parts it. Given partial assignment grounded parttheory valid justications formulas non-grounded part, justicationsprovide recipe construct complete assignment satises non-grounded part.justication particular formula becomes invalid search, new onederived; fails, formula split part grounded partjustied. Experimental results illustrate power generality approach.1. Introductionworld lled combinatorial problems.include important combinatorialoptimization tasks planning, scheduling rostering, combinatorics problemsextremal graph theory, countless puzzles games. Solving combinatorial problemshard, methods know tackle involve kind search.Variousdeclarative paradigmsdeveloped solve problems.approaches, objects attributes searched represented symbols,constraints satised objects represented expressions symbolsc 2015 AI Access Foundation. rights reserved.fiDe Cat, Denecker, Stuckey & Bruynooghedeclarative language. Solvers search values symbols satisfyconstraints. idea found elds Constraint Programming (CP) (Apt, 2003),ASP (Marek & Truszczyski, 1999), SAT, Mixed Integer Programming (MIP), etc.terminology logic, declarative method amounts expressing desired propertieslogical theory. data particular problem instancepartial interpretation (or structure). solving processapply model generation, specically model expansion (Mitchell & Ternovska, 2005),problem class sentencescorresponds naturallytask nding structure expands input partial structure satisestheory. resulting structure solution problem. Model generation/expansion,studied example eld Knowledge Representation (KR) (Baral, 2003), thusanalogous task solving constraint satisfaction problems, studied CP,generating answer sets logic programs, studied ASP.similarities areas go deeper extend level used techniques.State-of-the-art approaches often follow two-phase solving methodology.rst phase, input theory, rich language hand, reduced fragmentlanguage supported search algorithm. second phase, searchalgorithm applied reduced theory eectively search models. example,model generation language MiniZinc (Nethercote et al., 2007) performed reducing ground language FlatZinc, search algorithms available. Similarly,languagePC()FO()(Denecker & Ternovska, 2008) reduced propositional fragment(see, e.g., Wittocx, Marin, & Denecker, 2010), ASP reduced propositionalASP (see, e.g., Gebser, Schaub, & Thiele, 2007). reduced theory often groundfragment language, refer resulting reduced theoryrst phasegroundinggroundingphase (where quantiers instantiated elementsdomain). elds, grounding also referred attening, unrolling, splittingpropositionalization. solving methodology generally referredand-solve.ground-Grounding becomes bottleneck users turn applications large domainscomplex constraints. Indeed, easy see grounding size FO formulaexponential nesting depth quantiers arity predicates polynomialsize universe discourse.increasing number applicationssize grounded theory large memory.example, Son, Pontelli, Le (2014) discuss several ASP applications groundand-solve approach turns inadequate.paper, present novel approach remedy bottleneck, calledlazy modelexpansion, grounding generated lazily (on-the-y) search, instead upfront. approach works associating justications non-ground parts theory.valid justication non-ground formula recipe expand partial structureprecise (partial) structure satises formula. course, crucialrecipe lot compact grounding formula. Given partial structurevalid justication non-ground formulas, (total) structure obtainedextending partial structure literals justications non-groundformulas. Justications selected way total structure modelwhole initial theory.Consequently, model generation limited groundedpart theory; model found part, extended model236fiLazy Model Expansion: Interleaving Grounding Searchwhole theory. However, new assignment model generation conict onejustications. case, alternative justication needs sought. nonefound, associated formula split two parts, one part grounded onepart valid justication still available.Example 1.1.ConsiderSokobanproblem, planning problem robotpush blocks around 2-D grid arrange given goal conguration. constraintmove action target position(at timeT)pPmoved blockbBcurrentlyempty, expressed(t, b, p) B P : move(b, p, t) empty(p, t).(1)known advance many time steps needed, one ideally wants assumelarge even innite number steps. Using ground-and-solve, blows sizegrounding. Incremental grounding, iteratively extending time domainlarge enough allow plan, developed avoid blow-up contextplanning (Gebser et al., 2008). approach general dependpresence one domain incrementally increased.Returning example, instead grounding sentence (1), associatejustication, recipe satisfy it.Makemove(b, p, t)falseb, precipe. search nds model grounded part problemconict recipe, model extended literals recipe obtainmodel whole theory. However, search would decide move blockp1b1positiontime t1 , conict created recipe. resolve it, instance sentence (1)conict partial model search split sentence (1) replacedequivalent sentences:move(b1 , p1 , t1 ) empty(p1 , t1 )(2)(t, b, p) B P \(t1 , b1 , p1 ) : move(b, p, t) empty(p, t)(3)Sentence (2) grounded passed search component use checkempty(p1 , t1 )holds.Sentence (3) non-ground satised recipemove(b, p, t) false exceptmove(b1 , p1 , t1 ).search makes moves,instances grounded, search nds partial plan problem hand.literals recipe remaining non-ground formula makingmove(b, p, t)false instances sentence (1) grounded complete plan.main contributions paper are:theoretical frameworklazy model expansion.aiming minimally instantiat-ing quantied variables, paves way solution long-standing problemhandling quantiers search problems, encountered, e.g., elds ASP (Lefvre& Nicolas, 2009) SAT Modulo Theories (Ge & de Moura, 2009). frameworkalso generalizes existing approaches related grounding bottleneckincremental domain extension (Claessen & Srensson, 2003) lazy clause generation (Ohrimenko, Stuckey, & Codish, 2009).237fiDe Cat, Denecker, Stuckey & Bruynooghecomplete algorithm lazy model expansion logicFO(ID ),extensionrst-order logic (FO) inductive denitions (a language closely related ASPshown Denecker et al., 2012). includes ecient algorithms derive consistentsets justications maintain throughout changes partial structure(e.g., search).IDPimplementation extendingknowledge-base system (De Cat et al., 2014)experiments illustrate power generality lazy grounding.Lazy grounding new step ability solve complex combinatorial problems.avoiding up-front grounding step previous approaches, lazy grounding groundenough problem solve it. method developed logicFO(ID ),become clear, justications associated rules, rules similarrules used ASP systems. Hence, discussed towards end paper,framework algorithms applied also context ASP.paper organized follows.Section 2, necessary background nota-tions introduced. Formal denitions lazy groundingFO(ID )presentedSection 3, followed presentation relevant algorithms heuristics Sections 45. Experimental evaluation provided Section 6, followed discussion relatedfuture work conclusion. preliminary version paper appeared workDe Cat, Denecker, Stuckey (2012) De Cat (2014, ch. 7).2. Preliminariessection, provide necessary background logictasks model generation model expansionFO(ID )FO(ID ),inferenceground-and-solveapproach model expansion.2.1FO(ID )First, dene syntax semantics logicFO(ID )(Denecker & Ternovska, 2008),extension rst-order logic (FO) inductive denitions. assume familiarityFO. Without loss generality, limitFO(ID )function-free fragment. Functionsymbols always eliminated using graph predicates (Enderton, 2001).(function-free) vocabularyconsists set predicate symbols.Propositional> , denoting truefalse respectively. Predicate symbols usually denoted P , Q, R; atoms a, literals(atoms negation) l; variables x, ; domain elements d. edenote ordered set objects e1 , . . . , en ; P/n predicate P arity n.symbols 0-ary predicate symbols; include symbolsmethods model generating developed require (possibly innite)domain, domain atomP P/n Dn , n-tuple domain elements. Likewise,given xed. Given (function-free) vocabularyatom formconsiderdomain literals.consists domain n-ary relation P Dnpredicate symbols P/n . Alternatively, n-ary relation viewednfunction {t, f }. propositional symbols > respectively interpretedf .structureinterpreting238fiLazy Model Expansion: Interleaving Grounding SearchModel generation algorithms maintaininconsistentpartialstructures may (temporarily) ndstate, example conict arises. representintroduced; consist domainn-ary predicate P , three- four-valued relation P .nfunction {t, f , u, i}. structure two-valued range relations {t, f },partial three-valued range {t, f , u}) four-valued general. Thus, two-valuedstates, three-valued four-valued structuresand,structures also three-valued four-valued.unqualied, termstructurestands general, four-valued case.Given xed, alternative way representdomainaI=t=fa, aI = (inconsistent)aI = u (unknown)domain atom,domain literals.-structures,setIndeed, one-to-one correspondence setsotherwise. Hence,may treat four-valued structures sets domain literals vice versa. structureinconsistent least one domain atom inconsistent.vocabulary naturally viewed structure larger, namely setting aI = u domain atom predicate 0 \ .set predicate symbols, use I| denote restriction symbolsI|. set domain atoms, use I|S denote restriction : =I|= u otherwise. call two-valued structure two-valueddomain atoms unknown otherwise.1 truth value v dened follows: t1 = f , f 1 = t, u1 = uinverse v1= i. truth order >t truth values dened >t u >t f >t >t f .precision order >p dened >p >p u >p f >p u. orders pointwise0extended arbitrary -structures. say ' expansion p ,0domain atom a,p . Viewing structures sets domain literals,0corresponds .structure0vocabularyassume familiarity syntax (function-free) FO. facilitate reasoningpartially grounded formulas, deviate standard FO quantify explicitlyx D0 : x D0 : ,D. sometimes abbreviate x1 D1 : . . . xn Dn : x : , similarly. Given formula , [x] indicates x free variables . Substitutionvariable x formula term denoted [x/t]. ground formula (in domain )specied subsets domainD.denotedD0formula without variables (hence without quantiers). Similar properties notationsusedrules(introduced below).voc(T ) set predicate symbols occur theory .structure , voc(I) set symbols interpreted . Unless specied otherwise,theories structures range vocabulary .language FO(ID ) extends FO (inductive) denitions. theory FO(ID )(nite) set sentences denitions. denition (nite) set rulesform x : P (x1 , . . . , xn ) , P predicate symbol FO formula.atom P (x) referred head rule body. Given rule r , lethead (r) body(r) denote respectively head body r. Given denition ,domain atom P dened exists rule x : P (x). Otherwise P open . domain literal P dened Pdenote239fiDe Cat, Denecker, Stuckey & Bruynooghe. sets dened open domain atoms denoted defined ()open(), respectively.denedWithout loss generality, assume denition domain atom denedone rule. Technically, means rulesP (x) 2x D1 : P (x) 1 , x D2 :D1 D2 = . Rules always made disjunctx D1 D2 : P (x) 1 2 , x D1 \ D2 : P (x) 1 ,pairwise disjunct,transformingx D2 \ D1 : P (x) 2 .2.1.1 Model SemanticssemanticsFO(ID )two-valued model semantics.Nevertheless, introduceconcepts three- four-valued semantics useful dening semanticsdenitions formalizing lazy grounding.use standard four-valued truthassignment function, dened structural induction pairs FO domain formulasinterpret :P = P (d ),structures( )I = min<t ( , ),( )I = max<t ( , ),()I = ( )1 ,(x : )I = max<t ({[x/d]I | D}),(x : )I = min<t ({[x/d]I | D}).assignment function monotonic precision order:p 0 ,p0.Hence, formula true partial structure, true two-valued expansionsit.Also,two-valued (respectively three-valued, four-valued)two-valuedtwo-valued(respectively three-valued, four-valued).structure= t.model/satisessentence(notation|= )satisfaction relation dened denitions well. semanticsdenitions based parametrized well-founded semantics, extension wellfounded semantics logic programs informally described rst work Van Gelder(1993), formally denedFO(ID )'sdenitions Denecker (2000). semanticsformalizes informal semantics rule sets (inductive) denitions (Denecker, 1998;structure(notation |= ) two-valued wellfounded model denoted wf ( I|open() ) structure I|open() (Denecker& Ternovska, 2008). case wf ( I|open() ) two-valued, model expandingI|open() . structure satises theory two-valued modelsentences denitions . next subsection, present formalizationDenecker, Bruynooghe, & Marek, 2001; Denecker & Vennekens, 2014).model/satisesdenitionwell-founded semantics using notionAccordingFO(ID )'sjustication.methodology, (formal) denitions used express informaldenitions. work Denecker Vennekens (2014), shownFO(ID )de-nitions oer uniform representation important types informal denitions240fiLazy Model Expansion: Interleaving Grounding Searchexpressing informal denitions leads rule setscalledtotalwell-founded modeltwo-valued (Denecker & Ternovska, 2008).total.Formally, denitiontwo-valued structureopen()general, totality undecidable; howeverbroad, syntactically dened classes denitions proven total (e.g., nonrecursive, positive, stratied locally stratied denitions, see Denecker & Ternovska,2008). Inspection currentFO(ID )applications shows practice, non-total deni-tions occur rarely almost always contain modeling error. Also, cases totalityestablished simple syntactic check.Totality usefully exploitedcomputation. lazy grounding techniques introduced exploit totalityapplied total denitions. restriction matchesFO(ID )'smethodology and, practice, impose strong limitation.designcaseinput theory contain denitions known total, lost:denitions grounded completely up-front, case lazy grounding appliedsafely remaining sentences total denitions input.Equivalence.equivalentTwo theories0,dierent vocabularies,-0expanded model vicerestricted0 strongly -equivalent expansions alsounique. extension, (strong) -equivalence structure dened similarly:0model expanding expanded model expanding vice versa;obtain strong equivalence, expansions unique. theory , often0derive strongly voc(T )-equivalent theory given structure . transformations0preserve satisability number models model directly mappedmodel projection voc(T ).versa.modelTwo theoriesCanonical theories.simplify presentation, lazy grounding techniquespresented theories form{PT , },single denition function-free rules.PTpropositional symbol,without loss generality.First,mentioned above, standard techniques (Enderton, 2001) allow one make theory functionfree. Second, multiple denitions always combined one described DeneckerTernovska (2008) Marin, Gilis, Denecker (2004). achieved renamingdened predicates denitions, merging rules one set addingequivalence constraints predicates renamings.{1 , . . . , n , }equivalent theory{PT , {PT 1 n }}PT=voc(T )-Third, theoryresulting previous step translated stronglynew propositional symbol.transformation results ground set sentences denition consistingset (ground non-ground) rules, lazy grounding cope non-groundrules. Furthermore, assume rule bodies negation normal form (negationoccurs front atoms) that, dened domain atomrulex : P (x)dDP ,unique.methods proposed extended fullFO(ID )functions,extended methods implemented system. However, introduces numberrather irrelevant technicalities want avoid here.241fiDe Cat, Denecker, Stuckey & Bruynooghe2.1.2 Justificationscanonical theory = {, } explainedcorrespond one-to-one sets domain literals.Denition 2.1 (Directjustication). direct justication dened domain literal P(respectively P ) consistent non-empty set domain literals that,rule x : P (x) , holds [x/d] = (respectively[x/d] = f ).0consistent superset direct justication P direct justication0well. Indeed, body [x/d] true true precise . Also, directjustication empty denition; true every structure, minimaldirect justication {>}.assume presence domainabove. Recall, structures domainExample 2.2.Consider domaindirect justication= {d1 , . . . , dn }denitionx : P (x) Q(x) R(x)x : Q(x) P (x)Q(di ){P (di )}Q(di ){P (di )}.domain literalsmany direct justications, unique minimal onesP (di ) {Q(di )} {R(di )}P (di ) {Q(di ), R(di )}. Atoms R(di ) opensubset relation. Minimal direct justicationsminimal direct justicationdirect justication.G pair hV, Ei set V nodes set E directed(vi , vj ) nodes. node v V , denote G(v)G(v) = {w | (v, w) E}.(directed) graphi.e., ordered pairschildrenv,i.e.,Denition 2.3 (Justication).domain literalsjusticationdenitiondomain literall, J(l)graphJedges,setseteither empty directjustication l.Thus, justication graph encodes every dened domain literal none onedirect justication. sequel saydenoted set pairsl S,Jdened lJ(l) 6= .justicationdirect justication l.Denition 2.4 (Justication subgraph).LetJjustication.justicationliteral l subgraph Jl nodes edges J reachable l. justicationset literals L subgraph JL nodes edges J reachable l L.justication J total l J dened literal reachablel dened ; total set literals L total literal L.justication J consistent structure consistent none literalsJJdened falseI.total l, leavesJlopen domain literals.242fiLazy Model Expansion: Interleaving Grounding SearchDenition 2.5.li li+1 ,positive literals;cycleJpath justicationsequenceedge li li+1negativejusticationJJ.l0 l1 . . .pathpositiveconsists negative literals;set domain literals pathJthat,consistsmixedotherwise.starts endsdomain literal. cycle positive (respectively, negative) domain literalspositive literals (respectively, negative literals); otherwise cycle mixed.innite path may cyclic not.Intuitively, justicationtruthl.Jnite, every innite path cyclic.containing domain literallprovides argumentstrength argument depends truth leavesinnite paths cyclesprovides argumentJl .lleaves true every innite path negative,true. leaf false unknown,mixed loop, argumentlJlJlcontains positiveweak. Notice justicationslmay stillargue l's truth.Denition 2.6 (Justies).l well-founded justicationJ every innite path Jl negative. Otherwise l unfounded J .justication J justies set literals L dened (the set L literalsjustication J ) (i) JL total L; (ii) literal L well-founded J ; (iii)set literals JL consistent.P (d)Q(d)say dened literalP (d)R(d)P (d)Q(d)P (d)Q(d)R(d)Q(d)(i)(ii)(iii)Figure 1: Justications denitionExample 2.7.P (d)D.Example 2.2 contain dened domain atomsJustication (ii) justies(iii), however, totalExample 2.2,Figure 1, show possible justications (ordered (i)-(iv) leftright) denitionQ(d) (d D).(iv)P (d)P (d)Q(d)Q(d)(iv) justiesP (d)P (d)Q(d);(i) positive cycle unfoundedQ(d).relationship justications well-founded semantics investigated dierent publications (Denecker & De Schreye, 1993, 1992; Marin, 2009).recall results paper relies. rst result statesliteralsLL,JL .modelleavesJLJjustiestrue, satises literalsProposition 2.8. J justication justies set domain literals Lliterals JL true every model (open) leaves JL true.243fiDe Cat, Denecker, Stuckey & Bruynoogheinterpretation Iopen two-valued open(), well-founded modelwf (Iopen ) computed time polynomial size domain, shown ChenWarren (1996). general, wf (Iopen ) three-valued structure. wf (Iopen ) twovalued, unique model expands Iopen ; otherwise, modelexpands Iopen . proposition follows fact justication J justiesL leaves J true Iopen , literals L true wf (Iopen ).Example 2.9.R(d)trueR(d)Justication (ii) justies L = {Q(d)}Iopen interpreting open predicates ,wf (Iopen ). particular, model(Continued Example 2.7)unique open leafR(d).structureIopen , Q(d)true, Q(d) true.trueProposition 2.10. model , justication J exists consistsliterals true , dened dened domain literals true justiesthem.Corollary 2.11. casetotal, justication J justies set domainliterals L, every two-valued open()-structure consistent JL extendedunique way model satises literals L.{PT , }justies PT .Hence, canonical theoryjusticationJexists(recall,total), theory satisable2.2 Generating ModelsModel generationmodelT.inference task takes input theoryreturns outputModel Expansion (MX) dened Mitchell et al. (2006) inferencetask takes input theorysubvocabulary,vocabularyreturns expansiontwo-valued structuresatisesT.Here,general inference problem dened Wittocx, Marin, Denecker (2008)takes input (potentially partial) structuresatises,returns expansionT.already mentioned, state-of-the-art approach model expansion(similar ASP) groundingcontextFO(ID )afterwards applying searchresulting ground theory. latter can, e.g., accomplished SAT(ID) searchalgorithm (Marin et al., 2008).Below, present grounding algorithm basis lazy MX algorithm.assume familiarity basic Conict-Driven Clause-Learning (CDCL) algorithmSAT solvers (Marques Silva, Lynce, & Malik, 2009).2.2.1 Groundingoverview intelligent grounding techniquesFO(ID ),refer readerwork Wittocx, Denecker, Bruynooghe (2013) Wittocx et al. (2010).present basic principle.vocabulary , partial structure, returns ground theory 0 stronglygrounder takes input theorydomainD,interpreting least>244fiLazy Model Expansion: Interleaving Grounding Search-equivalentassumeI.TheoryT0calledcanonical theory formgroundinggivenI.Recall{PT , }.One way compute grounding using top-down process theory, iterativelyapplying grounding steps direct subformulas rule formula hand. groundingLet [x]let domains x. Tseitin transformation replaces1atom (x), new |x|-ary predicate symbol called Tseitin symbol, extendsrule x : (x) . new theory strongly -equivalentalgorithm may replace subformulas new predicate symbols follows.formulaoriginal one (Vennekens et al., 2007).procedure one_step_ground, outlined Figure 1, performs one step grounding process. Called formula rulecanonical form, algorithm replacesG(rules formulas) possibly non-ground part R (rules). formula, Gconsists ground formulas. Replacing returned ground formulas extendingreturned rules produces theory strongly voc(T )-equivalent original.rule , G consists ground rules, replacing sets returnedrules results theory strongly voc(T )-equivalent original.direct subformulas Tseitin symbols returns pair consisting ground partAlgorithm 1: one_step_ground algorithm.1 Function one_step_ground (formula rule )2switch3case []P return h{}, i;4case P5678910111213141516hG,:= one_step_ground( );return h{P gG g}, i;case 1 . . . Wnreturn h{ i[1,n] Ti }, {Ti | [1, n]}i;case 1 . . . nreturn h{Ti | [1, n]}, {Ti | [1, n]}i;case x : P (x)return h, {P (x)[x/d] [x/d] | D}i;case x :W[x]return h{ dD T[x/d] }, {T[x/d] [x/d] | D}i;case x : [x]return h{T[x/d] | D}, {T[x/d] [x/d] | D}i;VGrounding theory boils applying one_step_ground sentence(which copiesPTPTground part) rule theory repeatedly applyingone_step_ground returned rulesR (all returned sentences rules G ground).use ground refer algorithm overall process.1. Tseitin (1968) introduced symbols part normal form transformation.245fiDe Cat, Denecker, Stuckey & BruynoogheVarious improvements exist, returningreturning>/atoms interpretedconjunctions whenever false conjunct encountered (analogouslydisjunctions quantications).Also, algorithm one_step_ground introduces large number Tseitin symbols. Stateof-the-art grounding algorithms use number optimizations reduce numbersymbols. optimizations directly applicable techniques presentedpaper, start naive one_step_ground algorithm. Section 5, presentoptimized version one_step_ground introduces fewer Tseitin symbols henceresults smaller groundings.3. Lazy Grounding Lazy Model Expansionlazy grounding refer process partially grounding theorylazy model expansion (lazy MX) process interleaves lazy groundinguse termtermmodel expansion grounded part. Section 3.1, formalize frameworklazy model expansionFO(ID )theories; Section 3.2, formalize instanceframework basis current implementation; Section 3.3, illustrateoperation.3.1 Lazy Model Expansion FO(ID) TheoriesGiven canonical theory= {PT , }input structureIin ,models expandingIinsearched interleaving lazy grounding search already grounded part.rst focus lazy grounding.Apart initial step movesPTgrounded part, input stepconsists set rules still grounded, already grounded theory three-valuedstructure expansion initial input structure.subsequent grounding step replace non-ground rules ground rules mightintroduce new rules. Hence, state grounding includes setset(thedelayed denition )gground rules(possibly) non-ground rules. denitionsg (in follows abbreviated gd ) voc()-equivalentoriginal denition hence, gd total. grounding procedure guaranteethat, times, g total.Given partial structure Iin rule sets g , key idea behind lazymodel expansion (i) use search algorithm search model gexpansion Iin PT true; (ii) maintain justication J literalstrue dened justied gd J consistent ; (iii)interleave steps (i) (ii) move parts g literal denedpropertyneeds justied cannot justied.hg , , J, Iiyet grounded, justication J ,, g = , J empty graph.Thus, control lazy model expansion, suces maintain stateconsisting grounded rulesthree-valued structureI.g ,Initially,rulesIin ,Lazy model expansion searches spaceDenition 3.1 (Acceptable state).tenceacceptablestates.tuple hg , , J, Ii theory atomic senPT , total denition , input structure Iin acceptable state (i) gd , g246fiLazy Model Expansion: Interleaving Grounding Searchgd strongly voc()-equivalent , (ii) domain, (iii) J justication gd , (iv) expansionL literals true dened justied J , (vi) JL ,literals L, consistent .total denitionsatom denedvIin ,( ) setjusticationExample 3.2.gConsider theory{PT , },denitionPT T1 T2 T3 .x : Q(x).1T2 x : R(x).T3 x : Q(x).Letstructure{PT , T1 }(hence,T2T3unknown),gdenitions consisting rst rule remaining rules, respectively. Furthermore, letJ {T1 {Q(d) | D}}. tuple hg , , J, Ii acceptableT1 literal dened justied J .state. Indeed,already said, lazy model expansion algorithm starts initial state, = , J = , = Iin ,acceptable dened literals unknownstate, either renespropagation choice, backjumps.g =Iin .resultingstate unacceptable, repair operation restores acceptability; steps describedSection 3.2.gd .algorithm tries compute acceptable stateCorollary 2.11, would entail modelPTjustiedexists; computedeciently well-founded model computation. intermediate states, justicationmay non-totalPT ,iii),Note that, (justied.contain unfounded literals, inconsistent.justication mustgd .Indeed, assume literaljustication graph leaf denedgldepends positively negatively l. every attempt extend justication graphl gd might fail, e.g., forbiddencycle. Consider, e.g., denitions g = {P Q} = {Q P }. case,would correct take P justication Q true, even though validjustication within . Indeed, model exists justies Q full denition gd .total justication graph justiesProposition 3.3. Let hg , , J, Ii acceptable state.gd well-founded modelexpands literals true dened (delayed) denition .Proof.LLetset literals truejusties literalsexpandsL.denedopen literalsstate acceptable,JHence, Corollary 2.11, exists well-founded modelL.Example 3.4 (Continued Example 3.2).>.modelJ(i.e.,g , PTinterpreted randomly,well-founded evaluation, assigning{Q(d) | D}),derivesT1true.Moreover,also true well-founded model. NoteR-atomsoccurJ.following theorem states obtained expansion also model247T.RfiDe Cat, Denecker, Stuckey & BruynoogheTheorem 3.5. Let hg , , J, Ii acceptable state theory = (PT , ) inputstructure Iin PT true I|voc(g ) model g . existsmodel expands I|voc(g ) .Proof. I|voc(g )justicationJgmodelgdomain literals truecombine onefollows Proposition 2.10 existsI|voc(g ) .gconsiststwo justications:follows: dened literallgd ,JJJg .l,JgdenedJc (l) = Jg (l). Jc takes edges either Jdened literal, justication gd .verify Jc justies PT . First, total PT . Indeed, path PT eitherconsists literals dened g , branch total Jg g , passes0literal l dened , justied J according condition (v) hence(Jc )l0 = Jl0 total. such, PT cannot reach dened literal gdJc undened. Second, Jc contain unfounded literals starting PT .path PT either path Jg (so well-founded justies g )tail J (well-founded property (v)). Finally, set literals reachable PTJc consistent. Also see look paths Jc PT : rst followJg consists true literals , may get path J containsliterals consistent . case, impossible reach literalsetJc (l) = J(l);Jcg .justies every true dened literalotherwise, setnegation.follows Proposition 2.8 exists modelPTtrue. SincegdexpandsI|voc(g )gd strongly equivalent , proposition follows.achieved well-founded evalstarting two-valued open(gd )-Recall eectively computing modeluationgd ,polynomial data complexity,structure expandingI|voc(g )(Chen & Warren, 1996).theorem, requiredcompute two-valued modeljustication justiesPT .g .modelg .Actually, needsuces search partial structureSo, relax requirement expense alsomaintaining justications literals truedenedg .Corollary 3.6. Let hg , , J, Ii acceptable state theory= {PT , } inputstructure Iin PT true J justies PT gd . existsmodel expands I|S set dened literals JPT .g expanding Iin PT true implies lack modelsg model expanding Iin , unsatisablecore, i.e., set rules g model exists expands Iin . Hence, alsounsatisable core = (PT , ). nd unsatisable core, one can, example,Failure nd modelexpandingIin .Indeed,use techniques described Torlak, Chang, Jackson (2008).3.2 Practical Justication Management FO(ID) TheoriesRoughly speaking, lazy model expansion framework consists two components.one hand, standard model expansion algorithm operates{PT , g } and,gd lazilyhand, justication manager maintains justication248fiLazy Model Expansion: Interleaving Grounding Searchgrounds.Lazy model expansion performs search space acceptable statesaims reaching state Theorem 3.5 (or Corollary 3.6) applicable. avoid slowingsearch model expansion, work done justication managerlazy grounding must limited. achieve this, designed systemjustication manager access grounded denitiongneed restorestate search algorithm backtracks current structuremanager accessgparticular, literal denedI.justicationmaintains justications restricted.allowed direct justication. justicationmanager maintains following properties:Literals direct justications either opendirect justicationsstructureJgddened.kept consistent currentI.justication graph denedJunfounded literals total.distinguish acceptable states meet additional requirements acceptablestates dened Denition 3.1, callDenition 3.7(Default acceptable state).default acceptable states ; dene as:statehg , , J, Iidefault acceptablestate acceptable state and, addition, ( ) literals direct justications eitheropengddened,ii( )Jjusties set literalsJdened.follows default acceptable states satisfy two extra conditions: justifyliterals denedconsistent.terms literals denedg ,dened,consistent.Jtrueset literalsacceptable state, suces literalsJSince default acceptable states acceptable states,Theorem 3.5 Corollary 3.6 also hold default acceptable states.standard model expansion, main state-changing operations makeprecise (by making literals true, either choice propagation) makeless precise (by backjumping).model expansion modies',= hg , , J, Ii default acceptable state0new state hg , , J, necessarilydefault acceptable state. following propositions identify situations acceptabilitypreserved.Proposition 3.8. Let hg , , J, Ii default acceptable state, L set literals unknown0 consistent structure L. (i) literals L either deneddirect justication J (ii) direct justication J contains negationliteral L, hg , , J, 0 default acceptable state.Proof.literals true( ) literals trueJconsistentliterals trueI'I'I,direct justication, followsdirect justication. justicationsalso consistent '. Hence, J justiesdeneddenediithen, ( ),dened.Proposition 3.9. Let hg , , J, Ii default acceptable state. hg , , J, 00 <p default acceptable state.249fiDe Cat, Denecker, Stuckey & BruynoogheProof.I, JJjusticationjusties literals denedjusties literals denedtruedefault acceptable state, literals denedliterals denedhidden loopsg,.gd .trueI.I'subset'.gallowed direct justicationsrestriction quite limiting (see next section) avoidloops detected maintaining justicationcurrent implementation do. Several methods existl dened g allowed direct, provided established l's justication cannot loop gd .One case body rule l dened literals. step analyzedependency graph: literal dened g allowed direct justicationliteral dened provided literals belong strongly connectedextend class default acceptable states. Literalsjusticationscomponent dependency graph. case, cannot part cycle.3.3 Examplerest section, illustrate behavior lazy model expansion articialexample, constructed way main features illustrated. next section,processes involved described detail.focus operation justication manager interactionsolving process. manager activated unacceptable state, either solverfalsies literal occurs direct justicationJl denedl extendtrue literaljustied J . One option repair search justicationJ . general problem hard model expansion problem itself,Corollary 2.11. manager searchesextendJ,nd one, grounds l's denition movesexample uses theoryshownlocally direct justication justies lg .states symmetric graph (edge/2) existsR/1)= {d1 , . . . , dn }equality predicate identity relation (below omitted ). Predicates edge, Rroot interpreted; R root dened. particular, root denedsingleton {d1 }, specifying root d1 .least one node root node (predicateroot node. input structurePPTC1C2x : root(x)x : R(x)root/1)reachable (predicateinterprets domainC1 C2x : root(x) R(x)(x y) D2 : edge(x, y) edge(y, x)x = d1root(x) : edge(x, y) R(y)(1)(2)(3)(4)(5)lazy MX algorithm proceeds follows:1. initial default acceptable statehg , , J, Iig ,Jempty,= .2. Propagation{PT , g }sets{PT }.expands structureconditions Proposition 3.8 longer satised.250I,resulting statefiLazy Model Expansion: Interleaving Grounding Searchacceptable sinceJ.PTtrue deneddirect justicationJ direct justication PT .atom PT unique direct justication {C1 , C2 } extending Jrestore (default) acceptability since C1 , C2 direct justication JPT remains unjustied. Therefore, alternative taken rule (1) movedg . Now, default acceptable state obtained.One option repair acceptability extend{PT , C1 , C2 }. C1 C2 justied. ConsiderC2 rule (3). edge open, manager build direct justication{edge(d, d0 ) | (d, d0 ) D2 }, sets negative edge literals true, extends J(setting positive edge literals true would equally good). justies C2avoids grounding rule dening C2 .3. Unit propagation setsrst4. LiteralC1cannot justied (with local approach) since direct jus-tications contains unjustied dened literals.However, rule (2) existentiallyquantied, one avoid grounding whole rule performing Tseitin transformation isolate one instance ground instance. purposed1 :(root(d1 ) R(d1 ))(2a)x \ {d1 } : root(x) R(x) (2b)illustration, make (bad) choice instantiatingC1Rule (2a) movedgxdefault acceptable state reached.5. acceptable state propagation possible, choicemade.C1true, body rule (2a) become true. Preferablyselecting Tseitin (this would trigger grounding), rst disjunct selectedmodel expansion propagation extends structureliteralroot(d1 )denitiondeningrootroot{(d1 = d1 )}denedunique direct justicationroot(d1 ) R(d1 ).rule (4) cannot justied sincefalse. manager partially groundssplits ground rule (4a) non-ground rule (4b)domain elements:root(d1 ) d1 = d1 (4a)x \ {d1 } : root(x) x = d1 (4b)g . Note root(d1 ) justied {d1 = d1 } gd , henceroot(d1 ) direct justications . Whenever groundingRule (4a) movedsafe usedone, justication manager interrupted propagation, infer truthadditional literals, detect inconsistency (which result backjumping).cases, manager resume revision justication afterwards,acceptable state reached.unacceptable (due unjustiedHere, even though resulting state stillR(d1 )),groot(d1 ) conict.creation new rule (4a)interrupts manager. Propagation using new rule derives= {PT , C1 , C2 }, subsequent propagation sets structure{PT , C1 , C2 , root(d1 ), }. Still default acceptable state (T justied),backtrackingrule (2b) transformed split another instance.(root(d2 ) R(d2 )) T2(2ba)T2 x \ {d1 , d2 } : root(x) R(x) (2bb)251fiDe Cat, Denecker, Stuckey & Bruynoogheg ,Rule (2ba) movedrule (2bb) remains.state defaultacceptable.T2 , choosing rst disjunct rule (2ba)R(d2 ). literal root(d2 ) dened ,justied direct justication {(d2 = d1 )}. literal R(d2 ) cannot justieddirect justication (as edge literals false current justication graph)rule (5) transformed split instance d2 . Actually, instance6. Again, search avoids new Tseitinpropagatesroot(d2 )turn disjunctive body complex subformula, avoid groundingsubformula, break two parts introduce another Tseitin.R(d2 ) root(d2 ) T3(5aa)T3 : edge(d2 , y) R(y)(5ab)x \ {d2 } : R(x) root(x): edge(x, y) R(y)(5b)Rule (5aa) movedg ,others remain.{PT , C1 , C2 , root(d1 ), T, root(d2 ), R(d2 )}, hence propagation rule (5aa) g extends T3 . direct justication justifyingT3 and, hence, rule (5ab) partially grounded splitting d1 case:T3 (edge(d2 , d1 ) R(d1 )) T4(5aba)T4 \ {d1 } : edge(d2 , y) R(y) (5abb)7. current structureRule (5aba) movedgrule (5abb) remains8. search selects rst disjunctR(d1 ).it.literalExtendingJR(d1 )dened.T3 's rule body propagates edge(d2 , d1 ), {root(d1 )} direct justicationdirect justication yields acceptable defaultroot(d1 ) dened g . However, root(d1 ) justied gd ,J direct justication discussed earlier.justication manager faces new problem: true literal edge(d2 , d1 ) conict002direct justication {edge(d, ) | (d, ) } C2 (rule (3)). handleconict, splits aected instance (x = d2 , = d1 ) rule:C2 (edge(d2 , d1 ) edge(d1 , d2 )) T5(3a)T5 (x y) D2 \ {(d2 , d1 )} : edge(x, y) edge(y, x) (3b)acceptable state, sincemaking safe extendg rule (3b) remains . direct justication{edge(d, d0 ) | (d, d0 ) D2 \ {(d2 , d1 )}}, unaected part directjustication C2 . restores acceptability.Rule (3a) movedT5set9. Propagation rule (3a) extendsedge(d1 , d2 )T5 . literal edge(d1 , d2 ),T5 (rule (3b)). resolvetrue, conict direct justicationit, justication manager partially grounds rule (3b) splits instance{x = d1 , = d2 } follows.(3ba)T5 (edge(d1 , d2 ) edge(d2 , d1 )) T6T6 (x y) D2 \ {(d2 , d1 ), (d1 , d2 )} :edge(x, y) edge(y, x) (3bb)252fiLazy Model Expansion: Interleaving Grounding Searchg rule (3bb) remains ; T6 inherits directedge(d1 , d2 ) removed. Propagation rule (3ba) extendsstate acceptable, T6 dened justied.Rule (3ba) movedjusticationT6 .now,T5resultinggconsists rules (1), (2a), (4a), (2ba), (5aa), (5aba), (3a), (3ba),consists rules (4b), (2bb), (5b), (5abb), (3bb). cur{PT , C1 , C2 , root(d1 ), root(d2 ), edge(d2 , d1 ), edge(d1 , d2 ), R(d1 ), R(d2 ),T, T3 , T5 , T6 }, model PT g .literals, root(d2 ), R(d1 ) T6 dened . Literal root(d2 ), denedrule (4b) {(d2 = d1 )} direct justication. Literal R(d1 ), dened rule (5b),{root(d1)} direct justication. Literal T6 , dened rule (3bb) direct justicationset negative edge literals except edge(d1 , d2 ) edge(d2 , d1 ). obtainfull model theory, extended literals direct justications.residual denitionrent structurecase, assigns open literals model completed wellfounded model computationgd .Actually, done without groundingdenition (Jansen, Jorissen, & Janssens, 2013).4. Justication ManagementSection 3.2, instantiated general framework, developed Section 3.1,justication manager access.example Section 3.3, justi-cation constructed demand, i.e., time literal needed (dierent) directjustication, body dening rule analyzed justication extracted.failed, part rule grounded. calledimagineglobal approach,ruleslocal approach.One alsoconsidered attemptselect direct justications minimize grounding rules whole. Obviously,global approach time consuming, applied every timeadjustment justication required. section, describe approaches.describing algorithms, introduce notations assume normalizations done. function nnf reduces formula negation normal form.set single element, + used shorthands {s}S\{s}. J justication, denote J[l d] graph identical J except ljustied d. assume quantiers range single variable variable namesreused formula. Furthermore, assume basic reductions appliedformulas, e.g.,>reduces, x :reducest,...4.1 Local ApproachAlgorithm 2 shows top level lazy_mx model expansion algorithm, taking inputtheory{PT , }Iin . Denitions g initializedinitialized Iin . set ground sentences Tginitial justication J empty. auxiliary (FIFO)initialized empty.latter keeps track literals directinput structureempty denition, respectively,initialized factqueueqchPTjustication needs checked.main loop performs model expansionTg g ,interleaved workjustication manager towards establishing default acceptable state. model expansion253fiDe Cat, Denecker, Stuckey & Bruynooghepart consists propagation (the call propagate), test whether current stateinconsistent (with learning backjumping), test whether modelTg gfound (returning model justication) choice stepTg gselects literal unknownassigns value.Propagation returns literalsentailed ground theory (partial) structure, example applyingunit propagation unfounded/wellfoundedness propagation (Marin et al., 2008).test model performed default acceptable state (i.e., queueqchempty). test succeeds, ensures well-founded model computationexpand current structureextended direct justications literalsmodel whole theory. Also choice step takes place default acceptablestate; ensures search space limited state space default acceptablestates.justication manager activated propagation choice step assignsliteralvalid.l.lcalling check_literal, checked whether current justication remainsdenedqueueqchjustication, needs justication addedprocessing justication manager.l0justication another literal l , justication becomes inconsistentanother justication also addedqch .occursl0needsprocessing done selectingelements queue calling lazy_ground function.latter function rstattempts nd (dierent) consistent direct justication l; fails, splitsrule instance deninglpartially grounds it, hencegextended. newclauses may trigger propagation; therefore processing queued literals interleavedpropagation and, possibly, backtracking . Note backtracking might restoreconsistencydirect justicationJ(l)literallqch .4.1.1 Lazy Grounding One Rulefunction lazy_ground, Algorithm 3, checks whether literaltion; not, simply returns. Otherwise, checks whetherllneeds direct justica-valid justication, i.e.,one satises invariants detailed below. so, also returns; otherwise, passesrule body used construct justication (the negation dening ruleliteral negative) build_djust, function attempts nd valid directjustication. Besides literal rule body, also initial justication, derivedrule body, passed build_djust. latter function successful, justicationupdated lazy_ground done; not, direct justication literalfalselsetsplit_and_ground called ground part rule dening l.going details, rst analyze properties want maintaincurrent justicationconsidered partJJ.direct justications literalsqch queueJ are:since might invalid. global invariantsliterals unfoundedset literalsJ(recall, negative cycles allowed),Jconsistent.direct justication= J(l) Jlqueue, invariants lazygrounding process are:contains literals denedg(unless literal safely justieddiscussed before),254gd ,fiLazy Model Expansion: Interleaving Grounding SearchAlgorithm 2: lazy_mx lazy model expansion algorithm.1 Function lazy_mx (atomic sentence PT , denition , structure Iin )Output: either model g J false2345678910111213141516171819202122Tg:= {PT };gtrue:=;:=; J:=;:=Iin ; qch:=L := propagate(Tg g , );:= L;foreach l L qch :=check_literal(l,qch );inconsistentTg += learn_nogood(I , Tg );conict root level return false ;:= state backjump point;else qch empty(l, qch ) := dequeue(qch );lazy_ground(l);else model Tg greturn , J ;elseselect choice literal l;:=+ l;qch :=check_literal(l,qch );Function check_literal (literal l, literal queue qch )Data: global J Output: updated queuel dened J(l) = undef qch := enqueue(l,qch ) ;foreach l0 l J(l0 ) qch := enqueue(l0 ,qch ) ;return qch ;Algorithm 3: lazy grounding literal.1 Function lazy_ground (literal l)Data: global , J2l l dened3J(l) exists obeys invariants return;4else567891011;:= body rule dening l;l negative literal:= nnf ()dj := build_djust(l, , init_just(l));dj 6= false J := J[l dj]; return;elseJ=J[l false];split_and_ground(l);255;fiDe Cat, Denecker, Stuckey & Bruynoogheliteralsqueueqch .denedeither direct justicationJbelongqchinvariants imply default acceptable state reachedqueueempty. Indeed, follows invariants current justication totalsituation hence literals direct justication justied (Denition 2.6).Due policy followed queue literals, current justication also consistentliterals truedenedjustication, hencehg , , J, Iidefault acceptable state.4.1.2 Building Direct Justificationpurpose build_djust, Algorithm 4, extendliterall.denedI.formulalJsuitable direct justicationliteralJ(l)currently undenedinconsistentrecursive function takes three parameters: ( ) literall,made true direct justication (initially whole bodyii( )rule dening literal; note initialization takes negation ruleiii) description direct justication derived far, initializedliteral negative), (init_just(l). algorithm, assumedierent quantiers rangedierent variables.going details, discuss represent direct justications.Basically,could represent direct justication set ground literals. However, setquite large using ground representation might hence defy purpose lazyhL, Bi L setB set bindings xi Di xi variable Didomain. set bindings B = {x1 D1 , . . . , xn Dn } represents set variablesubstitutions SB = {{x1 /d1 , . . . , xn /dn } | di Di [1, n]}. set groundliterals represented hL, Bi {l | l L SB }. direct justicationliteral P (d1 , . . . , dn ), dened rule x : P (x) , initialized init_just(l)h, {x1 {d1 }, . . . , xn {dn }}i. eect, B allows identify relevant rule instantiationgrounding.Instead, represent direct justication pairpossibly non-ground literalsproviding appropriate variable instantiation domains, set literalsempty.build_djust algorithm searches set literals makingrecursively calling subformulaslarger justication.true.workscomposing results afterwardsset literals found, example noneexists consistent direct justications,falsereturned.base case formula literal. make literal true, instancesliteral set bindingsBmust true, hence, set literalsLextendedliteral itself. resulting direct justication satisfy invariants,checked call valid: returnstrue call valid(l, dj) djl J[l dj] satises(part ) direct justicationsatises invariantsinvariantsjustication.universally quantied formulaquantied variable.x D.x :true instanceHence, recursive call, set bindingsBextendedexistentially quantied formula, suces one instance true. Hence,minimal approach try instance separately one succeeds; fail,256falsefiLazy Model Expansion: Interleaving Grounding SearchAlgorithm 4: build_djust algorithm.1 Function build_djust (literal l, formula justication hL, Bi)Input: B binds free variablesOutput: either direct justication false2switch3case literal4valid(l, hL {}, Bi) return hL {}, Bi;5else return false ;6case x D0 :7return build_djust(l, , hL, B {x D0 }i);8case x D0 :9large(D0 )10return build_djust(l, , hL, B {x D0 }i);11foreach di D01213hL0 , B 0 := build_djust(l, , hL, B {x {di }}i);hL0 , B 0 =6 false return hL0 , B 0 i;141516171819return false ;case 1 . . . nforeach [1, n]2021222324return hL, Bi;case 1 . . . nforeach [1, n]25hL0 , B 0 := build_djust(l, , hL, Bi);hL0 , B 0 = false return false ;else hL, Bi := hL0 , B 0 i;hL0 , B 0 := build_djust(l, , hL, Bi);hL0 , B 0 =6 false return hL0 , B 0 i;return false ;257fiDe Cat, Denecker, Stuckey & Bruynooghereturned. Note however want iterate domain elementlarge, would similar constructing grounding itself. Instead,extend bindingx D.large,Conjunction similar universal quantication, exceptexplicit iteration conjunct needed. soon one conjunct fails, wholeconjunction fails. Disjunction similar existential quantication small domain.Note build_djust non-deterministic due choices domain element justifyexistentially quantied formula, disjunct justify disjunction.Example 4.1.Consider following rule large domainD.H x : P (x) (y : Q(x, y) R(x, y))AssumeJ emptyh{P (x)}, {x D}iloops keep track of.Applying build_djustP (x) body chosen.corresponds direct justication {P (x) | x D}. Alternatively, second disjunct chosen, returns h{Q(x, y), R(x, y)}, {x D, D}i, represents directjustication {Q(x, y) | x D, D} {R(x, y) | x D, D}.Hreturnsrst disjunct4.1.3 Partially Grounding Rulelast bit lazy model expansion algorithm handles case justicationfound denition literallgrounded.straightforward waywould call one_step_ground rule dening l, store resultg.However, many cases operation results much grounding.Example 4.2.form x : P (x) situationP (d). Applying one_step_ground r1 would instantiatex elements D, resulting |D| rules, fact suces split r1 two rules,one instance x = one remainder. Another example applies rule r2form H x : Q(x) R(x) direct justication J(H) = {Q(x) | x D}.Q(d) becomes false, justication manager may need ground rule. Applyingone_step_ground would instantiate universally quantied x elements .Instead, better split instance x = introduce Tseitinremainder, producing H (Q(d) R(d)) g x : Q(x) R(x). direct justication obtained incrementally removing Q(d)H , discussed Section 4.1.5.Consider ruler1justication found atomsplit_and_ground algorithm (Algorithm 5) ground part rule deninggiven literaldeningll,sayP .rst step split rule instance rulegrounded (the call split).denesP .Letreplace ruleadditionally return ruleP [x/d].x : P (x) rulex : P (x)Afterwards, apply one_step_groundlatter rule add computed rules eitherg. 2result split_and_ground denitionone. limit ground denitionequivalentgdgd ground previousempty g strongly voc()-.2. Recall, head new grounded rule always dierent head already grounded rules.258fiLazy Model Expansion: Interleaving Grounding SearchAlgorithm 5: split_and_ground algorithm.1 Function split_and_ground (literal l)Input: l denedResult: update g , , J , qch23r := split(l); // split updates(0g , 0d ) := one_step_ground(r);g = g '; = ';4Even justication found, better splittinglapplyingone_step_ground, shown Example 4.2. First, splitting made signicantlyintelligent, discussed Section 4.1.5. Second, improve one_step_groundground part expressions possible, describe below.Improving one_step_ground.subformulas/instantiationsresult consists|D|l iteratesx : P (x),Applying one_step_ground rule.examplesentencenew rules many new Tseitin symbols. Instead, dependingvalue l, sucient introduce one (or some) subformulas, shownAlgorithm 6, extends switch statement one_step_ground two higherpriority cases.ltrue, sucient ground one disjunct/existential instantiationdelay rest Tseitin introduction.lfalse, take similar approachconjunction/universal quantication.Algorithm 6: Additional cases one_step_ground algorithm.1 switch r2case l 1 . . . n I(l) =345678choose[1, n];return h{l }, {Tcase l x : I(l) =Wchoosej{1...n}i j }i;D;return h{l [x/d] }, {Tanalogous casesx : }i;combination I(l) = f .4.1.4 Algorithmic PropertiesCorrectness termination presented algorithms discussed following theorem.Theorem 4.3 (Correctness termination). lazy_mx returns interpretation ,expanding literals direct justications J , applying well-founded evaluation gd restricting voc(T ) results model . algorithm returnsfalse , interpretation exists precise Iin satises .259fiDe Cat, Denecker, Stuckey & BruynoogheAlgorithm lazy_mx terminates nite domain D. Otherwise, terminationpossible guaranteed.3Proof.lazy_mx returns interpretationI,modelgqchempty. Givenproperties split_and_ground, applying lazy_ground literal l, eithervalid justication denedstate and, Theorem 3.5,returnsfalse ,g .gdqchempty, default acceptableexpanded model whole theory. lazy_mxprovenmodelsHencelhencegmodelsIin .case, alsoIin .also models expandingWithout calls lazy_ground, search algorithm terminates nitetion lazy_ground produces ever-increasing ground theorylimit. Hence, lazy_mx always terminatesnite.gg ; func-full groundinginnite, limitginnite grounding, termination cannot guaranteed.4.1.5 Symbolic Justifications, Incremental Querying Splittingalgorithms presented sound complete.However,improved taking formulas justications derived account.Symbolic justications incremental querying.(subformulas ) formulaExample 4.4.,multiple justications existgrounding delayed.Consider formulax : P (x) Q(x),h{P (x)}, {x D}ih{Q(x)}, {x D}i justications. that, could derive justication:D, make either P (d) Q(d) true. Hence, grounding necessaryP (d) Q(d) become false d.automatically changing build_djust follows:algorithm allowed select multiple disjunctions / existential quanticationseven valid justication already found one (Lines 13 24).build_djust longer returns justication, symbolicjustication formulaentails original formula. formula built build_djust reectssubformulas/instantiations selected.,justication.{P (x) | xx : P (x) Q(x).derived directly set non-false literals (full) groundingexample, formulaD},x : P (x) Q(x),instead justicationbuild_djust might return justication formulavalidity check (valid) extended return false justication formula false.allowing complex formulas (instead conjunction universally quantied literals), validity check whether formula become false incremental changesexpensive. factincremental queryproblem. exper-iments, limit depth allowed formulas use straightforward (expensive)algorithm evaluates whole formula whenever assignment falsify it.3. possible change integrationlazy_ground lazy_mx guarantee termination nitemodel exists, see Section 5.1.260fiLazy Model Expansion: Interleaving Grounding SearchBody splitting.described Algorithm 3 Section 4.1.3, split simply splitsrule instance denes l, one_step_ground grounds rule instance step step,accordance structure formula. However, grounding triggeredconict current justication, one_step_ground blind originconict.using conicting literals, one could focus grounding partformula contributes conict. One restructuring rule partgrounded, contains conict, part grounded,old justication adjusted still apply.latter part splitintroducing new Tseitins transformation called body splitting. approachinserted Algorithm 3 call split. this, original justication (calljold )passed extra argument split_and_ground.Example 4.5.h x : P (x); let h{P (x)}, {x D}i justication h true . P (d) becomes false, easy see splitviolating instantiation rewriting original rule h P (d) addingrule x : P (x). Crucially, justication second part derivedoriginal justication, namely h{P (x)}, {x d}i. second part henceadded justication J rst part added g .Consider ruler direct justication jold done ecient way.v true domain literal partial structure direct justicationrule r contains negation v . implementation binding(s)justication instantiates v extracted representationdirect justication rule. simplicity, assume {x = d1 , . . . , dn } singleinstance. recursive algorithm visits formula body r depth-rst. Wheneverquantication x : encountered x equal xj x, replaced(x dj : ) [x = dj ]. Tseitin transformation applied left-handrevision ruleAssumeconjunct algorithm recurses right-hand conjunct remainsbinding. new rule dening new Tseitin jold v direct justication. Similarly,existential quantication replaced disjunction.result set new rulesnew justication sought smaller ruler0passedone_step_ground. Correctness follows fact jold v valid justication, nonenew rules containsExample 4.6.v,correctness Tseitin transformation.Example 4.1, justications soughtHruleH x : P (x) (y : Q(x, y) R(x, y)).J = {Q(x, y) | x D, D} {R(x, y) | x, l = Q(d1 , d2 ) becomes false, J longerconsistent cannot repaired. J l, however, still consistent ,justication whole body. hand, J l justicationsubformula P (x) : Q(x, y) R(x, y) instantiation x dierentd1 . Consequently, split quantication x : x d1 x = d1Assume selected justicationD, D}.P (d1 )trueapply Tseitin transformation former. Afterwards, recursively visit latterformula apply similar reasoning existential quantication. operations261fiDe Cat, Denecker, Stuckey & Bruynooghe(split 1)x d1.P (x)Q(x, y)(split 2)P (d1 )0 d2.R(x, y)Q(d1 , 0 )R(d1 , 0 ) Q(d1 , d2 ) R(d1 , d2 )x : P (x) : Q(x, y) R(x, y) split violatingQ(d1 , d2 ). original justication without Q(d1 , d2 ) justicationFigure 2: rule bodyliteralleft-hand side splits, justication formula shown blue.remaining non-justied formula shown red.formula illustrated Figure 2. result consists following rules,ruleHeven ground.H T1 (P (d1 ) (T2 (Q(d1 , d2 ) R(d1 , d2 )))).T1 x d1 : P (x) : Q(x, y) R(x, y).: Q(d , y) R(d , y).2211optimize traversal formulapath taken parse treeExample 4.7.,build_djust extended storedirect justications subformulas.C1 (x y) D2 : edge(x, y) edge(y, x)justied, J empty interpret edge. build_djust algorithm recursivelyvisits body rule edge(x, y) returned valid literal use. Goingone level, store edge(x, y) edge(y, x), selected {edge(x, y)}. Assumingdisjuncts selected, edge(x, y) returned again. Going backquantications, store that, quantications, selected set relevant2domain elements, build_djust returns justication formula (x y) : edge(x, y).Assume rulebuild_djust given accessdirect justication.jold ,similar optimizations possible repairingConsider Example 4.6, assumeP (d1 )unknownI.case, left branch Figure 2 also transformed rule still valid,P (d1 ) directjustication rule H T1 (P (d1 ) (y (D d1 ) : Q(d1 , y) R(d1 , y))),T1 Example 4.6.direct justication. right branch, repair select disjunct4.2 Global ApproachFinding justications using greedy local approach easily lead groundingnecessary. Consider example sentences262x : P (x)x : P (x) Q(x).fiLazy Model Expansion: Interleaving Grounding SearchApplying local approach second sentence rst (with emptyconstruction makes atomsPJ ),might resultfalse. applying local approachrst sentence nds valid justication it; fully grounded. globalapproach takes set rules input tries select direct justicationsexpectedgrounding size whole set minimal.cast task, calledoptimal justication problem, problem graphrule nodes R justication nodes J .follows. graph consists two types nodes,justication node symbolic set literals representing possible justication (for). rule node pair(t, f , u); pair hr, ti rule r head lexpresses exists direct justication l; pair hr, f exists directjustication l pair hr, ui r justication.literals dened rulehr, tirrule,given current partial structuretruth valuethree types edges:Valid edgesrule node(the negation ) headConict edgeshr, ti (hr, f i) justication node jjjustiesr.ii( ) rule nodes rule dierent truth value, ( )iii) rule node hr, ti (hr, f i)l (l), (iv) rule node hr, ui,justication nodes contain opposite literals, (rdenes(orrl)l,justication node containsdenesl,justication node containsl(a conictlhr, ti (hr, f i)jneeds justication).Depends-on edgesjustication nodecontains negative (positive) literals denedaim select subsetslRsel Rjr.Jsel Jrule nodeselected rule node connected valid edge least one selected justication node.conict edges exist pairs selected nodes.Neither positive mixed cycles exist subgraph consisting validdepends-on edges selected nodes.selection{Rsel , Jsel }extracted follows.rulersatisfying constraints, initial justicationliteralhr, ti (hr, f i)l (l)selected rule. direct justication unionjustications justication nodesedge.Jgiven direct justication denedJselconnectedhr, ti (hr, f i)validMoreover, literals dened rules rule node selectedadded initialqchqueue, handled local approach, complete solutionmust justication them.hr, uiselected, means groundinginstances rule delayed literals denes become assigned.type problem somewhat related NP-hardhitting set(orset cover )problem (Karp, 1972): given set top bottom nodes edges them,task nd minimal set bottom nodes top node edgeleast one selected bottom node.263fiDe Cat, Denecker, Stuckey & Bruynooghehg , , J, Ii, input optimal justication, node constructedtruth value known ) also conictGiven default acceptable stateproblem generated follows. rulethree truth values (only oneedges added.Valid edges justication nodes obtained using (straightforward)adaptation build_djust returns set possible justications make headrule true (false). E.g., ruleP (x)x : P (x) , build_djust called literal{x D}. Conict depends-on edges derivedbinding initializedchecking dependencies justications rules justications.keep ecient, done symbolic level.Example 4.8.PT , C1C2x : root(x) R(x)(x y) D2 : edge(x, y) edge(y, x)x = d1root(x) : edge(x, y) R(y)(2)(3)(4)(5)Consider theory running example,propagated true. DenitionC1C2x:root(x)x : R(x)associated optimal construction set input shown Figure 3. Note ruleC1 C2 true ,root(x) root(x)nodes, use dened head literals identify rule. Literalhence one rule node rules (2) (3). Neitherjustiedx D,hencehroot, uituple.four solutions subset-maximal respect rule nodes, namelyfollowing rule node selections:{hR, ui , hroot, ui , hC2 , ti}(a){hR, f , hC2 , ti}(b){hR, ti , hC2 , ti}(c){hC1 , ti , hC2 , ti}(d)these, multiple justication selections possible (also shown Figure 3).C1 ,select justication(iv),C2choose(v)(vi)(butboth).objective maximize number selected rule nodes, minimizeexpected grounding size.obtain estimate expected grounding size,following conditions taken account:depend size grounding rule.Assigning multiple justications rule result lower estimate rulegrounded false.Variables occurring multiple justications result less matching instantiations.practical applications, number false atoms far exceeds numbertrue ones model. Hence, positive literals justication highercost negative ones.264fiLazy Model Expansion: Interleaving Grounding SearchhR, fh{root(x), edge(x, y)}, {x D, D}ihR, uih{root(x), R(x)}, {x D}ihR, tih{root(x)}, {x D}ihC1 , tih{root(x), R(x)}, {x D}ihC2 , tih{edge(x, y)}, {x D, D}ihroot, uih{edge(x, y)}, {x D, D}i(i)(ii))(iii)(iv)(v)(vi)Figure 3: graph part input optimal justication problem Example 4.8.Rule nodes shown left, justication nodes right;valid edges shown green, conict edges red depends-on edges blue.readability, conicts justications unknown rule nodesshown.approximate expected grounding size functioninput ruler(with headh),J.takesnselected type justication (rule selected ( ),justication (u), justicationcationsexpsizeh (t)justicationh (f ))set justi-function returns expected grounding size rule (size(r), denedbelow) weighted depending type justication. weights derived twoestimates:pvalptrprobability atom become assignedprobabilitynassigned atom true. Hence, dened formally below, non-delayed rules ( ),ufull size used; rule without justication ( ) weighted( ) weightedptr ,product justicationsnamelyptliteralsffalse one ( )jJ.1 ptr ;true justicationlatter two weights multipliedfactor product sum two terms,times number negative literalsj.pval ;j1 ptrtimes number positiveeect expected size decreases number justicationsincreases expected size increases justication literals.expsize (r, n, ) = size(r)expsize (r, u, ) = size(r) pvalexpsize (r, f , J) = size(r) pval (1 ptr )((1 ptr ) |pos.lits.j|jJ+ ptr |neg. lits. j|)expsize (r, t, J) = size(r) pval ptr((1 ptr ) |pos. lits. j|jJ+ ptr |neg.probabilities, assumedpvallots literals get value,lits.j|)small (currently 0.1) reect hopeptrless half, reect atoms265fiDe Cat, Denecker, Stuckey & Bruynoogheoften assigned false true. functionsizedened below. function returnsnumber atoms grounding rule formula, except existential quanticationdisjunction. these, take account grounded partiallyusing Tseitin transformation, taking logarithm total grounding size.size(L) = 1size(L ) = size() + 1size(x : ) = size()Xsize(i )size(1 . . . n ) =i[1,n]size(x : ) = log(D) size()Pi[1,n] size(i )size(1 . . . n ) = log(n)nSolutions optimal justication problem minimize termXexpsize (r, t(r), J(r))rdt(r)type (t,f,u)J(r)justication literal denedr.Example 4.9 (Continued Example 4.8). size rule C1 1+log(D)2, C21+D2 log(2), root D(1+1), R D(1+log(2)(1+log(D)2)/2.Consider assigning justication (iv) C1 : results expected cost rule(1 + log(D) 2) 0.3 1 (as construction relies making R true). Additionally,would force grounding rule dening R, increasing cost sizerule R. optimal solution problem Figure 3 rule node selection (a)2justication (vi) C2 . cost sum (1 + log(2) 0.3 (for justication(vi)) 1 + log(D) 2 (the expected size rule C1 ). Now, rule C1passed local approach.solve optimal justication problem,IDP'soptimization inference applied(meta-level) declarative specication task.4larger theoriesT,problemturns quite hard, two approximations considered reduce searchspace.First, number selected justications rule limited 2.valuessize(r)grow quite large, approximationstandard approach). Rounding integer values applieddlog(size(r))eIDP'sSecond,used (asupport oatingpoint number still preliminary. resulting specication could solved optimalitywithin seconds tested theories. lazy model expansion, global approachapplied initial phase Tseitin literals representing sentences originaltheory propagated true.5. Heuristics Inference Taskssection discusses tune lazy grounding search heuristicsunderlying SAT solver obtain eective implementation lazy model expansion.4. specication partIDP's public distribution.266fiLazy Model Expansion: Interleaving Grounding Searchalso describe inferences tasks beyond model expansion usefulcontext lazy grounding. less important issues discussed Appendix A.5.1 HeuristicsHeuristics play important role lazy grounding algorithms, serve ndright balance much ground long search. rst discussheuristics chosen. Afterwards, discuss alternative approach minimizegrounding.5.1.1 Balance Grounding Searchalgorithms leave room number heuristic choices importanteect performance.briey discuss choices.guidelinedecisions, following principles used:Avoid leaving search process without enough information make informeddecision; example, avoid losing much (unit) propagation introducingmany Tseitin symbols.Prevent creating grounding large; may example happenresult long propagate-ground sequence.Recall, goal create minimal grounding, solve model expansion problemsavoiding large grounding.Below, introduce number parameters aect heuristics.exactvalues used experimental evaluation parameters introduced speciedAppendix A.split_and_ground, handling disjunction existential quantication,choice many disjuncts expand. expand one instantiation timeruleh x : P (x),done Algorithm 6 (lines 3 6), iterative application resultsground theoryh P (d1 ) T1T1 P (d2 ) T2T2 P (d3 ) T3...Tn x \ {d1 , d2 , . . . , dn } : P (x).SAT-solver MiniSAT, usedP (di )IDPsystem, initially assignsatoms; choice triggers iteration propagation grounding.fresulting thrashing behavior reduced somewhat, grounding compactgrounding introducesndisjuncts time:h P (d1 ) . . . P (dn )x \ {d1 , d2 , . . . , dn } : P (x).267fiDe Cat, Denecker, Stuckey & Bruynoogheremedy this, two search-related heuristics changed. First, initial truthvalue randomized, favoring false (as models, many atoms falsetrue). Second, search algorithms typicallyrestart(ever-increasing) thresholdnumber conicts, sometimes caching truth value assigned atoms (polarity caching ).allows solver take learned information account search heuristicstaying approximately part search space.case lazy grounding,might want jump another part search space come across longpropagate-ground sequences. end, introduce conceptrandomized restarts,take place (ever-increasing) threshold number timesgextendedrandomly ipping cached truth values.addition, build_djust always returnsfalseestimated formulasmall grounding. Indeed, grounding formulas help search. Whether formulaconsidered small determined terms (estimated) grounding size.strategy used split_and_ground:whenever formula one_step_groundwould applied small, ground applied instead, completely ground formula.5.1.2 Late GroundingGrounding applied search process soon unit propagation taken place.result focus current location search space, dangergrounding much solution part space. Alternatively, couldapply opposite strategy, namely ground late possible: apply additionalgrounding search algorithm terminates without ever found modelacceptable default state. strategy well-known elds incremental provingplanning, domain (number time steps) increased searchprevious, smaller bound nished. guarantees minimal grounding. prototypestrategy implementedIDPgood results planning problems.5.2 Related Inference Tasksbulk paper focuses model expansion (MX)solutions structures two-valuedvoc(T ).FO(ID )theoriesT,Often, one interestedsmall subset symbols voc(T ). example case model generationSO(ID), language extends FO(ID) existential quantication relations.SO(ID ) problem P1 , . . . , Pn : initial structure , relation symbols P1 , . . . ,Pn , FO(ID) theory, solved model generation FO(ID) theoryinitial structure dropping interpretation symbols P1 , . . . , Pnmodels. Another example query evaluation FO(ID ): given theory , initialstructure formula free variables x (all FO(ID )), purpose evaluatingquery hT , I, nd assignments domain elements x modelexists expands [x/d] true. solve model expansionFO(ID ), new predicate symbol introduced answers query tuplesdomain elements true model theory extendedsentence x : (x) denition {x : (x) }.cases, approaches using (standard) model expansion compute total interpretation afterwards drop unnecessary information, quite inecient. Lazy model268fiLazy Model Expansion: Interleaving Grounding Searchexpansion save lot work partially grounding theory. However,model found grounded part, justications remaining denitionsused expand structure model full theory.Although expansionobtained polynomial time, still inecient afterwards large part modeldropped.remedy this, dene variant model expansion task, denotedT,restrictedMX.additional list symbols O,5called output symbols. Solutions structures two-valued symbolsexpansion exists extends model . Adapting lazyRestricted MX takes input theorystructuregrounding solve restricted MX done analysis justicationsneed added (completely) structure, splittinggdmultiple denitionsevaluating dening output symbols symbols depend (usingstratication argument).above-mentioned inference tasks cast trivially restricted MX problemslazy restricted MX greatly improves eciency respect ground-and-solve,shown experimental section.extensionFO(ID )procedurally interpretedsymbols (De Cat et al., 2014)provides another class interesting problems. predicate symbols xed interpretation, know whether tuple belongs predicate, procedural functionexecuted. approach provides clean way combine declarative procedural specications. Consider example symbolisP rime(N)interpretedprocedure executes ecient prime-verication algorithm returns truegiven argument prime. generally interested complete interpretationisP rime, cast restricted MX problem isP rime O.Solvingproblem using lazy grounding benet executing associated functionsearch relevant atomsisP rime(d).Also task, show experimentalevaluation next section.6. ExperimentsIDPsystem state-of-the-art model expansion engine, observedprevious Answer-Set Programming competitions (Denecker et al., 2009; Calimeri et al., 2014;Alviano et al., 2013). lazy model expansion algorithms presented paperimplementedIDPsystem, extending existing algorithms (De Cat, Bogaerts,Devriendt, & Denecker, 2013).current implementation incomplete sense cycle check justications implemented yet. aects inductive denitions non-inductiveones replaced FO formulas completion.workaroundlack cycle check, build_djust, function constructs direct justication, returnsfalse rules dening inductive predicates. consequence, instance ruleimmediately grounded, although lazily, domain atom dened rule assigned value. Another consequence inductively dened predicates cannot usedjustications rules. aects three benchmarks ASP competition (de5. Within ASP community, sometimes referred show predicates.269fiDe Cat, Denecker, Stuckey & Bruynooghescribed Section 6.2), namelyReachability, SokobanLabyrinth.these,grounding might delayed even complete implementation.section organized follows. Section 6.1, evaluate overhead completelygrounding theory using presented approach. Section 6.2, evaluate eectlazy grounding number benchmarks ASP competition.Section 6.3,number additional properties presented algorithms demonstrated.tested three dierent setups:(referredg&s), IDPIDPstandard ground-and-solve approachlazy model expansion (lazy) award-winning ASPsystem Gringo-Clasp (ASP). usedIDPversion 3.2.1-lazy, Gringo 3.0.5 Clasp 2.1.2-st.parameters lazy grounding algorithms discussed Section 5.1, valuesused experiments documented Appendix A. experiments Sections 6.16.3 run 64-bit Ubuntu 13.10 system quad-core 2.53 GHz processor8 GB RAM. Experiments Section 6.2 run 64-bit Ubuntu 12.10 system24-core 2.40-Ghz processor 128 GB RAM. timeout 1000 secondsmemory limit 3 GB used; out-of-time indicatedT,out-of-memoryM.66.1 Eect Grounding TimeLazy grounding may reduce grounding size time also causes overhead. instance,expect (naive) incremental querying justications costly discussed previously. aim section quantify overhead caused lazy grounding.experiments compare grounding time standard IDP systemnaiveinstance lazy grounding algorithm forced generate completegrounding starting search. instance obtained standard algorithm using small changes: shortcut ground small formulas turnedo, disjuncts instances existentially quantied formulas grounded one one,dened literal enqueued lazy grounding soon appearsg .comparison,also measure cost standard lazy grounding algorithm computes partialgroundings.devised six benchmarks test various aspects novel algorithm. benchmark simple theory two sentences simple solve. benchmarksdesigned measure cost dierent aspects lazy grounding: delaying resuming grounding, querying needed resume grounding, splitting formulas, etc.Specically, tested aspects following:1. Overhead delaying resuming grounding case existential quantierlarge domain.sentencen disjuncts; naive lazyn 2 Tseitin symbols.clauseintroducesx : P (x).Standard grounding creates singlegrounding grounds formula piece piece2. Overhead case inductive denition,{x : P (x) P (x) Q(x)}.standard grounding naive lazy grounding construct ground ruleP (d)atom.6. Benchmarks, experimental data complete results availablekrr/experiments/lazygrounding/jair.270http://dtai.cs.kuleuven.be/fiLazy Model Expansion: Interleaving Grounding Search3. Overhead case universal quantication.standard grounding createsnsentencex : P (x).atomic formulas, naive lazy grounding splits oneinstance time introducesn2Tseitin symbols.4. Lifted Unit Propagation (LUP) (Wittocx et al., 2010, 2013) important preprocessing step reduce grounding size. Concretely, applying LUP rulesx : R(x)x : R(x) : P (x, y)derives second formula follows rst hence needgrounded all. theory used check whether LUP remains equally importantsystem lazy grounding.x :R(x) : P (x, y). Standard grounding creates formula instancex Tseitin grounding : P (d, y). Naive lazy grounding createsextra Tseitin instance x extra set Tseitins piece piecegrounding subformula : P (d, y).5. Overhead case nested universal quantication. sentence form6. Overhead incremental querying case symbolic justication validated. sentencex : R(x) S(x),identical justication formula.formula validated checking falsity queryquery re-evaluated timeR-atom-atomx : R(x) S(x).falsied.6.1.1 ResultsExperiments done predicatesPQarity 3Rarity 2,domains size 10, 20, 30, 40 50. None predicates symbols interpretedstructure.experiments, overhead time required solve initial optimizationproblem (for global approach) always around 0.02 seconds, negligible.results rst three experiments shown dierences standardgrounding naive lazy grounding negligible.expected experiment 2,experiments 1 3, shows actual implementation eliminates overheadTseitins quantiers nested. three experiments, standard lazygrounding able justify formulas without grounding hence fast almostinsensitive domain size. shown Figure 4, dierence standardgrounding naive lazy grounding experiment 4. cases, use LUPbig impact size grounding hence time. experiment 1 3showed top level quantier create overhead lazy grounding, experiment 5shows hold anymore nested quantiers naive lazy groundingsubstantial overhead compared standard grounding. Note overheadworst case.Tseitins justied, denitions grounded,explains normal lazy grounding faster standard grounding insensitivedomain size.Experiment 6 shows complex justication formula causessignicant overhead naive lazy grounding. Also here, overhead worst case271fiDe Cat, Denecker, Stuckey & Bruynooghe4. Grounding bounds5. Nested universal quantification166. Complex justification, shared variables4.514ground without LUPground LUPnaive lazy ground without LUPnaive lazy ground LUPlazy ground LUP1412groundnaive lazy-groundlazy-ground12groundnaive lazy-groundlazy-ground4.03.5103.08SecondsSecondsSeconds10862.52.061.5441.02200010203040500.50.0010Domain size2030Domain size405001020304050Domain sizeFigure 4: Time overhead naive lazy grounding ground-and-solve completelygrounding input theory, benchmarks 4, 5 6. time includes grounding, solving time needed nd justications. time requiredstandard lazy grounding algorithm also shown comparison.visible normal lazy grounding. Still, important part future research reduceoverhead incremental querying complex justication formulas.6.2 ASP Competition BenchmarksSecond, selected benchmarks previous ASP competitions evaluate lazygrounding algorithm realistic setting. Many benchmarks solutions competition carefully ne tuned speed minimal grounding. Lazy grounding usuallyunable substantially reduce grounding theories and, due overhead,slower standard ground solve. reason, sometimes selectedmodelings benchmarks natural less optimized time grounding size. justify ground aim work improve inferencedeclarativemodeling(De Cat et al., 2014), emphasis developing intricateencodings, modeling problem close natural language specication.selected following problems (see competition websites complete descriptions). consist problems known hard, order evaluate eectlazy model expansion search, problems typically result large grounding.Reachability:Given directed graph, determine whether path exists twogiven nodes.Labyrinth:planning problem agent traverses graph movingconnected nodes reach given goal node. addition, graph manipulatedchange connectedness.Packing:Given rectangle number squares, squares gridwithout overlaps.Disjunctive Scheduling:Schedule number actions given earliest startlatest end time additional constraints precedence disjointness.272fiLazy Model Expansion: Interleaving Grounding Search# inst.# solvedg&sbenchmarkSokobanDisj. Sched.PackingLabyrinthReachabilityStable Marr.Graph Col.5044215lazy25504421442618372162106216016943412avg. time (sec.)ASP50g&slazyASP102595130207617312120519624518140141411012564340218211214374485Table 1: number solved instances ASP benchmarks average time takensolved instances. Dierent solvers solve quite dierent sets instances.Sokoban:planning problem robot push number blocks goalpositions, constrained 2-D maze.Graph Colouring:Given graph, assign colour nodes (from given set colours),connected nodes colour.Stable Marriage:Given set men women set preferences, ndstable assignment: swap results better match.these, used instances 2011 2013 competitions, except2013Reachabilityinstances, huge data les none systemsStable Marriage, Graph Colouring Reachability,Packing DisjunctiveIDPScheduling, constructed natural FO() encoding made faithful translationASP. complex benchmarks Labyrinth Sokoban, used originalFO()IDP Gringo-Clasp's ASP specications submitted 2011 competition.designed handle.based encodings available ASP-Core-2 encodings.lazy model expansion, replaced cardinality expressions FO encodingformer justications derived yet; also increases size full grounding.6.2.1 Resultsnumber solved instances average time shown Table 1; average groundingsizeIDP7setup shown Table 2.time grounding size, unsolved instancesReachability (9 times g&s,ASP), Disjunctive Scheduling (6 times ASP) Labyrinth (160 times g&s,ASP), Packing (4 times g&s, 4 times lazy, 30 times ASP) StableMarriage (66 times ASP); unsolved instances caused time-out.8taken account. Memory overows happened9 times7. Grounding consists variable instantiation interleaved formula simplication (e.g., dropping falsedisjuncts, true conjuncts, replacing disjunctions true disjuncts true conjunctions falseconjunctions false, etc). simplication steps may seriously reduce grounding size.8.IDPautomatic symmetry breaking, cause dierenceColouring.273g&sASPGraphfiDe Cat, Denecker, Stuckey & Bruynoogheground size (# atoms)benchmarkSokobanDisj. Sched.PackingLabyrinthReachabilityStable Marr.Graph Col.g&s2.65 1045.17 1063.86 1071.68 1062.87 1072.11 1071.15 104lazy2.90 1052.72 1061.69 1071.38 1061.61 1041.20 1071.58 104ground timeASP4.63 1048.04 1054.53 1063.55 1051.35 1063.36 1062.80 104Table 2: average grounding size numbermarks, setups.taken.g&sASP,lazysolvedg&s(sec.)ASP2.0129.7165.6101.0109.7642.70.1(sec.)0.30.74.72.314.53.20.1instances ASP bench-setup, size nal ground theoryaverage grounding time also shown.results show lazy model expansion solved instances setupsfour seven cases. cases, problems also got solved signicantlytime threshold.seven cases, (nal) grounding size smallerlazy model expansion, orders magnitude one case.Colouring,Sokoban, Labyrinth Graphlazy model expansion outperformed ground-and-solve, indicatingSokoban,lazy grounding size even higher g&s (possible due FO encodingcardinalities), indicating large part search space explored. StableMarriage, relatively small dierence grounding size g&s lazy leads usloss information outweighed gain grounding less up-front. E.g.,nalbelieve dierent search heuristic main factor, lazy grounding itself.also experimentedAirport Pickup ASP-2011 benchmark, fairly standardscheduling problem (transporting passengers taxis taking account fuel consumption)except upper bound time provided.9Hence ground-and-solve approachwould need construct innite grounding. Applying straightforward lazy model expansion also resulted grounding large. However, prototype useslate grounding heuristic described Section 5.1,IDPsolved one ten instances.others, grounding problem, search took longtime intervals1..nconsidered get sucientnsolve problem (evenstandard search heuristic).presented results show that, although often benecial, lazy model expansionconsiderable overhead hard search problems. hand, inspectingoutcome experiments, observed class specications instancessolved lazy grounding traditional grounding partially overlap. suggestsmight good idea integrate approachesportfoliosystem. systemeither select heuristically whether use ground-and-solve lazy model expansion(based input) running parallel, aborting either one uses muchmemory. However, problems considered, lazy model expansion could start search9. possible derive nite worst-case thresholds Airport Pickup problem. is, however,part original specication.274fiLazy Model Expansion: Interleaving Grounding Searchmuch earlier ground-and-solve, even though got lost often search.leads us believe realize full potential lazy grounding, work necessarydeveloping suitable heuristics (possibly user-specied ones).6.3 Specic Experimentsaddition ASP competition benchmarks, experiments conducted usingcrafted benchmarks illustrate specic properties lazy grounding algorithm.rst part Table 3 shows results scalability experiments.benchmarksPacking, SokobanDisjunctive Scheduling,selected simple prob-lem instance gradually extended domain size orders magnitude: sizegrid (Packing) number time points (Sokoban,Disjunctive Scheduling).results show instances, lazy model expansion scales much betterground-and-solve strategiessatisable instances. However,signicantly.IDPGringo-Clasp satisable well un-Disjunctive Scheduling solving time still increasesreason lazy heuristics still naive make uninformedchoices often.mentioned previous section, ASP competition problems typically smallgroundings since running benchmarks large system handleprovide useful comparison systems. Hence, also evaluated lazy model expansionnumber crafted benchmarks grounding non-trivial.work look practical applications type.part futureconstructed followingbenchmarks:Dynamic reachability,Lazy evaluationexample described Section 3.3.procedurally interpretedprime numbers.symbols, using simple theorydescribed Section 5.2, predicate symbolisP rime/1interpreted procedure returns true argument prime.functionpredicate encodingexperiment simulates model generation theory unknown domain.huge domain.used/1; quantied formulasx : (used(x) ); model6domain size 10 .unknown domain expressed new predicatex :translatedx : (used(x) )x :generation simulated model expansionone, faithful ASP encoding constructed. second part Table 3 showsresults benchmarks. show signicant improvement lazy model expansion ground-and-solve examples: case,memory overow grounding,Disjunctive Scheduling,lazyg&sASPwentfound solutions within seconds. However,also evident lazy approach would benetimproved heuristics: increasing domain size signicantly increases solving time,instances intrinsically harder.6.3.1 Closer Inherent Complexity?modeling phase application, dierent encodings typically tested out,attempt improve performance locate bugs. modeling experimental275fiDe Cat, Denecker, Stuckey & Bruynooghebenchmarkpacking-10packing-25packing-50sokoban-103sokoban-104sokoban-105disj-sched-sat-103disj-sched-sat-104disj-sched-sat-105disj-sched-unsat-103disj-sched-unsat-104disj-sched-unsat-105dynamic reachabilityproceduralfunctionmodelgenerationlazyg&sASP0.22.00.10.32.00.11.110.035.80.310.30.10.520.01.12.668.00.390.490.0717.4413.0416.05164.180.240 490.094.1116.0419.85164.20.181.240.790.19Table 3: solving time additional crafted benchmarks, one instance each.benchmarks, noticed simplifying theory dropping constraints often resulteddramatic reduction time lazy model expansion took nd model. Standardmodel expansion, hand, much less aected simplications.opinion, observation, hardly denitive evidence, another indicationpresented algorithms able derive justications parts theorysatised cheaply. way, approach able distinguish better problemsinherently dicult problems would large grounding.7. Related WorkLazy model expansion oers solution blow-up grounding often occursground-and-solve model expansion methodologyFO(ID )theories.Answer SetProgramming (ASP) SAT Modulo Theories (SMT) techniques also process theorieslarge grounding; constraint store Constraint Programming (CP) MixedInteger Programming clauses SAT considered equivalent groundedtheory (they often derived quantied descriptions cij< cj. . . ) also become large. Lefvre Nicolas (2009) Gede Moura (2009) reported blow-up problem paradigms multitudetechniques developed address it. distinguish four approaches.First, concerning grounding up-front, research done towardsgroundingstatic analysisii( )reducing sizeinput derive bounds variableinstantiations (Wittocx et al., 2010, 2013), ( ) techniquescompilespecic types sen-tences compact ground sentences (Tamura et al., 2009; Metodi & Codish, 2012),iii) detect parts evaluated polynomially (Leone et al., 2006; Gebser et al., 2011;iv) detect parts relevant task hand (e.g.,(Jansen et al., 2013) (276fiLazy Model Expansion: Interleaving Grounding Searchcontext query problems) shown work Leone et al. (2006). Naturally,approaches used conjunction lazy grounding reducesize grounding.IDP,e.g., lazy grounding already combined ( ) (Second, size grounding reducedenrichinglanguage.iii).ex-ample, ASP solvers typically support ground aggregates (interpreted second-order functionscardinality sum take sets arguments), CP SMT solvers support(uninterpreted) functions. recently, Constraint-ASP paradigm developed (Ostrowski & Schaub, 2012), integrates ASP CP extending ASP languageconstraintatoms. interpreted constraints CSP problem thushandled using CP techniques. Various CASP solvers already available, Clingcon (Ostrowski Schaub), Ezcsp (Balduccini, 2011), Mingo (Liu, Janhunen, & Niemel,2012) Inca (Drescher & Walsh, 2012). technique also integratedCat et al., 2013).IncaIDPIDP(Defact implement Lazy Clause Generation (Ohrimenkoet al., 2009), optimized form lazy grounding specic types constraints.language HEX-ASP (Eiter et al., 2005) also extends ASP, timeexternalatomsrepresent (higher-order) external function calls.Third,incremental approacheswell-known model generation, theorem provingplanning. tasks, domain typically xed advance, partstructure sought, number time steps planning problem (recallSokoban example introduction). approach typically works groundingproblem initial guess (the number elements in) domain.Afterwards,search applied; model found, domain extended groundingdone. iterated model found bound maximum domain sizehit (if one known).technique applied, e.g., prover Paradox (Claessen &Srensson, 2003) ASP solver IClingo (Gebser et al., 2008).Fourth, closest lazy grounding itself, large body research devoteddelaying grounding specic types expressions necessary (for exampleresult propagation). Propagation techniques rst-order level delay groundingpropagation ensues researched within ASP (Lefvre & Nicolas, 2009; DalPal et al., 2009; Dao-Tran et al., 2012) within CP (Ohrimenko et al., 2009).techniques used conjunction lazy grounding derive intelligentjustications specic types constraints presented here. example, Dao-Tran etal. also presented ecient algorithm bottom-up propagation denition. WithinSMT, various theory propagators work lazily transforming theory SAT,theory Bit Vectors Bruttomesso et al. (2007).Ge de Moura (2009)investigated quantier handling combining heuristic instantiation methods researchdecidable fragments FO theories, eciently checked models.Within ASP, work done goal-directed reasoning. Bonatti, Pontelli,Son (2008) Marple, Bansal, Min, Gupta (2012) demonstrate approaches,style SLD resolution, apply top-down instantiation answer queries innitedomains. Saptawijaya Pereira (2013) extend abduction framework lazily generatepart relevant sentences. search algorithms, justications (orwatches )usedderive constraint result propagation already satised, hence needchecked propagation phase. Nightingale et al. (2013) show maintaining(short) justications signicantly reduce cost propagation phase.277fiDe Cat, Denecker, Stuckey & Bruynooghefact, well-known technique already exists combines search lazy instantiation quantiers, namelyskolemization,existentially quantied variables re-placed newly introduced function symbols.Universal quantications handledinstantiating introduced function symbols.Reasoning consistency can,e.g., achieved congruence closure algorithms, capable deriving consistency without eectively assigning interpretation function symbols.techniquesused Tableau theorem proving (Hhnle, 2001) SMT solvers (Detlefs, Nelson, & Saxe,2005).Formula (Jackson, Bjorner, & Schulte, 2013) interleaves creating ground pro-gram giving SMT solver, iterating symbolic guesses proved wrong.Skolemization-based techniques typically work well case small number constantsneeds introduced, diculty case relevant domain large. One alsosee lazy grounding (with support function symbols) could incorporate skolemization adapting rules grounding existential universal quantication. expectskolemization complementary lazy grounding, in-depth investigation partfuture work.eld probabilistic inference, several related techniques developedalso rely lazy instantiation. First, Problog system uses form static dependencyanalysis ground (probabilistic) program context given query, constructingpossible ways derive query top-down fashion (Kimmig et al., 2011). Second,so-calledlazy inference,applied e.g.LazySATfact that, considered inference, (xed)(Singla & Domingos, 2006), exploitsdefaultassumption existsexpression certainly contribute probabilities.Hence, expressionsassumption certainly holds considered search. Third,cutting plane inference(Riedel, 2009) applies lazy inference interleaved setting,constructing part program assumptions satised.8. Future WorkSeveral aspects presented work need investigation. One aspect extendingsupport lazily ground complex expressions, including aggregate expressionsP( xD P (x) f (x)) > 3,atom P (d) true, , P(nested) function terms. Consider example sentenceexpresses sum termspredicateff (d)function, larger 3. One observe necessaryground whole sentence up-front.(hence positive), setexample,{P (d1 ), f (d1 ) > 3}fmaps natural numbersminimal justication.Even easyjustication found, suce grounding part sentence delayPP (d1 ) f (d1 )) > 3 ,PPTseitin symbol dened (P (d1 ) f (d1 )) + ( xD\d1 P (x) f (x)) > 3. Indeed,model sentence false, original inequality satised.remainder.example, create ground sentence(second aspect whether advantages grounding earlier, exampleguarantee propagation lost, grounding later, possibly reducing size grounding even more. example, consider sentencesPP ,large formulas justication found. Instead grounding least onesentences, might addPlist atoms search algorithm assign278fiLazy Model Expansion: Interleaving Grounding Searchground either sentencesPassigned value (it might evenunsatisability detected grounding either one).Given lazy grounding useful, lazyforgettinggrounded theory?ground theory extended making structure precise, ground theorycould reduced backtracking.storing justication violationscaused grounding, derive grounding forgotten violationlonger problematic (e.g., backtracking). this, algorithm needs developedtracks grounding/splitting dependencies rules given justications.closely resembles techniques used tableau theorem proving SMT, theoryhand compacted moving dierent part search space.approach described lazy grounding also applied answer set generationeld ASP. ASP, logic program stable semantics seen one ruleset, single denition. However, ASP programs satisfy major conditionapply lazy grounding. Indeed programs typically non-total, due presenceconstraints rules formp np, np pchoice rulesresultmultiple stable models. However, described Denecker et al. (2012), practicalASP programs partitioned set choice rules, settotaldenitions setconstraints (the so-called Generate-Dene-Test partition). ASP programGDT-partitioned, translated straightforwardly equivalentFO(ID )theorycontains total denitions. suggests way apply lazy groundingASP programs.9. ConclusionSolvers used domains SAT, SMT ASP often confronted problemslarge ground. Lazy model expansion, technique described paper,interleaves grounding search order avoid grounding bottleneck. techniquebuilds upon concept justication, deterministic recipe extend interpretationsatises certain constraints. theoretical framework developed lazymodel expansion languageFO(ID ) algorithms presented derivemaintain justications interleave grounding state-of-the-art CDCL searchalgorithms.framework aims bounded model expansion, domainsnite, also initial step towards handling innite domains eciently. Experimentalevaluation provided, using implementationIDPsystem, lazymodel expansion compared state-of-the-art ground-and-solve approach.experiments showed considerable improvement ground-and-solve existing benchmarkswell new applications. main disadvantage less-informed search algorithm,caused delay propagation introduction additional symbols. possiblesolution develop new heuristics portfolio approaches combine strengthsmethods. Finally, indicated way proposed methods appliedbeyondFO(ID ),ASP solvers general.279fiDe Cat, Denecker, Stuckey & BruynoogheAcknowledgementsresearch, Broes De Cat funded Agency Innovation ScienceTechnology Flanders (IWT). research also supported FWO-Vlaanderenproject GOA 13/010, Research Fund KULeuven.NICTA fundedAustralian Government Department Communications AustralianResearch Council ICT Centre Excellence Program.Appendix A. Details Algorithmsappendix, mention parameter values well optimizationsreduce grounding overhead and/or improve search. optimization, indicate currently implemented (and part experimental results) partfuture work.A.1 Parameter Values5.1, number parameters introduced control behavior lazy modelexpansion.Here, provide details values used experimental evaluation.values set manually, based experience limited number observations(e.g., extension threshold works similar conict threshold SAT solver).part future work study impact dierent values.existential quantication, 10 instantiations grounded time; disjunction, 3 disjuncts grounded time. turned give best balanceintroducing many Tseitin atoms grounding much.initial truth valueinitial threshold randomized restarts 100 extensions ground theory.probability0.2fotherwise.doubled restart.formula considered small estimated grounding size104atoms.A.2 Extension FO()IDPfar, described lazy model expansion algorithm function-freeHowever,FO()IDP ,knowledge-base languageIDPFO(ID ).system, supports much richerinput language. Besides types use initialize domains also supports(partial) functions, aggregates arithmetic.current implementation ignoreslatter extensions straightforward adaptation build_djust (Algorithm 4):case literals extended returnFO(ID )falseliteral part function-freelanguage. example, given rulejusticationQ(f (x))h x : P (x) Q(f (x)), P (x) usedcannot. functions, also option replacegraph predicates preprocessing step. experiments Section 6.2,functions, any, given input structure hence play role.part future work extend lazy grounding extensions, especiallyfunctions. Techniques developed SMT Constraint Programming handle (ground)280fiLazy Model Expansion: Interleaving Grounding Searchatoms containing function symbols useful reduce size grounding improvesearch. previous work, techniques integratedIDPsystem (De Catet al., 2013) certainly worthwhile fully integrate lazy grounding.A.3 Cheap Propagation Checks.lazy_mx, checked assigned literal whether denedwhetherviolates justications. implement cheaply, implementation maintainsmapping literalsg .states whether literal denedalso listsjustications negation occurs. mapping extended whenever new literaladdedgmaintained whenever justications change.performancesearch loop unaected long literals assigned mapping empty.A.4 Stopping EarlyAlgorithm 2, took standard stopping criterion used search algorithms(Line 14): stop conict-free statetwo-valued symbolsprinciple, may stop earlier, partial structurePT .Indeed, Corollary 3.6 tells usTg g .admits total justicationexpanded model.denedirrelevant (in eect, appear justication) trigger groundingA's denition, turn might introduce new literals dened , causing cascadeconsiderable impact grounding size. Indeed, assigning truth value atomunnecessary groundings assignments.justicationg ,solver algorithm maintaincannot know exactly justication exists.Instead,implemented algorithm chooses literals watched formula/rule.stops partial structure unwatched literals may assigned.shown suces guaranteeadmits justication. Hence safe stopsearch.A.5 Approximate Justicationscases, build_djust cannot nd valid justication large formulaI.false least one atom Pliterals already falseexample formulax : P (x),build_djust returnsfalse. Instead, adapted build_djust heuristiccheck number expected violations. small enough, justication stillreturned. Naturally, required check whether real violations,querying justication formulaI,apply lazy_ground them.ReferencesAlviano, M., Calimeri, F., Charwat, G., Dao-Tran, M., Dodaro, C., Ianni, G., Krennwallner,T., Kronegger, M., Oetsch, J., Pfandler, A., Phrer, J., Redl, C., Ricca, F., Schneider,P., Schwengerer, M., Spendier, L. K., Wallner, J. P., & Xiao, G. (2013). fourthAnswer Set Programming competition: Preliminary report.T. C. (Eds.),Apt, K. R. (2003).Cabalar, P., & Son,LPNMR, Vol. 8148 LNCS, pp. 4253. Springer.Principles Constraint Programming. Cambridge University Press.281fiDe Cat, Denecker, Stuckey & BruynoogheBalduccini, M. (2011).Industrial-size scheduling ASP+CP.Delgrande, J. P., &LPNMR, Vol. 6645 LNCS, pp. 284296. Springer.Knowledge Representation, Reasoning, Declarative Problem Solving.Faber, W. (Eds.),Baral, C. (2003).Cambridge University Press, New York, NY, USA.Bonatti, P. A., Pontelli, E., & Son, T. C. (2008).Credulous resolution answer setprogramming. Fox, D., & Gomes, C. P. (Eds.),AAAI, pp. 418423. AAAI Press.Bruttomesso, R., Cimatti, A., Franzn, A., Griggio, A., Hanna, Z., Nadel, A., Palti, A.,& Sebastiani, R. (2007).verication problems.lazy layered SMT(BV) solver hard industrialDamm, W., & Hermanns, H. (Eds.),LNCS, pp. 547560. Springer.CAV,Vol. 4590Calimeri, F., Ianni, G., & Ricca, F. (2014). third open answer set programming competition.TPLP, 14 (1), 117135.Chen, W., & Warren, D. S. (1996). Tabled evaluation delaying general logic programs.J. ACM, 43 (1), 2074.Claessen, K., & Srensson, N. (2003).New techniques improve MACE-style modelProceedings CADE-19 Workshop: Model Computation - Principles,Algorithms, Applications.nding.Dal Pal, A., Dovier, A., Pontelli, E., & Rossi, G. (2009). Answer set programmingconstraints using lazy grounding. Hill, P. M., & Warren, D. S. (Eds.),5649LNCS, pp. 115129. Springer.Dao-Tran, M., Eiter, T., Fink, M., Weidinger, G., & Weinzierl, A. (2012).ICLP,Vol.Omiga :open minded grounding on-the-y answer set solver. del Cerro, L. F., Herzig, A.,JELIA, Vol. 7519 LNCS, pp. 480483. Springer.Cat, B. (2014). Separating Knowledge Computation: FO(.) Knowledge BaseSystem Model Expansion Inference. Ph.D. thesis, KU Leuven, Leuven, Belgium.& Mengin, J. (Eds.),DeDe Cat, B., Bogaerts, B., Bruynooghe, M., & Denecker, M. (2014).modelling language: IDP system.CoRR, abs/1401.6312.Predicate logicDe Cat, B., Bogaerts, B., Devriendt, J., & Denecker, M. (2013). Model expansionpresence function symbols using constraint programming.ICTAI, pp. 10681075.IEEE.De Cat, B., Denecker, M., & Stuckey, P. J. (2012). Lazy model expansion incrementalgrounding. Dovier, A., & Costa, V. S. (Eds.),ICLP (Technical Communications),LIPIcs, pp. 201211. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik.Delgrande, J. P., & Faber, W. (Eds.). (2011). Logic Programming Nonmonotonic Reasoning - 11th International Conference, LPNMR 2011, Vancouver, Canada, May 16-19,2011. Proceedings, Vol. 6645 LNCS. Springer.Vol. 17Denecker, M. (1998). well-founded semantics principle inductive denition.Dix, J., del Cerro, L. F., & Furbach, U. (Eds.),Springer.282JELIA, Vol. 1489 LNCS, pp. 116.fiLazy Model Expansion: Interleaving Grounding SearchDenecker, M. (2000). Extending classical logic inductive denitions. Lloyd, J. W.,Dahl, V., Furbach, U., Kerber, M., Lau, K.-K., Palamidessi, C., Pereira, L. M., Sagiv,Y., & Stuckey, P. J. (Eds.),CL, Vol. 1861 LNCS, pp. 703717. Springer.Denecker, M., Bruynooghe, M., & Marek, V. W. (2001). Logic programming revisited: Logicprograms inductive denitions.ACM Trans. Comput. Log., 2 (4), 623654.Denecker, M., & De Schreye, D. (1992). Justication semantics: unifying frameworksemantics logic programs.Tech. rep. 157, Department Computer Science,K.U.Leuven.Denecker, M., & De Schreye, D. (1993). Justication semantics: unifying frameworksemantics logic programs. Pereira, L. M., & Nerode, A. (Eds.),LPNMR, pp.365379. MIT Press.Denecker, M., Lierler, Y., Truszczynski, M., & Vennekens, J. (2012).mal semantics answer set programming.ICLP (Technical Communications),Vol. 17Tarskian infor-Dovier, A., & Costa, V. S. (Eds.),LIPIcs,pp. 277289. Schloss Dagstuhl- Leibniz-Zentrum fuer Informatik.Denecker, M., & Ternovska, E. (2008). logic nonmonotone inductive denitions.Trans. Comput. Log., 9 (2), 14:114:52.ACMDenecker, M., & Vennekens, J. (2014). well-founded semantics principle inductive denition, revisited. Baral, C., De Giacomo, G., & Eiter, T. (Eds.),KR,pp.2231. AAAI Press.Denecker, M., Vennekens, J., Bond, S., Gebser, M., & Truszczyski, M. (2009). secondanswer set programming competition.Erdem, E., Lin, F., & Schaub, T. (Eds.),LPNMR, Vol. 5753 LNCS, pp. 637654. Springer.Detlefs, D., Nelson, G., & Saxe, J. B. (2005).checking.J. ACM, 52 (3), 365473.Simplify: theorem prover programTechnical Communications 28th International Conference Logic Programming, ICLP 2012, September 4-8, 2012, Budapest,Hungary. Proceedings, Vol. 17 LIPIcs. Schloss Dagstuhl - Leibniz-Zentrum fuer In-Dovier, A., & Costa, V. S. (Eds.). (2012).formatik.Drescher, C., & Walsh, T. (2012). Answer set solving lazy nogood generation. Dovier,A., & Costa, V. S. (Eds.),ICLP (Technical Communications),Vol. 17LIPIcs,pp.188200. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik.Eiter, T., Ianni, G., Schindlauer, R., & Tompits, H. (2005). uniform integration higherorder reasoning external evaluations answer-set programming.Kaelbling,IJCAI, pp. 9096. Professional Book Center.Mathematical Introduction Logic (Second edition).L. P., & Saotti, A. (Eds.),Enderton, H. B. (2001).AcademicPress.Logic Programming Nonmonotonic Reasoning, 10th International Conference, LPNMR 2009, Potsdam, Germany, September14-18, 2009. Proceedings, Vol. 5753 LNCS. Springer.Erdem, E., Lin, F., & Schaub, T. (Eds.). (2009).283fiDe Cat, Denecker, Stuckey & BruynoogheGe, Y., & de Moura, L. M. (2009). Complete instantiation quantied formulas satisabiliby modulo theories. Bouajjani, A., & Maler, O. (Eds.),LNCS, pp. 306320. Springer.CAV,Vol. 5643Gebser, M., Kaminski, R., Kaufmann, B., Ostrowski, M., Schaub, T., & Thiele, S. (2008).Engineering incremental ASP solver.(Eds.),Garca de la Banda, M., & Pontelli, E.ICLP, Vol. 5366 LNCS, pp. 190205. Springer.Gebser, M., Kaminski, R., Knig, A., & Schaub, T. (2011). Advances Gringo series 3.Delgrande, J. P., & Faber, W. (Eds.),LPNMR,Vol. 6645LNCS,pp. 345351.Springer.Gebser, M., Schaub, T., & Thiele, S. (2007).GrinGo : new grounder Answer SetProgramming. Baral, C., Brewka, G., & Schlipf, J. S. (Eds.),LNCS, pp. 266271. Springer.Hhnle, R. (2001).(Eds.),Tableaux related methods.LPNMR, Vol. 4483Robinson, J. A., & Voronkov, A.Handbook Automated Reasoning, pp. 100178. Elsevier MIT Press.Jackson, E. K., Bjorner, N., & Schulte, W. (2013).Open-world logic programs: newfoundation formal specications. Tech. rep. MSR-TR-2013-55, Microsoft Research.Jansen, J., Jorissen, A., & Janssens, G. (2013). Compiling input3tabled Prolog rules IDP .FO() inductive denitionsTPLP, 13 (4-5), 691704.Karp, R. (1972). Reducibility among combinatorial problems. Miller, R., & Thatcher, J.(Eds.),Complexity Computer Computations, pp. 85103. Plenum Press.Kimmig, A., Demoen, B., De Raedt, L., Santos Costa, V., & Rocha, R. (2011).implementation probabilistic logic programming language ProbLog.11 (2-3), 235262.Lefvre, C., & Nicolas, P. (2009).TPLP,rst version new ASP solver: ASPeRiX.Erdem, E., Lin, F., & Schaub, T. (Eds.),LPNMR,Vol. 5753LNCS,pp. 522527.Springer.Leone, N., Pfeifer, G., Faber, W., Eiter, T., Gottlob, G., Perri, S., & Scarcello, F. (2006).DLV system knowledge representation reasoning.Log., 7 (3), 499562.ACM Trans. Comput.Liu, G., Janhunen, T., & Niemel, I. (2012). Answer Set Programming via Mixed IntegerProgramming.Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.),KR,pp. 3242.AAAI Press.Marek, V. W., & Truszczyski, M. (1999).gramming paradigm.D. S. (Eds.),Stable models alternative logic pro-Apt, K. R., Marek, V. W., Truszczyski, M., & Warren,Logic Programming Paradigm: 25-Year Perspective,pp. 375398.Springer-Verlag.Marin, M. (2009).Model Generation ID-Logic.Ph.D. thesis, Department ComputerScience, KU Leuven, Belgium.Marin, M., Gilis, D., & Denecker, M. (2004). relation ID-Logic answerset programming. Alferes, J. J., & Leite, J. A. (Eds.),pp. 108120. Springer.284JELIA,Vol. 3229LNCS,fiLazy Model Expansion: Interleaving Grounding SearchMarin, M., Wittocx, J., Denecker, M., & Bruynooghe, M. (2008). SAT(ID): Satisabilitypropositional logic extended inductive denitions. Kleine Bning, H., & Zhao,X. (Eds.),SAT, Vol. 4996 LNCS, pp. 211224. Springer.Marple, K., Bansal, A., Min, R., & Gupta, G. (2012). Goal-directed execution answerset programs. Schreye, D. D., Janssens, G., & King, A. (Eds.),PPDP,pp. 3544.ACM.Marques Silva, J. P., Lynce, I., & Malik, S. (2009).Conict-driven clause learning SATHandbookSatisability, Vol. 185 Frontiers Articial Intelligence Applications, pp. 131solvers.Biere, A., Heule, M., van Maaren, H., & Walsh, T. (Eds.),153. IOS Press.Metodi, A., & Codish, M. (2012). Compiling nite domain constraints SAT BEE.TPLP, 12 (4-5), 465483.Mitchell, D. G., & Ternovska, E. (2005).framework representing solving NPsearch problems. Veloso, M. M., & Kambhampati, S. (Eds.),AAAI,pp. 430435.AAAI Press / MIT Press.Mitchell, D. G., Ternovska, E., Hach, F., & Mohebali, R. (2006).Model expansionframework modelling solving search problems. Tech. rep. TR 2006-24, SimonFraser University, Canada.Nethercote, N., Stuckey, P., Becket, R., Brand, S., Duck, G., & Tack, G. (2007). Minizinc:Towards standard CP modelling language. Bessiere, C. (Ed.),LNCS, pp. 529543. Springer.CP'07,Vol. 4741Nightingale, P., Gent, I. P., Jeerson, C., & Miguel, I. (2013). Short long supportsconstraint propagation.J. Artif. Intell. Res. (JAIR), 46, 145.Ohrimenko, O., Stuckey, P. J., & Codish, M. (2009). Propagation via lazy clause generation.Constraints, 14 (3), 357391.Ostrowski, M., & Schaub, T. (2012).12 (4-5), 485503.ASP modulo CSP: clingcon system.Riedel, S. (2009). Cutting plane MAP inference Markov logic.Statistical Relational Learning (SRL-2009).Saptawijaya, A., & Pereira, L. M. (2013).TPLP,International WorkshopTowards practical tabled abduction logicprograms. Correia, L., Reis, L. P., & Cascalho, J. (Eds.),EPIA, Vol. 8154 LNCS,pp. 223234. Springer.Singla, P., & Domingos, P. (2006). Memory-ecient inference relational domains. Gil,Y., & Mooney, R. J. (Eds.),AAAI, pp. 488493. AAAI Press.Son, T. C., Pontelli, E., & Le, T. (2014).Two applications ASP-Prolog system:Decomposable programs multi-context systems. Flatt, M., & Guo, H.-F. (Eds.),PADL, Vol. 8324 Lecture Notes Computer Science, pp. 87103. Springer.Tamura, N., Taga, A., Kitagawa, S., & Banbara, M. (2009). Compiling nite linear CSPSAT.Constraints, 14 (2), 254272.285fiDe Cat, Denecker, Stuckey & BruynoogheTorlak, E., Chang, F. S.-H., & Jackson, D. (2008). Finding minimal unsatisable coresdeclarative specications. Cullar, J., Maibaum, T. S. E., & Sere, K. (Eds.),Vol. 5014LNCS, pp. 326341. Springer.FM,Tseitin, G. S. (1968). complexity derivation propositional calculus. Slisenko,A. O. (Ed.),Studies Constructive Mathematics Mathematical Logic II, pp. 115125. Consultants Bureau, N.Y.Van Gelder, A. (1993). alternating xpoint logic programs negation.Syst. Sci., 47 (1), 185221.J. Comput.Vennekens, J., Marin, M., Wittocx, J., & Denecker, M. (2007). Predicate introductionlogics xpoint semantics. Part I: Logic programming.79 (1-2), 187208.Fundamenta Informaticae,Wittocx, J., Denecker, M., & Bruynooghe, M. (2013). Constraint propagation rst-orderlogic inductive denitions.ACM Trans. Comput. Logic, 14 (3), 17:117:45.Wittocx, J., Marin, M., & Denecker, M. (2008).idp system: model expansion systemextension classical logic. Denecker, M. (Ed.),LaSh, pp. 153165. ACCO.Wittocx, J., Marin, M., & Denecker, M. (2010). Grounding FO FO(ID) bounds.J. Artif. Intell. Res. (JAIR), 38, 223269.286fiJournal Artificial Intelligence Research 52 (2015) 477-505Submitted 10/14; published 04/15Case-Based Reasoning Framework Choose Trust ModelsDifferent E-Marketplace EnvironmentsAthirai A. IrissappaneJie ZhangATHIRAI 001@ E . NTU . EDU . SGZHANGJ @ NTU . EDU . SGSchool Computer EngineeringNanyang Technological University, SingaporeAbstractperformance trust models highly depend characteristics environmentsapplied. Thus, becomes challenging choose suitable trust model givene-marketplace environment, especially ground truth agent (buyer seller) behavior unknown (called unknown environment). propose case-based reasoning frameworkchoose suitable trust models unknown environments, based intuition trust modelperforms well one environment, another similar environment. Firstly, buildcase base number simulated environments (with known ground truth) along trustmodels suitable them. Given unknown environment, case-based retrieval algorithms retrieve similar case(s), trust model similar case(s) chosensuitable model unknown environment. Evaluation results confirm effectivenessframework choosing suitable trust models different e-marketplace environments.1. Introductionmultiagent e-marketplaces, self-interested selling agents may act maliciously deliveringproducts quality promised. thus important buying agents reasontrustworthiness (quality) sellers providing good quality products determine sellersbusiness with. However, open large environments, buyers often encounter sellersprevious experience. case, buyers often obtain advice (i.e., ratings)sellers buyers (called advisors). However, advisors may dishonestprovide unfair ratings, promote demote sellers (Irissappane, Oliehoek, & Zhang, 2014).Many trust models (Sabater & Sierra, 2005) proposed assess seller trustworthiness, which, BLADE (Regan, Poupart, & Cohen, 2006), also address unfairrating problem. However, performance (accuracy predicting seller trustworthiness) trustmodels often highly affected characteristics environments applied.Specifically, Fullam Barber (2007) found performance trust models influenced environmental settings frequency transactions, honesty sellers accuracyadvisors ratings. detailed comparison BRS (Whitby, Jsang, & Indulska, 2004),TRAVOS (Teacy, Patel, Jennings, & Luck, 2006) Personalized (Zhang & Cohen, 2008) (seeSec. 2 details) conducted Zhang (2009) simulated dynamic e-marketplace environment. results show 1) BRS performs best buyers much experiencesellers environment majority advisors provide fair ratings sellers; 2)TRAVOS advantage scenario buyers sufficient experience advisorsc2015AI Access Foundation. rights reserved.fiI RISSAPPANE & Z HANGlie specific sellers 3) Personalized fares well majority advisorsdishonest sellers widely change behavior time.addition, almost trust models rely certain tuning parameters may significantlyaffect performance. example, identify dishonest advisor, BRS uses quantile parameter (q) determine whether trustworthiness seller falls q quantile 1 qquantile distribution formed advisors ratings seller. TRAVOS bin parameter divide [0, 1] bin number equal intervals, Personalized uses parameterminimum number ratings required buyers accurate modeling seller trustworthiness.Further, trust models evaluated simulated e-marketplace environments,ground truth i.e., actual truth agents malicious behavior known upfront,whether sellers deliver products lower quality promised whether advisorsprovide unfair ratings. simulated environments, performance trust models specificparameter values evaluated, best models easily chosen. However, reale-marketplaces, difficult obtain ground truth expensive time consumingmanually inspect every transaction. Even manage find ground truth real environments, cannot guarantee best models environments suitableenvironments. addition, environments may keep changing, suitable modelenvironment one period may another period. Thus, choosing suitable trust modelsreal environments (where ground truth agents behavior unknown, hence called unknownenvironments) challenging well addressed, important practical applications.paper, propose novel Case-Based Reasoning (CBR) framework choose suitabletrust models unknown e-marketplace environments. CBR well-known artificial intelligencetechnique, applied complicated unstructured problems relatively easily (Sormo,Cassens, & Aamodt, 2005). fundamental concept CBR similar problemssimilar solutions, advantage learning continuously adding new cases casebase. problem choosing trust models, similar intuition trust model performswell one environment, another similar environment. Thus, CBR becomessuitable technique address problem finding trust models suitable similar emarketplace environments (i.e., similar problems). Specifically, proposed framework, firstfind best trust models best parameter settings set simulated environments,representing case base. given unknown real environment, find similar case(s)case base using case-based retrieval methods (Watson & Marir, 1994) k-nearestneighbors, K-dimension (K-d) trees, decision trees, etc. trust model similar case(s)chosen suitable trust model unknown environment.presented work extension previous work (Irissappane, Jiang, & Zhang, 2013),describes simple framework choose trust models using similarity based computation.paper, make number additional contributions: 1) formalize framework choosetrust models using case-based reasoning paradigm. so, explored CBR techniquesi.e., case representation retrieval methodologies, choose suitable trust models efficientmanner; 2) introduce additional case indexing retrieval schemes, K-d trees decision treesapart k-nearest neighbors; 3) introduce feature weights (in addition feature selection),improve accuracy determining nearest neighbors k-nearest neighbors K-d treeretrieval techniques. contributions research perspective, alsoconducted extensive detailed experimentation demonstrate effectivenessframework. Experimental results show high probability, framework478fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACESchoose suitable trust models evaluate seller trustworthiness different unknown environments. Evaluations also indicate seller trustworthiness evaluated using trust models chosenframework set different e-market environments accurate applyingspecific trust model best parameter values environments. Specifically, additionalexperiments: 1) justify impact using suitable trust models e-marketplaces demonstratingsuitable trust models produce accurate estimate seller trustworthiness help buyersmake informed decisions, thereby resulting greater utility buyers using(unsuitable) trust models; 2) consider extended data set increasing number casescase base 972 2268 show performance framework improved usinglarger case base; 3) compare accuracy k-nearest neighbors, K-d trees decision treeschoosing suitable trust models show k-nearest neighbors K-d trees outperform decisiontrees, performing equally well; 4) compare time complexity retrieval techniques,showing decision trees require slightly lesser retrieval time K-d trees, turn requirelesser time k-nearest neighbors; 5) show adding weights features determining nearest neighbors k-nearest neighbors K-d trees improves accuracy choosingsuitable trust models slight margin; 6) demonstrate buyer chooses aggregateoutcomes trust models determine seller trustworthiness instead using singlesuitable trust model, results high margin error; 7) analyze time complexity involvedextending framework adding new features represent environments case baseadding new defense models, improve accuracy framework.rest paper organized follows. Sec. 2, provide overview relatedresearch choosing trust models. clearly point shortcomings existing approaches,explain cope shortcomings work. Sec. 3 describes backgroundcase-based reasoning. detailed description framework presented Sec. 4. Here,also describe framework extended accommodate trust models differente-marketplace environments. Sec. 5, present experimental results using seven trust modelsdemonstrate accuracy framework (using k-nearest neighbors, K-d tree decision treeretrieval) correctly selecting suitable trust models unknown environments. Finally,Sec. 6 concludes current work proposes future work.2. Related WorkHere, provide overview existing trust models frameworks choose trust models.2.1 Trust ModelsMany trust models proposed literature. Beta Reputation System (BRS) (Jsang& Ismail, 2002) models seller trustworthiness expected value beta probability distribution (binary) ratings given advisors seller. handle unfair ratings providedadvisors, Whitby et al. (2004) extend BRS filter ratings majorityamongst ones using Iterated Filtering approach. Specifically, cumulated trustworthiness score seller falls rejection area (q quantile 1 q quantile) betadistribution advisors ratings seller, advisor considered dishonest filtered out. However, Iterated Filtering approach effective significant majorityratings fair, thereby leading lower performance number dishonest advisorslarge. Teacy et al. (2006) propose TRAVOS evaluate advisor trustworthiness, using discount479fiI RISSAPPANE & Z HANGratings aggregated evaluate seller quality. TRAVOS divides interval[0, 1] bin number equal bins determine previous advice provided advisorsimilar current advice. Two pieces advice similar within bin.trustworthiness advisor calculated expected value beta probabilitydensity function representing amount successful unsuccessful interactionsbuyer seller based previous advice. However, model assumes sellersbehave consistently towards buyers e-marketplace, might true manycases. Yu Singh (2003) use belief theory represent trustworthiness scores. determineseller quality, rely referral network find advisors, thereby combine beliefsadvisors regarding seller. referral process begins buyer initially contactingpre-defined number neighbors/advisors, may give opinion seller referadvisors continues termination reached. referral process terminates successopinion received advisor failure depth limit referral networkreached arrives advisor neither gives opinion referral. Weightsalso assigned advisor, order identify deceptive ones.BLADE approach (Regan et al., 2006) applies Bayesian learning reinterpret advisorsratings instead filtering unfair ones. establishing correlation seller propertiesadvisors ratings, buyer infer advisors subjective evaluation functions derive certainproperties seller. Though reinterpretation helps cope advisors subjectivitydeception simultaneously, significant amount evidence (ratings) required accurately determine behavior advisors. Thereby, BLADE cannot perform effectively sparse scenarios,buyers sufficient ratings sellers. personalized approach (Zhang &Cohen, 2008), trustworthiness seller takes account buyers personal experience seller public knowledge seller. buyer enough privateinformation (personal experience with) seller (determined minimum numbertransactions seller using acceptable level error confidence level ), buyeruses private knowledge alone, otherwise uses aggregation private public knowledge compute trustworthiness seller. similar approach used compute advisortrustworthiness. Prob-Cog (Noorian, Marsh, & Fleming, 2011) two-layered cognitive approachfilter ratings provided advisors, based similarity ratings buyeradvisor advisors behavioral characteristics. first layer, advisorsfiltered average difference advisors opinions buyers personal ratingsexceeds threshold value . second layer, approach recognizes behavioral characteristics advisors passed first layer subjectively evaluates degreetrustworthiness. approach advantage proposed idea differentiate advisorsbehavior patterns. However, Prob-Cog assumes advisors behavior consistent across sellers,thereby making inefficient dynamically change behavior behaving honestly towardssellers dishonest others. iCLUB approach (Liu et al., 2011) adopts clustering technique DBSCAN, filter dishonest advisors based local global information.DBSCAN works grouping points density-reachable i.e., farther away givendistance other. also requires pre-defined minimum number points minP tsform dense region i.e., cluster specified. iCLUB, DBSCAN clusters formed usingratings given buyer advisors sellers. target seller, advisors ratingscluster containing active buyers ratings, advisors considered dishonest.480fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACESbuyer sufficient direct experience target seller (number transactionsless threshold ), process applied non-target sellers.see, performance trust model mentioned varies dependingenvironmental settings (especially buyer seller behavior), applied. trustmodel may suitable model environments. Thus, given unknownenvironment, necessary choose among pool trust models, order accuratelyassess seller trustworthiness choose good quality seller transaction.2.2 Existing Frameworks Choose Trust Modelsapproaches proposed choose trust models. example, Hang, WangSingh (2009) make use explicitly indicated trust relationships users real-world systems(e.g., FilmTrust) evaluate trust models. weighted graph vertices denoting agentsedges representing direct relationship trust agent source vertex agenttarget vertex, weight (extent trust agents vertices) particular edgedetermined relevant edges. evaluation, edge temporarily removedweight edge estimated. accuracy predicting weight edge determineseffectiveness trust model. major drawback method users may lietrust relationships, turn may affect evaluation process. works (Wang,Hang, & Singh, 2011; Irissappane & Zhang, 2014) use data real-world e-markets (e.g., eBayAmazon) evaluate performance trust models accuracy predicting ratingsgiven transactions (i.e., seller, ratings previous transactions used predict(i + 1)th rating seller). However, ground truth whether ratingstransactions unfair may unknown. One may argue rely buyerschoose trust models know true experience sellers. But, costlybuyers evaluate trust model various parameters given environment.Closely related work Personalized Trust Framework (PTF) (Huynh, 2009) selects appropriate trust model particular environment based users choice. Here, usersspecify select trust model based information whose trustworthinessevaluated configuration trust models. framework, 1) subject whose trustworthiness evaluated first sent trust manager. trust manager stores many trustprofiles contain rules suggested end users, regarding trust model usesubject; 2) trust manager matches subjects information trust profiles findsuitable trust model initializes trust engine selected model; 3) selected trustmodel derives trust value subject. PTF relies entirely human intervention (usersspecify rules select trust models). Though possible identify certain rules determinesuitable trust model environments (e.g., BRS performs well majority advisorshonest, BLADE performs well advisors subjective differences, etc.), impossibleknow models perform best complex real world environments mayvariety buyer seller behavior. Also, ground truth honesty subjectivitybuyers sellers extremely challenging determine, resulting rules partialthus insufficient accurately choose suitable trust models using PTF. hand,case-based reasoning framework, compare properties unknown environmentexisting cases case-base using automated approach choose suitable trust models,shown highly accurate experiments Sec. 5.481fiI RISSAPPANE & Z HANG3. BackgroundCase-Based Reasoning (CBR) process solving new problems based solutionssimilar past problems. Conceptually, CBR commonly described CBR-cycle (Aamodt &Plaza, 1994). CBR-cycle comprises four activities: retrieve, reuse, revise retain.retrieve phase, one cases, similar new problem selected casebase. Many case-based retrieval algorithms exist literature (Watson & Marir, 1994). Nearestneighbor techniques (Duda & Hart, 1973) perhaps widely used retrieval techniquesCBR. Distance measures Euclidean distance employed identify nearestneighbors (cases). Despite simplicity, nearest neighbor retrieval successful largenumber classification problems (Hastie, Tibshirani, & Friedman, 2009). However, casebase grows, efficiency retrieval decreases, increasing number cases must takenaccount find similar case. K-d trees (Wess, Althoff, & Derwand, 1994), organizecase base binary tree structure shown reduce complexity retrievalnearest neighbors. Alternatively, inductive retrieval algorithms (Soltani, 2013; Watson, 1999),determining features best job discriminating cases generate decision tree typestructure organize cases memory, also used improve retrieval efficiency.one similar cases retrieved, solution (or problem solvinginformation) contained cases reused solve current problem. Reusing retrievedsolution quite simple, solution returned unchanged proposed solutionnew problem. specifically case classification tasks limited number solutions(classes) large number cases. scenarios, every potential solution containedcase base hence adaptation usually required. hand, synthetic tasks (suchconfiguration planning) solution adaptation new problem necessary.revise phase, solution determined far verified real world possiblycorrected improved, e.g., domain expert. Finally, retain phase takes feedbackrevise phase updates knowledge, particularly case base new problem solvingexperience becomes available reuse future problem solving episodes.major challenge CBR resides retrieval existing cases sufficiently similarnew problem. Since e-marketplace environments ground truth (existing cases) mayexist (or may difficult obtain), framework, create simulations.addition, framework, features (characteristics e-marketplace environments) usedrepresent cases case base known beforehand. thus comeexhaustive list potential features (to describe e-marketplace) carefully selectrelevant ones, order efficiently choose suitable trust models.4. Proposed Case-Based Reasoning FrameworkFig. 1 illustrates detailed design framework. important component framework case base. build case base, first simulate large set e-marketplace environments known ground truth honesty agents behavior. Given set availabletrust models specific values parameters (referred candidate trust models),evaluate performance simulated environment, best model identifiedforms best environment-model pair (representing case case base). process, alsochoose relevant features represent cases, efficient retrieval. Given unknown482fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACESreal environment, framework extracts set carefully selected (most relevant) featuresdetermines similar case(s) case base using case-based retrieval techniques.trust model similar case(s) reused solution unknown real environment. given unknown environment along suitable trust model retainedframework reuse future problem solving episodes. major componentsframework detailed procedures described following subsections.Building Case BaseSimulatedEnvironmentsCase RetrievalCandidateTrust ModelsSimilar Case(s)Feature ValuesUnknown EnvironmentCase BaseFeature ExtractionSelectionEvaluate CandidateTrust Models(Environment, Model)UnknownEnvironmentCase ReuseRelevantFeaturesSuitableTrust ModelVerifyAccuracyCase RetainFigure 1: Design case-based reasoning framework4.1 Case BaseCBR heavily dependant structure content case base. framework, casecase base described e-marketplace environment (represented set carefullyselected features) along trust model performs best environment. Unlikedomains, real e-marketplace environments ground truth honesty sellersbuyers rare may exist, hence becomes challenging build case base.mainly rely simulations create existing cases case base.4.1.1 E-M ARKETPLACE E NVIRONMENTSe-marketplace environment (E) consists set sellers, set buyers, transactions (eachseller buyer certain monetary value) ratings (eachgiven buyer seller specific time indicating whether buyer satisfiedtransaction). So, E tuple,E = hS, B, {Ts,b |s = 1...Ns , b = 1...Nb } , {Rs,b |s = 1...Ns , b = 1...Nb }i(1)represents set sellers, B represents set buyers, Ns Nbnumbers sellers buyers E, respectively. Rs,b denotes set ratings buyer bseller transactions Ts,b . rating rs,b Rs,b transaction ts,b Ts,b tuple,rs,b = hid, s, b, hs , hb , t, vali483(2)fiI RISSAPPANE & Z HANGid, s, b denote rating index, index seller buyer, respectively.hs ( [0, 1]) hb ( {honest, dishonest}) denote ground truth i.e., actual seller trustworthiness honesty buyer transaction, respectively. dishonest seller (with lowtrustworthiness) may advertise products high quality actually deliver low quality onesdeliver all. Also, dishonest buyer may lie satisfaction level transactionproviding unfair rating. hs hb attributes help distinguish dishonest behaviorshonest ones. time (integer value denoting day simulation) ratinggiven denoted t. val denotes actual value rating, binary (e.g., 0 1),multi-nominal (e.g., 1 - 5) real (e.g., range [0, 1]).two types environments framework: 1) known environments (Eknown ),ground truth seller buyer honesty known. known environments alongsuitable trust models help building case base framework; 2) unknown environments (Etest ) ground truth known. represent test environmentssuitable trust models need determined.build case base, simulate large number Eknown environments, covermany scenarios possible closely depict real-world environments. example, may simulate environment many sellers fewer buyers (to represent high provision e-marketplace)many buyers fewer sellers (to illustrate competitive e-marketplace). may simulatesparse environment ratings provided buyers, dense environmentseller flooded large number ratings. may also simulate different scenariosbuyers active inactive providing ratings. environments, may also simulate sellers different levels honesty, buyers launching different types unfair ratingattacks (Hoffman et al., 2009), including example, unfair ratings reputable disreputable sellers, lot unfair ratings, unfair ratings given short long time period, etc.4.1.2 C ANDIDATE RUST ODELSexemplified Sec. 2, many trust models proposed evaluate seller trustworthinesse-marketplaces. New trust models also likely proposed future. trust modelsconsidered candidate trust models framework. addition,parameters tune, may result different performance. Thus, candidate trust model (T )defined trust model specific value parameters. parameter varyingrange, divide range number equal intervals randomly choose valueinterval. Ideally, larger number intervals better.4.1.3 F EATURE E XTRACTION ELECTIONformally represent environment case base, environment describedset features, representing characteristics environment (e.g., ratio number buyersversus sellers, variance ratings per seller per buyer, average number transactions per timeperiod, percentage rated sellers, etc.). exhaustive list potential features extractedrelevant features identified used represent environment, orderreduce computational cost increase efficiency framework. F = {f1 , ..., fn }set features P (F ) performance framework using subset F Ffeatures. relevant subset features F chosen framework achievesbest performance, formalized Eqn. 3.484fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACESF = arg max P (F )(3)F Fconstructing case base, simulate another set e-marketplace environmentsevaluate performance framework environments using possible features.features whose values significantly correlate performance framework determined using five widely used correlation regression analysis techniques, namely Pearson correlation, Kendall rank correlation, Spearman rank correlation, linear regression (backward) linearregression (stepwise). results correlation also analyzed Paired-Samples T-testcheck statistical significance. correlation regression analysis technique resultssubset significantly relevant features recognized technique (F F ). influential set features (F ) determined1 five subsets features (each recognizedfive techniques, respectively) using Eqn. 3, details presentedSec. 5.1.3. Thus, features F used represent environments case base.4.1.4 B EST E NVIRONMENT-M ODEL PAIRSGiven set known environments set candidate trust models, find environment (Eknown ), candidate model (T ) shows best performance. Specifically,performance P (Eknown , ) measured terms performance metric, MeanAbsolute Error (MAE) determining seller trustworthiness, given Eqn. 4, TstrueTspredicted represent actual predicted trustworthiness seller s, respectively. lowerMAE, better performance trust model. evaluations result set bestenvironment-model pairs (Eknown , ), form case base. several models performequally best environment, keep case base.MAE =1 X true|TsTspredicted |Ns(4)sS4.2 Case RetrievalGiven unknown environment Etest , case-based retrieval algorithms retrieve similarcase(s), (Eknown , ) pair(s), whose simulated environment Eknown similar Etest .Every retrieval algorithm combination procedure searching case base findsimilar case similarity assessment procedure, determines similaritygiven unknown environment Etest known environment Eknown case base.Firstly, consider structural manner cases represented case base,plays major role efficient retrieval cases. choice case representationchiefly depends type problems CBR system intended solve, varying relativelysimple feature-value vectors, complex data-structures. framework, propose representcase base using two structural representations (Watson & Marir, 1994): 1) flat representation; 2)hierarchical representation, analyze performance framework scenarios.1. feature selection process used determine influential features using k-nearestneighbors, K-d tree retrieval decision trees employs embedded feature selection methodology.485fiI RISSAPPANE & Z HANG4.2.1 F LAT R EPRESENTATIONsimplest format represent cases case base simple feature-value vectorsenvironments (Eqn. 5), obtained influential features (more suitable casesnumeric feature values). flat memory model, cases organized levelrelationships features cases shown.E =< fi | fi F >(5)Classical nearest neighbor (Duda & Hart, 1973) retrieval method choice retrievalcases flat representation, shown Fig. 2. Given unknown environment Etest ,compared cases case base similar cases found according similarityfeatures Etest Eknown environments, measured terms Euclidean distance,sXdist(Etest , Eknown ) =(Etest (fi ) Eknown (fi ))2(6)fi F= arg max N (T )(7)TMAdditionally, also assign weights different features calculating distanceEqn. 6. k-nearest neighbors, k cases, closest Etest based similarity, retrievedsimilar case(s) chosen majority vote, suitable trust modelsimilar case(s) occurs maximum number times among k closest cases, shownEqn. 7, N (T ) represents number times trust model appears k closestcases, TM represents set candidate trust models framework representstrust model similar case(s), suitable trust model Etest .retrieval time memory organization high (O(|C|), |C| numbercases case base), since retrieval, cases case base must comparedtarget case Etest , making unsuitable large case bases. However, approachverified provide maximum accuracy easy retention.Case RetrievalCaseRepresentationFlat StorageFeature ValuesUnknown EnvironmentCase Base(Environment, Model)UnknownEnvironmentk-NearestNeighborsMajority VoteCase ReuseSuitableTrust ModelFigure 2: k-Nearest Neighbors retrieval486fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES4.2.2 H IERARCHICAL R EPRESENTATIONefficient rapid retrieval, structured representation cases necessary,small subset cases need considered retrieval large case base.Following hierarchical representation helps organize cases share similar featuresgeneralized structure. demonstrate use two hierarchical tree structures differmethod indexing (assigning indices cases) greatly improve retrieval efficiency.Case RetrievalK-d TreesCaseRepresentationf1=m1f2=m2>(Environment, Model)f2=m3Bucket(Eknown, TM)>f3=m4.Case BaseFeature ValuesUnknown Environment>f3=m5>Bucket(Eknown, TM)Bucket(Eknown, TM)UnknownEnvironmentk-NearestNeighborsMajority VoteCase ReuseSuitableTrust ModelFigure 3: K-d Tree retrievalTraditionally, K-dimensional (K-d) tree representation demonstrated usefulreduce retrieval time similar cases using nearest neighbors (Wess et al., 1994). K-dtree, K represents number feature dimensions representing case (i.e., K = |F |),multi-dimensional binary search tree splits case base groups cases waygroup contains cases similar other. Specifically, node K-dtree splits children along specific feature, using hyperplane perpendicularcorresponding axis. root (which contains entire case base), children split basedfirst feature (f1 F ), i.e., cases f1 less (or equal to) root leftsub-tree greater root right sub-tree, shown Fig. 3. leveltree divides cases next feature fi F , returning first dimension f1features exhausted. leaves tree contain specific numbercases called buckets. partitioning, median point feature (f1 = m1 shownFig. 3) selected root node cases smaller value (than m1 f1 ) placedleft larger right. similar procedure followed left right sub-treeslast trees partitioned composed cases (not bucketsize).retrieval, recursive search procedure adopted. queue containing k similar casesmaintained throughout search. search examines leaf node, similarity casebucket given unknown environment Etest , computed using Eqn. 6 k-nearest487fiI RISSAPPANE & Z HANGneighbors queue updated. case non-leaf node, search recursively calledchild node, Etest belongs (by comparing features Etest partitioning valuenode). recursion terminates (at non-leaf node), tested whetherchild node needs examined (if geometric boundaries delimiting casesnode overlap ball centered Etest radius equal similarity k th nearest neighborencountered far, child needs examined, ignored otherwise).procedure (unwinding recursive search) repeated root reached. determiningk similar cases (present queue), suitable trust model determined usingEqn. 7. average retrieval time determining k similar cases K-d trees foundO(k log|C|), |C| size case base.Another hierarchical organization frequently used CBR Decision trees. Decision treesinduction-based models (Soltani, 2013) learn general domain-specific knowledge settraining data represent knowledge form trees. Decision trees (when comparedclasses learning methods), quite fast, directly applied trainingdata without much pre-processing produce relatively interpretable models (Hastie et al., 2009).Unlike k-nearest neighbors K-d trees, use similarity based retrieval techniques, decisiontrees learn rules order determine suitable trust model. also implicitfeature selection process. node decision tree specifies test feature attribute(e.g., f1 Fig. 4), branch descending node corresponds possible values(e.g., f1 v1 Fig. 4 ) feature attribute. making trees, much featurediscriminate cases calculated (e.g., information gain cases) featurehighest discriminative power located top tree. calculation performedremaining features, thereby building tree top-down fashion. solution i.e.,suitable trust model located leaves tree. Algorithms developed decision treesCase RetrievalDecision TreesCaseRepresentationf1f2Case Basev2(Environment, Model)Feature ValuesUnknown Environment>v1v1f3>v2; v3>v3>v4v4f4f5.TMUnknownEnvironmentTMCase ReuseSuitableTrust ModelFigure 4: Decision Tree retrievalmostly variations top-down, greedy search algorithm exemplified ID3 (Quinlan,1986) successor C4.5 (Quinlan, 1993). algorithms construct decision tree usingdivide conquer strategy i.e., build decision tree recursively dividing case basesubsets according splitting criterion called information gain ratio. intuition488fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACESpartition case base way information needed classify new case reducedmuch possible. Eqn. 8 represents information gain (discriminative power) featurefi F , regarding set cases C case base, V (fi ) set possible valuesfeature fi Cv C set cases feature fi taking value v. p(T ) proportioncases case base trust model suitable. Since decision treesbuilt-in feature selection methodology, influential features selected usedbuilding decision tree, employ feature selection process described Sec. 4.1.3case retrieval using decision trees, might affect retrieval accuracy otherwise.retrieval, features unknown environment (Etest ) compared nodes tree,gets one leaves contains suitable trust model (T shown Fig. 4).Inf ormationGain(fi , C) = Entropy(C)XvV (fi )where, Entropy(C) =X|Cv |Entropy(Cv )|C|p(T ) log2 p(T )(8)(9)TM4.3 Case Reuseretrieving similar case(s) (using retrieval methods discussed previous subsection) target case (Etest ), framework needs reason according retrieved casesfind reasonable accurate solution (most suitable trust model) Etest . reusesolution done two ways (Soltani, 2013): 1) reusing solution retrieved casesolution target case without adaptation (applicable classification problems);2) adapting retrieved solution target case, necessary problem-solving tasksdesign, configuration, planning. Since deal classification problem, identifying class (candidate trust model case) given unknown environment belongs,perform adaption simply reuse solution retrieved case(s)2 . Thereby,framework choose trust model retrieved case(s) suitable modelEtest (using k-nearest neighbors K-d trees, decision tree retrieval frameworkdirectly choose trust model suggested decision tree similar case(s) retrieved).4.4 Case RetainCase-based reasoning favors learning experience. choosing reuse solutionretrieved case(s) Etest , may found solution is, fact, incorrect, thus providingopportunity learn failure. framework offers simple procedure, case solutionevaluated solution incorrect, revised best solution Etest found.new case along best trust model (Etest , ) retained case base (Fig. 1).proposed case-based reasoning framework generic extended concretizedfollowing aspects: 1) whenever new trust model proposed, added framework. framework capable taking advantage trust model improve performanceevaluating seller trustworthiness; 2) whenever new insightful feature identified,added framework participate feature selection process fact mayimprove performance framework; 3) promising feature selection methods2. one similar cases, different solutions, randomly choose one solutions.489fiI RISSAPPANE & Z HANGincremental hill-climbers (Wettschereck & Aha, 1995), wrapper model measure importancefeatures, adopted enhance performance framework 4) sophisticatedmemory representations used efficient fast retrieval cases.5. Experimentationinstantiate framework conduct series experiments demonstrate effectivenesschoosing suitable trust models. Firstly, build case base generating number simulatedenvironments finding suitable trust models them. process, also determineinfluential features represent simulated environments case base.generate unknown (both simulated real) environments testing verify performanceframework choosing best trust models unknown environments. also compareperformance k-nearest neighbors (k-NN), K-d tree (K-dT) decision tree (DT) retrievaltechniques, finding suitable trust model given unknown environments.5.1 Case Basecase base built using large set simulated environments along suitablecandidate trust models environments, described below.5.1.1 IMULATED E NVIRONMENTSframework, 2268 e-marketplace environments (Eknown ) simulated, consisting different numbers sellers (chosen {10, 25, 50}) different levels trustworthiness Tstrue ,uniformly distributed [0, 1]. Sellers provide good quality products probability Tstrueinteracting buyers. Honest buyers always provide correct opinions (similaractual seller trustworthiness Tstrue ) sellers, dishonest buyers3 provide unfair ratings4 i.e., incorrect opinions complimentary actual seller trustworthiness(1 Tstrue ). simulate different distributions fair ratings given honest buyers: 1) sparse,honest buyer rates seller once; 2) intensive, honest buyer rates selleronce; 3) mixed, combination sparse intensive scenarios. also simulatedifferent unfair rating attack scenarios dishonest buyers adjusting 4 parameters: 1) individualattack frequency denoting average number unfair ratings provided dishonest buyerexhibit sparse, intensive mixed behavior; 2) attack period referring periodunfair ratings given, 7 100 denote dishonest buyers provide unfair ratingsone week (a concentrated attack) 100 days (a distributed attack), respectively. dishonestbuyers provide unfair ratings attack period, behave honestly providing fair ratingsoutside attack period. helps simulate dynamic environments buyers changebehaviors; (3) attack target taking value 0 1, indicating attack targets sellers lowtrustworthiness (Tstrue 0.5) high trustworthiness (Tstrue > 0.5), respectively; 4) overall attackrate denoting ratio number unfair ratings fair ratings, chosen {0.25, 1, 4}.parameters individual attack frequency overall attack rate, numbers dishonesthonest buyers determined. marketplaces operate 100 days. total number ratings chosen {50, 75, 100, 150, 175, 200, 250}. also limit total number ratings3. Buyers providing incorrect opinions due subjective differences ignorance also considered dishonest.4. Ratings simulated environments real type easily mapped types (binary, multi-nominal).490fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES{50}, {50, 100}, {50, 100, 200} {50, 100, 175, 250} simulate 324, 648, 972 1296 environments, respectively, order examine influence number simulated environments(size case base) performance framework.5.1.2 C ANDIDATE RUST ODELSframework includes 7 representative trust models: BRS (Whitby et al., 2004), iCLUB (Liuet al., 2011), TRAVOS (Teacy et al., 2006), Personalized (Zhang & Cohen, 2008), Referral Networks (Yu & Singh, 2003), BLADE (Regan et al., 2006) Prob-Cog (Noorian et al., 2011).following parameters (as described Sec. 2) considered design candidate trust models:1) BRS, quantile parameter q {0.05, 0.1, 0.3, 0.5}, used filter dishonest buyers considered; 2) TRAVOS, number bins determine acceptable error levelbuyers ratings bin {2, 3, 5, 8, 10} considered; 3) Referral Networks, number neighbors{2, 4, 6} depth limit referral networks {4, 6, 8} considered; 4) Personalized, errorlevel {0.3, 0.5, 0.7} confidence level {0.3, 0.5, 0.7} considered; 5) Prob-Cog,consider threshold filter dishonest buyers {0.1, 0.2, . . . , 0.9}; 6) iCLUB,consider minimum number ratings required form DBSCAN cluster minP ts [1, 6],maximum neighbor distance [0.3, 0.7] threshold choose local global component[3, 6]. end, obtain 45 candidate trust models (TM) total.Features123456789101112131415161718Pearson Kendall Spearman Backward Stepwise(C1)(C2)(C3)(C4)(C5) (C6)Variance Percentage Ratings SellerAvg. Number Ratings Provided Buyer SellerRatio Number Buyers versus Number SellersSkewness Rating PeriodVariance Percentage Ratings Provided BuyerSkewness Number Ratings Provided BuyerPercentage Satisfactory SellersNumber BuyersAvg. Number Ratings SellerVariance Number Ratings provided BuyerTotal Number RatingsVariance Number Ratings SellerSkewness Number Ratings SellerAvg. Number Transactions DayTotal Percentage Sellers Rated BuyersTime Period Marketplace OperatesMaximum Percentage Ratings SellersTotal Percentage Buyers Active MarketplaceTable 1: Selection relevant features5.1.3 F EATURE ELECTIONconsider set 18 potential features (F ) analyze characteristics simulated environments, listed Table 1. use general statistical metrics describe features.example, variance refers spread values, skewness describes asymmetrynormal distribution, etc. satisfactory seller refers one receives positive ratingsnegative ones buyers. active buyer refers buyer, provides least one ratingseller. feature values simulated environments extracted using parametersgenerate simulated environments, described Sec. 5.1.1. Since features (in Table 1)491fiI RISSAPPANE & Z HANGdepend ground truth (buyer seller honesty), also easier extract featurevalues unknown environments (with ground truth).select relevant features (for efficient retrieval using k-NN K-dT), adoptfive correlation regression analysis techniques mentioned Sec. 4.1.3. resultsanalysis 18 features correlated performance (MAE) frameworkshown Table 1. Here, * denotes feature significant correlation (after PairedSamples T-test) performance framework. Table 1, columns C1, C2, C3, C4 C5represent combination features flagged *. C6 represents combinationfeatures. verify effectiveness 6 feature combinations, randomly generate largenumber unknown environments compare results. obtain average MAE (using kNN retrieval5 ) 0.44, 0.36, 0.36, 0.25, 0.33, 0.32 combinations C1, C2, C3, C4, C5 C6,respectively. C4 lowest MAE chosen set influential features (F ),Eqn. 3. features C4 used comparing unknown simulated environmentsrest experiments (using k-NN K-dT retrieval obtain similar case(s) Etest ).5.1.4 B EST E NVIRONMENT-M ODEL PAIRSsimulated environment, find best candidate trust model based performance metric MAE. MAE suitable metric assess performance trust modelsaccurately determining trustworthiness sellers helps buyers choose good transaction partners, thereby increasing utility long run (as demonstrated experiments Sec. 5.4).first calculate MAE candidate trust models predicting seller trustworthinesssimulated environments select one lowest MAE value. Here, also computedifference MAE (for seller e-marketplace environment) pairs trust models assess MAE values suitable trust model significantly betterothers (using Paired-Samples T-test). end, obtain 3664 best environment-model pairs6(Eknown , ), form case base framework.Fig. 5(a) illustrates number simulated environments case base candidatetrust model achieves best performance, 733, 306, 448, 979, 190, 253 755BRS, iCLUB, TRAVOS, Personalized, Referral, BLADE Prob-Cog, respectively. numbersindicate case base contains sufficient number cases trust model. sample casei.e., (Eknown , ) shown Eqn. 10. Eknown described 18 features (we showfeatures considered feature selection process clarity) order mentionedTable 1 BLADE model environment.(Eknown , ) =(< 0.30, 0.09, 18.2, 0.04, 0.4, 1.5, 0.6, 182, 19, 0.36, 200, 0.18,1.3, 2, 1, 100, 0.62, 0.2 >, BLADE)(10)5.2 Case Retrieval Algorithmsuse k-NN, K-dT DT retrieval techniques identifying suitable trust models unknownenvironments compare performance.determine number nearest neighbors k-NN K-dT, randomly generate 972unknown environments (using different values parameters generate simulated envi5. K-d tree retrieval obtains similar MAE values.6. simulated environment one suitable trust model (which significantly outperformother) lowest MAE.492fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES1020.4755733648840.3MAENo. Use Times8K-dTk-NN979103060.20.1253190200BRPBLiCPeLU RrferAD robVO sonrCoBEalgize l12345678910k(a)(b)Figure 5: (a) No. times trust model selected suitable model simulatedenvironments; (b) Influence k MAE determining seller trustworthiness usingk-NN K-dTronments) evaluate performance k-NN k-dT choosing suitable trust models,different values k. Table 2 presents influence k accuracy choosingsuitable trust models (with suitable parameters) 972 randomly generated unknownenvironments, using k-NN k-dT retrieval techniques. Correct Model indicates trustmodel chosen best model identified evaluating candidate trust modelsgiven unknown environment. Correct Model Paras indicates correct trust modelchosen appropriate tuning parameters. Also, = 0.05 tolerance value, indicatingdifference MAE chosen trust model truly suitable modelwithin . find accuracy choosing correct trust models (with parameters)highest k = 3 acceptable k = 1, 2 k-NN K-dT. k values greater3, performance decreases, signifying boundaries classes (candidatetrust models) become less distinct. Fig. 5(b) shows MAE determining seller trustworthiness(corresponding accuracy Table 2), value k increased 1 10. k-NNK-dT obtain similar MAE values determining seller trustworthiness different values k.Again, find k {2, 3}, lowest MAE (0.24) achieved. Hence, use k = 3experiments (using k-NN K-dT) paper.k-NNCorrect ModelCorrect ModelCorrect Model ParasCorrect Model ParasK-dTCorrect ModelCorrect ModelCorrect Model ParasCorrect Model Parask=194.0%97.0%92.0%96.0%k=194.0%96.0%91.0%95.0%k=296.0%97.0%94.0%97.0%k=295.0%97.0%94.0%96.0%k=397.0%99.0%97.0%98.0%k=397.0%98.0%97.0%98.0%k=489.0%91.0%85.0%87.0%k=489.0%92.0%86.0%87.0%k=582.0%84.0%77.0%79.0%k=583.0%84.0%78.0%79.0%k=676.0%78.0%71.0%73.0%k=676.0%78.0%71.0%73.0%k=775.0%77.0%69.0%71.0%k=775.0%76.0%69.0%71.0%Table 2: Influence k accuracy framework493k=873.0%75.0%67.0%69.0%k=874.0%75.0%68.0%69.0%k=971.0%73.0%65.0%67.0%k=972.0%73.0%65.0%67.0%k=1071.0%73.0%65.0%67.0%k=1071.0%73.0%65.0%66.0%fiI RISSAPPANE & Z HANGK-dT retrieval, use weka implementation (median based partitioning maximum 20 instances leaf node). K-d tree built using 3664 best environment-modelpairs (described Sec. 5.1.4). DT retrieval, use C4.5 algorithm (J48 weka implementation pruning confidence 0.25 minimum number instances 2,default values). decision tree also built using 3664 best environment-model pairs,used find suitable model unknown environments.5.3 Unknown Environments Testingframework evaluated using 6 categories unknown environments Etest (where ground truthseller buyer honesty fact known) normal extreme scenarios.Specifically, Unknown Random Environments generated using parameter values differentsimulated environments as: 1) number sellers {33, 66, 99}; 2) total numberratings {333, 666, 999}; 3) ratio number unfair ratings versus fair ratings {0.1, 1, 10}; 4)time period attacks {50, 100}, 100 environments randomly chosen testing.Unknown Real Environments generated using Real data obtained IM DB.com,users rate movies directed different directors. remove outlying ratings select directors whose movies highly rated, resulting 40 different directors, 1142 moviesrated 188 users. simulate 3 types unfair rating attacks, namely RepBad, RepSelfRepTrap (Yang, Feng, Sun, & Dai, 2008), combination bad-mouth targeted directors(sellers case). Finally, generate 48 real environments simulated attacks.Large Environments number sellers larger 50, number ratingslarger 100 number buyers larger 80. generate 160 large environments.Extremely Sparse Environments buyers provide sufficient ratings. Specifically, buyer gives average 0.1 ratings sellers. generate 36 environmentsnumber sellers 10, total number ratings 100, overall attack rate {0.25, 1, 4}.Environments Dynamic Seller Buyer Behavior environments sellers/buyerschange behavior dynamically. Sellers change behavior providing complimentary quality products (than previously presented) random period operation e-marketplace.(Dishonest) buyers change behavior providing unfair ratings specific periodsbehaving honestly, otherwise. 35 environments (number sellers 10 total numberratings 50) dynamic behaviors generated.Environments Many Attacks intensive attacking scenarios, attack ratelarger 10. specifically use real data IM DB.com simulate RepBad, RepSelf,RepTrap attacks combination generate 24 environments.5.4 Experimental ResultsHere, present results unknown random real environments well results extremescenarios. also analyze possible extensions framework adding weightsfeatures determining nearest neighbors k-NN K-dT retrieval, etc.5.4.1 P ERFORMANCE C OMPARISON U NKNOWN R ANDOM R EAL E NVIRONMENTSTable 3 presents accuracy framework choosing suitable trust models (withsuitable parameters) unknown random real environments (using k-NN, K-dT DTretrieval techniques). mentioned Sec. 5.2, correct selection indicates trust model494fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACESchosen best model identified evaluating candidate trust models givenunknown environment tolerance value, indicating difference MAEchosen trust model truly suitable model within .Unknown Random Environmentsk-Nearest Neighbors (k-NN)324 SE648 SE972 SECorrect Model81.0%84.0%92.0%Correct Model87.0%89.0%95.0%Correct Models Paras72.0%76.0%82.0%Correct Model Paras 85.0%86.0%94.0%K-d Trees (K-dT)324 SE648 SE972 SECorrect Model80.0%84.0%92.5%Correct Model87.0%90.0%95.0%Correct Models Paras71.0%76.0%82.5%Correct Model Paras 85.0%87.0%94.2%Decision Trees (DT)324 SE648 SE972 SECorrect Model52.0%54.0%63.0%Correct Model64.0%67.0%72.0%Correct Model Paras31.0%33.0%35.0%Correct Model Paras 49.0%53.0%58.0%Unknown Real Environmentsk-Nearest Neighbors (k-NN)324 SE648 SE972 SECorrect Model81.3%83.3%83.3%Correct Model89.6%95.8%95.8%Correct Model Paras72.9%75.0%77.1%Correct Model Paras 89.6%95.8%95.8%K-d Trees (K-dT)324 SE648 SE972 SECorrect Model80.1%82.3%83.3%Correct Model89.0%95.1%96.2%Correct Model Paras71.0%74.0%78.1%Correct Model Paras 88.0%95.0%95.2%Decision Trees (DT)324 SE648 SE972 SECorrect Model08.3%08.3%14.6%Correct Model55.1%58.3%68.8%Correct Model Paras00.0%00.0%00.0%Correct Model Paras 54.3%59.2%68.8%1296 SE96.0%98.0%96.0%97.0%1296 SE95.0%98.0%95.0%97.0%1296 SE72.0%78.0%40.0%63.0%2268 SE97.0%98.0%97.0%98.0%2268 SE97.0%98.0%97.0%98.0%2268 SE80.0%85.0%46.0%67.0%1296 SE86.3%97.3%79.3%96.3%1296 SE86.7%97.0%79.0%96.0%1296 SE40.3%80.3%02.0%80.3%2268 SE87.5%97.3%81.3%97.2%2268 SE87.0%97.0%82.0%97.5%2268 SE45.0 %83.3%02.1%83.3%Table 3: Accuracy choosing suitable trust models (with parameters) unknown randomreal environmentsTable 3 (under unknown random environments), see accuracy framework increases number simulated environments (SE) case base increases (the trendk-NN, K-dT DT), best 2268 simulated environments(SE) case base. larger number cases case base, easierfind closely similar environment given unknown environment.also find k-NN K-dT show similar performance, outperforming DT retrieval technique. K-dT mainly used improve retrieval time k-NN appropriateorganization cases form trees. However, retrieval, K-dT uses similaritymeasure number nearest neighbors k-NN. reason similarperformance k-NN K-dT. hand, DT retrieval shows lower performancerequires training instances (cases case base) learn entire problem space (andbuild complete decision tree) known show surge performance dealingcontinuous feature values (Quinlan, 1996) (in framework feature values continuous).smaller number cases (say 324 SE), DT obtains accuracy 52.0% choosing495fiI RISSAPPANE & Z HANGk-NN0.4K-dTDTK-dTDT0.3MAEMAE0.30.20.10k-NN0.40.20.1324648972129602268|Simulated Environments|32464897212962268|Simulated Environments|(a)(b)Figure 6: Influence number simulated environments performance k-NN, K-dTDT retrieval techniques: (a) random environments; (b) real environmentsbest trust model unknown random environments. Even 2268 SE accuracy80.0%, less accuracy k-NN 324 SE. k-NN K-dT obtain accuracy97.0% selecting suitable models 98.0% accuracy tolerance = 0.05,2268 simulated environments. Thus, shows framework, using k-NN, K-dT retrieval techniques choose candidate models whose performance close ideal case.Even 324 simulated environments, performance k-NN K-dT still acceptable,selecting suitable models accuracy 81.0% 80.0%, respectively.Fig. 6 shows influence number simulated environments (size case base)MAE obtained, determining seller trustworthiness using candidate trust model suggestedk-NN, K-dT DT retrieval techniques. accurate selection best trust modelresults lower MAE value determining seller trustworthiness. Fig. 6(a) shows k-NNK-dT obtain lower MAE DT cases. number simulated environments2268, k-NN K-dT, obtain MAE 0.25, DT obtains MAE 0.31. However,number simulated environments increased, find rate MAEDT decreases greater k-NN K-dT, training instances, DTproduce accurate results. Eventually, number simulated environmentsincreased (greater 2268), DT may show (even better) performance k-NN K-dT.However, increase number simulated environments experiments duecomplexity involved building case base, evaluating 45 candidate trust modelssimulated environment selecting suitable model them.Table 3 (under unknown real environments) shows k-NN K-dT perform equallywell (with accuracy 87.5% 87.0% selecting suitable models 2268 simulated environments, respectively), outperforming DT retrieval (with accuracy 45.0%) realenvironments. also notice accuracy techniques lower random environments. because, characteristics real environments may vary extensivelysimulated environments case base, making difficult retrieval algorithmsidentify similar cases whose simulated environment similar real one. Nevertheless,performance k-NN K-dT unknown real environments also sufficient (greater 86%).Fig. 6(b) shows k-NN, K-dT perform better decision trees. However, see496fi0.70.70.60.60.50.50.40.4MAEMAEF RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES0.30.30.20.20.10.100BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NNB na ra E Cogliz ledBR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NNB na ra E Cogliz led(a)302822No. Use TimesNo. Use Times30(b)192014111022201310540BR4320PBLiCPeLU RrferAD robVO sonrCoBEalgize l(c)BR10PBLiCPeLU RrsferAD roboVOraCoBnaEllizged(d)Figure 7: MAE framework trust models for: (a) random environments; (b) realenvironments; No. times trust model selected suitable for: (c)random environments; (d) real environmentsMAE values Fig. 6(b) smaller Fig. 6(a), though accuracy realenvironments (Table 3) lower unknown random environments. because, assumesellers (in real environments) either high low quality (while unknown randomenvironments seller quality uniformly distributed [0, 1]), thus easily identifiable candidate trust models. also find average MAE determining seller trustworthiness usingtruly suitable trust models unknown real environments 0.01, unknownrandom environments 0.22, comparatively larger value. Thereby, unknown real environments, framework chooses trust model whose MAE similar truly suitabletrust model obtain better accuracy, case lower value MAEunknown random environments. greater rate MAE decreases DT evidentFig. 6(b), since real environments assume seller trustworthiness binary, thereby, evensmall variation choice trust models impact MAE values great extent.Fig. 7(a-b) show MAE framework comparison trust models unknown random unknown real environments. trust models unknown environ-497fi4433UtilityUtilityRISSAPPANE & Z HANG210210BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NNB na ra E Cogliz led(a)BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NNB na ra E Cogliz led(b)Figure 8: Average utility buyers: (a) random environments; (b) real environmentsment, use best parameter values. show performance k-NN, K-dT DT using2268 simulated environments, obtain best performance. demonstrate scenariobuyers may choose aggregate outcomes trust models (instead using singlesuitable trust model), determining seller trustworthiness, also show MAE obtainedadopting heuristic denoted AVG Fig. 7(a-b). Fig. 7(a), find k-NNK-dT obtain lowest MAE 0.25, showing able choose better trust modelsevaluate seller trustworthiness always applying single model. DT obtains MAE 0.31,higher value Personalized MAE 0.30. AVG obtains higher MAE 0.44, showingusing aggregated outcome trust models may result accurate values sellertrustworthiness. unknown real environments (Fig. 7(b)), k-NN K-dT obtainlowest MAE (0.022 0.025, respectively) compared trust models.Fig. 7(c-d) shows numbers unknown random environments unknown real environments, respectively trust model chosen suitable one, using k-NNretrieval technique (K-dT retrieval also obtains similar values). numbers 28, 4, 19, 14, 2, 2211 BRS, iCLUB, TRAVOS, Personalized, Referral, BLADE Prob-Cog, respectively100 unknown random environments, 3, 5, 13, 22, 1, 0 4 models 48 unknown real environments. numbers signify framework able choose different trustmodels candidate set various unknown environments. difference use timestrust models random real environments also indicate trust models performdifferently different kinds environments.Fig. 8(a-b) show average utility buyers e-marketplace correspondingMAE trust models Fig. 7(a-b). Specifically, buyer gains reward +5chooses high quality seller (by evaluating trustworthiness sellers market usingprescribed trust model), Tstrue > 0.5 penalty 5, choosing low quality sellerTstrue 0.5, transaction. Fig. 8(a) shows k-NN K-dT obtain highest utility2.20 2.19, respectively. trend also shows trust models higher MAE (in Fig. 7(a))lower utility lower MAE, buyers able accuratelypredict seller trustworthiness, correctly choose good quality sellers transaction partners.Fig. 8(b) also shows k-NN K-dT obtain highest utility 3.53 3.50, respectively.experiments Fig. 8(a-b) also justify MAE suitable metric assess performance498fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACESk-NN5DTK-dT4433SecondsSeconds5210DTK-dTk-NN21324648972129602268|Simulated Environments|32464897212962268|Simulated Environments|(a)(b)Figure 9: Time choose best trust models: (a) random environments; (b) real environmentstrust model given environment, (indirectly) monitors decisions buyers,large impact utility/value addition gained transactions sellers.Fig. 9(a-b) show time taken framework choose best trust models unknown random real environments, using k-NN, K-dT DT retrieval techniques. ThoughK-dT obtains similar accuracy k-NN (Table 3 Fig. 7), greatly improves time takenfind suitable trust model, shown Fig. 9(a-b). Specifically, Fig. 9(a) shows timetaken find suitable trust models 100 unknown random environments k-NN, K-dTDT 4.18s, 1.40s 1.10s, using 2268 simulated environments case base, respectively.find K-dT DT approaches faster k-NN, compares featuresunknown environment cases case base (Soltani, 2013). K-dT DT usetree structure represent cases case base (as described Sec. 4.2.2) retrievesuitable trust model traversing tree. However, decision tree retrieval slightly fasterK-dT. K-dT, dimensionality (number features) cases number similar cases (k nearest neighbors) needed retrieved, affect retrieval time(requiring number leaves visited backtracking). Literature (Ahmed, 2004;Vempala, 2012) also shows high-dimensional data (greater 20), leavesK-d tree visited efficiency better exhaustive k-NN search,concern feature space framework increased. Fig. 9(b),see unknown real environments K-dT DT require lesser retrieval time k-NN.time taken k-NN, K-dT DT 3.15s, 0.83s 0.35s, using 2268 simulated environmentscase base, respectively. However, values lower Fig. 9(a), since considertime taken choose best trust models 100 unknown random environments,number real environments considered 48. Though time taken K-dT slightlygreater DT, still comparable shows much better performance terms retrievalaccuracy (Table 3 Fig. 7).5.4.2 P ERFORMANCE C OMPARISON E XTREME CENARIOSFig. 10 shows MAE trust models 4 extreme scenarios (i.e., large environments, extremely sparse environments, environments dynamic seller buyer behavior environments many attacks). Table 4 presents probability choosing trust models499fi0.70.70.60.60.50.50.40.4MAEMAERISSAPPANE & Z HANG0.30.30.20.20.10.100BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NNB na ra E Cogliz led(b)0.70.70.60.60.50.50.40.4MAEMAE(a)0.30.30.20.20.10.10BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NNB na ra E Cogliz led0BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NNB na ra E Cogliz led(c)BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NNB na ra E Cogliz led(d)Figure 10: MAE framework trust models extreme scenarios: (a) large environments; (b) extremely sparse environments; (c) environments dynamic sellerbuyer behavior (d) environments many attacks4 extreme cases using k-NN retrieval technique (K-dT obtains almost probabilities k-NN). Results show k-NN K-dT outperform DT, AVG trustmodels, performing equally well environments.specifically, Fig. 10(a) shows k-NN K-dT obtain lowest MAE 0.24 largeenvironments. also find iCLUB, TRAVOS Personalized obtain smaller MAEtrust models large environments. reason three trust models abledistinguish dishonest honest advisors get sufficient rating sources. Table 4large environments, see k-NN selects iCLUB, TRAVOS Personalizedhighest probabilities, 26.9%, 23.8% 38.2%, respectively. Fig. 10(a) also shows DTAVG obtain larger MAE value (0.27 0.28, respectively) k-NN K-dT.Fig. 10(b), sparse environments, k-NN K-dT obtain lowest MAE 0.240.23 DT obtains MAE value 0.35. BRS Prob-Cog perform better trustmodels, BRS adopts majority-rule consider opinions advisors,500fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACESTrust ModelsBRSiCLUBTRAVOSPersonalizedReferralBLADEProb-CogLarge0.6%26.9%23.8%38.2%0.6%9.3%0.6%Sparse25.7%13.9%2.1%2.8%2.8%0.0%52.7%Dynamic7.4%7.2%4.6%27.8%18.5%31.5%3.0%Many Attacks6.9%10.3%13.8%58.7%0.0%0.0%10.3%Table 4: Probability choosing trust models four extreme e-market scenariosProb-Cog extends incompetence tolerance threshold incorporate larger number advisorsratings. models obtain comparatively low MAE less restrictive acceptingopinions advisors trust models. Table 4, sparse environments, k-NN selectsBRS Prob-Cog highest probabilities, 25.7% 52.7%, respectively.Fig. 10(c) shows k-NN K-dT obtain lowest MAE Personalized BLADEoutperform trust models environments sellers change behavior dynamically.explain, Personalized considers advisors latest ratings within certain time window, alleviates influence sellers dynamic behavior. BLADE re-interprets advisors ratings basedlearning, thereby takes account changing behavior buyers sellers. Table 4, k-NNselects Personalized BLADE highest probabilities, 27.8% 31.5%, respectively.Fig. 10(c) also signifies framework able deal scenarios buyers sellerschange behavior, case base already contains environments representing dynamic behavior (as described Sec. 5.1.1), along suitable trust models. Fig. 10(d)shows k-NN K-dT outperform trust models lowest MAE 0.02 Personalized TRAVOS perform well environments many attacks. DT obtains MAE0.03, comparable greater value k-NN K-dT. characteristics attacks play major role judging performance trust models. extreme environments, attackers(dishonest advisors) first give honest ratings non-target sellers promote themselves,provide unfair ratings bad-mouth target sellers. performance Personalized TRAVOSbetter model advisor trustworthiness accurately comparing buyersopinions advisors ratings commonly rated sellers. Also, environments,select buyers sufficient personal experience (ratings) Personalized TRAVOS takeadvantage of. Table 4, k-NN selects Personalized TRAVOS probabilities 58.7%13.8% environments many attacks, respectively.summary, Fig. 10 Table 4, results indicate framework (using k-NNK-dT retrieval technique) able select suitable trust models extreme scenarios obtainaccurate seller trustworthiness AVG individual trust model. Also, findperformance k-NN K-dT retrieval better DT retrieval cases. Decisiontrees induction models learn rules (based features) determine suitable trust models.comes close method PTF (described Sec. 2) works, differencedecision trees rules learned organized form trees, PTF rulesneed manually specified user pre-defined format (Huynh, 2009). However,see available number simulated environments (2268) case base, decision treescannot learn complete domain knowledge construct trees help accurately determinesuitable trust models. thereby infer using rule based system PTF alsoresult moderate performance decision trees (with given size case base).501fiI RISSAPPANE & Z HANG5.4.3 NALYSIS P OSSIBLE MPROVEMENTS F RAMEWORKdemonstrated literature feature weighting (assigning weights individual features) feature selection (selecting subset relevant features ignoring others), improve performance k-NN (Tahir, Bouridane, & Kurugollu, 2007). Thus, improveperformance framework (using k-NN thereby, K-dT retrieval), assign weights(most influential) feature. conduct experiments using linear adaptive filters-leastmean squares (Mitchell, 1997), learning rate 0.2, determine weights features,using 972 randomly generated environments. weights 13 influential features (inorder C4 Table 1), determined using k-NN are, 0.16, 0.02, 0.01, 0.02, 0.03, 0.2, 0.09,0.05, 0.04, 0.1, 0.17, 0.01 0.1, respectively. use weights K-dT analyzeperformance. Table 5 shows performance k-NN K-dT, using feature weightscalculating similarity environments order determine suitabletrust model. k-NN obtains improvement 1.0% 1.5% terms accuracy selectingsuitable trust model unknown random real environments (when compared valuesTable 3, k-NN K-dT assign equal weights 13 influential features), respectively.extreme scenarios, k-NN obtains improvement (at most) 2.0%. K-dT obtains similaraccuracy improvement 0.5% 2.0% unknown random real environments,extreme scenarios, obtains improvement (at most) 2.0%.k-NN + feature weightsCorrect ModelCorrect ModelCorrect Models ParasCorrect Model ParasMAEAccuracy ImprovementK-dT + feature weightsCorrect ModelCorrect ModelCorrect Models ParasCorrect Model ParasMAEAccuracy ImprovementRandom98.0%98.5%97.0%98.0%0.231.0%Random97.5%98.5%97.0%98.0%0.230.5%Real89.0%98.0%82.0%97.2%0.021.5%Real89.0%97.0%82.0%97.2%0.022.0%Large93.0%95.0%95.0%96.1%0.221.0%Large94.0%95.0%95.0%96.1%0.211.0%Sparse90.0%98.0%96.0%97.0%0.221.0%Sparse90.2%97.0%96.0%97.0%0.222.0%Dynamic97.0%97.0%97.0%98.0%0.302.0%Dynamic97.1%97.0%98.0%98.0%0.302.0%Attacks86.0%92.0%80.1%95.4%0.020.0%Attacks85.0%92.0%80.0%96.0%0.020.0%Table 5: Influence using feature weights k-NN K-dTAlso, mentioned Sec. 4, framework extended adding new features trustmodels. Specifically, add new feature need to: 1) generate new set Eknown environments,including new feature; 2) select influential features F (using 5 correlationregression techniques mentioned Table 1), new extended feature set, testingrandomly generated Etest environments, 3) build new case base. Thus, time complexityadding new feature O((|Eknown |+|Etest |)|TM|+5tmodel ), includes time takenfind actual performance defense models TM known Eknown test Etestenvironments (represented (|Eknown | + |Etest |) |TM|) time taken tmodel (model{k-NN, K-dT}) find suitable trust models using 5 different feature combinations(represented 5 tmodel ). |Eknown | = 2268, |Etest | = 972, |TM| = 45, k = 3model = K-dT, total time taken build new case base adding new feature nearly3 hours. Adding new trust model simply takes 3.6 mins (O(|Eknown |)), requiresrun new trust model 2268 environments build new case base. Though502fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACEScalculations show considerable computation time involved, adding new feature trustmodel lead improvement performance framework. Specifically, addingnew feature help accurately select suitable trust models adding new trust modelmay result lower MAE certain environments, leading better decision making therebyimproving utility buyers environment shown Fig. 8. Also,computation need done off-line online effort much lower seen Fig. 9.6. Conclusion Future Workpaper, propose case-based reasoning framework choose suitable trust modelsenvironments ground truth agents behavior unknown. framework,case base built generating number simulated environments determiningsuitable trust models environments. framework also offers choose differenttechniques (k-nearest neighbors, K-d trees decision trees) case retrieval. Given unknownenvironment, similar case(s) retrieved using retrieval techniques. Then, trustmodel corresponding similar case(s) chosen suitable one unknownenvironment. Experimental results confirm framework accurately select suitable trustmodels various unknown environments (both simulated real e-marketplaces). also findk-nearest neighbors K-d tree retrieval techniques accurately choose suitable trustmodels determining similar case(s) case base decision trees, especiallynumber simulated environments (cases case base) much smaller. alsodemonstrated K-d trees significantly improve time complexity choosing suitable trustmodels k-nearest neighbors. Experiments also verify using framework choosesuitable trust models unknown environments (using k-nearest neighbors K-d tree retrieval)better always applying single trust model (or aggregate trust models), termsaccuracy evaluating seller trustworthiness.Currently, framework achieves best performance number simulated environments case base large 2268 environments; performance increaseadding simulated environments. adding simulated environments feasibleoption improve performance framework, requires tremendous off-line computation determine suitable trust models simulated environments build case base.future, investigate methods generate simulated environments representative real world e-marketplaces, performance framework much highereven case base contains smaller number simulated environments. also analyzeeffective feature selection techniques accurately select trust models regard.Another important direction future work consider scenario featuresunknown environment deviate similar simulated environment determinedframework, execution time. One possible solution use proposed frameworkchoose suitable trust model regular intervals time unknown environmentoperates. conduct detailed experiments analyze performance frameworkscenarios. also continue evaluate framework incorporating sophisticatedtrust models involving real data sets.503fiI RISSAPPANE & Z HANGReferencesAamodt, A., & Plaza, E. (1994). Case-based reasoning: Foundational issues, methodological variations, system approaches. AI communications, 7(1), 3959.Ahmed, Y. S. (2004). Multiple Random Projection Fast, Approximate Nearest Neighbor SearchHigh Dimensions. Ph.D. thesis, University Toronto.Duda, R. O., & Hart, P. E. (1973). Pattern classification scene analysis, Vol. 3. Wiley.Fullam, K. K., & Barber, K. S. (2007). Dynamically learning sources trust information: Experience vs. reputation. Proceedings International Joint Conference AutonomousAgents Multiagent Systems (AAMAS).Hang, C. W., Wang, Y., & Singh, M. P. (2009). Operators propagating trust evaluationsocial networks. Proceedings International Joint Conference AutonomousAgents Multiagent Systems (AAMAS).Hastie, T., Tibshirani, R., & Friedman, J. (2009). elements statistical learning, Vol. 2.Springer.Hoffman, K., Zage, D., & Nita-Rotaru, C. (2009). survey attack defense techniquesreputation systems. ACM Computing Surveys (CSUR), 42(1), 1.Huynh, T. (2009). personalized framework trust assessment. Proceedings ACMSymposium Applied Computing (SAC).Irissappane, A. A., Jiang, S., & Zhang, J. (2013). framework choose trust models differente-marketplace environments. Proceedings 23rd International Joint ConferenceArtificial Intelligence (IJCAI).Irissappane, A. A., Oliehoek, F. A., & Zhang, J. (2014). POMDP based approach optimallyselect sellers electronic marketplaces. Proceedings 13th International ConferenceAutonomous Agents Multiagent Systems (AAMAS).Irissappane, A. A., & Zhang, J. (2014). testbed evaluate robustness reputation systemse-marketplaces. Proceedings 13th International Conference Autonomous AgentsMultiagent Systems (AAMAS).Jsang, A., & Ismail, R. (2002). Beta reputation system. Proceedings 15th BledElectronic Commerce Conference.Liu, S., Zhang, J., Miao, C., Theng, Y., & Kot, A. (2011). iCLUB: integrated clustering-basedapproach improve robustness reputation systems. Proceedings InternationalConference Autonomous Agents Multiagent Systems (AAMAS).Mitchell, T. M. (1997). Machine learning. McGraw-Hill.Noorian, Z., Marsh, S., & Fleming, M. (2011). Multi-layer cognitive filtering behavioral modeling. Proceedings International Conference Autonomous Agents MultiagentSystems (AAMAS).Quinlan, J. R. (1986). Induction decision trees. Machine learning, 1(1), 81106.Quinlan, J. R. (1996). Improved use continuous attributes C4.5. Journal Artificial Intelligence Research (JAIR), 4(1), 7790.504fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACESQuinlan, J. R. (1993). C4. 5: Programs machine learning, Vol. 1. Morgan kaufmann.Regan, K., Poupart, P., & Cohen, R. (2006). Bayesian reputation modeling e-marketplaces sensitive subjectivity, deception change. Proceedings National ConferenceArtificial Intelligence (AAAI).Sabater, J., & Sierra, C. (2005). Review computational trust reputation models. ArtificialIntelligence Review, 24(1), 3360.Soltani, S. (2013). Case-based reasoning diagnosis solution planning. Technical Report,Queens University.Sormo, F., Cassens, J., & Aamodt, A. (2005). Explanation case-based reasoningperspectivesgoals. Artificial Intelligence Review, 24(2), 109143.Tahir, M. A., Bouridane, A., & Kurugollu, F. (2007). Simultaneous feature selection featureweighting using hybrid tabu search/k-nearest neighbor classifier. Pattern Recognition Letters,28(4), 438446.Teacy, W., Patel, J., Jennings, N., & Luck, M. (2006). TRAVOS: Trust reputation contextinaccurate information sources. Autonomous Agents Multiagent Systems, 12, 183198.Vempala, S. S. (2012). Randomly-oriented k-d trees adapt intrinsic dimension. Proceedings32nd International Conference Foundations Software Technology TheoreticalComputer Science (FSTTCS).Wang, Y., Hang, C.-W., & Singh, M. P. (2011). probabilistic approach maintaining trust basedevidence. Journal Artificial Intelligence Research, 40(1), 221267.Watson, I. (1999). Case-based reasoning methodology technology. Knowledge-basedsystems, 12(5), 303308.Watson, I., & Marir, F. (1994). Case-based reasoning: review. Knowledge Engineering Review,9(4), 327354.Wess, S., Althoff, K.-D., & Derwand, G. (1994). Using k-d trees improve retrieval stepcase-based reasoning. Springer.Wettschereck, D., & Aha, D. W. (1995). Weighting features. Case-based Reasoning ResearchDevelopment, 347358.Whitby, A., Jsang, A., & Indulska, J. (2004). Filtering unfair ratings bayesian reputationsystems. Proceedings AAMAS Workshop Trust Agent Societies (TRUST).Yang, Y., Feng, Q., Sun, Y. L., & Dai, Y. (2008). RepTrap: novel attack feedback-basedreputation systems. Proceedings International Conference Security PrivacyCommunication Networks (SecureComm).Yu, B., & Singh, M. (2003). Detecting deception reputation management. ProceedingsInternational Joint Conference Autonomous Agents Multiagent Systems (AAMAS).Zhang, J., & Cohen, R. (2008). Evaluating trustworthiness advice seller agents emarketplaces: personalized approach. Electronic Commerce Research Applications,7(3), 330340.Zhang, J. (2009). Promoting honesty e-marketplaces: Combining trust modeling incentivemechanism design. Ph.D. thesis, University Waterloo.505fiJournal Artificial Intelligence Research 52 (2015) 445-475Submitted 09/14; published 03/15Modeling Lifespan Discourse EntitiesApplication Coreference ResolutionMarie-Catherine de MarneffeMCDM @ LING . OHIO - STATE . EDULinguistics DepartmentOhio State UniversityColumbus, OH 43210 USAMarta RecasensRECASENS @ GOOGLE . COMGoogle Inc.Mountain View, CA 94043 USAChristopher PottsCGPOTTS @ STANFORD . EDULinguistics DepartmentStanford UniversityStanford, CA 94035 USAAbstractdiscourse typically involves numerous entities, mentioned once. Distinguishing die one mention (singleton) lead longer lives(coreferent) would dramatically simplify hypothesis space coreference resolution models,leading increased performance. realize gains, build classifier predictingsingleton/coreferent distinction. models feature representations synthesize linguistic insightsfactors affecting discourse entity lifespans (especially negation, modality, attitudepredication) existing results benefits surface (part-of-speech n-gram-based)features coreference resolution. model effective right, feature representations help identify anchor phrases bridging anaphora well. Furthermore, incorporatingmodel two different state-of-the-art coreference resolution systems, one rule-basedlearning-based, yields significant performance improvements.1. IntroductionKarttunen imagined text interpreting system designed keep track individuals, is,events, objects, etc., mentioned text and, individual, record whatever said(Karttunen, 1976, p. 364). used term discourse referent describe abstract individuals.discourse referents easily mapped specific entities world, propernames. Others indeterminate sense compatible many different real-worldentities, indefinites like train. either case, discourse referents enter anaphoricrelations discourse; even know exactly real-world object train picksheard train distance . . . , nonetheless refer subsequent pronounsascribe properties (. . . loud horn).discourse referents enjoy repeat appearances discourse. lead long livesappear wide variety discourse contexts, whereas others never escape birthplaces,dying one mention. central question paper factors influencelifespan discourse referent. focus noun phrases, direct identifiersdiscourse referents English. specifically, seek predict whether given discoursec2015AI Access Foundation. rights reserved.fiDEARNEFFE , R ECASENS & P OTTSreferent coreferent (mentioned multiple times given discourse) singleton (mentionedonce). ability make distinction based properties noun phrases usedidentify referents (henceforth, mentions) would benefit coreference resolution models, simplifying hypothesis space consider predicting anaphoric links, could improveperformance tasks require accurately tracking discourse entities, including textual entailment (Delmonte, Bristot, Piccolino Boniforti, & Tonelli, 2007; Giampiccolo, Magnini, Dagan,& Dolan, 2007) discourse coherence (Hobbs, 1979; Grosz, Joshi, & Weinstein, 1995; Kehler,2002; Barzilay & Lapata, 2008; Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, & Webber, 2008).existing literature provides numerous generalizations relevant singleton/coreferentdistinction. known, example, internal syntax morphology phrase usedestablish discourse referent provide important clues lifespan referent (Prince,1981a, 1981b; Wang, McCready, & Asher, 2006). Information structuring also important; certaingrammatical discourse roles correlate long lifespans (Chafe, 1976; Hobbs, 1979; Walker,Joshi, & Prince, 1997; Beaver, 2004). Features based insights long integratedcoreference resolution systems. contribution explore interactionfeatures semantic operators like negation, modals, attitude predicates (know, certain,wonder). interactions Karttunens primary focus (Karttunen, 1973, 1976),long dominated work dynamic approaches linguistic meaning (Kamp, 1981; Heim, 1982,1992; Roberts, 1990; Groenendijk & Stokhof, 1991; Bittner, 2001). Here, highlight importance interactions predicting lifespans discourse referents actual text.approach also capitalizes results Durrett Klein (2013) Hall, Durrett,Klein (2014) concerning power surface features natural language processing (NLP)tasks. authors show large sets easily extracted part-of-speech (POS) n-grambased features achieve results least good achieved hand-engineeredlinguistic features. therefore investigate contribution surface features predictinglifespan discourse entities. find surface features alone substantial predictive valuetask, adding specialized linguistic features leads reliable performance gains.suggests linguistic constraints relevant lifespan prediction go beyondapproximated surface-level information given available data.first step analysis bring insights linguistic theories togethersingle logistic regression model lifespan model assess predictive power realdata. show linguistic features generally behave existing literature leads usexpect, model effective predicting whether given mention singletoncoreferent. second step bring surface features obtain predictive model.provide initial assessment engineering value making singleton/coreferent distinctionincorporating lifespan model two different, state-of-the-art coreference resolutionsystems: rule-based Stanford coreference system (Lee, Peirsman, Chang, Chambers, Surdeanu,& Jurafsky, 2011) learning-based Berkeley coreference system (Durrett & Klein, 2013).both, adding features results significant improvement precision CoNLL-2011CoNLL-2012 Shared Task data, across standardly used coreference resolution measures,see reliable boosts recall well.article subsumes extends work Recasens, de Marneffe, Potts (2013).specific differences follows. First, freed NAACLs tight space constraints, providemuch in-depth linguistic analysis various features lifespan model, includedetails throughout. Second, examine contribution surface features lifespan446fiM ODELING L IFESPAN ISCOURSE E NTITIESmodel. Third, assess value lifespan model predicting phrases actanchors bridging anaphora. Fourth, give fuller evaluation coreference applicationsmodel, incorporate best lifespan model learning-based system (the Berkeleycoreference system), complementing previous results rule-based Stanford coreferencesystem. Fifth, use recent version CoNLL scorer (v8.0), includes resultsaccording BLANC fixes bug incorrectly boosted B3 CEAF scores points.Sixth, benefit Kummerfeld Kleins (2013) error analysis tool gain deeper insightserrors lifespan model helps with.2. Linguistic Insightssection briefly summarizes previous research anaphora resolution, discourse structure,discourse coherence linguistic literature. goal obtain clear picturelifespan discourse referent shaped features mentions local morphosyntactic features also features syntactic semantic environmentsoccur. insights gather section inform design feature extraction functionslifespan model (Section 5) turn shape contributions Stanford Berkeleycoreference systems (Section 8).Karttunen (1976) primarily concerned ways semantic scopeindefinite influences lifespan associated discourse referent. three-sentence discourse(1), indefinite exam question sentence 1 text-level scope. result, associateddiscourse referent free lead long life, linking mention also text-level(sentence 2) one embedded negation (sentence 3).(1)Kim read exam question. hard. didnt understand it.contrast, Karttunen observed, indefinite interpreted scope negation,typically available anaphoric reference inside negative environment, (2),outside it, (3). (We use # mark discourses incoherent intended construal.)(2)Kim didnt understand exam question even reading twice.(3)Kim didnt understand exam question. # hard.course, (3) coherent construal exam question interpreted taking widescope respect negation (there question Kim didnt understand). inverse scopereadings often disfavored, become salient modifiers like certain particular included (Fodor & Sag, 1982; Schwarzschild, 2002), mention containspositive polarity item, is, item like tons resists scoping negationsemantically (Baker, 1970; Israel, 1996, 2001):(4)Kim didnt understand particular exam question. pondered hours avail.(5)Kim didnt understand exam question. pondered hours avail.Conversely, using negative polarity item (NPI) like inside indefinite mention essentially ensures narrow-scope reading (Ladusaw, 1996; Israel, 2004), leads impossibleto-resolve anaphoric link simple variants (3):(6)Kim didnt understand exam question. # hard.447fiDEARNEFFE , R ECASENS & P OTTSpattern Karttunen saw semantic scope anaphoric potential intimately related: given mention participate anaphoric relationships within scope,outside it. Broadly speaking, familiar quantificational binding logical languages(Cresswell, 2002) variable scope control structures programming languages (Muskens,van Benthem, & Visser, 1997). Thus, indefinite text-level scope free reign, whereas oneinside scope operator like negation restricted links span outer boundaries scopal environment. semantic generalizations might directlyreflected surface syntax, interpretive preferences internal morphosyntactic featuresmention help disambiguate intended logical form.Karttunen (1976) immediately generalized observations negation discourse reference modal auxiliaries non-factive attitude predicates like want claim. followingbased original examples:(7)Bill make kite. # long string.(8)John wants catch fish. # see here?(9)Sandy claims Jesse bought bicycle. # green frame.negation, pattern makes intuitive sense. Bills abilities regarding kite constructioninvolve specific kite, hence first sentence (7) automatically establishright sort discourse referent. Similarly, wanting catch fish guarantee salience (oreven existence) fish, Sandy might unreliable source bicycle statusoutside semantic scope claim.(7)(9) cohere indefinite interpreted outside scope relevant semanticoperator. relative preferences surface inverse scope harder characterizenegation, influenced complex ways semantics pragmaticsattitude predicate, reliability source information, nature conversational issues goals. example, speaker (9) regards Sandy reliable sourceregarding Jesses bike buying, bicycle likely attain text-level scope by-productJesse bought bicycle becoming text-level commitment. Karttunen (1973) discusses patterns, observing that, many contexts, pragmatic pressures encourage embedded content becomeelevated text level way. De Marneffe, Manning, Potts (2012) study newspaper dataextremely common pattern attitude verbs tend function evidential markers source embedded content (Rooryck, 2001; Simons, 2007).see later attitude predicates seem encourage long lifespans OntoNotes data (themajority news-like), arguably result pragmatic factors.far restricted attention anaphoric links indefinite establishes newdiscourse referent pronoun refers it. observations carry directly links indefinites definite noun phrases, linguistic theories treat roughly pronouns additionaldescriptive content (for discussion, see work Elbourne, 2008). mention-patterns tendquite different, though. discourse referents established definites named entities,interactions negation operators simpler definites named entitiesinteract scopally operators (but see work Aloni, 2000, related issuesinvolving presupposition intensionality). Thus, anaphoric connections unconstrainedfactors discussing. Conversely, truly quantified phrases like student every linguist severely limited, interaction operators also448fiM ODELING L IFESPAN ISCOURSE E NTITIESdeficiencies comes establishing discourse referents. casesexpressions establish new discourse referents, seem infrequent unusual (Wanget al., 2006).Cross-cutting considerations factors long central studies coreference anaphora within computational linguistics NLP. instance, animate nounsgenerally likely lead long discourse lives, whereas mentions refer abstract objects like quantities, percentages, measures tend singleton. assumestatistical patterns derive, narrow linguistic constraints, rather general cognitivebiases concerning people conceptualize discuss different kinds objects. However,evidence biases make way grammars specific languagesform morpho-semantic phenomena like obviation (Aissen, 1997) differential object marking(Aissen, 2003).syntactic environment phrases occur also modulate anaphoric potential hence lifespans. example, Prince (1981b) reports semantically indefinitephrases using this, guy back row, highly likely referredsubsequent clause. Similarly, Chafe (1976) shows information structuring choices alsopredictive whether given noun phrase serve antecedent later referential devices.also close correlations syntactic topic position leading long discourse life (Grosz et al., 1995; Beaver, 2004); focused evaluation ideas handlingcoreference, see work Beaver (2007).seek incorporate observations lifespan model. additional patterns literature pursue, infrequentdata. example, Karttunen (1976) also identified natural class counterexamples basic scope generalizations: certain sequences intensional predicates support exceptional anaphoriclinks, phenomenon later studied systematically heading modal subordination(Roberts, 1990, 1996):(10)Frank wants marry rich linguist. # kind.(11)Frank wants marry rich linguist. kind.addition, mentions inside parenthetical clauses less likely introduce long-term discoursereferents, due likelihood parenthetical clause conveys secondary contentcompared main clause hosts (Potts, 2005). Thus, anaphoric linksparentheticals possible (AnderBois, Brasoveanu, & Henderson, 2010; Potts, 2012), seemarise relatively rarely, valuable piece practical advice appositive-rich texts like scientificpapers unfortunately one could put action here.Karttunens observations helped set agenda dynamic approaches semantics nextdecades (Kamp, 1981; Heim, 1982; Groenendijk & Stokhof, 1991). literature refinedextended observations numerous ways. Taken together, findings suggest intensionaloperators negation interact complex ways discourse anaphora. default, expectphrases introduced scope operators lead short lifespans, possibletake wide-scope respect operators, broadens range anaphoric linksestablish. readings favored disfavored pragmatics situation welllexical syntactic nature phrases involved. follows, seek modelinteractions use inform lifespan model.449fiDEARNEFFE , R ECASENS & P OTTS3. Previous Engineering Efforts Quantitative Evaluationsinsights inspired NLP researchers try predict roles different mentionsplay coreference chains. Previous work area subdivided detecting fourdifferent targets: non-referential mentions, non-anaphoric mentions, discourse-new mentions,non-antecedent mentions. terminology always used consistent way linguistics NLP, believe results ultimately brought together. Here, aimclarify terminology find common insights behind various features used.first single singleton/coreferent detection task such, work findsimportant antecedents existing literature.3.1 Non-referential Mentionsnoun phrases refer discourse referent rather fill syntactic position.English, canonical example non-referential NP expletive pronoun it, obvioussucceed. lexical NPs introduce discourse referent either,linguist Pat linguist: mention Pat introduce discourse referent, linguistsimply predicates something her. Detecting non-referential uses plays role coreferenceresolution: since NPs pick discourse referents (new existing), cannot enteranaphoric relations kind consideration here.Early work non-referentiality detection focuses pronoun it, aiming distinguish referential uses non-referential ones. Paice Husk (1987) develop rule-based system, Evans(2001) uses supervised approach, Muller (2006) focuses use spoken dialog.studies mainly employ lexico-syntactic features immediate surrounding contextpronoun. Similarly, Bergsma, Lin, Goebel (2008) explore system uses Web-count features derived Google n-grams data (Brants & Franz, 2006) capture frequentsubjects replace pronoun it: referential cases (e.g., able to), wordsfrequent n-grams, able China able to, whereas non-referentialcases, pronoun likely frequent subject (e.g., important to).recently, Bergsma Yarowsky (2011) develop NADA system, improvesBergsma et al. (2008) incorporating lexical features. lexical features indicate presenceabsence strings specific positions around pronoun: three-grams five-gramsspanning pronoun; two tokens pronoun five tokens pronounpositions; token within twenty tokens right pronoun; token within tentokens left pronoun named entity belongs following list: that, this,and, said, says, it, It, its, itself. Using types features, lexical Web-count, achieve85% accuracy different datasets.Byron Gegg-Harrison (2004) apply linguistic insights highlighted Karttunen(Section 2) special case pronoun resolution, seeking discard non-referential indefiniteNPs set potential antecedents pronouns. use hard filter non-referentialmentions, looking presence indefinites, negation, apposition (hand-labeled), modals, adjectival phrases predication adjuncts (tagged -CLR Penn Treebank), predicates copularverbs (tagged -PRD), noun phrases express value. found removing nonreferential mentions gave small boost performance pronoun resolution.450fiM ODELING L IFESPAN ISCOURSE E NTITIES3.2 Non-anaphoric MentionsNon-anaphoric NPs whose interpretation depend previous mentiontext. example, phrase new Scorsese movie stars De Niro (12) (while manifestingmany kinds context dependence) depend overt phrases order capturedescriptive content. contrast, movie (13) crucially links back previous sentencedescriptive content; superficially involves predicate movie, construedadditional property seen speaker previous night.(12)Last night, watched new Scorsese movie stars De Niro.(13)Last night, watched movie read paper. movie directed Scorsese.direct correspondence anaphora coreferentiality. Coreferent mentionsnon-anaphoric (as text containing multiple tokens phrase White House),anaphoric mentions coreferent non-coreferent (van Deemter & Kibble, 2000). Casesbridging anaphora (Clark, 1975) like (14) involve non-coreferent anaphora. Here, ceilinginterpreted ceiling room mentioned previous sentence, thus anaphoricroom without coreferent phrase discourse.(14)looked room. ceiling high.return cases Section 6, use lifespan model characterize sensebridging anchors like room lead longer lifespans count strictly coreferentmentions would suggest.Poesio, Uryupina, Vieira, Alexandrov-Kabadjov, Goulart (2004) Poesio, AlexandrovKabadjov, Vieira, Goulart, Uryupina (2005) summarize previous approaches non-anaphoricitydetection, refer discourse-new detectors. Vieira Poesio (2000) focus definite NPs use syntactic heuristics based pre- post-modification distinguishanaphoric non-anaphoric NPs. Modification good indicator anaphoricity; heavily modified phrases like new Scorsese movie stars De Niro tend non-anaphoric, whereas shortphrases general descriptive content like movie tend anaphoric. Bean Riloff (1999)also focus definite NPs: addition syntactic heuristics based pre- post-modification,use techniques mining lists likely non-anaphoric NPs (such presence NPs firstsentence document). Compared Vieira Poesio (2000), obtain substantially higherrecall (with recall precision figures around 80%).non-anaphoricity detector, Poesio et al. (2005) use head feature (distanceNPs identical heads), syntactic features (e.g., occurring inside appositive copular clause,post-modified), capitalization mention, presence mention first sentenceWeb page, position mention text, probability mention definitecomputed Web using technique Uryupina (2003). find importantfeatures head feature definiteness probabilities.3.3 Discourse-New MentionsDiscourse-new mentions introduce new entity discourse (Prince, 1981b;Fraurud, 1990). entity might singleton involve chain coreferring mentionsfirst phrase discourse-new one rest considered discourse-old. Cast451fiDEARNEFFE , R ECASENS & P OTTSinformation status task, goal discourse-new mention detection find discourse referentspreviously available hearer/reader; e.g., see work Nissim (2006).Ng Cardie (2002) develop discourse-new classifier targets every kind NP usingvariety feature types: lexical (string head matching, conjunction), morpho-syntactic (definiteness, quantification, number), grammatical (appositional copular context, modifier structure,proper-noun embedding), shallow semantic (e.g., WordNet features). incorporateclassifier coreference resolution system, pre-filtering NPs tagged discoursenew. However, pre-filtering ultimately hurts coreference resolution system performance: eventhough precision increases, recall drops considerably. Section 8.2.3, report similar resultsmodel instantiated discourse-new pre-filtering, find recall dropavoided filtering applied mention analysis tagged discourse-newantecedent candidate tagged singleton.Ng Cardies (2002) work cast non-anaphoricity detection, model perhapsbetter described trying distinguish coreferent mentions singleton initiatecoreference chains. specifically, write, positive instance created NPinvolved coreference chain head chain (Ng & Cardie, 2002, p. 3),picks non-initial members coreference chains. Conversely, negative instance createdremaining NPs (Ng & Cardie, 2002, p. 3), i.e., without antecedents.Uryupina (2009) proposes discourse-new mention detector kind NP. classifierrelies features falling three categories defines: lexical (number words mention), syntactic (POS, number, person, determiner, pre- post-modification), semantic (gender, semantic class), salience (grammatical role, position sentence paragraph).addition, includes Karttunens features implemented Byron Gegg-Harrison(2004). classifier also checks mentions identical heads, distance these.syntactic head features deliver improvements majority baseline (which marksNP discourse-new), performing almost well features together. Uryupina notes,however, features, especially based Karttunens ideas,designed discourse-new mention detection.Ng Cardie (2002) Uryupina (2009) integrated discourse-new detectorcoreference resolution system pipeline manner. joint approach discourse-new detectioncoreference resolution, see work Denis Baldridge (2007).3.4 Non-antecedent MentionsUryupina (2009) observes, coreference resolution, matters fact NPsunavailable antecedents. therefore builds classifier marks NPs likely antecedentsnot. system based features discourse-new detector described(Section 3.3). non-antecedenthood detection, syntactic semantic features leadsignificant precision improvement majority baseline (which marks NP nonantecedent), syntactic features alone performing well features together.3.5 Approach: Singletonsmodel cross-cuts four categories. Unlike previous models non-referentality,restricted pronouns indefinite NPs, tries identify kind non-referential NPwell referential NP whose referent mentioned (i.e., singleton). Thus,452fiM ODELING L IFESPAN ISCOURSE E NTITIESDatasetDocsTokensTrainingDevelopmentTest2,8023433481.3M160K170KENTIONSCoreferent Singletons152,97418,85519,407181,27423,14023,657Table 1: CoNLL-2012 Shared Task data statistics. added singletons (noun phrases annotated coreferent), account 55% referents development set.non-referential NPs fall singleton class. hand, strict correspondence singleton/coreferent distinction non-anaphoric/anaphoric distinction,since anaphoricity based whether mention relies previous one interpretation,whereas singleton/coreferent divide based long lifespan entity is. Similarly,discourse-new mentions either coreferent singleton classification, dependingwhether entity mentioned not.terms feature representations, tried stay close possible Karttunensoriginal insights: extract features full syntactic parses, seeking remain faithfulunderlying semantic relationships involved, include feature interaction terms capturecomplex set dependencies reviewed Section 2. approach allows us evaluatelinguistic ideas quantitatively assess practical contributions full coreferencesystems.4. Datadata used throughout paper come CoNLL-2012 Shared Task data (Pradhan, Moschitti, Xue, Uryupina, & Zhang, 2012), included 1.6M English words OntoNotesv5.0 (Pradhan & Xue, 2009) several common layers annotation (coreference, parse trees,named-entity tags, etc.). OntoNotes corpus contains documents seven different domains:broadcast conversation (20%), broadcast news (13%), magazine (7%), newswire (21%), telephoneconversation (13%), weblogs newsgroups (15%), pivot text (11%). genresnews-like, exception pivot texts (which come New Testament)telephone conversations. used training, development, test splits defined sharedtask (Table 1). Since coreference annotations OntoNotes contain singleton mentions, automatically marked singleton noun phrases annotated coreferent.excluded verbal mentions.mark singleton noun phrases annotated coreferent, definitionsingletons includes non-referential noun phrases raining, president servedpresident two terms (Section 3.1). makes practical sense: starting pointcoreference resolution systems take noun phrases possible candidates coreferencesubsequently find clusters coreferent one another. phrasesaccurately identify singleton, phrases exclude clustering step,translate directly performance gains.453fiDEARNEFFE , R ECASENS & P OTTSReferents231402369Singleton279741523643614061923456-1011-1516-20>20MentionsFigure 1: Distribution referent lifespans 2012 OntoNotes development set.5. Predicting Lifespans Linguistic Featuresdescribe model predicting lifespan discourse referents using linguisticfactors proposed Section 2. model makes binary distinction discourse referentspart coreference chain (singleton) part one (coreferent).distribution lifespans data shown Figure 1.plot gives number entities associated single mention, number associatedtwo mentions, forth. fact singletons dominate data suggests binary singleton/coreferent division natural one. propensity toward singletons also highlightsrelevance detecting singletons coreference system. Following Bergsma Yarowsky(2011), use logistic regression model, shown perform well rangeNLP tasks. fit logistic regression model R (R Development Core Team, 2013) training data, coding singletons 0 coreferent mentions 1. Thus, throughout followingtables coefficient estimates, positive values favor coreferent mentions negative values favorsingletons. turn describing motivating features model.5.1 Morphosyntax MentionTable 2 summarizes features model concern internal morphology syntacticstructure mention, giving coefficient estimates. tables, indicated otherwise, coefficient estimates significant p < 0.001. use indicate significancep < 0.05, indicate estimates p 0.05. morphosyntactic features include type(pronoun, proper noun, common noun), animacy, named-entity tag, person, number, quantification (definite, indefinite, quantified), number modifiers mention. Manycommon coreference systems (Recasens & Hovy, 2009), model highlights influencelifespans. available, used gold annotations derive features, since primarygoal shed light relevance features claimed influence lifespans.454fiM ODELING L IFESPAN ISCOURSE E NTITIESmorphosyntactic features operationalized using static lists lexicons wellStanford dependencies output Stanford parser (version 2.0.3; de Marneffe, MacCartney, &Manning, 2006) gold constituent trees. features extracted following way:Type type feature captures whether mention pronoun, proper noun, common noun.value determined gold POS tag mention named-entity tag.Animacy set animacy values (animate, inanimate, unknown) using static list pronouns, named-entity tags (e.g., PERSON animate whereas LOCATION not), dictionarybootstrapped Web (Ji & Lin, 2009).Person Person values (1, 2, 3) assigned pronouns (identified POS tag), usingstatic list. Mentions pronouns get value 0.Number number value (singular, plural, unknown) based static list pronouns,POS tags, Bergsma Lins (2006) static dictionary, named-entity tags. (Mentions markednamed entity considered singular exception organizations,singular plural get value unknown.)Quantification discussed Section 2, indefinites definites given referentialsemantics pairs naturally discourse anaphora, whereas anaphoric possibilities trulyquantified terms restricted. operationalize quantification decide whether mentiondefinite, indefinite, quantified, use dependencies find possible determiners, possessors,numerical quantifiers mention. mention definite named entity,possessor (e.g., car Johns car definite), determiner definite (the), demonstrative,possessive. mention quantified numerical quantifier (e.g., two cars)determiner all, both, neither either. mentions indefinite.Number modifiers added feature counting many modifiers mention has, seeking capture correlation specificity referentiality. modifiers, counted adjectival,participial, infinitival, prepositional modifiers well relative clause modifiers, noun compounds, possessives. (Thus, four modifiers phrase modern multifunctionalbusiness center costing 60 million yuan.)Named entities model also includes named-entity features 18 OntoNotes entitytypes, NER = true non-named-entities. used gold entity-type annotation.Table 2 summarizes coefficient estimates obtain features. broad terms,picture one would expect taxonomy given new defined Prince (1981b)assumed throughout dynamic semantics (Kamp, 1981; Heim, 1982): pronouns depend anaphoricconnections previous mentions disambiguation thus likely coreferent.corroborated positive coefficient estimate Type = pronoun.quantified phrases participate discourse anaphora (Partee, 1987; Wang et al., 2006),accounting association quantifiers singletons (as measured negativecoefficient estimate Quantifier = quantified).negative coefficient indefinites initially surprising. seen Section 2, theoriesstretching back Karttunen (1976) say indefinites excel establishing new discourse entitiesfrequent participants coreference chains, association455fiDEARNEFFE , R ECASENS & P OTTSFeatureCoefficientFeatureCoefficientType = pronounType = proper nounAnimacy = inanimateAnimacy = unknownPerson = 1Person = 2Person = 3Number = singularNumber = unknownQuantifier = indefiniteQuantifier = quantifiedNumber modifiersNER = DATENER = EVENTNER = FACILITY1.171.891.360.391.040.131.620.610.171.431.250.391.832.892.94= GPENER = LANGUAGENER = LAWNER = LOCATIONNER = MONEYNER = NORPNER =NER = ORDINALNER = ORGANIZATIONNER = PERCENTNER = PERSONNER = PRODUCTNER = QUANTITYNER = TIMENER = WORK ART3.462.562.852.830.050.824.170.903.390.882.282.640.021.532.42NERTable 2: Internal morphosyntactic features lifespan model. indicates non-significant coefficient (p 0.05); sign indicates significant coefficient (p < 0.001).chains negative. return Section 5.3, argue interactions semanticoperators explain fact.behavior named-entity (NER) features closely aligned previous modelstheoretical discussion above. rule, named entities behave like Type = proper noun associating coreferent mentions. exceptions MONEY, ORDINAL, NORP (for nationalitiesreligions), PERCENT, QUANTITY, seem intuitively unlikely participate coreference chains. person, number, animacy features together suggest singular animatesexcellent coreferent noun phrases.one real surprise us concerns feature Number modifiers. Inspired observations Fodor Sag (1982) Schwarzschild (2002), expected feature positivelycorrelate coreferent. reasoning increased modification would likely resultincreased specificity, thereby making associated discourse referent identifiabledistinctive. opposite seems hold data. However, hesitate concludeoriginal hypothesis mistaken. Rather, suspect model insufficiently sensitiveinteractions modifier counts lexical semantics modifiers themselves.5.2 Grammatical Role MentionSynthesizing much work Centering Theory information structuring, hypothesizedcoreferent mentions likely appear core verbal arguments favor sentence-initial (topictracking) positions (Ward & Birner, 2004). capture insights, used grammaticalrelation mention given Stanford dependencies gold constituents, sentenceposition mention.456fiM ODELING L IFESPAN ISCOURSE E NTITIESFeatureCoefficientFeatureCoefficientSentence Position = endSentence Position = firstSentence Position = lastSentence Position = middlecoordination0.220.030.310.110.48Relation = noun argumentRelation =Relation = rootRelation = subjectRelation = verb argument0.560.670.610.650.32Table 3: Grammatical role features lifespan model. indicates non-significant coefficient(p 0.05); sign indicates significant coefficient (p < 0.001).Sentence position Sentence position determined based raw string: first indicatesmention first word sentence, end last word, begin, middle, lastindicate whether mention situated first, second, last third sentence, respectively.Relation distinguish among grammatical relations, check whether mention subject, adjunct (which includes prepositional objects, adverbial modifiers, temporal modifiers),verb argument (which includes direct indirect objects, clausal complements, adjectival complements attributes), noun argument (which includes relative clauses, appositions, possessives, noun compounds, adjectival modifiers).coordination also indicated whether mention conjunct see whetherinside coordinate phrase affects coreference ways go beyond grammatical rolecontaining phrase.coefficient estimates Table 3 support general hypotheses: arguments make good discoursereferents, subjects best all, whereas sentence-final positions disfavor coreference. addition,note model identifies negative correlation coordination coreference.5.3 Semantic Environment MentionTable 4 highlights complex interactions discourse anaphora semantic operators introduced Section 2. interactions focus logical semantics since Karttunen(1976), whose guiding observation semantic: indefinite interpreted inside scope negation, modal, attitude predicate generally unavailable anaphoric reference outsidescope operator. Heim (1992) also relates anaphoric properties NPs scope-takingentailments attitude predications.direct access semantic scope, expect syntactic scope correlatestrongly semantic scope. therefore used dependency representations define featurescapturing syntactic scope negation, modal auxiliaries, broad range attitude predicates(181 verbs 374 nouns Saur, 2008). Technically, given mention, producenegation, modal attitude verb feature according presence pre-defined negationmodality markers (such not, can, may) attitude predicates (e.g., accuse, allege, doubt, say)dependency path. example, NP relief given negation featurefinancial storm shows sign relief today, since scope sign. Similarly,mention scientific technological companies scope modal auxiliary would457fiDEARNEFFE , R ECASENS & P OTTSFeatureCoefficientPresence negationPresence modalityattitude verbAttitudeVerb * (Type = pronoun)AttitudeVerb * (Type = proper noun)AttitudeVerb * (Quantifier = indefinite)AttitudeVerb * (Quantifier = quantified)Modal * (Type = pronoun)Modal * (Type = proper noun)Modal * (Quantifier = indefinite)Modal * (Quantifier = quantified)Negation * (Type = pronoun)Negation * (Type = proper noun)Negation * (Quantifier = indefinite)Negation * (Quantifier = quantified)Negation * (Number modifiers)0.180.220.100.410.100.190.100.130.350.000.171.070.300.360.390.11Table 4: Semantic environment features interactions lifespan model. indicates nonsignificant coefficient (p 0.05); sign indicates significant coefficient (p < 0.001);indicates significance p < 0.05.attitude verb said firms Taiwan said would establish scientific technologicalcompanies zone, receives modal attitude verb features.Table 4 summarizes models semantic environment features interactions. interaction terms added model follow previous linguistic literature: expect scopesemantic operators (negation, modality attitude predicate) interact internal syntax mention, specifically type definiteness/quantification. resultsbeautifully aligned guiding linguistic hypotheses. First, negation modalitynegatively correlate coreference, expected given constraints impose lifespans.Interacting semantic features internal syntax mentions also yieldsexpected results: since proper names pronouns scope-taking, largely unaffectedenvironment features, whereas indefinites, affected scope, emerge evenrestricted, Karttunen others would predict.coefficient values attitude predicates interactions seem anomalous lightsemantics items. Section 2, noted non-factive attitude predicates like saycannot offer semantic guarantees mentions scope survive outside scope.might lead one think biased long-lived mentions, fact seeopposite. However, also observed pragmatic factors often facilitate exceptional anaphoricdependencies attitude predications. Karttunen (1973) referred leakinesspredicates information introduced scope seems often percolate text levelwide range contexts (Rooryck, 2001; Simons, 2007; Harris & Potts, 2009). Since lifespan458fiM ODELING L IFESPAN ISCOURSE E NTITIES# F EATURESL INGUISTICURFACEC OMBINEDC ONFIDENT12373,39373,51673,516INGLETONRecall Precision F180.280.281.156.077.579.980.889.878.880.080.969.0C OREFERENTRecall Precision F171.475.376.448.274.675.676.690.773.075.476.562.9ACCURACY76.378.079.052.5Table 5: Recall, precision, F1 accuracy three different sets features OntoNotesdevelopment set. C ONFIDENT C OMBINED model singleton predictedPr < 0.2 coreferent Pr > 0.8.model trained real usage data, surprising reflects pragmatic factors ratherlexical semantics (de Marneffe et al., 2012).noted earlier, features Table 4 standardly used coreference systems. Uryupina(2009) notes Karttunen features implemented (see Section 3) significantly improve performance discourse-new mention non-antecedent detectors. ContraryUryupina, adding features Table 4 model incorporates features describedTable 2 Table 3 results significantly better model (likelihood ratio test, p < 0.001).accuracy CoNLL-2012 development set also improves adding Karttunen features(McNemars test, p < 0.001).5.4 Resultshighlighted above, lifespan model built OntoNotes data confirms claimsKarttunen others concerning semantic operators interact specific kinds mention.novel quantitative evidence theories. model also successfully learns teasesingleton coreferent mentions apart, suggesting practical value NLP applications.first row Table 5 summarizes linguistic model performance development setOntoNotes data described Section 4, giving precision, recall, F1 measures singletoncoreferent mentions. accuracy model 76.3%. majority baseline, predictingmentions singletons, leads accuracy 55.1%.6. Extension Bridginglifespan model suggests new perspective bridging anaphora, discussed brieflySection 3.2 using example (14), repeated here:(15)looked room. ceiling high.anchor phrase room superficially singleton discourse, intuitive lifespanlonger: makes salient discourse referent ceiling room, ceilingsecond sentence refers to. bridging relationship keeps room alive discoursereferent, extending lifespan, though way read directly text. Togetherbasic tenets lifespan model, observations suggest testable hypothesis459fiDEARNEFFE , R ECASENS & P OTTSbridging: even bridging anchors superficially singleton (henceforth, singleton anchors),1lifespan model tend classify coreferent, since model designeddetect later mentions per se, rather capture abstract information rolesentities play discourse.OntoNotes contain annotations bridging anaphora, evaluating hypothesisstraightforward. However, Hou, Markert, Strube (2013) annotated 50 WSJ textsOntoNotes bridging information, yielding annotations 663 bridging anchors. these, 145singleton anchors sense identify (Section 4) thus used assessmodels ability detect abstract sense bridging anchors long-lived.Ideally, would simply run trained lifespan model examples. proves ineffective, though, (outside Hou et al.s data) OntoNotes annotations treat singletonanchors singleton, meaning trained lifespan model optimized data obscuredistinction interest. Nonetheless, expect feature representations form backbonelifespan model able distinguish true singletons singleton anchors given rightkind training data. small number relevant bridging annotations poses obstaclespursuing idea, sought navigate around follows: using annotated corpusHou et al., extract 145 singleton anchors sample additional 145 truesingletons documents (from total 5,804 cases). yields data setconfident makes relevant distinction. randomly divide data set 80%training data 20% testing data, conduct standard classifier evaluation. use logisticregression classifier, employing recursive feature elimination cross-validation (Guyon, Weston,& Barnhill, 2002), implemented Pedregosa et al. (2011), try find compact modeleffective small data set. model used `2 regularizer penalty 0.5, though`1 regularization changes penalty delivered essentially results,without recursive feature elimination step.train test sets small, performance varies greatly depending naturetrue singleton sample, repeat experiment 1,000 times average results.procedure, lifespan feature representations achieve mean F1 65% (standard error 0.002;mean precision 62%, mean recall 0.69%), indicating lifespan-based features sensitivedistinction singleton anchors true singletons. finding bolsters designlifespan feature representations also shows lifespan deeper abstractmerely counting referents. Given right kind annotations, believe model couldextended provide even fuller treatment bridging, governed partly mixlinguistic contextual factors (Hawkins, 1978; Prince, 1981b; Schwarz, 2009).7. Predicting Lifespans Surface FeaturesDurrett Klein (2013) Hall et al. (2014) showed that, tasks coreference resolutionparsing, large quantity surface-level information implicitly model linguistic features, also capture patterns data easily identified manually.Given large amount annotated data available OntoNotes corpus, might expectsufficient amount surface-level data capture linguistic insights hand-engineered1. bridging anchors also literal coreferent mentions, looked room. empty,ceiling high., room coreferent addition providing discourse support ceiling.set aside cases bridging experiments.460fiM ODELING L IFESPAN ISCOURSE E NTITIESlifespan model defined above. therefore tested model using POS tags n-gramsfares lifespan task.used following features surface model: lemmas words mention,POS tags words mention, POS tag head mention, lemmaPOS tags two words preceding following mention (with dummy BEGIN ENDwords mark beginning end sentences). suggested Durrett Klein (2013),features might capture information encoded NER tag, number, person, sentence position.surface models performance reported second row Table 5. modelsTable 5, `2 regularization penalty chosen via five-fold cross-validation training data.linguistic model, using tuned `2 regularization penalty rather default one makesalmost difference, substantially improves performance models features.additionally experimented different algorithms feature selection, foundresults invariably best, models, retained full sets features. lastrow table gives performance model combine linguisticsurface features evaluate whether surface features alone cover information capturedlinguistic features, whether linguistic features additional predictive value.surface model performs better linguistic-only model, especially coreferentcategory. However, small number linguistically-motivated features yields resultsrange obtained large number features surface model, mightimportance tasks small amount annotated data available,bridging experiment Section 6. (The obvious trade-off surface features easierspecify implement.) shown C OMBINED row Table 5, combined surfacefeature set, linguistically-motivated features give statistically significant boost performance.suggests surface features miss certain long-distance interactions discourseanaphora semantic operators interactions linguistic features explicitly encode.best model predicting lifespan combined one. Instead using standard 0.5threshold decision boundary, also make use full distribution returned logistic regression model rely confident decisions. resulting C ONFIDENT modelC OMBINED one predicts singleton Pr < 0.2 coreferent Pr > 0.8. threshold valuesreported best trade-off found precision score close 0.90 without losingmuch recall. expected, using highly confident model, increase precision,though cost recall. kind model preferred depend application; notedNg (2004) Uryupina (2009), incorporating lifespan model downstream NLPapplications, often want highly accurate predictions, favors model like C ONFIDENT.8. Application Coreference Resolutionassess value lifespan model NLP applications, incorporate bestfeature combination two state-of-the art coreference resolution systems: Stanford system(Lee et al., 2011) Berkeley system (Durrett & Klein, 2013). cases, originalmodel serves baseline, focus extent lifespan model contributesimprovements baseline. allows us quantify power effectivenesslifespan model two different systems rule-based one (Stanford) learning-basedone (Berkeley).461fiDEARNEFFE , R ECASENS & P OTTS8.1 Evaluation Measuresevaluate incorporation lifespan model coreference systems, use Englishdevelopment test sets CoNLL-2011 CoNLL-2012 Shared Tasks. AlthoughCoNLL shared tasks evaluated systems multi-mention (i.e., non-singleton) entities,still expect lifespan model help: stopping singletons linked multi-mentionentities, expect see increase precision. evaluation uses measures givenCoNLL scorer:MUC (Vilain, Burger, Aberdeen, Connolly, & Hirschman, 1995): Link-based metricmeasures many links gold system partitions common.B3 (Bagga & Baldwin, 1998): Mention-based metric measures proportion mentionoverlap gold predicted entities.CEAF-3 (Luo, 2005): Mention-based metric that, unlike B3 , enforces one-to-one alignment gold predicted entities.CEAF-4 (Luo, 2005): entity-based version metric.CoNLL (Denis & Baldridge, 2009; Pradhan, Ramshaw, Marcus, Palmer, Weischedel, & Xue,2011): Average MUC, B3 CEAF-4 .BLANC (Recasens & Hovy, 2011): Link-based metric takes mean coreferencenon-coreference links, thereby rewarding (but over-rewarding) singletons.use new CoNLL coreference scorer (Pradhan, Luo, Recasens, Hovy, Ng, & Strube, 2014,version 8.0), fixes bug previous versions concerning way gold predicted mentionsaligned evaluating automatically predicted mentions. new scorer modifyeither gold system output, implements measures originally proposed, extendsBLANC successfully handle predicted mentions, following Luo, Pradhan, Recasens, Hovy(2014).8.2 Incorporating Lifespan Model Stanford Coreference SystemStanford system highest-scoring system CoNLL-2011 Shared Task (Pradhanet al., 2011), also part highest-scoring system (Fernandes, dos Santos, & Milidiu,2012) CoNLL-2012 Shared Task (Pradhan et al., 2012). rule-based system includestotal ten rules (or sieves) entity coreference, exact string match pronominalresolution. sieves applied highest lowest precision, rule adding coreferencelinks. coreference resolution sieve, documents mentions traversed left right.prune search space, mention already linked another one previous sieve,mention first textual order considered subsequent sieves. Furthermore,mentions headed indefinite pronoun (e.g., some, other) start indefinitedeterminer (a, an) discarded antecedent exact string.mention compared previous mentions text coreferent antecedent found(according current sieve) beginning text reached. Candidates sorted usingleft-to-right breadth-first traversal parse tree, favors subjects syntactic saliencegeneral.lifespan model improve coreference resolution two different ways: (i) mentions classified singletons considered either antecedents coreferent, (ii) mentions462fiM ODELING L IFESPAN ISCOURSE E NTITIESclassified coreferent linked mention(s). successfully predicting singletons (i), enhance systems precision; successfully predicting coreferent mentions (ii),improve systems recall. focus (i) use lifespan model detectingsingletons. decision motivated two factors. First, given large number singletons(Figure 1), likely see gain performance discarding singletons. Second,multi-sieve nature Stanford coreference system make straightforward decideantecedent mention linked even know coreferent.integrate singleton model Stanford coreference system, depart previouswork letting sieve consider whether pair mentions coreferent mentionsclassified singletons C ONFIDENT model mentions named entity.this, discard 29% NPs consideration. Experiments development set yieldedhigher performance taking account named entities. Performance higherC ONFIDENT model TANDARD model.therefore use lifespan model help coreference resolution pre-filtering stepcoreference resolution, discarding mentions tagged singletons lifespan model. Previouswork incorporating non-referentiality discourse-new detection module pre-processingstep coreference resolution shown mixed results, discussed Section 3. generalarguments pipeline vs. joint approaches apply here: pipeline approaches prevent recoveringerrors earlier pipeline, joint approaches tend increase model complexity associatedoptimization challenges, easily allow separating different modules, makesfeature design error analysis difficult well. case, context Stanfordsystems sieve-architecture, natural add lifespan model pre-filtering step.8.2.1 R ESULTSTable 6 summarizes performance Stanford system CoNLL-2011 CoNLL-2012development test sets. evaluate incorporation lifespan model realistic setting,use automatic parses, POS NER tags provided CoNLL documents.scores automatically predicted mentions. baseline Stanford coreference system,w/ Lifespan system extended lifespan model discard singletons, explainedabove. Stars indicate statistically significant difference (Wilcoxon signed-rank test, p < 0.05)according jackknifing (10 partitions development set test set, balanceddifferent domains2 corpus). expected, lifespan model significantly increases precision(up +4.0 points) decreases recall (by 0.7 points). Overall, however, gain precisionhigher loss recall, obtain significant improvement 0.41.5 points F1score evaluation measures.8.2.2 E RROR NALYSISKummerfeld Klein (2013) provide useful tool automatically analyzing categorizingerrors made coreference resolution systems. tool identifies seven intuitive error types: spanerror, conflated entities (entity mentions corefer clustered together), extra entity(entities gold data added), extra mention (the system incorrectly introduces2. mentioned Section 4, OntoNotes corpus contains documents seven different domains coreferenceperformance shown vary highly depending domain (Pradhan et al., 2012).463fiDECoNLLF1Stanford2011 DEV SETBaselinew/ LifespanDiscourse-newRARNEFFE , R ECASENS & P OTTSMUCPF1RB3PF1RCEAF-4PF151.4952.23*51.5258.00* 55.97 56.9757.57 57.72* 57.65*56.30 58.98* 57.61*48.01* 49.81 48.8947.45 51.62* 49.45*45.51 52.33* 48.6854.27* 44.03 48.6253.46 46.27* 49.60*48.63 47.93* 48.282011 TEST SETBaseline50.55w/ Lifespan51.58*Discourse-new 51.26*60.09* 56.09 58.0259.75 58.32* 59.03*58.92 59.71* 59.31*47.57* 47.91 47.7447.06 50.18* 48.57*45.72 51.06* 48.25*52.28* 40.90 45.8951.42 43.50* 47.13*47.41 45.1* 46.222012 DEV SETBaselinew/ LifespanDiscourse-new55.2655.77*53.6361.36* 65.26 63.2560.99 66.70* 63.72*60.71 63.27 61.9648.35* 57.05 52.3447.87 58.57* 52.68*47.25 54.42 50.5853.86* 47.01 50.2053.10 48.91* 50.92*49.35 47.41* 48.362012 TEST SETBaseline53.31w/ Lifespan54.58*Discourse-new 53.0162.05* 61.35 61.7061.31 65.61* 63.39*61.22 62.73* 61.9748.00* 52.66 50.2246.91 57.05* 51.49*46.72 53.62* 49.9352.29* 44.36 48.0051.03 46.87* 48.86*48.38 45.92* 47.12(a)StanfordRCEAF-3PF1RBLANCPF12011 DEV SETBaselinew/ LifespanDiscourse-new57.11* 52.50 54.7156.55 54.43* 55.47*54.02 55.67* 54.8345.04* 46.84 45.1444.37 48.65* 45.85*42.59 49.57* 45.602011 TEST SETBaselinew/ LifespanDiscourse-new55.57* 49.56 52.3955.04 51.80* 53.37*53.253.08* 53.14*46.46* 47.51 46.1245.98 49.53* 47.06*44.87 50.82* 47.33*2012 DEV SETBaselinew/ LifespanDiscourse-new56.59* 57.22 56.9056.11 58.75* 57.40*55.00 56.18 55.5848.78* 56.47 51.9448.23 57.94* 52.36*48.11 54.12 50.732012 TEST SETBaselinew/ LifespanDiscourse-new56.12* 53.46 54.7654.98 56.69* 55.82*54.43 54.78* 54.6049.08* 54.48 50.8847.69 59.15* 52.28*47.95 55.81* 51.14*(b)Table 6: Performance Stanford system CoNLL-2011 CoNLL-2012 developmenttest sets. Scores (v8.0 CoNLL scorer) automatically predicted mentions,using CoNLL automatic annotations. Stars w/ Lifespan Discourse-newrows indicate significant difference baseline (Wilcoxon signed-rank test, p <0.05).464fiM ODELING L IFESPAN ISCOURSE E NTITIESErrorthey1Goldscientists1they1family2they2Extra entityvarious major Hong Kong mediamediaExtra mentionbookbookbookConflated entitiesSystemscientists1they1(a) Errors affecting precision.ErrorSystemscientists1they1family2they1Goldscientists1they1family2they2Missing entitynetworkMissing mentiontwo motherstwo motherstwo mothers lost loved onesDivided entity(b) Errors affecting recall.Table 7: Illustration error types provided Kummerfeld Kleins (2013) system: errorsmade Stanford coreference system CoNLL-2012 development set.mention coreferent cluster),3 divided entity (an entity split two differentclusters),4 missing entity (the system fails detect entity), missing mention (an entitymissing one mentions). Table 7 illustrates error types interested in,5 showing errorsmade Stanford system, separated affecting precision affecting recall.ran Kummerfeld Kleins (2013) system Stanford output quantify improvement obtained incorporating lifespan model coreference system CoNLL-2012development set. Figure 2 shows difference errors original Stanford coreferencesystem system lifespan model integrated. lifespan model generallyreduces errors affecting precision, notably getting rid spurious entities (Extraentity). top three errors Table 7 precision-related fixed integrating lifespan model Stanford system. hand, bottom two errors recall-related3. distinction two categories conflated entities extra mention makes sense corpus likeOntoNotes singletons annotated: former occurs system clusters one mentionsmulti-mention entity incorrect entity, whereas latter occurs system incorrectly clustersothers mention truly part singleton entity (and annotated gold).4. conflated-entities error divided-entity error often co-occur.5. span error category relevant comparison here: systems (with without lifespan) workpredicted mentions.465fiDEARNEFFE , R ECASENS & P OTTSStanford aloneConflated entitiesExtra entityExtra mentionlifespan897Stanford alone728lifespanStanford alone535lifespan523Stanford aloneDivided entitiesMissing entityMissing mention16351607lifespan20382021830Stanford alone877lifespanStanford alone1154lifespan1158Figure 2: Number errors Stanford coreference system (with without lifespanmodel) CoNLL-2012 development set.introduced lifespan model. However, cumulative gain error reduction across errorcategories results significant improvement overall coreference performance.8.2.3 U SING L IFESPAN ODEL ISCOURSE -N EW ENTION C LASSIFIERdiscussed Section 3.3, previous work (Ng & Cardie, 2002; Uryupina, 2009) reportsloss coreference resolution performance pre-filtering discourse-new mentions, i.e., singleton mentions well mentions start coreference chain. mimic pre-filtering,incorporate lifespan model Stanford system following way: mentionsmodel classify singletons considered every sieve hypothesized coreferprevious mention, discourse-new mentions removed consideration.so, also see performance loss, shown Discourse-new rows Table 6. clear significant gains across measures, compared performancestandard Stanford system (Baseline rows). improvements see Table 6 resultpre-filtering pairs mentions lifespan model classifies singletons. stricterconstraint seems balance loss pre-filtering many mentions early stage.8.3 Incorporating Lifespan Model Berkeley Coreference SystemBerkeley coreference system (Durrett & Klein, 2013; Durrett, Hall, & Klein, 2013) currentlyhighest scoring coreference system publicly available. uses mention-synchronousframework: mention, system either chooses one antecedent decides mentionstarts new cluster (perhaps leading singleton cluster). log-linear model featuresextracted mentions decide whether mentions anaphoric, featuresextracted pairs mentions decide whether pairs corefer. baseline compare466fiM ODELING L IFESPAN ISCOURSE E NTITIEStakes best feature set, FINAL one, reported Durrett Klein (2013),combines large number lexicalized surface features well semantic features.incorporate lifespan model Berkeley system, use probabilitiesmentions given lifespan model. pair mentions, add lifespan featuresadding lifespan probability mention. also add singleton feature mentionslifespan probability 0.2, coreferent feature mentions lifespanprobability 0.8. Unlike Stanford architecture, exploiting coreferent predictionsstraightforward (Section 8.2), learning-based setup Berkeley system allows usmake use lifespan probabilities without focusing singleton-class prediction.Instead incorporating lifespan probabilities lifespan model, also tried addingBerkeley system features lifespan model already present Berkeleysystem (i.e., features Table 3 Table 4). However, lead significantimprovements CoNLL 2012 development data, CoNLL 2012 test data.Moreover, overall results less good incorporating probabilities mannerdescribed above.8.3.1 R ESULTSTable 8 shows results Berkeley system CoNLL 2011 2012 developmenttest sets. Stanford system, scores automatically predicted mentions.use automatic POS tags, parse trees, NER annotations provided CoNLL datatraining testing. restrict training training data only.6 baseline FINALBerkeley coreference system, w/ Lifespan system extended lifespan,singleton coreferent features, explained above. Significance computed wayStanford system (we created 10 partitions development set test set, balanceddifferent domains corpus).learning-based context Berkeley system, lifespan model increases precisionwell recall, leading final improvement CoNLL score 1.0 2.0 points. Sinceuse lifespan model predicting singleton coreferent mentions, manage improve precision recall. provides additional empirical support splitting coreferenceresolution entity-lifespan task predicts mentions refer long-lived entitiesdiscourse coreference task focuses establishing coreference linksmentions.8.3.2 E RROR NALYSISParallel analysis Stanford coreference system output, ran Kummerfeld Kleins(2013) system Berkeley output. Figure 3 shows difference errors original Berkeley coreference system (FINAL feature set) system enhanced lifespanmodel. enhanced system commits fewer errors affecting precision (upper part Figure 3),6. also tried training gold POS tags, parse trees, NER annotations provided CoNLL data,using automatic annotations test time. make difference original Berkeley system.incorporating linguistic features (either lifespan probabilities features lifespan modelalready Berkeley system), setting lead significant improvements baseline. However,improvements hold consistently across development test sets: compared results obtainedtraining automatic annotations, training gold improves performance linguistically informed systemstest set.467fiDEBerkeleyCoNLLF1RARNEFFE , R ECASENS & P OTTSMUCPF1RB3PF1RCEAF-4PF12011 DEV SETBaseline59.72w/ Lifespan 61.03*62.67 70.22 66.2364.78* 72.24* 68.30*52.19 62.54 56.9054.65* 63.28* 58.65*53.77* 58.43 56.0052.89 59.83* 56.152011 TEST SETBaseline59.06w/ Lifespan 59.65*64.14 71.68 67.7064.96* 73.29* 68.87*50.81 61.31 55.5651.78* 62.38* 56.59*51.66* 56.34 53.9049.89 57.62* 53.482012 DEV SETBaseline61.49w/ Lifespan 63.42*69.06 71.32 70.1770.76* 74.30* 72.49*57.10 60.55 58.7859.35* 62.79* 61.02*55.20* 55.80 55.5054.74 58.94* 56.76*2012 TEST SETBaseline61.06w/ Lifespan 62.15*69.17 71.96 70.5470.42* 74.07* 72.20*55.77 60.50 58.0456.87* 62.21* 59.42*53.82* 55.37 54.5852.64 57.20* 54.83(a)BerkeleyRCEAF-3PF1RBLANCPF12011 DEV SETBaselinew/ Lifespan58.82 65.37 61.9259.29* 66.36* 62.63*50.38 59.93 54.7352.83* 62.92* 57.37*2011 TEST SETBaselinew/ Lifespan56.7156.3763.01 59.7063.96* 59.9349.11 59.67 53.8850.66* 61.87* 55.68*2012 DEV SETBaselinew/ Lifespan62.2962.6564.01 63.1466.18* 64.37*60.32 60.79 60.5362.19* 63.80* 62.86*2012 TEST SETBaselinew/ Lifespan60.83 63.12 61.9561.05* 64.68* 62.81*57.70 61.79 59.6858.92* 63.93* 61.32*(b)Table 8: Performance Berkeley system CoNLL 2011 CoNLL 2012 developmenttest sets. Scores (v8.0 CoNLL scorer) automatically predicted mentions,using CoNLL automatic annotations. Stars indicate significant difference (Wilcoxonsigned-rank test, p < 0.05).significantly category. However, cumulative gains result significantimprovement overall precision. Globally, lifespan model fixes errors brings in.468fiM ODELING L IFESPAN ISCOURSE E NTITIESBerkeley aloneConflated entitiesExtra entityExtra mentionlifespanBerkeley alonelifespan579533594Berkeley alonelifespan508Berkeley aloneDivided entitiesMissing entityMissing mention14481412lifespanBerkeley alone818lifespan820Berkeley alonelifespan16691572829941Figure 3: Number errors Berkeley coreference system (with without lifespanmodel) CoNLL 2012 development set.9. Conclusionfactors determine fate given discourse referent? nature (its internal morphosyntax) nurture (the broader syntactic semantic environments mentions)? lifespanmodel (Section 5) suggests nature, nurture, interactions important. modelvalidates existing linguistic generalizations discourse anaphora (Section 2), provides newinsights previous engineering efforts similar direction (Section 3). also showlinguistically-motivated features bring improvement top surface features (Section 7), demonstrating automatic language processing rely machine learning big data.lifespan model performs well right, achieving 79% accuracy predicting whethergiven mention singleton coreferent. alone could ramifications tracking topics,identifying protagonists, discourse coherence. paper, demonstrated benefitslifespan model coreference resolution. incorporated lifespan model twodifferent coreference resolution systems showed yields improvements practicalstatistical significance cases (Section 8).Stepping back, hope provided compelling illustration efforts theoreticallinguistics NLP complement other, developing models assessingscientific engineering contexts.Acknowledgmentsthank Jefferson Barlew, Greg Durrett, Micha Elsner, Gregory Kierstead, Craige Roberts, MichaelWhite, Stanford NLP Group, anonymous reviewers helpful suggestions earlier drafts paper. research supported part ONR grant No. N00014-10-1-0109ARO grant No. W911NF-07-1-0216.469fiDEARNEFFE , R ECASENS & P OTTSReferencesAissen, J. (1997). syntax obviation. Language, 73(4), 705750.Aissen, J. (2003). Differential object marking: Iconicity vs. economy. Natural LanguageLinguistic Theory, 21(3), 435483.Aloni, M. (2000). Quantification Conceptual Covers. Ph.D. thesis, University Amsterdam.AnderBois, S., Brasoveanu, A., & Henderson, R. (2010). Crossing appositive/at-issue meaningboundary. Li, N., & Lutz, D. (Eds.), Proceedings Semantics Linguistic Theory 20,pp. 328346. CLC Publications.Bagga, A., & Baldwin, B. (1998). Algorithms scoring coreference chains. ProceedingsLREC 1998 Workshop Linguistic Coreference, pp. 563566.Baker, C. L. (1970). Double negatives. Linguistic Inquiry, 1(2), 169186.Barzilay, R., & Lapata, M. (2008). Modeling local coherence: entity-based approach. Computational Linguistics, 34(1), 134.Bean, D. L., & Riloff, E. (1999). Corpus-based identification non-anaphoric noun phrases.Proceedings 37th Annual Meeting Association Computational Linguistics,pp. 373380. ACL.Beaver, D. (2004). optimization discourse anaphora. Linguistics Philosophy, 27(1),356.Beaver, D. I. (2007). Corpus pragmatics: Something old, something new. Paper presentedannual meeting Texas Linguistic Society.Bergsma, S., & Lin, D. (2006). Bootstrapping path-based pronoun resolution. Proceedings21st International Conference Computational Linguistics 44th Annual MeetingAssociation Computational Linguistics, pp. 3340. ACL.Bergsma, S., Lin, D., & Goebel, R. (2008). Distributional identification non-referential pronouns.Proceedings 46th Annual Meeting Association Computational Linguistics:Human Language Technologies, pp. 1018. ACL.Bergsma, S., & Yarowsky, D. (2011). NADA: robust system non-referential pronoun detection. Hendrickx, I., Lalitha Devi, S., Branco, A., & Mitkov, R. (Eds.), Anaphora ProcessingApplications, Vol. 7099 Lecture Notes Computer Science, pp. 1223. Springer.Bittner, M. (2001). Surface composition bridging. Journal Semantics, 18(2), 127177.Brants, T., & Franz, A. (2006). Google Web 1T 5gram corpus version 1.1. LDC2006T13.Byron, D. K., & Gegg-Harrison, W. (2004). Eliminating non-referring noun phrases coreference resolution. Proceedings Discourse Anaphora Reference Resolution Conference, pp. 2126.Chafe, W. L. (1976). Givenness, contrastiveness, definiteness, subjects, topics, point view.Li, C. N. (Ed.), Subject Topic, pp. 2555. Academic Press.Clark, H. H. (1975). Bridging. Schank, R. C., & Nash-Webber, B. L. (Eds.), Theoretical IssuesNatural Language Processing, pp. 169174. ACM.470fiM ODELING L IFESPAN ISCOURSE E NTITIESCresswell, M. J. (2002). Static semantics dynamic discourse. Linguistics Philosophy, 25(56), 545571.de Marneffe, M.-C., MacCartney, B., & Manning, C. D. (2006). Generating typed dependencyparses phrase structure parses. Proceedings Fifth International ConferenceLanguage Resources Evaluation, pp. 449454. ACL.de Marneffe, M.-C., Manning, C. D., & Potts, C. (2012). happen? pragmatic complexityveridicality assessment. Computational Linguistics, 38(2), 301333.Delmonte, R., Bristot, A., Piccolino Boniforti, M. A., & Tonelli, S. (2007). Entailment anaphoraresolution RTE3. Proceedings ACL-PASCAL Workshop Textual EntailmentParaphrasing, pp. 4853.Denis, P., & Baldridge, J. (2007). Joint determination anaphoricity coreference resolutionusing integer programming. Human Language Technologies 2007: ConferenceNorth American Chapter Association Computational Linguistics; ProceedingsMain Conference, pp. 236243. ACL.Denis, P., & Baldridge, J. (2009). Global joint models coreference resolution named entityclassification. Procesamiento del Lenguaje Natural, 42, 8796.Durrett, G., Hall, D., & Klein, D. (2013). Decentralized entity-level modeling coreferenceresolution. Proceedings 51st Annual Meeting Association ComputationalLinguistics (Volume 1: Long Papers), pp. 114124. ACL.Durrett, G., & Klein, D. (2013). Easy victories uphill battles coreference resolution.Proceedings 2013 Conference Empirical Methods Natural Language Processing,pp. 19711982. ACL.Elbourne, P. (2008). Demonstratives individual concepts. Linguistics Philosophy, 31(4),409466.Evans, R. (2001). Applying machine learning toward automatic classification it. LiteraryLinguistic Computing, 16(1), 4557.Fernandes, E., dos Santos, C., & Milidiu, R. (2012). Latent structure perceptron feature induction unrestricted coreference resolution. Joint Conference EMNLP CoNLL- Shared Task, pp. 4148. ACL.Fodor, J. D., & Sag, I. A. (1982). Referential quantificational indefinites. LinguisticsPhilosophy, 5(3), 355398.Fraurud, K. (1990). Definiteness processing noun phrases natural discourse. JournalSemantics, 7(4), 395433.Giampiccolo, D., Magnini, B., Dagan, I., & Dolan, B. (2007). third PASCAL recognizingtextual entailment challenge. Proceedings ACL-PASCAL Workshop Textual Entailment Paraphrasing, pp. 19.Groenendijk, J., & Stokhof, M. (1991). Dynamic predicate logic. Linguistics Philosophy, 14(1),39100.Grosz, B. J., Joshi, A. K., & Weinstein, S. (1995). Centering: framework modeling localcoherence discourse. Computational Linguistics, 21(2), 203225.471fiDEARNEFFE , R ECASENS & P OTTSGuyon, I., Weston, J., & Barnhill, S. (2002). Gene selection cancer classification using supportvector machines. Machine Learning, 46(13), 389422.Hall, D., Durrett, G., & Klein, D. (2014). Less grammar, features. Proceedings 52ndAnnual Meeting Association Computational Linguistics (Volume 1: Long Papers),pp. 228237. ACL.Harris, J. A., & Potts, C. (2009). Perspective-shifting appositives expressives. LinguisticsPhilosophy, 32(6), 523552.Hawkins, J. A. (1978). Definiteness Indefiniteness. Croom Helm.Heim, I. (1982). Semantics Definite Indefinite Noun Phrases. Ph.D. thesis, UMassAmherst.Heim, I. (1992). Presupposition projection semantics attitude verbs. Journal Semantics,9(2), 183221.Hobbs, J. R. (1979). Coherence coreference. Cognitive Science, 3(1), 6790.Hou, Y., Markert, K., & Strube, M. (2013). Global inference bridging anaphora resolution.Proceedings 2013 Conference North American Chapter AssociationComputational Linguistics: Human Language Technologies, pp. 907917. ACL.Israel, M. (1996). Polarity sensitivity lexical semantics. Linguistics Philosophy, 19(6),619666.Israel, M. (2001). Minimizers, maximizers, rhetoric scalar reasoning. Journal Semantics, 18(4), 297331.Israel, M. (2004). pragmatics polarity. Horn, L., & Ward, G. (Eds.), HandbookPragmatics, pp. 701723. Blackwell.Ji, H., & Lin, D. (2009). Gender animacy knowledge discovery web-scale n-gramsunsupervised person mention detection. Proceedings 23rd Pacific Asia ConferenceLanguage, Information Computation, pp. 220229.Kamp, H. (1981). theory truth discourse representation. Groenendijk, J., Janssen,T. M. V., & Stockhof, M. (Eds.), Formal Methods Study Language, pp. 277322.Mathematical Centre.Karttunen, L. (1973). Presuppositions compound sentences. Linguistic Inquiry, 4(2), 169193.Karttunen, L. (1976). Discourse referents. McCawley, J. D. (Ed.), Syntax Semantics, Vol. 7:Notes Linguistic Underground, pp. 363385. Academic Press.Kehler, A. (2002). Coherence, Reference, Theory Grammar. CSLI.Kummerfeld, J. K., & Klein, D. (2013). Error-driven analysis challenges coreference resolution. Proceedings 2013 Conference Empirical Methods Natural LanguageProcessing, pp. 265277. ACL.Ladusaw, W. A. (1996). Negation polarity items. Lappin, S. (Ed.), Handbook Contemporary Semantic Theory, pp. 321341. Blackwell.472fiM ODELING L IFESPAN ISCOURSE E NTITIESLee, H., Peirsman, Y., Chang, A., Chambers, N., Surdeanu, M., & Jurafsky, D. (2011). Stanfordsmulti-pass sieve coreference resolution system CoNLL-2011 shared task. Proceedings 15th Conference Computational Natural Language Learning: Shared Task, pp.2834. ACL.Luo, X. (2005). coreference resolution performance metrics. Proceedings Human Language Technology Conference Conference Empirical Methods Natural LanguageProcessing, pp. 2532. ACL.Luo, X., Pradhan, S., Recasens, M., & Hovy, E. (2014). extension BLANC system mentions. Proceedings 52nd Annual Meeting Association ComputationalLinguistics, pp. 2429. ACL.Muller, C. (2006). Automatic detection nonreferential spoken multi-party dialog. Proceedings European Chapter Association Computational Linguistics, pp. 4956.ACL.Muskens, R., van Benthem, J., & Visser (1997). Dynamics. van Benthem, J., & ter Meulen, A.(Eds.), Handbook Logic Language, pp. 587648. Elsevier.Ng, V. (2004). Learning noun phrase anaphoricity improve coreference resolution: Issuesrepresentation optimization. Proceedings 42nd Annual Meeting AssociationComputational Linguistics, pp. 152159. ACL.Ng, V., & Cardie, C. (2002). Identifying anaphoric non-anaphoric noun phrases improvecoreference resolution. Proceedings 19th International Conference Computational Linguistics, pp. 17. ACL.Nissim, M. (2006). Learning information status discourse entities. Proceedings 2006Conference Empirical Methods Natural Language Processing, pp. 94102.Paice, C. D., & Husk, G. D. (1987). Towards automatic recognition anaphoric featuresEnglish text: impersonal pronoun it. Computer Speech & Language, 2(2), 109132.Partee, B. H. (1987). Noun phrase interpretation type-shifting principles. Groenendijk,J., de Jong, D., & Stokhof, M. (Eds.), Studies Discourse Representation TheoryTheory Generalized Quantifiers, pp. 115143. Foris Publications.Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M.,Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning Python. JournalMachine Learning Research, 12, 28252830.Poesio, M., Alexandrov-Kabadjov, M., Vieira, R., Goulart, R., & Uryupina, O. (2005).discourse-new detection help definite description resolution?. Proceedings 6th International Workshop Computational Semantics, pp. 236246.Poesio, M., Uryupina, O., Vieira, R., Alexandrov-Kabadjov, M., & Goulart, R. (2004). Discoursenew detectors definite description resolution: survey preliminary proposal.Harabagiu, S., & Farwell, D. (Eds.), ACL 2004: Workshop Reference ResolutionApplications, pp. 4754. ACL.Potts, C. (2005). Logic Conventional Implicatures. Oxford University Press.473fiDEARNEFFE , R ECASENS & P OTTSPotts, C. (2012). Conventional implicature expressive content. Maienborn, C., von Heusinger,K., & Portner, P. (Eds.), Semantics: International Handbook Natural Language Meaning, Vol. 3, pp. 25162536. Mouton de Gruyter.Pradhan, S., Luo, X., Recasens, M., Hovy, E., Ng, V., & Strube, M. (2014). Scoring coreference partitions predicted mentions: reference implementation. Proceedings52nd Annual Meeting Association Computational Linguistics, pp. 3035. ACL.https://github.com/conll/reference-coreference-scorers.Pradhan, S., Moschitti, A., Xue, N., Uryupina, O., & Zhang, Y. (2012). Conll-2012 shared task:Modeling multilingual unrestricted coreference ontonotes. Joint Conference EMNLPCoNLL - Shared Task, pp. 140. ACL.Pradhan, S., Ramshaw, L., Marcus, M., Palmer, M., Weischedel, R., & Xue, N. (2011). CoNLL2011 shared task: Modeling unrestricted coreference OntoNotes. ProceedingsFifteenth Conference Computational Natural Language Learning: Shared Task, pp. 127.ACL.Pradhan, S. S., & Xue, N. (2009). Ontonotes: 90% solution. Proceedings Human Language Technologies: 2009 Annual Conference North American Chapter Association Computational Linguistics, Companion Volume: Tutorial Abstracts, pp. 1112.ACL.Prasad, R., Dinesh, N., Lee, A., Miltsakaki, E., Robaldo, L., Joshi, A., & Webber, B. (2008).Penn Discourse Treebank 2.0. Proceedings Sixth International Language ResourcesEvaluation, pp. 29612968. European Language Resources Association.Prince, E. (1981a). inferencing indefinite NPs. Webber, B. L., Sag, I., & Joshi,A. (Eds.), Elements Discourse Understanding, pp. 231250. Cambridge University Press.Prince, E. F. (1981b). Toward taxonomy givennew information. Cole, P. (Ed.), RadicalPragmatics, pp. 223255. Academic Press.R Development Core Team (2013). R: Language Environment Statistical Computing. RFoundation Statistical Computing.Recasens, M., de Marneffe, M.-C., & Potts, C. (2013). life death discourse entities:Identifying singleton mentions. Human Language Technologies: 2013 Annual Conference North American Chapter Association Computational Linguistics, pp.627633. ACL.Recasens, M., & Hovy, E. (2009). deeper look features coreference resolution.Lalitha Devi, S., Branco, A., & Mitkov, R. (Eds.), Anaphora Processing Applications,Vol. 5847 Lecture Notes Computer Science, pp. 2942. Springer.Recasens, M., & Hovy, E. (2011). BLANC: Implementing Rand index coreference evaluation. Natural Language Engineering, 17(4), 485510.Roberts, C. (1990). Modal Subordination, Anaphora, Distributivity. Garland.Roberts, C. (1996). Anaphora intensional contexts. Lappin, S. (Ed.), Handbook Contemporary Semantic Theory, pp. 215246. Blackwell.Rooryck, J. (2001). Evidentiality, Part II. Glot International, 5(5), 161168.474fiM ODELING L IFESPAN ISCOURSE E NTITIESSaur, R. (2008). Factuality Profiler Eventualities Text. Ph.D. thesis, Brandeis University.Schwarz, F. (2009). Two Types Definites Natural Language. Ph.D. thesis, UMass Amherst.Schwarzschild, R. (2002). Singleton indefinites. Journal Semantics, 19(3), 289314.Simons, M. (2007). Observations embedding verbs, evidentiality, presupposition. Lingua,117(6), 10341056.Uryupina, O. (2003). High-precision identification discourse new unique noun phrases.Proceedings 41st Annual Meeting Association Computational LinguisticsStudent Research Workshop, pp. 8086. ACL.Uryupina, O. (2009). Detecting anaphoricity antecedenthood coreference resolution. Procesamiento del lenguaje natural, 42, 113120.van Deemter, K., & Kibble, R. (2000). coreferring: Coreference MUC related annotationschemes. Computational linguistics, 26(4), 629637.Vieira, R., & Poesio, M. (2000). empirically based system processing definite descriptions.Computational Linguistics, 26(4), 539593.Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). model-theoreticcoreference scoring scheme. Proceedings 6th Message Understanding Conference,pp. 4552. Morgan Kaufman.Walker, M. A., Joshi, A. K., & Prince, E. F. (Eds.). (1997). Centering Discourse. Oxford University Press.Wang, L., McCready, E., & Asher, N. (2006). Information dependency quantificational subordination. von Heusinger, K., & Turner, K. (Eds.), Semantics Meets Pragmatics, pp.267304. Elsevier.Ward, G., & Birner, B. (2004). Information structure non-canonical syntax. Horn, L. R., &Ward, G. (Eds.), Handbook Pragmatics, pp. 153174. Blackwell Publishing Ltd.475fiJournal Artificial Intelligence Research 52 (2015) 331-360Submitted 01/15; published 02/15Scheduling Conservation Designs Maximum Flexibilityvia Network Cascade OptimizationXueAlan Fernxue@eecs.oregonstate.eduafern@eecs.oregonstate.eduSchool EECS, Oregon State UniversityCorvallis, 97331 USADaniel Sheldonsheldon@cs.umass.eduSchool Computer Science, University MassachusettsAmherst, 01003, USAAbstractOne approach conserving endangered species purchase protect set landparcels way maximizes expected future population spread. Unfortunately,ideal set parcels may cost beyond immediate budget constraintsmust thus purchased incrementally. raises challenge decidingschedule parcel purchases way maximizes flexibility budget usagekeeping population spread loss control. paper, introduce formulationscheduling problem rely knowing future budgets organization.particular, consider scheduling purchases way achieves population spreadless desired delays purchases long possible. schedules offer conservationplanners maximum flexibility use available budgets efficient way.develop problem formally stochastic optimization problem network cascademodel describing commonly used model population spread. solution approachbased reducing stochastic problem novel variant directed Steiner treeproblem, call set-weighted directed Steiner graph problem. showproblem computationally hard, motivating development primal-dual algorithmproblem computes feasible solution bound qualityoptimal solution. evaluate approach real synthetic conservation datastandard population spread model. algorithm shown produce near optimalresults much scalable generic off-the-shelf optimizers. Finally,evaluate variant algorithm explore trade-offs budget savingspopulation growth.1. IntroductionReserve site selection key problem conservation planning planners select landregions designated nature reserves, either achieve general conservation goalspreserving biodiversity, achieve specific goals supporting recoveryendangered species. general, problem extremely complex involves reasoninginterplay uncertain population spread, uncertain future budgets,problem specific factors. particular, properly assessing population spread involvesreasoning spatial aspects landscapes sizes, shapes, connectivity.Further, decision space huge, consisting possible land investment combinationstime.c2015AI Access Foundation. rights reserved.fiXue, Fern, & SheldonGiven factors, would highly desirable conservation practitionersenhance decision making via automated, semi-automated, planning schedulingalgorithms. Unfortunately, problem beyond scope existing off-the-shelf stochastic planners schedulers. largely due combination enormous stateaction spaces, highly uncertain, exogenous dynamics, need spatio-temporalreasoning. main contribution paper make progress toward handlingcomplexities studying useful subproblem conservation planning usedpractitioners realistic scenarios. general schema used develop algorithmwidely applicable (see Section 7) one received significant attentionAI community. Thus, hope work also inspire new specializedgeneral-purpose approaches complex stochastic planning/scheduling problems.Recently, Sheldon et al. (2010) studied restricted, still challenging, versionconservation planning problem, refer upfront conservation designoptimization. problem, planner given upfront budget stochasticmetapopulation model (Hanski & Ovaskainen, 2000) describes speciesconsideration spread throughout landscape available habitat. additionsystem given information costs potential land parcels availablepurchase conservation. objective select set land parcels immediatelypurchase conserve, subject budget constraint, maximize spreadpopulation within specified time horizon.key simplification present problem land parcels assumedpurchased upfront currently available budget. advantage simplificationallows reasonably efficient near optimal solution approach (Sheldonet al., 2010). However, upfront simplification limits utility approachnumber ways. First, conservation budgets generally arrive increments time,unrealistic purchase large set parcels advance restricting small setparcels using current budget may suboptimal long run. Moreover,often unnecessary purchase parcels spatially remote current populationspecies spread enough make relevant population growth.Second, upfront simplification requires planners commit advance conservationstrategies may take many years play out, ignores potential advantageobserving responding stochastic outcomes population spreading processunfolds. example, population spread observed, may beneficial divertmoney failed subpopulations purchase parcels near thriving populations.contrast upfront planning, ideal approach would fully adaptive planning,where, regular decision epochs, planner would make purchase decisions basedrecent population budgetary information. Unfortunately, currently-availableadaptive planning tools scale realistic conservation scenarios. duecombination enormous state space (possible population purchase configurations),enormous action space (possible subsets land parcels purchase), long horizons (tenshundreds years), high degree stochasticity population spread model.Given challenge arriving fully adaptive solution, main contributionpaper introduce problem strikes important middle-groundupfront fully adaptive approaches. particular, consider conservation designscheduling exploring trade-off future population cost,332fiScheduling Conservation Designs Maximum Flexibilitygiven initial conservation design (i.e. set parcels purchase) askedschedule purchase time parcel way (1) achieves population spreadtime horizon within arbitrary tolerance population loss, (2) maximizespurchase flexibility delaying specified purchase time (i.e. purchase deadline)parcel long possible.problem formulation simplifies fully adaptive problem number ways.First, set parcels purchased provided input, removes degreefreedom planning problem. Second, significantly, order selectaction current time step, general case fully adaptive planning requirescomputing policy dictates possible future contingency, leastreasonably likely ones. contrast, space possible schedules, focus here,much smaller space policies even partial policies. allows compactencoding scheduling problem, appear possible problemcomputing full, even, partial policies. distinction fully adaptivescheduling setting akin distinction closed-loop open-loop planning,generally computing closed-loop plans considered difficult openloop plans large stochastic problems.solution scheduling problem yields useful tool conservation planners,first develop conservation designs capture complex decision-makingobjectives, perhaps optimization software, schedule purchases obtainefficient cost-effective implementation design. conservation plannerflexibility purchase parcels time schedule-specifieddeadlines, knowing population spread hurt much purchasedelays.addition, scheduling problem potentially used componentadaptive planner. common successful approach many adaptive planning problemsreplanning, decision epoch non-adaptive plan computed currentstate first actions executed. work enables replanning approachdecision epoch first computes upfront design using existing work (e.g. Sheldonet al., 2010), computes schedule purchases parcels scheduledpurchased immediately. purchase strategy would spend minimum amountbudget step guaranteeing limited loss population spread.addition introducing formalizing problem conservation design scheduling, second contribution paper develop principled algorithm solvingit. key idea apply Sample Average Approximation (SAA) approach (Shapiro,2003) order arrive novel deterministic optimization problem, develop principled solution motivation special case. particular,approximated loss tolerance ratio 0, deterministic optimization problem onenetwork cascade optimization, show equivalent novel variant directedSteiner tree problem. traditional Steiner tree problem, graph edges associatedcosts, objective compute Steiner tree minimum cumulative edgecost. variant, set-weighted directed Steiner graph problem, costs associated sets edges (possibly non-disjoint) rather individual edges. showproblem computationally hard even restrictions traditional problemadmits efficient solution. present efficient primal-dual algorithm,333fiXue, Fern, & Sheldonguaranteed compute feasible solution bound quality optimalsolution. early-stopping version algorithm provides natural approachexplore trade-off future population budget flexibility.experiments real synthetic data Red-cockaded Woodpeckerconservation problem show primal-dual algorithm produces near optimal resultsmuch scalable standard optimization tools (CPLEX). also showtrade-off population budget allows flexibility purchasing landparcels.follows, Section 2 first presents related work, followed problem formulation Section 3. Section 4 shows reduce subproblem set-weighteddirected Steiner graph problem. Section 5 derives corresponding primal-dual algorithmnatural extension trade-off problem. Experiments presented Section 6.Finally conclude discuss future work.2. Related WorkPreviously, many different algorithms proposed select reserve sites formulating numerical measure reserve quality (together possible additionconstraints reserve must satisfy) solving optimal set sitesproposed model (e.g. see review article, Williams, Revelle, & Levin, 2005). Althoughearliest reserve site selection algorithms largely ignored spatial considerations, manynewer models incorporate spatial objectives constraints directly optimizationproblems. Williams, Revelle, Levin argue primary reason importancespatial attributes fact capture properties landscape favorableunderlying population dynamics, important, computationally difficult, research direction directly optimize respect model populationdynamics instead using spatial attributes proxy. directionfollowing paper addressing problem spatial conservation planningrespect specific widely adopted model population dynamics.recent approach explicitly reasons population dynamics modelwork Sheldon et al. (2010) upfront conservation design problem, describedSection 1. order cope stochasticity model, popular SampleAverage Approximation (SAA) approach employed transform stochastic problemdeterministic combinatorial optimization problem. problem encodedMixed Integer Program (MIP) solved using state-of-the-art solvers.approach able solve reasonably large problems via various speedup techniques,scalability still limited relatively small number sample scenarios used SAA,controls accuracy approach. Kumar et al. (2012) addressedaspect approach. Lagrangian relaxation used decompose SAA problemindependent subproblems could solved practical time frame, possiblyparallel, standard optimizers. shown significantly reduce runtimedependence number SAA samples used.Unfortunately, directly extending approach compute multi-stage adaptivesolutions, budget arrives increments time, seem practical. Oneattempt two-stage problems considered Ahmadizadeh et al. (2010).334fiScheduling Conservation Designs Maximum Flexibilityexplore re-planning using two-stage non-adaptive problem formulation, findindeed offer advantages upfront planning. setting, budget splitdecision epochs manually fixed. Unlike work, work explicitly separatesdecision parcels buy decision buy (the focuswork), may develop efficient special-purpose algorithms latter problemscale much easily bigger problems stages.several existing approaches might considered fully adaptivesolution conservation problem. example, fully adaptive problemencoded Markov Decision Process (MDP), resulting state action spaceswould far big state-of-the-art solvers. instance, recent advances solvinglarge spatio-temporal MDPs (Crowley & Poole, 2011) require significant restrictionssolution space, acceptable application. existing approachstochastic planning successfully applied Bent et al. (2004), Changet al. (2000), Chong et al. (2000) Yoon et al. (2008), Hindsight Optimizationsamples future outcomes optimistically estimates state value baseddetermined futures. However, action space huge, approach wouldcomputational problems current algorithms require enumeration candidateactions approximating state value. Another approach would formulateadaptive planning problem multi-stage stochastic integer program. However, sizeproblem formulation scales exponentially number stages, runningtime already costly single stage (Sheldon et al., 2010), two-stage problemsimpler setting fully adaptive (Ahmadizadeh et al., 2010).Recently, Golovin et al. (2011) proved simple greedy planning strategy providesnear-optimal solutions adaptive conservation setting first appears similarours. However, order provide approximation guarantees, authors restrictpopulation dynamics spread occurs distinct land parcels. mayreasonable assumption slow-moving species certain insects,focus work, ignores critical aspects population dynamics highly-mobileanimals birds, including Red-cockaded Woodpecker experimentsbased.3. Problem Formulationsection, first introduce basic terminology conservation design planningdefine main stochastic optimization problem. Next, describe Sample AverageApproximation (SAA) used transform problem deterministic optimizationproblem, focus remainder paper.3.1 Basic Conceptslargely follow formulation Sheldon et al. (2010). conservation problemsinvolve (large) land region interest divided land parcelssmallest land units available purchase. parcel contains number distincthabitat patches, atomic units population dynamics modeleither occupied unoccupied species interest. example, Redcockaded Woodpecker problem considered experiments, habitat patches correspond335fiXue, Fern, & Sheldonparticular trees prepared humans (or existing birds) facilitatenesting. parcel p cost c(p), denotes cost purchasing landrestoring conserving habitat patches suitable speciesoccupy.conservation design set parcels intended purchased conserved.Given conservation design D, purchase schedule mapping parcelspurchase times {0, 1, . . . , H, }, H time horizon interest purchasingparcel time = means parcel going purchased. Thus schedulermay choose purchase parcels even though part designrealize best tradeoff budget flexibility population spread. Althoughspecies population dynamics yearly time step model (described below),allowed purchase times (i.e. decision epochs) may less frequent depending specificproblem. upfront schedule one assigns parcels purchase time = 0.worth noting purchase times specified schedule best viewedpurchase deadlines. is, interpret schedule constraining purchases occurspecified times. view justified fact setup below,purchasing parcel earlier time specified never result worse populationspread.3.1.1 Population Dynamics Modeluse stochastic dynamics model Sheldon et al. (2010), instancepopular metapopulation model ecology literature (Hanski & Ovaskainen, 2000).patch two possible states time step, either unoccupied occupied,conserved patches may occupied. population dynamics consists two typesstochastic events. Colonization events occur population patch colonizesunoccupied patch b, happens probability pab . Extinction events occurpatch occupied time becomes unoccupied time + 1, happensprobability 1 paa . events independent. details probabilities usedexperiments given Section 6.single-step colonization probability pab experiments typically decaysdistance patches b, encodes spatio-temporal dynamicspopulations slowly spread source population new habitat made available.Thus, long-term planning population spread, often unnecessary purchaseparcels distant source population time = 0, since probabilitypopulation spreading distant patches near future negligible. delayingpurchases become relevant design (i.e. population spreadnearby), conservation organization use limited funds much flexibly. However,non-trivial decide much delay purchases harm spread, sincedecision depends much spatio-temporal details population spreadmodel. decision optimization problem defined designed make.336fiScheduling Conservation Designs Maximum Flexibility3.2 Stochastic Optimization Problemproblem statement rely two important concepts: 1) reward schedule,2) flexibility schedule. first define two concepts formulateoptimization problem terms them.reward schedule , denoted R(), random variable encodesamount population spread time H, simply count number occupiedpatches time H. easy show model upfront schedule alwaysachieves least much reward schedule thus maximizes expectedreward. Thus, define maximum expected reward R = E[R(upfront )].optimization goal find schedule almost achieves optimal expected reward,i.e. E[R()] (1 )R positive real number indicates percentagetolerance reward loss maximum purchase flexibility. knowupfront schedule achieves (1 )R , however, requires commitment expendituresfirst time step thus least flexible. Indeed, formalize notionflexibility terms expenditures time.Given schedule define corresponding cost curve C functionpurchase times accumulated cost, C (t) equal total cost parcelspurchased time 0 including time t. curve non-decreasingprovides view schedules spending profile time horizon. particular,profile cost curve C1 never C2 , i.e. total expenditures 1never exceed 2 , say 1 offers flexibility terms budgetmanagement compared 2 preferred else equal.define surrogate cost function schedulescostf () =Xc(p) f ((p))pparameterized function f times {0, . . . , H, } real numbers.require f () = 0 f parcel purchased within time horizonwould contribute surrogate cost. see surrogate cost functionsimply weighted sum parcel costs, weight determined f basedparcels purchase time. Although algorithm work real-valued function,assume henceforth f strictly decreasing. two reasons this. First,discounting future costs makes sense due economic factors inflation. Second,intuitively, since f decreases purchase times, minimizing respect costf wouldfavor schedules delay purchasing. particular, policy 1 cost profilenever greater 2 , 1 assigned lower surrogate cost fstrictly decreasing.parcels positive costs, upfront schedule unique elementmaximizes surrogate cost, schedule defers purchases time H givesunique minimum f strictly decreasing function. However, restrictschedules achieve least reward (1 )R , latter schedule excludedmay longer unique minimum.337fiXue, Fern, & Sheldonspecify problem conservation design scheduling, findschedule set possible schedules that:arg min costf () s.t. E[R()] (1 )R(1)is, schedules achieve reward (1 )R want return oneminimal terms surrogate cost (i.e. maximal flexibility). Thus, controlstrade-off flexibility reward. particular, using larger increasesset feasible schedules allows potential returning flexible schedulesacrificing reward.Note varying choice f may possible generate different solutionsEquation 1, minimal sense feasible policy strictlylower cost curve. experiments, use simple discounted f given f (t) =discount factor (0, 1).practice, likely conservation manager particular valuemind design time. Rather, best viewed parameter varied orderobserve different flexibility-reward trade-offs possible. final selectionschedule would based assessment possibilities.Finally, worth noting = 0 (no reward approximation), upfront solutionfeasible solution typical population spread models. Thus, using> 0 necessary achieving additional flexibility. requiringpolicy achieve expected reward exactly R (i.e., = 0) requires make purchasesaccommodate unlikely outcomes population spread model contribute tiny,positive, amount expected reward. example, consider outcomepopulation jumps initial location distant location first year,undergoes spread. vanishingly small, positive, probability.upfront schedule support population spread, since distant location purchasedfirst step. Thus, schedule purchase distant parcel firststep suffer tiny loss reward compared upfront schedule,achieve = 0. However, purchase tend useless vast majorityprobability mass.3.3 Deterministic Optimization Problemoptimization problem stochastic sense constraint definedterms expectation complicated population spread distribution. greatlycomplicates direct solution problem. prior work stochastic optimizationupfront schedules (Sheldon et al., 2010), address complication convertingstochastic problem approximately equivalent deterministic optimization problem.done via common Sample Average Approximation (SAA) approach (seeShapiro, 2003 survey results). key idea approximate stochasticoptimization problem using collection samples probability distribution,used approximate expectations probabilities via averages samples.problem formulation, sample corresponds so-called cascade scenario,particular realization population spread process time horizon.main idea behind application SAA generate set cascade scenarios338fiScheduling Conservation Designs Maximum Flexibilityprobabilistic population spread model, approximate expected rewardschedules average reward scenarios. scenarios combined singlescenario graph, illustrated Figure 1 explained detail remaindersection.concretely, cascade scenario layered graph, layers correspond timesteps, vertex va,t patch time step t. pair patches(a, b) time step t, coin flipped probability pab determine directededge (va,t , vb,t+1 ) present not. edge present patch occupied time(through previous colonizations non-extinctions), patch b colonizedbecome occupied time + 1, long conserved. is, presence edge(va,t , vb,t+1 ) interpreted meaning occupied time b conservedtime + 1, b occupied time + 1 particular scenario.way, cascade scenario graph encodes occupancy reachability. particular, assuming(for now) patches conserved, patch b occupied time exactlyvb,t reachable vertex va,0 corresponding initially occupied patch a.approximate probabilistic spread model, sample set N i.i.d. cascaden }. scenariosscenarios {C1 , . . . , CN }, denote vertices Cn {va,tcombined single scenario graph, additional root vertex rn ) vertex representing initially occupied patch a. Figure 1directed edges (r, va,0shows example scenario graph three scenarios range five time stepsinvolving three patches a, b, c, two parcels, one containing bcontaining c. example, initially occupied patch henceconnected time step zero root node r across three scenarios. Assumingparcels conserved (i.e. purchased upfront), vertex connected root node r,corresponding patch considered occupied corresponding timeparticular scenario. defined r connectedvertices initially occupied patches.Scenario graphs used work estimate reward schedules follows.n incomingGiven scenario graph, schedule said purchase node va,tedges patch purchased later time t, is, (p) belongsparcel p. Thus, purchasing parcel p time viewed purchasing verticesscenario graph, along incoming edges involve patches p occurlayer later. reflects fact patch purchased conserved,considered conserved hence eligible occupancy remainder timehorizon. Figure 1, example schedule shown purchases parcel p1 (containingb) time 0, parcel p2 (containing c) time 3. vertices purchasedschedule shown shaded region (purchased) incoming edgesshown bold.define conditions vertex scenario graph consideredn becomes occupied pathoccupied given schedule. Vertex va,tn . define variable X n (a, t) equal 1purchased edges r va,tnva,t occupied 0 otherwise. Figure 1, shaded red setvertices occupied example policy. example, note scenario 3,3 occupied since path r purchased edges.vertex vc,3despite fact path graph r, since path involves339fiXue, Fern, & SheldonFigure 1: Example scenario graph (N = 3) problem parcels p1 = {a, b}, p2 = {c}.schedule ((p1 ) = 0, (p2 ) = 3) also illustrated, using shaded boxesindicate purchased nodes heavy line weights indicate purchased edges.Vertices representing occupied patches schedule colored red.unpurchased edges. Note upfront schedule would purchase node, sincevertices edges would considered purchased schedule.average reward schedule relative scenario graph built scenarios{C1 , . . . , CN } denoted follows.R() =N1 XX nX (a, H)Nn=1average across scenarios number occupied patches time H.Figure 1 average reward example schedule would 2. key propertyscenario graphs N R() converges E[R()] fixed. implies set schedules { : R() (1 )R(upfront )} convergesset { : R() (1 )R } N grows, set policies wishoptimize flexibility over. Further, policy , one use standard probabilityconcentration bounds (e.g. Chernoff bounds) show event |R() R()|probability mass decreases exponentially fast N grows. suggestsrelatively small number scenarios required reliably obtain tight approximationtrue expected reward policy. practice, however, important empiricallyvalidate approximation errors reasonable number scenarios usedapproximation.340fiScheduling Conservation Designs Maximum Flexibilitymotivates deterministic SAA formulation original stochastic optimization problem (1) flexibility optimized subject constraint basedempirical reward R. is, deterministic problem solve:arg min costf () s.t. R() (1 )R(2)R = R(upfront ).3.4 Overview Solution ApproachRecall stochastic optimization problem (1) setting = 0 resulted optimization problem would typically upfront schedule feasible solution.Rather, here, approximate SAA formulation, typically non-upfrontsolutions feasible, even using = 0. even large (but practical) values N , set scenarios used approximation tend includehighly unlikely scenarios, need accounted stochastic solutionusing = 0. observation motivates solution approach (2). particular,Section 4, first consider problem = 0, turns new variantclassic Steiner tree problem. derive incremental primal-dual algorithmproblem (Section 5) used approximately solve = 0 case> 0 case early stopping. experiments show approach ableprovide significant flexibility little loss reward, flexibility-reward trade-offcontrolled > 0.4. Set-Weighted Directed Steiner Graph Formulationmotivated above, focus optimization problem (2) case = 0.is, must optimize flexibility subject constraint obtain optimalempirical reward measured R. section, show formulate problemnovel variant Steiner tree problem.4.1 Set-Weighted Directed Steiner Graph(2), arrive final optimization problem = 0:arg min costf () s.t. R() = R .(3)view problem type Steiner tree problem scenario graph.particular, say vertex time = H terminal vertex reachableroot r, set nodes Xnupfront (a, H) = 1 hence contributeupfront reward R . way satisfy constraint R() = R purchaseset edges scenario graph connect target nodes r. Thus,constraint Equation 3 corresponds purchasing edges r pathterminal, Steiner tree problem.example, consider scenario graph Figure 1. terminal nodes1 v 3 , twoexample nodes layer = 4 except vb,4b,4nodes connected r directed path. schedule satisfies constraint341fiXue, Fern, & SheldonR() = R must purchase edges terminals reachable r. Noteexample schedule Figure 1, set purchased edges satisfy3 .constraint since path purchased edges terminal vertex vc,4problem similar traditional Steiner tree problem,significant difference. traditional problem, edge associated distinctweight purchased individually, goal connecting terminals usingset edges minimum total weight (which always forms tree). Rather, situationcomplicated purchase parcels, correspond subsets edgesscenario graph. particular, purchasing parcel p time t, incurs cost c(p)f (t)Equation (3), corresponds purchasing edge set Ep,t cost c(p)f (t) containsn ) come vertex u arrive vertex v n p,edges (u, va,t0a,t0t0 n {1, . . . , N }. Note cost model, total cost edge setspurchased schedule exactly equals surrogate objective costf ().see problem instance problem callSet-weighted Directed Steiner Graph (SW-DSG) problem, novel variant Steinertree problem, goal select set vertices minimal total cost orderconnect terminal vertices root. remainder paperdiscuss problem general form simplify notation. input SW-DSGdirected graph G = (V, E) single root vertex r, set terminal vertices V,set edge sets E = {E1 , . . . , EM } Es E, non-negative costcs Es . particular, conservation problem edge sets E = {Ep,t }n ) : (u, v n ) E, p, t0 t, n {1, . . . , N }} cost cEp,t = {(u, va,t0p,t = c(p)f (t).a,t0subset E forms Steiner graph union edges connect r vertices .desired output minimum cost subset E forms Steiner graph. Noteoptimal Steiner graph need tree SW-DSG, unlike traditional Steinertree problem.clear SW-DSG general original deterministic optimizationproblem since latter specific edge set structure. instance, Ep,t1 Ep,t2t1 > t2 . However, structure make easier problem SW-DSG.following sections, prove problems NP-complete primaldual algorithm either setting. special structure leadalgorithmic advantages deriving primal-dual algorithm. Therefore, mainly discussproblem form SW-DSG simplify notation.SW-DSG problem motivated particular conservation application,relevant problems Steiner style objectives, edgeresources best considered groups. example, Steiner trees often useddesign communication networks edges correspond existing potentially newcommunication links. situations links must purchased coherent sets(e.g. communication infrastructure different companies/organizations), SW-DSGproblem would appropriate formulation.4.2 Computational Complexityknowledge, SW-DSG generalization Steiner tree problempreviously studied hence consider computational complexity.342fiScheduling Conservation Designs Maximum FlexibilitySW-DSG problem generalization traditional directed Steiner tree (DST)problem, known NP-complete (Hwang, Richards, & Winter, 1992). Further,standard complexity assumptions, DST hard approximate factor betterlog(|T |) (Charikar, Chekuri, Cheung, Dai, Guha, & Li, 1998). Note resultshold even acyclic directed graphs. number effective heuristic algorithmsDST (Drummond & Santos, 2009), many successful relying shortest path computations subroutine. shortest paths computed edgeweighted graphs efficiently, turns case set-weighted problem.particular, note shortest path problem special case DST (or SW-DSG)single terminal vertex. problem turns NP-Hard SWDSG, even restricted acyclic graphs special edge set structure shownoriginal deterministic optimization problem, case scenario graphsconservation problem.Theorem 1. SW-DSG problem NP-hard even restricted acyclic graphssingle terminal vertex edge set structure scenario graph.Proof. prove hardness reducing weighted set cover problem subclassSW-DSG problems restricted scenario graph one scenario exactly one terminal.Note consider decision version SW-DSG problem, asksfeasible Steiner graph whose cost less specified threshold C . instanceweighted set cover problem specifies ground set elements = {e1 , . . . , en }, set= {S1 , . . . , Sm } subsets Sj S, cost Cj subset, cost bound C .0problemasks whether collection total cost CSj 0 = S.Given set cover instance, first describe construct scenario graph illustrated Figure 2 later describe corresponding SW-DSG instance. graphcontains 2n layers, alternate set layers element layers startingset layer (n layers each, hence 2n layers). layer vertices labeled S1 , . . . , Smrepresent sets n vertices labeled e1 , . . . , en represent elements S.addition include root vertex r. vertex also seen parcelsingle patch. edges graph go one layer immediate next layerfollows. root vertex edge going Sj first set layer. ithelement layer (i.e. layer 2i graph), include edge vertex label Sjprevious layer vertex label ei current layer whenever ei Sj . Finally,vertex ei ith element layer edge Sj next layer.corresponding SW-DSG (conservation problem) instance graph specifiedfollows. root node r single terminal vertex en final element layer.edge sets specified follows, setting scenario graph.edge sets Ej,t Sj time t. particular, Ej,t contains every incomingedge vertex vertex labeled Sj layers t0 t. letstrictly decreasing f (t) sufficiently close 1 ts. cost Ej,t equalCj f (t) Cj cost Sj original set cover problem. words,cost Ej,t almost Cj . Similarly, edge sets Ei,t ei time t. setcosts 0. cost threshold SW-DSG problem equal threshold Cset cover problem.343fiXue, Fern, & Sheldonsee reduction correct, consider case resulting SW-DSGinstance feasible solution. solution provides path r en purchasededge sets total cost C .Since edge set Ei,t ei zero cost, edge set cost resultpurchasing edge sets Ej,t . construction path must go sequencealternating element nodes set nodes. particular, path must traverse elementnode ei = 1, . . . , n. way happen purchaseei least one edge leading ei one immediately preceding Sj layer 2i,possible ei Sj . happen purchasing correspondingedge set Ej,t , corresponding Sj , cost (almost) Cj . seecollection Sj corresponding purchased edge sets must cover elementstotal cost C . Thus, collection sets solution setcover problem.Conversely consider instance set cover problem feasible solution.easy verify feasible solution corresponding SW-DSG problem purchaseedge sets Ej,1 corresponding Sj set cover solution. Combiningsee feasible solution SW-DSG instance feasiblesolution set cover instance.Figure 2: Description reduction set cover SW-DSG single terminalvertex scenario graph.result proves shortest (or least cost) path problem also NP-hardSW-DSG, i.e. problem finding least cost path edges purchasedsets. Thus, difficult extend prior shortest-path-based heuristics Steiner treeproblem problem. Given SW-DSG NP, NP-complete. motivatesderivation efficient heuristic solution approach next section, computesfeasible solution along bound cost optimal solution. Importantlybound provides sense good computed solution compared optimal.344fiScheduling Conservation Designs Maximum Flexibility5. Primal-Dual Algorithmpotential solution approach SW-DSG problem encode Mixed IntegerProgram (MIP), straightforward, use off-the-shelf MIP optimizer.approach produced non-trivial results upfront conservation problem (Sheldon et al., 2010), experiments demonstrate, scale well problem.related approach could consider rounding procedure MIPs LP-relaxation.solving LP-relaxation easier solving MIP, experiments showscalability LP solvers also poor problem sizes interest us. Instead,exploit MIP encoding another way, following primal-dual schema (Vazirani, 2001) derive scalable algorithm performs near optimally experiments.work considered non-trivial generalization previous work (Wong, 1984),primal-dual schema applied DST. Moreover, early-stopping versionprimal-dual algorithm provides way trade-off schedule flexibility reward( > 0). Note primal-dual algorithms SW-DSG original deterministicconservation problem differ notations. words, edge set structureconservation problem offer improvements algorithm. Thusfollowing, present approach SW-DSG.5.1 Primal-Dual Algorithm SW-DSGapply primal-dual schema, start giving primal MIP SW-DSG problemalong dual LP-relaxation Figure 3. primal MIP includes binaryvariable y(Es ) edge set E, indicates whether Es purchased (y(Es ) = 1)(y(Es ) = 0). objective primal simply sum variablesweighted costs corresponding edge sets. Steiner graph constraint, requiringterminals connected root node purchased edges, encoded usingstandard network-flow encoding (lines 24) involving flow variables xki,j . flow variablexki,j encodes flow edge (i, j) destined terminal k. flow balance constraints(2) guarantee one unit flow carried path root node r terminal k.LP-relaxation primal simply replaces integer constraints y(Es )variables positivity constraint. dual relaxed problem (lines 69) includesk corresponding primal flow constraints. Notedual variables uki wi,jconstraint one unit flow leaves root implied flow constraints.omitting constraint, one could simplify dual eliminating ukr variables (or,equivalently, set ukr = 0 k ).Given primal dual formulations problem, apply primaldual schema designing optimization algorithms. particular, primal-dual algorithmiterative iteration increases value dual objective purchasessingle edge set Es , corresponds setting primal variable y(Es ) = 1.iteration stops purchased edges form Steiner graph (i.e. primal becomesfeasible). value dual objective end iteration serves lower boundoptimal primal objective, provides worst-case indication faroptimal returned solution is. high level, algorithm simple greedy heuristiccontinuously purchases beneficial edge set order build paths345fiXue, Fern, & Sheldon(Primal) minXy(Es ) cs ,subject to:(1)s=1X(i,h)Exki,hXxkj,i(j,i)Exki,j= r1,= 1, = k , k T, V0,=6 r, kXy(Es ), k T, (i, j) E(2)(3)s:(i,j)Esxki,j 0, (i, j) E, ky(Es ) {0, 1}(Dual) maxX(ukk ukr ),(4)(5)subject to:(6)kTXkwi,jcs , {1, . . . , }(7)kukj uki wi,j0, k T, (i, j) E(8)k,(i,j)Eskwi,j0(9)Figure 3: MIP SW-DSG problem corresponding dual LP MIPs LPrelaxation. SW-DSG problem defined graph G = (V, E), root vertexr, set terminal vertices , set edges sets E = {Es : = 1, . . . , },Es E.unconnected terminal. primal-dual schema provides principled way incrementallycomputing heuristic time computing lower bound.Algorithm 1 gives pseudo-code algorithm. main data structure auxiliarygraph G0 = (V, A) vertices input graph G. auxiliary graph edgeset initially empty iteration adds newly purchased edges Es E.algorithm terminates edges form Steiner graph. edge sets usedconstruct graph returned solution, following pruning stepremoves obviously redundant edge sets, algorithm sometimes includeiteration process.order describe algorithm detail, first introduce terminology. Givencurrent auxiliary graph A, let C(k) denote set vertices directedpaths terminal node k via edges A. Note consider k includedC(k). Also, define cut set terminal node k, denoted Cut(k), setedges (i, j) j C(k) 6 C(k). Intuitively, k already reachableroot, know least one edge Cut(k) must added orderarrive Steiner graph.346fiScheduling Conservation Designs Maximum FlexibilityAlgorithm 1 Primal-Dual Algorithm SW-DSG.1: {Inputs: Graph G = (V, E), edge sets E = {E1 , . . . , EM }, costs {c1 , , cM }, terminalsV}2: Initialize:k = 0, (i, j) E, kuki = 0, k T, V; wi,j0G = (V, A) =lowerBound = 0, solution =3: G0 Steiner graph4:Let k random vertex connected r G05:= {s | Es Cut(k) 6= , 6 solution}6:= arg minsS (s,k)Pk0(s, k) = cs k0 T,(m,n)Es wm,n/|Es Cut(k)|7:8:9:10:11:12:13:ukj = ukj + (s , k), j C(k)k = w k + (s , k), (i, j) Cut(k)wi,ji,j= EslowerBound = lowerBound + (s , k)solution = solution {s }endPruning: solution = solution {s | s0 solution, Es Es0 }algorithm first initializes dual variables zeros auxiliary graphinclude vertices edges. iteration proceeds first randomly selectingterminal vertex k connected r auxiliary graph. intuitive level,algorithm select edge set Es contains cutset edge k accordingheuristic (s, k) derived applying primal-dual schema. concretely,aim iteration raise dual objective value increasing value ukkmaintaining feasibility. Increasing ukk violate constraints type (8)dual lines 5 8 algorithm maintain feasibility selecting edge setEs among intersects cut set k raising variables correspondingvertices C(k) edges Cut(k) value (s , k) (including ukk ). doneway causes dual constraint type (7) corresponding edge set Es becometight. Since constraint corresponds primal variable y(Es ), algorithm effectivelysets y(Es ) = 1, indicating purchase, adding edges Es A. dual objectivevalue termination sum across iterations (s , k) returned lowerbound.key property algorithm iteration increases dual objective,also maintaining feasibility dual. guarantees iterationdual objective value corresponds true lower bound optimal value primal.Theorem 2. iteration primal-dual algorithm produces feasible dual solutionincreased objective.Proof. base case, initialization assigns dual variables zeros,l },feasible solution. suppose iteration q 1 starts feasible solution {uli , wi,jsatisfies dual constraints type (7) (8). algorithm terminates,347fiXue, Fern, & Sheldonget feasible solution. Otherwise let k terminal vertex selected. variablesl } l 6= k values changed, (8) satisfied. remaining{uli , wi,jvariables l = k, three cases. Case 1: j 6 C(k), variables ukjk unchanged, cannot contribute violation (7) (8). Case 2:wi,jedge (i, j) j, C(k), increase ukj uki (s , k) continuesatisfy corresponding constraint (8). Case 3: cut set edge (i, j) Cut(k),k (s , k) (8) remains satisfied. Since w k edgesincrease ukj wi,ji,jcut set increased, must ensure constraints type (7) become violated.choice (s , k) made algorithm verified never violateconstraints makes least one tight.main portion algorithm terminates, pruning step conductedremove edge set subset edge set solution, decreasestotal cost maintaining feasibility. particular, context conservationscheduling problem, pruning step ensures parcel purchasedfinal solution. aggressive computationally expensivepruning techniques could also used. example, one could consider removingone selected edge sets final solution test feasibility.solution still feasible, edge set eliminated. findaggressive style pruning necessary experiments.5.1.1 Implementation Running Timek dual variables.Note pseudo-code stores updates values ukj wi,jnaive implementation algorithm would result O(|E||T |) runtimeinitialization well computation per iteration, |E| number edgesgraph |T | number terminals. could much SW-DSGproblems large network one conservation application. However,algorithm described way presentation purposes. turnspurposes running algorithm, implemented significantly efficiently.k valuesparticular, need store update sum corresponding wi,jedge set (i.e. sum term appears inside definition (s, k) line6), maintain current objective value (stored lowerBound pseudo-code),updated line 10. Therefore, iterations + 1 variablesneed initialized, number edge sets much smallersize network. implementation dominant computation per iterationcomputation cut set selected terminal k. find cut set backwardtraversal terminal k toward root. time computation acceptableterminals connected relatively small parts overall graph.case conservation problem, terminals connected nodescascade among ones spatially close enough reached.applications terminals possibly connected large portion graph,may preferable incrementally maintain cut set every terminal iterationreduce computation. memory needed O(C|T |) C maximumsize cut set presumably C |E|. getting cut set, algorithm takesO(M C) time identify best edge set update solution.348fiScheduling Conservation Designs Maximum Flexibility5.2 Early-Stopping Fractional Connectionprimal-dual algorithm, computation continues terminalsscenario graph connected paths root. context conservationproblem corresponds reward approximation loss ( = 0). modifyprimal-dual algorithm allow reward approximation loss > 0.case corresponds modifying SW-DSG feasibility constraint requirefraction 1 terminals connected, leading natural way exploringtrade-off reward flexibility.Given incremental, greedy nature primal-dual algorithm, adds oneedge set iteration, natural choice fractional connection problem stopalgorithm whenever least fraction 1 terminals connected.basic early-stopping approach lead improvement cost returnedsolution, compared = 0, savings often quite minimal. due factprimal-dual algorithm grows paths terminal nodes rootunaware early-stopping condition. result, pathsgrown never actually connected root point algorithm stopped.unconnected paths considered waste resources respectmeeting fractional connection constraint. Thus, make early-stopping algorithmviable, necessary perform pruning early-stopping result. algorithmfractional coverage two stages: 1) Generation, early-stopping usedgenerate initial solution meets fractional connection constraint, 2) Pruning,solution produced stage 1 pruned maintaining fractional connectionconstraint.pruning stage use simple effective greedy strategy. ideaiterate purchased parcels schedule returned early-stopping stagedelay purchase parcel long possible ensuring numberconnected terminal nodes almost always within required fractional connectiontolerance.found conservation application, SW-DSG problem corresponds set cascades, beneficial prune using independently generatedlarger set scenarios used create initial solution. analogous using validation data tune algorithm parameters Machine Learning prediction problems,beneficial reasons. found pruning based original setscenarios often overly aggressive hurt empirical performance due over-fittingSAA scenarios. Since easily generate independent scenarios estimate trueexpected reward pruned policies, better prune based criterion instead.Also, since computational complexity evaluating reward pruned policies lowcompared SAA optimization, afford use larger set scenarios.particular, experiments formed initial schedules based set 10 cascadescenarios conducted pruning step respect 40 cascade scenarios.approach pruning also viewed directly enforcing threshold(independently estimated) expected reward original stochastic problem (Equation1) instead enforcing threshold objective value SAA problem (Equation2). Since cant calculate correct threshold value (the RHS Equation 1) without349fiXue, Fern, & Sheldonknowing true optimum R stochastic problem, use SAA optimum Rplace. SAA optimum stochastic upper bound R , generallyconservative approach enforcing Equation 1.6. Experimentssection, first evaluate primal-dual algorithm applying real, full-scaleconservation problem. Next, verify robustness approach problems,present results using synthetic conservation data problem generator used severalrecent studies. focus first two parts experimentation case = 0,see provides substantial gains flexibility. order explore trade-offflexibility reward (population spread), end section, evaluateearly-stopping approach > 0.6.1 Evaluation Primal-Dual Algorithm Real Conservation Mapreal map use dataset prior work Sheldon et al. (2010)computing upfront conservation designs. data derived conservation probleminvolving Red-cockaded Woodpecker (RCW) large land region southeasternUnited States interest Conservation Fund. region divided443 non-overlapping parcels (each area least 125 acres) 2500 patchesserving potential habitat sites. Parcel costs based land prices landparcels already conserved thus cost zero. use population spreadmodel Sheldon et al. (2010), based individual-based models RCW.Since approach requires conservation design input, use design computedSheldon et al. (2010) using total budget constraint $320M. map areashown left cell Figure 7, parcels making design shaded green freeparcels (with cost 0) shaded grey; red + marks indicate initially occupied patches.method also requires specifying strictly decreasing function defining surrogate costfunction, use f (t) = = 0.96. found resultssensitive value .6.1.1 Comparing Optimal Solutionscompare solutions primal-dual algorithm optimal solutions found usingCPLEX solver applied MIP encoding SW-DSG problem. MIP encodingsbecome large horizon number scenarios increase. particular,443 H + 2500 H N variables number constraints grows numberedges, cascade network, becomes impractical N H grow. Sinceoptimal solver cant scale large versions problem, consider problems involvingcascade networks 2 scenarios horizons ranging 15 40 years.also use CPLEX compute solutions LP-relaxation MIP. objective valuereturned LP provides alternative approach computing lower boundoptimal solution thus interesting compare lower bound terms tightnessruntime. Since primal-dual algorithm stochastic due random selection350fiScheduling Conservation Designs Maximum FlexibilityHHHHHH======152025303540CostMIP126.8123.6117.6130.4(M$)PD126.22125.7121.4134.0131.3127.5LowerLP122.2117.7104.7117.3109.9BoundPD84.971.961.556.964.159.7RunMIP5.58.2285126TimeLP6.137.6101518(s)PD0.92.59.0112545Table 1: Comparison Primal-Dual (PD) MIP LP.terminal nodes, report averages 20 runs, noting standard deviationsnegligible.first two data columns Table 6.1.1 show (surrogate) cost solutionsfound CPLEX solving MIP algorithm (PD) increasing horizons, largerhorizons correspond larger problems. method fails return solution duememory constraints value shown table. see horizons MIPable yield solutions CPLEX, algorithm produces solutions similarcosts (here lower cost better). also see MIP solver runs memoryunable return solutions 2 largest problem instances, already scaledversions problem (small number cascades horizon).next two columns Table 6.1.1 provide results lower bound computedCPLEX solving LP PD algorithm. see lower bound producedLP significantly tighter bound produced algorithm. However,LP cannot solved CPLEX largest problem, approach still yieldslower bound. Overall, though lower bound good LP (whencomputed), generally within factor two optimal solution, providesnon-trivial assurance quality returned solution large problems.final three columns Table 6.1.1 present time used approachesproblem, blank cells indicate method ran memory. algorithmsignificantly faster MIP approach, fails two largest problems,comparable LP approach, provides lower bound failslargest problem. later result indicates solution based LP-rounding wouldface difficulty, since even solving LP large problems (40 time steps 2500patches each) computationally demanding. advantage primal-dual algorithmavoids encoding LP rather works directly graph.2. PD cost less optimal MIP cost returned CPLEX. investigating, foundCPLEX correctly evaluates solution returns, thinks solution optimalnot. appears issue due small error tolerance allowed CPLEX solver.3. MIP takes less time LP. think possibly CPLEX uses different algorithmsLP MIP. Especially, MIP solved branch-and-bound algorithm uses modern featureslike cutting planes heuristics, making CPLEX powerful MIP solver.351fiXue, Fern, & Sheldon6.1.2 Number Cascades Scenario GraphAccording SAA, optimal solution finite set cascade scenarios convergetrue optimum scenarios. Previously two cascades used duepoor scalability CPLEX. study number cascade scenarios useensure good solution. Recall N increases, original stochastic problemapproximated accurately. Yet larger N corresponds computation.importantly, = 0, larger N leaves schedule less space flexibility.extreme case N , possible schedule upfront schedule minimalflexibility. find good value N practice, study primal-dual solutionsdifferent number cascade scenarios validating reward R() primaldual schedule achieve. Given population spread stochastic, computereward R() running 20 simulations stochastic population spread model.simulation provides reward value (number occupied patches horizon)average results. schedules produced primal-dual algorithmupfront schedule. Recall intention nearly match rewardupfront schedule. Figure 4 presents results time horizon H = 20.observe primal-dual schedule achieves reward N increases,reward converging towards expected reward upfront schedule. alsosee 10 cascades quite close get best performance rate improvementslowing down. Thus, remainder experiments use 10 cascades SAA.Figure 4: Rewards PD solutions w.r.t number cascade scenarios.6.1.3 Quality Conservation Schedulesevaluate algorithm problems realistic sizes. Here, considerproblems based 10 cascades horizons ranging 20 100 years, wellbeyond range approachable MIP LP. solution times algorithmranged 15 seconds H = 20 29 minutes H = 100.First, evaluate average accumulated reward schedule returnedmethod horizon Figure 5. average reward upfront schedule ranged332 H = 20 615 H = 100 time horizons primal-dual solutionattained average reward least 95.3% optimal, negligible error bars352fiScheduling Conservation Designs Maximum Flexibilityaverages. small gap indicates 10 scenarios SAA approximation quitegoodthe gap could reduced increasing number scenarios.Figure 5: Rewards PD schedules w.r.t.time horizon H.Figure 6: Cost curves PD scheduleshorizons 20 100.course, must also consider cost curves corresponding schedules, sinceaffords flexibility criterion problem. Figure 6 presents costcurves schedules. Note defined Section 3.2, cost curve shows (nondiscounted) accumulated cost schedule time. cost curve scheduleproduced horizon H increase time H remain flat, reflectingpurchases made time. see horizons cost curves showfairly gradual increase cost expenditures time, indicating schedulesindeed providing significant amount flexibility regarding purchase times, particularlycompared upfront schedule, cost curve flat black lineFigure 6 since parcels purchased time 0. experiments shown, foundcost curves vary small amount different values , generaltrend present. Interestingly, curves sudden jump cost around 20years. understand Figure 7 show parcel purchases madeschedule population spread map 100 year horizon. see= 20 sharp increase cost due purchase relatively expensivevast parcels southern part design. Looking population spread dynamics,apparent parcels critical gateway ensuring reliable spreadsouthwestern part design later years. Delaying purchase longer significantlyincreases probability spread occur, approach discovers.Another interesting observation seen comparing expected populationspread PD computed schedule expected population spread upfrontschedule (Figure 7). striking difference spreads seen time steps= 20, 60, 80 northeastern part map. upfront schedule entirenortheastern part occupied large part, PD schedule holenortheastern part near initial bird populations located. Note, however,hole finally occupied horizon problem (t = 100). time,spread upfront PD schedules visually similar, agrees factmeasured rewards also similar. reason difference populationspread PD schedule delays purchase northeastern parts353fiXue, Fern, & Sheldon= 20= 60= 80= 100Figure 7: (Left) Original conservation design used scheduling shown green shadedparcels. Free (zero-cost) parcels also shaded dark grey red + indicatesinitially occupied patches. (Right) top row shows parcels purchased(shaded green) PD schedule horizon 100 years. middlerow shows population spread horizon schedule,lighter red shading patch indicates smaller probability occupied(as measured 20 simulations). bottom row shows population spreadupfront schedule horizon 100 years.map near initial bird population 20 years time horizon ends.population spread process schedules, found parcelsclosely connected hence become occupied fairly short time bird populationnearby. apparent upfront schedule, areas already occupied= 20. Thus, purchase parcels delayed long enough timeleft population spread landscapes. Therefore, purchasingdelayed parcels far away current population, also parcelscovered quickly reliably. Note flexibility mainly due definitionreward function, takes population time H account. countpopulation every time step, presumably good schedule would purchase holearea soon population.6.2 Evaluation Primal-Dual Synthetic Mapsevaluate primal-dual algorithm thoroughly, randomly selected 10 syntheticmaps generated used prior work (Ahmadizadeh et al., 2010). maps consistregion 146 non-overlapping parcels 411 patches, different configurationsparcel costs initial population. map, considered problems involving354fiScheduling Conservation Designs Maximum Flexibilitydifferent conservation designs, design corresponded upfront solutionbudget limited factor b total parcel cost map, b ranged 0.10.5. section, present similar analysis Section 6.1 show consistentresults, indicating primal-dual algorithm stable across different problems.6.2.1 Comparing Optimal Solutionsfirst compare upper lower bounds returned primal-dual algorithmoptimal objective values MIP LP computed using CPLEX. Since CPLEX stillscalability issues solving larger synthetic problems, restrict comparisonproblems 2 4 scenarios horizon 20 years.Results 10 maps similar. example, Figure 8 showssurrogate cost (PD-UB) dual objective value (PD-LB) primal-dual solutionmap 768, together optimal surrogate cost (CPLEX-MIP) lower boundcomputed LP CPLEX. see compared optimal, algorithm stillproduce solutions similar costs, especially problem easy (b smaller).Also, see lower bound computed CPLEX better PD lowerbound. However, PD lower bound still within factor 2.Figure 8: Cost objective value problems map 768. horizontal axis variesamount budget used compute upfront solution usedconservation design given scheduling algorithms.6.2.2 Quality Conservation Schedulesconsider larger problems based 10 cascade scenarios horizon H = 40,MIP LP practically solvable.first compare average accumulated reward schedule returned primaldual algorithm upfront schedule. Figure 9 shows results one maps,indicating rewards achieved primal-dual schedules always closeupfront schedules, desired. results maps similar.also study cost curves schedules order illustrate flexibilitycompared upfront schedules. Figure 10 presents average cost curves schedulesacross 10 maps. noted budget limited, purchasedelayed much. example, b = 0.1, parcels purchased = 15,long ahead time horizon. analysis explained fact355fiXue, Fern, & SheldonFigure 9: Rewards primal-dual schedule upfront schedulemap 1027.Figure 10: Average cost curves PDschedules 10 maps.many maps small budgets, sets affordable parcels fairly spreadloosely connected. means population requires time orderreliably spread across sets. Thus, parcels must purchased quite earlyhorizon support spread. Rather larger budgets, sets parcels mustpurchased spread also tightly coupled, allows easier,reliable population spread. Thus, possible delay purchases much larger extentseen cost curves larger budgets. shows algorithm ableafford considerable flexibility initial conservation design supports reasonablyreliable population spread.6.3 Early-Stopping Trading Flexibility Rewardconsider early-stopping variant primal-dual algorithm, referredPD-ES, producing schedules trade flexibility reward using > 0.dataset use real conservation map. Figure 11 illustrates cost curvesearly-stopping schedules = 0.0, 0.05, 0.10, 0.15, 0.20 H = 20, 40, 60, 80,demonstrates possible budget saving time corresponding fraction reward lossallowed. Figure 12 shows average simulated rewards schedules.First notice average reward achieved early-stopping schedulesalmost always within specified error tolerance, shows pruning stepgeneralizing effectively. also see cost curves early-stopping schedulesshow significant improvement even small values compared early-stopping( = 0). example, H = 60 = 0.20, almost cost firstseveral years, several decades cost approximately half cost required= 0. results show approach able provide set schedules spansspectrum trade-offs, considered conservation managers.7. Summary Future Workwork, addressed problem scheduling purchases parcels conservationdesign. formulated problem network cascade optimization problem356fiScheduling Conservation Designs Maximum FlexibilityH = 20H = 40H = 60H = 80Figure 11: Cost curves PD-ES schedules pruning. red line ( = 0) showscost curve non-early-stopping PD schedule.reduced novel variant classic directed Steiner tree problem. showedproblem computationally hard developed primal-dual algorithmproblem. experiments showed algorithm produces close optimal resultsmuch scalable state-of-the-art MIP solver. also showedearly-stopping variant algorithm able explore possible trade-offsflexibility reward, important consideration practice.scheduling problem considered work poses considerable challenges genericoff-the-shelf schedulers planners. complicating factors include: 1) highly-stochastic,exogenous dynamics arise population spread model, 2) need reasonspatio-temporal processes, 3) long horizons must considered, 4)combinatorial space potential investment options point time. generalsolution schema pursued work likely applicable problemspose similar challenges existing techniques. particular, general schema suggestsapproximating problem via SAA studying resulting deterministicoptimization problem. Often resulting deterministic problem correspondexisting well-studied problems, state-of-the-art approximation algorithmsused. cases, work, resulting problem relatedexisting well-studied problem solution designed extending existingsolution frameworks. expect generic SAA schema particularly useful357fiXue, Fern, & SheldonH = 20H = 40H = 60H = 80Figure 12: Rewards primal-dual schedules early-stopping. numberdata point indicates percentage PD-ES reward PD reward.problems involving stochastic spread populations information across networks,since deterministic problems typical map graph-theoretic problems,vast literature.future work, plan consider several improvements primal-dual algorithm.Currently, iteration, algorithm randomly picks unconnected terminalgrow path from. likely intelligent selection mechanisms improveoverall results. also interested developing primal-dual algorithm directlyincorporates error tolerance constraint early-stopping approach. wouldprovide direct method trading reward improved flexibility. Furthermore,intend pursue fully adaptive approaches conservation problems.One idea incorporate scheduling approach replanning algorithm selectspurchases current decision epoch based up-to-date information.particular, decision epoch schedule would formed parcels scheduledpurchased immediately (those flexibility) would purchased subsetcases immediate budget would exceeded. Considering sophisticatedapproaches take account immediate budget would natural usefulextension. would also interesting consider conservation problemvariants reward function. species, rather caring population358fiScheduling Conservation Designs Maximum Flexibilityend, ecological goal may value spread whole periodeven more. Presumably models would different properties complexitiesone study paper.AcknowledgementsParts material paper appeared earlier work Xue, Fern, Sheldon(2012). work supported NSF grant IIS-0964705.ReferencesAhmadizadeh, K., Dilkina, C., Gomes, C. P., & Sabharwal, A. (2010). empirical studyoptimization maximizing diffusion network. 16th International ConferencePrinciples Practice Constraint Programming.Bent, R., & Van Hentenryck, P. (2004). Regret only! Online stochastic optimizationtime constraints. Nineteenth AAAI Conference Artificial Intelligence.Chang, H. S., Givan, R. L., & Chong, E. K. (2000). On-line scheduling via sampling.Artificial Intelligence Planning Scheduling.Charikar, M., Chekuri, C., Cheung, T., Dai, A., Guha, S., & Li, M. (1998). Approximation algorithm directed Steiner tree problems. Ninth Annual ACM-SIAMSymposium Discrete Algorithms.Chong, E. K., Givan, R. L., & Chang, H. S. (2000). framework simulation-basednetwork control via hindsight optimization. IEEE CDC conference.Crowley, M., & Poole, D. (2011). Policy gradient planning environmental decision makingexisting simulators. Twenty-fifth AAAI Conference Artificial Intelligence.Drummond, L. M., & Santos, M. (2009). distributed dual ascent algorithm Steinerproblems multicast routing. Networks, 53, 170183.Golovin, D., Krause, A., Gardner, B., Converse, S. J., & Morey, S. (2011). Dynamic resourceallocation conservation planning. Twenty-fifth AAAI Conference ArtificialIntelligence.Hanski, I., & Ovaskainen, O. (2000). metapopulation capacity fragmented landscape. Nature, 404 (6779), 755758.Hwang, F. K., Richards, D. S., & Winter, P. (1992). Steiner Tree Problem. Springer.Kumar, A., Wu, X., & Zilberstein, S. (2012). Lagrangian relaxation techniques scalable spatial conservation planning. Twenty-sixth AAAI Conference ArtificialIntelligence.Shapiro, A. (2003). Monte Carlo sampling methods. Stochastic Programming, HandbooksOperations Research Management Science, Vol. 10, pp. 353426.Sheldon, D., Dilkina, B., Elmachtoub, A., Finseth, R., Sabharwal, A., Conrad, J., Gomes,C., Shmoys, D., Allen, W., Amundsen, O., & Vaughan, B. (2010). Maximizing359fiXue, Fern, & Sheldonspread cascades using network design. Uncertainty Artificial Intelligence(UAI).Vazirani, V. V. (2001). Approximation Algorithms. Springer, Berlin.Williams, J., ReVelle, C., & Levin, S. (2005). Spatial attributes reserve design models:review. Environmental Modeling Assessment, 10 (3), 163181.Wong, R. T. (1984). dual ascent approach Steiner tree problems directed graph.Mathematical Programming, 28, 271287.Xue, S., Fern, A., & Sheldon, D. (2012). Scheduling conservation designs via networkcascade optimization. Twenty-sixty AAAI Conference Artificial Intelligence.Yoon, S., Fern, A., Givan, R. L., & Kambhampati, S. (2008). Probabilistic planning viadeterminization hindsight. Twenty-third AAAI Conference Artificial Intelligence.360fiJournal Artificial Intelligence Research 52 (2015) 543-600Submitted 09/14; published 04/15Distributed Evaluation Nonmonotonic Multi-context SystemsMinh Dao-TranThomas EiterMichael FinkThomas KrennwallnerDAO @ KR . TUWIEN . AC .EITER @ KR . TUWIEN . AC .FINK @ KR . TUWIEN . AC .TKREN @ KR . TUWIEN . AC .Institute fur Informationssysteme, TU WienFavoritenstrasse 9-11, A-1040 Vienna, AustriaAbstractMulti-context Systems (MCSs) formalism systems consisting knowledge bases(possibly heterogeneous non-monotonic) interlinked via bridge rules, globalsystem semantics emerges local semantics knowledge bases (also called contexts)equilibrium. MCSs related formalisms inherently targeted distributed settings, truly distributed algorithms evaluation available. address shortcoming present suite algorithms includes basic algorithm DMCS, advanced version DMCSOPT exploits topology-based optimizations, streaming algorithmDMCS-STREAMING computes equilibria packages bounded size. algorithms behave quite differently several respects, experienced thorough experimental evaluationsystem prototype. experimental results, derive guideline choosing appropriatealgorithm running mode particular situations, determined parameter settings.1. Introductionlast decade, increasing interest systems comprise informationmultiple knowledge bases. includes wide range application fields data integration, multi-agent systems, argumentation many others. picture concrete real-worldapplication, may consider METIS (Velikova et al., 2014), industrial prototype system facilitating timely human decision making maritime control. application, human operatorsneed support determine whether ship entering port might hide identity illegal activities might high risk environmental hazard. access risks, METIS reliesnumber heterogeneous external information sources commercial ship database IHSFairplay,1 ship tracking websites,2 news items history pollution events ship mayinvolved in.rise Word Wide Web distributed systems propelled development,date several AI-based formalisms available host multiple, possibly distributed knowledgebases compound system. Well-known formalisms distributed SAT solving (Hirayama& Yokoo, 2005), distributed constraint satisfaction (Faltings & Yokoo, 2005; Yokoo & Hirayama,2000), distributed ontologies different flavors (Homola, 2010), MWeb (Analyti, Antoniou, &Damasio, 2011), different approaches multi-context systems (Giunchiglia & Serafini, 1994;Ghidini & Giunchiglia, 2001; Brewka, Roelofsen, & Serafini, 2007; Brewka & Eiter, 2007; Bikakis1. www.ihs.com/products/maritime-information/2. marinetraffic.com, myship.comc2015AI Access Foundation. rights reserved.fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERFigure 1: Pinpointing Joker& Antoniou, 2010) rooted McCarthys (1993) work; among them, focus Heterogeneous Nonmonotonic Multi-context Systems (MCSs) (Brewka & Eiter, 2007).generalization previous proposals, MCSs powerful formalism specify systemsknowledge bases may different formats reasoning powers, ranging simplequery answering relational database reasoning description logic knowledge bases (seeBaader et al., 2003), well nonmonotonic formalisms default logic (Reiter, 1980)answer set programs (Gelfond & Lifschitz, 1991). allow heterogeneous knowledge basesdeal impedance mismatch them, MCSs abstract knowledge bases plain mathematical structures; top, special bridge rules interlink knowledge bases, bridge ruleadds formula knowledge base, depending certain beliefs knowledge bases. Hencesemantics knowledge base associated bridge rules, forms context, dependscontexts, possibly cyclic manner. Based this, MCSs equilibrium semanticsterms global states every context adopts abstract local model, called belief set,conformant local models adopted contexts addition obeysbridge rules. following simple example, paraphrase Ghidini Giunchiglias(2001) Magic Box, illustrates power idea,Example 1 Suppose computer game, players Batman Robin chased player Jokerpartially occluded area, shown Figure 1; Robin wounded cannot read distanceobjects. Neither Batman Robin tell Jokers exact position 33 box: Batmanassure columns 2 3, Robin tell row 1. However,exchange partial knowledge, pinpoint Joker row 1 column 1.model Batman Robin contexts whose local knowledge bases include information Jokers position, exchanged using bridge rules, row (X) (2 :row (X)). Batman, informally imports Robins knowledge (context 2) rowpositions; full encoding given Example 2. equilibrium emerging MCS disclosesJokers position Batman Robin.544fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSAlthough MCSs related formalisms inherently target distributed systems, truly distributed algorithms computing equilibria MCSs available. Brewka Eiter (2007)encoded equilibria HEX-programs (Eiter, Ianni, Schindlauer, & Tompits, 2005),evaluated using dlvhex solver. However, approach elegantly offers full heterogeneity, fully centralized needs technical assumptions. Roelofsen, Serafini, Cimatti (2004)proposed earlier algorithm check satisfiability homogeneous, monotonic MCScentralized control accesses contexts parallel (hence truly distributed). BikakisAntoniou (2010) instead gave distributed algorithm defeasible multi-context systems;however, latter homogeneous (possibly nonmonotonic) contexts particular typesemantics, algorithm serves query answering model building.lack distributed algorithms evaluating MCSs based local context handlers dueseveral obstacles:abstract view local semantics belief sets limits algorithm global levelinterference knowledge bases evaluation process context.Towards real life applications, certain levels information hiding security required(e.g. information exchange knowledge bases companies) selectedinformation transferred contexts via well-defined interfaces. prevents contextgetting insight neighbors optimization, instance learn conflicts (i.e.,joint beliefs leading contradiction) across contexts.MCS system topology, i.e., structure context linkage, might unknown context;disables decomposing system efficient, modular evaluation.bridge rules might fuel cyclic information flow group contexts. Evencontext easy evaluate (e.g., knowledge bases acyclic logic programs), global cyclesrequire nontrivial care.article, address obstacles present results towards efficient distributed evaluation MCSs. main contributions suite generic algorithms DMCS, DMCSOPT,DMCS-STREAMING work truly distributed, implementation system prototype.detail, contributions follows.1.1 Algorithms Optimization Techniques(1) first, basic algorithm DMCS aims fully distributed setting deal obstaclesgeneric way: contexts exchange belief sets call history (i.e., access pathtraversing bridge rules), information. global level, belief states formedtuples belief sets; context bridge rules must respect belief sets neighborscomputing belief sets using local solver knowledge base. Cycles detectedcall history, context gets request finds call history; break cycle,guessing technique used checks return path.(2) localizing contexts knowledge system information exchange, DMCSfairly easily adapt context changes (additions deletions), time facesscalability issues. enhance performance optimized version DMCSOPT, disclosemeta-level information contexts, viz. (i) topology context dependencies, exploiteddecomposing MCS sub-MCSs (blocks) linked block-tree, (ii) interface contexts, optimizing data transfer blocks. (i) breaks cycles545fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERadvance (ii) significantly reduces duplicate local evaluation; yields remarkable performance gain.(3) Still DMCS DMCSOPT compute equilibria MCS, escapescalability memory issues, multiple local belief sets lead combinatorial explosionglobal level. thus consider computing equilibria streaming mode; end, contexts passbelief sets one shot parents gradually small packages. Memory blowupthus avoided moreover contexts continue earlier wait answersneighbors. approach seems user-friendly equilibria gradually appear ratheronce, possibly long time; one may quit computation seeing sufficientlymany results (i.e., equilibria).1.2 Implementation Experimentsimplemented algorithms system prototype. assess effects optimization techniques, set benchmarking system conducted comprehensive experimentsMCSs various topologies interlinking. results confirm expectation optimization techniques general; nutshell, (i) decomposition technique clearly improvesperformance non-streaming mode; (ii) streaming worthwhile may still find answersnon-streaming times out; (iii) streaming, choosing package size important;(iv) system topology important optimization techniques show drastic improvementsspecific topologies; (v) sometimes, techniques yield gain incur overhead.results work provide truly distributed algorithms evaluating MCSs,also distributed versions non-monotonic knowledge base formalisms(e.g., distributed answer set programs), underlying principles techniques mightexploited related contexts. Furthermore, may provide basis evaluation extensions generalizations MCSs, non-ground MCSs (Fink, Ghionna, & Weinzierl, 2011),managed MCSs (Brewka, Eiter, Fink, & Weinzierl, 2011), supported MCS (Tasharrofi & Ternovska,2014), reactive MCSs (Goncalves, Knorr, & Leite, 2014; Brewka, Ellmauthaler, & Puhrer, 2014).1.3 Organizationremainder article organized follows. next section provides preliminariesMulti-context Systems. Section 3 introduces basic distributed algorithm DMCS, Section 4 develops optimized algorithm DMCSOPT; Section 5 presents streaming algorithm DMCS-STREAMING. Experimental results prototype implementation reportedSection 6. Section 7, consider related works, Section 8 summarize addressopen issues. increase readability, proofs moved Appendix.2. Preliminariessections briefly introduces preliminaries needed rest article.2.1 Multi-context SystemsFirst, present formalization Heterogeneous Nonmonotonic Multi-context Systems (MCSs)proposed Brewka Eiter (2007) described Brewka, Eiter, Fink (2011),546fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSserves base work. idea behind MCSs allow different logics useddifferent contexts, model information flow among contexts via bridge rules. notionlogic defined follows.Definition 1 (cf. Brewka & Eiter, 2007) logic L = (KBL , BSL , ACCL ) composedfollowing components:1. KBL set well-formed knowledge bases L, consists setelements called formulas;2. BSL set possible belief sets, BSL set elements called beliefs;3. ACCL : KBL 2BSL function describing semantics logic, assigningelement KBL set acceptable sets beliefs.notion logic generic, abstracts formation agents beliefs bareminimum. Structure formulas (both knowledge base belief sets) dismissed,viewed naked elements. Likewise particular inference mechanism associatedknowledge base, logical properties imposed belief sets; term beliefreflects statements held agent might epistemic basis, without goingdetail. assignment acceptable beliefs sets knowledge base, intuitivelyset beliefs agent willing adopt given knowledge base, captures logics(e.g., nonmonotonic logics) multiple even acceptable belief sets possible.abstract model allows us capture range different logics knowledge representationreasoning, including classical logic, modal logics, epistemic logics, spatial logics, descriptionlogics etc, also nonmonotonic logics default logic (Reiter, 1980) answer set programs(Gelfond & Lifschitz, 1991), different varieties settings. comparison formalismsgiven Brewka et al. (2011). example, classical (propositional predicate logic) maymodeled follows:KB: set (well-formed) sentences signature ,BS: set deductively closed sets -sentences, (i.e., Cn(S) = S, Cn()denotes deductive closure),ACC(kb): singleton containing deductive closure kb, i.e., ACC(kb) = {Cn(kb)}.example nonmonotonic logics, (disjunctive) logic programs answer set semantics (Gelfond & Lifschitz, 1991) modeledKB: set logic programs signature ,BS: set consistent sets literals ,ACC(kb): set (kb) answer sets kb according Gelfond Lifschitz (1991).3refer setting, used repeatedly sequel, Answer Set Programming(ASP). Note answer sets knowledge base kb amount particular 3-valued models kb;intuitively, positive literal p answer set S, p known true, negative3. common, exclude inconsistent answer sets admitted Gelfond Lifschitz (1991).547fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERliteral p S, p known false, known means literal presentfact derivable rules; neither p p S, truth value p unknown.MCS modeling possible worlds (scenarios) view via answer sets, generatedanswer set solver. However, ASP implementations also capture inference (truthquery respectively answer sets) forms belief set formation.Bridge rules. Based logics, bridge rules introduced provide uniform way interlinkingheterogeneous information sources follows.Definition 2 (cf. Brewka & Eiter, 2007) Let L = {L1 , . . . , Ln } (multi-)set logics. Lk bridge rule L, 1 k n, form(c1 : p1 ), . . . , (cj : pj ), (cj+1 : pj+1 ), . . . , (cm : pm )(1)(i) 1 m, ci {1, . . . , n} pi element belief set Lci ,(ii) kb KBk , holds kb {s} KBk .Informally, bridge rules refer bodies contexts (identified ci ) thus addinformation contexts knowledge base depending believed disbelievedcontexts. contrast Giunchiglias (1992) multi-context systems, single, global setbridge rules; context knows bridge rules.means connecting contexts available, MCSs formally defined.Definition 3 (Brewka & Eiter, 2007) multi-context system (MCS) = (C1 , . . . , Cn ) consistscollection contexts Ci = (Li , kb , br ) Li = (KBi , BSi , ACCi ) logic, kbKBi knowledge base, br set Li -bridge rules {L1 , . . . , Ln }.Example 2 (contd) scenario Example 1 formalized MCS = (C1 , C2 ),contexts L1 , L2 instances Answer Set Programming, and:col (X) see col (X).kb 1 = F F1R,col (X) see col (X).row (X) (2 : row (X)).br 1 =row (X) covered row (X) (2 : see row (X)), (1 : row (X)).row (X) see row (X).kb 2 = F F2R,row (X) see row (X).col (X) (1 : col (X)).br 2 =,col (X) covered col (X) (1 : see col (X)), (2 : col (X)).F = {row (1). row (2). row (3). col (1). col (2). col (3).},F1 = {see col (2). see col (3).},F2 = {see row (1).},548fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSR=joker row (X).joker col (X).row (X) jokerrow (X) jokercol (X) jokercol (X) jokerin, row (X), row (X).in, row (X), row (Y ), X 6= Y.in, col (X), col (X).in, col (X), col (Y ), X 6= Y..Here, X variables used schematic rules, range rows resp. columns (i.e.,1,2,3). Intuitively, C1 formalizes Batmans knowledge scene C2 Robin.knowledge bases kb 1 kb 2 , facts F represent box size 3 3, F1 F2 stateBatman Robin see, viz. Joker columns 2 3 respectivelyrow 1. next two rules simply map sensed locations respective facts. Informally, rulesR make guess row column Joker is, concluded box (firsttwo rules); may lead multiple belief sets. Importantly, Batman adjusts knowledge basedepending beliefs communicated Robin (bridge rules br 1 ) vice versa (bridge rules br 2 ).convenience, introduce following notation conventions. MCS =(C1 , . .. , Cn ), denote Bi setbeliefs occur belief sets context Ci , i.e.,Bi = SBSi S, let BM = ni=1 Bi (simply B, understood). Without lossgenerality, assume distinct contexts Ci Cj , Bi Bj = , bridgeatom form (i : bi ) appearing bridge rule , holds bi Bi .2.2 Semantics Multi-context Systemssemantics MCS defined terms special belief states, sequences =(S1 , . . . , Sn ) Si element BSi . Intuitively, Si belief setknowledge base kb ; however, also bridge rules must respected. end, kb augmentedconclusions bridge rules applicable. precisely, bridge rule r form (1)applicable S, pi Sci , 1 j, pk/ Sck , j + 1 k m. denotehead (r) head r, app(R, S) set bridge rules r R applicable S.Then,Definition 4 (Brewka & Eiter, 2007) belief state = (S1 , . . . , Sn ) MCS = (C1 , . . . ,Cn ) equilibrium, Si ACCi (kb {head (r) | r app(br , S)}), 1 n.equilibrium thus belief state contains context acceptable belief set,given belief sets contexts.Example 3 (contd) MCS Example 2 single equilibrium = (S1 , S2 )S1 = F F1 F3 S2 = F F2 F3 F3 = {joker in, row (1), row (2),row (3), col (1), col (2), col (3)}. equilibrium indeed reflects intuitionscenario Example 1, Batman Robin together infer location Joker,single one cannot accomplish task without communication.Example 4 Let = (C1 , C2 , C3 , C4 ) MCS Li ASP logics, signatures1 = {a}, 2 = {b}, 3 = {c, d, e}, 4 = {f, g}. Supposekb 1 = , br 1 = {a (2 : b), (3 : c)};549fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERkb 2 = , br 2 = {b (4 : g)};kb 3 = {c d; c}, br 3 = {c e (4 : f )};kb 4 = {f g }, br 4 = .One check = ({a}, {b}, {c, d}, {g}) equilibrium .computation equilibria given MCS realized declarative implementation using HEX-programs (Eiter et al., 2005) evaluated using dlvhex system.4idea translate MCS HEX-program (i) disjunctive facts guessingtruth values beliefs, (ii) HEX-rules capturing bridge rules, (iii) constraints externalatoms capturing acceptability functions. details concrete implementationapproach, refer reader MCS-IE system (Bogl, Eiter, Fink, & Schuller, 2010).article, pursue sophisticated approach, i.e., design implement distributedalgorithms, compute equilibria MCSs. evaluation, centralized componentcontrols communication contexts. context independently runs instancealgorithm communicates exchange beliefs well detect breakcycles. novel contributions described next sections.3. Basic Algorithm (DMCS)section introduces first, basic, truly distributed algorithm evaluating equilibriaMCS. algorithm takes general setting input, is, context minimalknowledge whole system; words, knows interface direct neighbors (parents child contexts) topological information metadatasystem. setting, concentrate distributeness. Section 4 shifts focus towardsoptimization techniques metadata provided.Taking local stance, consider context Ck compute parts (potential) equilibriasystem contain coherent information contexts reachable Ck .3.1 Basic Notionsstart basic concepts. import closure formally captures reachability.Definition 5 (Import Closure) Let = (C1 , . . . , Cn ) MCS. import neighborhoodcontext Ck , k {1, . . . , n}, setIn(k) = {ci | (ci : pi ) B(r), r br k }.Furthermore, import closure IC (k) Ck smallest set (i) k (ii)S, In(i) S.Equivalently, define import closure constructively IC (k) = {k}IC 0 (k) = In(k), IC j+1 (k) = iIC j (k) In(i).j0 ICj(k),Example 5 Consider Example 4. In(1) = {2, 3}, In(2) = In(3) = {4}, In(4) =; import closure C1 IC (1) = {1, 2, 3, 4} (see Figure 2).4. www.kr.tuwien.ac.at/research/systems/dlvhex/550fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSC1C1IC (1)In(1)C2C2C3C3C4C4(a) Import neighborhood C1(b) Import closure C1Figure 2: Import neighborhood Import closureS=S1.........Sj...Sn=......Ti...Tj...Tn./ =S1......Ti...Sj (= Tj )...Figure 3: Joining partial belief statesBased import closure define partial equilibria.Definition6 (Partial Belief States Equilibria) Let = (C1 , . . . , Cn ) MCS, letSn/ i=1 BSi . sequence = (S1 , . . . , Sn ) Si BSi {}, 1 n,partial belief state (PBS) , partial equilibrium (PE) w.r.t. Ck , k {1, . . . , n},IC (k) implies Si ACCi (kb {head (r) | r app(br , S)}), 6 IC (k) impliesSi = , 1 n.Note IC (k) essentially defines subsystem 0 connected bridge rules. usePEs instead equilibria 0 keep original MCS intact.combining partial belief states = (S1 , . . . , Sn ) = (T1 , . . . , Tn ), definejoin ./ partial belief state (U1 , . . . , Un )Si , Ti = Si = Ti ,Ui =, 1 nTi , Ti 6= Si = ,(see Figure 3). Note ./ void, couples Si , Ti BSi different. Naturally,join two sets partial belief states ./ = {S ./ | S, }.Example 6 Consider two sets partial belief states:= { (, {b}, , {f, g}) , (, {b}, , {f, g}) }= {(, , {c, d, e}, {f, g}), (, , {c, d, e}, {f, g}), (, , {c, d, e}, {f, g})} .551fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERjoin given./ =(, {b}, {c, d, e}, {f, g}), (, {b}, {c, d, e}, {f, g}),(, {b}, {c, d, e}, {f, g}).3.2 Basic AlgorithmGiven MCS starting context Ck , aim finding PEs w.r.t. Ck distributedway. end, design algorithm DMCS whose instances run independently nodecontext communicate exchanging sets partial belief states.provides method distributed model building, DMCS algorithm appliedMCS provided appropriate solvers respective context logics available. mainfeature DMCS, also compute projected partial equilibria, i.e., PEs projected relevantpart beliefs showing Ck import closure. exploited specific taskslike, e.g., local query answering consistency checking. computing projected PEs,information communicated contexts minimized, keeping communication cost low.sequel, present basic version algorithm, abstracting low-level implementation issues; overall MCS structure assumed unknown context nodes. ideafollows: starting context Ck , visit import closure expanding import neighborhoodcontext Ci like depth-first search (DFS), leaf context reached cycle detected, finding current context set hist already visited contexts. leaf context simplycomputes local belief sets, transforms partial belief states, returns resultparent (invoking context, Figure 4a). case cycle (Figure 4c), context Ci detectscycle must also break it, (i) guessing belief sets export interface, (ii) transformingguesses partial belief states, (iii) returning invoking context.intermediate context Ci produces partial belief states joined, i.e., consistentlycombined, partial belief states neighbors; enable this, Ci returns local belief sets,joined results neighbors (Figure 4b).computing projected PEs, algorithm offers parameter V called relevant interfacemust fulfill conditions w.r.t. import closure next discuss.Notation. Given (partial) belief state set V B beliefs, denote S|V restrictionV, i.e., (partial) belief state 0 = (S1 |V , . . . , Sn |V ), Si |V = Si V Si 6= ,|V = ; set (partial) belief states, let S|V = {S|V | S}. Next,Definition 7 (Recursive Import Interface) MCS = (C1 , . . . , Cn ) k {1, . . . , n},call V(k) = {pi | (ci : pi ) B(r), r brk } import interface context Ck V (k) =iIC (k) V(i) recursive import interface context Ck .correct relevant interface V, two extremal cases: (1) V = V (k) (2) V = VB = B.(1), DMCS basically checks consistency import closure Ck computing PEsprojected interface beliefs. (2), computes PEs w.r.t. Ck . between, providing fixedinterface V, problem-specific knowledge (such query variables) and/or infrastructure informationexploited keep computations focused relevant projections partial belief states.projections partial belief states cached every context recomputationrecombination belief states local belief sets kept minimum.assume context Ck background process (or daemon Unix terminology)waits incoming requests form (V, hist), upon starts computation outlined552fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSSi =SS` ./Sjlsolve(S)(V, hist)}){i(V, hist)Sj`CiC`(V,hCjC`lsolve((, . . . , )) =(a) Leaf context(b) Intermediate contextVCihist={...,i,Cj...}C`Ct(c) Cycle breakingFigure 4: Basic distributed algorithm - casewiseAlgorithm 1. process also serves purpose keeping cache c(k) persistent.write Ci .DMCS(V, hist) specify send (V, hist) process context Ci waitreturn message.Algorithm 1 uses following primitives:function lsolve(S) (Algorithm 2): augments knowledge base kb current contextheads bridge rules br applicable w.r.t. partial belief state S, computeslocal belief sets using function ACC, combines local belief set S, returnsresulting set partial belief states;function guess(V, Ck ): guesses possible truth assignments relevant interface w.r.t.Ck , i.e., Bk V.5DMCS proceeds following way:(a) check cache appropriate partial belief state;5. order relate beliefs Bk , V either vector sets, variables V prefixed context ids;simplicity, kept V set without assumptions.553fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERAlgorithm 1: DMCS(V, hist) Ck = (Lk , kb k , br k )Input: V: relevant interface, hist: visited contextsData: c(k): static cacheOutput: set accumulated partial belief states(a)c(k) empty return c(k):=(b)(c)(d)k hist:= guess(V, Ck )else:= {(, . . . , )} hist := hist {k}foreach In(k), Ti =:= ./ Ci .DMCS(V, hist)(e)foreach := lsolve(T )(f)c(k) := S|V// cyclic: guess local beliefs w.r.t. V// acyclic: collect neighbor beliefs add local onesreturn S|VAlgorithm 2: lsolve(S) Ck = (Lk , kb k , br k )Input: S: partial belief state = (S1 , . . . , Sn )Output: set locally acceptable partial belief states:= ACCk (kb k {head (r) | r app(brk , S)})return {(S1 , , . . . , Sk1 , Tk , Sk+1 , . . . , Sn ) | Tk T}(b) check cycle;(c) cycle detected, guess partial belief states relevant interface contextrunning DMCS;(d) cycle detected, import neighbor contexts needed, request partial beliefstates neighbors join them;(e) compute local belief states given partial belief states collected neighbors;(f) cache current (projected) partial belief state.next examples illustrate evaluation runs DMCS finding partial equilibriadifferent MCS. start acyclic run.Example 7 Reconsider Example 4. Suppose user invokes C1 .DMCS(V, ),V = {a, b, c, f, g}, trigger evaluation process. Next, C1 forwards (d) requests C2C3 , call C4 . called first time, C4 calculates (e) belief setsassembles set partial belief statesS4 = {(, , , {f, g}), (, , , {f, g})} .554fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSVc(1) : S1C1S10 |V2 |Vc(3) : S3c(2) : S2C2C33 |VFigure 5: cyclic topologycaching S4 |V (f), C4 returns S4 |V = S4 one contexts C2 , C3 whose request arrivedfirst. second call, C4 simply returns S4 |V context cache.C2 C3 next call lsolve (in (e)) two times each, results S2 = resp. S3 =S, Example 6.= { (, {b}, , {f, g}) , (, {b}, , {f, g}) }= {(, , {c, d, e}, {f, g}), (, , {c, d, e}, {f, g}), (, , {c, d, e}, {f, g})} .Thus,S2 |V= { (, {b}, , {f, g}) , (, {b}, , {f, g}) }S3 |V= {(, , {c}, {f, g}), (, , {c}, {f, g}), (, , {c}, {f, g})} .C1 , computing (d)S2 |V ./ S3 |V = {(, {b}, {c}, {f, g}), (, {b}, {c}, {f, g}), (, {b}, {c}, {f, g})}calls lsolve (e) thrice compute final result:S1 |V = {({a}, {b}, {c}, {f, g}), ({a}, {b}, {c}, {f, g}), ({a}, {b}, {c}, {f, g})} .next example illustrates run DMCS cyclic topology.Example 8 Let = (C1 , C2 , C3 ) MCS Li ASP logic,kb 1 = , br 1 = {a (2 : b)};kb 2 = , br 2 = {b (3 : c)};kb 3 = , br 3 = {c (1 : a)}.Figure 5 shows cyclic topology . Suppose user sends request C1 calling C1 .DMCS(V, ) V = {a, b, c}. step (d) Algorithm 1, C1 calls C2 .DMCS(V, {1}),context C2 issues call C3 .DMCS(V, {1, 2}), thus C3 invokes C1 .DMCS(V, {1, 2, 3}).point, instance DMCS C1 detects cycle (b) guesses partial belief statesS10 = {({a}, , ), ({a}, , )}555fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER1 V. Then, following dotted lines Figure 5, set S10 |V = S10 return valuerequest C3 , joins initial empty belief state (, , ), gives uscalls lsolve(T ) (e), resultingS3 = {({a}, , {c, d}), ({a}, , {c, d}), ({a}, , {c, d})} .next step C3 return S3 |V back C2 , proceed C3 before. resultset belief statesS2 = {({a}, {b}, {c}), ({a}, {b}, {c}), ({a}, {b}, {c})} ,sent back C1 S2 |V . Notice belief state ({a}, {b}, {c}) inconsistentC1 , eventually eliminated C1 evaluates S2 |V lsolve.Next, C1 join S2 |V (, , ), yields S2 |V , use result call lsolve.union gives usS1 = {({a}, {b}, {c}), ({a}, {b}, {c})} ,also sent back user final result.Given MCS = (C1 , . . . , Cn ) context Ck , using recursive import interface Ck ,i.e., V (k), relevant interface safe (lower) bound correctness Algorithm 1.follows, let , Ck , V (k) above.Theorem 1 (Correctness DMCS partial equilibrium) every V V (k), holds0 Ck .DMCS(V, ) iff partial equilibrium w.r.t. Ck 0 = S|V .compute partial equilibria Ck use VB . holds using VB preservesbelief sets returned step (e), projection step (f) takes effect.Corollary 2 partial equilibrium w.r.t. Ck iff Ck .DMCS(VB , ).assumption single root context C1 , i.e., IC (1)2 n, DMCS computes equilibria.Corollary 3 MCS single root context C1 , equilibrium iffC1 .DMCS(VB , ).analysis algorithm yields following upper bound computational complexitycommunication activity.Proposition 4 Let = (C1 , . . . , Cn ) MCS. run DMCS context Ckinterface V, holds(1) total number calls lsolve exponentially bound n |V|, i.e., O(2n|V| ).(2) number messages exchanged contexts Ci , IC (k), bounded2 |E(k)|, E(k) = {(i, cj ) | IC (k), r bri , (cj : pj ) B(r)}.556fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS3.3 DiscussionAlgorithm DMCS naturally proceeds forward import direction context Ck . Thus, startingthere, computes partial equilibria cover Ck contexts import closure.contexts ignored; fact, unknown contexts closure. partialequilibria may exist Ck import closure, whole MCS could equilibrium,because, e.g., (P1) contexts access beliefs Ck closure get inconsistent, (P2)isolated context subsystem inconsistent.Enhancements DMCS may deal situations: (P1), context neighborhoodmay include importing supporting contexts. Intuitively, Ci imports Cj , Cimust register Cj . carefully adapting DMCS, solve (P1). However, (P2) remains;needs knowledge global system topology.suitable assumption manager exists every context Ci systemreach ask whether isolated inconsistent context subsystem exists; confirms this,Ci DMCS instance simply returns , eliminating partial equilibria.improve decentralization information hiding, weaken manager assumptionintroducing routers. Instead asking M, context Ci queries assigned router R, collectstopology information needed Ci looks cache. information exchange CiR flexible, depending system setting, could contain contexts import informationCi , isolated inconsistent contexts.advantage topological information Ci recognize cyclic acyclicbranches upfront; invocation order neighborhood optimized, startingacyclic branches entering cyclic subsystems. caching mechanism adaptedacyclic branches, intermediate results complete cache meaningful even acrossdifferent evaluation sessions.setting, safe assuming V (k) V. needed resp. Ckimport closure join-contexts, i.e., contexts least two parents. accesspath information context, could calculate V fly adjustMCS traversal. particular, tree- ring-shaped , restrict V locally sharedinterface Ck import neighbors, i.e., restricting V bridge atoms br k .presence join-contexts, V must made big enough, e.g. using path information. Furthermore,join-contexts may eliminated virtually splitting them, orthogonal parts contextsaccessed. way, scalability many contexts achieved.Next, present optimization techniques using topological information system.4. Topology-Based Optimization Algorithm (DMCSOPT)basic version, Algorithm DMCS uses metadata apart minimal informationcontext must know: interface every neighboring context. scalabilityissues tracked following problems:(1) contexts unaware context dependencies system beyond neighbors, thustreat neighbors equally. Specifically, cyclic dependencies remain undetected context,seeing invocation chain, requests models context chain. Furthermore, contextCi know whether neighbor Cj already requested models another neighbor Cj 0would passed Ci ; hence, Ci makes possibly superfluous request Cj 0 .557fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER(2) context Ci returns local models combined results neighbors. casemultiple models, result size become huge system size number neighborsincreases. fact, one main performance obstacles.section address optimizations increase scalability distributed MCS evaluation. Resorting methods graph theory, aim decomposing, pruning, improvedcycle breaking dependencies MCSs. Focusing (1), describe decomposition methodusing biconnected components inter-context dependencies. Based break cyclesprune acyclic parts ahead create acyclic query plan. address (2), foster partial viewsystem, often sufficient satisfactory answer, compromise partialinformation performance. thus define set variables import dependencysystem project models context bare minimum remain meaningful.manner, omit needless information circumvent excessive model combinations.proceed follows. introducing running example superficial explanationoptimization it, present details techniques Section 4.2. Section 4.3 introducesnotion query plans, used Section 4.4 describe algorithm DMCSOPTintertwines decomposition pruning variable projection performance gains MCS evaluation.4.1 Running Scenariofirst present scenario Example 9 running example section.Example 9 (Scientists Group) group four scientists, Alice, Bob, Charlie, Demi meetsconference closing arrange travel back home. options going train car(which slower); use train, bring along food. Alice groupleader finally decides, based information gets Bob Charlie.6Alice prefers go car, would object Bob Charlie want go train.Charlie daughter, Fiona; mind either option, Fiona sick wantsfastest transport get home. Demi got married, husband, Eddie, wants backsoon, even sooner would come soon; Demi tries yield husbands plea.Charlie charge buying provisions go train. might choose either saladpeanuts; notably, Alice allergic nuts. options beverages coke juice. Bobmodest; agrees choice Charlie Demi transport dislikes coke. CharlieDemi want bother others personal matters communicatepreferences, sufficient reaching agreement.Example 10 scenario Example 9 encoded MCS = (C1 , . . . , C6 ),Alice = 1, Bob = 2, etc lexicographical order Li ASP logics. knowledge bases kbibridge rules bri follows:car 1 train 1 .train 1 (2 : train 2 ), (3 : train 3 ).C1 : kb 1 =br 1 =;nuts 1 .nuts 1 (3 : peanuts 3 ).6. Similar scenarios already investigated realm multi-agent systems (on social answer set programming see, e.g., Buccafurri & Caminiti, 2008). aim introducing new semantics scenarios;example serves plain MCS showcase algorithms.558fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSB11212343454B2634B3365(b) Diamond-ring block tree(a) Diamond-ringFigure 6: Topologies decomposition scientist group exampleC2 : kb 2C3 : kb 3C4 : kb 4C5 : kb 5C6 : kb 6car 2 (3 : car 3 ), (4 : car 4 ).= { car 2 , train 2 .} br 2 = train 2 (3 : train 3 ), (4 : train 4 ), ;(3 : coke 3 ).car 3 train 3 .train 3 urgent 3 .urgent 3 (6 : sick 6 ).=;br 3 =salad 3 peanuts 3 train 3 .train 3 (4 : train 4 )coke 3 juice 3 train 3= car 4 train 4 br 4 = train 4 (5 : sooner 5 ) ;= sooner 5 soon 5 br 5 = soon 5 (4 : train 4 ) ;= sick 6 fit 6 br 6 = .context dependencies shown Fig. 6a. three equilibria, namely:({train 1 }, {train 2 }, {train 3 , urgent 3 , juice 3 , salad 3 }, {train 4 },{soon 5 , sooner 5 }, {sick 6 });({train 1 }, {train 2 }, {train 3 , juice 3 , salad 3 }, {train 4 }, {soon 5 , sooner 5 }, {fit 6 });({car 1 }, {car 2 }, {car 3 }, {car 4 }, , {fit 6 }).Example 11 Consider MCS = (C1 , . . . , C7 ) context dependencies drawn Figure 7a. user queries C1 cares local belief sets C1 ,evaluation process, C4 discard local belief sets C5 C6 answering callC2 C3 . However, C1 calls C2 (or C3 ), invoked context must carry local belief setsC4 answers C1 . reason belief sets C4 cause inconsistent joins C1partial belief states returned C2 C3 , C5 C7 contribute directlycomputing local belief sets C4 . Note belief sets C4 C7 play role determiningapplicability bridge rules C1 .559fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERqueryqueryC1C2C1C3C2C4C4,SC7(a) Original TopologyC3C7(, , S3 )(b) TriangleC6)C2C53(,S2,,C6S3)C1(C5C3(c) Transitive ReductionFigure 7: Topology Example 11 (two stacked zig-zag diamonds)Now, take sub-system including C1 , C2 , C3 , assuming C1 bridge rules atoms(2 : p2 ) (3 : p3 ) body, C2 atoms (3 : p3 ). is, C1 depends C2C3 , C2 depends C3 (see Fig. 7b). straightforward approach evaluate MCS asksC1 belief sets C2 C3 . C2 also depends C3 , would need another queryC2 C3 evaluate C2 w.r.t. belief sets C3 . shows evident redundancy, C3need compute belief sets twice. Simple caching strategies could mellow second beliefstate building C3 ; nonetheless, C1 asks C3 , context transmit belief states back,thus consuming network resources.Moreover, C2 asks PEs C3 , receive set PEs covers belief setsC3 addition contexts C3 import closure. excessive C1 view,needs know (2 : p2 ) (3 : p3 ). However, C1 needs belief states C2C3 reply C2 : C2 reports belief sets (which consistent w.r.t. C3 ), C1 cantalign belief sets received C2 received C3 . Realizing C2 also reportsbelief sets C3 , call C3 must made.4.2 Decomposition Nonmonotonic MCSBased observations above, present optimization strategy pursues two orthogonalgoals: (i) prune dependencies MCS cut superfluous transmissions, belief state building,joining belief states; (ii) minimize content transmissions. start definingtopology MCS.Definition 8 (Topology) topology MCS = (C1 , . . . , Cn ) directed graph GM =(V, E), V = {C1 , . . . , Cn } resp. V = {1, . . . , n} (i, j) E iff rule bratom (j:p) body.first optimization technique made three graph operations. get coarse viewtopology splitting biconnected components, form tree representation MCS.Then, edge removal techniques yield acyclic structures.560fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSsequel, use standard terminology graph theory (see Bondy & Murty, 2008);graphs directed default. may view undirected graphs directed graphsedges (u, v), (v, u) undirected edge {u, v}.graph G edges E(G), denote G\S maximal subgraph Gedges S. Suppose V 0 V (G) nonempty. subgraph G0 = (V 0 , E 0 ) Gvertex set V 0 edge set E = {(u, v) E(G) | u, v V (G)} subgraph inducedV 0 , denoted G[V 0 ]. induced subgraph G[V \ V 0 ] denoted G\V 0 ; results Gdeleting vertices V 0 together incident edges. V 0 = {v}, write G\v G\{v}.Two vertices u v G said connected, (directed) path u vG, i.e., sequence vertices u = v1 , v2 , . . . , vn = v, (vi , vi+1 ) E(G), =1, . . . , n1; path trivial n = 1. undirected graph G, connectedness equivalencerelation V (G). Thus partition V (G) nonempty subsets V1 , V2 , . . . , Vwtwo vertices u v G connected iff belong set Vi .subgraphs G[V1 ], G[V2 ], . . . , G[Vw ] called components G. w = 1 (i.e., G exactlyone component), G connected; otherwise, G disconnected.directed graph G strongly connected, vertices u, v V (G) path u vvice versa exists. strongly connected components G subgraphs G[V1 ], . . . , G[Vm ]unique partition graph G pairwise disjoint induced subgraphs (i.e., Vi Vj = )strongly connected.Furthermore, directed graph G weakly connected, turning edges undirected edgesyields connected graph. vertex c weakly connected graph G cut vertex, G\cdisconnected. biconnected graph weakly connected graph without cut vertices.block graph G maximalbiconnectedsubgraph G. Given set blocks B,union blocks B defined B = BB B, union two graphs G1 = (V1 , E1 )G2 = (V2 , E2 ) defined G1 G2 = (V1 V2 , E1 E2 ).Let (G) = (B C, E) denote undirected bipartite graph, called block tree graph G,(i) B set blocks G,(ii) C set cut vertices G,(iii) (B, c) E B B c C iff c V (B).Note (G) forest graph G rooted tree G weakly connected.Example 12 Consider graph Figure 7a. One check 4 cut vertextwo blocks, viz. subgraphs induced {1, 2, 3, 4} {4, 5, 6, 7}.next example shows block tree scenario Example 9.Example 13 topology GM Example 10 shown Figure 6a. two cut vertices,namely 3 4; thus block tree (GM ) (Figure 6b) contains blocks B1 , B2 , B3 ,subgraphs GM induced {1, 2, 3, 4}, {4, 5}, {3, 6}, respectively.topological sort directed graph linear ordering vertices everydirected edge (u, v) vertex u vertex v, u comes v ordering.561fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERPruning. acyclic topologies, like triangle presented Figure 7b, exploit minimalgraph representation avoid unnecessary calls contexts, namely, transitive reductiongraph GM . Recall Aho, Garey, Ullman (1972) graph G transitivereduction directed graph G whenever following two conditions satisfied:(i) directed path vertex u vertex v G iff directed path u vG,(ii) graph fewer edges G satisfying condition (i).Note G unique G acyclic. instance, graph Figure 7c unique transitivereduction one Figure 7a.Ear decomposition Another essential part optimization strategy break cycles removing edges. end, use ear decompositions cyclic graphs. block may multiplecycles necessarily strongly connected; thus first decompose blocks stronglyconnected components. Using Tarjans algorithm (Tarjan, 1972) task, one gets byproduct topological sort directed acyclic graph formed strongly connected components.yield sequence nodes r1 , . . . , rs used entry points component. nextstep break cycles.ear decomposition strongly connected graph G rooted node r sequence P =hP0 , . . . , Pm subgraphs G(i) G = P0 Pm ,(ii) P0 simple cycle (i.e., repeated edges vertices) r V (P0 ),(iii) Pi (i > 0) non-trivial path (without cycles) whose endpoint ti P0 Pi1 ,nodes not.Let cb(G, P ) set edges containing (`0 , r) P0 last edge (`i , ti ) Pi , > 0.Here, `0 vertex belonging edge root node r simple cycle P0 .Example 14 Take, example, strongly connected graph G Figure 8a. ear decomposition G rooted node 1 P = hP0 , P1 , P2 , P3VP0 = {1, 2, 3}, EP0 = {(1, 2), (2, 3), (3, 1)},VP1 = {2, 4, 3}, EP1 = {(2, 4), (4, 3)},VP2 = {2, 5, 3}, EP2 = {(2, 5), (5, 3)},VP3 = {1, 4}, EP4 = {(1, 4)}.last edges Pi dashed. form set cb(G, P ) = {(3, 1), (4, 3), (5, 3), (1, 4)}.Removing edges results acyclic topology Figure 8b.Intuitively, ear decomposition used remove cycles original system .resulting acyclic topology, algorithms evaluating MCSs designed conveniently.trade edge (`, t) removed , context C` , despite leafcontext, guess values variables Ct . following example shows applicationoptimization techniques running scenario.Example 15 (contd) Block B1 (GM ) acyclic, transitive reduction gives B1edges {(1, 2), (2, 3), (3, 4)}. B2 cyclic, hB2 ear decomposition rooted 4;removing cb(B2 , hB2 i) = {(5, 4)}, obtain B20 edges {(4, 5)}. B3 acyclic alreadyreduced. Fig. 6b shows final result (dotted edges removed).562fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS1213234455(a) strongly connected component(b) Acyclic topologyFigure 8: Ear decomposition examplegraph-theoretic concepts introduced here, particular transitive reduction acyclic blocksear decomposition cyclic blocks, used implement first optimization MCSevaluation outlined above. Intuitively, block, apply ear decomposition get rid cycles (with trade-off guessing), use transitive reduction minimize communication.Given transitive reduction B acyclic block B B, total order V (B ) , oneevaluate respective contexts reverse order total order computing PEscontext Ck : first context simply computes local belief sets whichrepresentedset partial belief states S0 constitutes initial set partial belief states T0 . iterationstep 1, Ti computed joining Ti1 local belief sets Si considered context Ci .Given final Tk , Tk |V (k) set PEs Ck (restricted contexts V (B )).Refined recursive import. Next, define second part optimization strategyhandles minimization information needed transmission two neighboring contextsCi Cj . purpose, refine notion recursive import interface (Definition 7)context w.r.t. particular neighbor given (sub-)graph.Definition 9 Given MCS = (C1 , . . . , Cn ) subgraph G GM ,edge (i, j)E(G), recursive import interface Ci Cj w.r.t. G V (i, j)G = V (i) `G|j B`G|j contains nodes G reachable j.7Example 16 (contd) MCS Example 10, V (1) = {train 2 , train 3 , peanuts 3 ,car 3 , coke 3 , car 4 , train 4 , sooner 5 , sick 6 }. focus block B1 , refined recursiveimport interface V (1, 2)B obtained removing bridge atoms contexts1blocks B2 B3 , yielding {train 2 , train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }.Algorithms. Algorithms 3 4 combine optimization techniques outlined above. Intuitively,OptimizeTree takes input block tree parent cut vertex cp root cut vertex cr . traverses DFS manner calls OptimizeBlock every block. call results collected7. Note V (k) defined Definition 7.563fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERAlgorithm 3: OptimizeTree(T = (B C, E), cp , cr )Input: : block tree, cp : identifiesSlevel , cr : identifieslevel cpOutput: F : removed edges B, v: labels ( B)\FB 0 := , F := , v :=cp = crB 0 := {B B | cr V (B)}elseB 0 := {B B | (B, cp ) E}(a)(b)// initialize siblings B0 return valuesforeach sibling block B B 0// sibling blocks B parent cpE := OptimizeBlock(B, cp )// prune block0C := {c C | (B, c) E c 6= cp }// children cut vertices BB 0 := B\E, F := F Eforeach edge (i, j) B 0 doS// setup interface pruned Bv(i, j) := V (i, j)B 0 cC 0 V (cp )|Bc (`,t)E V (cp )|Btforeach child cut vertex c C 0// accumulate children(F 0 , v 0 ) := OptimizeTree(T \B, c, cp )F := F F 0 , v := v v 0return (F, v)set F removed edges; blocks processed, final result OptimizeTreepair (F, v) v labeling remaining edges. OptimizeBlock takes graph G callsCycleBreaker cyclic G, decomposes G strongly connected components, createsear decomposition P component Gc , breaks cycles removing edges cb(Gc , P ).resulting acyclic subgraph G, OptimizeBlock computes transitive reduction Greturns edges removed G. OptimizeTree continues computing labeling v remaining edges, building recursive import interface, keeping relevantinterface beliefs child cut vertices removed edges. Example 20 (Appendix B) illustratesAlgorithms 3 4 detailed run MCS Example 10.Formally, following property holds.Proposition 5 Given MCS context Ck k cut vertex topology GM ,OptimizeTree(T (GM ), k, k) returns pair (F, v)(i) subgraph G GM \F induced IC (k) acyclic,(ii) block B G (i, j) E(B), holds v(i, j) V (i, j)B .Regarding computational cost computation, obtain:Proposition 6 Given MCS context Ck k cut vertex topology GM ,OptimizeTree(T (GM ), k, k) runs polynomial (quadratic) time size (GM ) resp. GM .564fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSAlgorithm 4: OptimizeBlock(G : graph, r : context id)(c)(d)F :=G cyclicF := CycleBreaker(G, r)// ear decomposition strongly connected componentsLet G transitive reduction G\Freturn E(G) \ E(G )// removed edges G4.3 Query PlanGiven topology MCS, need represent stripped version containsminimal dependencies contexts interface beliefs need transferred contexts. representation query plan used execution processing.Syntactically, query plans following form.Definition 10 (Query Plan) query plan MCS w.r.t. context Ck labeled subgraphGM induced IC (k) E() E(GM ), edge labels v : E(G) 2 .MCS context Ck , every query plan suitable evaluating M; however,following query plan fact effective.Definition 11 (Effective Query Plan) Given MCS context Ck , effective query planrespect Ck k = (V (G), E(G)\F, v) G subgraph GM inducedIC (k) (F, v) = OptimizeTree(T (GM ), k, k).next use k MCS evaluation, tacitly assume query plans effective.4.4 Evaluation Query Planspresent algorithm DMCSOPT, based DMCS exploits optimizationtechniques above. idea DMCSOPT follows: start context Ck traversegiven query plan k expanding outgoing edges k context, like DFS,leaf context Ci reached. context simply computes local belief sets, transforms beliefsets partial belief states, returns result parent. Ci (j : p) bridge rulesbodies context Cj query plan (this means broke cycle removing lastedge Cj ), possible truth assignments import interface Cj considered.result context Ci set partial belief states, amounts join, i.e.,consistent combination, local belief sets results neighbors; final resultobtained Ck . keep recomputation recombination belief states local belief setsminimum, partial belief states cached every context.Algorithm 5 shows distributed algorithm, DMCSOPT, instance context Ck .input id c predecessor context (which process awaits), proceeds based(acyclic) query plan r w.r.t. context Cr , i.e., starting context system. algorithmmaintains cache(k) cache Ck (which kept persistent).Ci .DMCSOPT(c): send id c DMCSOPT context Ci wait result.guess(V): guess possible truth assignments interface beliefs V.565fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERAlgorithm 5: DMCSOPT(c : context id predecessor) Ck = (Lk , kb k , br k )Data: r : query plan w.r.t. starting context Cr label v, cache(k): cacheOutput: set accumulated partial belief states(a)(b)(c)(d)(e)cache(k) empty:= cache(k)else:= {(, . . . , )}foreach (k, i) E(r ) := ./ Ci .DMCSOPT(k)// neighbor belief setsIn(k) s.t. (k, i)/ E(r ) Ti =:= guess(v(c, k)) ./// guess removed dependencies r:=foreach := lsolve(T )// get local beliefs w.r.t.cache(k) :=(c, k) E(r ) (i.e., Ck non-root)return S|v(c,k)elsereturnlsolve(S) (Algorithm 2): given partial belief state S, augment kbk headsbridge rules brk applicable w.r.t. (=: kb0k ), compute local belief sets ACC(kb0k ),merge S; return resulting set partial belief states.steps Algorithm 5 explained follows:(a)+(b) check cache, empty get neighbor contexts query plan, requestpartial belief states neighbors join them;(c) (i : p) bridge rules brk (k, i)/ E(r ), neighbor deliveredbelief sets Ci step (b) (i.e., Ti = ), call guess interface v(c, k)join result (intuitively, happens edges removedcycles);(d) compute local belief states given partial belief states collected neighbors;(e) return locally computed belief states project variables v(c, k) nonroot contexts; point mask parts belief states neededcontexts lying different block (GM ).Theorem 7 shows DMCSOPT sound complete.Theorem 7 Let Ck context MCS , let k query plan Definition 11 letb = {p v(k, j) | (k, j) E(k )}. Then,V(i) 0 Ck .DMCSOPT(k), exists partial equilibrium w.r.t. Ck0 = S|Vb ;(ii) partial equilibrium w.r.t. Ck , exists 0 Ck .DMCSOPT(k)0 = S|Vb .566fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS5. Streaming Equilibria (DMCS-STREAMING)Algorithm DMCSOPT shows substantial improvements DMCS; however, sizeslocal knowledge bases context interfaces increase, also suffers bottlenecks.stems way models exchanged contexts. Suppose context C1accesses several neighbors C2 , . . . , Cm acyclic information flow, Ci , n,ni PEs. Ci computes DMCS resp. DMCSOPT local models, must join PEsneighbors; may lead n2 n3 nm many PEs, inputlocal solver. may take considerable time also exhaust memory, evenlocal model computation starts.Note however instead neighbor would transfer portion PEs,computation C1 avoid memory blowup. Moreover, strategy also helps reduceinactive running time C1 waiting neighbors return PEs, C1 already startlocal computing neighbors producing models.general, indispensable trade computation time, due recomputations, lessmemory eventually partial equilibria C1 shall computed. idea underlyingstreaming evaluation method distributed MCS. particularly useful user interestedobtaining instead answers system, also realistic scenarioscurrent evaluation algorithm manage output resource constraintspractice equilibrium all.section, turn idea concrete streaming algorithm DMCS-STREAMINGcomputing partial equilibria. main features briefly summarized follows:algorithm fully distributed, i.e., instances components run every contextcommunicate, thus cooperating level peers;invoked context Ci , algorithm streams (i.e. computes) k 1 partial equilibriaCi time; particular, setting k = 1 allows consistency checking MCS(sub-)system.issuing follow-up invocations one may compute next k partial equilibria context C1equilibria exist; i.e., evaluation scheme complete.local buffers used storing exchanging local models (partial belief states)contexts, avoiding space explosion problem.section mainly studies streaming aspect algorithm, simplify presentation omit interface contexts. principles presented appliedDMCS DMCSOPT adapting interface pruning topology preprocessingtime. Furthermore, assume work acyclic MCSs. Treatment cyclic cases easilyachieved adding guessing code solving component DMCS DMCSOPT.best knowledge, similar streaming algorithm neither developedparticular case computing equilibria MCS, generally computing modelsdistributed knowledge bases. Thus, results obtained interest settingheterogeneous MCS, also relevant general model computation reasoningdistributed (potentially homogeneous) knowledge bases like e.g. distributed SAT instances.567fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERrequest (k1 , k2 )k belief statesHandlerCiCj1SolverJoiner...OutputCjmFigure 9: DMCS-STREAMING architectureAlgorithm 6: Handler(k1 , k2 : package range) CiOutput.k1 := k1 , Output.k2 := k2 ,Solver.k2 := k2 , Joiner.k := k2 k1 + 1call Solver5.1 Basic Streaming Procedurebasic idea follows: pair neighboring contexts communicate multiple rounds,request effect receive k PEs. communication window k PEsranges k1 -th PE k2 -th (= k1 + k 1) PE. parent context Ci requests childcontext Cj pair (k1 , k2 ) receive time later package k PEs; receivingindicates Cj fewer k1 models. parallelized version discussed Section 5.2.Important subroutines new algorithm DMCS-STREAMING take care receivingrequests parents, receiving joining answers neighbors, local solving returningresults parents. reflected four components: Handler, Solver, Output, Joiner(only active non-leaf contexts); see Figure 9 architectural overview.components except Handler (shown Algorithm 6) communicate using message queues:Joiner j queues store partial equilibria j neighbors, Solver one queue hold joinedPEs Joiner, Output queue carry results Solver. bound space usage,queue limit number entries. queue full (resp., empty), enqueuing writer(resp., dequeuing reader) automatically blocked. Furthermore, getting element also removesqueue, makes room PEs queue later. property frees ussynchronization technicalities.Algorithms 7 8 show Solver Joiner work. use following primitives:lsolve(S): works lsolve DMCS DMCSOPT, addition may return oneanswer time may able tell whether models left. Moreover, requireresults lsolve returned fixed order, regardless called. propertykey guarantee correctness algorithm.get first(`1 , `2 , k): send neighbor c`1 c`2 request first k partial equilibria, i.e., k1 = 1 k2 = k; return models, store respective queuesreturn true; otherwise, return false (some neighbor inconsistent).568fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSAlgorithm 7: Solver() CiData: Input queue: q, maximal number models: k2(a)(b)count := 0count < k2Ci leaf :=else call Joiner pop q= count := k2(c)count < k2pick next model ? lsolve(S)? 6=push ? Output.qcount := count + 1else breakrefresh() push Output.qget next(`, k): request next k equilibria neighbor Cc` ; Cc` sends back models, store queue q` return true; otherwise, return false neighbor alreadyexhaustively returned PEs previous request. Note subroutine needs keeptrack range already asked neighbor, maintaining set counters.counter w.r.t. neighbor Cc` initialized 0 increased time get next(`, k) called.value t, request Cc` asks tth package k models, i.e., models rangegiven k1 = (t 1) k + 1 k2 = k. get first(`1 , `2 , k) called, countersrange [`1 , `2 ] reset 0.refresh(): reset counters flags Joiner starting states, e.g., first join true,counters 0.process context Ci triggered message parent, containsrange (k1 , k2 ) arrives Handler. latter notifies Solver compute k2 models Outputcollect range (k1 , k2 ) return parent. Furthermore, sets packagesize Joiner k = k2 k1 + 1 case Ci needs query neighbors (cf. Algorithm 6).Solver receives notification Handler, first prepares input local solver.Ci leaf context, input gets empty set assigned Step (a); otherwise, Solver triggersJoiner (Step (b)) input neighbors. Fed input them, lsolve used Step (c)compute k2 results send output queue.Joiner, activated intermediate contexts discussed, gathers partial equilibria neighbors fixed ordering stores joined, consistent input local buffer.communicates one input time Solver upon request. fixed joining order guaranteed always asking first package k models neighbors beginning Step (d).subsequent rounds, begin finding first neighbor Cc` return models(Step (e)), reset query ask first packs k models neighbors Cc1 Cc`1 .neighbors run models Step (f), joining process ends sends Solver.Note procedure guarantees models missed, lead consider combinations (inputs Solver) multiple times. Using cache helps mitigate569fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERAlgorithm 8: Joiner() CiData: Queue q1 , . . . , queue qj In(i) = {c1 , . . . , cj }, buffer partial equilibria: buf ,flag first jointruebuf emptypop buf , push Solver.qreturn(d)(e)(f)first joinget first(1, j, k) = falsepush Solver.qreturnelse first join := falseelse` := 1get next(`, k) = false ` j ` := ` + 11 < ` jget first(1, ` 1, k)else ` > jpush Solver.qreturnS1 q1 , . . . , Sj qj add S1 ./ ./ Sj bufC1C2C4C3C5C6C7Figure 10: Binary tree MCSrecomputation, unlimited buffering quickly exceeds memory limits, recomputationinevitable part trading computation time less memory.Output component simply reads queue receives reaches k2 models(cf. Algorithm 9). Upon reading, throws away first k1 1 models keeps onesk1 onwards. Eventually, fewer k1 models returned Solver, Outputreturn parent.Example 17 Let = (C1 , . . . , Cn ) MCS given integer > 0, n =2m+1 1 contexts, let ` > 0 integer. Let contexts ASP logics. < 2m ,570fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSAlgorithm 9: Output() CiData: Input queue: q, starting model: k1 , end model: k2buf := count := 0count < k1pick Output.q= count := k2 + 1else count := count + 1count < k2 + 1wait Output.q= count := k2 + 1elsecount := count + 1add bufbuf emptysend parentelsesend content buf parentcontext Ci = (Li , kbi , bri )(kbi ={ajiajiti | 1 j `} bri =)fifiti (2i : aj2i ),fi1j` ,ti (2i + 1 : aj2i+1 ) fi(2)2m , let Cikbi = {aji aji | 1 j `} bri = .(3)Intuitively, binary tree-shaped MCS depth `+1 size alphabetcontext. Figure 10 shows MCS n = 7 contexts depth = 2; internal contextsknowledge bases bridge rules (2), leaf contexts (3). directededges show dependencies bridge rules. system equilibria = (S1 , . . . , Sn )Si = {aki , ti }, 1 k `.compute one PE using DMCS DMCSOPT, one needs transfer packages 2`PEs context parent (as context Ci computes subsets {a1i , . . . , a`i }).intermediate context receives 2` results children, whose join leads 22` inputslsolve; invokes lsolve often returns 2` models parent,wait this.hand, DMCS-STREAMING needs transfer single PE pairconnected contexts, significant saving. Indeed, consider e.g. = 1, ` = 5, i.e.,= (C1 , C2 , C3 ). Querying C1 package size k = 1 first causes query forwardedC2 pair k1 = k2 = 1. C2 leaf context, invokes local solver eventually gets fivedifferent models. However, returns one PE C1 , say (, {a12 }, ). Note t2 projectedamong atoms C2 accessed C1 . happens C3 , assume571fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERreturn (, , {a23 }) C1 . root context C1 , two PEs neighbors consistentlycombined (, {a12 }, {a23 }). Feeding local solver, C1 obtains five models, returnsone them, say = ({a11 , t1 }, {a12 }, {a23 }).following proposition shows correctness algorithm.Proposition 8 Let = (C1 , . . . , Cn ) MCS, {1, . . . , n} let k 1 integer.input (1, k) Ci .Handler, Ci .Output returns k different partial equilibria respect Ci ,fact k least k partial equilibria exist.5.2 Parallelized Streamingone might expect, strategy ignoring k1 models collecting next klikely effective. reason context uses one Solver,general serve one parent, i.e., requests different ranges models size k.new parent context requests models, refresh state Solver Joinerredo scratch. unavoidable, unless context satisfies specific property oneparent call it.way address problem parallelization. idea serve parent suiteHandler, Joiner, Solver Output. basic interaction units still shownFigure 9, notable difference component runs individual thread.significant change Solver control Joiner waits queue get new inputlocal solving process. Joiner independently queries neighbors, combines PEsneighbors, puts results Solver queue.effect waste recomputation time unused models. However, parallelization limits practice. DMCSOPT may run memory, unlimited parallelinstances streaming algorithm exceed number threads/processes operatingsystem support; happens contexts reach others many alternative paths, likestacked diamond topology: number threads exponential number connected contexts, prohibits scaling large system sizes. However, real-world applications numberpaths might still ok.compromise two extremes bounded parallel algorithm. idea createfixed-size pool multiple threads components share among contexts; incoming requests cannot served resources available, algorithm continues basicstreaming procedure. realization remains future work.6. Experimental Evaluationimplemented algorithms using C ++ system prototype called DMCS,available online.8 space reasons, omit detailed presentation refer workBairakdar, Dao-Tran, Eiter, Fink, Krennwallner (2010b), Dao-Tran (2014, ch. 7). Briefly,main components global architecture (i) command-line frontend dmcs useraccess system; (ii) demons daemon represent nodes contain (a set of) contexts;(iii) manager dmcsm containing meta-information MCS (topology, interfaces)8. http://www.kr.tuwien.ac.at/research/systems/dmcs,https://github.com/DistributedMCS/dmcs/572fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSC1C2C1C3C2C3C2C1C4C2C5C3C4C6C5C3C1C6C4C4C5C6C7(a) Binary Tree (T)C7C7(b) Diamond (D)(c) Zig-Zag (Z)C4(d) Ring (R)Figure 11: Topologies testing DMCS algorithmshelper dmcsgen generating configurations optimized components. Contexts implemented groups threads communicate concurrent message queues.system two main command-line tools, viz. running algorihms test case generation, respectively. allows switch different algorithms modes simply changingcommand-line arguments.turn experimental evaluation DMCS various aspects. Next describebenchmarks set up, go runs results interpretation.6.1 Benchmark Setupidea analyze strong weak points algorithm respect different parameters,namely system topology, system size, local theory (i.e., knowledge base) size, interface size.Specifically, considered MCSs topologies Figure 11, including:Binary Tree (T): Binary trees grow balanced, i.e., every level except last one complete.topology, edge needs removed form optimal topology; everyintermediate node cut-vertex, import interface query plan drastically reduced,leading extreme performance improvement.(Stack of) Diamond(s) (D): diamond consists four nodes connecting C1 C4 Figure 11b. stack diamonds combines multiple diamonds row, i.e., stacking diamondstower 3m + 1 contexts. Similar Binary Tree, edge removed constructingquery plan. W.r.t. topology, every context connecting two diamonds cut-vertex.such, import interface query plan refined every diamond; avoidssignificantly repetition partial PEs evaluation.(Stack of) Zig-Zag Diamond(s) (Z): zig-zag diamond diamond connectiontwo middle contexts, depicted contexts C2 C4 Figure 11c. stack zigzag diamonds built above. topology interesting removing two edges perblock, query plan turns linear topology.Ring (R): ring (Figure 11d). query plan removes connection context Cn C1carries interface way back C1 . topology requires guess573fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERa1a2a3a4a5a6a7a8Figure 12: Local theory structureing checking DMCS algorithm; thus quite unpredictable algorithmperforms better general.quantitative parameters represented tuple P = (n, s, b, r),n system size (number contexts),local theory size (number ground atoms local theory),b number local atoms used bridge atoms contexts,words, number interface atoms,r maximal number bridge rules. generator generates bridge rule iterating1 r 50% chance; hence average r/2 bridge rules generated. allowbridge bodies size 1 2.test configuration formulated X/(n, s, b, r) X {T, D, Z, R} represents topology n, s, b, r integers representing quantitative (i.e., size-related) parameters.would like run several instances one configuration, final formulation test instanceXi /(n, s, b, r), index test instance.Inside context, local theories structured follows. Context Ci ground atomsindicated ai,1 , . . . , ai,s . Rules form ai,j ai,k k = j + 1, j odd;otherwise, randomly choose k j 1 j + 1 probability 50% possibility.case k > rule exist. example context local theory size 8illustrated dependency graph Figure 12. Here, bold arrows stand fixedrules dashed arrows stands rules decided randomization. corresponding localtheory figure is:a1 a2a2 a1a3 a4a4 a3a4 a5a5 a6a6 a7a7 a8.setting, local context 2m answer sets, [0, s/2].Furthermore, one obtain deterministic contexts (having one answer set) disallowingcycles structure local theories.6.2 Experimentsconducted experiments host system using 4-core Intel(R) Xeon(R) CPU 3.0GHz processor 16GB RAM, running Ubuntu Linux 12.04.1. Furthermore, used DLV [build BEN/Sep28 2011 gcc 4.3.3] back-end ASP solver.ran comprehensive set benchmarks setup described Section 6.1.parameter space P = (n, s, b, r) huge, singled initial probing phase followingvalues experiments:574fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS1000DMCSDMCSOPT1001010.10.01-5T7,10,5,505D7,10,5,510Z7,10,5,515R7,10,5,520T10,10,5,525D10,10,5,53035Z10,10,5,540R10,10,5,54550Figure 13: DMCS vs. DMCSOPT non-streaming modesystem size n, depending topology:T:D:n {7, 10, 15, 31, 70, 100}n {4, 7, 10, 13, 25, 31}Z:R:n {4, 7, 10, 13, 25, 31, 70}n {4, 7, 10, 13, 70}s, b, r fixed either 10, 5, 5 20, 10, 10, respectively.combination topology X parameters P = (n, s, b, r) denoted X(n, s, b, r) X n,s,b,r(used figures). parameter setting tested five instances. instance,measured total running time total number returned partial equilibria DMCS,DMCSOPT non-streaming streaming mode. latter mode, DMCS-STREAMING,asked k answers, k {1, 10, 100}. parameter also influences size packagestransferred contexts (at k partial equilibria transferred one message).streaming mode, asking one PE may require multiple rounds get answers,interest see fast first answers arrive compared answers. thus comparedrunning time tasks k = 10 k = 100.6.3 Observations InterpretationsFigures 13-17 summarize results experiments. Run times seconds timeout600 seconds. data, several interesting properties observed. organizeanalysis along following aspects: (1) comparing DMCS DMCSOPT, (2) comparingstreaming non-streaming mode, (3) effect package size, (4) role topologies,(5) behavior algorithms deterministic contexts.6.3.1 DMCS VS . DMCSOPTFigure 13 shows running time DMCS DMCSOPT computing partial equilibria, i.e.,non-streaming mode, five instances respective parameter settings. Clearly DMCSOPToutperforms DMCS. explained fact computing answers, DMCSalways produces partial equilibria DMCSOPT, one PE returned DMCSOPT575fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER100100DMCS-1stDMCSOPT-1stDMCS-10DMCSOPT-10DMCS-1stDMCSOPT-1stDMCS-100DMCSOPT-1001010110.1T1T2T3T4T5D1(a) (25, 10, 5, 5)D2D3D4D5(b) D(10, 10, 5, 5)1000100DMCS-1stDMCSOPT-1stDMCS-10DMCSOPT-10DMCS-1stDMCSOPT-1stDMCS-10DMCSOPT-101001010110.10.1Z1Z2Z3Z4Z5R1(c) Z(10, 10, 5, 5)R2R3R4R5(d) R(4, 10, 5, 5)Figure 14: DMCS vs. DMCSOPT streaming modeobtained projecting many partial equilibria returned DMCS imported interface.Furthermore, intermediate results transferred one message, makes differenceterms number communications algorithms. such, DMCS must spendtime processing possibly exponentially input; hence, unsurprisingly, consistentlyslower DMCSOPT.However, observation streaming mode different. Figure 14 shows running timeDMCS DMCSOPT streaming mode compute first 100 respectively 10 unique partialequilibria (25, 10, 5, 5) respectively D(10, 10, 5, 5), Z(10, 10, 5, 5) R(4, 10, 5, 5).first view, DMCSOPT consistently slower DMCS, one might question correctnessresults. However, surprise: PE returned DMCSOPT correspond several PEs returned DMCS. Hence, batch first k unique answers DMCScorresponds smaller number (few) unique answers DMCSOPT.Therefore, comparing DMCS DMCSOPT streaming mode measuring runtimecompute first k answers fair. thus took time algorithms finished firstround answers (denoted DMCS-1st DMCSOPT-1st Figure 14). setting,observed following:majority cases DMCSOPT finishes first round faster DMCS, however40% instances, way around; shows effect using query plan;576fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMShowever, cases DMCS wins. explained follows. First all, streamingmode, transfer packages k partial equilibria time; therefore, effect reducingamount total work done always apply non-streaming mode. Furthermore,every context, compute k PEs project output interface returningresults. According strategy, context Ci returns k1 partial equilibria non-streamingmode k2 partial equilibria streaming another context Cj , might happen k2 muchsmaller k1 hence provide enough input Cj compute k PEs. Therefore, Cjissue requests Ci asking packages k PEs, e.g., [k + 1, 2k], [2k + 1, 3k],etc; costs DMCSOPT time even compute first batch PEs root context.Another approach compute always k unique partial equilibria returning parentcontext. However, strategy risks compute even local models k unique partialequilibria found.Overall, much difference running time DMCSOPT slower DMCS, exceptinstance R3 (Figure 14d). however comes different reason: cyclic topologyguess-and-check effects, play much important role choosing DMCSDMCSOPT (see Section 6.3.4).6.3.2 TREAMING VS . N - STREAMING DMCScompare streaming non-streaming algorithm (DMCS resp. DMCSOPT).Figure 15 shows results DMCS (15a), results DMCSOPT computefirst 10 resp. 100 PEs small systems/local knowledge bases (15b) large systems/local theories (15c). Excluding Ring (which behaves abnormally due guess-and-check)one see that:DMCS, streaming mode definitely worth pursuing since DMCS non-streamingmode times many cases (see also Figure 13), streaming mode still could findanswers reasonable time.DMCSOPT, situation bit different, streaming loses non-streamingsmall instances. due recomputation streaming mode pays transferringchunks partial equilibria contexts; furthermore, duplications answers.one moves larger systems local knowledge bases, streaming mode starts gainingback. However, always win, recomputation still significantly takes time cases.Summing up, system small enough, one try non-streaming modeavoids recomputation duplication PEs different rounds computation. largesystems, streaming rescue us timing out. Even pay recomputation, stillhelps cases results needed, e.g. brave query answering (membershipquery PE).6.3.3 E FFECTS PACKAGE IZE TREAMING ODEconsiderations raise question optimal number PEs transferredreturn messages contexts. analyze experimental results streamingmode package sizes 1, 10, 100 give hints this.Figure 16 shows average time compute 1 PE DMCSOPT streaming moderespect three package sizes. One see transferring single PE get first answer577fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER1000Non-StreamingStreaming-10Streaming-1001001010.10.01-5T10,10,5,505D10,10,5,51015Z10,10,5,520R4,10,5,52530(a) DMCS1000Non-StreamingStreaming-10Streaming-1001001010.10.01-5T10,10,5,505D10,10,5,51015Z10,10,5,520R4,10,5,52530(b) DMCSOPT small systems local theories1000Non-StreamingStreaming-10Streaming-1001001010.10.01-50T31,20,10,105D10,20,10,101015Z10,20,10,1020R4,20,10,102530(c) DMCSOPT large systems local theoriesFigure 15: Non-streaming vs. streaming DMCS DMCSOPTacceptable cases, particular guessing needed. Moving size 1 smallpackage size like 10 sometimes better, one save communication time (sendingpackage 10 partial equilibria vs. sending ten times package single PE). setting(small package sizes like 10) effective communication big factor,578fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS1000Streaming-1Streaming-10Streaming-1001001010.10.010.001-50T100,20,10,105D25,20,10,1010Z70,20,10,1015R4,20,10,102025Figure 16: Average time DMCSOPT find one partial equilibrium streaming mode, varyingpackage sizehappens real applications contexts located physically distributed nodes.cases, computing 10 partial equilibria faster computing 1 PE 10 consecutive times.Furthermore, package size 1 safe cases guessing applied, e.g.,R3 (4, 20, 10, 10). cases, large enough package size might help cover correctguess; general, guarantee coverage. thoroughly solve problem,one needs apply conflict learning whole MCS evaluation.Also, interesting see package size 100, DMCSOPT usually times out.reason many duplications DMCSOPT stuck local search branchpromises fewer 100 partial equilibria, algorithm lose time without findingnew unique answers eventually time out.find good package size p specific setting (topology, system size, local theory size),one may run system training set apply binary search p.6.3.4 E FFECT OPOLOGYquick glance plots Figures 1316 reveals pattern algorithms, especiallyoptimizations, perform better tree zigzag diamond, depending DMCSDMCSOPT, worst ring.system topology plays important role here. aspects affect performancealgorithms (i) number connections, (ii) structure block trees cut vertices,(iii) acyclicity vs. cyclicity.Regarding (i), topology introduces number connections based system size. Treefewer connections Diamond Zigzag, reduces communication alsolocal solving time fewer requests made; performance DMCS topologiesproves observation. one follows argument, Ring must offer best performance.However, actually case due aspect (iii) shortly analyze below.Concerning (ii), tree ultimately optimized every intermediate node cut vertex.Hence, applying query plan DMCSOPT, strip beliefs PEs sentchild contexts parent context. words, local beliefs context Ci neededtransferred back parents. drastically decreases amount information579fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER1000DMCS-100DMCSOPT-100100101-20R4,10,5,5246R7,10,5,581012R4,20,10,10141618Figure 17: DMCS vs. DMCSOPT streaming mode package size 100 ringcommunicated, importantly, number calls lsolve. Due special property,DMCSOPT performs extremely well tree topology, scales hundreds contexts.Comparing Diamond Zigzag, number cut vertices. However, Zigzagconverted linear topology optimal query plan (cf. Figure 11c), thereforeprocessed much faster Diamond. Figure 16, DMCSOPT scales Zigzag 70 contextsaverage time compute one answer still better one diamond 25 contexts.Regarding (iii), Ring cyclic topology topologies acyclic. Hencealgorithms must guess-and-check context topology. Makingright guess important, even important reducing communication calls localsolvers. result running DMCS DMCSOPT topology (Figure 17) followpattern; absolutely depends specific instance whether sequential guessingluckily arrives result. Therefore, frequently see DMCS outperforms DMCSOPTstreaming mode, cases, guessing root context (after detecting cycle)effective guessing parent root context according optimal query plan.Based observations, one come best strategy evaluate different typestopologies. dealing MCSs arbitrary topologies, looks natural decomposeparts familiar topologies efficient strategies known, combinestrategies overall evaluation method. Studying beyond scope workinteresting issue future research.6.3.5 B EHAVIOR ETERMINISTIC C ONTEXTSconsidered algorithms MCSs consisting possibly non-deterministic contexts,i.e., one acceptable belief set per knowledge base. intriguing seealgorithms behave contexts always exactly one accepted belief set per knowledge base;might underlying logic genuinely deterministic accepted belief setclear (e.g., closure classical logic) among multiple candidates particular belief set chosen(in implementations typically first best solution computed, e.g. SAT solving ASP).observed that:non-cyclic topologies, performance difference DMCS DMCSOPT,smaller interface used DMCSOPT reduce number intermediate PEstransferred contexts, one partial equilibrium computed every context.580fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS1000DMCSDMCSOPTMCS-IE1001010.10.01-5T7,10,5,505D7,10,5,510Z7,10,5,515R7,10,5,520T10,10,5,525D10,10,5,53035Z10,10,5,540R10,10,5,54550Figure 18: DMCS vs. DMCSOPT streaming mode package size 100 ringcyclic topology (Ring), guessing plays main role. Hence depends individualinstance whether DMCS DMCSOPT wins, like case non-deterministic contexts (cf.Section 6.3.4).non-streaming mode much faster streaming (on DMCS DMCSOPT);reasonable request partial equilibria redundant.6.3.6 C OMPARISON MCS-IE P2P-DRSystems close DMCS MCS-IE (Bogl et al., 2010)9 P2P-DR (Bikakis, Antoniou, & Hassapis, 2011). former plugin dlvhex system originally developed computeexplanations inconsistency Multi-context Systems, also includes mode computingequilibria MCS. However, MCS-IE implemented centralized approach. Figure 18presents run time DMCS, DMCSOPT comparison MCS-IE computing partial equilibria respective configurations. shows MCS-IE outperforms DMCS sinceinherits powerful decomposition technique dlvhex; however, decomposition basedtopological information DMCSOPT turns efficient, also localizes interface beliefs communicate blocks contexts, specific MCSexploited general decomposition technique dlvhex.P2P-DR supports distributed query answering multi-context systems based defeasiblelogic; details, see Section 7. present comparison DMCS P2P-DR.converted benchmark P2P-DRs style converting local knowledge basesbridge rules defeasible local meta rules, added fixed trust order contexts.queried root context atom appearing one answers DMCS-STREAMINGpackage size 10. turned P2P-DR always found answers around 0.25 seconds,regardless tested instance. behavior explained follows. find answersquery atom, algorithm P2P-DR first evaluates local theory. determine truthvalue query, terminates; otherwise algorithm consults neighbors get evidencereasoning. local knowledge base structure, converted P2P-DRs defeasibletheories, allows local decision, system works local theory root context9. http://www.kr.tuwien.ac.at/research/systems/mcsie/tut/581fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERevery test case, thus results almost constant execution time. Even asking neighboursnecessary, P2P-DR general may much faster DMCS, query answering processinherently deterministic low-complexity logic; turn, formalism less expressive.detailed study issue remains future work.6.3.7 UMMARYSumming up, analysis experimental results shows clear winner amongalgorithms (DMCS vs. DMCSOPT) different running modes (streaming vs. non-streaming,different package size) different topologies. distill guideline choosesetup fits specific instances practice, including issues open investigation,briefly stated follows:choose DMCSOPT DMCS non-streaming mode, except cyclic topologies;streaming mode, choose appropriate package size carefully (e.g., binary searchtraining instances;decompose random topologies parts whose topologies effective strategies evaluate,study combine strategies systems.7. Related Worksection, resume discussion related work. Starting multi-context systems,provide details work Roelofsen et al. (2004), Bikakis et al. (2011) considerwork. move related formalisms SAT, CSP ASP.Roelofsen et al. (2004) described evaluation monotone MCS classical theories usingSAT solvers contexts parallel. used (co-inductive) fixpoint strategy check MCSsatisfiability, centralized process iteratively combines results SAT solvers. Aparttruly distributed, extension nonmonotonic MCS non-obvious; furthermore,caching technique used.Serafini, Borgida, Tamilin (2005) Serafini Tamilin (2005) developed distributedtableaux algorithms reasoning distributed ontologies, regarded multi-contextsystems special bridge rules. algorithms serve decide whether system consistent, provided cyclic context dependencies exist (in technical terms, distributed TBoxacyclic); DRAGO system (Serafini & Tamilin, 2005) implements approach OWL ontologies. Compared ours, work tailored specific class multi-context systems resp.knowledge bases, without nonmonotonic negation cyclic dependencies (which challenging); furthermore, targets query answering rather model building, sense dualproblem.related work regards distributed evaluation system P2P-DR Bikakiset al. (2011). developed distributed algorithm query evaluation multi-context systemframework specifically based (propositional) defeasible logic. framework, contexts built using defeasible rules exchange literals via bridge rules, trust ordercontexts supplied. knowledge base context has, terminology, singleaccepted belief set contains literals concluded; global system semantics giventerms (unique) three-valued assignment literals, determined using algorithm: whether literal l provably (not) logical conclusion system, whether remains582fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSopen. Apart tailored particular logic preference mechanisms evaluating interlinked contexts, applying algorithm model building straightforward; particular,produces unique belief sets, dealing nondeterminism multiple equilibria possible.work computing equilibria distributed multi-context systems clearly relatedwork solving constraint satisfaction problems (CSP) SAT solving distributed setting;Yokoo Hirayama (2000) survey algorithms distributed CSP solving, usually developed setting node (agent) holds exactly one variable, constraintsbinary, communication done via messages, every node holds constraintsinvolved. also adopted later works (Gao, Sun, & Zhang, 2007) generalized(Yokoo & Hirayama, 2000). relation topology-based optimization techniques Section 4,biconnected components used Baget Tognetti (2001) decompose CSP problems.decomposition used localize computation single solution components undirected constraint graphs. Along lines, approach based directed dependencies,allows us use query plan MCS evaluation.predominant solution methods CSP backtracking algorithms. Bessiere, Bouyakhf,Mechqrane, Wahbi (2011) took step backtracking dynamic total ordering agents guided nogoods. approach, however, allows cyclic dependencycontexts. Hirayama Yokoo (2005) presented suite algorithms solving distributed SAT (DisSAT), based random assignment improvement flips reduce conflicts.However, algorithms geared towards finding single model, extension streamingmultiple (or all) models straightforward; works distributed CSP SAT,similar.Finally, (distributed) SAT CSP solving concerns monotonic systems (removal clausesresp. constraints preserves satisfiability), MCSs evaluation concerns nonmonotonic systems,even contexts monotonic (e.g., clause sets); makes efficient evaluation difficult,important structural properties search space cannot exploited.Adjiman, Chatalic, Goasdoue, Rousset, Simon (2006) present framework peer-to-peerinference systems, local theories propositional clause sets share atoms special algorithm consequence finding available. pursue dual problem model building,applying needs straightforward; furthermore, dealing non-monotonicsystems, peer-to-peer systems Adjiman et al. monotonic.Moving ASP, Pontelli, Son, Nguyens (2011) ASP-PROLOG shares MCS ideaintegrating several knowledge bases, called modules, possibly different semantics. However, restricted module semantics ASP Prolog (that is, least Herbrand model),ASP-PROLOG pursues query answering instead model building.streaming, answer set streaming algorithm HEX-programs (which generalize ASPexternal information access) given Eiter, Fink, Ianni, Krennwallner, Schuller(2011). Despite similarities Algorithm DMCS-STREAMING, rather different: monolithic programs syntactically decomposed modules answer sets computed modularfashion; fully distributed combines partial models lower components inputupper components straightforwardly; moreover, may use exponential space components.583fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER8. Conclusionconsidered distributed evaluation Multi-context Systems (MCSs) introducedBrewka Eiter (2007) general formalism interlink possibly nonmonotonic heterogeneous knowledge bases. presented suite generic algorithms compute equilibria, i.e., semantics MCS fully distributed manner, using local solvers knowledgebases contexts. contains basic algorithm DMCS, advanced version DMCSOPTuses topology-based optimizations, streaming variant DMCS-STREAMING computingpartial equilibria gradually. believe underlying principles techniques mightexploited related contexts, particular distributed evaluation non-monotonicknowledge base formalisms.algorithms implemented prototype system available open source.8top implementation, conducted comprehensive experiments compare performance algorithms gave insight analysis results. points advantages,disadvantages well time/memory trade algorithms different situationsdepending parameters system topology, local interface theory size, numberequilibria desired user. Based this, user choose setting (algorithm mode)fits need best finding (partial) equilibria MCS. extensive treatment givenDao-Tran (2014).work open issues. Several issues remain investigation. One improvement algorithms. Here, experimental results Ring topology strongly suggestincorporate conflict learning, proved valuable ASP SAT solving, DMCSDMCSOPT; expect cyclic topologies benefit better guided guessing process. Another issue concerns semantics variants MCSs. former, groundedequilibria considered Dao-Tran (2014), akin answer sets logic programsapplicable MCSs satisfy certain algebraic conditions; characterized like answersets using (adapted) loop formula approach (Lee & Lifschitz, 2003). Dealing supportedequilibria (Tasharrofi & Ternovska, 2014), however, open.Regarding MCS variants, managed MCSs (Brewka et al., 2011) generalize bridge rules deriveoperations (commands) management function applied knowledge bases; seemspossible generalize algorithms setting, efficient realization straightforward. Another generalization MCS concerns dynamic data: areas like sensor networks, socialnetworks, smart city applications, data may change even continuously arrive nodes,motivates reactive stream processing MCSs (Goncalves et al., 2014; Brewka et al., 2014).Last least, allowing contexts evolve via interation users changes environment valuable extention. Extending algorithms settings interestingchallenging.Finally, extending work query answering MCSs, user poses querycontext receives results derived (partial) equilibria another natural issue.need building whole equilibria, better performance may achieved.Acknowledgmentsresearch supported Austrian Science Fund (FWF) projects P20841P26471.584fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSC1000C1C2C2000C3C1C3000Figure 19: Introducing guess context(s)thank reviewers pointing corrections constructive suggestionshelped improve presentation work, thank Antonis Bikakis providing usP2P-DR system experimental comparison.Appendix A. ProofsProof Theorem 1prove theorem, first prove following Lemmas 9 10. latter aims simplifyingproof cyclic case, based notion converting cyclic MCSs acyclic ones.Lemma 9 context Ck partial belief state MCS = (C1 , . . . , Cn ),app(brk , S) = app(brk , S|V ) VB V V (k).Proof r app(brk , S), (ci : pi ) B + (r) : pi Sci(cj : pj ) B (r) : pj/ Scj . need show pi Sci |Vci pj/ Scj |Vcj . Indeed:V VB Vcj VBj Scj |Vcj Scj . Therefore, pj/ Scj pj/ Scj |Vcj .Now, assume pi/ Sci |Vci . fact pi Sci , follows pi/ Vci , hencepi/ V (k). contradiction fact pi occurs bridge rule body.Therefore, r app(brk , S|V ).next Lemma 10 based following notions convert cyclic MCSs acyclicones show corresponding equilibria. intuition (illustrated Figure 19Examples 18, 19) introduce additional context Ck take care guessing every cyclebreaker Ck . Then, bridge rules Ck parents modified point Ck .formally realize idea starting function ren renames part bridge rules.Definition 12 Let Ck context MCS , let V interface running DMCS.renaming function ren defined follows:atom a: ren(a, k, V) =agBk Votherwisecontext index c: ren(c, k, V) =ccc {1, . . . , n}otherwisebridge atom (ci : pi ): ren((ci : pi ), k, V) = (ren(ci , k, V) : ren(pi , k, V))585fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERbridge body B = {(c1 : p1 ) . . . (cj : pj )}:ren(B, k, V) = {ren((ci : pi ), k, V) | (ci : pi ) B}bridge rule r = head (r) B(r):ren(r, k, V) = head (r) ren(B(r), k, V)set bridge rules br : ren(br , k, V) = {ren(r, k, V) | r br }context Ci = (Li , kb , br ) : ren(Ci , k, V) = (Li , kb , ren(bri , k, V)).Example 18 Let us slightly modify MCS = (C1 , C2 , C3 ) Example 8 follows:kb 1 = {e e}, br 1 = {a (1 : e), (2 : b)};kb 2 = , br 2 = {b (3 : c)};kb 3 = , br 3 = {c (1 : a)}.Applying function ren contexts C1 C3 results following bridge rules wrt. interface V = {a, b, c, e}:ren(br 1 , 1, V) = {a (1 : eg ), (2 : b)},ren(br 3 , 1, V) = {c (1 : ag )}.two contexts Ci Cj , former called parent latter respect interfaceV, denoted parent(Ci , Cj , V) iff exists bridge rule r br exists (c : p)B(r) p Bj V.set contexts {Cc1 , Cc2 , . . . , Cc` } MCS called cycle w.r.t. interface V iff^parent(Cc` , C1 , V)parent(Cci , Cci+1 , V)1i`1holds. One pick arbitrary context set cycle-breaker. Given MCS ,several ways choose (finite) set contexts cycle-breakers. Algorithm DMCS,Step (d) practically establishes cycle-breakers based order elements In(k)iterated. next definition, interested particular set cycle-breakers.Definition 13 Given MCS = (C1 , . . . , Cn ), let CB rM = {Cc1 , . . . , Ccj } set cyclebreakers based application DMCS starting context Cr . conversionequal acyclic ? based CB rM interface V done follows:ren(Ci , i, V) Ci CB rM00Let Ci = (Li , kb , br ) =CiotherwiseLet Ci00 = (Li , kb , br 00i ) = Ck CBM ren(Ci0 , k, V)1000br {a (i : ag ) | Bi V}000Let Ci000 = (Li , kb , br 000)br=br 00iCi CB rMotherwiseCj CB rM , introduce Cj = (Lj , kb j , br j ) br j = kb j = {ag ag |Bj V}. ? = (C1000 , . . . , Cn000 , Cc1 , . . . , Ccj ).10. order composing function ren different parameters k matter here.586fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSExample 19 (contd) Let MCS Example 18 CB rM = {C1 }. Then, conversion Definition 13 gives ? = (C1000 , C2000 , C3000 , C1 ), where:gkb 1 = {e e}, br 0001 = {a (1 : e ), (2 : b).(1 : ag ).e (1 : eg ).};kb 2 = , br 0002 = {b (3 : c)};gkb 3 = , br 0003 = {c (1 : )};kb 1 = {eg eg .ag ag .}, br 1 = .Lemma 10 Let MCS ? conversion acyclic MCS Definition 13.equilibria ? 1-1 correspondence.Proof (Sketch) Let (R1 ) (R2 ) runs DMCS ? , respectively. Dueselection CB rM construct ? , (R1 ) (R2 ) order visiting contexts,except (R1 ) revisits cycle-breaker Ck CB rM , counterpart (R2 ) visits Ck .corresponding locations:(R1 ) calls guess(V, Ck ) Step (c),(R2 ) calls lsolve({, . . . , }) Step (e) since Ck leaf context.construction local knowledge base Ck gives us exactly guess Ck . Furthermore,guesses passed parent contexts Ck later unified additionalbridge rules (k : ag ) introduced br 000k . Therefore, belief combinations (Step (d)) doneCk executed input runs (R1 ) (R2 ). correspondence equilibriahence follows.Proof (Theorem 1) Thanks Lemma 10, need prove Theorem 1 acycliccase automatically get result cyclic case.() start showing soundness DMCS. Let 0 Ck .DMCS(V, ) V V (k).show partial equilibrium acyclic w.r.t. Ck 0 = S|V .proceed structural induction topology .Base case: Ck leaf In(k) = brk = k/ hist. means (d)executed, hence, (e), lsolve runs exactly (, . . . , ), get result set beliefstates = lsolve((, . . . , )) = {(, . . . , , Tk , , . . . , ) | Tk ACCk (kbk )}. show0 S|V . Towards contradiction, assume partial equilibrium = (S1 , . . . , Sn )w.r.t. Ck 0 = S|V . In(k) = , get IC (k) = {k}, thus partial beliefstate (, . . . , , Tk , , . . . , ) (where Tk ACCk (kbk )) partial equilibrium w.r.t. Ck .Contradiction.Induction step: assume context Ck import neighborhood In(k) = {i1 , . . . , im }i1 = Ci1 .DMCS(V, hist {k}),...im = Cim .DMCS(V, hist {k}).587fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERinduction hypothesis, every 0ij ij , exists partial equilibrium ijw.r.t. Cij ij |V = 0ij .Let = Ck .DMCS(V, hist). need show every 0 S, partial equilibrium w.r.t. Ck 0 = S|V . Indeed, since In(k) 6= , Step (d) executed; let= i1 ./ ./ imresult combining partial belief statesScalling DMCS Ci1 , . . . , Cim . Furthermore,??Step (e), = |V = {lsolve(S) | }. Eventually, 0 S|V .Since every DMCS Ci1 , . . . , Cim returns partial equilibria w.r.t. Cij projected V,every partial equilibrium w.r.t. Cij projected V. acyclic visitedcontexts In(k), thus Lemma 9 get every , app(brk , ) gives usapplicable bridge rules r regardless Tj = , j/ In(k). Hence, , lsolve(T )returns partial belief states, component projected V except kth component.every preserves applicability rules Lemma 9, get every 0 S|V ,exists partial equilibrium w.r.t. Ck 0 = S|V .() give proof completeness DMCS structural induction topologyacyclic . Let = (S1 , . . . , Sn ) partial equilibrium w.r.t. Ck let 0 = S|V .show 0 Ck .DMCS(V, ).Base case: Ck leaf context. Then, executing Ck .DMCS(V, ), Step (d) ignoredStep (e) called input (, . . . , ), lsolve((, . . . , )) gives us belief sets Ck .equilibrium w.r.t. Ck , S; hence, 0 = S|V returned Ck .DMCS(V, ).Induction case: suppose import neighborhood context Ck In(k) = {i1 , . . . , im }. Letrestriction every context Cij In(k) denoted ij , where:S` ` IC (ij )000ij= (S1 , . . . , Sn ) S` =otherwiseInformally speaking, restriction keeps belief sets contexts reachable Cijsets non-reachable contexts . induction hypothesis, ij |V computedCij .DMCS(V, ) ij In(k). show S|V computed Ck .DMCS(V, ).Indeed, considering acyclic , holds ij |V also returned callCij .DMCS(V, {k}), k plays role calls Cij neighbors. meansstep (d), contains = Si1 ./ . . . ./ Sim Sij appears position ij S.Since partial equilibrium w.r.t. Ck , Sk ACCk (kbk {head (r) |r app(brk , S)}). Furthermore, choosing V V (k), Lemma 9 tells us applicabilitybridge rules preserved projection belief sets V. gives us Sk lsolve(T )step (e), hence 0 = S|V returned Ck .DMCS(V, ).Proof Proposition 4(1) context Ck , let number calls local solver denoted c(k). numbercalculated computation Step (d), bounded maximal numbercombined partial belief sets neighbors. Formally speaking:c(k) iIn(k) 2|VBi | 2|In(k)||V| 2n|V| .588fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSHence whole MCS, upper bound calls lsolve run DMCSc = 1kn c(k) n 2n|V|(2) context Ck MCS = (C1 , . . . , Cn ), set E(k) contains dependenciescontexts Ci IC (k). visit (i, j) E(k) exactly twice DFS-traversal :calling Cj .DMCS(V, hist) Ci , retrieving S|V Cj Ci . Furthermore,caching technique Step (a) prevents recomputation already visited nodes, thus preventsrecommunication subtree visited node. claim hence follows.Proof Proposition 5Item (i) trivial see since CycleBreaker applied Algorithm 4. prove item (ii), let us looktwo cases edge (`, t) removed original topology Step (a) Algorithm 3:(`, t) removed CycleBreaker: causes certain nodes graph cannot reachvia `. However, interface Ct provides already attached v(i, j) via V (cp )|Bt .(`, t) removed transitive reduction: change reachabilitynodes; therefore, interface Ct provides already included V (i, j)B 0 .argument gives us property (ii).Proof Proposition 6First, estimate complexity compute v(i, j) loop (a).[[v(i, j) := V (i, j)B 0V (cp )|BcV (cp )|BtcC 0(`,t)Eone hand, refined recursive import V (i, j)0B defined (Definition 9):V (i, j)0B = {V (i)[B` }`B 0 |jB 0 |j contains nodes reachable j.hand, since sets possible beliefs different contexts disjoint,[cC 0V (cp )|Bc[V (cp )|Bt = V (cp )|ScC0 Bc S(`,t)E Bt(`,t)ESince recursive import interface node k defined V (k) = iIC (k) V(i),expression compute v(i, j) end combination set intersection, union, projection.implementation sets using hash set, is, look takes O(1), operatorsimplemented linear time. Therefore, v(i, j) computed linear time total numberbeliefs contexts system.Given GM , block tree graph (GM ) constructed linear time (Vats & Moura,2010). Ear-decomposition (Step (c)) also done linear time (Valdes, Tarjan, & Lawler,589fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER1982). Transitive reduction (Step (d)) computed quadratic time respect numberedges block.OptimizeTree(T (GM ), k, k) iterates blocks. Assume blocks B1 . . . ,Bm , Bi contains ni edges, n =i=1 ni total number edges originalgraph. Let ti time process block Bi . bound total processing timeassessed follows:XXX2t=tini (ni )2 = n2 .i=1i=1i=1Therefore, ignore loop (a), OptimizeTree done quadratic time sizeoriginal input, i.e., size GM .Proof Theorem 7prove this, need Proposition 11 claim partial equilibria returned DMCSDMCSOPT correspondence. first, need following supportive notion.Definition 14 Let Ck context MCS , let k query plan Definition 11.block B k , block interface B, whose root vertex cB ,VB = {p v(i, j) | (i, j) E(B)} BcB .Let Ci context B. self-recursive import interface Ci B[V (i)B = BiV (i, `)B .(i,`)E(k )Proposition 11 Let Ck context MCS, let k query plan Definition 11Ck belongs block B k let V = Bk VB . Then,(i) 0 DMCSOPT(k) called Cc (c, k) E(k ) c = k, existspartial equilibrium Ck .DMCS(V, ) 0 = S|V (c,k)B (c, k) E(k )0 = S|V (k)B c = k;(ii) Ck .DMCS(V, ), exists DMCSOPT(k) called Cc0 = S|V (c,k)B (c, k) E(k ) 0 = S|V (k)B c = k.detailed proof Proposition 11 given next section, give proof Theorem 7.Proof (Theorem 7) (i) Let 0 Ck .DMCSOPT(k) result DMCSOPT. Proposi00000tion 11 (i)Sc = k, exists Ck .DMCS(V, ) = |V (k)B ,choose V = Bk VB . Note V (k) V V collects bridge atoms blocks,might contain blocks reachable k. Theorem 1, exists partial equilibrium00 = S|V . Thus,0 = (S|V )|V (k)B= S|V (k)BV (k)B Vb V (k)B= S|VbV(ii) Let partial equilibrium MS. Theorem 1, exists 00 Ck .DMCS(V, )00 = S|V choose V = Bk VB . above, V (k) V. Proposition 11 (ii)c = k, exists 0 Ck .DMCSOPT(k) 0 = 00 |V (k)B . above,0 = S|Vb .590fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSProof Proposition 11support proof Proposition 11, need following lemmas.Lemma 12 Assume context Ck import neighborhood In(k) = {i1 , . . . , im }, (k, ij ) removed original topology OptimizeBlock(B, cB ),0i1= DMCSOPT(k) Ci1...i1= Ci1 .DMCS(VB , )...0im= DMCSOPT(k) Cimim= Cim .DMCS(VB , )every partial equilibrium 0 0ij , exists ij 0 = S|V (k,ij )B .Let 0 = 0i1 ./ . . . ./ 0im = i1 ./ . . . ./ SSim . Then, 0 0 , exists2V (k, ij )B .0 = |Vinput (1,m) Vinput (`1 , `2 ) = `j=`1Proof prove induction number neighbors In(k).Base case: In(k) = {i}, claim trivially holds.Induction case: In(k) = {i1 , . . . , i` }, U 0 = 0i1 ./ . . . ./ 0i`1 , U = i1 ./ . . . ./ i`1 ,U 0 U 0 , exists U U U 0 = U |Vinput(1,`1) . need show0 U 0 ./ 0i` , exists U ./ i` 0 = |Vinput (1,`) .Assume opposite holds, i.e., exists = U 0 ./ 0 U 0 U 0 0 0i` ,U U, i` U 0 = U |Vinput (1,`1) 0 = S|V (k,i` )B ,U ./ void.means exists context Ct reachable Ck two different ways, one via i`via one i1 , . . . , i`1 Ut 6= , St 6= , Ut 6= St , either(i) Ut0 = St0 = ,(ii) Ut0 = St0 6=Case (i) cannot happen Ct reachable Ck , hence Vinput (1, ` 1) Bt 6=V (k, i` ) Bt 6= .Concerning case (ii), Ut |Vinput (1,`1) = St |V (k,i` ) 6= , hence existsUt \ Ut |Vinput (1,`1)/ St |V (k,i` ) . means Vinput (1, ` 1) Bt 6= V (k, i` ) Bt .However, Definition 9 recursive import interface, V (k, ix )B = V (k)`B|k B` , B|ix contains nodes B reachable ix . follows V (k, i` )V (k, ij ) 1 j ` 1 reaches t, share projection Bt , hence Vinput (1, `1) Bt = V (k, i` ) Bt .reach contradiction, therefore Lemma 12 proved.Lemma 13 join operator ./ following properties, given arbitrary belief states S, , Usize: (i) ./ = (ii) ./ = ./ (iii) ./ (T ./ U ) = (S ./ ) ./ U .properties also hold sets belief states.Proof first two properties trivial prove. prove associativity.Let R = ./ (T ./ U ) W = (S ./ ) ./ U . Consider joins left right.position (1 n), Ri Wi determined locally comparing Si , Ti Ui .591fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERSi =Ti =NUi =NNNNNNNNNNNSi = TiNNNNNNNNNTi = UiNNNNNNNNNUi = SiNNNNNNNNNNRiUiTiTivoidSiSivoidSivoidSivoidvoidvoidWiUiTiTivoidSiSivoidSivoidSivoidvoidvoidTable 1: Possible cases joining positionreach inconsistency, process terminates void returned; otherwise, conclude valueRi , Wi continue next position. final join returned position n processedwithout inconsistency.possible combination Si , Ti , Wi shown Table 1. One see alwaysoutcome Ri Wi . Therefore, end either R = Wvoid . concludes join operator ./ commutative.Lemma 14 Let Ci Cj two contexts block executingOptimizeTree directed path Ci Cj . Suppose = DMCSOPT(k) Cij = DMCSOPT(k) Cj . = ./ j .Proof use cache DMCSOPT change result disregarded, i.e.,assume without loss generality cache(k) = DMCSOPT. Indeed, cache(k) filledresult computation empty (i.e., Ck accessed first time),never changed DMCSOPT returns cache(k), i.e., value computationempty cache(k).assumption, Lemma 14 proven taking path Ci = Cp1 , . . . , Cph =Cj connects Ci Cj , arguing index ` {1, . . . , h}, holds p` = p` ./j (?). Indeed, show induction path.Base case: ` = h, statement (?) holds ph ./ j = j ./ j = j identity(Lemma (13), (i)).Induction case: consider ` < h, suppose already established induction hypothesisp`+1 = p`+1 ./ j .592fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSdefinition p` DMCSOPT, holds p` = lsolve(T )11 is,statements (b) (c), form = p`+1 ./ 0 ; holds edge (p` , p`+1 )E, ./ commutative associative (Lemma (13), (ii) (iii)). inductionhypothesis, get= p`+1 ./ 0 = (S p`+1 ./ j ) ./ 0 = j ./ (S p`+1 ./ 0 ),is, form j ./ 00 .Next, lsolve(T ) change value component interpretationdefined j ; is, lsolve(T ) ./ j = lsolve(T ). means p` = lsolve(T ) = lsolve(T ) ./j = p` ./ j , proves statement (?) holds `.Eventually, get ` = 1 = p1 = p1 ./ j = ./ j .Based Lemma 14, following result.Lemma 15 Assume import neighborhood context Ck In(k) = {i1 , . . . , im },ij = DMCSOPT(k) Cij , 1 j m. Furthermore, suppose edge (k, ij ) removedoptimization process (1 j m), Ci` neighbor Ck exists pathk ij i` optimized topology. i` = i` ./ ij ; words, inputDMCSOPT Ck affected removal (k, ij ).Proof Since Cij Ci` direct children Ck , follows belong block.Therefore, Lemma 14 i` = i` ./ ij .Proof (Proposition 11) proceed structural induction block tree MCS . First,consider case topology single block B. case, interface passedDMCS V = VB .Base case: Ck leaf. compare call DMCSOPT(k) Ck Ck .DMCS(V, ),V = V (k)B = Bk . Algorithm 1 returns local belief sets Ck projected V Algorithm 5 returns plain local belief sets, claim follows V = V (k)B = Bk .Induction case: Assume import neighborhood context Ck In(k) = {i1 , . . . , im },0i1= DMCSOPT(k) Ci1...i1= Ci1 .DMCS(VB , )...0im= DMCSOPT(k) Cimim= Cim .DMCS(VB , )every partial equilibrium 0 0ij , exists ij 0 = S|V (k,ij )B .two cases. First, edge (k, ij ) removed optimization procedure. Then,Lemma 12, correspondence input DMCSOPT DMCS Ck .hand, assume edge (k, ij ) removed optimization process.removal either transitive reduction ear decomposition. former case, Lemma 15shows input Ck affected removal edge. latter case, removalone three possibilities illustrated Figure 20, assuming context C1 gets called:(i) (6, 1), last edge simple cycle P0 = {1, 2, 3, 4, 5, 6}11. abuse notation, write lsolve(T )lsolve(T )593fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER112111092635487Figure 20: Possible cycle breakings(ii) (9, 6), last edge path P1 = {5, 7, 8, 9, 6}(iii) (12, 2), last edge path P2 = {3, 10, 11, 12}Cases (i) (iii) differ case (ii) sense cycle recognized DMCScase (ii), cycle detected along corresponding path.Now, consider (k, ij ) removed situations similar cases (i) (iii), DMCSOPTissue guess Step (c) Algorithm 5 v(k, ij ), includes V (cB )|Bij = VB Bij .hand, DMCS recognize cycle Cij issue guess VB Bij Step (c)Algorithm 1. Therefore, guess fed equally Ck .(k, ij ) removed situations similar case (ii), guesses Ck interfaceCij eventually filtered combined local belief states computed Cij ,starting node path containing (k, ij ) last edge (in ear decomposition).Figure 20, node 5.cases, whenever input 0 lsolve DMCSOPT(k) calledCc , input lsolve Ck .DMCS(VB , ). Therefore, claim output holds.Proposition 11 holds single leaf block, one see upper blocksneed import interfacebeliefs cut vertices (also root contexts lower blocks).setting V = Bk VB , results DMCSOPT DMCS projected interfacecut vertices identical. Therefore, upper blocks receive input regardinginterfaces cut vertices running algorithms. therefore final results projectedV (k)B end same.Proof Proposition 8Note components Handler Output simply take care communication partDMCS-STREAMING. Output makes sure models sent back invokers correspondence request Handler got. routines Joiner Solver maincomponents play role Step (b) (d) Algorithm 5, respectively.594fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMST1,1T1,1T1,1T1,1./T2,1./././T2,1./././T2,1./././T2,1././T1,1T1,1T1,1./T2,1./././T2,1./././T2,p2././Tm,1./Tm,pm./Tm,1./Tm,pmTm1,pm1Tm1,pm1./Tm,1./Tm,pmTm1,1./Tm,1./Tm,pm./Tm,1./T2,p2././T1,p1Tm1,2Tm1,2./T1,1Tm1,1Tm1,1Tm1,pm1./T2,1././Tm1,p1T1,p1./T2,p2././Tm1,pm1./Tm,pmT1,1T1,1T1,1T1,1T1,p1./T2,1././Tm1,1./F (m, m)./T2,1./././F (m, m)./T2,p2./././F (m, m)./T2,p2./././F (m, m)./T2,p2././Tm1,pm1Tm1,1Tm1,pm1Tm1,pm1./F (m, m)T1,1T1,1T1,p1./T2,1./././T2,p2./././T2,p2././F (m 1, m)F (m 1, m)F (m 1, m)=[T1,1 ./ F (2, m)][T1,p1 ./ F (2, m)]=F (1, m).==Table 2: Accumulation Joinerprove correctness DMCS-STREAMING, need show input lsolvecomplete sense Step (e) Algorithm 8 exhaustively executed, full joinpartial equilibria neighboring contexts delivered.595fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERFormally, assume current contexts import neighborhood {1, 2, . . . , m}. Assumeneighbor Ci 1 m, full partial equilibria Ti returned packages sizek denoted Ti,1 , . . . , Ti,pi , is, Ti = Ti,1 . . . Ti,pi . correctness algorithm,assume Ti,1 , . . . , Ti,pi fixed partition Ti . possible when, example, lsolvealways returns answers fixed order. need show accumulation joinAlgorithm 8 actually T1 ./ . . . ./ Tm .Indeed, possible join T1,i1 ./ T2,i2 ./ . . . ./ Tm,im considered Joiner, performslexicographical traversal suitable combinations. Formally speaking, let F (p, q), q <q, denote join result neighbors p q, is, F (p, q) = Tp ./ Tp+1 ./.1 . . ./ Tq .According lexicographical order, accumulation Joiner pj=1[T1,j ./F (2, m)] = F (1, m) demonstrated Table 2.shows input lsolve complete. Hence, DMCS-STREAMING correct.Appendix B. Detailed Run OptimizeTreeExample 20 illustrate call OptimizeTree(T = (B C, E), cp , cr ) block set B ={B1 , B2 , B3 }, B1 = {1, 2, 3, 4}, B2 = {4, 5}, B3 = {3, 6}, C = {1, 3, 4}, E = {(B1 , 1), (B2 , 4),(B3 , 3)}, cp = cr = 1.local knowledge bases presented Example 10, have:B1 = {car 1 , train 1 , nuts 1 }B2 = {car 2 , train 2 }B3 = {car 3 , train 3 , salad 3 , peanuts 3 , coke 3 , juice 3 , urgent 3 }B4 = {car 4 , train 4 }B5 = {soon5 , sooner 5 }B6 = {fit 6 , sick 6 }Since cp = cr , start B 0 = {B1 }. F = v = .call OptimizeBlock(B1 , 1). Since B1 acyclic, transitive reduction applied.get B1 = ({1, 2, 3, 4}, {(1, 2), (2, 3), (3, 4)}). subroutine returns E = {(1, 3), (2, 4)}.child cut vertices B1 C 0 = {3, 4}; update F {(1, 3), (2, 4)}.Next, update label edges (i, j) B1 . this, let us enumerate recursiveimport interfaces, starting import interface, every node 1 6:V(1) = {train 2 , train 3 , peanuts 3 }V(2) = {car 3 , coke 3 , train 3 , car 4 , train 4 }V (1)V (2)V (3)V (4)V (5)V (6)V(3) = {train 4 , sick 6 }V(5) = {train 4 }V(4) = {sooner 5 }V(6) ={train 2 , train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 , sooner 5 , sick 6 }{train 3 , car 3 , coke 3 , train 4 , car 4 , sooner 5 , sick 6 }{train 4 , sooner 5 , sick 6 }{train 4 , sooner 5 }{train 4 , sooner 5 }Now, let us compute V (1, 2)B1 = V (1) `B1 |2 B` . B1 |2 = {3, 4}, thus======V (1, 2)B1 = V (1) (B3 B4 ) = {train 2 , train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }Similarly, B1 |3 = B1 |4 = {4}, have:V (2, 3) = V (2) B3 = {car 4 , train 4 }V (3, 4) = V (3) B4 = {train 4 }596fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSremoved edges updated labels stored respectively F v block B1summarized as:F = {(1, 3), (2, 4)}v(1, 2) =V (1, 2)V (1)|B3V (1)|B4v(2, 3) = V (2, 3) V (1)|B3 V (1)|B4v(3, 4) = V (3, 4) V (1)|B3 V (1)|B4train 2 , train 3 , peanuts 3 ,=car 3 , coke 3 , car 4 , train 4= {train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }= {train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }Next, call OptimizeTree(T \ B1 , 3, 1) OptimizeTree(T \ B1 , 4, 1), eventually processblocks B2 B3 manner above. two calls respectively return:F 0 = {(5, 4)}v 0 (4, 5) = {sooner 5 }F 00 =v 00 (3, 6) = {train 4 , sick 6 }Combining results together, OptimizeTree(T, 1, 1) returns set removed edgesF = {(1, 2), (3, 4), (5, 4)}updated labels v remaining edges blocksv(1, 2)v(2, 3)v(3, 4)v(4, 5)v(3, 6)====={train 2 , train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }{train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }{train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }{sooner 5 }{train 4 , sick 6 }ReferencesAdjiman, P., Chatalic, P., Goasdoue, F., Rousset, M.-C., & Simon, L. (2006). Distributed reasoningpeer-to-peer setting: Application semantic web. J. Artif. Intell. Res., 25, 269314.Aho, A. V., Garey, M. R., & Ullman, J. D. (1972). Transitive Reduction Directed Graph.SIAM J. Comput., 1(2), 131137.Analyti, A., Antoniou, G., & Damasio, C. V. (2011). MWeb: principled framework modularweb rule bases semantics. ACM Trans. Comput. Log., 12(2), 17.Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.). (2003).Description Logic Handbook. Cambridge University Press.Baget, J.-F., & Tognetti, Y. S. (2001). Backtracking biconnected components constraintgraph. Nebel, B. (Ed.), Proceedings Seventeenth International Joint ConferenceArtificial Intelligence, IJCAI 2001, Seattle, Washington, USA, August 4-10, 2001, pp. 291296. Morgan Kaufmann.Bairakdar, S. E.-D., Dao-Tran, M., Eiter, T., Fink, M., & Krennwallner, T. (2010a). Decompositiondistributed nonmonotonic multi-context systems. Janhunen, T., & Niemela, I. (Eds.),Logics Artificial Intelligence - 12th European Conference, JELIA 2010, Helsinki, Finland,September 13-15, 2010. Proceedings, Vol. 6341 Lecture Notes Computer Science, pp.2437. Springer.597fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERBairakdar, S. E.-D., Dao-Tran, M., Eiter, T., Fink, M., & Krennwallner, T. (2010b). DMCSsolver distributed nonmonotonic multi-context systems. Janhunen, T., & Niemela, I.(Eds.), Logics Artificial Intelligence - 12th European Conference, JELIA 2010, Helsinki,Finland, September 13-15, 2010. Proceedings, Vol. 6341 Lecture Notes Computer Science, pp. 352355. Springer.Bessiere, C., Bouyakhf, E., Mechqrane, Y., & Wahbi, M. (2011). Agile asynchronous backtrackingdistributed constraint satisfaction problems. IEEE 23rd International ConferenceTools Artificial Intelligence, ICTAI 2011, Boca Raton, FL, USA, November 7-9, 2011,pp. 777784.Bikakis, A., & Antoniou, G. (2010). Defeasible contextual reasoning arguments ambientintelligence. IEEE Transactions Knowledge Data Engineering, 22(11), 14921506.Bikakis, A., Antoniou, G., & Hassapis, P. (2011). Strategies contextual reasoning conflictsambient intelligence. Knowl. Inf. Syst., 27(1), 4584.Bogl, M., Eiter, T., Fink, M., & Schuller, P. (2010). MCS-IE system explaining inconsistencymulti-context systems. Logics Artificial Intelligence - 12th European Conference,JELIA 2010, Helsinki, Finland, September 13-15, 2010. Proceedings, Vol. 6341 LectureNotes Computer Science, pp. 356359. Springer.Bondy, A., & Murty, U. S. R. (2008). Graph Theory, Vol. 244 Graduate Texts Mathematics.Springer.Brewka, G., Eiter, T., Fink, M., & Weinzierl, A. (2011). Managed multi-context systems. Walsh,T. (Ed.), Proceedings 22nd International Joint Conference Artificial Intelligence(IJCAI-11), pp. 786791. AAAI Press/IJCAI.Brewka, G., Ellmauthaler, S., & Puhrer, J. (2014). Multi-context systems reactive reasoningdynamic environments. Ellmauthaler, S., & Puhrer, J. (Eds.), Proceedings International Workshop Reactive Concepts Knowledge Representation (ReactKnow) 2014, pp.2330. Tech.Rep. 1, Computer Science Institute, Univ. Leipzig, ISSN 1430-3701.Brewka, G., & Eiter, T. (2007). Equilibria heterogeneous nonmonotonic multi-context systems.Proceedings Twenty-Second AAAI Conference Artificial Intelligence, July 22-26,2007, Vancouver, British Columbia, Canada, pp. 385390. AAAI Press.Brewka, G., Eiter, T., & Fink, M. (2011). Nonmonotonic Multi-Context Systems: Flexible Approach Integrating Heterogeneous Knowledge Sources. Balduccini, M., & Son, T. C.(Eds.), Logic Programming, Knowledge Representation, Nonmonotonic Reasoning - Essays Dedicated Michael Gelfond Occasion 65th Birthday, Vol. 6565 Lecture Notes Computer Science, pp. 233258. Springer.Brewka, G., Roelofsen, F., & Serafini, L. (2007). Contextual default reasoning. Veloso, M. M.(Ed.), IJCAI 2007, Proceedings 20th International Joint Conference Artificial Intelligence, Hyderabad, India, January 6-12, 2007, pp. 268273.Buccafurri, F., & Caminiti, G. (2008). Logic programming social features. Theory PracticeLogic Programming, 8(5-6), 643690.Dao-Tran, M. (2014). Distributed Nonmonotonic Multi-Context Systems: Algorithms EfficientEvaluation. Ph.D. thesis, Faculty Informatics, Vienna University Technology, Austria.598fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMSDao-Tran, M., Eiter, T., Fink, M., & Krennwallner, T. (2010). Distributed nonmonotonic multicontext systems. Lin, F., Sattler, U., & Truszczynski, M. (Eds.), Principles Knowledge Representation Reasoning: Proceedings Twelfth International Conference,KR 2010, Toronto, Ontario, Canada, May 9-13, 2010. AAAI Press.Dao-Tran, M., Eiter, T., Fink, M., & Krennwallner, T. (2011). Model streaming distributed multicontext systems. Mileo, A., & Fink, M. (Eds.), 2nd International Workshop Logicbased Interpretation Context: Modeling Applications, Vol. 738 CEUR WorkshopProceedings, pp. 1122.Eiter, T., Fink, M., Ianni, G., Krennwallner, T., & Schuller, P. (2011). Pushing efficient evaluationhex programs modular decomposition. Delgrande, J. P., & Faber, W. (Eds.), 11th International Conference Logic Programming Nonmonotonic Reasoning (LPNMR 2011),Vancouver, BC, Canada, May 16-19, 2011, Vol. 6645 Lecture Notes Computer Science,pp. 93106. Springer.Eiter, T., Ianni, G., Schindlauer, R., & Tompits, H. (2005). uniform integration higher-orderreasoning external evaluations answer-set programming. IJCAI, pp. 9096.Faltings, B., & Yokoo, M. (2005). Introduction: Special issue distributed constraint satisfaction.Artif. Intell., 161(1-2), 15.Fink, M., Ghionna, L., & Weinzierl, A. (2011). Relational information exchange aggregationmulti-context systems. Delgrande, J. P., & Faber, W. (Eds.), 11th International Conference Logic Programming Nonmonotonic Reasoning (LPNMR 2011), Vancouver, BC,Canada, 16-19 May, 2011, Vol. 6645 Lecture Notes Computer Science, pp. 120133.Springer.Gao, J., Sun, J., & Zhang, Y. (2007). improved concurrent search algorithm distributed CSPs.Australian Conference Artificial Intelligence, pp. 181190.Gelfond, M., & Lifschitz, V. (1991). Classical negation logic programs disjunctive databases.New Generation Comput., 9(3/4), 365386.Ghidini, C., & Giunchiglia, F. (2001).Local models semantics, contextual reasoning=locality+compatibility. Artif. Intell., 127(2), 221259.Giunchiglia, F. (1992). Contextual Reasoning. Epistemologia, Special Issue Linguaggi e leMacchine, 345, 345364.Giunchiglia, F., & Serafini, L. (1994). Multilanguage hierarchical logics or: withoutmodal logics. Artif. Intell., 65(1), 2970.Goncalves, R., Knorr, M., & Leite, J. (2014). Evolving multi-context systems. Schaub, T.,Friedrich, G., & OSullivan, B. (Eds.), Proceedings 21st Eureopean ConferenceArtificial Intelligence, ECAI2014, Prague, Czech Republic, August 18-23, 2014. IOS Press.Hirayama, K., & Yokoo, M. (2005). distributed breakout algorithms. Artif. Intell., 161(12),89115.Homola, M. (2010). Semantic Investigations Distributed Ontologies. Ph.D. thesis, ComeniusUniversity, Bratislava, Slovakia.599fiDAO -T RAN , E ITER , F INK , & K RENNWALLNERLee, J., & Lifschitz, V. (2003). Loop formulas disjunctive logic programs. Palamidessi, C.(Ed.), Logic Programming, 19th International Conference, ICLP 2003, Mumbai, India, December 9-13, 2003, Proceedings, Lecture Notes Computer Science, pp. 451465. Springer.McCarthy, J. (1993). Notes formalizing context. Bajcsy, R. (Ed.), Proceedings 13thInternational Joint Conference Artificial Intelligence. Chambery, France, August 28 September 3, 1993, pp. 555562. Morgan Kaufmann.Pontelli, E., Son, T., & Nguyen, N.-H. (2011). Combining answer set programming prolog:ASP-PROLOG system. Balduccini, M., & Son, T. (Eds.), Logic Programming, KnowledgeRepresentation, Nonmonotonic Reasoning, Vol. 6565, pp. 452472. Springer Berlin Heidelberg.Reiter, R. (1980). logic default reasoning. Artificial Intelligence, 13, 81132.Roelofsen, F., Serafini, L., & Cimatti, A. (2004). Many hands make light work: Localized satisfiability multi-context systems. de Mantaras, R. L., & Saitta, L. (Eds.), Proceedings16th Eureopean Conference Artificial Intelligence, ECAI2004, including PrestigiousApplicants Intelligent Systems, PAIS 2004, Valencia, Spain, August 22-27, 2004, pp. 5862.IOS Press.Serafini, L., Borgida, A., & Tamilin, A. (2005). Aspects distributed modular ontology reasoning. Nineteenth International Joint Conference Artificial Intelligence (IJCAI 2005),pp. 570575. AAAI Press.Serafini, L., & Tamilin, A. (2005). Drago: Distributed reasoning architecture semantic web.Gomez-Perez, A., & Euzenat, J. (Eds.), Semantic Web: Research Applications,Second European Semantic Web Conference, ESWC 2005, Heraklion, Crete, Greece, May 29- June 1, 2005, Proceedings, Lecture Notes Computer Science, pp. 361376. Springer.Tarjan, R. E. (1972). Depth-First Search Linear Graph Algorithms. SIAM J. Comput., 1(2),146160.Tasharrofi, S., & Ternovska, E. (2014). Generalized multi-context systems.. Baral, C., Giacomo,G. D., & Eiter, T. (Eds.), Principles Knowledge Representation Reasoning: Proceedings Fourteenth International Conference, KR 2014, Vienna, Austria, July 20-24, 2014.AAAI Press.Valdes, J., Tarjan, R. E., & Lawler, E. L. (1982). recognition series parallel digraphs. SIAMJ. Comput., 11(2), 298313.Vats, D., & Moura, J. M. F. (2010). Graphical models block-tree graphs. CoRR, abs/1007.0563.Velikova, M., Novak, P., Huijbrechts, B., Laarhuis, J., Hoeksma, J., & Michels, S. (2014).Integrated Reconfigurable System Maritime Situational Awareness. ECAI 2014 - 21stEuropean Conference Artificial Intelligence, 18-22 August 2014, Prague, Czech Republic- Including Prestigious Applications Intelligent Systems (PAIS 2014), pp. 11971202.Yokoo, M., & Hirayama, K. (2000). Algorithms distributed constraint satisfaction: review.Autonomous Agents Multi-Agent Systems, 3(2), 185207.600fiJournal Artificial Intelligence Research 52 (2015) 1-95Submitted 07/14; published 01/15Coherent Predictive Inference ExchangeabilityImprecise ProbabilitiesGert de CoomanJasper De Bockgert.decooman@UGent.bejasper.debock@UGent.beGhent University, SYSTeMS Research GroupTechnologieparkZwijnaarde 9149052 Zwijnaarde, BelgiumMrcio Alves Dinizmarcio.alves.diniz@gmail.comFederal University Carlos, Department StatisticsRod. Washington Luis, km 235Carlos, BrazilAbstractCoherent reasoning uncertainty represented general mannercoherent sets desirable gambles. context allow indecision, leadsapproach mathematically equivalent working coherent conditionalprobabilities. allow indecision, leads general foundation coherent(imprecise-)probabilistic inference. framework, given finite category set,coherent predictive inference exchangeability represented using Bernsteincoherent cones multivariate polynomials simplex generated category set.powerful generalisation de Finettis Representation Theorem allowingimprecision indecision.define inference system map associates Bernstein coherent conepolynomials every finite category set. Many inference principles encounteredliterature interpreted, represented mathematically, restrictionsmaps. discuss, particular examples, two important inference principles: representationinsensitivitya strengthened version Walleys representation invarianceand specificity.show infinity inference systems satisfy two principles,amongst discuss particular skeptically cautious inference system, inferencesystems corresponding (a modified version of) Walley Bernards Imprecise DirichletMultinomial Models (IDMM), skeptical IDMM inference systems, Haldaneinference system. also prove latter produces posterior inferenceswould obtained using Haldanes improper prior, implying infinityproper priors produce coherent posterior inferences Haldanes improper one.Finally, impose additional inference principle allows us characterise uniquelyimmediate predictions IDMM inference systems.1. Introductionpaper deals predictive inference categorical variables. therefore concerned(possibly infinite) sequence variables Xn assume values finite setcategories A. observed number n them, found that, say X1 = x1 ,X2 = x2 , . . . , Xn = xn , consider subjects belief model next n variablesXn+1 , . . . Xn+n . probabilistic traditionand want build tradition2015 AI Access Foundation. rights reserved.fiDe Cooman, De Bock, & Dinizcontext paperthis belief modelled conditional predictive probabilitymass function pn (|x1 , . . . , xn ) set possible values. probability massfunctions used prediction estimation, statistical inferences, decisionmaking involving uncertain values variables. sense, predictive inference liesheart statistics, generally, learning uncertainty. reason,also crucial importance dealing uncertainty Artificial Intelligence,instance, intelligent systems learn multinomial probabilities, Markovtransition probabilities, rates occurrence phenomena, local probabilities Bayesiancredal networks on. refer synthesis Geisser (1993) collectionessays Zabell (2005) good introductions predictive inference underlyingissues present paper also concerned with.connects predictive probability mass functions various values n, n(x1 , . . . , xn ) requirements time consistency coherence. former requiresn1 n2 , pn1 (|x1 , . . . , xn ) obtained pn2 (|x1 , . . . , xn )usual marginalisation procedure; latter essentially demands conditionalprobability mass functions connected time-consistent unconditional probabilitymass functions Bayess Rule.common assumption variables Xn exchangeable, meaningroughly subject believes order observed, presentthemselves, influence decisions inferences make regardingvariables. assumption, analysis consequences, goes back de Finetti(1937) (see also Cifarelli & Regazzini, 1996). famous Representation Theorem states,essence, time-consistent coherent conditional unconditional predictiveprobability mass functions associated countably infinite exchangeable sequencevariables completely characterised by1 completely characterisea uniqueprobability measure Borel sets simplex probability mass functions A,called representation.2leads us central problem predictive inference: since infinityprobability measures simplex, one subject choose particularcontext, given choice motivated justified? subjectivists deFinettis persuasion might answer question needs answer: subjects personalpredictive probabilities entirely his, time consistency coherencerequirements heed. Earlier scholars, like Laplace Bayes, wouldalso call subjectivists, invoked Principle Indifference justify using specific classpredictive mass functions. Proponents logicist approach predictive inference wouldtry enunciating general inference principles order narrow down, hopefully eliminateentirely, possible choices representing probability measures simplex.logicians W. E. Johnson (1924) and, much systematic fashion, Rudolf Carnap (1952)1. . . . unless observed sequence probability zero.2. Actually, order clarify connection shall later on, essence de Finettisargument representation coherent prevision set multinomial polynomialsorequivalently, continuous real functionson simplex (De Cooman, Quaeghebeur, & Miranda,2009b). (finitely additive) coherent prevision, extended uniquely far setlower semicontinuous functions, determine unique (countably additive) probabilitymeasure Borel sets simplex, F. Riesz Representation Theorem (De Cooman &Miranda, 2008a; Troffaes & De Cooman, 2014).2fiCoherent Predictive Inference Exchangeabilitytried develop axiom system predictive inference based reasonable inferenceprinciples. Carnaps first group axioms related called coherence,suggested, weak single particular predictivemodel. second group consisted invariance axioms, including exchangeability. alsoincluded axiom instantial relevance, translating intuitive principle predictiveinferences actually learn experience. last axiom, predictive irrelevance,also proposed earlier Johnson called sufficientness postulate Good (1965).Armed axioms, Carnap able derive continuum probabilistic inferencerules, closely related Dirichlet multinomial model Imprecise DirichletMultinomial Model (IDMM) proposed Walley (1996) Walley Bernard (1999),discuss Appendices C D, respectively.point view holds middle ground subjectivist logicist positions:possible subject make assessments certain predictive probabilities,combine certain inference principles finds reasonable, suitpurpose problem hand. Indeed, inference systems introduce discussSection 6, notion conservative coherent inferenceor natural extensionweassociate them, provide elegant framework tools making conservative coherentpredictive inferences combine (local) subjective probability assessments (general)inference principles. work Section 15 characterising immediate predictionsIDMM constitutes exercise inor example forprecisely that.idea conservative probabilistic inference brings us believemain contribution paper. central idea de Finettis (1975) approachprobabilitybut also course implicit Markov Chebyshev inequalitiesthatsubject makes probability assessments, consider bounds so-calledprecise probability models. Calculating conservative tightest bounds indeedde Finettis (1975) Fundamental Theorem Prevision (see also Lad, 1996) about.theory imprecise probabilities, brought synthesis Williams (1976) Walley(1991, 2000), going back Boole (1952) Keynes (1921), crucial contributionsquite number statisticians philosophers (Smith, 1961; Levi, 1980; Seidenfeld,Schervish, & Kadane, 1999), looks conservative probabilistic inference preciselyway: calculate efficiently possible consequencesin senseconservative tightest boundsof making certain probability assessments. may localassessments, inequalities imposed probabilities previsions certain eventsvariables, structural assessments, independence, exchangeability.One advantage imprecise probability models allow imprecision,words, use partial probability assessments using bounding inequalities ratherequalities. Another, related, advantage allow indecision modelledexplicitly: loosely stated, imposed bounds probabilities allow oneprobability model solution, may well two actions, firsthigher expected utility one compatible probability model, smaller anothercompatible probability model, meaning neither action robustly preferred other.current stated model beliefs, subject undecidedactions. Section 2, give concise overview relevant ideas, models techniquesfield imprecise probabilities. much extensive detailed recent overviewarea research published Augustin, Coolen, De Cooman, Troffaes (2014).3fiDe Cooman, De Bock, & Dinizpresent paper, then, described application ideas imprecise probabilities predictive inference. aim studyand develop general frameworkdealing withconservative coherent predictive inference using imprecise probability models.Using models also allow us represent subjects indecision, believenatural state knowing, learned little, problem hand.seems important theories learning uncertainty general, predictiveinference particular, least allow us (i) start conservative, impreciseindecisive models little learned, (ii) become precise decisiveobservations come in. shall see abstract notion inference systemintroduce on, allows forbut necessarily forcesuch behaviour,shall give number examples concrete inference systems display it.work builds on, manages reach much than, earlier paperone authors (De Cooman, Miranda, & Quaeghebeur, 2009a). One reasonso, earlier work deals immediate prediction models, shallsee on, predictive inference using imprecise probabilities completely determinedimmediate prediction, contrary expect using precise probabilities.main reason position use powerful mathematicallanguage represent imprecise-probabilistic inferences: Walleys (2000) coherent setsdesirable gambles. Earlier imprecise probability models (Boole, 1952, 1961; Koopman, 1940)centred lower upper probability bounds eventsor propositions. Later (Walley,1991, Section 2.7), became apparent language events lower upperprobabilities lacking power expression: much expressive theory uses randomvariables lower previsions expectations. successful theory coherent lowerprevisions quite well developed (Walley, 1991; Augustin et al., 2014; Troffaes &De Cooman, 2014). faces number problems, mathematical wellconceptual complexity, especially dealing conditioning independence,fact that, case many approaches probability, shall seeSection 2.5, issues conditioning sets (lower) probability zero.attractive solution problems offered Walley (2000), formcoherent sets desirable gambles, inspired earlier ideas (Smith, 1961; Williams, 1975b;Seidenfeld, Schervish, & Kadane, 1995). Here, primitive notions probabilitiesevents, expectations random variables. focus rather whether gamble,risky transaction, desirable subjectstrictly preferred zero transaction,status quo. basic belief model probability measure lower prevision,set desirable gambles. course, stating gamble desirable also leadsparticular lower prevision assessment: provides lower bound zero previsiongamble. explain prefer use sets desirable gambles basic uncertaintymodels Section 2.summary, then, aim paper use sets desirable gambles extendexisting probabilistic theory predictive inference. Let us explain detailintend go this. basic building blocks introduced Sections 27.already indicated above, give overview relevant notions results concerningimprecise probability model choicecoherent sets desirable gamblesin Section 2.particular, explain use conservative inference well conditioning;4fiCoherent Predictive Inference Exchangeabilityderive commonly used models, lower previsions lower probabilities,them; relate precise probability models.Section 3, explain describe subjects beliefs sequencevariables terms predictive sets desirable gambles, derived notion predictivelower previsions. imprecise probability models generalise above-mentioned predictiveprobability mass functions pn (|x1 , . . . , xn ), constitute basic tools shallworking with. also explain proper formulations above-mentionedtime consistency coherence requirements general context.Section 4, discuss number inference principles believe could reasonablyimposed predictive inferences, show represent mathematicallyterms predictive sets desirable gambles lower previsions. Pooling invarianceorWalley (1996) called Representation Invariance Principle (RIP)and renaminginvariance seem reasonable requirements type predictive inference, categorypermutation invariance seems natural thing require starting statecomplete ignorance. Taken together, constitute call representation insensitivity.means predictive inferences remain essentially unchanged transformset categories, words essentially insensitive choicerepresentationthe category set. Another inference principle look imposes so-calledspecificity property: predictive inference specific, certain type questioninvolving restricted number categories, general model replacedspecific model deals categories interest, producerelevant inferences (Bernard, 1997).next important step taken Section 5, recall literature (DeCooman et al., 2009b; De Cooman & Quaeghebeur, 2012) deal exchangeabilitypredictive inference models imprecise. recall de Finettis RepresentationTheorem significantly generalised. case, time-consistent coherentpredictive sets desirable gambles completely characterised set (multivariate)polynomials simplex probability mass functions category set.3set polynomials must satisfy number properties, taken together definenotion Bernstein coherence. Without becoming technical point, conclusionsection that, general context, precise-probabilistic notionrepresenting probability measure simplex probability mass functions replacedBernstein coherent set polynomials simplex. set polynomials servescompletely purpose representing probability measure: completely determines,conveniently densely summarises, predictive inferences. reasonrest developments paper expressed terms Bernstein coherentsets polynomials.introduce coherent inference systems Section 6 maps associatefinite set categories Bernstein coherent set polynomials simplex probabilitymass functions set. coherent inference system way fixing completelycoherent predictive inferences possible category sets. reasons introducingcoherent inference systems twofold. First all, inference principles Section 4 imposeconnections predictive inferences different category sets, represent3. contradistinction de Finettis version, version problems conditioning observedsequences (lower) probability zero.5fiDe Cooman, De Bock, & Dinizinference principles mathematically restrictions coherent inference systems,main topic Section 7. Secondly, allows us extend method natural extensionorconservative inferenceintroduced Section 2.2, also take account principlespredictive inference, generally, predictive inference multiple category sets once.leads method combining (local) predictive probability assessments (global)inference principles produce conservative predictive inferences compatiblethem.first illustration power methodology, look immediate predictionSection 8: implications representation insensitivity specificitypredictive inference single next observation? show approach allows usstreamline, simplify significantly extend previous attempts direction DeCooman et al. (2009a).material Sections 914 shows, producing explicit examples,quite different typeseven uncountable infinitiesof coherent inference systemsrepresentation insensitive and/or specific. discuss vacuous nearly vacuousinference systems Sections 9 10, skeptically cautious inference system Section 11,family IDMM inference systems Section 12, family skeptical IDMM inferencesystems Section 13, Haldane inference system Section 14. inferencesystems, apart IDMM, appear first time. Also, believefirst publish detailed explicitas well still elegantproof IDMMinference systems indeed representation insensitive specific. alreadymentioned here, however, IDMM inference systems based modified,arguably better behaved, version models originally introduced Walley Bernard(see Walley, 1996; Walley & Bernard, 1999; Bernard, 2005); refer Appendixexplanation, proof original IDMM specific that, contraryoften claimed, satisfy so-called nestedness property.results disprove conjecture (Bernard, 2007; De Cooman et al., 2009a)IDMM inference systemsour version original oneare ones, evenconservative ones, satisfy representation insensitivity specificity.show Section 15 IDMM family immediate predictionswhichversion original oneare definite sense conservative onesrepresentation insensitive specific, satisfy another requirement,called concave surprise.conclusion (Section 16) point number surprising consequencesresults, discuss avenues research.order make paper self-contained possible, included numberappendices additional discussion. help reader find way manynotions notations need paper, Appendix provides list commonones, short hint meaning, introduced. Appendix B providesuseful necessary background theory multivariate polynomials simplices,important part Bernstein basis polynomials there. discussion IDMMinference systems relies quite heavily Dirichlet densities simplices, expectationoperators associated them. discuss important relevant propertiesAppendix C. Appendix contains discussion original IDM IDMM models,proposed Walley Bernard (see Walley, 1991, 1996; Walley & Bernard, 1999; Bernard,6fiCoherent Predictive Inference Exchangeability2005), show claims make model needcarefully formulated. stated above, main reason introducing,Section 12, modified version IDMM models, suffershortcomings, produces immediate prediction models original version.Finally, effort make lengthy paper readable possible, movedproofs, additional technical discussion, Appendix E.2. Imprecise Probability Modelssection, give concise overview imprecise probability models representing,making inferences decisions under, uncertainty. suggested Introduction,shall focus sets desirable gambles uncertainty models choice.Let us briefly summarise next section why, present paper, worksets basic uncertainty models conservative probabilistic inference. readerwants dispense motivation proceed Section 2.2, introducemathematics behind models. later sections, shall course also briefly mentionderived results terms familiar language (lower) previsions probabilities.2.1 Sets Desirable Gambles?First all, number examples literature (Moral, 2005; Couso & Moral, 2011; DeCooman & Quaeghebeur, 2012; De Cooman & Miranda, 2012) shown workingmaking inferences using models general expressive.also simpler elegant mathematical point view, intuitivegeometrical interpretation (Quaeghebeur, 2014). shall see Sections 2.4 3marginalisation conditioning especially straightforward, issuesconditioning sets (lower) probability zero.Also, become apparent discussion Section 2.2, explaineddetail Moral Wilson (1995) De Cooman Miranda (2012),similarity accepting gamble one hand accepting proposition trueother, gives logical flavour conservative probabilistic inference. Indeed,strong analogy two, connects conservative probabilistic inferencealsocalled natural extension fieldwith logical deduction: classical propositionallogic looking smallest deductively closed set contains number givenpropositions, imprecise probabilities context looking smallest coherent setdesirable gambles contains number given gambles. context analogy,precise probability models closely related complete, maximal, deductively closedsetsperfect information states. clear indication precise probability modelswell suited dealing conservative inference, needbroader context imprecise probability models natural language settingthis. summary, working sets desirable gambles encompasses subsumesspecial cases classical (or precise) probabilistic inference inference classicalpropositional logic; see detailed discussion De Cooman Miranda (2012).Finally, briefly explain Section 5, De Cooman Quaeghebeur (2012)shown working sets coherent desirable gambles especially illuminatingcontext modelling exchangeability assessments: exposes simple geometrical meaning7fiDe Cooman, De Bock, & Diniznotion exchangeability, leads simple particularly elegant proofsignificant generalisation de Finettis (1937) Representation Theorem exchangeablerandom variables.summary, work sets desirable gambles powerful,expressive general models hand, intuitive work withthoughunfortunately less familiar people closely involved field, and,importantly, avoid problems conditioning sets (lower) probabilityzero. details, refer work Walley (2000), Moral (2005), Couso Moral(2011), De Cooman Quaeghebeur (2012), Quaeghebeur (2014).2.2 Coherent Sets Desirable Gambles Natural Extensionconsider variable X assumes values finite4 possibility space A. modelsubjects beliefs value X looking gambles variablesubject finds desirable, meaning strictly prefers5 zero gamblethe statusquo. general approach, extends usual rationalist subjectivistapproach probabilistic modelling allow indecision imprecision.gamble real-valued function f A. interpreted uncertain reward f (X)depends value X, expressed units predetermined linear utility.represents reward subject gets transaction first actual value xX determined, subject receives amount utility f (x)which maynegative, meaning pay it. Throughout paper, use device writing f (X)want make clear variable X gamble f depends on.Events subsets possibility space A. event B associatespecial gamble IB , called indicator, assumes value 1 B 0 elsewhere.denote set gambles L(A). linear space point-wiseaddition gambles, point-wise multiplication gambles real numbers.subset L(A), posi(A) set positive linear combinations gambles A:posi(A) :={n}k fk : fk A, k R>0 , n N .(1)k=1Here, N set natural numbers (without zero), R>0 set positive realnumbers. convex cone gambles subset L(A) closed positive linearcombinations, meaning posi(A) = A.two gambles f g A, write f g (x A)f (x) g(x), f > gf g f 6= g. gamble f > 0 called positive. gamble g 0 called non-positive.4. sake simplicity, restrict discussion finite possibility spaces,really need purposes paper. limited number remarks on, shalloccasion mention related notions infinite possibility spaces, give ample referencesguide interested reader relevant literature.5. want point notion strict preferenceor preference without indifferencecommonlyused preference modelling, confused Walleys (1991, Section 3.7.7) notion strictdesirability, one many ways construct lower prevision set gamblesstrictly preferred zero gamble; see also discussion near end Section 2.5.details, refer recent paper Quaeghebeur, De Cooman, Hermans (2014).8fiCoherent Predictive Inference ExchangeabilityL>0 (A) denotes convex cone positive gambles, L0 (A) convex conenon-positive gambles.collect gambles subject finds desirablestrictly prefers6 zero gambleset desirable gambles, shall take sets basic uncertainty models.course, satisfy certain rationality criteria:Definition 1 (Coherence). set desirable gambles L(A) called coherentsatisfies following requirements:D1. 0/ D;D2. L>0 (A) D;D3. = posi(D).D(A) denotes set coherent sets desirable gambles A.Requirement D3 turns convex cone. Due D2, includes L>0 (A); D1D3,avoids non-positivity:D4. f 0 f/ posi(D), equivalently L0 (A) posi(D) = .L>0 (A) smallest coherent subset L(A). so-called vacuous model thereforereflects minimal commitments part subject: knows absolutely nothinglikelihood different outcomes, strictly prefer zerogambles never decrease wealth possibility increasing it.D1 D2 , subject set desirable gambles D1 conservative, lesscommittal, subject set desirable gambles D2 , simply latter strictlyprefers zero gambles former does, possibly more. inclusion relationimposes natural partial ordering sets desirable gambles, simple interpretationleast conservative as.non-empty family coherent sets desirable gambles Di , I, intersectioniI Di still coherent. simple result underlies notion (conservative) coherentinference. subject gives us assessmenta set L(A) gamblesfinds desirablethen tells us exactly assessment extended coherentset desirable gambles, construct smallestand therefore least committalconservativesuch set:Theorem 2 (Natural Extension, De Cooman & Quaeghebeur, 2012). Let L(A),define natural extension by:7EA :={D D(A) : D} .following statements equivalent:(i) avoids non-positivity: L0 (A) posi(A) = ;(ii) included coherent set desirable gambles;6. See footnote 5.7. usual, expression, let = L(A).9fiDe Cooman, De Bock, & Diniz(iii) EA 6= L(A);(iv) set desirable gambles EA coherent;(v) EA smallest coherent set desirable gambles includes A.(and hence all) equivalent statements holds, EA = posi(L>0 (A) A).Moreover, coherent 6= L(A) EA = A.2.3 Maximal Coherent Sets Desirable Gambleselement D(A) called maximal strictly included elementD(A), words, adding gamble f makes sure longer extendset {f } set still coherent:(D0 D(A))(D D0 = D0 ).M(A) denotes set maximal elements D(A). coherent set desirable gamblesmaximal non-zero gambles f A, f/ f (see Couso &Moral, 2011 case finite A, De Cooman & Quaeghebeur, 2012 infinitecase). Coherence natural extension described completely terms maximalelements:Theorem 3 (Couso & Moral, 2011; De Cooman & Quaeghebeur, 2012). set avoidsnon-positivitymaximal M(A) D. Moreover,EA = {D M(A) : D}.2.4 Conditioning Sets Desirable GamblesLet us suppose subject coherent set desirable gambles A, expressingbeliefs value variable X assumes A. ask so-calledupdated set DcB desirable gambles B would be, receive additionalinformationand nothing morethat X actually belongs subset B A.updating, conditioning, rule sets desirable gambles states that:g DcB gIB gambles g B.(2)states gamble g desirable subject observe X Bcalled-off gamble gIB desirable him. called-off gamble gIBgamble variable X gives zero rewardis called offunless X B,case reduces gamble g new possibility space B. updated set DcBset desirable gambles B still coherent, provided (De Cooman& Quaeghebeur, 2012). See discussions Moral (2005), Couso Moral (2011), DeCooman Quaeghebeur (2012), De Cooman Miranda (2012) Quaeghebeur (2014)detailed information updating sets desirable gambles.2.5 Coherent Lower Previsionsuse coherent sets desirable gambles introduce derived concepts, coherentlower previsions, probabilities.10fiCoherent Predictive Inference ExchangeabilityGiven coherent set desirable gambles D, functional P defined L(A)P (f ) := sup { R : f D} f L(A),(3)coherent lower prevision (Walley, 1991, Thm. 3.8.1). means lowerenvelope expectations associated set probability mass functions,8 or,equivalently, satisfies following coherence properties (Walley, 1991, 2000; DeCooman & Quaeghebeur, 2012; Miranda & De Cooman, 2014; Troffaes & De Cooman, 2014):P1. P (f ) min f gambles f A;P2. P (f + g) P (f ) + P (g) gambles f, g A;P3. P (f ) = P (f ) gambles f real 0.used notation min f := min {f (x) : x A}; max f defined similarly.conjugate upper prevision P defined P (f ) := inf { R : f D} = P (f ).following properties implied P1P3:P4. max f P (f ) P (f ) min f gambles f A;P5. P (f + ) = P (f ) + P (f + ) = P (f ) + gambles f R.gamble f , P (f ) called lower prevision f , follows Equation (3)interpreted subjects supremum desirable price buying gamble f .event B, P (IB ) also denoted P (B), called lower probability B;interpreted subjects supremum desirable rate betting B. Similarlyupper previsions upper probabilities.lower prevision associated vacuous set desirable gambles L>0 (A) givenP (f ) = min f . called vacuous lower prevision, point-wise smallest,conservative, coherent lower previsions.coherent conditional model DcB, B non-empty subset A, induces conditional lower prevision P (|B) L(B), invoking Equation (3):P (g|B) := sup { R : g DcB} = sup { R : [g ]IB D}gambles g B. (4)difficult show (Walley, 1991) P P (|B) related followingcoherence condition:P ([g P (g|B)]IB ) = 0 g L(B),(GBR)called Generalised Bayes Rule. rule allows us infer P (|B) uniquely P ,provided P (B) > 0. Otherwise, usually infinity coherent lower previsionsP (|B) coherent P sense satisfy (GBR), equivalently,coherent set desirable gambles leads P P (|B). Two8. statement valid working finite A. infinite A, similar results shownhold (Walley, 1991; De Cooman & Quaeghebeur, 2012; Miranda & De Cooman, 2014; Troffaes &De Cooman, 2014), expectations involved coherent previsionsexpectation operatorsassociated finitely additive probability measures. See also discussion Section 2.6.11fiDe Cooman, De Bock, & Dinizparticular conditioning rules, namely natural regular extension (Walley, 1991; Miranda& De Cooman, 2014), always produce conditional lower previsions satisfy GBR,therefore coherent P . P (B) > 0but necessarily P (B) = 0!theyalways produce point-wise smallest largest coherent conditional lower previsions,respectively (Miranda, 2009; Miranda & De Cooman, 2014).9Many different coherent sets desirable gambles lead coherent lower previsionP , typically differ boundaries. sense, coherent sets desirablegambles informative coherent lower previsions: gamble positive lowerprevision always desirable one negative lower prevision never, gamblezero lower prevision lies border set desirable gambles, lowerprevision generally provide information desirability gambles.border behaviour importantand dealing conditioning eventszero (lower) probability (Walley, 2000; Moral, 2005; Couso & Moral, 2011; Quaeghebeur,2014)it useful work sets desirable gambles rather lower previsions,Equations (2) (4) tell us, allow us derive unique conditional modelsunconditional ones: coherent set desirable gambles corresponds uniqueconditional set desirable gambles DcB unique conditional lower prevision P (|B),non-empty event B. smallest set desirable gambles induces given coherentlower prevision, called associated set strictly desirable gambles (Walley, 1991)given {f L(A) : f > 0 P (f ) > 0}. See papers Walley (2000) Quaeghebeur(2014) additional discussion sets desirable gambles informativecoherent lower previsions.2.6 Linear Previsions Credal Setscoherent lower upper prevision coincide gambles, realfunctional P defined L(A) P (f ) := P (f ) = P (f ) f L(A) coherent prevision.Since assumed finite,10 means correspondsexpectationoperator associated probability mass function p: P (f ) = xA f (x)p(x) =: Ep (f )f L(A), p(x) := P (I{x} ) x A. happens particular lowerupper previsions induced maximal coherent set desirable gambles. Indeed,boundary behaviour, so-called precise probability models P correspond maximalcoherent sets desirable gambles; see discussions Williams (1975a), MirandaZaffalon (2011, Proposition 6) Couso Moral (2011, Section 5) information.coherent previsions P , Generalised Bayes Rule (GBR) reduces Bayess Rule:P (gIB ) = P (B)P (g|B) g L(B),(BR)indicating central probabilistic updating rule special case Equation (2).9. conditional lower previsions Section 12 IDMM produced regular extension.models Sections 11, 13 14 lower previsions amongst them, nearly casesdifferent conditional lower previsions, even though cases natural regular extensionscoincidethey vacuous there.10. already hinted footnote 8, similar things still said infinite A, would undulycomplicate discussion. details, see work Walley (1991), Troffaes De Cooman(2014) Miranda De Cooman (2014).12fiCoherent Predictive Inference Exchangeabilityassumed finite, define so-called credal set M(P ) associatedcoherent lower prevision P as:M(P ) := {p : (f L(A))Ep (f ) P (f )} ,closed convex subset so-called simplex probability massfunctions A.11 P lower envelope M(P ): P (f ) = min {Ep (f ) : p M(P )}f L(A) (Walley, 1991; Miranda & De Cooman, 2014; Troffaes & De Cooman,2014). sense, convex closed sets precise probability models also seenimprecise probability models, mathematically equivalent coherent lowerprevisions. therefore also less general powerful coherent sets desirablegambles, also suffer problems conditioning events (lower) probabilityzero.123. Predictive InferencePredictive inference, specific sense focussing here, considers numbervariables X1 , . . . , Xn assuming values category set Awe define category setnon-empty finite set.13 follows, shall occasion use many differentcategory sets, shall use italic capitals A, B, C, D, . . . refer them.start discussion predictive inference models general representationally powerful language: coherent sets desirable gambles, introduced previoussection. on, shall also pay attention specific derived models,predictive lower previsions, predictive lower probabilities.Predictive inference assumes generally number n observations made,= (x1 , . . . , xn ) first n variables X1 , . . . , Xn . Basedknow valuesn cvaluessubject posterior predictive model DAobservation sample ,nncoherent setnext n variables Xn+1 , . . . , Xn+n assume . DA cdesirable gambles f (Xn+1 , . . . , Xn+n ) . assume n N.hand, want allow n N0 := N {0}, set natural numberszero: also want able deal case previous observationsn prior predictive model.14 course,made. case, call corresponding model DAtechnically speaking, n + n n.said, subject may also prior, unconditional model, obnservations yet made. general form, coherent set DA11. See Section 5.2 explicit definition .12. Using sets full conditional measures (Dubins, 1975; Cozman, 2013), rather sets probabilitymass functions, leads imprecise probability model related sets desirable gambles (Couso& Moral, 2011), problems conditioning sets lower probability zero either,feel less elegant mathematically complicated.13. formal reasons, include trivial case category sets single element, casecertain value variables assume.14. terms posterior prior association predictive models indicate whether previousobservations made. But, order avoid well-known issues temporal coherence(Zaffalon & Miranda, 2013), assuming prior posterior models basedsubjects beliefs observations made, posterior models refer hypotheticalfuture situations.13fiDe Cooman, De Bock, & Dinizndesirable gambles f (X1 , . . . , Xn ) , n N. may also coherent sets DAndesirable gambles f (X1 , . . . , Xn ) , n natural numbern n must related followingn n; sets DAmarginalisation, time consistency, requirement:15nnf (X1 , . . . , Xn ) DAf (X1 , . . . , Xn ) DAgambles f .(5)expression, throughout paper, identify gamble f cylindricalextension f 0 , defined f 0 (x1 , . . . , xn , . . . , xn ) := f (x1 , . . . , xn ) (x1 , . . . , xn ) .introduce marginalisation operator margn () := L(An ), time consistencyn = marg (D n ) = n L(An ).condition also rewritten simply DAnn posterior (conditional) ones n cPrior (unconditional) predictive models DAmustalso related following updating requirement:nnf (Xn+1 , . . . , Xn+n )I{}f (Xn+1 , . . . , Xn+n ) DAc(X1 , . . . , Xn ) DAgambles f , (6)special case Equation (2): gamble f (Xn+1 , . . . , Xn+n ) desirable observ gamble f (Xn+1 , . . . , Xn+n )I{}ing sample(X1 , . . . , Xn ) desirableobservations made. called-off gamble f (Xn+1 , . . . , Xn+n )I{}(X1 , . . . , Xn )gamble gives zero rewardis called offunless first n observations ,case reduces gamble f (Xn+1 , . . . , Xn+n ) remaining variablesXn+1 , . . . , Xn+n . updating requirement generalisation Bayess Rule updating,fact reduces sets desirable gambles lead (precise) probabilitymass functions, described Section 2.6 proved detail Walley (2000) alsoDe Cooman Miranda (2012). contrary Bayess Rule probability massfunctions, updating rule (6) coherent sets desirable gambles clearly sufferproblems conditioning event (lower) probability zero: allows us inferunique conditional model unconditional one, regardless (lower upper)probability conditioning event. refer work De Cooman Miranda(2012) detailed discussions marginalisation updating sets desirable gamblesmany-variable context.explained Section 2.5, use relationship (3) derive prior (unconditional)n through:predictive lower previsions P nA () L(An ) prior set DAnP nA (f ) := sup { R : f DA} gambles f 1 n n,L(An ) posteriorposterior (conditional) predictive lower previsions P nA (|)nthrough:sets DA c{}n:= sup R : f DAgambles f .P nA (f |)c15. See also related discussion notion De Cooman Miranda (2008b) De CoomanQuaeghebeur (2012); confused temporal consistency discussed Goldstein(1983, 1985) Zaffalon Miranda (2013).14fiCoherent Predictive Inference Exchangeabilityon, shall also want condition predictive lower previsions additionalinformation (Xn+1 , . . . , Xn+n ) B n , proper subset B A. Using ideasSections 2.4 2.5, leads instance following lower prevision:{}nB n ) := sup R : [g ]IB n DAgambles g B n ,P nA (g|,c(7)conditioned event B n .lower prevision P nA (|)4. Principles Predictive Inferencefar, introduced coherence, marginalisation updating basic rationalityrequirements prior posterior predictive inference models must satisfy. couldenvisaged requirementsother inference principlescan imposedinference models. want show deal additionalrequirements theory conservative predictive inference, discuss, wayexamples, number additional conditions, suggested numberauthors reasonable properties ofor requirements forpredictive inference models.want stress considering requirements examples, wantdefend using circumstances, mean suggest always reasonableuseful. are: inference principles might want impose, whoseimplications conservative predictive inference might therefore want investigate.4.1 Pooling Invariancefirst consider Walleys (1996) notion representation invariance, prefer callpooling invariance. Consider set categories A, partition B non-emptypartition classes. course consider partition B set categories well.Therefore, order streamline discussion notation, shall henceforth denoteBas stated before, want use italic capitals category sets. elementssubset C Acorresponds single new category, consists originalcategories x C pooledconsidered one. Denote (x) unique elementpartition B original category x belongs to. leads us consider surjective(onto) map B.say gamble g differentiate pooled categories when:g() = g() , (k {1, . . . , n})(xk ) = (yk ),means gamble f B n that:( )g() = f ((x1 ), . . . , (xn )).idea underlying formulaor requirementis sample = (x1 , . . . , xn ), corresponds sample := ((x1 ), . . . , (xn )) B n pooled categories. Poolinginvariance requires gambles g = f differentiate pooledcategories, make difference whether make predictive inferences using setoriginal categories A, using set pooled categories B. formally, termspredictive lower previsions:15fiDe Cooman, De Bock, & Diniz= P nB (f |)P nA (f ) = P nB (f ) P nA (f |),n, n N considered, gambles f B nalternatively, generally, terms predictive sets desirable gambles:nnnnf DBf DAf DBf DAcc.n, n N considered, gambles f B nPooling invariance seems reasonable principle uphold cases categoryset known full detail. case useful start limited set broadlydefined categories, allow creation new ones, pooling splitting old categoriesobservations proceed. context, recall Walleys (1996) example:closed bag containing coloured marbles, probability drawing red marbleit? information, subject idea coloursmarbles bag, making difficult construct suitable detailed category setexperiment. draws bag, predictive inference model usedrespects pooling invariance, inferences made red marbles usescategory set {red, yellow, blue, other} using categoryset {red, non-red}, colours different red pooled together singlecategory. appears pooling invariance typically useful principle, instance,sampling species problems, one wants assess prevalence given speciescertain area.special case pooling invariance, called embedding invariance,16 concentrates case without prior observations. terms lower previsions:P nA (f ) = P nB (f ) n N considered, gambles f B n ,alternatively, generally, terms sets desirable gambles:nnf DAf DBn N considered, gambles f B n .4.2 Renaming InvarianceBesides pooling invariance, may also require renaming invariance: long confusionarise, matter subjects predictive inferences names, labels,gives different categories.may seem trivial even mention, far know, always implicitlytaken granted predictive inference. well devote attentionhere, order distinguish category permutation invariance discussedshortly, easily confused pay proper attention.renaming bijection (a one-to-one onto map) set original categoriesset renamed categories C, clearly distinguish elementsC, sample = (x1 , . . . , xn ) original categories,corresponds sample renamed categories := ((x1 ), . . . , (xn )). gamble16. Walley calls underlying requirement (lower) probability event dependpossibility space embedded, Embedding Principle (Walley, 1991, Section 5.5.1).16fiCoherent Predictive Inference Exchangeabilityf set C n renamed samples, corresponds gamble f setoriginal samples. Clearly, require make difference whethermake predictive inferences using set original categories A, using set renamedcategories C. formally, terms predictive lower previsions:= P nC (f |)P nA (f ) = P nC (f ) P nA (f |),n, n N considered, gambles f C nalternatively, generally, terms predictive sets desirable gambles:nnnnf DCf DAf DCf DAcc.n, n N considered, gambles f C n4.3 Category Permutation Invarianceshall especially interested predictive inference subject starts stateprior ignorance. state, reason distinguish different elementsset categories chosen. formalise idea, consider permutation$ elements A.17 sample , corresponds permuted sample$ := ($(x1 ), . . . , $(xn )). gamble f , corresponds permutedgamble f $ . subject reason distinguish categories zimages $(z), make sense require following category permutation invariance:18= P nA (f |$)P nA (f $) = P nA (f ) P nA (f $|),n, n N considered, gambles falternatively, generally, terms predictive sets desirable gambles:nnnnf DAf $ DAf DAf $ DAcc$.n, n N considered, gambles fFormally, requirement closely resembles renaming invariance, whereas lattertrivial requirement, category permutation invariance symmetry requirementcategories justified subject reason distinguishthem, may instance justified starts state prior ignorance.draw attention difference two somewhat loose manner: categorypermutation invariance allows confusion new old categories, somethingrenaming invariance carefully avoids.see principle could reasonable, recall Walleys (1996) bag marblesexample, introduced discussing pooling invariance. Since, drawn17. permutation $ elements A, words categories, contrastedpermutations order observations, i.e. time set {1, . . . , n}, considered discussionexchangeability, Section 5.18. requirement related notion (weak) permutation invariance De Cooman Miranda(2007) studied much detail paper dealing symmetry uncertainty modelling. goesback Walleys (1991, Section 5.5.1) Symmetry Principle.17fiDe Cooman, De Bock, & Dinizmarbles bag, subject idea marbles coloured, statecomplete prior ignorance. Therefore, starts sample space {red, non-red},observes outcomes draws, say twice non-red, consider probabilityobtaining red marble next draw. due symmetry originating completeignorance, permute categories, calling red marbles non-rednon-red ones red, situation looking completely before,therefore probability obtaining non-red marble next draw observingtwice red, must observing red one, observing non-red twice.principle reminiscent Axiom A8 proposed Carnap (1952) systeminductive logic. course, reasonable principle subject priorknowledge problem would, instance, allow impose orderingcategories.4.4 Representation Insensitivityshall call representation insensitivity combination pooling, renaming categorypermutation invariance. means predictive inferences remain essentially unchangedtransform set categories, words insensitivechoice representationthe category set. difficult see representationinsensitivity formally characterised follows. Consider two category setsso-called relabelling map : onto, i.e.= (A) := {(x) : x A}. sample , corresponds transformedsample := ((x1 ), . . . , (xn )) Dn . gamble f Dn correspondsgamble f .4.4.1 Representation Insensitivitycategory sets onto map : D, n, n Ngambles f Dn :considered,= P nD (f |),P nA (f ) = P nD (f ) P nA (f |)(RI1)alternatively, generally, terms predictive sets desirable gambles:nnnnf DDf DAf DDf DAcc.(RI2)also weaker combination pooling, renaming category permutationinvariance models prior observations.4.4.2 Prior Representation Insensitivitycategory sets onto map : D, n N consideredgambles f Dn :P nA (f ) = P nD (f ),(EI1)alternatively, generally, terms sets desirable gambles:nnf DAf DD.18(EI2)fiCoherent Predictive Inference Exchangeability4.5 Specificityturn another, rather peculiar view intuitively appealing, potential property predictive inferences. Assume addition observing sample observationsn observations category set A, subject comes know determineway n following observations belong proper subset B A, nothingelsewe might suppose instance observation (Xn+1 , . . . , Xn+n ) made,imperfect, allows conclude (Xn+1 , . . . , Xn+n ) B n .impose following requirement, uses models conditionedevent B n . conditional models introduced Equations (2) (4); seealso discussion leading Equation (7), near end Section 3.4.5.1 Specificitycategory sets B B A, n, n N considered,gambles f B n :B n ) = P nB (f |B ),P nA (f |B n ) = P nB (f ) P nA (f |,(SP1)alternatively, generally, terms predictive sets desirable gambles:nnnnf DBB,f IB n DAf DBf IB n DAcc(SP2)B tuple observations obtained eliminating tupleobservawhereB empty tuple, observationstions B. expressions,B, posterior predictive model simply taken reduce prior predictivemodel.Specificity means predictive inferences subject makesones would get focussing category set B, time discardingprevious observations producing values outside B, effect retaining observationsinside B! knowing future observations belong B allowssubject ignore previous observations happened lie outside B. termspecificity context seems proposed Bernard (1997, 2005), basedwork Rouanet Lecoutre (1983). so-called specific inference approach, questions,inferences decisions involving restricted number categories, generalmodel replaced specific model deals categories interest,specificity respected, general specific models produceinferences. Specificity seems relevant principle analysing categorical datadescribed tree structures, case of, instance, patients classifiedaccording symptoms (Bernard, 1997).give simple example involving, again, Walleys bag marbles, subjectmay observed, drawings, green, red, blue white marbles. askedprobability drawing red marble next, observer already seenis, informs us either green redperhaps due bad lighting conditionsshes colour blind. subject uses specific inference model, disregardprevious observations involving colours green red.19fiDe Cooman, De Bock, & Diniz4.6 Prior Near-Ignoranceuse notion near-ignorance defined Walley (1991, p. 521) give followingdefinition prior near-ignorance context predictive inference; see also relateddiscussions Walley (1991, Section 5.3.2), Walley (1997, Section 3) Walley Bernard(1999, Section 2.3). also refer paper Piatti, Zaffalon, Trojani, Hutter (2009)interesting discussion prior near-ignorance may produce undesirable resultscertain contexts.4.6.1 Prior Near-Ignoranceprior model single variable Xk assuming values arbitrary category setvacuous, category set A, n N considered, 1 k n gamblesf A:P nA (extnk (f )) = min f,alternatively, generally, terms sets desirable gambles:nextnk (f ) DAf > 0,extnk (f ) denotes cylindrical extension f gamble . definedextnk (f )(x1 , . . . , xn ) := f (xk ) (x1 , . . . , xn ) . perhaps intuitive, lessformally correct, notation gamble f (Xk ).Theorem 4. Prior representation insensitivity implies prior near-ignorance.simple result implies model whose predictive previsions preciseprior representation insensitive, let alone representation insensitive, prior modelimmediate predictions vacuous. shall see Section 14nevertheless possible representation insensitive coherent inferences deploy preciseposterior predictive previsions.5. Adding Exchangeability Picturenow, remainder paper, going add two additional assumptions.first assumption is, principle, upper bound numbervariables take account. words, considering n variablesX1 , . . . , Xn , always envisage looking one variable Xn+1 . effectivelymeans dealing countably infinite sequence variables X1 , . . . , Xn , . . .assume values category set A.n coherentpredictive inference models, means sequence DAsets desirable gambles , n N. sequence course time-consistentsense Requirement (5), meaningn1n2n2(n1 , n2 N)(n1 n2 DA= margn1 (DA) = DAL(An1 )).second assumption sequence variables exchangeable, means,roughly speaking, subject believes order variables observed,20fiCoherent Predictive Inference Exchangeabilitypresent themselves, influence decisions inferences make regardingthem.19section, explain succinctly deal assumptions technically,consequences predictive models interested in. detaileddiscussion derivation results presented here, refer papers De Coomanet al. (2009b) De Cooman Quaeghebeur (2012).begin useful notation, employed numerous timesfollows. Consider element RA . consider A-tuple, many (real)componentsx R categories x A. subset B A, denoteB := xB x sum components B.5.1 Permutations, Count Vectors Hypergeometric DistributionConsider arbitrary n N. denote = (x1 , . . . , xn ) generic, arbitrary element .P n set permutations index set {1, . . . , n}. permutation ,associate permutation , also denoted , defined ()k := x(k) ,words, (x1 , . . . , xn ) := (x(1) , . . . , x(n) ). Similarly, lift permutationL(An ) letting f := f , ( f )() := f ().permutation invariant atoms [] := { : P n }, smallest permutation invariant subsets . introduce counting map : NAn : 7 (),count vector () A-tuple componentsTz () := |{k {1, . . . , n} : xk = z}| z A,set possible count vectors n observations given{}NAn := NA0 : = n .(8)(9)Tz () number times category z appears sample . = (),[] = { : () = }, atom [] completely determined single countvector elements, therefore also denoted [].also consider linear expectation operator HynA (|) associated uniformdistribution invariant atom []:HynA (f |) :=1f () gambles f ,|[]|(10)[]number elements () := |[]| invariant atom [] givenmultinomial coefficient:() ( )nn!:=() ==.(11)zA mz !expectation operator Equation (10) characterisesor one associated(multivariate) hyper-geometric distribution (Johnson, Kotz, & Balakrishnan, 1997, Section 39.2), associated random sampling without replacement urn n balls19. Exchangeability also assumed Carnaphis Axiom A7and Johnson (1924), namedpermutation postulate.21fiDe Cooman, De Bock, & Diniztypes z A, whose composition characterised count vector . borne0fact that, , 0 n0 n 0 = (),{( 0 )/() 0HynA (I{} |) =0otherwiseprobability randomly selecting, without replacement, sequence n0 balls typesurn n balls whose composition determined count vector . Seealso running example concrete illustration.hyper-geometric expectation operator also seen linear transformationHynA linear space L(An ) generally much lower-dimensional linear spaceL(NAn ), turning gamble f so-called count gamble HynA (f ) := HynA (f |)count vectors.Running Example. order make argumentation, notions introducediscuss, tangible concrete, shall use simple running example,shall come back repeatedly number sections. notations assumptions mademaintained throughout series.Consider (potentially infinite) sequence coin flips, whose successive outcomesdenote variables X1 , X2 , . . . Xn , . . . assuming values category set {H , }.make somewhat interesting usual run-of-the-mill example, assumestepfor coin flipNathalie selects coin bag three coins, handsArthur, proceeds flip it. coin put back bag nextstep. subject whose beliefs modelling, may may know somethingnature coins, Nathalie choosing coins subsequent flips:might choose completely random, might specific deterministicmechanism selecting them, . . .= (H , , H , H ) first n = 4 observed coin flips. countConsider sequencecorresponds sequence given componentsvector ()TH ((H , , H , H )) = 3 TT ((H , , H , H )) = 1,= (3, 1), letting first component always refer H ,denoteon. corresponding permutation invariant atom[(H , , H , H )] = [(3, 1)] = {(T , H , H , H ), (H , , H , H ), (H , H , , H ), (H , H , H , )}4!3!1!4= 4 elements. set possible count vectors given N{H,T } =:= {(H , ), (T , H )} {H , }2{(0, 4), (1, 3), (2, 2), (3, 1), (4, 0)}. Consider event HTtwo different outcomes first two observations,((3, 1)) =11Hy4{H ,T } (IHT|(3, 1)) = (1 + 1 + 0 + 0) =42probability observing two different outcomes two random draws without replacement urn containing three balls marked H one ball marked , whosecomposition therefore determined count vector (3, 1).22fiCoherent Predictive Inference Exchangeability5.2 Multinomial DistributionNext, consider simplex probability mass functions A:{}:= RA : 0 = 1 , where, before: :=x .(12)xAprobability mass function A, corresponds following multinomialexpectation operator MnnA (|):20MnnA (f |) :=f ()zTz () gambles f ,(13)zAcharacterises multinomial distribution, associated n independent trialsexperiment possible outcomes probability mass function . Observe)( 1f ()) ()zmzMnnA (f |) =()nzANA[]nmz=HyA (f |)()z = CoMnnA (HynA (f )|),nNAzAused so-called count multinomial expectation operator:21CoMnnA (g|) :=g()()zmz gambles g NAn .nNA(14)zARunning Example. Consider n = 4 independent trials experiment possible outcomescategory set {H , } probability mass function = (H , ).32 232Mn4{H ,T } (IHT|(H , )) = 2H + 4H + 2H = 2H (H + ) = 2H ,. Observe, way, Mnngives probability event HT|(H , )) ={H ,T } (IHT2H n 2.gamble fHT:= IHTobservation sequences (X1 , . . . , X4 ), corresponds4count gamble gHT:= Hy{H ,T } (fHT|) given by:gHT(0, 4) = 0 gHT(1, 3) =121gHTgHTgHT(2, 2) =(3, 1) =(4, 0) = 0,23212 2 21 33CoMn4{H ,T } (g|(H , )) = 4H+ 6H+ 4H= 2H232leads polynomial before, should.20. avoid confusion, make (perhaps non-standard) distinction multinomial expectation,associated sequences observations, count multinomial expectation, associatedcount vectors.21. See footnote 20.23fiDe Cooman, De Bock, & Diniz5.3 Multivariate PolynomialsLet us introduce notation NA := mN NAm set possible count vectorscorresponding samples least one observation. Equation (9), also let n = 0,turns NA0 singletoncontaining null count vector 0, whosecomponents zero. mN0 NAm = NA {0} set possible count vectors.count vector NA {0}, consider (multivariate) Bernstein basispolynomial BA, degree , defined by:()mzmzBA, () := ()z =z .(15)zAzAparticular, course, BA,0 = 1.linear combination p Bernstein basis polynomials degree n 0 (multivariate)polynomial , whose degree deg(p) n.22 denote linear spacepolynomials degree n V n (A). course, polynomials degree zero simply realconstants. gathered relevant useful information multivariate polynomialsAppendix B. follows discussion that, n 0, introducelinear isomorphism CoMnnA linear spaces L(NAn ) V n (A):gamble gNAn , corresponds polynomial CoMnnA (g) := CoMnnA (g|) = N n g()BA,V n (A), conversely, polynomial p V n (A) unique gamble bnp NAnp = CoMnnA (bnp ).23 Observe particular, n 0 NAn :CoMnnA ({}|) = BA, () .(16)denote V (A) := nN0 V n (A) linear space (multivariate) polynomials ,arbitrary degree.set HA V (A) polynomials called Bernstein coherent satisfiesfollowing properties:B1. 0/ HA ;B2. V + (A) HA ;B3. posi(HA ) = HA .Here, V + (A) set Bernstein positive polynomials : polynomials pn deg(p) bnp > 0. follows Proposition 28 Appendix BV + (A) subset set V ++ (A) polynomials p p() > 0interior int(A ) := { : (x A)x > 0} . consequence B1B3,find set V0 (A) := V + (A) Bernstein negative polynomials that:B4. V0 (A) HA = .22. degree may smaller n sum Bernstein basis polynomials fixed degreeone. Strictly speaking, polynomials p restrictions multivariate polynomials q RA ,called representations p. p, multiple representations, possibly different degrees.smallest degree called degree deg(p) p.23. Strictly speaking, Equation (14) defines count multinomial expectation operator CoMnnn > 0, clear definition extends trivially case n = 0.24fiCoherent Predictive Inference ExchangeabilityFinally, every Bernstein coherent set HA polynomials induces lower previsionH V (A) defined by:H (p) := sup { R : p HA } p V (A).(17)lower prevision coherent, mathematical sense satisfies coherencerequirements P1P3.245.4 Exchangeability Representation Theoremready deal exchangeability. shall give definition coherent setsdesirable gambles generalises de Finettis (1937, 1975) definition, allowssignificant generalisation Representation Theorem.First all, fix n N. subject considers variables X1 , . . . , Xnexchangeable distinguish gamble f permutedversion f , words, gamble f f equivalent zero gamble fororindifferent tohim. means so-called set indifferent gambles:{}n:= f f : f L(An ) P n .IAn , set must compatiblesubject also coherent set desirable gambles DAn , sense must satisfy rationalityset indifferent gambles IAnnnrequirement DA + IA = DA ; see detailed explanations justifications De CoomanQuaeghebeur (2012) Quaeghebeur et al. (2014) so-called desiring sweetenedn ,deals requirement. say sequence X1 , . . . , Xn , model DAexchangeable.Next, countably infinite sequence variables X1 , . . . , Xn . . . called exchangeablen, n Nfinite subsequences X1 , . . . , Xn are, n N. means models DAexchangeable. course also time-consistent.formulate powerful generalisation de Finettis (1937, 1975) RepresentationTheorem, straightforward compilation various results proved De CoomanQuaeghebeur (2012):Theorem 5 (Representation Theorem, De Cooman & Quaeghebeur, 2012). sequencen desirable gambles , n N coherent, time-consistent exchangeablesets DABernstein coherent set HA polynomialsNA[]:n N, gambles f ,nnMnnA (f )BA,f DAMnnA (f ) HA f DAcHA .case representation HA unique given HA :=(18)nnnN MnA (DA ).follows Condition (18) HA completely determines predictive inferencesnsequence variables X1 , . . . , Xn , . . . , fixes prior predictive models DA24. Actually, suitably adapted version, underlying possibility space need longer finite(Walley, 1991; Troffaes & De Cooman, 2014), domain restricted polynomials(De Cooman & Quaeghebeur, 2012).25fiDe Cooman, De Bock, & Dinizn c.25 tells us representation HA setposterior predictive models DApolynomials plays role probability measure, density, distributionfunction, precise-probabilistic case.Indeed, corresponding coherent lower prevision H V (A) given Equation (17),shown determine convex closed (compact) setM(H ) := {HA : (p V (A))HA (p) H (p)}coherent previsions HA V (A) (Walley, 1991; De Cooman et al., 2009b; De Cooman &Quaeghebeur, 2012; Troffaes & De Cooman, 2014). pointed footnote 2andcome back footnote 36each coherent prevision HA uniquelydetermines -additive probability measure Borel sets , therefore setpolynomials HA , via M(H ), uniquely determines set probability measures. But,argued before, HA informative H M(H ), problemsconditioning sets lower probability zero: Bernstein coherent set polynomialsHA determines unique lower prevision H , therefore M(H ) unique setprobability measuresand densities absolutely continuouson simplex ,converse necessarilyand usually notthe case. set probability densitiesused define coherent set polynomialswe provide exampleSection 12but generally one coherent set polynomialsleads set densities, updating behaviour different setspolynomials different conditioning events lower probability zero.n cdependCondition (18) also tells us posterior predictive models DAcount vector= ():count vectors sufficientobserved sequencestatistics exchangeability. reason, shall denote posteriorn cn c.well DAAlso, every then, shall usepredictive models DAnnDA c0 alternative notation DA .immediate interesting consequence Theorem 5 updating observationspreserves exchangeability: observing values first n variables, countremaining sequence variables Xn+1 , Xn+2 , . . . still exchangeable,vector ,Condition (18) tells us representation given Bernstein coherent setdefined by:polynomials HA c:= {p V (A) : BA,HA cp HA } .(19)compare Expressions (2) (6), tells us that, essentially, Bernsteinbasis polynomials serve likelihood functions updating sets polynomials. userefer coherent lower prevision V (A) derived HA cmeansH (|)= 0, find HA c0 = HA H (|0) = H .Equation (17). special caserelated following version GeneralisedObserve H H (|)Bayes Rule:H ([p H (p|)]B(20)) = 0 p V (A).A,completely determined HA . One consider HA prior modelClearly, HA cplays role posterior derived it.parameter space , HA c25. contrasted usual precise-probabilistic version, posterior predictivemodels uniquely determined observed sequences non-zero probability; see also footnote 3.26fiCoherent Predictive Inference Exchangeabilitysee Condition (18) Equation (19) thatsimilarly happens preciseprobabilistic settingthe multinomial distribution serves direct link onen and, hand,hand, prior HA prior predictive inference models DAn c.posterior predictive inference models DARecallingposterior HA cNA {0}:convention = 0, summarise follows: n N{}n= f L(An ) : MnnA (f ) HA cDAc(21)and, immediate consequence:{}= sup R : MnnA (f ) HA cf L(An )P nA (f |)(22)or, equivalently:= H (MnnA (f )|)f L(An ).P nA (f |)(23)practical point view, Equation (23) often easier work Equa often admit simpler expressiontion (22), shall see on, H (|)compare Equations (45), (54), (61) (69) Equations (49), (55), (65)HA c;always uniquely determined H : relaand (73), respectively. But, H (|)uniquely H prior lower probabilitytion (20) allows us determine H (|)H (BA,)observingnon-zero.Therefore,sets polynomials HAuniquely. quite dramaticfundamental models, allow us determine HA cillustration this, shall Sections 11, 13 14 come across numberquite different inference systemswith different HA give rise prior Hdifferent posterior H (|)!Running Example. assume subject assesses sequence coin flipsexchangeable, finds desirable gamble type I{H } (Xn ), fixed(0, 1]; upper probability observing heads coin flip . Sinceinfer Equation (13) N n, MnN{H ,T } (I{H } (Xn )|) = H , inferTheorem 5 assessment corresponds following coherent set polynomials:{}H := 1 p+ + 2 ( H ) : p+ V + ({H , }), 1 , 2 R0 max{1 , 2 } > 0 ,smallest Bernstein coherent set polynomials contains polynomialH ; explanation, see also discussions De Cooman et al. (2009b)De Cooman Quaeghebeur (2012). followsafter manipulationsfromEquation (17) Proposition 28 corresponding lower prevision V ({H , })completely determined following optimisation:H (p) = supmin [p() + (H )]0 {H ,T }givenHence, lower probability event HTH (2H ) = sup min [2x(1 x) + (x )] = 0,0 x[0,1]upper probabilityH (2H ) = H (2H ) = inf max [2x(1 x) (x )]0 x[0,1]27fiDe Cooman, De Bock, & Diniz={2(1 )1212otherwise.tells us exchangeability alone already guarantees upper probability HT12 . three coins bag assumed biased towards heads, < 12 ,upper probability drops 12 .finish section representation, want stress polynomialsgiven behavioural interpretation gambles may may desirable:merely mathematical representational tools help us characterisegambles observation sequences desirable.26 Similarly, set polynomials HAlower prevision H merely mathematical tools allow convenientrepresentation predictive models observation sequences.Running Example. illustrate polynomial representation much convenientefficient, recall want make inferences sequence coin flipslength n, need work sets desirable gambles {H , }n , words,cones 2n -dimensional space. work polynomial representations,led consider cones polynomials degree n, constitute linearspace spanned n + 1 Bernstein basis polynomials degree n, thereforen + 1-dimensional. Working polynomial representations therefore leadsdramaticexponentialreduction complexity.6. Reasoning Inference Systemsseen previous section that, fix category set A, predictive inferencesexchangeable sequences assuming values completely determined Bernsteincoherent set HA polynomials . way associating Bernsteincoherent set HA every possible set categories A, would completely fix predictiveinferences. leads us following definition.Definition 6 (Inference Systems). denote F collection category sets, i.e. finitenon-empty sets. inference system map maps category set Fset polynomials (A) = HA . inference system called coherentcategory sets F, (A) Bernstein coherent set polynomials .So, coherent inference system way systematically associate coherent predictiveinferences category set. Since inference principles Section 4 impose connectionspredictive inferences different category sets, see interpretinference principlesor rather, represent mathematicallyas properties of,restrictions on, coherent inference systems. shall Section 7,provides one important motivation introducing systems. Another, equally26. makes operational, behavioural sense consider notion accepting polynomial, findingdesirable. much like classical case, de Finetti (1975) probability distributionssimplex used mathematical representations, direct behaviouralmeaningalthough Bayesians less careful foundations de Finetti might care makedistinction.28fiCoherent Predictive Inference Exchangeabilityimportant reason so, allows us extend method natural extensionconservative inferenceintroduced Section 2.2, also take account inferenceprinciples predictive inference, generally, predictive inference multiple categorysets once.see comes about, let us show conservative reasoninginference systems. two inference systems 1 2 , say 1 less committalconservativethan 2 , write 1 v 2(A F)1 (A) 2 (A).simply means predictive inferences category set less committalfirst second inference system. denote set inferencesystems, clearly set partially ordered v. Actually, complete lattice,infimum supremum non-empty family , given by:()()inf (A) =(A) sup (A) =(A) category sets A.iIiIiIiIdenote C set coherent inference systems:C := { : (A F)(A) Bernstein coherent} .(24)clear C complete meet-semilattice, meaning closed arbitrarynon-empty infima:27(i I)i C inf C.(25)iIbottom structurethe conservative coherent inference systemis calledvacuous inference system V , coherent inference system given by:V (A) = V + (A) category sets A.shall come back detail vacuous inference system Section 9.property (25) allows us conservative reasoning coherent inference systems.Suppose, instance, collection category sets F F, assessmentsform set polynomials AA V (A), F. Then, exists,conservative coherent inference system compatible assessments givenby:= inf { C : (A F)AA (A)} .And, course, exist set polynomials AA includedBernstein coherent set polynomials HA A, F. case, difficultsee, given discussion Section 5.3, (A) = posi(V + (A) AA ) F(A) = V + (A) F \ F.27. necessarily closed suprema, however, union Bernstein coherent sets polynomialsneed Bernstein coherent.29fiDe Cooman, De Bock, & Diniz7. Representation Insensitivity Specificity ExchangeabilityLet us investigate form inference principles representation insensitivity (RI2)specificity (SP2) take predictive inference exchangeability, inferencecompletely characterised Bernstein coherent sets polynomials. allow usreformulate principles constraints onor properties ofinference systems.7.1 Representation Insensitivityrecall notations assumptions Section 4.4. surjective (onto) map: associate surjective map R : RA RD letting:R ()z :=xRA z D.(26)xA : (x)=zmap allows us give following elegant characterisation representation insensitivity.Theorem 7. coherent inference system representation insensitivecategory sets onto map : D, p V (D)NA {0}:(p R )BA, (A) pBD,R () (D).(RI3)Running Example. Assume coins bag actually rather thick, implyingnon-negligible chance fall one flat sides,remain upright. denote new state U , new category set:= {H , , U }. also consider new flat state F , meaning either heads tails,also consider, instead A, category set := {F , U } distinguishheads tails. relabelling map (H ) := (T ) := F (U ) := Uidentifies proper relations categories D.Suppose want say something lower probability eventobserving U one flip H other, immediately observingUFsequence (H , U , H , ) count vector = (2, 1, 1)the last count threerefers number U observation sequence. A-domain, gamble IUFn28expressed polynomial q = Mn{H ,T ,U } (IUF), n 2 given by:q() = 2(H + )U {H ,T ,U } .2 belongwant find whether polynomials type [2(H + )U ]12HU({H , , U }); see Equation (18).hand, seen previously, D-domain, gamble IUFexpressed polynomial p given p() = 2F U {F ,U } . Observeq = p R . count vector = (2, 1, 1) A-domain corresponds count vectorR () = (3, 1) D-domain, first component refers number Fsecond number U s. here, need check whether polynomials type[2F U ]43F U belong ({F , U }).28. similar contexts, easy check polynomial remains n 2.30fiCoherent Predictive Inference Exchangeabilitynice thing representation insensitivity makes checking whether2polynomials type [2(H + )U ]12HU belong ({H , , U }) Adomain equivalent checking whether polynomials type [2F U ]43F U belong({F , U }) D-domain.interestingly, representation insensitivity preserved taking arbitrary nonempty infima coherent inference systems, allows us look conservativerepresentation insensitive coherent inference system compatible assessmentF, way straightforward extension discussion near end Section 6.Theorem 8. Consider non-empty family , representation insensitive coherentinference systems. infimum inf iI representation insensitive coherentinference system well.7.2 SpecificityNext, turn specificity, recall notations assumptions Section 4.5. Let usdefine surjective restriction map rB : RA RB by:rB ()z := z RA z B,(27)particular, rB () count vector B obtained restricting B (indicesthe) components count vector A. also define one-to-one injection mapiA : RB RA by:{x x BiA ()x :=RB x A.(28)0otherwisemap used define following one-to-one maps IrB,A : V (B) V (A),r N0 , follows:IrB,A (p) :=bdeg(p)+r()BA,iA () polynomials p V (B).(29)pdeg(p)+rNBderive meaning following observation. polynomial p Bequivalently represented Bernstein basis B degree deg(p) + r.interpret different representations polynomials , longer equivalent,lead different polynomials IrB,A (p), r N0 . following propositions clarifyexactly effect operator IrB,A is.Proposition 9. polynomial p B r N0 : IrB,A (p) iA = p.introduce following notation, B > 0: |+B := rB ()/B .Observe |+whenever>0.BBBProposition 10. Consider polynomial p B , r N0 .deg(p) + r = 0 p = c R, IrB,A (p|) = c. Otherwise, deg(p) + r > 0:{deg(p)+rBp(|+B > 0B)IrB,A (p|) =0otherwise.31fiDe Cooman, De Bock, & Dinizmaps IrB,A allow us give following elegant characterisation specificity:Theorem 11. coherent inference system specific category setsB B A, p V (B), NA {0} r N0 :IrB,A (p)BA, (A) pBB,rB () (B).(SP3)Running Example. Suppose, before, made observation (H , U , H , ),count vector = (2, 1, 1). interested posterior lower probability, somebody told us neither two subsequent coin flipsafterevent HTfirst fourresulted U . specific inference system, allowed considerpredictive inference problem reduced category space B = {H , }, rathercategory space = {H , , U }. then, B-space, use reduced countvector rB () = (2, 1), obtained leaving number observed U s. polynomialslead consider here, therefore type [2H ]32H , wantknow whether belong ({F , U }).A-space, polynomial p() = 2H , whose degree deg(p) = 2,transformed polynomials][HrIB,A (p|) = 2(H + )2+r = [2H (H + )2 ](H + )rH + H +r N0 . follows argumentation proof Theorem 11 originalproblem requires us check whether polynomials type2[2H (H + )2 ](H + )r 12HU({H , , U }). Specificity allows us look problem B-space,easier.Observe close formal similarity conditions (RI3) (SP3).therefore surprise us specificity, too, preserved taking arbitrary non-emptyinfima inference systems.Theorem 12. Consider non-empty family , specific coherent inference systems.infimum inf iI specific coherent inference system well.Let us denote Crs set coherent inference systems representationinsensitive specific. follows Theorems 8 12 Crs , like C, closedarbitrary non-empty infima, perform conservative reasoning, muchway discussed near end Section 6.8. Immediate Predictioninference system , look special case immediate prediction,given category set A, observing sample n 0 variables count vectorNAn , want express beliefs value next observation Xn+1assume A. specific case predictive inference n = 1, Condition (18)NAn :simplified somewhat, gambles f11BA,f DASA (f ) (A) f DAcSA (f ) (A),32fiCoherent Predictive Inference Exchangeabilityletso-called sampling expectation SA (f ) linear polynomial givenSA (f |) := xA f (x)x .reason NA1 = {x : x A} x count vector correspondingsingle observation category x, words, exz = xz z [Kroneckerdelta]. Hence, x :( )( )11xxf(z)=f(x)B()=zez = x ,Hy1A (f |x ) =A,xxxzAz[ ]leading to:Mn1A (f |) =Hy1A (f |x )BA,x () =1x NAf (x)x = SA (f |).(30)xAmatter straightforward verification that, due Bernstein coherence HA ,1 ccoherent set desirable gambles A,so-called immediate prediction model DANA {0}. induces following predictive lower previsions:every count vector{}1= sup R : f DAP 1A (f |)c= sup { R : [SA (f ) ]BA,(A)} .(31)Immediate prediction context exchangeable imprecise probability modelsstudied detail De Cooman et al. (2009a). Lower previsions, rathersets desirable gambles, model choice paper, that,authors encountered problems conditioning sets (lower) probability zero. fact,problems provided motivation dealing much generalproblem (not necessarily immediate) predictive inference using sets desirable gamblespresent paper. section, want illustrate many results provedmade stronger (and easier proofs, borne Appendix E.3)present context.requirement (RI2) representation insensitivity reduces following simplerrequirement immediate prediction models: category setsonto map : D, gambles f NA {0}:11f DAc f DDcR ().(RI4)Similarly, requirement (SP2) specificity reduces following simpler requirementimmediate prediction models: category sets B B A,gambles f B NA {0}:11f IB DAc f DBcrB ().(SP4)Let us show simple characterisation immediate predictionmodels satisfy representation insensitivity. get there, observe considergamble g category set (surjective) pooling map finite subsetg(A) Ralso category set. corresponding Rg : RA Rg(A) given by:Rg ()r =x r g(A).xA : g(x)=r33fiDe Cooman, De Bock, & Dinizsimple idea allows intriguing reformulation representation insensitivityrequirement immediate prediction models:Proposition 13. immediate prediction models associated coherent inferencesystem representation insensitive category sets A, gambles gcount vectors NA {0}:11g DAc idg(A) Dg(A)cRg ().(RI5)Here, non-empty set B, denote idB identity map B, defined idB (z) := zz B.Proposition 13 tells us whether gamble desirable depends valuesassumesand assumedand number timesvalues observed pastor rather would observingg(Xk ) rather Xk .Let us focus happens events. Consider event B nontrivial meaning B neither empty equal A. real gambleIB assumes two values, 1 , see applying Proposition 13NA {0}:11IB DAc id{1,} D{1,}c(mB , mA\B ),therefore{}1P 1A (B|) = sup R : IB DAc{}1= sup R : id{1,} D{1,}c(mB , mA\B ) =: (mA , mB ),(32)(33)meaning that, representation insensitivity, predictive lower probability non-trivialevent B depends number times mB observed pastexperiments, total number observations . thing holds predictiveupper probability 1 (mA , mB ). precise predictive probabilities, similar propertyknown Johnsons sufficientness postulate (Johnson, 1924; Zabell, 1982).representation insensitive{coherent inference system,see define}2so-called lower probability function : (n, k) N0 : k n [0, 1] Equation (33),completely characterises one-step-ahead predictive lower upper probabilities29non-trivial events count vectors. shall use representation insensitivityspecificity requirements try say lower probability function.following theorem strengthens, simplifies, extends similar results De Cooman et al.(2009a).Theorem 14. Consider representation insensitive coherent inference system .associated lower probability function following properties:L1. bounded: 0 (n, k) 1 n, k N0 k n.L2. super-additive second argument: (n, k + `) (n, k) + (n, `)n, k, ` N0 k + ` n.29. . . . necessarily predictive lower upper previsions . . .34fiCoherent Predictive Inference ExchangeabilityL3. (n, 0) = 0 n N0 .L4. (n, k) k(n, 1) n(n, 1) 1 n, k N0 1 k n.L5. non-decreasing second argument: (n, k) (n, `) n, k, ` N0k ` n.L6. (n, k) (n + 1, k) + (n, k)[(n + 1, k + 1) (n + 1, k)] n, k N0k n.L7. non-increasing first argument: (n + 1, k) (n, k) n, k N0k n.L8. Suppose (n, 1) > 0 n N, let sn :=1. sn 0 sn+1 sn .(n, 1) = n+sn1(n,1)n, equivalently,moreover specific, following properties:nL9. Consider real (0, 1) suppose (1, 1) , (n, n) 1+n1n N0 . consequence, consider > 0 suppose (1, 1) 1+s ,n(n, n) n+sn N0 .know Theorem 4 representation insensitive coherent inference systemsnear-ignorant, meaning vacuous therefore completely indecisivesingle observation prior observations made. also borneTheorem 14.L3. Let us define imprecision function(n, k) := 1 (n, n k) (n, k) n, k N0 k n.(34)1clear P (B|) P 1A (B|) = (mA , mB ) width probability intervalevent B observed mB times. representationinsensitive coherent inference system whose imprecision function (n, k) satisfies followingproperty:}(n + 1, k) (n, k)0 k n,(35)(n + 1, k + 1) (n, k)imprecision increase total number observations increases. suggestsrepresentation insensitive coherent inference systems displaydesirable behaviour mentioned Introduction: conservative littlelearned, never become less precise observations come in. followingsections, intendamongst thingsto take closer look whether behaviourpresent number systems.Immediate prediction important predictive inference precise probabilities,Law Total Probability guarantees completely determined immediatepredictions. Perhaps surprisingly, case predictive inference impreciseprobabilities: Appendix provides counterexample. also pointslimitations scope earlier work De Cooman et al. (2009a). reason,leave immediate prediction models are, rest paper concentrategeneral notion inference system.35fiDe Cooman, De Bock, & Diniz9. Vacuous Inference Systemfollowing sections, provide explicit interesting examples representation insensitive, specific coherent inference systems. begin simplestone: vacuous inference system V , introduced Section 6 smallest,conservative, coherent inference system. associates category setsmallest Bernstein coherent set V (A) = HV,A := V + (A) containing Bernstein positivepolynomialsthe ones guaranteed anyway, Bernstein coherence alone.deduce Proposition 30 Appendix B that:= HV,A = V + (A)NA {0},HV,A cProposition 28 Appendix B that:{}= H V,A (p) = sup R : p V + (A)H V,A (p|)= min p = min p() p V (A).predictive models inference system straightforward find,NA {0},follow directly Equations (21) (23). n Ndeduce that:{}nn= f L(An ) : MnnA (f ) V + (A) ,DV,A= DV,Ac(36)= min MnnA (f |) f L(An ).P nV,A (f ) = P nV,A (f |)(37)particular:11= L>0 (A),DV,A= DV,AcP 1V,A (f )=P 1V,A (f |)(38)= min f f L(A),(39)V (n, k) = 0 n, k N0 k n.(40)conservative exchangeable predictive models are, arisemaking assessments exchangeability alone. gather Equations (36)(40), interesting, involve non-trivial commitments,allow learning observations. also borne correspondingimprecision function, given by:V (n, k) = 1 n, k N0 k n.Running Example. seen Mnn{H ,T } (IHT|) = 2H n 2,thereforenP nV,{H ,T } (IHT) = P V,{H ,T } (IHT|(3, 1)) =minMnn{H ,T } (IHT|) =maxMnn{H ,T } (IHT|) ={H ,T }min{H ,T }2H = 0nnP V,{H ,T } (IHT) = P V,{H ,T } (IHT|(3, 1)) ={H ,T }3612H = .{H ,T }2maxfiCoherent Predictive Inference Exchangeabilityshows vacuous inference model produce completely vacuous inferences:allows us find consequences making assessments exchangeability.allow us change lower upper probabilities previsionsnew observations come in.Even though makes non-trivial inferences, vacuous inference system satisfiesrepresentation insensitivity, specific.Theorem 15. vacuous inference system V coherent representation insensitive.Let us show means counterexample V specific,Running Example. Let us go back inferences category space = {H , , U }reduced category space B = {H , }. Consider polynomial p() = 2H H + 2T{H ,T } . polynomial Bernstein positiveso p V + ({H , })becausep() = (2H H + 2T )(H + ) = 3H + 3Texpansion Bernstein basis degree 3 positive. let us considercorresponding polynomial {H ,T ,U } :22q() := I0B,A (p|) = HH +.(41)polynomial Bernstein positive: easy see every n N0 ,22q() = (HH +)(H + + U )nn . q = I0 (p)always term H U/ V + ({H , , U }), inferB,ATheorem 11 V cannot specific.following sections, shall prove infinity committal,specific representation insensitive coherent inference systems. begin introducingslightly modified version vacuous inference system coherent, representationinsensitive specific.10. Nearly Vacuous Inference SystemLet us introduce nearly vacuous inference system NV reason namebecome clear presentlyby:NV (A) := HNV,A := V ++ (A) := {p V (A) : ( int(A ))p() > 0}category sets A.Since V ++ (A) consists polynomials positive int(A ), deduceNA {0}: HNV,A c= HNV,A = V ++ (A)Proposition 28 Appendix B that,that:= H NV,A (p) =H NV,A (p|)p() = min p() p V (A).infint(A )37fiDe Cooman, De Bock, & DinizSince know Proposition 28 Appendix B, counterexample following it,generally speaking V + (A) V ++ (A), see inference system less conservativevacuous one. case vacuous inference system, predictive modelsnearly vacuous inference system straightforward find, follow directlyNA {0}, deduce that:Equations (21) (23). n N{}nn= f L(An ) : MnnA (f ) V ++ (A) ,DNV,A= DNV,Ac= min MnnA (f |) f L(An ).P nNV,A (f ) = P nNV,A (f |)particular:11= L>0 (A),DNV,A= DNV,Ac= min f f L(A).P 1NV,A (f ) = P 1NV,A (f |)see immediate prediction models, predictive lower previsions,inference system exactly ones vacuous inference systems.30allow learning observations.Interestingly, contrast vacuous inference system, nearly vacuousinference system specific, already tells us Crs 6= .Theorem 16. nearly vacuous inference system NV coherent, representation insensitive specific: NV Crs .11. Skeptically Cautious Inference Systemconstruct rather simple inference system quite intuitive slightlyinformative vacuous nearly vacuous ones. Suppose subject usesfollowing system making inferences based sequence n > 0 observations countcategory set A. skeptical believes future,vector ,observe categories seen previously, categories set::= {x : mx > 0} .A[](42)also cautious, beliefs already observed categoriesobserved future, nearly vacuous. explain this, assume firstparticular, n future observations, vacuous beliefs count vectorobserve set{}nnNA : (y \ A[])my = 0 = NA[]holds possible observing count vector ,future count vectors31 Lemma 47 Appendix B,namely count vectors observation outside A[].30. first example shows immediate prediction models completely determineinference system. shall come across another example Appendix D.31. last equality equation actually device allows us identify count vectorszero, count vectors A[].shall using repeatedly,whose components outside A[]without explicit mention, rest paper.38fiCoherent Predictive Inference Exchangeabilitywould lead us associate following set polynomials count vector NA :{}+nV[](A) := p V (A) : (n deg(p)) bnp |NA[]>0{}+= p V (A) : p|A[] V (A[]) .But, already know vacuous models V + (A) lead specific systems,whereas nearly vacuous models V ++ (A) do, modify slightly, ratherassociate following set polynomials count vector NA :{}++V[](A) := p V (A) : p|A[] V ++ (A[]) .++polynomials V[](A) desirable representation32 observing samplecount vector , infer Equation (19) subject considers desirablerepresentation polynomials in:{}++++V[](A)BA, = pBA, : p V[](A) .thus led consider following assessment:++ASC,A :=V[](A)BA, ,NAset positive linear combinations:HSC,A:= posi (ASC,A ) ={`pk BA,k : ` N, nk N, kNAnk , pk}++V[(A)k]. (43)k=1following proposition guarantees sets HSC,A appropriate conservativemodels summarise exchangeable inferences skeptically cautious subject.Proposition 17. HSC,A smallest Bernstein coherent set polynomialsincludes ASC,A .also shows inference system SC , defined SC (A) := HSC,A categorysets A, coherent. shall call skeptically cautious inference system.want find updating works system. end, introduceslight generalisation set defined Equation (43). Consider NA {0}, letHSC,A, :={`pk BA,k : ` N, nk N0 , + nk > 0, kNAnk , pk}++V[+(A)k],k=1(44)see that, particular, HSC,A = HSC,A, = 0.sets HSC,A, following interesting characterisation:32. stated before, polynomials direct behavioural indirect representational meaning,conveniently condensed representations desirable gambles observation sequences. Hencecaution using term desirable representation.39fiDe Cooman, De Bock, & DinizProposition 18. NA {0}:{}HSC,A, = p V (A) \ {0} : (K min SA, (p))p|K V ++ (K) ,(45){}SA, (p) := =6 K : A[] K p|K 6= 0 .(46)min SA, (p) mean set minimal, non-dominating, elements SA, (p),min SA, (p) := {C SA, (p) : (K SA, (p))(K C K = {C)}. formally extend}Equation (42) include case = 0, A[0] = SA,0 (p) = =6 K : p|K 6= 0 .Proposition 19. NA {0}: HSC,A c = HSC,A, .combining result Equation (21), deriveadmittedly rather involvedexpressions predictive sets desirable gambles skeptically cautious inferenceNA {0}:system. n N{}n= f L(An ) : MnnA (f ) HSC,A,c(47)DSC,A.NA :immediate prediction, expressions simplify significantly.{}11= f L(A) : f |A[]DSC,A= L>0 (A) DSC,Ac> 0 L>0 (A).(48)NA :lower previsions derived HSC,A,tractable.=H SC,A (p) = min p(x ) H SC,A (p|)xAmin p() p V (A),A[](49)where, x A, x degenerate probability mass function assignsprobability mass x.predictive lower previsions skeptically cautious inference systemNA :easily obtained combining Equations (49) (23). n N=P nSC,A (f |)min MnnA (f |) f L(An )A[](50)P nSC,A (f ) = min f (x, x, . . . , x) f L(An ).(51)= min f (x) f L(A).P 1SC,A (f ) = min f P 1SC,A (f |)(52)xAparticular:xA[]lower probability function given by:{1 k = n > 0SC (n, k) =0 otherwisen, k N0 k n,corresponding imprecision function by:{1 n = 0 0 < k < nSC (n, k) =0 otherwise40n, k N0 k n.fiCoherent Predictive Inference ExchangeabilityRunning Example. before, Mnn{H ,T } (IHT|) = 2H n 2. also takeaccount {H , }[(3, 1)] = {H , }, get:nP nSC,{H ,T } (IHT) = P SC,{H ,T } (IHT|(3, 1)) =minMnn{H ,T } (IHT|) ={H ,T }maxMnn{H ,T } (IHT|) ={H ,T }{H ,T }min2H = 0max12H = .2nnP SC,{H ,T } (IHT) = P SC,{H ,T } (IHT|(3, 1)) ={H ,T }categories observed count vector (3, 1)meaning {H , }[(3, 1)] ={H , }we find inferences vacuous inference system.Interestingly, coherent inference system SC also satisfies representation insensitivity specificity.Theorem 20. skeptically cautious inference system SC coherent, representationinsensitive specific: SC Crs .12. IDMM Inference SystemsImprecise Dirichlet Modelsor IDMs, shortare family parametric inference modelsintroduced Walley (1996) conveniently chosen sets Dirichlet densities diA (|)constant prior weight s:{}{diA (|) : KsA } , KsA := RA(53)>0 : = = {s : int(A )} ,value (so-called) hyperparameter R>0 category set A. Dirichletdensities diA (|) defined int(A ); see Appendix C explicit definitionextensive discussion.IDMs generalise Imprecise Beta models introduced earlier Walley (1991).later paper, Walley Bernard (1999) focussed closely related family predictiveinference models, called Imprecise Dirichlet Multinomial Modelsor IDMMs,short.33 refer papers, recent overview paper Bernard(2005) extensive motivating discussion IDM(M)s, inferences properties.precise Dirichlet models expectations, related Dirichlet multinomialmodels, gathered Appendix C important facts, properties results,necessary proper understanding present discussion IDM(M)s contextinference systems.One reasons Walley (1996) suggesting IDM reasonable modelprecisely satisfies pooling34 invariance properties discussed Section 4.1.also discussed emphasis Walley Bernard (1999) Bernard (2005),know detailed explicit formulations properties literature,proofs seen fairly sketchy. Bernard (1997, 2005) also suggests IDM33. later paper, Walley Bernard (1999) clearly distinguish name parametric IDMspredictive IDMMs, earlier paper Walley (1996), types models referredIDMs.34. Walley uses term representation invariance rather pooling invariance.41fiDe Cooman, De Bock, & Dinizunderlying precise Dirichlet models satisfy so-called specificity property,tried translate present context predictive inference Section 4.5.present section, use ideas behind Walley Bernards IDM(M)s constructinteresting family coherent inference systems, give detailed formal proofAppendix E fact inference systems indeed representation insensitivespecific. Interestingly, shall need slightly modified version Walleys IDM(M)make things work. reason Walleys original version, describedExpression (53), number less desirable properties, seem eitherunknown to, ignored by, Walley Bernard. describe shortcomingsdetail Appendix D. present purposes, suffices mention that, contraryoften claimed, contradistinction new version, inferences using originalversion IDM(M) necessarily become conservative (or less committal)hyperparameter increases.version, rather using hyperparameter sets KsA , consider sets{}sA := RA>0 : < R>0 .Observe{}sA = s0 : s0 R>0 , s0 < int(A ) =0KsA .0<s0 <sR>0 , category set A, consider following set polynomialsp, positive Dirichlet expectation DiA (p|) hyperparameters sA ::= {p V (A) : ( sA ) DiA (p|) > 0} .HIDM,Ashall see Theorem 21 set Bernstein coherent. call inferencesystem sIDM , defined by:sIDM (A) := HIDM,Acategory sets A,IDMM inference system hyperparameter > 0. corresponding updated modelsNA {0}, given by:are,= {p V (A) : ( sA ) DiA (p|+ ) > 0}HIDM,Ac(54)= inf DiA (p|+ ) p V (A).H sIDM,A (p|)(55)Using expressions, predictive models IDMM inference system straightforward find; suffices apply Equations (21) (23). n NNA {0}:{}s,n= f L(An ) : ( sA ) DiA (MnnA (f )|+ ) > 0 ,DIDM,Ac(56)= inf DiA (MnnA (f )|+ ) f L(An ),P s,nIDM,A (f |)42(57)fiCoherent Predictive Inference Exchangeabilitywhere, using notations introduced Appendix C:+ ) = DiMnnA (HynA (f )|+ )DiA (MnnA (f )|( )n1n=HyA (f |)(mx + x )(mx ) .(n)(mA + )nxAN(58)general, expressions seem forbidding, immediate prediction modelsNA {0}:manageable enough.{}1s,1= f L(A) : f >(59)DIDM,A cf (x)mx ,xA1P s,1(f|)=f (x)mx +min f f L(A),(60)IDM,A++xAkn, k N0 k n.n+scorresponding imprecision function given by:sIDM (n, k) =sIDM (n, k) =n, k N0 k n,n+sdecreasing first constant second argument, impliessatisfies Condition (35). suggests IDMM inference systems conservativelittle learned, become precise observations come in.Running Example. before, Mnn{H ,T } (IHT|) = 2H n 2, find that,using results Appendix C:)(Di{H ,T } Mnn{H ,T } (IHT|) =2H.(H + )(H + + 1)difficult verify using Equation (57) 0 < s:s,nP s,n) = 0 P IDM,{H ,T } (IHT) =IDM,{H ,T } (IHT1.21+sobserving count vector (3, 1), find manipulations that:2(3 + )2(3 + s)=,0<<s (4 + )(5 + )(4 + s)(5 + s)P s,n|(3, 1)) = infIDM,{H ,T } (IHTsimilarly:61+s(4 + s)(5 + s)s,nP IDM,{H ,T } (IHT|(3, 1)) =14+s25+s22.Observe infinitely large s, recover inferences vacuous system.43fiDe Cooman, De Bock, & DinizInterestingly, immediate prediction models version IDMM inferencesystem coincide Walleys original version. Hence, many practical applications concerned immediate prediction only, approaches yield identicalresults.IDMM inference systems constitute uncountably infinite family coherentinference systems, satisfies representation insensitivity specificityrequirements.Theorem 21. R>0 , IDMM inference system sIDM coherent, representationinsensitive specific: sIDM Crs .Since Crs closed non-empty infima, infimumIDM IDM , > 0still coherent, representation insensitive specific, conservativeIDMM inference systems. given by:{}+++(A) := p V (A) : ( RAIDM (A) = V>0 ) DiA (p|) > 0 ,although set generally strictly includes sets V + (A) V ++ (A), associatedimmediate prediction models predictive lower previsions shown coincideones vacuous nearly vacuous inference systems.13. Skeptical IDMM Inference Systemscombine ideas previous two sections: suppose subject usesfollowing system making inferences based sequence n > 0 observationscategory set A. Section 11, skepticalcount vector ,believes future, observe categories seen previously,rather cautious completely vacuouscategories set A[].beliefs already observed categories observed future,uses IDMM-like inference them, described Section 12.turns done quite simply replacing, characterisation (45)sets HSC,A, skeptically cautious inference system, nearly vacuous modelsV ++ (K) appropriate IDMM models HIDM,KcrK (). define, categoryset A, NA {0} R>0 , following set polynomials:{}:= p V (A) \ {0} : (K min SA, (p))p|K HIDM,KHSI,A,crK () ,(61)recall K min SA, (p), A[] K therefore K[rK ()] =A[] K = A[], rK () essentially count vectors. also let:= HSI,A,HSI,A= 0, words:}{:= p V (A) \ {0} : (K min SA,0 (p))p|K HIDM,KHSI,A,{}where, again, SA,0 (p) = =6 K : p|K 6= 0 . remainder section, showsets polynomials HSI,Aindeed lead definition reasonable potentiallyuseful type inference system. begin coherence.Proposition 22. HSI,ABernstein coherent set polynomials .44fiCoherent Predictive Inference Exchangeabilityshows inference system sSI , given sSI (A) := HSI,Acategory sets A,coherent. call SI skeptical IDMM inference system hyperparameter s.want find updating works inference system. followingproposition really come surprise.Proposition 23. NA {0}: HSI,Ac = HSI,A,.combining Equation (21), obtain followingagain, rather involvedpredictive sets desirable gambles skeptical IDMM inference systems. n NNA {0}:{}s,n= f L(An ) : MnnA (f ) HSI,A,DSI,Ac(62).rather abstract, caseAlthough expressions HSI,AcNA :corresponding lower previsions.H sSI,A (p) = min p(x ) p V (A)xA(63)=H sSI,A (p|)infsA[]+ )DiA[]|rA[](p|A[]()p V (A).= H sIDM,A[]|rA[]())(p|A[](64)(65)Combining Equation (23), immediately obtain following predictive lowerNA :previsions skeptical IDMM inference systems. n NnP s,nSI,A (f ) = min f (x, x, . . . , x) f L(A )xA=P s,nSI,A (f |)infsA[]n+ )DiA[](MnA[]()(f |A[]n )|rA[]f L(An ).= P s,n())n |rA[](f |A[]IDM,A[](66)immediate prediction models skeptical IDMM inference systems surprisinglymanageable:s,1DSI,A= L>0 (A) P s,1SI,A (f ) = min f f L(A)NA :and,{}1s,1= f L(A) : f |A[]DSI,A cf (x)mx L>0 (A)>(67)xA[]1s,1=(f |)f (x)mx +min f (x) f L(A).P SI,A++ xA[]xA[]45(68)fiDe Cooman, De Bock, & Dinizlower probability function given by:{kk < n n = 0SI (n, k) = n+s1k = n > 0corresponding imprecision function by:{n = 0 0 < k < nSI (n, k) = n+s0otherwisen, k N0 k n,n, k N0 k n.consider case n > 0, see sSI (n, n) = 0 sSI (n + 1, n) =imprecision function satisfy Condition (35).n+1+s> 0,Running Example. {H , }[(3, 1)] = {H , }, infer Equation (66)IDMM inference systems.inferences event HTcoherent inference systems sSI also satisfy representation insensitivityspecificity.Theorem 24. R>0 , corresponding skeptical IDMM inference systemcoherent, representation insensitive, specific: sSI Crs .Since Crs closed non-empty infima, infimumSI SI , > 0 stillcoherent, representation insensitive specific, conservativeskeptical IDMM inference systems. shown associated immediate predictionmodels predictive lower previsions coincide ones skeptically cautiousinference system.14. Haldane Inference Systemalready know discussion near-ignorance following Theorem 4 representation insensitive coherent inference system fully precise, immediate predictionmodels observations made, must completely vacuous. askwhether representation insensitive (and specific) inference systems whoseposterior predictive lower previsions become precise (linear) previsions. problemaddress section. shall first construct inference system, showsystem is, definite sense, unique linear posterior predictive previsions.use family IDMM inference systems sIDM , R>0 , define inferencesystem H committal them:HIDM,A=sIDM (A) category sets A.H (A) = HH,A :=sR>0sR>0call H Haldane inference system, reasons become clearsection.Theorem 25. Haldane inference system H coherent, representation insensitivespecific: H Crs .46fiCoherent Predictive Inference ExchangeabilityDue representation insensitivity, Haldane system satisfies prior near-ignorance.implies making observation, immediate prediction model vacuous,far away precise probability model possible. showthat, making even single observation, inferences become precise-probabilistic:coincide inferences generated Haldane (improper) prior.get there, first take look models involving sets desirable gambles.NA {0}:= {p V (A) : (s R>0 )( sA ) DiA (p|+ ) > 0} =HH,A cHIDM,Ac.sR>0(69)corresponding predictive models easily derived applying Equation (21).NA {0}:n N{}n= f L(An ) : (s R>0 )( sA ) DiA (MnnA (f )|+ ) > 0DH,Acs,n=DIDM,Ac.(70)sR>0immediate prediction models obtained combining Equations (70) (59).NA :{}11= f L(A) :DH,A = L>0 (A) DH,A cf (x)mx > 0 L>0 (A).xAturns expressions corresponding lower previsions muchNA {0}:manageable. First all, find+ ) = lim H sIDM,A (p|)p V (A).inf DiA (p|= limH H,A (p|)s+0 sAs+0(71)= 0, simplifies to:particular,H H,A (p) = min p(x ) p V (A),xA(72)NA , find linear previsions:35whereas= H H,A (p|)= HH,A (p|)= DiA (p|)p V (A).H H,A (p|)(73)corresponding predictive models easily derived applying Equation (23).NA {0}:n N= limP nH,A (f |)+ ) = lim P s,nf L(An ).inf DiA (MnnA (f )|IDM,A (f |)s+0 sAs+0(74)= 0:particular,P nH,A (f ) = min f (x, x, . . . , x) f L(An ),xA35. Dirichlet expectations DiA (|) strictly speaking defined RA>0 , argueAppendix C, continuously extended components zero, others strictlypositive.47fiDe Cooman, De Bock, & DinizNA :P nH,A (f |)=nP H,A (f |)=nPH,A(f |)=nNA( )(nx )nxA mx.(n)HynA (f |)(75)NA :immediate prediction models, findmx1=P 1H,A (f ) = min f PH,A(f |)f (x)f L(A),xAlower probability function given by:{kn > 0H (n, k) = nn, k N0 k n.0n = 0corresponding imprecision function given by:{1 n = 0H (n, k) =n, k N0 k n,0 n > 0satisfies Condition (35), suggests also Haldane inference system displaysalbeit extreme interesting mannerthe desirable behaviour mentionedIntroduction: conservative little learned, never become lessprecise observations come in.Running Example. use Equation (74) results previously obtainedIDMM inference systems findnnP nH,{H ,T } (IHT) = 0 PH,{H ,T } (IHT|(3, 1)) =) = P H,{H ,T } (IHT3.10want point first equalities contradict prior near-ignoranceHaldane inference system, pertains immediate predictions: predictionssingle future observations.precise posterior predictive previsions Equation (75) exactly oneswould found formally apply Bayess rule multinomial likelihoodHaldanes improper prior (Haldane,1945; Jeffreys, 1998; Jaynes, 2003), whose densityfunction int(A ) proportional xA x1 . This, course, use Haldanes nameinference system produces them. argumentation shows nothingwrong posterior predictive previsions, based coherent inferences.fact, analysis shows infinity precise proper priors simplexthat, together multinomial likelihood, coherent posterior predictiveprevisions: every coherent prevision V (A) dominates coherent lower previsionH H,A V (A).36 binomial parametric inferences Haldane prior, Walley (1991,Section 7.4.8) comes related conclusion completely different manner.36. immediate consequence F. Riesz Representation Theorem coherent previsionrestriction polynomials expectation operator unique -additive probability measureBorel sets ; see instance discussion De Cooman Miranda (2008a) alsofootnote 2.48fiCoherent Predictive Inference Exchangeabilitysimple argument show Haldane posterior predictive previsionsprecise ones compatible representation insensitivity. Indeed,shown representation insensitive coherent inference system preciseposterior predictive previsions, lower probability function must satisfy (n, k) = k/nn > 0 0 k n,37 straightforward prove, using Bayess Theorem goimmediate prediction general predictive inference, posterior predictiveprevisions must Haldanes.15. Characterisation IDMM Immediate Predictionslower probability function (n, k) representation insensitive coherent inferencesystem gives lower probability observing non-trivial event observed ktimes n trials.suppose subject specifies single lower probability, namely value(1, 1) [0, 1]: probability observing something (again) observed (once)single trial. ask conservative consequencesassessment are, take representation insensitivity specificity granted.words, conservative representation insensitive specific coherent inferencesystem (at least) given value (1, 1) lower probability function?question makes sense representation insensitive specific coherent inferencesystems constitute complete meet-semilattice Statement (25) Theorems 8 12.38Clearly, (1, 1) = 0, smallest representation insensitive specific coherentinference system, know discussion Sections 9 10, mustimmediate prediction models predictive lower previsions (nearly) vacuousinference system. consider case 0 < (1, 1) < 1,39 words, useparametrisation turn convenient purposes, that:(1, 1) =11positive real number :=1.1+s(1, 1)(76)Let us denote conservative inference system , lower probability1function , assumption (1, 1) 1+s. follows Theorem 14.L9n(n, n) n+s n N0 . since IDMM inference system sIDM , Equation (60)ntells us sIDM (n, n) = n+s, since assumption sIDM (n, n) (n, n), concludethat:n(n, n) = sIDM (n, n) =n N0 .(77)n+ssurmised (Bernard, 2007; De Cooman et al., 2009a) IDMM inferencesystem hyperparameter could smallest, conservative, representation1insensitive specific coherent inference system given value (1, 1) = 1+s.fact, trying prove made us start research present paper.conjecture turns false: apart lower bound (77) (n, n),37. suffices exploit additivity precise probabilities symmetry implied representationinsensitivity; explicit proof, see paper De Cooman et al. (2009a, Thm. 7).38. See discussion near end Section 7.39. surmise, prove here, conservative representation insensitive specificcoherent inference system corresponding (1, 1) = 1 might skeptically cautious one.49fiDe Cooman, De Bock, & Dinizrepresentation insensitivity specificity impose lower bounds (n, k) k < n.see this, consider inference system sMC := inf{SC , sIDM }, Statement (25)Theorems 8, 12, 20 21 coherent, representation insensitive specific: sMC Crs .lower probability function sMC satisfies:{nnmin{1, n+s} = n+sk = n > 0sMC (n, k) = min{SC (n, k), sIDM (n, k)} =kmin{0, n+s } = 0otherwise,substantiating claim made above. See also Figure 1, depicted lower (andupper) probability functions Haldane system H , IDMM system sIDM , sMCninference system inf{4sSI , IDM }. latter three share value n+s (n, n),n 0. conjecture sMC could smallest, conservative, representation1insensitive specific coherent inference system given value (1, 1) = 1+s, offerproof this.(n, k)1nn+sn+s0012...n1nkFigure 1: Lower upper probability functions: H Haldane system (dark grey,4), sIDM IDMM system hyperparameter (blue, ?), min{4sSI , IDM }(orange, ) sMC = min{SC , sIDM } (red, ). specific plot maden = 10 = 2.means want characterise IDMM inference systems wayconservative ones, need add, besides coherence, representation insensitivityspecificity, another requirement preserved taking infima. One possiblecandidate this, shall prove job inspired Figure 1,following requirement.Let us define subjects surprise event supremum rate bettingopposite event, words, lower probability opposite event. surprisehighclose onewhen subject believes strongly event occur,lowclose zerowhen subject strong beliefs occur.50fiCoherent Predictive Inference Exchangeabilityallows us associate so-called surprise function (n, k) := (n, n k)lower probability function, (n, k) subjects surprise observing non-trivialevent observed k n times before.follows Theorem 14.L5 representation insensitive system, surprisefunction non-increasing second argument:(n, k) := (n, k + 1) (n, k) = (n, n k 1) (n, n k) 0 0 k n 1.fairly intuitive property: often event observed before,smaller surprise seeing again.shall say representation insensitive system concave surprise2 (n, k) := (n, k + 1) (n, k) 0 0 k n 2,where, course, 2 (n, k) = (n, n k 2) 2(n, n k 1) + (n, n k).difficult see concave surprise preserved taking non-empty infimainference systems, makes sense go looking smallest (most conservative)coherent representation insensitive specific coherent inference system concavesurprise, satisfies additional local assessments, (76).Looking Figure 1 makes us suspect IDMM inference system sIDM mightsystem, again, offer proof conjecture. however provide prooffollowing, related (probably) weaker, statement, focusses immediateprediction only:Theorem 26. immediate prediction models P 1A (|), NA {0} smallest(most conservative) coherent representation insensitive specific coherent inference systemconcave surprise satisfies (76), coincide ones IDMM inferencesystem sIDM hyperparameter s.16. Conclusionbelieve first paper tries deal systematic fashion principlespredictive inference exchangeability using imprecise probability models. Two salientfeatures approach (i) consistently use coherent sets desirable gamblesuncertainty models choice; (ii) notion inference system allows usderive conservative predictive inference method combining local predictive probabilityassessments general inference principles.first feature allows us, contradistinction approachesprobability theory, avoid problems determining unique conditional modelsunconditional ones conditioning events (lower) probability zero. setn cpolynomials HA completely determines prior posterior predictive models DAnneven (lower) prior probability P ([])= H (BA,P (|),) observingzero. approach using lower previsions probabilities would makecount vectormuch complicated involved, impossible. Interestingly, provideperfect illustration fact using results Sections 11, 13 14.40 three40. Something similarly dramatic happens Sections 9 10: inference systemsimmediate prediction models (predictive) lower previsions, one specificnot.51fiDe Cooman, De Bock, & Dinizinference systems described therethe skeptically cautious, skeptical IDMMHaldane systemshave, given category set A, three different sets polynomialsHA . Nevertheless, gather Equations (49), (63) (72),lower prevision H therefore prior predictive models P nA . count vectorNA prior lower probability:= H (BA,P nA ([])) = min BA,(x ) = 0.xAzero lower probability makes sure posterior lower previsions H (|)uniquely determined prior lower previsionposterior predictive models P nA (|)H : infer Equations (49), (65) (73) indeed differentthree types inference systems. fail see could come withlet aloneproved necessary results forthese three systems relying lower prevision credalset theory.canand musttake line argumentation even further. Theorem 4,inference system satisfies (prior) representation insensitivity near-vacuous priorpredictive models, therefore, time consistency coherence [monotonicity], seen= 0prior predictive lower previsions must satisfy H (BA,) = P ([])NA well. simply means impossible (prior) representation insensitivecoherent inference system lower prevision H uniquely determine conditionaltherefore systematic way dealing inferencelower previsions H (|).systems must able resolveor deal withthis non-unicity way. believeapproach involving coherent sets desirable gambles one mathematicallyelegant ways this.second feature allowed us, example, characterise IDMM immediatepredictions conservative ones satisfying number inference principles.approach follow canat least principlealso used types inferencesystems inference principles. key requirement inference principlemake amenable approach that, formulated property inferencesystem, preserved taking arbitrary non-empty infima. three inferenceprinciples considering aboverepresentation insensitivity, specificityconcave surprisehave property, nothing prevents analysisapproach extended inference principle too.complications see, point, technical mathematical nature. readerdoubt noticed proofs results later sections quite involvedtechnical, rely quite heavily properties polynomials simplex. feelpresent paper made headway mathematical territory, instancenew discussion Bernstein positivity polynomials near Proposition 28Appendix B. Conclusions paper De Cooman Quaeghebeur (2012),characterisation Bernstein positivity mentioned open problem interestingpractical applications inferencenatural extensionunder exchangeability.much remains open exploration, determined study mathematicalstructure properties polynomials would certainly help alleviating technicaldifficulties working inference principles inference systems.paper opened feel interesting lineresearch foundations predictive inference, nevertheless provided answers52fiCoherent Predictive Inference Exchangeabilitynumber ofif allopen problems formulated Conclusions earlier paperDe Cooman et al. (2009a), tried deal representation insensitivity immediateprediction. first example: asked whether representation insensitivecoherent inference systems whose lower probability functions additive secondargument? suffices look Figure 1 see answer is, clearly, yes. Anotherquestion was: representation insensitive coherent inference systemsmixing predictive systems?41 follows Equation (68) answer yes:skeptical IDMM inference systems provides example. Finally, use infimumsMC skeptically cautious inference system SC IDMM inference system sIDM ,mentioned briefly Section 15, answer two questions. representationinsensitive coherent inference systems inequality Theorem 14.L6 strict?representation insensitive coherent inference systems whose behaviourgambles completely determined lower probability function? inferencesystem sMC provides positive answer questions.inference systems mentioned above, apart IDMM Haldanesystems, appear first time. may appear contrived perhapseven artificial, found useful constructing (counter)examples,shaping intuition, building new models, Figure 1 argumentation clearlyindicate. might also wonder whether representation insensitive and/orspecific coherent inference systems, cannot produced appropriately chosen infimaexamples introduced here. suggest, candidates consideration,inference systems derived using Walleys (1997) bounded derivative model,inference systems constructed using sets infinitely divisible distributions,recently proposed Mangili Benavoli (2013). framework provided here, wellsimple characterisation results Theorems 7 11, quite useful addressingsimilar problems.end, want draw attention simple direct, quite appealing,consequence argumentation Section 14: infinity precise properpriors that, together multinomial likelihood, coherent Haldane posteriorpredictive previsions. So, need improper priors justify posteriors,proper priors job perfectly well. (precise-)probabilistic conclusionfollows easily looking problem using general powerful languageimprecise probabilities. Moreover, seen properties representationinsensitivity cannot satisfied precise probabilistic models. Finally, entire frameworkconservative predictive inference using inference principles would impossible developwithin limitative context precise probabilities. shows distinctadvantages using imprecise probability models dealing predictive inference.AcknowledgementsGert de Coomans research partially funded project number 3G012512Research Foundation Flanders (FWO). Jasper De Bock PhD Fellow Research41. Loosely speaking: cannot written (specific kind of) convex mixture Haldane inferencesystem IDMM inference system; see paper De Cooman et al. (2009a, Section 5)information.53fiDe Cooman, De Bock, & DinizFoundation Flanders wishes acknowledge financial support. Marcio Dinizsupported FAPESP (So Paulo Research Foundation), project 2012/14764-0wishes thank SYSTeMS Research Group Ghent University hospitalitysupport sabbatical visit there. authors would like thank three anonymousreviewers many insightful comments suggestions aimed making papereasier read cleaning misunderstandings. special thank also greatArthur Van Camp enthusiasm everything and, particular, helping us checklittle examples.Appendix A. Notationappendix, provide list commonly used important notation,defined first introduced.notationmeaningintroducedA, B, C,IBX, Xnnnposi(A)L(A)L>0 (A)L0 (A)nDAcategory sets, eventsindicator event Bvariable, variable time nnumber already observed variablesnumber observed variablescone generatedset gamblesset positive gamblesset non-positive gamblesobserved sampleobserved count vectorprior predictive set desirable gamblescategory set n future observationsposterior predictive set desirable gamblesprior predictive lower previsionposterior predictive lower previsionpooling map relabelling maprenaming bijectioncategory permutationsample observations outside B eliminatedcounting mapset count vectors n observationsset count vectors, zerohypergeometric expectation operatormultinomial coefficient count vectormultinomial expectation operatorsimplex probability mass functionssum components x x BBernstein basis polynomialset polynomials degree nSection 1Section 2.2Section 1Section 1Section 1Equation (1)Section 2.2Section 2.2Section 2.2Section 3Section 5.4Section 3n c,n cDADAnP ()P nA (|)P nA (|),$BNAnNA , NA {0}HynA (|)()MnnA (|)BBA,V n (A)54Section 3Section 3Section 3Sections 4.1&4.4Section 4.2Sections 4.3Section 4.5Equation (8)Equation (9)Section 5.3Equation (10)Equation (11)Equation (13)Equation (12)Equation (12)Equation (15)Section 5.3fiCoherent Predictive Inference ExchangeabilityV (A)V + (A)V ++ (A)HAHA cHAH (|)FCCrsRrBiAIrB,ASAsubscriptsubscriptsubscriptsubscriptsubscriptsubscriptsubscriptA[]++V[](A)VNVSCIDMSIHOIdiA (|)DiA (|)DiMnnA (|)bnpset polynomialsset Bernstein positive polynomialsset polynomialspositive int(A )representing set polynomialsupdated representing set polynomialslower prevision induced HAlower prevision induced HA cset category setsinference systemset coherent inference systemsset coherent inference systemsrepresentation insensitive specificextended relabelling maprestriction mapinjection mapextended injection mapsampling expectationlower probability functionimprecision functionsurprise functionrelated vacuous inference systemrelated nearly vacuous inference systemrelated skeptically cautious inference systemrelated IDMM inference systemsrelated skeptical IDMM inference systemsrelated Haldane inference systemrelated original IDMM inference systemscategories already observedset polynomialspositive int(A[] )Dirichlet densityDirichlet expectation operatorDirichlet multinomial expectation operatorexpansion polynomial pBernstein basis degree nSection 5.3Section 5.3Section 10Theorem 5Equation (19)Equation (17)Equation (20)Definition 6Definition 6Equation (24)Theorem 12Equation (26)Equation (27)Equation (28)Equation (29)Section 8Equation (33)Equation (34)Section 15Section 9Section 10Section 11Section 12Section 13Section 14AppendixEquation (42)Section 11AppendixAppendixAppendixAppendixCCCBAppendix B. Multivariate Bernstein Basis Polynomialsn 0 NAn correspondsBernstein basis polynomial(multivariate)x:=degree n , given BA, ()() xA x , . polynomialsnumber interesting properties (see instance Prautzsch, Boehm, & Paluszny, 2002,Chapters 10 11), list here:BB1. set {BA, :NAn } Bernstein basis polynomials fixed degree n linearlyindependent: N n BA, = 0, = 0 NAn .55fiDe Cooman, De Bock, & DinizNAn } Bernstein basis polynomials fixed degree n formsBB2. set {BA, :partition unity: N n BA, = 1.BB3. Bernstein basis polynomials non-negative, strictly positive interiorint(A ) .BB4. set {BA, : NAn } Bernstein basis polynomials fixed degree n formsbasis linear space polynomials whose degree n.Property BB4 follows BB1 BB2.42 follows BB4 that:BB5. polynomial p unique expansion terms Bernstein basis polynomialsalso called Bernstein expansionof fixed degree n deg(p),words, unique count gamble bnp NAn that:p() =bnp ()BA, () .(78)nNAtells us [also use BB2 BB3] p() convex combination Bernsteincoefficients bnp (), NAn whence:min bnp min p p() max p max bnp .(79)following proposition adds detail picture.Proposition 27. polynomial p :lim [min bnp , max bnp ] = [min p, max p] = p(A ).n+ndeg(p)Proof Proposition 27. Since bnp converges uniformly polynomial p n +(Trump & Prautzsch, 1996), sense( )lim maxn pbnp () = 0,n+ NAnndeg(p)findlimn+ndeg(p)min bnp min p =lim[]minn bnp () min pn+ NAndeg(p)[( )]minn bnp () pn+ NAnndeg(p)( )lim maxn pbnp () = 0,n+ NAnlimndeg(p)therefore limn+,ndeg(p) min bnp min p. Furthermore, Statement (79), seelimn+,ndeg(p) min bnp min p. Hence indeed limn+,ndeg(p) min bnp = min p. proofequality completely analogous.42. see how: clearly polynomials definition linear combinations Bernstein basis polynomials,possibly different degrees. terms, use BB2 raise degree common higherdegree nmultiply appropriate version 1. shows Bernstein basis polynomialsfixed degree n generating polynomials lower degrees. also independent BB1.56fiCoherent Predictive Inference ExchangeabilityUsing results, prove number useful relations Bernsteinpositivity polynomial positivity (the interior of) simplex. relatedproperty first proved Hausdorff univariate case (Hausdorff, 1923, p. 124).Proposition 28. Let p polynomial . Consider following statements:(i) ( )p() > 0;(ii) p V + (A), meaning n deg(p) bnp > 0;(iii) p V ++ (A), meaning ( int(A ))p() > 0;(iv) ( )p() 0.(i)(ii)(iii)(iv).Proof Proposition 28. first implication direct consequence Proposition 27:infer (i) continuity p min p > 0 therefore, Proposition 27,limn+,ndeg(p) min bnp = min p > 0, implies (ii).prove (ii)(iii), assume n deg(p) bnp > 0,consider int(A ). since BA, () > 0 NAn [BB3], sinceassumption bnp 0 bnp () > 0 NAn , seep() =bnp ()BA, () bnp ()BA, () > 0.nNAthird implication immediate consequence continuity p.following counterexample shows necessarily V + (A) = V ++ (A).Running Example. go back polynomial q {H ,T ,U } defined Equation (41):22q() = HH += (H )2 + H {H ,T ,U } .already argued polynomial Bernstein positive. Nevertheless,obviously positive interior {H ,T ,U } .also quite easy trace effect Bernstein expansion multiplyingBernstein basis polynomial:Proposition 29. polynomials p , natural n deg(p), NA {0}NAn+mA :()bn ( )pn+mA()()bpBA, () =0otherwise.Proof Proposition 29. Observe that:()npBA, =bp ()BA, BA, =bnp ()BA, BA,nNAnNA57fiDe Cooman, De Bock, & Diniz=bnp ()nNA( + )BA,+ ,()()use uniqueness (Bernstein) basis expansion.allows us prove following simple interesting result Bernstein positivity:Proposition 30. Consider NA {0} polynomial p . Then:pBA, V + (A) p V + (A).Proof Proposition 30. First, assume pBA, V + (A), natural nndeg(p) bn+mpBA, > 0. follows Proposition 29 also bp > 0,therefore p V + (A).Assume, conversely, p V + (A), n deg(p) bnp > 0.+follows Proposition 29 also bn+mpBA, > 0, therefore pBA, V (A).Appendix C. Dirichlet Distributiondensity diA (|) Dirichlet distribution hyperparameter RA>0 given by:diA (|) :=(A )xx 1 int(A ),()xxAxApolynomial p define corresponding expectation as:43(A )p()DiA (p|) :=xx 1 d.()xxAxAparticular,()(A )xx 1(x )xAxAxA( )( )n(A )(mx + x )1n=x (mx ) ,=(n + )(x )(n)DiA (BA, |) =nxmx(80)xAxAusing ascending factorial (r) := (+r)() = ( + 1) . . . ( + r 1), Rr N0 .Dirichlet distribution used prior combination multinomiallikelihood, leading so-called Dirichlet multinomial distribution, describedfollows. probability observing (a sample n 0 observations with) count vectorNA {0} multinomial process Dirichlet prior density diA (|) given by:nDiMnA ({}|) :=CoMnnA ({}|) diA (|)43. integrals section interpreted multiple Riemann integrals.58fiCoherent Predictive Inference ExchangeabilityBA, () diA (|) = DiA (BA, |),=second equality follows Equation (16). Therefore, generally, takeexpansion polynomial p Bernstein basis polynomials degree n deg(p):DiA (p|) =bnp () DiA (BA, |) =nNA= DiMnnAbnp () DiMnnA (I{} |)nNA(nNA)nbp ()I{} = DiMnnA (bnp |),Dirichlet multinomial expectation count gamble bnp . generaluseful relationship Dirichlet expectation polynomial p, Dirichletmultinomial expectation Bernstein expansion bnp . Although expectationsstrictly speaking defined RA>0 , extend definition continuouslyelements RA\{0}takingappropriatelimits, Equation (80) indicates.C.1 Special Properties Dirichlet Distributionrecall interesting properties Dirichlet distribution. beginupdating property:Proposition 31 (Updating). category set A, polynomial p V (A), countvector NA {0} RA>0 :DiA (pBA, |) = DiA (BA, |) DiA (p| + ).Proof Proposition 31.DiA (pBA, |) =p()BA, () diA (|)()mx (A )xx 1=p()x()xxAxAxA()(mx + x )(A )=p() diA (| + )(mA + )(x )xA= DiA (BA, |) DiA (p| + ),last equality follows Equation (80).Next, turn so-called renaming property:Proposition 32 (Renaming). category sets C bijective(one-to-one onto) map : C, polynomial p V (C) RA>0 :DiA (p R |) = DiC (p|R ()).59fiDe Cooman, De Bock, & DinizProof Proposition 32. Due linear nature Dirichlet expectation, clearlysuffices prove property Bernstein basis polynomials p = BC, ,NC {0}. Observe R bijection too. Then, using Equation (80), let := R ():= R1 (), z = 1 (z) mz = n1 (z) z C, = CnA = mC , get:()(mz + z )mC(C )DiC (BC, |R ()) = DiC (BC, |) =(mC + C )(z )zC( )(n1 (z) + 1 (z) )(A )nA=(nA + )(1 (z) )zC( )(nx + x )(A )nA== DiA (BA, |),(nA + )(x )xAtake account int(A ):(BC, R )() = BC, (R ())()()()mC mzmCmC n1 (z)mz=1 (z)1 (z) =R ()z =zCzCzC( )nAxnx = BA, (),=xAsee indeed DiC (BC, |R ()) = DiA (BC, R |).so-called pooling property generalises renaming property:Proposition 33 (Pooling). category sets ontomap : D, polynomial p V (D) RA>0 :DiA (p R |) = (p|R ()).Proof Proposition 33. Due linear nature Dirichlet expectation, sufficesprove property Bernstein basis polynomials p = BD, , ND {0}.Also, take account renaming property Proposition 32, enough considerfollowing special case, non-empty set different categories b,c belonging it, let := {b, c} := {d}, define letting(x) := x x (b) = (c) := d.one hand, taking account Equation (80), letting := R () :()(mz + z )mD(D )(BD, |R ()) = (BD, |) =(mD + )(z )zD()mD(D )(md + ) (mz + z )=.(81)(mD + ) (d )(z )zDohand,60fiCoherent Predictive Inference ExchangeabilityDiA (BD, R |)())(mDmdmz(b + c )zdiA (|)=zDo)((A )mD=(b + c )md bb 1 cc 1zmz +z 1xA (x )zDo())(md(A )mDbk+b 1 cmd k+c 1zmz +z 1=k()xxAzDok=0)()((A )md (k + b )(md k + c ) zDo (mz + z )mD=.k(mD + )xA (x )k=0So, compare results recall = , z = z z = b + c ,see must prove that:)md (1md(md + b + c )=(k + b )(md k + c )(b + c )(b )(c )kk=0equivalently, using ascending factorials:(b + c )(md ) =)md (mdb (k) c (md k) .k(82)k=0see proving pooling property essentially equivalent proving Equation (82),binomial theorem ascending factorials. well-known result,follows fact ascending factorials Sheffer sequences binomial type (Sheffer,1939). completeness, give proof here, easy,shown hold prove pooling property particular case= {a}, category different b, c d. = {a, b, c} = {a, d},case rewrite Equation (81) as:(BD, |R ())()+ md(a + b + c )(md + b + c ) (ma + )=(ma + md + + b + c ) (b + c )(a )whereasDiA (BD, R |) =let1 (:=01(10(1=1a)md bb 1 (1)md ama +a 1(()+ md (a + b + c )(a )(b )(c )1ab )c 1 ama +a 1 dbbb 1 (1b )c 1)da)db da( 1)1md +b +c 1 +a 1b 1c 1=(1 )(1 t)dt da000061fiDe Cooman, De Bock, & Diniz= B(ma + , md + b + c )B(b , c ) =(ma + )(md + b + c ) (b )(c ),(ma + md + + b + c ) (b + c )using well-known evaluation Beta function terms Gamma functions.Finally, look properties related restriction.Proposition 34 (Restriction). category sets B B A,polynomial p V (B), RA>0 r N0 :DiA (IrB,A (p)|) =(deg(p) + r + B )(A )DiB (p|rB ()).(deg(p) + r + )(B )Proof Proposition 34. Let n := deg(p) + r, due linearity Dirichletexpectation operator, Equations (29) (80):DiA (IrB,A (p)|) =bnp () DiA (BA,iA () |)nNB( )n(A )xB (nx + x )xA\B (x )=(n + )nxA\B (x )xB (x )NB( )n(A ) (nx + x )=bnp ()(n + )(x )nbnp ()NB=nNB=xBbnp ()(A ) (n + B )DiB (BB, |rB ())(n + ) (B )(A ) (n + B )DiB (p|rB ()),(n + ) (B )concluding proof.Appendix D. Original IDMM Inference System WalleyBernardIDMM inference system sIDM , introduced Section 12, differs oneoriginally proposed Walley Bernard (1999).44 appendix, discuss originalIDMM inference system, denote sOI , explain related ours,illustrate advantages version one Walley Bernard.D.1 Defining Original IDMM Inference SystemR>0 , category set A, consider following set polynomials::= {p V (A) : ( KsA ) DiA (p|) > 0}HOI,A= {p V (A) : ( int(A )) DiA (p|s) > 0} .44. Strictly speaking, Walley Bernard propose inference system sense, rathercollection prior posterior predictive lower previsions category set A. inference systemcall original IDMM inference system one produces predictive lower previsions.62fiCoherent Predictive Inference Exchangeabilityreasons become clear shortly, call inference system sOI definedsOI (A) := HOI,Acategory sets A,original IDMM inference system hyperparameter > 0. Updating done muchNA {0}:way inference system sIDM Section 12.= {p V (A) : ( int(A )) DiA (p|+ s) > 0} ,HOI,Accompared Equation (54). leave exercise readercheck sOI coherent representation insensitive.45 However, illustratedcounterexample Section D.3, sOI specific.predictive models sOI easily derived mimicking approach usedSection 12 derive predictive models sIDM ; see Equations (56) (57).NAn :n N0 , n N{}s,n= f L(An ) : ( int(A )) DiA (MnnA (f )|+ s) > 0 ,DOI,Ac(83)=P s,nOI,A (f |)(84)infint(A )+ s) gambles f .DiA (MnnA (f )|latter expression motivates refer sOI original IDMM inference system:predictive lower previsions coincide proposed Walley Bernard (1999).Using Equation (83) n = 1, mimicking argument proof Equation (59)Appendix E.7, sees,1=DOI,Ac{f L(A) : f >}1s,1NA {0}.f (x)mx = DIDM,AcxAtells us IDMM original IDMM immediate predictionmodels. corresponding immediate predictive lower previsions original IDMMwell-known course identical ones produced version IDMMinference system, given Equation (60). However, examples next sectionillustrate, equality extend beyond immediate prediction: IDMMoriginal IDMM different coherent inference systems, leads us generalimportant conclusion coherent inference systems completely determinedimmediate prediction models.Nevertheless, approaches closely related; comparing Equations (84) (57),NAnsee n N0 , n N0,n= inf0 P sOI,Agambles f .P s,n(f |)IDM,A (f |)0<s <s45. proof similar one sIDM [see Theorem 21].63(85)fiDe Cooman, De Bock, & DinizD.2 Original IDMM Inference System Monotonehyperparameter original IDMM inference system usually interpreteddegree caution. Higher values often claimed produce inferencescautious less informative. following quote Walley Bernard (1999,Section 2.4) makes explicit:B event concerning future observations, IDMM(s) produces intervalsposterior probabilities [P (B|), P (B|)] nested become widerincreases. means inferences produced two IDMMs differentvalues always consistent other, effect increasingsimply make inferences cautious less informative.Similar statements found related papers Walley (1996, Section 2.5) Bernard(2005, Section 4.6). Although indeed true many inferences, including many importantonesfor example, immediate predictions, hold event concerningfuture observations, illustrated following example, lower probabilityevent concerning two future observations shown initially increase s.Example 1. Consider situation possibility space consists two elementsonly, say heads (H) tails (T ), observed once, n = 2= (mH , mT ) = (1, 1). interested predictive lower probabilitynext two trials, heads tails observed once: n = 2 looking= {(H, ), (T, H)},= (mH , mT ) = (1, 1).predictive lower probability event []original IDMM inference system, following formula provides closed-formexpression:=P s,n|)OI,A (I[]+ s) = inf DiA (BA,+ s)DiA (MnnA (I[]|)|int(A )( )1n(mx + stx )(mx )= inf(n)int(A ) (n + s)xAinfint(A )= inf0<t<12(1 + st)(1 + s(1 t))2(1 + s)=.(2 + s)(3 + s)(2 + s)(3 + s)initially increases s; see also Figure 2.conclude P s,n|)OI,A (I[](86)version IDMM inference system, statement made aforementionedquote hold event concerning future observations. follows triviallyEquation (85). illustrate next example.Example 2. Consider problem Example 1. time, solve using versionIDMM. result also depicted Figure 2, function hyperparameter s.s,nP IDM,Anon-increasing function s. Indeed,contrast P s,n(I[]|),|)OI,A (I[]0 ,n1=(I[]0 < < 1P sOI,A|)slim0 03s,n=P IDM,A (I[]|)2(1 + s)P s,n=1|)OI,A (I[](2 + s)(3 + s)closed-form expression find combining Equations (85) (86).64fiCoherent Predictive Inference Exchangeability0.36P s,n|)OI,A (I[]0.340.32P s,n|)IDM,A (I[]0.30.2800.511.52Figure 2: Lower probability observing two different outcomes next two experiments, given possibility space consists two categories,already observed once: solutions according sOI (solid line) sIDM(dashed solid line); see Examples 1 2 information.Clearly, inferences sOI sIDM differ: suffices compare resultsExamples 1 2; see Figure 2 well. Therefore, seems clear Walleys (1996, p. 51)statement [. . . ] allowed vary 0 s, produces exactlyinferences IDM = s. equivalently, sOI sIDM produceinferences, taken apply immediate prediction only.D.3 Original IDMM Inference System Specificannounced Theorem 21, version IDMM inference system specific.show that, least values hyperparameter s, true originalversion.NBn . B f L(B n ):Consider n N0 , n Ns,n( int(A )) DiA (MnnA (f IB n )|iA ()+ s) > 0f IB n DOI,AciA ()+ srB ()) > 0,( int(A )) DiB (MnnB (f )|last equivalence consequence Propositions 41 34 fact= .particular B A, hard see sB = {srB () : int(A )},rB (iA ())implies that:s,ns,n( sB ) DiB (MnnB (f )|+ ) > 0 f DIDM,Bf IB n DOI,AciA ()c,therefore also:B n ) = inf DiB (MnnB (f )|+ ) = P s,nP s,nOI,A (f |iA (),IDM,B (f |).B65fiDe Cooman, De Bock, & Dinizhand, due Equation (SP1), sOI specific, would that:B n ) = P s,n= P s,nP s,nOI,A (f |iA (),OI,B (f |rB (iA ()))OI,B (f |).P s,nHence, order sOI specific, necessary P s,nOI,B (|)IDM,B (|)coincide. illustrated examples previous section, necessarilycase. Therefore, sOI always specific. counterexample provided,difference occurs < 1 only, whereas practice, usually chosen either 12 (Walley & Bernard, 1999, Section 2.4). would interesting see whether similarcounterexamples constructed 1.original IDMM inference systems specific, apparently contradicts Theorem 11 De Cooman et al. (2009a), seems state are. fact,theorem states original IDMM immediate prediction models satisfy weakerspecificity condition, tailored immediate prediction only. Since immediate predictionmodels original IDMM IDMM coincide, contradiction.Appendix E. Proofs Additional Results TechnicalE.1 Proofs Results Section 4Proof Theorem 4. sake notational simplicity, use intuitive notation f (Xk )extnk (f ). give proof general definition, terms sets desirablegambles. proof lower previsions follows immediately.Consider category set A, n N, 1 k n gamble fn may assume without loss generality singleton.f (Xk ) DAalready implies f 6 0, coherence [D4]. Hence particular f 6= 0 max f > 0.Assume ex absurdo f 6> 0, must f (a) < 0. Definegamble g letting g(a) := f (a) g(x) := max f > 0 x \ {a}.g f therefore g(Xk ) f (Xk ), implies, coherence [use D2 D3], alson . let := max f f (a) > 0 := f (a)/ > 0, defineg(Xk ) DAn , > 0.gamble h := g/ = + IA\{a} , also, coherence [D3], h(Xk ) DAconsider natural number N 2, follows repeatedly applying poolingnrenaming invariance appropriate manner + I{a1 } (Zk ) D{a,1 ,...,aN }Zk variable assumes value a1 Xk 6= assumes value{a2 , . . . , } Xk = a. repeatedly applying category permutation invariance, findn+ I{a` } (Zk ) D{a` {1, . . . , N }. Coherence [D3] tells us1 ,...,aN }NnN + 1 = `=1 [ + I{a` } (Zk )] D{a. leads contradiction coherence1 ,...,aN }[D4] choose N large enough.E.2 Proofs Results Section 7Proposition 35. n N : () = R ( ()).Proof Proposition 35. Consider z D,Tz () = |{k {1, . . . , n} : (xk ) = z}| =yA : (y)=z66|{k {1, . . . , n} : xk = y}|fiCoherent Predictive Inference Exchangeability=Ty () = R ( ())z ,yA : (y)=zconcluding proof.Lemma 36. n N, NAn Dn :11I{} () =().()(R ()) [R ()][]Proof Lemma 36. Consider map : Dn R defined := [] I{} .permutation index set {1, . . . , n} Dn , see() =I{} () =I{(1 )} ()[][]=I{} () =[]I{} () = (),[]tells us permutation invariant thereforeconstant atoms[], NDn . means that, obvious notations, = N n ()I[] .() > 0 implies [] = , therefore,Proposition 35, () = () = R ( ()) = R () therefore [R ()]. tellsus () = 0 unless = R () therefore = (R ())I[R ()] .plug f := 1 Equation (87), see() =() =(R ())I[R ()] () = (R ())(R ()).DnDnLemma 37. n N NDn :BD, R =BA,n : R ()=NAProof Lemma 37. ,( ) ()nzn(BD, R )() =xzD x1 ({z})( )( )nnz=znzzzD N( )n==1 ({z})(n : R ()=NAn:NAR ()=(n67xAzxmxx1 ({z})xmx) (xA)concluding proof.xmx =zD)nz|1 ({z})n:NAR ()=BA, (),fiDe Cooman, De Bock, & Dinizlemma allows us prove two related propositions.Proposition 38. n N gambles f Dn : MnnA (f ) = MnnD (f ) R .Proof Proposition 38. First all, count vector NAnHynA (f |) =11f () =I{} ()f ()()()n[][]=f ()DnI{} ()(87)[]1()f ()(R ()) [R ()]n==1()1(R ())f () = HynD (f |R ()),[R ()]fourth equality follows Lemma 36. Therefore indeed:MnnA (f ) =HynA (f |)BA, =HynD (f |R ())BA,nNA=nNAHynD (f |)nND=BA,n : R ()=NAHynD (f |)(BD, R ) = MnnD (f ) R ,nNDfourth equality follows Lemma 37.Proposition 39. polynomials p n N0 n deg(p):bnpR = bnp R .Proof Proposition 39. find expanding p appropriate Bernstein basis:()np R =bp ()BD, R =bnp ()(BD, R )nND=bnp ()nND=nNDBA, =bnp (R ())BA,nn : R ()=NDNAn : R ()=NA(bnp R )()BA, ,nNAthird equality follows Lemma 37. desired result followsuniqueness expansion (Bernstein) basis.Proof Theorem 7. Fix category sets onto map : D,gamble f Dn . use notation HA := (A)n, n N,68fiCoherent Predictive Inference ExchangeabilityHD := (D), transform Condition (RI2) using equivalence Condition (18).:= ():one hand, lettingnf DAMnnA (f ) HA MnnD (f ) R HAnnnBA,f DAcMnA (f ) HA BA,(MnD (f ) R ) HA ,second equivalences follow Proposition 38. hand, recalling= R ( ())= R ()Proposition 35:()nf DDMnnD (f ) HDnnBD,R ()f DDcMnD (f ) HD .tells us equivalences Condition (RI2) rewritten as:MnnD (f ) R HA MnnD (f ) HDnnBA,(MnD (f ) R ) HA BD,R ()MnD (f ) HD .proof complete observe (and recall discussion Section 5.3Appendix B) varying n N f L(Dn ), let p := MnnD (f ) range, let:= ()rangepolynomials , varying n Ncount vectors NA .Proof Theorem 8. Let, ease notation := inf iI , coherent using Equation (25). Consider category sets onto map : D,p V (D) NA {0}. Then, using representation insensitivitycoherent Theorem 7:(p R )BA, (A) (i I)(p R )BA, (A)(i I)pBD,R () (D) pBD,R () (D),concludes proof.Proposition 40. , (B ) = rB ( ()).Proof Proposition 40. Immediate, since B sample whose components belongB, category B, number times occurs B exactlynumber times occurs .Proof Proposition 9. Consider B , let, simplicity notation = iA ().since NBn , n := deg(p) + r()( )nniA ()xBA,iA () () =x=nx x = BB, (),iA ()xAxBsee indeed:IrB,A (p|) =bnp ()BA,iA () () =nNBnNB69bnp ()BB, () = p().fiDe Cooman, De Bock, & DinizProof Proposition 10. deg(p) + r = 0, r = 0 p = c R, triviallyIrB,A (p|) = I0B,A (c)() = c. let us assume deg(p) + r > 0. First all, observedeg(p)+rNB:()()deg(p) + r iA ()xdeg(p) + r nxBA,iA () () =x=xiA ()xAxB{deg(p)+rBBB, (|+B > 0B)=0otherwise.(88)therefore already follows Condition (29) IrB,A (p|) = 0 B = 0. Let us thereforeassume B > 0. Condition (29) Equation (88) tell us that:IrB,A (p|) =bdeg(p)+r()BA,iA () ()pdeg(p)+rNBdeg(p)+r=bdeg(p)+r()BpBB, (|+B)deg(p)+rNBdeg(p)+rdeg(p)+r= Bbdeg(p)+r()BB, (|+pB ) = Bp(|+B ),deg(p)+rNBconcludes proof.Proposition 41. n N gambles f B n :MnnA (f IB n ) = IrB,A (MnnB (f )), r := n deg(MnnB (f )).Proof Proposition 41. First all, count vector NAn thatwithslight abuse notation:HynA (f IB n |) =11(f IB n )() =()()[]f ()[]B nzero unless = iA () NBn . case, since obviously () = (),[iA ()] B n []again slight abuse notation:HynA (f IB n |iA ()) =1f () = HynB (f |).()[]Therefore, recall Condition (29):HynB (f |)BA,iA ()MnnA (f IB n ) =HynA (f IB n |iA ())BA,iA () =nNBnNB= IrB,A (MnnB (f )),r := n deg(MnnB (f )).70fiCoherent Predictive Inference ExchangeabilityProof Theorem 11. Fix category sets B B A, n, n N,gamble f B n . use notation HA := (A) HB := (B),transform Condition (SP2) using equivalence Condition (18). one hand,:= ()r := n deg(MnnB (f )):lettingnf IB n DAMnnA (f IB n ) HA IrB,A (MnnB (f )) HAnnrnBA,f IB n DAcMnA (f IB n ) HA BA,IB,A (MnB (f )) HA ,second equivalences follow Proposition 41. hand, recallingB ) = rB ( ())= rB ()Proposition 40:(nf DBMnnB (f ) HBnnB BB,rB ()f DBcMnB (f ) HB .tells us equivalences Condition (SP2) rewritten as:IrB,A (MnnB (f )) HA MnnB (f ) HBrnnBA,IB,A (MnB (f )) HA BB,rB ()MnB (f ) HB .proof complete recall discussion Section 5.3 Appendix Bvarying n N f L(B n ), let p := MnnB (f ) = CoMnnB (HynB (f )) rangepolynomials B r = n deg(MnnB (f )) range elements N0 ,, let:= ()range count vectors NA .varying n NProof Theorem 12. Let, ease notation := inf iI , coherent usingEquation (25). Consider category sets B B A, p V (B),NA {0} r N0 . Then, using specificity :IrB,A (p)BA, (A) (i I)IrB,A (p)BA, (A)(i I)pBB,rB () (B) pBB,rB () (B),concludes proof.E.3 Proofs Results Section 8Proof Proposition 13. sufficiency, fix category set A, gamble g A, countvector NA {0}. Condition (RI4) := g(A), := g, f := idD yields Condition (RI5).necessity, fix category sets onto map : D,gamble f D, count vector NA {0}. Observe (f )(A) = f (D)r f (D)Rf ()r =mx =mxxA : (f )(x)=rzD : f (z)=r xA : (x)=z=zD : f (z)=r71R ()z = Rf (R ())r ,fiDe Cooman, De Bock, & DinizRf = Rf R . infer invoking Condition (RI5) twice that:11f DAc id(f )(A) D(f)(A) cRf ()1idf (D) Df1 (D) cRf (R ()) f DDcR (),concluding proof.Proof Theorem 14. arguments proof rely heavily following expressionlower probability function:{}1(n, k) = sup R : I{a} D{a,b}c(k, n k)}{= sup R : ak bnk [a ] ({a, b})(89)related expressions equivalent representation insensitivityBernstein coherence [B3]. expressions follow Equations (32) (33), Bernsteincoherence [B3] representation insensitivity form (RI4).L1. Immediate Bernstein coherence fact (n, k) lower probability:use Equation (89), B2 B4.L2. Fix non-negative integers n, k ` k + ` n. Consider real< (n, k) < (n, `), follows applying Equation (89) Condition (RI4)xk y` znk` [x ] ({x, y, z}) xk y` znk` [y ] ({x, y, z}), whence,Bernstein coherence [B3], xk y` znk` [(x + ) ( + )] ({x, y, z}). ApplyingEquation (89) Condition (RI4) tells us uk+` znk` [u ( + )] ({u, z}),whence + (n, k + `).L3, L4 L5 immediate consequences L1 L2.L6. Consider category set := {a, b} count vector := kmb := n k. Define gamble g g(a) := (n + 1, k + 1) g(b) := (n + 1, k).g(a) g(b) L5, therefore coherence [P5 P3] predictive lowerprevision P 1A (|) tells us P 1A (g|) = g(b) + [g(a) g(b)]P 1A ({a}|) = (n + 1, k) +(n, k)[(n + 1, k + 1) (n + 1, k)] [see also Equation (33)]. clearly suffices proveP 1A (g|) (n, k) = P 1A ({a}|). Consider < P 1A (g|), follows usingEquation (31) that:ak bnk [g(a)a + g(b)b ] (A).(90)Also, > 0, ak+1 bnk [a g(a) + ] (A) ak bn+1k [a g(b) + ] (A),therefore, coherence [B3], recalling + b = 1,(A) 3 ak+1 bnk [a g(a) + ] + ak bn+1k [a g(b) + ]= ak bnk [a g(a)a g(b)b + ]. (91)Combining Statements (90) (91) using coherence [B3], leads ak bnk [a + ](A), whence (n, k) , completes proof.L7. Use L1 L5 find (n, k)[(n + 1, k + 1) (n + 1, k)] 0, use L6.L8. sn 0 follows L4, need prove sn+1 sn , equivalently,(n, 1) (n + 1, 1)[1 + (n, 1)]. Indeed:(n, 1) (n + 1, 1) + (n, 1)[(n + 1, 2) (n + 1, 1)]72fiCoherent Predictive Inference Exchangeability(n + 1, 1) + (n, 1)[2(n + 1, 1) (n + 1, 1)]= (n + 1, 1) + (n, 1)(n + 1, 1),first inequality follows L6 k = 1, second L4 L1.L9. inequalities hold trivially n = 0, due L1. consider n N,category sets := {x, y} B := {x1 , x2 , . . . , xn , y}. Let 0 < < := > 0.Since (1, 1) > , see x [x ] (A), equivalently, x [x (1 ) ] (A),since x + = 1. Representation insensitivity [use Equation (89) Condition (RI4)]tells us xk [xk (1) ] ({xk , y}), specificity [use Theorem 11] allows usinfer ( nk=1 xk)[xk (1 )n ] (B), k {1, . . . , n}.ninfer coherence [B3] ( k=1 xk )[ k=1 xk (1 ) ny ] (B), applyrepresentation insensitivity get xn [x (1 ) ny ] (A). Since = 1 x ,nequivalent xn [x (1 + n) n] (A). shows (n, n) 1+n, usingEquation (89). rest proof immediate.E.4 Proofs Results Section 9Proof Theorem 15. V coherent obvious, category set F,V (A) = V + (A) Bernstein coherent set polynomials .prove representation insensitivity, use Theorem 7. Consider category setsonto map : D, p V (D) NA {0}.indeed(p R )BA, V + (A) p R V + (A) p V + (D) pBD,R () V + (D),first last equivalences follow Proposition 30, second oneLemma 48 K = A.E.5 Proofs Results Section 10Proof Theorem 16. V coherent obvious, category set F,V (A) = V ++ (A) obviously convex cone includes V + (A) [Proposition 28]contain zero polynomial: V ++ (A) therefore Bernstein coherent set polynomials.prove representation insensitivity, use Theorem 7. Consider category setsonto map : D, p V (D) NA {0}.indeed(p R )BA, V ++ (A) ( int(A ))p(R ())BA, () > 0( int(A ))p(R ()) > 0( int(D ))p() > 0( int(D ))p()BD,R () () > 0 pBD,R () V ++ (D),second fourth equivalences follow Bernstein positivity Bernsteinbasis polynomials Proposition 28, third one Lemma 48 K = A.73fiDe Cooman, De Bock, & Dinizprove specificity, use Theorem 11. Consider category sets BB A, p V (B), NA {0} r N0 . indeed:IrB,A (p)BA, V ++ (A) ( int(A ))IrB,A (p|)BA, () > 0( int(A ))IrB,A (p|) > 0( int(B ))p() > 0( int(B ))p()BB,rB () () > 0 pBB,rB () V ++ (B),second fourth equivalences follow Bernstein positivity Bernsteinbasis polynomials Proposition 28, third one Lemma 52 K = A.E.6 Proofs Results Section 11Below, use convenient device identifying, proper subset B A, elementB unique corresponding element = iA () whose components outsideB zero:(x B)x = x (x \ B)x = 0.Also observe that, using convention, identify int(A[] ) subset ,characterise follows:: int(A[] ) (x A)(x > 0 mx > 0).Proof Proposition 17. clearly suffices prove V + (A) HSC,A 0/ HSC,A .first statement easy prove ASC,A trivially includes non-constantBernstein basis polynomials, Proposition 28. Since V + (A) consists finite, strictly positivelinear combinations non-constant Bernstein basis polynomials, immediatelyV + (A) HSC,A .prove second statement, suppose ex absurdo 0 HSC,A . implies++finitely many nk > 0, count vectors k NAnk pk V[(A)k]0 = k pk BA,k . always possible find (at least) one count vector, 1 say,A[k ] 6 A[1 ] k. words, either A[k ] = A[1 ]A[k ] \ A[1 ] 6= . consider int(A[1 ] ). A[k ] \ A[1 ] 6= ,++BA,k () = 0. A[k ] = A[1 ], BA,k () > 0, moreover, since pk V[(A),k]pk () > 0. Hence 0 = k pk ()BA,k () > 0, contradiction.Lemma 42. Consider NA {0} p HSC,A, , ` N, nk N0+++ nk > 0, k NAnk pk V[+(A) p = `k=1 pk BA,k .k]SA, (p) = {K : A[ + k ] K k {1, . . . , `}}thereforemin SA, (p) = min {A[ + k ] : k {1, . . . , `}} .Proof Lemma 42. second statement trivial, given first. restrictattention proving first statement.Assume first A[ + r ] K r {1, . . . , `}. clearly K 6= ,since + nr > 0. may assume without loss generality A[ + r ] minimal74fiCoherent Predictive Inference Exchangeabilityelement set {A[ + k ] : k {1, . . . , `}}. Consider int(A[+r ] ), whencealso K . k {1, . . . , `} A[ + k ] = A[ + r ]and++clearly least one kwe see pk () > 0 since pk V[+(A),k]BA,k () > 0, whence (pk BA,k )() > 0. k must A[ + k ] \A[ + r ] 6= , therefore (pk BA,k )() = 0 since BA,k () = 0. guaranteesp() = `k=1 (pk BA,k )() > 0, whence indeed K SA, (p), since already knowK , A[] A[ + r ] K K 6= .Assume, conversely, K SA, (p), implies 6= K A[] K,K p() 6= 0. Observe A[ + k ] = A[]A[k ],assume ex absurdo A[ + k ] * K therefore A[k ] * K k {1, . . . , `}.Fix k {1, . . . , `}, x A[/ K, therefore x = 0,k ] xwhence BA,k () = 0. shows p() = `k=1 (pk BA,k )() = 0, contradiction.Lemma 43. Consider NA {0}, p V (A) n N n deg(p).NAn :bnp () 6= 0 (K min SA, (p))K \ A[] A[].Proof Lemma 43. Fix NAn . prove contraposition, supposeK min SA, (p), K \ A[] * A[] therefore K * A[ + ], sinceA[ + ] = A[] A[]. Hence, A[ + ]/ SA, (p). Since moreover 6= A[ + ]A[] A[ + ], infer Equation (46) p|B = 0, let, easenotation, B := A[ + ]. rewrite [see also Lemma 47]:0 = p|B =nNAbnp ()BA, |B =nNBbnp ()BA, |B =bnp ()BB, .nNBDue uniqueness Bernstein expansion, possible bnp () = 0nnNA[+]. concludes proof since, clearly, NA[+].Proof Proposition 18. First, assume p HSC,A, , implying p = `k=1 pk BA,k++` N, nk N0 + nk > 0, k NAnk pk V[+(A). alreadyk]follows Lemma 42 p =6 0 min SA, (p) = min {A[ + k ] : k {1, . . . , `}}.Consider K min {A[ + k ] : k {1, . . . , `}} int(K ).k, either A[ + k ] = K A[ + k ] \ K 6= . A[ + k ] = Kwhichhappens least one k, due choice Kthen pk () > 0 BA,k () > 0.A[ + k ] \ K 6= , since A[] K, A[k ] \ K =6 , implying BA,k () = 0.Hence, p() > 0. Since holds int(K ), find p|K V ++ (K).Assume, conversely, p V (A)\{0}p|K V ++ (K)K n min SA, (p).nFix n N n deg(p), p = N n bp ()BA, = bp ()BA, ,{}:= NAn : bnp () 6= 0 . Since p 6= 0, infer Equation (46) min SA, (p) 6=[observe SA, (p)]. know Lemma 43 , leastone K min SA, (p) K \ A[] A[]. Let us pick K, callK . let, K min SA, (p), MK := { : K = K}, foundway divide disjoint subsets MK , one every K min SA, (p)75fiDe Cooman, De Bock, & Dinizmay empty, K \ A[] A[] MK , = Kmin SA, (p) MKtherefore p = Kmin SA, (p) MK bnp ()BA, .fix K min SA, (p), construct count vector K letting (mK )x := 1x K \ A[] (mK )x := 0 otherwise. Notice K NAnK , nK numberelements |K \ A[]| set K \ A[], therefore nK n. Consider MK ,since (mK )x = 1 implies x A[] therefore x 1, see :BA, () = ()xx = ()xA[]xx (mK )xxA[]x(mK )xxA[]= (K, )BA,K ()BA,K (),n1 ( )1 . Hence, rewrite(K, ) := ()()KKMK bp ()BA,:= MK (K, )bnp ()BA,K . way, find p =pK BA,K , pKKmin SA, (p) BA,K pK .Hence, fix K min SA, (p) 6= , left prove + nK > 0++pK V[+(A). Assume first, ex absurdo, + nK = 0. particularK]++K = , contradicts K SA, (p). remains prove pK V[+(A).K]Consider int(A[+K ] ). derive K min SA, (p) SA, (p)A[] K. Since A[K ] = K \A[], implies A[ + K ] = A[]A[K ] =A[] (K \ A[]) = K, therefore also int(K ). K 0 min SA, (p) \ {K},K0 \ K =6 therefore BA,K 0 () = 0. Hence, p() = BA,K ()pK (). knowp() > 0 p|K V ++ (K) BA,K () > 0 A[K ] = K \ A[] K.conclude indeed pK () > 0.Lemma 44. NA {0} p V (A):SA, (p) = SA,0 (pBA, ) therefore min SA, (p) = min SA,0 (pBA, ).Proof Lemma 44. First, assume K SA, (p). 6= K A, A[] Kp|K 6= 0. last inequality continuity polynomials, inferint(K ) p() 6= 0. Since A[] K, find p()BA, () 6= 0therefore (pBA, )|K 6= 0.Assume, conversely, K SA,0 (pBA, ). =6 K (pBA, )|K 6= 0.last inequality implies K (pBA, )() 6= 0 thereforeBA, () 6= 0 p() 6= 0. BA, () 6= 0, derive A[] Kp() 6= 0, derive p|K 6= 0.Proof Proposition 19. way HSC,A HSC,A, constructed [see definingexpressions (43) (44)], clearly suffices prove HSC,A c HSC,A, . Considertherefore p V (A) pBA, HSC,A , Proposition 18, impliespBA, 6= 0 (pBA, )|K V ++ (K) K min SA,0 (pBA, ). setprove p HSC,A, . Applying Proposition 18 again, since, clearly, p 6= 0,see suffices show p|K V ++ (K) K min SA, (p). considerK min SA, (p). Then, Lemma 44, K min SA,0 (pBA, ), already argued(pBA, )|K V ++ (K). Hence indeed also p|K V ++ (K).76fiCoherent Predictive Inference ExchangeabilityProof Equation (48). Combining Equations (47) (30), see that,NA {0}:1= {f L(A) : SA (f ) HSC,A,DSC,Ac(92)}.Also, f L(A) =6 K A:SA (f ) = 0 f = 0, SA (f )|K V ++ (K) f |K > 0 SA (f )|K = 0 f |K = 0. (93)= 0. f L(A):start casemin SA,0 (SA (f )) = {{x} : x f (x) 6= 0} ,1Statement (93). Hence, Proposition 18 Equations (92) (93): DSC,A=L>0 (A).NA . f L(A):Next, consider{{A[]}f |A[]6 0=min SA,(SA (f )) ={x} : x \ A[]f (x) 6= 0} f |A[]{A[]=0(94)Equation (93). recall Proposition 18 Equations (92) (93)1consider two cases: f |A[]6= 0 f |A[]= 0. f |A[]6= 0, f DSC,A c{if f 6= 0 [which redundant]f |A[]> 0 or, equivalently [since f |A[]6= 0],}1f h L(A) : h|A[]> 0 L>0 (A). f |A[]= 0, f DSC,A cor, equivalently [since f |A[]f 6={0 f (x) 0 }x \ A[]= 0],f h L(A) : h|A[]> 0 L>0 (A).Proof Equation (49). start first part Equation (49). Due Equation (17)Proposition 19, suffices prove that, p V (A), minxA p(x ) > 0 pHSC,A,0 minxA p(x ) < 0 p/ HSC,A,0 .First, assume minxA p(x ) < 0. p(y ) < 0.Hence, since p|{y} = p(y ) < 0, find {y} min SA,0 (p) therefore alsop/ HSC,A,0 , Proposition 18.Next, assume minxA p(x ) > 0. p|{x} = p(x ) > 0 x A, implyingmin SA,0 (p) = {{x} : x A} therefore also, since p 6= 0, p HSC,A,0 ,Proposition 18.turn second part Equation (49). Due Equation (17) Proposition 19,NA p V (A), minA[]suffices prove that,p() > 0 pHSC,A,p() < 0 p/ HSC,A,minA[].First, assume minA[]p()<0.int(A[])++p() < 0, implying p|A[]6= 0 p|A[]/ V (A[]).Hence, find A[]min SA,/ HSC,A,(p) therefore also p, Proposition 18.Next, assume minA[]p()>0.p|A[]=6 0 p|A[]V ++ (A[]).therefore also, since p 6= 0, p HSC,A,Hence, find min SA,(p) = {A[]},Proposition 18.77fiDe Cooman, De Bock, & DinizProof Equation (52). first part Equation (52) trivial consequence Equa NA f L(A). Then, combiningtion (51). second part, considerEquations (50) (30):= minf (x)x = min f (x).P 1SC,A (f |)f (x)x = minA[]xAA[]xA[]xA[]Lemma 45. Consider category sets onto map : D,p V (D) =6 K A. (p R )|K 6= 0 p|(K) 6= 0.Proof Lemma 45. First, assume p|(K) 6= 0, (K)p() 6= 0. choose K R () = . Then, clearly, (p R )() =p(R ()) = p() 6= 0 therefore (p R )|K 6= 0.Assume, conversely, (pR )|K 6= 0, K (pR )() 6= 0.let := R (), (K) p() = p(R ()) = (p R )() 6= 0. Hence,p|(K) 6= 0.Lemma 46. Consider category sets onto map : D,p V (D), NA {0}.{}SA, (p R ) = K : A[] K (K) SD,R () (p) ,therefore(SA, (p R )) = SD,R () (p) (min SA, (p R )) = min SD,R () (p).Proof Lemma 46. start proving first statement. First, assume K SA, (pR ), implying =6 K A, A[] K (p R )|K 6= 0. 6= (K) D,D[R ()] = (A[]) (K) and, Lemma 45, p|(K) 6= 0. Hence, (K) SD,R () (p).Conversely, assume K A, A[] K (K) SD,R () (p). =6 (K),implies =6 K, also p|(K) 6= 0, which, Lemma 45, implies (p R )|K 6= 0.Hence, K SA, (p R ).first statement implies (SA, (p R )) SD,R () (p) therefore, orderprove second statement, suffices show SD,R () (p) (SA, (p R )) or,equivalently, every L SD,R () (p), K SA, (p R )(K) = L. choose L SD,R () (p) let K := {x : (x) L} = 1 (L).(K) = L onto, since (A[]) = D[R ()] L, follows A[] K.Hence, first statement, K SA, (p R ).prove third statement, first assume K min SA, (p R ), implyingK SA, (p R ) that, K 0 SA, (p R ), K 0 6 K. second statement,(K) SD,R () (p). prove (K) min SD,R () (p), assume ex absurdoL SD,R () (p) L (K). Let K 0 := {x K : (x) L} = K 1 (L).K 0 K (K 0 ) = L, therefore, Lemma 45, (p R )|K 0 6= 0,K 0 6= p|L =6 0. Since L SD,R () (p), see (A[]) = D[R ()] Ltherefore A[] 1 (L). Since K SA, (p R ), also know A[] K,therefore A[] K 1 (L) = K 0 . tells us K 0 SA, (p R ), contradiction.Assume, conversely, L min SD,R () (p), implying L SD,R () (p). Then,78fiCoherent Predictive Inference Exchangeabilitysecond statement, K 0 SA, (p R ) (K 0 ) = L. Hence,K min SA, (p R ) K K 0 therefore (K) (K 0 ) = L. SinceL min SD,R () (p) since, due second statement, (K) SD,R () (p), also(K) 6 L therefore (K) = L.Lemma 47. Let =6 K let p polynomial . n deg(p):bnp| = bnp |NKn .KProof Lemma 47. followsp() =bnp ()BA, ()nNAK :p|K () =nNA=bnp ()BA, (iA ()) =bnp ()BA, (iA ())n : A[]KNAbnp |NKn ()BK, (),nNKcompletes proof.Lemma 48. Consider category sets onto map : D,p V (D) =6 K A. Then:(i) (p R )|K V + (K) p|(K) V + ((K));(ii) (p R )|K V ++ (K) p|(K) V ++ ((K)).Proof Lemma 48. first statement follows fact that, n deg(p):bn(pR )|Kn> 0 bn(pR ) |NKn > 0 (bnp R )|NKn > 0 bnp |N(K)> 0 bnp|> 0,(K)first last equivalence due Lemma 47, second equivalence followsn) = NnProposition 39, third equivalence holds R (NK(K) .turn second statement, prove following statementsequivalent:(a) ( int(K ))p(R ()) > 0;(b) ( int((K) ))p() > 0.First assume (a) holds, consider int((K) ). prove p() > 0.1construct K follows.Consider z (K). x ({z}) K, choosex > 0 way xK : (x)=z x = z . way, found Ksatisfying R () = , moreover x > 0 x K, whence int(K ).infer (a) indeed p() = p(R ()) > 0.Assume, conversely, (b) holds, consider int(K ). Then, z D,R ()z > 0 z (K) R ()z = 0 otherwise. means R () int((K) )infer (b) indeed p(R ()) > 0.79fiDe Cooman, De Bock, & DinizProposition 49. SC representation insensitive.Proof Proposition 49. use characterisation representation insensitivity Theorem 7. Consider category sets onto map : D,p V (D) NA {0}. Then, Proposition 19, need provep R HSC,A, p HSC,D,R () .First, assume p HSC,D,R () , which, Proposition 18, implies p 6= 0p|L V ++ (L) L min SD,R () (p). Applying Lemma 45 K = A, inferp =6 0 p R 6= 0. Consider K min SA, (p R ). Then, Lemma 46,(K) min SD,R () (p), implying that, due assumption, p|(K) V ++ ((K)). SinceK 6= , apply Lemma 48 find (p R )|K V ++ (K). Hence, Proposition 18,p R HSC,A, .Assume, conversely, pR HSC,A, , which, Proposition 18, implies pR 6= 0(p R )|K V ++ (K) K min SA, (p R ). Applying Lemma 45,K = A, infer p R 6= 0 p 6= 0. Now, consider L min SD,R () (p),Lemma 46, K min SA, (p R ) (K) = L. Since K =6 and,assumption, (p R )|K V ++ (K), infer Lemma 48 p|L V ++ (L). Hence,Proposition 18, p HSC,D,R () .Lemma 50. Consider category sets B B A, p V (B),K K B 6= r N0 . IrB,A (p)|K 6= 0 p|KB 6= 0.Proof Lemma 50. may assume without loss generality r + deg(p) > 0,proof trivial otherwise.First, assume p|KB =6 0, means KBp() 6= 0. := iA () K , infer Proposition 9 IrB,A (p|) = p() 6= 0therefore IrB,A (p)|K 6= 0.Assume, conversely, IrB,A (p)|K 6= 0, means, due continuity polynomials, int(K ) IrB,A (p|) 6= 0. infer K B 6=+B > 0, Proposition 10 guarantees p(|+B ) 6= 0. Since |B KB , findp|KB 6= 0.Lemma 51. Consider category sets B B A, p V (B)r N0 r + deg(p) > 0, NA {0}.{}SA, (IrB,A (p)) = K : A[] K K B SB,rB () (p) ,therefore{}SB,rB () (p) = K B : K SA, (IrB,A (p)){}min SB,rB () (p) = K B : K min SA, (IrB,A (p)) .Proof Lemma 51. begin first statement. First, assume K SA, (IrB,A (p))therefore 6= K A, A[] K IrB,A (p)|K =6 0. K impliesKB B, A[] K implies B[rB ()] = A[]B KB. Moreover, IrB,A (p)|K =6 0together Proposition 10 r + deg(p) > 0 implies K B 6= , turn,Lemma 50, implies p|KB 6= 0. Hence, K B SB,rB () (p). Conversely, assume80fiCoherent Predictive Inference ExchangeabilityK A, A[] K K B SB,rB () (p). K B 6= , implying K 6= ,p|KB 6= 0, which, Lemma 50, implies IrB,A (p)|K 6= 0. Hence, K SA, (IrB,A (p)).order prove second statement, clearly suffices show SB,rB () (p){K B : K SA, (IrB,A (p))}, since converse inclusion follows directly firststatement. consider L SB,rB () (p) let K := L A[]. K A, A[] KK B = L (A[] B) = L B[rB ()] = L. Hence, first statement, indeedK SA, (IrB,A (p)).prove third statement, first assume K min SA, (IrB,A (p)), implyingparticular K SA, (IrB,A (p)). Then, second statement, K B SB,rB () (p).prove K B min SB,rB () (p), consider L SB,rB () (p) L K B,let K 0 := L A[]. Then, argument identical one used proof secondstatement, K 0 B = L K 0 SA, (IrB,A (p)). However, since K 0 B = L K BK 0 \ B = A[] \ B K \ B, find K 0 = (K 0 B) (K 0 \ B) (K B) (K \ B) = K,therefore K 0 = K, assumption. Hence indeed L = K 0 B = K B. Assume,conversely, L min SB,rB () (p), implying L SB,rB () (p). Then, secondstatement, K 0 SA, (IrB,A (p)) K 0 B = L,K min SA, (IrB,A (p)) K K 0 therefore K B K 0 B = L. SinceL min SB,rB () (p) and, second statement, K B SB,rB () (p), alsoK B = L.Lemma 52. Consider category sets B B A, p V (B), KK B 6= r N0 . IrB,A (p)|K V ++ (K) p|KB V ++ (K B).Proof Lemma 52. may assume without loss generality r + deg(p) > 0,proof trivial otherwise. Using Proposition 10, considering that, since K B =6 , B > 0int(K ), suffices prove following statements equivalent:(a) ( int(K ))p(|+B ) > 0;(b) ( int(KB ))p() > 0.First assume (a) holds, consider int(KB ). prove p() > 0.constructK follows. x K \ B, choose x > 0 way:= xK\B x < 1, always possible. x K B, let x := (1)x > 0.follows construction B = 1 > 0, |+B = int(K ),infer (a) indeed p() = p(|+)>0.BAssume, conversely, (b) holds, consider int(K ). B > 0+K B =6 0 therefore, z B, (|+B )z > 0 z K B. Hence |B int(KB ),+infer (b) p(|B ) > 0.Proposition 53. SC specific.Proof Proposition 53. use characterisation specificity Theorem 11. Considercategory sets B B A, p V (B), NA {0}, r N0 .Then, Proposition 19, need prove IrB,A (p) HSC,A, p HSC,B,rB () .First, assume p HSC,B,rB () , which, Proposition 18, implies p 6= 0p|L V ++ (L) L min SB,rB () (p). Applying Lemma 50 K = A, inferp 6= 0 IrB,A (p) 6= 0. Consider K min SA, (IrB,A (p)), Lemma 51,81fiDe Cooman, De Bock, & DinizK B min SB,rB () (p), implying that, due assumption, p|KB V ++ (K B).Since K B 6= 0, apply Lemma 52 find IrB,A (p)|K V ++ (K). Hence,Proposition 18, IrB,A (p) HSC,A, .Assume, conversely, IrB,A (p) HSC,A, , which, Proposition 18, impliesrIB,A (p) 6= 0 IrB,A (p)|K V ++ (K) K min SA, (IrB,A (p)). Lemma 50K = A, IrB,A (p) 6= 0, infer p 6= 0. Consider L min SB,rB () (p),then, Lemma 51, K min SA, (IrB,A (p)) KB = L. Since thereforeK B 6= 0 since, assumption, IrB,A (p)|K V ++ (K), infer Lemma 52p|L V ++ (L). Hence, Proposition 18, p HSC,B,rB () .Proof Theorem 20. immediate consequence Propositions 17 [coherence], 49[representation insensitivity] 53 [specificity].E.7 Proofs Results Section 12NA {0} p V (A).Proof Equation (54). ConsiderBA, p HIDM,Ap HIDM,Ac( sA ) DiA (BA, p|) > 0+ ) > 0( sA ) DiA (BA, |) DiA (p|+ ) > 0,( sA ) DiA (p|third equivalence follows Updating Property Dirichlet expectation[Proposition 31].NA {0}. Then, combining Equations (56)Proof Equation (59). Consider(58) n = 1:{}mx + xs,1= f L(A) : ( sA )DIDM,Acf (x)>0 .+xAconsider f L(A). sA :xAf (x)mx + xx1>0f (x)(mx + x ) > 0>f (x)f (x)mx .+xAxACombining equations above, letting c := 1sfind that:xAxA f (x)mxs,1(s0 (0, s))( int(A ))f DIDM,Acease notation,s0f (x)tx > c.(95)xAf c, f (y) < c therefore, Statement (95),s,1[choose s0 ty close enough 1, respectively]. f = c, duef/ DIDM,Acs,1Finally, let usdefinition c, f = c = 0. Hence, Statement (95), f/ DIDM,Ac.see happensf > c. clearly c 0. Consider s0 (0,s) int(A ).00since f > c, xA f (x)tx > c therefore also, since c 0, ss xA f (x)tx > ss c c.s,1Statement (95).Hence f DIDM,Ac82fiCoherent Predictive Inference ExchangeabilityNA {0} f L(A). combiningProof Equation (60). ConsiderEquations (57) (58):mx + xmx + s0 tx= infinff (x)+ s0 (0,s) int(A )+ s0xAxA()1s0= inff (x)mx +inff (x)tx+ s0 int(A )s0 (0,s) + s0xAxA()1s0= inff (x)mx +min f+ s0s0 (0,s) + s0xA1f (x)mx +min f,=++= infP s,1IDM,A (f |)f (x)xAlast equality follows min fmxxA f (x) ,property convex combinations.Proof Theorem 21. coherence, fix category set A, must proveHIDM,Asatisfies requirements B1B3 Bernstein coherence. trivialdefinition HIDM,A, linearity Dirichlet expectation operator, factDirichlet expectation Bernstein basis polynomial positive.Next, turn representation insensitivity, use characterisation Theorem 7.Consider category sets onto map : D, p V (D)NA {0}. Then, using Pooling Property [Proposition 33] Dirichletexpectation Equation (54), find indeed:(p R )BA, HIDM,A( sA ) DiA (p R | + ) > 0( sA ) (p|R ( + )) > 0( sD ) (p|R () + ) > 0 pBD,R () HIDM,D,third equivalence follows equality sD = R (sA ).Finally, turn specificity, use characterisation Theorem 11. Considercategory sets B B A, p V (B), NA {0}r N0 . Then, using Restriction Property [Proposition 34] Dirichlet expectationEquation (54), find indeed:IrB,A (p)BA, HIDM,A( sA ) DiA (IrB,A (p)| + ) > 0( sA ) DiB (p|rB ( + )) > 0( sB ) DiB (p|rB () + ) > 0 pBB,rB () HIDM,B,third equivalence follows sB = rB (sA ).E.8 Proofs Results Section 13Lemma 54. p1 , p2 HSI,A: SA,0 (p1 + p2 ) = SA,0 (p1 ) SA,0 (p2 ).83fiDe Cooman, De Bock, & DinizProof Lemma 54. First, consider K SA,0 (p1 + p2 ), meaning 6= K(p1 + p2 )|K 6= 0. Assume, ex absurdo, K/ SA,0 (p1 ) K/ SA,0 (p2 ). p1 |K = 0p2 |K = 0 therefore (p1 + p2 )|K = 0, contradiction. Hence indeedK SA,0 (p1 ) SA,0 (p2 ).Next, consider K SA,0 (p1 ) SA,0 (p2 ), implying =6 K A.least one K 0 min(SA,0 (p1 ) SA,0 (p2 )) K 0 K, assume withoutloss generality K 0 SA,0 (p1 ). Since K 0 min(SA,0 (p1 ) SA,0 (p2 )),L 6 K 0 L SA,0 (p1 ) SA,0 (p2 ), therefore K 0 min SA,0 (p1 ). already tells us0p1 |K 0 HIDM,K0 . two possibilities. first one K SA,0 (p2 ),then, much way above, find p2 |K 0 HIDM,K 0 . Hence,due Bernstein coherence [B3] HIDM,K0 , (p1 + p2 )| 0 = p1 | 0 + p2 | 0 HIDM,K 0 .KKK00second possibility K/ SA,0 (p2 ), p2 |K 0 = 0 since K 6= , find,too, (p1 + p2 )|K 0 = p1 |K 0 + p2 |K 0 = p1 |K 0 HIDM,K0 . cases, therefore,(p1 + p2 )|K 0 HIDM,K 0 , Bernstein coherence [B1] HIDM,K0 allows us conclude6 0. Since K 0 K, find also (p1 + p2 )|K 6= 0 therefore(p1 + p2 )|K 0 =K SA,0 (p1 + p2 ).Proof Proposition 22. Since 0/ HSI,A, left prove V + (A) HSI,Athat,> 0 p, p1 , p2 HSI,A , p HSI,A p1 + p2 HSI,A .First, consider > 0 p HSI,A. Then, clearly, SA,0 (p) = SA,0 (p) thereforemin SA,0 (p) = min SA,0 (p). K min SA,0 (p), K min SA,0 (p),which, since p HSI,A, implies p|K HIDM,Ktherefore, due Bernsteincoherence HIDM,K , (p)|K = (p|K ) HIDM,K. Furthermore, since p 6= 0 alsop 6= 0, therefore p HSI,A .Next, consider p1 , p2 HSI,A. p1 6= 0 p2 6= 0, implying SA,0 (p1 ) 6=SA,0 (p2 ) 6= , therefore SA,0 (p1 ) SA,0 (p2 ) 6= . Applying Lemma 54, findSA,0 (p1 + p2 ) 6= , K 6= K A, (p1 + p2 )|K =6 0therefore p1 + p2 6= 0. K 0 min SA,0 (p1 + p2 ), equivalently, due Lemma 54,K 0 min(SA,0 (p1 ) SA,0 (p2 )). Then, applying reasoning second partproof Lemma 54, find (p1 + p2 )|K 0 HIDM,K0 . Hence, p1 + p2 HSI,A .Since already shown HSI,A closed taking positive linear combinations,since V + (A) consists positive linear combinations Bernstein basis polynomials,need show HSI,Acontains Bernstein basis polynomials order prove+V (A) HSI,A . consider NA {0}. Then, K 6= K A,BA, |K = BK,rK () A[] K, BA, |K = 0 otherwise. impliesSA,0 (BA, ) = { =6 K : A[] K} that, due Bernstein coherence HIDM,K,BA, |K = BK,rK () HIDM,K K SA,0 (BA, ). Hence, BA, |K HIDM,KK min SA,0 (BA, ). Since also BA, 6= 0, find indeed BA, HSI,A.Proof Proposition 23. first prove HSI,Ac HSI,A,. Consider p V (A)pBA, HSI,A , meaning pBA, 6= 0 (pBA, )|K HIDM,KK min SA,0 (pBA, ). set prove p HSI,A, . Since, clearly, p 6= 0, sufficesshow p|K HIDM,KcrK () K min SA, (p). consider K min SA, (p),implying A[] K therefore also K[rK ()] = A[]. also inferLemma 44 K min SA,0 (pBA, ), tells us (pBA, )|K HIDM,K. Since(pBA, )|K = p|K BA, |K = p|K BK,rK () , find p|K HIDM,K crK ().84fiCoherent Predictive Inference ExchangeabilityNext, prove HSI,A,HSI,Ac. Consider p HSI,A,, meaningp 6= 0 p|K HIDM,K crK () K min SA, (p). set provepBA, HSI,Aor, equivalently, pBA, =6 0 (pBA, )|K HIDM,KK min SA,0 (pBA, ). Since p 6= 0, continuity polynomials guaranteesint(A ) p() 6= 0 therefore also (pBA, )() 6= 0. know alreadypBA, 6= 0. Consider K min SA,0 (pBA, ). Then, Lemma 44, K min SA, (p),implying A[] K p|K HIDM,KcrK () therefore p|K BK,rK () HIDM,K.Since moreover p|K BK,rK () = (pBA, )|K , find indeed (pBA, )|K HIDM,K .Proof Equation (63). Due Equation (17), suffices prove that, p V (A),minxA p(x ) > 0 p HSI,AminxA p(x ) < 0 p/ HSI,A.First, assume minxA p(x ) < 0. p(y ) < 0.Hence, since p|{y} = p(y ) < 0, find {y} min SA,0 (p) therefore also, dueBernstein coherence HIDM,{y}[see Theorem 21], p|{y}/ HIDM,{y},infer p/ HSI,A .Next, assume minxA p(x ) > 0. p|{x} = p(x ) > 0 x A, implyingmin SA,0 (p) = {{x} : x A} that, x A, p|{x} HIDM,{x},Bernstein coherence HIDM,{x} . Hence, since p 6= 0, find p HSI,A.Proof Equations (64) (65). Equation (65) follows directly Equation (55).prove Equation (64). Due Equation (17) Proposition 23, suffices prove that,NA p V (A):> 0 p HSI,A,<0pc(p, )/ HSI,A,c(p, ),where, ease notation, let:=c(p, )infsA[]+ ).DiA[]|rA[](p|A[]()< 0, implying DiA[]+ ) < 0First, assume c(p, )|rA[](p|A[]()A[]6= 0 and, Equation (54), p|A[]/therefore also p|A[]HIDM,A[](p) therefore also(). Hence, find A[] min SA,crA[]p/ HSI,A,.> 0, implying p|A[]Next, assume c(p, )6= 0 and, Equation (54),thereforep|A[]Hcr().Hence,findmin(p) = {A[]}A,A[]IDM,A[]also p HSI,A,.NA :Proof Equation (67). combining Equations (62) (30), see that,{}s,1= f L(A) : SA (f ) HSI,A,DSI,Ac(96).Consider f L(A) distinguish two cases: f |A[]6= 0 f |A[]= 0.f |A[]6= 0 [and therefore also f 6= 0],s,1SA (f )|A[]f DSI,AcHIDM,A[()crA[]]SA[](f |A[]) HIDM,A[]()crA[]85fiDe Cooman, De Bock, & Dinizs,1f |A[]DIDM,A[]()crA[]11f |A[]>f(x)f>f (x)mx f > 0,x|A[]xA[]xA[]first equivalence due Statement (93) Equations (94), (61) (96).second equivalence follows definition SA SA[]third one dueEquations (21) (30). fourth equivalence consequence Equation (67)final equivalence holds f > 0 redundant, given f |A[]6= 0.f |A[]= 0, [again, using Statement (93) Equations (94), (61) (96)]s,1f 6= 0 x \ A[]:f DSI,Acf (x) = 0 SA (f )|A[]{x}HIDM,A[crA[]{x}().]{x}Since f |A[]crA[]{x}() Bernstein coherent [Theorem 21],= 0 HIDM,A[]{x}latter statement equivalent f (x) > 0. Hence, find that:s,1f 6= 0 (x \ A[])ff DSI,Ac(x) 0f >0f |A[]>1f (x)mx f > 0,xA[]second third equivalences consequences f |A[]= 0.Lemma 55. Consider category sets onto map : D,p V (D), NA {0}, 6= K A[] K.(p R )|K HIDM,KcrK () p|(K) HIDM,(K)cr(K) (R ()).Proof Lemma 55. Let := K, := (K), := |K p := p|(K) .onto map , p V (D ) p R = p|(K) R|K = (p R )|K .Since A[] K, identify element := rK () NKthereforeresult follows representation insensitivity IDMM inference systemhyperparameter s, also R ( ) = R|K (rK ()) = r(K) (R ()):p R HIDM,Ac p HIDM,D cR ( ).Proposition 56. sSI representation insensitive.Proof Proposition 56. use characterisation representation insensitivity Theorem 7. Consider category sets onto map : D,p V (D) NA {0}. Then, Proposition 23, need provep R HSI,A,p HSI,D,R.()First, assume p HSI,D,R () , meaning p 6= 0 p|L HIDM,LcrL (R ())L min SD,R () (p). Applying Lemma 45 K = A, infer p 6= 0p R 6= 0. Consider K min SA, (p R ), 6= K A[] K. Then,Lemma 46, (K) min SD,R () (p), implying that, due assumption, p|(K)86fiCoherent Predictive Inference ExchangeabilityHIDM,(K)cr(K) (R ()). Applying Lemma 55, find (p R )|K HIDM,KcrK ().Hence, p R HSI,A, .Assume, conversely, p R HSI,A,, meaning p R 6= 0 (p R )|KHIDM,K crK () K min SA, (p R ). Applying Lemma 45 K = A, inferp R 6= 0 p 6= 0. Consider L min SD,R () (p). Lemma 46,K min SA, (p R ) (K) = L. Since 6= K A, A[] Kand, assumption, (p R )|K HIDM,KcrK (), infer Lemma 55p|L HIDM,L crL (R ()). Hence, p HSI,D,R () .Lemma 57. Consider category sets B B A, p V (B),NA {0}, r N0 K K B 6= A[] K.IrB,A (p)|K HIDM,KcrK () p|KB HIDM,KBcrKB ().Proof Lemma 57. Let := K, B := K B, p := p|KB r = deg(p) deg(p ) + r.B , p V (B ), r r 0, r + deg(p ) = r + deg(p),bdeg(p)+r()BA,iA ()|K =bdeg(p)+r()BA,iA ()|KIrB,A (p)|K =ppdeg(p)+rdeg(p)+rNB=NBB[]Kbdeg(p)+r()BK,iK () = IrB ,A (p ),p|KBdeg(p)+rNKBthird equality follows unicity Bernstein expansion polynomial.Since A[] K, identify element := rK () NK {0} thereforeresult follows specificity IDMM inference system hyperparameter s,also rB ( ) = rKB ():IrB ,A (p ) HIDM,Ac p HIDM,B crB ( ).Proposition 58. sSI specific.Proof Proposition 58. use characterisation specificity Theorem 11. Considercategory sets B B A, p V (B), NA {0}r N0 . Then, Proposition 23, need prove IrB,A (p) HSI,A,p HSI,B,r.B ()clear Propositions 10 22 assume without loss generalityr + deg(p) > 0.First, assume p HSI,B,r, implying p =6 0 p|L HIDM,LcrBL ()B ()L min SB,rB () (p). Applying Lemma 50 K = A, infer p 6= 0IrB,A (p) 6= 0. Consider K min SA, (IrB,A (p)). infer Lemma 51 KcrKB ().B min SB,rB () (p), implying that, due assumption, p|KB HIDM,KBrSince K B 6= 0 A[] K, IB,A (p)|K HIDM,K crK () Lemma 57. Hence,IrB,A (p) HSI,A,.Assume, conversely, IrB,A (p) HSI,A,, implies IrB,A (p) 6= 0rIB,A (p)|K HIDM,K crK () K min SA, (IrB,A (p)). Applying Lemma 50K = A, infer IrB,A (p) 6= 0 p 6= 0. Consider L min SB,rB () (p).Lemma 51, K min SA, (IrB,A (p)) K B = L. Since K B 6= 0,87fiDe Cooman, De Bock, & DinizA[] K and, assumption, IrB,A (p)|K HIDM,KcrK (), infer Lemma 57p|KB HIDM,KB crKB (), words, p|L HIDM,LcrBL (). Hence,p HSI,B,rB () .Proof Theorem 24. immediate consequence Propositions 22 [coherence], 56[representation insensitivity] 58 [specificity].E.9 Proofs Results Section 14Proof Theorem 25. begin coherence. Consider category set A,prove HH,A Bernstein coherent. B1, recall 0/ HIDM,A> 0,+therefore also 0/ HH,A . Similarly, B2, recall V (A) HIDM,A > 0,+therefore also V (A) HH,A . B3, consider n N k R>0 pk HH,Ak {1, . . . ,n}. > 0 pk HIDM,Ak {1, . . . , n},nn therefore k=1 k pk HIDM,A , Bernstein coherence [Theorem 21]. Hence indeedk=1 k pk HH,A .Next, turn representation insensitivity, use characterisation Theorem 7.Consider category sets onto map : D, p V (D)NA {0}. find indeed:(p R )BA, HH,A (s R>0 )(p R )BA, HIDM,A(s R>0 )pBD,R () HIDM,DpBD,R () HH,D ,second equivalence follows representation insensitivity IDMMinference systems [Theorem 21].Finally, turn specificity, use characterisation Theorem 11. Considercategory sets B B A, p V (B), NA {0} r N0 .find indeed:IrB,A (p)BA, HH,A (s R>0 )IrB,A (p)BA, HIDM,A(s R>0 )pBB,rB () HIDM,BpBB,rB () HH,B ,second equivalence follows specificity IDMM inference systems [Theorem 21].NA {0} p V (A):Proof Equation (69).pBA,p HH,A cHH,A (s R>0 )pBA,HIDM,A(s R>0 )p HIDM,Ac.Combined Equation (54), yields desired result.NA {0} p V (A):Proof Equation (71).= sup { R : p HH,A c}H H,A (p|){}= sup sup R : p HIDM,AcsR>088fiCoherent Predictive Inference Exchangeability+ ) = liminf DiA (p|= sup+ ),inf DiA (p|s+0 sAsR>0second equality due Equation (69), third one due Equation (54).Proof Equation (72). Consider p V (A) apply Equation (71):H H,A (p) = liminf DiA (p|) = lims+0 sAinf DiA (p|s0 )infs+0 int(A ) s0 (0,s)(97)fix n max{deg(p), 1} int(A ). Using Equation (80), findNAn :(10DiA (BA, |s ) =s0 (n))( )n 0 (mx )n1(m )(s tx )= (n)(s0 tx ) x ,0xAxA[]x A[]:(mx )(s0 tx )= (s0 tx )(s0 tx + 1) . . . (s0 tx + mx 1) = s0 tx (mx 1)![1 + O(s0 )]similarly:1s0 (n)=s0 (n1[1 + O(s0 )].1)!Hence, find(0DiA (BA, |s ) =xA[] tx (mx1)!)(n 1)!s0|A[]|1 [1 + O(s0 )].consider two cases: |A[]| > 1 |A[]| = 1 [since n 1, casesexhaustive]. |A[]| > 1, DiA (BA, |s0 ) = O(s0 ). |A[]| = 1 or, equivalently,x = nx , DiA (BA,nx |s0 ) = tx [1 + O(s0 )]. combineEquation (78), findDiA (p|s0 ) =bnp () DiA (BA, |s0 ) =nNAbnp (nx )tx + O(s0 ).xAFurthermore, due Equation (78):bnp (nx ) =bnp ()BA, (x ) = p(x ) x A.nNAHence, concludeDiA (p|s0 ) =p(x )tx + O(s0 ),xAwhich, combined Equation (97), leads desired result.89fiDe Cooman, De Bock, & DinizNA p V (A) use Equation (71):Proof Equation (73). Consider= limH H,A (p|)= lim sup DiA (p|+ ). (98)+ ) H H,A (p|)inf DiA (p|s+0 sAs+0Bernstein coherent [Theorem 25], follows H H,A (|)coherentSince HH,A csuper-additive, conjugate upperlower prevision. implies H H,A (|)sub-additive. Hence, suffices prove equalities Equation (73)prevision H H,A (|)Bernstein basis polynomial p = BA, , NA {0}. sAgather Equation (80) Appendix B that:( )n+ ) =(mx + x )(nx ) .DiA (BA, |(n)(mA + )xA1Observe that:x)(mx + x )(nx ) = (mx + x )(mx + x + 1) . . . (mx + x + nx 1) = m(n[1 + O(x )],xsimilarly, since > 0:1(mA + )(n)1=(n)[1 + O(A )]Therefore:( )(nx )nxA mx+ ) =DiA (BA, |[1+O()][1 + O(x )],(n)xAwhich, using Equation (98), leads to:46( )(nx )nxA mx= H H,A (BA, |)=H H,A (BA, |)= DiA (BA, |).(n)E.10 Proofs Results Section 15Proof Theorem 26. already argued smallest inferencesystem , shall denote lower probability function . First, assume n 2.denote (n, k) := (n, k + 1) (n, k), follows assumptions(n, k + 1) (n, k) 0 k n 2.(99)first going prove induction implies(n, k)k(n, n) 0 k n.n46. See footnote 35.90(100)fiCoherent Predictive Inference ExchangeabilityObserve inequality holds trivially k = 0 [Theorem 14.L1]. assumeinequality holds k = `, ` {0, . . . , n 1}. must show also holdsk = ` + 1. Assume, ex absurdo, not, therefore(n, ` + 1) <`+11(n, n) (n, `) + (n, n),nn(101)second inequality follows induction hypothesis. also(n, n) = (n, ` + 1) +n1(n, m) (n, ` + 1) + (n ` 1)(n, `)m=`+1<`+1n`1(n, n) +(n, n) = (n, n),nnfirst inequality follows Equation (99), second firstsecond inequalities Equation (101). contradiction, completes proofinduction (100).infer (100), Theorem 14.L9 assumption (76)(n, k)k nk=0 k n.nn+sn+s(102)Also observe inequality holds trivially n {0, 1}. get predictivelower prevision P 1A (h|) gamble h A:[h(x) min h]P 1A (I{x} |)P 1A (h|) = min h + P 1A (h min h|) min h +xA= min h +[h(x) min h](n, mx )xAmin h +xA[h(x) min h]mx= P s,1IDM,A (h|),n+sfirst equality first inequality follow coherence [P5, P2 P3]P 1A (|), second equality representation insensitivity [Equation (33)],second inequality Equation (102). converse inequality, observe IDMMinference system sIDM coherent, representation insensitive, specific Theorem 21,clearly concave surprise, satisfies assumption (76), therefore dominates smallestinference system.ReferencesAugustin, T., Coolen, F. P. A., De Cooman, G., & Troffaes, M. C. M. (Eds.). (2014).Introduction Imprecise Probabilities. John Wiley & Sons.Bernard, J.-M. (1997). Bayesian analysis tree-structured categorized data. Revue Internationale de Systmique, 11, 1129.Bernard, J.-M. (2005). introduction imprecise Dirichlet model multinomialdata. International Journal Approximate Reasoning, 39, 123150.91fiDe Cooman, De Bock, & DinizBernard, J.-M. (2007). personal conversation..Boole, G. (1847, reprinted 1961). Laws Thought. Dover Publications, New York.Boole, G. (2004, reprint work originally published Watts & Co., London, 1952).Studies Logic Probability. Dover Publications, Mineola, NY.Carnap, R. (1952). continuum inductive methods. University Chicago Press.Cifarelli, D. M., & Regazzini, E. (1996). De Finettis contributions probability statistics.Statistical Science, 11, 253282.Couso, I., & Moral, S. (2011). Sets desirable gambles: conditioning, representation,precise probabilities. International Journal Approximate Reasoning, 52 (7), 10341055.Cozman, F. G. (2013). Independence full conditional probabilities: Structure, factorization, non-uniqueness, bayesian networks. International Journal ApproximateReasoning, 54 (9), 12611278.De Cooman, G., & Miranda, E. (2007). Symmetry models versus models symmetry.Harper, W. L., & Wheeler, G. R. (Eds.), Probability Inference: Essays HonorHenry E. Kyburg, Jr., pp. 67149. Kings College Publications.De Cooman, G., & Miranda, E. (2008a). F. Riesz Representation Theorem finiteadditivity. Dubois, D., Lubiano, M. A., Prade, H., Gil, M. A., Grzegorzewski,P., & Hryniewicz, O. (Eds.), Soft Methods Handling Variability Imprecision(Proceedings SMPS 2008), pp. 243252. Springer.De Cooman, G., & Miranda, E. (2008b). Weak strong laws large numbers coherentlower previsions. Journal Statistical Planning Inference, 138 (8), 24092432.De Cooman, G., & Miranda, E. (2012). Irrelevant independent natural extensionsets desirable gambles.. Journal Artificial Intelligence Research, 45, 601640.De Cooman, G., Miranda, E., & Quaeghebeur, E. (2009a). Representation insensitivityimmediate prediction exchangeability. International Journal ApproximateReasoning, 50 (2), 204216.De Cooman, G., & Quaeghebeur, E. (2012). Exchangeability sets desirable gambles.International Journal Approximate Reasoning, 53 (3), 363395. Special issuehonour Henry E. Kyburg, Jr.De Cooman, G., Quaeghebeur, E., & Miranda, E. (2009b). Exchangeable lower previsions.Bernoulli, 15 (3), 721735.de Finetti, B. (1937). La prvision: ses lois logiques, ses sources subjectives. Annales delInstitut Henri Poincar, 7, 168. English translation Kyburg Jr. Smokler(1964).de Finetti, B. (1970). Teoria delle Probabilit. Einaudi, Turin.de Finetti, B. (19741975). Theory Probability: Critical Introductory Treatment. JohnWiley & Sons, Chichester. English translation de Finettis (1970) book, two volumes.Dubins, L. E. (1975). Finitely additive conditional probabilities, conglomerabilitydisintegrations. Annals Probability, 3, 8899.92fiCoherent Predictive Inference ExchangeabilityGeisser, S. (1993). Predictive Inference: Introduction. Chapman & Hall.Goldstein, M. (1983). prevision prevision. Journal American StatisticalSociety, 87, 817819.Goldstein, M. (1985). Temporal coherence. Bernardo, J. M., DeGroot, M. H., Lindley,D. V., & Smith, A. F. M. (Eds.), Bayesian Statistics, Vol. 2, pp. 231248. North-Holland,Amsterdam. discussion.Good, I. J. (1965). Estimation Probabilities: Essay Modern Bayesian Methods.MIT Press.Haldane, J. B. S. (1945). method estimating frequencies. Biometrika, 33, 222225.Hausdorff, F. (1923). Momentprobleme fr ein endliches Intervall. Mathematische Zeitschrift,13, 220248.Jaynes, E. T. (2003). Probability Theory: Logic Science. Cambridge University Press.Jeffreys, H. (1998). Theory Probability. Oxford Classics series. Oxford University Press.Reprint third edition (1961), corrections.Johnson, N. L., Kotz, S., & Balakrishnan, N. (1997). Discrete Multivariate Distributions.Wiley Series Probability Statistics. John Wiley Sons, New York.Johnson, W. E. (1924). Logic, Part III. Logical Foundations Science. CambridgeUniversity Press. Reprinted Dover Publications 1964.Keynes, J. M. (1921). Treatise Probability. Macmillan, London.Koopman, B. O. (1940). Axioms Algebra Intuitive Probability. AnnalsMathematics, Second Series, 41 (2), 269292.Kyburg Jr., H. E., & Smokler, H. E. (Eds.). (1964). Studies Subjective Probability. Wiley,New York. Second edition (with new material) 1980.Lad, F. (1996). Operational Subjective Statistical Methods: Mathematical, PhilosophicalHistorical Introduction. John Wiley & Sons.Levi, I. (1980). Enterprise Knowledge. MIT Press, London.Mangili, F., & Benavoli, A. (2013). New prior near-ignorance models simplex.Cozman, F., Denux, T., Destercke, S., & Seidenfeld, T. (Eds.), ISIPTA 13Proceedings Eighth International Symposium Imprecise Probability: TheoriesApplications, pp. 213222. SIPTA.Miranda, E. (2009). Updating coherent lower previsions finite spaces. Fuzzy SetsSystems, 160 (9), 12861307.Miranda, E., & De Cooman, G. (2014). Introduction Imprecise Probabilities, chap. Lowerprevisions. John Wiley & Sons.Miranda, E., & Zaffalon, M. (2011). Notes desirability conditional lower previsions.Annals Mathematics Artificial Intelligence, 60 (3-4), 251309.Moral, S. (2005). Epistemic irrelevance sets desirable gambles. Annals MathematicsArtificial Intelligence, 45, 197214.93fiDe Cooman, De Bock, & DinizMoral, S., & Wilson, N. (1995). Revision rules convex sets probabilities. Coletti,G., Dubois, D., & Scozzafava, R. (Eds.), Mathematical Models Handling PartialKnowledge Artificial Intelligence, pp. 113128. Plenum Press, New York.Piatti, A., Zaffalon, M., Trojani, F., & Hutter, M. (2009). Limits learning categoricallatent variable prior near-ignorance. International Journal ApproximateReasoning, 50 (4), 597611.Prautzsch, H., Boehm, W., & Paluszny, M. (2002). Bzier B-Spline Techniques. Springer,Berlin.Quaeghebeur, E. (2014). Introduction Imprecise Probabilities, chap. Desirability. JohnWiley & Sons.Quaeghebeur, E., De Cooman, G., & Hermans, F. (2014). Accept & reject statement-baseduncertainty models. International Journal Approximate Reasoning. Acceptedpublication.Rouanet, H., & Lecoutre, B. (1983). Specific inference ANOVA: significance testsBayesian procedures. British Journal Mathematical Statistical Psychology,36 (2), 252268.Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (1995). representation partially orderedpreferences. Annals Statistics, 23, 21682217. Reprinted collectionSeidenfeld et al. (1999, pp. 69129).Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (1999). Rethinking FoundationsStatistics. Cambridge University Press, Cambridge.Sheffer, I. M. (1939). properties polynomial sets type zero. Duke MathematicalJournal, 5, 590622.Smith, C. A. B. (1961). Consistency statistical inference decision. JournalRoyal Statistical Society, Series A, 23, 137.Troffaes, M. C. M., & De Cooman, G. (2014). Lower Previsions. Wiley.Trump, W., & Prautzsch, H. (1996). Arbitrary degree elevation Bzier representations.Computer Aided Geometric Design, 13, 387398.Walley, P. (1991). Statistical Reasoning Imprecise Probabilities. Chapman Hall,London.Walley, P. (1996). Inferences multinomial data: learning bag marbles. JournalRoyal Statistical Society, Series B, 58, 357. discussion.Walley, P. (1997). bounded derivative model prior ignorance real-valuedparameter. Scandinavian Journal Statistics, 24 (4), 463483.Walley, P. (2000). Towards unified theory imprecise probability. International JournalApproximate Reasoning, 24, 125148.Walley, P., & Bernard, J.-M. (1999). Imprecise probabilistic prediction categorical data.Tech. rep. CAF-9901, Laboratoire Cognition et Activites Finalises, Universit deParis 8.94fiCoherent Predictive Inference ExchangeabilityWilliams, P. M. (1975a). Coherence, strict coherence zero probabilities. ProceedingsFifth International Congress Logic, Methodology Philosophy Science,Vol. VI, pp. 2933. Dordrecht. Proceedings 1974 conference held Warsaw.Williams, P. M. (1975b). Notes conditional previsions. Tech. rep., School MathematicalPhysical Science, University Sussex, UK. See also revised journal versionWilliams (2007).Williams, P. M. (1976). Indeterminate probabilities. Przelecki, M., Szaniawski, K., &Wojcicki, R. (Eds.), Formal Methods Methodology Empirical Sciences, pp.229246. Reidel, Dordrecht. Proceedings 1974 conference held Warsaw.Williams, P. M. (2007). Notes conditional previsions. International Journal ApproximateReasoning, 44, 366383.Zabell, S. L. (1982). W. E. Johnsons sufficientness postulate. Annals Statistics, 10,10901099. Reprinted collection Zabell (2005).Zabell, S. L. (2005). Symmetry Discontents: Essays History Inductive Probability. Cambridge Studies Probability, Induction, Decision Theory. CambridgeUniversity Press, Cambridge, UK.Zaffalon, M., & Miranda, E. (2013). Probability time. Artificial Intelligence, 198, 151.95fiJournal Artificial Intelligence Research 52 (2015) 399443Submitted 08/14; published 03/15Computing Convex Coverage SetsFaster Multi-objective CoordinationDiederik M. RoijersShimon WhitesonFrans A. Oliehoekd.m.roijers@uva.nls.a.whiteson@uva.nlf.a.oliehoek@uva.nlInformatics InstituteUniversity AmsterdamAmsterdam, NetherlandsAbstractarticle, propose new algorithms multi-objective coordination graphs (MOCoGs). Key efficiency algorithms compute convex coverageset (CCS) instead Pareto coverage set (PCS). CCS sufficient solutionset large class problems, also important characteristics facilitateefficient solutions. propose two main algorithms computing CCS MO-CoGs.Convex multi-objective variable elimination (CMOVE) computes CCS performingseries agent eliminations, seen solving series local multi-objectivesubproblems. Variable elimination linear support (VELS) iteratively identifies singleweight vector w lead maximal possible improvement partial CCScalls variable elimination solve scalarized instance problem w. VELSfaster CMOVE small medium numbers objectives compute-approximate CCS fraction runtime. addition, propose variantsmethods employ AND/OR tree search instead variable elimination achievememory efficiency. analyze runtime space complexities methods, provecorrectness, compare empirically naive baseline existingPCS method, terms memory-usage runtime. results show that,focusing CCS, methods achieve much better scalability numberagents current state art.1. Introductionmany real-world problem domains, maintenance planning (Scharpff, Spaan,Volker, & De Weerdt, 2013) traffic light control (Pham et al., 2013), multiple agentsneed coordinate actions order maximize common utility. Key coordinatingefficiently domains exploiting loose couplings agents (Guestrin, Koller,& Parr, 2002; Kok & Vlassis, 2004): agents actions directly affect subsetagents.Multi-agent coordination complicated fact that, many domains, agents needbalance multiple objectives (Roijers, Vamplew, Whiteson, & Dazeley, 2013a). example, agents might maximize performance computer network minimizingpower consumption (Tesauro, Das, Chan, Kephart, Lefurgy, Levine, & Rawson, 2007),maximize cost efficiency maintenance tasks road network minimizing trafficdelays (Roijers, Scharpff, Spaan, Oliehoek, de Weerdt, & Whiteson, 2014).c2015AI Access Foundation. rights reserved.fiRoijers, Whiteson, & OliehoekFigure 1: Mining company example.However, presence multiple objectives per se necessitate usespecialized multi-objective solution methods. problem scalarized, i.e.,utility function converted scalar utility function, problem may solvableexisting single-objective methods. conversion involves two steps (Roijers et al.,2013a). first step specify scalarization function.Definition 1. scalarization function f , function maps multi-objective utilitysolution decision problem, u(a), scalar utility uw (a):uw (a) = f (u(a), w),w weight vector parameterizes f .second step define single-objective version decision problemutility solution equals scalarized utility original problem uw (a).Unfortunately, scalarizing problem solving always possiblew may known advance. example, consider company mines differentresources. Figure 1, depict problem company faces: morning one vanper village needs transport workers village nearby mine, variousresources mined. Different mines yield different quantities resource per worker.market prices per resource vary stochastic process every price changealter optimal assignment vans. expected price variation increasespassage time. maximize performance, thus critical act based latestpossible price information. Since computing optimal van assignment takes time, redoingcomputation every price change highly undesirable.settings, need multi-objective method computes, advance, optimal solution possible prices, w. call set coverage set (CS). manycases, w revealed solution must executed, case solutionautomatically selected CS given w. cases, w never made explicitinstead human involved decision making selects one solutionCS, perhaps basis constraints preferences difficult formalizeobjectives (Roijers et al., 2013a). cases, CS typicallymuch smaller complete set solutions, selecting optimal joint actionCS typically much easier selecting directly complete set solutions.400fiComputing CCSs Faster Multi-objective Coordinationarticle, consider multi-objective methods made efficient problems require coordination multiple, loosely coupled agents. particular, address multi-objective coordination graphs (MO-CoGs): one-shot multi-agent decision problems loose couplings expressed using graphical model. MO-CoGs formimportant class decision problems. used model variety realworld problems (Delle Fave, Stranders, Rogers, & Jennings, 2011; Marinescu, 2011; Rollon,2008), many sequential decision problems modeled series MO-CoGs,common single-objective problems (Guestrin et al., 2002; Kok & Vlassis, 2004; Oliehoek,Spaan, Dibangoye, & Amato, 2010).Key efficiency MO-CoG methods propose compute convexcoverage set (CCS) instead Pareto coverage set (PCS). CCS subsetPCS sufficient solution multi-objective problem linear scalarizationfunction. example, mining company example Figure 1, f linear, sincetotal revenue simply sum quantity resource mined times price perunit. However, even f nonlinear, stochastic solutions allowed, CCSsufficient.1CCS previously considered solution concept MO-CoGscomputing CCS requires running linear programs, whilst computing PCS requirespairwise comparisons solutions. However, key insight article2 that, looselycoupled systems, CCSs easier compute PCSs, two reasons. First, CCS(typically much smaller) subset PCS. loosely coupled settings, efficient methodswork solving series local subproblems; focusing CCS greatly reduce sizesubproblems. Second, focusing CCS makes solving MO-CoG equivalentfinding optimal piecewise-linear convex (PWLC) scalarized value function,efficient techniques adapted. reasons, argue CCS oftenconcept choice MO-CoGs.propose two approaches exploit insights solve MO-CoGs efficientlyexisting methods (Delle Fave et al., 2011; Dubus, Gonzales, & Perny, 2009; Marinescu,Razak, & Wilson, 2012; Rollon & Larrosa, 2006). first approach deals multipleobjectives level individual agents, second deals globallevel.first approach extends algorithm Rollon Larrosa (2006) referPareto multi-objective variable elimination (PMOVE) 3 , computes local Paretosets agent elimination, compute CCS instead. call resulting algorithmconvex multi-objective variable elimination (CMOVE).second approach new abstract algorithm call optimistic linear support(OLS) much faster small medium numbers objectives. Furthermore, OLS1. precise, case stochastic strategies CCS deterministic strategies always sufficient(Vamplew, Dazeley, Barker, & Kelarev, 2009); case deterministic strategies, linearityscalarization function makes CCS sufficient (Roijers et al., 2013a).2. article synthesizes extends research already reported two conference papers. Specifically,CMOVE algorithm (Section 4) previously published ADT (Roijers, Whiteson, & Oliehoek,2013b) VELS algorithm (Section 5) AAMAS (Roijers, Whiteson, & Oliehoek, 2014).memory-efficient methods computing CCSs (Section 6) novel contribution article.3. original article, algorithm called multi-objective bucket elimination (MOBE). However,use PMOVE consistent names algorithms mentioned article.401fiRoijers, Whiteson, & Oliehoekused produce bounded approximation CCS, -CCS,enough time compute full CCS. OLS generic method employs single-objectivesolvers subroutine. article, consider two implementations subroutine.Using variable elimination (VE) subroutine yields variable elimination linear support(VELS), particularly fast small moderate numbers objectivesmemory-efficient CMOVE. However, memory highly limited, reductionmemory usage may enough. cases, using AND/OR search (Mateescu &Dechter, 2005) instead yields AND/OR tree search linear support (TSLS),slower VELS much memory efficient.prove correctness CMOVE OLS. analyze runtime spacecomplexities methods show methods better guaranteesPCS methods. show CMOVE OLS complementary, i.e., various trade-offs existvariants.Furthermore, demonstrate empirically, randomized realistic problems, CMOVE VELS scale much better previous algorithms. also empirically confirm trade-offs CMOVE OLS. show OLS, usedbounded approximation algorithm, save additional orders magnitude runtime,even small . Finally, show that, even memory highly limited, TSLSstill solve large problems.rest article structured follows. First, provide formal definitionmodel, well overview existing solution methods Section 2.presenting naive approach Section 3, Sections 4, 5 6, analyze runtimespace complexities algorithm, compare empirically,existing algorithms, end section. Finally, conclude Section 7overview contributions findings, suggestions future research.2. Backgroundsection, formalize multi-objective coordination graph (MO-CoG).however, describe single-objective version problem, coordination graph(CoG), MO-CoG extension, variable elimination (VE) algorithmsolving CoGs. methods present Section 4 5 build different ways.2.1 (Single-Objective) Coordination Graphscoordination graph (CoG) (Guestrin et al., 2002; Kok & Vlassis, 2004) tuple hD, A, Ui,= {1, ..., n} set n agents,= Ai ... joint action space: Cartesian product finite actionspaces agents. joint action thus tuple containing action agent= ha1 , ..., i,U = u1 , ..., u set scalar local payoff functions, limitedscope, i.e., depends onlyPa subset agents. total team payoff sumlocal payoffs: u(a) = e=1 ue (ae ).402fiComputing CCSs Faster Multi-objective CoordinationFigure 2: (a) CoG 3 agents 2 local payoff functions (b) eliminating agent 3adding u3 (c) eliminating agent 2 adding u4 .a1a1a23.251.25a203.75a2a2a32.50a31.51Table 1: payoff matrices u1 (a1 , a2 ) (left) u2 (a2 , a3 ) (right). two possibleactions per agent, denoted dot (a1 ) bar (a1 ).agents share payoff function u(a). abuse notation e index localpayoff function ue denote subset agents scope; ae thus local jointaction, i.e., joint action subset agents.decomposition u(a) local payoff functions represented factorgraph (Bishop, 2006), bipartite graph containing two types vertices: agents (variables)local payoff functions (factors), edges connecting local payoff functionsagents scope.Figure 2a shows factor graph example CoG team payoff functiondecomposes two local payoff functions, two agents scope:u(a) =Xue (ae ) = u1 (a1 , a2 ) + u2 (a2 , a3 ).e=1local payoff functions defined Table 1. factor graph illustrates loosecouplings result decomposition local payoff functions. particular,agents choice action directly depends immediate neighbors, e.g.,agent 1 knows agent 2s action, choose action without considering agent 3.2.2 Variable Eliminationdiscuss variable elimination (VE) algorithm, several multi-objectiveextensions (Rollon & Larrosa, 2006; Rollon, 2008) build, including CMOVE algorithm (Section 4). also use subroutine OLS algorithm (Section 5).exploits loose couplings expressed local payoff functions efficientlycompute optimal joint action, i.e., joint action maximizing u(a). First, forward403fiRoijers, Whiteson, & Oliehoekpass, eliminates agents turn computing value agents bestresponse every possible joint action neighbors. values used constructnew local payoff function encodes value best response replaces agentpayoff functions participated. original algorithm, agentseliminated, backward pass assembles optimal joint action using constructedpayoff functions. Here, present slight variant payoff taggedaction generates it, obviating need backwards pass. two algorithmsequivalent, variant amenable multi-objective extension presentSection 4.eliminates agents graph predetermined order. Algorithm 1 showspseudocode elimination single agent i. First, determines set localpayoff functions connected i, Ui , neighboring agents i, ni (lines 1-2).Definition 2. set neighboring local payoff functions Ui set localpayoff functions agent scope.Definition 3. set neighboring agents i, ni , set agentsscope one local payoff functions Ui .Then, constructs new payoff function computing value agent bestresponse possible joint action ani agents ni (lines 3-12). so,loops joint actions Ani (line 4). ani , loops actions Aiavailable agent (line 6). ai Ai , computes local payoff agentresponds ani ai (line 7). tags total payoff ai , action generates(line 8) order able retrieve optimal joint action later. alreadytags present, appends ai them; way, entire joint action incrementallyconstructed. maintains value best response taking maximumpayoffs (line 11). Finally, eliminates agent payoff functions Ui replacesnewly constructed local payoff function (line 13).Algorithm 1: elimVE(U, i)1234567Input: CoG U, agentUi set local payoff functions involvingni set neighboring agentsunew new factor taking joint actions ni , ani , inputforeach ani Aniforeach aXAivuj (ani , ai )uj Ui8910111213tag v ai{v}endunew (ani ) max(S)endreturn (U \ Ui ) {unew }404fiComputing CCSs Faster Multi-objective CoordinationConsider example Figure 2a Table 1. optimal payoff maximizes sumtwo payoff functions:max u(a) = max u1 (a1 , a2 ) + u2 (a2 , a3 ).a1 ,a2 ,a3eliminates agent 3 first, pushes maximization a3 inwardgoes local payoff functions involving agent 3, case u2 :12max u(a) = max u (a1 , a2 ) + max u (a2 , a3 ) .a1 ,a2a3solves inner maximization replaces new local payoff function u3depends agent 3s neighbors, thereby eliminating agent 1:max u(a) = max u1 (a1 , a2 ) + u3 (a2 ) ,a1 ,a2leads new factor graph depicted Figure 2b. values u3 (a2 ) u3 (a2 ) =2.5, using a3 , u3 (a2 ) = 1 using a3 , optimal payoffs actionsagent 2, given payoffs shown Table 1. ultimately want optimal jointaction, optimal payoff, tags payoff u3 action agent 3generates it, i.e., think u3 (a2 ) (value, tag) pair. denote pairparentheses subscript: u3 (a2 ) = (2.5)a3 , u3 (a2 ) = (1)a3 .next eliminates agent 2, yielding factor graph shown Figure 2c:13max u(a) = max max u (a1 , a2 ) + u (a2 ) = max u4 (a1 ).a1a2a1appends new tags agent 2 existing tags agent 3, yielding followingtagged payoff values: u4 (a1 ) = maxa2 u1 (a1 , a2 ) + u3 (a2 ) =(3.25)a2 + (2.5)a2 a3 = (5.75)a2 a3u4 (a1 ) = (3.75)a2 + (1)a2 a3 = (4.75)a2 a3 . Finally, maximizing a1 yields optimalpayoff (5.75)a1 a2 a3 , optimal action contained tags.runtime complexity exponential, number agents,induced width, often much less number agents.Theorem 1. computational complexity O(n|Amax |w ) |Amax |maximal number actions single agent w induced width, i.e., maximalnumber neighboring agents agent plus one (the agent ), momenteliminated (Guestrin et al., 2002).Theorem 2. space complexity O( n |Amax |w ).space complexity arises because, every agent elimination, new local payofffunction created O(|Amax |w ) fields (possible input actions). Since impossibletell priori many new local payoff functions exist given timeexecution VE, need multiplied total number new local payofffunctions created execution, n.designed minimize runtime4 methods focus memory efficiencyinstead (Mateescu & Dechter, 2005). discuss memory efficiency Section 6.1.4. fact, proven best runtime guarantees within large class algorithms (Rosenthal,1977).405fiRoijers, Whiteson, & Oliehoeka1a1a2(4,1)(1,2)a2(0,0)(3,6)a2a2a3(3,1)(0,0)a3(1,3)(1,1)Table 2: two-dimensional payoff matrices u1 (a1 , a2 ) (left) u2 (a2 , a3 ) (right).2.3 Multi-objective Coordination Graphsmulti-objective coordinationgraph (MO-CoG) tuple hD, A, Uibut, U = u1 , ..., u set , d-dimensional local Ppayoff functions.total team payoff sum local vector-valued payoffs: u(a) = e=1 ue (ae ). useui indicate value i-th objective. denote set possible joint actionvalues V. Table 2 shows two-dimensional MO-CoG structuresingle-objective example Section 2.1, multi-objective payoffs.solution MO-CoG coverage set (CS) joint actions associated valuesu(a) contains least one optimal joint action possible parameter vector wscalarization function f (Definition 1). CS subset undominated set:Definition 4. undominated set (U) MO-CoG, set joint actionsassociated payoff values optimal w scalarization function f .U (V) = u(a) : u(a) V wa0 uw (a) uw (a0 ) .care least one optimal joint action every w, ratheroptimal joint actions, lossless subset U suffices:Definition 5. coverage set (CS), CS(V), subset U , possible w,least one optimal solution CS, i.e.,wau(a) CS(V) a0 uw (a) uw (a0 ) .Note CS necessarily unique. Typically seek smallest possible CS.convenience, assume payoff vectors CS contain values associatedjoint actions, suggested tagging scheme described Section 2.2.payoff vectors V CS depends knowscalarization function f . minimal assumption f monotonically increasing, i.e.,value one objective ui , increases uj6=i stay constant, scalarized valueu(a) cannot decrease. assumption ensures objectives desirable, i.e., elseequal, always better.Definition 6. Pareto front undominated set arbitrary strictly monotonicallyincreasing scalarization functions f .P F (V) = u(a) : u(a) V a0 u(a0 ) P u(a) ,P indicates Pareto dominance (P-dominance): greater equal objectivesstrictly greater least one objective.406fiComputing CCSs Faster Multi-objective Coordinationorder optimal scalarized values, necessary compute entirePF. E.g., two joint actions equal payoffs need retain one those.Definition 7. Pareto coverage set (PCS), P CS(V) P F (V), coverage setarbitrary strictly monotonically increasing scalarization functions f , i.e.,a0 u(a) P CS(V) (u(a) P u(a0 ) u(a) = u(a0 )) .Computing P-dominance requires pairwise comparison payoff vectors (Feng &Zilberstein, 2004).5highly prevalent scenario that, addition f monotonically increasing,also know linear, is, parameter vectors w weightsvalues individual objectives multiplied, f = w u(a). mining exampleFigure 1, resources traded open market resources positive unitprice. case, scalarization linear combination amount resourcemined, weights correspond price per unit resource. Manyexamples linear scalarization functions exist literature, e.g., (Lizotte, Bowling, &Murphy, 2010). assume linear scalarization monotonically increasing,represent without loss generality convex combination objectives: i.e.,weights positive sum 1. case, convex coverage set (CCS)needed, subset convex hull (CH) 6 :Definition 8. convex hull (CH) undominated set linear non-decreasingscalarizations f (u(a), w) = w u(a):CH(V) = u(a) : u(a) V wa0 w u(a) w u(a0 ) .is, CH contains solutions attain optimal value least one weight.Vectors CH C-dominated. contrast P-domination, C-domination cannottested pairwise comparisons take two payoff vectorsC-dominate payoff vector. Note CH contains solutions needed guarantee optimal scalarized value value: contain multiple solutions optimalone specific weight. lossless subset CH respect linear scalarizationscalled convex coverage set (CCS), i.e., CCS retains least one u(a) maximizesscalarized payoff, w u(a), every w:Definition 9. convex coverage set (CCS), CCS(V) CH(V), CS linear nondecreasing scalarizations, i.e.,wa u(a) CCS(V) a0 w u(a) w u(a0 ) .Since linear non-decreasing functions specific type monotonically increasing function, always CCS subset smallest possible PCS.previously mentioned, CSs like PCS CCS, may unique. example,two joint actions equal payoff vectors, need onemake PCS CCS.5. P-dominance often called pairwise dominance POMDP literature.6. Note term convex hull overloaded. graphics, convex hull superset meanconvex hull article.407fiRoijers, Whiteson, & OliehoekFigure 3: CCS (filled circles left, solid black lines right) versus PCS (filled circlessquares left, dashed solid black lines right) twelve random2-dimensional payoff vectors.practice, PCS CCS often equal PF CH. However,algorithms proposed article guaranteed produce PCS CCS,necessarily entire PF CH. PCSs CCSs sufficient solutionsterms scalarized value, say algorithms solve MO-CoGs.Figure 3 (left) values joint actions, u(a), represented points valuespace, two-objective MO-CoG. joint action value CCSPCS. B, however, PCS, CCS, weightlinear scalarization Bs value would optimal, shown Figure 3 (right),scalarized value strategies plotted function weight first objective(w2 = 1 w1 ). C neither CCS PCS: Pareto-dominated A.Many multi-objective methods, e.g., (Delle Fave et al., 2011; Dubus et al., 2009; Marinescu et al., 2012; Rollon, 2008) simply assume PCS appropriate solutionconcept. However, argue choice CS depends one assumeutility defined respect multiple objectives, i.e., scalarization function used scalarize vector-valued payoffs. argue many situationsscalarization function linear, cases one use CCS.addition shape f , choice solution concept depends whetherdeterministic joint actions considered whether stochastic strategies also permitted. stochastic strategy assigns probabilityPto joint action [0, 1].probabilities joint actions together sum 1, aA (a) = 1. value stochastic strategy linearP combination value vectors joint actionsmixture: u =aA (a)u(a). Therefore, optimal values, monotonicallyincreasing f , lie convex upper surface spanned strategies CCS,indicated lines Figure 3 (left). Therefore, optimal values monotonicallyincreasing f , including nonlinear ones, constructed taking mixture policiesCCS (Vamplew et al., 2009).article considers methods computing CCSs, which, show Sections 45, computed efficiently PCSs. Furthermore, CCSs typically much408fiComputing CCSs Faster Multi-objective Coordinationsmaller. particularly important final selection joint done (agroup of) humans, compare possible alternatives solution set.methods presented article based variable elimination (VE) (Sections4 5) AND/OR tree search (TS) (Section 6). algorithms exact solutionmethods CoGs.CMOVE algorithm propose Section 5 based VE. differs anothermulti-objective algorithm based VE, refer PMOVE (Rollon & Larrosa,2006), produces CCS rather PCS. alternative messagepassing algorithms, like max-plus (Pearl, 1988; Kok & Vlassis, 2006a). However,guaranteed exact tree-structured CoGs. Multi-objective methods buildmax-plus Delle Fave et al. (2011), limitation, unlesspreprocess CoG form clique-tree GAI network (Dubus et al., 2009).tree structured graphs, message-passing algorithms produce optimal solutionssimilar runtime guarantees. Note that, like PMOVE, existing multi-objective methodsbased message passing produce PCS rather CCS.Section 5, take different approach multi-objective coordination basedouter loop approach. explain, approach applicable computing CCS,PCS, considerable advantages terms runtime memory usage.3. Non-graphical Approachnaive way compute CCS ignore graphical structure, calculate setpossible payoffs joint actions V, prune away C-dominated joint actions.first translate problem set value set factors (VSFs), F. VSF f functionmapping local joint actions sets payoff vectors. initial VSFs constructedlocal payoff functionsf e (ae ) = {ue (ae )},i.e., VSF maps local joint action singleton set containing actionslocal payoff. define V terms F using cross-sum operator VSFsF joint action a:[MV(F) =f e (ae ),f e Fcross-sum two sets B contains possible vectors madesumming one payoff vector set:B = {a + b : b B} .CCS calculated applying pruning operator CPrune (described below)removes C-dominated vectors set value vectors, V:[MCCS(V(F)) = CPrune(V(F)) = CPrune(f e (ae )).(1)f e Fnon-graphical CCS algorithm simply computes righthand side Equation 1, i.e.,computes V(F) explicitly looping actions, action loopinglocal VSFs, pruning set CCS.409fiRoijers, Whiteson, & OliehoekCCS contains least one payoff vector maximizes scalarized value everyw:w= arg max w u(a)= a0u(a0 ) CCS(V(F)) w u(a) = w u(a0 ). (2)aAis, every w solution a0 part CCS achievesvalue maximizing solution a. Moreover value solutions givendot product. Thus, finding CCS analogous problem faced partially observableMarkov decision processes (POMDPs) (Feng & Zilberstein, 2004), optimal -vectors(corresponding value vectors u(a)) beliefs (corresponding weight vectorsw) must found. Therefore, employ pruning operators POMDP literature.Algorithm 2 describes implementation CPrune, based FengZilberstein (2004) one modification. order improve runtime guarantees, CPrunefirst pre-prunes candidate solutions U PCS using PPrune (Algorithm 3) line1. PPrune computes PCS O(d|U||P CS|) running pairwise comparisons. Next,partial CCS, U , constructed follows: random vector u U selected line 4.u algorithm tries find weight vector w u better vectorsU (line 5), solving linear program Algorithm 4. w, CPrunefinds best vector v w U moves U (line 1113). weightu better C-dominated thus removed u U (line 8).Algorithm 2: CPrune(U)12345678910111213141516Input: set payoff vectors UU PPrune(U)UnotEmpty(U)select random u Uw findWeight(u, U )w=null//did find weight u optimalremove u Uendelsev arg maxuU w uU U \ {v}U U {v}endendreturn Uruntime CPrune defined Algorithm 2O(d|U||P CS| + |P CS|P (d|CCS|)),(3)P (d|CCS|) polynomial size CCS number objectives d,runtime linear program tests C-domination (Algorithm 4).410fiComputing CCSs Faster Multi-objective CoordinationAlgorithm 3: PPrune(U)123456789101112Input: set payoff vectors UUU 6=u first element Uforeach v Uv P uu v // Continue v instead uendendRemove u, vectors P-dominated u, UAdd u Uendreturn UAlgorithm 4: findWeight(u, U)max xx,wsubject w (u u0 ) x 0, u0 UXwi = 1i=1x > 0 return w else return nullkey downside non-graphical approach requires explicitly enumeratingpossible joint actions calculating payoffs associated one. Consequently,intractable small numbers agents, number joint actions growsexponentially number agents.Theorem 3. time complexity computing CCS MO-CoG containing localpayoff functions, following non-graphical approach (Equation 1) is:O(d|Amax |n + d|Amax |n |P CS| + |P CS|P (d|CCS|)Proof. First, V computed looping VSFs joint action a, summingvectors length d. maximum size action space agent AmaxO(|Amax |n ) joint actions. V contains one payoff vector joint action. V inputCPrune.next two sections, present two approaches compute CCSs efficiently.first approach pushed CPrune operator Equation 1 cross-sumunion, max-operator pushed summation VE. callinner loop approach, uses pruning operators agent eliminations,inner loop algorithm. second approach inspired linear support (Cheng,411fiRoijers, Whiteson, & Oliehoek1988), POMDP pruning operator requires finding optimal solution certain w. Instead performing maximization entire set V, original linearsupport algorithm, show use finite number scalarized instancesMO-CoG, avoiding explicit calculation V. call approach outer loopapproach, creates outer loop around single objective method (like VE),calls subroutine.4. Convex Variable Elimination MO-CoGssection show exploit loose couplings calculate CCS using inner loop approach, i.e., pushing pruning operators cross-sum unionoperators Equation 1. result CMOVE, extension Rollon LarrosasPareto-based extension VE, refer PMOVE (Rollon & Larrosa, 2006).analyzing CMOVEs complexity terms local convex coverage sets, showapproach yields much better runtime complexity guarantees non-graphicalapproach computing CCSs presented Section 3.4.1 Exploiting Loose Couplings Inner Loopnon-graphical approach, computing CCS expensive computing PCS,shown Section 3. show that, MO-CoGs, compute CCSmuch efficiently exploiting MO-CoGs graphical structure. particular, likeVE, solve MO-CoG series local subproblems, eliminating agentsmanipulating set VSFs F describe MO-CoG. key idea computelocal CCSs (LCCSs) eliminating agent instead single best response (as VE).computing LCCS, algorithm prunes away many vectors possible.minimizes number payoff vectors calculated global level,greatly reduce computation time. describe elim operator eliminating agentsused CMOVE Section 4.2.first need update definition neighboring local payoff functions (Definition2), neighboring VSFs.Definition 10. set neighboring VSFs Fi set local payoff functionsagent scope.neighboring agents ni agent agents scope VSFFi , except itself, corresponding Definition 3. possible local joint actionni , compute LCCS contains payoffs C-undominated responsesagent i, best response values i. words, CCS subproblemarises considering Fi fixing specific local joint action ani . computeLCCS, must consider payoff vectors subproblem, Vi , prune dominatedones.Definition 11. fix actionsani , ai , set payoff vectorsLsubproblem is: Vi (Fi , ani ) = ai f e Fi f e (ae ), ae formed aiappropriate part ani .Using Definition 11, define LCCS CCS Vi .412fiComputing CCSs Faster Multi-objective CoordinationDefinition 12. local CCS, LCCS, C-undominated subset Vi (Fi , ani ):LCCSi (Fi , ani ) = CCS(Vi (Fi , ani )).Using LCCSs, create new VSF, f new , conditioned actionsagents ni :ani f new (ani ) = LCCS (Fi , ani ).elim operator replaces VSFs Fi F new factor:elim(F, i) = (F \ Fi ) {f new (ani )}.Theorem 4. elim preserves CCS: F CCS(V(F)) = CCS(V(elim(F, i))).Proof. show using implication Equation 2, i.e., joint actionsw scalarized value maximal, vector-valued payoffu(a0 ) w u(a0 ) = w u(a0 ) CCS. show maximal scalarizedpayoff cannot lost result elim.function distributes local payoff functions: w u(a) =P linear scalarizationPw e ue (ae ) = e w ue (ae ). Thus, eliminating agent i, divide set VSFsnon-neighbors (nn), agent participate, neighbors (ni )that:XXw u(a) =w ue (ae ) +w ue (ae ).enneniNow, following Equation 2, CCS contains maxaA w u(a) w. elim pushesmaximization in:max w u(a) = maxaAai AiXw ue (ae ) + maxai AiennXw ue (ae ).enielimagent-i factors term f new (ani ) satisfies w f new (ani ) = maxaiP replaceseeni w u (ae ) per definition, thus preserving maximum scalarized value wthereby preserving CCS.Instead LCCS, could compute local PCS (LPCS), is, using PCScomputation Vi instead CCS computation. Note that, since LCCS LPCS Vi ,elim reduces problem size respect Vi , wouldpossible considered P-dominance. Therefore, focusing CCS greatlyreduce sizes local subproblems. Since solution local subproblem inputnext agent elimination, size subsequent local subproblems also reduced,lead considerable speed-ups.413fiRoijers, Whiteson, & Oliehoek4.2 Convex Multi-objective Variable Eliminationpresent convex multi-objective variable elimination (CMOVE) algorithm,implements elim using CPrune. Like VE, CMOVE iteratively eliminates agents noneleft. However, implementation elim computes CCS outputs correctjoint actions payoff vector CCS, rather single joint action. CMOVEextension Rollon Larrosas Pareto-based extension VE, referPMOVE (Rollon & Larrosa, 2006).important difference CMOVE PMOVE CMOVE computes CCS, typically leads much smaller subproblems thus much bettercomputational efficiency. addition, identify three places pruning takeplace, yielding flexible algorithm different trade-offs. Finally, use tagging scheme instead backwards pass, Section 2.2.Algorithm 5 presents abstract version CMOVE leaves pruning operatorsunspecified. Section 3, CMOVE first translates problem set vector-setfactors (VSFs), F line 1. Next, CMOVE iteratively eliminates agents using elim (line25). elimination order determined using techniques devised single-objective(Koller & Friedman, 2009).Algorithm 5: CMOVE(U, prune1, prune2, prune3, q)12345678Input: set local payoff functions U elimination order q (a queue containingagents)F create one VSF every local payoff function Uani Aniq.dequeue()F elim(F, i, prune1, prune2)endf retrieve final factor Ff (a )return prune3(S)Algorithm 6 shows implementation elim, parameterized two pruning operators, prune1 prune2, corresponding two different pruning locations insideoperator computes LCCSi : ComputeLCCSi (Fi , ani , prune1, prune2).Algorithm 6: elim(F, i, prune1, prune2)12345678Input: set VSFs F, agentni set neighboring agentsFi subset VSF scopef new (ani ) new VSFforeach ani Anif new (ani ) ComputeLCCS (Fi , ani , prune1, prune2)endF F \ Fi {f new }return F414fiComputing CCSs Faster Multi-objective CoordinationComputeLCCSi implemented follows: first define new cross-sum-and-prune= prune1(A B). LCCSi applies operator sequentially:operator AB[Mf e (ae )).(4)ComputeLCCSi (Fi , ani , prune1, prune2) = prune2(eaif Fioperator, leading incrementalprune1 applied cross-sum two sets, viapruning (Cassandra, Littman, & Zhang, 1997). prune2 applied coarser level,union. CMOVE applies elim iteratively agents remain, resulting CCS.Note that, agents left, f new line 3 agents condition on.case, consider actions neighbors single empty action: .Pruning also applied end, agents eliminated,call prune3. increasing level coarseness, thus three pruning operators: incremental pruning (prune1), pruning union actions eliminatedagent (prune2), pruning agents eliminated (prune3), reflectedAlgorithm 5. agents eliminated, final factor taken setfactors (line 6), single set, contained factor retrieved (line 7). Noteuse empty action denote field final factor, agentsscope. Finally prune3 called S.Consider example Figure 2a, using payoffs defined Table 2, applyCMOVE. First, CMOVE creates VSFs f 1 f 2 u1 u2 . eliminate agent 3,creates new VSF f 3 (a2 ) computing LCCSs every a2 tagging elementset action agent 3 generates it. a2 , CMOVE first generatesset {(3, 1)a3 , (1, 3)a3 }. Since vectors optimal w, neitherremoved pruning thus f 3 (a2 ) = {(3, 1)a3 , (1, 3)a3 }. a2 , CMOVE first generates{(0, 0)a3 , (1, 1)a3 }. CPrune determines (0, 0)a3 dominated consequently removesit, yielding f 3 (a2 ) = {(1, 1)a3 }. CMOVE adds f 3 graph removes f 2agent 3, yielding factor graph shown Figure 2b.CMOVE eliminates agent 2 combining f 1 f 3 create f 4 . f 4 (a1 ),CMOVE must calculate LCCS of:(f 1 (a1 , a2 ) f 3 (a2 )) (f 1 (a1 , a2 ) f 3 (a2 )).first cross sum yields {(7, 2)a2 a3 , (5, 4)a2 a3 } second yields {(1, 1)a2 a3 }. Pruningunion yields f 4 (a1 ) = {(7, 2)a2 a3 , (5, 4)a2 a3 }. Similarly, a1 taking union yields{(4, 3)a2 a3 , (2, 5)a2 a3 , (4, 7)a2 a3 }, LCCS f 4 (a1 ) = {(4, 7)a2 a3 }. Adding f 4results graph Figure 2c.Finally, CMOVE eliminates agent 1. Since neighboring agents left, Aicontains empty action. CMOVE takes union f 4 (a1 ) f 4 (a1 ). Since(7, 2){a1 a2 a3 } (4, 7){a1 a2 a3 } dominate (5, 4){a1 a2 a3 } , latter pruned, leaving CCS ={(7, 2){a1 a2 a3 } , (4, 7){a1 a2 a3 } }.4.3 CMOVE Variantsseveral ways implement pruning operators lead correct instantiations CMOVE. PPrune (Algorithm 2) CPrune (Algorithm 1) used,long either prune2 prune3 CPrune. Note prune2 computes CCS, prune3necessary.415fiRoijers, Whiteson, & Oliehoekarticle, consider Basic CMOVE, use prune1 prune3prunes prune2 using CPrune, well Incremental CMOVE, uses CPruneprune1 prune2. latter invests effort intermediate pruning,result smaller cross-sums, resulting speedup. However, vectorspruned intermediate steps, additional speedup may occur,algorithm creates unnecessary overhead.7 empirically investigate variantsSection 4.5One could also consider using pruning operators contain prior knowledgerange possible weight vectors. information available, could easilyincorporated changing pruning operators accordingly, leading even smaller LCCSs,thus faster algorithm. article however, focus caseprior knowledge available.4.4 Analysisanalyze correctness complexity CMOVE.Theorem 5. MOVE correctly computes CCS.Proof. proof works induction number agents. base case originalMO-CoG, f e (ae ) F singleton set. Then, since elim preserves CCS(see Theorem 1), necessary vectors lost. last agent eliminated,one factor remains; since conditioned agent actions resultLCCS computation, must contain one set: CCS.Theorem 6. computational complexity CMOVEO( n |Amax |wa (wf R1 + R2 ) + R3 ),(5)wa induced agent width, i.e., maximum number neighboring agents (connected via factors) agent eliminated, wf induced factor width, i.e.,maximum number neighboring factors agent eliminated, R1 , R2 R3cost applying prune1, prune2 prune3 operators.Proof. CMOVE eliminates n agents one computes LCCS jointaction eliminated agents neighbors, field new VSF. CMOVE computesO(|Amax |wa ) fields per iteration, calling prune1 (Equation 4) adjacent factor,prune2 taking union actions eliminated agent. prune3 calledexactly once, eliminating agents (line 8 Algorithm 5).Unlike non-graphical approach, CMOVE exponential wa , numberagents. respect, results similar PMOVE (Rollon, 2008).However, earlier complexity results make effect pruning explicit. Instead,complexity bound makes use additional problem constraints, limit totalnumber possible different value vectors. Specifically, analysis PMOVE,payoff vectors integer-valued, maximum value objectives. practice,7. also compute PCS first, using prune1 prune2, compute CCS prune3.However, useful small problems PCS cheaper compute CCS.416fiComputing CCSs Faster Multi-objective Coordinationbounds loose even impossible define (e.g., payoff valuesreal-valued one objectives). Therefore, instead give descriptioncomputational complexity makes explicit dependence effectivenesspruning. Even though complexity bounds better worst case (i.e.,pruning possible), allow greater insight runtimes algorithmsevaluate, apparent analysis experimental results Section 4.5.Theorem 6 demonstrates complexity CMOVE depends heavily runtimepruning operators, turn depends sizes input sets. inputset prune2 union returned series applications prune1,prune3 uses output last application prune2. therefore need balanceeffort lower-level pruning higher-level pruning, occurs lessoften dependent output lower level. bigger LCCSs,gained lower-level pruning.Theorem 7. space complexity CMOVEO( n |Amax |wa |LCCSmax | + |Amax ||emax | ),|LCCSmax | maximum size local CCS, original number VSFs,|emax | maximum scope size original VSFs.Proof. CMOVE computes local CCS new VSF joint action eliminated agents neighbors. maximally wa neighbors. maximally n newfactors. payoff vector stores real numbers.VSFs created initialization CMOVE. VSFsexactly one payoff vector containing real numbers, per joint action agents scope.maximally |Amax ||emax | joint actions.PMOVE, space complexity |P CCSmax | instead |LCCSmax |.LCCS subset corresponding LPCS, CMOVE thus strictlymemory efficient PMOVE.Note Theorem 7 rather loose upper bound space complexity,VSFs, original new, exist time. However, possible predictpriori many VSFs exist time, resulting space complexitybound basis VSFs exist point execution CMOVE.4.5 Empirical Evaluationtest efficiency CMOVE, compare runtimes PMOVE8non-graphical approach problems varying numbers agents objectives.also analyze runtimes correspond sizes PCS CCS.use two types experiments. first experiments done random MOCoGs directly control variables. second experiment, useMining Day, realistic benchmark, structured random MO-CoGsstill randomized.8. compare PMOVE using prune2 = PPrune, rather prune1 = prune2 = PPrune,proposed original article (Rollon & Larrosa, 2006) found former option slightlyconsistently faster.417fiRoijers, Whiteson, & Oliehoek(a)(b)(c)Figure 4: (a) Runtimes (ms) log-scale nongraphical method, PMOVE CMOVEstandard deviation mean (error bars), (b) corresponding number vectorsPCS CCS, (c) corresponding spread induced width.4.5.1 Random Graphsgenerate random MO-CoGs, employ procedure takes input: n, numberagents; d, number payoff dimensions; number local payoff functions;|Ai |, action space size agents, agents. procedurestarts fully connected graph local payoff functions connecting two agentseach. Then, local payoff functions randomly removed, ensuring graphremains connected, local payoff functions remain. values differentobjectives local payoff function real numbers drawn independentlyuniformly interval [0, 10]. compare algorithms set randomlygenerated MO-CoGs separate value n, d, , |Ai |.compare basic CMOVE, incremental CMOVE, PMOVE, non-graphicalmethod, test random MO-CoGs number agents ranging10 85, average number factors per agent held = 1.5n, numberobjectives = 2. experiment run 2.4 GHz Intel Core i5 computer, 4 GBmemory. Figure 4 shows results, averaged 20 MO-CoGs number agents.runtime (Figure 4a) non-graphical method quickly explodes. CMOVEvariants slower PMOVE small numbers agents, runtime grows muchslowly PMOVE. 70 agents, CMOVE variants fasterPMOVE average. 75 agents, one MO-CoGs generated caused PMOVEtime 5000s, basic CMOVE maximum runtime 132s, incrementalCMOVE 136s. explained differences size solutions, i.e.,PCS CCS (Figure 4b). PCS grows much quickly numberagents CCS does. two-objective problems, incremental CMOVE seemsconsistently slower basic CMOVE.CMOVEs runtime grows much slowly nongraphical method,still exponential number agents, counterintuitive result since worst-casecomplexity linear number agents. explained induced widthMO-CoGs, runtime CMOVE exponential. Figure 4c, seeinduced width increases linearly number agents random graphs.418fiComputing CCSs Faster Multi-objective CoordinationFigure 5: Runtimes (ms) non-graphical method, PMOVE CMOVE log-scalestandard deviation mean (error bars) (left) corresponding number vectorsPCS CCS (right), increasing numbers agents 5 objectives.therefore conclude that, two-objective MO-CoGs, non-graphical methodintractable, even small numbers agents, runtime CMOVE increasesmuch less number agents PMOVE does.test runtime behavior changes higher number objectives, runexperiment average number factors per agent held = 1.5nincreasing numbers agents again, = 5. remaining experimentsdescribed section executed Xeon L5520 2.26 GHz computer 24 GBmemory. Figure 5 (left) shows results experiment, averaged 85 MO-CoGsnumber agents. Note plot induced widths,change number objectives. results demonstrate that, numberagents grows, using CMOVE becomes key containing computational cost solvingMO-CoG. CMOVE outperforms nongraphical method 12 agents onwards.25 agents, basic CMOVE 38 times faster. CMOVE also significantly betterPMOVE. Though one order magnitude slower 10 agents (238ms (basic)416ms (incremental) versus 33ms average), runtime grows much slowlyPMOVE. 20 agents, CMOVE variants faster PMOVE28 agents, Basic CMOVE almost one order magnitude faster (228s versus 1, 650saverage), difference increases every agent.before, runtime CMOVE exponential induced width, increasesnumber agents, 3.1 n = 10 6.0 n = 30 average, resultrandom MO-CoG generation procedure. However, CMOVEs runtime polynomialsize CCS, size grows exponentially, shown Figure 5 (right).fact CMOVE much faster PMOVE explained sizes PCSCCS, former grows much faster latter. 10 agents, average PCSsize 230 average CCS size 65. 30 agents, average PCS size risen51, 745 average CCS size 1, 575.Figure 6 (left) compares scalability algorithms number objectives,random MO-CoGs n = 20 = 30, averaged 100 MO-CoGs. CMOVEalways outperforms nongraphical method. Interestingly, nongraphical method419fiRoijers, Whiteson, & OliehoekFigure 6: Runtimes (ms) non-graphical method, PMOVE CMOVE logscalestandard deviation mean (error bars) (left) corresponding number vectorsPCS CCS (right), increasing numbers objectives.several orders magnitude slower = 2, grows slowly = 5, startsgrow exponent PMOVE. explained facttime takes enumerate joint actions payoffs remains approximately constant,time takes prune increases exponentially number objectives.= 2, CMOVE order magnitude slower PMOVE (163ms (basic)377 (incremental) versus 30ms). However, = 5, CMOVE variants alreadyfaster PMOVE 8 dimensions respectively 3.2 2.4 times faster.happens CCS grows much slowly PCS, shown Figure6 (right). difference incremental basic CMOVE decreases numberdimensions increases, factor 2.3 = 2 1.3 = 8. trend indicatespruning every cross-sum, i.e., prune1, becomes (relatively) better highernumbers objectives. Although unable solve problem instances manyobjectives within reasonable time, expect trend continue incrementalCMOVE would faster basic CMOVE problems many objectives.Overall, conclude that, random graphs, CMOVE key solving MO-CoGswithin reasonable time, especially problem size increases either numberagents, number objectives, both.4.5.2 Mining DayMining Day, mining company mines gold silver (objectives) set mines(local payoff functions) located mountains (see Figure 1). mine workers livevillages foot mountains. company one van village (agents)transporting workers must determine every morning mine vango (actions). However, vans travel nearby mines (graph connectivity). Workersefficient workers mine: 3% efficiency bonus perworker amount resource mined per worker x 1.03w , xbase rate per worker w number workers mine. base rategold silver properties mine. Since company aims maximize revenue,best strategy depends fluctuating prices gold silver. maximize revenue,420fiComputing CCSs Faster Multi-objective CoordinationFigure 7: Runtimes (ms) basic incremental CMOVE, PMOVE, log-scalestandard deviation mean (error bars) (left) corresponding number vectorsPCS CCS (right), increasing numbers agents.mining company wants use latest possible price information, lose timerecomputing optimal strategy every price change. Therefore, must calculateCCS.generate Mining Day instance v villages (agents), randomly assign 2-5workers village connect 2-4 mines. village connected minesgreater equal index, i.e., village connected mines, connectedmines + 1. last village connected 4 mines thus number minesv + 3. base rates per worker resource mine drawn uniformlyindependently interval [0, 10].order compare runtimes basic incremental CMOVE PMOVErealistic benchmark, generate Mining Day instances varying numbersagents. Note include non-graphical method, runtime mainlydepends number agents, thus considerably faster problemrandom graphs. runtime results shown Figure 7 (left). CMOVEPMOVE able tackle problems 100 agents. However, runtimePMOVE grows much quickly CMOVE. two-objective setting,basic CMOVE better incremental CMOVE. Basic CMOVE PMOVEruntimes around 2.8s 60 agents, 100 agents, basic CMOVE runs 5.9sPMOVE 21s. Even though incremental CMOVE worse basic CMOVE,runtime still grows much slowly PMOVE, beats PMOVEmany agents.difference PMOVE CMOVE results relationshipnumber agents sizes CCS, grows linearly, PCS, growspolynomially, shown Figure 7 (right). induced width remains around 4 regardlessnumber agents. results demonstrate that, CCS grows slowlyPCS number agents, CMOVE solve MO-CoGs efficientlyPMOVE number agents increases.421fiRoijers, Whiteson, & Oliehoek5. Linear Support MO-CoGssection, present variable elimination linear support (VELS). VELS newmethod computing CCS MO-CoGs several advantages CMOVE:moderate numbers objectives, runtime complexity better; anytimealgorithm, i.e., time, VELS produces intermediate results become betterbetter approximations CCS therefore, provided maximum scalarizederror , VELS compute -optimal CCS.Rather dealing multiple objectives inner loop (like CMOVE), VELSdeals outer loop employs subroutine. VELS thus buildsCCS incrementally. iteration outer loop, VELS adds one newvector partial CCS. find vector, VELS selects single w (the one offersmaximal possible improvement), passes w inner loop. inner loop,VELS uses (Section 2.2) solve single-objective coordination graph (CoG)results scalarizing MO-CoG using w selected outer loop. jointaction optimal CoG multi-objective payoff addedpartial CCS.departure point creating VELS Chengs linear support (Cheng, 1988). Chengslinear support originally designed pruning algorithm POMDPs. Unfortunately,algorithm rarely used POMDPs practice, runtime exponentialnumber states. However, number states POMDP corresponds numberobjectives MO-CoG, realistic POMDPs typically many states, manyMO-CoGs handful objectives. Therefore, MO-CoGs, scalabilitynumber agents important, making Chengs linear support attractive startingpoint developing efficient MO-CoG solution method.Building Chengs linear support, Section 5.1 create abstract algorithmcall optimistic linear support (OLS), builds CCS incrementally.OLS takes arbitrary single-objective problem solver input, seen genericmulti-objective method. show OLS chooses w iteration that,finite number iterations, improvements partial CCS madeOLS terminate. Furthermore, bound maximum scalarized errorintermediate results, used bounded approximations CCS.Then, Section 5.2, instantiate OLS using single-objective problem solver,yielding VELS, effective MO-CoG algorithm.5.1 Optimistic Linear SupportOLS constructs CCS incrementally, adding vectors initially empty partial CCS :Definition 13. partial CCS, S, subset CCS, turn subset V:CCS V.define scalarized value function S, corresponding convex upper surface(shown bold) Figure 8b-d:Definition 14. scalarized value function partial CCS, S, function takesweight vector w input, returns maximal attainable scalarized value422fiComputing CCSs Faster Multi-objective Coordination(a)(b)(c)(d)Figure 8: (a) possible payoff vectors 2-objective MO-CoG. (b) OLS finds two payoffvectors extrema (red vertical lines), new corner weight wc = (0.5, 0.5)found, maximal possible improvement . CCS shown dotted line.(c) OLS finds new vector (0.5, 0.5), adds two new corner weights Q.(d) OLS calls SolveCoG corner weights (in two iterations), findsnew vectors, ensuring = CCS = CCS.payoff vector S:uS (w) = max w u(a).u(a)SSimilarly, define set maximizing joint actions:Definition 15. optimal joint action set function respect functiongives joint actions maximize scalarized value:(w) = arg max w u(a).u(a)SNote (w) set w multiple joint actions providescalarized value.Using definitions, describe optimistic linear support (OLS). OLS addsvectors partial CCS, S, finding new vectors so-called corner weights. cornerweights weights uS (w) (Definition 14) changes slope directions.must thus weights (w) (Definition 15) consists multiple payoff vectors. Everycorner weight prioritized maximal possible improvement finding new payoffvector corner weight. maximal possible improvement 0, OLS knowspartial CCS complete. example process given Figure 8,(corner) weights algorithm searched new payoff vectors indicatedred vertical lines.OLS shown Algorithm 7. find optimal payoff corner weight, OLSassumes access function called SolveCoG computes best payoff vectorgiven w. now, leave implementation SolveCoG abstract. Section 5.2,discuss implement SolveCoG. OLS also takes input m, MO-CoG solved,, maximal tolerable error result.first describe OLS initialized (Section 5.1.1). Then, define corner weightsformally describe OLS identifies (Section 5.1.2). Finally, describe423fiRoijers, Whiteson, & OliehoekAlgorithm 7: OLS(m, SolveCoG, )123456789101112131415161718192021222324Input:CoGGCCS,agent eliminate.//partialW //set checked weightsQ empty priority queueforeach extremum weight simplexQ.add(we , ) // add extrema infinite priorityendQ.isEmpty() timeOutw Q.pop()u SolveCoG(m, w)u 6Wdel remove corner weights made obsolete u Q, storeWdel {w} Wdel //corner weights removed adding uWu newCornerWeights(u, Wdel , S){u}foreach w Wur (w) calculate improvement using maxValueLP(w, S, W)r (w) >Q.add(w, r (w))endendendW W {w}endreturn highest r (w) left QOLS prioritizes corner weights also used bound errorstopping OLS done finding full CCS (Section 5.1.3).5.1.1 InitializationOLS starts initializing partial CCS, S, contain payoff vectorsCCS discovered far (line 1 Algorithm 7), well set visited weights W (line2). Then, adds extrema weight simplex, i.e., pointsweight one objective, priority queue Q, infinite priority (line 5).extrema popped priority queue OLS enters main loop (line7), w highest priority selected (line 8). SolveCoG calledw (line 9) find u, best payoff vector w.example, Figure 8b shows two payoff vectors 2-dimensional MOCoG found applying SolveCoG extrema weight simplex: ={(1, 8), (7, 2)}. vectors must part CCS optimalleast one w: one SolveCoG returned solution (the extremaweight simplex). set weights W OLS tested far marked verticalred line segments.424fiComputing CCSs Faster Multi-objective Coordination5.1.2 Corner Weightsevaluated extrema, consists (the number objectives) payoff vectorsassociated joint actions. However, many weights simplex, yetcontain optimal payoff vector. Therefore, identifying new vector u add(line 9), OLS must determine new weights add Q. Like Chengs linear support,OLS identifying corner weights: weights corners convexupper surface, i.e., points PWLC surface uS (w) changes slope. definecorner weights precisely, must first define P , polyhedral subspace weightsimplex uS (w) (Bertsimas & Tsitsiklis, 1997). corner weightsvertices P, defined set linear inequalities:Definition 16. set known payoff vectors, define polyhedronXP = {x <d+1 : + x ~0, i, wi > 0,wi = 1},+ matrix elements row vectors, augmented column vector1s. setPof linear inequalities + x ~0, supplemented simplex constraints:wi > 0 wi = 1. vector x = (w1 , ..., wd , u) consists weight vectorscalarized value weights. corner weights weights containedvertices P , also form (w1 , ..., wd , u).Note that, due simplex constraints, P d-dimensional. Furthermore,extrema weight simplex special cases corner weights.identifying u, OLS identifies corner weights change polyhedron Padding u S. Fortunately, require recomputation corner weights,done incrementally: first, corner weights Q u yields bettervalue currently known deleted queue (line 11) functionnewCornerWeights(u, Wdel , S) line 13 calculates new corner weights involve usolving system linear equations see u intersects boundariesrelevant subset present vectors S.newCornerWeights(u, Wdel , S) (line 13) first calculates set relevant payoffvectors, Arel , taking union maximizing vectors weights Wdel 9 :Arel =[(w).wWdel(w) contains fewer payoff vectors, boundary weight simplexinvolved. boundaries also stored. possible subsets size 1 (of vectorsboundaries) taken. subset weight 1 payoff vectors(and/or boundaries) intersect u computed solving systemlinear equations. intersection weights subsets together form set candidatecorner weights: Wcan . newCornerWeights(u, Wdel , S) returns subset Wcaninside weight simplex u higher scalarized value payoff9. fact, implementation, optimize step caching (w) w Q.425fiRoijers, Whiteson, & Oliehoekvector already S. Figure 8b shows one new corner weight labelled wc = (0.5, 0.5).practice, |Arel | small, systems linear equations need solved.10calculating new corner weights Wu line 13, u added line 14.Cheng showed finding best payoff vector corner weight addingpartial CCS, i.e., {SolveCoG(w)}, guarantees best improvement S:Theorem 8. (Cheng 1988) maximum value of:maxmin w u w v,w,uCCS vSi.e., maximal improvement adding vector it, one corner weights(Cheng, 1988).Theorem 8 guarantees correctness OLS: corner weights checked,new payoff vectors; thus maximal improvement must 0 OLS foundfull CCS.5.1.3 PrioritizationChengs linear support assumes corner weights checked inexpensively,reasonable assumption POMDP setting. However, since SolveCoG expensiveoperation, testing corner weights may feasible MO-CoGs. Therefore, unlikeChengs linear support, OLS pops one w Q tested per iteration. MakingOLS efficient thus critically depends giving w suitable priority addingQ. end, OLS prioritizes corner weight w according maximal possibleimprovement, upper bound improvement uS (w). upper bound computedrespect CCS, optimistic hypothetical CCS, i.e., best-case scenariofinal CCS given current partial CCS W set weights alreadytested SolveCoG. key advantage OLS Chengs linear supportpriorities computed without calling SolveCoG, obviating need run SolveCoGcorner weights.Definition 17. optimistic hypothetical CCS, CCS set payoff vectors yieldshighest possible scalarized value possible w consistent finding vectorsweights W.Figure 8b denotes CCS = {(1, 8), (7, 2), (7, 8)} dotted line. Note CCSsuperset value uCCS (w) uS (w) weights W.given w, maxValueLP finds scalarized value uCCS(w) solving:max w vsubject W v uS,W ,10. However, theory possible construct partial CCS, corner weightpayoff vectors Adel .426fiComputing CCSs Faster Multi-objective CoordinationuS,W vector containing uS (w0 ) w0 W. Note abuse notationW, case matrix whose rows consist weight vectors setW.11Using CCS, define maximal possible improvement:(w) = uCCS (w) uS (w).Figure 8b shows (wc ) dashed line. use maximal relative possible improvement, r (w) = (w)/uCCS (w), priority new corner weight w Wu .Figure 8b, r (wc )= (0.5,0.5)((7,8)(1,8))= 0.4. corner weight w identified (line 13),7.5added Q priority r (w) long r (w) > (lines 16-18).wc Figure 8b added Q, popped (as elementQ). SolveCoG(wc ) generates new vector (5, 6), yielding = {(1, 8), (7, 2), (5, 6)},illustrated Figure 8c. new corner weights (0.667, 0.333) (0.333, 0.667)points (5, 6) intersects (7, 2) (1, 8). Testing weights, illustratedFigure 8d, result new payoff vectors, causing OLS terminate. maximalimprovement corner weights 0 thus, due Theorem 8, = CCS upontermination. OLS called solveCoG 5 weights resulting exactly 3 payoffvectors CCS. 7 payoff vectors V (displayed grey dashed blacklines Figure 8a) never generated.5.2 Variable Elimination Linear Supportexact CoG algorithm used implement SolveCoG. naive approachexplicitly compute values joint actions V select joint action maximizesvalue:SolveCoG(m, w) = arg max w u(a).u(a)Vimplementation SolveCoG combination OLS yields algorithmrefer non-graphical linear support (NGLS), ignores graphical structure,flattening CoG standard multi-objective cooperative normal form game.main downside computational complexity SolveCoG linear |V| (whichequal |A|), exponential number agents, making feasibleMO-CoGs agents.contrast, use (Section 2.2) implement SolveCoG, better.call resulting algorithm variable elimination linear support (VELS). dealtmultiple objectives outer loop OLS, VELS relies exploit graphicalstructure inner loop, yielding much efficient method NGLS.5.3 Analysisanalyze computational complexity VELS.11. implementation OLS reduces size LP using subset weights Wjoint actions involved w, (w), found optimal. lead slightoverestimation uCCS(w).427fiRoijers, Whiteson, & OliehoekTheorem 9. runtime VELS = 0O((|CCS| + |WCCS |)(n|Amax |w + Cnw + Cheur )),w induced width running VE, |CCS| size CCS, |WCCS |number corner weights uCCS (w), Cnw time costs run newCornerWeights,Cheur cost computation value optimistic CCS using maxValueLP.Proof. Since n|Amax |w runtime (Theorem 1), runtime VELSquantity (plus overhead per corner weight Cnw + Cheur ) multiplied numbercalls VE. count calls, consider two cases: calls result addingnew vector result new vector instead confirmoptimality scalarized value weight. former size final CCS,|CCS|, latter number corner weights final CCS, |WCCS |.overhead OLS itself, i.e., computing new corner weights, Cnw , calculatingmaximal relative improvement, Cheur , small compared SolveCoG calls.practice, newCornerWeights(u, Wdel , S) computes solutions small setlinear equations (of equations each). maxValueLP(w, S, W) computes solutionslinear programs, polynomial size inputs.12= 2, number corner weights smaller |CCS| runtimeVELS thus O(n|Amax |w |CCS|). = 3, number corner weights twice |CCS|(minus constant) because, SolveCoG finds new payoff vector, one corner weightremoved three new corner weights added. > 3, loose bound |WCCS |total number possible combinations payoff vectors boundaries: O( |CCS|+d).However, obtain tighter bound observing counting number cornerweights given CCS equivalent vertex enumeration, dual problemfacet enumeration, i.e., counting number vertices given corner weights (Kaibel &Pfetsch, 2003).Theorem 10. arbitrary d, |WCCS | bounded O((Avis & Devroye, 2000).|CCS|b d+1c2|CCS|d+|CCS|b d+2c2)|CCS|dProof. result follows directly McMullens upper bound theorem facet enumeration (Henk, Richter-Gebert, & Ziegler, 1997; McMullen, 1970).reasoning used prove Theorems 9 also used establish following:Corollary 1. runtime VELS 0O((|-CCS| + |WCCS |)(n|Amax |w + Cnw + Cheur ), |-CCS| size -CCS,|WCCS | number corner weights uCCS (w).practice, VELS often test corner weights polyhedron spanned-CCS, cannot guaranteed general. Section 5.4, show empirically|-CCS| decreases rapidly increases.12. reduction Footnote 11 used, small subset W used, making even smaller.428fiComputing CCSs Faster Multi-objective CoordinationFigure 9: (left) runtimes PMOVE, CMOVE VELS different values ,varying numbers agents, n, = 1.5n factors, 2 actions per agent,2 objectives (right) corresponding sizes -CCSs.Theorem 11. space complexity VELS O(d|-CCS|+d|WCCS |+n|Amax |w )0.Proof. OLS needs store every corner weight (a vector length d) queue,|WCCS |. OLS also needs store every vector (also vectors length d).Furthermore, SolveCoG called, memory usage added memoryusage outer loop OLS. memory usage n|Amax |w (Theorem 2).OLS adds memory requirements VE, VELS almost memoryefficient thus considerably memory efficient CMOVE (Theorem 7).5.4 Empirical Evaluationempirically evaluate VELS, comparison CMOVE PMOVE. longercompare non-graphical method clearly dominated CMOVEPMOVE. refer CMOVE section, mean basic CMOVE,fastest tested scenarios. Like before, use random graphs Mining Daybenchmark. experiments section run 2.4 GHx Intel Core i5 computer,4 GB memory.5.4.1 Random Graphstest VELS randomly generated MO-CoGs, use MO-CoG generationprocedure Section 4. determine scalability exact approximateVELS compares PMOVE CMOVE, tested random MO-CoGsincreasing numbers agents. average number factors per agent held= 1.5n number objectives = 2. Figure 9 shows results,averaged 30 MO-CoGs number agents. Note runtimes left,y-axis, log-scale set sizes right not.results demonstrate VELS efficient CMOVE two-objectiverandom MO-CoGs. runtime exact VELS ( = 0) average 16 times less429fiRoijers, Whiteson, & OliehoekCMOVE. CMOVE solves random MO-CoGs 85 agents 74s average, whilstexact VELS handle 110 agents 71s.already large gain, achieve even lower growth rate permittingsmall . 110 agents, permitting 0.001 error margin yields gainorder magnitude, reducing runtime 5.7s. Permitting 0.01 error reducesruntime 1.3s. thus reduce runtime VELS factor 57,retaining 99% accuracy. Compared CMOVE 85 agents, VELS = 0.01 109times faster.speedups explained slower growth -CCS (Figure 9 (right)).small numbers agents, size -CCS grows slightly slowlysize full CCS. However, certain number agents onwards, size-CCS grows marginally size full CCS keeps growing. = 0.01,-CCS grew 2.95 payoff vectors 5.45 payoff vectors 5 20 agents,marginally 5.50 110 agents. contrast, full CCS grew 3.009.90 vectors 5 20 agents, keeps growing 44.50 110 agents.similar picture holds 0.001-CCS, grows rapidly 3.00 vectors 514.75 vectors 50 agents, grows slowly 16.00 90 agents, stabilizes,reach 16.30 vectors 120 agents. 90 120 agents, full CCS grows35.07 vectors 45.40 vectors, making almost 3 times large 0.001-CCS 9times larger 0.01-CCS .test scalability VELS respect number objectives, testedrandom MO-CoGs constant number agents factors n = 25 = 1.5n,increased number objectives, = 0 = 0.1. comparescalability CMOVE. kept number agents (n = 25) number localpayoff functions ( = 37) small order test limits scalability numberobjectives. number actions per agent 2. Figure 10 (left) plots numberobjectives runtime (in log scale). CCS grows exponentiallynumber objectives (Figure 10 (right)), runtime CMOVE also exponentialnumber objectives. VELS however linear number corner weights,exponential size CCS, making VELS doubly exponential. Exact VELS ( = 0)faster CMOVE = 2 = 3, = 4 approximate VELS = 0.120 times faster. However = 5 even approximate VELS = 0.1slower CMOVE.Unlike number agents grows, size -CCS (Figure 10 (right))stabilize number objectives grows, seen following table:|CCS|d=2d=3d=4=010.668.8295.1= 0.0017.364.6286.1= 0.015.641.0242.6= 0.13.034.8221.7therefore conclude VELS compute CCS faster CMOVE 3 objectivesless, CMOVE scales better number objectives. VELS however, scalesbetter number agents.430fiComputing CCSs Faster Multi-objective CoordinationFigure 10: (left) runtimes CMOVE VELS ( = 0 = 0.1), varying numbers objectives (right) size -CCS varying numbers objectives.Figure 11: (left) plot runtimes CMOVE VELS different values ,varying n (up 500). (right) loglogplot runtime VELS 250, 500,1000 agent mining day instances, varying values .5.4.2 Mining Daycompare CMOVE VELS Mining Day benchmark using generation procedure Section 4.5.2. generated 30 Mining Day instances increasing naveraged runtimes (Figure 11 (left)). 160 agents, CMOVE reached runtime22s. Exact VELS ( = 0) compute complete CCS MO-CoG 420 agentstime. indicates VELS greatly outperforms CMOVE structured2-objective MO-CoG. Moreover, allow 0.1% error ( = 0.001), takes1.1s compute -CCS 420 agents, speedup order magnitude.measure additional speedups obtainable increasing , test VELSlarge problems, generated Mining Day instances n {250, 500, 1000}.averaged 25 instances per value . instances, exact VELS runs 4.2sn = 250, 30s n = 500 218s n = 1000 average. expected, increasingleads greater speedups (Figure 11 (right)). However, close 0, i.e.,431fiRoijers, Whiteson, & Oliehoek-CCS close full CCS, speedup small. increased beyond certainvalue (dependent n), decline becomes steady, shown line log-log plot.increases factor 10, runtime decreases factor 1.6.Thus, results show VELS compute exact CCS unprecedentednumbers agents (1000) well-structured problems. addition, show smallvalues enable large speedups, increasing leads even bigger improvementsscalability.6. Memory-Efficient MethodsCMOVE VELS designed minimize runtime required compute CCS.However, cases, bottleneck may memory instead. Memory-efficient methodsCoGs related problems recently received considerable attention (Dechter &Mateescu, 2007; Marinescu, 2008, 2009; Mateescu & Dechter, 2005). section,show that, outer loop method, VELS naturally memory efficienttherefore solve much larger MO-CoGs inner loop method like CMOVEmemory restricted. addition, show CMOVE VELS modifiedproduce even memory-efficient variants.6.1 And/Or Tree Searchbegin background AND/OR tree search (Dechter & Mateescu, 2007;Marinescu, 2008; Mateescu & Dechter, 2005; Yeoh, Felner, & Koenig, 2010), classalgorithms solving single-objective CoGs tuned provide better spacecomplexity guarantees VE. However, improvement space complexity comesprice, i.e., runtime complexity worse (Mateescu & Dechter, 2005). backgroundprovide brief; broader overview AND/OR tree search CoGs relatedmodels please see work Dechter (2013) Marinescu (2008), multi-objectiveversions work Marinescu (2009, 2011).AND/OR tree search algorithms work converting graph pseudo tree (PT)agent need know actions ancestors descendants PTtake order select action. example, agent (a node) PT twosubtrees (T1 T2 ) it, agents T1 conditionally independentagents T2 given ancestors i. Figure 12a shows PT coordinationgraph Figure 2a.Next, AND/OR tree search algorithms perform tree search results AND/ORsearch tree (AOST). agent AOST OR-node. children AND-nodes,corresponding one agent actions. turn, children AND-nodesOR-nodes corresponding agent children PT. action (AND-nodes)agent agents OR-nodes, agents actions appeartree multiple times. Figure 12b shows AOST graph Figure 2a.specific joint action constructed traversing tree, starting rootselecting one alternative childen OR-node, i.e., one action agent,continuing children AND-node. example, Figure 12b, jointaction < a1 , a2 , a3 > indicated grey. retrieve value joint action, mustfirst define value AND-nodes.432fiComputing CCSs Faster Multi-objective CoordinationFigure 12: (a) pseudo tree, (b) corresponding AND/OR search tree.Definition 18. value AND-node vai , representing action ai agentsum local payoff functions scope; ai , together AND-nodeancestors actions, specifies action agent scope local payoff functions.example, Figure 12b, total payoff CoG u(a1 , a2 , a3 ) = u1 (a1 , a2 ) +u2 (a2 , a3 ). value grey AND-node a3 u2 (a2 , a3 ), u3 payoff functionagent 3 scope and, together ancestral AND-nodes, grey a2 -node, a3completes joint local action u2 .retrieve optimal action, must define value subtree AOST:Definition 19. value subtree v(Ti ) rooted OR-node AOSTmaximum value subtrees rooted (AND-node) children i. valuesubtree v(Tai ) rooted AND-node ai AOST value ai (Definition18) plus sum value subtrees rooted (OR-node) children ai .memory-efficient way retrieve optimal joint action using AOSTEuler-touring it, i.e., performing depth-first search computing valuessubtrees. generating nodes fly deleting evaluated, memoryusage minimized. refer algorithm simply AND/OR tree search (TS).earlier sections, implementation employs tagging scheme, tagging valuesubtree actions maximize it.TS single-objective method, extended compute PCS,yielding algorithm call Pareto TS (PTS) (Marinescu, 2009). define PTS, mustupdate Definition 19 set Pareto-optimal payoffs. refer subtree valueset intermediate PCS (IPCS).Definition 20. intermediate PCS subtree, IP CS(Ti ) rooted OR-nodePCS union intermediate PCSs children, ch(i), i:IP CS(Ti ) = PPrune([aj ch(i)433IP CS(Taj )).fiRoijers, Whiteson, & Oliehoekintermediate PCS subtree, IP CS(Tai ) rooted AND-node ai PCSvalue ai (Definition 18) plus cross-sum intermediate PCSs subtreesrooted (OR-node) children ai :IP CS(Tj ) {vai }).IP CS(Tai ) = PPrune(jch(ai )Thus, PTS replaces max operator TS pruning operator, PMOVE replacesmax operator pruning operator.6.2 Memory-Efficient CCS Algorithmspropose two memory-efficient algorithms computing CCS. straightforward variants CMOVE VELS.first algorithm, call Convex TS (CTS), simply replaces PPrune CPruneDefinition 20. Thus, CTS like PTS different pruning operator.also seen CMOVE replaced TS. advantage CTS PTSanalogous CMOVE PMOVE: highly beneficial compute localCCSs instead local PCSs intermediate coverage sets input nextsubproblem sequential search scheme, regardless whether scheme TS.CTS memory efficient CMOVE, still requires computing intermediatecoverage sets take space. typically large CCS,size bounded total number joint actions.second algorithm addresses problem employing OLS TS singleobjective solver subroutine, SolveCoG, yielding tree search linear support (TSLS). Thus,TSLS like VELS replaced TS. TSLS outer-loop method,runs TS sequence, requiring memory used TS overheadouter loop, consists partial CCS (Definition 13) priority queue.Consequently, TSLS even memory efficient CTS.6.3 AnalysisTS much better space complexity VE, i.e., linear number agents n:Theorem 12. time complexity TS O(n|Amax |m ), n number agents,|Amax | maximal number actions single agent depth pseudotree, uses linear space, O(n).Proof. number nodes AOST bounded O(n|Amax |m ). tree createsmaximally |Amax | children OR-node. every AND-node exactly one child,number nodes would bounded O(|Amax |m ), PT deep. However,branching PT, AND-node multiple children. branch increasessize AOST O(|Amax |m ) nodes. exactly n agentsPT, happen n times. node AOST, TS performs eithersummation scalars, maximization scalars. TS performs depth-firstsearch, O(n) nodes need exist point execution.434fiComputing CCSs Faster Multi-objective CoordinationTSs memory usage usually lower required store original (singleobjective) problem memory: O(|Amax |emax ), number local payofffunctions problem, |Amax | maximal size action space single agent,emax maximal size scope single local payoff function.PT-depth different constant induced width w, typicallylarger. However, bounded w.Theorem 13. Given MO-CoG induced width w, exists pseudo treedepth w log n (Dechter & Mateescu, 2007).Thus, combining Theorems 12 13 shows that, agents, TSmuch memory efficient relatively small runtime penalty.Using time space complexity results TS, establish followingcorollaries time space complexity CTS TSLS.Corollary 2. time complexity CTS O(n|Amax |m R), R runtimeCPrune.Proof. O(n|Amax |m ) bounds number nodes AOST. node AOSTCPrune called.runtime CPrune terms size input given Equation 3. Notesize input CPrune depends size intermediate CCSschildren node. case AND-node, input size O(|ICCSmax |c ),c maximum number children AND-node.13 OR-nodesO(|Amax ||ICCSmax |).Corollary 3. space complexity CTS O(n|ICCSmax |), |ICCSmax |maximum size intermediate CCS execution CTS.Proof. Like TS, O(n) nodes AOST need exist pointexecution, node contains intermediate CCS.CTS thus much memory efficient CMOVE, space complexityexponential induced width (Theorem 7).Corollary 4. time complexity TSLS O((|-CCS|+|W -CCS |) (n |Amax |m +Cnw +Cheur )), w log n 0.Proof. proof Theorem 9 time complexityreplaced TS.terms memory usage, outer loop approach (OLS) large advantageinner loop approach, overhead outer loop consists partialCCS (Definition 13) priority queue. VELS (Theorem 11) thus much betterspace complexity CMOVE (Theorem 7). TSLS advantage CTSVELS CMOVE. Therefore, TSLS low memory usage, since requiresmemory used TS plus overhead outer loop.13. Note c turn upper bounded n loose bound.435fiRoijers, Whiteson, & OliehoekCorollary 5. space complexity TSLS O(d|-CCS| + d|W -CCS | + n)),w log n 0.Proof. proof Theorem 11 space complexityreplaced TS.mentioned Section 6.1, TS memory-efficient member classAND/OR tree search algorithms. members class offer different trade-offstime space complexity. possible create inner loop algorithmscorresponding outer loop algorithms basis algorithms. timespace complexity analyses algorithms performed similar mannerCorollaries 25. advantages outer loop methods compared correspondinginner loop methods however remain TSLS CTS. Therefore,article focus comparing memory-efficient inner loop methodmemory-efficient outer loop method.6.4 Empirical Evaluationsection, compare CTS TSLS CMOVE VELS. before, userandom graphs Mining Day benchmark. obtain PTs CTS TSLS,use heuristic CMOVE VELS generate elimination ordertransform PT w log n holds (whose existence guaranteedTheorem 13), using procedure suggested Bayardo Miranker (1995).6.4.1 Random GraphsFirst, test algorithms random graphs, employing generation procedureSection 4.5.1. connections agents graphs generatedrandomly, induced width varies different problems. average, inducedwidth increases number local payoff functions, even ratiolocal payoff factors number agents remains constant.order test sizes problems different MO-CoG solution methodshandle within limited memory, generate random graphs two objectives, varyingnumber agents n, = 1.5n local payoff functions, previous sections.limited maximal available memory 1kB imposed timeout 1800s.Figure 13a shows VELS scale agents within given memory constraints non-memory efficient methods. particular, PMOVE CMOVEhandle 30 40 agents, respectively, because, given induced width w,must store O(|Amax |w ) local CSs. 30 agents, induced width (Figure 13c)6, 40 agents induced width 8. VELS handle 65 agents,induced width 11, memory demands come runninginner loop, outer loop adds little overhead. need store one payoffnew local payoff function results agent elimination, whereas PMOVECMOVE must store local coverage sets. Thus, using outer loop approach (VELS)instead inner loop approach (CMOVE) already yields significant improvementproblem sizes tackled limited memory.436fiComputing CCSs Faster Multi-objective Coordination(a)(b)(c)Figure 13: (a) Runtimes ms TSLS, VELS, CTS, CMOVE PMOVE random 2objective MO-CoGs varying numbers agents n = 1.5n local payofffactors. (b) Runtimes approximate TSLS varying amounts allowed error, compared (Exact) VELS, problem parameters (a). (c)corresponding induced widths MO-CoGs (b).However, scaling beyond 65 agents requires memory-efficient approach. Figure 13aalso shows that, CTS TSLS require runtime, handle agentswithin memory constraints. fact, unable generate MO-CoG enoughagents cause methods run memory. TSLS faster CTS, case4.2 times faster, reasons VELS faster CMOVE.However, speed advantage outer loop approach. allowbit error scalarized value, , trade accuracy runtime (Figure 13b). 65agents, exact TSLS ( = 0), average runtime 106s, 51 times slowerVELS. However, = 0.0001, runtime 70s (33 times slower). = 0.0111s (5.4 times slower), = 0.1 6s (2.9 times slower). Furthermore,relative increase runtime number agents increases less higher . Thus,approximate version TSLS highly attractive method cases memoryruntime limited.6.4.2 Mining Fieldcompare performance CMOVE VELS TSLS variation MiningDay call Mining Field. longer consider CLS consistently higherruntime TSLS worse space complexity. use Mining Field order ensureinteresting problem memory-restricted setting. Mining Day (see Section 4),induced width depends parameter specifying connectivity villagesincrease number agents factors. Therefore, whetherVELS memory-efficient enough handle particular instance depends primarilyparameter number agents.Mining Field, villages situated along mountain ridge placedgrid. number agents thus n = s2 . use random placement mines,ensuring graph connected. induced width connected gridgenerate grid-like graphs, larger instances higher induced width.437fiRoijers, Whiteson, & Oliehoekvillage(a)mine(b)(c)Figure 14: (a) example 4 4 Mining Field instance. additional minesmarked +. (b) Runtimes ms TSLS (for varying amounts allowederror ), VELS ( = 0), CMOVE 2-objective Mining Field instancesvarying numbers additional mines [2..14] grid size = 7. (c)corresponding induced widths Mining Field instances.induced width thus longer depends connectivity parameter also increasesnumber agents factors graph.example Mining Field instance provided Figure 14a. choose distanceadjacent villages grid unit length. map, placemines (local payoff functions). connect agents using arbitrary tree using 2-agentlocal payoff functions (mines). figures, mines span tree unmarkedconnected mines black edges. require s2 1 factors build tree.add additional mines, (independently) placing random pointmap inside grid. mine placed, connect villages withinr = 12 + radius mine map. chose = 0.2. Therefore, maximumconnectivity factor (mine) created fashion 4. figure, minesmarked +. rewards per mine per worker, well number workers pervillage, generated way Mining Day.compare runtimes memory requirements CMOVE, VELS, TSLSMining Field, tested 7 7 instance (49 agents), 1MB available memory.TSLS, use three different values : 0 (exact), 0.01 0.1. use time limit1.8 106 (30 minutes). increase number additional mines 2 (50 factorstotal) onwards, steps 2.Using setup, possible solve problem instances using PMOVE,ran memory problems. fact, PMOVE succeeded tree-shapedproblem. i.e., one without additional factors. Figures 14b 14c) show resultsremaining methods. CMOVE runs memory 6 additional factors (54 factorstotal). contrast, VELS runs memory 16 additional factors, inducedwidth 6.Compared random-graph results Section 6.4.1, induced widthsproblems CMOVE VELS handle lower Mining Field. suspect438fiComputing CCSs Faster Multi-objective Coordinationbecause, grid-shaped problem, number factors highest inducedwidth need exist parallel execution algorithms higher.TSLS run memory tested instances. face,unable generate instances TSLS run memory. However,run time. = 0, TSLS first exceeds time limit = 10 additional mines.= 0.01, happens = 14. = 0.1, TSLS ran time = 16.differences runtime TSLS VELS larger random graphstherefore difficult compensate slower runtime TSLS choosinghigher . much slower TSLS compared VELS thus seems dependstructure MO-CoG.Mining Field results confirm conclusion random-graph experimentsusing outer loop approach (VELS) instead inner loop approach (CMOVE) yieldssignificant improvement problem sizes tackled limited memory.Futhermore, TSLS used solve problem sizes beyond VELS handlewithin limited memory. approximate version TSLS appealing choice casesmemory runtime limited.7. Conclusions Future Workarticle, proposed new algorithms exploit loose couplings compute CCSmulti-objective coordination graphs. showed exploiting loose couplingskey solving MO-CoGs many agents. particular, showed, theoreticallyempirically, computing CCS considerable advantages computing PCSterms runtime memory usage. experiments consistently shownruntime PCS methods grows lot faster CCS methods.CMOVE deals multiple objectives inner loop, i.e., computes local CCSslooping agents. contrast, VELS deals multiple objectivesouter loop, i.e., identifies weights maximal improvement upon partial CCSmade solves scalarized (single-objective) problems using weights, yieldinganytime approach. addition, CTS TSLS memory-efficient variantsmethods. proved correctness algorithms analyzed complexity.CMOVE VELS complementary methods. CMOVE scales better numberobjectives, VELS scales better number agents compute CCS, leading large additional speedups. Furthermore, VELS memory-efficientCMOVE. fact, VELS uses little memory single-objective VE.However, memory restricted VELS cannot applied, TSLS providesmemory-efficient alternative. TSLS considerably slower VELS,loss compensated allowing error ().numerous possibilities future work. mentioned Section 5, OLSgeneric method also applied multi-objective problems. fact, (togetherauthors) already applied OLS large multi-objective MDPs showedOLS extended permit non-exact single-objective solvers (Roijers et al., 2014).future work, intend investigate -approximate methods MO-CoGs, using approximate single-objective solvers CoGs, using, e.g., LP-relaxation methods (Sontag,Globerson, & Jaakkola, 2011). attempt find optimal balance439fiRoijers, Whiteson, & Oliehoeklevels approximation inner outer loop, respect runtime guaranteesempirical runtimes.Many methods exist single-objective coordination graphs single parametercontrols trade-off memory usage runtime (Furcy & Koenig, 2005; Rollon,2008). algorithms, corresponding multi-objective inner-loop versioncomputes PCS (Marinescu, 2009, 2011) devised. would interestingcreate inner outer loop methods based methods compute CCSinstead compare performance. particular, shown OLS requireslittle extra memory usage compared single-objective solvers. would interestinginvestigate much extra memory could used single-objective solver inside OLS,comparison corresponding inner-loop method.addition work MO-CoGs, also aim extend work sequentialsettings. particular, look developing efficient planning method multiagent multi-objective MDPs better exploiting loosely couplings. First, trydevelop -approximate planning version sparse-cooperative Q-learning (Kok & Vlassis,2006b). However, may possible general effects agentagents via state impossible bound general. Therefore, hope identifybroadly applicable subclass multi-agent MOMDPs -approximate planningmethod yields substantial speed-up compared exact planning methods.Acknowledgementsthank Rina Dechter introducing us memory-efficient methods CoGsMO-CoGs, Radu Marinescu tips memory-efficient methods implementation. Also, would like thank Maarten Inja, well anonymous reviewers, valuable feedback. research supported NWO DTC-NCAP(#612.001.109) NWO CATCH (#640.005.003) projects NWO Innovational Research Incentives Scheme Veni (#639.021.336). Frans Oliehoek affiliatedUniversity Amsterdam University Liverpool.ReferencesAvis, D., & Devroye, L. (2000). Estimating number vertices polyhedron. Information processing letters, 73 (3), 137143.Bayardo, R. J. J., & Miranker, D. P. (1995). space-time trade-off solving constraintsatisfaction problems. IJCAI 1995: Proceedings Fourteenth InternationalJoint Conference Artificial Intelligence.Bertsimas, D., & Tsitsiklis, J. (1997). Introduction Linear Optimization. Athena Scientific.Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.Cassandra, A., Littman, M., & Zhang, N. (1997). Incremental pruning: simple, fast, exactmethod partially observable markov decision processes. UAI 1997: ProceedingsThirteenth Conference Uncertainty Artificial Intelligence, pp. 5461.440fiComputing CCSs Faster Multi-objective CoordinationCheng, H.-T. (1988). Algorithms partially observable Markov decision processes. Ph.D.thesis, University British Columbia, Vancouver.Dechter, R. (2013). Reasoning Probabilistic Deterministic Graphical Models: Exact Algorithms, Vol. 7 Synthesis Lectures Artificial Intelligence MachineLearning. Morgan & Claypool Publishers.Dechter, R., & Mateescu, R. (2007). And/or search spaces graphical models. Artificialintelligence, 171 (2), 73106.Delle Fave, F., Stranders, R., Rogers, A., & Jennings, N. (2011). Bounded decentralisedcoordination multiple objectives. Proceedings Tenth International JointConference Autonomous Agents Multiagent Systems, pp. 371378.Dubus, J., Gonzales, C., & Perny, P. (2009). Choquet optimization using gai networksmultiagent/multicriteria decision-making. ADT 2009: Proceedings FirstInternational Conference Algorithmic Decision Theory, pp. 377389.Feng, Z., & Zilberstein, S. (2004). Region-based incremental pruning POMDPs. UAI2004: Proceedings Twentieth Conference Uncertainty Artificial Intelligence, pp. 146153.Furcy, D., & Koenig, S. (2005). Limited discrepancy beam search. IJCAI 2005: Proceedings Nineteenth International Joint Conference Artificial Intelligence, pp.125131.Guestrin, C., Koller, D., & Parr, R. (2002). Multiagent planning factored MDPs.Advances Neural Information Processing Systems 15 (NIPS02).Henk, M., Richter-Gebert, J., & Ziegler, G. M. (1997). Basic properties convex polytopes.Handbook Discrete Computational Geometry, Ch.13, pp. 243270. CRCPress, Boca.Kaibel, V., & Pfetsch, M. E. (2003). algorithmic problems polytope theory.Algebra, Geometry Software Systems, pp. 2347. Springer.Kok, J. R., & Vlassis, N. (2004). Sparse cooperative Q-learning. Proceedingstwenty-first international conference Machine learning, ICML 04, New York, NY,USA. ACM.Kok, J. R., & Vlassis, N. (2006a). Using max-plus algorithm multiagent decisionmaking coordination graphs. RoboCup 2005: Robot Soccer World Cup IX, pp.112.Kok, J., & Vlassis, N. (2006b). Collaborative multiagent reinforcement learning payoffpropagation. Journal Machine Learning Research, 7, 17891828.Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles Techniques. MIT Press.Lizotte, D., Bowling, M., & Murphy, S. (2010). Efficient reinforcement learning multiplereward functions randomized clinical trial analysis. Proceedings 27thInternational Conference Machine Learning (ICML-10), pp. 695702.441fiRoijers, Whiteson, & OliehoekMarinescu, R., Razak, A., & Wilson, N. (2012). Multi-objective influence diagrams.UAI 2012: Proceedings Twenty-Eighth Conference Uncertainty ArtificialIntelligence.Marinescu, R. (2008). AND/OR Search Strategies Combinatorial Optimization Graphical Models. Ph.D. thesis, University California, Irvine.Marinescu, R. (2009). Exploiting problem decomposition multi-objective constraint optimization. Principles Practice Constraint Programming-CP 2009, pp. 592607. Springer.Marinescu, R. (2011). Efficient approximation algorithms multi-objective constraintoptimization. ADT 2011: Proceedings Second International ConferenceAlgorithmic Decision Theory, pp. 150164. Springer.Mateescu, R., & Dechter, R. (2005). relationship AND/OR search variableelimination. UAI 2005: Proceedings Twenty-First Conference UncertaintyArtificial Intelligence, pp. 380387.McMullen, P. (1970). maximum numbers faces convex polytope. Mathematika,17 (2), 179184.Oliehoek, F. A., Spaan, M. T. J., Dibangoye, J. S., & Amato, C. (2010). Heuristic searchidentical payoff bayesian games. AAMAS 2010: Proceedings Ninth International Joint Conference Autonomous Agents Multiagent Systems, pp.11151122.Pearl, J. (1988). Probabilistic reasoning intelligent systems: networks plausible inference. Morgan Kaufmann.Pham, T. T., Brys, T., Taylor, M. E., Brys, T., Drugan, M. M., Bosman, P. A., Cock,M.-D., Lazar, C., Demarchi, L., Steenhoff, D., et al. (2013). Learning coordinatedtraffic light control. Proceedings Adaptive Learning Agents workshop (atAAMAS-13), Vol. 10, pp. 11961201.Roijers, D. M., Scharpff, J., Spaan, M. T. J., Oliehoek, F. A., de Weerdt, M., & Whiteson,S. (2014). Bounded approximations linear multi-objective planning uncertainty. ICAPS 2014: Proceedings Twenty-Fourth International ConferenceAutomated Planning Scheduling, pp. 262270.Roijers, D. M., Vamplew, P., Whiteson, S., & Dazeley, R. (2013a). survey multiobjective sequential decision-making. Journal Artificial Intelligence Research, 47,67113.Roijers, D. M., Whiteson, S., & Oliehoek, F. (2013b). Computing convex coverage setsmulti-objective coordination graphs. ADT 2013: Proceedings Third International Conference Algorithmic Decision Theory, pp. 309323.Roijers, D. M., Whiteson, S., & Oliehoek, F. A. (2014). Linear support multi-objectivecoordination graphs. AAMAS 2014: Proceedings Thirteenth InternationalJoint Conference Autonomous Agents Multi-Agent Systems, pp. 12971304.Rollon, E. (2008). Multi-Objective Optimization Graphical Models. Ph.D. thesis, Universitat Politecnica de Catalunya, Barcelona.442fiComputing CCSs Faster Multi-objective CoordinationRollon, E., & Larrosa, J. (2006). Bucket elimination multiobjective optimization problems. Journal Heuristics, 12, 307328.Rosenthal, A. (1977). Nonserial dynamic programming optimal. ProceedingsNinth Annual ACM Symposium Theory Computing, pp. 98105. ACM.Scharpff, J., Spaan, M. T. J., Volker, L., & De Weerdt, M. (2013). Planning uncertainty coordinating infrastructural maintenance. Proceedings 8th annualworkshop Multiagent Sequencial Decision Making Certainty.Sontag, D., Globerson, A., & Jaakkola, T. (2011). Introduction dual decompositioninference. Optimization Machine Learning, 1, 219254.Tesauro, G., Das, R., Chan, H., Kephart, J. O., Lefurgy, C., Levine, D. W., & Rawson, F.(2007). Managing power consumption performance computing systems usingreinforcement learning. Advances Neural Information Processing Systems 20(NIPS07).Vamplew, P., Dazeley, R., Barker, E., & Kelarev, A. (2009). Constructing stochastic mixture policies episodic multiobjective reinforcement learning tasks. AdvancesArtificial Intelligence, pp. 340349.Yeoh, W., Felner, A., & Koenig, S. (2010). BnB-ADOPT: asynchronous branch-andbound DCOP algorithm. Journal Artificial Intelligence Research, 38, 85133.443fiJournal Artificial Intelligence Research 52 (2015) 179-201Submitted 5/14; published 1/15Agnostic Pointwise-Competitive Selective ClassificationYair WienerRan El-Yanivwyair@tx.technion.ac.ilrani@cs.technion.ac.ilComputer Science DepartmentTechnion Israel Institute TechnologyHaifa 32000, IsraelAbstractpointwise competitive classifier class F required classify identicallybest classifier hindsight F. noisy, agnostic settings present strategylearning pointwise-competitive classifiers finite training sample providedclassifier abstain prediction certain region choice. interesting hypothesis classes families distributions,measurerejected region/2shown diminishing rate 1 (polylog(m) log(1/)/m) 2, high probability, sample size, standard confidence parameter, 1 , 2smoothness parameters Bernstein type condition associated excess loss class(related F 0/1 loss). Exact implementation proposed learning strategydependent ERM oracle hard compute agnostic case. thusconsider heuristic approximation procedure based SVMs, show empiricallyalgorithm consistently outperforms traditional rejection mechanism baseddistance decision boundary.1. IntroductionGiven labeled training set class models F, possible select F, basedfinite training sample, model whose predictions always identical best modelhindsight? classical results statistical learning theory surely precludepossibility within standard model, changing rules game possible.Indeed, consider game classifier allowed abstain prediction withoutpenalty region choice (a.k.a classification reject option). game,assuming noise free realizable setting, shown El-Yaniv Wiener (2010)one train perfect classifier never errs whenever willing predict.always abstaining render perfect classification vacuous, shownquite broad set problems (each specified underlying distribution familyhypothesis class), perfect realizable classification achievable rejection ratediminishes quickly zero training sample size.general, perfect classification cannot achieved noisy setting. paper,objective achieve pointwise competitiveness, property ensuring predictionevery non-rejected test point identical prediction best predictor hindsightclass. consider pointwise-competitive selective classificationgeneralize results El-Yaniv Wiener (2010) agnostic case. particular,show pointwise-competitive classification achievable high probabilitylearning strategy called low error selective strategy (LESS). Given training sample Smc2015AI Access Foundation. rights reserved.fiWiener & El-Yanivhypothesis class F, LESS outputs pointwise-competitive selective classifier (f, g),f (x) standard classifier, g(x) selection function qualifiespredictions dont knows (see definitions Section 2). classifier f simplytaken empirical risk minimizer (ERM) classifier, f. Pointwise competitivenessachieved g follows. Using standard concentration inequalities, showtrue risk minimizer, f , achieves empirical error close f. Thus, highprobability f belongs class low empirical error hypotheses. leftset g(x) allows prediction label x, f(x),hypotheses low error class unanimously agree label x.simpler, realizable setting (El-Yaniv & Wiener, 2010), low error class simply reducesversion space.bulk analysis (in Sections 3, 4 5) concerns coverage bounds LESS,namely, showing measure region classifier (f, g) refuses classify,diminishes quickly, high probability, training sample size grows (see Section 2formal definition). provide several general distribution-dependent coveragebounds. particular, show (in Corollaries 12 14, respectively) high probabilitybounds coverage (f, g) classifier (f, g, ) form,(f, g) 1 1 (polylog(m) log(1/)/m)2 /2 ,linear models (unknown) distribution P (X, ), X feature space pointslabels, whose marginal P (X) finite mixture Gaussians, axisaligned rectangles P (X, ) whose marginal P (X) product distribution,1 , 2 Bernstein class smoothness parameters depending hypothesis classunderlying distribution (and loss function, 0/1 case).outset, efficient implementation LESS seems reachrequired track supremum empirical error possibly infinite hypothesissubset, general might intractable. overcome computational difficulty,propose reduction problem problem calculating (two) constrained ERMs.given test point x, calculate ERM training sample Smconstraint label x (one positive label constraint one negative). showthresholding difference empirical error two constrained ERMsequivalent tracking supremum entire (infinite) hypothesis subset. Basedreduction introduce Section 6 disbelief principle motivates heuristic implementation LESS, relies constrained SVMs, mimics optimalbehavior.Section 7 present numerical examples medical classification problemsexamine empirical performance new algorithm compare performancewidely used selective classification method rejection, based distancedecision boundary.2. Pointwise-Competitive Selective Classification: PreliminaryDefinitionsLet X feature space, example, d-dimensional vectors Rd ,output space. standard classification, goal learn classifier f : X Y, using172fiAgnostic Pointwise-Competitive Selective Classificationfinite training sample labeled examples, Sm = {(xi , yi )}mi=1 , assumed sampledi.i.d. unknown underlying distribution P (X, ) X Y. classifierselected hypothesis class F. Let : [0, 1] bounded loss function.selective classification (El-Yaniv & Wiener, 2010), learning algorithm receives Smrequired output selective classifier, defined pair (f, g), f Fclassifier, g : X {0, 1} selection function, serving qualifier f follows.x X , (f, g)(x) = f (x) iff g(x) = 1. Otherwise, classifier outputs dont know.general performance selective predictor characterized terms two quantities: coverage risk. coverage (f, g) (f, g) , EP [g(x)] . true risk (f, g),respect loss function , average loss f restricted region activityqualified g, normalized coverage, R(f, g) , EP [(f (x), y) g(x)] /(f, g).easy verify g 1 (and therefore (f, g) = 1), R(f, g) reducesfamil1 Pmiar risk functional R(f ) , EP [(f (x), y)]. classifier f , let R(f ) , i=1 (f (xi ), yi ),standard empirical error f sample Sm . Let f = argminf F R(f )empirical risk minimizer (ERM), let f = argminf F R(f ) true risk minimizerrespect unknown distribution P (X, ).1 Clearly, true risk minimizer f unknown. selective classifier (f, g) called pointwise-competitive x X ,g(x) > 0, f (x) = f (x).3. Low Error Selective Strategy (LESS)hypothesis class F, hypothesis f F, distribution P , sample Sm , real numberr > 0, define true empirical low-error sets,V(f, r) , f F : R(f ) R(f ) + r(1)nV(f, r) , f F : R(f ) R(f ) + r .(2)Throughout paper denote (m, , d) slack standard uniform deviationbound, given terms training sample size, m, confidence parameter, ,VC-dimension, d, class F,+ ln 22d ln 2me(m, , d) , 2.(3)following theorem slight extension statement made Bousquet, Boucheron,Lugosi (2004, p. 184).Theorem 1 (Bousquet et al., 2004). Let 0/1 loss function F, hypothesisclass whose VC-dimension d. 0 < < 1, probability least 1choice Sm P , hypothesis f F satisfiesR(f ) R(f ) + (m, , d).Similarly, R(f ) R(f ) + (m, , d) conditions.1formally, f classifier R(f ) = inf f F R(f ) inf f F P ((x, y) : f (x) 6= f (x)) = 0.existence (measurable) f guaranteed sufficient considerations (see Hanneke, 2012,pp. 1511-2).173fiWiener & El-YanivRemark 2. use Theorem 1 and, particular, VC bounds classification problems(0/1 loss) mandatory developing theory presented paper. Similartheories developed using types bounds (e.g., Rademacher compressionbounds) learning problems.Let G F. disagreement set (Hanneke, 2007a; El-Yaniv & Wiener, 2010) w.r.t. GdefinedDIS(G) , {x X : f1 , f2 G s.t. f1 (x) 6= f2 (x)} .(4)Let us motivate low-error selective strategy (LESS) whose pseudo-code appearsStrategy 1. strategy define whenever empirical risk minimizer (ERM) exists,example, case 0/1 loss. Using standard uniform deviation bound,one Theorem 1, one show training error true risk minimizer, f ,cannot far training error empirical risk minimizer, f. Therefore,guarantee, high probability, empirical low error class V f, r (appliedappropriately chosen r) includes true risk minimizer f . selection functiong constructed accept subset domain X , hypothesesempirical low-error set unanimously agree. Strategy 1 formulates idea. callstrategy rather algorithm lacks implementation details. Indeed,clear outset strategy implemented.Strategy 1 Agnostic low-error selective strategy (LESS)Input: Sm , m, ,Output: pointwise-competitive selective classifier (h, g) w.p. 11: Set f = ERM(F, Sm ), i.e., f empirical risk minimizer F w.r.t. Sm2: Set G = V f, 2(m, /4, d) (see Eq. (2) (3))3: Construct g g(x) = 1 x {X \ DIS (G)}4: f = fbegin analysis LESS. following lemma establishes pointwise competitiveness. Section 4 develop general coverage bounds terms undetermineddisagreement coefficient. Then, Section 5 present distribution-dependent boundsrely disagreement coefficient.Lemma 3 (pointwise competitiveness). Let 0/1 loss function F, hypothesisclass whose VC-dimension d. Let > 0 given let (f, g) selective classifierchosen LESS. Then, probability least 1 /2, (f, g) pointwise competitiveselective classifier.Proof. Theorem 1, probability least 1 /4,R(f ) R(f ) + (m, /4, d) .Clearly, since f minimizes true error, R(f ) R(f). Applying Theorem 1,know probability least 1 /4,R(f) R(f) + (m, /4, d) .174fiAgnostic Pointwise-Competitive Selective ClassificationUsing union bound, follows probability least 1 /2,R(f ) R(f) + 2 (m, /4, d) .Hence, probability least 1 /2,f V f, 2 (m, /4, d) , G.definition, LESS constructs selection function g(x) equals one iff xX \DIS (G) . Thus, x X , g(x) = 1, hypotheses G agree,particular f f agree. Therefore (f, g) pointwise-competitive high probability.4. General Coverage Bounds LESS Terms DisagreementCoefficientrequire following definitions facilitate coverage analysis. f Fr > 0, define set B(f, r) hypotheses reside ball radius r around f ,B(f, r) , f F : Pr f (X) 6= f (X) r .XPG F, distribution P , denote G volume disagreement setG (see (4)), G , Pr {DIS(G)}. Let r0 0. disagreement coefficient (Hanneke,2009) hypothesis class F respect target distribution P(r0 ) , f (r0 ) = supr>r0B(f , r).r(5)disagreement coefficient utilized later analysis. See also discussioncharacteristics Corollary 7. associated excess loss class class Floss function (Massart, 2000; Mendelson, 2002; Bartlett, Mendelson, & Philips, 2004)definedXL(F, )(x, y) , {(f (x), y) (f (x), y) : f F} .Whenever F fixed abbreviate XL = XL(F, )(x, y). XL said(1 , 2 )-Bernstein class respect P (where 0 < 2 1 1 1), every h XLsatisfiesEh2 1 (Eh)2 .(6)Bernstein classes arise many natural situations (see, e.g., Koltchinskii, 2006; Bartlett &Mendelson, 2006; Bartlett & Wegkamp, 2008). example, conditional probabilityP (Y |X) bounded away 1/2, satisfies Tsybakovs noise conditions2 ,excess loss function Bernstein class (Bartlett & Mendelson, 2006; Tsybakov, 2004).32data generated unknown deterministic hypothesis limited noise P (Y |X)bounded away 1/2.3Specifically, 0/1 loss, Assumption Proposition 1 work Tsybakov (2004), equivalentBernstein class condition Equation (6) 2 = 1+, Tsybakov noiseparameter.175fiWiener & El-Yanivfollowing sequence lemmas theorems assume binary hypothesis class FVC-dimension d, underlying distribution P X {1}, 0/1 lossfunction. Also, XL denotes associated excess loss class. results extendedloss functions 0/1 similar techniques used Beygelzimer, Dasgupta,Langford (2009).Figure 1 schematically depict hypothesis class F (the gray area), targethypothesis (filled black circle outside F), best hypothesis class f .distance two points diagram relates distance two hypothesismarginal distribution P (X). first observation excess loss class(1 , 2 )-Bernstein class, set low true error (depicted Figure 1 (a)) resideswithin larger ball centered around f (see Figure 1 (b)).Figure 1: set low true error (a) resides within ball around f (b).Lemma 4. XL (1 , 2 )-Bernstein class respect P , r > 0V(f , r) B f , 1 r2 .Proof. f V(f , r) then, definition, E {I(f (X) 6= )} E {I(f (X) 6= )} + r.linearity expectation have,E {I(f (X) 6= ) I(f (X) 6= )} r.Since XL (1 , 2 )-Bernstein,E {I(f (X) 6= f (X))} = E {|I(f (X) 6= ) I(f (X) 6= )|}n= E ((f (X), ) (f (X), ))2 , Eh2 1 (Eh)2, 1 (E {I(f (X) 6= ) I(f (X) 6= )})2 .(7), E {I(f (X) 6= f (X))} 1 r2 . Therefore, definition, f B f , 1 r2 .176(7)fiAgnostic Pointwise-Competitive Selective Classificationfar seen set low true error resides within ball around f .would like prove high probability set low empirical error (depictedFigure 2 (a)) resides within set low true error (see Figure 2 (b)). emphasizedistance hypotheses Figure 2 (a) based empirical error,distance Figure 2 (b) based true error.Figure 2: set low empirical error (a) resides within set low true error (b).Lemma 5. r > 0, 0 < < 1, probability least 1 ,V(f, r) V (f , 2 (m, /2, d) + r) .Proof. f V(f, r), then, definition, R(f ) R(f) + r. Since f minimizes empiricalerror, know R(f) R(f ). Using Theorem 1 twice, applying union bound,see probability least 1 ,R(f ) R(f ) + (m, /2, d)R(f ) R(f ) + (m, /2, d).Therefore,R(f ) R(f ) + 2 (m, /2, d) + r,f V (f , 2 (m, /2, d) + r) .shown that, high probability, set low empirical error subsetcertain ball around f . Therefore, probability least two hypotheses setlow empirical error disagree bounded probabilityleast two hypotheses ball around f disagree other. Fortunately,latter bounded disagreement coefficient established following lemma.177fiWiener & El-YanivLemma 6. r > 0 0 < < 1, probability least 1 ,V(f, r) 1 (2 (m, /2, d) + r)2 (r0 ),(r0 ) disagreement coefficient F respect P , applied r0 =(2(m, /2, d))2 (see (5)).Proof. Applying Lemmas 5 4 get probability least 1 ,V(f, r) B f , 1 (2 (m, /2, d) + r)2 .Therefore,V(f, r) B f , 1 (2 (m, /2, d) + r)2 .definition disagreement coefficient (5), r > r0 , B(f , r ) (r0 )r .Recalling 1 1 thus observing r = 1 (2 (m, /2, d) + r)2 > (2 (m, /2, d))2 =r0 , proof complete.position state first coverage bound selective classifierconstructed LESS. bound given terms disagreement coefficient.Corollary 7. Let F hypothesis class Theorem 1, assume XL(1 , 2 )-Bernstein class w.r.t. P . Let (f, g) selective classifier constructed LESS.Then, probability least 1 , (f, g) pointwise competitive selective classifier(f, g) 1 1 (4 (m, /4, d))2 (r0 ),(r0 ) disagreement coefficient F respect P , r0 = (2(m, /4, d))2 .Proof.Lemma 3,probability least 1/2, (f, g) pointwise-competitive. SetG , V f , 2 (m, /4, d) . construction, f = f, selection function g(x) equalsone iff x X \ DIS (G). Thus, definition coverage, (f, g) = E{g(X)} = 1 G.Therefore, applications Lemma 6 union bound imply probabilityleast 1 , (f, g) pointwise-competitive coverage satisfies,(f, g) = E{g(X)} = 1 G 1 1 (4 (m, /4, d))2 (r0 ),Noting (r) monotone non-increasing r, know coverage boundCorollary 7 clearly applies (0). quantity (0) discussed numerous papers shown finite various settings including thresholds Rdistribution ((0) = 2) (Hanneke, 2009), linearseparators originRd uniform distribution sphere ((0) d) (Hanneke, 2009), linearseparators Rd smooth data distribution bounded away zero ((0) c(f )d,c(f ) unknown constant depends target hypothesis) (Friedman,2009). cases, application Corollary 7 sufficient guarantee pointwisecompetitiveness bounded coverage converges one. Unfortunately manyhypothesis classes distributions disagreement coefficient (0) infinite (Hanneke,2009). Fortunately, disagreement coefficient (r) grows slowly respect 1/r (asshown Wang, 2011, sufficient smoothness conditions), Corollary 7 sufficientguarantee bounded coverage.178fiAgnostic Pointwise-Competitive Selective Classification5. Distribution-Dependent Coverage Bounds LESSsection establish distribution-dependent coverage bounds LESS. startingpoint bounds following corollary.Corollary 8. Let F hypothesis class Theorem 1, assume F disagreement coefficient(r0 ) = (polylog (1/r0 ))(8)w.r.t. distribution P , XL (1 , 2 )-Bernstein class w.r.t. distribution.Let (f, g) selective classifier chosen LESS. Then, probability least 1 ,(f, g) pointwise competitive coverage satisfies,!polylog(m)1 2 /2(f, g) 1 1log.Proof. Plugging (8) coverage bound Corollary 7 immediately yields result.Corollary 8 states fast coverage bounds LESS cases disagreement coefficient grows slowly respect 1/r0 .4 Recent results disagreement-based activelearning selective prediction (Wiener et al., 2014; Wiener, 2013) established tight relations disagreement coefficient empirical quantity called version spacecompression set size. quantity analyzed El-Yaniv Wiener (2010)context realizable selective classification, known distribution-dependentbounds it. plan rest section introduce version space compression set size, discuss relation disagreement coefficient, showapply results agnostic setting.interested solving agnostic case, consider momentrealizable setting utilize known results used analysis. Specifically,assume f F P(Y = f (x)|X = x) = 1 x X ,(X, ) P . Given training sample Sm , let VSF ,S induced version space, i.e.,set hypotheses consistent given sample Sm . version space compressionset size, denoted n(Sm ) = n(F, Sm ), defined size smallest subsetSm inducing version space (Hanneke, 2007b; El-Yaniv & Wiener, 2010).function Sm , clearly n(Sm ) random variable, specific realization Smvalue unique.(0, 1], define version space compression set size minimal boundBn (m, ) , min {b N : P(n(Sm ) b) 1 } .(9)rely following lemma (Wiener et al., 2014). sake self-containmentprovide proof appendix.4disagreement coefficient grow ploy-logarithmically 1/r0 still o(1/r0 ),still possible prove lower bound coverage. Specifically, (r0 ) = ((1/r0 ) ) < 1, oneshow (f, g) 1 O(1/( m)2 (1) ).179fiWiener & El-YanivLemma 9 (Wiener et al., 2014). realizablecase, Bn (m,) = (polylog(m) log (1/)),11Bn m, 20 = (polylog(m)), (r0 ) = polylog r0 .Obviously, statement Lemma 9 holds (and well defined) within realizablesetting (the version space compression set size defined setting). turnback agnostic setting consider arbitrary underlying distribution P X Y.Recall agnostic setting, let f : X denote (measurable) classifierR(f ) = inf f F R(f ) inf f F P ((x, y) : f (x) 6= f (x)) = 0, guaranteedexist sufficient assumptions (see Hanneke, 2012, Section 6.1); call f infimal(best) hypothesis (of F, w.r.t. P ). Clearly several different infimal hypotheses.note, however, XL (1 , 2 )-Bernstein class respect P (as assumepaper), Lemma 4 ensures infimal hypotheses identical measurezero.definitions version space version space compression set size naturallygeneralized agnostic setting respect infimal hypothesis (Wiener et al.,2014) follows. Let f infimal hypothesis F w.r.t. P . agnostic version spaceSmVSF ,Sm ,f , {f F : (x, y) Sm , f (x) = f (x)}.agnostic version space compression set size, denoted n(Sm ) = n(F, Sm , f ), definedsize smallest subset Sm inducing agnostic version space VSF ,Sm ,f .Finally, extend also definition version space compression set minimal boundagnostic setting follows.Bn (m, , f ) , min{b N : P(n(F, Sm , f ) b) 1 }.key observation allows surprisingly easy utilization Lemma 9agnostic setting disagreement coefficient depends hypothesis classF marginal distribution P (X). Using infimal hypothesis f thereforetake agnostic learning problem consider realizable projection, whereby pointslabeled f marginal distribution P (X). two problems(essentially) disagreement coefficients. idea initially observedHanneke (2013) Wiener (2013). formulate slight variationformulation work Wiener, Hanneke, El-Yaniv (2014).define disagreement agnostic setting (5) respect infimal hypothesis f . agnostic learning problem (F, P ) define realizableprojection (F , P ) follows. Let F , F {f } f infimal hypothesisagnostic problem. Define P distribution marginal P (X) = P (X),P(Y = f (x)|X = x) = 1 x X . easy verify (F , P ) realizablelearning problem, i.e., f F PP (X,Y ) (Y = f (x)|X = x) = 1 x X .Lemma 10 (Realizable projection). Given agnostic learning problem, (F, P ), let(F , P ) realizable projection. Let (r0 ) (r0 ) associated disagreement coefficients agnostic realizable projection problems, respectively. Then, (r0 ) (r0 ).Proof. First note depend, respectively, P P via fmarginal distributions P (X) = P (X). Since F F {f } = F , readily get(r0 ) (r0 ).180fiAgnostic Pointwise-Competitive Selective ClassificationLet us summarize derivation. Given agnostic problem (F, P ), considerrealizable projection (F , P ). Bn (m, ) = (polylog(m) log (1/)) (or Bn (m, 1/20) =(polylog(m))) realizable problem, Lemma 9, (r0 ) = (polylog (1/r0 )),which, Lemma 10, also holds original agnostic problem. Therefore, Corollary 7applies obtain fast coverage bound LESS w.r.t. (F, P ).New agnostic coverage bounds LESS obtained using following known bounds(realizable) version space compression set size. first one, El-Yaniv Wiener(2010), applies problem learning linear separators mixture Gaussiandistributions. following theorem direct application Lemma 32 workEl-Yaniv Wiener (2010).Theorem 11 (El-Yaniv & Wiener, 2010). d, n N, let X Rd , F spacelinear separators Rd , P distribution marginal Rd mixturen multivariate normal distributions. Then, constant cd,n > 0 (dependingd, n, otherwise independent P ) 2,Bn (m, 1/20) cd,n (log(m))d1 .Applying Theorem 11, together Lemma 10, Lemma 9 Corollary 8, immediatelyyields following result.Corollary 12. Assume conditions Theorem 11. Assume also XL (1 , 2 )Bernstein class w.r.t. P (X, ). Let (f, g) selective classifier constructed LESS.Then, probability least 1 , (f, g) pointwise competitive selective classifier(f, g) 1 1 (polylog(m) log(1/)/m)2 /2 .second version space compression set size bound concerns realizable learningaxis-aligned rectangles product densities Rn . bounds previouslyproposed Wiener, Hanneke, El-Yaniv (2014) El-Yaniv Wiener (2010, 2012).state (without proof) recent bound (Wiener, Hanneke, & El-Yaniv, 2014) givingversion space compression set size bound learning problem (whose positive classbounded away zero).Theorem 13 (Wiener et al., 2014). d, N , (0, 1), let X Rd . Pmarginal distribution Rd product densities Rd marginalscontinuous CDFs, F space axis-aligned rectangles f RdP ((x, y) : f (x) = 1) ,8d8dBn (m, )ln.again, application Theorem 13, together Lemma 10, Lemma 9 Corollary 8 yields following corollary.Corollary 14. d, N , (0, 1), let X Rd . Let P (X, ) underlyingdistribution marginal P (X) product densities Rd marginalscontinuous CDFs. Let F space axis-aligned rectangles f Rd P ((x, y) : f (x) =1) , Assume XL (1 , 2 )-Bernstein class w.r.t. P (X, ). Let (f, g)181fiWiener & El-Yanivselective classifier constructed LESS. Then, probability least 1 , (f, g)pointwise competitive selective classifier(f, g) 1 1 (polylog(m) log(1/)/m)2 /2 .6. ERM Oracles Disbelief principleoutset, efficient construction selection function g prescribed LESS seemsreach required verify, point x question, whetherhypotheses low error class agree label. Moreover, g computedentire domain. Luckily, possible compute g lazy manner showcompute g(x) calculating (two) constrained ERMs. given test point x,calculate ERM training sample Sm constraint label x (onepositive label constraint one negative). show thresholding differenceempirical error two constrained ERMs equivalent tracking supremumentire (infinite) hypothesis subset. following lemma establishes reduction.Lemma 15. Let (f, g) selective classifier chosen LESS observing trainingsample Sm . Let f empirical risk minimizer Sm . Let x point Xdefinenfx , argmin R(f ) | f (x) = sign f(x) ,f Fi.e., empirical risk minimizer forced label x opposite f(x).g(x) = 0R(fx ) R(f) 2 (m, /4, d) .Proof. First note according definition V (see Eq (2)),R(fx ) R(f) 2 (m, /4, d) fx V f, 2 (m, /4, d) .(10)(11)prove first direction (=) (10), assume RHS (10) holds. (11),get f, fx V. However, construction, f(x) = fx (x), x DIS(V)g(x) = 0.prove direction (=), assume R(fx ) R(f) > 2 (m, /4, d).assumption, prove f V, f (x) = f(x), therefore, xX \ DIS(V), entailing g(x) = 1. Indeed, assume contradiction existsf V f (x) = fx (x) 6= f(x). construction, holdsR(f ) R(fx ) > R(f) + 2 (m, /4, d) ,f 6 V. Contradiction.Lemma 15 tells us order decide point x rejected need measureempirical error R(fx ) special empirical risk minimizer, fx , constrainedlabel x opposite h(x). error sufficiently close R(h), classifier cannotsure label x must reject it. Thus, provided computeERMs, decide whether predict reject individual test point x X ,182fiAgnostic Pointwise-Competitive Selective Classificationwithout actually constructing g entire domain X . Figure 3 illustrates principle2-dimensional example. hypothesis class class linear classifiers R2source distribution two normal distributions. Negative samples representedblue circles positive samples red squares. usual, f denotes empiricalFigure 3: Constrained ERM.risk minimizer. Let us assume want classify point x1 . point classifiedpositive f. Therefore, force point negative calculate restrictedERM (depicted doted line marked fx1 ). difference empirical risk ffx1 large enough, point x1 rejected. However, want classifypoint x2 , difference empirical risk f fx2 quite large pointclassified positive.Equation (11) motivates following definition disbelief index DF (x, Sm )individual point X . Specifically, x X , define disbelief index w.r.t. SmF,D(x) , DF (x, Sm ) , R(fx ) R(f).Observe D(x) large whenever model sensitive label x senseforced bend best model fit opposite label x, modelsubstantially deteriorates, giving rise large disbelief index. large D(x)interpreted disbelief possibility x labeled differently.case definitely predict label x using unforced model. Conversely,D(x) small, model indifferent label x sense, committedlabel. case abstain prediction x. Notice LESSspecific application thresholded disbelief index.note similar technique using ERM oracle enforce arbitrarynumber example-based constraints used Dasgupta, Hsu, Monteleoni (2007a)Beygelzimer, Hsu, Langford, Zhang (2010), context active learning.disbelief index, difference empirical risk (or importance weightedempirical risk, see Beygelzimer et al., 2010) two ERM oracles (with different constraints)used estimate prediction confidence.183fiWiener & El-Yaniv0.10.160.140.09test errortest error0.120.10.080.080.070.060.040.060.0200.20.40.60.80.050.11c0.20.30.40.50.6cFigure 4: RC curve technique (depicted red) compared rejection baseddistance decision boundary (depicted dashed green line). RC curveright figure zooms lower coverage regions left curve.practical applications selective prediction desirable allow controltrade-off risk coverage; words, desirable abledevelop entire risk-coverage (RC) curve classifier hand (see, e.g., El-Yaniv &Wiener, 2010) let user choose cutoff point along curve accordancepractical considerations constraints. disbelief index facilitates explorationrisk-coverage trade-off curve classifier follows. Given pool test pointsrank test points according disbelief index, points low indexrejected first. Thus, ranking provides means constructing risk-coveragetrade-off curve. Ignoring moment implementation details (which discussedSection 7), typical RC curve generated LESS depicted Figure 4 (red curve)5 .dashed green RC curve computed using traditional distance-based techniquesrejection (see discussion common technique Section 8) right graph zoomsection entire RC curve (depicted left graph). dashed horizontal linetest error f entire domain dotted line Bayes error.high coverage values two techniques statistically indistinguishable, coverageless 60% get significant advantage LESS. clear caseestimation error reduced, also test error goes significantly optimaltest error f low coverage values.Interestingly, disbelief index generates rejection regions fundamentally different obtained traditional distance-based techniques rejection (seeSection 8). illustrate point (and still ignoring implementation details), considerFigure 5 depict rejection regions training sample 150 points sampledmixture two identical normal distributions (centered different locations).height map figure, correspond disbelief index magnitude (a), distancedecision boundary (b), reflect confidence regions technique accordingconfidence measure.5learning problem synthetic problem used generating Figure 6.184fiAgnostic Pointwise-Competitive Selective Classification(a)(b)Figure 5: Linear classifier. Confidence height map using (a) disbelief index; (b) distancedecision boundary.Figure 6: SVM polynomial kernel. Confidence height map using (a) disbelief index;(b) distance decision boundary.intuitively explain height map Figure 5(a), recall disbelief indexdifference empirical error ERM restricted ERM. testpoint resides high density region, expect forcing wrong label pointresult large increase training error. result, denser area is,larger disbelief index, therefore, higher classification confidence.second synthetic 2D source distribution consider even striking. Xdistributed uniformly [0, 3] [2, 2] labels sampled accordingfollowing conditional distribution0.95, x2 sin(x1 );P (Y = 1|X = (x1 , x2 )) ,0.05, else.thick red line depicts decision boundary Bayes classifier. hight mapsFigure 6 depict rejection regions obtained (our approximation of) LESS185fiWiener & El-Yanivtraditional (distance decision boundary) technique training sample 50points sampled distribution (averaged 100 iterations). hypothesisclass used training SVM polynomial kernel degree 5. qualitativedifference two techniques, particular, nice fit disbeliefprinciple technique compared SVM quite surprising.Figure 7: RC curves SVM linear kernel. method solid red, rejectionbased distance decision boundary dashed green. Horizontal axis (c)represents coverage.7. Heuristic Procedure Using SVM Empirical Performancecomputation (constrained) ERM oracle efficiently achieved caserealizable learning linear models (see, e.g., El-Yaniv & Wiener, 2010) caselinear regression (Wiener & El Yaniv, 2012). However, noisy setting computationlinear ERM oracle reduced variant MAX FLS C MAX FLSproblems (with strict non-strict inequalities) (Amaldi & Kann, 1995). Unfortunately,186fiAgnostic Pointwise-Competitive Selective ClassificationMAX FLS APX-complete (within factor 2). C MAX FLS MAX IND SET-hard,cannot approximated efficiently all. Moreover, extensions resultsclasses, including axis-aligned hyper-rectangles, showing approximating ERMclasses NP-hard (Ben-David et al., 2003).present known hardness results (and related lowerbounds) hold half spaces nice distributions Gaussian (mixtures), noteTauman Kalai et al. (2008) studied problem agnostically learning halfspacesdistributional assumptions. particular, showed data distributionuniform d-dimensional unit sphere (or hyper-cube, related distributions),4possible agnostically learn -accurate halfspaces time poly(d1/ ). However,known particular distributions elicit effective pointwise competitivelearning. contrary, uniform distribution unit sphere amongworst possible distributions pointwise-competitive classification (and disagreement-basedactive learning) unless one utilizes homogeneous halfspaces (see discussion in, e.g., El-Yaniv& Wiener, 2010).Figure 8: SVM linear kernel. maximum coverage distance-based rejectiontechnique allows error rate method specific coverage.187fiWiener & El-Yanivdiscussed computational hurdles, recall much appliedmachine learning research many applications quite well heuristicapproximations (rather formal ones). practical performance objective,clever heuristics tricks sometimes make difference. point papertherefore switch theory practice, aiming implementing rejection methodinspired disbelief principle see well work real world problems.approximate ERM follows. Using support vector machines (SVMs) usehigh C value (105 experiments) penalize training errors smallmargin (see definitions SVM parameters in, e.g. Chang & Lin, 2011). waysolution optimization problem tend get closer ERM. order estimateR(fx ) restrict SVM optimizer consider hypotheses classifypoint x specific way. accomplish use weighted SVM unbalanced data.add point x another training point weight 10 times larger weighttraining points combined. Thus, penalty misclassification x largeoptimizer finds solution doesnt violate constraint.Another problem face disbelief index noisy statistic highlydepends sample Sm . overcome noise use robust statistics. First1 , 2 , . . . k ) using bootstrap samplinggenerate odd number k different samples (Sm(we used k = 11). sample calculate disbelief index test pointspoint take median measurements final index. also notefinite training sample disbelief index discrete variable. often caseseveral test points share disbelief index. cases use confidencemeasure tie breaker. experiments use distance decision boundarybreak ties. Focusing SVMs linear kernel compared RC (Risk-Coverage)curves achieved proposed method achieved SVM rejection baseddistance decision boundary. latter approach common practicalapplications selective classification. implementation used LIBSVM (Chang &Lin, 2011).tested algorithm standard medical diagnosis problems UCI repository, including datasets used Grandvalet, Rakotomamonjy, Keshet, Canu (2008).transformed nominal features numerical ones standard way using binary indicator attributes. also normalized attribute independently dynamicrange [0, 1]. preprocessing employed. iteration choose uniformlyrandom non-overlapping training set (100 samples) test set (200 samples)dataset.6 SVM trained entire training set, test samples sortedaccording confidence (either using distance decision boundary disbelief index).Figure 7 depicts RC curves technique (red solid line) rejection baseddistance decision boundary (green dashed line) linear kernel 6 datasets.results averaged 500 iterations (error bars show standard error). exceptionHepatitis dataset, methods statistically indistinguishable,datasets proposed method exhibits significant advantage traditionalapproach. would like highlight performance proposed methodPima dataset. traditional approach cannot achieve error less 8%6Due size Hepatitis dataset test set limited 29 samples.188fiAgnostic Pointwise-Competitive Selective ClassificationFigure 9: RC curves SVM RBF kernel. method solid red rejectionbased distance decision boundary dashed green.rejection rate, approach test error decreases monotonically zero rejectionrate. Furthermore, clear advantage method large range rejection ratesevident Haberman dataset.7 .sake fairness, note running time algorithm (as presentedhere) substantially longer traditional technique. performance algorithm substantially improved many unlabeled samples available.case rejection function evaluated unlabeled samples generate newlabeled sample. new rejection classifier trained sample.Figure 8 depicts maximum coverage distance-based rejection techniqueallows error rate method specific coverage. example, let usassume method error rate 10% coverage 60%7Haberman dataset contains survival data patients undergone surgery breast cancer.estimated 207,090 new cases breast cancer united states 2010 (Society, 2010)improvement 1% affects lives 2000 women.189fiWiener & El-YanivFigure 10: SVM RBF kernel. maximum coverage distance-based rejectiontechnique allows error rate method specific coverage.distance-based rejection technique achieves error maximum coverage 40%.point (0.6, 0.4) red line. Thus, red line bellow diagonaltechnique advantage distance-based rejection visa versa.example, consider Haberman dataset, observe regardless rejection rate,distance-based technique cannot achieve error technique coveragelower 80%.Figures 9 10 depict results obtained RBF kernel. case statisticallysignificant advantage technique observed datasets.8. Related WorkPointwise-competitive classification unique extreme instance classificationabstention option, idea emerged pattern recognition community,first proposed studied 50 years ago Chow (1957, 1970), generated lots interest190fiAgnostic Pointwise-Competitive Selective Classification(Fumera et al., 2001; Tortorella, 2001; Santos-Pereira & Pires, 2005; Fumera & Roli, 2002;Pietraszek, 2005; Bounsiar et al., 2006; Landgrebe et al., 2006; Herbei & Wegkamp, 2006;Hellman, 1970; El-Yaniv & Pidan, 2011; Bartlett & Wegkamp, 2008; Wegkap, 2007; Freundet al., 2004). Taking broader perspective, pointwise-competitive selective prediction (andparticular, classification) particular instance broader concept confidencerated learning, whereby learner must formally quantify confidence prediction.Achieving effective confidence-rated prediction (including abstention) longstandingchallenging goal number disciplines research communities. Let us first discussprominent approaches confidence-rated prediction noterelated present work.knows-what-it-knows (KWIK) framework studied reinforcement-learning (Li,Littman, & Walsh, 2008; Strehl & Littman, 2007; Li & Littman, 2010) similar notionpointwise competitiveness studied, coverage rates analyzed (Li et al., 2008;Li, 2009). However, KWIK limited realizable model concernedadversarial setting target hypothesis training data selectedadversary. positive results KWIK adversarial setting applystatistical pointwise-competitive prediction setting (where training examples sampledi.i.d.), adversarial setting precludes non trivial coverage interesting hypothesisclasses currently addressed pointwise-competitive prediction. deficiency comessurprise KWIK adversarial setting much challenging statisticalpointwise-competitive prediction assumptions.conformal prediction framework (Vovk, Gammerman, & Shafer, 2005) provideshedged predictions allowing possibility multi-labeled predictions guaranteesuser-desired confidence rate asymptotic sense. Conformal prediction mainly concerned online probabilistic setting. Rather predicting single labelsample point, conformal predictor assign multiple labels. user-defined confidence level error rate asymptotically guaranteed. interpretingmulti-labeled predictions rejection, compare pointwise-competitive prediction. sense, conformal prediction construct online predictors reject optionasymptotic performance guarantees. important differences conformal predictions pointwise-competitive prediction pointed out.approaches provide hedged predictions, use different notions hedging. Whereaspointwise-competitive prediction goal guarantee high probabilitytraining sample predictor agrees best predictor classpoints accepted domain, goal conformal predictions provide guaranteesaverage error rate, average taken possible samples testpoints.8 sense, conformal prediction cannot achieve pointwise competitiveness.addition, conformal prediction also utilizes different notion error one usedpointwise-competitive model. pointwise-competitive prediction focused performance guarantees error rate covered (accepted) examples, conformalprediction provides guarantee examples (including multiple predictions none all). increasing multi-labeled prediction rate (uncertain prediction),8noted Vovk et al.: impossible achieve conditional probability error equal givenobserved examples, unconditional probability error equals . Therefore, implicitlyinvolves averaging different data sequences... (Vovk et al., 2005, p. 295).191fiWiener & El-Yaniverror rate decreased arbitrarily small value. casepointwise-competitive prediction error notion covered examples, boundedBayes error covered region. Finally, conformal prediction mentionsnotion efficiency, similar coverage but, best knowledge,finite sample results established. Another interesting scheme vicinityconfidence-rated learning guaranteed error machine (GEM) (Campi, 2010).GEM model reject option considered correct answer, means riskreduced arbitrarily (as conformal prediction).Pointwise-competitive classification special case pointwise-competitive prediction(El-Yaniv & Wiener, 2010, 2011; Wiener & El Yaniv, 2012; El-Yaniv & Wiener, 2012;Wiener, 2013; Wiener et al., 2014). Pointwise-competitive selective classification firstaddressed El-Yaniv Wiener (2010) realizable case studied (inpaper pointwise-competitiveness termed perfect classification). present articleextends pointwise-competitive classification noisy problemsalso number theoretical studies (general) selective classification (notpointwise-competitive). Freund et al. (2004) studied simple ensemble method binaryclassification. Given hypothesis class F, method outputs weighted averagehypotheses F, weight hypothesis exponentially dependsindividual training error. algorithm abstains prediction whenever weightedaverage individual predictions close zero. able bound probabilitymisclassification 2R(f ) + (m) and, conditions, proved bound5R(f ) + (F, m) rejection rate. LESS strategy viewed extremevariation Freund et al. method. include ensemble hypothesessufficiently low empirical error abstain weighted average predictionsdefinitive ( 6= 1). risk coverage bounds asymptotically tighter.Excess risk bounds developed Herbei Wegkamp (2006) modelrejection incurs cost [0, 1/2]. bound applies empirical risk minimizerhypothesis class ternary hypotheses (whose output {1, reject}). See alsovarious extensions Wegkap (2007) Bartlett Wegkamp (2008).rejection mechanism SVMs based distance decision boundary perhapswidely known used rejection technique. routinely used medical applications (Mukherjee et al., 1998; Guyon et al., 2002; Mukherjee, 2003). papers proposedalternative techniques rejection case SVMs. include taking rejectarea account optimization (Fumera & Roli, 2002), training two SVM classifiersasymmetric cost (Sousa, Mora, & Cardoso, 2009), using hinge loss (Bartlett &Wegkamp, 2008). Grandvalet et al. (2008) proposed efficient implementation SVMreject option using double hinge loss. empirically compared resultstwo selective classifiers: one proposed Bartlett Wegkamp (2008)traditional rejection based distance decision boundary. experimentsstatistically significant advantage either method compared traditionalapproach high rejection rates.Pointwise selective classification strongly tied disagreement-based active learning.realizable case, El-Yaniv Wiener (2012) presented reduction stream-basedactive learning CAL algorithm Cohn et al. (1994) pointwise-competitiveclassification. reduction roughly states rejection rate (the reciprocal192fiAgnostic Pointwise-Competitive Selective Classificationcoverage) LESS O(polylog(m/)/m) problem (F, P ) actively learnableCAL exponential speedup. consequence reduction resulted firstexponential speedup bounds CAL general linear models finite mixtureGaussians. direction, showing exponential speedup CAL impliesrejection rate LESS (in realizable setting) recently established Wiener(2013) Wiener, Hanneke, El-Yaniv (2014) (using two different techniques).version space compression set size, extensively utilized presentwork, introduced implicitly Hanneke (2007b) special case extendedteaching dimension, context, version space compression set calledminimal specifying set. introduced explicitly El-Yaniv Wiener (2010)context selective classification, proved El-Yaniv Wiener (2012)special case extended teaching dimension Hanneke (2007b). Relationsdisagreement coefficient version space compression set size first discussedEl-Yaniv Wiener (2012). Sharp ties two quantities, statedLemma 9, others recently developed Wiener, Hanneke, El-Yaniv(2014).9. Concluding Remarksfind existence pointwise-competitive classification quite fascinating. strikingfeature classifier that, definition, pointwise-competitive predictor freeestimation error cannot overfit. means hypothesis classexpressive like still protected overfitting. However, withouteffective coverage bounds pointwise-competitive classifier may refuse predicttimes.current paper, recent studies selective prediction (El-Yaniv & Wiener,2015) active learning (Wiener, Hanneke, & El-Yaniv, 2014), place version spacecompression set size center stage, leading quantity drive resultsintuition domains. present, known technique able prove fastcoverage pointwise-competitive classification exponential label complexity speedupdisagreement-based active learning general linear models fixed mixtureGaussians axis aligned rectangles product distributions.. possibleextend results beyond linear classifiers axis aligned rectangles interestingdistribution families? example, plausible existing results axis-alignedrectangles extended decision trees.formal relationship active learning pointwise-competitive classification(El-Yaniv & Wiener, 2012; Wiener, 2013; Wiener et al., 2014) created powerful synergyallows migrating results two models. Currently, formal connection manifested via two links. first, within realizable setting, equivalenceLESS-based classification fast coverage CAL-based active learning exponential speedup. second link consists bounds relate underlying complexitymeasures: disagreement coefficient active learning, version space compressionset size pointwise-competitive classification. number non-established relations significantly substantiate interaction two problems couldconsidered. example, possible prove direct equivalence LESS-based193fiWiener & El-Yanivpointwise-competitive agnostic classification fast coverage rates LESS-based activelearning exponential speedup? expect resolution questionvarious interesting implications. example, relationship could potentially facilitatemigration interesting algorithms techniques devised active learningpointwise-competitive framework. immediate candidate algorithm Beygelzimer et al. (2010), builds ideas Dasgupta et al. (2007b) Beygelzimer et al.(2009). Resembling implementation proposed LESS via calls (a constrained) ERMoracle, algorithm works without tracking version space final choicehypothesis well querying component. Instead, querying, reliesERM oracle enforces one example-based constrain. Thus, importanceweighting technique based resembles disbelief principle outline here.regard, interesting also consider migrate ideas activelearning algorithms emerging online learning branch (Orabona & Cesa-Bianchi,2011; Cesa-Bianchi et al., 2009; Dekel et al., 2010) using, required, online batchconversion techniques (Zhang, 2005; Kakade & Tewari, 2009; Cesa-Bianchi & Gentile, 2008;Dekel, 2008).LESS strategy requires unanimous vote among hypotheses low empiricalerror subset hypothesis class. considering, e.g., linear models, subsethypotheses uncountable, case (even finite) size huge. Clearly,LESS extremely radical defensive strategy. immediate question ariseswhether LESS unanimity requirement relaxed majority vote.achieve pointwise competitiveness (strong) majority vote instead unanimity?Besides greater flexibility general voting scheme, may lead different typesinteresting learning algorithms, relaxation potentially ease computationalcomplexity implementing LESS (which, discussed above, bottleneck agnosticclassification). example, relaxed voting scheme might utilize hypothesissampling, classical example related context celebrated query-bycommittee (QBC) strategy (Seung et al., 1992; Freund et al., 1997; Fine et al., 2002; GiladBachrach, 2007; Gilad-Bachrach et al., 2005). However, strict pointwise competitivenessadvocated, easy see strong majority vote sufficient. Indeed, considerf differs hypotheses F single point X . Unless probabilitypoint large (not typical case), high probability point parttraining set Sm , therefore, majority vote (even strong) labelopposite f . Hence, worst case, even strong majority sufficient pointwisecompetitiveness. natural compromise pointwise competitiveness objective, onerevert standard excess-risk bounds (Bartlett et al., 2006) whereby compareoverall average performance predictor, R(f ), optimal predictor, R(f )(not pointwise). regard, work Freund, Mansour, Schapire (2004) discussedSection 8, result excess-risk bound R(f, g) 2R(f) + 1/(m1/2)( hyper-parameter) coverage bound (f, g) 1 5R(f ) ln |F|/ m1/2 .Considering excess-risk bounds f , possible beat risk coveragebounds using relaxed voting scheme rejection? would optimal boundsfully agnostic setting? better bounds devised specific distributions likeGaussian mixtures? note Freund et al. strategy also interesting194fiAgnostic Pointwise-Competitive Selective Classificationfinal aggregated predictor general outside F can, principle, significantlyoutperform f F (the bound elicit behavior). emphasizespotential usefulness ensembles, applied rejection scheme, alsofinal predictor. Recall LESS strategy final predictor always belongs F.Thus, considering ensembles allowing excess-risk bounds, evenambitious goals, strictly beating f average.Acknowledgmentsthank anonymous referees good comments, grateful Steve Hanneke helpful insightful discussions. Also, warmly thank Intel CollaborativeResearch Institute Computational Intelligence (ICRI-CI), Israel Science Foundation(ISF) generous support.Appendix A. Proofsproof Lemma 9 relies following Lemma 16 (Wiener et al., 2014), whoseproof also provided sake self-containment.Lemma 16 (Wiener et al., 2014). realizable case, r0 (0, 1),11,, 512 .(r0 ) max max 16Bnr 20r(r0 ,1)Proof. prove that, r (0, 1),B(f , r)11max 16Bn,, 512 .rr 20(12)result follows taking supremum sides r (r0 , 1).Fix r (0, 1), let = 1/r, {1, . . . , m}, define Sm\i = Sm \ {(xi , yi )}. Alsodefine Dm\i = DIS(VSF ,Sm\i B(f , r)) m\i = P(xi Dm\i |Sm\i ) = P (Dm\i Y).B(f , r)m 512, (12) clearly holds. Otherwise, suppose B(f , r)m > 512. xiDIS(VSF ,Sm\i ), must (xi , yi ) CSm .n(Sm )Xi=11DIS(VSF ,Sm\i ) (xi ).195fiWiener & El-YanivTherefore,P {n(Sm ) (1/16)B(f , r)m})(mXPDIS(VSF ,S) (xi ) (1/16)B(f , r)m1P=Pm\ii=1(mXDm\i (xi )1i=1(mX(1/16)B(f , r)mDIS(B(f ,r)) (xi )1i=1=P(XDIS(B(f ,r)) (xi )11+P1i=1XPDIS(B(f ,r)) (xi )i=1Xi=1(mXi=1(mX11)11Dm\i (xi )Dm\i (xi )X+PDIS(B(f ,r)) (xi )i=1X1B(f , r)m,DIS(B(f ,r)) (xi )161Dm\i (xi )X11i=1(1/16)B(f , r)m)7DIS(B(f ,r)) (xi ) < B(f , r)m81B(f , r)m,DIS(B(f ,r)) (xi )16DIS(B(f ,r)) (xi ))Xi=1)1)7DIS(B(f ,r)) (xi ) B(f , r)m8< (7/8)B(f , r)mi=1(1DIS(B(f ,r)) (xi )i=11Dm\i (xi ))(13/16)B(f , r)m.Since considering case B(f , r)m > 512, Chernoff bound implies!Xexp {B(f , r)m/128} < e4 .PDIS(B(f ,r)) (xi ) < (7/8)B(f , r)m1i=1Furthermore, Markovs inequality impliesPX1DIS(B(f ,r)) (xi )i=11Dm\i (xi )!(13/16)B(f , r)mmB(f , r) EhPi=11Dm\i (xi )(13/16)mB(f , r)Since xi values exchangeable,#"mfih hii XXXfiE E Dm\i (xi )fiSm\i =E m\i = m\m .EDm\i (xi ) =1i=11i=1i=1196.fiAgnostic Pointwise-Competitive Selective Classificationshown (Hanneke, 2012) leastm(1 r)m1 B(f , r).particular, B(f , r)m > 512, must r < 1/511 < 1/2, implies(1 r)1/r1 1/4,#"mXEDm\i (xi ) (1/4)mB(f , r).1i=1Altogether, establishedP (n(Sm ) (1/16)B(f , r)m) <mB(f , r) (1/4)mB(f , r)+ e4(13/16)mB(f , r)12=+ e4 < 19/20.131Thus, since n(Sm ) Bn m, 20probability least 19/20, mustB(f , r)1.Bn m,> (1/16)B(f , r)m (1/16)20rProof Lemma 9. Assuming Bn (m, ) = polylog(m) log 1 holds, existsconstant 1 (0, 1/20)Bn (m, ) non Bn (m, 1 ) = (polylog(m)).11Bn (m, 1 ), thus Bn m, 20= (polylog(m)). Therefore,increasing , Bn m, 2011max Bn m,= max polylog(m) = polylog,20r0m1/r0m1/r0using Lemma 16 have,1(r0 ) maxmax 16Bn m,, 51220m1/r011.= polylog528 + 16 max Bn m,20r0m1/r0ReferencesAmaldi, E., & Kann, V. (1995). complexity approximability finding maximumfeasible subsystems linear relations. Theoretical computer science, 147 (1), 181210.Bartlett, P. L., Jordan, M. I., & McAuliffe, J. D. (2006). Convexity, classification, riskbounds. Journal American Statistical Association, 101 (473), 138156.Bartlett, P., & Mendelson, S. (2006). Discussion 2004 IMS medallion lecture: Localrademacher complexities oracle inequalities risk minimization V. koltchinskii. Annals Statistics, 34, 26572663.197fiWiener & El-YanivBartlett, P., Mendelson, S., & Philips, P. (2004). Local complexities empirical riskminimization. COLT: Proceedings Workshop Computational LearningTheory, Morgan Kaufmann Publishers.Bartlett, P., & Wegkamp, M. (2008). Classification reject option using hinge loss.Journal Machine Learning Research, 9, 18231840.Ben-David, S., Eiron, N., & Long, P. (2003). difficulty approximately maximizingagreements. Journal Computer System Sciences, 66 (3), 496514.Beygelzimer, A., Dasgupta, S., & Langford, J. (2009). Importance weighted active learning.Proceedings 26th Annual International Conference Machine Learning, pp.4956. ACM.Beygelzimer, A., Hsu, D., Langford, J., & Zhang, T. (2010). Agnostic active learning withoutconstraints. Advances Neural Information Processing Systems 23.Beygelzimer, A., Dasgupta, S., & Langford, J. (2009). Importance weighted active learning.Proceedings 26th annual international conference machine learning, pp.4956. ACM.Beygelzimer, A., Hsu, D., Langford, J., & Zhang, T. (2010). Agnostic active learning withoutconstraints. arXiv preprint arXiv:1006.2588.Bounsiar, A., Grall, E., & Beauseroy, P. (2006). kernel based rejection method supervised classification. International Journal Computational Intelligence, 3, 312321.Bousquet, O., Boucheron, S., & Lugosi, G. (2004). Introduction statistical learningtheory. Advanced Lectures Machine Learning, Vol. 3176 Lecture NotesComputer Science, pp. 169207. Springer.Campi, M. (2010). Classification guaranteed probability error. Mach. Learn., 80 (1),6384.Cesa-Bianchi, N., & Gentile, C. (2008). Improved risk tail bounds on-line algorithms.Information Theory, IEEE Transactions on, 54 (1), 386390.Cesa-Bianchi, N., Gentile, C., & Orabona, F. (2009). Robust bounds classification viaselective sampling. Proceedings 26th Annual International ConferenceMachine Learning, pp. 121128. ACM.Chang, C., & Lin, C. (2011). LIBSVM: library support vector machines. ACMTransactions Intelligent Systems Technology, 2, 27:127:27. Software availablehttp://www.csie.ntu.edu.tw/ cjlin/libsvm.Chow, C. (1957). optimum character recognition system using decision function. IEEETrans. Computer, 6 (4), 247254.Chow, C. (1970). optimum recognition error reject trade-off. IEEE Trans.Information Theory, 16, 4136.Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization active learning.Machine Learning, 15 (2), 201221.Dasgupta, S., Hsu, D., & Monteleoni, C. (2007a). general agnostic active learning algorithm. NIPS.198fiAgnostic Pointwise-Competitive Selective ClassificationDasgupta, S., Monteleoni, C., & Hsu, D. J. (2007b). general agnostic active learningalgorithm. Advances neural information processing systems, pp. 353360.Dekel, O. (2008). online batch learning cutoff-averaging.. NIPS.Dekel, O., Gentile, C., & Sridharan, K. (2010). Robust selective sampling singlemultiple teachers.. COLT, pp. 346358.El-Yaniv, R., & Pidan, D. (2011). Selective prediction financial trends hiddenmarkov models. NIPS, pp. 855863.El-Yaniv, R., & Wiener, Y. (2010). foundations noise-free selective classification.Journal Machine Learning Research, 11, 16051641.El-Yaniv, R., & Wiener, Y. (2011). Agnostic selective classification. Neural InformationProcessing Systems (NIPS).El-Yaniv, R., & Wiener, Y. (2012). Active learning via perfect selective classification.Journal Machine Learning Research, 13, 255279.El-Yaniv, R., & Wiener, Y. (2015). version space compression set sizeapplications. Vovk, V., Papadopoulos, H., & Gammerman, A. (Eds.), MeasuresComplexity: Festschrift Alexey Chervonenkis. Springer, Berlin.Fine, S., Gilad-Bachrach, R., & Shamir, E. (2002). Query committee, linear separationrandom walks. Theoretical Computer Science, 284 (1), 2551.Freund, Y., Mansour, Y., & Schapire, R. (2004). Generalization bounds averaged classifiers. Annals Statistics, 32 (4), 16981722.Freund, Y., Seung, H., Shamir, E., & Tishby, N. (1997). Selective sampling using querycommittee algorithm. Machine Learning, 28, 133168.Friedman, E. (2009). Active learning smooth problems. Proceedings 22ndAnnual Conference Learning Theory.Fumera, G., & Roli, F. (2002). Support vector machines embedded reject option.Pattern Recognition Support Vector Machines: First International Workshop, pp.811919.Fumera, G., Roli, F., & Giacinto, G. (2001). Multiple reject thresholds improvingclassification reliability. Lecture Notes Computer Science, 1876.Gilad-Bachrach, R. (2007). PAC Beyond. Ph.D. thesis, Hebrew UniversityJerusalem.Gilad-Bachrach, R., Navot, A., & Tishby, N. (2005). Query committee made real.NIPS.Grandvalet, Y., Rakotomamonjy, A., Keshet, J., & Canu, S. (2008). Support vector machines reject option. NIPS, pp. 537544. MIT Press.Guyon, I., Weston, J., Barnhill, S., & Vapnik, V. (2002). Gene selection cancer classification using support vector machines.. Machine Learning, 389422.Hanneke, S. (2007a). bound label complexity agnostic active learning. ICML,pp. 353360.199fiWiener & El-YanivHanneke, S. (2007b). Teaching dimension complexity active learning. Proceedings 20th Annual Conference Learning Theory (COLT), Vol. 4539 LectureNotes Artificial Intelligence, pp. 6681.Hanneke, S. (2009). Theoretical Foundations Active Learning. Ph.D. thesis, CarnegieMellon University.Hanneke, S. (2013). statistical theory active learning. Unpublished.Hanneke, S. (2012). Activized learning: Transforming passive active improved labelcomplexity. Journal Machine Learning Research, 98888, 14691587.Hellman, M. (1970). nearest neighbor classification rule reject option. IEEETrans. Systems Sc. Cyb., 6, 179185.Herbei, R., & Wegkamp, M. (2006). Classification reject option. Canadian JournalStatistics, 34 (4), 709721.Kakade, S., & Tewari, A. (2009). generalization ability online strongly convexprogramming algorithms. Advances Neural Information Processing Systems(NIPS), pp. 801808.Koltchinskii, V. (2006). 2004 IMS medallion lecture: Local rademacher complexitiesoracle inequalities risk minimization. Annals Statistics, 34, 25932656.Landgrebe, T., Tax, D., Paclk, P., & Duin, R. (2006). interaction classificationreject performance distance-based reject-option classifiers. Pattern RecognitionLetters, 27 (8), 908917.Li, L., & Littman, M. L. (2010). Reducing reinforcement learning kwik online regression.Annals Mathematics Artificial Intelligence, 217237.Li, L., Littman, M., & Walsh, T. (2008). Knows knows: framework self-awarelearning. Proceedings 25th international conference Machine learning, pp.568575. ACM.Li, L. (2009). unifying framework computational reinforcement learning theory. Ph.D.thesis, Rutgers, State University New Jersey.Massart, P. (2000). applications concentration inequalities statistics. Annalesde la Faculte des Sciences de Toulouse, Vol. 9, pp. 245303. Universite Paul Sabatier.Mendelson, S. (2002). Improving sample complexity using global data. InformationTheory, IEEE Transactions on, 48 (7), 19771991.Mukherjee, S. (2003). Chapter 9. classifying microarray data using support vector machines.scientists University Pennsylvania School Medicine SchoolEngineering Applied Science. Kluwer Academic Publishers.Mukherjee, S., Tamayo, P., Slonim, D., Verri, A., Golub, T., Mesirov, J. P., & Poggio, T.(1998). Support vector machine classification microarray data. Tech. rep., AI Memo1677, Massachusetts Institute Technology.Orabona, F., & Cesa-Bianchi, N. (2011). Better algorithms selective sampling.Proceedings 28th International Conference Machine Learning (ICML-11),pp. 433440.200fiAgnostic Pointwise-Competitive Selective ClassificationPietraszek, T. (2005). Optimizing abstaining classifiers using ROC analysis. Proceedings Twenty-Second International Conference Machine Learning(ICML), pp.665672.Santos-Pereira, C., & Pires, A. (2005). optimal reject rules ROC curves. PatternRecognition Letters, 26 (7), 943952.Seung, H., Opper, M., & Sompolinsky, H. (1992). Query committee. ProceedingsFifth Annual Workshop Computational Learning theory (COLT), pp. 287294.Society, A. C. (2010). Cancer facts & figures 2010..Sousa, R., Mora, B., & Cardoso, J. (2009). ordinal data method classificationreject option. ICMLA, pp. 746750. IEEE Computer Society.Strehl, A. L., & Littman, M. L. (2007). Online linear regression applicationmodel-based reinforcement learning. Advances Neural Information ProcessingSystems, pp. 14171424.Tauman Kalai, A., Klivans, A., Mansour, Y., & Servedio, R. (2008). Agnostically learninghalfspaces. SIAM J. Comput., 37 (6), 17771805.Tortorella, F. (2001). optimal reject rule binary classifiers. Lecture Notes ComputerScience, 1876, 611620.Tsybakov, A. (2004). Optimal aggregation classifiers statistical learning. AnnalsMathematical Statistics, 32, 135166.Vovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic Learning Random World.Springer, New York.Wang, L. (2011). Smoothness, disagreement coefficient, label complexity agnosticactive learning. JMLR, 22692292.Wegkap, M. (2007). Lasso type classifiers reject option. Electronic JournalStatistics, 1, 155168.Wiener, Y. (2013). Theoretical Foundations Selective Prediction. Ph.D. thesis, TechnionIsrael Institute Technology.Wiener, Y., & El Yaniv, R. (2012). Pointwise tracking optimal regression function.Advances Neural Information Processing Systems 25, pp. 20512059.Wiener, Y., Hanneke, S., & El-Yaniv, R. (2014). compression technique analyzingdisagreement-based active learning. arXiv preprint arXiv:1404.1504.Zhang, T. (2005). Data dependent concentration bounds sequential prediction algorithms. Learning Theory, pp. 173187. Springer.201fiJournal Artificial Intelligence Research 52 (2015) 287-329Submitted 10/14; published 02/15Revision HistoryPaolo Liberatoreliberato@dis.uniroma1.itSapienza University Rome, DIAGVia Ariosto 25, 00185 Rome, ItalyAbstractarticle proposes solution problem obtaining plausibility information,necessary perform belief revision: given sequence revisions, togetherresults, derive possible initial order generated them; differentusual assumption starting all-equal initial order modifying sequence revisions. Four semantics iterated revision considered: natural, restrained,lexicographic reinforcement. each, necessary sufficient condition existence order generating given history revisions results proved. Complexityproved coNP complete cases one (reinforcement revision unbounded sequence length).1. IntroductionMany belief revision operators based sort plausibility order (Spohn, 1988;Boutilier, 1996; Nayak, 1994; Williams, 1994; Areces & Becher, 2001; Zhang, 2004; Benferhat, Kaci, Le Berre, & Williams, 2004; Hild & Spohn, 2008; Ferme & Hansson, 2011).Whenever revising done two different ways, result disjunctioneither (Alchourron & Makinson, 1982; Fagin, Ullman, & Vardi, 1983; Winslett,1988) plausible ones according order (Gardenfors, 1988; Katsuno& Mendelzon, 1991; Peppas, 2008; Nebel, 1992; Ferme & Hansson, 2011). Fewer disjunctsimply formulae; therefore, discriminating order, informativeresult. fine-grained order central usefulness revised knowledge base.Iterated revision provides way obtaining plausibility order. Even startingall-equal plausibility order (the least discriminating one), revision changesmaking possibilities plausible others (Spohn, 1988; Boutilier, 1996; Nayak,1994; Williams, 1994; Booth & Meyer, 2006; Jin & Thielscher, 2007). sequence revisionsproduces order that, depending revising formulae, less informative.cases, solution problem obtaining plausibility order: sequenceprevious revisions (Konieczny & Pino Perez, 2000; Baltag, Gierasimczuk, & Smets, 2011).However, even long history revisions may produce fine discrimination. limitcase, revising a, a, a, a, etc., final order discriminates modelsmodels a.way obtain initial plausibility order? One possibility deriveknowledge previous results (this also done merging, Liberatore,2014b, 2014a). words, previous revising formulae given, alsoresults produced. K0 initial knowledge base P1 first revisingformula, result another knowledge base K1 , revised P2 .article, Ki Pi assumed known certain point:c2015AI Access Foundation. rights reserved.fiLiberatorePPPP123nK0K1K2K3 . . . Kn1Knsequence consistent formulae [K0 , P1 , K1 , . . . , Pn , Kn ] called revision sequence. gives information initial plausibility order models, like following example shows.Example 1 Let [K0 , P1 , K1 ] revision sequence where:K0 =P1 =K1 = b cspecific K1 = b c possible result revising K0 =P1 = a. example b c also possible, b c actual revisionresult. means model {a, b, c} considered plausible {a, b, c},another model P1 . information useful subsequent revisions.revision sequence may seen form training: first n revisions manually performed human operators, following others done automatically using initial plausibility order obtained training. Technically, revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] containing revising formulae resultingknowledge bases Kn , one derives initial plausibility order, revisedP1 , . . . , Pn , Pn+1 , Pn+2 , . . . obtain Kn+1 , Kn+2 , . . . similar mechanism studiedNittka Booth (2008); comparison approach conclusions.Example 2 part research project, PhD student Polyxena tasked automating incorporation new data database, process far manually performeddatabase maintainers, specialized group people. soon realizes informationincorporated may disjunctive, revisions may done multiple ways. Introducing data form either b b database requireserasing either a, b both. Studying relevant literature topic, findschoices require plausibility information. Since previous revisions performeddatabase maintainers, possess information. therefore asksrank models, getting answers ranging mean? rank Brazilianredhead models top, Swiss models bad. spending half hour tryingexplain concepts plausibility, consistency, propositional models (anddiffer fashion models) ranking, gives up.leave room, one maintainers suggestslook logs, since files record everything happened database.does, indeed previous revisions stored logs: new informationdatabase maintainers incorporated it. problem shifts elicitingrank database maintainers, proved difficult, determiningprevious history revisions. finds sequence revisions resultsnatural-compatible (Definition 4) compatible revisions. means288fiRevision Historyinitial plausibility order calculated using Lemma 2, alsopeople performed previous revisions (unknowingly) adopted policy minimalplausibility change.Apart remarks operators, admittedly surreal comedic purposes,example shows not-so-uncommon scenario: process performed hand automated, eliciting information people performed far difficult.First, information may never expressed explicit form; second, mayhard formalize people lacking background formal logic.case belief revision, information needed perform subsequent revisionsinitial order models. However, eliciting order easy may seem,shown research similar concept preference (Sandholm & Conen, 2010; Carson &Louviere, 2011), mention experimental results cognitive psychology (Tversky& Kahneman, 1983): given background information, majorityparticipants test reckoned Linda bank teller active feminist movementlikely Linda bank teller, probability theory forbids blikely a.Furthermore, providing plausibility information additional work peoplemanually performed process far. Instead, Example 1 shows, informationmay derived previous history revisions.Another example data synchronization: SyncML protocol (OMA, 2002) allowssynchronizing data (phonebook, calendar notes, etc.) mobile phone computer, conflicts may arise; implementation may ask user do,take decision (like phone always wins) may however later manually reverseduser. Either way, result tells conflict solved. Again,revising formula resulting knowledge base given, usedderive information unknown plausibility order.Example 1 shows,knowledge revision sequence[K0 , P1 , K1 , P2 , K2 , . . . , Pn , Kn ] information initial plausibility modelsderived. information depends revision semantics, sequencesgenerated semantics.Example 3 Let [K0 , P1 , K1 , P2 , K2 , P3 , K3 ] revision sequence defined follows.K0 =P1 = bK1 = bP2 = cK2 = b cP3 =K3 =shown exists initial order models generates sequenceusing natural revision semantics. contrast, order generates restrainedlexicographic semantics.289fiLiberatoretechnical results provided article are: first, equivalent formulationsproblem establishing existence order generating revision sequence usingnatural, lexicographical, restrained reinforcement revision; second, initial orderbuilt one exists; third, complexity characterization.Since number models exponential number variables, existsorder models . . . quantification data structure mayexponential size. result, brute-force search takes double exponential time.equivalent formulations avoid high computational cost recasting problemterms polynomial-size data structures.problem establishing existence initial order generating given sequencecoNP-complete cases one (reinforcement revision unbounded sequencelength), therefore showing problem expressed validity QBF.proves problem recast form contain existentialquantification initial order.2. PreliminariesBelief revision considered article propositional formulae built finitealphabet variables. truth evaluation alphabet called model: functionvariables either true false. Following common terminology propositionallogic, model satisfies formula model formula, formulamodel. set models formula F denoted Mod(F ). QBFpropositional formula variables quantified, either universally (like a.ab), existentially (like ab.a b) (like ab.a b). variablesuniversally quantified formula QBF .revision sequence represents evolution beliefs time, includingrevising formulae results.Definition 1 revision sequence odd sequence consistent propositional formulae[K0 , P1 , K1 , . . . , Pn , Kn ] finite set variables.semantics belief revision considered article work orderingmodels, representing relative plausibility, modified new informationarrives. orderings defined follows.Definition 2 total preorder C partition models finite sequence classes[C(0), C(1), C(2), . . . , C(m)] C(0) 6= .Intuitively, partition represents way compare models: J compareclass, compares greater J class higherindex. use partitions instead usual notation J simplifies definitionsproofs. Since classes empty (except first), several partitions may representway comparing models. problem total preorders never checkedequality article.total preorder depicted shelf, Figure 1. bottom drawer C(0)contains plausible models. represent situations currently believed possible: C(0) = Mod(K0 ). Revising C P1 changes new preorder CP1 takes290fiRevision Historyaccount new information. class CP1 (0) contains models consideredplausible; therefore, CP1 (0) = Mod(K1 ).C(7)C(6)C(5)C(4)C(3)C(2)C(1)C(0)Figure 1: graphical representation total preorder Cpartition formalizes plausibility models: models C(i) plausiblemodels C(i + 1). lower class, plausible model; reason,total preorder often seen representing implausibility rather plausibility.inverse ordinal conditional function (Spohn, 1988): (I) = n C(n).study two semantics considered article involves prefixesmaxsets sequence. Sequences denoted using brackets [. . .]. Given sequenceformulae [P1 , . . . , Pn ], h-prefix sequence containing first h 1 formulaesequence. maxset sequence extends concept maximal consistent subsetssets sequences.maxset([P1 , . . . , Pn ]) = maxset(; [P1 , . . . , Pn ])maxset([Q1 , . . . , Qi , P1 ]; [P2 , . . . , Pn ])Q1 Qi P1 consistentmaxset([Q1 , . . . , Qi ]; [P1 , P2 , . . . , Pn ]) =maxset([Q1 , . . . , Qi , true], [P2 , . . . , Pn ])otherwisemaxset([Q1 , . . . , Qn ]; ) = [Q1 , . . . , Qn ]sequence formulae used context propositional formula expectedimplicitly represents conjunctions formulae. example, [b, c, c d] means(b c (c d)). According notation, Q1 Qi P1 inconsistent[Q1 , . . . , Qi ] |= P1 . result, [Q1 , . . . , Qi , true] replaced [Q1 , . . . , Qi , P1 ]definition maxset.definition prefixes maxsets, commute: h-prefix maxsetsequence maxset h-prefix sequence. result, Ph-th element sequence P maxset(S) P maxset(prefixh (S))consistent.maxset often written maxset(P1 , . . . , Pn ) shorthand maxset([P1 , . . . , Pn ]).number properties maxsets shown. proofs appendix.291fiLiberatoreLemma 1 F consistent F |= maxset(P1 , . . . , Pn ), maxset(P1 , . . . , Pn ){Pi | 1 n F |= Pi }.Lemma 2 F consistent F |= maxset(P1 , . . . , Pn ), every consistent subset{P1 , . . . , Pn } contains formulae entailed F equivalent maxset(P1 , . . . , Pn ).article, sequence formulae replaced true calledsubsequence. similar usual definition, difference formulaemaintain position sequence.Lemma 3 F consistent F 6|= maxset(P1 , . . . , Pn ) existssubsequence R [P1 , . . . , Pn ] that:1. R consistent;2. F |= Pi Pi R;3. i, Pi 6 R Pi prefixi (R) consistent.conditions lemma existential type: exists R, existsmodel R, either F 6|= Pi Pi R Pi prefixi (R) consistent. proveschecking F 6|= maxset(P1 , . . . , Pn ) expressed validity QBF,therefore NP.Corollary 1 F consistent, checking F 6|= maxset(P1 , . . . , Pn ) NP.lemma avoids constructing maxset one formula time replacing testsatisfiability Pi prefixi (maxset(P1 , . . . , Pn )) F |= Pi , Fconsistent entails maxset. way, sequence satisfiability checks requiredbuild maxset parallelized, is, turned number validity checksperformed parallel.order check F maxset(P1 , . . . , Pn ), one first checks whether F |=maxset(P1 , . . . , Pn ), maxset(P1 , . . . , Pn ) |= F . Assuming first conditiontrue, second shown coNP.Theorem 1 F consistent, checking F maxset(P1 , . . . , Pn ) coNP.article, revisions satisfying AGM postulate 4 considered: K PK P K P consistent. Also, formulae Pi Ki sequences assumedconsistent. checking sequence generated total preorder, Ki1 Piconsistent Ki Ki1 Pi . particular, sequence generatedtotal preorder Ki1 Pi consistent AGM postulate Ki1 Pi equivalentKi ; conversely, Ki Ki1 Pi consistent consistency Ki impliesKi1 Pi . property important allows replacing satisfiability testunsatisfiability test.292fiRevision HistoryPP-Figure 2: Natural revision3. Natural RevisionNatural revision (Boutilier, 1996) modifies total preorder plausibility models Clight new piece information P new total preorder CP close possibleoriginal one. new preorder P true plausible models,CP (0). minimal change C ensuring setting CP (0) minimal modelsP according C, leaving rest preorder unaltered.Definition 3 natural revision total preorder C formula P definedtotal preorder CP follows, minimal index C(i) Mod(P ) 6= :(CP (j) =C(i) Mod(P ) j = 0C(j 1)\CP (0) otherwiseexample, CP (0) = C(i) Mod(P ), CP (1) = C(1 1)\CP (0) = C(0)\CP (0).Graphically, change P produces preorder natural revision depictedcutting lowest models P placing others, shownFigure 2.CP1 ,...,Pi result revising C P1 , P2 , etc. using natural revision,Mod(Ki ) = CP1 ,...,Pi (0) revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ].Example 4 Let C = [C(0), C(1)] total preorder that:C(0) = Mod(a)C(1) = Mod(a)Revising P1 = b, P2 = c P3 = using natural revision generates revisionsequence Example 3. Indeed, revising C P1 = b makes minimal models P1form new class zero. Since C(0) Mod(P ) 6= , index definition naturalrevision zero. resulting preorder therefore:293fiLiberatoreCP1 (0) = C(0) Mod(P1 ) = Mod(a b)CP1 (1) = C(0)\CP1 (0) = Mod(a b)CP1 (2) = C(1)\CP1 (0) = Mod(a)Since Mod(K1 ) = CP1 (0), follows K1 b. similar change happensrevising P2 = c, since CP (0) Mod(P2 ) 6= , implies = 0.CP1 P2 (0) = Mod(a b c)CP1 P2 (1) = Mod(a b c)CP1 P2 (2) = Mod(a b)CP1 P2 (3) = Mod(a)Again, Mod(K2 ) = CP1 P2 (0), implies K2 b c. minimal modelsP3 = entire class CP1 P2 (3). Therefore, = 3 preorder becomes:CP1 P2 P3 (0) = Mod(a)CP1 P2 P3 (1) = Mod(a b c)CP1 P2 P3 (2) = Mod(a b c)CP1 P2 P3 (3) = Mod(a b)proves K3 a. revision sequence coincides Example 3.Looking example direction, shows revision sequence [a, b,b, c, b c, a, a] generated natural revision preorder. provedcase restrained lexicographic revisions.aim article establish whether sequence generated preorder,finding it. Unfortunately, direct search space total preorders unfeasible:number models exponential number variables, numbertotal preorders therefore double exponential. Fortunately, natural revisiondifficulty overcome thanks necessary sufficient condition sequencegenerated total preorder. number lemmas needed prove it. firstshows revising formula alter relative order modelsresulting knowledge base.Lemma 4 CP (0) Mod(F ) = CP compares models F C does, CPnatural revision total preorder C formula P .result iterated number revising formulae: resulting knowledgebases Ki inconsistent formula F , relative order models Fchanged. result final revision F therefore calculated originalordering, case. following lemma formulated fragment revisionsequence later applied.294fiRevision HistoryLemma 5 Let [Kj , Pj+1 , . . . , Pi , Ki ] revision sequence generated natural revisiontotal preorder C. Kj Pi consistent none Kj+1 Pi , . . . , Ki1 Pi is,CPj+1 ,...,Pi (0) = Mod(Kj Pi ).lemma similar result Boutilier (1996, Thm. 17), lifts assumptionconjunctions Kj Pj+1 , . . . , Ki2 Pi1 consistent. shows Piconsistent previous Kj , natural revision Pi produces resultdetermined Kj only, independent initial preorder. following lemma coverscase, Pi inconsistent previous Kj .Lemma 6 revision sequence [K0 , P1 , K1 , . . . , Pi , Ki ] generated natural revisiontotal preorder C Pi inconsistent K0 , . . . , Ki1 , modelsKi minimal models Pi according C.last two lemmas prove that, natural revision, Ki equivalent Kj Pimaximal j conjunction consistent one exists, otherwise determinedinitial preorder. first necessary condition existence totalpreorder generating sequence.Definition 4 revision sequence [K0 , P1 , . . . , Pn , Kn ] natural-compatible if, every{1, . . . , n}, holds:1. Ki |= Pi ;2. j maximal index j < Kj Pi consistent (if any),Ki Kj Pi .sequence natural-compatible generated natural revisioninitial total preorder.Theorem 2 [K0 , P1 , K1 , . . . , Pn , Kn ] natural-compatible generated naturalrevision initial preorder C = [C(0), . . . , C(n + 1)].Mod(Ki )n l < . Kl Pi |=otherwise, nC(i) =S{Mod(K ) | l < j . K P 6|= } = n + 1jjlNatural-compatibility sufficient condition sequence generatednatural revision initial preorder. following theorem proves alsonecessary. Therefore, characterizes exactly revision sequences natural revisiongenerates.Theorem 3 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated natural revisioninitial total preorder natural-compatible.following example illustrates natural compatibility application naturalrevision.295fiLiberatoreExample 5 revision sequence previous example natural-compatible. firstcondition natural compatibility satisfied: K1 = b implies P1 = b; K2 = b cimplies P2 = c; K3 = implies P3 = a.last preceding formula Kj consistent P1 = b K0 = a, indeed K1 =K0 P1 = b. last preceding formula Kj consistent P2 = c K1 = b,indeed K2 = K1 P2 = b c. Finally, P3 = consistent none K0 , K1 , K2 .Therefore, second condition natural compatibility places constraint it.Theorem 2 proves revision sequence generated natural revisionpreorder, also provides one: C(0) = Mod(a), C(1) = Mod(a).indeed preorder used previous example generate sequence.Natural compatibility rewritten number satisfiability unsatisfiabilitytests. particular, j maximal index property written as:Kj Pi consistent Kh Pi not, j < h < i. way, satisfiability checkdependent another, problem solved two parallel calls NP oracle,one positive one negative. assumption formulae revision sequencesconsistent allows rewriting first.Lemma 7 Checking existence total preorder C generating revision sequence[K0 , P1 , K1 , . . . , Pn , Kn ] using natural revision coNP.problem also hard coNP. Therefore, coNP complete.Theorem 4 problem establishing existence preorder generating revisionsequence using natural revision coNP complete.4. Restrained RevisionRestrained revision (Booth & Meyer, 2006) common natural revisionrevising total preorder C formula P , minimal models P becomes new classzero. addition, every class split two according satisfaction P : modelsP go lower class, others higher.Equivalently, every class refined (Papini, 2001) classes 2i 2i + 1,first class contains models class satisfying P second models classsatisfying P ; then, natural revision applied.Definition 5 restrained revision total preorder C formula P definedtotal preorder CP follows, minimal index C(i) Mod(P ) 6=/ denotes quotient (integer division, truncated):C(i) Mod(P )CP (j) =j = 0(C((j 1)/2)\CP (0)) Mod(P ) j > 0 odd(C((j 1)/2)\C (0))\Mod(P )otherwisePexample,CP (0) = C(i) Mod(P ),CP (1) = (C((1 1)/2)\CP (0))Mod(P ) = (C(0)\CP (0)) Mod(P ) CP (2) = (C((2 1)/2)\CP (0))\Mod(P ) =(C(0)\CP (0))\Mod(P ) since (2 1)/2 = 1/2 = 0 using integer division.296fiRevision History-P--Figure 3: Restrained revisiongraphical example application restrained revision total preorderFigure 3.revision sequence Example 3 generated preorder using restrainedrevision. proved using necessary sufficient condition existencepreorder generating sequence. now, illustrate restrained revision works,preorder shown natural revision used.Example 6 Let C following total preorder:C(0) = Mod(a)C(1) = Mod(a)Restrained revision P1 = b, P2 = c P3 = generates revision sequencedifferent Example 3. Since K0 = Mod(C(0)), follows K0 a. Revising CP1 = b makes minimal models P1 new class zero splits everyclass b/b. resulting total preorder removing empty classes therefore:CP1 (0) = Mod(a b)CP1 (1) = Mod(a b)CP1 (2) = Mod(a b)CP1 (3) = Mod(a b)Since Mod(K1 ) = CP1 (0), follows K1 b. similar change happensrevising P2 = c:297fiLiberatoreCP1 P2 (0) = Mod(a b c)CP1 P2 (1) = Mod(a b c)CP1 P2 (2) = Mod(a b c)CP1 P2 (3) = Mod(a b c)CP1 P2 (4) = Mod(a b c)CP1 P2 (5) = Mod(a b c)CP1 P2 (6) = Mod(a b c)CP1 P2 (7) = Mod(a b c)Again, Mod(K2 ) = CP1 P2 (0), implies K2 b c. minimal modelsP3 = whole class CP1 P2 (4). preorder therefore becomes:CP1 P2 P3 (0) = Mod(a b c)CP1 P2 P3 (1) = Mod(a b c)CP1 P2 P3 (2) = Mod(a b c)CP1 P2 P3 (3) = Mod(a b c)CP1 P2 P3 (4) = Mod(a b c)CP1 P2 P3 (5) = Mod(a b c)CP1 P2 P3 (6) = Mod(a b c)CP1 P2 P3 (7) = Mod(a b c)result, K3 b c. revision sequence coincides Example 3P3 K3 different, K3 = previous example. shownpreorder generates sequence using restrained revision.following property similar Lemma 5 natural revision, differencemaxset introduces account class split.Lemma 8 Let [Kj , Pj+1 , . . . , Pi , Ki ] revision sequence generated restrained revisioninitial total preorder C. Kj Pi consistent none Kj+1 Pi , . . . ,Ki1 Pi is, CPj+1 ,...,Pi (0) = Mod(maxset(Kj Pi , Pj+1 , . . . , Pi1 )).result first half necessary sufficient condition sequencegenerated restrained revision preorder, involves following definition.Definition 6 revision sequence [K0 , P1 , K1 . . . , Pn , Kn ] restrained-compatible if,every {1, . . . , n}, holds:1. Ki |= Pi ;2. Ki maxset(Kj Pi , Pj+1 , . . . , Pii ) j maximal index j <Kj Pi consistent, any;298fiRevision History3. either Ki |= Pl Ki |= Pl every l < j exists.side remark, condition either Ki |= Pl Ki |= Pl third pointdefinition refers indexes l < i, including ones Kl consistentprevious Kj .revision sequence restrained-compatible generated restrained revisioninitial preorder. following lemma specifies one is.Lemma 9 [K0 , P1 , K1 , . . . , Pn , Kn ] restrained-compatible generated restrained revision total preorder C = [C(0), . . . , C(n + 1)], where:Mod(Ki )C(i) =n l < . Pi Kl |=otherwise, nS{Mod(K ) | l < j P K 6|= } = n + 1jjlresults proved far collected equivalent formulation existencepreorder generating sequence.Theorem 5 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated restrained revisioninitial total preorder restrained-compatible.sequence Example 3 shown restrained-compatible. Therefore,generated restrained revision preorder worked naturalrevision, generated other.Example 7 revision sequence [K0 , P1 , K1 , P2 , K2 , P3 , K3 ] K0 = a, P1 = b, K1 =b, P2 = c, K2 = b c, P3 = a, K3 = restrained-compatible (thissequence Example 3). Indeed, P3 = inconsistent K0 , K1 K2 ,yet K3 = entails neither P1 = b P1 = b, thereby violating third conditionrestrained compatibility.definition restrained compatibility involves consistency entailment checks.following lemma rewrites form shown use inconsistencies.Lemma 10 revision sequence [K0 , P1 , K1 . . . , Pn , Kn ] restrained-compatibleif, every index i:1. Ki |= Pi ;2. every 0 j < i, either Kj Pi inconsistent Ki |= Kl Pi j < l <Ki maxset(Kj Pi , Pj+1 , . . . , Pi1 );3. every 0 j < i, Ki |= Pj , Ki |= Pj , Ki |= Kl Pi 0 l < i.Since checking equivalence consistent formula maxset problem coNPTheorem 1, expressed universally quantified formula. result,quantifiers conditions lemma universal, whole problem coNP.Hardness class easy prove.Theorem 6 Establishing existence total preorder generating restrained revisionsequence coNP-complete.299fiLiberatore5. Intermezzo: Multiple Preordersrevision sequence may generated one total preorder. Examplesexist even two variables only, sequence [K0 , P1 , K1 , P2 , K2 ] K0 = ab,P1 = b, K1 = b, P2 = b K2 = b.ababb b bAccording Lemma 9, sequence generated restrained revision initialpreorder C = [C(0), C(1), C(2), C(3)].C(0) = Mod(a b)C(1) = Mod(a b)C(2) = Mod(a b)C(3) = Mod(a b)However, preorder generating sequence. Classes C(1) C(2)swapped, still leading result.C(0) = Mod(a b)C(1) = Mod(a b)C(2) = Mod(a b)C(4) = Mod(a b)Intuitively, revising single-model formula always single possible outcome:formula itself. result, even Mod(K1 ) contained class greaterMod(K2 ), still minimal models P1 Mod(K1 ).way, expected: revision performed single possibleway, initial preorder irrelevant. intuition holds considered revisions,confirmed simplifying preorder: indeed, apart C(0), classesshuffled every possible way, even merged:C(0) = Mod(a b)C(1) = Mod(a b) Mod(a b) Mod(a b)kind preorder works whenever Pi single model, general.Even restricting Pi Pi Kj |= j < i, l < Pi Klguaranteed inconsistent, Pl Ki not. words, Mod(Ki ) cannot alwaysswapped merged Mod(Kl ).one preorder possible, sensible principle choose least discriminating one. preorder would compare different J strictly necessaryobtain revision sequence. words, carry plausibility informationfollow revision sequence. minimization similar spirit300fiRevision Historyway rational closure conditional logic rationally rooted consequencerelation (Lehmann & Magidor, 1992; Booth & Nittka, 2008). last preorder shownobeys principle, question whether least discriminating preorder existsrevision sequences considered revision semantics open problem.different possible solution proceed refutation:sequence[K0 , P1 , K2 , . . . , Pn , Kn ] generated revision preorder[K0 , P1 , K2 , . . . , Pn , Kn , Pn+1 , Kn+1 ] not, Kn+1 might considered truerevising Pn+1 .correctly pointed one reviewers, revision sequence may evengenerated different revisions different initial preorder, adding second dimensionproblem: preorder, also semantics revision chosen.sequence shown example one kind: generated four revisionsemantics considered article every initial preorder C C(0) = Mod(a b).6. Lexicographic Revisionseminal work iterated revision, Spohn (1988) defined tentative semantics basedprinciple newer formulae plausible older ones levelsplausibility: even unlikely models P preferred likelyP . spite apparent drawbacks pointed author, semanticslater recognized principled way perform iterated revision (Nayak, 1994; Darwiche &Pearl, 1997; Booth & Meyer, 2006; Jin & Thielscher, 2007; Konieczny & Pino Perez, 2000).Like revisions used article, lexicographic revision works totalpreorder plausibility models C. particular, revision P changes movingmodels P classes index lower others.Definition 7 lexicographic revision total preorder C formula P definedfollowing total preorder, j respectively indexes lowesthighest classes containing models P :(CP (k) =C(k + i) Mod(P )k jC(k j + 1)\Mod(P ) otherwisenew class zero CP (0) = C(0 + i) Mod(P ); expected, comprises minimalmodels P , since C(i) lowest class containing models P . class CP (j i) =C(j + i) Mod(P ) = C(j) Mod(P ) contains highest-class models P , sinceassumption C(j). models C(0) satisfy P , any, movedclass CP (j + 1) = C(j + 1 j + 1)\Mod(P ) = C(0)\Mod(P ). indexj + 1 lower classes CP (0), . . . , CP (j i) contain models P comingC(i), . . . , C(j).Figure 4 shows total preorder C changed formula P using lexicographicrevision.Graphically, lexicographic revision cuts models P classeswedges shelf. way, every model P belongs lower classmodels P . time, relative position two models P changed,holds every two models P .301fiLiberatoreP-PFigure 4: Lexicographic revisionExample 8 shown sequence Example 3 generated lexicographicrevision total preorder. Meanwhile, illustrate definition lexicographicrevision preorder used example natural revision revised P1 = b, P2 = cP3 = a.C(0) = Mod(a)C(1) = Mod(a)Revising C P1 = b using lexicographic revision removes models P1classes creates new classes bottom:CP1 (0) = Mod(a b)CP1 (1) = Mod(a b)CP1 (2) = Mod(a b)CP1 (3) = Mod(a b)result, K1 = b. Revising P2 = c similar effect:CP1 P2 (0) = Mod(a b c)CP1 P2 (1) = Mod(a b c)CP1 P2 (2) = Mod(a b c)CP1 P2 (3) = Mod(a b c)CP1 P2 (4) = Mod(a b c)302fiRevision HistoryCP1 P2 (5) = Mod(a b c)CP1 P2 (6) = Mod(a b c)CP1 P2 (7) = Mod(a b c)preorder produces K2 = b c. Finally, revising P3 = removingempty classes makes classes 1, 3, 5, 7 become new classes 0, 1, 2, 3.CP1 P2 P3 (0) = Mod(a b c)CP1 P2 P3 (1) = Mod(a b c)CP1 P2 P3 (2) = Mod(a b c)CP1 P2 P3 (3) = Mod(a b c)CP1 P2 P3 (4) = Mod(a b c)CP1 P2 P3 (5) = Mod(a b c)CP1 P2 P3 (6) = Mod(a b c)CP1 P2 P3 (7) = Mod(a b c)Since K3 = b c equivalent a, revision sequence Example 3generated lexicographic revision initial preorder C. shownpreorder generates sequence using lexicographic revision.every preorder C consistent formula P , revised preorder CP using lexicographic revision three properties:1. CP (0) set minimal models P preorder C;2. exists index hi=0,...,h CP (i)= Mod(P );3. two models satisfy P falsify it, CP compares C does.Lexicographic revision recast terms maxsets reversed sequence.Booth Nittka (2008) proved following property; precisely, provedimplies following property arbitrary preorder [C(0), . . . , C(n)] obtainedrevising ordering models class zero sequence formulaeC(n), . . . , C(0) sets models.Property 1 [K0 , P1 , . . . , Kn , Pn ] revision sequence generated lexicographic revision total preorder C, Mod(Ki ) set minimal modelsmaxset(Pi , . . . , P1 ) according C.property following consequences:1. models Ki class C;303fiLiberatore2. models maxset(Pi , . . . , P1 ) greater classes.Two properties follow. First, Ki Kj consistent models Mod(Ki )Mod(Kj ) class C. Indeed, since Ki Kj consistent, modelI; since models Ki class I, Kj , modelstwo formulae class. Second, Kj models commonKi maxset(Pi , . . . , P1 ), models class C, greatermodels Ki .allows shifting total preorder among models total preorder amongformulae Ki . Since preorder formulae Ki , resultsrevision process, called result preorder.Definition 8 result preorder revision sequence [K0 , P1 , . . . , Pn , Kn ] totalpreorder among formulae Ki that:1. Ki Kj consistent Ki Kj class;2. Ki Kj maxset(Pi , . . . , P1 ) consistent Ki lower class Kj .advantage result preorders built revision sequence,shown. Before, proved existence result preorderexistence initial total preorder models generating sequencelexicographic revision. Since two preorders involved (one among models, one amongformulae), distinction made preorder among models C preorder amongformulae R; result preorders second kind.Lemma 11 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated lexicographicrevision total preorder among models C result preorder R definedby:R(i) = {Ki | Mod(Ki ) C(i)}converse also holds: result preorder revision sequence one derivepreorder among models generates sequence using lexicographic revision.Lemma 12 R result preorder revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ]Ki |= maxset(Pi , . . . , P1 ) every i, lexicographic revision followingtotal preorder C among models generates revision sequence, z indexgreatest class R:(C(h) ={Mod(Ki ) | Ki R(h)} h z{I | 6 K1 Kn }h = z + 1two lemmas together imply following corollary.Corollary 2 revision sequence [K0 , P1 , . . . , Pn , Kn ] generated lexicographic revisioninitial preorder among models result preorder sequenceexists Ki |= maxset(Pi , . . . , P1 ) every i.304fiRevision Historycondition existence result preorder already simpler existenceinitial total preorder models, since number formulae Ki linear sizerevision sequence number models may exponential. existenceresult preorder recast terms condition revision sequence.Definition 9 revision sequence [K0 , P1 , . . . , Kn , Pn ] lexicographic compatible Ki |=maxset(Pi , . . . , P1 ) every relations ' < defined follows formcycles containing < links:Ki ' Kj Ki Kj consistent;Ki < Kj Ki Kj maxset(Pi , . . . , P1 ) consistent.rationale definition Ki ' Kj Ki Kjclass R, Ki < Kj Ki lower class. intuitionconfirmed following lemma.Lemma 13 revision sequence generated lexicographic revision total preorder lexicographic compatible.result applied running example.Example 9 revision sequence presented Example 3 lexicographic compatible.Indeed, maxset(P3 , P2 , P1 ) = maxset(a, c, b) = c b, entailed K3 =a. result, revision sequence generated lexicographic revisionpreorder.Given revision sequence, one determine consistency Ki Kj Ki Kjmaxset(Pi , . . . , P1 ) every pair formulae Ki Kj . problem non-existencepreorder generating sequence turned existence cycles,computationally easy (polynomial size revision sequence). hard partchecking consistency. Since problem polynomial NP oracle available (whichturns consistency checks constant-time operations), problem p2 . However,proved even computationally easier that.Lemma 14 revision sequence [K0 , P1 , K1 . . . , Pn , Kn ] lexicographic compatibleeither Ki 6|= maxset(Pi , . . . , P1 ) consistent sets R1 , . . . , Rn existthat:1. {Pj | 1 j Ki |= Pj } Ri every i;2. exists cycle Ki1 , . . . , Kim = Ki1 either Kij Kij+1 Kij Kij+1 Riconsistent ij {i1 , . . . , im1 }, second consistent least oneindex.advantage reformulation lexicographic incompatibility entailment tests contains reformulated terms consistency. means incompatibility NP. Therefore, compatibility coNP. also shown hardclass.Theorem 7 problem checking existence total preorder generating revisionsequence using lexicographic revision coNP-complete.305fiLiberatorePPFigure 5: Reinforcement revision7. Reinforcement RevisionReinforcement revision (Jin & Thielscher, 2007) takes input total preorderrevise C revising formula P , also parameter encodes degreebelief P (more precisely, degree disbelief P ). sake simplicity,restriction case = 1 analyzed.Definition 10 reinforcement revision total preorder C formula P parameter = 1 following total preorder CP , minimal indexC(i) Mod(P ) 6= .(CP (j) =C(i) Mod(P )j = 0C(j 1)\Mod(P ) C(j + i) Mod(P ) j > 0general definition j instead j 1; article, always 1. twocases merged single one CP (j) = C(j 1)\Mod(P ) C(j + i) Mod(P )j 0 assuming C(1) = . example, CP (0) = C(1)\Mod(P ) C(i)Mod(P ) = C(i) Mod(P ), CP (1) = C(0)\Mod(P ) C(i + 1) Mod(P ).graphical example revision Figure 5.behavior revision shown preorder formulae Example 3.Example 10 Revising preorder C = [C(0), C(1)] C(0) = Mod(a) C(1) =Mod(a) P1 = b using reinforcement revision effect increasing classevery model P1 one; models P1 decrease class sincealready class zero, means = 0 definition CP .CP1 (0) = Mod(a b)CP1 (1) = Mod((a b) (a b))CP1 (2) = Mod(a b)306fiRevision Historyhappens revising P2 = c: every class union original classconjoined c previous class conjoined c:CP1 P2 (0) = Mod(a b c)CP1 P2 (1) = Mod((a b c) (a b c) (a b c))CP1 P2 (2) = Mod((a b c) (a b c) (a b c))CP1 P2 (3) = Mod(a b c)minimal class containing models P3 = CP1 P2 (1). Therefore, modelsdecreased one class. Models P3 = increased one class, usual:CP1 P2 P3 (0) = Mod((a b c))CP1 P2 P3 (1) = Mod((a b c) (a b c) (a b c))CP1 P2 P3 (2) = Mod((a b c) (a b c) (a b c))CP1 P2 P3 (3) = Mod(a b c)result, K3 = b c.revision semantics, every Ki assumed consistent; case inconsistent Ki degenerated, preorder produces knowledge base.Given fixed revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated reinforcement revision initial total preorder C, lowest class index models Piordering CP1 ,...,Pi denoted DC (i). Formally:DC (i) = min{j | CP1 ,...,Pi1 (j) Mod(Pi ) 6= }Revising Pi shifts models Pi DC (i) classes raises modelsone class. result, DC (1), . . . , DC (n) tell model moved everystep, allowing determine model moved step step j.Definition 11 movement j according DC revision sequence[K0 , P1 , K1 , . . . , Pn , Kn ] MDC (I, i, j) where:MDC (I, i, i) = 0;MDC (I, i, + 1) = DC (i + 1) |= Pi+1 MDC (I, i, + 1) = 1 otherwise;j > MDC (I, i, j) =Pl=i,...,j1 MDC (I, l, l+ 1);j < MDC (I, i, j) = MDC (I, j, i).Since DC (i) minimal class models Pi step 1 according initial preorder, MDC (I, i, j) change classes j usingpreorder. initial preorder C affects definition MDC (I, i, j) indirectly, viasequence DC = [DC (1), . . . , DC (n)]. result, MV (I, i, j) definedarbitrary sequence n numbers V = [V (1), . . . , V (n)].307fiLiberatoreLemma 15 every sequence n numbers V = [V (1), . . . , V (n)], holds MV (I, i, j) =MV (I, i, h) + MV (I, h, j) every three indexes i, j h.lemma holds even h j.Since MDC (I, i, j) defined change class model stepstep j, particular case |= Ki class step j, sincestep zero.Lemma 16 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated reinforcement revision total preorder C DC = [DC (1), . . . , DC (n)] DC (i) =min{j | CP1 ,...,Pi1 (j) Mod(Pi ) 6= } |= Ki then, every j:MDC (I, i, j) = 0 |= Kj ;MDC (I, i, j) > 0 otherwise.lemma reversed, sense sequence values propertyallows determine preorder generating sequence.Definition 12 sequence nonnegative integer values V = [V (1), . . . , V (n)] reinforcement mover revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] if, every j,|= Ki then:MV (I, i, j) = 0 |= Kj ;MV (I, i, j) > 0 otherwise.previous lemma therefore recast as: sequence generated reinforcement revision total preorder C, reinforcement mover: DC .converse also holds: reinforcement mover one determine total preorder generating revision sequence.Lemma 17 V = [V (1), . . . , V (n)] reinforcement mover revision sequence[K0 , P1 , K1 , . . . , Pn , Kn ], following initial preorder C = [C(0), . . . , C(V (1) + + V (n +1))] generates revision sequence reinforcement revision DC = V .(C(j) ={I | |= Ki MV (I, i, 0) = j} j < V (1) + . . . + V (n) + 1{I | . 6|= Ki }j = V (1) + . . . + V (n) + 1contrast condition compatibility revision semantics, oneexplicitly require Ki |= Pi . however implied: 6|= Pi MV (I, 1, i) = 1definition movement (Definition 11) |= Ki MV (I, i, j) 0 every jdefinition reinforcement mover (Definition 12). particular case j = 1MV (I, i, j) = MV (I, i, 1) = MV (I, 1, i) = 1, greaterequal zero. Therefore, reinforcement mover exists Ki 6|= Pi .lemma allows checking existence preorder generating sequenceguessing V (1), . . . , V (n) checking class every model satisfies leastKi . However, membership polynomial hierarchy follows values V (i)representable polynomial space, possible values boundedexponential size sequence.308fiRevision HistoryLemma 18 reinforcement revision generates revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ]total preorder, also generates sequence preorderminimal initial class models P1 0 1.lemma proves sequence generated preorder also generatedpreorder C DC (1) either 0 1. particular, K0 P1 consistentDC (1) = 0, otherwise DC (1) = 1. base case recursive proof giving boundsize DC (i). Intuitively, done lowering models Pinumber classes initial preorder; revising Pi moved togetherminimal ones class zero, resulting ordering obtainedoriginal one. lowering cannot however large models enterclass zero previous step j satisfy Kj .Lemma 19 reinforcement revision generates sequence [K0 , P1 , K1 , . . . , Pn , Kn ]total preorder, also generates sequence total preorder CDC (i + 1) DC (1) + . . . + DC (i) + + 1.lemma seen inductive part proof, previous onebase case. lead following conclusion.Lemma 20 Reinforcement revision generates revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ]total preorder generates sequence total preorderDC (i) bounded 2i 1.values DC (1), . . . , DC (n) bounded exponential value n,also lower bound size sequence [K0 , P1 , . . . , Pn , Kn ]. Therefore, DC (i)represented space polynomial size sequence.Theorem 8 Establishing existence total preorder generating revision sequence[K0 , P1 , . . . , Pn , Kn ] reinforcement revision p2 , coNP n constant.case constant-length sequences, hardness easy prove.Theorem 9 Checking existence preorder generating reinforcement revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] coNP-complete, n constant.8. ConclusionsBelief revision employs plausibility orders revise knowledge base, obtainorders largely neglected. solution proposed studied articleassume knowledge previous revisions, reversing obtain initial order,used revisions. method similar deriving orderhypothetical revisions like K P |= Q, K R |= , etc., assumes knowledgewould happen revising formula P , R, etc. approach consideredarticle results iteratively incorporating series new formulae given, likenested counterfactuals (Eiter & Gottlob, 1996). Booth Nittka (2008) consideredproblem deriving facts holding time points partial information expressed309fiLiberatoreterms positive negative conditions (i.e., formulae hold othershold certain time points), done constructing initial ordering.study focuses lexicographic revision only, allows partial knowledgerevisions results, includes choice initial ordering among possible ones.Rather entailment information like K P R |= Q, article history revisionsresulting knowledge bases assumed fully known. could actual resultsmanually performed changes, may already available.analysis shown simple equivalent conditions existence ordering generating given series revisions results natural (Boutilier, 1996), restrained (Booth& Meyer, 2006), lexicographic (Spohn, 1988; Nayak, 1994) reinforcement revisions (Jin& Thielscher, 2007). conditions allow construct initial ordering one exists.Using equivalent conditions, complexity establishing existence orderings generating sequence established considered semantics. Surprisingly,turned relatively simple: coNP complete cases one (reinforcementrevision unbounded sequence length). checking propositional entailment, means checking generability increase complexitybase language propositional logic.shown Section 5, revision sequence may generated oneordering. leads question whether one may considerednatural sequence. example less J ordering another,second may seen cautious, end rational: sequencegenerated without assuming represents plausible world J,reason draw conclusion. question whether single least informativeordering exists every sequence, considered revision semantics, openproblem.Still open comparison generable sequences various revisions: shownrunning example, sequence generated natural revision generatedordering restrained lexicographic revision. sequenceopposite property? not, natural revision may seen suited explainingrevision sequence generated. hand, may also give less informationinitial ordering used generate them; case orderings generatingsequence numerous semantics.four semantics iterated belief revisions ones defined literature (Williams, 1994; Darwiche & Pearl, 1997; Areces & Becher, 2001; Benferhat et al.,2004; Konieczny & Pino Perez, 2000; Zhang, 2004); recent survey counted least twentyseven revision-related operators (Rott, 2009). Natural lexicographic semantics regarded extreme forms revision satisfying Darwiche-Pearl postulates (Darwiche &Pearl, 1997), minimal maximal hearing respectively given new information. Restrained reinforcement revision considered middle,also obey conditions (Booth & Meyer, 2006; Jin & Thielscher, 2007). fourconsidered semantics therefore constitute reasonable spectrum possibilities, othersexist.require additional information (like strength every revision, Spohn, 1988;Williams, 1994; Benferhat et al., 2004), others families revisions rather singleones (Darwiche & Pearl, 1997; Zhang, 2004). These, particular, open interesting310fiRevision Historyline research: whether revision sequence generated orderingrevision semantics satisfying given set conditions, like Darwiche Pearl (1997)postulates. words, ordering semantics solutionproblem, given revision sequence.assumption reliability strictly increasing time also gives directionsstudy. Indeed, principle realized true general (Peppas, 2008)iterated revision recognized form prioritized merging (Delgrande, Dubois, & Lang,2006). perspective, giving preference last formula particular case.case interest, yet searching initial plausibility order also possiblegeneral case.Yet another open problem combine approach article resultspeople actually perform revision. Indeed, experimentally provedhuman revision suffers number biases (Tversky & Kahneman, 1983; See,Morrison, Rothman, & Soll, 2011; Wang, Zhang, & Johnson, 2000), anchoringorder effect, excessive preference knowledge acquired early. studies showrevision performed people fully rational, contrary belief revisionformal semantics attempt be. psychological, extra-logical biases keptaccount working sequences manually-performed revisions.8.1 Acknowledgementsauthor thanks anonymous referees useful suggestions previousversions article.Appendix A. Proofsfollowing sections contain proofs lemmas theorems article.A.1 Preliminaries: ProofsLemma 1 F consistent F |= maxset(P1 , . . . , Pn ), maxset(P1 , . . . , Pn ){Pi | 1 n F |= Pi }.Proof. Since F |= maxset(P1 , . . . , Pn ) F implies every Pi maxset.possibility claim hold F also implies Pimaxset. Since every element maxset(P1 , . . . , Pn ) implied F , also every elementprefixi (maxset(P1 , . . . , Pn )) is. Since F also entails Pi , every model F satisfiesprefixi (maxset(P1 , . . . , Pn )) Pi . consistency Pi prefixi (maxset(P1 , . . . , Pn ))contradicts assumption Pi maxset.Lemma 2 F consistent F |= maxset(P1 , . . . , Pn ), every consistent subset{P1 , . . . , Pn } contains formulae entailed F equivalent maxset(P1 , . . . , Pn ).Proof. Lemma 1, since F consistent F |= maxset(P1 , . . . , Pn )maxset(P1 , . . . , Pn ) {Pi | 1 n F |= Pi }. proves maxset(P1 , . . . , Pn )contains formulae entailed F .Let R consistent proper superset311fiLiberatoremaxset(P1 , . . . , Pn ). Let Pi lowest-index formula Rmaxset(P1 , . . . , Pn ). Since lowest index, R maxset(P1 , . . . , Pn )formulae among {P1 , . . . , Pi1 }. Since R consistent, intersection setconsistent well. Since R also contains Pi , follows prefixi (P1 , . . . , Pn ) {Pi }consistent, contradicting assumption Pi maxset(P1 , . . . , Pn ).Lemma 3 F consistent F 6|= maxset(P1 , . . . , Pn ) existssubsequence R [P1 , . . . , Pn ] that:1. R consistent;2. F |= Pi Pi R;3. i, Pi 6 R Pi prefixi (R) consistent.Proof. Two cases considered: first, F entails maxset; second, F not.first, R proved exists; second, one R shown.F |= maxset(P1 , . . . , Pn ), F implies elements maxset. result,second condition true R contains formulae maxset. R alsocontains formulae maxset, since formula maxset inconsistentmaxset, R inconsistent. result, R consistent coincidesmaxset. contradicts third point, showing every R, firstsecond conditions true third false.F 6|= maxset(P1 , . . . , Pn ), three conditions satisfied R containing preciselyformulae Pi entailed F . choice meets first condition F consistentsecond construction. third condition proved hold well.Since F entail maxset F entail formulae maxset.Let least index formula maxset(P1 , . . . , Pn ) entailed F .construction, every formula Pj maxset R j < i. shownconverse also holds. contrary, let j < lowest index formula Rmaxset.assumptions j least indexes R maxset differ (inway other) imply prefixj (R) = prefixj (maxset(P1 , . . . , Pn )). Since assumptionPj maxset, inconsistent prefixj (maxset(P1 , . . . , Pn )). result,also inconsistent prefixj (R). Since Pj R, implies inconsistency R,proved consistent.contradiction proves R maxset equal indexj < i, i. words, prefixi (R) = prefixi (maxset(P1 , . . . , Pn )).assumption Pi formula maxset R. maxsetmeans Pi prefixi (maxset(P1 , . . . , Pn )) consistent. proved,latter equivalent Pi prefixi (R). concludes proof third condition.Theorem 1 F consistent, checking F maxset(P1 , . . . , Pn ) coNP.Proof. F maxset(P1 , . . . , Pn ) holds F |= maxset(P1 , . . . , Pn ) maxset(P1 , . . . , Pn ) |=F . first condition true, Lemma 1 maxset comprises exactly formulae312fiRevision Historyentailed F . Therefore, converse maxset(P1 , . . . , Pn ) 6|= F happenexists model satisfying formulae Pi entailed F F itself. words:F maxset(P1 , . . . , Pn )iff F |= maxset(P1 , . . . , Pn ) maxset(P1 , . . . , Pn ) |= Fiff F |= maxset(P1 , . . . , Pn ) (I . 6|= F i(F |= Pi |= Pi ))iff F |= maxset(P1 , . . . , Pn ) (I . 6|= F i(F 6|= Pi |= Pi ))iff F |= maxset(P1 , . . . , Pn ) . |= F i(F |= Pi 6|= Pi ))Lemma 3 reformulates converse first condition F |= maxset(P1 , . . . , Pn ) usingexistential quantifiers (the second point equivalent either F 6|= Pi Pi R).result, first condition expressed using universal quantifiers.existential quantifier second condition i, replaceddisjunction. Since quantifiers universal, problem coNP.A.2 Natural Revision: ProofsLemma 4 CP (0) Mod(F ) = CP compares models F C does, CPnatural revision total preorder C formula P .Proof. Let J two models F , l classes. Since models FCP (0) Mod(F ) = , belong CP (0). result, CP (m + 1) = C(m)\CP (0)contains CP (l + 1) = C(l)\CP (0) contains J. proves revising C Fincreases classes J one each. Therefore, greater equal Jaccording CP according C.Lemma 5 Let [Kj , Pj+1 , . . . , Pi , Ki ] revision sequence generated natural revisiontotal preorder C. Kj Pi consistent none Kj+1 Pi , . . . , Ki1 Pi is,CPj+1 ,...,Pi (0) = Mod(Kj Pi ).Proof. Lemma 4, CPj+1 ,...,Pi1 compares models Pi way C does.result, Mod(Ki ) minimal models Pi C. Since C(0) = Mod(Kj ) Kj Piconsistent, minimal models C(0) Mod(Pi ) = Mod(Kj Pi ).Lemma 6 revision sequence [K0 , P1 , K1 , . . . , Pi , Ki ] generated natural revisiontotal preorder C Pi inconsistent K0 , . . . , Ki1 , modelsKi minimal models Pi according C.Proof. Lemma 4, revising C P1 , . . . , Pi1 affect ordermodels Pi . result, minimal models Pi according CP1 ,...,Pi1 minimalmodels Pi according C.313fiLiberatoreTheorem 2 [K0 , P1 , K1 , . . . , Pn , Kn ] natural-compatible generated naturalrevision initial preorder C = [C(0), . . . , C(n + 1)].Mod(Ki )C(i) =n l < . Kl Pi |=otherwise, nS{Mod(K ) | l < j . K P 6|= } = n + 1jjlProof. Since formula index lower zero, C(0) equal Mod(K0 ) thereforeempty. classes C contain models C(n + 1) comprises every modelC(0), . . . , C(n). prove C total preorder, proved setsC(i) disjoint. model C(i) n Mod(Ki ). Therefore,also model Pi . Since Pi inconsistent K0 , . . . , Ki1 , modelC(0), . . . , C(i 1). class C(n + 1) contains exactly modelsC(0) C(n). proves C total preorder.formulae Ki generated different ways depending whether Kj Pi consistent j < i:index j exists, assumption natural compatibility Ki Kj Pimaximal j; Lemma 5, exactly result revising CP1 , . . . , Pi .otherwise, Pi inconsistent K0 , . . . , Ki1 ; Lemma 6, Mod(Ki )minimal models Pi initial preorder C. Since Pi inconsistent K0 , . . . , Ki1 , then: first, since C(0), . . . , C(i 1) subsetsMod(K0 ), . . . , Mod(Ki1 ), classes contain models Pi ; second, C(i)empty equal Mod(Ki ). result, minimal models Pi exactlyMod(Ki ).Theorem 3 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated natural revisioninitial total preorder natural-compatible.Proof. previous theorem shows every natural-compatible sequence generatednatural revision total preorder. converse proved: sequencenatural-compatible generated natural revision preorder.Since Mod(Ki ) = CP1 ,...,Pi1 ,Pi (0) = CP1 ,...,Pi1 (l) Mod(Pi ) l, holdsMod(Ki ) Mod(Pi ), Ki |= Pi . Therefore, Ki 6|= Pi preordergenerate revision sequence. Otherwise, natural-compatibility violated if, ji:1. Kj Pi consistent;2. formulae Kj+1 Pi , . . . , Ki1 Pi inconsistent;3. Kj Pi equivalent Ki .314fiRevision HistoryLemma 5, first two points imply revising Cj Pj+1 , . . . , Pi generatesformula equivalent Kj Pi , contradicting third point. proves revisionsequence natural-compatible generated natural revision preorder.Lemma 7 Checking existence total preorder C generating revision sequence[K0 , P1 , K1 , . . . , Pn , Kn ] using natural revision coNP.Proof. According Theorem 3, preorder C exists if, every{1, . . . , n}, holds Ki |= Pi and, j maximal index j < Kj Piconsistent (if any), Ki Kj Pi . first part, Ki |= Pi i, verifiedlinear number independent unsatisfiability tests.second part rewritten as: Kj Pi consistent Kh Pi inconsistentevery h j i, Ki Kj Pi . Rewriting implication disjunction,every j = 0, . . . , 1:1. either K0 Pi inconsistent Kh Pi consistent 0 < h < Ki K0 Pi ;2. either K1 Pi inconsistent Kh Pi consistent 1 < h < i, Ki K1 Pi ;3. . . . ;4. either Ki1 Pi inconsistent Ki K0 Pi .conditions index i. hold every {1, . . . , n}. Sinceformulae Ki consistent, conditions simplified: Kj Pi consistent,either Kh Pi consistent j < h < Ki Kj Pi ; latter case, Ki Kh Pione index h (the last). Therefore, condition recast as:Kj Pi |= Ki Kj+1 Pi . . . Ki Ki1 Pi Ki Kj Picondition checked number independent unsatisfiability tests,therefore coNP.Theorem 4 problem establishing existence preorder generating revisionsequence using natural revision coNP complete.Proof. Membership proved previous lemma. Hardness proved reductionproblem checking whether formula G unsatisfiable. instance [K0 , P1 , K2 ]K0 = a, K1 = P1 = G, new variable, G. G satisfiable,K0 P1 consistent; therefore, equivalent K1 . Instead, G.G unsatisfiable, K1 = P1 = inconsistent K0 . Theorem 3, preordergenerating sequence using natural revision exists.315fiLiberatoreA.3 Restrained Revision: ProofsLemma 8 Let [Kj , Pj+1 , . . . , Pi , Ki ] revision sequence generated restrained revisioninitial total preorder C. Kj Pi consistent none Kj+1 Pi , . . . ,Ki1 Pi is, CPj+1 ,...,Pi (0) = Mod(maxset(Kj Pi , Pj+1 , . . . , Pi1 )).Proof. Proof induction j. j = + 1 claim CPi (0) = Mod(maxset(KjPi )) holds Kj Pi consistent assumption.inductive claim CPj+1 ,...,Pi2 ,Pi1 ,Pi (0)=Mod(maxset(KjPi , Pj+1 , . . . , Pi2 , Pi1 )), inductive assumption without Pi1 :CPj+1 ,...,Pi2 ,Pi (0) = Mod(maxset(Kj Pi , Pj+1 , . . . , Pi2 ))definition, CPj+1 ,...,Pi2 ,Pi (0) CPj+1 ,...,Pi2 (k) Mod(Pi ) k minimalinteger making intersection non-empty. result, CPj+1 ,...,Pi2 (l) Mod(Pi ) =indexes l 0 l < k. Revising preorder Pi1 changes classes of:models Ki1 , become class zero;models class, split according whether satisfy Pi1 .Since model Ki1 satisfies Pi assumption, CPj+1 ,...,Pi2 ,Pi1 (0) Mod(Pi ) =holds. None classes CPj+1 ,...,Pi2 index 0 l < k intersect Mod(Pi ); therefore,neither ones resulting splitting them. result, minimal-index classintersecting Mod(Pi ) one two resulting splitting CPj+1 ,...,Pi2 (k), are:CPj+1 ,...,Pi2 (k) Mod(Pi1 )CPj+1 ,...,Pi2 (k)\Mod(Pi1 )first intersects Mod(Pi ), Mod(Ki ); otherwise, Mod(Ki ) second.formulae:(Mod(Ki ) =CPj+1 ,...,Pi2 (k) Mod(Pi1 ) Mod(Pi ) empty(CPj+1 ,...,Pi2 (k)\Mod(Pi1 )) Mod(Pi ) otherwiseproperties set operators, equation rewritten as:(Mod(Ki ) =CPj+1 ,...,Pi2 (k) Mod(Pi ) Mod(Pi1 ) empty(CPj+1 ,...,Pi2 (k) Mod(Pi ))\Mod(Pi1 ) otherwiseway k defined, CPj+1 ,...,Pi2 (k) Mod(Pi ) equal CPj+1 ,...,Pi2 ,Pi (0).latter inductive assumption Mod(maxset(Kj Pi , Pj+1 , . . . , Pi2 )). Intersectingset Mod(Pi1 ) result empty, subtracting Mod(Pi1 ) otherwiseadding Pi1 end sequence, definition maxsetsequence. Since Mod(Ki ) = CPj+1 ,...,Pi2 ,Pi1 ,Pi (0), proves inductive claimset equal Mod(maxset(Kj Pi , Pj+1 , . . . , Pi2 , Pi1 )).316fiRevision HistoryLemma 9 [K0 , P1 , K1 , . . . , Pn , Kn ] restrained-compatible generated restrained revision total preorder C = [C(0), . . . , C(n + 1)], where:Mod(Ki )C(i) =n l < . Pi Kl |=otherwise, nS{Mod(K ) | l < j P K 6|= } = n + 1jjlProof. formula index less zero; therefore, C(0) = Mod(K0 ), emptyformulae sequences consistent assumption. Since C(n + 1) containsmodels C(0) C(n), union classes include models, modelC(n+1) also another classes. order prove C partition, shownmodel C(i) also C(l) l < n. C(i) n Mod(Ki )Pi Kl inconsistent l < i. Since Ki |= Pi , also Ki Kl inconsistent.proves model cannot C(i) also C(l) l < i.previous lemma shows that, regardless initial total preorder, revisionsequence generated restrained revision Ki maxset(Kj Pi , Pj+1 , . . . , Pii )j maximal index j < Kj Pi consistent, any. Since restrainedcompatibility ensures condition holds, Ki j exists obtainedrevision regardless C.Remains show C generates Ki even Pi Kl inconsistent l < i.models Pi C(i) = Mod(Ki ) classes greater index. Indeed, modelPi C(l) l < k Pi Kl would consistent. Revising C Pl ,l = 1, . . . , 1, changes preorder two ways:1. models Kl moved class zero;2. classes split according satisfaction Pl .Since Pi Kl inconsistent, model Pi moved class zero. result,relative position models Pi modified second change, splittingclasses. could break class Mod(Ki ) two, case. Indeed, sinceeither Ki |= Pl Ki |= Pl , either models Ki satisfy Pl falsify Pl .words, change may alter comparison two models Pi ,Mod(Ki ).Two claims therefore proved: models Pi C(i) = Mod(Ki )greater classes; first 1 revisions minimal models Pi stillMod(Ki ). proves result i-th revision Ki .Theorem 5 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated restrained revisioninitial total preorder restrained-compatible.Proof. previous lemma shows every restrained-compatible revision sequencegenerated certain initial preorder. Remains therefore prove converse:revision sequence restrained-compatible generated initial preorder.Lemma 8 proves every sequence generated restrained revision satisfies condition Ki maxset(Pi Kj , Pj+1 , . . . , Pi1 ) j maximal j <317fiLiberatorePi Kj consistent. Ki |= Pi holds consequence Mod(Ki ) set minimal models satisfying Pi . Remains therefore prove necessity third conditionrestrained compatibility: Pi consistent Kj j < k either Ki |= PlKi |= Pl every l < i.contrary, let l < Ki 6|= Pl Ki 6|= Pl . two conditionsimply Mod(Ki ) contains models Pl models Pl . Evenmodels Ki class l 1 revisions, l-th one separates onessatisfying Pl ones satisfying Pl . may end two consecutive classes,new class zero another class, either way models Ki placed twoseparate classes. Since constrained revision never merges classes, models stillseparate classes 1-th revision. result, revising Pi select partMod(Ki ) it.Lemma 10 revision sequence [K0 , P1 , K1 . . . , Pn , Kn ] restrained-compatibleif, every index i:1. Ki |= Pi ;2. every 0 j < i, either Kj Pi inconsistent Ki |= Kl Pi j < l <Ki maxset(Kj Pi , Pj+1 , . . . , Pi1 );3. every 0 j < i, Ki |= Pj , Ki |= Pj , Ki |= Kl Pi 0 l < i.Proof. three conditions restrained compatibility rewritten follows,every index i.1. Ki |= Pi ;2. every j 0 j < i, either Kj Pi inconsistent Kl Pi consistentj < l < Ki maxset(Kj Pi , Pj+1 , . . . , Pi1 );3. every j 0 j < i, either Ki |= Pj Ki |= Pj Kl Pi consistent0 l < i.second third point include Kl Pi consistent h l < i:first, h = j + 1; second, h = 0. particular conditions lemma,two conditions shown equivalent:1. Kl Pi satisfiable l h l < i;2. Ki |= Kl Pi l h l < i.proved that: first, sequence restrained-compatible Condition 1 impliesCondition 2; second, three conditions lemma true Condition 2 impliesCondition 1.Condition 1 Kl Pi consistent h l < i. Either l maximalindex property index h is. Let g maximalindex. restrained compatibility, Ki maxset(Kg Pi , Pi1 , . . . , Pg+1 ). maxset318fiRevision Historysequence implies first element, consistent. Since case, Ki |= Kg Pi .means Condition 2 holds index g.Condition 2 Ki |= Kh Pi h l < i. Since Ki consistent, Kh Piconsistent well, proving Condition 1 index l.Theorem 6 Establishing existence total preorder generating restrained revisionsequence coNP-complete.Proof. Membership follows previous lemma. conditions reformulatedentailments (like Ki |= Pi ), inconsistencies (like Kj Pi ) equivalencesmaxsets (Ki maxset(Kj Pi , Pj+1 , . . . , Pi1 )). problems coNP,therefore rewritten universal quantified formulae. whole problem combinationthese; renaming variables taking quantifiers, single universallyquantified formula results. Since QBF coNP, problem coNP.Hardness proved reduction problem propositional unsatisfiability. Givenformula F , associated revision sequence [a, b F, b], b freshvariables, occurring F . F unsatisfiable b F equivalent b,sequence [a, b, b] generated preorder C = [C(0), C(1)] C(0) = Mod(a)C(1) = Mod(a). F satisfiable (b F ) satisfiable equivalentb, makes sequence generated restrained revision preorder.sequence therefore generated restrained revision preorderF unsatisfiable.A.4 Lexicographic Revision: ProofsLemma 11 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated lexicographicrevision total preorder among models C result preorder R definedby:R(i) = {Ki | Mod(Ki ) C(i)}Proof. R total preorder because: first, R(0) empty C(0) = Mod(K0 ),implies Mod(K0 ) C(0) K0 R(0); second, formula two classesR C partition. Third, every Ki class R Property 1:since Mod(Ki ) set minimal models C, models class C(j);therefore, Ki R(j). Remains prove preorder satisfies two conditionsresult preorder sequence.Ki Kj consistent other, common model I. Let hclass. construction, since |= Ki Ki R(h). holds Kj , since|= Kj . result, Kj R(h).Let Ki Kj Ki Kj maxset(Pi , . . . , P1 ) consistent. assumption,sequence generated lexicographic revision C. result, models Kiexactly minimal models maxset(Pi , . . . , P1 ) according C. Let h classmodels. construction R, holds Ki R(h). models Ki maxset(Pi , . . . , P1 )belong greater class l > h. Since Ki Kj maxset(Pi , . . . , P1 ) consistent, Kj319fiLiberatoremodels C(l). Since models Kj class, follows Mod(Kj ) C(l),implies Kj R(l). Since l > h, claim proved.Lemma 12 R result preorder revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ]Ki |= maxset(Pi , . . . , P1 ) every i, lexicographic revision followingtotal preorder C among models generates revision sequence, z indexgreatest class R:(C(h) ={Mod(Ki ) | Ki R(h)} h z{I | 6 K1 Kn }h = z + 1Proof. C proved total preorder. First, C(0) empty since R(0) empty.Second, model belongs one class: C(h) C(l)definition exists Ki R(h) Kj R(l) satisfied I; impliesKi Kj consistent; since R result preorder sequence, follows l = h.Models C(z + 1) exactly ones contained classes.Remains proved lexicographic revision generates revision sequenceC: revising C P1 , . . . , Pi generates Ki . Since Ki |= maxset(Pi , . . . , P1 ) assumption,models Ki also models maxset(Pi , . . . , P1 ). Remains provedminimal ones: models greater classes. Let h class modelsKi , model Ki maxset(Pi , . . . , P1 ). satisfy KjR(z + 1), z + 1 > h since z greatest index classes R.satisfies Kj satisfies Ki Kj maxset(Pi , . . . , P1 ), therefore satisfiable.Since R result preorder sequence, Kj class R(l) l > h. impliesC(l). proves every model maxset(Pi , . . . , P1 ) Mod(Ki )class greater h.Lemma 13 revision sequence generated lexicographic revision total preorder lexicographic compatible.Proof. revision sequence generated total preorder Ki |=maxset(Pi , . . . , P1 ) holds every i. Furthermore, result preorder R existsLemma 11. it, define relations ' < as: Ki ' Kj Ki Kjclass R(h); Ki < Kj Ki R(h), Kj R(l) h < l. Since R revisionpreorder sequence, Ki Kj consistent Ki Kj class,implies Ki ' Kj . way, Ki Kj maxset(Pi , . . . , P1 ) consistentKi lower class Kj , implying Ki < Kj . cycle containing < impossibleKi < Kj means class Ki lower class Kj .prove lexicographic compatibility, requires Ki ' Kj Ki < Kjrespectively equivalent consistency Ki Kj Ki Ki maxset(Pi , . . . , P1 ),merely implied them. However, removing relations pairs KiKj satisfying respective condition makes relations weaker. Therefore, newcycle created.prove direction, assume Ki |= maxset(Pi , . . . , P1 ) every' < two relations defined sequence specified definition320fiRevision Historylexicographic compatibility. result preorder sequence shown. TogetherKi |= maxset(Pi , . . . , P1 ), implies revision sequence generated initialpreorder Lemma 12.' <, relation defined: Ki Kj either Ki ' Kj Ki < Kj .relation necessarily transitive, number properties:1. Ki ' Kj Ki Kj Kj Ki ; indeed, Ki ' Kj holds Ki Kjconsistent, implies Kj Ki consistent; therefore, Kj ' Ki , impliesKj Ki ;2. Ki < Kj Ki Kj Kj 6 Ki ; converse, Kj Ki , eitherKj ' Ki Kj < Ki ; cases, cycle ' < containing one < link,contradicts assumption cycle exists;3. reflexive; indeed, formulae Ki consistent assumption; therefore, Ki Kiconsistent, implies Ki ' Ki Ki Ki ;4. Suzumura consistent (Suzumura, 1976): form cycles Ki1 , . . . , Kim =Ki1 Kij Kij+1 j j also Kij+1 6 Kij ; provedbelow.Properties 1 2 mean ' < equivalence strict part , respectively. Indeed, Ki Kj holds either Ki ' Kj Ki < Kj ; former impliesKj Ki , latter Kj 6 Ki . Property 4 consequence fact assumptionnonexistence cycles ' < containing least one link <.Since reflexive (Property 3) Suzumura consistent (Property 4), Suzumuraextension theorem (Suzumura, 1976) total preorder R extending exists. Extendingmeans equivalence strict parts preserved R. Sinceproved ' <, total preorder R Ki Kj class Ki ' KjKi lower class Ki < Kj .Ki Kj consistent Ki ' Kj , implies Ki Kjclass R. Ki Kj maxset(Pi , . . . , P1 ) consistent Ki < Kj , impliesclass Ki R less class Kj . proves R result preorderrevision sequence.Lemma 14 revision sequence [K0 , P1 , K1 . . . , Pn , Kn ] lexicographic compatibleeither Ki 6|= maxset(Pi , . . . , P1 ) consistent sets R1 , . . . , Rn existthat:1. {Pj | 1 j Ki |= Pj } Ri every i;2. exists cycle Ki1 , . . . , Kim = Ki1 either Kij Kij+1 Kij Kij+1 Riconsistent ij {i1 , . . . , im1 }, second consistent least oneindex.Proof.revision sequence lexicographic compatible Ki |=maxset(Pi , . . . , P1 ) cycle specified definition. Inverting321fiLiberatorecondition, sequence lexicographic compatible either Ki 6|= maxset(Pi , . . . , P1 )cycle exists. result, one check whether Ki 6|= maxset(Pi , . . . , P1 ) i;true, check needed: sequence lexicographic compatible.presence cycles irrelevant case. case Ki |= maxset(Pi , . . . , P1 )i, sequence lexicographic compatible containscycles.point condition presence cycles writtenassumption Ki |= maxset(Pi , . . . , P1 ), since case conditionmatters. Lemma 2, consistent subset {P1 , . . . , Pi } containing formulae entailed Ki maxset(Pi , . . . , P1 ). means Ri used placemaxset(Pi , . . . , P1 ), since Ri satisfying first condition statementlemma maxset(Pi , . . . , P1 ).Theorem 7 problem checking existence total preorder generating revisionsequence using lexicographic revision coNP-complete.Proof. Membership follows previous lemma: sequence generatedlexicographic revision preorder lexicographic compatible,turns checked existential quantifiers only:1. Ki 6|= maxset(Pi , . . . , P1 );2. exists Ri ;3. Ri consistent;4. either Ki 6|= Pj Pj Ri ;5. exists cycle Ki1 , . . . , Kim = Ki1 ;6. Kij Kij+1 consistent;7. Kij Kij+1 Ri consistent.first condition expressed terms existential quantifiers shownLemma 3. holds conditions well. result, incompatibilityNP, means existence preorder generating sequence coNP.Hardness proved reduction propositional unsatisfiability. formula Fsatisfiable [K0 , P1 , K1 ] generated preorder, K0 = a, P1 =K1 = F new variable contained F . Indeed, F unsatisfiable,K1 = a, sequence generated preorder C = [C(0), C(1)] C(0) = Mod(a)C(1) = Mod(a). F satisfiable, K1 models satisfy P1 :ones F . result, K1 6|= P1 , sequence generated preorder.322fiRevision HistoryA.5 Reinforcement Revision: ProofsLemma 15 every sequence n numbers V = [V (1), . . . , V (n)], holds MV (I, i, j) =MV (I, i, h) + MV (I, h, j) every three indexes i, j h.Proof. < j MV (I, i, j) sum MV (I, l, l+1) < l j. Otherwise,sum MV (I, l, l + 1) = MV (I, l + 1, l). Also MV (I, i, h) MV (I, h, j) expressedway. h j result follows immediately. Otherwise, < jh > j MV (I, i, h) includes MV (I, l, l + 1) l > j, MV (I, h, j) includesMV (I, l + 1, l) = MV (I, l, l + 1), subtracts amount sum.case h < similar.Lemma 16 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated reinforcement revision total preorder C DC = [DC (1), . . . , DC (n)] DC (i) =min{j | CP1 ,...,Pi1 (j) Mod(Pi ) 6= } |= Ki then, every j:MDC (I, i, j) = 0 |= Kj ;MDC (I, i, j) > 0 otherwise.Proof. model CP1 ...Pi1 (c) CP1 ...Pi (c DC (i)) |= PiCP1 ...Pi1 (c + 1) otherwise. result, MDC (I, i, + 1) difference classpreorder step + 1 preorder step i. Since MDC (I, i, j) sumamounts index index j, difference class stepj i. |= Ki class step zero. result, MDC (I, i, j) classstep j. zero |= Kj greater otherwise.Lemma 17 V = [V (1), . . . , V (n)] reinforcement mover revision sequence[K0 , P1 , K1 , . . . , Pn , Kn ], following initial preorder C = [C(0), . . . , C(V (1) + + V (n +1))] generates revision sequence reinforcement revision DC = V .(C(j) ={I | |= Ki MV (I, i, 0) = j} j < V (1) + . . . + V (n) + 1{I | . 6|= Ki }j = V (1) + . . . + V (n) + 1Proof. Since K0 consistent, least model I. Since |= K0 MV (I, 0, 0) = 0definition, follows C(0), proving C(0) empty. Every modelclass |= Ki C(MV (I, i, 0)), otherwise C(V (1) + . . . + V (n) +1). prove C total preorder, remains prove model belongs twoclasses. |= Ki |= Kj MV (I, j, i) = 0 V reinforcement mover;therefore, MV (I, i, 0) = 0 + MV (I, i, 0) = MV (I, j, i) + MV (I, i, 0) = MV (I, j, 0). modelC(V (1) + . . . + V (n) + 1) satisfy Ki ; therefore, cannotclass C(j). proves C total preorder.longest part proof show reinforcement revision generatessequence C. Inductively, assumed DC (1) = V (1), DC (2) = V (2), . . . ,DC (i) = V (i), DC (l) = min{j | CP1 ,...,Pl1 (j) Mod(Pl ) 6= }. proved323fiLiberatoremodels Ki CP1 ,...,Pi (0) models classes greater indextotal preorder. Furthermore, minimal class models Pi+1 preorderV (i + 1), proving DC (i + 1) = V (i + 1), allows iterate proof.base case = 0: proved Mod(K0 ) = C(0) DC (1) =V (1). construction C, model C(0) MV (I, i, 0) = 0 |= Ki i;hold = 0. Vice versa, C(0) |= Ki MV (I, i, 0) = 0 i.6|= K0 , definition reinforced mover j = 0 implies MV (I, i, 0) > 0,turn implies 6 C(0), contradiction.order prove DC (1) = V (1), let model P1 . construction C,|= K1 C(V (1)). 6|= K1 two cases possible. first, 6|= Kj everyj. implies C(V (1) + . . . + V (n) + 1), V (1) + . . . + V (n) + 1 > V (1).second case, |= Kj j. definition reinforcement mover, MV (I, j, 1) > 0.class C containing index MV (I, j, 0) = MV (I, j, 1)+MV (I, 1, 0) = MV (I, j, 1)+V (1), last step consequence |= Pi . Since MV (I, j, 1) > 0, followsamount greater V (1). proves V (1) index minimal classmodels P1 C: DC (1) = V (1). concludes base case.assumed DC (1) = V (1), . . . , DC (i) = V (i), proved CP1 ...Piclass zero equal Mod(Ki ) minimal models Pi+1 class V (i + 1),proves DC (i + 1) = V (i + 1). induction, proves sequence generatedreinforcement revision total preorder C.Let model Ki . construction, C(MV (I, i, 0)). first revisionsincrease class MDC (I, 0, i). Since DC (1), . . . , DC (i) equal V (1), . . . , V (i)inductive assumption, MV (I, 0, i). definition MV ,holds MV (I, i, 0) = MV (I, 0, i). result, MV (I, i, 0) + MDC (I, 0, i) = MV (I, i, 0)MV (I, i, 0) = 0: model CP1 ...Pi (0).model Ki , may model Kj not. secondcase, C(V (1) + + V (n) + 1). first steps reduce class numberDC (1) + . . . + DC (i), leading V (1) + + V (n) + 1 + DC (1) + . . . + DC (i). Sincefirst values V DC coincide, V (1) + + V (n) + 1 V (1) V (i) =1 + V (i + 1) + + V (n), greater zero.model Kj C(MV (I, j, 0)) definition C. class steptherefore MV (I, j, 0)+MDC (I, 0, i). Since DC (1), . . . , DC (i) coincide V (1), . . . , V (i)induction hypothesis, second term equal MV (I, 0, i). sum thereforeequal MV (I, j, 0) + MV (I, 0, i) = MV (I, j, 0) MV (I, i, 0) = MV (I, j, i) + MV (I, i, 0)MV (I, i, 0) = MV (I, j, i). definition reinforced mover j reversed, since|= Kj 6|= Ki MV (I, j, i) > 0. proves class orderCP1 ...Pi index larger zero.last step proof show models Pi+1 classes indexV (i + 1) greater according ordering step i, CP1 ...Pi . Let modelPi+1 . satisfy Kj C(V (1)+ +V (n)+1). step classleast V (1) + + V (n) + 1 DC (1) DC (i), since DC (j) maximal decreaseclasses step j. Since DC (1), . . . , DC (i) coincide V (1), , V (i) inductionassumption, equal V (1)+ +V (n)+1V (1) V (i) = V (i+1)+ +V (n)+1,larger V (i + 1).324fiRevision Historysatisfies Kj , possibly j = + 1, C(MV (I, j, 0)) definitionC. step i, class index MV (I, j, 0) + MDC (I, 0, i). Since DC (1), . . . , DC (i)assumed equal V (1), . . . , V (i), second term equal MV (I, 0, i) sumMV (I, j, 0) + MV (I, 0, i) = MV (I, j, i). Since |= Pi+1 , MV (I, i, + 1) = V (i + 1)definition MV (Definition 11 V place DC ). definition reinforced moverensures MV (I, j, + 1) = MV (I, j, i) + MV (I, i, + 1) = MV (I, j, i) V (i + 1) equal0 |= Ki+1 greater otherwise. Since Ki+1 models, minimal valueMV (I, j, i) V (i + 1) zero, proving minimal value MV (I, j, i) V (i + 1).proves DC (i + 1) = V (i + 1).Lemma 18 reinforcement revision generates revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ]total preorder, also generates sequence preorderminimal initial class models P1 0 1.Proof. K0 P1 consistent P1 models K0 , class zero.Otherwise, let class indexes models P1 k1 < k2 < k3 < . . . Revision P1decreases numbers k1 , making k1 k1 = 0, k2 k1 , k3 k1 , etc.given initial preorder, new one generated reducing modelP1 k1 1 classes. new initial preorder generates revision sequence.models satisfy P1 changed initial class, revising P1still moved one class.models P1 classes k1 (k1 1), k2 (k1 1), k3 (k1 1), etc.new preorder. Since k1 minimal index models P1 , none indexes zero.Therefore, model P1 enters K0 .minimal class models P1 new initial preorder 1, proving DC (1) =1. implies revising P1 decreases class models P1 one.result, indexes classes k1 (k1 1) 1, k2 (k1 1) 1, k3 (k1 1) 1,etc. coincide k1 k1 = 0, k2 k1 , k3 k1 , etc. classesobtained revising original preorder P1 . models satisfy P1initial class still moved one class. proves orderingrevising P1 before. Therefore, point revision sequencesidentical.Lemma 19 reinforcement revision generates sequence [K0 , P1 , K1 , . . . , Pn , Kn ]total preorder, also generates sequence total preorder CDC (i + 1) DC (1) + . . . + DC (i) + + 1.Proof. claim proved two cases separately: Pi+1 models satisfyKj j < i, not.Case 1: models Pi+1 satisfy Kj j < i. models class zerostep j. Therefore, class j step i. proves Pi+1models class j, minimal class models j less: DC (i + 1) j,less DC (1) + . . . + DC (i) + + 1 j < + 1.Case 2: model Pi+1 Kj j < i. Let C(l) minimal initial classmodels Pi+1 . l > DC (1) + . . . + DC (i), models Pi+1 decreased325fiLiberatorel (DC (1) + . . . + DC (i)) 1 classes without affecting generated sequence. Modelsclass l move class l(l(DC (1)+. . .+DC (i)1)) = DC (1)+. . .+DC (i)+1. maximalclass model may reach step DC (1) + . . . + DC (i) + 1 + i, since step mayincrease class model one. Therefore, DC (i+1) DC (1)+. . .+DC (i)+i+1.remains proved change affect revision results.Regarding steps + 1, since models Pi+1 initial class DC (1) +. . . + DC (i) + 1 greater, models K1 initial class DC (1), minimalclass models P1 still DC (1) result revision still K1 . Since DC (1)before, similar line reasoning applied models K2 DC (2),K3 Ki , proving neither K1 , . . . , Ki DC (1), . . . , DC (i)affected change. Since DC (1), . . . , DC (i) tell models moved, stepmodels Pi+1 classes change. models Pi+1lowered DC (i + 1) classes. Since change altered relative initial positions,modify relative positions step i. preorder step + 1 thereforebefore.Lemma 20 Reinforcement revision generates revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ]total preorder generates sequence total preorderDC (i) bounded 2i 1.Proof. proved DC (1) 1, preorder modified makeso.Since DC (i + 1) bounded DC (1) + . . . + DC (i) + + 1, assuming claimDC (1), . . . , DC (i) leads DC (i + 1) (21 1) + + (2i 1) + + 1,DC (i + 1) (20 + 21 + + 2i ) 20 1 + + 1 = 21 + + 2i , 2i+1 1.Theorem 8 Establishing existence total preorder generating revision sequence[K0 , P1 , . . . , Pn , Kn ] reinforcement revision p2 , coNP n constant.Proof. lemma, revision sequence generated preordergenerated preorder DC (i) 0 2i 1. result,problem solved guessing reinforcement mover V = [V (1), . . . , V (n)]sequence, V (i) 0 2i 1, since implies existencetotal preorder C generating sequence also DC (i) = V (i) indexes.n constant guessing replaced disjunction. Checking whether Vreinforcement mover amount check whether, I, j, holds |= Kiimplies MV (I, i, j) = 0 |= Kj MV (I, i, j) > 0 otherwise. Calculating MV (I, i, j)done polynomial time since checks requires form j >|= Pl . Therefore, verification coNP, whole problem p2guessing V .Theorem 9 Checking existence preorder generating reinforcement revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] coNP-complete, n constant.326fiRevision HistoryProof. Membership proved previous theorem. Hardness provedproblem propositional unsatisfiability. Given formula F , corresponding revisionsequence [K0 , P1 , K1 ] K0 = F , P1 = b K1 = b, bfresh variables occurring F . sequence, K0 = F consistent P1 = b.Therefore, K1 = b equivalent conjunction (a F ) b. Funsatisfiable case. Otherwise, model F extended assigning falseb false model K0 P1 model K1 . proves sequencegenerated preorder F unsatisfiable.ReferencesAlchourron, C., & Makinson, D. (1982). logic theory change: Contraction functionsassociated revision functions. Theoria, 48 (1), 1437.Areces, C., & Becher, V. (2001). Iterable AGM functions. Rott, H., & Williams, M.A. (Eds.), Frontiers Belief Revision, Applied Logic Series, pp. 261277. KluwerAcademic Publisher.Baltag, A., Gierasimczuk, N., & Smets, S. (2011). Belief revision truth-tracking process.Proceedings Thirteenth Conference Theoretical Aspects RationalityKnowledge (TARK 2011), pp. 187190.Benferhat, S., Kaci, S., Le Berre, D., & Williams, M.-A. (2004). Weakening conflictinginformation iterated revision knowledge integration. Artificial Intelligence,153, 339371.Booth, R., & Meyer, T. (2006). Admissible restrained revision. Journal ArtificialIntelligence Research, 26, 127151.Booth, R., & Nittka, A. (2008). Reconstructing agents epistemic state observationsbeliefs non-beliefs. Journal Logic Computation, 18, 755782.Boutilier, C. (1996). Iterated revision minimal change conditional beliefs. JournalPhilosophical Logic, 23, 263305.Carson, R., & Louviere, J. (2011). common nomenclature stated preference elicitationapproaches. Environmental Resource Economics, 49 (4), 539559.Darwiche, A., & Pearl, J. (1997). logic iterated belief revision. Artificial Intelligence Journal, 89 (12), 129.Delgrande, J., Dubois, D., & Lang, J. (2006). Iterated revision prioritized merging.Proceedings, Tenth International Conference Principles Knowledge Representation Reasoning, KR-2006, pp. 210220.Eiter, T., & Gottlob, G. (1996). complexity nested counterfactuals iteratedknowledge base revisions. Journal Computer System Sciences, 53 (3), 497512.Fagin, R., Ullman, J. D., & Vardi, M. Y. (1983). semantics updates databases.Proceedings Second ACM SIGACT SIGMOD Symposium PrinciplesDatabase Systems (PODS83), pp. 352365.327fiLiberatoreFerme, E., & Hansson, S. (2011). AGM 25 years - Twenty-five years research beliefchange. Journal Philosophical Logic, 40 (2), 295331.Gardenfors, P. (1988). Knowledge Flux: Modeling Dynamics Epistemic States.Bradford Books, MIT Press, Cambridge, MA.Hild, M., & Spohn, W. (2008). measurement ranks laws iterated contraction. Artificial Intelligence, 172 (10), 11951218.Jin, Y., & Thielscher, M. (2007). Iterated belief revision, revised. Artificial IntelligenceJournal, 171 (1), 118.Katsuno, H., & Mendelzon, A. O. (1991). Propositional knowledge base revision minimal change. Artificial Intelligence, 52, 263294.Konieczny, S., & Pino Perez, R. (2000). framework iterated revision. JournalApplied Non-Classical Logics, 10, 339367.Lehmann, D., & Magidor, M. (1992). conditional knowledge base entail?Artificial Intelligence, 55, 160.Liberatore, P. (2014a). Belief revision examples.(CoRR), abs/1409.5340.Computing Research RepositoryLiberatore, P. (2014b). Belief revision reliability assessment. Manuscript.Nayak, A. (1994). Iterated belief change based epistemic entrenchment. Erkenntnis, 41,353390.Nebel, B. (1992). Syntax-Based Approaches Belief Revision, pp. 5288. CambridgeUniversity Press.Nittka, A., & Booth, R. (2008). method reasoning agents beliefsobservations. Logic foundation game decision theory, Vol. 3 Textslogic games, pp. 153182.Papini, O. (2001). Iterated revision operations stemming history agentsobservations. Frontiers belief revision, Vol. 22 Applied Logic Series, pp. 279301. Springer.Peppas, P. (2008). Belief revision, pp. 317359. Elsevier.Rott, H. (2009). Shifting priorities: Simple representations twenty-seven iterated theory change operators. Towards Mathematical Philosophy, Vol. 28, pp. 269296.Springer.Sandholm, T., & Conen, W. (2010). Preference elicitation combinatorial auctions. USPatent 7,742,971.See, K., Morrison, W., Rothman, N., & Soll, J. (2011). detrimental effects powerconfidence, advice taking, accuracy. Organizational Behavior HumanDecision Processes, 116 (2), 272285.Spohn, W. (1988). Ordinal conditional functions: dynamic theory epistemic states.Causation Decision, Belief Change, Statistics, pp. 105134. Kluwer Academics.Suzumura, K. (1976). Remarks theory collective choice. Economica, New Series,43, 381390.328fiRevision HistorySyncML (2002). SyncML sync protocol, version 1.1.Tversky, A., & Kahneman, D. (1983). Extensional versus intuitive reasoning: conjunction fallacy probability judgment. Psychological review, 90 (4), 293315.Wang, H., Zhang, J., & Johnson, T. R. (2000). Human belief revision order effect.Proceedings 22th Annual Conference Cognitive Science Society.Williams, M. (1994). Transmutations knowledge systems. Proceedings Fourth International Conference Principles Knowledge Representation Reasoning(KR94), pp. 619629.Winslett, M. (1988). Reasoning actions using possible models approach. Proceedings Seventh National Conference Artificial Intelligence (AAAI88), pp.8993.Zhang, D. (2004). Properties iterated multiple belief revision. Proceedings Seventh International Conference Logic Programming Nonmonotonic Reasoning(LPNMR 2004), pp. 314325.329fiJournal Artificial Intelligence Research 52 (2015) 507-542Submitted 11/14; published 04/15Weighted Electoral ControlPiotr Faliszewskifaliszew@agh.edu.plDepartment Computer ScienceAGH University Science TechnologyKrakow, PolandEdith Hemaspaandraeh@cs.rit.eduDepartment Computer ScienceRochester Institute TechnologyRochester, NY 14623, USALane A. Hemaspaandralane@cs.rochester.eduDepartment Computer ScienceUniversity RochesterRochester, NY 14627, USAAbstractAlthough manipulation bribery extensively studied weighted voting, almost work done election control weighted voting.unfortunate, since weighted voting appears many important natural settings.paper, study complexity controlling outcome weighted electionsadding deleting voters. obtain polynomial-time algorithms, NP-completeness results, many NP-complete cases, approximation algorithms. particular, scoring rules completely characterize complexity weighted voter control. workshows quite important cases, either polynomial-time exact algorithmspolynomial-time approximation algorithms exist.1. Introductionmany real-world election systems voters come weights. Examples rangestockholder elections weighted shares, US Electoral College, often-used example Nassau County Board Supervisors, (in effect) parliamentary systemparties typically vote blocks, Swedens system wealth-weighted votinginstituted 1866 (and longer used) wealthiest members rural communities received many 5,000 votes 10 percent districts weightedvotes three voters could decisive (Congleton, 2011). Furthermore,many important voting applications within multiagent systems, e.g., recommender systems (Ghosh, Mundhe, Hernandez, & Sen, 1999; Lu & Boutilier, 2011), planning (Ephrati& Rosenschein, 1997), web search (Dwork, Kumar, Naor, & Sivakumar, 2001).applications, quite natural voters (i.e., agents) weighted (e.g.,amount trust put power resources possess).surprising study manipulative attacks elections, weightedvoting given great attention. bribery manipulation, two threestudied types manipulative attacks elections, study case weighted votersextensively conducted. Yet remaining one three studied typesc2015AI Access Foundation. rights reserved.fiFaliszewski, Hemaspaandra, & Hemaspaandraattacks elections, so-called control attacks, almost attention givencase weighted voting; best knowledge, time issuepreviously raised two M.S./Ph.D. theses (Russell, 2007; Lin, 2012). lackattention troubling, since key types control attacks, adding deletingvoters, certainly occur many weighted elections. coda section,give examples.study complexity weighted elections arguably important typescontroladding deleting votersfor various election systems. focus scoringrules, families scoring rules, Condorcet-consistent rules, weakCondorcet-consistentrules. Control deleting (adding) voters asks whether given election given candidatemade win deleting (adding) certain number voters (atcertain number members pool potential additional voters). controltypes model issues found many electoral settings, ranging human electronic. (abstractions of) issues often faced people seeking steer election,experts campaign management, deciding example k peopleoffer rides polls. Adding deleting voters also occur multiagent systems.example, agents entities Internet, one attempt denial-of-serviceattack prevent votes arriving time. hand,adding voters pertains simply encouraging agents vote, multiplying existingagents, performing false-name attacks (for false-name attacks related settings, see,example, Wagman & Conitzer, 2014; Waggoner, Xia, & Conitzer, 2012; Aziz, Bachrach,Elkind, & Paterson, 2011).Control introduced (without weights) 1992 seminal paper Bartholdi,Tovey, Trick (1992). Control subject much attention since. attention, present paper, part line work, started Bartholdi, Tovey,Trick (1989, 1992) Bartholdi Orlin (1991), seeks determinetypes manipulative attacks elections attackers task requires polynomial-timecomputation. detailed discussion line work, refer readerrelated work section end paper surveys Faliszewski, Hemaspaandra, Hemaspaandra, Rothe (2009), Faliszewski, Hemaspaandra, Hemaspaandra(2010), Brandt, Conitzer, Endriss (2013).main results follows (see Section 6 tables summarizing results). First,Section 4.1 provide detailed study complexity voter control scoringprotocols, case fixed numbers candidates. show constructivecontrol adding voters constructive control deleting voters P t-approval(and also covers plurality t0 -veto1 ) NP-complete otherwise. interesting compare result analogous theorem regarding weighted coalitionalmanipulation: cases complexities voter control manipulation(e.g., plurality Borda) also cases voter controleasier (t-approval 2, elections candidates). ever possible weighted voter control harder weighted voting manipulation? showweighted voter control NP-hard Condorcet-consistent rules least threecandidates (and clearly also NP-hard weakCondorcet-consistent rules least1. number candidates fixed, t-veto expressed (m t)-approval,number candidates. number candidates unbounded, t-veto t0 -approval.508fiWeighted Electoral Controlthree candidates). Since weighted coalitional manipulation 3-candidate Llull systemP (Faliszewski, Hemaspaandra, & Schnoor, 2008), together fact LlullweakCondorcet-consistent, implies setting weighted voter controlharder weighted coalitional manipulation.Sections 4.2 4.3 focus complexity weighted voter controlt-approval t-veto, case unbounded numbers candidates. startSection 4.2, explain interesting cases. Section 4.2resolve six problems left open Lin (2012). establish complexity weightedcontrol adding voters 2-approval, 2-veto, 3-approval, weighted controldeleting voters 2-approval, 2-veto, 3-veto. Section 4.3, give polynomialtime approximation algorithms weighted voter control t-approval t-veto.algorithms seek minimize number voters added deleted.believe complexity weighted voter control, generally complexity attacks weighted elections, important interesting research directiondeserves much study. particular, research suggests worthwhileseek approximation algorithms weighted elections problemslead interesting algorithms.2. Motivation Studying Control Weighted ElectionsIntroduction, noted importance weights many electoral settingsdescribed natural importance ofand gave pointers extensive line workstudyingcontrol attacks many (unweighted) settings. also stated control attacks naturally expected occur even many weighted election settings.present section, give examples motivating study weighted electoralcontrol.Let us consider academic department salient issue particularterm question course add B.S. majors requirements. Supposedepartment highly polarized issue research area, i.e., faculty givenresearch area vote block (either agree, traditionmeet actual faculty meetings to, within group, reach group positionsupport). suppose group days/timesentire group would unlikely attend, e.g., time timemajor yearly research conference area. department chair, knowingpower schedule faculty meetings held, agendameeting, might well model task weighted control deleting votersproblem, voters groups, groups weight number facultymembers groups, deletion limit one.fact, generally, individual voters may blur collection weighted votessettings voter set partitions groups express identical preferences.another example sort, one authors schools, members faculty senatechosen election system known single transferable vote. However,unheard departmental leaders send friendly suggestion departmentsmembers regarding vote. one assumes departments vote blocks,one trying decide candidates add election, convince509fiFaliszewski, Hemaspaandra, & Hemaspaandrarun election, order make given candidate win (or win), one effectstudying weighted constructive control adding candidates, weighted constructive controldeleting candidates, weighted destructive control adding candidates, weighteddestructive control deleting candidates.examples given ones weightings created individualsforming blocks, occur even highly political contexts.example, US House Representatives, issues (for example, water rights farmsubsidies) states delegation tends vote block parochial interestsstates constituents companies, pressure lobbyists state delegations abstaingiven vote effect (give take issues failing quorum) controldeleting voters attack.However, also many voting cases weights inherent standalone individual voters, many cases control attacks may well occur.example, consider US Corporate Elections. these, vote stockholder weightednumber shares. natural way frame control problemssetting case adding deleting candidates, example, regarding runningspot company officer director. even voter control come play here,example, actor sending mailings toor phoning speaking tovotersconvince abstain voting, encourage voters vote election.(The bound additions/deletions model counts number voters, ratherweights, quite reasonable setting, regardless weight, giventargeted voter addressed by, example, mailing/visit/phone-call, althoughreality one admittedly might focus resources biggest stockholders.)many weighted control examples presented. Let us finishextremely high-stakes example. US Electoral College, worksmajority rule among electors, electors state usually vote block,since system lets whoever greatest popular vote given state select everyelector state (note: two fifty states different policies). Thus issueof, example, whether someone Ralph Nader run,withdraw name consideration particular time, sweepingeffect nation, effect weighted control adding/deleting candidatesscenario.given number examples, voter control candidatecontrol, settings weighted control may occur. examples given varynaturalness, weighted unweighted control certainlypoints models dont capture nuances real world. example,electoral partitioning problems geographic/contiguity constraints, groupsmodeled voting blocks may fact defectors, Internet denial-of-serviceattacks may freedom suppress vote independently rather maysuppress none votes coming given line/provider (see, e.g., Chen,Faliszewski, Niedermeier, & Talmon, 2014). Nonetheless, belief importanceweighted elections importance control attacks remain forever separate.feel control attacks sufficiently natural many weighted settingsvaryingacademic departments companies stockholders nationsthat studying weightedcontrol worth undertaking. also feel that, although subject510fiWeighted Electoral Controlpresent paper, important experimental studies undertaken seeextent heuristic approaches circumvent worst-case hardness results regardingweighted control (see Rothe & Schend, 2013, assessment type approachsettings, although see also Hemaspaandra & Williams, 2012, discussionlimitations heuristic attacks).3. Preliminariesassume reader familiar basic notions computational complexitytheory theory algorithms. provide relevant definitions conventionsregarding elections, election rules, control elections. also review NPcomplete problems use reductions.3.1 Electionstake election pair E = (C, V ), C set candidates Vcollection voters. voter preference order set C. preference ordertotal, linear order ranks candidates preferred one leastpreferred one. example, C = {a, b, c} voter likes best, b,c, preference order > b > c. weighted elections, voter v alsopositive integer weight (v). voter weight (v) treated election system(v) unweighted voters. Given two collections voters, V W , write V + Wdenote concatenation.3.2 Election Ruleselection rule (or voting rule) function R given election E = (C, V ) returnssubset R(E) C, namely candidates said win election.m-candidate scoring rule defined nonincreasing vector = (1 , . . . , )nonnegative integers. voter v, candidate c receives pos(v,c) points,pos(v, c) position c vs preference order. candidates maximumtotal score winners. Given election E voting rule R assigns scorescandidates, write score E (c) denote cs total score E R. voting ruleused always clear context. Many election rules defined familiesscoring rules, one scoring vector possible number candidates. example:1. Plurality rule uses vectors form (1, 0, . . . , 0).2. t-approval uses vectors (1 , . . . , ), = 1 {1, . . . , t}, = 0> t. t-veto mean system candidates uses (mt)-approvalscoring vector. m-candidate t-approval t-veto systems often treatvote 0/1 m-dimensional approval vector indicates candidates receivepoints vote. Naturally, vector contains exactly ones t-approvalexactly zeroes t-veto.22. emphasize view t-approval t-veto correct settings set candidatesremains fixed. set candidates change (e.g., control adding/deleting candidates),would use standard, preference-order-based definition.511fiFaliszewski, Hemaspaandra, & Hemaspaandra3. Bordas rule uses vectors form (m 1, 2, . . . , 0), numbercandidates.Given election E = (C, V ), candidate c Condorcet winner (weak Condorcetwinner) every candidate C {c} holds half (at least half)voters prefer c d. Note possible Condorcet winner givenelection, even possible weak Condorcet winner given election.Let Condorcet denote election system whose winner set exactly set Condorcetwinners, let weakCondorcet denote election system whose winner set exactlyset weak Condorcet winners. say rule R Condorcet-consistent wheneverCondorcet winner sole winner elected R. Analogously, ruleweakCondorcet-consistent elects exactly weak Condorcet winners wheneverexist. Every weakCondorcet-consistent system Condorcet-consistent, conversealways hold.many Condorcet-consistent rules. briefly touch upon Copelandfamily rules Maximin rule. given election E = (C, V ) two distinctcandidates c, C, let NE (c, d) number voters prefer c d. Letrational number, 0 1. Copeland score candidate c C defined as:k{d C {c} | NE (c, d) > NE (d, c)}k + k{d C {c} | NE (c, d) = NE (d, c)}k,Maximin score candidate c C defined mindC{c} NE (c, d).candidates highest score winners. Llull another name Copeland1 . Clearly,Llull Maximin weakCondorcet-consistent.3.3 Electoral Controlfocus constructive control adding/deleting voters weighted elections. However,also standard types control studied literature (e.g., controladding/deleting candidates various forms partitioning candidates voters;point reader Section 5 discussion related work).Definition 3.1. Let R voting rule. weighted constructive control addingvoters rule R (R-WCCAV) weighted constructive control deleting votersrule R (R-WCCDV), input contains set candidates C, collection weightedvoters V (sometimes referred registered voters) preferences C, preferredcandidate p C, nonnegative integer k. R-WCCAV also additionalcollection W weighted voters (sometimes referred unregistered voters)preferences C. problems ask following questions:1. R-WCCAV: subcollection W 0 W , k voters, pR(C, V +W 0 )?2. R-WCCDV: subcollection V 0 V , k voters, pR(C, V V 0 )?Although paper focus primarily constructive control, Section 4.1 makescomments so-called destructive variants control problems. Given voting rule R, weighted destructive control adding voters rule R (R-WDCAV)512fiWeighted Electoral Controlweighted destructive control deleting voters rule R (R-WDCDV) defined analogously constructive variants, difference goalensure distinguished candidate p winner. mention passingthroughout paper use known nonunique-winner model (a.k.a.cowinner model), i.e., goal make p be, prevent p being, elementwinner set. consider nonunique-winner model cleaner naturalmodel so-called unique-winner model, p must made keptone-and-only winner election, model strongly blurs tie-breaking issuescontrol issues.Note definitions parameter k defines number votersadded/deleted, total weight voters added/deleted.standard approach modeling strategic behavior weighted elections. example,study R-weighted-bribery (Faliszewski, Hemaspaandra, & Hemaspaandra, 2009),bribing weighted voter unit cost regardless voters weight, studyweighted manipulation nearly single-peaked societies (Faliszewski, Hemaspaandra,& Hemaspaandra, 2014), mavericity society depends number so-calledmavericks rather total weight. k might practice, k reflectingability chair add/delete voters, k practice would reflect manyvoters chair viewed resources lure pressure out.consider approximation algorithms WCCAV WCCDV t-approvalt-veto. so, assume input instances contain integerk. Rather, goal simply find (when success possible all) small possiblecollection voters add/delete p winner resulting election. (Justmentioned previous paragraph, counting number added/deletedvoters, total weight added/deleted voters.) positive integer h,h-approximation algorithm WCCAV/WCCDV algorithm (when successpossible all) always finds solution adds/deletes h times many votersoptimal action does. notion f ()-approximation algorithm WCCAV/WCCDVdefined analogously, argument f variable related probleminstance. meaning O(f ())-approximation algorithms similarly clearcontext. natural worry seemingly incomplete definitionsinteract possibility success might impossible regardless many votesone adds/deletes. However, t-approval WCCDV t-veto WCCDV (and indeed,scoring rule), always possible ensure p winner, example deletingvoters (recall nonunique-winner model). t-approval WCCAVt-veto WCCAV, possible ensure ps victory adding votersp winner add unregistered voters approve p. observationsmake particularly easy discuss study approximation algorithms t-approvalt-veto, always easily check whether solution. votingrules dont easy-checking property, analysis might muchcomplicated. reader may wish compare work Brelsford et al.s attemptframing general election-problem approximation framework (Brelsford, Faliszewski,Hemaspaandra, Schnoor, & Schnoor, 2008).paper consider candidate-control cases (such weighted constructive control adding candidates weighted constructive control deleting candidates,513fiFaliszewski, Hemaspaandra, & HemaspaandraWCCAC WCCDC). reason bounded number candidates,winner determination given weighted election system P holds WCCAC WCCDC P brute-force search. hand, numbercandidates bounded candidate control already NP-hard plurality (andt-approval t-veto, constructive setting destructive setting) evenwithout weights (Bartholdi et al., 1992; Hemaspaandra, Hemaspaandra, & Rothe, 2007;Elkind, Faliszewski, & Slinko, 2011; Lin, 2012). Furthermore, many results candidatecontrol Condorcet-consistent rules claimed weighted setting. example, Maximin rule Copeland family rules, hardness results translateimmediately, straightforward see existing polynomial-time algorithmsunweighted cases also work weighted cases (Faliszewski, Hemaspaandra, &Hemaspaandra, 2011).3.4 Weighted Coalitional ManipulationOne goals compare complexity weighted voter control complexityweighted coalitional manipulation (WCM). WCM similar WCCAV alsoadd voters, differs (a) add exactly given number voters,(b) pick preference orders added voters. quite interesting seedifferences problems definitions affect complexities.Definition 3.2. Let R voting rule. R-WCM given weighted election (C, V ),preferred candidate p C, sequence k1 , . . . , kn positive integers. ask whetherpossible construct collection W = (w1 , . . . , wn ) n voters i,1 n, (wi ) = ki , p winner R election (C, V +W ). voters Wcalled manipulators.3.5 Computational ComplexityNP-hardness proofs use reductions following NP-complete problems.Definition 3.3. instance Partition consists sequence (k1 , . . . , kt )Ppositiveintegers whose sum even. ask whether set {1, . . . , t} iI ki =1 Pti=1 ki .2proof Theorem 4.3 use following restricted version Partition,greater control numbers involved problem.Definition 3.4. instance Partition 0 consists sequence (k1 , . . . , kt ) positiveintegers, whose sum Peven, (a) even number, (b) ki , 1 t,1holds ki t+1 tj=1 kj . ask whether set {1, . . . , t} cardinalityP1 PtiI ki = 2i=1 ki .2Showing NP-completeness problem standard exercise. (In particular,NP-completeness variant problem established Lemma 2.3 Faliszewskiet al., 2009; approach used show NP-completeness Partition0 .)remaining hardness proofs based reductions restricted version wellknown Exact-Cover-By-3-Sets problem. restricted version still NP-complete (Garey& Johnson, 1979).514fiWeighted Electoral ControlDefinition 3.5. instance X3C 0 consists set B = {b1 , . . . , b3t } family= {S1 , . . . , Sn } 3-element subsets B every element B occurs leastone three sets S. ask whether contains exact cover B, i.e.,whether exist sets whose union B.choice use X3C0 basis reductions, particular wayuse it, allow us achieve something beyond simply showing given weightedcontrol result NP-complete. indeed able show certain weighted controlresults important election systems remain NP-complete even allowed setweights highly restricted, e.g., cases, allowed weight set {1, 2}{1, 3}. cases sort appear within proof Theorem 4.13 highlightedparagraph immediately preceding theorem.4. Resultspresent results. Section 4.1 focus fixed numbers candidatesscoring protocols, weakCondorcet-consistent rules, Condorcet-consistent rules.Sections 4.2 4.3 consider case unbounded number candidates, t-approvalt-veto.4.1 Bounded Numbers Candidateswell-known weighted manipulation scoring protocols always hard, unlessscoring protocol effect plurality triviality (Hemaspaandra & Hemaspaandra, 2007).contrast, weighted voter control easy m-candidate t-approval.Theorem 4.1. t, WCCAV WCCDV m-candidate t-approvalP.Proof. Let (C, V, W, p, k) instance WCCAV m-candidate t-approval.assume add voters approve p. also assume addheaviest voters particular set approvals, i.e., add ` voters approvingp, c1 , . . . , ct1 , assumeadded ` heaviest voters approving p, c1 , . . . , ct1 .m1Since t1 constantdifferent sets approvals consider,suffices try sequences nonnegative integers k1 , . . . , k(m1) whose sumt1k, sequence check whether adding heaviest ki voters ithapproval collection makes p winner.fixed t, clear algorithm, although brute-force nature,runs time polynomial input size. (The actions algorithm uses relativelyinnocuous, fact. example, use sorting group together votes within Widentical sets approvals, sort descending order voter weight.number sequences nonnegative integers k1 , . . . , k(m1) whose sumt1m1k easily bounded (k + 1)( t1 ) , fixed polynomial inputsize despite fact k input binary, may without loss generalitym1assume k kW k. mention passing (k + 1)( t1 ) bound often wildlyloose. particular, exact number sequences nonnegative integers k1 , . . . , k(m1)t1515fiFaliszewski, Hemaspaandra, & Hemaspaandra+k0 1(m1t1 ). summing k 0 equals 0 k 0 equals km1( t1 )1gives number sequences face.)approach argument work WCCDV. Here, delete votersapprove p, delete heaviest voters approval collection.Again, fixed, running time easily seen polynomial.whose sum exactly k 0One might think argument works scoring protocol,case. example, consider 3-candidate Borda instance V consists oneweight-1 voter b > p > W consists weight-2 weight-1 voter preferenceorder > p > b. adding weight-1 voter makes p winner, adding weight-2voter not. And, fact, following result.3Theorem 4.2. WCCAV WCCDV Borda NP-complete. result holds evenrestricted fixed number 3 candidates.Proof. start considering case adding voters. reduce Partition. Givensequence k1 , . . . , kt positive integers sum 2K, construct election oneregistered voter weight K voting b > p > > , unregistered voters weightsk1 , . . . , kt voting > p > b > . Set addition limit t. candidates,(initial) score b K(m 1), score p K(m 2), score K(m 3).Thus, p become winner, bs score (relative p) needs go least K,score (relative p) go K. follows k1 , . . . , ktpartition p made winner.use construction deleting voters case. Now, voters registereddeletion limit t. Since cant delete voters since goal make pwinner, cannot delete one voter voting b > p > > (since wouldunique winner). rest argument identical case adding voters.Interestingly, possible extend proof work scoring protocolst-approval (the main idea stays same, technical detailsinvolved). so, regarding complexity WCCAV WCCDV scoring protocolsfixed number candidates, cases Theorem 4.1 P cases (assumingP 6= NP).Theorem 4.3. scoring protocol (1 , . . . , ), exists i, 1 < < m,1 > > , WCCAV WCCDV (1 , . . . , ) NP-complete.Proof. Let = (1 , . . . , ) scoring protocol 1 > >. Let third largest value set {1 , . . . , }. show WCCAVWCCDV NP-complete scoring protocol = (1 , . . . , ) = (1 , . . . , ).formally defined scoring protocols contain nonnegative values, usingsimplifies construction affect correctness proof.simplify notation, given candidates x1 , . . . , x` , F [x1 = i1 , x2 = i2 , . . . , x` = i` ]mean fixed preference order ensures, , xj , 1 j `, ranked3. analogue theorem model bounding total weight votesadded/deleted obtained Russell (2007).516fiWeighted Electoral Controlposition gives ij points. (The candidates mentioned F [. . .] notationranked arbitrarily.) let 1 , 2 , 3 three highest values set {1 , . . . , }.Clearly, 1 = 1 > 2 > 3 = 0. (Note 2 might different 2 , 3 mightdifferent 3 . example, = (3, 3, 2, 0, 0, 1, 1), 1 = 3, 2 = 2,3 = 0, 1 = 3, 2 = 3, 3 = 2.)give reduction Partition -WCCAV (the membership -WCCAV NPclear); let (k1 , . . . , kt ) instance Partition, i.e., sequence positive integerssum 2K. form election E = (C, V ) C = {p, a, b, c4 , . . . , cm }collection V contains following three groups voters (for WCCAV part proofbelow, set = 1; WCCDV part proof use constructionlarger value ):1. group voters, weight K preference order F [b = 1 , = 2 , p = 0].2. group voters, weight K preference order F [p = 1 , b = 2 , = 0].3. ci C, 6 collections 2T voters, one collection permutation (x, y, z) (p, a, b); voters collection weight K preferenceorder F [x = 1 , = 2 , z = 3 , ci = ].Let number points a, b, p receive third groupvoters (each candidates receives number points voters).ci C x {p, a, b}, x receives least 4T K1 points civoters third group (in vote third group, x receives least manypoints ci , two collections 2T voters x receives 1 = 1 pointsci receives 0 points). Thus holds candidates following scores:1. p + K1 points,2. + K2 points,3. b + K(1 + 2 ) points,4. candidate ci C 2T K1 points (each ci C receives4T K1 points third group voters 2T K1 pointsfirst two groups voters).result, b unique winner. unregistered voters weightsk1 , . . . , kt , preference order F = [a = 1 , p = 2 , b = 0]. set addition limit t. clear irrespective voters added, nonecandidates {c4 , . . . , cm } becomes winner.subcollection (k1 , . . . , kt ) sums K, adding correspondingunregistered voters election ensures three p, a, b winners (eachscore + K(1 + 2 )). hand, assume unregistered voterstotal weight L, whose addition election ensures p among winners.adding voters election, p +T K1 +T L2 points, +T L1 +T K2points, b + K1 + K2 points. p score least high b,must L K. However, score higher p, must case517fiFaliszewski, Hemaspaandra, & HemaspaandraL K (recall 1 > 2 ). means L = K. Thus possible ensurep winner election adding unregistered voterssubcollection (k1 , . . . , kt ) sums K. And, completing proof, notereduction carried polynomial time.Let us move case WCCDV. use construction,following modifications:1. reduction Partition0 . Thus without loss generality assume1even number i, 1 t, holds ki 1+t2K.l12. set = 2t (t + 1) 1+ 1 (the reasons choice become apparent2course proof; intuitively convenient think large valuethat, nonetheless, polynomially bounded respect t).3. include unregistered voters fourth group voters.4. set deletion limit 2t .Including fourth group voters, candidates following scores: p +K1 + 2T K2 points, + K2 + 2T K1 points, b + K(1 + 2 ) points,candidate ci C points.reasoning WCCAV case, see size- 2t subcollection k1 , . . . , kt sums K, deleting corresponding voters ensuresp among winners (together b); may imagine first removevoters fourth group add back 2t them, whose weights sumK. show way delete 2t voters ensure pamong winners, deleted voters must come fourth group, musttotal weight K, must exactly 2t them. sake contradiction, let usassume possible ensure ps victory deleting 2t voters, fewer2t come fourth group. Let number deleted voters fourthgroup (s < 2t ) let x real number xT K total weight.xT K (see explanation regarding first inequality)ts2 +1(2T K) 2T K 1.xT K 2T K= TK1+t1+t1+tis, 0 x 1+t. see first inequality holds, recall1lowest weight voter fourth group least 1+t2T K (because reduce0Partition ). Thus highest total weight voters fourth group is, most,total weight fourth-group voters (2T K) less weight lightest votersts(2T K)).group (which least 1+tPrior deleting voters, K(1 2 ) points p. deletingvoters fourth group, difference decreases K(1 x)(1 2 ).additionally delete 2t voters first three groups voters, weight K,difference scores p decreases, most, following value(note deleted vote p ranked positions receive 1 ,518fiWeighted Electoral Control2 , 0 points):1K(1x)(1 2 ) K1 K(1 2 ) K1 = K2t+12(t + 1)1(1 2 )2t+1t+1> 0.final inequality follows choice . calculation showsway ensure ps victory deleting 2t voters requires deleting exactly2 voters fourth group. reasoning case WCCAV shows2t deleted voters must correspond size- 2t subcollection (k1 , . . . , kt ) sumsK.side comment, mention WDCAV WDCDV scoring protocols (thatis, destructive variants WCCAV WCCDV) simple polynomial-time algorithms: suffices loop candidates c, c 6= p, greedily add/delete votersboost score c relative p much possible.Theorem 4.4. scoring protocol = (1 , . . . , ), -WDCAV -WDCDVP.Combining Theorems 4.1 4.3, obtain following corollary, contrast analogous result WCM (Hemaspaandra & Hemaspaandra, 2007); alsomention passing recent attainment dichotomy result voter controlso-called pure scoring rules, unweighted elections unbounded number candidates (Hemaspaandra, Hemaspaandra, & Schnoor, 2014).Corollary 4.5. scoring protocol (1 , . . . , ) problems WCCAV WCCDVNP-complete k{1 , . . . , }k 3 P otherwise.Theorem 4.6 (Hemaspaandra & Hemaspaandra, 2007). scoring protocol(1 , . . . , ), 2, WCM NP-complete 2 > P otherwise.see scoring protocols fixed number candidates, either WCMharder WCCAV WCCDV (for case t-approval 2 < m),complexity WCM, WCCAV, WCCDV (P-membership pluralitytriviality, NP-completeness remaining cases). One may wonderproperty WCM responsible fact t-approval, 2 m, WCMharder WCCAV WCCDC. Speaking informally, answer WCMintimately involves instantiation values (initially unspecified) votesmanipulators, particular setting is, effect, requiring solvePartition problem. hand, WCCAV WCCDV preference ordersvoters fixed input, chair chooses votes add; this,example, facilitated polynomial-time algorithm proof Theorem 4.1.are, nonetheless, voting rules WCM easier WCCAVWCCDV. happens, example, one hand WCCAV WCCDV particular rule chair balance differing votes way makesproblems hard, yet hand WCM particular ruleshow successful manipulation one manipulators cast identical votes. Theorem 4.7, Corollary 4.8, proofs present exactlycase.519fiFaliszewski, Hemaspaandra, & HemaspaandraTheorem 4.7. every weakCondorcet-consistent election system everyCondorcet-consistent election system, WCCAV WCCDV NP-hard. result holdseven restricted fixed number 3 candidates.Proof. show WCCAV NP-hard, reduce Partition. Given sequencek1 , . . . , kt positive integers sum 2K, construct election two registeredvoters, one voter weight 1 voting p > > b > one voter weight 2K votingb > p > > , unregistered voters weights 2k1 , . . . , 2kt voting > p > b > .Set addition limit t. Suppose add unregistered voters election totalvote weight equal 2L.L < K, b Condorcet winner, thus unique winner election.L > K, Condorcet winner, thus unique winner election.L = K, p Condorcet winner, thus unique winner election.WCCDV case uses construction. Now, voters registereddeletion limit t. Since delete + 2 voters, since goalmake p winner, cant delete sole voter voting b > p > a, since wouldCondorcet winner. rest argument similar adding voters case.Recall Section 3 Condorcet denotes election system whose winner setexactly set Condorcet winners, weakCondorcet denotes election system whosewinner set exactly set weak Condorcet winners.Corollary 4.8. Condorcet weakCondorcet, WCM P WCCAVWCCDV NP-complete. result holds even restricted fixed number 3candidates.Proof. immediate WCM Condorcet weakCondorcet P. seeyes-instance WCM, suffices check whether letting manipulators rankp (the preferred candidate) first ranking remaining candidates arbitraryorder ensures ps victory. NP-completeness WCCAV WCCDV follows directlyTheorem 4.7.Condorcet weakCondorcet always winners. prefervoting systems always least one winner, note WCM 3-candidate LlullP (Faliszewski et al., 2008).Corollary 4.9. 3-candidate Llull, WCM P WCCAV WCCDV NPcomplete.main results section also presented Table 1 Section 6.520fiWeighted Electoral Control4.2 t-Approval t-Veto Unbounded Number CandidatesLet us look cases t-approval t-veto rules, unbounded numbercandidates. reason focus interesting familiesscoring protocols whose complexity already resolved previous section.reason say Theorem 4.3 shows whenever least threedistinct values scoring vector, NP-completeness. scoring-protocol familythat, number candidates, three distinct values scoring vector NPhard WCCAV WCCDV. Thus really interesting cases indeed t-approvalt-veto.starting point work Lin (2012), showed 4, WCCAVt-approval WCCDV t-veto NP-complete, 3, WCCDVt-approval WCCAV t-veto NP-complete. results hold evenunweighted case. also known remaining unweighted cases P (Bartholdiet al., 1992; Lin, 2012) WCCAV WCCDV plurality veto P (Lin,2012). section, look solve remaining open cases, WCCAV 2approval, 3-approval, 2-veto, WCCDV 2-approval, 2-veto, 3-veto.start showing 2-approval-WCCAV P. point proof techniques(especially polynomial-time algorithms 2-approval-WCCAV 2-veto-WCCDV)quite different Lin (2012).Theorem 4.10. WCCAV 2-approval P.Proof. claim Algorithm 1 solves 2-approval-WCCAV polynomial time. (Inalgorithm proof correctness, whenever speak r heaviest voters voterset X, mean min(r, kXk) heaviest voters X.)note add voters approve p. Thus delete Wvoters approve p.Let us consider repeat-until loop Algorithm 1. reject first iterationloop (in first forall loop) then, clearly, solution given instance.Furthermore, claim solution input instance, secondforall loop still possible find it. see this, consider candidate c C {p}number ` {1, . . . , k 1}. sum weights k ` heaviest votersW approve c less sc (that is, less differencescore c score p original election), certainly need add leastk ` + 1 voters approve c. However, since altogether add kvoters, means add ` 1 voters approve c. effect,safely delete W ` 1 heaviest voters approve c (as proofTheorem 4.1, decide add r voters approving {p, c}, may assumeadd r heaviest voters approving {p, c}; thus keeping ` 1 heaviest votersapprove {p, c} correct strategy).So, reject first iteration repeat-until loop, certainlysolution input instance, not, start second iterationinstance solution original one had. Thus, induction,never reject incorrectly repeat-until loop. get repeat-until withoutrejecting, fewer k voters left W , adding W best(since voters W approve p).521fiFaliszewski, Hemaspaandra, & HemaspaandraAlgorithm 1: 2-approval-WCCAVInput: (C, V, W, p, k)forall c C {p}let sc = score(C,V ) (c) score(C,V ) (p).Delete W voters approve p.repeatforall c C {p}sum weights k heaviest voters W approve cless sc reject// impossible get score(c) score(p) adding less equal k votersW .forall c C {p} ` {1, . . . , k 1}sum weights k ` heaviest voters W approve cless scdelete W voters approving c except ` 1 heaviest voters.// need add least k ` + 1 voters approve c,add ` 1 voters approving c.changes.kW k k accept // make p winner adding k heaviest votersW.kW k < kadding W make p winner accept else rejecthand, get repeat-until loop, least k votersleft W , adding k heaviest voters W make p winner. Why? Let ccandidate C {p}. Let r number voters W added approvec. Since made repeat-until, know [the sum weightsk heaviest voters W approve c] least sc (because rejectfirst forall loop). show adding voters, score(c) score(p) 0,implies p winner. r = 0, score(c) score(p) = sc - [the sum weightsk heaviest voters W ] 0. r > 0, [the sum weights k rheaviest voters W approve c] least sc (for otherwise wouldr 1 voters approving c left W due statement second forall loop).score(c) score(p) = sc - [the sum weights k r heaviest voters Wapprove c] 0.Theorem 4.11. WCCDV 2-veto P.Instead proving theorem directly, show general relationcomplexity t-approval/t-veto WCCAV WCCDV.Theorem 4.12. fixed t, holds t-veto-WCCDV (t-approval-WCCDV)polynomial-time many-one reduces t-approval-WCCAV (t-veto-WCCAV).Proof. first give reduction t-veto-WCCDV t-approval-WCCAV. ideadeleting t-veto vote v t-veto election (C, V ) equivalent, terms net effectscores, adding t-approval vote v 0 election, v 0 approves exactly522fiWeighted Electoral Controlcandidates v disapproves of. problem approachreduce t-veto-WCCDV t-approval -WCCAV thus show implementt-veto scores t-approval votes.Let (C, V, p, k) instance t-veto-WCCDV, V = (v1 , . . . , vn ). Let = kCk.Let max highest weight vote V . set set 1 newcandidates, kCk + kDk multiple t. set V0 collection kCk+kDkt-approval votes, vote weight max candidate C approved(t1)(mt)exactly one votes. vote vi V create set Ci = {c1i , . . . , ci}candidates create collection voters Vi = (vi1 , . . . , vimt ). voter vij ,1 j t, weight (vi ) approves jth candidate approved v(j1)(t1)+1j(t1)1 candidates ci, . . . , ci.000form election E = (C , V ), C 0 = CD ni=1 Ci V 0 = V0 +V1 + +Vn .candidate c, let sc cs t-veto score (C, V ); see cs t-approval scoreE 0 max + sc (every candidate C receives single approval one weight maxvoter V0 voter vi V candidate c vi approves of,unique voter Vi weight vi approves c). Furthermore,candidate c C 0 C t-approval score max E 0 (each candidate C 0 Capproved exactly one voter V 0 voter V 0 weight max ).form instance (C 0 , V 0 , W, p, k) t-approval-WCCAV, W = (w1 , . . . , wn ),i, 1 n, (wi ) = (vi ), wi approves exactly candidatesvi disapproves of; adding voter wi t-approval election (C 0 , V 0 ) neteffect scores candidates C deleting vi t-veto election (C, V ).(The role candidates pad election easy use t-approvalvotesthose V0 ensure candidates C least many pointscandidates, irrespective voters add.) completes reduction.Let us give reduction t-approval-WCCDV t-veto-WCCAV. ideaprevious reduction main difficulty proof showimplement t-approval scores t-veto votes.4 particular, role candidatesis, again, provide convenient way padding election implementing scorescandidates. However, time construction involvednature t-veto: opposed case t-approval, t-veto add candidateelection total number candidates approved per vote increases.Let (C, V, p, k) instance t-approval-WCCDV, V = (v1 , . . . , vn ). Let= kCk let max highest weight vote V . set setcandidates kDk 2t 1 kCk + kDk = integer s, 3(note setting trivial must case > t). set V0collection 4n(s 2) (t-veto) votes (over candidate set C D), weightmax ; candidate C approved votes whereas candidatedisapproved least half (since kDk 2t 1, easy construct4. reader may wonder simply use previous argument applying (m t)-veto(m t)-approval. reason given instance (m t)-veto-WCCDV (with candidatesn voters), reduction would output instance (m t)-approval-WCCAVcandidates. Thus would correct interpret instance t-veto-WCCAV instance.523fiFaliszewski, Hemaspaandra, & Hemaspaandravotes5 ). vote vi V , create collection Vi (s 1) votes satisfyingfollowing requirements: (a) candidate approved vi also approvedvotes Vi , (b) candidate approved vi , approved exactly (s 2) votesVi . (Such votes easy construct: always place top candidates vitop positions vote; remaining positions, first vote placecandidates arbitrary, easily computable order, following vote shiftcandidates cyclically positions respect previous vote.) voteVi weight (vi ).form election E 0 = (C 0 , V 0 ), C 0 = C V 0 = V0 + V1 + + Vn .0candidate c, let sc csPnt-approval score (C, V ); see cs t-veto score E4n(s 2)max + (s 2)( i=1 (vi )) + sc (c approved every voter V0least 2 voters group Vi , 1 n; additionally, every voter viapproves c, (s 1)th voter group Vi approves c). Furthermore,candidate t-veto score 3n(s 2)max E 0 (each gets2n(s 2)max points voters V0 (s 2)max pointsVi , 1 n).form instance (C 0 , V 0 , W, p, k) t-veto-WCCAV, W = (w1 , . . . , wn ),i, 1 n, (wi ) = (vi ), wi disapproves exactly candidates viapproves of; adding voter wi t-veto election (C 0 , V 0 ) net effect scorescandidates C deleting voter vi t-approval election (C, V ) has. Furthermore,since candidate least nmax fewer points candidate C,fact adding wi increases scores candidates affect correctnessreduction.remaining cases (WCCDV 2-approval, WCCAV 3-approval, WCCAV2-veto, WCCDV 3-veto) NP-complete. Interestingly, contrast manyNP-complete weighted election problems, need limited set weightsmake reductions work. Namely, due choice reducing X3C0 dueparticular reductions build, proof following theorem establishes (thedetails given within proof) (a) every pair integers 1 < b,holds WCCDV 2-approval WCCAV 2-veto NP-complete evenlegal set weights restricted {a, b}, (b) WCCDV 3-approval WCCAV3-veto NP-complete even legal set weights restricted {1, 3}.Theorem 4.13. WCCAV 2-veto 3-approval WCCDV 2-approval 3-vetoNP-complete.Proof. Membership NP immediate, suffices prove NP-hardness. firstgive proof WCCDV 2-approval. Theorem 4.12 also immediately givesresult WCCAV 2-veto. reduce X3C0 Definition 3.5. LetB = {b1 , ..., b3t } let = {S1 , ..., Sn } family 3-element subsets Bevery element B occurs least one three sets S. construct5. one{d1 , . . . , dt }evenexactlypossible construction. Let = {d1 , . . . , d` }, ` 2t 1. form sets D0 =D1 = {d` , . . . , d`t+1 }. = D0 D1 (D0 D1 might overlap).number voters V0 ; exactly half disapprove candidates set D0half disapprove candidates set D1 .524fiWeighted Electoral Controlfollowing instance (C, V, p, k) WCCDV 2-approval. set C = {p} {bj | 1 j3t} {si , s0i | 1 n} {d0 , d1 , . . . , d3t } (d0 , d1 , . . . , d3t dummy candidatesused padding). 1 j 3t, let `j number sets contain bj .assumption, j, 1 j 3t, 1 `j 3. V consists followingvoters:weight211123 `jpreference ordersi > s0i >si > bi1 >si > bi2 >s0i > bi3 >p > d0 >bj > dj >1 n Si = {bi1 , bi2 , bi3 }1 j 3t `j < 3.Note score(si ) = 4, score(s0i ) = 3, score(bj ) = 3, score(p) = 2, score(dj ) 2.set k = n + 2t claim contains exact cover p becomewinner deleting n + 2t voters.(): Delete (n t) weight-2 voters corresponding sets coverdelete 3t weight-1 voters corresponding sets cover. score pchange, score si decreases 2, score s0i decreasesleast 1, score bj decreases 1. So, p winner.(): need delete 3t voters decrease score every bj candidate 1. (Notereason delete voters preference orders form bj > dj >(1 j 3t). suffices decrease score bj one and, since also needdecrease scores candidates si s0i (1 n), always better delete voterspreference orders form si > bj > s0i > bj > .) deleting 3tvoters, values i, 1 n, score si scores0i 2 (for i, obtaining score 2 candidates si s0i takesleast 3 unique voters 3t deleted ones).exactly values i, 1 n, score si score s0i2, values correspond cover. (Why so? considersituation already deleted 3t voters preference orders formssi > bj > s0i > bj > , bj Si . If, deleting voters,scores si s0i decreased 2, must deleted exactly three voterscorrespond members Si . Thus, deleting voters corresponding 3t membersB ensured values scores si s0i decreased 2,must case values correspond cover.) lessvalues i, 1 n, score si score s0i 2,remaining voters deleted, n them, need decreasescore si and/or s0i n values i, 1 n. possible,since voter approves si s0i sj s0j 6= j.Note construction uses weights 1 2. fact, establish NPcompleteness WCCDV 2-approval every set allowed weights size leasttwo (note set weights size one, problem P, since essenceunweighted case resolved Lin, 2012). Since reductions Theorem 4.12change set voter weights, result WCCAV 2-veto.525fiFaliszewski, Hemaspaandra, & HemaspaandraSo, suppose weight set contains w1 w2 , w2 > w1 > 0. modify construction follows. keep set candidates change votersfollows.#111121` `jweightw2w1w1w1w1w2w1preference ordersi > s0i >si > bi1 >si > bi2 >s0i > bi3 >p > d0 >p > d0 >bj > dj >1 n Si = {bi1 , bi2 , bi3 }w2 2w1w2 > 2w11 j 3t.Here, ` smallest integer `w1 > max(2w1 , w2 ). Note ` 3``j never negative. Note score(si ) = w2 +2w1 , score(s0i ) = w2 +w1 , score(bj ) = `w1 ,score(p) = max(2w1 , w2 ), score(dj ) max(2w1 , w2 ). argumentshows contains exact cover p become winner deletingn + 2t voters.turn proof WCCDV 3-veto. construction use weights1 3. Since reductions Theorem 4.12 change set voter weights, weights1 3 also suffice get NP-completeness WCCAV 3-approval. Given instanceX3C0 described above, construct following instance (C, V, p, k) WCCDV3-veto. set C = {p} B {si | 1 n} {r, d, d0 } (d d0 dummy candidatesused padding) V consists following voters:#11113n 3t3n 33n + 1 `jweight3111111preference order> p > si > r> p > si > bi1> p > si > bi2> p > si > bi3> > d0 > r> > d0 > si> > d0 > bj1 n Si = {bi1 , bi2 , bi3 }1 n1 j 3t.convenient count number vetoes candidate countnumber approvals. Note vetoes(si ) = 3n+3, vetoes(bj ) = 3n+1, vetoes(r) = 6n3t,vetoes(p) = 6n, vetoes(d) = vetoes(d0 ) 3n. claim contains exact coverp become winner (i.e., lowest number vetoes) deletingn + 2t voters.(): Delete (n t) weight-3 voters corresponding sets coverdelete 3t weight-1 voters veto p correspond sets cover.vetoes(si ) = vetoes(bj ) = vetoes(r) = vetoes(p) = 3n vetoes(d) = vetoes(d0 ) 3n. So,p winner.(): assume delete voters veto p. Suppose delete k1 weight1 voters k2 weight-3 voters, k1 +k2 n+2t. deletion, vetoes(p) = 6nk1 3k2 ,vetoes(r) = 6n 3t 3k2 , vetoes(bj ) 3n + 1. order p winner, needvetoes(p) vetoes(r). implies k1 3t. also need vetoes(p) vetoes(bj ) 0.526fiWeighted Electoral ControlSince vetoes(p) vetoes(bj ) 6n k1 3k2 (3n + 1) 6n (n + 2t k2 ) 3k2 3n 1 =2n2t2k2 1, follows k2 nt. (To see case, note requirevetoes(p) vetoes(bj ) 0 know vetoes(p) vetoes(bj ) 2n 2t 2k2 1,must require 2n2t2k2 1 0. expression equivalent k2 nt 21 .Since k2 , n, integers, must case k2 n t.) delete 3t weight-1votes n weight-3 votes, deleting voters vetoes(p) = 3n. orderp winner, delete one veto bj three vetoessi . implies set deleted weight-1 voters corresponds cover.4.3 Approximation Greedy Algorithmsproblems computationally difficult, NP-complete, natural wonder whether good polynomial-time approximation algorithms exist. So, motivated NP-completeness results discussed earlier paper casesWCCAV/WCCDV t-approval t-veto, section studies greedy approximation algorithms problems. (Recall WCCAV NP-complete t-approval,3, t-veto, 2, WCCDV NP-complete t-approval, 2,t-veto, 3.) First, establish connection weighted multicover problem,use obtain approximation results. obtain approximationalgorithm work direct action problem. Table 3 Section 6 summarizesresults approximation algorithms t-approval/t-veto WCCAV/WCCDV.undertake this, let us address detail issue, valuably raisedreferee, one might want build approximation algorithms control problems,might use algorithms, whether unwise obtain algorithmspeople using might good guys. mentioned above, seeking goodpolynomial-time approximation algorithms one standard approach exact solutionsknown intractable, e.g., NP-complete. algorithms allow campaignstrategist to, faced intractability computing optimal number votes adddelete achieve victory candidate, least able quickly find actionguaranteed within particular multiplicative factor optimal action. Onemight expect desire get approximations would hit wall regardingpotential impossibility exerting control certain instances, discussedElectoral Control subpart Section 3, worry hold particular problemsobtain approximation algorithms. Finally, worry people(chairs) employ approximation algorithms may good guys,following somewhat multilayered reply. First, good evil highly contextual.Whether strategists attempts help candidate win good evilmuch eye beholder. may decry attempts part brutal naturepolitics. Others may view attempts, long illegal actions taken,valid indeed valuable part spirited, vibrant playing field democracy. Second,settings, control may simply modeling optimization problem, wellapproximating control isnt even candidates, simply efficiency. Third,even one views approximating control helping evil-doers, using reasonlearn control problems approximated well approximatedmakes sense sticking ones head sand hoping cryptosystems527fiFaliszewski, Hemaspaandra, & Hemaspaandracant broken. Since evil-doers may well try build approximation algorithms, breakcryptosystems, natural way thwarting field richly exploreapproximations vulnerabilities exist, choose election systemuse given problem choose one weak respect goodapproximations whatever attacks fear.4.3.1 Weighted Multicover ApproachLet us first consider extent known algorithms Set-Cover familyproblems apply setting. Specifically, use following multicover problem.Definition 4.14. instance Weighted Multicover (WMC) consists set B ={b1 , . . . , bm }, sequence r = (r1 , . . . , rm ) nonnegative integers (covering requirements),collection = (S1 , . . . , Sn ) subsets B, sequence = (1 , . . . , n ) positive integers (weights sets S). goal findP minimum-cardinality set {1, . . . , n}, declare set exists.bj B holds rjiIbj Siis, given WMC instance seek smallest collection subsetssatisfies covering requirements elements B (keeping mind setweight covers elements times). WMC extension Set-Cover unitcosts. define problem known Covering Integer Programming (seeKolliopoulos & Young, 2005), short written CIP. However, problemquite important us here. reason observe WMC specialcase CIP (with multiplicity constraints but) without packing constraints; footnote 6effect describing embed problem problem. approximationalgorithm Kolliopoulos Young CIP (with multiplicity constraints but) withoutpacking constraints, applied special case WMC, gives following result.6Theorem 4.15 (Kolliopoulos & Young, 2005). polynomial-time algorithmgiven instance WMC set contains elements givesO(log t)-approximation.t-approval WCCAV WCCDV naturally translate equivalent WMCinstances. consider WCCAV first. Let (C, V, W, p, k) instance t-approval6. paper Kolliopoulos Young (2005) directly speak WMC problem, seeingresults indeed apply WMC easy, tedious, exercise. readers would likeverify Theorem 4.15 holds, footnote describe exactly paper KolliopoulosYoung one finds relevant result parameters one use. warn readerfootnote makes direct references parts paper make sense papersimultaneously hand. footnote merely guide understanding particular way drawpapers important work; providing full-fledged survey of, even real discussion of, CIPproblem beyond needs scope paper.Theorem 4.15 follows sentenceon page 496 work Kolliopoulos Young(2005)starting second algorithm finds solution (which follows Theorem 8),keeping mind none so-called packing constraints, may takecall one matrix vector call B b wont factor here. vectorcorresponds rj s; element jth row ith column matrix us setSi contains bj 0 otherwise; set cost vector c vector 1s; setmultiplicity vector vector 1s; vector x corresponds characteristic functionI; Theorem 4.15s bound number elements B contained Si .528fiWeighted Electoral ControlWCCAV, W = (w1 , . . . , wn ) collection voters may add. assumewithout loss generality voter W ranks p among top candidates (i.e.,approves p).form instance (B, r, S, ) WMC follows. set B = C {p}.c B, set covering requirement rc = score (C,V ) (c) score (C,V ) (p),j =def max(0, ij). vote w W , let Sw set candidates wapprove of. assumption regarding voter ranking p among top candidates,Sw contains p. set = (Sw1 , . . . , Swn ) set = ((w1 ), . . . , (wn )). easysee set {1, . . . , n} solution instance WMC (that is, satisfiescovering requirements) adding voters {wi | I} election (C, V )ensures p winner. reason following: add voter wielection candidate c Swi , difference score c scorep decreases (wi ), candidate c 6 Swi difference change.covering requirements set guarantee ps score match exceed scorescandidates election.stress construction assume constant. Indeed,construction applies t-veto well t-approval. using Theorem 4.15obtain following result.Theorem 4.16. polynomial-time O(log m)-approximation algorithm tapproval-WCCAV. polynomial-time algorithm given instancet-veto-WCCAV (t N) gives O(log t)-approximation.Proof. suffices use reduction t-approval/t-veto WMC apply algorithmTheorem 4.15. case t-approval, reduction guarantees setWMC instance contains elements. case t-veto, setscontains elements.obtain analogous results case t-approval/t-veto WCCDV. Oneeither provide direct reduction problems WMC notice reductionsgiven proof Theorem 4.12 maintain approximation properties.Theorem 4.17. polynomial-time algorithm given instance tapproval-WCCDV (t N) gives O(log t)-approximation. polynomial-timeO(log m)-approximation algorithm t-veto-WCCDV.4.3.2 Direct ApproachUsing algorithms WMC, able obtain relatively strong algorithmsWCCAV/WCCDV t-approval t-veto. However, approachfind approximation algorithms t-approval-WCCAV t-veto-WCCDV whose approximation ratios depend (and not, example, kCk, i.e., m, kV k).following seek direct algorithms problems.show simple greedy approach yields polynomial-time tapproximation algorithm t-approval-WCCAV t-veto-WCCDV. (Recallmeans cases making p win possible, number voters algorithmadds/deletes reach victory never times optimal set additions/deletions.)529fiFaliszewski, Hemaspaandra, & HemaspaandraLet GBW (greedy weight) define following simple algorithm WCCAV.(The votes weighted t-approval vectors induced preferences voters.)(Pre)discard unregistered votes approve preferred candidate p. Order(remaining) unregistered votes heaviest lightest, breaking ties voter weightssimple, transparent way (for concreteness, let us say lexicographic ordervotes representations). GBW goes unregistered votes order,reaches vote adds vote exactly vote disapproves least one candidatewhose score (i.e., total weight approvals) currently strictly greater p.stops successfully p become winner unsuccessfully happensalgorithm runs votes consider. following result says GBW tapproximation algorithm t-approval-WCCAV, also t-veto-WCCDV, usingobvious analogue GBW t-veto-WCCDV, also call GBW.7Theorem 4.18. Let 3. polynomial-time greedy algorithm GBW tapproximation algorithm t-approval-WCCAV t-veto-WCCDV; instances GBWs approximation factor problems better t.prove Theorem 4.18s upper lower bound parts separately, followingtwo lemmas theorem immediately follows.Lemma 4.19. Let 3. instances polynomial-time greedy algorithm GBW approximation factor t-approval-WCCAV better t.instances polynomial-time greedy algorithm GBW approximationfactor t-veto-WCCDV better t.Lemma 4.20. Let 3. polynomial-time greedy algorithm GBW t-approximationalgorithm t-approval-WCCAV t-veto-WCCDV.proof lower-bound claim, Lemma 4.19, consists somewhat detailed pairconstructions, less interest upper-bound part Theorem 4.18, namelyLemma 4.20. thus defer appendix proof Lemma 4.19.Proof Lemma 4.20. Let us prove two claims GBW t-approximationalgorithm. prove result = 3 WCCAV, immediately clearproof straightforwardly generalizes greater t; WCCDV case followsusing Theorem 4.12.Clearly GBW polynomial-time algorithm. Consider given input instance tapproval-WCCAV, preferred candidate p. Without loss generality, assume unregistered voters approve p. say candidate gap (under currentset registered voters whatever unregistered voters already added)candidate strictly weight approvals p does. candidate7. completeness clarity, describe mean GBW t-veto-WCCDV. Order votesapprove p heaviest lightest, breaking ties voter weights simple, transparent way (for concreteness, let us say lexicographic order votes representations). GBWgoes votes order, reaches vote removes vote exactlyvote approves least one candidate whose score (i.e., total weight approvals) currently strictlygreater p. stops successfully p become winner unsuccessfullyhappens algorithm runs votes consider.530fiWeighted Electoral Controlgap, 6= p, define id minimum number unregistered voters one addremove ds gap; is, one went heaviest lightest among unregistered voters,adding turn disapproved d, id number voters one would addlonger gap. candidate holds integer realizes id , controlimpossible using unregistered voter set. Clearly, successful addition voters mustadd least maxd id voters (the max throughout proof candidates initiallygap).Let us henceforth assume control possible input case. showadded 3 maxd id voters GBW made p winner, GBW3-approximation algorithm.giving detailed proof, let us informally give sense proofs idea.Let z candidate allegedly gap GBW added 3maxd id voters,freeze action GBW point. proof argues relatively many3 maxd id voters added GBW (i.e., least maxd id them) approve z, zclearly gap point time GBW frozen, assumedgap cant exist case. proof argues relatively 3 maxd idvoters added GBW (i.e., (maxd id )1 them) approve z (equivalently,least 1 + 2 maxd id approve z), also arrive contradiction.latter argument subtle one, involving asking candidates (call y) gapcaused last added vote added, needed drilling extra levelrelated few/many argument focused y, show GBW must pointacted way violates definition, thus also yielding contradiction. Sincemany cases cover possible cases, proof achievedgoal. provide formal analysis carries argument line.So, suppose 3 maxd id additions candidate, z, still gap.discussed perform case analysis case arrive contradiction.Case 1 [In least maxd id first 3maxd id votes added GBW, z approved].Since last one added z must still gap addition,earlier vote considered disapproved z gap z consideredwould added reached. So, keeping mind iz maxd id ,fact must added iz heaviest voters disapproving z, contraryassumption, z longer gap additions.Case 2 [Case 1 hold]. z approved least 1 + 2 maxd id addedvotes. made final one added votes, call v 0 , eligible addition? mustcandidate, say y, still gap v 0 added.Case 2a [y disapproved least maxd id 2 maxd id votes added v 0approved z]. Then, since ys gap removed unregistered voters disapprovingwould excluded GBW, ys iy heaviest voters added. contraryCase 2s assumption, gap get adding vote v 0 .Case 2b [Case 2 holds Case 2a not]. approved least 1 + maxd id2 maxd id votes v 0 GBW added approve z. 1 + maxd idvotes added approving exactly z y. made last 1 + maxd idvotes, call v 00 , eligible added? must hold candidate w gapv 00 . moment adding v 00 would added maxd id iw votesapproving exactly z disapproving w, since w allegedly still gap,531fiFaliszewski, Hemaspaandra, & HemaspaandraGBW would fact added iw heaviest voters disapprovingw, ws gap would removed v 00 , contrary assumption wgap made v 00 eligible.One might naturally wonder GBW performs t-veto-WCCAV t-approvalWCCDV. argument far easier used proof Lemma 4.20,cases GBW provides t-approximation algorithm.Theorem 4.21. GBW t-approximation algorithm t-veto-WCCAV. GBW tapproximation algorithm t-approval-WCCDV.Proof. Consider t-veto-WCCAV. Let p preferred candidate. candidateinitial positive gap relative preferred candidate p (i.e., surplus ptotal weight approvals), let id defined proof Lemma 4.20. (Recall idnumber votes would need add remove surplus p tookunregistered votes, discarded didnt simultaneously approve p disapproved,P added one time heaviest lightest gap removed.)Clearly,id , sum taken candidates initial surplus relativep, upper bound number votes added GBW. true since GBWworks adding extra votes heaviest lightest, restricted vetoing candidatepoint positive gap relative p; GBW gap closedlargest weight votes address it. hand, overall optimal solutionid lower bound smallest number votes solutions added-vote setwould suffice remove ds positive gap (since takes id even use heaviestvotes addressing gap). overall optimal solution added vote narrowsgaps. GBWs solution uses worst times many added votes optimalsolution.claim t-approval-WCCDV follows Theorem 4.12.result replaces flawed claim conference version paper (Faliszewski,Hemaspaandra, & Hemaspaandra, 2013) GBW cousins provideO(1) approximations problems.8 course, t-approximation twoproblems (namely, t-veto-WCCAV t-approval-WCCDV) wildly exciting, sinceproblems multicover-based approach earlier section showedfunction f (t), f (t) = O(log t), even f (t)-approximation algorithmsproblems. However, constant big oh algorithm large,possible sufficiently small values approach may give betterapproximation. Also, feel interesting learn behavior explicitheuristics, especially attractive approaches greedy algorithms.natural ask whether similar greedy algorithms work well scoring rules,e.g., Bordas rule. Unfortunately, families scoring rules t-approvalt-veto analysis, possible, would likely significantly differentours. main reason thatas discussed Electoral Control subpart8. Note treat constant and, so, t-approximation algorithm provides (indeed, is)O(1) approximate one. reason true that, technically speaking, WCCAV WCCDVproblems defined separately voting rule. example, 2-approval-WCCAV differentproblem than, say, 200-approval-WCCAV.532fiWeighted Electoral ControlSection 3for t-approval t-veto always easy verify whether existssolution (although, perhaps, one far optimal). scoringrules, e.g., Borda, clear whether possible (and conjecture that,indeed, NP-complete so). However, might interesting research directionevaluate effectiveness greedy algorithms empirically (we point readerwork Rothe & Schend, 2013, recent survey covering experimental studiescomplexity control elections).5. Related Workstudy complexity (unweighted) electoral control initiatedBartholdi, Tovey, Trick (1992), considered constructive controladding/deleting/partitioning candidates/voters plurality rule Condorcet rule (that is, rule chooses Condorcet winner whenever one,winners otherwise). various types control model least flavoractions occur real world, voter suppression targeted get-out-the-votedrives (see survey Faliszewski et al., 2010, examples discussions).major motivation study control obtain complexity barrier results, is,results show detecting opportunities various control attacks computationallydifficult. particular, Bartholdi, Tovey, Trick focused NP-hardness measurecomputational difficulty.research direction continued Hemaspaandra, Hemaspaandra,Rothe (2007), first study destructive control attacks elections. Sincethen, many authors studied electoral control many varied settings manydifferent rules; refer reader survey Faliszewski et al. (2010). recentresearch, covered survey, includes complexity-of-control results t-approvalfamily rules (Lin, 2012), Bucklins rule (and fallback, extension truncatedvotes; Erdelyi, Fellows, Rothe, & Schend, 2015a), maximin (Faliszewski et al., 2011),range voting (Menton, 2013), Schultzes rule ranked pairs rule (Parkes& Xia, 2012; Menton & Singh, 2013; Hemaspaandra, Lavaee, & Menton, 2013).present paper, compare control manipulation. recent paper Fitzsimmons,Hemaspaandra, Hemaspaandra (2013) studies settings control manipulation occurring. Researchers have, quite different setting electing membersfill fixed-size, multimember panel, defined variants control coexisting constructive destructive aspects (Meir, Procaccia, Rosenschein, & Zohar, 2008).also work analyzing counting variants control (Wojtas & Faliszewski, 2012),goal decide given control attack possible, also count numberways attack carried out.complexity-barrier research line turned successful. votingrules considered, significant number control attacks NP-hard. Indeed,even possible construct artificial election system resistant types controlattacks (Hemaspaandra, Hemaspaandra, & Rothe, 2009). However, also numberresults suggest practice complexity barrier might strong onemight first think. example, Faliszewski, Hemaspaandra, Hemaspaandra, Rothe(2011) Brandt, Brill, Hemaspaandra, Hemaspaandra (2010) shown533fiFaliszewski, Hemaspaandra, & Hemaspaandravotes restricted single-peaked, many control problems knownNP-complete become polynomial-time solvable. Indeed, often holds even electionsnearly single-peaked (Faliszewski et al., 2014), many real-world elections seem(see, e.g., discussion Gehrlein & Lepelley, 2012, ch. 2). Similarly, initialexperimental results Erdelyi, Fellows, Rothe, Schend (2015b) suggest that, leastcertain distributions settings, NP-hard control problems solvedpractice many instances. part different line research, Xia (2012) studiedasymptotic behavior number voters added to/deletedrandomly constructed election successful control action.number problems involving changing structure elections.problems include candidate cloning, possible replace given candidatec number clones (Elkind et al., 2011; Elkind, Faliszewski, & Slinko, 2012),possible winner problem new alternatives join, additional, yet rankedcandidates introduced (Chevaleyre, Lang, Maudet, Monnot, & Xia, 2012; Xia, Lang,& Monnot, 2011). last problem also related possible winner problemtruncated ballots (Baumeister, Faliszewski, Lang, & Rothe, 2012a).papers directly raise issue weighted control are, bestknowledge, theses Russell (2007) Lin (2012). However, also mentionpapers Baumeister, Roos, Rothe, Schend, Xia (2012b), Perek, Faliszewski,Pini, Rossi (2013), authors, effect, consider problems affecting resultelection picking weights voters. (The paper Perek et al. motivatesstudy differently, effect studies constrained variant choosing voter weights.)problems similar to, though different from, simultaneous (multimode) additiondeletion voters (Faliszewski et al., 2011).paper given f ()-approximation results weighted election control problems. Elkind Faliszewski (2010) given 2-approximation algorithm weighted,bribery-related case.6. Conclusionsstudied voter control number voting rules, including scoring protocols,families scoring protocols, (weak)Condorcet-consistent rules. showncomplexity voter control quite different complexity weightedcoalitional manipulation: natural voting rules weighted coalitional manipulation easy weighted voter control hard, natural rulesopposite case. Furthermore, shown weighted voter controlt-approval t-veto, good, natural approximation algorithms. resultsvoter control weighted elections summarized Tables 1, 2, 3.Acknowledgementsgrateful anonymous AAMAS 2013 JAIR referees extremelyhelpful comments suggestions, incorporated examples.thank editor, Jerome Lang, wise guidance.work sup534fiWeighted Electoral ControlWCCAVWCCDVWCMPluralityP (Thm. 4.1)P (Thm. 4.1)Pt-approval, 2 <BordaP (Thm. 4.1)P (Thm. 4.1)NP-comp.NP-comp. (Thm. 4.2)NP-comp. (Thm. 4.2)NP-comp.= (1 , . . . , ),k{1 , . . . , }k 3NP-comp. (Thm. 4.3)NP-comp. (Thm. 4.3)NP-comp.Llull (3 candidates)NP-comp. (Cor. 4.9)NP-comp. (Cor. 4.9)P(weak)Condorcetconsistent rulesNP-hard (Thm. 4.7)NP-hard (Thm. 4.7)variouscomplexitiesTable 1: results complexity control adding/deleting voters weightedelections fixed number candidates, 3, compared complexity weighted coalitional manipulation. result marked dueConitzer et al. (2007), results marked due HemaspaandraHemaspaandra (2007), result marked due Faliszewski et al.(2008).WCCAVWCCDVt-approvalt=2t=3t4P (Thm. 4.10)NP-complete (Thm. 4.13)NP-completeNP-complete (Thm. 4.13)NP-completeNP-completet-vetot=2t=3t4NP-complete (Thm. 4.13)NP-completeNP-completeP (Thm. 4.11)NP-complete (Thm. 4.13)NP-completeTable 2: complexity control adding deleting voters t-approval t-vetounbounded number candidates. results marked dueLin (2012).ported part grants AGH-11.11.230.124, NCN-DEC-2011/03/B/ST6/01393, NCNUMO-2012/06/M/ST1/00358, NSF-CCF-{0915792,1101452,1101479}, two BesselAwards Alexander von Humboldt Foundation.535fiFaliszewski, Hemaspaandra, & HemaspaandraWCCAVWCCDVt-approvalO(log m) (Thm. 4.16)(Thm. 4.18)O(log t) (Thm. 4.17)(Thm. 4.21)t-vetoO(log t) (Thm. 4.16)(Thm. 4.21)O(log m) (Thm. 4.17)(Thm. 4.18)Table 3: Approximation ratios algorithms WCCAV WCCDV tapproval t-veto.Appendix A. Additional Details Related Section 4.3present deferred proof Lemma 4.19 details related Section 4.3.Proof Lemma 4.19. goal show GBW sometimes really use fullytimes optimal number added/deleted votes, cases question. Examples(somewhat detailed but) hard construct, lower bound even holds = 2,though Section 4.2 obtained exact solution different approach. However, onecareful set gap pattern created unregistered votersrealizable one. t-approval-WCCAV construction, easy directly.t-veto-WCCDV construction, establish realizability small toolhope may prove useful elsewherethat lets one set certain patterns gaps.state tool Tool A.1.Fix {2, 3, 4, . . .}. construct instance t-approval-WCCAVGBW uses times many additions optimal strategy. construction2t candidates: preferred candidate p, candidates a1 , . . . , , candidatesd1 , . . . , dt1 . Now, suppose votes registered voters, gapsfollows. candidate ai , total weight approvals ai exceeds total weightapprovals p exactly 2t. candidate di , total weight approvalsdi equals total weight approvals p. easily realized, namelyregistered voter set one weight-2t voter approves ai .set unregistered voters follows. one unregistered voter,call nice, weight 2t, approves p 1 candidates di ,disapproves candidates ai . j, 1 j t, singleunregistered voter, call j , weight 3t, approves p ai aj ,disapproves aj di s.Note GBW add voters . ideal would add single voter callednice, since suffices make p winner. 2 constructedsetting GBW t-approval-WCCAV takes times optimal numberadded votes.also holds 2, similarly construct setting GBWt-veto-WCCDV takes times optimal number deleted votes, provesetting realizable. fact, following something flavor536fiWeighted Electoral Controlscheme, except slightly different vote set adjusts handle casedeleting voters, care regarding realizability. construction. Fix{2, 3, 4, . . .}. candidate set preferred candidate p, candidatesa1 , . . . , , candidates d1 , . . . , dt1 . Let us specify voter set. putvoter set collection weight-1 votes gaps total approval weight relatived1 created votes follows. d2 dt1 totalapproval weight d1 . total approval weight p exceeds d1 3t2 + 3t.total approval weight ai exceeds d1 3t2 .Tool A.1 below, observe 2t-candidate t-approval voting, gappattern gaps multiples realized. Since current proofusing 2t-candidate t-veto, 2t-candidate t-approval, Tool A.1applies here. particular, Tool A.1 easily builds set weight-1 votes realizing preciselydesired set gaps. (The exact number weight-1 votes used constructionimportant. However, gaps mentioned vote-set size mentionedtool, precise number easily seen (3t + 3 + t(3t))(2t 1).)yet done building voter set. also voter set onevoter, call nice, weight 2t, approves exactly ai s. j,1 j t, one voter weight 3t approves exactly aj 1di s.entire set votes created abovethe votes tool combinednice votes mentionedit easy see d1 candidateleast total approval weight, tied total approval weight di .total approval weight p exceeds d1 3t. ai exceeds d1 totalapproval weight 5t.However, light pattern votes weights here, clear GBW(in version t-veto) delete weight-3t voters. (Note votes addedTool A.1 weight-1 votes, highly unattractive GBW.) ideal woulddelete single voter called nice, since suffices make p winner.2 constructed realizable setting GBW t-veto-WCCDV takestimes optimal number deleted votes.Within proof, referred used small tool build certainpatterns vote weight gaps certain approval elections. would overreachclaim McGarvey-like tool, since different setting than, far lessflexible result than, famous theorem McGarvey (1953). However, small waytool perhaps might useful elsewhere, state prove modest tool.Tool A.1. Let 2. Let n1 , . . . , n2t1 list nonnegative integers divisible t.exists collection t-approval votes, 2t candidates,votes, relative candidate getting fewest approvals, list gaps numberapprovals candidate otherP2t 1 candidates precisely (n1 , . . . , n2t1 ).Furthermore, done (2t 1)( ni )/t unweighted (i.e., weight 1) votes.alternatively done (2t 1)2 weighted votes (or even (2t 1)k{i | ni 6= 0}kweighted votes).Proof. Consider election 2t candidates, votes cast t-approval votes.Consider collection 2t 1 votes, weight one, votes approve537fiFaliszewski, Hemaspaandra, & Hemaspaandraparticular candidate (for example, let one first candidate),remaining 1 approvals cyclically rotate around candidates. t-approvalvotes, viewed bit vectors, these: 1 1t1 0t , 1 0 1t1 0t1 , . . ., 1 0t 1t1 , 1 1 0t 1t2 , . . .,1 1t1 0t 1. Note first candidate approved 2t 1 votes,candidate approved exactly 1 votes. collection votes setsgap favor first candidate, total approval weight firstcandidate candidate difference total approval weightpair candidates zero.Given gap pattern stated tool, gap least-approvedcandidate (call candidate c) multiple t, simply use approachparagraph repeatedly, boost candidate, d, one time whatevermultiple supposed exceed c total approval weight. (In this, playrole first candidate previous paragraph.) ds surplus relative c ktwish use weight-1 votes, k(2t 1) weight-1 votes.Otherwise, 2t 1 weight-k votes. total number votesused given statement tool.appendix seeking provide comprehensive study gap collectionsrealizable t-approval voting, seeking find smallest number votersneeded realize realizable gap collection. interesting direction study,goal here. However, mention clearly exist gap collectionscannot realized. example, exists claim Tool A.1 evenalways true one removes assumption divisibility t. example showingfollowing. Consider 4-candidate setting votes 2-approval votes,desire gap list relative least-approved candidate (1, 1, 1), i.e.,candidates one approval least-approved candidate. Clearly,total number approvals set votes achieving 4B + 3, B whatevernumber approvals least-approved candidate happens get vote set onetrying, total number approvals odd. However, vote set 2-approvalvotes even total number approvals. gap collection cannot realized.ReferencesAziz, H., Bachrach, Y., Elkind, E., & Paterson, M. (2011). False-name manipulationsweighted voting games. Journal Artificial Intelligence Research, 40, 5793.Bartholdi, III, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. SocialChoice Welfare, 8 (4), 341354.Bartholdi, III, J., Tovey, C., & Trick, M. (1989). computational difficulty manipulating election. Social Choice Welfare, 6 (3), 227241.Bartholdi, III, J., Tovey, C., & Trick, M. (1992). hard control election?.Mathematical Computer Modeling, 16 (8/9), 2740.Baumeister, D., Faliszewski, P., Lang, J., & Rothe, J. (2012a). Campaigns lazy voters:Truncated ballots. Proceedings 11th International Conference AutonomousAgents Multiagent Systems, pp. 577584.538fiWeighted Electoral ControlBaumeister, D., Roos, M., Rothe, J., Schend, L., & Xia, L. (2012b). possible winnerproblem uncertain weights. Proceedings 20th European ConferenceArtificial Intelligence, pp. 133138.Brandt, F., Brill, M., Hemaspaandra, E., & Hemaspaandra, L. (2010). Bypassing combinatorial protections: Polynomial-time algorithms single-peaked electorates.Proceedings 24th AAAI Conference Artificial Intelligence, pp. 715722.Brandt, F., Conitzer, V., & Endriss, U. (2013). Computational social choice. Wei, G.(Ed.), Multiagent Systems (2nd edition). MIT Press.Brelsford, E., Faliszewski, P., Hemaspaandra, E., Schnoor, H., & Schnoor, I. (2008). Approximability manipulating elections. Proceedings 23rd AAAI ConferenceArtificial Intelligence, pp. 4449. AAAI Press.Chen, J., Faliszewski, P., Niedermeier, R., & Talmon, N. (2014). Combinatorial voter controlelections. Proceedings 39th International Symposium MathematicalFoundations Computer Science, Part II, pp. 153164. Springer-Verlag Lecture NotesComputer Science #8635.Chevaleyre, Y., Lang, J., Maudet, N., Monnot, J., & Xia, L. (2012). New candidates welcome! Possible winners respect addition new candidates. MathematicalSocial Sciences, 64 (1), 7488.Congleton, R. (2011). Swedish transition democracy (Chapter 14). PerfectingParliament. Cambridge University Press.Conitzer, V., Sandholm, T., & Lang, J. (2007). elections candidateshard manipulate?. Journal ACM, 54 (3), Article 14.Dwork, C., Kumar, R., Naor, M., & Sivakumar, D. (2001). Rank aggregation methodsweb. Proceedings 10th International World Wide Web Conference, pp.613622. ACM Press.Elkind, E., & Faliszewski, P. (2010). Approximation algorithms campaign management. Proceedings 6th International Workshop Internet NetworkEconomics, pp. 473482.Elkind, E., Faliszewski, P., & Slinko, A. (2011). Cloning elections: Finding possiblewinners. Journal Artificial Intelligence Research, 42, 529573.Elkind, E., Faliszewski, P., & Slinko, A. (2012). Clone structures voters preferences.Proceedings 13th ACM Conference Electronic Commerce, pp. 496513.Ephrati, E., & Rosenschein, J. (1997). heuristic technique multi-agent planning.Annals Mathematics Artificial Intelligence, 20 (14), 1367.Erdelyi, G., Fellows, M., Rothe, J., & Schend, L. (2015a). Control complexity Bucklinfallback voting: theoretical analysis. Journal Computer System Sciences,81 (4), 632660.Erdelyi, G., Fellows, M., Rothe, J., & Schend, L. (2015b). Control complexity Bucklinfallback voting: experimental analysis. Journal Computer System Sciences,81 (4), 661670.539fiFaliszewski, Hemaspaandra, & HemaspaandraFaliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2009). hard briberyelections?. Journal Artificial Intelligence Research, 35, 485532.Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2010). Using complexity protectelections. Communications ACM, 53 (11), 7482.Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2011). Multimode attackselections. Journal Artificial Intelligence Research, 40, 305351.Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2013). Weighted electoral control. Proceedings 12th International Conference Autonomous AgentsMultiagent Systems, pp. 367374.Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2014). complexity manipulative attacks nearly single-peaked electorates. Artificial Intelligence, 207, 6999.Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). richer understanding complexity election systems. Ravi, S., & Shukla, S. (Eds.), Fundamental Problems Computing: Essays Honor Professor Daniel J. Rosenkrantz,pp. 375406. Springer.Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2011). shieldnever was: Societies single-peaked preferences open manipulationcontrol. Information Computation, 209 (2), 89107.Faliszewski, P., Hemaspaandra, E., & Schnoor, H. (2008). Copeland voting: Ties matter.Proceedings 7th International Conference Autonomous Agents Multiagent Systems, pp. 983990. International Foundation Autonomous AgentsMultiagent Systems.Fitzsimmons, Z., Hemaspaandra, E., & Hemaspaandra, L. (2013). Control presence manipulators: Cooperative competitive cases. Proceedings 23rdInternational Joint Conference Artificial Intelligence, pp. 113119. AAAI Press.Garey, M., & Johnson, D. (1979). Computers Intractability: Guide TheoryNP-Completeness. W. H. Freeman Company.Gehrlein, W., & Lepelley, D. (2012). Voting Paradoxes Group Coherence: CondorcetEfficiency Voting Rules. Springer.Ghosh, S., Mundhe, M., Hernandez, K., & Sen, S. (1999). Voting movies: anatomyrecommender systems. Proceedings 3rd Annual Conference AutonomousAgents, pp. 434435. ACM Press.Hemaspaandra, E., & Hemaspaandra, L. (2007). Dichotomy voting systems. JournalComputer System Sciences, 73 (1), 7383.Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Anyone him: complexityprecluding alternative. Artificial Intelligence, 171 (56), 255285.Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). Hybrid elections broadencomplexity-theoretic resistance control. Mathematical Logic Quarterly, 55 (4), 397424.540fiWeighted Electoral ControlHemaspaandra, E., Hemaspaandra, L., & Schnoor, H. (2014). control dichotomy purescoring rules. Proceedings 28th AAAI Conference Artificial Intelligence,pp. 712720. AAAI Press.Hemaspaandra, L., Lavaee, R., & Menton, C. (2013). Schulze ranked-pairs votingfixed-parameter tractable bribe, manipulate, control. Proceedings12th International Conference Autonomous Agents Multiagent Systems, pp.13451346.Hemaspaandra, L., & Williams, R. (2012). atypical survey typical-case heuristicalgorithms. SIGACT News, 43 (4), 7189.Kolliopoulos, S., & Young, N. (2005). Approximation algorithms covering/packing integer programs. Journal Computer System Sciences, 71 (4), 495505.Lin, A. (2012). Solving Hard Problems Election Systems. Ph.D. thesis, Rochester InstituteTechnology, Rochester, NY.Lu, T., & Boutilier, C. (2011). Budgeted social choice: consensus personalized decision making. Proceedings 22nd International Joint Conference ArtificialIntelligence, pp. 280286.McGarvey, D. (1953). theorem construction voting paradoxes. Econometrica,21 (4), 608610.Meir, R., Procaccia, A., Rosenschein, J., & Zohar, A. (2008). complexity strategicbehavior multi-winner elections. Journal Artificial Intelligence Research, 33,149178.Menton, C. (2013). Normalized range voting broadly resists control. Theory ComputingSystems, 53 (4), 507531.Menton, C., & Singh, P. (2013). Control complexity Schulze voting. Proceedings23rd International Joint Conference Artificial Intelligence, pp. 286292.Parkes, D., & Xia, L. (2012). complexity-of-strategic-behavior comparisonSchulzes rule ranked pairs. Proceedings 26th AAAI ConferenceArtificial Intelligence, pp. 14291435.Perek, T., Faliszewski, P., Pini, M., & Rossi, F. (2013). complexity losing voters. Proceedings 12th International Conference Autonomous AgentsMultiagent Systems, pp. 407414.Rothe, J., & Schend, L. (2013). Challenges complexity shields supposedprotect elections manipulation control: survey. Annals MathematicsArtificial Intelligence, 68 (13), 161193.Russell, N. (2007). Complexity control Borda count elections.Rochester Institute Technology.Masters thesis,Waggoner, B., Xia, L., & Conitzer, V. (2012). Evaluating resistance false-name manipulations elections. Proceedings 26th AAAI Conference ArtificialIntelligence, pp. 14851491.Wagman, L., & Conitzer, V. (2014). False-name-proof voting costs two alternatives. International Journal Game Theory, 43 (3), 599618.541fiFaliszewski, Hemaspaandra, & HemaspaandraWojtas, K., & Faliszewski, P. (2012). Possible winners noisy elections. Proceedings26th AAAI Conference Artificial Intelligence, pp. 14991505.Xia, L. (2012). many vote operations needed manipulate voting system?.Proceedings (Workshop Notes) 4th International Workshop ComputationalSocial Choice, pp. 443454.Xia, L., Lang, J., & Monnot, J. (2011). Possible winners new alternatives join:New results coming up!. Proceedings 10th International Conference Autonomous Agents Multiagent Systems, pp. 829836. International FoundationAutonomous Agents Multiagent Systems.542fiJournal Artificial Intelligence Research 52 (2015) 361-398Submitted 7/14; published 3/15Inferring Team Task Plans Human Meetings:Generative Modeling Approach Logic-Based PriorKimCaleb M. ChachaJulie A. Shahbeenkim@csail.mit.educ chacha@csail.mit.edujulie shah@csail.mit.eduMassachusetts Institute Technology77 Massachusetts Ave. 02139, USAAbstractaim reduce burden programming deploying autonomous systemswork concert people time-critical domains military field operationsdisaster response. Deployment plans operations frequently negotiated on-thefly teams human planners. human operator translates agreed-upon planmachine instructions robots. present algorithm reduces translation burden inferring final plan processed form human teams planningconversation. hybrid approach combines probabilistic generative modeling logicalplan validation used compute highly structured prior possible plans, enabling usovercome challenge performing inference large solution spacesmall amount noisy data team planning session. validate algorithmhuman subject experimentations show able infer human teamsfinal plan 86% accuracy average. also describe robot demonstrationtwo people plan execute first-response collaborative task PR2 robot.best knowledge, first work integrate logical planning technique withingenerative model perform plan inference.1. IntroductionRobots increasingly introduced work concert people high-intensitydomains military field operations disaster response. example, robot deployment allow access areas would otherwise inaccessible people (Casper& Murphy, 2003; Micire, 2002), inform situation assessment (Larochelle, Kruijff, Smets,Mioch, & Groenewegen, 2011). human-robot interface long identified major bottleneck utilizing robotic systems full potential (Murphy, 2004).result, significant research efforts aimed easing use systemsfield, including careful design validation supervisory control interfaces (Jones,Rock, Burns, & Morris, 2002; Cummings, Brzezinski, & Lee, 2007; Barnes, Chen, Jentsch,& Redden, 2011; Goodrich, Morse, Engh, Cooper, & Adams, 2009). Much priorwork focused ease use execution time. However, significant bottleneck alsoexists planning deployment autonomous systems programmingsystems coordinate task execution human team. Deployment plans frequentlynegotiated human team members on-the-fly time pressure (Casper & Murphy,2002, 2003). robot aid execution plan, human operator musttranscribe translate result team planning session.c2015AI Access Foundation. rights reserved.fiKim, Chacha & Shahpaper, present algorithm reduces translation burden inferringfinal plan processed form human teams planning conversation. Inferringplan noisy incomplete observation formulated plan recognitionproblem (Ryall, Marks, & Shieber, 1997; Bauer, Biundo, Dengler, Koehler, & Paul, 2011;Mayfield, 1992; Charniak & Goldman, 1993; Carberry, 1990; Grosz & Sidner, 1990; Gal,Reddy, Shieber, Rubin, & Grosz, 2012). noisy incomplete characteristics observation stem fact observed data (e.g., teams planning conversation)part plan trying infer, entire plan mayobserved. focus existing plan recognition algorithms often search existingknowledge base given noisy observation. However, deployment plans emergency situations seldom same, making infeasible build knowledge base. addition,planning conversations often conducted time pressure are, therefore, oftenshort (i.e., contain small amount data). Shorter conversations result limited amountavailable data inference, often making inference problem challenging.approach combines probabilistic generative modeling logical plan validation,used compute highly structured prior possible plans. hybrid approachenables us overcome challenge performing inference large solution spacesmall amount noisy data collected team planning session.work, focus inferring final plan using text data loggedchat transcribed speech. Processing human dialogue machine-understandableforms important research area (Kruijff, Jancek, & Lison, 2010; Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, & Roy, 2011; Koomen, Punyakanok, Roth, & Yih, 2005;Palmer, Gildea, & Xue, 2010; Pradhan, Ward, Hacioglu, Martin, & Jurafsky, 2004),view separate problem focus paper.form input use preserves many challenging aspects natural humanplanning conversations, thought noisy observation final plan.team discussing plan time pressure, planning sessions often consistsmall number succinct communications. approach infer final agreed-uponplan using single planning session, despite small amount noisy data.validate algorithm experiments 96 human subjects showable infer human teams final plan 86% accuracy average. bestknowledge, first work integrate logical planning technique withingenerative model perform plan inference.summary, work includes following contributions:formulate novel problem performing inference extract finally agreedupon plan human team planning conversation.propose validate hybrid approach perform inference applieslogic-based prior probability space possible agreed-upon plans.approach performs efficient inference probabilistic generative model.demonstrate benefit approach using human team meeting data collectedlarge-scale human subject experiments (total 96 subjects) able inferhuman teams final plan 86% accuracy average.362fiA Generative Modeling Approach Logic-Based Priorwork extends preliminary version work (Kim, Chacha, & Shah, 2013)include inference complex durative task plans infer plans newinformation becomes available human team. addition, extend probabilisticmodel flexible different data sets learning hyper-parameters. alsoimprove performance algorithm designing better proposal distributioninference.formulation problem presented Section 2, followed technicalapproach related work Section 3. algorithm described Section 4.evaluation algorithm using various data sets shown Sections 5 6. Finally,discuss benefits limitations current approach Section 7, concludeconsiderations future work Section 8.2. Problem FormulationDisaster response teams increasingly utilizing web-based planning tools plan deployments (Di Ciaccio, Pullen, & Breimyer, 2011). Hundreds responders access toolsdevelop plans using audio/video conferencing, text chat annotatable maps.Rather working raw, natural language, algorithm takes structured formhuman dialogue web-based planning tools input. goal workinfer human teams final plan human dialogue. so, workused design intelligent agent planning tools actively participateplanning session improve teams decision.section describes formal definition, input output problem. Formally,problem viewed one plan recognition, wherein plan follows formalrepresentation Planning Domain Description Language (PDDL). PDDLwidely used planning research community planning competitions (i.e.,International Planning Competition). plan valid achieves user-specified goal statewithout violating user-specified plan constraints. Actions may constrained executesequence parallel actions. plan constraints include discreteresource constraints (e.g. presence two medical teams) temporal deadlinestime-durative actions (e.g. robot deployed 1 hour time duebattery life constraints).assume team reaches agreement final plan. techniques introducedKim Shah (2014) used detect strength agreement,encourage team discuss plan reach agreement necessary. Situationsteam agrees upon flexible plan multiple options exploredincluded future study. Also, assume team likely agreevalid plan, rule possibility final plan invalid.2.1 Algorithm InputText data human team conversation collected form utterances,utterance one persons turn discussion, shown Table 1. inputalgorithm machine-understandable form human conversation data, illustratedright-hand column Table 1. structured form captures actions discussedproposed ordering relations among actions utterance.363fiKim, Chacha & ShahNatural dialogueU1U2U3U4U5U6U7suggest using Red robot cover upperrooms (A, B, C, D) Blue robot coverlower rooms (E, F, G, H).Okay. first send Red robot B Bluerobot G?order inspection would (B, C, D, A)Red (G, F, E, H) Blue.Oops meant (B, D, C, A) Red.medical crew go Brobot inspecting CFirst, Red robot inspects BYes, Red robot inspects D, Red medical crew treat BStructured form (ordered tuplesets grounded predicates)({ST(rr,A),ST(br,E),ST(rr,B),ST(br,F),ST(rr,C),ST(br,G),ST(rr,D),ST(br,H)})({ST(rr,B),ST(br,G)})({ST(rr,B),ST(br,G)},{ST(rr,C),ST(br,F)},{ST(rr,D),ST(br,E)},{ST(rr,A), ST(br,H)})({ST(rr,B)},{ST(rr,D)},{ST(rr,C)},{ST(rr,A)})({ST(m,B), ST(r,C)})({ST(r,B)})({ST(r,D),ST(rm,B)})Table 1: Utterance tagging: Dialogue structured form examples. (The structured formuses following shorthand - ST: send to, rr: red robot, br: blue robot, rm: red medical,bm: blue medical, e.g. ST(br,A) : send blue robot room A.)Although working raw, natural language, form data still captures many characteristics make plan inference based human conversationchallenging. Table 1 shows part data using following shorthand:ST = SendTorr = red robot, br = blue robotrm = red medical, bm = blue medicale.g. ST(br, A) = SendTo(blue robot, room A)2.1.1 Utterance Taggingutterance tagged ordered tuple sets grounded predicates. Followingformal definition first-order languages, grounded predicate atomic formula whoseargument terms grounded (i.e., free variables; variables assigned value).case, predicate represents action applied set objects (a crew member,robot, room, etc.), utterance represented ordered sets actions.consider utterances related plan formation; greetings jokes, example,tagged. set grounded predicates represents collection actions that, accordingutterance, happen simultaneously. order sets grounded predicatesindicates relative order collections actions happen. example,364fiA Generative Modeling Approach Logic-Based Prior({ST(rr, B), ST(br, G)}, {ST(rm, B)}) corresponds sending red robot room Bblue robot room G simultaneously, followed sending red medical teamroom B.indicated Table 1, structured dialogue still includes high levels noise.utterance (i.e. U1-U7) discusses partial plan, predicates explicitly mentionedutterance tagged (e.g. U6-U7: U7 implies sequencing constraintpredicate discussed U6, structured form U7 include ST(r,B)).Typos misinformation tagged without correction (e.g. U3), utterancesindicating need revise information placed context (e.g. U4). Utterancesclearly violate ordering constraints (e.g. U1: actions cannot happentime) also tagged without correction. addition, information regarding whetherutterance suggestion, rejection agreement partial plan coded.Note utterance tagging contains information relative ordering predicates appearing utterance, absolute ordering appearance final plan. example, U2 specifies two grounded predicates happentime. state two predicates happen final plan,whether predicates happen parallel. simulates humans conversationsoften unfold utterance, humans observe relative ordering, inferabsolute order predicates based whole conversation understandingorderings would make valid plan. utterance tagging scheme also designedsupport future transition automatic natural language processing. Automatic semantic role labeling (Jurafsky & Martin, 2000), example, used detectarguments predicates sentences. One challenges incorporating semanticrole labeling system dialogue experiments often colloquialkey grammatical components sentences often omitted. Solving problemprocessing free-form human dialogue machine-understandable forms important research area, view separate problem focuspaper.2.2 Algorithm Outputoutput algorithm inferred final plan, sampled probability distribution final plans. final plan representation structuredutterance tags (ordered tuple sets grounded predicates). predicates setrepresent actions happen parallel, ordering sets indicates sequence. Unlike utterance tags, however, sequence ordering relations finalplan represent absolute order actions carried out. exampleplan ({A1 , A2 }, {A3 }, {A4 , A5 , A6 }), Ai represents predicate. plan, A1A2 happen step 1 plan, A3 happens step 2 plan, on.3. Approach Nutshell Related WorkPlanning conversations performed time pressure exhibit unique characteristicschallenges inferring final plan. First, planning conversations succinctparticipants tend write shortly briefly, hurry make final decision.Second, may number different valid plans teams deployment even365fiKim, Chacha & ShahFigure 1: Web-based tool developed used data collection366fiA Generative Modeling Approach Logic-Based Priorsimple scenario, people tend generate broad range final plans. representstypical challenges faced real rescue missions, incident uniqueparticipants cannot library plans choose time. Third,conversations noisy often, many suggestions made rejected quicklywould casual setting. addition, likely manyrepeated confirmations agreement, might typically ease detection agreedupon plan.might seem natural take probabilistic approach plan inference problem,working noisy data. However, combination small amount noisydata large number possible plans means inference using typical, uninformativepriors plans may fail converge teams final plan timely manner.problem could also approached logical constraint problem partial orderplanning, noise utterances: team discuss partialplans relating final plan, without errors revisions, plan generatorscheduler (Coles, Fox, Halsey, Long, & Smith, 2009) could produce final plan usingglobal sequencing. Unfortunately, data collected human conversation sufficientlynoisy preclude approach.circumstances provided motivation combined approach, wherein builtprobabilistic generative model used logic-based plan validator (Howey, Long, &Fox, 2004) compute highly structured prior distribution possible plans. Intuitivelyspeaking, prior encodes assumption final plan likely, required,valid plan. approach naturally deals noise datachallenge performing inference plans limited amount data.performed sampling inference model using Gibbs Metropolis-Hastings samplingapproximate posterior distribution final plans, empirical validationhuman subject experiments indicated algorithm achieves 86% accuracy average.details model inference methods presented Section 4.related work categorized two categories: 1) application 2) technique.terms application, work relates plan recognition (Section 3.1). termstechnique, approach relates methods combine logic probability, thoughdifferent focused applications (Section 3.2).3.1 Plan RecognitionPlan recognition area interest within many domains, including interactivesoftware (Ryall et al., 1997; Bauer et al., 2011; Mayfield, 1992), story understanding (Charniak & Goldman, 1993) natural language dialogue (Carberry, 1990; Grosz & Sidner,1990).literature categorized two ways. first terms requirements.studies (Lochbaum, 1998; Kautz, 1987) require library plans, others (Zhuo,Yang, & Kambhampati, 2012; Ramrez & Geffner, 2009; Pynadath & Wellman, 2000;Sadilek & Kautz, 2010) replace library relevant structural information. libraryplans required, studies (Weida & Litman, 1992; Kautz, Pelavin, Tenenberg, &Kaufmann, 1991) assumed library collected, future plansguaranteed included within collected library. contrast, library plans367fiKim, Chacha & Shahrequired, replaced related structure information, domain theorypossible set actions performable agents. second categorizationliterature terms technical approach. studies incorporated constraint-basedapproaches, others took probabilistic combination approaches.First, reviewed work treated plan recognition knowledge base search problem. method assumes either build knowledge base,goal efficiently search knowledge base (Lochbaum, 1998; Kautz, 1987).approach often includes strong assumptions regarding correctness completenessplan library, addition restrictions noisy data (Weida & Litman, 1992; Kautzet al., 1991), applicable domains plan reoccurs naturally.example, Gal et al. (2012) studied adaptively adjust educational content betterlearning experience, given students misconceptions, using computer-based tutoring tool.Similarly, Brown Burton (1978) investigated users underlying misconceptions usinguser data collected multiple sessions spent trying achieve goal. termstechnical approach, approaches used logical methods solve orderingconstraints problem searching plan library.also reviewed work replaced knowledge base structural informationplanning problem. Zhuo et al. (2012) replaced knowledge base action modelsdomain, formulated problem one satisfiability recognize multi-agentplans. similar approach taken Ramrez Geffner (2009), wherein action modelsused replace plan library, Pynadath Wellman (2000) incorporatedextension probabilistic context free grammars (PCFGs) encode set predefinedactions improve efficiency. recently, Markov logic applied model geometry, motion model rules recognition multi-agent plans playing gamecapture flag (Sadilek & Kautz, 2010). Replacing knowledge base structuralinformation reduces amount prior information required. However, two major issues application prior work recognize plans team conversation:First, work assumed repetition previous plans. example, learningweights Markov logic (which represent importance strictness constraints)requires prior data mission, conditions resources. Second, using first-order logic express plan constraints quickly becomes computationallyintractable complexity plan increases.contrast logical approaches, probabilistic approaches allow noisy observations.Probabilistic models used predict users next action, given noisy data (Albrecht,Zuckerman, Nicholson, & Bud, 1997; Horvitz, Breese, Heckerman, Hovel, & Rommelse,1998). works use actions normally performed users training data.However, approaches consider particular actions (e.g., actions mustperformed users achieve certain goals within software system) likely.words, deal noisy data, incorporate structuralinformation could perhaps guide plan recognition algorithm. additional limitation methods assume predefined domains. defining domain,set possible plans limited, possible plans time-critical missions generallylimited set. situation available resources incident likelydifferent. method recognize plan noisy observations, openset possible plans, required.368fiA Generative Modeling Approach Logic-Based Priorprobabilistic approaches incorporate structure format plan library. Pynadath Wellman (2000) represented plan libraries probabilistic contextfree grammars (PCFGs). used build Bayes networks modeled underlying generative process plan construction. However, parsing-based approachesdeal partially-ordered plans temporally interleaved plans. Geib et al. (2008)Geib Goldman (2009) overcame issue working directly plan representation without generating intermediate representation form belief network.time step, technique observed previous action agent generatedpending action set. approach, too, assumed existing plan library relieddomains repetition previous plans. recently, Nguyen, Kambhampati,(2013) introduced techniques address incomplete knowledge plan library,plan generation rather plan recognition applications.approach combines probabilistic approach logic-based prior infer teamplans without need historical data, using situational information datasingle planning session. situational information includes operators resourcesdomain problem specifications, may updated modified onescenario another. require development addition plan library inferplan, demonstrate solution robust incomplete knowledge planningproblem.3.2 Combining Logic Probabilitycombination logical approach probabilistic modeling gained interestrecent years. Getoor Mihalkova (2011) introduced language description statistical models typed relational domains, demonstrated model learning using noisyuncertain real-world data. Poon Domingos (2006) proposed statistical samplingimprove searching efficiency satisfiability testing. particular, combinationfirst-order logic probability, often referred Markov Logic Networks (MLN),studied. MLN forms joint distribution probabilistic graphical model weightingformulas first-order logic (Richardson & Domingos, 2006; Singla & Domingos, 2007;Poon & Domingos, 2009; Raedt, 2008).approach shares MLNs philosophy combining logical tools probabilistic modeling. MLNs utilize first-order logic express relationships among objects.General first-order logic allows use expressive constraints across various applications. However, within planning domain, enumerating constraints first-order logicquickly becomes intractable complexity plan increases. example, first-orderlogic allow explicit expression action preconditions postconditions, letalone constraints among actions. PDDL well-studied planning researchcommunity (McDermott, Ghallab, Howe, Knoblock, Ram, Veloso, Weld, & Wilkins, 1998),main focus develop efficient ways express solve planning problems.approach exploits tool build highly structured planning domain withinprobabilistic generative model framework.369fiKim, Chacha & Shah4. Algorithmsection presents details algorithm. describe probabilistic generativemodel indicate model combined logic-based prior perform efficient inference. generative model specifies joint probability distribution observedvariables (e.g., human team planning conversation) latent variables (e.g., final plan);model learns distribution teams final plan, incorporating logic-basedprior (plan validation tool). key contribution design generative modellogic-based prior. also derive Gibbs sampling (Andrieu, De Freitas, Doucet, &Jordan, 2003) representation design scheme applying Metropolis-Hasting sampling (Metropolis, Rosenbluth, Rosenbluth, Teller, & Teller, 1953) performing inferencemodel.4.1 Generative Modelmodel human team planning process, represented dialogue, probabilisticBayesian model. particular, utilize probabilistic generative modeling approachextensively used topic modeling (e.g., Blei, Ng, & Jordan, 2003).start plan latent variable must inferred observation utterances made planning session. model generates utteranceconversation sampling subset predicates plan computing relative ordering appear within utterance. mapping absolute orderingplan relative ordering predicates utterance described detailbelow. Since conversation short level noise high, modeldistinguish utterances based order appear conversation.assumption produces simple yet powerful model, simplifies inference stepsenables 86% accuracy inference final plan. However, modelalso generalized take ordering account simple extension. includediscussion assumption extension Section 7.4.following step-by-step description generative model:1. Variable plan: plan variable Figure 2 defined ordered tuple setsgrounded predicates, represents final plan agreed upon team.distributed space ordered tuples sets grounded predicates. assumetotal number grounded predicates one domain fixed.prior distribution plan variable given by:(e plan validp(plan)1plan invalid.(1)positive number. models assumption final planlikely, necessarily required, valid plan.likelihood plan defined as:370fiA Generative Modeling Approach Logic-Based PriorkkpplanpsntpntpNs0tFigure 2: Graphical model representation generative model. plan latent variablerepresents final agreed-upon plan. pnt variable represents nth predicate tthutterance, snt represents absolute ordering predicate plan.s0t represents relative ordering sn within utterance t. latent variable prepresents noisiness predicates, represents noisiness ordering.p(s, p|plan, p )YYYYp(snt , pnt |plan, p )np(pnt |plan, snt , , p)p(snt |plan)nset predicates plan assigned consecutive absolute plan step index s,starting = 1 working left right ordered tuple. example, givenplan = ({A1 , A2 }, {A3 }, {A4 , A5 , A6 }), Ai predicate, A1 A2 occurparallel plan step = 1, A6 occurs plan step = 3.2. Variable snt : snt represents step index (i.e., absolute ordering) nth predicatetth utterance plan. step index represents absolute timestamppredicate plan. words, snt indicates absolute order predicatepnt appears plan. use st = {s1t , s2t . . . } represent vector orderingstth utterance, vector st may set consecutive numbers.snt sampled follows: utterance, n predicates sampled plan.example, consider n = 2, first sampled predicate appears secondset (i.e., second timestamp) plan second sampled predicate appearsfourth set. conditions, s1t = 2 s2t = 4. probability setsampled proportional number predicates contained within set.example, given plan = ({A1 , A2 }, {A3 }, {A4 , A5 , A6 }), probability selectingfirst set ({A1 , A2 }) 62 . models notion people likely371fiKim, Chacha & Shahdiscuss plan steps include many predicates, since plan steps many actionsmay require effort elaborate. Formally:p(snt = i|plan) =# predicates set plan.total # predicates plan(2)likelihood st defined as:00p(st |pt , st ) p(st |plan)p(pt , st |st , , p , plan)0p(st |st , )p(snt |plan)p(pnt |plan, snt , p )n0003. Variable s0t : variable s0t array size n, s0t = {st1 , st2 . . . stn }.0stn random variable represents relative ordering nth predicate withintth utterance plan, respect grounded predicates appearingtth utterance.s0t generated st follows:(ep(s0t |st )1s0t = f (st )s0t =6 f (st ).(3)> 0. random variable hyper-parameter represents noisinessordering grounded predicates appearing throughout entire conversation.takes scalar value, sampled gamma distribution:p(|k , ) Gamma(k , ),(4)k set 10.function f deterministic mapping absolute ordering st relativeordering s0t . f takes vector absolute plan step indices input, producesvector consecutive indices. example, f maps st = (2, 4) s0t = (1, 2)st = (5, 7, 2) s0t = (2, 3, 1).variable models way predicates orders appear human conversation: People frequently use relative terms, after, describepartial sequences full plan, often refer absolute ordering. Peoplealso make mistakes otherwise incorrectly specify ordering. model allowsinconsistent relative orderings nonzero probability; types mistakesmodeled value .4. Variable pnt p : variable pnt represents nth predicate appearingtth utterance. absolute ordering grounded predicate snt . pntsampled given snt , plan variable parameter, p .372fiA Generative Modeling Approach Logic-Based Priorp random variable (hyper-parameter) represents noisiness grounded predicates appearing throughout entire conversation. takes scalar value,sampled beta distribution:p(p |kp , p ) beta(kp , p ),(5)kp set 40, p set 10.probability p , sample predicate pnt uniformly replacementcorrect set snt plan follows:(1set jnnp(pt = i|plan, st = j) = # pred. set j0o.w.probability 1 p , sample predicate pnt uniformly replacementset plan (i.e., predicates mentioned dialogue). Therefore:p(pnt = i|plan, snt = j) =1.total # predicates(6)words, higher probability p , sample value pnt consistentsnt allows nonzero probability pnt sampled random plan.allows model incorporate noise planning conversation, includingmistakes plans later revised.4.2 Plan Validation Tooluse Planning Domain Description Language (PDDL) 2.1 plan validation tool (Howeyet al., 2004) evaluate prior distribution possible plans. section, brieflyreview PDDL, plan validation tool used form prior Equation1.4.2.1 Planning Domain Description LanguagePlanning Domain Description Language (PDDL) (McDermott et al., 1998) standardplanning language, inspired Stanford Research Institute Problem Solver (STRIPS)(Fikes & Nilsson, 1972) Action Description Language (ADL) (Pednault, 1987),utilized International Planning Competition.PDDL model planning problem two major components: domain specificationproblem specification. domain description consists domain-name definition,requirements language expressivity, definition object types, definition constantobjects, definition predicates (i.e. templates logical facts), definitionpossible actions instantiated execution. Actions parameters mayinstantiated objects, preconditions, conditional unconditional effects.excerpt PDDL domain file used work, called RESCUE domain, shownbelow. example, predicate isSaved encodes logical fact whetherparticular patient rescued, action SENDROBOT instantiatedexecution send particular robot particular location.373fiKim, Chacha & Shah(define (domain RESCUE)(:requirements :typing :durativeactions :negativepreconditions)(:types patient valve thingsToFix location locationmedcrew mechanic robot resource)(:predicates(isAt ?p thingsToFix ?l location)(isSaved ?p patient)(isFixed ?v valve)(isInspected ?l location)(isAvail ?r resource))(:durativeaction SENDROBOT:parameters (?r robot ?l location):duration (= ?duration 1):condition (and(at start (isAvail ?r))(at start (not (isInspected ?l)))):effect (and(at start (not (isAvail ?r) ) )(at end (isAvail ?r))(at end (isInspected ?l))))...)problem specification consists problem-name definition, definitionrelated domain-name, definition possible objects relevant planningproblem, initial state planning environment conjunction true/false facts,definition goal-states logical expression true/false facts. excerptPDDL problem specification file used work shown below. init sectiondescribes initial conditions example, patient pB initially situated location B,patient pD D. goal section indicates desired final state caserooms must inspected, patients must rescued, valves fixed. metricsection defines metric planner optimizes producing plan.374fiA Generative Modeling Approach Logic-Based Prior(define (problem rescuepeople)(:domain RESCUE)(:objectspB pD pG patientvC vF valveB C E F G H locationredMed blueMed medcrewredR blueR robotmech1 mechanic)(:init(isAt pB B)(isAt pD D)...(isAvail redMed)(isAvail blueMed)(isAvail redR)(isAvail blueR)(isAvail mech1)(not (isSaved pB))...(not (isInspected A))(not (isInspected B))...(not (isFixed vC))(not (isFixed vF)))(:goal (and(isSaved pB)(isSaved pD)...(isFixed vC)(isFixed vF)(isInspected A)(isInspected B)...));(:metric minimize (totaltime)))work, note domain specification could reused one planningsession another capabilities team change. example, setpossible set actions defined domain specification, may sufficient merely modifynumber names locations, medical crews, robots problem specification.domain problem specification files represent prior knowledgeapproach requires order infer final agreed-upon plan. valid plan definedtotally partially ordered sequence grounded predicates achieves goal stateinitial state, without violating constraints. Otherwise, plan invalid.next section describes plan validation tool assesses validity given plan, givendomain problem specification files.4.2.2 Plan Validation Tool (VAL)plan validator standard tool takes input planning problem describedPDDL proposed solution plan. tool incorporates three input files: 1) domaindefinition file, 2) problem definition file 3) proposed solution plan file. domain375fiKim, Chacha & Shahdefinition file contains types parameters (e.g., resources, locations), predicate definitionsactions (which also parameters, conditions effects). problem definitionfile contains information specific situation: example, number locationsvictims, initial goal conditions metric optimize. proposed solution plan filecontains single complete plan, described PDDL. output plan validation toolindicates whether proposed solution plan valid (true) (false).Metrics represent ways compute plan quality value. purpose study,metrics used included: 1) minimization total execution time radioactivematerial leakage scenario, 2) maximization number incidents respondedpolice incident response scenario. Intuitively speaking, metrics reflect rationalbehavior human experts. natural assume human experts would tryminimize total time completion time-critical missions (such radioactivematerial leakage scenario). first responders cannot accomplish necessary tasksscenario due limited availability resources, would likely try maximizenumber completed tasks (such police incident response scenario).One could imagine input files could reused subsequent missions,capabilities (actions) team may change dramatically. However, numberavailable resources might vary, might rules implicit specific situationencoded files (e.g., save endangered humans first fixing damagedbridge). Section 6, demonstrate robustness approach using completedegraded PDDL plan specifications.computation plans validity generally cheaper valid plan generation. gives us way compute p(plan) (defined Section 4.1) proportionalitycomputationally efficient manner. Leveraging efficiency, use Metropolis-Hastingssampling, (details described Section 4.3.1) without calculating partition function.4.3 Gibbs Samplinguse Gibbs sampling perform inference generative model. four latentvariables sample: plan, collection variables snt , p . iteratesampling latent variable, given variables. PDDL validator usedplan variable sampled.4.3.1 Sampling Plan using Metropolis-HastingsUnlike snt , write analytic form sample posterior,intractable directly resample plan variable, would require calculatingnumber possible plans, valid invalid. Therefore, use Metropolis-Hasting(MH) algorithm sample plan posterior distribution within Gibbs samplingsteps.376fiA Generative Modeling Approach Logic-Based Priorposterior plan represented product prior likelihood,follows:p(plan|s, p) p(plan)p(s, p|plan)= p(plan)Np(snt , pnt |plan)t=1 n=1= p(plan)Np(snt |plan)p(pnt |plan, snt )(7)t=1 n=1MH sampling algorithm widely used sample distribution directsampling difficult. algorithm allows us sample posterior distribution according user-specified proposal distribution without calculate partitionfunction. typical MH algorithm defines proposal distribution, Q(x0 |xt ), samples new point (i.e., x0 : value plan variable case) given current pointxt . new point achieved randomly selecting one several possible moves,defined below. proposed point accepted rejected, probabilitymin(1, acceptance ratio).Unlike simple cases, Gaussian distribution used proposal distribution,distribution needs defined plan space. Recall plan representedordered tuple sets predicates. work, new point (i.e., candidate plan)generated performing one following moves current plan:Move next: Randomly select predicate current plan, movenext timestamp. last timestamp plan, move firsttimestamp.Move previous: Randomly select predicate current plan, moveprevious timestamp. first timestamp plan, movelast timestamp.Add predicate plan: Randomly select predicate current plan,randomly choose one timestamp current plan. Add predicatechosen timestamp.Remove predicate plan: Randomly select remove predicatecurrent plan.moves sufficient allow movement one arbitrary plan another.intuition behind designing proposal distribution described Section 7.5.Note proposal distribution, is, symmetrical Q(x0 |xt ) 6= Q(xt |x0 ).need compensate according following,p (x0 )Q(x0 |xt ) = p (x)Q(xt |x0 ),(8)p target distribution. done simply counting numbermoves possible x0 get x, x0 x, weighing acceptance ratio377fiKim, Chacha & ShahEquation 8 true. often referred Hastings correction, performedensure proposal distribution favor states others.Next, ratios proposal distribution current proposed pointscalculated. plan valid, p(plan) proportional e , plan invalid,proportional 1, described Equation 1. Plan validity calculated usingplan validation tool. remaining term, p(snt |plan)p(pnt |plan, snt ), calculated usingEquations 2 6.Then,proposedplan accepted following probability:p (plan=x0 |s,p)min 1, p (plan=xt |s,p) , p function proportional posterior distribution.Although chose incorporate MH, usable sampling method.method require calculation normalization constant (e.g., rejectionsampling slice sampling) could also used. However, methods, samplingordered tuple sets grounded predicates slow complicated, pointedNeal (2003).4.3.2 Sampling Hyper-Parameters p Slice Samplinguse slice sampling sample p . method simple implementworks well scalar variables. Distribution choices made based valid valuetake. take value, preferably one mode, p takevalue [0, 1]. MH sampling may also work; however, method could overlycomplicated simple scalar value. chose stepping procedure, describedNeal et al (Neal, 2003).4.3.3 Sampling sntFortunately, analytic expression exists posterior snt :p(st |plan, pt , s0t ) p(st |plan)p(pt , s0t |plan, st )= p(st |plan)p(pt |plan, st )p(s0t |st )= p(s0t |st )Np(snt |plan)p(pnt |plan, snt )n=1Note analytic expression expensive evaluate number possiblevalues snt large. case, one marginalize snt , variable truly careplan variable.5. Experimentationsection, explain web-based collaboration tool used experimenttwo fictional rescue scenarios given human subjects experiment.5.1 Web-Based Tool DesignDisaster response teams increasingly using web-based tools coordinate missionsshare situational awareness. One tools currently used first responders Next378fiA Generative Modeling Approach Logic-Based PriorGeneration Incident Command System (NICS) (Di Ciaccio et al., 2011). integratedsensing command-and-control system enables distribution large-scale coordination across multiple jurisdictions agencies. provides video audio conferencingcapabilities, drawing tools chat window, allows sharing maps resource information. Overall, NICS enables collection exchange informationcritical mission planning.designed web-based collaboration tool modeled system, modification requires team communicate solely via text chat. tool developedusing Django (Holovaty & Kaplan-Moss, 2009), free open-source Web applicationframework written Python. Django designed ease working heavy-duty data,provides Python API enable rapid prototyping testing. Incoming dataeasily maintained user-friendly administrative interface. Although simplified version NICS, provides essence emerging technology large-scaledisaster coordination (Figure 1).5.2 ScenariosHuman subjects given one two fictional rescue scenarios asked formulateplan collaborating partners. collected human team planning dataresulting conversations, used data validate algorithm. first scenario involves radioactive material leakage accident building multiple rooms,tasks (described below) assumed take one unit time. added complexity scenario announcing new piece information halfwayplanning conversation, requiring team change plan. second scenario alsoincluded time-durative actions (e.g., action take place action B takingplace). scenarios inspired described emergency response team training manuals (FEMA, 2014), designed completed reasonable timeexperiments.5.2.1 First Scenario: Radioactive Material Leakagedisaster scenario involves leakage radioactive material floor consistingeight rooms. room contains either patient requiring in-person assessment valvemust repaired (Figure 4).Goal State: patients assessed in-person medical crew. valves fixedmechanic. rooms inspected robot.Constraints: two medical crews, red blue (discrete resource constraint),one human mechanic (discrete resource constraint) two robots, red blue (discreteresource constraint). safety purposes, robot must inspect radioactivity roomhuman crews sent inside (sequence constraint).Assumption: tasks (e.g. inspecting room, fixing valve) take amounttime (one unit), hard temporal constraints. assumption madeconduct initial proof-of-concept experimentation described paper, relaxedscenario described Section 5.2.2.379fiKim, Chacha & ShahFigure 3: Radioactive material leakage scenarioAnnouncement: planning session, team receives situation updatered robot order, requiring team modify previously discussedplan use one robot deployment. announcement triggers automaticallyteam exchanged 20 utterances. purpose announcement increasetask complexity team, least two competing plans increase levelnoise conversation.scenario produces large number possible plans (more 1012 ), manyvalid achieving goals without violating constraints.5.2.2 Second Scenario: Police Incidents Responsesecond scenario involves team police officers firefighters responding seriesincidents occurring different time frames. scenario includes complicatedtime-durative actions first, well interdependency taskstaken account planning. current time given 8 p.m. Two firesstarted time: one college dorm another theater building, shownFigure 4. Also, three street corners, indicated crime hot-spots (places predictedexperience serious crimes, based prior data), become active 8:30 p.m. 9 p.m.also report street robbery taking place 8 p.m. injury occurred;however, police officer must speak victim file incident report.Goal State: Respond many incidents possible given resources listed Table2.Constraints:Putting fire requires one fire truck one police car equipped robot.police car must stay robot evacuation over.robot perform evacuation.robot used once.Successfully responding fire requires evacuating building puttingfire. actions happen simultaneously.380fiA Generative Modeling Approach Logic-Based PriorFigure 4: Police incident response scenarioResourcesNamePolice teamsrobotsAlphaBravoCharlieFunctionPatrol hotspotDeploy robot evacuationRespond street robberyFire trucksDeltaEchoFoxtrotPut fireDurationEvacuate one building in:30 min one robot15 min two robots10 min three robotsTalk victim in:10 min one police carPut fire in:30 min one fire truck15 min two fire trucks10 min three fire trucks(same dorm theater)Table 2: Resources available police incident response scenario381fiKim, Chacha & ShahResponding hot-spot patrol requires one police car located sitespecified amount time.one police car necessary respond street robbery.Assumption Announcement: information traffic provided, traveltime place place assumed negligible. planning session, teamreceives following announcement: traffic officer contacted us, saidFirst Second bridges experience heavy traffic 8:15 pm. take least 20minutes car get across bridge. travel time theater hot-spot20 minutes without using bridges. announcement made, teammust account traffic plan.6. Evaluationsection, evaluate performance plan inference algorithm initialproof-of-concept human subject experimentation, show able infer humanteams final plan 86% accuracy average, accuracy defined compositemeasure task allocation plan sequence accuracy measures. also describe robotdemonstration two people plan execute first-response collaborative taskPR2 robot.6.1 Human Team Planning Dataindicated previously, designed web-based collaboration tool modeled NICSsystem (Di Ciaccio et al., 2011) used first-response teams, modificationrequires team communicate solely via text chat. radioactive material leakagescenario, announcement, 13 teams two (a total 26 participants) recruitedAmazon Mechanical Turk greater Boston area. Recruitmentrestricted located US increase probability participantsfluent English. radioactive material leakage scenario, announcement, 21teams two (a total 42 participants) recruited Amazon Mechanical Turkgreater Boston area. police incident response scenario, 14 teamstwo (total 28 participants) recruited greater Boston area. Participantsrequired prior experience expertise emergency disaster planning,note may structural differences dialog expert novice planners.leave topic future investigation.team received one two fictional rescue scenarios described Section 5.2,asked collaboratively plan rescue mission. Upon completion planning session,participant asked summarize final agreed-upon plan structured formdescribed previously. independent analyst reviewed planning sessions resolvediscrepancies two members descriptions necessary. first secondauthors, well two independent analysts, performed utterance tagging, teamplanning session tagged reviewed two four analysts. average, 36%predicates mentioned per data set end final plan.382fiA Generative Modeling Approach Logic-Based Prior6.2 Algorithm Implementationalgorithm implemented Python, VAL PDDL 2.1 plan validator (Howeyet al., 2004) used. performed 2,000 Gibbs sampling steps dataplanning session. initial plan value set two five moves (from MH proposaldistribution) away true plan. initial value variable randomly settimestamp initial plan value.Within one Gibbs sampling step, performed 30 steps Metropolis-Hastings(MH) algorithm sample plan. Every 20 samples selected measure accuracy(median), burn-in period 200 samples.Results assessed quality final plan produced algorithm termsaccuracy task allocation among agents (e.g. medic travels room)accuracy plan sequence.Two metrics task allocation accuracy evaluated: 1) percent inferred planpredicates appearing teams final plan [% Inferred], 2) percent noise rejectionextraneous predicates discussed appear teams final plan [%Noise Rej].evaluated accuracy plan sequence follows: pair predicatescorrectly ordered consistent relative ordering true final plan. meaordered pairs correct predicatessured percent accuracy sequencing [% Seq] # correctly.total # pairs correct predicatescorrectly estimated predicates compared, ground truth relationpredicates included true final plan. used relative sequencing measurecompound sequence errors, absolute difference measure would(e.g. error ordering one predicate early plan shifts positionsubsequent predicates).Overall composite plan accuracy computed arithmetic mean taskallocation plan sequence accuracy measures. metric summarizes two relevantaccuracy measures provide single metric comparison conditions.evaluated algorithm four conditions: 1) perfect PDDL files, 2) PDDL problem filemissing goals/constants (e.g. delete available agents), 3) PDDL domain file missingconstraint (e.g. delete precondition), 4) using uninformative prior possibleplans.purpose second condition, PDDL problem file missing goals/constants,test robustness approach incomplete problem information. PDDLproblem specifiction intentionally designed omit information regarding one patient(pG) one robot (blueR). also omitted following facts initial state:pG located G, blueR available perform inspections, patient pGpatient yet rescued. goal state omitted pG patient rescued.condition represented significant degradation problem definition file, sinceoriginal planning problem involved three patients two robots.purpose third condition, PDDL domain file missing constant,test robustness approach missing constraints (or rules successful execution).potentially easy person miss specifying rule often implicitly assumed.third condition omitted following constraint domain file: roomsinspected prior sending medical crews. condition represented significant383fiKim, Chacha & Shahdegradation domain file, since constraint affected action involving onemedical crew teams.Results shown Tables 3-5 produced sampling plan variables fixing= 5 p = 0.8. tables report median values percent inferredplan predicates appearing final plan [% Inferred], noise rejection [% Noise Rej.],sequence accuracy [% Seq.]. show algorithm infers final plans greater86% composite accuracy average. also show approach relatively robustdegraded PDDL specifications (i.e., PDDL missing goals, constants constraints).discussion sampling hyper-parameters found Section 7.2.6.3 Concept-of-Operations Robot Demonstrationillustrate use plan inference algorithm robot demonstrationtwo people plan execute first-response collaborative task PR2 robot.participants plan impending deployment using web-based collaborative tooldeveloped. planning session complete, dialogue tagged manually.plan inferred data confirmed human planners providedrobot execution. registration predicates robot actions, room namesmap locations, performed offline advance. first responders wayaccident scene, PR2 autonomously navigates room, performing onlinelocalization, path planning obstacle avoidance. robot informs rest teaminspects room confirms safe human team members enter. Videodemo found here: http://tiny.cc/uxhcrw.7. Discussionsection discuss results trends Tables 3-5. discuss sampling hyper-parameters improves inference accuracy, provide interpretation inferred hyper-parameter values relate data characteristics. also provideadditional support use PDDL analyzing multiple Gibbs sampling runs. rationale behind i.i.d assumption utterances made generative model explained,show simple extension model relax assumption. Finally,provide rationale designing proposal distribution sampling algorithm.7.1 Resultsaverage accuracy inferred final plan improved across three scenariosuse perfect PDDL compared uninformative prior possible plans.sequence accuracy also consistency improved use PDDL, regardless noise leveltype PDDL degradation. three scenarios exhibited different levels noise,defined percentage utterances end finally agreed uponplan. police incidents response scenario produced substantially higher noise (53%),compared radioactive material leaking scenario announcement (38%)announcement (17%). possibly police incidents scenario includeddurative-actions, whereas others not. Interestingly, perfect PDDL produced384fiA Generative Modeling Approach Logic-Based PriorPDDLPDDL missing goalsconstantsPDDL missing constraintPDDLTask Allocation% Inferred % Noise Rej.6110097Composite% Acc.86% Seq.10058777870701005887668665Table 3: Radioactive material leakage scenario plan accuracy results, announcement(13 teams / 26 subjects). table reports median values percent inferredplan predicates appearing final plan [% Inferred], noise rejection [% Noise Rej.],sequence accuracy [% Seq.]. Composite % Accuracy calculated averageprevious three measures.PDDLPDDL missing goalsconstantsPDDL missing constraintPDDLTask Allocation% Inferred % Noise Rej.7710083Composite% Acc.87% Seq.100549784721001005490818778Table 4: Radioactive material leakage scenario plan accuracy results, announcement(21 teams / 42 subjects). table reports median values percent inferredplan predicates appearing final plan [% Inferred], noise rejection [% Noise Rej.],sequence accuracy [% Seq.]. Composite % Accuracy calculated averageprevious three measures.PDDLPDDL missing goalsconstantsPDDL missing constraintPDDLTask Allocation% Inferred % Noise Rej.978997Composite% Acc.86% Seq.928692839781899597818582Table 5: Police incidents response scenario plan accuracy results (14 teams / 28 subjects).table reports median values percent inferred plan predicates appearingfinal plan [% Inferred], noise rejection [% Noise Rej.], sequence accuracy [% Seq.].Composite % Accuracy calculated average previous three measures.385fiKim, Chacha & Shahsubstantial improvements sequence accuracy noise level higher, radioactive material leaking scenario announcement, police incidents scenario.Accuracy task allocation, hand, differ depending noise leveltype PDDL degradation. noise rejection ratio betterPDDL PDDL missing constraint, compared uninformative prior,scenarios less noise (e.g. radioactive material leaking scenariosannouncement). However, PDDL provide benefit noise rejection ratiopolice incidents scenario noise level 50%. However, casePDDL provide improvements inferred task allocation.7.2 Sampling Hyper-Parameterssection discusses results hyper-parameter sampling. First, showdata point (i.e., teams conversation) converges different hyper-parameter values,show values capture characteristics data point. Second, showlearning different sets hyper-parameters improves different measures accuracy,describe consistent interpretation hyper-parametersmodel.PDDLPDDL missing goals constantsPDDL missing constraintPDDLPDDLPDDL missing goals constantsPDDL missing constraintPDDL50%46%42%30%46%26%42%25%30%20%13%11%10%10%0%5%0%0%0%0%% improved accuracy% improved accuracy40%20%17%15%12%18%17%12%10%5%5%0%3%-4%-8% -7%0%-5%-10%Radioactive Radioactive-10%PoliceRadioactive RadioactivePolice(a) Improvements noise rejection sampling (b) Improvements sequence accuracy sampplingFigure 5: Percent improvements median noise rejection median sequence accuracysampling hyper-parameters versus setting p = 0.8 = 5.7.2.1 Improvements Sequence Accuracy versus Noise Rejectionhyper-parameter represents noisiness predicate ordering, hyperparameter p represents noisiness assignment predicates. Setting parameters fixed value corresponds assumption noisiness data set.learn parameters Gibbs sampling, allowing values adjustedaccording different characteristics data set. details sample hyperparameters explained Section 4.3.2. performed 2,000 Gibbs sampling steps386fiA Generative Modeling Approach Logic-Based Priordata planning session. initial values p sampledprior, parameters set values described Section 4.1.found learned p (with = 5), noise rejection rate improvedcompared fixed p = 0.8. radioactive material leakage scenario,mid-scenario announcement, noise rejection ratio improvedmuch 41% 45%, respectively; police incident response scenario, observed13% improvement (Figure 5a). Note cases median noise rejectionratio maintained improved sampling p .Similarly, learned (with p = 0.8), sequence accuracies generally improved.radioactive material leakage scenario, announcement, sequenceaccuracy improved 26% 16%, respectively; police incident responsescenario, observed 18% improvement (Figure 5b). Note three casessee degradation accuracy 4-8%. However nine twelve casessequence accuracy maintained improved sampling .Interestingly, samples achieved highest overall composite accuracyplan learned, hyper-parameters fixed. particular,observed average 5% ( 3%) decrease composite accuracy sampling fourvariables together. One possible explanations finding that, duelimited amount data, Gibbs sampling may require many iterations convergevariables. result suggests one may choose set hyper-parameters learnbased measure accuracy important user.7.2.2 Interpretation Inferred Values Hyper-Parametersdescribed Section 4.1, p parameter models level noise predicates withindata. words, p parameter designed model many suggestionsteam makes conversation subsequently included final plan.noise level high, lower-valued p represent characteristics conversationwell, may allow better performance. (However, noise level high,inference may still fail.)compare learned value p characteristics conversation, needway calculate noisy conversation is. following one way manuallyestimate value p : First, count utterances contain predicates. Then,count utterances contain predicates included final plan. ratiotwo numbers interpreted noisiness predicates; lower number,team talked many possible plans.performed manual calculation two teams trials Team 3 10compare values learned values. Team 3s trial, 19.4% suggestionsmade conversation included final plan (i.e., almost 80% suggestionsrelevant final plan). hand, 68% suggestions made Team10s trial included final plan. Using interpretation, Team 3s trialtwice noisy Team 10s trial.converged value p lower Team 3s trial Team 10s trial, reflectingcharacteristics data set. Figure 6b shows converged value pteams trial (sub-sampled, subset dataset). figure presents values387fiKim, Chacha & Shah(a) Examples value convergences(b) Examples p value convergenceFigure 6: Inferred values hyper-parameters (only showing subset data set)p iteration Gibbs sampling step. Note samples Team 3s trialconverge 20%, samples Team 10s trial converge 40%. lower valuep represents higher noise level, matches intuition.However, conclusive way prove converged values truevalues. theory, Gibbs sampling algorithm guarantees convergences true388fiA Generative Modeling Approach Logic-Based Priorvalue infinite number iterations. Therefore, cannot prove convergedp variables shown Figure 6 true values. practice, trace plot,Figure 6, drawn order demonstrate convergence local optimum. factvalues appear plateau burn-in period provides support convergencelocal optimum point. Investigation potentially local optimum point suggestsp value data point different, observerelationship p value characteristics data set. addition,manual calculation noisiness one way interpreting noisiness dataset. Therefore, analysis considered one possible way gain insightlearned values; rigorous proof relation learned valuehyper-parameter characteristics data.7.3 Benefit PDDLsection provides additional evidence benefit using PDDL analyzing multipleruns using data sampling algorithm. explained Section 4.3, Gibbssampling approximate inference algorithm produce different resultsrun.section evaluate runs wide range different settings showbenefit PDDL applies particular setting parameters, also differentsettings. analyzed three cases across range parameters: 1) learning plans, 2) learning plan, p 3) learning plan, . first case, changedvalue range 3 1,000, p 0.3 0.8, 1 100. secondcase, addition parameters, varied parameters prior distributionp kp p ; ranging 2 70. third case, additionp parameters, varied parameters prior distribution k ;ranging 0.1 50. Values ranges selected randomly producetotal 613 runs.Eighty-two percent 613 runs showed higher accuracy PDDL usedPDDL used. suggests adding structured prior improves accuracywide range parameter settings. Figure 7 presents ratio runs saw benefituse PDDL, three scenarios.Interestingly, highest accuracy always achieved perfect PDDL files;cases, highest accuracy achieved imperfect PDDL files (e.g., PDDLfile missing goals/constraints, described Section 6). observation mayexplained possibility finally agreed-upon plans 1) complete and/or2) violate constraints (mostly due participants misunderstandings). example: Priorannouncement radioactive material leakage scenario, number teamsfinished building complete plans. Therefore, final plans cases maybetter inferred incomplete PDDL files (consistent Table 4). policeincident response scenario, however, number teams missed constraint hotspot patrolling task considered complete hot-spot fully covered 8:30p.m. 9 p.m. number teams dispatched police cars portion timewindow, resulting invalid plans perfect PDDL files (consistent Table 5)389fiRatio runs using PDDL improved composite accuracyKim, Chacha & Shah1198/2300.9173/216134/1670.80.70.60.50.40.30.20.10RadioactiveRadioactivePoliceFigure 7: Ratio runs show benefit using PDDLimprovements achieved adding structure prior using PDDL suggeststructural information beneficial inference problem. would interestingsystematically investigate smallest set structural information achieves accuracyimprovements, given fixed computation budget, future work.7.4 i.i.d Assumption Utterance Generative Modelgenerative model considers utterances independent identically distributed samples plan variable. words, consider utterancesgive equal evidence plan, regardless order appear conversation. alternative would different weight utterance, takeordering account. section, explain reasons i.i.d. assumption,simple extension current model relax assumption.human subject data collected work, observe clear relationshiporder utterance whether suggestion included final plan.example, number teams decided include parts plan discussedbeginning conversation within final plan, discussing manypossibilities. distribution utterances included final plan shownFigure 8. addition, team discusses plans time pressure, planningsessions often consist small number succinct communications. example,average number predicates utterances planning session 90, whereasaverage number predicates final plan 12. succinct conversation yields lessavailable data inference; therefore, complicated model may fail correctly inferlatent variables. time series model, wherein ordering taken accountweight utterance latent variable needs learned data,example model.390fiA Generative Modeling Approach Logic-Based PriorFigure 8: distribution utterances included final plan (normalized)However, simple extension current model relax assumption incorporate different importance utterance. One way decide utterances importance integrate human cognitive models. Human cognitive architectures (Anderson,1983) model human cognitive operations, memory model (Anderson, Bothell,Lebiere, & Matessa, 1998). example, decrease importance utterancetime proceeds planning session applying varying weights utterance.simple extension current model made incorporate memory model.Specifically, variables p modified vectors represent weightactivation level utterance human cognition model (Anderson et al., 1998).vector p length utterances, p = {p,1 , p,2 , , p,T },p,t represents activation level utterance. Similarly, extendvector, represent noisy utterance is, weighing accordingly. However, cognitive models empirically well-verified, Whitehill (2013)pointed structured way set parameters models. addition,unclear human memory model would differ depending characteristicsgiven task. example, memory model may differ significantly short, succinctconversations conducted time pressure.7.5 Engineering Proposal Distribution Metropolis-HastingsSampling Algorithmsection describes impact different proposal distributions MH samplingstep, rationale designing proposal distribution described Section 4.3.1.numerous studies conducted selecting family candidate-generatingdensity functions (Metropolis et al., 1953; Hastings, 1970; Geweke, 1989; Gelman & Rubin,391fiKim, Chacha & ShahResult proposal distribution preliminary workResult proposal distribution current work100%Percent composite accuracy90%80%70%60%50%40%30%20%10%0%Radioactive RadioactivePoliceFigure 9: impact different proposal distributions (The highest accuracy perfectPDDL files)1992). However, pointed Chib Greenberg (1995), structured waychoose proposal distribution. becomes challenging sampled objectsimple scalar variable, complicated object, plan variable(i.e., tuples sets grounded predicates) work. object, largerspaces potential proposal distributions choose from.However, good choice proposal distribution improve performance. Figure 9shows results two different proposal distributions used work. preliminary version work (Kim et al., 2013) applied following distribution:Select predicate set possible predicates. current plan, moveeither: 1) next set predicates 2) previous set, 3) removecurrent plan. current plan, move one existing sets.Select two sets current plan switch orders.One difference proposal distribution one outlined Section 4.3.1set allowed timestamps selected predicate move iteration.proposed distribution allows predicate move timestamp, whereasone Section 4.3.1 allows predicate move adjacent timestamp.key insight proposal distribution work gained investigating sequences MH sampling steps observing proposal distribution fails proposegood move. words, identify moves necessary move proposedvalue (i.e., proposed new plan) true value latent variable (i.e., true plan)close other. Often, predicate one timestamp true timestamp (i.e., one timestamp before), proposal distribution containedpreliminary work (Kim et al., 2013) often fails suggest better proposed point.392fiA Generative Modeling Approach Logic-Based Priormotivated us create proposal distribution enabling frequent moves adjacent timestamps two timestamps. result, observed substantialimprovement accuracy scenarios, shown Figure 9.particular proposal distribution cannot applied cases, insightsuggests following approach could useful designing proposal distributionnon-scalar valued variables: First, distance metric defined two nonscalar valued variables. case, step included defining distance twotuples sets predicates (i.e., plan variables). example, distance couldaverage number missing extraneous predicates number predicatesincorrect timestamps. Second, starting initial proposed distribution, distancesample true value measured. Third, filter sample sequencesdistance short, visualize them. shorter distance indicates momentssampling could almost reached true value, not. Finally, proposeddistribution modified include move converts samples third steptrue value within one two moves. process allows insight designingproposal distribution. leave investigation systematic approach futurework.8. Conclusion Future Workwork, formulated novel problem performing inference extractfinally agreed-upon plan human teams planning conversation. presentedalgorithm combines probabilistic approach logical plan validation, usedcompute highly structured prior possible plans. approach infers team planswithout need historical data, using situational information datasingle planning session. require development addition plan libraryinfer plan, demonstrate solution robust incomplete knowledgeplanning problem. demonstrated benefit approach using human team meetingdata collected large-scale human subject experiments (total 96 subjects) ableinfer human teams final plans 86% accuracy average.future, plan build work design interactive agent participates improve human teams planning decisions. Specifically envision workdescribed starting point utilizing building human domain expertsknowledge, improving quality finally agreed-upon plan human-machineinteraction.9. Acknowledgementwork sponsored ASD (R&E) Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions recommendations authorsnecessarily endorsed United States Government.393fiKim, Chacha & ShahAppendix A. Visualization Gibbs Sampling Convergence: TracePlotknown conclusive way determine whether Markov chainGibbs sampling reached stationary, desired posterior, distribution (Cowles& Carlin, 1996). Many available diagnostic tools designed test necessaryinsufficient conditions convergence, work done Gelman Rubin (1992),Geweke (1991), Heidelberger Welch (1981) Raftery Lewis (1995), mentionfew. work utilize much simpler yet still informative approach,visually check whether convergence reached using trace plot.trace plot simply scatter plot statistics successive parameter estimates(e.g., estimated values) respect iteration steps. statisticsmeans, variances covariance. trace plot informative scalar variablesplotted. Figure 10 shows examples trace plots p variables.ReferencesAlbrecht, D. W., Zuckerman, I., Nicholson, A. E., & Bud, A. (1997). Towards Bayesianmodel keyhole plan recognition large domains. Proceedings SixthInternational Conference User Modeling, pp. 365376. Springer-Verlag.Anderson, J. R. (1983). spreading activation theory memory. Journal Verbal LearningVerbal Behavior, 22 (3), 261295.Anderson, J. R., Bothell, D., Lebiere, C., & Matessa, M. (1998). integrated theorylist memory. Journal Memory Language, 38 (4), 341380.Andrieu, C., De Freitas, N., Doucet, A., & Jordan, M. I. (2003). introduction mcmcmachine learning. Machine learning, 50 (1-2), 543.Barnes, M., Chen, J., Jentsch, F., & Redden, E. (2011). Designing effective soldier-robotteams complex environments: training, interfaces, individual differences. EPCE,484493.Bauer, M., Biundo, S., Dengler, D., Koehler, J., & Paul, G. (2011). PHI: logic-based toolintelligent help systems..Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. JournalMachine Learning Research, 3, 9931022.Brown, J. S., & Burton, R. R. (1978). Diagnostic models procedural bugs basicmathematical skills. Cognitive Science, 2 (2), 155192.Carberry, S. (1990). Plan recognition natural language dialogue. MIT Press.Casper, J., & Murphy, R. (2003). Human-robot interactions robot-assisted urbansearch rescue response World Trade Center. IEEE SMCS, 33 (3), 367385.Casper, J., & Murphy, R. (2002). Workflow study human-robot interaction USAR.IEEE ICRA, 2, 19972003.Charniak, E., & Goldman, R. P. (1993). Bayesian model plan recognition. ArtificialIntelligence, 64 (1), 5379.394fiA Generative Modeling Approach Logic-Based Prior(a) Examples value convergences(b) Examples p value convergenceFigure 10: Trace Plots (only showing subset data set)Chib, S., & Greenberg, E. (1995). Understanding Metropolis-Hastings algorithm.American Statistician, 49 (4), 327335.Coles, A., Fox, M., Halsey, K., Long, D., & Smith, A. (2009). Managing concurrencytemporal planning using planner-scheduler interaction. Artificial Intelligence, 173 (1),144.Cowles, M. K., & Carlin, B. P. (1996). Markov chain Monte Carlo convergence diagnostics:comparative review. Journal American Statistical Association, 91 (434), 883904.Cummings, M. L., Brzezinski, A. S., & Lee, J. D. (2007). Operator performance intelligent aiding unmanned aerial vehicle scheduling. IEEE Intelligent Systems, 22 (2),5259.395fiKim, Chacha & ShahDi Ciaccio, R., Pullen, J., & Breimyer, P. (2011). Enabling distributed commandcontrol standards-based geospatial collaboration. IEEE International ConferenceHST.FEMA (2014). Federal emergency management agency.. [Online; accessed 3-December2014].Fikes, R. E., & Nilsson, N. J. (1972). Strips: new approach application theoremproving problem solving. Artificial intelligence, 2 (3), 189208.Gal, Y., Reddy, S., Shieber, S. M., Rubin, A., & Grosz, B. J. (2012). Plan recognitionexploratory domains. Artificial Intelligence, 176 (1), 22702290.Geib, C. W., & Goldman, R. P. (2009). probabilistic plan recognition algorithm basedplan tree grammars. Artificial Intelligence, 173 (11), 11011132.Geib, C. W., Maraist, J., & Goldman, R. P. (2008). new probabilistic plan recognitionalgorithm based string rewriting.. ICAPS, pp. 9198.Gelman, A., & Rubin, D. B. (1992). Inference iterative simulation using multiplesequences. Statistical Science, 457472.Getoor, L., & Mihalkova, L. (2011). Learning statistical models relational data. International Conference Management Data, 11951198.Geweke, J. (1989). Bayesian inference econometric models using Monte Carlo integration.Econometrica: Journal Econometric Society, 13171339.Geweke, J. (1991). Evaluating accuracy sampling-based approaches calculationposterior moments. Federal Reserve Bank Minneapolis, Research Department.Goodrich, M. A., Morse, B. S., Engh, C., Cooper, J. L., & Adams, J. A. (2009). Towardsusing UAVs wilderness search rescue: Lessons field trials. InteractionStudies, Special Issue Robots Wild: Exploring Human-Robot InteractionNaturalistic Environments, 10 (3), 453478.Grosz, B. J., & Sidner, C. L. (1990). Plans discourse. Cohen, P. R., Morgan,J., & Pollack, M. E. (Eds.), Intentions Communication, pp. 417444. MIT Press,Cambridge, MA.Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chainsapplications. Biometrika, 57 (1), 97109.Heidelberger, P., & Welch, P. D. (1981). spectral method confidence interval generationrun length control simulations. Communications ACM, 24 (4), 233245.Holovaty, A., & Kaplan-Moss, J. (2009). definitive guide Django: Web developmentdone right. Apress.Horvitz, E., Breese, J., Heckerman, D., Hovel, D., & Rommelse, K. (1998). Lumiereproject: Bayesian user modeling inferring goals needs software users.Proceedings Fourteenth Conference Uncertainty Artificial Intelligence,256265.Howey, R., Long, D., & Fox, M. (2004). Val: Automatic plan validation, continuous effectsmixed initiative planning using PDDL. IEEE ICTAI, 294301.396fiA Generative Modeling Approach Logic-Based PriorJones, H., Rock, S., Burns, D., & Morris, S. (2002). Autonomous robots SWAT applications: Research, design, operations challenges. AUVSI.Jurafsky, D., & Martin, J. H. (2000). Speech Language Processing: IntroductionNatural Language Processing, Computational Linguistics, Speech Recognition (1stedition). Prentice Hall PTR, Upper Saddle River, NJ, USA.Kautz, H. A., Pelavin, R. N., Tenenberg, J. D., & Kaufmann, M. (1991). formal theoryplan recognition implementation. Reasoning Plans, 69125.Kautz, H. A. (1987). formal theory plan recognition. Ph.D. thesis, Bell Laboratories.Kim, B., Chacha, C. M., & Shah, J. (2013). Inferring robot task plans human teammeetings: generative modeling approach logic-based prior. AAAI.Kim, J., & Shah, J. A. (2014). Automatic prediction consistency among team members understanding group decisions meetings. Systems, Man Cybernetics(SMC), 2014 IEEE International Conference on, pp. 37023708. IEEE.Koomen, P., Punyakanok, V., Roth, D., & Yih, W. (2005). Generalized inferencemultiple semantic role labeling systems. CoNLL, 181184.Kruijff, G., Jancek, M., & Lison, P. (2010). Continual processing situated dialoguehuman-robot collaborative activities. IEEE Ro-Man.Larochelle, B., Kruijff, G., Smets, N., Mioch, T., & Groenewegen, P. (2011). Establishinghuman situation awareness using multi-modal operator control unit urbansearch & rescue human-robot team. IEEE Ro-Man, 229234.Lochbaum, K. E. (1998). collaborative planning model intentional structure. Computational Linguistics, 24 (4), 525572.Mayfield, J. (1992). Controlling inference plan recognition. User Modeling UserAdapted Interaction, 2 (1-2), 5582.McDermott, D., Ghallab, M., Howe, A., Knoblock, C., Ram, A., Veloso, M., Weld, D., &Wilkins, D. (1998). PDDL-the planning domain definition language..Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953).Equation state calculations fast computing machines. Journal ChemicalPhysics, 21, 1087.Micire, M. (2002). Analysis robotic platforms used World Trade Center disaster.Ph.D. thesis, MS thesis, Department Computer Science Engineering, Univ. SouthFlorida.Murphy, R. (2004). Human-robot interaction rescue robotics. IEEE SMCS, 34 (2), 138153.Neal, R. M. (2003). Slice sampling. Annals Statistics, 705741.Nguyen, T. A., Kambhampati, S., & Do, M. (2013). Synthesizing robust plans incomplete domain models. Advances Neural Information Processing Systems, 24722480.Palmer, M., Gildea, D., & Xue, N. (2010). Semantic role labeling. Synthesis LecturesHuman Language Technologies, 3 (1), 1103.397fiKim, Chacha & ShahPednault, E. P. D. (1987). Formulating Multi-Agent Dynamic-World Problems Classical Planning Framework. Reasoning Actions Plans: Proceedings1986 Workshop. Morgan Kaufmann Publishers.Poon, H., & Domingos, P. (2006). Sound efficient inference probabilisticdeterministic dependencies. AAAI, 21 (1), 458.Poon, H., & Domingos, P. (2009). Unsupervised semantic parsing. EMNLP.Pradhan, S., Ward, W., Hacioglu, K., Martin, J., & Jurafsky, D. (2004). Shallow semanticparsing using support vector machines. NAACL-HLT, 233.Pynadath, D. V., & Wellman, M. P. (2000). Probabilistic state-dependent grammarsplan recognition. Proceedings Sixteenth conference Uncertainty ArtificialIntelligence, 507514.Raedt, L. (2008). Probabilistic logic learning. Logical Relational Learning, 223288.Raftery, A. E., & Lewis, S. M. (1995). number iterations, convergence diagnosticsgeneric metropolis algorithms. Practical Markov Chain Monte Carlo, 115130.Ramrez, M., & Geffner, H. (2009). Plan recognition planning. Proceedings 21stinternational joint conference Artificial Intelligence, 17781783.Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine learning, 62 (1),107136.Ryall, K., Marks, J., & Shieber, S. (1997). interactive constraint-based systemdrawing graphs. Proceedings 10th Annual ACM Symposium User InterfaceSoftware Technology, 97104.Sadilek, A., & Kautz, H. A. (2010). Recognizing multi-agent activities GPS data.AAAI.Singla, P., & Domingos, P. (2007). Markov logic infinite domains. UAI, 368375.Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., & Roy, N. (2011).Understanding natural language commands robotic navigation mobile manipulation. AAAI.Weida, R., & Litman, D. (1992). Terminological Reasoning Constraint NetworksApplication Plan Recognition.Whitehill, J. (2013). Understanding ACT-R - outsiders perspective. CoRR, 1306.0125.Zhuo, H. H., Yang, Q., & Kambhampati, S. (2012). Action-model based multi-agent planrecognition. Advances Neural Information Processing Systems 25, 377385.398fiJournal Artificial Intelligence Research 52 (2015) 203-234Submitted 8/14; published 1/15Subexponential-Time Complexity CSPRonald de HaanDEHAAN @ KR . TUWIEN . AC .Vienna University TechnologyVienna, AustriaIyad KanjIKANJ @ CS . DEPAUL . EDUSchool Computing, DePaul UniversityChicago, USAStefan SzeiderSTEFAN @ SZEIDER . NETVienna University TechnologyVienna, AustriaAbstractNP-complete problems share practical hardness respect exact computation.Whereas NP-complete problems amenable efficient computational methods, othersyet show sign. becomes major challenge develop theoretical frameworkfine-grained theory NP-completeness, explain distinctionexact complexities various NP-complete problems. distinction highly relevantconstraint satisfaction problems natural restrictions, various shades hardnessobserved practice.Acknowledging NP-hardness problems, one look beyond polynomial timecomputation. theory subexponential-time complexity provides framework,enjoying increasing popularity complexity theory. instance constraint satisfactionproblem n variables domain values solved brute-force dn steps (omittingpolynomial factor). paper study existence subexponential-time algorithms,is, algorithms running do(n) steps, various natural restrictions constraint satisfactionproblem. consider constraint satisfaction problem constraints givenextensionally tables, constraints given intensionally formglobal constraints. provide tight characterizations subexponential-time complexityaforementioned problems respect several natural structural parameters, allows usdraw detailed landscape subexponential-time complexity constraint satisfaction problem. analysis provides fundamental results indicating whether one significantlyimprove brute-force search approach solving constraint satisfaction problem.1. Introductionobserved various practical contexts NP-hard problems accessibleefficient exact computational methods, whereas others methods futile. centralchallenge theoreticians develop framework, fine-grained theoryNP-completeness, explain distinction exact complexities NP-hardproblems. Subexponential-time complexity framework complexity theory providesdistinction (Lokshtanov, Marx, & Saurabh, 2011). based observationNP-complete problems, one improve exponent exponential term upper boundc2015AI Access Foundation. rights reserved.fiDEH AAN , K ANJ , & ZEIDERrunning time indefinitelysuch problems admit subexponential-time algorithmswhereasothers apparently possible commonly-believed hypotheses complexity theory.particular, subexponential-time algorithms developed many graph problems, includingNDEPENDENT ET OMINATING ET, natural structural restrictions; e.g., see workAlber, Fernau, Niedermeier (2004), Chen, Kanj, Perkovic, Sedgwick, Xia (2007)Demaine, Fomin, Hajiaghayi, Thilikos (2005). benchmark problem subexponential-timecomputation satisfiability problem CNF formulas, clause contains threeliterals, denoted 3-CNF-S AT. Exponential Time Hypothesis (ETH), proposed ImpagliazzoPaturi (2001), states 3-CNF-S n variables decidable subexponential time,i.e., decidable time 2o(n) (omitting polynomial factors).Constraint Satisfaction Problem (CSP) provides general uniform frameworkrepresentation solution hard combinatorial problems arise various areas ArtificialIntelligence Computer Science (Rossi, van Beek, & Walsh, 2006). instance, databasetheory, CSP equivalent evaluation problem conjunctive queries relational databases(Gottlob, Leone, & Scarcello, 2002). well known CSP NP-hard, entails fundamentalNP-hard problems 3-C OLORABILITY 3-CNF-S AT. Hence, cannot hopepolynomial-time algorithm CSP. hand, CSP obviously solved exponentialtime: simply trying possible instantiations variables, solve CSP instanceconsisting n variables range domain values time dn (omitting polynomialfactor input size). Significant work concerned improving trivial upper boundvarious restrictions CSP (Beigel & Eppstein, 2005; Feder & Motwani, 2002; Grandoni &Italiano, 2006; Moser & Scheder, 2011; Schning, 1999). instance, Razgon (2006) showedbinary CSP domain size solved time (d 1)n forward-checking algorithmemploying fail-first variable ordering heuristic; although faster algorithms known,result indicates exponential running time CSP improved using heuristic methodsdesigned solving real-world CSP instances practice. improvementstrivial brute-force search give exponential running times exponent linear n.aim paper investigate theoretical limits improvements. precisely,explore whether exponential factor dn reduced subexponential factor do(n)not, considering various natural NP-hard restrictions classical CSP constraintsgiven extensionally form tables, CSP constraints specifiedintensionally using global constraints. CSP global constraints, consider CSPglobal constraints eitherAllDifferent constraints (denoted CSP6= ),NValue constraints (denoted CSP= ),AtLeastNValue constraints (denoted CSP ),AtMostNValue constraints (denoted CSP ),cTable constraints, i.e., constraints specified tables compressed tuples (denotedCSPc ).study CSP global constraints highly relevant central modelingsolving real-world problems use various global constraints come along efficient204fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSPpropagation filtering techniques (Rgin, 2011; Van Hoeve & Katriel, 2006). Therefore,study existence subexponential-time algorithms generic problems variousrestrictions prime interest.paper, obtain lower upper bounds results, cases draw detailedcomplexity landscape CSP extensionally represented constraints CSP globalconstraints respect subexponential-time solvability. lower bounds subjectExponential Time Hypothesis (ETH), even though results derived weakercomplexity-theoretic hypotheses (Proposition 2, Proposition 3, Proposition 10). structuralparameters CSP instance focus (when relevant) are: (instance) size, domainsize, number constraints, arity (i.e., maximum size constraint scope), maximumdegree (i.e., maximum number occurrences variable), treewidth primalincidence graph. highlight results obtain. turns out,almost restrictions consideration, CSP generalization, CSP (global)compressed table constraints (CSPc ), exhibit behavior respect subexponentialtime solvability. unless explicitly indicated results below, results CSP (positivenegative) mentioned hold well CSPc .easy see CSP bounded domain size bounded arity subexponential-timealgorithm ETH fails. first result provides evidence dropbound domain size bound arity, problem becomes harder; referdiscussion preceding Proposition 2 (n number variables instance):1. B OOLEAN CSP solvable nonuniform subexponential time (unrestricted) CNFS AT. B OOLEAN CSPc , show B OOLEAN CSPc solvable subexponentialtime parameterized complexity hierarchy collapses second level, consequenceimplies CNF-S solvable subexponential time.2. 2-CSP (all constraints arity 2) solvable subexponential time C LIQUEsolvable time N o(k) (N number vertices k clique-size).turns out, number tuples plays important role characterizing subexponential-timecomplexity CSP. show following tight result:3. CSP solvable subexponential time instances number tuples o(n),unless ETH fails, solvable subexponential time number tuplesinstances (n).B OOLEAN CSP linear size even derive equivalence ETH:4. B OOLEAN CSP instances size (n) solvable subexponential timeETH fails.Results 3 4 also hold consider total number tuples constraint relations insteadinput size.5. CSP solvable subexponential time instances whose primal treewidth o(n),solvable subexponential time instances whose primal treewidth (n) unlessETH fails.205fiDEH AAN , K ANJ , & ZEIDER6. CSP solvable polynomial time instances whose incidence treewidth O(1),solvable subexponential time instances whose incidence treewidth (1) unlessETH fails.CSP6= show following results:7. CSP6= solvable subexponential time instances whose domain size lower boundedfunction (1), solvable subexponential time constant domain sizeleast 3 unless ETH fails.note aforementioned result may sound strange implies problemeasier larger domain size. explained fact domain size getslarge, allowable upper bound subexponential time solving problem (i.e., d(n)o(n) )gets larger well.8. CSP6= solvable subexponential time instances whose primal treewidth o(n),solvable subexponential time instances whose primal treewidth (n) unlessETH fails.9. CSP6= solvable subexponential time instances whose incidence treewidth o(n),solvable subexponential time instances whose primal treewidth (n) unlessETH fails. Contrast result result (6) above.CSP= , CSP , CSP , show following:10. CSP solvable subexponential time instances whose number constraints constantwhose domain size lower bounded function (1), solvablesubexponential time number constraints linear domain size constantunless ETH fails.11. CSP= CSP solvable subexponential time instances whose domain sizeconstant whose number constraints (n) unless ETH fails.12. CSP= , CSP , CSP solvable subexponential time instances whose primaltreewidth o(n), solvable subexponential time instances whose primaltreewidth (n) unless ETH fails.table provides map that, structural parameters consideredpaper, lists results paper pertaining structural parameter. structural parametersconsider instance CSP, CSP global constraints, are: size (size),maximum size constraint scope (arity), cardinality domain (dom), numbertuples (tuples), number constraints (cons), treewidth incidence graph (tw ),treewidth primal graph (tw), maximum number occurrences variable (deg).206fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSPParameterResultssizearitydomtuplesconstwtwdegTheorem 3Propositions 1, 2, 12Theorems 1, 2, 3, 7; Propositions 1, 3, 12, 14, 17; Corollaries 1, 3Theorem 2Theorems 4, 6, 7; Propositions 14, 17; Corollaries 2, 3Theorem 5; Propositions 16, 19Theorem 5; Propositions 15, 18Proposition 11results paper shed light instances aforementioned variantsCSP (with without global constraints) may feasible respect exact computation.Moreover, results derived paper provide strong theoretical evidencenatural restrictions CSP may harder k-CNF-S ATfor subexponential-timealgorithm would lead failure ETH. Hence, results provide new point viewrelationship CNF-S CSP, important topic recent AI research (Jeavons & Petke,2012; Dimopoulos & Stergiou, 2006; Benhamou, Paris, & Siegel, 2012; Bennaceur, 2004).close section mentioning work subexponential-time complexityCSP problems AI. Already pioneering work ETH (Impagliazzo & Paturi, 2001;Impagliazzo, Paturi, & Zane, 2001) considered k-C OLORABILITY problem, constitutesimportant special case 2-CSP fixed domain size k. several results 2-CSPbounds tw, treewidth primal graph (see Section 3.3 definitions). Lokshtanovet al. (2011) showed following lower bound, using result L IST C OLORING problem(Fellows et al., 2011a): 2-CSP cannot solved time f (tw)no(tw) unless ETH fails. Marx(2010) showed recursively enumerable class G graphs unbounded treewidthfunction f 2-CSP solved time f (G)no(tw/ log tw) instances whose primalgraph G, ETH fails. Jonsson, Lagerkvist, Nordh (2013) investigated B OOLEANCSP finite constraint languages identify easiest Boolean constraint languageCSP still NP-hard, show already problem subexponential-time algorithmunless ETH fails. Traxler (2008) studied subexponential-time complexity CSPconstraints represented listing forbidden tuples; contrast standardrepresentation use, allowed tuples given, naturally captures databaseproblems (Gottlob et al., 2002; Grohe, 2006; Papadimitriou & Yannakakis, 1999). settingconsidered generalization CNF-S AT; single clause gives rise constraintexactly one forbidden tuple. arity bounded constant, insignificant whetherconstraints represented forbidden allowed tuples, one translate tworepresentations polynomial time. Finally would like point recent use ETHcomplexity analysis problems highly relevant AI like Planning (Bckstrm &Jonsson, 2011), Probabilistic Inference (Kwisthout, Bodlaender, & van der Gaag, 2010), TextAnalysis (Ge, 2013).Parts paper published preliminary form proceedings AAAI13CP14 (Kanj & Szeider, 2013; De Haan, Kanj, & Szeider, 2014).207fiDEH AAN , K ANJ , & ZEIDER2. Preliminariessection introduce terminologies background material needed paper.2.1 Constraint Satisfiability CNF-Satisfiabilityinstance C ONSTRAINT ATISFACTION P ROBLEM (or CSP, short) triple (V, D, C),V finite set variables, finite set domain values, C finite set constraints.constraint C pair (S, R), S, constraint scope, non-empty sequencedistinct variables V , R, constraint relation, relation whose arity matcheslength S; relationPis considered set tuples. Therefore,P size CSP instance= (V, D, C) sum (S,R)C |S| |R|; total number tuples (S,R)C |R|. assume,without loss generality, every variable occurs least one constraint scope every domainelement occurs least one constraint relation. Consequently, size instance leastlarge number variables I. write var (C) set variables occurscope constraint C.assignment instantiation mapping set V variables domain D.assignment satisfies constraint C = ((x1 , . . . , xn ), R) ( (x1 ), . . . , (xn )) R, satisfiesCSP instance satisfies constraints. instance consistent satisfiablesatisfied assignment. CSP problem deciding whether given instance CSPconsistent. B OOLEAN CSP denotes CSP Boolean domain {0, 1}. r-CSP denoterestriction CSP instances arity constraint r.primal graph CSP instance vertices variables I, two variablesjoined edge variables occur together constraint I. incidencegraph CSP instance bipartite graph, one side consists variablesside consists constraints I; variable constraint joined edgevariable occurs constraint.tree decomposition graph G = (V, E) pair (T, ) consisting tree mappingassigns node subset (t) V following conditions satisfied:(i) every edge {u, v} E node u, v (t); (ii) three nodest1 , t2 , t3 (t2 ) (t1 ) (t3 ) t2 lies path t1 t3 . width(T, ) size largest set (t) minus 1. treewidth G smallest widthtree decompositions. Bounding treewidth classical method restricting structureCSP instances. method dates back Freuder (1982). treewidth parameter appliedCSP terms primal graphs incidence graphs giving rise primal treewidth (also calledinduced width (Dechter, 2003)) incidence treewidth, respectively (Samer & Szeider, 2010),CSP instances.instance = (V, D, C) CSP define following basic parameters.vars: number |V | variables, usually denoted n.Psize: size CSP instance defined (S,R)C |S| |R|.dom: number |D| values; is, union values variablesassume.cons: number |C| constraints.208fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSParity: maximum size constraint scope.deg: maximum number occurrences variable.tw: treewidth primal graph I.tw : treewidth incidence graph I.propositional formula F set variable {x1 , . . . , xn } conjunctive normal form(CNF) conjunction set clauses {C1 , . . . , Cm }, clause Ci , = 1, . . . , m,disjunction literals (i.e., variables negations variables). say propositionalformula F satisfiable exists truth assignment variables F assigns leastone literal clause F value 1 (TRUE); also say case satisfies F .CNF-S ATISFIABILITY problem, CNF-S short, given formula F CNF form, decidewhether F satisfiable. width clause CNF formula F number literalsclause. k-CNF-S problem, k 2, restriction CNF-S probleminstances width clause k. well known k-CNF-Sproblem k 3 NP-complete (Garey & Johnson, 1979), whereas 2-CNF-S problemsolvable polynomial time (Papadimitriou, 1994).2.2 Global Constraintsoften preferred represent constraint succinctly listing tuplesconstraint relation. intensionally represented constraint called global constraint (Rgin,2011; Van Hoeve & Katriel, 2006). Global Constraints Catalogue (Beldiceanu, Carlsson, &Rampon, 2006) lists several hundred global constraints. paper focus followingglobal constraints.AllDifferent global constraint probably best-known, influential,studied global constraint constraint programming (Van Hoeve & Katriel, 2006). admitsefficient matching based filtering algorithms (Rgin, 1994). AllDifferent constraintset variables satisfied variable assigned different value.global constraints NValue (Pachet & Roy, 1999), AtLeastNValue (Rgin, 1995), AtMostNValue (Bessiere, Hebrard, Hnich, Kiziltan, & Walsh, 2006) widely used constraintprogramming (Beldiceanu et al., 2006). constraint C associated integernC N; consider nC given integer, value variable CSP instance. NValue constraint C set SC variables satisfied number distinctvalues assigned variables SC exactly nC . AtLeastNValue AtMostNValueconstraints satisfied number distinct values nC nC , respectively.special case NValue AtLeastNValue constraint C nC equals arity Cequivalent AllDifferent constraint.global constraint cTable table constraint compressed tuples. global constraintadmits potentially exponential reduction space compared extensional tableconstraint propagated using variant GAC-schema algorithm (Katsirelos& Walsh, 2007). cTable constraints also studied name generalized DNFconstraints (Chen & Grohe, 2010). cTable constraint pair (S, U ) = (v1 , . . . , vr )209fiDEH AAN , K ANJ , & ZEIDERnon-empty sequence distinct variables, U set compressed tuples,sequences form (V1 , . . . , Vr ), Vi D(vi ), 1 r. One compressed tuple(V1 , . . . , Vr ) represents tuples (d1 , . . . , dr ) di Vi . Thus, decompressionone compute (S, U ) (unique) equivalent table constraint (S, R) R containstuples represented compressed tuples U .CSP constraints AllDifferent constraints denoted CSP6= . variant CSPstudied Fellows, Friedrich, Hermelin, Narodytska, Rosamond (2011b) calledMAD-CSP (multiple different CSP). CSP constraints NValue, AtLeastNValue,AtMostNValue constraints, denoted CSP= , CSP , CSP , respectively. CSPconstraints cTable constraints denoted CSPc .note CSP6= , CSP= , CSP , CSP , CSPc , NP-complete. fact, CSP6= (and therefore general CSP ) even NP-hard instances consisting two constraints (Kutz,Elbassioni, Katriel, & Mahajan, 2008), CSP CSP= even NP-hard instances consisting single constraint (Bessiere et al., 2007). CSPc clearly NP-hard contains classical CSP(with table constraints) special case. Hence considered problems admit representationNP-hard combinatorial problems.Consider CSP instance models real-world problem uses, among others,global constraints considered above, say AllDifferent constraint. Then, combineAllDifferent constraints instance new global constraint, multi-AllDifferent constraint.Filtering combined constraint polynomial time equivalent solving one instance CSP6= .combination several global constraints new one considered severaldifferent global constraints (see, e.g., Hnich et al., 2004; Rgin & Rueher, 2000).Guarantees limits polynomial-time preprocessing single NValue, AtLeastNValue,AtMostNValue constraints given Gaspers Szeider (2014).Boolean versions global constraints problems, parameters vars, dom,cons, arity, deg, tw,CSP. size instance = (V, D, C)P tw , defined6=CSP defined CC |SC |. CSP= , CSP , CSP , size instance = (V, D, C)Pdefined| + log (nC )). instance = (V, D, C) CSPc , size definedPCC (|SCPc(S,U )C(V1 ,...,Vr )U (|V1 | + + |Vr |). Note definition instance size CSPencompasses CSP.2.3 Subexponential Timeproper complexity function complexity theory stands nondecreasing function fcomputable O(n + f (n)) time O(f (n)) space, n length input (see Papadimitriou, 1994). time complexity functions used paper assumed proper complexityfunction. o() notation used denotes oeff () notation (Flum & Grohe, 2006). formally,two proper complexity functions f, g : N N, writing f (n) = o(g(n)) meanexists proper complexity function (n) : N N, n0 N, f (n) g(n)/(n)n n0 . () notation defined similarly above.clear CSP CNF-S solvable time domn |I|O(1) 2n |I|O(1) , respectively,input instance n number variables I. say CSP (resp. CNF-S AT)solvable uniform subexponential time exists algorithm solves problem timedomo(n) |I|O(1) (resp. 2o(n) |I|O(1) ). Using results Chen, Kanj, Xia (2009) Flum210fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSPGrohe (2006), definition equivalent following: CSP (respectively, CNF-S AT)solvable uniform subexponential time exists algorithm = 1/`, `positive integer, solves problem time domn |I|O(1) (resp. 2n |I|O(1) ). CSP (resp. CNF-S AT)solvable nonuniform subexponential time = 1/`, ` positive integer,exists algorithm solves problem time domn |I|O(n) (resp. 2n |I|O(1) ) (that is,algorithm depends ). note problem admits subexponential-time algorithm (uniformnonuniform) means improve exponent exponential-termrunning time algorithm indefinitely.Let Q Q0 two problems, let 0 two functions defined instances QQ0 , respectively, assigning instance corresponding problem parameter value.case CSP CNF-S AT, 0 assign number variables instancesproblems. subexponential-time Turing reduction family (Impagliazzo, Paturi & Zane, 2001; seealso Flum & Grohe, 2006) serf-reduction 1 short, algorithm oracle Q0computable functions f, g : N N satisfying: (1) given pair (I, ) Q= 1/` (` positive integer), decides time f (1/)dom(I) |I|O(1) (for CNF-Sdom = 2); (2) oracle queries form 0 Q0 posed input (I, ),0 (I 0 ) g(1/)((I) + log |I|).optimization class SNP consists search problems expressible second-order existentialformulas whose first-order part universal (Papadimitriou & Yannakakis, 1991). Impagliazzo, Paturi,Zane (2001) introduced notion completeness class SNP serf-reductions,identified class problems complete SNP serf-reductions,subexponential-time solvability problems implies subexponential-time solvabilityproblems SNP. Many well-known NP-hard problems proved SNP-completeserf-reduction, including 3-S AT, V ERTEX C OVER, NDEPENDENT ET, extensiveefforts made last three decades develop subexponential-time algorithmssuccess. fact led exponential-time hypothesis, ETH, equivalentstatement SNP problems solvable subexponential time:Exponential-Time Hypothesis (ETH): problem k-CNF-S AT, k 3, cannot solvedtime 2o(n) , n number variables input formula. Therefore, existsc > 0 k-CNF-S cannot solved time 2cn .following result implied, using standard technique renaming variables (Impagliazzo,Paturi & Zane, 2001, Corollary 1, 2) proof Sparsification Lemma (Impagliazzo,Paruri & Zane, 2001; Flum & Grohe, 2006, Lemma 16.17). sake completeness, providesketch aforementioned results literature combined give statementlemma.Lemma 1. k-CNF-S (k 3) solvable 2o(n) time k-CNF-S linearnumber clauses number occurrences variable 3 solvabletime 2o(n) , n number variables formula (note size instancek-CNF-S polynomial n). particular, choosing k = 3 get: 3-CNF-S everyvariable occurs 3 times, denoted 3-3-S AT, solvable 2o(n) time unless ETH fails.1. Serf-reductions introduced Impagliazzo, Paturi, Zane (2001). use definition given FlumGrohe (2006). slight difference two definitions, latter definition flexiblepurposes.211fiDEH AAN , K ANJ , & ZEIDERProof. shown Impagliazzo et al. (2001, Corollary 1, 2) that, k 3,serf-reduction k-CNF-S k-CNF-S number clauses linearnumber variables n. instance k-CNF-S = O(n), total numberoccurrences variables also linear n (because width clause k).variable appears ` > 3 times, using standard technique renamingvariables, replace (rename) ` occurrences new variable, add cycle` implications (using ` new 2-CNF-S clauses) enforcing ` new variables receivevalue satisfying assignment. resulting formula k-CNF-S formulanumber occurrences variable 3, number new variableslinear original number variables n. gives serf-reduction k-CNF-S (fork 3) k-CNF-S number occurrences variable 3 (and hencealso linear number clauses).ETH become standard hypothesis complexity theory (Lokshtanov et al., 2011).Remark 1. paper, consider CSP (with without global constraints) restrictedinstances certain parameter (g(n)) (resp. (g(n)), O(g(n)), o(g(n))),proper complexity function g(n) number variables n instance, mean CSP restrictedinstances parameter upper bounded prespecified function (g(n))(resp. (g(n)), O(g(n)), o(g(n))). example, say CSP restricted instances whoseprimal treewidth o(n) solvable subexponential time mean following: propercomplexity function g(n) = o(n), problem consisting restriction CSP instanceswhose primal treewidth g(n) solvable subexponential time.3. CSP CSPcsection investigate subexponential-time complexity CSP CSPc respectrestrictions various structural parameters. start Subsection 3.1 establishing relationsamong subexponential-time complexity CNF-S AT, CSP, CSPc ; resultscorner stones results subsequent (sub)sections rely upon.3.1 Relations Among CSP, CSPc , CNF-Sstart following simple observation:Observation 1. positive integer constant r, serf-reduction r-CSP r-CSPcvice versa. Moreover, reductions produces instance setvariables domain values original instance.fact serf-reduction r-CSP r-CSPc trivially follows factr-CSP special case r-CSPc . opposite direction, observe cTable constraintbounded arity decompressed table constraint, set variables,polynomial time enumerating tuples satisfy cTable constraint. polynomialtime serf-reduction r-CSPc r-CSP.Proposition 1. B OOLEAN r-CSP, r 3, solvable subexponential timeETH fails.212fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSPProof. prove first part statement, give serf-reduction r-CNF-SB OOLEAN r-CSP. Given instance F r-CNF-S AT, easy see correspondevery clause F constraint arity r (over variables) containing 2rtuples clause satisfied corresponding constraint is. Clearly,polynomial-time reduction results instance B OOLEAN r-CSP variable-setF , hence serf-reduction.prove converse, give serf-reduction B OOLEAN r-CSP r-CNF-S AT. Letinstance B OOLEAN r-CSP. construct instance F r-CNF-S follows. Let Cconstraint I. Since arity r, C contains r variables 2r tuples.associate C set clauses F , width r, C satisfiedassociated clauses are. easily done considering tuplevariable-set C contained C, adding clause F consisting disjunctionnegation set literals tuple represents. Since tuple represents conjunctionset r literals, results 2r clauses, disjunctionr literals. Clearly, F instance r-CNF-S variable-set I, Fcomputable polynomial time. proof follows.following proposition suggests Proposition 1 may extend r-CSP unboundeddomain size. Chen, Chor, Fellows, Huang, Juedes, Kanj, Xia (2005) showed C LIQUE(decide whether given graph N vertices contains complete subgraph k vertices) solvabletime N o(k) ETH fails. converse, however, generally believed true.idea behind proof following proposition goes back paper PapadimitriouYannakakis (1999), used context studying complexity database queries.provide proof completeness.Proposition 2. 2-CSP solvable subexponential time C LIQUE solvable time N o(k) .Proof. Assume 2-CSP solvable time domo(n) , let (G, k) instance C LIQUE,G N vertices. Assume vertices G labeled {1, . . . , N }. constructinstance 2-CSP follows. variable-set {x1 , . . . , xk }, variables rangedomain {1, . . . , N }; is, variables used select vertices G formclique (if exists). every pair distinct variables xi , xj , < j, add constraint Cijcontaining pairs/tuples form (u, v) uv edge G u < v.difficult verify G clique k vertices consistent. Sincek variables dom = N , follows I, hence (G, k), decided time N o(k) .proof follows.Observation 1, statement Proposition 1 holds true B OOLEAN r-CSPc ,statement Proposition 2 holds true 2-CSPc .explore next relation B OOLEAN CSP unbounded arity CNF-S AT.show B OOLEAN CSP solvable nonuniform subexponential time CNF-S AT.so, exhibit nonuniform subexponential-time Turing reduction CNF-S B OOLEANCSP.Intuitively, one would try reduce instance F CNF-S instance CSPassociating every clause F constraint whose variables variables clause,whose relation consists tuples satisfy clause. slight complicationattempted reduction number tuples constraint could exponential213fiDEH AAN , K ANJ , & ZEIDERnumber variables corresponding clause linear (in total number variables).overcome subtlety, idea first apply subexponential-time (Turing) reduction,originally due Schuler (2005) also used analyzed Calabro, Impagliazzo, Paturi(2006), reduces instance F subexponentially many (in n) instances widthclause constant k; case, however, reduce width suitablenonconstant value. follow reduction reduction B OOLEAN CSP describedproof Proposition 1.Theorem 1. B OOLEAN CSP nonuniform subexponential-time algorithmCNF-S AT.Proof. Suppose B OOLEAN CSP solvable nonuniform subexponential time. every> 0, exists algorithm A0 that, given instance B OOLEAN CSP n0 variables,00A0 solves time 2n |I|c , constant c0 > 0.Let 0 < < 1 given. describe algorithm solves CNF-S time 2n mO(1) .nSet k = b 2(1+c0 ) c. Let F instance CNF-S n variables clauses. algorithmsearch-tree algorithm, works follows. algorithm picks clause C F widthk; clause exists algorithm stops. Let l1 , . . . , lk k literals C.algorithm branches C two branches. first branch, referred left branch, correspondsone k literals assigned value 1 satisfying assignment sought,case C replaced F clause (l1 . . . lk ), thus reducing number clauses Fwidth k 1. second branch, referred right branch, corresponds assigningk literals value 0 satisfying assignment sought; case valuesvariables corresponding literals determined, variables removedF F gets updated accordingly. Therefore, right branch number variables Freduced k. execution part algorithm described far depicted binarysearch tree whose leaves correspond instances resulting F end branching,clause width k. running time part algorithm proportionalnumber leaves search tree, equivalently, number root-leaf paths searchtree.continue description algorithm , illustrate branching phasealgorithm following concrete example. Suppose F instance CNF-S6 variables {x1 , . . . , x6 } consisting 3 clauses C1 , C2 , C3 , C1 = {x1 , x2 , x3 , x4 , x5 },C2 = {x2 , x3 , x5 , x6 }, C3 = {x1 , x3 , x4 , x5 , x6 }. Suppose want reduce formulawidth 3 (i.e., k = 3). pick clause width 3, say C1 , branch 3literals C1 , say x1 , x2 , x3 . left branch (at least one 3 literals 1) obtain(CNF) formula F1 consisting 3 clauses {x1 , x2 , x3 }, C2 , C3 ; right branch (eachliterals assigned 0), obtain formula F2 consisting clause {x4 , x5 } (C2 C3satisfied case). Note branch anymore F2 since width 2. Since F1still contains clauses width 3, namely C2 C3 , branch F2 pickingclause width 3, say C3 , branching 3 literals C3 , say x1 , x3 , x4 . leftbranch, obtain formula F1,1 consisting 3 clauses {x1 , x2 , x3 }, C2 , {x1 , x3 , x4 };right branch obtain formula F1,2 consisting 2 clauses {x2 , x5 , x6 } {x5 , x6 }.branch F1,2 since width 3. Since F1,1 contains clause C2 width 3,branch 3 literals C2 , say x2 , x3 , x5 . left branch obtain formula F1,1,1 consisting3 clauses {x1 , x2 , x3 }, {x2 , x3 , x5 }, {x1 , x3 , x4 }; branch F1,1,1 since214fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSPFF1F1,1F2F1,2F1,1,1 F1,1,2Figure 1: search tree corresponding branching algorithm example.width 3. right branch obtain formula F1,1,2 consisting two clauses {x1 , x4 }{x6 }; branch F1,1,2 . algorithm branch anymore since leavessearch tree formulas width 3. Figure 1 depicts search tree correspondingbranching example.continue description algorithm . Let F 0 instance resulting Fleaf search tree. reduce F 0 instance 0 B OOLEAN CSP follows.clause C 0 F 0 , correspond constraint whose variable-set set variables C 0 ,whose tuples consist 2k 1 tuples corresponding assignments variables C 0satisfy C 0 . Clearly, 0 constructed time 2k mO(1) (note number clausesF 0 m). instance 0 , apply algorithm A0 = /2. algorithmaccepts F A0 accepts one instances 0 , F 0 resulting F leafsearch tree.illustrate phase algorithm using example above, formulas F2 ,F1,2 , F1,1,1 , F1,1,2 , corresponding leaves search tree (see Figure 1), associateinstance B OOLEAN CSP. example, instance B OOLEAN CSP associated F1,2consists two constraints. first constraint corresponds clause {x2 , x5 , x6 }; (x2 , x5 , x6 )sequence variables,{(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (1, 0, 1)} relation. second constraint corresponds clause {x5 , x6 } F1,2 ; (x5 , x6 ) sequence variables,{(0, 0), (0, 1), (1, 0)} relation.running time upper bounded number leaves search tree, multipliedpolynomial length F (polynomial m) corresponding (maximum) total runningtime along root-leaf path search tree, multiplied time construct instance 0corresponding F 0 leaf tree, multiplied running time algorithm A0applied 0 . Note binary search tree depicting execution algorithmcomplete binary tree. upper bound size search tree, let P root-leaf pathsearch tree, let ` number right branches along P . Since right branch removes kvariables, ` n/k number variables left instance F 0 leaf endpoint Pn `k. Noting length path ` right branches + ` (each left branchreduces 1 hence branches P , ` right branches),conclude number root-leaf paths, hence number leaves, search treePdn/ke`=0 m+`.`215fiDEH AAN , K ANJ , & ZEIDERreduction F 0 instance B OOLEAN CSP carried time 2k mO(1) ,results instance 0 number variables n0 = n `k, numberconstraints m, total size 2k mO(1) . Summing possible pathssearch tree, running time 2n mO(1) . consequence following estimation:dn/keX`=0+ ` k O(1) (n`k) k O(1) c022.(2)`2(1+c0 )k+n2(1+c0 )k+nO(1)dn/keX`=0mO(1)+ dn/ke`2mdn/ke02(1+c )k+n mO(1) (2m)n/k2(1+c0 )k+nn2O(1)O(1)(1)(2)(3).first inequality follows replacing ` larger value dn/ke upper partbinomial coefficient, upper bounding term 2`k 1. Inequality(1) follows2mfact largest binomial coefficient summation m+dn/ke(mdn/ke,dn/kedn/keotherwise constant, instance CNF-S solved polynomial timebeginning), hence, summation replaced largest binomial coefficient multipliednumber terms (dn/ke+1) summation, gets absorbed term mO(1) .Inequality (2) follows trivial upper bound binomial coefficient (the ceilingremoved polynomials get absorbed). Inequality (3) follows noting n/kconstant (depends ), substituting k values/bounds.follows algorithm solves CNF-S time 2n mO(1) . Therefore, B OOLEANCSP nonuniform subexponential-time algorithm, CNF-S AT. algorithmnonuniform polynomial factor running time (exponent m) depends .Theorem 1 provides strong evidence B OOLEAN CSP solvable subexponentialtime. show next B OOLEAN CSPc solvable subexponential time weakerhypothesis assumed Theorem 1. SAT[3] denote satisfiability normalizedpropositional formulas depth 3 (Flum & Grohe, 2006), is, propositional formulasconjunction-of-disjunction-of-conjunction literals. well known SAT[3] solvablesubexponential time W -hierarchy parameterized complexity collapses secondlevel (Chen et al., 2006), is, W [2] = FPT, consequence deemed unlikelywould imply CNF-S solvable subexponential time (Chen et al., 2006).Proposition 3. Unless W [2] = FPT, B OOLEAN CSPc solvable subexponential time.Proof. easy see instance SAT[3] polynomial-time reducible instanceB OOLEAN CSPc set variables. reduction, every disjunction-of-conjunctionliterals Boolean formula associated cTable constraint, compressed tuple(V1 , . . . , Vr ) constraint represents conjunction literals: positive literal xi representedVi = {1}, negative literal xi represented Vi = {0}, variable xi occurconjunction, represented Vi = {0, 1}. Therefore, serf-reduction SAT[3]B OOLEAN CSPc . statement follows result Chen et al. (2006).216fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP3.2 Instance Size Number Tuplessection give characterizations subexponential-time complexity CSP CSPcrespect instance size number tuples. also show subexponential-timesolvability B OOLEAN CSP B OOLEAN CSPc linear size, linear number tuples,equivalent statement ETH fails.Proposition 4. Unless ETH fails, restriction B OOLEAN CSP instances whose size(n) solvable subexponential time.Proof. Let s(n) = (n) cn proper complexity function, c > 0 constant. Supposerestriction B OOLEAN CSP instances size s(n) solvable subexponentialtime, show 3-CNF-S solvable subexponential time. Lemma 1,sufficient show 3-CNF-S linear number clauses solvable 2o(n) time. Usingpadding argument2 , prove preceding statement assuming linear upper boundnumber clauses; true pad instance 3-CNF-S largenumber new variables obtain equivalent instance number clauses satisfies(smaller) desired upper bound. pick linear upper bound cn/24, c constantupper bound s(n).Let F instance 3-CNF-S n variables cn/24 clauses. reduce Finstance B OOLEAN CSP using reduction described proof Theorem 1:clause C F correspond constraint whose variables C whose tuplescorresponding satisfying assignments C. Since width C 3 numberclauses cn/24, instance consists cn/24 constraints, containing3 variables 8 tuples. Therefore, size cn. apply hypotheticalsubexponential-time algorithm . Since |I| linear n, since reduction takes linear timen, conclude 3-CNF-S solvable time 2o(n) nO(1) = 2o(n) . proof follows.Since B OOLEAN CSP special case B OOLEAN CSPc , statement Proposition 4 holdstrue B OOLEAN CSPc well.Proposition 5. restriction CSPc instances o(n) tuples solvable subexponentialtime.Proof. Let s(n) = o(n) proper complexity function, consider restriction CSPcinstances s(n) tuples. show problem solvable time doms(n) |I|O(1) .Let instance problem consideration. Consider algorithm that,compressed tuple constraint I, branches whether compressed tuple satisfiedsatisfying assignment sought. branch one compressed tupleconstraint selected satisfied rejected, likewise branch compressedtuple constraint selected. remaining branch, algorithm checks branchconsistent, would imply assignment variables aligns branchsatisfies I. Checking branch consistent done follows. Let x variable I, lett1 , . . . , tp compressed tuples selected branch cTables contain x variable.2. padding argument general tool used complexity theory extend result larger class problems.purpose paper, padding argument works adding/padding dummy part instance createequivalent new instance relation holds true certain parameters new instance. usepadding argument several times paper, skip details argument clear.217fiDEH AAN , K ANJ , & ZEIDERLet Vix , = 1, . . . , p, set values admissible xTin cTable ti selectedbranch. branch consistent respect x pi=1 Vix 6= , branch consistentconsistent respect every variable I. Clearly, given branch algorithm A,checking whether branch consistent done polynomial time |I|.branch consistent, algorithm accepts; algorithm rejects branch correspondsconsistent assignment. Clearly, algorithm correct, runs time 2s(n) |I|O(1) =doms(n) |I|O(1) (we assume dom 2, otherwise problem trivial).Noting number tuples lower bound instance size, following propositionfollows Proposition 4 Proposition 5:Proposition 6. restriction CSP instances number tuples o(n) solvablesubexponential time, unless ETH fails, restriction CSP instancesnumber tuples (n) solvable subexponential time. holds true CSPc .Next, show subexponential-time solvability B OOLEAN CSP linear size,linear number tuples, equivalent statement ETH fails. first needfollowing proposition:Proposition 7. ETH fails restriction B OOLEAN CSPc instances linearnumber tuples solvable subexponential time.Proof. give polynomial-time serf-reduction B OOLEAN CSPc linear number tuplesC IRCUIT ATISFIABILITY linear size circuits. result follow factC IRCUIT ATISFIABILITY linear size circuits SNP-complete serf-reductions (and hencesolvable subexponential time ETH fails) (Impagliazzo, Paturi & Zane, 2001).Let s(n) cn proper complexity function, c > 0 constant. Consider restrictionB OOLEAN CSPc instances number tuples cn, let instanceproblem. construct Boolean circuit CI follows. circuit CI depth-3 circuitwhose output gate AND-gate, whose set variables I.cTable constraint correspond OR-gate gT connected output gate C. LetcTable constraint Boolean variables (v1 , . . . , vr ), let = (V1 , . . . , Vr )compressed tuple . correspond AND-gate gt CI connected OR-gategT corresponding CI ; input gt literals CI determined follows.vi , = 1, . . . , r, Vi = {1} connect variable corresponding vi CI gt ,Vi = {0} connect negation variable corresponding vi CI gt (we nothingVi = {0, 1} constraint imposed tuple Boolean value vi ).easy see satisfied corresponding gate gt CI evaluates 1, hencesatisfied gT evaluates 1. follows satisfied CI is. Moreover,size CI linear number tuples I, subsequently number variablesCI . Since construction CI done polynomial time, proof follows.Clearly, statement proposition holds true B OOLEAN CSP well.Proposition 4, combined Proposition 7 noting size upper boundnumber tuples, gives following results:Theorem 2. restriction B OOLEAN CSP instances linear number tuples solvablesubexponential time ETH fails. result holds B OOLEAN CSPc .218fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSPTheorem 3. restriction B OOLEAN CSP instances linear size solvable subexponential time ETH fails. result holds B OOLEAN CSPc .3.3 Number Constraints Treewidthsection give characterizations subexponential-time complexity CSP CSPcrespect number constraints, treewidth primal incidence graphs.start withe following proposition:Proposition 8. Unless ETH fails, restriction CSP instances numberconstraints (1) solvable subexponential time.Proof. Let (n) = (1) proper complexity function. show that, unless ETH fails,restriction CSP instances cons (n), denoted CSP solvable domo(n)time. Proposition 1, suffices provide serf-reduction B OOLEAN 3-CSP linearnumber constraints B OOLEAN CSP .Let instance B OOLEAN CSP cons = n0 cn, c > 0 constant. LetC1 , . . . , Cn0 constraints I; partition constraints arbitrarily b(n)c many groupsC1 , . . . , Cr , r b(n)c, containing dn0 /(n)e constraints. serf-reductionworks follows. merges constraints group Ci , = 1, . . . , r, one constraintCi0 follows. variable-set Ci0 consists union variable-sets constraintsCi . constraint C Ci , iterate tuples C. selecting tupleconstraint Ci , check selected tuples consistent, merge tuplessingle tuple add Ci0 . merging tuples mean form single tuple variablestuples, value variable value selected tuples (notevalues consistent). Since constraint arity 3, hence contains8 tuples, since group contains dn0 /(n)e constraints, Ci0 constructed0time 8dn /(n)e n0O(1) = 2o(n) , hence, constraints C10 , . . . , Cr0 constructedtime 2o(n) nO(1) = 2o(n) . form instance 0 whose variable-set I, whoseconstraints C10 , . . . , Cr0 . Since r b(n)c, 0 instance CSP . Moreover, easy seeconsistent 0 is. Since 0 constructed subexponential timenumber variables 0 I, follows serf-reduction B OOLEAN3-CSP linear number constraints CSP .Proposition 9. restriction CSPc instances cons = O(1) solvable polynomialtime.Proof. number constraints instance O(1), polynomial time enumeratesubset tuples subset contains exactly one compressed tuple constraintinstance (because size subset O(1)). verify consistency (asdescribed proof Proposition 5), deduce instantiation set variables existspolynomial time.Clearly, Proposition 8 holds true CSPc , Proposition 9 holds true CSP. Therefore,combining Proposition 8 Proposition 9 have:Theorem 4. restriction CSP instances O(1) constraints solvable polynomialtime, unless ETH fails, restriction CSP instances (1) constraintssolvable subexponential time. holds true CSPc .219fiDEH AAN , K ANJ , & ZEIDERturn attention treewidth. following proposition:Proposition 10. Unless CSP (in general) solvable subexponential time (and hence ETHfails), restriction CSP instances whose tw (n) solvable subexponential time.Proof. Let s(n) = cn, c > 0 constant, consider restriction CSP instanceswhose tw s(n), denoted L INEAR -tw-CSP. Note number vertices primalgraph n, hence tw n. Therefore, c 1, statement trivially follows. Supposec < 1, let instance CSP n variables. padding d1/ce disjoint copiesobtain instance 0 equivalent I, whose number variables N 0 = d1/cen,whose tw I. Since tw n, follows tw 0cN 0 , hence 0 instance L INEAR -tw-CSP. gives serf-reduction CSPL INEAR -tw-CSP.note hypothesis CSP solvable subexponential time theoremimplies ETH fails Proposition 1, implies CNF-S nonuniformsubexponential-time algorithm Theorem 1.following theorem provides tight characterization subexponential-time complexityCSPc (and CSP) respect primal incidence treewidth.Theorem 5. following statements true:(i) restriction CSPc instances tw = o(n) solvable subexponential time,unless ETH fails, restriction CSPc instances tw = (n)solvable subexponential time.(ii) restriction CSPc instances tw = O(1) solvable subexponential time(even P), unless ETH fails, restriction CSPc instances tw = (1)solvable subexponential time.Proof. (i) Note upper bound primal treewidth implies upper boundarity. Let instance CSPc whose tw = o(n). Since arity = o(n), constraint containsd(n)o(n) many satisfying tuples. decompressing compressed tuples, i.e., enumeratingsatisfying tuples constraint time (d(n)o(n) ) reduce instanceinstance CSP set variables, domain, primal tree width.compute tree decomposition width 4 tw time 24.38tw |I|O(1) (Amir, 2010). wellknown (Freuder, 1990) CSP solvable time (d(n)tw ) (d(n)o(n) ), hencedecided subexponential time. hardness result follows hardness resultCSP Proposition 10.(ii) hardness result direct consequence hardness result Theorem 4, since consupper bound tw . Establishing first statement requires work. Consider instanceCSPc whose incidence treewidth constant w.apply construction Samer Szeider (2010) transform equivalent instance0 CSPc whose incidence treewidth w + 1 variable appearsscope 3 constraints. construction keeps constraints adds binary equalityconstraints copies variables. equality constraints enforce variable copiesget assigned value. construction Samer Szeider stated table constraints220fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSPclearly works also cTable, since constraints changed all, newlyintroduced constraints binary.Consider dual graph Gd 0 vertices constraints 0 , twoconstraints joined edge share least one variable.variable appears scope 3 constraints, result Samer Szeider (2010,Lemma 2(5)) applies, based construction due Kolaitis Vardi (2000),follows treewidth Gd 2w + 2.Next obtain CSP instance 00 dual instance 0 . constructionstraightforward generalization known construction CSP table constraints (see, e.g.,Dechter, 2003, Definition 2.1). constraint C = (S, U ) 0 gives rise variable x[C] 00 ;domain D(x[C]) U , set compressed tuples. two variables x[C1 ], x[C2 ]00 corresponding constraints C1 = (S1 , U1 ) C2 = (S2 , U2 ), respectively, 0 shareleast one variable add binary table constraint ((x[C1 ], x[C2 ]), R). Here, relation R containspairs (t1 , t2 ) U1 U2 consistent sense variables x appearscopes C1 C2 , coordinate Vi1 t1 corresponding x coordinate Vj2t2 corresponding x nonempty intersection. straightforward see 0 00equivalent. remains observe Gd isomorphic primal graph 00 , henceprimal treewidth 00 2w + 2, constant. Hence solve 00 polynomial time (Freuder,1990).Clearly results Theorem 5 hold true CSP since positive results theoremshown general CSPc , negative results proved CSP.note difference subexponential-time complexity CSPc (and CSP)respect two structural parameters tw tw : Whereas threshold functionsubexponential-time solvability CSPc CSP respect tw o(n), threshold functionrespect tw O(1).3.4 Degree Aritysection give characterizations subexponential-time complexity CSP CSPcrespect degree arity.Proposition 11. Unless ETH fails, restriction CSP instances whose deg 2solvable subexponential time.Proof. statement follows proof Theorem 1 noting that, Lemma 1, oneuse 3-3-S reduction. result instances B OOLEAN 3-CSP degree3 well. variable x degree 3 instance B OOLEAN 3-CSP, introduce twonew variables x0 , x00 , add constraint whose variables {x, x0 , x00 }, containing twotuples (0, 0, 0) (1, 1, 1); constraint stipulates values x, x0 , x00 same.substitute variable x one constraints appears x0 , another constraintappears x00 . Therefore, new instance, degree x, x0 , x00 becomes 2.repeating step every variable degree 3, obtain instance B OOLEAN 3-CSPdegree variable 2. Since increase number variables linear,reduction serf-reduction 3-3-S B OOLEAN 3-CSP degree 2,gives statement proposition.221fiDEH AAN , K ANJ , & ZEIDERProposition 12. Unless ETH fails, restriction CSP instances whose arity 2 (anddom 3) solvable subexponential time.Proof. give serf-reduction 3-C OLORABILITY problem CSP arity = 2dom = 3. Since 3-C OLORABILITY problem SNP-complete serf-reductions (Impagliazzo, Paturi & Zane, 2001), statement theorem follow. Recall 3C OLORABILITY problem asks vertices given graph properly colored (no twoadjacent vertices assigned color) 3 colors.reduction folklore. Given instance G = (V, E) 3-C OLORABILITY, Gn vertices, construct instance CSP follows. variables correspondvertices G, domain corresponds color-set {1, 2, 3}. every edge graphconstruct constraint arity = 2 two variables corresponding endpointedge. constraint contains tuples corresponding valid colorings endpoints edge.easy see G 3-coloring consistent. Since instancevars = n, number vertices G, since arity = 2 dom = 3,(polynomial-time) serf-reduction 3-C OLORABILITY problem CSP arity = 2dom = 3.Clearly, Proposition 11 Proposition 12 hold true CSPc well.note CSPc CSP dom = 2 arity = 2 solvable polynomial time viasimple reductions 2-CNF-S AT.turns out, CSP CSPc exhibit subexponential-time complexity behaviorrespect restrictions structural parameters considered above.hand, negative result proved Proposition 3 B OOLEAN CSPc assumes weaker hypothesisresult B OOLEAN CSP proved Theorem 1.4. CSP6= , CSP= , CSP , CSPsection consider CSP6= , CSP= , CSP , CSP . Since results CSP= , CSP ,CSP related, rely results establish CSP6= , start presentingresults CSP6= .4.1 CSP6=Let instance CSP6= constraints C1 , . . . , Cc integer c > 0, setvariables {x1 , . . . , xn }. Denote Di , = 1, . . . , n, domain xi .Proposition 13. CSP6= solved time (2n ).Proof. reduce instance instance L IST C OLORING problem. RecallL IST C OLORING problem given graph, whose vertices associated listcolors, asked decide exists proper coloring graphvertex assigned color list. reduce instance L IST C OLORING, constructgraph G whose vertices x1 , . . . , xn (without loss generality, label vertices Gcorresponding variables names I) edge two verticesxi xj , 1 < j n, xi xj appear together constraint I.vertex xi G, associate list colors Li = Di . difficult see222fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSPyes-instance CSP6= graph G proper list coloring. known L ISTC OLORING problem solvable time (2n ) (Bjrklund, Husfeldt, & Koivisto, 2009), henceCSP6= .Corollary 1. Let d(n) = (1) proper complexity function. restriction CSP6= instancesdom d(n) solvable subexponential time.Proof. Let d(n) = (1) proper complexity function, consider restriction CSP6=instances dom d(n). Proposition 13, CSP6= solvable time (2n ) =(d(n)n/ log (d(n)) ) (domo(n) ).note result may sound strange, especially taken conjunctionnext proposition, implies problem becomes easier larger domain size.explained fact domain size gets large, allowable upper boundsubexponential time solving problem (i.e., d(n)o(n) ) gets larger well.Corollary 1, focus investigation subexponential-time complexity CSP6=instances dom = O(1) = d, integer constant d. Note dom upperbound arity constraint must arity dom (otherwise cannot satisfied).2, constraint arity 2, CSP6= case reduces 2-CNF-S AT,P. Therefore, assume remainder section 3.Proposition 14. Unless ETH fails, restriction CSP6= instances dom = 3cons = (n) solvable subexponential time.Proof. suffices prove result cons = s(n), s(n) specific functions(n) linear n ((n)), result would extend using padding argument functionlinear n (we add new dummy variables new dummy constraints newvariables make relation constraints variables satisfy desired functions()).Lemma 1, 3-3-S solvable subexponential time unless ETH fails. standardpolynomial-time reduction 3-S 3-C OLORABILITY (see Cormen et al., 2009), establishingNP-hardness 3-C OLORABILITY, reduces instance 3-S n variables clausesinstance 3-C OLORABILITY O(n + m) vertices O(n + m) edges. Therefore,use reduction start 3-3-S instead 3-S AT, end instance3-C OLORABILITY number vertices O(n) number edges O(n) well.Hence, serf-reduction 3-3-S restriction 3-C OLORABILITY instanceswhose size linear number vertices, denoted L INEAR -3-C OLORABILITY. usestandard reduction 3-C OLORABILITY CSP6= (in vertex becomes variable,edge becomes constraint arity 2, domain set 3 colors), insteadstart instance L INEAR -3-C OLORABILITY, obtain instance CSP6= n variables(the number vertices graph), linear number constraints, domain sizedom = 3. Therefore, previous reduction serf-reduction L INEAR -3-C OLORABILITYrestriction CSP6= instances number constraints linear, dom = 3.Composing two serf-reductions gives serf reduction 3-3-S problemconsideration, thus proves proposition.Remark 2. note phrase statement Corollary 1 consider restrictionCSP6= instances dom = (1) (as paper)223fiDEH AAN , K ANJ , & ZEIDERrestriction encompass slice problem hard (instances whose domain size upperbounded constant), shown Proposition 14. explicitly consider instanceswhose domain size lower-bounded function (1). Proposition 17, next section,handled similarly.Remark 3. consider restriction CSP6= instances cons = o(n)dom = O(1). constraint must arity dom, hence, cons = o(n)would follow total number variables o(n). follows Proposition 14Corollary 1 provide tight characterizations subexponential-time complexity CSP6=respect cons dom.following proposition provides tight characterization subexponential-time complexityCSP6= respect treewidth primal graph:Proposition 15. restriction CSP6= instances tw = o(n) solvable subexponential time, unless ETH fails, restriction CSP6= instances tw = (n)solvable subexponential time.Proof. derive subexponential-time result, assume domain size d,constant 3, case get CSP6= solvable subexponential timeCorollary 1. Let instance CSP6= treewidth primal graph o(n). Sincearity constraint domain size d, polynomial timereduce instance CSP set variables, domain, constraints,primal treewidth. part (i) Theorem 5, restriction CSP instances whose tw = o(n)solvable subexponential time, hence decided subexponential time.hardness result follows general observation primal treewidth CSPinstances. First note number variables n upper bound primal treewidth;is, tw n. Therefore, upper bound s(n) = (n) tw, using padding argument (addinglinear number dummy new variables singleton constraints increase primaltreewidth) reduce general instance CSP6= instance tw s(n) costlinear increase number variables instance size. provides serf-reductiongeneral instance CSP6= instance tw s(n) = (n). result followsresult CSP6= general instances (implied, e.g., Proposition 14).well known tw arity (tw 1) tw tw + 1 (Kolaitis & Vardi, 2000).arity = O(1), tw tw within multiplicative constant one another. Therefore,Proposition 15 infer following tight result:Proposition 16. restriction CSP6= instances tw = o(n) solvable subexponential time, unless ETH fails, restriction CSP6= instances tw = (n)solvable subexponential time.Remark 4. several width parameters CSP even general twsense instances tw small, width parameter small well;instances width parameter small tw arbitrarilylarge. Prominent examples width parameters hypertree width (Gottlob et al., 2002)submodular width (Marx, 2013). lower bound statement Proposition 16 clearly carriesgeneral width parameters. holds true lower bound statementsProposition 19 Theorem 5.224fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP4.2 CSP= , CSP , CSPstart presenting exact algorithm CSP ; reducing CSP CSP6= . useexample illustrated Figure 2 running example explain idea behind reduction.example, instance CSP consists three constraints C1 , C2 , C3 , variablesC1 x1 , x2 , x3 , x4 , variables C2 x4 , x5 , variables C3 x1 , x5 , x6 , x7 .domain x1 {a, b}, domain x2 x3 {b}, domain x4 {b, c}, domainx5 {a}, domain x6 x7 {d, e}. number distinct values needassigned variables C1 least 3, variables C2 least 2, variablesC3 least 3.solution (i.e., assignment variables domain values) instance CSP ,constraint C I, possible several variables C assigned valuesolution (in running example forced assign x2 x3 value b). Therefore,attempt straightforward reduction CSP CSP6= produces instance I,solution instance CSP may solution instance CSP6= .possible happens due fact variables removedwithout affecting satisfiability I, solution constraintstill satisfied without considering values assigned variables.algorithm starts trying subset variables subset existssolution variables essential solution; algorithm removes(nonessential) variables, updates instance, works toward finding solutionassumption resulting instance. (In running example, remove x3 C1 ;see Venn diagram left Figure 2.) Even assumption, still possiblesolution resulting instance, two variables constraint C assignedvalue. One cannot simply ignore (remove) one variables basis removingaffect satisfiability C, removed variable may contribute satisfiabilityconstraint C, variable appears well. (In running example,forced assign x1 x5 value, would violate constraint C3 CSP6= .)Therefore, resulting instance, even though may satisfiable instance CSP , maysatisfiable instance CSP6= . However, shown Lemma 2, possibleinstance reassign variable subset constraints appears in,reassignment/repartitioning variable contributes satisfiability constraintappears in. reassignment, resulting instance CSP becomes equivalentinstance CSP6= . (In running example, variable x5 contributing C3 , safelyreassigned C2 ; see Venn diagram right Figure 2.) proceed formalproofs.Let instance CSP constraints C1 , . . . , Cc integer value c > 0,variables x1 , . . . , xn . Let ni , = 1, . . . , c, nonnegative integer associatedconstraint Ci .Denote Di , = 1, . . . , n, domain variable xi , let = ni=1 Di . Set k = |D|.consider Ci , = 1, . . . , c, set consisting variables Ci , draw Venndiagram Ci s, Venn diagram consists 2c many nonempty regions,region Rj , j = 1, . . . , s, defined intersection sets containingvariables lie Rj Venn diagram. solution instance I, call variable xiessential (to S) discounting value assigned xi violates least one constraints(containing xi ), hence longer gives solution I. clear enumerating every225fiDEH AAN , K ANJ , & ZEIDERC20C2x5x4x2C1x5x4x1 x6 x7x2x1 x6 x7C10C3C30Figure 2: Illustration example reduction CSP CSP6= .subset variables I, takes O(2n ) time, work assumptionlooking solution every variable essential S. Since working instanceCSP , adding nonessential variables solution afterwards (and assigning valuesdomains) hurt solution. Therefore, without loss generality, assumevariables x1 , . . . , xn essential solution sought (if exists). startfollowing lemma.Lemma 2 (The Repartitioning Lemma). Let instance CSP . solutioninstance 0 set variables I, whose constraintsC10 , . . . , Cc0 , that:(1) variables Ci0 subset Ci , = 1, . . . , c;(2) numbers n1 , . . . , nc 0 ;(3) solution 0 satisfying every value v, two distinct variablesxi , xj assigned value v solution 0 , set constraints xi belongs0 disjoint xj belongs 0 .Proof. Suppose solution S; discussion preceding lemma, assumeevery variable essential S. define instance 0 set variables follows.constants n1 , . . . , nc remain 0 . define constraints 0 sequencechanges performed constraints I; initially constraints 0 identical I.every value v assigned variable solution S, let x1v , . . . , x`v variablesassigned value v S. xjv , j = 1, . . . , ` 1, considered listed order, let Cvjjset constraints containing xjv 0 , let Cv,union constraints containingj+1jj`variables xv , . . . , xv . Remove xv constraint Cvj Cv,.0claim solution solution satisfies conditionsstatement lemma. First, construction constraints 0 , value vsolution, set constraints containing variable assigned value v mutually disjointvariable xiv (i < `) assigned value v removed constraint`0subsequent variable xi+1v , . . . , xv contained in. Moreover, constraint Ci obtained0Ci (possibly) removing variables Ci , Ci Ci , = 1, . . . , c. Finally,226fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSPvariable xiv assigned value v removed constraint Cj0 , removalaffect number different values assigned variables Cj0 S; knowsure subsequent variable xpv , p {i + 1, . . . , `}, assigned value vremain Cj0 , namely variable xpv maximum index p appears Cj0 .Conversely, Ci0 subset Ci , = 1, . . . , c, easy see solution0 also solution I.Theorem 6. CSP solved time ((2(cons + 1) + 1)n ).Proof. Let instance CSP constraints C1 , . . . , Cc integer c > 0,variables x1 , . . . , xn . Let ni , = 1, . . . , c, nonnegative integer associated constraint Ci .first enumerate subset variables {x1 , . . . , xn } subset essential variablessolution sought. Fix enumerated subset X, remove variables I,update instance accordingly (i.e., update constraints); without loss generality, stillrefer resulting instance I.Lemma 2, solution instance 0 setvariables I, whose constraints C10 , . . . , Cc0 , that: (1) variables Ci0 form subsetCi , = 1, . . . , c, (2) numbers n1 , . . . , nc 0 , (3)solution 0 satisfying every value v, two distinct variables xi , xjassigned value v solution 0 , set constraints xi belongs 0 disjointxj belongs 0 .find instance 0 , try every possible partitioning variables X cconstraints determine new constraints C10 , . . . , Cc0 0 . partitioningCi0 Ci least ni variables Ci0 , = 1, . . . , c, form instance CSP6= setvariables X set constraints C10 , . . . , Cc0 , invoke algorithm CSP6= describedProposition 13 instance; algorithm returns solution return solutionsolution I. enumerated subset X enumerated partitioning algorithmCSP6= rejects, reject instance I.easy see correctness algorithm. Clearly, solution CSP6=instance solution 0 , hence I. constraint containsleast ni variables, must receive ni distinct values solution CSP6= instance, hencesatisfying constraint Ci satisfying I. hand, solution,enumerated partitioning variables X correspond constraints 0 .solution 0 satisfies properties (1)-(3) Lemma 2, two variablesconstraint 0 receive value v solution (by property (3)). Therefore,solution also solution constructed instance CSP6= . shows correctnessalgorithm.running time algorithm time taken enumerate subsets variables,subset X, time enumerate partitions X c constraints, finallypartition time taken invoke PCSP6= algorithmresulting instance. numbersubsets variables {x1 , . . . , xn } ni=0 ni . subset cardinality i,2ci many ways partitioning c constraints. Finally, instance variables,CSP6= algorithm takes (2i ) time. Putting everything together, overall running timealgorithm polynomial factor multiplied by:227fiDEnXni=0ciH AAN , K ANJ , & ZEIDER2 2 =nXni=02(c+1)i = (2(c+1) + 1)n .Therefore, running time algorithm ((2(cons + 1) + 1)n ) claimed.Corollary 2. restriction CSP instances cons = O(1) solvable (2O(n) )time.Corollary 3. restriction CSP instances cons = o(log dom) solvablesubexponential time.Proof. result follows Theorem 6 noticing cons = o(log dom) 2cons =domo(1) .Proposition 17. Let d(n) = (1) proper complexity function. restriction CSPinstances cons = O(1) dom d(n) solvable subexponential time, unlessETH fails, restriction CSP instances cons = (n) (even dom = O(1))solvable subexponential time.Proof. positive result follows Corollary 3. hardness result follows hardnessresult CSP6= Proposition 14 (CSP6= special case CSP ).NP-hardness reduction CSP= single constraint (and linear domain size), givenBessiere et al. (2007), also works CSP , actually serf-reduction 3-CNF-S AT.implies that, unless ETH fails, neither CSP= CSP , restricted instances singleconstraint dom = O(n), solvable subexponential time. show next resultrestrictions CSP CSP= instances constant domain size linear numberconstraints:Theorem 7. restrictions CSP CSP= instances dom = O(1) cons =(n) solvable subexponential time, unless ETH fails.Proof. give serf-reduction 3-3-S CSP ; result follow Lemma 1.serf-reduction also works case CSP= . Take instance 3-3-S n variables.construct polynomial time instance CSP , cons = O(n) dom = O(1)yes-instance 3-3-S AT. proceed two steps: firstly, modifywell-known polynomial-time reduction 3-S V ERTEX C (Garey & Johnson, 1979)reduction 3-3-S CSP , resulting instance cons = O(n) dom = O(n);secondly, transform instance CSP equivalent instance CSP cons = O(n)dom = O(1).start first step. Let consist clauses c1 , . . . , cm , ci = l1i l2i l3i1 m. well-known reduction V ERTEX C produces graph G =(V, E), containing vertices vx , vx variable x occurring , vertex vji literaloccurrence, 1 1 j 3. variables vx vx adjacent,variable x, vertices v1i , v2i , v3i form triangle, 1 m. Moreover,edge vji vl , l = lji . satisfiable G vertexcover consisting n + 2m vertices. specifically, satisfiable G228fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSPvertex cover containing exactly one vertex vx , vx variable x exactly two verticesv1i , v2i , v3i 1 m. construct instance CSP follows.edge e = {v1 , v2 } E, introduce variable ue domain {v1 , v2 }. Then, clause ci ,define set Eci consist edges v1i , v2i , v3i , vji vli vlijjvli , 1 j 3. Then, add constraint ensuring variables uejnine e Eci take 5 different values. assignments variables ue satisfyconstraints exactly correspond vertex covers G containing exactly one vertex vx , vxvariable x exactly two vertices v1i , v2i , v3i 1 m. particularvertex covers, turn, correspond exactly truth assignments (which set one x, x true,variable x) satisfying . construction constraint illustrated Figure 3.v1iv3iv1jv2iv2jv3jvx1 vx1 vx4 vx4 vx5 vx5 vx6 vx6 vx7 vx7Figure 3: CSP constraints corresponding example clauses ci = (x1 x4 x5 ) cj =(x5 x6 x7 ). Variables denoted , values . constraints indicateddashed lines. nine variables constraint must assigned 5 differentvalues. double lines indicate assignment variables satisfying constraintcorresponds truth assignment {x1 7 >, x4 7 , x5 7 >, x6 7 >, x7 7 }.second step, transform instance CSP way dom = O(1). orderso, use following observation. Whenever two vertices v1 , v2 V propertyconstraint containing variable ue1 edge e1 incident v1variable ue2 edge e2 incident v2 , safely identify domain values v1v2 instance CSP . Consequently, identify many domain values v11 , . . . , v1msingle value, similarly identify domain values v21 , . . . , v2m v31 , . . . , v3m . Next,reduce dom even more, identify number domain values vx (and similarlyidentify complementary values vx other). Consider primal graph , i.e.,graph Gp containing vertices variables two vertices x, x0 adjacentx x0 occur together clause (positively negatively). Since variable occurs 3times , know maximum degree Gp bounded 8. Then, BrooksTheorem (Brooks, 1941), know exists proper coloring Gp 9 colors,coloring computed linear time. Take proper coloring c Gp . Now,color b used coloring c, let Xb Var() set variables x c(x) = b.Then, since c proper coloring primal graph Gp , know color b twovariables x, x0 Xb occur together clause . Therefore, color 1 b 3safely identify domain values vx x Xb instance CSP ,similarly safely identify domain values vx x Xb other. resultsequivalent instance CSP cons = O(n) dom = O(1).229fiDEH AAN , K ANJ , & ZEIDERnext consider subexponential-time complexity CSP= , CSP , CSP respectprimal treewidth. following tight result:Proposition 18. restriction CSP= , CSP , CSP instances tw = o(n)solvable subexponential time, unless ETH fails, restriction CSP= , CSP ,CSP instances tw = (n) solvable subexponential time.Proof. proof proposition CSP= , CSP , CSP exactlyproof Proposition 15.Finally, following hardness result CSP= CSP respect tw followsProposition 16 since CSP6= special case CSP= CSP :Proposition 19. Unless ETH fails, restriction CSP= (resp. CSP ) instancestw = (n) solvable subexponential time.5. Conclusionprovided first analysis subexponential-time complexity CSP extensionallyrepresented constraints CSP global constraints, latter focusing instancescomposed fundamental global constraints AllDifferent, AtLeastNValue, AtMostNValue,cTable, respectively. results show detailed complexity landscape problemsvarious natural structural restrictions. cases, able obtain tight bounds exactlydetermine borderline classes instances solved subexponential time,existence subexponential-time algorithms unlikely. severalways extending current work considering global constraints, combinationdifferent global constraints, structural restrictions primal incidence graphs.ReferencesAlber, J., Fernau, H., & Niedermeier, R. (2004). Parameterized complexity: exponential speed-upplanar graph problems. Algorithmica, 52(1), 2656.Amir, E. (2010). Approximation algorithms treewidth. Algorithmica, 56(4), 448479.Bckstrm, C., & Jonsson, P. (2011). pspace-complete planning problems equalequal others. Borrajo, D., Likhachev, M., & Lpez, C. L. (Eds.), ProceedingsFourth Annual Symposium Combinatorial Search, SOCS 2011, Castell de Cardona,Barcelona, Spain, July 15.16, 2011. AAAI Press.Beigel, R., & Eppstein, D. (2005). 3-coloring time O(1.3289n ). J. Algorithms, 54(2), 168204.Beldiceanu, N., Carlsson, M., & Rampon, J.-X. (2006). Global constraint catalog. Tech.rep. T2005:08, SICS, SE-16 429 Kista, Sweden. On-line version http://www.emn.fr/xinfo/sdemasse/gccat/.Benhamou, B., Paris, L., & Siegel, P. (2012). Dealing satisfiability n-ary CSPs logicalframework. Journal Automated Reasoning, 48(3), 391417.Bennaceur, H. (2004). comparison SAT CSP techniques. Constraints, 9(2), 123138.230fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSPBessiere, C., Hebrard, E., Hnich, B., Kiziltan, Z., & Walsh, T. (2006). Filtering algorithmsNValue constraint. Constraints, 11(4), 271293.Bessiere, C., Hebrard, E., Hnich, B., & Walsh, T. (2007). complexity reasoning globalconstraints. Constraints, 12(2), 239259.Bjrklund, A., Husfeldt, T., & Koivisto, M. (2009). Set partitioning via inclusion-exclusion. SIAM J.Comput., 39(2), 546563.Brooks, R. L. (1941). colouring nodes network. Mathematical ProceedingsCambridge Philosophical Society, 37, 194197.Calabro, C., Impagliazzo, R., & Paturi, R. (2006). duality clause width clause densitySAT. 21st Annual IEEE Conference Computational Complexity (CCC 2006), 16-20July 2006, Prague, Czech Republic, pp. 252260. IEEE Computer Society.Chen, H., & Grohe, M. (2010). Constraint satisfaction succinctly specified relations. J.Computer System Sciences, 76(8), 847860.Chen, J., Kanj, I., Perkovic, L., Sedgwick, E., & Xia, G. (2007). Genus characterizes complexitycertain graph problems: tight results. Journal Computer System Sciences,73(6), 892907.Chen, J., Chor, B., Fellows, M., Huang, X., Juedes, D., Kanj, I. A., & Xia, G. (2005). Tight lowerbounds certain parameterized NP-hard problems. Information Computation, 201(2),216231.Chen, J., Huang, X., Kanj, I. A., & Xia, G. (2006). Strong computational lower bounds via parameterized complexity. J. Computer System Sciences, 72(8), 13461367.Chen, J., Kanj, I. A., & Xia, G. (2009). parameterized exponential time complexity. TheoreticalComputer Science, 410(27-29), 26412648.Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction Algorithms (Thirdedition). MIT Press, Cambridge, MA.Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.Demaine, E., Fomin, F., Hajiaghayi, M., & Thilikos, D. (2005). Subexponential parameterizedalgorithms bounded-genus graphs H-minor-free graphs. J. ACM, 52, 866893.Dimopoulos, Y., & Stergiou, K. (2006). Propagation CSP SAT. Benhamou, F. (Ed.),Principles Practice Constraint Programming - CP 2006, 12th International Conference,CP 2006, Nantes, France, September 25-29, 2006, Proceedings, Vol. 4204 Lecture NotesComputer Science, pp. 137151. Springer Verlag.Feder, T., & Motwani, R. (2002). Worst-case time bounds coloring satisfiability problems. J.Algorithms, 45(2), 192201.Fellows, M. R., Fomin, F. V., Lokshtanov, D., Rosamond, F., Saurabh, S., Szeider, S., & Thomassen, C.(2011a). complexity colorful problems parameterized treewidth. InformationComputation, 209(2), 143153.Fellows, M. R., Friedrich, T., Hermelin, D., Narodytska, N., & Rosamond, F. A. (2011b). Constraintsatisfaction problems: Convexity makes alldifferent constraints tractable. Walsh, T. (Ed.),IJCAI 2011, Proceedings 22nd International Joint Conference Artificial Intelligence,Barcelona, Catalonia, Spain, July 16-22, 2011, pp. 522527. IJCAI/AAAI.231fiDEH AAN , K ANJ , & ZEIDERFlum, J., & Grohe, M. (2006). Parameterized Complexity Theory, Vol. XIV Texts TheoreticalComputer Science. EATCS Series. Springer Verlag, Berlin.Freuder, E. C. (1982). sufficient condition backtrack-bounded search. J. ACM, 29(1),2432.Freuder, E. C. (1990). Complexity k-tree structured constraint satisfaction problems. Shrobe,H. E., Dietterich, T. G., & Swartout, W. R. (Eds.), Proceedings 8th National ConferenceArtificial Intelligence. Boston, Massachusetts, July 29 - August 3, 1990, 2 Volumes, pp. 49.AAAI Press / MIT Press.Garey, M. R., & Johnson, D. R. (1979). Computers Intractability. W. H. Freeman Company,New York, San Francisco.Gaspers, S., & Szeider, S. (2014). Guarantees limits preprocessing constraint satisfactionreasoning. Artificial Intelligence, 216, 119.Ge, R. (2013). Provable Algorithms Machine Learning Problems. Ph.D. thesis, PrincetonUniversity.Gottlob, G., Leone, N., & Scarcello, F. (2002). Hypertree decompositions tractable queries. J.Computer System Sciences, 64(3), 579627.Grandoni, F., & Italiano, G. F. (2006). Algorithms constraint programming. Benhamou,F. (Ed.), Principles Practice Constraint Programming - CP 2006, 12th InternationalConference, CP 2006, Nantes, France, September 25-29, 2006, Proceedings, Vol. 4204Lecture Notes Computer Science, pp. 214. Springer Verlag.Grohe, M. (2006). structure tractable constraint satisfaction problems. Kralovic, R., &Urzyczyn, P. (Eds.), Mathematical Foundations Computer Science 2006, 31st InternationalSymposium, MFCS 2006, Star Lesn, Slovakia, August 28-September 1, 2006, Proceedings,Vol. 4162 Lecture Notes Computer Science, pp. 5872. Springer Verlag.de Haan, R., Kanj, I., & Szeider, S. (2014). Subexponential time complexity CSP globalconstraints. Proceedings CP 2014, 20th International Conference PrinciplesPractice Constraint Programming, Lyon, France, September 8-12, 2014. Springer Verlag.Hnich, B., Kiziltan, Z., & Walsh, T. (2004). Combining symmetry breaking constraints:Lexicographic ordering sums. AI&M 1-2004, Eighth International SymposiumArtificial Intelligence Mathematics, January 4-6, 2004, Fort Lauderdale, Florida, USA.van Hoeve, W.-J., & Katriel, I. (2006). Global constraints. Rossi, F., van Beek, P., & Walsh, T.(Eds.), Handbook Constraint Programming, chap. 6. Elsevier.Impagliazzo, R., & Paturi, R. (2001). complexity k-SAT. J. Computer SystemSciences, 62(2), 367375.Impagliazzo, R., Paturi, R., & Zane, F. (2001). problems strongly exponential complexity?. J. Computer System Sciences, 63(4), 512530.Jeavons, P., & Petke, J. (2012). Local consistency SAT-solvers. J. Artif. Intell. Res., 43, 329351.Jonsson, P., Lagerkvist, V., & Nordh, G. (2013). Blowing holes various aspects computationalproblems, applications constraint satisfaction. Schulte, C. (Ed.), PrinciplesPractice Constraint Programming - 19th International Conference, CP 2013, Uppsala,232fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSPSweden, September 16-20, 2013. Proceedings, Vol. 8124 Lecture Notes Computer Science,pp. 398414. Springer Verlag.Kanj, I., & Szeider, S. (2013). subexponential time complexity CSP. ProceedingsTwenty-Seventh AAAI Conference Artificial Intelligence. AAAI Press.Katsirelos, G., & Walsh, T. (2007). compression algorithm large arity extensional constraints.Bessiere, C. (Ed.), Principles Practice Constraint Programming - CP 2007, 13thInternational Conference, CP 2007, Providence, RI, USA, September 23-27, 2007, Proceedings,Vol. 4741 Lecture Notes Computer Science, pp. 379393. Springer Verlag.Kolaitis, P. G., & Vardi, M. Y. (2000). Conjunctive-query containment constraint satisfaction. J.Computer System Sciences, 61(2), 302332. Special issue Seventeenth ACMSIGACT-SIGMOD-SIGART Symposium Principles Database Systems (Seattle, WA,1998).Kutz, M., Elbassioni, K., Katriel, I., & Mahajan, M. (2008). Simultaneous matchings: hardnessapproximation. J. Computer System Sciences, 74(5), 884897.Kwisthout, J., Bodlaender, H. L., & van der Gaag, L. C. (2010). necessity bounded treewidthefficient inference Bayesian networks. Coelho, H., Studer, R., & Wooldridge, M. (Eds.),ECAI 2010 - 19th European Conference Artificial Intelligence, Lisbon, Portugal, August16-20, 2010, Proceedings, Vol. 215 Frontiers Artificial Intelligence Applications, pp.237242. IOS Press.Lokshtanov, D., Marx, D., & Saurabh, S. (2011). Lower bounds based exponential timehypothesis. Bulletin European Association Theoretical Computer Science, 105,4172.Marx, D. (2010). beat treewidth?. Theory Computing, 6, 85112.Marx, D. (2013). Tractable hypergraph properties constraint satisfaction conjunctive queries.J. ACM, 60(6), Art. 42, 51.Moser, R. A., & Scheder, D. (2011). full derandomization Schnings k-SAT algorithm.STOC11Proceedings 43rd ACM Symposium Theory Computing, pp. 245251.ACM, New York.Pachet, F., & Roy, P. (1999). Automatic generation music programs. Jaffar, J. (Ed.), PrinciplesPractice Constraint Programming - CP99, 5th International Conference, Alexandria,Virginia, USA, October 11-14, 1999, Proceedings, Vol. 1713 Lecture Notes ComputerScience, pp. 331345. Springer Verlag.Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.Papadimitriou, C. H., & Yannakakis, M. (1991). Optimization, approximation, complexityclasses. J. Computer System Sciences, 43(3), 425440.Papadimitriou, C. H., & Yannakakis, M. (1999). complexity database queries. J.Computer System Sciences, 58(3), 407427.Razgon, I. (2006). Complexity analysis heuristic CSP search algorithms. Hnich, B., Carlsson,M., Fages, F., & Rossi, F. (Eds.), Recent Advances Constraints, Joint ERCIM/CoLogNETInternational Workshop Constraint Solving Constraint Logic Programming, CSCLP233fiDEH AAN , K ANJ , & ZEIDER2005, Uppsala, Sweden, June 20-22, 2005, Revised Selected Invited Papers, Vol. 3978Lecture Notes Computer Science, pp. 8899. Springer Verlag.Rgin, J.-C. (1994). filtering algorithm constraints difference CSPs. Hayes-Roth, B.,& Korf, R. E. (Eds.), Proceedings 12th National Conference Artificial Intelligence,Seattle, WA, USA, July 31 - August 4, 1994, Volume 1, pp. 362367. AAAI Press / MITPress.Rgin, J.-C. (1995). Dveloppement doutils algorithmiques pour lIntelligence Artificielle. Ph.D.thesis, Montpellier II. French.Rgin, J.-C. (2011). Global constraints: survey. van Hentenryck, P., & Milano, M. (Eds.),Hybrid Optimization: Ten Years CPAIOR, Vol. 45 Optimization Applications,chap. 3, pp. 63134. Springer Verlag.Rgin, J.-C., & Rueher, M. (2000). global constraint combining sum constraint differenceconstraints. Dechter, R. (Ed.), Principles Practice Constraint Programming - CP2000, 6th International Conference, Singapore, September 18-21, 2000, Proceedings, Vol.1894 Lecture Notes Computer Science, pp. 384395. Springer Verlag.Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006). Handbook Constraint Programming. Elsevier.Samer, M., & Szeider, S. (2010). Constraint satisfaction bounded treewidth revisited. J.Computer System Sciences, 76(2), 103114.Schning, U. (1999). probabilistic algorithm k-SAT constraint satisfaction problems.40th Annual Symposium Foundations Computer Science (New York, 1999), pp. 410414.IEEE Computer Soc., Los Alamitos, CA.Schuler, R. (2005). algorithm satisfiability problem formulas conjunctive normalform. J. Algorithms, 54(1), 4044.Traxler, P. (2008). time complexity constraint satisfaction. Grohe, M., & Niedermeier, R.(Eds.), Parameterized Exact Computation, Third International Workshop, IWPEC 2008,Victoria, Canada, May 14-16, 2008. Proceedings, Vol. 5018 Lecture Notes ComputerScience, pp. 190201. Springer Verlag.234fiJournal Artificial Intelligence Research 52 (2015) 97-169Submitted 05/14; published 01/15Deterministic Oversubscription Planning Heuristic Search:Abstractions ReformulationsCarmel DomshlakVitaly Mirkisdcarmel@ie.technion.ac.ilmirkis80@gmail.comFaculty Industrial Engineering & Management,Technion - Israel Institute Technology,Haifa, IsraelAbstractclassical planning objective achieve one equally attractive goalstates low total action cost possible, objective deterministic oversubscriptionplanning (OSP) achieve valuable possible subset goals within fixedallowance total action cost. Although numerous applications various fields sharelatter objective, substantial algorithmic advances made deterministicOSP. Tracing key sources progress classical planning, identify severe lackeffective domain-independent approximations OSP.focus optimal planning, goal bridge gap. Two classesapproximation techniques found especially useful context optimalclassical planning: based state-space abstractions based logical landmarks goal reachability. question study whether similar-in-spirit,yet possibly mathematically different, approximation techniques developed OSP.context abstractions, define notion additive abstractions OSP, studycomplexity deriving effective abstractions rich space hypotheses, revealsubstantial, empirically relevant islands tractability. context landmarks,show standard goal-reachability landmarks certain classical planning taskscompiled OSP task interest, resulting equivalent OSP task lowercost allowance, thus smaller search space. empirical evaluation confirmseffectiveness proposed techniques, opens wide gate developmentsoversubscription planning.1. Introductiontools automated action planning allow autonomous systems selecting courseaction get things done. Deterministic planning probably basic, thusfundamental, setting automated action planning (Russell & Norvig, 2009).viewed problem finding trajectories interest large-scale yet conciselyrepresented state-transition systems. Computational approaches deterministic planningvary around way trajectories interest defined.basic structure acting situations underconstrained overconstrainedresources respectively captured days called classical deterministicplanning (Fikes & Nilsson, 1971), Smith (2004) termed oversubscriptiondeterministic planning (OSP). classical planning, task find cost-effectivetrajectory possible goal-satisfying state. oversubscription planning, taskfind goal-effective (or valuable) state possible via cost-satisfying trajectory.c2015AI Access Foundation. rights reserved.fiDomshlak & Mirkisoptimal classical planning optimal OSP, tasks constrained findingcost-effective trajectories goal-effective states, respectively. Classicalplanning OSP viewed foundational variants deterministic planning,many variants, net-benefit planning cost-bounded planning, definedterms mixing relaxing two.1OSP extensively advocated years, theory practiceclassical planning studied advanced much intensively. remarkablesuccess continuing progress heuristic-search solvers classical planning one notable example. Primary enablers success advances domain-independentapproximations, heuristics, cost needed achieve goal state given state.thus possible similarly rich palette effective heuristic functionsOSP would advance state art problem.Two classes approximation techniques found especially useful context optimal classical planning: based state-space abstractions (Edelkamp, 2001;Haslum, Botea, Helmert, Bonet, & Koenig, 2007; Helmert, Haslum, Hoffmann, & Nissim,2014; Katz & Domshlak, 2010a) based logical landmarks goal reachability (Karpas & Domshlak, 2009; Helmert & Domshlak, 2009; Domshlak, Katz, & Lefler,2012; Bonet & Helmert, 2010; Pommerening & Helmert, 2013). Considering OSP heuristic search, question whether similar-in-spirit, yet possibly mathematicallydifferent, approximation techniques developed heuristic-search OSP. precisely question study here.Starting basic question state-space abstractions OSP actually are, show notion abstraction differs substantially classical planning OSP. Hence, first define (additive) abstractions abstractionheuristics OSP. investigate computational complexity derivingeffective abstraction heuristics scope homomorphic abstraction skeletons,paired cost, value, budget partitions. Along revealing significantislands tractability, study exposes interesting interplay knapsackstyle problems combinatorial optimization, continuous convex optimization,certain principles borrowed explicit abstractions classical planning.introduce study -landmarks, logical properties OSP plans achievevaluable states. show -landmarks correspond regular goal-reachabilitylandmarks certain classical planning tasks straightforwardly derivedOSP tasks interest. show -landmarks compiledback OSP task interest, resulting equivalent OSP task,stricter cost satisfaction constraint, thus smaller effective search space.Finally, show landmark-based task enrichment combinedmutually stratifying way best-first branch-and-bound search used OSPplanning, resulting incremental procedure interleaves search landmarkdiscovery. entire framework independent OSP planner specifics,particular, heuristic functions employs.1. connections differences popular variants deterministic planning discussedSection 2.98fiOn Oversubscription Planning Heuristic Searchempirical evaluation large set OSP tasks confirms effectiveness proposed techniques. Moreover, best knowledge, implementation constitutesfirst domain-independent solver optimal OSP, hope advancesimportant computational problem follow.work revision extension formulations results presentedauthors ICAPS-2013 ECAI-2014 (Mirkis & Domshlak, 2013, 2014). paperstructured follows. Section 2 formulate general model deterministic planning,define several variants deterministic planning terms model, and, particular,show oversubscription planning differs conceptually classical planning,also popular variants deterministic planning net-benefit planningcost-bounded planning. also specify simple model representation languageOSP, well provide essential background heuristic search, and, particular,OSP heuristic search. Sections 3 4 devoted, respectively, abstractionsabstraction approximations OSP. Section 5 devoted exploiting reachabilitylandmarks OSP tasks. Section 6 conclude discuss promising directionsfuture work. sake readability, proofs relegated Appendix A,details empirical results relegated Appendix B.2. Backgroundmentioned introduction, specific variants deterministic planning differway interest preference trajectories defined. instance, classicalplanning (Fikes & Nilsson, 1971), trajectory interest connects designated initialstate one designated goal states, preference towards trajectorieslower total cost transitions along them. Among other, non-classical variantsdeterministic planningoversubscription planning (Smith, 2004), topic interest here;net-benefit planning (van den Briel, Sanchez, Do, & Kambhampati, 2004; Sanchez& Kambhampati, 2005; Baier, Bacchus, & McIlraith, 2009; Bonet & Geffner, 2008;Benton, Do, & Kambhampati, 2009; Coles & Coles, 2011; Keyder & Geffner, 2009);cost-bounded (also known resource-constrained) planning (Haslum & Geffner, 2001;Hoffmann, Gomes, Selman, & Kautz, 2007; Gerevini, Saetti, & Serina, 2008; Thayer &Ruml, 2011; Thayer, Stern, Felner, & Ruml, 2012; Haslum, 2013; Nakhost, Hoffmann,& Muller, 2012);planning preferences temporal properties trajectories (Baier et al.,2009; Gerevini, Haslum, Long, Saetti, & Dimopoulos, 2009; Benton, Coles, & Coles,2012).Interestingly, working paper, learned quite differentvariants deterministic planning often collectively referred oversubscriptionplanning. result, difference terms expressiveness necessarily clear, thus, relationship alreadydone collective sense oversubscription planning always apparent.issue address first.99fiDomshlak & Mirkis2.1 ModelsAdopting extending notation Geffner Bonet (2013), view manyvariants deterministic planning, including classical planning, well many popularnon-classical variants, special cases state model= hS, s0 , u, O, , c, Qi(1)with:finite set states S,initial state s0 S,state value function u : 7 R0+ {},operators O(s) applicable state S,deterministic state transition function (s, o) s0 = (s, o) standsstate resulting applying O(s) s,operator cost function c : R0+ ,quality measure Q : P 7 R {}, P (infinite) set trajectoriess0 along operators O. trajectory P sequence operators ho1 , . . . ,o1 O(s0 ) and, inductively, oi O(((. . . (s0 , o1 ) . . . , oi2 ), oi1 )).model, trajectory P solution, preference towards solutionshigher quality.P follows, sJK stands end-state trajectory appliedstate s, c() = c(o) additive cost . Likewise, graphical skeletonGM = hS, , Oi model refers edge-annotated, unweighted digraph inducednodes GM states S, edge labels operators O,contains edge s0 labeled iff O(s) s0 = (s, o).First, consider quality measureQ+ () = u(sJK) c().(2)measure assumes state values operator costs comparable, thus represents tradeoff value end-state cost trajectory. Considerfragment state model (1), instances quality measure Q+ ,instance, value function(,Sgoalu(s) =(3), otherwisepartitions state space Sgoal S, u takes finite value 0,rest states, u takes value . Finding optimal solutioninstance fragment corresponds finding shortest path s0 single nodeedge-weighted digraph G, obtained GM (i) annotating edgeslatter costs c, (ii) adding dummy node zero-cost edges100fiOn Oversubscription Planning Heuristic SearchconstraintpreferenceNet BenefitOversubscriptionconstraintEnd-state valueAction costpreferenceClassicalCost-boundedFigure 1: Schematic classification four deterministic planning models along strictnessapproach cost operator sequences valueoperator sequence end-states. White blocks planning modelssolved single-source single-target shortest path problems.goal nodes Sgoal . specified non-canonical way, fragmenteasily verified correspond model classical planning, Sgoal classicalplanning goal states.Staying quality measure Q+ removing requirement u complyEq. 3, obtain fragment generalizes classical planning, constitutesbasic model called net-benefit planning (Sanchez & Kambhampati, 2005). Importantly, instance fragment reduced finding shortest pathsingle node s0 single node edge-weighted digraph G, obtained GM(i) annotating edges GM costs c, (ii) adding dummy node andPedgesnodes , (ii) setting cost new edge (s, ) s0 S\{s} u(s0 ).reduction works net-value maximization end state equivalentminimization net-loss giving possible end states.basic idea underlies Keyder Geffners (2009) scheme compiling certain standard representation formalisms net-benefit planning standard classical planningformalism.2Consider alternative quality measure(u(sJK), c() bQ () =,,otherwiseb(4)2. worth noting wost-case complexity equivalence classical planning net-benefitplanning shown prior work Keyder Geffner (2009) van den Briel et al. (2004).However, equivalence prescriptive enough suggest practically effective compilationscompactly represented net-benefit planning tasks classical planning tasks.101fiDomshlak & Mirkisb R0+ predefined bound cost trajectories. fragmentbasic model, instances characterized quality measure Qbvalue functions Eq. 3, constitutes model called costbounded planning (Thayer & Ruml, 2011). well, finding optimal solutionproblem instance corresponds finding shortest path s0 edge-weighteddigraph G, derived GM identically case classical planning.3 This,particular, explains natural heuristic-search methods cost-boundedplanning exploit heuristics developed classical planning (Haslum, 2013).arrive fourth fragment basic model. Staying quality measure Qb removing requirement u comply Eq. 3, obtain fragmentgeneralizes cost-bounded planning, constitutes model oversubscription planning (Smith, 2004). illustrated Figure 1, hard constraint classical planningtranslates soft preference OSP, hard constraint OSP translates soft preference classical planning. However, contrast cost-optimal, net-benefit, classicalplanning, fragment appear reducible single-source single-targetshortest path problem. terms digraph G obtained GM annotatingedges costs c, finding optimal solution instance oversubscription planningrequires (i) finding shortest paths s0 states u(s) > 0, (ii) filteringstates reachable s0 within cost allowance b, (iii)selecting remaining states state maximizes u.contrast oversubscription planning three popular variantsdeterministic planning discussed least two important implications. First,single shortest path searched using best-first forward search procedures, searching shortest paths numerous targets simultaneously requires different,exhaustive, forward search framework branch-and-bound. Second, net-benefitcost-bounded planning clearly potential (directly indirectly) reuse richtoolbox heuristic functions developed years classical planning. contrast,due differences underlying computational model, necessarilytrue oversubscription planning, examining prospects heuristic functionsOSP precisely focus work here.2.2 Notationk N+ , [k] denote set {1, 2, . . . , k}. indicator function subsetset X function 1A : X {0, 1} defined 1A (x) = 1 x 1A (x) = 0 x 6 A.Following Nebel (2000), talk size mathematically well-defined objectx, symbolically ||x||, mean size (reasonable) encoding x. assignmentvariable v value denoted hv/di; often refer single variable assignmentspropositions.3. Strictly speaking, shortest path s0 found, still checkedcost bound b. test, however, local , problem solving finishes independently testsoutcome.102fiOn Oversubscription Planning Heuristic Search2.3 Model RepresentationDeparting general model oversubscription planning, follows restrict attention instances model compactly representable languageclose sas+ language classical planning (Backstrom & Klein, 1991; Backstrom &Nebel, 1995). language, deterministic oversubscription planning (OSP) taskgiven sextuple= hV, s0 , u; O, c, bi,(5)(1) V = {v1 , . . . , vn } finite set finite-domain state variables, completeassignment V representing state, = dom(v1 ) dom(vn ) statespace task;(2) s0 designated initial state;(3) u efficiently computable state value function u : R0+ ;(4) finite set operators, operator represented pairhpre(o), eff(o)i partial assignments V , called preconditions effects o, respectively;(5) c : R0+ operator cost function;(6) b R0+ cost budget allowed task.consider semantics task description termsbasic model.ffOSP task = hV, s0 , u; O, c, bi said induce model = S, s0 , u, O, , c, Qb ,Qb quality measure (4) instantiated budget b, transitionfunction specified follows. partial assignment p V , let V(p) V denotesubset variables instantiated p, and, v V(p), p[v] denote value providedp variable v. Similarly classical planning semantics sas+ , operatorapplicable state iff s[v] = pre(o)[v] v V(pre(o)). Applying changesvalue v V(eff(o)) eff(o)[v], resulting state denoted sJoK.notation defined applicable s. Denoting empty sequence operators ,applying sequence operators ho1 , . . . , om state defined inductively sJK :=sJo1 , . . . , oj K := sJo1 , . . . , oj1 KJoj K. operator sequence called s-planapplicable state Qb () 6= , is, c() b.auxiliary notation used later on: OSP task = hV, s0 , u; O, c, bi,= vV dom(v) denote union (uniquely labeled) state-variable domains.state proposition hv/di D, hv/di used shortcut notations[v] = d.example simple OSP task Figure 2 used illustrate model representation. example, truck initially location A, drive (only) locationlocation B location B location C. Two packages, x y, initiallylocation B. package truck location, packageloaded onto truck, package truck, unloaded103fiDomshlak & MirkisxBC(a)oipre(oi )eff(oi )driveABi=1{ht/Ai}{ht/Bi}driveBCi=2{ht/Bi}{ht/Ci}loadBxi=3{ht/Bi , hx/Bi}{hx/T i}loadByi=4{ht/Bi , hy/Bi}{hy/T i}unloadCxi=5{ht/Ci , hx/T i}{hx/Ci}oipre(oi )eff(oi )unloadBxi=6{ht/Bi , hx/T i}{hx/Bi}unloadByi=7{ht/Bi , hy/T i}{hy/Bi}unloadCyi=8{ht/Ci , hy/T i}{hy/Ci}loadCxi=9{ht/Ci , hx/Ci}{hx/T i}loadCy= 10{ht/Ci , hy/Ci}{hy/T i}(b)u=1CBBo2ABBo1BBBBTBo6o3o7o4BBTCTBo9o5CCBBTTo2CTTo2o7o4o3o6o2o10o8u=1o10o8CBCCTBo9o5CCBBTTo2CTTCBTu=1o9o5CTCu=1CCTo9 u=2o5CCCo10o8(c)u=1CBBo2ABBo1BBBBTBo6o3o7o4BBTo2o7o4o3o6o2o10o8u=1CBTo10o8u=1o9o5CTCu=1CCTo9 u=2o5CCCo10o8CBC(d)Figure 2: simple running example OSP task, (a) illustrating story, (b) listing operators, (c)-(d) depicting graphical skeleton induced statemodel; (c) shows region graphical skeleton GM structurallyreachable initial state ABB, grayed area (d) correspondssub-region cannot reached initial state budgetb = 4.trucks current location. (drive, load, unload) operator task costs one unitcost, cost budget set four units cost. Finally, value one (value unit)earned package present location C.OSP task described using three state variables V = {t, x, y},dom(t) = {A, B, C} dom(x) = dom(y) = {A, B, C, }, corresponding possiblelocations truck two packages, respectively.operator set ffO = {o1 , . . . , o10 }detailed Figure 2(b). state model = S, s0 , u, O, , c, Qb induced104fiOn Oversubscription Planning Heuristic SearchBFBB ( = hV, s0 , u; O, c, bi)open := new max-heap ordered f (n) = h(shni, b g(n))initialize best solution n := make-root-node(s0 )open.insert(n )closed:= ;best-cost:= 0open.empty()n := open.pop-max()f (n) u(shn i): breaku(shni) > u(shn i): update n := nshni 6 closed g(n) < best-cost(shni):closed:= closed {shni}best-cost(shni) := g(n)foreach O(shni):n0 := make-node(shniJoK, n)g(n0 ) > b f (n0 ) u(shn i): continueopen.insert(n0 )return nFigure 3: Best-first branch-and-bound (BFBB) search OSPtask, = dom(t)dom(x)dom(y), initial state s0 = ABB (with three letters names states capturing three components domain cross-product),operator cost c(oi ) = 1 operators oi , cost budget b = 4, state values1, {?AC, ?BC, ?CA, ?CB}u(s) = 2, {?CC}0, otherwise.graphical skeleton GM depicted Figures 2(c) 2(d): Figure 2(c) showsregion graphical skeleton GM structurally reachable initial stateABB, grayed area Figure 2(d) corresponds sub-region cannotreached initial state budget b = 4.2.4 OSP Heuristic Searchtwo major ingredients heuristic-search planner search algorithmheuristic function. classical planning, heuristic typically function h : R0+{}, h(s) estimating cost h (s) optimal s-plans. heuristic h admissiblelower-bounding, is, h(s) h (s) states s. common heuristic searchalgorithms optimal classical planning, , require admissible heuristics.contrast, heuristic OSP function h : R0+ R0+ , h(s, b) estimatingvalue h (s, b) optimal s-plans cost budget b. heuristic h admissibleupper-bounding, is, h(s, b) h (s, b) states cost budgets b. well,105fiDomshlak & Mirkissearch algorithms optimal OSP, best-first branch-and-bound (BFBB),4 requireadmissible heuristics pruning search branches without violating solution optimality.Figure 3 depicts pseudo-code description BFBB OSP; shni denotes stateassociated search node n, cost-so-far g(n) total cost action sequenceassociated n. Unlike , order nodes selected OPENlist affect optimality guarantees (though may, course, seriously affectempirical efficiency search). Figure 3, ordering OPEN correspondsdecreasing order h(shni, b g(n)). duplicate detection reopening mechanismsBFBB similar (Pearl, 1984). addition, BFBB maintains bestsolution n found far uses prune generated nodes evaluated higheru(shn i). Likewise, complying semantics OSP, generated nodes n costso-far g(n) higher problems budget b also immediately pruned.OPEN list becomes empty node n selected list promises less lowerbound, BFBB returns (the plan associated with) best solution n . h admissible,is, h-based pruning generated nodes sound, returned planguaranteed optimal.Let us return heuristic functions. domain-independent planningautomatically derived description model language choice.useful heuristic function must efficiently computable descriptionmodel, well relatively accurate estimates. Improving accuracy heuristicfunction without substantially worsening time complexity computing translatesfaster search plans.classical planning, numerous approximation techniques, monotonic relaxation (Bonet & Geffner, 2001, 2001; Hoffmann & Nebel, 2001), critical trees (Haslum& Geffner, 2000), network flow (van den Briel, Benton, Kambhampati, & Vossen, 2007;Bonet, 2013), logical landmarks goal reachability (Richter, Helmert, & Westphal, 2008;Karpas & Domshlak, 2009; Helmert & Domshlak, 2009; Bonet & Helmert, 2010),abstractions (Edelkamp, 2001; Helmert, Haslum, & Hoffmann, 2007; Katz & Domshlak,2010a), translated effective heuristic functions. Likewise, different heuristicsclassical planning also combined point-wise maximizing and/or additiveensembles (Edelkamp, 2001; Haslum, Bonet, & Geffner, 2005; Coles, Fox, Long, & Smith,2008; Katz & Domshlak, 2010b; Helmert & Domshlak, 2009).contrast, development heuristic functions OSP progressed beyondinitial ideas Smith (2004). principle, reduction Keyder Geffner (2009)net-benefit classical planning used reduce OSP classical planning realvalued state variables (Koehler, 1998; Helmert, 2002; Fox & Long, 2003; Hoffmann, 2003;Gerevini, Saetti, & Serina, 2003; Gerevini et al., 2008; Edelkamp, 2003; Dvorak & Bartak,2010; Coles, Coles, Fox, & Long, 2013). far, however, progress heuristic-search classicalplanning numeric state variables mostly achieved around direct extensionsdelete relaxation heuristics via numeric relaxed planning graphs (Hoffmann, 2003;Edelkamp, 2003; Gerevini et al., 2003, 2008). Unfortunately, heuristics preserveinformation consumable resources budgeted operator cost oversubscription4. BFBB also extensively used net-benefit planning (Benton, van den Briel, & Kambhampati, 2007;Coles & Coles, 2011; Do, Benton, van den Briel, & Kambhampati, 2007), well variantsdeterministic planning (Bonet & Geffner, 2008; Brafman & Chernyavsky, 2005).106fiOn Oversubscription Planning Heuristic Searchplanning: negative action effects decrease values numeric variablesignored, possibly special handling so-called cyclic resource transfer (Coleset al., 2013).first step overcoming lack effective heuristics OSP, next sectionstudy abstractions OSP, definition properties, prospectsderiving admissible abstraction heuristics. Section 5 study prospectsadapting OSP toolbox logical landmarks goal reachability. date, abstractionslandmarks responsible state-of-the-art admissible heuristics classicalplanning, thus special interest here.3. Abstractionsterm abstraction usually associated simplifying original model, factoringdetails less crucial given context. Context determines details reduced, better preserved, abstraction created used (Cousot& Cousot, 1992; Clarke, Grumberg, & Peled, 1999; Helmert et al., 2014; Domshlak, Hoffmann, & Sabharwal, 2009; Katz & Domshlak, 2010b). general terms, abstracting modelcorresponds associating set (typically computationally attractive)models M1 , . . . , Mk solutions models satisfy certain properties respect solutions . particular, deterministic planning heuristic search,abstractions used derive heuristic estimates states model interest :Given state abstraction M1 , . . . , Mk ,(1) mapped abstract states s1 M1 , . . . , sk Mk ,(2) k models abstraction solved respective initial states s1 , . . . , sk ,(3) aggregation quality resulting k solutions used heuristic estimates.Sometimes schematically sometimes precisely, process constructing abstractions state model = hS, s0 , u, O, , c, Qi seen two-step process(1) selecting abstraction skeleton = {(G1 , 1 ), . . . , (Gk , k )}, pair(Gi , ) comprises edge-labeled digraph Gi = hSi , Ti , Oi i, nodes Si , edges Ti ,edge labels Oi , state mapping : Si ,(2) extending set abstract models = {M1 , . . . , Mk }, that, [k],Gi graphical skeleton GMi Mi .qualify valid abstraction model , resulting set abstract modelssatisfy certain conditions specific variant deterministic planningconsideration. instance, optimal solutions abstract models classicalplanning required costly respective solutions originalmodels, constraint satisfied individual abstract models case maxaggregation (Pearl, 1984), k abstract models jointly, case additive abstractions (Yang, Culberson, Holte, Zahavi, & Felner, 2008; Katz & Domshlak, 2010b).107fiDomshlak & Mirkisshow, concept abstractions general, additive abstractions particular,different OSP, and, better worse, many degrees freedomrespective concepts classical planning.3.1 Abstractions OSP ProblemsGivenabstractionff skeleton = {(G1 , 1 ), . . . , (Gk , k )} OSP state model =S, s0 , u, O, , c, Qb , digraph Gi = hSi , Ti , Oi implicitly defines set OSP statemodels consistent it. set given Ci Ui Bi , Ci setfunctions operators Oi R0+ , Ui set functions states Si R0+ ,Bi = R0+ . terms, point (c, u, b) Ci Ui Bi induces OSP modelconsistent Gi , vice versa.Connecting sets models digraphs AS, letC = C1 Ck ,U = U1 Uk ,B = B1 Bk .state , every point (c, u, b) C U B induces set modelsn(c,u,b)(c,u,b)M(c,u,b) = M1, . . . , Mk,(c,u,b)Miff= Si , (s0 ), u[i], Oi , , c[i], Qb[i] :states Si operators Oi correspond nodes edge labels Gi ;transition function (s, o) = s0 iff Ti contains arc s0 labeledOi ;initial state (s0 ) determined initial state s0 state mapping ;operator cost function, state value function, cost budget directly determined choice (c, u, b).choices (c, u, b) C U B, induced sets models M(c,u,b)used deriving admissible estimates state interest s0 , others cannot.respective qualification defined below.Definition1 (Additive OSPff Abstraction)Let = S, s0 , u, O, , c, Qb OSP model = {(G1 , 1 ), . . . , (G1 , k )}abstraction skeleton . (c, u, b) C U B, M(c,u,b) (additive)abstraction , denotedM(c,u,b) AAS M,defh (s0 , b) hM(c,u,b) (s0 , b) =Xhi (i (s0 ), b[i]),i[k]is, hM(c,u,b) (s0 , b) admissible estimate h (s0 , b).108fiOn Oversubscription Planning Heuristic SearchGM= s1o1 |||||||/ s2s0o2o3/ s3G1G2= 2;1o1 zzzzzzzo1o5o4/ s4s1;0o2/ s1;2(a)o4/ 1;4s2;0o2o3/ s2;3o5/ s2;4(b)Figure 4: Illustration running examplesimple terms, set models forms additive OSP abstraction jointly modelsunderestimate value obtained initial state, within5 example, let Ggiven cost budget.4a graphical skeleton stateFigure ffmodel = {s0 , . . . , s4 }, s0 , u, {o1 , . . . , o5 }, , c, Qb , c(oi ) = 1 operators oi ,b = 2, u(si ) = 1{4} (i). Let = {(G1 , 1 ), (G2 , 2 )} abstraction skeleton, G1 G2 Figure 4b state mappings(s1;4 , {1, 3}1 (si ) =,s1;i , otherwise(s2;4 , = 22 (si ) =.s2;i , otherwiseConsider set models M(c,u,b) , constant c[1]() = c[2]() = 1, b[1] = b[2] = 2, and,j [2], u[i](si;j ) = 1{4} (j). optimal plan s0 -plan = h(s0 , o2 , s2 ), (s2 , o4 , s4 )i,(c,u,b)Qb () = 1, optimal 1 (s0 )-plan M11 = h(s1;0 , o1 , s1;4 )i,(c,u,b)b[1]Q (1 ) = 1, optimal 2 (s0 )-plan M22 = h(s2;0 , o2 , s2;4 )i,Qb[2] (2 ) = 1. Sinceh (s0 , b) = Qb () Qb[1] (1 ) + Qb[2] (2 ) = h1 (1 (s0 ), b[1]) + h2 (2 (s0 ), b[2]),M(c,u,b) additive abstraction .Theorem 1 OSP task = hV, s0 , u; O, c, bi, abstraction skeleton ={(G1 , 1 ), . . . , (Gk , k )} , AAS , digraphs givenexplicitly, hM (s0 , b) computed time polynomial |||| ||M||.ffProof: Let = {Mi }i[k] , Mi = Si , (s0 ), ui , Oi , , ci , Qbi , additive abstraction basis AS. [k], let Si0 = {s Si | ci (i (s0 ), s) bi }. Since5. optimal classical planning, requirement abstraction overestimate costs typicallyposed states original model, initial state (Yang et al., 2008; Katz &Domshlak, 2010b; Helmert et al., 2014). extra requirement, however, pragmatic reasonsefficiency allows abstraction computed preprocessing individually everystate examined search. Heuristics OSP, however, functions state alsoavailable cost budget, latter directly applies initial (aka current) state only.sum, defining abstractions respect entire state space necessity classicalplanning, OSP even clear whether defining abstractions respect specific pairstate budget deliver practical benefits. not, however, interpretedformal impossibility claim, investigation direction definitely worthwhile.109fiDomshlak & Mirkis(, , )SSSSkkSSkkkk(, u, )SSSSkk (c, , ) SSSSkk (, , b)kkkkSSkkkkSS(c, u, )(, u, b)(c, , b)SSSSSSkkkkkk(c, u, b)Figure 5: Fragments restricted optimization abstractions C U Bdigraphs given explicitly, shortest paths (s0 ) states Gi (andthus, particular, determining Si0 ) computed time polynomial||M||P[k]. turn, since hi (i (s0 ), bi ) = maxsSi0 ui (s), hM (s0 , b) = i[k] hi (i (s0 ), bi )computed time polynomial ||M||.message Theorem 1 positive, yet establishes necessary conditionrelevance OSP abstractions practice. Given OSP task , fixedabstraction skeleton joint performance measure space C U B,able automatically separate (c, u, b) C U B constituteabstractions not, within former set, denotedC U B,home abstraction provides us accurate (aka low) estimateh (s0 , b) possible. Here, even first item agenda necessarily trivial as,general, seems lack convenient combinatorial properties. instance, generallyform combinatorial rectangle C U B: Consider OSP state modelGM abstraction skeleton running example. Let c C cost functionvector c[1] c[2] constant functions value 1, two performancemeasures (c, u, b), (c, u0 , b0 ) CUB defined via budget vectors b = {b[1] = 2, b[2] = 0}b0 = {b0 [1] = 0, b0 [2] = 2}, value function vectors u u0 , u[1], u[2], u0 [1],u0 [2] evaluating zero states except u[1](s1;4 ) = u0 [2](s2;4 ) = 1.0 0hard verify M(c,u,b) AAS M(c,u ,b ) AAS : M(c,u,b) , state(c,u,b)s1;4 u[1](s1;4 ) = 1 reachable M1s1;0 = 1 (s0 ) b[1] = 2,00(c,u0 ,b0 )(c,u,b)0, state s2;4 u [2](s2;4 ) = 1 reachable M2s2;0 = 2 (s0 )00b0 [2] = 2. contrast, M(c,u ,b) 6AAS M(c,u,b ) 6AAS : setsmodels, model either comes budget (and initial state model0value zero), states non-zero value all. Hence, M(c,u ,b)0M(c,u,b ) estimate h (s0 , b) zero, h (s0 , b) = 1.light above, approach overall agenda complexity analysis abstractionbased heuristic functions steps, different fixations three dimensionsA: If, instance, given vector value functions u known belongprojection U, search quality abstraction abstractionsubset A(, u, ) A, corresponding projection {u}. show below,even constrained optimizations kind challenging. lattice Figure 5 depicts range options constrained optimization; extreme settings,110fiOn Oversubscription Planning Heuristic Searcho1G1G2: s2;1uuuuuuo3/ s2;3o1s1;0o2/ s1;2o4/ 1;4js2;0o3 ,o5o2o5/ s2;4jo4Figure 6: Homomorphic abstraction skeleton G() Figure 4A(, , ) simply renaming A, A(c, u, b) corresponds single abstractionM(c,u,b) A.3.2 Partitions Homomorphic Abstractionsproceed consider specific family additive abstractions, revealinteresting properties, show contains substantial islands tractability.Definition 1 allowing general abstraction skeletons, work focus homomorphic abstraction skeletons6 (Helmert et al., 2014).Definition2 abstraction skeleton= {(G1 , 1 ), . . . , (Gk , k )} OSP stateffmodel = S, s0 , u, O, , c, Qb homomorphic if, [k], Oi = O, (s, o) = s0(i (s), o, (s0 )) Ti .instance, running example, abstraction skeleton Figure 4b homomorphic (since, e.g., (s1 , o3 , s3 ) GM yet (1 (s1 ), o3 , 1 (s3 )) = (s1;4 , o3 , s1;4 ) 6 GM1 ),abstraction skeleton Figure 6 homomorphic. Furthermore, focus fragmentadditive abstractionsAp = [Cp Bp ] ,Cp C, U, Bp B correspond cost, value, budget partitions,respectively.ffDefinition 3 Given OSP state model = S, s0 , u, O, , c, Qb , homomorphicabstraction skeleton = {(G1 , 1 ), . . . , (Gk , k )} joint performance measureC U B,Pc C cost partition iff, operator O, i[k] c[i](o) c(o);Pu U value partition iff, state S, i[k] u[i](i (s)) u(s);Pb B budget partition iff, i[k] b[i] b.follows, node x lattice Figure 5, Ap (x) refer A(x)Ap ;e.g., Ap (, u, ) = A(, u, ) Ap .begin analysis Ap establishing interesting completeness relationshipsets Cp Bp , well even stronger individual completeness CpBp . Formulated Theorem 2, properties Ap play key role computationalanalysis later on.6. results also hold verbatim general labeled paths preserving abstraction skeletonsstudied Katz Domshlak (2010b) context optimal classical planning. However,presentation somewhat accessible restricted homomorphic abstraction skeletons.111fiDomshlak & MirkiscCpbBp(1)bcBpCp(2)Figure 7: Illustration sub-claims (1) (2) Theorem 2: (1), gray ellipsewithin Bp stands subset budget partitions b pair cabstraction, is, Ap (c, , b) 6= . However, pairing budgetpartitions b c requires careful selection value partition u (soM(c,u,b) abstraction), exists budget partition bchoice u job.Theorem 2 Given OSP task = hV, s0 , u; O, c, bi homomorphic abstractionskeleton = {(G1 , 1 ), . . . , (Gk , k )} ,(1) cost partition c Cp , exists budget partition b BpM(c,u,b ) value partitions u ;(2) budget partition b Bp , exists cost partition c CpM(c ,u,b) value partitions u .proof Theorem 2 appears Appendix A, p. 145. Figure 7 illustrates statement sub-claim (1) Theorem 2, well as, indirectly, corollaries.7 firstcorollary Theorem 2 projections Ap Cp , , Bp entire setsCp , , Bp , respectively. is, cost partition c (and similarly, budget partition value partition) matched abstraction partitioncomponent. Second, budget partition b paired given costpartition c abstractions , is, b Bp , Ap (c, , b) 6= ,always budget partitions paired c. Finally, pairingc-compatible budget partitions b c requires careful selection valuepartition u, exists c-compatible budget partition b choiceu result M(c,u,b) abstraction .priori, properties Ap simplify task abstraction discoveryoptimization within space partitions Cp Bp , later showindeed case. However, complexity analysis abstraction discovery within Cp Bpgeneral terms still problematic OSP formalism parametric7. respective illustration sub-claim (2) Theorem 2 completely similar, mutatis mutandis.112fiOn Oversubscription Planning Heuristic Searchrepresentation value functions. Hence, proceed examining abstractiondiscovery OSP context fixed value partitions u .4. Value Partitions Complete AbstractionsLet OSP task, explicitly given homomorphic abstraction skeleton ,u value partition AS. immediate corollary Theorem 2Ap (, u, ) empty, thus try computing min(c,u,b)Ap (,u,) hM(c,u,b) (s0 ).yet, however, know whether task polynomial-time solvablenon-trivial class value partitions. fact, although Ap (, u, ) known, Theorem 2,non-empty, so, too, subsets Ap (, u, b) Ap (c, u, ), finding evenabstraction (c, u, b) Ap (, u, ) necessarily easy.4.1 0-Binary Value Partitionsfirst step, examine abstraction discovery within fragment Apvalue functions u[i] abstract models call 0-binary. Later, Section 4.2,show findings 0-binary abstract value functions extended generalvalue partitions.Definition 4 real-valued function f called 0-binary codomain f {0, }R+ . set F 0-binary functions called strong functions Fcodomain {0, }.one hand, 0-binary functions constitute rather basic family value functions.Hence, abstraction optimization hard them, likely hard nontrivial family abstract value functions. hand, 0-binary abstract valuefunctions seem fit well abstractions planning tasks value functions linearcombinations indicators, representing achievement goal value statevariable.respect, first tractability results abstraction discovery Ap (, u, )u strong 0-binary value partition. first (and simpler) result Theorem 3assumes fixed action cost partition, next result, Theorem 7,simultaneous selection admissible pairs cost budget partitions. Corollary 4Theorem 10 show results Theorem 3 Theorem 7, respectively,extended pseudo-polynomial algorithms general 0-binary value partitions.4.1.1 Strong 0-Binary Value Partitions Knapsack problemfirst tractability result abstraction discovery within Ap (c, u, ) u strong0-binary value partition c arbitrary cost partition. key role playedwell-knownKnapsackffproblem (Dantzig, 1930; Kellerer, Pferschy, & Pisinger, 2004).instance {wi , }i[n] , W Knapsack problem given weight allowance Wset objects [n], object [n] annotated.Pwith weight wi value0 [n]objectivefindsubsetZ[n]maximizessubsetsZiZPwW.strictKnapsackrefervariant Knapsack0iZinequality constraint strict. Knapsack NP-hard (Karp, 1972; Garey & Johnson,113fiDomshlak & Mirkis1978), exist pseudo-polynomial algorithms run time polynomialdescription problem unary representation W (Dudzinski & Walukiewicz,1987). latter property makes solving Knapsack practical many applicationsratio minWi wi reasonably low. Likewise, = j i, j [n], greedy algorithmsolves problem linear time iteratively expanding Z one weight-wiselightest objects [n] \ Z, Z cannot expanded within W .Theorem 3 (Ap (c, u, ) & strong 0-binary u)Let = hV, s0 , u; O, c, bi OSP task, explicit homomorphic abstraction skeleton , u strong 0-binary value partition. Given cost partition c Cp ,finding abstraction (c, u, b) Ap (c, u, ) computing corresponding heuristicestimate hM(c,u,b) (s0 , b) done time polynomial |||| ||AS||.Proof: proof reduction polynomial fragment Knapsack problemcorresponding items identical value. Let = {(G1 , 1 ), . . . , (Gk , k )}, and,given u strong 0-binary value partition, let codomain u[i] {0, }R+ .[k], let wi cost cheapest path Gi (s0 ) (one the)states Si u[i](s) = . Since explicit abstraction skeleton, set {wi }i[k]computed time polynomial ||AS|| using one standard algorithmsffsingle-source shortest paths problem. Consider Knapsack problem {wi , }i[k] , b ,weights wi value identical objects. Let Z [k]solution (optimization) Knapsack problem; recall computablepolynomial time. Given that, define budget profile b B follows:(wi , Z[k], b [i] =0,otherwise.remains shown (c, u, b ) actually induces additive abstraction. Assume contrary M(c,u,b ) 6AAS , let optimal s0 -plan. construction Knapsack problem b , Z,(c,u,b )(s)-plan PQb [i] (i ) = . Definition 1, assumption impliesMiQb () > iZ Qb [i] (i ) = |Z|. However, Theorem 2, exists least onebudget partition b Bp M(c,u,b) AAS . Note budget partitioninduces aPfeasible solution Z 0 = {i | wi b[i]} Knapsack problem, satisfyingQb () iZ 0 Qb[i] (i ) = |Z 0 |. This, however, implies |Z| < |Z 0 |, contradictingoptimality Z, thus accomplishing proof M(c,u,b ) AAS .construction proof Theorem 3 may appear somewhat counterintuitive:interested minimizing heuristic estimate h (s0 , b), abstractionM(c,u,b ) selected via value-maximizing Knapsack problem. Indeed, ultimatelywould like obtainminhM(c,u,b) (s0 , b),(6)b : (c,u,b)Apheuristic manage compute polynomial time actuallymaxb : (c,u,b)AphM(c,u,b) (s0 , b).114(7)fiOn Oversubscription Planning Heuristic Searchtime, note that, fixed pair c Cp u , estimate Eq. 7still least (and possibly much more) accurate estimate would obtainedproviding k abstract models entire budget b. Later showsuperior accuracy verified experiments, first proceed examiningworking general 0-binary value partitions.strong 0-binary value partitions rather restrictive, finding elementAp (c, u, ) general 0-binary u longer polynomiala reduction Knapsackstraightforward. However, Knapsack solvable pseudo-polynomial time, pluggingKnapsack algorithm proof Theorem 3 results search algorithmAp (c, u, ) general 0-binary u.Corollary 4 (Ap (c, u, ) & 0-binary u)Let = hV, s0 , u; O, c, bi OSP task, explicit homomorphic abstraction skeleton , u 0-binary value partition. Given cost partition c Cp , findingabstraction (c, u, b) Ap (c, u, ) computing corresponding heuristic estimatehM(c,u,b) (s0 , b) done time polynomial ||||, ||AS||, unary representationbudget b .test illustrate value additive abstractions bring heuristic-searchOSP, implemented prototype heuristic-search OSP solver8 top Fast Downwardplanner (Helmert, 2006). Since, unlike classical net-benefit planning, OSP still lacksstandard suite benchmarks comparative evaluation, cast roleSTRIPS classical planning tasks International Planning Competitions (IPC) 19982006. translation OSP done associating separate unit-valueproposition conjunctive goal corresponding classical IPC task.Within prototype, implemented BFBB search OSP, provided supportbasic pattern-database abstraction skeletons, action cost partitions, abstraction selection Ap (c, u, ) strong 0-binary value partitions proof Theorem 3.Specifically, task k sub-goals:(i) abstraction skeleton comprised set k projections planning task ontoconnected subsets ancestors respective k goal variables causal graph.size projection limited 1000 abstract states, ancestorsgoal variable v added corresponding projection (initialized containv) breadth-first manner, v back along arcs causal graph,abstraction could expanded within aforementioned size limit.(ii) value partition u associated entire value sub-goal hv/di (only)projection associated v.(iii) cost partition c distributed cost operator uniformlyprojections invalidate o, i.e., reflected least one state variableaffected o.evaluation, compared BFBB node expansions three heuristic functions,tagged blind, basic, hM . three heuristics, h-value node n set 0cost budget n over-consumed. cost budget over-consumed, then:8. aware domain-independent planner optimal OSP.115fiDomshlak & Mirkisairport (25)blocks (23)depot (3)driverlog (12)freecell (5)grid (2)gripper (6)logistics (10)miconic (50)mystery (4)openstacks (7)rovers (10)satellite (9)tpp (7)trucks (9)pipesw-t (12)pipesw-nt (7)psr-small (30)zenotravel (10)totalhM232331252610504710979127301023925%basic2323312526105047108791273010238blind2323312526105047108791273010238hM202331252610504710779127301023450%basic20233125261050477678127309228blind20233115261050477678127308226hM1922311526105047766612730922275%basic2018395261050476465117308211blind1817395261050476565117308209hM19173105161050376565117308209100%basic2017275161045375455106307195blind1817265161045275455106307191Table 1: Number problems solved across different budgets using OPEN list ordered heuristic evaluation Figure 3Blind BFBB constitutes trivial baseline h(n) simply set total valuegoals.basic BFBB, h(n) set total value goals, individuallyachieved within respective projection abstraction (see Theorem 1) given entireremaining budget.hM additive abstraction heuristic selected Ap (c, u, )proof Theorem 3.evaluation contained planning tasks could determine offlineminimal cost budget needed achieve goals. task approachedfour different budgets, corresponding 25%, 50%, 75%, 100% minimalcost needed achieve goals task, run restricted 10 minutes.Table 1 shows number tasks solved within domain level cost budget.9Figure 8 depicts results terms expanded nodes across four levels cost budget.(Figures 18-21 Appendix B provide detailed view results Figure 8breaking different levels cost budget.) Despite simplicityabstraction skeletons used, number nodes expanded BFBB hMtypically substantially lower number nodes expanded basic BFBB,difference sometimes reaching three orders magnitude.9. reiterate task considered solved upon termination BFBB, is,optimal plan found proven optimal.116fiOn Oversubscription Planning Heuristic Search(a)108unsolved107106105hM104103102unsolvedairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel101100 010 101 102 103 104 105 106 107 108blind(b)108unsolved107106hM105104103unsolved102101100 010 101 102 103 104 105 106 107 108basicFigure 8: Comparative view empirical results Table 1 terms expanded nodes4.1.2 Freeing Cost Partition: Knapsack Meets Convex OptimizationReturning algorithmic analysis context strong 0-binary value partitions,proceed relaxing constraint sticking fixed action cost partition c.buys flexibility selecting abstractions Ap (, u, ), allowing us improveaccuracy heuristic estimates, still retaining computational tractability.117fiDomshlak & Mirkisinput: = hV, s0 , u; O, c, bi, = {(G1 , 1 ), . . . , (Gk , k )} ,strong 0-binary value partition uoutput: (u)= 1 kreduce Gi nodes reachable (s0 )= k downto 1always-achievable(m) returnreturn 0always-achievable(m):ellipsoid-method(separation-oracle-Lm1 ) 7 solution x dom(X ) L1x[] b return trueelse return false(a)separation-oracle-Lm1 (x dom(X )):let permutation [k]x[b[P(1)]] x[b[ (2)]] x[b[ (k)]]x[] i[m] x[b[ (i)]] return YesPelse return constraint i[m] b[ (i)](b)Figure 9: polynomial-time algorithm computing (u) strong 0-binary valuepartition u (Theorem 7)Given OSP task = hV, s0 , u; O, c, bi, homomorphic abstraction skeleton AS,value partition u AS, let(u) = minmax(8)hM(c,u,b) (s0 , b) .cCpb : (c,u,b)ApObviously, estimate h(s0 , b) = (u) least accurate estimate Eq. 7derived respect fixed cost partition c.show that, OSP task , abstraction skeleton = {(G1 , 1 ), . . . ,(Gk , k )} , strong 0-binary value partition u AS, (u)computed polynomial time. corresponding algorithm shown Figure 9,Figure 9a depicting macro-flow algorithm Figure 9b depicting specificimplementation solve sub-routine makes overall time complexityalgorithm polynomial.high-level flow algorithm Figure 9a follows. Since u strong 0binary value partition, let codomain abstract value functions u[i] {0, }R+ . Given that, (c, u, b) Ap (, u, ), holds hM(c,u,b) (s) ={0} [k]. k abstract models M(c,u,b)contribute additive estimate hM(c,u,b) (s) either 0.118fiOn Oversubscription Planning Heuristic Searchfirst loop algorithm preprocessing for-loop eliminatesabstraction skeleton nodes structurally unreachable abstract initialstates 1 (s0 ), . . . , k (s0 ).10 ease presentation, follows assumecleanup abstraction skeleton leaves Gi least one state whose value. second for-loop algorithm decreasingly iterates values {k, (k1), . . . , 2, } possibly come abstractions Ap (, u, ) positiveestimate h (s0 , b). candidates (u) tested turn via sub-routinealways-achievable. test returns positive first time, done,tested candidate identified (u). Otherwise, test fails [k],(u) = 0, particular implying state value greater 0 reacheds0 budget b.test always-achievable (u) = based linear program (LP) Lm1 , givenEq. 10. linear program defined variables#"[[(9){c[i](o)} ,X = {}{d(s)}sGi {b[i]}oOi[k]constraints (10a)-(10c), objective maximizing value variable .Lm1 :maxsubjectd(i (s0 )) = 0,[k] : d(s) d(s0 ) + c[i](o), (s0 , o, s) Gi,b[i] d(s),Gi s.t. u[i](s) =(c[i](o) 0,[k]: P,i[k] c[i](o) c(o)XZ [k], |Z| = :b[i].(10a)(10b)(10c)iZroles different variables Lm1 follows.Variable c[i](o) captures cost associated label digraph GiAS.state Gi , variable d(s) captures cost cheapest path Gi(s0 ) s, given edges Gi weighted consistently valuesvariables c[i]().Variable b[i] captures minimal budget needed reaching Gi state valuestate (s0 ), given that, again, edges Gi weighted consistentlyvariable vector c[i].10. preprocessing step replaced adding extra constraints linear program describedbelow. However, would unnecessarily complicate presentation without adding much value.119fiDomshlak & Mirkissingleton variable captures minimal total cost reaching states valueprecisely k models M(c,u,b) .semantics constraints Lm1 follows.first two sets constraints (10a) come simple LP formulationP singleP source shortest paths problem source node (s0 ): Optimizingi[k]sGi d(s) fixed weighting c edges leads computing preciselythat, k digraphs simultaneously.third set constraints (10a) establishes costs cheapest paths {Gi }states (s0 ) states valued , enforcing semantics variables b[1], . . . , b[k].Constraints (10b) cost partition constraints enforce c Cp .Constraints (10c) enforce aforementioned semantics objective variable .Two things worth noting here. First, nodes digraphs G1 , . . . , Gkstructurally reachable source nodes 1 (s0 ), . . . , k (s0 ), respectively (asensured first for-loop algorithm),polytope induced L1 boundednon-empty. Indeed, assignment oO {c[i](o)} consistentpositiveness constraints (10b), variables d() boundedlengths respective shortest paths. turn, bounding d() boundsvariables c[1], . . . , c[k] via third set constraints (10a), constraints (10c)bound objective .Second, number variables, well number constraints (10a)k(10b), polynomial |||| ||AS||, number constraints (10c). Thus,solving Lm1 using standard methods linear programming infeasible. Lemma 5show problem actually mitigated, then, Lemma 6 showsemantics Lm1 match objective finding (u).Lemma 5 algorithm Figure 9 terminates time polynomial |||| ||AS||.Proof: runtime complexity algorithm boils complexity solvingLm1 , and, number variables L1 (m), well number constraints(10a)polynomial |||| ||AS||, number constraints (10c)(10b),kcannot solved polynomial time using standard methods linear.Thus,L1programming, Simplex algorithm (Dantzig, 1963) Interior-Point methods (Nemirovsky & Yudin, 1994). However, using algorithms,Ellipsoid algorithm (Grotschel, Lovasz, & Schrijver, 1981) Random Walks familyalgorithms originating work Bertsimas Vempala (2004), LPexponential number constraints solved polynomial time, providedpolynomial time separation oracle LP. polynomial-time separating oracleconvex set K Rn procedure given x Rn , either verifies x Kreturns hyperplane separating x K. procedure run polynomial time.case, separation problem is, given assignment variables Lm1 , testwhether satisfies (10a), (10b), (10c), not, produce inequality among (10a),(10b), (10c) violated assignment.120fiOn Oversubscription Planning Heuristic Searchshow separation problem Lm1 solved polynomial timeusing called m-sum minimization LPs (Punnen, 1992), precisely(parametrized m) procedure separation-oracle-Lm1 Figure 9b does. numberconstraints (10a) (10b) polynomial, satisfaction assignment x dom(X )tested directly substitution. constraints (10c), letP permutation [k]x[b[ (1)]] x[b[ (2)]] x[b[ (k)]]. x[] i[m] x[b[ (i)]],easy seePx satisfies constraints (10c). Otherwise, violatedinequality i[m] b[ (i)].Lemma 6 algorithm Figure 9a computes (u).proof Lemma 6 appears Appendix A, p. 146. Combining statementsLemmas 5 6, Theorem 7 summarizes tractability result abstraction discoveryAp (, u, ) strong 0-binary value partitions u.Theorem 7 (Ap (, u, )(s) & strong 0-binary u)Given OSP task = hV, s0 , u; O, c, bi, homomorphic explicit abstraction skeleton, strong 0-binary value partition u , (u) computed time polynomial|||| ||AS||.Unfortunately, practical value result Theorem 7 yet evaluated.far, found reasonably efficient implementation Ellipsoid methodlinear inequalities, while, best knowledge, Random Walks algorithms (Bertsimas & Vempala, 2004) never implemented all. hope stateaffairs change soon, allowing powerful algorithms used theory,also practice.4.1.3 Strong General 0-Binary Value PartitionsRecall polynomial result Theorem 3 strong 0-binary value partitions easilyextends Corollary 4 pseudo-polynomial algorithm general 0-binary value partitions. turns pseudo-polynomial extension Theorem 7 possible well,though technically involved. corresponding algorithm shown Figure 10.Following format Figure 9, Figure 10a depicts macro-flow algorithmFigure 10b shows specific implementation solve sub-routine desiredtime complexity achieved.Similarly algorithm Figure 9, preprocessing for-loop algorithm firsteliminates abstraction skeleton nodes structurally unreachableabstract initial states 1 (s0 ), . . . , k (s0 ). Next, algorithm performs binarysearch interval containing (u).11 Since u 0-binary value partition, [k],{0, }, R+ , denote codomain abstract valueP function u[i]. Given that,(c, u, b) Ap (, u, ), holds h(c,u,b) (s) = iZ Z [k].size combinatorial hypothesis space prohibitive, while-loop Figure 10performs binary search relaxed hypothesis space, corresponding continuous11. binary search could used algorithm Figure 9 well, wouldmere optimization, necessary avoid exponential blowup time complexity.121fiDomshlak & Mirkisinput: = hV, s0 , u; O, c, bi, = {(G1 , 1 ), . . . , (Gk , k )} ,0-binary value partition uoutput: (u)= 1 kreduce Gi nodes reachable (s0 )let 0 < < mini[k]P0i[k]>v + ( )/2always-achievable(v) velse v= 0 return 0else returnalways-achievable(v):ellipsoid-method(separation-oracle-Lv2 ) 7 solution x dom(X ) Lv2x[] b return trueelse return false(a)separation-oracle-Lv2(x dom(X )):ffstrict-Knapsack( {x[b[i]], }i[k] , x[] ) 7 solution Z [k]PiZ < v return YesPelse return constraint iZ b[i](b)Figure 10: pseudo-polynomial algorithm approximating (u) general 0-binaryvalue partitions u (Theorem 10)Pinterval [0, i[k] ] R+0 . parameter serves sufficient precision criteriontermination.iteration corresponding interval [, ], algorithm uses sub-routinealways-achievable test hypothesis (u) v, v mid-point [, ].test positive, next tested hypothesis (u) v 0 , v 0 midpoint[v, ]. Otherwise, next hypothesis corresponds midpoint [, v).while-loop done, reported estimate set ; still might lag(u), lag arbitrarily reduced reducing , anyway, (u)ensures admissibility estimate. If, however, while-loop terminates = 0,(u) < mini[k] implies (u) = 0, return.test always-achievable (u) v based linear program Lv2 ,defined variables X Eq. 9, obtained Lm1 replacing constraints (10c)constraints (11c):122fiOn Oversubscription Planning Heuristic SearchLv2 :maxsubjectd(i (s0 )) = 0,[k] : d(s) d(s0 ) + c[i](o), (s0 , o, s) Gi,b[i] d(s),Gi s.t. u[i](s) =(c[i](o) 0,[k]: P,i[k] c[i](o) c(o)XXZ [k] s.t.v :b[i].iZ(11a)(11b)(11c)iZsemantics variables remains Lm1 , captures minimaltotal cost ofPreaching states {si }i[k] abstract models M(c,u,b)total value i[k] u[i](si ) v. new constraint (11c) enforces semantics .Lemma 8 > 0, algorithm Figure 10 terminates time polynomial||||, ||AS||, log 1 , unary representation budget b .PProof: number iterations while-loop approximately log2 i[k],run-time iterations boils complexity solving Lv2 . Here,vLemma 5 linear programs Lm1 , number variables L2 , well numberconstraints (11a) (11b), polynomial |||| ||AS||, numberconstraints (11c) (2k ). Therefore, always-achievable(v) also employs ellipsoidmethod sub-routine separation-oracle-Lv2 associated separation problem.show separation problem Lv2 solved pseudo-polynomial timeusing standard pseudo-polynomial procedure strict Knapsack problem.Given assignment x dom(X ), feasibility respect (11a) (11b)tested directly substitution.constraints (11c),ff let Z [k] optimal solutionstrict Knapsack problem {x[b[i]], }i[k] , x[] , weight allowance x[] kobjects, object [k] associated weight x[b[i]] value .Pvalue iZ Z smaller v, x satisfies constraints(11c). Assume contrary x violatesPconstraint (11c), corresponding0setPZ [k]. definition (11c),iZ 0 v, assumption,x[] >x[b[i]].That,however,impliesZ 0 feasible solution0iZstrict Knapsack, value higher presumably optimal Z.POtherwise, iZ v, Z itselfPprovides us constraint (11c)violated x. x[] >iZ x[b[i]] holdsff virtue Zsolution strict Knapsack problem {x[b[i]], }i[k] , x[] .123fiDomshlak & MirkisLemma 9 0 < < mini[k] , algorithm Figure 10a computes(u) .proof Lemma 9 appears Appendix A, p. 147. Combining statementsLemmas 8 9, Theorem 10 summarizes result optimized abstraction discoveryAp (, u, ) general 0-binary value partitions u. Importantly, note algorithmFigure 10 depends unary representation budget, possiblestate values. particular, means dependence complexity numberalternative sub-goals OSP task interest polynomial. Finally, Theorem 10formulated terms estimate precision values abstract valuefunctions u[i] arbitrary real numbers. case integer-valued sets functionsu, well various special cases real-valued functions, (u) determinedprecisely using simplification algorithm Figure 10. instance, 1 , . . . , kintegers, setting value (0, 1) results while-loop terminating= (u). details, however, theoretical interest; reasonably smallvalues , practice difference estimates h(s, b) h(s, b) + .Theorem 10 (Ap (, u, )(s) & 0-binary u)Given OSP task = hV, s0 , u; O, c, bi, homomorphic explicit abstraction skeleton ={(G1 , 1 ), . . . , (Gk , k )} , 0-binary value partition u , > 0, possibleapproximate (u) within additive factor time polynomial ||||, ||AS||, log 1 ,unary representation budget b .4.2 General Value Partitions0-binary value partitions rather useful themselves, turnspseudo-polynomial algorithms abstraction discovery explicit homomorphic abstraction skeletons 0-binary value partitions extended rather easily arbitrary valuepartitions, using following observations:(1) OSP task = hV, s0 , u; O, c, bi, homomorphic abstraction skeleton ={(G1 , 1 ), . . . , (Gk , k )} , value partition u AS, numberdistinct values taken u[i] trivially upper-bounded number states Gi ;(2) pseudo-polynomial solvability Knapsack problem extends generalvariant known Multiple-Choice Knapsack (Dudzinski & Walukiewicz, 1987; Kellereret al., 2004).Multiple-Choice (MC) Knapsack problem hN1 , . . . , Nm ; W given weightallowance W classes objects N1 , . . . , Nm , object j Ni annotatedweight wij value ij . objectivefind set Z containsPoneP object class maximizes (i,j)Z ij sets satisfying(i,j)Z wij W. strict MC-Knapsack, refer variant MC-Knapsackinequality constraint strict. MC-Knapsack generalizes regular Knapsackthus NP-hard. However, similarly regular Knapsack problem, MC-Knapsack alsoadmits pseudo-polynomial, dynamic programming algorithm runs time polynomial124fiOn Oversubscription Planning Heuristic Searchdescription problem unary representation W (Dudzinski &Walukiewicz, 1987; Kellerer et al., 2004).Theorem 11 (Ap (c, u, ))Let = hV, s0 , u; O, c, bi OSP task, let = {(G1 , 1 ), . . . , (Gk , k )} explicit homomorphic abstraction skeleton , let u arbitrary valuepartition AS. Given cost partition c Cp , possible find abstraction(c, u, b) Ap (c, u, ) compute corresponding heuristic estimate hM(c,u,b) (s0 , b)time polynomial ||||, ||AS||, unary representation budget b.Proof: proof similar proof Theorem 3, compilationMC-Knapsack problem.[k], let ni number distinct values taken u[i], let {i1 , . . . , ini } R+codomain u[i], and, j [ni ], let wij cost cheapest path Gi(s0 ) (one the) states Si u[i](s) = ij . Since explicit abstractionskeleton, [k], holds ni |Si |, set {wij }i[k],j[ni ] computedtime polynomial ||AS|| using one standard algorithms single-source shortestpaths problem.Consider MC-Knapsack problem weight allowance b k classesobjects N1 , . . . , Nk , |Ni | = ni object j Ni annotated weight wijvalue ij . Let Z ki=1 Ni solution (optimization) MC-Knapsack problem;recall computable pseudo-polynomial time. Given that, define budget profileb B follows:(wij , (i, j) Z[k], b [i] =0,otherwise.Showing (c, u, b ) actually induces additive abstraction completely identical proof corresponding argument Theorem 3, thus omitted.Theorem 12 (Ap (, u, ))Given OSP task = hV, s0 , u; O, c, bi, homomorphic explicit abstraction skeleton ={(G1 , 1 ), . . . , (Gk , k )} , arbitrary value partition u AS, > 0,possible approximate (u) within additive factor time polynomial ||||,||AS||, log 1 , unary representation budget b .algorithm abstraction discovery Theorem 12 depicted Figure 11.high-level flow differs flow algorithm Figure 10 general 0-binaryvalue partitions initialization parameters . major differencealgorithms tests candidate values v based linearprograms Lv3 , defined follows.[k], let {i1 , . . . , ini } R+ codomain u[i]. v R+ , linearprogram Lv3 defined Eq. 13 variables[[[{d(s)}sGiX = {}{b[i, j]}{c[i](o)}.(12)i[k]j[ni ]125oOfiDomshlak & Mirkisinput: = hV, s0 , u; O, c, bi, = {(G1 , 1 ), . . . , (Gk , k )} ,0-binary value partition uoutput: (u)= 1 kreduce Gi nodes reachable (s0 )let 0 < < mini[k] minj[ni ] ijP0i[k] maxj[ni ] ij>v + ( )/2always-achievable(v) velse v= 0 return 0else returnalways-achievable(v):ellipsoid-method(separation-oracle-Lv3 ) 7 solution x dom(X ) Lv3x[] b return trueelse return false(a)separation-oracle-Lv3 (xdom(X )):ffstrict-MC-Knapsack( {x[b[1, j]], 1j }j[n1 ] , . . . , {x[b[k, j]], kj }j[nk ] ; x[] )7 solution Z [n1 ] [nk ]Pi[k] iZ(i) < v return YesPelse return constraint i[k] b[i, Z(i)](b)Figure 11: (a) modification algorithm Figure 10 arbitrary value partitionsu (Theorem 12), (b) pseudo-polynomial time separation oraclecorresponding linear programs Lv3 Eq. 13variables differ variable set Lv2 (see Eq. 9) larger set b-variables:Variable b[i, j] captures minimal budget needed reaching Gi statevalue i,j state (s0 ), given edges Gi weighted consistentlyvariable vector c[i].126fiOn Oversubscription Planning Heuristic SearchLv3 :maxsubjectd(i (s0 )) = 0,[k] : d(s) d(s0 ) + c[i](o), (s0 , o, s) Gi,b[i, j] d(s),j [ni ]s Gi s.t. u[i](s) = ij(13a)(c[i](o) 0,[k]: P,(13b)i[k] c[i](o) c(o)Z [n1 ] [nk ]XXs.t.iZ(i) v :b[i, Z(i)].i[k](13c)i[k]Like Lemma 8 linear programs Lv2 , number variablesLv3 , well number constraints (13a) (13b), polynomial ||||||AS||, number constraints (13c) (dk ) = maxi[k] ni . Therefore,always-achievable(v) also employs ellipsoid method pseudo-polynomial time separation oracle, latter based solving strict MC-Knapsack problem (seeFigure 11b). Otherwise, solving Lv2 solving Lv3 similar.Lemma 13 > 0, algorithm Figure 11 terminates time polynomial||||, ||AS||, log 1 , unary representation budget b .Lemma 14 Given OSP task = hV, s0 , u; O, c, bi, homomorphic explicit abstractionskeleton = {(G1 , 1 ), . . . , (Gk , k )} , arbitrary value partition uAS, > 0, algorithm Figure 11 computes (u) .proof Lemma 13 similar proof Lemma 8, strict Knapsackseparation problems replaced strict MC-Knapsack separation problems.proof Lemma 14 also similar proof Lemma 9, mutatis mutandis. Together,Lemmas 14 13 establish Theorem 12.5. Landmarks OSPaddition state-space abstractions, family approximation techniquesfound extremely effective context optimal classical planning based notionlogical landmarks goal reachability (Karpas & Domshlak, 2009; Helmert & Domshlak,2009; Domshlak et al., 2012; Bonet & Helmert, 2010; Pommerening & Helmert, 2013).section proceed examining prospects reachability landmarksheuristic-search OSP planning.127fiDomshlak & Mirkis5.1 Landmarks Classical Planningstate classical planning task , landmark property operator sequencessatisfied s-plans (Hoffmann, Porteous, & Sebastia, 2004). instance,fact landmark state assignment single variable true pointevery s-plan. state-of-the-art admissible heuristics classical planning usecalled disjunctive action landmarks, corresponding set operatorsevery s-plan contains least one operator set (Karpas & Domshlak, 2009;Helmert & Domshlak, 2009; Bonet & Helmert, 2010; Pommerening & Helmert, 2013).follows consider popular notion landmarks, simply refer disjunctiveaction landmarks state s-landmarks. ease presentation, discussion take place context landmarks initial state task,simply referred landmarks (for ).Deciding whether operator set L landmark classical planning taskPSPACE-hard (Porteous, Sebastia, & Hoffmann, 2001). Therefore, landmark heuristicsemploy landmark discovery methods polynomial-time sound, incomplete.follows assume access procedure; actual way landmarksdiscovered tangential contribution.landmark costP set L s-landmarks,0+function lcost : L R admissiblelcost(L) h (s). singleton setL = {L}, lcost(L) := minoL c(o) natural admissible landmark cost function,extends directly non-singleton sets pairwise disjoint landmarks. general setslandmarks, lcost devised polynomial time via operator cost partitioning (Katz& Domshlak, 2010b), either given L (Karpas & Domshlak, 2009), within actualprocess generating L (Helmert & Domshlak, 2009).5.2 -Landmarks Budget Reductionlandmarks play important role (both satisficing optimal) classical planning,far exploited OSP. first glance, probably surprise,OSP investigated much less classical planning: Sincelandmarks must satisfied plans empty operator sequence alwaysplan OSP task, notion landmark seem useful here. saidthat, consider anytime output improvement property BFBB forward search.empty plan interesting useless, alsofound BFBB right beginning. general, stages search,anytime search algorithms like BFBB maintain best-so-far solution , prunebranches promise value lower equal Qb (). Hence, principle, algorithmsmay benefit information properties satisfied plans valuelarger Qb (). Polynomial-time discovery value landmarks arbitrary OSPtasks still open problem. However, looking needed available,show classical planning machinery reachability landmarks actuallyeffectively exploited OSP.P follows, assume value function additive, u(s) =hv/dis uv (d), uv (d) 0 variable-value pairs hv/di. is, value statesum (mutually independent) non-negative marginal values propositionscomprising s. value different s-plans OSP task varying zero128fiOn Oversubscription Planning Heuristic Searchvalue optimal s-plan (which may also zero), let -landmark stateproperty satisfied s-plan achieves something valuable.instance, disjunctive action landmarks use here, L -landmarks, every s-plan Qb () > 0 contains operator L. follows, unlessstated otherwise, focus -landmarks (the initial state of) .Definition 5 Given OSP task = hV, s0 , u; O, c, bi, -compilation classical planning task = hV , s0 , G ; , cV = V {g},dom(g) = {0, 1},s0 = s0 {hg/0i},G = {hg/1i},= Og = ohv/di | hv/di D, uv (d) > 0 ,pre(ohv/di ) = {hv/di} eff(ohv/di ) = {hg/1i},(c(), =c (o) =.0,= ohv/di Ogput simply, semantics value hg/1i auxiliary variable gverified proposition positive value achieved.terms, simply extends structure set zero-cost actionsapplying corresponds verifying positive value achieved. Constructing trivially polynomial time, allows us discover-landmarks using standard machinery classical planning landmark discovery.Theorem 15 OSP task , landmark L L landmark .Proof: proof rather straightforward. Let P set plansQb () > 0 P set plans . definition P, plan P,exists proposition hv/di uv (d) > 0 hv/di s0 JK. Likewise, sinces0:= s0ff {hg/0i} O, applicables0 . ffHence, definitionff ohv/di ,ohv/di applicable s0hg/1iJK,is,0hv/dihv/di P . turn,ffL landmark , ohv/di contains operator L, L O,contains operator L well. proves landmarks Loperators -landmarks .Theorem 15 hand, derive -landmarks using methodclassical planning landmark extraction, employed LAMA planner (Richter et al., 2008) LM-Cut family techniques (Helmert & Domshlak, 2009;Bonet & Helmert, 2010). However, first glance, discriminative power knowingneeded achieve something valuable seems negligible comes deriving effective heuristic estimates OSP. good news that, OSP, informationeffectively exploited slightly different way.129fiDomshlak & MirkisConsider schematic example searching optimal plan OPS taskbudget b, using BFBB admissible heuristic h. Suppose onesequence (all unit-cost) operators, = ho1 , o2 , . . . , ob+1 i, applicable initial state, positive value state along end-state. clearlyvalue higher zero achieved given budget b, searchcontinue beyond initial state, unless h(s0 , ) counts cost b + 1 operators. Now, suppose h(s0 , ) counts cost {oi , . . . , ob+1 } > 0,{o1 }, {o2 }, . . . , {oi1 } discovered -landmarks . Given that, supposemodify (a) setting cost operators o1 , o2 , . . . , oi1 zero, (b) reducingbudget b + 1. Since operators o1 , o2 , . . . , oi1 applied anywayalong value collecting plan , modification seems preserve semantics. time, modified task, BFBB heuristic h pruneinitial state thus establish without search empty plan optimal plan. course, way modified example simplistic example itself.Yet, example motivate idea landmark-based budget reduction OSP,well illustrates basic idea behind generically sound task modificationsdiscuss next.Definition 6 Let = hV, s0 , u; O, c, bi OSP task, L = {L1 , . . . , Ln } setpairwise disjoint -landmarks , lcost admissible landmark cost functionL. budget reducing compilation OSP task L = hVL , s0L , uL ; OL , cL , bLnXbL = blcost(Li )(14)i=1VL = V {vL1 , . . . , vLn }dom(vLi ) = {0, 1},s0L = s0 {hvL1 /1i , . . . , hvLn /1i},uL = u,OL =n[i=1OLi =n[{o | Li },i=1pre(o) = pre(o) {hvLi /1i} eff(o) = eff(o) {hvLi /0i},(c(o),=oOcL () =.c(o) lcost(Li ), = OLiwords, L extends structuremirroring operators -landmark Li cheaper lcost(Li ) versions,130fiOn Oversubscription Planning Heuristic SearchCBBo2o2o2 o2ABBo1o1o3BTBo6o3BBBo7o4BBTo4CTBo4o7o4BTTo3o3o6o2CBTo2u=1o5o9o5u=1CCBo8o10o8o5CTTo9o5u=1o2o2o8o10o8CTCu=1CCTo5u=2o9o5o8CCCo10o8CBC(a)u=1CBBCTBo2ABBo1BBBBTBo6o3o7o4BBTo2o7o4BTTo3o6o2o9o5o2CBBo10o8CTTu=1CBTo10o8u=1CCBo9o5CTCo2 o2o9 u=2o5u=1CCTCCCABBo10o8CBCo1o1o3BTBo6o3BBBo7o4BBTo4o2o2CTBo4o7o4BTTo3o3o6o2CBTo2o5o9o5o2o2o8o10o8u=1CCBo8o10o8o5CTTo9o5u=1u=1CTCu=1CCTo5u=2o9o5o8CCCo10o8CBCFigure 12: Illustrations example landmark-based budget reducing compilationL : (a) structurally reachable parts graphical skeleton modelinduced L , illustrated projection L variables originaltask , along comparison budget-wise reachable partsgraphical skeletons induced models (b) original task (c)compiled task L .using disposable propositions hvL1 /1i , . . . , hvLn /1i ensure oneinstance discounted operators Li applied along operatorsequence initial state12 ,compensating discounted operators Li reducing budget preciselylcost(Li ).example, consider simple OSP task Figure 2 (p. 104) cost budgetb = 4, assume provided set four landmarks L = {L1 , . . . , L4 }L1 = {o1 }, L2 = {o2 }, L3 = {o3 , o4 } L4 = {o5 , o8 }, admissible landmark costfunction lcost(Li ) = 1 [4]. Compiling (L, lcost) using budget reducingcompilation Definition6 results task L budget bL = 0 c(o) = 0discounted operators ni=1 OLi = {o1 , o2 , o3 , o4 , o5 , o8 }.states correspond complete assignments three variables V ={t, x, y}, L already seven variables VL = {t, x, y, vL1 , vL2 , vL3 , vL4 }. Thus, depicting12. Note that, auxiliary variable g -compilation effectively change valuehg/0i hg/1i, auxiliary variables vLi L change values (only) hvLi /1i hvLi /0i.difference reflects positive semantics usually associated value 1, aka valuetrue, planning propositions: semantics state L containing proposition hvLi /1istill allowed apply (one the) discounted operators associated landmark Lionwards.131fiDomshlak & Mirkiscompile-and-BFBB ( = hV, s0 , u; O, c, bi):= -compilationL := set landmarkslcost := admissible landmark cost function LL := budget reducing compilation (L, lcost)n := BFBB(L )return plan associated nFigure 13: BFBB search landmark-based budget reductionstructurally reachable parts graphical skeleton GML problematic. Still,illustrate search space , Figure 12(a) show (structurally reachable partsthe) graphical skeleton model induced projection variables{t, x, y} only. arcs corresponding discounted operators colored, colordistinguishing landmark responsible respective discounted operators.Figures 12(b) 12(c) illustrate effect budget-reducing compilation depicting parts graphical skeletons GM GML actually reachablerespective cost budgets b = 4 bL = 0: states BTT CTTreachable initial state budget allowance 4, states corresponding BTT CTT longer reachable L , reducing size search spaceBFBB. time, formulated Theorem 16 below, reductionsearch space affect plans lead valuable states, resulting effectiveequivalence L .Theorem 16 Let = hV, s0 , u; O, c, bi OSP task, L set pairwise disjoint-landmarks , lcost admissible landmark cost function L, Lrespective budget reducing compilation . every Qb () > 0,plan L L QbL (L ) = Qb (), vice versa.proof Theorem 16 appears Appendix A, p. 149. budget reducing OSPto-OSP compilation Definition 6 clearly polynomial time. compile-and-BFBBprocedure, depicted Figure 13,(1) generates -compilation ;(2) uses off-the-shelf tools classical planning generate set landmarks Ladmissible landmark cost function lcost;(3) compiles (L, lcost) , obtaining OSP task L .optimal solution L (and thus ) searched using search algorithmoptimal OSP BFBB.proceed consider general sets landmarks, comments concerning setup Theorem 16 order. First, reduced budget bL turnslower cost cheapest action applicable initial state, obviouslysearch needed, empty plan reported optimal right away. Second,132fiOn Oversubscription Planning Heuristic Searchzero-cost landmarks useless compilation much useless derivinglandmark heuristics optimal planning. Hence, lcost follows assumedstrictly positive. Third, applicable state brings benefitsyet adds branching search. Hence, implementation, landmark Li Loperator Li , precondition regular operators OL extended{hvLi /0i}. hard verify extension preserves correctnessL terms Theorem 16. Finally, value initial state zero, is,empty plan positive value, -compilation positivecost landmarks all. However, easily fixed considering valuablepropositions hv/di uv (d) > 0 hv/di 6 s0 . ignore timeproblem non-zero-value initial states (and assume Qb () = 0), returnlater systematic discussion.5.3 Non-Disjoint -Landmarksbudget reducing compilation L sound pairwise disjoint landmarks,general sets -landmarks. example, consider planning taskwhich, operator o, c(o) = b, Qb (hoi) > 0, Qb () = 0operator sequences 6= hoi. is, value greater zero achievable ,via operator o. Suppose set -landmarks L = {L1 , . . . , Ln },n > 1, lcost(Li ) > 0 [n],Pnthat -landmarks contain o.case, budget L bL = b i=1 lcost(Li ), cost cheapest replicao, is, cost cheapest operator sequence achieving non-zero value ,nni=1i=1c(o) max lcost(Li ) = b max lcost(Li ) > bnXlcost(Li ) = bL .i=1Hence, state positive value reachable s0L L , thus Lvalue equivalent sense Theorem 16.example shows compiling non-disjoint -landmarks independentlysound. principle, made sound follows. Let = hV, s0 , u; O, c, biOSP task, let L = {L1 , . . . , Ln } set -landmarks , let lcost admissiblelandmark cost function L. components L = hVL , s0L , uL ; OL , cL , bLstill defined Definition 6, except operator sets OL1 , . . . , OLn . latterconstructed independently other, sequentially, contentOLi depending content OLj , j < i. ordering sets OLiconstructed arbitrary.operator 1 n, let Oo;i denote set costdiscounted representatives introducedconstruction OL1 , . . . , OLi .1 n, operator o0 Li Oo0 ;i1 cL (o) = 0, OLi := .Otherwise, OLi contains operator operatorLi[o0 Li133Oo0 ;i1 ,(15)fiDomshlak & Mirkisdefined similarly Definition 6 as:pre(o) = pre(o) {hvLi /1i},eff(o) = eff(o) {hvLi /0i},(c(o) lcost(Li ),cL (o) =cL (o) lcost(Li ),Li ,.o0 Li Oo0 ;i1(16)compilation extended way sound arbitrary sets -landmarks,pairwise disjoint landmarks reduces basic compilation used Theorem 16.general, however, extended compilation longer polynomial sizeexplicit representation|Oo;i | = 2|{Lj |ji,oLj }| .example, let L = {L1 , L2 , L3 }, L1 = {a, b}, L2 = {a, c}, L3 = {a, d}. GenerationOL1 := {a1 , b1 } effectively follows Definition 6, OL2 , base set operatorsEq. 15 already {a, c, a1 }. Thus, OL2 := {a2 , c1 , a3 }, where, {2, 3} denotinga0 , ai derived according Eq. 16 ai2 . Consequently, base set operatorsOL3 {a, d, a1 , a2 , a3 }, resulting OL3 = {a4 , d1 , a5 , a6 , a7 }, where, {4, 5, 6, 7},ai derived ai4 . sum, L ends 8 = 2|L| representatives operator a.Since non-disjoint landmarks bring information, typical outputsstandard techniques landmark extraction classical planning, presentdifferent, slightly involved, compilation polynomial sound arbitrarysets -landmarks.Definition 7 Let = hV, s0 , u; O, c, bi OSP task, L = {L1 , . . . , Ln } setpairwise disjoint -landmarks , lcost admissible landmark cost functionL. operator o, let L(o) denote set landmarks L containo. Then, generalized budget reducing compilation OSP task L =hVL , s0L , uL ; OL , cL , bLbL = bnXlcost(Li ),i=1VL = V {vL1 , . . . , vLn }dom(vLi ) = {0, 1},s0L = s0 {hvL1 /1i , . . . , hvLn /1i},uL = u,OL = {o | L} {get(L) | L L}pre(o) = pre(o) {hvL /1i | L L(o)},eff(o) = eff(o) {hvL /0i | L L(o)},(17)pre(get(L)) = {hvL /0i},eff(get(L)) = {hvL /1i},134(18)fiOn Oversubscription Planning Heuristic Searchc(o), PcL () = c(o) LL(o) lcost(L),lcost(L),=oO.=o(19)= get(L)illustrate compilation, let L = {L1 , L2 , L3 },L1 = {a, b},L2 = {b, c},L3 = {a, c},operators cost 2, letlcost(L1 ) = lcost(L2 ) = lcost(L3 ) = 1.L , VL = V {vL1 , vL2 , vL3 }OL = {a, b, c, get(L1 ), get(L2 ), get(L3 )},with, e.g.,pre(a) = pre(a) {hvL1 /1i , hvL3 /1i},eff(a) = eff(a) {hvL1 /0i , hvL3 /0i},cL (a) = 0,and, get(L1 ),pre(get(L1 )) = {hvL /0i},eff(get(L1 )) = {hvL /1i},cL (get(L1 )) = 1.intuition behind compilation Definition 7 follows. Eq. 19, applyingdiscounted operator saves total cost landmarks containing o. Therefore,executed states corresponding control propositions{hvL /1i | L L(o)} hold, indicating cost landmark L(o) alreadysaved reaching s,avoid double savings around L(o), applying turns control propositions sJoK.However, considering example above, suppose optimal plan originaltask contains instance operator a, followed instance operator b,instance operator c. Applying instead would block us applying b insteadb, thus value optimal plan compilation lower Qb ().rescue comes get(L) actions allow selective spendingindividual landmark costs lcost(L). example, applying saves costlandmarks L1 L3 , applying get(L1 ) spend lcost(L1 ) safely set135fiDomshlak & Mirkiscontrol proposition hvL1 /1i. turn, enable b applied next steps,applying b save cost L2 re-save cost L1 . way,compilation leads equivalence L , formulated Theorem 17proven Appendix A, p. 149.Theorem 17 Let = hV, s0 , u; O, c, bi OSP task, let L = {L1 , . . . , Ln } set-landmarks , let lcost admissible landmark cost function L, let L(generalized) budget reducing compilation . every Qb () > 0,plan L L QbL (L ) = Qb (), vice versa.5.4 -Landmarks & Incremental BFBBdiscussed earlier, value initial state zero, empty planpositive value, thus -compilation Definition 5landmarks positive cost. passing noted small problem remediedconsidering valuable facts hv/di uv (d) > 0 hv/di 6 s0 .consider aspect OSP closely, show discovery -landmarksincremental revelation plans BFBB combined mutually stratifyingway.Let = hV, s0 , u; O, c, bi OSP task interest, suppose given setplans 1 , . . . , n . so, longer interested searching plansachieve something, searching plans achieve something beyond1 , . . . , n already achieve. Specifically, let si = s0 Ji K end-state ,set propositions D, let goods(s) set propositions hv/diuv (d) > 0. new plan end-state achieves something beyond 1 , . . . , nalready achieve, then, 1 n,goods(s) \ goods(si ) 6= .put observation work.Definition 8 Given OSP task = hV, s0 , u; O, c, bi set reference states Sref ={s1 , . . . , sn } , (, Sref )-compilation classical planning task (,Sref ) =hV , s0 , G ; , cV = V {x1 , . . . , xn , search, collect},dom(xi ) = dom(search) = dom(collect) = {0, 1},s0 = s0 {hsearch/1i , hcollect/0i , hx1 /0i , . . . , hxn /0i},G = {hx1 /1i , . . . , hxn /1i},n[=Oi {f inish},i=1136fiOn Oversubscription Planning Heuristic Search= {o | O},pre(o) = pre(o) {hsearch/1i},eff(o) = eff(o),c (o) = c(o).pre(f inish) = ,eff(f inish) = {hcollect/1i , hsearch/0i},c (f inish) = 0.Oi = {oi,g | si Sref , g goods(D) \ si },pre(oi,g ) = {g, hcollect/1i},eff(oi,g ) = {hxi /1i},c (oi,g ) = 0.Notegoal G cannot achieved without applying f inish operator;regular operators applied f inish;subgoal achieving operators oi,g applied f inish.way, first part plan (,Sref ) determines plan , second partverifies end-state plan achieves subset value-carrying propositionsgoods(D) included state Sref .13Theorem 18 Let = hV, s0 , u; O, c, bi OSP task, Sref = {s1 , . . . , sn } subsetstates, L landmark (,Sref ) L O. plangoods(s0 JK) \ goods(si ) 6= si Sref , contains instance least oneoperator L0 = {o | L}.Proof: Assume contrary exists plan = ho1 , . . . , okgoods(s0 JK) \ goods(si ) 6= si Sref , yet L0 = . Let {g1 , . . . , gn }arbitrary set propositions goods(s0 JK) \ goods(s1 ), . . . , goods(s0 JK) \ goods(sn ),respectively. construction (,Sref ) , immediate(,Sref ) = ho1 , . . . , ok , f inish, o1,g1 , . . . , on,gnplan (,Sref ) and, assumption L0 , holds (,Sref ) L = .This, however, contradicts L landmark (,Sref ) .13. solve & verify technique appears helpful many planning formalism compilations; see,e.g., work Keyder Geffner (2009).137fiDomshlak & Mirkisinc-compile-and-BFBB ( = hV, O; s0 , c, u, bi)initialize global variables:n := s0// best solution farSref := {s0 } // current reference statesloop:(,Sref ) = (, Sref )-compilationL := set landmarks (,Sref )lcost := admissible landmark cost function LL := budget reducing compilation (L, lcost)inc-BFBB(L , Sref , n ) = done:return plan associated ninc-BFBB (, Sref , n )open := new max-heap ordered f (n) = h(shni, b g(n))open.insert(make-root-node(s0 ))closed:=best-cost:= 0open.empty()n := open.pop-max()f (n) u(shn i): breaku(shni) > u(shn i): update n := ngoods(shni) 6 goods(s0 ) s0 Sref :Sref := Sref {shni}termination criterion: return updated// rest similar BFBB Figure 3shni 6 closed g(n) < best-cost(shni):closed:= closed {shni}best-cost(shni) := g(n)foreach O(shni):n0 := make-node(shniJoK, n)g(n0 ) > b f (n0 ) u(shn i): continueopen.insert(n0 )return doneFigure 14: Iterative BFBB landmark enhancementTheorem 18 allows us define iterative version BFBB, inc-compile-and-BFBB,depicted Figure 14. successive iterations inc-compile-and-BFBB correspondrunning regular BFBB successively informed (, Sref )-compilations ,states discovered iteration making (, Sref )-compilation used iteration + 1informed.inc-compile-and-BFBB maintains pair global variables: set reference statesSref best solution far n . iteration loop, modified versionBFBB, inc-BFBB, called (, Sref )-compilation , created basis138fiOn Oversubscription Planning Heuristic Searchcurrent Sref . reference set Sref extended inc-BFBB non-redundantvalue-carrying states discovered search, n updated search discoversnodes higher value.OPEN list becomes empty node n selected list promisesless lower bound, inc-BFBB returns indicator, done, best solutionn found far, across iterations inc-compile-and-BFBB, optimal. case,inc-compile-and-BFBB leaves loop extracts optimal plan n . However,inc-BFBB may also terminate different way, certain complementary terminationcriterion satisfied. latter criterion comes assess whether updates Srefperformed current session inc-BFBB warrant updating (, Sref )-compilationrestarting search. terminated way, inc-BFBB returns respective indicator,inc-compile-and-BFBB goes another iteration loop, updated Srefn . note that, optimality algorithm holds terminationcondition, latter greatly affect runtime efficiency algorithm.Theorem 19 inc-compile-and-BFBB search algorithm sound complete optimal OSP.Proof: First, complementary termination criterion employed inc-BFBB procedure, inc-compile-and-BFBB guaranteed terminate. complementary termination criterion checked inc-BFBB proper expansionglobal reference set Sref , thus number calls inc-BFBB inc-compile-and-BFBBupper-bounded |S|.terms search, inc-BFBB different regular BFBB procedure. turn,Theorem 18, additional pruning power budget-reducing compilation referencestates Sref affects search nodes n u(shni) < maxsSref u(s). Note also that,time best solution far n updated inc-BFBB, necessarily added Sref(since goods(shn i) new n included goods(s) Sref ). Thus, optimalsolutions cannot pruned inc-BFBB overall search inc-compile-and-BFBBtherefore sound.5.5 Empirical Evaluationevaluate merits landmark-based budget reducing compilation, extended prototype OSP solver Section 3 following components:(, Sref )-compilation OSP tasks arbitrary sets reference states Sref ;generation disjunctive action landmarks (, Sref )-compilations using LM-Cutprocedure (Helmert & Domshlak, 2009) Fast Downward;incremental BFBB procedure inc-compile-and-BFBB Figure 14,search termination criterion satisfied (only) examined node n improvescurrent value lower bound, i.e., n becomes new best-so-far node n .preliminary evaluation, also added two optimality preserving enhancements search. auxiliary variables compilations increase dimensionality problem, known negatively affect quality abstraction139fiDomshlak & Mirkis(a) blind108unsolved107106105104103102unsolvedcompile-and-BFBBairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel101100100 101 102 103 104 105 106 107 108BFBB(b) hM108unsolved106105104103102unsolvedcompile-and-BFBB107101100100 101 102 103 104 105 106 107 108BFBBFigure 15: Comparative view empirical results terms expanded nodes, BFBBvs. compile-and-BFBB, (a) blind (b) abstraction hM heuristicsheuristics (Domshlak et al., 2012), first devised projections respect original OSP problem , open list ordered search done originalproblem, is,Xh shniV , b g(n) +lcost(L) ,vL 6shnisV projection L state variables original OSP task .change heuristic evaluation sound, Theorem 17 particular implies140fiOn Oversubscription Planning Heuristic Searchadmissible heuristic also admissible heuristic L , vice versa. Second,new node n generated, check whetherXXlcost(L),lcost(L) g(n0 ) +g(n) +L:hvL /0ishn0L:hvL /0ishnipreviously generated node n0 corresponds state originalproblem , is, shn0 iV = shniV . so, n pruned right away. Optimalitypreservation enhancement established Lemma 20 proven Appendix A,p. 151.Lemma 20 Let OSP task, (,Sref ) (, Sref )-compilation , L setlandmarks (,Sref ) , lcost admissible landmark cost function L, Lrespective budget reducing compilation (L, lcost) . Let 1 2 pair plansVL end-states s1 s2 , respectively, sV1 = s2cL (1 ) +Xlcost(L) cL (2 ) +L:hvL /0is1Xlcost(L).(20)L:hvL /0is2Then, plan 10 extends 1 , exists plan 20 extends 2= QbL (10 ).QbL (20 )evaluation included regular BFBB planning , solving using landmarkbased compilation via compile-and-BFBB, simple setting inc-compile-and-BFBBdescribed above. three approaches evaluated blind heuristicadditive abstraction heuristic hM described Section 3. Figures 15-17 depict resultsevaluation terms expanded nodes. Similarly experiment reportedSection 3, task approached four different budgets, corresponding 25%,50%, 75%, 100% minimal cost needed achieve goals task,run restricted 10 minutes. Figures 15a 15b compare numberexpanded nodes BFBB compile-and-BFBB across four levels cost budget,blind (a) abstraction hM (b) heuristics. Figures 16a 16b provide similarcomparison BFBB inc-compile-and-BFBB. Figures 17a 17bcompile-and-BFBB inc-compile-and-BFBB.14 Figures 22-25 Figures 26-29Appendix B provide detailed view results Figures 15 16, respectively,breaking different levels cost budget.Figure 8 shows, results satisfactory. informative heuristicguidance all, number nodes expanded compile-and-BFBB typically muchlower number nodes expanded BFBB, difference reaching threeorders magnitude once. 760 task/budget pairs behind Figure 8a, 81pairs solved compile-and-BFBB search (by proving planachieve value higher initial state), while, unsurprisingly, 4tasks solved search BFBB.14. present detailed comparison terms running times, per-node CPUtime overhead due landmark-based budget reduction 10%. technical difficultiesimplementation inc-compile-and-BFBB led us limit comparison graph taskssolved methods.141fiDomshlak & Mirkis(a) blind108107inc-compile-and-BFBBairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel106105104103102101100100 101 102 103 104 105 106 107 108BFBB(b) hM108inc-compile-and-BFBB107106105104103102101100100 101 102 103 104 105 106 107 108BFBBFigure 16: Comparative view empirical results terms expanded nodes, BFBBvs. inc-compile-and-BFBB, (a) blind (b) abstraction hM heuristicsexpected, impact landmark-based budget reduction lowersearch equipped meaningful heuristic (Figure 15b). Nonetheless, evenabstraction heuristic hand, number nodes expanded compile-and-BFBBoften substantially lower number nodes expanded BFBB. Here, BFBBcompile-and-BFBB solved search 39 85 task/budget pairs, respectively. Finally, despite rather ad hoc setting incremental inc-compile-and-BFBB procedure,switching compile-and-BFBB inc-compile-and-BFBB typically beneficial. Obvi142fiOn Oversubscription Planning Heuristic Search(a) blind108107inc-compile-and-BFBBairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel106105104103102101100 010 101 102 103 104 105 106 107 108compile-and-BFBB(b) hM108inc-compile-and-BFBB107106105104103102101100 010 101 102 103 104 105 106 107 108compile-and-BFBBFigure 17: Comparative view empirical results terms expanded nodes,compile-and-BFBB vs. inc-compile-and-BFBB, (a) blind (b) abstraction hM heuristicsously, much deeper investigation development inc-compile-and-BFBB still required,especially context choice iteration termination criterion.6. Summary Future WorkDeterministic oversubscription planning captures computational core oneimportant setups automated action selection, yet, despite apparent importance143fiDomshlak & Mirkisproblem, sufficiently investigated. work, progressed towardstranslating spectacular advances classical deterministic planning deterministicOSP. Tracing key sources progress classical planning, identified severe lackeffective approximations OSP, worked towards bridging gap.focus two classes approximation techniques underly state-ofthe-art optimal heuristic-search solvers classical planning: state-space abstractionsgoal-reachability landmarks. First, defined notion additive abstractions OSP,studied complexity deriving effective abstractions rich space hypotheses,revealed substantial, empirically relevant islands tractability abstractiondiscovery problem. Next, showed standard goal-reachability landmarks certainclassical planning tasks compiled OSP task interest, resultingequivalent OSP task lower cost allowance, thus sometimes dramaticallysmaller search space.techniques proposed satisfy properties required efficient searchalgorithms optimal OSP. However, believe techniques, especiallylandmark-based budget reducing compilations, beneficial satisficing OSPoptimal OSP, particular difference optimal satisficingplanning appears much smaller OSP classical deterministic planning.Many interesting questions remain open future work, prospectsdevelopments oversubscription planning appear quite promising. Within specificcontext work, two interesting research directions (1) optimization valuepartitions given cost partitions, is, optimizing abstraction discovery Ap (c, , ),(2) thoroughly investigating interleaved landmark discovery search OSPintroduced Section 5.4. broader context, propose, well, additional candidatesfuture research:Following work Katz Domshlak (2010a) implicit abstractions classical planning, computational merits implicit abstractions OSPinvestigated. inevitably give us better understanding computationaltractability boundaries deterministic OSP.basic model deterministic planning Section 2.1 used provide unifyingcomparative view basic models classical, cost-bounded, net-benefit,oversubscription planning. One practically motivated extension model liftaction costs vectors action costs. variant cost-bounded planningalready investigated (Nakhost et al., 2012), natural examineextension context OSP.Unfortunately, results abstractions seem extend directly vectorscosts: level planning model, adding cost measures shifts problem solvingpolynomial time shortest path(s) problems NP-hard restricted shortest path(s)problems (Handler & Zang, 1980). Nonetheless, like Knapsack problem, restricted shortest path problem solved pseudo-polynomial time (Desrochers& Soumis, 1988), thus extension results vectors costs might stillachievable.time, machinery landmark-based budget reducing compilationsOSP straightforwardly extends vectors costs budgets. Hence, even144fiOn Oversubscription Planning Heuristic Searchquality heuristic OSP multiple cost measures available, blind searchstill stratified information coming problem landmarks.pruning mechanism BFBB must rely admissible, upper-boundingheuristic estimates, special properties required heuristic used guidesearch choices BFBB. Thus, developing informative yet necessarily admissibleheuristics OSP clearly interest.Acknowledgmentswork partially supported EOARD grant FA8655-12-1-2096, ISFgrant 1045/12.Appendix A. ProofsTheorem 2 Given OSP task = hV, s0 , u; O, c, bi homomorphic abstractionskeleton = {(G1 , 1 ), . . . , (Gk , k )} ,(1) cost partition c Cp , exists budget partition b BpM(c,u,b ) value partitions u ;(2) budget partition b Bp , exists cost partition c CpM(c ,u,b) value partitions u .Proof: Let = h(s0 , o1 , s1 ), (s1 , o2 , s2 ), . . . , (sn1 , , sn )i optimal s0 -plan ,and, [k], let = h(i (s0 ), o1 , (s1 )), . . . , (i (sn1 ), , (on ))i mappingGi . Since homomorphic, paths 1 , . . . , k well-defined.(1) PGiven cost partition c Cp , let budget profile b B defined b [i] =j[n] c[i](oj ), [k]. First, note b Bp sinceXi[k]()X Xb [i] =c[i](oj )i[k] j[n]X()c(oj ) b,j[n]() c cost partition, () s0 -plan .Second, u U, construction b , (s0 )-plan abstract(c,u,b )model Mi. Now, let u , [k], let optimal (s0 )-plan(c,u,b )Mi.Xi[k]Qb[i]()(i )X[i]Qb()(i ) Qb (),(21)i[k]() optimality , () (sn ) end-state uvalue partition. Therefore, (c, u, b ) induces additive abstraction ,is, M(c,u,b ) AAS .145fiDomshlak & Mirkis(2) Given budget partition b, let cost profile c C defined c [i](o) = c(o) b[i]b ,forPall operators O, [k]. First, c Cp since b Bp implies1i[k] b[i] [0, 1]. Second, u U, construction c , (s0 )b(c ,u,b)plan Mi. Following exactly line reasoning Eq. 21accomplishes proof M(c ,u,b) AAS u .Lemma 6 algorithm Figure 9a computes (u).Proof: Due boundness non-emptiness polytope induced Lm1 , termination algorithm straightforward. Thus, given strong 0-binary partition u,question whether value algorithm terminates (u). First, letus show that:() [k], x solution Lm1 , x[] b if, cost partitionc Cp , exists budget partition b Bp (c, u, b) abstractionhM(c,u,b) (s0 ) m.() Assume contrary that, cost partition c Cp , exists budget partition b Bp hM(c,u,b)(s0 ) m, yet x[] > b. Given values provided x cost variables oO {c[i](o)}, let c corresponding cost partition,1 , . . . , k induced lengths shortest paths 1 (s0 ), . . . , k (s0 ) valued states G1 , . . . , Gk , respectively. assumption, let b budget partitionhM(c,u,b) (s0 ) m. First, definition strong 0-binary value partitions,hM(c,u,b) (s0 ) implies exists Z k, |Z| = that, Z, b[i] .Second, constraint (10c), maximization , fact bound b[i]imply together that, Z, x[b[i]] = . Putting things together, obtainbBpbXb[i]iZX=iZX(10c)x[b[i]] ,iZcontradicting assumption.() Assume contrary that, x[] b, yet exists cost partition c Cpthat, budget partitions b Bp (c, u, b) Ap , hM(c,u,b) (s0 ) < m.Let shortest path lengths 1 , . . . , k defined above, respectspecific cost partition c assumption.Likewise, let xc solution Lm1extra constraint cost variables oO {c[i](o)} assigned c. Since objectiveLm1 maximize value ,x[] xc [].(22)Now, letZ=argmaxXZ 0 [k],|Z 0 |=m iZ 0146.fiOn Oversubscription Planning Heuristic SearchTogether, constraint (10c), maximization , fact boundb[i] (via cost variables) implyxc [] =Xxc [b[i]] =iZX.(23)iZturn, together x[] b Eq. 22, Eq. 23 implies(xc [b[i]],b[i] =0,iZotherwisebudget partition (c, u, b) Ap , hM(c,u,b) (s0 ) m, contradicting assumption.proved sub-claim (), basically captures semantics Lm1 , supposealgorithm terminates within loop, returns > 0.construction algorithm, x solution Lm1 , x[] b. (), costpartition c Cp , exists (c, u, b) Ap h(c,u,b) (s) m. = k,trivially (u) = m. Otherwise, < k, know algorithm terminateprevious iteration corresponding + 1. Again, () implies existscost partition c Cp (c, u, b) Ap induce h(c,u,b) (s) (m + 1). Hence,definition (u), (u) < (m + 1), turn, since u strong 0-binary valuepartition, (u) = m. Finally, algorithm terminates loopreturns 0, precisely argument basis () implies (u) = 0.Lemma 9 0 < < mini[k] , algorithm Figure 10a computes(u) .Proof: arguments boundness non-emptiness polytope inducedLv2 precisely polytope Lm1 studied Lemma 6, thustermination algorithm straightforward. follows, prove valuereturned algorithm satisfies claim lemma. Let u given 0-binarypartition. Similarly proof Lemma 9, first prove sub-claim that:() v R0+ , x solution Lv2 , x[] b if, cost partitionc Cp , exists budget partition b Bp (c, u, b) abstractionhM(c,u,b) (s0 ) v.proof () mirrors proof respective sub-claim Lemma 5, mutatis mutandis,thus provided ease verification.() Assume contrary that, cost partition c Cp , exists budgetpartition b Bp hM(c,u,b) (s0 ) v, yet x[] > b.Given values provided x cost variables oO {c[i](o)}, let c corresponding cost partition, and, [k], let induced length shortestpath (s0 ) -valued states Gi . assumption, let b budgetpartition hM(c,u,b) (s0 ) v. First,P definition 0-binary value partitions,hM(c,u,b) (s0 ) v implies exists Z k, iZ v that, Z, b[i] .147fiDomshlak & MirkisSecond, constraint (11c), maximization , fact bound b[i], imply together that, Z, x[b[i]] = . Putting things together, obtainbBpbXb[i]iZX=iZX(11c)x[b[i]] ,iZcontradicting assumption.() Assume contrary that, x[] b, yet exists cost partition c Cpthat, budget partitions b Bp (c, u, b) Ap , hM(c,u,b) (s0 ) < v.Let shortest path lengths 1 , . . . , k defined above, respectspecific cost partition c assumption.Likewise, let xc solution Lv2extra constraint cost variables oO {c[i](o)} assigned c. Since objectiveLv2 maximize value ,x[] xc [].(24)Now, letZ = argmaxX0PZ [k], iZ 0iZ 0 v.Together, constraint (11c), maximization , fact boundb[i] (via cost variables) implyxc [] =Xxc [b[i]] =iZX.(25)iZturn, together x[] b Eq. 24, Eq. 25 implies(xc [b[i]], Zb[i] =,0,otherwisebudget partition (c, u, b) Ap , hM(c,u,b) (s0 ) v, contradicting assumption.finalizes proof sub-claim (). Now,Pconsider interval end-pointsterminationwhile-loop. =i[k] , then, trivially, (u) .POtherwise, < i[k] , then, construction algorithm, iterationloop, test always-achievable() issued, came back negative, thus,solutions x L2 , x [] > b. Hence, (), (u) < . Now, 6= 0, then,construction algorithm, iteration loop, test always-achievable()issued, came back positive, thus, solutions x L2 , x [] b.Hence, (), (u) . Putting properties together while-loopstermination condition implies (u) = (u) . Finally, = 0,< mini[k] implies < mini[k] . turn, since (u) corresponds sum valuesstates k models M(c,u,b) , (u) concluded implies = (u) = 0.148fiOn Oversubscription Planning Heuristic SearchTheorem 16 Let = hV, s0 , u; O, c, bi OSP task, L set pairwise disjoint-landmarks , lcost admissible landmark cost function L, Lrespective budget reducing compilation . every Qb () > 0,plan L L QbL (L ) = Qb (), vice versa.Proof: Let L planSnfor L , let operator sequence obtained replacingoperators i=1 OLi along L respective operators O.definitionaction set L Eq. 15, applicable s0 , s0 JK =s0L JL K \ ni=1 dom(vLi ). Thus, Qb () = QbL (L ). Likewise, definitionaction set L Eq. 15 fact operator OL achieves controlpropositions {hvL1 /1i , . . . , hvLn /1i}, |OLi L | 1. that,c() cL (L ) +nXlcost(Li ).i=1Pturn, b = bL + ni=1 lcost(Li ) Eq. 14, cL (L ) bL virtue Lplan L . Therefore, holds c() b, thus plan .opposite direction, let plan Qb () > 0, let Loperator sequence obtained replacing, -landmark L L, every first occurrenceoperator L respective cost reduced operator OL . easyverify L applicable s0L , QbL (L ) = Qb (). Likewise, definition-landmarks, every L L presence along . that,c(L ) = c()nXlcost(Li ) bi=1nXlcost(Li ) = bL ,i=1first equality pairwise disjointness {L1 , . . . , Ln }, inequalityplan , second equality Eq. 14. Thus, L plan L .Theorem 17 Let = hV, s0 , u; O, c, bi OSP task, let L = {L1 , . . . , Ln } set-landmarks , let lcost admissible landmark cost function L, let L(generalized) budget reducing compilation . every Qb () > 0,plan L L QbL (L ) = Qb (), vice versa.Proof: Let L plan L , let operator sequence obtained (i) replacingoperators respective operators O, (ii) removal get operators.Eq. 17, applicable s0 , s0 JK = s0L JL K \ {hvL1 /1i , . . . , hvLn /1i}. Thus,Qb () = QbL (L ). Now, -landmark L L, let (L) number instancescost reduced counterparts operators L along L . Eqs. 17 18,L L, L must contain least (L) 1 instances operator get(L). that,149fiDomshlak & MirkisXc() cL (L ) +Xlcost(L)oL LL(o)= cL (L ) +XX((L) 1)lcost(L)(L)lcost(L)= cL (L ) +XX((L) 1)lcost(L)lcost(L)bL +Xlcost(L)= b,thus plan .opposite direction, let = ho1 , . . . , om plan Qb () > 0.definition -landmarks, every landmark Li L presence along . Let(i) , f (i) [n], first occurrenceffoperator Li along , is, f (i) =let = o(1) , . . . , o(k) , k n, operator sequence obtainedargminj[m] {oj Li },ordering operators i[n] {of (i) } consistently . Note that, since -landmarksL necessarily disjoint, may f (i) = f (j) 1 6= j n, thusk strictly smaller n.Given above, let L operator sequence obtained based(1) replacing o(i) along o(i) ,(2) inserting right o(i) arbitrary ordered sequence actionsi1[{get(L) | L L, {o(j) , o(i) } L}.(26)j=1Note set union semantics Eq. 26: even multiple operators {o(1) , . . . , o(i1) }appear landmark L together o(i) , one instance operator get(L)inserted step (2) o(i) .hard verify L applicable s0L , QbL (L ) = Qb (). Now,step (1) expanding L reduces cost operator sequencekXXlcost(L) =i=1 LL(o(i) )X(L)lcost(L),(L) number occurrences operators fromPL . turn, step (2)expanding L increases cost operator sequence ((L) 1)lcost(L).because, Eq. 26, among (L) operators o(i) along L o(i) L,first preceded dedicated instances operator get(L). Thus,XXcL (L ) = c()lcost(L) blcost(L) = bL ,is, L plan L .150fiOn Oversubscription Planning Heuristic SearchLemma 20 Let OSP task, (,Sref ) (, Sref )-compilation , L setlandmarks (,Sref ) , lcost admissible landmark cost function L, Lrespective budget reducing compilation (L, lcost) . Let 1 2 pair plansVL end-states s1 s2 , respectively, sV1 = s2cL (1 ) +Xlcost(L) cL (2 ) +Then, planbLQ (20 ) = QbL (10 ).lcost(L).(20)L:hvL /0is2L:hvL /0is110Xextends 1 , exists plan 20 extends 2Proof: notation claim, proof constructive mapping plan10 corresponding plan 20 .First, derive 10 plan 01 (i) removing f inish operatorget() operators, (ii) replacing instances discounted operatorinstances respective original operator o. results plan 01 := 1 1ePs0 [[1 ]] = sV1 c(1 ) = cL (1 ) +L:hvL /0is1 lcost(L). see latter,operator OL , let () 0 denote number instances along 1 . Given that,c(1 ) = cL (1 )X(get(L))lcost(L) +X= cL (1 ) +lcost(L)= cL (1 ) +XXX(o)o:LL(o)Xlcost(L)(o) (get(L))o:LL(o)(27)lcost(L) 1s1 (hvL /0i)= cL (1 ) +Xlcost(L),L:hvL /0is1second fourth equalities formula manipulations, first equalitydirect construction 1 , third equality definition budgetreducing compilation, specifically, Eqs. 17 18.Similarly construction P1 1 , construct 2 2 ,s0 [[2 ]] = sVc()=c()+22L2L:hvL /0is2 lcost(L). Thus, Eq. 20, c(1 ) c(2 ),also, setting lemma, s0 [[1 ]] = s0 [[2 ]]. Hence, 02 = 2 1e also plan, Qb (01 ) = Qb (02 ).last step, construct 02 plan 20 L claim. First,properties 2 claim, plan 2 achieves landmarks L6s2 = {L |hvL /0i s2 }. Second, definition landmark set L, 1e must satisfy restlandmarks, is, Ls2 = {L | hvL /1i s2 }. Let us denote operator instances along1e ho1 , . . . , ok i, k = |1e |, let {L1 , . . . , Lk } partition Ls2 Li Ls2subset landmarks Ls2 oi first achiever along 1e .Given that, consider operator sequence 2e := (k) , recursively defined via (0) = ,and, Li = , (i) = (i1) hoi i, else (i) = (i1) hoi i, (arbitrary)151fiDomshlak & Mirkissequencing operators{get(L) | L Li hvL /0i s0 J2 KJ (i1) K}.Finally, set 20 := 2 2e .Eqs. 17 18 definition budget reducing compilation,easyPverify construction 2e ensures cL (2e ) = c(1e ) hvL /1is2 lcost(L)QbL (2e ) = Qb (02 ). turn, properties 2 , implies QbL (20 ) = QbL (10 )cL (20 ) = cL (2 ) + cL (2e ).Finally, sinceXlcost(L)cL (2 ) = c(2 )hvL /0is2XcL (2e ) = c(1e )lcost(L),hvL /1is2cL (20 ) = c(2 ) + c(1e )Xlcost(L).Thus, since c(1 ) c(2 ) 01 = 1 1e valid plan ,XcL (20 ) c(1 ) + c(1e )lcost(L)c(01 )Xlcost(L)bXlcost(L),finalizing proof 20 plan L claim.152fiOn Oversubscription Planning Heuristic SearchAppendix B. Detailed Evaluation Results(a)108unsolved107106105hM104103102unsolvedairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel101100 010 101 102 103 104 105 106 107 108blind(b)108unsolved107106hM105104103unsolved102101100 010 101 102 103 104 105 106 107 108basicFigure 18: comparison Figure 8, p. 117, restricted tasks budgeted 25%minimal cost achieving entire set subgoals153fiDomshlak & Mirkis(a)108unsolved107106105hM104103102unsolvedairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel101100 010 101 102 103 104 105 106 107 108blind(b)108unsolved107106hM105104103unsolved102101100 010 101 102 103 104 105 106 107 108basicFigure 19: comparison Figure 8, p. 117, restricted tasks budgeted 50%minimal cost achieving entire set subgoals154fiOn Oversubscription Planning Heuristic Search(a)108unsolved107106105hM104103102unsolvedairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel101100 010 101 102 103 104 105 106 107 108blind(b)108unsolved107106hM105104103unsolved102101100 010 101 102 103 104 105 106 107 108basicFigure 20: comparison Figure 8, p. 117, restricted tasks budgeted 75%minimal cost achieving entire set subgoals155fiDomshlak & Mirkis(a)108unsolved107106105hM104103102unsolvedairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel101100 010 101 102 103 104 105 106 107 108blind(b)108unsolved107106hM105104103unsolved102101100 010 101 102 103 104 105 106 107 108basicFigure 21: comparison Figure 8, p. 117, restricted tasks budgeted 100%minimal cost achieving entire set subgoals156fiOn Oversubscription Planning Heuristic Search(a) blind108unsolved107compile-and-BFBB106105104103102unsolvedairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel101100100 101 102 103 104 105 106 107 108BFBB(b) hM108unsolvedcompile-and-BFBB107106105104103unsolved102101100100 101 102 103 104 105 106 107 108BFBBFigure 22: comparison Figure 15, p. 140, restricted tasks budgeted 25%minimal cost achieving entire set subgoals157fiDomshlak & Mirkis(a) blind108unsolved107compile-and-BFBB106105104103102unsolvedairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel101100100 101 102 103 104 105 106 107 108BFBB(b) hM108unsolvedcompile-and-BFBB107106105104103unsolved102101100100 101 102 103 104 105 106 107 108BFBBFigure 23: comparison Figure 15, p. 140, restricted tasks budgeted 50%minimal cost achieving entire set subgoals158fiOn Oversubscription Planning Heuristic Search(a) blind108unsolved107compile-and-BFBB106105104103102unsolvedairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel101100100 101 102 103 104 105 106 107 108BFBB(b) hM108unsolvedcompile-and-BFBB107106105104103unsolved102101100100 101 102 103 104 105 106 107 108BFBBFigure 24: comparison Figure 15, p. 140, restricted tasks budgeted 75%minimal cost achieving entire set subgoals159fiDomshlak & Mirkis(a) blind108unsolved107compile-and-BFBB106105104103102unsolvedairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel101100 010 101 102 103 104 105 106 107 108BFBB(b) hM108unsolvedcompile-and-BFBB107106105104103unsolved102101100 010 101 102 103 104 105 106 107 108BFBBFigure 25: comparison Figure 15, p. 140, restricted tasks budgeted 100%minimal cost achieving entire set subgoals160fiOn Oversubscription Planning Heuristic Search(a) blind108107inc-compile-and-BFBBairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel106105104103102101100100 101 102 103 104 105 106 107 108BFBB(b) hM108inc-compile-and-BFBB107106105104103102101100100 101 102 103 104 105 106 107 108BFBBFigure 26: comparison Figure 16, p. 142, restricted tasks budgeted 25%minimal cost achieving entire set subgoals161fiDomshlak & Mirkis(a) blind108107inc-compile-and-BFBBairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel106105104103102101100100 101 102 103 104 105 106 107 108BFBB(b) hM108inc-compile-and-BFBB107106105104103102101100100 101 102 103 104 105 106 107 108BFBBFigure 27: comparison Figure 16, p. 142, restricted tasks budgeted 50%minimal cost achieving entire set subgoals162fiOn Oversubscription Planning Heuristic Search(a) blind108107inc-compile-and-BFBBairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel106105104103102101100100 101 102 103 104 105 106 107 108BFBB(b) hM108inc-compile-and-BFBB107106105104103102101100100 101 102 103 104 105 106 107 108BFBBFigure 28: comparison Figure 16, p. 142, restricted tasks budgeted 75%minimal cost achieving entire set subgoals163fiDomshlak & Mirkis(a) blind108107inc-compile-and-BFBBairportblocksdepotdriverlogfreecellgridgripperlogisticsmiconicmysteryopenstackspipesworldpsr-smalltpptrucksroverssatellitezenotravel106105104103102101100 010 101 102 103 104 105 106 107 108BFBB(b) hM108inc-compile-and-BFBB107106105104103102101100 010 101 102 103 104 105 106 107 108BFBBFigure 29: comparison Figure 16, p. 142, restricted tasks budgeted 100%minimal cost achieving entire set subgoals164fiOn Oversubscription Planning Heuristic SearchReferencesBackstrom, C., & Klein, I. (1991). Planning polynomial time: SAS-PUBS class.Computational Intelligence, 7 (3), 181197.Backstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. ComputationalIntelligence, 11 (4), 625655.Baier, J. A., Bacchus, F., & McIlraith, S. (2009). heuristic search approach planningtemporally extended preferences. Artificial Intelligence, 173 (5-6), 593618.Benton, J., Coles, A. J., & Coles, A. I. (2012). Temporal planning preferencestime-dependent continuous costs. Proceedings 22nd International ConferenceAutomated Planning Scheduling (ICAPS), pp. 210.Benton, J., Do, M., & Kambhampati, S. (2009). Anytime heuristic search partial satisfaction planning. Artificial Intelligence, 173 (5-6), 562592.Benton, J., van den Briel, M., & Kambhampati, S. (2007). hybrid linear programmingrelaxed plan heuristic partial satisfaction planning problems. ProceedingsSeventeenth International Conference Automated Planning Scheduling(ICAPS), pp. 3441.Bertsimas, D., & Vempala, S. (2004). Solving convex programs random walks. JournalACM, 51 (4), 540556.Bonet, B. (2013). admissible heuristic SAS+ planning obtained stateequation. Proceedings 23rd International Joint Conference ArtificialIntelligence (IJCAI), pp. 22682274.Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (12), 533.Bonet, B., & Geffner, H. (2008). Heuristics planning penalties rewards formulated logic computed circuits. Artificial Intelligence, 172 (12-13),15791604.Bonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets.Proceedings 19th European Conference Artificial Intelligence (ECAI), pp.329334.Brafman, R. I., & Chernyavsky, Y. (2005). Planning goal preferences constraints.Proceedings International Conference Automated Planning Scheduling, pp. 182191.Clarke, E., Grumberg, O., & Peled, D. (1999). Model Checking. MIT Press.Coles, A. I., Fox, M., Long, D., & Smith, A. J. (2008). Additive-disjunctive heuristicsoptimal planning. Proceedings 18th International Conference AutomatedPlanning Scheduling (ICAPS), pp. 4451.Coles, A. J., Coles, A., Fox, M., & Long, D. (2013). hybrid LP-RPG heuristic modellingnumeric resource flows planning. Journal Artificial Intelligence Research, 46,343412.165fiDomshlak & MirkisColes, A. J., & Coles, A. I. (2011). LPRPG-P: Relaxed plan heuristics planningpreferences. Proceedings 21st International Conference Automated Planning Scheduling (ICAPS), pp. 3745.Cousot, P., & Cousot, R. (1992). Abstract interpretation frameworks. Journal LogicComputation, 2 (4), 511547.Dantzig, G. B. (1963). Linear Programming Extensions. Princeton University Press.Dantzig, T. (1930). Number: Language Science. Macmillan.Desrochers, M., & Soumis, F. (1988). generalized permanent labelling algorithmshortest path problem time windows. Information Systems OperationsResearch, 26, 191212.Do, M. B., Benton, J., van den Briel, M., & Kambhampati, S. (2007). Planning goalutility dependencies. Proceedings 20th International Joint ConferenceArtificial Intelligence (IJCAI), pp. 18721878.Domshlak, C., Hoffmann, J., & Sabharwal, A. (2009). Friends foes? planningsatisfiability abstract CNF encodings. Journal Artificial Intelligence Research,36, 415469.Domshlak, C., Katz, M., & Lefler, S. (2012). Landmark-enhanced abstraction heuristics.Artificial Intelligence, 189, 4868.Dudzinski, K., & Walukiewicz, S. (1987). Exact methods Knapsack problemgeneralizations. European Journal Operational Research, 28, 321.Dvorak, F., & Bartak, R. (2010). Integrating time resources planning. Proceedings 22nd IEEE International Conference Tools Artificial Intelligence(ICTAI), pp. 7178.Edelkamp, S. (2001). Planning pattern databases. Proceedings EuropeanConference Planning (ECP), pp. 8490.Edelkamp, S. (2003). Taming numbers durations model checking integratedplanning system. Journal Artificial Intelligence Research, 20, 195238.Fikes, R. E., & Nilsson, N. (1971). STRIPS: new approach application theoremproving problem solving. Artificial Intelligence, 2, 189208.Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporalplanning problems. Journal Artificial Intelligence Research, 20, 61124.Garey, M. R., & Johnson, D. S. (1978). Computers Intractability: Guide TheoryNP-Completeness. W.H. Freeman Company, New York.Geffner, H., & Bonet, B. (2013). Concise Introduction Models Methods Automated Planning. Synthesis Lectures Artificial Intelligence Machine Learning.Morgan & Claypool.Gerevini, A., Haslum, P., Long, D., Saetti, A., & Dimopoulos, Y. (2009). Deterministicplanning fifth international planning competition: PDDL3 experimentalevaluation planners. Artificial Intelligence, 173 (5-6), 619668.166fiOn Oversubscription Planning Heuristic SearchGerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local searchtemporal action graphs LPG. Journal Artificial Intelligence Research, 20, 239290.Gerevini, A., Saetti, A., & Serina, I. (2008). approach efficient planning numericalfluents multi-criteria plan quality. Artificial Intelligence, 172 (8-9), 899944.Grotschel, M., Lovasz, L., & Schrijver, A. (1981). ellipsoid method consequencestheorems combinatorial optimization. Combinatorica, 1, 169197.Handler, G., & Zang, I. (1980). dual algorithm constrained shortest path problem.Networks, 10, 293310.Haslum, P. (2013). Heuristics bounded-cost search. Proceedings 23rd International Conference Automated Planning Scheduling (ICAPS), pp. 312316.Haslum, P., Bonet, B., & Geffner, H. (2005). New admissible heuristics domainindependent planning. Proceedings 20th National Conference ArtificialIntelligence (AAAI), pp. 11631168.Haslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independentconstruction pattern database heuristics cost-optimal planning. Proceedings19th National Conference Artificial Intelligence (AAAI), pp. 10071012.Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. Proceedings 15th International Conference Artificial Intelligence Planning Systems(AIPS), pp. 140149.Haslum, P., & Geffner, H. (2001). Heuristic planning time resources. Proceedings6th European Conference Planning (ECP), pp. 107112.Helmert, M. (2002). Decidability undecidability results planning numericalstate variables. Proceedings Sixth International Conference ArtificialIntelligence Planning Scheduling (AIPS), pp. 4453.Helmert, M. (2006). Fast Downward planning system. Journal Artificial IntelligenceResearch, 26, 191246.Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whatsdifference anyway?. Proceedings 19th International Conference Automated Planning Scheduling (ICAPS), pp. 162169.Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics optimalsequential planning. Proceedings 17th International Conference AutomatedPlanning Scheduling (ICAPS), pp. 200207.Helmert, M., Haslum, P., Hoffmann, J., & Nissim, R. (2014). Merge-and-shrink abstraction:method generating lower bounds factored state spaces. Journal ACM,61 (3), 16:163.Hoffmann, J. (2003). Metric-FF planning system: Translating ignoring delete listsnumeric state variables. Journal Artificial Intelligence Research, 20, 291341.Hoffmann, J., Gomes, C. P., Selman, B., & Kautz, H. A. (2007). SAT encodings statespace reachability problems numeric domains. Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI), pp. 19181923.167fiDomshlak & MirkisHoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generationheuristic search. Journal Artificial Intelligence Research, 14, 253302.Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks planning. JournalArtificial Intelligence Research, 22, 215278.Karp, R. (1972). Reducibility among combinatorial problems. Complexity ComputerComputations, pp. 85103. Plenum Press, New York.Karpas, E., & Domshlak, C. (2009). Cost-optimal planning landmarks. ProceedingsInternational Joint Conference Artificial Intelligence (IJCAI-09), pp. 17281733.Katz, M., & Domshlak, C. (2010a). Implicit abstraction heuristics. Journal ArtificialIntelligence Research, 39, 51126.Katz, M., & Domshlak, C. (2010b). Optimal admissible composition abstraction heuristics. Artificial Intelligence, 174, 767798.Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack Problems. Springer-VerlagBerlin.Keyder, E., & Geffner, H. (2009). Soft goals compiled away. Journal ArtificialIntelligence Research, 36, 547556.Koehler, J. (1998). Planning resource constraints. Proceedings 13th EuropeanConference Artificial Intelligence (ECAI), pp. 489493.Mirkis, V., & Domshlak, C. (2013). Abstractions oversubscription planning. Proceedings 23rd International Conference Automated Planning Scheduling(ICAPS), pp. 153161.Mirkis, V., & Domshlak, C. (2014). Landmarks oversubscription planning. Proceedings23rd European Conference Artificial Intelligence (ECAI), pp. 633638.Nakhost, H., Hoffmann, J., & Muller, M. (2012). Resource-constrained planning: MonteCarlo random walk approach. Proceedings 22nd International ConferenceAutomated Planning Scheduling (ICAPS), pp. 181189.Nebel, B. (2000). compilability expressive power propositional planningformalisms. Journal Artificial Intelligence Research, 12, 271315.Nemirovsky, A., & Yudin, N. (1994). Interior-Point Polynomial Methods Convex Programming. SIAM.Pearl, J. (1984). Heuristics - Intelligent Search Strategies Computer Problem Solving.Addison-Wesley.Pommerening, F., & Helmert, M. (2013). Incremental LM-Cut. Proceedings 23rdInternational Conference Automated Planning Scheduling (ICAPS), pp. 162170, Rome, Italy.Porteous, J., Sebastia, L., & Hoffmann, J. (2001). extraction, ordering, usagelandmarks planning. Proceedings 6th European Conference Planning(ECP 01), pp. 3749.168fiOn Oversubscription Planning Heuristic SearchPunnen, A. P. (1992). K-sum linear programming. Journal Operational ResearchSociety, 43 (4), 359363.Richter, S., Helmert, M., & Westphal, M. (2008). Landmarks revisited. Proceedings23rd AAAI Conference Artificial Intelligence (AAAI-08), pp. 975982.Russell, S., & Norvig, P. (2009). Artificial Intelligence: Modern Approach (3 edition).Pearson.Sanchez, R., & Kambhampati, S. (2005). Planning graph heuristics selecting objectives over-subscription planning problems. Proceedings 15th InternationalConference Automated Planning Scheduling (ICAPS), pp. 192201.Smith, D. (2004). Choosing objectives over-subscription planning. Proceedings14th International Conference Automated Planning Scheduling (ICAPS), pp.393401.Thayer, J. T., & Ruml, W. (2011). Bounded suboptimal search: direct approach usinginadmissible estimates. Proceedings 22nd International Joint ConferenceArtificial Intelligence (IJCAI), pp. 674679.Thayer, J. T., Stern, R. T., Felner, A., & Ruml, W. (2012). Faster bounded-cost searchusing inadmissible estimates. Proceedings 22nd International ConferenceAutomated Planning Scheduling (ICAPS), pp. 270278.van den Briel, M., Sanchez, R., Do, M. B., & Kambhampati, S. (2004). Effective approachespartial satisfaction (over-subscription) planning. Proceedings 19th AAAIConference Artificial Intelligence (AAAI), pp. 562569.van den Briel, M., Benton, J., Kambhampati, S., & Vossen, T. (2007). LP-based heuristic optimal planning. Proceedings 13th International ConferencePrinciples Practice Constraint Programming (CP), pp. 651665.Yang, F., Culberson, J., Holte, R., Zahavi, U., & Felner, A. (2008). general theoryadditive state space abstractions. Journal Artificial Intelligence Research, 32,631662.169fi