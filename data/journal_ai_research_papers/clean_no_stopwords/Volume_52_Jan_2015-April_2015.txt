Journal Artificial Intelligence Research 52 (2015) 601-713

Submitted 08/14; published 04/15

Compositional Framework Grounding Language
Inference, Generation, Acquisition Video
Haonan Yu
N. Siddharth
Andrei Barbu
Jerey Mark Siskind

haonan@haonanyu.com
siddharth@iffsid.com
andrei@0xab.com
qobi@purdue.edu

School Electrical Computer Engineering
Purdue University
465 Northwestern Avenue
West Lafayette, 47907-2035 USA

Abstract
present approach simultaneously reasoning video clip entire
natural-language sentence. compositional nature language exploited construct
models represent meanings entire sentences composed meanings
words sentences mediated grammar encodes predicate-argument
relations. demonstrate models faithfully represent meanings sentences
sensitive roles played participants (nouns), characteristics (adjectives), actions performed (verbs), manner actions (adverbs), changing
spatial relations participants (prepositions) affect meaning sentence
grounded video. exploit methodology three ways. first, video
clip along sentence taken input participants event described
sentence highlighted, even clip depicts multiple similar simultaneous
events. second, video clip taken input without sentence sentence
generated describes event clip. third, corpus video clips paired
sentences describe events clips meanings
words sentences learned. learn meanings without needing specify
attribute video clips word given sentence refers to. learned
meaning representations shown intelligible humans.

1. Introduction
People use knowledge language make sense world around them,
describe observations communicate others. work, present
approach able describe video clips natural language simultaneously
using capacity reason content clips. earlier approaches
detect individual features video (Laptev, 2005; Kuehne, Jhuang, Garrote, Poggio, &
Serre, 2011), objects events, show knowledge language integrate
information different feature detectors order improve performance
support novel functionality. this, exploit compositional nature language
construct models entire sentences individual word models, use models
determine entire sentence describes video clip. call mechanism determining
well video clip depicts sentence, alternatively well sentence describes
c
2015
AI Access Foundation. rights reserved.

fiYu, Siddharth, Barbu, & Siskind

video clip, sentence tracker (Yu & Siskind, 2013; Siddharth, Barbu, & Siskind, 2014),
simultaneously performs multi-object tracking recognition events described
sentences. ability score video-sentence pairs also allows us perform another
important task humans naturally engage in: learning word meanings. show
sentence tracker perform task using kind information
available children, namely, video paired entire sentences describe
events depicted. general-purpose inference mechanism combining bottom-up
information low-level video-feature detectors top-down information naturallanguage semantics allows us perform three novel tasks: tracking objects
engaged specific event described sentence, generating sentence describe
video clip, learning word meaning video clips paired entire sentences.
Fundamentally, approach relies solving two separate problems simultaneously:
tracking participants event recognizing occurrence event.
formulate combination two measures: measure well video clip
depicts track collection well track collection depicts event. Note
mean event complex state affairs described entire sentence,
common definition used computer vision community, refers single verb
label attached video clip. order solve problems simultaneously, show
similarity tracking event recognition facilities common inference
algorithm. perform single-object tracking combining output unreliable
detection source, object detector, estimate motion present video,
optical flow. tracks produced consist strong detections motion agrees
motion present video. perform single-word recognition representing
meaning word terms gross motion object tracks. Finally, show
single-object tracking single-word recognition combine perform multi-object tracking
whole-sentence recognition exploiting compositionality language combine
word models sentence models formulating tasks way amenable
dynamic programming.
ability perform tasks simultaneouslyin words, score videosentence pair well video clip depicts sentenceis crucial attaining
good performance. integrating top-down bottom-up information, corrects errors
object-detector output. important object detectors highly unreliable,
achieving 40%-50% average precision PASCAL Visual Object Classes (VOC)
challenge (Everingham, Van Gool, Williams, Winn, & Zisserman, 2010). Barbu, Siddharth,
Michaux, Siskind (2012b) showed reliability object tracking singleword recognition (typically verb) improved performing simultaneously.
build earlier work extend track multiple objects recognize whole
sentences. extend work novel approach sentence generation
learning word meanings.
Following Yamoto, Ohya, Ishii (1992), Siskind Morris (1996), Starner,
Weaver, Pentland (1998), represent word meanings fashion grounded
video multi-state time-series classifiers, either hidden Markov models (HMMs)
finite-state machines (FSMs), features extracted object tracks video.
example, model approach might use three states encode event distance two tracked objects initially high, time decreases, finally ends
602

fiGrounding Language Inference, Generation, Acquisition Video

small. earlier approaches confined representing meaning
verbs, employ representation words lexicon regardless
part speech. allows us combine word models together sentence
models, essence, creating large factorial models. Unlike earlier work (Kulkarni, Premraj, Dhar, Li, Choi, Berg, & Berg, 2011; Hanckmann, Schutte, & Burghouts, 2012; Barbu,
Bridge, Burchill, Coroian, Dickinson, Fidler, Michaux, Mussman, Siddharth, Salvi, Schmidt,
Shangguan, Siskind, Waggoner, Wang, Wei, Yin, & Zhang, 2012a; Krishnamoorthy, Malkarnenkar, Mooney, Saenko, & Guadarrama, 2013), exploit linguistics, namely concept
linking, construct particular factorial model encodes predicate-argument
structure specific sentence, sentences happen share words.
example sentence, person picked backpack different meaning
sentence backpack picked person, despite sharing words,
method encodes distinctions.
overview operation sentence tracker shown Figure 1. Information
extracted video using object detectors optical flow, discussed Section 2.1.
Independently, sentence parsed number participants determined, together
linking function, discussed Sections 3. word sentence
associated model, discussed Section 2.2. information extracted sentence
combines per-word models form model entire sentence, discussed
Sections 2.3 2.4. model takes, input, data extracted video clip
computes well clip depicts given sentence, video-sentence score shown
Equation 10.
order formally articulate approach applications, represent
measure well video clip depicts sentence function : (B, s, ) 7 ( , J),
B represents information extracted video clip, represents sentence,
represents word meanings, video-sentence score, J collection tracks,
one participant event described sentence, corresponding optimal
video-sentence score. use SJ refer two components produced S.
function internally makes use number L event participants , linking function.
linking function maps arguments words sentence event participants.
make use linking process, function : 7 (L, ), described Section 3,
derive number L participants linking function . elaborate
three applications approach demonstrate: language inference, language
generation, language acquisition.
language inference, one apply sentence tracker video clip B,
depicts multiple simultaneous events taking place field view, two different sentences s1 s2 . words, one compute J1 = SJ (B, s1 , ) J2 = SJ (B, s2 , )
yield two different track collections J1 J2 corresponding different sets
participants different events described s1 s2 . demonstrate Section 5.3. Specifically, show language inference, unlike many approaches
event recognition, deals video depicts multiple simultaneous events,
also sensitive subtle changes sentence meaning. present experiment
construct minimal pairs sentences, given grammar, differ single lexical
constituent, varying lexical constituent vary among parts speech
sentential positions. example two sentences
603

fiYu, Siddharth, Barbu, & Siskind

Section 3

Section 2.1

sentence

video B

linking process
number
participants L

object
detections

linking function

optical flow

lexicon
sentence tracker
word models

Sections 2.3 2.4

Section 4

Section 2.2
track collection J

video-sentence score

Equation 10
Figure 1: overview approach presented roadmap presentation. Section 5.3 demonstrates language inference. Section 5.4 demonstrates language generation.
Section 5.5 demonstrates language acquisition.
person left trash put object.
person right trash put object.
minimal pairs differ preposition attached subject noun phrase.
construct video corpus sentences minimal pairs occur simultaneously
video clip demonstrate language inference sensitive changes
sentential meaning producing two distinct semantically appropriate sets tracks
given two sentences input. conduct thorough1 evaluation, employ
vocabulary 17 lexical items (5 nouns, 2 adjectives, 4 verbs, 2 adverbs, 2 spatial-relation
prepositions, 2 motion prepositions) video corpus 94 clips.
1. thorough mean following:
1. evaluate three applications general method: inference, generation, acquisition.
2. show performance entire corpus, without cherry picking.
3. illustrate deep semantic grounding way minimal pairs vary lexical items
sentential positions.
4. demonstrate deep semantic grounding rendering thematic-role assignments sentences
associated videos, illustrating correct assignment event participants roles predicate
arguments.
5. compare learned models ground-truth meaning representations precisely measure
KL divergence Table 10.

604

fiGrounding Language Inference, Generation, Acquisition Video

language generation, take video clip B input systematically search
space possible sentences s, generated context-free grammar, find
sentence maximal video-sentence score:
argmax (B, s, )


generates sentence best describes input video clip B. demonstrate
Section 5.4. Unlike previous approaches sentence generation video
largely ad hoc(Barbu et al., 2012a; Hanckmann et al., 2012; Krishnamoorthy et al., 2013),
present approach optimal, sense generated sentence
produce highest video-sentence score. evaluation language generation
uses video corpus, grammar, lexicon used language inference.
language acquisition, exploit fact simultaneously reason
presence motion participants video clip meaning sentence
describing clip compute models word meaning training set video clips
paired sentences. words, given training set {(B1 , s1 ), . . . , (BM , sM )}
video-sentence pairs word meanings unknown, compute
argmax



X

(Bm , sm , )

m=1

finds word meanings maximize aggregate score video-sentence
pairs training set. demonstrate Section 5.5. learn word meanings
without needing annotate word refers attribute video without
annotating tracks objects participate event described training
sentences. conduct thorough evaluation, employ vocabulary 16 lexical items
(6 nouns, 4 verbs, 2 adverbs, 2 spatial-relation prepositions, 2 motion prepositions)
video corpus 94 clips total 276 video-sentence pairs constructed.
central contribution work sentence tracker, precise mathematical
computational framework performing simultaneous object detection, multi-object tracking, action recognition, recognition multiple predicates assigned different subsets
participants, culminating Equation 10, implemented efficient algorithm
illustrated Figures 11 12, implements exact inference joint model, along
method training solely videos paired sentential annotation.
current focus computational linguistics community large-scale unrestricted text processing long time now. computer vision community currently
undergoing similar transition towards processing large-scale unrestricted image video
corpora. sentence tracker intended process unrestricted text video.
intended produce natural-sounding text descriptions video. concerned
semantics, reflected truth text descriptions accuracy
learned meaning representations. Moreover, evaluate corpus considerably
smaller currently used computational linguistics computer
vision communities. intend work address orthogonal set
concerns:
1. provide unified framework supports inference, generation, acquisition.
605

fiYu, Siddharth, Barbu, & Siskind

2. demonstrate learns correct meanings words, prior meanings
words, video paired whole sentences, manual guidance
words correspond components video.
3. demonstrate deep understanding sentential semantics, grounded
video, derived systematic computational process deep
word meanings grounded video.
4. deep understanding allows framework distinguish subtle semantic
distinctions manifest two sentences differ single word word
order, i.e., understands mapping objects detected video
particular semantic roles play sentences.
mean greater lesser limitations current work computational linguistics computer vision. Different research different limitations.
four points highlight limitations current work exhibits absent
work presented here.

2. Joint Tracking Event Recognition
represent word meanings, ultimately sentence meanings, constraints
time-varying spatial relations event participants: relative and/or absolute
positions, velocities, and/or accelerations. requires track positions
event participants course video clip. ideal world, would able
accurately determine object classes present video frame
are, precisely determine positions instances classes field
view. Unfortunately, current state art object detection far ideal.
Object detectors achieve 3.8% 65% average precision PASCAL
VOC benchmark (Everingham et al., 2010). means that, practice, suffer
false positives false negatives, illustrated Figure 2. wish produce
single detection person backpack, shown Figure 2(a), practice,
often get spurious detections (false positives), happens person detector
Figure 2(b), fail obtain desired detection (false negatives), happens
backpack detector Figure 2(c).
2.1 Detection-Based Tracking
general approach resolving problem overgenerate. lower acceptance
threshold detector, trading higher false-positive rate lower false-negative
rate, Figure 2(d). attempt lower threshold sufficiently completely eliminate false negatives, biasing preponderance false positives. tracking
problem reduces problem selecting detections frames video clip
assemble coherent tracks.
Let us assume, moment, wish track single instance specified object
class known present field view throughout video clip. track object
selecting single detection frame pool detections object
class. sequence top-scoring detection frame might temporally
coherent, shown Figure 3(a). Likewise, temporally-coherent sequence
detections might consist low-scoring misdetections, shown Figure 3(b). Thus
606

fiGrounding Language Inference, Generation, Acquisition Video

(a)

(b)

(c)

(d)

Figure 2: State-of-the-art object detectors imperfect. wish single detection
person backpack, (a), practice often get spurious detections (false
positives), (b), fail obtain desired detection (false negatives), (c).
Reducing acceptance threshold biases detector trade higher false-positive
rate lower false-negative rate, (d).

approach balance two extremes incorporating detection score
temporal-coherence score selection criterion. often yield desired track,
shown Figure 3(c).
adopt objective function linearly combines sum detection
scores video frames sum temporal-coherence score applied pairs
adjacent video frames. formally, video clip B frames, J detections bt1 , . . . , btJ frame t, seek track j, namely sequence j 1 , . . . , j detection
indices, maximizes sum detection scores f (btj ) temporal-coherence
scores g(bt1
, bt ):
j t1 j
max
j


X
t=1

f (btj )

!

+


X
t=2

607

g(bt1
, bt )
j t1 j

!

(1)

fiYu, Siddharth, Barbu, & Siskind

(a)

(b)

(c)

Figure 3: Assembling track single detection per frame selected pool
overgenerated detections. Selecting top-scoring detection frame video
clip yield incoherent track, shown (a). Selecting tracks maximize temporal
coherence lead tracks incorporating solely low-scoring misdetections, shown (b).
Selecting tracks maximize appropriate combination detection score temporalcoherence score lead desired track, shown (c).
objective function Equation 1 constitutes measure well video clip B depicts
track j. employ particular objective function optimized efficiently
dynamic programming (Bellman, 1957), namely Viterbi (1967) algorithm.
leads lattice, shown Figure 4. columns lattice correspond video
frames, detections frame constitute columns, track constitutes path
lattice.
general approach tracking overgenerating detections selecting among
yield track known detection-based tracking (Han, Sethi, Hua, & Gong,
2004; Avidan, 2004; Wu & Nevatia, 2007). approach using Viterbi algorithm
purpose first explored Wolf, Viterbi, Dixon (1989) track radar detections.
relies analogy:
... detections correspond HMM states, detection score corresponds
HMM output probability, temporal-coherence score corresponds
HMM state-transition probability, finding optimal track corresponds
finding maximum posteriori probability (MAP) estimate HMM
state sequence (where computation MAP estimate performed log
space).
crucially rely analogy entire remainder paper.
trivially modified denote MAP estimate log space suitable
normalization constant factor. purposes, however, relevant
optimizes linear combination two score components: sum state-based scores
sum transition-based scores. particular, Viterbi algorithm applied
Equation 1, without constraint permissible values scores f (b) g(b , b).
608

fiGrounding Language Inference, Generation, Acquisition Video

t=1

t=2

t=3

j=1

b11

b21

b31

...

bT1

j=2

b12

b22

b32

...

bT2

j=3

b13

b23

b33

...

bT3

..
.

..
.

..
.

b1J 1

b2J 2

b3J 3

j = Jt

t=T

..
.
...

bTJ

g
f
detection temporal
score coherence
score
Figure 4: lattice constructed Viterbi algorithm detection-based tracking.
columns correspond video frames = 1, . . . , . column contains overgenerated collection bt1 , . . . , btJ detections frame. rows correspond detection
indices j. track j, namely sequence j 1 , . . . , j detection indices, corresponds
path lattice. Viterbi algorithm finds path optimizes Equation 1,
among exponentially many potential tracks, time O(T J 2 ), J maximum
J 1 , . . . , J .
detection-based tracking framework general. use detection
source(s), method f (b) scoring detections b, method g(b , b) scoring temporal coherence detections b b adjacent frames. work reported here, use deformable part model (DPM) detector (Felzenszwalb, Girshick,
McAllester, & Ramanan, 2010a; Felzenszwalb, Girshick, & McAllester, 2010b) detection source, yields detections represented axis-aligned rectangles use
scores provided DPM basis f (b). raw DPM score ranges
. Nominally, Equation 1 Viterbi algorithm support scores. However,
raw DPM scores, unfortunately, incomparable across object classes. reasons
discussed Section 2.3, joint tracking multiple objects requires detection
scores comparable across object classes. Moreover, reasons discussed
Section 4, language acquisition requires moderately accurate indication object
classes present field view, could ascertained detection scores
comparable across object classes. address above, normalize detection
scores f (b) within object class using sigmoid
1
1 + exp((f (b) ))
609

fiYu, Siddharth, Barbu, & Siskind

parameters empirically determined per object class detection
score correlates probability detection true positive. convert this,
values discussed later sections, log space, protect underflow
floating-point calculations. Choosing parameters fashion per-class
basis allows resulting detection scores comparable across classes. Note
resulting values f (b) range (, 0], take represent log
probabilities.
use optical flow compute adjacent-frame temporal-coherence score. employ
FlowLib optical-flow library (Werlberger, Pock, & Bischof, 2010) one
highest-performing methods optical-flow benchmarks (Baker, Scharstein, Lewis, Roth,
Black, & Szeliski, 2011). specifically, compute g(bt1
, bt ), compute optical
j t1 j
flow frame 1, compute average flow vector v inside axis-aligned rectangle
detection bt1
, forward project detection one frame translating rectangle
j t1
along v, compute square Euclidean distance center
translated rectangle center corresponding rectangle btj . yields
value measures well local detection displacement matches local estimate
velocity ranges 0 fashion inversely related temporal
coherence. wish value comparable detection score f (b) temporal
coherence neither overpowers overpowered detection score. Thus normalize
temporal coherence sigmoid well, using negative invert polarity,
convert log space. Unlike detection score, single set sigmoid parameters
used across object classes, temporal-coherence score depends
detection centers. Note again, resulting values g(b , b) range
(, 0], take represent log probabilities. Moreover, even though
values f (b) g(b , b) range (, 0], values produced Equation 1
also lie range, represent log probabilities.
2.2 Event Recognition Based Motion Prole using HMMs
Given particular track collection, one determine whether tracks depict given
event measuring time-varying properties tracks. properties could
relative and/or absolute object positions, velocities, and/or accelerations. time-varying
properties represented abstractly time-series feature vectors computed
tracks. view, event recognition formulated time-series classification.
classification performed hidden Markov models (HMMs), either computing likelihood MAP estimate. Let us limit consideration, moment, events
single participant. case, abstractly take HMM consist
K states, state-transition function a(k , k) log space, output model h(k, b)
denotes log probability generating detection b state k. Let us refer
collection K, a, h event model . log space, MAP estimate
particular track j
!
!


X
X

t1
max
h(k , bj ) +
a(k , k )
(2)
k

t=1

t=2

k1 , . . . , kT

k sequence
states. Let Bj denote detection sequence b1j 1 , . . . , bTjT
selected video clip B track j. Equation 2 constitutes measure
610

fiGrounding Language Inference, Generation, Acquisition Video

well detection sequence Bj selected video clip B track j depicts event
model . Higher MAP estimates result tracks better depict event model.
MAP estimates computed efficiently using Viterbi algorithm time O(T K 2 ).
Note similarity Equations 2 1. due aforementioned analogy.
Momentarily, crucially avail fact computed
Viterbi algorithm. first need address several subtleties formulation.
use HMMs encode probability distributions time-series feature vectors
extracted object tracks. turn serve represent meanings verbs
describe motion participant objects. example, meaning word
bounce might represented HMM, like Figure 5, places high probability track exhibits alternating downward upward motion.
representations tolerant noisy input learned using Baum-Welch (Baum,
Petrie, Soules, & Weiss, 1970; Baum, 1972), HMMs many states, many features,
non-sparsely populated state-transition functions output models difficult humans understand create. Sections 5.3 5.4, conduct experiments
human-generated meaning representations. While, Section 5.5, conduct experiments
machine-learned meaning representations, also compare human-generated
meaning representations. facilitate perspicuity human-generated meaning representations, adopt regular-expression notation, following representation
meaning word bounce:


bounce = (movingDown+ movingUp+ )+
above, movingDown(b) movingUp(b) predicates detections b
used construct output model h(k, b) regular expression used determine
number K states, state-transition function a(k , k), predicate employ
output model given state. straightforwardly converted finitestate machines (FSMs) can, turn, viewed special case HMMs 0/1
state-transition functions output models (/0 log space).
Equation 2 formulated abstractly around single state-transition function a(k , k).
also must include distributions initial final states. Traditional HMM formulations
incorporate initial-state distributions final-state distributions. HMMs
might recognize prefix event specification constrained match
entire event specification. (Without initial-state distribution, might recognize
subinterval event specification.) actual formulations include initial-
final-state distributions omit presentation sake expository
clarity.
Formulating output model h(k, b) depend detections single
track allows HMM encode time-varying constraints single track.
used represent meaning intransitive verb describes motion single
participant. wish, however, also able represent meanings transitive verbs
describe motion pairs participants. accomplish extending
output model h(k, b1 , b2 ) depend pairs detections, one track.
two distinct tracks j1 = (j11 , . . . , j1T ) j2 = (j21 , . . . , j2T ) two distinct participants,
think deriving detection pool. allows extending
611

fiYu, Siddharth, Barbu, & Siskind

0.01
0.99

0.99

0.98

0.01

0.01

0.01

0

1.0

upward

rightward

leftward

downward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

upward

rightward

leftward

downward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

upward

rightward

leftward

downward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
velocity orientation
first argument first argument

Figure 5: HMM represents meaning word bounce track exhibits
alternating downward upward motion.
Equation 2
max
k


X
t=1

h(k , btj , btj )
1

2

!

+


X
t=2

a(k t1 , k )

!

(3)

support this.
HMMs susceptible short-term noise input signal. one
event model, Figure 6(a), intended match time series
interval velocity zero, followed interval upward
motion, followed interval velocity zero, may unintentionally
match time series interval upward motion single frame
spurious result noisy tracking feature extraction. thing might
happen FSM representation


rest(b1 , b2 ) = stationary(b1 ) stationary(b2 ) close(b1 , b2 )


action(b1 , b2 ) = stationary(b1 ) movingUp(b2 ) close(b1 , b2 )


pick = rest+ action+ rest+
intended model meaning pick period time agent
stationary close patient subdivided three sequential intervals
612

fiGrounding Language Inference, Generation, Acquisition Video

0.99

0.99
0.01

1.00
0.01

0

1.0

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

0.99

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
first argument

(a)

0.99

0.99

0.99

0.01

0.01

0.01

1.00
0.01

0

1.0

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
first argument

(b)

Figure 6: (a) HMM susceptible short-term noise input signal.
central state might admit noisy impulse lasting single frame. (b) variant (a)
constrains central interval hold least 3 frames.
patient first stationary, moves up, stationary again.
unintentionally match time series patient continually stationary except
single frame spurious result noisy tracking feature extraction.
address issue requiring central interval minimum duration.


indicate regular-expression operator R{n,} = R
. . R} R indicate
| .{z

R must repeated least n times. definition

n



pick = rest+ action{3,} rest+

reduced FSM within framework. Similarly, one add minimum stateduration requirement HMM, Figure 6(a), recoding Figure 6(b).
handles short-term false positives, namely presence short-term spuriously true signal. also need handle short-term false negatives, namely intended
613

fiYu, Siddharth, Barbu, & Siskind

longer interval signal must meet specified condition fails due
short-term failure meet condition. use new regular-expression operator


R[n,] = (R [true]){n,} indicate R must repeated least n times optionally single frame noise repetition. One extend HMMs
similar fashion though found need output models
already tolerate noise.
Nominally, detections btj axis-aligned rectangles represented image coordinates. allows output models h(k, b) depend quantities computed
such, e.g., position detection center, size detection, aspect
ratio detection, indicate notions like big, small, tall, wide. also allows
two-track output models h(k, b1 , b2 ) depend quantities like distance detection centers orientation line centers, indicate notions
like close, far, above, below. Without information, possible output
models depend relative absolute velocity, would needed encode notions
like fast, slow, stationary, moving, upwards, downwards, towards, away from. One way
achieve would extend output models depend detections adjacent frames, h(k, b , b) h(k, b1 , b1 , b2 , b2 ). accomplish variant
Equation 2 sums pairs adjacent detections.
!

X
t1

t1
max
h(k , bj t1 , bj ) + a(k , k )
k

t=2

generalized extending sums three adjacent frames acceleration, even frames longer-term velocity acceleration. However,
multiple-point estimates, e.g., two-point velocity estimates three-point acceleration estimates, suffer noise due inaccurate tracking. Moreover, extensions would
support desired features could extracted image, color.
Thus instead extend notion detection include information might
extracted image location detection, average hue optical
flow inside detection, retain initial formulation output models h(k, b)
h(k, b1 , b2 ) depends detections single frame.
2.3 Event Tracker
aforementioned method operates feed-forward pipeline. Equation 1 produces tracks
event participants, time series feature vectors extracted tracks,
time series classified HMMs detect verb/event occurrences. approach,
however, brittle. Failure earlier pipeline necessarily leads failure later
pipeline. particularly concern, since pipeline starts object detections
and, mentioned before, state-of-the-art object detection unreliable.
Barbu et al. (2012b) presented novel approach addressing brittleness called
event tracker. approach originates observation Equations 1 2
share structure due aforementioned analogy, thus share analogous
algorithmic framework performing optimization analogous lattices.
feed-forward pipeline essentially cascades algorithms lattices, shown Figure 7(a). independently optimizes Equation 1, measure well video clip B
614

fiGrounding Language Inference, Generation, Acquisition Video

depicts track j, Equation 2, measure well detection sequence Bj
selected video clip B track j depicts event model , performing former latter, constructing latter optimization problem around track j
produced former. takes Equation 2 sole measure well video
clip B depicts event model . precisely, performs following optimization:
!
!


X
X
max
h(k , btt ) +
a(k t1 , k )
k
t=1
t=2
!
!
(4)


X
X
t1
= argmax
f (btj ) +
g(bj t1 , btj )
j

t=1

t=2

measure well detection sequence Bj selected video clip B
track j depicts event model , might measure well video clip B
depicts event model fails incorporate measure well
video clip B depicts track j. Thus, might instead take sum Equations 1 2
measure well video clip B depicts event model . precisely,
could adopt following measure involves optimization Equation 4:
!# "
"
!
!
!#




X
X
X
X
t1


t1

max
h(k , bt ) +
f (bj ) +
a(k , k )
g(bj t1 , bj )
+ max
j
k
t=1
t=2
t=1
t=2
!
!


X
X
g(bt1
, bt )
f (btj ) +
= argmax
j t1 j
j

t=2

t=1

(5)
still independently optimizes track j Equation 1 state sequence k
Equation 2. could, however, attempt jointly optimize track j state
sequence k. could done lifting maximizations track j
state sequence k outside summation measures well video clip B
depicts track j well detection sequence Bj selected video clip B
track j depicts event model . leads following optimization problem:
!
!
!
!




X
X
X
X
max
a(k t1 , k )
(6)
f (btj ) +
g(bt1
, bt ) +
h(k , btj ) +
j t1 j
j,k

t=1

t=2

t=1

t=2

crucial observation Barbu et al. (2012b) Equation 6 structure
Equations 1 2 optimized using Viterbi algorithm forming
cross-product tracker HMM lattices, shown Figure 7(b), node
resulting lattice combines detection HMM state, shown Figure 7(c).
Since width cross-product lattice O(JK), applying Viterbi algorithm
cross-product lattice finds path optimizes Equation 6, among exponentially
many potential paths, time O(T (JK)2 ).
Like before, Equation 6 trivially modified denote MAP estimate log space
suitable normalization. However, need constant factor
introduced normalization would change result joint optimization
track j state sequence k. Like before, Viterbi algorithm applied
615

fiYu, Siddharth, Barbu, & Siskind

(a)

t=1

t=2

t=3

j=1

b11

b21

b31

...

bT1

j=2

b12

b22

b32

...

bT2

j=3

b13

b23

b33

...

bT3

j = Jt

..
.

..
.

..
.

b1J 1

b2J 2

b3J 3

t=1

t=2

t=3

k=1

1

1

1

...

1

k=2

2

2

2

...

2

k=3

3

3

3

...

3

..
.

..
.

..
.

K

K

K

t=T

..
.
...

bTJ

k=K

f
g
detection temporal
score coherence
score

max
j

(b)


X
t=1

tracker lattice
!
!

X
t1


f (bj ) +
g(bj t1 , bj )

max
k

t=2

t=1

t=2

t=3

b11

b21

b31

...

bT1

j=2

b12

b22

b32

...

bT2

j=3

b13

b23

b33

...

bT3

..
.

..
.

..
.

b1J 1

b2J 2

b3J 3


X
t=1

...

K

verb lattice
!
!

X

t1
h(k , bj ) +
a(k , k )
t=2

t=1

t=2

t=3

k=1

1

1

1

...

1

k=2

2

2

2

...

2

k=3

3

3

3

...

3

..
.

..
.

..
.

K

K

K

t=T



..
.
...

bTJ

k=K

f
g
detection temporal
score coherence
score

t=T

..
.
...

K

h

output state
model transition
function

tracker lattice
!
!


X
X
t1


max
f (bj ) +
g(bj t1 , bj ) +
j,k

..
.

h

output state
model transition
function

j=1

j = Jt

t=T

t=1

t=2

j=1

j = Jt


X
t=1

verb lattice
!
!

X

t1
h(k , bj ) +
a(k , k )
t=2

t=1

t=2

t=3

b11 , 1

b21 , 1

b31 , 1

t=T

..
.

..
.

..
.

b1J 1 , 1

b2J 2 , 1

b3J 3 , 1

...

bTJ , 1

b11 , 2

b21 , 2

b31 , 2

...

bT1 , 2

..
.

..
.

..
.

b2J 2 , K

b3J 3 , K

...

bT1 , 1
..
.

(c)

b1J 1 , K

g,

..
.
...

bTJ , K

f, h

Figure 7: (a) pipeline consisting cascade tracker lattice followed HMM lattice
used verb/event recognition. (a), finding track j optimizes measure well
video clip B depicts track, Equation 1, happens independently prior finding
state sequence k optimizes measure well detection sequence Bj selected
video clip B track j depicts event model , Equation 2, latter depending
track j produced former. Since portion Equation 2 used measure
well video clip B depicts event model , corresponds optimizing scoring function
Equation 4. Taking measure well video clip B depicts event model
combination measures well video clip B depicts track j well detection
sequence Bj selected video clip B track j depicts event model viewed
optimizing scoring function Equation 5, sum two measures. (b) variant (a)
jointly optimizes two measures corresponding optimization Equation 6 migrates
optimization outside sum. (c) method performing joint optimization (b)
forming cross-product lattice.
616

fiGrounding Language Inference, Generation, Acquisition Video

Equation 6 without constraint permissible values detection score f (b),
temporal-coherence score g(b , b), output model h(k, b), state-transition function
a(k , k) However, constraining lie range empirically allows serve
good scoring function.
event tracker ameliorates brittleness feed-forward pipeline allowing topdown information event influence tracking. Using HMMs event recognizers
accomplished selecting event model best fits event. involves running
event model independently data. context running particular event
model data, event model could influence tracking top-down fashion.
example, context evaluating well event model walk fits data,
tracker would biased produce tracks move normal walking pace. Stationary
tracks, move quickly, would depict target event would
filtered Equation 6 Equations 1, 4, 5, tracks comprised
high-scoring detections temporally coherent.
Equation 6 jointly optimizes single tracker single event model. such,
recognize events single participant, described intransitive
verbs. Events two participants, described transitive verbs,
encoded using methods Section 2.2, using Equation 3 instead Equation 2
forming cross product two trackers instead one.
!
!
!
!




X
X
X
X
t1
t1




(7)
g(bj t1 , bj )
f (bj ) +
g(bj t1 , bj ) +
f (bj ) +
max
2
2
1
1
j1 ,j2 ,k
2
1
t=2
t=1
t=2!
t=1
!


X
X
a(k t1 , k )
h(k , btj , btj ) +
+
t=1

1

2

t=2

generalized two participants L participants.
!#
" L
!


X X
X
t1


g(bj t1 , bj )
max
f (bj ) +
l
l
J,k
l
t=2
t=1
l=1
!
!


X
X
a(k t1 , k )
+
h(k , btj , . . . , btj ) +
t=1

L

1

(8)

t=2

above, J denotes track collection j1 , . . . , jL which, turn, comprises detection
indices jlt . Equations 7 8 also optimized Viterbi algorithm forming
cross-product lattice. Since width cross-product lattice O(J L K), applying
Viterbi algorithm cross-product lattice finds path optimizes Equation 8,
among exponentially many potential paths, time O(T (J L K)2 ). Note
exponential number L participants. practice, however, arity semantic
predicate underlying events limited, three case ditransitive verbs.
Let BJ denote detection-sequence collection b1j 1 , . . . , bTjT , . . . , b1j 1 , . . . , bTjT selected
1
L
1
L
video clip B track collection J. Equation 8 jointly optimizes measure
well video clip B depicts event model combination measures
well video clip B depicts track collection J well detectionsequence collection BJ selected video clip B track collection J depicts
617

fiYu, Siddharth, Barbu, & Siskind

p2

b1 b2

b1
p1

p1

b2
p2
b1

p2

b2 p1

Figure 8: Example showing necessity normalization detection scores across different object classes. (left) Image depicting two pairs detections person
backpack object classes. (right top) Distribution raw detection scores two object
classes. Indicated scores corresponding detections image f (b1 ) = 4,
f (b2 ) = 6, f (p2 ) = 11, f (p1 ) = 14. (right bottom) Distribution detection scores
two object classes cross-object-class normalization f (b1 ) = 5, f (b2 ) = 12,
f (p2 ) = 9, f (p1 ) = 14.

event model . Note Equation 8 involves summation multiple detection-score
components f , one L participants. fact raw detection scores
incomparable across object class means detection scores different participants
contribute different extents final score. Figure 8 shows example differences
variance detection scores person trash result better score
Equation 8 spurious set detections. values p1 p2 indicate detections
person object class values b1 b2 indicate detections backpack object
class. Let us assume pairs detections, (p1 , b1 ) (p2 , b2 ), match event
model, carry, equally well. case, raw detections scores would yield
(p1 , b1 ) best match f (p1 ) + f (b1 ) > f (p2 ) + f (b2 ). reason
employ normalization detection scores discussed Section 2.1.
results selection correct pair detections, (p2 , b2 ), since normalization,
f (p2 ) + f (b2 ) > f (p1 ) + f (b1 ).
Figure 9 illustrates power event tracker. objective track person.
However, due poor performance state-of-the-art person detector, produces
strong false-positive detections bench background. Even overgenerating
detections, shown Figure 9(a), selecting track optimizes Equation 1,
shown Figure 9(b), tracks bench background portion video clip,
618

fiGrounding Language Inference, Generation, Acquisition Video

(a)

(b)

(c)

Figure 9: Keyframes video clip demonstrates advantages event
tracker. (a) Overgenerated person detections. (b) Detections selected detection-based
tracking Equation 1. Note selects strong false-positive detection bench
background able rule detections exception single
large jump, rest track happens temporally coherent. (c) Detections selected
event tracker top-down information, form model transitive
verb carry, constraining detections fill role agent event, context
backpack, patient, carried person bench.
instead person. happens track largely temporally coherent within
segments, combination strong false-positive detections background,
overpowers adverse effect single large jump, thus yielding high score Equation 1.
However, top-down information form event model transitive verb carry,
linked two trackers, one agent one patient, selects track agent,
comprising true-positive person detections, accurately reflects role played
person event, shown Figure 9(c), backpack, patient, carried
person bench background.
2.4 Sentence Tracker
event tracker previous section, generally HMM-based event recognizers, model events varying numbers participants (one, two, L participants
619

fiYu, Siddharth, Barbu, & Siskind

event trackers Equations 6, 7, 8 one two participants HMM-based
event recognizers Equations 2 3). Nominally, think events
described verbs: one-participant events intransitive verbs, two-participant events
transitive verbs, three-participant events ditransitive verbs. Figures 25 28
Appendix B gives examples HMMs represent meanings verbs. However,
nothing framework formally restricts us so. meanings words
parts speech often also represented HMMs. example, meaning
noun describes object class represented single-state one-participant
HMM whose output model serves classifier object class. Figure 23 Appendix B gives examples HMMs represent meanings nouns. Similarly,
meaning adjective describes object characteristics represented singlestate one-participant HMM whose output model serves select detections exhibit
desired characteristics reflected adjective. example, meanings adjectives
like big tall could represented output models areas aspect ratios
participant detections. Likewise, meaning preposition describes spatial
relation two objects represented single-state two-participant HMM
whose output model serves select collection features encode relation.
example, meaning preposition left could represented output
model relative x-coordinates detections participants. Figure 24
Appendix B gives examples HMMs represent meanings spatial-relation prepositions. generally, static property either single participant, collection
participants, encoded single-state HMM.
Multiple-state HMMs encode dynamic properties either single participant
collection participants. reflect meanings adverbs prepositions
addition verbs. example, meaning adverb quickly describes
changing characteristics motion single participant could represented
three-state HMM describing transition motion, motion high velocity,
back motion. Figure 29 Appendix B gives examples HMMs represent
meanings adverbs. Similarly, meaning preposition towards describes
changing relative motion pair participants could represented threestate HMM describing transition agent distant goal, period
distance agent goal decreases goal stationary,
ending agent close goal. Figure 30 Appendix B gives examples
HMMs represent meanings motion prepositions.
thus see distinction different parts speech primarily syntactic,
semantic, i.e., word use reflected grammar, potential meaning.
may coarse-grained trends, canonical structure realizations
(CSRs) proposed Grimshaw (1979, 1981) Pinker (1984), nouns typically describe object class, adjectives typically describe object properties, verbs typically describe
event class, adverbs typically describe event properties, prepositions typically describe
spatial relations, universally case. intransitive verbs like sleep describe static object property, transitive verbs like hold describe static
spatial relation pairs objects, nouns like wedding describe event.
might seem like overkill represent static classifiers single-state HMMs,
several advantages adopting single uniform meaning representation form
620

fiGrounding Language Inference, Generation, Acquisition Video

HMMs. First, capacity multiple states affords ability encode resilience
temporal noise. Thus practice, even static properties might robustly encoded
multiple states. Second, adopting single uniform representation simplifies overall
framework associated algorithms.
event tracker previous section could influence detection-based tracking
top-down information event model. event model could represent meaning
individual word. could constrain single track single-participant words like
intransitive verbs (Equation 6), pair tracks two-participant words like transitive
verbs (Equation 7), even collection L tracks L-participant words (Equation 8).
possible take cross products multiple trackers single event
model, one extend framework take cross products multiple trackers
multiple event models, thereby constraining track collection jointly satisfy
collection event models words s1 , . . . , sW sentence s.
!#
" L
!


X X
X
t1


(9)
max
f (bj ) +
g(bj t1 , bj )
l
l
J,K
l
t=2
l=1
" W t=1
!
!#

X X
X

t1
+
hsw (kw
, btj , . . . , btj ) +
asw (kw
, kw )
w=1

L

1

t=1

t=2

above, K denotes state-sequence collection k1 , . . . , kW which, turn, comprises
. L distinct trackers distinct detection indices j select
state indices kw
l
optimal detection participant l frame t.
distinguish words lexicon occurrences sentences.
refer former lexical entries e latter words w. given lexical entry
may appear one word sentence. lexicon contains E event models
1 , . . . , E , one event model e lexical entry e. sentence formulated
sequence s1 , . . . , sW W lexical entries sw , one word w. Equation 9 W distinct
event models sw , one word w sentence s, taken event model
lexical entry sw word w. event model sw distinct numbers Ksw
states, state-transition functions asw , output models hsw . Note statetransition functions asw output models hsw vary word w, detection score f
temporal-coherence score g vary participant l.
, bt , . . . , bt ) word w
formulated Equation 9, output model hsw (kw

jL
j1t
depends detections frame selected tracks j1 , . . . , jL L participants.
practice, meaning individual word applies subset participants,
illustrated Figure 10. Here, sentence person left stool carried
traffic cone towards trash describes event four participants:
agent, referent, patient, goal. nouns person, stool, traffic cone trash
refer agent, referent, patient, goal respectively. verb carried describes
semantic relation agent patient. preposition left
describes semantic relation agent referent. preposition
towards describes semantic relation agent goal. employ
indicate participant fills argument event model
linking function w
word w. Let Bhs, t, w, Ji denote btj , . . . , btj , collection detections selected

w w

1
w

621

fiYu, Siddharth, Barbu, & Siskind

frame track collection J assigned Isw arguments event model
word w linking function . incorporate arity event model , along
number K states, state-transition function a, output model h.
allows reformulating Equation 9
!
!#
" L


X
X X
g(bt1
, bt )
f (btj ) +
(10)
max
jlt1 jl
l
J,K
t=2
t=1
l=1
"W
!
!#


X X
X

t1
+
hsw (kw , Bhs, t, w, Ji) +
asw (kw , kw )
w=1

t=1

t=2

refer Equation 10 sentence tracker. remainder paper, Isw 2.
Equation 10 also optimized Viterbi algorithm forming cross-product
lattice. Since width cross-product lattice O(J L K W ), K maximum Ks1 , . . . , KsW , applying Viterbi algorithm cross-product lattice finds
path optimizes Equation 10, among exponentially many potential paths, time
O(T (J L K W )2 ). Note exponential number L participants
sentence length W . practice, however, natural-language sentences bounded length
typically short. Moreover, quadratic time complexity mitigated somewhat
W

fact K W approximation
Ksw . practice, nouns, adjectives,
w=1

spatial-relation prepositions describe static properties tracks thus word models Ksw = 1. Even longer sentences comprised predominantly word
models contain relatively verbs, adverbs, motion prepositions.
Modeling meaning sentence collection words whose meanings
modeled HMMs defines factorial HMM sentence, overall Markov
process sentence factored independent component processes (Brand, Oliver,
& Pentland, 1997; Zhong & Ghosh, 2001) individual words. view, K denotes
state sequence combined factorial HMM kw denotes factor state
sequence word w. Figure 11 illustrates formation cross product two tracker
lattices (Equation 1) three word lattices (Equation 2), linked together appropriate
linking function implement sentence tracker (Equation 10) sentence
person carried backpack. Figure 12 illustrates resulting cross-product lattice
node lattice consists combination two detections, one tracker
lattice, three HMM states, one word lattice. state thus represented
node cross-product lattice factored collection states written
inside node separated commas.
Equation 10 constitutes : (B, s, ) 7 ( , J). scores video-sentence pair
measure well given video clip B depicts given sentence s, interpreted given
lexicon . Alternatively, score measures well given sentence s, interpreted
given lexicon , describes given video clip B. J 1 , . . . , J determined B,
W determined s, arities Isw , numbers Ksw states, state-transition
functions asw output models hsw taken words models sw ,
number L participants linking function computed sentence
linking process : 7 (L, ) described Section 3. result Equation 10 constitutes
video-sentence score . track collection yields score constitutes J.
622

fiGrounding Language Inference, Generation, Acquisition Video

person left stool carried trac cone towards trash can.


w
agent

patient

referent

goal

jlt
detection 0

detection 1

detection 2

detection 3

3
0

1
2

Figure 10: illustration linking function used sentence tracker.
word sentence one arguments. (When words two arguments,
first argument indicated solid line second dashed line.)
argument word filled participant event described sentence.
given participant fill arguments one words. participant tracked
tracker selects detections pool detections produced multiple object
arguments words w participants determined
detectors. upper mapping w
parsing sentence. lower mapping jlt participants l frames detections
determined automatically Equation 10. figure shows possible (but erroneous)
interpretation sentence lower mapping, indicated darker lines, is:
agent 7 detection 3, referent 7 detection 0, patient 7 detection 1, goal 7
detection 2.

623

fiYu, Siddharth, Barbu, & Siskind

t=1

t=2

t=3

t=1

t=2

t=3

j1 = 1

b11

b21

b31

...

bT1

j1 = 1

b11

b21

b31

...

bT1

j1 = 2

b12

b22

b32

...

bT2

j1 = 2

b12

b22

b32

...

bT2

j1 = 3

b13

b23

b33

...

bT3

j1 = 3

b13

b23

b33

...

bT3

..
.

..
.

..
.

..
.

..
.

..
.

b1J 1

b2J 2

b3J 3

b1J 1

b2J 2

b3J 3

...

bTJ

j1 = J

fa

t=T



..
.
...

bTJ

j1 = J

fp

ga

t=1

t=2

t=3

1

1

1

...

1

k wp = 2

2

2

2

...

2

k wp = 3

3

3

3

...

3

..
.

..
.

..
.

Kswp

Kswp

Kswp

kwp = Kswp

patient-tracker

t=1

t=2

t=3

k wc = 1

1

1

1

...

1

k wc = 2

2

2

2

...

2

k wc = 3

3

3

3

...

3

..
.

..
.

..
.

Kswc

Kswc

Kswc

t=T



..
.
...

Kswp

h wp wp

..
.

gp

agent-tracker

k wp = 1

t=T

kwc = Kswc

t=1

t=2

t=3

k wb = 1

1

1

1

...

1

k wb = 2

2

2

2

...

2

k wb = 3

3

3

3

...

3

..
.

..
.

..
.

Kswb

K swb

K wb

...

K wb

t=T



..
.
...

Kswc

k w b = K wb

..
.

h wb wb

hs wc wc

person

t=T

backpack

carried

Figure 11: Forming cross product two tracker lattices (Equation 1) three word
lattices (Equation 2) implement sentence tracker (Equation 10) sentence
person carried backpack. connections tracker lattices word
lattices denote linking function .

t=1

t=2

t=3

b11 ,b11
1,1,1

b21 ,b21
1,1,1

b31 ,b31
1,1,1

..
.

..
.

..
.

j1 =J ,j2 =J
kwp =1,kwc =1,kwb =1

b1 1 ,b1 1
J
J
1,1,1

b2 2 ,b2 2
J
J
1,1,1

b3 3 ,b3 3
J
J
1,1,1

...

bT ,bT
J
J
1,1,1

j1 =1,j2 =1
kwp =1,kwc =1,kwb =2

b11 ,b11
1,1,2

b21 ,b21
1,1,2

b31 ,b31
1,1,2

...


bT
1 ,b1
1,1,2

..
.

..
.

..
.

j1 =1,j2 =1
kwp =1,kwc =1,kwb =1

j1 =J ,j2 =J
kwp =Kswp ,kwc =Kswc ,kwb =Ksw

b1 1 ,b1 1
J
J
Kswp ,Kswc ,Ksw

b

b

ga , gp , aswp , aswc , aswb

b2 2 ,b2 2
J
J
Kswp ,Kswc ,Ksw

b

b3 3 ,b3 3
J
J
Kswp ,Kswc ,Ksw

t=T
...


bT
1 ,b1
1,1,1

..
.

..
.
...
b

bT ,bT
J
J
Kswp ,Kswc ,Ksw

b

fa , fp , hswp , hswc , hswb

Figure 12: actual cross-product lattice produced example Figure 11. Note
node lattice consists combination two detections, one
tracker lattice, three HMM states, one word lattice.

624

fiGrounding Language Inference, Generation, Acquisition Video

3. Linking Process
sentence tracker requires specification number L participants linking
indicates participant fills argument word w argument
function w
word sentence. Often, participant (i.e., tracker) fill multiple
arguments multiple words. sentence like
person right chair picked backpack
| {z } |
| {z } | {z }
| {z }
{z
}

(11)

11 = 1 21 = 1 22 = 2 31 = 2 41 = 1 42 = 3 51 = 3

(12)

1

2

3

4

5

3 participants requires linking function like

assigns argument person first argument right
picked first participant, argument chair second argument
right second participant, argument backpack second argument
picked third participant. number L participants sentence s,
corresponding linking function , produced linking process : 7 (L, ).
use particular linking process described details Appendix A. process makes use techniques mainstream linguistics, namely X-bar theory (Jackendoff,
1977) government relations (Chomsky, 1982; Aoun & Sportiche, 1983; Haegeman, 1992;
Chomsky, 2002). such, limited small hand-built grammar (Figure 11a)
small lexicon (Figure 11b). purposes, restrictive. state art
computer vision limits number distinct object classes reliably detected
number distinct action classes reliably detected. restricts
number nouns verbs supported method, ours, attempts ground language computer vision methods detect objects actions.
restricts class utterances constructed small set
nouns verbs. this, small hand-constructed grammar suffices. one could
conceivably use methods support larger grammars vocabularies process
larger space unrestricted text, would possible ground current
state-of-the art computer vision techniques. discuss detail Sections 6
7.
linking process employ uses well-known techniques mainstream linguistics. central contribution work. Rather, central contribution
sentence tracker (Section 2.4). sentence tracker requires linking process : 7 (L, ) maps sentence number L participants linking
function . need restricted particular grammar lexicon Figure 11. Indeed, employ one plethora well-known well-understood
techniques common computational linguistics community. need even
restricted particular grammar lexicon. possible construct linking
process standard mechanisms, dependency relations produced parsing
dependency grammar. example, Stanford Parser (Klein & Manning, 2003)
produces dependencies right sentence Equation 11, also
used determine requisite number participants construct requisite linking
function. output correctly identifies three participants, person-2, chair-8,
625

fiYu, Siddharth, Barbu, & Siskind

det(person-2, The-1)
nsubj(picked-9, person-2)
det(right-5, the-4)
prep_to(person-2, right-5)
det(chair-8, the-7)
prep_of(right-5, chair-8)
root(ROOT-0, picked-9)
prt(picked-9, up-10)
det(backpack-12, the-11)
dobj(picked-9, backpack-12)

backpack-12. Note transitive verb picked-9 distinguishes two arguments,
identifying person-2 first argument nsubj dependency backpack-12
second argument dobj dependency. Also note spatial relation
right-5 distinguishes two arguments, identifying person-2 first argument
prep dependency chair-8 second argument prep
dependency.

4. Language Acquisition Sentence Tracker
Children learn language exposure rich perceptual context. observe events
hearing descriptions events. correlating many events corresponding
descriptions, learn map words, phrases, sentences meaning representations
refer world. come know noun chair refers object class
typically back four legs. also come know verb approach
refers dynamic process one object moves towards another. learned
concepts purely symbolic; used decide presence absence
intended reference perceptual input. Thus concepts perceptually grounded.
children learn language, usually given information
words sentence correspond concepts see. example, child hears
dog chased cat seeing dog chase cat, prior knowledge
meaning word sentence, might entertain least two possible correspondences
mappings: (i) dog 7 dog cat 7 cat (ii) dog 7 cat cat 7 dog. first,
child might assume chased means ran second child might
assume means ran before. Thus child hears description context
observed event need disambiguate among several possible interpretations
meanings words description. Things get worse process exhibits
referential uncertainty (Siskind, 1996): multiple simultaneous descriptions context
multiple simultaneous events.
situation faced children motivates formulation shown Figure 13,
video clips represent children see textual sentences represent hear. Note
given video clip paired one sentence given sentence
paired one video clip. Siskind (1996, 2001) showed even referential
uncertainty noise, system based cross-situational learning (Smith, Smith, Blythe, &
Vogt, 2006; Smith, Smith, & Blythe, 2011) robustly acquire lexicon, mapping words
word-level meanings sentences paired sentence-level meanings. However,
symbolic representations word- sentence-level meanings
626

fiGrounding Language Inference, Generation, Acquisition Video

person picked traffic cone left stool.

person picked traffic cone.

person carried chair.

chair approached backpack.

chair approached traffic cone slowly.

person carried chair away backpack.

Figure 13: Video-sentence pairs language-acquisition problem. video clip
paired multiple sentences sentence paired multiple video clips.
perceptually grounded. ideal system would require detailed word-level labelings
acquire word meanings video rather could learn language largely unsupervised
fashion, child does, video paired sentences. algorithm presented
section resolve ambiguity inherent referential uncertainty yield
lexicon intended meaning word. algorithm solve problem
reminiscent faced children, make psychological neurophysiological
claims.
One view language-acquisition task constraint-satisfaction problem (CSP),
depicted Figure 14. treats words variables, initially unknown
meaning. video-sentence pair viewed constraint imposed words
sentence: words sentence mutually constrained requirement
collection word meanings allow sentence describe video clip. constraint
formulated using variant sentence tracker Section 2. Since
word may appear different sentences, sufficient number video-sentence pairs
form connected network. two types inference network. First, one
perform inference across different words sentence. Suppose know
meanings words sentence except one. case, meaning
unknown word inferred applying video-sentence constraint. example,
Figure 14, know meaning backpack person, meaning picked could
627

fiYu, Siddharth, Barbu, & Siskind

chair

picked
person picked chair.
(b)

person
chair approached backpack.

person picked backpack.
(a)
approached
backpack

Figure 14: Viewing language acquisition constraint-satisfaction problem (CSP)
solved propagating information word meanings around network. Word meanings
green used learn word meanings orange used learn
word meanings red. performs inference across different words
sentence, shown (a), word different sentences, shown (b).

inferred constraint (a), process occurred
person backpack. Second, one perform inference across word
different sentences. meaning given word shared exploited multiple
sentences inferring meanings words sentences. example,
learning meaning picked up, constraint (b), meaning chair also
inferred. Thus, information word meanings propagate network.
result, word meanings mutually constrained learned. Siskind (1996) refers
learning mechanism cross-situational learning. practice, process starts
information word meanings. formulation using EM (Dempster,
Laird, & Rubin, 1977) propagate partial information word meanings. Thus
starting initial guess meaning word iterating process,
converge intended lexicon.
discussed earlier, sentence tracker supports representing word meanings HMMs
FSMS, special case HMMs state-transition functions output mod628

fiGrounding Language Inference, Generation, Acquisition Video

els 0/1 (/0 log space). Section 5.2, formulate output models
manually-constructed FSMs regular expressions Boolean features computed
detections using predicates shown Table 6. procedure learning word
meanings employs HMMs state-transition functions output models
0/1. case, output models derived features shown Table 8.
use denote computation produces feature vectors detections N
denote length feature vectors. Word models extended incorporate N
.
employ discrete distributions output models h. Further, assume
distributions factorial features, i.e., distributions features
feature vector independent. end, quantize feature bins.
particular binning process described Section 5.5. means output models
take form
N
X
(k, b1 , . . . , bIe ) =
hne (k, ne (b1 , . . . , bIe ))
n=1



ne (b1 , . . . , bIe ) {ne,1 , . . . , ne,Zen }
Zen indicates number bins feature n lexical entry e ne,z indicates
quantized value bin z feature n lexical entry e.
learning procedure makes five assumptions.
1. training set contains samples, pairing short video clip Bm
sentence sm describes clip. procedure able determine
alignment multiple sentences longer video segments. Note
requirement clip depict sentence. objects may present
events may occur. fact, nothing precludes training set multiple
copies clip, paired different sentence describing different
aspect clip. Similarly, nothing precludes training set multiple copies
sentence, paired different clip depicts sentence.
Moreover, procedure potentially handle small amount noise, clip
paired incorrect sentence describe clip.
2. already (pre-trained) low-level object detectors capable detecting instances
target event participants individual frames video. allow detections unreliable; method handle moderate amount false positives
false negatives using techniques Section 2. need know
mapping object-detection classes nouns; procedure determines that.
words, detectors locate classify objects symbolic labels like
chair, labels distinct lexical entries like chair. procedure learns
mapping lexical entries object-class labels. mapping need
one-to-one noisy. Learning mapping, however, requires
object classes present times, would provide constraint
required learn mappinga lexical entry could correspond object class.
case, additionally need identify object classes
present video clip. made possible fact detection scores
rendered comparable, using normalization process described Section 2.1,
629

fiYu, Siddharth, Barbu, & Siskind

thus use normalized scores indicator object presence
video clip.
3. know part speech ce associated lexical entry e. particular
mapping lexical entry part speech used experiments Section 5.5
given Table 11(a).
4. word models lexical entries part speech
arity I, number K states, feature-vector length N ,
computation produces feature vectors, together associated
binning process quantizing features. values known learned.
particular values parameters used experiments Section 5.5
given Table 8.
5. know linking process grammar lexicon portion needed
determine number L participants linking function training
sentence. particular linking process used experiments Section 5.5
described Section 3 using grammar lexicon portion Table 11.
know track collection J chosen training sample. determined
automatically methods Section 2.
grammar, portions lexicon , namely components I, K, N , ,
linking process prespecified learned. state-transition functions
output models hn learned. One imagine learning grammar,
nonlearned portions lexicon, perhaps even linking process ,
done Kwiatkowski, Goldwater, Zettlemoyer, Steedman (2012). leave
future work.
4.1 General Approach
given grammar, portions lexicon , namely components I, K, N , ,
linking process . lexicon contains E word models e lexical entries e.
given training set samples, video clip Bm paired sentence sm . Let B
denote B1 , . . . , BM denote s1 , . . . , sM . use grammar, nonlearned portions
lexicon , linking process determine number L participants
linking function training sentence. state-transition functions ae
output models hne word models e lexicon , could instantiate
sentence tracker Equation 10 training sample compute video-sentence
score sample. side effect would compute track collection J
yielded video-sentence score. Moreover, could compute aggregate score
entire training set summing per-sample scores. However, dont know
state-transition functions ae output models hne . constitute unknown
meanings words training set wish learn. jointly learn ae
hne lexical entries e searching maximize aggregate score.
4.2 Learning Procedure
perform search Baum-Welch. Equation 10 constitutes score potentially could maximized, easier adapt scoring function like
likelihood calculation, Equation 10, like MAP estimate, EM
630

fiGrounding Language Inference, Generation, Acquisition Video

framework. P
Thus convert Equation 10 log space linear space replace
max
redefine scoring function follows:
X
J,K

"

L




f (btj )
l

t=1



"l=1
W


w=1

!



t=2

, bt )
g(bt1
jlt1 jl


hsw (kw
, Bhs, t, w, Ji)

t=1

!

!#

(13)




t1
, kw )
asw (kw

t=2

!#

f , g, h, linear space. Recall Equation 6 jointly maximizes sum
measure well video clip B depicts track j measure well detection
sequence Bj selected video clip B track j depicts event model . Similarly,
Equation 10 jointly maximizes sum measure well video clip B depicts
track collection J measure well detection-sequence collection BJ selected
video clip B track collection J depicts given sentence s, interpreted
given lexicon . One maximize first component latter sum.
!#
!
" L


X
X X
t1


(14)
max
g(bj t1 , bj )
f (bj ) +
J

l=1

t=1

l

t=2

l

l

variant Equation 1 track collection. One
P similarly convert Equation 14
log space linear space replace max
yield:
X
J

"

L



l=1

t=1

f (btj )
l

!



t=2

, bt )
g(bt1
jlt1 jl

!#

(15)

suitable normalization constant factor, Equation 15 used obtain
probability particular track collection J relative distribution possible
track collections probability given track collection proportional
summand. Let us denote probability given track collection J P (J|B).
given track collection J, one similarly maximize measure well
detection-sequence collection BJ selected video clip B track collection J
depicts sentence s, interpreted given lexicon .
"W
!
!#


X X
X

t1
max
(16)
hsw (kw , Bhs, t, w, Ji) +
asw (kw , kw )
K

w=1

t=1

t=2

variant Equation 2 factorial HMM multiple words. One
P similarly
convert Equation 16 log space linear space replace max
yield:
X
K

"

W


w=1





, Bhs, t, w, Ji)
hsw (kw

t=1

!



t=2

t1
, kw )
asw (kw

!#

(17)

summand Equation 17 joint probability state sequence K BJ depicting
sentence s, interpreted given lexicon : P (K, BJ |s, ) = P (BJ |K, s, )P (K|s, ).
631

fiYu, Siddharth, Barbu, & Siskind

Equation 17 (marginal) probability BJ depicting sentence s, interpreted
given lexicon : P (BJ |s, ). divide Equation 13 Equation 15 obtain:
L(B; s, ) =

X

P (J|B)P (BJ |s, )

J

expected probability BJ depicting sentence s, interpreted given
lexicon , track collection distribution underlying P (J|B). Equations 13 15
computed efficiently forward algorithm (Baum & Petrie, 1966).
allows us take L(B; s, ) sample score adopt
L(B; S, ) =




L(Bm ; sm , )

m=1

training-set score. seek h maximize L(B; S, ). Note
sample training-set scores [0, 1].
find local maximum objective function using techniques used
Baum-Welch. reestimation formulas derived auxiliary functions
analogous used HMMs (Bilmes, 1998). Let us first define J = J1 , . . . , JM
K = K1 , . . . , KM track collections state-sequence collections entire training
set. let us define L(B, J , K; S, ) product summand Equation 13
training set divided product Equation 15 training set. Thus have:
X
L(B; S, ) =
L(B, J , K; S, )
J ,K

adopt following auxiliary function:
X
F (, ) =
L(B, J , K; S, ) log L(B, J , K; S, )
J ,K

current lexicon potential new lexicon. One show
F (, ) F ( , ) implies L(B; S, ) L(B; S, ).

X L(B, J , K; S, )
L(B, J , K; S, )
F (, ) F ( , ) = L(B; S, )
log
L(B; S, )
L(B, J , K; S, )
J ,K

X
L(B, J , K; S, )
P (J , K|B, S, ) log

L(B, J , K; S, )
J ,K

X
L(B, J , K; S, )
P (J , K|B, S, )
log
L(B, J , K; S, )
J ,K

= log

X L(B, J , K; S, )

J ,K

= log

L(B; S, )

L(B; S, )
L(B; S, )

632

fiGrounding Language Inference, Generation, Acquisition Video

second step holds training-set score L(B; S, ) nonnegative.
third step holds due Jensens (1906) inequality. Thus given current lexicon ,
find new lexicon F (, ) F ( , ), one iterate process, increasing
training-set score local maximum. done maximizing F (, )
respect . Since L(B, J , K; S, ) proportional product summands
Equation 13 training set, product two terms, latter
depends , following holds:

X L(B, J , K; S, )

F (, )

J ,K

L(B; S, )

X L(B, J , K; S, )



J ,K

L(B; S, )

log L(B, J , K; S, )
Wm
X
X

m=1 w=1





Tm
X



, Bm hsm , t, w, Jm i)
log hsm,w (km,w
|
{z
}
t=1
h


Tm
X

t1
)
, km,w
log asm,w (km,w
+
{z
}
|
t=2






Tm number frames video clip Bm training sample , Wm
number words sentence sm training sample m, sm,w lexical entry

stateword w sentence sm training sample m, km,w
state kw
sequence collection Km training sample m. above, Bm hsm , t, w, Jm extended
, collection detections selected frame video
denote btj , . . . , btj
1
m,w

Ism,w
m,w

clip Bm track collection Jm assigned Ism,w arguments word model

word w sentence sm linking function m,w
produced sm determines
participant argument word w sentence sm . Thus F (, ) comprises two terms,
one which, H, weighted sum terms h which, A, weighted
sum terms a. One maximize F (, ) maximizing H independently.
lead reestimation procedures output models h state-transition functions a.
First consider A. Rewrite term explicitly sum lexical entries e pairs
states k k.

A=

=

=

X

X
X

t1 = k , k

L(B, km,w
m,w = k; S, )
log ae (k , k)
L(B; S, )
t1 = k , k


L(Bm , km,w
m,w = k; sm , )L(Bm 6=m ; Sm 6=m , )
log ae (k , k)
L(Bm ; sm , )L(Bm 6=m ; Sm 6=m , )
t1 = k , k

L(Bm , km,w
m,w = k; sm , )
log ae (k , k)
L(Bm ; sm , )

633

(18)

fiYu, Siddharth, Barbu, & Siskind



X

denotes

Tm
Wm X
Ke X
Ke X
X
E X
X



e=1 k =1 k=1 m=1 w=1 t=2
sm,w =e

t1 = k , k

L(B, km,w
m,w = k; S, ) =

X X

L(B, J , K; S, )

X X

L(Bm , Jm , Km ; sm , )

J

t1 = k , k

L(Bm , km,w
m,w = k; sm , ) =

Jm

L(Bm 6=m ; Sm 6=m

, )

=

K
t1
km,w
=k

km,w =k

Km
t1
km,w
=k

km,w =k




L(Bm ; sm , )

=1
6=m

second step Equation 18 holds assumption training samples
i.i.d. Taking derivative respect ae (k , k), get reestimation
formula state-transition function:
ae (k , k) := e (k )

Wm X
Tm
X
t1 = k , k

X
L(Bm , km,w
m,w = k; sm , )
L(Bm ; sm , )
m=1 w=1 t=2 |
{z
}
sm,w =e

(m,w,k ,k,t)

coefficient e (k ) chosen normalize distribution sums one.
reestimation formula output model derived similarly H.
make use fact output model factorial model factors discrete
distributions. linear space:

(k, b1 , . . . , bIe ) =

Ne


hne (k, ne (b1 , . . . , bIe ))

n=1

Again, rewrite H explicitly sum lexical entries e, states k, features n, bins z.

H=
=
=

X

X

X


L(B, km,w
= k, ne (Bm hsm , t, w, Jm i) = ne,z ; S, )
log hne (k, ne,z )
L(B; S, )

L(Bm , km,w
= k, ne (Bm hsm , t, w, Jm i) = ne,z ; sm , )L(Bm 6=m ; Sm 6=m , )
log hne (k, ne,z )
L(Bm ; sm , )L(Bm 6=m ; Sm 6=m , )

L(Bm , km,w
= k, ne (Bm hsm , t, w, Jm i) = ne,z ; sm , )
log hne (k, ne,z )
L(Bm ; sm , )

634

fiGrounding Language Inference, Generation, Acquisition Video



X

n

denotes

Z e Wm Tm
Ke X
Ne X
E X
X XX
X



e=1 k=1 n=1 z=1 m=1 w=1 t=1
sm,w =e


) =
L(B, km,w
= k, ne (Bm hsm , t, w, JmX
i) = ne,z ; S, X

L(B, J , K; S, )

K
J

n
n
e (Bm hsm ,t,w,Jm i)=e,z km,w =k


n
L(Bm , km,w
= k, ne (Bm hsm , t, w, JX
, ) =
i) = e,z ; smX
L(Bm , Jm , Km ; sm , )
Km
Jm
n

n
e (Bm hsm ,t,w,Jm i)=e,z km,w =k

Taking derivative H respect hne (k, ne,z ), get reestimation formula
output model:
hne (k, ) := en (k)

Tm
Wm X
X

X
L(Bm , km,w
= k, ne (Bm hsm , t, w, Jm i) = ; sm , )
L(Bm ; sm , )
m=1 w=1 t=1 |
{z
}
sm,w =e

(m,w,n,k,,t)

coefficient en (k) chosen normalize distribution sums one.
reestimation formulas involve occurrence counting. Since use factorial HMMs
involve cross-product lattice use scoring function derived Equation 13
incorporates tracking (Equation 1) word models (Equation 2), need count occurrences whole cross-product lattice. example cross-product occurrence
counting, counting transitions state k k word w frame t1
sample m, i.e., (m, w, k , k, t), need count possible paths adjacent
t1
t1
t1
t1

, . . . , jt

factorial states, i.e., jm,1
, . . . , jm,L
, km,1
, . . . , km,W
jm,1
m,L , km,1 , . . . , km,W
t1 = k k
km,w
m,w = k. Similarly, counting frequency
state k observing value feature n frame sample word w,
i.e., (m, w, n, k, , t), need count possible paths factorial state
, . . . , jt



n
jm,1
m,L , km,1 , . . . , km,W km,w = k e (Bm hsm , t, w, Jm i) = .
reestimation one word model depend previous estimate word
models. dependence happens linking function assign participant arguments different words sentence lexical entry appear
different training sentences. precisely dependence leads cross-situational
learning: former performs inference across different words sentence
latter performs inference across word different sentences.

5. Experiments
sentence tracker implements function : (B, s, ) 7 ( , J) takes video clip B
input, along sentence lexicon , produces, output, video-sentence
score , together track collection J depicts sentence interpreted
lexicon . ability produce score track collection allows sentence
tracker used variety ways, among them:
635

fiYu, Siddharth, Barbu, & Siskind

language inference Using track collection produces, take sentence
input focus attention event described sentence. allows
processing video clip depicts many participants, various subsets
engaged different events, track particular participants engaged
particular event specified sentence.
language generation Using score produces, generate sentential descriptions video clips efficiently searching space possible sentences
find one best describes given clip.
language acquisition Using score produces, learn word meanings
training set video clips paired sentences describe clips, searching
space potential word meanings find collectively allows sentences
best describe associated clips.
evaluate first use Section 5.3, second use Section 5.4, third use
Section 5.5.
5.1 Corpora
conduct evaluation, filmed two different corpora, containing 94 video clips.
One corpus used experiments Sections 5.3 5.4 used
experiments Section 5.5. corpora filmed 640480 resolution 30 fps.
contained clips varied length 3 5 seconds. filmed
variety outdoor environments, first varying three different environments
second varying four. camera moved filming clip
varying background precluded unanticipated confounds.
video clips filmed variety actors objects. clips first
corpus contain one two people collection three actors clips
second corpus contain single person collection four actors. first corpus
filmed three objects, backpack, chair, trash can,
present field view clips. second corpus filmed five objects,
backpack, chair, traffic cone, trash can, stool, either two three present
field view given clip. whole dataset counterbalanced avoid
artifactual correlation. object class combination object classes appears clips
nearly equal frequency.
four different environments second corpus used construct three different cross-validation folds. 29 video clips filmed one environment always contain
exactly two objects 23, 22, 20 clips filmed three environments respectively always contain exactly three objects. test set given fold
comprised clips filmed one latter three environments. Thus test sets
three folds contained 23, 22, 20 clips respectively. training set given
fold comprised clips except test set fold. Thus training sets
three folds contained 71, 72, 74 clips respectively.
video clips depict multiple simultaneous events. depiction, clip clip,
varied scene layout actor(s) performing event. clips first corpus
depicted one 21 sentences Table 1. clips second corpus
depicted one 187 sentences Tables 2 3. sentences
636

fiGrounding Language Inference, Generation, Acquisition Video

1 a.
b.
2 a.
b.
3 a.
b.
4 a.
b.
5 a.
b.
6 a.
b.
7 a.
b.
8 a.
b.
9 a.
b.
1 0.
1 1.
1 2.























backpack approached trash can.
chair approached trash can.
red object approached trash can.
blue object approached trash can.
person left trash put object.
person right trash put object.
person put trash can.
person put backpack.
person carried red object.
person carried blue object.
person picked object left trash can.
person picked object right trash can.
person picked object.
person put object.
person picked object quickly.
person picked object slowly.
person carried object towards trash can.
person carried object away trash can.
backpack approached chair.
red object approached chair.
person put chair.

Table 1: selection sentences drawn grammar Table 11(a) based
collected multiple video clips first corpus. Note sentence pairs 1 9
constitute minimal pairs, single constituent varies two lexical entries
pair. varying constituent ranges parts speech sentential positions.
constrained conform grammar Table 11(a). 187 sentences
second corpus divided two groups, one consisting 175 sentences used
exclusively training one consisting 12 sentences used exclusively
test. delineation indicated horizontal line Table 3.
corpora carefully constructed number ways. First, many video clips
depict one sentence. particular, many clips depict simultaneous distinct events.
Second, sentence describes multiple clips. Third, first corpus constructed
minimal pairs: clips described pair sentences differ exactly one lexical item.
minimal pairs help evaluate language inference indicated b
variants sentences 19 Table 1. varying lexical item carefully chosen span
parts speech sentential positions: sentence 1 varies subject noun, sentence 2
varies subject adjective, sentence 3 varies subject preposition, sentence 4 varies object noun,
sentence 5 varies object adjective, sentence 6 varies object preposition, sentence 7 varies
verb, sentence 8 varies adverb, sentence 9 varies motion preposition. Fourth, clip
second corpus contains subset objects used corpus. Without
asymmetry would difficult (but impossible) determine correspondence
637

fiYu, Siddharth, Barbu, & Siskind






























































chair approached stool.
chair right backpack approached stool.
chair left stool approached stool.
person picked stool.
person picked stool left backpack.
person carried trash can.
person carried trash left backpack.
person put trash can.
person put trash quickly.
person put trash left stool.
person left backpack put trash can.
person picked chair.
person picked chair quickly.
person picked chair left traffic cone.
person picked chair left backpack.
person put chair.
person put chair quickly.
person left traffic cone put chair.
person carried traffic cone.
person left backpack carried traffic cone.
person carried traffic cone away trash can.
backpack approached traffic cone.
backpack right chair approached traffic cone.
backpack left traffic cone approached traffic cone.
person put traffic cone.
person put traffic cone left stool.
person left chair put traffic cone.
person carried backpack.
person left chair carried backpack.
person carried backpack away stool.
person put stool left trash can.
person approached trash can.
stool approached trash can.
person carried stool.
person carried stool towards trash can.
stool approached trash left traffic cone.
backpack left traffic cone approached trash can.
backpack right trash approached trash can.
traffic cone approached stool left trash can.
trash approached chair.
trash left chair approached chair.
trash approached chair left backpack.
person approached chair.
person picked trash left stool.
person approached traffic cone.
chair approached traffic cone.
person left backpack approached traffic cone.
person carried chair towards traffic cone.
person put chair right backpack.
person right traffic cone put chair.
person right trash put traffic cone.
person left backpack put traffic cone.
person put traffic cone slowly.
person picked chair right backpack.
person right trash picked chair.
stool approached traffic cone right chair.
stool approached traffic cone left person.
person picked traffic cone quickly.
person picked traffic cone left stool.
person left chair picked traffic cone.






























































person picked backpack.
person left chair picked backpack.
person put backpack.
person put backpack slowly.
person right chair put backpack.
person put backpack right trash can.
traffic cone approached stool.
traffic cone left trash approached stool.
traffic cone right stool approached stool.
backpack approached trash can.
backpack approached trash right stool.
backpack right stool approached trash can.
person carried chair.
person left stool carried chair.
person carried chair left traffic cone.
person picked trash can.
person picked trash quickly.
person picked trash right stool.
person picked traffic cone.
person picked traffic cone slowly.
person left stool picked traffic cone.
person picked traffic cone right trash can.
stool approached traffic cone.
stool left traffic cone approached traffic cone.
stool right chair approached traffic cone.
chair approached trash can.
chair left traffic cone approached trash can.
chair left trash approached trash can.
person put stool.
person left traffic cone put stool.
traffic cone approached chair left stool.
traffic cone approached chair.
person carried traffic cone towards chair.
person left stool carried traffic cone.
person carried traffic cone away chair.
person left stool put backpack.
person put backpack right chair.
person picked stool slowly.
person right trash put stool.
traffic cone approached trash can.
traffic cone right stool approached trash can.
chair approached stool left traffic cone.
chair right stool approached stool.
person left stool put trash can.
person put trash left traffic cone.
person approached stool.
backpack approached stool.
person carried backpack towards stool.
backpack approached chair.
backpack right chair approached chair.
backpack right traffic cone approached chair.
person carried stool away traffic cone.
person left traffic cone picked backpack.
stool approached backpack.
stool approached backpack right trash can.
stool left backpack approached backpack.
person left chair approached stool.
person carried stool towards chair.
person left chair put stool.
person put stool slowly.

Table 2: selection sentences (first part) drawn grammar Table 11(a)
used annotate clips second corpus.

638

fiGrounding Language Inference, Generation, Acquisition Video





























person right trash approached chair.
person right trash carried chair.
person right trash put chair.
person put chair slowly.
person right trash approached stool.
person picked stool right trash can.
person put stool right trash can.
person left stool approached chair.
person picked chair left stool.
person carried chair towards stool.
person left stool put chair.
person right chair approached trash can.
person picked trash right chair.
person carried trash away chair.
person put trash right chair.
person picked stool quickly.
person put stool quickly.
person approached chair left stool.
person put chair left stool.
trash approached traffic cone.
trash right backpack approached traffic cone.
trash approached traffic cone right backpack.
person right chair put trash can.
person carried chair towards backpack.
chair approached backpack.
chair approached backpack left stool.
person carried trash towards traffic cone.








person
person
person
person
person
person






























picked stool right traffic cone.
left stool picked trash can.
put stool left chair.
left trash carried stool.
put backpack quickly.
left backpack put chair.

person right backpack picked stool.
person right backpack picked traffic cone.
person left trash picked traffic cone.
trash approached stool.
trash left stool approached stool.
trash right chair approached stool.
person picked trash left chair.
person carried backpack away chair.
person left traffic cone carried backpack.
person carried stool away chair.
person right chair picked backpack.
person left trash picked backpack.
person picked backpack quickly.
traffic cone approached backpack.
traffic cone left backpack approached backpack.
traffic cone approached backpack left stool.
person left traffic cone picked chair.
person left trash put traffic cone.
person put traffic cone right stool.
person carried traffic cone towards trash can.
person carried traffic cone away stool.
stool approached chair.
stool approached chair right traffic cone.
stool right traffic cone approached chair.
person left traffic cone put backpack.
person right trash put backpack.
chair left backpack approached backpack.
chair approached backpack left trash can.








trash left backpack approached chair.
person carried trash towards chair.
person picked chair slowly.
person picked stool left chair.
person picked backpack right trash can.
person carried trash away backpack.

Table 3: selection sentences (second part) drawn grammar Table 11(a).
sentences horizontal line used annotate clips second corpus
used test.
nouns object classes. Note, however, since training clips contain
one object, task learning noun meanings still challenging. filmed
corpora unaware existing corpora exhibit properties.2
annotated 94 video clips corpus human judgments.
first corpus annotated 21 sentences Table 1, indicating whether
given clip depicted given sentence. Table 4 provides statistics annotation.
resulting set 94 21 = 1974 judgments associated statistics used
compare contrast machine-generated results human judgments
analyses Sections 5.3 5.4. clip second corpus used either training
test, depending cross-validation fold, described earlier. included
training set, paired 1 5 sentences selected 175
2. video clips, sentential annotation described below, code needed replicate experiments section available http://upplysingaoflun.ecn.purdue.edu/~qobi/cccp
/grounding-language-in-video.html.

639

fiYu, Siddharth, Barbu, & Siskind

#Clips depict given sentence
#Sentences describe given clip





12.33
2.76

6.48
1.22

Table 4: Annotation statistics first corpus.


#Clips depict given sentence
#Sentences describe given clip

2.00
0.37

0.58
0.61

Table 5: Annotation statistics second corpus.
training sentences Tables 2 3 deemed describe associated training
clip human judge. average, training clip paired 2.94 sentences.
Collectively, corpus contains 276 video-sentence pairs used training. three
training folds contained 213, 208, 204 video-sentence pairs respectively. given
clip included test set, paired 12 test sentences Table 3. Thus
94 29 = 65 potential test clips second corpus annotated
12 test sentences Table 3, indicating whether given clip depicted given
sentence. Table 5 provides statistics annotation. resulting set 65 12 = 780
judgments associated statistics used compare contrast machinegenerated results human judgments analyses Section 5.5.
experiments use off-the-shelf object detector (Felzenszwalb et al., 2010a,
2010b) outputs detections form scored axis-aligned rectangles. particular, used implementation described Song, Zickler, Althoff, Girshick, Fritz, Geyer,
Felzenszwalb, Darrell (2012). Using off-the-shelf software, trained six object detectors, one six object classes corpora: person, backpack, chair, traffic
cone, trash can, stool. compensate false negatives, described Section 2.1,
lowered acceptance threshold models produced automatic training.
per-part thresholds uniformly reduced 1.2, model thresholds uniformly
reduced 2.0, non-maxima suppression set 0.6 first corpus 0.55
second. applied person, backpack, chair, trash detectors uniformly
frames video clips first corpus six detectors frames
clips second corpus. first corpus, selected five highest-scoring detections produced object detector frame pooled results yielding
twenty detections per frame. second corpus, selected two highest-scoring
detections produced object detector frame pooled results yielding
twelve detections per frame. larger pool detections per frame better
compensate false negatives object detection potentially yield smoother tracks,
increases size lattice concomitant running time lead
appreciably better performance corpora.
5.2 Manually-Constructed Lexicons
experiments Sections 5.3 5.4 use manually-constructed FSMs represent word
meanings evaluating language inference language generation. hand-written
representations word meaning clearly encode pretheoretic human intuition make
640

fiGrounding Language Inference, Generation, Acquisition Video

intuition perspicuous. experiments, formulate word models lexical
entries Table 11(a) appear sentences Table 1. experiments Section 5.5
learn word models represented HMMs. evaluated learned word models, part,
comparison manually-constructed HMMs. manually-constructed HMMs
discussed Section 5.5.
formulate FSMs regular expressions predicates computed detections.
particular set regular expressions associated predicates used
experiments Sections 5.3 5.4 given Table 6. predicates formulated
around number primitive functions. function avgFlow(b) computes vector
represents average optical flow inside detection b. function model(b) returns
object class b. function x(b) returns x-coordinate center b. function
hue(b) returns average hue pixels inside b. function angleSep determines
angular distance two angular arguments. function fwdProj(b) displaces b
average optical flow inside b. function 6 determines angular component given
vector. function computes normal unit vector given vector. argument v
noJitter denotes specified direction represented 2D unit vector direction.
Predicates take single detection b sole argument serve 0/1 output
models h(k, b) (/0 log space) single-participant word models. Predicates
take pair detections b1 b2 sole arguments serve 0/1 output models
h(k, b1 , b2 ) (/0 log space) two-participant word models. Regular expressions
formulated around predicates atoms. given regular expression must formed solely
output models arity denotes word model 0/1 state-transition
function (/0 log space) output models associated appropriate
states.
5.3 Experiment 1: Language Inference
Tracking traditionally performed using cues motion, object detection, and/or manual
initialization object interest (Yilmaz, Javed, & Shah, 2006). However,
case cluttered scene involving multiple events occurring simultaneously,
many moving objects, many instances object class, perhaps even multiple
simultaneously occurring instances event class. illustrate one
use sentential description guide tracking objects based ones participate
target event.
sentence tracker focus attention objects participate
event specified sentential description. description differentiate
different simultaneous events taking place many moving objects scene using
descriptions constructed variety parts speech. Using nouns specify object
class, one could differentiate
person picked backpack
person picked chair.
Using adjectives specify object properties, one could differentiate
person picked red object
person picked blue object.
641

fiYu, Siddharth, Barbu, & Siskind

Constants




xBoundary = 300px



nextTo = 50px



static = 6px



slow = 30px



angle = 30

closing = 10px



jump = 30px



quick = 80px



hue = 30

Simple Predicates




noJitter(b, v) = kavgFlow(b) vk jump

alike(b1 , b2 ) = model(b1 ) = model(b2 )





close(b1 , b2 ) = |x(b1 ) x(b2 )| < xBoundary

far(b1 , b2 ) = |x(b1 ) x(b2 )| xBoundary





left(b1 , b2 ) = 0 < x(b2 ) x(b1 ) nextTo

right(b1 , b2 ) = 0 < x(b1 ) x(b2 ) nextTo



hasColor(b, hue) = angleSep(hue(b), hue) hue



stationary(b) = kavgFlow(b)k static





quick(b) = kavgFlow(b)k quick

slow(b) = kavgFlow(b)k slow





person(b) = model(b) = person

backpack(b) = model(b) = backpack





chair(b) = model(b) = chair

trashcan(b) = model(b) = trashcan





blue(b) = hasColor(b, 225 )

red(b) = hasColor(b, 0 )
Complex Predicates



stationaryClose(b1 , b2 ) = stationary(b1 ) stationary(b2 ) alike(b1 , b2 ) close(b1 , b2 )


stationaryFar(b1 , b2 ) = stationary(b1 ) stationary(b2 ) alike(b1 , b2 ) far(b1 , b2 )


closer(b1 , b2 ) = |x(b1 ) x(b2 )| > |x(fwdProj(b1 )) x(b2 )| + closing


farther(b1 , b2 ) = |x(b1 ) x(b2 )| < |x(fwdProj(b1 )) x(b2 )| + closing


moveCloser(b1 , b2 ) = noJitter(b1 , (0, 1)) noJitter(b2 , (0, 1)) closer(b1 , b2 )


moveFarther(b1 , b2 ) = noJitter(b1 , (0, 1)) noJitter(b2 , (0, 1)) farther(b1 , b2 )


inDirection(b, v) = noJitter(b, (v)) stationary(b) angleSep(6 avgFlow(b), 6 v) < angle


approaching(b1 , b2 ) = alike(b1 , b2 ) stationary(b2 ) moveCloser(b1 , b2 )


departing(b1 , b2 ) = alike(b1 , b2 ) stationary(b2 ) moveFarther(b1 , b2 )


carry(b1 , b2 , v) = person(b1 ) alike(b1 , b2 ) inDirection(b1 , v) inDirection(b2 , v)


carrying(b1 , b2 ) = carry(b1 , b2 , (0, 1)) carry(b1 , b2 , (0, 1))


pickingUp(b1 , b2 ) = person(b1 ) alike(b1 , b2 ) stationary(b1 ) inDirection(b2 , (0, 1))


puttingDown(b1 , b2 ) = person(b1 ) alike(b1 , b2 ) stationary(b1 ) inDirection(b2 , (0, 1))
Regular Expressions


person = person+


trash = trashcan+


blue = blue+



backpack = backpack+


object = (backpack | chair | trashcan)+


quickly = true+ quick[3,] true+
red = red+




right = right+

left = left+



chair = chair+



slowly = true+ slow[3,] true+



approached = stationaryFar+ approaching[3,] stationaryClose+


carried = stationaryClose+ carrying[3,] stationaryClose+


picked = stationaryClose+ pickingUp[3,] stationaryClose+


put = stationaryClose+ puttingDown[3,] stationaryClose+


towards = stationaryFar+ approaching[3,] stationaryClose+


away = stationaryClose+ departing[3,] stationaryFar+

Table 6: FSMs representing meanings lexical entries Table 11(a)
appear sentences Table 1 used experiments Sections 5.3 5.4.

642

fiGrounding Language Inference, Generation, Acquisition Video

Using verbs specify events, one could differentiate
person picked red object
person put red object.
Using adverbs specify motion properties, one could differentiate
person quickly picked red object
person slowly picked red object.
Using prepositions specify (changing) spatial relations objects, one could differentiate
person right chair picked object
person left chair picked object.
Furthermore, sentential description even differentiate objects track
based role play event: agent, patient, source, goal, referent.
example, sentence person picked backpack left chair differs
person picked chair left backpack roles backpack
chair exchanged. Although objects involved described
events, roles events differ, distinguished tracker. Figure 15
demonstrates ability: different tracks produced video depicts
multiple simultaneous events focused different sentences. figure, well
Figure 16 Figures 21 22 Appendix B, boxes around participants
color coded indicate semantic role: agent red, patient blue, source violet, goal
turquoise, referent green. particularly illustrates system understands
image regions correspond participants particular mapping
argument positions predicates denote meanings lexical items sentential
description. illustrates deep semantic understanding.
Figure 15 evaluates ability sentential position. Figure 21 Appendix B
evaluates ability 9 minimal pairs, indicated b variants
sentences 19 Table 1, collectively applied 25 suitable video clips first corpus.
discard two clips original set 9 3 = 27 video clips due fact
involve adjective (grey), corresponding chair, cannot reliably extracted
video. 18 25, sentences minimal pair yielded track
collections deemed correct depictions. determine error subjective human
judgment whether track collection system produces matches desired
description. errors encountered task fall one two categories. One
category deals use color adjective along generic word object
presence entity video intended object incidentally
similar color. sole error category involves tracker selecting detections
persons red shirt instead red backpack, one three instances minimal
pair 2 Table 1: red object approached chair blue object approached
chair. correct result obtained instance minimal pair
associated different video clips. category largely due deficiencies
detectors, particularly trash can. least four instances, paucity
643

fiYu, Siddharth, Barbu, & Siskind

Differentiate

verb
person picked object.

person put object.

backpack approached trash can.

chair approached trash can.

red object approached chair.

blue object approached chair.

person left trash put object.

person right trash put object.

person put trash can.

person put backpack.

person carried red object.

person carried blue object.

person picked object left trash can.

person picked object right trash can.

person carried object towards trash can.

person carried object away trash can.

subject
noun

adjective
subject
NP

preposition
subject
NP

object
noun

adjective
object
NP

preposition
object
NP

preposition

adjunct

Figure 15: Language inference: two different track collections video clip produced guidance two different sentences. clip processed minimal pair,
sentence varies single lexical item highlighted red vs. green. varying lexical
item varies among sentential positions across eight examples. Results
video clips processed minimal pairs sentences 19 Table 1 included
Figure 21 Appendix B. figure, well Figures 16, 21, 22, indicate
thematic role participants color bounding box: red box denotes
agent, blue box denotes patient, violet box denotes source, turquoise
box denotes goal, green box denotes referent. roles determined
automatically using techniques Appendix A.

644

fiGrounding Language Inference, Generation, Acquisition Video

Contraction Threshold

Accuracy

0.95
67.02%
0.90
71.27%
0.85
64.89%
Table 7: Accuracy function contraction threshold.
detections trash detector results either poor tracks complete failure
satisfy FSMs corresponding word models. exacerbated
case adverbs. Since adverbs modify verbs, verbs vary manner
execution, tight bounds would constitute quickly slowly difficult obtain.
bounds able impose sufficiently noisy sometimes distinction
action happening quickly slowly lost. Two errors occur here, namely
two instances minimal pair 8 Table 1: person picked object quickly
person picked object slowly. correct result obtained remaining
instance minimal pair associated different video clip.
5.4 Experiment 2: Language Generation
use ability sentence tracker score video-sentence pair generate
sentence describes given video clip searching highest-scoring sentence
clip. However, problem. Recall f , g, h, values log
space range (, 0] increasing value denotes higher score, i.e., better fit
model. Since sentence-tracker scoring function (Equation 10) sums these, scores
decrease longer word strings greater numbers participants result
longer word strings. dont actually search highest-scoring sentence,
would bias process towards short sentences. Instead seek complex sentences
describe clip informative.
Nominally, search process would intractable since space possible sentences
huge even infinite. However, use beam search get approximate
answer. possible sentence tracker score word sequence,
complete sentences, long one construct linking function . select
top-scoring single-word sequences repeatedly extend top-scoring W -word
sequences, one word, select top-scoring W + 1-word sequences, subject
constraint linking function exists W + 1 words W + 1-word
sequences extended grammatical sentences insertion additional words.
terminate search process contraction threshold, ratio score
sequence score sequence expanding it, drops specified
value sequence expanded complete sentence. contraction threshold
controls complexity generated sentence.
restricted FSMs, h 0/1 become /0 log space. Thus
increase number words decrease score , meaning sequence
words no-longer describes video clip. Since seek sentences do, terminate
beam-search process score goes . case,
approximation: beam search maintaining W -word sequences finite score yields
highest-scoring sentence contraction threshold met.
645

fiYu, Siddharth, Barbu, & Siskind

evaluate approach, searched space sentences generated grammar
Table 11(a) find top-scoring sentence 94 video clips first corpus.
Note grammar generates infinite number sentences due recursion NP.
Even restricting grammar eliminate NP recursion yields space 147,123,874,800
sentences. Despite restricting grammar fashion, able effectively
find good descriptions video clips.
evaluated accuracy sentence tracker generating descriptions 94
video clips first corpus multiple contraction thresholds. Accuracy computed
percentage 94 clips sentence tracker produced descriptions
deemed describe video human judges. resulting accuracy
different contraction thresholds shown Table 7. Figure 16 shows highest-scoring
sentence generated approach several clips first corpus contraction
threshold 0.90. Figure 22 Appendix B shows highest-scoring sentence generated
approach 94 clips first corpus. illustrate effect
contraction threshold, show below, generated sentence corresponding
contraction thresholds first video clip Figure 16.
0.95
0.90
0.85

backpack approached trash can.
backpack left chair approached trash can.
backpack left chair approached trash can.

important distinction approach state art generating
sentential video description generativity labeling domain. existing work
(Kulkarni et al., 2011; Gupta, Verma, & Jawahar, 2012), process labeling events
video involves searching phrases sentences best match video using trained
set classifiers. process usually involves extracting correspondences labels
video features training corpus. training corpus labels video word
phrase sentence-generation process unseen video labels video either
existing label training corpus simple concatenation labels.
contrast, approach label unseen video grammatical utterance admitted
grammar lexicon, potentially unbounded set, even ones never
appeared, whole part, training set.
sentence-tracker framework also generate sentential video description
fixed set sentential labels, simply scoring potential label unseen video
clip selecting top-scoring label. evaluate ability labeling 94
video clips first corpus fixed label set 21 sentences shown Table 1
comparing human judgments. performed three analyses. First, measured
percentage clips depict top-scoring sentence determined human judges.
determined 94.68%. Chance performance 13.12%, since average, 2.76
sentences deemed describe given clip, shown Table 4. Second, relax
selection criterion slightly, consider percentage clips described least one
top-three sentences, obtain 100% accuracy. Chance performance 1 (1 0.1312)3
34.42%. Finally, threshold video-sentence score, yielding binary machine
judgment whether given sentence describes given clip, alternatively whether
given clip depicts given sentence. ask well machine judgments
match human judgment 94 21 = 1974 video-sentence pairs first corpus.
646

fiGrounding Language Inference, Generation, Acquisition Video

backpack left chair approached trash can.

person right backpack carried chair.

person right trash approached trash can.

chair right person approached trash can.

backpack left trash approached trash can.
Figure 16: Sentential descriptions generated several video clips first corpus subject
contraction threshold 0.90. highest-scoring sentence clip generated,
among sentences generated grammar Table 11(a), means beam
search. sentences deemed human judges describe associated clips indicated
green, ones indicated red. Sentential descriptions generated
94 video clips first corpus shown Figure 22 Appendix B.

647

fiYu, Siddharth, Barbu, & Siskind

Searching threshold maximizes accuracy yields accuracy 86.88%.
Chance performance 13.12%, since 259 1974 human judgments positive.
Thus sentence tracker performs significantly chance three analyses.
5.5 Experiment 3: Language Acquisition
sentence tracker, wrapped EM, learn lexicon maps words
meanings training set video clips paired sentences. crucial distinction
approach prior state art learning object event recognizers
video that, approach, training videos paired entire sentences,
individual class labels. sentential labels generative; set possible labels
infinite generated context-free grammar contains recursion. Thus
vast majority potential labels never appear training set. Yet method
learn describe previously unseen videos previously unseen sentential labels
composed words likely occur single training sample instead require
composing words learned exposure distinct training samples.
evaluate use sentence tracker perform language acquisition, employ
second corpus described Section 5.1, particular Tables 2 3, together
grammar lexicon Table 11. language fragment contains 17 lexical entries
6 parts speech (1 determiner, 6 nouns, 2 spatial-relation prepositions, 4 verbs,
2 adverbs, 2 motion prepositions). model learn meanings content
words lexicon. Table 8 specifies arity I, number K states, feature-vector
length N , number Z bins fore feature, feature computation
word models part speech c. specify different subset features
part speech, presume that, principle, enough training data, could include
features parts speech automatically learn ones noninformative
lead uniform distributions.
compute continuous features, velocity, distance, size ratio, x-position
detections quantize features bins follows:
velocity reduce noise, compute velocity participant averaging optical
flow detection. velocity magnitude quantized 5 levels. expository clarity, refer levels mnemonically absolutely stationary, mostly
stationary, moving slowly, moving quickly, moving quickly. velocity orientation quantized 4 directions: leftward, upward, rightward, downward.
distance compute Euclidean distance detection centers two participants, quantized 3 levels: near, moderate distance, far.
size ratio compute ratio detection area first participant detection area second participant, quantized 2 levels: larger smaller
than.
x-position compute difference x-coordinates participants, quantized 2 levels: left right of.
binning process determined preprocessing step clustered subset
training data. addition continuous features need quantization, also
incorporate index detector produced detection discrete feature.
648

fiGrounding Language Inference, Generation, Acquisition Video

c

K N Z

N

1

1



1

6

detector index
velocity magnitude first argument
velocity orientation first argument
velocity magnitude second argument
velocity orientation second argument
distance first second arguments
size first argument / size second argument

V

2

3

6

5
4
5
4
3
2

P

2

1

1

2

difference x-positions first second arguments

Adv 1

3

1

5

velocity magnitude

PM

3

2

5
3

velocity magnitude first argument
distance first second arguments

2

Table 8: Characteristics HMMs used model word meanings various parts
speech c. denotes arity, K denotes number states, N denotes number features
output model, Z denotes number bins particular feature, denotes
feature computation.
detector index mainly used identifying detection learning nouns.
particular features computed part speech given Table 8.
Note use English phrases, like left of, refer particular bins
particular features, object detectors train samples particular
object class backpack, phrases mnemonic clustering objectdetector training process. fixed correspondence lexical entries
particular feature value. Moreover, correspondence need one-to-one:
given lexical entry may correspond (time variant) constellation feature values
given feature value may participate meaning multiple lexical entries.
performed three-fold cross validation using partitioning described Section 5.1.
important stress fold, test set disjoint training set,
video clips sentential labels. crucially allowed us evaluate generative
nature sentential labels: ability learn generate previously unseen labels
previously unseen video.
fold, trained lexicon training set fold using procedure
Section 4. evaluated trained lexicon test set fold
performing three distinct analyses:
1. comparing F1 score test set variety baselines
2. comparing ROC curve test set variety baselines
3. inspection learned models comparison hand-constructed models
first two analyses require scoring unseen video-sentence pairs. could scored
Equation 10. However, score depend sentence length W , length
video clip, number L participants, collective numbers states K
feature-vector lengths N word models words sentence. One remove
649

fiYu, Siddharth, Barbu, & Siskind

Fold

Baselines

method

Chance

Blind

Hand

1
2
3

0.06
0.07
0.04

0.10
0.12
0.08

0.73
0.65
0.50

0.56
0.50
0.31

average

0.06

0.10

0.62

0.46

Table 9: comparison F1 scores test sets method variety
baselines.
dependence number L participants using L(B; s, ) score. However,
remove dependence factors.
render scores comparable across variation, apply sentence-length prior
(s) average per-frame score computed whole-video score L(B; s, ):
1

[L(B; s, )] (s)


(s) = exp

W
X

w=1

(Z) =

Z
X
z=1



Ns w

(Ksw ) +

X

n=1

1
1
log = log Z
Z
Z



(Zsnw )

above, (Z) entropy uniform distribution Z bins. prior prefers
longer sentences descriptive video.
resulting scores thresholded decide hits, together manual
annotation, generate True Positive (TP), True Negative (TN), False Positive (FP),
False Negative (FN) counts. conduct first analysis, fold, selected
threshold led maximal F1 score training set, used threshold
compute F1 score test set. Table 9 reports per-fold F1 scores along
average across folds.
comparison, also report F1 scores three baselines: Chance, Blind,
Hand. Chance baseline randomly classifies video-sentence pair hit probability 0.5. Blind baseline determines hits potentially looking sentence
never looking video. strategy make decision video-sentence
pairs pairs contain sentence. find upper bound F1
score blind method could test sets solving 0/1 fractionalprogramming problem follows. optimal blind baseline try find decision dm
test sentences sm maximizes F1 score. Suppose, comparison
ground-truth yields FPm false positives TPm true positives test set
650

fiGrounding Language Inference, Generation, Acquisition Video

dm = 1. Also suppose setting dm = 0 yields FNm false negatives. F1 score then:
1

1+


X

dm FPm + (1 dm )FNm

m=1

X

2dm TPm

m=1

|

{z


}

Thus maximize F1 seek minimize term . instance 0/1 fractionalprogramming problem solved binary search Dinkelbachs (1967) algorithm. yields best possible F1 score blind algorithm produce.
Hand baseline determines hits hand-crafted HMMs described below.
carefully designed yield believe near-optimal performance. seen
Table 9, trained word models perform substantially better Chance
Blind baselines approach performance Hand baseline. corpus
counterbalanced, Chance Blind baselines exhibit similar poor performance.
conduct second analysis, varied threshold used decide hits produce
ROC curves. Figure 17 shows curves folds along average across
folds, comparing trained word models various baselines. Again, trained
word models significantly outperform baselines essentially match performance
hand-crafted word models.
Good F1 scores ROC curves necessary sufficient demonstrate successful learning. possible trained word models reflect artifactual properties
corpus dont encode natural pretheoretic intended meaning. example,
dataset spurious unintended correlations, whenever approach happens,
agent always larger goal, learned word model may reflect correlation
correlation may primary factor leading good performance test set.
artifactual correlation overly strong, could even overpower correlations
relevant features allow learning meanings rely features would fail generalize corpora exhibit artifactual
properties.
evaluate whether occurs experiments, conducted third analysis
compared trained word models (for fold 2) hand-crafted ones illustrated Figures 23 30 Appendix B. qualitative comparison, render hand-crafted
trained word models side side lexical entry, graphically illustrating
output distributions textually illustrating initial-state state-transition-function
distributions. Qualitative inspection indicates corresponding word models indeed quite similar except noise learned word models. crucial qualitative
observation large extent initial-state state-transition-function distributions place bulk probability mass state relevant output
distributions exhibit peaks bins. example, word person, two
word models peak first bin denotes object-detector class person.
Similarly, word models verb approached describe qualitative motion profile.
depict initial state which:
651

fiYu, Siddharth, Barbu, & Siskind

(a)

(b)

(c)

(d)

Figure 17: ROC curves comparing performance trained models various
baselines three folds (a-c) averaged across fold (d).

1. agent goal stationary
2. agent far goal
followed intermediate state which:
1. agent moving horizontally,
2. goal stationary,
3. distance participants decreasing
followed final state which:
1. agent goal stationary
2. agent close goal.
two primary qualitative differences learned hand-crafted distributions. first noise. second hand-crafted distributions irrelevant
features intentionally uniform learned distributions features sometimes encode artifactual properties corpus small extent. example,
second state trained word model picked indicates first argument
652

fiGrounding Language Inference, Generation, Acquisition Video

trained word models
person
backpack
chair
traffic cone
trash
stool
left
right
approached
carried
picked
put
towards
away

random word models

1

2

3

average

1

2

3

average

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
12.63
15.89
9.40
8.73
1.71
3.21

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
15.43
10.60
9.44
13.09
4.69
6.72

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
12.44
11.74
10.97
10.05
3.14
2.86

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
13.50
12.74
9.94
10.62
3.18
4.27

1.11
5.43
4.17
2.09
0.82
1.12
1.19
0.50
11.32
14.42
12.86
16.59
3.97
10.91

1.09
1.72
1.44
1.47
1.35
1.33
0.26
0.09
18.92
11.97
8.49
11.87
3.88
5.32

3.45
1.14
1.64
1.78
1.09
4.10
0.59
0.53
18.10
15.10
14.44
14.02
4.65
9.81

1.88
2.76
2.42
1.78
1.09
2.18
0.68
0.37
16.11
13.83
11.93
14.16
4.17
8.68

Table 10: upper bound KL-divergence hand-crafted trained
word models fold averaged across folds. (left) KL-divergence trained
word models hand-crafted word models. (right) KL-divergence random word
models hand-crafted word models.

moving upward hand-crafted word model contains uniform distribution
velocity orientation first argument. Similarly, second third states
trained word model carried appear, first glance, quite different handwritten one. However, closer inspection reveals encode similar information.
second state hand-written word model actually corresponds last two states
trained word model, collectively encode mixture distribution. mixture distribution encodes fact carried bidirectional involve leftward
rightward motion. hand-written word model encodes single state
bimodal output distribution trained word model encodes two states
unimodal output distributions. lack additional state forces trained
word model merge output distributions velocity features last state
hand-crafted word model two states code mixture distribution.
expect differences eliminated larger training set accurate feature
extraction.
augmented qualitative analysis similarity hand-crafted
trained word models quantitative analysis. computed KL-divergence
output distributions corresponding word models. true KL-divergence
two word models, ignores initial-state distributions state-transition
functions, provides loose lower bound actual KL-divergence. Table 10 reports
word lexicon. Across board, trained word models much
closer hand-trained ones random word models.
653

fiYu, Siddharth, Barbu, & Siskind

6. Related Work
language-inference task discussed Section 5.3 requires mechanism focus attention
particular activity video depicts multiple simultaneous activities. Obtaining
capability extension state-of-the-art methods identify activity
video trivial. large portion work, recently done Kuehne et al.
(2011) Sadanand Corso (2012), identify either single activity given video
rank ordering possible activities. videos depicted multiple simultaneous identical
activities, methods would identify single instance activity.
partly due fact matching features, say STIP (Laptev, 2005), provides
score, means localization. method, hand, so. exist
two instances activity, say pick up, occurring simultaneously, specify
one focus attention means elements video, characteristics
participants (adjectives), manner action (adverbs), relations
participants unrelated objects scene (prepositions). discussed previously
Section 5.4, much prior work generating sentences describe images (Jie,
Caputo, & Ferrari, 2009; Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, &
Forsyth, 2010; Kulkarni et al., 2011; Li & Ma, 2011; Yang, Teo, Daume III, & Aloimonos,
2011; Gupta et al., 2012; Mitchell, Dodge, Goyal, Yamaguchi, Stratos, Han, Mensch, Berg,
Berg, & III, 2012) video (Kojima, Tamura, & Fukunaga, 2002; Fernandez Tena, Baiget,
Roca, & Gonzalez, 2007; Barbu et al., 2012a; Hanckmann et al., 2012; Khan & Gotoh, 2012;
Krishnamoorthy et al., 2013; Wang, Guan, Qiu, Zhuo, & Feng, 2013) uses special-purpose
natural-language-generation methods. method, contrast, systematically searches
highest-scoring sentence generated grammar using video-sentence scoring
function used language inference language acquisition. generativity
labeling domain allows us label unseen video sentence, potentially
unbounded set, including never appeared, whole part, form
training.
active research grounded language learning computational
linguistics community. research employs approaches directly map words
perceptual features extracted external world. Roy (2002) paired training sentences vectors real-valued features extracted synthesized images depict
2D blocks-world scenes, learn specific set features adjectives, nouns, adjuncts. Roy Pentland (2002) presented computational model acquires word
meanings directly multimodal sensory input. Yu Ballard (2004) paired training
images containing multiple objects spoken name candidates objects find
correspondence lexical items visual features. Marocco, Cangelosi, Fischer,
Belpaeme (2010) grounded meanings action words link robots
action effects behavior observed manipulated objects
action. approaches directly learn word meanings associated features,
robustly understand limited set sentential fragments lack capability deal complex syntactic structures, since resulting word meanings
neither generative compositional.
work within computational linguistics community focused learning
symbolic representations word meanings corpora sentences paired sym654

fiGrounding Language Inference, Generation, Acquisition Video

bolic representations sentential meaning, illustrated Figure 18(a). Thompson
Mooney (2003) described system called Wolfie acquires semantic lexicon phrasemeaning pairs corpus sentences paired semantic representations. Zettlemoyer Collins (2005) presented method learning sentence meanings form
lambda-calculus encodings. Dominey Boucher (2005) paired narrated sentences
symbolic representations meanings, automatically extracted video, learn
object names, spatial-relation terms, event names mappings grammatical
structure sentential fragments semantic structure associated meaning representation. Piantadosi, Goodman, Ellis, Tenenbaum (2008) employed unsupervised,
cross-situational Bayesian learning model acquisition compositional semantics,
solve problem referential uncertainty. Chen Mooney (2008) Kim Mooney
(2010) learned language sportscasting determining alignment game
commentaries meaning representations output rule-based simulation
game. later reduced task learning Probabilistic Context-Free Grammar (PCFG) Borschinger, Jones, Johnson (2011). subsequent work (Chen
& Mooney, 2011; Kim & Mooney, 2012, 2013) proposed techniques learning follow
navigation instructions observation given weak, ambiguous supervision. Kwiatkowski,
Zettlemoyer, Goldwater, Steedman (2010) Kwiatkowski et al. (2012) presented
approach learns Montague-grammar representations word meanings together
combinatory categorial grammar (CCG) child-directed sentences paired firstorder formulas represent meaning. Although methods succeed learning
word meanings sentential descriptions, symbolic representations
might extracted simple synthesized visual input; fail bridge gap
language computer vision, i.e., extract meaning representations
complex visual scenes.
recent work computational linguistics robotics communities attempted learn grounded word meanings richer perceptual input paired multiword phrases. Krishnamurthy Kollar (2013) introduced Logical Semantics
Perception (LSP) framework grounded language acquisition learns map natural
language statements referents physical environment. However,
nouns spatial-relation prepositions small set static images. Tellex,
Thaker, Joseph, Roy (2013) learned mapping specific phrases aspects
external world robotic system, assumed ideal scene: perfect object classification, 3D coordinate system, unambiguous demonstration robot
correctly executing action environment.
also research training object event models large corpora
complex images video computer vision community (Feng, Manmatha, &
Lavrenko, 2004; Yao, Yang, Lin, Lee, & Zhu, 2010; Kulkarni et al., 2011; Ordonez, Kulkarni, & Berg, 2011; Kuznetsova, Ordonez, Berg, Berg, & Choi, 2012; Sadanand & Corso,
2012; Chen & Grauman, 2013; Everts, van Gemert, & Gevers, 2013; Song, Morency, &
Davis, 2013; Tian, Sukthankar, & Shah, 2013a), illustrated Figure 18(b).
viewed learning meanings nouns verbs. However, work requires
training data labels individual concepts individual words (i.e., objects delineated
via bounding boxes images nouns events occur short video clips verbs).
words, specify correspondence concepts data
655

fiYu, Siddharth, Barbu, & Siskind

training sentence
training meaning
learned representations

person picked chair.
cause(person, go(chair, up))
person
picked
chair

person
cause(x,go(y,up))
chair

(a)
nouns
training words

training
images/videos

person

verbs
chair

picked

...

...

upward

rightward

downward

leftward

velocity orientation
second argument

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
second argument

learned
representations

(b)

Figure 18: illustration dominant paradigms prior work. (a) work
computational linguistics community learns symbolic representations word meanings
sentences paired symbolic representations sentential meanings. (b) work
computer vision community learns word independently, training data
annotates image video portion corresponds object event label, distinct
representations part speech.

words trained. attempt model phrasal sentential meaning,
let alone acquire object event models training data labeled phrasal
sentential annotation. result, learned word meanings neither generative
compositional. Descriptions new images video produced mosaicing together
previously learned sentence fragments. Moreover, unlike methods presented here,
approaches use distinct representations different parts speech; i.e., object event
recognizers use different representations.
method differs prior work three ways. First, input consists realistic
video filmed outdoor environment. Second, learn entire lexicon, including
nouns, verbs, adverbs, prepositions, simultaneously video described whole
656

fiGrounding Language Inference, Generation, Acquisition Video

sentences. Third, adopt uniform representation meanings words
parts speech, namely hidden Markov models (HMMs) whose states distributions
allow multiple possible interpretations word sentence ambiguous perceptual
context.
work presented similar three recent papers (Das, Xu, Doell, &
Corso, 2013; Rohrbach, Qin, Titov, Thater, Pinkal, & Schiele, 2013; Guadarrama, Krishnamoorthy, Malkarnenkar, Venugopalan, Mooney, Darrell, & Saenko, 2013) generate
text descriptions video. surface, papers appear describe approaches
handle unrestricted text video. However, deeper analysis reveals
case. Indeed, analysis demonstrates space text supported systems
far restrictive present here. discuss prior work depth
along analysis.
Das et al. (2013) generate text descriptions cooking videos garnered YouTube.
using shallow vision features unseen video index training
corpus videos paired text annotations find similar videos stitching together
fragments text associated indexed videos obtain new text annotation
unseen video.
1. model word sentence meanings. doesnt know
words sentences annotations refer video. One cant point
component system say definition particular word.
precisely Table 6 Figures 5, 6, 2330 (in Appendix B).
Moreover, one cant analyze portions meanings correct
wrong, page 651. system generates incorrect annotation,
nothing much one say so.
2. (1) cant call inference. cant process video
simultaneous actions taking place different subsets actors objects
video two different sentences highlight different sets participants
different sentences. precisely demonstrate Figure 15 Figure 21
Appendix B. demonstrates deep understanding. fact
minimal pairs, pairs sentences differ single word, vary word
lexical entries sentential positions demonstrates semantic model
reflects deep understanding every word.
3. work Das et al. (2013) lacks such. causes method generate
huge number erroneous descriptions. Das et al. (2013, Figure 6) show five sample
sentences generated five sample videos. sample videos,
three four generated sentences false video. completely
incorrect objects actions. examples picked showcase system.
Presumably, performs worse examples. contrast, conduct present
results thorough evaluation: Figure 21 Appendix B presents results
examples, without exception.
4. nouns adjectives generated sentences work Das et al. (2013,
Figure 6) describe objects far beyond ability state-of-the-art objectdetection systems detect (e.g., knob, pliers pieces metal, glass bowl, porcelain
bowl, sponge, old food, dish towel, hand held brush, vacuum, panel, health care reform,
. . .), particularly size field view. Ditto verbs denoting
657

fiYu, Siddharth, Barbu, & Siskind

actions (e.g., clean, speak, sit, stand, open, renovate install, bend, cook, mix, . . .).
system really grounding meanings words video. Rather
indexing based surface features. mean say
system uses linguistics. system may use techniques prevalent
natural-language-processing community, one might even call computational
linguistics, one would call linguistics. denigrate system.
simply incomparable work.
5. Das et al. (2013) report measured alignment words text
portions video. Thus one unable determine whether sentence
generation really based video features convey meanings words
sentences generated whether based accidental correlation
features background reflective true meanings.
Rohrbach et al. (2013) generate text annotations videos two-step process.
first translate video x intermediate representation (SR) translate
SR sentence z. SR five discrete random variables (activity, tool, object,
source, target). 66 possible activities, 43 possible tools, 109 possible objects,
51 possible sources, 35 possible targets. mapping video SR mediated
joint probability model implemented conditional random field (CRF) mutually
constrains five random variables. CRF trained supervised fashion.
training data contains videos paired human annotated SRs. mapping process
video SR yields quantized SR returning SR training set
lowest Hamming (1950) distance SR estimated CRF.
text-generation process involves second step maps SR sentence.
However, process use information video already
abstracted SR. purposes comparing work, process relevant.
discrete quantized SR component work Rohrbach et al. (2013)
analogous individual words generate. case, mapping
words sentences done deterministic grammar neither introduces errors
contains joint-distribution information filter errors. mapping
SR sentences introduce errors also constitutes additional level jointconstrained-distribution information filter errors. Thus compare system
first step work.
Due Hamming-distance post processing, Rohrbach et al. (2013) output
one 5,609 possible SRs total 66431095135=552,175,470 ones
nominally possible. Thus generate 5,609 possible sentences. contrast,
system generate 235,575 possible sentences three objects
first corpus, 406,296 possible sentences three objects second
corpus, 6,614,325 possible sentences four objects first corpus,
13,633,272 possible sentences four objects second corpus.
Thus surface, appears system Rohrbach et al. (2013) handles
unrestricted text, reality handles space sentences four five orders
magnitude smaller do. Thus solving immensely easier problem
much smaller space possible outputs. Yet, Rohrbach et al. (2013, Table 1) indicate
obtain correct SR 21.6% time. obtain true sentence
658

fiGrounding Language Inference, Generation, Acquisition Video

64% time. Moreover, system learns solely videos paired sentences.
system requires additional human annotation SRs associated video.
Points 15 comparison work Das et al. (2013) also apply
work Rohrbach et al. (2013). particular, vast majority object classes well
beyond state-of-the-art ability support recognition CRF (e.g.,
avocado, egg, cucumber, bag chilies, cutting board, loaf bread, lime, knife, plate, butter,
carrot, (half ) kiwi, package beans, orange, saucer, . . .), particularly size
field view. Similarly, vast majority verbs well beyond state
art support action recognition, CRF (e.g., slice, crack, take
out, rinses, put away, select, split, . . .). Rohrbach et al. (2013) derive success
highly constrained set possible SRs distribution encoded CRF.
means cannot describe videos exhibit person taking kiwi fridge
never occurred training corpus, even though might perfectly reasonable
video. Surely vastly 5,609 552,175,470 possible SRs plausible
perhaps even likely. Yet even constraint, Rohrbach et al. (2013, Table 2) report
human judges evaluated truth generated sentences, average report
3.1 scale 1 5, 3 7080% good. Moreover, limited
particular representation employed SRs. encode sentence meanings
formulated terms particular five random variables (activity, tool, object,
source, target). contrast, approach formulate sentence meanings terms
arbitrary conjunctions predicates applied subset event participants long
predicates formulated HMMs arbitrary output distributions
features extracted video.
Guadarrama et al. (2013) describe method outputs three-word sentences summarize video activity. Like Rohrbach et al. (2013), encoded three variables:
actor (subject), action (verb), object (object). 45 possible subjects,
218 possible verbs, 241 possible objects. Given training corpus comprising video
clips paired annotated SVO triples, method first builds three semantic hierarchies,
represented trees, one subject, verb, object, indicate similarity
relationships among meanings words occur training corpus. word
appears training corpus constitutes leaf node one hierarchy trees.
internal nodes represent sets dominated leaf nodes, generalized concept less
specificity leaf nodes.
visual classifier associated leaf nodes individual subject, verb,
object. leaf classifier uses
1. Dense Trajectories (Wang, Klaser, Schmid, & Liu, 2011, 2013; Wang & Schmid, 2013),
encoded using pre-trained codebook,
2. vector object-detector scores, entry denoting maximal score
object class,
3. multi-channel approach combines two features classifies
non-linear SVM.
classifiers trained, probability estimates nodes hierarchy trees
obtained unseen video clip. nodes three hierarchies representing
words generated unseen video clip predicted optimizing cost function
trades specificity accuracy. internal node predicted, represen659

fiYu, Siddharth, Barbu, & Siskind

tative leaf word selected set leaves dominated node leaf
highest cumulative WUP WordNet (Miller, 1995; Fellbaum, 1998). Guadarrama
et al. (2013) also introduce zero-shot approach generate verbs appear
training corpus thus absent verb hierarchy. that, verb
determined text-mined likelihoods fit detected subject object.
paint virtue, view deficit. Essentially, guessing verb.
celebrated cases objects predict verbs vice versa (e.g., hammer ),
believe accounts far less actual video. one sees dog cat,
still plethora possible verbs: approach, leave, run away from, fight with, ignore,
chase, flee from,bite, lick, . . . easy pick examples showcase works
says little well approach works general.
approach taken Guadarrama et al. (2013) similar taken
Rohrbach et al. (2013), construct joint probability models collection
random variables, five case latter three case former. Rohrbach
et al. add quantization Hamming distance absent work Guadarrama
et al., Guadarrama et al. add zero-shot approach along hierarchies
balance accuracy specificity absent work Rohrbach et al.. Without zero-shot extension, Guadarrama et al. output one 45218241=2,364,210
possible sentences. number roughly equivalent number sentences
method produce.
work Guadarrama et al. exhibits shortcomings work
Rohrbach et al. Das et al. (2013). Points 15 comparison work
Das et al. also apply work Guadarrama et al.. case work
Rohrbach et al., vast majority object classes well beyond state-of-theart ability support recognition (e.g., chef, cook, microphone, flute, flour, music, pasta,
spaghetti, . . .), particularly size field view. Similarly, vast
majority verbs well beyond state art support action recognition
(e.g., slice, cut, chop, prepare, make, . . .). almost certain visual-feature space
would separate verbs chop cut, nouns pasta spaghetti.
words also similar semantic meanings even quite difficult
humans distinguish short video clips. Thus number words appear
generated sentences increased considering similar lexical items, difficult
generation task increase much expected evaluation lax (e.g.,
considering chop correct even though slice actually happens video).
hand, specificity-accuracy tradeoff, generated sentences
sometimes uninformative, e.g., animal plays something animal something instrument (Guadarrama et al., 2013, Table 4). Also zero-shot approach
seems override actual activity recognition quite easily, seen fourth
row (Guadarrama et al., 2013, Table 4): car rides vehicle. Finally, Guadarrama et al.
evaluate truth sentences generated. Instead, calculate WUP
similarity generated annotated subjects, verbs, objects, independently.
estimates, seven eleven generated sentences false corresponding video clip. examples picked showcase system. Presumably,
performs worse examples. unclear actual truth accuracy
generated sentences entire corpus.
660

fiGrounding Language Inference, Generation, Acquisition Video

also something deeply unsettling general approach taken
Rohrbach et al. Guadarrama et al. using joint probability model derived text
mining influence activity recognition. Suppose corpus text much higher
frequency occurrence dog chases cat dog is-bigger-than cat dog eats-with cat.
says nothing actual prior truth probability underlying propositions,
let alone actual posterior truth probability conditioned particular video.
sense, Rohrbach et al. Guadarrama et al. actually grounding language
video rather generating natural-language utterances using information obtained
ungrounded language.

7. Discussion
computational linguistics community become accustomed employing large lexicons grammars trained large text corpora process unrestricted text. Similarly,
computer vision community become accustomed employing methods
trained large image video corpora process unrestricted images video. One
may wonder would take extend methods explored
apply large-scale unrestricted text video corpora. One might assume
simply matter employing better state-of-the-art methods computational
linguistics computer vision. This, however, case. use
state-of-the-art methods computational linguistics, computer vision methods
state art. use deformable part model (DPM) object detector (Felzenszwalb
et al., 2010a, 2010b) action detector exhibits state-of-the-art performance.
approach limited computer vision, computational linguistics. state
art object detection reflected ongoing Pascal Visual Object Category (VOC)
Challenge (Everingham et al., 2010). currently 20 classes current state-of-the-art
performance 40-50% best classes, far worse classes. state
art action recognition reflected standard corpora used community, e.g., Weizmann (9 classes; Blank, Gorelick, Shechtman, Irani, & Basri, 2005), KTH
(6 classes; Schuldt, Laptev, & Caputo, 2004a), UCF Sports (10 classes; Rodriguez, Ahmed,
& Shah, 2008), UCF YouTube (11 classes; Liu, Luo, & Shah, 2009), Olympic Sports (16
classes; Niebles, Chen, & Fei-Fei, 2010). best reported performance corpora
(Weizmann 100%: Tian, Sukthankar, & Shah, 2013b; KTH 95.49%: Yuan, Li, Hu, Ling, &
Maybank, 2013; UCF Sports 95%: Sadanand & Corso, 2012; UCF YouTube 89.4%: Zhu,
Wang, Yang, Zhang, & Tu, 2013; Olympic Sports 85%: Gaidon, Harchaoui, & Schmid,
2014) might lead one mistaken conclusion action classification solved small
numbers classes. However, Barbu, Barrett, Chen, Siddharth, Xiong, Corso, Fellbaum,
Hanson, Hanson, Helie, Malaia, Pearlmutter, Siskind, Talavage, Wilbur (2014) illustrate false, 6-class corpus state-of-the-art methods get
52.34%. Moreover, largest corpora actively used action recognition contain 50
classes (UCF50, Reddy & Shah, 2013 HMDB51, Kuehne et al., 2011). best reported
performance UCF50 91.2% HMDB51 57.2%, (Wang & Schmid, 2013). Thus
approach, ours, grounds meaning individual word stateof-the-art computer vision object detectors, trackers, action recognizers inherently
limited small number concepts. space natural-language utterances
661

fiYu, Siddharth, Barbu, & Siskind

one erect around thus limited effectively captured small fixed
unambiguous context-free grammar. Thus employing state-of-the-art methods computational linguistics would improve generality approach given limited
state art computer vision.
paper we, thus, employ state-of-the-art methods computational
linguistics. employ small fixed lexicon grammar. make claim
lexicon grammar general. particular lexicon grammar focus
work. serve illustrate framework capability framework
supporting concerns outlined end Section 1. One change lexicon
grammar still use framework. Indeed, done report this.
Table 11(a) reports two slightly different grammars. experiments employ two different video corpora two different sets sentential annotations use different
grammars reported Tables 13. video corpora use different sets objects
associated object-detector models. Barbu, Siddharth, Siskind (2014) employ yet another corpus, different set objects object-detector models, different lexicon,
different grammar, different set word-meaning representations. demonstrates
framework adapted variety such. beyond this, Barbu et al.
(2014) demonstrate yet another whole different application framework, namely
video retrieval. corpus ten full-length Hollywood movies.
corpus far toy. framework support large-scale real-world video
wild. Yet concept vocabulary still small natural-language fragment still
restricted. one could employ state-of-the-art methods computational linguistics,
supported concept set thus supported language fragment would still small.
Thus one would using state-of-the-art methods potential
designed for.
two general approaches towards action recognition computer vision. One
employs methods detect track people objects participate action,
classifying action properties derived detected objects tracks.
extracts classifies features video without detecting tracking people objects. latter methods generally employ bag spatio-temporal visual-words approach
(BOW). generally extract feature vectors, spatio-temporal interest points
(STIP; Schuldt, Laptev, & Caputo, 2004b), subset space-time points, build codebook pooling such, vector quantize feature vectors codebook, compute
histogram codebook-entry occurrences pooled frames video, classify
histograms temporally invariant models. Early approaches action recognition generally employed former method (e.g., Siskind & Morris, 1996; Mann, Jepson, & Siskind,
1996, 1997; Siskind, 1999, 2000, 2001; Fern, Givan, & Siskind, 2002a; Fern, Siskind, & Givan, 2002c; Fern, Givan, & Siskind, 2002b; Siskind, 2003). approach eschewed
recent work, favor latter method, difficulty detecting
tracking people objects reliably (e.g., Schuldt et al., 2004b; Liu et al., 2009; IkizlerCinbis & Sclaroff, 2010; Kuehne et al., 2011; Reddy & Shah, 2013). However, BOW
approach suffers severe limitation: localize event participants.
may able generate verbs describe classified actions, cannot generate nouns
describe object class event participants, adjectives describe properties
event participants, spatial-relation prepositions describe relative position event
662

fiGrounding Language Inference, Generation, Acquisition Video

participants, adverbs describe event properties, motion prepositions describe
path taken event participants. distinguishing, novel, unique aspect
approach. Moreover, systems, one proposed Guadarrama et al.
(2013), employ object detector addition STIP-based event detector,
link objects arguments event predicates. system using similar approach
like would
1. fail distinguish dog approached person cat approached person
dog cat present field view
2. fail distinguish dog approached person person approached dog.
approach correctly makes distinctions. One design principles behind
corpus multiple people appear most, all, videos, most, all, objects
appear every video. Beyond this, videos depict simultaneous different actions
different subsets participants. renders minimal-pairs experiment
(Section 5.3) acquisition experiment (Section 5.5) far trivial.
make claim particular features employ Tables 6 8
sufficient represent semantics possible words utterances. serve
support experimental evaluation conducted Section 5. One could employ
sentence-tracker approach discussed different set features. Indeed,
done (Barbu et al., 2014). Moreover, make claim one employ HMMs
form core sentence tracker represent semantics possible words
utterances. limitation HMM-based approach requires
object detectors trackers; BOW approaches suffer well. BOW approach
cannot represent verb approach. neither BOW HMM approach represent
verbs liberate, contemplate, discuss, help, finish, . . . Representing semantics
entire space verbs, let alone natural language, even non-grounded fashion,
even so, grounded video, central unsolved problem computational
linguistics, AI, cognitive science.
surface, may appear BOW approaches robust recognizing
certain action classes like play instrument approaches involve detecting
tracking objects. However, none standard datasets (Weizmann, KTH, UCF Sports,
UCF YouTube, Olympic Sports, UCF50, HMDB51) class playing instrument
(in general). one, UCF50, classes playing small number specific instruments: drumming, playing guitar, playing piano, playing violin. unaware
published action-recognition systems perform well dataset. One best
performing methods dataset Action Bank (Sadanand & Corso, 2012),
use BOW. performance method enlightening current state
art. gets roughly 80% accuracy classes. Moreover confusion matrix enlightening: drumming confused biking yoyo, playing piano confused
basketball, drumming, golf swing, tennis swing, soccer, juggling, playing violin
confused drumming, rope-climbing, taichi, tennis, yoyo, rock climbing.
confusions indicate lacks deep understanding characteristic actions
question appears triggering spurious correlations particular
dataset. particular, dataset contain people sitting next drum set
piano, holding guitar violin without playing it. way know whether
actually recognizing playing activity simply recognizing gross image statistics
663

fiYu, Siddharth, Barbu, & Siskind

indicate instruments present field view. one gets
motions air guitar, banging piano keys elbow, simply waving
violin air constitute activity instrument question dont constitute playing said instrument. Beyond this, see little ability generalize
playing specific instrument playing instrument general.
BOW-based systems often encode true semantics actions question.
often trigger spurious correlations dataset. acknowledged
authors systems themselves.
instance, v spiking normally happens crowd people, diving
happens pool. common professional sport actions take
place highly structured environments (Liu et al., 2009, p. 2002)
Basketball shooting volleyball actions also confused cases:
largely time, basketball volleyball sports use
similar courts (Ikizler-Cinbis & Sclarof, 2010, p. 505)
One may desire, even expect, form characterization space possible
words videos approach support. Unfortunately, know way provide such. know way, general, formally characterizing space words,
images, video supported action-recognition system, matter object-recognition system or, generally, computer vision, computational
linguistics, AI system.
current corpus lacks camera motion. restriction approach.
restriction appear mathematical algorithmic formulations
Section 2 4, even implementation. sentence tracker extension
prior work detection-based tracking (Barbu et al., 2012b) employed perform
action recognition sentence generation videos involve camera motion (Barbu
et al., 2012a). Barbu et al. (2014) apply sentence tracker perform video retrieval
corpus ten full-length Hollywood movies, vast majority involve camera
motion.
framework expressly restricted using verbs represent events.
current linking process particular grammar used support process restricted
such. nothing turns that. discussed above, sentence tracker use
linking process construct factorial utterance-level HMM constituent wordlevel HMMs. expedience, limit set features entertained learning
part-of-speech basis. restriction could lifted change algorithm
implementation. introduced allow convergence smaller training set.
know reason method Section 4 would work without restriction.
would require larger corpus would unwieldy perform experiments with.
method represents word meanings parts speech simply predicates
one tracks sentential meanings conjunctions such. Presumably, different linking process could construct logical form man(x) pause(x)
sentences like man made pause well could man paused.
beauty approach, employing unified representation meanings
664

fiGrounding Language Inference, Generation, Acquisition Video

words parts speech, common cost function, common algorithm, common
implementation.
State-of-the-art object-recognition systems highly unreliable. image datasets,
trained object model, say person chair, may succeed one image fail another, even chair person pose wearing
clothing background. video, even happens adjacent
frames video. State-of-the-art object detection suffers immense false positives negatives. Moreover, reliable object detection imply reliable
action recognition, state-of-the-art action recognizers similarly highly unreliable. Stateof-the-art recognizers bend wave trained one dataset yield chance performance
different datasets. Even dataset, action recognizers mysteriously
succeed fail similar samples, background, actors,
manner performance action, etc. central novel contribution work
sentence tracker Equation 10, method overcoming severe limitations
object detectors action detectors formulating joint model object detection,
tracking (temporal coherence), sentential semantics.
video corpora may appear simpler typically used current
action-recognition work computer vision community (e.g., Weizmann, KTH, UCF
Sports, UCF YouTube, Olympic Sports, UCF50, HMDB51) apparent simplicity
misleading. Several aspects video corpora far complex used
vast majority related work.
1. videos contain many, all, objects repertoire. makes
language acquisition difficult. One needs determine objects referred
training sentences ignore extraneous ones field view.
done automatically without human annotation.
2. videos contain least two simultaneous actions, often performed different
people different objects. One needs determine action referred
training sentence associated video, pay attention particular
subset people objects participate action, ignore extraneous
activity occurs field view. done automatically without
human annotation.
3. system process complex natural-language sentences contain many participants, e.g., something complex person left chair carried
backpack right traffic cone towards stool left person. even support multiple instances noun sentence refer
distinct instances object class video (as person above).
determine semantic-role assignment, nouns arguments
words correspond regions video frames. assignment determined
automatically without human annotation change small subtle
changes sentence. Moreover, learn solely complex sentential annotation, without human annotation words correspond
regions video frames.
novel central technical contribution formulation sentence tracker
Equation 10 observation optimized using standard well-known
techniques adapted HMMs, namely Viterbi algorithm (1967) Baum-Welch
665

fiYu, Siddharth, Barbu, & Siskind

(1970, 1972). key understanding Equation 10 jointly optimizes cost function incorporates multiple detection-based trackers, one event participant,
multiple factorial event models, one lexical item sentence, judiciously linking
detection-based trackers factors sentential model way consistent
predicate-argument structure sentence, model truth-conditional semantics
sentence derived constituent words. Formulating truth-conditional
sentential semantics way allows exiting algorithms like Viterbi Baum-Welch
ground semantics natural language video perform novel applications
language inference (Section 5.3), language generation (Section 5.4, language acquisition
(Section 5.5), particularly minimal-pair experiment Figure 15 acquisition
videos labeled whole sentences human annotation.
significant prior work multi-object tracking (e.g., Berclaz, Fleuret,
Turetken, & Fua, 2011; Pirsiavash, Ramanan, & Fowlkes, 2011). novel aspect event
tracker particular formulation detection-based tracking cost function
optimized Viterbi algorithm allows forming joint model HMM-based
event detector also optimized Viterbi algorithm cross-product
lattice. might possible trackers event models. Beyond
this, sentence tracker forms joint model multiple trackers factorial HMM,
linking particular factors particular trackers, way again, also optimized
Viterbi algorithm cross-product lattice. also might possible
multi-object trackers event models.
video corpora filmed giving actors instructions actions perform. such, staged. computational linguistics community attempted
use unsolicited samples natural language fear solicited samples might introduce bias. One might wonder whether desirable, even possible, video
corpora well. However, appears infeasible gather unsolicited video corpora except
surveillance situations. Surveillance video tends highly uniform sparse:
event classes occur occur infrequently. renders ill suited action
recognition. Almost situations video recorded, even recorded
explicitly computer vision use, solicited. amateur video form uploaded
YouTube similarly staged level usually records activity elicited specifically
filming. Indeed, prominent video corpora used computer vision community
evaluate action recognition filmed specifically purpose constructing
corpus: Weizmann, KTH, Activities Daily Living corpus (Messing, Pal, & Kautz,
2009), DARPA Minds Eye corpus (both year 1 year 2), TaCOS corpus
used Rohrbach et al. (2013), name few. YouCook corpus used
Das et al. (2013) culled YouTube, videos appear staged,
above.
related work generating sentences describe video evaluates generated
sentences comparison human-elicited sentences video. often
done computing BLEU scores (Rohrbach et al., 2013) measuring fraction
words common machine-generated human-elicited descriptions (Khan,
Zhang, & Gotoh, 2011). might evaluate degree machine-generated
sentences natural sounding, fails evaluate truth machine-generated
sentences, central objective work. Indeed, machine-generated sentences
666

fiGrounding Language Inference, Generation, Acquisition Video

high BLEU scores high commonality human-elicited descriptions often false
video even human-elicited descriptions true.
current linking process would fail ambiguous sentence parse. linking
process might also fail yield unambiguous role assignment unique linking function. Further, current lexicon contains lexical ambiguity current linking
process would support such. myriad approaches parsing constructing
logical form presence ambiguity could brought bear problem.
beyond this, current approach offers novel possibility existing approach
support. One imagine using video disambiguate parsing construction
logical form. One could imagine evaluating truth various word senses, sentence fragments, attachment alternatives, alternate logical forms video using sentence
tracker.
sentence tracker general-purpose inference mechanism combining information multiple frames video using language vision.
presented particular instantiation sentence tracker, particular detectors, particular temporal-coherence scores, particular event models operating 2D, general
approach could instantiated numerous ways. employed object detectors detection sources, method selects image regions could used
approach presented. need rectangular: one imagine variants sentence tracker employ general-purpose foreground-background segmentation instead
object detection. also need two-dimensional: one imagine variants
sentence tracker employ projection models reconstruct temporally-coherent tracks
3D 2D images also satisfy 3D event models. could even pool detections
variety sources scale scores prefer reliable ones possible.
Moreover, temporal-coherence score uses optical flow, could employ appearance model order alleviate situations tracks converge image
location swapped two tracked objects diverge
location. one employ human-pose detector, one could incorporate coherence
human-pose variation temporal-coherence model. One could similarly incorporate changing human pose event model. event tracker would
allow event model influence improve recovered human pose estimated
top-down fashion, much way event model influence improve recovered tracks. Finally, event models formulated HMMs,
general frameworks possible. Even nongenerative frameworks, like maximum-entropy
Markov models, could accommodated long inference could performed using
lattice dynamic programming. One even imagine forgoing lattice dynamic
programming integrate complex models object detection, temporal coherence,
events using message-passing inference.
sentence tracker also learn word meanings video paired sentences.
Unlike prior work, method deals video labeled whole sentences, instead
individual words. Moreover, method successfully learns without prior delineation
correspondence words sentence labels visual features associated
video used object and/or event recognition. experiments show correctly
learn meaning representations terms HMM parameters lexical entries,
highly ambiguous training data, training video clip depicts one
667

fiYu, Siddharth, Barbu, & Siskind

sentence sentence describes one clip. performing
inter- intra-sentential inference: determining meaning word cross-situationally
collection training samples appears well spreading
sentential meaning across words sentence way consistent across
training set.
method amenable extension. First, due nature Markov
models, state depends immediate predecessor. discussed Section 2.2,
property implies output model employ features computed single
frames two adjacent frames. features may prove inadequate larger lexicons.
example, models often exhibit difficulty differentiating picked put
down, since difference encoded second-argument velocity orientation
second state. current implementation computes orientation using optical flow
noisy. One could reliably differentiate two event classes
one could encode model overall displacement second argument, along
direction displacement, event proceeds: picked involves significant
upward displacement put involves significant downward displacement
possible encode multiple-frame feature HMM, possible
complex graphical models conditional random fields (CRFs). One
imagine employing CRFs event model, together object detection temporal
coherence, variant sentence tracker.
Another possible extension employing state-duration models HMMs. Without explicit state-duration models, implicit state-duration model exponential:
probability staying state k frames a(k, k)t . exponential stateduration model encode minimum duration event, filter short-term noise
signal, discussed Section 2.2, cannot bias event detector towards typical
duration performing event. experiments, manifest difficulty
distinguishing picked put similar initial final
states differ short transition period. Employing explicit state-duration models,
hidden semi-Markov models (HSMMs; Yu, 2010) event models within
sentence-tracker framework could potentially improve alleviate difficulty.
third possible extension employ discriminative training instead maximumlikelihood training. Maximum-likelihood training makes use positive sentential labels
training data. Discriminative training also make use negative sentential labels.
could reduce amount training data required also could yield better results
trains models competitively. would require method obtaining negative
sentence labels. One could manual annotation, positive sentence
labels. However, discriminative training works well number negative labels far
exceeds number positive ones. Thus rather manual annotation, one imagine
form sentential inference automatically generate negative sentential labels
could possibly true video associated positive sentential label. may
allow learning larger lexicons complex video without excessive training data.
668

fiGrounding Language Inference, Generation, Acquisition Video

8. Conclusion
presented novel framework utilizes compositional structure events
compositional structure language drive semantically meaningful targeted
approach towards event recognition. multimodal framework integrates low-level visual
components, object detectors, high-level semantic information, form
sentential descriptions natural language. integration facilitated shared
structure detection-based tracking, encodes low-level visual features,
event models, form HMMs, encode sentential semantics.
demonstrated utility expressiveness framework performing three
separate tasks video corpora, simply leveraging framework different manners. first, language inference, showcases ability focus attention
tracker event described sentence, demonstrating capability correctly
identify subtle distinctions person picked chair left
trash person picked chair right trash can. second,
language generation, showcases ability produce complex sentential description
video clip, involving multiple parts speech, performing efficient search
best description though space possible descriptions. third, language
acquisition, showcases ability learn lexicon corpus video clips annotated
sentential descriptions searching among possible lexicons find one allows
sentences best collectively describe associated video clips.

Acknowledgments
research sponsored Army Research Laboratory accomplished
Cooperative Agreement Number W911NF-10-2-0060. views conclusions contained
document authors interpreted representing
official policies, either express implied, Army Research Laboratory U.S.
Government. U.S. Government authorized reproduce distribute reprints
Government purposes, notwithstanding copyright notation herein.

Appendix A. Linking Process
use linking process mediated grammar portions lexicon .
lexicon portion specifies arity permissible roles individual lexical entries.
grammar used experiments Section 5 shown Table 11(a). portion
lexicon specifies arity permissible roles used experiments shown
Table 11(b). grammar lexicon portion, linking process described
determine sentence Equation 11 3 participants produce
linking function Equation 12.
linking process operates first constructing parse tree sentence
given grammar. means recursive-descent parser. lexical-category
heads parse tree map words used sentence tracker. Nominally, lexical
categories, e.g., noun (N), adjective (A), verb (V), adverb (Adv), preposition (P),
serve heads corresponding phrasal categories NP, AP, VP, AdvP, PP.
669

fiYu, Siddharth, Barbu, & Siskind

(a)

NP VP
NP [A] N [PP]
|
blue | red
N person | backpack | chair |trash | traffic cone | stool | object
PP P NP
P left | right
VP V NP [Adv] [PPM ]
V approached | carried | picked | put
Adv quickly | slowly
PPM PM NP
PM towards | away

(b)

left : {agent, patient, source, goal, referent}, {referent}
right : {agent, patient, source, goal, referent}, {referent}
approached : {agent}, {goal}
carried : {agent}, {patient}
picked up: {agent}, {patient}
put down: {agent}, {patient}
towards: {agent, patient}, {goal}
away from: {agent, patient}, {source}
other: {agent, patient, source, goal, referent}

Table 11: (a) grammar used experiments Section 5. Terminals nonterminals red used experiments Sections 5.3 5.4 first corpus.
Terminals nonterminals green used experiments Section 5.5
second corpus. Terminals nonterminals black used experiments
corpora. first corpus uses 19 lexical entries 7 parts speech (2 determiners, 2 adjectives, 5 nouns, 2 spatial-relation prepositions, 4 verbs, 2 adverbs, 2 motion
prepositions). second corpus uses 17 lexical entries 6 parts speech (1 determiner,
6 nouns, 2 spatial-relation prepositions, 4 verbs, 2 adverbs, 2 motion prepositions). Note
grammar allows infinite recursion noun phrase. (b) portion
lexicon specifies arity permissible roles experiments Section 5.

structure parse tree encodes linking function different words
form government relations (Chomsky, 1982; Aoun & Sportiche, 1983; Haegeman, 1992;
Chomsky, 2002). government relation defined formally Figure 19.
example, determine Figure 20, N person governs P right
N chair, P right governs N chair.
government relation, coupled lexicon portion, determines number L
participants linking function . construct word w head.
lexicon portion specifies arity lexical entry, namely fact person, chair,
670

fiGrounding Language Inference, Generation, Acquisition Video






lexical categories N, A, V, Adv, P heads.
Parse-tree nodes labeled heads governors.
parse-tree node dominates parse-tree node iff subtree .
X-bar theory (Jackendoff, 1977), parse-tree node maximal projection
parse-tree node iff
labeled lexical category X,
labeled corresponding phrasal category XP,
dominates ,
parse-tree node exists
labeled XP,
dominates ,
dominates .
parse-tree node m-commands parse-tree node iff dominate
maximal projection dominates .
parse-tree node c-commands parse-tree node iff dominate
immediate parent dominates .
parse-tree node governs parse-tree node iff
governor,
m-commands ,
parse-tree node exists
governor,
m-commands ,
c-commands ,
c-command .
Figure 19: government relation underlying linking process .

backpack unary right picked binary. sole argument
word associated head noun filled distinct participant.3 sole
argument word associated unary non-noun head filled sole
argument word associated head noun governs . first argument
word associated binary non-noun head also filled sole argument
word associated head noun governs . second argument word
associated binary non-noun head filled sole argument word
associated head noun governed . example Figure 20, sole
arguments words associated nouns person, chair, backpack assigned
distinct participants 1, 2, 3 respectively. arguments word associated
preposition right assigned participants 1 2, since N person
governs P right turn governs N chair. Similarly, arguments
word associated verb picked assigned participants 1 3, since
N person governs V picked turn governs N backpack.
3. document concern anaphora, thus omit discussion support potential
coreference. implementation, fact, support mediates analysis
determiners.

671

fiYu, Siddharth, Barbu, & Siskind


VP

NP


person

picked

NP

P
right

NP

V

PP

N



N



N

backpack

chair
Figure 20: parse tree example sentence person right chair picked
backpack. highlighted portion indicates government relations P
right used determine arguments. N person governs P
right of, N chair, P right governs N chair.
determine consistent assignment roles, one agent, patient, source,
goal, referent, participants. allowed roles argument word
specified lexicon portion. specification arity permissible roles used
experiments Section 5 given Table 11(b). specification e : {r11 , . . .}, . . . , {r1Ie , . . .}
means arity lexical entry e Ie r1i , . . . constitute permissible roles
argument i. participant constrained assigned role intersection
sets permissible roles argument word participant appears.
constrain role assignment assign role one participant.
example sentence Equation 11, role assignment computed follows:
role(1) {agent, patient, source, goal, referent} {agent, patient} {agent}
role(2) {agent, patient, source, goal, referent} {referent}
role(3) {agent, patient, source, goal, referent} {patient}
leading to:
role(1) = agent role(2) = referent role(3) = patient

672

fiGrounding Language Inference, Generation, Acquisition Video

Appendix B. Complete Experimental Results

backpack approached trash can.

chair approached trash can.

person left trash put object.

person right trash put object.
Figure 21: Language inference: two different track collections video clip produced guidance two different sentences. minimal pairs sentences correspond
sentences 19 Table 1 differences (a) (b) variants highlighted. track collections deemed human judges depict given sentences
indicated green, ones indicated red.

673

fiYu, Siddharth, Barbu, & Siskind

person put trash can.

person put backpack.

person carried red object.

person carried blue object.

person picked object left trash can.

person picked object right trash can.
Figure 21: Language-inference examples continued.
674

fiGrounding Language Inference, Generation, Acquisition Video

person picked object.

person put object.

person picked object quickly.

person picked object slowly.

person carried object towards trash can.

person carried object away trash can.
Figure 21: Language-inference examples continued.
675

fiYu, Siddharth, Barbu, & Siskind

backpack approached trash can.

chair approached trash can.

red object approached chair.

blue object approached chair.

person left trash put object.

person right trash put object.
Figure 21: Language-inference examples continued.
676

fiGrounding Language Inference, Generation, Acquisition Video

person put trash can.

person put backpack.

person picked object left trash can.

person picked object right trash can.

person picked object left trash can.

person picked object right trash can.
Figure 21: Language-inference examples continued.
677

fiYu, Siddharth, Barbu, & Siskind

person picked object.

person put object.

person picked object quickly.

person picked object slowly.

person carried object towards trash can.

person carried object away trash can.
Figure 21: Language-inference examples continued.
678

fiGrounding Language Inference, Generation, Acquisition Video

backpack approached trash can.

chair approached trash can.

red object approached chair.

blue object approached chair.

person put chair.

person put backpack.
Figure 21: Language-inference examples continued.
679

fiYu, Siddharth, Barbu, & Siskind

person carried red object.

person carried blue object.

person picked object left trash can.

person picked object right trash can.

person picked object.

person put object.
Figure 21: Language-inference examples continued.
680

fiGrounding Language Inference, Generation, Acquisition Video

person picked object quickly.

person picked object slowly.

person carried object towards trash can.

person carried object away trash can.
Figure 21: Language-inference examples continued.

681

fiYu, Siddharth, Barbu, & Siskind

backpack left chair approached trash can.

person right backpack carried chair.

person right trash approached trash can.

chair right person approached trash can.

backpack left trash approached trash can.
Figure 22: Sentential descriptions generated 94 video clips first
corpus subject contraction threshold 0.90. highest-scoring sentence clip
generated, among sentences generated grammar Table 11(a),
means beam search. sentences deemed human judges describe associated
clips indicated green, ones indicated red.

682

fiGrounding Language Inference, Generation, Acquisition Video

chair left trash approached trash can.

backpack right trash approached trash can.

backpack right trash approached trash can.

person left trash put chair.

backpack right person approached trash can.

person right chair put backpack.
Figure 22: Sentential-description examples continued.
683

fiYu, Siddharth, Barbu, & Siskind

chair left trash approached backpack.

trash right person approached chair.

person right chair put trash can.

person right chair put trash can.

person right chair approached trash can.

trash right chair approached chair.
Figure 22: Sentential-description examples continued.
684

fiGrounding Language Inference, Generation, Acquisition Video

backpack right chair approached chair.

person left trash picked chair.

person right chair picked backpack.

person right trash picked backpack.

person left chair picked backpack.

trash right person approached chair.
Figure 22: Sentential-description examples continued.
685

fiYu, Siddharth, Barbu, & Siskind

backpack left trash approached trash can.

person right chair put chair.

trash right chair approached person.

person right chair picked trash can.

person left trash picked chair.

backpack right chair approached trash can.
Figure 22: Sentential-description examples continued.
686

fiGrounding Language Inference, Generation, Acquisition Video

person right chair carried backpack.

chair left trash approached trash can.

person right chair approached chair.

backpack right person approached trash can.

person left trash approached trash can.

backpack right trash approached trash can.
Figure 22: Sentential-description examples continued.
687

fiYu, Siddharth, Barbu, & Siskind

backpack left chair approached chair.

trash right backpack approached chair.

trash right chair approached chair.

person right trash put chair.

person left chair put backpack.

chair right trash approached trash can.
Figure 22: Sentential-description examples continued.
688

fiGrounding Language Inference, Generation, Acquisition Video

trash left person approached backpack.

trash left chair approached backpack.

person left chair put backpack.

person left chair put backpack.

person left chair put backpack.

chair right backpack approached trash can.
Figure 22: Sentential-description examples continued.
689

fiYu, Siddharth, Barbu, & Siskind

trash left chair approached backpack.

trash left backpack approached backpack.

backpack left chair approached trash can.

person right trash picked chair.

person left trash picked trash can.

person left trash picked backpack.
Figure 22: Sentential-description examples continued.
690

fiGrounding Language Inference, Generation, Acquisition Video

person right chair picked backpack.

person right trash put backpack.

person left chair approached chair.

person right chair picked backpack.

person right chair picked backpack.

person right trash picked backpack.
Figure 22: Sentential-description examples continued.
691

fiYu, Siddharth, Barbu, & Siskind

person left backpack picked chair.

trash right chair approached chair.

person right trash carried backpack.

chair left trash approached trash can.

person left backpack approached trash can.

chair left backpack approached trash can.
Figure 22: Sentential-description examples continued.
692

fiGrounding Language Inference, Generation, Acquisition Video

trash right person approached chair.

backpack right trash approached trash can.

backpack left chair approached chair.

trash right backpack approached chair.

backpack left chair approached chair.

person right trash put chair.
Figure 22: Sentential-description examples continued.
693

fiYu, Siddharth, Barbu, & Siskind

person left chair put backpack.

person left trash put backpack.

person right chair put backpack.

person right chair put chair.

person right trash put chair.

backpack right trash approached chair.
Figure 22: Sentential-description examples continued.
694

fiGrounding Language Inference, Generation, Acquisition Video

person left backpack carried trash can.

backpack right chair approached chair.

person right chair approached trash can.

person right chair picked backpack.

person right trash picked backpack.

person left backpack picked backpack.
Figure 22: Sentential-description examples continued.
695

fiYu, Siddharth, Barbu, & Siskind

trash right chair approached person.

person right chair put backpack.

trash left person approached person.

person right chair picked backpack.

person left chair put trash can.

person left trash picked trash can.
Figure 22: Sentential-description examples continued.
696

fiGrounding Language Inference, Generation, Acquisition Video

person left trash picked backpack.

person left trash picked trash can.

backpack right chair approached trash can.

person right trash carried backpack.

chair left trash approached trash can.
Figure 22: Sentential-description examples continued.

697

fiYu, Siddharth, Barbu, & Siskind

stool
traffic cone
trash
chair
backpack
person

stool
traffic cone
trash
chair
backpack
person

detector index
first argument

stool
traffic cone
trash
chair
backpack
person

stool
traffic cone
trash
chair
backpack
person

detector index
first argument

stool
traffic cone
trash
chair
backpack
person

stool
traffic cone
trash
chair
backpack
person

detector index
first argument

stool
traffic cone
trash
chair
backpack
person

stool
traffic cone
trash
chair
backpack
person

detector index
first argument

stool
traffic cone
trash
chair
backpack
person

stool
traffic cone
trash
chair
backpack
person

detector index
first argument

stool
traffic cone
trash
chair
backpack
person

stool
traffic cone
trash
chair
backpack
person

detector index
first argument

698

1.0

0
0
0
0

1.00
1.00
1.00
1.00

1.0
1.0
1.0

stool
traffic cone

trained
hand-crafted
trained
hand-crafted

1.0

0
0
0
0

1.00
1.00
1.00
1.00

trained
hand-crafted
trained
hand-crafted

1.0
1.0
1.0

trash
chair

1.0

0
0
0
0

1.00
1.00
1.00
1.00

trained
hand-crafted
trained
hand-crafted

1.0
1.0
1.0

backpack
person

Figure 23: Comparison hand-crafted trained models nouns.

fiGrounding Language Inference, Generation, Acquisition Video

left
hand-crafted

right
trained

1.00

hand-crafted

1.00

1.00

0

0

1.00

0

1.0

1.0

trained

0

1.0

1.0

right

left

right

left

difference
x-position first
second arguments

right

left

right

left

difference
x-position first
second arguments

Figure 24: Comparison hand-crafted trained models spatial-relation prepositions.

699

fiYu, Siddharth, Barbu, & Siskind

approached

0.05
0.01
0.01

2

4
4
0

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

leftward

leftward

leftward

leftward

upward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

velocity orientation
first argument

downward

rightward

upward

downward

rightward

upward

downward

rightward

upward

downward

rightward

upward

downward

rightward

upward

downward

rightward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
first argument

upward

leftward

upward

leftward

upward

leftward

near

far

near

far

near

far

near

smaller

larger

smaller

larger

smaller

larger

smaller

larger

smaller

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

far

larger

far

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

upward

near

smaller

upward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

far

downward

rightward

downward

rightward

downward

rightward

downward

rightward

downward

rightward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

upward
rightward
downward

near

moderate distance

moderate distance

moderate distance

moderate distance

moderate distance

moderate distance

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

ratio size
velocity orientation distance first
first argument
second argument second arguments size second argument

larger

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
second argument

700

0.0

0.4
0.5
1.0

1.00
0.95
0.91
1.00
0.99
0.99

trained
0.09
hand-crafted

Figure 25: Comparison hand-crafted trained models verb approached.

fiGrounding Language Inference, Generation, Acquisition Video

carried

4

6
0

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

leftward

leftward

leftward

leftward

upward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

velocity orientation
first argument

downward

rightward

upward

downward

rightward

upward

downward

rightward

upward

downward

rightward

upward

downward

rightward

upward

downward

rightward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
first argument

upward

leftward

upward

leftward

upward

leftward

near

far

near

far

near

far

near

smaller

larger

smaller

larger

smaller

larger

smaller

larger

smaller

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

far

larger

far

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

upward

near

smaller

upward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

far

downward

rightward

downward

rightward

downward

rightward

downward

rightward

downward

rightward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

upward
rightward
downward

near

moderate distance

moderate distance

moderate distance

moderate distance

moderate distance

moderate distance

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

ratio size
velocity orientation distance first
first argument
second argument second arguments size second argument

larger

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
second argument

701

0.0

0.9
1.0

0.02
0.08
0.01
0.01

1.00
0.98
0.86
1.00
0.99
0.99

trained
0.05
hand-crafted

Figure 26: Comparison hand-crafted trained models verb carried.

fiYu, Siddharth, Barbu, & Siskind

picked

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

leftward

leftward

leftward

leftward

upward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

velocity orientation
first argument

downward

rightward

upward

downward

rightward

upward

downward

rightward

upward

downward

rightward

upward

downward

rightward

upward

downward

rightward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
first argument

upward

leftward

upward

leftward

upward

leftward

near

far

near

far

near

far

near

smaller

larger

smaller

larger

smaller

larger

smaller

larger

smaller

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

far

larger

far

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

upward

near

smaller

upward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

far

downward

rightward

downward

rightward

downward

rightward

downward

rightward

downward

rightward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

upward
rightward
downward

near

moderate distance

moderate distance

moderate distance

moderate distance

moderate distance

moderate distance

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

ratio size
velocity orientation distance first
first argument
second argument second arguments size second argument

larger

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
second argument

702

7

0
1

0.04
0.04
0.01
0.01

0.96
1.00
0.99

0.0

.93
.00

1.00
trained
0.96
hand-crafted
0.99

Figure 27: Comparison hand-crafted trained models verb picked up.

fiGrounding Language Inference, Generation, Acquisition Video

put

1.0

0
1.0

0

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

leftward

leftward

leftward

leftward

upward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

velocity orientation
first argument

downward

rightward

upward

downward

rightward

upward

downward

rightward

upward

downward

rightward

upward

downward

rightward

upward

downward

rightward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
first argument

upward

leftward

upward

leftward

upward

leftward

near

far

near

far

near

far

near

smaller

larger

smaller

larger

smaller

larger

smaller

larger

smaller

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

far

larger

far

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

upward

near

smaller

upward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

far

downward

rightward

downward

rightward

downward

rightward

downward

rightward

downward

rightward

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

upward
rightward
downward

near

moderate distance

moderate distance

moderate distance

moderate distance

moderate distance

moderate distance

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

leftward

ratio size
velocity orientation distance first
first argument
second argument second arguments size second argument

larger

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
second argument

703

0.06
0.01
0.01
0.01

1.00
0.94
0.97
1.00
0.99
0.99

trained
0.02
hand-crafted

Figure 28: Comparison hand-crafted trained models verb put down.

fiYu, Siddharth, Barbu, & Siskind

slowly

.00
1
.00
1

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
first argument

704

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

0
0

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
first argument

1.0
1.0

0.05
0.08
0.01
0.01

1.00
trained
0.95
0.92
1.00
hand-crafted
0.99
0.99

0.04
0.02
0.01
0.01

1.00
trained
0.96
0.98
1.00
hand-crafted
0.99
0.99

quickly

Figure 29: Comparison hand-crafted trained models adverbs.

fiGrounding Language Inference, Generation, Acquisition Video

towards

1.0

0
1.0

0

near

far

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

near

moderate distance

far

moderate distance

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

near

near

near

far
moderate distance

far
moderate distance

far
moderate distance

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

near

near

far

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

near

moderate distance

far

moderate distance

far

moderate distance

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

near

near

far

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

near

moderate distance

far

moderate distance

far

moderate distance

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
distance
first argument first second arguments

705

far

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

near

1
1

0.07
0.07
0.01
0.01

moderate distance

moving quickly
moving quickly
moving slowly
mostly stationary
absolutely stationary

velocity magnitude
distance
first argument first second arguments

.00
.00

1.00
trained
0.93
0.93
1.00
hand-crafted
0.99
0.99



1.00
0.01
0.01

1.00
0.94
1.00
0.99
0.99

trained
0.06
hand-crafted

away

Figure 30: Comparison hand-crafted trained models motion prepositions.

fiYu, Siddharth, Barbu, & Siskind

References
Aoun, J., & Sportiche, D. (1983). formal theory government. Linguistic Review,
2 (3), 211236.
Avidan, S. (2004). Support vector tracking. IEEE Transactions Pattern Analysis
Machine Intelligence, 26 (8), 10641072.
Baker, S., Scharstein, D., Lewis, J., Roth, S., Black, M. J., & Szeliski, R. (2011). database
evaluation methodology optical flow. International Journal Computer Vision, 92 (1), 131.
Barbu, A., Barrett, D. P., Chen, W., Siddharth, N., Xiong, C., Corso, J. J., Fellbaum,
C. D., Hanson, C., Hanson, S. J., Helie, S., Malaia, E., Pearlmutter, B. A., Siskind,
J. M., Talavage, T. M., & Wilbur, R. B. (2014). Seeing worse believing:
Reading peoples minds better computer-vision methods recognize actions.
Proceedings European Conference Computer Vision, pp. 612627.
Barbu, A., Bridge, A., Burchill, Z., Coroian, D., Dickinson, S., Fidler, S., Michaux, A.,
Mussman, S., Siddharth, N., Salvi, D., Schmidt, L., Shangguan, J., Siskind, J. M.,
Waggoner, J., Wang, S., Wei, J., Yin, Y., & Zhang, Z. (2012a). Video sentences
out. Proceedings Conference Uncertainty Artificial Intelligence, pp.
102112.
Barbu, A., Siddharth, N., Michaux, A., & Siskind, J. M. (2012b). Simultaneous object
detection, tracking, event recognition. Advances Cognitive Systems, 2, 203
220.
Barbu, A., Siddharth, N., & Siskind, J. M. (2014). Language-driven video retrieval.
Proceedings IEEE Conference Computer Vision Pattern Recognition
Workshop Vision Meets Cognition.
Baum, L. E. (1972). inequality associated maximization technique statistical
estimation probabilistic functions Markov process. Inequalities, 3, 18.
Baum, L. E., & Petrie, T. (1966). Statistical inference probabilistic functions finite
state Markov chains. Annals Mathematical Statistics, 37 (6), 15541563.
Baum, L. E., Petrie, T., Soules, G., & Weiss, N. (1970). maximization technique occuring
statistical analysis probabilistic functions Markov chains. Annals
Mathematical Statistics, 41 (1), 164171.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Berclaz, J., Fleuret, F., Turetken, E., & Fua, P. (2011). Multiple object tracking using
K-shortest paths optimization. IEEE Transactions Pattern Analysis Machine
Intelligence, 33 (9), 18061819.
Bilmes, J. (1998). gentle tutorial EM algorithm application parameter
estimation Gaussian mixture hidden Markov models. Tech. rep. TR-97-021,
ICSI.
Blank, M., Gorelick, L., Shechtman, E., Irani, M., & Basri, R. (2005). Actions space-time
shapes. Proceedings IEEE International Conference Computer Vision,
pp. 13951402.
706

fiGrounding Language Inference, Generation, Acquisition Video

Borschinger, B., Jones, B. K., & Johnson, M. (2011). Reducing grounded learning tasks
grammatical inference. Proceedings Conference Empirical Methods
Natural Language Processing, pp. 14161425.
Brand, M., Oliver, N., & Pentland, A. (1997). Coupled hidden Markov models complex
action recognition. Proceedings IEEE Conference Computer Vision
Pattern Recognition, pp. 994999.
Chen, C.-Y., & Grauman, K. (2013). Watching unlabeled videos helps learn new human
actions labeled snapshots. Proceedings IEEE Conference
Computer Vision Pattern Recognition, pp. 572579.
Chen, D. L., & Mooney, R. J. (2008). Learning sportscast: test grounded language
acquisition. Proceedings International Conference Machine Learning, pp.
128135.
Chen, D. L., & Mooney, R. J. (2011). Learning interpret natural language navigation
instructions observations. Proceedings Conference Artificial Intelligence, pp. 859865.
Chomsky, N. (1982). Concepts Consequences Theory Government
Binding. MIT Press.
Chomsky, N. (2002). Syntactic Structures (Second edition). Walter de Gruyter.
Das, P., Xu, C., Doell, R. F., & Corso, J. J. (2013). thousand frames words:
Lingual description videos latent topics sparse object stitching.
Proceedings IEEE Conference Computer Vision Pattern Recognition,
pp. 26342641.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incomplete
data via EM algorithm (with discussion). Journal Royal Statistical Society
B, 39 (1), 138.
Dinkelbach, W. (1967). nonlinear fractional programming. Management Science, 13 (7),
492498.
Dominey, P. F., & Boucher, J.-D. (2005). Learning talk events narrated video
construction grammar framework. Artificial Intelligence, 167 (1-2), 3161.
Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserman, A. (2010).
PASCAL Visual Object Classes (VOC) challenge. International Journal Computer
Vision, 88 (2), 303338.
Everts, I., van Gemert, J. C., & Gevers, T. (2013). Evaluation color stips human
action recognition. Proceedings IEEE Conference Computer Vision
Pattern Recognition, pp. 28502857.
Farhadi, A., Hejrati, M., Sadeghi, M., Young, P., Rashtchian, C., Hockenmaier, J., &
Forsyth, D. (2010). Every picture tells story: Generating sentences images.
Proceedings European Conference Computer Vision, pp. 1529.
Fellbaum, C. (1998). WordNet: electronic lexical database. MIT Press.
707

fiYu, Siddharth, Barbu, & Siskind

Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010a). Object
detection discriminatively trained part-based models. IEEE Transactions
Pattern Analysis Machine Intelligence, 32 (9), 16271645.
Felzenszwalb, P. F., Girshick, R. B., & McAllester, D. A. (2010b). Cascade object detection
deformable part models. Proceedings IEEE Conference Computer
Vision Pattern Recognition, pp. 22412248.
Feng, S. L., Manmatha, R., & Lavrenko, V. (2004). Multiple Bernoulli relevance models
image video annotation. Proceedings IEEE Conference Computer
Vision Pattern Recognition, pp. 10021009.
Fern, A. P., Givan, R. L., & Siskind, J. M. (2002a). Specific-to-general learning temporal
events. Proceedings Conference Artificial Intelligence, pp. 152158.
Fern, A. P., Givan, R. L., & Siskind, J. M. (2002b). Specific-to-general learning temporal
events application learning event definitions video. Journal Artificial
Intelligence Research, 17, 379449.
Fern, A. P., Siskind, J. M., & Givan, R. L. (2002c). Learning temporal, relational, forcedynamic event definitions video. Proceedings Conference Artificial
Intelligence, pp. 159166.
Fernandez Tena, C., Baiget, P., Roca, X., & Gonzalez, J. (2007). Natural language descriptions human behavior video sequences. Advances Artificial Intelligence,
pp. 279292.
Gaidon, A., Harchaoui, Z., & Schmid, C. (2014). Activity representation motion
hierarchies. International Journal Computer Vision, 107 (3), 219238.
Grimshaw, J. (1979). Complement selection lexicon. Linguistic Inquiry, 10 (2),
279326.
Grimshaw, J. (1981). Form, function, language acquisition device. Baker, C. L.,
& McCarthy, J. J. (Eds.), Logical Problem Language Acquisition, pp. 165182.
MIT Press.
Guadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S., Mooney, R., Darrell, T., & Saenko, K. (2013). Youtube2text: Recognizing describing arbitrary
activities using semantic hierarchies zero-shot recognition. Proceedings
IEEE International Conference Computer Vision, pp. 27122719.
Gupta, A., Verma, Y., & Jawahar, C. (2012). Choosing linguistics vision describe
images. Proceedings Conference Artificial Intelligence, pp. 606612.
Haegeman, L. (1992). Introduction government binding theory. Blackwell.
Hamming, R. W. (1950). Error detecting error correcting codes. Bell System Technical
Journal, 29 (2), 147160.
Han, M., Sethi, A., Hua, W., & Gong, Y. (2004). detection-based multiple object tracking
method. Proceedings IEEE International Conference Image Processing,
pp. 30653068.
708

fiGrounding Language Inference, Generation, Acquisition Video

Hanckmann, P., Schutte, K., & Burghouts, G. J. (2012). Automated textual descriptions
wide range video events 48 human actions. Proceedings European
Conference Computer Vision (Workshops Demonstrations), pp. 372380.
Ikizler-Cinbis, N., & Sclaroff, S. (2010). Object, scene actions: Combining multiple
features human action recognition. Proceedings European Conference
Computer Vision, pp. 494507.
Jackendoff, R. (1977). X-bar-syntax: study phrase structure. MIT Press.
Jensen, J. L. W. V. (1906). Sur les fonctions convexes et les inegalites entre les valeurs
moyennes. Acta Mathematica, 30 (1), 175193.
Jie, L., Caputo, B., & Ferrari, V. (2009). Whos what: Joint modeling names
verbs simultaneous face pose annotation. Proceedings Neural
Information Processing Systems Conference, pp. 11681176.
Khan, M. U. G., & Gotoh, Y. (2012). Describing video contents natural language.
Proceedings Workshop Innovative Hybrid Approaches Processing
Textual Data, pp. 2735.
Khan, M. U. G., Zhang, L., & Gotoh, Y. (2011). Human focused video description.
Proceedings IEEE International Conference Computer Vision (Workshops),
pp. 14801487.
Kim, J., & Mooney, R. J. (2010). Generative alignment semantic parsing learning ambiguous supervision. Proceedings International Conference
Computational Linguistics, pp. 543551.
Kim, J., & Mooney, R. J. (2012). Unsupervised PCFG induction grounded language
learning highly ambiguous supervision. Proceedings Conference
Empirical Methods Natural Language Processing, pp. 433444.
Kim, J., & Mooney, R. J. (2013). Adapting discriminative reranking grounded language
learning. Proceedings Annual Meeting Association Computational
Linguistics, pp. 218227.
Klein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. Proceedings
Annual Meeting Association Computational Linguistics, pp. 423430.
Kojima, A., Tamura, T., & Fukunaga, K. (2002). Natural language description human
activities video images based concept hierarchy actions. International
Journal Computer Vision, 50 (2), 171184.
Krishnamoorthy, N., Malkarnenkar, G., Mooney, R. J., Saenko, K., & Guadarrama, S.
(2013). Generating natural-language video descriptions using text-mined knowledge.
Proceedings NAACL HLT Workshop Vision Language, pp. 1019.
Krishnamurthy, J., & Kollar, T. (2013). Jointly learning parse perceive: Connecting
natural language physical world. Transactions Association Computational Linguistics, 1, 193206.
Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., & Serre, T. (2011). HMDB: large video
database human motion recognition. Proceedings IEEE International
Conference Computer Vision, pp. 25562563.
709

fiYu, Siddharth, Barbu, & Siskind

Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A. C., & Berg, T. L. (2011).
Baby talk: Understanding generating simple image descriptions. Proceedings
IEEE Conference Computer Vision Pattern Recognition, pp. 16011608.
Kuznetsova, P., Ordonez, V., Berg, A. C., Berg, T. L., & Choi, Y. (2012). Collective
generation natural image descriptions. Proceedings Annual Meeting
Association Computational Linguistics, pp. 359368.
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L., & Steedman, M. (2012). probabilistic
model syntactic semantic acquisition child-directed utterances
meanings. Proceedings Conference European Chapter Association
Computational Linguistics, pp. 234244.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., & Steedman, M. (2010). Inducing probabilistic CCG grammars logical form higher-order unification. Proceedings
Conference Empirical Methods Natural Language Processing, pp. 1223
1233.
Laptev, I. (2005). space-time interest points. International Journal Computer Vision,
64 (2/3), 107123.
Li, P., & Ma, J. (2011). happening still picture?. Proceedings Asian
Conference Pattern Recognition, pp. 3236.
Liu, J., Luo, J., & Shah, M. (2009). Recognizing realistic actions videos wild.
Proceedings IEEE Conference Computer Vision Pattern Recognition,
pp. 19962003.
Mann, R., Jepson, A. D., & Siskind, J. M. (1996). computational perception scene
dynamics. Proceedings European Conference Computer Vision, pp. 528
539.
Mann, R., Jepson, A. D., & Siskind, J. M. (1997). computational perception scene
dynamics. Computer Vision Image Understanding, 65 (2), 113128.
Marocco, D., Cangelosi, A., Fischer, K., & Belpaeme, T. (2010). Grounding action words
sensorimotor interaction world: experiments simulated iCub
humanoid robot. Frontiers Neurorobotics, 4 (7), 115.
Messing, R., Pal, C., & Kautz, H. (2009). Activity recognition using velocity histories
tracked keypoints. Proceedings IEEE International Conference Computer
Vision, pp. 104111.
Miller, G. A. (1995). WordNet: lexical database English. Communications
ACM, 38 (11), 3941.
Mitchell, M., Dodge, J., Goyal, A., Yamaguchi, K., Stratos, K., Han, X., Mensch, A., Berg,
A. C., Berg, T. L., & III, H. D. (2012). Midge: Generating image descriptions
computer vision detections. Proceedings Conference European Chapter
Association Computational Linguistics, pp. 747756.
Niebles, J. C., Chen, C.-W., & Fei-Fei, L. (2010). Modeling temporal structure decomposable motion segments activity classification. Proceedings European
Conference Computer Vision, pp. 392405.
710

fiGrounding Language Inference, Generation, Acquisition Video

Ordonez, V., Kulkarni, G., & Berg, T. L. (2011). Im2text: Describing images using 1 million
captioned photographs. Proceedings Neural Information Processing Systems
Conference, pp. 11431151.
Piantadosi, S. T., Goodman, N. D., Ellis, B. A., & Tenenbaum, J. B. (2008). Bayesian
model acquisition compositional semantics. Proceedings Annual
Conference Cognitive Science Society, pp. 16201625.
Pinker, S. (1984). Language Learnability Language Development. Harvard University
Press.
Pirsiavash, H., Ramanan, D., & Fowlkes, C. C. (2011). Globally-optimal greedy algorithms
tracking variable number objects. Proceedings IEEE Conference
Computer Vision Pattern Recognition, pp. 12011208.
Reddy, K. K., & Shah, M. (2013). Recognizing 50 human action categories web videos.
Machine Vision Applications, 24 (5), 971981.
Rodriguez, M. D., Ahmed, J., & Shah, M. (2008). Action MACH: spatio-temporal maximum average correlation height filter action recognition. Proceedings
IEEE Conference Computer Vision Pattern Recognition, pp. 18.
Rohrbach, M., Qin, W., Titov, I., Thater, S., Pinkal, M., & Schiele, B. (2013). Translating video content natural language descriptions. Proceedings IEEE
International Conference Computer Vision, pp. 433440.
Roy, D. (2002). Learning visually-grounded words syntax scene description task.
Computer Speech Language, 16 (3-4), 353385.
Roy, D. K., & Pentland, A. P. (2002). Learning words sights sounds: computational model. Cognitive Science, 26 (1), 113146.
Sadanand, S., & Corso, J. J. (2012). Action bank: high-level representation activity
video. Proceedings IEEE Conference Computer Vision Pattern
Recognition, pp. 12341241.
Schuldt, C., Laptev, I., & Caputo, B. (2004a). Recognizing human actions: local SVM
approach. Proceedings International Conference Pattern Recognition, pp.
3236.
Schuldt, C., Laptev, I., & Caputo, B. (2004b). Recognizing human actions: local svm
approach. Proceedings International Conference Pattern Recognition, pp.
3236.
Siddharth, N., Barbu, A., & Siskind, J. M. (2014). Seeing youre told: Sentence-guided
activity recognition video. Proceedings IEEE Conference Computer
Vision Pattern Recognition, pp. 732739.
Siskind, J. M. (1996). computational study cross-situational techniques learning
word-to-meaning mappings. Cognition, 61 (1-2), 3991.
Siskind, J. M. (2001). Grounding lexical semantics verbs visual perception using
force dynamics event logic. Journal Artificial Intelligence Research, 15, 3190.
Siskind, J. M., & Morris, Q. (1996). maximum-likelihood approach visual event classification. Proceedings European Conference Computer Vision, pp. 347360.
711

fiYu, Siddharth, Barbu, & Siskind

Siskind, J. M. (1999). Visual event perception. Proceedings NEC Research Symposium, pp. 91154.
Siskind, J. M. (2000). Visual event classification via force dynamics. Proceedings
Conference Artificial Intelligence, pp. 149155.
Siskind, J. M. (2003). Reconstructing force-dynamic models video sequences. Artificial
Intelligence, 151 (1-2), 91154.
Siskind, J. M., & Morris, Q. (1996). maximum-likelihood approach visual event classification. Proceedings European Conference Computer Vision, pp. 347360.
Smith, K., Smith, A. D. M., & Blythe, R. A. (2011). Cross-situational learning: experimental study word-learning mechanisms. Cognitive Science, 35 (3), 480498.
Smith, K., Smith, A. D. M., Blythe, R. A., & Vogt, P. (2006). Cross-situational learning: mathematical approach. Proceedings International Workshop
Emergence Evolution Linguistic Communication, pp. 3144.
Song, H. O., Zickler, S., Althoff, T., Girshick, R., Fritz, M., Geyer, C., Felzenszwalb, P.,
& Darrell, T. (2012). Sparselet models efficient multiclass object detection.
Proceedings European Conference Computer Vision, pp. 802815.
Song, Y., Morency, L.-P., & Davis, R. (2013). Action recognition hierarchical sequence
summarization. Proceedings IEEE Conference Computer Vision
Pattern Recognition, pp. 35623569.
Starner, T., Weaver, J., & Pentland, A. (1998). Real-time American Sign Language recognition using desk wearable computer based video. IEEE Transactions Pattern
Analysis Machine Intelligence, 20 (12), 13711375.
Tellex, S., Thaker, P., Joseph, J., & Roy, N. (2013). Learning perceptually grounded word
meanings unaligned parallel data. Machine Learning, 0, 117.
Thompson, C. A., & Mooney, R. J. (2003). Acquiring word-meaning mappings natural
language interfaces. Journal Artificial Intelligence Research, 18, 144.
Tian, Y., Sukthankar, R., & Shah, M. (2013a). Spatiotemporal deformable part models
action detection. Proceedings IEEE Conference Computer Vision
Pattern Recognition, pp. 26422649.
Tian, Y., Sukthankar, R., & Shah, M. (2013b). Spatiotemporal deformable part models
action detection. Proceedings IEEE Conference Computer Vision
Pattern Recognition, pp. 26422649.
Viterbi, A. J. (1967). Error bounds convolutional codes asymtotically optimum
decoding algorithm. IEEE Transactions Information Theory, 13 (2), 260267.
Wang, H., Klaser, A., Schmid, C., & Liu, C.-L. (2011). Action recognition dense trajectories. Proceedings IEEE Conference Computer Vision Pattern
Recognition, pp. 31693176.
Wang, H., Klaser, A., Schmid, C., & Liu, C.-L. (2013). Dense trajectories motion
boundary descriptors action recognition. International Journal Computer Vision, 103 (1), 6079.
712

fiGrounding Language Inference, Generation, Acquisition Video

Wang, H., & Schmid, C. (2013). Action recognition improved trajectories. Proceedings IEEE International Conference Computer Vision, pp. 35513558.
Wang, Z., Guan, G., Qiu, Y., Zhuo, L., & Feng, D. (2013). Semantic context based refinement news video annotation. Multimedia Tools Applications, 67 (3), 607627.
Werlberger, M., Pock, T., & Bischof, H. (2010). Motion estimation non-local total
variation regularization. Proceedings IEEE Conference Computer Vision
Pattern Recognition, pp. 24642471.
Wolf, J. K., Viterbi, A. M., & Dixon, G. S. (1989). Finding best set K paths
trellis application multitarget tracking. IEEE Transactions Aerospace
Electronic Systems, 25 (2), 287296.
Wu, B., & Nevatia, R. (2007). Detection tracking multiple, partially occluded humans
Bayesian combination edgelet based part detectors. International Journal
Computer Vision, 75 (2), 247266.
Yamoto, J., Ohya, J., & Ishii, K. (1992). Recognizing human action time-sequential images using hidden Markov model. Proceedings IEEE Conference Computer
Vision Pattern Recognition, pp. 379385.
Yang, Y., Teo, C. L., Daume III, H., & Aloimonos, Y. (2011). Corpus-guided sentence
generation natural images. Proceedings Conference Empirical Methods
Natural Language Processing, pp. 444454.
Yao, B. Z., Yang, X., Lin, L., Lee, M. W., & Zhu, S.-C. (2010). I2T: Image parsing text
description. Proceedings IEEE, 98 (8), 14851508.
Yilmaz, A., Javed, O., & Shah, M. (2006). Object tracking: survey. ACM Computing
Surveys, 38 (4), 145.
Yu, C., & Ballard, D. H. (2004). integration grounding language learning
objects. Proceedings Conference Artificial Intelligence, pp. 488493.
Yu, H., & Siskind, J. M. (2013). Grounded language learning video described
sentences. Proceedings Annual Meeting Association Computational
Linguistics, pp. 5363.
Yu, S.-Z. (2010). Hidden semi-Markov models. Artificial Intelligence, 174 (2), 215243.
Yuan, C., Li, X., Hu, W., Ling, H., & Maybank, S. (2013). 3D R transform spatiotemporal interest points action recognition. Proceedings IEEE Conference
Computer Vision Pattern Recognition, pp. 724730.
Zettlemoyer, L. S., & Collins, M. (2005). Learning map sentences logical form: Structured classification probabilistic categorial grammars. Proceedings Conference Uncertainty Artificial Intelligence, pp. 658666.
Zhong, S., & Ghosh, J. (2001). new formulation coupled hidden Markov models. Tech.
rep., Department Electrical Computer Engineering, University Texas
Austin.
Zhu, J., Wang, B., Yang, X., Zhang, W., & Tu, Z. (2013). Action recognition Actons.
Proceedings IEEE International Conference Computer Vision, pp. 3559
3566.
713

fiJournal Articial Intelligence Research 52 (2015) 235-286

Submitted 10/14; published 02/15

Lazy Model Expansion:
Interleaving Grounding Search

broes.decat@gmail.com

Broes De Cat

OM Partners, Belgium

Marc.Denecker@cs.kuleuven.be

Marc Denecker

Dept. Computer Science, KULeuven, Belgium

pstuckey@unimelb.edu.au

Peter Stuckey

National ICT Australia
Dept. Computing Information Systems
University Melbourne, Australia

Maurice Bruynooghe

Dept. Computer Science, KULeuven, Belgium

Maurice.Bruynooghe@cs.kuleuven.be

Abstract

Finding satisfying assignments variables involved set constraints
cast (bounded) model generation problem: search (bounded) models theory
logic. state-of-the-art approach bounded model generation rich knowledge representation languages like Answer Set Programming (ASP) FO() CSP
modeling language Zinc, ground-and-solve : reduce theory ground
propositional one apply search algorithm resulting theory.
important bottleneck blow-up size theory caused grounding
phase. Lazily grounding theory search way overcome bottleneck.
present theoretical framework implementation context FO()
knowledge representation language. Instead grounding parts theory, justications
derived parts it. Given partial assignment grounded part
theory valid justications formulas non-grounded part, justications
provide recipe construct complete assignment satises non-grounded part.
justication particular formula becomes invalid search, new one
derived; fails, formula split part grounded part
justied. Experimental results illustrate power generality approach.

1. Introduction
world lled combinatorial problems.

include important combinatorial

optimization tasks planning, scheduling rostering, combinatorics problems
extremal graph theory, countless puzzles games. Solving combinatorial problems
hard, methods know tackle involve kind search.
Various

declarative paradigms

developed solve problems.



approaches, objects attributes searched represented symbols,
constraints satised objects represented expressions symbols

c 2015 AI Access Foundation. rights reserved.


fiDe Cat, Denecker, Stuckey & Bruynooghe
declarative language. Solvers search values symbols satisfy
constraints. idea found elds Constraint Programming (CP) (Apt, 2003),
ASP (Marek & Truszczyski, 1999), SAT, Mixed Integer Programming (MIP), etc.
terminology logic, declarative method amounts expressing desired properties

logical theory. data particular problem instance
partial interpretation (or structure). solving process
apply model generation, specically model expansion (Mitchell & Ternovska, 2005),
problem class sentences

corresponds naturally

task nding structure expands input partial structure satises
theory. resulting structure solution problem. Model generation/expansion,
studied example eld Knowledge Representation (KR) (Baral, 2003), thus
analogous task solving constraint satisfaction problems, studied CP,
generating answer sets logic programs, studied ASP.
similarities areas go deeper extend level used techniques.

State-of-the-art approaches often follow two-phase solving methodology.



rst phase, input theory, rich language hand, reduced fragment
language supported search algorithm. second phase, search
algorithm applied reduced theory eectively search models. example,
model generation language MiniZinc (Nethercote et al., 2007) performed reducing ground language FlatZinc, search algorithms available. Similarly,
language

PC()

FO()

(Denecker & Ternovska, 2008) reduced propositional fragment

(see, e.g., Wittocx, Marin, & Denecker, 2010), ASP reduced propositional

ASP (see, e.g., Gebser, Schaub, & Thiele, 2007). reduced theory often ground
fragment language, refer resulting reduced theory
rst phase

grounding

grounding



phase (where quantiers instantiated elements

domain). elds, grounding also referred attening, unrolling, splitting
propositionalization. solving methodology generally referred

and-solve.

ground-

Grounding becomes bottleneck users turn applications large domains
complex constraints. Indeed, easy see grounding size FO formula
exponential nesting depth quantiers arity predicates polynomial
size universe discourse.

increasing number applications

size grounded theory large memory.



example, Son, Pontelli, Le (2014) discuss several ASP applications groundand-solve approach turns inadequate.
paper, present novel approach remedy bottleneck, called

lazy model

expansion, grounding generated lazily (on-the-y) search, instead upfront. approach works associating justications non-ground parts theory.
valid justication non-ground formula recipe expand partial structure
precise (partial) structure satises formula. course, crucial
recipe lot compact grounding formula. Given partial structure
valid justication non-ground formulas, (total) structure obtained
extending partial structure literals justications non-ground
formulas. Justications selected way total structure model
whole initial theory.

Consequently, model generation limited grounded

part theory; model found part, extended model

236

fiLazy Model Expansion: Interleaving Grounding Search
whole theory. However, new assignment model generation conict one
justications. case, alternative justication needs sought. none
found, associated formula split two parts, one part grounded one
part valid justication still available.

Example 1.1.

Consider

Sokoban

problem, planning problem robot

push blocks around 2-D grid arrange given goal conguration. constraint
move action target position
(at time

T)

pP

moved block

bB

currently

empty, expressed

(t, b, p) B P : move(b, p, t) empty(p, t).

(1)

known advance many time steps needed, one ideally wants assume
large even innite number steps. Using ground-and-solve, blows size
grounding. Incremental grounding, iteratively extending time domain
large enough allow plan, developed avoid blow-up context
planning (Gebser et al., 2008). approach general depend
presence one domain incrementally increased.
Returning example, instead grounding sentence (1), associate
justication, recipe satisfy it.

Make

move(b, p, t)

false

b, p







recipe. search nds model grounded part problem
conict recipe, model extended literals recipe obtain
model whole theory. However, search would decide move block

p1

b1

position

time t1 , conict created recipe. resolve it, instance sentence (1)

conict partial model search split sentence (1) replaced
equivalent sentences:

move(b1 , p1 , t1 ) empty(p1 , t1 )

(2)

(t, b, p) B P \(t1 , b1 , p1 ) : move(b, p, t) empty(p, t)

(3)

Sentence (2) grounded passed search component use check


empty(p1 , t1 )

holds.

Sentence (3) non-ground satised recipe

move(b, p, t) false except

move(b1 , p1 , t1 ).

search makes moves,

instances grounded, search nds partial plan problem hand.
literals recipe remaining non-ground formula making

move(b, p, t)

false instances sentence (1) grounded complete plan.
main contributions paper are:



theoretical framework

lazy model expansion.

aiming minimally instantiat-

ing quantied variables, paves way solution long-standing problem
handling quantiers search problems, encountered, e.g., elds ASP (Lefvre
& Nicolas, 2009) SAT Modulo Theories (Ge & de Moura, 2009). framework
also generalizes existing approaches related grounding bottleneck
incremental domain extension (Claessen & Srensson, 2003) lazy clause generation (Ohrimenko, Stuckey, & Codish, 2009).

237

fiDe Cat, Denecker, Stuckey & Bruynooghe


complete algorithm lazy model expansion logic

FO(ID ),

extension

rst-order logic (FO) inductive denitions (a language closely related ASP
shown Denecker et al., 2012). includes ecient algorithms derive consistent
sets justications maintain throughout changes partial structure
(e.g., search).



IDP

implementation extending

knowledge-base system (De Cat et al., 2014)

experiments illustrate power generality lazy grounding.
Lazy grounding new step ability solve complex combinatorial problems.
avoiding up-front grounding step previous approaches, lazy grounding ground
enough problem solve it. method developed logic

FO(ID ),



become clear, justications associated rules, rules similar
rules used ASP systems. Hence, discussed towards end paper,
framework algorithms applied also context ASP.
paper organized follows.

Section 2, necessary background nota-

tions introduced. Formal denitions lazy grounding

FO(ID )

presented

Section 3, followed presentation relevant algorithms heuristics Sections 4
5. Experimental evaluation provided Section 6, followed discussion related
future work conclusion. preliminary version paper appeared work
De Cat, Denecker, Stuckey (2012) De Cat (2014, ch. 7).

2. Preliminaries
section, provide necessary background logic
tasks model generation model expansion

FO(ID )

FO(ID ),

inference

ground-and-solve

approach model expansion.

2.1

FO(ID )

First, dene syntax semantics logic

FO(ID )

(Denecker & Ternovska, 2008),

extension rst-order logic (FO) inductive denitions. assume familiarity
FO. Without loss generality, limit

FO(ID )

function-free fragment. Function

symbols always eliminated using graph predicates (Enderton, 2001).
(function-free) vocabulary



consists set predicate symbols.

Propositional

> , denoting true
false respectively. Predicate symbols usually denoted P , Q, R; atoms a, literals
(atoms negation) l; variables x, ; domain elements d. e
denote ordered set objects e1 , . . . , en ; P/n predicate P arity n.

symbols 0-ary predicate symbols; include symbols

methods model generating developed require (possibly innite)
domain



, domain atom

P P/n Dn , n-tuple domain elements. Likewise,

given xed. Given (function-free) vocabulary

atom form
consider

domain literals.

consists domain n-ary relation P Dn
predicate symbols P/n . Alternatively, n-ary relation viewed
n
function {t, f }. propositional symbols > respectively interpreted
f .
structure



interpreting

238

fiLazy Model Expansion: Interleaving Grounding Search
Model generation algorithms maintain


inconsistent

partial

structures may (temporarily) nd

state, example conict arises. represent

introduced; consist domain
n-ary predicate P , three- four-valued relation P .
n
function {t, f , u, i}. structure two-valued range relations {t, f },
partial three-valued range {t, f , u}) four-valued general. Thus, two-valued
states, three-valued four-valued structures



and,

structures also three-valued four-valued.

unqualied, term

structure

stands general, four-valued case.
Given xed





, alternative way represent

domain

aI

=t







=f







a, aI = (inconsistent)
aI = u (unknown)

domain atom


,

domain literals.
-structures
,

set

Indeed, one-to-one correspondence sets

otherwise. Hence,

may treat four-valued structures sets domain literals vice versa. structure

inconsistent least one domain atom inconsistent.

vocabulary naturally viewed structure larger
, namely setting aI = u domain atom predicate 0 \ .
set predicate symbols, use I| denote restriction symbols
I|

. set domain atoms, use I|S denote restriction : =
I|
= u otherwise. call two-valued structure two-valued
domain atoms unknown otherwise.
1 truth value v dened follows: t1 = f , f 1 = t, u1 = u
inverse v
1
= i. truth order >t truth values dened >t u >t f >t >t f .
precision order >p dened >p >p u >p f >p u. orders pointwise
0
extended arbitrary -structures. say ' expansion p ,
0


domain atom a,
p . Viewing structures sets domain literals,
0
corresponds .
structure

0
vocabulary

assume familiarity syntax (function-free) FO. facilitate reasoning
partially grounded formulas, deviate standard FO quantify explicitly

x D0 : x D0 : ,
D. sometimes abbreviate x1 D1 : . . . xn Dn : x : , similarly
. Given formula , [x] indicates x free variables . Substitution
variable x formula term denoted [x/t]. ground formula (in domain )
specied subsets domain

D.

denoted

D0

formula without variables (hence without quantiers). Similar properties notations
used

rules

(introduced below).

voc(T ) set predicate symbols occur theory .
structure , voc(I) set symbols interpreted . Unless specied otherwise,
theories structures range vocabulary .
language FO(ID ) extends FO (inductive) denitions. theory FO(ID )
(nite) set sentences denitions. denition (nite) set rules
form x : P (x1 , . . . , xn ) , P predicate symbol FO formula.
atom P (x) referred head rule body. Given rule r , let
head (r) body(r) denote respectively head body r. Given denition ,
domain atom P dened exists rule x : P (x)



. Otherwise P open . domain literal P dened P
denote

239

fiDe Cat, Denecker, Stuckey & Bruynooghe
. sets dened open domain atoms denoted defined ()
open(), respectively.

dened


Without loss generality, assume denition domain atom dened
one rule. Technically, means rules

P (x) 2

x D1 : P (x) 1 , x D2 :

D1 D2 = . Rules always made disjunct
x D1 D2 : P (x) 1 2 , x D1 \ D2 : P (x) 1 ,

pairwise disjunct,

transforming

x D2 \ D1 : P (x) 2 .
2.1.1 Model Semantics
semantics

FO(ID )

two-valued model semantics.

Nevertheless, introduce

concepts three- four-valued semantics useful dening semantics
denitions formalizing lazy grounding.

use standard four-valued truth

assignment function, dened structural induction pairs FO domain formulas



interpret :


P = P (d ),

structures

( )I = min<t ( , ),
( )I = max<t ( , ),
()I = ( )1 ,
(x : )I = max<t ({[x/d]I | D}),
(x : )I = min<t ({[x/d]I | D}).
assignment function monotonic precision order:

p 0 ,



p

0

.

Hence, formula true partial structure, true two-valued expansions
it.

Also,



two-valued (respectively three-valued, four-valued)



two-valued



two-valued

(respectively three-valued, four-valued).
structure


= t.





model

/

satises

sentence



(notation

|= )



satisfaction relation dened denitions well. semantics

denitions based parametrized well-founded semantics, extension wellfounded semantics logic programs informally described rst work Van Gelder
(1993), formally dened

FO(ID )'s

denitions Denecker (2000). semantics

formalizes informal semantics rule sets (inductive) denitions (Denecker, 1998;
structure
(notation |= ) two-valued wellfounded model denoted wf ( I|open() ) structure I|open() (Denecker
& Ternovska, 2008). case wf ( I|open() ) two-valued, model expanding
I|open() . structure satises theory two-valued model
sentences denitions . next subsection, present formalization
Denecker, Bruynooghe, & Marek, 2001; Denecker & Vennekens, 2014).


model

/

satises

denition

well-founded semantics using notion
According

FO(ID )'s

justication.

methodology, (formal) denitions used express informal

denitions. work Denecker Vennekens (2014), shown

FO(ID )

de-

nitions oer uniform representation important types informal denitions

240

fiLazy Model Expansion: Interleaving Grounding Search
expressing informal denitions leads rule sets



called

total

well-founded model



two-valued (Denecker & Ternovska, 2008).

total.

Formally, denition

two-valued structure



open()



general, totality undecidable; however

broad, syntactically dened classes denitions proven total (e.g., nonrecursive, positive, stratied locally stratied denitions, see Denecker & Ternovska,
2008). Inspection current

FO(ID )

applications shows practice, non-total deni-

tions occur rarely almost always contain modeling error. Also, cases totality
established simple syntactic check.

Totality usefully exploited

computation. lazy grounding techniques introduced exploit totality
applied total denitions. restriction matches

FO(ID )'s

methodology and, practice, impose strong limitation.

design

case

input theory contain denitions known total, lost:
denitions grounded completely up-front, case lazy grounding applied
safely remaining sentences total denitions input.

Equivalence.
equivalent

Two theories





0,

dierent vocabularies,

-

0
expanded model vice

restricted
0 strongly -equivalent expansions also
unique. extension, (strong) -equivalence structure dened similarly:
0
model expanding expanded model expanding vice versa;
obtain strong equivalence, expansions unique. theory , often
0
derive strongly voc(T )-equivalent theory given structure . transformations
0
preserve satisability number models model directly mapped
model projection voc(T ).
versa.

model

Two theories



Canonical theories.



simplify presentation, lazy grounding techniques

presented theories form

{PT , },

single denition function-free rules.



PT

propositional symbol,

without loss generality.



First,

mentioned above, standard techniques (Enderton, 2001) allow one make theory functionfree. Second, multiple denitions always combined one described Denecker
Ternovska (2008) Marin, Gilis, Denecker (2004). achieved renaming
dened predicates denitions, merging rules one set adding
equivalence constraints predicates renamings.

{1 , . . . , n , }

equivalent theory

{PT , {PT 1 n }}



PT

=
voc(T )-

Third, theory

resulting previous step translated strongly

new propositional symbol.

transformation results ground set sentences denition consisting
set (ground non-ground) rules, lazy grounding cope non-ground
rules. Furthermore, assume rule bodies negation normal form (negation
occurs front atoms) that, dened domain atom
rule

x : P (x)



dD


P ,

unique

.

methods proposed extended full

FO(ID )

functions,

extended methods implemented system. However, introduces number
rather irrelevant technicalities want avoid here.

241

fiDe Cat, Denecker, Stuckey & Bruynooghe
2.1.2 Justifications

canonical theory = {, } explained
correspond one-to-one sets domain literals.

Denition 2.1 (Direct
justication). direct justication dened domain literal P

(respectively P ) consistent non-empty set domain literals that,

rule x : P (x) , holds [x/d] = (respectively

[x/d] = f ).

0
consistent superset direct justication P direct justication
0
well. Indeed, body [x/d] true true precise . Also, direct
justication empty denition; true every structure, minimal
direct justication {>}.

assume presence domain

above. Recall, structures domain

Example 2.2.

Consider domain



direct justication

= {d1 , . . . , dn }

denition

x : P (x) Q(x) R(x)
x : Q(x) P (x)

Q(di )



{P (di )}



Q(di )







{P (di )}.

domain literals

many direct justications, unique minimal ones

P (di ) {Q(di )} {R(di )}
P (di ) {Q(di ), R(di )}. Atoms R(di ) open

subset relation. Minimal direct justications
minimal direct justication
direct justication.

G pair hV, Ei set V nodes set E directed
(vi , vj ) nodes. node v V , denote G(v)
G(v) = {w | (v, w) E}.

(directed) graph
i.e., ordered pairs
children

v,

i.e.,

Denition 2.3 (Justication).
domain literals





justication

denition

domain literal

l, J(l)



graph

J

edges,
set

set

either empty direct

justication l.
Thus, justication graph encodes every dened domain literal none one
direct justication. sequel say
denoted set pairs

l S,





J



dened l



J(l) 6= .

justication

direct justication l.

Denition 2.4 (Justication subgraph).

Let

J

justication

.

justication

literal l subgraph Jl nodes edges J reachable l. justication
set literals L subgraph JL nodes edges J reachable l L.
justication J total l J dened literal reachable
l dened ; total set literals L total literal L.
justication J consistent structure consistent none literals



J
J

dened false

I.

total l, leaves

Jl

open domain literals.

242

fiLazy Model Expansion: Interleaving Grounding Search
Denition 2.5.
li li+1 ,

positive literals;

cycle

J

path justication

sequence

edge li li+1

negative

justication

J

J.

l0 l1 . . .

path

positive

consists negative literals;

set domain literals path

J

that,

consists

mixed

otherwise.

starts ends

domain literal. cycle positive (respectively, negative) domain literals
positive literals (respectively, negative literals); otherwise cycle mixed.
innite path may cyclic not.
Intuitively, justication
truth

l.

J



nite, every innite path cyclic.

containing domain literal

l

provides argument

strength argument depends truth leaves

innite paths cycles
provides argument

Jl .
l

leaves true every innite path negative,
true. leaf false unknown,

mixed loop, argument

l

Jl

Jl

contains positive

weak. Notice justications

l

may still

argue l's truth.

Denition 2.6 (Justies).

l well-founded justication
J every innite path Jl negative. Otherwise l unfounded J .
justication J justies set literals L dened (the set L literals
justication J ) (i) JL total L; (ii) literal L well-founded J ; (iii)
set literals JL consistent.

P (d)

Q(d)

say dened literal

P (d)

R(d)

P (d)

Q(d)

P (d)

Q(d)

R(d)

Q(d)
(i)

(ii)

(iii)

Figure 1: Justications denition

Example 2.7.



P (d)

D.

Example 2.2 contain dened domain atoms

Justication (ii) justies

(iii), however, total


Example 2.2,

Figure 1, show possible justications (ordered (i)-(iv) left

right) denition

Q(d) (d D).



(iv)



P (d)



P (d)
Q(d)



Q(d)

(iv) justies

P (d)

P (d)
Q(d);



(i) positive cycle unfounded

Q(d).

relationship justications well-founded semantics investigated dierent publications (Denecker & De Schreye, 1993, 1992; Marin, 2009).
recall results paper relies. rst result states
literals


L

L,
JL .

model







leaves

JL

J

justies

true, satises literals



Proposition 2.8. J justication justies set domain literals L
literals JL true every model (open) leaves JL true.
243

fiDe Cat, Denecker, Stuckey & Bruynooghe
interpretation Iopen two-valued open(), well-founded model
wf (Iopen ) computed time polynomial size domain, shown Chen
Warren (1996). general, wf (Iopen ) three-valued structure. wf (Iopen ) twovalued, unique model expands Iopen ; otherwise, model
expands Iopen . proposition follows fact justication J justies
L leaves J true Iopen , literals L true wf (Iopen ).

Example 2.9

.



R(d)

true



R(d)

Justication (ii) justies L = {Q(d)}
Iopen interpreting open predicates ,
wf (Iopen ). particular, model

(Continued Example 2.7)

unique open leaf


R(d).

structure

Iopen , Q(d)
true, Q(d) true.

true

Proposition 2.10. model , justication J exists consists
literals true , dened dened domain literals true justies
them.

Corollary 2.11. case

total, justication J justies set domain
literals L, every two-valued open()-structure consistent JL extended
unique way model satises literals L.
{PT , }
justies PT .

Hence, canonical theory
justication

J

exists

(recall,



total), theory satisable

2.2 Generating Models
Model generation
model

T.

inference task takes input theory



returns output

Model Expansion (MX) dened Mitchell et al. (2006) inference

task takes input theory
subvocabulary

,



vocabulary

returns expansion










two-valued structure
satises

T.



Here,

general inference problem dened Wittocx, Marin, Denecker (2008)
takes input (potentially partial) structure
satises





,

returns expansion







T.

already mentioned, state-of-the-art approach model expansion
(similar ASP) grounding



context



FO(ID )

afterwards applying search

resulting ground theory. latter can, e.g., accomplished SAT(ID) search
algorithm (Marin et al., 2008).
Below, present grounding algorithm basis lazy MX algorithm.
assume familiarity basic Conict-Driven Clause-Learning (CDCL) algorithm
SAT solvers (Marques Silva, Lynce, & Malik, 2009).

2.2.1 Grounding
overview intelligent grounding techniques

FO(ID ),

refer reader

work Wittocx, Denecker, Bruynooghe (2013) Wittocx et al. (2010).
present basic principle.

vocabulary , partial structure
, returns ground theory 0 strongly

grounder takes input theory
domain

D,

interpreting least

>



244

fiLazy Model Expansion: Interleaving Grounding Search
-equivalent



assume







I.

Theory

T0

called

canonical theory form

grounding





given

I.

Recall

{PT , }.

One way compute grounding using top-down process theory, iteratively
applying grounding steps direct subformulas rule formula hand. grounding
Let [x]
let domains x. Tseitin transformation replaces
1
atom (x), new |x|-ary predicate symbol called Tseitin symbol, extends
rule x : (x) . new theory strongly -equivalent

algorithm may replace subformulas new predicate symbols follows.
formula

original one (Vennekens et al., 2007).
procedure one_step_ground, outlined Figure 1, performs one step grounding process. Called formula rule



canonical form, algorithm replaces

G
(rules formulas) possibly non-ground part R (rules). formula, G
consists ground formulas. Replacing returned ground formulas extending
returned rules produces theory strongly voc(T )-equivalent original.
rule , G consists ground rules, replacing sets returned
rules results theory strongly voc(T )-equivalent original.
direct subformulas Tseitin symbols returns pair consisting ground part

Algorithm 1: one_step_ground algorithm.
1 Function one_step_ground (formula rule )
2
switch
3
case []P return h{}, i;
4
case P
5
6
7
8
9
10
11
12
13
14
15
16

hG,

:= one_step_ground( );

return h{P gG g}, i;
case 1 . . . W
n
return h{ i[1,n] Ti }, {Ti | [1, n]}i;
case 1 . . . n
return h{Ti | [1, n]}, {Ti | [1, n]}i;
case x : P (x)
return h, {P (x)[x/d] [x/d] | D}i;
case x :W[x]
return h{ dD T[x/d] }, {T[x/d] [x/d] | D}i;
case x : [x]
return h{T[x/d] | D}, {T[x/d] [x/d] | D}i;


V

Grounding theory boils applying one_step_ground sentence
(which copies

PT

PT

ground part) rule theory repeatedly applying

one_step_ground returned rules

R (all returned sentences rules G ground).

use ground refer algorithm overall process.
1. Tseitin (1968) introduced symbols part normal form transformation.

245

fiDe Cat, Denecker, Stuckey & Bruynooghe
Various improvements exist, returning



returning

>/

atoms interpreted





conjunctions whenever false conjunct encountered (analogously

disjunctions quantications).
Also, algorithm one_step_ground introduces large number Tseitin symbols. Stateof-the-art grounding algorithms use number optimizations reduce number
symbols. optimizations directly applicable techniques presented
paper, start naive one_step_ground algorithm. Section 5, present
optimized version one_step_ground introduces fewer Tseitin symbols hence
results smaller groundings.

3. Lazy Grounding Lazy Model Expansion
lazy grounding refer process partially grounding theory
lazy model expansion (lazy MX) process interleaves lazy grounding

use term
term

model expansion grounded part. Section 3.1, formalize framework
lazy model expansion

FO(ID )

theories; Section 3.2, formalize instance

framework basis current implementation; Section 3.3, illustrate
operation.

3.1 Lazy Model Expansion FO(ID) Theories
Given canonical theory

= {PT , }

input structure

Iin ,

models expanding

Iin

searched interleaving lazy grounding search already grounded part.
rst focus lazy grounding.
Apart initial step moves

PT

grounded part, input step

consists set rules still grounded, already grounded theory three-valued
structure expansion initial input structure.
subsequent grounding step replace non-ground rules ground rules might
introduce new rules. Hence, state grounding includes set
set



(the

delayed denition )

g

ground rules

(possibly) non-ground rules. denitions

g (in follows abbreviated gd ) voc()-equivalent
original denition hence, gd total. grounding procedure guarantee
that, times, g total.
Given partial structure Iin rule sets g , key idea behind lazy
model expansion (i) use search algorithm search model g
expansion Iin PT true; (ii) maintain justication J literals
true dened justied gd J consistent ; (iii)
interleave steps (i) (ii) move parts g literal dened
property

needs justied cannot justied.

hg , , J, Ii
yet grounded, justication J ,
, g = , J empty graph.

Thus, control lazy model expansion, suces maintain state
consisting grounded rules
three-valued structure

I.

g ,

Initially,

rules





Iin ,

Lazy model expansion searches space

Denition 3.1 (Acceptable state).
tence

acceptable

states.

tuple hg , , J, Ii theory atomic senPT , total denition , input structure Iin acceptable state (i) gd , g

246

fiLazy Model Expansion: Interleaving Grounding Search




gd strongly voc()-equivalent , (ii) domain
, (iii) J justication gd , (iv) expansion
L literals true dened justied J , (vi) JL ,
literals L, consistent .

total denitions

atom dened


v

Iin ,

( ) set

justication

Example 3.2.

g



Consider theory

{PT , },





denition



PT T1 T2 T3 .



x : Q(x).
1

T2 x : R(x).




T3 x : Q(x).
Let



structure

{PT , T1 }

(hence,

T2



T3













unknown),

g







denitions consisting rst rule remaining rules, respectively. Furthermore, let

J {T1 {Q(d) | D}}. tuple hg , , J, Ii acceptable
T1 literal dened justied J .

state. Indeed,

already said, lazy model expansion algorithm starts initial state

, = , J = , = Iin ,

acceptable dened literals unknown

state, either renes



propagation choice, backjumps.

g =

Iin .



resulting

state unacceptable, repair operation restores acceptability; steps described
Section 3.2.


gd .

algorithm tries compute acceptable state



Corollary 2.11, would entail model

PT

justied

exists; computed

eciently well-founded model computation. intermediate states, justication
may non-total

PT ,

iii),

Note that, (
justied

.

contain unfounded literals, inconsistent.

justication must

gd .

Indeed, assume literal

justication graph leaf dened

g

l



depends positively negatively l. every attempt extend justication graph

l gd might fail, e.g., forbidden
cycle. Consider, e.g., denitions g = {P Q} = {Q P }. case,
would correct take P justication Q true, even though valid
justication within . Indeed, model exists justies Q full denition gd .
total justication graph justies

Proposition 3.3. Let hg , , J, Ii acceptable state.

gd well-founded model
expands literals true dened (delayed) denition .

Proof.

L

Let

set literals true

justies literals
expands

L.



dened

open literals



state acceptable,

J

Hence, Corollary 2.11, exists well-founded model

L.

Example 3.4 (Continued Example 3.2).
>

.



model

J

(i.e.,

g , PT

interpreted randomly,

well-founded evaluation, assigning

{Q(d) | D}),

derives

T1

true.

Moreover,

also true well-founded model. Note

R-atoms

occur





J.

following theorem states obtained expansion also model

247

T.

R



fiDe Cat, Denecker, Stuckey & Bruynooghe
Theorem 3.5. Let hg , , J, Ii acceptable state theory = (PT , ) input
structure Iin PT true I|voc(g ) model g . exists
model expands I|voc(g ) .
Proof. I|voc(g )
justication

Jg

model


g

domain literals true
combine one

follows Proposition 2.10 exists

I|voc(g ) .

g

consists

two justications:

follows: dened literal

l



gd ,



J

J



Jg .



l,
Jg

dened

Jc (l) = Jg (l). Jc takes edges either J
dened literal, justication gd .
verify Jc justies PT . First, total PT . Indeed, path PT either
consists literals dened g , branch total Jg g , passes
0
literal l dened , justied J according condition (v) hence
(Jc )l0 = Jl0 total. such, PT cannot reach dened literal gd
Jc undened. Second, Jc contain unfounded literals starting PT .
path PT either path Jg (so well-founded justies g )
tail J (well-founded property (v)). Finally, set literals reachable PT
Jc consistent. Also see look paths Jc PT : rst follow
Jg consists true literals , may get path J contains
literals consistent . case, impossible reach literal
set

Jc (l) = J(l);

Jc

g .

justies every true dened literal

otherwise, set

negation.
follows Proposition 2.8 exists model


PT

true. Since

gd

expands

I|voc(g )

gd strongly equivalent , proposition follows.

achieved well-founded evalstarting two-valued open(gd )-

Recall eectively computing model
uation

gd ,

polynomial data complexity,

structure expanding

I|voc(g )

(Chen & Warren, 1996).

theorem, required
compute two-valued model
justication justies

PT .

g .



model

g .

Actually, need

suces search partial structure

So, relax requirement expense also

maintaining justications literals true



dened

g .

Corollary 3.6. Let hg , , J, Ii acceptable state theory

= {PT , } input

structure Iin PT true J justies PT gd . exists
model expands I|S set dened literals JPT .
g expanding Iin PT true implies lack models
g model expanding Iin , unsatisable
core, i.e., set rules g model exists expands Iin . Hence, also
unsatisable core = (PT , ). nd unsatisable core, one can, example,
Failure nd model





expanding

Iin .

Indeed,

use techniques described Torlak, Chang, Jackson (2008).

3.2 Practical Justication Management FO(ID) Theories
Roughly speaking, lazy model expansion framework consists two components.
one hand, standard model expansion algorithm operates

{PT , g } and,
gd lazily

hand, justication manager maintains justication

248



fiLazy Model Expansion: Interleaving Grounding Search
grounds

.

Lazy model expansion performs search space acceptable states

aims reaching state Theorem 3.5 (or Corollary 3.6) applicable. avoid slowing
search model expansion, work done justication manager
lazy grounding must limited. achieve this, designed system
justication manager access grounded denition

g

need restore

state search algorithm backtracks current structure
manager access



g

particular, literal dened

I.

justication

maintains justications restricted

.



allowed direct justication. justication

manager maintains following properties:



Literals direct justications either open



direct justications
structure



J

gd

dened

.

kept consistent current

I.

justication graph dened

J

unfounded literals total.

distinguish acceptable states meet additional requirements acceptable
states dened Denition 3.1, call

Denition 3.7

(Default acceptable state)

.

default acceptable states ; dene as:

state



hg , , J, Ii

default acceptable

state acceptable state and, addition, ( ) literals direct justications either
open

gd

dened

,

ii

( )

J

justies set literals

J

dened.

follows default acceptable states satisfy two extra conditions: justify
literals dened
consistent.



terms literals dened

g ,

dened

,

consistent.

J



true



set literals

acceptable state, suces literals

J

Since default acceptable states acceptable states,

Theorem 3.5 Corollary 3.6 also hold default acceptable states.
standard model expansion, main state-changing operations make



precise (by making literals true, either choice propagation) make



less precise (by backjumping).

model expansion modies






',

= hg , , J, Ii default acceptable state
0
new state hg , , J, necessarily

default acceptable state. following propositions identify situations acceptability
preserved.

Proposition 3.8. Let hg , , J, Ii default acceptable state, L set literals unknown
0 consistent structure L. (i) literals L either dened
direct justication J (ii) direct justication J contains negation
literal L, hg , , J, 0 default acceptable state.
Proof.



literals true

( ) literals true


J

consistent

literals true

I'

I'

I,



direct justication, follows
direct justication. justications
also consistent '. Hence, J justies

dened

dened

ii

then, ( ),

dened

.

Proposition 3.9. Let hg , , J, Ii default acceptable state. hg , , J, 0
0 <p default acceptable state.

249

fiDe Cat, Denecker, Stuckey & Bruynooghe
Proof.


I, J

J

justication

justies literals dened

justies literals dened



true

default acceptable state, literals dened
literals dened
hidden loops


g



,

.

gd .



true

I.



I'

subset

'.

g

allowed direct justications

restriction quite limiting (see next section) avoid

loops detected maintaining justication

current implementation do. Several methods exist

l dened g allowed direct
, provided established l's justication cannot loop gd .
One case body rule l dened literals. step analyze
dependency graph: literal dened g allowed direct justication
literal dened provided literals belong strongly connected

extend class default acceptable states. Literals
justications

component dependency graph. case, cannot part cycle.

3.3 Example
rest section, illustrate behavior lazy model expansion articial
example, constructed way main features illustrated. next section,
processes involved described detail.
focus operation justication manager interaction
solving process. manager activated unacceptable state, either solver
falsies literal occurs direct justication

J

l dened
l extend

true literal

justied J . One option repair search justication
J . general problem hard model expansion problem itself,
Corollary 2.11. manager searches
extend

J,

nd one, grounds l's denition moves

example uses theory



shown

locally direct justication justies l
g .

states symmetric graph (edge/2) exists

R/1)
= {d1 , . . . , dn }
equality predicate identity relation (below omitted ). Predicates edge, R
root interpreted; R root dened. particular, root dened
singleton {d1 }, specifying root d1 .

least one node root node (predicate
root node. input structure

P












PT
C1
C2
x : root(x)
x : R(x)



root/1)

reachable (predicate

interprets domain

C1 C2
x : root(x) R(x)
(x y) D2 : edge(x, y) edge(y, x)
x = d1
root(x) : edge(x, y) R(y)

(1)
(2)
(3)
(4)
(5)













lazy MX algorithm proceeds follows:
1. initial default acceptable state


hg , , J, Ii



g ,



J

empty,

= .

2. Propagation

{PT , g }

sets





{PT }.

expands structure

conditions Proposition 3.8 longer satised.

250

I,



resulting state

fiLazy Model Expansion: Interleaving Grounding Search
acceptable since

J.

PT

true dened



direct justication

J direct justication PT .
atom PT unique direct justication {C1 , C2 } extending J
restore (default) acceptability since C1 , C2 direct justication J
PT remains unjustied. Therefore, alternative taken rule (1) moved
g . Now, default acceptable state obtained.
One option repair acceptability extend

{PT , C1 , C2 }. C1 C2 justied. Consider
C2 rule (3). edge open, manager build direct justication
{edge(d, d0 ) | (d, d0 ) D2 }, sets negative edge literals true, extends J
(setting positive edge literals true would equally good). justies C2
avoids grounding rule dening C2 .

3. Unit propagation sets
rst

4. Literal

C1

cannot justied (with local approach) since direct jus-

tications contains unjustied dened literals.

However, rule (2) existentially

quantied, one avoid grounding whole rule performing Tseitin transformation isolate one instance ground instance. purpose

d1 :

(root(d1 ) R(d1 ))
(2a)
x \ {d1 } : root(x) R(x) (2b)

illustration, make (bad) choice instantiating



C1


Rule (2a) moved

g

x



default acceptable state reached.

5. acceptable state propagation possible, choice
made.

C1

true, body rule (2a) become true. Preferably

selecting Tseitin (this would trigger grounding), rst disjunct selected
model expansion propagation extends structure
literal

root(d1 )

denition
dening

root

root


{(d1 = d1 )}

dened

unique direct justication

root(d1 ) R(d1 ).

rule (4) cannot justied since
false. manager partially grounds

splits ground rule (4a) non-ground rule (4b)

domain elements:



root(d1 ) d1 = d1 (4a)
x \ {d1 } : root(x) x = d1 (4b)



g . Note root(d1 ) justied {d1 = d1 } gd , hence
root(d1 ) direct justications . Whenever grounding

Rule (4a) moved
safe use

done, justication manager interrupted propagation, infer truth
additional literals, detect inconsistency (which result backjumping).
cases, manager resume revision justication afterwards,
acceptable state reached.
unacceptable (due unjustied

Here, even though resulting state still

R(d1 )),

g
root(d1 ) conict.

creation new rule (4a)

interrupts manager. Propagation using new rule derives

= {PT , C1 , C2 }, subsequent propagation sets structure
{PT , C1 , C2 , root(d1 ), }. Still default acceptable state (T justied),

backtracking





rule (2b) transformed split another instance.



(root(d2 ) R(d2 )) T2
(2ba)
T2 x \ {d1 , d2 } : root(x) R(x) (2bb)

251



fiDe Cat, Denecker, Stuckey & Bruynooghe
g ,

Rule (2ba) moved

rule (2bb) remains

.

state default

acceptable.

T2 , choosing rst disjunct rule (2ba)
R(d2 ). literal root(d2 ) dened ,
justied direct justication {(d2 = d1 )}. literal R(d2 ) cannot justied
direct justication (as edge literals false current justication graph)
rule (5) transformed split instance d2 . Actually, instance

6. Again, search avoids new Tseitin
propagates

root(d2 )



turn disjunctive body complex subformula, avoid grounding
subformula, break two parts introduce another Tseitin.







R(d2 ) root(d2 ) T3
(5aa)


T3 : edge(d2 , y) R(y)
(5ab)
x \ {d2 } : R(x) root(x)





: edge(x, y) R(y)
(5b)
Rule (5aa) moved

g ,

others remain

.

{PT , C1 , C2 , root(d1 ), T, root(d2 ), R(d2 )}, hence propagation rule (5aa) g extends T3 . direct justication justifying
T3 and, hence, rule (5ab) partially grounded splitting d1 case:


T3 (edge(d2 , d1 ) R(d1 )) T4
(5aba)
T4 \ {d1 } : edge(d2 , y) R(y) (5abb)

7. current structure

Rule (5aba) moved

g

rule (5abb) remains

8. search selects rst disjunct

R(d1 ).
it.

literal

Extending

J

R(d1 )

dened

.

T3 's rule body propagates edge(d2 , d1 )
, {root(d1 )} direct justication

direct justication yields acceptable default

root(d1 ) dened g . However, root(d1 ) justied gd ,
J direct justication discussed earlier.
justication manager faces new problem: true literal edge(d2 , d1 ) conict
0
0
2
direct justication {edge(d, ) | (d, ) } C2 (rule (3)). handle
conict, splits aected instance (x = d2 , = d1 ) rule:


C2 (edge(d2 , d1 ) edge(d1 , d2 )) T5
(3a)
T5 (x y) D2 \ {(d2 , d1 )} : edge(x, y) edge(y, x) (3b)

acceptable state, since

making safe extend

g rule (3b) remains . direct justication
{edge(d, d0 ) | (d, d0 ) D2 \ {(d2 , d1 )}}, unaected part direct
justication C2 . restores acceptability.

Rule (3a) moved

T5

set

9. Propagation rule (3a) extends





edge(d1 , d2 )



T5 . literal edge(d1 , d2 ),
T5 (rule (3b)). resolve

true, conict direct justication

it, justication manager partially grounds rule (3b) splits instance

{x = d1 , = d2 } follows.


(3ba)
T5 (edge(d1 , d2 ) edge(d2 , d1 )) T6
T6 (x y) D2 \ {(d2 , d1 ), (d1 , d2 )} :


edge(x, y) edge(y, x) (3bb)

252

fiLazy Model Expansion: Interleaving Grounding Search
g rule (3bb) remains ; T6 inherits direct
edge(d1 , d2 ) removed. Propagation rule (3ba) extends
state acceptable, T6 dened justied.

Rule (3ba) moved
justication


T6 .

now,

T5



resulting

g

consists rules (1), (2a), (4a), (2ba), (5aa), (5aba), (3a), (3ba),

consists rules (4b), (2bb), (5b), (5abb), (3bb). cur{PT , C1 , C2 , root(d1 ), root(d2 ), edge(d2 , d1 ), edge(d1 , d2 ), R(d1 ), R(d2 ),
T, T3 , T5 , T6 }, model PT g .
literals, root(d2 ), R(d1 ) T6 dened . Literal root(d2 ), dened
rule (4b) {(d2 = d1 )} direct justication. Literal R(d1 ), dened rule (5b),
{root(d1)} direct justication. Literal T6 , dened rule (3bb) direct justication
set negative edge literals except edge(d1 , d2 ) edge(d2 , d1 ). obtain
full model theory, extended literals direct justications.

residual denition
rent structure





case, assigns open literals model completed wellfounded model computation

gd .

Actually, done without grounding

denition (Jansen, Jorissen, & Janssens, 2013).

4. Justication Management
Section 3.2, instantiated general framework, developed Section 3.1,
justication manager access

.

example Section 3.3, justi-

cation constructed demand, i.e., time literal needed (dierent) direct
justication, body dening rule analyzed justication extracted.
failed, part rule grounded. called
imagine

global approach,

rules



local approach.

One also

considered attempt

select direct justications minimize grounding rules whole. Obviously,
global approach time consuming, applied every time
adjustment justication required. section, describe approaches.
describing algorithms, introduce notations assume normalizations done. function nnf reduces formula negation normal form.

set single element, + used shorthands {s}
S\{s}. J justication, denote J[l d] graph identical J except l
justied d. assume quantiers range single variable variable names


reused formula. Furthermore, assume basic reductions applied
formulas, e.g.,

>

reduces

, x :

reduces

t,

...

4.1 Local Approach
Algorithm 2 shows top level lazy_mx model expansion algorithm, taking input
theory

{PT , }



Iin . Denitions g initialized
initialized Iin . set ground sentences Tg
initial justication J empty. auxiliary (FIFO)

initialized empty.

latter keeps track literals direct

input structure

empty denition, respectively,
initialized fact
queue

qch

PT

justication needs checked.
main loop performs model expansion

Tg g ,

interleaved work

justication manager towards establishing default acceptable state. model expansion

253

fiDe Cat, Denecker, Stuckey & Bruynooghe
part consists propagation (the call propagate), test whether current state
inconsistent (with learning backjumping), test whether model

Tg g

found (returning model justication) choice step

Tg g

selects literal unknown

assigns value.

Propagation returns literals

entailed ground theory (partial) structure, example applying
unit propagation unfounded/wellfoundedness propagation (Marin et al., 2008).
test model performed default acceptable state (i.e., queue

qch

empty). test succeeds, ensures well-founded model computation
expand current structure



extended direct justications literals

model whole theory. Also choice step takes place default acceptable
state; ensures search space limited state space default acceptable
states.

justication manager activated propagation choice step assigns

literal
valid.



l.
l

calling check_literal, checked whether current justication remains
dened

queue

qch



justication, needs justication added

processing justication manager.



l

0
justication another literal l , justication becomes inconsistent
another justication also added

qch .

occurs





l0

needs

processing done selecting

elements queue calling lazy_ground function.

latter function rst

attempts nd (dierent) consistent direct justication l; fails, splits
rule instance dening

l





partially grounds it, hence

g

extended. new

clauses may trigger propagation; therefore processing queued literals interleaved
propagation and, possibly, backtracking . Note backtracking might restore
consistency



direct justication

J(l)

literal

l



qch .

4.1.1 Lazy Grounding One Rule
function lazy_ground, Algorithm 3, checks whether literal
tion; not, simply returns. Otherwise, checks whether

l

l

needs direct justica-

valid justication, i.e.,

one satises invariants detailed below. so, also returns; otherwise, passes
rule body used construct justication (the negation dening rule
literal negative) build_djust, function attempts nd valid direct
justication. Besides literal rule body, also initial justication, derived
rule body, passed build_djust. latter function successful, justication
updated lazy_ground done; not, direct justication literal

false

l

set

split_and_ground called ground part rule dening l.

going details, rst analyze properties want maintain
current justication
considered part

J

J.

direct justications literals

qch queue
J are:



since might invalid. global invariants



literals unfounded



set literals

J

(recall, negative cycles allowed),

J

consistent.

direct justication

= J(l) J



l

queue, invariants lazy

grounding process are:



contains literals dened

g

(unless literal safely justied

discussed before),

254

gd ,



fiLazy Model Expansion: Interleaving Grounding Search

Algorithm 2: lazy_mx lazy model expansion algorithm.
1 Function lazy_mx (atomic sentence PT , denition , structure Iin )
Output: either model g J false

2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

20
21
22

Tg

:= {PT };

g

true

:=

;

:=

; J

:=

;

:=

Iin ; qch

:=

L := propagate(Tg g , );
:= L;
foreach l L qch :=check_literal(l,qch );
inconsistent
Tg += learn_nogood(I , Tg );
conict root level return false ;
:= state backjump point;
else qch empty
(l, qch ) := dequeue(qch );
lazy_ground(l);
else model Tg g
return , J ;

else

select choice literal l;



:=

+ l;

qch :=check_literal(l,qch );

Function check_literal (literal l, literal queue qch )
Data: global J Output: updated queue
l dened J(l) = undef qch := enqueue(l,qch ) ;
foreach l0 l J(l0 ) qch := enqueue(l0 ,qch ) ;
return qch ;

Algorithm 3: lazy grounding literal.
1 Function lazy_ground (literal l)
Data: global , J
2
l l dened
3
J(l) exists obeys invariants return;
4
else

5
6
7
8
9
10
11

;



:= body rule dening l;

l negative literal

:= nnf ()
dj := build_djust(l, , init_just(l));
dj 6= false J := J[l dj]; return;

else

J

=

J[l false];

split_and_ground(l);

255

;

fiDe Cat, Denecker, Stuckey & Bruynooghe


literals
queue


qch .

dened



either direct justication

J

belong

qch

invariants imply default acceptable state reached

queue

empty. Indeed, follows invariants current justication total
situation hence literals direct justication justied (Denition 2.6).
Due policy followed queue literals, current justication also consistent



literals true



dened



justication, hence

hg , , J, Ii



default acceptable state.

4.1.2 Building Direct Justification
purpose build_djust, Algorithm 4, extend
literal

l

.

dened

I.
formula



l

J

suitable direct justication

literal

J(l)

currently undened



inconsistent

recursive function takes three parameters: ( ) literal

l,

made true direct justication (initially whole body

ii

( )

rule dening literal; note initialization takes negation rule

iii) description direct justication derived far, initialized

literal negative), (

init_just(l). algorithm, assume

dierent quantiers range

dierent variables.
going details, discuss represent direct justications.

Basically,

could represent direct justication set ground literals. However, set
quite large using ground representation might hence defy purpose lazy

hL, Bi L set
B set bindings xi Di xi variable Di
domain. set bindings B = {x1 D1 , . . . , xn Dn } represents set variable
substitutions SB = {{x1 /d1 , . . . , xn /dn } | di Di [1, n]}. set ground
literals represented hL, Bi {l | l L SB }. direct justication
literal P (d1 , . . . , dn ), dened rule x : P (x) , initialized init_just(l)
h, {x1 {d1 }, . . . , xn {dn }}i. eect, B allows identify relevant rule instantiation

grounding.

Instead, represent direct justication pair

possibly non-ground literals

providing appropriate variable instantiation domains, set literals
empty.
build_djust algorithm searches set literals making
recursively calling subformulas
larger justication

.





true.

works

composing results afterwards

set literals found, example none

exists consistent direct justications,

false

returned.

base case formula literal. make literal true, instances
literal set bindings

B

must true, hence, set literals

L

extended

literal itself. resulting direct justication satisfy invariants,
checked call valid: returns

true call valid(l, dj) dj
l J[l dj] satises

(part ) direct justication

satises invariants
invariants

justication.
universally quantied formula
quantied variable.

x D.

x :

true instance

Hence, recursive call, set bindings

B

extended

existentially quantied formula, suces one instance true. Hence,

minimal approach try instance separately one succeeds; fail,

256

false



fiLazy Model Expansion: Interleaving Grounding Search

Algorithm 4: build_djust algorithm.
1 Function build_djust (literal l, formula justication hL, Bi)
Input: B binds free variables
Output: either direct justication false
2
switch
3
case literal
4
valid(l, hL {}, Bi) return hL {}, Bi;
5
else return false ;
6
case x D0 :
7
return build_djust(l, , hL, B {x D0 }i);
8
case x D0 :
9
large(D0 )
10
return build_djust(l, , hL, B {x D0 }i);
11
foreach di D0

12
13

hL0 , B 0 := build_djust(l, , hL, B {x {di }}i);
hL0 , B 0 =
6 false return hL0 , B 0 i;

14
15
16
17
18
19

return false ;
case 1 . . . n
foreach [1, n]

20
21
22
23
24

return hL, Bi;
case 1 . . . n
foreach [1, n]

25

hL0 , B 0 := build_djust(l, , hL, Bi);
hL0 , B 0 = false return false ;
else hL, Bi := hL0 , B 0 i;

hL0 , B 0 := build_djust(l, , hL, Bi);
hL0 , B 0 =
6 false return hL0 , B 0 i;

return false ;

257

fiDe Cat, Denecker, Stuckey & Bruynooghe
returned. Note however want iterate domain element
large, would similar constructing grounding itself. Instead,
extend binding

x D.







large,

Conjunction similar universal quantication, except

explicit iteration conjunct needed. soon one conjunct fails, whole
conjunction fails. Disjunction similar existential quantication small domain.
Note build_djust non-deterministic due choices domain element justify
existentially quantied formula, disjunct justify disjunction.

Example 4.1.

Consider following rule large domain

D.

H x : P (x) (y : Q(x, y) R(x, y))
Assume

J empty
h{P (x)}, {x D}i

loops keep track of.

Applying build_djust

P (x) body chosen.
corresponds direct justication {P (x) | x D}. Alternatively, second disjunct chosen, returns h{Q(x, y), R(x, y)}, {x D, D}i, represents direct
justication {Q(x, y) | x D, D} {R(x, y) | x D, D}.


H

returns

rst disjunct

4.1.3 Partially Grounding Rule
last bit lazy model expansion algorithm handles case justication
found denition literal

l

grounded.

straightforward way

would call one_step_ground rule dening l, store result

g



.

However, many cases operation results much grounding.

Example 4.2.

form x : P (x) situation
P (d). Applying one_step_ground r1 would instantiate
x elements D, resulting |D| rules, fact suces split r1 two rules,
one instance x = one remainder. Another example applies rule r2
form H x : Q(x) R(x) direct justication J(H) = {Q(x) | x D}.
Q(d) becomes false, justication manager may need ground rule. Applying
one_step_ground would instantiate universally quantied x elements .
Instead, better split instance x = introduce Tseitin
remainder, producing H (Q(d) R(d)) g x : Q(x) R(x)
. direct justication obtained incrementally removing Q(d)
H , discussed Section 4.1.5.
Consider rule

r1

justication found atom

split_and_ground algorithm (Algorithm 5) ground part rule dening
given literal
dening



l

l,

say


P .

rst step split rule instance rule

grounded (the call split).

denes


P .

Let

replace rule

additionally return rule


P [x/d].

x : P (x) rule
x : P (x)

Afterwards, apply one_step_ground

latter rule add computed rules either

g



. 2

result split_and_ground denition
one. limit ground denition
equivalent

gd



gd ground previous
empty g strongly voc()-

.

2. Recall, head new grounded rule always dierent head already grounded rules.

258

fiLazy Model Expansion: Interleaving Grounding Search
Algorithm 5: split_and_ground algorithm.
1 Function split_and_ground (literal l)
Input: l dened
Result: update g , , J , qch
2
3

r := split(l); // split updates
(0g , 0d ) := one_step_ground(r);
g = g '; = ';

4

Even justication found, better splitting

l

applying

one_step_ground, shown Example 4.2. First, splitting made signicantly
intelligent, discussed Section 4.1.5. Second, improve one_step_ground
ground part expressions possible, describe below.

Improving one_step_ground.
subformulas/instantiations
result consists

|D|

l iterates
x : P (x),

Applying one_step_ground rule

.

example



sentence

new rules many new Tseitin symbols. Instead, depending

value l, sucient introduce one (or some) subformulas, shown
Algorithm 6, extends switch statement one_step_ground two higherpriority cases.



l

true, sucient ground one disjunct/existential instantiation

delay rest Tseitin introduction.

l

false, take similar approach

conjunction/universal quantication.

Algorithm 6: Additional cases one_step_ground algorithm.
1 switch r
2
case l 1 . . . n I(l) =

3
4

5
6
7
8

choose

[1, n];

return h{l }, {T
case l x : I(l) =

W

choose

j{1...n}i j }i;

D;

return h{l [x/d] }, {T
analogous cases









x : }i;
combination I(l) = f .

4.1.4 Algorithmic Properties
Correctness termination presented algorithms discussed following theorem.

Theorem 4.3 (Correctness termination). lazy_mx returns interpretation ,

expanding literals direct justications J , applying well-founded evaluation gd restricting voc(T ) results model . algorithm returns
false , interpretation exists precise Iin satises .

259

fiDe Cat, Denecker, Stuckey & Bruynooghe
Algorithm lazy_mx terminates nite domain D. Otherwise, termination
possible guaranteed.3
Proof.

lazy_mx returns interpretation

I,

model

g



qch

empty. Given

properties split_and_ground, applying lazy_ground literal l, either
valid justication dened
state and, Theorem 3.5,
returns

false ,



g .

gd

qch



empty, default acceptable

expanded model whole theory. lazy_mx

proven

models

Hence

l

hence



g

models

Iin .

case, also

Iin .

also models expanding

Without calls lazy_ground, search algorithm terminates nite
tion lazy_ground produces ever-increasing ground theory
limit. Hence, lazy_mx always terminates





nite.

g

g ; func-

full grounding

innite, limit

g



innite grounding, termination cannot guaranteed.

4.1.5 Symbolic Justifications, Incremental Querying Splitting
algorithms presented sound complete.

However,

improved taking formulas justications derived account.

Symbolic justications incremental querying.
(subformulas ) formula

Example 4.4.

,

multiple justications exist

grounding delayed.

Consider formula

x : P (x) Q(x),



h{P (x)}, {x D}i

h{Q(x)}, {x D}i justications. that, could derive justication:
D, make either P (d) Q(d) true. Hence, grounding necessary
P (d) Q(d) become false d.





automatically changing build_djust follows:



algorithm allowed select multiple disjunctions / existential quantications
even valid justication already found one (Lines 13 24).



build_djust longer returns justication, symbolic

justication formula



entails original formula. formula built build_djust reects
subformulas/instantiations selected.



,

justication

.
{P (x) | x
x : P (x) Q(x).

derived directly set non-false literals (full) grounding
example, formula

D},


x : P (x) Q(x),

instead justication

build_djust might return justication formula

validity check (valid) extended return false justication formula false.

allowing complex formulas (instead conjunction universally quantied literals), validity check whether formula become false incremental changes




expensive. fact

incremental query

problem. exper-

iments, limit depth allowed formulas use straightforward (expensive)
algorithm evaluates whole formula whenever assignment falsify it.
3. possible change integration

lazy_ground lazy_mx guarantee termination nite

model exists, see Section 5.1.

260

fiLazy Model Expansion: Interleaving Grounding Search
Body splitting.

described Algorithm 3 Section 4.1.3, split simply splits

rule instance denes l, one_step_ground grounds rule instance step step,
accordance structure formula. However, grounding triggered
conict current justication, one_step_ground blind origin
conict.

using conicting literals, one could focus grounding part

formula contributes conict. One restructuring rule part
grounded, contains conict, part grounded,
old justication adjusted still apply.

latter part split

introducing new Tseitins transformation called body splitting. approach
inserted Algorithm 3 call split. this, original justication (call

jold )

passed extra argument split_and_ground.

Example 4.5.

h x : P (x); let h{P (x)}, {x D}i justication h true . P (d) becomes false, easy see split
violating instantiation rewriting original rule h P (d) adding
rule x : P (x). Crucially, justication second part derived
original justication, namely h{P (x)}, {x d}i. second part hence
added justication J rst part added g .
Consider rule

r direct justication jold done ecient way.
v true domain literal partial structure direct justication
rule r contains negation v . implementation binding(s)
justication instantiates v extracted representation
direct justication rule. simplicity, assume {x = d1 , . . . , dn } single
instance. recursive algorithm visits formula body r depth-rst. Whenever
quantication x : encountered x equal xj x, replaced
(x dj : ) [x = dj ]. Tseitin transformation applied left-hand
revision rule

Assume

conjunct algorithm recurses right-hand conjunct remains
binding. new rule dening new Tseitin jold v direct justication. Similarly,
existential quantication replaced disjunction.

result set new rules

new justication sought smaller rule

r0

passed

one_step_ground. Correctness follows fact jold v valid justication, none
new rules contains

Example 4.6.

v,

correctness Tseitin transformation.

Example 4.1, justications sought

H

rule

H x : P (x) (y : Q(x, y) R(x, y)).
J = {Q(x, y) | x D, D} {R(x, y) | x
, l = Q(d1 , d2 ) becomes false, J longer
consistent cannot repaired. J l, however, still consistent ,
justication whole body. hand, J l justication
subformula P (x) : Q(x, y) R(x, y) instantiation x dierent
d1 . Consequently, split quantication x : x d1 x = d1

Assume selected justication

D, D}.



P (d1 )

true

apply Tseitin transformation former. Afterwards, recursively visit latter
formula apply similar reasoning existential quantication. operations

261

fiDe Cat, Denecker, Stuckey & Bruynooghe
(split 1)
x d1

.





P (x)




Q(x, y)

(split 2)

P (d1 )

0 d2

.





R(x, y)
Q(d1 , 0 )

R(d1 , 0 ) Q(d1 , d2 ) R(d1 , d2 )

x : P (x) : Q(x, y) R(x, y) split violating
Q(d1 , d2 ). original justication without Q(d1 , d2 ) justication

Figure 2: rule body
literal

left-hand side splits, justication formula shown blue.
remaining non-justied formula shown red.

formula illustrated Figure 2. result consists following rules,
rule

H

even ground.




H T1 (P (d1 ) (T2 (Q(d1 , d2 ) R(d1 , d2 )))).






T1 x d1 : P (x) : Q(x, y) R(x, y).




: Q(d , y) R(d , y).

2
2
1
1
optimize traversal formula
path taken parse tree

Example 4.7.



,

build_djust extended store

direct justications subformulas.

C1 (x y) D2 : edge(x, y) edge(y, x)
justied, J empty interpret edge. build_djust algorithm recursively
visits body rule edge(x, y) returned valid literal use. Going
one level, store edge(x, y) edge(y, x), selected {edge(x, y)}. Assuming
disjuncts selected, edge(x, y) returned again. Going back
quantications, store that, quantications, selected set relevant
2
domain elements, build_djust returns justication formula (x y) : edge(x, y).
Assume rule

build_djust given access
direct justication.

jold ,

similar optimizations possible repairing

Consider Example 4.6, assume

P (d1 )

unknown

I.



case, left branch Figure 2 also transformed rule still valid,

P (d1 ) direct
justication rule H T1 (P (d1 ) (y (D d1 ) : Q(d1 , y) R(d1 , y))),
T1 Example 4.6.

direct justication. right branch, repair select disjunct

4.2 Global Approach
Finding justications using greedy local approach easily lead grounding
necessary. Consider example sentences

262

x : P (x)



x : P (x) Q(x).

fiLazy Model Expansion: Interleaving Grounding Search
Applying local approach second sentence rst (with empty
construction makes atoms

P

J ),

might result

false. applying local approach

rst sentence nds valid justication it; fully grounded. global
approach takes set rules input tries select direct justications


expected

grounding size whole set minimal.

cast task, called

optimal justication problem, problem graph
rule nodes R justication nodes J .

follows. graph consists two types nodes,

justication node symbolic set literals representing possible justication (for

). rule node pair
(t, f , u); pair hr, ti rule r head l
expresses exists direct justication l; pair hr, f exists direct
justication l pair hr, ui r justication.

literals dened rule

hr, ti

r

rule





,

given current partial structure

truth value

three types edges:

Valid edges

rule node

(the negation ) head

Conict edges

hr, ti (hr, f i) justication node j



j

justies

r.



ii

( ) rule nodes rule dierent truth value, ( )

iii) rule node hr, ti (hr, f i)
l (l), (iv) rule node hr, ui,

justication nodes contain opposite literals, (

r

denes


(or

r

l)

l,

justication node contains

denes

l,

justication node contains



l

(a conict

l

hr, ti (hr, f i)

j

needs justication).

Depends-on edges

justication node

contains negative (positive) literals dened
aim select subsets



l

Rsel R



j
r.

Jsel J

rule node





selected rule node connected valid edge least one selected justication node.



conict edges exist pairs selected nodes.



Neither positive mixed cycles exist subgraph consisting valid
depends-on edges selected nodes.

selection

{Rsel , Jsel }

extracted follows.
rule

r



satisfying constraints, initial justication

literal

hr, ti (hr, f i)

l (l)



selected rule. direct justication union

justications justication nodes
edge.

J

given direct justication dened

Jsel

connected

hr, ti (hr, f i)

valid

Moreover, literals dened rules rule node selected

added initial

qch

queue, handled local approach, complete solution

must justication them.

hr, ui

selected, means grounding

instances rule delayed literals denes become assigned.
type problem somewhat related NP-hard

hitting set

(or

set cover )

problem (Karp, 1972): given set top bottom nodes edges them,
task nd minimal set bottom nodes top node edge
least one selected bottom node.

263

fiDe Cat, Denecker, Stuckey & Bruynooghe
hg , , J, Ii, input optimal justication
, node constructed
truth value known ) also conict

Given default acceptable state

problem generated follows. rule
three truth values (only one
edges added.

Valid edges justication nodes obtained using (straightforward)

adaptation build_djust returns set possible justications make head
rule true (false). E.g., rule

P (x)

x : P (x) , build_djust called literal
{x D}. Conict depends-on edges derived

binding initialized

checking dependencies justications rules justications.



keep ecient, done symbolic level.

Example 4.8.

PT , C1



C2

x : root(x) R(x)
(x y) D2 : edge(x, y) edge(y, x)
x = d1
root(x) : edge(x, y) R(y)

(2)
(3)
(4)
(5)






Consider theory running example,

propagated true. Denition






C1
C2
x


:
root(x)



x : R(x)











associated optimal construction set input shown Figure 3. Note rule

C1 C2 true ,
root(x) root(x)

nodes, use dened head literals identify rule. Literal
hence one rule node rules (2) (3). Neither
justied

x D,

hence

hroot, ui

tuple.

four solutions subset-maximal respect rule nodes, namely
following rule node selections:

{hR, ui , hroot, ui , hC2 , ti}

(a)

{hR, f , hC2 , ti}

(b)

{hR, ti , hC2 , ti}

(c)

{hC1 , ti , hC2 , ti}

(d)

these, multiple justication selections possible (also shown Figure 3).


C1 ,

select justication

(iv),



C2

choose

(v)



(vi)

(but

both).
objective maximize number selected rule nodes, minimize
expected grounding size.

obtain estimate expected grounding size,

following conditions taken account:



depend size grounding rule.



Assigning multiple justications rule result lower estimate rule
grounded false.



Variables occurring multiple justications result less matching instantiations.



practical applications, number false atoms far exceeds number
true ones model. Hence, positive literals justication higher
cost negative ones.

264

fiLazy Model Expansion: Interleaving Grounding Search
hR, f

h{root(x), edge(x, y)}, {x D, D}i

hR, ui

h{root(x), R(x)}, {x D}i

hR, ti

h{root(x)}, {x D}i

hC1 , ti

h{root(x), R(x)}, {x D}i

hC2 , ti

h{edge(x, y)}, {x D, D}i

hroot, ui

h{edge(x, y)}, {x D, D}i

(i)

(ii))

(iii)
(iv)
(v)
(vi)

Figure 3: graph part input optimal justication problem Example 4.8.

Rule nodes shown left, justication nodes right;

valid edges shown green, conict edges red depends-on edges blue.
readability, conicts justications unknown rule nodes
shown.

approximate expected grounding size function
input rule

r

(with head

h),

J.

takes

n

selected type justication (rule selected ( ),

justication (u), justication
cations

expsize

h (t)

justication

h (f ))

set justi-

function returns expected grounding size rule (size(r), dened

below) weighted depending type justication. weights derived two
estimates:

pval

ptr

probability atom become assigned

probability

n

assigned atom true. Hence, dened formally below, non-delayed rules ( ),

u

full size used; rule without justication ( ) weighted



( ) weighted

ptr ,

product justications
namely

pt

literals

f

false one ( )

j



J.

1 ptr ;

true justication

latter two weights multiplied

factor product sum two terms,

times number negative literals

j.

pval ;

j



1 ptr

times number positive

eect expected size decreases number justications

increases expected size increases justication literals.

expsize (r, n, ) = size(r)
expsize (r, u, ) = size(r) pval
expsize (r, f , J) = size(r) pval (1 ptr )



((1 ptr ) |pos.

lits.

j|

jJ

+ ptr |neg. lits. j|)

expsize (r, t, J) = size(r) pval ptr
((1 ptr ) |pos. lits. j|
jJ

+ ptr |neg.
probabilities, assumed

pval

lots literals get value,

lits.

j|)

small (currently 0.1) reect hope

ptr

less half, reect atoms

265

fiDe Cat, Denecker, Stuckey & Bruynooghe
often assigned false true. function

size

dened below. function returns

number atoms grounding rule formula, except existential quantication
disjunction. these, take account grounded partially
using Tseitin transformation, taking logarithm total grounding size.

size(L) = 1
size(L ) = size() + 1
size(x : ) = size()
X
size(i )
size(1 . . . n ) =
i[1,n]

size(x : ) = log(D) size()
P

i[1,n] size(i )

size(1 . . . n ) = log(n)

n

Solutions optimal justication problem minimize term

X

expsize (r, t(r), J(r))

rd


t(r)

type (t,

f,



u)



J(r)

justication literal dened

r.

Example 4.9 (Continued Example 4.8). size rule C1 1+log(D)2, C2

1+D2 log(2), root D(1+1), R D(1+log(2)(1+log(D)2)/2.
Consider assigning justication (iv) C1 : results expected cost rule
(1 + log(D) 2) 0.3 1 (as construction relies making R true). Additionally,
would force grounding rule dening R, increasing cost size
rule R. optimal solution problem Figure 3 rule node selection (a)
2
justication (vi) C2 . cost sum (1 + log(2) 0.3 (for justication
(vi)) 1 + log(D) 2 (the expected size rule C1 ). Now, rule C1


passed local approach.
solve optimal justication problem,

IDP's

optimization inference applied

(meta-level) declarative specication task.

4

larger theories

T,

problem

turns quite hard, two approximations considered reduce search
space.

First, number selected justications rule limited 2.

values

size(r)

grow quite large, approximation

standard approach). Rounding integer values applied

dlog(size(r))e

IDP's

Second,

used (a

support oating

point number still preliminary. resulting specication could solved optimality
within seconds tested theories. lazy model expansion, global approach
applied initial phase Tseitin literals representing sentences original
theory propagated true.

5. Heuristics Inference Tasks
section discusses tune lazy grounding search heuristics
underlying SAT solver obtain eective implementation lazy model expansion.
4. specication part

IDP

's public distribution.

266

fiLazy Model Expansion: Interleaving Grounding Search
also describe inferences tasks beyond model expansion useful
context lazy grounding. less important issues discussed Appendix A.

5.1 Heuristics
Heuristics play important role lazy grounding algorithms, serve nd
right balance much ground long search. rst discuss
heuristics chosen. Afterwards, discuss alternative approach minimize
grounding.

5.1.1 Balance Grounding Search
algorithms leave room number heuristic choices important
eect performance.

briey discuss choices.

guideline

decisions, following principles used:



Avoid leaving search process without enough information make informed
decision; example, avoid losing much (unit) propagation introducing
many Tseitin symbols.



Prevent creating grounding large; may example happen
result long propagate-ground sequence.

Recall, goal create minimal grounding, solve model expansion problems
avoiding large grounding.
Below, introduce number parameters aect heuristics.

exact

values used experimental evaluation parameters introduced specied
Appendix A.
split_and_ground, handling disjunction existential quantication,
choice many disjuncts expand. expand one instantiation time
rule

h x : P (x),

done Algorithm 6 (lines 3 6), iterative application results

ground theory

h P (d1 ) T1
T1 P (d2 ) T2
T2 P (d3 ) T3
.
.
.

Tn x \ {d1 , d2 , . . . , dn } : P (x).
SAT-solver MiniSAT, used


P (di )

IDP

system, initially assigns

atoms; choice triggers iteration propagation grounding.

f





resulting thrashing behavior reduced somewhat, grounding compact
grounding introduces

n

disjuncts time:

h P (d1 ) . . . P (dn )
x \ {d1 , d2 , . . . , dn } : P (x).

267

fiDe Cat, Denecker, Stuckey & Bruynooghe
remedy this, two search-related heuristics changed. First, initial truth
value randomized, favoring false (as models, many atoms false
true). Second, search algorithms typically

restart

(ever-increasing) threshold

number conicts, sometimes caching truth value assigned atoms (

polarity caching ).

allows solver take learned information account search heuristic
staying approximately part search space.

case lazy grounding,

might want jump another part search space come across long
propagate-ground sequences. end, introduce concept

randomized restarts,

take place (ever-increasing) threshold number times

g

extended

randomly ipping cached truth values.
addition, build_djust always returns

false

estimated formula

small grounding. Indeed, grounding formulas help search. Whether formula
considered small determined terms (estimated) grounding size.
strategy used split_and_ground:



whenever formula one_step_ground

would applied small, ground applied instead, completely ground formula.

5.1.2 Late Grounding
Grounding applied search process soon unit propagation taken place.
result focus current location search space, danger
grounding much solution part space. Alternatively, could
apply opposite strategy, namely ground late possible: apply additional
grounding search algorithm terminates without ever found model
acceptable default state. strategy well-known elds incremental proving
planning, domain (number time steps) increased search
previous, smaller bound nished. guarantees minimal grounding. prototype
strategy implemented

IDP

good results planning problems.

5.2 Related Inference Tasks
bulk paper focuses model expansion (MX)
solutions structures two-valued

voc(T ).

FO(ID )

theories

T,



Often, one interested

small subset symbols voc(T ). example case model generation
SO(ID), language extends FO(ID) existential quantication relations.
SO(ID ) problem P1 , . . . , Pn : initial structure , relation symbols P1 , . . . ,
Pn , FO(ID) theory, solved model generation FO(ID) theory
initial structure dropping interpretation symbols P1 , . . . , Pn
models. Another example query evaluation FO(ID ): given theory , initial
structure formula free variables x (all FO(ID )), purpose evaluating
query hT , I, nd assignments domain elements x model
exists expands [x/d] true. solve model expansion
FO(ID ), new predicate symbol introduced answers query tuples

domain elements true model theory extended
sentence x : (x) denition {x : (x) }.
cases, approaches using (standard) model expansion compute total interpretation afterwards drop unnecessary information, quite inecient. Lazy model

268

fiLazy Model Expansion: Interleaving Grounding Search
expansion save lot work partially grounding theory. However,
model found grounded part, justications remaining denitions
used expand structure model full theory.

Although expansion

obtained polynomial time, still inecient afterwards large part model
dropped.
remedy this, dene variant model expansion task, denoted

T,

restricted

MX.

additional list symbols O,
5
called output symbols. Solutions structures two-valued symbols
expansion exists extends model . Adapting lazy
Restricted MX takes input theory

structure

grounding solve restricted MX done analysis justications
need added (completely) structure, splitting

gd

multiple denitions

evaluating dening output symbols symbols depend (using
stratication argument).
above-mentioned inference tasks cast trivially restricted MX problems
lazy restricted MX greatly improves eciency respect ground-and-solve,
shown experimental section.
extension

FO(ID )



procedurally interpreted

symbols (De Cat et al., 2014)

provides another class interesting problems. predicate symbols xed interpretation, know whether tuple belongs predicate, procedural function
executed. approach provides clean way combine declarative procedural specications. Consider example symbol

isP rime(N)

interpreted

procedure executes ecient prime-verication algorithm returns true
given argument prime. generally interested complete interpretation

isP rime, cast restricted MX problem isP rime O.

Solving

problem using lazy grounding benet executing associated function



search relevant atoms

isP rime(d).

Also task, show experimental

evaluation next section.

6. Experiments


IDP

system state-of-the-art model expansion engine, observed

previous Answer-Set Programming competitions (Denecker et al., 2009; Calimeri et al., 2014;
Alviano et al., 2013). lazy model expansion algorithms presented paper
implemented

IDP

system, extending existing algorithms (De Cat, Bogaerts,

Devriendt, & Denecker, 2013).
current implementation incomplete sense cycle check justications implemented yet. aects inductive denitions non-inductive
ones replaced FO formulas completion.

workaround

lack cycle check, build_djust, function constructs direct justication, returns
false rules dening inductive predicates. consequence, instance rule
immediately grounded, although lazily, domain atom dened rule assigned value. Another consequence inductively dened predicates cannot used
justications rules. aects three benchmarks ASP competition (de5. Within ASP community, sometimes referred show predicates.

269

fiDe Cat, Denecker, Stuckey & Bruynooghe
scribed Section 6.2), namely

Reachability, Sokoban



Labyrinth.

these,

grounding might delayed even complete implementation.
section organized follows. Section 6.1, evaluate overhead completely
grounding theory using presented approach. Section 6.2, evaluate eect
lazy grounding number benchmarks ASP competition.

Section 6.3,

number additional properties presented algorithms demonstrated.
tested three dierent setups:
(referred

g&s), IDP

IDP

standard ground-and-solve approach

lazy model expansion (lazy) award-winning ASP

system Gringo-Clasp (ASP). used

IDP

version 3.2.1-lazy, Gringo 3.0.5 Clasp 2.1.2-st.

parameters lazy grounding algorithms discussed Section 5.1, values
used experiments documented Appendix A. experiments Sections 6.1
6.3 run 64-bit Ubuntu 13.10 system quad-core 2.53 GHz processor
8 GB RAM. Experiments Section 6.2 run 64-bit Ubuntu 12.10 system
24-core 2.40-Ghz processor 128 GB RAM. timeout 1000 seconds
memory limit 3 GB used; out-of-time indicated

T,

out-of-memory

M.6

6.1 Eect Grounding Time
Lazy grounding may reduce grounding size time also causes overhead. instance,
expect (naive) incremental querying justications costly discussed previously. aim section quantify overhead caused lazy grounding.
experiments compare grounding time standard IDP system


naive

instance lazy grounding algorithm forced generate complete

grounding starting search. instance obtained standard algorithm using small changes: shortcut ground small formulas turned
o, disjuncts instances existentially quantied formulas grounded one one,
dened literal enqueued lazy grounding soon appears

g .

comparison,

also measure cost standard lazy grounding algorithm computes partial
groundings.
devised six benchmarks test various aspects novel algorithm. benchmark simple theory two sentences simple solve. benchmarks
designed measure cost dierent aspects lazy grounding: delaying resuming grounding, querying needed resume grounding, splitting formulas, etc.
Specically, tested aspects following:
1. Overhead delaying resuming grounding case existential quantier
large domain.

sentence

n disjuncts; naive lazy
n 2 Tseitin symbols.

clause
introduces

x : P (x).

Standard grounding creates single

grounding grounds formula piece piece

2. Overhead case inductive denition,

{x : P (x) P (x) Q(x)}.

standard grounding naive lazy grounding construct ground rule



P (d)

atom.
6. Benchmarks, experimental data complete results available

krr/experiments/lazygrounding/jair.

270

http://dtai.cs.kuleuven.be/

fiLazy Model Expansion: Interleaving Grounding Search
3. Overhead case universal quantication.
standard grounding creates

n

sentence

x : P (x).



atomic formulas, naive lazy grounding splits one

instance time introduces

n2

Tseitin symbols.

4. Lifted Unit Propagation (LUP) (Wittocx et al., 2010, 2013) important preprocessing step reduce grounding size. Concretely, applying LUP rules

x : R(x)
x : R(x) : P (x, y)
derives second formula follows rst hence need
grounded all. theory used check whether LUP remains equally important
system lazy grounding.

x :
R(x) : P (x, y). Standard grounding creates formula instance
x Tseitin grounding : P (d, y). Naive lazy grounding creates
extra Tseitin instance x extra set Tseitins piece piece
grounding subformula : P (d, y).

5. Overhead case nested universal quantication. sentence form

6. Overhead incremental querying case symbolic justication validated. sentence

x : R(x) S(x),

identical justication formula.

formula validated checking falsity query
query re-evaluated time

R-atom



-atom

x : R(x) S(x).



falsied.

6.1.1 Results
Experiments done predicates

P



Q

arity 3

R





arity 2,

domains size 10, 20, 30, 40 50. None predicates symbols interpreted
structure.
experiments, overhead time required solve initial optimization
problem (for global approach) always around 0.02 seconds, negligible.
results rst three experiments shown dierences standard
grounding naive lazy grounding negligible.

expected experiment 2,

experiments 1 3, shows actual implementation eliminates overhead
Tseitins quantiers nested. three experiments, standard lazy
grounding able justify formulas without grounding hence fast almost
insensitive domain size. shown Figure 4, dierence standard
grounding naive lazy grounding experiment 4. cases, use LUP
big impact size grounding hence time. experiment 1 3
showed top level quantier create overhead lazy grounding, experiment 5
shows hold anymore nested quantiers naive lazy grounding
substantial overhead compared standard grounding. Note overhead
worst case.

Tseitins justied, denitions grounded,

explains normal lazy grounding faster standard grounding insensitive
domain size.

Experiment 6 shows complex justication formula causes

signicant overhead naive lazy grounding. Also here, overhead worst case

271

fiDe Cat, Denecker, Stuckey & Bruynooghe

4. Grounding bounds

5. Nested universal quantification

16

6. Complex justification, shared variables
4.5

14

ground without LUP
ground LUP
naive lazy ground without LUP
naive lazy ground LUP
lazy ground LUP

14
12

ground
naive lazy-ground
lazy-ground

12

ground
naive lazy-ground
lazy-ground

4.0
3.5

10
3.0

8

Seconds

Seconds

Seconds

10
8

6

2.5
2.0

6
1.5
4

4

1.0

2

2

0

0
0

10

20

30

40

50

0.5
0.0
0

10

Domain size

20

30

Domain size

40

50

0

10

20

30

40

50

Domain size

Figure 4: Time overhead naive lazy grounding ground-and-solve completely
grounding input theory, benchmarks 4, 5 6. time includes grounding, solving time needed nd justications. time required
standard lazy grounding algorithm also shown comparison.

visible normal lazy grounding. Still, important part future research reduce
overhead incremental querying complex justication formulas.

6.2 ASP Competition Benchmarks
Second, selected benchmarks previous ASP competitions evaluate lazy
grounding algorithm realistic setting. Many benchmarks solutions competition carefully ne tuned speed minimal grounding. Lazy grounding usually
unable substantially reduce grounding theories and, due overhead,
slower standard ground solve. reason, sometimes selected
modelings benchmarks natural less optimized time grounding size. justify ground aim work improve inference
declarative

modeling

(De Cat et al., 2014), emphasis developing intricate

encodings, modeling problem close natural language specication.
selected following problems (see competition websites complete descriptions). consist problems known hard, order evaluate eect
lazy model expansion search, problems typically result large grounding.

Reachability:

Given directed graph, determine whether path exists two

given nodes.

Labyrinth:

planning problem agent traverses graph moving

connected nodes reach given goal node. addition, graph manipulated
change connectedness.

Packing:

Given rectangle number squares, squares grid

without overlaps.

Disjunctive Scheduling:

Schedule number actions given earliest start

latest end time additional constraints precedence disjointness.

272

fiLazy Model Expansion: Interleaving Grounding Search
# inst.

# solved

g&s

benchmark

Sokoban
Disj. Sched.
Packing
Labyrinth
Reachability
Stable Marr.
Graph Col.

50

44

21

5

lazy
25

50

44

21
44

261

83

72

16

2

106

21

60

16
94

34

12

avg. time (sec.)

ASP

50

g&s

lazy

ASP

102

59

5

130

207

6

173

121

20
5

196

245

181
40

141
4

110

12

5

643

402

18

211

21

437
44
85

Table 1: number solved instances ASP benchmarks average time taken
solved instances. Dierent solvers solve quite dierent sets instances.

Sokoban:

planning problem robot push number blocks goal

positions, constrained 2-D maze.

Graph Colouring:

Given graph, assign colour nodes (from given set colours),

connected nodes colour.

Stable Marriage:

Given set men women set preferences, nd

stable assignment: swap results better match.
these, used instances 2011 2013 competitions, except
2013

Reachability

instances, huge data les none systems

Stable Marriage, Graph Colouring Reachability,
Packing Disjunctive
IDP
Scheduling, constructed natural FO() encoding made faithful translation
ASP. complex benchmarks Labyrinth Sokoban, used original
FO()IDP Gringo-Clasp's ASP specications submitted 2011 competition.

designed handle.



based encodings available ASP-Core-2 encodings.

lazy model expansion, replaced cardinality expressions FO encoding
former justications derived yet; also increases size full grounding.

6.2.1 Results
number solved instances average time shown Table 1; average grounding
size

IDP

7

setup shown Table 2.

time grounding size, unsolved instances

Reachability (9 times g&s,
ASP), Disjunctive Scheduling (6 times ASP) Labyrinth (160 times g&s,
ASP), Packing (4 times g&s, 4 times lazy, 30 times ASP) Stable
Marriage (66 times ASP); unsolved instances caused time-out.8
taken account. Memory overows happened
9 times

7. Grounding consists variable instantiation interleaved formula simplication (e.g., dropping false
disjuncts, true conjuncts, replacing disjunctions true disjuncts true conjunctions false
conjunctions false, etc). simplication steps may seriously reduce grounding size.
8.

IDP

automatic symmetry breaking, cause dierence

Colouring.

273

g&s



ASP



Graph

fiDe Cat, Denecker, Stuckey & Bruynooghe
ground size (# atoms)
benchmark

Sokoban
Disj. Sched.
Packing
Labyrinth
Reachability
Stable Marr.
Graph Col.

g&s
2.65 104
5.17 106
3.86 107
1.68 106
2.87 107
2.11 107
1.15 104

lazy
2.90 105
2.72 106
1.69 107
1.38 106
1.61 104
1.20 107
1.58 104

ground time

ASP
4.63 104
8.04 105
4.53 106
3.55 105
1.35 106
3.36 106
2.80 104

Table 2: average grounding size number
marks, setups.
taken.

g&s



ASP,

lazy

solved

g&s(sec.)

ASP

2.0
129.7
165.6
101.0
109.7
642.7

0.1

(sec.)

0.3
0.7
4.7
2.3
14.5
3.2
0.1

instances ASP bench-

setup, size nal ground theory

average grounding time also shown.

results show lazy model expansion solved instances setups
four seven cases. cases, problems also got solved signicantly
time threshold.

seven cases, (nal) grounding size smaller

lazy model expansion, orders magnitude one case.

Colouring,

Sokoban, Labyrinth Graph

lazy model expansion outperformed ground-and-solve, indicating

Sokoban,
lazy grounding size even higher g&s (possible due FO encoding
cardinalities), indicating large part search space explored. Stable
Marriage, relatively small dierence grounding size g&s lazy leads us
loss information outweighed gain grounding less up-front. E.g.,
nal

believe dierent search heuristic main factor, lazy grounding itself.
also experimented

Airport Pickup ASP-2011 benchmark, fairly standard

scheduling problem (transporting passengers taxis taking account fuel consumption)
except upper bound time provided.

9

Hence ground-and-solve approach

would need construct innite grounding. Applying straightforward lazy model expansion also resulted grounding large. However, prototype uses
late grounding heuristic described Section 5.1,

IDP

solved one ten instances.

others, grounding problem, search took long
time intervals

1..n

considered get sucient

n

solve problem (even

standard search heuristic).
presented results show that, although often benecial, lazy model expansion
considerable overhead hard search problems. hand, inspecting
outcome experiments, observed class specications instances
solved lazy grounding traditional grounding partially overlap. suggests
might good idea integrate approaches

portfolio

system. system

either select heuristically whether use ground-and-solve lazy model expansion
(based input) running parallel, aborting either one uses much
memory. However, problems considered, lazy model expansion could start search
9. possible derive nite worst-case thresholds Airport Pickup problem. is, however,
part original specication.

274

fiLazy Model Expansion: Interleaving Grounding Search
much earlier ground-and-solve, even though got lost often search.
leads us believe realize full potential lazy grounding, work necessary
developing suitable heuristics (possibly user-specied ones).

6.3 Specic Experiments
addition ASP competition benchmarks, experiments conducted using
crafted benchmarks illustrate specic properties lazy grounding algorithm.
rst part Table 3 shows results scalability experiments.
benchmarks

Packing, Sokoban



Disjunctive Scheduling,

selected simple prob-

lem instance gradually extended domain size orders magnitude: size
grid (Packing) number time points (Sokoban,

Disjunctive Scheduling).



results show instances, lazy model expansion scales much better
ground-and-solve strategies
satisable instances. However,
signicantly.

IDP

Gringo-Clasp satisable well un-

Disjunctive Scheduling solving time still increases

reason lazy heuristics still naive make uninformed

choices often.
mentioned previous section, ASP competition problems typically small
groundings since running benchmarks large system handle
provide useful comparison systems. Hence, also evaluated lazy model expansion
number crafted benchmarks grounding non-trivial.
work look practical applications type.

part future

constructed following

benchmarks:

Dynamic reachability,


Lazy evaluation

example described Section 3.3.

procedurally interpreted

prime numbers.

symbols, using simple theory

described Section 5.2, predicate symbol

isP rime/1



interpreted procedure returns true argument prime.

function



predicate encoding



experiment simulates model generation theory unknown domain.

huge domain.

used/1; quantied formulas
x : (used(x) ); model
6
domain size 10 .

unknown domain expressed new predicate

x :

translated

x : (used(x) )



x :

generation simulated model expansion



one, faithful ASP encoding constructed. second part Table 3 shows
results benchmarks. show signicant improvement lazy model expansion ground-and-solve examples: case,
memory overow grounding,


Disjunctive Scheduling,

lazy

g&s



ASP

went

found solutions within seconds. However,

also evident lazy approach would benet

improved heuristics: increasing domain size signicantly increases solving time,
instances intrinsically harder.

6.3.1 Closer Inherent Complexity?
modeling phase application, dierent encodings typically tested out,
attempt improve performance locate bugs. modeling experimental

275

fiDe Cat, Denecker, Stuckey & Bruynooghe
benchmark

packing-10
packing-25
packing-50
sokoban-103
sokoban-104
sokoban-105
disj-sched-sat-103
disj-sched-sat-104
disj-sched-sat-105
disj-sched-unsat-103
disj-sched-unsat-104
disj-sched-unsat-105
dynamic reachability
procedural
function
modelgeneration

lazy

g&s

ASP

0.2

2.0

0.1

0.3

2.0

0.1

1.1

10.03

5.8

0.31

0.3

0.1

0.5

20.0

1.1

2.6



68.0

0.39

0.49

0.07
17.44

13.04

16.05

164.18





0.24

0 49

0.09

4.11

16.04

19.85













164.2
0.18
1.24
0.79
0.19

Table 3: solving time additional crafted benchmarks, one instance each.

benchmarks, noticed simplifying theory dropping constraints often resulted
dramatic reduction time lazy model expansion took nd model. Standard
model expansion, hand, much less aected simplications.



opinion, observation, hardly denitive evidence, another indication
presented algorithms able derive justications parts theory
satised cheaply. way, approach able distinguish better problems
inherently dicult problems would large grounding.

7. Related Work
Lazy model expansion oers solution blow-up grounding often occurs
ground-and-solve model expansion methodology

FO(ID )

theories.

Answer Set

Programming (ASP) SAT Modulo Theories (SMT) techniques also process theories
large grounding; constraint store Constraint Programming (CP) Mixed
Integer Programming clauses SAT considered equivalent grounded
theory (they often derived quantied descriptions ci

j

< cj



. . . ) also become large. Lefvre Nicolas (2009) Ge

de Moura (2009) reported blow-up problem paradigms multitude
techniques developed address it. distinguish four approaches.
First, concerning grounding up-front, research done towards

grounding

static analysis
ii

( )

reducing size

input derive bounds variable

instantiations (Wittocx et al., 2010, 2013), ( ) techniques

compile

specic types sen-

tences compact ground sentences (Tamura et al., 2009; Metodi & Codish, 2012),

iii) detect parts evaluated polynomially (Leone et al., 2006; Gebser et al., 2011;
iv) detect parts relevant task hand (e.g.,

(

Jansen et al., 2013) (

276

fiLazy Model Expansion: Interleaving Grounding Search
context query problems) shown work Leone et al. (2006). Naturally,
approaches used conjunction lazy grounding reduce
size grounding.

IDP,



e.g., lazy grounding already combined ( ) (

Second, size grounding reduced

enriching

language.

iii).

ex-

ample, ASP solvers typically support ground aggregates (interpreted second-order functions
cardinality sum take sets arguments), CP SMT solvers support
(uninterpreted) functions. recently, Constraint-ASP paradigm developed (Ostrowski & Schaub, 2012), integrates ASP CP extending ASP language

constraint

atoms. interpreted constraints CSP problem thus

handled using CP techniques. Various CASP solvers already available, Clingcon (Ostrowski Schaub), Ezcsp (Balduccini, 2011), Mingo (Liu, Janhunen, & Niemel,
2012) Inca (Drescher & Walsh, 2012). technique also integrated
Cat et al., 2013).

Inca

IDP

IDP

(De

fact implement Lazy Clause Generation (Ohrimenko

et al., 2009), optimized form lazy grounding specic types constraints.
language HEX-ASP (Eiter et al., 2005) also extends ASP, time

external



atoms

represent (higher-order) external function calls.
Third,

incremental approaches

well-known model generation, theorem proving

planning. tasks, domain typically xed advance, part
structure sought, number time steps planning problem (recall
Sokoban example introduction). approach typically works grounding
problem initial guess (the number elements in) domain.

Afterwards,

search applied; model found, domain extended grounding
done. iterated model found bound maximum domain size
hit (if one known).

technique applied, e.g., prover Paradox (Claessen &

Srensson, 2003) ASP solver IClingo (Gebser et al., 2008).
Fourth, closest lazy grounding itself, large body research devoted
delaying grounding specic types expressions necessary (for example
result propagation). Propagation techniques rst-order level delay grounding
propagation ensues researched within ASP (Lefvre & Nicolas, 2009; Dal
Pal et al., 2009; Dao-Tran et al., 2012) within CP (Ohrimenko et al., 2009).



techniques used conjunction lazy grounding derive intelligent
justications specic types constraints presented here. example, Dao-Tran et
al. also presented ecient algorithm bottom-up propagation denition. Within
SMT, various theory propagators work lazily transforming theory SAT,
theory Bit Vectors Bruttomesso et al. (2007).

Ge de Moura (2009)

investigated quantier handling combining heuristic instantiation methods research
decidable fragments FO theories, eciently checked models.
Within ASP, work done goal-directed reasoning. Bonatti, Pontelli,
Son (2008) Marple, Bansal, Min, Gupta (2012) demonstrate approaches,
style SLD resolution, apply top-down instantiation answer queries innite
domains. Saptawijaya Pereira (2013) extend abduction framework lazily generate
part relevant sentences. search algorithms, justications (or

watches )

used

derive constraint result propagation already satised, hence need
checked propagation phase. Nightingale et al. (2013) show maintaining
(short) justications signicantly reduce cost propagation phase.

277

fiDe Cat, Denecker, Stuckey & Bruynooghe
fact, well-known technique already exists combines search lazy instantiation quantiers, namely

skolemization,

existentially quantied variables re-

placed newly introduced function symbols.

Universal quantications handled

instantiating introduced function symbols.

Reasoning consistency can,

e.g., achieved congruence closure algorithms, capable deriving consistency without eectively assigning interpretation function symbols.

techniques

used Tableau theorem proving (Hhnle, 2001) SMT solvers (Detlefs, Nelson, & Saxe,
2005).

Formula (Jackson, Bjorner, & Schulte, 2013) interleaves creating ground pro-

gram giving SMT solver, iterating symbolic guesses proved wrong.
Skolemization-based techniques typically work well case small number constants
needs introduced, diculty case relevant domain large. One also
see lazy grounding (with support function symbols) could incorporate skolemization adapting rules grounding existential universal quantication. expect
skolemization complementary lazy grounding, in-depth investigation part
future work.
eld probabilistic inference, several related techniques developed
also rely lazy instantiation. First, Problog system uses form static dependency
analysis ground (probabilistic) program context given query, constructing
possible ways derive query top-down fashion (Kimmig et al., 2011). Second,
so-called

lazy inference,

applied e.g.

LazySAT

fact that, considered inference, (xed)

(Singla & Domingos, 2006), exploits

default

assumption exists

expression certainly contribute probabilities.

Hence, expressions

assumption certainly holds considered search. Third,

cutting plane inference

(Riedel, 2009) applies lazy inference interleaved setting,

constructing part program assumptions satised.

8. Future Work
Several aspects presented work need investigation. One aspect extending
support lazily ground complex expressions, including aggregate expressions

P
( xD P (x) f (x)) > 3,
atom P (d) true, , P

(nested) function terms. Consider example sentence
expresses sum terms
predicate

f

f (d)



function, larger 3. One observe necessary

ground whole sentence up-front.
(hence positive), set

example,

{P (d1 ), f (d1 ) > 3}

f

maps natural numbers

minimal justication.

Even easy

justication found, suce grounding part sentence delay

P

P (d1 ) f (d1 )) > 3 ,
P
P
Tseitin symbol dened (
P (d1 ) f (d1 )) + ( xD\d1 P (x) f (x)) > 3. Indeed,
model sentence false, original inequality satised.

remainder.

example, create ground sentence

(

second aspect whether advantages grounding earlier, example
guarantee propagation lost, grounding later, possibly reducing size grounding even more. example, consider sentences

P



P ,









large formulas justication found. Instead grounding least one
sentences, might add

P

list atoms search algorithm assign

278

fiLazy Model Expansion: Interleaving Grounding Search
ground either sentences

P

assigned value (it might even

unsatisability detected grounding either one).
Given lazy grounding useful, lazy

forgetting

grounded theory?

ground theory extended making structure precise, ground theory
could reduced backtracking.

storing justication violations

caused grounding, derive grounding forgotten violation
longer problematic (e.g., backtracking). this, algorithm needs developed
tracks grounding/splitting dependencies rules given justications.
closely resembles techniques used tableau theorem proving SMT, theory
hand compacted moving dierent part search space.
approach described lazy grounding also applied answer set generation
eld ASP. ASP, logic program stable semantics seen one rule
set, single denition. However, ASP programs satisfy major condition
apply lazy grounding. Indeed programs typically non-total, due presence
constraints rules form

p np, np p



choice rules

result

multiple stable models. However, described Denecker et al. (2012), practical
ASP programs partitioned set choice rules, set

total

denitions set

constraints (the so-called Generate-Dene-Test partition). ASP program
GDT-partitioned, translated straightforwardly equivalent

FO(ID )

theory

contains total denitions. suggests way apply lazy grounding
ASP programs.

9. Conclusion
Solvers used domains SAT, SMT ASP often confronted problems
large ground. Lazy model expansion, technique described paper,
interleaves grounding search order avoid grounding bottleneck. technique
builds upon concept justication, deterministic recipe extend interpretation
satises certain constraints. theoretical framework developed lazy
model expansion language

FO(ID ) algorithms presented derive

maintain justications interleave grounding state-of-the-art CDCL search
algorithms.

framework aims bounded model expansion, domains

nite, also initial step towards handling innite domains eciently. Experimental
evaluation provided, using implementation

IDP

system, lazy

model expansion compared state-of-the-art ground-and-solve approach.



experiments showed considerable improvement ground-and-solve existing benchmarks
well new applications. main disadvantage less-informed search algorithm,
caused delay propagation introduction additional symbols. possible
solution develop new heuristics portfolio approaches combine strengths
methods. Finally, indicated way proposed methods applied
beyond

FO(ID ),

ASP solvers general.

279

fiDe Cat, Denecker, Stuckey & Bruynooghe
Acknowledgements
research, Broes De Cat funded Agency Innovation Science
Technology Flanders (IWT). research also supported FWO-Vlaanderen
project GOA 13/010, Research Fund KULeuven.

NICTA funded

Australian Government Department Communications Australian
Research Council ICT Centre Excellence Program.

Appendix A. Details Algorithms
appendix, mention parameter values well optimizations
reduce grounding overhead and/or improve search. optimization, indicate currently implemented (and part experimental results) part
future work.

A.1 Parameter Values
5.1, number parameters introduced control behavior lazy model
expansion.

Here, provide details values used experimental evaluation.

values set manually, based experience limited number observations
(e.g., extension threshold works similar conict threshold SAT solver).
part future work study impact dierent values.



existential quantication, 10 instantiations grounded time; disjunction, 3 disjuncts grounded time. turned give best balance
introducing many Tseitin atoms grounding much.



initial truth value



initial threshold randomized restarts 100 extensions ground theory.



probability

0.2



f

otherwise.

doubled restart.



formula considered small estimated grounding size

104

atoms.

A.2 Extension FO()IDP
far, described lazy model expansion algorithm function-free
However,

FO()IDP ,

knowledge-base language

IDP

FO(ID ).

system, supports much richer

input language. Besides types use initialize domains also supports
(partial) functions, aggregates arithmetic.

current implementation ignores

latter extensions straightforward adaptation build_djust (Algorithm 4):
case literals extended return

FO(ID )

false

literal part function-free

language. example, given rule

justication

Q(f (x))

h x : P (x) Q(f (x)), P (x) used

cannot. functions, also option replace

graph predicates preprocessing step. experiments Section 6.2,
functions, any, given input structure hence play role.
part future work extend lazy grounding extensions, especially
functions. Techniques developed SMT Constraint Programming handle (ground)

280

fiLazy Model Expansion: Interleaving Grounding Search
atoms containing function symbols useful reduce size grounding improve
search. previous work, techniques integrated

IDP

system (De Cat

et al., 2013) certainly worthwhile fully integrate lazy grounding.

A.3 Cheap Propagation Checks.
lazy_mx, checked assigned literal whether dened



whether

violates justications. implement cheaply, implementation maintains
mapping literals

g .

states whether literal dened



also lists

justications negation occurs. mapping extended whenever new literal
added

g

maintained whenever justications change.

performance

search loop unaected long literals assigned mapping empty.

A.4 Stopping Early
Algorithm 2, took standard stopping criterion used search algorithms
(Line 14): stop conict-free state



two-valued symbols

principle, may stop earlier, partial structure

PT .

Indeed, Corollary 3.6 tells us





Tg g .



admits total justication

expanded model.

dened
irrelevant (in eect, appear justication) trigger grounding
A's denition, turn might introduce new literals dened , causing cascade

considerable impact grounding size. Indeed, assigning truth value atom


unnecessary groundings assignments.
justication

g ,

solver algorithm maintain

cannot know exactly justication exists.

Instead,

implemented algorithm chooses literals watched formula/rule.



stops partial structure unwatched literals may assigned.
shown suces guarantee



admits justication. Hence safe stop

search.

A.5 Approximate Justications
cases, build_djust cannot nd valid justication large formula

I.
false least one atom P
literals already false

example formula

x : P (x),

build_djust returns

false. Instead, adapted build_djust heuristic

check number expected violations. small enough, justication still
returned. Naturally, required check whether real violations,
querying justication formula

I,

apply lazy_ground them.

References
Alviano, M., Calimeri, F., Charwat, G., Dao-Tran, M., Dodaro, C., Ianni, G., Krennwallner,
T., Kronegger, M., Oetsch, J., Pfandler, A., Phrer, J., Redl, C., Ricca, F., Schneider,
P., Schwengerer, M., Spendier, L. K., Wallner, J. P., & Xiao, G. (2013). fourth
Answer Set Programming competition: Preliminary report.
T. C. (Eds.),
Apt, K. R. (2003).

Cabalar, P., & Son,

LPNMR, Vol. 8148 LNCS, pp. 4253. Springer.
Principles Constraint Programming. Cambridge University Press.
281

fiDe Cat, Denecker, Stuckey & Bruynooghe
Balduccini, M. (2011).

Industrial-size scheduling ASP+CP.

Delgrande, J. P., &

LPNMR, Vol. 6645 LNCS, pp. 284296. Springer.
Knowledge Representation, Reasoning, Declarative Problem Solving.

Faber, W. (Eds.),
Baral, C. (2003).

Cambridge University Press, New York, NY, USA.
Bonatti, P. A., Pontelli, E., & Son, T. C. (2008).

Credulous resolution answer set

programming. Fox, D., & Gomes, C. P. (Eds.),

AAAI, pp. 418423. AAAI Press.

Bruttomesso, R., Cimatti, A., Franzn, A., Griggio, A., Hanna, Z., Nadel, A., Palti, A.,
& Sebastiani, R. (2007).
verication problems.

lazy layered SMT(BV) solver hard industrial

Damm, W., & Hermanns, H. (Eds.),

LNCS, pp. 547560. Springer.

CAV,

Vol. 4590

Calimeri, F., Ianni, G., & Ricca, F. (2014). third open answer set programming competition.

TPLP, 14 (1), 117135.

Chen, W., & Warren, D. S. (1996). Tabled evaluation delaying general logic programs.

J. ACM, 43 (1), 2074.

Claessen, K., & Srensson, N. (2003).

New techniques improve MACE-style model

Proceedings CADE-19 Workshop: Model Computation - Principles,
Algorithms, Applications.
nding.

Dal Pal, A., Dovier, A., Pontelli, E., & Rossi, G. (2009). Answer set programming
constraints using lazy grounding. Hill, P. M., & Warren, D. S. (Eds.),
5649

LNCS, pp. 115129. Springer.

Dao-Tran, M., Eiter, T., Fink, M., Weidinger, G., & Weinzierl, A. (2012).

ICLP,

Vol.

Omiga :

open minded grounding on-the-y answer set solver. del Cerro, L. F., Herzig, A.,

JELIA, Vol. 7519 LNCS, pp. 480483. Springer.
Cat, B. (2014). Separating Knowledge Computation: FO(.) Knowledge Base
System Model Expansion Inference. Ph.D. thesis, KU Leuven, Leuven, Belgium.
& Mengin, J. (Eds.),

De

De Cat, B., Bogaerts, B., Bruynooghe, M., & Denecker, M. (2014).
modelling language: IDP system.

CoRR, abs/1401.6312.

Predicate logic

De Cat, B., Bogaerts, B., Devriendt, J., & Denecker, M. (2013). Model expansion
presence function symbols using constraint programming.

ICTAI, pp. 10681075.

IEEE.
De Cat, B., Denecker, M., & Stuckey, P. J. (2012). Lazy model expansion incremental
grounding. Dovier, A., & Costa, V. S. (Eds.),

ICLP (Technical Communications),

LIPIcs, pp. 201211. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik.
Delgrande, J. P., & Faber, W. (Eds.). (2011). Logic Programming Nonmonotonic Reasoning - 11th International Conference, LPNMR 2011, Vancouver, Canada, May 16-19,
2011. Proceedings, Vol. 6645 LNCS. Springer.
Vol. 17

Denecker, M. (1998). well-founded semantics principle inductive denition.
Dix, J., del Cerro, L. F., & Furbach, U. (Eds.),
Springer.

282

JELIA, Vol. 1489 LNCS, pp. 116.

fiLazy Model Expansion: Interleaving Grounding Search
Denecker, M. (2000). Extending classical logic inductive denitions. Lloyd, J. W.,
Dahl, V., Furbach, U., Kerber, M., Lau, K.-K., Palamidessi, C., Pereira, L. M., Sagiv,
Y., & Stuckey, P. J. (Eds.),

CL, Vol. 1861 LNCS, pp. 703717. Springer.

Denecker, M., Bruynooghe, M., & Marek, V. W. (2001). Logic programming revisited: Logic
programs inductive denitions.

ACM Trans. Comput. Log., 2 (4), 623654.

Denecker, M., & De Schreye, D. (1992). Justication semantics: unifying framework
semantics logic programs.

Tech. rep. 157, Department Computer Science,

K.U.Leuven.
Denecker, M., & De Schreye, D. (1993). Justication semantics: unifying framework
semantics logic programs. Pereira, L. M., & Nerode, A. (Eds.),

LPNMR, pp.

365379. MIT Press.
Denecker, M., Lierler, Y., Truszczynski, M., & Vennekens, J. (2012).
mal semantics answer set programming.

ICLP (Technical Communications),

Vol. 17

Tarskian infor-

Dovier, A., & Costa, V. S. (Eds.),

LIPIcs,

pp. 277289. Schloss Dagstuhl

- Leibniz-Zentrum fuer Informatik.
Denecker, M., & Ternovska, E. (2008). logic nonmonotone inductive denitions.

Trans. Comput. Log., 9 (2), 14:114:52.

ACM

Denecker, M., & Vennekens, J. (2014). well-founded semantics principle inductive denition, revisited. Baral, C., De Giacomo, G., & Eiter, T. (Eds.),

KR,

pp.

2231. AAAI Press.
Denecker, M., Vennekens, J., Bond, S., Gebser, M., & Truszczyski, M. (2009). second
answer set programming competition.

Erdem, E., Lin, F., & Schaub, T. (Eds.),

LPNMR, Vol. 5753 LNCS, pp. 637654. Springer.
Detlefs, D., Nelson, G., & Saxe, J. B. (2005).
checking.

J. ACM, 52 (3), 365473.

Simplify: theorem prover program

Technical Communications 28th International Conference Logic Programming, ICLP 2012, September 4-8, 2012, Budapest,
Hungary. Proceedings, Vol. 17 LIPIcs. Schloss Dagstuhl - Leibniz-Zentrum fuer In-

Dovier, A., & Costa, V. S. (Eds.). (2012).

formatik.
Drescher, C., & Walsh, T. (2012). Answer set solving lazy nogood generation. Dovier,
A., & Costa, V. S. (Eds.),

ICLP (Technical Communications),

Vol. 17

LIPIcs,

pp.

188200. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik.
Eiter, T., Ianni, G., Schindlauer, R., & Tompits, H. (2005). uniform integration higherorder reasoning external evaluations answer-set programming.

Kaelbling,

IJCAI, pp. 9096. Professional Book Center.
Mathematical Introduction Logic (Second edition).

L. P., & Saotti, A. (Eds.),
Enderton, H. B. (2001).

Academic

Press.

Logic Programming Nonmonotonic Reasoning, 10th International Conference, LPNMR 2009, Potsdam, Germany, September
14-18, 2009. Proceedings, Vol. 5753 LNCS. Springer.

Erdem, E., Lin, F., & Schaub, T. (Eds.). (2009).

283

fiDe Cat, Denecker, Stuckey & Bruynooghe
Ge, Y., & de Moura, L. M. (2009). Complete instantiation quantied formulas satisabiliby modulo theories. Bouajjani, A., & Maler, O. (Eds.),

LNCS, pp. 306320. Springer.

CAV,

Vol. 5643

Gebser, M., Kaminski, R., Kaufmann, B., Ostrowski, M., Schaub, T., & Thiele, S. (2008).
Engineering incremental ASP solver.
(Eds.),

Garca de la Banda, M., & Pontelli, E.

ICLP, Vol. 5366 LNCS, pp. 190205. Springer.

Gebser, M., Kaminski, R., Knig, A., & Schaub, T. (2011). Advances Gringo series 3.
Delgrande, J. P., & Faber, W. (Eds.),

LPNMR,

Vol. 6645

LNCS,

pp. 345351.

Springer.
Gebser, M., Schaub, T., & Thiele, S. (2007).

GrinGo : new grounder Answer Set

Programming. Baral, C., Brewka, G., & Schlipf, J. S. (Eds.),

LNCS, pp. 266271. Springer.

Hhnle, R. (2001).
(Eds.),

Tableaux related methods.

LPNMR, Vol. 4483

Robinson, J. A., & Voronkov, A.

Handbook Automated Reasoning, pp. 100178. Elsevier MIT Press.

Jackson, E. K., Bjorner, N., & Schulte, W. (2013).

Open-world logic programs: new

foundation formal specications. Tech. rep. MSR-TR-2013-55, Microsoft Research.
Jansen, J., Jorissen, A., & Janssens, G. (2013). Compiling input

3
tabled Prolog rules IDP .

FO() inductive denitions

TPLP, 13 (4-5), 691704.

Karp, R. (1972). Reducibility among combinatorial problems. Miller, R., & Thatcher, J.
(Eds.),

Complexity Computer Computations, pp. 85103. Plenum Press.

Kimmig, A., Demoen, B., De Raedt, L., Santos Costa, V., & Rocha, R. (2011).
implementation probabilistic logic programming language ProbLog.

11 (2-3), 235262.

Lefvre, C., & Nicolas, P. (2009).



TPLP,

rst version new ASP solver: ASPeRiX.

Erdem, E., Lin, F., & Schaub, T. (Eds.),

LPNMR,

Vol. 5753

LNCS,



pp. 522527.

Springer.
Leone, N., Pfeifer, G., Faber, W., Eiter, T., Gottlob, G., Perri, S., & Scarcello, F. (2006).
DLV system knowledge representation reasoning.

Log., 7 (3), 499562.

ACM Trans. Comput.

Liu, G., Janhunen, T., & Niemel, I. (2012). Answer Set Programming via Mixed Integer
Programming.

Brewka, G., Eiter, T., & McIlraith, S. A. (Eds.),

KR,

pp. 3242.

AAAI Press.
Marek, V. W., & Truszczyski, M. (1999).
gramming paradigm.
D. S. (Eds.),

Stable models alternative logic pro-

Apt, K. R., Marek, V. W., Truszczyski, M., & Warren,

Logic Programming Paradigm: 25-Year Perspective,

pp. 375398.

Springer-Verlag.
Marin, M. (2009).

Model Generation ID-Logic.

Ph.D. thesis, Department Computer

Science, KU Leuven, Belgium.
Marin, M., Gilis, D., & Denecker, M. (2004). relation ID-Logic answer
set programming. Alferes, J. J., & Leite, J. A. (Eds.),
pp. 108120. Springer.

284

JELIA,

Vol. 3229

LNCS,

fiLazy Model Expansion: Interleaving Grounding Search
Marin, M., Wittocx, J., Denecker, M., & Bruynooghe, M. (2008). SAT(ID): Satisability
propositional logic extended inductive denitions. Kleine Bning, H., & Zhao,
X. (Eds.),

SAT, Vol. 4996 LNCS, pp. 211224. Springer.

Marple, K., Bansal, A., Min, R., & Gupta, G. (2012). Goal-directed execution answer
set programs. Schreye, D. D., Janssens, G., & King, A. (Eds.),

PPDP,

pp. 3544.

ACM.
Marques Silva, J. P., Lynce, I., & Malik, S. (2009).

Conict-driven clause learning SAT

Handbook
Satisability, Vol. 185 Frontiers Articial Intelligence Applications, pp. 131
solvers.

Biere, A., Heule, M., van Maaren, H., & Walsh, T. (Eds.),

153. IOS Press.
Metodi, A., & Codish, M. (2012). Compiling nite domain constraints SAT BEE.

TPLP, 12 (4-5), 465483.

Mitchell, D. G., & Ternovska, E. (2005).

framework representing solving NP

search problems. Veloso, M. M., & Kambhampati, S. (Eds.),

AAAI,

pp. 430435.

AAAI Press / MIT Press.
Mitchell, D. G., Ternovska, E., Hach, F., & Mohebali, R. (2006).

Model expansion

framework modelling solving search problems. Tech. rep. TR 2006-24, Simon
Fraser University, Canada.
Nethercote, N., Stuckey, P., Becket, R., Brand, S., Duck, G., & Tack, G. (2007). Minizinc:
Towards standard CP modelling language. Bessiere, C. (Ed.),


LNCS, pp. 529543. Springer.

CP'07,

Vol. 4741

Nightingale, P., Gent, I. P., Jeerson, C., & Miguel, I. (2013). Short long supports
constraint propagation.

J. Artif. Intell. Res. (JAIR), 46, 145.

Ohrimenko, O., Stuckey, P. J., & Codish, M. (2009). Propagation via lazy clause generation.

Constraints, 14 (3), 357391.

Ostrowski, M., & Schaub, T. (2012).

12 (4-5), 485503.

ASP modulo CSP: clingcon system.

Riedel, S. (2009). Cutting plane MAP inference Markov logic.

Statistical Relational Learning (SRL-2009).

Saptawijaya, A., & Pereira, L. M. (2013).

TPLP,

International Workshop

Towards practical tabled abduction logic

programs. Correia, L., Reis, L. P., & Cascalho, J. (Eds.),

EPIA, Vol. 8154 LNCS,

pp. 223234. Springer.
Singla, P., & Domingos, P. (2006). Memory-ecient inference relational domains. Gil,
Y., & Mooney, R. J. (Eds.),

AAAI, pp. 488493. AAAI Press.

Son, T. C., Pontelli, E., & Le, T. (2014).

Two applications ASP-Prolog system:

Decomposable programs multi-context systems. Flatt, M., & Guo, H.-F. (Eds.),

PADL, Vol. 8324 Lecture Notes Computer Science, pp. 87103. Springer.

Tamura, N., Taga, A., Kitagawa, S., & Banbara, M. (2009). Compiling nite linear CSP
SAT.

Constraints, 14 (2), 254272.

285

fiDe Cat, Denecker, Stuckey & Bruynooghe
Torlak, E., Chang, F. S.-H., & Jackson, D. (2008). Finding minimal unsatisable cores
declarative specications. Cullar, J., Maibaum, T. S. E., & Sere, K. (Eds.),
Vol. 5014

LNCS, pp. 326341. Springer.

FM,

Tseitin, G. S. (1968). complexity derivation propositional calculus. Slisenko,
A. O. (Ed.),

Studies Constructive Mathematics Mathematical Logic II, pp. 115

125. Consultants Bureau, N.Y.
Van Gelder, A. (1993). alternating xpoint logic programs negation.

Syst. Sci., 47 (1), 185221.

J. Comput.

Vennekens, J., Marin, M., Wittocx, J., & Denecker, M. (2007). Predicate introduction
logics xpoint semantics. Part I: Logic programming.

79 (1-2), 187208.

Fundamenta Informaticae,

Wittocx, J., Denecker, M., & Bruynooghe, M. (2013). Constraint propagation rst-order
logic inductive denitions.

ACM Trans. Comput. Logic, 14 (3), 17:117:45.

Wittocx, J., Marin, M., & Denecker, M. (2008).

idp system: model expansion system

extension classical logic. Denecker, M. (Ed.),

LaSh, pp. 153165. ACCO.

Wittocx, J., Marin, M., & Denecker, M. (2010). Grounding FO FO(ID) bounds.

J. Artif. Intell. Res. (JAIR), 38, 223269.

286

fiJournal Artificial Intelligence Research 52 (2015) 477-505

Submitted 10/14; published 04/15

Case-Based Reasoning Framework Choose Trust Models
Different E-Marketplace Environments
Athirai A. Irissappane
Jie Zhang

ATHIRAI 001@ E . NTU . EDU . SG
ZHANGJ @ NTU . EDU . SG

School Computer Engineering
Nanyang Technological University, Singapore

Abstract
performance trust models highly depend characteristics environments
applied. Thus, becomes challenging choose suitable trust model given
e-marketplace environment, especially ground truth agent (buyer seller) behavior unknown (called unknown environment). propose case-based reasoning framework
choose suitable trust models unknown environments, based intuition trust model
performs well one environment, another similar environment. Firstly, build
case base number simulated environments (with known ground truth) along trust
models suitable them. Given unknown environment, case-based retrieval algorithms retrieve similar case(s), trust model similar case(s) chosen
suitable model unknown environment. Evaluation results confirm effectiveness
framework choosing suitable trust models different e-marketplace environments.

1. Introduction
multiagent e-marketplaces, self-interested selling agents may act maliciously delivering
products quality promised. thus important buying agents reason
trustworthiness (quality) sellers providing good quality products determine sellers
business with. However, open large environments, buyers often encounter sellers
previous experience. case, buyers often obtain advice (i.e., ratings)
sellers buyers (called advisors). However, advisors may dishonest
provide unfair ratings, promote demote sellers (Irissappane, Oliehoek, & Zhang, 2014).
Many trust models (Sabater & Sierra, 2005) proposed assess seller trustworthiness, which, BLADE (Regan, Poupart, & Cohen, 2006), also address unfair
rating problem. However, performance (accuracy predicting seller trustworthiness) trust
models often highly affected characteristics environments applied.
Specifically, Fullam Barber (2007) found performance trust models influenced environmental settings frequency transactions, honesty sellers accuracy
advisors ratings. detailed comparison BRS (Whitby, Jsang, & Indulska, 2004),
TRAVOS (Teacy, Patel, Jennings, & Luck, 2006) Personalized (Zhang & Cohen, 2008) (see
Sec. 2 details) conducted Zhang (2009) simulated dynamic e-marketplace environment. results show 1) BRS performs best buyers much experience
sellers environment majority advisors provide fair ratings sellers; 2)
TRAVOS advantage scenario buyers sufficient experience advisors

c
2015
AI Access Foundation. rights reserved.

fiI RISSAPPANE & Z HANG

lie specific sellers 3) Personalized fares well majority advisors
dishonest sellers widely change behavior time.
addition, almost trust models rely certain tuning parameters may significantly
affect performance. example, identify dishonest advisor, BRS uses quantile parameter (q) determine whether trustworthiness seller falls q quantile 1 q
quantile distribution formed advisors ratings seller. TRAVOS bin parameter divide [0, 1] bin number equal intervals, Personalized uses parameter
minimum number ratings required buyers accurate modeling seller trustworthiness.
Further, trust models evaluated simulated e-marketplace environments,
ground truth i.e., actual truth agents malicious behavior known upfront,
whether sellers deliver products lower quality promised whether advisors
provide unfair ratings. simulated environments, performance trust models specific
parameter values evaluated, best models easily chosen. However, real
e-marketplaces, difficult obtain ground truth expensive time consuming
manually inspect every transaction. Even manage find ground truth real environments, cannot guarantee best models environments suitable
environments. addition, environments may keep changing, suitable model
environment one period may another period. Thus, choosing suitable trust models
real environments (where ground truth agents behavior unknown, hence called unknown
environments) challenging well addressed, important practical applications.
paper, propose novel Case-Based Reasoning (CBR) framework choose suitable
trust models unknown e-marketplace environments. CBR well-known artificial intelligence
technique, applied complicated unstructured problems relatively easily (Sormo,
Cassens, & Aamodt, 2005). fundamental concept CBR similar problems
similar solutions, advantage learning continuously adding new cases case
base. problem choosing trust models, similar intuition trust model performs
well one environment, another similar environment. Thus, CBR becomes
suitable technique address problem finding trust models suitable similar emarketplace environments (i.e., similar problems). Specifically, proposed framework, first
find best trust models best parameter settings set simulated environments,
representing case base. given unknown real environment, find similar case(s)
case base using case-based retrieval methods (Watson & Marir, 1994) k-nearest
neighbors, K-dimension (K-d) trees, decision trees, etc. trust model similar case(s)
chosen suitable trust model unknown environment.
presented work extension previous work (Irissappane, Jiang, & Zhang, 2013),
describes simple framework choose trust models using similarity based computation.
paper, make number additional contributions: 1) formalize framework choose
trust models using case-based reasoning paradigm. so, explored CBR techniques
i.e., case representation retrieval methodologies, choose suitable trust models efficient
manner; 2) introduce additional case indexing retrieval schemes, K-d trees decision trees
apart k-nearest neighbors; 3) introduce feature weights (in addition feature selection),
improve accuracy determining nearest neighbors k-nearest neighbors K-d tree
retrieval techniques. contributions research perspective, also
conducted extensive detailed experimentation demonstrate effectiveness
framework. Experimental results show high probability, framework
478

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

choose suitable trust models evaluate seller trustworthiness different unknown environments. Evaluations also indicate seller trustworthiness evaluated using trust models chosen
framework set different e-market environments accurate applying
specific trust model best parameter values environments. Specifically, additional
experiments: 1) justify impact using suitable trust models e-marketplaces demonstrating
suitable trust models produce accurate estimate seller trustworthiness help buyers
make informed decisions, thereby resulting greater utility buyers using
(unsuitable) trust models; 2) consider extended data set increasing number cases
case base 972 2268 show performance framework improved using
larger case base; 3) compare accuracy k-nearest neighbors, K-d trees decision trees
choosing suitable trust models show k-nearest neighbors K-d trees outperform decision
trees, performing equally well; 4) compare time complexity retrieval techniques,
showing decision trees require slightly lesser retrieval time K-d trees, turn require
lesser time k-nearest neighbors; 5) show adding weights features determining nearest neighbors k-nearest neighbors K-d trees improves accuracy choosing
suitable trust models slight margin; 6) demonstrate buyer chooses aggregate
outcomes trust models determine seller trustworthiness instead using single
suitable trust model, results high margin error; 7) analyze time complexity involved
extending framework adding new features represent environments case base
adding new defense models, improve accuracy framework.
rest paper organized follows. Sec. 2, provide overview related
research choosing trust models. clearly point shortcomings existing approaches,
explain cope shortcomings work. Sec. 3 describes background
case-based reasoning. detailed description framework presented Sec. 4. Here,
also describe framework extended accommodate trust models different
e-marketplace environments. Sec. 5, present experimental results using seven trust models
demonstrate accuracy framework (using k-nearest neighbors, K-d tree decision tree
retrieval) correctly selecting suitable trust models unknown environments. Finally,
Sec. 6 concludes current work proposes future work.

2. Related Work
Here, provide overview existing trust models frameworks choose trust models.
2.1 Trust Models
Many trust models proposed literature. Beta Reputation System (BRS) (Jsang
& Ismail, 2002) models seller trustworthiness expected value beta probability distribution (binary) ratings given advisors seller. handle unfair ratings provided
advisors, Whitby et al. (2004) extend BRS filter ratings majority
amongst ones using Iterated Filtering approach. Specifically, cumulated trustworthiness score seller falls rejection area (q quantile 1 q quantile) beta
distribution advisors ratings seller, advisor considered dishonest filtered out. However, Iterated Filtering approach effective significant majority
ratings fair, thereby leading lower performance number dishonest advisors
large. Teacy et al. (2006) propose TRAVOS evaluate advisor trustworthiness, using discount
479

fiI RISSAPPANE & Z HANG

ratings aggregated evaluate seller quality. TRAVOS divides interval
[0, 1] bin number equal bins determine previous advice provided advisor
similar current advice. Two pieces advice similar within bin.
trustworthiness advisor calculated expected value beta probability
density function representing amount successful unsuccessful interactions
buyer seller based previous advice. However, model assumes sellers
behave consistently towards buyers e-marketplace, might true many
cases. Yu Singh (2003) use belief theory represent trustworthiness scores. determine
seller quality, rely referral network find advisors, thereby combine beliefs
advisors regarding seller. referral process begins buyer initially contacting
pre-defined number neighbors/advisors, may give opinion seller refer
advisors continues termination reached. referral process terminates success
opinion received advisor failure depth limit referral network
reached arrives advisor neither gives opinion referral. Weights
also assigned advisor, order identify deceptive ones.
BLADE approach (Regan et al., 2006) applies Bayesian learning reinterpret advisors
ratings instead filtering unfair ones. establishing correlation seller properties
advisors ratings, buyer infer advisors subjective evaluation functions derive certain
properties seller. Though reinterpretation helps cope advisors subjectivity
deception simultaneously, significant amount evidence (ratings) required accurately determine behavior advisors. Thereby, BLADE cannot perform effectively sparse scenarios,
buyers sufficient ratings sellers. personalized approach (Zhang &
Cohen, 2008), trustworthiness seller takes account buyers personal experience seller public knowledge seller. buyer enough private
information (personal experience with) seller (determined minimum number
transactions seller using acceptable level error confidence level ), buyer
uses private knowledge alone, otherwise uses aggregation private public knowledge compute trustworthiness seller. similar approach used compute advisor
trustworthiness. Prob-Cog (Noorian, Marsh, & Fleming, 2011) two-layered cognitive approach
filter ratings provided advisors, based similarity ratings buyer
advisor advisors behavioral characteristics. first layer, advisors
filtered average difference advisors opinions buyers personal ratings
exceeds threshold value . second layer, approach recognizes behavioral characteristics advisors passed first layer subjectively evaluates degree
trustworthiness. approach advantage proposed idea differentiate advisors
behavior patterns. However, Prob-Cog assumes advisors behavior consistent across sellers,
thereby making inefficient dynamically change behavior behaving honestly towards
sellers dishonest others. iCLUB approach (Liu et al., 2011) adopts clustering technique DBSCAN, filter dishonest advisors based local global information.
DBSCAN works grouping points density-reachable i.e., farther away given
distance other. also requires pre-defined minimum number points minP ts
form dense region i.e., cluster specified. iCLUB, DBSCAN clusters formed using
ratings given buyer advisors sellers. target seller, advisors ratings
cluster containing active buyers ratings, advisors considered dishonest.

480

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

buyer sufficient direct experience target seller (number transactions
less threshold ), process applied non-target sellers.
see, performance trust model mentioned varies depending
environmental settings (especially buyer seller behavior), applied. trust
model may suitable model environments. Thus, given unknown
environment, necessary choose among pool trust models, order accurately
assess seller trustworthiness choose good quality seller transaction.
2.2 Existing Frameworks Choose Trust Models
approaches proposed choose trust models. example, Hang, Wang
Singh (2009) make use explicitly indicated trust relationships users real-world systems
(e.g., FilmTrust) evaluate trust models. weighted graph vertices denoting agents
edges representing direct relationship trust agent source vertex agent
target vertex, weight (extent trust agents vertices) particular edge
determined relevant edges. evaluation, edge temporarily removed
weight edge estimated. accuracy predicting weight edge determines
effectiveness trust model. major drawback method users may lie
trust relationships, turn may affect evaluation process. works (Wang,
Hang, & Singh, 2011; Irissappane & Zhang, 2014) use data real-world e-markets (e.g., eBay
Amazon) evaluate performance trust models accuracy predicting ratings
given transactions (i.e., seller, ratings previous transactions used predict
(i + 1)th rating seller). However, ground truth whether ratings
transactions unfair may unknown. One may argue rely buyers
choose trust models know true experience sellers. But, costly
buyers evaluate trust model various parameters given environment.
Closely related work Personalized Trust Framework (PTF) (Huynh, 2009) selects appropriate trust model particular environment based users choice. Here, users
specify select trust model based information whose trustworthiness
evaluated configuration trust models. framework, 1) subject whose trustworthiness evaluated first sent trust manager. trust manager stores many trust
profiles contain rules suggested end users, regarding trust model use
subject; 2) trust manager matches subjects information trust profiles find
suitable trust model initializes trust engine selected model; 3) selected trust
model derives trust value subject. PTF relies entirely human intervention (users
specify rules select trust models). Though possible identify certain rules determine
suitable trust model environments (e.g., BRS performs well majority advisors
honest, BLADE performs well advisors subjective differences, etc.), impossible
know models perform best complex real world environments may
variety buyer seller behavior. Also, ground truth honesty subjectivity
buyers sellers extremely challenging determine, resulting rules partial
thus insufficient accurately choose suitable trust models using PTF. hand,
case-based reasoning framework, compare properties unknown environment
existing cases case-base using automated approach choose suitable trust models,
shown highly accurate experiments Sec. 5.

481

fiI RISSAPPANE & Z HANG

3. Background
Case-Based Reasoning (CBR) process solving new problems based solutions
similar past problems. Conceptually, CBR commonly described CBR-cycle (Aamodt &
Plaza, 1994). CBR-cycle comprises four activities: retrieve, reuse, revise retain.
retrieve phase, one cases, similar new problem selected case
base. Many case-based retrieval algorithms exist literature (Watson & Marir, 1994). Nearest
neighbor techniques (Duda & Hart, 1973) perhaps widely used retrieval techniques
CBR. Distance measures Euclidean distance employed identify nearest
neighbors (cases). Despite simplicity, nearest neighbor retrieval successful large
number classification problems (Hastie, Tibshirani, & Friedman, 2009). However, case
base grows, efficiency retrieval decreases, increasing number cases must taken
account find similar case. K-d trees (Wess, Althoff, & Derwand, 1994), organize
case base binary tree structure shown reduce complexity retrieval
nearest neighbors. Alternatively, inductive retrieval algorithms (Soltani, 2013; Watson, 1999),
determining features best job discriminating cases generate decision tree type
structure organize cases memory, also used improve retrieval efficiency.
one similar cases retrieved, solution (or problem solving
information) contained cases reused solve current problem. Reusing retrieved
solution quite simple, solution returned unchanged proposed solution
new problem. specifically case classification tasks limited number solutions
(classes) large number cases. scenarios, every potential solution contained
case base hence adaptation usually required. hand, synthetic tasks (such
configuration planning) solution adaptation new problem necessary.
revise phase, solution determined far verified real world possibly
corrected improved, e.g., domain expert. Finally, retain phase takes feedback
revise phase updates knowledge, particularly case base new problem solving
experience becomes available reuse future problem solving episodes.
major challenge CBR resides retrieval existing cases sufficiently similar
new problem. Since e-marketplace environments ground truth (existing cases) may
exist (or may difficult obtain), framework, create simulations.
addition, framework, features (characteristics e-marketplace environments) used
represent cases case base known beforehand. thus come
exhaustive list potential features (to describe e-marketplace) carefully select
relevant ones, order efficiently choose suitable trust models.

4. Proposed Case-Based Reasoning Framework
Fig. 1 illustrates detailed design framework. important component framework case base. build case base, first simulate large set e-marketplace environments known ground truth honesty agents behavior. Given set available
trust models specific values parameters (referred candidate trust models),
evaluate performance simulated environment, best model identified
forms best environment-model pair (representing case case base). process, also
choose relevant features represent cases, efficient retrieval. Given unknown

482

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

real environment, framework extracts set carefully selected (most relevant) features
determines similar case(s) case base using case-based retrieval techniques.
trust model similar case(s) reused solution unknown real environment. given unknown environment along suitable trust model retained
framework reuse future problem solving episodes. major components
framework detailed procedures described following subsections.
Building Case Base

Simulated
Environments

Case Retrieval

Candidate
Trust Models

Similar Case(s)

Feature Values
Unknown Environment

Case Base

Feature Extraction
Selection

Evaluate Candidate
Trust Models

(Environment, Model)
Unknown
Environment

Case Reuse
Relevant
Features

Suitable
Trust Model

Verify
Accuracy

Case Retain

Figure 1: Design case-based reasoning framework

4.1 Case Base
CBR heavily dependant structure content case base. framework, case
case base described e-marketplace environment (represented set carefully
selected features) along trust model performs best environment. Unlike
domains, real e-marketplace environments ground truth honesty sellers
buyers rare may exist, hence becomes challenging build case base.
mainly rely simulations create existing cases case base.
4.1.1 E-M ARKETPLACE E NVIRONMENTS
e-marketplace environment (E) consists set sellers, set buyers, transactions (each
seller buyer certain monetary value) ratings (each
given buyer seller specific time indicating whether buyer satisfied
transaction). So, E tuple,
E = hS, B, {Ts,b |s = 1...Ns , b = 1...Nb } , {Rs,b |s = 1...Ns , b = 1...Nb }i

(1)

represents set sellers, B represents set buyers, Ns Nb
numbers sellers buyers E, respectively. Rs,b denotes set ratings buyer b
seller transactions Ts,b . rating rs,b Rs,b transaction ts,b Ts,b tuple,
rs,b = hid, s, b, hs , hb , t, vali
483

(2)

fiI RISSAPPANE & Z HANG

id, s, b denote rating index, index seller buyer, respectively.
hs ( [0, 1]) hb ( {honest, dishonest}) denote ground truth i.e., actual seller trustworthiness honesty buyer transaction, respectively. dishonest seller (with low
trustworthiness) may advertise products high quality actually deliver low quality ones
deliver all. Also, dishonest buyer may lie satisfaction level transaction
providing unfair rating. hs hb attributes help distinguish dishonest behaviors
honest ones. time (integer value denoting day simulation) rating
given denoted t. val denotes actual value rating, binary (e.g., 0 1),
multi-nominal (e.g., 1 - 5) real (e.g., range [0, 1]).
two types environments framework: 1) known environments (Eknown ),
ground truth seller buyer honesty known. known environments along
suitable trust models help building case base framework; 2) unknown environments (Etest ) ground truth known. represent test environments
suitable trust models need determined.
build case base, simulate large number Eknown environments, cover
many scenarios possible closely depict real-world environments. example, may simulate environment many sellers fewer buyers (to represent high provision e-marketplace)
many buyers fewer sellers (to illustrate competitive e-marketplace). may simulate
sparse environment ratings provided buyers, dense environment
seller flooded large number ratings. may also simulate different scenarios
buyers active inactive providing ratings. environments, may also simulate sellers different levels honesty, buyers launching different types unfair rating
attacks (Hoffman et al., 2009), including example, unfair ratings reputable disreputable sellers, lot unfair ratings, unfair ratings given short long time period, etc.
4.1.2 C ANDIDATE RUST ODELS
exemplified Sec. 2, many trust models proposed evaluate seller trustworthiness
e-marketplaces. New trust models also likely proposed future. trust models
considered candidate trust models framework. addition,
parameters tune, may result different performance. Thus, candidate trust model (T )
defined trust model specific value parameters. parameter varying
range, divide range number equal intervals randomly choose value
interval. Ideally, larger number intervals better.
4.1.3 F EATURE E XTRACTION ELECTION
formally represent environment case base, environment described
set features, representing characteristics environment (e.g., ratio number buyers
versus sellers, variance ratings per seller per buyer, average number transactions per time
period, percentage rated sellers, etc.). exhaustive list potential features extracted
relevant features identified used represent environment, order
reduce computational cost increase efficiency framework. F = {f1 , ..., fn }
set features P (F ) performance framework using subset F F
features. relevant subset features F chosen framework achieves
best performance, formalized Eqn. 3.

484

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

F = arg max P (F )

(3)

F F

constructing case base, simulate another set e-marketplace environments
evaluate performance framework environments using possible features.
features whose values significantly correlate performance framework determined using five widely used correlation regression analysis techniques, namely Pearson correlation, Kendall rank correlation, Spearman rank correlation, linear regression (backward) linear
regression (stepwise). results correlation also analyzed Paired-Samples T-test
check statistical significance. correlation regression analysis technique results
subset significantly relevant features recognized technique (F F ). influential set features (F ) determined1 five subsets features (each recognized
five techniques, respectively) using Eqn. 3, details presented
Sec. 5.1.3. Thus, features F used represent environments case base.
4.1.4 B EST E NVIRONMENT-M ODEL PAIRS
Given set known environments set candidate trust models, find environment (Eknown ), candidate model (T ) shows best performance. Specifically,
performance P (Eknown , ) measured terms performance metric, Mean
Absolute Error (MAE) determining seller trustworthiness, given Eqn. 4, Tstrue
Tspredicted represent actual predicted trustworthiness seller s, respectively. lower
MAE, better performance trust model. evaluations result set best
environment-model pairs (Eknown , ), form case base. several models perform
equally best environment, keep case base.
MAE =

1 X true
|Ts
Tspredicted |
Ns

(4)

sS

4.2 Case Retrieval
Given unknown environment Etest , case-based retrieval algorithms retrieve similar
case(s), (Eknown , ) pair(s), whose simulated environment Eknown similar Etest .
Every retrieval algorithm combination procedure searching case base find
similar case similarity assessment procedure, determines similarity
given unknown environment Etest known environment Eknown case base.
Firstly, consider structural manner cases represented case base,
plays major role efficient retrieval cases. choice case representation
chiefly depends type problems CBR system intended solve, varying relatively
simple feature-value vectors, complex data-structures. framework, propose represent
case base using two structural representations (Watson & Marir, 1994): 1) flat representation; 2)
hierarchical representation, analyze performance framework scenarios.
1. feature selection process used determine influential features using k-nearest
neighbors, K-d tree retrieval decision trees employs embedded feature selection methodology.

485

fiI RISSAPPANE & Z HANG

4.2.1 F LAT R EPRESENTATION
simplest format represent cases case base simple feature-value vectors
environments (Eqn. 5), obtained influential features (more suitable cases
numeric feature values). flat memory model, cases organized level
relationships features cases shown.
E =< fi | fi F >

(5)

Classical nearest neighbor (Duda & Hart, 1973) retrieval method choice retrieval
cases flat representation, shown Fig. 2. Given unknown environment Etest ,
compared cases case base similar cases found according similarity
features Etest Eknown environments, measured terms Euclidean distance,
sX
dist(Etest , Eknown ) =
(Etest (fi ) Eknown (fi ))2
(6)
fi F


= arg max N (T )

(7)

TM

Additionally, also assign weights different features calculating distance
Eqn. 6. k-nearest neighbors, k cases, closest Etest based similarity, retrieved
similar case(s) chosen majority vote, suitable trust model
similar case(s) occurs maximum number times among k closest cases, shown
Eqn. 7, N (T ) represents number times trust model appears k closest
cases, TM represents set candidate trust models framework represents
trust model similar case(s), suitable trust model Etest .
retrieval time memory organization high (O(|C|), |C| number
cases case base), since retrieval, cases case base must compared
target case Etest , making unsuitable large case bases. However, approach
verified provide maximum accuracy easy retention.
Case Retrieval
Case
Representation
Flat Storage

Feature Values
Unknown Environment

Case Base
(Environment, Model)
Unknown
Environment

k-Nearest
Neighbors

Majority Vote
Case Reuse
Suitable
Trust Model

Figure 2: k-Nearest Neighbors retrieval

486

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

4.2.2 H IERARCHICAL R EPRESENTATION
efficient rapid retrieval, structured representation cases necessary,
small subset cases need considered retrieval large case base.
Following hierarchical representation helps organize cases share similar features
generalized structure. demonstrate use two hierarchical tree structures differ
method indexing (assigning indices cases) greatly improve retrieval efficiency.
Case Retrieval

K-d Trees
Case
Representation

f1=m1
f2=m2
>



(Environment, Model)

f2=m3

Bucket
(Eknown, TM)

>

f3=m4

.

Case Base

Feature Values
Unknown Environment

>



f3=m5

>


Bucket
(Eknown, TM)

Bucket
(Eknown, TM)

Unknown
Environment

k-Nearest
Neighbors

Majority Vote
Case Reuse
Suitable
Trust Model

Figure 3: K-d Tree retrieval
Traditionally, K-dimensional (K-d) tree representation demonstrated useful
reduce retrieval time similar cases using nearest neighbors (Wess et al., 1994). K-d
tree, K represents number feature dimensions representing case (i.e., K = |F |),
multi-dimensional binary search tree splits case base groups cases way
group contains cases similar other. Specifically, node K-d
tree splits children along specific feature, using hyperplane perpendicular
corresponding axis. root (which contains entire case base), children split based
first feature (f1 F ), i.e., cases f1 less (or equal to) root left
sub-tree greater root right sub-tree, shown Fig. 3. level
tree divides cases next feature fi F , returning first dimension f1
features exhausted. leaves tree contain specific number
cases called buckets. partitioning, median point feature (f1 = m1 shown
Fig. 3) selected root node cases smaller value (than m1 f1 ) placed
left larger right. similar procedure followed left right sub-trees
last trees partitioned composed cases (not bucketsize).
retrieval, recursive search procedure adopted. queue containing k similar cases
maintained throughout search. search examines leaf node, similarity case
bucket given unknown environment Etest , computed using Eqn. 6 k-nearest
487

fiI RISSAPPANE & Z HANG

neighbors queue updated. case non-leaf node, search recursively called
child node, Etest belongs (by comparing features Etest partitioning value
node). recursion terminates (at non-leaf node), tested whether
child node needs examined (if geometric boundaries delimiting cases
node overlap ball centered Etest radius equal similarity k th nearest neighbor
encountered far, child needs examined, ignored otherwise).
procedure (unwinding recursive search) repeated root reached. determining
k similar cases (present queue), suitable trust model determined using
Eqn. 7. average retrieval time determining k similar cases K-d trees found
O(k log|C|), |C| size case base.
Another hierarchical organization frequently used CBR Decision trees. Decision trees
induction-based models (Soltani, 2013) learn general domain-specific knowledge set
training data represent knowledge form trees. Decision trees (when compared
classes learning methods), quite fast, directly applied training
data without much pre-processing produce relatively interpretable models (Hastie et al., 2009).
Unlike k-nearest neighbors K-d trees, use similarity based retrieval techniques, decision
trees learn rules order determine suitable trust model. also implicit
feature selection process. node decision tree specifies test feature attribute
(e.g., f1 Fig. 4), branch descending node corresponds possible values
(e.g., f1 v1 Fig. 4 ) feature attribute. making trees, much feature
discriminate cases calculated (e.g., information gain cases) feature
highest discriminative power located top tree. calculation performed
remaining features, thereby building tree top-down fashion. solution i.e.,
suitable trust model located leaves tree. Algorithms developed decision trees
Case Retrieval

Decision Trees

Case
Representation

f1

f2

Case Base
v2

(Environment, Model)

Feature Values
Unknown Environment

>v1

v1

f3

>v2; v3

>v3

>v4

v4

f4

f5
.

TM

Unknown
Environment

TM

Case Reuse
Suitable
Trust Model

Figure 4: Decision Tree retrieval
mostly variations top-down, greedy search algorithm exemplified ID3 (Quinlan,
1986) successor C4.5 (Quinlan, 1993). algorithms construct decision tree using
divide conquer strategy i.e., build decision tree recursively dividing case base
subsets according splitting criterion called information gain ratio. intuition
488

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

partition case base way information needed classify new case reduced
much possible. Eqn. 8 represents information gain (discriminative power) feature
fi F , regarding set cases C case base, V (fi ) set possible values
feature fi Cv C set cases feature fi taking value v. p(T ) proportion
cases case base trust model suitable. Since decision trees
built-in feature selection methodology, influential features selected used
building decision tree, employ feature selection process described Sec. 4.1.3
case retrieval using decision trees, might affect retrieval accuracy otherwise.
retrieval, features unknown environment (Etest ) compared nodes tree,
gets one leaves contains suitable trust model (T shown Fig. 4).
Inf ormationGain(fi , C) = Entropy(C)

X
vV (fi )

where, Entropy(C) =

X

|Cv |
Entropy(Cv )
|C|

p(T ) log2 p(T )

(8)
(9)

TM

4.3 Case Reuse
retrieving similar case(s) (using retrieval methods discussed previous subsection) target case (Etest ), framework needs reason according retrieved cases
find reasonable accurate solution (most suitable trust model) Etest . reuse
solution done two ways (Soltani, 2013): 1) reusing solution retrieved case
solution target case without adaptation (applicable classification problems);
2) adapting retrieved solution target case, necessary problem-solving tasks
design, configuration, planning. Since deal classification problem, identifying class (candidate trust model case) given unknown environment belongs,
perform adaption simply reuse solution retrieved case(s)2 . Thereby,
framework choose trust model retrieved case(s) suitable model
Etest (using k-nearest neighbors K-d trees, decision tree retrieval framework
directly choose trust model suggested decision tree similar case(s) retrieved).
4.4 Case Retain
Case-based reasoning favors learning experience. choosing reuse solution
retrieved case(s) Etest , may found solution is, fact, incorrect, thus providing
opportunity learn failure. framework offers simple procedure, case solution
evaluated solution incorrect, revised best solution Etest found.
new case along best trust model (Etest , ) retained case base (Fig. 1).
proposed case-based reasoning framework generic extended concretized
following aspects: 1) whenever new trust model proposed, added framework. framework capable taking advantage trust model improve performance
evaluating seller trustworthiness; 2) whenever new insightful feature identified,
added framework participate feature selection process fact may
improve performance framework; 3) promising feature selection methods
2. one similar cases, different solutions, randomly choose one solutions.

489

fiI RISSAPPANE & Z HANG

incremental hill-climbers (Wettschereck & Aha, 1995), wrapper model measure importance
features, adopted enhance performance framework 4) sophisticated
memory representations used efficient fast retrieval cases.

5. Experimentation
instantiate framework conduct series experiments demonstrate effectiveness
choosing suitable trust models. Firstly, build case base generating number simulated
environments finding suitable trust models them. process, also determine
influential features represent simulated environments case base.
generate unknown (both simulated real) environments testing verify performance
framework choosing best trust models unknown environments. also compare
performance k-nearest neighbors (k-NN), K-d tree (K-dT) decision tree (DT) retrieval
techniques, finding suitable trust model given unknown environments.
5.1 Case Base
case base built using large set simulated environments along suitable
candidate trust models environments, described below.
5.1.1 IMULATED E NVIRONMENTS
framework, 2268 e-marketplace environments (Eknown ) simulated, consisting different numbers sellers (chosen {10, 25, 50}) different levels trustworthiness Tstrue ,
uniformly distributed [0, 1]. Sellers provide good quality products probability Tstrue
interacting buyers. Honest buyers always provide correct opinions (similar
actual seller trustworthiness Tstrue ) sellers, dishonest buyers3 provide unfair ratings4 i.e., incorrect opinions complimentary actual seller trustworthiness
(1 Tstrue ). simulate different distributions fair ratings given honest buyers: 1) sparse,
honest buyer rates seller once; 2) intensive, honest buyer rates seller
once; 3) mixed, combination sparse intensive scenarios. also simulate
different unfair rating attack scenarios dishonest buyers adjusting 4 parameters: 1) individual
attack frequency denoting average number unfair ratings provided dishonest buyer
exhibit sparse, intensive mixed behavior; 2) attack period referring period
unfair ratings given, 7 100 denote dishonest buyers provide unfair ratings
one week (a concentrated attack) 100 days (a distributed attack), respectively. dishonest
buyers provide unfair ratings attack period, behave honestly providing fair ratings
outside attack period. helps simulate dynamic environments buyers change
behaviors; (3) attack target taking value 0 1, indicating attack targets sellers low
trustworthiness (Tstrue 0.5) high trustworthiness (Tstrue > 0.5), respectively; 4) overall attack
rate denoting ratio number unfair ratings fair ratings, chosen {0.25, 1, 4}.
parameters individual attack frequency overall attack rate, numbers dishonest
honest buyers determined. marketplaces operate 100 days. total number ratings chosen {50, 75, 100, 150, 175, 200, 250}. also limit total number ratings
3. Buyers providing incorrect opinions due subjective differences ignorance also considered dishonest.
4. Ratings simulated environments real type easily mapped types (binary, multi-nominal).

490

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

{50}, {50, 100}, {50, 100, 200} {50, 100, 175, 250} simulate 324, 648, 972 1296 environments, respectively, order examine influence number simulated environments
(size case base) performance framework.
5.1.2 C ANDIDATE RUST ODELS
framework includes 7 representative trust models: BRS (Whitby et al., 2004), iCLUB (Liu
et al., 2011), TRAVOS (Teacy et al., 2006), Personalized (Zhang & Cohen, 2008), Referral Networks (Yu & Singh, 2003), BLADE (Regan et al., 2006) Prob-Cog (Noorian et al., 2011).
following parameters (as described Sec. 2) considered design candidate trust models:
1) BRS, quantile parameter q {0.05, 0.1, 0.3, 0.5}, used filter dishonest buyers considered; 2) TRAVOS, number bins determine acceptable error level
buyers ratings bin {2, 3, 5, 8, 10} considered; 3) Referral Networks, number neighbors
{2, 4, 6} depth limit referral networks {4, 6, 8} considered; 4) Personalized, error
level {0.3, 0.5, 0.7} confidence level {0.3, 0.5, 0.7} considered; 5) Prob-Cog,
consider threshold filter dishonest buyers {0.1, 0.2, . . . , 0.9}; 6) iCLUB,
consider minimum number ratings required form DBSCAN cluster minP ts [1, 6],
maximum neighbor distance [0.3, 0.7] threshold choose local global component
[3, 6]. end, obtain 45 candidate trust models (TM) total.
Features
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

Pearson Kendall Spearman Backward Stepwise
(C1)
(C2)
(C3)
(C4)
(C5) (C6)
Variance Percentage Ratings Seller


Avg. Number Ratings Provided Buyer Seller






Ratio Number Buyers versus Number Sellers




Skewness Rating Period




Variance Percentage Ratings Provided Buyer






Skewness Number Ratings Provided Buyer



Percentage Satisfactory Sellers






Number Buyers





Avg. Number Ratings Seller






Variance Number Ratings provided Buyer



Total Number Ratings






Variance Number Ratings Seller






Skewness Number Ratings Seller





Avg. Number Transactions Day



Total Percentage Sellers Rated Buyers






Time Period Marketplace Operates




Maximum Percentage Ratings Sellers





Total Percentage Buyers Active Marketplace





Table 1: Selection relevant features
5.1.3 F EATURE ELECTION
consider set 18 potential features (F ) analyze characteristics simulated environments, listed Table 1. use general statistical metrics describe features.
example, variance refers spread values, skewness describes asymmetry
normal distribution, etc. satisfactory seller refers one receives positive ratings
negative ones buyers. active buyer refers buyer, provides least one rating
seller. feature values simulated environments extracted using parameters
generate simulated environments, described Sec. 5.1.1. Since features (in Table 1)
491

fiI RISSAPPANE & Z HANG

depend ground truth (buyer seller honesty), also easier extract feature
values unknown environments (with ground truth).
select relevant features (for efficient retrieval using k-NN K-dT), adopt
five correlation regression analysis techniques mentioned Sec. 4.1.3. results
analysis 18 features correlated performance (MAE) framework
shown Table 1. Here, * denotes feature significant correlation (after PairedSamples T-test) performance framework. Table 1, columns C1, C2, C3, C4 C5
represent combination features flagged *. C6 represents combination
features. verify effectiveness 6 feature combinations, randomly generate large
number unknown environments compare results. obtain average MAE (using kNN retrieval5 ) 0.44, 0.36, 0.36, 0.25, 0.33, 0.32 combinations C1, C2, C3, C4, C5 C6,
respectively. C4 lowest MAE chosen set influential features (F ),
Eqn. 3. features C4 used comparing unknown simulated environments
rest experiments (using k-NN K-dT retrieval obtain similar case(s) Etest ).
5.1.4 B EST E NVIRONMENT-M ODEL PAIRS
simulated environment, find best candidate trust model based performance metric MAE. MAE suitable metric assess performance trust models
accurately determining trustworthiness sellers helps buyers choose good transaction partners, thereby increasing utility long run (as demonstrated experiments Sec. 5.4).
first calculate MAE candidate trust models predicting seller trustworthiness
simulated environments select one lowest MAE value. Here, also compute
difference MAE (for seller e-marketplace environment) pairs trust models assess MAE values suitable trust model significantly better
others (using Paired-Samples T-test). end, obtain 3664 best environment-model pairs6
(Eknown , ), form case base framework.
Fig. 5(a) illustrates number simulated environments case base candidate
trust model achieves best performance, 733, 306, 448, 979, 190, 253 755
BRS, iCLUB, TRAVOS, Personalized, Referral, BLADE Prob-Cog, respectively. numbers
indicate case base contains sufficient number cases trust model. sample case
i.e., (Eknown , ) shown Eqn. 10. Eknown described 18 features (we show
features considered feature selection process clarity) order mentioned
Table 1 BLADE model environment.
(Eknown , ) =(< 0.30, 0.09, 18.2, 0.04, 0.4, 1.5, 0.6, 182, 19, 0.36, 200, 0.18,
1.3, 2, 1, 100, 0.62, 0.2 >, BLADE)

(10)

5.2 Case Retrieval Algorithms
use k-NN, K-dT DT retrieval techniques identifying suitable trust models unknown
environments compare performance.
determine number nearest neighbors k-NN K-dT, randomly generate 972
unknown environments (using different values parameters generate simulated envi5. K-d tree retrieval obtains similar MAE values.
6. simulated environment one suitable trust model (which significantly outperform
other) lowest MAE.

492

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

102

0.4
755

733

6
488
4

0.3
MAE

No. Use Times

8

K-dT

k-NN

979

10

306

0.2
0.1

253
190

2

0
0

BR


P
BL

iC

Pe
LU R
r
fer
AD robVO son
r
Co
B

E
al
g

ize l


1

2

3

4

5

6

7

8

9

10

k

(a)

(b)

Figure 5: (a) No. times trust model selected suitable model simulated
environments; (b) Influence k MAE determining seller trustworthiness using
k-NN K-dT
ronments) evaluate performance k-NN k-dT choosing suitable trust models,
different values k. Table 2 presents influence k accuracy choosing
suitable trust models (with suitable parameters) 972 randomly generated unknown
environments, using k-NN k-dT retrieval techniques. Correct Model indicates trust
model chosen best model identified evaluating candidate trust models
given unknown environment. Correct Model Paras indicates correct trust model
chosen appropriate tuning parameters. Also, = 0.05 tolerance value, indicating
difference MAE chosen trust model truly suitable model
within . find accuracy choosing correct trust models (with parameters)
highest k = 3 acceptable k = 1, 2 k-NN K-dT. k values greater
3, performance decreases, signifying boundaries classes (candidate
trust models) become less distinct. Fig. 5(b) shows MAE determining seller trustworthiness
(corresponding accuracy Table 2), value k increased 1 10. k-NN
K-dT obtain similar MAE values determining seller trustworthiness different values k.
Again, find k {2, 3}, lowest MAE (0.24) achieved. Hence, use k = 3
experiments (using k-NN K-dT) paper.
k-NN
Correct Model
Correct Model
Correct Model Paras
Correct Model Paras
K-dT
Correct Model
Correct Model
Correct Model Paras
Correct Model Paras

k=1
94.0%
97.0%
92.0%
96.0%
k=1
94.0%
96.0%
91.0%
95.0%

k=2
96.0%
97.0%
94.0%
97.0%
k=2
95.0%
97.0%
94.0%
96.0%

k=3
97.0%
99.0%
97.0%
98.0%
k=3
97.0%
98.0%
97.0%
98.0%

k=4
89.0%
91.0%
85.0%
87.0%
k=4
89.0%
92.0%
86.0%
87.0%

k=5
82.0%
84.0%
77.0%
79.0%
k=5
83.0%
84.0%
78.0%
79.0%

k=6
76.0%
78.0%
71.0%
73.0%
k=6
76.0%
78.0%
71.0%
73.0%

k=7
75.0%
77.0%
69.0%
71.0%
k=7
75.0%
76.0%
69.0%
71.0%

Table 2: Influence k accuracy framework

493

k=8
73.0%
75.0%
67.0%
69.0%
k=8
74.0%
75.0%
68.0%
69.0%

k=9
71.0%
73.0%
65.0%
67.0%
k=9
72.0%
73.0%
65.0%
67.0%

k=10
71.0%
73.0%
65.0%
67.0%
k=10
71.0%
73.0%
65.0%
66.0%

fiI RISSAPPANE & Z HANG

K-dT retrieval, use weka implementation (median based partitioning maximum 20 instances leaf node). K-d tree built using 3664 best environment-model
pairs (described Sec. 5.1.4). DT retrieval, use C4.5 algorithm (J48 weka implementation pruning confidence 0.25 minimum number instances 2,
default values). decision tree also built using 3664 best environment-model pairs,
used find suitable model unknown environments.
5.3 Unknown Environments Testing
framework evaluated using 6 categories unknown environments Etest (where ground truth
seller buyer honesty fact known) normal extreme scenarios.
Specifically, Unknown Random Environments generated using parameter values different
simulated environments as: 1) number sellers {33, 66, 99}; 2) total number
ratings {333, 666, 999}; 3) ratio number unfair ratings versus fair ratings {0.1, 1, 10}; 4)
time period attacks {50, 100}, 100 environments randomly chosen testing.
Unknown Real Environments generated using Real data obtained IM DB.com,
users rate movies directed different directors. remove outlying ratings select directors whose movies highly rated, resulting 40 different directors, 1142 movies
rated 188 users. simulate 3 types unfair rating attacks, namely RepBad, RepSelf
RepTrap (Yang, Feng, Sun, & Dai, 2008), combination bad-mouth targeted directors
(sellers case). Finally, generate 48 real environments simulated attacks.
Large Environments number sellers larger 50, number ratings
larger 100 number buyers larger 80. generate 160 large environments.
Extremely Sparse Environments buyers provide sufficient ratings. Specifically, buyer gives average 0.1 ratings sellers. generate 36 environments
number sellers 10, total number ratings 100, overall attack rate {0.25, 1, 4}.
Environments Dynamic Seller Buyer Behavior environments sellers/buyers
change behavior dynamically. Sellers change behavior providing complimentary quality products (than previously presented) random period operation e-marketplace.
(Dishonest) buyers change behavior providing unfair ratings specific periods
behaving honestly, otherwise. 35 environments (number sellers 10 total number
ratings 50) dynamic behaviors generated.
Environments Many Attacks intensive attacking scenarios, attack rate
larger 10. specifically use real data IM DB.com simulate RepBad, RepSelf,
RepTrap attacks combination generate 24 environments.
5.4 Experimental Results
Here, present results unknown random real environments well results extreme
scenarios. also analyze possible extensions framework adding weights
features determining nearest neighbors k-NN K-dT retrieval, etc.
5.4.1 P ERFORMANCE C OMPARISON U NKNOWN R ANDOM R EAL E NVIRONMENTS
Table 3 presents accuracy framework choosing suitable trust models (with
suitable parameters) unknown random real environments (using k-NN, K-dT DT
retrieval techniques). mentioned Sec. 5.2, correct selection indicates trust model
494

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

chosen best model identified evaluating candidate trust models given
unknown environment tolerance value, indicating difference MAE
chosen trust model truly suitable model within .
Unknown Random Environments
k-Nearest Neighbors (k-NN)
324 SE
648 SE
972 SE
Correct Model
81.0%
84.0%
92.0%
Correct Model
87.0%
89.0%
95.0%
Correct Models Paras
72.0%
76.0%
82.0%
Correct Model Paras 85.0%
86.0%
94.0%
K-d Trees (K-dT)
324 SE
648 SE
972 SE
Correct Model
80.0%
84.0%
92.5%
Correct Model
87.0%
90.0%
95.0%
Correct Models Paras
71.0%
76.0%
82.5%
Correct Model Paras 85.0%
87.0%
94.2%
Decision Trees (DT)
324 SE
648 SE
972 SE
Correct Model
52.0%
54.0%
63.0%
Correct Model
64.0%
67.0%
72.0%
Correct Model Paras
31.0%
33.0%
35.0%
Correct Model Paras 49.0%
53.0%
58.0%
Unknown Real Environments
k-Nearest Neighbors (k-NN)
324 SE
648 SE
972 SE
Correct Model
81.3%
83.3%
83.3%
Correct Model
89.6%
95.8%
95.8%
Correct Model Paras
72.9%
75.0%
77.1%
Correct Model Paras 89.6%
95.8%
95.8%
K-d Trees (K-dT)
324 SE
648 SE
972 SE
Correct Model
80.1%
82.3%
83.3%
Correct Model
89.0%
95.1%
96.2%
Correct Model Paras
71.0%
74.0%
78.1%
Correct Model Paras 88.0%
95.0%
95.2%
Decision Trees (DT)
324 SE
648 SE
972 SE
Correct Model
08.3%
08.3%
14.6%
Correct Model
55.1%
58.3%
68.8%
Correct Model Paras
00.0%
00.0%
00.0%
Correct Model Paras 54.3%
59.2%
68.8%

1296 SE
96.0%
98.0%
96.0%
97.0%
1296 SE
95.0%
98.0%
95.0%
97.0%
1296 SE
72.0%
78.0%
40.0%
63.0%

2268 SE
97.0%
98.0%
97.0%
98.0%
2268 SE
97.0%
98.0%
97.0%
98.0%
2268 SE
80.0%
85.0%
46.0%
67.0%

1296 SE
86.3%
97.3%
79.3%
96.3%
1296 SE
86.7%
97.0%
79.0%
96.0%
1296 SE
40.3%
80.3%
02.0%
80.3%

2268 SE
87.5%
97.3%
81.3%
97.2%
2268 SE
87.0%
97.0%
82.0%
97.5%
2268 SE
45.0 %
83.3%
02.1%
83.3%

Table 3: Accuracy choosing suitable trust models (with parameters) unknown random
real environments
Table 3 (under unknown random environments), see accuracy framework increases number simulated environments (SE) case base increases (the trend
k-NN, K-dT DT), best 2268 simulated environments
(SE) case base. larger number cases case base, easier
find closely similar environment given unknown environment.
also find k-NN K-dT show similar performance, outperforming DT retrieval technique. K-dT mainly used improve retrieval time k-NN appropriate
organization cases form trees. However, retrieval, K-dT uses similarity
measure number nearest neighbors k-NN. reason similar
performance k-NN K-dT. hand, DT retrieval shows lower performance
requires training instances (cases case base) learn entire problem space (and
build complete decision tree) known show surge performance dealing
continuous feature values (Quinlan, 1996) (in framework feature values continuous).
smaller number cases (say 324 SE), DT obtains accuracy 52.0% choosing
495

fiI RISSAPPANE & Z HANG

k-NN

0.4

K-dT

DT

K-dT

DT

0.3
MAE

MAE

0.3

0.2

0.1

0

k-NN

0.4

0.2

0.1

324

648

972

1296

0

2268

|Simulated Environments|

324

648

972

1296

2268

|Simulated Environments|

(a)

(b)

Figure 6: Influence number simulated environments performance k-NN, K-dT
DT retrieval techniques: (a) random environments; (b) real environments
best trust model unknown random environments. Even 2268 SE accuracy
80.0%, less accuracy k-NN 324 SE. k-NN K-dT obtain accuracy
97.0% selecting suitable models 98.0% accuracy tolerance = 0.05,
2268 simulated environments. Thus, shows framework, using k-NN, K-dT retrieval techniques choose candidate models whose performance close ideal case.
Even 324 simulated environments, performance k-NN K-dT still acceptable,
selecting suitable models accuracy 81.0% 80.0%, respectively.
Fig. 6 shows influence number simulated environments (size case base)
MAE obtained, determining seller trustworthiness using candidate trust model suggested
k-NN, K-dT DT retrieval techniques. accurate selection best trust model
results lower MAE value determining seller trustworthiness. Fig. 6(a) shows k-NN
K-dT obtain lower MAE DT cases. number simulated environments
2268, k-NN K-dT, obtain MAE 0.25, DT obtains MAE 0.31. However,
number simulated environments increased, find rate MAE
DT decreases greater k-NN K-dT, training instances, DT
produce accurate results. Eventually, number simulated environments
increased (greater 2268), DT may show (even better) performance k-NN K-dT.
However, increase number simulated environments experiments due
complexity involved building case base, evaluating 45 candidate trust models
simulated environment selecting suitable model them.
Table 3 (under unknown real environments) shows k-NN K-dT perform equally
well (with accuracy 87.5% 87.0% selecting suitable models 2268 simulated environments, respectively), outperforming DT retrieval (with accuracy 45.0%) real
environments. also notice accuracy techniques lower random environments. because, characteristics real environments may vary extensively
simulated environments case base, making difficult retrieval algorithms
identify similar cases whose simulated environment similar real one. Nevertheless,
performance k-NN K-dT unknown real environments also sufficient (greater 86%).
Fig. 6(b) shows k-NN, K-dT perform better decision trees. However, see

496

fi0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

MAE

MAE

F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

0.3

0.3

0.2

0.2

0.1

0.1

0

0

BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NN
B na ra E Co
g
liz l
ed

BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NN
B na ra E Co
g
liz l
ed

(a)
30

28
22

No. Use Times

No. Use Times

30

(b)

19

20

14
11
10

22
20
13
10
5

4
0

BR


4

3

2
0

P
BL

iC

Pe
LU R
r
fer
AD robVO son
r
Co
B

E
al
g

ize l


(c)

BR


1
0
P
BL

iC

Pe
LU R
rs
fer
AD robo
VO
ra
Co
B
na
E
l
liz
g

ed

(d)

Figure 7: MAE framework trust models for: (a) random environments; (b) real
environments; No. times trust model selected suitable for: (c)
random environments; (d) real environments
MAE values Fig. 6(b) smaller Fig. 6(a), though accuracy real
environments (Table 3) lower unknown random environments. because, assume
sellers (in real environments) either high low quality (while unknown random
environments seller quality uniformly distributed [0, 1]), thus easily identifiable candidate trust models. also find average MAE determining seller trustworthiness using
truly suitable trust models unknown real environments 0.01, unknown
random environments 0.22, comparatively larger value. Thereby, unknown real environments, framework chooses trust model whose MAE similar truly suitable
trust model obtain better accuracy, case lower value MAE
unknown random environments. greater rate MAE decreases DT evident
Fig. 6(b), since real environments assume seller trustworthiness binary, thereby, even
small variation choice trust models impact MAE values great extent.
Fig. 7(a-b) show MAE framework comparison trust models unknown random unknown real environments. trust models unknown environ-

497

fi4

4

3

3
Utility

Utility

RISSAPPANE & Z HANG

2

1

0

2

1

0

BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NN
B na ra E Co
g
liz l
ed

(a)

BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NN
B na ra E Co
g
liz l
ed

(b)

Figure 8: Average utility buyers: (a) random environments; (b) real environments
ment, use best parameter values. show performance k-NN, K-dT DT using
2268 simulated environments, obtain best performance. demonstrate scenario
buyers may choose aggregate outcomes trust models (instead using single
suitable trust model), determining seller trustworthiness, also show MAE obtained
adopting heuristic denoted AVG Fig. 7(a-b). Fig. 7(a), find k-NN
K-dT obtain lowest MAE 0.25, showing able choose better trust models
evaluate seller trustworthiness always applying single model. DT obtains MAE 0.31,
higher value Personalized MAE 0.30. AVG obtains higher MAE 0.44, showing
using aggregated outcome trust models may result accurate values seller
trustworthiness. unknown real environments (Fig. 7(b)), k-NN K-dT obtain
lowest MAE (0.022 0.025, respectively) compared trust models.
Fig. 7(c-d) shows numbers unknown random environments unknown real environments, respectively trust model chosen suitable one, using k-NN
retrieval technique (K-dT retrieval also obtains similar values). numbers 28, 4, 19, 14, 2, 22
11 BRS, iCLUB, TRAVOS, Personalized, Referral, BLADE Prob-Cog, respectively
100 unknown random environments, 3, 5, 13, 22, 1, 0 4 models 48 unknown real environments. numbers signify framework able choose different trust
models candidate set various unknown environments. difference use times
trust models random real environments also indicate trust models perform
differently different kinds environments.
Fig. 8(a-b) show average utility buyers e-marketplace corresponding
MAE trust models Fig. 7(a-b). Specifically, buyer gains reward +5
chooses high quality seller (by evaluating trustworthiness sellers market using
prescribed trust model), Tstrue > 0.5 penalty 5, choosing low quality seller
Tstrue 0.5, transaction. Fig. 8(a) shows k-NN K-dT obtain highest utility
2.20 2.19, respectively. trend also shows trust models higher MAE (in Fig. 7(a))
lower utility lower MAE, buyers able accurately
predict seller trustworthiness, correctly choose good quality sellers transaction partners.
Fig. 8(b) also shows k-NN K-dT obtain highest utility 3.53 3.50, respectively.
experiments Fig. 8(a-b) also justify MAE suitable metric assess performance
498

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

k-NN

5

DT

K-dT

4

4

3

3

Seconds

Seconds

5

2

1

0

DT

K-dT

k-NN

2

1

324

648

972

1296

0

2268

|Simulated Environments|

324

648

972

1296

2268

|Simulated Environments|

(a)

(b)

Figure 9: Time choose best trust models: (a) random environments; (b) real environments
trust model given environment, (indirectly) monitors decisions buyers,
large impact utility/value addition gained transactions sellers.
Fig. 9(a-b) show time taken framework choose best trust models unknown random real environments, using k-NN, K-dT DT retrieval techniques. Though
K-dT obtains similar accuracy k-NN (Table 3 Fig. 7), greatly improves time taken
find suitable trust model, shown Fig. 9(a-b). Specifically, Fig. 9(a) shows time
taken find suitable trust models 100 unknown random environments k-NN, K-dT
DT 4.18s, 1.40s 1.10s, using 2268 simulated environments case base, respectively.
find K-dT DT approaches faster k-NN, compares features
unknown environment cases case base (Soltani, 2013). K-dT DT use
tree structure represent cases case base (as described Sec. 4.2.2) retrieve
suitable trust model traversing tree. However, decision tree retrieval slightly faster
K-dT. K-dT, dimensionality (number features) cases number similar cases (k nearest neighbors) needed retrieved, affect retrieval time
(requiring number leaves visited backtracking). Literature (Ahmed, 2004;
Vempala, 2012) also shows high-dimensional data (greater 20), leaves
K-d tree visited efficiency better exhaustive k-NN search,
concern feature space framework increased. Fig. 9(b),
see unknown real environments K-dT DT require lesser retrieval time k-NN.
time taken k-NN, K-dT DT 3.15s, 0.83s 0.35s, using 2268 simulated environments
case base, respectively. However, values lower Fig. 9(a), since consider
time taken choose best trust models 100 unknown random environments,
number real environments considered 48. Though time taken K-dT slightly
greater DT, still comparable shows much better performance terms retrieval
accuracy (Table 3 Fig. 7).
5.4.2 P ERFORMANCE C OMPARISON E XTREME CENARIOS
Fig. 10 shows MAE trust models 4 extreme scenarios (i.e., large environments, extremely sparse environments, environments dynamic seller buyer behavior environments many attacks). Table 4 presents probability choosing trust models

499

fi0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

MAE

MAE

RISSAPPANE & Z HANG

0.3

0.3

0.2

0.2

0.1

0.1

0

0

BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NN
B na ra E Co
g
liz l
ed

(b)

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

MAE

MAE

(a)

0.3

0.3

0.2

0.2

0.1

0.1

0

BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NN
B na ra E Co
g
liz l
ed

0

BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NN
B na ra E Co
g
liz l
ed

(c)

BR iC TR Pe BL Pr AV K kS LU AV rso fer AD ob- G -dT NN
B na ra E Co
g
liz l
ed

(d)

Figure 10: MAE framework trust models extreme scenarios: (a) large environments; (b) extremely sparse environments; (c) environments dynamic seller
buyer behavior (d) environments many attacks
4 extreme cases using k-NN retrieval technique (K-dT obtains almost probabilities k-NN). Results show k-NN K-dT outperform DT, AVG trust
models, performing equally well environments.
specifically, Fig. 10(a) shows k-NN K-dT obtain lowest MAE 0.24 large
environments. also find iCLUB, TRAVOS Personalized obtain smaller MAE
trust models large environments. reason three trust models able
distinguish dishonest honest advisors get sufficient rating sources. Table 4
large environments, see k-NN selects iCLUB, TRAVOS Personalized
highest probabilities, 26.9%, 23.8% 38.2%, respectively. Fig. 10(a) also shows DT
AVG obtain larger MAE value (0.27 0.28, respectively) k-NN K-dT.
Fig. 10(b), sparse environments, k-NN K-dT obtain lowest MAE 0.24
0.23 DT obtains MAE value 0.35. BRS Prob-Cog perform better trust
models, BRS adopts majority-rule consider opinions advisors,

500

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

Trust Models
BRS
iCLUB
TRAVOS
Personalized
Referral
BLADE
Prob-Cog

Large
0.6%
26.9%
23.8%
38.2%
0.6%
9.3%
0.6%

Sparse
25.7%
13.9%
2.1%
2.8%
2.8%
0.0%
52.7%

Dynamic
7.4%
7.2%
4.6%
27.8%
18.5%
31.5%
3.0%

Many Attacks
6.9%
10.3%
13.8%
58.7%
0.0%
0.0%
10.3%

Table 4: Probability choosing trust models four extreme e-market scenarios
Prob-Cog extends incompetence tolerance threshold incorporate larger number advisors
ratings. models obtain comparatively low MAE less restrictive accepting
opinions advisors trust models. Table 4, sparse environments, k-NN selects
BRS Prob-Cog highest probabilities, 25.7% 52.7%, respectively.
Fig. 10(c) shows k-NN K-dT obtain lowest MAE Personalized BLADE
outperform trust models environments sellers change behavior dynamically.
explain, Personalized considers advisors latest ratings within certain time window, alleviates influence sellers dynamic behavior. BLADE re-interprets advisors ratings based
learning, thereby takes account changing behavior buyers sellers. Table 4, k-NN
selects Personalized BLADE highest probabilities, 27.8% 31.5%, respectively.
Fig. 10(c) also signifies framework able deal scenarios buyers sellers
change behavior, case base already contains environments representing dynamic behavior (as described Sec. 5.1.1), along suitable trust models. Fig. 10(d)
shows k-NN K-dT outperform trust models lowest MAE 0.02 Personalized TRAVOS perform well environments many attacks. DT obtains MAE
0.03, comparable greater value k-NN K-dT. characteristics attacks play major role judging performance trust models. extreme environments, attackers
(dishonest advisors) first give honest ratings non-target sellers promote themselves,
provide unfair ratings bad-mouth target sellers. performance Personalized TRAVOS
better model advisor trustworthiness accurately comparing buyers
opinions advisors ratings commonly rated sellers. Also, environments,
select buyers sufficient personal experience (ratings) Personalized TRAVOS take
advantage of. Table 4, k-NN selects Personalized TRAVOS probabilities 58.7%
13.8% environments many attacks, respectively.
summary, Fig. 10 Table 4, results indicate framework (using k-NN
K-dT retrieval technique) able select suitable trust models extreme scenarios obtain
accurate seller trustworthiness AVG individual trust model. Also, find
performance k-NN K-dT retrieval better DT retrieval cases. Decision
trees induction models learn rules (based features) determine suitable trust models.
comes close method PTF (described Sec. 2) works, difference
decision trees rules learned organized form trees, PTF rules
need manually specified user pre-defined format (Huynh, 2009). However,
see available number simulated environments (2268) case base, decision trees
cannot learn complete domain knowledge construct trees help accurately determine
suitable trust models. thereby infer using rule based system PTF also
result moderate performance decision trees (with given size case base).

501

fiI RISSAPPANE & Z HANG

5.4.3 NALYSIS P OSSIBLE MPROVEMENTS F RAMEWORK
demonstrated literature feature weighting (assigning weights individual features) feature selection (selecting subset relevant features ignoring others), improve performance k-NN (Tahir, Bouridane, & Kurugollu, 2007). Thus, improve
performance framework (using k-NN thereby, K-dT retrieval), assign weights
(most influential) feature. conduct experiments using linear adaptive filters-least
mean squares (Mitchell, 1997), learning rate 0.2, determine weights features,
using 972 randomly generated environments. weights 13 influential features (in
order C4 Table 1), determined using k-NN are, 0.16, 0.02, 0.01, 0.02, 0.03, 0.2, 0.09,
0.05, 0.04, 0.1, 0.17, 0.01 0.1, respectively. use weights K-dT analyze
performance. Table 5 shows performance k-NN K-dT, using feature weights
calculating similarity environments order determine suitable
trust model. k-NN obtains improvement 1.0% 1.5% terms accuracy selecting
suitable trust model unknown random real environments (when compared values
Table 3, k-NN K-dT assign equal weights 13 influential features), respectively.
extreme scenarios, k-NN obtains improvement (at most) 2.0%. K-dT obtains similar
accuracy improvement 0.5% 2.0% unknown random real environments,
extreme scenarios, obtains improvement (at most) 2.0%.
k-NN + feature weights
Correct Model
Correct Model
Correct Models Paras
Correct Model Paras
MAE
Accuracy Improvement
K-dT + feature weights
Correct Model
Correct Model
Correct Models Paras
Correct Model Paras
MAE
Accuracy Improvement

Random
98.0%
98.5%
97.0%
98.0%
0.23
1.0%
Random
97.5%
98.5%
97.0%
98.0%
0.23
0.5%

Real
89.0%
98.0%
82.0%
97.2%
0.02
1.5%
Real
89.0%
97.0%
82.0%
97.2%
0.02
2.0%

Large
93.0%
95.0%
95.0%
96.1%
0.22
1.0%
Large
94.0%
95.0%
95.0%
96.1%
0.21
1.0%

Sparse
90.0%
98.0%
96.0%
97.0%
0.22
1.0%
Sparse
90.2%
97.0%
96.0%
97.0%
0.22
2.0%

Dynamic
97.0%
97.0%
97.0%
98.0%
0.30
2.0%
Dynamic
97.1%
97.0%
98.0%
98.0%
0.30
2.0%

Attacks
86.0%
92.0%
80.1%
95.4%
0.02
0.0%
Attacks
85.0%
92.0%
80.0%
96.0%
0.02
0.0%

Table 5: Influence using feature weights k-NN K-dT
Also, mentioned Sec. 4, framework extended adding new features trust
models. Specifically, add new feature need to: 1) generate new set Eknown environments,
including new feature; 2) select influential features F (using 5 correlation
regression techniques mentioned Table 1), new extended feature set, testing
randomly generated Etest environments, 3) build new case base. Thus, time complexity
adding new feature O((|Eknown |+|Etest |)|TM|+5tmodel ), includes time taken
find actual performance defense models TM known Eknown test Etest
environments (represented (|Eknown | + |Etest |) |TM|) time taken tmodel (model
{k-NN, K-dT}) find suitable trust models using 5 different feature combinations
(represented 5 tmodel ). |Eknown | = 2268, |Etest | = 972, |TM| = 45, k = 3
model = K-dT, total time taken build new case base adding new feature nearly
3 hours. Adding new trust model simply takes 3.6 mins (O(|Eknown |)), requires
run new trust model 2268 environments build new case base. Though
502

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

calculations show considerable computation time involved, adding new feature trust
model lead improvement performance framework. Specifically, adding
new feature help accurately select suitable trust models adding new trust model
may result lower MAE certain environments, leading better decision making thereby
improving utility buyers environment shown Fig. 8. Also,
computation need done off-line online effort much lower seen Fig. 9.

6. Conclusion Future Work
paper, propose case-based reasoning framework choose suitable trust models
environments ground truth agents behavior unknown. framework,
case base built generating number simulated environments determining
suitable trust models environments. framework also offers choose different
techniques (k-nearest neighbors, K-d trees decision trees) case retrieval. Given unknown
environment, similar case(s) retrieved using retrieval techniques. Then, trust
model corresponding similar case(s) chosen suitable one unknown
environment. Experimental results confirm framework accurately select suitable trust
models various unknown environments (both simulated real e-marketplaces). also find
k-nearest neighbors K-d tree retrieval techniques accurately choose suitable trust
models determining similar case(s) case base decision trees, especially
number simulated environments (cases case base) much smaller. also
demonstrated K-d trees significantly improve time complexity choosing suitable trust
models k-nearest neighbors. Experiments also verify using framework choose
suitable trust models unknown environments (using k-nearest neighbors K-d tree retrieval)
better always applying single trust model (or aggregate trust models), terms
accuracy evaluating seller trustworthiness.
Currently, framework achieves best performance number simulated environments case base large 2268 environments; performance increase
adding simulated environments. adding simulated environments feasible
option improve performance framework, requires tremendous off-line computation determine suitable trust models simulated environments build case base.
future, investigate methods generate simulated environments representative real world e-marketplaces, performance framework much higher
even case base contains smaller number simulated environments. also analyze
effective feature selection techniques accurately select trust models regard.
Another important direction future work consider scenario features
unknown environment deviate similar simulated environment determined
framework, execution time. One possible solution use proposed framework
choose suitable trust model regular intervals time unknown environment
operates. conduct detailed experiments analyze performance framework
scenarios. also continue evaluate framework incorporating sophisticated
trust models involving real data sets.

503

fiI RISSAPPANE & Z HANG

References
Aamodt, A., & Plaza, E. (1994). Case-based reasoning: Foundational issues, methodological variations, system approaches. AI communications, 7(1), 3959.
Ahmed, Y. S. (2004). Multiple Random Projection Fast, Approximate Nearest Neighbor Search
High Dimensions. Ph.D. thesis, University Toronto.
Duda, R. O., & Hart, P. E. (1973). Pattern classification scene analysis, Vol. 3. Wiley.
Fullam, K. K., & Barber, K. S. (2007). Dynamically learning sources trust information: Experience vs. reputation. Proceedings International Joint Conference Autonomous
Agents Multiagent Systems (AAMAS).
Hang, C. W., Wang, Y., & Singh, M. P. (2009). Operators propagating trust evaluation
social networks. Proceedings International Joint Conference Autonomous
Agents Multiagent Systems (AAMAS).
Hastie, T., Tibshirani, R., & Friedman, J. (2009). elements statistical learning, Vol. 2.
Springer.
Hoffman, K., Zage, D., & Nita-Rotaru, C. (2009). survey attack defense techniques
reputation systems. ACM Computing Surveys (CSUR), 42(1), 1.
Huynh, T. (2009). personalized framework trust assessment. Proceedings ACM
Symposium Applied Computing (SAC).
Irissappane, A. A., Jiang, S., & Zhang, J. (2013). framework choose trust models different
e-marketplace environments. Proceedings 23rd International Joint Conference
Artificial Intelligence (IJCAI).
Irissappane, A. A., Oliehoek, F. A., & Zhang, J. (2014). POMDP based approach optimally
select sellers electronic marketplaces. Proceedings 13th International Conference
Autonomous Agents Multiagent Systems (AAMAS).
Irissappane, A. A., & Zhang, J. (2014). testbed evaluate robustness reputation systems
e-marketplaces. Proceedings 13th International Conference Autonomous Agents
Multiagent Systems (AAMAS).
Jsang, A., & Ismail, R. (2002). Beta reputation system. Proceedings 15th Bled
Electronic Commerce Conference.
Liu, S., Zhang, J., Miao, C., Theng, Y., & Kot, A. (2011). iCLUB: integrated clustering-based
approach improve robustness reputation systems. Proceedings International
Conference Autonomous Agents Multiagent Systems (AAMAS).
Mitchell, T. M. (1997). Machine learning. McGraw-Hill.
Noorian, Z., Marsh, S., & Fleming, M. (2011). Multi-layer cognitive filtering behavioral modeling. Proceedings International Conference Autonomous Agents Multiagent
Systems (AAMAS).
Quinlan, J. R. (1986). Induction decision trees. Machine learning, 1(1), 81106.
Quinlan, J. R. (1996). Improved use continuous attributes C4.5. Journal Artificial Intelligence Research (JAIR), 4(1), 7790.

504

fiA F RAMEWORK C HOOSE RUST ODELS IFFERENT E-M ARKETPLACES

Quinlan, J. R. (1993). C4. 5: Programs machine learning, Vol. 1. Morgan kaufmann.
Regan, K., Poupart, P., & Cohen, R. (2006). Bayesian reputation modeling e-marketplaces sensitive subjectivity, deception change. Proceedings National Conference
Artificial Intelligence (AAAI).
Sabater, J., & Sierra, C. (2005). Review computational trust reputation models. Artificial
Intelligence Review, 24(1), 3360.
Soltani, S. (2013). Case-based reasoning diagnosis solution planning. Technical Report,
Queens University.
Sormo, F., Cassens, J., & Aamodt, A. (2005). Explanation case-based reasoningperspectives
goals. Artificial Intelligence Review, 24(2), 109143.
Tahir, M. A., Bouridane, A., & Kurugollu, F. (2007). Simultaneous feature selection feature
weighting using hybrid tabu search/k-nearest neighbor classifier. Pattern Recognition Letters,
28(4), 438446.
Teacy, W., Patel, J., Jennings, N., & Luck, M. (2006). TRAVOS: Trust reputation context
inaccurate information sources. Autonomous Agents Multiagent Systems, 12, 183198.
Vempala, S. S. (2012). Randomly-oriented k-d trees adapt intrinsic dimension. Proceedings
32nd International Conference Foundations Software Technology Theoretical
Computer Science (FSTTCS).
Wang, Y., Hang, C.-W., & Singh, M. P. (2011). probabilistic approach maintaining trust based
evidence. Journal Artificial Intelligence Research, 40(1), 221267.
Watson, I. (1999). Case-based reasoning methodology technology. Knowledge-based
systems, 12(5), 303308.
Watson, I., & Marir, F. (1994). Case-based reasoning: review. Knowledge Engineering Review,
9(4), 327354.
Wess, S., Althoff, K.-D., & Derwand, G. (1994). Using k-d trees improve retrieval step
case-based reasoning. Springer.
Wettschereck, D., & Aha, D. W. (1995). Weighting features. Case-based Reasoning Research
Development, 347358.
Whitby, A., Jsang, A., & Indulska, J. (2004). Filtering unfair ratings bayesian reputation
systems. Proceedings AAMAS Workshop Trust Agent Societies (TRUST).
Yang, Y., Feng, Q., Sun, Y. L., & Dai, Y. (2008). RepTrap: novel attack feedback-based
reputation systems. Proceedings International Conference Security Privacy
Communication Networks (SecureComm).
Yu, B., & Singh, M. (2003). Detecting deception reputation management. Proceedings
International Joint Conference Autonomous Agents Multiagent Systems (AAMAS).
Zhang, J., & Cohen, R. (2008). Evaluating trustworthiness advice seller agents emarketplaces: personalized approach. Electronic Commerce Research Applications,
7(3), 330340.
Zhang, J. (2009). Promoting honesty e-marketplaces: Combining trust modeling incentive
mechanism design. Ph.D. thesis, University Waterloo.

505

fiJournal Artificial Intelligence Research 52 (2015) 445-475

Submitted 09/14; published 03/15

Modeling Lifespan Discourse Entities
Application Coreference Resolution
Marie-Catherine de Marneffe

MCDM @ LING . OHIO - STATE . EDU

Linguistics Department
Ohio State University
Columbus, OH 43210 USA

Marta Recasens

RECASENS @ GOOGLE . COM

Google Inc.
Mountain View, CA 94043 USA

Christopher Potts

CGPOTTS @ STANFORD . EDU

Linguistics Department
Stanford University
Stanford, CA 94035 USA

Abstract
discourse typically involves numerous entities, mentioned once. Distinguishing die one mention (singleton) lead longer lives
(coreferent) would dramatically simplify hypothesis space coreference resolution models,
leading increased performance. realize gains, build classifier predicting
singleton/coreferent distinction. models feature representations synthesize linguistic insights
factors affecting discourse entity lifespans (especially negation, modality, attitude
predication) existing results benefits surface (part-of-speech n-gram-based)
features coreference resolution. model effective right, feature representations help identify anchor phrases bridging anaphora well. Furthermore, incorporating
model two different state-of-the-art coreference resolution systems, one rule-based
learning-based, yields significant performance improvements.

1. Introduction
Karttunen imagined text interpreting system designed keep track individuals, is,
events, objects, etc., mentioned text and, individual, record whatever said
(Karttunen, 1976, p. 364). used term discourse referent describe abstract individuals.
discourse referents easily mapped specific entities world, proper
names. Others indeterminate sense compatible many different real-world
entities, indefinites like train. either case, discourse referents enter anaphoric
relations discourse; even know exactly real-world object train picks
heard train distance . . . , nonetheless refer subsequent pronouns
ascribe properties (. . . loud horn).
discourse referents enjoy repeat appearances discourse. lead long lives
appear wide variety discourse contexts, whereas others never escape birthplaces,
dying one mention. central question paper factors influence
lifespan discourse referent. focus noun phrases, direct identifiers
discourse referents English. specifically, seek predict whether given discourse
c
2015
AI Access Foundation. rights reserved.

fiDE

ARNEFFE , R ECASENS & P OTTS

referent coreferent (mentioned multiple times given discourse) singleton (mentioned
once). ability make distinction based properties noun phrases used
identify referents (henceforth, mentions) would benefit coreference resolution models, simplifying hypothesis space consider predicting anaphoric links, could improve
performance tasks require accurately tracking discourse entities, including textual entailment (Delmonte, Bristot, Piccolino Boniforti, & Tonelli, 2007; Giampiccolo, Magnini, Dagan,
& Dolan, 2007) discourse coherence (Hobbs, 1979; Grosz, Joshi, & Weinstein, 1995; Kehler,
2002; Barzilay & Lapata, 2008; Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, & Webber, 2008).
existing literature provides numerous generalizations relevant singleton/coreferent
distinction. known, example, internal syntax morphology phrase used
establish discourse referent provide important clues lifespan referent (Prince,
1981a, 1981b; Wang, McCready, & Asher, 2006). Information structuring also important; certain
grammatical discourse roles correlate long lifespans (Chafe, 1976; Hobbs, 1979; Walker,
Joshi, & Prince, 1997; Beaver, 2004). Features based insights long integrated
coreference resolution systems. contribution explore interaction
features semantic operators like negation, modals, attitude predicates (know, certain,
wonder). interactions Karttunens primary focus (Karttunen, 1973, 1976),
long dominated work dynamic approaches linguistic meaning (Kamp, 1981; Heim, 1982,
1992; Roberts, 1990; Groenendijk & Stokhof, 1991; Bittner, 2001). Here, highlight importance interactions predicting lifespans discourse referents actual text.
approach also capitalizes results Durrett Klein (2013) Hall, Durrett,
Klein (2014) concerning power surface features natural language processing (NLP)
tasks. authors show large sets easily extracted part-of-speech (POS) n-grambased features achieve results least good achieved hand-engineered
linguistic features. therefore investigate contribution surface features predicting
lifespan discourse entities. find surface features alone substantial predictive value
task, adding specialized linguistic features leads reliable performance gains.
suggests linguistic constraints relevant lifespan prediction go beyond
approximated surface-level information given available data.
first step analysis bring insights linguistic theories together
single logistic regression model lifespan model assess predictive power real
data. show linguistic features generally behave existing literature leads us
expect, model effective predicting whether given mention singleton
coreferent. second step bring surface features obtain predictive model.
provide initial assessment engineering value making singleton/coreferent distinction
incorporating lifespan model two different, state-of-the-art coreference resolution
systems: rule-based Stanford coreference system (Lee, Peirsman, Chang, Chambers, Surdeanu,
& Jurafsky, 2011) learning-based Berkeley coreference system (Durrett & Klein, 2013).
both, adding features results significant improvement precision CoNLL-2011
CoNLL-2012 Shared Task data, across standardly used coreference resolution measures,
see reliable boosts recall well.
article subsumes extends work Recasens, de Marneffe, Potts (2013).
specific differences follows. First, freed NAACLs tight space constraints, provide
much in-depth linguistic analysis various features lifespan model, include
details throughout. Second, examine contribution surface features lifespan
446

fiM ODELING L IFESPAN ISCOURSE E NTITIES

model. Third, assess value lifespan model predicting phrases act
anchors bridging anaphora. Fourth, give fuller evaluation coreference applications
model, incorporate best lifespan model learning-based system (the Berkeley
coreference system), complementing previous results rule-based Stanford coreference
system. Fifth, use recent version CoNLL scorer (v8.0), includes results
according BLANC fixes bug incorrectly boosted B3 CEAF scores points.
Sixth, benefit Kummerfeld Kleins (2013) error analysis tool gain deeper insights
errors lifespan model helps with.

2. Linguistic Insights
section briefly summarizes previous research anaphora resolution, discourse structure,
discourse coherence linguistic literature. goal obtain clear picture
lifespan discourse referent shaped features mentions local morphosyntactic features also features syntactic semantic environments
occur. insights gather section inform design feature extraction functions
lifespan model (Section 5) turn shape contributions Stanford Berkeley
coreference systems (Section 8).
Karttunen (1976) primarily concerned ways semantic scope
indefinite influences lifespan associated discourse referent. three-sentence discourse
(1), indefinite exam question sentence 1 text-level scope. result, associated
discourse referent free lead long life, linking mention also text-level
(sentence 2) one embedded negation (sentence 3).
(1)

Kim read exam question. hard. didnt understand it.

contrast, Karttunen observed, indefinite interpreted scope negation,
typically available anaphoric reference inside negative environment, (2),
outside it, (3). (We use # mark discourses incoherent intended construal.)
(2)

Kim didnt understand exam question even reading twice.

(3)

Kim didnt understand exam question. # hard.

course, (3) coherent construal exam question interpreted taking widescope respect negation (there question Kim didnt understand). inverse scope
readings often disfavored, become salient modifiers like certain particular included (Fodor & Sag, 1982; Schwarzschild, 2002), mention contains
positive polarity item, is, item like tons resists scoping negation
semantically (Baker, 1970; Israel, 1996, 2001):
(4)

Kim didnt understand particular exam question. pondered hours avail.

(5)

Kim didnt understand exam question. pondered hours avail.

Conversely, using negative polarity item (NPI) like inside indefinite mention essentially ensures narrow-scope reading (Ladusaw, 1996; Israel, 2004), leads impossibleto-resolve anaphoric link simple variants (3):
(6)

Kim didnt understand exam question. # hard.
447

fiDE

ARNEFFE , R ECASENS & P OTTS

pattern Karttunen saw semantic scope anaphoric potential intimately related: given mention participate anaphoric relationships within scope,
outside it. Broadly speaking, familiar quantificational binding logical languages
(Cresswell, 2002) variable scope control structures programming languages (Muskens,
van Benthem, & Visser, 1997). Thus, indefinite text-level scope free reign, whereas one
inside scope operator like negation restricted links span outer boundaries scopal environment. semantic generalizations might directly
reflected surface syntax, interpretive preferences internal morphosyntactic features
mention help disambiguate intended logical form.
Karttunen (1976) immediately generalized observations negation discourse reference modal auxiliaries non-factive attitude predicates like want claim. following
based original examples:
(7)

Bill make kite. # long string.

(8)

John wants catch fish. # see here?

(9)

Sandy claims Jesse bought bicycle. # green frame.

negation, pattern makes intuitive sense. Bills abilities regarding kite construction
involve specific kite, hence first sentence (7) automatically establish
right sort discourse referent. Similarly, wanting catch fish guarantee salience (or
even existence) fish, Sandy might unreliable source bicycle status
outside semantic scope claim.
(7)(9) cohere indefinite interpreted outside scope relevant semantic
operator. relative preferences surface inverse scope harder characterize
negation, influenced complex ways semantics pragmatics
attitude predicate, reliability source information, nature conversational issues goals. example, speaker (9) regards Sandy reliable source
regarding Jesses bike buying, bicycle likely attain text-level scope by-product
Jesse bought bicycle becoming text-level commitment. Karttunen (1973) discusses patterns, observing that, many contexts, pragmatic pressures encourage embedded content become
elevated text level way. De Marneffe, Manning, Potts (2012) study newspaper data
extremely common pattern attitude verbs tend function evidential markers source embedded content (Rooryck, 2001; Simons, 2007).
see later attitude predicates seem encourage long lifespans OntoNotes data (the
majority news-like), arguably result pragmatic factors.
far restricted attention anaphoric links indefinite establishes new
discourse referent pronoun refers it. observations carry directly links indefinites definite noun phrases, linguistic theories treat roughly pronouns additional
descriptive content (for discussion, see work Elbourne, 2008). mention-patterns tend
quite different, though. discourse referents established definites named entities,
interactions negation operators simpler definites named entities
interact scopally operators (but see work Aloni, 2000, related issues
involving presupposition intensionality). Thus, anaphoric connections unconstrained
factors discussing. Conversely, truly quantified phrases like student every linguist severely limited, interaction operators also
448

fiM ODELING L IFESPAN ISCOURSE E NTITIES

deficiencies comes establishing discourse referents. cases
expressions establish new discourse referents, seem infrequent unusual (Wang
et al., 2006).
Cross-cutting considerations factors long central studies coreference anaphora within computational linguistics NLP. instance, animate nouns
generally likely lead long discourse lives, whereas mentions refer abstract objects like quantities, percentages, measures tend singleton. assume
statistical patterns derive, narrow linguistic constraints, rather general cognitive
biases concerning people conceptualize discuss different kinds objects. However,
evidence biases make way grammars specific languages
form morpho-semantic phenomena like obviation (Aissen, 1997) differential object marking
(Aissen, 2003).
syntactic environment phrases occur also modulate anaphoric potential hence lifespans. example, Prince (1981b) reports semantically indefinite
phrases using this, guy back row, highly likely referred
subsequent clause. Similarly, Chafe (1976) shows information structuring choices also
predictive whether given noun phrase serve antecedent later referential devices.
also close correlations syntactic topic position leading long discourse life (Grosz et al., 1995; Beaver, 2004); focused evaluation ideas handling
coreference, see work Beaver (2007).
seek incorporate observations lifespan model. additional patterns literature pursue, infrequent
data. example, Karttunen (1976) also identified natural class counterexamples basic scope generalizations: certain sequences intensional predicates support exceptional anaphoric
links, phenomenon later studied systematically heading modal subordination
(Roberts, 1990, 1996):
(10)

Frank wants marry rich linguist. # kind.

(11)

Frank wants marry rich linguist. kind.

addition, mentions inside parenthetical clauses less likely introduce long-term discourse
referents, due likelihood parenthetical clause conveys secondary content
compared main clause hosts (Potts, 2005). Thus, anaphoric links
parentheticals possible (AnderBois, Brasoveanu, & Henderson, 2010; Potts, 2012), seem
arise relatively rarely, valuable piece practical advice appositive-rich texts like scientific
papers unfortunately one could put action here.
Karttunens observations helped set agenda dynamic approaches semantics next
decades (Kamp, 1981; Heim, 1982; Groenendijk & Stokhof, 1991). literature refined
extended observations numerous ways. Taken together, findings suggest intensional
operators negation interact complex ways discourse anaphora. default, expect
phrases introduced scope operators lead short lifespans, possible
take wide-scope respect operators, broadens range anaphoric links
establish. readings favored disfavored pragmatics situation well
lexical syntactic nature phrases involved. follows, seek model
interactions use inform lifespan model.
449

fiDE

ARNEFFE , R ECASENS & P OTTS

3. Previous Engineering Efforts Quantitative Evaluations
insights inspired NLP researchers try predict roles different mentions
play coreference chains. Previous work area subdivided detecting four
different targets: non-referential mentions, non-anaphoric mentions, discourse-new mentions,
non-antecedent mentions. terminology always used consistent way linguistics NLP, believe results ultimately brought together. Here, aim
clarify terminology find common insights behind various features used.
first single singleton/coreferent detection task such, work finds
important antecedents existing literature.
3.1 Non-referential Mentions
noun phrases refer discourse referent rather fill syntactic position.
English, canonical example non-referential NP expletive pronoun it, obvious
succeed. lexical NPs introduce discourse referent either,
linguist Pat linguist: mention Pat introduce discourse referent, linguist
simply predicates something her. Detecting non-referential uses plays role coreference
resolution: since NPs pick discourse referents (new existing), cannot enter
anaphoric relations kind consideration here.
Early work non-referentiality detection focuses pronoun it, aiming distinguish referential uses non-referential ones. Paice Husk (1987) develop rule-based system, Evans
(2001) uses supervised approach, Muller (2006) focuses use spoken dialog.
studies mainly employ lexico-syntactic features immediate surrounding context
pronoun. Similarly, Bergsma, Lin, Goebel (2008) explore system uses Web-count features derived Google n-grams data (Brants & Franz, 2006) capture frequent
subjects replace pronoun it: referential cases (e.g., able to), words
frequent n-grams, able China able to, whereas non-referential
cases, pronoun likely frequent subject (e.g., important to).
recently, Bergsma Yarowsky (2011) develop NADA system, improves
Bergsma et al. (2008) incorporating lexical features. lexical features indicate presence
absence strings specific positions around pronoun: three-grams five-grams
spanning pronoun; two tokens pronoun five tokens pronoun
positions; token within twenty tokens right pronoun; token within ten
tokens left pronoun named entity belongs following list: that, this,
and, said, says, it, It, its, itself. Using types features, lexical Web-count, achieve
85% accuracy different datasets.
Byron Gegg-Harrison (2004) apply linguistic insights highlighted Karttunen
(Section 2) special case pronoun resolution, seeking discard non-referential indefinite
NPs set potential antecedents pronouns. use hard filter non-referential
mentions, looking presence indefinites, negation, apposition (hand-labeled), modals, adjectival phrases predication adjuncts (tagged -CLR Penn Treebank), predicates copular
verbs (tagged -PRD), noun phrases express value. found removing nonreferential mentions gave small boost performance pronoun resolution.
450

fiM ODELING L IFESPAN ISCOURSE E NTITIES

3.2 Non-anaphoric Mentions
Non-anaphoric NPs whose interpretation depend previous mention
text. example, phrase new Scorsese movie stars De Niro (12) (while manifesting
many kinds context dependence) depend overt phrases order capture
descriptive content. contrast, movie (13) crucially links back previous sentence
descriptive content; superficially involves predicate movie, construed
additional property seen speaker previous night.
(12)

Last night, watched new Scorsese movie stars De Niro.

(13)

Last night, watched movie read paper. movie directed Scorsese.

direct correspondence anaphora coreferentiality. Coreferent mentions
non-anaphoric (as text containing multiple tokens phrase White House),
anaphoric mentions coreferent non-coreferent (van Deemter & Kibble, 2000). Cases
bridging anaphora (Clark, 1975) like (14) involve non-coreferent anaphora. Here, ceiling
interpreted ceiling room mentioned previous sentence, thus anaphoric
room without coreferent phrase discourse.
(14)

looked room. ceiling high.

return cases Section 6, use lifespan model characterize sense
bridging anchors like room lead longer lifespans count strictly coreferent
mentions would suggest.
Poesio, Uryupina, Vieira, Alexandrov-Kabadjov, Goulart (2004) Poesio, AlexandrovKabadjov, Vieira, Goulart, Uryupina (2005) summarize previous approaches non-anaphoricity
detection, refer discourse-new detectors. Vieira Poesio (2000) focus definite NPs use syntactic heuristics based pre- post-modification distinguish
anaphoric non-anaphoric NPs. Modification good indicator anaphoricity; heavily modified phrases like new Scorsese movie stars De Niro tend non-anaphoric, whereas short
phrases general descriptive content like movie tend anaphoric. Bean Riloff (1999)
also focus definite NPs: addition syntactic heuristics based pre- post-modification,
use techniques mining lists likely non-anaphoric NPs (such presence NPs first
sentence document). Compared Vieira Poesio (2000), obtain substantially higher
recall (with recall precision figures around 80%).
non-anaphoricity detector, Poesio et al. (2005) use head feature (distance
NPs identical heads), syntactic features (e.g., occurring inside appositive copular clause,
post-modified), capitalization mention, presence mention first sentence
Web page, position mention text, probability mention definite
computed Web using technique Uryupina (2003). find important
features head feature definiteness probabilities.
3.3 Discourse-New Mentions
Discourse-new mentions introduce new entity discourse (Prince, 1981b;
Fraurud, 1990). entity might singleton involve chain coreferring mentions
first phrase discourse-new one rest considered discourse-old. Cast
451

fiDE

ARNEFFE , R ECASENS & P OTTS

information status task, goal discourse-new mention detection find discourse referents
previously available hearer/reader; e.g., see work Nissim (2006).
Ng Cardie (2002) develop discourse-new classifier targets every kind NP using
variety feature types: lexical (string head matching, conjunction), morpho-syntactic (definiteness, quantification, number), grammatical (appositional copular context, modifier structure,
proper-noun embedding), shallow semantic (e.g., WordNet features). incorporate
classifier coreference resolution system, pre-filtering NPs tagged discoursenew. However, pre-filtering ultimately hurts coreference resolution system performance: even
though precision increases, recall drops considerably. Section 8.2.3, report similar results
model instantiated discourse-new pre-filtering, find recall drop
avoided filtering applied mention analysis tagged discourse-new
antecedent candidate tagged singleton.
Ng Cardies (2002) work cast non-anaphoricity detection, model perhaps
better described trying distinguish coreferent mentions singleton initiate
coreference chains. specifically, write, positive instance created NP
involved coreference chain head chain (Ng & Cardie, 2002, p. 3),
picks non-initial members coreference chains. Conversely, negative instance created
remaining NPs (Ng & Cardie, 2002, p. 3), i.e., without antecedents.
Uryupina (2009) proposes discourse-new mention detector kind NP. classifier
relies features falling three categories defines: lexical (number words mention), syntactic (POS, number, person, determiner, pre- post-modification), semantic (gender, semantic class), salience (grammatical role, position sentence paragraph).
addition, includes Karttunens features implemented Byron Gegg-Harrison
(2004). classifier also checks mentions identical heads, distance these.
syntactic head features deliver improvements majority baseline (which marks
NP discourse-new), performing almost well features together. Uryupina notes,
however, features, especially based Karttunens ideas,
designed discourse-new mention detection.
Ng Cardie (2002) Uryupina (2009) integrated discourse-new detector
coreference resolution system pipeline manner. joint approach discourse-new detection
coreference resolution, see work Denis Baldridge (2007).
3.4 Non-antecedent Mentions
Uryupina (2009) observes, coreference resolution, matters fact NPs
unavailable antecedents. therefore builds classifier marks NPs likely antecedents
not. system based features discourse-new detector described
(Section 3.3). non-antecedenthood detection, syntactic semantic features lead
significant precision improvement majority baseline (which marks NP nonantecedent), syntactic features alone performing well features together.
3.5 Approach: Singletons
model cross-cuts four categories. Unlike previous models non-referentality,
restricted pronouns indefinite NPs, tries identify kind non-referential NP
well referential NP whose referent mentioned (i.e., singleton). Thus,
452

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Dataset

Docs

Tokens

Training
Development
Test

2,802
343
348

1.3M
160K
170K

ENTIONS
Coreferent Singletons
152,974
18,855
19,407

181,274
23,140
23,657

Table 1: CoNLL-2012 Shared Task data statistics. added singletons (noun phrases annotated coreferent), account 55% referents development set.

non-referential NPs fall singleton class. hand, strict correspondence singleton/coreferent distinction non-anaphoric/anaphoric distinction,
since anaphoricity based whether mention relies previous one interpretation,
whereas singleton/coreferent divide based long lifespan entity is. Similarly,
discourse-new mentions either coreferent singleton classification, depending
whether entity mentioned not.
terms feature representations, tried stay close possible Karttunens
original insights: extract features full syntactic parses, seeking remain faithful
underlying semantic relationships involved, include feature interaction terms capture
complex set dependencies reviewed Section 2. approach allows us evaluate
linguistic ideas quantitatively assess practical contributions full coreference
systems.

4. Data
data used throughout paper come CoNLL-2012 Shared Task data (Pradhan, Moschitti, Xue, Uryupina, & Zhang, 2012), included 1.6M English words OntoNotes
v5.0 (Pradhan & Xue, 2009) several common layers annotation (coreference, parse trees,
named-entity tags, etc.). OntoNotes corpus contains documents seven different domains:
broadcast conversation (20%), broadcast news (13%), magazine (7%), newswire (21%), telephone
conversation (13%), weblogs newsgroups (15%), pivot text (11%). genres
news-like, exception pivot texts (which come New Testament)
telephone conversations. used training, development, test splits defined shared
task (Table 1). Since coreference annotations OntoNotes contain singleton mentions, automatically marked singleton noun phrases annotated coreferent.
excluded verbal mentions.
mark singleton noun phrases annotated coreferent, definition
singletons includes non-referential noun phrases raining, president served
president two terms (Section 3.1). makes practical sense: starting point
coreference resolution systems take noun phrases possible candidates coreference
subsequently find clusters coreferent one another. phrases
accurately identify singleton, phrases exclude clustering step,
translate directly performance gains.
453

fiDE

ARNEFFE , R ECASENS & P OTTS

Referents

23140

2369
Singleton

2

797

415

236

436

140

61

92

3

4

5

6-10

11-15

16-20

>20

Mentions

Figure 1: Distribution referent lifespans 2012 OntoNotes development set.

5. Predicting Lifespans Linguistic Features
describe model predicting lifespan discourse referents using linguistic
factors proposed Section 2. model makes binary distinction discourse referents
part coreference chain (singleton) part one (coreferent).
distribution lifespans data shown Figure 1.
plot gives number entities associated single mention, number associated
two mentions, forth. fact singletons dominate data suggests binary singleton/coreferent division natural one. propensity toward singletons also highlights
relevance detecting singletons coreference system. Following Bergsma Yarowsky
(2011), use logistic regression model, shown perform well range
NLP tasks. fit logistic regression model R (R Development Core Team, 2013) training data, coding singletons 0 coreferent mentions 1. Thus, throughout following
tables coefficient estimates, positive values favor coreferent mentions negative values favor
singletons. turn describing motivating features model.
5.1 Morphosyntax Mention
Table 2 summarizes features model concern internal morphology syntactic
structure mention, giving coefficient estimates. tables, indicated otherwise, coefficient estimates significant p < 0.001. use indicate significance
p < 0.05, indicate estimates p 0.05. morphosyntactic features include type
(pronoun, proper noun, common noun), animacy, named-entity tag, person, number, quantification (definite, indefinite, quantified), number modifiers mention. Many
common coreference systems (Recasens & Hovy, 2009), model highlights influence
lifespans. available, used gold annotations derive features, since primary
goal shed light relevance features claimed influence lifespans.
454

fiM ODELING L IFESPAN ISCOURSE E NTITIES

morphosyntactic features operationalized using static lists lexicons well
Stanford dependencies output Stanford parser (version 2.0.3; de Marneffe, MacCartney, &
Manning, 2006) gold constituent trees. features extracted following way:
Type type feature captures whether mention pronoun, proper noun, common noun.
value determined gold POS tag mention named-entity tag.
Animacy set animacy values (animate, inanimate, unknown) using static list pronouns, named-entity tags (e.g., PERSON animate whereas LOCATION not), dictionary
bootstrapped Web (Ji & Lin, 2009).
Person Person values (1, 2, 3) assigned pronouns (identified POS tag), using
static list. Mentions pronouns get value 0.
Number number value (singular, plural, unknown) based static list pronouns,
POS tags, Bergsma Lins (2006) static dictionary, named-entity tags. (Mentions marked
named entity considered singular exception organizations,
singular plural get value unknown.)
Quantification discussed Section 2, indefinites definites given referential
semantics pairs naturally discourse anaphora, whereas anaphoric possibilities truly
quantified terms restricted. operationalize quantification decide whether mention
definite, indefinite, quantified, use dependencies find possible determiners, possessors,
numerical quantifiers mention. mention definite named entity,
possessor (e.g., car Johns car definite), determiner definite (the), demonstrative,
possessive. mention quantified numerical quantifier (e.g., two cars)
determiner all, both, neither either. mentions indefinite.
Number modifiers added feature counting many modifiers mention has, seeking capture correlation specificity referentiality. modifiers, counted adjectival,
participial, infinitival, prepositional modifiers well relative clause modifiers, noun compounds, possessives. (Thus, four modifiers phrase modern multifunctional
business center costing 60 million yuan.)
Named entities model also includes named-entity features 18 OntoNotes entitytypes, NER = true non-named-entities. used gold entity-type annotation.
Table 2 summarizes coefficient estimates obtain features. broad terms,
picture one would expect taxonomy given new defined Prince (1981b)
assumed throughout dynamic semantics (Kamp, 1981; Heim, 1982): pronouns depend anaphoric
connections previous mentions disambiguation thus likely coreferent.
corroborated positive coefficient estimate Type = pronoun.
quantified phrases participate discourse anaphora (Partee, 1987; Wang et al., 2006),
accounting association quantifiers singletons (as measured negative
coefficient estimate Quantifier = quantified).
negative coefficient indefinites initially surprising. seen Section 2, theories
stretching back Karttunen (1976) say indefinites excel establishing new discourse entities
frequent participants coreference chains, association
455

fiDE

ARNEFFE , R ECASENS & P OTTS

Feature

Coefficient

Feature

Coefficient

Type = pronoun
Type = proper noun
Animacy = inanimate
Animacy = unknown
Person = 1
Person = 2
Person = 3
Number = singular
Number = unknown
Quantifier = indefinite
Quantifier = quantified
Number modifiers
NER = DATE
NER = EVENT
NER = FACILITY

1.17
1.89
1.36
0.39
1.04
0.13
1.62
0.61
0.17
1.43
1.25
0.39
1.83
2.89
2.94

= GPE
NER = LANGUAGE
NER = LAW
NER = LOCATION
NER = MONEY
NER = NORP
NER =
NER = ORDINAL
NER = ORGANIZATION
NER = PERCENT
NER = PERSON
NER = PRODUCT
NER = QUANTITY
NER = TIME
NER = WORK ART

3.46
2.56
2.85
2.83
0.05
0.82
4.17
0.90
3.39
0.88
2.28
2.64
0.02
1.53
2.42

NER





Table 2: Internal morphosyntactic features lifespan model. indicates non-significant coefficient (p 0.05); sign indicates significant coefficient (p < 0.001).

chains negative. return Section 5.3, argue interactions semantic
operators explain fact.
behavior named-entity (NER) features closely aligned previous models
theoretical discussion above. rule, named entities behave like Type = proper noun associating coreferent mentions. exceptions MONEY, ORDINAL, NORP (for nationalities
religions), PERCENT, QUANTITY, seem intuitively unlikely participate coreference chains. person, number, animacy features together suggest singular animates
excellent coreferent noun phrases.
one real surprise us concerns feature Number modifiers. Inspired observations Fodor Sag (1982) Schwarzschild (2002), expected feature positively
correlate coreferent. reasoning increased modification would likely result
increased specificity, thereby making associated discourse referent identifiable
distinctive. opposite seems hold data. However, hesitate conclude
original hypothesis mistaken. Rather, suspect model insufficiently sensitive
interactions modifier counts lexical semantics modifiers themselves.
5.2 Grammatical Role Mention
Synthesizing much work Centering Theory information structuring, hypothesized
coreferent mentions likely appear core verbal arguments favor sentence-initial (topictracking) positions (Ward & Birner, 2004). capture insights, used grammatical
relation mention given Stanford dependencies gold constituents, sentence
position mention.
456

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Feature

Coefficient

Feature

Coefficient

Sentence Position = end
Sentence Position = first
Sentence Position = last
Sentence Position = middle
coordination

0.22
0.03
0.31
0.11
0.48

Relation = noun argument
Relation =
Relation = root
Relation = subject
Relation = verb argument

0.56
0.67
0.61
0.65
0.32



Table 3: Grammatical role features lifespan model. indicates non-significant coefficient
(p 0.05); sign indicates significant coefficient (p < 0.001).

Sentence position Sentence position determined based raw string: first indicates
mention first word sentence, end last word, begin, middle, last
indicate whether mention situated first, second, last third sentence, respectively.
Relation distinguish among grammatical relations, check whether mention subject, adjunct (which includes prepositional objects, adverbial modifiers, temporal modifiers),
verb argument (which includes direct indirect objects, clausal complements, adjectival complements attributes), noun argument (which includes relative clauses, appositions, possessives, noun compounds, adjectival modifiers).
coordination also indicated whether mention conjunct see whether
inside coordinate phrase affects coreference ways go beyond grammatical role
containing phrase.
coefficient estimates Table 3 support general hypotheses: arguments make good discourse
referents, subjects best all, whereas sentence-final positions disfavor coreference. addition,
note model identifies negative correlation coordination coreference.
5.3 Semantic Environment Mention
Table 4 highlights complex interactions discourse anaphora semantic operators introduced Section 2. interactions focus logical semantics since Karttunen
(1976), whose guiding observation semantic: indefinite interpreted inside scope negation, modal, attitude predicate generally unavailable anaphoric reference outside
scope operator. Heim (1992) also relates anaphoric properties NPs scope-taking
entailments attitude predications.
direct access semantic scope, expect syntactic scope correlate
strongly semantic scope. therefore used dependency representations define features
capturing syntactic scope negation, modal auxiliaries, broad range attitude predicates
(181 verbs 374 nouns Saur, 2008). Technically, given mention, produce
negation, modal attitude verb feature according presence pre-defined negation
modality markers (such not, can, may) attitude predicates (e.g., accuse, allege, doubt, say)
dependency path. example, NP relief given negation feature
financial storm shows sign relief today, since scope sign. Similarly,
mention scientific technological companies scope modal auxiliary would
457

fiDE

ARNEFFE , R ECASENS & P OTTS

Feature

Coefficient

Presence negation
Presence modality
attitude verb
AttitudeVerb * (Type = pronoun)
AttitudeVerb * (Type = proper noun)
AttitudeVerb * (Quantifier = indefinite)
AttitudeVerb * (Quantifier = quantified)
Modal * (Type = pronoun)
Modal * (Type = proper noun)
Modal * (Quantifier = indefinite)
Modal * (Quantifier = quantified)
Negation * (Type = pronoun)
Negation * (Type = proper noun)
Negation * (Quantifier = indefinite)
Negation * (Quantifier = quantified)
Negation * (Number modifiers)

0.18
0.22
0.10
0.41
0.10
0.19
0.10
0.13
0.35
0.00
0.17
1.07
0.30
0.36
0.39
0.11








Table 4: Semantic environment features interactions lifespan model. indicates nonsignificant coefficient (p 0.05); sign indicates significant coefficient (p < 0.001);
indicates significance p < 0.05.

attitude verb said firms Taiwan said would establish scientific technological
companies zone, receives modal attitude verb features.
Table 4 summarizes models semantic environment features interactions. interaction terms added model follow previous linguistic literature: expect scope
semantic operators (negation, modality attitude predicate) interact internal syntax mention, specifically type definiteness/quantification. results
beautifully aligned guiding linguistic hypotheses. First, negation modality
negatively correlate coreference, expected given constraints impose lifespans.
Interacting semantic features internal syntax mentions also yields
expected results: since proper names pronouns scope-taking, largely unaffected
environment features, whereas indefinites, affected scope, emerge even
restricted, Karttunen others would predict.
coefficient values attitude predicates interactions seem anomalous light
semantics items. Section 2, noted non-factive attitude predicates like say
cannot offer semantic guarantees mentions scope survive outside scope.
might lead one think biased long-lived mentions, fact see
opposite. However, also observed pragmatic factors often facilitate exceptional anaphoric
dependencies attitude predications. Karttunen (1973) referred leakiness
predicates information introduced scope seems often percolate text level
wide range contexts (Rooryck, 2001; Simons, 2007; Harris & Potts, 2009). Since lifespan
458

fiM ODELING L IFESPAN ISCOURSE E NTITIES

# F EATURES
L INGUISTIC
URFACE
C OMBINED
C ONFIDENT

123
73,393
73,516
73,516

INGLETON
Recall Precision F1
80.2
80.2
81.1
56.0

77.5
79.9
80.8
89.8

78.8
80.0
80.9
69.0

C OREFERENT
Recall Precision F1
71.4
75.3
76.4
48.2

74.6
75.6
76.6
90.7

73.0
75.4
76.5
62.9

ACCURACY
76.3
78.0
79.0
52.5

Table 5: Recall, precision, F1 accuracy three different sets features OntoNotes
development set. C ONFIDENT C OMBINED model singleton predicted
Pr < 0.2 coreferent Pr > 0.8.

model trained real usage data, surprising reflects pragmatic factors rather
lexical semantics (de Marneffe et al., 2012).
noted earlier, features Table 4 standardly used coreference systems. Uryupina
(2009) notes Karttunen features implemented (see Section 3) significantly improve performance discourse-new mention non-antecedent detectors. Contrary
Uryupina, adding features Table 4 model incorporates features described
Table 2 Table 3 results significantly better model (likelihood ratio test, p < 0.001).
accuracy CoNLL-2012 development set also improves adding Karttunen features
(McNemars test, p < 0.001).
5.4 Results
highlighted above, lifespan model built OntoNotes data confirms claims
Karttunen others concerning semantic operators interact specific kinds mention.
novel quantitative evidence theories. model also successfully learns tease
singleton coreferent mentions apart, suggesting practical value NLP applications.
first row Table 5 summarizes linguistic model performance development set
OntoNotes data described Section 4, giving precision, recall, F1 measures singleton
coreferent mentions. accuracy model 76.3%. majority baseline, predicting
mentions singletons, leads accuracy 55.1%.

6. Extension Bridging
lifespan model suggests new perspective bridging anaphora, discussed briefly
Section 3.2 using example (14), repeated here:
(15)

looked room. ceiling high.

anchor phrase room superficially singleton discourse, intuitive lifespan
longer: makes salient discourse referent ceiling room, ceiling
second sentence refers to. bridging relationship keeps room alive discourse
referent, extending lifespan, though way read directly text. Together
basic tenets lifespan model, observations suggest testable hypothesis
459

fiDE

ARNEFFE , R ECASENS & P OTTS

bridging: even bridging anchors superficially singleton (henceforth, singleton anchors),1
lifespan model tend classify coreferent, since model designed
detect later mentions per se, rather capture abstract information roles
entities play discourse.
OntoNotes contain annotations bridging anaphora, evaluating hypothesis
straightforward. However, Hou, Markert, Strube (2013) annotated 50 WSJ texts
OntoNotes bridging information, yielding annotations 663 bridging anchors. these, 145
singleton anchors sense identify (Section 4) thus used assess
models ability detect abstract sense bridging anchors long-lived.
Ideally, would simply run trained lifespan model examples. proves ineffective, though, (outside Hou et al.s data) OntoNotes annotations treat singleton
anchors singleton, meaning trained lifespan model optimized data obscure
distinction interest. Nonetheless, expect feature representations form backbone
lifespan model able distinguish true singletons singleton anchors given right
kind training data. small number relevant bridging annotations poses obstacles
pursuing idea, sought navigate around follows: using annotated corpus
Hou et al., extract 145 singleton anchors sample additional 145 true
singletons documents (from total 5,804 cases). yields data set
confident makes relevant distinction. randomly divide data set 80%
training data 20% testing data, conduct standard classifier evaluation. use logistic
regression classifier, employing recursive feature elimination cross-validation (Guyon, Weston,
& Barnhill, 2002), implemented Pedregosa et al. (2011), try find compact model
effective small data set. model used `2 regularizer penalty 0.5, though
`1 regularization changes penalty delivered essentially results,
without recursive feature elimination step.
train test sets small, performance varies greatly depending nature
true singleton sample, repeat experiment 1,000 times average results.
procedure, lifespan feature representations achieve mean F1 65% (standard error 0.002;
mean precision 62%, mean recall 0.69%), indicating lifespan-based features sensitive
distinction singleton anchors true singletons. finding bolsters design
lifespan feature representations also shows lifespan deeper abstract
merely counting referents. Given right kind annotations, believe model could
extended provide even fuller treatment bridging, governed partly mix
linguistic contextual factors (Hawkins, 1978; Prince, 1981b; Schwarz, 2009).

7. Predicting Lifespans Surface Features
Durrett Klein (2013) Hall et al. (2014) showed that, tasks coreference resolution
parsing, large quantity surface-level information implicitly model linguistic features, also capture patterns data easily identified manually.
Given large amount annotated data available OntoNotes corpus, might expect
sufficient amount surface-level data capture linguistic insights hand-engineered
1. bridging anchors also literal coreferent mentions, looked room. empty,
ceiling high., room coreferent addition providing discourse support ceiling.
set aside cases bridging experiments.

460

fiM ODELING L IFESPAN ISCOURSE E NTITIES

lifespan model defined above. therefore tested model using POS tags n-grams
fares lifespan task.
used following features surface model: lemmas words mention,
POS tags words mention, POS tag head mention, lemma
POS tags two words preceding following mention (with dummy BEGIN END
words mark beginning end sentences). suggested Durrett Klein (2013),
features might capture information encoded NER tag, number, person, sentence position.
surface models performance reported second row Table 5. models
Table 5, `2 regularization penalty chosen via five-fold cross-validation training data.
linguistic model, using tuned `2 regularization penalty rather default one makes
almost difference, substantially improves performance models features.
additionally experimented different algorithms feature selection, found
results invariably best, models, retained full sets features. last
row table gives performance model combine linguistic
surface features evaluate whether surface features alone cover information captured
linguistic features, whether linguistic features additional predictive value.
surface model performs better linguistic-only model, especially coreferent
category. However, small number linguistically-motivated features yields results
range obtained large number features surface model, might
importance tasks small amount annotated data available,
bridging experiment Section 6. (The obvious trade-off surface features easier
specify implement.) shown C OMBINED row Table 5, combined surface
feature set, linguistically-motivated features give statistically significant boost performance.
suggests surface features miss certain long-distance interactions discourse
anaphora semantic operators interactions linguistic features explicitly encode.
best model predicting lifespan combined one. Instead using standard 0.5
threshold decision boundary, also make use full distribution returned logistic regression model rely confident decisions. resulting C ONFIDENT model
C OMBINED one predicts singleton Pr < 0.2 coreferent Pr > 0.8. threshold values
reported best trade-off found precision score close 0.90 without losing
much recall. expected, using highly confident model, increase precision,
though cost recall. kind model preferred depend application; noted
Ng (2004) Uryupina (2009), incorporating lifespan model downstream NLP
applications, often want highly accurate predictions, favors model like C ONFIDENT.

8. Application Coreference Resolution
assess value lifespan model NLP applications, incorporate best
feature combination two state-of-the art coreference resolution systems: Stanford system
(Lee et al., 2011) Berkeley system (Durrett & Klein, 2013). cases, original
model serves baseline, focus extent lifespan model contributes
improvements baseline. allows us quantify power effectiveness
lifespan model two different systems rule-based one (Stanford) learning-based
one (Berkeley).
461

fiDE

ARNEFFE , R ECASENS & P OTTS

8.1 Evaluation Measures
evaluate incorporation lifespan model coreference systems, use English
development test sets CoNLL-2011 CoNLL-2012 Shared Tasks. Although
CoNLL shared tasks evaluated systems multi-mention (i.e., non-singleton) entities,
still expect lifespan model help: stopping singletons linked multi-mention
entities, expect see increase precision. evaluation uses measures given
CoNLL scorer:
MUC (Vilain, Burger, Aberdeen, Connolly, & Hirschman, 1995): Link-based metric
measures many links gold system partitions common.
B3 (Bagga & Baldwin, 1998): Mention-based metric measures proportion mention
overlap gold predicted entities.
CEAF-3 (Luo, 2005): Mention-based metric that, unlike B3 , enforces one-to-one alignment gold predicted entities.
CEAF-4 (Luo, 2005): entity-based version metric.
CoNLL (Denis & Baldridge, 2009; Pradhan, Ramshaw, Marcus, Palmer, Weischedel, & Xue,
2011): Average MUC, B3 CEAF-4 .
BLANC (Recasens & Hovy, 2011): Link-based metric takes mean coreference
non-coreference links, thereby rewarding (but over-rewarding) singletons.
use new CoNLL coreference scorer (Pradhan, Luo, Recasens, Hovy, Ng, & Strube, 2014,
version 8.0), fixes bug previous versions concerning way gold predicted mentions
aligned evaluating automatically predicted mentions. new scorer modify
either gold system output, implements measures originally proposed, extends
BLANC successfully handle predicted mentions, following Luo, Pradhan, Recasens, Hovy
(2014).
8.2 Incorporating Lifespan Model Stanford Coreference System
Stanford system highest-scoring system CoNLL-2011 Shared Task (Pradhan
et al., 2011), also part highest-scoring system (Fernandes, dos Santos, & Milidiu,
2012) CoNLL-2012 Shared Task (Pradhan et al., 2012). rule-based system includes
total ten rules (or sieves) entity coreference, exact string match pronominal
resolution. sieves applied highest lowest precision, rule adding coreference
links. coreference resolution sieve, documents mentions traversed left right.
prune search space, mention already linked another one previous sieve,
mention first textual order considered subsequent sieves. Furthermore,
mentions headed indefinite pronoun (e.g., some, other) start indefinite
determiner (a, an) discarded antecedent exact string.
mention compared previous mentions text coreferent antecedent found
(according current sieve) beginning text reached. Candidates sorted using
left-to-right breadth-first traversal parse tree, favors subjects syntactic salience
general.
lifespan model improve coreference resolution two different ways: (i) mentions classified singletons considered either antecedents coreferent, (ii) mentions
462

fiM ODELING L IFESPAN ISCOURSE E NTITIES

classified coreferent linked mention(s). successfully predicting singletons (i), enhance systems precision; successfully predicting coreferent mentions (ii),
improve systems recall. focus (i) use lifespan model detecting
singletons. decision motivated two factors. First, given large number singletons
(Figure 1), likely see gain performance discarding singletons. Second,
multi-sieve nature Stanford coreference system make straightforward decide
antecedent mention linked even know coreferent.
integrate singleton model Stanford coreference system, depart previous
work letting sieve consider whether pair mentions coreferent mentions
classified singletons C ONFIDENT model mentions named entity.
this, discard 29% NPs consideration. Experiments development set yielded
higher performance taking account named entities. Performance higher
C ONFIDENT model TANDARD model.
therefore use lifespan model help coreference resolution pre-filtering step
coreference resolution, discarding mentions tagged singletons lifespan model. Previous
work incorporating non-referentiality discourse-new detection module pre-processing
step coreference resolution shown mixed results, discussed Section 3. general
arguments pipeline vs. joint approaches apply here: pipeline approaches prevent recovering
errors earlier pipeline, joint approaches tend increase model complexity associated
optimization challenges, easily allow separating different modules, makes
feature design error analysis difficult well. case, context Stanford
systems sieve-architecture, natural add lifespan model pre-filtering step.
8.2.1 R ESULTS
Table 6 summarizes performance Stanford system CoNLL-2011 CoNLL-2012
development test sets. evaluate incorporation lifespan model realistic setting,
use automatic parses, POS NER tags provided CoNLL documents.
scores automatically predicted mentions. baseline Stanford coreference system,
w/ Lifespan system extended lifespan model discard singletons, explained
above. Stars indicate statistically significant difference (Wilcoxon signed-rank test, p < 0.05)
according jackknifing (10 partitions development set test set, balanced
different domains2 corpus). expected, lifespan model significantly increases precision
(up +4.0 points) decreases recall (by 0.7 points). Overall, however, gain precision
higher loss recall, obtain significant improvement 0.41.5 points F1
score evaluation measures.
8.2.2 E RROR NALYSIS
Kummerfeld Klein (2013) provide useful tool automatically analyzing categorizing
errors made coreference resolution systems. tool identifies seven intuitive error types: span
error, conflated entities (entity mentions corefer clustered together), extra entity
(entities gold data added), extra mention (the system incorrectly introduces
2. mentioned Section 4, OntoNotes corpus contains documents seven different domains coreference
performance shown vary highly depending domain (Pradhan et al., 2012).

463

fiDE

CoNLL
F1

Stanford
2011 DEV SET
Baseline
w/ Lifespan
Discourse-new

R

ARNEFFE , R ECASENS & P OTTS

MUC
P

F1

R

B3
P

F1

R

CEAF-4
P
F1

51.49
52.23*
51.52

58.00* 55.97 56.97
57.57 57.72* 57.65*
56.30 58.98* 57.61*

48.01* 49.81 48.89
47.45 51.62* 49.45*
45.51 52.33* 48.68

54.27* 44.03 48.62
53.46 46.27* 49.60*
48.63 47.93* 48.28

2011 TEST SET
Baseline
50.55
w/ Lifespan
51.58*
Discourse-new 51.26*

60.09* 56.09 58.02
59.75 58.32* 59.03*
58.92 59.71* 59.31*

47.57* 47.91 47.74
47.06 50.18* 48.57*
45.72 51.06* 48.25*

52.28* 40.90 45.89
51.42 43.50* 47.13*
47.41 45.1* 46.22

2012 DEV SET
Baseline
w/ Lifespan
Discourse-new

55.26
55.77*
53.63

61.36* 65.26 63.25
60.99 66.70* 63.72*
60.71 63.27 61.96

48.35* 57.05 52.34
47.87 58.57* 52.68*
47.25 54.42 50.58

53.86* 47.01 50.20
53.10 48.91* 50.92*
49.35 47.41* 48.36

2012 TEST SET
Baseline
53.31
w/ Lifespan
54.58*
Discourse-new 53.01

62.05* 61.35 61.70
61.31 65.61* 63.39*
61.22 62.73* 61.97

48.00* 52.66 50.22
46.91 57.05* 51.49*
46.72 53.62* 49.93

52.29* 44.36 48.00
51.03 46.87* 48.86*
48.38 45.92* 47.12

(a)

Stanford

R

CEAF-3
P
F1

R

BLANC
P
F1

2011 DEV SET
Baseline
w/ Lifespan
Discourse-new

57.11* 52.50 54.71
56.55 54.43* 55.47*
54.02 55.67* 54.83

45.04* 46.84 45.14
44.37 48.65* 45.85*
42.59 49.57* 45.60

2011 TEST SET
Baseline
w/ Lifespan
Discourse-new

55.57* 49.56 52.39
55.04 51.80* 53.37*
53.2
53.08* 53.14*

46.46* 47.51 46.12
45.98 49.53* 47.06*
44.87 50.82* 47.33*

2012 DEV SET
Baseline
w/ Lifespan
Discourse-new

56.59* 57.22 56.90
56.11 58.75* 57.40*
55.00 56.18 55.58

48.78* 56.47 51.94
48.23 57.94* 52.36*
48.11 54.12 50.73

2012 TEST SET
Baseline
w/ Lifespan
Discourse-new

56.12* 53.46 54.76
54.98 56.69* 55.82*
54.43 54.78* 54.60

49.08* 54.48 50.88
47.69 59.15* 52.28*
47.95 55.81* 51.14*

(b)

Table 6: Performance Stanford system CoNLL-2011 CoNLL-2012 development
test sets. Scores (v8.0 CoNLL scorer) automatically predicted mentions,
using CoNLL automatic annotations. Stars w/ Lifespan Discourse-new
rows indicate significant difference baseline (Wilcoxon signed-rank test, p <
0.05).
464

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Error

they1

Gold
scientists1
they1
family2
they2

Extra entity

various major Hong Kong media
media




Extra mention

book
book



book


Conflated entities

System
scientists1
they1

(a) Errors affecting precision.

Error

System
scientists1
they1
family2
they1

Gold
scientists1
they1
family2
they2

Missing entity




network


Missing mention

two mothers



two mothers

two mothers lost loved ones

Divided entity

(b) Errors affecting recall.

Table 7: Illustration error types provided Kummerfeld Kleins (2013) system: errors
made Stanford coreference system CoNLL-2012 development set.

mention coreferent cluster),3 divided entity (an entity split two different
clusters),4 missing entity (the system fails detect entity), missing mention (an entity
missing one mentions). Table 7 illustrates error types interested in,5 showing errors
made Stanford system, separated affecting precision affecting recall.
ran Kummerfeld Kleins (2013) system Stanford output quantify improvement obtained incorporating lifespan model coreference system CoNLL-2012
development set. Figure 2 shows difference errors original Stanford coreference
system system lifespan model integrated. lifespan model generally
reduces errors affecting precision, notably getting rid spurious entities (Extra
entity). top three errors Table 7 precision-related fixed integrating lifespan model Stanford system. hand, bottom two errors recall-related
3. distinction two categories conflated entities extra mention makes sense corpus like
OntoNotes singletons annotated: former occurs system clusters one mentions
multi-mention entity incorrect entity, whereas latter occurs system incorrectly clusters
others mention truly part singleton entity (and annotated gold).
4. conflated-entities error divided-entity error often co-occur.
5. span error category relevant comparison here: systems (with without lifespan) work
predicted mentions.

465

fiDE

ARNEFFE , R ECASENS & P OTTS

Stanford alone

Conflated entities

Extra entity

Extra mention

lifespan

897

Stanford alone

728

lifespan

Stanford alone

535

lifespan

523

Stanford alone

Divided entities

Missing entity

Missing mention

1635
1607

lifespan

2038
2021

830

Stanford alone

877

lifespan

Stanford alone

1154

lifespan

1158

Figure 2: Number errors Stanford coreference system (with without lifespan
model) CoNLL-2012 development set.

introduced lifespan model. However, cumulative gain error reduction across error
categories results significant improvement overall coreference performance.
8.2.3 U SING L IFESPAN ODEL ISCOURSE -N EW ENTION C LASSIFIER
discussed Section 3.3, previous work (Ng & Cardie, 2002; Uryupina, 2009) reports
loss coreference resolution performance pre-filtering discourse-new mentions, i.e., singleton mentions well mentions start coreference chain. mimic pre-filtering,
incorporate lifespan model Stanford system following way: mentions
model classify singletons considered every sieve hypothesized corefer
previous mention, discourse-new mentions removed consideration.
so, also see performance loss, shown Discourse-new rows Table 6. clear significant gains across measures, compared performance
standard Stanford system (Baseline rows). improvements see Table 6 result
pre-filtering pairs mentions lifespan model classifies singletons. stricter
constraint seems balance loss pre-filtering many mentions early stage.
8.3 Incorporating Lifespan Model Berkeley Coreference System
Berkeley coreference system (Durrett & Klein, 2013; Durrett, Hall, & Klein, 2013) currently
highest scoring coreference system publicly available. uses mention-synchronous
framework: mention, system either chooses one antecedent decides mention
starts new cluster (perhaps leading singleton cluster). log-linear model features
extracted mentions decide whether mentions anaphoric, features
extracted pairs mentions decide whether pairs corefer. baseline compare
466

fiM ODELING L IFESPAN ISCOURSE E NTITIES

takes best feature set, FINAL one, reported Durrett Klein (2013),
combines large number lexicalized surface features well semantic features.
incorporate lifespan model Berkeley system, use probabilities
mentions given lifespan model. pair mentions, add lifespan features
adding lifespan probability mention. also add singleton feature mentions
lifespan probability 0.2, coreferent feature mentions lifespan
probability 0.8. Unlike Stanford architecture, exploiting coreferent predictions
straightforward (Section 8.2), learning-based setup Berkeley system allows us
make use lifespan probabilities without focusing singleton-class prediction.
Instead incorporating lifespan probabilities lifespan model, also tried adding
Berkeley system features lifespan model already present Berkeley
system (i.e., features Table 3 Table 4). However, lead significant
improvements CoNLL 2012 development data, CoNLL 2012 test data.
Moreover, overall results less good incorporating probabilities manner
described above.
8.3.1 R ESULTS
Table 8 shows results Berkeley system CoNLL 2011 2012 development
test sets. Stanford system, scores automatically predicted mentions.
use automatic POS tags, parse trees, NER annotations provided CoNLL data
training testing. restrict training training data only.6 baseline FINAL
Berkeley coreference system, w/ Lifespan system extended lifespan,
singleton coreferent features, explained above. Significance computed way
Stanford system (we created 10 partitions development set test set, balanced
different domains corpus).
learning-based context Berkeley system, lifespan model increases precision
well recall, leading final improvement CoNLL score 1.0 2.0 points. Since
use lifespan model predicting singleton coreferent mentions, manage improve precision recall. provides additional empirical support splitting coreference
resolution entity-lifespan task predicts mentions refer long-lived entities
discourse coreference task focuses establishing coreference links
mentions.
8.3.2 E RROR NALYSIS
Parallel analysis Stanford coreference system output, ran Kummerfeld Kleins
(2013) system Berkeley output. Figure 3 shows difference errors original Berkeley coreference system (FINAL feature set) system enhanced lifespan
model. enhanced system commits fewer errors affecting precision (upper part Figure 3),
6. also tried training gold POS tags, parse trees, NER annotations provided CoNLL data,
using automatic annotations test time. make difference original Berkeley system.
incorporating linguistic features (either lifespan probabilities features lifespan model
already Berkeley system), setting lead significant improvements baseline. However,
improvements hold consistently across development test sets: compared results obtained
training automatic annotations, training gold improves performance linguistically informed systems
test set.

467

fiDE

Berkeley

CoNLL
F1

R

ARNEFFE , R ECASENS & P OTTS

MUC
P

F1

R

B3
P

F1

R

CEAF-4
P
F1

2011 DEV SET
Baseline
59.72
w/ Lifespan 61.03*

62.67 70.22 66.23
64.78* 72.24* 68.30*

52.19 62.54 56.90
54.65* 63.28* 58.65*

53.77* 58.43 56.00
52.89 59.83* 56.15

2011 TEST SET
Baseline
59.06
w/ Lifespan 59.65*

64.14 71.68 67.70
64.96* 73.29* 68.87*

50.81 61.31 55.56
51.78* 62.38* 56.59*

51.66* 56.34 53.90
49.89 57.62* 53.48

2012 DEV SET
Baseline
61.49
w/ Lifespan 63.42*

69.06 71.32 70.17
70.76* 74.30* 72.49*

57.10 60.55 58.78
59.35* 62.79* 61.02*

55.20* 55.80 55.50
54.74 58.94* 56.76*

2012 TEST SET
Baseline
61.06
w/ Lifespan 62.15*

69.17 71.96 70.54
70.42* 74.07* 72.20*

55.77 60.50 58.04
56.87* 62.21* 59.42*

53.82* 55.37 54.58
52.64 57.20* 54.83

(a)

Berkeley

R

CEAF-3
P
F1

R

BLANC
P
F1

2011 DEV SET
Baseline
w/ Lifespan

58.82 65.37 61.92
59.29* 66.36* 62.63*

50.38 59.93 54.73
52.83* 62.92* 57.37*

2011 TEST SET
Baseline
w/ Lifespan

56.71
56.37

63.01 59.70
63.96* 59.93

49.11 59.67 53.88
50.66* 61.87* 55.68*

2012 DEV SET
Baseline
w/ Lifespan

62.29
62.65

64.01 63.14
66.18* 64.37*

60.32 60.79 60.53
62.19* 63.80* 62.86*

2012 TEST SET
Baseline
w/ Lifespan

60.83 63.12 61.95
61.05* 64.68* 62.81*

57.70 61.79 59.68
58.92* 63.93* 61.32*

(b)

Table 8: Performance Berkeley system CoNLL 2011 CoNLL 2012 development
test sets. Scores (v8.0 CoNLL scorer) automatically predicted mentions,
using CoNLL automatic annotations. Stars indicate significant difference (Wilcoxon
signed-rank test, p < 0.05).

significantly category. However, cumulative gains result significant
improvement overall precision. Globally, lifespan model fixes errors brings in.
468

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Berkeley alone

Conflated entities

Extra entity

Extra mention

lifespan

Berkeley alone
lifespan

579
533

594

Berkeley alone
lifespan

508

Berkeley alone

Divided entities

Missing entity

Missing mention

1448
1412

lifespan

Berkeley alone

818

lifespan

820

Berkeley alone
lifespan

1669
1572

829
941

Figure 3: Number errors Berkeley coreference system (with without lifespan
model) CoNLL 2012 development set.

9. Conclusion
factors determine fate given discourse referent? nature (its internal morphosyntax) nurture (the broader syntactic semantic environments mentions)? lifespan
model (Section 5) suggests nature, nurture, interactions important. model
validates existing linguistic generalizations discourse anaphora (Section 2), provides new
insights previous engineering efforts similar direction (Section 3). also show
linguistically-motivated features bring improvement top surface features (Section 7), demonstrating automatic language processing rely machine learning big data.
lifespan model performs well right, achieving 79% accuracy predicting whether
given mention singleton coreferent. alone could ramifications tracking topics,
identifying protagonists, discourse coherence. paper, demonstrated benefits
lifespan model coreference resolution. incorporated lifespan model two
different coreference resolution systems showed yields improvements practical
statistical significance cases (Section 8).
Stepping back, hope provided compelling illustration efforts theoretical
linguistics NLP complement other, developing models assessing
scientific engineering contexts.

Acknowledgments
thank Jefferson Barlew, Greg Durrett, Micha Elsner, Gregory Kierstead, Craige Roberts, Michael
White, Stanford NLP Group, anonymous reviewers helpful suggestions earlier drafts paper. research supported part ONR grant No. N00014-10-1-0109
ARO grant No. W911NF-07-1-0216.
469

fiDE

ARNEFFE , R ECASENS & P OTTS

References
Aissen, J. (1997). syntax obviation. Language, 73(4), 705750.
Aissen, J. (2003). Differential object marking: Iconicity vs. economy. Natural Language
Linguistic Theory, 21(3), 435483.
Aloni, M. (2000). Quantification Conceptual Covers. Ph.D. thesis, University Amsterdam.
AnderBois, S., Brasoveanu, A., & Henderson, R. (2010). Crossing appositive/at-issue meaning
boundary. Li, N., & Lutz, D. (Eds.), Proceedings Semantics Linguistic Theory 20,
pp. 328346. CLC Publications.
Bagga, A., & Baldwin, B. (1998). Algorithms scoring coreference chains. Proceedings
LREC 1998 Workshop Linguistic Coreference, pp. 563566.
Baker, C. L. (1970). Double negatives. Linguistic Inquiry, 1(2), 169186.
Barzilay, R., & Lapata, M. (2008). Modeling local coherence: entity-based approach. Computational Linguistics, 34(1), 134.
Bean, D. L., & Riloff, E. (1999). Corpus-based identification non-anaphoric noun phrases.
Proceedings 37th Annual Meeting Association Computational Linguistics,
pp. 373380. ACL.
Beaver, D. (2004). optimization discourse anaphora. Linguistics Philosophy, 27(1),
356.
Beaver, D. I. (2007). Corpus pragmatics: Something old, something new. Paper presented
annual meeting Texas Linguistic Society.
Bergsma, S., & Lin, D. (2006). Bootstrapping path-based pronoun resolution. Proceedings
21st International Conference Computational Linguistics 44th Annual Meeting
Association Computational Linguistics, pp. 3340. ACL.
Bergsma, S., Lin, D., & Goebel, R. (2008). Distributional identification non-referential pronouns.
Proceedings 46th Annual Meeting Association Computational Linguistics:
Human Language Technologies, pp. 1018. ACL.
Bergsma, S., & Yarowsky, D. (2011). NADA: robust system non-referential pronoun detection. Hendrickx, I., Lalitha Devi, S., Branco, A., & Mitkov, R. (Eds.), Anaphora Processing
Applications, Vol. 7099 Lecture Notes Computer Science, pp. 1223. Springer.
Bittner, M. (2001). Surface composition bridging. Journal Semantics, 18(2), 127177.
Brants, T., & Franz, A. (2006). Google Web 1T 5gram corpus version 1.1. LDC2006T13.
Byron, D. K., & Gegg-Harrison, W. (2004). Eliminating non-referring noun phrases coreference resolution. Proceedings Discourse Anaphora Reference Resolution Conference, pp. 2126.
Chafe, W. L. (1976). Givenness, contrastiveness, definiteness, subjects, topics, point view.
Li, C. N. (Ed.), Subject Topic, pp. 2555. Academic Press.
Clark, H. H. (1975). Bridging. Schank, R. C., & Nash-Webber, B. L. (Eds.), Theoretical Issues
Natural Language Processing, pp. 169174. ACM.
470

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Cresswell, M. J. (2002). Static semantics dynamic discourse. Linguistics Philosophy, 25(5
6), 545571.
de Marneffe, M.-C., MacCartney, B., & Manning, C. D. (2006). Generating typed dependency
parses phrase structure parses. Proceedings Fifth International Conference
Language Resources Evaluation, pp. 449454. ACL.
de Marneffe, M.-C., Manning, C. D., & Potts, C. (2012). happen? pragmatic complexity
veridicality assessment. Computational Linguistics, 38(2), 301333.
Delmonte, R., Bristot, A., Piccolino Boniforti, M. A., & Tonelli, S. (2007). Entailment anaphora
resolution RTE3. Proceedings ACL-PASCAL Workshop Textual Entailment
Paraphrasing, pp. 4853.
Denis, P., & Baldridge, J. (2007). Joint determination anaphoricity coreference resolution
using integer programming. Human Language Technologies 2007: Conference
North American Chapter Association Computational Linguistics; Proceedings
Main Conference, pp. 236243. ACL.
Denis, P., & Baldridge, J. (2009). Global joint models coreference resolution named entity
classification. Procesamiento del Lenguaje Natural, 42, 8796.
Durrett, G., Hall, D., & Klein, D. (2013). Decentralized entity-level modeling coreference
resolution. Proceedings 51st Annual Meeting Association Computational
Linguistics (Volume 1: Long Papers), pp. 114124. ACL.
Durrett, G., & Klein, D. (2013). Easy victories uphill battles coreference resolution.
Proceedings 2013 Conference Empirical Methods Natural Language Processing,
pp. 19711982. ACL.
Elbourne, P. (2008). Demonstratives individual concepts. Linguistics Philosophy, 31(4),
409466.
Evans, R. (2001). Applying machine learning toward automatic classification it. Literary
Linguistic Computing, 16(1), 4557.
Fernandes, E., dos Santos, C., & Milidiu, R. (2012). Latent structure perceptron feature induction unrestricted coreference resolution. Joint Conference EMNLP CoNLL
- Shared Task, pp. 4148. ACL.
Fodor, J. D., & Sag, I. A. (1982). Referential quantificational indefinites. Linguistics
Philosophy, 5(3), 355398.
Fraurud, K. (1990). Definiteness processing noun phrases natural discourse. Journal
Semantics, 7(4), 395433.
Giampiccolo, D., Magnini, B., Dagan, I., & Dolan, B. (2007). third PASCAL recognizing
textual entailment challenge. Proceedings ACL-PASCAL Workshop Textual Entailment Paraphrasing, pp. 19.
Groenendijk, J., & Stokhof, M. (1991). Dynamic predicate logic. Linguistics Philosophy, 14(1),
39100.
Grosz, B. J., Joshi, A. K., & Weinstein, S. (1995). Centering: framework modeling local
coherence discourse. Computational Linguistics, 21(2), 203225.
471

fiDE

ARNEFFE , R ECASENS & P OTTS

Guyon, I., Weston, J., & Barnhill, S. (2002). Gene selection cancer classification using support
vector machines. Machine Learning, 46(13), 389422.
Hall, D., Durrett, G., & Klein, D. (2014). Less grammar, features. Proceedings 52nd
Annual Meeting Association Computational Linguistics (Volume 1: Long Papers),
pp. 228237. ACL.
Harris, J. A., & Potts, C. (2009). Perspective-shifting appositives expressives. Linguistics
Philosophy, 32(6), 523552.
Hawkins, J. A. (1978). Definiteness Indefiniteness. Croom Helm.
Heim, I. (1982). Semantics Definite Indefinite Noun Phrases. Ph.D. thesis, UMass
Amherst.
Heim, I. (1992). Presupposition projection semantics attitude verbs. Journal Semantics,
9(2), 183221.
Hobbs, J. R. (1979). Coherence coreference. Cognitive Science, 3(1), 6790.
Hou, Y., Markert, K., & Strube, M. (2013). Global inference bridging anaphora resolution.
Proceedings 2013 Conference North American Chapter Association
Computational Linguistics: Human Language Technologies, pp. 907917. ACL.
Israel, M. (1996). Polarity sensitivity lexical semantics. Linguistics Philosophy, 19(6),
619666.
Israel, M. (2001). Minimizers, maximizers, rhetoric scalar reasoning. Journal Semantics, 18(4), 297331.
Israel, M. (2004). pragmatics polarity. Horn, L., & Ward, G. (Eds.), Handbook
Pragmatics, pp. 701723. Blackwell.
Ji, H., & Lin, D. (2009). Gender animacy knowledge discovery web-scale n-grams
unsupervised person mention detection. Proceedings 23rd Pacific Asia Conference
Language, Information Computation, pp. 220229.
Kamp, H. (1981). theory truth discourse representation. Groenendijk, J., Janssen,
T. M. V., & Stockhof, M. (Eds.), Formal Methods Study Language, pp. 277322.
Mathematical Centre.
Karttunen, L. (1973). Presuppositions compound sentences. Linguistic Inquiry, 4(2), 169193.
Karttunen, L. (1976). Discourse referents. McCawley, J. D. (Ed.), Syntax Semantics, Vol. 7:
Notes Linguistic Underground, pp. 363385. Academic Press.
Kehler, A. (2002). Coherence, Reference, Theory Grammar. CSLI.
Kummerfeld, J. K., & Klein, D. (2013). Error-driven analysis challenges coreference resolution. Proceedings 2013 Conference Empirical Methods Natural Language
Processing, pp. 265277. ACL.
Ladusaw, W. A. (1996). Negation polarity items. Lappin, S. (Ed.), Handbook Contemporary Semantic Theory, pp. 321341. Blackwell.
472

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Lee, H., Peirsman, Y., Chang, A., Chambers, N., Surdeanu, M., & Jurafsky, D. (2011). Stanfords
multi-pass sieve coreference resolution system CoNLL-2011 shared task. Proceedings 15th Conference Computational Natural Language Learning: Shared Task, pp.
2834. ACL.
Luo, X. (2005). coreference resolution performance metrics. Proceedings Human Language Technology Conference Conference Empirical Methods Natural Language
Processing, pp. 2532. ACL.
Luo, X., Pradhan, S., Recasens, M., & Hovy, E. (2014). extension BLANC system mentions. Proceedings 52nd Annual Meeting Association Computational
Linguistics, pp. 2429. ACL.
Muller, C. (2006). Automatic detection nonreferential spoken multi-party dialog. Proceedings European Chapter Association Computational Linguistics, pp. 4956.
ACL.
Muskens, R., van Benthem, J., & Visser (1997). Dynamics. van Benthem, J., & ter Meulen, A.
(Eds.), Handbook Logic Language, pp. 587648. Elsevier.
Ng, V. (2004). Learning noun phrase anaphoricity improve coreference resolution: Issues
representation optimization. Proceedings 42nd Annual Meeting Association
Computational Linguistics, pp. 152159. ACL.
Ng, V., & Cardie, C. (2002). Identifying anaphoric non-anaphoric noun phrases improve
coreference resolution. Proceedings 19th International Conference Computational Linguistics, pp. 17. ACL.
Nissim, M. (2006). Learning information status discourse entities. Proceedings 2006
Conference Empirical Methods Natural Language Processing, pp. 94102.
Paice, C. D., & Husk, G. D. (1987). Towards automatic recognition anaphoric features
English text: impersonal pronoun it. Computer Speech & Language, 2(2), 109132.
Partee, B. H. (1987). Noun phrase interpretation type-shifting principles. Groenendijk,
J., de Jong, D., & Stokhof, M. (Eds.), Studies Discourse Representation Theory
Theory Generalized Quantifiers, pp. 115143. Foris Publications.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M.,
Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning Python. Journal
Machine Learning Research, 12, 28252830.
Poesio, M., Alexandrov-Kabadjov, M., Vieira, R., Goulart, R., & Uryupina, O. (2005).
discourse-new detection help definite description resolution?. Proceedings 6th International Workshop Computational Semantics, pp. 236246.
Poesio, M., Uryupina, O., Vieira, R., Alexandrov-Kabadjov, M., & Goulart, R. (2004). Discoursenew detectors definite description resolution: survey preliminary proposal.
Harabagiu, S., & Farwell, D. (Eds.), ACL 2004: Workshop Reference Resolution
Applications, pp. 4754. ACL.
Potts, C. (2005). Logic Conventional Implicatures. Oxford University Press.
473

fiDE

ARNEFFE , R ECASENS & P OTTS

Potts, C. (2012). Conventional implicature expressive content. Maienborn, C., von Heusinger,
K., & Portner, P. (Eds.), Semantics: International Handbook Natural Language Meaning, Vol. 3, pp. 25162536. Mouton de Gruyter.
Pradhan, S., Luo, X., Recasens, M., Hovy, E., Ng, V., & Strube, M. (2014). Scoring coreference partitions predicted mentions: reference implementation. Proceedings
52nd Annual Meeting Association Computational Linguistics, pp. 3035. ACL.
https://github.com/conll/reference-coreference-scorers.
Pradhan, S., Moschitti, A., Xue, N., Uryupina, O., & Zhang, Y. (2012). Conll-2012 shared task:
Modeling multilingual unrestricted coreference ontonotes. Joint Conference EMNLP
CoNLL - Shared Task, pp. 140. ACL.
Pradhan, S., Ramshaw, L., Marcus, M., Palmer, M., Weischedel, R., & Xue, N. (2011). CoNLL2011 shared task: Modeling unrestricted coreference OntoNotes. Proceedings
Fifteenth Conference Computational Natural Language Learning: Shared Task, pp. 127.
ACL.
Pradhan, S. S., & Xue, N. (2009). Ontonotes: 90% solution. Proceedings Human Language Technologies: 2009 Annual Conference North American Chapter Association Computational Linguistics, Companion Volume: Tutorial Abstracts, pp. 1112.
ACL.
Prasad, R., Dinesh, N., Lee, A., Miltsakaki, E., Robaldo, L., Joshi, A., & Webber, B. (2008).
Penn Discourse Treebank 2.0. Proceedings Sixth International Language Resources
Evaluation, pp. 29612968. European Language Resources Association.
Prince, E. (1981a). inferencing indefinite NPs. Webber, B. L., Sag, I., & Joshi,
A. (Eds.), Elements Discourse Understanding, pp. 231250. Cambridge University Press.
Prince, E. F. (1981b). Toward taxonomy givennew information. Cole, P. (Ed.), Radical
Pragmatics, pp. 223255. Academic Press.
R Development Core Team (2013). R: Language Environment Statistical Computing. R
Foundation Statistical Computing.
Recasens, M., de Marneffe, M.-C., & Potts, C. (2013). life death discourse entities:
Identifying singleton mentions. Human Language Technologies: 2013 Annual Conference North American Chapter Association Computational Linguistics, pp.
627633. ACL.
Recasens, M., & Hovy, E. (2009). deeper look features coreference resolution.
Lalitha Devi, S., Branco, A., & Mitkov, R. (Eds.), Anaphora Processing Applications,
Vol. 5847 Lecture Notes Computer Science, pp. 2942. Springer.
Recasens, M., & Hovy, E. (2011). BLANC: Implementing Rand index coreference evaluation. Natural Language Engineering, 17(4), 485510.
Roberts, C. (1990). Modal Subordination, Anaphora, Distributivity. Garland.
Roberts, C. (1996). Anaphora intensional contexts. Lappin, S. (Ed.), Handbook Contemporary Semantic Theory, pp. 215246. Blackwell.
Rooryck, J. (2001). Evidentiality, Part II. Glot International, 5(5), 161168.
474

fiM ODELING L IFESPAN ISCOURSE E NTITIES

Saur, R. (2008). Factuality Profiler Eventualities Text. Ph.D. thesis, Brandeis University.
Schwarz, F. (2009). Two Types Definites Natural Language. Ph.D. thesis, UMass Amherst.
Schwarzschild, R. (2002). Singleton indefinites. Journal Semantics, 19(3), 289314.
Simons, M. (2007). Observations embedding verbs, evidentiality, presupposition. Lingua,
117(6), 10341056.
Uryupina, O. (2003). High-precision identification discourse new unique noun phrases.
Proceedings 41st Annual Meeting Association Computational Linguistics
Student Research Workshop, pp. 8086. ACL.
Uryupina, O. (2009). Detecting anaphoricity antecedenthood coreference resolution. Procesamiento del lenguaje natural, 42, 113120.
van Deemter, K., & Kibble, R. (2000). coreferring: Coreference MUC related annotation
schemes. Computational linguistics, 26(4), 629637.
Vieira, R., & Poesio, M. (2000). empirically based system processing definite descriptions.
Computational Linguistics, 26(4), 539593.
Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). model-theoretic
coreference scoring scheme. Proceedings 6th Message Understanding Conference,
pp. 4552. Morgan Kaufman.
Walker, M. A., Joshi, A. K., & Prince, E. F. (Eds.). (1997). Centering Discourse. Oxford University Press.
Wang, L., McCready, E., & Asher, N. (2006). Information dependency quantificational subordination. von Heusinger, K., & Turner, K. (Eds.), Semantics Meets Pragmatics, pp.
267304. Elsevier.
Ward, G., & Birner, B. (2004). Information structure non-canonical syntax. Horn, L. R., &
Ward, G. (Eds.), Handbook Pragmatics, pp. 153174. Blackwell Publishing Ltd.

475

fiJournal Artificial Intelligence Research 52 (2015) 331-360

Submitted 01/15; published 02/15

Scheduling Conservation Designs Maximum Flexibility
via Network Cascade Optimization
Xue
Alan Fern

xue@eecs.oregonstate.edu
afern@eecs.oregonstate.edu

School EECS, Oregon State University
Corvallis, 97331 USA

Daniel Sheldon

sheldon@cs.umass.edu

School Computer Science, University Massachusetts
Amherst, 01003, USA

Abstract
One approach conserving endangered species purchase protect set land
parcels way maximizes expected future population spread. Unfortunately,
ideal set parcels may cost beyond immediate budget constraints
must thus purchased incrementally. raises challenge deciding
schedule parcel purchases way maximizes flexibility budget usage
keeping population spread loss control. paper, introduce formulation
scheduling problem rely knowing future budgets organization.
particular, consider scheduling purchases way achieves population spread
less desired delays purchases long possible. schedules offer conservation
planners maximum flexibility use available budgets efficient way.
develop problem formally stochastic optimization problem network cascade
model describing commonly used model population spread. solution approach
based reducing stochastic problem novel variant directed Steiner tree
problem, call set-weighted directed Steiner graph problem. show
problem computationally hard, motivating development primal-dual algorithm
problem computes feasible solution bound quality
optimal solution. evaluate approach real synthetic conservation data
standard population spread model. algorithm shown produce near optimal
results much scalable generic off-the-shelf optimizers. Finally,
evaluate variant algorithm explore trade-offs budget savings
population growth.

1. Introduction
Reserve site selection key problem conservation planning planners select land
regions designated nature reserves, either achieve general conservation goals
preserving biodiversity, achieve specific goals supporting recovery
endangered species. general, problem extremely complex involves reasoning
interplay uncertain population spread, uncertain future budgets,
problem specific factors. particular, properly assessing population spread involves
reasoning spatial aspects landscapes sizes, shapes, connectivity.
Further, decision space huge, consisting possible land investment combinations
time.
c
2015
AI Access Foundation. rights reserved.

fiXue, Fern, & Sheldon

Given factors, would highly desirable conservation practitioners
enhance decision making via automated, semi-automated, planning scheduling
algorithms. Unfortunately, problem beyond scope existing off-the-shelf stochastic planners schedulers. largely due combination enormous state
action spaces, highly uncertain, exogenous dynamics, need spatio-temporal
reasoning. main contribution paper make progress toward handling
complexities studying useful subproblem conservation planning used
practitioners realistic scenarios. general schema used develop algorithm
widely applicable (see Section 7) one received significant attention
AI community. Thus, hope work also inspire new specialized
general-purpose approaches complex stochastic planning/scheduling problems.
Recently, Sheldon et al. (2010) studied restricted, still challenging, version
conservation planning problem, refer upfront conservation design
optimization. problem, planner given upfront budget stochastic
metapopulation model (Hanski & Ovaskainen, 2000) describes species
consideration spread throughout landscape available habitat. addition
system given information costs potential land parcels available
purchase conservation. objective select set land parcels immediately
purchase conserve, subject budget constraint, maximize spread
population within specified time horizon.
key simplification present problem land parcels assumed
purchased upfront currently available budget. advantage simplification
allows reasonably efficient near optimal solution approach (Sheldon
et al., 2010). However, upfront simplification limits utility approach
number ways. First, conservation budgets generally arrive increments time,
unrealistic purchase large set parcels advance restricting small set
parcels using current budget may suboptimal long run. Moreover,
often unnecessary purchase parcels spatially remote current population
species spread enough make relevant population growth.
Second, upfront simplification requires planners commit advance conservation
strategies may take many years play out, ignores potential advantage
observing responding stochastic outcomes population spreading process
unfolds. example, population spread observed, may beneficial divert
money failed subpopulations purchase parcels near thriving populations.
contrast upfront planning, ideal approach would fully adaptive planning,
where, regular decision epochs, planner would make purchase decisions based
recent population budgetary information. Unfortunately, currently-available
adaptive planning tools scale realistic conservation scenarios. due
combination enormous state space (possible population purchase configurations),
enormous action space (possible subsets land parcels purchase), long horizons (tens
hundreds years), high degree stochasticity population spread model.
Given challenge arriving fully adaptive solution, main contribution
paper introduce problem strikes important middle-ground
upfront fully adaptive approaches. particular, consider conservation design
scheduling exploring trade-off future population cost,
332

fiScheduling Conservation Designs Maximum Flexibility

given initial conservation design (i.e. set parcels purchase) asked
schedule purchase time parcel way (1) achieves population spread
time horizon within arbitrary tolerance population loss, (2) maximizes
purchase flexibility delaying specified purchase time (i.e. purchase deadline)
parcel long possible.
problem formulation simplifies fully adaptive problem number ways.
First, set parcels purchased provided input, removes degree
freedom planning problem. Second, significantly, order select
action current time step, general case fully adaptive planning requires
computing policy dictates possible future contingency, least
reasonably likely ones. contrast, space possible schedules, focus here,
much smaller space policies even partial policies. allows compact
encoding scheduling problem, appear possible problem
computing full, even, partial policies. distinction fully adaptive
scheduling setting akin distinction closed-loop open-loop planning,
generally computing closed-loop plans considered difficult openloop plans large stochastic problems.
solution scheduling problem yields useful tool conservation planners,
first develop conservation designs capture complex decision-making
objectives, perhaps optimization software, schedule purchases obtain
efficient cost-effective implementation design. conservation planner
flexibility purchase parcels time schedule-specified
deadlines, knowing population spread hurt much purchase
delays.
addition, scheduling problem potentially used component
adaptive planner. common successful approach many adaptive planning problems
replanning, decision epoch non-adaptive plan computed current
state first actions executed. work enables replanning approach
decision epoch first computes upfront design using existing work (e.g. Sheldon
et al., 2010), computes schedule purchases parcels scheduled
purchased immediately. purchase strategy would spend minimum amount
budget step guaranteeing limited loss population spread.
addition introducing formalizing problem conservation design scheduling, second contribution paper develop principled algorithm solving
it. key idea apply Sample Average Approximation (SAA) approach (Shapiro,
2003) order arrive novel deterministic optimization problem, develop principled solution motivation special case. particular,
approximated loss tolerance ratio 0, deterministic optimization problem one
network cascade optimization, show equivalent novel variant directed
Steiner tree problem. traditional Steiner tree problem, graph edges associated
costs, objective compute Steiner tree minimum cumulative edge
cost. variant, set-weighted directed Steiner graph problem, costs associated sets edges (possibly non-disjoint) rather individual edges. show
problem computationally hard even restrictions traditional problem
admits efficient solution. present efficient primal-dual algorithm,
333

fiXue, Fern, & Sheldon

guaranteed compute feasible solution bound quality optimal
solution. early-stopping version algorithm provides natural approach
explore trade-off future population budget flexibility.
experiments real synthetic data Red-cockaded Woodpecker
conservation problem show primal-dual algorithm produces near optimal results
much scalable standard optimization tools (CPLEX). also show
trade-off population budget allows flexibility purchasing land
parcels.
follows, Section 2 first presents related work, followed problem formulation Section 3. Section 4 shows reduce subproblem set-weighted
directed Steiner graph problem. Section 5 derives corresponding primal-dual algorithm
natural extension trade-off problem. Experiments presented Section 6.
Finally conclude discuss future work.

2. Related Work
Previously, many different algorithms proposed select reserve sites formulating numerical measure reserve quality (together possible addition
constraints reserve must satisfy) solving optimal set sites
proposed model (e.g. see review article, Williams, Revelle, & Levin, 2005). Although
earliest reserve site selection algorithms largely ignored spatial considerations, many
newer models incorporate spatial objectives constraints directly optimization
problems. Williams, Revelle, Levin argue primary reason importance
spatial attributes fact capture properties landscape favorable
underlying population dynamics, important, computationally difficult, research direction directly optimize respect model population
dynamics instead using spatial attributes proxy. direction
following paper addressing problem spatial conservation planning
respect specific widely adopted model population dynamics.
recent approach explicitly reasons population dynamics model
work Sheldon et al. (2010) upfront conservation design problem, described
Section 1. order cope stochasticity model, popular Sample
Average Approximation (SAA) approach employed transform stochastic problem
deterministic combinatorial optimization problem. problem encoded
Mixed Integer Program (MIP) solved using state-of-the-art solvers.
approach able solve reasonably large problems via various speedup techniques,
scalability still limited relatively small number sample scenarios used SAA,
controls accuracy approach. Kumar et al. (2012) addressed
aspect approach. Lagrangian relaxation used decompose SAA problem
independent subproblems could solved practical time frame, possibly
parallel, standard optimizers. shown significantly reduce runtime
dependence number SAA samples used.
Unfortunately, directly extending approach compute multi-stage adaptive
solutions, budget arrives increments time, seem practical. One
attempt two-stage problems considered Ahmadizadeh et al. (2010).
334

fiScheduling Conservation Designs Maximum Flexibility

explore re-planning using two-stage non-adaptive problem formulation, find
indeed offer advantages upfront planning. setting, budget split
decision epochs manually fixed. Unlike work, work explicitly separates
decision parcels buy decision buy (the focus
work), may develop efficient special-purpose algorithms latter problem
scale much easily bigger problems stages.
several existing approaches might considered fully adaptive
solution conservation problem. example, fully adaptive problem
encoded Markov Decision Process (MDP), resulting state action spaces
would far big state-of-the-art solvers. instance, recent advances solving
large spatio-temporal MDPs (Crowley & Poole, 2011) require significant restrictions
solution space, acceptable application. existing approach
stochastic planning successfully applied Bent et al. (2004), Chang
et al. (2000), Chong et al. (2000) Yoon et al. (2008), Hindsight Optimization
samples future outcomes optimistically estimates state value based
determined futures. However, action space huge, approach would
computational problems current algorithms require enumeration candidate
actions approximating state value. Another approach would formulate
adaptive planning problem multi-stage stochastic integer program. However, size
problem formulation scales exponentially number stages, running
time already costly single stage (Sheldon et al., 2010), two-stage problem
simpler setting fully adaptive (Ahmadizadeh et al., 2010).
Recently, Golovin et al. (2011) proved simple greedy planning strategy provides
near-optimal solutions adaptive conservation setting first appears similar
ours. However, order provide approximation guarantees, authors restrict
population dynamics spread occurs distinct land parcels. may
reasonable assumption slow-moving species certain insects,
focus work, ignores critical aspects population dynamics highly-mobile
animals birds, including Red-cockaded Woodpecker experiments
based.

3. Problem Formulation
section, first introduce basic terminology conservation design planning
define main stochastic optimization problem. Next, describe Sample Average
Approximation (SAA) used transform problem deterministic optimization
problem, focus remainder paper.
3.1 Basic Concepts
largely follow formulation Sheldon et al. (2010). conservation problems
involve (large) land region interest divided land parcels
smallest land units available purchase. parcel contains number distinct
habitat patches, atomic units population dynamics model
either occupied unoccupied species interest. example, Redcockaded Woodpecker problem considered experiments, habitat patches correspond
335

fiXue, Fern, & Sheldon

particular trees prepared humans (or existing birds) facilitate
nesting. parcel p cost c(p), denotes cost purchasing land
restoring conserving habitat patches suitable species
occupy.
conservation design set parcels intended purchased conserved.
Given conservation design D, purchase schedule mapping parcels
purchase times {0, 1, . . . , H, }, H time horizon interest purchasing
parcel time = means parcel going purchased. Thus scheduler
may choose purchase parcels even though part design
realize best tradeoff budget flexibility population spread. Although
species population dynamics yearly time step model (described below),
allowed purchase times (i.e. decision epochs) may less frequent depending specific
problem. upfront schedule one assigns parcels purchase time = 0.
worth noting purchase times specified schedule best viewed
purchase deadlines. is, interpret schedule constraining purchases occur
specified times. view justified fact setup below,
purchasing parcel earlier time specified never result worse population
spread.

3.1.1 Population Dynamics Model
use stochastic dynamics model Sheldon et al. (2010), instance
popular metapopulation model ecology literature (Hanski & Ovaskainen, 2000).
patch two possible states time step, either unoccupied occupied,
conserved patches may occupied. population dynamics consists two types
stochastic events. Colonization events occur population patch colonizes
unoccupied patch b, happens probability pab . Extinction events occur
patch occupied time becomes unoccupied time + 1, happens
probability 1 paa . events independent. details probabilities used
experiments given Section 6.
single-step colonization probability pab experiments typically decays
distance patches b, encodes spatio-temporal dynamics
populations slowly spread source population new habitat made available.
Thus, long-term planning population spread, often unnecessary purchase
parcels distant source population time = 0, since probability
population spreading distant patches near future negligible. delaying
purchases become relevant design (i.e. population spread
nearby), conservation organization use limited funds much flexibly. However,
non-trivial decide much delay purchases harm spread, since
decision depends much spatio-temporal details population spread
model. decision optimization problem defined designed make.
336

fiScheduling Conservation Designs Maximum Flexibility

3.2 Stochastic Optimization Problem
problem statement rely two important concepts: 1) reward schedule,
2) flexibility schedule. first define two concepts formulate
optimization problem terms them.
reward schedule , denoted R(), random variable encodes
amount population spread time H, simply count number occupied
patches time H. easy show model upfront schedule always
achieves least much reward schedule thus maximizes expected
reward. Thus, define maximum expected reward R = E[R(upfront )].
optimization goal find schedule almost achieves optimal expected reward,
i.e. E[R()] (1 )R positive real number indicates percentage
tolerance reward loss maximum purchase flexibility. know
upfront schedule achieves (1 )R , however, requires commitment expenditures
first time step thus least flexible. Indeed, formalize notion
flexibility terms expenditures time.
Given schedule define corresponding cost curve C function
purchase times accumulated cost, C (t) equal total cost parcels
purchased time 0 including time t. curve non-decreasing
provides view schedules spending profile time horizon. particular,
profile cost curve C1 never C2 , i.e. total expenditures 1
never exceed 2 , say 1 offers flexibility terms budget
management compared 2 preferred else equal.
define surrogate cost function schedules
costf () =

X

c(p) f ((p))

p

parameterized function f times {0, . . . , H, } real numbers.
require f () = 0 f parcel purchased within time horizon
would contribute surrogate cost. see surrogate cost function
simply weighted sum parcel costs, weight determined f based
parcels purchase time. Although algorithm work real-valued function,
assume henceforth f strictly decreasing. two reasons this. First,
discounting future costs makes sense due economic factors inflation. Second,
intuitively, since f decreases purchase times, minimizing respect costf would
favor schedules delay purchasing. particular, policy 1 cost profile
never greater 2 , 1 assigned lower surrogate cost f
strictly decreasing.
parcels positive costs, upfront schedule unique element
maximizes surrogate cost, schedule defers purchases time H gives
unique minimum f strictly decreasing function. However, restrict
schedules achieve least reward (1 )R , latter schedule excluded
may longer unique minimum.
337

fiXue, Fern, & Sheldon

specify problem conservation design scheduling, find
schedule set possible schedules that:
arg min costf () s.t. E[R()] (1 )R


(1)

is, schedules achieve reward (1 )R want return one
minimal terms surrogate cost (i.e. maximal flexibility). Thus, controls
trade-off flexibility reward. particular, using larger increases
set feasible schedules allows potential returning flexible schedule
sacrificing reward.
Note varying choice f may possible generate different solutions
Equation 1, minimal sense feasible policy strictly
lower cost curve. experiments, use simple discounted f given f (t) =
discount factor (0, 1).
practice, likely conservation manager particular value
mind design time. Rather, best viewed parameter varied order
observe different flexibility-reward trade-offs possible. final selection
schedule would based assessment possibilities.
Finally, worth noting = 0 (no reward approximation), upfront solution
feasible solution typical population spread models. Thus, using
> 0 necessary achieving additional flexibility. requiring
policy achieve expected reward exactly R (i.e., = 0) requires make purchases
accommodate unlikely outcomes population spread model contribute tiny,
positive, amount expected reward. example, consider outcome
population jumps initial location distant location first year,
undergoes spread. vanishingly small, positive, probability.
upfront schedule support population spread, since distant location purchased
first step. Thus, schedule purchase distant parcel first
step suffer tiny loss reward compared upfront schedule,
achieve = 0. However, purchase tend useless vast majority
probability mass.
3.3 Deterministic Optimization Problem
optimization problem stochastic sense constraint defined
terms expectation complicated population spread distribution. greatly
complicates direct solution problem. prior work stochastic optimization
upfront schedules (Sheldon et al., 2010), address complication converting
stochastic problem approximately equivalent deterministic optimization problem.
done via common Sample Average Approximation (SAA) approach (see
Shapiro, 2003 survey results). key idea approximate stochastic
optimization problem using collection samples probability distribution,
used approximate expectations probabilities via averages samples.
problem formulation, sample corresponds so-called cascade scenario,
particular realization population spread process time horizon.
main idea behind application SAA generate set cascade scenarios
338

fiScheduling Conservation Designs Maximum Flexibility

probabilistic population spread model, approximate expected reward
schedules average reward scenarios. scenarios combined single
scenario graph, illustrated Figure 1 explained detail remainder
section.
concretely, cascade scenario layered graph, layers correspond time
steps, vertex va,t patch time step t. pair patches
(a, b) time step t, coin flipped probability pab determine directed
edge (va,t , vb,t+1 ) present not. edge present patch occupied time
(through previous colonizations non-extinctions), patch b colonized
become occupied time + 1, long conserved. is, presence edge
(va,t , vb,t+1 ) interpreted meaning occupied time b conserved
time + 1, b occupied time + 1 particular scenario.
way, cascade scenario graph encodes occupancy reachability. particular, assuming
(for now) patches conserved, patch b occupied time exactly
vb,t reachable vertex va,0 corresponding initially occupied patch a.
approximate probabilistic spread model, sample set N i.i.d. cascade
n }. scenarios
scenarios {C1 , . . . , CN }, denote vertices Cn {va,t
combined single scenario graph, additional root vertex r
n ) vertex representing initially occupied patch a. Figure 1
directed edges (r, va,0
shows example scenario graph three scenarios range five time steps
involving three patches a, b, c, two parcels, one containing b
containing c. example, initially occupied patch hence
connected time step zero root node r across three scenarios. Assuming
parcels conserved (i.e. purchased upfront), vertex connected root node r,
corresponding patch considered occupied corresponding time
particular scenario. defined r connected
vertices initially occupied patches.
Scenario graphs used work estimate reward schedules follows.
n incoming
Given scenario graph, schedule said purchase node va,t
edges patch purchased later time t, is, (p) belongs
parcel p. Thus, purchasing parcel p time viewed purchasing vertices
scenario graph, along incoming edges involve patches p occur
layer later. reflects fact patch purchased conserved,
considered conserved hence eligible occupancy remainder time
horizon. Figure 1, example schedule shown purchases parcel p1 (containing
b) time 0, parcel p2 (containing c) time 3. vertices purchased
schedule shown shaded region (purchased) incoming edges
shown bold.
define conditions vertex scenario graph considered
n becomes occupied path
occupied given schedule. Vertex va,t
n . define variable X n (a, t) equal 1
purchased edges r va,t

n
va,t occupied 0 otherwise. Figure 1, shaded red set
vertices occupied example policy. example, note scenario 3,
3 occupied since path r purchased edges.
vertex vc,3
despite fact path graph r, since path involves
339

fiXue, Fern, & Sheldon

Figure 1: Example scenario graph (N = 3) problem parcels p1 = {a, b}, p2 = {c}.
schedule ((p1 ) = 0, (p2 ) = 3) also illustrated, using shaded boxes
indicate purchased nodes heavy line weights indicate purchased edges.
Vertices representing occupied patches schedule colored red.

unpurchased edges. Note upfront schedule would purchase node, since
vertices edges would considered purchased schedule.
average reward schedule relative scenario graph built scenarios
{C1 , . . . , CN } denoted follows.
R() =

N
1 XX n
X (a, H)
N

n=1

average across scenarios number occupied patches time H.
Figure 1 average reward example schedule would 2. key property
scenario graphs N R() converges E[R()] fixed
. implies set schedules { : R() (1 )R(upfront )} converges
set { : R() (1 )R } N grows, set policies wish
optimize flexibility over. Further, policy , one use standard probability
concentration bounds (e.g. Chernoff bounds) show event |R() R()|
probability mass decreases exponentially fast N grows. suggests
relatively small number scenarios required reliably obtain tight approximation
true expected reward policy. practice, however, important empirically
validate approximation errors reasonable number scenarios used
approximation.
340

fiScheduling Conservation Designs Maximum Flexibility

motivates deterministic SAA formulation original stochastic optimization problem (1) flexibility optimized subject constraint based
empirical reward R. is, deterministic problem solve:
arg min costf () s.t. R() (1 )R


(2)

R = R(upfront ).
3.4 Overview Solution Approach
Recall stochastic optimization problem (1) setting = 0 resulted optimization problem would typically upfront schedule feasible solution.
Rather, here, approximate SAA formulation, typically non-upfront
solutions feasible, even using = 0. even large (but practical) values N , set scenarios used approximation tend include
highly unlikely scenarios, need accounted stochastic solution
using = 0. observation motivates solution approach (2). particular,
Section 4, first consider problem = 0, turns new variant
classic Steiner tree problem. derive incremental primal-dual algorithm
problem (Section 5) used approximately solve = 0 case
> 0 case early stopping. experiments show approach able
provide significant flexibility little loss reward, flexibility-reward trade-off
controlled > 0.

4. Set-Weighted Directed Steiner Graph Formulation
motivated above, focus optimization problem (2) case = 0.
is, must optimize flexibility subject constraint obtain optimal
empirical reward measured R. section, show formulate problem
novel variant Steiner tree problem.
4.1 Set-Weighted Directed Steiner Graph
(2), arrive final optimization problem = 0:
arg min costf () s.t. R() = R .


(3)

view problem type Steiner tree problem scenario graph.
particular, say vertex time = H terminal vertex reachable
root r, set nodes Xnupfront (a, H) = 1 hence contribute
upfront reward R . way satisfy constraint R() = R purchase
set edges scenario graph connect target nodes r. Thus,
constraint Equation 3 corresponds purchasing edges r path
terminal, Steiner tree problem.
example, consider scenario graph Figure 1. terminal nodes
1 v 3 , two
example nodes layer = 4 except vb,4
b,4
nodes connected r directed path. schedule satisfies constraint
341

fiXue, Fern, & Sheldon

R() = R must purchase edges terminals reachable r. Note
example schedule Figure 1, set purchased edges satisfy
3 .
constraint since path purchased edges terminal vertex vc,4
problem similar traditional Steiner tree problem,
significant difference. traditional problem, edge associated distinct
weight purchased individually, goal connecting terminals using
set edges minimum total weight (which always forms tree). Rather, situation
complicated purchase parcels, correspond subsets edges
scenario graph. particular, purchasing parcel p time t, incurs cost c(p)f (t)
Equation (3), corresponds purchasing edge set Ep,t cost c(p)f (t) contains
n ) come vertex u arrive vertex v n p,
edges (u, va,t
0
a,t0
t0 n {1, . . . , N }. Note cost model, total cost edge sets
purchased schedule exactly equals surrogate objective costf ().
see problem instance problem call
Set-weighted Directed Steiner Graph (SW-DSG) problem, novel variant Steiner
tree problem, goal select set vertices minimal total cost order
connect terminal vertices root. remainder paper
discuss problem general form simplify notation. input SW-DSG
directed graph G = (V, E) single root vertex r, set terminal vertices V,
set edge sets E = {E1 , . . . , EM } Es E, non-negative cost
cs Es . particular, conservation problem edge sets E = {Ep,t }
n ) : (u, v n ) E, p, t0 t, n {1, . . . , N }} cost c
Ep,t = {(u, va,t
0
p,t = c(p)f (t).
a,t0
subset E forms Steiner graph union edges connect r vertices .
desired output minimum cost subset E forms Steiner graph. Note
optimal Steiner graph need tree SW-DSG, unlike traditional Steiner
tree problem.
clear SW-DSG general original deterministic optimization
problem since latter specific edge set structure. instance, Ep,t1 Ep,t2
t1 > t2 . However, structure make easier problem SW-DSG.
following sections, prove problems NP-complete primaldual algorithm either setting. special structure lead
algorithmic advantages deriving primal-dual algorithm. Therefore, mainly discuss
problem form SW-DSG simplify notation.
SW-DSG problem motivated particular conservation application,
relevant problems Steiner style objectives, edge
resources best considered groups. example, Steiner trees often used
design communication networks edges correspond existing potentially new
communication links. situations links must purchased coherent sets
(e.g. communication infrastructure different companies/organizations), SW-DSG
problem would appropriate formulation.
4.2 Computational Complexity
knowledge, SW-DSG generalization Steiner tree problem
previously studied hence consider computational complexity.
342

fiScheduling Conservation Designs Maximum Flexibility

SW-DSG problem generalization traditional directed Steiner tree (DST)
problem, known NP-complete (Hwang, Richards, & Winter, 1992). Further,
standard complexity assumptions, DST hard approximate factor better
log(|T |) (Charikar, Chekuri, Cheung, Dai, Guha, & Li, 1998). Note results
hold even acyclic directed graphs. number effective heuristic algorithms
DST (Drummond & Santos, 2009), many successful relying shortest path computations subroutine. shortest paths computed edge
weighted graphs efficiently, turns case set-weighted problem.
particular, note shortest path problem special case DST (or SW-DSG)
single terminal vertex. problem turns NP-Hard SWDSG, even restricted acyclic graphs special edge set structure shown
original deterministic optimization problem, case scenario graphs
conservation problem.
Theorem 1. SW-DSG problem NP-hard even restricted acyclic graphs
single terminal vertex edge set structure scenario graph.
Proof. prove hardness reducing weighted set cover problem subclass
SW-DSG problems restricted scenario graph one scenario exactly one terminal.
Note consider decision version SW-DSG problem, asks
feasible Steiner graph whose cost less specified threshold C . instance
weighted set cover problem specifies ground set elements = {e1 , . . . , en }, set
= {S1 , . . . , Sm } subsets Sj S, cost Cj subset, cost bound C .
0

problem
asks whether collection total cost C
Sj 0 = S.
Given set cover instance, first describe construct scenario graph illustrated Figure 2 later describe corresponding SW-DSG instance. graph
contains 2n layers, alternate set layers element layers starting
set layer (n layers each, hence 2n layers). layer vertices labeled S1 , . . . , Sm
represent sets n vertices labeled e1 , . . . , en represent elements S.
addition include root vertex r. vertex also seen parcel
single patch. edges graph go one layer immediate next layer
follows. root vertex edge going Sj first set layer. ith
element layer (i.e. layer 2i graph), include edge vertex label Sj
previous layer vertex label ei current layer whenever ei Sj . Finally,
vertex ei ith element layer edge Sj next layer.
corresponding SW-DSG (conservation problem) instance graph specified
follows. root node r single terminal vertex en final element layer.
edge sets specified follows, setting scenario graph.
edge sets Ej,t Sj time t. particular, Ej,t contains every incoming
edge vertex vertex labeled Sj layers t0 t. let
strictly decreasing f (t) sufficiently close 1 ts. cost Ej,t equal
Cj f (t) Cj cost Sj original set cover problem. words,
cost Ej,t almost Cj . Similarly, edge sets Ei,t ei time t. set
costs 0. cost threshold SW-DSG problem equal threshold C
set cover problem.
343

fiXue, Fern, & Sheldon

see reduction correct, consider case resulting SW-DSG
instance feasible solution. solution provides path r en purchased
edge sets total cost C .
Since edge set Ei,t ei zero cost, edge set cost result
purchasing edge sets Ej,t . construction path must go sequence
alternating element nodes set nodes. particular, path must traverse element
node ei = 1, . . . , n. way happen purchase
ei least one edge leading ei one immediately preceding Sj layer 2i,
possible ei Sj . happen purchasing corresponding
edge set Ej,t , corresponding Sj , cost (almost) Cj . see
collection Sj corresponding purchased edge sets must cover elements
total cost C . Thus, collection sets solution set
cover problem.
Conversely consider instance set cover problem feasible solution.
easy verify feasible solution corresponding SW-DSG problem purchase
edge sets Ej,1 corresponding Sj set cover solution. Combining
see feasible solution SW-DSG instance feasible
solution set cover instance.

Figure 2: Description reduction set cover SW-DSG single terminal
vertex scenario graph.

result proves shortest (or least cost) path problem also NP-hard
SW-DSG, i.e. problem finding least cost path edges purchased
sets. Thus, difficult extend prior shortest-path-based heuristics Steiner tree
problem problem. Given SW-DSG NP, NP-complete. motivates
derivation efficient heuristic solution approach next section, computes
feasible solution along bound cost optimal solution. Importantly
bound provides sense good computed solution compared optimal.
344

fiScheduling Conservation Designs Maximum Flexibility

5. Primal-Dual Algorithm
potential solution approach SW-DSG problem encode Mixed Integer
Program (MIP), straightforward, use off-the-shelf MIP optimizer.
approach produced non-trivial results upfront conservation problem (Sheldon et al., 2010), experiments demonstrate, scale well problem.
related approach could consider rounding procedure MIPs LP-relaxation.
solving LP-relaxation easier solving MIP, experiments show
scalability LP solvers also poor problem sizes interest us. Instead,
exploit MIP encoding another way, following primal-dual schema (Vazirani, 2001) derive scalable algorithm performs near optimally experiments.
work considered non-trivial generalization previous work (Wong, 1984),
primal-dual schema applied DST. Moreover, early-stopping version
primal-dual algorithm provides way trade-off schedule flexibility reward
( > 0). Note primal-dual algorithms SW-DSG original deterministic
conservation problem differ notations. words, edge set structure
conservation problem offer improvements algorithm. Thus
following, present approach SW-DSG.
5.1 Primal-Dual Algorithm SW-DSG
apply primal-dual schema, start giving primal MIP SW-DSG problem
along dual LP-relaxation Figure 3. primal MIP includes binary
variable y(Es ) edge set E, indicates whether Es purchased (y(Es ) = 1)
(y(Es ) = 0). objective primal simply sum variables
weighted costs corresponding edge sets. Steiner graph constraint, requiring
terminals connected root node purchased edges, encoded using
standard network-flow encoding (lines 24) involving flow variables xki,j . flow variable
xki,j encodes flow edge (i, j) destined terminal k. flow balance constraints
(2) guarantee one unit flow carried path root node r terminal k.
LP-relaxation primal simply replaces integer constraints y(Es )
variables positivity constraint. dual relaxed problem (lines 69) includes
k corresponding primal flow constraints. Note
dual variables uki wi,j
constraint one unit flow leaves root implied flow constraints.
omitting constraint, one could simplify dual eliminating ukr variables (or,
equivalently, set ukr = 0 k ).
Given primal dual formulations problem, apply primaldual schema designing optimization algorithms. particular, primal-dual algorithm
iterative iteration increases value dual objective purchases
single edge set Es , corresponds setting primal variable y(Es ) = 1.
iteration stops purchased edges form Steiner graph (i.e. primal becomes
feasible). value dual objective end iteration serves lower bound
optimal primal objective, provides worst-case indication far
optimal returned solution is. high level, algorithm simple greedy heuristic
continuously purchases beneficial edge set order build paths
345

fiXue, Fern, & Sheldon

(Primal) min


X

y(Es ) cs ,

subject to:

(1)

s=1

X
(i,h)E

xki,h

X

xkj,i

(j,i)E

xki,j



= r
1,
= 1, = k , k T, V


0,
=
6 r, k
X
y(Es ), k T, (i, j) E


(2)

(3)

s:(i,j)Es

xki,j 0, (i, j) E, k
y(Es ) {0, 1}

(Dual) max

X

(ukk ukr ),

(4)
(5)

subject to:

(6)

kT

X

k
wi,j
cs , {1, . . . , }

(7)

k
ukj uki wi,j
0, k T, (i, j) E

(8)

k,(i,j)Es

k
wi,j

0

(9)

Figure 3: MIP SW-DSG problem corresponding dual LP MIPs LPrelaxation. SW-DSG problem defined graph G = (V, E), root vertex
r, set terminal vertices , set edges sets E = {Es : = 1, . . . , },
Es E.

unconnected terminal. primal-dual schema provides principled way incrementally
computing heuristic time computing lower bound.
Algorithm 1 gives pseudo-code algorithm. main data structure auxiliary
graph G0 = (V, A) vertices input graph G. auxiliary graph edge
set initially empty iteration adds newly purchased edges Es E.
algorithm terminates edges form Steiner graph. edge sets used
construct graph returned solution, following pruning step
removes obviously redundant edge sets, algorithm sometimes include
iteration process.
order describe algorithm detail, first introduce terminology. Given
current auxiliary graph A, let C(k) denote set vertices directed
paths terminal node k via edges A. Note consider k included
C(k). Also, define cut set terminal node k, denoted Cut(k), set
edges (i, j) j C(k) 6 C(k). Intuitively, k already reachable
root, know least one edge Cut(k) must added order
arrive Steiner graph.
346

fiScheduling Conservation Designs Maximum Flexibility

Algorithm 1 Primal-Dual Algorithm SW-DSG.
1: {Inputs: Graph G = (V, E), edge sets E = {E1 , . . . , EM }, costs {c1 , , cM }, terminals
V}
2: Initialize:
k = 0, (i, j) E, k
uki = 0, k T, V; wi,j
0
G = (V, A) =
lowerBound = 0, solution =
3: G0 Steiner graph
4:
Let k random vertex connected r G0
5:
= {s | Es Cut(k) 6= , 6 solution}
6:
= arg minsS (s,
k)P

k0
(s, k) = cs k0 T,(m,n)Es wm,n
/|Es Cut(k)|
7:
8:
9:
10:
11:
12:
13:

ukj = ukj + (s , k), j C(k)
k = w k + (s , k), (i, j) Cut(k)
wi,j
i,j
= Es
lowerBound = lowerBound + (s , k)
solution = solution {s }
end
Pruning: solution = solution {s | s0 solution, Es Es0 }

algorithm first initializes dual variables zeros auxiliary graph
include vertices edges. iteration proceeds first randomly selecting
terminal vertex k connected r auxiliary graph. intuitive level,
algorithm select edge set Es contains cutset edge k according
heuristic (s, k) derived applying primal-dual schema. concretely,
aim iteration raise dual objective value increasing value ukk
maintaining feasibility. Increasing ukk violate constraints type (8)
dual lines 5 8 algorithm maintain feasibility selecting edge set
Es among intersects cut set k raising variables corresponding
vertices C(k) edges Cut(k) value (s , k) (including ukk ). done
way causes dual constraint type (7) corresponding edge set Es become
tight. Since constraint corresponds primal variable y(Es ), algorithm effectively
sets y(Es ) = 1, indicating purchase, adding edges Es A. dual objective
value termination sum across iterations (s , k) returned lower
bound.
key property algorithm iteration increases dual objective,
also maintaining feasibility dual. guarantees iteration
dual objective value corresponds true lower bound optimal value primal.
Theorem 2. iteration primal-dual algorithm produces feasible dual solution
increased objective.
Proof. base case, initialization assigns dual variables zeros,
l },
feasible solution. suppose iteration q 1 starts feasible solution {uli , wi,j
satisfies dual constraints type (7) (8). algorithm terminates,
347

fiXue, Fern, & Sheldon

get feasible solution. Otherwise let k terminal vertex selected. variables
l } l 6= k values changed, (8) satisfied. remaining
{uli , wi,j
variables l = k, three cases. Case 1: j 6 C(k), variables ukj
k unchanged, cannot contribute violation (7) (8). Case 2:
wi,j
edge (i, j) j, C(k), increase ukj uki (s , k) continue
satisfy corresponding constraint (8). Case 3: cut set edge (i, j) Cut(k),
k (s , k) (8) remains satisfied. Since w k edges
increase ukj wi,j
i,j
cut set increased, must ensure constraints type (7) become violated.
choice (s , k) made algorithm verified never violate
constraints makes least one tight.
main portion algorithm terminates, pruning step conducted
remove edge set subset edge set solution, decreases
total cost maintaining feasibility. particular, context conservation
scheduling problem, pruning step ensures parcel purchased
final solution. aggressive computationally expensive
pruning techniques could also used. example, one could consider removing
one selected edge sets final solution test feasibility.
solution still feasible, edge set eliminated. find
aggressive style pruning necessary experiments.
5.1.1 Implementation Running Time
k dual variables.
Note pseudo-code stores updates values ukj wi,j
naive implementation algorithm would result O(|E||T |) runtime
initialization well computation per iteration, |E| number edges
graph |T | number terminals. could much SW-DSG
problems large network one conservation application. However,
algorithm described way presentation purposes. turns
purposes running algorithm, implemented significantly efficiently.
k values
particular, need store update sum corresponding wi,j
edge set (i.e. sum term appears inside definition (s, k) line
6), maintain current objective value (stored lowerBound pseudo-code),
updated line 10. Therefore, iterations + 1 variables
need initialized, number edge sets much smaller
size network. implementation dominant computation per iteration
computation cut set selected terminal k. find cut set backward
traversal terminal k toward root. time computation acceptable
terminals connected relatively small parts overall graph.
case conservation problem, terminals connected nodes
cascade among ones spatially close enough reached.
applications terminals possibly connected large portion graph,
may preferable incrementally maintain cut set every terminal iteration
reduce computation. memory needed O(C|T |) C maximum
size cut set presumably C |E|. getting cut set, algorithm takes
O(M C) time identify best edge set update solution.

348

fiScheduling Conservation Designs Maximum Flexibility

5.2 Early-Stopping Fractional Connection
primal-dual algorithm, computation continues terminals
scenario graph connected paths root. context conservation
problem corresponds reward approximation loss ( = 0). modify
primal-dual algorithm allow reward approximation loss > 0.
case corresponds modifying SW-DSG feasibility constraint require
fraction 1 terminals connected, leading natural way exploring
trade-off reward flexibility.
Given incremental, greedy nature primal-dual algorithm, adds one
edge set iteration, natural choice fractional connection problem stop
algorithm whenever least fraction 1 terminals connected.
basic early-stopping approach lead improvement cost returned
solution, compared = 0, savings often quite minimal. due fact
primal-dual algorithm grows paths terminal nodes root
unaware early-stopping condition. result, paths
grown never actually connected root point algorithm stopped.
unconnected paths considered waste resources respect
meeting fractional connection constraint. Thus, make early-stopping algorithm
viable, necessary perform pruning early-stopping result. algorithm
fractional coverage two stages: 1) Generation, early-stopping used
generate initial solution meets fractional connection constraint, 2) Pruning,
solution produced stage 1 pruned maintaining fractional connection
constraint.
pruning stage use simple effective greedy strategy. idea
iterate purchased parcels schedule returned early-stopping stage
delay purchase parcel long possible ensuring number
connected terminal nodes almost always within required fractional connection
tolerance.
found conservation application, SW-DSG problem corresponds set cascades, beneficial prune using independently generated
larger set scenarios used create initial solution. analogous using validation data tune algorithm parameters Machine Learning prediction problems,
beneficial reasons. found pruning based original set
scenarios often overly aggressive hurt empirical performance due over-fitting
SAA scenarios. Since easily generate independent scenarios estimate true
expected reward pruned policies, better prune based criterion instead.
Also, since computational complexity evaluating reward pruned policies low
compared SAA optimization, afford use larger set scenarios.
particular, experiments formed initial schedules based set 10 cascade
scenarios conducted pruning step respect 40 cascade scenarios.
approach pruning also viewed directly enforcing threshold
(independently estimated) expected reward original stochastic problem (Equation
1) instead enforcing threshold objective value SAA problem (Equation
2). Since cant calculate correct threshold value (the RHS Equation 1) without
349

fiXue, Fern, & Sheldon

knowing true optimum R stochastic problem, use SAA optimum R
place. SAA optimum stochastic upper bound R , generally
conservative approach enforcing Equation 1.

6. Experiments
section, first evaluate primal-dual algorithm applying real, full-scale
conservation problem. Next, verify robustness approach problems,
present results using synthetic conservation data problem generator used several
recent studies. focus first two parts experimentation case = 0,
see provides substantial gains flexibility. order explore trade-off
flexibility reward (population spread), end section, evaluate
early-stopping approach > 0.
6.1 Evaluation Primal-Dual Algorithm Real Conservation Map
real map use dataset prior work Sheldon et al. (2010)
computing upfront conservation designs. data derived conservation problem
involving Red-cockaded Woodpecker (RCW) large land region southeastern
United States interest Conservation Fund. region divided
443 non-overlapping parcels (each area least 125 acres) 2500 patches
serving potential habitat sites. Parcel costs based land prices land
parcels already conserved thus cost zero. use population spread
model Sheldon et al. (2010), based individual-based models RCW.
Since approach requires conservation design input, use design computed
Sheldon et al. (2010) using total budget constraint $320M. map area
shown left cell Figure 7, parcels making design shaded green free
parcels (with cost 0) shaded grey; red + marks indicate initially occupied patches.
method also requires specifying strictly decreasing function defining surrogate cost
function, use f (t) = = 0.96. found results
sensitive value .
6.1.1 Comparing Optimal Solutions
compare solutions primal-dual algorithm optimal solutions found using
CPLEX solver applied MIP encoding SW-DSG problem. MIP encodings
become large horizon number scenarios increase. particular,
443 H + 2500 H N variables number constraints grows number
edges, cascade network, becomes impractical N H grow. Since
optimal solver cant scale large versions problem, consider problems involving
cascade networks 2 scenarios horizons ranging 15 40 years.
also use CPLEX compute solutions LP-relaxation MIP. objective value
returned LP provides alternative approach computing lower bound
optimal solution thus interesting compare lower bound terms tightness
runtime. Since primal-dual algorithm stochastic due random selection
350

fiScheduling Conservation Designs Maximum Flexibility

H
H
H
H
H
H

=
=
=
=
=
=

15
20
25
30
35
40

Cost
MIP
126.8
123.6
117.6
130.4

(M$)
PD
126.22
125.7
121.4
134.0
131.3
127.5

Lower
LP
122.2
117.7
104.7
117.3
109.9

Bound
PD
84.9
71.9
61.5
56.9
64.1
59.7

Run
MIP
5.5
8.2
28
5126

Time
LP
6.13
7.6
10
15
18

(s)
PD
0.9
2.5
9.0
11
25
45

Table 1: Comparison Primal-Dual (PD) MIP LP.

terminal nodes, report averages 20 runs, noting standard deviations
negligible.
first two data columns Table 6.1.1 show (surrogate) cost solutions
found CPLEX solving MIP algorithm (PD) increasing horizons, larger
horizons correspond larger problems. method fails return solution due
memory constraints value shown table. see horizons MIP
able yield solutions CPLEX, algorithm produces solutions similar
costs (here lower cost better). also see MIP solver runs memory
unable return solutions 2 largest problem instances, already scaled
versions problem (small number cascades horizon).
next two columns Table 6.1.1 provide results lower bound computed
CPLEX solving LP PD algorithm. see lower bound produced
LP significantly tighter bound produced algorithm. However,
LP cannot solved CPLEX largest problem, approach still yields
lower bound. Overall, though lower bound good LP (when
computed), generally within factor two optimal solution, provides
non-trivial assurance quality returned solution large problems.
final three columns Table 6.1.1 present time used approaches
problem, blank cells indicate method ran memory. algorithm
significantly faster MIP approach, fails two largest problems,
comparable LP approach, provides lower bound fails
largest problem. later result indicates solution based LP-rounding would
face difficulty, since even solving LP large problems (40 time steps 2500
patches each) computationally demanding. advantage primal-dual algorithm
avoids encoding LP rather works directly graph.

2. PD cost less optimal MIP cost returned CPLEX. investigating, found
CPLEX correctly evaluates solution returns, thinks solution optimal
not. appears issue due small error tolerance allowed CPLEX solver.
3. MIP takes less time LP. think possibly CPLEX uses different algorithms
LP MIP. Especially, MIP solved branch-and-bound algorithm uses modern features
like cutting planes heuristics, making CPLEX powerful MIP solver.

351

fiXue, Fern, & Sheldon

6.1.2 Number Cascades Scenario Graph
According SAA, optimal solution finite set cascade scenarios converge
true optimum scenarios. Previously two cascades used due
poor scalability CPLEX. study number cascade scenarios use
ensure good solution. Recall N increases, original stochastic problem
approximated accurately. Yet larger N corresponds computation.
importantly, = 0, larger N leaves schedule less space flexibility.
extreme case N , possible schedule upfront schedule minimal
flexibility. find good value N practice, study primal-dual solutions
different number cascade scenarios validating reward R() primaldual schedule achieve. Given population spread stochastic, compute
reward R() running 20 simulations stochastic population spread model.
simulation provides reward value (number occupied patches horizon)
average results. schedules produced primal-dual algorithm
upfront schedule. Recall intention nearly match reward
upfront schedule. Figure 4 presents results time horizon H = 20.
observe primal-dual schedule achieves reward N increases,
reward converging towards expected reward upfront schedule. also
see 10 cascades quite close get best performance rate improvement
slowing down. Thus, remainder experiments use 10 cascades SAA.

Figure 4: Rewards PD solutions w.r.t number cascade scenarios.

6.1.3 Quality Conservation Schedules
evaluate algorithm problems realistic sizes. Here, consider
problems based 10 cascades horizons ranging 20 100 years, well
beyond range approachable MIP LP. solution times algorithm
ranged 15 seconds H = 20 29 minutes H = 100.
First, evaluate average accumulated reward schedule returned
method horizon Figure 5. average reward upfront schedule ranged
332 H = 20 615 H = 100 time horizons primal-dual solution
attained average reward least 95.3% optimal, negligible error bars
352

fiScheduling Conservation Designs Maximum Flexibility

averages. small gap indicates 10 scenarios SAA approximation quite
goodthe gap could reduced increasing number scenarios.

Figure 5: Rewards PD schedules w.r.t.
time horizon H.

Figure 6: Cost curves PD schedules
horizons 20 100.

course, must also consider cost curves corresponding schedules, since
affords flexibility criterion problem. Figure 6 presents cost
curves schedules. Note defined Section 3.2, cost curve shows (nondiscounted) accumulated cost schedule time. cost curve schedule
produced horizon H increase time H remain flat, reflecting
purchases made time. see horizons cost curves show
fairly gradual increase cost expenditures time, indicating schedules
indeed providing significant amount flexibility regarding purchase times, particularly
compared upfront schedule, cost curve flat black line
Figure 6 since parcels purchased time 0. experiments shown, found
cost curves vary small amount different values , general
trend present. Interestingly, curves sudden jump cost around 20
years. understand Figure 7 show parcel purchases made
schedule population spread map 100 year horizon. see
= 20 sharp increase cost due purchase relatively expensive
vast parcels southern part design. Looking population spread dynamics,
apparent parcels critical gateway ensuring reliable spread
southwestern part design later years. Delaying purchase longer significantly
increases probability spread occur, approach discovers.
Another interesting observation seen comparing expected population
spread PD computed schedule expected population spread upfront
schedule (Figure 7). striking difference spreads seen time steps
= 20, 60, 80 northeastern part map. upfront schedule entire
northeastern part occupied large part, PD schedule hole
northeastern part near initial bird populations located. Note, however,
hole finally occupied horizon problem (t = 100). time,
spread upfront PD schedules visually similar, agrees fact
measured rewards also similar. reason difference population
spread PD schedule delays purchase northeastern parts
353

fiXue, Fern, & Sheldon

= 20

= 60

= 80

= 100

Figure 7: (Left) Original conservation design used scheduling shown green shaded
parcels. Free (zero-cost) parcels also shaded dark grey red + indicates
initially occupied patches. (Right) top row shows parcels purchased
(shaded green) PD schedule horizon 100 years. middle
row shows population spread horizon schedule,
lighter red shading patch indicates smaller probability occupied
(as measured 20 simulations). bottom row shows population spread
upfront schedule horizon 100 years.

map near initial bird population 20 years time horizon ends.
population spread process schedules, found parcels
closely connected hence become occupied fairly short time bird population
nearby. apparent upfront schedule, areas already occupied
= 20. Thus, purchase parcels delayed long enough time
left population spread landscapes. Therefore, purchasing
delayed parcels far away current population, also parcels
covered quickly reliably. Note flexibility mainly due definition
reward function, takes population time H account. count
population every time step, presumably good schedule would purchase hole
area soon population.
6.2 Evaluation Primal-Dual Synthetic Maps
evaluate primal-dual algorithm thoroughly, randomly selected 10 synthetic
maps generated used prior work (Ahmadizadeh et al., 2010). maps consist
region 146 non-overlapping parcels 411 patches, different configurations
parcel costs initial population. map, considered problems involving
354

fiScheduling Conservation Designs Maximum Flexibility

different conservation designs, design corresponded upfront solution
budget limited factor b total parcel cost map, b ranged 0.1
0.5. section, present similar analysis Section 6.1 show consistent
results, indicating primal-dual algorithm stable across different problems.
6.2.1 Comparing Optimal Solutions
first compare upper lower bounds returned primal-dual algorithm
optimal objective values MIP LP computed using CPLEX. Since CPLEX still
scalability issues solving larger synthetic problems, restrict comparison
problems 2 4 scenarios horizon 20 years.
Results 10 maps similar. example, Figure 8 shows
surrogate cost (PD-UB) dual objective value (PD-LB) primal-dual solution
map 768, together optimal surrogate cost (CPLEX-MIP) lower bound
computed LP CPLEX. see compared optimal, algorithm still
produce solutions similar costs, especially problem easy (b smaller).
Also, see lower bound computed CPLEX better PD lower
bound. However, PD lower bound still within factor 2.

Figure 8: Cost objective value problems map 768. horizontal axis varies
amount budget used compute upfront solution used
conservation design given scheduling algorithms.

6.2.2 Quality Conservation Schedules
consider larger problems based 10 cascade scenarios horizon H = 40,
MIP LP practically solvable.
first compare average accumulated reward schedule returned primaldual algorithm upfront schedule. Figure 9 shows results one maps,
indicating rewards achieved primal-dual schedules always close
upfront schedules, desired. results maps similar.
also study cost curves schedules order illustrate flexibility
compared upfront schedules. Figure 10 presents average cost curves schedules
across 10 maps. noted budget limited, purchase
delayed much. example, b = 0.1, parcels purchased = 15,
long ahead time horizon. analysis explained fact
355

fiXue, Fern, & Sheldon

Figure 9: Rewards primal-dual schedule upfront schedule
map 1027.

Figure 10: Average cost curves PD
schedules 10 maps.

many maps small budgets, sets affordable parcels fairly spread
loosely connected. means population requires time order
reliably spread across sets. Thus, parcels must purchased quite early
horizon support spread. Rather larger budgets, sets parcels must
purchased spread also tightly coupled, allows easier,
reliable population spread. Thus, possible delay purchases much larger extent
seen cost curves larger budgets. shows algorithm able
afford considerable flexibility initial conservation design supports reasonably
reliable population spread.
6.3 Early-Stopping Trading Flexibility Reward
consider early-stopping variant primal-dual algorithm, referred
PD-ES, producing schedules trade flexibility reward using > 0.
dataset use real conservation map. Figure 11 illustrates cost curves
early-stopping schedules = 0.0, 0.05, 0.10, 0.15, 0.20 H = 20, 40, 60, 80,
demonstrates possible budget saving time corresponding fraction reward loss
allowed. Figure 12 shows average simulated rewards schedules.
First notice average reward achieved early-stopping schedules
almost always within specified error tolerance, shows pruning step
generalizing effectively. also see cost curves early-stopping schedules
show significant improvement even small values compared early-stopping
( = 0). example, H = 60 = 0.20, almost cost first
several years, several decades cost approximately half cost required
= 0. results show approach able provide set schedules spans
spectrum trade-offs, considered conservation managers.

7. Summary Future Work
work, addressed problem scheduling purchases parcels conservation
design. formulated problem network cascade optimization problem
356

fiScheduling Conservation Designs Maximum Flexibility

H = 20

H = 40

H = 60

H = 80

Figure 11: Cost curves PD-ES schedules pruning. red line ( = 0) shows
cost curve non-early-stopping PD schedule.
reduced novel variant classic directed Steiner tree problem. showed
problem computationally hard developed primal-dual algorithm
problem. experiments showed algorithm produces close optimal results
much scalable state-of-the-art MIP solver. also showed
early-stopping variant algorithm able explore possible trade-offs
flexibility reward, important consideration practice.
scheduling problem considered work poses considerable challenges generic
off-the-shelf schedulers planners. complicating factors include: 1) highly-stochastic,
exogenous dynamics arise population spread model, 2) need reason
spatio-temporal processes, 3) long horizons must considered, 4)
combinatorial space potential investment options point time. general
solution schema pursued work likely applicable problems
pose similar challenges existing techniques. particular, general schema suggests
approximating problem via SAA studying resulting deterministic
optimization problem. Often resulting deterministic problem correspond
existing well-studied problems, state-of-the-art approximation algorithms
used. cases, work, resulting problem related
existing well-studied problem solution designed extending existing
solution frameworks. expect generic SAA schema particularly useful
357

fiXue, Fern, & Sheldon

H = 20

H = 40

H = 60

H = 80

Figure 12: Rewards primal-dual schedules early-stopping. number
data point indicates percentage PD-ES reward PD reward.

problems involving stochastic spread populations information across networks,
since deterministic problems typical map graph-theoretic problems,
vast literature.
future work, plan consider several improvements primal-dual algorithm.
Currently, iteration, algorithm randomly picks unconnected terminal
grow path from. likely intelligent selection mechanisms improve
overall results. also interested developing primal-dual algorithm directly
incorporates error tolerance constraint early-stopping approach. would
provide direct method trading reward improved flexibility. Furthermore,
intend pursue fully adaptive approaches conservation problems.
One idea incorporate scheduling approach replanning algorithm selects
purchases current decision epoch based up-to-date information.
particular, decision epoch schedule would formed parcels scheduled
purchased immediately (those flexibility) would purchased subset
cases immediate budget would exceeded. Considering sophisticated
approaches take account immediate budget would natural useful
extension. would also interesting consider conservation problem
variants reward function. species, rather caring population
358

fiScheduling Conservation Designs Maximum Flexibility

end, ecological goal may value spread whole period
even more. Presumably models would different properties complexities
one study paper.

Acknowledgements
Parts material paper appeared earlier work Xue, Fern, Sheldon
(2012). work supported NSF grant IIS-0964705.

References
Ahmadizadeh, K., Dilkina, C., Gomes, C. P., & Sabharwal, A. (2010). empirical study
optimization maximizing diffusion network. 16th International Conference
Principles Practice Constraint Programming.
Bent, R., & Van Hentenryck, P. (2004). Regret only! Online stochastic optimization
time constraints. Nineteenth AAAI Conference Artificial Intelligence.
Chang, H. S., Givan, R. L., & Chong, E. K. (2000). On-line scheduling via sampling.
Artificial Intelligence Planning Scheduling.
Charikar, M., Chekuri, C., Cheung, T., Dai, A., Guha, S., & Li, M. (1998). Approximation algorithm directed Steiner tree problems. Ninth Annual ACM-SIAM
Symposium Discrete Algorithms.
Chong, E. K., Givan, R. L., & Chang, H. S. (2000). framework simulation-based
network control via hindsight optimization. IEEE CDC conference.
Crowley, M., & Poole, D. (2011). Policy gradient planning environmental decision making
existing simulators. Twenty-fifth AAAI Conference Artificial Intelligence.
Drummond, L. M., & Santos, M. (2009). distributed dual ascent algorithm Steiner
problems multicast routing. Networks, 53, 170183.
Golovin, D., Krause, A., Gardner, B., Converse, S. J., & Morey, S. (2011). Dynamic resource
allocation conservation planning. Twenty-fifth AAAI Conference Artificial
Intelligence.
Hanski, I., & Ovaskainen, O. (2000). metapopulation capacity fragmented landscape. Nature, 404 (6779), 755758.
Hwang, F. K., Richards, D. S., & Winter, P. (1992). Steiner Tree Problem. Springer.
Kumar, A., Wu, X., & Zilberstein, S. (2012). Lagrangian relaxation techniques scalable spatial conservation planning. Twenty-sixth AAAI Conference Artificial
Intelligence.
Shapiro, A. (2003). Monte Carlo sampling methods. Stochastic Programming, Handbooks
Operations Research Management Science, Vol. 10, pp. 353426.
Sheldon, D., Dilkina, B., Elmachtoub, A., Finseth, R., Sabharwal, A., Conrad, J., Gomes,
C., Shmoys, D., Allen, W., Amundsen, O., & Vaughan, B. (2010). Maximizing
359

fiXue, Fern, & Sheldon

spread cascades using network design. Uncertainty Artificial Intelligence
(UAI).
Vazirani, V. V. (2001). Approximation Algorithms. Springer, Berlin.
Williams, J., ReVelle, C., & Levin, S. (2005). Spatial attributes reserve design models:
review. Environmental Modeling Assessment, 10 (3), 163181.
Wong, R. T. (1984). dual ascent approach Steiner tree problems directed graph.
Mathematical Programming, 28, 271287.
Xue, S., Fern, A., & Sheldon, D. (2012). Scheduling conservation designs via network
cascade optimization. Twenty-sixty AAAI Conference Artificial Intelligence.
Yoon, S., Fern, A., Givan, R. L., & Kambhampati, S. (2008). Probabilistic planning via
determinization hindsight. Twenty-third AAAI Conference Artificial Intelligence.

360

fiJournal Artificial Intelligence Research 52 (2015) 543-600

Submitted 09/14; published 04/15

Distributed Evaluation Nonmonotonic Multi-context Systems
Minh Dao-Tran
Thomas Eiter
Michael Fink
Thomas Krennwallner

DAO @ KR . TUWIEN . AC .
EITER @ KR . TUWIEN . AC .
FINK @ KR . TUWIEN . AC .
TKREN @ KR . TUWIEN . AC .

Institute fur Informationssysteme, TU Wien
Favoritenstrasse 9-11, A-1040 Vienna, Austria

Abstract
Multi-context Systems (MCSs) formalism systems consisting knowledge bases
(possibly heterogeneous non-monotonic) interlinked via bridge rules, global
system semantics emerges local semantics knowledge bases (also called contexts)
equilibrium. MCSs related formalisms inherently targeted distributed settings, truly distributed algorithms evaluation available. address shortcoming present suite algorithms includes basic algorithm DMCS, advanced version DMCSOPT exploits topology-based optimizations, streaming algorithm
DMCS-STREAMING computes equilibria packages bounded size. algorithms behave quite differently several respects, experienced thorough experimental evaluation
system prototype. experimental results, derive guideline choosing appropriate
algorithm running mode particular situations, determined parameter settings.

1. Introduction
last decade, increasing interest systems comprise information
multiple knowledge bases. includes wide range application fields data integration, multi-agent systems, argumentation many others. picture concrete real-world
application, may consider METIS (Velikova et al., 2014), industrial prototype system facilitating timely human decision making maritime control. application, human operators
need support determine whether ship entering port might hide identity illegal activities might high risk environmental hazard. access risks, METIS relies
number heterogeneous external information sources commercial ship database IHS
Fairplay,1 ship tracking websites,2 news items history pollution events ship may
involved in.
rise Word Wide Web distributed systems propelled development,
date several AI-based formalisms available host multiple, possibly distributed knowledge
bases compound system. Well-known formalisms distributed SAT solving (Hirayama
& Yokoo, 2005), distributed constraint satisfaction (Faltings & Yokoo, 2005; Yokoo & Hirayama,
2000), distributed ontologies different flavors (Homola, 2010), MWeb (Analyti, Antoniou, &
Damasio, 2011), different approaches multi-context systems (Giunchiglia & Serafini, 1994;
Ghidini & Giunchiglia, 2001; Brewka, Roelofsen, & Serafini, 2007; Brewka & Eiter, 2007; Bikakis
1. www.ihs.com/products/maritime-information/
2. marinetraffic.com, myship.com
c
2015
AI Access Foundation. rights reserved.

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Figure 1: Pinpointing Joker
& Antoniou, 2010) rooted McCarthys (1993) work; among them, focus Heterogeneous Nonmonotonic Multi-context Systems (MCSs) (Brewka & Eiter, 2007).
generalization previous proposals, MCSs powerful formalism specify systems
knowledge bases may different formats reasoning powers, ranging simple
query answering relational database reasoning description logic knowledge bases (see
Baader et al., 2003), well nonmonotonic formalisms default logic (Reiter, 1980)
answer set programs (Gelfond & Lifschitz, 1991). allow heterogeneous knowledge bases
deal impedance mismatch them, MCSs abstract knowledge bases plain mathematical structures; top, special bridge rules interlink knowledge bases, bridge rule
adds formula knowledge base, depending certain beliefs knowledge bases. Hence
semantics knowledge base associated bridge rules, forms context, depends
contexts, possibly cyclic manner. Based this, MCSs equilibrium semantics
terms global states every context adopts abstract local model, called belief set,
conformant local models adopted contexts addition obeys
bridge rules. following simple example, paraphrase Ghidini Giunchiglias
(2001) Magic Box, illustrates power idea,
Example 1 Suppose computer game, players Batman Robin chased player Joker
partially occluded area, shown Figure 1; Robin wounded cannot read distance
objects. Neither Batman Robin tell Jokers exact position 33 box: Batman
assure columns 2 3, Robin tell row 1. However,
exchange partial knowledge, pinpoint Joker row 1 column 1.
model Batman Robin contexts whose local knowledge bases include information Jokers position, exchanged using bridge rules, row (X) (2 :
row (X)). Batman, informally imports Robins knowledge (context 2) row
positions; full encoding given Example 2. equilibrium emerging MCS discloses
Jokers position Batman Robin.
544

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Although MCSs related formalisms inherently target distributed systems, truly distributed algorithms computing equilibria MCSs available. Brewka Eiter (2007)
encoded equilibria HEX-programs (Eiter, Ianni, Schindlauer, & Tompits, 2005),
evaluated using dlvhex solver. However, approach elegantly offers full heterogeneity, fully centralized needs technical assumptions. Roelofsen, Serafini, Cimatti (2004)
proposed earlier algorithm check satisfiability homogeneous, monotonic MCS
centralized control accesses contexts parallel (hence truly distributed). Bikakis
Antoniou (2010) instead gave distributed algorithm defeasible multi-context systems;
however, latter homogeneous (possibly nonmonotonic) contexts particular type
semantics, algorithm serves query answering model building.
lack distributed algorithms evaluating MCSs based local context handlers due
several obstacles:
abstract view local semantics belief sets limits algorithm global level
interference knowledge bases evaluation process context.
Towards real life applications, certain levels information hiding security required
(e.g. information exchange knowledge bases companies) selected
information transferred contexts via well-defined interfaces. prevents context
getting insight neighbors optimization, instance learn conflicts (i.e.,
joint beliefs leading contradiction) across contexts.
MCS system topology, i.e., structure context linkage, might unknown context;
disables decomposing system efficient, modular evaluation.
bridge rules might fuel cyclic information flow group contexts. Even
context easy evaluate (e.g., knowledge bases acyclic logic programs), global cycles
require nontrivial care.
article, address obstacles present results towards efficient distributed evaluation MCSs. main contributions suite generic algorithms DMCS, DMCSOPT,
DMCS-STREAMING work truly distributed, implementation system prototype.
detail, contributions follows.
1.1 Algorithms Optimization Techniques
(1) first, basic algorithm DMCS aims fully distributed setting deal obstacles
generic way: contexts exchange belief sets call history (i.e., access path
traversing bridge rules), information. global level, belief states formed
tuples belief sets; context bridge rules must respect belief sets neighbors
computing belief sets using local solver knowledge base. Cycles detected
call history, context gets request finds call history; break cycle,
guessing technique used checks return path.
(2) localizing contexts knowledge system information exchange, DMCS
fairly easily adapt context changes (additions deletions), time faces
scalability issues. enhance performance optimized version DMCSOPT, disclose
meta-level information contexts, viz. (i) topology context dependencies, exploited
decomposing MCS sub-MCSs (blocks) linked block-tree, (ii) interface contexts, optimizing data transfer blocks. (i) breaks cycles
545

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

advance (ii) significantly reduces duplicate local evaluation; yields remarkable performance gain.
(3) Still DMCS DMCSOPT compute equilibria MCS, escape
scalability memory issues, multiple local belief sets lead combinatorial explosion
global level. thus consider computing equilibria streaming mode; end, contexts pass
belief sets one shot parents gradually small packages. Memory blowup
thus avoided moreover contexts continue earlier wait answers
neighbors. approach seems user-friendly equilibria gradually appear rather
once, possibly long time; one may quit computation seeing sufficiently
many results (i.e., equilibria).
1.2 Implementation Experiments
implemented algorithms system prototype. assess effects optimization techniques, set benchmarking system conducted comprehensive experiments
MCSs various topologies interlinking. results confirm expectation optimization techniques general; nutshell, (i) decomposition technique clearly improves
performance non-streaming mode; (ii) streaming worthwhile may still find answers
non-streaming times out; (iii) streaming, choosing package size important;
(iv) system topology important optimization techniques show drastic improvements
specific topologies; (v) sometimes, techniques yield gain incur overhead.
results work provide truly distributed algorithms evaluating MCSs,
also distributed versions non-monotonic knowledge base formalisms
(e.g., distributed answer set programs), underlying principles techniques might
exploited related contexts. Furthermore, may provide basis evaluation extensions generalizations MCSs, non-ground MCSs (Fink, Ghionna, & Weinzierl, 2011),
managed MCSs (Brewka, Eiter, Fink, & Weinzierl, 2011), supported MCS (Tasharrofi & Ternovska,
2014), reactive MCSs (Goncalves, Knorr, & Leite, 2014; Brewka, Ellmauthaler, & Puhrer, 2014).
1.3 Organization
remainder article organized follows. next section provides preliminaries
Multi-context Systems. Section 3 introduces basic distributed algorithm DMCS, Section 4 develops optimized algorithm DMCSOPT; Section 5 presents streaming algorithm DMCS-STREAMING. Experimental results prototype implementation reported
Section 6. Section 7, consider related works, Section 8 summarize address
open issues. increase readability, proofs moved Appendix.

2. Preliminaries
sections briefly introduces preliminaries needed rest article.
2.1 Multi-context Systems
First, present formalization Heterogeneous Nonmonotonic Multi-context Systems (MCSs)
proposed Brewka Eiter (2007) described Brewka, Eiter, Fink (2011),
546

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

serves base work. idea behind MCSs allow different logics used
different contexts, model information flow among contexts via bridge rules. notion
logic defined follows.
Definition 1 (cf. Brewka & Eiter, 2007) logic L = (KBL , BSL , ACCL ) composed
following components:
1. KBL set well-formed knowledge bases L, consists set
elements called formulas;
2. BSL set possible belief sets, BSL set elements called beliefs;

3. ACCL : KBL 2BSL function describing semantics logic, assigning
element KBL set acceptable sets beliefs.
notion logic generic, abstracts formation agents beliefs bare
minimum. Structure formulas (both knowledge base belief sets) dismissed,
viewed naked elements. Likewise particular inference mechanism associated
knowledge base, logical properties imposed belief sets; term belief
reflects statements held agent might epistemic basis, without going
detail. assignment acceptable beliefs sets knowledge base, intuitively
set beliefs agent willing adopt given knowledge base, captures logics
(e.g., nonmonotonic logics) multiple even acceptable belief sets possible.
abstract model allows us capture range different logics knowledge representation
reasoning, including classical logic, modal logics, epistemic logics, spatial logics, description
logics etc, also nonmonotonic logics default logic (Reiter, 1980) answer set programs
(Gelfond & Lifschitz, 1991), different varieties settings. comparison formalisms
given Brewka et al. (2011). example, classical (propositional predicate logic) may
modeled follows:
KB: set (well-formed) sentences signature ,
BS: set deductively closed sets -sentences, (i.e., Cn(S) = S, Cn()
denotes deductive closure),
ACC(kb): singleton containing deductive closure kb, i.e., ACC(kb) = {Cn(kb)}.
example nonmonotonic logics, (disjunctive) logic programs answer set semantics (Gelfond & Lifschitz, 1991) modeled
KB: set logic programs signature ,
BS: set consistent sets literals ,
ACC(kb): set (kb) answer sets kb according Gelfond Lifschitz (1991).3
refer setting, used repeatedly sequel, Answer Set Programming
(ASP). Note answer sets knowledge base kb amount particular 3-valued models kb;
intuitively, positive literal p answer set S, p known true, negative
3. common, exclude inconsistent answer sets admitted Gelfond Lifschitz (1991).

547

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

literal p S, p known false, known means literal present
fact derivable rules; neither p p S, truth value p unknown.
MCS modeling possible worlds (scenarios) view via answer sets, generated
answer set solver. However, ASP implementations also capture inference (truth
query respectively answer sets) forms belief set formation.
Bridge rules. Based logics, bridge rules introduced provide uniform way interlinking
heterogeneous information sources follows.
Definition 2 (cf. Brewka & Eiter, 2007) Let L = {L1 , . . . , Ln } (multi-)set logics. Lk bridge rule L, 1 k n, form
(c1 : p1 ), . . . , (cj : pj ), (cj+1 : pj+1 ), . . . , (cm : pm )

(1)

(i) 1 m, ci {1, . . . , n} pi element belief set Lci ,
(ii) kb KBk , holds kb {s} KBk .
Informally, bridge rules refer bodies contexts (identified ci ) thus add
information contexts knowledge base depending believed disbelieved
contexts. contrast Giunchiglias (1992) multi-context systems, single, global set
bridge rules; context knows bridge rules.
means connecting contexts available, MCSs formally defined.
Definition 3 (Brewka & Eiter, 2007) multi-context system (MCS) = (C1 , . . . , Cn ) consists
collection contexts Ci = (Li , kb , br ) Li = (KBi , BSi , ACCi ) logic, kb
KBi knowledge base, br set Li -bridge rules {L1 , . . . , Ln }.
Example 2 (contd) scenario Example 1 formalized MCS = (C1 , C2 ),
contexts L1 , L2 instances Answer Set Programming, and:


col (X) see col (X).
kb 1 = F F1
R,
col (X) see col (X).


row (X) (2 : row (X)).
br 1 =
row (X) covered row (X) (2 : see row (X)), (1 : row (X)).


row (X) see row (X).
kb 2 = F F2
R,
row (X) see row (X).


col (X) (1 : col (X)).
br 2 =
,
col (X) covered col (X) (1 : see col (X)), (2 : col (X)).

F = {row (1). row (2). row (3). col (1). col (2). col (3).},
F1 = {see col (2). see col (3).},
F2 = {see row (1).},
548

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS


R=





joker row (X).
joker col (X).

row (X) joker
row (X) joker
col (X) joker



col (X) joker




in, row (X), row (X).
in, row (X), row (Y ), X 6= Y.
in, col (X), col (X).
in, col (X), col (Y ), X 6= Y.





.




Here, X variables used schematic rules, range rows resp. columns (i.e.,
1,2,3). Intuitively, C1 formalizes Batmans knowledge scene C2 Robin.
knowledge bases kb 1 kb 2 , facts F represent box size 3 3, F1 F2 state
Batman Robin see, viz. Joker columns 2 3 respectively
row 1. next two rules simply map sensed locations respective facts. Informally, rules
R make guess row column Joker is, concluded box (first
two rules); may lead multiple belief sets. Importantly, Batman adjusts knowledge base
depending beliefs communicated Robin (bridge rules br 1 ) vice versa (bridge rules br 2 ).
convenience, introduce following notation conventions. MCS =
(C1 , . .
. , Cn ), denote Bi set
beliefs occur belief sets context Ci , i.e.,
Bi = SBSi S, let BM = ni=1 Bi (simply B, understood). Without loss
generality, assume distinct contexts Ci Cj , Bi Bj = , bridge
atom form (i : bi ) appearing bridge rule , holds bi Bi .
2.2 Semantics Multi-context Systems
semantics MCS defined terms special belief states, sequences =
(S1 , . . . , Sn ) Si element BSi . Intuitively, Si belief set
knowledge base kb ; however, also bridge rules must respected. end, kb augmented
conclusions bridge rules applicable. precisely, bridge rule r form (1)
applicable S, pi Sci , 1 j, pk
/ Sck , j + 1 k m. denote
head (r) head r, app(R, S) set bridge rules r R applicable S.
Then,
Definition 4 (Brewka & Eiter, 2007) belief state = (S1 , . . . , Sn ) MCS = (C1 , . . . ,
Cn ) equilibrium, Si ACCi (kb {head (r) | r app(br , S)}), 1 n.
equilibrium thus belief state contains context acceptable belief set,
given belief sets contexts.
Example 3 (contd) MCS Example 2 single equilibrium = (S1 , S2 )
S1 = F F1 F3 S2 = F F2 F3 F3 = {joker in, row (1), row (2),
row (3), col (1), col (2), col (3)}. equilibrium indeed reflects intuition
scenario Example 1, Batman Robin together infer location Joker,
single one cannot accomplish task without communication.
Example 4 Let = (C1 , C2 , C3 , C4 ) MCS Li ASP logics, signatures
1 = {a}, 2 = {b}, 3 = {c, d, e}, 4 = {f, g}. Suppose
kb 1 = , br 1 = {a (2 : b), (3 : c)};
549

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

kb 2 = , br 2 = {b (4 : g)};
kb 3 = {c d; c}, br 3 = {c e (4 : f )};
kb 4 = {f g }, br 4 = .
One check = ({a}, {b}, {c, d}, {g}) equilibrium .
computation equilibria given MCS realized declarative implementation using HEX-programs (Eiter et al., 2005) evaluated using dlvhex system.4
idea translate MCS HEX-program (i) disjunctive facts guessing
truth values beliefs, (ii) HEX-rules capturing bridge rules, (iii) constraints external
atoms capturing acceptability functions. details concrete implementation
approach, refer reader MCS-IE system (Bogl, Eiter, Fink, & Schuller, 2010).
article, pursue sophisticated approach, i.e., design implement distributed
algorithms, compute equilibria MCSs. evaluation, centralized component
controls communication contexts. context independently runs instance
algorithm communicates exchange beliefs well detect break
cycles. novel contributions described next sections.

3. Basic Algorithm (DMCS)
section introduces first, basic, truly distributed algorithm evaluating equilibria
MCS. algorithm takes general setting input, is, context minimal
knowledge whole system; words, knows interface direct neighbors (parents child contexts) topological information metadata
system. setting, concentrate distributeness. Section 4 shifts focus towards
optimization techniques metadata provided.
Taking local stance, consider context Ck compute parts (potential) equilibria
system contain coherent information contexts reachable Ck .
3.1 Basic Notions
start basic concepts. import closure formally captures reachability.
Definition 5 (Import Closure) Let = (C1 , . . . , Cn ) MCS. import neighborhood
context Ck , k {1, . . . , n}, set
In(k) = {ci | (ci : pi ) B(r), r br k }.
Furthermore, import closure IC (k) Ck smallest set (i) k (ii)
S, In(i) S.
Equivalently, define import closure constructively IC (k) = {k}

IC 0 (k) = In(k), IC j+1 (k) = iIC j (k) In(i).



j0 IC

j

(k),

Example 5 Consider Example 4. In(1) = {2, 3}, In(2) = In(3) = {4}, In(4) =
; import closure C1 IC (1) = {1, 2, 3, 4} (see Figure 2).
4. www.kr.tuwien.ac.at/research/systems/dlvhex/

550

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

C1

C1

IC (1)

In(1)
C2

C2

C3

C3

C4

C4
(a) Import neighborhood C1

(b) Import closure C1

Figure 2: Import neighborhood Import closure
S=

S1

...



...



...

Sj

...

Sn

=



...



...

Ti

...

Tj

...

Tn

./ =

S1

...



...

Ti

...

Sj (= Tj )

...

Figure 3: Joining partial belief states
Based import closure define partial equilibria.
Definition
6 (Partial Belief States Equilibria) Let = (C1 , . . . , Cn ) MCS, let
Sn

/ i=1 BSi . sequence = (S1 , . . . , Sn ) Si BSi {}, 1 n,
partial belief state (PBS) , partial equilibrium (PE) w.r.t. Ck , k {1, . . . , n},
IC (k) implies Si ACCi (kb {head (r) | r app(br , S)}), 6 IC (k) implies
Si = , 1 n.
Note IC (k) essentially defines subsystem 0 connected bridge rules. use
PEs instead equilibria 0 keep original MCS intact.
combining partial belief states = (S1 , . . . , Sn ) = (T1 , . . . , Tn ), define
join ./ partial belief state (U1 , . . . , Un )

Si , Ti = Si = Ti ,
Ui =
, 1 n
Ti , Ti 6= Si = ,
(see Figure 3). Note ./ void, couples Si , Ti BSi different. Naturally,
join two sets partial belief states ./ = {S ./ | S, }.
Example 6 Consider two sets partial belief states:
= { (, {b}, , {f, g}) , (, {b}, , {f, g}) }
= {(, , {c, d, e}, {f, g}), (, , {c, d, e}, {f, g}), (, , {c, d, e}, {f, g})} .
551

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

join given

./ =

(, {b}, {c, d, e}, {f, g}), (, {b}, {c, d, e}, {f, g}),
(, {b}, {c, d, e}, {f, g})


.

3.2 Basic Algorithm
Given MCS starting context Ck , aim finding PEs w.r.t. Ck distributed
way. end, design algorithm DMCS whose instances run independently node
context communicate exchanging sets partial belief states.
provides method distributed model building, DMCS algorithm applied
MCS provided appropriate solvers respective context logics available. main
feature DMCS, also compute projected partial equilibria, i.e., PEs projected relevant
part beliefs showing Ck import closure. exploited specific tasks
like, e.g., local query answering consistency checking. computing projected PEs,
information communicated contexts minimized, keeping communication cost low.
sequel, present basic version algorithm, abstracting low-level implementation issues; overall MCS structure assumed unknown context nodes. idea
follows: starting context Ck , visit import closure expanding import neighborhood
context Ci like depth-first search (DFS), leaf context reached cycle detected, finding current context set hist already visited contexts. leaf context simply
computes local belief sets, transforms partial belief states, returns result
parent (invoking context, Figure 4a). case cycle (Figure 4c), context Ci detects
cycle must also break it, (i) guessing belief sets export interface, (ii) transforming
guesses partial belief states, (iii) returning invoking context.
intermediate context Ci produces partial belief states joined, i.e., consistently
combined, partial belief states neighbors; enable this, Ci returns local belief sets,
joined results neighbors (Figure 4b).
computing projected PEs, algorithm offers parameter V called relevant interface
must fulfill conditions w.r.t. import closure next discuss.
Notation. Given (partial) belief state set V B beliefs, denote S|V restriction
V, i.e., (partial) belief state 0 = (S1 |V , . . . , Sn |V ), Si |V = Si V Si 6= ,
|V = ; set (partial) belief states, let S|V = {S|V | S}. Next,
Definition 7 (Recursive Import Interface) MCS = (C1 , . . . , Cn ) k {1, . . . , n},
call V(k) = {pi | (ci : pi ) B(r), r brk } import interface context Ck V (k) =

iIC (k) V(i) recursive import interface context Ck .
correct relevant interface V, two extremal cases: (1) V = V (k) (2) V = VB = B.
(1), DMCS basically checks consistency import closure Ck computing PEs
projected interface beliefs. (2), computes PEs w.r.t. Ck . between, providing fixed
interface V, problem-specific knowledge (such query variables) and/or infrastructure information
exploited keep computations focused relevant projections partial belief states.
projections partial belief states cached every context recomputation
recombination belief states local belief sets kept minimum.
assume context Ck background process (or daemon Unix terminology)
waits incoming requests form (V, hist), upon starts computation outlined
552

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Si =



SS` ./Sj

lsolve(S)

(V, hist)

})
{i


(V, hist)

Sj



`

Ci

C`

(V

,h






Cj

C`
lsolve((, . . . , )) =
(a) Leaf context

(b) Intermediate context

V
Ci

hi

st

=

{.
.

.,

i,

Cj

..

.}

C`
Ct
(c) Cycle breaking

Figure 4: Basic distributed algorithm - casewise
Algorithm 1. process also serves purpose keeping cache c(k) persistent.
write Ci .DMCS(V, hist) specify send (V, hist) process context Ci wait
return message.
Algorithm 1 uses following primitives:
function lsolve(S) (Algorithm 2): augments knowledge base kb current context
heads bridge rules br applicable w.r.t. partial belief state S, computes
local belief sets using function ACC, combines local belief set S, returns
resulting set partial belief states;
function guess(V, Ck ): guesses possible truth assignments relevant interface w.r.t.
Ck , i.e., Bk V.5
DMCS proceeds following way:

(a) check cache appropriate partial belief state;
5. order relate beliefs Bk , V either vector sets, variables V prefixed context ids;
simplicity, kept V set without assumptions.

553

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Algorithm 1: DMCS(V, hist) Ck = (Lk , kb k , br k )
Input: V: relevant interface, hist: visited contexts
Data: c(k): static cache
Output: set accumulated partial belief states
(a)

c(k) empty return c(k)
:=

(b)
(c)

(d)

k hist
:= guess(V, Ck )
else
:= {(, . . . , )} hist := hist {k}
foreach In(k)
, Ti =
:= ./ Ci .DMCS(V, hist)

(e)

foreach := lsolve(T )

(f)

c(k) := S|V

// cyclic: guess local beliefs w.r.t. V
// acyclic: collect neighbor beliefs add local ones

return S|V
Algorithm 2: lsolve(S) Ck = (Lk , kb k , br k )
Input: S: partial belief state = (S1 , . . . , Sn )
Output: set locally acceptable partial belief states
:= ACCk (kb k {head (r) | r app(brk , S)})
return {(S1 , , . . . , Sk1 , Tk , Sk+1 , . . . , Sn ) | Tk T}

(b) check cycle;
(c) cycle detected, guess partial belief states relevant interface context
running DMCS;
(d) cycle detected, import neighbor contexts needed, request partial belief
states neighbors join them;
(e) compute local belief states given partial belief states collected neighbors;
(f) cache current (projected) partial belief state.
next examples illustrate evaluation runs DMCS finding partial equilibria
different MCS. start acyclic run.
Example 7 Reconsider Example 4. Suppose user invokes C1 .DMCS(V, ),
V = {a, b, c, f, g}, trigger evaluation process. Next, C1 forwards (d) requests C2
C3 , call C4 . called first time, C4 calculates (e) belief sets
assembles set partial belief states
S4 = {(, , , {f, g}), (, , , {f, g})} .
554

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

V

c(1) : S1
C1
S10 |V

2 |V

c(3) : S3

c(2) : S2
C2

C3
3 |V

Figure 5: cyclic topology
caching S4 |V (f), C4 returns S4 |V = S4 one contexts C2 , C3 whose request arrived
first. second call, C4 simply returns S4 |V context cache.
C2 C3 next call lsolve (in (e)) two times each, results S2 = resp. S3 =
S, Example 6.
= { (, {b}, , {f, g}) , (, {b}, , {f, g}) }
= {(, , {c, d, e}, {f, g}), (, , {c, d, e}, {f, g}), (, , {c, d, e}, {f, g})} .
Thus,
S2 |V

= { (, {b}, , {f, g}) , (, {b}, , {f, g}) }

S3 |V

= {(, , {c}, {f, g}), (, , {c}, {f, g}), (, , {c}, {f, g})} .

C1 , computing (d)
S2 |V ./ S3 |V = {(, {b}, {c}, {f, g}), (, {b}, {c}, {f, g}), (, {b}, {c}, {f, g})}
calls lsolve (e) thrice compute final result:
S1 |V = {({a}, {b}, {c}, {f, g}), ({a}, {b}, {c}, {f, g}), ({a}, {b}, {c}, {f, g})} .
next example illustrates run DMCS cyclic topology.
Example 8 Let = (C1 , C2 , C3 ) MCS Li ASP logic,
kb 1 = , br 1 = {a (2 : b)};
kb 2 = , br 2 = {b (3 : c)};
kb 3 = , br 3 = {c (1 : a)}.
Figure 5 shows cyclic topology . Suppose user sends request C1 calling C1 .DMCS(V, ) V = {a, b, c}. step (d) Algorithm 1, C1 calls C2 .DMCS(V, {1}),
context C2 issues call C3 .DMCS(V, {1, 2}), thus C3 invokes C1 .DMCS(V, {1, 2, 3}).
point, instance DMCS C1 detects cycle (b) guesses partial belief states
S10 = {({a}, , ), ({a}, , )}
555

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

1 V. Then, following dotted lines Figure 5, set S10 |V = S10 return value
request C3 , joins initial empty belief state (, , ), gives us
calls lsolve(T ) (e), resulting
S3 = {({a}, , {c, d}), ({a}, , {c, d}), ({a}, , {c, d})} .
next step C3 return S3 |V back C2 , proceed C3 before. result
set belief states
S2 = {({a}, {b}, {c}), ({a}, {b}, {c}), ({a}, {b}, {c})} ,
sent back C1 S2 |V . Notice belief state ({a}, {b}, {c}) inconsistent
C1 , eventually eliminated C1 evaluates S2 |V lsolve.
Next, C1 join S2 |V (, , ), yields S2 |V , use result call lsolve.
union gives us
S1 = {({a}, {b}, {c}), ({a}, {b}, {c})} ,
also sent back user final result.
Given MCS = (C1 , . . . , Cn ) context Ck , using recursive import interface Ck ,
i.e., V (k), relevant interface safe (lower) bound correctness Algorithm 1.
follows, let , Ck , V (k) above.
Theorem 1 (Correctness DMCS partial equilibrium) every V V (k), holds
0 Ck .DMCS(V, ) iff partial equilibrium w.r.t. Ck 0 = S|V .
compute partial equilibria Ck use VB . holds using VB preserves
belief sets returned step (e), projection step (f) takes effect.
Corollary 2 partial equilibrium w.r.t. Ck iff Ck .DMCS(VB , ).
assumption single root context C1 , i.e., IC (1)
2 n, DMCS computes equilibria.
Corollary 3 MCS single root context C1 , equilibrium iff
C1 .DMCS(VB , ).
analysis algorithm yields following upper bound computational complexity
communication activity.
Proposition 4 Let = (C1 , . . . , Cn ) MCS. run DMCS context Ck
interface V, holds
(1) total number calls lsolve exponentially bound n |V|, i.e., O(2n|V| ).
(2) number messages exchanged contexts Ci , IC (k), bounded
2 |E(k)|, E(k) = {(i, cj ) | IC (k), r bri , (cj : pj ) B(r)}.
556

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

3.3 Discussion
Algorithm DMCS naturally proceeds forward import direction context Ck . Thus, starting
there, computes partial equilibria cover Ck contexts import closure.
contexts ignored; fact, unknown contexts closure. partial
equilibria may exist Ck import closure, whole MCS could equilibrium,
because, e.g., (P1) contexts access beliefs Ck closure get inconsistent, (P2)
isolated context subsystem inconsistent.
Enhancements DMCS may deal situations: (P1), context neighborhood
may include importing supporting contexts. Intuitively, Ci imports Cj , Ci
must register Cj . carefully adapting DMCS, solve (P1). However, (P2) remains;
needs knowledge global system topology.
suitable assumption manager exists every context Ci system
reach ask whether isolated inconsistent context subsystem exists; confirms this,
Ci DMCS instance simply returns , eliminating partial equilibria.
improve decentralization information hiding, weaken manager assumption
introducing routers. Instead asking M, context Ci queries assigned router R, collects
topology information needed Ci looks cache. information exchange Ci
R flexible, depending system setting, could contain contexts import information
Ci , isolated inconsistent contexts.
advantage topological information Ci recognize cyclic acyclic
branches upfront; invocation order neighborhood optimized, starting
acyclic branches entering cyclic subsystems. caching mechanism adapted
acyclic branches, intermediate results complete cache meaningful even across
different evaluation sessions.
setting, safe assuming V (k) V. needed resp. Ck
import closure join-contexts, i.e., contexts least two parents. access
path information context, could calculate V fly adjust
MCS traversal. particular, tree- ring-shaped , restrict V locally shared
interface Ck import neighbors, i.e., restricting V bridge atoms br k .
presence join-contexts, V must made big enough, e.g. using path information. Furthermore,
join-contexts may eliminated virtually splitting them, orthogonal parts contexts
accessed. way, scalability many contexts achieved.
Next, present optimization techniques using topological information system.

4. Topology-Based Optimization Algorithm (DMCSOPT)
basic version, Algorithm DMCS uses metadata apart minimal information
context must know: interface every neighboring context. scalability
issues tracked following problems:
(1) contexts unaware context dependencies system beyond neighbors, thus
treat neighbors equally. Specifically, cyclic dependencies remain undetected context,
seeing invocation chain, requests models context chain. Furthermore, context
Ci know whether neighbor Cj already requested models another neighbor Cj 0
would passed Ci ; hence, Ci makes possibly superfluous request Cj 0 .
557

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

(2) context Ci returns local models combined results neighbors. case
multiple models, result size become huge system size number neighbors
increases. fact, one main performance obstacles.
section address optimizations increase scalability distributed MCS evaluation. Resorting methods graph theory, aim decomposing, pruning, improved
cycle breaking dependencies MCSs. Focusing (1), describe decomposition method
using biconnected components inter-context dependencies. Based break cycles
prune acyclic parts ahead create acyclic query plan. address (2), foster partial view
system, often sufficient satisfactory answer, compromise partial
information performance. thus define set variables import dependency
system project models context bare minimum remain meaningful.
manner, omit needless information circumvent excessive model combinations.
proceed follows. introducing running example superficial explanation
optimization it, present details techniques Section 4.2. Section 4.3 introduces
notion query plans, used Section 4.4 describe algorithm DMCSOPT
intertwines decomposition pruning variable projection performance gains MCS evaluation.
4.1 Running Scenario
first present scenario Example 9 running example section.
Example 9 (Scientists Group) group four scientists, Alice, Bob, Charlie, Demi meets
conference closing arrange travel back home. options going train car
(which slower); use train, bring along food. Alice group
leader finally decides, based information gets Bob Charlie.6
Alice prefers go car, would object Bob Charlie want go train.
Charlie daughter, Fiona; mind either option, Fiona sick wants
fastest transport get home. Demi got married, husband, Eddie, wants back
soon, even sooner would come soon; Demi tries yield husbands plea.
Charlie charge buying provisions go train. might choose either salad
peanuts; notably, Alice allergic nuts. options beverages coke juice. Bob
modest; agrees choice Charlie Demi transport dislikes coke. Charlie
Demi want bother others personal matters communicate
preferences, sufficient reaching agreement.

Example 10 scenario Example 9 encoded MCS = (C1 , . . . , C6 ),
Alice = 1, Bob = 2, etc lexicographical order Li ASP logics. knowledge bases kbi
bridge rules bri follows:




car 1 train 1 .
train 1 (2 : train 2 ), (3 : train 3 ).
C1 : kb 1 =
br 1 =
;
nuts 1 .
nuts 1 (3 : peanuts 3 ).
6. Similar scenarios already investigated realm multi-agent systems (on social answer set programming see, e.g., Buccafurri & Caminiti, 2008). aim introducing new semantics scenarios;
example serves plain MCS showcase algorithms.

558

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

B1

1
2

1
2

3
4

3
4
5

4

B2

6

3

4

B3
3
6

5
(b) Diamond-ring block tree

(a) Diamond-ring

Figure 6: Topologies decomposition scientist group example

C2 : kb 2

C3 : kb 3

C4 : kb 4
C5 : kb 5
C6 : kb 6



car 2 (3 : car 3 ), (4 : car 4 ).
= { car 2 , train 2 .} br 2 = train 2 (3 : train 3 ), (4 : train 4 ), ;


(3 : coke 3 ).


car 3 train 3 .








train 3 urgent 3 .
urgent 3 (6 : sick 6 ).
=
;
br 3 =
salad 3 peanuts 3 train 3 .
train 3 (4 : train 4 )





coke 3 juice 3 train 3




= car 4 train 4 br 4 = train 4 (5 : sooner 5 ) ;




= sooner 5 soon 5 br 5 = soon 5 (4 : train 4 ) ;


= sick 6 fit 6 br 6 = .

context dependencies shown Fig. 6a. three equilibria, namely:
({train 1 }, {train 2 }, {train 3 , urgent 3 , juice 3 , salad 3 }, {train 4 },
{soon 5 , sooner 5 }, {sick 6 });
({train 1 }, {train 2 }, {train 3 , juice 3 , salad 3 }, {train 4 }, {soon 5 , sooner 5 }, {fit 6 });
({car 1 }, {car 2 }, {car 3 }, {car 4 }, , {fit 6 }).
Example 11 Consider MCS = (C1 , . . . , C7 ) context dependencies drawn Figure 7a. user queries C1 cares local belief sets C1 ,
evaluation process, C4 discard local belief sets C5 C6 answering call
C2 C3 . However, C1 calls C2 (or C3 ), invoked context must carry local belief sets
C4 answers C1 . reason belief sets C4 cause inconsistent joins C1
partial belief states returned C2 C3 , C5 C7 contribute directly
computing local belief sets C4 . Note belief sets C4 C7 play role determining
applicability bridge rules C1 .
559

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

query

query
C1
C2

C1
C3

C2

C4

C4

,S

C7
(a) Original Topology

C3

C7

(, , S3 )
(b) Triangle

C6

)

C2

C5

3

(

,S

2,

,

C6

S3

)

C1

(

C5

C3

(c) Transitive Reduction

Figure 7: Topology Example 11 (two stacked zig-zag diamonds)
Now, take sub-system including C1 , C2 , C3 , assuming C1 bridge rules atoms
(2 : p2 ) (3 : p3 ) body, C2 atoms (3 : p3 ). is, C1 depends C2
C3 , C2 depends C3 (see Fig. 7b). straightforward approach evaluate MCS asks
C1 belief sets C2 C3 . C2 also depends C3 , would need another query
C2 C3 evaluate C2 w.r.t. belief sets C3 . shows evident redundancy, C3
need compute belief sets twice. Simple caching strategies could mellow second belief
state building C3 ; nonetheless, C1 asks C3 , context transmit belief states back,
thus consuming network resources.
Moreover, C2 asks PEs C3 , receive set PEs covers belief sets
C3 addition contexts C3 import closure. excessive C1 view,
needs know (2 : p2 ) (3 : p3 ). However, C1 needs belief states C2
C3 reply C2 : C2 reports belief sets (which consistent w.r.t. C3 ), C1 cant
align belief sets received C2 received C3 . Realizing C2 also reports
belief sets C3 , call C3 must made.
4.2 Decomposition Nonmonotonic MCS
Based observations above, present optimization strategy pursues two orthogonal
goals: (i) prune dependencies MCS cut superfluous transmissions, belief state building,
joining belief states; (ii) minimize content transmissions. start defining
topology MCS.
Definition 8 (Topology) topology MCS = (C1 , . . . , Cn ) directed graph GM =
(V, E), V = {C1 , . . . , Cn } resp. V = {1, . . . , n} (i, j) E iff rule br
atom (j:p) body.
first optimization technique made three graph operations. get coarse view
topology splitting biconnected components, form tree representation MCS.
Then, edge removal techniques yield acyclic structures.
560

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

sequel, use standard terminology graph theory (see Bondy & Murty, 2008);
graphs directed default. may view undirected graphs directed graphs
edges (u, v), (v, u) undirected edge {u, v}.
graph G edges E(G), denote G\S maximal subgraph G
edges S. Suppose V 0 V (G) nonempty. subgraph G0 = (V 0 , E 0 ) G
vertex set V 0 edge set E = {(u, v) E(G) | u, v V (G)} subgraph induced
V 0 , denoted G[V 0 ]. induced subgraph G[V \ V 0 ] denoted G\V 0 ; results G
deleting vertices V 0 together incident edges. V 0 = {v}, write G\v G\{v}.
Two vertices u v G said connected, (directed) path u v
G, i.e., sequence vertices u = v1 , v2 , . . . , vn = v, (vi , vi+1 ) E(G), =
1, . . . , n1; path trivial n = 1. undirected graph G, connectedness equivalence
relation V (G). Thus partition V (G) nonempty subsets V1 , V2 , . . . , Vw
two vertices u v G connected iff belong set Vi .
subgraphs G[V1 ], G[V2 ], . . . , G[Vw ] called components G. w = 1 (i.e., G exactly
one component), G connected; otherwise, G disconnected.
directed graph G strongly connected, vertices u, v V (G) path u v
vice versa exists. strongly connected components G subgraphs G[V1 ], . . . , G[Vm ]
unique partition graph G pairwise disjoint induced subgraphs (i.e., Vi Vj = )
strongly connected.
Furthermore, directed graph G weakly connected, turning edges undirected edges
yields connected graph. vertex c weakly connected graph G cut vertex, G\c
disconnected. biconnected graph weakly connected graph without cut vertices.
block graph G maximal
biconnected
subgraph G. Given set blocks B,


union blocks B defined B = BB B, union two graphs G1 = (V1 , E1 )
G2 = (V2 , E2 ) defined G1 G2 = (V1 V2 , E1 E2 ).
Let (G) = (B C, E) denote undirected bipartite graph, called block tree graph G,

(i) B set blocks G,
(ii) C set cut vertices G,
(iii) (B, c) E B B c C iff c V (B).
Note (G) forest graph G rooted tree G weakly connected.
Example 12 Consider graph Figure 7a. One check 4 cut vertex
two blocks, viz. subgraphs induced {1, 2, 3, 4} {4, 5, 6, 7}.
next example shows block tree scenario Example 9.
Example 13 topology GM Example 10 shown Figure 6a. two cut vertices,
namely 3 4; thus block tree (GM ) (Figure 6b) contains blocks B1 , B2 , B3 ,
subgraphs GM induced {1, 2, 3, 4}, {4, 5}, {3, 6}, respectively.
topological sort directed graph linear ordering vertices every
directed edge (u, v) vertex u vertex v, u comes v ordering.
561

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Pruning. acyclic topologies, like triangle presented Figure 7b, exploit minimal
graph representation avoid unnecessary calls contexts, namely, transitive reduction
graph GM . Recall Aho, Garey, Ullman (1972) graph G transitive
reduction directed graph G whenever following two conditions satisfied:
(i) directed path vertex u vertex v G iff directed path u v
G,
(ii) graph fewer edges G satisfying condition (i).
Note G unique G acyclic. instance, graph Figure 7c unique transitive
reduction one Figure 7a.
Ear decomposition Another essential part optimization strategy break cycles removing edges. end, use ear decompositions cyclic graphs. block may multiple
cycles necessarily strongly connected; thus first decompose blocks strongly
connected components. Using Tarjans algorithm (Tarjan, 1972) task, one gets byproduct topological sort directed acyclic graph formed strongly connected components.
yield sequence nodes r1 , . . . , rs used entry points component. next
step break cycles.
ear decomposition strongly connected graph G rooted node r sequence P =
hP0 , . . . , Pm subgraphs G
(i) G = P0 Pm ,
(ii) P0 simple cycle (i.e., repeated edges vertices) r V (P0 ),
(iii) Pi (i > 0) non-trivial path (without cycles) whose endpoint ti P0 Pi1 ,
nodes not.
Let cb(G, P ) set edges containing (`0 , r) P0 last edge (`i , ti ) Pi , > 0.
Here, `0 vertex belonging edge root node r simple cycle P0 .
Example 14 Take, example, strongly connected graph G Figure 8a. ear decomposition G rooted node 1 P = hP0 , P1 , P2 , P3
VP0 = {1, 2, 3}, EP0 = {(1, 2), (2, 3), (3, 1)},

VP1 = {2, 4, 3}, EP1 = {(2, 4), (4, 3)},

VP2 = {2, 5, 3}, EP2 = {(2, 5), (5, 3)},

VP3 = {1, 4}, EP4 = {(1, 4)}.

last edges Pi dashed. form set cb(G, P ) = {(3, 1), (4, 3), (5, 3), (1, 4)}.
Removing edges results acyclic topology Figure 8b.
Intuitively, ear decomposition used remove cycles original system .
resulting acyclic topology, algorithms evaluating MCSs designed conveniently.
trade edge (`, t) removed , context C` , despite leaf
context, guess values variables Ct . following example shows application
optimization techniques running scenario.
Example 15 (contd) Block B1 (GM ) acyclic, transitive reduction gives B1
edges {(1, 2), (2, 3), (3, 4)}. B2 cyclic, hB2 ear decomposition rooted 4;
removing cb(B2 , hB2 i) = {(5, 4)}, obtain B20 edges {(4, 5)}. B3 acyclic already
reduced. Fig. 6b shows final result (dotted edges removed).
562

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

1

2

1

3

2

3

4

4

5

5

(a) strongly connected component

(b) Acyclic topology

Figure 8: Ear decomposition example
graph-theoretic concepts introduced here, particular transitive reduction acyclic blocks
ear decomposition cyclic blocks, used implement first optimization MCS
evaluation outlined above. Intuitively, block, apply ear decomposition get rid cycles (with trade-off guessing), use transitive reduction minimize communication.
Given transitive reduction B acyclic block B B, total order V (B ) , one
evaluate respective contexts reverse order total order computing PEs
context Ck : first context simply computes local belief sets whichrepresented
set partial belief states S0 constitutes initial set partial belief states T0 . iteration
step 1, Ti computed joining Ti1 local belief sets Si considered context Ci .
Given final Tk , Tk |V (k) set PEs Ck (restricted contexts V (B )).
Refined recursive import. Next, define second part optimization strategy
handles minimization information needed transmission two neighboring contexts
Ci Cj . purpose, refine notion recursive import interface (Definition 7)
context w.r.t. particular neighbor given (sub-)graph.
Definition 9 Given MCS = (C1 , . . . , Cn ) subgraph G GM ,
edge (i, j)


E(G), recursive import interface Ci Cj w.r.t. G V (i, j)G = V (i) `G|j B`
G|j contains nodes G reachable j.7
Example 16 (contd) MCS Example 10, V (1) = {train 2 , train 3 , peanuts 3 ,
car 3 , coke 3 , car 4 , train 4 , sooner 5 , sick 6 }. focus block B1 , refined recursive
import interface V (1, 2)B obtained removing bridge atoms contexts
1
blocks B2 B3 , yielding {train 2 , train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }.
Algorithms. Algorithms 3 4 combine optimization techniques outlined above. Intuitively,
OptimizeTree takes input block tree parent cut vertex cp root cut vertex cr . traverses DFS manner calls OptimizeBlock every block. call results collected
7. Note V (k) defined Definition 7.

563

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Algorithm 3: OptimizeTree(T = (B C, E), cp , cr )
Input: : block tree, cp : identifiesSlevel , cr : identifies
level cp

Output: F : removed edges B, v: labels ( B)\F
B 0 := , F := , v :=
cp = cr
B 0 := {B B | cr V (B)}
else
B 0 := {B B | (B, cp ) E}

(a)

(b)

// initialize siblings B0 return values

foreach sibling block B B 0
// sibling blocks B parent cp
E := OptimizeBlock(B, cp )
// prune block
0
C := {c C | (B, c) E c 6= cp }
// children cut vertices B
B 0 := B\E, F := F E
foreach edge (i, j) B 0 doS
// setup interface pruned B

v(i, j) := V (i, j)B 0 cC 0 V (cp )|Bc (`,t)E V (cp )|Bt
foreach child cut vertex c C 0
// accumulate children
(F 0 , v 0 ) := OptimizeTree(T \B, c, cp )
F := F F 0 , v := v v 0
return (F, v)

set F removed edges; blocks processed, final result OptimizeTree
pair (F, v) v labeling remaining edges. OptimizeBlock takes graph G calls
CycleBreaker cyclic G, decomposes G strongly connected components, creates
ear decomposition P component Gc , breaks cycles removing edges cb(Gc , P ).
resulting acyclic subgraph G, OptimizeBlock computes transitive reduction G
returns edges removed G. OptimizeTree continues computing labeling v remaining edges, building recursive import interface, keeping relevant
interface beliefs child cut vertices removed edges. Example 20 (Appendix B) illustrates
Algorithms 3 4 detailed run MCS Example 10.
Formally, following property holds.
Proposition 5 Given MCS context Ck k cut vertex topology GM ,
OptimizeTree(T (GM ), k, k) returns pair (F, v)
(i) subgraph G GM \F induced IC (k) acyclic,
(ii) block B G (i, j) E(B), holds v(i, j) V (i, j)B .
Regarding computational cost computation, obtain:
Proposition 6 Given MCS context Ck k cut vertex topology GM ,
OptimizeTree(T (GM ), k, k) runs polynomial (quadratic) time size (GM ) resp. GM .

564

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Algorithm 4: OptimizeBlock(G : graph, r : context id)

(c)
(d)

F :=
G cyclic
F := CycleBreaker(G, r)

// ear decomposition strongly connected components

Let G transitive reduction G\F
return E(G) \ E(G )

// removed edges G

4.3 Query Plan
Given topology MCS, need represent stripped version contains
minimal dependencies contexts interface beliefs need transferred contexts. representation query plan used execution processing.
Syntactically, query plans following form.
Definition 10 (Query Plan) query plan MCS w.r.t. context Ck labeled subgraph
GM induced IC (k) E() E(GM ), edge labels v : E(G) 2 .
MCS context Ck , every query plan suitable evaluating M; however,
following query plan fact effective.
Definition 11 (Effective Query Plan) Given MCS context Ck , effective query plan
respect Ck k = (V (G), E(G)\F, v) G subgraph GM induced
IC (k) (F, v) = OptimizeTree(T (GM ), k, k).
next use k MCS evaluation, tacitly assume query plans effective.
4.4 Evaluation Query Plans
present algorithm DMCSOPT, based DMCS exploits optimization
techniques above. idea DMCSOPT follows: start context Ck traverse
given query plan k expanding outgoing edges k context, like DFS,
leaf context Ci reached. context simply computes local belief sets, transforms belief
sets partial belief states, returns result parent. Ci (j : p) bridge rules
bodies context Cj query plan (this means broke cycle removing last
edge Cj ), possible truth assignments import interface Cj considered.
result context Ci set partial belief states, amounts join, i.e.,
consistent combination, local belief sets results neighbors; final result
obtained Ck . keep recomputation recombination belief states local belief sets
minimum, partial belief states cached every context.
Algorithm 5 shows distributed algorithm, DMCSOPT, instance context Ck .
input id c predecessor context (which process awaits), proceeds based
(acyclic) query plan r w.r.t. context Cr , i.e., starting context system. algorithm
maintains cache(k) cache Ck (which kept persistent).
Ci .DMCSOPT(c): send id c DMCSOPT context Ci wait result.
guess(V): guess possible truth assignments interface beliefs V.
565

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Algorithm 5: DMCSOPT(c : context id predecessor) Ck = (Lk , kb k , br k )
Data: r : query plan w.r.t. starting context Cr label v, cache(k): cache
Output: set accumulated partial belief states
(a)

(b)
(c)

(d)

(e)

cache(k) empty
:= cache(k)
else
:= {(, . . . , )}
foreach (k, i) E(r ) := ./ Ci .DMCSOPT(k)

// neighbor belief sets

In(k) s.t. (k, i)
/ E(r ) Ti =
:= guess(v(c, k)) ./
// guess removed dependencies r
:=
foreach := lsolve(T )
// get local beliefs w.r.t.
cache(k) :=
(c, k) E(r ) (i.e., Ck non-root)
return S|v(c,k)
else
return

lsolve(S) (Algorithm 2): given partial belief state S, augment kbk heads
bridge rules brk applicable w.r.t. (=: kb0k ), compute local belief sets ACC(kb0k ),
merge S; return resulting set partial belief states.
steps Algorithm 5 explained follows:
(a)+(b) check cache, empty get neighbor contexts query plan, request
partial belief states neighbors join them;
(c) (i : p) bridge rules brk (k, i)
/ E(r ), neighbor delivered
belief sets Ci step (b) (i.e., Ti = ), call guess interface v(c, k)
join result (intuitively, happens edges removed
cycles);
(d) compute local belief states given partial belief states collected neighbors;
(e) return locally computed belief states project variables v(c, k) nonroot contexts; point mask parts belief states needed
contexts lying different block (GM ).
Theorem 7 shows DMCSOPT sound complete.
Theorem 7 Let Ck context MCS , let k query plan Definition 11 let
b = {p v(k, j) | (k, j) E(k )}. Then,
V
(i) 0 Ck .DMCSOPT(k), exists partial equilibrium w.r.t. Ck
0 = S|Vb ;
(ii) partial equilibrium w.r.t. Ck , exists 0 Ck .DMCSOPT(k)
0 = S|Vb .
566

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

5. Streaming Equilibria (DMCS-STREAMING)
Algorithm DMCSOPT shows substantial improvements DMCS; however, sizes
local knowledge bases context interfaces increase, also suffers bottlenecks.
stems way models exchanged contexts. Suppose context C1
accesses several neighbors C2 , . . . , Cm acyclic information flow, Ci , n,
ni PEs. Ci computes DMCS resp. DMCSOPT local models, must join PEs
neighbors; may lead n2 n3 nm many PEs, input
local solver. may take considerable time also exhaust memory, even
local model computation starts.
Note however instead neighbor would transfer portion PEs,
computation C1 avoid memory blowup. Moreover, strategy also helps reduce
inactive running time C1 waiting neighbors return PEs, C1 already start
local computing neighbors producing models.
general, indispensable trade computation time, due recomputations, less
memory eventually partial equilibria C1 shall computed. idea underlying
streaming evaluation method distributed MCS. particularly useful user interested
obtaining instead answers system, also realistic scenarios
current evaluation algorithm manage output resource constraints
practice equilibrium all.
section, turn idea concrete streaming algorithm DMCS-STREAMING
computing partial equilibria. main features briefly summarized follows:
algorithm fully distributed, i.e., instances components run every context
communicate, thus cooperating level peers;
invoked context Ci , algorithm streams (i.e. computes) k 1 partial equilibria
Ci time; particular, setting k = 1 allows consistency checking MCS
(sub-)system.
issuing follow-up invocations one may compute next k partial equilibria context C1
equilibria exist; i.e., evaluation scheme complete.
local buffers used storing exchanging local models (partial belief states)
contexts, avoiding space explosion problem.
section mainly studies streaming aspect algorithm, simplify presentation omit interface contexts. principles presented applied
DMCS DMCSOPT adapting interface pruning topology preprocessing
time. Furthermore, assume work acyclic MCSs. Treatment cyclic cases easily
achieved adding guessing code solving component DMCS DMCSOPT.
best knowledge, similar streaming algorithm neither developed
particular case computing equilibria MCS, generally computing models
distributed knowledge bases. Thus, results obtained interest setting
heterogeneous MCS, also relevant general model computation reasoning
distributed (potentially homogeneous) knowledge bases like e.g. distributed SAT instances.
567

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

request (k1 , k2 )

k belief states

Handler

Ci

Cj1

Solver

Joiner

..
.

Output

Cjm

Figure 9: DMCS-STREAMING architecture
Algorithm 6: Handler(k1 , k2 : package range) Ci
Output.k1 := k1 , Output.k2 := k2 ,
Solver.k2 := k2 , Joiner.k := k2 k1 + 1
call Solver

5.1 Basic Streaming Procedure
basic idea follows: pair neighboring contexts communicate multiple rounds,
request effect receive k PEs. communication window k PEs
ranges k1 -th PE k2 -th (= k1 + k 1) PE. parent context Ci requests child
context Cj pair (k1 , k2 ) receive time later package k PEs; receiving
indicates Cj fewer k1 models. parallelized version discussed Section 5.2.
Important subroutines new algorithm DMCS-STREAMING take care receiving
requests parents, receiving joining answers neighbors, local solving returning
results parents. reflected four components: Handler, Solver, Output, Joiner
(only active non-leaf contexts); see Figure 9 architectural overview.
components except Handler (shown Algorithm 6) communicate using message queues:
Joiner j queues store partial equilibria j neighbors, Solver one queue hold joined
PEs Joiner, Output queue carry results Solver. bound space usage,
queue limit number entries. queue full (resp., empty), enqueuing writer
(resp., dequeuing reader) automatically blocked. Furthermore, getting element also removes
queue, makes room PEs queue later. property frees us
synchronization technicalities.
Algorithms 7 8 show Solver Joiner work. use following primitives:
lsolve(S): works lsolve DMCS DMCSOPT, addition may return one
answer time may able tell whether models left. Moreover, require
results lsolve returned fixed order, regardless called. property
key guarantee correctness algorithm.
get first(`1 , `2 , k): send neighbor c`1 c`2 request first k partial equilibria, i.e., k1 = 1 k2 = k; return models, store respective queues
return true; otherwise, return false (some neighbor inconsistent).
568

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Algorithm 7: Solver() Ci
Data: Input queue: q, maximal number models: k2

(a)
(b)

count := 0
count < k2
Ci leaf :=
else call Joiner pop q
= count := k2

(c)

count < k2
pick next model ? lsolve(S)
? 6=
push ? Output.q
count := count + 1
else break
refresh() push Output.q

get next(`, k): request next k equilibria neighbor Cc` ; Cc` sends back models, store queue q` return true; otherwise, return false neighbor already
exhaustively returned PEs previous request. Note subroutine needs keep
track range already asked neighbor, maintaining set counters.
counter w.r.t. neighbor Cc` initialized 0 increased time get next(`, k) called.
value t, request Cc` asks tth package k models, i.e., models range
given k1 = (t 1) k + 1 k2 = k. get first(`1 , `2 , k) called, counters
range [`1 , `2 ] reset 0.
refresh(): reset counters flags Joiner starting states, e.g., first join true,
counters 0.
process context Ci triggered message parent, contains
range (k1 , k2 ) arrives Handler. latter notifies Solver compute k2 models Output
collect range (k1 , k2 ) return parent. Furthermore, sets package
size Joiner k = k2 k1 + 1 case Ci needs query neighbors (cf. Algorithm 6).
Solver receives notification Handler, first prepares input local solver.
Ci leaf context, input gets empty set assigned Step (a); otherwise, Solver triggers
Joiner (Step (b)) input neighbors. Fed input them, lsolve used Step (c)
compute k2 results send output queue.
Joiner, activated intermediate contexts discussed, gathers partial equilibria neighbors fixed ordering stores joined, consistent input local buffer.
communicates one input time Solver upon request. fixed joining order guaranteed always asking first package k models neighbors beginning Step (d).
subsequent rounds, begin finding first neighbor Cc` return models
(Step (e)), reset query ask first packs k models neighbors Cc1 Cc`1 .
neighbors run models Step (f), joining process ends sends Solver.
Note procedure guarantees models missed, lead consider combinations (inputs Solver) multiple times. Using cache helps mitigate
569

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Algorithm 8: Joiner() Ci
Data: Queue q1 , . . . , queue qj In(i) = {c1 , . . . , cj }, buffer partial equilibria: buf ,
flag first join
true
buf empty
pop buf , push Solver.q
return

(d)

(e)

(f)

first join
get first(1, j, k) = false
push Solver.q
return
else first join := false
else
` := 1
get next(`, k) = false ` j ` := ` + 1
1 < ` j
get first(1, ` 1, k)
else ` > j
push Solver.q
return
S1 q1 , . . . , Sj qj add S1 ./ ./ Sj buf
C1

C2

C4

C3

C5

C6

C7

Figure 10: Binary tree MCS

recomputation, unlimited buffering quickly exceeds memory limits, recomputation
inevitable part trading computation time less memory.
Output component simply reads queue receives reaches k2 models
(cf. Algorithm 9). Upon reading, throws away first k1 1 models keeps ones
k1 onwards. Eventually, fewer k1 models returned Solver, Output
return parent.
Example 17 Let = (C1 , . . . , Cn ) MCS given integer > 0, n =
2m+1 1 contexts, let ` > 0 integer. Let contexts ASP logics. < 2m ,
570

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Algorithm 9: Output() Ci
Data: Input queue: q, starting model: k1 , end model: k2
buf := count := 0
count < k1
pick Output.q
= count := k2 + 1
else count := count + 1
count < k2 + 1
wait Output.q
= count := k2 + 1
else
count := count + 1
add buf
buf empty
send parent
else
send content buf parent

context Ci = (Li , kbi , bri )
(
kbi =

{aji



aji

ti | 1 j `} bri =

)
fi
fi
ti (2i : aj2i ),
fi1j` ,
ti (2i + 1 : aj2i+1 ) fi

(2)

2m , let Ci
kbi = {aji aji | 1 j `} bri = .

(3)

Intuitively, binary tree-shaped MCS depth `+1 size alphabet
context. Figure 10 shows MCS n = 7 contexts depth = 2; internal contexts
knowledge bases bridge rules (2), leaf contexts (3). directed
edges show dependencies bridge rules. system equilibria = (S1 , . . . , Sn )
Si = {aki , ti }, 1 k `.
compute one PE using DMCS DMCSOPT, one needs transfer packages 2`
PEs context parent (as context Ci computes subsets {a1i , . . . , a`i }).
intermediate context receives 2` results children, whose join leads 22` inputs
lsolve; invokes lsolve often returns 2` models parent,
wait this.
hand, DMCS-STREAMING needs transfer single PE pair
connected contexts, significant saving. Indeed, consider e.g. = 1, ` = 5, i.e.,
= (C1 , C2 , C3 ). Querying C1 package size k = 1 first causes query forwarded
C2 pair k1 = k2 = 1. C2 leaf context, invokes local solver eventually gets five
different models. However, returns one PE C1 , say (, {a12 }, ). Note t2 projected
among atoms C2 accessed C1 . happens C3 , assume
571

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

return (, , {a23 }) C1 . root context C1 , two PEs neighbors consistently
combined (, {a12 }, {a23 }). Feeding local solver, C1 obtains five models, returns
one them, say = ({a11 , t1 }, {a12 }, {a23 }).
following proposition shows correctness algorithm.
Proposition 8 Let = (C1 , . . . , Cn ) MCS, {1, . . . , n} let k 1 integer.
input (1, k) Ci .Handler, Ci .Output returns k different partial equilibria respect Ci ,
fact k least k partial equilibria exist.
5.2 Parallelized Streaming
one might expect, strategy ignoring k1 models collecting next k
likely effective. reason context uses one Solver,
general serve one parent, i.e., requests different ranges models size k.
new parent context requests models, refresh state Solver Joiner
redo scratch. unavoidable, unless context satisfies specific property one
parent call it.
way address problem parallelization. idea serve parent suite
Handler, Joiner, Solver Output. basic interaction units still shown
Figure 9, notable difference component runs individual thread.
significant change Solver control Joiner waits queue get new input
local solving process. Joiner independently queries neighbors, combines PEs
neighbors, puts results Solver queue.
effect waste recomputation time unused models. However, parallelization limits practice. DMCSOPT may run memory, unlimited parallel
instances streaming algorithm exceed number threads/processes operating
system support; happens contexts reach others many alternative paths, like
stacked diamond topology: number threads exponential number connected contexts, prohibits scaling large system sizes. However, real-world applications number
paths might still ok.
compromise two extremes bounded parallel algorithm. idea create
fixed-size pool multiple threads components share among contexts; incoming requests cannot served resources available, algorithm continues basic
streaming procedure. realization remains future work.

6. Experimental Evaluation
implemented algorithms using C ++ system prototype called DMCS,
available online.8 space reasons, omit detailed presentation refer work
Bairakdar, Dao-Tran, Eiter, Fink, Krennwallner (2010b), Dao-Tran (2014, ch. 7). Briefly,
main components global architecture (i) command-line frontend dmcs user
access system; (ii) demons daemon represent nodes contain (a set of) contexts;
(iii) manager dmcsm containing meta-information MCS (topology, interfaces)
8. http://www.kr.tuwien.ac.at/research/systems/dmcs,
https://github.com/DistributedMCS/dmcs/

572

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

C1

C2

C1

C3

C2

C3
C2

C1

C4

C2

C5

C3

C4

C6

C5

C3
C1

C6
C4

C4

C5

C6

C7

(a) Binary Tree (T)

C7

C7

(b) Diamond (D)

(c) Zig-Zag (Z)

C4

(d) Ring (R)

Figure 11: Topologies testing DMCS algorithms
helper dmcsgen generating configurations optimized components. Contexts implemented groups threads communicate concurrent message queues.
system two main command-line tools, viz. running algorihms test case generation, respectively. allows switch different algorithms modes simply changing
command-line arguments.
turn experimental evaluation DMCS various aspects. Next describe
benchmarks set up, go runs results interpretation.
6.1 Benchmark Setup
idea analyze strong weak points algorithm respect different parameters,
namely system topology, system size, local theory (i.e., knowledge base) size, interface size.
Specifically, considered MCSs topologies Figure 11, including:
Binary Tree (T): Binary trees grow balanced, i.e., every level except last one complete.
topology, edge needs removed form optimal topology; every
intermediate node cut-vertex, import interface query plan drastically reduced,
leading extreme performance improvement.
(Stack of) Diamond(s) (D): diamond consists four nodes connecting C1 C4 Figure 11b. stack diamonds combines multiple diamonds row, i.e., stacking diamonds
tower 3m + 1 contexts. Similar Binary Tree, edge removed constructing
query plan. W.r.t. topology, every context connecting two diamonds cut-vertex.
such, import interface query plan refined every diamond; avoids
significantly repetition partial PEs evaluation.
(Stack of) Zig-Zag Diamond(s) (Z): zig-zag diamond diamond connection
two middle contexts, depicted contexts C2 C4 Figure 11c. stack zigzag diamonds built above. topology interesting removing two edges per
block, query plan turns linear topology.
Ring (R): ring (Figure 11d). query plan removes connection context Cn C1
carries interface way back C1 . topology requires guess573

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

a1

a2

a3

a4

a5

a6

a7

a8

Figure 12: Local theory structure
ing checking DMCS algorithm; thus quite unpredictable algorithm
performs better general.
quantitative parameters represented tuple P = (n, s, b, r),
n system size (number contexts),
local theory size (number ground atoms local theory),
b number local atoms used bridge atoms contexts,
words, number interface atoms,
r maximal number bridge rules. generator generates bridge rule iterating
1 r 50% chance; hence average r/2 bridge rules generated. allow
bridge bodies size 1 2.
test configuration formulated X/(n, s, b, r) X {T, D, Z, R} represents topology n, s, b, r integers representing quantitative (i.e., size-related) parameters.
would like run several instances one configuration, final formulation test instance
Xi /(n, s, b, r), index test instance.
Inside context, local theories structured follows. Context Ci ground atoms
indicated ai,1 , . . . , ai,s . Rules form ai,j ai,k k = j + 1, j odd;
otherwise, randomly choose k j 1 j + 1 probability 50% possibility.
case k > rule exist. example context local theory size 8
illustrated dependency graph Figure 12. Here, bold arrows stand fixed
rules dashed arrows stands rules decided randomization. corresponding local
theory figure is:


a1 a2
a2 a1

a3 a4
a4 a3

a4 a5
a5 a6

a6 a7
a7 a8


.

setting, local context 2m answer sets, [0, s/2].
Furthermore, one obtain deterministic contexts (having one answer set) disallowing
cycles structure local theories.
6.2 Experiments
conducted experiments host system using 4-core Intel(R) Xeon(R) CPU 3.0GHz processor 16GB RAM, running Ubuntu Linux 12.04.1. Furthermore, used DLV [build BEN/Sep
28 2011 gcc 4.3.3] back-end ASP solver.
ran comprehensive set benchmarks setup described Section 6.1.
parameter space P = (n, s, b, r) huge, singled initial probing phase following
values experiments:
574

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

1000

DMCS
DMCSOPT

100

10

1

0.1

0.01
-5

T7,10,5,5
0

5

D7,10,5,5
10

Z7,10,5,5
15

R7,10,5,5
20

T10,10,5,5
25

D10,10,5,5
30

35

Z10,10,5,5
40

R10,10,5,5
45

50

Figure 13: DMCS vs. DMCSOPT non-streaming mode
system size n, depending topology:
T:
D:

n {7, 10, 15, 31, 70, 100}
n {4, 7, 10, 13, 25, 31}

Z:
R:

n {4, 7, 10, 13, 25, 31, 70}
n {4, 7, 10, 13, 70}

s, b, r fixed either 10, 5, 5 20, 10, 10, respectively.
combination topology X parameters P = (n, s, b, r) denoted X(n, s, b, r) X n,s,b,r
(used figures). parameter setting tested five instances. instance,
measured total running time total number returned partial equilibria DMCS,
DMCSOPT non-streaming streaming mode. latter mode, DMCS-STREAMING,
asked k answers, k {1, 10, 100}. parameter also influences size packages
transferred contexts (at k partial equilibria transferred one message).
streaming mode, asking one PE may require multiple rounds get answers,
interest see fast first answers arrive compared answers. thus compared
running time tasks k = 10 k = 100.
6.3 Observations Interpretations
Figures 13-17 summarize results experiments. Run times seconds timeout
600 seconds. data, several interesting properties observed. organize
analysis along following aspects: (1) comparing DMCS DMCSOPT, (2) comparing
streaming non-streaming mode, (3) effect package size, (4) role topologies,
(5) behavior algorithms deterministic contexts.
6.3.1 DMCS VS . DMCSOPT
Figure 13 shows running time DMCS DMCSOPT computing partial equilibria, i.e.,
non-streaming mode, five instances respective parameter settings. Clearly DMCSOPT
outperforms DMCS. explained fact computing answers, DMCS
always produces partial equilibria DMCSOPT, one PE returned DMCSOPT
575

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

100

100
DMCS-1st
DMCSOPT-1st
DMCS-10
DMCSOPT-10

DMCS-1st
DMCSOPT-1st
DMCS-100
DMCSOPT-100

10

10

1

1

0.1

T1

T2

T3

T4

T5

D1

(a) (25, 10, 5, 5)

D2

D3

D4

D5

(b) D(10, 10, 5, 5)

1000

100
DMCS-1st
DMCSOPT-1st
DMCS-10
DMCSOPT-10

DMCS-1st
DMCSOPT-1st
DMCS-10
DMCSOPT-10

100

10

10

1

1

0.1

0.1

Z1

Z2

Z3

Z4

Z5

R1

(c) Z(10, 10, 5, 5)

R2

R3

R4

R5

(d) R(4, 10, 5, 5)

Figure 14: DMCS vs. DMCSOPT streaming mode
obtained projecting many partial equilibria returned DMCS imported interface.
Furthermore, intermediate results transferred one message, makes difference
terms number communications algorithms. such, DMCS must spend
time processing possibly exponentially input; hence, unsurprisingly, consistently
slower DMCSOPT.
However, observation streaming mode different. Figure 14 shows running time
DMCS DMCSOPT streaming mode compute first 100 respectively 10 unique partial
equilibria (25, 10, 5, 5) respectively D(10, 10, 5, 5), Z(10, 10, 5, 5) R(4, 10, 5, 5).
first view, DMCSOPT consistently slower DMCS, one might question correctness
results. However, surprise: PE returned DMCSOPT correspond several PEs returned DMCS. Hence, batch first k unique answers DMCS
corresponds smaller number (few) unique answers DMCSOPT.
Therefore, comparing DMCS DMCSOPT streaming mode measuring runtime
compute first k answers fair. thus took time algorithms finished first
round answers (denoted DMCS-1st DMCSOPT-1st Figure 14). setting,
observed following:
majority cases DMCSOPT finishes first round faster DMCS, however
40% instances, way around; shows effect using query plan;
576

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

however, cases DMCS wins. explained follows. First all, streaming
mode, transfer packages k partial equilibria time; therefore, effect reducing
amount total work done always apply non-streaming mode. Furthermore,
every context, compute k PEs project output interface returning
results. According strategy, context Ci returns k1 partial equilibria non-streaming
mode k2 partial equilibria streaming another context Cj , might happen k2 much
smaller k1 hence provide enough input Cj compute k PEs. Therefore, Cj
issue requests Ci asking packages k PEs, e.g., [k + 1, 2k], [2k + 1, 3k],
etc; costs DMCSOPT time even compute first batch PEs root context.
Another approach compute always k unique partial equilibria returning parent
context. However, strategy risks compute even local models k unique partial
equilibria found.
Overall, much difference running time DMCSOPT slower DMCS, except
instance R3 (Figure 14d). however comes different reason: cyclic topology
guess-and-check effects, play much important role choosing DMCS
DMCSOPT (see Section 6.3.4).
6.3.2 TREAMING VS . N - STREAMING DMCS
compare streaming non-streaming algorithm (DMCS resp. DMCSOPT).
Figure 15 shows results DMCS (15a), results DMCSOPT compute
first 10 resp. 100 PEs small systems/local knowledge bases (15b) large systems/local theories (15c). Excluding Ring (which behaves abnormally due guess-and-check)
one see that:
DMCS, streaming mode definitely worth pursuing since DMCS non-streaming
mode times many cases (see also Figure 13), streaming mode still could find
answers reasonable time.
DMCSOPT, situation bit different, streaming loses non-streaming
small instances. due recomputation streaming mode pays transferring
chunks partial equilibria contexts; furthermore, duplications answers.
one moves larger systems local knowledge bases, streaming mode starts gaining
back. However, always win, recomputation still significantly takes time cases.
Summing up, system small enough, one try non-streaming mode
avoids recomputation duplication PEs different rounds computation. large
systems, streaming rescue us timing out. Even pay recomputation, still
helps cases results needed, e.g. brave query answering (membership
query PE).
6.3.3 E FFECTS PACKAGE IZE TREAMING ODE
considerations raise question optimal number PEs transferred
return messages contexts. analyze experimental results streaming
mode package sizes 1, 10, 100 give hints this.
Figure 16 shows average time compute 1 PE DMCSOPT streaming mode
respect three package sizes. One see transferring single PE get first answer
577

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

1000

Non-Streaming
Streaming-10
Streaming-100

100

10

1

0.1

0.01
-5

T10,10,5,5

0

5

D10,10,5,5
10

15

Z10,10,5,5

20

R4,10,5,5
25

30

(a) DMCS
1000

Non-Streaming
Streaming-10
Streaming-100

100

10

1

0.1

0.01
-5

T10,10,5,5

0

5

D10,10,5,5
10

15

Z10,10,5,5

20

R4,10,5,5
25

30

(b) DMCSOPT small systems local theories
1000

Non-Streaming
Streaming-10
Streaming-100

100

10

1

0.1

0.01
-5

0

T31,20,10,10

5

D10,20,10,10
10

15

Z10,20,10,10

20

R4,20,10,10
25

30

(c) DMCSOPT large systems local theories

Figure 15: Non-streaming vs. streaming DMCS DMCSOPT
acceptable cases, particular guessing needed. Moving size 1 small
package size like 10 sometimes better, one save communication time (sending
package 10 partial equilibria vs. sending ten times package single PE). setting
(small package sizes like 10) effective communication big factor,
578

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

1000

Streaming-1
Streaming-10
Streaming-100

100
10
1
0.1
0.01
0.001
-5

0

T100,20,10,10

5

D25,20,10,10

10

Z70,20,10,10
15

R4,20,10,10
20

25

Figure 16: Average time DMCSOPT find one partial equilibrium streaming mode, varying
package size
happens real applications contexts located physically distributed nodes.
cases, computing 10 partial equilibria faster computing 1 PE 10 consecutive times.
Furthermore, package size 1 safe cases guessing applied, e.g.,
R3 (4, 20, 10, 10). cases, large enough package size might help cover correct
guess; general, guarantee coverage. thoroughly solve problem,
one needs apply conflict learning whole MCS evaluation.
Also, interesting see package size 100, DMCSOPT usually times out.
reason many duplications DMCSOPT stuck local search branch
promises fewer 100 partial equilibria, algorithm lose time without finding
new unique answers eventually time out.
find good package size p specific setting (topology, system size, local theory size),
one may run system training set apply binary search p.
6.3.4 E FFECT OPOLOGY
quick glance plots Figures 1316 reveals pattern algorithms, especially
optimizations, perform better tree zigzag diamond, depending DMCS
DMCSOPT, worst ring.
system topology plays important role here. aspects affect performance
algorithms (i) number connections, (ii) structure block trees cut vertices,
(iii) acyclicity vs. cyclicity.
Regarding (i), topology introduces number connections based system size. Tree
fewer connections Diamond Zigzag, reduces communication also
local solving time fewer requests made; performance DMCS topologies
proves observation. one follows argument, Ring must offer best performance.
However, actually case due aspect (iii) shortly analyze below.
Concerning (ii), tree ultimately optimized every intermediate node cut vertex.
Hence, applying query plan DMCSOPT, strip beliefs PEs sent
child contexts parent context. words, local beliefs context Ci needed
transferred back parents. drastically decreases amount information
579

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

1000
DMCS-100
DMCSOPT-100

100

10

1
-2

0

R4,10,5,5
2

4

6

R7,10,5,5
8

10

12

R4,20,10,10
14

16

18

Figure 17: DMCS vs. DMCSOPT streaming mode package size 100 ring
communicated, importantly, number calls lsolve. Due special property,
DMCSOPT performs extremely well tree topology, scales hundreds contexts.
Comparing Diamond Zigzag, number cut vertices. However, Zigzag
converted linear topology optimal query plan (cf. Figure 11c), therefore
processed much faster Diamond. Figure 16, DMCSOPT scales Zigzag 70 contexts
average time compute one answer still better one diamond 25 contexts.
Regarding (iii), Ring cyclic topology topologies acyclic. Hence
algorithms must guess-and-check context topology. Making
right guess important, even important reducing communication calls local
solvers. result running DMCS DMCSOPT topology (Figure 17) follow
pattern; absolutely depends specific instance whether sequential guessing
luckily arrives result. Therefore, frequently see DMCS outperforms DMCSOPT
streaming mode, cases, guessing root context (after detecting cycle)
effective guessing parent root context according optimal query plan.
Based observations, one come best strategy evaluate different types
topologies. dealing MCSs arbitrary topologies, looks natural decompose
parts familiar topologies efficient strategies known, combine
strategies overall evaluation method. Studying beyond scope work
interesting issue future research.
6.3.5 B EHAVIOR ETERMINISTIC C ONTEXTS
considered algorithms MCSs consisting possibly non-deterministic contexts,
i.e., one acceptable belief set per knowledge base. intriguing see
algorithms behave contexts always exactly one accepted belief set per knowledge base;
might underlying logic genuinely deterministic accepted belief set
clear (e.g., closure classical logic) among multiple candidates particular belief set chosen
(in implementations typically first best solution computed, e.g. SAT solving ASP).
observed that:
non-cyclic topologies, performance difference DMCS DMCSOPT,
smaller interface used DMCSOPT reduce number intermediate PEs
transferred contexts, one partial equilibrium computed every context.
580

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

1000

DMCS
DMCSOPT
MCS-IE

100

10

1

0.1

0.01
-5

T7,10,5,5
0

5

D7,10,5,5
10

Z7,10,5,5
15

R7,10,5,5
20

T10,10,5,5
25

D10,10,5,5
30

35

Z10,10,5,5
40

R10,10,5,5
45

50

Figure 18: DMCS vs. DMCSOPT streaming mode package size 100 ring
cyclic topology (Ring), guessing plays main role. Hence depends individual
instance whether DMCS DMCSOPT wins, like case non-deterministic contexts (cf.
Section 6.3.4).
non-streaming mode much faster streaming (on DMCS DMCSOPT);
reasonable request partial equilibria redundant.
6.3.6 C OMPARISON MCS-IE P2P-DR
Systems close DMCS MCS-IE (Bogl et al., 2010)9 P2P-DR (Bikakis, Antoniou, & Hassapis, 2011). former plugin dlvhex system originally developed compute
explanations inconsistency Multi-context Systems, also includes mode computing
equilibria MCS. However, MCS-IE implemented centralized approach. Figure 18
presents run time DMCS, DMCSOPT comparison MCS-IE computing partial equilibria respective configurations. shows MCS-IE outperforms DMCS since
inherits powerful decomposition technique dlvhex; however, decomposition based
topological information DMCSOPT turns efficient, also localizes interface beliefs communicate blocks contexts, specific MCS
exploited general decomposition technique dlvhex.
P2P-DR supports distributed query answering multi-context systems based defeasible
logic; details, see Section 7. present comparison DMCS P2P-DR.
converted benchmark P2P-DRs style converting local knowledge bases
bridge rules defeasible local meta rules, added fixed trust order contexts.
queried root context atom appearing one answers DMCS-STREAMING
package size 10. turned P2P-DR always found answers around 0.25 seconds,
regardless tested instance. behavior explained follows. find answers
query atom, algorithm P2P-DR first evaluates local theory. determine truth
value query, terminates; otherwise algorithm consults neighbors get evidence
reasoning. local knowledge base structure, converted P2P-DRs defeasible
theories, allows local decision, system works local theory root context
9. http://www.kr.tuwien.ac.at/research/systems/mcsie/tut/

581

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

every test case, thus results almost constant execution time. Even asking neighbours
necessary, P2P-DR general may much faster DMCS, query answering process
inherently deterministic low-complexity logic; turn, formalism less expressive.
detailed study issue remains future work.
6.3.7 UMMARY
Summing up, analysis experimental results shows clear winner among
algorithms (DMCS vs. DMCSOPT) different running modes (streaming vs. non-streaming,
different package size) different topologies. distill guideline choose
setup fits specific instances practice, including issues open investigation,
briefly stated follows:
choose DMCSOPT DMCS non-streaming mode, except cyclic topologies;
streaming mode, choose appropriate package size carefully (e.g., binary search
training instances;
decompose random topologies parts whose topologies effective strategies evaluate,
study combine strategies systems.

7. Related Work
section, resume discussion related work. Starting multi-context systems,
provide details work Roelofsen et al. (2004), Bikakis et al. (2011) consider
work. move related formalisms SAT, CSP ASP.
Roelofsen et al. (2004) described evaluation monotone MCS classical theories using
SAT solvers contexts parallel. used (co-inductive) fixpoint strategy check MCS
satisfiability, centralized process iteratively combines results SAT solvers. Apart
truly distributed, extension nonmonotonic MCS non-obvious; furthermore,
caching technique used.
Serafini, Borgida, Tamilin (2005) Serafini Tamilin (2005) developed distributed
tableaux algorithms reasoning distributed ontologies, regarded multi-context
systems special bridge rules. algorithms serve decide whether system consistent, provided cyclic context dependencies exist (in technical terms, distributed TBox
acyclic); DRAGO system (Serafini & Tamilin, 2005) implements approach OWL ontologies. Compared ours, work tailored specific class multi-context systems resp.
knowledge bases, without nonmonotonic negation cyclic dependencies (which challenging); furthermore, targets query answering rather model building, sense dual
problem.
related work regards distributed evaluation system P2P-DR Bikakis
et al. (2011). developed distributed algorithm query evaluation multi-context system
framework specifically based (propositional) defeasible logic. framework, contexts built using defeasible rules exchange literals via bridge rules, trust order
contexts supplied. knowledge base context has, terminology, single
accepted belief set contains literals concluded; global system semantics given
terms (unique) three-valued assignment literals, determined using algorithm: whether literal l provably (not) logical conclusion system, whether remains
582

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

open. Apart tailored particular logic preference mechanisms evaluating interlinked contexts, applying algorithm model building straightforward; particular,
produces unique belief sets, dealing nondeterminism multiple equilibria possible.
work computing equilibria distributed multi-context systems clearly related
work solving constraint satisfaction problems (CSP) SAT solving distributed setting;
Yokoo Hirayama (2000) survey algorithms distributed CSP solving, usually developed setting node (agent) holds exactly one variable, constraints
binary, communication done via messages, every node holds constraints
involved. also adopted later works (Gao, Sun, & Zhang, 2007) generalized
(Yokoo & Hirayama, 2000). relation topology-based optimization techniques Section 4,
biconnected components used Baget Tognetti (2001) decompose CSP problems.
decomposition used localize computation single solution components undirected constraint graphs. Along lines, approach based directed dependencies,
allows us use query plan MCS evaluation.
predominant solution methods CSP backtracking algorithms. Bessiere, Bouyakhf,
Mechqrane, Wahbi (2011) took step backtracking dynamic total ordering agents guided nogoods. approach, however, allows cyclic dependency
contexts. Hirayama Yokoo (2005) presented suite algorithms solving distributed SAT (DisSAT), based random assignment improvement flips reduce conflicts.
However, algorithms geared towards finding single model, extension streaming
multiple (or all) models straightforward; works distributed CSP SAT,
similar.
Finally, (distributed) SAT CSP solving concerns monotonic systems (removal clauses
resp. constraints preserves satisfiability), MCSs evaluation concerns nonmonotonic systems,
even contexts monotonic (e.g., clause sets); makes efficient evaluation difficult,
important structural properties search space cannot exploited.
Adjiman, Chatalic, Goasdoue, Rousset, Simon (2006) present framework peer-to-peer
inference systems, local theories propositional clause sets share atoms special algorithm consequence finding available. pursue dual problem model building,
applying needs straightforward; furthermore, dealing non-monotonic
systems, peer-to-peer systems Adjiman et al. monotonic.
Moving ASP, Pontelli, Son, Nguyens (2011) ASP-PROLOG shares MCS idea
integrating several knowledge bases, called modules, possibly different semantics. However, restricted module semantics ASP Prolog (that is, least Herbrand model),
ASP-PROLOG pursues query answering instead model building.
streaming, answer set streaming algorithm HEX-programs (which generalize ASP
external information access) given Eiter, Fink, Ianni, Krennwallner, Schuller
(2011). Despite similarities Algorithm DMCS-STREAMING, rather different: monolithic programs syntactically decomposed modules answer sets computed modular
fashion; fully distributed combines partial models lower components input
upper components straightforwardly; moreover, may use exponential space components.
583

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

8. Conclusion
considered distributed evaluation Multi-context Systems (MCSs) introduced
Brewka Eiter (2007) general formalism interlink possibly nonmonotonic heterogeneous knowledge bases. presented suite generic algorithms compute equilibria, i.e., semantics MCS fully distributed manner, using local solvers knowledge
bases contexts. contains basic algorithm DMCS, advanced version DMCSOPT
uses topology-based optimizations, streaming variant DMCS-STREAMING computing
partial equilibria gradually. believe underlying principles techniques might
exploited related contexts, particular distributed evaluation non-monotonic
knowledge base formalisms.
algorithms implemented prototype system available open source.8
top implementation, conducted comprehensive experiments compare performance algorithms gave insight analysis results. points advantages,
disadvantages well time/memory trade algorithms different situations
depending parameters system topology, local interface theory size, number
equilibria desired user. Based this, user choose setting (algorithm mode)
fits need best finding (partial) equilibria MCS. extensive treatment given
Dao-Tran (2014).
work open issues. Several issues remain investigation. One improvement algorithms. Here, experimental results Ring topology strongly suggest
incorporate conflict learning, proved valuable ASP SAT solving, DMCS
DMCSOPT; expect cyclic topologies benefit better guided guessing process. Another issue concerns semantics variants MCSs. former, grounded
equilibria considered Dao-Tran (2014), akin answer sets logic programs
applicable MCSs satisfy certain algebraic conditions; characterized like answer
sets using (adapted) loop formula approach (Lee & Lifschitz, 2003). Dealing supported
equilibria (Tasharrofi & Ternovska, 2014), however, open.
Regarding MCS variants, managed MCSs (Brewka et al., 2011) generalize bridge rules derive
operations (commands) management function applied knowledge bases; seems
possible generalize algorithms setting, efficient realization straightforward. Another generalization MCS concerns dynamic data: areas like sensor networks, social
networks, smart city applications, data may change even continuously arrive nodes,
motivates reactive stream processing MCSs (Goncalves et al., 2014; Brewka et al., 2014).
Last least, allowing contexts evolve via interation users changes environment valuable extention. Extending algorithms settings interesting
challenging.
Finally, extending work query answering MCSs, user poses query
context receives results derived (partial) equilibria another natural issue.
need building whole equilibria, better performance may achieved.

Acknowledgments
research supported Austrian Science Fund (FWF) projects P20841
P26471.
584

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

C1000

C1

C2

C2000

C3

C1

C3000

Figure 19: Introducing guess context(s)
thank reviewers pointing corrections constructive suggestions
helped improve presentation work, thank Antonis Bikakis providing us
P2P-DR system experimental comparison.

Appendix A. Proofs
Proof Theorem 1
prove theorem, first prove following Lemmas 9 10. latter aims simplifying
proof cyclic case, based notion converting cyclic MCSs acyclic ones.
Lemma 9 context Ck partial belief state MCS = (C1 , . . . , Cn ),
app(brk , S) = app(brk , S|V ) VB V V (k).
Proof r app(brk , S), (ci : pi ) B + (r) : pi Sci
(cj : pj ) B (r) : pj
/ Scj . need show pi Sci |Vci pj
/ Scj |Vcj . Indeed:
V VB Vcj VBj Scj |Vcj Scj . Therefore, pj
/ Scj pj
/ Scj |Vcj .
Now, assume pi
/ Sci |Vci . fact pi Sci , follows pi
/ Vci , hence

pi
/ V (k). contradiction fact pi occurs bridge rule body.
Therefore, r app(brk , S|V ).

next Lemma 10 based following notions convert cyclic MCSs acyclic
ones show corresponding equilibria. intuition (illustrated Figure 19
Examples 18, 19) introduce additional context Ck take care guessing every cycle
breaker Ck . Then, bridge rules Ck parents modified point Ck .
formally realize idea starting function ren renames part bridge rules.
Definition 12 Let Ck context MCS , let V interface running DMCS.
renaming function ren defined follows:

atom a: ren(a, k, V) =

ag


Bk V
otherwise


context index c: ren(c, k, V) =

c
c

c {1, . . . , n}
otherwise

bridge atom (ci : pi ): ren((ci : pi ), k, V) = (ren(ci , k, V) : ren(pi , k, V))

585

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

bridge body B = {(c1 : p1 ) . . . (cj : pj )}:
ren(B, k, V) = {ren((ci : pi ), k, V) | (ci : pi ) B}
bridge rule r = head (r) B(r):
ren(r, k, V) = head (r) ren(B(r), k, V)
set bridge rules br : ren(br , k, V) = {ren(r, k, V) | r br }
context Ci = (Li , kb , br ) : ren(Ci , k, V) = (Li , kb , ren(bri , k, V)).
Example 18 Let us slightly modify MCS = (C1 , C2 , C3 ) Example 8 follows:
kb 1 = {e e}, br 1 = {a (1 : e), (2 : b)};
kb 2 = , br 2 = {b (3 : c)};
kb 3 = , br 3 = {c (1 : a)}.
Applying function ren contexts C1 C3 results following bridge rules wrt. interface V = {a, b, c, e}:
ren(br 1 , 1, V) = {a (1 : eg ), (2 : b)},
ren(br 3 , 1, V) = {c (1 : ag )}.
two contexts Ci Cj , former called parent latter respect interface
V, denoted parent(Ci , Cj , V) iff exists bridge rule r br exists (c : p)
B(r) p Bj V.
set contexts {Cc1 , Cc2 , . . . , Cc` } MCS called cycle w.r.t. interface V iff
^
parent(Cc` , C1 , V)
parent(Cci , Cci+1 , V)
1i`1

holds. One pick arbitrary context set cycle-breaker. Given MCS ,
several ways choose (finite) set contexts cycle-breakers. Algorithm DMCS,
Step (d) practically establishes cycle-breakers based order elements In(k)
iterated. next definition, interested particular set cycle-breakers.
Definition 13 Given MCS = (C1 , . . . , Cn ), let CB rM = {Cc1 , . . . , Ccj } set cyclebreakers based application DMCS starting context Cr . conversion
equal acyclic ? based CB rM interface V done follows:

ren(Ci , i, V) Ci CB rM
0
0
Let Ci = (Li , kb , br ) =
Ci
otherwise
Let Ci00 = (Li , kb , br 00i ) = Ck CBM ren(Ci0 , k, V)10
00
br {a (i : ag ) | Bi V}
000
Let Ci000 = (Li , kb , br 000
)

br
=


br 00i

Ci CB rM
otherwise

Cj CB rM , introduce Cj = (Lj , kb j , br j ) br j = kb j = {ag ag |
Bj V}. ? = (C1000 , . . . , Cn000 , Cc1 , . . . , Ccj ).
10. order composing function ren different parameters k matter here.

586

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Example 19 (contd) Let MCS Example 18 CB rM = {C1 }. Then, conversion Definition 13 gives ? = (C1000 , C2000 , C3000 , C1 ), where:
g
kb 1 = {e e}, br 000
1 = {a (1 : e ), (2 : b).

(1 : ag ).

e (1 : eg ).};

kb 2 = , br 000
2 = {b (3 : c)};
g
kb 3 = , br 000
3 = {c (1 : )};

kb 1 = {eg eg .

ag ag .}, br 1 = .

Lemma 10 Let MCS ? conversion acyclic MCS Definition 13.
equilibria ? 1-1 correspondence.
Proof (Sketch) Let (R1 ) (R2 ) runs DMCS ? , respectively. Due
selection CB rM construct ? , (R1 ) (R2 ) order visiting contexts,
except (R1 ) revisits cycle-breaker Ck CB rM , counterpart (R2 ) visits Ck .
corresponding locations:
(R1 ) calls guess(V, Ck ) Step (c),
(R2 ) calls lsolve({, . . . , }) Step (e) since Ck leaf context.
construction local knowledge base Ck gives us exactly guess Ck . Furthermore,
guesses passed parent contexts Ck later unified additional
bridge rules (k : ag ) introduced br 000
k . Therefore, belief combinations (Step (d)) done
Ck executed input runs (R1 ) (R2 ). correspondence equilibria
hence follows.

Proof (Theorem 1) Thanks Lemma 10, need prove Theorem 1 acyclic
case automatically get result cyclic case.
() start showing soundness DMCS. Let 0 Ck .DMCS(V, ) V V (k).
show partial equilibrium acyclic w.r.t. Ck 0 = S|V .
proceed structural induction topology .
Base case: Ck leaf In(k) = brk = k
/ hist. means (d)
executed, hence, (e), lsolve runs exactly (, . . . , ), get result set belief
states = lsolve((, . . . , )) = {(, . . . , , Tk , , . . . , ) | Tk ACCk (kbk )}. show
0 S|V . Towards contradiction, assume partial equilibrium = (S1 , . . . , Sn )
w.r.t. Ck 0 = S|V . In(k) = , get IC (k) = {k}, thus partial belief
state (, . . . , , Tk , , . . . , ) (where Tk ACCk (kbk )) partial equilibrium w.r.t. Ck .
Contradiction.
Induction step: assume context Ck import neighborhood In(k) = {i1 , . . . , im }
i1 = Ci1 .DMCS(V, hist {k}),
..
.
im = Cim .DMCS(V, hist {k}).
587

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

induction hypothesis, every 0ij ij , exists partial equilibrium ij
w.r.t. Cij ij |V = 0ij .
Let = Ck .DMCS(V, hist). need show every 0 S, partial equilibrium w.r.t. Ck 0 = S|V . Indeed, since In(k) 6= , Step (d) executed; let
= i1 ./ ./ im
result combining partial belief states
Scalling DMCS Ci1 , . . . , Cim . Furthermore,
?
?
Step (e), = |V = {lsolve(S) | }. Eventually, 0 S|V .
Since every DMCS Ci1 , . . . , Cim returns partial equilibria w.r.t. Cij projected V,
every partial equilibrium w.r.t. Cij projected V. acyclic visited
contexts In(k), thus Lemma 9 get every , app(brk , ) gives us
applicable bridge rules r regardless Tj = , j
/ In(k). Hence, , lsolve(T )
returns partial belief states, component projected V except kth component.
every preserves applicability rules Lemma 9, get every 0 S|V ,
exists partial equilibrium w.r.t. Ck 0 = S|V .
() give proof completeness DMCS structural induction topology
acyclic . Let = (S1 , . . . , Sn ) partial equilibrium w.r.t. Ck let 0 = S|V .
show 0 Ck .DMCS(V, ).
Base case: Ck leaf context. Then, executing Ck .DMCS(V, ), Step (d) ignored
Step (e) called input (, . . . , ), lsolve((, . . . , )) gives us belief sets Ck .
equilibrium w.r.t. Ck , S; hence, 0 = S|V returned Ck .DMCS(V, ).
Induction case: suppose import neighborhood context Ck In(k) = {i1 , . . . , im }. Let
restriction every context Cij In(k) denoted ij , where:

S` ` IC (ij )
0
0
0
ij
= (S1 , . . . , Sn ) S` =

otherwise
Informally speaking, restriction keeps belief sets contexts reachable Cij
sets non-reachable contexts . induction hypothesis, ij |V computed
Cij .DMCS(V, ) ij In(k). show S|V computed Ck .DMCS(V, ).
Indeed, considering acyclic , holds ij |V also returned call
Cij .DMCS(V, {k}), k plays role calls Cij neighbors. means
step (d), contains = Si1 ./ . . . ./ Sim Sij appears position ij S.
Since partial equilibrium w.r.t. Ck , Sk ACCk (kbk {head (r) |
r app(brk , S)}). Furthermore, choosing V V (k), Lemma 9 tells us applicability
bridge rules preserved projection belief sets V. gives us Sk lsolve(T )
step (e), hence 0 = S|V returned Ck .DMCS(V, ).

Proof Proposition 4
(1) context Ck , let number calls local solver denoted c(k). number
calculated computation Step (d), bounded maximal number
combined partial belief sets neighbors. Formally speaking:
c(k) iIn(k) 2|VBi | 2|In(k)||V| 2n|V| .
588

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Hence whole MCS, upper bound calls lsolve run DMCS
c = 1kn c(k) n 2n|V|
(2) context Ck MCS = (C1 , . . . , Cn ), set E(k) contains dependencies
contexts Ci IC (k). visit (i, j) E(k) exactly twice DFS-traversal :
calling Cj .DMCS(V, hist) Ci , retrieving S|V Cj Ci . Furthermore,
caching technique Step (a) prevents recomputation already visited nodes, thus prevents
recommunication subtree visited node. claim hence follows.

Proof Proposition 5
Item (i) trivial see since CycleBreaker applied Algorithm 4. prove item (ii), let us look
two cases edge (`, t) removed original topology Step (a) Algorithm 3:
(`, t) removed CycleBreaker: causes certain nodes graph cannot reach
via `. However, interface Ct provides already attached v(i, j) via V (cp )|Bt .
(`, t) removed transitive reduction: change reachability
nodes; therefore, interface Ct provides already included V (i, j)B 0 .


argument gives us property (ii).
Proof Proposition 6
First, estimate complexity compute v(i, j) loop (a).
[
[
v(i, j) := V (i, j)B 0
V (cp )|Bc
V (cp )|Bt
cC 0

(`,t)E

one hand, refined recursive import V (i, j)0B defined (Definition 9):
V (i, j)0B = {V (i)

[

B` }

`B 0 |j

B 0 |j contains nodes reachable j.
hand, since sets possible beliefs different contexts disjoint,

[
cC 0

V (cp )|Bc

[

V (cp )|Bt = V (cp )|ScC0 Bc S(`,t)E Bt

(`,t)E


Since recursive import interface node k defined V (k) = iIC (k) V(i),
expression compute v(i, j) end combination set intersection, union, projection.
implementation sets using hash set, is, look takes O(1), operators
implemented linear time. Therefore, v(i, j) computed linear time total number
beliefs contexts system.
Given GM , block tree graph (GM ) constructed linear time (Vats & Moura,
2010). Ear-decomposition (Step (c)) also done linear time (Valdes, Tarjan, & Lawler,
589

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

1982). Transitive reduction (Step (d)) computed quadratic time respect number
edges block.
OptimizeTree(T (GM ), k, k) iterates blocks. Assume blocks B1 . . . ,
Bm , Bi contains ni edges, n =
i=1 ni total number edges original
graph. Let ti time process block Bi . bound total processing time
assessed follows:



X
X
X
2
t=
ti
ni (
ni )2 = n2 .
i=1

i=1

i=1

Therefore, ignore loop (a), OptimizeTree done quadratic time size
original input, i.e., size GM .

Proof Theorem 7
prove this, need Proposition 11 claim partial equilibria returned DMCS
DMCSOPT correspondence. first, need following supportive notion.

Definition 14 Let Ck context MCS , let k query plan Definition 11.
block B k , block interface B, whose root vertex cB ,
VB = {p v(i, j) | (i, j) E(B)} BcB .
Let Ci context B. self-recursive import interface Ci B
[
V (i)B = Bi
V (i, `)B .
(i,`)E(k )

Proposition 11 Let Ck context MCS
, let k query plan Definition 11
Ck belongs block B k let V = Bk VB . Then,
(i) 0 DMCSOPT(k) called Cc (c, k) E(k ) c = k, exists
partial equilibrium Ck .DMCS(V, ) 0 = S|V (c,k)B (c, k) E(k )
0 = S|V (k)B c = k;
(ii) Ck .DMCS(V, ), exists DMCSOPT(k) called Cc
0 = S|V (c,k)B (c, k) E(k ) 0 = S|V (k)B c = k.
detailed proof Proposition 11 given next section, give proof Theorem 7.
Proof (Theorem 7) (i) Let 0 Ck .DMCSOPT(k) result DMCSOPT. Proposi00
0
00
tion 11 (i)
Sc = k, exists Ck .DMCS(V, ) = |V (k)B ,
choose V = Bk VB . Note V (k) V V collects bridge atoms blocks,
might contain blocks reachable k. Theorem 1, exists partial equilibrium
00 = S|V . Thus,
0 = (S|V )|V (k)B
= S|V (k)B
V (k)B V
b V (k)B
= S|Vb
V
(ii) Let partial equilibrium MS. Theorem 1, exists 00 Ck .DMCS(V, )
00 = S|V choose V = Bk VB . above, V (k) V. Proposition 11 (ii)
c = k, exists 0 Ck .DMCSOPT(k) 0 = 00 |V (k)B . above,
0 = S|Vb .

590

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Proof Proposition 11
support proof Proposition 11, need following lemmas.
Lemma 12 Assume context Ck import neighborhood In(k) = {i1 , . . . , im }, (k, ij ) removed original topology OptimizeBlock(B, cB ),
0i1

= DMCSOPT(k) Ci1
..
.

i1

= Ci1 .DMCS(VB , )
..
.

0im

= DMCSOPT(k) Cim

im

= Cim .DMCS(VB , )

every partial equilibrium 0 0ij , exists ij 0 = S|V (k,ij )B .
Let 0 = 0i1 ./ . . . ./ 0im = i1 ./ . . . ./ SSim . Then, 0 0 , exists
2
V (k, ij )B .
0 = |Vinput (1,m) Vinput (`1 , `2 ) = `j=`
1
Proof prove induction number neighbors In(k).
Base case: In(k) = {i}, claim trivially holds.
Induction case: In(k) = {i1 , . . . , i` }, U 0 = 0i1 ./ . . . ./ 0i`1 , U = i1 ./ . . . ./ i`1 ,
U 0 U 0 , exists U U U 0 = U |Vinput(1,`1) . need show
0 U 0 ./ 0i` , exists U ./ i` 0 = |Vinput (1,`) .
Assume opposite holds, i.e., exists = U 0 ./ 0 U 0 U 0 0 0i` ,
U U, i` U 0 = U |Vinput (1,`1) 0 = S|V (k,i` )B ,
U ./ void.
means exists context Ct reachable Ck two different ways, one via i`
via one i1 , . . . , i`1 Ut 6= , St 6= , Ut 6= St , either
(i) Ut0 = St0 = ,
(ii) Ut0 = St0 6=
Case (i) cannot happen Ct reachable Ck , hence Vinput (1, ` 1) Bt 6=
V (k, i` ) Bt 6= .
Concerning case (ii), Ut |Vinput (1,`1) = St |V (k,i` ) 6= , hence exists
Ut \ Ut |Vinput (1,`1)
/ St |V (k,i` ) . means Vinput (1, ` 1) Bt 6= V (k, i` ) Bt .


However, Definition 9 recursive import interface, V (k, ix )B = V (k)
`B|k B` , B|ix contains nodes B reachable ix . follows V (k, i` )

V (k, ij ) 1 j ` 1 reaches t, share projection Bt , hence Vinput (1, `
1) Bt = V (k, i` ) Bt .
reach contradiction, therefore Lemma 12 proved.


Lemma 13 join operator ./ following properties, given arbitrary belief states S, , U
size: (i) ./ = (ii) ./ = ./ (iii) ./ (T ./ U ) = (S ./ ) ./ U .
properties also hold sets belief states.
Proof first two properties trivial prove. prove associativity.
Let R = ./ (T ./ U ) W = (S ./ ) ./ U . Consider joins left right.
position (1 n), Ri Wi determined locally comparing Si , Ti Ui .
591

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Si =




Ti =


N

Ui =

N




N

N

N





N



N

N

N



N

N

N

Si = Ti


N
N
N
N
N
N

N


N
N

Ti = Ui

N
N

N

N
N
N
N

N

N

Ui = Si

N

N
N
N

N
N
N

N
N
N

Ri

Ui
Ti
Ti
void
Si
Si
void
Si
void
Si
void
void
void

Wi

Ui
Ti
Ti
void
Si
Si
void
Si
void
Si
void
void
void

Table 1: Possible cases joining position

reach inconsistency, process terminates void returned; otherwise, conclude value
Ri , Wi continue next position. final join returned position n processed
without inconsistency.
possible combination Si , Ti , Wi shown Table 1. One see always
outcome Ri Wi . Therefore, end either R = W
void . concludes join operator ./ commutative.

Lemma 14 Let Ci Cj two contexts block executing
OptimizeTree directed path Ci Cj . Suppose = DMCSOPT(k) Ci
j = DMCSOPT(k) Cj . = ./ j .

Proof use cache DMCSOPT change result disregarded, i.e.,
assume without loss generality cache(k) = DMCSOPT. Indeed, cache(k) filled
result computation empty (i.e., Ck accessed first time),
never changed DMCSOPT returns cache(k), i.e., value computation
empty cache(k).
assumption, Lemma 14 proven taking path Ci = Cp1 , . . . , Cph =
Cj connects Ci Cj , arguing index ` {1, . . . , h}, holds p` = p` ./
j (?). Indeed, show induction path.
Base case: ` = h, statement (?) holds ph ./ j = j ./ j = j identity
(Lemma (13), (i)).
Induction case: consider ` < h, suppose already established induction hypothesis
p`+1 = p`+1 ./ j .
592

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

definition p` DMCSOPT, holds p` = lsolve(T )11 is,
statements (b) (c), form = p`+1 ./ 0 ; holds edge (p` , p`+1 )
E, ./ commutative associative (Lemma (13), (ii) (iii)). induction
hypothesis, get
= p`+1 ./ 0 = (S p`+1 ./ j ) ./ 0 = j ./ (S p`+1 ./ 0 ),
is, form j ./ 00 .
Next, lsolve(T ) change value component interpretation
defined j ; is, lsolve(T ) ./ j = lsolve(T ). means p` = lsolve(T ) = lsolve(T ) ./
j = p` ./ j , proves statement (?) holds `.
Eventually, get ` = 1 = p1 = p1 ./ j = ./ j .

Based Lemma 14, following result.
Lemma 15 Assume import neighborhood context Ck In(k) = {i1 , . . . , im },
ij = DMCSOPT(k) Cij , 1 j m. Furthermore, suppose edge (k, ij ) removed
optimization process (1 j m), Ci` neighbor Ck exists path
k ij i` optimized topology. i` = i` ./ ij ; words, input
DMCSOPT Ck affected removal (k, ij ).
Proof Since Cij Ci` direct children Ck , follows belong block.
Therefore, Lemma 14 i` = i` ./ ij .

Proof (Proposition 11) proceed structural induction block tree MCS . First,
consider case topology single block B. case, interface passed
DMCS V = VB .
Base case: Ck leaf. compare call DMCSOPT(k) Ck Ck .DMCS(V, ),
V = V (k)B = Bk . Algorithm 1 returns local belief sets Ck projected V Algorithm 5 returns plain local belief sets, claim follows V = V (k)B = Bk .
Induction case: Assume import neighborhood context Ck In(k) = {i1 , . . . , im },
0i1

= DMCSOPT(k) Ci1
..
.

i1

= Ci1 .DMCS(VB , )
..
.

0im

= DMCSOPT(k) Cim

im

= Cim .DMCS(VB , )

every partial equilibrium 0 0ij , exists ij 0 = S|V (k,ij )B .
two cases. First, edge (k, ij ) removed optimization procedure. Then,
Lemma 12, correspondence input DMCSOPT DMCS Ck .
hand, assume edge (k, ij ) removed optimization process.
removal either transitive reduction ear decomposition. former case, Lemma 15
shows input Ck affected removal edge. latter case, removal
one three possibilities illustrated Figure 20, assuming context C1 gets called:
(i) (6, 1), last edge simple cycle P0 = {1, 2, 3, 4, 5, 6}
11. abuse notation, write lsolve(T )





lsolve(T )

593

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

1

12
11

10

9

2

6

3

5
4

8

7

Figure 20: Possible cycle breakings
(ii) (9, 6), last edge path P1 = {5, 7, 8, 9, 6}
(iii) (12, 2), last edge path P2 = {3, 10, 11, 12}
Cases (i) (iii) differ case (ii) sense cycle recognized DMCS
case (ii), cycle detected along corresponding path.
Now, consider (k, ij ) removed situations similar cases (i) (iii), DMCSOPT
issue guess Step (c) Algorithm 5 v(k, ij ), includes V (cB )|Bij = VB Bij .
hand, DMCS recognize cycle Cij issue guess VB Bij Step (c)
Algorithm 1. Therefore, guess fed equally Ck .
(k, ij ) removed situations similar case (ii), guesses Ck interface
Cij eventually filtered combined local belief states computed Cij ,
starting node path containing (k, ij ) last edge (in ear decomposition).
Figure 20, node 5.
cases, whenever input 0 lsolve DMCSOPT(k) called
Cc , input lsolve Ck .DMCS(VB , ). Therefore, claim output holds.
Proposition 11 holds single leaf block, one see upper blocks
need import interface
beliefs cut vertices (also root contexts lower blocks).
setting V = Bk VB , results DMCSOPT DMCS projected interface
cut vertices identical. Therefore, upper blocks receive input regarding
interfaces cut vertices running algorithms. therefore final results projected
V (k)B end same.


Proof Proposition 8
Note components Handler Output simply take care communication part
DMCS-STREAMING. Output makes sure models sent back invokers correspondence request Handler got. routines Joiner Solver main
components play role Step (b) (d) Algorithm 5, respectively.
594

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

T1,1

T1,1
T1,1

T1,1

./

T2,1

./



./

./

T2,1

./



./

./

T2,1

./



./

./

T2,1

./



./


T1,1

T1,1

T1,1

./

T2,1

./



./

./

T2,1

./



./

./

T2,p2

./



./

Tm,1



./

Tm,pm



./

Tm,1



./

Tm,pm



Tm1,pm1

Tm1,pm1


./

Tm,1



./

Tm,pm



Tm1,1

./

Tm,1



./

Tm,pm



./

Tm,1




./

T2,p2

./



./


T1,p1

Tm1,2

Tm1,2

./




T1,1

Tm1,1

Tm1,1

Tm1,pm1


./

T2,1

./



./



Tm1,p1


T1,p1

./

T2,p2

./



./

Tm1,pm1

./

Tm,pm

T1,1

T1,1

T1,1

T1,1

T1,p1

./

T2,1

./



./

Tm1,1


./

F (m, m)



./

T2,1

./



./

./

F (m, m)



./

T2,p2

./



./

./

F (m, m)



./

T2,p2

./



./

./

F (m, m)



./

T2,p2

./



./

Tm1,pm1

Tm1,1

Tm1,pm1

Tm1,pm1

./

F (m, m)

T1,1

T1,1

T1,p1

./

T2,1

./



./



./

T2,p2

./



./

./

T2,p2

./



./

F (m 1, m)

F (m 1, m)

F (m 1, m)

=

[T1,1 ./ F (2, m)]







[T1,p1 ./ F (2, m)]

=

F (1, m).

=

=



Table 2: Accumulation Joiner

prove correctness DMCS-STREAMING, need show input lsolve
complete sense Step (e) Algorithm 8 exhaustively executed, full join
partial equilibria neighboring contexts delivered.
595

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Formally, assume current contexts import neighborhood {1, 2, . . . , m}. Assume
neighbor Ci 1 m, full partial equilibria Ti returned packages size
k denoted Ti,1 , . . . , Ti,pi , is, Ti = Ti,1 . . . Ti,pi . correctness algorithm,
assume Ti,1 , . . . , Ti,pi fixed partition Ti . possible when, example, lsolve
always returns answers fixed order. need show accumulation join
Algorithm 8 actually T1 ./ . . . ./ Tm .
Indeed, possible join T1,i1 ./ T2,i2 ./ . . . ./ Tm,im considered Joiner, performs
lexicographical traversal suitable combinations. Formally speaking, let F (p, q), q <
q, denote join result neighbors p q, is, F (p, q) = Tp ./ Tp+1 ./
.1 . . ./ Tq .
According lexicographical order, accumulation Joiner pj=1
[T1,j ./
F (2, m)] = F (1, m) demonstrated Table 2.
shows input lsolve complete. Hence, DMCS-STREAMING correct.


Appendix B. Detailed Run OptimizeTree
Example 20 illustrate call OptimizeTree(T = (B C, E), cp , cr ) block set B =
{B1 , B2 , B3 }, B1 = {1, 2, 3, 4}, B2 = {4, 5}, B3 = {3, 6}, C = {1, 3, 4}, E = {(B1 , 1), (B2 , 4),
(B3 , 3)}, cp = cr = 1.
local knowledge bases presented Example 10, have:
B1 = {car 1 , train 1 , nuts 1 }
B2 = {car 2 , train 2 }
B3 = {car 3 , train 3 , salad 3 , peanuts 3 , coke 3 , juice 3 , urgent 3 }

B4 = {car 4 , train 4 }
B5 = {soon5 , sooner 5 }
B6 = {fit 6 , sick 6 }

Since cp = cr , start B 0 = {B1 }. F = v = .
call OptimizeBlock(B1 , 1). Since B1 acyclic, transitive reduction applied.
get B1 = ({1, 2, 3, 4}, {(1, 2), (2, 3), (3, 4)}). subroutine returns E = {(1, 3), (2, 4)}.
child cut vertices B1 C 0 = {3, 4}; update F {(1, 3), (2, 4)}.
Next, update label edges (i, j) B1 . this, let us enumerate recursive
import interfaces, starting import interface, every node 1 6:
V(1) = {train 2 , train 3 , peanuts 3 }
V(2) = {car 3 , coke 3 , train 3 , car 4 , train 4 }
V (1)
V (2)
V (3)
V (4)
V (5)
V (6)

V(3) = {train 4 , sick 6 }
V(5) = {train 4 }

V(4) = {sooner 5 }
V(6) =

{train 2 , train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 , sooner 5 , sick 6 }
{train 3 , car 3 , coke 3 , train 4 , car 4 , sooner 5 , sick 6 }
{train 4 , sooner 5 , sick 6 }
{train 4 , sooner 5 }
{train 4 , sooner 5 }


Now, let us compute V (1, 2)B1 = V (1) `B1 |2 B` . B1 |2 = {3, 4}, thus
=
=
=
=
=
=

V (1, 2)B1 = V (1) (B3 B4 ) = {train 2 , train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }
Similarly, B1 |3 = B1 |4 = {4}, have:
V (2, 3) = V (2) B3 = {car 4 , train 4 }
V (3, 4) = V (3) B4 = {train 4 }
596

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

removed edges updated labels stored respectively F v block B1
summarized as:
F = {(1, 3), (2, 4)}
v(1, 2) =

V (1, 2)



V (1)|B3



V (1)|B4

v(2, 3) = V (2, 3) V (1)|B3 V (1)|B4
v(3, 4) = V (3, 4) V (1)|B3 V (1)|B4




train 2 , train 3 , peanuts 3 ,
=
car 3 , coke 3 , car 4 , train 4
= {train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }
= {train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }

Next, call OptimizeTree(T \ B1 , 3, 1) OptimizeTree(T \ B1 , 4, 1), eventually process
blocks B2 B3 manner above. two calls respectively return:
F 0 = {(5, 4)}
v 0 (4, 5) = {sooner 5 }

F 00 =
v 00 (3, 6) = {train 4 , sick 6 }

Combining results together, OptimizeTree(T, 1, 1) returns set removed edges
F = {(1, 2), (3, 4), (5, 4)}
updated labels v remaining edges blocks
v(1, 2)
v(2, 3)
v(3, 4)
v(4, 5)
v(3, 6)

=
=
=
=
=

{train 2 , train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }
{train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }
{train 3 , peanuts 3 , car 3 , coke 3 , car 4 , train 4 }
{sooner 5 }
{train 4 , sick 6 }

References
Adjiman, P., Chatalic, P., Goasdoue, F., Rousset, M.-C., & Simon, L. (2006). Distributed reasoning
peer-to-peer setting: Application semantic web. J. Artif. Intell. Res., 25, 269314.
Aho, A. V., Garey, M. R., & Ullman, J. D. (1972). Transitive Reduction Directed Graph.
SIAM J. Comput., 1(2), 131137.
Analyti, A., Antoniou, G., & Damasio, C. V. (2011). MWeb: principled framework modular
web rule bases semantics. ACM Trans. Comput. Log., 12(2), 17.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.). (2003).
Description Logic Handbook. Cambridge University Press.
Baget, J.-F., & Tognetti, Y. S. (2001). Backtracking biconnected components constraint
graph. Nebel, B. (Ed.), Proceedings Seventeenth International Joint Conference
Artificial Intelligence, IJCAI 2001, Seattle, Washington, USA, August 4-10, 2001, pp. 291
296. Morgan Kaufmann.
Bairakdar, S. E.-D., Dao-Tran, M., Eiter, T., Fink, M., & Krennwallner, T. (2010a). Decomposition
distributed nonmonotonic multi-context systems. Janhunen, T., & Niemela, I. (Eds.),
Logics Artificial Intelligence - 12th European Conference, JELIA 2010, Helsinki, Finland,
September 13-15, 2010. Proceedings, Vol. 6341 Lecture Notes Computer Science, pp.
2437. Springer.
597

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Bairakdar, S. E.-D., Dao-Tran, M., Eiter, T., Fink, M., & Krennwallner, T. (2010b). DMCS
solver distributed nonmonotonic multi-context systems. Janhunen, T., & Niemela, I.
(Eds.), Logics Artificial Intelligence - 12th European Conference, JELIA 2010, Helsinki,
Finland, September 13-15, 2010. Proceedings, Vol. 6341 Lecture Notes Computer Science, pp. 352355. Springer.
Bessiere, C., Bouyakhf, E., Mechqrane, Y., & Wahbi, M. (2011). Agile asynchronous backtracking
distributed constraint satisfaction problems. IEEE 23rd International Conference
Tools Artificial Intelligence, ICTAI 2011, Boca Raton, FL, USA, November 7-9, 2011,
pp. 777784.
Bikakis, A., & Antoniou, G. (2010). Defeasible contextual reasoning arguments ambient
intelligence. IEEE Transactions Knowledge Data Engineering, 22(11), 14921506.
Bikakis, A., Antoniou, G., & Hassapis, P. (2011). Strategies contextual reasoning conflicts
ambient intelligence. Knowl. Inf. Syst., 27(1), 4584.
Bogl, M., Eiter, T., Fink, M., & Schuller, P. (2010). MCS-IE system explaining inconsistency
multi-context systems. Logics Artificial Intelligence - 12th European Conference,
JELIA 2010, Helsinki, Finland, September 13-15, 2010. Proceedings, Vol. 6341 Lecture
Notes Computer Science, pp. 356359. Springer.
Bondy, A., & Murty, U. S. R. (2008). Graph Theory, Vol. 244 Graduate Texts Mathematics.
Springer.
Brewka, G., Eiter, T., Fink, M., & Weinzierl, A. (2011). Managed multi-context systems. Walsh,
T. (Ed.), Proceedings 22nd International Joint Conference Artificial Intelligence
(IJCAI-11), pp. 786791. AAAI Press/IJCAI.
Brewka, G., Ellmauthaler, S., & Puhrer, J. (2014). Multi-context systems reactive reasoning
dynamic environments. Ellmauthaler, S., & Puhrer, J. (Eds.), Proceedings International Workshop Reactive Concepts Knowledge Representation (ReactKnow) 2014, pp.
2330. Tech.Rep. 1, Computer Science Institute, Univ. Leipzig, ISSN 1430-3701.
Brewka, G., & Eiter, T. (2007). Equilibria heterogeneous nonmonotonic multi-context systems.
Proceedings Twenty-Second AAAI Conference Artificial Intelligence, July 22-26,
2007, Vancouver, British Columbia, Canada, pp. 385390. AAAI Press.
Brewka, G., Eiter, T., & Fink, M. (2011). Nonmonotonic Multi-Context Systems: Flexible Approach Integrating Heterogeneous Knowledge Sources. Balduccini, M., & Son, T. C.
(Eds.), Logic Programming, Knowledge Representation, Nonmonotonic Reasoning - Essays Dedicated Michael Gelfond Occasion 65th Birthday, Vol. 6565 Lecture Notes Computer Science, pp. 233258. Springer.
Brewka, G., Roelofsen, F., & Serafini, L. (2007). Contextual default reasoning. Veloso, M. M.
(Ed.), IJCAI 2007, Proceedings 20th International Joint Conference Artificial Intelligence, Hyderabad, India, January 6-12, 2007, pp. 268273.
Buccafurri, F., & Caminiti, G. (2008). Logic programming social features. Theory Practice
Logic Programming, 8(5-6), 643690.
Dao-Tran, M. (2014). Distributed Nonmonotonic Multi-Context Systems: Algorithms Efficient
Evaluation. Ph.D. thesis, Faculty Informatics, Vienna University Technology, Austria.
598

fiD ISTRIBUTED E VALUATION N ONMONOTONIC ULTI - CONTEXT YSTEMS

Dao-Tran, M., Eiter, T., Fink, M., & Krennwallner, T. (2010). Distributed nonmonotonic multicontext systems. Lin, F., Sattler, U., & Truszczynski, M. (Eds.), Principles Knowledge Representation Reasoning: Proceedings Twelfth International Conference,
KR 2010, Toronto, Ontario, Canada, May 9-13, 2010. AAAI Press.
Dao-Tran, M., Eiter, T., Fink, M., & Krennwallner, T. (2011). Model streaming distributed multicontext systems. Mileo, A., & Fink, M. (Eds.), 2nd International Workshop Logicbased Interpretation Context: Modeling Applications, Vol. 738 CEUR Workshop
Proceedings, pp. 1122.
Eiter, T., Fink, M., Ianni, G., Krennwallner, T., & Schuller, P. (2011). Pushing efficient evaluation
hex programs modular decomposition. Delgrande, J. P., & Faber, W. (Eds.), 11th International Conference Logic Programming Nonmonotonic Reasoning (LPNMR 2011),
Vancouver, BC, Canada, May 16-19, 2011, Vol. 6645 Lecture Notes Computer Science,
pp. 93106. Springer.
Eiter, T., Ianni, G., Schindlauer, R., & Tompits, H. (2005). uniform integration higher-order
reasoning external evaluations answer-set programming. IJCAI, pp. 9096.
Faltings, B., & Yokoo, M. (2005). Introduction: Special issue distributed constraint satisfaction.
Artif. Intell., 161(1-2), 15.
Fink, M., Ghionna, L., & Weinzierl, A. (2011). Relational information exchange aggregation
multi-context systems. Delgrande, J. P., & Faber, W. (Eds.), 11th International Conference Logic Programming Nonmonotonic Reasoning (LPNMR 2011), Vancouver, BC,
Canada, 16-19 May, 2011, Vol. 6645 Lecture Notes Computer Science, pp. 120133.
Springer.
Gao, J., Sun, J., & Zhang, Y. (2007). improved concurrent search algorithm distributed CSPs.
Australian Conference Artificial Intelligence, pp. 181190.
Gelfond, M., & Lifschitz, V. (1991). Classical negation logic programs disjunctive databases.
New Generation Comput., 9(3/4), 365386.
Ghidini, C., & Giunchiglia, F. (2001).
Local models semantics, contextual reasoning=locality+compatibility. Artif. Intell., 127(2), 221259.
Giunchiglia, F. (1992). Contextual Reasoning. Epistemologia, Special Issue Linguaggi e le
Macchine, 345, 345364.
Giunchiglia, F., & Serafini, L. (1994). Multilanguage hierarchical logics or: without
modal logics. Artif. Intell., 65(1), 2970.
Goncalves, R., Knorr, M., & Leite, J. (2014). Evolving multi-context systems. Schaub, T.,
Friedrich, G., & OSullivan, B. (Eds.), Proceedings 21st Eureopean Conference
Artificial Intelligence, ECAI2014, Prague, Czech Republic, August 18-23, 2014. IOS Press.
Hirayama, K., & Yokoo, M. (2005). distributed breakout algorithms. Artif. Intell., 161(12),
89115.
Homola, M. (2010). Semantic Investigations Distributed Ontologies. Ph.D. thesis, Comenius
University, Bratislava, Slovakia.
599

fiDAO -T RAN , E ITER , F INK , & K RENNWALLNER

Lee, J., & Lifschitz, V. (2003). Loop formulas disjunctive logic programs. Palamidessi, C.
(Ed.), Logic Programming, 19th International Conference, ICLP 2003, Mumbai, India, December 9-13, 2003, Proceedings, Lecture Notes Computer Science, pp. 451465. Springer.
McCarthy, J. (1993). Notes formalizing context. Bajcsy, R. (Ed.), Proceedings 13th
International Joint Conference Artificial Intelligence. Chambery, France, August 28 September 3, 1993, pp. 555562. Morgan Kaufmann.
Pontelli, E., Son, T., & Nguyen, N.-H. (2011). Combining answer set programming prolog:
ASP-PROLOG system. Balduccini, M., & Son, T. (Eds.), Logic Programming, Knowledge
Representation, Nonmonotonic Reasoning, Vol. 6565, pp. 452472. Springer Berlin Heidelberg.
Reiter, R. (1980). logic default reasoning. Artificial Intelligence, 13, 81132.
Roelofsen, F., Serafini, L., & Cimatti, A. (2004). Many hands make light work: Localized satisfiability multi-context systems. de Mantaras, R. L., & Saitta, L. (Eds.), Proceedings
16th Eureopean Conference Artificial Intelligence, ECAI2004, including Prestigious
Applicants Intelligent Systems, PAIS 2004, Valencia, Spain, August 22-27, 2004, pp. 5862.
IOS Press.
Serafini, L., Borgida, A., & Tamilin, A. (2005). Aspects distributed modular ontology reasoning. Nineteenth International Joint Conference Artificial Intelligence (IJCAI 2005),
pp. 570575. AAAI Press.
Serafini, L., & Tamilin, A. (2005). Drago: Distributed reasoning architecture semantic web.
Gomez-Perez, A., & Euzenat, J. (Eds.), Semantic Web: Research Applications,
Second European Semantic Web Conference, ESWC 2005, Heraklion, Crete, Greece, May 29
- June 1, 2005, Proceedings, Lecture Notes Computer Science, pp. 361376. Springer.
Tarjan, R. E. (1972). Depth-First Search Linear Graph Algorithms. SIAM J. Comput., 1(2),
146160.
Tasharrofi, S., & Ternovska, E. (2014). Generalized multi-context systems.. Baral, C., Giacomo,
G. D., & Eiter, T. (Eds.), Principles Knowledge Representation Reasoning: Proceedings Fourteenth International Conference, KR 2014, Vienna, Austria, July 20-24, 2014.
AAAI Press.
Valdes, J., Tarjan, R. E., & Lawler, E. L. (1982). recognition series parallel digraphs. SIAM
J. Comput., 11(2), 298313.
Vats, D., & Moura, J. M. F. (2010). Graphical models block-tree graphs. CoRR, abs/1007.0563.
Velikova, M., Novak, P., Huijbrechts, B., Laarhuis, J., Hoeksma, J., & Michels, S. (2014).
Integrated Reconfigurable System Maritime Situational Awareness. ECAI 2014 - 21st
European Conference Artificial Intelligence, 18-22 August 2014, Prague, Czech Republic
- Including Prestigious Applications Intelligent Systems (PAIS 2014), pp. 11971202.
Yokoo, M., & Hirayama, K. (2000). Algorithms distributed constraint satisfaction: review.
Autonomous Agents Multi-Agent Systems, 3(2), 185207.

600

fiJournal Artificial Intelligence Research 52 (2015) 1-95

Submitted 07/14; published 01/15

Coherent Predictive Inference Exchangeability
Imprecise Probabilities
Gert de Cooman
Jasper De Bock

gert.decooman@UGent.be
jasper.debock@UGent.be

Ghent University, SYSTeMS Research Group
TechnologieparkZwijnaarde 914
9052 Zwijnaarde, Belgium

Mrcio Alves Diniz

marcio.alves.diniz@gmail.com

Federal University Carlos, Department Statistics
Rod. Washington Luis, km 235
Carlos, Brazil

Abstract
Coherent reasoning uncertainty represented general manner
coherent sets desirable gambles. context allow indecision, leads
approach mathematically equivalent working coherent conditional
probabilities. allow indecision, leads general foundation coherent
(imprecise-)probabilistic inference. framework, given finite category set,
coherent predictive inference exchangeability represented using Bernstein
coherent cones multivariate polynomials simplex generated category set.
powerful generalisation de Finettis Representation Theorem allowing
imprecision indecision.
define inference system map associates Bernstein coherent cone
polynomials every finite category set. Many inference principles encountered
literature interpreted, represented mathematically, restrictions
maps. discuss, particular examples, two important inference principles: representation
insensitivitya strengthened version Walleys representation invarianceand specificity.
show infinity inference systems satisfy two principles,
amongst discuss particular skeptically cautious inference system, inference
systems corresponding (a modified version of) Walley Bernards Imprecise Dirichlet
Multinomial Models (IDMM), skeptical IDMM inference systems, Haldane
inference system. also prove latter produces posterior inferences
would obtained using Haldanes improper prior, implying infinity
proper priors produce coherent posterior inferences Haldanes improper one.
Finally, impose additional inference principle allows us characterise uniquely
immediate predictions IDMM inference systems.

1. Introduction
paper deals predictive inference categorical variables. therefore concerned
(possibly infinite) sequence variables Xn assume values finite set
categories A. observed number n them, found that, say X1 = x1 ,
X2 = x2 , . . . , Xn = xn , consider subjects belief model next n variables
Xn+1 , . . . Xn+n . probabilistic traditionand want build tradition
2015 AI Access Foundation. rights reserved.

fiDe Cooman, De Bock, & Diniz

context paperthis belief modelled conditional predictive probability
mass function pn (|x1 , . . . , xn ) set possible values. probability mass
functions used prediction estimation, statistical inferences, decision
making involving uncertain values variables. sense, predictive inference lies
heart statistics, generally, learning uncertainty. reason,
also crucial importance dealing uncertainty Artificial Intelligence,
instance, intelligent systems learn multinomial probabilities, Markov
transition probabilities, rates occurrence phenomena, local probabilities Bayesian
credal networks on. refer synthesis Geisser (1993) collection
essays Zabell (2005) good introductions predictive inference underlying
issues present paper also concerned with.
connects predictive probability mass functions various values n, n
(x1 , . . . , xn ) requirements time consistency coherence. former requires
n1 n2 , pn1 (|x1 , . . . , xn ) obtained pn2 (|x1 , . . . , xn )
usual marginalisation procedure; latter essentially demands conditional
probability mass functions connected time-consistent unconditional probability
mass functions Bayess Rule.
common assumption variables Xn exchangeable, meaning
roughly subject believes order observed, present
themselves, influence decisions inferences make regarding
variables. assumption, analysis consequences, goes back de Finetti
(1937) (see also Cifarelli & Regazzini, 1996). famous Representation Theorem states,
essence, time-consistent coherent conditional unconditional predictive
probability mass functions associated countably infinite exchangeable sequence
variables completely characterised by1 completely characterisea unique
probability measure Borel sets simplex probability mass functions A,
called representation.2
leads us central problem predictive inference: since infinity
probability measures simplex, one subject choose particular
context, given choice motivated justified? subjectivists de
Finettis persuasion might answer question needs answer: subjects personal
predictive probabilities entirely his, time consistency coherence
requirements heed. Earlier scholars, like Laplace Bayes, would
also call subjectivists, invoked Principle Indifference justify using specific class
predictive mass functions. Proponents logicist approach predictive inference would
try enunciating general inference principles order narrow down, hopefully eliminate
entirely, possible choices representing probability measures simplex.
logicians W. E. Johnson (1924) and, much systematic fashion, Rudolf Carnap (1952)
1. . . . unless observed sequence probability zero.
2. Actually, order clarify connection shall later on, essence de Finettis
argument representation coherent prevision set multinomial polynomialsor
equivalently, continuous real functionson simplex (De Cooman, Quaeghebeur, & Miranda,
2009b). (finitely additive) coherent prevision, extended uniquely far set
lower semicontinuous functions, determine unique (countably additive) probability
measure Borel sets simplex, F. Riesz Representation Theorem (De Cooman &
Miranda, 2008a; Troffaes & De Cooman, 2014).

2

fiCoherent Predictive Inference Exchangeability

tried develop axiom system predictive inference based reasonable inference
principles. Carnaps first group axioms related called coherence,
suggested, weak single particular predictive
model. second group consisted invariance axioms, including exchangeability. also
included axiom instantial relevance, translating intuitive principle predictive
inferences actually learn experience. last axiom, predictive irrelevance,
also proposed earlier Johnson called sufficientness postulate Good (1965).
Armed axioms, Carnap able derive continuum probabilistic inference
rules, closely related Dirichlet multinomial model Imprecise Dirichlet
Multinomial Model (IDMM) proposed Walley (1996) Walley Bernard (1999),
discuss Appendices C D, respectively.
point view holds middle ground subjectivist logicist positions:
possible subject make assessments certain predictive probabilities,
combine certain inference principles finds reasonable, suit
purpose problem hand. Indeed, inference systems introduce discuss
Section 6, notion conservative coherent inferenceor natural extensionwe
associate them, provide elegant framework tools making conservative coherent
predictive inferences combine (local) subjective probability assessments (general)
inference principles. work Section 15 characterising immediate predictions
IDMM constitutes exercise inor example forprecisely that.
idea conservative probabilistic inference brings us believe
main contribution paper. central idea de Finettis (1975) approach
probabilitybut also course implicit Markov Chebyshev inequalitiesthat
subject makes probability assessments, consider bounds so-called
precise probability models. Calculating conservative tightest bounds indeed
de Finettis (1975) Fundamental Theorem Prevision (see also Lad, 1996) about.
theory imprecise probabilities, brought synthesis Williams (1976) Walley
(1991, 2000), going back Boole (1952) Keynes (1921), crucial contributions
quite number statisticians philosophers (Smith, 1961; Levi, 1980; Seidenfeld,
Schervish, & Kadane, 1999), looks conservative probabilistic inference precisely
way: calculate efficiently possible consequencesin sense
conservative tightest boundsof making certain probability assessments. may local
assessments, inequalities imposed probabilities previsions certain events
variables, structural assessments, independence, exchangeability.
One advantage imprecise probability models allow imprecision,
words, use partial probability assessments using bounding inequalities rather
equalities. Another, related, advantage allow indecision modelled
explicitly: loosely stated, imposed bounds probabilities allow one
probability model solution, may well two actions, first
higher expected utility one compatible probability model, smaller another
compatible probability model, meaning neither action robustly preferred other.
current stated model beliefs, subject undecided
actions. Section 2, give concise overview relevant ideas, models techniques
field imprecise probabilities. much extensive detailed recent overview
area research published Augustin, Coolen, De Cooman, Troffaes (2014).
3

fiDe Cooman, De Bock, & Diniz

present paper, then, described application ideas imprecise probabilities predictive inference. aim studyand develop general framework
dealing withconservative coherent predictive inference using imprecise probability models.
Using models also allow us represent subjects indecision, believe
natural state knowing, learned little, problem hand.
seems important theories learning uncertainty general, predictive
inference particular, least allow us (i) start conservative, imprecise
indecisive models little learned, (ii) become precise decisive
observations come in. shall see abstract notion inference system
introduce on, allows forbut necessarily forcesuch behaviour,
shall give number examples concrete inference systems display it.
work builds on, manages reach much than, earlier paper
one authors (De Cooman, Miranda, & Quaeghebeur, 2009a). One reason
so, earlier work deals immediate prediction models, shall
see on, predictive inference using imprecise probabilities completely determined
immediate prediction, contrary expect using precise probabilities.
main reason position use powerful mathematical
language represent imprecise-probabilistic inferences: Walleys (2000) coherent sets
desirable gambles. Earlier imprecise probability models (Boole, 1952, 1961; Koopman, 1940)
centred lower upper probability bounds eventsor propositions. Later (Walley,
1991, Section 2.7), became apparent language events lower upper
probabilities lacking power expression: much expressive theory uses random
variables lower previsions expectations. successful theory coherent lower
previsions quite well developed (Walley, 1991; Augustin et al., 2014; Troffaes &
De Cooman, 2014). faces number problems, mathematical well
conceptual complexity, especially dealing conditioning independence,
fact that, case many approaches probability, shall see
Section 2.5, issues conditioning sets (lower) probability zero.
attractive solution problems offered Walley (2000), form
coherent sets desirable gambles, inspired earlier ideas (Smith, 1961; Williams, 1975b;
Seidenfeld, Schervish, & Kadane, 1995). Here, primitive notions probabilities
events, expectations random variables. focus rather whether gamble,
risky transaction, desirable subjectstrictly preferred zero transaction,
status quo. basic belief model probability measure lower prevision,
set desirable gambles. course, stating gamble desirable also leads
particular lower prevision assessment: provides lower bound zero prevision
gamble. explain prefer use sets desirable gambles basic uncertainty
models Section 2.
summary, then, aim paper use sets desirable gambles extend
existing probabilistic theory predictive inference. Let us explain detail
intend go this. basic building blocks introduced Sections 27.
already indicated above, give overview relevant notions results concerning
imprecise probability model choicecoherent sets desirable gamblesin Section 2.
particular, explain use conservative inference well conditioning;
4

fiCoherent Predictive Inference Exchangeability

derive commonly used models, lower previsions lower probabilities,
them; relate precise probability models.
Section 3, explain describe subjects beliefs sequence
variables terms predictive sets desirable gambles, derived notion predictive
lower previsions. imprecise probability models generalise above-mentioned predictive
probability mass functions pn (|x1 , . . . , xn ), constitute basic tools shall
working with. also explain proper formulations above-mentioned
time consistency coherence requirements general context.
Section 4, discuss number inference principles believe could reasonably
imposed predictive inferences, show represent mathematically
terms predictive sets desirable gambles lower previsions. Pooling invarianceor
Walley (1996) called Representation Invariance Principle (RIP)and renaming
invariance seem reasonable requirements type predictive inference, category
permutation invariance seems natural thing require starting state
complete ignorance. Taken together, constitute call representation insensitivity.
means predictive inferences remain essentially unchanged transform
set categories, words essentially insensitive choice
representationthe category set. Another inference principle look imposes so-called
specificity property: predictive inference specific, certain type question
involving restricted number categories, general model replaced
specific model deals categories interest, produce
relevant inferences (Bernard, 1997).
next important step taken Section 5, recall literature (De
Cooman et al., 2009b; De Cooman & Quaeghebeur, 2012) deal exchangeability
predictive inference models imprecise. recall de Finettis Representation
Theorem significantly generalised. case, time-consistent coherent
predictive sets desirable gambles completely characterised set (multivariate)
polynomials simplex probability mass functions category set.3
set polynomials must satisfy number properties, taken together define
notion Bernstein coherence. Without becoming technical point, conclusion
section that, general context, precise-probabilistic notion
representing probability measure simplex probability mass functions replaced
Bernstein coherent set polynomials simplex. set polynomials serves
completely purpose representing probability measure: completely determines,
conveniently densely summarises, predictive inferences. reason
rest developments paper expressed terms Bernstein coherent
sets polynomials.
introduce coherent inference systems Section 6 maps associate
finite set categories Bernstein coherent set polynomials simplex probability
mass functions set. coherent inference system way fixing completely
coherent predictive inferences possible category sets. reasons introducing
coherent inference systems twofold. First all, inference principles Section 4 impose
connections predictive inferences different category sets, represent
3. contradistinction de Finettis version, version problems conditioning observed
sequences (lower) probability zero.

5

fiDe Cooman, De Bock, & Diniz

inference principles mathematically restrictions coherent inference systems,
main topic Section 7. Secondly, allows us extend method natural extensionor
conservative inferenceintroduced Section 2.2, also take account principles
predictive inference, generally, predictive inference multiple category sets once.
leads method combining (local) predictive probability assessments (global)
inference principles produce conservative predictive inferences compatible
them.
first illustration power methodology, look immediate prediction
Section 8: implications representation insensitivity specificity
predictive inference single next observation? show approach allows us
streamline, simplify significantly extend previous attempts direction De
Cooman et al. (2009a).
material Sections 914 shows, producing explicit examples,
quite different typeseven uncountable infinitiesof coherent inference systems
representation insensitive and/or specific. discuss vacuous nearly vacuous
inference systems Sections 9 10, skeptically cautious inference system Section 11,
family IDMM inference systems Section 12, family skeptical IDMM inference
systems Section 13, Haldane inference system Section 14. inference
systems, apart IDMM, appear first time. Also, believe
first publish detailed explicitas well still elegantproof IDMM
inference systems indeed representation insensitive specific. already
mentioned here, however, IDMM inference systems based modified,
arguably better behaved, version models originally introduced Walley Bernard
(see Walley, 1996; Walley & Bernard, 1999; Bernard, 2005); refer Appendix
explanation, proof original IDMM specific that, contrary
often claimed, satisfy so-called nestedness property.
results disprove conjecture (Bernard, 2007; De Cooman et al., 2009a)
IDMM inference systemsour version original oneare ones, even
conservative ones, satisfy representation insensitivity specificity.
show Section 15 IDMM family immediate predictionswhich
version original oneare definite sense conservative ones
representation insensitive specific, satisfy another requirement,
called concave surprise.
conclusion (Section 16) point number surprising consequences
results, discuss avenues research.
order make paper self-contained possible, included number
appendices additional discussion. help reader find way many
notions notations need paper, Appendix provides list common
ones, short hint meaning, introduced. Appendix B provides
useful necessary background theory multivariate polynomials simplices,
important part Bernstein basis polynomials there. discussion IDMM
inference systems relies quite heavily Dirichlet densities simplices, expectation
operators associated them. discuss important relevant properties
Appendix C. Appendix contains discussion original IDM IDMM models,
proposed Walley Bernard (see Walley, 1991, 1996; Walley & Bernard, 1999; Bernard,
6

fiCoherent Predictive Inference Exchangeability

2005), show claims make model need
carefully formulated. stated above, main reason introducing,
Section 12, modified version IDMM models, suffer
shortcomings, produces immediate prediction models original version.
Finally, effort make lengthy paper readable possible, moved
proofs, additional technical discussion, Appendix E.

2. Imprecise Probability Models
section, give concise overview imprecise probability models representing,
making inferences decisions under, uncertainty. suggested Introduction,
shall focus sets desirable gambles uncertainty models choice.
Let us briefly summarise next section why, present paper, work
sets basic uncertainty models conservative probabilistic inference. reader
wants dispense motivation proceed Section 2.2, introduce
mathematics behind models. later sections, shall course also briefly mention
derived results terms familiar language (lower) previsions probabilities.
2.1 Sets Desirable Gambles?
First all, number examples literature (Moral, 2005; Couso & Moral, 2011; De
Cooman & Quaeghebeur, 2012; De Cooman & Miranda, 2012) shown working
making inferences using models general expressive.
also simpler elegant mathematical point view, intuitive
geometrical interpretation (Quaeghebeur, 2014). shall see Sections 2.4 3
marginalisation conditioning especially straightforward, issues
conditioning sets (lower) probability zero.
Also, become apparent discussion Section 2.2, explained
detail Moral Wilson (1995) De Cooman Miranda (2012),
similarity accepting gamble one hand accepting proposition true
other, gives logical flavour conservative probabilistic inference. Indeed,
strong analogy two, connects conservative probabilistic inferencealso
called natural extension fieldwith logical deduction: classical propositional
logic looking smallest deductively closed set contains number given
propositions, imprecise probabilities context looking smallest coherent set
desirable gambles contains number given gambles. context analogy,
precise probability models closely related complete, maximal, deductively closed
setsperfect information states. clear indication precise probability models
well suited dealing conservative inference, need
broader context imprecise probability models natural language setting
this. summary, working sets desirable gambles encompasses subsumes
special cases classical (or precise) probabilistic inference inference classical
propositional logic; see detailed discussion De Cooman Miranda (2012).
Finally, briefly explain Section 5, De Cooman Quaeghebeur (2012)
shown working sets coherent desirable gambles especially illuminating
context modelling exchangeability assessments: exposes simple geometrical meaning
7

fiDe Cooman, De Bock, & Diniz

notion exchangeability, leads simple particularly elegant proof
significant generalisation de Finettis (1937) Representation Theorem exchangeable
random variables.
summary, work sets desirable gambles powerful,
expressive general models hand, intuitive work withthough
unfortunately less familiar people closely involved field, and,
importantly, avoid problems conditioning sets (lower) probability
zero. details, refer work Walley (2000), Moral (2005), Couso Moral
(2011), De Cooman Quaeghebeur (2012), Quaeghebeur (2014).
2.2 Coherent Sets Desirable Gambles Natural Extension
consider variable X assumes values finite4 possibility space A. model
subjects beliefs value X looking gambles variable
subject finds desirable, meaning strictly prefers5 zero gamblethe status
quo. general approach, extends usual rationalist subjectivist
approach probabilistic modelling allow indecision imprecision.
gamble real-valued function f A. interpreted uncertain reward f (X)
depends value X, expressed units predetermined linear utility.
represents reward subject gets transaction first actual value x
X determined, subject receives amount utility f (x)which may
negative, meaning pay it. Throughout paper, use device writing f (X)
want make clear variable X gamble f depends on.
Events subsets possibility space A. event B associate
special gamble IB , called indicator, assumes value 1 B 0 elsewhere.
denote set gambles L(A). linear space point-wise
addition gambles, point-wise multiplication gambles real numbers.
subset L(A), posi(A) set positive linear combinations gambles A:
posi(A) :=

{
n

}
k fk : fk A, k R>0 , n N .

(1)

k=1

Here, N set natural numbers (without zero), R>0 set positive real
numbers. convex cone gambles subset L(A) closed positive linear
combinations, meaning posi(A) = A.
two gambles f g A, write f g (x A)f (x) g(x), f > g
f g f 6= g. gamble f > 0 called positive. gamble g 0 called non-positive.
4. sake simplicity, restrict discussion finite possibility spaces,
really need purposes paper. limited number remarks on, shall
occasion mention related notions infinite possibility spaces, give ample references
guide interested reader relevant literature.
5. want point notion strict preferenceor preference without indifferencecommonly
used preference modelling, confused Walleys (1991, Section 3.7.7) notion strict
desirability, one many ways construct lower prevision set gambles
strictly preferred zero gamble; see also discussion near end Section 2.5.
details, refer recent paper Quaeghebeur, De Cooman, Hermans (2014).

8

fiCoherent Predictive Inference Exchangeability

L>0 (A) denotes convex cone positive gambles, L0 (A) convex cone
non-positive gambles.
collect gambles subject finds desirablestrictly prefers6 zero gamble
set desirable gambles, shall take sets basic uncertainty models.
course, satisfy certain rationality criteria:
Definition 1 (Coherence). set desirable gambles L(A) called coherent
satisfies following requirements:
D1. 0
/ D;
D2. L>0 (A) D;
D3. = posi(D).
D(A) denotes set coherent sets desirable gambles A.
Requirement D3 turns convex cone. Due D2, includes L>0 (A); D1D3,
avoids non-positivity:
D4. f 0 f
/ posi(D), equivalently L0 (A) posi(D) = .
L>0 (A) smallest coherent subset L(A). so-called vacuous model therefore
reflects minimal commitments part subject: knows absolutely nothing
likelihood different outcomes, strictly prefer zero
gambles never decrease wealth possibility increasing it.
D1 D2 , subject set desirable gambles D1 conservative, less
committal, subject set desirable gambles D2 , simply latter strictly
prefers zero gambles former does, possibly more. inclusion relation
imposes natural partial ordering sets desirable gambles, simple interpretation
least conservative as.
non-empty family coherent sets desirable gambles Di , I, intersection
iI Di still coherent. simple result underlies notion (conservative) coherent
inference. subject gives us assessmenta set L(A) gambles
finds desirablethen tells us exactly assessment extended coherent
set desirable gambles, construct smallestand therefore least committal
conservativesuch set:
Theorem 2 (Natural Extension, De Cooman & Quaeghebeur, 2012). Let L(A),
define natural extension by:7

EA :=
{D D(A) : D} .
following statements equivalent:
(i) avoids non-positivity: L0 (A) posi(A) = ;
(ii) included coherent set desirable gambles;
6. See footnote 5.

7. usual, expression, let = L(A).

9

fiDe Cooman, De Bock, & Diniz

(iii) EA 6= L(A);
(iv) set desirable gambles EA coherent;
(v) EA smallest coherent set desirable gambles includes A.
(and hence all) equivalent statements holds, EA = posi(L>0 (A) A).
Moreover, coherent 6= L(A) EA = A.
2.3 Maximal Coherent Sets Desirable Gambles
element D(A) called maximal strictly included element
D(A), words, adding gamble f makes sure longer extend
set {f } set still coherent:
(D0 D(A))(D D0 = D0 ).
M(A) denotes set maximal elements D(A). coherent set desirable gambles
maximal non-zero gambles f A, f
/ f (see Couso &
Moral, 2011 case finite A, De Cooman & Quaeghebeur, 2012 infinite
case). Coherence natural extension described completely terms maximal
elements:
Theorem 3 (Couso & Moral, 2011; De Cooman & Quaeghebeur, 2012). set avoids
non-positivity
maximal M(A) D. Moreover,

EA = {D M(A) : D}.
2.4 Conditioning Sets Desirable Gambles
Let us suppose subject coherent set desirable gambles A, expressing
beliefs value variable X assumes A. ask so-called
updated set DcB desirable gambles B would be, receive additional
informationand nothing morethat X actually belongs subset B A.
updating, conditioning, rule sets desirable gambles states that:
g DcB gIB gambles g B.

(2)

states gamble g desirable subject observe X B
called-off gamble gIB desirable him. called-off gamble gIB
gamble variable X gives zero rewardis called offunless X B,
case reduces gamble g new possibility space B. updated set DcB
set desirable gambles B still coherent, provided (De Cooman
& Quaeghebeur, 2012). See discussions Moral (2005), Couso Moral (2011), De
Cooman Quaeghebeur (2012), De Cooman Miranda (2012) Quaeghebeur (2014)
detailed information updating sets desirable gambles.
2.5 Coherent Lower Previsions
use coherent sets desirable gambles introduce derived concepts, coherent
lower previsions, probabilities.
10

fiCoherent Predictive Inference Exchangeability

Given coherent set desirable gambles D, functional P defined L(A)
P (f ) := sup { R : f D} f L(A),

(3)

coherent lower prevision (Walley, 1991, Thm. 3.8.1). means lower
envelope expectations associated set probability mass functions,8 or,
equivalently, satisfies following coherence properties (Walley, 1991, 2000; De
Cooman & Quaeghebeur, 2012; Miranda & De Cooman, 2014; Troffaes & De Cooman, 2014):
P1. P (f ) min f gambles f A;
P2. P (f + g) P (f ) + P (g) gambles f, g A;
P3. P (f ) = P (f ) gambles f real 0.
used notation min f := min {f (x) : x A}; max f defined similarly.
conjugate upper prevision P defined P (f ) := inf { R : f D} = P (f ).
following properties implied P1P3:
P4. max f P (f ) P (f ) min f gambles f A;
P5. P (f + ) = P (f ) + P (f + ) = P (f ) + gambles f R.
gamble f , P (f ) called lower prevision f , follows Equation (3)
interpreted subjects supremum desirable price buying gamble f .
event B, P (IB ) also denoted P (B), called lower probability B;
interpreted subjects supremum desirable rate betting B. Similarly
upper previsions upper probabilities.
lower prevision associated vacuous set desirable gambles L>0 (A) given
P (f ) = min f . called vacuous lower prevision, point-wise smallest,
conservative, coherent lower previsions.
coherent conditional model DcB, B non-empty subset A, induces conditional lower prevision P (|B) L(B), invoking Equation (3):
P (g|B) := sup { R : g DcB} = sup { R : [g ]IB D}
gambles g B. (4)
difficult show (Walley, 1991) P P (|B) related following
coherence condition:
P ([g P (g|B)]IB ) = 0 g L(B),
(GBR)
called Generalised Bayes Rule. rule allows us infer P (|B) uniquely P ,
provided P (B) > 0. Otherwise, usually infinity coherent lower previsions
P (|B) coherent P sense satisfy (GBR), equivalently,
coherent set desirable gambles leads P P (|B). Two
8. statement valid working finite A. infinite A, similar results shown
hold (Walley, 1991; De Cooman & Quaeghebeur, 2012; Miranda & De Cooman, 2014; Troffaes &
De Cooman, 2014), expectations involved coherent previsionsexpectation operators
associated finitely additive probability measures. See also discussion Section 2.6.

11

fiDe Cooman, De Bock, & Diniz

particular conditioning rules, namely natural regular extension (Walley, 1991; Miranda
& De Cooman, 2014), always produce conditional lower previsions satisfy GBR,
therefore coherent P . P (B) > 0but necessarily P (B) = 0!they
always produce point-wise smallest largest coherent conditional lower previsions,
respectively (Miranda, 2009; Miranda & De Cooman, 2014).9
Many different coherent sets desirable gambles lead coherent lower prevision
P , typically differ boundaries. sense, coherent sets desirable
gambles informative coherent lower previsions: gamble positive lower
prevision always desirable one negative lower prevision never, gamble
zero lower prevision lies border set desirable gambles, lower
prevision generally provide information desirability gambles.
border behaviour importantand dealing conditioning events
zero (lower) probability (Walley, 2000; Moral, 2005; Couso & Moral, 2011; Quaeghebeur,
2014)it useful work sets desirable gambles rather lower previsions,
Equations (2) (4) tell us, allow us derive unique conditional models
unconditional ones: coherent set desirable gambles corresponds unique
conditional set desirable gambles DcB unique conditional lower prevision P (|B),
non-empty event B. smallest set desirable gambles induces given coherent
lower prevision, called associated set strictly desirable gambles (Walley, 1991)
given {f L(A) : f > 0 P (f ) > 0}. See papers Walley (2000) Quaeghebeur
(2014) additional discussion sets desirable gambles informative
coherent lower previsions.
2.6 Linear Previsions Credal Sets
coherent lower upper prevision coincide gambles, real
functional P defined L(A) P (f ) := P (f ) = P (f ) f L(A) coherent prevision.
Since assumed finite,10 means corresponds
expectation

operator associated probability mass function p: P (f ) = xA f (x)p(x) =: Ep (f )
f L(A), p(x) := P (I{x} ) x A. happens particular lower
upper previsions induced maximal coherent set desirable gambles. Indeed,
boundary behaviour, so-called precise probability models P correspond maximal
coherent sets desirable gambles; see discussions Williams (1975a), Miranda
Zaffalon (2011, Proposition 6) Couso Moral (2011, Section 5) information.
coherent previsions P , Generalised Bayes Rule (GBR) reduces Bayess Rule:
P (gIB ) = P (B)P (g|B) g L(B),

(BR)

indicating central probabilistic updating rule special case Equation (2).
9. conditional lower previsions Section 12 IDMM produced regular extension.
models Sections 11, 13 14 lower previsions amongst them, nearly cases
different conditional lower previsions, even though cases natural regular extensions
coincidethey vacuous there.
10. already hinted footnote 8, similar things still said infinite A, would unduly
complicate discussion. details, see work Walley (1991), Troffaes De Cooman
(2014) Miranda De Cooman (2014).

12

fiCoherent Predictive Inference Exchangeability

assumed finite, define so-called credal set M(P ) associated
coherent lower prevision P as:
M(P ) := {p : (f L(A))Ep (f ) P (f )} ,
closed convex subset so-called simplex probability mass
functions A.11 P lower envelope M(P ): P (f ) = min {Ep (f ) : p M(P )}
f L(A) (Walley, 1991; Miranda & De Cooman, 2014; Troffaes & De Cooman,
2014). sense, convex closed sets precise probability models also seen
imprecise probability models, mathematically equivalent coherent lower
previsions. therefore also less general powerful coherent sets desirable
gambles, also suffer problems conditioning events (lower) probability
zero.12

3. Predictive Inference
Predictive inference, specific sense focussing here, considers number
variables X1 , . . . , Xn assuming values category set Awe define category set
non-empty finite set.13 follows, shall occasion use many different
category sets, shall use italic capitals A, B, C, D, . . . refer them.
start discussion predictive inference models general representationally powerful language: coherent sets desirable gambles, introduced previous
section. on, shall also pay attention specific derived models,
predictive lower previsions, predictive lower probabilities.
Predictive inference assumes generally number n observations made,
= (x1 , . . . , xn ) first n variables X1 , . . . , Xn . Based
know values
n c
values
subject posterior predictive model DA
observation sample ,
n
n
coherent set
next n variables Xn+1 , . . . , Xn+n assume . DA c
desirable gambles f (Xn+1 , . . . , Xn+n ) . assume n N.
hand, want allow n N0 := N {0}, set natural numbers
zero: also want able deal case previous observations
n prior predictive model.14 course,
made. case, call corresponding model DA
technically speaking, n + n n.
said, subject may also prior, unconditional model, obn
servations yet made. general form, coherent set DA
11. See Section 5.2 explicit definition .
12. Using sets full conditional measures (Dubins, 1975; Cozman, 2013), rather sets probability
mass functions, leads imprecise probability model related sets desirable gambles (Couso
& Moral, 2011), problems conditioning sets lower probability zero either,
feel less elegant mathematically complicated.
13. formal reasons, include trivial case category sets single element, case
certain value variables assume.
14. terms posterior prior association predictive models indicate whether previous
observations made. But, order avoid well-known issues temporal coherence
(Zaffalon & Miranda, 2013), assuming prior posterior models based
subjects beliefs observations made, posterior models refer hypothetical
future situations.

13

fiDe Cooman, De Bock, & Diniz

n
desirable gambles f (X1 , . . . , Xn ) , n N. may also coherent sets DA
n
desirable gambles f (X1 , . . . , Xn ) , n natural number
n n must related following
n n; sets DA

marginalisation, time consistency, requirement:15
n
n
f (X1 , . . . , Xn ) DA
f (X1 , . . . , Xn ) DA
gambles f .

(5)

expression, throughout paper, identify gamble f cylindrical
extension f 0 , defined f 0 (x1 , . . . , xn , . . . , xn ) := f (x1 , . . . , xn ) (x1 , . . . , xn ) .
introduce marginalisation operator margn () := L(An ), time consistency
n = marg (D n ) = n L(An ).
condition also rewritten simply DA
n


n posterior (conditional) ones n c
Prior (unconditional) predictive models DA
must
also related following updating requirement:
n
n
f (Xn+1 , . . . , Xn+n )I{}
f (Xn+1 , . . . , Xn+n ) DA
c
(X1 , . . . , Xn ) DA

gambles f , (6)
special case Equation (2): gamble f (Xn+1 , . . . , Xn+n ) desirable observ gamble f (Xn+1 , . . . , Xn+n )I{}
ing sample
(X1 , . . . , Xn ) desirable
observations made. called-off gamble f (Xn+1 , . . . , Xn+n )I{}
(X1 , . . . , Xn )

gamble gives zero rewardis called offunless first n observations ,
case reduces gamble f (Xn+1 , . . . , Xn+n ) remaining variables
Xn+1 , . . . , Xn+n . updating requirement generalisation Bayess Rule updating,
fact reduces sets desirable gambles lead (precise) probability
mass functions, described Section 2.6 proved detail Walley (2000) also
De Cooman Miranda (2012). contrary Bayess Rule probability mass
functions, updating rule (6) coherent sets desirable gambles clearly suffer
problems conditioning event (lower) probability zero: allows us infer
unique conditional model unconditional one, regardless (lower upper)
probability conditioning event. refer work De Cooman Miranda
(2012) detailed discussions marginalisation updating sets desirable gambles
many-variable context.
explained Section 2.5, use relationship (3) derive prior (unconditional)
n through:
predictive lower previsions P nA () L(An ) prior set DA
n
P nA (f ) := sup { R : f DA
} gambles f 1 n n,

L(An ) posterior
posterior (conditional) predictive lower previsions P nA (|)
n
through:
sets DA c
{
}
n
:= sup R : f DA
gambles f .
P nA (f |)
c
15. See also related discussion notion De Cooman Miranda (2008b) De Cooman
Quaeghebeur (2012); confused temporal consistency discussed Goldstein
(1983, 1985) Zaffalon Miranda (2013).

14

fiCoherent Predictive Inference Exchangeability

on, shall also want condition predictive lower previsions additional
information (Xn+1 , . . . , Xn+n ) B n , proper subset B A. Using ideas
Sections 2.4 2.5, leads instance following lower prevision:
{
}
n
B n ) := sup R : [g ]IB n DA
gambles g B n ,
P nA (g|,
c
(7)
conditioned event B n .
lower prevision P nA (|)

4. Principles Predictive Inference
far, introduced coherence, marginalisation updating basic rationality
requirements prior posterior predictive inference models must satisfy. could
envisaged requirementsother inference principlescan imposed
inference models. want show deal additional
requirements theory conservative predictive inference, discuss, way
examples, number additional conditions, suggested number
authors reasonable properties ofor requirements forpredictive inference models.
want stress considering requirements examples, want
defend using circumstances, mean suggest always reasonable
useful. are: inference principles might want impose, whose
implications conservative predictive inference might therefore want investigate.
4.1 Pooling Invariance
first consider Walleys (1996) notion representation invariance, prefer call
pooling invariance. Consider set categories A, partition B non-empty
partition classes. course consider partition B set categories well.
Therefore, order streamline discussion notation, shall henceforth denote
Bas stated before, want use italic capitals category sets. elements
subset C Acorresponds single new category, consists original
categories x C pooledconsidered one. Denote (x) unique element
partition B original category x belongs to. leads us consider surjective
(onto) map B.
say gamble g differentiate pooled categories when:
g() = g() , (k {1, . . . , n})(xk ) = (yk ),
means gamble f B n that:
( )g() = f ((x1 ), . . . , (xn )).
idea underlying formulaor requirementis sample = (x1 , . . . , xn )
, corresponds sample := ((x1 ), . . . , (xn )) B n pooled categories. Pooling
invariance requires gambles g = f differentiate pooled
categories, make difference whether make predictive inferences using set
original categories A, using set pooled categories B. formally, terms
predictive lower previsions:

15

fiDe Cooman, De Bock, & Diniz

= P nB (f |)

P nA (f ) = P nB (f ) P nA (f |)
,
n, n N considered, gambles f B n
alternatively, generally, terms predictive sets desirable gambles:
n
n
n
n
f DB

f DA
f DB
f DA
c
c

.
n, n N considered, gambles f B n
Pooling invariance seems reasonable principle uphold cases category
set known full detail. case useful start limited set broadly
defined categories, allow creation new ones, pooling splitting old categories
observations proceed. context, recall Walleys (1996) example:
closed bag containing coloured marbles, probability drawing red marble
it? information, subject idea colours
marbles bag, making difficult construct suitable detailed category set
experiment. draws bag, predictive inference model used
respects pooling invariance, inferences made red marbles uses
category set {red, yellow, blue, other} using category
set {red, non-red}, colours different red pooled together single
category. appears pooling invariance typically useful principle, instance,
sampling species problems, one wants assess prevalence given species
certain area.
special case pooling invariance, called embedding invariance,16 concentrates case without prior observations. terms lower previsions:
P nA (f ) = P nB (f ) n N considered, gambles f B n ,
alternatively, generally, terms sets desirable gambles:
n
n
f DA
f DB
n N considered, gambles f B n .

4.2 Renaming Invariance
Besides pooling invariance, may also require renaming invariance: long confusion
arise, matter subjects predictive inferences names, labels,
gives different categories.
may seem trivial even mention, far know, always implicitly
taken granted predictive inference. well devote attention
here, order distinguish category permutation invariance discussed
shortly, easily confused pay proper attention.
renaming bijection (a one-to-one onto map) set original categories
set renamed categories C, clearly distinguish elements
C, sample = (x1 , . . . , xn ) original categories,
corresponds sample renamed categories := ((x1 ), . . . , (xn )). gamble
16. Walley calls underlying requirement (lower) probability event depend
possibility space embedded, Embedding Principle (Walley, 1991, Section 5.5.1).

16

fiCoherent Predictive Inference Exchangeability

f set C n renamed samples, corresponds gamble f set
original samples. Clearly, require make difference whether
make predictive inferences using set original categories A, using set renamed
categories C. formally, terms predictive lower previsions:
= P nC (f |)

P nA (f ) = P nC (f ) P nA (f |)
,
n, n N considered, gambles f C n
alternatively, generally, terms predictive sets desirable gambles:
n
n
n
n
f DC

f DA
f DC
f DA
c
c

.
n, n N considered, gambles f C n
4.3 Category Permutation Invariance
shall especially interested predictive inference subject starts state
prior ignorance. state, reason distinguish different elements
set categories chosen. formalise idea, consider permutation
$ elements A.17 sample , corresponds permuted sample
$ := ($(x1 ), . . . , $(xn )). gamble f , corresponds permuted
gamble f $ . subject reason distinguish categories z
images $(z), make sense require following category permutation invariance:18
= P nA (f |$)

P nA (f $) = P nA (f ) P nA (f $|)
,
n, n N considered, gambles f
alternatively, generally, terms predictive sets desirable gambles:
n
n
n
n
f DA

f $ DA
f DA
f $ DA
c
c$

.
n, n N considered, gambles f
Formally, requirement closely resembles renaming invariance, whereas latter
trivial requirement, category permutation invariance symmetry requirement
categories justified subject reason distinguish
them, may instance justified starts state prior ignorance.
draw attention difference two somewhat loose manner: category
permutation invariance allows confusion new old categories, something
renaming invariance carefully avoids.
see principle could reasonable, recall Walleys (1996) bag marbles
example, introduced discussing pooling invariance. Since, drawn
17. permutation $ elements A, words categories, contrasted
permutations order observations, i.e. time set {1, . . . , n}, considered discussion
exchangeability, Section 5.
18. requirement related notion (weak) permutation invariance De Cooman Miranda
(2007) studied much detail paper dealing symmetry uncertainty modelling. goes
back Walleys (1991, Section 5.5.1) Symmetry Principle.

17

fiDe Cooman, De Bock, & Diniz

marbles bag, subject idea marbles coloured, state
complete prior ignorance. Therefore, starts sample space {red, non-red},
observes outcomes draws, say twice non-red, consider probability
obtaining red marble next draw. due symmetry originating complete
ignorance, permute categories, calling red marbles non-red
non-red ones red, situation looking completely before,
therefore probability obtaining non-red marble next draw observing
twice red, must observing red one, observing non-red twice.
principle reminiscent Axiom A8 proposed Carnap (1952) system
inductive logic. course, reasonable principle subject prior
knowledge problem would, instance, allow impose ordering
categories.
4.4 Representation Insensitivity
shall call representation insensitivity combination pooling, renaming category
permutation invariance. means predictive inferences remain essentially unchanged
transform set categories, words insensitive
choice representationthe category set. difficult see representation
insensitivity formally characterised follows. Consider two category sets
so-called relabelling map : onto, i.e.
= (A) := {(x) : x A}. sample , corresponds transformed
sample := ((x1 ), . . . , (xn )) Dn . gamble f Dn corresponds
gamble f .
4.4.1 Representation Insensitivity
category sets onto map : D, n, n N
gambles f Dn :
considered,
= P nD (f |),

P nA (f ) = P nD (f ) P nA (f |)

(RI1)

alternatively, generally, terms predictive sets desirable gambles:
n
n
n
n
f DD

f DA
f DD
f DA
c
c.

(RI2)

also weaker combination pooling, renaming category permutation
invariance models prior observations.
4.4.2 Prior Representation Insensitivity
category sets onto map : D, n N considered
gambles f Dn :
P nA (f ) = P nD (f ),
(EI1)
alternatively, generally, terms sets desirable gambles:
n
n
f DA
f DD
.

18

(EI2)

fiCoherent Predictive Inference Exchangeability

4.5 Specificity
turn another, rather peculiar view intuitively appealing, potential property predictive inferences. Assume addition observing sample observations
n observations category set A, subject comes know determine

way n following observations belong proper subset B A, nothing
elsewe might suppose instance observation (Xn+1 , . . . , Xn+n ) made,
imperfect, allows conclude (Xn+1 , . . . , Xn+n ) B n .
impose following requirement, uses models conditioned
event B n . conditional models introduced Equations (2) (4); see
also discussion leading Equation (7), near end Section 3.
4.5.1 Specificity

category sets B B A, n, n N considered,
gambles f B n :
B n ) = P nB (f |
B ),
P nA (f |B n ) = P nB (f ) P nA (f |,

(SP1)

alternatively, generally, terms predictive sets desirable gambles:
n
n
n
n
f DB
B,
f IB n DA
f DB
f IB n DA
c
c

(SP2)

B tuple observations obtained eliminating tuple
observawhere
B empty tuple, observations
tions B. expressions,
B, posterior predictive model simply taken reduce prior predictive

model.
Specificity means predictive inferences subject makes
ones would get focussing category set B, time discarding
previous observations producing values outside B, effect retaining observations
inside B! knowing future observations belong B allows
subject ignore previous observations happened lie outside B. term
specificity context seems proposed Bernard (1997, 2005), based
work Rouanet Lecoutre (1983). so-called specific inference approach, questions,
inferences decisions involving restricted number categories, general
model replaced specific model deals categories interest,
specificity respected, general specific models produce
inferences. Specificity seems relevant principle analysing categorical data
described tree structures, case of, instance, patients classified
according symptoms (Bernard, 1997).
give simple example involving, again, Walleys bag marbles, subject
may observed, drawings, green, red, blue white marbles. asked
probability drawing red marble next, observer already seen
is, informs us either green redperhaps due bad lighting conditions
shes colour blind. subject uses specific inference model, disregard
previous observations involving colours green red.
19

fiDe Cooman, De Bock, & Diniz

4.6 Prior Near-Ignorance
use notion near-ignorance defined Walley (1991, p. 521) give following
definition prior near-ignorance context predictive inference; see also related
discussions Walley (1991, Section 5.3.2), Walley (1997, Section 3) Walley Bernard
(1999, Section 2.3). also refer paper Piatti, Zaffalon, Trojani, Hutter (2009)
interesting discussion prior near-ignorance may produce undesirable results
certain contexts.
4.6.1 Prior Near-Ignorance
prior model single variable Xk assuming values arbitrary category set
vacuous, category set A, n N considered, 1 k n gambles
f A:
P nA (extnk (f )) = min f,
alternatively, generally, terms sets desirable gambles:
n
extnk (f ) DA
f > 0,

extnk (f ) denotes cylindrical extension f gamble . defined
extnk (f )(x1 , . . . , xn ) := f (xk ) (x1 , . . . , xn ) . perhaps intuitive, less
formally correct, notation gamble f (Xk ).
Theorem 4. Prior representation insensitivity implies prior near-ignorance.
simple result implies model whose predictive previsions precise
prior representation insensitive, let alone representation insensitive, prior model
immediate predictions vacuous. shall see Section 14
nevertheless possible representation insensitive coherent inferences deploy precise
posterior predictive previsions.

5. Adding Exchangeability Picture
now, remainder paper, going add two additional assumptions.
first assumption is, principle, upper bound number
variables take account. words, considering n variables
X1 , . . . , Xn , always envisage looking one variable Xn+1 . effectively
means dealing countably infinite sequence variables X1 , . . . , Xn , . . .
assume values category set A.
n coherent
predictive inference models, means sequence DA
sets desirable gambles , n N. sequence course time-consistent
sense Requirement (5), meaning
n1
n2
n2
(n1 , n2 N)(n1 n2 DA
= margn1 (DA
) = DA
L(An1 )).

second assumption sequence variables exchangeable, means,
roughly speaking, subject believes order variables observed,
20

fiCoherent Predictive Inference Exchangeability

present themselves, influence decisions inferences make regarding
them.19
section, explain succinctly deal assumptions technically,
consequences predictive models interested in. detailed
discussion derivation results presented here, refer papers De Cooman
et al. (2009b) De Cooman Quaeghebeur (2012).
begin useful notation, employed numerous times
follows. Consider element RA . consider A-tuple, many (real)
components
x R categories x A. subset B A, denote
B := xB x sum components B.
5.1 Permutations, Count Vectors Hypergeometric Distribution
Consider arbitrary n N. denote = (x1 , . . . , xn ) generic, arbitrary element .
P n set permutations index set {1, . . . , n}. permutation ,
associate permutation , also denoted , defined ()k := x(k) ,
words, (x1 , . . . , xn ) := (x(1) , . . . , x(n) ). Similarly, lift permutation
L(An ) letting f := f , ( f )() := f ().
permutation invariant atoms [] := { : P n }, smallest permutation invariant subsets . introduce counting map : NAn : 7 (),
count vector () A-tuple components
Tz () := |{k {1, . . . , n} : xk = z}| z A,
set possible count vectors n observations given
{
}
NAn := NA
0 : = n .

(8)

(9)

Tz () number times category z appears sample . = (),
[] = { : () = }, atom [] completely determined single count
vector elements, therefore also denoted [].
also consider linear expectation operator HynA (|) associated uniform
distribution invariant atom []:
HynA (f |) :=


1
f () gambles f ,
|[]|

(10)

[]

number elements () := |[]| invariant atom [] given
multinomial coefficient:
(
) ( )

n
n!
:=
() =
=
.
(11)


zA mz !
expectation operator Equation (10) characterisesor one associated
(multivariate) hyper-geometric distribution (Johnson, Kotz, & Balakrishnan, 1997, Section 39.2), associated random sampling without replacement urn n balls
19. Exchangeability also assumed Carnaphis Axiom A7and Johnson (1924), named
permutation postulate.

21

fiDe Cooman, De Bock, & Diniz

types z A, whose composition characterised count vector . borne
0
fact that, , 0 n0 n 0 = (),
{
( 0 )/() 0
HynA (I{} |) =
0
otherwise
probability randomly selecting, without replacement, sequence n0 balls types
urn n balls whose composition determined count vector . See
also running example concrete illustration.
hyper-geometric expectation operator also seen linear transformation
HynA linear space L(An ) generally much lower-dimensional linear space
L(NAn ), turning gamble f so-called count gamble HynA (f ) := HynA (f |)
count vectors.
Running Example. order make argumentation, notions introduce
discuss, tangible concrete, shall use simple running example,
shall come back repeatedly number sections. notations assumptions made
maintained throughout series.
Consider (potentially infinite) sequence coin flips, whose successive outcomes
denote variables X1 , X2 , . . . Xn , . . . assuming values category set {H , }.
make somewhat interesting usual run-of-the-mill example, assume
stepfor coin flipNathalie selects coin bag three coins, hands
Arthur, proceeds flip it. coin put back bag next
step. subject whose beliefs modelling, may may know something
nature coins, Nathalie choosing coins subsequent flips:
might choose completely random, might specific deterministic
mechanism selecting them, . . .
= (H , , H , H ) first n = 4 observed coin flips. count
Consider sequence
corresponds sequence given components
vector ()
TH ((H , , H , H )) = 3 TT ((H , , H , H )) = 1,
= (3, 1), letting first component always refer H ,
denote
on. corresponding permutation invariant atom
[(H , , H , H )] = [(3, 1)] = {(T , H , H , H ), (H , , H , H ), (H , H , , H ), (H , H , H , )}
4!
3!1!

4
= 4 elements. set possible count vectors given N{H
,T } =
:= {(H , ), (T , H )} {H , }2
{(0, 4), (1, 3), (2, 2), (3, 1), (4, 0)}. Consider event HT
two different outcomes first two observations,

((3, 1)) =

1
1
Hy4{H ,T } (IHT
|(3, 1)) = (1 + 1 + 0 + 0) =
4
2
probability observing two different outcomes two random draws without replacement urn containing three balls marked H one ball marked , whose
composition therefore determined count vector (3, 1).

22

fiCoherent Predictive Inference Exchangeability

5.2 Multinomial Distribution
Next, consider simplex probability mass functions A:

{
}
:= RA : 0 = 1 , where, before: :=
x .

(12)

xA

probability mass function A, corresponds following multinomial
expectation operator MnnA (|):20
MnnA (f |) :=




f ()



zTz () gambles f ,

(13)

zA

characterises multinomial distribution, associated n independent trials
experiment possible outcomes probability mass function . Observe
)


( 1
f ()) ()
zmz
MnnA (f |) =
()
n
zA
NA
[]


n
mz
=
HyA (f |)()
z = CoMnnA (HynA (f )|),
n
NA

zA

used so-called count multinomial expectation operator:21


CoMnnA (g|) :=
g()()
zmz gambles g NAn .
n
NA

(14)

zA

Running Example. Consider n = 4 independent trials experiment possible outcomes
category set {H , } probability mass function = (H , ).
3
2 2
3
2
Mn4{H ,T } (IHT
|(H , )) = 2H + 4H + 2H = 2H (H + ) = 2H ,

. Observe, way, Mnn
gives probability event HT
|(H , )) =
{H ,T } (IHT
2H n 2.
gamble fHT
:= IHT
observation sequences (X1 , . . . , X4 ), corresponds
4
count gamble gHT
:= Hy{H ,T } (fHT
|) given by:
gHT
(0, 4) = 0 gHT
(1, 3) =

1
2
1
gHT
gHT
gHT
(2, 2) =
(3, 1) =
(4, 0) = 0,
2
3
2



1
2 2 2
1 3
3
CoMn4{H ,T } (g|(H , )) = 4H
+ 6H
+ 4H
= 2H
2
3
2
leads polynomial before, should.



20. avoid confusion, make (perhaps non-standard) distinction multinomial expectation,
associated sequences observations, count multinomial expectation, associated
count vectors.
21. See footnote 20.

23

fiDe Cooman, De Bock, & Diniz

5.3 Multivariate Polynomials

Let us introduce notation NA := mN NAm set possible count vectors
corresponding samples least one observation. Equation (9), also let n = 0,
turns NA0 singleton
containing null count vector 0, whose

components zero. mN0 NAm = NA {0} set possible count vectors.
count vector NA {0}, consider (multivariate) Bernstein basis
polynomial BA, degree , defined by:
(
)

mz
mz
BA, () := ()
z =
z .
(15)

zA

zA

particular, course, BA,0 = 1.
linear combination p Bernstein basis polynomials degree n 0 (multivariate)
polynomial , whose degree deg(p) n.22 denote linear space
polynomials degree n V n (A). course, polynomials degree zero simply real
constants. gathered relevant useful information multivariate polynomials
Appendix B. follows discussion that, n 0, introduce
linear isomorphism CoMnnA linear spaces L(NAn ) V n (A):
gamble g
NAn , corresponds polynomial CoMnnA (g) := CoMnnA (g|) = N n g()BA,

V n (A), conversely, polynomial p V n (A) unique gamble bnp NAn
p = CoMnnA (bnp ).23 Observe particular, n 0 NAn :
CoMnnA ({}|) = BA, () .
(16)

denote V (A) := nN0 V n (A) linear space (multivariate) polynomials ,
arbitrary degree.
set HA V (A) polynomials called Bernstein coherent satisfies
following properties:
B1. 0
/ HA ;
B2. V + (A) HA ;
B3. posi(HA ) = HA .
Here, V + (A) set Bernstein positive polynomials : polynomials p
n deg(p) bnp > 0. follows Proposition 28 Appendix B
V + (A) subset set V ++ (A) polynomials p p() > 0
interior int(A ) := { : (x A)x > 0} . consequence B1B3,
find set V0 (A) := V + (A) Bernstein negative polynomials that:
B4. V0 (A) HA = .
22. degree may smaller n sum Bernstein basis polynomials fixed degree
one. Strictly speaking, polynomials p restrictions multivariate polynomials q RA ,
called representations p. p, multiple representations, possibly different degrees.
smallest degree called degree deg(p) p.
23. Strictly speaking, Equation (14) defines count multinomial expectation operator CoMnn

n > 0, clear definition extends trivially case n = 0.

24

fiCoherent Predictive Inference Exchangeability

Finally, every Bernstein coherent set HA polynomials induces lower prevision
H V (A) defined by:
H (p) := sup { R : p HA } p V (A).

(17)

lower prevision coherent, mathematical sense satisfies coherence
requirements P1P3.24
5.4 Exchangeability Representation Theorem
ready deal exchangeability. shall give definition coherent sets
desirable gambles generalises de Finettis (1937, 1975) definition, allows
significant generalisation Representation Theorem.
First all, fix n N. subject considers variables X1 , . . . , Xn
exchangeable distinguish gamble f permuted
version f , words, gamble f f equivalent zero gamble foror
indifferent tohim. means so-called set indifferent gambles:
{
}
n
:= f f : f L(An ) P n .
IA
n , set must compatible
subject also coherent set desirable gambles DA
n , sense must satisfy rationality
set indifferent gambles IA
n
n
n
requirement DA + IA = DA ; see detailed explanations justifications De Cooman
Quaeghebeur (2012) Quaeghebeur et al. (2014) so-called desiring sweetened
n ,
deals requirement. say sequence X1 , . . . , Xn , model DA
exchangeable.
Next, countably infinite sequence variables X1 , . . . , Xn . . . called exchangeable
n, n N
finite subsequences X1 , . . . , Xn are, n N. means models DA
exchangeable. course also time-consistent.
formulate powerful generalisation de Finettis (1937, 1975) Representation
Theorem, straightforward compilation various results proved De Cooman
Quaeghebeur (2012):

Theorem 5 (Representation Theorem, De Cooman & Quaeghebeur, 2012). sequence
n desirable gambles , n N coherent, time-consistent exchangeable
sets DA
Bernstein coherent set HA polynomials
NA
[]:

n N, gambles f ,
n
n
MnnA (f )BA,
f DA
MnnA (f ) HA f DA
c
HA .

case representation HA unique given HA :=



(18)

n
n
nN MnA (DA ).

follows Condition (18) HA completely determines predictive inferences
n
sequence variables X1 , . . . , Xn , . . . , fixes prior predictive models DA
24. Actually, suitably adapted version, underlying possibility space need longer finite
(Walley, 1991; Troffaes & De Cooman, 2014), domain restricted polynomials
(De Cooman & Quaeghebeur, 2012).

25

fiDe Cooman, De Bock, & Diniz

n c.
25 tells us representation HA set
posterior predictive models DA
polynomials plays role probability measure, density, distribution
function, precise-probabilistic case.
Indeed, corresponding coherent lower prevision H V (A) given Equation (17),
shown determine convex closed (compact) set

M(H ) := {HA : (p V (A))HA (p) H (p)}
coherent previsions HA V (A) (Walley, 1991; De Cooman et al., 2009b; De Cooman &
Quaeghebeur, 2012; Troffaes & De Cooman, 2014). pointed footnote 2and
come back footnote 36each coherent prevision HA uniquely
determines -additive probability measure Borel sets , therefore set
polynomials HA , via M(H ), uniquely determines set probability measures. But,
argued before, HA informative H M(H ), problems
conditioning sets lower probability zero: Bernstein coherent set polynomials
HA determines unique lower prevision H , therefore M(H ) unique set
probability measuresand densities absolutely continuouson simplex ,
converse necessarilyand usually notthe case. set probability densities
used define coherent set polynomialswe provide example
Section 12but generally one coherent set polynomials
leads set densities, updating behaviour different sets
polynomials different conditioning events lower probability zero.
n c
depend
Condition (18) also tells us posterior predictive models DA
count vector
= ():
count vectors sufficient
observed sequence
statistics exchangeability. reason, shall denote posterior
n c
n c.
well DA
Also, every then, shall use
predictive models DA
n
n
DA c0 alternative notation DA .
immediate interesting consequence Theorem 5 updating observations
preserves exchangeability: observing values first n variables, count
remaining sequence variables Xn+1 , Xn+2 , . . . still exchangeable,
vector ,
Condition (18) tells us representation given Bernstein coherent set
defined by:
polynomials HA c
:= {p V (A) : BA,
HA c
p HA } .

(19)

compare Expressions (2) (6), tells us that, essentially, Bernstein
basis polynomials serve likelihood functions updating sets polynomials. use
refer coherent lower prevision V (A) derived HA c
means
H (|)
= 0, find HA c0 = HA H (|0) = H .
Equation (17). special case
related following version Generalised
Observe H H (|)
Bayes Rule:

H ([p H (p|)]B
(20)
) = 0 p V (A).
A,
completely determined HA . One consider HA prior model
Clearly, HA c
plays role posterior derived it.
parameter space , HA c
25. contrasted usual precise-probabilistic version, posterior predictive
models uniquely determined observed sequences non-zero probability; see also footnote 3.

26

fiCoherent Predictive Inference Exchangeability

see Condition (18) Equation (19) thatsimilarly happens preciseprobabilistic settingthe multinomial distribution serves direct link one
n and, hand,
hand, prior HA prior predictive inference models DA
n c.
posterior predictive inference models DA
Recalling
posterior HA c

NA {0}:
convention = 0, summarise follows: n N
{
}
n
= f L(An ) : MnnA (f ) HA c

DA
c
(21)
and, immediate consequence:
{
}
= sup R : MnnA (f ) HA c
f L(An )
P nA (f |)

(22)

or, equivalently:
= H (MnnA (f )|)
f L(An ).
P nA (f |)

(23)

practical point view, Equation (23) often easier work Equa often admit simpler expression
tion (22), shall see on, H (|)
compare Equations (45), (54), (61) (69) Equations (49), (55), (65)
HA c;
always uniquely determined H : relaand (73), respectively. But, H (|)
uniquely H prior lower probability
tion (20) allows us determine H (|)

H (BA,
)

observing

non-zero.
Therefore,
sets polynomials HA


uniquely. quite dramatic
fundamental models, allow us determine HA c
illustration this, shall Sections 11, 13 14 come across number
quite different inference systemswith different HA give rise prior H

different posterior H (|)!
Running Example. assume subject assesses sequence coin flips
exchangeable, finds desirable gamble type I{H } (Xn ), fixed
(0, 1]; upper probability observing heads coin flip . Since
infer Equation (13) N n, MnN
{H ,T } (I{H } (Xn )|) = H , infer
Theorem 5 assessment corresponds following coherent set polynomials:
{
}
H := 1 p+ + 2 ( H ) : p+ V + ({H , }), 1 , 2 R0 max{1 , 2 } > 0 ,
smallest Bernstein coherent set polynomials contains polynomial
H ; explanation, see also discussions De Cooman et al. (2009b)
De Cooman Quaeghebeur (2012). followsafter manipulationsfrom
Equation (17) Proposition 28 corresponding lower prevision V ({H , })
completely determined following optimisation:
H (p) = sup

min [p() + (H )]

0 {H ,T }

given
Hence, lower probability event HT
H (2H ) = sup min [2x(1 x) + (x )] = 0,
0 x[0,1]

upper probability
H (2H ) = H (2H ) = inf max [2x(1 x) (x )]
0 x[0,1]

27

fiDe Cooman, De Bock, & Diniz

=

{
2(1 )
1
2

12
otherwise.


tells us exchangeability alone already guarantees upper probability HT
1
2 . three coins bag assumed biased towards heads, < 12 ,
upper probability drops 12 .

finish section representation, want stress polynomials
given behavioural interpretation gambles may may desirable:
merely mathematical representational tools help us characterise
gambles observation sequences desirable.26 Similarly, set polynomials HA
lower prevision H merely mathematical tools allow convenient
representation predictive models observation sequences.
Running Example. illustrate polynomial representation much convenient
efficient, recall want make inferences sequence coin flips
length n, need work sets desirable gambles {H , }n , words,
cones 2n -dimensional space. work polynomial representations,
led consider cones polynomials degree n, constitute linear
space spanned n + 1 Bernstein basis polynomials degree n, therefore
n + 1-dimensional. Working polynomial representations therefore leads
dramaticexponentialreduction complexity.


6. Reasoning Inference Systems
seen previous section that, fix category set A, predictive inferences
exchangeable sequences assuming values completely determined Bernstein
coherent set HA polynomials . way associating Bernstein
coherent set HA every possible set categories A, would completely fix predictive
inferences. leads us following definition.
Definition 6 (Inference Systems). denote F collection category sets, i.e. finite
non-empty sets. inference system map maps category set F
set polynomials (A) = HA . inference system called coherent
category sets F, (A) Bernstein coherent set polynomials .
So, coherent inference system way systematically associate coherent predictive
inferences category set. Since inference principles Section 4 impose connections
predictive inferences different category sets, see interpret
inference principlesor rather, represent mathematicallyas properties of,
restrictions on, coherent inference systems. shall Section 7,
provides one important motivation introducing systems. Another, equally
26. makes operational, behavioural sense consider notion accepting polynomial, finding
desirable. much like classical case, de Finetti (1975) probability distributions
simplex used mathematical representations, direct behavioural
meaningalthough Bayesians less careful foundations de Finetti might care make
distinction.

28

fiCoherent Predictive Inference Exchangeability

important reason so, allows us extend method natural extension
conservative inferenceintroduced Section 2.2, also take account inference
principles predictive inference, generally, predictive inference multiple category
sets once.
see comes about, let us show conservative reasoning
inference systems. two inference systems 1 2 , say 1 less committal
conservativethan 2 , write 1 v 2
(A F)1 (A) 2 (A).
simply means predictive inferences category set less committal
first second inference system. denote set inference
systems, clearly set partially ordered v. Actually, complete lattice,
infimum supremum non-empty family , given by:
(

)
(
)


inf (A) =
(A) sup (A) =
(A) category sets A.
iI

iI

iI

iI

denote C set coherent inference systems:
C := { : (A F)(A) Bernstein coherent} .

(24)

clear C complete meet-semilattice, meaning closed arbitrary
non-empty infima:27
(i I)i C inf C.
(25)
iI

bottom structurethe conservative coherent inference systemis called
vacuous inference system V , coherent inference system given by:
V (A) = V + (A) category sets A.
shall come back detail vacuous inference system Section 9.
property (25) allows us conservative reasoning coherent inference systems.
Suppose, instance, collection category sets F F, assessments
form set polynomials AA V (A), F. Then, exists,
conservative coherent inference system compatible assessments given
by:
= inf { C : (A F)AA (A)} .
And, course, exist set polynomials AA included
Bernstein coherent set polynomials HA A, F. case, difficult
see, given discussion Section 5.3, (A) = posi(V + (A) AA ) F
(A) = V + (A) F \ F.
27. necessarily closed suprema, however, union Bernstein coherent sets polynomials
need Bernstein coherent.

29

fiDe Cooman, De Bock, & Diniz

7. Representation Insensitivity Specificity Exchangeability
Let us investigate form inference principles representation insensitivity (RI2)
specificity (SP2) take predictive inference exchangeability, inference
completely characterised Bernstein coherent sets polynomials. allow us
reformulate principles constraints onor properties ofinference systems.
7.1 Representation Insensitivity
recall notations assumptions Section 4.4. surjective (onto) map
: associate surjective map R : RA RD letting:
R ()z :=



x

RA z D.

(26)

xA : (x)=z

map allows us give following elegant characterisation representation insensitivity.
Theorem 7. coherent inference system representation insensitive
category sets onto map : D, p V (D)
NA {0}:
(p R )BA, (A) pBD,R () (D).
(RI3)
Running Example. Assume coins bag actually rather thick, implying
non-negligible chance fall one flat sides,
remain upright. denote new state U , new category set
:= {H , , U }. also consider new flat state F , meaning either heads tails,
also consider, instead A, category set := {F , U } distinguish
heads tails. relabelling map (H ) := (T ) := F (U ) := U
identifies proper relations categories D.
Suppose want say something lower probability event
observing U one flip H other, immediately observing
UF
sequence (H , U , H , ) count vector = (2, 1, 1)the last count three
refers number U observation sequence. A-domain, gamble IUF

n
28
expressed polynomial q = Mn{H ,T ,U } (IUF
), n 2 given by:
q() = 2(H + )U {H ,T ,U } .
2 belong
want find whether polynomials type [2(H + )U ]12H
U
({H , , U }); see Equation (18).
hand, seen previously, D-domain, gamble IUF

expressed polynomial p given p() = 2F U {F ,U } . Observe
q = p R . count vector = (2, 1, 1) A-domain corresponds count vector
R () = (3, 1) D-domain, first component refers number F
second number U s. here, need check whether polynomials type
[2F U ]43F U belong ({F , U }).

28. similar contexts, easy check polynomial remains n 2.

30

fiCoherent Predictive Inference Exchangeability

nice thing representation insensitivity makes checking whether
2
polynomials type [2(H + )U ]12H
U belong ({H , , U }) Adomain equivalent checking whether polynomials type [2F U ]43F U belong
({F , U }) D-domain.

interestingly, representation insensitivity preserved taking arbitrary nonempty infima coherent inference systems, allows us look conservative
representation insensitive coherent inference system compatible assessment
F, way straightforward extension discussion near end Section 6.
Theorem 8. Consider non-empty family , representation insensitive coherent
inference systems. infimum inf iI representation insensitive coherent
inference system well.
7.2 Specificity
Next, turn specificity, recall notations assumptions Section 4.5. Let us
define surjective restriction map rB : RA RB by:
rB ()z := z RA z B,

(27)

particular, rB () count vector B obtained restricting B (indices
the) components count vector A. also define one-to-one injection map
iA : RB RA by:
{
x x B
iA ()x :=
RB x A.
(28)
0
otherwise
map used define following one-to-one maps IrB,A : V (B) V (A),
r N0 , follows:

IrB,A (p) :=
bdeg(p)+r
()BA,iA () polynomials p V (B).
(29)
p
deg(p)+r

NB

derive meaning following observation. polynomial p B
equivalently represented Bernstein basis B degree deg(p) + r.
interpret different representations polynomials , longer equivalent,
lead different polynomials IrB,A (p), r N0 . following propositions clarify
exactly effect operator IrB,A is.
Proposition 9. polynomial p B r N0 : IrB,A (p) iA = p.
introduce following notation, B > 0: |+
B := rB ()/B .
Observe |+


whenever

>
0.
B
B
B
Proposition 10. Consider polynomial p B , r N0 .
deg(p) + r = 0 p = c R, IrB,A (p|) = c. Otherwise, deg(p) + r > 0:
{
deg(p)+r
B
p(|+
B > 0
B)
IrB,A (p|) =
0
otherwise.
31

fiDe Cooman, De Bock, & Diniz

maps IrB,A allow us give following elegant characterisation specificity:
Theorem 11. coherent inference system specific category sets
B B A, p V (B), NA {0} r N0 :
IrB,A (p)BA, (A) pBB,rB () (B).

(SP3)

Running Example. Suppose, before, made observation (H , U , H , ),
count vector = (2, 1, 1). interested posterior lower probability
, somebody told us neither two subsequent coin flipsafter
event HT
first fourresulted U . specific inference system, allowed consider
predictive inference problem reduced category space B = {H , }, rather
category space = {H , , U }. then, B-space, use reduced count
vector rB () = (2, 1), obtained leaving number observed U s. polynomials
lead consider here, therefore type [2H ]32H , want
know whether belong ({F , U }).
A-space, polynomial p() = 2H , whose degree deg(p) = 2,
transformed polynomials
]
[

H
r
IB,A (p|) = 2
(H + )2+r = [2H (H + )2 ](H + )r
H + H +
r N0 . follows argumentation proof Theorem 11 original
problem requires us check whether polynomials type
2
[2H (H + )2 ](H + )r 12H
U

({H , , U }). Specificity allows us look problem B-space,
easier.

Observe close formal similarity conditions (RI3) (SP3).
therefore surprise us specificity, too, preserved taking arbitrary non-empty
infima inference systems.
Theorem 12. Consider non-empty family , specific coherent inference systems.
infimum inf iI specific coherent inference system well.
Let us denote Crs set coherent inference systems representation
insensitive specific. follows Theorems 8 12 Crs , like C, closed
arbitrary non-empty infima, perform conservative reasoning, much
way discussed near end Section 6.

8. Immediate Prediction
inference system , look special case immediate prediction,
given category set A, observing sample n 0 variables count vector
NAn , want express beliefs value next observation Xn+1

assume A. specific case predictive inference n = 1, Condition (18)
NAn :
simplified somewhat, gambles f
1
1
BA,
f DA
SA (f ) (A) f DA
c
SA (f ) (A),

32

fiCoherent Predictive Inference Exchangeability

let
so-called sampling expectation SA (f ) linear polynomial given
SA (f |) := xA f (x)x .
reason NA1 = {x : x A} x count vector corresponding
single observation category x, words, exz = xz z [Kronecker
delta]. Hence, x :
( )
( )
1
1
x
x
f
(z)
=
f
(x)

B
()
=
zez = x ,
Hy1A (f |x ) =
A,
x
x

x
zA

z[ ]

leading to:
Mn1A (f |) =



Hy1A (f |x )BA,x () =

1
x NA



f (x)x = SA (f |).

(30)

xA

matter straightforward verification that, due Bernstein coherence HA ,
1 c
coherent set desirable gambles A,
so-called immediate prediction model DA
NA {0}. induces following predictive lower previsions:
every count vector
{
}
1
= sup R : f DA

P 1A (f |)
c
= sup { R : [SA (f ) ]BA,
(A)} .

(31)

Immediate prediction context exchangeable imprecise probability models
studied detail De Cooman et al. (2009a). Lower previsions, rather
sets desirable gambles, model choice paper, that,
authors encountered problems conditioning sets (lower) probability zero. fact,
problems provided motivation dealing much general
problem (not necessarily immediate) predictive inference using sets desirable gambles
present paper. section, want illustrate many results proved
made stronger (and easier proofs, borne Appendix E.3)
present context.
requirement (RI2) representation insensitivity reduces following simpler
requirement immediate prediction models: category sets
onto map : D, gambles f NA {0}:
1
1
f DA
c f DD
cR ().

(RI4)

Similarly, requirement (SP2) specificity reduces following simpler requirement
immediate prediction models: category sets B B A,
gambles f B NA {0}:
1
1
f IB DA
c f DB
crB ().

(SP4)

Let us show simple characterisation immediate prediction
models satisfy representation insensitivity. get there, observe consider
gamble g category set (surjective) pooling map finite subset
g(A) Ralso category set. corresponding Rg : RA Rg(A) given by:

Rg ()r =
x r g(A).
xA : g(x)=r

33

fiDe Cooman, De Bock, & Diniz

simple idea allows intriguing reformulation representation insensitivity
requirement immediate prediction models:
Proposition 13. immediate prediction models associated coherent inference
system representation insensitive category sets A, gambles g
count vectors NA {0}:
1
1
g DA
c idg(A) Dg(A)
cRg ().

(RI5)

Here, non-empty set B, denote idB identity map B, defined idB (z) := z
z B.
Proposition 13 tells us whether gamble desirable depends values
assumesand assumedand number times
values observed pastor rather would observing
g(Xk ) rather Xk .
Let us focus happens events. Consider event B nontrivial meaning B neither empty equal A. real gamble
IB assumes two values, 1 , see applying Proposition 13
NA {0}:
1
1
IB DA
c id{1,} D{1,}
c(mB , mA\B ),

therefore
{
}
1
P 1A (B|) = sup R : IB DA
c
{
}
1
= sup R : id{1,} D{1,}
c(mB , mA\B ) =: (mA , mB ),

(32)
(33)

meaning that, representation insensitivity, predictive lower probability non-trivial
event B depends number times mB observed past
experiments, total number observations . thing holds predictive
upper probability 1 (mA , mB ). precise predictive probabilities, similar property
known Johnsons sufficientness postulate (Johnson, 1924; Zabell, 1982).
representation insensitive{coherent inference system,
see define
}
2
so-called lower probability function : (n, k) N0 : k n [0, 1] Equation (33),
completely characterises one-step-ahead predictive lower upper probabilities29
non-trivial events count vectors. shall use representation insensitivity
specificity requirements try say lower probability function.
following theorem strengthens, simplifies, extends similar results De Cooman et al.
(2009a).
Theorem 14. Consider representation insensitive coherent inference system .
associated lower probability function following properties:
L1. bounded: 0 (n, k) 1 n, k N0 k n.
L2. super-additive second argument: (n, k + `) (n, k) + (n, `)
n, k, ` N0 k + ` n.
29. . . . necessarily predictive lower upper previsions . . .

34

fiCoherent Predictive Inference Exchangeability

L3. (n, 0) = 0 n N0 .
L4. (n, k) k(n, 1) n(n, 1) 1 n, k N0 1 k n.
L5. non-decreasing second argument: (n, k) (n, `) n, k, ` N0
k ` n.
L6. (n, k) (n + 1, k) + (n, k)[(n + 1, k + 1) (n + 1, k)] n, k N0
k n.
L7. non-increasing first argument: (n + 1, k) (n, k) n, k N0
k n.
L8. Suppose (n, 1) > 0 n N, let sn :=
1
. sn 0 sn+1 sn .
(n, 1) = n+s
n

1
(n,1)

n, equivalently,

moreover specific, following properties:
n
L9. Consider real (0, 1) suppose (1, 1) , (n, n) 1+n

1
n N0 . consequence, consider > 0 suppose (1, 1) 1+s ,
n
(n, n) n+s
n N0 .

know Theorem 4 representation insensitive coherent inference systems
near-ignorant, meaning vacuous therefore completely indecisive
single observation prior observations made. also borne
Theorem 14.L3. Let us define imprecision function
(n, k) := 1 (n, n k) (n, k) n, k N0 k n.

(34)

1

clear P (B|) P 1A (B|) = (mA , mB ) width probability interval
event B observed mB times. representation
insensitive coherent inference system whose imprecision function (n, k) satisfies following
property:
}
(n + 1, k) (n, k)
0 k n,
(35)
(n + 1, k + 1) (n, k)
imprecision increase total number observations increases. suggests
representation insensitive coherent inference systems display
desirable behaviour mentioned Introduction: conservative little
learned, never become less precise observations come in. following
sections, intendamongst thingsto take closer look whether behaviour
present number systems.
Immediate prediction important predictive inference precise probabilities,
Law Total Probability guarantees completely determined immediate
predictions. Perhaps surprisingly, case predictive inference imprecise
probabilities: Appendix provides counterexample. also points
limitations scope earlier work De Cooman et al. (2009a). reason,
leave immediate prediction models are, rest paper concentrate
general notion inference system.
35

fiDe Cooman, De Bock, & Diniz

9. Vacuous Inference System
following sections, provide explicit interesting examples representation insensitive, specific coherent inference systems. begin simplest
one: vacuous inference system V , introduced Section 6 smallest,
conservative, coherent inference system. associates category set
smallest Bernstein coherent set V (A) = HV,A := V + (A) containing Bernstein positive
polynomialsthe ones guaranteed anyway, Bernstein coherence alone.
deduce Proposition 30 Appendix B that:
= HV,A = V + (A)
NA {0},
HV,A c
Proposition 28 Appendix B that:
{
}
= H V,A (p) = sup R : p V + (A)
H V,A (p|)
= min p = min p() p V (A).


predictive models inference system straightforward find,
NA {0},
follow directly Equations (21) (23). n N
deduce that:
{
}
n
n
= f L(An ) : MnnA (f ) V + (A) ,
DV,A
= DV,A
c
(36)

= min MnnA (f |) f L(An ).
P nV,A (f ) = P nV,A (f |)


(37)

particular:
1
1
= L>0 (A),
DV,A
= DV,A
c

P 1V,A (f )

=


P 1V,A (f |)

(38)

= min f f L(A),

(39)


V (n, k) = 0 n, k N0 k n.

(40)

conservative exchangeable predictive models are, arise
making assessments exchangeability alone. gather Equations (36)
(40), interesting, involve non-trivial commitments,
allow learning observations. also borne corresponding
imprecision function, given by:
V (n, k) = 1 n, k N0 k n.
Running Example. seen Mnn{H ,T } (IHT
|) = 2H n 2,
therefore
n
P nV,{H ,T } (IHT
) = P V,{H ,T } (IHT
|(3, 1)) =

min

Mnn{H ,T } (IHT
|) =

max

Mnn{H ,T } (IHT
|) =

{H ,T }

min

{H ,T }

2H = 0


n

n

P V,{H ,T } (IHT
) = P V,{H ,T } (IHT
|(3, 1)) =

{H ,T }

36

1
2H = .
{H ,T }
2
max

fiCoherent Predictive Inference Exchangeability

shows vacuous inference model produce completely vacuous inferences:
allows us find consequences making assessments exchangeability.
allow us change lower upper probabilities previsions
new observations come in.

Even though makes non-trivial inferences, vacuous inference system satisfies
representation insensitivity, specific.
Theorem 15. vacuous inference system V coherent representation insensitive.
Let us show means counterexample V specific,
Running Example. Let us go back inferences category space = {H , , U }
reduced category space B = {H , }. Consider polynomial p() = 2H H + 2T
{H ,T } . polynomial Bernstein positiveso p V + ({H , })because
p() = (2H H + 2T )(H + ) = 3H + 3T
expansion Bernstein basis degree 3 positive. let us consider
corresponding polynomial {H ,T ,U } :
2
2
q() := I0B,A (p|) = H
H +
.

(41)

polynomial Bernstein positive: easy see every n N0 ,
2
2
q() = (H
H +
)(H + + U )n
n . q = I0 (p)
always term H U
/ V + ({H , , U }), infer
B,A
Theorem 11 V cannot specific.


following sections, shall prove infinity committal,
specific representation insensitive coherent inference systems. begin introducing
slightly modified version vacuous inference system coherent, representation
insensitive specific.

10. Nearly Vacuous Inference System
Let us introduce nearly vacuous inference system NV reason name
become clear presentlyby:
NV (A) := HNV,A := V ++ (A) := {p V (A) : ( int(A ))p() > 0}
category sets A.
Since V ++ (A) consists polynomials positive int(A ), deduce
NA {0}: HNV,A c
= HNV,A = V ++ (A)
Proposition 28 Appendix B that,
that:
= H NV,A (p) =
H NV,A (p|)

p() = min p() p V (A).

inf
int(A )

37



fiDe Cooman, De Bock, & Diniz

Since know Proposition 28 Appendix B, counterexample following it,
generally speaking V + (A) V ++ (A), see inference system less conservative
vacuous one. case vacuous inference system, predictive models
nearly vacuous inference system straightforward find, follow directly
NA {0}, deduce that:
Equations (21) (23). n N
{
}
n
n
= f L(An ) : MnnA (f ) V ++ (A) ,
DNV,A
= DNV,A
c

= min MnnA (f |) f L(An ).
P nNV,A (f ) = P nNV,A (f |)


particular:
1
1
= L>0 (A),
DNV,A
= DNV,A
c

= min f f L(A).
P 1NV,A (f ) = P 1NV,A (f |)
see immediate prediction models, predictive lower previsions,
inference system exactly ones vacuous inference systems.30
allow learning observations.
Interestingly, contrast vacuous inference system, nearly vacuous
inference system specific, already tells us Crs 6= .
Theorem 16. nearly vacuous inference system NV coherent, representation insensitive specific: NV Crs .

11. Skeptically Cautious Inference System
construct rather simple inference system quite intuitive slightly
informative vacuous nearly vacuous ones. Suppose subject uses
following system making inferences based sequence n > 0 observations count
category set A. skeptical believes future,
vector ,
observe categories seen previously, categories set:
:= {x : mx > 0} .
A[]

(42)

also cautious, beliefs already observed categories
observed future, nearly vacuous. explain this, assume first
particular, n future observations, vacuous beliefs count vector
observe set
{
}
n
n


NA : (y \ A[])my = 0 = NA[

]
holds possible observing count vector ,

future count vectors
31 Lemma 47 Appendix B,
namely count vectors observation outside A[].
30. first example shows immediate prediction models completely determine
inference system. shall come across another example Appendix D.
31. last equality equation actually device allows us identify count vectors
zero, count vectors A[].
shall using repeatedly,
whose components outside A[]
without explicit mention, rest paper.

38

fiCoherent Predictive Inference Exchangeability

would lead us associate following set polynomials count vector NA :
{
}
+
n
V[]
(A) := p V (A) : (n deg(p)) bnp |NA[]
>0
{
}
+
= p V (A) : p|A[] V (A[]) .
But, already know vacuous models V + (A) lead specific systems,
whereas nearly vacuous models V ++ (A) do, modify slightly, rather
associate following set polynomials count vector NA :
{
}
++
V[]
(A) := p V (A) : p|A[] V ++ (A[]) .
++
polynomials V[]
(A) desirable representation32 observing sample
count vector , infer Equation (19) subject considers desirable
representation polynomials in:
{
}
++
++
V[]
(A)BA, = pBA, : p V[]
(A) .

thus led consider following assessment:

++
ASC,A :=
V[]
(A)BA, ,
NA

set positive linear combinations:
HSC,A

:= posi (ASC,A ) =

{
`

pk BA,k : ` N, nk N, k

NAnk , pk

}



++
V[
(A)
k]

. (43)

k=1

following proposition guarantees sets HSC,A appropriate conservative
models summarise exchangeable inferences skeptically cautious subject.
Proposition 17. HSC,A smallest Bernstein coherent set polynomials
includes ASC,A .
also shows inference system SC , defined SC (A) := HSC,A category
sets A, coherent. shall call skeptically cautious inference system.
want find updating works system. end, introduce
slight generalisation set defined Equation (43). Consider NA {0}, let
HSC,A, :=

{
`

pk BA,k : ` N, nk N0 , + nk > 0, k

NAnk , pk

}



++
V[+
(A)
k]

,

k=1

(44)
see that, particular, HSC,A = HSC,A, = 0.
sets HSC,A, following interesting characterisation:
32. stated before, polynomials direct behavioural indirect representational meaning,
conveniently condensed representations desirable gambles observation sequences. Hence
caution using term desirable representation.

39

fiDe Cooman, De Bock, & Diniz

Proposition 18. NA {0}:
{
}
HSC,A, = p V (A) \ {0} : (K min SA, (p))p|K V ++ (K) ,

(45)


{
}
SA, (p) := =
6 K : A[] K p|K 6= 0 .

(46)

min SA, (p) mean set minimal, non-dominating, elements SA, (p),
min SA, (p) := {C SA, (p) : (K SA, (p))(K C K = {C)}. formally extend
}
Equation (42) include case = 0, A[0] = SA,0 (p) = =
6 K : p|K 6= 0 .
Proposition 19. NA {0}: HSC,A c = HSC,A, .
combining result Equation (21), deriveadmittedly rather involved
expressions predictive sets desirable gambles skeptically cautious inference
NA {0}:
system. n N
{
}
n
= f L(An ) : MnnA (f ) HSC,A,
c
(47)
DSC,A
.
NA :
immediate prediction, expressions simplify significantly.
{
}
1
1
= f L(A) : f |A[]
DSC,A
= L>0 (A) DSC,A
c
> 0 L>0 (A).

(48)

NA :
lower previsions derived HSC,A,
tractable.
=
H SC,A (p) = min p(x ) H SC,A (p|)
xA

min p() p V (A),

A[]


(49)

where, x A, x degenerate probability mass function assigns
probability mass x.
predictive lower previsions skeptically cautious inference system
NA :
easily obtained combining Equations (49) (23). n N
=
P nSC,A (f |)

min MnnA (f |) f L(An )

A[]


(50)


P nSC,A (f ) = min f (x, x, . . . , x) f L(An ).

(51)

= min f (x) f L(A).
P 1SC,A (f ) = min f P 1SC,A (f |)

(52)

xA

particular:

xA[]

lower probability function given by:
{
1 k = n > 0
SC (n, k) =
0 otherwise

n, k N0 k n,

corresponding imprecision function by:
{
1 n = 0 0 < k < n
SC (n, k) =
0 otherwise
40

n, k N0 k n.

fiCoherent Predictive Inference Exchangeability

Running Example. before, Mnn{H ,T } (IHT
|) = 2H n 2. also take
account {H , }[(3, 1)] = {H , }, get:
n
P nSC,{H ,T } (IHT
) = P SC,{H ,T } (IHT
|(3, 1)) =

min

Mnn{H ,T } (IHT
|) =

{H ,T }

max

Mnn{H ,T } (IHT
|) =

{H ,T }

{H ,T }

min

2H = 0

max

1
2H = .
2


n

n

P SC,{H ,T } (IHT
) = P SC,{H ,T } (IHT
|(3, 1)) =

{H ,T }

categories observed count vector (3, 1)meaning {H , }[(3, 1)] =
{H , }we find inferences vacuous inference system.

Interestingly, coherent inference system SC also satisfies representation insensitivity specificity.
Theorem 20. skeptically cautious inference system SC coherent, representation
insensitive specific: SC Crs .

12. IDMM Inference Systems
Imprecise Dirichlet Modelsor IDMs, shortare family parametric inference models
introduced Walley (1996) conveniently chosen sets Dirichlet densities diA (|)
constant prior weight s:
{
}
{diA (|) : KsA } , KsA := RA
(53)
>0 : = = {s : int(A )} ,
value (so-called) hyperparameter R>0 category set A. Dirichlet
densities diA (|) defined int(A ); see Appendix C explicit definition
extensive discussion.
IDMs generalise Imprecise Beta models introduced earlier Walley (1991).
later paper, Walley Bernard (1999) focussed closely related family predictive
inference models, called Imprecise Dirichlet Multinomial Modelsor IDMMs,
short.33 refer papers, recent overview paper Bernard
(2005) extensive motivating discussion IDM(M)s, inferences properties.
precise Dirichlet models expectations, related Dirichlet multinomial
models, gathered Appendix C important facts, properties results,
necessary proper understanding present discussion IDM(M)s context
inference systems.
One reasons Walley (1996) suggesting IDM reasonable model
precisely satisfies pooling34 invariance properties discussed Section 4.1.
also discussed emphasis Walley Bernard (1999) Bernard (2005),
know detailed explicit formulations properties literature,
proofs seen fairly sketchy. Bernard (1997, 2005) also suggests IDM
33. later paper, Walley Bernard (1999) clearly distinguish name parametric IDMs
predictive IDMMs, earlier paper Walley (1996), types models referred
IDMs.
34. Walley uses term representation invariance rather pooling invariance.

41

fiDe Cooman, De Bock, & Diniz

underlying precise Dirichlet models satisfy so-called specificity property,
tried translate present context predictive inference Section 4.5.
present section, use ideas behind Walley Bernards IDM(M)s construct
interesting family coherent inference systems, give detailed formal proof
Appendix E fact inference systems indeed representation insensitive
specific. Interestingly, shall need slightly modified version Walleys IDM(M)
make things work. reason Walleys original version, described
Expression (53), number less desirable properties, seem either
unknown to, ignored by, Walley Bernard. describe shortcomings
detail Appendix D. present purposes, suffices mention that, contrary
often claimed, contradistinction new version, inferences using original
version IDM(M) necessarily become conservative (or less committal)
hyperparameter increases.
version, rather using hyperparameter sets KsA , consider sets
{
}
sA := RA
>0 : < R>0 .
Observe
{
}
sA = s0 : s0 R>0 , s0 < int(A ) =



0

KsA .

0<s0 <s

R>0 , category set A, consider following set polynomials
p, positive Dirichlet expectation DiA (p|) hyperparameters sA :

:= {p V (A) : ( sA ) DiA (p|) > 0} .
HIDM,A

shall see Theorem 21 set Bernstein coherent. call inference
system sIDM , defined by:

sIDM (A) := HIDM,A
category sets A,

IDMM inference system hyperparameter > 0. corresponding updated models
NA {0}, given by:
are,

= {p V (A) : ( sA ) DiA (p|
+ ) > 0}
HIDM,A
c

(54)

= inf DiA (p|
+ ) p V (A).
H sIDM,A (p|)

(55)




Using expressions, predictive models IDMM inference system straightforward find; suffices apply Equations (21) (23). n N
NA {0}:

{
}
s,n
= f L(An ) : ( sA ) DiA (MnnA (f )|
+ ) > 0 ,
DIDM,A
c
(56)

= inf DiA (MnnA (f )|
+ ) f L(An ),
P s,n
IDM,A (f |)


42

(57)

fiCoherent Predictive Inference Exchangeability

where, using notations introduced Appendix C:
+ ) = DiMnnA (HynA (f )|
+ )
DiA (MnnA (f )|
( )

n
1
n

=
HyA (f |)
(mx + x )(mx ) .
(n)

(mA + )
n
xA

N

(58)



general, expressions seem forbidding, immediate prediction models
NA {0}:
manageable enough.
{
}
1
s,1
= f L(A) : f >
(59)
DIDM,A c
f (x)mx ,

xA

1


P s,1
(f
|
)
=
f (x)mx +
min f f L(A),
(60)
IDM,A
+
+
xA



k
n, k N0 k n.
n+s
corresponding imprecision function given by:
sIDM (n, k) =

sIDM (n, k) =


n, k N0 k n,
n+s

decreasing first constant second argument, implies
satisfies Condition (35). suggests IDMM inference systems conservative
little learned, become precise observations come in.
Running Example. before, Mnn{H ,T } (IHT
|) = 2H n 2, find that,
using results Appendix C:
)
(

Di{H ,T } Mnn{H ,T } (IHT
|) =

2H
.
(H + )(H + + 1)

difficult verify using Equation (57) 0 < s:
s,n

P s,n
) = 0 P IDM,{H ,T } (IHT
) =
IDM,{H ,T } (IHT

1
.
21+s

observing count vector (3, 1), find manipulations that:
2(3 + )
2(3 + s)
=
,
0<<s (4 + )(5 + )
(4 + s)(5 + s)

P s,n
|(3, 1)) = inf
IDM,{H ,T } (IHT
similarly:




6

1+s
(4 + s)(5 + s)
s,n
P IDM,{H ,T } (IHT
|(3, 1)) =

1
4+s


25+s

2
2.

Observe infinitely large s, recover inferences vacuous system.
43



fiDe Cooman, De Bock, & Diniz

Interestingly, immediate prediction models version IDMM inference
system coincide Walleys original version. Hence, many practical applications concerned immediate prediction only, approaches yield identical
results.
IDMM inference systems constitute uncountably infinite family coherent
inference systems, satisfies representation insensitivity specificity
requirements.
Theorem 21. R>0 , IDMM inference system sIDM coherent, representation
insensitive specific: sIDM Crs .

Since Crs closed non-empty infima, infimum
IDM IDM , > 0
still coherent, representation insensitive specific, conservative
IDMM inference systems. given by:
{
}
+++

(A) := p V (A) : ( RA
IDM (A) = V
>0 ) DiA (p|) > 0 ,

although set generally strictly includes sets V + (A) V ++ (A), associated
immediate prediction models predictive lower previsions shown coincide
ones vacuous nearly vacuous inference systems.

13. Skeptical IDMM Inference Systems
combine ideas previous two sections: suppose subject uses
following system making inferences based sequence n > 0 observations
category set A. Section 11, skeptical
count vector ,
believes future, observe categories seen previously,
rather cautious completely vacuous
categories set A[].
beliefs already observed categories observed future,
uses IDMM-like inference them, described Section 12.
turns done quite simply replacing, characterisation (45)
sets HSC,A, skeptically cautious inference system, nearly vacuous models

V ++ (K) appropriate IDMM models HIDM,K
crK (). define, category
set A, NA {0} R>0 , following set polynomials:
{
}


:= p V (A) \ {0} : (K min SA, (p))p|K HIDM,K
HSI,A,
crK () ,
(61)
recall K min SA, (p), A[] K therefore K[rK ()] =
A[] K = A[], rK () essentially count vectors. also let


:= HSI,A,
HSI,A
= 0, words:
}
{


:= p V (A) \ {0} : (K min SA,0 (p))p|K HIDM,K
HSI,A
,
{
}
where, again, SA,0 (p) = =
6 K : p|K 6= 0 . remainder section, show

sets polynomials HSI,A
indeed lead definition reasonable potentially
useful type inference system. begin coherence.

Proposition 22. HSI,A
Bernstein coherent set polynomials .

44

fiCoherent Predictive Inference Exchangeability


shows inference system sSI , given sSI (A) := HSI,A
category sets A,

coherent. call SI skeptical IDMM inference system hyperparameter s.
want find updating works inference system. following
proposition really come surprise.


Proposition 23. NA {0}: HSI,A
c = HSI,A,
.

combining Equation (21), obtain followingagain, rather involved
predictive sets desirable gambles skeptical IDMM inference systems. n N
NA {0}:

{
}
s,n

= f L(An ) : MnnA (f ) HSI,A,
DSI,A
c
(62)
.


rather abstract, case
Although expressions HSI,A
c
NA :
corresponding lower previsions.

H sSI,A (p) = min p(x ) p V (A)
xA

(63)


=
H sSI,A (p|)

inf

sA[]


+ )
DiA[]
|rA[]
(p|A[]
()


p V (A).
= H sIDM,A[]
|rA[]
())
(p|A[]


(64)
(65)

Combining Equation (23), immediately obtain following predictive lower
NA :
previsions skeptical IDMM inference systems. n N
n
P s,n
SI,A (f ) = min f (x, x, . . . , x) f L(A )
xA


=
P s,n
SI,A (f |)

inf

sA[]


n
+ )
DiA[]
(MnA[]
()
(f |A[]
n )|rA[]

f L(An ).
= P s,n
())
n |rA[]
(f |A[]
IDM,A[]

(66)

immediate prediction models skeptical IDMM inference systems surprisingly
manageable:
s,1
DSI,A
= L>0 (A) P s,1
SI,A (f ) = min f f L(A)

NA :
and,
{
}
1
s,1
= f L(A) : f |A[]
DSI,A c
f (x)mx L>0 (A)
>


(67)


xA[]


1

s,1
=
(f |)
f (x)mx +
min f (x) f L(A).
P SI,A
+
+ xA[]


xA[]

45

(68)

fiDe Cooman, De Bock, & Diniz

lower probability function given by:
{
k
k < n n = 0

SI (n, k) = n+s
1
k = n > 0
corresponding imprecision function by:
{

n = 0 0 < k < n

SI (n, k) = n+s
0
otherwise

n, k N0 k n,

n, k N0 k n.

consider case n > 0, see sSI (n, n) = 0 sSI (n + 1, n) =
imprecision function satisfy Condition (35).


n+1+s

> 0,

Running Example. {H , }[(3, 1)] = {H , }, infer Equation (66)
IDMM inference systems.
inferences event HT

coherent inference systems sSI also satisfy representation insensitivity
specificity.
Theorem 24. R>0 , corresponding skeptical IDMM inference system
coherent, representation insensitive, specific: sSI Crs .

Since Crs closed non-empty infima, infimum
SI SI , > 0 still
coherent, representation insensitive specific, conservative
skeptical IDMM inference systems. shown associated immediate prediction
models predictive lower previsions coincide ones skeptically cautious
inference system.

14. Haldane Inference System
already know discussion near-ignorance following Theorem 4 representation insensitive coherent inference system fully precise, immediate prediction
models observations made, must completely vacuous. ask
whether representation insensitive (and specific) inference systems whose
posterior predictive lower previsions become precise (linear) previsions. problem
address section. shall first construct inference system, show
system is, definite sense, unique linear posterior predictive previsions.
use family IDMM inference systems sIDM , R>0 , define inference
system H committal them:



HIDM,A
=
sIDM (A) category sets A.
H (A) = HH,A :=
sR>0

sR>0

call H Haldane inference system, reasons become clear
section.
Theorem 25. Haldane inference system H coherent, representation insensitive
specific: H Crs .
46

fiCoherent Predictive Inference Exchangeability

Due representation insensitivity, Haldane system satisfies prior near-ignorance.
implies making observation, immediate prediction model vacuous,
far away precise probability model possible. show
that, making even single observation, inferences become precise-probabilistic:
coincide inferences generated Haldane (improper) prior.
get there, first take look models involving sets desirable gambles.
NA {0}:



= {p V (A) : (s R>0 )( sA ) DiA (p|
+ ) > 0} =

HH,A c
HIDM,A
c.
sR>0
(69)
corresponding predictive models easily derived applying Equation (21).
NA {0}:
n N
{
}
n
= f L(An ) : (s R>0 )( sA ) DiA (MnnA (f )|
+ ) > 0
DH,A
c

s,n

=
DIDM,A
c.
(70)
sR>0

immediate prediction models obtained combining Equations (70) (59).
NA :

{
}

1
1
= f L(A) :
DH,A = L>0 (A) DH,A c
f (x)mx > 0 L>0 (A).
xA

turns expressions corresponding lower previsions much
NA {0}:
manageable. First all, find
+ ) = lim H sIDM,A (p|)
p V (A).
inf DiA (p|

= lim
H H,A (p|)

s+0 sA

s+0

(71)

= 0, simplifies to:
particular,
H H,A (p) = min p(x ) p V (A),
xA

(72)

NA , find linear previsions:35
whereas
= H H,A (p|)
= HH,A (p|)
= DiA (p|)
p V (A).
H H,A (p|)

(73)

corresponding predictive models easily derived applying Equation (23).
NA {0}:
n N
= lim
P nH,A (f |)

+ ) = lim P s,n
f L(An ).
inf DiA (MnnA (f )|
IDM,A (f |)

s+0 sA

s+0

(74)
= 0:
particular,
P nH,A (f ) = min f (x, x, . . . , x) f L(An ),
xA

35. Dirichlet expectations DiA (|) strictly speaking defined RA
>0 , argue
Appendix C, continuously extended components zero, others strictly
positive.

47

fiDe Cooman, De Bock, & Diniz

NA :


P nH,A (f |)

=

n

P H,A (f |)

=

n

PH,A
(f |)

=


n
NA

( )
(nx )
n
xA mx
.
(n)



HynA (f |)

(75)



NA :
immediate prediction models, find

mx
1
=
P 1H,A (f ) = min f PH,A
(f |)
f (x)
f L(A),

xA

lower probability function given by:
{
k
n > 0
H (n, k) = n
n, k N0 k n.
0
n = 0
corresponding imprecision function given by:
{
1 n = 0
H (n, k) =
n, k N0 k n,
0 n > 0
satisfies Condition (35), suggests also Haldane inference system displays
albeit extreme interesting mannerthe desirable behaviour mentioned
Introduction: conservative little learned, never become less
precise observations come in.
Running Example. use Equation (74) results previously obtained
IDMM inference systems find
n

n
P nH,{H ,T } (IHT
) = 0 PH,{H ,T } (IHT
|(3, 1)) =
) = P H,{H ,T } (IHT

3
.
10

want point first equalities contradict prior near-ignorance
Haldane inference system, pertains immediate predictions: predictions
single future observations.

precise posterior predictive previsions Equation (75) exactly ones
would found formally apply Bayess rule multinomial likelihood
Haldanes improper prior (Haldane,
1945; Jeffreys, 1998; Jaynes, 2003), whose density
function int(A ) proportional xA x1 . This, course, use Haldanes name
inference system produces them. argumentation shows nothing
wrong posterior predictive previsions, based coherent inferences.
fact, analysis shows infinity precise proper priors simplex
that, together multinomial likelihood, coherent posterior predictive
previsions: every coherent prevision V (A) dominates coherent lower prevision
H H,A V (A).36 binomial parametric inferences Haldane prior, Walley (1991,
Section 7.4.8) comes related conclusion completely different manner.
36. immediate consequence F. Riesz Representation Theorem coherent prevision
restriction polynomials expectation operator unique -additive probability measure
Borel sets ; see instance discussion De Cooman Miranda (2008a) also
footnote 2.

48

fiCoherent Predictive Inference Exchangeability

simple argument show Haldane posterior predictive previsions
precise ones compatible representation insensitivity. Indeed,
shown representation insensitive coherent inference system precise
posterior predictive previsions, lower probability function must satisfy (n, k) = k/n
n > 0 0 k n,37 straightforward prove, using Bayess Theorem go
immediate prediction general predictive inference, posterior predictive
previsions must Haldanes.

15. Characterisation IDMM Immediate Predictions
lower probability function (n, k) representation insensitive coherent inference
system gives lower probability observing non-trivial event observed k
times n trials.
suppose subject specifies single lower probability, namely value
(1, 1) [0, 1]: probability observing something (again) observed (once)
single trial. ask conservative consequences
assessment are, take representation insensitivity specificity granted.
words, conservative representation insensitive specific coherent inference
system (at least) given value (1, 1) lower probability function?
question makes sense representation insensitive specific coherent inference
systems constitute complete meet-semilattice Statement (25) Theorems 8 12.38
Clearly, (1, 1) = 0, smallest representation insensitive specific coherent
inference system, know discussion Sections 9 10, must
immediate prediction models predictive lower previsions (nearly) vacuous
inference system. consider case 0 < (1, 1) < 1,39 words, use
parametrisation turn convenient purposes, that:
(1, 1) =

1
1
positive real number :=
1.
1+s
(1, 1)

(76)

Let us denote conservative inference system , lower probability
1
function , assumption (1, 1) 1+s
. follows Theorem 14.L9
n

(n, n) n+s n N0 . since IDMM inference system sIDM , Equation (60)
n
tells us sIDM (n, n) = n+s
, since assumption sIDM (n, n) (n, n), conclude
that:
n
(n, n) = sIDM (n, n) =
n N0 .
(77)
n+s
surmised (Bernard, 2007; De Cooman et al., 2009a) IDMM inference
system hyperparameter could smallest, conservative, representation
1
insensitive specific coherent inference system given value (1, 1) = 1+s
.
fact, trying prove made us start research present paper.
conjecture turns false: apart lower bound (77) (n, n),
37. suffices exploit additivity precise probabilities symmetry implied representation
insensitivity; explicit proof, see paper De Cooman et al. (2009a, Thm. 7).
38. See discussion near end Section 7.
39. surmise, prove here, conservative representation insensitive specific
coherent inference system corresponding (1, 1) = 1 might skeptically cautious one.

49

fiDe Cooman, De Bock, & Diniz

representation insensitivity specificity impose lower bounds (n, k) k < n.
see this, consider inference system sMC := inf{SC , sIDM }, Statement (25)
Theorems 8, 12, 20 21 coherent, representation insensitive specific: sMC Crs .
lower probability function sMC satisfies:
{
n
n
min{1, n+s
} = n+s
k = n > 0
sMC (n, k) = min{SC (n, k), sIDM (n, k)} =
k
min{0, n+s } = 0
otherwise,
substantiating claim made above. See also Figure 1, depicted lower (and
upper) probability functions Haldane system H , IDMM system sIDM , sMC
n

inference system inf{4s
SI , IDM }. latter three share value n+s (n, n),
n 0. conjecture sMC could smallest, conservative, representation
1
insensitive specific coherent inference system given value (1, 1) = 1+s
, offer
proof this.
(n, k)
1
n
n+s


n+s

0
0

1

2

...

n1

n

k

Figure 1: Lower upper probability functions: H Haldane system (dark grey,

4), sIDM IDMM system hyperparameter (blue, ?), min{4s
SI , IDM }
(orange, ) sMC = min{SC , sIDM } (red, ). specific plot made
n = 10 = 2.
means want characterise IDMM inference systems way
conservative ones, need add, besides coherence, representation insensitivity
specificity, another requirement preserved taking infima. One possible
candidate this, shall prove job inspired Figure 1,
following requirement.
Let us define subjects surprise event supremum rate betting
opposite event, words, lower probability opposite event. surprise
highclose onewhen subject believes strongly event occur,
lowclose zerowhen subject strong beliefs occur.
50

fiCoherent Predictive Inference Exchangeability

allows us associate so-called surprise function (n, k) := (n, n k)
lower probability function, (n, k) subjects surprise observing non-trivial
event observed k n times before.
follows Theorem 14.L5 representation insensitive system, surprise
function non-increasing second argument:
(n, k) := (n, k + 1) (n, k) = (n, n k 1) (n, n k) 0 0 k n 1.
fairly intuitive property: often event observed before,
smaller surprise seeing again.
shall say representation insensitive system concave surprise
2 (n, k) := (n, k + 1) (n, k) 0 0 k n 2,
where, course, 2 (n, k) = (n, n k 2) 2(n, n k 1) + (n, n k).
difficult see concave surprise preserved taking non-empty infima
inference systems, makes sense go looking smallest (most conservative)
coherent representation insensitive specific coherent inference system concave
surprise, satisfies additional local assessments, (76).
Looking Figure 1 makes us suspect IDMM inference system sIDM might
system, again, offer proof conjecture. however provide proof
following, related (probably) weaker, statement, focusses immediate
prediction only:
Theorem 26. immediate prediction models P 1A (|), NA {0} smallest
(most conservative) coherent representation insensitive specific coherent inference system
concave surprise satisfies (76), coincide ones IDMM inference
system sIDM hyperparameter s.

16. Conclusion
believe first paper tries deal systematic fashion principles
predictive inference exchangeability using imprecise probability models. Two salient
features approach (i) consistently use coherent sets desirable gambles
uncertainty models choice; (ii) notion inference system allows us
derive conservative predictive inference method combining local predictive probability
assessments general inference principles.
first feature allows us, contradistinction approaches
probability theory, avoid problems determining unique conditional models
unconditional ones conditioning events (lower) probability zero. set
n c

polynomials HA completely determines prior posterior predictive models DA
n
n
even (lower) prior probability P ([])
= H (BA,
P (|),
) observing
zero. approach using lower previsions probabilities would make
count vector
much complicated involved, impossible. Interestingly, provide
perfect illustration fact using results Sections 11, 13 14.40 three
40. Something similarly dramatic happens Sections 9 10: inference systems
immediate prediction models (predictive) lower previsions, one specific
not.

51

fiDe Cooman, De Bock, & Diniz

inference systems described therethe skeptically cautious, skeptical IDMM
Haldane systemshave, given category set A, three different sets polynomials
HA . Nevertheless, gather Equations (49), (63) (72),
lower prevision H therefore prior predictive models P nA . count vector
NA prior lower probability:


= H (BA,
P nA ([])
) = min BA,
(x ) = 0.
xA


zero lower probability makes sure posterior lower previsions H (|)
uniquely determined prior lower prevision
posterior predictive models P nA (|)
H : infer Equations (49), (65) (73) indeed different
three types inference systems. fail see could come withlet alone
proved necessary results forthese three systems relying lower prevision credal
set theory.
canand musttake line argumentation even further. Theorem 4,
inference system satisfies (prior) representation insensitivity near-vacuous prior
predictive models, therefore, time consistency coherence [monotonicity], see
n
= 0
prior predictive lower previsions must satisfy H (BA,
) = P ([])
NA well. simply means impossible (prior) representation insensitive

coherent inference system lower prevision H uniquely determine conditional
therefore systematic way dealing inference
lower previsions H (|).
systems must able resolveor deal withthis non-unicity way. believe
approach involving coherent sets desirable gambles one mathematically
elegant ways this.
second feature allowed us, example, characterise IDMM immediate
predictions conservative ones satisfying number inference principles.
approach follow canat least principlealso used types inference
systems inference principles. key requirement inference principle
make amenable approach that, formulated property inference
system, preserved taking arbitrary non-empty infima. three inference
principles considering aboverepresentation insensitivity, specificity
concave surprisehave property, nothing prevents analysis
approach extended inference principle too.
complications see, point, technical mathematical nature. reader
doubt noticed proofs results later sections quite involved
technical, rely quite heavily properties polynomials simplex. feel
present paper made headway mathematical territory, instance
new discussion Bernstein positivity polynomials near Proposition 28
Appendix B. Conclusions paper De Cooman Quaeghebeur (2012),
characterisation Bernstein positivity mentioned open problem interesting
practical applications inferencenatural extensionunder exchangeability.
much remains open exploration, determined study mathematical
structure properties polynomials would certainly help alleviating technical
difficulties working inference principles inference systems.
paper opened feel interesting line
research foundations predictive inference, nevertheless provided answers
52

fiCoherent Predictive Inference Exchangeability

number ofif allopen problems formulated Conclusions earlier paper
De Cooman et al. (2009a), tried deal representation insensitivity immediate
prediction. first example: asked whether representation insensitive
coherent inference systems whose lower probability functions additive second
argument? suffices look Figure 1 see answer is, clearly, yes. Another
question was: representation insensitive coherent inference systems
mixing predictive systems?41 follows Equation (68) answer yes:
skeptical IDMM inference systems provides example. Finally, use infimum
sMC skeptically cautious inference system SC IDMM inference system sIDM ,
mentioned briefly Section 15, answer two questions. representation
insensitive coherent inference systems inequality Theorem 14.L6 strict?
representation insensitive coherent inference systems whose behaviour
gambles completely determined lower probability function? inference
system sMC provides positive answer questions.
inference systems mentioned above, apart IDMM Haldane
systems, appear first time. may appear contrived perhaps
even artificial, found useful constructing (counter)examples,
shaping intuition, building new models, Figure 1 argumentation clearly
indicate. might also wonder whether representation insensitive and/or
specific coherent inference systems, cannot produced appropriately chosen infima
examples introduced here. suggest, candidates consideration,
inference systems derived using Walleys (1997) bounded derivative model,
inference systems constructed using sets infinitely divisible distributions,
recently proposed Mangili Benavoli (2013). framework provided here, well
simple characterisation results Theorems 7 11, quite useful addressing
similar problems.
end, want draw attention simple direct, quite appealing,
consequence argumentation Section 14: infinity precise proper
priors that, together multinomial likelihood, coherent Haldane posterior
predictive previsions. So, need improper priors justify posteriors,
proper priors job perfectly well. (precise-)probabilistic conclusion
follows easily looking problem using general powerful language
imprecise probabilities. Moreover, seen properties representation
insensitivity cannot satisfied precise probabilistic models. Finally, entire framework
conservative predictive inference using inference principles would impossible develop
within limitative context precise probabilities. shows distinct
advantages using imprecise probability models dealing predictive inference.

Acknowledgements
Gert de Coomans research partially funded project number 3G012512
Research Foundation Flanders (FWO). Jasper De Bock PhD Fellow Research
41. Loosely speaking: cannot written (specific kind of) convex mixture Haldane inference
system IDMM inference system; see paper De Cooman et al. (2009a, Section 5)
information.

53

fiDe Cooman, De Bock, & Diniz

Foundation Flanders wishes acknowledge financial support. Marcio Diniz
supported FAPESP (So Paulo Research Foundation), project 2012/14764-0
wishes thank SYSTeMS Research Group Ghent University hospitality
support sabbatical visit there. authors would like thank three anonymous
reviewers many insightful comments suggestions aimed making paper
easier read cleaning misunderstandings. special thank also great
Arthur Van Camp enthusiasm everything and, particular, helping us check
little examples.

Appendix A. Notation
appendix, provide list commonly used important notation,
defined first introduced.
notation

meaning

introduced

A, B, C,
IB
X, Xn
n
n
posi(A)
L(A)
L>0 (A)
L0 (A)




n
DA

category sets, events
indicator event B
variable, variable time n
number already observed variables
number observed variables
cone generated
set gambles
set positive gambles
set non-positive gambles
observed sample
observed count vector
prior predictive set desirable gambles
category set n future observations
posterior predictive set desirable gambles
prior predictive lower prevision
posterior predictive lower prevision
pooling map relabelling map
renaming bijection
category permutation
sample observations outside B eliminated
counting map
set count vectors n observations
set count vectors, zero
hypergeometric expectation operator
multinomial coefficient count vector
multinomial expectation operator
simplex probability mass functions
sum components x x B
Bernstein basis polynomial
set polynomials degree n

Section 1
Section 2.2
Section 1
Section 1
Section 1
Equation (1)
Section 2.2
Section 2.2
Section 2.2
Section 3
Section 5.4
Section 3

n c,
n c
DA

DA
n
P ()
P nA (|)

P nA (|),


$
B


NAn
NA , NA {0}
HynA (|)
()
MnnA (|)

B
BA,
V n (A)

54

Section 3
Section 3
Section 3
Sections 4.1&4.4
Section 4.2
Sections 4.3
Section 4.5
Equation (8)
Equation (9)
Section 5.3
Equation (10)
Equation (11)
Equation (13)
Equation (12)
Equation (12)
Equation (15)
Section 5.3

fiCoherent Predictive Inference Exchangeability

V (A)
V + (A)
V ++ (A)
HA

HA c
HA

H (|)
F

C
Crs
R
rB
iA
IrB,A
SA



subscript
subscript
subscript
subscript
subscript
subscript
subscript

A[]
++
V[]
(A)

V
NV
SC
IDM
SI
H
OI

diA (|)
DiA (|)
DiMnnA (|)
bnp

set polynomials
set Bernstein positive polynomials
set polynomials
positive int(A )
representing set polynomials
updated representing set polynomials
lower prevision induced HA

lower prevision induced HA c
set category sets
inference system
set coherent inference systems
set coherent inference systems
representation insensitive specific
extended relabelling map
restriction map
injection map
extended injection map
sampling expectation
lower probability function
imprecision function
surprise function
related vacuous inference system
related nearly vacuous inference system
related skeptically cautious inference system
related IDMM inference systems
related skeptical IDMM inference systems
related Haldane inference system
related original IDMM inference systems
categories already observed
set polynomials
positive int(A[] )
Dirichlet density
Dirichlet expectation operator
Dirichlet multinomial expectation operator
expansion polynomial p
Bernstein basis degree n

Section 5.3
Section 5.3
Section 10
Theorem 5
Equation (19)
Equation (17)
Equation (20)
Definition 6
Definition 6
Equation (24)
Theorem 12
Equation (26)
Equation (27)
Equation (28)
Equation (29)
Section 8
Equation (33)
Equation (34)
Section 15
Section 9
Section 10
Section 11
Section 12
Section 13
Section 14
Appendix
Equation (42)
Section 11
Appendix
Appendix
Appendix
Appendix

C
C
C
B

Appendix B. Multivariate Bernstein Basis Polynomials
n 0 NAn corresponds
Bernstein basis polynomial
(multivariate)

x
:=
degree n , given BA, ()
() xA x , . polynomials
number interesting properties (see instance Prautzsch, Boehm, & Paluszny, 2002,
Chapters 10 11), list here:
BB1. set {BA, :
NAn } Bernstein basis polynomials fixed degree n linearly
independent: N n BA, = 0, = 0 NAn .


55

fiDe Cooman, De Bock, & Diniz

NAn } Bernstein basis polynomials fixed degree n forms
BB2. set {BA, :
partition unity: N n BA, = 1.


BB3. Bernstein basis polynomials non-negative, strictly positive interior
int(A ) .
BB4. set {BA, : NAn } Bernstein basis polynomials fixed degree n forms
basis linear space polynomials whose degree n.
Property BB4 follows BB1 BB2.42 follows BB4 that:
BB5. polynomial p unique expansion terms Bernstein basis polynomials
also called Bernstein expansionof fixed degree n deg(p),
words, unique count gamble bnp NAn that:

p() =
bnp ()BA, () .

(78)

n
NA

tells us [also use BB2 BB3] p() convex combination Bernstein
coefficients bnp (), NAn whence:
min bnp min p p() max p max bnp .

(79)

following proposition adds detail picture.
Proposition 27. polynomial p :
lim [min bnp , max bnp ] = [min p, max p] = p(A ).

n+
ndeg(p)

Proof Proposition 27. Since bnp converges uniformly polynomial p n +
(Trump & Prautzsch, 1996), sense
( )



lim maxn p
bnp () = 0,
n+ NA
n
ndeg(p)

find
lim

n+
ndeg(p)

min bnp min p =

lim

[
]
minn bnp () min p

n+ NA
ndeg(p)

[
( )]
minn bnp () p
n+ NA
n
ndeg(p)
( )



lim maxn p
bnp () = 0,
n+ NA
n



lim

ndeg(p)

therefore limn+,ndeg(p) min bnp min p. Furthermore, Statement (79), see
limn+,ndeg(p) min bnp min p. Hence indeed limn+,ndeg(p) min bnp = min p. proof
equality completely analogous.
42. see how: clearly polynomials definition linear combinations Bernstein basis polynomials,
possibly different degrees. terms, use BB2 raise degree common higher
degree nmultiply appropriate version 1. shows Bernstein basis polynomials
fixed degree n generating polynomials lower degrees. also independent BB1.

56

fiCoherent Predictive Inference Exchangeability

Using results, prove number useful relations Bernstein
positivity polynomial positivity (the interior of) simplex. related
property first proved Hausdorff univariate case (Hausdorff, 1923, p. 124).
Proposition 28. Let p polynomial . Consider following statements:
(i) ( )p() > 0;
(ii) p V + (A), meaning n deg(p) bnp > 0;
(iii) p V ++ (A), meaning ( int(A ))p() > 0;
(iv) ( )p() 0.
(i)(ii)(iii)(iv).
Proof Proposition 28. first implication direct consequence Proposition 27:
infer (i) continuity p min p > 0 therefore, Proposition 27,
limn+,ndeg(p) min bnp = min p > 0, implies (ii).
prove (ii)(iii), assume n deg(p) bnp > 0,
consider int(A ). since BA, () > 0 NAn [BB3], since
assumption bnp 0 bnp () > 0 NAn , see
p() =



bnp ()BA, () bnp ()BA, () > 0.

n
NA

third implication immediate consequence continuity p.
following counterexample shows necessarily V + (A) = V ++ (A).
Running Example. go back polynomial q {H ,T ,U } defined Equation (41):
2
2
q() = H
H +
= (H )2 + H {H ,T ,U } .

already argued polynomial Bernstein positive. Nevertheless,
obviously positive interior {H ,T ,U } .

also quite easy trace effect Bernstein expansion multiplying
Bernstein basis polynomial:
Proposition 29. polynomials p , natural n deg(p), NA {0}
NAn+mA :

()
bn ( )

p
n+mA
(

)()
bpBA, () =

0
otherwise.
Proof Proposition 29. Observe that:
(
)

n
pBA, =
bp ()BA, BA, =
bnp ()BA, BA,
n
NA

n
NA

57

fiDe Cooman, De Bock, & Diniz

=



bnp ()

n
NA

( + )
BA,+ ,
()()

use uniqueness (Bernstein) basis expansion.
allows us prove following simple interesting result Bernstein positivity:
Proposition 30. Consider NA {0} polynomial p . Then:
pBA, V + (A) p V + (A).
Proof Proposition 30. First, assume pBA, V + (A), natural n
n

deg(p) bn+m
pBA, > 0. follows Proposition 29 also bp > 0,
therefore p V + (A).
Assume, conversely, p V + (A), n deg(p) bnp > 0.
+

follows Proposition 29 also bn+m
pBA, > 0, therefore pBA, V (A).

Appendix C. Dirichlet Distribution
density diA (|) Dirichlet distribution hyperparameter RA
>0 given by:
diA (|) :=


(A )
xx 1 int(A ),
(
)
x
xA
xA

polynomial p define corresponding expectation as:43


(A )
p()
DiA (p|) :=
xx 1 d.
(
)
x

xA
xA

particular,


(

)


(A )
xx 1
(x )

xA
xA
xA
( )
( )

n
(A )
(mx + x )
1
n
=
x (mx ) ,
=
(n + )
(x )
(n)

DiA (BA, |) =

n


xmx

(80)

xA

xA

using ascending factorial (r) := (+r)
() = ( + 1) . . . ( + r 1), R
r N0 .
Dirichlet distribution used prior combination multinomial
likelihood, leading so-called Dirichlet multinomial distribution, described
follows. probability observing (a sample n 0 observations with) count vector
NA {0} multinomial process Dirichlet prior density diA (|) given by:

n
DiMnA ({}|) :=
CoMnnA ({}|) diA (|)


43. integrals section interpreted multiple Riemann integrals.

58

fiCoherent Predictive Inference Exchangeability


BA, () diA (|) = DiA (BA, |),

=


second equality follows Equation (16). Therefore, generally, take
expansion polynomial p Bernstein basis polynomials degree n deg(p):
DiA (p|) =



bnp () DiA (BA, |) =

n
NA

= DiMnnA



bnp () DiMnnA (I{} |)

n
NA

(
n
NA

)

n
bp ()I{} = DiMnnA (bnp |),

Dirichlet multinomial expectation count gamble bnp . general
useful relationship Dirichlet expectation polynomial p, Dirichlet
multinomial expectation Bernstein expansion bnp . Although expectations
strictly speaking defined RA
>0 , extend definition continuously
elements RA
\
{0}

taking
appropriate
limits, Equation (80) indicates.

C.1 Special Properties Dirichlet Distribution
recall interesting properties Dirichlet distribution. begin
updating property:
Proposition 31 (Updating). category set A, polynomial p V (A), count
vector NA {0} RA
>0 :
DiA (pBA, |) = DiA (BA, |) DiA (p| + ).
Proof Proposition 31.

DiA (pBA, |) =
p()BA, () diA (|)

(
)


mx (A )
xx 1
=
p()
x
(
)

x

xA
xA
xA
(
)


(mx + x )

(A )
=
p() diA (| + )
(mA + )
(x )

xA

= DiA (BA, |) DiA (p| + ),
last equality follows Equation (80).
Next, turn so-called renaming property:
Proposition 32 (Renaming). category sets C bijective
(one-to-one onto) map : C, polynomial p V (C) RA
>0 :
DiA (p R |) = DiC (p|R ()).
59

fiDe Cooman, De Bock, & Diniz

Proof Proposition 32. Due linear nature Dirichlet expectation, clearly
suffices prove property Bernstein basis polynomials p = BC, ,
NC {0}. Observe R bijection too. Then, using Equation (80), let := R ()
:= R1 (), z = 1 (z) mz = n1 (z) z C, = C
nA = mC , get:
(
)
(mz + z )
mC
(C )
DiC (BC, |R ()) = DiC (BC, |) =
(mC + C )
(z )
zC
( )
(n1 (z) + 1 (z) )
(A )
nA
=
(nA + )
(1 (z) )
zC
( )
(nx + x )
(A )
nA
=
= DiA (BA, |),
(nA + )
(x )
xA

take account int(A ):
(BC, R )() = BC, (R ())
(
)
(
)
(
)
mC mz
mC
mC n1 (z)
mz
=
1 (z)
1 (z) =
R ()z =



zC
zC
zC
( )
nA
xnx = BA, (),
=

xA

see indeed DiC (BC, |R ()) = DiA (BC, R |).
so-called pooling property generalises renaming property:
Proposition 33 (Pooling). category sets onto
map : D, polynomial p V (D) RA
>0 :
DiA (p R |) = (p|R ()).
Proof Proposition 33. Due linear nature Dirichlet expectation, suffices
prove property Bernstein basis polynomials p = BD, , ND {0}.
Also, take account renaming property Proposition 32, enough consider
following special case, non-empty set different categories b,
c belonging it, let := {b, c} := {d}, define letting
(x) := x x (b) = (c) := d.
one hand, taking account Equation (80), letting := R () :
(
)
(mz + z )
mD
(D )
(BD, |R ()) = (BD, |) =
(mD + )
(z )
zD
(
)
mD
(D )
(md + ) (mz + z )
=
.
(81)
(mD + ) (d )
(z )
zDo

hand,

60

fiCoherent Predictive Inference Exchangeability

DiA (BD, R |)
(
)
)
(
mD
md
mz
(b + c )
z
diA (|)
=


zDo
)
(


(A )
mD

=
(b + c )md bb 1 cc 1
zmz +z 1

xA (x )
zDo
(
)
)
(



md

(A )
mD

bk+b 1 cmd k+c 1
zmz +z 1
=
k
(
)

x

xA
zDo
k=0

)
(
)
(



(A )
md (k + b )(md k + c ) zDo (mz + z )
mD

=
.

k
(mD + )
xA (x )
k=0

So, compare results recall = , z = z z = b + c ,
see must prove that:
)
md (

1
md
(md + b + c )
=
(k + b )(md k + c )
(b + c )
(b )(c )
k
k=0

equivalently, using ascending factorials:
(b + c )(md ) =

)
md (

md
b (k) c (md k) .
k

(82)

k=0

see proving pooling property essentially equivalent proving Equation (82),
binomial theorem ascending factorials. well-known result,
follows fact ascending factorials Sheffer sequences binomial type (Sheffer,
1939). completeness, give proof here, easy,
shown hold prove pooling property particular case
= {a}, category different b, c d. = {a, b, c} = {a, d},
case rewrite Equation (81) as:
(BD, |R ())
(
)
+ md
(a + b + c )
(md + b + c ) (ma + )
=

(ma + md + + b + c ) (b + c )
(a )
whereas
DiA (BD, R |) =
let
1 (
:=
0
1

(1

0

(1

=

1a

)md bb 1 (1

)md ama +a 1

(

(
)
+ md (a + b + c )


(a )(b )(c )



1a

b )c 1 ama +a 1 db

bb 1 (1

b )

c 1

)
da

)

db da
( 1
)
1
md +b +c 1 +a 1
b 1
c 1
=
(1 )


(1 t)
dt da
0

0

0

0

61

fiDe Cooman, De Bock, & Diniz

= B(ma + , md + b + c )B(b , c ) =

(ma + )(md + b + c ) (b )(c )
,
(ma + md + + b + c ) (b + c )

using well-known evaluation Beta function terms Gamma functions.
Finally, look properties related restriction.
Proposition 34 (Restriction). category sets B B A,
polynomial p V (B), RA
>0 r N0 :
DiA (IrB,A (p)|) =

(deg(p) + r + B )
(A )
DiB (p|rB ()).
(deg(p) + r + )
(B )

Proof Proposition 34. Let n := deg(p) + r, due linearity Dirichlet
expectation operator, Equations (29) (80):

DiA (IrB,A (p)|) =
bnp () DiA (BA,iA () |)
n
NB



( )
n
(A )
xB (nx + x )
xA\B (x )


=
(n + )
n
xA\B (x )
xB (x )
NB
( )

n
(A ) (nx + x )
=
bnp ()
(n + )
(x )
n


bnp ()

NB

=


n
NB

=

xB

bnp ()

(A ) (n + B )
DiB (BB, |rB ())
(n + ) (B )

(A ) (n + B )
DiB (p|rB ()),
(n + ) (B )

concluding proof.

Appendix D. Original IDMM Inference System Walley
Bernard
IDMM inference system sIDM , introduced Section 12, differs one
originally proposed Walley Bernard (1999).44 appendix, discuss original
IDMM inference system, denote sOI , explain related ours,
illustrate advantages version one Walley Bernard.
D.1 Defining Original IDMM Inference System
R>0 , category set A, consider following set polynomials:

:= {p V (A) : ( KsA ) DiA (p|) > 0}
HOI,A

= {p V (A) : ( int(A )) DiA (p|s) > 0} .
44. Strictly speaking, Walley Bernard propose inference system sense, rather
collection prior posterior predictive lower previsions category set A. inference system
call original IDMM inference system one produces predictive lower previsions.

62

fiCoherent Predictive Inference Exchangeability

reasons become clear shortly, call inference system sOI defined

sOI (A) := HOI,A
category sets A,

original IDMM inference system hyperparameter > 0. Updating done much
NA {0}:
way inference system sIDM Section 12.

= {p V (A) : ( int(A )) DiA (p|
+ s) > 0} ,
HOI,A
c

compared Equation (54). leave exercise reader
check sOI coherent representation insensitive.45 However, illustrated
counterexample Section D.3, sOI specific.
predictive models sOI easily derived mimicking approach used
Section 12 derive predictive models sIDM ; see Equations (56) (57).
NAn :
n N0 , n N
{
}
s,n
= f L(An ) : ( int(A )) DiA (MnnA (f )|
+ s) > 0 ,
DOI,A
c

(83)

=
P s,n
OI,A (f |)

(84)


inf
int(A )

+ s) gambles f .
DiA (MnnA (f )|

latter expression motivates refer sOI original IDMM inference system:
predictive lower previsions coincide proposed Walley Bernard (1999).
Using Equation (83) n = 1, mimicking argument proof Equation (59)
Appendix E.7, see
s,1
=
DOI,A
c

{
f L(A) : f >

}
1
s,1

NA {0}.
f (x)mx = DIDM,A
c

xA

tells us IDMM original IDMM immediate prediction
models. corresponding immediate predictive lower previsions original IDMM
well-known course identical ones produced version IDMM
inference system, given Equation (60). However, examples next section
illustrate, equality extend beyond immediate prediction: IDMM
original IDMM different coherent inference systems, leads us general
important conclusion coherent inference systems completely determined
immediate prediction models.
Nevertheless, approaches closely related; comparing Equations (84) (57),
NAn
see n N0 , n N
0

,n
= inf0 P sOI,A
gambles f .
P s,n
(f |)
IDM,A (f |)
0<s <s

45. proof similar one sIDM [see Theorem 21].

63

(85)

fiDe Cooman, De Bock, & Diniz

D.2 Original IDMM Inference System Monotone
hyperparameter original IDMM inference system usually interpreted
degree caution. Higher values often claimed produce inferences
cautious less informative. following quote Walley Bernard (1999,
Section 2.4) makes explicit:
B event concerning future observations, IDMM(s) produces intervals
posterior probabilities [P (B|), P (B|)] nested become wider
increases. means inferences produced two IDMMs different
values always consistent other, effect increasing
simply make inferences cautious less informative.
Similar statements found related papers Walley (1996, Section 2.5) Bernard
(2005, Section 4.6). Although indeed true many inferences, including many important
onesfor example, immediate predictions, hold event concerning
future observations, illustrated following example, lower probability
event concerning two future observations shown initially increase s.
Example 1. Consider situation possibility space consists two elements
only, say heads (H) tails (T ), observed once, n = 2
= (mH , mT ) = (1, 1). interested predictive lower probability

next two trials, heads tails observed once: n = 2 looking
= {(H, ), (T, H)},
= (mH , mT ) = (1, 1).
predictive lower probability event []
original IDMM inference system, following formula provides closed-form
expression:
=
P s,n
|)
OI,A (I[]

+ s) = inf DiA (BA,
+ s)
DiA (MnnA (I[]
|
)|
int(A )
( )
1
n
(mx + stx )(mx )
= inf
(n)


int(A ) (n + s)
xA
inf

int(A )

= inf

0<t<1

2(1 + st)(1 + s(1 t))
2(1 + s)
=
.
(2 + s)(3 + s)
(2 + s)(3 + s)

initially increases s; see also Figure 2.
conclude P s,n
|)
OI,A (I[]

(86)


version IDMM inference system, statement made aforementioned
quote hold event concerning future observations. follows trivially
Equation (85). illustrate next example.
Example 2. Consider problem Example 1. time, solve using version
IDMM. result also depicted Figure 2, function hyperparameter s.
s,n
P IDM,A
non-increasing function s. Indeed,
contrast P s,n
(I[]
|),
|)
OI,A (I[]

0 ,n
1

=
(I[]
0 < < 1
P sOI,A

|)
slim
0 0
3
s,n
=
P IDM,A (I[]
|)

2(1 + s)

P s,n
=
1
|)
OI,A (I[]
(2 + s)(3 + s)
closed-form expression find combining Equations (85) (86).
64



fiCoherent Predictive Inference Exchangeability

0.36


P s,n
|)
OI,A (I[]

0.34
0.32


P s,n
|)
IDM,A (I[]

0.3
0.28

0

0.5

1

1.5

2

Figure 2: Lower probability observing two different outcomes next two experiments, given possibility space consists two categories,
already observed once: solutions according sOI (solid line) sIDM
(dashed solid line); see Examples 1 2 information.

Clearly, inferences sOI sIDM differ: suffices compare results
Examples 1 2; see Figure 2 well. Therefore, seems clear Walleys (1996, p. 51)
statement [. . . ] allowed vary 0 s, produces exactly
inferences IDM = s. equivalently, sOI sIDM produce
inferences, taken apply immediate prediction only.
D.3 Original IDMM Inference System Specific
announced Theorem 21, version IDMM inference system specific.
show that, least values hyperparameter s, true original
version.
NBn . B f L(B n ):
Consider n N0 , n N
s,n
( int(A )) DiA (MnnA (f IB n )|iA ()
+ s) > 0
f IB n DOI,A
ciA ()

+ srB ()) > 0,
( int(A )) DiB (MnnB (f )|
last equivalence consequence Propositions 41 34 fact
= .
particular B A, hard see sB = {srB () : int(A )},
rB (iA ())
implies that:
s,n
s,n
( sB ) DiB (MnnB (f )|
+ ) > 0 f DIDM,B

f IB n DOI,A
ciA ()
c,

therefore also:

B n ) = inf DiB (MnnB (f )|
+ ) = P s,n
P s,n
OI,A (f |iA (),
IDM,B (f |).
B

65

fiDe Cooman, De Bock, & Diniz

hand, due Equation (SP1), sOI specific, would that:
B n ) = P s,n


= P s,n
P s,n
OI,A (f |iA (),
OI,B (f |rB (iA ()))
OI,B (f |).
P s,n

Hence, order sOI specific, necessary P s,n
OI,B (|)
IDM,B (|)
coincide. illustrated examples previous section, necessarily
case. Therefore, sOI always specific. counterexample provided,
difference occurs < 1 only, whereas practice, usually chosen either 1
2 (Walley & Bernard, 1999, Section 2.4). would interesting see whether similar
counterexamples constructed 1.
original IDMM inference systems specific, apparently contradicts Theorem 11 De Cooman et al. (2009a), seems state are. fact,
theorem states original IDMM immediate prediction models satisfy weaker
specificity condition, tailored immediate prediction only. Since immediate prediction
models original IDMM IDMM coincide, contradiction.

Appendix E. Proofs Additional Results Technical
E.1 Proofs Results Section 4
Proof Theorem 4. sake notational simplicity, use intuitive notation f (Xk )
extnk (f ). give proof general definition, terms sets desirable
gambles. proof lower previsions follows immediately.
Consider category set A, n N, 1 k n gamble f
n may assume without loss generality singleton.
f (Xk ) DA
already implies f 6 0, coherence [D4]. Hence particular f 6= 0 max f > 0.
Assume ex absurdo f 6> 0, must f (a) < 0. Define
gamble g letting g(a) := f (a) g(x) := max f > 0 x \ {a}.
g f therefore g(Xk ) f (Xk ), implies, coherence [use D2 D3], also
n . let := max f f (a) > 0 := f (a)/ > 0, define
g(Xk ) DA
n , > 0.
gamble h := g/ = + IA\{a} , also, coherence [D3], h(Xk ) DA
consider natural number N 2, follows repeatedly applying pooling
n
renaming invariance appropriate manner + I{a1 } (Zk ) D{a
,
1 ,...,aN }
Zk variable assumes value a1 Xk 6= assumes value
{a2 , . . . , } Xk = a. repeatedly applying category permutation invariance, find
n
+ I{a` } (Zk ) D{a
` {1, . . . , N }. Coherence [D3] tells us
1 ,...,aN }
N
n
N + 1 = `=1 [ + I{a` } (Zk )] D{a
. leads contradiction coherence
1 ,...,aN }
[D4] choose N large enough.
E.2 Proofs Results Section 7
Proposition 35. n N : () = R ( ()).
Proof Proposition 35. Consider z D,
Tz () = |{k {1, . . . , n} : (xk ) = z}| =


yA : (y)=z

66

|{k {1, . . . , n} : xk = y}|

fiCoherent Predictive Inference Exchangeability



=

Ty () = R ( ())z ,

yA : (y)=z

concluding proof.
Lemma 36. n N, NAn Dn :

1
1
I{} () =

().
()
(R ()) [R ()]
[]


Proof Lemma 36. Consider map : Dn R defined := [] I{} .
permutation index set {1, . . . , n} Dn , see


() =
I{} () =
I{(1 )} ()
[]

[]



=



I{} () =

[]

I{} () = (),

[]

tells us permutation invariant thereforeconstant atoms
[], NDn . means that, obvious notations, = N n ()I[] .

() > 0 implies [] = , therefore,
Proposition 35, () = () = R ( ()) = R () therefore [R ()]. tells
us () = 0 unless = R () therefore = (R ())I[R ()] .
plug f := 1 Equation (87), see


() =
() =
(R ())I[R ()] () = (R ())(R ()).
Dn

Dn

Lemma 37. n N NDn :


BD, R =

BA,

n : R ()=
NA


Proof Lemma 37. ,
( ) (
)nz
n
(BD, R )() =
x

zD x1 ({z})
( )
( )

n
nz
=
z


nz
z
zD N

( )
n
=

=

1 ({z})

(


n : R ()=
NA



n:
NA

R ()=

(

n


67

xA

z

xmx

x1 ({z})

xmx

) (

xA

)

concluding proof.



xmx =

zD

)

nz

|1 ({z})


n:
NA

R ()=

BA, (),

fiDe Cooman, De Bock, & Diniz

lemma allows us prove two related propositions.
Proposition 38. n N gambles f Dn : MnnA (f ) = MnnD (f ) R .
Proof Proposition 38. First all, count vector NAn
HynA (f |) =



1
1
f () =
I{} ()f ()
()
()
n
[]

[]



=

f ()

Dn

I{} ()

(87)

[]

1

()
f ()
(R ()) [R ()]
n



=



=

1
()



1
(R ())



f () = HynD (f |R ()),

[R ()]

fourth equality follows Lemma 36. Therefore indeed:


MnnA (f ) =
HynA (f |)BA, =
HynD (f |R ())BA,
n
NA

=



n
NA

HynD (f |)

n
ND

=





BA,

n : R ()=
NA


HynD (f |)(BD, R ) = MnnD (f ) R ,

n
ND

fourth equality follows Lemma 37.
Proposition 39. polynomials p n N0 n deg(p):
bnpR = bnp R .
Proof Proposition 39. find expanding p appropriate Bernstein basis:
(
)

n
p R =
bp ()BD, R =
bnp ()(BD, R )
n
ND

=



bnp ()

n
ND

=



n
ND



BA, =





bnp (R ())BA,

n
n : R ()=
ND
NA


n : R ()=
NA


(bnp R )()BA, ,

n
NA

third equality follows Lemma 37. desired result follows
uniqueness expansion (Bernstein) basis.
Proof Theorem 7. Fix category sets onto map : D,
gamble f Dn . use notation HA := (A)
n, n N,
68

fiCoherent Predictive Inference Exchangeability

HD := (D), transform Condition (RI2) using equivalence Condition (18).
:= ():

one hand, letting
n
f DA
MnnA (f ) HA MnnD (f ) R HA
n
n
n
BA,
f DA
c
MnA (f ) HA BA,
(MnD (f ) R ) HA ,

second equivalences follow Proposition 38. hand, recalling
= R ( ())
= R ()
Proposition 35:
()
n
f DD
MnnD (f ) HD
n
n
BD,R ()
f DD
c
MnD (f ) HD .

tells us equivalences Condition (RI2) rewritten as:
MnnD (f ) R HA MnnD (f ) HD
n
n
BA,
(MnD (f ) R ) HA BD,R ()
MnD (f ) HD .

proof complete observe (and recall discussion Section 5.3
Appendix B) varying n N f L(Dn ), let p := MnnD (f ) range
, let
:= ()
range
polynomials , varying n N
count vectors NA .
Proof Theorem 8. Let, ease notation := inf iI , coherent using Equation (25). Consider category sets onto map : D,
p V (D) NA {0}. Then, using representation insensitivity
coherent Theorem 7:
(p R )BA, (A) (i I)(p R )BA, (A)
(i I)pBD,R () (D) pBD,R () (D),
concludes proof.
Proposition 40. , (B ) = rB ( ()).
Proof Proposition 40. Immediate, since B sample whose components belong
B, category B, number times occurs B exactly
number times occurs .
Proof Proposition 9. Consider B , let, simplicity notation = iA ().
since NBn , n := deg(p) + r
(
)
( )
n
n
iA ()x
BA,iA () () =
x
=
nx x = BB, (),
iA ()

xA

xB

see indeed:
IrB,A (p|) =



bnp ()BA,iA () () =

n
NB


n
NB

69

bnp ()BB, () = p().

fiDe Cooman, De Bock, & Diniz

Proof Proposition 10. deg(p) + r = 0, r = 0 p = c R, trivially
IrB,A (p|) = I0B,A (c)() = c. let us assume deg(p) + r > 0. First all, observe
deg(p)+r

NB

:
(
)
(
)
deg(p) + r iA ()x
deg(p) + r nx
BA,iA () () =
x
=
x
iA ()

xA
xB
{
deg(p)+r
B
BB, (|+
B > 0
B)
=
0
otherwise.

(88)

therefore already follows Condition (29) IrB,A (p|) = 0 B = 0. Let us therefore
assume B > 0. Condition (29) Equation (88) tell us that:

IrB,A (p|) =
bdeg(p)+r
()BA,iA () ()
p
deg(p)+r

NB

deg(p)+r



=

bdeg(p)+r
()B
p

BB, (|+
B)

deg(p)+r

NB

deg(p)+r

deg(p)+r



= B

bdeg(p)+r
()BB, (|+
p
B ) = B

p(|+
B ),

deg(p)+r

NB

concludes proof.
Proposition 41. n N gambles f B n :
MnnA (f IB n ) = IrB,A (MnnB (f )), r := n deg(MnnB (f )).
Proof Proposition 41. First all, count vector NAn thatwith
slight abuse notation:
HynA (f IB n |) =


1
1
(f IB n )() =
()
()
[]



f ()

[]B n

zero unless = iA () NBn . case, since obviously () = (),
[iA ()] B n []again slight abuse notation:
HynA (f IB n |iA ()) =

1
f () = HynB (f |).
()
[]

Therefore, recall Condition (29):


HynB (f |)BA,iA ()
MnnA (f IB n ) =
HynA (f IB n |iA ())BA,iA () =
n
NB

n
NB

= IrB,A (MnnB (f )),
r := n deg(MnnB (f )).
70

fiCoherent Predictive Inference Exchangeability

Proof Theorem 11. Fix category sets B B A, n, n N,
gamble f B n . use notation HA := (A) HB := (B),

transform Condition (SP2) using equivalence Condition (18). one hand,
:= ()
r := n deg(MnnB (f )):
letting
n
f IB n DA
MnnA (f IB n ) HA IrB,A (MnnB (f )) HA
n
n
r
n
BA,
f IB n DA
c
MnA (f IB n ) HA BA,
IB,A (MnB (f )) HA ,

second equivalences follow Proposition 41. hand, recalling
B ) = rB ( ())
= rB ()
Proposition 40:
(
n
f DB
MnnB (f ) HB
n
n
B BB,rB ()
f DB
c
MnB (f ) HB .

tells us equivalences Condition (SP2) rewritten as:
IrB,A (MnnB (f )) HA MnnB (f ) HB
r
n
n
BA,
IB,A (MnB (f )) HA BB,rB ()
MnB (f ) HB .

proof complete recall discussion Section 5.3 Appendix B
varying n N f L(B n ), let p := MnnB (f ) = CoMnnB (HynB (f )) range
polynomials B r = n deg(MnnB (f )) range elements N0 ,
, let
:= ()
range count vectors NA .
varying n N
Proof Theorem 12. Let, ease notation := inf iI , coherent using
Equation (25). Consider category sets B B A, p V (B),
NA {0} r N0 . Then, using specificity :
IrB,A (p)BA, (A) (i I)IrB,A (p)BA, (A)
(i I)pBB,rB () (B) pBB,rB () (B),
concludes proof.
E.3 Proofs Results Section 8
Proof Proposition 13. sufficiency, fix category set A, gamble g A, count
vector NA {0}. Condition (RI4) := g(A), := g, f := idD yields Condition (RI5).
necessity, fix category sets onto map : D,
gamble f D, count vector NA {0}. Observe (f )(A) = f (D)
r f (D)



Rf ()r =
mx =
mx
xA : (f )(x)=r

zD : f (z)=r xA : (x)=z



=

zD : f (z)=r

71

R ()z = Rf (R ())r ,

fiDe Cooman, De Bock, & Diniz

Rf = Rf R . infer invoking Condition (RI5) twice that:
1
1
f DA
c id(f )(A) D(f
)(A) cRf ()
1
idf (D) Df1 (D) cRf (R ()) f DD
cR (),

concluding proof.
Proof Theorem 14. arguments proof rely heavily following expression
lower probability function:
{
}
1
(n, k) = sup R : I{a} D{a,b}
c(k, n k)
}
{
= sup R : ak bnk [a ] ({a, b})
(89)
related expressions equivalent representation insensitivity
Bernstein coherence [B3]. expressions follow Equations (32) (33), Bernstein
coherence [B3] representation insensitivity form (RI4).
L1. Immediate Bernstein coherence fact (n, k) lower probability:
use Equation (89), B2 B4.
L2. Fix non-negative integers n, k ` k + ` n. Consider real
< (n, k) < (n, `), follows applying Equation (89) Condition (RI4)
xk y` znk` [x ] ({x, y, z}) xk y` znk` [y ] ({x, y, z}), whence,
Bernstein coherence [B3], xk y` znk` [(x + ) ( + )] ({x, y, z}). Applying
Equation (89) Condition (RI4) tells us uk+` znk` [u ( + )] ({u, z}),
whence + (n, k + `).
L3, L4 L5 immediate consequences L1 L2.
L6. Consider category set := {a, b} count vector := k
mb := n k. Define gamble g g(a) := (n + 1, k + 1) g(b) := (n + 1, k).
g(a) g(b) L5, therefore coherence [P5 P3] predictive lower
prevision P 1A (|) tells us P 1A (g|) = g(b) + [g(a) g(b)]P 1A ({a}|) = (n + 1, k) +
(n, k)[(n + 1, k + 1) (n + 1, k)] [see also Equation (33)]. clearly suffices prove
P 1A (g|) (n, k) = P 1A ({a}|). Consider < P 1A (g|), follows using
Equation (31) that:
ak bnk [g(a)a + g(b)b ] (A).
(90)
Also, > 0, ak+1 bnk [a g(a) + ] (A) ak bn+1k [a g(b) + ] (A),
therefore, coherence [B3], recalling + b = 1,
(A) 3 ak+1 bnk [a g(a) + ] + ak bn+1k [a g(b) + ]
= ak bnk [a g(a)a g(b)b + ]. (91)
Combining Statements (90) (91) using coherence [B3], leads ak bnk [a + ]
(A), whence (n, k) , completes proof.
L7. Use L1 L5 find (n, k)[(n + 1, k + 1) (n + 1, k)] 0, use L6.
L8. sn 0 follows L4, need prove sn+1 sn , equivalently,
(n, 1) (n + 1, 1)[1 + (n, 1)]. Indeed:
(n, 1) (n + 1, 1) + (n, 1)[(n + 1, 2) (n + 1, 1)]
72

fiCoherent Predictive Inference Exchangeability

(n + 1, 1) + (n, 1)[2(n + 1, 1) (n + 1, 1)]
= (n + 1, 1) + (n, 1)(n + 1, 1),
first inequality follows L6 k = 1, second L4 L1.
L9. inequalities hold trivially n = 0, due L1. consider n N,
category sets := {x, y} B := {x1 , x2 , . . . , xn , y}. Let 0 < < := > 0.
Since (1, 1) > , see x [x ] (A), equivalently, x [x (1 ) ] (A),
since x + = 1. Representation insensitivity [use Equation (89) Condition (RI4)]
tells us xk [xk (1
) ] ({xk , y}), specificity [use Theorem 11] allows us
infer ( nk=1 xk
)[xk (1 )
n ] (B), k {1, . . . , n}.
n
infer coherence [B3] ( k=1 xk )[ k=1 xk (1 ) ny ] (B), apply
representation insensitivity get xn [x (1 ) ny ] (A). Since = 1 x ,
n
equivalent xn [x (1 + n) n] (A). shows (n, n) 1+n
, using
Equation (89). rest proof immediate.
E.4 Proofs Results Section 9
Proof Theorem 15. V coherent obvious, category set F,
V (A) = V + (A) Bernstein coherent set polynomials .
prove representation insensitivity, use Theorem 7. Consider category sets
onto map : D, p V (D) NA {0}.
indeed
(p R )BA, V + (A) p R V + (A) p V + (D) pBD,R () V + (D),
first last equivalences follow Proposition 30, second one
Lemma 48 K = A.
E.5 Proofs Results Section 10
Proof Theorem 16. V coherent obvious, category set F,
V (A) = V ++ (A) obviously convex cone includes V + (A) [Proposition 28]
contain zero polynomial: V ++ (A) therefore Bernstein coherent set polynomials
.
prove representation insensitivity, use Theorem 7. Consider category sets
onto map : D, p V (D) NA {0}.
indeed
(p R )BA, V ++ (A) ( int(A ))p(R ())BA, () > 0
( int(A ))p(R ()) > 0
( int(D ))p() > 0
( int(D ))p()BD,R () () > 0 pBD,R () V ++ (D),
second fourth equivalences follow Bernstein positivity Bernstein
basis polynomials Proposition 28, third one Lemma 48 K = A.
73

fiDe Cooman, De Bock, & Diniz

prove specificity, use Theorem 11. Consider category sets B
B A, p V (B), NA {0} r N0 . indeed:
IrB,A (p)BA, V ++ (A) ( int(A ))IrB,A (p|)BA, () > 0
( int(A ))IrB,A (p|) > 0
( int(B ))p() > 0
( int(B ))p()BB,rB () () > 0 pBB,rB () V ++ (B),
second fourth equivalences follow Bernstein positivity Bernstein
basis polynomials Proposition 28, third one Lemma 52 K = A.
E.6 Proofs Results Section 11
Below, use convenient device identifying, proper subset B A, element
B unique corresponding element = iA () whose components outside
B zero:
(x B)x = x (x \ B)x = 0.
Also observe that, using convention, identify int(A[] ) subset ,
characterise follows:
: int(A[] ) (x A)(x > 0 mx > 0).
Proof Proposition 17. clearly suffices prove V + (A) HSC,A 0
/ HSC,A .
first statement easy prove ASC,A trivially includes non-constant
Bernstein basis polynomials, Proposition 28. Since V + (A) consists finite, strictly positive
linear combinations non-constant Bernstein basis polynomials, immediately
V + (A) HSC,A .
prove second statement, suppose ex absurdo 0 HSC,A . implies
++
finitely many nk > 0, count vectors k NAnk pk V[
(A)
k]

0 = k pk BA,k . always possible find (at least) one count vector, 1 say,
A[k ] 6 A[1 ] k. words, either A[k ] = A[1 ]
A[k ] \ A[1 ] 6= . consider int(A[1 ] ). A[k ] \ A[1 ] 6= ,
++
BA,k () = 0. A[k ] = A[1 ], BA,k () > 0, moreover, since pk V[
(A),
k]

pk () > 0. Hence 0 = k pk ()BA,k () > 0, contradiction.
Lemma 42. Consider NA {0} p HSC,A, , ` N, nk N0

++
+ nk > 0, k NAnk pk V[+
(A) p = `k=1 pk BA,k .
k]

SA, (p) = {K : A[ + k ] K k {1, . . . , `}}
therefore
min SA, (p) = min {A[ + k ] : k {1, . . . , `}} .
Proof Lemma 42. second statement trivial, given first. restrict
attention proving first statement.
Assume first A[ + r ] K r {1, . . . , `}. clearly K 6= ,
since + nr > 0. may assume without loss generality A[ + r ] minimal
74

fiCoherent Predictive Inference Exchangeability

element set {A[ + k ] : k {1, . . . , `}}. Consider int(A[+r ] ), whence
also K . k {1, . . . , `} A[ + k ] = A[ + r ]and
++
clearly least one kwe see pk () > 0 since pk V[+
(A),
k]
BA,k () > 0, whence (pk BA,k )() > 0. k must A[ + k ] \
A[ + r ] 6= , therefore (pk BA,k )() = 0 since BA,k () = 0. guarantees

p() = `k=1 (pk BA,k )() > 0, whence indeed K SA, (p), since already know
K , A[] A[ + r ] K K 6= .
Assume, conversely, K SA, (p), implies 6= K A[] K,
K p() 6= 0. Observe A[ + k ] = A[]A[k ],
assume ex absurdo A[ + k ] * K therefore A[k ] * K k {1, . . . , `}.
Fix k {1, . . . , `}, x A[
/ K, therefore x = 0,
k ] x
whence BA,k () = 0. shows p() = `k=1 (pk BA,k )() = 0, contradiction.
Lemma 43. Consider NA {0}, p V (A) n N n deg(p).
NAn :
bnp () 6= 0 (K min SA, (p))K \ A[] A[].
Proof Lemma 43. Fix NAn . prove contraposition, suppose
K min SA, (p), K \ A[] * A[] therefore K * A[ + ], since
A[ + ] = A[] A[]. Hence, A[ + ]
/ SA, (p). Since moreover 6= A[ + ]
A[] A[ + ], infer Equation (46) p|B = 0, let, ease
notation, B := A[ + ]. rewrite [see also Lemma 47]:
0 = p|B =


n
NA

bnp ()BA, |B =


n
NB

bnp ()BA, |B =



bnp ()BB, .

n
NB

Due uniqueness Bernstein expansion, possible bnp () = 0
n
n
NA[+]
. concludes proof since, clearly, NA[+]
.

Proof Proposition 18. First, assume p HSC,A, , implying p = `k=1 pk BA,k
++
` N, nk N0 + nk > 0, k NAnk pk V[+
(A). already
k]
follows Lemma 42 p =
6 0 min SA, (p) = min {A[ + k ] : k {1, . . . , `}}.
Consider K min {A[ + k ] : k {1, . . . , `}} int(K ).
k, either A[ + k ] = K A[ + k ] \ K 6= . A[ + k ] = Kwhich
happens least one k, due choice Kthen pk () > 0 BA,k () > 0.
A[ + k ] \ K 6= , since A[] K, A[k ] \ K =
6 , implying BA,k () = 0.
Hence, p() > 0. Since holds int(K ), find p|K V ++ (K).
Assume, conversely, p V (A)\{0}
p|K V ++ (K)
K n min SA, (p).
n
Fix n N n deg(p), p = N n bp ()BA, = bp ()BA, ,

{
}
:= NAn : bnp () 6= 0 . Since p 6= 0, infer Equation (46) min SA, (p) 6=
[observe SA, (p)]. know Lemma 43 , least
one K min SA, (p) K \ A[] A[]. Let us pick K, call
K . let, K min SA, (p), MK := { : K = K}, found
way divide disjoint subsets MK , one every K min SA, (p)
75

fiDe Cooman, De Bock, & Diniz


may empty, K \ A[] A[] MK , = Kmin SA, (p) MK


therefore p = Kmin SA, (p) MK bnp ()BA, .
fix K min SA, (p), construct count vector K letting (mK )x := 1
x K \ A[] (mK )x := 0 otherwise. Notice K NAnK , nK number
elements |K \ A[]| set K \ A[], therefore nK n. Consider MK ,
since (mK )x = 1 implies x A[] therefore x 1, see :
BA, () = ()



xx = ()

xA[]



xx (mK )x

xA[]



x(mK )x

xA[]

= (K, )BA,K ()BA,K (),

n
1 ( )1 . Hence, rewrite
(K, ) := ()(

)
K
K
MK bp ()BA,

:= MK (K, )bnp ()BA,K . way, find p =

pK BA,K , pK
Kmin SA, (p) BA,K pK .
Hence, fix K min SA, (p) 6= , left prove + nK > 0
++
pK V[+
(A). Assume first, ex absurdo, + nK = 0. particular
K]
++
K = , contradicts K SA, (p). remains prove pK V[+
(A).
K]
Consider int(A[+K ] ). derive K min SA, (p) SA, (p)
A[] K. Since A[K ] = K \A[], implies A[ + K ] = A[]A[K ] =
A[] (K \ A[]) = K, therefore also int(K ). K 0 min SA, (p) \ {K},
K0 \ K =
6 therefore BA,K 0 () = 0. Hence, p() = BA,K ()pK (). know
p() > 0 p|K V ++ (K) BA,K () > 0 A[K ] = K \ A[] K.
conclude indeed pK () > 0.
Lemma 44. NA {0} p V (A):
SA, (p) = SA,0 (pBA, ) therefore min SA, (p) = min SA,0 (pBA, ).
Proof Lemma 44. First, assume K SA, (p). 6= K A, A[] K
p|K 6= 0. last inequality continuity polynomials, infer
int(K ) p() 6= 0. Since A[] K, find p()BA, () 6= 0
therefore (pBA, )|K 6= 0.
Assume, conversely, K SA,0 (pBA, ). =
6 K (pBA, )|K 6= 0.
last inequality implies K (pBA, )() 6= 0 therefore
BA, () 6= 0 p() 6= 0. BA, () 6= 0, derive A[] K
p() 6= 0, derive p|K 6= 0.
Proof Proposition 19. way HSC,A HSC,A, constructed [see defining
expressions (43) (44)], clearly suffices prove HSC,A c HSC,A, . Consider
therefore p V (A) pBA, HSC,A , Proposition 18, implies
pBA, 6= 0 (pBA, )|K V ++ (K) K min SA,0 (pBA, ). set
prove p HSC,A, . Applying Proposition 18 again, since, clearly, p 6= 0,
see suffices show p|K V ++ (K) K min SA, (p). consider
K min SA, (p). Then, Lemma 44, K min SA,0 (pBA, ), already argued
(pBA, )|K V ++ (K). Hence indeed also p|K V ++ (K).
76

fiCoherent Predictive Inference Exchangeability


Proof Equation (48). Combining Equations (47) (30), see that,
NA {0}:
1
= {f L(A) : SA (f ) HSC,A,
DSC,A
c
(92)
}.
Also, f L(A) =
6 K A:
SA (f ) = 0 f = 0, SA (f )|K V ++ (K) f |K > 0 SA (f )|K = 0 f |K = 0. (93)
= 0. f L(A):
start case
min SA,0 (SA (f )) = {{x} : x f (x) 6= 0} ,
1
Statement (93). Hence, Proposition 18 Equations (92) (93): DSC,A
=
L>0 (A).
NA . f L(A):
Next, consider

{

{A[]}
f |A[]
6 0
=
min SA,
(SA (f )) =
{x} : x \ A[]
f (x) 6= 0} f |A[]
{A[]
=0

(94)

Equation (93). recall Proposition 18 Equations (92) (93)
1

consider two cases: f |A[]
6= 0 f |A[]
= 0. f |A[]
6= 0, f DSC,A c
{if f 6= 0 [which redundant]
f |A[]
> 0 or, equivalently [since f |A[]
6= 0],
}
1

f h L(A) : h|A[]
> 0 L>0 (A). f |A[]
= 0, f DSC,A c
or, equivalently [since f |A[]
f 6={0 f (x) 0 }
x \ A[]
= 0],
f h L(A) : h|A[]
> 0 L>0 (A).
Proof Equation (49). start first part Equation (49). Due Equation (17)
Proposition 19, suffices prove that, p V (A), minxA p(x ) > 0 p
HSC,A,0 minxA p(x ) < 0 p
/ HSC,A,0 .

First, assume minxA p(x ) < 0. p(y ) < 0.
Hence, since p|{y} = p(y ) < 0, find {y} min SA,0 (p) therefore also
p
/ HSC,A,0 , Proposition 18.
Next, assume minxA p(x ) > 0. p|{x} = p(x ) > 0 x A, implying
min SA,0 (p) = {{x} : x A} therefore also, since p 6= 0, p HSC,A,0 ,
Proposition 18.
turn second part Equation (49). Due Equation (17) Proposition 19,
NA p V (A), minA[]
suffices prove that,
p() > 0 p

HSC,A,
p() < 0 p
/ HSC,A,
minA[]
.

First, assume minA[]
p()
<
0.

int(A[]
)

++


p() < 0, implying p|A[]
6= 0 p|A[]

/ V (A[]).
Hence, find A[]


min SA,
/ HSC,A,
(p) therefore also p
, Proposition 18.

Next, assume minA[]
p()
>
0.

p|A[]
=
6 0 p|A[]
V ++ (A[]).



therefore also, since p 6= 0, p HSC,A,
Hence, find min SA,
(p) = {A[]}
,
Proposition 18.
77

fiDe Cooman, De Bock, & Diniz

Proof Equation (52). first part Equation (52) trivial consequence Equa NA f L(A). Then, combining
tion (51). second part, consider
Equations (50) (30):


= min
f (x)x = min f (x).
P 1SC,A (f |)
f (x)x = min
A[]


xA

A[]



xA[]


xA[]

Lemma 45. Consider category sets onto map : D,
p V (D) =
6 K A. (p R )|K 6= 0 p|(K) 6= 0.
Proof Lemma 45. First, assume p|(K) 6= 0, (K)
p() 6= 0. choose K R () = . Then, clearly, (p R )() =
p(R ()) = p() 6= 0 therefore (p R )|K 6= 0.
Assume, conversely, (pR )|K 6= 0, K (pR )() 6= 0.
let := R (), (K) p() = p(R ()) = (p R )() 6= 0. Hence,
p|(K) 6= 0.
Lemma 46. Consider category sets onto map : D,
p V (D), NA {0}.
{
}
SA, (p R ) = K : A[] K (K) SD,R () (p) ,
therefore
(SA, (p R )) = SD,R () (p) (min SA, (p R )) = min SD,R () (p).
Proof Lemma 46. start proving first statement. First, assume K SA, (p
R ), implying =
6 K A, A[] K (p R )|K 6= 0. 6= (K) D,
D[R ()] = (A[]) (K) and, Lemma 45, p|(K) 6= 0. Hence, (K) SD,R () (p).
Conversely, assume K A, A[] K (K) SD,R () (p). =
6 (K),
implies =
6 K, also p|(K) 6= 0, which, Lemma 45, implies (p R )|K 6= 0.
Hence, K SA, (p R ).
first statement implies (SA, (p R )) SD,R () (p) therefore, order
prove second statement, suffices show SD,R () (p) (SA, (p R )) or,
equivalently, every L SD,R () (p), K SA, (p R )
(K) = L. choose L SD,R () (p) let K := {x : (x) L} = 1 (L).
(K) = L onto, since (A[]) = D[R ()] L, follows A[] K.
Hence, first statement, K SA, (p R ).
prove third statement, first assume K min SA, (p R ), implying
K SA, (p R ) that, K 0 SA, (p R ), K 0 6 K. second statement,
(K) SD,R () (p). prove (K) min SD,R () (p), assume ex absurdo
L SD,R () (p) L (K). Let K 0 := {x K : (x) L} = K 1 (L).
K 0 K (K 0 ) = L, therefore, Lemma 45, (p R )|K 0 6= 0,
K 0 6= p|L =
6 0. Since L SD,R () (p), see (A[]) = D[R ()] L
therefore A[] 1 (L). Since K SA, (p R ), also know A[] K,
therefore A[] K 1 (L) = K 0 . tells us K 0 SA, (p R ), contradiction.
Assume, conversely, L min SD,R () (p), implying L SD,R () (p). Then,
78

fiCoherent Predictive Inference Exchangeability

second statement, K 0 SA, (p R ) (K 0 ) = L. Hence,
K min SA, (p R ) K K 0 therefore (K) (K 0 ) = L. Since
L min SD,R () (p) since, due second statement, (K) SD,R () (p), also
(K) 6 L therefore (K) = L.
Lemma 47. Let =
6 K let p polynomial . n deg(p):
bnp| = bnp |NKn .
K

Proof Lemma 47. follows

p() =
bnp ()BA, ()
n
NA

K :
p|K () =


n
NA

=





bnp ()BA, (iA ()) =

bnp ()BA, (iA ())

n : A[]K
NA

bnp |NKn ()BK, (),

n
NK

completes proof.
Lemma 48. Consider category sets onto map : D,
p V (D) =
6 K A. Then:
(i) (p R )|K V + (K) p|(K) V + ((K));
(ii) (p R )|K V ++ (K) p|(K) V ++ ((K)).
Proof Lemma 48. first statement follows fact that, n deg(p):
bn(pR )|

K

n
> 0 bn(pR ) |NKn > 0 (bnp R )|NKn > 0 bnp |N(K)
> 0 bnp|

> 0,
(K)

first last equivalence due Lemma 47, second equivalence follows
n) = Nn
Proposition 39, third equivalence holds R (NK
(K) .
turn second statement, prove following statements
equivalent:
(a) ( int(K ))p(R ()) > 0;
(b) ( int((K) ))p() > 0.
First assume (a) holds, consider int((K) ). prove p() > 0.
1
construct K follows.
Consider z (K). x ({z}) K, choose
x > 0 way xK : (x)=z x = z . way, found K
satisfying R () = , moreover x > 0 x K, whence int(K ).
infer (a) indeed p() = p(R ()) > 0.
Assume, conversely, (b) holds, consider int(K ). Then, z D,
R ()z > 0 z (K) R ()z = 0 otherwise. means R () int((K) )
infer (b) indeed p(R ()) > 0.
79

fiDe Cooman, De Bock, & Diniz

Proposition 49. SC representation insensitive.
Proof Proposition 49. use characterisation representation insensitivity Theorem 7. Consider category sets onto map : D,
p V (D) NA {0}. Then, Proposition 19, need prove
p R HSC,A, p HSC,D,R () .
First, assume p HSC,D,R () , which, Proposition 18, implies p 6= 0
p|L V ++ (L) L min SD,R () (p). Applying Lemma 45 K = A, infer
p =
6 0 p R 6= 0. Consider K min SA, (p R ). Then, Lemma 46,
(K) min SD,R () (p), implying that, due assumption, p|(K) V ++ ((K)). Since
K 6= , apply Lemma 48 find (p R )|K V ++ (K). Hence, Proposition 18,
p R HSC,A, .
Assume, conversely, pR HSC,A, , which, Proposition 18, implies pR 6= 0
(p R )|K V ++ (K) K min SA, (p R ). Applying Lemma 45,
K = A, infer p R 6= 0 p 6= 0. Now, consider L min SD,R () (p),
Lemma 46, K min SA, (p R ) (K) = L. Since K =
6 and,
assumption, (p R )|K V ++ (K), infer Lemma 48 p|L V ++ (L). Hence,
Proposition 18, p HSC,D,R () .
Lemma 50. Consider category sets B B A, p V (B),
K K B 6= r N0 . IrB,A (p)|K 6= 0 p|KB 6= 0.
Proof Lemma 50. may assume without loss generality r + deg(p) > 0,
proof trivial otherwise.
First, assume p|KB =
6 0, means KB
p() 6= 0. := iA () K , infer Proposition 9 IrB,A (p|) = p() 6= 0
therefore IrB,A (p)|K 6= 0.
Assume, conversely, IrB,A (p)|K 6= 0, means, due continuity polynomials, int(K ) IrB,A (p|) 6= 0. infer K B 6=
+
B > 0, Proposition 10 guarantees p(|+
B ) 6= 0. Since |B KB , find
p|KB 6= 0.
Lemma 51. Consider category sets B B A, p V (B)
r N0 r + deg(p) > 0, NA {0}.
{
}
SA, (IrB,A (p)) = K : A[] K K B SB,rB () (p) ,
therefore
{
}
SB,rB () (p) = K B : K SA, (IrB,A (p))

{
}
min SB,rB () (p) = K B : K min SA, (IrB,A (p)) .
Proof Lemma 51. begin first statement. First, assume K SA, (IrB,A (p))
therefore 6= K A, A[] K IrB,A (p)|K =
6 0. K implies
KB B, A[] K implies B[rB ()] = A[]B KB. Moreover, IrB,A (p)|K =
6 0
together Proposition 10 r + deg(p) > 0 implies K B 6= , turn,
Lemma 50, implies p|KB 6= 0. Hence, K B SB,rB () (p). Conversely, assume
80

fiCoherent Predictive Inference Exchangeability

K A, A[] K K B SB,rB () (p). K B 6= , implying K 6= ,
p|KB 6= 0, which, Lemma 50, implies IrB,A (p)|K 6= 0. Hence, K SA, (IrB,A (p)).
order prove second statement, clearly suffices show SB,rB () (p)
{K B : K SA, (IrB,A (p))}, since converse inclusion follows directly first
statement. consider L SB,rB () (p) let K := L A[]. K A, A[] K
K B = L (A[] B) = L B[rB ()] = L. Hence, first statement, indeed
K SA, (IrB,A (p)).
prove third statement, first assume K min SA, (IrB,A (p)), implying
particular K SA, (IrB,A (p)). Then, second statement, K B SB,rB () (p).
prove K B min SB,rB () (p), consider L SB,rB () (p) L K B,
let K 0 := L A[]. Then, argument identical one used proof second
statement, K 0 B = L K 0 SA, (IrB,A (p)). However, since K 0 B = L K B
K 0 \ B = A[] \ B K \ B, find K 0 = (K 0 B) (K 0 \ B) (K B) (K \ B) = K,
therefore K 0 = K, assumption. Hence indeed L = K 0 B = K B. Assume,
conversely, L min SB,rB () (p), implying L SB,rB () (p). Then, second
statement, K 0 SA, (IrB,A (p)) K 0 B = L,
K min SA, (IrB,A (p)) K K 0 therefore K B K 0 B = L. Since
L min SB,rB () (p) and, second statement, K B SB,rB () (p), also
K B = L.
Lemma 52. Consider category sets B B A, p V (B), K
K B 6= r N0 . IrB,A (p)|K V ++ (K) p|KB V ++ (K B).
Proof Lemma 52. may assume without loss generality r + deg(p) > 0,
proof trivial otherwise. Using Proposition 10, considering that, since K B =
6 , B > 0
int(K ), suffices prove following statements equivalent:
(a) ( int(K ))p(|+
B ) > 0;
(b) ( int(KB ))p() > 0.
First assume (a) holds, consider int(KB ). prove p() > 0.
construct
K follows. x K \ B, choose x > 0 way

:= xK\B x < 1, always possible. x K B, let x := (1)x > 0.
follows construction B = 1 > 0, |+
B = int(K ),
infer (a) indeed p() = p(|+
)
>
0.
B
Assume, conversely, (b) holds, consider int(K ). B > 0
+
K B =
6 0 therefore, z B, (|+
B )z > 0 z K B. Hence |B int(KB ),
+
infer (b) p(|B ) > 0.
Proposition 53. SC specific.
Proof Proposition 53. use characterisation specificity Theorem 11. Consider
category sets B B A, p V (B), NA {0}, r N0 .
Then, Proposition 19, need prove IrB,A (p) HSC,A, p HSC,B,rB () .
First, assume p HSC,B,rB () , which, Proposition 18, implies p 6= 0
p|L V ++ (L) L min SB,rB () (p). Applying Lemma 50 K = A, infer
p 6= 0 IrB,A (p) 6= 0. Consider K min SA, (IrB,A (p)), Lemma 51,
81

fiDe Cooman, De Bock, & Diniz

K B min SB,rB () (p), implying that, due assumption, p|KB V ++ (K B).
Since K B 6= 0, apply Lemma 52 find IrB,A (p)|K V ++ (K). Hence,
Proposition 18, IrB,A (p) HSC,A, .
Assume, conversely, IrB,A (p) HSC,A, , which, Proposition 18, implies
r
IB,A (p) 6= 0 IrB,A (p)|K V ++ (K) K min SA, (IrB,A (p)). Lemma 50
K = A, IrB,A (p) 6= 0, infer p 6= 0. Consider L min SB,rB () (p),
then, Lemma 51, K min SA, (IrB,A (p)) KB = L. Since therefore
K B 6= 0 since, assumption, IrB,A (p)|K V ++ (K), infer Lemma 52
p|L V ++ (L). Hence, Proposition 18, p HSC,B,rB () .
Proof Theorem 20. immediate consequence Propositions 17 [coherence], 49
[representation insensitivity] 53 [specificity].
E.7 Proofs Results Section 12
NA {0} p V (A).
Proof Equation (54). Consider


BA, p HIDM,A
p HIDM,A
c
( sA ) DiA (BA, p|) > 0

+ ) > 0
( sA ) DiA (BA, |) DiA (p|
+ ) > 0,
( sA ) DiA (p|
third equivalence follows Updating Property Dirichlet expectation
[Proposition 31].
NA {0}. Then, combining Equations (56)
Proof Equation (59). Consider
(58) n = 1:
{
}

mx + x
s,1
= f L(A) : ( sA )
DIDM,A
c
f (x)
>0 .
+
xA

consider f L(A). sA :

xA

f (x)



mx + x
x
1
>0
f (x)(mx + x ) > 0
>
f (x)
f (x)mx .
+


xA

xA

Combining equations above, letting c := 1s
find that:

xA



xA f (x)mx

s,1
(s0 (0, s))( int(A ))
f DIDM,A
c

ease notation,

s0
f (x)tx > c.


(95)

xA

f c, f (y) < c therefore, Statement (95),
s,1
[choose s0 ty close enough 1, respectively]. f = c, due
f
/ DIDM,A
c
s,1
Finally, let us
definition c, f = c = 0. Hence, Statement (95), f
/ DIDM,A
c.
see happens
f > c. clearly c 0. Consider s0 (0,
s) int(A ).
0
0
since f > c, xA f (x)tx > c therefore also, since c 0, ss xA f (x)tx > ss c c.
s,1
Statement (95).
Hence f DIDM,A
c
82

fiCoherent Predictive Inference Exchangeability

NA {0} f L(A). combining
Proof Equation (60). Consider
Equations (57) (58):

mx + x
mx + s0 tx
= inf
inf
f (x)

+ s0 (0,s) int(A )
+ s0
xA
xA
(
)


1
s0
= inf
f (x)mx +
inf
f (x)tx
+ s0 int(A )
s0 (0,s) + s0
xA
xA
(
)

1
s0
= inf
f (x)mx +
min f
+ s0
s0 (0,s) + s0
xA


1
f (x)mx +
min f,
=
+
+

= inf
P s,1
IDM,A (f |)



f (x)

xA

last equality follows min f

mx
xA f (x) ,



property convex combinations.

Proof Theorem 21. coherence, fix category set A, must prove

HIDM,A
satisfies requirements B1B3 Bernstein coherence. trivial

definition HIDM,A
, linearity Dirichlet expectation operator, fact
Dirichlet expectation Bernstein basis polynomial positive.
Next, turn representation insensitivity, use characterisation Theorem 7.
Consider category sets onto map : D, p V (D)
NA {0}. Then, using Pooling Property [Proposition 33] Dirichlet
expectation Equation (54), find indeed:

(p R )BA, HIDM,A
( sA ) DiA (p R | + ) > 0

( sA ) (p|R ( + )) > 0

( sD ) (p|R () + ) > 0 pBD,R () HIDM,D
,

third equivalence follows equality sD = R (sA ).
Finally, turn specificity, use characterisation Theorem 11. Consider
category sets B B A, p V (B), NA {0}
r N0 . Then, using Restriction Property [Proposition 34] Dirichlet expectation
Equation (54), find indeed:

IrB,A (p)BA, HIDM,A
( sA ) DiA (IrB,A (p)| + ) > 0

( sA ) DiB (p|rB ( + )) > 0

( sB ) DiB (p|rB () + ) > 0 pBB,rB () HIDM,B
,

third equivalence follows sB = rB (sA ).
E.8 Proofs Results Section 13

Lemma 54. p1 , p2 HSI,A
: SA,0 (p1 + p2 ) = SA,0 (p1 ) SA,0 (p2 ).

83

fiDe Cooman, De Bock, & Diniz

Proof Lemma 54. First, consider K SA,0 (p1 + p2 ), meaning 6= K
(p1 + p2 )|K 6= 0. Assume, ex absurdo, K
/ SA,0 (p1 ) K
/ SA,0 (p2 ). p1 |K = 0
p2 |K = 0 therefore (p1 + p2 )|K = 0, contradiction. Hence indeed
K SA,0 (p1 ) SA,0 (p2 ).
Next, consider K SA,0 (p1 ) SA,0 (p2 ), implying =
6 K A.
least one K 0 min(SA,0 (p1 ) SA,0 (p2 )) K 0 K, assume without
loss generality K 0 SA,0 (p1 ). Since K 0 min(SA,0 (p1 ) SA,0 (p2 )),
L 6 K 0 L SA,0 (p1 ) SA,0 (p2 ), therefore K 0 min SA,0 (p1 ). already tells us

0
p1 |K 0 HIDM,K
0 . two possibilities. first one K SA,0 (p2 ),

then, much way above, find p2 |K 0 HIDM,K 0 . Hence,


due Bernstein coherence [B3] HIDM,K
0 , (p1 + p2 )| 0 = p1 | 0 + p2 | 0 HIDM,K 0 .
K
K
K
0
0
second possibility K
/ SA,0 (p2 ), p2 |K 0 = 0 since K 6= , find,

too, (p1 + p2 )|K 0 = p1 |K 0 + p2 |K 0 = p1 |K 0 HIDM,K
0 . cases, therefore,


(p1 + p2 )|K 0 HIDM,K 0 , Bernstein coherence [B1] HIDM,K
0 allows us conclude
6 0. Since K 0 K, find also (p1 + p2 )|K 6= 0 therefore
(p1 + p2 )|K 0 =
K SA,0 (p1 + p2 ).


Proof Proposition 22. Since 0
/ HSI,A
, left prove V + (A) HSI,A
that,



> 0 p, p1 , p2 HSI,A , p HSI,A p1 + p2 HSI,A .

First, consider > 0 p HSI,A
. Then, clearly, SA,0 (p) = SA,0 (p) therefore
min SA,0 (p) = min SA,0 (p). K min SA,0 (p), K min SA,0 (p),


which, since p HSI,A
, implies p|K HIDM,K
therefore, due Bernstein


coherence HIDM,K , (p)|K = (p|K ) HIDM,K
. Furthermore, since p 6= 0 also

p 6= 0, therefore p HSI,A .

Next, consider p1 , p2 HSI,A
. p1 6= 0 p2 6= 0, implying SA,0 (p1 ) 6=
SA,0 (p2 ) 6= , therefore SA,0 (p1 ) SA,0 (p2 ) 6= . Applying Lemma 54, find
SA,0 (p1 + p2 ) 6= , K 6= K A, (p1 + p2 )|K =
6 0
therefore p1 + p2 6= 0. K 0 min SA,0 (p1 + p2 ), equivalently, due Lemma 54,
K 0 min(SA,0 (p1 ) SA,0 (p2 )). Then, applying reasoning second part


proof Lemma 54, find (p1 + p2 )|K 0 HIDM,K
0 . Hence, p1 + p2 HSI,A .

Since already shown HSI,A closed taking positive linear combinations,
since V + (A) consists positive linear combinations Bernstein basis polynomials,

need show HSI,A
contains Bernstein basis polynomials order prove
+

V (A) HSI,A . consider NA {0}. Then, K 6= K A,
BA, |K = BK,rK () A[] K, BA, |K = 0 otherwise. implies

SA,0 (BA, ) = { =
6 K : A[] K} that, due Bernstein coherence HIDM,K
,


BA, |K = BK,rK () HIDM,K K SA,0 (BA, ). Hence, BA, |K HIDM,K

K min SA,0 (BA, ). Since also BA, 6= 0, find indeed BA, HSI,A
.


Proof Proposition 23. first prove HSI,A
c HSI,A,
. Consider p V (A)


pBA, HSI,A , meaning pBA, 6= 0 (pBA, )|K HIDM,K


K min SA,0 (pBA, ). set prove p HSI,A, . Since, clearly, p 6= 0, suffices

show p|K HIDM,K
crK () K min SA, (p). consider K min SA, (p),
implying A[] K therefore also K[rK ()] = A[]. also infer

Lemma 44 K min SA,0 (pBA, ), tells us (pBA, )|K HIDM,K
. Since

(pBA, )|K = p|K BA, |K = p|K BK,rK () , find p|K HIDM,K crK ().

84

fiCoherent Predictive Inference Exchangeability




Next, prove HSI,A,
HSI,A
c. Consider p HSI,A,
, meaning

p 6= 0 p|K HIDM,K crK () K min SA, (p). set prove


pBA, HSI,A
or, equivalently, pBA, =

6 0 (pBA, )|K HIDM,K
K min SA,0 (pBA, ). Since p 6= 0, continuity polynomials guarantees
int(A ) p() 6= 0 therefore also (pBA, )() 6= 0. know already
pBA, 6= 0. Consider K min SA,0 (pBA, ). Then, Lemma 44, K min SA, (p),


implying A[] K p|K HIDM,K
crK () therefore p|K BK,rK () HIDM,K
.

Since moreover p|K BK,rK () = (pBA, )|K , find indeed (pBA, )|K HIDM,K .

Proof Equation (63). Due Equation (17), suffices prove that, p V (A),


minxA p(x ) > 0 p HSI,A
minxA p(x ) < 0 p
/ HSI,A
.

First, assume minxA p(x ) < 0. p(y ) < 0.
Hence, since p|{y} = p(y ) < 0, find {y} min SA,0 (p) therefore also, due


Bernstein coherence HIDM,{y}
[see Theorem 21], p|{y}
/ HIDM,{y}
,

infer p
/ HSI,A .
Next, assume minxA p(x ) > 0. p|{x} = p(x ) > 0 x A, implying

min SA,0 (p) = {{x} : x A} that, x A, p|{x} HIDM,{x}
,


Bernstein coherence HIDM,{x} . Hence, since p 6= 0, find p HSI,A
.
Proof Equations (64) (65). Equation (65) follows directly Equation (55).
prove Equation (64). Due Equation (17) Proposition 23, suffices prove that,
NA p V (A):



> 0 p HSI,A,
<0p
c(p, )
/ HSI,A,
c(p, )
,



where, ease notation, let
:=
c(p, )

inf

sA[]


+ ).
DiA[]
|rA[]
(p|A[]
()


< 0, implying DiA[]
+ ) < 0
First, assume c(p, )
|rA[]
(p|A[]
()


A[]
6= 0 and, Equation (54), p|A[]

/
therefore also p|A[]





HIDM,A[]
(p) therefore also
(). Hence, find A[] min SA,
crA[]

p
/ HSI,A,
.


> 0, implying p|A[]
Next, assume c(p, )
6= 0 and, Equation (54),



therefore
p|A[]

H
cr
(
).
Hence,

find

min

(p) = {A[]}
A,

A[]

IDM,A[]


also p HSI,A,
.
NA :
Proof Equation (67). combining Equations (62) (30), see that,
{
}
s,1

= f L(A) : SA (f ) HSI,A,
DSI,A
c
(96)
.

Consider f L(A) distinguish two cases: f |A[]
6= 0 f |A[]
= 0.
f |A[]
6= 0 [and therefore also f 6= 0],
s,1

SA (f )|A[]
f DSI,A
c
HIDM,A[
()
crA[]
]


SA[]
(f |A[]
) HIDM,A[]
()
crA[]

85

fiDe Cooman, De Bock, & Diniz

s,1
f |A[]
DIDM,A[]
()
crA[]

1
1
f |A[]
>

f
(x)


f
>

f (x)mx f > 0,
x


|A[]



xA[]


xA[]

first equivalence due Statement (93) Equations (94), (61) (96).
second equivalence follows definition SA SA[]
third one due
Equations (21) (30). fourth equivalence consequence Equation (67)
final equivalence holds f > 0 redundant, given f |A[]
6= 0.
f |A[]
= 0, [again, using Statement (93) Equations (94), (61) (96)]
s,1
f 6= 0 x \ A[]:

f DSI,A
c

f (x) = 0 SA (f )|A[]{x}
HIDM,A[
crA[]{x}
().


]{x}


Since f |A[]
crA[]{x}
() Bernstein coherent [Theorem 21],
= 0 HIDM,A[]{x}


latter statement equivalent f (x) > 0. Hence, find that:
s,1
f 6= 0 (x \ A[])f

f DSI,A
c
(x) 0

f >0
f |A[]
>

1
f (x)mx f > 0,


xA[]

second third equivalences consequences f |A[]
= 0.
Lemma 55. Consider category sets onto map : D,
p V (D), NA {0}, 6= K A[] K.


(p R )|K HIDM,K
crK () p|(K) HIDM,(K)
cr(K) (R ()).
Proof Lemma 55. Let := K, := (K), := |K p := p|(K) .
onto map , p V (D ) p R = p|(K) R|K = (p R )|K .

Since A[] K, identify element := rK () NK
therefore
result follows representation insensitivity IDMM inference system
hyperparameter s, also R ( ) = R|K (rK ()) = r(K) (R ()):





p R HIDM,A
c p HIDM,D cR ( ).

Proposition 56. sSI representation insensitive.
Proof Proposition 56. use characterisation representation insensitivity Theorem 7. Consider category sets onto map : D,
p V (D) NA {0}. Then, Proposition 23, need prove


p R HSI,A,
p HSI,D,R
.
()


First, assume p HSI,D,R () , meaning p 6= 0 p|L HIDM,L
crL (R ())
L min SD,R () (p). Applying Lemma 45 K = A, infer p 6= 0
p R 6= 0. Consider K min SA, (p R ), 6= K A[] K. Then,
Lemma 46, (K) min SD,R () (p), implying that, due assumption, p|(K)
86

fiCoherent Predictive Inference Exchangeability



HIDM,(K)
cr(K) (R ()). Applying Lemma 55, find (p R )|K HIDM,K
crK ().

Hence, p R HSI,A, .

Assume, conversely, p R HSI,A,
, meaning p R 6= 0 (p R )|K

HIDM,K crK () K min SA, (p R ). Applying Lemma 45 K = A, infer
p R 6= 0 p 6= 0. Consider L min SD,R () (p). Lemma 46,
K min SA, (p R ) (K) = L. Since 6= K A, A[] K

and, assumption, (p R )|K HIDM,K
crK (), infer Lemma 55


p|L HIDM,L crL (R ()). Hence, p HSI,D,R () .

Lemma 57. Consider category sets B B A, p V (B),
NA {0}, r N0 K K B 6= A[] K.


IrB,A (p)|K HIDM,K
crK () p|KB HIDM,KB
crKB ().
Proof Lemma 57. Let := K, B := K B, p := p|KB r = deg(p) deg(p ) + r.
B , p V (B ), r r 0, r + deg(p ) = r + deg(p),


bdeg(p)+r
()BA,iA ()|K =
bdeg(p)+r
()BA,iA ()|K
IrB,A (p)|K =
p
p
deg(p)+r

deg(p)+r

NB

=



NB
B[]K


bdeg(p)+r
()BK,iK () = IrB ,A (p ),
p|
KB

deg(p)+r

NKB

third equality follows unicity Bernstein expansion polynomial.
Since A[] K, identify element := rK () NK {0} therefore
result follows specificity IDMM inference system hyperparameter s,
also rB ( ) = rKB ():







IrB ,A (p ) HIDM,A
c p HIDM,B crB ( ).

Proposition 58. sSI specific.
Proof Proposition 58. use characterisation specificity Theorem 11. Consider
category sets B B A, p V (B), NA {0}


r N0 . Then, Proposition 23, need prove IrB,A (p) HSI,A,
p HSI,B,r
.
B ()
clear Propositions 10 22 assume without loss generality
r + deg(p) > 0.


First, assume p HSI,B,r
, implying p =
6 0 p|L HIDM,L
crBL ()
B ()
L min SB,rB () (p). Applying Lemma 50 K = A, infer p 6= 0
IrB,A (p) 6= 0. Consider K min SA, (IrB,A (p)). infer Lemma 51 K

crKB ().
B min SB,rB () (p), implying that, due assumption, p|KB HIDM,KB
r

Since K B 6= 0 A[] K, IB,A (p)|K HIDM,K crK () Lemma 57. Hence,

IrB,A (p) HSI,A,
.

Assume, conversely, IrB,A (p) HSI,A,
, implies IrB,A (p) 6= 0
r

IB,A (p)|K HIDM,K crK () K min SA, (IrB,A (p)). Applying Lemma 50
K = A, infer IrB,A (p) 6= 0 p 6= 0. Consider L min SB,rB () (p).
Lemma 51, K min SA, (IrB,A (p)) K B = L. Since K B 6= 0,
87

fiDe Cooman, De Bock, & Diniz


A[] K and, assumption, IrB,A (p)|K HIDM,K
crK (), infer Lemma 57


p|KB HIDM,KB crKB (), words, p|L HIDM,L
crBL (). Hence,

p HSI,B,rB () .

Proof Theorem 24. immediate consequence Propositions 22 [coherence], 56
[representation insensitivity] 58 [specificity].
E.9 Proofs Results Section 14
Proof Theorem 25. begin coherence. Consider category set A,

prove HH,A Bernstein coherent. B1, recall 0
/ HIDM,A
> 0,
+

therefore also 0
/ HH,A . Similarly, B2, recall V (A) HIDM,A > 0,
+
therefore also V (A) HH,A . B3, consider n N k R>0 pk HH,A

k {1, . . . ,
n}. > 0 pk HIDM,A
k {1, . . . , n},
n


n therefore k=1 k pk HIDM,A , Bernstein coherence [Theorem 21]. Hence indeed
k=1 k pk HH,A .
Next, turn representation insensitivity, use characterisation Theorem 7.
Consider category sets onto map : D, p V (D)
NA {0}. find indeed:

(p R )BA, HH,A (s R>0 )(p R )BA, HIDM,A

(s R>0 )pBD,R () HIDM,D
pBD,R () HH,D ,

second equivalence follows representation insensitivity IDMM
inference systems [Theorem 21].
Finally, turn specificity, use characterisation Theorem 11. Consider
category sets B B A, p V (B), NA {0} r N0 .
find indeed:

IrB,A (p)BA, HH,A (s R>0 )IrB,A (p)BA, HIDM,A

(s R>0 )pBB,rB () HIDM,B
pBB,rB () HH,B ,

second equivalence follows specificity IDMM inference systems [Theorem 21].
NA {0} p V (A):
Proof Equation (69).

pBA,
p HH,A c
HH,A (s R>0 )pBA,
HIDM,A


(s R>0 )p HIDM,A
c.

Combined Equation (54), yields desired result.
NA {0} p V (A):
Proof Equation (71).
= sup { R : p HH,A c}

H H,A (p|)
{
}


= sup sup R : p HIDM,A
c
sR>0

88

fiCoherent Predictive Inference Exchangeability

+ ) = lim
inf DiA (p|

= sup

+ ),
inf DiA (p|

s+0 sA

sR>0

second equality due Equation (69), third one due Equation (54).
Proof Equation (72). Consider p V (A) apply Equation (71):
H H,A (p) = lim

inf DiA (p|) = lim

s+0 sA

inf DiA (p|s0 )

inf

s+0 int(A ) s0 (0,s)

(97)

fix n max{deg(p), 1} int(A ). Using Equation (80), find
NAn :
(

1

0

DiA (BA, |s ) =

s0 (n)

)
( )
n 0 (mx )
n
1
(m )
(s tx )
= (n)
(s0 tx ) x ,
0



xA
xA[]

x A[]:
(mx )

(s0 tx )

= (s0 tx )(s0 tx + 1) . . . (s0 tx + mx 1) = s0 tx (mx 1)![1 + O(s0 )]

similarly:
1
s0 (n)

=

s0 (n

1
[1 + O(s0 )].
1)!

Hence, find
(
0

DiA (BA, |s ) =

xA[] tx (mx

1)!

)

(n 1)!

s0|A[]|1 [1 + O(s0 )].

consider two cases: |A[]| > 1 |A[]| = 1 [since n 1, cases
exhaustive]. |A[]| > 1, DiA (BA, |s0 ) = O(s0 ). |A[]| = 1 or, equivalently,
x = nx , DiA (BA,nx |s0 ) = tx [1 + O(s0 )]. combine
Equation (78), find
DiA (p|s0 ) =



bnp () DiA (BA, |s0 ) =

n
NA



bnp (nx )tx + O(s0 ).

xA

Furthermore, due Equation (78):
bnp (nx ) =



bnp ()BA, (x ) = p(x ) x A.

n
NA

Hence, conclude
DiA (p|s0 ) =



p(x )tx + O(s0 ),

xA

which, combined Equation (97), leads desired result.
89

fiDe Cooman, De Bock, & Diniz

NA p V (A) use Equation (71):
Proof Equation (73). Consider
= lim
H H,A (p|)

= lim sup DiA (p|
+ ). (98)
+ ) H H,A (p|)
inf DiA (p|

s+0 sA

s+0



Bernstein coherent [Theorem 25], follows H H,A (|)
coherent
Since HH,A c
super-additive, conjugate upper
lower prevision. implies H H,A (|)
sub-additive. Hence, suffices prove equalities Equation (73)
prevision H H,A (|)
Bernstein basis polynomial p = BA, , NA {0}. sA
gather Equation (80) Appendix B that:
( )
n
+ ) =
(mx + x )(nx ) .
DiA (BA, |
(n)
(mA + )
xA
1

Observe that:
x)
(mx + x )(nx ) = (mx + x )(mx + x + 1) . . . (mx + x + nx 1) = m(n
[1 + O(x )],
x

similarly, since > 0:
1
(mA + )

(n)

1

=

(n)



[1 + O(A )]

Therefore:
( )
(nx )

n
xA mx
+ ) =
DiA (BA, |
[1
+
O(
)]
[1 + O(x )],

(n)


xA



which, using Equation (98), leads to:46
( )
(nx )
n
xA mx
= H H,A (BA, |)
=

H H,A (BA, |)
= DiA (BA, |).
(n)




E.10 Proofs Results Section 15
Proof Theorem 26. already argued smallest inference
system , shall denote lower probability function . First, assume n 2.
denote (n, k) := (n, k + 1) (n, k), follows assumptions
(n, k + 1) (n, k) 0 k n 2.

(99)

first going prove induction implies
(n, k)

k
(n, n) 0 k n.
n

46. See footnote 35.

90

(100)

fiCoherent Predictive Inference Exchangeability

Observe inequality holds trivially k = 0 [Theorem 14.L1]. assume
inequality holds k = `, ` {0, . . . , n 1}. must show also holds
k = ` + 1. Assume, ex absurdo, not, therefore
(n, ` + 1) <

`+1
1
(n, n) (n, `) + (n, n),
n
n

(101)

second inequality follows induction hypothesis. also
(n, n) = (n, ` + 1) +

n1


(n, m) (n, ` + 1) + (n ` 1)(n, `)

m=`+1

<

`+1
n`1
(n, n) +
(n, n) = (n, n),
n
n

first inequality follows Equation (99), second first
second inequalities Equation (101). contradiction, completes proof
induction (100).
infer (100), Theorem 14.L9 assumption (76)
(n, k)

k n
k
=
0 k n.
nn+s
n+s

(102)

Also observe inequality holds trivially n {0, 1}. get predictive
lower prevision P 1A (h|) gamble h A:

[h(x) min h]P 1A (I{x} |)
P 1A (h|) = min h + P 1A (h min h|) min h +
xA

= min h +



[h(x) min h](n, mx )

xA

min h +


xA

[h(x) min h]

mx
= P s,1
IDM,A (h|),
n+s

first equality first inequality follow coherence [P5, P2 P3]
P 1A (|), second equality representation insensitivity [Equation (33)],
second inequality Equation (102). converse inequality, observe IDMM
inference system sIDM coherent, representation insensitive, specific Theorem 21,
clearly concave surprise, satisfies assumption (76), therefore dominates smallest
inference system.

References
Augustin, T., Coolen, F. P. A., De Cooman, G., & Troffaes, M. C. M. (Eds.). (2014).
Introduction Imprecise Probabilities. John Wiley & Sons.
Bernard, J.-M. (1997). Bayesian analysis tree-structured categorized data. Revue Internationale de Systmique, 11, 1129.
Bernard, J.-M. (2005). introduction imprecise Dirichlet model multinomial
data. International Journal Approximate Reasoning, 39, 123150.
91

fiDe Cooman, De Bock, & Diniz

Bernard, J.-M. (2007). personal conversation..
Boole, G. (1847, reprinted 1961). Laws Thought. Dover Publications, New York.
Boole, G. (2004, reprint work originally published Watts & Co., London, 1952).
Studies Logic Probability. Dover Publications, Mineola, NY.
Carnap, R. (1952). continuum inductive methods. University Chicago Press.
Cifarelli, D. M., & Regazzini, E. (1996). De Finettis contributions probability statistics.
Statistical Science, 11, 253282.
Couso, I., & Moral, S. (2011). Sets desirable gambles: conditioning, representation,
precise probabilities. International Journal Approximate Reasoning, 52 (7), 1034
1055.
Cozman, F. G. (2013). Independence full conditional probabilities: Structure, factorization, non-uniqueness, bayesian networks. International Journal Approximate
Reasoning, 54 (9), 12611278.
De Cooman, G., & Miranda, E. (2007). Symmetry models versus models symmetry.
Harper, W. L., & Wheeler, G. R. (Eds.), Probability Inference: Essays Honor
Henry E. Kyburg, Jr., pp. 67149. Kings College Publications.
De Cooman, G., & Miranda, E. (2008a). F. Riesz Representation Theorem finite
additivity. Dubois, D., Lubiano, M. A., Prade, H., Gil, M. A., Grzegorzewski,
P., & Hryniewicz, O. (Eds.), Soft Methods Handling Variability Imprecision
(Proceedings SMPS 2008), pp. 243252. Springer.
De Cooman, G., & Miranda, E. (2008b). Weak strong laws large numbers coherent
lower previsions. Journal Statistical Planning Inference, 138 (8), 24092432.
De Cooman, G., & Miranda, E. (2012). Irrelevant independent natural extension
sets desirable gambles.. Journal Artificial Intelligence Research, 45, 601640.
De Cooman, G., Miranda, E., & Quaeghebeur, E. (2009a). Representation insensitivity
immediate prediction exchangeability. International Journal Approximate
Reasoning, 50 (2), 204216.
De Cooman, G., & Quaeghebeur, E. (2012). Exchangeability sets desirable gambles.
International Journal Approximate Reasoning, 53 (3), 363395. Special issue
honour Henry E. Kyburg, Jr.
De Cooman, G., Quaeghebeur, E., & Miranda, E. (2009b). Exchangeable lower previsions.
Bernoulli, 15 (3), 721735.
de Finetti, B. (1937). La prvision: ses lois logiques, ses sources subjectives. Annales de
lInstitut Henri Poincar, 7, 168. English translation Kyburg Jr. Smokler
(1964).
de Finetti, B. (1970). Teoria delle Probabilit. Einaudi, Turin.
de Finetti, B. (19741975). Theory Probability: Critical Introductory Treatment. John
Wiley & Sons, Chichester. English translation de Finettis (1970) book, two volumes.
Dubins, L. E. (1975). Finitely additive conditional probabilities, conglomerability
disintegrations. Annals Probability, 3, 8899.
92

fiCoherent Predictive Inference Exchangeability

Geisser, S. (1993). Predictive Inference: Introduction. Chapman & Hall.
Goldstein, M. (1983). prevision prevision. Journal American Statistical
Society, 87, 817819.
Goldstein, M. (1985). Temporal coherence. Bernardo, J. M., DeGroot, M. H., Lindley,
D. V., & Smith, A. F. M. (Eds.), Bayesian Statistics, Vol. 2, pp. 231248. North-Holland,
Amsterdam. discussion.
Good, I. J. (1965). Estimation Probabilities: Essay Modern Bayesian Methods.
MIT Press.
Haldane, J. B. S. (1945). method estimating frequencies. Biometrika, 33, 222225.
Hausdorff, F. (1923). Momentprobleme fr ein endliches Intervall. Mathematische Zeitschrift,
13, 220248.
Jaynes, E. T. (2003). Probability Theory: Logic Science. Cambridge University Press.
Jeffreys, H. (1998). Theory Probability. Oxford Classics series. Oxford University Press.
Reprint third edition (1961), corrections.
Johnson, N. L., Kotz, S., & Balakrishnan, N. (1997). Discrete Multivariate Distributions.
Wiley Series Probability Statistics. John Wiley Sons, New York.
Johnson, W. E. (1924). Logic, Part III. Logical Foundations Science. Cambridge
University Press. Reprinted Dover Publications 1964.
Keynes, J. M. (1921). Treatise Probability. Macmillan, London.
Koopman, B. O. (1940). Axioms Algebra Intuitive Probability. Annals
Mathematics, Second Series, 41 (2), 269292.
Kyburg Jr., H. E., & Smokler, H. E. (Eds.). (1964). Studies Subjective Probability. Wiley,
New York. Second edition (with new material) 1980.
Lad, F. (1996). Operational Subjective Statistical Methods: Mathematical, Philosophical
Historical Introduction. John Wiley & Sons.
Levi, I. (1980). Enterprise Knowledge. MIT Press, London.
Mangili, F., & Benavoli, A. (2013). New prior near-ignorance models simplex.
Cozman, F., Denux, T., Destercke, S., & Seidenfeld, T. (Eds.), ISIPTA 13
Proceedings Eighth International Symposium Imprecise Probability: Theories
Applications, pp. 213222. SIPTA.
Miranda, E. (2009). Updating coherent lower previsions finite spaces. Fuzzy Sets
Systems, 160 (9), 12861307.
Miranda, E., & De Cooman, G. (2014). Introduction Imprecise Probabilities, chap. Lower
previsions. John Wiley & Sons.
Miranda, E., & Zaffalon, M. (2011). Notes desirability conditional lower previsions.
Annals Mathematics Artificial Intelligence, 60 (3-4), 251309.
Moral, S. (2005). Epistemic irrelevance sets desirable gambles. Annals Mathematics
Artificial Intelligence, 45, 197214.
93

fiDe Cooman, De Bock, & Diniz

Moral, S., & Wilson, N. (1995). Revision rules convex sets probabilities. Coletti,
G., Dubois, D., & Scozzafava, R. (Eds.), Mathematical Models Handling Partial
Knowledge Artificial Intelligence, pp. 113128. Plenum Press, New York.
Piatti, A., Zaffalon, M., Trojani, F., & Hutter, M. (2009). Limits learning categorical
latent variable prior near-ignorance. International Journal Approximate
Reasoning, 50 (4), 597611.
Prautzsch, H., Boehm, W., & Paluszny, M. (2002). Bzier B-Spline Techniques. Springer,
Berlin.
Quaeghebeur, E. (2014). Introduction Imprecise Probabilities, chap. Desirability. John
Wiley & Sons.
Quaeghebeur, E., De Cooman, G., & Hermans, F. (2014). Accept & reject statement-based
uncertainty models. International Journal Approximate Reasoning. Accepted
publication.
Rouanet, H., & Lecoutre, B. (1983). Specific inference ANOVA: significance tests
Bayesian procedures. British Journal Mathematical Statistical Psychology,
36 (2), 252268.
Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (1995). representation partially ordered
preferences. Annals Statistics, 23, 21682217. Reprinted collection
Seidenfeld et al. (1999, pp. 69129).
Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (1999). Rethinking Foundations
Statistics. Cambridge University Press, Cambridge.
Sheffer, I. M. (1939). properties polynomial sets type zero. Duke Mathematical
Journal, 5, 590622.
Smith, C. A. B. (1961). Consistency statistical inference decision. Journal
Royal Statistical Society, Series A, 23, 137.
Troffaes, M. C. M., & De Cooman, G. (2014). Lower Previsions. Wiley.
Trump, W., & Prautzsch, H. (1996). Arbitrary degree elevation Bzier representations.
Computer Aided Geometric Design, 13, 387398.
Walley, P. (1991). Statistical Reasoning Imprecise Probabilities. Chapman Hall,
London.
Walley, P. (1996). Inferences multinomial data: learning bag marbles. Journal
Royal Statistical Society, Series B, 58, 357. discussion.
Walley, P. (1997). bounded derivative model prior ignorance real-valued
parameter. Scandinavian Journal Statistics, 24 (4), 463483.
Walley, P. (2000). Towards unified theory imprecise probability. International Journal
Approximate Reasoning, 24, 125148.
Walley, P., & Bernard, J.-M. (1999). Imprecise probabilistic prediction categorical data.
Tech. rep. CAF-9901, Laboratoire Cognition et Activites Finalises, Universit de
Paris 8.
94

fiCoherent Predictive Inference Exchangeability

Williams, P. M. (1975a). Coherence, strict coherence zero probabilities. Proceedings
Fifth International Congress Logic, Methodology Philosophy Science,
Vol. VI, pp. 2933. Dordrecht. Proceedings 1974 conference held Warsaw.
Williams, P. M. (1975b). Notes conditional previsions. Tech. rep., School Mathematical
Physical Science, University Sussex, UK. See also revised journal version
Williams (2007).
Williams, P. M. (1976). Indeterminate probabilities. Przelecki, M., Szaniawski, K., &
Wojcicki, R. (Eds.), Formal Methods Methodology Empirical Sciences, pp.
229246. Reidel, Dordrecht. Proceedings 1974 conference held Warsaw.
Williams, P. M. (2007). Notes conditional previsions. International Journal Approximate
Reasoning, 44, 366383.
Zabell, S. L. (1982). W. E. Johnsons sufficientness postulate. Annals Statistics, 10,
10901099. Reprinted collection Zabell (2005).
Zabell, S. L. (2005). Symmetry Discontents: Essays History Inductive Probability. Cambridge Studies Probability, Induction, Decision Theory. Cambridge
University Press, Cambridge, UK.
Zaffalon, M., & Miranda, E. (2013). Probability time. Artificial Intelligence, 198, 151.

95

fiJournal Artificial Intelligence Research 52 (2015) 399443

Submitted 08/14; published 03/15

Computing Convex Coverage Sets
Faster Multi-objective Coordination
Diederik M. Roijers
Shimon Whiteson
Frans A. Oliehoek

d.m.roijers@uva.nl
s.a.whiteson@uva.nl
f.a.oliehoek@uva.nl

Informatics Institute
University Amsterdam
Amsterdam, Netherlands

Abstract
article, propose new algorithms multi-objective coordination graphs (MOCoGs). Key efficiency algorithms compute convex coverage
set (CCS) instead Pareto coverage set (PCS). CCS sufficient solution
set large class problems, also important characteristics facilitate
efficient solutions. propose two main algorithms computing CCS MO-CoGs.
Convex multi-objective variable elimination (CMOVE) computes CCS performing
series agent eliminations, seen solving series local multi-objective
subproblems. Variable elimination linear support (VELS) iteratively identifies single
weight vector w lead maximal possible improvement partial CCS
calls variable elimination solve scalarized instance problem w. VELS
faster CMOVE small medium numbers objectives compute
-approximate CCS fraction runtime. addition, propose variants
methods employ AND/OR tree search instead variable elimination achieve
memory efficiency. analyze runtime space complexities methods, prove
correctness, compare empirically naive baseline existing
PCS method, terms memory-usage runtime. results show that,
focusing CCS, methods achieve much better scalability number
agents current state art.

1. Introduction
many real-world problem domains, maintenance planning (Scharpff, Spaan,
Volker, & De Weerdt, 2013) traffic light control (Pham et al., 2013), multiple agents
need coordinate actions order maximize common utility. Key coordinating
efficiently domains exploiting loose couplings agents (Guestrin, Koller,
& Parr, 2002; Kok & Vlassis, 2004): agents actions directly affect subset
agents.
Multi-agent coordination complicated fact that, many domains, agents need
balance multiple objectives (Roijers, Vamplew, Whiteson, & Dazeley, 2013a). example, agents might maximize performance computer network minimizing
power consumption (Tesauro, Das, Chan, Kephart, Lefurgy, Levine, & Rawson, 2007),
maximize cost efficiency maintenance tasks road network minimizing traffic
delays (Roijers, Scharpff, Spaan, Oliehoek, de Weerdt, & Whiteson, 2014).
c
2015
AI Access Foundation. rights reserved.

fiRoijers, Whiteson, & Oliehoek

Figure 1: Mining company example.
However, presence multiple objectives per se necessitate use
specialized multi-objective solution methods. problem scalarized, i.e.,
utility function converted scalar utility function, problem may solvable
existing single-objective methods. conversion involves two steps (Roijers et al.,
2013a). first step specify scalarization function.
Definition 1. scalarization function f , function maps multi-objective utility
solution decision problem, u(a), scalar utility uw (a):
uw (a) = f (u(a), w),
w weight vector parameterizes f .
second step define single-objective version decision problem
utility solution equals scalarized utility original problem uw (a).
Unfortunately, scalarizing problem solving always possible
w may known advance. example, consider company mines different
resources. Figure 1, depict problem company faces: morning one van
per village needs transport workers village nearby mine, various
resources mined. Different mines yield different quantities resource per worker.
market prices per resource vary stochastic process every price change
alter optimal assignment vans. expected price variation increases
passage time. maximize performance, thus critical act based latest
possible price information. Since computing optimal van assignment takes time, redoing
computation every price change highly undesirable.
settings, need multi-objective method computes, advance, optimal solution possible prices, w. call set coverage set (CS). many
cases, w revealed solution must executed, case solution
automatically selected CS given w. cases, w never made explicit
instead human involved decision making selects one solution
CS, perhaps basis constraints preferences difficult formalize
objectives (Roijers et al., 2013a). cases, CS typically
much smaller complete set solutions, selecting optimal joint action
CS typically much easier selecting directly complete set solutions.
400

fiComputing CCSs Faster Multi-objective Coordination

article, consider multi-objective methods made efficient problems require coordination multiple, loosely coupled agents. particular, address multi-objective coordination graphs (MO-CoGs): one-shot multi-agent decision problems loose couplings expressed using graphical model. MO-CoGs form
important class decision problems. used model variety realworld problems (Delle Fave, Stranders, Rogers, & Jennings, 2011; Marinescu, 2011; Rollon,
2008), many sequential decision problems modeled series MO-CoGs,
common single-objective problems (Guestrin et al., 2002; Kok & Vlassis, 2004; Oliehoek,
Spaan, Dibangoye, & Amato, 2010).
Key efficiency MO-CoG methods propose compute convex
coverage set (CCS) instead Pareto coverage set (PCS). CCS subset
PCS sufficient solution multi-objective problem linear scalarization
function. example, mining company example Figure 1, f linear, since
total revenue simply sum quantity resource mined times price per
unit. However, even f nonlinear, stochastic solutions allowed, CCS
sufficient.1
CCS previously considered solution concept MO-CoGs
computing CCS requires running linear programs, whilst computing PCS requires
pairwise comparisons solutions. However, key insight article2 that, loosely
coupled systems, CCSs easier compute PCSs, two reasons. First, CCS
(typically much smaller) subset PCS. loosely coupled settings, efficient methods
work solving series local subproblems; focusing CCS greatly reduce size
subproblems. Second, focusing CCS makes solving MO-CoG equivalent
finding optimal piecewise-linear convex (PWLC) scalarized value function,
efficient techniques adapted. reasons, argue CCS often
concept choice MO-CoGs.
propose two approaches exploit insights solve MO-CoGs efficiently
existing methods (Delle Fave et al., 2011; Dubus, Gonzales, & Perny, 2009; Marinescu,
Razak, & Wilson, 2012; Rollon & Larrosa, 2006). first approach deals multiple
objectives level individual agents, second deals global
level.
first approach extends algorithm Rollon Larrosa (2006) refer
Pareto multi-objective variable elimination (PMOVE) 3 , computes local Pareto
sets agent elimination, compute CCS instead. call resulting algorithm
convex multi-objective variable elimination (CMOVE).
second approach new abstract algorithm call optimistic linear support
(OLS) much faster small medium numbers objectives. Furthermore, OLS
1. precise, case stochastic strategies CCS deterministic strategies always sufficient
(Vamplew, Dazeley, Barker, & Kelarev, 2009); case deterministic strategies, linearity
scalarization function makes CCS sufficient (Roijers et al., 2013a).
2. article synthesizes extends research already reported two conference papers. Specifically,
CMOVE algorithm (Section 4) previously published ADT (Roijers, Whiteson, & Oliehoek,
2013b) VELS algorithm (Section 5) AAMAS (Roijers, Whiteson, & Oliehoek, 2014).
memory-efficient methods computing CCSs (Section 6) novel contribution article.
3. original article, algorithm called multi-objective bucket elimination (MOBE). However,
use PMOVE consistent names algorithms mentioned article.

401

fiRoijers, Whiteson, & Oliehoek

used produce bounded approximation CCS, -CCS,
enough time compute full CCS. OLS generic method employs single-objective
solvers subroutine. article, consider two implementations subroutine.
Using variable elimination (VE) subroutine yields variable elimination linear support
(VELS), particularly fast small moderate numbers objectives
memory-efficient CMOVE. However, memory highly limited, reduction
memory usage may enough. cases, using AND/OR search (Mateescu &
Dechter, 2005) instead yields AND/OR tree search linear support (TSLS),
slower VELS much memory efficient.
prove correctness CMOVE OLS. analyze runtime space
complexities methods show methods better guarantees
PCS methods. show CMOVE OLS complementary, i.e., various trade-offs exist
variants.
Furthermore, demonstrate empirically, randomized realistic problems, CMOVE VELS scale much better previous algorithms. also empirically confirm trade-offs CMOVE OLS. show OLS, used
bounded approximation algorithm, save additional orders magnitude runtime,
even small . Finally, show that, even memory highly limited, TSLS
still solve large problems.
rest article structured follows. First, provide formal definition
model, well overview existing solution methods Section 2.
presenting naive approach Section 3, Sections 4, 5 6, analyze runtime
space complexities algorithm, compare empirically,
existing algorithms, end section. Finally, conclude Section 7
overview contributions findings, suggestions future research.

2. Background
section, formalize multi-objective coordination graph (MO-CoG).
however, describe single-objective version problem, coordination graph
(CoG), MO-CoG extension, variable elimination (VE) algorithm
solving CoGs. methods present Section 4 5 build different ways.
2.1 (Single-Objective) Coordination Graphs
coordination graph (CoG) (Guestrin et al., 2002; Kok & Vlassis, 2004) tuple hD, A, Ui,

= {1, ..., n} set n agents,
= Ai ... joint action space: Cartesian product finite action
spaces agents. joint action thus tuple containing action agent
= ha1 , ..., i,


U = u1 , ..., u set scalar local payoff functions, limited
scope, i.e., depends onlyPa subset agents. total team payoff sum
local payoffs: u(a) = e=1 ue (ae ).
402

fiComputing CCSs Faster Multi-objective Coordination

Figure 2: (a) CoG 3 agents 2 local payoff functions (b) eliminating agent 3
adding u3 (c) eliminating agent 2 adding u4 .

a1
a1

a2
3.25
1.25

a2
0
3.75

a2
a2

a3
2.5
0

a3
1.5
1

Table 1: payoff matrices u1 (a1 , a2 ) (left) u2 (a2 , a3 ) (right). two possible
actions per agent, denoted dot (a1 ) bar (a1 ).

agents share payoff function u(a). abuse notation e index local
payoff function ue denote subset agents scope; ae thus local joint
action, i.e., joint action subset agents.
decomposition u(a) local payoff functions represented factor
graph (Bishop, 2006), bipartite graph containing two types vertices: agents (variables)
local payoff functions (factors), edges connecting local payoff functions
agents scope.
Figure 2a shows factor graph example CoG team payoff function
decomposes two local payoff functions, two agents scope:
u(a) =


X

ue (ae ) = u1 (a1 , a2 ) + u2 (a2 , a3 ).

e=1

local payoff functions defined Table 1. factor graph illustrates loose
couplings result decomposition local payoff functions. particular,
agents choice action directly depends immediate neighbors, e.g.,
agent 1 knows agent 2s action, choose action without considering agent 3.
2.2 Variable Elimination
discuss variable elimination (VE) algorithm, several multi-objective
extensions (Rollon & Larrosa, 2006; Rollon, 2008) build, including CMOVE algorithm (Section 4). also use subroutine OLS algorithm (Section 5).
exploits loose couplings expressed local payoff functions efficiently
compute optimal joint action, i.e., joint action maximizing u(a). First, forward
403

fiRoijers, Whiteson, & Oliehoek

pass, eliminates agents turn computing value agents best
response every possible joint action neighbors. values used construct
new local payoff function encodes value best response replaces agent
payoff functions participated. original algorithm, agents
eliminated, backward pass assembles optimal joint action using constructed
payoff functions. Here, present slight variant payoff tagged
action generates it, obviating need backwards pass. two algorithms
equivalent, variant amenable multi-objective extension present
Section 4.
eliminates agents graph predetermined order. Algorithm 1 shows
pseudocode elimination single agent i. First, determines set local
payoff functions connected i, Ui , neighboring agents i, ni (lines 1-2).
Definition 2. set neighboring local payoff functions Ui set local
payoff functions agent scope.
Definition 3. set neighboring agents i, ni , set agents
scope one local payoff functions Ui .
Then, constructs new payoff function computing value agent best
response possible joint action ani agents ni (lines 3-12). so,
loops joint actions Ani (line 4). ani , loops actions Ai
available agent (line 6). ai Ai , computes local payoff agent
responds ani ai (line 7). tags total payoff ai , action generates
(line 8) order able retrieve optimal joint action later. already
tags present, appends ai them; way, entire joint action incrementally
constructed. maintains value best response taking maximum
payoffs (line 11). Finally, eliminates agent payoff functions Ui replaces
newly constructed local payoff function (line 13).
Algorithm 1: elimVE(U, i)
1
2
3
4
5
6
7

Input: CoG U, agent
Ui set local payoff functions involving
ni set neighboring agents
unew new factor taking joint actions ni , ani , input
foreach ani Ani

foreach aX
Ai
v
uj (ani , ai )
uj Ui

8
9
10
11
12
13

tag v ai
{v}
end
unew (ani ) max(S)
end
return (U \ Ui ) {unew }

404

fiComputing CCSs Faster Multi-objective Coordination

Consider example Figure 2a Table 1. optimal payoff maximizes sum
two payoff functions:
max u(a) = max u1 (a1 , a2 ) + u2 (a2 , a3 ).


a1 ,a2 ,a3

eliminates agent 3 first, pushes maximization a3 inward
goes local payoff functions involving agent 3, case u2 :


1
2
max u(a) = max u (a1 , a2 ) + max u (a2 , a3 ) .


a1 ,a2

a3

solves inner maximization replaces new local payoff function u3
depends agent 3s neighbors, thereby eliminating agent 1:

max u(a) = max u1 (a1 , a2 ) + u3 (a2 ) ,


a1 ,a2

leads new factor graph depicted Figure 2b. values u3 (a2 ) u3 (a2 ) =
2.5, using a3 , u3 (a2 ) = 1 using a3 , optimal payoffs actions
agent 2, given payoffs shown Table 1. ultimately want optimal joint
action, optimal payoff, tags payoff u3 action agent 3
generates it, i.e., think u3 (a2 ) (value, tag) pair. denote pair
parentheses subscript: u3 (a2 ) = (2.5)a3 , u3 (a2 ) = (1)a3 .
next eliminates agent 2, yielding factor graph shown Figure 2c:


1
3
max u(a) = max max u (a1 , a2 ) + u (a2 ) = max u4 (a1 ).


a1

a2

a1

appends new tags agent 2 existing tags agent 3, yielding following
tagged payoff values: u4 (a1 ) = maxa2 u1 (a1 , a2 ) + u3 (a2 ) =(3.25)a2 + (2.5)a2 a3 = (5.75)a2 a3
u4 (a1 ) = (3.75)a2 + (1)a2 a3 = (4.75)a2 a3 . Finally, maximizing a1 yields optimal
payoff (5.75)a1 a2 a3 , optimal action contained tags.
runtime complexity exponential, number agents,
induced width, often much less number agents.
Theorem 1. computational complexity O(n|Amax |w ) |Amax |
maximal number actions single agent w induced width, i.e., maximal
number neighboring agents agent plus one (the agent ), moment
eliminated (Guestrin et al., 2002).
Theorem 2. space complexity O( n |Amax |w ).
space complexity arises because, every agent elimination, new local payoff
function created O(|Amax |w ) fields (possible input actions). Since impossible
tell priori many new local payoff functions exist given time
execution VE, need multiplied total number new local payoff
functions created execution, n.
designed minimize runtime4 methods focus memory efficiency
instead (Mateescu & Dechter, 2005). discuss memory efficiency Section 6.1.
4. fact, proven best runtime guarantees within large class algorithms (Rosenthal,
1977).

405

fiRoijers, Whiteson, & Oliehoek

a1
a1

a2
(4,1)
(1,2)

a2
(0,0)
(3,6)

a2
a2

a3
(3,1)
(0,0)

a3
(1,3)
(1,1)

Table 2: two-dimensional payoff matrices u1 (a1 , a2 ) (left) u2 (a2 , a3 ) (right).
2.3 Multi-objective Coordination Graphs
multi-objective coordination

graph (MO-CoG) tuple hD, A, Ui
but, U = u1 , ..., u set , d-dimensional local P
payoff functions.
total team payoff sum local vector-valued payoffs: u(a) = e=1 ue (ae ). use
ui indicate value i-th objective. denote set possible joint action
values V. Table 2 shows two-dimensional MO-CoG structure
single-objective example Section 2.1, multi-objective payoffs.
solution MO-CoG coverage set (CS) joint actions associated values
u(a) contains least one optimal joint action possible parameter vector w
scalarization function f (Definition 1). CS subset undominated set:
Definition 4. undominated set (U) MO-CoG, set joint actions
associated payoff values optimal w scalarization function f .


U (V) = u(a) : u(a) V wa0 uw (a) uw (a0 ) .
care least one optimal joint action every w, rather
optimal joint actions, lossless subset U suffices:
Definition 5. coverage set (CS), CS(V), subset U , possible w,
least one optimal solution CS, i.e.,
wa


u(a) CS(V) a0 uw (a) uw (a0 ) .

Note CS necessarily unique. Typically seek smallest possible CS.
convenience, assume payoff vectors CS contain values associated
joint actions, suggested tagging scheme described Section 2.2.
payoff vectors V CS depends know
scalarization function f . minimal assumption f monotonically increasing, i.e.,
value one objective ui , increases uj6=i stay constant, scalarized value
u(a) cannot decrease. assumption ensures objectives desirable, i.e., else
equal, always better.
Definition 6. Pareto front undominated set arbitrary strictly monotonically
increasing scalarization functions f .


P F (V) = u(a) : u(a) V a0 u(a0 ) P u(a) ,
P indicates Pareto dominance (P-dominance): greater equal objectives
strictly greater least one objective.
406

fiComputing CCSs Faster Multi-objective Coordination

order optimal scalarized values, necessary compute entire
PF. E.g., two joint actions equal payoffs need retain one those.
Definition 7. Pareto coverage set (PCS), P CS(V) P F (V), coverage set
arbitrary strictly monotonically increasing scalarization functions f , i.e.,

a0 u(a) P CS(V) (u(a) P u(a0 ) u(a) = u(a0 )) .
Computing P-dominance requires pairwise comparison payoff vectors (Feng &
Zilberstein, 2004).5
highly prevalent scenario that, addition f monotonically increasing,
also know linear, is, parameter vectors w weights
values individual objectives multiplied, f = w u(a). mining example
Figure 1, resources traded open market resources positive unit
price. case, scalarization linear combination amount resource
mined, weights correspond price per unit resource. Many
examples linear scalarization functions exist literature, e.g., (Lizotte, Bowling, &
Murphy, 2010). assume linear scalarization monotonically increasing,
represent without loss generality convex combination objectives: i.e.,
weights positive sum 1. case, convex coverage set (CCS)
needed, subset convex hull (CH) 6 :
Definition 8. convex hull (CH) undominated set linear non-decreasing
scalarizations f (u(a), w) = w u(a):


CH(V) = u(a) : u(a) V wa0 w u(a) w u(a0 ) .
is, CH contains solutions attain optimal value least one weight.
Vectors CH C-dominated. contrast P-domination, C-domination cannot
tested pairwise comparisons take two payoff vectors
C-dominate payoff vector. Note CH contains solutions needed guarantee optimal scalarized value value: contain multiple solutions optimal
one specific weight. lossless subset CH respect linear scalarizations
called convex coverage set (CCS), i.e., CCS retains least one u(a) maximizes
scalarized payoff, w u(a), every w:
Definition 9. convex coverage set (CCS), CCS(V) CH(V), CS linear nondecreasing scalarizations, i.e.,

wa u(a) CCS(V) a0 w u(a) w u(a0 ) .
Since linear non-decreasing functions specific type monotonically increasing function, always CCS subset smallest possible PCS.
previously mentioned, CSs like PCS CCS, may unique. example,
two joint actions equal payoff vectors, need one
make PCS CCS.
5. P-dominance often called pairwise dominance POMDP literature.
6. Note term convex hull overloaded. graphics, convex hull superset mean
convex hull article.

407

fiRoijers, Whiteson, & Oliehoek

Figure 3: CCS (filled circles left, solid black lines right) versus PCS (filled circles
squares left, dashed solid black lines right) twelve random
2-dimensional payoff vectors.

practice, PCS CCS often equal PF CH. However,
algorithms proposed article guaranteed produce PCS CCS,
necessarily entire PF CH. PCSs CCSs sufficient solutions
terms scalarized value, say algorithms solve MO-CoGs.
Figure 3 (left) values joint actions, u(a), represented points valuespace, two-objective MO-CoG. joint action value CCS
PCS. B, however, PCS, CCS, weight
linear scalarization Bs value would optimal, shown Figure 3 (right),
scalarized value strategies plotted function weight first objective
(w2 = 1 w1 ). C neither CCS PCS: Pareto-dominated A.
Many multi-objective methods, e.g., (Delle Fave et al., 2011; Dubus et al., 2009; Marinescu et al., 2012; Rollon, 2008) simply assume PCS appropriate solution
concept. However, argue choice CS depends one assume
utility defined respect multiple objectives, i.e., scalarization function used scalarize vector-valued payoffs. argue many situations
scalarization function linear, cases one use CCS.
addition shape f , choice solution concept depends whether
deterministic joint actions considered whether stochastic strategies also permitted. stochastic strategy assigns probabilityPto joint action [0, 1].
probabilities joint actions together sum 1, aA (a) = 1. value stochastic strategy linear
P combination value vectors joint actions

mixture: u =
aA (a)u(a). Therefore, optimal values, monotonically
increasing f , lie convex upper surface spanned strategies CCS,
indicated lines Figure 3 (left). Therefore, optimal values monotonically
increasing f , including nonlinear ones, constructed taking mixture policies
CCS (Vamplew et al., 2009).
article considers methods computing CCSs, which, show Sections 4
5, computed efficiently PCSs. Furthermore, CCSs typically much
408

fiComputing CCSs Faster Multi-objective Coordination

smaller. particularly important final selection joint done (a
group of) humans, compare possible alternatives solution set.
methods presented article based variable elimination (VE) (Sections
4 5) AND/OR tree search (TS) (Section 6). algorithms exact solution
methods CoGs.
CMOVE algorithm propose Section 5 based VE. differs another
multi-objective algorithm based VE, refer PMOVE (Rollon & Larrosa,
2006), produces CCS rather PCS. alternative messagepassing algorithms, like max-plus (Pearl, 1988; Kok & Vlassis, 2006a). However,
guaranteed exact tree-structured CoGs. Multi-objective methods build
max-plus Delle Fave et al. (2011), limitation, unless
preprocess CoG form clique-tree GAI network (Dubus et al., 2009).
tree structured graphs, message-passing algorithms produce optimal solutions
similar runtime guarantees. Note that, like PMOVE, existing multi-objective methods
based message passing produce PCS rather CCS.
Section 5, take different approach multi-objective coordination based
outer loop approach. explain, approach applicable computing CCS,
PCS, considerable advantages terms runtime memory usage.

3. Non-graphical Approach
naive way compute CCS ignore graphical structure, calculate set
possible payoffs joint actions V, prune away C-dominated joint actions.
first translate problem set value set factors (VSFs), F. VSF f function
mapping local joint actions sets payoff vectors. initial VSFs constructed
local payoff functions
f e (ae ) = {ue (ae )},
i.e., VSF maps local joint action singleton set containing actions
local payoff. define V terms F using cross-sum operator VSFs
F joint action a:
[M
V(F) =
f e (ae ),
f e F

cross-sum two sets B contains possible vectors made
summing one payoff vector set:
B = {a + b : b B} .
CCS calculated applying pruning operator CPrune (described below)
removes C-dominated vectors set value vectors, V:
[M
CCS(V(F)) = CPrune(V(F)) = CPrune(
f e (ae )).
(1)
f e F

non-graphical CCS algorithm simply computes righthand side Equation 1, i.e.,
computes V(F) explicitly looping actions, action looping
local VSFs, pruning set CCS.
409

fiRoijers, Whiteson, & Oliehoek

CCS contains least one payoff vector maximizes scalarized value every
w:
w



= arg max w u(a)



= a0

u(a0 ) CCS(V(F)) w u(a) = w u(a0 ). (2)

aA

is, every w solution a0 part CCS achieves
value maximizing solution a. Moreover value solutions given
dot product. Thus, finding CCS analogous problem faced partially observable
Markov decision processes (POMDPs) (Feng & Zilberstein, 2004), optimal -vectors
(corresponding value vectors u(a)) beliefs (corresponding weight vectors
w) must found. Therefore, employ pruning operators POMDP literature.
Algorithm 2 describes implementation CPrune, based Feng
Zilberstein (2004) one modification. order improve runtime guarantees, CPrune
first pre-prunes candidate solutions U PCS using PPrune (Algorithm 3) line
1. PPrune computes PCS O(d|U||P CS|) running pairwise comparisons. Next,
partial CCS, U , constructed follows: random vector u U selected line 4.
u algorithm tries find weight vector w u better vectors
U (line 5), solving linear program Algorithm 4. w, CPrune
finds best vector v w U moves U (line 1113). weight
u better C-dominated thus removed u U (line 8).
Algorithm 2: CPrune(U)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

Input: set payoff vectors U
U PPrune(U)
U
notEmpty(U)
select random u U
w findWeight(u, U )
w=null
//did find weight u optimal
remove u U
end
else
v arg maxuU w u
U U \ {v}
U U {v}
end
end
return U

runtime CPrune defined Algorithm 2
O(d|U||P CS| + |P CS|P (d|CCS|)),

(3)

P (d|CCS|) polynomial size CCS number objectives d,
runtime linear program tests C-domination (Algorithm 4).
410

fiComputing CCSs Faster Multi-objective Coordination

Algorithm 3: PPrune(U)
1
2
3
4
5
6
7
8
9
10
11
12

Input: set payoff vectors U
U
U 6=
u first element U
foreach v U
v P u
u v // Continue v instead u
end
end
Remove u, vectors P-dominated u, U
Add u U
end
return U

Algorithm 4: findWeight(u, U)
max x
x,w

subject w (u u0 ) x 0, u0 U

X

wi = 1

i=1

x > 0 return w else return null

key downside non-graphical approach requires explicitly enumerating
possible joint actions calculating payoffs associated one. Consequently,
intractable small numbers agents, number joint actions grows
exponentially number agents.
Theorem 3. time complexity computing CCS MO-CoG containing local
payoff functions, following non-graphical approach (Equation 1) is:
O(d|Amax |n + d|Amax |n |P CS| + |P CS|P (d|CCS|)
Proof. First, V computed looping VSFs joint action a, summing
vectors length d. maximum size action space agent Amax
O(|Amax |n ) joint actions. V contains one payoff vector joint action. V input
CPrune.
next two sections, present two approaches compute CCSs efficiently.
first approach pushed CPrune operator Equation 1 cross-sum
union, max-operator pushed summation VE. call
inner loop approach, uses pruning operators agent eliminations,
inner loop algorithm. second approach inspired linear support (Cheng,
411

fiRoijers, Whiteson, & Oliehoek

1988), POMDP pruning operator requires finding optimal solution certain w. Instead performing maximization entire set V, original linear
support algorithm, show use finite number scalarized instances
MO-CoG, avoiding explicit calculation V. call approach outer loop
approach, creates outer loop around single objective method (like VE),
calls subroutine.

4. Convex Variable Elimination MO-CoGs
section show exploit loose couplings calculate CCS using inner loop approach, i.e., pushing pruning operators cross-sum union
operators Equation 1. result CMOVE, extension Rollon Larrosas
Pareto-based extension VE, refer PMOVE (Rollon & Larrosa, 2006).
analyzing CMOVEs complexity terms local convex coverage sets, show
approach yields much better runtime complexity guarantees non-graphical
approach computing CCSs presented Section 3.
4.1 Exploiting Loose Couplings Inner Loop
non-graphical approach, computing CCS expensive computing PCS,
shown Section 3. show that, MO-CoGs, compute CCS
much efficiently exploiting MO-CoGs graphical structure. particular, like
VE, solve MO-CoG series local subproblems, eliminating agents
manipulating set VSFs F describe MO-CoG. key idea compute
local CCSs (LCCSs) eliminating agent instead single best response (as VE).
computing LCCS, algorithm prunes away many vectors possible.
minimizes number payoff vectors calculated global level,
greatly reduce computation time. describe elim operator eliminating agents
used CMOVE Section 4.2.
first need update definition neighboring local payoff functions (Definition
2), neighboring VSFs.
Definition 10. set neighboring VSFs Fi set local payoff functions
agent scope.
neighboring agents ni agent agents scope VSF
Fi , except itself, corresponding Definition 3. possible local joint action
ni , compute LCCS contains payoffs C-undominated responses
agent i, best response values i. words, CCS subproblem
arises considering Fi fixing specific local joint action ani . compute
LCCS, must consider payoff vectors subproblem, Vi , prune dominated
ones.
Definition 11. fix actions
ani , ai , set payoff vectors
L
subproblem is: Vi (Fi , ani ) = ai f e Fi f e (ae ), ae formed ai
appropriate part ani .
Using Definition 11, define LCCS CCS Vi .
412

fiComputing CCSs Faster Multi-objective Coordination

Definition 12. local CCS, LCCS, C-undominated subset Vi (Fi , ani ):
LCCSi (Fi , ani ) = CCS(Vi (Fi , ani )).

Using LCCSs, create new VSF, f new , conditioned actions
agents ni :
ani f new (ani ) = LCCS (Fi , ani ).
elim operator replaces VSFs Fi F new factor:
elim(F, i) = (F \ Fi ) {f new (ani )}.
Theorem 4. elim preserves CCS: F CCS(V(F)) = CCS(V(elim(F, i))).
Proof. show using implication Equation 2, i.e., joint actions
w scalarized value maximal, vector-valued payoff
u(a0 ) w u(a0 ) = w u(a0 ) CCS. show maximal scalarized
payoff cannot lost result elim.

function distributes local payoff functions: w u(a) =
P linear scalarization
P
w e ue (ae ) = e w ue (ae ). Thus, eliminating agent i, divide set VSFs
non-neighbors (nn), agent participate, neighbors (ni )
that:
X
X
w u(a) =
w ue (ae ) +
w ue (ae ).
enn

eni

Now, following Equation 2, CCS contains maxaA w u(a) w. elim pushes
maximization in:
max w u(a) = max
aA

ai Ai

X

w ue (ae ) + max

ai Ai

enn

X

w ue (ae ).

eni

elim
agent-i factors term f new (ani ) satisfies w f new (ani ) = maxai
P replaces
e
eni w u (ae ) per definition, thus preserving maximum scalarized value w
thereby preserving CCS.
Instead LCCS, could compute local PCS (LPCS), is, using PCS
computation Vi instead CCS computation. Note that, since LCCS LPCS Vi ,
elim reduces problem size respect Vi , would
possible considered P-dominance. Therefore, focusing CCS greatly
reduce sizes local subproblems. Since solution local subproblem input
next agent elimination, size subsequent local subproblems also reduced,
lead considerable speed-ups.
413

fiRoijers, Whiteson, & Oliehoek

4.2 Convex Multi-objective Variable Elimination
present convex multi-objective variable elimination (CMOVE) algorithm,
implements elim using CPrune. Like VE, CMOVE iteratively eliminates agents none
left. However, implementation elim computes CCS outputs correct
joint actions payoff vector CCS, rather single joint action. CMOVE
extension Rollon Larrosas Pareto-based extension VE, refer
PMOVE (Rollon & Larrosa, 2006).
important difference CMOVE PMOVE CMOVE computes CCS, typically leads much smaller subproblems thus much better
computational efficiency. addition, identify three places pruning take
place, yielding flexible algorithm different trade-offs. Finally, use tagging scheme instead backwards pass, Section 2.2.
Algorithm 5 presents abstract version CMOVE leaves pruning operators
unspecified. Section 3, CMOVE first translates problem set vector-set
factors (VSFs), F line 1. Next, CMOVE iteratively eliminates agents using elim (line
25). elimination order determined using techniques devised single-objective
(Koller & Friedman, 2009).
Algorithm 5: CMOVE(U, prune1, prune2, prune3, q)

1
2
3
4
5
6
7
8

Input: set local payoff functions U elimination order q (a queue containing
agents)
F create one VSF every local payoff function U
ani Ani
q.dequeue()
F elim(F, i, prune1, prune2)
end
f retrieve final factor F
f (a )
return prune3(S)

Algorithm 6 shows implementation elim, parameterized two pruning operators, prune1 prune2, corresponding two different pruning locations inside
operator computes LCCSi : ComputeLCCSi (Fi , ani , prune1, prune2).
Algorithm 6: elim(F, i, prune1, prune2)
1
2
3
4
5
6
7
8

Input: set VSFs F, agent
ni set neighboring agents
Fi subset VSF scope
f new (ani ) new VSF
foreach ani Ani
f new (ani ) ComputeLCCS (Fi , ani , prune1, prune2)
end
F F \ Fi {f new }
return F

414

fiComputing CCSs Faster Multi-objective Coordination

ComputeLCCSi implemented follows: first define new cross-sum-and-prune
= prune1(A B). LCCSi applies operator sequentially:
operator AB
[M

f e (ae )).
(4)
ComputeLCCSi (Fi , ani , prune1, prune2) = prune2(
e
ai

f Fi

operator, leading incremental
prune1 applied cross-sum two sets, via
pruning (Cassandra, Littman, & Zhang, 1997). prune2 applied coarser level,
union. CMOVE applies elim iteratively agents remain, resulting CCS.
Note that, agents left, f new line 3 agents condition on.
case, consider actions neighbors single empty action: .
Pruning also applied end, agents eliminated,
call prune3. increasing level coarseness, thus three pruning operators: incremental pruning (prune1), pruning union actions eliminated
agent (prune2), pruning agents eliminated (prune3), reflected
Algorithm 5. agents eliminated, final factor taken set
factors (line 6), single set, contained factor retrieved (line 7). Note
use empty action denote field final factor, agents
scope. Finally prune3 called S.
Consider example Figure 2a, using payoffs defined Table 2, apply
CMOVE. First, CMOVE creates VSFs f 1 f 2 u1 u2 . eliminate agent 3,
creates new VSF f 3 (a2 ) computing LCCSs every a2 tagging element
set action agent 3 generates it. a2 , CMOVE first generates
set {(3, 1)a3 , (1, 3)a3 }. Since vectors optimal w, neither
removed pruning thus f 3 (a2 ) = {(3, 1)a3 , (1, 3)a3 }. a2 , CMOVE first generates
{(0, 0)a3 , (1, 1)a3 }. CPrune determines (0, 0)a3 dominated consequently removes
it, yielding f 3 (a2 ) = {(1, 1)a3 }. CMOVE adds f 3 graph removes f 2
agent 3, yielding factor graph shown Figure 2b.
CMOVE eliminates agent 2 combining f 1 f 3 create f 4 . f 4 (a1 ),
CMOVE must calculate LCCS of:
(f 1 (a1 , a2 ) f 3 (a2 )) (f 1 (a1 , a2 ) f 3 (a2 )).
first cross sum yields {(7, 2)a2 a3 , (5, 4)a2 a3 } second yields {(1, 1)a2 a3 }. Pruning
union yields f 4 (a1 ) = {(7, 2)a2 a3 , (5, 4)a2 a3 }. Similarly, a1 taking union yields
{(4, 3)a2 a3 , (2, 5)a2 a3 , (4, 7)a2 a3 }, LCCS f 4 (a1 ) = {(4, 7)a2 a3 }. Adding f 4
results graph Figure 2c.
Finally, CMOVE eliminates agent 1. Since neighboring agents left, Ai
contains empty action. CMOVE takes union f 4 (a1 ) f 4 (a1 ). Since
(7, 2){a1 a2 a3 } (4, 7){a1 a2 a3 } dominate (5, 4){a1 a2 a3 } , latter pruned, leaving CCS =
{(7, 2){a1 a2 a3 } , (4, 7){a1 a2 a3 } }.
4.3 CMOVE Variants
several ways implement pruning operators lead correct instantiations CMOVE. PPrune (Algorithm 2) CPrune (Algorithm 1) used,
long either prune2 prune3 CPrune. Note prune2 computes CCS, prune3
necessary.
415

fiRoijers, Whiteson, & Oliehoek

article, consider Basic CMOVE, use prune1 prune3
prunes prune2 using CPrune, well Incremental CMOVE, uses CPrune
prune1 prune2. latter invests effort intermediate pruning,
result smaller cross-sums, resulting speedup. However, vectors
pruned intermediate steps, additional speedup may occur,
algorithm creates unnecessary overhead.7 empirically investigate variants
Section 4.5
One could also consider using pruning operators contain prior knowledge
range possible weight vectors. information available, could easily
incorporated changing pruning operators accordingly, leading even smaller LCCSs,
thus faster algorithm. article however, focus case
prior knowledge available.
4.4 Analysis
analyze correctness complexity CMOVE.
Theorem 5. MOVE correctly computes CCS.
Proof. proof works induction number agents. base case original
MO-CoG, f e (ae ) F singleton set. Then, since elim preserves CCS
(see Theorem 1), necessary vectors lost. last agent eliminated,
one factor remains; since conditioned agent actions result
LCCS computation, must contain one set: CCS.
Theorem 6. computational complexity CMOVE
O( n |Amax |wa (wf R1 + R2 ) + R3 ),

(5)

wa induced agent width, i.e., maximum number neighboring agents (connected via factors) agent eliminated, wf induced factor width, i.e.,
maximum number neighboring factors agent eliminated, R1 , R2 R3
cost applying prune1, prune2 prune3 operators.
Proof. CMOVE eliminates n agents one computes LCCS joint
action eliminated agents neighbors, field new VSF. CMOVE computes
O(|Amax |wa ) fields per iteration, calling prune1 (Equation 4) adjacent factor,
prune2 taking union actions eliminated agent. prune3 called
exactly once, eliminating agents (line 8 Algorithm 5).
Unlike non-graphical approach, CMOVE exponential wa , number
agents. respect, results similar PMOVE (Rollon, 2008).
However, earlier complexity results make effect pruning explicit. Instead,
complexity bound makes use additional problem constraints, limit total
number possible different value vectors. Specifically, analysis PMOVE,
payoff vectors integer-valued, maximum value objectives. practice,
7. also compute PCS first, using prune1 prune2, compute CCS prune3.
However, useful small problems PCS cheaper compute CCS.

416

fiComputing CCSs Faster Multi-objective Coordination

bounds loose even impossible define (e.g., payoff values
real-valued one objectives). Therefore, instead give description
computational complexity makes explicit dependence effectiveness
pruning. Even though complexity bounds better worst case (i.e.,
pruning possible), allow greater insight runtimes algorithms
evaluate, apparent analysis experimental results Section 4.5.
Theorem 6 demonstrates complexity CMOVE depends heavily runtime
pruning operators, turn depends sizes input sets. input
set prune2 union returned series applications prune1,
prune3 uses output last application prune2. therefore need balance
effort lower-level pruning higher-level pruning, occurs less
often dependent output lower level. bigger LCCSs,
gained lower-level pruning.
Theorem 7. space complexity CMOVE
O( n |Amax |wa |LCCSmax | + |Amax ||emax | ),
|LCCSmax | maximum size local CCS, original number VSFs,
|emax | maximum scope size original VSFs.
Proof. CMOVE computes local CCS new VSF joint action eliminated agents neighbors. maximally wa neighbors. maximally n new
factors. payoff vector stores real numbers.
VSFs created initialization CMOVE. VSFs
exactly one payoff vector containing real numbers, per joint action agents scope.
maximally |Amax ||emax | joint actions.
PMOVE, space complexity |P CCSmax | instead |LCCSmax |.
LCCS subset corresponding LPCS, CMOVE thus strictly
memory efficient PMOVE.
Note Theorem 7 rather loose upper bound space complexity,
VSFs, original new, exist time. However, possible predict
priori many VSFs exist time, resulting space complexity
bound basis VSFs exist point execution CMOVE.
4.5 Empirical Evaluation
test efficiency CMOVE, compare runtimes PMOVE8
non-graphical approach problems varying numbers agents objectives.
also analyze runtimes correspond sizes PCS CCS.
use two types experiments. first experiments done random MOCoGs directly control variables. second experiment, use
Mining Day, realistic benchmark, structured random MO-CoGs
still randomized.
8. compare PMOVE using prune2 = PPrune, rather prune1 = prune2 = PPrune,
proposed original article (Rollon & Larrosa, 2006) found former option slightly
consistently faster.

417

fiRoijers, Whiteson, & Oliehoek

(a)

(b)

(c)

Figure 4: (a) Runtimes (ms) log-scale nongraphical method, PMOVE CMOVE
standard deviation mean (error bars), (b) corresponding number vectors
PCS CCS, (c) corresponding spread induced width.

4.5.1 Random Graphs
generate random MO-CoGs, employ procedure takes input: n, number
agents; d, number payoff dimensions; number local payoff functions;
|Ai |, action space size agents, agents. procedure
starts fully connected graph local payoff functions connecting two agents
each. Then, local payoff functions randomly removed, ensuring graph
remains connected, local payoff functions remain. values different
objectives local payoff function real numbers drawn independently
uniformly interval [0, 10]. compare algorithms set randomly
generated MO-CoGs separate value n, d, , |Ai |.
compare basic CMOVE, incremental CMOVE, PMOVE, non-graphical
method, test random MO-CoGs number agents ranging
10 85, average number factors per agent held = 1.5n, number
objectives = 2. experiment run 2.4 GHz Intel Core i5 computer, 4 GB
memory. Figure 4 shows results, averaged 20 MO-CoGs number agents.
runtime (Figure 4a) non-graphical method quickly explodes. CMOVE
variants slower PMOVE small numbers agents, runtime grows much
slowly PMOVE. 70 agents, CMOVE variants faster
PMOVE average. 75 agents, one MO-CoGs generated caused PMOVE
time 5000s, basic CMOVE maximum runtime 132s, incremental
CMOVE 136s. explained differences size solutions, i.e.,
PCS CCS (Figure 4b). PCS grows much quickly number
agents CCS does. two-objective problems, incremental CMOVE seems
consistently slower basic CMOVE.
CMOVEs runtime grows much slowly nongraphical method,
still exponential number agents, counterintuitive result since worst-case
complexity linear number agents. explained induced width
MO-CoGs, runtime CMOVE exponential. Figure 4c, see
induced width increases linearly number agents random graphs.
418

fiComputing CCSs Faster Multi-objective Coordination

Figure 5: Runtimes (ms) non-graphical method, PMOVE CMOVE log-scale
standard deviation mean (error bars) (left) corresponding number vectors
PCS CCS (right), increasing numbers agents 5 objectives.

therefore conclude that, two-objective MO-CoGs, non-graphical method
intractable, even small numbers agents, runtime CMOVE increases
much less number agents PMOVE does.
test runtime behavior changes higher number objectives, run
experiment average number factors per agent held = 1.5n
increasing numbers agents again, = 5. remaining experiments
described section executed Xeon L5520 2.26 GHz computer 24 GB
memory. Figure 5 (left) shows results experiment, averaged 85 MO-CoGs
number agents. Note plot induced widths,
change number objectives. results demonstrate that, number
agents grows, using CMOVE becomes key containing computational cost solving
MO-CoG. CMOVE outperforms nongraphical method 12 agents onwards.
25 agents, basic CMOVE 38 times faster. CMOVE also significantly better
PMOVE. Though one order magnitude slower 10 agents (238ms (basic)
416ms (incremental) versus 33ms average), runtime grows much slowly
PMOVE. 20 agents, CMOVE variants faster PMOVE
28 agents, Basic CMOVE almost one order magnitude faster (228s versus 1, 650s
average), difference increases every agent.
before, runtime CMOVE exponential induced width, increases
number agents, 3.1 n = 10 6.0 n = 30 average, result
random MO-CoG generation procedure. However, CMOVEs runtime polynomial
size CCS, size grows exponentially, shown Figure 5 (right).
fact CMOVE much faster PMOVE explained sizes PCS
CCS, former grows much faster latter. 10 agents, average PCS
size 230 average CCS size 65. 30 agents, average PCS size risen
51, 745 average CCS size 1, 575.
Figure 6 (left) compares scalability algorithms number objectives,
random MO-CoGs n = 20 = 30, averaged 100 MO-CoGs. CMOVE
always outperforms nongraphical method. Interestingly, nongraphical method
419

fiRoijers, Whiteson, & Oliehoek

Figure 6: Runtimes (ms) non-graphical method, PMOVE CMOVE logscale
standard deviation mean (error bars) (left) corresponding number vectors
PCS CCS (right), increasing numbers objectives.

several orders magnitude slower = 2, grows slowly = 5, starts
grow exponent PMOVE. explained fact
time takes enumerate joint actions payoffs remains approximately constant,
time takes prune increases exponentially number objectives.
= 2, CMOVE order magnitude slower PMOVE (163ms (basic)
377 (incremental) versus 30ms). However, = 5, CMOVE variants already
faster PMOVE 8 dimensions respectively 3.2 2.4 times faster.
happens CCS grows much slowly PCS, shown Figure
6 (right). difference incremental basic CMOVE decreases number
dimensions increases, factor 2.3 = 2 1.3 = 8. trend indicates
pruning every cross-sum, i.e., prune1, becomes (relatively) better higher
numbers objectives. Although unable solve problem instances many
objectives within reasonable time, expect trend continue incremental
CMOVE would faster basic CMOVE problems many objectives.
Overall, conclude that, random graphs, CMOVE key solving MO-CoGs
within reasonable time, especially problem size increases either number
agents, number objectives, both.
4.5.2 Mining Day
Mining Day, mining company mines gold silver (objectives) set mines
(local payoff functions) located mountains (see Figure 1). mine workers live
villages foot mountains. company one van village (agents)
transporting workers must determine every morning mine van
go (actions). However, vans travel nearby mines (graph connectivity). Workers
efficient workers mine: 3% efficiency bonus per
worker amount resource mined per worker x 1.03w , x
base rate per worker w number workers mine. base rate
gold silver properties mine. Since company aims maximize revenue,
best strategy depends fluctuating prices gold silver. maximize revenue,
420

fiComputing CCSs Faster Multi-objective Coordination

Figure 7: Runtimes (ms) basic incremental CMOVE, PMOVE, log-scale
standard deviation mean (error bars) (left) corresponding number vectors
PCS CCS (right), increasing numbers agents.

mining company wants use latest possible price information, lose time
recomputing optimal strategy every price change. Therefore, must calculate
CCS.
generate Mining Day instance v villages (agents), randomly assign 2-5
workers village connect 2-4 mines. village connected mines
greater equal index, i.e., village connected mines, connected
mines + 1. last village connected 4 mines thus number mines
v + 3. base rates per worker resource mine drawn uniformly
independently interval [0, 10].
order compare runtimes basic incremental CMOVE PMOVE
realistic benchmark, generate Mining Day instances varying numbers
agents. Note include non-graphical method, runtime mainly
depends number agents, thus considerably faster problem
random graphs. runtime results shown Figure 7 (left). CMOVE
PMOVE able tackle problems 100 agents. However, runtime
PMOVE grows much quickly CMOVE. two-objective setting,
basic CMOVE better incremental CMOVE. Basic CMOVE PMOVE
runtimes around 2.8s 60 agents, 100 agents, basic CMOVE runs 5.9s
PMOVE 21s. Even though incremental CMOVE worse basic CMOVE,
runtime still grows much slowly PMOVE, beats PMOVE
many agents.
difference PMOVE CMOVE results relationship
number agents sizes CCS, grows linearly, PCS, grows
polynomially, shown Figure 7 (right). induced width remains around 4 regardless
number agents. results demonstrate that, CCS grows slowly
PCS number agents, CMOVE solve MO-CoGs efficiently
PMOVE number agents increases.
421

fiRoijers, Whiteson, & Oliehoek

5. Linear Support MO-CoGs
section, present variable elimination linear support (VELS). VELS new
method computing CCS MO-CoGs several advantages CMOVE:
moderate numbers objectives, runtime complexity better; anytime
algorithm, i.e., time, VELS produces intermediate results become better
better approximations CCS therefore, provided maximum scalarized
error , VELS compute -optimal CCS.
Rather dealing multiple objectives inner loop (like CMOVE), VELS
deals outer loop employs subroutine. VELS thus builds
CCS incrementally. iteration outer loop, VELS adds one new
vector partial CCS. find vector, VELS selects single w (the one offers
maximal possible improvement), passes w inner loop. inner loop,
VELS uses (Section 2.2) solve single-objective coordination graph (CoG)
results scalarizing MO-CoG using w selected outer loop. joint
action optimal CoG multi-objective payoff added
partial CCS.
departure point creating VELS Chengs linear support (Cheng, 1988). Chengs
linear support originally designed pruning algorithm POMDPs. Unfortunately,
algorithm rarely used POMDPs practice, runtime exponential
number states. However, number states POMDP corresponds number
objectives MO-CoG, realistic POMDPs typically many states, many
MO-CoGs handful objectives. Therefore, MO-CoGs, scalability
number agents important, making Chengs linear support attractive starting
point developing efficient MO-CoG solution method.
Building Chengs linear support, Section 5.1 create abstract algorithm
call optimistic linear support (OLS), builds CCS incrementally.
OLS takes arbitrary single-objective problem solver input, seen generic
multi-objective method. show OLS chooses w iteration that,
finite number iterations, improvements partial CCS made
OLS terminate. Furthermore, bound maximum scalarized error
intermediate results, used bounded approximations CCS.
Then, Section 5.2, instantiate OLS using single-objective problem solver,
yielding VELS, effective MO-CoG algorithm.
5.1 Optimistic Linear Support
OLS constructs CCS incrementally, adding vectors initially empty partial CCS :
Definition 13. partial CCS, S, subset CCS, turn subset V:
CCS V.
define scalarized value function S, corresponding convex upper surface
(shown bold) Figure 8b-d:
Definition 14. scalarized value function partial CCS, S, function takes
weight vector w input, returns maximal attainable scalarized value
422

fiComputing CCSs Faster Multi-objective Coordination

(a)

(b)

(c)

(d)

Figure 8: (a) possible payoff vectors 2-objective MO-CoG. (b) OLS finds two payoff
vectors extrema (red vertical lines), new corner weight wc = (0.5, 0.5)
found, maximal possible improvement . CCS shown dotted line.
(c) OLS finds new vector (0.5, 0.5), adds two new corner weights Q.
(d) OLS calls SolveCoG corner weights (in two iterations), finds
new vectors, ensuring = CCS = CCS.
payoff vector S:
uS (w) = max w u(a).
u(a)S

Similarly, define set maximizing joint actions:
Definition 15. optimal joint action set function respect function
gives joint actions maximize scalarized value:
(w) = arg max w u(a).
u(a)S

Note (w) set w multiple joint actions provide
scalarized value.
Using definitions, describe optimistic linear support (OLS). OLS adds
vectors partial CCS, S, finding new vectors so-called corner weights. corner
weights weights uS (w) (Definition 14) changes slope directions.
must thus weights (w) (Definition 15) consists multiple payoff vectors. Every
corner weight prioritized maximal possible improvement finding new payoff
vector corner weight. maximal possible improvement 0, OLS knows
partial CCS complete. example process given Figure 8,
(corner) weights algorithm searched new payoff vectors indicated
red vertical lines.
OLS shown Algorithm 7. find optimal payoff corner weight, OLS
assumes access function called SolveCoG computes best payoff vector
given w. now, leave implementation SolveCoG abstract. Section 5.2,
discuss implement SolveCoG. OLS also takes input m, MO-CoG solved,
, maximal tolerable error result.
first describe OLS initialized (Section 5.1.1). Then, define corner weights
formally describe OLS identifies (Section 5.1.2). Finally, describe
423

fiRoijers, Whiteson, & Oliehoek

Algorithm 7: OLS(m, SolveCoG, )
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

Input:
CoGGCCS,
agent eliminate.

//partial
W //set checked weights
Q empty priority queue
foreach extremum weight simplex
Q.add(we , ) // add extrema infinite priority
end
Q.isEmpty() timeOut
w Q.pop()
u SolveCoG(m, w)
u 6
Wdel remove corner weights made obsolete u Q, store
Wdel {w} Wdel //corner weights removed adding u
Wu newCornerWeights(u, Wdel , S)
{u}
foreach w Wu
r (w) calculate improvement using maxValueLP(w, S, W)
r (w) >
Q.add(w, r (w))
end
end
end
W W {w}
end
return highest r (w) left Q

OLS prioritizes corner weights also used bound error
stopping OLS done finding full CCS (Section 5.1.3).
5.1.1 Initialization
OLS starts initializing partial CCS, S, contain payoff vectors
CCS discovered far (line 1 Algorithm 7), well set visited weights W (line
2). Then, adds extrema weight simplex, i.e., points
weight one objective, priority queue Q, infinite priority (line 5).
extrema popped priority queue OLS enters main loop (line
7), w highest priority selected (line 8). SolveCoG called
w (line 9) find u, best payoff vector w.
example, Figure 8b shows two payoff vectors 2-dimensional MOCoG found applying SolveCoG extrema weight simplex: =
{(1, 8), (7, 2)}. vectors must part CCS optimal
least one w: one SolveCoG returned solution (the extrema
weight simplex). set weights W OLS tested far marked vertical
red line segments.
424

fiComputing CCSs Faster Multi-objective Coordination

5.1.2 Corner Weights
evaluated extrema, consists (the number objectives) payoff vectors
associated joint actions. However, many weights simplex, yet
contain optimal payoff vector. Therefore, identifying new vector u add
(line 9), OLS must determine new weights add Q. Like Chengs linear support,
OLS identifying corner weights: weights corners convex
upper surface, i.e., points PWLC surface uS (w) changes slope. define
corner weights precisely, must first define P , polyhedral subspace weight
simplex uS (w) (Bertsimas & Tsitsiklis, 1997). corner weights
vertices P, defined set linear inequalities:
Definition 16. set known payoff vectors, define polyhedron
X
P = {x <d+1 : + x ~0, i, wi > 0,
wi = 1},


+ matrix elements row vectors, augmented column vector
1s. setPof linear inequalities + x ~0, supplemented simplex constraints:
wi > 0 wi = 1. vector x = (w1 , ..., wd , u) consists weight vector
scalarized value weights. corner weights weights contained
vertices P , also form (w1 , ..., wd , u).
Note that, due simplex constraints, P d-dimensional. Furthermore,
extrema weight simplex special cases corner weights.
identifying u, OLS identifies corner weights change polyhedron P
adding u S. Fortunately, require recomputation corner weights,
done incrementally: first, corner weights Q u yields better
value currently known deleted queue (line 11) function
newCornerWeights(u, Wdel , S) line 13 calculates new corner weights involve u
solving system linear equations see u intersects boundaries
relevant subset present vectors S.
newCornerWeights(u, Wdel , S) (line 13) first calculates set relevant payoff
vectors, Arel , taking union maximizing vectors weights Wdel 9 :
Arel =

[

(w).

wWdel

(w) contains fewer payoff vectors, boundary weight simplex
involved. boundaries also stored. possible subsets size 1 (of vectors
boundaries) taken. subset weight 1 payoff vectors
(and/or boundaries) intersect u computed solving system
linear equations. intersection weights subsets together form set candidate
corner weights: Wcan . newCornerWeights(u, Wdel , S) returns subset Wcan
inside weight simplex u higher scalarized value payoff
9. fact, implementation, optimize step caching (w) w Q.

425

fiRoijers, Whiteson, & Oliehoek

vector already S. Figure 8b shows one new corner weight labelled wc = (0.5, 0.5).
practice, |Arel | small, systems linear equations need solved.10
calculating new corner weights Wu line 13, u added line 14.
Cheng showed finding best payoff vector corner weight adding
partial CCS, i.e., {SolveCoG(w)}, guarantees best improvement S:
Theorem 8. (Cheng 1988) maximum value of:
max

min w u w v,

w,uCCS vS

i.e., maximal improvement adding vector it, one corner weights
(Cheng, 1988).
Theorem 8 guarantees correctness OLS: corner weights checked,
new payoff vectors; thus maximal improvement must 0 OLS found
full CCS.
5.1.3 Prioritization
Chengs linear support assumes corner weights checked inexpensively,
reasonable assumption POMDP setting. However, since SolveCoG expensive
operation, testing corner weights may feasible MO-CoGs. Therefore, unlike
Chengs linear support, OLS pops one w Q tested per iteration. Making
OLS efficient thus critically depends giving w suitable priority adding
Q. end, OLS prioritizes corner weight w according maximal possible
improvement, upper bound improvement uS (w). upper bound computed
respect CCS, optimistic hypothetical CCS, i.e., best-case scenario
final CCS given current partial CCS W set weights already
tested SolveCoG. key advantage OLS Chengs linear support
priorities computed without calling SolveCoG, obviating need run SolveCoG
corner weights.
Definition 17. optimistic hypothetical CCS, CCS set payoff vectors yields
highest possible scalarized value possible w consistent finding vectors
weights W.
Figure 8b denotes CCS = {(1, 8), (7, 2), (7, 8)} dotted line. Note CCS
superset value uCCS (w) uS (w) weights W.

given w, maxValueLP finds scalarized value uCCS
(w) solving:
max w v
subject W v uS,W ,
10. However, theory possible construct partial CCS, corner weight
payoff vectors Adel .

426

fiComputing CCSs Faster Multi-objective Coordination

uS,W vector containing uS (w0 ) w0 W. Note abuse notation
W, case matrix whose rows consist weight vectors set
W.11
Using CCS, define maximal possible improvement:
(w) = uCCS (w) uS (w).
Figure 8b shows (wc ) dashed line. use maximal relative possible improvement, r (w) = (w)/uCCS (w), priority new corner weight w Wu .

Figure 8b, r (wc )= (0.5,0.5)((7,8)(1,8))
= 0.4. corner weight w identified (line 13),
7.5
added Q priority r (w) long r (w) > (lines 16-18).
wc Figure 8b added Q, popped (as element
Q). SolveCoG(wc ) generates new vector (5, 6), yielding = {(1, 8), (7, 2), (5, 6)},
illustrated Figure 8c. new corner weights (0.667, 0.333) (0.333, 0.667)
points (5, 6) intersects (7, 2) (1, 8). Testing weights, illustrated
Figure 8d, result new payoff vectors, causing OLS terminate. maximal
improvement corner weights 0 thus, due Theorem 8, = CCS upon
termination. OLS called solveCoG 5 weights resulting exactly 3 payoff
vectors CCS. 7 payoff vectors V (displayed grey dashed black
lines Figure 8a) never generated.
5.2 Variable Elimination Linear Support
exact CoG algorithm used implement SolveCoG. naive approach
explicitly compute values joint actions V select joint action maximizes
value:
SolveCoG(m, w) = arg max w u(a).
u(a)V

implementation SolveCoG combination OLS yields algorithm
refer non-graphical linear support (NGLS), ignores graphical structure,
flattening CoG standard multi-objective cooperative normal form game.
main downside computational complexity SolveCoG linear |V| (which
equal |A|), exponential number agents, making feasible
MO-CoGs agents.
contrast, use (Section 2.2) implement SolveCoG, better.
call resulting algorithm variable elimination linear support (VELS). dealt
multiple objectives outer loop OLS, VELS relies exploit graphical
structure inner loop, yielding much efficient method NGLS.
5.3 Analysis
analyze computational complexity VELS.
11. implementation OLS reduces size LP using subset weights W
joint actions involved w, (w), found optimal. lead slight

overestimation uCCS
(w).

427

fiRoijers, Whiteson, & Oliehoek

Theorem 9. runtime VELS = 0
O((|CCS| + |WCCS |)(n|Amax |w + Cnw + Cheur )),
w induced width running VE, |CCS| size CCS, |WCCS |
number corner weights uCCS (w), Cnw time costs run newCornerWeights,
Cheur cost computation value optimistic CCS using maxValueLP.
Proof. Since n|Amax |w runtime (Theorem 1), runtime VELS
quantity (plus overhead per corner weight Cnw + Cheur ) multiplied number
calls VE. count calls, consider two cases: calls result adding
new vector result new vector instead confirm
optimality scalarized value weight. former size final CCS,
|CCS|, latter number corner weights final CCS, |WCCS |.
overhead OLS itself, i.e., computing new corner weights, Cnw , calculating
maximal relative improvement, Cheur , small compared SolveCoG calls.
practice, newCornerWeights(u, Wdel , S) computes solutions small set
linear equations (of equations each). maxValueLP(w, S, W) computes solutions
linear programs, polynomial size inputs.12
= 2, number corner weights smaller |CCS| runtime
VELS thus O(n|Amax |w |CCS|). = 3, number corner weights twice |CCS|
(minus constant) because, SolveCoG finds new payoff vector, one corner weight
removed three new corner weights added. > 3, loose bound |WCCS |

total number possible combinations payoff vectors boundaries: O( |CCS|+d
).

However, obtain tighter bound observing counting number corner
weights given CCS equivalent vertex enumeration, dual problem
facet enumeration, i.e., counting number vertices given corner weights (Kaibel &
Pfetsch, 2003).
Theorem 10. arbitrary d, |WCCS | bounded O(
(Avis & Devroye, 2000).

|CCS|b d+1
c
2
|CCS|d

+

|CCS|b d+2
c
2
)
|CCS|d

Proof. result follows directly McMullens upper bound theorem facet enumeration (Henk, Richter-Gebert, & Ziegler, 1997; McMullen, 1970).
reasoning used prove Theorems 9 also used establish following:
Corollary 1. runtime VELS 0
O((|-CCS| + |WCCS |)(n|Amax |w + Cnw + Cheur ), |-CCS| size -CCS,
|WCCS | number corner weights uCCS (w).
practice, VELS often test corner weights polyhedron spanned
-CCS, cannot guaranteed general. Section 5.4, show empirically
|-CCS| decreases rapidly increases.
12. reduction Footnote 11 used, small subset W used, making even smaller.

428

fiComputing CCSs Faster Multi-objective Coordination

Figure 9: (left) runtimes PMOVE, CMOVE VELS different values ,
varying numbers agents, n, = 1.5n factors, 2 actions per agent,
2 objectives (right) corresponding sizes -CCSs.
Theorem 11. space complexity VELS O(d|-CCS|+d|WCCS |+n|Amax |w )
0.
Proof. OLS needs store every corner weight (a vector length d) queue,
|WCCS |. OLS also needs store every vector (also vectors length d).
Furthermore, SolveCoG called, memory usage added memory
usage outer loop OLS. memory usage n|Amax |w (Theorem 2).
OLS adds memory requirements VE, VELS almost memory
efficient thus considerably memory efficient CMOVE (Theorem 7).
5.4 Empirical Evaluation
empirically evaluate VELS, comparison CMOVE PMOVE. longer
compare non-graphical method clearly dominated CMOVE
PMOVE. refer CMOVE section, mean basic CMOVE,
fastest tested scenarios. Like before, use random graphs Mining Day
benchmark. experiments section run 2.4 GHx Intel Core i5 computer,
4 GB memory.
5.4.1 Random Graphs
test VELS randomly generated MO-CoGs, use MO-CoG generation
procedure Section 4. determine scalability exact approximate
VELS compares PMOVE CMOVE, tested random MO-CoGs
increasing numbers agents. average number factors per agent held
= 1.5n number objectives = 2. Figure 9 shows results,
averaged 30 MO-CoGs number agents. Note runtimes left,
y-axis, log-scale set sizes right not.
results demonstrate VELS efficient CMOVE two-objective
random MO-CoGs. runtime exact VELS ( = 0) average 16 times less
429

fiRoijers, Whiteson, & Oliehoek

CMOVE. CMOVE solves random MO-CoGs 85 agents 74s average, whilst
exact VELS handle 110 agents 71s.
already large gain, achieve even lower growth rate permitting
small . 110 agents, permitting 0.001 error margin yields gain
order magnitude, reducing runtime 5.7s. Permitting 0.01 error reduces
runtime 1.3s. thus reduce runtime VELS factor 57,
retaining 99% accuracy. Compared CMOVE 85 agents, VELS = 0.01 109
times faster.
speedups explained slower growth -CCS (Figure 9 (right)).
small numbers agents, size -CCS grows slightly slowly
size full CCS. However, certain number agents onwards, size
-CCS grows marginally size full CCS keeps growing. = 0.01,
-CCS grew 2.95 payoff vectors 5.45 payoff vectors 5 20 agents,
marginally 5.50 110 agents. contrast, full CCS grew 3.00
9.90 vectors 5 20 agents, keeps growing 44.50 110 agents.
similar picture holds 0.001-CCS, grows rapidly 3.00 vectors 5
14.75 vectors 50 agents, grows slowly 16.00 90 agents, stabilizes,
reach 16.30 vectors 120 agents. 90 120 agents, full CCS grows
35.07 vectors 45.40 vectors, making almost 3 times large 0.001-CCS 9
times larger 0.01-CCS .
test scalability VELS respect number objectives, tested
random MO-CoGs constant number agents factors n = 25 = 1.5n,
increased number objectives, = 0 = 0.1. compare
scalability CMOVE. kept number agents (n = 25) number local
payoff functions ( = 37) small order test limits scalability number
objectives. number actions per agent 2. Figure 10 (left) plots number
objectives runtime (in log scale). CCS grows exponentially
number objectives (Figure 10 (right)), runtime CMOVE also exponential
number objectives. VELS however linear number corner weights,
exponential size CCS, making VELS doubly exponential. Exact VELS ( = 0)
faster CMOVE = 2 = 3, = 4 approximate VELS = 0.1
20 times faster. However = 5 even approximate VELS = 0.1
slower CMOVE.
Unlike number agents grows, size -CCS (Figure 10 (right))
stabilize number objectives grows, seen following table:
|CCS|
d=2
d=3
d=4

=0
10.6
68.8
295.1

= 0.001
7.3
64.6
286.1

= 0.01
5.6
41.0
242.6

= 0.1
3.0
34.8
221.7

therefore conclude VELS compute CCS faster CMOVE 3 objectives
less, CMOVE scales better number objectives. VELS however, scales
better number agents.
430

fiComputing CCSs Faster Multi-objective Coordination

Figure 10: (left) runtimes CMOVE VELS ( = 0 = 0.1), varying numbers objectives (right) size -CCS varying numbers objectives.

Figure 11: (left) plot runtimes CMOVE VELS different values ,
varying n (up 500). (right) loglogplot runtime VELS 250, 500,
1000 agent mining day instances, varying values .
5.4.2 Mining Day
compare CMOVE VELS Mining Day benchmark using generation procedure Section 4.5.2. generated 30 Mining Day instances increasing n
averaged runtimes (Figure 11 (left)). 160 agents, CMOVE reached runtime
22s. Exact VELS ( = 0) compute complete CCS MO-CoG 420 agents
time. indicates VELS greatly outperforms CMOVE structured
2-objective MO-CoG. Moreover, allow 0.1% error ( = 0.001), takes
1.1s compute -CCS 420 agents, speedup order magnitude.
measure additional speedups obtainable increasing , test VELS
large problems, generated Mining Day instances n {250, 500, 1000}.
averaged 25 instances per value . instances, exact VELS runs 4.2s
n = 250, 30s n = 500 218s n = 1000 average. expected, increasing
leads greater speedups (Figure 11 (right)). However, close 0, i.e.,
431

fiRoijers, Whiteson, & Oliehoek

-CCS close full CCS, speedup small. increased beyond certain
value (dependent n), decline becomes steady, shown line log-log plot.
increases factor 10, runtime decreases factor 1.6.
Thus, results show VELS compute exact CCS unprecedented
numbers agents (1000) well-structured problems. addition, show small
values enable large speedups, increasing leads even bigger improvements
scalability.

6. Memory-Efficient Methods
CMOVE VELS designed minimize runtime required compute CCS.
However, cases, bottleneck may memory instead. Memory-efficient methods
CoGs related problems recently received considerable attention (Dechter &
Mateescu, 2007; Marinescu, 2008, 2009; Mateescu & Dechter, 2005). section,
show that, outer loop method, VELS naturally memory efficient
therefore solve much larger MO-CoGs inner loop method like CMOVE
memory restricted. addition, show CMOVE VELS modified
produce even memory-efficient variants.
6.1 And/Or Tree Search
begin background AND/OR tree search (Dechter & Mateescu, 2007;
Marinescu, 2008; Mateescu & Dechter, 2005; Yeoh, Felner, & Koenig, 2010), class
algorithms solving single-objective CoGs tuned provide better space
complexity guarantees VE. However, improvement space complexity comes
price, i.e., runtime complexity worse (Mateescu & Dechter, 2005). background
provide brief; broader overview AND/OR tree search CoGs related
models please see work Dechter (2013) Marinescu (2008), multi-objective
versions work Marinescu (2009, 2011).
AND/OR tree search algorithms work converting graph pseudo tree (PT)
agent need know actions ancestors descendants PT
take order select action. example, agent (a node) PT two
subtrees (T1 T2 ) it, agents T1 conditionally independent
agents T2 given ancestors i. Figure 12a shows PT coordination
graph Figure 2a.
Next, AND/OR tree search algorithms perform tree search results AND/OR
search tree (AOST). agent AOST OR-node. children AND-nodes,
corresponding one agent actions. turn, children AND-nodes
OR-nodes corresponding agent children PT. action (AND-nodes)
agent agents OR-nodes, agents actions appear
tree multiple times. Figure 12b shows AOST graph Figure 2a.
specific joint action constructed traversing tree, starting root
selecting one alternative childen OR-node, i.e., one action agent,
continuing children AND-node. example, Figure 12b, joint
action < a1 , a2 , a3 > indicated grey. retrieve value joint action, must
first define value AND-nodes.
432

fiComputing CCSs Faster Multi-objective Coordination

Figure 12: (a) pseudo tree, (b) corresponding AND/OR search tree.
Definition 18. value AND-node vai , representing action ai agent
sum local payoff functions scope; ai , together AND-node
ancestors actions, specifies action agent scope local payoff functions.
example, Figure 12b, total payoff CoG u(a1 , a2 , a3 ) = u1 (a1 , a2 ) +
u2 (a2 , a3 ). value grey AND-node a3 u2 (a2 , a3 ), u3 payoff function
agent 3 scope and, together ancestral AND-nodes, grey a2 -node, a3
completes joint local action u2 .
retrieve optimal action, must define value subtree AOST:
Definition 19. value subtree v(Ti ) rooted OR-node AOST
maximum value subtrees rooted (AND-node) children i. value
subtree v(Tai ) rooted AND-node ai AOST value ai (Definition
18) plus sum value subtrees rooted (OR-node) children ai .
memory-efficient way retrieve optimal joint action using AOST
Euler-touring it, i.e., performing depth-first search computing values
subtrees. generating nodes fly deleting evaluated, memory
usage minimized. refer algorithm simply AND/OR tree search (TS).
earlier sections, implementation employs tagging scheme, tagging value
subtree actions maximize it.
TS single-objective method, extended compute PCS,
yielding algorithm call Pareto TS (PTS) (Marinescu, 2009). define PTS, must
update Definition 19 set Pareto-optimal payoffs. refer subtree value
set intermediate PCS (IPCS).
Definition 20. intermediate PCS subtree, IP CS(Ti ) rooted OR-node
PCS union intermediate PCSs children, ch(i), i:
IP CS(Ti ) = PPrune(

[

aj ch(i)

433

IP CS(Taj )).

fiRoijers, Whiteson, & Oliehoek

intermediate PCS subtree, IP CS(Tai ) rooted AND-node ai PCS
value ai (Definition 18) plus cross-sum intermediate PCSs subtrees
rooted (OR-node) children ai :



IP CS(Tj ) {vai }).
IP CS(Tai ) = PPrune(
jch(ai )

Thus, PTS replaces max operator TS pruning operator, PMOVE replaces
max operator pruning operator.
6.2 Memory-Efficient CCS Algorithms
propose two memory-efficient algorithms computing CCS. straightforward variants CMOVE VELS.
first algorithm, call Convex TS (CTS), simply replaces PPrune CPrune
Definition 20. Thus, CTS like PTS different pruning operator.
also seen CMOVE replaced TS. advantage CTS PTS
analogous CMOVE PMOVE: highly beneficial compute local
CCSs instead local PCSs intermediate coverage sets input next
subproblem sequential search scheme, regardless whether scheme TS.
CTS memory efficient CMOVE, still requires computing intermediate
coverage sets take space. typically large CCS,
size bounded total number joint actions.
second algorithm addresses problem employing OLS TS singleobjective solver subroutine, SolveCoG, yielding tree search linear support (TSLS). Thus,
TSLS like VELS replaced TS. TSLS outer-loop method,
runs TS sequence, requiring memory used TS overhead
outer loop, consists partial CCS (Definition 13) priority queue.
Consequently, TSLS even memory efficient CTS.
6.3 Analysis
TS much better space complexity VE, i.e., linear number agents n:
Theorem 12. time complexity TS O(n|Amax |m ), n number agents,
|Amax | maximal number actions single agent depth pseudo
tree, uses linear space, O(n).
Proof. number nodes AOST bounded O(n|Amax |m ). tree creates
maximally |Amax | children OR-node. every AND-node exactly one child,
number nodes would bounded O(|Amax |m ), PT deep. However,
branching PT, AND-node multiple children. branch increases
size AOST O(|Amax |m ) nodes. exactly n agents
PT, happen n times. node AOST, TS performs either
summation scalars, maximization scalars. TS performs depth-first
search, O(n) nodes need exist point execution.
434

fiComputing CCSs Faster Multi-objective Coordination

TSs memory usage usually lower required store original (singleobjective) problem memory: O(|Amax |emax ), number local payoff
functions problem, |Amax | maximal size action space single agent,
emax maximal size scope single local payoff function.
PT-depth different constant induced width w, typically
larger. However, bounded w.
Theorem 13. Given MO-CoG induced width w, exists pseudo tree
depth w log n (Dechter & Mateescu, 2007).
Thus, combining Theorems 12 13 shows that, agents, TS
much memory efficient relatively small runtime penalty.
Using time space complexity results TS, establish following
corollaries time space complexity CTS TSLS.
Corollary 2. time complexity CTS O(n|Amax |m R), R runtime
CPrune.
Proof. O(n|Amax |m ) bounds number nodes AOST. node AOST
CPrune called.
runtime CPrune terms size input given Equation 3. Note
size input CPrune depends size intermediate CCSs
children node. case AND-node, input size O(|ICCSmax |c ),
c maximum number children AND-node.13 OR-nodes
O(|Amax ||ICCSmax |).
Corollary 3. space complexity CTS O(n|ICCSmax |), |ICCSmax |
maximum size intermediate CCS execution CTS.
Proof. Like TS, O(n) nodes AOST need exist point
execution, node contains intermediate CCS.
CTS thus much memory efficient CMOVE, space complexity
exponential induced width (Theorem 7).
Corollary 4. time complexity TSLS O((|-CCS|+|W -CCS |) (n |Amax |m +Cnw +
Cheur )), w log n 0.
Proof. proof Theorem 9 time complexity
replaced TS.
terms memory usage, outer loop approach (OLS) large advantage
inner loop approach, overhead outer loop consists partial
CCS (Definition 13) priority queue. VELS (Theorem 11) thus much better
space complexity CMOVE (Theorem 7). TSLS advantage CTS
VELS CMOVE. Therefore, TSLS low memory usage, since requires
memory used TS plus overhead outer loop.
13. Note c turn upper bounded n loose bound.

435

fiRoijers, Whiteson, & Oliehoek

Corollary 5. space complexity TSLS O(d|-CCS| + d|W -CCS | + n)),
w log n 0.
Proof. proof Theorem 11 space complexity
replaced TS.
mentioned Section 6.1, TS memory-efficient member class
AND/OR tree search algorithms. members class offer different trade-offs
time space complexity. possible create inner loop algorithms
corresponding outer loop algorithms basis algorithms. time
space complexity analyses algorithms performed similar manner
Corollaries 25. advantages outer loop methods compared corresponding
inner loop methods however remain TSLS CTS. Therefore,
article focus comparing memory-efficient inner loop method
memory-efficient outer loop method.
6.4 Empirical Evaluation
section, compare CTS TSLS CMOVE VELS. before, use
random graphs Mining Day benchmark. obtain PTs CTS TSLS,
use heuristic CMOVE VELS generate elimination order
transform PT w log n holds (whose existence guaranteed
Theorem 13), using procedure suggested Bayardo Miranker (1995).
6.4.1 Random Graphs
First, test algorithms random graphs, employing generation procedure
Section 4.5.1. connections agents graphs generated
randomly, induced width varies different problems. average, induced
width increases number local payoff functions, even ratio
local payoff factors number agents remains constant.
order test sizes problems different MO-CoG solution methods
handle within limited memory, generate random graphs two objectives, varying
number agents n, = 1.5n local payoff functions, previous sections.
limited maximal available memory 1kB imposed timeout 1800s.
Figure 13a shows VELS scale agents within given memory constraints non-memory efficient methods. particular, PMOVE CMOVE
handle 30 40 agents, respectively, because, given induced width w,
must store O(|Amax |w ) local CSs. 30 agents, induced width (Figure 13c)
6, 40 agents induced width 8. VELS handle 65 agents,
induced width 11, memory demands come running
inner loop, outer loop adds little overhead. need store one payoff
new local payoff function results agent elimination, whereas PMOVE
CMOVE must store local coverage sets. Thus, using outer loop approach (VELS)
instead inner loop approach (CMOVE) already yields significant improvement
problem sizes tackled limited memory.
436

fiComputing CCSs Faster Multi-objective Coordination

(a)

(b)

(c)

Figure 13: (a) Runtimes ms TSLS, VELS, CTS, CMOVE PMOVE random 2objective MO-CoGs varying numbers agents n = 1.5n local payoff
factors. (b) Runtimes approximate TSLS varying amounts allowed error
, compared (Exact) VELS, problem parameters (a). (c)
corresponding induced widths MO-CoGs (b).

However, scaling beyond 65 agents requires memory-efficient approach. Figure 13a
also shows that, CTS TSLS require runtime, handle agents
within memory constraints. fact, unable generate MO-CoG enough
agents cause methods run memory. TSLS faster CTS, case
4.2 times faster, reasons VELS faster CMOVE.
However, speed advantage outer loop approach. allow
bit error scalarized value, , trade accuracy runtime (Figure 13b). 65
agents, exact TSLS ( = 0), average runtime 106s, 51 times slower
VELS. However, = 0.0001, runtime 70s (33 times slower). = 0.01
11s (5.4 times slower), = 0.1 6s (2.9 times slower). Furthermore,
relative increase runtime number agents increases less higher . Thus,
approximate version TSLS highly attractive method cases memory
runtime limited.
6.4.2 Mining Field
compare performance CMOVE VELS TSLS variation Mining
Day call Mining Field. longer consider CLS consistently higher
runtime TSLS worse space complexity. use Mining Field order ensure
interesting problem memory-restricted setting. Mining Day (see Section 4),
induced width depends parameter specifying connectivity villages
increase number agents factors. Therefore, whether
VELS memory-efficient enough handle particular instance depends primarily
parameter number agents.
Mining Field, villages situated along mountain ridge placed
grid. number agents thus n = s2 . use random placement mines,
ensuring graph connected. induced width connected grid
generate grid-like graphs, larger instances higher induced width.
437

fiRoijers, Whiteson, & Oliehoek

village

(a)

mine

(b)

(c)

Figure 14: (a) example 4 4 Mining Field instance. additional mines
marked +. (b) Runtimes ms TSLS (for varying amounts allowed
error ), VELS ( = 0), CMOVE 2-objective Mining Field instances
varying numbers additional mines [2..14] grid size = 7. (c)
corresponding induced widths Mining Field instances.

induced width thus longer depends connectivity parameter also increases
number agents factors graph.
example Mining Field instance provided Figure 14a. choose distance
adjacent villages grid unit length. map, place
mines (local payoff functions). connect agents using arbitrary tree using 2-agent
local payoff functions (mines). figures, mines span tree unmarked
connected mines black edges. require s2 1 factors build tree.
add additional mines, (independently) placing random point
map inside grid. mine placed, connect villages within
r = 12 + radius mine map. chose = 0.2. Therefore, maximum
connectivity factor (mine) created fashion 4. figure, mines
marked +. rewards per mine per worker, well number workers per
village, generated way Mining Day.
compare runtimes memory requirements CMOVE, VELS, TSLS
Mining Field, tested 7 7 instance (49 agents), 1MB available memory.
TSLS, use three different values : 0 (exact), 0.01 0.1. use time limit
1.8 106 (30 minutes). increase number additional mines 2 (50 factors
total) onwards, steps 2.
Using setup, possible solve problem instances using PMOVE,
ran memory problems. fact, PMOVE succeeded tree-shaped
problem. i.e., one without additional factors. Figures 14b 14c) show results
remaining methods. CMOVE runs memory 6 additional factors (54 factors
total). contrast, VELS runs memory 16 additional factors, induced
width 6.
Compared random-graph results Section 6.4.1, induced widths
problems CMOVE VELS handle lower Mining Field. suspect
438

fiComputing CCSs Faster Multi-objective Coordination

because, grid-shaped problem, number factors highest induced
width need exist parallel execution algorithms higher.
TSLS run memory tested instances. face,
unable generate instances TSLS run memory. However,
run time. = 0, TSLS first exceeds time limit = 10 additional mines.
= 0.01, happens = 14. = 0.1, TSLS ran time = 16.
differences runtime TSLS VELS larger random graphs
therefore difficult compensate slower runtime TSLS choosing
higher . much slower TSLS compared VELS thus seems depend
structure MO-CoG.
Mining Field results confirm conclusion random-graph experiments
using outer loop approach (VELS) instead inner loop approach (CMOVE) yields
significant improvement problem sizes tackled limited memory.
Futhermore, TSLS used solve problem sizes beyond VELS handle
within limited memory. approximate version TSLS appealing choice cases
memory runtime limited.

7. Conclusions Future Work
article, proposed new algorithms exploit loose couplings compute CCS
multi-objective coordination graphs. showed exploiting loose couplings
key solving MO-CoGs many agents. particular, showed, theoretically
empirically, computing CCS considerable advantages computing PCS
terms runtime memory usage. experiments consistently shown
runtime PCS methods grows lot faster CCS methods.
CMOVE deals multiple objectives inner loop, i.e., computes local CCSs
looping agents. contrast, VELS deals multiple objectives
outer loop, i.e., identifies weights maximal improvement upon partial CCS
made solves scalarized (single-objective) problems using weights, yielding
anytime approach. addition, CTS TSLS memory-efficient variants
methods. proved correctness algorithms analyzed complexity.
CMOVE VELS complementary methods. CMOVE scales better number
objectives, VELS scales better number agents compute CCS, leading large additional speedups. Furthermore, VELS memory-efficient
CMOVE. fact, VELS uses little memory single-objective VE.
However, memory restricted VELS cannot applied, TSLS provides
memory-efficient alternative. TSLS considerably slower VELS,
loss compensated allowing error ().
numerous possibilities future work. mentioned Section 5, OLS
generic method also applied multi-objective problems. fact, (together
authors) already applied OLS large multi-objective MDPs showed
OLS extended permit non-exact single-objective solvers (Roijers et al., 2014).
future work, intend investigate -approximate methods MO-CoGs, using approximate single-objective solvers CoGs, using, e.g., LP-relaxation methods (Sontag,
Globerson, & Jaakkola, 2011). attempt find optimal balance
439

fiRoijers, Whiteson, & Oliehoek

levels approximation inner outer loop, respect runtime guarantees
empirical runtimes.
Many methods exist single-objective coordination graphs single parameter
controls trade-off memory usage runtime (Furcy & Koenig, 2005; Rollon,
2008). algorithms, corresponding multi-objective inner-loop version
computes PCS (Marinescu, 2009, 2011) devised. would interesting
create inner outer loop methods based methods compute CCS
instead compare performance. particular, shown OLS requires
little extra memory usage compared single-objective solvers. would interesting
investigate much extra memory could used single-objective solver inside OLS,
comparison corresponding inner-loop method.
addition work MO-CoGs, also aim extend work sequential
settings. particular, look developing efficient planning method multiagent multi-objective MDPs better exploiting loosely couplings. First, try
develop -approximate planning version sparse-cooperative Q-learning (Kok & Vlassis,
2006b). However, may possible general effects agent
agents via state impossible bound general. Therefore, hope identify
broadly applicable subclass multi-agent MOMDPs -approximate planning
method yields substantial speed-up compared exact planning methods.

Acknowledgements
thank Rina Dechter introducing us memory-efficient methods CoGs
MO-CoGs, Radu Marinescu tips memory-efficient methods implementation. Also, would like thank Maarten Inja, well anonymous reviewers, valuable feedback. research supported NWO DTC-NCAP
(#612.001.109) NWO CATCH (#640.005.003) projects NWO Innovational Research Incentives Scheme Veni (#639.021.336). Frans Oliehoek affiliated
University Amsterdam University Liverpool.

References
Avis, D., & Devroye, L. (2000). Estimating number vertices polyhedron. Information processing letters, 73 (3), 137143.
Bayardo, R. J. J., & Miranker, D. P. (1995). space-time trade-off solving constraint
satisfaction problems. IJCAI 1995: Proceedings Fourteenth International
Joint Conference Artificial Intelligence.
Bertsimas, D., & Tsitsiklis, J. (1997). Introduction Linear Optimization. Athena Scientific.
Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.
Cassandra, A., Littman, M., & Zhang, N. (1997). Incremental pruning: simple, fast, exact
method partially observable markov decision processes. UAI 1997: Proceedings
Thirteenth Conference Uncertainty Artificial Intelligence, pp. 5461.
440

fiComputing CCSs Faster Multi-objective Coordination

Cheng, H.-T. (1988). Algorithms partially observable Markov decision processes. Ph.D.
thesis, University British Columbia, Vancouver.
Dechter, R. (2013). Reasoning Probabilistic Deterministic Graphical Models: Exact Algorithms, Vol. 7 Synthesis Lectures Artificial Intelligence Machine
Learning. Morgan & Claypool Publishers.
Dechter, R., & Mateescu, R. (2007). And/or search spaces graphical models. Artificial
intelligence, 171 (2), 73106.
Delle Fave, F., Stranders, R., Rogers, A., & Jennings, N. (2011). Bounded decentralised
coordination multiple objectives. Proceedings Tenth International Joint
Conference Autonomous Agents Multiagent Systems, pp. 371378.
Dubus, J., Gonzales, C., & Perny, P. (2009). Choquet optimization using gai networks
multiagent/multicriteria decision-making. ADT 2009: Proceedings First
International Conference Algorithmic Decision Theory, pp. 377389.
Feng, Z., & Zilberstein, S. (2004). Region-based incremental pruning POMDPs. UAI
2004: Proceedings Twentieth Conference Uncertainty Artificial Intelligence, pp. 146153.
Furcy, D., & Koenig, S. (2005). Limited discrepancy beam search. IJCAI 2005: Proceedings Nineteenth International Joint Conference Artificial Intelligence, pp.
125131.
Guestrin, C., Koller, D., & Parr, R. (2002). Multiagent planning factored MDPs.
Advances Neural Information Processing Systems 15 (NIPS02).
Henk, M., Richter-Gebert, J., & Ziegler, G. M. (1997). Basic properties convex polytopes.
Handbook Discrete Computational Geometry, Ch.13, pp. 243270. CRC
Press, Boca.
Kaibel, V., & Pfetsch, M. E. (2003). algorithmic problems polytope theory.
Algebra, Geometry Software Systems, pp. 2347. Springer.
Kok, J. R., & Vlassis, N. (2004). Sparse cooperative Q-learning. Proceedings
twenty-first international conference Machine learning, ICML 04, New York, NY,
USA. ACM.
Kok, J. R., & Vlassis, N. (2006a). Using max-plus algorithm multiagent decision
making coordination graphs. RoboCup 2005: Robot Soccer World Cup IX, pp.
112.
Kok, J., & Vlassis, N. (2006b). Collaborative multiagent reinforcement learning payoff
propagation. Journal Machine Learning Research, 7, 17891828.
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles Techniques. MIT Press.
Lizotte, D., Bowling, M., & Murphy, S. (2010). Efficient reinforcement learning multiple
reward functions randomized clinical trial analysis. Proceedings 27th
International Conference Machine Learning (ICML-10), pp. 695702.
441

fiRoijers, Whiteson, & Oliehoek

Marinescu, R., Razak, A., & Wilson, N. (2012). Multi-objective influence diagrams.
UAI 2012: Proceedings Twenty-Eighth Conference Uncertainty Artificial
Intelligence.
Marinescu, R. (2008). AND/OR Search Strategies Combinatorial Optimization Graphical Models. Ph.D. thesis, University California, Irvine.
Marinescu, R. (2009). Exploiting problem decomposition multi-objective constraint optimization. Principles Practice Constraint Programming-CP 2009, pp. 592
607. Springer.
Marinescu, R. (2011). Efficient approximation algorithms multi-objective constraint
optimization. ADT 2011: Proceedings Second International Conference
Algorithmic Decision Theory, pp. 150164. Springer.
Mateescu, R., & Dechter, R. (2005). relationship AND/OR search variable
elimination. UAI 2005: Proceedings Twenty-First Conference Uncertainty
Artificial Intelligence, pp. 380387.
McMullen, P. (1970). maximum numbers faces convex polytope. Mathematika,
17 (2), 179184.
Oliehoek, F. A., Spaan, M. T. J., Dibangoye, J. S., & Amato, C. (2010). Heuristic search
identical payoff bayesian games. AAMAS 2010: Proceedings Ninth International Joint Conference Autonomous Agents Multiagent Systems, pp.
11151122.
Pearl, J. (1988). Probabilistic reasoning intelligent systems: networks plausible inference. Morgan Kaufmann.
Pham, T. T., Brys, T., Taylor, M. E., Brys, T., Drugan, M. M., Bosman, P. A., Cock,
M.-D., Lazar, C., Demarchi, L., Steenhoff, D., et al. (2013). Learning coordinated
traffic light control. Proceedings Adaptive Learning Agents workshop (at
AAMAS-13), Vol. 10, pp. 11961201.
Roijers, D. M., Scharpff, J., Spaan, M. T. J., Oliehoek, F. A., de Weerdt, M., & Whiteson,
S. (2014). Bounded approximations linear multi-objective planning uncertainty. ICAPS 2014: Proceedings Twenty-Fourth International Conference
Automated Planning Scheduling, pp. 262270.
Roijers, D. M., Vamplew, P., Whiteson, S., & Dazeley, R. (2013a). survey multiobjective sequential decision-making. Journal Artificial Intelligence Research, 47,
67113.
Roijers, D. M., Whiteson, S., & Oliehoek, F. (2013b). Computing convex coverage sets
multi-objective coordination graphs. ADT 2013: Proceedings Third International Conference Algorithmic Decision Theory, pp. 309323.
Roijers, D. M., Whiteson, S., & Oliehoek, F. A. (2014). Linear support multi-objective
coordination graphs. AAMAS 2014: Proceedings Thirteenth International
Joint Conference Autonomous Agents Multi-Agent Systems, pp. 12971304.
Rollon, E. (2008). Multi-Objective Optimization Graphical Models. Ph.D. thesis, Universitat Politecnica de Catalunya, Barcelona.
442

fiComputing CCSs Faster Multi-objective Coordination

Rollon, E., & Larrosa, J. (2006). Bucket elimination multiobjective optimization problems. Journal Heuristics, 12, 307328.
Rosenthal, A. (1977). Nonserial dynamic programming optimal. Proceedings
Ninth Annual ACM Symposium Theory Computing, pp. 98105. ACM.
Scharpff, J., Spaan, M. T. J., Volker, L., & De Weerdt, M. (2013). Planning uncertainty coordinating infrastructural maintenance. Proceedings 8th annual
workshop Multiagent Sequencial Decision Making Certainty.
Sontag, D., Globerson, A., & Jaakkola, T. (2011). Introduction dual decomposition
inference. Optimization Machine Learning, 1, 219254.
Tesauro, G., Das, R., Chan, H., Kephart, J. O., Lefurgy, C., Levine, D. W., & Rawson, F.
(2007). Managing power consumption performance computing systems using
reinforcement learning. Advances Neural Information Processing Systems 20
(NIPS07).
Vamplew, P., Dazeley, R., Barker, E., & Kelarev, A. (2009). Constructing stochastic mixture policies episodic multiobjective reinforcement learning tasks. Advances
Artificial Intelligence, pp. 340349.
Yeoh, W., Felner, A., & Koenig, S. (2010). BnB-ADOPT: asynchronous branch-andbound DCOP algorithm. Journal Artificial Intelligence Research, 38, 85133.

443

fiJournal Artificial Intelligence Research 52 (2015) 179-201

Submitted 5/14; published 1/15

Agnostic Pointwise-Competitive Selective Classification
Yair Wiener
Ran El-Yaniv

wyair@tx.technion.ac.il
rani@cs.technion.ac.il

Computer Science Department
Technion Israel Institute Technology
Haifa 32000, Israel

Abstract
pointwise competitive classifier class F required classify identically
best classifier hindsight F. noisy, agnostic settings present strategy
learning pointwise-competitive classifiers finite training sample provided
classifier abstain prediction certain region choice. interesting hypothesis classes families distributions,
measure

rejected region
/2
shown diminishing rate 1 (polylog(m) log(1/)/m) 2
, high probability, sample size, standard confidence parameter, 1 , 2
smoothness parameters Bernstein type condition associated excess loss class
(related F 0/1 loss). Exact implementation proposed learning strategy
dependent ERM oracle hard compute agnostic case. thus
consider heuristic approximation procedure based SVMs, show empirically
algorithm consistently outperforms traditional rejection mechanism based
distance decision boundary.

1. Introduction
Given labeled training set class models F, possible select F, based
finite training sample, model whose predictions always identical best model
hindsight? classical results statistical learning theory surely preclude
possibility within standard model, changing rules game possible.
Indeed, consider game classifier allowed abstain prediction without
penalty region choice (a.k.a classification reject option). game,
assuming noise free realizable setting, shown El-Yaniv Wiener (2010)
one train perfect classifier never errs whenever willing predict.
always abstaining render perfect classification vacuous, shown
quite broad set problems (each specified underlying distribution family
hypothesis class), perfect realizable classification achievable rejection rate
diminishes quickly zero training sample size.
general, perfect classification cannot achieved noisy setting. paper,
objective achieve pointwise competitiveness, property ensuring prediction
every non-rejected test point identical prediction best predictor hindsight
class. consider pointwise-competitive selective classification
generalize results El-Yaniv Wiener (2010) agnostic case. particular,
show pointwise-competitive classification achievable high probability
learning strategy called low error selective strategy (LESS). Given training sample Sm

c
2015
AI Access Foundation. rights reserved.

fiWiener & El-Yaniv

hypothesis class F, LESS outputs pointwise-competitive selective classifier (f, g),
f (x) standard classifier, g(x) selection function qualifies
predictions dont knows (see definitions Section 2). classifier f simply
taken empirical risk minimizer (ERM) classifier, f. Pointwise competitiveness
achieved g follows. Using standard concentration inequalities, show
true risk minimizer, f , achieves empirical error close f. Thus, high
probability f belongs class low empirical error hypotheses. left
set g(x) allows prediction label x, f(x),
hypotheses low error class unanimously agree label x.
simpler, realizable setting (El-Yaniv & Wiener, 2010), low error class simply reduces
version space.
bulk analysis (in Sections 3, 4 5) concerns coverage bounds LESS,
namely, showing measure region classifier (f, g) refuses classify,
diminishes quickly, high probability, training sample size grows (see Section 2
formal definition). provide several general distribution-dependent coverage
bounds. particular, show (in Corollaries 12 14, respectively) high probability
bounds coverage (f, g) classifier (f, g, ) form,


(f, g) 1 1 (polylog(m) log(1/)/m)2 /2 ,

linear models (unknown) distribution P (X, ), X feature space points
labels, whose marginal P (X) finite mixture Gaussians, axis
aligned rectangles P (X, ) whose marginal P (X) product distribution,
1 , 2 Bernstein class smoothness parameters depending hypothesis class
underlying distribution (and loss function, 0/1 case).
outset, efficient implementation LESS seems reach
required track supremum empirical error possibly infinite hypothesis
subset, general might intractable. overcome computational difficulty,
propose reduction problem problem calculating (two) constrained ERMs.
given test point x, calculate ERM training sample Sm
constraint label x (one positive label constraint one negative). show
thresholding difference empirical error two constrained ERMs
equivalent tracking supremum entire (infinite) hypothesis subset. Based
reduction introduce Section 6 disbelief principle motivates heuristic implementation LESS, relies constrained SVMs, mimics optimal
behavior.
Section 7 present numerical examples medical classification problems
examine empirical performance new algorithm compare performance
widely used selective classification method rejection, based distance
decision boundary.

2. Pointwise-Competitive Selective Classification: Preliminary
Definitions
Let X feature space, example, d-dimensional vectors Rd ,
output space. standard classification, goal learn classifier f : X Y, using
172

fiAgnostic Pointwise-Competitive Selective Classification

finite training sample labeled examples, Sm = {(xi , yi )}m
i=1 , assumed sampled
i.i.d. unknown underlying distribution P (X, ) X Y. classifier
selected hypothesis class F. Let : [0, 1] bounded loss function.
selective classification (El-Yaniv & Wiener, 2010), learning algorithm receives Sm
required output selective classifier, defined pair (f, g), f F
classifier, g : X {0, 1} selection function, serving qualifier f follows.
x X , (f, g)(x) = f (x) iff g(x) = 1. Otherwise, classifier outputs dont know.
general performance selective predictor characterized terms two quantities: coverage risk. coverage (f, g) (f, g) , EP [g(x)] . true risk (f, g),
respect loss function , average loss f restricted region activity
qualified g, normalized coverage, R(f, g) , EP [(f (x), y) g(x)] /(f, g).
easy verify g 1 (and therefore (f, g) = 1), R(f, g) reduces
famil1 Pm
iar risk functional R(f ) , EP [(f (x), y)]. classifier f , let R(f ) , i=1 (f (xi ), yi ),
standard empirical error f sample Sm . Let f = argminf F R(f )
empirical risk minimizer (ERM), let f = argminf F R(f ) true risk minimizer
respect unknown distribution P (X, ).1 Clearly, true risk minimizer f unknown. selective classifier (f, g) called pointwise-competitive x X ,
g(x) > 0, f (x) = f (x).

3. Low Error Selective Strategy (LESS)
hypothesis class F, hypothesis f F, distribution P , sample Sm , real number
r > 0, define true empirical low-error sets,


V(f, r) , f F : R(f ) R(f ) + r
(1)


n

V(f, r) , f F : R(f ) R(f ) + r .

(2)

Throughout paper denote (m, , d) slack standard uniform deviation
bound, given terms training sample size, m, confidence parameter, ,
VC-dimension, d, class F,


+ ln 2
2d ln 2me

(m, , d) , 2
.
(3)

following theorem slight extension statement made Bousquet, Boucheron,
Lugosi (2004, p. 184).
Theorem 1 (Bousquet et al., 2004). Let 0/1 loss function F, hypothesis
class whose VC-dimension d. 0 < < 1, probability least 1
choice Sm P , hypothesis f F satisfies
R(f ) R(f ) + (m, , d).
Similarly, R(f ) R(f ) + (m, , d) conditions.
1

formally, f classifier R(f ) = inf f F R(f ) inf f F P ((x, y) : f (x) 6= f (x)) = 0.
existence (measurable) f guaranteed sufficient considerations (see Hanneke, 2012,
pp. 1511-2).

173

fiWiener & El-Yaniv

Remark 2. use Theorem 1 and, particular, VC bounds classification problems
(0/1 loss) mandatory developing theory presented paper. Similar
theories developed using types bounds (e.g., Rademacher compression
bounds) learning problems.
Let G F. disagreement set (Hanneke, 2007a; El-Yaniv & Wiener, 2010) w.r.t. G
defined
DIS(G) , {x X : f1 , f2 G s.t. f1 (x) 6= f2 (x)} .
(4)
Let us motivate low-error selective strategy (LESS) whose pseudo-code appears
Strategy 1. strategy define whenever empirical risk minimizer (ERM) exists,
example, case 0/1 loss. Using standard uniform deviation bound,
one Theorem 1, one show training error true risk minimizer, f ,
cannot far training error empirical risk minimizer, f. Therefore,

guarantee, high probability, empirical low error class V f, r (applied
appropriately chosen r) includes true risk minimizer f . selection function
g constructed accept subset domain X , hypotheses
empirical low-error set unanimously agree. Strategy 1 formulates idea. call
strategy rather algorithm lacks implementation details. Indeed,
clear outset strategy implemented.
Strategy 1 Agnostic low-error selective strategy (LESS)
Input: Sm , m, ,
Output: pointwise-competitive selective classifier (h, g) w.p. 1

1: Set f = ERM
(F, Sm ), i.e., f empirical risk minimizer F w.r.t. Sm
2: Set G = V f, 2(m, /4, d) (see Eq. (2) (3))
3: Construct g g(x) = 1 x {X \ DIS (G)}
4: f = f
begin analysis LESS. following lemma establishes pointwise competitiveness. Section 4 develop general coverage bounds terms undetermined
disagreement coefficient. Then, Section 5 present distribution-dependent bounds
rely disagreement coefficient.
Lemma 3 (pointwise competitiveness). Let 0/1 loss function F, hypothesis
class whose VC-dimension d. Let > 0 given let (f, g) selective classifier
chosen LESS. Then, probability least 1 /2, (f, g) pointwise competitive
selective classifier.
Proof. Theorem 1, probability least 1 /4,
R(f ) R(f ) + (m, /4, d) .
Clearly, since f minimizes true error, R(f ) R(f). Applying Theorem 1,
know probability least 1 /4,
R(f) R(f) + (m, /4, d) .
174

fiAgnostic Pointwise-Competitive Selective Classification

Using union bound, follows probability least 1 /2,
R(f ) R(f) + 2 (m, /4, d) .
Hence, probability least 1 /2,


f V f, 2 (m, /4, d) , G.

definition, LESS constructs selection function g(x) equals one iff x
X \DIS (G) . Thus, x X , g(x) = 1, hypotheses G agree,
particular f f agree. Therefore (f, g) pointwise-competitive high probability.

4. General Coverage Bounds LESS Terms Disagreement
Coefficient
require following definitions facilitate coverage analysis. f F
r > 0, define set B(f, r) hypotheses reside ball radius r around f ,





B(f, r) , f F : Pr f (X) 6= f (X) r .
XP

G F, distribution P , denote G volume disagreement set
G (see (4)), G , Pr {DIS(G)}. Let r0 0. disagreement coefficient (Hanneke,
2009) hypothesis class F respect target distribution P
(r0 ) , f (r0 ) = sup

r>r0

B(f , r)
.
r

(5)

disagreement coefficient utilized later analysis. See also discussion
characteristics Corollary 7. associated excess loss class class F
loss function (Massart, 2000; Mendelson, 2002; Bartlett, Mendelson, & Philips, 2004)
defined
XL(F, )(x, y) , {(f (x), y) (f (x), y) : f F} .
Whenever F fixed abbreviate XL = XL(F, )(x, y). XL said
(1 , 2 )-Bernstein class respect P (where 0 < 2 1 1 1), every h XL
satisfies
Eh2 1 (Eh)2 .
(6)
Bernstein classes arise many natural situations (see, e.g., Koltchinskii, 2006; Bartlett &
Mendelson, 2006; Bartlett & Wegkamp, 2008). example, conditional probability
P (Y |X) bounded away 1/2, satisfies Tsybakovs noise conditions2 ,
excess loss function Bernstein class (Bartlett & Mendelson, 2006; Tsybakov, 2004).3
2

data generated unknown deterministic hypothesis limited noise P (Y |X)
bounded away 1/2.

3

Specifically, 0/1 loss, Assumption Proposition 1 work Tsybakov (2004), equivalent

Bernstein class condition Equation (6) 2 = 1+
, Tsybakov noise
parameter.

175

fiWiener & El-Yaniv

following sequence lemmas theorems assume binary hypothesis class F
VC-dimension d, underlying distribution P X {1}, 0/1 loss
function. Also, XL denotes associated excess loss class. results extended
loss functions 0/1 similar techniques used Beygelzimer, Dasgupta,
Langford (2009).
Figure 1 schematically depict hypothesis class F (the gray area), target
hypothesis (filled black circle outside F), best hypothesis class f .
distance two points diagram relates distance two hypothesis
marginal distribution P (X). first observation excess loss class
(1 , 2 )-Bernstein class, set low true error (depicted Figure 1 (a)) resides
within larger ball centered around f (see Figure 1 (b)).

Figure 1: set low true error (a) resides within ball around f (b).
Lemma 4. XL (1 , 2 )-Bernstein class respect P , r > 0


V(f , r) B f , 1 r2 .

Proof. f V(f , r) then, definition, E {I(f (X) 6= )} E {I(f (X) 6= )} + r.
linearity expectation have,
E {I(f (X) 6= ) I(f (X) 6= )} r.
Since XL (1 , 2 )-Bernstein,
E {I(f (X) 6= f (X))} = E {|I(f (X) 6= ) I(f (X) 6= )|}
n

= E ((f (X), ) (f (X), ))2 , Eh2 1 (Eh)2
, 1 (E {I(f (X) 6= ) I(f (X) 6= )})2 .


(7), E {I(f (X) 6= f (X))} 1 r2 . Therefore, definition, f B f , 1 r2 .
176

(7)

fiAgnostic Pointwise-Competitive Selective Classification

far seen set low true error resides within ball around f .
would like prove high probability set low empirical error (depicted
Figure 2 (a)) resides within set low true error (see Figure 2 (b)). emphasize
distance hypotheses Figure 2 (a) based empirical error,
distance Figure 2 (b) based true error.

Figure 2: set low empirical error (a) resides within set low true error (b).
Lemma 5. r > 0, 0 < < 1, probability least 1 ,
V(f, r) V (f , 2 (m, /2, d) + r) .
Proof. f V(f, r), then, definition, R(f ) R(f) + r. Since f minimizes empirical
error, know R(f) R(f ). Using Theorem 1 twice, applying union bound,
see probability least 1 ,
R(f ) R(f ) + (m, /2, d)



R(f ) R(f ) + (m, /2, d).

Therefore,
R(f ) R(f ) + 2 (m, /2, d) + r,

f V (f , 2 (m, /2, d) + r) .

shown that, high probability, set low empirical error subset
certain ball around f . Therefore, probability least two hypotheses set
low empirical error disagree bounded probability
least two hypotheses ball around f disagree other. Fortunately,
latter bounded disagreement coefficient established following lemma.

177

fiWiener & El-Yaniv

Lemma 6. r > 0 0 < < 1, probability least 1 ,
V(f, r) 1 (2 (m, /2, d) + r)2 (r0 ),
(r0 ) disagreement coefficient F respect P , applied r0 =
(2(m, /2, d))2 (see (5)).
Proof. Applying Lemmas 5 4 get probability least 1 ,


V(f, r) B f , 1 (2 (m, /2, d) + r)2 .

Therefore,



V(f, r) B f , 1 (2 (m, /2, d) + r)2 .

definition disagreement coefficient (5), r > r0 , B(f , r ) (r0 )r .
Recalling 1 1 thus observing r = 1 (2 (m, /2, d) + r)2 > (2 (m, /2, d))2 =
r0 , proof complete.
position state first coverage bound selective classifier
constructed LESS. bound given terms disagreement coefficient.
Corollary 7. Let F hypothesis class Theorem 1, assume XL
(1 , 2 )-Bernstein class w.r.t. P . Let (f, g) selective classifier constructed LESS.
Then, probability least 1 , (f, g) pointwise competitive selective classifier

(f, g) 1 1 (4 (m, /4, d))2 (r0 ),

(r0 ) disagreement coefficient F respect P , r0 = (2(m, /4, d))2 .

Proof.
Lemma 3,
probability least 1/2, (f, g) pointwise-competitive. Set

G , V f , 2 (m, /4, d) . construction, f = f, selection function g(x) equals
one iff x X \ DIS (G). Thus, definition coverage, (f, g) = E{g(X)} = 1 G.
Therefore, applications Lemma 6 union bound imply probability
least 1 , (f, g) pointwise-competitive coverage satisfies,
(f, g) = E{g(X)} = 1 G 1 1 (4 (m, /4, d))2 (r0 ),

Noting (r) monotone non-increasing r, know coverage bound
Corollary 7 clearly applies (0). quantity (0) discussed numerous papers shown finite various settings including thresholds R
distribution ((0) = 2) (Hanneke, 2009), linear
separators origin
Rd uniform distribution sphere ((0) d) (Hanneke, 2009), linear
separators Rd smooth data distribution bounded away zero ((0) c(f )d,
c(f ) unknown constant depends target hypothesis) (Friedman,
2009). cases, application Corollary 7 sufficient guarantee pointwisecompetitiveness bounded coverage converges one. Unfortunately many
hypothesis classes distributions disagreement coefficient (0) infinite (Hanneke,
2009). Fortunately, disagreement coefficient (r) grows slowly respect 1/r (as
shown Wang, 2011, sufficient smoothness conditions), Corollary 7 sufficient
guarantee bounded coverage.

178

fiAgnostic Pointwise-Competitive Selective Classification

5. Distribution-Dependent Coverage Bounds LESS
section establish distribution-dependent coverage bounds LESS. starting
point bounds following corollary.
Corollary 8. Let F hypothesis class Theorem 1, assume F disagreement coefficient
(r0 ) = (polylog (1/r0 ))
(8)
w.r.t. distribution P , XL (1 , 2 )-Bernstein class w.r.t. distribution.
Let (f, g) selective classifier chosen LESS. Then, probability least 1 ,
(f, g) pointwise competitive coverage satisfies,
!


polylog(m)
1 2 /2
(f, g) 1 1
log
.


Proof. Plugging (8) coverage bound Corollary 7 immediately yields result.

Corollary 8 states fast coverage bounds LESS cases disagreement coefficient grows slowly respect 1/r0 .4 Recent results disagreement-based active
learning selective prediction (Wiener et al., 2014; Wiener, 2013) established tight relations disagreement coefficient empirical quantity called version space
compression set size. quantity analyzed El-Yaniv Wiener (2010)
context realizable selective classification, known distribution-dependent
bounds it. plan rest section introduce version space compression set size, discuss relation disagreement coefficient, show
apply results agnostic setting.
interested solving agnostic case, consider moment
realizable setting utilize known results used analysis. Specifically,
assume f F P(Y = f (x)|X = x) = 1 x X ,
(X, ) P . Given training sample Sm , let VSF ,S induced version space, i.e.,
set hypotheses consistent given sample Sm . version space compression
set size, denoted n(Sm ) = n(F, Sm ), defined size smallest subset
Sm inducing version space (Hanneke, 2007b; El-Yaniv & Wiener, 2010).
function Sm , clearly n(Sm ) random variable, specific realization Sm
value unique.
(0, 1], define version space compression set size minimal bound
Bn (m, ) , min {b N : P(n(Sm ) b) 1 } .

(9)

rely following lemma (Wiener et al., 2014). sake self-containment
provide proof appendix.
4

disagreement coefficient grow ploy-logarithmically 1/r0 still o(1/r0 ),
still possible prove lower bound coverage. Specifically, (r0 ) = ((1/r0 ) ) < 1, one

show (f, g) 1 O(1/( m)2 (1) ).

179

fiWiener & El-Yaniv

Lemma 9 (Wiener et al., 2014). realizablecase, Bn (m,
) = (polylog(m) log (1/)),

1
1
Bn m, 20 = (polylog(m)), (r0 ) = polylog r0 .

Obviously, statement Lemma 9 holds (and well defined) within realizable
setting (the version space compression set size defined setting). turn
back agnostic setting consider arbitrary underlying distribution P X Y.
Recall agnostic setting, let f : X denote (measurable) classifier
R(f ) = inf f F R(f ) inf f F P ((x, y) : f (x) 6= f (x)) = 0, guaranteed
exist sufficient assumptions (see Hanneke, 2012, Section 6.1); call f infimal
(best) hypothesis (of F, w.r.t. P ). Clearly several different infimal hypotheses.
note, however, XL (1 , 2 )-Bernstein class respect P (as assume
paper), Lemma 4 ensures infimal hypotheses identical measure
zero.
definitions version space version space compression set size naturally
generalized agnostic setting respect infimal hypothesis (Wiener et al.,
2014) follows. Let f infimal hypothesis F w.r.t. P . agnostic version space
Sm
VSF ,Sm ,f , {f F : (x, y) Sm , f (x) = f (x)}.

agnostic version space compression set size, denoted n(Sm ) = n(F, Sm , f ), defined
size smallest subset Sm inducing agnostic version space VSF ,Sm ,f .
Finally, extend also definition version space compression set minimal bound
agnostic setting follows.
Bn (m, , f ) , min{b N : P(n(F, Sm , f ) b) 1 }.

key observation allows surprisingly easy utilization Lemma 9
agnostic setting disagreement coefficient depends hypothesis class
F marginal distribution P (X). Using infimal hypothesis f therefore
take agnostic learning problem consider realizable projection, whereby points
labeled f marginal distribution P (X). two problems
(essentially) disagreement coefficients. idea initially observed
Hanneke (2013) Wiener (2013). formulate slight variation
formulation work Wiener, Hanneke, El-Yaniv (2014).
define disagreement agnostic setting (5) respect infimal hypothesis f . agnostic learning problem (F, P ) define realizable
projection (F , P ) follows. Let F , F {f } f infimal hypothesis
agnostic problem. Define P distribution marginal P (X) = P (X),
P(Y = f (x)|X = x) = 1 x X . easy verify (F , P ) realizable
learning problem, i.e., f F PP (X,Y ) (Y = f (x)|X = x) = 1 x X .
Lemma 10 (Realizable projection). Given agnostic learning problem, (F, P ), let
(F , P ) realizable projection. Let (r0 ) (r0 ) associated disagreement coefficients agnostic realizable projection problems, respectively. Then, (r0 ) (r0 ).

Proof. First note depend, respectively, P P via f
marginal distributions P (X) = P (X). Since F F {f } = F , readily get
(r0 ) (r0 ).
180

fiAgnostic Pointwise-Competitive Selective Classification

Let us summarize derivation. Given agnostic problem (F, P ), consider
realizable projection (F , P ). Bn (m, ) = (polylog(m) log (1/)) (or Bn (m, 1/20) =
(polylog(m))) realizable problem, Lemma 9, (r0 ) = (polylog (1/r0 )),
which, Lemma 10, also holds original agnostic problem. Therefore, Corollary 7
applies obtain fast coverage bound LESS w.r.t. (F, P ).
New agnostic coverage bounds LESS obtained using following known bounds
(realizable) version space compression set size. first one, El-Yaniv Wiener
(2010), applies problem learning linear separators mixture Gaussian
distributions. following theorem direct application Lemma 32 work
El-Yaniv Wiener (2010).
Theorem 11 (El-Yaniv & Wiener, 2010). d, n N, let X Rd , F space
linear separators Rd , P distribution marginal Rd mixture
n multivariate normal distributions. Then, constant cd,n > 0 (depending
d, n, otherwise independent P ) 2,
Bn (m, 1/20) cd,n (log(m))d1 .
Applying Theorem 11, together Lemma 10, Lemma 9 Corollary 8, immediately
yields following result.
Corollary 12. Assume conditions Theorem 11. Assume also XL (1 , 2 )Bernstein class w.r.t. P (X, ). Let (f, g) selective classifier constructed LESS.
Then, probability least 1 , (f, g) pointwise competitive selective classifier



(f, g) 1 1 (polylog(m) log(1/)/m)2 /2 .
second version space compression set size bound concerns realizable learning
axis-aligned rectangles product densities Rn . bounds previously
proposed Wiener, Hanneke, El-Yaniv (2014) El-Yaniv Wiener (2010, 2012).
state (without proof) recent bound (Wiener, Hanneke, & El-Yaniv, 2014) giving
version space compression set size bound learning problem (whose positive class
bounded away zero).

Theorem 13 (Wiener et al., 2014). d, N , (0, 1), let X Rd . P
marginal distribution Rd product densities Rd marginals
continuous CDFs, F space axis-aligned rectangles f Rd
P ((x, y) : f (x) = 1) ,

8d
8d
Bn (m, )
ln
.


again, application Theorem 13, together Lemma 10, Lemma 9 Corollary 8 yields following corollary.
Corollary 14. d, N , (0, 1), let X Rd . Let P (X, ) underlying
distribution marginal P (X) product densities Rd marginals
continuous CDFs. Let F space axis-aligned rectangles f Rd P ((x, y) : f (x) =
1) , Assume XL (1 , 2 )-Bernstein class w.r.t. P (X, ). Let (f, g)
181

fiWiener & El-Yaniv

selective classifier constructed LESS. Then, probability least 1 , (f, g)
pointwise competitive selective classifier


(f, g) 1 1 (polylog(m) log(1/)/m)2 /2 .

6. ERM Oracles Disbelief principle
outset, efficient construction selection function g prescribed LESS seems
reach required verify, point x question, whether
hypotheses low error class agree label. Moreover, g computed
entire domain. Luckily, possible compute g lazy manner show
compute g(x) calculating (two) constrained ERMs. given test point x,
calculate ERM training sample Sm constraint label x (one
positive label constraint one negative). show thresholding difference
empirical error two constrained ERMs equivalent tracking supremum
entire (infinite) hypothesis subset. following lemma establishes reduction.
Lemma 15. Let (f, g) selective classifier chosen LESS observing training
sample Sm . Let f empirical risk minimizer Sm . Let x point X
define


n
fx , argmin R(f ) | f (x) = sign f(x) ,
f F

i.e., empirical risk minimizer forced label x opposite f(x).
g(x) = 0



R(fx ) R(f) 2 (m, /4, d) .

Proof. First note according definition V (see Eq (2)),


R(fx ) R(f) 2 (m, /4, d) fx V f, 2 (m, /4, d) .

(10)

(11)

prove first direction (=) (10), assume RHS (10) holds. (11),
get f, fx V. However, construction, f(x) = fx (x), x DIS(V)
g(x) = 0.
prove direction (=), assume R(fx ) R(f) > 2 (m, /4, d).
assumption, prove f V, f (x) = f(x), therefore, x
X \ DIS(V), entailing g(x) = 1. Indeed, assume contradiction exists
f V f (x) = fx (x) 6= f(x). construction, holds
R(f ) R(fx ) > R(f) + 2 (m, /4, d) ,
f 6 V. Contradiction.
Lemma 15 tells us order decide point x rejected need measure
empirical error R(fx ) special empirical risk minimizer, fx , constrained
label x opposite h(x). error sufficiently close R(h), classifier cannot
sure label x must reject it. Thus, provided compute
ERMs, decide whether predict reject individual test point x X ,
182

fiAgnostic Pointwise-Competitive Selective Classification

without actually constructing g entire domain X . Figure 3 illustrates principle
2-dimensional example. hypothesis class class linear classifiers R2
source distribution two normal distributions. Negative samples represented
blue circles positive samples red squares. usual, f denotes empirical

Figure 3: Constrained ERM.
risk minimizer. Let us assume want classify point x1 . point classified
positive f. Therefore, force point negative calculate restricted
ERM (depicted doted line marked fx1 ). difference empirical risk f
fx1 large enough, point x1 rejected. However, want classify
point x2 , difference empirical risk f fx2 quite large point
classified positive.
Equation (11) motivates following definition disbelief index DF (x, Sm )
individual point X . Specifically, x X , define disbelief index w.r.t. Sm
F,
D(x) , DF (x, Sm ) , R(fx ) R(f).
Observe D(x) large whenever model sensitive label x sense
forced bend best model fit opposite label x, model
substantially deteriorates, giving rise large disbelief index. large D(x)
interpreted disbelief possibility x labeled differently.
case definitely predict label x using unforced model. Conversely,
D(x) small, model indifferent label x sense, committed
label. case abstain prediction x. Notice LESS
specific application thresholded disbelief index.
note similar technique using ERM oracle enforce arbitrary
number example-based constraints used Dasgupta, Hsu, Monteleoni (2007a)
Beygelzimer, Hsu, Langford, Zhang (2010), context active learning.
disbelief index, difference empirical risk (or importance weighted
empirical risk, see Beygelzimer et al., 2010) two ERM oracles (with different constraints)
used estimate prediction confidence.

183

fiWiener & El-Yaniv

0.1

0.16
0.14

0.09
test error

test error

0.12
0.1
0.08

0.08
0.07

0.06
0.04

0.06

0.02
0

0.2

0.4

0.6

0.8

0.05
0.1

1

c

0.2

0.3

0.4

0.5

0.6

c

Figure 4: RC curve technique (depicted red) compared rejection based
distance decision boundary (depicted dashed green line). RC curve
right figure zooms lower coverage regions left curve.

practical applications selective prediction desirable allow control
trade-off risk coverage; words, desirable able
develop entire risk-coverage (RC) curve classifier hand (see, e.g., El-Yaniv &
Wiener, 2010) let user choose cutoff point along curve accordance
practical considerations constraints. disbelief index facilitates exploration
risk-coverage trade-off curve classifier follows. Given pool test points
rank test points according disbelief index, points low index
rejected first. Thus, ranking provides means constructing risk-coverage
trade-off curve. Ignoring moment implementation details (which discussed
Section 7), typical RC curve generated LESS depicted Figure 4 (red curve)5 .
dashed green RC curve computed using traditional distance-based techniques
rejection (see discussion common technique Section 8) right graph zoom
section entire RC curve (depicted left graph). dashed horizontal line
test error f entire domain dotted line Bayes error.
high coverage values two techniques statistically indistinguishable, coverage
less 60% get significant advantage LESS. clear case
estimation error reduced, also test error goes significantly optimal
test error f low coverage values.
Interestingly, disbelief index generates rejection regions fundamentally different obtained traditional distance-based techniques rejection (see
Section 8). illustrate point (and still ignoring implementation details), consider
Figure 5 depict rejection regions training sample 150 points sampled
mixture two identical normal distributions (centered different locations).
height map figure, correspond disbelief index magnitude (a), distance
decision boundary (b), reflect confidence regions technique according
confidence measure.
5

learning problem synthetic problem used generating Figure 6.

184

fiAgnostic Pointwise-Competitive Selective Classification

(a)

(b)

Figure 5: Linear classifier. Confidence height map using (a) disbelief index; (b) distance
decision boundary.

Figure 6: SVM polynomial kernel. Confidence height map using (a) disbelief index;
(b) distance decision boundary.

intuitively explain height map Figure 5(a), recall disbelief index
difference empirical error ERM restricted ERM. test
point resides high density region, expect forcing wrong label point
result large increase training error. result, denser area is,
larger disbelief index, therefore, higher classification confidence.
second synthetic 2D source distribution consider even striking. X
distributed uniformly [0, 3] [2, 2] labels sampled according
following conditional distribution

0.95, x2 sin(x1 );
P (Y = 1|X = (x1 , x2 )) ,
0.05, else.
thick red line depicts decision boundary Bayes classifier. hight maps
Figure 6 depict rejection regions obtained (our approximation of) LESS

185

fiWiener & El-Yaniv

traditional (distance decision boundary) technique training sample 50
points sampled distribution (averaged 100 iterations). hypothesis
class used training SVM polynomial kernel degree 5. qualitative
difference two techniques, particular, nice fit disbelief
principle technique compared SVM quite surprising.

Figure 7: RC curves SVM linear kernel. method solid red, rejection
based distance decision boundary dashed green. Horizontal axis (c)
represents coverage.

7. Heuristic Procedure Using SVM Empirical Performance
computation (constrained) ERM oracle efficiently achieved case
realizable learning linear models (see, e.g., El-Yaniv & Wiener, 2010) case
linear regression (Wiener & El Yaniv, 2012). However, noisy setting computation
linear ERM oracle reduced variant MAX FLS C MAX FLS
problems (with strict non-strict inequalities) (Amaldi & Kann, 1995). Unfortunately,

186

fiAgnostic Pointwise-Competitive Selective Classification

MAX FLS APX-complete (within factor 2). C MAX FLS MAX IND SET-hard,
cannot approximated efficiently all. Moreover, extensions results
classes, including axis-aligned hyper-rectangles, showing approximating ERM
classes NP-hard (Ben-David et al., 2003).
present known hardness results (and related lower
bounds) hold half spaces nice distributions Gaussian (mixtures), note
Tauman Kalai et al. (2008) studied problem agnostically learning halfspaces
distributional assumptions. particular, showed data distribution
uniform d-dimensional unit sphere (or hyper-cube, related distributions),
4
possible agnostically learn -accurate halfspaces time poly(d1/ ). However,
known particular distributions elicit effective pointwise competitive
learning. contrary, uniform distribution unit sphere among
worst possible distributions pointwise-competitive classification (and disagreement-based
active learning) unless one utilizes homogeneous halfspaces (see discussion in, e.g., El-Yaniv
& Wiener, 2010).

Figure 8: SVM linear kernel. maximum coverage distance-based rejection
technique allows error rate method specific coverage.

187

fiWiener & El-Yaniv

discussed computational hurdles, recall much applied
machine learning research many applications quite well heuristic
approximations (rather formal ones). practical performance objective,
clever heuristics tricks sometimes make difference. point paper
therefore switch theory practice, aiming implementing rejection method
inspired disbelief principle see well work real world problems.
approximate ERM follows. Using support vector machines (SVMs) use
high C value (105 experiments) penalize training errors small
margin (see definitions SVM parameters in, e.g. Chang & Lin, 2011). way
solution optimization problem tend get closer ERM. order estimate
R(fx ) restrict SVM optimizer consider hypotheses classify
point x specific way. accomplish use weighted SVM unbalanced data.
add point x another training point weight 10 times larger weight
training points combined. Thus, penalty misclassification x large
optimizer finds solution doesnt violate constraint.
Another problem face disbelief index noisy statistic highly
depends sample Sm . overcome noise use robust statistics. First
1 , 2 , . . . k ) using bootstrap sampling
generate odd number k different samples (Sm


(we used k = 11). sample calculate disbelief index test points
point take median measurements final index. also note
finite training sample disbelief index discrete variable. often case
several test points share disbelief index. cases use confidence
measure tie breaker. experiments use distance decision boundary
break ties. Focusing SVMs linear kernel compared RC (Risk-Coverage)
curves achieved proposed method achieved SVM rejection based
distance decision boundary. latter approach common practical
applications selective classification. implementation used LIBSVM (Chang &
Lin, 2011).
tested algorithm standard medical diagnosis problems UCI repository, including datasets used Grandvalet, Rakotomamonjy, Keshet, Canu (2008).
transformed nominal features numerical ones standard way using binary indicator attributes. also normalized attribute independently dynamic
range [0, 1]. preprocessing employed. iteration choose uniformly
random non-overlapping training set (100 samples) test set (200 samples)
dataset.6 SVM trained entire training set, test samples sorted
according confidence (either using distance decision boundary disbelief index).
Figure 7 depicts RC curves technique (red solid line) rejection based
distance decision boundary (green dashed line) linear kernel 6 datasets.
results averaged 500 iterations (error bars show standard error). exception
Hepatitis dataset, methods statistically indistinguishable,
datasets proposed method exhibits significant advantage traditional
approach. would like highlight performance proposed method
Pima dataset. traditional approach cannot achieve error less 8%
6

Due size Hepatitis dataset test set limited 29 samples.

188

fiAgnostic Pointwise-Competitive Selective Classification

Figure 9: RC curves SVM RBF kernel. method solid red rejection
based distance decision boundary dashed green.

rejection rate, approach test error decreases monotonically zero rejection
rate. Furthermore, clear advantage method large range rejection rates
evident Haberman dataset.7 .
sake fairness, note running time algorithm (as presented
here) substantially longer traditional technique. performance algorithm substantially improved many unlabeled samples available.
case rejection function evaluated unlabeled samples generate new
labeled sample. new rejection classifier trained sample.
Figure 8 depicts maximum coverage distance-based rejection technique
allows error rate method specific coverage. example, let us
assume method error rate 10% coverage 60%
7

Haberman dataset contains survival data patients undergone surgery breast cancer.
estimated 207,090 new cases breast cancer united states 2010 (Society, 2010)
improvement 1% affects lives 2000 women.

189

fiWiener & El-Yaniv

Figure 10: SVM RBF kernel. maximum coverage distance-based rejection
technique allows error rate method specific coverage.

distance-based rejection technique achieves error maximum coverage 40%.
point (0.6, 0.4) red line. Thus, red line bellow diagonal
technique advantage distance-based rejection visa versa.
example, consider Haberman dataset, observe regardless rejection rate,
distance-based technique cannot achieve error technique coverage
lower 80%.
Figures 9 10 depict results obtained RBF kernel. case statistically
significant advantage technique observed datasets.

8. Related Work
Pointwise-competitive classification unique extreme instance classification
abstention option, idea emerged pattern recognition community,
first proposed studied 50 years ago Chow (1957, 1970), generated lots interest

190

fiAgnostic Pointwise-Competitive Selective Classification

(Fumera et al., 2001; Tortorella, 2001; Santos-Pereira & Pires, 2005; Fumera & Roli, 2002;
Pietraszek, 2005; Bounsiar et al., 2006; Landgrebe et al., 2006; Herbei & Wegkamp, 2006;
Hellman, 1970; El-Yaniv & Pidan, 2011; Bartlett & Wegkamp, 2008; Wegkap, 2007; Freund
et al., 2004). Taking broader perspective, pointwise-competitive selective prediction (and
particular, classification) particular instance broader concept confidencerated learning, whereby learner must formally quantify confidence prediction.
Achieving effective confidence-rated prediction (including abstention) longstanding
challenging goal number disciplines research communities. Let us first discuss
prominent approaches confidence-rated prediction note
related present work.
knows-what-it-knows (KWIK) framework studied reinforcement-learning (Li,
Littman, & Walsh, 2008; Strehl & Littman, 2007; Li & Littman, 2010) similar notion
pointwise competitiveness studied, coverage rates analyzed (Li et al., 2008;
Li, 2009). However, KWIK limited realizable model concerned
adversarial setting target hypothesis training data selected
adversary. positive results KWIK adversarial setting apply
statistical pointwise-competitive prediction setting (where training examples sampled
i.i.d.), adversarial setting precludes non trivial coverage interesting hypothesis
classes currently addressed pointwise-competitive prediction. deficiency comes
surprise KWIK adversarial setting much challenging statistical
pointwise-competitive prediction assumptions.
conformal prediction framework (Vovk, Gammerman, & Shafer, 2005) provides
hedged predictions allowing possibility multi-labeled predictions guarantees
user-desired confidence rate asymptotic sense. Conformal prediction mainly concerned online probabilistic setting. Rather predicting single label
sample point, conformal predictor assign multiple labels. user-defined confidence level error rate asymptotically guaranteed. interpreting
multi-labeled predictions rejection, compare pointwise-competitive prediction. sense, conformal prediction construct online predictors reject option
asymptotic performance guarantees. important differences conformal predictions pointwise-competitive prediction pointed out.
approaches provide hedged predictions, use different notions hedging. Whereas
pointwise-competitive prediction goal guarantee high probability
training sample predictor agrees best predictor class
points accepted domain, goal conformal predictions provide guarantees
average error rate, average taken possible samples test
points.8 sense, conformal prediction cannot achieve pointwise competitiveness.
addition, conformal prediction also utilizes different notion error one used
pointwise-competitive model. pointwise-competitive prediction focused performance guarantees error rate covered (accepted) examples, conformal
prediction provides guarantee examples (including multiple predictions none all). increasing multi-labeled prediction rate (uncertain prediction),
8

noted Vovk et al.: impossible achieve conditional probability error equal given
observed examples, unconditional probability error equals . Therefore, implicitly
involves averaging different data sequences... (Vovk et al., 2005, p. 295).

191

fiWiener & El-Yaniv

error rate decreased arbitrarily small value. case
pointwise-competitive prediction error notion covered examples, bounded
Bayes error covered region. Finally, conformal prediction mentions
notion efficiency, similar coverage but, best knowledge,
finite sample results established. Another interesting scheme vicinity
confidence-rated learning guaranteed error machine (GEM) (Campi, 2010).
GEM model reject option considered correct answer, means risk
reduced arbitrarily (as conformal prediction).
Pointwise-competitive classification special case pointwise-competitive prediction
(El-Yaniv & Wiener, 2010, 2011; Wiener & El Yaniv, 2012; El-Yaniv & Wiener, 2012;
Wiener, 2013; Wiener et al., 2014). Pointwise-competitive selective classification first
addressed El-Yaniv Wiener (2010) realizable case studied (in
paper pointwise-competitiveness termed perfect classification). present article
extends pointwise-competitive classification noisy problems
also number theoretical studies (general) selective classification (not
pointwise-competitive). Freund et al. (2004) studied simple ensemble method binary
classification. Given hypothesis class F, method outputs weighted average
hypotheses F, weight hypothesis exponentially depends
individual training error. algorithm abstains prediction whenever weighted
average individual predictions close zero. able bound probability
misclassification 2R(f ) + (m) and, conditions, proved bound
5R(f ) + (F, m) rejection rate. LESS strategy viewed extreme
variation Freund et al. method. include ensemble hypotheses
sufficiently low empirical error abstain weighted average predictions
definitive ( 6= 1). risk coverage bounds asymptotically tighter.
Excess risk bounds developed Herbei Wegkamp (2006) model
rejection incurs cost [0, 1/2]. bound applies empirical risk minimizer
hypothesis class ternary hypotheses (whose output {1, reject}). See also
various extensions Wegkap (2007) Bartlett Wegkamp (2008).
rejection mechanism SVMs based distance decision boundary perhaps
widely known used rejection technique. routinely used medical applications (Mukherjee et al., 1998; Guyon et al., 2002; Mukherjee, 2003). papers proposed
alternative techniques rejection case SVMs. include taking reject
area account optimization (Fumera & Roli, 2002), training two SVM classifiers
asymmetric cost (Sousa, Mora, & Cardoso, 2009), using hinge loss (Bartlett &
Wegkamp, 2008). Grandvalet et al. (2008) proposed efficient implementation SVM
reject option using double hinge loss. empirically compared results
two selective classifiers: one proposed Bartlett Wegkamp (2008)
traditional rejection based distance decision boundary. experiments
statistically significant advantage either method compared traditional
approach high rejection rates.
Pointwise selective classification strongly tied disagreement-based active learning.
realizable case, El-Yaniv Wiener (2012) presented reduction stream-based
active learning CAL algorithm Cohn et al. (1994) pointwise-competitive
classification. reduction roughly states rejection rate (the reciprocal

192

fiAgnostic Pointwise-Competitive Selective Classification

coverage) LESS O(polylog(m/)/m) problem (F, P ) actively learnable
CAL exponential speedup. consequence reduction resulted first
exponential speedup bounds CAL general linear models finite mixture
Gaussians. direction, showing exponential speedup CAL implies
rejection rate LESS (in realizable setting) recently established Wiener
(2013) Wiener, Hanneke, El-Yaniv (2014) (using two different techniques).
version space compression set size, extensively utilized present
work, introduced implicitly Hanneke (2007b) special case extended
teaching dimension, context, version space compression set called
minimal specifying set. introduced explicitly El-Yaniv Wiener (2010)
context selective classification, proved El-Yaniv Wiener (2012)
special case extended teaching dimension Hanneke (2007b). Relations
disagreement coefficient version space compression set size first discussed
El-Yaniv Wiener (2012). Sharp ties two quantities, stated
Lemma 9, others recently developed Wiener, Hanneke, El-Yaniv
(2014).

9. Concluding Remarks
find existence pointwise-competitive classification quite fascinating. striking
feature classifier that, definition, pointwise-competitive predictor free
estimation error cannot overfit. means hypothesis class
expressive like still protected overfitting. However, without
effective coverage bounds pointwise-competitive classifier may refuse predict
times.
current paper, recent studies selective prediction (El-Yaniv & Wiener,
2015) active learning (Wiener, Hanneke, & El-Yaniv, 2014), place version space
compression set size center stage, leading quantity drive results
intuition domains. present, known technique able prove fast
coverage pointwise-competitive classification exponential label complexity speedup
disagreement-based active learning general linear models fixed mixture
Gaussians axis aligned rectangles product distributions.. possible
extend results beyond linear classifiers axis aligned rectangles interesting
distribution families? example, plausible existing results axis-aligned
rectangles extended decision trees.
formal relationship active learning pointwise-competitive classification
(El-Yaniv & Wiener, 2012; Wiener, 2013; Wiener et al., 2014) created powerful synergy
allows migrating results two models. Currently, formal connection manifested via two links. first, within realizable setting, equivalence
LESS-based classification fast coverage CAL-based active learning exponential speedup. second link consists bounds relate underlying complexity
measures: disagreement coefficient active learning, version space compression
set size pointwise-competitive classification. number non-established relations significantly substantiate interaction two problems could
considered. example, possible prove direct equivalence LESS-based

193

fiWiener & El-Yaniv

pointwise-competitive agnostic classification fast coverage rates LESS-based active
learning exponential speedup? expect resolution question
various interesting implications. example, relationship could potentially facilitate
migration interesting algorithms techniques devised active learning
pointwise-competitive framework. immediate candidate algorithm Beygelzimer et al. (2010), builds ideas Dasgupta et al. (2007b) Beygelzimer et al.
(2009). Resembling implementation proposed LESS via calls (a constrained) ERM
oracle, algorithm works without tracking version space final choice
hypothesis well querying component. Instead, querying, relies
ERM oracle enforces one example-based constrain. Thus, importanceweighting technique based resembles disbelief principle outline here.
regard, interesting also consider migrate ideas active
learning algorithms emerging online learning branch (Orabona & Cesa-Bianchi,
2011; Cesa-Bianchi et al., 2009; Dekel et al., 2010) using, required, online batch
conversion techniques (Zhang, 2005; Kakade & Tewari, 2009; Cesa-Bianchi & Gentile, 2008;
Dekel, 2008).
LESS strategy requires unanimous vote among hypotheses low empirical
error subset hypothesis class. considering, e.g., linear models, subset
hypotheses uncountable, case (even finite) size huge. Clearly,
LESS extremely radical defensive strategy. immediate question arises
whether LESS unanimity requirement relaxed majority vote.
achieve pointwise competitiveness (strong) majority vote instead unanimity?
Besides greater flexibility general voting scheme, may lead different types
interesting learning algorithms, relaxation potentially ease computational
complexity implementing LESS (which, discussed above, bottleneck agnostic
classification). example, relaxed voting scheme might utilize hypothesis
sampling, classical example related context celebrated query-bycommittee (QBC) strategy (Seung et al., 1992; Freund et al., 1997; Fine et al., 2002; GiladBachrach, 2007; Gilad-Bachrach et al., 2005). However, strict pointwise competitiveness
advocated, easy see strong majority vote sufficient. Indeed, consider
f differs hypotheses F single point X . Unless probability
point large (not typical case), high probability point part
training set Sm , therefore, majority vote (even strong) label
opposite f . Hence, worst case, even strong majority sufficient pointwise
competitiveness. natural compromise pointwise competitiveness objective, one
revert standard excess-risk bounds (Bartlett et al., 2006) whereby compare
overall average performance predictor, R(f ), optimal predictor, R(f )
(not pointwise). regard, work Freund, Mansour, Schapire (2004) discussed
Section 8, result excess-risk bound R(f, g) 2R(f
) + 1/(m1/2)

( hyper-parameter) coverage bound (f, g) 1 5R(f ) ln |F|/ m1/2 .
Considering excess-risk bounds f , possible beat risk coverage
bounds using relaxed voting scheme rejection? would optimal bounds
fully agnostic setting? better bounds devised specific distributions like
Gaussian mixtures? note Freund et al. strategy also interesting

194

fiAgnostic Pointwise-Competitive Selective Classification

final aggregated predictor general outside F can, principle, significantly
outperform f F (the bound elicit behavior). emphasizes
potential usefulness ensembles, applied rejection scheme, also
final predictor. Recall LESS strategy final predictor always belongs F.
Thus, considering ensembles allowing excess-risk bounds, even
ambitious goals, strictly beating f average.

Acknowledgments
thank anonymous referees good comments, grateful Steve Hanneke helpful insightful discussions. Also, warmly thank Intel Collaborative
Research Institute Computational Intelligence (ICRI-CI), Israel Science Foundation
(ISF) generous support.

Appendix A. Proofs
proof Lemma 9 relies following Lemma 16 (Wiener et al., 2014), whose
proof also provided sake self-containment.

Lemma 16 (Wiener et al., 2014). realizable case, r0 (0, 1),



1
1
,
, 512 .
(r0 ) max max 16Bn
r 20
r(r0 ,1)


Proof. prove that, r (0, 1),




B(f , r)
1
1
max 16Bn
,
, 512 .
r
r 20

(12)

result follows taking supremum sides r (r0 , 1).
Fix r (0, 1), let = 1/r, {1, . . . , m}, define Sm\i = Sm \ {(xi , yi )}. Also
define Dm\i = DIS(VSF ,Sm\i B(f , r)) m\i = P(xi Dm\i |Sm\i ) = P (Dm\i Y).
B(f , r)m 512, (12) clearly holds. Otherwise, suppose B(f , r)m > 512. xi
DIS(VSF ,Sm\i ), must (xi , yi ) CSm .

n(Sm )


X
i=1

1

DIS(VSF ,Sm\i ) (xi ).

195

fiWiener & El-Yaniv

Therefore,
P {n(Sm ) (1/16)B(f , r)m}
)
(m
X

P
DIS(VSF ,S
) (xi ) (1/16)B(f , r)m
1

P
=P

m\i

i=1
(m
X

Dm\i (xi )

1

i=1
(m
X

(1/16)B(f , r)m

DIS(B(f ,r)) (xi )

1

i=1

=P

(


X



DIS(B(f ,r)) (xi )



1

1

+P

1

i=1

X

P



DIS(B(f ,r)) (xi )
i=1

X

i=1
(m
X

i=1
(m
X
1

1

)



1

1

Dm\i (xi )

Dm\i (xi )




X

+P

DIS(B(f ,r)) (xi )

i=1


X

1
B(f , r)m,
DIS(B(f ,r)) (xi )
16
1

Dm\i (xi )


X

1

1

i=1

(1/16)B(f , r)m

)
7

DIS(B(f ,r)) (xi ) < B(f , r)m
8



1
B(f , r)m,
DIS(B(f ,r)) (xi )
16

DIS(B(f ,r)) (xi )

)




X
i=1

)

1

)
7

DIS(B(f ,r)) (xi ) B(f , r)m
8

< (7/8)B(f , r)m

i=1

(

1



DIS(B(f ,r)) (xi )

i=1



1

Dm\i (xi )



)

(13/16)B(f , r)m

.

Since considering case B(f , r)m > 512, Chernoff bound implies
!

X

exp {B(f , r)m/128} < e4 .
P
DIS(B(f ,r)) (xi ) < (7/8)B(f , r)m
1

i=1

Furthermore, Markovs inequality implies
P


X

1

DIS(B(f ,r)) (xi )

i=1



1

Dm\i (xi )

!

(13/16)B(f , r)m


mB(f , r) E

hP


i=1

1

Dm\i (xi )

(13/16)mB(f , r)

Since xi values exchangeable,
#
"m


fi
h h
ii X
X
X




fi
E E Dm\i (xi )fiSm\i =
E m\i = m\m .
E
Dm\i (xi ) =
1

i=1

1

i=1

i=1

196



.

fiAgnostic Pointwise-Competitive Selective Classification

shown (Hanneke, 2012) least
m(1 r)m1 B(f , r).
particular, B(f , r)m > 512, must r < 1/511 < 1/2, implies
(1 r)1/r1 1/4,
#
"m
X

E
Dm\i (xi ) (1/4)mB(f , r).
1

i=1

Altogether, established
P (n(Sm ) (1/16)B(f , r)m) <

mB(f , r) (1/4)mB(f , r)
+ e4
(13/16)mB(f , r)
12
=
+ e4 < 19/20.
13


1
Thus, since n(Sm ) Bn m, 20
probability least 19/20, must


B(f , r)
1
.
Bn m,
> (1/16)B(f , r)m (1/16)
20
r

Proof Lemma 9. Assuming Bn (m, ) = polylog(m) log 1 holds, exists
constant 1 (0, 1/20)
Bn (m, ) non Bn (m, 1 ) = (polylog(m)).

1
1
Bn (m, 1 ), thus Bn m, 20
= (polylog(m)). Therefore,
increasing , Bn m, 20






1
1
max Bn m,
= max polylog(m) = polylog
,
20
r0
m1/r0
m1/r0
using Lemma 16 have,




1
(r0 ) max
max 16Bn m,
, 512
20
m1/r0





1
1
.
= polylog
528 + 16 max Bn m,
20
r0
m1/r0

References
Amaldi, E., & Kann, V. (1995). complexity approximability finding maximum
feasible subsystems linear relations. Theoretical computer science, 147 (1), 181210.
Bartlett, P. L., Jordan, M. I., & McAuliffe, J. D. (2006). Convexity, classification, risk
bounds. Journal American Statistical Association, 101 (473), 138156.
Bartlett, P., & Mendelson, S. (2006). Discussion 2004 IMS medallion lecture: Local
rademacher complexities oracle inequalities risk minimization V. koltchinskii. Annals Statistics, 34, 26572663.

197

fiWiener & El-Yaniv

Bartlett, P., Mendelson, S., & Philips, P. (2004). Local complexities empirical risk
minimization. COLT: Proceedings Workshop Computational Learning
Theory, Morgan Kaufmann Publishers.
Bartlett, P., & Wegkamp, M. (2008). Classification reject option using hinge loss.
Journal Machine Learning Research, 9, 18231840.
Ben-David, S., Eiron, N., & Long, P. (2003). difficulty approximately maximizing
agreements. Journal Computer System Sciences, 66 (3), 496514.
Beygelzimer, A., Dasgupta, S., & Langford, J. (2009). Importance weighted active learning.
Proceedings 26th Annual International Conference Machine Learning, pp.
4956. ACM.
Beygelzimer, A., Hsu, D., Langford, J., & Zhang, T. (2010). Agnostic active learning without
constraints. Advances Neural Information Processing Systems 23.
Beygelzimer, A., Dasgupta, S., & Langford, J. (2009). Importance weighted active learning.
Proceedings 26th annual international conference machine learning, pp.
4956. ACM.
Beygelzimer, A., Hsu, D., Langford, J., & Zhang, T. (2010). Agnostic active learning without
constraints. arXiv preprint arXiv:1006.2588.
Bounsiar, A., Grall, E., & Beauseroy, P. (2006). kernel based rejection method supervised classification. International Journal Computational Intelligence, 3, 312321.
Bousquet, O., Boucheron, S., & Lugosi, G. (2004). Introduction statistical learning
theory. Advanced Lectures Machine Learning, Vol. 3176 Lecture Notes
Computer Science, pp. 169207. Springer.
Campi, M. (2010). Classification guaranteed probability error. Mach. Learn., 80 (1),
6384.
Cesa-Bianchi, N., & Gentile, C. (2008). Improved risk tail bounds on-line algorithms.
Information Theory, IEEE Transactions on, 54 (1), 386390.
Cesa-Bianchi, N., Gentile, C., & Orabona, F. (2009). Robust bounds classification via
selective sampling. Proceedings 26th Annual International Conference
Machine Learning, pp. 121128. ACM.
Chang, C., & Lin, C. (2011). LIBSVM: library support vector machines. ACM
Transactions Intelligent Systems Technology, 2, 27:127:27. Software available
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Chow, C. (1957). optimum character recognition system using decision function. IEEE
Trans. Computer, 6 (4), 247254.
Chow, C. (1970). optimum recognition error reject trade-off. IEEE Trans.
Information Theory, 16, 4136.
Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization active learning.
Machine Learning, 15 (2), 201221.
Dasgupta, S., Hsu, D., & Monteleoni, C. (2007a). general agnostic active learning algorithm. NIPS.

198

fiAgnostic Pointwise-Competitive Selective Classification

Dasgupta, S., Monteleoni, C., & Hsu, D. J. (2007b). general agnostic active learning
algorithm. Advances neural information processing systems, pp. 353360.
Dekel, O. (2008). online batch learning cutoff-averaging.. NIPS.
Dekel, O., Gentile, C., & Sridharan, K. (2010). Robust selective sampling single
multiple teachers.. COLT, pp. 346358.
El-Yaniv, R., & Pidan, D. (2011). Selective prediction financial trends hidden
markov models. NIPS, pp. 855863.
El-Yaniv, R., & Wiener, Y. (2010). foundations noise-free selective classification.
Journal Machine Learning Research, 11, 16051641.
El-Yaniv, R., & Wiener, Y. (2011). Agnostic selective classification. Neural Information
Processing Systems (NIPS).
El-Yaniv, R., & Wiener, Y. (2012). Active learning via perfect selective classification.
Journal Machine Learning Research, 13, 255279.
El-Yaniv, R., & Wiener, Y. (2015). version space compression set size
applications. Vovk, V., Papadopoulos, H., & Gammerman, A. (Eds.), Measures
Complexity: Festschrift Alexey Chervonenkis. Springer, Berlin.
Fine, S., Gilad-Bachrach, R., & Shamir, E. (2002). Query committee, linear separation
random walks. Theoretical Computer Science, 284 (1), 2551.
Freund, Y., Mansour, Y., & Schapire, R. (2004). Generalization bounds averaged classifiers. Annals Statistics, 32 (4), 16981722.
Freund, Y., Seung, H., Shamir, E., & Tishby, N. (1997). Selective sampling using query
committee algorithm. Machine Learning, 28, 133168.
Friedman, E. (2009). Active learning smooth problems. Proceedings 22nd
Annual Conference Learning Theory.
Fumera, G., & Roli, F. (2002). Support vector machines embedded reject option.
Pattern Recognition Support Vector Machines: First International Workshop, pp.
811919.
Fumera, G., Roli, F., & Giacinto, G. (2001). Multiple reject thresholds improving
classification reliability. Lecture Notes Computer Science, 1876.
Gilad-Bachrach, R. (2007). PAC Beyond. Ph.D. thesis, Hebrew University
Jerusalem.
Gilad-Bachrach, R., Navot, A., & Tishby, N. (2005). Query committee made real.
NIPS.
Grandvalet, Y., Rakotomamonjy, A., Keshet, J., & Canu, S. (2008). Support vector machines reject option. NIPS, pp. 537544. MIT Press.
Guyon, I., Weston, J., Barnhill, S., & Vapnik, V. (2002). Gene selection cancer classification using support vector machines.. Machine Learning, 389422.
Hanneke, S. (2007a). bound label complexity agnostic active learning. ICML,
pp. 353360.

199

fiWiener & El-Yaniv

Hanneke, S. (2007b). Teaching dimension complexity active learning. Proceedings 20th Annual Conference Learning Theory (COLT), Vol. 4539 Lecture
Notes Artificial Intelligence, pp. 6681.
Hanneke, S. (2009). Theoretical Foundations Active Learning. Ph.D. thesis, Carnegie
Mellon University.
Hanneke, S. (2013). statistical theory active learning. Unpublished.
Hanneke, S. (2012). Activized learning: Transforming passive active improved label
complexity. Journal Machine Learning Research, 98888, 14691587.
Hellman, M. (1970). nearest neighbor classification rule reject option. IEEE
Trans. Systems Sc. Cyb., 6, 179185.
Herbei, R., & Wegkamp, M. (2006). Classification reject option. Canadian Journal
Statistics, 34 (4), 709721.
Kakade, S., & Tewari, A. (2009). generalization ability online strongly convex
programming algorithms. Advances Neural Information Processing Systems
(NIPS), pp. 801808.
Koltchinskii, V. (2006). 2004 IMS medallion lecture: Local rademacher complexities
oracle inequalities risk minimization. Annals Statistics, 34, 25932656.
Landgrebe, T., Tax, D., Paclk, P., & Duin, R. (2006). interaction classification
reject performance distance-based reject-option classifiers. Pattern Recognition
Letters, 27 (8), 908917.
Li, L., & Littman, M. L. (2010). Reducing reinforcement learning kwik online regression.
Annals Mathematics Artificial Intelligence, 217237.
Li, L., Littman, M., & Walsh, T. (2008). Knows knows: framework self-aware
learning. Proceedings 25th international conference Machine learning, pp.
568575. ACM.
Li, L. (2009). unifying framework computational reinforcement learning theory. Ph.D.
thesis, Rutgers, State University New Jersey.
Massart, P. (2000). applications concentration inequalities statistics. Annales
de la Faculte des Sciences de Toulouse, Vol. 9, pp. 245303. Universite Paul Sabatier.
Mendelson, S. (2002). Improving sample complexity using global data. Information
Theory, IEEE Transactions on, 48 (7), 19771991.
Mukherjee, S. (2003). Chapter 9. classifying microarray data using support vector machines.
scientists University Pennsylvania School Medicine School
Engineering Applied Science. Kluwer Academic Publishers.
Mukherjee, S., Tamayo, P., Slonim, D., Verri, A., Golub, T., Mesirov, J. P., & Poggio, T.
(1998). Support vector machine classification microarray data. Tech. rep., AI Memo
1677, Massachusetts Institute Technology.
Orabona, F., & Cesa-Bianchi, N. (2011). Better algorithms selective sampling.
Proceedings 28th International Conference Machine Learning (ICML-11),
pp. 433440.

200

fiAgnostic Pointwise-Competitive Selective Classification

Pietraszek, T. (2005). Optimizing abstaining classifiers using ROC analysis. Proceedings Twenty-Second International Conference Machine Learning(ICML), pp.
665672.
Santos-Pereira, C., & Pires, A. (2005). optimal reject rules ROC curves. Pattern
Recognition Letters, 26 (7), 943952.
Seung, H., Opper, M., & Sompolinsky, H. (1992). Query committee. Proceedings
Fifth Annual Workshop Computational Learning theory (COLT), pp. 287294.
Society, A. C. (2010). Cancer facts & figures 2010..
Sousa, R., Mora, B., & Cardoso, J. (2009). ordinal data method classification
reject option. ICMLA, pp. 746750. IEEE Computer Society.
Strehl, A. L., & Littman, M. L. (2007). Online linear regression application
model-based reinforcement learning. Advances Neural Information Processing
Systems, pp. 14171424.
Tauman Kalai, A., Klivans, A., Mansour, Y., & Servedio, R. (2008). Agnostically learning
halfspaces. SIAM J. Comput., 37 (6), 17771805.
Tortorella, F. (2001). optimal reject rule binary classifiers. Lecture Notes Computer
Science, 1876, 611620.
Tsybakov, A. (2004). Optimal aggregation classifiers statistical learning. Annals
Mathematical Statistics, 32, 135166.
Vovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic Learning Random World.
Springer, New York.
Wang, L. (2011). Smoothness, disagreement coefficient, label complexity agnostic
active learning. JMLR, 22692292.
Wegkap, M. (2007). Lasso type classifiers reject option. Electronic Journal
Statistics, 1, 155168.
Wiener, Y. (2013). Theoretical Foundations Selective Prediction. Ph.D. thesis, Technion
Israel Institute Technology.
Wiener, Y., & El Yaniv, R. (2012). Pointwise tracking optimal regression function.
Advances Neural Information Processing Systems 25, pp. 20512059.
Wiener, Y., Hanneke, S., & El-Yaniv, R. (2014). compression technique analyzing
disagreement-based active learning. arXiv preprint arXiv:1404.1504.
Zhang, T. (2005). Data dependent concentration bounds sequential prediction algorithms. Learning Theory, pp. 173187. Springer.

201

fiJournal Artificial Intelligence Research 52 (2015) 287-329

Submitted 10/14; published 02/15

Revision History
Paolo Liberatore

liberato@dis.uniroma1.it

Sapienza University Rome, DIAG
Via Ariosto 25, 00185 Rome, Italy

Abstract
article proposes solution problem obtaining plausibility information,
necessary perform belief revision: given sequence revisions, together
results, derive possible initial order generated them; different
usual assumption starting all-equal initial order modifying sequence revisions. Four semantics iterated revision considered: natural, restrained,
lexicographic reinforcement. each, necessary sufficient condition existence order generating given history revisions results proved. Complexity
proved coNP complete cases one (reinforcement revision unbounded sequence length).

1. Introduction
Many belief revision operators based sort plausibility order (Spohn, 1988;
Boutilier, 1996; Nayak, 1994; Williams, 1994; Areces & Becher, 2001; Zhang, 2004; Benferhat, Kaci, Le Berre, & Williams, 2004; Hild & Spohn, 2008; Ferme & Hansson, 2011).
Whenever revising done two different ways, result disjunction
either (Alchourron & Makinson, 1982; Fagin, Ullman, & Vardi, 1983; Winslett,
1988) plausible ones according order (Gardenfors, 1988; Katsuno
& Mendelzon, 1991; Peppas, 2008; Nebel, 1992; Ferme & Hansson, 2011). Fewer disjuncts
imply formulae; therefore, discriminating order, informative
result. fine-grained order central usefulness revised knowledge base.
Iterated revision provides way obtaining plausibility order. Even starting
all-equal plausibility order (the least discriminating one), revision changes
making possibilities plausible others (Spohn, 1988; Boutilier, 1996; Nayak,
1994; Williams, 1994; Booth & Meyer, 2006; Jin & Thielscher, 2007). sequence revisions
produces order that, depending revising formulae, less informative.
cases, solution problem obtaining plausibility order: sequence
previous revisions (Konieczny & Pino Perez, 2000; Baltag, Gierasimczuk, & Smets, 2011).
However, even long history revisions may produce fine discrimination. limit
case, revising a, a, a, a, etc., final order discriminates models
models a.
way obtain initial plausibility order? One possibility derive
knowledge previous results (this also done merging, Liberatore,
2014b, 2014a). words, previous revising formulae given, also
results produced. K0 initial knowledge base P1 first revising
formula, result another knowledge base K1 , revised P2 .
article, Ki Pi assumed known certain point:
c
2015
AI Access Foundation. rights reserved.

fiLiberatore

P

P

P

P

1
2
3
n
K0
K1
K2
K3 . . . Kn1
Kn

sequence consistent formulae [K0 , P1 , K1 , . . . , Pn , Kn ] called revision sequence. gives information initial plausibility order models, like following example shows.
Example 1 Let [K0 , P1 , K1 ] revision sequence where:
K0 =
P1 =
K1 = b c
specific K1 = b c possible result revising K0 =
P1 = a. example b c also possible, b c actual revision
result. means model {a, b, c} considered plausible {a, b, c},
another model P1 . information useful subsequent revisions.
revision sequence may seen form training: first n revisions manually performed human operators, following others done automatically using initial plausibility order obtained training. Technically, revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] containing revising formulae resulting
knowledge bases Kn , one derives initial plausibility order, revised
P1 , . . . , Pn , Pn+1 , Pn+2 , . . . obtain Kn+1 , Kn+2 , . . . similar mechanism studied
Nittka Booth (2008); comparison approach conclusions.
Example 2 part research project, PhD student Polyxena tasked automating incorporation new data database, process far manually performed
database maintainers, specialized group people. soon realizes information
incorporated may disjunctive, revisions may done multiple ways. Introducing data form either b b database requires
erasing either a, b both. Studying relevant literature topic, finds
choices require plausibility information. Since previous revisions performed
database maintainers, possess information. therefore asks
rank models, getting answers ranging mean? rank Brazilian
redhead models top, Swiss models bad. spending half hour trying
explain concepts plausibility, consistency, propositional models (and
differ fashion models) ranking, gives up.
leave room, one maintainers suggests
look logs, since files record everything happened database.
does, indeed previous revisions stored logs: new information
database maintainers incorporated it. problem shifts eliciting
rank database maintainers, proved difficult, determining
previous history revisions. finds sequence revisions results
natural-compatible (Definition 4) compatible revisions. means
288

fiRevision History

initial plausibility order calculated using Lemma 2, also
people performed previous revisions (unknowingly) adopted policy minimal
plausibility change.
Apart remarks operators, admittedly surreal comedic purposes,
example shows not-so-uncommon scenario: process performed hand automated, eliciting information people performed far difficult.
First, information may never expressed explicit form; second, may
hard formalize people lacking background formal logic.
case belief revision, information needed perform subsequent revisions
initial order models. However, eliciting order easy may seem,
shown research similar concept preference (Sandholm & Conen, 2010; Carson &
Louviere, 2011), mention experimental results cognitive psychology (Tversky
& Kahneman, 1983): given background information, majority
participants test reckoned Linda bank teller active feminist movement
likely Linda bank teller, probability theory forbids b
likely a.
Furthermore, providing plausibility information additional work people
manually performed process far. Instead, Example 1 shows, information
may derived previous history revisions.
Another example data synchronization: SyncML protocol (OMA, 2002) allows
synchronizing data (phonebook, calendar notes, etc.) mobile phone computer, conflicts may arise; implementation may ask user do,
take decision (like phone always wins) may however later manually reversed
user. Either way, result tells conflict solved. Again,
revising formula resulting knowledge base given, used
derive information unknown plausibility order.
Example 1 shows,
knowledge revision sequence
[K0 , P1 , K1 , P2 , K2 , . . . , Pn , Kn ] information initial plausibility models
derived. information depends revision semantics, sequences
generated semantics.
Example 3 Let [K0 , P1 , K1 , P2 , K2 , P3 , K3 ] revision sequence defined follows.
K0 =
P1 = b
K1 = b
P2 = c
K2 = b c
P3 =
K3 =
shown exists initial order models generates sequence
using natural revision semantics. contrast, order generates restrained
lexicographic semantics.
289

fiLiberatore

technical results provided article are: first, equivalent formulations
problem establishing existence order generating revision sequence using
natural, lexicographical, restrained reinforcement revision; second, initial order
built one exists; third, complexity characterization.
Since number models exponential number variables, exists
order models . . . quantification data structure may
exponential size. result, brute-force search takes double exponential time.
equivalent formulations avoid high computational cost recasting problem
terms polynomial-size data structures.
problem establishing existence initial order generating given sequence
coNP-complete cases one (reinforcement revision unbounded sequence
length), therefore showing problem expressed validity QBF.
proves problem recast form contain existential
quantification initial order.

2. Preliminaries
Belief revision considered article propositional formulae built finite
alphabet variables. truth evaluation alphabet called model: function
variables either true false. Following common terminology propositional
logic, model satisfies formula model formula, formula
model. set models formula F denoted Mod(F ). QBF
propositional formula variables quantified, either universally (like a.a
b), existentially (like ab.a b) (like ab.a b). variables
universally quantified formula QBF .
revision sequence represents evolution beliefs time, including
revising formulae results.
Definition 1 revision sequence odd sequence consistent propositional formulae
[K0 , P1 , K1 , . . . , Pn , Kn ] finite set variables.
semantics belief revision considered article work ordering
models, representing relative plausibility, modified new information
arrives. orderings defined follows.
Definition 2 total preorder C partition models finite sequence classes
[C(0), C(1), C(2), . . . , C(m)] C(0) 6= .
Intuitively, partition represents way compare models: J compare
class, compares greater J class higher
index. use partitions instead usual notation J simplifies definitions
proofs. Since classes empty (except first), several partitions may represent
way comparing models. problem total preorders never checked
equality article.
total preorder depicted shelf, Figure 1. bottom drawer C(0)
contains plausible models. represent situations currently believed possible: C(0) = Mod(K0 ). Revising C P1 changes new preorder CP1 takes
290

fiRevision History

account new information. class CP1 (0) contains models considered
plausible; therefore, CP1 (0) = Mod(K1 ).

C(7)
C(6)
C(5)
C(4)
C(3)
C(2)
C(1)
C(0)

Figure 1: graphical representation total preorder C
partition formalizes plausibility models: models C(i) plausible
models C(i + 1). lower class, plausible model; reason,
total preorder often seen representing implausibility rather plausibility.
inverse ordinal conditional function (Spohn, 1988): (I) = n C(n).
study two semantics considered article involves prefixes
maxsets sequence. Sequences denoted using brackets [. . .]. Given sequence
formulae [P1 , . . . , Pn ], h-prefix sequence containing first h 1 formulae
sequence. maxset sequence extends concept maximal consistent subsets
sets sequences.

maxset([P1 , . . . , Pn ]) = maxset(; [P1 , . . . , Pn ])


maxset([Q1 , . . . , Qi , P1 ]; [P2 , . . . , Pn ])


Q1 Qi P1 consistent
maxset([Q1 , . . . , Qi ]; [P1 , P2 , . . . , Pn ]) =

maxset([Q1 , . . . , Qi , true], [P2 , . . . , Pn ])



otherwise
maxset([Q1 , . . . , Qn ]; ) = [Q1 , . . . , Qn ]
sequence formulae used context propositional formula expected
implicitly represents conjunctions formulae. example, [b, c, c d] means
(b c (c d)). According notation, Q1 Qi P1 inconsistent
[Q1 , . . . , Qi ] |= P1 . result, [Q1 , . . . , Qi , true] replaced [Q1 , . . . , Qi , P1 ]
definition maxset.
definition prefixes maxsets, commute: h-prefix maxset
sequence maxset h-prefix sequence. result, P
h-th element sequence P maxset(S) P maxset(prefixh (S))
consistent.
maxset often written maxset(P1 , . . . , Pn ) shorthand maxset([P1 , . . . , Pn ]).
number properties maxsets shown. proofs appendix.
291

fiLiberatore

Lemma 1 F consistent F |= maxset(P1 , . . . , Pn ), maxset(P1 , . . . , Pn )
{Pi | 1 n F |= Pi }.
Lemma 2 F consistent F |= maxset(P1 , . . . , Pn ), every consistent subset
{P1 , . . . , Pn } contains formulae entailed F equivalent maxset(P1 , . . . , Pn ).
article, sequence formulae replaced true called
subsequence. similar usual definition, difference formulae
maintain position sequence.
Lemma 3 F consistent F 6|= maxset(P1 , . . . , Pn ) exists
subsequence R [P1 , . . . , Pn ] that:
1. R consistent;
2. F |= Pi Pi R;
3. i, Pi 6 R Pi prefixi (R) consistent.
conditions lemma existential type: exists R, exists
model R, either F 6|= Pi Pi R Pi prefixi (R) consistent. proves
checking F 6|= maxset(P1 , . . . , Pn ) expressed validity QBF,
therefore NP.
Corollary 1 F consistent, checking F 6|= maxset(P1 , . . . , Pn ) NP.
lemma avoids constructing maxset one formula time replacing test
satisfiability Pi prefixi (maxset(P1 , . . . , Pn )) F |= Pi , F
consistent entails maxset. way, sequence satisfiability checks required
build maxset parallelized, is, turned number validity checks
performed parallel.
order check F maxset(P1 , . . . , Pn ), one first checks whether F |=
maxset(P1 , . . . , Pn ), maxset(P1 , . . . , Pn ) |= F . Assuming first condition
true, second shown coNP.
Theorem 1 F consistent, checking F maxset(P1 , . . . , Pn ) coNP.
article, revisions satisfying AGM postulate 4 considered: K P
K P K P consistent. Also, formulae Pi Ki sequences assumed
consistent. checking sequence generated total preorder, Ki1 Pi
consistent Ki Ki1 Pi . particular, sequence generated
total preorder Ki1 Pi consistent AGM postulate Ki1 Pi equivalent
Ki ; conversely, Ki Ki1 Pi consistent consistency Ki implies
Ki1 Pi . property important allows replacing satisfiability test
unsatisfiability test.
292

fiRevision History



P

P

-

Figure 2: Natural revision

3. Natural Revision
Natural revision (Boutilier, 1996) modifies total preorder plausibility models C
light new piece information P new total preorder CP close possible
original one. new preorder P true plausible models,
CP (0). minimal change C ensuring setting CP (0) minimal models
P according C, leaving rest preorder unaltered.
Definition 3 natural revision total preorder C formula P defined
total preorder CP follows, minimal index C(i) Mod(P ) 6= :
(

CP (j) =

C(i) Mod(P ) j = 0
C(j 1)\CP (0) otherwise

example, CP (0) = C(i) Mod(P ), CP (1) = C(1 1)\CP (0) = C(0)\CP (0).
Graphically, change P produces preorder natural revision depicted
cutting lowest models P placing others, shown
Figure 2.
CP1 ,...,Pi result revising C P1 , P2 , etc. using natural revision,
Mod(Ki ) = CP1 ,...,Pi (0) revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ].
Example 4 Let C = [C(0), C(1)] total preorder that:

C(0) = Mod(a)
C(1) = Mod(a)
Revising P1 = b, P2 = c P3 = using natural revision generates revision
sequence Example 3. Indeed, revising C P1 = b makes minimal models P1
form new class zero. Since C(0) Mod(P ) 6= , index definition natural
revision zero. resulting preorder therefore:
293

fiLiberatore

CP1 (0) = C(0) Mod(P1 ) = Mod(a b)
CP1 (1) = C(0)\CP1 (0) = Mod(a b)
CP1 (2) = C(1)\CP1 (0) = Mod(a)
Since Mod(K1 ) = CP1 (0), follows K1 b. similar change happens
revising P2 = c, since CP (0) Mod(P2 ) 6= , implies = 0.
CP1 P2 (0) = Mod(a b c)
CP1 P2 (1) = Mod(a b c)
CP1 P2 (2) = Mod(a b)
CP1 P2 (3) = Mod(a)
Again, Mod(K2 ) = CP1 P2 (0), implies K2 b c. minimal models
P3 = entire class CP1 P2 (3). Therefore, = 3 preorder becomes:
CP1 P2 P3 (0) = Mod(a)
CP1 P2 P3 (1) = Mod(a b c)
CP1 P2 P3 (2) = Mod(a b c)
CP1 P2 P3 (3) = Mod(a b)
proves K3 a. revision sequence coincides Example 3.
Looking example direction, shows revision sequence [a, b,
b, c, b c, a, a] generated natural revision preorder. proved
case restrained lexicographic revisions.
aim article establish whether sequence generated preorder,
finding it. Unfortunately, direct search space total preorders unfeasible:
number models exponential number variables, number
total preorders therefore double exponential. Fortunately, natural revision
difficulty overcome thanks necessary sufficient condition sequence
generated total preorder. number lemmas needed prove it. first
shows revising formula alter relative order models
resulting knowledge base.
Lemma 4 CP (0) Mod(F ) = CP compares models F C does, CP
natural revision total preorder C formula P .
result iterated number revising formulae: resulting knowledge
bases Ki inconsistent formula F , relative order models F
changed. result final revision F therefore calculated original
ordering, case. following lemma formulated fragment revision
sequence later applied.
294

fiRevision History

Lemma 5 Let [Kj , Pj+1 , . . . , Pi , Ki ] revision sequence generated natural revision
total preorder C. Kj Pi consistent none Kj+1 Pi , . . . , Ki1 Pi is,
CPj+1 ,...,Pi (0) = Mod(Kj Pi ).
lemma similar result Boutilier (1996, Thm. 17), lifts assumption
conjunctions Kj Pj+1 , . . . , Ki2 Pi1 consistent. shows Pi
consistent previous Kj , natural revision Pi produces result
determined Kj only, independent initial preorder. following lemma covers
case, Pi inconsistent previous Kj .
Lemma 6 revision sequence [K0 , P1 , K1 , . . . , Pi , Ki ] generated natural revision
total preorder C Pi inconsistent K0 , . . . , Ki1 , models
Ki minimal models Pi according C.
last two lemmas prove that, natural revision, Ki equivalent Kj Pi
maximal j conjunction consistent one exists, otherwise determined
initial preorder. first necessary condition existence total
preorder generating sequence.
Definition 4 revision sequence [K0 , P1 , . . . , Pn , Kn ] natural-compatible if, every
{1, . . . , n}, holds:
1. Ki |= Pi ;
2. j maximal index j < Kj Pi consistent (if any),
Ki Kj Pi .
sequence natural-compatible generated natural revision
initial total preorder.
Theorem 2 [K0 , P1 , K1 , . . . , Pn , Kn ] natural-compatible generated natural
revision initial preorder C = [C(0), . . . , C(n + 1)].


Mod(Ki )

n l < . Kl Pi |=

otherwise, n
C(i) =

S{Mod(K ) | l < j . K P 6|= } = n + 1
j
j
l
Natural-compatibility sufficient condition sequence generated
natural revision initial preorder. following theorem proves also
necessary. Therefore, characterizes exactly revision sequences natural revision
generates.
Theorem 3 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated natural revision
initial total preorder natural-compatible.
following example illustrates natural compatibility application natural
revision.
295

fiLiberatore

Example 5 revision sequence previous example natural-compatible. first
condition natural compatibility satisfied: K1 = b implies P1 = b; K2 = b c
implies P2 = c; K3 = implies P3 = a.
last preceding formula Kj consistent P1 = b K0 = a, indeed K1 =
K0 P1 = b. last preceding formula Kj consistent P2 = c K1 = b,
indeed K2 = K1 P2 = b c. Finally, P3 = consistent none K0 , K1 , K2 .
Therefore, second condition natural compatibility places constraint it.
Theorem 2 proves revision sequence generated natural revision
preorder, also provides one: C(0) = Mod(a), C(1) = Mod(a).
indeed preorder used previous example generate sequence.
Natural compatibility rewritten number satisfiability unsatisfiability
tests. particular, j maximal index property written as:
Kj Pi consistent Kh Pi not, j < h < i. way, satisfiability check
dependent another, problem solved two parallel calls NP oracle,
one positive one negative. assumption formulae revision sequences
consistent allows rewriting first.
Lemma 7 Checking existence total preorder C generating revision sequence
[K0 , P1 , K1 , . . . , Pn , Kn ] using natural revision coNP.
problem also hard coNP. Therefore, coNP complete.
Theorem 4 problem establishing existence preorder generating revision
sequence using natural revision coNP complete.

4. Restrained Revision
Restrained revision (Booth & Meyer, 2006) common natural revision
revising total preorder C formula P , minimal models P becomes new class
zero. addition, every class split two according satisfaction P : models
P go lower class, others higher.
Equivalently, every class refined (Papini, 2001) classes 2i 2i + 1,
first class contains models class satisfying P second models class
satisfying P ; then, natural revision applied.
Definition 5 restrained revision total preorder C formula P defined
total preorder CP follows, minimal index C(i) Mod(P ) 6=
/ denotes quotient (integer division, truncated):


C(i) Mod(P )

CP (j) =

j = 0
(C((j 1)/2)\CP (0)) Mod(P ) j > 0 odd

(C((j 1)/2)\C (0))\Mod(P )
otherwise
P

example,
CP (0) = C(i) Mod(P ),
CP (1) = (C((1 1)/2)\CP (0))
Mod(P ) = (C(0)\CP (0)) Mod(P ) CP (2) = (C((2 1)/2)\CP (0))\Mod(P ) =
(C(0)\CP (0))\Mod(P ) since (2 1)/2 = 1/2 = 0 using integer division.
296

fiRevision History

-

P



-

-

Figure 3: Restrained revision
graphical example application restrained revision total preorder
Figure 3.
revision sequence Example 3 generated preorder using restrained
revision. proved using necessary sufficient condition existence
preorder generating sequence. now, illustrate restrained revision works,
preorder shown natural revision used.
Example 6 Let C following total preorder:
C(0) = Mod(a)
C(1) = Mod(a)
Restrained revision P1 = b, P2 = c P3 = generates revision sequence
different Example 3. Since K0 = Mod(C(0)), follows K0 a. Revising C
P1 = b makes minimal models P1 new class zero splits every
class b/b. resulting total preorder removing empty classes therefore:
CP1 (0) = Mod(a b)
CP1 (1) = Mod(a b)
CP1 (2) = Mod(a b)
CP1 (3) = Mod(a b)
Since Mod(K1 ) = CP1 (0), follows K1 b. similar change happens
revising P2 = c:
297

fiLiberatore

CP1 P2 (0) = Mod(a b c)
CP1 P2 (1) = Mod(a b c)
CP1 P2 (2) = Mod(a b c)
CP1 P2 (3) = Mod(a b c)
CP1 P2 (4) = Mod(a b c)
CP1 P2 (5) = Mod(a b c)
CP1 P2 (6) = Mod(a b c)
CP1 P2 (7) = Mod(a b c)
Again, Mod(K2 ) = CP1 P2 (0), implies K2 b c. minimal models
P3 = whole class CP1 P2 (4). preorder therefore becomes:
CP1 P2 P3 (0) = Mod(a b c)
CP1 P2 P3 (1) = Mod(a b c)
CP1 P2 P3 (2) = Mod(a b c)
CP1 P2 P3 (3) = Mod(a b c)
CP1 P2 P3 (4) = Mod(a b c)
CP1 P2 P3 (5) = Mod(a b c)
CP1 P2 P3 (6) = Mod(a b c)
CP1 P2 P3 (7) = Mod(a b c)
result, K3 b c. revision sequence coincides Example 3
P3 K3 different, K3 = previous example. shown
preorder generates sequence using restrained revision.
following property similar Lemma 5 natural revision, difference
maxset introduces account class split.
Lemma 8 Let [Kj , Pj+1 , . . . , Pi , Ki ] revision sequence generated restrained revision
initial total preorder C. Kj Pi consistent none Kj+1 Pi , . . . ,
Ki1 Pi is, CPj+1 ,...,Pi (0) = Mod(maxset(Kj Pi , Pj+1 , . . . , Pi1 )).
result first half necessary sufficient condition sequence
generated restrained revision preorder, involves following definition.
Definition 6 revision sequence [K0 , P1 , K1 . . . , Pn , Kn ] restrained-compatible if,
every {1, . . . , n}, holds:
1. Ki |= Pi ;
2. Ki maxset(Kj Pi , Pj+1 , . . . , Pii ) j maximal index j <
Kj Pi consistent, any;
298

fiRevision History

3. either Ki |= Pl Ki |= Pl every l < j exists.
side remark, condition either Ki |= Pl Ki |= Pl third point
definition refers indexes l < i, including ones Kl consistent
previous Kj .
revision sequence restrained-compatible generated restrained revision
initial preorder. following lemma specifies one is.
Lemma 9 [K0 , P1 , K1 , . . . , Pn , Kn ] restrained-compatible generated restrained revision total preorder C = [C(0), . . . , C(n + 1)], where:


Mod(Ki )

C(i) =

n l < . Pi Kl |=
otherwise, n

S{Mod(K ) | l < j P K 6|= } = n + 1
j
j
l


results proved far collected equivalent formulation existence
preorder generating sequence.
Theorem 5 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated restrained revision
initial total preorder restrained-compatible.
sequence Example 3 shown restrained-compatible. Therefore,
generated restrained revision preorder worked natural
revision, generated other.
Example 7 revision sequence [K0 , P1 , K1 , P2 , K2 , P3 , K3 ] K0 = a, P1 = b, K1 =
b, P2 = c, K2 = b c, P3 = a, K3 = restrained-compatible (this
sequence Example 3). Indeed, P3 = inconsistent K0 , K1 K2 ,
yet K3 = entails neither P1 = b P1 = b, thereby violating third condition
restrained compatibility.
definition restrained compatibility involves consistency entailment checks.
following lemma rewrites form shown use inconsistencies.
Lemma 10 revision sequence [K0 , P1 , K1 . . . , Pn , Kn ] restrained-compatible
if, every index i:
1. Ki |= Pi ;
2. every 0 j < i, either Kj Pi inconsistent Ki |= Kl Pi j < l <
Ki maxset(Kj Pi , Pj+1 , . . . , Pi1 );
3. every 0 j < i, Ki |= Pj , Ki |= Pj , Ki |= Kl Pi 0 l < i.
Since checking equivalence consistent formula maxset problem coNP
Theorem 1, expressed universally quantified formula. result,
quantifiers conditions lemma universal, whole problem coNP.
Hardness class easy prove.
Theorem 6 Establishing existence total preorder generating restrained revision
sequence coNP-complete.
299

fiLiberatore

5. Intermezzo: Multiple Preorders
revision sequence may generated one total preorder. Examples
exist even two variables only, sequence [K0 , P1 , K1 , P2 , K2 ] K0 = ab,
P1 = b, K1 = b, P2 = b K2 = b.
ab

ab

b b b
According Lemma 9, sequence generated restrained revision initial
preorder C = [C(0), C(1), C(2), C(3)].
C(0) = Mod(a b)
C(1) = Mod(a b)
C(2) = Mod(a b)
C(3) = Mod(a b)
However, preorder generating sequence. Classes C(1) C(2)
swapped, still leading result.
C(0) = Mod(a b)
C(1) = Mod(a b)
C(2) = Mod(a b)
C(4) = Mod(a b)
Intuitively, revising single-model formula always single possible outcome:
formula itself. result, even Mod(K1 ) contained class greater
Mod(K2 ), still minimal models P1 Mod(K1 ).
way, expected: revision performed single possible
way, initial preorder irrelevant. intuition holds considered revisions,
confirmed simplifying preorder: indeed, apart C(0), classes
shuffled every possible way, even merged:
C(0) = Mod(a b)
C(1) = Mod(a b) Mod(a b) Mod(a b)
kind preorder works whenever Pi single model, general.
Even restricting Pi Pi Kj |= j < i, l < Pi Kl
guaranteed inconsistent, Pl Ki not. words, Mod(Ki ) cannot always
swapped merged Mod(Kl ).
one preorder possible, sensible principle choose least discriminating one. preorder would compare different J strictly necessary
obtain revision sequence. words, carry plausibility information
follow revision sequence. minimization similar spirit
300

fiRevision History

way rational closure conditional logic rationally rooted consequence
relation (Lehmann & Magidor, 1992; Booth & Nittka, 2008). last preorder shown
obeys principle, question whether least discriminating preorder exists
revision sequences considered revision semantics open problem.
different possible solution proceed refutation:
sequence
[K0 , P1 , K2 , . . . , Pn , Kn ] generated revision preorder
[K0 , P1 , K2 , . . . , Pn , Kn , Pn+1 , Kn+1 ] not, Kn+1 might considered true
revising Pn+1 .
correctly pointed one reviewers, revision sequence may even
generated different revisions different initial preorder, adding second dimension
problem: preorder, also semantics revision chosen.
sequence shown example one kind: generated four revision
semantics considered article every initial preorder C C(0) = Mod(a b).

6. Lexicographic Revision
seminal work iterated revision, Spohn (1988) defined tentative semantics based
principle newer formulae plausible older ones levels
plausibility: even unlikely models P preferred likely
P . spite apparent drawbacks pointed author, semantics
later recognized principled way perform iterated revision (Nayak, 1994; Darwiche &
Pearl, 1997; Booth & Meyer, 2006; Jin & Thielscher, 2007; Konieczny & Pino Perez, 2000).
Like revisions used article, lexicographic revision works total
preorder plausibility models C. particular, revision P changes moving
models P classes index lower others.
Definition 7 lexicographic revision total preorder C formula P defined
following total preorder, j respectively indexes lowest
highest classes containing models P :
(

CP (k) =

C(k + i) Mod(P )
k j
C(k j + 1)\Mod(P ) otherwise

new class zero CP (0) = C(0 + i) Mod(P ); expected, comprises minimal
models P , since C(i) lowest class containing models P . class CP (j i) =
C(j + i) Mod(P ) = C(j) Mod(P ) contains highest-class models P , since
assumption C(j). models C(0) satisfy P , any, moved
class CP (j + 1) = C(j + 1 j + 1)\Mod(P ) = C(0)\Mod(P ). index
j + 1 lower classes CP (0), . . . , CP (j i) contain models P coming
C(i), . . . , C(j).
Figure 4 shows total preorder C changed formula P using lexicographic
revision.
Graphically, lexicographic revision cuts models P classes
wedges shelf. way, every model P belongs lower class
models P . time, relative position two models P changed,
holds every two models P .
301

fiLiberatore

P


-

P

Figure 4: Lexicographic revision
Example 8 shown sequence Example 3 generated lexicographic
revision total preorder. Meanwhile, illustrate definition lexicographic
revision preorder used example natural revision revised P1 = b, P2 = c
P3 = a.
C(0) = Mod(a)
C(1) = Mod(a)
Revising C P1 = b using lexicographic revision removes models P1
classes creates new classes bottom:
CP1 (0) = Mod(a b)
CP1 (1) = Mod(a b)
CP1 (2) = Mod(a b)
CP1 (3) = Mod(a b)
result, K1 = b. Revising P2 = c similar effect:
CP1 P2 (0) = Mod(a b c)
CP1 P2 (1) = Mod(a b c)
CP1 P2 (2) = Mod(a b c)
CP1 P2 (3) = Mod(a b c)
CP1 P2 (4) = Mod(a b c)
302

fiRevision History

CP1 P2 (5) = Mod(a b c)
CP1 P2 (6) = Mod(a b c)
CP1 P2 (7) = Mod(a b c)

preorder produces K2 = b c. Finally, revising P3 = removing
empty classes makes classes 1, 3, 5, 7 become new classes 0, 1, 2, 3.
CP1 P2 P3 (0) = Mod(a b c)
CP1 P2 P3 (1) = Mod(a b c)
CP1 P2 P3 (2) = Mod(a b c)
CP1 P2 P3 (3) = Mod(a b c)
CP1 P2 P3 (4) = Mod(a b c)
CP1 P2 P3 (5) = Mod(a b c)
CP1 P2 P3 (6) = Mod(a b c)
CP1 P2 P3 (7) = Mod(a b c)

Since K3 = b c equivalent a, revision sequence Example 3
generated lexicographic revision initial preorder C. shown
preorder generates sequence using lexicographic revision.
every preorder C consistent formula P , revised preorder CP using lexicographic revision three properties:
1. CP (0) set minimal models P preorder C;
2. exists index h



i=0,...,h CP (i)

= Mod(P );

3. two models satisfy P falsify it, CP compares C does.
Lexicographic revision recast terms maxsets reversed sequence.
Booth Nittka (2008) proved following property; precisely, proved
implies following property arbitrary preorder [C(0), . . . , C(n)] obtained
revising ordering models class zero sequence formulae
C(n), . . . , C(0) sets models.
Property 1 [K0 , P1 , . . . , Kn , Pn ] revision sequence generated lexicographic revision total preorder C, Mod(Ki ) set minimal models
maxset(Pi , . . . , P1 ) according C.
property following consequences:
1. models Ki class C;
303

fiLiberatore

2. models maxset(Pi , . . . , P1 ) greater classes.
Two properties follow. First, Ki Kj consistent models Mod(Ki )
Mod(Kj ) class C. Indeed, since Ki Kj consistent, model
I; since models Ki class I, Kj , models
two formulae class. Second, Kj models common
Ki maxset(Pi , . . . , P1 ), models class C, greater
models Ki .
allows shifting total preorder among models total preorder among
formulae Ki . Since preorder formulae Ki , results
revision process, called result preorder.
Definition 8 result preorder revision sequence [K0 , P1 , . . . , Pn , Kn ] total
preorder among formulae Ki that:
1. Ki Kj consistent Ki Kj class;
2. Ki Kj maxset(Pi , . . . , P1 ) consistent Ki lower class Kj .
advantage result preorders built revision sequence,
shown. Before, proved existence result preorder
existence initial total preorder models generating sequence
lexicographic revision. Since two preorders involved (one among models, one among
formulae), distinction made preorder among models C preorder among
formulae R; result preorders second kind.
Lemma 11 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated lexicographic
revision total preorder among models C result preorder R defined
by:
R(i) = {Ki | Mod(Ki ) C(i)}
converse also holds: result preorder revision sequence one derive
preorder among models generates sequence using lexicographic revision.
Lemma 12 R result preorder revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ]
Ki |= maxset(Pi , . . . , P1 ) every i, lexicographic revision following
total preorder C among models generates revision sequence, z index
greatest class R:
(

C(h) =

{Mod(Ki ) | Ki R(h)} h z
{I | 6 K1 Kn }
h = z + 1

two lemmas together imply following corollary.
Corollary 2 revision sequence [K0 , P1 , . . . , Pn , Kn ] generated lexicographic revision
initial preorder among models result preorder sequence
exists Ki |= maxset(Pi , . . . , P1 ) every i.
304

fiRevision History

condition existence result preorder already simpler existence
initial total preorder models, since number formulae Ki linear size
revision sequence number models may exponential. existence
result preorder recast terms condition revision sequence.
Definition 9 revision sequence [K0 , P1 , . . . , Kn , Pn ] lexicographic compatible Ki |=
maxset(Pi , . . . , P1 ) every relations ' < defined follows form
cycles containing < links:
Ki ' Kj Ki Kj consistent;
Ki < Kj Ki Kj maxset(Pi , . . . , P1 ) consistent.
rationale definition Ki ' Kj Ki Kj
class R, Ki < Kj Ki lower class. intuition
confirmed following lemma.
Lemma 13 revision sequence generated lexicographic revision total preorder lexicographic compatible.
result applied running example.
Example 9 revision sequence presented Example 3 lexicographic compatible.
Indeed, maxset(P3 , P2 , P1 ) = maxset(a, c, b) = c b, entailed K3 =
a. result, revision sequence generated lexicographic revision
preorder.
Given revision sequence, one determine consistency Ki Kj Ki Kj
maxset(Pi , . . . , P1 ) every pair formulae Ki Kj . problem non-existence
preorder generating sequence turned existence cycles,
computationally easy (polynomial size revision sequence). hard part
checking consistency. Since problem polynomial NP oracle available (which
turns consistency checks constant-time operations), problem p2 . However,
proved even computationally easier that.
Lemma 14 revision sequence [K0 , P1 , K1 . . . , Pn , Kn ] lexicographic compatible
either Ki 6|= maxset(Pi , . . . , P1 ) consistent sets R1 , . . . , Rn exist
that:
1. {Pj | 1 j Ki |= Pj } Ri every i;
2. exists cycle Ki1 , . . . , Kim = Ki1 either Kij Kij+1 Kij Kij+1 Ri
consistent ij {i1 , . . . , im1 }, second consistent least one
index.
advantage reformulation lexicographic incompatibility entailment tests contains reformulated terms consistency. means incompatibility NP. Therefore, compatibility coNP. also shown hard
class.
Theorem 7 problem checking existence total preorder generating revision
sequence using lexicographic revision coNP-complete.
305

fiLiberatore

P


P

Figure 5: Reinforcement revision

7. Reinforcement Revision
Reinforcement revision (Jin & Thielscher, 2007) takes input total preorder
revise C revising formula P , also parameter encodes degree
belief P (more precisely, degree disbelief P ). sake simplicity,
restriction case = 1 analyzed.
Definition 10 reinforcement revision total preorder C formula P parameter = 1 following total preorder CP , minimal index
C(i) Mod(P ) 6= .
(

CP (j) =

C(i) Mod(P )
j = 0
C(j 1)\Mod(P ) C(j + i) Mod(P ) j > 0

general definition j instead j 1; article, always 1. two
cases merged single one CP (j) = C(j 1)\Mod(P ) C(j + i) Mod(P )
j 0 assuming C(1) = . example, CP (0) = C(1)\Mod(P ) C(i)
Mod(P ) = C(i) Mod(P ), CP (1) = C(0)\Mod(P ) C(i + 1) Mod(P ).
graphical example revision Figure 5.
behavior revision shown preorder formulae Example 3.
Example 10 Revising preorder C = [C(0), C(1)] C(0) = Mod(a) C(1) =
Mod(a) P1 = b using reinforcement revision effect increasing class
every model P1 one; models P1 decrease class since
already class zero, means = 0 definition CP .
CP1 (0) = Mod(a b)
CP1 (1) = Mod((a b) (a b))
CP1 (2) = Mod(a b)

306

fiRevision History

happens revising P2 = c: every class union original class
conjoined c previous class conjoined c:
CP1 P2 (0) = Mod(a b c)
CP1 P2 (1) = Mod((a b c) (a b c) (a b c))
CP1 P2 (2) = Mod((a b c) (a b c) (a b c))
CP1 P2 (3) = Mod(a b c)
minimal class containing models P3 = CP1 P2 (1). Therefore, models
decreased one class. Models P3 = increased one class, usual:
CP1 P2 P3 (0) = Mod((a b c))
CP1 P2 P3 (1) = Mod((a b c) (a b c) (a b c))
CP1 P2 P3 (2) = Mod((a b c) (a b c) (a b c))
CP1 P2 P3 (3) = Mod(a b c)
result, K3 = b c.
revision semantics, every Ki assumed consistent; case inconsistent Ki degenerated, preorder produces knowledge base.
Given fixed revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated reinforcement revision initial total preorder C, lowest class index models Pi
ordering CP1 ,...,Pi denoted DC (i). Formally:
DC (i) = min{j | CP1 ,...,Pi1 (j) Mod(Pi ) 6= }
Revising Pi shifts models Pi DC (i) classes raises models
one class. result, DC (1), . . . , DC (n) tell model moved every
step, allowing determine model moved step step j.
Definition 11 movement j according DC revision sequence
[K0 , P1 , K1 , . . . , Pn , Kn ] MDC (I, i, j) where:
MDC (I, i, i) = 0;
MDC (I, i, + 1) = DC (i + 1) |= Pi+1 MDC (I, i, + 1) = 1 otherwise;
j > MDC (I, i, j) =

P

l=i,...,j1 MDC (I, l, l

+ 1);

j < MDC (I, i, j) = MDC (I, j, i).
Since DC (i) minimal class models Pi step 1 according initial preorder, MDC (I, i, j) change classes j using
preorder. initial preorder C affects definition MDC (I, i, j) indirectly, via
sequence DC = [DC (1), . . . , DC (n)]. result, MV (I, i, j) defined
arbitrary sequence n numbers V = [V (1), . . . , V (n)].
307

fiLiberatore

Lemma 15 every sequence n numbers V = [V (1), . . . , V (n)], holds MV (I, i, j) =
MV (I, i, h) + MV (I, h, j) every three indexes i, j h.
lemma holds even h j.
Since MDC (I, i, j) defined change class model step
step j, particular case |= Ki class step j, since
step zero.
Lemma 16 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated reinforcement revision total preorder C DC = [DC (1), . . . , DC (n)] DC (i) =
min{j | CP1 ,...,Pi1 (j) Mod(Pi ) 6= } |= Ki then, every j:
MDC (I, i, j) = 0 |= Kj ;
MDC (I, i, j) > 0 otherwise.
lemma reversed, sense sequence values property
allows determine preorder generating sequence.
Definition 12 sequence nonnegative integer values V = [V (1), . . . , V (n)] reinforcement mover revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] if, every j,
|= Ki then:
MV (I, i, j) = 0 |= Kj ;
MV (I, i, j) > 0 otherwise.
previous lemma therefore recast as: sequence generated reinforcement revision total preorder C, reinforcement mover: DC .
converse also holds: reinforcement mover one determine total preorder generating revision sequence.
Lemma 17 V = [V (1), . . . , V (n)] reinforcement mover revision sequence
[K0 , P1 , K1 , . . . , Pn , Kn ], following initial preorder C = [C(0), . . . , C(V (1) + + V (n +
1))] generates revision sequence reinforcement revision DC = V .
(

C(j) =

{I | |= Ki MV (I, i, 0) = j} j < V (1) + . . . + V (n) + 1
{I | . 6|= Ki }
j = V (1) + . . . + V (n) + 1

contrast condition compatibility revision semantics, one
explicitly require Ki |= Pi . however implied: 6|= Pi MV (I, 1, i) = 1
definition movement (Definition 11) |= Ki MV (I, i, j) 0 every j
definition reinforcement mover (Definition 12). particular case j = 1
MV (I, i, j) = MV (I, i, 1) = MV (I, 1, i) = 1, greater
equal zero. Therefore, reinforcement mover exists Ki 6|= Pi .
lemma allows checking existence preorder generating sequence
guessing V (1), . . . , V (n) checking class every model satisfies least
Ki . However, membership polynomial hierarchy follows values V (i)
representable polynomial space, possible values bounded
exponential size sequence.
308

fiRevision History

Lemma 18 reinforcement revision generates revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ]
total preorder, also generates sequence preorder
minimal initial class models P1 0 1.
lemma proves sequence generated preorder also generated
preorder C DC (1) either 0 1. particular, K0 P1 consistent
DC (1) = 0, otherwise DC (1) = 1. base case recursive proof giving bound
size DC (i). Intuitively, done lowering models Pi
number classes initial preorder; revising Pi moved together
minimal ones class zero, resulting ordering obtained
original one. lowering cannot however large models enter
class zero previous step j satisfy Kj .
Lemma 19 reinforcement revision generates sequence [K0 , P1 , K1 , . . . , Pn , Kn ]
total preorder, also generates sequence total preorder C
DC (i + 1) DC (1) + . . . + DC (i) + + 1.
lemma seen inductive part proof, previous one
base case. lead following conclusion.
Lemma 20 Reinforcement revision generates revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ]
total preorder generates sequence total preorder
DC (i) bounded 2i 1.
values DC (1), . . . , DC (n) bounded exponential value n,
also lower bound size sequence [K0 , P1 , . . . , Pn , Kn ]. Therefore, DC (i)
represented space polynomial size sequence.
Theorem 8 Establishing existence total preorder generating revision sequence
[K0 , P1 , . . . , Pn , Kn ] reinforcement revision p2 , coNP n constant.
case constant-length sequences, hardness easy prove.
Theorem 9 Checking existence preorder generating reinforcement revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] coNP-complete, n constant.

8. Conclusions
Belief revision employs plausibility orders revise knowledge base, obtain
orders largely neglected. solution proposed studied article
assume knowledge previous revisions, reversing obtain initial order,
used revisions. method similar deriving order
hypothetical revisions like K P |= Q, K R |= , etc., assumes knowledge
would happen revising formula P , R, etc. approach considered
article results iteratively incorporating series new formulae given, like
nested counterfactuals (Eiter & Gottlob, 1996). Booth Nittka (2008) considered
problem deriving facts holding time points partial information expressed
309

fiLiberatore

terms positive negative conditions (i.e., formulae hold others
hold certain time points), done constructing initial ordering.
study focuses lexicographic revision only, allows partial knowledge
revisions results, includes choice initial ordering among possible ones.
Rather entailment information like K P R |= Q, article history revisions
resulting knowledge bases assumed fully known. could actual results
manually performed changes, may already available.
analysis shown simple equivalent conditions existence ordering generating given series revisions results natural (Boutilier, 1996), restrained (Booth
& Meyer, 2006), lexicographic (Spohn, 1988; Nayak, 1994) reinforcement revisions (Jin
& Thielscher, 2007). conditions allow construct initial ordering one exists.
Using equivalent conditions, complexity establishing existence orderings generating sequence established considered semantics. Surprisingly,
turned relatively simple: coNP complete cases one (reinforcement
revision unbounded sequence length). checking propositional entailment, means checking generability increase complexity
base language propositional logic.
shown Section 5, revision sequence may generated one
ordering. leads question whether one may considered
natural sequence. example less J ordering another,
second may seen cautious, end rational: sequence
generated without assuming represents plausible world J,
reason draw conclusion. question whether single least informative
ordering exists every sequence, considered revision semantics, open
problem.
Still open comparison generable sequences various revisions: shown
running example, sequence generated natural revision generated
ordering restrained lexicographic revision. sequence
opposite property? not, natural revision may seen suited explaining
revision sequence generated. hand, may also give less information
initial ordering used generate them; case orderings generating
sequence numerous semantics.
four semantics iterated belief revisions ones defined literature (Williams, 1994; Darwiche & Pearl, 1997; Areces & Becher, 2001; Benferhat et al.,
2004; Konieczny & Pino Perez, 2000; Zhang, 2004); recent survey counted least twenty
seven revision-related operators (Rott, 2009). Natural lexicographic semantics regarded extreme forms revision satisfying Darwiche-Pearl postulates (Darwiche &
Pearl, 1997), minimal maximal hearing respectively given new information. Restrained reinforcement revision considered middle,
also obey conditions (Booth & Meyer, 2006; Jin & Thielscher, 2007). four
considered semantics therefore constitute reasonable spectrum possibilities, others
exist.
require additional information (like strength every revision, Spohn, 1988;
Williams, 1994; Benferhat et al., 2004), others families revisions rather single
ones (Darwiche & Pearl, 1997; Zhang, 2004). These, particular, open interesting
310

fiRevision History

line research: whether revision sequence generated ordering
revision semantics satisfying given set conditions, like Darwiche Pearl (1997)
postulates. words, ordering semantics solution
problem, given revision sequence.
assumption reliability strictly increasing time also gives directions
study. Indeed, principle realized true general (Peppas, 2008)
iterated revision recognized form prioritized merging (Delgrande, Dubois, & Lang,
2006). perspective, giving preference last formula particular case.
case interest, yet searching initial plausibility order also possible
general case.
Yet another open problem combine approach article results
people actually perform revision. Indeed, experimentally proved
human revision suffers number biases (Tversky & Kahneman, 1983; See,
Morrison, Rothman, & Soll, 2011; Wang, Zhang, & Johnson, 2000), anchoring
order effect, excessive preference knowledge acquired early. studies show
revision performed people fully rational, contrary belief revision
formal semantics attempt be. psychological, extra-logical biases kept
account working sequences manually-performed revisions.
8.1 Acknowledgements
author thanks anonymous referees useful suggestions previous
versions article.

Appendix A. Proofs
following sections contain proofs lemmas theorems article.
A.1 Preliminaries: Proofs
Lemma 1 F consistent F |= maxset(P1 , . . . , Pn ), maxset(P1 , . . . , Pn )
{Pi | 1 n F |= Pi }.
Proof. Since F |= maxset(P1 , . . . , Pn ) F implies every Pi maxset.
possibility claim hold F also implies Pi
maxset. Since every element maxset(P1 , . . . , Pn ) implied F , also every element
prefixi (maxset(P1 , . . . , Pn )) is. Since F also entails Pi , every model F satisfies
prefixi (maxset(P1 , . . . , Pn )) Pi . consistency Pi prefixi (maxset(P1 , . . . , Pn ))
contradicts assumption Pi maxset.
Lemma 2 F consistent F |= maxset(P1 , . . . , Pn ), every consistent subset
{P1 , . . . , Pn } contains formulae entailed F equivalent maxset(P1 , . . . , Pn ).
Proof. Lemma 1, since F consistent F |= maxset(P1 , . . . , Pn )
maxset(P1 , . . . , Pn ) {Pi | 1 n F |= Pi }. proves maxset(P1 , . . . , Pn )
contains formulae entailed F .
Let R consistent proper superset
311

fiLiberatore

maxset(P1 , . . . , Pn ). Let Pi lowest-index formula R
maxset(P1 , . . . , Pn ). Since lowest index, R maxset(P1 , . . . , Pn )
formulae among {P1 , . . . , Pi1 }. Since R consistent, intersection set
consistent well. Since R also contains Pi , follows prefixi (P1 , . . . , Pn ) {Pi }
consistent, contradicting assumption Pi maxset(P1 , . . . , Pn ).
Lemma 3 F consistent F 6|= maxset(P1 , . . . , Pn ) exists
subsequence R [P1 , . . . , Pn ] that:
1. R consistent;
2. F |= Pi Pi R;
3. i, Pi 6 R Pi prefixi (R) consistent.
Proof. Two cases considered: first, F entails maxset; second, F not.
first, R proved exists; second, one R shown.
F |= maxset(P1 , . . . , Pn ), F implies elements maxset. result,
second condition true R contains formulae maxset. R also
contains formulae maxset, since formula maxset inconsistent
maxset, R inconsistent. result, R consistent coincides
maxset. contradicts third point, showing every R, first
second conditions true third false.
F 6|= maxset(P1 , . . . , Pn ), three conditions satisfied R containing precisely
formulae Pi entailed F . choice meets first condition F consistent
second construction. third condition proved hold well.
Since F entail maxset F entail formulae maxset.
Let least index formula maxset(P1 , . . . , Pn ) entailed F .
construction, every formula Pj maxset R j < i. shown
converse also holds. contrary, let j < lowest index formula R
maxset.
assumptions j least indexes R maxset differ (in
way other) imply prefixj (R) = prefixj (maxset(P1 , . . . , Pn )). Since assumption
Pj maxset, inconsistent prefixj (maxset(P1 , . . . , Pn )). result,
also inconsistent prefixj (R). Since Pj R, implies inconsistency R,
proved consistent.
contradiction proves R maxset equal index
j < i, i. words, prefixi (R) = prefixi (maxset(P1 , . . . , Pn )).
assumption Pi formula maxset R. maxset
means Pi prefixi (maxset(P1 , . . . , Pn )) consistent. proved,
latter equivalent Pi prefixi (R). concludes proof third condition.
Theorem 1 F consistent, checking F maxset(P1 , . . . , Pn ) coNP.
Proof. F maxset(P1 , . . . , Pn ) holds F |= maxset(P1 , . . . , Pn ) maxset(P1 , . . . , Pn ) |=
F . first condition true, Lemma 1 maxset comprises exactly formulae
312

fiRevision History

entailed F . Therefore, converse maxset(P1 , . . . , Pn ) 6|= F happen
exists model satisfying formulae Pi entailed F F itself. words:
F maxset(P1 , . . . , Pn )
iff F |= maxset(P1 , . . . , Pn ) maxset(P1 , . . . , Pn ) |= F
iff F |= maxset(P1 , . . . , Pn ) (I . 6|= F i(F |= Pi |= Pi ))
iff F |= maxset(P1 , . . . , Pn ) (I . 6|= F i(F 6|= Pi |= Pi ))
iff F |= maxset(P1 , . . . , Pn ) . |= F i(F |= Pi 6|= Pi ))

Lemma 3 reformulates converse first condition F |= maxset(P1 , . . . , Pn ) using
existential quantifiers (the second point equivalent either F 6|= Pi Pi R).
result, first condition expressed using universal quantifiers.
existential quantifier second condition i, replaced
disjunction. Since quantifiers universal, problem coNP.
A.2 Natural Revision: Proofs
Lemma 4 CP (0) Mod(F ) = CP compares models F C does, CP
natural revision total preorder C formula P .
Proof. Let J two models F , l classes. Since models F
CP (0) Mod(F ) = , belong CP (0). result, CP (m + 1) = C(m)\CP (0)
contains CP (l + 1) = C(l)\CP (0) contains J. proves revising C F
increases classes J one each. Therefore, greater equal J
according CP according C.
Lemma 5 Let [Kj , Pj+1 , . . . , Pi , Ki ] revision sequence generated natural revision
total preorder C. Kj Pi consistent none Kj+1 Pi , . . . , Ki1 Pi is,
CPj+1 ,...,Pi (0) = Mod(Kj Pi ).
Proof. Lemma 4, CPj+1 ,...,Pi1 compares models Pi way C does.
result, Mod(Ki ) minimal models Pi C. Since C(0) = Mod(Kj ) Kj Pi
consistent, minimal models C(0) Mod(Pi ) = Mod(Kj Pi ).
Lemma 6 revision sequence [K0 , P1 , K1 , . . . , Pi , Ki ] generated natural revision
total preorder C Pi inconsistent K0 , . . . , Ki1 , models
Ki minimal models Pi according C.
Proof. Lemma 4, revising C P1 , . . . , Pi1 affect order
models Pi . result, minimal models Pi according CP1 ,...,Pi1 minimal
models Pi according C.
313

fiLiberatore

Theorem 2 [K0 , P1 , K1 , . . . , Pn , Kn ] natural-compatible generated natural
revision initial preorder C = [C(0), . . . , C(n + 1)].


Mod(Ki )

C(i) =

n l < . Kl Pi |=
otherwise, n

S{Mod(K ) | l < j . K P 6|= } = n + 1
j
j
l


Proof. Since formula index lower zero, C(0) equal Mod(K0 ) therefore
empty. classes C contain models C(n + 1) comprises every model
C(0), . . . , C(n). prove C total preorder, proved sets
C(i) disjoint. model C(i) n Mod(Ki ). Therefore,
also model Pi . Since Pi inconsistent K0 , . . . , Ki1 , model
C(0), . . . , C(i 1). class C(n + 1) contains exactly models
C(0) C(n). proves C total preorder.
formulae Ki generated different ways depending whether Kj Pi consistent j < i:
index j exists, assumption natural compatibility Ki Kj Pi
maximal j; Lemma 5, exactly result revising C
P1 , . . . , Pi .
otherwise, Pi inconsistent K0 , . . . , Ki1 ; Lemma 6, Mod(Ki )
minimal models Pi initial preorder C. Since Pi inconsistent K0 , . . . , Ki1 , then: first, since C(0), . . . , C(i 1) subsets
Mod(K0 ), . . . , Mod(Ki1 ), classes contain models Pi ; second, C(i)
empty equal Mod(Ki ). result, minimal models Pi exactly
Mod(Ki ).

Theorem 3 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated natural revision
initial total preorder natural-compatible.
Proof. previous theorem shows every natural-compatible sequence generated
natural revision total preorder. converse proved: sequence
natural-compatible generated natural revision preorder.
Since Mod(Ki ) = CP1 ,...,Pi1 ,Pi (0) = CP1 ,...,Pi1 (l) Mod(Pi ) l, holds
Mod(Ki ) Mod(Pi ), Ki |= Pi . Therefore, Ki 6|= Pi preorder
generate revision sequence. Otherwise, natural-compatibility violated if, j
i:
1. Kj Pi consistent;
2. formulae Kj+1 Pi , . . . , Ki1 Pi inconsistent;
3. Kj Pi equivalent Ki .
314

fiRevision History

Lemma 5, first two points imply revising Cj Pj+1 , . . . , Pi generates
formula equivalent Kj Pi , contradicting third point. proves revision
sequence natural-compatible generated natural revision preorder.
Lemma 7 Checking existence total preorder C generating revision sequence
[K0 , P1 , K1 , . . . , Pn , Kn ] using natural revision coNP.
Proof. According Theorem 3, preorder C exists if, every
{1, . . . , n}, holds Ki |= Pi and, j maximal index j < Kj Pi
consistent (if any), Ki Kj Pi . first part, Ki |= Pi i, verified
linear number independent unsatisfiability tests.
second part rewritten as: Kj Pi consistent Kh Pi inconsistent
every h j i, Ki Kj Pi . Rewriting implication disjunction,
every j = 0, . . . , 1:
1. either K0 Pi inconsistent Kh Pi consistent 0 < h < Ki K0 Pi ;

2. either K1 Pi inconsistent Kh Pi consistent 1 < h < i, Ki K1 Pi ;

3. . . . ;
4. either Ki1 Pi inconsistent Ki K0 Pi .
conditions index i. hold every {1, . . . , n}. Since
formulae Ki consistent, conditions simplified: Kj Pi consistent,
either Kh Pi consistent j < h < Ki Kj Pi ; latter case, Ki Kh Pi
one index h (the last). Therefore, condition recast as:
Kj Pi |= Ki Kj+1 Pi . . . Ki Ki1 Pi Ki Kj Pi
condition checked number independent unsatisfiability tests,
therefore coNP.
Theorem 4 problem establishing existence preorder generating revision
sequence using natural revision coNP complete.
Proof. Membership proved previous lemma. Hardness proved reduction
problem checking whether formula G unsatisfiable. instance [K0 , P1 , K2 ]
K0 = a, K1 = P1 = G, new variable, G. G satisfiable,
K0 P1 consistent; therefore, equivalent K1 . Instead, G.
G unsatisfiable, K1 = P1 = inconsistent K0 . Theorem 3, preorder
generating sequence using natural revision exists.

315

fiLiberatore

A.3 Restrained Revision: Proofs
Lemma 8 Let [Kj , Pj+1 , . . . , Pi , Ki ] revision sequence generated restrained revision
initial total preorder C. Kj Pi consistent none Kj+1 Pi , . . . ,
Ki1 Pi is, CPj+1 ,...,Pi (0) = Mod(maxset(Kj Pi , Pj+1 , . . . , Pi1 )).
Proof. Proof induction j. j = + 1 claim CPi (0) = Mod(maxset(Kj
Pi )) holds Kj Pi consistent assumption.
inductive claim CPj+1 ,...,Pi2 ,Pi1 ,Pi (0)
=
Mod(maxset(Kj
Pi , Pj+1 , . . . , Pi2 , Pi1 )), inductive assumption without Pi1 :
CPj+1 ,...,Pi2 ,Pi (0) = Mod(maxset(Kj Pi , Pj+1 , . . . , Pi2 ))
definition, CPj+1 ,...,Pi2 ,Pi (0) CPj+1 ,...,Pi2 (k) Mod(Pi ) k minimal
integer making intersection non-empty. result, CPj+1 ,...,Pi2 (l) Mod(Pi ) =
indexes l 0 l < k. Revising preorder Pi1 changes classes of:
models Ki1 , become class zero;
models class, split according whether satisfy Pi1 .
Since model Ki1 satisfies Pi assumption, CPj+1 ,...,Pi2 ,Pi1 (0) Mod(Pi ) =
holds. None classes CPj+1 ,...,Pi2 index 0 l < k intersect Mod(Pi ); therefore,
neither ones resulting splitting them. result, minimal-index class
intersecting Mod(Pi ) one two resulting splitting CPj+1 ,...,Pi2 (k), are:
CPj+1 ,...,Pi2 (k) Mod(Pi1 )
CPj+1 ,...,Pi2 (k)\Mod(Pi1 )
first intersects Mod(Pi ), Mod(Ki ); otherwise, Mod(Ki ) second.
formulae:
(

Mod(Ki ) =

CPj+1 ,...,Pi2 (k) Mod(Pi1 ) Mod(Pi ) empty
(CPj+1 ,...,Pi2 (k)\Mod(Pi1 )) Mod(Pi ) otherwise

properties set operators, equation rewritten as:
(

Mod(Ki ) =

CPj+1 ,...,Pi2 (k) Mod(Pi ) Mod(Pi1 ) empty
(CPj+1 ,...,Pi2 (k) Mod(Pi ))\Mod(Pi1 ) otherwise

way k defined, CPj+1 ,...,Pi2 (k) Mod(Pi ) equal CPj+1 ,...,Pi2 ,Pi (0).
latter inductive assumption Mod(maxset(Kj Pi , Pj+1 , . . . , Pi2 )). Intersecting
set Mod(Pi1 ) result empty, subtracting Mod(Pi1 ) otherwise
adding Pi1 end sequence, definition maxset
sequence. Since Mod(Ki ) = CPj+1 ,...,Pi2 ,Pi1 ,Pi (0), proves inductive claim
set equal Mod(maxset(Kj Pi , Pj+1 , . . . , Pi2 , Pi1 )).

316

fiRevision History

Lemma 9 [K0 , P1 , K1 , . . . , Pn , Kn ] restrained-compatible generated restrained revision total preorder C = [C(0), . . . , C(n + 1)], where:


Mod(Ki )

C(i) =

n l < . Pi Kl |=
otherwise, n

S{Mod(K ) | l < j P K 6|= } = n + 1
j
j
l


Proof. formula index less zero; therefore, C(0) = Mod(K0 ), empty
formulae sequences consistent assumption. Since C(n + 1) contains
models C(0) C(n), union classes include models, model
C(n+1) also another classes. order prove C partition, shown
model C(i) also C(l) l < n. C(i) n Mod(Ki )
Pi Kl inconsistent l < i. Since Ki |= Pi , also Ki Kl inconsistent.
proves model cannot C(i) also C(l) l < i.
previous lemma shows that, regardless initial total preorder, revision
sequence generated restrained revision Ki maxset(Kj Pi , Pj+1 , . . . , Pii )
j maximal index j < Kj Pi consistent, any. Since restrained
compatibility ensures condition holds, Ki j exists obtained
revision regardless C.
Remains show C generates Ki even Pi Kl inconsistent l < i.
models Pi C(i) = Mod(Ki ) classes greater index. Indeed, model
Pi C(l) l < k Pi Kl would consistent. Revising C Pl ,
l = 1, . . . , 1, changes preorder two ways:
1. models Kl moved class zero;
2. classes split according satisfaction Pl .
Since Pi Kl inconsistent, model Pi moved class zero. result,
relative position models Pi modified second change, splitting
classes. could break class Mod(Ki ) two, case. Indeed, since
either Ki |= Pl Ki |= Pl , either models Ki satisfy Pl falsify Pl .
words, change may alter comparison two models Pi ,
Mod(Ki ).
Two claims therefore proved: models Pi C(i) = Mod(Ki )
greater classes; first 1 revisions minimal models Pi still
Mod(Ki ). proves result i-th revision Ki .
Theorem 5 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated restrained revision
initial total preorder restrained-compatible.
Proof. previous lemma shows every restrained-compatible revision sequence
generated certain initial preorder. Remains therefore prove converse:
revision sequence restrained-compatible generated initial preorder.
Lemma 8 proves every sequence generated restrained revision satisfies condition Ki maxset(Pi Kj , Pj+1 , . . . , Pi1 ) j maximal j <
317

fiLiberatore

Pi Kj consistent. Ki |= Pi holds consequence Mod(Ki ) set minimal models satisfying Pi . Remains therefore prove necessity third condition
restrained compatibility: Pi consistent Kj j < k either Ki |= Pl
Ki |= Pl every l < i.
contrary, let l < Ki 6|= Pl Ki 6|= Pl . two conditions
imply Mod(Ki ) contains models Pl models Pl . Even
models Ki class l 1 revisions, l-th one separates ones
satisfying Pl ones satisfying Pl . may end two consecutive classes,
new class zero another class, either way models Ki placed two
separate classes. Since constrained revision never merges classes, models still
separate classes 1-th revision. result, revising Pi select part
Mod(Ki ) it.
Lemma 10 revision sequence [K0 , P1 , K1 . . . , Pn , Kn ] restrained-compatible
if, every index i:
1. Ki |= Pi ;
2. every 0 j < i, either Kj Pi inconsistent Ki |= Kl Pi j < l <
Ki maxset(Kj Pi , Pj+1 , . . . , Pi1 );
3. every 0 j < i, Ki |= Pj , Ki |= Pj , Ki |= Kl Pi 0 l < i.
Proof. three conditions restrained compatibility rewritten follows,
every index i.
1. Ki |= Pi ;
2. every j 0 j < i, either Kj Pi inconsistent Kl Pi consistent
j < l < Ki maxset(Kj Pi , Pj+1 , . . . , Pi1 );
3. every j 0 j < i, either Ki |= Pj Ki |= Pj Kl Pi consistent
0 l < i.
second third point include Kl Pi consistent h l < i:
first, h = j + 1; second, h = 0. particular conditions lemma,
two conditions shown equivalent:
1. Kl Pi satisfiable l h l < i;
2. Ki |= Kl Pi l h l < i.
proved that: first, sequence restrained-compatible Condition 1 implies
Condition 2; second, three conditions lemma true Condition 2 implies
Condition 1.
Condition 1 Kl Pi consistent h l < i. Either l maximal
index property index h is. Let g maximal
index. restrained compatibility, Ki maxset(Kg Pi , Pi1 , . . . , Pg+1 ). maxset
318

fiRevision History

sequence implies first element, consistent. Since case, Ki |= Kg Pi .
means Condition 2 holds index g.
Condition 2 Ki |= Kh Pi h l < i. Since Ki consistent, Kh Pi
consistent well, proving Condition 1 index l.
Theorem 6 Establishing existence total preorder generating restrained revision
sequence coNP-complete.
Proof. Membership follows previous lemma. conditions reformulated
entailments (like Ki |= Pi ), inconsistencies (like Kj Pi ) equivalences
maxsets (Ki maxset(Kj Pi , Pj+1 , . . . , Pi1 )). problems coNP,
therefore rewritten universal quantified formulae. whole problem combination
these; renaming variables taking quantifiers, single universally
quantified formula results. Since QBF coNP, problem coNP.
Hardness proved reduction problem propositional unsatisfiability. Given
formula F , associated revision sequence [a, b F, b], b fresh
variables, occurring F . F unsatisfiable b F equivalent b,
sequence [a, b, b] generated preorder C = [C(0), C(1)] C(0) = Mod(a)
C(1) = Mod(a). F satisfiable (b F ) satisfiable equivalent
b, makes sequence generated restrained revision preorder.
sequence therefore generated restrained revision preorder
F unsatisfiable.
A.4 Lexicographic Revision: Proofs
Lemma 11 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated lexicographic
revision total preorder among models C result preorder R defined
by:
R(i) = {Ki | Mod(Ki ) C(i)}
Proof. R total preorder because: first, R(0) empty C(0) = Mod(K0 ),
implies Mod(K0 ) C(0) K0 R(0); second, formula two classes
R C partition. Third, every Ki class R Property 1:
since Mod(Ki ) set minimal models C, models class C(j);
therefore, Ki R(j). Remains prove preorder satisfies two conditions
result preorder sequence.
Ki Kj consistent other, common model I. Let h
class. construction, since |= Ki Ki R(h). holds Kj , since
|= Kj . result, Kj R(h).
Let Ki Kj Ki Kj maxset(Pi , . . . , P1 ) consistent. assumption,
sequence generated lexicographic revision C. result, models Ki
exactly minimal models maxset(Pi , . . . , P1 ) according C. Let h class
models. construction R, holds Ki R(h). models Ki maxset(Pi , . . . , P1 )
belong greater class l > h. Since Ki Kj maxset(Pi , . . . , P1 ) consistent, Kj
319

fiLiberatore

models C(l). Since models Kj class, follows Mod(Kj ) C(l),
implies Kj R(l). Since l > h, claim proved.
Lemma 12 R result preorder revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ]
Ki |= maxset(Pi , . . . , P1 ) every i, lexicographic revision following
total preorder C among models generates revision sequence, z index
greatest class R:
(

C(h) =

{Mod(Ki ) | Ki R(h)} h z
{I | 6 K1 Kn }
h = z + 1

Proof. C proved total preorder. First, C(0) empty since R(0) empty.
Second, model belongs one class: C(h) C(l)
definition exists Ki R(h) Kj R(l) satisfied I; implies
Ki Kj consistent; since R result preorder sequence, follows l = h.
Models C(z + 1) exactly ones contained classes.
Remains proved lexicographic revision generates revision sequence
C: revising C P1 , . . . , Pi generates Ki . Since Ki |= maxset(Pi , . . . , P1 ) assumption,
models Ki also models maxset(Pi , . . . , P1 ). Remains proved
minimal ones: models greater classes. Let h class models
Ki , model Ki maxset(Pi , . . . , P1 ). satisfy Kj
R(z + 1), z + 1 > h since z greatest index classes R.
satisfies Kj satisfies Ki Kj maxset(Pi , . . . , P1 ), therefore satisfiable.
Since R result preorder sequence, Kj class R(l) l > h. implies
C(l). proves every model maxset(Pi , . . . , P1 ) Mod(Ki )
class greater h.
Lemma 13 revision sequence generated lexicographic revision total preorder lexicographic compatible.
Proof. revision sequence generated total preorder Ki |=
maxset(Pi , . . . , P1 ) holds every i. Furthermore, result preorder R exists
Lemma 11. it, define relations ' < as: Ki ' Kj Ki Kj
class R(h); Ki < Kj Ki R(h), Kj R(l) h < l. Since R revision
preorder sequence, Ki Kj consistent Ki Kj class,
implies Ki ' Kj . way, Ki Kj maxset(Pi , . . . , P1 ) consistent
Ki lower class Kj , implying Ki < Kj . cycle containing < impossible
Ki < Kj means class Ki lower class Kj .
prove lexicographic compatibility, requires Ki ' Kj Ki < Kj
respectively equivalent consistency Ki Kj Ki Ki maxset(Pi , . . . , P1 ),
merely implied them. However, removing relations pairs Ki
Kj satisfying respective condition makes relations weaker. Therefore, new
cycle created.
prove direction, assume Ki |= maxset(Pi , . . . , P1 ) every
' < two relations defined sequence specified definition
320

fiRevision History

lexicographic compatibility. result preorder sequence shown. Together
Ki |= maxset(Pi , . . . , P1 ), implies revision sequence generated initial
preorder Lemma 12.
' <, relation defined: Ki Kj either Ki ' Kj Ki < Kj .
relation necessarily transitive, number properties:
1. Ki ' Kj Ki Kj Kj Ki ; indeed, Ki ' Kj holds Ki Kj
consistent, implies Kj Ki consistent; therefore, Kj ' Ki , implies
Kj Ki ;
2. Ki < Kj Ki Kj Kj 6 Ki ; converse, Kj Ki , either
Kj ' Ki Kj < Ki ; cases, cycle ' < containing one < link,
contradicts assumption cycle exists;
3. reflexive; indeed, formulae Ki consistent assumption; therefore, Ki Ki
consistent, implies Ki ' Ki Ki Ki ;
4. Suzumura consistent (Suzumura, 1976): form cycles Ki1 , . . . , Kim =
Ki1 Kij Kij+1 j j also Kij+1 6 Kij ; proved
below.
Properties 1 2 mean ' < equivalence strict part , respectively. Indeed, Ki Kj holds either Ki ' Kj Ki < Kj ; former implies
Kj Ki , latter Kj 6 Ki . Property 4 consequence fact assumption
nonexistence cycles ' < containing least one link <.
Since reflexive (Property 3) Suzumura consistent (Property 4), Suzumura
extension theorem (Suzumura, 1976) total preorder R extending exists. Extending
means equivalence strict parts preserved R. Since
proved ' <, total preorder R Ki Kj class Ki ' Kj
Ki lower class Ki < Kj .
Ki Kj consistent Ki ' Kj , implies Ki Kj
class R. Ki Kj maxset(Pi , . . . , P1 ) consistent Ki < Kj , implies
class Ki R less class Kj . proves R result preorder
revision sequence.
Lemma 14 revision sequence [K0 , P1 , K1 . . . , Pn , Kn ] lexicographic compatible
either Ki 6|= maxset(Pi , . . . , P1 ) consistent sets R1 , . . . , Rn exist
that:
1. {Pj | 1 j Ki |= Pj } Ri every i;
2. exists cycle Ki1 , . . . , Kim = Ki1 either Kij Kij+1 Kij Kij+1 Ri
consistent ij {i1 , . . . , im1 }, second consistent least one
index.
Proof.
revision sequence lexicographic compatible Ki |=
maxset(Pi , . . . , P1 ) cycle specified definition. Inverting
321

fiLiberatore

condition, sequence lexicographic compatible either Ki 6|= maxset(Pi , . . . , P1 )
cycle exists. result, one check whether Ki 6|= maxset(Pi , . . . , P1 ) i;
true, check needed: sequence lexicographic compatible.
presence cycles irrelevant case. case Ki |= maxset(Pi , . . . , P1 )
i, sequence lexicographic compatible contains
cycles.
point condition presence cycles written
assumption Ki |= maxset(Pi , . . . , P1 ), since case condition
matters. Lemma 2, consistent subset {P1 , . . . , Pi } containing formulae entailed Ki maxset(Pi , . . . , P1 ). means Ri used place
maxset(Pi , . . . , P1 ), since Ri satisfying first condition statement
lemma maxset(Pi , . . . , P1 ).
Theorem 7 problem checking existence total preorder generating revision
sequence using lexicographic revision coNP-complete.
Proof. Membership follows previous lemma: sequence generated
lexicographic revision preorder lexicographic compatible,
turns checked existential quantifiers only:
1. Ki 6|= maxset(Pi , . . . , P1 );
2. exists Ri ;
3. Ri consistent;
4. either Ki 6|= Pj Pj Ri ;
5. exists cycle Ki1 , . . . , Kim = Ki1 ;
6. Kij Kij+1 consistent;
7. Kij Kij+1 Ri consistent.
first condition expressed terms existential quantifiers shown
Lemma 3. holds conditions well. result, incompatibility
NP, means existence preorder generating sequence coNP.
Hardness proved reduction propositional unsatisfiability. formula F
satisfiable [K0 , P1 , K1 ] generated preorder, K0 = a, P1 =
K1 = F new variable contained F . Indeed, F unsatisfiable,
K1 = a, sequence generated preorder C = [C(0), C(1)] C(0) = Mod(a)
C(1) = Mod(a). F satisfiable, K1 models satisfy P1 :
ones F . result, K1 6|= P1 , sequence generated preorder.

322

fiRevision History

A.5 Reinforcement Revision: Proofs
Lemma 15 every sequence n numbers V = [V (1), . . . , V (n)], holds MV (I, i, j) =
MV (I, i, h) + MV (I, h, j) every three indexes i, j h.
Proof. < j MV (I, i, j) sum MV (I, l, l+1) < l j. Otherwise,
sum MV (I, l, l + 1) = MV (I, l + 1, l). Also MV (I, i, h) MV (I, h, j) expressed
way. h j result follows immediately. Otherwise, < j
h > j MV (I, i, h) includes MV (I, l, l + 1) l > j, MV (I, h, j) includes
MV (I, l + 1, l) = MV (I, l, l + 1), subtracts amount sum.
case h < similar.
Lemma 16 revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] generated reinforcement revision total preorder C DC = [DC (1), . . . , DC (n)] DC (i) =
min{j | CP1 ,...,Pi1 (j) Mod(Pi ) 6= } |= Ki then, every j:
MDC (I, i, j) = 0 |= Kj ;
MDC (I, i, j) > 0 otherwise.
Proof. model CP1 ...Pi1 (c) CP1 ...Pi (c DC (i)) |= Pi
CP1 ...Pi1 (c + 1) otherwise. result, MDC (I, i, + 1) difference class
preorder step + 1 preorder step i. Since MDC (I, i, j) sum
amounts index index j, difference class step
j i. |= Ki class step zero. result, MDC (I, i, j) class
step j. zero |= Kj greater otherwise.
Lemma 17 V = [V (1), . . . , V (n)] reinforcement mover revision sequence
[K0 , P1 , K1 , . . . , Pn , Kn ], following initial preorder C = [C(0), . . . , C(V (1) + + V (n +
1))] generates revision sequence reinforcement revision DC = V .
(

C(j) =

{I | |= Ki MV (I, i, 0) = j} j < V (1) + . . . + V (n) + 1
{I | . 6|= Ki }
j = V (1) + . . . + V (n) + 1

Proof. Since K0 consistent, least model I. Since |= K0 MV (I, 0, 0) = 0
definition, follows C(0), proving C(0) empty. Every model
class |= Ki C(MV (I, i, 0)), otherwise C(V (1) + . . . + V (n) +
1). prove C total preorder, remains prove model belongs two
classes. |= Ki |= Kj MV (I, j, i) = 0 V reinforcement mover;
therefore, MV (I, i, 0) = 0 + MV (I, i, 0) = MV (I, j, i) + MV (I, i, 0) = MV (I, j, 0). model
C(V (1) + . . . + V (n) + 1) satisfy Ki ; therefore, cannot
class C(j). proves C total preorder.
longest part proof show reinforcement revision generates
sequence C. Inductively, assumed DC (1) = V (1), DC (2) = V (2), . . . ,
DC (i) = V (i), DC (l) = min{j | CP1 ,...,Pl1 (j) Mod(Pl ) 6= }. proved
323

fiLiberatore

models Ki CP1 ,...,Pi (0) models classes greater index
total preorder. Furthermore, minimal class models Pi+1 preorder
V (i + 1), proving DC (i + 1) = V (i + 1), allows iterate proof.
base case = 0: proved Mod(K0 ) = C(0) DC (1) =
V (1). construction C, model C(0) MV (I, i, 0) = 0 |= Ki i;
hold = 0. Vice versa, C(0) |= Ki MV (I, i, 0) = 0 i.
6|= K0 , definition reinforced mover j = 0 implies MV (I, i, 0) > 0,
turn implies 6 C(0), contradiction.
order prove DC (1) = V (1), let model P1 . construction C,
|= K1 C(V (1)). 6|= K1 two cases possible. first, 6|= Kj every
j. implies C(V (1) + . . . + V (n) + 1), V (1) + . . . + V (n) + 1 > V (1).
second case, |= Kj j. definition reinforcement mover, MV (I, j, 1) > 0.
class C containing index MV (I, j, 0) = MV (I, j, 1)+MV (I, 1, 0) = MV (I, j, 1)+
V (1), last step consequence |= Pi . Since MV (I, j, 1) > 0, follows
amount greater V (1). proves V (1) index minimal class
models P1 C: DC (1) = V (1). concludes base case.
assumed DC (1) = V (1), . . . , DC (i) = V (i), proved CP1 ...Pi
class zero equal Mod(Ki ) minimal models Pi+1 class V (i + 1),
proves DC (i + 1) = V (i + 1). induction, proves sequence generated
reinforcement revision total preorder C.
Let model Ki . construction, C(MV (I, i, 0)). first revisions
increase class MDC (I, 0, i). Since DC (1), . . . , DC (i) equal V (1), . . . , V (i)
inductive assumption, MV (I, 0, i). definition MV ,
holds MV (I, i, 0) = MV (I, 0, i). result, MV (I, i, 0) + MDC (I, 0, i) = MV (I, i, 0)
MV (I, i, 0) = 0: model CP1 ...Pi (0).
model Ki , may model Kj not. second
case, C(V (1) + + V (n) + 1). first steps reduce class number
DC (1) + . . . + DC (i), leading V (1) + + V (n) + 1 + DC (1) + . . . + DC (i). Since
first values V DC coincide, V (1) + + V (n) + 1 V (1) V (i) =
1 + V (i + 1) + + V (n), greater zero.
model Kj C(MV (I, j, 0)) definition C. class step
therefore MV (I, j, 0)+MDC (I, 0, i). Since DC (1), . . . , DC (i) coincide V (1), . . . , V (i)
induction hypothesis, second term equal MV (I, 0, i). sum therefore
equal MV (I, j, 0) + MV (I, 0, i) = MV (I, j, 0) MV (I, i, 0) = MV (I, j, i) + MV (I, i, 0)
MV (I, i, 0) = MV (I, j, i). definition reinforced mover j reversed, since
|= Kj 6|= Ki MV (I, j, i) > 0. proves class order
CP1 ...Pi index larger zero.
last step proof show models Pi+1 classes index
V (i + 1) greater according ordering step i, CP1 ...Pi . Let model
Pi+1 . satisfy Kj C(V (1)+ +V (n)+1). step class
least V (1) + + V (n) + 1 DC (1) DC (i), since DC (j) maximal decrease
classes step j. Since DC (1), . . . , DC (i) coincide V (1), , V (i) induction
assumption, equal V (1)+ +V (n)+1V (1) V (i) = V (i+1)+ +V (n)+1,
larger V (i + 1).
324

fiRevision History

satisfies Kj , possibly j = + 1, C(MV (I, j, 0)) definition
C. step i, class index MV (I, j, 0) + MDC (I, 0, i). Since DC (1), . . . , DC (i)
assumed equal V (1), . . . , V (i), second term equal MV (I, 0, i) sum
MV (I, j, 0) + MV (I, 0, i) = MV (I, j, i). Since |= Pi+1 , MV (I, i, + 1) = V (i + 1)
definition MV (Definition 11 V place DC ). definition reinforced mover
ensures MV (I, j, + 1) = MV (I, j, i) + MV (I, i, + 1) = MV (I, j, i) V (i + 1) equal
0 |= Ki+1 greater otherwise. Since Ki+1 models, minimal value
MV (I, j, i) V (i + 1) zero, proving minimal value MV (I, j, i) V (i + 1).
proves DC (i + 1) = V (i + 1).
Lemma 18 reinforcement revision generates revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ]
total preorder, also generates sequence preorder
minimal initial class models P1 0 1.
Proof. K0 P1 consistent P1 models K0 , class zero.
Otherwise, let class indexes models P1 k1 < k2 < k3 < . . . Revision P1
decreases numbers k1 , making k1 k1 = 0, k2 k1 , k3 k1 , etc.
given initial preorder, new one generated reducing model
P1 k1 1 classes. new initial preorder generates revision sequence.
models satisfy P1 changed initial class, revising P1
still moved one class.
models P1 classes k1 (k1 1), k2 (k1 1), k3 (k1 1), etc.
new preorder. Since k1 minimal index models P1 , none indexes zero.
Therefore, model P1 enters K0 .
minimal class models P1 new initial preorder 1, proving DC (1) =
1. implies revising P1 decreases class models P1 one.
result, indexes classes k1 (k1 1) 1, k2 (k1 1) 1, k3 (k1 1) 1,
etc. coincide k1 k1 = 0, k2 k1 , k3 k1 , etc. classes
obtained revising original preorder P1 . models satisfy P1
initial class still moved one class. proves ordering
revising P1 before. Therefore, point revision sequences
identical.
Lemma 19 reinforcement revision generates sequence [K0 , P1 , K1 , . . . , Pn , Kn ]
total preorder, also generates sequence total preorder C
DC (i + 1) DC (1) + . . . + DC (i) + + 1.
Proof. claim proved two cases separately: Pi+1 models satisfy
Kj j < i, not.
Case 1: models Pi+1 satisfy Kj j < i. models class zero
step j. Therefore, class j step i. proves Pi+1
models class j, minimal class models j less: DC (i + 1) j,
less DC (1) + . . . + DC (i) + + 1 j < + 1.
Case 2: model Pi+1 Kj j < i. Let C(l) minimal initial class
models Pi+1 . l > DC (1) + . . . + DC (i), models Pi+1 decreased
325

fiLiberatore

l (DC (1) + . . . + DC (i)) 1 classes without affecting generated sequence. Models
class l move class l(l(DC (1)+. . .+DC (i)1)) = DC (1)+. . .+DC (i)+1. maximal
class model may reach step DC (1) + . . . + DC (i) + 1 + i, since step may
increase class model one. Therefore, DC (i+1) DC (1)+. . .+DC (i)+i+1.
remains proved change affect revision results.
Regarding steps + 1, since models Pi+1 initial class DC (1) +
. . . + DC (i) + 1 greater, models K1 initial class DC (1), minimal
class models P1 still DC (1) result revision still K1 . Since DC (1)
before, similar line reasoning applied models K2 DC (2),
K3 Ki , proving neither K1 , . . . , Ki DC (1), . . . , DC (i)
affected change. Since DC (1), . . . , DC (i) tell models moved, step
models Pi+1 classes change. models Pi+1
lowered DC (i + 1) classes. Since change altered relative initial positions,
modify relative positions step i. preorder step + 1 therefore
before.
Lemma 20 Reinforcement revision generates revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ]
total preorder generates sequence total preorder
DC (i) bounded 2i 1.
Proof. proved DC (1) 1, preorder modified make
so.
Since DC (i + 1) bounded DC (1) + . . . + DC (i) + + 1, assuming claim
DC (1), . . . , DC (i) leads DC (i + 1) (21 1) + + (2i 1) + + 1,
DC (i + 1) (20 + 21 + + 2i ) 20 1 + + 1 = 21 + + 2i , 2i+1 1.
Theorem 8 Establishing existence total preorder generating revision sequence
[K0 , P1 , . . . , Pn , Kn ] reinforcement revision p2 , coNP n constant.
Proof. lemma, revision sequence generated preorder
generated preorder DC (i) 0 2i 1. result,
problem solved guessing reinforcement mover V = [V (1), . . . , V (n)]
sequence, V (i) 0 2i 1, since implies existence
total preorder C generating sequence also DC (i) = V (i) indexes.
n constant guessing replaced disjunction. Checking whether V
reinforcement mover amount check whether, I, j, holds |= Ki
implies MV (I, i, j) = 0 |= Kj MV (I, i, j) > 0 otherwise. Calculating MV (I, i, j)
done polynomial time since checks requires form j >
|= Pl . Therefore, verification coNP, whole problem p2
guessing V .
Theorem 9 Checking existence preorder generating reinforcement revision sequence [K0 , P1 , K1 , . . . , Pn , Kn ] coNP-complete, n constant.
326

fiRevision History

Proof. Membership proved previous theorem. Hardness proved
problem propositional unsatisfiability. Given formula F , corresponding revision
sequence [K0 , P1 , K1 ] K0 = F , P1 = b K1 = b, b
fresh variables occurring F . sequence, K0 = F consistent P1 = b.
Therefore, K1 = b equivalent conjunction (a F ) b. F
unsatisfiable case. Otherwise, model F extended assigning false
b false model K0 P1 model K1 . proves sequence
generated preorder F unsatisfiable.

References
Alchourron, C., & Makinson, D. (1982). logic theory change: Contraction functions
associated revision functions. Theoria, 48 (1), 1437.
Areces, C., & Becher, V. (2001). Iterable AGM functions. Rott, H., & Williams, M.A. (Eds.), Frontiers Belief Revision, Applied Logic Series, pp. 261277. Kluwer
Academic Publisher.
Baltag, A., Gierasimczuk, N., & Smets, S. (2011). Belief revision truth-tracking process.
Proceedings Thirteenth Conference Theoretical Aspects Rationality
Knowledge (TARK 2011), pp. 187190.
Benferhat, S., Kaci, S., Le Berre, D., & Williams, M.-A. (2004). Weakening conflicting
information iterated revision knowledge integration. Artificial Intelligence,
153, 339371.
Booth, R., & Meyer, T. (2006). Admissible restrained revision. Journal Artificial
Intelligence Research, 26, 127151.
Booth, R., & Nittka, A. (2008). Reconstructing agents epistemic state observations
beliefs non-beliefs. Journal Logic Computation, 18, 755782.
Boutilier, C. (1996). Iterated revision minimal change conditional beliefs. Journal
Philosophical Logic, 23, 263305.
Carson, R., & Louviere, J. (2011). common nomenclature stated preference elicitation
approaches. Environmental Resource Economics, 49 (4), 539559.
Darwiche, A., & Pearl, J. (1997). logic iterated belief revision. Artificial Intelligence Journal, 89 (12), 129.
Delgrande, J., Dubois, D., & Lang, J. (2006). Iterated revision prioritized merging.
Proceedings, Tenth International Conference Principles Knowledge Representation Reasoning, KR-2006, pp. 210220.
Eiter, T., & Gottlob, G. (1996). complexity nested counterfactuals iterated
knowledge base revisions. Journal Computer System Sciences, 53 (3), 497512.
Fagin, R., Ullman, J. D., & Vardi, M. Y. (1983). semantics updates databases.
Proceedings Second ACM SIGACT SIGMOD Symposium Principles
Database Systems (PODS83), pp. 352365.
327

fiLiberatore

Ferme, E., & Hansson, S. (2011). AGM 25 years - Twenty-five years research belief
change. Journal Philosophical Logic, 40 (2), 295331.
Gardenfors, P. (1988). Knowledge Flux: Modeling Dynamics Epistemic States.
Bradford Books, MIT Press, Cambridge, MA.
Hild, M., & Spohn, W. (2008). measurement ranks laws iterated contraction. Artificial Intelligence, 172 (10), 11951218.
Jin, Y., & Thielscher, M. (2007). Iterated belief revision, revised. Artificial Intelligence
Journal, 171 (1), 118.
Katsuno, H., & Mendelzon, A. O. (1991). Propositional knowledge base revision minimal change. Artificial Intelligence, 52, 263294.
Konieczny, S., & Pino Perez, R. (2000). framework iterated revision. Journal
Applied Non-Classical Logics, 10, 339367.
Lehmann, D., & Magidor, M. (1992). conditional knowledge base entail?
Artificial Intelligence, 55, 160.
Liberatore, P. (2014a). Belief revision examples.
(CoRR), abs/1409.5340.

Computing Research Repository

Liberatore, P. (2014b). Belief revision reliability assessment. Manuscript.
Nayak, A. (1994). Iterated belief change based epistemic entrenchment. Erkenntnis, 41,
353390.
Nebel, B. (1992). Syntax-Based Approaches Belief Revision, pp. 5288. Cambridge
University Press.
Nittka, A., & Booth, R. (2008). method reasoning agents beliefs
observations. Logic foundation game decision theory, Vol. 3 Texts
logic games, pp. 153182.
Papini, O. (2001). Iterated revision operations stemming history agents
observations. Frontiers belief revision, Vol. 22 Applied Logic Series, pp. 279
301. Springer.
Peppas, P. (2008). Belief revision, pp. 317359. Elsevier.
Rott, H. (2009). Shifting priorities: Simple representations twenty-seven iterated theory change operators. Towards Mathematical Philosophy, Vol. 28, pp. 269296.
Springer.
Sandholm, T., & Conen, W. (2010). Preference elicitation combinatorial auctions. US
Patent 7,742,971.
See, K., Morrison, W., Rothman, N., & Soll, J. (2011). detrimental effects power
confidence, advice taking, accuracy. Organizational Behavior Human
Decision Processes, 116 (2), 272285.
Spohn, W. (1988). Ordinal conditional functions: dynamic theory epistemic states.
Causation Decision, Belief Change, Statistics, pp. 105134. Kluwer Academics.
Suzumura, K. (1976). Remarks theory collective choice. Economica, New Series,
43, 381390.
328

fiRevision History

SyncML (2002). SyncML sync protocol, version 1.1.
Tversky, A., & Kahneman, D. (1983). Extensional versus intuitive reasoning: conjunction fallacy probability judgment. Psychological review, 90 (4), 293315.
Wang, H., Zhang, J., & Johnson, T. R. (2000). Human belief revision order effect.
Proceedings 22th Annual Conference Cognitive Science Society.
Williams, M. (1994). Transmutations knowledge systems. Proceedings Fourth International Conference Principles Knowledge Representation Reasoning
(KR94), pp. 619629.
Winslett, M. (1988). Reasoning actions using possible models approach. Proceedings Seventh National Conference Artificial Intelligence (AAAI88), pp.
8993.
Zhang, D. (2004). Properties iterated multiple belief revision. Proceedings Seventh International Conference Logic Programming Nonmonotonic Reasoning
(LPNMR 2004), pp. 314325.

329

fiJournal Artificial Intelligence Research 52 (2015) 507-542

Submitted 11/14; published 04/15

Weighted Electoral Control
Piotr Faliszewski

faliszew@agh.edu.pl

Department Computer Science
AGH University Science Technology
Krakow, Poland

Edith Hemaspaandra

eh@cs.rit.edu

Department Computer Science
Rochester Institute Technology
Rochester, NY 14623, USA

Lane A. Hemaspaandra

lane@cs.rochester.edu

Department Computer Science
University Rochester
Rochester, NY 14627, USA

Abstract
Although manipulation bribery extensively studied weighted voting, almost work done election control weighted voting.
unfortunate, since weighted voting appears many important natural settings.
paper, study complexity controlling outcome weighted elections
adding deleting voters. obtain polynomial-time algorithms, NP-completeness results, many NP-complete cases, approximation algorithms. particular, scoring rules completely characterize complexity weighted voter control. work
shows quite important cases, either polynomial-time exact algorithms
polynomial-time approximation algorithms exist.

1. Introduction
many real-world election systems voters come weights. Examples range
stockholder elections weighted shares, US Electoral College, often-used example Nassau County Board Supervisors, (in effect) parliamentary system
parties typically vote blocks, Swedens system wealth-weighted voting
instituted 1866 (and longer used) wealthiest members rural communities received many 5,000 votes 10 percent districts weighted
votes three voters could decisive (Congleton, 2011). Furthermore,
many important voting applications within multiagent systems, e.g., recommender systems (Ghosh, Mundhe, Hernandez, & Sen, 1999; Lu & Boutilier, 2011), planning (Ephrati
& Rosenschein, 1997), web search (Dwork, Kumar, Naor, & Sivakumar, 2001).
applications, quite natural voters (i.e., agents) weighted (e.g.,
amount trust put power resources possess).
surprising study manipulative attacks elections, weighted
voting given great attention. bribery manipulation, two three
studied types manipulative attacks elections, study case weighted voters
extensively conducted. Yet remaining one three studied types
c
2015
AI Access Foundation. rights reserved.

fiFaliszewski, Hemaspaandra, & Hemaspaandra

attacks elections, so-called control attacks, almost attention given
case weighted voting; best knowledge, time issue
previously raised two M.S./Ph.D. theses (Russell, 2007; Lin, 2012). lack
attention troubling, since key types control attacks, adding deleting
voters, certainly occur many weighted elections. coda section,
give examples.
study complexity weighted elections arguably important types
controladding deleting votersfor various election systems. focus scoring
rules, families scoring rules, Condorcet-consistent rules, weakCondorcet-consistent
rules. Control deleting (adding) voters asks whether given election given candidate
made win deleting (adding) certain number voters (at
certain number members pool potential additional voters). control
types model issues found many electoral settings, ranging human electronic. (abstractions of) issues often faced people seeking steer election,
experts campaign management, deciding example k people
offer rides polls. Adding deleting voters also occur multiagent systems.
example, agents entities Internet, one attempt denial-of-service
attack prevent votes arriving time. hand,
adding voters pertains simply encouraging agents vote, multiplying existing
agents, performing false-name attacks (for false-name attacks related settings, see,
example, Wagman & Conitzer, 2014; Waggoner, Xia, & Conitzer, 2012; Aziz, Bachrach,
Elkind, & Paterson, 2011).
Control introduced (without weights) 1992 seminal paper Bartholdi,
Tovey, Trick (1992). Control subject much attention since. attention, present paper, part line work, started Bartholdi, Tovey,
Trick (1989, 1992) Bartholdi Orlin (1991), seeks determine
types manipulative attacks elections attackers task requires polynomial-time
computation. detailed discussion line work, refer reader
related work section end paper surveys Faliszewski, Hemaspaandra, Hemaspaandra, Rothe (2009), Faliszewski, Hemaspaandra, Hemaspaandra
(2010), Brandt, Conitzer, Endriss (2013).
main results follows (see Section 6 tables summarizing results). First,
Section 4.1 provide detailed study complexity voter control scoring
protocols, case fixed numbers candidates. show constructive
control adding voters constructive control deleting voters P t-approval
(and also covers plurality t0 -veto1 ) NP-complete otherwise. interesting compare result analogous theorem regarding weighted coalitional
manipulation: cases complexities voter control manipulation
(e.g., plurality Borda) also cases voter control
easier (t-approval 2, elections candidates). ever possible weighted voter control harder weighted voting manipulation? show
weighted voter control NP-hard Condorcet-consistent rules least three
candidates (and clearly also NP-hard weakCondorcet-consistent rules least
1. number candidates fixed, t-veto expressed (m t)-approval,
number candidates. number candidates unbounded, t-veto t0 -approval.

508

fiWeighted Electoral Control

three candidates). Since weighted coalitional manipulation 3-candidate Llull system
P (Faliszewski, Hemaspaandra, & Schnoor, 2008), together fact Llull
weakCondorcet-consistent, implies setting weighted voter control
harder weighted coalitional manipulation.
Sections 4.2 4.3 focus complexity weighted voter control
t-approval t-veto, case unbounded numbers candidates. start
Section 4.2, explain interesting cases. Section 4.2
resolve six problems left open Lin (2012). establish complexity weighted
control adding voters 2-approval, 2-veto, 3-approval, weighted control
deleting voters 2-approval, 2-veto, 3-veto. Section 4.3, give polynomialtime approximation algorithms weighted voter control t-approval t-veto.
algorithms seek minimize number voters added deleted.
believe complexity weighted voter control, generally complexity attacks weighted elections, important interesting research direction
deserves much study. particular, research suggests worthwhile
seek approximation algorithms weighted elections problems
lead interesting algorithms.

2. Motivation Studying Control Weighted Elections
Introduction, noted importance weights many electoral settings
described natural importance ofand gave pointers extensive line work
studyingcontrol attacks many (unweighted) settings. also stated control attacks naturally expected occur even many weighted election settings.
present section, give examples motivating study weighted electoral
control.
Let us consider academic department salient issue particular
term question course add B.S. majors requirements. Suppose
department highly polarized issue research area, i.e., faculty given
research area vote block (either agree, tradition
meet actual faculty meetings to, within group, reach group position
support). suppose group days/times
entire group would unlikely attend, e.g., time time
major yearly research conference area. department chair, knowing
power schedule faculty meetings held, agenda
meeting, might well model task weighted control deleting voters
problem, voters groups, groups weight number faculty
members groups, deletion limit one.
fact, generally, individual voters may blur collection weighted votes
settings voter set partitions groups express identical preferences.
another example sort, one authors schools, members faculty senate
chosen election system known single transferable vote. However,
unheard departmental leaders send friendly suggestion departments
members regarding vote. one assumes departments vote blocks,
one trying decide candidates add election, convince
509

fiFaliszewski, Hemaspaandra, & Hemaspaandra

run election, order make given candidate win (or win), one effect
studying weighted constructive control adding candidates, weighted constructive control
deleting candidates, weighted destructive control adding candidates, weighted
destructive control deleting candidates.
examples given ones weightings created individuals
forming blocks, occur even highly political contexts.
example, US House Representatives, issues (for example, water rights farm
subsidies) states delegation tends vote block parochial interests
states constituents companies, pressure lobbyists state delegations abstain
given vote effect (give take issues failing quorum) control
deleting voters attack.
However, also many voting cases weights inherent standalone individual voters, many cases control attacks may well occur.
example, consider US Corporate Elections. these, vote stockholder weighted
number shares. natural way frame control problems
setting case adding deleting candidates, example, regarding running
spot company officer director. even voter control come play here,
example, actor sending mailings toor phoning speaking tovoters
convince abstain voting, encourage voters vote election.
(The bound additions/deletions model counts number voters, rather
weights, quite reasonable setting, regardless weight, given
targeted voter addressed by, example, mailing/visit/phone-call, although
reality one admittedly might focus resources biggest stockholders.)
many weighted control examples presented. Let us finish
extremely high-stakes example. US Electoral College, works
majority rule among electors, electors state usually vote block,
since system lets whoever greatest popular vote given state select every
elector state (note: two fifty states different policies). Thus issue
of, example, whether someone Ralph Nader run,
withdraw name consideration particular time, sweeping
effect nation, effect weighted control adding/deleting candidates
scenario.
given number examples, voter control candidate
control, settings weighted control may occur. examples given vary
naturalness, weighted unweighted control certainly
points models dont capture nuances real world. example,
electoral partitioning problems geographic/contiguity constraints, groups
modeled voting blocks may fact defectors, Internet denial-of-service
attacks may freedom suppress vote independently rather may
suppress none votes coming given line/provider (see, e.g., Chen,
Faliszewski, Niedermeier, & Talmon, 2014). Nonetheless, belief importance
weighted elections importance control attacks remain forever separate.
feel control attacks sufficiently natural many weighted settingsvarying
academic departments companies stockholders nationsthat studying weighted
control worth undertaking. also feel that, although subject
510

fiWeighted Electoral Control

present paper, important experimental studies undertaken see
extent heuristic approaches circumvent worst-case hardness results regarding
weighted control (see Rothe & Schend, 2013, assessment type approach
settings, although see also Hemaspaandra & Williams, 2012, discussion
limitations heuristic attacks).

3. Preliminaries
assume reader familiar basic notions computational complexity
theory theory algorithms. provide relevant definitions conventions
regarding elections, election rules, control elections. also review NPcomplete problems use reductions.
3.1 Elections
take election pair E = (C, V ), C set candidates V
collection voters. voter preference order set C. preference order
total, linear order ranks candidates preferred one least
preferred one. example, C = {a, b, c} voter likes best, b,
c, preference order > b > c. weighted elections, voter v also
positive integer weight (v). voter weight (v) treated election system
(v) unweighted voters. Given two collections voters, V W , write V + W
denote concatenation.
3.2 Election Rules
election rule (or voting rule) function R given election E = (C, V ) returns
subset R(E) C, namely candidates said win election.
m-candidate scoring rule defined nonincreasing vector = (1 , . . . , )
nonnegative integers. voter v, candidate c receives pos(v,c) points,
pos(v, c) position c vs preference order. candidates maximum
total score winners. Given election E voting rule R assigns scores
candidates, write score E (c) denote cs total score E R. voting rule
used always clear context. Many election rules defined families
scoring rules, one scoring vector possible number candidates. example:
1. Plurality rule uses vectors form (1, 0, . . . , 0).
2. t-approval uses vectors (1 , . . . , ), = 1 {1, . . . , t}, = 0
> t. t-veto mean system candidates uses (mt)-approval
scoring vector. m-candidate t-approval t-veto systems often treat
vote 0/1 m-dimensional approval vector indicates candidates receive
points vote. Naturally, vector contains exactly ones t-approval
exactly zeroes t-veto.2
2. emphasize view t-approval t-veto correct settings set candidates
remains fixed. set candidates change (e.g., control adding/deleting candidates),
would use standard, preference-order-based definition.

511

fiFaliszewski, Hemaspaandra, & Hemaspaandra

3. Bordas rule uses vectors form (m 1, 2, . . . , 0), number
candidates.
Given election E = (C, V ), candidate c Condorcet winner (weak Condorcet
winner) every candidate C {c} holds half (at least half)
voters prefer c d. Note possible Condorcet winner given
election, even possible weak Condorcet winner given election.
Let Condorcet denote election system whose winner set exactly set Condorcet
winners, let weakCondorcet denote election system whose winner set exactly
set weak Condorcet winners. say rule R Condorcet-consistent whenever
Condorcet winner sole winner elected R. Analogously, rule
weakCondorcet-consistent elects exactly weak Condorcet winners whenever
exist. Every weakCondorcet-consistent system Condorcet-consistent, converse
always hold.
many Condorcet-consistent rules. briefly touch upon Copeland
family rules Maximin rule. given election E = (C, V ) two distinct
candidates c, C, let NE (c, d) number voters prefer c d. Let
rational number, 0 1. Copeland score candidate c C defined as:
k{d C {c} | NE (c, d) > NE (d, c)}k + k{d C {c} | NE (c, d) = NE (d, c)}k,
Maximin score candidate c C defined mindC{c} NE (c, d).
candidates highest score winners. Llull another name Copeland1 . Clearly,
Llull Maximin weakCondorcet-consistent.
3.3 Electoral Control
focus constructive control adding/deleting voters weighted elections. However,
also standard types control studied literature (e.g., control
adding/deleting candidates various forms partitioning candidates voters;
point reader Section 5 discussion related work).
Definition 3.1. Let R voting rule. weighted constructive control adding
voters rule R (R-WCCAV) weighted constructive control deleting voters
rule R (R-WCCDV), input contains set candidates C, collection weighted
voters V (sometimes referred registered voters) preferences C, preferred
candidate p C, nonnegative integer k. R-WCCAV also additional
collection W weighted voters (sometimes referred unregistered voters)
preferences C. problems ask following questions:
1. R-WCCAV: subcollection W 0 W , k voters, p
R(C, V +W 0 )?
2. R-WCCDV: subcollection V 0 V , k voters, p
R(C, V V 0 )?
Although paper focus primarily constructive control, Section 4.1 makes
comments so-called destructive variants control problems. Given voting rule R, weighted destructive control adding voters rule R (R-WDCAV)
512

fiWeighted Electoral Control

weighted destructive control deleting voters rule R (R-WDCDV) defined analogously constructive variants, difference goal
ensure distinguished candidate p winner. mention passing
throughout paper use known nonunique-winner model (a.k.a.
cowinner model), i.e., goal make p be, prevent p being, element
winner set. consider nonunique-winner model cleaner natural
model so-called unique-winner model, p must made kept
one-and-only winner election, model strongly blurs tie-breaking issues
control issues.
Note definitions parameter k defines number voters
added/deleted, total weight voters added/deleted.
standard approach modeling strategic behavior weighted elections. example,
study R-weighted-bribery (Faliszewski, Hemaspaandra, & Hemaspaandra, 2009),
bribing weighted voter unit cost regardless voters weight, study
weighted manipulation nearly single-peaked societies (Faliszewski, Hemaspaandra,
& Hemaspaandra, 2014), mavericity society depends number so-called
mavericks rather total weight. k might practice, k reflecting
ability chair add/delete voters, k practice would reflect many
voters chair viewed resources lure pressure out.
consider approximation algorithms WCCAV WCCDV t-approval
t-veto. so, assume input instances contain integer
k. Rather, goal simply find (when success possible all) small possible
collection voters add/delete p winner resulting election. (Just
mentioned previous paragraph, counting number added/deleted
voters, total weight added/deleted voters.) positive integer h,
h-approximation algorithm WCCAV/WCCDV algorithm (when success
possible all) always finds solution adds/deletes h times many voters
optimal action does. notion f ()-approximation algorithm WCCAV/WCCDV
defined analogously, argument f variable related problem
instance. meaning O(f ())-approximation algorithms similarly clear
context. natural worry seemingly incomplete definitions
interact possibility success might impossible regardless many votes
one adds/deletes. However, t-approval WCCDV t-veto WCCDV (and indeed,
scoring rule), always possible ensure p winner, example deleting
voters (recall nonunique-winner model). t-approval WCCAV
t-veto WCCAV, possible ensure ps victory adding voters
p winner add unregistered voters approve p. observations
make particularly easy discuss study approximation algorithms t-approval
t-veto, always easily check whether solution. voting
rules dont easy-checking property, analysis might much
complicated. reader may wish compare work Brelsford et al.s attempt
framing general election-problem approximation framework (Brelsford, Faliszewski,
Hemaspaandra, Schnoor, & Schnoor, 2008).
paper consider candidate-control cases (such weighted constructive control adding candidates weighted constructive control deleting candidates,
513

fiFaliszewski, Hemaspaandra, & Hemaspaandra

WCCAC WCCDC). reason bounded number candidates,
winner determination given weighted election system P holds WCCAC WCCDC P brute-force search. hand, number
candidates bounded candidate control already NP-hard plurality (and
t-approval t-veto, constructive setting destructive setting) even
without weights (Bartholdi et al., 1992; Hemaspaandra, Hemaspaandra, & Rothe, 2007;
Elkind, Faliszewski, & Slinko, 2011; Lin, 2012). Furthermore, many results candidate
control Condorcet-consistent rules claimed weighted setting. example, Maximin rule Copeland family rules, hardness results translate
immediately, straightforward see existing polynomial-time algorithms
unweighted cases also work weighted cases (Faliszewski, Hemaspaandra, &
Hemaspaandra, 2011).
3.4 Weighted Coalitional Manipulation
One goals compare complexity weighted voter control complexity
weighted coalitional manipulation (WCM). WCM similar WCCAV also
add voters, differs (a) add exactly given number voters,
(b) pick preference orders added voters. quite interesting see
differences problems definitions affect complexities.
Definition 3.2. Let R voting rule. R-WCM given weighted election (C, V ),
preferred candidate p C, sequence k1 , . . . , kn positive integers. ask whether
possible construct collection W = (w1 , . . . , wn ) n voters i,
1 n, (wi ) = ki , p winner R election (C, V +W ). voters W
called manipulators.
3.5 Computational Complexity
NP-hardness proofs use reductions following NP-complete problems.
Definition 3.3. instance Partition consists sequence (k1 , . . . , kt )
Ppositive
integers whose sum even. ask whether set {1, . . . , t} iI ki =
1 Pt
i=1 ki .
2
proof Theorem 4.3 use following restricted version Partition,
greater control numbers involved problem.
Definition 3.4. instance Partition 0 consists sequence (k1 , . . . , kt ) positive
integers, whose sum P
even, (a) even number, (b) ki , 1 t,
1
holds ki t+1 tj=1 kj . ask whether set {1, . . . , t} cardinality
P

1 Pt
iI ki = 2
i=1 ki .
2
Showing NP-completeness problem standard exercise. (In particular,
NP-completeness variant problem established Lemma 2.3 Faliszewski
et al., 2009; approach used show NP-completeness Partition0 .)
remaining hardness proofs based reductions restricted version wellknown Exact-Cover-By-3-Sets problem. restricted version still NP-complete (Garey
& Johnson, 1979).
514

fiWeighted Electoral Control

Definition 3.5. instance X3C 0 consists set B = {b1 , . . . , b3t } family
= {S1 , . . . , Sn } 3-element subsets B every element B occurs least
one three sets S. ask whether contains exact cover B, i.e.,
whether exist sets whose union B.
choice use X3C0 basis reductions, particular way
use it, allow us achieve something beyond simply showing given weighted
control result NP-complete. indeed able show certain weighted control
results important election systems remain NP-complete even allowed set
weights highly restricted, e.g., cases, allowed weight set {1, 2}
{1, 3}. cases sort appear within proof Theorem 4.13 highlighted
paragraph immediately preceding theorem.

4. Results
present results. Section 4.1 focus fixed numbers candidates
scoring protocols, weakCondorcet-consistent rules, Condorcet-consistent rules.
Sections 4.2 4.3 consider case unbounded number candidates, t-approval
t-veto.
4.1 Bounded Numbers Candidates
well-known weighted manipulation scoring protocols always hard, unless
scoring protocol effect plurality triviality (Hemaspaandra & Hemaspaandra, 2007).
contrast, weighted voter control easy m-candidate t-approval.
Theorem 4.1. t, WCCAV WCCDV m-candidate t-approval
P.
Proof. Let (C, V, W, p, k) instance WCCAV m-candidate t-approval.
assume add voters approve p. also assume add
heaviest voters particular set approvals, i.e., add ` voters approving
p, c1 , . . . , ct1 , assume
added ` heaviest voters approving p, c1 , . . . , ct1 .
m1
Since t1 constantdifferent sets approvals consider,
suffices try sequences nonnegative integers k1 , . . . , k(m1) whose sum
t1
k, sequence check whether adding heaviest ki voters ith
approval collection makes p winner.
fixed t, clear algorithm, although brute-force nature,
runs time polynomial input size. (The actions algorithm uses relatively
innocuous, fact. example, use sorting group together votes within W
identical sets approvals, sort descending order voter weight.
number sequences nonnegative integers k1 , . . . , k(m1) whose sum
t1

m1
k easily bounded (k + 1)( t1 ) , fixed polynomial input
size despite fact k input binary, may without loss generality
m1
assume k kW k. mention passing (k + 1)( t1 ) bound often wildly
loose. particular, exact number sequences nonnegative integers k1 , . . . , k(m1)
t1

515

fiFaliszewski, Hemaspaandra, & Hemaspaandra

+k0 1
(m1
t1 )
. summing k 0 equals 0 k 0 equals k
m1
( t1 )1
gives number sequences face.)
approach argument work WCCDV. Here, delete voters
approve p, delete heaviest voters approval collection.
Again, fixed, running time easily seen polynomial.

whose sum exactly k 0

One might think argument works scoring protocol,
case. example, consider 3-candidate Borda instance V consists one
weight-1 voter b > p > W consists weight-2 weight-1 voter preference
order > p > b. adding weight-1 voter makes p winner, adding weight-2
voter not. And, fact, following result.3
Theorem 4.2. WCCAV WCCDV Borda NP-complete. result holds even
restricted fixed number 3 candidates.
Proof. start considering case adding voters. reduce Partition. Given
sequence k1 , . . . , kt positive integers sum 2K, construct election one
registered voter weight K voting b > p > > , unregistered voters weights
k1 , . . . , kt voting > p > b > . Set addition limit t. candidates,
(initial) score b K(m 1), score p K(m 2), score K(m 3).
Thus, p become winner, bs score (relative p) needs go least K,
score (relative p) go K. follows k1 , . . . , kt
partition p made winner.
use construction deleting voters case. Now, voters registered
deletion limit t. Since cant delete voters since goal make p
winner, cannot delete one voter voting b > p > > (since would
unique winner). rest argument identical case adding voters.
Interestingly, possible extend proof work scoring protocols
t-approval (the main idea stays same, technical details
involved). so, regarding complexity WCCAV WCCDV scoring protocols
fixed number candidates, cases Theorem 4.1 P cases (assuming
P 6= NP).
Theorem 4.3. scoring protocol (1 , . . . , ), exists i, 1 < < m,
1 > > , WCCAV WCCDV (1 , . . . , ) NP-complete.
Proof. Let = (1 , . . . , ) scoring protocol 1 > >
. Let third largest value set {1 , . . . , }. show WCCAV
WCCDV NP-complete scoring protocol = (1 , . . . , ) = (1 , . . . , ).
formally defined scoring protocols contain nonnegative values, using
simplifies construction affect correctness proof.
simplify notation, given candidates x1 , . . . , x` , F [x1 = i1 , x2 = i2 , . . . , x` = i` ]
mean fixed preference order ensures, , xj , 1 j `, ranked
3. analogue theorem model bounding total weight votes
added/deleted obtained Russell (2007).

516

fiWeighted Electoral Control

position gives ij points. (The candidates mentioned F [. . .] notation
ranked arbitrarily.) let 1 , 2 , 3 three highest values set {1 , . . . , }.
Clearly, 1 = 1 > 2 > 3 = 0. (Note 2 might different 2 , 3 might
different 3 . example, = (3, 3, 2, 0, 0, 1, 1), 1 = 3, 2 = 2,
3 = 0, 1 = 3, 2 = 3, 3 = 2.)
give reduction Partition -WCCAV (the membership -WCCAV NP
clear); let (k1 , . . . , kt ) instance Partition, i.e., sequence positive integers
sum 2K. form election E = (C, V ) C = {p, a, b, c4 , . . . , cm }
collection V contains following three groups voters (for WCCAV part proof
below, set = 1; WCCDV part proof use construction
larger value ):
1. group voters, weight K preference order F [b = 1 , = 2 , p = 0].
2. group voters, weight K preference order F [p = 1 , b = 2 , = 0].
3. ci C, 6 collections 2T voters, one collection permutation (x, y, z) (p, a, b); voters collection weight K preference
order F [x = 1 , = 2 , z = 3 , ci = ].
Let number points a, b, p receive third group
voters (each candidates receives number points voters).
ci C x {p, a, b}, x receives least 4T K1 points ci
voters third group (in vote third group, x receives least many
points ci , two collections 2T voters x receives 1 = 1 points
ci receives 0 points). Thus holds candidates following scores:
1. p + K1 points,
2. + K2 points,
3. b + K(1 + 2 ) points,
4. candidate ci C 2T K1 points (each ci C receives
4T K1 points third group voters 2T K1 points
first two groups voters).
result, b unique winner. unregistered voters weights
k1 , . . . , kt , preference order F = [a = 1 , p = 2 , b = 0]. set addition limit t. clear irrespective voters added, none
candidates {c4 , . . . , cm } becomes winner.
subcollection (k1 , . . . , kt ) sums K, adding corresponding
unregistered voters election ensures three p, a, b winners (each
score + K(1 + 2 )). hand, assume unregistered voters
total weight L, whose addition election ensures p among winners.
adding voters election, p +T K1 +T L2 points, +T L1 +T K2
points, b + K1 + K2 points. p score least high b,
must L K. However, score higher p, must case
517

fiFaliszewski, Hemaspaandra, & Hemaspaandra

L K (recall 1 > 2 ). means L = K. Thus possible ensure
p winner election adding unregistered voters
subcollection (k1 , . . . , kt ) sums K. And, completing proof, note
reduction carried polynomial time.
Let us move case WCCDV. use construction,
following modifications:
1. reduction Partition0 . Thus without loss generality assume
1
even number i, 1 t, holds ki 1+t
2K.
l

1
2. set = 2t (t + 1) 1
+ 1 (the reasons choice become apparent
2
course proof; intuitively convenient think large value
that, nonetheless, polynomially bounded respect t).
3. include unregistered voters fourth group voters.
4. set deletion limit 2t .
Including fourth group voters, candidates following scores: p +
K1 + 2T K2 points, + K2 + 2T K1 points, b + K(1 + 2 ) points,
candidate ci C points.
reasoning WCCAV case, see size- 2t subcollection k1 , . . . , kt sums K, deleting corresponding voters ensures
p among winners (together b); may imagine first remove
voters fourth group add back 2t them, whose weights sum
K. show way delete 2t voters ensure p
among winners, deleted voters must come fourth group, must
total weight K, must exactly 2t them. sake contradiction, let us
assume possible ensure ps victory deleting 2t voters, fewer
2t come fourth group. Let number deleted voters fourth
group (s < 2t ) let x real number xT K total weight.
xT K (see explanation regarding first inequality)



ts

2 +1
(2T K) 2T K 1
.
xT K 2T K
= TK
1+t
1+t
1+t

is, 0 x 1+t
. see first inequality holds, recall
1
lowest weight voter fourth group least 1+t
2T K (because reduce
0
Partition ). Thus highest total weight voters fourth group is, most,
total weight fourth-group voters (2T K) less weight lightest voters
ts
(2T K)).
group (which least 1+t
Prior deleting voters, K(1 2 ) points p. deleting
voters fourth group, difference decreases K(1 x)(1 2 ).
additionally delete 2t voters first three groups voters, weight K,
difference scores p decreases, most, following value
(note deleted vote p ranked positions receive 1 ,

518

fiWeighted Electoral Control

2 , 0 points):

1

K(1x)(1 2 ) K1 K
(1 2 ) K1 = K
2
t+1
2




(t + 1)1
(1 2 )
2
t+1
t+1


> 0.

final inequality follows choice . calculation shows
way ensure ps victory deleting 2t voters requires deleting exactly

2 voters fourth group. reasoning case WCCAV shows
2t deleted voters must correspond size- 2t subcollection (k1 , . . . , kt ) sums
K.
side comment, mention WDCAV WDCDV scoring protocols (that
is, destructive variants WCCAV WCCDV) simple polynomial-time algorithms: suffices loop candidates c, c 6= p, greedily add/delete voters
boost score c relative p much possible.
Theorem 4.4. scoring protocol = (1 , . . . , ), -WDCAV -WDCDV
P.
Combining Theorems 4.1 4.3, obtain following corollary, contrast analogous result WCM (Hemaspaandra & Hemaspaandra, 2007); also
mention passing recent attainment dichotomy result voter control
so-called pure scoring rules, unweighted elections unbounded number candidates (Hemaspaandra, Hemaspaandra, & Schnoor, 2014).
Corollary 4.5. scoring protocol (1 , . . . , ) problems WCCAV WCCDV
NP-complete k{1 , . . . , }k 3 P otherwise.
Theorem 4.6 (Hemaspaandra & Hemaspaandra, 2007). scoring protocol
(1 , . . . , ), 2, WCM NP-complete 2 > P otherwise.
see scoring protocols fixed number candidates, either WCM
harder WCCAV WCCDV (for case t-approval 2 < m),
complexity WCM, WCCAV, WCCDV (P-membership plurality
triviality, NP-completeness remaining cases). One may wonder
property WCM responsible fact t-approval, 2 m, WCM
harder WCCAV WCCDC. Speaking informally, answer WCM
intimately involves instantiation values (initially unspecified) votes
manipulators, particular setting is, effect, requiring solve
Partition problem. hand, WCCAV WCCDV preference orders
voters fixed input, chair chooses votes add; this,
example, facilitated polynomial-time algorithm proof Theorem 4.1.
are, nonetheless, voting rules WCM easier WCCAV
WCCDV. happens, example, one hand WCCAV WCCDV particular rule chair balance differing votes way makes
problems hard, yet hand WCM particular rule
show successful manipulation one manipulators cast identical votes. Theorem 4.7, Corollary 4.8, proofs present exactly
case.
519

fiFaliszewski, Hemaspaandra, & Hemaspaandra

Theorem 4.7. every weakCondorcet-consistent election system every
Condorcet-consistent election system, WCCAV WCCDV NP-hard. result holds
even restricted fixed number 3 candidates.
Proof. show WCCAV NP-hard, reduce Partition. Given sequence
k1 , . . . , kt positive integers sum 2K, construct election two registered
voters, one voter weight 1 voting p > > b > one voter weight 2K voting
b > p > > , unregistered voters weights 2k1 , . . . , 2kt voting > p > b > .
Set addition limit t. Suppose add unregistered voters election total
vote weight equal 2L.
L < K, b Condorcet winner, thus unique winner election.
L > K, Condorcet winner, thus unique winner election.
L = K, p Condorcet winner, thus unique winner election.
WCCDV case uses construction. Now, voters registered
deletion limit t. Since delete + 2 voters, since goal
make p winner, cant delete sole voter voting b > p > a, since would
Condorcet winner. rest argument similar adding voters case.
Recall Section 3 Condorcet denotes election system whose winner set
exactly set Condorcet winners, weakCondorcet denotes election system whose
winner set exactly set weak Condorcet winners.
Corollary 4.8. Condorcet weakCondorcet, WCM P WCCAV
WCCDV NP-complete. result holds even restricted fixed number 3
candidates.
Proof. immediate WCM Condorcet weakCondorcet P. see
yes-instance WCM, suffices check whether letting manipulators rank
p (the preferred candidate) first ranking remaining candidates arbitrary
order ensures ps victory. NP-completeness WCCAV WCCDV follows directly
Theorem 4.7.
Condorcet weakCondorcet always winners. prefer
voting systems always least one winner, note WCM 3-candidate Llull
P (Faliszewski et al., 2008).
Corollary 4.9. 3-candidate Llull, WCM P WCCAV WCCDV NPcomplete.
main results section also presented Table 1 Section 6.
520

fiWeighted Electoral Control

4.2 t-Approval t-Veto Unbounded Number Candidates
Let us look cases t-approval t-veto rules, unbounded number
candidates. reason focus interesting families
scoring protocols whose complexity already resolved previous section.
reason say Theorem 4.3 shows whenever least three
distinct values scoring vector, NP-completeness. scoring-protocol family
that, number candidates, three distinct values scoring vector NPhard WCCAV WCCDV. Thus really interesting cases indeed t-approval
t-veto.
starting point work Lin (2012), showed 4, WCCAV
t-approval WCCDV t-veto NP-complete, 3, WCCDV
t-approval WCCAV t-veto NP-complete. results hold even
unweighted case. also known remaining unweighted cases P (Bartholdi
et al., 1992; Lin, 2012) WCCAV WCCDV plurality veto P (Lin,
2012). section, look solve remaining open cases, WCCAV 2approval, 3-approval, 2-veto, WCCDV 2-approval, 2-veto, 3-veto.
start showing 2-approval-WCCAV P. point proof techniques
(especially polynomial-time algorithms 2-approval-WCCAV 2-veto-WCCDV)
quite different Lin (2012).
Theorem 4.10. WCCAV 2-approval P.
Proof. claim Algorithm 1 solves 2-approval-WCCAV polynomial time. (In
algorithm proof correctness, whenever speak r heaviest voters voter
set X, mean min(r, kXk) heaviest voters X.)
note add voters approve p. Thus delete W
voters approve p.
Let us consider repeat-until loop Algorithm 1. reject first iteration
loop (in first forall loop) then, clearly, solution given instance.
Furthermore, claim solution input instance, second
forall loop still possible find it. see this, consider candidate c C {p}
number ` {1, . . . , k 1}. sum weights k ` heaviest voters
W approve c less sc (that is, less difference
score c score p original election), certainly need add least
k ` + 1 voters approve c. However, since altogether add k
voters, means add ` 1 voters approve c. effect,
safely delete W ` 1 heaviest voters approve c (as proof
Theorem 4.1, decide add r voters approving {p, c}, may assume
add r heaviest voters approving {p, c}; thus keeping ` 1 heaviest voters
approve {p, c} correct strategy).
So, reject first iteration repeat-until loop, certainly
solution input instance, not, start second iteration
instance solution original one had. Thus, induction,
never reject incorrectly repeat-until loop. get repeat-until without
rejecting, fewer k voters left W , adding W best
(since voters W approve p).
521

fiFaliszewski, Hemaspaandra, & Hemaspaandra

Algorithm 1: 2-approval-WCCAV
Input: (C, V, W, p, k)
forall c C {p}
let sc = score(C,V ) (c) score(C,V ) (p).
Delete W voters approve p.
repeat
forall c C {p}
sum weights k heaviest voters W approve c
less sc reject
// impossible get score(c) score(p) adding less equal k voters
W .
forall c C {p} ` {1, . . . , k 1}
sum weights k ` heaviest voters W approve c
less sc
delete W voters approving c except ` 1 heaviest voters.
// need add least k ` + 1 voters approve c,
add ` 1 voters approving c.
changes.
kW k k accept // make p winner adding k heaviest voters
W.
kW k < k
adding W make p winner accept else reject

hand, get repeat-until loop, least k voters
left W , adding k heaviest voters W make p winner. Why? Let c
candidate C {p}. Let r number voters W added approve
c. Since made repeat-until, know [the sum weights
k heaviest voters W approve c] least sc (because reject
first forall loop). show adding voters, score(c) score(p) 0,
implies p winner. r = 0, score(c) score(p) = sc - [the sum weights
k heaviest voters W ] 0. r > 0, [the sum weights k r
heaviest voters W approve c] least sc (for otherwise would
r 1 voters approving c left W due statement second forall loop).
score(c) score(p) = sc - [the sum weights k r heaviest voters W
approve c] 0.
Theorem 4.11. WCCDV 2-veto P.
Instead proving theorem directly, show general relation
complexity t-approval/t-veto WCCAV WCCDV.
Theorem 4.12. fixed t, holds t-veto-WCCDV (t-approval-WCCDV)
polynomial-time many-one reduces t-approval-WCCAV (t-veto-WCCAV).
Proof. first give reduction t-veto-WCCDV t-approval-WCCAV. idea
deleting t-veto vote v t-veto election (C, V ) equivalent, terms net effect
scores, adding t-approval vote v 0 election, v 0 approves exactly
522

fiWeighted Electoral Control

candidates v disapproves of. problem approach
reduce t-veto-WCCDV t-approval -WCCAV thus show implement
t-veto scores t-approval votes.
Let (C, V, p, k) instance t-veto-WCCDV, V = (v1 , . . . , vn ). Let = kCk.
Let max highest weight vote V . set set 1 new
candidates, kCk + kDk multiple t. set V0 collection kCk+kDk

t-approval votes, vote weight max candidate C approved
(t1)(mt)
exactly one votes. vote vi V create set Ci = {c1i , . . . , ci
}
candidates create collection voters Vi = (vi1 , . . . , vimt ). voter vij ,
1 j t, weight (vi ) approves jth candidate approved v
(j1)(t1)+1
j(t1)
1 candidates ci
, . . . , ci
.

0
0
0
form election E = (C , V ), C 0 = CD ni=1 Ci V 0 = V0 +V1 + +Vn .
candidate c, let sc cs t-veto score (C, V ); see cs t-approval score
E 0 max + sc (every candidate C receives single approval one weight max
voter V0 voter vi V candidate c vi approves of,
unique voter Vi weight vi approves c). Furthermore,
candidate c C 0 C t-approval score max E 0 (each candidate C 0 C
approved exactly one voter V 0 voter V 0 weight max ).
form instance (C 0 , V 0 , W, p, k) t-approval-WCCAV, W = (w1 , . . . , wn ),
i, 1 n, (wi ) = (vi ), wi approves exactly candidates
vi disapproves of; adding voter wi t-approval election (C 0 , V 0 ) net
effect scores candidates C deleting vi t-veto election (C, V ).
(The role candidates pad election easy use t-approval
votesthose V0 ensure candidates C least many points
candidates, irrespective voters add.) completes reduction.
Let us give reduction t-approval-WCCDV t-veto-WCCAV. idea
previous reduction main difficulty proof show
implement t-approval scores t-veto votes.4 particular, role candidates
is, again, provide convenient way padding election implementing scores
candidates. However, time construction involved
nature t-veto: opposed case t-approval, t-veto add candidate
election total number candidates approved per vote increases.
Let (C, V, p, k) instance t-approval-WCCDV, V = (v1 , . . . , vn ). Let
= kCk let max highest weight vote V . set set
candidates kDk 2t 1 kCk + kDk = integer s, 3
(note setting trivial must case > t). set V0
collection 4n(s 2) (t-veto) votes (over candidate set C D), weight
max ; candidate C approved votes whereas candidate
disapproved least half (since kDk 2t 1, easy construct
4. reader may wonder simply use previous argument applying (m t)-veto
(m t)-approval. reason given instance (m t)-veto-WCCDV (with candidates
n voters), reduction would output instance (m t)-approval-WCCAV
candidates. Thus would correct interpret instance t-veto-WCCAV instance.

523

fiFaliszewski, Hemaspaandra, & Hemaspaandra

votes5 ). vote vi V , create collection Vi (s 1) votes satisfying
following requirements: (a) candidate approved vi also approved
votes Vi , (b) candidate approved vi , approved exactly (s 2) votes
Vi . (Such votes easy construct: always place top candidates vi
top positions vote; remaining positions, first vote place
candidates arbitrary, easily computable order, following vote shift
candidates cyclically positions respect previous vote.) vote
Vi weight (vi ).
form election E 0 = (C 0 , V 0 ), C 0 = C V 0 = V0 + V1 + + Vn .
0
candidate c, let sc cs
Pnt-approval score (C, V ); see cs t-veto score E
4n(s 2)max + (s 2)( i=1 (vi )) + sc (c approved every voter V0
least 2 voters group Vi , 1 n; additionally, every voter vi
approves c, (s 1)th voter group Vi approves c). Furthermore,
candidate t-veto score 3n(s 2)max E 0 (each gets
2n(s 2)max points voters V0 (s 2)max points
Vi , 1 n).
form instance (C 0 , V 0 , W, p, k) t-veto-WCCAV, W = (w1 , . . . , wn ),
i, 1 n, (wi ) = (vi ), wi disapproves exactly candidates vi
approves of; adding voter wi t-veto election (C 0 , V 0 ) net effect scores
candidates C deleting voter vi t-approval election (C, V ) has. Furthermore,
since candidate least nmax fewer points candidate C,
fact adding wi increases scores candidates affect correctness
reduction.
remaining cases (WCCDV 2-approval, WCCAV 3-approval, WCCAV
2-veto, WCCDV 3-veto) NP-complete. Interestingly, contrast many
NP-complete weighted election problems, need limited set weights
make reductions work. Namely, due choice reducing X3C0 due
particular reductions build, proof following theorem establishes (the
details given within proof) (a) every pair integers 1 < b,
holds WCCDV 2-approval WCCAV 2-veto NP-complete even
legal set weights restricted {a, b}, (b) WCCDV 3-approval WCCAV
3-veto NP-complete even legal set weights restricted {1, 3}.
Theorem 4.13. WCCAV 2-veto 3-approval WCCDV 2-approval 3-veto
NP-complete.
Proof. Membership NP immediate, suffices prove NP-hardness. first
give proof WCCDV 2-approval. Theorem 4.12 also immediately gives
result WCCAV 2-veto. reduce X3C0 Definition 3.5. Let
B = {b1 , ..., b3t } let = {S1 , ..., Sn } family 3-element subsets B
every element B occurs least one three sets S. construct
5. one
{d1 , . . . , dt }
even
exactly

possible construction. Let = {d1 , . . . , d` }, ` 2t 1. form sets D0 =
D1 = {d` , . . . , d`t+1 }. = D0 D1 (D0 D1 might overlap).
number voters V0 ; exactly half disapprove candidates set D0
half disapprove candidates set D1 .

524

fiWeighted Electoral Control

following instance (C, V, p, k) WCCDV 2-approval. set C = {p} {bj | 1 j
3t} {si , s0i | 1 n} {d0 , d1 , . . . , d3t } (d0 , d1 , . . . , d3t dummy candidates
used padding). 1 j 3t, let `j number sets contain bj .
assumption, j, 1 j 3t, 1 `j 3. V consists following
voters:
weight
2
1
1
1
2
3 `j

preference order
si > s0i >
si > bi1 >
si > bi2 >
s0i > bi3 >
p > d0 >
bj > dj >





1 n Si = {bi1 , bi2 , bi3 }



1 j 3t `j < 3.

Note score(si ) = 4, score(s0i ) = 3, score(bj ) = 3, score(p) = 2, score(dj ) 2.
set k = n + 2t claim contains exact cover p become
winner deleting n + 2t voters.
(): Delete (n t) weight-2 voters corresponding sets cover
delete 3t weight-1 voters corresponding sets cover. score p
change, score si decreases 2, score s0i decreases
least 1, score bj decreases 1. So, p winner.
(): need delete 3t voters decrease score every bj candidate 1. (Note
reason delete voters preference orders form bj > dj >
(1 j 3t). suffices decrease score bj one and, since also need
decrease scores candidates si s0i (1 n), always better delete voters
preference orders form si > bj > s0i > bj > .) deleting 3t
voters, values i, 1 n, score si score
s0i 2 (for i, obtaining score 2 candidates si s0i takes
least 3 unique voters 3t deleted ones).
exactly values i, 1 n, score si score s0i
2, values correspond cover. (Why so? consider
situation already deleted 3t voters preference orders forms
si > bj > s0i > bj > , bj Si . If, deleting voters,
scores si s0i decreased 2, must deleted exactly three voters
correspond members Si . Thus, deleting voters corresponding 3t members
B ensured values scores si s0i decreased 2,
must case values correspond cover.) less
values i, 1 n, score si score s0i 2,
remaining voters deleted, n them, need decrease
score si and/or s0i n values i, 1 n. possible,
since voter approves si s0i sj s0j 6= j.
Note construction uses weights 1 2. fact, establish NPcompleteness WCCDV 2-approval every set allowed weights size least
two (note set weights size one, problem P, since essence
unweighted case resolved Lin, 2012). Since reductions Theorem 4.12
change set voter weights, result WCCAV 2-veto.
525

fiFaliszewski, Hemaspaandra, & Hemaspaandra

So, suppose weight set contains w1 w2 , w2 > w1 > 0. modify construction follows. keep set candidates change voters
follows.
#
1
1
1
1
2
1
` `j

weight
w2
w1
w1
w1
w1
w2
w1

preference order
si > s0i >
si > bi1 >
si > bi2 >
s0i > bi3 >
p > d0 >
p > d0 >
bj > dj >





1 n Si = {bi1 , bi2 , bi3 }



w2 2w1
w2 > 2w1
1 j 3t.

Here, ` smallest integer `w1 > max(2w1 , w2 ). Note ` 3
``j never negative. Note score(si ) = w2 +2w1 , score(s0i ) = w2 +w1 , score(bj ) = `w1 ,
score(p) = max(2w1 , w2 ), score(dj ) max(2w1 , w2 ). argument
shows contains exact cover p become winner deleting
n + 2t voters.
turn proof WCCDV 3-veto. construction use weights
1 3. Since reductions Theorem 4.12 change set voter weights, weights
1 3 also suffice get NP-completeness WCCAV 3-approval. Given instance
X3C0 described above, construct following instance (C, V, p, k) WCCDV
3-veto. set C = {p} B {si | 1 n} {r, d, d0 } (d d0 dummy candidates
used padding) V consists following voters:
#
1
1
1
1
3n 3t
3n 3
3n + 1 `j

weight
3
1
1
1
1
1
1

preference order
> p > si > r
> p > si > bi1
> p > si > bi2
> p > si > bi3
> > d0 > r
> > d0 > si
> > d0 > bj





1 n Si = {bi1 , bi2 , bi3 }



1 n
1 j 3t.

convenient count number vetoes candidate count
number approvals. Note vetoes(si ) = 3n+3, vetoes(bj ) = 3n+1, vetoes(r) = 6n3t,
vetoes(p) = 6n, vetoes(d) = vetoes(d0 ) 3n. claim contains exact cover
p become winner (i.e., lowest number vetoes) deleting
n + 2t voters.
(): Delete (n t) weight-3 voters corresponding sets cover
delete 3t weight-1 voters veto p correspond sets cover.
vetoes(si ) = vetoes(bj ) = vetoes(r) = vetoes(p) = 3n vetoes(d) = vetoes(d0 ) 3n. So,
p winner.
(): assume delete voters veto p. Suppose delete k1 weight1 voters k2 weight-3 voters, k1 +k2 n+2t. deletion, vetoes(p) = 6nk1 3k2 ,
vetoes(r) = 6n 3t 3k2 , vetoes(bj ) 3n + 1. order p winner, need
vetoes(p) vetoes(r). implies k1 3t. also need vetoes(p) vetoes(bj ) 0.
526

fiWeighted Electoral Control

Since vetoes(p) vetoes(bj ) 6n k1 3k2 (3n + 1) 6n (n + 2t k2 ) 3k2 3n 1 =
2n2t2k2 1, follows k2 nt. (To see case, note require
vetoes(p) vetoes(bj ) 0 know vetoes(p) vetoes(bj ) 2n 2t 2k2 1,
must require 2n2t2k2 1 0. expression equivalent k2 nt 21 .
Since k2 , n, integers, must case k2 n t.) delete 3t weight-1
votes n weight-3 votes, deleting voters vetoes(p) = 3n. order
p winner, delete one veto bj three vetoes
si . implies set deleted weight-1 voters corresponds cover.
4.3 Approximation Greedy Algorithms
problems computationally difficult, NP-complete, natural wonder whether good polynomial-time approximation algorithms exist. So, motivated NP-completeness results discussed earlier paper cases
WCCAV/WCCDV t-approval t-veto, section studies greedy approximation algorithms problems. (Recall WCCAV NP-complete t-approval,
3, t-veto, 2, WCCDV NP-complete t-approval, 2,
t-veto, 3.) First, establish connection weighted multicover problem,
use obtain approximation results. obtain approximation
algorithm work direct action problem. Table 3 Section 6 summarizes
results approximation algorithms t-approval/t-veto WCCAV/WCCDV.
undertake this, let us address detail issue, valuably raised
referee, one might want build approximation algorithms control problems,
might use algorithms, whether unwise obtain algorithms
people using might good guys. mentioned above, seeking good
polynomial-time approximation algorithms one standard approach exact solutions
known intractable, e.g., NP-complete. algorithms allow campaign
strategist to, faced intractability computing optimal number votes add
delete achieve victory candidate, least able quickly find action
guaranteed within particular multiplicative factor optimal action. One
might expect desire get approximations would hit wall regarding
potential impossibility exerting control certain instances, discussed
Electoral Control subpart Section 3, worry hold particular problems
obtain approximation algorithms. Finally, worry people
(chairs) employ approximation algorithms may good guys,
following somewhat multilayered reply. First, good evil highly contextual.
Whether strategists attempts help candidate win good evil
much eye beholder. may decry attempts part brutal nature
politics. Others may view attempts, long illegal actions taken,
valid indeed valuable part spirited, vibrant playing field democracy. Second,
settings, control may simply modeling optimization problem, wellapproximating control isnt even candidates, simply efficiency. Third,
even one views approximating control helping evil-doers, using reason
learn control problems approximated well approximated
makes sense sticking ones head sand hoping cryptosystems
527

fiFaliszewski, Hemaspaandra, & Hemaspaandra

cant broken. Since evil-doers may well try build approximation algorithms, break
cryptosystems, natural way thwarting field richly explore
approximations vulnerabilities exist, choose election system
use given problem choose one weak respect good
approximations whatever attacks fear.
4.3.1 Weighted Multicover Approach
Let us first consider extent known algorithms Set-Cover family
problems apply setting. Specifically, use following multicover problem.
Definition 4.14. instance Weighted Multicover (WMC) consists set B =
{b1 , . . . , bm }, sequence r = (r1 , . . . , rm ) nonnegative integers (covering requirements),
collection = (S1 , . . . , Sn ) subsets B, sequence = (1 , . . . , n ) positive integers (weights sets S). goal find
P minimum-cardinality set {1, . . . , n}
, declare set exists.
bj B holds rj
iIbj Si

is, given WMC instance seek smallest collection subsets
satisfies covering requirements elements B (keeping mind set
weight covers elements times). WMC extension Set-Cover unit
costs. define problem known Covering Integer Programming (see
Kolliopoulos & Young, 2005), short written CIP. However, problem
quite important us here. reason observe WMC special
case CIP (with multiplicity constraints but) without packing constraints; footnote 6
effect describing embed problem problem. approximation
algorithm Kolliopoulos Young CIP (with multiplicity constraints but) without
packing constraints, applied special case WMC, gives following result.6
Theorem 4.15 (Kolliopoulos & Young, 2005). polynomial-time algorithm
given instance WMC set contains elements gives
O(log t)-approximation.
t-approval WCCAV WCCDV naturally translate equivalent WMC
instances. consider WCCAV first. Let (C, V, W, p, k) instance t-approval6. paper Kolliopoulos Young (2005) directly speak WMC problem, seeing
results indeed apply WMC easy, tedious, exercise. readers would like
verify Theorem 4.15 holds, footnote describe exactly paper Kolliopoulos
Young one finds relevant result parameters one use. warn reader
footnote makes direct references parts paper make sense paper
simultaneously hand. footnote merely guide understanding particular way draw
papers important work; providing full-fledged survey of, even real discussion of, CIP
problem beyond needs scope paper.
Theorem 4.15 follows sentenceon page 496 work Kolliopoulos Young
(2005)starting second algorithm finds solution (which follows Theorem 8),
keeping mind none so-called packing constraints, may take
call one matrix vector call B b wont factor here. vector
corresponds rj s; element jth row ith column matrix us set
Si contains bj 0 otherwise; set cost vector c vector 1s; set
multiplicity vector vector 1s; vector x corresponds characteristic function
I; Theorem 4.15s bound number elements B contained Si .

528

fiWeighted Electoral Control

WCCAV, W = (w1 , . . . , wn ) collection voters may add. assume
without loss generality voter W ranks p among top candidates (i.e.,
approves p).
form instance (B, r, S, ) WMC follows. set B = C {p}.
c B, set covering requirement rc = score (C,V ) (c) score (C,V ) (p),
j =def max(0, ij). vote w W , let Sw set candidates w
approve of. assumption regarding voter ranking p among top candidates,
Sw contains p. set = (Sw1 , . . . , Swn ) set = ((w1 ), . . . , (wn )). easy
see set {1, . . . , n} solution instance WMC (that is, satisfies
covering requirements) adding voters {wi | I} election (C, V )
ensures p winner. reason following: add voter wi
election candidate c Swi , difference score c score
p decreases (wi ), candidate c 6 Swi difference change.
covering requirements set guarantee ps score match exceed scores
candidates election.
stress construction assume constant. Indeed,
construction applies t-veto well t-approval. using Theorem 4.15
obtain following result.
Theorem 4.16. polynomial-time O(log m)-approximation algorithm tapproval-WCCAV. polynomial-time algorithm given instance
t-veto-WCCAV (t N) gives O(log t)-approximation.
Proof. suffices use reduction t-approval/t-veto WMC apply algorithm
Theorem 4.15. case t-approval, reduction guarantees set
WMC instance contains elements. case t-veto, sets
contains elements.
obtain analogous results case t-approval/t-veto WCCDV. One
either provide direct reduction problems WMC notice reductions
given proof Theorem 4.12 maintain approximation properties.
Theorem 4.17. polynomial-time algorithm given instance tapproval-WCCDV (t N) gives O(log t)-approximation. polynomial-time
O(log m)-approximation algorithm t-veto-WCCDV.
4.3.2 Direct Approach
Using algorithms WMC, able obtain relatively strong algorithms
WCCAV/WCCDV t-approval t-veto. However, approach
find approximation algorithms t-approval-WCCAV t-veto-WCCDV whose approximation ratios depend (and not, example, kCk, i.e., m, kV k).
following seek direct algorithms problems.
show simple greedy approach yields polynomial-time tapproximation algorithm t-approval-WCCAV t-veto-WCCDV. (Recall
means cases making p win possible, number voters algorithm
adds/deletes reach victory never times optimal set additions/deletions.)
529

fiFaliszewski, Hemaspaandra, & Hemaspaandra

Let GBW (greedy weight) define following simple algorithm WCCAV.
(The votes weighted t-approval vectors induced preferences voters.)
(Pre)discard unregistered votes approve preferred candidate p. Order
(remaining) unregistered votes heaviest lightest, breaking ties voter weights
simple, transparent way (for concreteness, let us say lexicographic order
votes representations). GBW goes unregistered votes order,
reaches vote adds vote exactly vote disapproves least one candidate
whose score (i.e., total weight approvals) currently strictly greater p.
stops successfully p become winner unsuccessfully happens
algorithm runs votes consider. following result says GBW tapproximation algorithm t-approval-WCCAV, also t-veto-WCCDV, using
obvious analogue GBW t-veto-WCCDV, also call GBW.7
Theorem 4.18. Let 3. polynomial-time greedy algorithm GBW tapproximation algorithm t-approval-WCCAV t-veto-WCCDV; instances GBWs approximation factor problems better t.
prove Theorem 4.18s upper lower bound parts separately, following
two lemmas theorem immediately follows.
Lemma 4.19. Let 3. instances polynomial-time greedy algorithm GBW approximation factor t-approval-WCCAV better t.
instances polynomial-time greedy algorithm GBW approximation
factor t-veto-WCCDV better t.
Lemma 4.20. Let 3. polynomial-time greedy algorithm GBW t-approximation
algorithm t-approval-WCCAV t-veto-WCCDV.
proof lower-bound claim, Lemma 4.19, consists somewhat detailed pair
constructions, less interest upper-bound part Theorem 4.18, namely
Lemma 4.20. thus defer appendix proof Lemma 4.19.
Proof Lemma 4.20. Let us prove two claims GBW t-approximation
algorithm. prove result = 3 WCCAV, immediately clear
proof straightforwardly generalizes greater t; WCCDV case follows
using Theorem 4.12.
Clearly GBW polynomial-time algorithm. Consider given input instance tapproval-WCCAV, preferred candidate p. Without loss generality, assume unregistered voters approve p. say candidate gap (under current
set registered voters whatever unregistered voters already added)
candidate strictly weight approvals p does. candidate
7. completeness clarity, describe mean GBW t-veto-WCCDV. Order votes
approve p heaviest lightest, breaking ties voter weights simple, transparent way (for concreteness, let us say lexicographic order votes representations). GBW
goes votes order, reaches vote removes vote exactly
vote approves least one candidate whose score (i.e., total weight approvals) currently strictly
greater p. stops successfully p become winner unsuccessfully
happens algorithm runs votes consider.

530

fiWeighted Electoral Control

gap, 6= p, define id minimum number unregistered voters one add
remove ds gap; is, one went heaviest lightest among unregistered voters,
adding turn disapproved d, id number voters one would add
longer gap. candidate holds integer realizes id , control
impossible using unregistered voter set. Clearly, successful addition voters must
add least maxd id voters (the max throughout proof candidates initially
gap).
Let us henceforth assume control possible input case. show
added 3 maxd id voters GBW made p winner, GBW
3-approximation algorithm.
giving detailed proof, let us informally give sense proofs idea.
Let z candidate allegedly gap GBW added 3maxd id voters,
freeze action GBW point. proof argues relatively many
3 maxd id voters added GBW (i.e., least maxd id them) approve z, z
clearly gap point time GBW frozen, assumed
gap cant exist case. proof argues relatively 3 maxd id
voters added GBW (i.e., (maxd id )1 them) approve z (equivalently,
least 1 + 2 maxd id approve z), also arrive contradiction.
latter argument subtle one, involving asking candidates (call y) gap
caused last added vote added, needed drilling extra level
related few/many argument focused y, show GBW must point
acted way violates definition, thus also yielding contradiction. Since
many cases cover possible cases, proof achieved
goal. provide formal analysis carries argument line.
So, suppose 3 maxd id additions candidate, z, still gap.
discussed perform case analysis case arrive contradiction.
Case 1 [In least maxd id first 3maxd id votes added GBW, z approved].
Since last one added z must still gap addition,
earlier vote considered disapproved z gap z considered
would added reached. So, keeping mind iz maxd id ,
fact must added iz heaviest voters disapproving z, contrary
assumption, z longer gap additions.
Case 2 [Case 1 hold]. z approved least 1 + 2 maxd id added
votes. made final one added votes, call v 0 , eligible addition? must
candidate, say y, still gap v 0 added.
Case 2a [y disapproved least maxd id 2 maxd id votes added v 0
approved z]. Then, since ys gap removed unregistered voters disapproving
would excluded GBW, ys iy heaviest voters added. contrary
Case 2s assumption, gap get adding vote v 0 .
Case 2b [Case 2 holds Case 2a not]. approved least 1 + maxd id
2 maxd id votes v 0 GBW added approve z. 1 + maxd id
votes added approving exactly z y. made last 1 + maxd id
votes, call v 00 , eligible added? must hold candidate w gap
v 00 . moment adding v 00 would added maxd id iw votes
approving exactly z disapproving w, since w allegedly still gap,
531

fiFaliszewski, Hemaspaandra, & Hemaspaandra

GBW would fact added iw heaviest voters disapproving
w, ws gap would removed v 00 , contrary assumption w
gap made v 00 eligible.
One might naturally wonder GBW performs t-veto-WCCAV t-approvalWCCDV. argument far easier used proof Lemma 4.20,
cases GBW provides t-approximation algorithm.
Theorem 4.21. GBW t-approximation algorithm t-veto-WCCAV. GBW tapproximation algorithm t-approval-WCCDV.
Proof. Consider t-veto-WCCAV. Let p preferred candidate. candidate
initial positive gap relative preferred candidate p (i.e., surplus p
total weight approvals), let id defined proof Lemma 4.20. (Recall id
number votes would need add remove surplus p took
unregistered votes, discarded didnt simultaneously approve p disapprove
d,
P added one time heaviest lightest gap removed.)
Clearly,
id , sum taken candidates initial surplus relative
p, upper bound number votes added GBW. true since GBW
works adding extra votes heaviest lightest, restricted vetoing candidate
point positive gap relative p; GBW gap closed
largest weight votes address it. hand, overall optimal solution
id lower bound smallest number votes solutions added-vote set
would suffice remove ds positive gap (since takes id even use heaviest
votes addressing gap). overall optimal solution added vote narrows
gaps. GBWs solution uses worst times many added votes optimal
solution.
claim t-approval-WCCDV follows Theorem 4.12.
result replaces flawed claim conference version paper (Faliszewski,
Hemaspaandra, & Hemaspaandra, 2013) GBW cousins provide
O(1) approximations problems.8 course, t-approximation two
problems (namely, t-veto-WCCAV t-approval-WCCDV) wildly exciting, since
problems multicover-based approach earlier section showed
function f (t), f (t) = O(log t), even f (t)-approximation algorithms
problems. However, constant big oh algorithm large,
possible sufficiently small values approach may give better
approximation. Also, feel interesting learn behavior explicit
heuristics, especially attractive approaches greedy algorithms.
natural ask whether similar greedy algorithms work well scoring rules,
e.g., Bordas rule. Unfortunately, families scoring rules t-approval
t-veto analysis, possible, would likely significantly different
ours. main reason thatas discussed Electoral Control subpart
8. Note treat constant and, so, t-approximation algorithm provides (indeed, is)
O(1) approximate one. reason true that, technically speaking, WCCAV WCCDV
problems defined separately voting rule. example, 2-approval-WCCAV different
problem than, say, 200-approval-WCCAV.

532

fiWeighted Electoral Control

Section 3for t-approval t-veto always easy verify whether exists
solution (although, perhaps, one far optimal). scoring
rules, e.g., Borda, clear whether possible (and conjecture that,
indeed, NP-complete so). However, might interesting research direction
evaluate effectiveness greedy algorithms empirically (we point reader
work Rothe & Schend, 2013, recent survey covering experimental studies
complexity control elections).

5. Related Work
study complexity (unweighted) electoral control initiated
Bartholdi, Tovey, Trick (1992), considered constructive control
adding/deleting/partitioning candidates/voters plurality rule Condorcet rule (that is, rule chooses Condorcet winner whenever one,
winners otherwise). various types control model least flavor
actions occur real world, voter suppression targeted get-out-the-vote
drives (see survey Faliszewski et al., 2010, examples discussions).
major motivation study control obtain complexity barrier results, is,
results show detecting opportunities various control attacks computationally
difficult. particular, Bartholdi, Tovey, Trick focused NP-hardness measure
computational difficulty.
research direction continued Hemaspaandra, Hemaspaandra,
Rothe (2007), first study destructive control attacks elections. Since
then, many authors studied electoral control many varied settings many
different rules; refer reader survey Faliszewski et al. (2010). recent
research, covered survey, includes complexity-of-control results t-approval
family rules (Lin, 2012), Bucklins rule (and fallback, extension truncated
votes; Erdelyi, Fellows, Rothe, & Schend, 2015a), maximin (Faliszewski et al., 2011),
range voting (Menton, 2013), Schultzes rule ranked pairs rule (Parkes
& Xia, 2012; Menton & Singh, 2013; Hemaspaandra, Lavaee, & Menton, 2013).
present paper, compare control manipulation. recent paper Fitzsimmons,
Hemaspaandra, Hemaspaandra (2013) studies settings control manipulation occurring. Researchers have, quite different setting electing members
fill fixed-size, multimember panel, defined variants control coexisting constructive destructive aspects (Meir, Procaccia, Rosenschein, & Zohar, 2008).
also work analyzing counting variants control (Wojtas & Faliszewski, 2012),
goal decide given control attack possible, also count number
ways attack carried out.
complexity-barrier research line turned successful. voting
rules considered, significant number control attacks NP-hard. Indeed,
even possible construct artificial election system resistant types control
attacks (Hemaspaandra, Hemaspaandra, & Rothe, 2009). However, also number
results suggest practice complexity barrier might strong one
might first think. example, Faliszewski, Hemaspaandra, Hemaspaandra, Rothe
(2011) Brandt, Brill, Hemaspaandra, Hemaspaandra (2010) shown
533

fiFaliszewski, Hemaspaandra, & Hemaspaandra

votes restricted single-peaked, many control problems known
NP-complete become polynomial-time solvable. Indeed, often holds even elections
nearly single-peaked (Faliszewski et al., 2014), many real-world elections seem
(see, e.g., discussion Gehrlein & Lepelley, 2012, ch. 2). Similarly, initial
experimental results Erdelyi, Fellows, Rothe, Schend (2015b) suggest that, least
certain distributions settings, NP-hard control problems solved
practice many instances. part different line research, Xia (2012) studied
asymptotic behavior number voters added to/deleted
randomly constructed election successful control action.
number problems involving changing structure elections.
problems include candidate cloning, possible replace given candidate
c number clones (Elkind et al., 2011; Elkind, Faliszewski, & Slinko, 2012),
possible winner problem new alternatives join, additional, yet ranked
candidates introduced (Chevaleyre, Lang, Maudet, Monnot, & Xia, 2012; Xia, Lang,
& Monnot, 2011). last problem also related possible winner problem
truncated ballots (Baumeister, Faliszewski, Lang, & Rothe, 2012a).
papers directly raise issue weighted control are, best
knowledge, theses Russell (2007) Lin (2012). However, also mention
papers Baumeister, Roos, Rothe, Schend, Xia (2012b), Perek, Faliszewski,
Pini, Rossi (2013), authors, effect, consider problems affecting result
election picking weights voters. (The paper Perek et al. motivates
study differently, effect studies constrained variant choosing voter weights.)
problems similar to, though different from, simultaneous (multimode) addition
deletion voters (Faliszewski et al., 2011).
paper given f ()-approximation results weighted election control problems. Elkind Faliszewski (2010) given 2-approximation algorithm weighted,
bribery-related case.

6. Conclusions
studied voter control number voting rules, including scoring protocols,
families scoring protocols, (weak)Condorcet-consistent rules. shown
complexity voter control quite different complexity weighted
coalitional manipulation: natural voting rules weighted coalitional manipulation easy weighted voter control hard, natural rules
opposite case. Furthermore, shown weighted voter control
t-approval t-veto, good, natural approximation algorithms. results
voter control weighted elections summarized Tables 1, 2, 3.

Acknowledgements
grateful anonymous AAMAS 2013 JAIR referees extremely
helpful comments suggestions, incorporated examples.
thank editor, Jerome Lang, wise guidance.
work sup534

fiWeighted Electoral Control

WCCAV

WCCDV

WCM

Plurality

P (Thm. 4.1)

P (Thm. 4.1)

P

t-approval, 2 <

Borda

P (Thm. 4.1)

P (Thm. 4.1)

NP-comp.

NP-comp. (Thm. 4.2)

NP-comp. (Thm. 4.2)

NP-comp.

= (1 , . . . , ),
k{1 , . . . , }k 3

NP-comp. (Thm. 4.3)

NP-comp. (Thm. 4.3)

NP-comp.

Llull (3 candidates)

NP-comp. (Cor. 4.9)

NP-comp. (Cor. 4.9)

P

(weak)Condorcetconsistent rules

NP-hard (Thm. 4.7)

NP-hard (Thm. 4.7)

various
complexities

Table 1: results complexity control adding/deleting voters weighted
elections fixed number candidates, 3, compared complexity weighted coalitional manipulation. result marked due
Conitzer et al. (2007), results marked due Hemaspaandra
Hemaspaandra (2007), result marked due Faliszewski et al.
(2008).

WCCAV

WCCDV

t-approval
t=2
t=3
t4

P (Thm. 4.10)
NP-complete (Thm. 4.13)
NP-complete

NP-complete (Thm. 4.13)
NP-complete
NP-complete

t-veto
t=2
t=3
t4

NP-complete (Thm. 4.13)
NP-complete
NP-complete

P (Thm. 4.11)
NP-complete (Thm. 4.13)
NP-complete

Table 2: complexity control adding deleting voters t-approval t-veto
unbounded number candidates. results marked due
Lin (2012).

ported part grants AGH-11.11.230.124, NCN-DEC-2011/03/B/ST6/01393, NCNUMO-2012/06/M/ST1/00358, NSF-CCF-{0915792,1101452,1101479}, two Bessel
Awards Alexander von Humboldt Foundation.
535

fiFaliszewski, Hemaspaandra, & Hemaspaandra

WCCAV

WCCDV

t-approval

O(log m) (Thm. 4.16)
(Thm. 4.18)

O(log t) (Thm. 4.17)
(Thm. 4.21)

t-veto

O(log t) (Thm. 4.16)
(Thm. 4.21)

O(log m) (Thm. 4.17)
(Thm. 4.18)

Table 3: Approximation ratios algorithms WCCAV WCCDV tapproval t-veto.

Appendix A. Additional Details Related Section 4.3
present deferred proof Lemma 4.19 details related Section 4.3.
Proof Lemma 4.19. goal show GBW sometimes really use fully
times optimal number added/deleted votes, cases question. Examples
(somewhat detailed but) hard construct, lower bound even holds = 2,
though Section 4.2 obtained exact solution different approach. However, one
careful set gap pattern created unregistered voters
realizable one. t-approval-WCCAV construction, easy directly.
t-veto-WCCDV construction, establish realizability small tool
hope may prove useful elsewherethat lets one set certain patterns gaps.
state tool Tool A.1.
Fix {2, 3, 4, . . .}. construct instance t-approval-WCCAV
GBW uses times many additions optimal strategy. construction
2t candidates: preferred candidate p, candidates a1 , . . . , , candidates
d1 , . . . , dt1 . Now, suppose votes registered voters, gaps
follows. candidate ai , total weight approvals ai exceeds total weight
approvals p exactly 2t. candidate di , total weight approvals
di equals total weight approvals p. easily realized, namely
registered voter set one weight-2t voter approves ai .
set unregistered voters follows. one unregistered voter,
call nice, weight 2t, approves p 1 candidates di ,
disapproves candidates ai . j, 1 j t, single
unregistered voter, call j , weight 3t, approves p ai aj ,
disapproves aj di s.
Note GBW add voters . ideal would add single voter called
nice, since suffices make p winner. 2 constructed
setting GBW t-approval-WCCAV takes times optimal number
added votes.
also holds 2, similarly construct setting GBW
t-veto-WCCDV takes times optimal number deleted votes, prove
setting realizable. fact, following something flavor
536

fiWeighted Electoral Control

scheme, except slightly different vote set adjusts handle case
deleting voters, care regarding realizability. construction. Fix
{2, 3, 4, . . .}. candidate set preferred candidate p, candidates
a1 , . . . , , candidates d1 , . . . , dt1 . Let us specify voter set. put
voter set collection weight-1 votes gaps total approval weight relative
d1 created votes follows. d2 dt1 total
approval weight d1 . total approval weight p exceeds d1 3t2 + 3t.
total approval weight ai exceeds d1 3t2 .
Tool A.1 below, observe 2t-candidate t-approval voting, gap
pattern gaps multiples realized. Since current proof
using 2t-candidate t-veto, 2t-candidate t-approval, Tool A.1
applies here. particular, Tool A.1 easily builds set weight-1 votes realizing precisely
desired set gaps. (The exact number weight-1 votes used construction
important. However, gaps mentioned vote-set size mentioned
tool, precise number easily seen (3t + 3 + t(3t))(2t 1).)
yet done building voter set. also voter set one
voter, call nice, weight 2t, approves exactly ai s. j,
1 j t, one voter weight 3t approves exactly aj 1
di s.
entire set votes created abovethe votes tool combined
nice votes mentionedit easy see d1 candidate
least total approval weight, tied total approval weight di .
total approval weight p exceeds d1 3t. ai exceeds d1 total
approval weight 5t.
However, light pattern votes weights here, clear GBW
(in version t-veto) delete weight-3t voters. (Note votes added
Tool A.1 weight-1 votes, highly unattractive GBW.) ideal would
delete single voter called nice, since suffices make p winner.
2 constructed realizable setting GBW t-veto-WCCDV takes
times optimal number deleted votes.
Within proof, referred used small tool build certain
patterns vote weight gaps certain approval elections. would overreach
claim McGarvey-like tool, since different setting than, far less
flexible result than, famous theorem McGarvey (1953). However, small way
tool perhaps might useful elsewhere, state prove modest tool.
Tool A.1. Let 2. Let n1 , . . . , n2t1 list nonnegative integers divisible t.
exists collection t-approval votes, 2t candidates,
votes, relative candidate getting fewest approvals, list gaps number
approvals candidate otherP2t 1 candidates precisely (n1 , . . . , n2t1 ).
Furthermore, done (2t 1)( ni )/t unweighted (i.e., weight 1) votes.
alternatively done (2t 1)2 weighted votes (or even (2t 1)k{i | ni 6= 0}k
weighted votes).
Proof. Consider election 2t candidates, votes cast t-approval votes.
Consider collection 2t 1 votes, weight one, votes approve
537

fiFaliszewski, Hemaspaandra, & Hemaspaandra

particular candidate (for example, let one first candidate),
remaining 1 approvals cyclically rotate around candidates. t-approval
votes, viewed bit vectors, these: 1 1t1 0t , 1 0 1t1 0t1 , . . ., 1 0t 1t1 , 1 1 0t 1t2 , . . .,
1 1t1 0t 1. Note first candidate approved 2t 1 votes,
candidate approved exactly 1 votes. collection votes sets
gap favor first candidate, total approval weight first
candidate candidate difference total approval weight
pair candidates zero.
Given gap pattern stated tool, gap least-approved
candidate (call candidate c) multiple t, simply use approach
paragraph repeatedly, boost candidate, d, one time whatever
multiple supposed exceed c total approval weight. (In this, play
role first candidate previous paragraph.) ds surplus relative c kt
wish use weight-1 votes, k(2t 1) weight-1 votes.
Otherwise, 2t 1 weight-k votes. total number votes
used given statement tool.
appendix seeking provide comprehensive study gap collections
realizable t-approval voting, seeking find smallest number voters
needed realize realizable gap collection. interesting direction study,
goal here. However, mention clearly exist gap collections
cannot realized. example, exists claim Tool A.1 even
always true one removes assumption divisibility t. example showing
following. Consider 4-candidate setting votes 2-approval votes,
desire gap list relative least-approved candidate (1, 1, 1), i.e.,
candidates one approval least-approved candidate. Clearly,
total number approvals set votes achieving 4B + 3, B whatever
number approvals least-approved candidate happens get vote set one
trying, total number approvals odd. However, vote set 2-approval
votes even total number approvals. gap collection cannot realized.

References
Aziz, H., Bachrach, Y., Elkind, E., & Paterson, M. (2011). False-name manipulations
weighted voting games. Journal Artificial Intelligence Research, 40, 5793.
Bartholdi, III, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. Social
Choice Welfare, 8 (4), 341354.
Bartholdi, III, J., Tovey, C., & Trick, M. (1989). computational difficulty manipulating election. Social Choice Welfare, 6 (3), 227241.
Bartholdi, III, J., Tovey, C., & Trick, M. (1992). hard control election?.
Mathematical Computer Modeling, 16 (8/9), 2740.
Baumeister, D., Faliszewski, P., Lang, J., & Rothe, J. (2012a). Campaigns lazy voters:
Truncated ballots. Proceedings 11th International Conference Autonomous
Agents Multiagent Systems, pp. 577584.
538

fiWeighted Electoral Control

Baumeister, D., Roos, M., Rothe, J., Schend, L., & Xia, L. (2012b). possible winner
problem uncertain weights. Proceedings 20th European Conference
Artificial Intelligence, pp. 133138.
Brandt, F., Brill, M., Hemaspaandra, E., & Hemaspaandra, L. (2010). Bypassing combinatorial protections: Polynomial-time algorithms single-peaked electorates.
Proceedings 24th AAAI Conference Artificial Intelligence, pp. 715722.
Brandt, F., Conitzer, V., & Endriss, U. (2013). Computational social choice. Wei, G.
(Ed.), Multiagent Systems (2nd edition). MIT Press.
Brelsford, E., Faliszewski, P., Hemaspaandra, E., Schnoor, H., & Schnoor, I. (2008). Approximability manipulating elections. Proceedings 23rd AAAI Conference
Artificial Intelligence, pp. 4449. AAAI Press.
Chen, J., Faliszewski, P., Niedermeier, R., & Talmon, N. (2014). Combinatorial voter control
elections. Proceedings 39th International Symposium Mathematical
Foundations Computer Science, Part II, pp. 153164. Springer-Verlag Lecture Notes
Computer Science #8635.
Chevaleyre, Y., Lang, J., Maudet, N., Monnot, J., & Xia, L. (2012). New candidates welcome! Possible winners respect addition new candidates. Mathematical
Social Sciences, 64 (1), 7488.
Congleton, R. (2011). Swedish transition democracy (Chapter 14). Perfecting
Parliament. Cambridge University Press.
Conitzer, V., Sandholm, T., & Lang, J. (2007). elections candidates
hard manipulate?. Journal ACM, 54 (3), Article 14.
Dwork, C., Kumar, R., Naor, M., & Sivakumar, D. (2001). Rank aggregation methods
web. Proceedings 10th International World Wide Web Conference, pp.
613622. ACM Press.
Elkind, E., & Faliszewski, P. (2010). Approximation algorithms campaign management. Proceedings 6th International Workshop Internet Network
Economics, pp. 473482.
Elkind, E., Faliszewski, P., & Slinko, A. (2011). Cloning elections: Finding possible
winners. Journal Artificial Intelligence Research, 42, 529573.
Elkind, E., Faliszewski, P., & Slinko, A. (2012). Clone structures voters preferences.
Proceedings 13th ACM Conference Electronic Commerce, pp. 496513.
Ephrati, E., & Rosenschein, J. (1997). heuristic technique multi-agent planning.
Annals Mathematics Artificial Intelligence, 20 (14), 1367.
Erdelyi, G., Fellows, M., Rothe, J., & Schend, L. (2015a). Control complexity Bucklin
fallback voting: theoretical analysis. Journal Computer System Sciences,
81 (4), 632660.
Erdelyi, G., Fellows, M., Rothe, J., & Schend, L. (2015b). Control complexity Bucklin
fallback voting: experimental analysis. Journal Computer System Sciences,
81 (4), 661670.
539

fiFaliszewski, Hemaspaandra, & Hemaspaandra

Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2009). hard bribery
elections?. Journal Artificial Intelligence Research, 35, 485532.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2010). Using complexity protect
elections. Communications ACM, 53 (11), 7482.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2011). Multimode attacks
elections. Journal Artificial Intelligence Research, 40, 305351.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2013). Weighted electoral control. Proceedings 12th International Conference Autonomous Agents
Multiagent Systems, pp. 367374.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2014). complexity manipulative attacks nearly single-peaked electorates. Artificial Intelligence, 207, 6999.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). richer understanding complexity election systems. Ravi, S., & Shukla, S. (Eds.), Fundamental Problems Computing: Essays Honor Professor Daniel J. Rosenkrantz,
pp. 375406. Springer.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2011). shield
never was: Societies single-peaked preferences open manipulation
control. Information Computation, 209 (2), 89107.
Faliszewski, P., Hemaspaandra, E., & Schnoor, H. (2008). Copeland voting: Ties matter.
Proceedings 7th International Conference Autonomous Agents Multiagent Systems, pp. 983990. International Foundation Autonomous Agents
Multiagent Systems.
Fitzsimmons, Z., Hemaspaandra, E., & Hemaspaandra, L. (2013). Control presence manipulators: Cooperative competitive cases. Proceedings 23rd
International Joint Conference Artificial Intelligence, pp. 113119. AAAI Press.
Garey, M., & Johnson, D. (1979). Computers Intractability: Guide Theory
NP-Completeness. W. H. Freeman Company.
Gehrlein, W., & Lepelley, D. (2012). Voting Paradoxes Group Coherence: Condorcet
Efficiency Voting Rules. Springer.
Ghosh, S., Mundhe, M., Hernandez, K., & Sen, S. (1999). Voting movies: anatomy
recommender systems. Proceedings 3rd Annual Conference Autonomous
Agents, pp. 434435. ACM Press.
Hemaspaandra, E., & Hemaspaandra, L. (2007). Dichotomy voting systems. Journal
Computer System Sciences, 73 (1), 7383.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Anyone him: complexity
precluding alternative. Artificial Intelligence, 171 (56), 255285.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). Hybrid elections broaden
complexity-theoretic resistance control. Mathematical Logic Quarterly, 55 (4), 397
424.
540

fiWeighted Electoral Control

Hemaspaandra, E., Hemaspaandra, L., & Schnoor, H. (2014). control dichotomy pure
scoring rules. Proceedings 28th AAAI Conference Artificial Intelligence,
pp. 712720. AAAI Press.
Hemaspaandra, L., Lavaee, R., & Menton, C. (2013). Schulze ranked-pairs voting
fixed-parameter tractable bribe, manipulate, control. Proceedings
12th International Conference Autonomous Agents Multiagent Systems, pp.
13451346.
Hemaspaandra, L., & Williams, R. (2012). atypical survey typical-case heuristic
algorithms. SIGACT News, 43 (4), 7189.
Kolliopoulos, S., & Young, N. (2005). Approximation algorithms covering/packing integer programs. Journal Computer System Sciences, 71 (4), 495505.
Lin, A. (2012). Solving Hard Problems Election Systems. Ph.D. thesis, Rochester Institute
Technology, Rochester, NY.
Lu, T., & Boutilier, C. (2011). Budgeted social choice: consensus personalized decision making. Proceedings 22nd International Joint Conference Artificial
Intelligence, pp. 280286.
McGarvey, D. (1953). theorem construction voting paradoxes. Econometrica,
21 (4), 608610.
Meir, R., Procaccia, A., Rosenschein, J., & Zohar, A. (2008). complexity strategic
behavior multi-winner elections. Journal Artificial Intelligence Research, 33,
149178.
Menton, C. (2013). Normalized range voting broadly resists control. Theory Computing
Systems, 53 (4), 507531.
Menton, C., & Singh, P. (2013). Control complexity Schulze voting. Proceedings
23rd International Joint Conference Artificial Intelligence, pp. 286292.
Parkes, D., & Xia, L. (2012). complexity-of-strategic-behavior comparison
Schulzes rule ranked pairs. Proceedings 26th AAAI Conference
Artificial Intelligence, pp. 14291435.
Perek, T., Faliszewski, P., Pini, M., & Rossi, F. (2013). complexity losing voters. Proceedings 12th International Conference Autonomous Agents
Multiagent Systems, pp. 407414.
Rothe, J., & Schend, L. (2013). Challenges complexity shields supposed
protect elections manipulation control: survey. Annals Mathematics
Artificial Intelligence, 68 (13), 161193.
Russell, N. (2007). Complexity control Borda count elections.
Rochester Institute Technology.

Masters thesis,

Waggoner, B., Xia, L., & Conitzer, V. (2012). Evaluating resistance false-name manipulations elections. Proceedings 26th AAAI Conference Artificial
Intelligence, pp. 14851491.
Wagman, L., & Conitzer, V. (2014). False-name-proof voting costs two alternatives. International Journal Game Theory, 43 (3), 599618.
541

fiFaliszewski, Hemaspaandra, & Hemaspaandra

Wojtas, K., & Faliszewski, P. (2012). Possible winners noisy elections. Proceedings
26th AAAI Conference Artificial Intelligence, pp. 14991505.
Xia, L. (2012). many vote operations needed manipulate voting system?.
Proceedings (Workshop Notes) 4th International Workshop Computational
Social Choice, pp. 443454.
Xia, L., Lang, J., & Monnot, J. (2011). Possible winners new alternatives join:
New results coming up!. Proceedings 10th International Conference Autonomous Agents Multiagent Systems, pp. 829836. International Foundation
Autonomous Agents Multiagent Systems.

542

fiJournal Artificial Intelligence Research 52 (2015) 361-398

Submitted 7/14; published 3/15

Inferring Team Task Plans Human Meetings:
Generative Modeling Approach Logic-Based Prior
Kim
Caleb M. Chacha
Julie A. Shah

beenkim@csail.mit.edu
c chacha@csail.mit.edu
julie shah@csail.mit.edu

Massachusetts Institute Technology
77 Massachusetts Ave. 02139, USA

Abstract
aim reduce burden programming deploying autonomous systems
work concert people time-critical domains military field operations
disaster response. Deployment plans operations frequently negotiated on-thefly teams human planners. human operator translates agreed-upon plan
machine instructions robots. present algorithm reduces translation burden inferring final plan processed form human teams planning
conversation. hybrid approach combines probabilistic generative modeling logical
plan validation used compute highly structured prior possible plans, enabling us
overcome challenge performing inference large solution space
small amount noisy data team planning session. validate algorithm
human subject experimentations show able infer human teams
final plan 86% accuracy average. also describe robot demonstration
two people plan execute first-response collaborative task PR2 robot.
best knowledge, first work integrate logical planning technique within
generative model perform plan inference.

1. Introduction
Robots increasingly introduced work concert people high-intensity
domains military field operations disaster response. example, robot deployment allow access areas would otherwise inaccessible people (Casper
& Murphy, 2003; Micire, 2002), inform situation assessment (Larochelle, Kruijff, Smets,
Mioch, & Groenewegen, 2011). human-robot interface long identified major bottleneck utilizing robotic systems full potential (Murphy, 2004).
result, significant research efforts aimed easing use systems
field, including careful design validation supervisory control interfaces (Jones,
Rock, Burns, & Morris, 2002; Cummings, Brzezinski, & Lee, 2007; Barnes, Chen, Jentsch,
& Redden, 2011; Goodrich, Morse, Engh, Cooper, & Adams, 2009). Much prior
work focused ease use execution time. However, significant bottleneck also
exists planning deployment autonomous systems programming
systems coordinate task execution human team. Deployment plans frequently
negotiated human team members on-the-fly time pressure (Casper & Murphy,
2002, 2003). robot aid execution plan, human operator must
transcribe translate result team planning session.
c
2015
AI Access Foundation. rights reserved.

fiKim, Chacha & Shah

paper, present algorithm reduces translation burden inferring
final plan processed form human teams planning conversation. Inferring
plan noisy incomplete observation formulated plan recognition
problem (Ryall, Marks, & Shieber, 1997; Bauer, Biundo, Dengler, Koehler, & Paul, 2011;
Mayfield, 1992; Charniak & Goldman, 1993; Carberry, 1990; Grosz & Sidner, 1990; Gal,
Reddy, Shieber, Rubin, & Grosz, 2012). noisy incomplete characteristics observation stem fact observed data (e.g., teams planning conversation)
part plan trying infer, entire plan may
observed. focus existing plan recognition algorithms often search existing
knowledge base given noisy observation. However, deployment plans emergency situations seldom same, making infeasible build knowledge base. addition,
planning conversations often conducted time pressure are, therefore, often
short (i.e., contain small amount data). Shorter conversations result limited amount
available data inference, often making inference problem challenging.
approach combines probabilistic generative modeling logical plan validation,
used compute highly structured prior possible plans. hybrid approach
enables us overcome challenge performing inference large solution space
small amount noisy data collected team planning session.
work, focus inferring final plan using text data logged
chat transcribed speech. Processing human dialogue machine-understandable
forms important research area (Kruijff, Jancek, & Lison, 2010; Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, & Roy, 2011; Koomen, Punyakanok, Roth, & Yih, 2005;
Palmer, Gildea, & Xue, 2010; Pradhan, Ward, Hacioglu, Martin, & Jurafsky, 2004),
view separate problem focus paper.
form input use preserves many challenging aspects natural human
planning conversations, thought noisy observation final plan.
team discussing plan time pressure, planning sessions often consist
small number succinct communications. approach infer final agreed-upon
plan using single planning session, despite small amount noisy data.
validate algorithm experiments 96 human subjects show
able infer human teams final plan 86% accuracy average. best
knowledge, first work integrate logical planning technique within
generative model perform plan inference.
summary, work includes following contributions:
formulate novel problem performing inference extract finally agreedupon plan human team planning conversation.
propose validate hybrid approach perform inference applies
logic-based prior probability space possible agreed-upon plans.
approach performs efficient inference probabilistic generative model.
demonstrate benefit approach using human team meeting data collected
large-scale human subject experiments (total 96 subjects) able infer
human teams final plan 86% accuracy average.
362

fiA Generative Modeling Approach Logic-Based Prior

work extends preliminary version work (Kim, Chacha, & Shah, 2013)
include inference complex durative task plans infer plans new
information becomes available human team. addition, extend probabilistic
model flexible different data sets learning hyper-parameters. also
improve performance algorithm designing better proposal distribution
inference.
formulation problem presented Section 2, followed technical
approach related work Section 3. algorithm described Section 4.
evaluation algorithm using various data sets shown Sections 5 6. Finally,
discuss benefits limitations current approach Section 7, conclude
considerations future work Section 8.

2. Problem Formulation
Disaster response teams increasingly utilizing web-based planning tools plan deployments (Di Ciaccio, Pullen, & Breimyer, 2011). Hundreds responders access tools
develop plans using audio/video conferencing, text chat annotatable maps.
Rather working raw, natural language, algorithm takes structured form
human dialogue web-based planning tools input. goal work
infer human teams final plan human dialogue. so, work
used design intelligent agent planning tools actively participate
planning session improve teams decision.
section describes formal definition, input output problem. Formally,
problem viewed one plan recognition, wherein plan follows formal
representation Planning Domain Description Language (PDDL). PDDL
widely used planning research community planning competitions (i.e.,
International Planning Competition). plan valid achieves user-specified goal state
without violating user-specified plan constraints. Actions may constrained execute
sequence parallel actions. plan constraints include discrete
resource constraints (e.g. presence two medical teams) temporal deadlines
time-durative actions (e.g. robot deployed 1 hour time due
battery life constraints).
assume team reaches agreement final plan. techniques introduced
Kim Shah (2014) used detect strength agreement,
encourage team discuss plan reach agreement necessary. Situations
team agrees upon flexible plan multiple options explored
included future study. Also, assume team likely agree
valid plan, rule possibility final plan invalid.
2.1 Algorithm Input
Text data human team conversation collected form utterances,
utterance one persons turn discussion, shown Table 1. input
algorithm machine-understandable form human conversation data, illustrated
right-hand column Table 1. structured form captures actions discussed
proposed ordering relations among actions utterance.
363

fiKim, Chacha & Shah

Natural dialogue

U1

U2
U3

U4

U5

U6
U7

suggest using Red robot cover upper
rooms (A, B, C, D) Blue robot cover
lower rooms (E, F, G, H).
Okay. first send Red robot B Blue
robot G?
order inspection would (B, C, D, A)
Red (G, F, E, H) Blue.

Oops meant (B, D, C, A) Red.

medical crew go B
robot inspecting C

First, Red robot inspects B
Yes, Red robot inspects D, Red medical crew treat B

Structured form (ordered tuple
sets grounded predicates)
({ST(rr,A),ST(br,E),
ST(rr,B),ST(br,F),
ST(rr,C),ST(br,G),
ST(rr,D),ST(br,H)})
({ST(rr,B),ST(br,G)})
({ST(rr,B),ST(br,G)},
{ST(rr,C),ST(br,F)},
{ST(rr,D),ST(br,E)},
{ST(rr,A), ST(br,H)})
({ST(rr,B)},{ST(rr,D)},
{ST(rr,C)},{ST(rr,A)})
({ST(m,B), ST(r,C)})

({ST(r,B)})
({ST(r,D),ST(rm,B)})

Table 1: Utterance tagging: Dialogue structured form examples. (The structured form
uses following shorthand - ST: send to, rr: red robot, br: blue robot, rm: red medical,
bm: blue medical, e.g. ST(br,A) : send blue robot room A.)

Although working raw, natural language, form data still captures many characteristics make plan inference based human conversation
challenging. Table 1 shows part data using following shorthand:
ST = SendTo
rr = red robot, br = blue robot
rm = red medical, bm = blue medical
e.g. ST(br, A) = SendTo(blue robot, room A)

2.1.1 Utterance Tagging
utterance tagged ordered tuple sets grounded predicates. Following
formal definition first-order languages, grounded predicate atomic formula whose
argument terms grounded (i.e., free variables; variables assigned value).
case, predicate represents action applied set objects (a crew member,
robot, room, etc.), utterance represented ordered sets actions.
consider utterances related plan formation; greetings jokes, example,
tagged. set grounded predicates represents collection actions that, according
utterance, happen simultaneously. order sets grounded predicates
indicates relative order collections actions happen. example,
364

fiA Generative Modeling Approach Logic-Based Prior

({ST(rr, B), ST(br, G)}, {ST(rm, B)}) corresponds sending red robot room B
blue robot room G simultaneously, followed sending red medical team
room B.
indicated Table 1, structured dialogue still includes high levels noise.
utterance (i.e. U1-U7) discusses partial plan, predicates explicitly mentioned
utterance tagged (e.g. U6-U7: U7 implies sequencing constraint
predicate discussed U6, structured form U7 include ST(r,B)).
Typos misinformation tagged without correction (e.g. U3), utterances
indicating need revise information placed context (e.g. U4). Utterances
clearly violate ordering constraints (e.g. U1: actions cannot happen
time) also tagged without correction. addition, information regarding whether
utterance suggestion, rejection agreement partial plan coded.
Note utterance tagging contains information relative ordering predicates appearing utterance, absolute ordering appearance final plan. example, U2 specifies two grounded predicates happen
time. state two predicates happen final plan,
whether predicates happen parallel. simulates humans conversations
often unfold utterance, humans observe relative ordering, infer
absolute order predicates based whole conversation understanding
orderings would make valid plan. utterance tagging scheme also designed
support future transition automatic natural language processing. Automatic semantic role labeling (Jurafsky & Martin, 2000), example, used detect
arguments predicates sentences. One challenges incorporating semantic
role labeling system dialogue experiments often colloquial
key grammatical components sentences often omitted. Solving problem
processing free-form human dialogue machine-understandable forms important research area, view separate problem focus
paper.
2.2 Algorithm Output
output algorithm inferred final plan, sampled probability distribution final plans. final plan representation structured
utterance tags (ordered tuple sets grounded predicates). predicates set
represent actions happen parallel, ordering sets indicates sequence. Unlike utterance tags, however, sequence ordering relations final
plan represent absolute order actions carried out. example
plan ({A1 , A2 }, {A3 }, {A4 , A5 , A6 }), Ai represents predicate. plan, A1
A2 happen step 1 plan, A3 happens step 2 plan, on.

3. Approach Nutshell Related Work
Planning conversations performed time pressure exhibit unique characteristics
challenges inferring final plan. First, planning conversations succinct
participants tend write shortly briefly, hurry make final decision.
Second, may number different valid plans teams deployment even
365

fiKim, Chacha & Shah

Figure 1: Web-based tool developed used data collection

366

fiA Generative Modeling Approach Logic-Based Prior

simple scenario, people tend generate broad range final plans. represents
typical challenges faced real rescue missions, incident unique
participants cannot library plans choose time. Third,
conversations noisy often, many suggestions made rejected quickly
would casual setting. addition, likely many
repeated confirmations agreement, might typically ease detection agreedupon plan.
might seem natural take probabilistic approach plan inference problem,
working noisy data. However, combination small amount noisy
data large number possible plans means inference using typical, uninformative
priors plans may fail converge teams final plan timely manner.
problem could also approached logical constraint problem partial order
planning, noise utterances: team discuss partial
plans relating final plan, without errors revisions, plan generator
scheduler (Coles, Fox, Halsey, Long, & Smith, 2009) could produce final plan using
global sequencing. Unfortunately, data collected human conversation sufficiently
noisy preclude approach.
circumstances provided motivation combined approach, wherein built
probabilistic generative model used logic-based plan validator (Howey, Long, &
Fox, 2004) compute highly structured prior distribution possible plans. Intuitively
speaking, prior encodes assumption final plan likely, required,
valid plan. approach naturally deals noise data
challenge performing inference plans limited amount data.
performed sampling inference model using Gibbs Metropolis-Hastings sampling
approximate posterior distribution final plans, empirical validation
human subject experiments indicated algorithm achieves 86% accuracy average.
details model inference methods presented Section 4.
related work categorized two categories: 1) application 2) technique.
terms application, work relates plan recognition (Section 3.1). terms
technique, approach relates methods combine logic probability, though
different focused applications (Section 3.2).
3.1 Plan Recognition
Plan recognition area interest within many domains, including interactive
software (Ryall et al., 1997; Bauer et al., 2011; Mayfield, 1992), story understanding (Charniak & Goldman, 1993) natural language dialogue (Carberry, 1990; Grosz & Sidner,
1990).
literature categorized two ways. first terms requirements.
studies (Lochbaum, 1998; Kautz, 1987) require library plans, others (Zhuo,
Yang, & Kambhampati, 2012; Ramrez & Geffner, 2009; Pynadath & Wellman, 2000;
Sadilek & Kautz, 2010) replace library relevant structural information. library
plans required, studies (Weida & Litman, 1992; Kautz, Pelavin, Tenenberg, &
Kaufmann, 1991) assumed library collected, future plans
guaranteed included within collected library. contrast, library plans
367

fiKim, Chacha & Shah

required, replaced related structure information, domain theory
possible set actions performable agents. second categorization
literature terms technical approach. studies incorporated constraint-based
approaches, others took probabilistic combination approaches.
First, reviewed work treated plan recognition knowledge base search problem. method assumes either build knowledge base,
goal efficiently search knowledge base (Lochbaum, 1998; Kautz, 1987).
approach often includes strong assumptions regarding correctness completeness
plan library, addition restrictions noisy data (Weida & Litman, 1992; Kautz
et al., 1991), applicable domains plan reoccurs naturally.
example, Gal et al. (2012) studied adaptively adjust educational content better
learning experience, given students misconceptions, using computer-based tutoring tool.
Similarly, Brown Burton (1978) investigated users underlying misconceptions using
user data collected multiple sessions spent trying achieve goal. terms
technical approach, approaches used logical methods solve ordering
constraints problem searching plan library.
also reviewed work replaced knowledge base structural information
planning problem. Zhuo et al. (2012) replaced knowledge base action models
domain, formulated problem one satisfiability recognize multi-agent
plans. similar approach taken Ramrez Geffner (2009), wherein action models
used replace plan library, Pynadath Wellman (2000) incorporated
extension probabilistic context free grammars (PCFGs) encode set predefined
actions improve efficiency. recently, Markov logic applied model geometry, motion model rules recognition multi-agent plans playing game
capture flag (Sadilek & Kautz, 2010). Replacing knowledge base structural
information reduces amount prior information required. However, two major issues application prior work recognize plans team conversation:
First, work assumed repetition previous plans. example, learning
weights Markov logic (which represent importance strictness constraints)
requires prior data mission, conditions resources. Second, using first-order logic express plan constraints quickly becomes computationally
intractable complexity plan increases.
contrast logical approaches, probabilistic approaches allow noisy observations.
Probabilistic models used predict users next action, given noisy data (Albrecht,
Zuckerman, Nicholson, & Bud, 1997; Horvitz, Breese, Heckerman, Hovel, & Rommelse,
1998). works use actions normally performed users training data.
However, approaches consider particular actions (e.g., actions must
performed users achieve certain goals within software system) likely.
words, deal noisy data, incorporate structural
information could perhaps guide plan recognition algorithm. additional limitation methods assume predefined domains. defining domain,
set possible plans limited, possible plans time-critical missions generally
limited set. situation available resources incident likely
different. method recognize plan noisy observations, open
set possible plans, required.
368

fiA Generative Modeling Approach Logic-Based Prior

probabilistic approaches incorporate structure format plan library. Pynadath Wellman (2000) represented plan libraries probabilistic context
free grammars (PCFGs). used build Bayes networks modeled underlying generative process plan construction. However, parsing-based approaches
deal partially-ordered plans temporally interleaved plans. Geib et al. (2008)
Geib Goldman (2009) overcame issue working directly plan representation without generating intermediate representation form belief network.
time step, technique observed previous action agent generated
pending action set. approach, too, assumed existing plan library relied
domains repetition previous plans. recently, Nguyen, Kambhampati,
(2013) introduced techniques address incomplete knowledge plan library,
plan generation rather plan recognition applications.
approach combines probabilistic approach logic-based prior infer team
plans without need historical data, using situational information data
single planning session. situational information includes operators resources
domain problem specifications, may updated modified one
scenario another. require development addition plan library infer
plan, demonstrate solution robust incomplete knowledge planning
problem.
3.2 Combining Logic Probability
combination logical approach probabilistic modeling gained interest
recent years. Getoor Mihalkova (2011) introduced language description statistical models typed relational domains, demonstrated model learning using noisy
uncertain real-world data. Poon Domingos (2006) proposed statistical sampling
improve searching efficiency satisfiability testing. particular, combination
first-order logic probability, often referred Markov Logic Networks (MLN),
studied. MLN forms joint distribution probabilistic graphical model weighting
formulas first-order logic (Richardson & Domingos, 2006; Singla & Domingos, 2007;
Poon & Domingos, 2009; Raedt, 2008).
approach shares MLNs philosophy combining logical tools probabilistic modeling. MLNs utilize first-order logic express relationships among objects.
General first-order logic allows use expressive constraints across various applications. However, within planning domain, enumerating constraints first-order logic
quickly becomes intractable complexity plan increases. example, first-order
logic allow explicit expression action preconditions postconditions, let
alone constraints among actions. PDDL well-studied planning research
community (McDermott, Ghallab, Howe, Knoblock, Ram, Veloso, Weld, & Wilkins, 1998),
main focus develop efficient ways express solve planning problems.
approach exploits tool build highly structured planning domain within
probabilistic generative model framework.

369

fiKim, Chacha & Shah

4. Algorithm
section presents details algorithm. describe probabilistic generative
model indicate model combined logic-based prior perform efficient inference. generative model specifies joint probability distribution observed
variables (e.g., human team planning conversation) latent variables (e.g., final plan);
model learns distribution teams final plan, incorporating logic-based
prior (plan validation tool). key contribution design generative model
logic-based prior. also derive Gibbs sampling (Andrieu, De Freitas, Doucet, &
Jordan, 2003) representation design scheme applying Metropolis-Hasting sampling (Metropolis, Rosenbluth, Rosenbluth, Teller, & Teller, 1953) performing inference
model.
4.1 Generative Model
model human team planning process, represented dialogue, probabilistic
Bayesian model. particular, utilize probabilistic generative modeling approach
extensively used topic modeling (e.g., Blei, Ng, & Jordan, 2003).
start plan latent variable must inferred observation utterances made planning session. model generates utterance
conversation sampling subset predicates plan computing relative ordering appear within utterance. mapping absolute ordering
plan relative ordering predicates utterance described detail
below. Since conversation short level noise high, model
distinguish utterances based order appear conversation.
assumption produces simple yet powerful model, simplifies inference steps
enables 86% accuracy inference final plan. However, model
also generalized take ordering account simple extension. include
discussion assumption extension Section 7.4.
following step-by-step description generative model:
1. Variable plan: plan variable Figure 2 defined ordered tuple sets
grounded predicates, represents final plan agreed upon team.
distributed space ordered tuples sets grounded predicates. assume
total number grounded predicates one domain fixed.
prior distribution plan variable given by:
(
e plan valid
p(plan)
1
plan invalid.

(1)

positive number. models assumption final plan
likely, necessarily required, valid plan.
likelihood plan defined as:

370

fiA Generative Modeling Approach Logic-Based Prior



k





kp

plan

p

snt

pnt

p

N

s0t


Figure 2: Graphical model representation generative model. plan latent variable
represents final agreed-upon plan. pnt variable represents nth predicate tth
utterance, snt represents absolute ordering predicate plan.
s0t represents relative ordering sn within utterance t. latent variable p
represents noisiness predicates, represents noisiness ordering.

p(s, p|plan, p )

YY



YY





p(snt , pnt |plan, p )

n

p(pnt |plan, snt , , p)p(snt |plan)

n

set predicates plan assigned consecutive absolute plan step index s,
starting = 1 working left right ordered tuple. example, given
plan = ({A1 , A2 }, {A3 }, {A4 , A5 , A6 }), Ai predicate, A1 A2 occur
parallel plan step = 1, A6 occurs plan step = 3.
2. Variable snt : snt represents step index (i.e., absolute ordering) nth predicate
tth utterance plan. step index represents absolute timestamp
predicate plan. words, snt indicates absolute order predicate
pnt appears plan. use st = {s1t , s2t . . . } represent vector orderings
tth utterance, vector st may set consecutive numbers.
snt sampled follows: utterance, n predicates sampled plan.
example, consider n = 2, first sampled predicate appears second
set (i.e., second timestamp) plan second sampled predicate appears
fourth set. conditions, s1t = 2 s2t = 4. probability set
sampled proportional number predicates contained within set.
example, given plan = ({A1 , A2 }, {A3 }, {A4 , A5 , A6 }), probability selecting
first set ({A1 , A2 }) 62 . models notion people likely
371

fiKim, Chacha & Shah

discuss plan steps include many predicates, since plan steps many actions
may require effort elaborate. Formally:

p(snt = i|plan) =

# predicates set plan
.
total # predicates plan

(2)

likelihood st defined as:

0

0

p(st |pt , st ) p(st |plan)p(pt , st |st , , p , plan)

0
p(st |st , )
p(snt |plan)p(pnt |plan, snt , p )
n
0

0

0

3. Variable s0t : variable s0t array size n, s0t = {st1 , st2 . . . stn }.
0
stn random variable represents relative ordering nth predicate within
tth utterance plan, respect grounded predicates appearing
tth utterance.
s0t generated st follows:
(
e
p(s0t |st )
1

s0t = f (st )
s0t =
6 f (st ).

(3)

> 0. random variable hyper-parameter represents noisiness
ordering grounded predicates appearing throughout entire conversation.
takes scalar value, sampled gamma distribution:
p(|k , ) Gamma(k , ),

(4)

k set 10.
function f deterministic mapping absolute ordering st relative
ordering s0t . f takes vector absolute plan step indices input, produces
vector consecutive indices. example, f maps st = (2, 4) s0t = (1, 2)
st = (5, 7, 2) s0t = (2, 3, 1).
variable models way predicates orders appear human conversation: People frequently use relative terms, after, describe
partial sequences full plan, often refer absolute ordering. People
also make mistakes otherwise incorrectly specify ordering. model allows
inconsistent relative orderings nonzero probability; types mistakes
modeled value .
4. Variable pnt p : variable pnt represents nth predicate appearing
tth utterance. absolute ordering grounded predicate snt . pnt
sampled given snt , plan variable parameter, p .
372

fiA Generative Modeling Approach Logic-Based Prior

p random variable (hyper-parameter) represents noisiness grounded predicates appearing throughout entire conversation. takes scalar value,
sampled beta distribution:
p(p |kp , p ) beta(kp , p ),

(5)

kp set 40, p set 10.
probability p , sample predicate pnt uniformly replacement
correct set snt plan follows:
(
1
set j
n
n
p(pt = i|plan, st = j) = # pred. set j
0
o.w.
probability 1 p , sample predicate pnt uniformly replacement
set plan (i.e., predicates mentioned dialogue). Therefore:
p(pnt = i|plan, snt = j) =

1
.
total # predicates

(6)

words, higher probability p , sample value pnt consistent
snt allows nonzero probability pnt sampled random plan.
allows model incorporate noise planning conversation, including
mistakes plans later revised.
4.2 Plan Validation Tool
use Planning Domain Description Language (PDDL) 2.1 plan validation tool (Howey
et al., 2004) evaluate prior distribution possible plans. section, briefly
review PDDL, plan validation tool used form prior Equation
1.
4.2.1 Planning Domain Description Language
Planning Domain Description Language (PDDL) (McDermott et al., 1998) standard
planning language, inspired Stanford Research Institute Problem Solver (STRIPS)
(Fikes & Nilsson, 1972) Action Description Language (ADL) (Pednault, 1987),
utilized International Planning Competition.
PDDL model planning problem two major components: domain specification
problem specification. domain description consists domain-name definition,
requirements language expressivity, definition object types, definition constant
objects, definition predicates (i.e. templates logical facts), definition
possible actions instantiated execution. Actions parameters may
instantiated objects, preconditions, conditional unconditional effects.
excerpt PDDL domain file used work, called RESCUE domain, shown
below. example, predicate isSaved encodes logical fact whether
particular patient rescued, action SENDROBOT instantiated
execution send particular robot particular location.
373

fiKim, Chacha & Shah

(define (domain RESCUE)
(:requirements :typing :durativeactions :negativepreconditions)
(:types patient valve thingsToFix location location
medcrew mechanic robot resource)
(:predicates
(isAt ?p thingsToFix ?l location)
(isSaved ?p patient)
(isFixed ?v valve)
(isInspected ?l location)
(isAvail ?r resource)
)
(:durativeaction SENDROBOT
:parameters (?r robot ?l location)
:duration (= ?duration 1)
:condition (and
(at start (isAvail ?r))
(at start (not (isInspected ?l)))
)
:effect (and
(at start (not (isAvail ?r) ) )
(at end (isAvail ?r))
(at end (isInspected ?l))
)
)
...)

problem specification consists problem-name definition, definition
related domain-name, definition possible objects relevant planning
problem, initial state planning environment conjunction true/false facts,
definition goal-states logical expression true/false facts. excerpt
PDDL problem specification file used work shown below. init section
describes initial conditions example, patient pB initially situated location B,
patient pD D. goal section indicates desired final state case
rooms must inspected, patients must rescued, valves fixed. metric
section defines metric planner optimizes producing plan.

374

fiA Generative Modeling Approach Logic-Based Prior

(define (problem rescuepeople)
(:domain RESCUE)
(:objects
pB pD pG patient
vC vF valve
B C E F G H location
redMed blueMed medcrew
redR blueR robot
mech1 mechanic)
(:init
(isAt pB B)
(isAt pD D)
...
(isAvail redMed)
(isAvail blueMed)
(isAvail redR)
(isAvail blueR)
(isAvail mech1)
(not (isSaved pB))
...
(not (isInspected A))
(not (isInspected B))
...
(not (isFixed vC))
(not (isFixed vF))
)
(:goal (and
(isSaved pB)
(isSaved pD)
...
(isFixed vC)
(isFixed vF)
(isInspected A)
(isInspected B)
...
)
)
;(:metric minimize (totaltime))
)

work, note domain specification could reused one planning
session another capabilities team change. example, set
possible set actions defined domain specification, may sufficient merely modify
number names locations, medical crews, robots problem specification.
domain problem specification files represent prior knowledge
approach requires order infer final agreed-upon plan. valid plan defined
totally partially ordered sequence grounded predicates achieves goal state
initial state, without violating constraints. Otherwise, plan invalid.
next section describes plan validation tool assesses validity given plan, given
domain problem specification files.
4.2.2 Plan Validation Tool (VAL)
plan validator standard tool takes input planning problem described
PDDL proposed solution plan. tool incorporates three input files: 1) domain
definition file, 2) problem definition file 3) proposed solution plan file. domain

375

fiKim, Chacha & Shah

definition file contains types parameters (e.g., resources, locations), predicate definitions
actions (which also parameters, conditions effects). problem definition
file contains information specific situation: example, number locations
victims, initial goal conditions metric optimize. proposed solution plan file
contains single complete plan, described PDDL. output plan validation tool
indicates whether proposed solution plan valid (true) (false).
Metrics represent ways compute plan quality value. purpose study,
metrics used included: 1) minimization total execution time radioactive
material leakage scenario, 2) maximization number incidents responded
police incident response scenario. Intuitively speaking, metrics reflect rational
behavior human experts. natural assume human experts would try
minimize total time completion time-critical missions (such radioactive
material leakage scenario). first responders cannot accomplish necessary tasks
scenario due limited availability resources, would likely try maximize
number completed tasks (such police incident response scenario).
One could imagine input files could reused subsequent missions,
capabilities (actions) team may change dramatically. However, number
available resources might vary, might rules implicit specific situation
encoded files (e.g., save endangered humans first fixing damaged
bridge). Section 6, demonstrate robustness approach using complete
degraded PDDL plan specifications.
computation plans validity generally cheaper valid plan generation. gives us way compute p(plan) (defined Section 4.1) proportionality
computationally efficient manner. Leveraging efficiency, use Metropolis-Hastings
sampling, (details described Section 4.3.1) without calculating partition function.
4.3 Gibbs Sampling
use Gibbs sampling perform inference generative model. four latent
variables sample: plan, collection variables snt , p . iterate
sampling latent variable, given variables. PDDL validator used
plan variable sampled.
4.3.1 Sampling Plan using Metropolis-Hastings
Unlike snt , write analytic form sample posterior,
intractable directly resample plan variable, would require calculating
number possible plans, valid invalid. Therefore, use Metropolis-Hasting
(MH) algorithm sample plan posterior distribution within Gibbs sampling
steps.

376

fiA Generative Modeling Approach Logic-Based Prior

posterior plan represented product prior likelihood,
follows:
p(plan|s, p) p(plan)p(s, p|plan)
= p(plan)


N


p(snt , pnt |plan)

t=1 n=1

= p(plan)


N


p(snt |plan)p(pnt |plan, snt )

(7)

t=1 n=1

MH sampling algorithm widely used sample distribution direct
sampling difficult. algorithm allows us sample posterior distribution according user-specified proposal distribution without calculate partition
function. typical MH algorithm defines proposal distribution, Q(x0 |xt ), samples new point (i.e., x0 : value plan variable case) given current point
xt . new point achieved randomly selecting one several possible moves,
defined below. proposed point accepted rejected, probability
min(1, acceptance ratio).
Unlike simple cases, Gaussian distribution used proposal distribution,
distribution needs defined plan space. Recall plan represented
ordered tuple sets predicates. work, new point (i.e., candidate plan)
generated performing one following moves current plan:
Move next: Randomly select predicate current plan, move
next timestamp. last timestamp plan, move first
timestamp.
Move previous: Randomly select predicate current plan, move
previous timestamp. first timestamp plan, move
last timestamp.
Add predicate plan: Randomly select predicate current plan,
randomly choose one timestamp current plan. Add predicate
chosen timestamp.
Remove predicate plan: Randomly select remove predicate
current plan.
moves sufficient allow movement one arbitrary plan another.
intuition behind designing proposal distribution described Section 7.5.
Note proposal distribution, is, symmetrical Q(x0 |xt ) 6= Q(xt |x0 ).
need compensate according following,
p (x0 )Q(x0 |xt ) = p (x)Q(xt |x0 ),

(8)

p target distribution. done simply counting number
moves possible x0 get x, x0 x, weighing acceptance ratio
377

fiKim, Chacha & Shah

Equation 8 true. often referred Hastings correction, performed
ensure proposal distribution favor states others.
Next, ratios proposal distribution current proposed points
calculated. plan valid, p(plan) proportional e , plan invalid,
proportional 1, described Equation 1. Plan validity calculated using
plan validation tool. remaining term, p(snt |plan)p(pnt |plan, snt ), calculated using
Equations 2 6.
Then,
proposed
plan accepted following probability:
p (plan=x0 |s,p)
min 1, p (plan=xt |s,p) , p function proportional posterior distribution.
Although chose incorporate MH, usable sampling method.
method require calculation normalization constant (e.g., rejection
sampling slice sampling) could also used. However, methods, sampling
ordered tuple sets grounded predicates slow complicated, pointed
Neal (2003).
4.3.2 Sampling Hyper-Parameters p Slice Sampling
use slice sampling sample p . method simple implement
works well scalar variables. Distribution choices made based valid value
take. take value, preferably one mode, p take
value [0, 1]. MH sampling may also work; however, method could overly
complicated simple scalar value. chose stepping procedure, described
Neal et al (Neal, 2003).
4.3.3 Sampling snt
Fortunately, analytic expression exists posterior snt :
p(st |plan, pt , s0t ) p(st |plan)p(pt , s0t |plan, st )
= p(st |plan)p(pt |plan, st )p(s0t |st )
= p(s0t |st )

N


p(snt |plan)p(pnt |plan, snt )

n=1

Note analytic expression expensive evaluate number possible
values snt large. case, one marginalize snt , variable truly care
plan variable.

5. Experimentation
section, explain web-based collaboration tool used experiment
two fictional rescue scenarios given human subjects experiment.
5.1 Web-Based Tool Design
Disaster response teams increasingly using web-based tools coordinate missions
share situational awareness. One tools currently used first responders Next
378

fiA Generative Modeling Approach Logic-Based Prior

Generation Incident Command System (NICS) (Di Ciaccio et al., 2011). integrated
sensing command-and-control system enables distribution large-scale coordination across multiple jurisdictions agencies. provides video audio conferencing
capabilities, drawing tools chat window, allows sharing maps resource information. Overall, NICS enables collection exchange information
critical mission planning.
designed web-based collaboration tool modeled system, modification requires team communicate solely via text chat. tool developed
using Django (Holovaty & Kaplan-Moss, 2009), free open-source Web application
framework written Python. Django designed ease working heavy-duty data,
provides Python API enable rapid prototyping testing. Incoming data
easily maintained user-friendly administrative interface. Although simplified version NICS, provides essence emerging technology large-scale
disaster coordination (Figure 1).
5.2 Scenarios
Human subjects given one two fictional rescue scenarios asked formulate
plan collaborating partners. collected human team planning data
resulting conversations, used data validate algorithm. first scenario involves radioactive material leakage accident building multiple rooms,
tasks (described below) assumed take one unit time. added complexity scenario announcing new piece information halfway
planning conversation, requiring team change plan. second scenario also
included time-durative actions (e.g., action take place action B taking
place). scenarios inspired described emergency response team training manuals (FEMA, 2014), designed completed reasonable time
experiments.
5.2.1 First Scenario: Radioactive Material Leakage
disaster scenario involves leakage radioactive material floor consisting
eight rooms. room contains either patient requiring in-person assessment valve
must repaired (Figure 4).
Goal State: patients assessed in-person medical crew. valves fixed
mechanic. rooms inspected robot.
Constraints: two medical crews, red blue (discrete resource constraint),
one human mechanic (discrete resource constraint) two robots, red blue (discrete
resource constraint). safety purposes, robot must inspect radioactivity room
human crews sent inside (sequence constraint).
Assumption: tasks (e.g. inspecting room, fixing valve) take amount
time (one unit), hard temporal constraints. assumption made
conduct initial proof-of-concept experimentation described paper, relaxed
scenario described Section 5.2.2.

379

fiKim, Chacha & Shah

Figure 3: Radioactive material leakage scenario

Announcement: planning session, team receives situation update
red robot order, requiring team modify previously discussed
plan use one robot deployment. announcement triggers automatically
team exchanged 20 utterances. purpose announcement increase
task complexity team, least two competing plans increase level
noise conversation.
scenario produces large number possible plans (more 1012 ), many
valid achieving goals without violating constraints.
5.2.2 Second Scenario: Police Incidents Response
second scenario involves team police officers firefighters responding series
incidents occurring different time frames. scenario includes complicated
time-durative actions first, well interdependency tasks
taken account planning. current time given 8 p.m. Two fires
started time: one college dorm another theater building, shown
Figure 4. Also, three street corners, indicated crime hot-spots (places predicted
experience serious crimes, based prior data), become active 8:30 p.m. 9 p.m.
also report street robbery taking place 8 p.m. injury occurred;
however, police officer must speak victim file incident report.
Goal State: Respond many incidents possible given resources listed Table
2.
Constraints:
Putting fire requires one fire truck one police car equipped robot.
police car must stay robot evacuation over.
robot perform evacuation.
robot used once.
Successfully responding fire requires evacuating building putting
fire. actions happen simultaneously.

380

fiA Generative Modeling Approach Logic-Based Prior

Figure 4: Police incident response scenario

Resources

Name

Police teams

robots

Alpha
Bravo
Charlie

Function
Patrol hotspot
Deploy robot evacuation
Respond street robbery

Fire trucks

Delta
Echo
Foxtrot

Put fire

Duration
Evacuate one building in:
30 min one robot
15 min two robots
10 min three robots
Talk victim in:
10 min one police car
Put fire in:
30 min one fire truck
15 min two fire trucks
10 min three fire trucks
(same dorm theater)

Table 2: Resources available police incident response scenario

381

fiKim, Chacha & Shah

Responding hot-spot patrol requires one police car located site
specified amount time.
one police car necessary respond street robbery.
Assumption Announcement: information traffic provided, travel
time place place assumed negligible. planning session, team
receives following announcement: traffic officer contacted us, said
First Second bridges experience heavy traffic 8:15 pm. take least 20
minutes car get across bridge. travel time theater hot-spot
20 minutes without using bridges. announcement made, team
must account traffic plan.

6. Evaluation
section, evaluate performance plan inference algorithm initial
proof-of-concept human subject experimentation, show able infer human
teams final plan 86% accuracy average, accuracy defined composite
measure task allocation plan sequence accuracy measures. also describe robot
demonstration two people plan execute first-response collaborative task
PR2 robot.
6.1 Human Team Planning Data
indicated previously, designed web-based collaboration tool modeled NICS
system (Di Ciaccio et al., 2011) used first-response teams, modification
requires team communicate solely via text chat. radioactive material leakage
scenario, announcement, 13 teams two (a total 26 participants) recruited
Amazon Mechanical Turk greater Boston area. Recruitment
restricted located US increase probability participants
fluent English. radioactive material leakage scenario, announcement, 21
teams two (a total 42 participants) recruited Amazon Mechanical Turk
greater Boston area. police incident response scenario, 14 teams
two (total 28 participants) recruited greater Boston area. Participants
required prior experience expertise emergency disaster planning,
note may structural differences dialog expert novice planners.
leave topic future investigation.
team received one two fictional rescue scenarios described Section 5.2,
asked collaboratively plan rescue mission. Upon completion planning session,
participant asked summarize final agreed-upon plan structured form
described previously. independent analyst reviewed planning sessions resolve
discrepancies two members descriptions necessary. first second
authors, well two independent analysts, performed utterance tagging, team
planning session tagged reviewed two four analysts. average, 36%
predicates mentioned per data set end final plan.

382

fiA Generative Modeling Approach Logic-Based Prior

6.2 Algorithm Implementation
algorithm implemented Python, VAL PDDL 2.1 plan validator (Howey
et al., 2004) used. performed 2,000 Gibbs sampling steps data
planning session. initial plan value set two five moves (from MH proposal
distribution) away true plan. initial value variable randomly set
timestamp initial plan value.
Within one Gibbs sampling step, performed 30 steps Metropolis-Hastings
(MH) algorithm sample plan. Every 20 samples selected measure accuracy
(median), burn-in period 200 samples.
Results assessed quality final plan produced algorithm terms
accuracy task allocation among agents (e.g. medic travels room)
accuracy plan sequence.
Two metrics task allocation accuracy evaluated: 1) percent inferred plan
predicates appearing teams final plan [% Inferred], 2) percent noise rejection
extraneous predicates discussed appear teams final plan [%
Noise Rej].
evaluated accuracy plan sequence follows: pair predicates
correctly ordered consistent relative ordering true final plan. meaordered pairs correct predicates
sured percent accuracy sequencing [% Seq] # correctly
.
total # pairs correct predicates
correctly estimated predicates compared, ground truth relation
predicates included true final plan. used relative sequencing measure
compound sequence errors, absolute difference measure would
(e.g. error ordering one predicate early plan shifts position
subsequent predicates).
Overall composite plan accuracy computed arithmetic mean task
allocation plan sequence accuracy measures. metric summarizes two relevant
accuracy measures provide single metric comparison conditions.
evaluated algorithm four conditions: 1) perfect PDDL files, 2) PDDL problem file
missing goals/constants (e.g. delete available agents), 3) PDDL domain file missing
constraint (e.g. delete precondition), 4) using uninformative prior possible
plans.
purpose second condition, PDDL problem file missing goals/constants,
test robustness approach incomplete problem information. PDDL
problem specifiction intentionally designed omit information regarding one patient
(pG) one robot (blueR). also omitted following facts initial state:
pG located G, blueR available perform inspections, patient pG
patient yet rescued. goal state omitted pG patient rescued.
condition represented significant degradation problem definition file, since
original planning problem involved three patients two robots.
purpose third condition, PDDL domain file missing constant,
test robustness approach missing constraints (or rules successful execution).
potentially easy person miss specifying rule often implicitly assumed.
third condition omitted following constraint domain file: rooms
inspected prior sending medical crews. condition represented significant

383

fiKim, Chacha & Shah

degradation domain file, since constraint affected action involving one
medical crew teams.
Results shown Tables 3-5 produced sampling plan variables fixing
= 5 p = 0.8. tables report median values percent inferred
plan predicates appearing final plan [% Inferred], noise rejection [% Noise Rej.],
sequence accuracy [% Seq.]. show algorithm infers final plans greater
86% composite accuracy average. also show approach relatively robust
degraded PDDL specifications (i.e., PDDL missing goals, constants constraints).
discussion sampling hyper-parameters found Section 7.2.
6.3 Concept-of-Operations Robot Demonstration
illustrate use plan inference algorithm robot demonstration
two people plan execute first-response collaborative task PR2 robot.
participants plan impending deployment using web-based collaborative tool
developed. planning session complete, dialogue tagged manually.
plan inferred data confirmed human planners provided
robot execution. registration predicates robot actions, room names
map locations, performed offline advance. first responders way
accident scene, PR2 autonomously navigates room, performing online
localization, path planning obstacle avoidance. robot informs rest team
inspects room confirms safe human team members enter. Video
demo found here: http://tiny.cc/uxhcrw.

7. Discussion
section discuss results trends Tables 3-5. discuss sampling hyper-parameters improves inference accuracy, provide interpretation inferred hyper-parameter values relate data characteristics. also provide
additional support use PDDL analyzing multiple Gibbs sampling runs. rationale behind i.i.d assumption utterances made generative model explained,
show simple extension model relax assumption. Finally,
provide rationale designing proposal distribution sampling algorithm.
7.1 Results
average accuracy inferred final plan improved across three scenarios
use perfect PDDL compared uninformative prior possible plans.
sequence accuracy also consistency improved use PDDL, regardless noise level
type PDDL degradation. three scenarios exhibited different levels noise,
defined percentage utterances end finally agreed upon
plan. police incidents response scenario produced substantially higher noise (53%),
compared radioactive material leaking scenario announcement (38%)
announcement (17%). possibly police incidents scenario included
durative-actions, whereas others not. Interestingly, perfect PDDL produced

384

fiA Generative Modeling Approach Logic-Based Prior

PDDL
PDDL missing goals
constants
PDDL missing constraint
PDDL

Task Allocation
% Inferred % Noise Rej.
61
100

97

Composite
% Acc.
86

% Seq.

100

58

77

78

70
70

100
58

87
66

86
65

Table 3: Radioactive material leakage scenario plan accuracy results, announcement
(13 teams / 26 subjects). table reports median values percent inferred
plan predicates appearing final plan [% Inferred], noise rejection [% Noise Rej.],
sequence accuracy [% Seq.]. Composite % Accuracy calculated average
previous three measures.

PDDL
PDDL missing goals
constants
PDDL missing constraint
PDDL

Task Allocation
% Inferred % Noise Rej.
77
100

83

Composite
% Acc.
87

% Seq.

100

54

97

84

72
100

100
54

90
81

87
78

Table 4: Radioactive material leakage scenario plan accuracy results, announcement
(21 teams / 42 subjects). table reports median values percent inferred
plan predicates appearing final plan [% Inferred], noise rejection [% Noise Rej.],
sequence accuracy [% Seq.]. Composite % Accuracy calculated average
previous three measures.

PDDL
PDDL missing goals
constants
PDDL missing constraint
PDDL

Task Allocation
% Inferred % Noise Rej.
97
89

97

Composite
% Acc.
86

% Seq.

92

86

92

83

97
81

89
95

97
81

85
82

Table 5: Police incidents response scenario plan accuracy results (14 teams / 28 subjects).
table reports median values percent inferred plan predicates appearing
final plan [% Inferred], noise rejection [% Noise Rej.], sequence accuracy [% Seq.].
Composite % Accuracy calculated average previous three measures.

385

fiKim, Chacha & Shah

substantial improvements sequence accuracy noise level higher, radioactive material leaking scenario announcement, police incidents scenario.
Accuracy task allocation, hand, differ depending noise level
type PDDL degradation. noise rejection ratio better
PDDL PDDL missing constraint, compared uninformative prior,
scenarios less noise (e.g. radioactive material leaking scenarios
announcement). However, PDDL provide benefit noise rejection ratio
police incidents scenario noise level 50%. However, case
PDDL provide improvements inferred task allocation.
7.2 Sampling Hyper-Parameters
section discusses results hyper-parameter sampling. First, show
data point (i.e., teams conversation) converges different hyper-parameter values,
show values capture characteristics data point. Second, show
learning different sets hyper-parameters improves different measures accuracy,
describe consistent interpretation hyper-parameters
model.
PDDL
PDDL missing goals constants
PDDL missing constraint
PDDL

PDDL
PDDL missing goals constants
PDDL missing constraint
PDDL
50%

46%
42%

30%

46%

26%

42%

25%

30%
20%
13%
11%
10%

10%
0%

5%
0%

0%

0%

0%

% improved accuracy

% improved accuracy

40%

20%

17%

15%

12%

18%

17%
12%

10%
5%

5%
0%

3%
-4%

-8% -7%

0%

-5%
-10%

Radioactive Radioactive

-10%

Police

Radioactive Radioactive

Police

(a) Improvements noise rejection sampling (b) Improvements sequence accuracy samp
pling

Figure 5: Percent improvements median noise rejection median sequence accuracy
sampling hyper-parameters versus setting p = 0.8 = 5.

7.2.1 Improvements Sequence Accuracy versus Noise Rejection
hyper-parameter represents noisiness predicate ordering, hyperparameter p represents noisiness assignment predicates. Setting parameters fixed value corresponds assumption noisiness data set.
learn parameters Gibbs sampling, allowing values adjusted
according different characteristics data set. details sample hyperparameters explained Section 4.3.2. performed 2,000 Gibbs sampling steps

386

fiA Generative Modeling Approach Logic-Based Prior

data planning session. initial values p sampled
prior, parameters set values described Section 4.1.
found learned p (with = 5), noise rejection rate improved
compared fixed p = 0.8. radioactive material leakage scenario,
mid-scenario announcement, noise rejection ratio improved
much 41% 45%, respectively; police incident response scenario, observed
13% improvement (Figure 5a). Note cases median noise rejection
ratio maintained improved sampling p .
Similarly, learned (with p = 0.8), sequence accuracies generally improved.
radioactive material leakage scenario, announcement, sequence
accuracy improved 26% 16%, respectively; police incident response
scenario, observed 18% improvement (Figure 5b). Note three cases
see degradation accuracy 4-8%. However nine twelve cases
sequence accuracy maintained improved sampling .
Interestingly, samples achieved highest overall composite accuracy
plan learned, hyper-parameters fixed. particular,
observed average 5% ( 3%) decrease composite accuracy sampling four
variables together. One possible explanations finding that, due
limited amount data, Gibbs sampling may require many iterations converge
variables. result suggests one may choose set hyper-parameters learn
based measure accuracy important user.
7.2.2 Interpretation Inferred Values Hyper-Parameters
described Section 4.1, p parameter models level noise predicates within
data. words, p parameter designed model many suggestions
team makes conversation subsequently included final plan.
noise level high, lower-valued p represent characteristics conversation
well, may allow better performance. (However, noise level high,
inference may still fail.)
compare learned value p characteristics conversation, need
way calculate noisy conversation is. following one way manually
estimate value p : First, count utterances contain predicates. Then,
count utterances contain predicates included final plan. ratio
two numbers interpreted noisiness predicates; lower number,
team talked many possible plans.
performed manual calculation two teams trials Team 3 10
compare values learned values. Team 3s trial, 19.4% suggestions
made conversation included final plan (i.e., almost 80% suggestions
relevant final plan). hand, 68% suggestions made Team
10s trial included final plan. Using interpretation, Team 3s trial
twice noisy Team 10s trial.
converged value p lower Team 3s trial Team 10s trial, reflecting
characteristics data set. Figure 6b shows converged value p
teams trial (sub-sampled, subset dataset). figure presents values

387

fiKim, Chacha & Shah

(a) Examples value convergences

(b) Examples p value convergence

Figure 6: Inferred values hyper-parameters (only showing subset data set)
p iteration Gibbs sampling step. Note samples Team 3s trial
converge 20%, samples Team 10s trial converge 40%. lower value
p represents higher noise level, matches intuition.
However, conclusive way prove converged values true
values. theory, Gibbs sampling algorithm guarantees convergences true

388

fiA Generative Modeling Approach Logic-Based Prior

value infinite number iterations. Therefore, cannot prove converged
p variables shown Figure 6 true values. practice, trace plot,
Figure 6, drawn order demonstrate convergence local optimum. fact
values appear plateau burn-in period provides support convergence
local optimum point. Investigation potentially local optimum point suggests
p value data point different, observe
relationship p value characteristics data set. addition,
manual calculation noisiness one way interpreting noisiness data
set. Therefore, analysis considered one possible way gain insight
learned values; rigorous proof relation learned value
hyper-parameter characteristics data.
7.3 Benefit PDDL
section provides additional evidence benefit using PDDL analyzing multiple
runs using data sampling algorithm. explained Section 4.3, Gibbs
sampling approximate inference algorithm produce different results
run.
section evaluate runs wide range different settings show
benefit PDDL applies particular setting parameters, also different
settings. analyzed three cases across range parameters: 1) learning plan
s, 2) learning plan, p 3) learning plan, . first case, changed
value range 3 1,000, p 0.3 0.8, 1 100. second
case, addition parameters, varied parameters prior distribution
p kp p ; ranging 2 70. third case, addition
p parameters, varied parameters prior distribution k ;
ranging 0.1 50. Values ranges selected randomly produce
total 613 runs.
Eighty-two percent 613 runs showed higher accuracy PDDL used
PDDL used. suggests adding structured prior improves accuracy
wide range parameter settings. Figure 7 presents ratio runs saw benefit
use PDDL, three scenarios.
Interestingly, highest accuracy always achieved perfect PDDL files;
cases, highest accuracy achieved imperfect PDDL files (e.g., PDDL
file missing goals/constraints, described Section 6). observation may
explained possibility finally agreed-upon plans 1) complete and/or
2) violate constraints (mostly due participants misunderstandings). example: Prior
announcement radioactive material leakage scenario, number teams
finished building complete plans. Therefore, final plans cases may
better inferred incomplete PDDL files (consistent Table 4). police
incident response scenario, however, number teams missed constraint hotspot patrolling task considered complete hot-spot fully covered 8:30
p.m. 9 p.m. number teams dispatched police cars portion time
window, resulting invalid plans perfect PDDL files (consistent Table 5)

389

fiRatio runs using PDDL improved composite accuracy

Kim, Chacha & Shah

1
198/230

0.9
173/216

134/167

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Radioactive

Radioactive

Police

Figure 7: Ratio runs show benefit using PDDL

improvements achieved adding structure prior using PDDL suggest
structural information beneficial inference problem. would interesting
systematically investigate smallest set structural information achieves accuracy
improvements, given fixed computation budget, future work.
7.4 i.i.d Assumption Utterance Generative Model
generative model considers utterances independent identically distributed samples plan variable. words, consider utterances
give equal evidence plan, regardless order appear conversation. alternative would different weight utterance, take
ordering account. section, explain reasons i.i.d. assumption,
simple extension current model relax assumption.
human subject data collected work, observe clear relationship
order utterance whether suggestion included final plan.
example, number teams decided include parts plan discussed
beginning conversation within final plan, discussing many
possibilities. distribution utterances included final plan shown
Figure 8. addition, team discusses plans time pressure, planning
sessions often consist small number succinct communications. example,
average number predicates utterances planning session 90, whereas
average number predicates final plan 12. succinct conversation yields less
available data inference; therefore, complicated model may fail correctly infer
latent variables. time series model, wherein ordering taken account
weight utterance latent variable needs learned data,
example model.

390

fiA Generative Modeling Approach Logic-Based Prior

Figure 8: distribution utterances included final plan (normalized)

However, simple extension current model relax assumption incorporate different importance utterance. One way decide utterances importance integrate human cognitive models. Human cognitive architectures (Anderson,
1983) model human cognitive operations, memory model (Anderson, Bothell,
Lebiere, & Matessa, 1998). example, decrease importance utterance
time proceeds planning session applying varying weights utterance.
simple extension current model made incorporate memory model.
Specifically, variables p modified vectors represent weight
activation level utterance human cognition model (Anderson et al., 1998).
vector p length utterances, p = {p,1 , p,2 , , p,T },
p,t represents activation level utterance. Similarly, extend
vector, represent noisy utterance is, weighing accordingly. However, cognitive models empirically well-verified, Whitehill (2013)
pointed structured way set parameters models. addition,
unclear human memory model would differ depending characteristics
given task. example, memory model may differ significantly short, succinct
conversations conducted time pressure.
7.5 Engineering Proposal Distribution Metropolis-Hastings
Sampling Algorithm
section describes impact different proposal distributions MH sampling
step, rationale designing proposal distribution described Section 4.3.1.
numerous studies conducted selecting family candidate-generating
density functions (Metropolis et al., 1953; Hastings, 1970; Geweke, 1989; Gelman & Rubin,

391

fiKim, Chacha & Shah

Result proposal distribution preliminary work
Result proposal distribution current work
100%

Percent composite accuracy

90%
80%
70%
60%
50%
40%
30%
20%
10%
0%

Radioactive Radioactive

Police

Figure 9: impact different proposal distributions (The highest accuracy perfect
PDDL files)

1992). However, pointed Chib Greenberg (1995), structured way
choose proposal distribution. becomes challenging sampled object
simple scalar variable, complicated object, plan variable
(i.e., tuples sets grounded predicates) work. object, larger
spaces potential proposal distributions choose from.
However, good choice proposal distribution improve performance. Figure 9
shows results two different proposal distributions used work. preliminary version work (Kim et al., 2013) applied following distribution:
Select predicate set possible predicates. current plan, move
either: 1) next set predicates 2) previous set, 3) remove
current plan. current plan, move one existing sets.
Select two sets current plan switch orders.
One difference proposal distribution one outlined Section 4.3.1
set allowed timestamps selected predicate move iteration.
proposed distribution allows predicate move timestamp, whereas
one Section 4.3.1 allows predicate move adjacent timestamp.
key insight proposal distribution work gained investigating sequences MH sampling steps observing proposal distribution fails propose
good move. words, identify moves necessary move proposed
value (i.e., proposed new plan) true value latent variable (i.e., true plan)
close other. Often, predicate one timestamp true timestamp (i.e., one timestamp before), proposal distribution contained
preliminary work (Kim et al., 2013) often fails suggest better proposed point.
392

fiA Generative Modeling Approach Logic-Based Prior

motivated us create proposal distribution enabling frequent moves adjacent timestamps two timestamps. result, observed substantial
improvement accuracy scenarios, shown Figure 9.
particular proposal distribution cannot applied cases, insight
suggests following approach could useful designing proposal distribution
non-scalar valued variables: First, distance metric defined two nonscalar valued variables. case, step included defining distance two
tuples sets predicates (i.e., plan variables). example, distance could
average number missing extraneous predicates number predicates
incorrect timestamps. Second, starting initial proposed distribution, distance
sample true value measured. Third, filter sample sequences
distance short, visualize them. shorter distance indicates moments
sampling could almost reached true value, not. Finally, proposed
distribution modified include move converts samples third step
true value within one two moves. process allows insight designing
proposal distribution. leave investigation systematic approach future
work.

8. Conclusion Future Work
work, formulated novel problem performing inference extract
finally agreed-upon plan human teams planning conversation. presented
algorithm combines probabilistic approach logical plan validation, used
compute highly structured prior possible plans. approach infers team plans
without need historical data, using situational information data
single planning session. require development addition plan library
infer plan, demonstrate solution robust incomplete knowledge
planning problem. demonstrated benefit approach using human team meeting
data collected large-scale human subject experiments (total 96 subjects) able
infer human teams final plans 86% accuracy average.
future, plan build work design interactive agent participates improve human teams planning decisions. Specifically envision work
described starting point utilizing building human domain experts
knowledge, improving quality finally agreed-upon plan human-machine
interaction.

9. Acknowledgement
work sponsored ASD (R&E) Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions recommendations authors
necessarily endorsed United States Government.

393

fiKim, Chacha & Shah

Appendix A. Visualization Gibbs Sampling Convergence: Trace
Plot
known conclusive way determine whether Markov chain
Gibbs sampling reached stationary, desired posterior, distribution (Cowles
& Carlin, 1996). Many available diagnostic tools designed test necessary
insufficient conditions convergence, work done Gelman Rubin (1992),
Geweke (1991), Heidelberger Welch (1981) Raftery Lewis (1995), mention
few. work utilize much simpler yet still informative approach,
visually check whether convergence reached using trace plot.
trace plot simply scatter plot statistics successive parameter estimates
(e.g., estimated values) respect iteration steps. statistics
means, variances covariance. trace plot informative scalar variables
plotted. Figure 10 shows examples trace plots p variables.

References
Albrecht, D. W., Zuckerman, I., Nicholson, A. E., & Bud, A. (1997). Towards Bayesian
model keyhole plan recognition large domains. Proceedings Sixth
International Conference User Modeling, pp. 365376. Springer-Verlag.
Anderson, J. R. (1983). spreading activation theory memory. Journal Verbal Learning
Verbal Behavior, 22 (3), 261295.
Anderson, J. R., Bothell, D., Lebiere, C., & Matessa, M. (1998). integrated theory
list memory. Journal Memory Language, 38 (4), 341380.
Andrieu, C., De Freitas, N., Doucet, A., & Jordan, M. I. (2003). introduction mcmc
machine learning. Machine learning, 50 (1-2), 543.
Barnes, M., Chen, J., Jentsch, F., & Redden, E. (2011). Designing effective soldier-robot
teams complex environments: training, interfaces, individual differences. EPCE,
484493.
Bauer, M., Biundo, S., Dengler, D., Koehler, J., & Paul, G. (2011). PHI: logic-based tool
intelligent help systems..
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal
Machine Learning Research, 3, 9931022.
Brown, J. S., & Burton, R. R. (1978). Diagnostic models procedural bugs basic
mathematical skills. Cognitive Science, 2 (2), 155192.
Carberry, S. (1990). Plan recognition natural language dialogue. MIT Press.
Casper, J., & Murphy, R. (2003). Human-robot interactions robot-assisted urban
search rescue response World Trade Center. IEEE SMCS, 33 (3), 367385.
Casper, J., & Murphy, R. (2002). Workflow study human-robot interaction USAR.
IEEE ICRA, 2, 19972003.
Charniak, E., & Goldman, R. P. (1993). Bayesian model plan recognition. Artificial
Intelligence, 64 (1), 5379.
394

fiA Generative Modeling Approach Logic-Based Prior

(a) Examples value convergences

(b) Examples p value convergence

Figure 10: Trace Plots (only showing subset data set)
Chib, S., & Greenberg, E. (1995). Understanding Metropolis-Hastings algorithm.
American Statistician, 49 (4), 327335.
Coles, A., Fox, M., Halsey, K., Long, D., & Smith, A. (2009). Managing concurrency
temporal planning using planner-scheduler interaction. Artificial Intelligence, 173 (1),
144.
Cowles, M. K., & Carlin, B. P. (1996). Markov chain Monte Carlo convergence diagnostics:
comparative review. Journal American Statistical Association, 91 (434), 883
904.
Cummings, M. L., Brzezinski, A. S., & Lee, J. D. (2007). Operator performance intelligent aiding unmanned aerial vehicle scheduling. IEEE Intelligent Systems, 22 (2),
5259.
395

fiKim, Chacha & Shah

Di Ciaccio, R., Pullen, J., & Breimyer, P. (2011). Enabling distributed command
control standards-based geospatial collaboration. IEEE International Conference
HST.
FEMA (2014). Federal emergency management agency.. [Online; accessed 3-December2014].
Fikes, R. E., & Nilsson, N. J. (1972). Strips: new approach application theorem
proving problem solving. Artificial intelligence, 2 (3), 189208.
Gal, Y., Reddy, S., Shieber, S. M., Rubin, A., & Grosz, B. J. (2012). Plan recognition
exploratory domains. Artificial Intelligence, 176 (1), 22702290.
Geib, C. W., & Goldman, R. P. (2009). probabilistic plan recognition algorithm based
plan tree grammars. Artificial Intelligence, 173 (11), 11011132.
Geib, C. W., Maraist, J., & Goldman, R. P. (2008). new probabilistic plan recognition
algorithm based string rewriting.. ICAPS, pp. 9198.
Gelman, A., & Rubin, D. B. (1992). Inference iterative simulation using multiple
sequences. Statistical Science, 457472.
Getoor, L., & Mihalkova, L. (2011). Learning statistical models relational data. International Conference Management Data, 11951198.
Geweke, J. (1989). Bayesian inference econometric models using Monte Carlo integration.
Econometrica: Journal Econometric Society, 13171339.
Geweke, J. (1991). Evaluating accuracy sampling-based approaches calculation
posterior moments. Federal Reserve Bank Minneapolis, Research Department.
Goodrich, M. A., Morse, B. S., Engh, C., Cooper, J. L., & Adams, J. A. (2009). Towards
using UAVs wilderness search rescue: Lessons field trials. Interaction
Studies, Special Issue Robots Wild: Exploring Human-Robot Interaction
Naturalistic Environments, 10 (3), 453478.
Grosz, B. J., & Sidner, C. L. (1990). Plans discourse. Cohen, P. R., Morgan,
J., & Pollack, M. E. (Eds.), Intentions Communication, pp. 417444. MIT Press,
Cambridge, MA.
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains
applications. Biometrika, 57 (1), 97109.
Heidelberger, P., & Welch, P. D. (1981). spectral method confidence interval generation
run length control simulations. Communications ACM, 24 (4), 233245.
Holovaty, A., & Kaplan-Moss, J. (2009). definitive guide Django: Web development
done right. Apress.
Horvitz, E., Breese, J., Heckerman, D., Hovel, D., & Rommelse, K. (1998). Lumiere
project: Bayesian user modeling inferring goals needs software users.
Proceedings Fourteenth Conference Uncertainty Artificial Intelligence,
256265.
Howey, R., Long, D., & Fox, M. (2004). Val: Automatic plan validation, continuous effects
mixed initiative planning using PDDL. IEEE ICTAI, 294301.
396

fiA Generative Modeling Approach Logic-Based Prior

Jones, H., Rock, S., Burns, D., & Morris, S. (2002). Autonomous robots SWAT applications: Research, design, operations challenges. AUVSI.
Jurafsky, D., & Martin, J. H. (2000). Speech Language Processing: Introduction
Natural Language Processing, Computational Linguistics, Speech Recognition (1st
edition). Prentice Hall PTR, Upper Saddle River, NJ, USA.
Kautz, H. A., Pelavin, R. N., Tenenberg, J. D., & Kaufmann, M. (1991). formal theory
plan recognition implementation. Reasoning Plans, 69125.
Kautz, H. A. (1987). formal theory plan recognition. Ph.D. thesis, Bell Laboratories.
Kim, B., Chacha, C. M., & Shah, J. (2013). Inferring robot task plans human team
meetings: generative modeling approach logic-based prior. AAAI.
Kim, J., & Shah, J. A. (2014). Automatic prediction consistency among team members understanding group decisions meetings. Systems, Man Cybernetics
(SMC), 2014 IEEE International Conference on, pp. 37023708. IEEE.
Koomen, P., Punyakanok, V., Roth, D., & Yih, W. (2005). Generalized inference
multiple semantic role labeling systems. CoNLL, 181184.
Kruijff, G., Jancek, M., & Lison, P. (2010). Continual processing situated dialogue
human-robot collaborative activities. IEEE Ro-Man.
Larochelle, B., Kruijff, G., Smets, N., Mioch, T., & Groenewegen, P. (2011). Establishing
human situation awareness using multi-modal operator control unit urban
search & rescue human-robot team. IEEE Ro-Man, 229234.
Lochbaum, K. E. (1998). collaborative planning model intentional structure. Computational Linguistics, 24 (4), 525572.
Mayfield, J. (1992). Controlling inference plan recognition. User Modeling UserAdapted Interaction, 2 (1-2), 5582.
McDermott, D., Ghallab, M., Howe, A., Knoblock, C., Ram, A., Veloso, M., Weld, D., &
Wilkins, D. (1998). PDDL-the planning domain definition language..
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953).
Equation state calculations fast computing machines. Journal Chemical
Physics, 21, 1087.
Micire, M. (2002). Analysis robotic platforms used World Trade Center disaster.
Ph.D. thesis, MS thesis, Department Computer Science Engineering, Univ. South
Florida.
Murphy, R. (2004). Human-robot interaction rescue robotics. IEEE SMCS, 34 (2), 138
153.
Neal, R. M. (2003). Slice sampling. Annals Statistics, 705741.
Nguyen, T. A., Kambhampati, S., & Do, M. (2013). Synthesizing robust plans incomplete domain models. Advances Neural Information Processing Systems, 24722480.
Palmer, M., Gildea, D., & Xue, N. (2010). Semantic role labeling. Synthesis Lectures
Human Language Technologies, 3 (1), 1103.

397

fiKim, Chacha & Shah

Pednault, E. P. D. (1987). Formulating Multi-Agent Dynamic-World Problems Classical Planning Framework. Reasoning Actions Plans: Proceedings
1986 Workshop. Morgan Kaufmann Publishers.
Poon, H., & Domingos, P. (2006). Sound efficient inference probabilistic
deterministic dependencies. AAAI, 21 (1), 458.
Poon, H., & Domingos, P. (2009). Unsupervised semantic parsing. EMNLP.
Pradhan, S., Ward, W., Hacioglu, K., Martin, J., & Jurafsky, D. (2004). Shallow semantic
parsing using support vector machines. NAACL-HLT, 233.
Pynadath, D. V., & Wellman, M. P. (2000). Probabilistic state-dependent grammars
plan recognition. Proceedings Sixteenth conference Uncertainty Artificial
Intelligence, 507514.
Raedt, L. (2008). Probabilistic logic learning. Logical Relational Learning, 223288.
Raftery, A. E., & Lewis, S. M. (1995). number iterations, convergence diagnostics
generic metropolis algorithms. Practical Markov Chain Monte Carlo, 115130.
Ramrez, M., & Geffner, H. (2009). Plan recognition planning. Proceedings 21st
international joint conference Artificial Intelligence, 17781783.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine learning, 62 (1),
107136.
Ryall, K., Marks, J., & Shieber, S. (1997). interactive constraint-based system
drawing graphs. Proceedings 10th Annual ACM Symposium User Interface
Software Technology, 97104.
Sadilek, A., & Kautz, H. A. (2010). Recognizing multi-agent activities GPS data.
AAAI.
Singla, P., & Domingos, P. (2007). Markov logic infinite domains. UAI, 368375.
Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., & Roy, N. (2011).
Understanding natural language commands robotic navigation mobile manipulation. AAAI.
Weida, R., & Litman, D. (1992). Terminological Reasoning Constraint Networks
Application Plan Recognition.
Whitehill, J. (2013). Understanding ACT-R - outsiders perspective. CoRR, 1306.0125.
Zhuo, H. H., Yang, Q., & Kambhampati, S. (2012). Action-model based multi-agent plan
recognition. Advances Neural Information Processing Systems 25, 377385.

398

fiJournal Artificial Intelligence Research 52 (2015) 203-234

Submitted 8/14; published 1/15

Subexponential-Time Complexity CSP
Ronald de Haan

DEHAAN @ KR . TUWIEN . AC .

Vienna University Technology
Vienna, Austria

Iyad Kanj

IKANJ @ CS . DEPAUL . EDU

School Computing, DePaul University
Chicago, USA

Stefan Szeider

STEFAN @ SZEIDER . NET

Vienna University Technology
Vienna, Austria

Abstract
NP-complete problems share practical hardness respect exact computation.
Whereas NP-complete problems amenable efficient computational methods, others
yet show sign. becomes major challenge develop theoretical framework
fine-grained theory NP-completeness, explain distinction
exact complexities various NP-complete problems. distinction highly relevant
constraint satisfaction problems natural restrictions, various shades hardness
observed practice.
Acknowledging NP-hardness problems, one look beyond polynomial time
computation. theory subexponential-time complexity provides framework,
enjoying increasing popularity complexity theory. instance constraint satisfaction
problem n variables domain values solved brute-force dn steps (omitting
polynomial factor). paper study existence subexponential-time algorithms,
is, algorithms running do(n) steps, various natural restrictions constraint satisfaction
problem. consider constraint satisfaction problem constraints given
extensionally tables, constraints given intensionally form
global constraints. provide tight characterizations subexponential-time complexity
aforementioned problems respect several natural structural parameters, allows us
draw detailed landscape subexponential-time complexity constraint satisfaction problem. analysis provides fundamental results indicating whether one significantly
improve brute-force search approach solving constraint satisfaction problem.

1. Introduction
observed various practical contexts NP-hard problems accessible
efficient exact computational methods, whereas others methods futile. central
challenge theoreticians develop framework, fine-grained theory
NP-completeness, explain distinction exact complexities NP-hard
problems. Subexponential-time complexity framework complexity theory provides
distinction (Lokshtanov, Marx, & Saurabh, 2011). based observation
NP-complete problems, one improve exponent exponential term upper bound
c
2015
AI Access Foundation. rights reserved.

fiDE

H AAN , K ANJ , & ZEIDER

running time indefinitelysuch problems admit subexponential-time algorithmswhereas
others apparently possible commonly-believed hypotheses complexity theory.
particular, subexponential-time algorithms developed many graph problems, including
NDEPENDENT ET OMINATING ET, natural structural restrictions; e.g., see work
Alber, Fernau, Niedermeier (2004), Chen, Kanj, Perkovic, Sedgwick, Xia (2007)
Demaine, Fomin, Hajiaghayi, Thilikos (2005). benchmark problem subexponential-time
computation satisfiability problem CNF formulas, clause contains three
literals, denoted 3-CNF-S AT. Exponential Time Hypothesis (ETH), proposed Impagliazzo
Paturi (2001), states 3-CNF-S n variables decidable subexponential time,
i.e., decidable time 2o(n) (omitting polynomial factors).
Constraint Satisfaction Problem (CSP) provides general uniform framework
representation solution hard combinatorial problems arise various areas Artificial
Intelligence Computer Science (Rossi, van Beek, & Walsh, 2006). instance, database
theory, CSP equivalent evaluation problem conjunctive queries relational databases
(Gottlob, Leone, & Scarcello, 2002). well known CSP NP-hard, entails fundamental
NP-hard problems 3-C OLORABILITY 3-CNF-S AT. Hence, cannot hope
polynomial-time algorithm CSP. hand, CSP obviously solved exponential
time: simply trying possible instantiations variables, solve CSP instance
consisting n variables range domain values time dn (omitting polynomial
factor input size). Significant work concerned improving trivial upper bound
various restrictions CSP (Beigel & Eppstein, 2005; Feder & Motwani, 2002; Grandoni &
Italiano, 2006; Moser & Scheder, 2011; Schning, 1999). instance, Razgon (2006) showed
binary CSP domain size solved time (d 1)n forward-checking algorithm
employing fail-first variable ordering heuristic; although faster algorithms known,
result indicates exponential running time CSP improved using heuristic methods
designed solving real-world CSP instances practice. improvements
trivial brute-force search give exponential running times exponent linear n.
aim paper investigate theoretical limits improvements. precisely,
explore whether exponential factor dn reduced subexponential factor do(n)
not, considering various natural NP-hard restrictions classical CSP constraints
given extensionally form tables, CSP constraints specified
intensionally using global constraints. CSP global constraints, consider CSP
global constraints either
AllDifferent constraints (denoted CSP6= ),
NValue constraints (denoted CSP= ),
AtLeastNValue constraints (denoted CSP ),
AtMostNValue constraints (denoted CSP ),
cTable constraints, i.e., constraints specified tables compressed tuples (denoted
CSPc ).
study CSP global constraints highly relevant central modeling
solving real-world problems use various global constraints come along efficient
204

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

propagation filtering techniques (Rgin, 2011; Van Hoeve & Katriel, 2006). Therefore,
study existence subexponential-time algorithms generic problems various
restrictions prime interest.
paper, obtain lower upper bounds results, cases draw detailed
complexity landscape CSP extensionally represented constraints CSP global
constraints respect subexponential-time solvability. lower bounds subject
Exponential Time Hypothesis (ETH), even though results derived weaker
complexity-theoretic hypotheses (Proposition 2, Proposition 3, Proposition 10). structural
parameters CSP instance focus (when relevant) are: (instance) size, domain
size, number constraints, arity (i.e., maximum size constraint scope), maximum
degree (i.e., maximum number occurrences variable), treewidth primal
incidence graph. highlight results obtain. turns out,
almost restrictions consideration, CSP generalization, CSP (global)
compressed table constraints (CSPc ), exhibit behavior respect subexponentialtime solvability. unless explicitly indicated results below, results CSP (positive
negative) mentioned hold well CSPc .
easy see CSP bounded domain size bounded arity subexponential-time
algorithm ETH fails. first result provides evidence drop
bound domain size bound arity, problem becomes harder; refer
discussion preceding Proposition 2 (n number variables instance):
1. B OOLEAN CSP solvable nonuniform subexponential time (unrestricted) CNFS AT. B OOLEAN CSPc , show B OOLEAN CSPc solvable subexponential
time parameterized complexity hierarchy collapses second level, consequence
implies CNF-S solvable subexponential time.
2. 2-CSP (all constraints arity 2) solvable subexponential time C LIQUE
solvable time N o(k) (N number vertices k clique-size).
turns out, number tuples plays important role characterizing subexponential-time
complexity CSP. show following tight result:
3. CSP solvable subexponential time instances number tuples o(n),
unless ETH fails, solvable subexponential time number tuples
instances (n).
B OOLEAN CSP linear size even derive equivalence ETH:
4. B OOLEAN CSP instances size (n) solvable subexponential time
ETH fails.
Results 3 4 also hold consider total number tuples constraint relations instead
input size.
5. CSP solvable subexponential time instances whose primal treewidth o(n),
solvable subexponential time instances whose primal treewidth (n) unless
ETH fails.
205

fiDE

H AAN , K ANJ , & ZEIDER

6. CSP solvable polynomial time instances whose incidence treewidth O(1),
solvable subexponential time instances whose incidence treewidth (1) unless
ETH fails.
CSP6= show following results:
7. CSP6= solvable subexponential time instances whose domain size lower bounded
function (1), solvable subexponential time constant domain size
least 3 unless ETH fails.
note aforementioned result may sound strange implies problem
easier larger domain size. explained fact domain size gets
large, allowable upper bound subexponential time solving problem (i.e., d(n)o(n) )
gets larger well.
8. CSP6= solvable subexponential time instances whose primal treewidth o(n),
solvable subexponential time instances whose primal treewidth (n) unless
ETH fails.
9. CSP6= solvable subexponential time instances whose incidence treewidth o(n),
solvable subexponential time instances whose primal treewidth (n) unless
ETH fails. Contrast result result (6) above.
CSP= , CSP , CSP , show following:
10. CSP solvable subexponential time instances whose number constraints constant
whose domain size lower bounded function (1), solvable
subexponential time number constraints linear domain size constant
unless ETH fails.
11. CSP= CSP solvable subexponential time instances whose domain size
constant whose number constraints (n) unless ETH fails.
12. CSP= , CSP , CSP solvable subexponential time instances whose primal
treewidth o(n), solvable subexponential time instances whose primal
treewidth (n) unless ETH fails.
table provides map that, structural parameters considered
paper, lists results paper pertaining structural parameter. structural parameters
consider instance CSP, CSP global constraints, are: size (size),
maximum size constraint scope (arity), cardinality domain (dom), number
tuples (tuples), number constraints (cons), treewidth incidence graph (tw ),
treewidth primal graph (tw), maximum number occurrences variable (deg).
206

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

Parameter

Results

size
arity
dom
tuples
cons
tw
tw
deg

Theorem 3
Propositions 1, 2, 12
Theorems 1, 2, 3, 7; Propositions 1, 3, 12, 14, 17; Corollaries 1, 3
Theorem 2
Theorems 4, 6, 7; Propositions 14, 17; Corollaries 2, 3
Theorem 5; Propositions 16, 19
Theorem 5; Propositions 15, 18
Proposition 11

results paper shed light instances aforementioned variants
CSP (with without global constraints) may feasible respect exact computation.
Moreover, results derived paper provide strong theoretical evidence
natural restrictions CSP may harder k-CNF-S ATfor subexponential-time
algorithm would lead failure ETH. Hence, results provide new point view
relationship CNF-S CSP, important topic recent AI research (Jeavons & Petke,
2012; Dimopoulos & Stergiou, 2006; Benhamou, Paris, & Siegel, 2012; Bennaceur, 2004).
close section mentioning work subexponential-time complexity
CSP problems AI. Already pioneering work ETH (Impagliazzo & Paturi, 2001;
Impagliazzo, Paturi, & Zane, 2001) considered k-C OLORABILITY problem, constitutes
important special case 2-CSP fixed domain size k. several results 2-CSP
bounds tw, treewidth primal graph (see Section 3.3 definitions). Lokshtanov
et al. (2011) showed following lower bound, using result L IST C OLORING problem
(Fellows et al., 2011a): 2-CSP cannot solved time f (tw)no(tw) unless ETH fails. Marx
(2010) showed recursively enumerable class G graphs unbounded treewidth
function f 2-CSP solved time f (G)no(tw/ log tw) instances whose primal
graph G, ETH fails. Jonsson, Lagerkvist, Nordh (2013) investigated B OOLEAN
CSP finite constraint languages identify easiest Boolean constraint language
CSP still NP-hard, show already problem subexponential-time algorithm
unless ETH fails. Traxler (2008) studied subexponential-time complexity CSP
constraints represented listing forbidden tuples; contrast standard
representation use, allowed tuples given, naturally captures database
problems (Gottlob et al., 2002; Grohe, 2006; Papadimitriou & Yannakakis, 1999). setting
considered generalization CNF-S AT; single clause gives rise constraint
exactly one forbidden tuple. arity bounded constant, insignificant whether
constraints represented forbidden allowed tuples, one translate two
representations polynomial time. Finally would like point recent use ETH
complexity analysis problems highly relevant AI like Planning (Bckstrm &
Jonsson, 2011), Probabilistic Inference (Kwisthout, Bodlaender, & van der Gaag, 2010), Text
Analysis (Ge, 2013).
Parts paper published preliminary form proceedings AAAI13
CP14 (Kanj & Szeider, 2013; De Haan, Kanj, & Szeider, 2014).
207

fiDE

H AAN , K ANJ , & ZEIDER

2. Preliminaries
section introduce terminologies background material needed paper.
2.1 Constraint Satisfiability CNF-Satisfiability
instance C ONSTRAINT ATISFACTION P ROBLEM (or CSP, short) triple (V, D, C),
V finite set variables, finite set domain values, C finite set constraints.
constraint C pair (S, R), S, constraint scope, non-empty sequence
distinct variables V , R, constraint relation, relation whose arity matches
length S; relation
Pis considered set tuples. Therefore,
P size CSP instance
= (V, D, C) sum (S,R)C |S| |R|; total number tuples (S,R)C |R|. assume,
without loss generality, every variable occurs least one constraint scope every domain
element occurs least one constraint relation. Consequently, size instance least
large number variables I. write var (C) set variables occur
scope constraint C.
assignment instantiation mapping set V variables domain D.
assignment satisfies constraint C = ((x1 , . . . , xn ), R) ( (x1 ), . . . , (xn )) R, satisfies
CSP instance satisfies constraints. instance consistent satisfiable
satisfied assignment. CSP problem deciding whether given instance CSP
consistent. B OOLEAN CSP denotes CSP Boolean domain {0, 1}. r-CSP denote
restriction CSP instances arity constraint r.
primal graph CSP instance vertices variables I, two variables
joined edge variables occur together constraint I. incidence
graph CSP instance bipartite graph, one side consists variables
side consists constraints I; variable constraint joined edge
variable occurs constraint.
tree decomposition graph G = (V, E) pair (T, ) consisting tree mapping
assigns node subset (t) V following conditions satisfied:
(i) every edge {u, v} E node u, v (t); (ii) three nodes
t1 , t2 , t3 (t2 ) (t1 ) (t3 ) t2 lies path t1 t3 . width
(T, ) size largest set (t) minus 1. treewidth G smallest width
tree decompositions. Bounding treewidth classical method restricting structure
CSP instances. method dates back Freuder (1982). treewidth parameter applied
CSP terms primal graphs incidence graphs giving rise primal treewidth (also called
induced width (Dechter, 2003)) incidence treewidth, respectively (Samer & Szeider, 2010),
CSP instances.
instance = (V, D, C) CSP define following basic parameters.
vars: number |V | variables, usually denoted n.
P
size: size CSP instance defined (S,R)C |S| |R|.
dom: number |D| values; is, union values variables
assume.
cons: number |C| constraints.
208

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

arity: maximum size constraint scope.
deg: maximum number occurrences variable.
tw: treewidth primal graph I.
tw : treewidth incidence graph I.
propositional formula F set variable {x1 , . . . , xn } conjunctive normal form
(CNF) conjunction set clauses {C1 , . . . , Cm }, clause Ci , = 1, . . . , m,
disjunction literals (i.e., variables negations variables). say propositional
formula F satisfiable exists truth assignment variables F assigns least
one literal clause F value 1 (TRUE); also say case satisfies F .
CNF-S ATISFIABILITY problem, CNF-S short, given formula F CNF form, decide
whether F satisfiable. width clause CNF formula F number literals
clause. k-CNF-S problem, k 2, restriction CNF-S problem
instances width clause k. well known k-CNF-S
problem k 3 NP-complete (Garey & Johnson, 1979), whereas 2-CNF-S problem
solvable polynomial time (Papadimitriou, 1994).
2.2 Global Constraints
often preferred represent constraint succinctly listing tuples
constraint relation. intensionally represented constraint called global constraint (Rgin,
2011; Van Hoeve & Katriel, 2006). Global Constraints Catalogue (Beldiceanu, Carlsson, &
Rampon, 2006) lists several hundred global constraints. paper focus following
global constraints.
AllDifferent global constraint probably best-known, influential,
studied global constraint constraint programming (Van Hoeve & Katriel, 2006). admits
efficient matching based filtering algorithms (Rgin, 1994). AllDifferent constraint
set variables satisfied variable assigned different value.
global constraints NValue (Pachet & Roy, 1999), AtLeastNValue (Rgin, 1995), AtMostNValue (Bessiere, Hebrard, Hnich, Kiziltan, & Walsh, 2006) widely used constraint
programming (Beldiceanu et al., 2006). constraint C associated integer
nC N; consider nC given integer, value variable CSP instance. NValue constraint C set SC variables satisfied number distinct
values assigned variables SC exactly nC . AtLeastNValue AtMostNValue
constraints satisfied number distinct values nC nC , respectively.
special case NValue AtLeastNValue constraint C nC equals arity C
equivalent AllDifferent constraint.
global constraint cTable table constraint compressed tuples. global constraint
admits potentially exponential reduction space compared extensional table
constraint propagated using variant GAC-schema algorithm (Katsirelos
& Walsh, 2007). cTable constraints also studied name generalized DNF
constraints (Chen & Grohe, 2010). cTable constraint pair (S, U ) = (v1 , . . . , vr )
209

fiDE

H AAN , K ANJ , & ZEIDER

non-empty sequence distinct variables, U set compressed tuples,
sequences form (V1 , . . . , Vr ), Vi D(vi ), 1 r. One compressed tuple
(V1 , . . . , Vr ) represents tuples (d1 , . . . , dr ) di Vi . Thus, decompression
one compute (S, U ) (unique) equivalent table constraint (S, R) R contains
tuples represented compressed tuples U .
CSP constraints AllDifferent constraints denoted CSP6= . variant CSP
studied Fellows, Friedrich, Hermelin, Narodytska, Rosamond (2011b) called
MAD-CSP (multiple different CSP). CSP constraints NValue, AtLeastNValue,
AtMostNValue constraints, denoted CSP= , CSP , CSP , respectively. CSP
constraints cTable constraints denoted CSPc .
note CSP6= , CSP= , CSP , CSP , CSPc , NP-complete. fact, CSP6= (and therefore general CSP ) even NP-hard instances consisting two constraints (Kutz,
Elbassioni, Katriel, & Mahajan, 2008), CSP CSP= even NP-hard instances consisting single constraint (Bessiere et al., 2007). CSPc clearly NP-hard contains classical CSP
(with table constraints) special case. Hence considered problems admit representation
NP-hard combinatorial problems.
Consider CSP instance models real-world problem uses, among others,
global constraints considered above, say AllDifferent constraint. Then, combine
AllDifferent constraints instance new global constraint, multi-AllDifferent constraint.
Filtering combined constraint polynomial time equivalent solving one instance CSP6= .
combination several global constraints new one considered several
different global constraints (see, e.g., Hnich et al., 2004; Rgin & Rueher, 2000).
Guarantees limits polynomial-time preprocessing single NValue, AtLeastNValue,
AtMostNValue constraints given Gaspers Szeider (2014).
Boolean versions global constraints problems, parameters vars, dom,

cons, arity, deg, tw,
CSP. size instance = (V, D, C)
P tw , defined
6=
CSP defined CC |SC |. CSP= , CSP , CSP , size instance = (V, D, C)

Pdefined
| + log (nC )). instance = (V, D, C) CSPc , size defined
PCC (|SCP
c
(S,U )C
(V1 ,...,Vr )U (|V1 | + + |Vr |). Note definition instance size CSP
encompasses CSP.
2.3 Subexponential Time
proper complexity function complexity theory stands nondecreasing function f
computable O(n + f (n)) time O(f (n)) space, n length input (see Papadimitriou, 1994). time complexity functions used paper assumed proper complexity
function. o() notation used denotes oeff () notation (Flum & Grohe, 2006). formally,
two proper complexity functions f, g : N N, writing f (n) = o(g(n)) mean
exists proper complexity function (n) : N N, n0 N, f (n) g(n)/(n)
n n0 . () notation defined similarly above.
clear CSP CNF-S solvable time domn |I|O(1) 2n |I|O(1) , respectively,
input instance n number variables I. say CSP (resp. CNF-S AT)
solvable uniform subexponential time exists algorithm solves problem time
domo(n) |I|O(1) (resp. 2o(n) |I|O(1) ). Using results Chen, Kanj, Xia (2009) Flum
210

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

Grohe (2006), definition equivalent following: CSP (respectively, CNF-S AT)
solvable uniform subexponential time exists algorithm = 1/`, `
positive integer, solves problem time domn |I|O(1) (resp. 2n |I|O(1) ). CSP (resp. CNF-S AT)
solvable nonuniform subexponential time = 1/`, ` positive integer,
exists algorithm solves problem time domn |I|O(n) (resp. 2n |I|O(1) ) (that is,
algorithm depends ). note problem admits subexponential-time algorithm (uniform
nonuniform) means improve exponent exponential-term
running time algorithm indefinitely.
Let Q Q0 two problems, let 0 two functions defined instances Q
Q0 , respectively, assigning instance corresponding problem parameter value.
case CSP CNF-S AT, 0 assign number variables instances
problems. subexponential-time Turing reduction family (Impagliazzo, Paturi & Zane, 2001; see
also Flum & Grohe, 2006) serf-reduction 1 short, algorithm oracle Q0
computable functions f, g : N N satisfying: (1) given pair (I, ) Q
= 1/` (` positive integer), decides time f (1/)dom(I) |I|O(1) (for CNF-S
dom = 2); (2) oracle queries form 0 Q0 posed input (I, ),
0 (I 0 ) g(1/)((I) + log |I|).
optimization class SNP consists search problems expressible second-order existential
formulas whose first-order part universal (Papadimitriou & Yannakakis, 1991). Impagliazzo, Paturi,
Zane (2001) introduced notion completeness class SNP serf-reductions,
identified class problems complete SNP serf-reductions,
subexponential-time solvability problems implies subexponential-time solvability
problems SNP. Many well-known NP-hard problems proved SNP-complete
serf-reduction, including 3-S AT, V ERTEX C OVER, NDEPENDENT ET, extensive
efforts made last three decades develop subexponential-time algorithms
success. fact led exponential-time hypothesis, ETH, equivalent
statement SNP problems solvable subexponential time:
Exponential-Time Hypothesis (ETH): problem k-CNF-S AT, k 3, cannot solved
time 2o(n) , n number variables input formula. Therefore, exists
c > 0 k-CNF-S cannot solved time 2cn .
following result implied, using standard technique renaming variables (Impagliazzo,
Paturi & Zane, 2001, Corollary 1, 2) proof Sparsification Lemma (Impagliazzo,
Paruri & Zane, 2001; Flum & Grohe, 2006, Lemma 16.17). sake completeness, provide
sketch aforementioned results literature combined give statement
lemma.
Lemma 1. k-CNF-S (k 3) solvable 2o(n) time k-CNF-S linear
number clauses number occurrences variable 3 solvable
time 2o(n) , n number variables formula (note size instance
k-CNF-S polynomial n). particular, choosing k = 3 get: 3-CNF-S every
variable occurs 3 times, denoted 3-3-S AT, solvable 2o(n) time unless ETH fails.
1. Serf-reductions introduced Impagliazzo, Paturi, Zane (2001). use definition given Flum
Grohe (2006). slight difference two definitions, latter definition flexible
purposes.

211

fiDE

H AAN , K ANJ , & ZEIDER

Proof. shown Impagliazzo et al. (2001, Corollary 1, 2) that, k 3,
serf-reduction k-CNF-S k-CNF-S number clauses linear
number variables n. instance k-CNF-S = O(n), total number
occurrences variables also linear n (because width clause k).
variable appears ` > 3 times, using standard technique renaming
variables, replace (rename) ` occurrences new variable, add cycle
` implications (using ` new 2-CNF-S clauses) enforcing ` new variables receive
value satisfying assignment. resulting formula k-CNF-S formula
number occurrences variable 3, number new variables
linear original number variables n. gives serf-reduction k-CNF-S (for
k 3) k-CNF-S number occurrences variable 3 (and hence
also linear number clauses).
ETH become standard hypothesis complexity theory (Lokshtanov et al., 2011).
Remark 1. paper, consider CSP (with without global constraints) restricted
instances certain parameter (g(n)) (resp. (g(n)), O(g(n)), o(g(n))),
proper complexity function g(n) number variables n instance, mean CSP restricted
instances parameter upper bounded prespecified function (g(n))
(resp. (g(n)), O(g(n)), o(g(n))). example, say CSP restricted instances whose
primal treewidth o(n) solvable subexponential time mean following: proper
complexity function g(n) = o(n), problem consisting restriction CSP instances
whose primal treewidth g(n) solvable subexponential time.

3. CSP CSPc
section investigate subexponential-time complexity CSP CSPc respect
restrictions various structural parameters. start Subsection 3.1 establishing relations
among subexponential-time complexity CNF-S AT, CSP, CSPc ; results
corner stones results subsequent (sub)sections rely upon.
3.1 Relations Among CSP, CSPc , CNF-S
start following simple observation:
Observation 1. positive integer constant r, serf-reduction r-CSP r-CSPc
vice versa. Moreover, reductions produces instance set
variables domain values original instance.
fact serf-reduction r-CSP r-CSPc trivially follows fact
r-CSP special case r-CSPc . opposite direction, observe cTable constraint
bounded arity decompressed table constraint, set variables,
polynomial time enumerating tuples satisfy cTable constraint. polynomial
time serf-reduction r-CSPc r-CSP.
Proposition 1. B OOLEAN r-CSP, r 3, solvable subexponential time
ETH fails.
212

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

Proof. prove first part statement, give serf-reduction r-CNF-S
B OOLEAN r-CSP. Given instance F r-CNF-S AT, easy see correspond
every clause F constraint arity r (over variables) containing 2r
tuples clause satisfied corresponding constraint is. Clearly,
polynomial-time reduction results instance B OOLEAN r-CSP variable-set
F , hence serf-reduction.
prove converse, give serf-reduction B OOLEAN r-CSP r-CNF-S AT. Let
instance B OOLEAN r-CSP. construct instance F r-CNF-S follows. Let C
constraint I. Since arity r, C contains r variables 2r tuples.
associate C set clauses F , width r, C satisfied
associated clauses are. easily done considering tuple
variable-set C contained C, adding clause F consisting disjunction
negation set literals tuple represents. Since tuple represents conjunction
set r literals, results 2r clauses, disjunction
r literals. Clearly, F instance r-CNF-S variable-set I, F
computable polynomial time. proof follows.
following proposition suggests Proposition 1 may extend r-CSP unbounded
domain size. Chen, Chor, Fellows, Huang, Juedes, Kanj, Xia (2005) showed C LIQUE
(decide whether given graph N vertices contains complete subgraph k vertices) solvable
time N o(k) ETH fails. converse, however, generally believed true.
idea behind proof following proposition goes back paper Papadimitriou
Yannakakis (1999), used context studying complexity database queries.
provide proof completeness.
Proposition 2. 2-CSP solvable subexponential time C LIQUE solvable time N o(k) .
Proof. Assume 2-CSP solvable time domo(n) , let (G, k) instance C LIQUE,
G N vertices. Assume vertices G labeled {1, . . . , N }. construct
instance 2-CSP follows. variable-set {x1 , . . . , xk }, variables range
domain {1, . . . , N }; is, variables used select vertices G form
clique (if exists). every pair distinct variables xi , xj , < j, add constraint Cij
containing pairs/tuples form (u, v) uv edge G u < v.
difficult verify G clique k vertices consistent. Since
k variables dom = N , follows I, hence (G, k), decided time N o(k) .
proof follows.
Observation 1, statement Proposition 1 holds true B OOLEAN r-CSPc ,
statement Proposition 2 holds true 2-CSPc .
explore next relation B OOLEAN CSP unbounded arity CNF-S AT.
show B OOLEAN CSP solvable nonuniform subexponential time CNF-S AT.
so, exhibit nonuniform subexponential-time Turing reduction CNF-S B OOLEAN
CSP.
Intuitively, one would try reduce instance F CNF-S instance CSP
associating every clause F constraint whose variables variables clause,
whose relation consists tuples satisfy clause. slight complication
attempted reduction number tuples constraint could exponential
213

fiDE

H AAN , K ANJ , & ZEIDER

number variables corresponding clause linear (in total number variables).
overcome subtlety, idea first apply subexponential-time (Turing) reduction,
originally due Schuler (2005) also used analyzed Calabro, Impagliazzo, Paturi
(2006), reduces instance F subexponentially many (in n) instances width
clause constant k; case, however, reduce width suitable
nonconstant value. follow reduction reduction B OOLEAN CSP described
proof Proposition 1.
Theorem 1. B OOLEAN CSP nonuniform subexponential-time algorithm
CNF-S AT.
Proof. Suppose B OOLEAN CSP solvable nonuniform subexponential time. every
> 0, exists algorithm A0 that, given instance B OOLEAN CSP n0 variables,
0
0
A0 solves time 2n |I|c , constant c0 > 0.
Let 0 < < 1 given. describe algorithm solves CNF-S time 2n mO(1) .
n
Set k = b 2(1+c
0 ) c. Let F instance CNF-S n variables clauses. algorithm
search-tree algorithm, works follows. algorithm picks clause C F width
k; clause exists algorithm stops. Let l1 , . . . , lk k literals C.
algorithm branches C two branches. first branch, referred left branch, corresponds
one k literals assigned value 1 satisfying assignment sought,
case C replaced F clause (l1 . . . lk ), thus reducing number clauses F
width k 1. second branch, referred right branch, corresponds assigning
k literals value 0 satisfying assignment sought; case values
variables corresponding literals determined, variables removed
F F gets updated accordingly. Therefore, right branch number variables F
reduced k. execution part algorithm described far depicted binary
search tree whose leaves correspond instances resulting F end branching,
clause width k. running time part algorithm proportional
number leaves search tree, equivalently, number root-leaf paths search
tree.
continue description algorithm , illustrate branching phase
algorithm following concrete example. Suppose F instance CNF-S
6 variables {x1 , . . . , x6 } consisting 3 clauses C1 , C2 , C3 , C1 = {x1 , x2 , x3 , x4 , x5 },
C2 = {x2 , x3 , x5 , x6 }, C3 = {x1 , x3 , x4 , x5 , x6 }. Suppose want reduce formulawidth 3 (i.e., k = 3). pick clause width 3, say C1 , branch 3
literals C1 , say x1 , x2 , x3 . left branch (at least one 3 literals 1) obtain
(CNF) formula F1 consisting 3 clauses {x1 , x2 , x3 }, C2 , C3 ; right branch (each
literals assigned 0), obtain formula F2 consisting clause {x4 , x5 } (C2 C3
satisfied case). Note branch anymore F2 since width 2. Since F1
still contains clauses width 3, namely C2 C3 , branch F2 picking
clause width 3, say C3 , branching 3 literals C3 , say x1 , x3 , x4 . left
branch, obtain formula F1,1 consisting 3 clauses {x1 , x2 , x3 }, C2 , {x1 , x3 , x4 };
right branch obtain formula F1,2 consisting 2 clauses {x2 , x5 , x6 } {x5 , x6 }.
branch F1,2 since width 3. Since F1,1 contains clause C2 width 3,
branch 3 literals C2 , say x2 , x3 , x5 . left branch obtain formula F1,1,1 consisting
3 clauses {x1 , x2 , x3 }, {x2 , x3 , x5 }, {x1 , x3 , x4 }; branch F1,1,1 since
214

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

F
F1
F1,1

F2
F1,2

F1,1,1 F1,1,2

Figure 1: search tree corresponding branching algorithm example.

width 3. right branch obtain formula F1,1,2 consisting two clauses {x1 , x4 }
{x6 }; branch F1,1,2 . algorithm branch anymore since leaves
search tree formulas width 3. Figure 1 depicts search tree corresponding
branching example.
continue description algorithm . Let F 0 instance resulting F
leaf search tree. reduce F 0 instance 0 B OOLEAN CSP follows.
clause C 0 F 0 , correspond constraint whose variable-set set variables C 0 ,
whose tuples consist 2k 1 tuples corresponding assignments variables C 0
satisfy C 0 . Clearly, 0 constructed time 2k mO(1) (note number clauses
F 0 m). instance 0 , apply algorithm A0 = /2. algorithm
accepts F A0 accepts one instances 0 , F 0 resulting F leaf
search tree.
illustrate phase algorithm using example above, formulas F2 ,
F1,2 , F1,1,1 , F1,1,2 , corresponding leaves search tree (see Figure 1), associate
instance B OOLEAN CSP. example, instance B OOLEAN CSP associated F1,2
consists two constraints. first constraint corresponds clause {x2 , x5 , x6 }; (x2 , x5 , x6 )
sequence variables,
{(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (1, 0, 1)} relation. second constraint corresponds clause {x5 , x6 } F1,2 ; (x5 , x6 ) sequence variables,
{(0, 0), (0, 1), (1, 0)} relation.
running time upper bounded number leaves search tree, multiplied
polynomial length F (polynomial m) corresponding (maximum) total running
time along root-leaf path search tree, multiplied time construct instance 0
corresponding F 0 leaf tree, multiplied running time algorithm A0
applied 0 . Note binary search tree depicting execution algorithm
complete binary tree. upper bound size search tree, let P root-leaf path
search tree, let ` number right branches along P . Since right branch removes k
variables, ` n/k number variables left instance F 0 leaf endpoint P
n `k. Noting length path ` right branches + ` (each left branch
reduces 1 hence branches P , ` right branches),
conclude number root-leaf paths, hence number leaves, search tree

Pdn/ke
`=0 m+`
.
`
215

fiDE

H AAN , K ANJ , & ZEIDER

reduction F 0 instance B OOLEAN CSP carried time 2k mO(1) ,
results instance 0 number variables n0 = n `k, number
constraints m, total size 2k mO(1) . Summing possible paths
search tree, running time 2n mO(1) . consequence following estimation:
dn/ke

X
`=0


+ ` k O(1) (n`k) k O(1) c0
2
2
.(2
)
`

2

(1+c0 )k+n

2

(1+c0 )k+n



O(1)

dn/ke

X
`=0

mO(1)




+ dn/ke
`


2m
dn/ke

0

2(1+c )k+n mO(1) (2m)n/k
2

(1+c0 )k+n
n

2

O(1)



O(1)

(1)
(2)
(3)

.

first inequality follows replacing ` larger value dn/ke upper part
binomial coefficient, upper bounding term 2`k 1. Inequality
(1) follows



2m
fact largest binomial coefficient summation m+dn/ke

(m

dn/ke,
dn/ke
dn/ke
otherwise constant, instance CNF-S solved polynomial time
beginning), hence, summation replaced largest binomial coefficient multiplied
number terms (dn/ke+1) summation, gets absorbed term mO(1) .
Inequality (2) follows trivial upper bound binomial coefficient (the ceiling
removed polynomials get absorbed). Inequality (3) follows noting n/k
constant (depends ), substituting k values/bounds.
follows algorithm solves CNF-S time 2n mO(1) . Therefore, B OOLEAN
CSP nonuniform subexponential-time algorithm, CNF-S AT. algorithm
nonuniform polynomial factor running time (exponent m) depends .
Theorem 1 provides strong evidence B OOLEAN CSP solvable subexponential
time. show next B OOLEAN CSPc solvable subexponential time weaker
hypothesis assumed Theorem 1. SAT[3] denote satisfiability normalized
propositional formulas depth 3 (Flum & Grohe, 2006), is, propositional formulas
conjunction-of-disjunction-of-conjunction literals. well known SAT[3] solvable
subexponential time W -hierarchy parameterized complexity collapses second
level (Chen et al., 2006), is, W [2] = FPT, consequence deemed unlikely
would imply CNF-S solvable subexponential time (Chen et al., 2006).
Proposition 3. Unless W [2] = FPT, B OOLEAN CSPc solvable subexponential time.
Proof. easy see instance SAT[3] polynomial-time reducible instance
B OOLEAN CSPc set variables. reduction, every disjunction-of-conjunction
literals Boolean formula associated cTable constraint, compressed tuple
(V1 , . . . , Vr ) constraint represents conjunction literals: positive literal xi represented
Vi = {1}, negative literal xi represented Vi = {0}, variable xi occur
conjunction, represented Vi = {0, 1}. Therefore, serf-reduction SAT[3]
B OOLEAN CSPc . statement follows result Chen et al. (2006).
216

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

3.2 Instance Size Number Tuples
section give characterizations subexponential-time complexity CSP CSPc
respect instance size number tuples. also show subexponential-time
solvability B OOLEAN CSP B OOLEAN CSPc linear size, linear number tuples,
equivalent statement ETH fails.
Proposition 4. Unless ETH fails, restriction B OOLEAN CSP instances whose size
(n) solvable subexponential time.
Proof. Let s(n) = (n) cn proper complexity function, c > 0 constant. Suppose
restriction B OOLEAN CSP instances size s(n) solvable subexponential
time, show 3-CNF-S solvable subexponential time. Lemma 1,
sufficient show 3-CNF-S linear number clauses solvable 2o(n) time. Using
padding argument2 , prove preceding statement assuming linear upper bound
number clauses; true pad instance 3-CNF-S large
number new variables obtain equivalent instance number clauses satisfies
(smaller) desired upper bound. pick linear upper bound cn/24, c constant
upper bound s(n).
Let F instance 3-CNF-S n variables cn/24 clauses. reduce F
instance B OOLEAN CSP using reduction described proof Theorem 1:
clause C F correspond constraint whose variables C whose tuples
corresponding satisfying assignments C. Since width C 3 number
clauses cn/24, instance consists cn/24 constraints, containing
3 variables 8 tuples. Therefore, size cn. apply hypothetical
subexponential-time algorithm . Since |I| linear n, since reduction takes linear time
n, conclude 3-CNF-S solvable time 2o(n) nO(1) = 2o(n) . proof follows.
Since B OOLEAN CSP special case B OOLEAN CSPc , statement Proposition 4 holds
true B OOLEAN CSPc well.
Proposition 5. restriction CSPc instances o(n) tuples solvable subexponential
time.
Proof. Let s(n) = o(n) proper complexity function, consider restriction CSPc
instances s(n) tuples. show problem solvable time doms(n) |I|O(1) .
Let instance problem consideration. Consider algorithm that,
compressed tuple constraint I, branches whether compressed tuple satisfied
satisfying assignment sought. branch one compressed tuple
constraint selected satisfied rejected, likewise branch compressed
tuple constraint selected. remaining branch, algorithm checks branch
consistent, would imply assignment variables aligns branch
satisfies I. Checking branch consistent done follows. Let x variable I, let
t1 , . . . , tp compressed tuples selected branch cTables contain x variable.
2. padding argument general tool used complexity theory extend result larger class problems.
purpose paper, padding argument works adding/padding dummy part instance create
equivalent new instance relation holds true certain parameters new instance. use
padding argument several times paper, skip details argument clear.

217

fiDE

H AAN , K ANJ , & ZEIDER

Let Vix , = 1, . . . , p, set values admissible xTin cTable ti selected
branch. branch consistent respect x pi=1 Vix 6= , branch consistent
consistent respect every variable I. Clearly, given branch algorithm A,
checking whether branch consistent done polynomial time |I|.
branch consistent, algorithm accepts; algorithm rejects branch corresponds
consistent assignment. Clearly, algorithm correct, runs time 2s(n) |I|O(1) =
doms(n) |I|O(1) (we assume dom 2, otherwise problem trivial).
Noting number tuples lower bound instance size, following proposition
follows Proposition 4 Proposition 5:
Proposition 6. restriction CSP instances number tuples o(n) solvable
subexponential time, unless ETH fails, restriction CSP instances
number tuples (n) solvable subexponential time. holds true CSPc .
Next, show subexponential-time solvability B OOLEAN CSP linear size,
linear number tuples, equivalent statement ETH fails. first need
following proposition:
Proposition 7. ETH fails restriction B OOLEAN CSPc instances linear
number tuples solvable subexponential time.
Proof. give polynomial-time serf-reduction B OOLEAN CSPc linear number tuples
C IRCUIT ATISFIABILITY linear size circuits. result follow fact
C IRCUIT ATISFIABILITY linear size circuits SNP-complete serf-reductions (and hence
solvable subexponential time ETH fails) (Impagliazzo, Paturi & Zane, 2001).
Let s(n) cn proper complexity function, c > 0 constant. Consider restriction
B OOLEAN CSPc instances number tuples cn, let instance
problem. construct Boolean circuit CI follows. circuit CI depth-3 circuit
whose output gate AND-gate, whose set variables I.
cTable constraint correspond OR-gate gT connected output gate C. Let
cTable constraint Boolean variables (v1 , . . . , vr ), let = (V1 , . . . , Vr )
compressed tuple . correspond AND-gate gt CI connected OR-gate
gT corresponding CI ; input gt literals CI determined follows.
vi , = 1, . . . , r, Vi = {1} connect variable corresponding vi CI gt ,
Vi = {0} connect negation variable corresponding vi CI gt (we nothing
Vi = {0, 1} constraint imposed tuple Boolean value vi ).
easy see satisfied corresponding gate gt CI evaluates 1, hence
satisfied gT evaluates 1. follows satisfied CI is. Moreover,
size CI linear number tuples I, subsequently number variables
CI . Since construction CI done polynomial time, proof follows.
Clearly, statement proposition holds true B OOLEAN CSP well.
Proposition 4, combined Proposition 7 noting size upper bound
number tuples, gives following results:
Theorem 2. restriction B OOLEAN CSP instances linear number tuples solvable
subexponential time ETH fails. result holds B OOLEAN CSPc .
218

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

Theorem 3. restriction B OOLEAN CSP instances linear size solvable subexponential time ETH fails. result holds B OOLEAN CSPc .
3.3 Number Constraints Treewidth
section give characterizations subexponential-time complexity CSP CSPc
respect number constraints, treewidth primal incidence graphs.
start withe following proposition:
Proposition 8. Unless ETH fails, restriction CSP instances number
constraints (1) solvable subexponential time.
Proof. Let (n) = (1) proper complexity function. show that, unless ETH fails,
restriction CSP instances cons (n), denoted CSP solvable domo(n)
time. Proposition 1, suffices provide serf-reduction B OOLEAN 3-CSP linear
number constraints B OOLEAN CSP .
Let instance B OOLEAN CSP cons = n0 cn, c > 0 constant. Let
C1 , . . . , Cn0 constraints I; partition constraints arbitrarily b(n)c many groups
C1 , . . . , Cr , r b(n)c, containing dn0 /(n)e constraints. serf-reduction
works follows. merges constraints group Ci , = 1, . . . , r, one constraint
Ci0 follows. variable-set Ci0 consists union variable-sets constraints
Ci . constraint C Ci , iterate tuples C. selecting tuple
constraint Ci , check selected tuples consistent, merge tuples
single tuple add Ci0 . merging tuples mean form single tuple variables
tuples, value variable value selected tuples (note
values consistent). Since constraint arity 3, hence contains
8 tuples, since group contains dn0 /(n)e constraints, Ci0 constructed
0
time 8dn /(n)e n0O(1) = 2o(n) , hence, constraints C10 , . . . , Cr0 constructed
time 2o(n) nO(1) = 2o(n) . form instance 0 whose variable-set I, whose
constraints C10 , . . . , Cr0 . Since r b(n)c, 0 instance CSP . Moreover, easy see
consistent 0 is. Since 0 constructed subexponential time
number variables 0 I, follows serf-reduction B OOLEAN
3-CSP linear number constraints CSP .
Proposition 9. restriction CSPc instances cons = O(1) solvable polynomial
time.
Proof. number constraints instance O(1), polynomial time enumerate
subset tuples subset contains exactly one compressed tuple constraint
instance (because size subset O(1)). verify consistency (as
described proof Proposition 5), deduce instantiation set variables exists
polynomial time.
Clearly, Proposition 8 holds true CSPc , Proposition 9 holds true CSP. Therefore,
combining Proposition 8 Proposition 9 have:
Theorem 4. restriction CSP instances O(1) constraints solvable polynomial
time, unless ETH fails, restriction CSP instances (1) constraints
solvable subexponential time. holds true CSPc .
219

fiDE

H AAN , K ANJ , & ZEIDER

turn attention treewidth. following proposition:
Proposition 10. Unless CSP (in general) solvable subexponential time (and hence ETH
fails), restriction CSP instances whose tw (n) solvable subexponential time.
Proof. Let s(n) = cn, c > 0 constant, consider restriction CSP instances
whose tw s(n), denoted L INEAR -tw-CSP. Note number vertices primal
graph n, hence tw n. Therefore, c 1, statement trivially follows. Suppose
c < 1, let instance CSP n variables. padding d1/ce disjoint copies
obtain instance 0 equivalent I, whose number variables N 0 = d1/cen,
whose tw I. Since tw n, follows tw 0
cN 0 , hence 0 instance L INEAR -tw-CSP. gives serf-reduction CSP
L INEAR -tw-CSP.
note hypothesis CSP solvable subexponential time theorem
implies ETH fails Proposition 1, implies CNF-S nonuniform
subexponential-time algorithm Theorem 1.
following theorem provides tight characterization subexponential-time complexity
CSPc (and CSP) respect primal incidence treewidth.
Theorem 5. following statements true:
(i) restriction CSPc instances tw = o(n) solvable subexponential time,
unless ETH fails, restriction CSPc instances tw = (n)
solvable subexponential time.
(ii) restriction CSPc instances tw = O(1) solvable subexponential time
(even P), unless ETH fails, restriction CSPc instances tw = (1)
solvable subexponential time.
Proof. (i) Note upper bound primal treewidth implies upper bound
arity. Let instance CSPc whose tw = o(n). Since arity = o(n), constraint contains
d(n)o(n) many satisfying tuples. decompressing compressed tuples, i.e., enumerating
satisfying tuples constraint time (d(n)o(n) ) reduce instance
instance CSP set variables, domain, primal tree width.
compute tree decomposition width 4 tw time 24.38tw |I|O(1) (Amir, 2010). well
known (Freuder, 1990) CSP solvable time (d(n)tw ) (d(n)o(n) ), hence
decided subexponential time. hardness result follows hardness result
CSP Proposition 10.
(ii) hardness result direct consequence hardness result Theorem 4, since cons
upper bound tw . Establishing first statement requires work. Consider instance
CSPc whose incidence treewidth constant w.
apply construction Samer Szeider (2010) transform equivalent instance
0 CSPc whose incidence treewidth w + 1 variable appears
scope 3 constraints. construction keeps constraints adds binary equality
constraints copies variables. equality constraints enforce variable copies
get assigned value. construction Samer Szeider stated table constraints
220

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

clearly works also cTable, since constraints changed all, newly
introduced constraints binary.
Consider dual graph Gd 0 vertices constraints 0 , two
constraints joined edge share least one variable.
variable appears scope 3 constraints, result Samer Szeider (2010,
Lemma 2(5)) applies, based construction due Kolaitis Vardi (2000),
follows treewidth Gd 2w + 2.
Next obtain CSP instance 00 dual instance 0 . construction
straightforward generalization known construction CSP table constraints (see, e.g.,
Dechter, 2003, Definition 2.1). constraint C = (S, U ) 0 gives rise variable x[C] 00 ;
domain D(x[C]) U , set compressed tuples. two variables x[C1 ], x[C2 ]
00 corresponding constraints C1 = (S1 , U1 ) C2 = (S2 , U2 ), respectively, 0 share
least one variable add binary table constraint ((x[C1 ], x[C2 ]), R). Here, relation R contains
pairs (t1 , t2 ) U1 U2 consistent sense variables x appear
scopes C1 C2 , coordinate Vi1 t1 corresponding x coordinate Vj2
t2 corresponding x nonempty intersection. straightforward see 0 00
equivalent. remains observe Gd isomorphic primal graph 00 , hence
primal treewidth 00 2w + 2, constant. Hence solve 00 polynomial time (Freuder,
1990).
Clearly results Theorem 5 hold true CSP since positive results theorem
shown general CSPc , negative results proved CSP.
note difference subexponential-time complexity CSPc (and CSP)
respect two structural parameters tw tw : Whereas threshold function
subexponential-time solvability CSPc CSP respect tw o(n), threshold function
respect tw O(1).
3.4 Degree Arity
section give characterizations subexponential-time complexity CSP CSPc
respect degree arity.
Proposition 11. Unless ETH fails, restriction CSP instances whose deg 2
solvable subexponential time.
Proof. statement follows proof Theorem 1 noting that, Lemma 1, one
use 3-3-S reduction. result instances B OOLEAN 3-CSP degree
3 well. variable x degree 3 instance B OOLEAN 3-CSP, introduce two
new variables x0 , x00 , add constraint whose variables {x, x0 , x00 }, containing two
tuples (0, 0, 0) (1, 1, 1); constraint stipulates values x, x0 , x00 same.
substitute variable x one constraints appears x0 , another constraint
appears x00 . Therefore, new instance, degree x, x0 , x00 becomes 2.
repeating step every variable degree 3, obtain instance B OOLEAN 3-CSP
degree variable 2. Since increase number variables linear,
reduction serf-reduction 3-3-S B OOLEAN 3-CSP degree 2,
gives statement proposition.
221

fiDE

H AAN , K ANJ , & ZEIDER

Proposition 12. Unless ETH fails, restriction CSP instances whose arity 2 (and
dom 3) solvable subexponential time.
Proof. give serf-reduction 3-C OLORABILITY problem CSP arity = 2
dom = 3. Since 3-C OLORABILITY problem SNP-complete serf-reductions (Impagliazzo, Paturi & Zane, 2001), statement theorem follow. Recall 3C OLORABILITY problem asks vertices given graph properly colored (no two
adjacent vertices assigned color) 3 colors.
reduction folklore. Given instance G = (V, E) 3-C OLORABILITY, G
n vertices, construct instance CSP follows. variables correspond
vertices G, domain corresponds color-set {1, 2, 3}. every edge graph
construct constraint arity = 2 two variables corresponding endpoint
edge. constraint contains tuples corresponding valid colorings endpoints edge.
easy see G 3-coloring consistent. Since instance
vars = n, number vertices G, since arity = 2 dom = 3,
(polynomial-time) serf-reduction 3-C OLORABILITY problem CSP arity = 2
dom = 3.
Clearly, Proposition 11 Proposition 12 hold true CSPc well.
note CSPc CSP dom = 2 arity = 2 solvable polynomial time via
simple reductions 2-CNF-S AT.
turns out, CSP CSPc exhibit subexponential-time complexity behavior
respect restrictions structural parameters considered above.
hand, negative result proved Proposition 3 B OOLEAN CSPc assumes weaker hypothesis
result B OOLEAN CSP proved Theorem 1.

4. CSP6= , CSP= , CSP , CSP
section consider CSP6= , CSP= , CSP , CSP . Since results CSP= , CSP ,
CSP related, rely results establish CSP6= , start presenting
results CSP6= .
4.1 CSP6=
Let instance CSP6= constraints C1 , . . . , Cc integer c > 0, set
variables {x1 , . . . , xn }. Denote Di , = 1, . . . , n, domain xi .
Proposition 13. CSP6= solved time (2n ).
Proof. reduce instance instance L IST C OLORING problem. Recall
L IST C OLORING problem given graph, whose vertices associated list
colors, asked decide exists proper coloring graph
vertex assigned color list. reduce instance L IST C OLORING, construct
graph G whose vertices x1 , . . . , xn (without loss generality, label vertices G
corresponding variables names I) edge two vertices
xi xj , 1 < j n, xi xj appear together constraint I.
vertex xi G, associate list colors Li = Di . difficult see
222

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

yes-instance CSP6= graph G proper list coloring. known L IST
C OLORING problem solvable time (2n ) (Bjrklund, Husfeldt, & Koivisto, 2009), hence
CSP6= .
Corollary 1. Let d(n) = (1) proper complexity function. restriction CSP6= instances
dom d(n) solvable subexponential time.
Proof. Let d(n) = (1) proper complexity function, consider restriction CSP6=
instances dom d(n). Proposition 13, CSP6= solvable time (2n ) =
(d(n)n/ log (d(n)) ) (domo(n) ).
note result may sound strange, especially taken conjunction
next proposition, implies problem becomes easier larger domain size.
explained fact domain size gets large, allowable upper bound
subexponential time solving problem (i.e., d(n)o(n) ) gets larger well.
Corollary 1, focus investigation subexponential-time complexity CSP6=
instances dom = O(1) = d, integer constant d. Note dom upper
bound arity constraint must arity dom (otherwise cannot satisfied).
2, constraint arity 2, CSP6= case reduces 2-CNF-S AT,
P. Therefore, assume remainder section 3.
Proposition 14. Unless ETH fails, restriction CSP6= instances dom = 3
cons = (n) solvable subexponential time.
Proof. suffices prove result cons = s(n), s(n) specific function
s(n) linear n ((n)), result would extend using padding argument function
linear n (we add new dummy variables new dummy constraints new
variables make relation constraints variables satisfy desired function
s()).
Lemma 1, 3-3-S solvable subexponential time unless ETH fails. standard
polynomial-time reduction 3-S 3-C OLORABILITY (see Cormen et al., 2009), establishing
NP-hardness 3-C OLORABILITY, reduces instance 3-S n variables clauses
instance 3-C OLORABILITY O(n + m) vertices O(n + m) edges. Therefore,
use reduction start 3-3-S instead 3-S AT, end instance
3-C OLORABILITY number vertices O(n) number edges O(n) well.
Hence, serf-reduction 3-3-S restriction 3-C OLORABILITY instances
whose size linear number vertices, denoted L INEAR -3-C OLORABILITY. use
standard reduction 3-C OLORABILITY CSP6= (in vertex becomes variable,
edge becomes constraint arity 2, domain set 3 colors), instead
start instance L INEAR -3-C OLORABILITY, obtain instance CSP6= n variables
(the number vertices graph), linear number constraints, domain size
dom = 3. Therefore, previous reduction serf-reduction L INEAR -3-C OLORABILITY
restriction CSP6= instances number constraints linear, dom = 3.
Composing two serf-reductions gives serf reduction 3-3-S problem
consideration, thus proves proposition.
Remark 2. note phrase statement Corollary 1 consider restriction
CSP6= instances dom = (1) (as paper)
223

fiDE

H AAN , K ANJ , & ZEIDER

restriction encompass slice problem hard (instances whose domain size upper
bounded constant), shown Proposition 14. explicitly consider instances
whose domain size lower-bounded function (1). Proposition 17, next section,
handled similarly.
Remark 3. consider restriction CSP6= instances cons = o(n)
dom = O(1). constraint must arity dom, hence, cons = o(n)
would follow total number variables o(n). follows Proposition 14
Corollary 1 provide tight characterizations subexponential-time complexity CSP6=
respect cons dom.
following proposition provides tight characterization subexponential-time complexity
CSP6= respect treewidth primal graph:
Proposition 15. restriction CSP6= instances tw = o(n) solvable subexponential time, unless ETH fails, restriction CSP6= instances tw = (n)
solvable subexponential time.
Proof. derive subexponential-time result, assume domain size d,
constant 3, case get CSP6= solvable subexponential time
Corollary 1. Let instance CSP6= treewidth primal graph o(n). Since
arity constraint domain size d, polynomial time
reduce instance CSP set variables, domain, constraints,
primal treewidth. part (i) Theorem 5, restriction CSP instances whose tw = o(n)
solvable subexponential time, hence decided subexponential time.
hardness result follows general observation primal treewidth CSP
instances. First note number variables n upper bound primal treewidth;
is, tw n. Therefore, upper bound s(n) = (n) tw, using padding argument (adding
linear number dummy new variables singleton constraints increase primal
treewidth) reduce general instance CSP6= instance tw s(n) cost
linear increase number variables instance size. provides serf-reduction
general instance CSP6= instance tw s(n) = (n). result follows
result CSP6= general instances (implied, e.g., Proposition 14).
well known tw arity (tw 1) tw tw + 1 (Kolaitis & Vardi, 2000).
arity = O(1), tw tw within multiplicative constant one another. Therefore,
Proposition 15 infer following tight result:
Proposition 16. restriction CSP6= instances tw = o(n) solvable subexponential time, unless ETH fails, restriction CSP6= instances tw = (n)
solvable subexponential time.
Remark 4. several width parameters CSP even general tw
sense instances tw small, width parameter small well;
instances width parameter small tw arbitrarily
large. Prominent examples width parameters hypertree width (Gottlob et al., 2002)
submodular width (Marx, 2013). lower bound statement Proposition 16 clearly carries
general width parameters. holds true lower bound statements
Proposition 19 Theorem 5.
224

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

4.2 CSP= , CSP , CSP
start presenting exact algorithm CSP ; reducing CSP CSP6= . use
example illustrated Figure 2 running example explain idea behind reduction.
example, instance CSP consists three constraints C1 , C2 , C3 , variables
C1 x1 , x2 , x3 , x4 , variables C2 x4 , x5 , variables C3 x1 , x5 , x6 , x7 .
domain x1 {a, b}, domain x2 x3 {b}, domain x4 {b, c}, domain
x5 {a}, domain x6 x7 {d, e}. number distinct values need
assigned variables C1 least 3, variables C2 least 2, variables
C3 least 3.
solution (i.e., assignment variables domain values) instance CSP ,
constraint C I, possible several variables C assigned value
solution (in running example forced assign x2 x3 value b). Therefore,
attempt straightforward reduction CSP CSP6= produces instance I,
solution instance CSP may solution instance CSP6= .
possible happens due fact variables removed
without affecting satisfiability I, solution constraint
still satisfied without considering values assigned variables.
algorithm starts trying subset variables subset exists
solution variables essential solution; algorithm removes
(nonessential) variables, updates instance, works toward finding solution
assumption resulting instance. (In running example, remove x3 C1 ;
see Venn diagram left Figure 2.) Even assumption, still possible
solution resulting instance, two variables constraint C assigned
value. One cannot simply ignore (remove) one variables basis removing
affect satisfiability C, removed variable may contribute satisfiability
constraint C, variable appears well. (In running example,
forced assign x1 x5 value, would violate constraint C3 CSP6= .)
Therefore, resulting instance, even though may satisfiable instance CSP , may
satisfiable instance CSP6= . However, shown Lemma 2, possible
instance reassign variable subset constraints appears in,
reassignment/repartitioning variable contributes satisfiability constraint
appears in. reassignment, resulting instance CSP becomes equivalent
instance CSP6= . (In running example, variable x5 contributing C3 , safely
reassigned C2 ; see Venn diagram right Figure 2.) proceed formal
proofs.
Let instance CSP constraints C1 , . . . , Cc integer value c > 0,
variables x1 , . . . , xn . Let ni , = 1, . . . , c, nonnegative integer associated
constraint Ci .

Denote Di , = 1, . . . , n, domain variable xi , let = ni=1 Di . Set k = |D|.
consider Ci , = 1, . . . , c, set consisting variables Ci , draw Venn
diagram Ci s, Venn diagram consists 2c many nonempty regions,
region Rj , j = 1, . . . , s, defined intersection sets containing
variables lie Rj Venn diagram. solution instance I, call variable xi
essential (to S) discounting value assigned xi violates least one constraints
(containing xi ), hence longer gives solution I. clear enumerating every
225

fiDE

H AAN , K ANJ , & ZEIDER

C20

C2

x5
x4
x2
C1

x5

x4

x1 x6 x7

x2

x1 x6 x7

C10

C3

C30

Figure 2: Illustration example reduction CSP CSP6= .
subset variables I, takes O(2n ) time, work assumption
looking solution every variable essential S. Since working instance
CSP , adding nonessential variables solution afterwards (and assigning values
domains) hurt solution. Therefore, without loss generality, assume
variables x1 , . . . , xn essential solution sought (if exists). start
following lemma.
Lemma 2 (The Repartitioning Lemma). Let instance CSP . solution
instance 0 set variables I, whose constraints
C10 , . . . , Cc0 , that:
(1) variables Ci0 subset Ci , = 1, . . . , c;
(2) numbers n1 , . . . , nc 0 ;
(3) solution 0 satisfying every value v, two distinct variables
xi , xj assigned value v solution 0 , set constraints xi belongs
0 disjoint xj belongs 0 .
Proof. Suppose solution S; discussion preceding lemma, assume
every variable essential S. define instance 0 set variables follows.
constants n1 , . . . , nc remain 0 . define constraints 0 sequence
changes performed constraints I; initially constraints 0 identical I.
every value v assigned variable solution S, let x1v , . . . , x`v variables
assigned value v S. xjv , j = 1, . . . , ` 1, considered listed order, let Cvj
j
set constraints containing xjv 0 , let Cv,
union constraints containing
j+1
j
j
`
variables xv , . . . , xv . Remove xv constraint Cvj Cv,
.
0
claim solution solution satisfies conditions
statement lemma. First, construction constraints 0 , value v
solution, set constraints containing variable assigned value v mutually disjoint
variable xiv (i < `) assigned value v removed constraint
`
0
subsequent variable xi+1
v , . . . , xv contained in. Moreover, constraint Ci obtained
0
Ci (possibly) removing variables Ci , Ci Ci , = 1, . . . , c. Finally,
226

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

variable xiv assigned value v removed constraint Cj0 , removal
affect number different values assigned variables Cj0 S; know
sure subsequent variable xpv , p {i + 1, . . . , `}, assigned value v
remain Cj0 , namely variable xpv maximum index p appears Cj0 .
Conversely, Ci0 subset Ci , = 1, . . . , c, easy see solution
0 also solution I.
Theorem 6. CSP solved time ((2(cons + 1) + 1)n ).
Proof. Let instance CSP constraints C1 , . . . , Cc integer c > 0,
variables x1 , . . . , xn . Let ni , = 1, . . . , c, nonnegative integer associated constraint Ci .
first enumerate subset variables {x1 , . . . , xn } subset essential variables
solution sought. Fix enumerated subset X, remove variables I,
update instance accordingly (i.e., update constraints); without loss generality, still
refer resulting instance I.
Lemma 2, solution instance 0 set
variables I, whose constraints C10 , . . . , Cc0 , that: (1) variables Ci0 form subset
Ci , = 1, . . . , c, (2) numbers n1 , . . . , nc 0 , (3)
solution 0 satisfying every value v, two distinct variables xi , xj
assigned value v solution 0 , set constraints xi belongs 0 disjoint
xj belongs 0 .
find instance 0 , try every possible partitioning variables X c
constraints determine new constraints C10 , . . . , Cc0 0 . partitioning
Ci0 Ci least ni variables Ci0 , = 1, . . . , c, form instance CSP6= set
variables X set constraints C10 , . . . , Cc0 , invoke algorithm CSP6= described
Proposition 13 instance; algorithm returns solution return solution
solution I. enumerated subset X enumerated partitioning algorithm
CSP6= rejects, reject instance I.
easy see correctness algorithm. Clearly, solution CSP6=
instance solution 0 , hence I. constraint contains
least ni variables, must receive ni distinct values solution CSP6= instance, hence
satisfying constraint Ci satisfying I. hand, solution,
enumerated partitioning variables X correspond constraints 0 .
solution 0 satisfies properties (1)-(3) Lemma 2, two variables
constraint 0 receive value v solution (by property (3)). Therefore,
solution also solution constructed instance CSP6= . shows correctness
algorithm.
running time algorithm time taken enumerate subsets variables,
subset X, time enumerate partitions X c constraints, finally
partition time taken invoke P
CSP6= algorithm
resulting instance. number

subsets variables {x1 , . . . , xn } ni=0 ni . subset cardinality i,
2ci many ways partitioning c constraints. Finally, instance variables,
CSP6= algorithm takes (2i ) time. Putting everything together, overall running time
algorithm polynomial factor multiplied by:
227

fiDE

n
X
n
i=0



ci

H AAN , K ANJ , & ZEIDER



2 2 =

n
X
n
i=0



2(c+1)i = (2(c+1) + 1)n .

Therefore, running time algorithm ((2(cons + 1) + 1)n ) claimed.
Corollary 2. restriction CSP instances cons = O(1) solvable (2O(n) )
time.
Corollary 3. restriction CSP instances cons = o(log dom) solvable
subexponential time.
Proof. result follows Theorem 6 noticing cons = o(log dom) 2cons =
domo(1) .
Proposition 17. Let d(n) = (1) proper complexity function. restriction CSP
instances cons = O(1) dom d(n) solvable subexponential time, unless
ETH fails, restriction CSP instances cons = (n) (even dom = O(1))
solvable subexponential time.
Proof. positive result follows Corollary 3. hardness result follows hardness
result CSP6= Proposition 14 (CSP6= special case CSP ).
NP-hardness reduction CSP= single constraint (and linear domain size), given
Bessiere et al. (2007), also works CSP , actually serf-reduction 3-CNF-S AT.
implies that, unless ETH fails, neither CSP= CSP , restricted instances single
constraint dom = O(n), solvable subexponential time. show next result
restrictions CSP CSP= instances constant domain size linear number
constraints:
Theorem 7. restrictions CSP CSP= instances dom = O(1) cons =
(n) solvable subexponential time, unless ETH fails.
Proof. give serf-reduction 3-3-S CSP ; result follow Lemma 1.
serf-reduction also works case CSP= . Take instance 3-3-S n variables.
construct polynomial time instance CSP , cons = O(n) dom = O(1)
yes-instance 3-3-S AT. proceed two steps: firstly, modify
well-known polynomial-time reduction 3-S V ERTEX C (Garey & Johnson, 1979)
reduction 3-3-S CSP , resulting instance cons = O(n) dom = O(n);
secondly, transform instance CSP equivalent instance CSP cons = O(n)
dom = O(1).
start first step. Let consist clauses c1 , . . . , cm , ci = l1i l2i l3i
1 m. well-known reduction V ERTEX C produces graph G =
(V, E), containing vertices vx , vx variable x occurring , vertex vji literal
occurrence, 1 1 j 3. variables vx vx adjacent,
variable x, vertices v1i , v2i , v3i form triangle, 1 m. Moreover,
edge vji vl , l = lji . satisfiable G vertex
cover consisting n + 2m vertices. specifically, satisfiable G
228

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

vertex cover containing exactly one vertex vx , vx variable x exactly two vertices
v1i , v2i , v3i 1 m. construct instance CSP follows.
edge e = {v1 , v2 } E, introduce variable ue domain {v1 , v2 }. Then, clause ci ,
define set Eci consist edges v1i , v2i , v3i , vji vli vli
j
j
vli , 1 j 3. Then, add constraint ensuring variables ue
j
nine e Eci take 5 different values. assignments variables ue satisfy
constraints exactly correspond vertex covers G containing exactly one vertex vx , vx
variable x exactly two vertices v1i , v2i , v3i 1 m. particular
vertex covers, turn, correspond exactly truth assignments (which set one x, x true,
variable x) satisfying . construction constraint illustrated Figure 3.
v1i










v3i



v1j

v2i












v2j



v3j






vx1 vx1 vx4 vx4 vx5 vx5 vx6 vx6 vx7 vx7
Figure 3: CSP constraints corresponding example clauses ci = (x1 x4 x5 ) cj =
(x5 x6 x7 ). Variables denoted , values . constraints indicated
dashed lines. nine variables constraint must assigned 5 different
values. double lines indicate assignment variables satisfying constraint
corresponds truth assignment {x1 7 >, x4 7 , x5 7 >, x6 7 >, x7 7 }.
second step, transform instance CSP way dom = O(1). order
so, use following observation. Whenever two vertices v1 , v2 V property
constraint containing variable ue1 edge e1 incident v1
variable ue2 edge e2 incident v2 , safely identify domain values v1
v2 instance CSP . Consequently, identify many domain values v11 , . . . , v1m
single value, similarly identify domain values v21 , . . . , v2m v31 , . . . , v3m . Next,
reduce dom even more, identify number domain values vx (and similarly
identify complementary values vx other). Consider primal graph , i.e.,
graph Gp containing vertices variables two vertices x, x0 adjacent
x x0 occur together clause (positively negatively). Since variable occurs 3
times , know maximum degree Gp bounded 8. Then, Brooks
Theorem (Brooks, 1941), know exists proper coloring Gp 9 colors,
coloring computed linear time. Take proper coloring c Gp . Now,
color b used coloring c, let Xb Var() set variables x c(x) = b.
Then, since c proper coloring primal graph Gp , know color b two
variables x, x0 Xb occur together clause . Therefore, color 1 b 3
safely identify domain values vx x Xb instance CSP ,
similarly safely identify domain values vx x Xb other. results
equivalent instance CSP cons = O(n) dom = O(1).
229

fiDE

H AAN , K ANJ , & ZEIDER

next consider subexponential-time complexity CSP= , CSP , CSP respect
primal treewidth. following tight result:
Proposition 18. restriction CSP= , CSP , CSP instances tw = o(n)
solvable subexponential time, unless ETH fails, restriction CSP= , CSP ,
CSP instances tw = (n) solvable subexponential time.
Proof. proof proposition CSP= , CSP , CSP exactly
proof Proposition 15.
Finally, following hardness result CSP= CSP respect tw follows
Proposition 16 since CSP6= special case CSP= CSP :
Proposition 19. Unless ETH fails, restriction CSP= (resp. CSP ) instances
tw = (n) solvable subexponential time.

5. Conclusion
provided first analysis subexponential-time complexity CSP extensionally
represented constraints CSP global constraints, latter focusing instances
composed fundamental global constraints AllDifferent, AtLeastNValue, AtMostNValue,
cTable, respectively. results show detailed complexity landscape problems
various natural structural restrictions. cases, able obtain tight bounds exactly
determine borderline classes instances solved subexponential time,
existence subexponential-time algorithms unlikely. several
ways extending current work considering global constraints, combination
different global constraints, structural restrictions primal incidence graphs.

References
Alber, J., Fernau, H., & Niedermeier, R. (2004). Parameterized complexity: exponential speed-up
planar graph problems. Algorithmica, 52(1), 2656.
Amir, E. (2010). Approximation algorithms treewidth. Algorithmica, 56(4), 448479.
Bckstrm, C., & Jonsson, P. (2011). pspace-complete planning problems equal
equal others. Borrajo, D., Likhachev, M., & Lpez, C. L. (Eds.), Proceedings
Fourth Annual Symposium Combinatorial Search, SOCS 2011, Castell de Cardona,
Barcelona, Spain, July 15.16, 2011. AAAI Press.
Beigel, R., & Eppstein, D. (2005). 3-coloring time O(1.3289n ). J. Algorithms, 54(2), 168204.
Beldiceanu, N., Carlsson, M., & Rampon, J.-X. (2006). Global constraint catalog. Tech.
rep. T2005:08, SICS, SE-16 429 Kista, Sweden. On-line version http://www.emn.fr/xinfo/sdemasse/gccat/.
Benhamou, B., Paris, L., & Siegel, P. (2012). Dealing satisfiability n-ary CSPs logical
framework. Journal Automated Reasoning, 48(3), 391417.
Bennaceur, H. (2004). comparison SAT CSP techniques. Constraints, 9(2), 123138.
230

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

Bessiere, C., Hebrard, E., Hnich, B., Kiziltan, Z., & Walsh, T. (2006). Filtering algorithms
NValue constraint. Constraints, 11(4), 271293.
Bessiere, C., Hebrard, E., Hnich, B., & Walsh, T. (2007). complexity reasoning global
constraints. Constraints, 12(2), 239259.
Bjrklund, A., Husfeldt, T., & Koivisto, M. (2009). Set partitioning via inclusion-exclusion. SIAM J.
Comput., 39(2), 546563.
Brooks, R. L. (1941). colouring nodes network. Mathematical Proceedings
Cambridge Philosophical Society, 37, 194197.
Calabro, C., Impagliazzo, R., & Paturi, R. (2006). duality clause width clause density
SAT. 21st Annual IEEE Conference Computational Complexity (CCC 2006), 16-20
July 2006, Prague, Czech Republic, pp. 252260. IEEE Computer Society.
Chen, H., & Grohe, M. (2010). Constraint satisfaction succinctly specified relations. J.
Computer System Sciences, 76(8), 847860.
Chen, J., Kanj, I., Perkovic, L., Sedgwick, E., & Xia, G. (2007). Genus characterizes complexity
certain graph problems: tight results. Journal Computer System Sciences,
73(6), 892907.
Chen, J., Chor, B., Fellows, M., Huang, X., Juedes, D., Kanj, I. A., & Xia, G. (2005). Tight lower
bounds certain parameterized NP-hard problems. Information Computation, 201(2),
216231.
Chen, J., Huang, X., Kanj, I. A., & Xia, G. (2006). Strong computational lower bounds via parameterized complexity. J. Computer System Sciences, 72(8), 13461367.
Chen, J., Kanj, I. A., & Xia, G. (2009). parameterized exponential time complexity. Theoretical
Computer Science, 410(27-29), 26412648.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction Algorithms (Third
edition). MIT Press, Cambridge, MA.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
Demaine, E., Fomin, F., Hajiaghayi, M., & Thilikos, D. (2005). Subexponential parameterized
algorithms bounded-genus graphs H-minor-free graphs. J. ACM, 52, 866893.
Dimopoulos, Y., & Stergiou, K. (2006). Propagation CSP SAT. Benhamou, F. (Ed.),
Principles Practice Constraint Programming - CP 2006, 12th International Conference,
CP 2006, Nantes, France, September 25-29, 2006, Proceedings, Vol. 4204 Lecture Notes
Computer Science, pp. 137151. Springer Verlag.
Feder, T., & Motwani, R. (2002). Worst-case time bounds coloring satisfiability problems. J.
Algorithms, 45(2), 192201.
Fellows, M. R., Fomin, F. V., Lokshtanov, D., Rosamond, F., Saurabh, S., Szeider, S., & Thomassen, C.
(2011a). complexity colorful problems parameterized treewidth. Information
Computation, 209(2), 143153.
Fellows, M. R., Friedrich, T., Hermelin, D., Narodytska, N., & Rosamond, F. A. (2011b). Constraint
satisfaction problems: Convexity makes alldifferent constraints tractable. Walsh, T. (Ed.),
IJCAI 2011, Proceedings 22nd International Joint Conference Artificial Intelligence,
Barcelona, Catalonia, Spain, July 16-22, 2011, pp. 522527. IJCAI/AAAI.
231

fiDE

H AAN , K ANJ , & ZEIDER

Flum, J., & Grohe, M. (2006). Parameterized Complexity Theory, Vol. XIV Texts Theoretical
Computer Science. EATCS Series. Springer Verlag, Berlin.
Freuder, E. C. (1982). sufficient condition backtrack-bounded search. J. ACM, 29(1),
2432.
Freuder, E. C. (1990). Complexity k-tree structured constraint satisfaction problems. Shrobe,
H. E., Dietterich, T. G., & Swartout, W. R. (Eds.), Proceedings 8th National Conference
Artificial Intelligence. Boston, Massachusetts, July 29 - August 3, 1990, 2 Volumes, pp. 49.
AAAI Press / MIT Press.
Garey, M. R., & Johnson, D. R. (1979). Computers Intractability. W. H. Freeman Company,
New York, San Francisco.
Gaspers, S., & Szeider, S. (2014). Guarantees limits preprocessing constraint satisfaction
reasoning. Artificial Intelligence, 216, 119.
Ge, R. (2013). Provable Algorithms Machine Learning Problems. Ph.D. thesis, Princeton
University.
Gottlob, G., Leone, N., & Scarcello, F. (2002). Hypertree decompositions tractable queries. J.
Computer System Sciences, 64(3), 579627.
Grandoni, F., & Italiano, G. F. (2006). Algorithms constraint programming. Benhamou,
F. (Ed.), Principles Practice Constraint Programming - CP 2006, 12th International
Conference, CP 2006, Nantes, France, September 25-29, 2006, Proceedings, Vol. 4204
Lecture Notes Computer Science, pp. 214. Springer Verlag.
Grohe, M. (2006). structure tractable constraint satisfaction problems. Kralovic, R., &
Urzyczyn, P. (Eds.), Mathematical Foundations Computer Science 2006, 31st International
Symposium, MFCS 2006, Star Lesn, Slovakia, August 28-September 1, 2006, Proceedings,
Vol. 4162 Lecture Notes Computer Science, pp. 5872. Springer Verlag.
de Haan, R., Kanj, I., & Szeider, S. (2014). Subexponential time complexity CSP global
constraints. Proceedings CP 2014, 20th International Conference Principles
Practice Constraint Programming, Lyon, France, September 8-12, 2014. Springer Verlag.
Hnich, B., Kiziltan, Z., & Walsh, T. (2004). Combining symmetry breaking constraints:
Lexicographic ordering sums. AI&M 1-2004, Eighth International Symposium
Artificial Intelligence Mathematics, January 4-6, 2004, Fort Lauderdale, Florida, USA.
van Hoeve, W.-J., & Katriel, I. (2006). Global constraints. Rossi, F., van Beek, P., & Walsh, T.
(Eds.), Handbook Constraint Programming, chap. 6. Elsevier.
Impagliazzo, R., & Paturi, R. (2001). complexity k-SAT. J. Computer System
Sciences, 62(2), 367375.
Impagliazzo, R., Paturi, R., & Zane, F. (2001). problems strongly exponential complexity?. J. Computer System Sciences, 63(4), 512530.
Jeavons, P., & Petke, J. (2012). Local consistency SAT-solvers. J. Artif. Intell. Res., 43, 329351.
Jonsson, P., Lagerkvist, V., & Nordh, G. (2013). Blowing holes various aspects computational
problems, applications constraint satisfaction. Schulte, C. (Ed.), Principles
Practice Constraint Programming - 19th International Conference, CP 2013, Uppsala,
232

fiO N UBEXPONENTIAL -T IME C OMPLEXITY CSP

Sweden, September 16-20, 2013. Proceedings, Vol. 8124 Lecture Notes Computer Science,
pp. 398414. Springer Verlag.
Kanj, I., & Szeider, S. (2013). subexponential time complexity CSP. Proceedings
Twenty-Seventh AAAI Conference Artificial Intelligence. AAAI Press.
Katsirelos, G., & Walsh, T. (2007). compression algorithm large arity extensional constraints.
Bessiere, C. (Ed.), Principles Practice Constraint Programming - CP 2007, 13th
International Conference, CP 2007, Providence, RI, USA, September 23-27, 2007, Proceedings,
Vol. 4741 Lecture Notes Computer Science, pp. 379393. Springer Verlag.
Kolaitis, P. G., & Vardi, M. Y. (2000). Conjunctive-query containment constraint satisfaction. J.
Computer System Sciences, 61(2), 302332. Special issue Seventeenth ACM
SIGACT-SIGMOD-SIGART Symposium Principles Database Systems (Seattle, WA,
1998).
Kutz, M., Elbassioni, K., Katriel, I., & Mahajan, M. (2008). Simultaneous matchings: hardness
approximation. J. Computer System Sciences, 74(5), 884897.
Kwisthout, J., Bodlaender, H. L., & van der Gaag, L. C. (2010). necessity bounded treewidth
efficient inference Bayesian networks. Coelho, H., Studer, R., & Wooldridge, M. (Eds.),
ECAI 2010 - 19th European Conference Artificial Intelligence, Lisbon, Portugal, August
16-20, 2010, Proceedings, Vol. 215 Frontiers Artificial Intelligence Applications, pp.
237242. IOS Press.
Lokshtanov, D., Marx, D., & Saurabh, S. (2011). Lower bounds based exponential time
hypothesis. Bulletin European Association Theoretical Computer Science, 105,
4172.
Marx, D. (2010). beat treewidth?. Theory Computing, 6, 85112.
Marx, D. (2013). Tractable hypergraph properties constraint satisfaction conjunctive queries.
J. ACM, 60(6), Art. 42, 51.
Moser, R. A., & Scheder, D. (2011). full derandomization Schnings k-SAT algorithm.
STOC11Proceedings 43rd ACM Symposium Theory Computing, pp. 245251.
ACM, New York.
Pachet, F., & Roy, P. (1999). Automatic generation music programs. Jaffar, J. (Ed.), Principles
Practice Constraint Programming - CP99, 5th International Conference, Alexandria,
Virginia, USA, October 11-14, 1999, Proceedings, Vol. 1713 Lecture Notes Computer
Science, pp. 331345. Springer Verlag.
Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.
Papadimitriou, C. H., & Yannakakis, M. (1991). Optimization, approximation, complexity
classes. J. Computer System Sciences, 43(3), 425440.
Papadimitriou, C. H., & Yannakakis, M. (1999). complexity database queries. J.
Computer System Sciences, 58(3), 407427.
Razgon, I. (2006). Complexity analysis heuristic CSP search algorithms. Hnich, B., Carlsson,
M., Fages, F., & Rossi, F. (Eds.), Recent Advances Constraints, Joint ERCIM/CoLogNET
International Workshop Constraint Solving Constraint Logic Programming, CSCLP
233

fiDE

H AAN , K ANJ , & ZEIDER

2005, Uppsala, Sweden, June 20-22, 2005, Revised Selected Invited Papers, Vol. 3978
Lecture Notes Computer Science, pp. 8899. Springer Verlag.
Rgin, J.-C. (1994). filtering algorithm constraints difference CSPs. Hayes-Roth, B.,
& Korf, R. E. (Eds.), Proceedings 12th National Conference Artificial Intelligence,
Seattle, WA, USA, July 31 - August 4, 1994, Volume 1, pp. 362367. AAAI Press / MIT
Press.
Rgin, J.-C. (1995). Dveloppement doutils algorithmiques pour lIntelligence Artificielle. Ph.D.
thesis, Montpellier II. French.
Rgin, J.-C. (2011). Global constraints: survey. van Hentenryck, P., & Milano, M. (Eds.),
Hybrid Optimization: Ten Years CPAIOR, Vol. 45 Optimization Applications,
chap. 3, pp. 63134. Springer Verlag.
Rgin, J.-C., & Rueher, M. (2000). global constraint combining sum constraint difference
constraints. Dechter, R. (Ed.), Principles Practice Constraint Programming - CP
2000, 6th International Conference, Singapore, September 18-21, 2000, Proceedings, Vol.
1894 Lecture Notes Computer Science, pp. 384395. Springer Verlag.
Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006). Handbook Constraint Programming. Elsevier.
Samer, M., & Szeider, S. (2010). Constraint satisfaction bounded treewidth revisited. J.
Computer System Sciences, 76(2), 103114.
Schning, U. (1999). probabilistic algorithm k-SAT constraint satisfaction problems.
40th Annual Symposium Foundations Computer Science (New York, 1999), pp. 410414.
IEEE Computer Soc., Los Alamitos, CA.
Schuler, R. (2005). algorithm satisfiability problem formulas conjunctive normal
form. J. Algorithms, 54(1), 4044.
Traxler, P. (2008). time complexity constraint satisfaction. Grohe, M., & Niedermeier, R.
(Eds.), Parameterized Exact Computation, Third International Workshop, IWPEC 2008,
Victoria, Canada, May 14-16, 2008. Proceedings, Vol. 5018 Lecture Notes Computer
Science, pp. 190201. Springer Verlag.

234

fiJournal Artificial Intelligence Research 52 (2015) 97-169

Submitted 05/14; published 01/15

Deterministic Oversubscription Planning Heuristic Search:
Abstractions Reformulations
Carmel Domshlak
Vitaly Mirkis

dcarmel@ie.technion.ac.il
mirkis80@gmail.com

Faculty Industrial Engineering & Management,
Technion - Israel Institute Technology,
Haifa, Israel

Abstract
classical planning objective achieve one equally attractive goal
states low total action cost possible, objective deterministic oversubscription
planning (OSP) achieve valuable possible subset goals within fixed
allowance total action cost. Although numerous applications various fields share
latter objective, substantial algorithmic advances made deterministic
OSP. Tracing key sources progress classical planning, identify severe lack
effective domain-independent approximations OSP.
focus optimal planning, goal bridge gap. Two classes
approximation techniques found especially useful context optimal
classical planning: based state-space abstractions based logical landmarks goal reachability. question study whether similar-in-spirit,
yet possibly mathematically different, approximation techniques developed OSP.
context abstractions, define notion additive abstractions OSP, study
complexity deriving effective abstractions rich space hypotheses, reveal
substantial, empirically relevant islands tractability. context landmarks,
show standard goal-reachability landmarks certain classical planning tasks
compiled OSP task interest, resulting equivalent OSP task lower
cost allowance, thus smaller search space. empirical evaluation confirms
effectiveness proposed techniques, opens wide gate developments
oversubscription planning.

1. Introduction
tools automated action planning allow autonomous systems selecting course
action get things done. Deterministic planning probably basic, thus
fundamental, setting automated action planning (Russell & Norvig, 2009).
viewed problem finding trajectories interest large-scale yet concisely
represented state-transition systems. Computational approaches deterministic planning
vary around way trajectories interest defined.
basic structure acting situations underconstrained overconstrained
resources respectively captured days called classical deterministic
planning (Fikes & Nilsson, 1971), Smith (2004) termed oversubscription
deterministic planning (OSP). classical planning, task find cost-effective
trajectory possible goal-satisfying state. oversubscription planning, task
find goal-effective (or valuable) state possible via cost-satisfying trajectory.
c
2015
AI Access Foundation. rights reserved.

fiDomshlak & Mirkis

optimal classical planning optimal OSP, tasks constrained finding
cost-effective trajectories goal-effective states, respectively. Classical
planning OSP viewed foundational variants deterministic planning,
many variants, net-benefit planning cost-bounded planning, defined
terms mixing relaxing two.1
OSP extensively advocated years, theory practice
classical planning studied advanced much intensively. remarkable
success continuing progress heuristic-search solvers classical planning one notable example. Primary enablers success advances domain-independent
approximations, heuristics, cost needed achieve goal state given state.
thus possible similarly rich palette effective heuristic functions
OSP would advance state art problem.
Two classes approximation techniques found especially useful context optimal classical planning: based state-space abstractions (Edelkamp, 2001;
Haslum, Botea, Helmert, Bonet, & Koenig, 2007; Helmert, Haslum, Hoffmann, & Nissim,
2014; Katz & Domshlak, 2010a) based logical landmarks goal reachability (Karpas & Domshlak, 2009; Helmert & Domshlak, 2009; Domshlak, Katz, & Lefler,
2012; Bonet & Helmert, 2010; Pommerening & Helmert, 2013). Considering OSP heuristic search, question whether similar-in-spirit, yet possibly mathematically
different, approximation techniques developed heuristic-search OSP. precisely question study here.
Starting basic question state-space abstractions OSP actually are, show notion abstraction differs substantially classical planning OSP. Hence, first define (additive) abstractions abstraction
heuristics OSP. investigate computational complexity deriving
effective abstraction heuristics scope homomorphic abstraction skeletons,
paired cost, value, budget partitions. Along revealing significant
islands tractability, study exposes interesting interplay knapsackstyle problems combinatorial optimization, continuous convex optimization,
certain principles borrowed explicit abstractions classical planning.
introduce study -landmarks, logical properties OSP plans achieve
valuable states. show -landmarks correspond regular goal-reachability
landmarks certain classical planning tasks straightforwardly derived
OSP tasks interest. show -landmarks compiled
back OSP task interest, resulting equivalent OSP task,
stricter cost satisfaction constraint, thus smaller effective search space.
Finally, show landmark-based task enrichment combined
mutually stratifying way best-first branch-and-bound search used OSP
planning, resulting incremental procedure interleaves search landmark
discovery. entire framework independent OSP planner specifics,
particular, heuristic functions employs.
1. connections differences popular variants deterministic planning discussed
Section 2.

98

fiOn Oversubscription Planning Heuristic Search

empirical evaluation large set OSP tasks confirms effectiveness proposed techniques. Moreover, best knowledge, implementation constitutes
first domain-independent solver optimal OSP, hope advances
important computational problem follow.
work revision extension formulations results presented
authors ICAPS-2013 ECAI-2014 (Mirkis & Domshlak, 2013, 2014). paper
structured follows. Section 2 formulate general model deterministic planning,
define several variants deterministic planning terms model, and, particular,
show oversubscription planning differs conceptually classical planning,
also popular variants deterministic planning net-benefit planning
cost-bounded planning. also specify simple model representation language
OSP, well provide essential background heuristic search, and, particular,
OSP heuristic search. Sections 3 4 devoted, respectively, abstractions
abstraction approximations OSP. Section 5 devoted exploiting reachability
landmarks OSP tasks. Section 6 conclude discuss promising directions
future work. sake readability, proofs relegated Appendix A,
details empirical results relegated Appendix B.

2. Background
mentioned introduction, specific variants deterministic planning differ
way interest preference trajectories defined. instance, classical
planning (Fikes & Nilsson, 1971), trajectory interest connects designated initial
state one designated goal states, preference towards trajectories
lower total cost transitions along them. Among other, non-classical variants
deterministic planning
oversubscription planning (Smith, 2004), topic interest here;
net-benefit planning (van den Briel, Sanchez, Do, & Kambhampati, 2004; Sanchez
& Kambhampati, 2005; Baier, Bacchus, & McIlraith, 2009; Bonet & Geffner, 2008;
Benton, Do, & Kambhampati, 2009; Coles & Coles, 2011; Keyder & Geffner, 2009);
cost-bounded (also known resource-constrained) planning (Haslum & Geffner, 2001;
Hoffmann, Gomes, Selman, & Kautz, 2007; Gerevini, Saetti, & Serina, 2008; Thayer &
Ruml, 2011; Thayer, Stern, Felner, & Ruml, 2012; Haslum, 2013; Nakhost, Hoffmann,
& Muller, 2012);
planning preferences temporal properties trajectories (Baier et al.,
2009; Gerevini, Haslum, Long, Saetti, & Dimopoulos, 2009; Benton, Coles, & Coles,
2012).
Interestingly, working paper, learned quite different
variants deterministic planning often collectively referred oversubscription
planning. result, difference terms expressiveness necessarily clear, thus, relationship already
done collective sense oversubscription planning always apparent.
issue address first.
99

fiDomshlak & Mirkis

2.1 Models
Adopting extending notation Geffner Bonet (2013), view many
variants deterministic planning, including classical planning, well many popular
non-classical variants, special cases state model
= hS, s0 , u, O, , c, Qi

(1)

with:
finite set states S,
initial state s0 S,
state value function u : 7 R0+ {},
operators O(s) applicable state S,
deterministic state transition function (s, o) s0 = (s, o) stands
state resulting applying O(s) s,
operator cost function c : R0+ ,
quality measure Q : P 7 R {}, P (infinite) set trajectories
s0 along operators O. trajectory P sequence operators ho1 , . . . ,
o1 O(s0 ) and, inductively, oi O(((. . . (s0 , o1 ) . . . , oi2 ), oi1 )).
model, trajectory P solution, preference towards solutions
higher quality.
P follows, sJK stands end-state trajectory applied
state s, c() = c(o) additive cost . Likewise, graphical skeleton
GM = hS, , Oi model refers edge-annotated, unweighted digraph induced
nodes GM states S, edge labels operators O,
contains edge s0 labeled iff O(s) s0 = (s, o).
First, consider quality measure
Q+ () = u(sJK) c().

(2)

measure assumes state values operator costs comparable, thus represents tradeoff value end-state cost trajectory. Consider
fragment state model (1), instances quality measure Q+ ,
instance, value function
(
,
Sgoal
u(s) =
(3)
, otherwise
partitions state space Sgoal S, u takes finite value 0,
rest states, u takes value . Finding optimal solution
instance fragment corresponds finding shortest path s0 single node
edge-weighted digraph G, obtained GM (i) annotating edges
latter costs c, (ii) adding dummy node zero-cost edges
100

fiOn Oversubscription Planning Heuristic Search

constraint

preference

Net Benefit

Oversubscription

constraint

End-state value

Action cost
preference

Classical

Cost-bounded

Figure 1: Schematic classification four deterministic planning models along strictness
approach cost operator sequences value
operator sequence end-states. White blocks planning models
solved single-source single-target shortest path problems.

goal nodes Sgoal . specified non-canonical way, fragment
easily verified correspond model classical planning, Sgoal classical
planning goal states.
Staying quality measure Q+ removing requirement u comply
Eq. 3, obtain fragment generalizes classical planning, constitutes
basic model called net-benefit planning (Sanchez & Kambhampati, 2005). Importantly, instance fragment reduced finding shortest path
single node s0 single node edge-weighted digraph G, obtained GM
(i) annotating edges GM costs c, (ii) adding dummy node andPedges
nodes , (ii) setting cost new edge (s, ) s0 S\{s} u(s0 ).
reduction works net-value maximization end state equivalent
minimization net-loss giving possible end states.
basic idea underlies Keyder Geffners (2009) scheme compiling certain standard representation formalisms net-benefit planning standard classical planning
formalism.2
Consider alternative quality measure
(
u(sJK), c() b
Q () =
,
,
otherwise
b

(4)

2. worth noting wost-case complexity equivalence classical planning net-benefit
planning shown prior work Keyder Geffner (2009) van den Briel et al. (2004).
However, equivalence prescriptive enough suggest practically effective compilations
compactly represented net-benefit planning tasks classical planning tasks.

101

fiDomshlak & Mirkis

b R0+ predefined bound cost trajectories. fragment
basic model, instances characterized quality measure Qb
value functions Eq. 3, constitutes model called costbounded planning (Thayer & Ruml, 2011). well, finding optimal solution
problem instance corresponds finding shortest path s0 edge-weighted
digraph G, derived GM identically case classical planning.3 This,
particular, explains natural heuristic-search methods cost-bounded
planning exploit heuristics developed classical planning (Haslum, 2013).
arrive fourth fragment basic model. Staying quality measure Qb removing requirement u comply Eq. 3, obtain fragment
generalizes cost-bounded planning, constitutes model oversubscription planning (Smith, 2004). illustrated Figure 1, hard constraint classical planning
translates soft preference OSP, hard constraint OSP translates soft preference classical planning. However, contrast cost-optimal, net-benefit, classical
planning, fragment appear reducible single-source single-target
shortest path problem. terms digraph G obtained GM annotating
edges costs c, finding optimal solution instance oversubscription planning
requires (i) finding shortest paths s0 states u(s) > 0, (ii) filtering
states reachable s0 within cost allowance b, (iii)
selecting remaining states state maximizes u.
contrast oversubscription planning three popular variants
deterministic planning discussed least two important implications. First,
single shortest path searched using best-first forward search procedures
, searching shortest paths numerous targets simultaneously requires different,
exhaustive, forward search framework branch-and-bound. Second, net-benefit
cost-bounded planning clearly potential (directly indirectly) reuse rich
toolbox heuristic functions developed years classical planning. contrast,
due differences underlying computational model, necessarily
true oversubscription planning, examining prospects heuristic functions
OSP precisely focus work here.
2.2 Notation
k N+ , [k] denote set {1, 2, . . . , k}. indicator function subset
set X function 1A : X {0, 1} defined 1A (x) = 1 x 1A (x) = 0 x 6 A.
Following Nebel (2000), talk size mathematically well-defined object
x, symbolically ||x||, mean size (reasonable) encoding x. assignment
variable v value denoted hv/di; often refer single variable assignments
propositions.

3. Strictly speaking, shortest path s0 found, still checked
cost bound b. test, however, local , problem solving finishes independently tests
outcome.

102

fiOn Oversubscription Planning Heuristic Search

2.3 Model Representation
Departing general model oversubscription planning, follows restrict attention instances model compactly representable language
close sas+ language classical planning (Backstrom & Klein, 1991; Backstrom &
Nebel, 1995). language, deterministic oversubscription planning (OSP) task
given sextuple
= hV, s0 , u; O, c, bi,

(5)


(1) V = {v1 , . . . , vn } finite set finite-domain state variables, complete
assignment V representing state, = dom(v1 ) dom(vn ) state
space task;
(2) s0 designated initial state;
(3) u efficiently computable state value function u : R0+ ;
(4) finite set operators, operator represented pair
hpre(o), eff(o)i partial assignments V , called preconditions effects o, respectively;
(5) c : R0+ operator cost function;
(6) b R0+ cost budget allowed task.
consider semantics task description terms
basic model.
ff
OSP task = hV, s0 , u; O, c, bi said induce model = S, s0 , u, O, , c, Qb ,
Qb quality measure (4) instantiated budget b, transition
function specified follows. partial assignment p V , let V(p) V denote
subset variables instantiated p, and, v V(p), p[v] denote value provided
p variable v. Similarly classical planning semantics sas+ , operator
applicable state iff s[v] = pre(o)[v] v V(pre(o)). Applying changes
value v V(eff(o)) eff(o)[v], resulting state denoted sJoK.
notation defined applicable s. Denoting empty sequence operators ,
applying sequence operators ho1 , . . . , om state defined inductively sJK :=
sJo1 , . . . , oj K := sJo1 , . . . , oj1 KJoj K. operator sequence called s-plan
applicable state Qb () 6= , is, c() b.

auxiliary notation used later on: OSP task = hV, s0 , u; O, c, bi,
= vV dom(v) denote union (uniquely labeled) state-variable domains.
state proposition hv/di D, hv/di used shortcut notation
s[v] = d.
example simple OSP task Figure 2 used illustrate model representation. example, truck initially location A, drive (only) location
location B location B location C. Two packages, x y, initially
location B. package truck location, package
loaded onto truck, package truck, unloaded
103

fiDomshlak & Mirkis

x



B

C

(a)
oi
pre(oi )
eff(oi )

driveAB
i=1
{ht/Ai}
{ht/Bi}

driveBC
i=2
{ht/Bi}
{ht/Ci}

loadBx
i=3
{ht/Bi , hx/Bi}
{hx/T i}

loadBy
i=4
{ht/Bi , hy/Bi}
{hy/T i}

unloadCx
i=5
{ht/Ci , hx/T i}
{hx/Ci}

oi
pre(oi )
eff(oi )

unloadBx
i=6
{ht/Bi , hx/T i}
{hx/Bi}

unloadBy
i=7
{ht/Bi , hy/T i}
{hy/Bi}

unloadCy
i=8
{ht/Ci , hy/T i}
{hy/Ci}

loadCx
i=9
{ht/Ci , hx/Ci}
{hx/T i}

loadCy
= 10
{ht/Ci , hy/Ci}
{hy/T i}

(b)
u=1

CBB

o2
ABB

o1

BBB

BTB
o6
o3
o7
o4
BBT

CTB

o9
o5

CCB

BTT

o2

CTT

o2
o7
o4
o3
o6
o2

o10
o8
u=1

o10
o8

CBC

CTB

o9
o5

CCB

BTT

o2

CTT

CBT

u=1

o9
o5

CTC

u=1
CCT

o9 u=2
o5
CCC

o10
o8

(c)
u=1

CBB

o2
ABB

o1

BBB

BTB
o6
o3
o7
o4
BBT

o2
o7
o4
o3
o6
o2

o10
o8
u=1

CBT

o10
o8

u=1

o9
o5

CTC

u=1
CCT

o9 u=2
o5
CCC

o10
o8

CBC

(d)

Figure 2: simple running example OSP task, (a) illustrating story, (b) listing operators, (c)-(d) depicting graphical skeleton induced state
model; (c) shows region graphical skeleton GM structurally
reachable initial state ABB, grayed area (d) corresponds
sub-region cannot reached initial state budget
b = 4.

trucks current location. (drive, load, unload) operator task costs one unit
cost, cost budget set four units cost. Finally, value one (value unit)
earned package present location C.
OSP task described using three state variables V = {t, x, y},
dom(t) = {A, B, C} dom(x) = dom(y) = {A, B, C, }, corresponding possible
locations truck two packages, respectively.

operator set ffO = {o1 , . . . , o10 }
detailed Figure 2(b). state model = S, s0 , u, O, , c, Qb induced
104

fiOn Oversubscription Planning Heuristic Search

BFBB ( = hV, s0 , u; O, c, bi)
open := new max-heap ordered f (n) = h(shni, b g(n))
initialize best solution n := make-root-node(s0 )
open.insert(n )
closed:= ;
best-cost:= 0
open.empty()
n := open.pop-max()
f (n) u(shn i): break
u(shni) > u(shn i): update n := n
shni 6 closed g(n) < best-cost(shni):
closed:= closed {shni}
best-cost(shni) := g(n)
foreach O(shni):
n0 := make-node(shniJoK, n)
g(n0 ) > b f (n0 ) u(shn i): continue
open.insert(n0 )

return n
Figure 3: Best-first branch-and-bound (BFBB) search OSP
task, = dom(t)dom(x)dom(y), initial state s0 = ABB (with three letters names states capturing three components domain cross-product),
operator cost c(oi ) = 1 operators oi , cost budget b = 4, state values


1, {?AC, ?BC, ?CA, ?CB}
u(s) = 2, {?CC}


0, otherwise

.

graphical skeleton GM depicted Figures 2(c) 2(d): Figure 2(c) shows
region graphical skeleton GM structurally reachable initial state
ABB, grayed area Figure 2(d) corresponds sub-region cannot
reached initial state budget b = 4.
2.4 OSP Heuristic Search
two major ingredients heuristic-search planner search algorithm
heuristic function. classical planning, heuristic typically function h : R0+
{}, h(s) estimating cost h (s) optimal s-plans. heuristic h admissible
lower-bounding, is, h(s) h (s) states s. common heuristic search
algorithms optimal classical planning, , require admissible heuristics.
contrast, heuristic OSP function h : R0+ R0+ , h(s, b) estimating
value h (s, b) optimal s-plans cost budget b. heuristic h admissible
upper-bounding, is, h(s, b) h (s, b) states cost budgets b. well,
105

fiDomshlak & Mirkis

search algorithms optimal OSP, best-first branch-and-bound (BFBB),4 require
admissible heuristics pruning search branches without violating solution optimality.
Figure 3 depicts pseudo-code description BFBB OSP; shni denotes state
associated search node n, cost-so-far g(n) total cost action sequence
associated n. Unlike , order nodes selected OPEN
list affect optimality guarantees (though may, course, seriously affect
empirical efficiency search). Figure 3, ordering OPEN corresponds
decreasing order h(shni, b g(n)). duplicate detection reopening mechanisms
BFBB similar (Pearl, 1984). addition, BFBB maintains best
solution n found far uses prune generated nodes evaluated higher
u(shn i). Likewise, complying semantics OSP, generated nodes n costso-far g(n) higher problems budget b also immediately pruned.
OPEN list becomes empty node n selected list promises less lower
bound, BFBB returns (the plan associated with) best solution n . h admissible,
is, h-based pruning generated nodes sound, returned plan
guaranteed optimal.
Let us return heuristic functions. domain-independent planning
automatically derived description model language choice.
useful heuristic function must efficiently computable description
model, well relatively accurate estimates. Improving accuracy heuristic
function without substantially worsening time complexity computing translates
faster search plans.
classical planning, numerous approximation techniques, monotonic relaxation (Bonet & Geffner, 2001, 2001; Hoffmann & Nebel, 2001), critical trees (Haslum
& Geffner, 2000), network flow (van den Briel, Benton, Kambhampati, & Vossen, 2007;
Bonet, 2013), logical landmarks goal reachability (Richter, Helmert, & Westphal, 2008;
Karpas & Domshlak, 2009; Helmert & Domshlak, 2009; Bonet & Helmert, 2010),
abstractions (Edelkamp, 2001; Helmert, Haslum, & Hoffmann, 2007; Katz & Domshlak,
2010a), translated effective heuristic functions. Likewise, different heuristics
classical planning also combined point-wise maximizing and/or additive
ensembles (Edelkamp, 2001; Haslum, Bonet, & Geffner, 2005; Coles, Fox, Long, & Smith,
2008; Katz & Domshlak, 2010b; Helmert & Domshlak, 2009).
contrast, development heuristic functions OSP progressed beyond
initial ideas Smith (2004). principle, reduction Keyder Geffner (2009)
net-benefit classical planning used reduce OSP classical planning realvalued state variables (Koehler, 1998; Helmert, 2002; Fox & Long, 2003; Hoffmann, 2003;
Gerevini, Saetti, & Serina, 2003; Gerevini et al., 2008; Edelkamp, 2003; Dvorak & Bartak,
2010; Coles, Coles, Fox, & Long, 2013). far, however, progress heuristic-search classical
planning numeric state variables mostly achieved around direct extensions
delete relaxation heuristics via numeric relaxed planning graphs (Hoffmann, 2003;
Edelkamp, 2003; Gerevini et al., 2003, 2008). Unfortunately, heuristics preserve
information consumable resources budgeted operator cost oversubscription
4. BFBB also extensively used net-benefit planning (Benton, van den Briel, & Kambhampati, 2007;
Coles & Coles, 2011; Do, Benton, van den Briel, & Kambhampati, 2007), well variants
deterministic planning (Bonet & Geffner, 2008; Brafman & Chernyavsky, 2005).

106

fiOn Oversubscription Planning Heuristic Search

planning: negative action effects decrease values numeric variables
ignored, possibly special handling so-called cyclic resource transfer (Coles
et al., 2013).
first step overcoming lack effective heuristics OSP, next section
study abstractions OSP, definition properties, prospects
deriving admissible abstraction heuristics. Section 5 study prospects
adapting OSP toolbox logical landmarks goal reachability. date, abstractions
landmarks responsible state-of-the-art admissible heuristics classical
planning, thus special interest here.

3. Abstractions
term abstraction usually associated simplifying original model, factoring
details less crucial given context. Context determines details reduced, better preserved, abstraction created used (Cousot
& Cousot, 1992; Clarke, Grumberg, & Peled, 1999; Helmert et al., 2014; Domshlak, Hoffmann, & Sabharwal, 2009; Katz & Domshlak, 2010b). general terms, abstracting model
corresponds associating set (typically computationally attractive)
models M1 , . . . , Mk solutions models satisfy certain properties respect solutions . particular, deterministic planning heuristic search,
abstractions used derive heuristic estimates states model interest :
Given state abstraction M1 , . . . , Mk ,
(1) mapped abstract states s1 M1 , . . . , sk Mk ,
(2) k models abstraction solved respective initial states s1 , . . . , sk ,

(3) aggregation quality resulting k solutions used heuristic estimate
s.
Sometimes schematically sometimes precisely, process constructing abstractions state model = hS, s0 , u, O, , c, Qi seen two-step process

(1) selecting abstraction skeleton = {(G1 , 1 ), . . . , (Gk , k )}, pair
(Gi , ) comprises edge-labeled digraph Gi = hSi , Ti , Oi i, nodes Si , edges Ti ,
edge labels Oi , state mapping : Si ,
(2) extending set abstract models = {M1 , . . . , Mk }, that, [k],
Gi graphical skeleton GMi Mi .
qualify valid abstraction model , resulting set abstract models
satisfy certain conditions specific variant deterministic planning
consideration. instance, optimal solutions abstract models classical
planning required costly respective solutions original
models, constraint satisfied individual abstract models case maxaggregation (Pearl, 1984), k abstract models jointly, case additive abstractions (Yang, Culberson, Holte, Zahavi, & Felner, 2008; Katz & Domshlak, 2010b).
107

fiDomshlak & Mirkis

show, concept abstractions general, additive abstractions particular,
different OSP, and, better worse, many degrees freedom
respective concepts classical planning.
3.1 Abstractions OSP Problems
Given
abstraction


ff skeleton = {(G1 , 1 ), . . . , (Gk , k )} OSP state model =
S, s0 , u, O, , c, Qb , digraph Gi = hSi , Ti , Oi implicitly defines set OSP state
models consistent it. set given Ci Ui Bi , Ci set
functions operators Oi R0+ , Ui set functions states Si R0+ ,
Bi = R0+ . terms, point (c, u, b) Ci Ui Bi induces OSP model
consistent Gi , vice versa.
Connecting sets models digraphs AS, let
C = C1 Ck ,
U = U1 Uk ,
B = B1 Bk .
state , every point (c, u, b) C U B induces set models
n

(c,u,b)
(c,u,b)
M(c,u,b) = M1
, . . . , Mk
,
(c,u,b)

Mi



ff
= Si , (s0 ), u[i], Oi , , c[i], Qb[i] :

states Si operators Oi correspond nodes edge labels Gi ;
transition function (s, o) = s0 iff Ti contains arc s0 labeled
Oi ;
initial state (s0 ) determined initial state s0 state mapping ;

operator cost function, state value function, cost budget directly determined choice (c, u, b).
choices (c, u, b) C U B, induced sets models M(c,u,b)
used deriving admissible estimates state interest s0 , others cannot.
respective qualification defined below.
Definition
1 (Additive OSP
ff Abstraction)
Let = S, s0 , u, O, , c, Qb OSP model = {(G1 , 1 ), . . . , (G1 , k )}
abstraction skeleton . (c, u, b) C U B, M(c,u,b) (additive)
abstraction , denoted
M(c,u,b) AAS M,

def

h (s0 , b) hM(c,u,b) (s0 , b) =

X

hi (i (s0 ), b[i]),

i[k]

is, hM(c,u,b) (s0 , b) admissible estimate h (s0 , b).
108

fiOn Oversubscription Planning Heuristic Search

GM

= s1
o1 |||
||
||
/ s2
s0
o2

o3

/ s3

G1

G2


= 2;1
o1 zz
z
zz
zz

o1

o5
o4

/ s4

s1;0

o2

/ s1;2

(a)

o4

/ 1;4

s2;0

o2

o3

/ s2;3
o5



/ s2;4

(b)
Figure 4: Illustration running example

simple terms, set models forms additive OSP abstraction jointly models
underestimate value obtained initial state, within
5 example, let G
given cost budget.
4a graphical skeleton state
Figure ff


model = {s0 , . . . , s4 }, s0 , u, {o1 , . . . , o5 }, , c, Qb , c(oi ) = 1 operators oi ,
b = 2, u(si ) = 1{4} (i). Let = {(G1 , 1 ), (G2 , 2 )} abstraction skeleton
, G1 G2 Figure 4b state mappings
(
s1;4 , {1, 3}
1 (si ) =
,
s1;i , otherwise
(
s2;4 , = 2
2 (si ) =
.
s2;i , otherwise
Consider set models M(c,u,b) , constant c[1]() = c[2]() = 1, b[1] = b[2] = 2, and,
j [2], u[i](si;j ) = 1{4} (j). optimal plan s0 -plan = h(s0 , o2 , s2 ), (s2 , o4 , s4 )i,
(c,u,b)

Qb () = 1, optimal 1 (s0 )-plan M1
1 = h(s1;0 , o1 , s1;4 )i,
(c,u,b)
b[1]
Q (1 ) = 1, optimal 2 (s0 )-plan M2
2 = h(s2;0 , o2 , s2;4 )i,
Qb[2] (2 ) = 1. Since
h (s0 , b) = Qb () Qb[1] (1 ) + Qb[2] (2 ) = h1 (1 (s0 ), b[1]) + h2 (2 (s0 ), b[2]),
M(c,u,b) additive abstraction .
Theorem 1 OSP task = hV, s0 , u; O, c, bi, abstraction skeleton =
{(G1 , 1 ), . . . , (Gk , k )} , AAS , digraphs given
explicitly, hM (s0 , b) computed time polynomial |||| ||M||.


ff
Proof: Let = {Mi }i[k] , Mi = Si , (s0 ), ui , Oi , , ci , Qbi , additive abstraction basis AS. [k], let Si0 = {s Si | ci (i (s0 ), s) bi }. Since
5. optimal classical planning, requirement abstraction overestimate costs typically
posed states original model, initial state (Yang et al., 2008; Katz &
Domshlak, 2010b; Helmert et al., 2014). extra requirement, however, pragmatic reasons
efficiency allows abstraction computed preprocessing individually every
state examined search. Heuristics OSP, however, functions state also
available cost budget, latter directly applies initial (aka current) state only.
sum, defining abstractions respect entire state space necessity classical
planning, OSP even clear whether defining abstractions respect specific pair
state budget deliver practical benefits. not, however, interpreted
formal impossibility claim, investigation direction definitely worthwhile.

109

fiDomshlak & Mirkis

(, , )

SSSS
kk
SS
kkkk
(, u, )
SSSSkk (c, , ) SSSSkk (, , b)
kkkkSS
kkkkSS
(c, u, )

(, u, b)
(c, , b)
SSS
SSS
kkk
kkk
(c, u, b)

Figure 5: Fragments restricted optimization abstractions C U B
digraphs given explicitly, shortest paths (s0 ) states Gi (and
thus, particular, determining Si0 ) computed time polynomial
||M||
P
[k]. turn, since hi (i (s0 ), bi ) = maxsSi0 ui (s), hM (s0 , b) = i[k] hi (i (s0 ), bi )
computed time polynomial ||M||.

message Theorem 1 positive, yet establishes necessary condition
relevance OSP abstractions practice. Given OSP task , fixed
abstraction skeleton joint performance measure space C U B,
able automatically separate (c, u, b) C U B constitute
abstractions not, within former set, denoted
C U B,
home abstraction provides us accurate (aka low) estimate
h (s0 , b) possible. Here, even first item agenda necessarily trivial as,
general, seems lack convenient combinatorial properties. instance, generally
form combinatorial rectangle C U B: Consider OSP state model
GM abstraction skeleton running example. Let c C cost function
vector c[1] c[2] constant functions value 1, two performance
measures (c, u, b), (c, u0 , b0 ) CUB defined via budget vectors b = {b[1] = 2, b[2] = 0}
b0 = {b0 [1] = 0, b0 [2] = 2}, value function vectors u u0 , u[1], u[2], u0 [1],
u0 [2] evaluating zero states except u[1](s1;4 ) = u0 [2](s2;4 ) = 1.
0 0
hard verify M(c,u,b) AAS M(c,u ,b ) AAS : M(c,u,b) , state
(c,u,b)
s1;4 u[1](s1;4 ) = 1 reachable M1
s1;0 = 1 (s0 ) b[1] = 2,
0
0
(c,u0 ,b0 )
(c,u
,b
)
0

, state s2;4 u [2](s2;4 ) = 1 reachable M2
s2;0 = 2 (s0 )
0
0
b0 [2] = 2. contrast, M(c,u ,b) 6AAS M(c,u,b ) 6AAS : sets
models, model either comes budget (and initial state model
0
value zero), states non-zero value all. Hence, M(c,u ,b)
0
M(c,u,b ) estimate h (s0 , b) zero, h (s0 , b) = 1.
light above, approach overall agenda complexity analysis abstractionbased heuristic functions steps, different fixations three dimensions
A: If, instance, given vector value functions u known belong
projection U, search quality abstraction abstraction
subset A(, u, ) A, corresponding projection {u}. show below,
even constrained optimizations kind challenging. lattice Figure 5 depicts range options constrained optimization; extreme settings,
110

fiOn Oversubscription Planning Heuristic Search

o1

G1

G2

: s2;1
uu
u
u
uu

o3

/ s2;3

o1

s1;0

o2

/ s1;2

o4

/ 1;4
j

s2;0

o3 ,o5

o2



o5

/ s2;4
j

o4

Figure 6: Homomorphic abstraction skeleton G() Figure 4
A(, , ) simply renaming A, A(c, u, b) corresponds single abstraction
M(c,u,b) A.
3.2 Partitions Homomorphic Abstractions
proceed consider specific family additive abstractions, reveal
interesting properties, show contains substantial islands tractability.
Definition 1 allowing general abstraction skeletons, work focus homomorphic abstraction skeletons6 (Helmert et al., 2014).
Definition
2 abstraction skeleton
= {(G1 , 1 ), . . . , (Gk , k )} OSP state
ff
model = S, s0 , u, O, , c, Qb homomorphic if, [k], Oi = O, (s, o) = s0
(i (s), o, (s0 )) Ti .
instance, running example, abstraction skeleton Figure 4b homomorphic (since, e.g., (s1 , o3 , s3 ) GM yet (1 (s1 ), o3 , 1 (s3 )) = (s1;4 , o3 , s1;4 ) 6 GM1 ),
abstraction skeleton Figure 6 homomorphic. Furthermore, focus fragment
additive abstractions
Ap = [Cp Bp ] ,
Cp C, U, Bp B correspond cost, value, budget partitions,
respectively.


ff
Definition 3 Given OSP state model = S, s0 , u, O, , c, Qb , homomorphic
abstraction skeleton = {(G1 , 1 ), . . . , (Gk , k )} joint performance measure
C U B,
P
c C cost partition iff, operator O, i[k] c[i](o) c(o);
P
u U value partition iff, state S, i[k] u[i](i (s)) u(s);
P
b B budget partition iff, i[k] b[i] b.
follows, node x lattice Figure 5, Ap (x) refer A(x)Ap ;
e.g., Ap (, u, ) = A(, u, ) Ap .
begin analysis Ap establishing interesting completeness relationship
sets Cp Bp , well even stronger individual completeness Cp
Bp . Formulated Theorem 2, properties Ap play key role computational
analysis later on.
6. results also hold verbatim general labeled paths preserving abstraction skeletons
studied Katz Domshlak (2010b) context optimal classical planning. However,
presentation somewhat accessible restricted homomorphic abstraction skeletons.

111

fiDomshlak & Mirkis

c

Cp

b

Bp



(1)

b

c

Bp

Cp



(2)

Figure 7: Illustration sub-claims (1) (2) Theorem 2: (1), gray ellipse
within Bp stands subset budget partitions b pair c
abstraction, is, Ap (c, , b) 6= . However, pairing budget
partitions b c requires careful selection value partition u (so
M(c,u,b) abstraction), exists budget partition b
choice u job.

Theorem 2 Given OSP task = hV, s0 , u; O, c, bi homomorphic abstraction
skeleton = {(G1 , 1 ), . . . , (Gk , k )} ,
(1) cost partition c Cp , exists budget partition b Bp

M(c,u,b ) value partitions u ;

(2) budget partition b Bp , exists cost partition c Cp

M(c ,u,b) value partitions u .
proof Theorem 2 appears Appendix A, p. 145. Figure 7 illustrates statement sub-claim (1) Theorem 2, well as, indirectly, corollaries.7 first
corollary Theorem 2 projections Ap Cp , , Bp entire sets
Cp , , Bp , respectively. is, cost partition c (and similarly, budget partition value partition) matched abstraction partition
component. Second, budget partition b paired given cost
partition c abstractions , is, b Bp , Ap (c, , b) 6= ,
always budget partitions paired c. Finally, pairing
c-compatible budget partitions b c requires careful selection value
partition u, exists c-compatible budget partition b choice
u result M(c,u,b) abstraction .
priori, properties Ap simplify task abstraction discovery
optimization within space partitions Cp Bp , later show
indeed case. However, complexity analysis abstraction discovery within Cp Bp
general terms still problematic OSP formalism parametric
7. respective illustration sub-claim (2) Theorem 2 completely similar, mutatis mutandis.

112

fiOn Oversubscription Planning Heuristic Search

representation value functions. Hence, proceed examining abstraction
discovery OSP context fixed value partitions u .

4. Value Partitions Complete Abstractions
Let OSP task, explicitly given homomorphic abstraction skeleton ,
u value partition AS. immediate corollary Theorem 2
Ap (, u, ) empty, thus try computing min(c,u,b)Ap (,u,) hM(c,u,b) (s0 ).
yet, however, know whether task polynomial-time solvable
non-trivial class value partitions. fact, although Ap (, u, ) known, Theorem 2,
non-empty, so, too, subsets Ap (, u, b) Ap (c, u, ), finding even
abstraction (c, u, b) Ap (, u, ) necessarily easy.
4.1 0-Binary Value Partitions
first step, examine abstraction discovery within fragment Ap
value functions u[i] abstract models call 0-binary. Later, Section 4.2,
show findings 0-binary abstract value functions extended general
value partitions.
Definition 4 real-valued function f called 0-binary codomain f {0, }
R+ . set F 0-binary functions called strong functions F
codomain {0, }.
one hand, 0-binary functions constitute rather basic family value functions.
Hence, abstraction optimization hard them, likely hard nontrivial family abstract value functions. hand, 0-binary abstract value
functions seem fit well abstractions planning tasks value functions linear
combinations indicators, representing achievement goal value state
variable.
respect, first tractability results abstraction discovery Ap (, u, )
u strong 0-binary value partition. first (and simpler) result Theorem 3
assumes fixed action cost partition, next result, Theorem 7,
simultaneous selection admissible pairs cost budget partitions. Corollary 4
Theorem 10 show results Theorem 3 Theorem 7, respectively,
extended pseudo-polynomial algorithms general 0-binary value partitions.
4.1.1 Strong 0-Binary Value Partitions Knapsack problem
first tractability result abstraction discovery within Ap (c, u, ) u strong
0-binary value partition c arbitrary cost partition. key role played
well-known
Knapsackffproblem (Dantzig, 1930; Kellerer, Pferschy, & Pisinger, 2004).


instance {wi , }i[n] , W Knapsack problem given weight allowance W
set objects [n], object [n] annotated
.
Pwith weight wi value
0 [n]
objective


find

subset
Z

[n]

maximizes



subsets
Z
iZ
P

w

W.

strict
Knapsack

refer


variant Knapsack
0

iZ
inequality constraint strict. Knapsack NP-hard (Karp, 1972; Garey & Johnson,
113

fiDomshlak & Mirkis

1978), exist pseudo-polynomial algorithms run time polynomial
description problem unary representation W (Dudzinski & Walukiewicz,
1987). latter property makes solving Knapsack practical many applications
ratio minWi wi reasonably low. Likewise, = j i, j [n], greedy algorithm
solves problem linear time iteratively expanding Z one weight-wise
lightest objects [n] \ Z, Z cannot expanded within W .
Theorem 3 (Ap (c, u, ) & strong 0-binary u)
Let = hV, s0 , u; O, c, bi OSP task, explicit homomorphic abstraction skeleton , u strong 0-binary value partition. Given cost partition c Cp ,
finding abstraction (c, u, b) Ap (c, u, ) computing corresponding heuristic
estimate hM(c,u,b) (s0 , b) done time polynomial |||| ||AS||.
Proof: proof reduction polynomial fragment Knapsack problem
corresponding items identical value. Let = {(G1 , 1 ), . . . , (Gk , k )}, and,
given u strong 0-binary value partition, let codomain u[i] {0, }
R+ .
[k], let wi cost cheapest path Gi (s0 ) (one the)
states Si u[i](s) = . Since explicit abstraction skeleton, set {wi }i[k]
computed time polynomial ||AS|| using one standard algorithms



ff
single-source shortest paths problem. Consider Knapsack problem {wi , }i[k] , b ,
weights wi value identical objects. Let Z [k]
solution (optimization) Knapsack problem; recall computable
polynomial time. Given that, define budget profile b B follows:
(
wi , Z
[k], b [i] =
0,
otherwise.
remains shown (c, u, b ) actually induces additive abstraction

. Assume contrary M(c,u,b ) 6AAS , let optimal s0 -plan
. construction Knapsack problem b , Z,

(c,u,b )
(s)-plan P
Qb [i] (i ) = . Definition 1, assumption implies
Mi

Qb () > iZ Qb [i] (i ) = |Z|. However, Theorem 2, exists least one
budget partition b Bp M(c,u,b) AAS . Note budget partition
induces aP
feasible solution Z 0 = {i | wi b[i]} Knapsack problem, satisfying
Qb () iZ 0 Qb[i] (i ) = |Z 0 |. This, however, implies |Z| < |Z 0 |, contradicting

optimality Z, thus accomplishing proof M(c,u,b ) AAS .

construction proof Theorem 3 may appear somewhat counterintuitive:
interested minimizing heuristic estimate h (s0 , b), abstraction

M(c,u,b ) selected via value-maximizing Knapsack problem. Indeed, ultimately
would like obtain
min
hM(c,u,b) (s0 , b),
(6)
b : (c,u,b)Ap

heuristic manage compute polynomial time actually
max
b : (c,u,b)Ap

hM(c,u,b) (s0 , b).
114

(7)

fiOn Oversubscription Planning Heuristic Search

time, note that, fixed pair c Cp u , estimate Eq. 7
still least (and possibly much more) accurate estimate would obtained
providing k abstract models entire budget b. Later show
superior accuracy verified experiments, first proceed examining
working general 0-binary value partitions.
strong 0-binary value partitions rather restrictive, finding element
Ap (c, u, ) general 0-binary u longer polynomiala reduction Knapsack
straightforward. However, Knapsack solvable pseudo-polynomial time, plugging
Knapsack algorithm proof Theorem 3 results search algorithm
Ap (c, u, ) general 0-binary u.
Corollary 4 (Ap (c, u, ) & 0-binary u)
Let = hV, s0 , u; O, c, bi OSP task, explicit homomorphic abstraction skeleton , u 0-binary value partition. Given cost partition c Cp , finding
abstraction (c, u, b) Ap (c, u, ) computing corresponding heuristic estimate
hM(c,u,b) (s0 , b) done time polynomial ||||, ||AS||, unary representation
budget b .
test illustrate value additive abstractions bring heuristic-search
OSP, implemented prototype heuristic-search OSP solver8 top Fast Downward
planner (Helmert, 2006). Since, unlike classical net-benefit planning, OSP still lacks
standard suite benchmarks comparative evaluation, cast role
STRIPS classical planning tasks International Planning Competitions (IPC) 19982006. translation OSP done associating separate unit-value
proposition conjunctive goal corresponding classical IPC task.
Within prototype, implemented BFBB search OSP, provided support
basic pattern-database abstraction skeletons, action cost partitions, abstraction selection Ap (c, u, ) strong 0-binary value partitions proof Theorem 3.
Specifically, task k sub-goals:
(i) abstraction skeleton comprised set k projections planning task onto
connected subsets ancestors respective k goal variables causal graph.
size projection limited 1000 abstract states, ancestors
goal variable v added corresponding projection (initialized contain
v) breadth-first manner, v back along arcs causal graph,
abstraction could expanded within aforementioned size limit.
(ii) value partition u associated entire value sub-goal hv/di (only)
projection associated v.
(iii) cost partition c distributed cost operator uniformly
projections invalidate o, i.e., reflected least one state variable
affected o.
evaluation, compared BFBB node expansions three heuristic functions,
tagged blind, basic, hM . three heuristics, h-value node n set 0
cost budget n over-consumed. cost budget over-consumed, then:
8. aware domain-independent planner optimal OSP.

115

fiDomshlak & Mirkis

airport (25)
blocks (23)
depot (3)
driverlog (12)
freecell (5)
grid (2)
gripper (6)
logistics (10)
miconic (50)
mystery (4)
openstacks (7)
rovers (10)
satellite (9)
tpp (7)
trucks (9)
pipesw-t (12)
pipesw-nt (7)
psr-small (30)
zenotravel (10)
total

hM
23
23
3
12
5
2
6
10
50
4
7
10
9
7
9
12
7
30
10
239

25%
basic
23
23
3
12
5
2
6
10
50
4
7
10
8
7
9
12
7
30
10
238

blind
23
23
3
12
5
2
6
10
50
4
7
10
8
7
9
12
7
30
10
238

hM
20
23
3
12
5
2
6
10
50
4
7
10
7
7
9
12
7
30
10
234

50%
basic
20
23
3
12
5
2
6
10
50
4
7
7
6
7
8
12
7
30
9
228

blind
20
23
3
11
5
2
6
10
50
4
7
7
6
7
8
12
7
30
8
226

hM
19
22
3
11
5
2
6
10
50
4
7
7
6
6
6
12
7
30
9
222

75%
basic
20
18
3
9
5
2
6
10
50
4
7
6
4
6
5
11
7
30
8
211

blind
18
17
3
9
5
2
6
10
50
4
7
6
5
6
5
11
7
30
8
209

hM
19
17
3
10
5
1
6
10
50
3
7
6
5
6
5
11
7
30
8
209

100%
basic
20
17
2
7
5
1
6
10
45
3
7
5
4
5
5
10
6
30
7
195

blind
18
17
2
6
5
1
6
10
45
2
7
5
4
5
5
10
6
30
7
191

Table 1: Number problems solved across different budgets using OPEN list ordered heuristic evaluation Figure 3

Blind BFBB constitutes trivial baseline h(n) simply set total value
goals.
basic BFBB, h(n) set total value goals, individually
achieved within respective projection abstraction (see Theorem 1) given entire
remaining budget.
hM additive abstraction heuristic selected Ap (c, u, )
proof Theorem 3.
evaluation contained planning tasks could determine offline
minimal cost budget needed achieve goals. task approached
four different budgets, corresponding 25%, 50%, 75%, 100% minimal
cost needed achieve goals task, run restricted 10 minutes.
Table 1 shows number tasks solved within domain level cost budget.9
Figure 8 depicts results terms expanded nodes across four levels cost budget.
(Figures 18-21 Appendix B provide detailed view results Figure 8
breaking different levels cost budget.) Despite simplicity
abstraction skeletons used, number nodes expanded BFBB hM
typically substantially lower number nodes expanded basic BFBB,
difference sometimes reaching three orders magnitude.
9. reiterate task considered solved upon termination BFBB, is,
optimal plan found proven optimal.

116

fiOn Oversubscription Planning Heuristic Search

(a)
108

unsolved

107
106
105
hM

104
103
102

unsolved

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

101

100 0
10 101 102 103 104 105 106 107 108
blind

(b)
108

unsolved

107
106
hM

105
104
103
unsolved

102
101

100 0
10 101 102 103 104 105 106 107 108
basic

Figure 8: Comparative view empirical results Table 1 terms expanded nodes

4.1.2 Freeing Cost Partition: Knapsack Meets Convex Optimization
Returning algorithmic analysis context strong 0-binary value partitions,
proceed relaxing constraint sticking fixed action cost partition c.
buys flexibility selecting abstractions Ap (, u, ), allowing us improve
accuracy heuristic estimates, still retaining computational tractability.
117

fiDomshlak & Mirkis

input: = hV, s0 , u; O, c, bi, = {(G1 , 1 ), . . . , (Gk , k )} ,
strong 0-binary value partition u
output: (u)
= 1 k
reduce Gi nodes reachable (s0 )
= k downto 1
always-achievable(m) return
return 0
always-achievable(m):

ellipsoid-method(separation-oracle-Lm
1 ) 7 solution x dom(X ) L1
x[] b return true
else return false
(a)
separation-oracle-Lm
1 (x dom(X )):
let permutation [k]
x[b[
P(1)]] x[b[ (2)]] x[b[ (k)]]
x[] i[m] x[b[ (i)]] return Yes
P
else return constraint i[m] b[ (i)]
(b)
Figure 9: polynomial-time algorithm computing (u) strong 0-binary value
partition u (Theorem 7)

Given OSP task = hV, s0 , u; O, c, bi, homomorphic abstraction skeleton AS,
value partition u AS, let


(u) = min
max
(8)
hM(c,u,b) (s0 , b) .
cCp

b : (c,u,b)Ap

Obviously, estimate h(s0 , b) = (u) least accurate estimate Eq. 7
derived respect fixed cost partition c.
show that, OSP task , abstraction skeleton = {(G1 , 1 ), . . . ,
(Gk , k )} , strong 0-binary value partition u AS, (u)
computed polynomial time. corresponding algorithm shown Figure 9,
Figure 9a depicting macro-flow algorithm Figure 9b depicting specific
implementation solve sub-routine makes overall time complexity
algorithm polynomial.
high-level flow algorithm Figure 9a follows. Since u strong 0binary value partition, let codomain abstract value functions u[i] {0, }
R+ . Given that, (c, u, b) Ap (, u, ), holds hM(c,u,b) (s) =
{0} [k]. k abstract models M(c,u,b)
contribute additive estimate hM(c,u,b) (s) either 0.
118

fiOn Oversubscription Planning Heuristic Search

first loop algorithm preprocessing for-loop eliminates
abstraction skeleton nodes structurally unreachable abstract initial
states 1 (s0 ), . . . , k (s0 ).10 ease presentation, follows assume
cleanup abstraction skeleton leaves Gi least one state whose value
. second for-loop algorithm decreasingly iterates values {k, (k
1), . . . , 2, } possibly come abstractions Ap (, u, ) positive
estimate h (s0 , b). candidates (u) tested turn via sub-routine
always-achievable. test returns positive first time, done,
tested candidate identified (u). Otherwise, test fails [k],
(u) = 0, particular implying state value greater 0 reached
s0 budget b.
test always-achievable (u) = based linear program (LP) Lm
1 , given
Eq. 10. linear program defined variables
#
"
[
[
(9)
{c[i](o)} ,
X = {}
{d(s)}sGi {b[i]}
oO

i[k]

constraints (10a)-(10c), objective maximizing value variable .
Lm
1 :
max
subject


d(i (s0 )) = 0,
[k] : d(s) d(s0 ) + c[i](o), (s0 , o, s) Gi
,


b[i] d(s),
Gi s.t. u[i](s) =
(
c[i](o) 0,
[k]
: P
,
i[k] c[i](o) c(o)
X
Z [k], |Z| = :
b[i].

(10a)

(10b)
(10c)

iZ

roles different variables Lm
1 follows.
Variable c[i](o) captures cost associated label digraph Gi
AS.
state Gi , variable d(s) captures cost cheapest path Gi
(s0 ) s, given edges Gi weighted consistently values
variables c[i]().
Variable b[i] captures minimal budget needed reaching Gi state value
state (s0 ), given that, again, edges Gi weighted consistently
variable vector c[i].
10. preprocessing step replaced adding extra constraints linear program described
below. However, would unnecessarily complicate presentation without adding much value.

119

fiDomshlak & Mirkis

singleton variable captures minimal total cost reaching states value
precisely k models M(c,u,b) .
semantics constraints Lm
1 follows.
first two sets constraints (10a) come simple LP formulation

P single
P source shortest paths problem source node (s0 ): Optimizing
i[k]
sGi d(s) fixed weighting c edges leads computing precisely
that, k digraphs simultaneously.
third set constraints (10a) establishes costs cheapest paths {Gi }
states (s0 ) states valued , enforcing semantics variables b[1], . . . , b[k].
Constraints (10b) cost partition constraints enforce c Cp .
Constraints (10c) enforce aforementioned semantics objective variable .
Two things worth noting here. First, nodes digraphs G1 , . . . , Gk
structurally reachable source nodes 1 (s0 ), . . . , k (s0 ), respectively (as

ensured first for-loop algorithm),
polytope induced L1 bounded
non-empty. Indeed, assignment oO {c[i](o)} consistent
positiveness constraints (10b), variables d() bounded
lengths respective shortest paths. turn, bounding d() bounds
variables c[1], . . . , c[k] via third set constraints (10a), constraints (10c)
bound objective .
Second, number variables, well number constraints (10a)


k
(10b), polynomial |||| ||AS||, number constraints (10c)
. Thus,
solving Lm
1 using standard methods linear programming infeasible. Lemma 5
show problem actually mitigated, then, Lemma 6 show
semantics Lm
1 match objective finding (u).
Lemma 5 algorithm Figure 9 terminates time polynomial |||| ||AS||.
Proof: runtime complexity algorithm boils complexity solving
Lm
1 , and, number variables L1 (m), well number constraints
(10a)
polynomial |||| ||AS||, number constraints (10c)
(10b),
k
cannot solved polynomial time using standard methods linear
.
Thus,
L
1

programming, Simplex algorithm (Dantzig, 1963) Interior-Point methods (Nemirovsky & Yudin, 1994). However, using algorithms,
Ellipsoid algorithm (Grotschel, Lovasz, & Schrijver, 1981) Random Walks family
algorithms originating work Bertsimas Vempala (2004), LP
exponential number constraints solved polynomial time, provided
polynomial time separation oracle LP. polynomial-time separating oracle
convex set K Rn procedure given x Rn , either verifies x K
returns hyperplane separating x K. procedure run polynomial time.
case, separation problem is, given assignment variables Lm
1 , test
whether satisfies (10a), (10b), (10c), not, produce inequality among (10a),
(10b), (10c) violated assignment.
120

fiOn Oversubscription Planning Heuristic Search

show separation problem Lm
1 solved polynomial time
using called m-sum minimization LPs (Punnen, 1992), precisely
(parametrized m) procedure separation-oracle-Lm
1 Figure 9b does. number
constraints (10a) (10b) polynomial, satisfaction assignment x dom(X )
tested directly substitution. constraints (10c), letP permutation [k]
x[b[ (1)]] x[b[ (2)]] x[b[ (k)]]. x[] i[m] x[b[ (i)]],
easy see
Px satisfies constraints (10c). Otherwise, violated
inequality i[m] b[ (i)].

Lemma 6 algorithm Figure 9a computes (u).
proof Lemma 6 appears Appendix A, p. 146. Combining statements
Lemmas 5 6, Theorem 7 summarizes tractability result abstraction discovery
Ap (, u, ) strong 0-binary value partitions u.
Theorem 7 (Ap (, u, )(s) & strong 0-binary u)
Given OSP task = hV, s0 , u; O, c, bi, homomorphic explicit abstraction skeleton
, strong 0-binary value partition u , (u) computed time polynomial
|||| ||AS||.
Unfortunately, practical value result Theorem 7 yet evaluated.
far, found reasonably efficient implementation Ellipsoid method
linear inequalities, while, best knowledge, Random Walks algorithms (Bertsimas & Vempala, 2004) never implemented all. hope state
affairs change soon, allowing powerful algorithms used theory,
also practice.
4.1.3 Strong General 0-Binary Value Partitions
Recall polynomial result Theorem 3 strong 0-binary value partitions easily
extends Corollary 4 pseudo-polynomial algorithm general 0-binary value partitions. turns pseudo-polynomial extension Theorem 7 possible well,
though technically involved. corresponding algorithm shown Figure 10.
Following format Figure 9, Figure 10a depicts macro-flow algorithm
Figure 10b shows specific implementation solve sub-routine desired
time complexity achieved.
Similarly algorithm Figure 9, preprocessing for-loop algorithm first
eliminates abstraction skeleton nodes structurally unreachable
abstract initial states 1 (s0 ), . . . , k (s0 ). Next, algorithm performs binary
search interval containing (u).11 Since u 0-binary value partition, [k],
{0, }, R+ , denote codomain abstract value
P function u[i]. Given that,
(c, u, b) Ap (, u, ), holds h(c,u,b) (s) = iZ Z [k].
size combinatorial hypothesis space prohibitive, while-loop Figure 10
performs binary search relaxed hypothesis space, corresponding continuous
11. binary search could used algorithm Figure 9 well, would
mere optimization, necessary avoid exponential blowup time complexity.

121

fiDomshlak & Mirkis

input: = hV, s0 , u; O, c, bi, = {(G1 , 1 ), . . . , (Gk , k )} ,
0-binary value partition u
output: (u)
= 1 k
reduce Gi nodes reachable (s0 )
let 0 < < mini[k]
P
0
i[k]
>
v + ( )/2
always-achievable(v) v
else v
= 0 return 0
else return
always-achievable(v):
ellipsoid-method(separation-oracle-Lv2 ) 7 solution x dom(X ) Lv2
x[] b return true
else return false
(a)
separation-oracle-Lv2
(x dom(X )):
ff
strict-Knapsack( {x[b[i]], }i[k] , x[] ) 7 solution Z [k]
P
iZ < v return Yes
P
else return constraint iZ b[i]
(b)
Figure 10: pseudo-polynomial algorithm approximating (u) general 0-binary
value partitions u (Theorem 10)

P
interval [0, i[k] ] R+0 . parameter serves sufficient precision criterion
termination.
iteration corresponding interval [, ], algorithm uses sub-routine
always-achievable test hypothesis (u) v, v mid-point [, ].
test positive, next tested hypothesis (u) v 0 , v 0 midpoint
[v, ]. Otherwise, next hypothesis corresponds midpoint [, v).
while-loop done, reported estimate set ; still might lag
(u), lag arbitrarily reduced reducing , anyway, (u)
ensures admissibility estimate. If, however, while-loop terminates = 0,
(u) < mini[k] implies (u) = 0, return.
test always-achievable (u) v based linear program Lv2 ,
defined variables X Eq. 9, obtained Lm
1 replacing constraints (10c)
constraints (11c):
122

fiOn Oversubscription Planning Heuristic Search

Lv2 :
max
subject


d(i (s0 )) = 0,
[k] : d(s) d(s0 ) + c[i](o), (s0 , o, s) Gi
,


b[i] d(s),
Gi s.t. u[i](s) =
(
c[i](o) 0,
[k]
: P
,
i[k] c[i](o) c(o)
X
X
Z [k] s.t.
v :
b[i].
iZ

(11a)

(11b)
(11c)

iZ

semantics variables remains Lm
1 , captures minimal
total cost ofPreaching states {si }i[k] abstract models M(c,u,b)
total value i[k] u[i](si ) v. new constraint (11c) enforces semantics .
Lemma 8 > 0, algorithm Figure 10 terminates time polynomial
||||, ||AS||, log 1 , unary representation budget b .
P



Proof: number iterations while-loop approximately log2 i[k]
,

run-time iterations boils complexity solving Lv2 . Here,
v
Lemma 5 linear programs Lm
1 , number variables L2 , well number
constraints (11a) (11b), polynomial |||| ||AS||, number
constraints (11c) (2k ). Therefore, always-achievable(v) also employs ellipsoid
method sub-routine separation-oracle-Lv2 associated separation problem.
show separation problem Lv2 solved pseudo-polynomial time
using standard pseudo-polynomial procedure strict Knapsack problem.
Given assignment x dom(X ), feasibility respect (11a) (11b)
tested directly substitution.
constraints (11c),
ff let Z [k] optimal solution
strict Knapsack problem {x[b[i]], }i[k] , x[] , weight allowance x[] k
objects, object [k] associated weight x[b[i]] value .
P
value iZ Z smaller v, x satisfies constraints
(11c). Assume contrary x violatesP
constraint (11c), corresponding
0
setPZ [k]. definition (11c),
iZ 0 v, assumption,
x[] >
x[b[i]].
That,
however,
implies
Z 0 feasible solution
0
iZ
strict Knapsack, value higher presumably optimal Z.
P
Otherwise, iZ v, Z itselfPprovides us constraint (11c)
violated x. x[] >
iZ x[b[i]] holds
ff virtue Z
solution strict Knapsack problem {x[b[i]], }i[k] , x[] .


123

fiDomshlak & Mirkis

Lemma 9 0 < < mini[k] , algorithm Figure 10a computes
(u) .
proof Lemma 9 appears Appendix A, p. 147. Combining statements
Lemmas 8 9, Theorem 10 summarizes result optimized abstraction discovery
Ap (, u, ) general 0-binary value partitions u. Importantly, note algorithm
Figure 10 depends unary representation budget, possible
state values. particular, means dependence complexity number
alternative sub-goals OSP task interest polynomial. Finally, Theorem 10
formulated terms estimate precision values abstract value
functions u[i] arbitrary real numbers. case integer-valued sets functions
u, well various special cases real-valued functions, (u) determined
precisely using simplification algorithm Figure 10. instance, 1 , . . . , k
integers, setting value (0, 1) results while-loop terminating
= (u). details, however, theoretical interest; reasonably small
values , practice difference estimates h(s, b) h(s, b) + .
Theorem 10 (Ap (, u, )(s) & 0-binary u)
Given OSP task = hV, s0 , u; O, c, bi, homomorphic explicit abstraction skeleton =
{(G1 , 1 ), . . . , (Gk , k )} , 0-binary value partition u , > 0, possible
approximate (u) within additive factor time polynomial ||||, ||AS||, log 1 ,
unary representation budget b .
4.2 General Value Partitions
0-binary value partitions rather useful themselves, turns
pseudo-polynomial algorithms abstraction discovery explicit homomorphic abstraction skeletons 0-binary value partitions extended rather easily arbitrary value
partitions, using following observations:
(1) OSP task = hV, s0 , u; O, c, bi, homomorphic abstraction skeleton =
{(G1 , 1 ), . . . , (Gk , k )} , value partition u AS, number
distinct values taken u[i] trivially upper-bounded number states Gi ;

(2) pseudo-polynomial solvability Knapsack problem extends general
variant known Multiple-Choice Knapsack (Dudzinski & Walukiewicz, 1987; Kellerer
et al., 2004).
Multiple-Choice (MC) Knapsack problem hN1 , . . . , Nm ; W given weight
allowance W classes objects N1 , . . . , Nm , object j Ni annotated
weight wij value ij . objective
find set Z contains
P
one
P object class maximizes (i,j)Z ij sets satisfying
(i,j)Z wij W. strict MC-Knapsack, refer variant MC-Knapsack
inequality constraint strict. MC-Knapsack generalizes regular Knapsack
thus NP-hard. However, similarly regular Knapsack problem, MC-Knapsack also
admits pseudo-polynomial, dynamic programming algorithm runs time polynomial
124

fiOn Oversubscription Planning Heuristic Search

description problem unary representation W (Dudzinski &
Walukiewicz, 1987; Kellerer et al., 2004).
Theorem 11 (Ap (c, u, ))
Let = hV, s0 , u; O, c, bi OSP task, let = {(G1 , 1 ), . . . , (Gk , k )} explicit homomorphic abstraction skeleton , let u arbitrary value
partition AS. Given cost partition c Cp , possible find abstraction
(c, u, b) Ap (c, u, ) compute corresponding heuristic estimate hM(c,u,b) (s0 , b)
time polynomial ||||, ||AS||, unary representation budget b.
Proof: proof similar proof Theorem 3, compilation
MC-Knapsack problem.
[k], let ni number distinct values taken u[i], let {i1 , . . . , ini } R+
codomain u[i], and, j [ni ], let wij cost cheapest path Gi
(s0 ) (one the) states Si u[i](s) = ij . Since explicit abstraction
skeleton, [k], holds ni |Si |, set {wij }i[k],j[ni ] computed
time polynomial ||AS|| using one standard algorithms single-source shortest
paths problem.
Consider MC-Knapsack problem weight allowance b k classes
objects N1 , . . . , Nk , |Ni | = ni object j Ni annotated weight wij

value ij . Let Z ki=1 Ni solution (optimization) MC-Knapsack problem;
recall computable pseudo-polynomial time. Given that, define budget profile
b B follows:
(
wij , (i, j) Z

[k], b [i] =
0,
otherwise.
Showing (c, u, b ) actually induces additive abstraction completely identical proof corresponding argument Theorem 3, thus omitted.

Theorem 12 (Ap (, u, ))
Given OSP task = hV, s0 , u; O, c, bi, homomorphic explicit abstraction skeleton =
{(G1 , 1 ), . . . , (Gk , k )} , arbitrary value partition u AS, > 0,
possible approximate (u) within additive factor time polynomial ||||,
||AS||, log 1 , unary representation budget b .
algorithm abstraction discovery Theorem 12 depicted Figure 11.
high-level flow differs flow algorithm Figure 10 general 0-binary
value partitions initialization parameters . major difference
algorithms tests candidate values v based linear
programs Lv3 , defined follows.
[k], let {i1 , . . . , ini } R+ codomain u[i]. v R+ , linear
program Lv3 defined Eq. 13 variables


[
[
[
{d(s)}sGi
X = {}
{b[i, j]}
{c[i](o)}.
(12)
i[k]

j[ni ]

125

oO

fiDomshlak & Mirkis

input: = hV, s0 , u; O, c, bi, = {(G1 , 1 ), . . . , (Gk , k )} ,
0-binary value partition u
output: (u)
= 1 k
reduce Gi nodes reachable (s0 )
let 0 < < mini[k] minj[ni ] ij
P
0
i[k] maxj[ni ] ij
>
v + ( )/2
always-achievable(v) v
else v
= 0 return 0
else return
always-achievable(v):
ellipsoid-method(separation-oracle-Lv3 ) 7 solution x dom(X ) Lv3
x[] b return true
else return false
(a)
separation-oracle-Lv3 (x
dom(X )):
ff
strict-MC-Knapsack( {x[b[1, j]], 1j }j[n1 ] , . . . , {x[b[k, j]], kj }j[nk ] ; x[] )
7 solution Z [n1 ] [nk ]
P
i[k] iZ(i) < v return Yes
P
else return constraint i[k] b[i, Z(i)]
(b)
Figure 11: (a) modification algorithm Figure 10 arbitrary value partitions
u (Theorem 12), (b) pseudo-polynomial time separation oracle
corresponding linear programs Lv3 Eq. 13

variables differ variable set Lv2 (see Eq. 9) larger set b-variables:
Variable b[i, j] captures minimal budget needed reaching Gi state
value i,j state (s0 ), given edges Gi weighted consistently
variable vector c[i].
126

fiOn Oversubscription Planning Heuristic Search

Lv3 :
max
subject


d(i (s0 )) = 0,
[k] : d(s) d(s0 ) + c[i](o), (s0 , o, s) Gi
,


b[i, j] d(s),
j [ni ]s Gi s.t. u[i](s) = ij
(13a)
(
c[i](o) 0,
[k]
: P
,
(13b)
i[k] c[i](o) c(o)
Z [n1 ] [nk ]
X
X
s.t.
iZ(i) v :
b[i, Z(i)].
i[k]

(13c)

i[k]

Like Lemma 8 linear programs Lv2 , number variables
Lv3 , well number constraints (13a) (13b), polynomial ||||
||AS||, number constraints (13c) (dk ) = maxi[k] ni . Therefore,
always-achievable(v) also employs ellipsoid method pseudo-polynomial time separation oracle, latter based solving strict MC-Knapsack problem (see
Figure 11b). Otherwise, solving Lv2 solving Lv3 similar.
Lemma 13 > 0, algorithm Figure 11 terminates time polynomial
||||, ||AS||, log 1 , unary representation budget b .
Lemma 14 Given OSP task = hV, s0 , u; O, c, bi, homomorphic explicit abstraction
skeleton = {(G1 , 1 ), . . . , (Gk , k )} , arbitrary value partition u
AS, > 0, algorithm Figure 11 computes (u) .
proof Lemma 13 similar proof Lemma 8, strict Knapsack
separation problems replaced strict MC-Knapsack separation problems.
proof Lemma 14 also similar proof Lemma 9, mutatis mutandis. Together,
Lemmas 14 13 establish Theorem 12.

5. Landmarks OSP
addition state-space abstractions, family approximation techniques
found extremely effective context optimal classical planning based notion
logical landmarks goal reachability (Karpas & Domshlak, 2009; Helmert & Domshlak,
2009; Domshlak et al., 2012; Bonet & Helmert, 2010; Pommerening & Helmert, 2013).
section proceed examining prospects reachability landmarks
heuristic-search OSP planning.
127

fiDomshlak & Mirkis

5.1 Landmarks Classical Planning
state classical planning task , landmark property operator sequences
satisfied s-plans (Hoffmann, Porteous, & Sebastia, 2004). instance,
fact landmark state assignment single variable true point
every s-plan. state-of-the-art admissible heuristics classical planning use
called disjunctive action landmarks, corresponding set operators
every s-plan contains least one operator set (Karpas & Domshlak, 2009;
Helmert & Domshlak, 2009; Bonet & Helmert, 2010; Pommerening & Helmert, 2013).
follows consider popular notion landmarks, simply refer disjunctive
action landmarks state s-landmarks. ease presentation, discussion take place context landmarks initial state task,
simply referred landmarks (for ).
Deciding whether operator set L landmark classical planning task
PSPACE-hard (Porteous, Sebastia, & Hoffmann, 2001). Therefore, landmark heuristics
employ landmark discovery methods polynomial-time sound, incomplete.
follows assume access procedure; actual way landmarks
discovered tangential contribution.
landmark cost
P set L s-landmarks,

0+
function lcost : L R admissible
lcost(L) h (s). singleton set
L = {L}, lcost(L) := minoL c(o) natural admissible landmark cost function,
extends directly non-singleton sets pairwise disjoint landmarks. general sets
landmarks, lcost devised polynomial time via operator cost partitioning (Katz
& Domshlak, 2010b), either given L (Karpas & Domshlak, 2009), within actual
process generating L (Helmert & Domshlak, 2009).
5.2 -Landmarks Budget Reduction
landmarks play important role (both satisficing optimal) classical planning,
far exploited OSP. first glance, probably surprise,
OSP investigated much less classical planning: Since
landmarks must satisfied plans empty operator sequence always
plan OSP task, notion landmark seem useful here. said
that, consider anytime output improvement property BFBB forward search.
empty plan interesting useless, also
found BFBB right beginning. general, stages search,
anytime search algorithms like BFBB maintain best-so-far solution , prune
branches promise value lower equal Qb (). Hence, principle, algorithms
may benefit information properties satisfied plans value
larger Qb (). Polynomial-time discovery value landmarks arbitrary OSP
tasks still open problem. However, looking needed available,
show classical planning machinery reachability landmarks actually
effectively exploited OSP.
P follows, assume value function additive, u(s) =
hv/dis uv (d), uv (d) 0 variable-value pairs hv/di. is, value state
sum (mutually independent) non-negative marginal values propositions
comprising s. value different s-plans OSP task varying zero
128

fiOn Oversubscription Planning Heuristic Search

value optimal s-plan (which may also zero), let -landmark state
property satisfied s-plan achieves something valuable.
instance, disjunctive action landmarks use here, L -landmark
s, every s-plan Qb () > 0 contains operator L. follows, unless
stated otherwise, focus -landmarks (the initial state of) .
Definition 5 Given OSP task = hV, s0 , u; O, c, bi, -compilation classical planning task = hV , s0 , G ; , c
V = V {g},
dom(g) = {0, 1},
s0 = s0 {hg/0i},
G = {hg/1i},


= Og = ohv/di | hv/di D, uv (d) > 0 ,
pre(ohv/di ) = {hv/di} eff(ohv/di ) = {hg/1i},
(
c(), =
c (o) =
.
0,
= ohv/di Og
put simply, semantics value hg/1i auxiliary variable g
verified proposition positive value achieved.
terms, simply extends structure set zero-cost actions
applying corresponds verifying positive value achieved
. Constructing trivially polynomial time, allows us discover
-landmarks using standard machinery classical planning landmark discovery.
Theorem 15 OSP task , landmark L L landmark .
Proof: proof rather straightforward. Let P set plans
Qb () > 0 P set plans . definition P, plan P,
exists proposition hv/di uv (d) > 0 hv/di s0 JK. Likewise, since
s0
:= s0ff {hg/0i} O, applicable

s0 . ffHence, definition


ff ohv/di ,
ohv/di applicable s0
hg/1i

J

K,

is,


0
hv/di
hv/di P . turn,
ff
L landmark , ohv/di contains operator L, L O,
contains operator L well. proves landmarks L
operators -landmarks .

Theorem 15 hand, derive -landmarks using method
classical planning landmark extraction, employed LAMA planner (Richter et al., 2008) LM-Cut family techniques (Helmert & Domshlak, 2009;
Bonet & Helmert, 2010). However, first glance, discriminative power knowing
needed achieve something valuable seems negligible comes deriving effective heuristic estimates OSP. good news that, OSP, information
effectively exploited slightly different way.
129

fiDomshlak & Mirkis

Consider schematic example searching optimal plan OPS task
budget b, using BFBB admissible heuristic h. Suppose one
sequence (all unit-cost) operators, = ho1 , o2 , . . . , ob+1 i, applicable initial state
, positive value state along end-state. clearly
value higher zero achieved given budget b, search
continue beyond initial state, unless h(s0 , ) counts cost b + 1 operators
. Now, suppose h(s0 , ) counts cost {oi , . . . , ob+1 } > 0,
{o1 }, {o2 }, . . . , {oi1 } discovered -landmarks . Given that, suppose
modify (a) setting cost operators o1 , o2 , . . . , oi1 zero, (b) reducing
budget b + 1. Since operators o1 , o2 , . . . , oi1 applied anyway
along value collecting plan , modification seems preserve semantics
. time, modified task, BFBB heuristic h prune
initial state thus establish without search empty plan optimal plan
. course, way modified example simplistic example itself.
Yet, example motivate idea landmark-based budget reduction OSP,
well illustrates basic idea behind generically sound task modifications
discuss next.

Definition 6 Let = hV, s0 , u; O, c, bi OSP task, L = {L1 , . . . , Ln } set
pairwise disjoint -landmarks , lcost admissible landmark cost function
L. budget reducing compilation OSP task L = hVL , s0L , uL ; OL , cL , bL

n
X
bL = b
lcost(Li )
(14)
i=1


VL = V {vL1 , . . . , vLn }
dom(vLi ) = {0, 1},
s0L = s0 {hvL1 /1i , . . . , hvLn /1i},
uL = u,
OL =

n
[
i=1

OLi =

n
[

{o | Li },

i=1

pre(o) = pre(o) {hvLi /1i} eff(o) = eff(o) {hvLi /0i},
(
c(o),
=oO
cL () =
.
c(o) lcost(Li ), = OLi
words, L extends structure
mirroring operators -landmark Li cheaper lcost(Li ) versions,
130

fiOn Oversubscription Planning Heuristic Search

CBB

o2
o2

o2 o2
ABB

o1
o1

o3
BTB
o6
o3
BBB
o7
o4
BBT
o4

CTB

o4
o7
o4
BTT
o3
o3
o6
o2
CBT
o2

u=1

o5
o9
o5

u=1

CCB

o8
o10
o8
o5
CTT
o9
o5
u=1

o2
o2
o8
o10
o8

CTC

u=1
CCT

o5
u=2
o9
o5
o8
CCC
o10
o8

CBC

(a)
u=1

CBB
CTB

o2
ABB

o1

BBB

BTB
o6
o3
o7
o4
BBT

o2
o7
o4
BTT

o3
o6
o2

o9
o5

o2

CBB

o10
o8
CTT

u=1
CBT

o10
o8

u=1

CCB

o9
o5

CTC

o2 o2

o9 u=2

o5
u=1
CCT

CCC

ABB

o10
o8

CBC

o1
o1

o3
BTB
o6
o3
BBB
o7
o4
BBT
o4

o2
o2

CTB

o4
o7
o4
BTT
o3
o3
o6
o2
CBT
o2

o5
o9
o5

o2
o2
o8
o10
o8

u=1
CCB

o8
o10
o8
o5
CTT
o9
o5
u=1

u=1
CTC

u=1
CCT

o5
u=2
o9
o5
o8
CCC
o10
o8

CBC

Figure 12: Illustrations example landmark-based budget reducing compilation
L : (a) structurally reachable parts graphical skeleton model
induced L , illustrated projection L variables original
task , along comparison budget-wise reachable parts
graphical skeletons induced models (b) original task (c)
compiled task L .

using disposable propositions hvL1 /1i , . . . , hvLn /1i ensure one
instance discounted operators Li applied along operator
sequence initial state12 ,
compensating discounted operators Li reducing budget precisely
lcost(Li ).
example, consider simple OSP task Figure 2 (p. 104) cost budget
b = 4, assume provided set four landmarks L = {L1 , . . . , L4 }
L1 = {o1 }, L2 = {o2 }, L3 = {o3 , o4 } L4 = {o5 , o8 }, admissible landmark cost
function lcost(Li ) = 1 [4]. Compiling (L, lcost) using budget reducing
compilation Definition
6 results task L budget bL = 0 c(o) = 0
discounted operators ni=1 OLi = {o1 , o2 , o3 , o4 , o5 , o8 }.
states correspond complete assignments three variables V =
{t, x, y}, L already seven variables VL = {t, x, y, vL1 , vL2 , vL3 , vL4 }. Thus, depicting
12. Note that, auxiliary variable g -compilation effectively change value
hg/0i hg/1i, auxiliary variables vLi L change values (only) hvLi /1i hvLi /0i.
difference reflects positive semantics usually associated value 1, aka value
true, planning propositions: semantics state L containing proposition hvLi /1i
still allowed apply (one the) discounted operators associated landmark Li
onwards.

131

fiDomshlak & Mirkis

compile-and-BFBB ( = hV, s0 , u; O, c, bi)
:= -compilation
L := set landmarks
lcost := admissible landmark cost function L
L := budget reducing compilation (L, lcost)
n := BFBB(L )
return plan associated n
Figure 13: BFBB search landmark-based budget reduction
structurally reachable parts graphical skeleton GML problematic. Still,
illustrate search space , Figure 12(a) show (structurally reachable parts
the) graphical skeleton model induced projection variables
{t, x, y} only. arcs corresponding discounted operators colored, color
distinguishing landmark responsible respective discounted operators.
Figures 12(b) 12(c) illustrate effect budget-reducing compilation depicting parts graphical skeletons GM GML actually reachable
respective cost budgets b = 4 bL = 0: states BTT CTT
reachable initial state budget allowance 4, states corresponding BTT CTT longer reachable L , reducing size search space
BFBB. time, formulated Theorem 16 below, reduction
search space affect plans lead valuable states, resulting effective
equivalence L .
Theorem 16 Let = hV, s0 , u; O, c, bi OSP task, L set pairwise disjoint
-landmarks , lcost admissible landmark cost function L, L
respective budget reducing compilation . every Qb () > 0,
plan L L QbL (L ) = Qb (), vice versa.
proof Theorem 16 appears Appendix A, p. 149. budget reducing OSPto-OSP compilation Definition 6 clearly polynomial time. compile-and-BFBB
procedure, depicted Figure 13,
(1) generates -compilation ;
(2) uses off-the-shelf tools classical planning generate set landmarks L
admissible landmark cost function lcost;
(3) compiles (L, lcost) , obtaining OSP task L .
optimal solution L (and thus ) searched using search algorithm
optimal OSP BFBB.
proceed consider general sets landmarks, comments concerning setup Theorem 16 order. First, reduced budget bL turns
lower cost cheapest action applicable initial state, obviously
search needed, empty plan reported optimal right away. Second,
132

fiOn Oversubscription Planning Heuristic Search

zero-cost landmarks useless compilation much useless deriving
landmark heuristics optimal planning. Hence, lcost follows assumed
strictly positive. Third, applicable state brings benefits
yet adds branching search. Hence, implementation, landmark Li L
operator Li , precondition regular operators OL extended
{hvLi /0i}. hard verify extension preserves correctness
L terms Theorem 16. Finally, value initial state zero, is,
empty plan positive value, -compilation positive
cost landmarks all. However, easily fixed considering valuable
propositions hv/di uv (d) > 0 hv/di 6 s0 . ignore time
problem non-zero-value initial states (and assume Qb () = 0), return
later systematic discussion.
5.3 Non-Disjoint -Landmarks
budget reducing compilation L sound pairwise disjoint landmarks,
general sets -landmarks. example, consider planning task
which, operator o, c(o) = b, Qb (hoi) > 0, Qb () = 0
operator sequences 6= hoi. is, value greater zero achievable ,
via operator o. Suppose set -landmarks L = {L1 , . . . , Ln },
n > 1, lcost(Li ) > 0 [n],
Pnthat -landmarks contain o.
case, budget L bL = b i=1 lcost(Li ), cost cheapest replica
o, is, cost cheapest operator sequence achieving non-zero value ,
n

n

i=1

i=1

c(o) max lcost(Li ) = b max lcost(Li ) > b

n
X

lcost(Li ) = bL .

i=1

Hence, state positive value reachable s0L L , thus L
value equivalent sense Theorem 16.
example shows compiling non-disjoint -landmarks independently
sound. principle, made sound follows. Let = hV, s0 , u; O, c, bi
OSP task, let L = {L1 , . . . , Ln } set -landmarks , let lcost admissible
landmark cost function L. components L = hVL , s0L , uL ; OL , cL , bL
still defined Definition 6, except operator sets OL1 , . . . , OLn . latter
constructed independently other, sequentially, content
OLi depending content OLj , j < i. ordering sets OLi
constructed arbitrary.
operator 1 n, let Oo;i denote set cost
discounted representatives introduced
construction OL1 , . . . , OLi .

1 n, operator o0 Li Oo0 ;i1 cL (o) = 0, OLi := .
Otherwise, OLi contains operator operator
Li

[
o0 Li

133

Oo0 ;i1 ,

(15)

fiDomshlak & Mirkis

defined similarly Definition 6 as:
pre(o) = pre(o) {hvLi /1i},
eff(o) = eff(o) {hvLi /0i},
(
c(o) lcost(Li ),
cL (o) =
cL (o) lcost(Li ),

Li ,
.

o0 Li Oo0 ;i1

(16)

compilation extended way sound arbitrary sets -landmarks,
pairwise disjoint landmarks reduces basic compilation used Theorem 16.
general, however, extended compilation longer polynomial size
explicit representation
|Oo;i | = 2|{Lj |ji,oLj }| .
example, let L = {L1 , L2 , L3 }, L1 = {a, b}, L2 = {a, c}, L3 = {a, d}. Generation
OL1 := {a1 , b1 } effectively follows Definition 6, OL2 , base set operators
Eq. 15 already {a, c, a1 }. Thus, OL2 := {a2 , c1 , a3 }, where, {2, 3} denoting
a0 , ai derived according Eq. 16 ai2 . Consequently, base set operators
OL3 {a, d, a1 , a2 , a3 }, resulting OL3 = {a4 , d1 , a5 , a6 , a7 }, where, {4, 5, 6, 7},
ai derived ai4 . sum, L ends 8 = 2|L| representatives operator a.
Since non-disjoint landmarks bring information, typical outputs
standard techniques landmark extraction classical planning, present
different, slightly involved, compilation polynomial sound arbitrary
sets -landmarks.
Definition 7 Let = hV, s0 , u; O, c, bi OSP task, L = {L1 , . . . , Ln } set
pairwise disjoint -landmarks , lcost admissible landmark cost function
L. operator o, let L(o) denote set landmarks L contain
o. Then, generalized budget reducing compilation OSP task L =
hVL , s0L , uL ; OL , cL , bL
bL = b

n
X

lcost(Li ),

i=1

VL = V {vL1 , . . . , vLn }
dom(vLi ) = {0, 1},
s0L = s0 {hvL1 /1i , . . . , hvLn /1i},
uL = u,
OL = {o | L} {get(L) | L L}

pre(o) = pre(o) {hvL /1i | L L(o)},
eff(o) = eff(o) {hvL /0i | L L(o)},

(17)


pre(get(L)) = {hvL /0i},
eff(get(L)) = {hvL /1i},
134

(18)

fiOn Oversubscription Planning Heuristic Search




c(o), P
cL () = c(o) LL(o) lcost(L),


lcost(L),

=oO
.
=o

(19)

= get(L)

illustrate compilation, let L = {L1 , L2 , L3 },
L1 = {a, b},
L2 = {b, c},
L3 = {a, c},
operators cost 2, let
lcost(L1 ) = lcost(L2 ) = lcost(L3 ) = 1.
L , VL = V {vL1 , vL2 , vL3 }
OL = {a, b, c, get(L1 ), get(L2 ), get(L3 )},
with, e.g.,
pre(a) = pre(a) {hvL1 /1i , hvL3 /1i},
eff(a) = eff(a) {hvL1 /0i , hvL3 /0i},
cL (a) = 0,
and, get(L1 ),
pre(get(L1 )) = {hvL /0i},
eff(get(L1 )) = {hvL /1i},
cL (get(L1 )) = 1.
intuition behind compilation Definition 7 follows. Eq. 19, applying
discounted operator saves total cost landmarks containing o. Therefore,
executed states corresponding control propositions
{hvL /1i | L L(o)} hold, indicating cost landmark L(o) already
saved reaching s,
avoid double savings around L(o), applying turns control propositions sJoK.
However, considering example above, suppose optimal plan original
task contains instance operator a, followed instance operator b,
instance operator c. Applying instead would block us applying b instead
b, thus value optimal plan compilation lower Qb ().
rescue comes get(L) actions allow selective spending
individual landmark costs lcost(L). example, applying saves cost
landmarks L1 L3 , applying get(L1 ) spend lcost(L1 ) safely set
135

fiDomshlak & Mirkis

control proposition hvL1 /1i. turn, enable b applied next steps,
applying b save cost L2 re-save cost L1 . way,
compilation leads equivalence L , formulated Theorem 17
proven Appendix A, p. 149.
Theorem 17 Let = hV, s0 , u; O, c, bi OSP task, let L = {L1 , . . . , Ln } set
-landmarks , let lcost admissible landmark cost function L, let L
(generalized) budget reducing compilation . every Qb () > 0,
plan L L QbL (L ) = Qb (), vice versa.
5.4 -Landmarks & Incremental BFBB
discussed earlier, value initial state zero, empty plan
positive value, thus -compilation Definition 5
landmarks positive cost. passing noted small problem remedied
considering valuable facts hv/di uv (d) > 0 hv/di 6 s0 .
consider aspect OSP closely, show discovery -landmarks
incremental revelation plans BFBB combined mutually stratifying
way.
Let = hV, s0 , u; O, c, bi OSP task interest, suppose given set
plans 1 , . . . , n . so, longer interested searching plans
achieve something, searching plans achieve something beyond
1 , . . . , n already achieve. Specifically, let si = s0 Ji K end-state ,
set propositions D, let goods(s) set propositions hv/di
uv (d) > 0. new plan end-state achieves something beyond 1 , . . . , n
already achieve, then, 1 n,
goods(s) \ goods(si ) 6= .
put observation work.
Definition 8 Given OSP task = hV, s0 , u; O, c, bi set reference states Sref =
{s1 , . . . , sn } , (, Sref )-compilation classical planning task (,Sref ) =
hV , s0 , G ; , c
V = V {x1 , . . . , xn , search, collect},
dom(xi ) = dom(search) = dom(collect) = {0, 1},
s0 = s0 {hsearch/1i , hcollect/0i , hx1 /0i , . . . , hxn /0i},
G = {hx1 /1i , . . . , hxn /1i},
n
[
=
Oi {f inish},
i=1


136

fiOn Oversubscription Planning Heuristic Search

= {o | O},
pre(o) = pre(o) {hsearch/1i},
eff(o) = eff(o),
c (o) = c(o).

pre(f inish) = ,
eff(f inish) = {hcollect/1i , hsearch/0i},
c (f inish) = 0.
Oi = {oi,g | si Sref , g goods(D) \ si },
pre(oi,g ) = {g, hcollect/1i},
eff(oi,g ) = {hxi /1i},
c (oi,g ) = 0.
Note
goal G cannot achieved without applying f inish operator;
regular operators applied f inish;
subgoal achieving operators oi,g applied f inish.
way, first part plan (,Sref ) determines plan , second part
verifies end-state plan achieves subset value-carrying propositions
goods(D) included state Sref .13
Theorem 18 Let = hV, s0 , u; O, c, bi OSP task, Sref = {s1 , . . . , sn } subset
states, L landmark (,Sref ) L O. plan
goods(s0 JK) \ goods(si ) 6= si Sref , contains instance least one
operator L0 = {o | L}.
Proof: Assume contrary exists plan = ho1 , . . . , ok
goods(s0 JK) \ goods(si ) 6= si Sref , yet L0 = . Let {g1 , . . . , gn }
arbitrary set propositions goods(s0 JK) \ goods(s1 ), . . . , goods(s0 JK) \ goods(sn ),
respectively. construction (,Sref ) , immediate
(,Sref ) = ho1 , . . . , ok , f inish, o1,g1 , . . . , on,gn
plan (,Sref ) and, assumption L0 , holds (,Sref ) L = .
This, however, contradicts L landmark (,Sref ) .

13. solve & verify technique appears helpful many planning formalism compilations; see,
e.g., work Keyder Geffner (2009).

137

fiDomshlak & Mirkis

inc-compile-and-BFBB ( = hV, O; s0 , c, u, bi)
initialize global variables:
n := s0
// best solution far
Sref := {s0 } // current reference states
loop:
(,Sref ) = (, Sref )-compilation
L := set landmarks (,Sref )
lcost := admissible landmark cost function L
L := budget reducing compilation (L, lcost)
inc-BFBB(L , Sref , n ) = done:
return plan associated n
inc-BFBB (, Sref , n )
open := new max-heap ordered f (n) = h(shni, b g(n))
open.insert(make-root-node(s0 ))
closed:=
best-cost:= 0
open.empty()
n := open.pop-max()
f (n) u(shn i): break
u(shni) > u(shn i): update n := n
goods(shni) 6 goods(s0 ) s0 Sref :
Sref := Sref {shni}
termination criterion: return updated
// rest similar BFBB Figure 3

shni 6 closed g(n) < best-cost(shni):
closed:= closed {shni}
best-cost(shni) := g(n)
foreach O(shni):
n0 := make-node(shniJoK, n)
g(n0 ) > b f (n0 ) u(shn i): continue
open.insert(n0 )
return done
Figure 14: Iterative BFBB landmark enhancement
Theorem 18 allows us define iterative version BFBB, inc-compile-and-BFBB,
depicted Figure 14. successive iterations inc-compile-and-BFBB correspond
running regular BFBB successively informed (, Sref )-compilations ,
states discovered iteration making (, Sref )-compilation used iteration + 1
informed.
inc-compile-and-BFBB maintains pair global variables: set reference states
Sref best solution far n . iteration loop, modified version
BFBB, inc-BFBB, called (, Sref )-compilation , created basis
138

fiOn Oversubscription Planning Heuristic Search

current Sref . reference set Sref extended inc-BFBB non-redundant
value-carrying states discovered search, n updated search discovers
nodes higher value.
OPEN list becomes empty node n selected list promises
less lower bound, inc-BFBB returns indicator, done, best solution
n found far, across iterations inc-compile-and-BFBB, optimal. case,
inc-compile-and-BFBB leaves loop extracts optimal plan n . However,
inc-BFBB may also terminate different way, certain complementary termination
criterion satisfied. latter criterion comes assess whether updates Sref
performed current session inc-BFBB warrant updating (, Sref )-compilation
restarting search. terminated way, inc-BFBB returns respective indicator,
inc-compile-and-BFBB goes another iteration loop, updated Sref
n . note that, optimality algorithm holds termination
condition, latter greatly affect runtime efficiency algorithm.
Theorem 19 inc-compile-and-BFBB search algorithm sound complete optimal OSP.
Proof: First, complementary termination criterion employed inc-BFBB procedure, inc-compile-and-BFBB guaranteed terminate. complementary termination criterion checked inc-BFBB proper expansion
global reference set Sref , thus number calls inc-BFBB inc-compile-and-BFBB
upper-bounded |S|.
terms search, inc-BFBB different regular BFBB procedure. turn,
Theorem 18, additional pruning power budget-reducing compilation reference
states Sref affects search nodes n u(shni) < maxsSref u(s). Note also that,
time best solution far n updated inc-BFBB, necessarily added Sref
(since goods(shn i) new n included goods(s) Sref ). Thus, optimal
solutions cannot pruned inc-BFBB overall search inc-compile-and-BFBB
therefore sound.

5.5 Empirical Evaluation
evaluate merits landmark-based budget reducing compilation, extended prototype OSP solver Section 3 following components:
(, Sref )-compilation OSP tasks arbitrary sets reference states Sref ;
generation disjunctive action landmarks (, Sref )-compilations using LM-Cut
procedure (Helmert & Domshlak, 2009) Fast Downward;
incremental BFBB procedure inc-compile-and-BFBB Figure 14,
search termination criterion satisfied (only) examined node n improves
current value lower bound, i.e., n becomes new best-so-far node n .
preliminary evaluation, also added two optimality preserving enhancements search. auxiliary variables compilations increase dimensionality problem, known negatively affect quality abstraction
139

fiDomshlak & Mirkis

(a) blind
108

unsolved

107
106
105
104
103
102

unsolved

compile-and-BFBB

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

101

100
100 101 102 103 104 105 106 107 108
BFBB

(b) hM
108

unsolved

106
105
104
103
102

unsolved

compile-and-BFBB

107

101

100
100 101 102 103 104 105 106 107 108
BFBB

Figure 15: Comparative view empirical results terms expanded nodes, BFBB
vs. compile-and-BFBB, (a) blind (b) abstraction hM heuristics

heuristics (Domshlak et al., 2012), first devised projections respect original OSP problem , open list ordered search done original
problem, is,


X
h shniV , b g(n) +
lcost(L) ,
vL 6shni

sV projection L state variables original OSP task .
change heuristic evaluation sound, Theorem 17 particular implies
140

fiOn Oversubscription Planning Heuristic Search

admissible heuristic also admissible heuristic L , vice versa. Second,
new node n generated, check whether
X
X
lcost(L),
lcost(L) g(n0 ) +
g(n) +
L:hvL /0ishn0

L:hvL /0ishni

previously generated node n0 corresponds state original
problem , is, shn0 iV = shniV . so, n pruned right away. Optimality
preservation enhancement established Lemma 20 proven Appendix A,
p. 151.
Lemma 20 Let OSP task, (,Sref ) (, Sref )-compilation , L set
landmarks (,Sref ) , lcost admissible landmark cost function L, L
respective budget reducing compilation (L, lcost) . Let 1 2 pair plans
V
L end-states s1 s2 , respectively, sV
1 = s2
cL (1 ) +

X

lcost(L) cL (2 ) +

L:hvL /0is1

X

lcost(L).

(20)

L:hvL /0is2

Then, plan 10 extends 1 , exists plan 20 extends 2
= QbL (10 ).

QbL (20 )

evaluation included regular BFBB planning , solving using landmarkbased compilation via compile-and-BFBB, simple setting inc-compile-and-BFBB
described above. three approaches evaluated blind heuristic
additive abstraction heuristic hM described Section 3. Figures 15-17 depict results
evaluation terms expanded nodes. Similarly experiment reported
Section 3, task approached four different budgets, corresponding 25%,
50%, 75%, 100% minimal cost needed achieve goals task,
run restricted 10 minutes. Figures 15a 15b compare number
expanded nodes BFBB compile-and-BFBB across four levels cost budget,
blind (a) abstraction hM (b) heuristics. Figures 16a 16b provide similar
comparison BFBB inc-compile-and-BFBB. Figures 17a 17b
compile-and-BFBB inc-compile-and-BFBB.14 Figures 22-25 Figures 26-29
Appendix B provide detailed view results Figures 15 16, respectively,
breaking different levels cost budget.
Figure 8 shows, results satisfactory. informative heuristic
guidance all, number nodes expanded compile-and-BFBB typically much
lower number nodes expanded BFBB, difference reaching three
orders magnitude once. 760 task/budget pairs behind Figure 8a, 81
pairs solved compile-and-BFBB search (by proving plan
achieve value higher initial state), while, unsurprisingly, 4
tasks solved search BFBB.
14. present detailed comparison terms running times, per-node CPU
time overhead due landmark-based budget reduction 10%. technical difficulties
implementation inc-compile-and-BFBB led us limit comparison graph tasks
solved methods.

141

fiDomshlak & Mirkis

(a) blind
108
107
inc-compile-and-BFBB

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

106
105
104
103
102
101
100
100 101 102 103 104 105 106 107 108
BFBB

(b) hM
108
inc-compile-and-BFBB

107
106
105
104
103
102
101
100
100 101 102 103 104 105 106 107 108
BFBB

Figure 16: Comparative view empirical results terms expanded nodes, BFBB
vs. inc-compile-and-BFBB, (a) blind (b) abstraction hM heuristics

expected, impact landmark-based budget reduction lower
search equipped meaningful heuristic (Figure 15b). Nonetheless, even
abstraction heuristic hand, number nodes expanded compile-and-BFBB
often substantially lower number nodes expanded BFBB. Here, BFBB
compile-and-BFBB solved search 39 85 task/budget pairs, respectively. Finally, despite rather ad hoc setting incremental inc-compile-and-BFBB procedure,
switching compile-and-BFBB inc-compile-and-BFBB typically beneficial. Obvi142

fiOn Oversubscription Planning Heuristic Search

(a) blind
108
107
inc-compile-and-BFBB

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

106
105
104
103
102
101
100 0
10 101 102 103 104 105 106 107 108
compile-and-BFBB

(b) hM
108
inc-compile-and-BFBB

107
106
105
104
103
102
101
100 0
10 101 102 103 104 105 106 107 108
compile-and-BFBB

Figure 17: Comparative view empirical results terms expanded nodes,
compile-and-BFBB vs. inc-compile-and-BFBB, (a) blind (b) abstraction hM heuristics

ously, much deeper investigation development inc-compile-and-BFBB still required,
especially context choice iteration termination criterion.

6. Summary Future Work
Deterministic oversubscription planning captures computational core one
important setups automated action selection, yet, despite apparent importance
143

fiDomshlak & Mirkis

problem, sufficiently investigated. work, progressed towards
translating spectacular advances classical deterministic planning deterministic
OSP. Tracing key sources progress classical planning, identified severe lack
effective approximations OSP, worked towards bridging gap.
focus two classes approximation techniques underly state-ofthe-art optimal heuristic-search solvers classical planning: state-space abstractions
goal-reachability landmarks. First, defined notion additive abstractions OSP,
studied complexity deriving effective abstractions rich space hypotheses,
revealed substantial, empirically relevant islands tractability abstraction
discovery problem. Next, showed standard goal-reachability landmarks certain
classical planning tasks compiled OSP task interest, resulting
equivalent OSP task lower cost allowance, thus sometimes dramatically
smaller search space.
techniques proposed satisfy properties required efficient search
algorithms optimal OSP. However, believe techniques, especially
landmark-based budget reducing compilations, beneficial satisficing OSP
optimal OSP, particular difference optimal satisficing
planning appears much smaller OSP classical deterministic planning.
Many interesting questions remain open future work, prospects
developments oversubscription planning appear quite promising. Within specific
context work, two interesting research directions (1) optimization value
partitions given cost partitions, is, optimizing abstraction discovery Ap (c, , ),
(2) thoroughly investigating interleaved landmark discovery search OSP
introduced Section 5.4. broader context, propose, well, additional candidates
future research:
Following work Katz Domshlak (2010a) implicit abstractions classical planning, computational merits implicit abstractions OSP
investigated. inevitably give us better understanding computational
tractability boundaries deterministic OSP.
basic model deterministic planning Section 2.1 used provide unifying
comparative view basic models classical, cost-bounded, net-benefit,
oversubscription planning. One practically motivated extension model lift
action costs vectors action costs. variant cost-bounded planning
already investigated (Nakhost et al., 2012), natural examine
extension context OSP.
Unfortunately, results abstractions seem extend directly vectors
costs: level planning model, adding cost measures shifts problem solving
polynomial time shortest path(s) problems NP-hard restricted shortest path(s)
problems (Handler & Zang, 1980). Nonetheless, like Knapsack problem, restricted shortest path problem solved pseudo-polynomial time (Desrochers
& Soumis, 1988), thus extension results vectors costs might still
achievable.
time, machinery landmark-based budget reducing compilations
OSP straightforwardly extends vectors costs budgets. Hence, even
144

fiOn Oversubscription Planning Heuristic Search

quality heuristic OSP multiple cost measures available, blind search
still stratified information coming problem landmarks.
pruning mechanism BFBB must rely admissible, upper-bounding
heuristic estimates, special properties required heuristic used guide
search choices BFBB. Thus, developing informative yet necessarily admissible
heuristics OSP clearly interest.
Acknowledgments
work partially supported EOARD grant FA8655-12-1-2096, ISF
grant 1045/12.

Appendix A. Proofs
Theorem 2 Given OSP task = hV, s0 , u; O, c, bi homomorphic abstraction
skeleton = {(G1 , 1 ), . . . , (Gk , k )} ,
(1) cost partition c Cp , exists budget partition b Bp

M(c,u,b ) value partitions u ;
(2) budget partition b Bp , exists cost partition c Cp

M(c ,u,b) value partitions u .
Proof: Let = h(s0 , o1 , s1 ), (s1 , o2 , s2 ), . . . , (sn1 , , sn )i optimal s0 -plan ,
and, [k], let = h(i (s0 ), o1 , (s1 )), . . . , (i (sn1 ), , (on ))i mapping
Gi . Since homomorphic, paths 1 , . . . , k well-defined.
(1) P
Given cost partition c Cp , let budget profile b B defined b [i] =

j[n] c[i](oj ), [k]. First, note b Bp since
X
i[k]

()

X X

b [i] =

c[i](oj )

i[k] j[n]

X

()

c(oj ) b,

j[n]

() c cost partition, () s0 -plan .
Second, u U, construction b , (s0 )-plan abstract
(c,u,b )
model Mi
. Now, let u , [k], let optimal (s0 )-plan
(c,u,b )
Mi
.
X
i[k]

Qb

[i]

()

(i )

X

[i]

Qb

()

(i ) Qb (),

(21)

i[k]

() optimality , () (sn ) end-state u
value partition. Therefore, (c, u, b ) induces additive abstraction ,

is, M(c,u,b ) AAS .
145

fiDomshlak & Mirkis

(2) Given budget partition b, let cost profile c C defined c [i](o) = c(o) b[i]
b ,
forPall operators O, [k]. First, c Cp since b Bp implies
1

i[k] b[i] [0, 1]. Second, u U, construction c , (s0 )b
(c ,u,b)

plan Mi
. Following exactly line reasoning Eq. 21

accomplishes proof M(c ,u,b) AAS u .

Lemma 6 algorithm Figure 9a computes (u).
Proof: Due boundness non-emptiness polytope induced Lm
1 , termination algorithm straightforward. Thus, given strong 0-binary partition u,
question whether value algorithm terminates (u). First, let
us show that:
() [k], x solution Lm
1 , x[] b if, cost partition
c Cp , exists budget partition b Bp (c, u, b) abstraction
hM(c,u,b) (s0 ) m.
() Assume contrary that, cost partition c Cp , exists budget partition b Bp hM(c,u,b)
(s0 ) m, yet x[] > b. Given values provided x cost variables oO {c[i](o)}, let c corresponding cost partition,
1 , . . . , k induced lengths shortest paths 1 (s0 ), . . . , k (s0 ) valued states G1 , . . . , Gk , respectively. assumption, let b budget partition
hM(c,u,b) (s0 ) m. First, definition strong 0-binary value partitions,
hM(c,u,b) (s0 ) implies exists Z k, |Z| = that, Z, b[i] .
Second, constraint (10c), maximization , fact bound b[i]
imply together that, Z, x[b[i]] = . Putting things together, obtain
bBp

b

X

b[i]

iZ

X

=

iZ

X

(10c)

x[b[i]] ,

iZ

contradicting assumption.
() Assume contrary that, x[] b, yet exists cost partition c Cp
that, budget partitions b Bp (c, u, b) Ap , hM(c,u,b) (s0 ) < m.
Let shortest path lengths 1 , . . . , k defined above, respect
specific cost partition c assumption.
Likewise, let xc solution Lm
1

extra constraint cost variables oO {c[i](o)} assigned c. Since objective
Lm
1 maximize value ,
x[] xc [].

(22)

Now, let
Z=

argmax

X

Z 0 [k],|Z 0 |=m iZ 0

146

.

fiOn Oversubscription Planning Heuristic Search

Together, constraint (10c), maximization , fact bound
b[i] (via cost variables) imply
xc [] =

X

xc [b[i]] =

iZ

X

.

(23)

iZ

turn, together x[] b Eq. 22, Eq. 23 implies
(
xc [b[i]],
b[i] =
0,

iZ
otherwise

budget partition (c, u, b) Ap , hM(c,u,b) (s0 ) m, contradicting assumption.
proved sub-claim (), basically captures semantics Lm
1 , suppose
algorithm terminates within loop, returns > 0.
construction algorithm, x solution Lm
1 , x[] b. (), cost
partition c Cp , exists (c, u, b) Ap h(c,u,b) (s) m. = k,
trivially (u) = m. Otherwise, < k, know algorithm terminate
previous iteration corresponding + 1. Again, () implies exists
cost partition c Cp (c, u, b) Ap induce h(c,u,b) (s) (m + 1). Hence,
definition (u), (u) < (m + 1), turn, since u strong 0-binary value
partition, (u) = m. Finally, algorithm terminates loop
returns 0, precisely argument basis () implies (u) = 0.

Lemma 9 0 < < mini[k] , algorithm Figure 10a computes
(u) .
Proof: arguments boundness non-emptiness polytope induced
Lv2 precisely polytope Lm
1 studied Lemma 6, thus
termination algorithm straightforward. follows, prove value
returned algorithm satisfies claim lemma. Let u given 0-binary
partition. Similarly proof Lemma 9, first prove sub-claim that:
() v R0+ , x solution Lv2 , x[] b if, cost partition
c Cp , exists budget partition b Bp (c, u, b) abstraction
hM(c,u,b) (s0 ) v.
proof () mirrors proof respective sub-claim Lemma 5, mutatis mutandis,
thus provided ease verification.
() Assume contrary that, cost partition c Cp , exists budget
partition b Bp hM(c,u,b) (s0 ) v, yet x[] > b.

Given values provided x cost variables oO {c[i](o)}, let c corresponding cost partition, and, [k], let induced length shortest
path (s0 ) -valued states Gi . assumption, let b budget
partition hM(c,u,b) (s0 ) v. First,
P definition 0-binary value partitions,
hM(c,u,b) (s0 ) v implies exists Z k, iZ v that, Z, b[i] .
147

fiDomshlak & Mirkis

Second, constraint (11c), maximization , fact bound b[i]
, imply together that, Z, x[b[i]] = . Putting things together, obtain
bBp

b

X

b[i]

iZ

X

=

iZ

X

(11c)

x[b[i]] ,

iZ

contradicting assumption.
() Assume contrary that, x[] b, yet exists cost partition c Cp
that, budget partitions b Bp (c, u, b) Ap , hM(c,u,b) (s0 ) < v.
Let shortest path lengths 1 , . . . , k defined above, respect
specific cost partition c assumption.
Likewise, let xc solution Lv2

extra constraint cost variables oO {c[i](o)} assigned c. Since objective
Lv2 maximize value ,
x[] xc [].

(24)

Now, let
Z = argmax

X

0
PZ [k], iZ 0
iZ 0 v

.

Together, constraint (11c), maximization , fact bound
b[i] (via cost variables) imply
xc [] =

X

xc [b[i]] =

iZ

X

.

(25)

iZ

turn, together x[] b Eq. 24, Eq. 25 implies
(
xc [b[i]], Z
b[i] =
,
0,
otherwise
budget partition (c, u, b) Ap , hM(c,u,b) (s0 ) v, contradicting assumption.
finalizes proof sub-claim (). Now,Pconsider interval end-points
termination
while-loop. =
i[k] , then, trivially, (u) .
P
Otherwise, < i[k] , then, construction algorithm, iteration
loop, test always-achievable() issued, came back negative, thus,
solutions x L2 , x [] > b. Hence, (), (u) < . Now, 6= 0, then,
construction algorithm, iteration loop, test always-achievable()
issued, came back positive, thus, solutions x L2 , x [] b.
Hence, (), (u) . Putting properties together while-loops
termination condition implies (u) = (u) . Finally, = 0,
< mini[k] implies < mini[k] . turn, since (u) corresponds sum values
states k models M(c,u,b) , (u) concluded implies = (u) = 0.

148

fiOn Oversubscription Planning Heuristic Search

Theorem 16 Let = hV, s0 , u; O, c, bi OSP task, L set pairwise disjoint
-landmarks , lcost admissible landmark cost function L, L
respective budget reducing compilation . every Qb () > 0,
plan L L QbL (L ) = Qb (), vice versa.
Proof: Let L plan
Snfor L , let operator sequence obtained replacing
operators i=1 OLi along L respective operators O.
definition
action set L Eq. 15, applicable s0 , s0 JK =
s0L JL K \ ni=1 dom(vLi ). Thus, Qb () = QbL (L ). Likewise, definition
action set L Eq. 15 fact operator OL achieves control
propositions {hvL1 /1i , . . . , hvLn /1i}, |OLi L | 1. that,

c() cL (L ) +

n
X

lcost(Li ).

i=1

P
turn, b = bL + ni=1 lcost(Li ) Eq. 14, cL (L ) bL virtue L
plan L . Therefore, holds c() b, thus plan .
opposite direction, let plan Qb () > 0, let L
operator sequence obtained replacing, -landmark L L, every first occurrence
operator L respective cost reduced operator OL . easy
verify L applicable s0L , QbL (L ) = Qb (). Likewise, definition
-landmarks, every L L presence along . that,

c(L ) = c()

n
X

lcost(Li ) b

i=1

n
X

lcost(Li ) = bL ,

i=1

first equality pairwise disjointness {L1 , . . . , Ln }, inequality
plan , second equality Eq. 14. Thus, L plan L .


Theorem 17 Let = hV, s0 , u; O, c, bi OSP task, let L = {L1 , . . . , Ln } set
-landmarks , let lcost admissible landmark cost function L, let L
(generalized) budget reducing compilation . every Qb () > 0,
plan L L QbL (L ) = Qb (), vice versa.
Proof: Let L plan L , let operator sequence obtained (i) replacing
operators respective operators O, (ii) removal get operators.
Eq. 17, applicable s0 , s0 JK = s0L JL K \ {hvL1 /1i , . . . , hvLn /1i}. Thus,
Qb () = QbL (L ). Now, -landmark L L, let (L) number instances
cost reduced counterparts operators L along L . Eqs. 17 18,
L L, L must contain least (L) 1 instances operator get(L). that,

149

fiDomshlak & Mirkis

X

c() cL (L ) +

X

lcost(L)

oL LL(o)

= cL (L ) +

X
X

((L) 1)lcost(L)



(L)lcost(L)



= cL (L ) +

X

X

((L) 1)lcost(L)



lcost(L)



bL +

X

lcost(L)



= b,
thus plan .
opposite direction, let = ho1 , . . . , om plan Qb () > 0.
definition -landmarks, every landmark Li L presence along . Let
(i) , f (i) [n], first occurrence
ffoperator Li along , is, f (i) =


let = o(1) , . . . , o(k) , k n, operator sequence obtained
argminj[m] {oj Li },
ordering operators i[n] {of (i) } consistently . Note that, since -landmarks
L necessarily disjoint, may f (i) = f (j) 1 6= j n, thus
k strictly smaller n.
Given above, let L operator sequence obtained based
(1) replacing o(i) along o(i) ,
(2) inserting right o(i) arbitrary ordered sequence actions
i1
[

{get(L) | L L, {o(j) , o(i) } L}.

(26)

j=1

Note set union semantics Eq. 26: even multiple operators {o(1) , . . . , o(i1) }
appear landmark L together o(i) , one instance operator get(L)
inserted step (2) o(i) .
hard verify L applicable s0L , QbL (L ) = Qb (). Now,
step (1) expanding L reduces cost operator sequence
k
X

X

lcost(L) =

i=1 LL(o(i) )

X

(L)lcost(L),



(L) number occurrences operators fromP
L . turn, step (2)
expanding L increases cost operator sequence ((L) 1)lcost(L).
because, Eq. 26, among (L) operators o(i) along L o(i) L,
first preceded dedicated instances operator get(L). Thus,
X
X
cL (L ) = c()
lcost(L) b
lcost(L) = bL ,




is, L plan L .


150

fiOn Oversubscription Planning Heuristic Search

Lemma 20 Let OSP task, (,Sref ) (, Sref )-compilation , L set
landmarks (,Sref ) , lcost admissible landmark cost function L, L
respective budget reducing compilation (L, lcost) . Let 1 2 pair plans
V
L end-states s1 s2 , respectively, sV
1 = s2
cL (1 ) +

X

lcost(L) cL (2 ) +

Then, plan
b
L
Q (20 ) = QbL (10 ).

lcost(L).

(20)

L:hvL /0is2

L:hvL /0is1

10

X

extends 1 , exists plan 20 extends 2

Proof: notation claim, proof constructive mapping plan
10 corresponding plan 20 .
First, derive 10 plan 01 (i) removing f inish operator
get() operators, (ii) replacing instances discounted operator
instances respective original operator o. results plan 01 := 1 1e
P
s0 [[1 ]] = sV
1 c(1 ) = cL (1 ) +
L:hvL /0is1 lcost(L). see latter,

operator OL , let () 0 denote number instances along 1 . Given that,

c(1 ) = cL (1 )

X

(get(L))lcost(L) +



X

= cL (1 ) +

lcost(L)



= cL (1 ) +

X

X

X

(o)



o:LL(o)






X

lcost(L)

(o) (get(L))

o:LL(o)

(27)

lcost(L) 1s1 (hvL /0i)



= cL (1 ) +

X

lcost(L),

L:hvL /0is1

second fourth equalities formula manipulations, first equality
direct construction 1 , third equality definition budget
reducing compilation, specifically, Eqs. 17 18.
Similarly construction P
1 1 , construct 2 2 ,

s0 [[2 ]] = sV

c(
)
=
c
(
)
+
2
2
L
2
L:hvL /0is2 lcost(L). Thus, Eq. 20, c(1 ) c(2 ),
also, setting lemma, s0 [[1 ]] = s0 [[2 ]]. Hence, 02 = 2 1e also plan
, Qb (01 ) = Qb (02 ).
last step, construct 02 plan 20 L claim. First,
properties 2 claim, plan 2 achieves landmarks L6s2 = {L |
hvL /0i s2 }. Second, definition landmark set L, 1e must satisfy rest
landmarks, is, Ls2 = {L | hvL /1i s2 }. Let us denote operator instances along
1e ho1 , . . . , ok i, k = |1e |, let {L1 , . . . , Lk } partition Ls2 Li Ls2
subset landmarks Ls2 oi first achiever along 1e .
Given that, consider operator sequence 2e := (k) , recursively defined via (0) = ,
and, Li = , (i) = (i1) hoi i, else (i) = (i1) hoi i, (arbitrary)
151

fiDomshlak & Mirkis

sequencing operators
{get(L) | L Li hvL /0i s0 J2 KJ (i1) K}.
Finally, set 20 := 2 2e .
Eqs. 17 18 definition budget reducing compilation,
easy
P
verify construction 2e ensures cL (2e ) = c(1e ) hvL /1is2 lcost(L)
QbL (2e ) = Qb (02 ). turn, properties 2 , implies QbL (20 ) = QbL (10 )
cL (20 ) = cL (2 ) + cL (2e ).
Finally, since
X
lcost(L)
cL (2 ) = c(2 )
hvL /0is2


X

cL (2e ) = c(1e )

lcost(L),

hvL /1is2


cL (20 ) = c(2 ) + c(1e )

X

lcost(L).



Thus, since c(1 ) c(2 ) 01 = 1 1e valid plan ,
X
cL (20 ) c(1 ) + c(1e )
lcost(L)




c(01 )



X

lcost(L)



b

X

lcost(L),



finalizing proof 20 plan L claim.

152



fiOn Oversubscription Planning Heuristic Search

Appendix B. Detailed Evaluation Results
(a)
108

unsolved

107
106
105
hM

104
103
102

unsolved

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

101

100 0
10 101 102 103 104 105 106 107 108
blind

(b)
108

unsolved

107
106
hM

105
104
103
unsolved

102
101

100 0
10 101 102 103 104 105 106 107 108
basic

Figure 18: comparison Figure 8, p. 117, restricted tasks budgeted 25%
minimal cost achieving entire set subgoals

153

fiDomshlak & Mirkis

(a)
108

unsolved

107
106
105
hM

104
103
102

unsolved

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

101

100 0
10 101 102 103 104 105 106 107 108
blind

(b)
108

unsolved

107
106
hM

105
104
103
unsolved

102
101

100 0
10 101 102 103 104 105 106 107 108
basic

Figure 19: comparison Figure 8, p. 117, restricted tasks budgeted 50%
minimal cost achieving entire set subgoals

154

fiOn Oversubscription Planning Heuristic Search

(a)
108

unsolved

107
106
105
hM

104
103
102

unsolved

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

101

100 0
10 101 102 103 104 105 106 107 108
blind

(b)
108

unsolved

107
106
hM

105
104
103
unsolved

102
101

100 0
10 101 102 103 104 105 106 107 108
basic

Figure 20: comparison Figure 8, p. 117, restricted tasks budgeted 75%
minimal cost achieving entire set subgoals

155

fiDomshlak & Mirkis

(a)
108

unsolved

107
106
105
hM

104
103
102

unsolved

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

101

100 0
10 101 102 103 104 105 106 107 108
blind

(b)
108

unsolved

107
106
hM

105
104
103
unsolved

102
101

100 0
10 101 102 103 104 105 106 107 108
basic

Figure 21: comparison Figure 8, p. 117, restricted tasks budgeted 100%
minimal cost achieving entire set subgoals

156

fiOn Oversubscription Planning Heuristic Search

(a) blind
108

unsolved

107
compile-and-BFBB

106
105
104
103
102

unsolved

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

101

100
100 101 102 103 104 105 106 107 108
BFBB

(b) hM
108

unsolved

compile-and-BFBB

107
106
105
104
103
unsolved

102
101

100
100 101 102 103 104 105 106 107 108
BFBB

Figure 22: comparison Figure 15, p. 140, restricted tasks budgeted 25%
minimal cost achieving entire set subgoals

157

fiDomshlak & Mirkis

(a) blind
108

unsolved

107
compile-and-BFBB

106
105
104
103
102

unsolved

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

101

100
100 101 102 103 104 105 106 107 108
BFBB

(b) hM
108

unsolved

compile-and-BFBB

107
106
105
104
103
unsolved

102
101

100
100 101 102 103 104 105 106 107 108
BFBB

Figure 23: comparison Figure 15, p. 140, restricted tasks budgeted 50%
minimal cost achieving entire set subgoals

158

fiOn Oversubscription Planning Heuristic Search

(a) blind
108

unsolved

107
compile-and-BFBB

106
105
104
103
102

unsolved

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

101

100
100 101 102 103 104 105 106 107 108
BFBB

(b) hM
108

unsolved

compile-and-BFBB

107
106
105
104
103
unsolved

102
101

100
100 101 102 103 104 105 106 107 108
BFBB

Figure 24: comparison Figure 15, p. 140, restricted tasks budgeted 75%
minimal cost achieving entire set subgoals

159

fiDomshlak & Mirkis

(a) blind
108

unsolved

107
compile-and-BFBB

106
105
104
103
102

unsolved

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

101

100 0
10 101 102 103 104 105 106 107 108
BFBB

(b) hM
108

unsolved

compile-and-BFBB

107
106
105
104
103
unsolved

102
101

100 0
10 101 102 103 104 105 106 107 108
BFBB

Figure 25: comparison Figure 15, p. 140, restricted tasks budgeted 100%
minimal cost achieving entire set subgoals

160

fiOn Oversubscription Planning Heuristic Search

(a) blind
108
107
inc-compile-and-BFBB

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

106
105
104
103
102
101
100
100 101 102 103 104 105 106 107 108
BFBB

(b) hM
108

inc-compile-and-BFBB

107
106
105
104
103
102
101
100
100 101 102 103 104 105 106 107 108
BFBB

Figure 26: comparison Figure 16, p. 142, restricted tasks budgeted 25%
minimal cost achieving entire set subgoals

161

fiDomshlak & Mirkis

(a) blind
108
107
inc-compile-and-BFBB

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

106
105
104
103
102
101
100
100 101 102 103 104 105 106 107 108
BFBB

(b) hM
108

inc-compile-and-BFBB

107
106
105
104
103
102
101
100
100 101 102 103 104 105 106 107 108
BFBB

Figure 27: comparison Figure 16, p. 142, restricted tasks budgeted 50%
minimal cost achieving entire set subgoals

162

fiOn Oversubscription Planning Heuristic Search

(a) blind
108
107
inc-compile-and-BFBB

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

106
105
104
103
102
101
100
100 101 102 103 104 105 106 107 108
BFBB

(b) hM
108

inc-compile-and-BFBB

107
106
105
104
103
102
101
100
100 101 102 103 104 105 106 107 108
BFBB

Figure 28: comparison Figure 16, p. 142, restricted tasks budgeted 75%
minimal cost achieving entire set subgoals

163

fiDomshlak & Mirkis

(a) blind
108
107
inc-compile-and-BFBB

airport
blocks
depot
driverlog
freecell
grid
gripper
logistics
miconic
mystery
openstacks
pipesworld
psr-small
tpp
trucks
rovers
satellite
zenotravel

106
105
104
103
102
101
100 0
10 101 102 103 104 105 106 107 108
BFBB

(b) hM
108

inc-compile-and-BFBB

107
106
105
104
103
102
101
100 0
10 101 102 103 104 105 106 107 108
BFBB

Figure 29: comparison Figure 16, p. 142, restricted tasks budgeted 100%
minimal cost achieving entire set subgoals

164

fiOn Oversubscription Planning Heuristic Search

References
Backstrom, C., & Klein, I. (1991). Planning polynomial time: SAS-PUBS class.
Computational Intelligence, 7 (3), 181197.
Backstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. Computational
Intelligence, 11 (4), 625655.
Baier, J. A., Bacchus, F., & McIlraith, S. (2009). heuristic search approach planning
temporally extended preferences. Artificial Intelligence, 173 (5-6), 593618.
Benton, J., Coles, A. J., & Coles, A. I. (2012). Temporal planning preferences
time-dependent continuous costs. Proceedings 22nd International Conference
Automated Planning Scheduling (ICAPS), pp. 210.
Benton, J., Do, M., & Kambhampati, S. (2009). Anytime heuristic search partial satisfaction planning. Artificial Intelligence, 173 (5-6), 562592.
Benton, J., van den Briel, M., & Kambhampati, S. (2007). hybrid linear programming
relaxed plan heuristic partial satisfaction planning problems. Proceedings
Seventeenth International Conference Automated Planning Scheduling
(ICAPS), pp. 3441.
Bertsimas, D., & Vempala, S. (2004). Solving convex programs random walks. Journal
ACM, 51 (4), 540556.
Bonet, B. (2013). admissible heuristic SAS+ planning obtained state
equation. Proceedings 23rd International Joint Conference Artificial
Intelligence (IJCAI), pp. 22682274.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (1
2), 533.
Bonet, B., & Geffner, H. (2008). Heuristics planning penalties rewards formulated logic computed circuits. Artificial Intelligence, 172 (12-13),
15791604.
Bonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets.
Proceedings 19th European Conference Artificial Intelligence (ECAI), pp.
329334.
Brafman, R. I., & Chernyavsky, Y. (2005). Planning goal preferences constraints.
Proceedings International Conference Automated Planning Scheduling, pp. 182191.
Clarke, E., Grumberg, O., & Peled, D. (1999). Model Checking. MIT Press.
Coles, A. I., Fox, M., Long, D., & Smith, A. J. (2008). Additive-disjunctive heuristics
optimal planning. Proceedings 18th International Conference Automated
Planning Scheduling (ICAPS), pp. 4451.
Coles, A. J., Coles, A., Fox, M., & Long, D. (2013). hybrid LP-RPG heuristic modelling
numeric resource flows planning. Journal Artificial Intelligence Research, 46,
343412.
165

fiDomshlak & Mirkis

Coles, A. J., & Coles, A. I. (2011). LPRPG-P: Relaxed plan heuristics planning
preferences. Proceedings 21st International Conference Automated Planning Scheduling (ICAPS), pp. 3745.
Cousot, P., & Cousot, R. (1992). Abstract interpretation frameworks. Journal Logic
Computation, 2 (4), 511547.
Dantzig, G. B. (1963). Linear Programming Extensions. Princeton University Press.
Dantzig, T. (1930). Number: Language Science. Macmillan.
Desrochers, M., & Soumis, F. (1988). generalized permanent labelling algorithm
shortest path problem time windows. Information Systems Operations
Research, 26, 191212.
Do, M. B., Benton, J., van den Briel, M., & Kambhampati, S. (2007). Planning goal
utility dependencies. Proceedings 20th International Joint Conference
Artificial Intelligence (IJCAI), pp. 18721878.
Domshlak, C., Hoffmann, J., & Sabharwal, A. (2009). Friends foes? planning
satisfiability abstract CNF encodings. Journal Artificial Intelligence Research,
36, 415469.
Domshlak, C., Katz, M., & Lefler, S. (2012). Landmark-enhanced abstraction heuristics.
Artificial Intelligence, 189, 4868.
Dudzinski, K., & Walukiewicz, S. (1987). Exact methods Knapsack problem
generalizations. European Journal Operational Research, 28, 321.
Dvorak, F., & Bartak, R. (2010). Integrating time resources planning. Proceedings 22nd IEEE International Conference Tools Artificial Intelligence
(ICTAI), pp. 7178.
Edelkamp, S. (2001). Planning pattern databases. Proceedings European
Conference Planning (ECP), pp. 8490.
Edelkamp, S. (2003). Taming numbers durations model checking integrated
planning system. Journal Artificial Intelligence Research, 20, 195238.
Fikes, R. E., & Nilsson, N. (1971). STRIPS: new approach application theorem
proving problem solving. Artificial Intelligence, 2, 189208.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal
planning problems. Journal Artificial Intelligence Research, 20, 61124.
Garey, M. R., & Johnson, D. S. (1978). Computers Intractability: Guide Theory
NP-Completeness. W.H. Freeman Company, New York.
Geffner, H., & Bonet, B. (2013). Concise Introduction Models Methods Automated Planning. Synthesis Lectures Artificial Intelligence Machine Learning.
Morgan & Claypool.
Gerevini, A., Haslum, P., Long, D., Saetti, A., & Dimopoulos, Y. (2009). Deterministic
planning fifth international planning competition: PDDL3 experimental
evaluation planners. Artificial Intelligence, 173 (5-6), 619668.
166

fiOn Oversubscription Planning Heuristic Search

Gerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local search
temporal action graphs LPG. Journal Artificial Intelligence Research, 20, 239
290.
Gerevini, A., Saetti, A., & Serina, I. (2008). approach efficient planning numerical
fluents multi-criteria plan quality. Artificial Intelligence, 172 (8-9), 899944.
Grotschel, M., Lovasz, L., & Schrijver, A. (1981). ellipsoid method consequences
theorems combinatorial optimization. Combinatorica, 1, 169197.
Handler, G., & Zang, I. (1980). dual algorithm constrained shortest path problem.
Networks, 10, 293310.
Haslum, P. (2013). Heuristics bounded-cost search. Proceedings 23rd International Conference Automated Planning Scheduling (ICAPS), pp. 312316.
Haslum, P., Bonet, B., & Geffner, H. (2005). New admissible heuristics domainindependent planning. Proceedings 20th National Conference Artificial
Intelligence (AAAI), pp. 11631168.
Haslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independent
construction pattern database heuristics cost-optimal planning. Proceedings
19th National Conference Artificial Intelligence (AAAI), pp. 10071012.
Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. Proceedings 15th International Conference Artificial Intelligence Planning Systems
(AIPS), pp. 140149.
Haslum, P., & Geffner, H. (2001). Heuristic planning time resources. Proceedings
6th European Conference Planning (ECP), pp. 107112.
Helmert, M. (2002). Decidability undecidability results planning numerical
state variables. Proceedings Sixth International Conference Artificial
Intelligence Planning Scheduling (AIPS), pp. 4453.
Helmert, M. (2006). Fast Downward planning system. Journal Artificial Intelligence
Research, 26, 191246.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whats
difference anyway?. Proceedings 19th International Conference Automated Planning Scheduling (ICAPS), pp. 162169.
Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics optimal
sequential planning. Proceedings 17th International Conference Automated
Planning Scheduling (ICAPS), pp. 200207.
Helmert, M., Haslum, P., Hoffmann, J., & Nissim, R. (2014). Merge-and-shrink abstraction:
method generating lower bounds factored state spaces. Journal ACM,
61 (3), 16:163.
Hoffmann, J. (2003). Metric-FF planning system: Translating ignoring delete lists
numeric state variables. Journal Artificial Intelligence Research, 20, 291341.
Hoffmann, J., Gomes, C. P., Selman, B., & Kautz, H. A. (2007). SAT encodings statespace reachability problems numeric domains. Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI), pp. 19181923.
167

fiDomshlak & Mirkis

Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 253302.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks planning. Journal
Artificial Intelligence Research, 22, 215278.
Karp, R. (1972). Reducibility among combinatorial problems. Complexity Computer
Computations, pp. 85103. Plenum Press, New York.
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning landmarks. Proceedings
International Joint Conference Artificial Intelligence (IJCAI-09), pp. 1728
1733.
Katz, M., & Domshlak, C. (2010a). Implicit abstraction heuristics. Journal Artificial
Intelligence Research, 39, 51126.
Katz, M., & Domshlak, C. (2010b). Optimal admissible composition abstraction heuristics. Artificial Intelligence, 174, 767798.
Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack Problems. Springer-Verlag
Berlin.
Keyder, E., & Geffner, H. (2009). Soft goals compiled away. Journal Artificial
Intelligence Research, 36, 547556.
Koehler, J. (1998). Planning resource constraints. Proceedings 13th European
Conference Artificial Intelligence (ECAI), pp. 489493.
Mirkis, V., & Domshlak, C. (2013). Abstractions oversubscription planning. Proceedings 23rd International Conference Automated Planning Scheduling
(ICAPS), pp. 153161.
Mirkis, V., & Domshlak, C. (2014). Landmarks oversubscription planning. Proceedings
23rd European Conference Artificial Intelligence (ECAI), pp. 633638.
Nakhost, H., Hoffmann, J., & Muller, M. (2012). Resource-constrained planning: Monte
Carlo random walk approach. Proceedings 22nd International Conference
Automated Planning Scheduling (ICAPS), pp. 181189.
Nebel, B. (2000). compilability expressive power propositional planning
formalisms. Journal Artificial Intelligence Research, 12, 271315.
Nemirovsky, A., & Yudin, N. (1994). Interior-Point Polynomial Methods Convex Programming. SIAM.
Pearl, J. (1984). Heuristics - Intelligent Search Strategies Computer Problem Solving.
Addison-Wesley.
Pommerening, F., & Helmert, M. (2013). Incremental LM-Cut. Proceedings 23rd
International Conference Automated Planning Scheduling (ICAPS), pp. 162
170, Rome, Italy.
Porteous, J., Sebastia, L., & Hoffmann, J. (2001). extraction, ordering, usage
landmarks planning. Proceedings 6th European Conference Planning
(ECP 01), pp. 3749.
168

fiOn Oversubscription Planning Heuristic Search

Punnen, A. P. (1992). K-sum linear programming. Journal Operational Research
Society, 43 (4), 359363.
Richter, S., Helmert, M., & Westphal, M. (2008). Landmarks revisited. Proceedings
23rd AAAI Conference Artificial Intelligence (AAAI-08), pp. 975982.
Russell, S., & Norvig, P. (2009). Artificial Intelligence: Modern Approach (3 edition).
Pearson.
Sanchez, R., & Kambhampati, S. (2005). Planning graph heuristics selecting objectives over-subscription planning problems. Proceedings 15th International
Conference Automated Planning Scheduling (ICAPS), pp. 192201.
Smith, D. (2004). Choosing objectives over-subscription planning. Proceedings
14th International Conference Automated Planning Scheduling (ICAPS), pp.
393401.
Thayer, J. T., & Ruml, W. (2011). Bounded suboptimal search: direct approach using
inadmissible estimates. Proceedings 22nd International Joint Conference
Artificial Intelligence (IJCAI), pp. 674679.
Thayer, J. T., Stern, R. T., Felner, A., & Ruml, W. (2012). Faster bounded-cost search
using inadmissible estimates. Proceedings 22nd International Conference
Automated Planning Scheduling (ICAPS), pp. 270278.
van den Briel, M., Sanchez, R., Do, M. B., & Kambhampati, S. (2004). Effective approaches
partial satisfaction (over-subscription) planning. Proceedings 19th AAAI
Conference Artificial Intelligence (AAAI), pp. 562569.
van den Briel, M., Benton, J., Kambhampati, S., & Vossen, T. (2007). LP-based heuristic optimal planning. Proceedings 13th International Conference
Principles Practice Constraint Programming (CP), pp. 651665.
Yang, F., Culberson, J., Holte, R., Zahavi, U., & Felner, A. (2008). general theory
additive state space abstractions. Journal Artificial Intelligence Research, 32,
631662.

169

fi
