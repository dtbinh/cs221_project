Journal Artificial Intelligence Research 45 (2012) 565600Submitted 6/12; published 12/12Replanning Domains Partial Information Sensing ActionsRonen I. BrafmanBRAFMAN @ CS . BGU . AC . ILDepartment Computer ScienceBen-Gurion University NegevGuy ShaniSHANIGU @ BGU . AC . ILDepartment Information Systems EngineeringBen-Gurion University NegevAbstractReplanning via determinization recent, popular approach online planning MDPs.paper adapt idea classical, non-stochastic domains partial informationsensing actions, presenting new planner: SDR (Sample, Determinize, Replan). stepgenerate solution plan classical planning problem induced original problem.execute plan long safe so. longer case, replan.classical planning problem generate based translation-based approach conformantplanning introduced Palacios Geffner. state classical planning problem generatedapproach captures belief state agent original problem. Unfortunately,method applied planning problems sensing, yields non-deterministic planningproblem typically large. main contribution introduction state samplingtechniques overcoming two problems. addition, introduce novel, lazy, regressionbased method querying agents belief state run-time. provide comprehensiveexperimental evaluation planner, showing scales better state-of-the-art CLGplanner existing benchmark problems, also highlighting weaknesses new domains.also discuss theoretical guarantees.1. Introductionmany real world scenarios agent must complete task required featuresunknown, observed special sensing actions. Consider example Mars rovermust collect rock samples (Smith & Simmons, 2004). rover knowrocks surrounding contains interesting mineral, move closer rocksactivate sensor detects minerals. domains modeled planningpartially observability sensing actions (PPOS).Planning partial observability sensing actions one hardest problemsautomated planning. difficulty stems large number contingencies occur,need take account planning. address contingencies, plannermust generate conditional plan, plan tree, rather linear plan. plan tree growexponentially number propositions problem description, making offline generationcomplete plan tree impossible even moderately complex problems. difficultyovercome, extent, using online planner generates next action only, givencurrent state. One technique online planning replanning (Zelinsky, 1992), made popularFF-replan planner (Yoon, Fern, & Givan, 2007). replanning, state, agent findsc2012AI Access Foundation. rights reserved.fiB RAFMAN & HANIplan based partial, possible inaccurate model, using simpler planning problem. executesprefix it, replanning new information arrives.key component replanning algorithm method generating solvingsimpler problems. Recent replanners focus generating fully-deterministic classical planningproblems solving using off-the-shelf (Yoon et al., 2007), modified (Albore, Palacios, &Geffner, 2009) classical planners. approach particularly beneficial given large array existing classical planners, ability immediately enjoy progress made intensivelystudied area. However, still leaves open key question generate appropriate classical planning problem given agents current state. context probabilistic planningfull observability, current planners use multiple samples classical planning problems obtainedtransforming stochastic actions deterministic actions selecting single effect actioninstance (Yoon et al., 2007; Yoon, Fern, Givan, & Kambhampati, 2008; Kolobov, Mausam, & Weld,2010).context PPOS sophisticated translation scheme introduced CLGplanner (Albore et al., 2009). translation based techniques introduced PalaciosGeffner (2009) solving conformant planning representing agents belief state withinclassical planners state. achieved extending language propositions formKp, Kp, denoting fact agent knows p true false, respectively.ideas extended PPOS, result non-deterministic planning problems effectsensing action agents belief state cannot known offline. CLG handles problemrelaxing problem number ways, using complex translation. keyaspect translation that, action precondition p, value psensed, CLG plan as-if p true (and thus, executed), provided actionsenses p executed a. Thus, makes optimistic assumptions regarding future outcomessensing actions. This, however, imply CLG actually execute a,actual outcome differs expected one, CLG replan using new information.SDR planning algorithm propose paper follows similar high-level approachbased replanning. state, generate classical planning problem reflects information agents belief state. specifics approach, are, however bit different,main aim provider better scalability, requires generating smaller classical planningproblems. achieve using state sampling. is, instead planning possibleinitial states, sample small subset states, plan possible initialstates. also use sampling remove optimistic bias CLG: sample arbitrary initialstate sI , assume observation values obtained sI true initial state.use sampling leads much smaller classical planing problems, hence, better scalability.planner operates assumptions alway true (i.e., considerssubset initial states one possible set observations), possible preconditionsactions selected, well goal, hold possible worlds. Thus, agent mustmaintain representation current set possible states, also known belief state,order verify conditions. many methods maintaining belief state (Alboreet al., 2009; To, Pontelli, & Son, 2011; To, Son, & Pontelli, 2011a; To, Pontelli, & Son, 2009),work well problems certain structure, well problem structures.general, belief state maintenance difficult, use belief state limited,suggest lazy belief state querying mechanism spirit CFF (Hoffmann & Brafman,2006) require explicit representation update belief state following566fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSaction. maintain symbolic representation initial belief state, only. determineliteral holds current belief state, regress literal history actionsobservations, check consistency regressed formula initial belief state.augment regression process caching mechanism, call partially-specified states,allows us keep regressed formulas compact.resulting planner SDR (Sample, Determinize, Replan) compares favorably CLG(Albore et al., 2009), current state-of-the-art contingent planner. existingbenchmarks generates plans faster solve problems CLG cannot solve, planssimilar slightly worse size.paper contains comprehensive experimental evaluation aimed identifying strengthsespecially weaknesses SDR replanning approach. end, formulatednumber new benchmark domains, including domains sensing requires performing actionspath goal, domains dead-ends. addition, also evaluateeffectiveness new regression-based method maintaining information belief statecomparing closest lazy approach CFF (Hoffmann & Brafman, 2006). Finally,describe theoretical guarantees associated SDR. First, show translationscheme use sound complete whenever sampled initial state true initial state.Then, show that, certain assumptions connectivity domain, SDRcomplete description initial belief state reach goal, goal reachable.paper organized follows: next section describe problem contingentplanning partial observability sensing. Then, describe idealized versionSDR planner ignores efficiency problems arise belief state large usesentire belief state generate classical planning problem. provide theoretical analysiscorrectness convergence properties idealized algorithm. Next, describe fullSDR algorithm. algorithm uses state sampling manage size belief state wellregression mechanism querying belief state. followed overview comparisonrelated work, followed empirical evaluation analyzing strengths weakness SDR.2. Problem Definitionfocus planning problems partial observability sensing actions (PPOS). shallassume actions deterministic throughout paper.Formally, PPOS problems described quadruple: P, A, , G, P setpropositions, set actions, propositional formula P describes setpossible initial states, G P set goal propositions. follows often abusenotation treat sets literals conjunction literals set, well assignmentvalues propositions appearing set. example, {p, q} also treated p qassignment true p false q.state world, s, assigns truth value elements P , usually representedusing closed-world assumption via set propositions assigned true s. belief-stateset possible states, initial belief state, bI = {s : |= } defines set statespossible initially.action, A, three-tuple, {pre(a),effects(a),obs(a)}. action preconditions, pre(a),set literals must valid action executed. action effects, effects(a),567fiB RAFMAN & HANIset pairs, (c, e), denoting conditional effects, c set (conjunction) literals esingle literal.Finally, obs(a) set propositions, denoting propositions whose value observedfollowing execution a. assume consistent, is, (c, e) effects(a)cpre(a) consistent, (c, e), (c , e ) effects(a) |= c c state s,e e consistent.current benchmark problems, either set effects set obs empty. is, actionseither alter state world provide information, pure sensing actionsalter state world, reason case general.use a(s) denote state obtained executed state s.satisfy literals pre(a) a(s) undefined. Otherwise, a(s) assigns proposition pvalue s, unless exists pair (c, e) effects(a) |= c e assigns pdifferent value s. sequence actions, use a(s) denote resulting state, definedanalogously.Observations affect agents belief state. assume observations deterministicaccurate, reflect state world prior execution action.1 Thus,p obs(a) agent observe p p holds (i.e., prior effect), otherwiseobserve p. Thus, true state world, b current belief stateagent, ba,s , belief state following execution state defined as:ba,s = {a(s )|s b, agree obs(a)}is, progression states agent would receive, following execution a, observation state s. Extending definition sequencesactions, sequence actions, ba,s denotes belief state reached b executingstarting state b.Alternatively, define belief progression without explicit world state using:ba,o = {a(s )|s b, observation world state o}.contingent plan PPOS problem annotated tree = (N, E). nodes, N ,labeled actions, edges, E, labeled observations. node labeled actionobservations single child, edge leading labeled null observationtrue. Otherwise, node one child possible observation value, i.e., one childpossible assignment observed propositions. edge leading child labeledcorresponding observation. plan executed follows: action root executed,observation (possibly null) made, execution continues recursively childcorresponds edge labeled observation. assume actions observationsdeterministic, single possible execution path along tree initial state.use (s) denote state obtained executed starting state s. solution plan (planshort) PPOS P = P, A, , G (s) |= G every |= .Complete contingent plans consider possible future observations, prohibitivelylarge problems practical interest. paper, thus, concerned online planningPPOS. is, stage, planner selects next action execute, executes it,reconsiders.1. One choose observations reflect state world following execution actionprice slightly complicated notation below.568fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSFigure 1: 4 4 Wumpus DomainExample 1. illustrate definitions using 4 4 simplified Wumpus domain (Albore et al.,2009), serve running example. domain, illustrated Figure 1, agentnavigates 4 4 grid bottom-left corner top-right corner (the goal) movingfour directions. squares around top two squares diagonal gridcontain monsters called Wumpus one every pair squares adjacent diagonal (e.g.,theres either Wumpus square 3,4 4,3). agent move square safe.is, contains Wumpus, specified precondition move action. Thus, agentnever enter square Wumpus die, dead-ends domain.2initial state agent know location Wumpuses, directly observelocation. However, Wumpus emits stench drifts adjacent squares. Hence,agent adjacent square Wumpus smell stench, although cannot determineadjacent square Wumpus hiding. Thus, measurements different locations mayrequired determine precise position Wumpus. domain demonstrates complexhidden state multiple sensing actions, conditional effects.formalize problem follows:set propositions at-x-y 1 x, 4, wumpus-at-2-3, wumpus-at-3-2, wumpusat-3-4, wumpus-at-4-3 stench-at-x-y 1 x, 4.actions move-from-x1 -y1 -to-x2 -y2 adjacent x1 , y1 , x2 , y2 pairs, smell.initial state is: at-1-1 at-1-2 at-4-4 (oneof wumpus-at-2-3 wumpus-at-3-2)(oneof wumpus-at-4-3 wumpus-at-3-4).2. Later on, introduce natural formalization domain require square safe moveit, thus, contains dead-ends.569fiB RAFMAN & HANIgoal at-4-4.initial belief state consists four possible world-states, corresponding four possible truth assignments (wumpus-at-2-3 wumpus-at-3-2) (wumpus-at-4-3 wumpus-at3-4) addition known literals, at-1-1 adjacency propositions.3. Idealized SDR Plannerdescribe idealized version Sample Determinize Replan (SDR) planner. samplessingle distinguished state current belief, creates deterministic classical problem,observations correspond derived initial state . solving classical problem using classical planner, applies resulting plan sensing action performed.Then, belief state updated, removing states agree observed value,process repeated. idealized version SDR complete explicit descriptionagents belief state maintained used generate classical planning problem. actualalgorithm modifies version using sampled belief state lazy belief-state maintenance;described Section 5.3.1 Replanning using Complete Translationscentral component SDR translation PPOS classical planning problem.well known planning uncertainty reduced planning belief space, i.e.,move sets possible states world, modeling current knowledge.sensing actions, conformant planning, results classical, deterministicplanning problem belief space. sensing actions exist, however, resulting problemnon-deterministic, effect sensing actions agents state knowledge dependsvalues observed online, depend true state world, hidden agent.Since want generate deterministic classical planning problem solvedoff-the-shelf classical planner, need determinize non-deterministic problem describedabove, simplifying process. selecting one possible initial state assumingobservation values correspond obtained true initial state. Noteimply planner plans actual initial state world (which wouldyield simple classical planning problem standard state space) planneractually reasons belief state online true initial state, attempts reachbelief state goal known, i.e., possible states goal states.cases, assumption true initial state turns incorrect.planner learns online executes sensing action discovers outcomedifferent expected true initial world state. point, learned(and possibly initial states) cannot possibly true initial world state. Thus,uncertainty reduced, replan using new belief state new initial state sampledit.algorithm terminates cannot find solution classical problem generated,implies goal cannot achieved set states indistinguishableselected state , or, belief state indicates goal, G known, i.e., every stateb, |= G.high-level SDR algorithm complete initial belief state described Algorithm 1.570fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSAlgorithm 1 SDR (Complete Translation)Input: PPOS Problem: P = P, A, , G, Integer: size1: b0 := initial belief state bI = {s : |= }2: := 03: G hold states bi4:Select state bi5:Generate deterministic planning problem C given P, bi ,6:Find solution plan C7:solution exists8:return failure9:end10:=11::=first()12:Execute a, observe13:bi+1 bia,o update belief given a,14:ii+115:Remove16:inconsistent17:break18:end19:end20: end3.2 Generating Classical Planning ProblemGiven input PPOS P = P, A, , G, current belief state b, selected state b (hypothesized current true system state), generate classical planning problem Pc (b, ) =Pc (b), Ac (b), Ic (b, ), Gc . Notice influences definition classical initial state only,b influences elements except goal. Pc (b, ) defined follows:Propositions Pc (b) = P {Kp, Kp|p P } {p/s|p P, b} {Ks|s b} :1. P set propositions appear original problem. value initializedaccording distinguished state updated reflect current stateworld given true initial state.2. {Kp, Kp|p P } Propositions encoding knowledge obtained agent. Kpholds agent knows p true, i.e., p holds possible states. knowledgeobtained sensing action p observed truenecessary consequence action. agent know p true (denoted Kp),know p false (Kp), know value p (denoted KpKp).3. {p/s|p P, b} Propositions capture value p given trueinitial state. use rule certain states. example, observed ptrue, rule state true initial state p/s holds.571fiB RAFMAN & HANI4. {Ks|s b} Propositions capture states ruled out.concluding certain state initial state system, acquire Ks.shall use Kc shorthand notation Kl1 Klm , c = l1 lm ,Kc shorthand notation Kl1 Klm . also note numberpropositions generated idealized translation exponentially large,actual SDR planner, described Section 5, uses various approximations.Actions every action A, Ac (b) contains action ac defined follows:pre(ac ) = pre(a) {Kp|p pre(a)}. is, precondition action must holdagent must know true prior applying action.every (c, e) effects(a), effects(ac ) contains following conditional effects:1. (c, e) original effect. conditions update state assumedtrue state world.2. {(c/s, e/s)|s b} above, conditioned states consistent b.conditions update values propositions given possible states b.3. (Kc, Ke) know condition c holds prior executing a, knoweffect holds following a. condition allows us gain knowledge.4. (Kc, Ke) c known false prior executing a, e knownfalse afterwards.5. {(p, Kp), (p, Kp)|p obs(a)} observing value p, gain knowledgeform either Kp Kp, depending value p state assumedtrue world state.6. {(p p/s, Ks), (p p/s, Ks)|p obs(a), b} rule possible states binconsistent observation. sometimes known refutationstates.addition, literal l (w.r.t. P ) merge action allows us conclude absoluteknowledge knowledge conditional states b. is, states agreevalue proposition, know value proposition. exclude statesalready refuted, i.e., found inconsistent observation.pre(merge(l)) = {l/sKs|s b} state s, either agrees l, previouslyrefuted.effects(merge(l)) = {(true,Kl)}.Initial State Ic (b, ) = l:s |=l lsb Ks:sb,s|=l l/spP Kp Kp1. {l : |= l} set propositions P appear original problem, initializedvalue distinguished state . Notice elementtranslation affected choice hypothesized initial state.2. {Kp Kp : p P } Knowledge propositions Kp Kp initializedfalse, denoting initial knowledge value propositions.572fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS3. {l/s : b, |= l} Propositions form p/s initialized value pcorresponding state s.4. {Ks : b} Propositions Ks initialized false, knowinitial state b impossible observing value proposition.Goal Gc = KG, is, goal literals known true.translation similar to, inspired KS0 translation introduced PalaciosGeffner (2009) conformant planning. adapt PPOS, chose determinize observation actions sampling distinguished initial state, , whose value track throughoutplan, use select outcome observation actions. Albore et al. (2009) provide differenttranslation contains propositions encode fact value propositionsensed (denoted Ap). imply p true, agent knows value p.Instead requiring Kp hold prior executing action precondition p, require Aphold. results optimistic choice values sensed propositions.natural extension idea conditioning value propositions initial state,capture p/s propositions use, condition value multiple initial statesp progresses identically. is, suppose differ valueproposition r, r appear actions affect value p. case, p/sp/s always value. Consequently, simply maintain single proposition, p/{s, }. This, indeed, approach taken Palacios Geffner (2009) Alboreet al. (2009), sets states called tags. larger set states denoted tag,fewer tags needed, smaller representation. fact, Palacios Geffner showconformant planning problems require small set tags, linear number proposition.use tags important optimization, lead exponential reduction sizegenerated planning problem. decided introduce tags wouldcomplicate description translation, primary technique reducing problemsize state sampling discussed Section 5. Nevertheless, SDR could optimizedusing tags instead states.Example 2. demonstrate translation using small toy example identifyingtreating disease. n possible diseases, uniquely identified usingsingle test, cured using unique treatment. applying treatment, must identifydisease, avoid applying wrong treatment, causing damage. PPOS hencedefined follows:one proposition per disease, diseasei {1..n}, proposition resulttest test-passed.need n test actions testi , preconditions conditional effect (diseasei ,testpassed), n treatment actions treati precondition diseasei effect diseasei .also one sensing action observe-test-result allowing us sense value test-passed.initial state is: (oneof disease1 , ..., diseasen ) test-passed. initial belief stateconsists n possible world-states, corresponding n possible diseases.goal i[1,n] diseasei .573fiB RAFMAN & HANIdenote possible states using si {0, 1..n}, si patientdisease state s0 patient disease. Let us choose sk , is, chooseassume patient k th disease.set propositions translation is:original propositions diseasei test-passed.Propositions representing unconditional knowledge:Kdiseasei , Kdiseasei {1..n}.Ktest-passed, Ktest-passed.Propositions representing knowledge conditional upon initial states:diseasei /sj , {1..n}, j {0..n}.test-passed/sj , j {0..n}.Ksj j {0..n}.set actions is:Test: testi action have:preconditions.effects:(diseasei , test-passed).(diseasei , test-passed).(diseasei /sj , test-passed/sj ), j {0..n}.(diseasei /sj , test-passed/sj ), j {0..n}.Treat: treati action have:precondition: diseaseieffects: diseasei Kdiseasei .Observing test result:preconditions:Effects:(test-passed, Ktest-passed) positive observations.(test-passed test-passed/sj ), Ksj , j {0..n} refuting statesagree positive observation.(test-passed, Ktest-passed) negative observations.(test-passed test-passed/sj , Ksj ), j {0..n} refuting statesagree negative observation.Merge: need merges diseasei proposition (see implementation note below):574fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSpreconditions:j[0..n] diseasei /sjKsjeffects: Kdiseaseiactions merging diseasei :preconditions: j[0..n] diseasei /sj Ksjeffects: Kdiseaseiinitial state conjunction following conjuncts (we omit negations simplicitypresentation):distinguished initial state: diseasekUncertainty current state: Ksj , j {0..n}.Conditional knowledge:diseasei /si , diseasei /sj , {1..n}, j [0..n], = j.Finally, goal i[1..n] Kdiseasei .3.3 Notes Efficient Implementationtranslation correct, propositions removed. many problems,original propositions always known. example, Wumpus example,location agent always known, identical possible current states. valuep always known, remove propositions Kp, Kp, p/s, use proposition p only.propositions require merge actions, either.refutation effect actions (item 6 list action effects above) movedindependent action, similar merge actions. create proposition p observableaction (i.e., p obs(a) A) state two refutation actions,preconditions p p/s p p/s, identical effect Ks. reduces numberconditions action poses difficulty current classical planners.4. Theoretical Guaranteesprove two important properties algorithm applied deterministic PPOS P.first result proof correctness translation. show plan existsoriginal problem iff plan exists classical problems generate, assuming guessedcorrect initial state. show that, standard assumptions, algorithm eventuallyreach goal. note understanding results required followingsections.begin section definitions notations used theorems below:Classical planning notations: b belief state b (world) state recallIc (b, s) denotes initial state classical planning problem SDRs translation wouldgenerate selected distinguished initial state. plan P b initialbelief state c corresponding plan classical planning problem Pc (b, s),action replaced corresponding action generated classicalproblem. addition, prior first action c , following translated actions,575fiB RAFMAN & HANImerge actions inserted. assume modified version merge actionspreconditions instead, current precondition replaces condition partconditional effect previously empty (i.e., = true). allows us insert arbitrarymerge actions without risking making plan undefined. Adding merges, plan cforced make possible inferences following every action.Sensing non-sensing actions: without loss generality, assume actioneither makes observation changes state world, both. Actionsmodeled consecutive pair actions; first, one changes world,one makes observations.Indistinguishable states: say s, indistinguishable applicableiff applicable , observations generated executingidentical. Note s, may indistinguishable distinguishableplan .Applicability actions plans: say action applicable statesatisfies preconditions a. say action applicable belief state bb satisfies preconditions a. Applicability generalized plansnatural manner. is, given plan = a1 , ..., state s, plan applicablea1 applicable = a2 , ..., applicable a1 (s).begin showing c achieves Kl literal l, belief resultingexecuting plan belief space satisfy l.Theorem 1. Let b belief state b true initial state. Let sequence actionsoriginal problem. every literal l, b,s |= l iff c (Ic (b, s)) |= Kl.Proof. prove induction || following conditions hold:1. applicable b iff c applicable Ic (b, s)2. every b indistinguishable every literal l: (s ) |= l iffc (Ic (b, s)) |= l/s3. every b, distinguishable iff c (Ic (b, s)) |= Ks4. every literal l: b,s |= l iff c (Ic (b, s)) |= KlBase case. base case = c includes merge actions. Conditions 1, 2, 3immediate construction. is, empty plan always applicable (because containsactions), distinguishable states. Condition 4 consequence definitionmerge action.Inductive step. inductive step, assume conditions 1-4 hold considersequence = a. consider two cases:sensing action: condition 1, observe given induction hypothesis,need show applicable following iff ac applicable following c . (Ifprefix inapplicable one case, know induction hypothesis also576fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSinapplicable other). Suppose applicable, i.e., b ,s |= p every preconditionp a. condition 4 conclude c (Ic (b, s)) |= Kp correspondingprecondition ac . additional merge actions preconditions. directionsimilar.conditions 2 3 notice indistinguishable states become distinguishable observing literal holds one other. Thus, non-sensing action cannotcause two states indistinguishable given become distinguishable following .Hence, Condition 3 follows immediately induction hypothesis factnon-sensing action effect form Ks. Condition 2: laffected last action , follows induction hypothesis. Otherwise,l added condition c holds . construction, (c/s , l/s )conditional effect ac . Given induction hypothesis, c held prior executioniff c/s held prior execution ac merge actions follow c (Ic (b, s))c (Ic (b, s)). Consequently, l/s holds l holds.Finally, Condition 4, b,s |= l iff every indistinguishable given ,(s ) |= l. Condition 2 above, happens iff c (Ic (b, s)) |= l/s every .Condition 3 guarantees c (Ic (b, s)) |= Ks every distinguishables. Thus, suitable merge action, included definition c conclude Kl.direction, Kl holds, know either true affectedaction, consequence merge action. former case, know using inductionhypothesis b ,s |= l. Since l influenced a, conclude b,s |= l.latter case merge action, use Condition 2 3 conclude b,s |= l.sensing action: states remain indistinguishable s, conditions 1,2immediate: effect deduce Kl literal l. Condition 1 notesensing actions always applicable. Condition 2 note sensing actions affectstate.Condition 3; distinguishable given either distinguishablebefore, case distinguishable following a, Ks never removeddeduced (Ks effect action translation). became distinguishable executed, construction ac Ks effect.direction, indistinguishable given , must indistinguishable given. Hence, induction hypothesis, c (Ic (b, s)) |= Ks . Since indistinguishable now, sensing action effect , ac addKs , construction.Condition 4, first suppose b ,s |= l. induction hypothesis happens iffc (Ic (b, s)) |= Kl. However, b ,s |= l c (Ic (b, s)) |= Kl affected a, thusremain true well. Therefore, need consider case b ,s |= lc (Ic (b, s)) |= Kl.Suppose b ,s |= l b,s |= l. implies state b (s ) |= ldistinguishable . Thus, c (Ic (b, s)) |= Ksusing merge action, conclude c (Ic (b, s)) |= Kl.577fiB RAFMAN & HANINext, suppose b ,s |= l b,s |= l. Thus, exists b indistinguishablegiven (s ) |= l. Since b ,s |= l merge action applied,conclude b,s |= l.result provides local soundness completeness proof (i.e., per replanning phase). Applying theorem goal G get following corollary:Corollary 1. Let b belief state, state it, G goal. SDR (with sound completeunderlying classical planner) find (standard, sequential) plan executedinitial belief state b reach belief state satisfying G, iff one exists.Proof. Theorem 1 b,s |= l iff c (Ic (b, s)) |= Kl literal l. Thus,b,s |= G iff c (Ic (b, s)) |= KG. Thus, plan exists original problem iff plan existstranslated problem. (Recall KG shorthand Kg1 Kgm , gi literalsG = g1 gm .)also use theorem deduce belief state reduced every replanningepisode:Corollary 2. Let plan generated SDR (with sound complete underlying classicalplanner) belief state b initial state b. Let real initial state. executeb reach belief state satisfying G, = s.Proof. Given Theorem 1, know SDR generate correct plan b s. Thus,reach goal indistinguishable, consequently, mustdifferent.show next global version correctness. show standard,admittedly strong assumptions, algorithm reach goal within finite number steps.above, assume complete representation belief state maintained (as opposedusing sample, later).Dead-ends well known pitfall replanning algorithms, also online algorithms generate complete solution problem, unlikely feasiblepractice, may require exponential space. Thus, provide reasonable guarantees, analysisfocuses domains dead-ends. formally, say PPOS problem defines connected state-space, connected3 , state satisfying G reachable every state reachableI.Another problem case partial observability inability recognize goalreached. Consider following simple example; single proposition p,unobservable whose value unknown initial state. single action, flip,flips value. goal p, evidently reached state. Yet, never knowp holds. is, never reach belief state states satisfy p.Thus, deadends enough, must also require belief dead-ends, i.e.,belief states belief state satisfying goal reachable. say PPOS3. analogous term context RL algorithms MDPs ergodic.578fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSbelief-connected given belief state b reachable initial belief state, bI ,world-state consistent b (i.e., b), exists sequence actions that,true starting state, leads b b , b |= G. notation: ba,s |= G. Clearly,belief-connectedness implies connectedness.Theorem 2. Given belief-connected PPOS problem, SDR sound complete underlyingclassical planner set tags corresponds possible initial states, reach goalfinite number steps.Proof. proof based ideas PAC-RL algorithms E 3 (Kearns & Singh, 2002)Rmax (Brafman & Tennenholtz, 2003) follows immediately Corrolary 2. Consider plangenerated assumption true initial state. plan successfulevery initial state indistinguishable given . plan fails, recognizefact maintain sound complete description current belief state. alsoconclude neither initial state indistinguishable given possible,hence, belief state reduced least one state. belief-connectedness,still reach goal. continue process number times equalsize initial belief state, point belief state singleton, leftclassical planning problem, underlying classical planner solve.proof makes apparent many cases, polynomial number replanningphases required, number initial states ruled could large, e.g.,iteration learn initial value single proposition. Bonet Geffner (2011) identify onecase, which, consequently, requires linear number replanning phases succeed.5. SDR Belief State Sampling Belief Trackingsize classical problem generated translation method suggested Section 3.2depends cardinality belief state, i.e., number possible initial states: generateone proposition original proposition possible initial state, conditionaleffect every action generate conditional effect possible initial state. leadexponential (in size P) larger classical planning problem. Thus, generated problemssize may become large current classical solvers handle. addition, belief stateexponentially large, explicit representation impractical. addressissues using sampled subset current belief state generate classical planningproblem, using implicit description belief state.5.1 Sampling Belief Stateaddress problem large belief state suggest using sampled subset belief stategenerate classical planning problem. Conceptually technically, change requiredmethod described Section 3.2 minor: select subset b generate classicalplanning problem true belief state. also implies distinguished initialstate chosen . Thus given PPOS P = P, A, , G, sampled set state|= , distinguished initial state S, simply generate classical planningproblem Pc (S , ).579fiB RAFMAN & HANIsampling translation method similar complete translation, important semantic difference. complete translation Kp denoted knowing p truepossible states, sampling translation Kp denotes knowing p true sampledstates. Thus, upon execution, agent intends execute action, might preconditions whose value known true possible states. call actions unsafe.must ensure unsafe actions never executed.new translation uses instead b cannot guarantee action resultingplan unsafe. reason, must maintain representation true belief state throughoutexecution. use information check whether possible state existsprecondition next action hold. call state witness state. witness statefound, must sample replan. ensure generate another planexecutable witness state, add set sampled states, . new plan eitherlearn distinguish witness state rest states , choose differentpath valid witness state.Example 3. Returning Wumpus example, show sampled translation reducessize translation. example, four possible initial states, denotesll , slr , srl , srr , sll denotes initial state Wumpus leftdiagonal, etc. select sll , slr sampled belief state , sll distinguished initialstate .set propositions is:original propositions:at-x-y 1 x, 4 explained above, proposition always knownrequire conditional knowledge propositions.wumpus-at-2-3, wumpus-at-3-2, wumpus-at-3-4, wumpus-at-4-3stench-at-x-y 1 x, 4.Propositions representing unconditional knowledge:Kwumpus-at-2-3, Kwumpus-at-3-2, Kwumpus-at-3-4, Kwumpus-at-4-3.Kwumpus-at-2-3, Kwumpus-at-3-2, Kwumpus-at-3-4, Kwumpus-at-4-3.Kstench-at-x-y 1 x, 4.Kstench-at-x-y 1 x, 4.Propositions representing conditional knowledge:sll conditional propositions:wumpus-at-2-3/sll , wumpus-at-3-2/sll , wumpus-at-3-4/sll , wumpus-at-4-3/sll .stench-x-y/sll 1 x, 4.slr conditional propositions:wumpus-at-2-3/slr , wumpus-at-3-2/slr , wumpus-at-3-4/slr , wumpus-at-4-3/slr .stench-at-x-y/slr 1 x, 4.Ksll , Kslr propositions denoting refuted states.580fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSset actions is:Move: move-from-x1 -y1 -to-x2 -y2 actions have:preconditions: at-x1 -y1 wumpus-at-x2 -y2 Kwumpus-at-x2 -y2 )effects 4 :at-x1 -y1at-x2 -y2Smell: smell-stench-at-x-y action have:preconditions: at-x-yeffects:stench-at-x-y , Kstench-at-x-ystench-at-x-y stench-at-x-y/sll , Ksllstench-at-x-y stench-at-x-y/slr , Kslrstench-at-x-y , Kstench-at-x-ystench-at-x-y stench-at-x-y/sll , Ksllstench-at-x-y stench-at-x-y/slr , KslrMerge: illustrate merges using stench-at-x-y proposition:preconditions:(stench-at-x-y/sll Ksll ) ( stench-at-x-y/slr Kslr )effects: Kstench-at-x-yinitial state conjunction following conjuncts (we omit negations simplicity presentation):distinguished initial state:at-1-1 x=1..4,y=1..4,x=1y=1 at-x-ywumpus-at-2-3 wumpus-at-4-3 wumpus-at-3-2 wumpus-at-3-4stench-at-1-3 stench-at-2-2 stench-at-2-4 stench-at-3-3 stench-at-4-2 stenchat-4-4Uncertainty current state: Ksll Kslr .Conditional knowledge:state sllwumpus-at-3-2/sll wumpus-at-2-3/sll wumpus-at-4-3/sll wumpus-at-3-4/sll .stench-at-1-3/sll stench-at-2-2/sll stench-at-2-4/sll stench-at-3-3/sll stenchat-4-2/sll stench-at-4-4/sll stench-at-x-y/sll x locations.state slr4. domain conditional effects, list effect e directly rather writing (true, e).581fiB RAFMAN & HANIwumpus-at-2-3/slr wumpus-at-3-2/slr wumpus-at-3-4/slr wumpus-at-4-3/slr .stench-at-1-3/sll stench-at-2-2/sll stench-at-2-4/sll stench-at-3-3/sll stenchat-4-4/sll stench-at-x-y/sll x-y locations.Finally, goal Kat-4-4.5.2 Note Theorytheoretical guarantees Section 4 hold sampled translation. Specifically,expected, translation longer sound: plan works states may workstates. However, since never apply illegal action (because maintain complete descriptionbelief state plan execution), maintain assumption belief-connectedness,goal always remains reachable. question whether ensure progress towardsgoal. Unfortunately, planner may always come unsound plan, even beliefconnectedness, progress cannot guaranteed without additional assumptions. example,assume sample size grows time plan unsound, i.e., accumulatewitness states discussed above, ensure progress made, eventually goalreached, i.e., assured completeness.5.3 Belief State Maintenance Regressionnoted above, must maintain information true belief state. Belief state maintenancedifficult task various representations CNF, DNF, Prime Implicates, Prime Implicants,(To et al., 2009; To, Son, & Pontelli, 2010, 2011b; et al., 2011, 2011a), work welldomains poorly domains. is, representation method suitablefamily domains special features, work well domainsfeatures exist. However, require belief state order answer two typesqueries: (1) Sampling subset current possible states executed replanningepisode constructing classical problem, (2) Checking whether literal l holdscurrently possible states executed prior action execution ensure unsafe,order check whether goal reached.propose method answering queries without maintaining explicit representationbelief space, regressing queries execution history towards initial beliefstate. approach requires maintain initial belief state formula executionhistory only, somewhat similarly situation calculus (McCarthy & Hayes, 1969; Reiter, 1991).answer query current state, regress query entire historycompare initial state. improve performance, also cache limited informationintermediate belief states.main benefit approach focused querys condition only, thereforeyields small formulas easier check satisfiability than, e.g., formula describingcomplete history conjunction initial belief (Hoffmann & Brafman, 2006). However,process must repeated every query.5.3.1 Q UERYING C URRENT TATE P ROPERTIESThroughout online process maintain symbolic initial state formula historyactions observations made. use information check, prior applying action a,582fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSwhether preconditions hold current state, is, whether action safe. must alsocheck whether goal conditions hold current state determine whether goalachieved. check whether condition c holds, regress c current history, obtainingcI . world state currently satisfies c iff result executing current history initialstate satisfying cI . cI inconsistent iff c holds states currently possible.specifically, checking whether literal l (or set literals) holds currentbelief state, regress negation current history, resulting formula . Next,check, using SAT solver, whether satisfiable. satisfiable, know lvalid.Recall c = regress(c, a) weakest condition state executingyields state satisfying c. compute regress(c, a) using following recursive procedure:regress(l, a) = false (true, l) effects(a).regress(l, a) = pre(a) (true, l) effects(a). case pre(a) eliminatedregression, preconditions already regressed proven validprior applying a. Thus, (true, l) effects(a) regress(l, a) = true.regress(l, a) = pre(a) (l (c,l)effects(a) c) (c,l) c. is, either l existed prioraction executed, added one conditions l effect. alsoimpossible conditions removing l apply. above, pre(a) eliminatedregression.regress(c1 c2 , a) = regress(c1 , a) regress(c2 , a)regress(c1 c2 , a) = regress(c1 , a) regress(c2 , a)5Applying regression histories sequences actions, extend meaning regress operator:regress(c, ) = cregress(c, h a) = regress(regress(c, a),h)maintain correct description set initial world states, must also update initialbelief-state formula whenever make observation. Thus, regress every obtained observationh, obtaining regressed formula , conjoin bI . Thus, updated set initialstate described = . optimize, apply unit propagation maintainsemi-CNF form conjunction disjunctions xor (oneof ) statements. is, convertnewly added CNF form. current benchmarks find maintaining initialbelief formula CNF easy.5.3.2 AMPLING B ELIEF TATESDR samples belief state generate subset possible states computing translationclassical problem. Using regression mechanism, maintain formula describingpossible set initial states given initial constraints possible states current history,i.e., actions executed observations sensed.5. Recall effects deterministic effect condition conjunction.583fiB RAFMAN & HANIsample n states current belief state, begin finding n possible satisfyingassignments initial belief formula . running simple SAT solver,picks propositions randomly set unassigned propositions formula, setsvalue them, propagates value formula. formula unsolvable,backtrack. current benchmarks structure initial belief formula simple, e.g.,disjunctive set oneof statements, clauses, simple SAT solver finds solutionsrapidly.n satisfying assignments represent set initial states consistentcurrent information (i.e., initial belief state observations made far). obtain samplestates current belief state, progress history.5.3.3 PTIMIZATION : PARTIALLY-S PECIFIED TATESstep execution current plan maintain list literals known holdbelief state. propositions appear among literals list currently unknown.call construct partially-specified belief state (PSBS), serves cache.execute action a, propagate set forward update follows:(c, l) effect a, c must true execution, add l remove l.l l may true (that is, l unknown PSBS), deletes l l holds (possiblyconditional conditions necessarily hold), add l l (andconditions possible) holds, conclude l must true execution a.performing regression condition c, may learn literal l validintermediate PSBS. update PSBS successors, accordingly.instance, observing test-passed Example 2, add last PSBS. Then,regression action testi , obtain diseasei , add previous PSBS,forth. Following regression, simplification techniques mentioned above,may learn literal l true initially. progress l history addinformation PSBSs. Example 1, instance, regressing stench observationinitial state formula, may learn wumpus-at-3-4 holds, progresshistory add learned information PSBS.PSBS may reduce formulas constructed regression. example, supposeregressed formula intermediate belief state b form l, l literalbelongs current PSBS, i.e., known hold b. Then, need regress backb. Or, know l holds b, immediately conclude regressed formulainconsistent initial belief state.conclude, belief state representation uses initial belief, represented symbolicformula (initialized given PPOS definition), sets literals shown holdtime step. update initial belief adding formulas resulting regressingobservations, thus adding constraints set possible states, reducing cardinalityinitial belief state. Whenever discover literal holds time step, eitheraction (unconditional) effect, regressing observation, cache this. regressionmechanism uses cached facts whenever possible reduce size regressed formula.584fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS5.4 Complete SDR Planner Algorithmexplaining essential components SDR planner, i.e., sampling, translation,regression-based belief maintenance, present complete SDR algorithm (Algorithm 2).Algorithm 2 SDR (Sampling Translation)Input: PPOS Problem: P = P, A, , G, Integer: size number states sampled SI1: h := , empty history2: G known current belief state3:Select distinct set states SI consistent bI , finding satisfying assignmentss.t. |S | size4:Select distinguished state sI SI5:Propagate SI sI h, resulting6:Generate deterministic planning problem Pc (S , )7:Find solution C8:solution exists9:return failure10:end11:=12::=first()13:Regress pre(a) h obtaining pre(a),h14:pre(a),h inconsistent bI15:break16:end17:Execute a, observe18:Append a, h19:Regress h obtaining o,h20:Update initial belief state formula given o,h : o,h21:Remove22:inconsistent23:break24:end25:Update current state: a(s )26:end27: endalgorithm begins querying goal state already reached (line 2).done regressing negation goal conditions current history, checkingwhether negation consistent initial belief state. latter true, knowstate goal conditions apply.choose sub-sample SI bI (the initial belief state) sI SI (lines 3 4).propagate states SI history applying executed actions h statesSI (line 5). Lines 3-5 thus equivalent sampling current belief state.obtain , use translation Section 3.2 (replacing b ) generateclassical problem Pc (S , ), solve using classical planner (lines 6 7).585fiB RAFMAN & HANIsolution problem, means goal cannot obtained currentstate, thus solution PPOS.execute obtained plan; first check preconditions current action holdcurrent belief state. done regressing negation preconditionshistory, checking regressed formula consistent bI (line 13-16). consistent,i.e., state bI (regressed) negation preconditions hold,must choose new SI replan.Finding preconditions hold current belief state, execute current action, observing observation o. regress history update initial belief state usingregressed formula (lines 19 20). inconsistent (that is, o,h inconsistentsI ) must sample replan (lines 22-24). Otherwise, even inconsistentstate sI SI , sI = sI , continue executing plan.execution plan terminates check see goal met,obtained, sample replan again.5.4.1 B IAS BSERVATION K NOWLEDGE .possible bias SDR planner make observations. crude simple method executesensing action sense unknown value without affecting state. contextcurrent benchmarks improves planners run-time performance. refer versionSDR SDR-Obs.focused method augment goal state requirement provedistinguished initial state correct. refer version SDR state-refutation (SDR-SR).Recall distinguished state determines value propositions initial state,affect knowledge. Thus, Wumpus domain, sll distinguished statewumpus-at-2-3 holds, Kwumpus-at-2-3 not. change goal Kat-4-4Kslr Ksrl Ksrr planner generate plan knowledge effectswell. is, valid plan must prove, e.g., initial state srr , Wumpusesright, invalid. state refutation achieved actions whose effects s,effects obtained following differentiating observation, method encouragesplanner take sensing actions.distinguished initial state unlikely true initial state, plans generatedmodified goal likely quickly identify fact. This, turn, cause replanningtrigger sooner, information. course, necessarily need knowidentity initial state succeed, may also add sensing actions mayrequired optimal plan.6. Related WorkSDR borrows extends ideas various related planners, notably: replanning/onlineplanning, translation-based techniques, lazy belief state representation. briefly discusshere.Replanning recently become popular online planning uncertainty FFReplan (Yoon et al., 2007) MDP solver. Replanning technique planning uncertaintyonline, stage, planner solves simplified problem typically removes uncertainty, e.g., making assumptions value unknown variables effects actions.586fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSplanner executes obtained solution receives information contradicts assumptions, updates model new information, repeats process. example, FF-Replanassumes certain deterministic effects stochastic actions, obtaining classical planning problem.replanning essentially ignores certain aspects model, runs risk getting stuckdead-ends, regions state space difficult reach goal. However, combined smart sampling techniques recently developed stochastic planning problems,UCT (Kocsis & Szepesvari, 2006) , replanning becomes powerful technique. example, FFReplan later improved using idea hindsight optimization (Yoon et al., 2008; Yoon, Ruml,Benton, & Do, 2010), multiple, non-stationary determinizations MDP examined.choice next action guided solution multiple resulting classical planningproblems, enabling planner account different possible future dynamics.noted above, essential element replanning reduction current problemsimpler problem. SDR builds translation-based approach conformant planning introduced Palacios Geffner (2009) generate classical planning problem, specifically,KS0 translation. conformant planning, resulting classical planning problem equivalentoriginal problem (has set solutions), may much larger size. Appliedcontingent planning, translation methods generates non-deterministic, fully observable, planning problem. make problem deterministic, SDR simplifies assuming observationsconform specific initial state. reduce size, SDR samples subset initialstates.Palacios Geffner (2009) suggest different technique controlling problem size.SDR maintain value propositions conditioned initial state, planner maintains value proposition conditioned sets initial states, called tags. Ideally, tagsproposition p contain initial states differ value propositions whose initialvalue affect future value p. sets quite large, implying fewer tagsrequired. This, turn, leads considerable savings size generated problem.set tags required complete translation large, one may use tags deterministicfollowing sense: proposition may different values different states belongingtag. soundness still maintained case, one must sacrifice completeness.CLG planner (Albore et al., 2009) takes different approach extending ideas Palacios Geffner contingent planning. P original contingent planning problem, denoteX(P ) fully-observable non-deterministic problem obtained using transformation Palacios Geffner. CLG solves X(P ) obtaining heuristics relaxed version problemmoves precondition condition effects action similar way doneCFF (Hoffmann & Brafman, 2006), drops non-deterministic effects, introduces propositionsform Ap, roughly say p observed. effect sensing action sensesp thus Ap, rather Kp (which would erroneously imply offline know valuesensed). Actions require p precondition, require Ap classical problem generated.forces classical solver insert sensing action senses p insert actionrequires p. course, actual value sensed online may correspond assumptionsmade rest plan. why, following every sensing action, CLG (in online version) replans. Thus, plan executed first sensing action. initial belief statecorrectly computed, translation ensures fragment plan executed. Thus,CLG SDR use replanning translation classical planning replanning phase.However, SDR uses translation simpler two ways. use Ap type proposi587fiB RAFMAN & HANItions, instead uses sampling determinize sensing actions. addition, SDR uses samplingselect subset possible initial states. leads much smaller classical planning problemsfaster generate solve, less informed. Consequently, often SDR takessteps reach goal, able scale better CLG.Recently, Bonet et. al. introduced replanning-based contingent planner (Bonet & Geffner,2011) called K-planner, similar SDR, focuses special class domainshandled efficiently. domains hidden variables static, i.e., valuehidden propositions change throughout execution plan. example,Wumpus domain, Wumpuses move, location Wumpuses along diagonalstatic. Localize domain, however, hidden current wall configuration changes everymove action. Thus, K-planner unsuitable domain. addition, K-planner assumesvalue observable variables always observed following action i.e., explicitsensing actions.K-planner SDR developed parallel, share many ideas, provide similar theoretical guarantees. K-planner uses replanning translation classical planning muchlike SDR (and CLG). Whereas SDRs translation assumes sensed value correspondsampled initial state, K-planner actions correspond making assumptions sensedvalues. is, real sensing actions translated multiple classical actions lead knowledge different values sensed variable. Thus, classical planner may choose valuewould like sensor sense. essentially allows planner make optimistic assumptions ensuring goal focused sensing. However, multiple sensing actions plan mayassumed effects cannot realized initial state. may affect qualityclassical plan generated, lead unsound behavior because, online, plan executedfirst sensing action, belief state updated replanning takes place.SDRs use select initial state ensures sensed values consistent, maypessimistic cannot conceive sensed value different dictatedspecial initial state selected.noted above, K-planner makes certain assumptions nature uncertaintydomain, one important contribution makes showing properties leveragedprovide efficient representation stronger guarantees. Essentially, static hiddenvariable assumption made K-planner viewed saying uncertainty domainessentially value multi-value variable, conditional effects dependvalue variable. case, one represent belief state compactly onegenerate compact translations classical planning. Bonet Geffner prove soundnesscompleteness K-planner similar assumptions made here, also showadditional assumption nature uncertainty domain discussed above,achieve properties generating classical planning problems linear sizeoriginal planning problem.Like planners operate uncertainty, SDR maintains form belief state.SDR taken lazy approach belief state maintenance extreme. lazy, implicitapproach belief state maintenance introduced CFF (Hoffmann & Brafman, 2006). CFFmaintains single complete formula describes history initial state jointly.requires using different copy state variables time point. action applied stepconjoined formula clauses describe value variables time + 1function value time t. determine whether condition c holds current belief state,588fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONScurrent formula conjoined c. resulting formula consistent, knowpossible states c holds. Otherwise, know c valid. example, cgoal condition, know plan found. c precondition action a,know applied safely. CFF also caches information discovered simplifyingformula whenever conclusion obtained, via unit propagation. regression methoduse thought constructing, query, part CFF formulaneeded answering current query. downside approach could,principle, reconstruct formula, parts formula repeatedly. advantageformulas construct much smaller easier satisfy complete CFF formula.Many belief state representations explored literature, including binary decisiondiagrams (Bryce, Kambhampati, & Smith, 2006), DNF CNF representations, Prime Implicates Prime Implicants (To et al., 2011b, 2011, 2011a, 2009, 2010). Overall, (2011)concludes different domains require different representations. is, belief representation method works well domains certain structure actions, welldomains. would interesting compare various belief representation methods lazyregression based technique large set benchmarks, leave future research.et al. also suggest number contingent planners, built using belief representations AND/OR forward search algorithm, generate complete plan trees contingentproblems. planners CFF, POND (Bryce et al., 2006), CLG offline mode alsocompute complete plan trees. general, plan trees exponential, must producedifferent plans possible initial state worst case. Thus, offline contingent planninginherently difficult scale larger domains many possible initial states.7. Experimental Resultsdemonstrate power replanning approach compared SDR state artcontingent planner CLG (Albore et al., 2009). use CLG so-called execution mode,becomes online planner. compare SDR CLG domains CLG paper:Wumpus: grid-based navigation problem, agent must move lowest leftcorner upper right corner. diagonal grid surrounded squares containingeither monsters called Wumpuses, pits. agent must ensure square safe, i.e.monster pit, entering it. Squares cannot directly checked safety. Instead,squares surrounding Wumpus agent smell stench, squares surroundingpit agent feel breeze. agent know, though, given breeze stenchneighboring squares unsafe. ensure safety agent must use informationcollected various squares around suspected squares. domain demonstratescomplex hidden state multiple sensing actions, conditional effects,key bottleneck CLG translation. Thus, domain unknown featuresenvironment fixed, uncertainty propagate propositions , i.e.known features become unknown. type domains K-plannerhandle.Doors: grid-based navigation task, agent must move leftgrid right. Along way walls passage (unlocked door)single location. sense whether door unlocked agent must try it. naive589fiB RAFMAN & HANIstrategy hence move along wall try doors unlocked one found.Then, agent start new wall. domain exhibits simple strategiesconditional effects. Like Wumpus domain, uncertainty propagated here.Color-balls: domain several colored balls hidden grid, agent must findballs transfer bin appropriate color. domain large statespace, multiple sensing actions, conditional effects. Here, again, propositionswhose values known cannot become unknown.Unix: domain agent must find file folder tree transfer root.smart technique searching tree agent must exhaustively checkevery subfolder. domain contain conditional effects. domainuncertainty propagate.Localize: yet another grid-based navigation problem, agent must reachtop right corner grid. However, agent unaware position within grid.agent sense nearby walls, hence deduce position within grid. domainpresents complex conditional effects domains, thus difficultscaling using CLG translation. domain, due conditional effectsunknown features, uncertainty propagate, i.e. known features may change valuedepending value variables whose value unknown, causing them, turn, becomeunknown. example, sense wall right, move knowledgeobserving wall right lost. domain unsuitable K-planner.clear description benchmark domains somewhat limited. Specifically, domains use conditional effects, challenging partialobservability. dead-ends. come back issue Section 7.5. Nevertheless,assess performance SDR comparison current state art, evaluatedomains, allow scaling using larger grids, balls forth.implemented SDR algorithm using C#. experiments conducted WindowsServer 2008 machine 24 2.66GHz cores (although experiment uses single core)32GB RAM. used FF (Hoffmann & Nebel, 2001) compiled Cygwin environmentsolving deterministic problems, used Minisat SAT solver (Een & Sorensson,2003) search satisfying assignments (or lack of). Minisat provide randomassignments, also implemented naive solver generates random assignments trueenvironment states.7.1 Comparing Plan Quality Execution Timebegin comparing SDR variants CLG number benchmarks. compute averagenumber actions average time several (25) iterations problem instance,iteration initial state uniformly sampled. Arguably, computing averages casecontingent planning perhaps incorrect estimation, averages assume uniform samplingconditions (initial states case), part PPOS formalism. Indeed, definingreasonable comparison metric online contingent planners still open question, lackbetter measure, report measured averages.590fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSNamecloghugeelog7ebtcs70CB9-1CB9-3CB9-5CB9-7doors5doors7doors9doors11doors13doors15doors17localize3localize5localize9localize11localize13localize15localize17#Actions82.42( 0.64 )22.08( 0.05 )33.96( 0.8 )244( 6.16 )841.44( 7.23 )1068.6( 4.44 )Failed21.6( 0.22 )47.76( 0.52 )97.76( 1.1 )148.44( 1.3 )229.04( 1.7 )343.26( 2.77 )519.13( 4.7 )8( 0.12 )14.56( 0.24 )28.52( 0.42 )34.67( 0.61 )37.52( 0.62 )40.08( 0.61 )45( 0.86 )SDRTime(secs)321.28( 9.32 )1.81( 0.02 )17.4( 0.38 )214.1( 7.65 )912.73( 18.7 )1356( 10.7 )3.76( 0.05 )18( 0.26 )72.5( 0.87 )216.52( 4.4 )524.03(7)1086( 20 )1582( 26 )1.77( 0.03 )7.12( 0.1 )72.69( 1.43 )155.6( 3.87 )396.76( 10.72 )667.22( 19.7 )928.56( 33.2 )SDR-obs#Actions Time(secs)61.17117.13( 0.44 )( 4.19 )21.760.85( 0.07 )( 0.01 )35.523.18( 0.75 )( 0.07 )124.5671.02( 2.49 )( 1.57 )247.28245.87( 2.91 )( 4.03 )392.16505.48( 2.81 )( 8.82 )487.04833.52( 2.95 )( 15.82 )18.042.14( 0.18 )( 0.03 )35.369.29( 0.41 )( 0.1 )51.8428( 0.55 )( 0.31 )88.0479.75( 0.91 )( 1.04 )120.8158.54( 0.93 )( 2.01 )143.24268.16( 1.36 )( 3.78 )188416.88( 1.64 )( 6.16 )8.880.81( 0.11 )( 0.01 )15.322.87( 0.21 )( 0.04 )29.4426.61( 0.47 )( 0.48 )41.277.11( 0.83 )( 1.97 )56.96159.53( 0.69 )( 4.18 )68.44352.36( 0.9 )( 9.72 )81.24527.53( 1.16 )( 15.25 )SDR-SR#Actions Time(secs)82786.17( 0.42 )( 31.19 )21.521.67( 0.05 )( 0.02 )3525.18( 0.64 )( 0.39 )264.24140.64( 5.65 )( 5.21 )665.4565( 6.37 )( 18.1 )918.07716( 4.72 )( 15.3 )Failed16.64( 0.23 )40.52( 0.45 )77.68( 0.92 )125.08(1)185.64( 1.88 )252.24( 2.04 )299.28( 2.78 )8.88( 0.14 )13.08( 0.22 )22.12( 0.43 )31.12( 0.6 )39.96( 0.54 )50.63( 0.59 )59.48( 0.81 )3.05( 0.04 )17.87( 0.22 )61.57( 0.9 )174.04( 1.75 )383.88( 5.42 )725.76( 8.52 )1089( 17 )1.95( 0.02 )8.24( 0.16 )81.44( 1.12 )199.35( 3.84 )387.75( 7.01 )721.53( 13.33 )1031( 25 )#Actions51.76( 0.33 )20.12( 0.05 )36.52( 0.86 )94.36( 1.83 )252.76( 2.66 )PFCLGTime(secs)8.25( 0.08 )1.4( 0.08 )73.96( 0.14 )129.3( 0.26 )819.52( 0.47 )TF16.44( 0.18 )30.4( 0.24 )50.48( 0.5 )71.68( 0.79 )105.48( 0.89 )PF2.4( 0.1 )20.44( 0.02 )38.52( 0.06 )126.59( 0.1 )330.73( 0.21 )PFCSUCSUCSUPFPFPFPFTable 1: Comparing CLG (execution mode) various SDR methods. domains conditional actions(localize) CLG execution cannot simulated. TF denotes CLG translation failed; CSU denotesCLG cannot run simulation uniform distribution; PF denotes CLG planner failed, either duemany propositions due timeout.Table 1 Table 2 lists results various SDR methods CLG. report resultspure SDR, SDR observation bias (denoted SDR-obs), SDR state refutation addedgoal (denoted SDR-SR). method report average number actions591fiB RAFMAN & HANINameunix1unix2unix3unix4wumpus5wumpus10wumpus15wumpus20SDR#Actions Time(sec)9.40.46( 0.16 )( 0.01 )31.042.01( 0.79 )( 0.05 )78.489.61( 2.23 )( 0.28 )195.853.1( 4.73 )( 1.38 )27.199.8( 0.39 )( 0.16 )45.18102.08( 1.57 )( 2.17 )61.2464.74( 1.01 )( 10.05 )80.331296( 1.47 )( 21 )SDR-obs#Actions Time(sec)12.20.48( 0.16 )( 0.01 )26.441.41( 0.72 )( 0.03 )56.325.47( 1.72 )( 0.18 )151.7235.22( 4.12 )( 0.94 )34.726.51( 0.3 )( 0.07 )70.6465.89( 1.13 )( 1.13 )120.14324.32( 2.4 )( 7.14 )173.21773.01( 3.4 )( 20.78 )SDR-SR#Actions Time(sec)9.325.28( 0.18 )( 0.95 )29.282.33( 0.7 )( 0.05 )10021.19( 2.12 )( 1.04 )202.2478.81( 6.02 )( 2.38 )26.489.37( 0.24 )( 0.1 )39.7254.21( 0.49 )( 0.72 )72.32368.53( 1.04 )( 12.48 )112.33952.52( 3.12 )( 17.62 )CLG#Actions Time(sec)11.680.35( 0.23 )( 0.01 )19.882.69( 0.47 )( 0.01 )51.3218.56( 0.97 )( 0.05 )90.8189.41( 2.12 )( 0.6 )24.122.38( 0.1 )( 0.09 )40.4436.29( 0.18 )( 0.04 )101.12330.54( 0.67 )( 0.25 )155.321432( 0.95 )( 0.47 )Table 2: Comparing CLG (execution mode) various SDR methods. results averaged 25executions, standard error reported brackets.NamelogisticsCBdoorslocalizeunixWumpusOverallLargestSDR#Actions Time1000000010204020SDR-obs#Actions Time023437070302625410SDR-SR#Actions Time0000106010109020CLG#Actions Time211050311212431Table 3: Counting number times method performed best category. bottom rowshows results two largest problems category (except first category cloghugeconsidered 11 problems overall).average time (seconds) goal reached 25 iterations (standard error reported brackets). Different executions correspond case different selections initial states,deterministic PPOS, initial state governs complete observation behavior. SDR variants,various executions also correspond different possible samplings states current beliefstate.offline contingent planners compute complete plan tree, execution time (as opposedplanning time) negligible. case online replanning algorithms, however, execution timeencapsulates various important aspects performance, belief update computation,time required replanning episodes. real applications, controlling robots, wouldtranslate time required system deciding next action. timeconsiderable, possible robot would stop operating order computenext action. Thus, execution time important factor deciding online replanningapproach appropriate.592fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSExecution time CLG includes translation time domain, CLG execution, plan verification time environment, adds seconds execution. translationtimeout 20 minutes, CLG execution also stopped 30 minutes. allowed FFtimeout 2 minutes replanning episode, timeout never reached. casesFF solves deterministic plan seconds. domains CLGs simulator support uniform sampling initial states (denoted CSU Table 1). domains SDR scaleslarger instances CLG, problem crucial comparison. domainbolded shortest plan fastest execution.see Table 3, SDR SDR-obs typically faster CLG, differencegrows scale larger instances. SDR variants also scale larger problems CLGfails. benchmark domains, observation bias SDR-obs resulted faster execution,typically beneficial SDR variants learn much concerninghidden state early possible, replan accordingly.domains planners solve, efficiency (in terms avg. steps reach goal)mixed. CLG clearly generates shorter plans Unix Doors, many instances SDRobs better, larger Wumpus instances, SDR SDR-SR better. surprisinggeneral CLG produces shorter plans, considers contingencies (the completebelief state), also reason difficulty scaling larger domains. CLGproduces plans lesser quality, speculate heuristic search beliefspace embedded CLG, due problems translation approach.SDR also computes much smaller translated domain descriptions, ranging 10KB 200KB.However, direct comparison CLG impossible SDR generates parameterized domains CLG generates grounded translations, hence provide detailed resultsmodel sizes.conclusion, expected, one interested shorter plans, CLG seemsappropriate candidate, one wishes scale larger domains, SDR variants betterchoice.7.2 K-PlannerK-Planner (Bonet & Geffner, 2011) state art planner handles PPOS problemsstatic hidden variables use explicit sensing actions, assumes possibleobservations immediately available upon entering state. domains maintenancebelief state especially simple, translation relatively easy generate. Thus,surprise K-Planner performs much better SDR CLG domains.Looking Table 4 one see differences especially pronounced caseWumpus domains, SDR-OBS significant overhead updating belief. Furthermore,K-Planner employs optimistic heuristic, especially appropriate Wumpus domain.K-Planner simply assumes top-right square safe, goes immediately.square safe, traces back finds passage top-left square, goes directly.domains, optimistic heuristic successful.conclude, K-Planner far best approach domains static hidden variables,difficult see direct extension K-Planner handles types PPOS problems.593fiB RAFMAN & HANINameCB-9-1CB-9-3CB-9-5CB-9-7doors5doors7doors9doors11doors13doors15doors17unix1unix2unix3unix4Wumpus05Wumpus10Wumpus15Wumpus20SDR-obs#ActionsTime(secs)124.56 ( 2.49 ) 71.02 ( 1.57 )247.28 ( 2.91 ) 245.87 ( 4.03 )392.16 ( 2.81 ) 505.48 ( 8.82 )487.04 ( 2.95 ) 833.52 ( 15.82 )18.04 ( 0.18 )2.14 ( 0.03 )35.36 ( 0.41 )9.29 ( 0.1 )51.84 ( 0.55 )28 ( 0.31 )88.04 ( 0.91 )79.75 ( 1.04 )120.8 ( 0.93 )158.54 ( 2.01 )143.24 ( 1.36 ) 268.16 ( 3.78 )188 ( 1.64 )416.88 ( 6.16 )12.2 ( 0.16 )0.48 ( 0.01 )26.44 ( 0.72 )1.41 ( 0.03 )56.32 ( 1.72 )5.47 ( 0.18 )151.72 ( 4.12 ) 35.22 ( 0.94 )34.72 ( 0.3 )6.51 ( 0.07 )70.64 ( 1.13 )65.89 ( 1.13 )120.14 ( 2.4 )324.32 ( 7.14 )173.21 ( 3.4 )773.01 ( 20.78 )K-Planner#ActionsTime(secs)117.04 ( 10.99 ) 34.83 ( 3.9 )219.6 ( 10.09 )60.63 ( 3.05 )358.08 ( 15.8 )94.18 ( 3.31 )458.36 ( 14.64 ) 116.63 ( 3.24 )17 ( 1.05 )4.57 ( 0.35 )33.2 ( 1.67 )9.01 ( 0.55 )52.12 ( 2.61 )15.04 ( 0.95 )80.8 ( 3.04 )25.82 ( 1.2 )109.72 ( 4.76 )37.96 ( 1.72 )150.88 ( 4.7 )55.24 ( 2 )188.8 ( 5.79 )79.24 ( 2.62 )9.68 ( 0.85 )3.71 ( 0.2522.04 ( 2.27 )8.13 ( 0.71 )45.48 ( 4.59 )16.87 ( 1.56 )87.04 ( 8.54 )38.81 ( 3.53 )35.76 ( 1.53 )2.45 ( 0.21 )90.52 ( 6.4 )5.39 ( 0.61 )107.64 ( 4.6 )7.17 ( 0.6 )151.52 ( 6.29 )16.03 ( 1 )Table 4: Comparing SDR-OBS K-Planner, domains static hidden variables.7.3 Effects |SI |important parameter algorithm size SI number initial statesdeterministic planner recognizes. number grows, plan must distinguishvarious states hence becomes accurate. However, states use largertranslation deterministic problem, difficult FF handle. examineimpact parameter tested performance SDR function size SI .show Figure 2, plan quality (the number actions reach goal) SDRchange considerably number states. domain significant benefitadding states Wumpus, one sees farther significant improvement beyond8 states. expected, running time grows growth number states.conclude, hence, that, least current domains, need use handfulstates.7.4 Belief Maintenanceexamine efficiency proposed lazy belief maintenance method. natural candidate compare belief maintenance method CFF (Hoffmann & Brafman, 2006,2005) introduced lazy belief-state maintenance method motivated considerations guided us. CFF maintains propositional formula sets propositions representvalue original proposition every time point, thus, p(t) represents value proposition p time t. Initially, formula contains formula describing initial belief stateusing propositions form p(0). CFF updates formula following execution action.executed time certain constraints propositions time + 1 mustsatisfied. constraints capture add delete effects a, well frame axiom.Given formula, check whether literal l holds possible states time t, CFF checks594fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSFigure 2: Effect |SI | number initial states (tags) number actions (solid) executiontime (dashed) Wumpus10, doors 9, localize 9, unix 2.whether l(t) consequence current formula. conclusions cached addingformula simplifying using unit propagation.Although update process used CFF quite simple, still needs maintain potentiallylarge formula perform satisfiability checks it. SDRs method even lazier reconstructsformula every query. formula focused literal question, muchsmaller reconstructed query, although noted above, information cached.natural ask approach provides better trade-off.compare two methods ran SDR belief maintenance method CFFnew method. rest algorithm remains same, two experimentsexecuted using random seeds, differences runtime stem differentbelief maintenance method. experimented domains, report domainsbelief maintenance CFF could solve.Table 5 shows, belief maintenance method CFF scales poorly compared lazyformula construction method. domain differences substantial localize, mainly domain planner quickly learns value propositionsquickly decouple formula set formulas disjoint sets propositions.(2011) suggests number approaches belief state maintenance update. Unfortunately, planners available online6 belief update mechanism deeply tiedplanning mechanism, currently unable independently measure performancevarious belief update methods compare lazy regression approach, althoughcomparison interesting.6. http://www.cs.nmsu.edu/sto/595fiB RAFMAN & HANIDomaincloghugeebtcs-70elog7doors5doors7doors9localize3localize5localize9localize11localize13localize15unix1unix2unix3Wumpus05Wumpus10CFF410.39 (4.94)481.27 (15.89)6.88 (0.94)4.28 (0.06)41.05 (1.02)283.93 (6.86)2.32 (0.04)7.54 (0.18)78.08 (1.69)109 (1.53)458.89 (10.06)909.12 (32.91)0.81 (0.01)6.29 (0.19)427.73 (13.22)21.07 (0.27)688.84 (29.46)SDR321.28 (9.32)17.4 (0.38)1.81 (0.02)3.76 (0.05)18 (0.26)72.5 (0.87)1.77 (0.03)7.12 (0.1)72.69 (1.43)155.6 (3.87)396.76 (10.72)667.22 (19.7)0.46 (0.01)2.01 (0.05)9.61 (0.28)9.8 (0.16)102.08 (2.17)Table 5: Comparing belief maintenance methods CFF SDR. report time (in seconds)solving domains different methods. reported models type largestbelief maintenance method CFF could handle within given timeout.7.5 New Domainsnoted earlier existing benchmark problems contingent planning focus problemslimited features. First, dead-ends, which, noted Little Thiebaux (2007),problematic replanning based methods. Second, fact perform well sampledinitial belief state size 2 seems imply solution sensitive identityinitial state. related fact type amount conditional effects seecurrent domains quite limited. Finally, success sensing bias suggestsinvestigate domains sensing actions carry cost, sensing necessarilyseparate action. Domains sensing actions require substantial effort attain preconditions may also provide interesting insights. Many domains above, colorballs,doors, unix also problematic isnt smart exploration method reducesbelief space faster. domains agent must move location independentlyquery object interest (door, ball, file). agent cant, example, sense whetherdoor current position, thus cutting belief space half.thus suggest number new benchmark domains, either variationscurrent domains, adaptations domains POMDP community. new domainsespecially designed expose difficulties current planners, point towards neededimprovements. such, SDR CLG perform well many them, sometimes failutterly.first explore variations interesting Wumpus problem. Wumpus requires smart exploration sensing policy, thus one challenging benchmarks. originalWumpus definition requires cell safe entering it. removing precondition, changing move action agent alive wumpus pit exist cell,create domain deadends. experimented domain and, expected, SDRvariations fail utterly handle it. CLG, however, solves domains without failing. suc596fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSceeds fully represents belief state hence detects possible statesagent would dead, would avoid consequences. SDR, even complete belief representation, makes strong assumption single initial state, plans state specifically.state agent dead resulting plan, execute without botheringsense deadends. true world state assumed state, eventually enterunsafe cell would die.NameWumpus 4Wumpus 8Wumpus 16SDRSDR-OBSSDR-SRFailFailFailFailFailFailFailFailFailCLG#ActionsTime (secs)17.7 (0.04)0.17 (0.001)40.5 (0.31)2.8 (0.01)119.7 (0.91) 182.5 (1.73)Table 6: Wumpus domains deadendsexperimented Wumpus variation agent enters unsafe cell, runsback start cell (at 1,1). thus cost sensing wumpus. cost introducesinteresting tradeoff agent close start cell, better enter cell withoutverifying safety. However, agent far beginning, better sensesafety rather pay price going back. Here, CLG fails utterly underlying revised FFunable solve translation within given timeout. SDR variations solve model,see sensing bias reduces performance case. Still, closely lookingplans SDR executes, observe suboptimal SDR weigh costssensing vs. entering possibly unsafe square causing restart. Instead, always enterspossibly unsafe square pays restart price.NameWumpus 4Wumpus 8Wumpus 16#Actions13.1(1.13)34.84(0.67)67.08(1.1)SDRTime (secs)3.2(0.04)50.3(1.14)178.9(7.6)SDR-OBS#Actions Time (secs)22.23.4(0.1)(0.03)69.1259.23(0.7)(0.74)FailSDR-SR#Actions Time (secs)175.4(0.14)(0.1)37.951.5(0.7)(1.15)74.8450.1(1.14)(14.2)CLGFailFailFailTable 7: Wumpus domains restartsNext, introduce domain POMDP community, known RockSample (Smith &Simmons, 2004), motivated Mars rover task. agent (the rover) sample mineralsset nearby visible rocks. agent knows rocks are, order knowwhether rock sampled, must activate sensors. version problem,agent sensor senses presence nearby minerals, set antenna. agentextends antenna higher, sensor senses minerals farther of. antennacompletely folded, agent senses minerals immediate vicinity. Solving problemsmartly, without visiting rocks contain minerals, requires smart sensing strategy, involving raising lowering antenna order know rocks contain minerals.SDR currently solves problem, smartly. example, SDR observationbias (SDR-OBS) significant advantage get additional observations,getting long-range observations requires preparation actions.597fiB RAFMAN & HANINameRockSample 4RockSample 8RockSample 12RockSample 14#Actions42.2(0.4)85.08(0.65)127.24(0.68)142.08(0.8)SDRTime (secs)37.2(0.36)109.3109.3 (1.15)113.4(0.79)146.75(1.19)SDR-OBS#Actions Time (secs)45.332.12(0.41)85.592(0.62)(0.93)125.36101.2(0.81)(0.75)145.04128.2(0.63)(0.8)SDR-SR#Actions Time (secs)45.637.2(0.39)(0.38)89.12106.04(0.63)(1.12)120.72111.66(0.64)(0.79)146.84139.3(0.86)(1.14)CLGCSUCSUCSUCSUTable 8: RockSample domains 8 8 board 4 14 rocks. CLG properly simulateunderlying world state observations given conditional effects thus performance cannotevaluated, even though manages solve domains (denoted CSU).Finally, also experiment another domain explored POMDP communitywell-known MasterMind game, agent must guess correctly order color khidden pegs n possible colors. agent guesses configuration, receives feedbackform number correctly guessed colors number correctly guessed locations.problem interesting agent never directly observes crucial features state,i.e., peg currently correctly guessed. Furthermore, exists optimal sensing strategyprovably solves game five guesses less 4 pegs 6 colors (MasterMind 6 4)(Koyama & Lai, 1993). CLG cannot solve problem problem-width1. SDR SDR-SR poorly task use simplest possible strategyguess setting see guess correct, i.e., whether result n location hits, rulingsingle state guess. SDR observation bias better, observesnumber location color hits, thus ruling many states guess.noteworthy underlying FF planner particularly badly translations generatedtask, executing many guesses without observing results obvious reason.NameMasterMind 2 4MasterMind 3 4MasterMind 4 6#Actions25.6(0.52)63.5(1.5)SDRTime (secs)8.2(0.16)46.07(1.09)FailSDR-OBS#Actions Time (secs)14.482.68(0.134)(0.02)26.769.44(0.47)(0.12)52.7274.18(1.08)(0.68)SDR-SR#Actions Time (secs)28.488.2(0.6)(0.2)66.852.9(1.42)(1.22)FailCLGTFTFTFTFTFTFTable 9: MasterMind color guessing game. MasterMind n k stands MasterMind n pegs k colors.CLG cannot translate problem width 1.8. Conclusiondescribed SDR, new contingent planner extends replanning approach domainspartial observability uncertainty initial state. SDR also introduces novel, lazymethod maintaining information querying current belief state, nice theoretical properties. empirical evaluation shows SDR improves state art current598fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONSbenchmark domains, scaling much better CLG. However, success current simplesampling techniques also highlights weakness current benchmark problems. highlight this,generated new problem domains challenging current contingent planners,serve measure progress area.Acknowledgmentsauthors grateful Alexander Albore, Hector Geffner, Blai Bonet, Son helpunderstanding using systems anonymous referees many useful suggestionscorrections. Ronen Brafman partially supported ISF grant 1101/07, Paul Ivanier Center Robotics Research Production Management, Lynn William Frankel CenterComputer Science.ReferencesAlbore, A., Palacios, H., & Geffner, H. (2009). translation-based approach contingent planning.IJCAI, pp. 16231628.Bonet, B., & Geffner, H. (2011). Planning partial observability classical replanning: Theory experiments. IJCAI, pp. 19361941.Brafman, R. I., & Tennenholtz, M. (2003). Learning coordinate efficiently: model-based approach. Journal Artificial Intelligence Research (JAIR), 19, 1123.Bryce, D., Kambhampati, S., & Smith, D. E. (2006). Planning graph heuristics belief spacesearch. JOURNAL AI RESEARCH, 26, 3599.Een, N., & Sorensson, N. (2003). extensible sat-solver. SAT, pp. 502518.Hoffmann, J., & Brafman, R. I. (2005). Contingent planning via heuristic forward searchimplicit belief states. ICAPS, pp. 7180.Hoffmann, J., & Brafman, R. I. (2006). Conformant planning via heuristic forward search: newapproach. Artif. Intell., 170(6-7), 507541.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristicsearch. JAIR, 14, 253302.Kearns, M. J., & Singh, S. P. (2002). Near-optimal reinforcement learning polynomial time.Machine Learning, 49(2-3), 209232.Kocsis, L., & Szepesvari, C. (2006). Bandit based monte-carlo planning. ECML, pp. 282293.Kolobov, A., Mausam, & Weld, D. S. (2010). Classical planning MDP heuristics: littlehelp generalization. ICAPS, pp. 97104.Koyama, M., & Lai, T. (1993). Circumscription: form non-monotonic reasoning. JournalRecreational Mathematics, 25, 251256.Little, I., & Thiebaux, S. (2007). Probabilistic planning vs. replanning. ICAPS Workshop IPC:Past, Present Future.McCarthy, J., & Hayes, P. J. (1969). philosophical problems standpoint artificialintelligence. Meltzer, B., & Michie, D. (Eds.), Machine Intelligence, Vol. 4, pp. 463502.599fiB RAFMAN & HANIPalacios, H., & Geffner, H. (2009). Compiling uncertainty away conformant planning problemsbounded width. JAIR, 35, 623675.Reiter, R. (1991). frame problem situation calculus: simple solution (sometimes)completeness result goal regression. Lifshitz, V. (Ed.), AI Mathematical TheoryComputation: papers honour John McCarthy, pp. 359380.Smith, T., & Simmons, R. (2004). Heuristic search value iteration POMDPs. UAI 2004,Banff, Alberta.To, S. T. (2011). impact belief state representation planning uncertainty.IJCAI, pp. 28562857.To, S. T., Pontelli, E., & Son, T. C. (2009). conformant planner explicit disjunctive representation belief states. ICAPS.To, S. T., Pontelli, E., & Son, T. C. (2011). effectiveness cnf dnf representationscontingent planning. IJCAI, pp. 20332038.To, S. T., Son, T. C., & Pontelli, E. (2010). use prime implicates conformant planning.AAAI.To, S. T., Son, T. C., & Pontelli, E. (2011a). Conjunctive representations contingent planning:Prime implicates versus minimal cnf formula. AAAI.To, S. T., Son, T. C., & Pontelli, E. (2011b). effectiveness belief state representationcontingent planning. AAAI.Yoon, S. W., Fern, A., & Givan, R. (2007). FF-Replan: baseline probabilistic planning.ICAPS.Yoon, S. W., Fern, A., Givan, R., & Kambhampati, S. (2008). Probabilistic planning via determinization hindsight. AAAI, pp. 10101016.Yoon, S. W., Ruml, W., Benton, J., & Do, M. B. (2010). Improving determinization hindsighton-line probabilistic planning. ICAPS, pp. 209217.Zelinsky, A. (1992). mobile robot exploration algorithm. IEEE Trans. Robotics Automation, 8(6).600fiJournal Artificial Intelligence Research 45 (2012) 685-729Submitted 08/12; published 12/12Time Complexity Approximate HeuristicsMultiple-Solution Search SpacesHang Dinhhtdinh@iusb.eduDepartment Computer & Information SciencesIndiana University South Bend1700 Mishawaka Ave. P.O. Box 7111South Bend, 46634 USAHieu Dinhhieu.dinh@mathworks.comMathWorks3 Apple Hill DriveNatick, 01760-2098 USALaurent MichelAlexander Russellldm@engr.uconn.eduacr@cse.uconn.eduDepartment Computer Science & EngineeringUniversity Connecticut371 Fairfield Way, Unit 2155Storrs, CT 06269-2155 USAAbstractstudy behavior search algorithm coupled heuristic h satisfying(1 1 )h h (1 + 2 )h , 1 , 2 [0, 1) small constants h denotes optimalcost solution. prove rigorous, general upper bound time complexity searchtrees depends accuracy heuristic distribution solutions.upper bound essentially tight worst case; fact, show nearly matching lower boundsattained even non-adversarially chosen solution sets induced simple stochasticmodel. consequence rigorous results effective branching factor searchreduced long 1 + 2 < 1 number near-optimal solutions search treelarge. go provide upper bound search graphs contextestablish bound running time determined spectrum graph.experimentally explore extent rigorous upper bounds predict behaviornatural, combinatorially-rich search spaces. begin applying solveknapsack problem near-accurate admissible heuristics constructed efficient approximation algorithm problem. additionally apply analysis search partialLatin square problem, provide quite exact analytic bounds number nearoptimal solutions. results demonstrate dramatic reduction effective branching factorcoupled near-accurate heuristics search spaces suitably sparse solution sets.1. Introductionclassical search procedure (Hart, Nilson, & Raphael, 1968) method bringing heuristicinformation bear natural class search problems. One celebrated featurescoupled admissible heuristic function, is, one always returns lower bounddistance solution, guaranteed find optimal solution. worst-casebehavior (even admissible heuristic function) better of, say, breadthfirst search, practice intuition suggest availability accurate heuristicdecrease running time. Indeed, methods computing accurate admissible heuristic functionsvarious search problems presented literature (see, e.g., Felner, Korf, & Hanan,2004). article, investigate effect accuracy running time search;2012 AI Access Foundation. rights reserved.fiDinh, Dinh, Michel, & Russellspecifically, focus rigorous estimates running time coupled accurateheuristics.initial notion accuracy adopt motivated standard framework approximation algorithms: f () hard combinatorial optimization problem (e.g., permanentmatrix, value Euclidean traveling salesman problem, etc.), algorithm efficient -approximation f runs polynomial time (1 )f (x) A(x) (1 + )f (x),inputs x, f (x) optimal solution cost input x A(x) solution costreturned algorithm input x. approximation algorithms community developedefficient approximation algorithms wide swath NP-hard combinatorial optimization problems and, cases, provided dramatic lower bounds asserting various problems cannotapproximated beyond certain thresholds (see Vazirani, 2001; Hochbaum, 1996, surveysliterature). Considering great multiplicity problems successfully addressedway (including problems believed far outside NP, like matrix permanent), naturalstudy behavior coupled heuristic function possessing properties. Indeed,interesting cases (e.g., Euclidean travelling salesman, matrix permanent, knapsack), hardcombinatorial problems approximated polynomial time within fixed constant > 0;cases, polynomial depends constant . remark, also, many celebratedapproximation algorithms provable performance guarantees proceed iterative update methods coupled bounds local change objective value (e.g., basis reduction Lenstra,Lenstra, & Lovasz, 1981, typical primal-dual methods Vazirani, 2002).Encouraged possibility utilizing heuristics practice natural questionunderstanding structural properties heuristics (and search spaces) indeed guaranteepalatable performance part , study behavior provided heuristicfunction -approximation cost cheapest path solution. certain naturalsituations arise approximation quality asymmetric (i.e., case admissible heuristic),slightly refine notion accuracy distinguishing multiplicative factors two sidesapproximation: say heuristic h (1 , 2 )-approximation actual costfunction h , simply (1 , 2 )-approximate, (1 1 )h h (1 + 2 )h . particular, admissibleheuristics -approximation (, 0)-approximate. call heuristic -accurate(1 , 2 )-approximate = 1 + 2 . detailed description appears Section 2.1.1.1 Sketch Resultsinitially model search space infinite b-ary tree distinguished root. probleminstance determined set nodes treethe solutions problem. costassociated solution simply depth. search procedure equipped (i.)oracle which, given node n, determines n S, (ii.) heuristic function h, assignsnode n tree estimate actual length h (n) shortest (descending) pathsolution. Let solution set first (and hence optimal) solution appearsdepth d. establish family upper bounds number nodes expanded : h(1 , 2 )-approximation h , finds solution cost worse (1 + 2 )d expands2b(1 +2 )d + dN1 +2 nodes, N denotes number solutions depth less(1 + )d. See Lemma 3.1 stronger results. emphasize bound appliessolution space generalized search models non-uniform branching factorsnon-uniform edge costs (see Section 5).go show upper bound essentially tight; fact, show boundnearly achieved even non-adversarially determined solution spaces selected according simplestochastic rule (see Theorems 3.1 4.1.). remark bounds running time fallrapidly accuracy heuristics increases, long number near-optimal solutionslarge (although may grow exponentially). instance, effective branching factorguided admissible -accurate heuristic reduced b N = O(bd ). However,686fiThe Time Complexity Approximate Heuristicsworst cases, occur search space overwhelming number near-optimalsolutions, still expand almost many nodes brute-force does, regardless heuristicaccuracy. Likewise, strong guarantees < 1 are, general, necessary effect appreciablechanges average branching factor. discussed Theorem 4.2.establishing bounds tree-based search model, examine time complexitygraph unrolling graph equivalent tree bounding numbernear-optimal solutions tree lift solution original graph. appearsSection 6. Using spectral graph theory, show number N lifted solutionstree corresponding b-regular graph G O((1+)d ), assuming optimal solution depthO(logb |G|) number solutions G constant, second largest eigenvalue (inabsolute value) adjacency matrix G. particular,for almost b-regular graphsb grow size graphs, 2 b, yields effective branchingfactor search graphs roughly 8b(1+)/2 heuristic -accurate.also experimentally evaluate heuristics.Experimental Results Relationship Practice. course, upperbounds interesting reflect behavior search problems practice. boundsguarantee, general, E, number nodes expanded -accurate heuristic,satisfiesE 2bd + dN .plausible condition N bd , simply E cbd node expansionsconstant c depend (c may depend k and/or properties searchspace). suggests hypothesis hard combinatorial problems suitably sparsenear-optimal solutions,E cbdor, equivalently,log E log c + log b .(1)particular, suggests linear dependence log E .explore hypothesis, conducted battery experiments natural search-treepresentation well-studied Knapsack Problem. obtain admissible -accurate heuristic applying Fully Polynomial Time Approximation Scheme (FPTAS) problem duework Ibarra Kim (1975) (see also Vazirani, 2001, p. 70), provides usconvenient method varying without changing parameters search. remarknatural search space problem quite irregular edge-weighted directed graphavoid reopening node. Thus, search space equivalent one spanningsubtrees terms behaviors. order focus computationally nontrivial examples,generate Knapsack instances distributions empirically hard best known exactalgorithms (Pisinger, 2005). results experiments yield remarkably linear behavior (oflog E function ) quite wide window values: indeed, tests yield R2 correlationcoefficients (of least-square linear regression model) excess 90% range (.5, 1)Knapsack instances. See Section 7.1 details.experimental results discussed Knapsack problem support linearscaling (1), several actual parameters search unknown: example, cannot rulepossibility approximation algorithm, asked produce -approximation,fact produce significantly better approximation. seems far-fetched,behavior could provide spurious evidence linear scaling. explore hypothesisdetail, additionally explore artificial search space partial Latin square completion(PLS) problem provide precise control (and, fact, N ). PLS problemfeatured number benchmarks local search complete search methods. Roughly,problem finding assignment values empty cells partially filled n n tablerow column completed table permutation set {1, . . . , n}.formulation problem, search space 2n-regular graph, thus brute-force branching687fiDinh, Dinh, Michel, & Russellfactor 2n. search space, controlling N , prove asymptotic upper bound(1 + ) (1 + 1/) n effective branching factor coupled -accurate heuristic.also experimentally evaluate effective branching factor admissible -accurateheuristic (1)h , expands nodes admissible -accurate heuristicstrictly larger (1 )h .remark PLS problem well-studied natural, invent specific searchspace structure problem allows us analytically control number near-optimalsolutions. Unlike Knapsack problem, construct efficient admissible -accurateheuristic every fixed thanks given FPTAS, known approximation algorithms PLSproblem much weakerthey provide approximations specific constants (1/e). avoidhurdle, construct instances PLS known solution, extract heuristics(1 )h . Despite planted solutions contrived heuristics, infrastructure providesexample combinatorially rich search space known solution multiplicity heuristicknown quality, provides means experimentally measuring relationshipheuristic accuracy running time. empirical data results remarkable agreementtheoretical upper bounds. subtly, empirically analyzing linear dependence log E, see effective branching factor using heuristic (1 )h given PLSsearch space roughly (2n)0.8 ; see Section 7.2.far aware, first experimental results explore relationshipE. Understanding heuristic accuracy solution space structure general (andensuing bounds running time) problems heuristics practical interest remainsintriguing open problem. remark problems (n2 1)-puzzle,extensively used test cases , seems difficult find heuristics accuracy sufficientsignificantly reduce average branching factor. best rigorous algorithms give ratherlarge constant guarantees (Ratner & Warmuth, 1990; Parberry, 1995): particular, Parberry (1995)shows one quickly compute solutions (and hence approximate heuristics)factor 19 worse optimal; situation somewhat better random instances,establishes 7.5-factor. See Demaines (2001) work general discussion.Observe search algorithm privy heuristic information requires (bd ) runningtime, general, find solution. High probability statements kind madesolution space selected sufficiently rich family. pessimistic lower bounds exist evensituations search space highly structured (Aaronson, 2004). results suggestaccurate heuristic information dramatic impact search, even face substantialsolution multiplicity.article expands conference article (Dinh, Russell, & Su, 2007) complexity-approximate heuristic function studied trees. article, generalizeasymmetric approximation, develop analogous bounds general search spaces, establishingconnection algebraic graph theory, report battery supporting experimental results.1.2 Motivation Related Workalgorithm subject enormous body literature, often investigatingbehavior relation specific heuristic search problem combination, (e.g., Zahavi, Felner,Schaeffer, & Sturtevant, 2007; Sen, Bagchi, & Zhang, 2004; Korf & Reid, 1998; Korf, Reid, &Edelkamp, 2001; Helmert & Roger, 2008). space complexity (Korf, 1985) time complexityaddressed various levels abstraction. Abstract formulations, involving accuracyguarantees like consider, studied, tree models searchspace possesses single solution. single solution framework, Gaschnig (1979) givenexponential lower bounds (bd ) time complexity admissible -accurate heuristics,defb = b/(2) b (see also Pearl, 1984, p. 180), Pohl (1977) studied restrictive(additive) approximation guarantees h result linear time complexity. Average-case688fiThe Time Complexity Approximate Heuristicsanalysis based probabilistic accuracy heuristics also given single-solutionsearch spaces (Huyn, Dechter, & Pearl, 1980). previous analysis suggested effectheuristic functions would reduce effective branching factor search, consistentresults applied single-solution model (the special case N = 1 > 0).single solution model, however, appears inappropriate abstraction searchproblems featuring multiple solutions, recognized . . . presence multiplesolutions may significantly deteriorate ability benefit improved precision. (Pearl, 1984,p. 192) (emphasis added).problem understanding time complexity terms structural properties hmultiple-solution spaces studied Korf Reid (1998), Korf et al. (2001), Korf(2000), using estimate based distribution h() values. particular, studiedabstract search space given b-ary tree concluded effect heuristic functionreduce effective depth search rather effective branching factor (Korf & Reid,1998; Korf et al., 2001). case accurate heuristics controlled solution multiplicity,conclusion directly contradicts findings, indicate dramatic reduction effective branchingfactor cases. explain discrepancy, observe analysis relies equilibrium assumption fails accurate heuristics (in fact, fails even much weaker heuristicguarantees, h(v) h (v) small > 0). basic structure argument, however,naturally adapted case accurate heuristics, case yields reductioneffective branching factor. give detailed discussion Section 8.follow-up Korf Reid (1998), Korf et al. (2001), Korfs (2000) work, Edelkamp(2001) examined (indeed, IDA ) undirected graphs, relying equilibrium assumption.Edelkamps new technique use graph spectrum estimate number n(`) nodescertain depth ` brute-force search tree (same cover tree). However, unlike spectralanalysis, original search graph G, Edelkamp analyzed spectrum relatedequivalence graph, quite different structural properties. Specifically, Edelkamp foundasymptotic branching factor, defined ratio n(`) /n(`1) large `, equals largesteigenvalue adjacency matrix equivalence graph certain Puzzle problems. compare,spectral analysis depends second largest eigenvalue adjacency matrix AGoriginal search graph G, largest eigenvalue AG always equals branching factor,assuming G regular.Additionally, analyses Korf Reid (1998), Korf et al. (2001), Korf (2000) (andtherefore, Edelkamp, 2001) focus particular subclass admissible heuristics, called consistentheuristics. remark heuristics used experiments Knapsack problemadmissible likely inconsistent. Zhang, Sturtevant, Holte, Schaeffer, Felner (2009) Zahaviet al. (2007) discuss usages inconsistent heuristics practice.work explores worst-case average-case time complexity searchtrees graphs multiple solutions coupled heuristics possessing accuracyguarantees. make assumptions regarding consistency admissibility heuristics, thoughseveral results naturally specialized case. addition studying effectheuristic accuracy, results also shed light sensitivity distribution solutionscombinatorial structure underlying search spaces (e.g., graph eigenvalues,measure, among things, extent connectedness graphs). far aware,first rigorous results combining search space structure heuristic accuracy singleframework predicting behavior .2. Preliminariestypical search problem defined search graph starting node set goal nodescalled solutions. instance search graph, however, simulated searchcover tree without reducing running time; discussed Section 6.1. Since number689fiDinh, Dinh, Michel, & Russellexpansions cover tree graph larger equal original graph,sufficient upper bound running time search cover tree. justification,begin considering algorithm search problems rooted tree.Problem Definition Notations. Let tree representing infinite search space,let r denote root . convenience, also use symbol denote set verticestree . Solutions specified nonempty subset nodes . edgeassigned positive number called edge cost. vertex v , letSubTree(v) denote subtree rooted v,Path(v) denote path root r v,g(v) denote total (edge) cost Path(v),h (v) denote cost least costly path v solution SubTree(v). (We writeh (v) = solution exists.)objective value search problem h (r), cost cheapest path root rsolution. cost solution value g(s). solution cost equal h (r)referred optimal.algorithm best-first search employing additive evaluation function f (v) = g(v) +h(v), h function heuristically estimates actual cost h . Given heuristicfunction h : [0, ], algorithm using h defined search problem treedescribed follows:Algorithm 1 search tree1. Initialize Open := {r}.2. Repeat Open empty:(a) Remove Open node v function f = g + h minimum.(b) v solution, exit success return v.(c) Otherwise, expand node v, adding children Open.3. Exit failure.known (e.g., Dechter & Pearl, 1985, Lemma 2) time terminates,always vertex v present Open v lies solution path f (v) ,min-max value defined follows:def= minmax f (u) .(2)sSuPath(s)fact leads following node expansion conditions:vertex v expanded (with heuristic h) must f (v) . (cf., Dechter & Pearl,1985, Thm. 3). say vertex v satisfying f (v) potentially expanded .vertex vmaxf (u) <uPath(v)must expanded (with heuristic h) (cf., Dechter & Pearl, 1985, Thm. 5). particular,function f monotonically increases along path root r v, node vmust expanded f (v) < .690fiThe Time Complexity Approximate Heuristicsvalue obtained solution path search terminates (Dechter &Pearl, 1985, Lemma 3), implies upper bound cost solution foundsearch.remark h reasonable approximation h along path optimal solution,immediately provides control . particular:Proposition 2.1. (See also Davis, Bramanti-Gregor, & Wang, 1988) Suppose 1,h(v) h (v) vertices v lying optimal solution path; h (r).Proof. Let optimal solution. v Path(s),f (v) g(v) + h (v) = g(v) + (g(s) g(v)) g(s) .Hencemaxf (v) g(s) = h (r).vPath(s)particular, = h (r) heuristic function satisfies h(v) h (v) v ,case heuristic function called admissible. observation recovers factalways finds optimal solution coupled admissible heuristic function (cf., Pearl,1984, Thm. 2, 3.1). Admissible heuristics also possess natural dominance property (Pearl, 1984,Thm. 7, p. 81): admissible heuristic functions h1 h2 , h1 informedh2 , i.e., h1 (v) > h2 (v) v \S, using h1 dominates using h2 , i.e., every nodeexpanded using h1 also expanded using h2 .2.1 Approximate HeuristicsRecall introduction shall focus heuristics providing (1 , 2 )-approximationactual optimal cost reach solution:Definition. Let 1 , 2 [0, 1]. heuristic function h called (1 , 2 )-approximate(1 1 )h (v) h(v) (1 + 2 )h (v)v .(1 , 2 )-approximate heuristic simply called -approximate 1 2 .heuristic function h (1 , 2 )-approximate, shall say h heuristic error 1 + 2 , h(1 + 2 )-accurate.see below, two approximation factors control performance searchrather different ways: 1 effects running time , 2 impactrunning time quality solution found . Particularly, special case 2 = 0corresponds admissible heuristics, always finds optimal solution. general,Proposition 2.1, have:Fact 1. h (1 , 2 )-approximate, (1 + 2 )h (r).Hence, solution found using (1 , 2 )-approximate heuristic must cost(1 + 2 )h (r) thus exceeds optimal cost multiplicative factor equal2 .Definition. Let 0. solution cost less (1 + )h (r) called -optimal solution.Assumptions. simplify analysis now, assume search tree b-aryevery edge unit cost unless otherwise specified. case, cost g(v) simplydepth node v h (v) shortest distance v solution descendant v.Throughout, parameters b 2 (the branching factor search space) 1 (0, 1], 2 [0, 1](the quality approximation provided heuristic function) fixed. rule case1 = 0 simplicity.691fiDinh, Dinh, Michel, & Russell3. Upper Bounds Running Time Treesgoing establish upper bounds running time search tree model.first show generic upper bound applies solution space. applygeneric upper bound natural stochastic solution space model.3.1 Generic Upper Boundmentioned introduction, begin upper bound time complexitysearch depending weight distribution solution set, addition heuristicsapproximation factors. shall, fact, upper bound number potentially expanded nodes,clearly upper bound number nodes actually expanded :Lemma 3.1. Let solution set whose optimal solutions lie depth d. Then, every 0,number nodes expanded search tree (1 , 2 )-approximate heuristic2b(1 +2 +1)d + (1 1 )dN1 +2nodes, N number -optimal solutions.presence independent parameter offers flexible way apply upper boundLemma 3.1. particular, applying Lemma 3.1 = 1 using fact 1 1 1,arrive upper bound 2b(1 +2 )d + dN1 +2 mentioned introduction. bound worksbest when1 N1 +2 = (b(1 +2 )d ). general, N1 +2 = O(b(1 +2 )d ), choose least1 N1 +2 = O(b(1 +2 )d ). opposite case, N1 +2 = (b(1 +2 +c)d )positive constant c 1 1 , obtain better bound choosing = 1 c/(1 1 ) < 1, sinceN1 +2 dominates terms (b(1 +2 +1)d ) N1 +2 given choice .Proof Lemma 3.1. Let = h (r) let = 1 + 2 . Consider node v liepath root -optimal solution, h (v) (1 + )d g(v).f (v) g(v) + (1 1 )[(1 + )d g(v)] = (1 1 )(1 + )d + 1 g(v) .Recall node potentially expanded f -value less equal . Since(1 + 2 )d, node v potentially expanded(1 1 )(1 + )d + 1 g(v) > (1 + 2 )d .(3)Since 1 > 0, inequality (3) equivalentg(v) > (2 /1 /1 + 1 + )d = (1 + 2 + 1 )d .words, node depths range(1 + 2 + 1 )d, (1 + 2 )dpotentially expanded lies path root -optimal solution.hand, -optimalsolution path, (1 1 )d nodes depths(1 + 2 + 1 )d, (1 + 2 )d . Pessimistically assuming nodes depth(1 + 2 + 1 )d potentially expanded addition paths -optimal solutionsP`yields statement lemma. (Note b 2, i=0 bi 2b` every potentiallyexpanded node v must depth g(v) f (v) (1 + 2 )d.)1. Recall asymptotic notations: f (n) = (g(n)) means exist constants c1 , c2 > 0 c1 g(n)f (n) c2 g(n) sufficiently large n; f (n) = (g(n)) means exists constant c > 0 cg(n) f (n)sufficiently large n.692fiThe Time Complexity Approximate Heuristics3.2 Upper Bound Natural Search Space Modelactual time complexity depend, course, precise structure h, showbound essentially tight rich family solution spaces. consider sequencesearch problems increasing difficulty, expressed terms depth optimal solution.Stochastic Solution Space Model. parameter p [0, 1], consider solution setobtained independently placing node probability p.setting, random variable written Sp . solutions distributed according Sp ,observe expected number solutions depth precisely pbd p = bdoptimal solution lies depth constant probability. reason, focus specificvalues pd = bd consider solution set Spd > 0. Recall model,likely optimal solutions lie depth and, generally, seehigh probability optimal solutions particular subtree located near depth (withrespect root subtree). make precise below.Lemma 3.2. Suppose solutions distributed according Spk . node v> 0,td1 2btd Pr[h (v) > t] eb .PtProof. tree SubTree(v), n = i=0 bi = (bt+1 1)/(b 1) nodes depths less,Pr[h (v) > t] = (1 bd )n .1 nbd (1 bd )n exp nbd .first inequality obtained applying Bernoullis inequality, last one impliedfact 1 x ex x. Observingbtbt+1 12btb1b 2 completes proof.Observe Spd model, conditioned likely event optimal solutions appeardepth d, expected number -optimal solutions (bd ). situation, accordingLemma 3.1, expands O(b(1 +2 +1)d ) + O(db(1 +2 )d ) vertices expectation,0. leading exponential term bound equalmax {(1 + 2 + 1 )d, (1 + 2 )d} ,minimal = 1. suggests best upper bound inferredfamily bounds Lemma 3.1 poly(d)b(1 +2 )d (for Spd ).discussing average-case time complexity search, record following wellknown Chernoff bound, used control tail bounds analysis later.Lemma 3.3 (Chernoff bound, Chernoff, 1952). Let Z sum mutually independent indicatorrandom variables expected value = E [Z]. > 0,ePr[Z > (1 + )] <.(1 + )1+detailed proof found book Motwani Raghavan (1995). several casesbelow, know exactly expected value variable wish applytail bound Lemma 3.3, compute sufficiently good upper bounds expected value.order Papply Chernoffcase, actually require monotonicity argument:Pboundnn00Z =XZ=Xi=1i=1 sums independent identically distributed (i.i.d.)indicator random variables E [Xi ] E [Xi0 ], Pr[Z > ] Pr[Z 0 > ] .argument applying Lemma 3.3 = e 1, obtain:693fiDinh, Dinh, Michel, & RussellCorollary 3.1. Let Z sum n i.i.d. indicator random variables E [Z] n,Pr[Z > e] < e .Adopting search space whose solutions distributed according Spd , readybound running time average guided (1 , 2 )-approximate heuristic:3Theorem 3.1. Let sufficiently large. probability least 1 ed e2d , searchtree using (1 , 2 )-approximate heuristic function expands 12d4 b(1 +2 )d verticessolutions distributed according random variable Spd .Proof. Let X random variable equal total number nodes expanded(1 , 2 )-approximate heuristic. course exact value of, say, E [X] depends h; proveupper bounds achieved high probability (1 , 2 )-approximate h. Applying Lemma 3.1= 1, concludeX 2b(1 +2 )h (r) + (1 1 )h (r)N1 +2 .Thus suffices control h (r) number N1 +2 (1 + 2 )-optimal solutions.utilize fact Spd model, optimal solutions unlikely locatedfar depth d. end, let Efar event h (r) > + < setlater. Lemma 3.2 immediately gives Pr[Efar ] eb .Observe conditioned Efar , h (r) d+ N1 +2 Z, Z randomvariable equal number solutions depth (1 + 1 + 2 )(d + ).(1+1 +2 )(d+)= 2b(1 +2 )d+(1+1 +2 ) < 2b(1 +2 )d+3E[Z] b 2band, applying Chernoff bound Corollary 3.1 control Z,h3Pr Z > 2eb(1 +2 )d+3 exp 2b(1 +2 )d+3 e2b .Letting Ethick event Z 6b(1 +2 )d+3 , observeh3Pr[Ethick ] Pr Z > 2eb(1 +2 )d+3 e2b .summarize: neither Efar Ethick occurs,X 2b(1 +2 )(d+) + (1 1 )(d + )6b(1 +2 )d+36(d + )b(1 +2 )d+312db(1 +2 )d+3 .Hence,h3Pr X > 12db(1 +2 )d+3 Pr[Efar Ethick ] eb + e2b .infer bound stated theorem, set b = b(1 +2 )d+3 = d3 b(1 +2 )d , completingproof.Remark similar methods, trade-offs error probability resulting boundnumber expanded nodes obtained.694fiThe Time Complexity Approximate Heuristics4. Lower Bounds Running Time Treesestablish upper bounds Theorem 3.1 tight within O(1/ d) termexponent. begin recording following easy fact solution distances discretemodel.Fact 2. Let h (r) nonnegative integer. every solution s, node vPath(s) h (v) = .Proof. Fix distance h (r). prove lemma induction depth solutions.lemma clearly holds optimal solutions. Consider solution may optimal,let v Path(s) node level far h (v) . h (v) < ,must another solution s0 SubTree(v) closer v. induction assumption,node v 0 Path(s0 ) h (v 0 ) = . node v 0 must ancestor v, since distancev s0 less distance v 0 s0 least , completingproof.proceed lower bound.Theorem 4.1.Let sufficiently large. solutions distributed according Spd , probabilityleast 1 b , exists (1 , 2 )-approximate heuristic functionh numbervertices expanded search tree using h least b(1 +2 )d4 /8.Proof. plan define pathological heuristic function forces expand manynodes possible. Note heuristic function allowed overestimate h . Intuitively,wish construct heuristic function overestimates h nodes close solutionunderestimates h nodes far solutions, leading astray whenever possible. Recallevery vertex v, likely solution lying depth SubTree(v). Thus usequantity h (v) formalize intuitive notion node v close solution,quantity < determined later. heuristic function h formally defined follows:((1 + 2 )h (v) h (v) ,h(v) =(1 1 )h (v) otherwise.Observe chance node overestimated small since, Lemma 3.2,Pr[v overestimated] = Pr[h (v) ] 2b(4)node v. Also note node v overestimated ancestor, fvalues monotonically increase along path root v.Naturally, also wish ensure optimal solution close root. Let Ecloseevent h (r) . Lemma 3.2,Pr[Eclose ] 2b .see conditioned event Eclose , means h (r) > , everysolution obscured overestimated node close solution. Concretely,issues integrality, Fact 2 asserts every solution s, must node vpath root h (v) = , long < h (r).Assume Eclose : whenever h (v) = , g(v) h (r) (d ) > 0 h(v) =(1 + 2 )(d ), thus f (v) > (1 + 2 )(d ). Since every solution obscuredoverestimated node whose f value larger (1 + 2 )(d ), > (1 + 2 )(d ),min-max value defined (2). follows node v must expanded Path(v)695fiDinh, Dinh, Michel, & Russellcontain overestimated node f (v) (1 + 2 )(d ). Path(v) containoverestimated node, f (v) = g(v) + (1 1 )h (v),f (v) (1 + 2 )(d ) (1 1 )h (v) (1 + 2 )(d ) g(v) ,since 1 < 1. Therefore, say node v required overestimated node Path(v)(1 1 )h (v) (1 + 2 )(d ) g(v). recap, conditioned Eclose , set required nodessubset set nodes expanded search using defined heuristic function. useChernoff bound control size R` denotes set non-required nodes depth`.Let v node depth ` < (1 + 2 )d. Equation (4) impliesPr[ overestimated node Path(v)] 2`b < 1/16 .last inequality holds sufficiently large d, long = poly(d). hand,1 < 1,(1 + 2 )(d ) `Pr v R` = Pr h (v) >1 1(1+2 )(d)`11exp b(by Lemma 3.2)(1 +2 )d(1+2 )`11.(5)= exp bset ` = (1 + 2 )d (1 + 2 ) logb 4. Equation (5) implieslogd 4Pr v R` exp b 11 e4 1/16 .case 1 = 1, event (1 1 )h (v) > (1 + 2)(d ) ` nevergiven valuefifi happens` set. Hence, case, Pr v R` 1/8 E fiR` fi b` /8. ApplyingChernoff bound Corollary 3.1 yieldsfi fiPr fiR` fi > eb` /8 exp(b` /8) .fi fiLet Ethin event fiR` fi b` /2. Since b` /2 > eb` /8,Pr[Ethin ] exp(b` /8) .Putting pieces together,`Pr expands less b` /2 nodes Pr[Eclose Ethin ] 2b + eb /8 .Setting = 2 ` = (1 + 2 )d 2(1 + 2 ) logb 4, thushPr expands less b(1 +2 )d4 /8 nodes bsufficiently large d.contrast, explore behavior adversarially selected solution set;achieves lower bound nearly tight (in comparison general upper boundworst-case running time obtained setting = 0 bound Lemma 3.1 above).696fiThe Time Complexity Approximate HeuristicsTheorem 4.2. > 1, exists solution set whose optimal solutions lie depth(1 , 2 )-approximate heuristic function h tree using h expandsleast b(1+2 )d12 /1 nodes.Proof. Consider solution set 2 -optimal solutions share ancestor u lying depth1. Furthermore, contains every node depth (1 + 2 )d descendant u,= h (r).define (1 , 2 )-approximate heuristic h follows: h(u) = (1 + 2 )h (u) h(v) =(1 1 )h (v) v 6= u. heuristic, every 2 -optimal solution hidden searchprocedure ancestor u. Precisely, since f (u) = 1 + (1 + 2 )(d 1) = (1 + 2 )d 2 , every2 -optimal solution (which descendant u)maxvPath(s)f (v) f (u) = (1 + 2 )d 2 .Thus (1 + 2 )d 2 , min-max value defined Equation (2).Let v node depth ` (1 + 2 )d lie inside SubTree(u). Notef values monotonically increase along path root r v, implies node v mustexpanded f (v) < . hand, since every non-descendant u depth (1 + 2 )dsolution, ` + h (v) (1 + 2 )d, thusf (v) ` + (1 1 )[(1 + 2 )d `] = (1 1 )(1 + 2 )d + 1 ` .Hence, node v must expanded (1 1 )(1 + 2 )d + 1 ` < (1 + 2 )d 2 , equivalent` < (1 + 2 )d 2 /1 . follows number nodes expanded least(1+2 )d12 /1X`=0(1+2 )d22 /1b`Xb` = b(1+2 )d12 /1 .`=0According Theorem 4.2, set 2 = 0 let 1 arbitrarily small provided 1 > 0,obtain near-accurate heuristic forces expand least many bd1 nodes.lower bound partially explains perform poorly, even almost perfectheuristic, certain applications (Helmert & Roger, 2008): adversarially-chosen solution setgiven proof Theorem 4.2 overwhelming number near-optimal solutions. Indeed,N+2 b(1+2 )d b(1+2 )d1 b(1+2 )d1> 0.5. Generalizations: Non-uniform Edge Costs Branching Factorssection, discuss generic upper bounds Lemma 3.1 generalized applynatural search models non-uniform branching factors non-uniformedge costs; Section 6, show extended general graph search models.consider general search tree without assumptions uniform branching factoruniform edge costs. argument given proof Lemma 3.1, derive assertionheuristic (1 , 2 )-approximate, node cost (1 + 2 + 1 )cpotentially expanded lie (1 + 2 )-optimal solution path,arbitrary nonnegative number c = h (r) optimal solution cost.Hence, number nodes potentially expanded (1 , 2 )-approximate heuristicboundedF (1 + 2 + 1 )c + R (1 + 2 + 1 )c , 1 + 2 .(6)697fiDinh, Dinh, Michel, & RussellF () number nodes cost , call free nodes; R(, )number nodes cost range (, (1 + 2 )c ] lie -optimal solution path,call restricted nodes.bound number free restricted nodes, respectively, assume branchingfactors upper bounded edge costs lower bounded. Let B 2 maximal branchingfactor let minimal edge cost. Since node cost must liedepth larger /m,F () 2B /m .-optimal solution path, ((1 + 2 )c )/m nodes cost range(, (1 + 2 )c ]. Thus,(1 + 2 )cN .R(, )Letting = (1 + 2 + 1 )c , = 1 + 2 , applying bounds F () R(, )bound (6), obtain another upper bound number expanded nodes heuristic(1 , 2 )-approximate:2B (1 +2 +1)c/m+ N1 +2 (1 1 )c /m(7)0. equation (7) generalized version bound Lemma 3.1. Substituting= 1 (7), arrive following simpler upper bound number expanded nodes:2B (1 +2 )c/m+ N1 +2 (1 1 )c /m .(8)6. Bounding Running Time Graphsprevious parts, established bounds running time tree model.apply bounds graph model. order that, first unrollgraph cover tree, bound number solutions lifted cover tree.6.1 Unrolling Graphs Treespreceding generic upper bounds developed tree-based models; section discussnatural extension general graph search models. principal connection obtained unrolling graph tree expands least many nodes originalgraph (including repetitions). specifically, given directed graph G starting node x0 G,define cover tree (G) whose nodes one-to-one correspondence finite-length pathsG x0 . shall write path (x0 , . . . , x` ) G node (G). root (G)(x0 ). parent node (x0 , x1 , . . . , x` ) (G) node (x0 , x1 , . . . , x`1 ), edge costtwo nodes (x0 , x1 , . . . , x`1 ) (x0 , x1 , . . . , x` ) (G) equals cost edge(x`1 , x` ) G. Hence, node P (G), cost value g(P ) equal total edgecost path P G. node (x0 , . . . , x` ) (G) designated solution whenever x`solution G.node (G) corresponds path ending node x G called copy x.Observe solution G may lift multiple times solutions (G), node G maymultiple copies (G). Figure 1 illustrates example unrolling graph cover tree.example, node solution graph first two copies cover tree correspondpaths (0, 3, s) (0, 5, 3, s), 0 starting node given graph.search graph G described Algorithm 2 below, h(x) heuristicnode x, g(x) cost current path x0 x, c(x, x0 ) denotes costedge (x, x0 ) G. assume value h(x) depends x, i.e., h(x) dependparticular path x0 x. Unlike search tree, node x Open Closed,698fiThe Time Complexity Approximate Heuristics016135223611522053Figure 1: Unrolling graph cover tree.Algorithm 2 also keeps track current path P x0 x pointers,current f -value x equal g(P ) + h(x). current path cheapest path x0 xpasses nodes expanded.Algorithm 2 search graph (Pearl, 1984, p. 64)1. Initialize Open := {x0 } g(x0 ) := 0.2. Repeat Open empty.(a) Remove Open place Closed node x function f = g + hminimum.(b) x solution, exit success return x.(c) Otherwise, expand x, generating successors. successor x0 x,i. x0 Open Closed, estimate h(x0 ) calculate f (x0 ) = g(x0 ) + h(x0 )g(x0 ) = g(x) + c(x, x0 ), put x0 Open pointer back x.ii. x0 Open Closed, compare g(x0 ) g(x) + c(x, x0 ). g(x) + c(x, x0 ) <g(x0 ), direct pointer x0 back x reopen x0 Closed.3. Exit failure.consider search cover tree (G) graph G using heuristic function h:node P (G), set heuristic value h(P ) equal h(x) P copy nodex G, i.e., P path G x0 x. Observe cover tree (G) graph G sharethreshold (defined Equation (2)). Hence, whenever node x G expandedcurrent path P , must g(P ) + h(x) , implies P potentially expandedsearch cover tree (G). shows following fact:Fact 3. number node expansions G number nodes potentiallyexpanded (G) using heuristic.Here, node expansion, mean execution expand step , i.e. Step (2c). Notethat, general, node G expanded many times along different paths.Remark running time cover tree also used upper bound running timeiterative-deepening (IDA ) graph. Recall running time IDA dominatedlast iteration. hand, last iteration IDA G merely depth-first search699fiDinh, Dinh, Michel, & Russellcover tree (G) cost threshold . Hence, number expansions lastiteration IDA number nodes potentially expanded covertree.So, upper-bound time complexity IDA graph, suffices unroll graphcover tree apply upper bounds number nodes potentially expandedcover tree. particular, bound Equation (7) applied directly searchG.Note bounds applied directly, problem determining exactlysolutions G lift solutions cover tree depends delicate structural properties Gspecifically, depends growth number distinct paths x0 solutionfunction length paths. particular, order obtain general resultscomplexity model, must invoke measure connectedness graph G.show bound complexity terms spectral properties G. chooseapproach offers single parameter notion connectedness (the second eigenvalue)analytically tractable actually analyzed bounded many graphsinterest, including various families Cayley graphs combinatorial graphs methodsconductance.6.2 Upper Bound via Graph Spectrashall consider undirected2 graph G n vertices search space. Let x0 startingnode let set solutions G. simplicity, assume G b-regular (2 < b n)edge costs uniformly equal one, cover tree (G) b-ary uniform edge cost.assume, additionally, |S| treated constant n .Fact 3 Lemma 3.1, number node expansions G (1 , 2 )approximate heuristic 2b(1 +2 )d + dN1 +2 , optimal solution cost,equals optimal solution depth (G), N number -optimal solutions (G).goal upper bound N (of cover tree (G)) terms spectral properties G.introduce principal definitions spectral graph theory below, primarily setnotation. complete treatment spectral graph theory found work Chung(1997).Graph Spectra. graph G, let adjacency matrix G: A(x, y) = 1 x adjacenty, 0 otherwise. real, symmetric matrix (AT = A) thus real eigenvaluesb = 1/bb = 1 2 . . . n b, spectral theorem (Horn & Johnson, 1999). Letbdenote normalized adjacency matrix G; eigenvalues 1 = 1 2 . . . n 1,referred spectrum G, = /b. eigenvalues, alongassociated eigenvectors, determine many combinatorial aspects graph G. applicationsdefgraph eigenvalues, however, critical value = (G) = max {|2 |, |n |} invokedand, moreover, real parameter interest gap = /b largest eigenvalue1 = 1 normalized adjacency matrix. Intuitively, measures connectedness G.Sparsely connected graphs 1; n-cycle, example, = 1 O(1/n). hypercubeN = 2n vertices = 1 (1/ log N ). Similar bounds , known manyfamilies Cayley graphs. Random graphs, even constant degree b 3, achieve = o(1)high probability. fact, recent result Friedman (2003) strengthens this:Theorem 6.1. (Friedman, 2003) Fix real c > 0 integer b 2. probability1 o(1) (as n ),(Gn,b ) 2 b 1 + c2. one produce analogous cover tree directed case, spectral machinery apply nextsection somewhat complicated presence directed edges. See work Chung (2006) HornJohnson (1999, Perron-Frobenius theorem) details.700fiThe Time Complexity Approximate HeuristicsGn,b random b-regular graph n vertices.remark non-bipartite connected graph diameter D, alwaysb 1/(Dn). stronger conditions, graph vertex-transitive (which saypair v0 , v1 vertices G automorphism G sending v0 v1 ), oneb (1/D2 ) (Babai, 1991). vertex transitivity strong condition, satisfiedmany natural algebraic search problems (e.g., 15-puzzle-like search spaces Rubiks cube).principal spectral tool apply section described Lemma 6.1 below. beginnotation.Notations. function G viewed column vector indexed vertices Gvice versa. vertex x G, let 1x denote function G value 1 x0 everyPvertex x. real-valued functions , G, definepinner producth, = xG (x)(x). shall use k k denote L2 -norm, i.e., kk = h,function G.b symmetric real, spectral theorem (Horn & Johnson, 1999),Recall sinceexist associated eigenfunctions 1 , . . . , n form orthonormal basis space real-valuedb particular,functions G, eigenfunction associated eigenvalue A.bPn Ai = ki k = 1 i, hi , j = 0 6= j. basis, write= i=1 h, real-valued function G.Lemma 6.1. Let G undirected b-regular graph n vertices, = (G)/b.probability distributions p q vertices G, integers s, 0,fifiDEfifib p,bt q 1 fi s+t kpk kqk 1 .fifinfinPnPnai q = j=1 bj j ai = hp, , bj = hq, j i.+* nnnnEXXXXb p,bq =s+tai bi .ai bj si tj hi , j =bj j j =ai ,Proof. Write p =i=1i=1i,j=1j=1i=1Cauchy-Schwartz inequality,nXv!! nu nXu X22|ai bi |bi = kpk kqk .aii=1i=1i=11/nWithout loss generality, assume 1 (x) =vertices x G. Since p probabilitydistribution,X11 Xp(x) = .a1 = hp, 1 =p(x)1 (x) =nnxGSimilarly, b1 =1 .nThus, a1 b1 =xG1n.fi nfifiDfiE 1 fifi fiXfififis+tb p,bt q fi = fifibfififinfi fii=2s+tnX|ai bi |(as = max |i |)2ini=2s+t kpk kqkcompleting proof lemma.7011n,fiDinh, Dinh, Michel, & RussellLemma 6.1 hand, establish following bound number paths prescribedlength ` connecting pair vertices. apply control number -optimal solutionscover tree G. Let P` (u, v) denote number paths G length ` u v.Lemma 6.2. Let G undirected b-regular graph n vertices, = (G). verticesu, v G ` 0,fifi`fififiP` (u, v) b fi ` 1 1 < ` .finfinProof. Since P` (u, v) number `-length paths u v, P` (u, v) = b` p(`) (v),p(`) (v) probability natural random walk G length` startingu endsEffP` (u,v)`(`)`(`)(`)bb= 1v , 1u . Applying Lemma 6.1v. Since p = 1u p (v) = 1v , p ,`byieldsfifi fiE 1 fififi P` (u, v)1 fifi fifiD`bfifi ` k1v k k1u k 1 = ` 1 1 .1,1=vufi b`nfi finfinn= /b, multiplying sides last inequality b` completes proof lemma.major consequence Lemma 6.2 application following bound number-optimal solutions (G).Theorem 6.2. Let G undirected b-regular graph n vertices, = (G). sufficientlylarge n 0, number -optimal solutions (G)(1+)dbN < 2|S|+ (1+)d ,ndepth optimal solutions (G), set solution nodes G.Proof. solution S, number copies level ` (G) equals P` (x0 , s),less b`/n + ` Lemma 6.2. Hence, number solutions level ` (G)`XbP` (x0 , s) < |S|+ ` .(9)nsSSumming sides (9) ` ranging (1 + )d,(1+)d(1+)d(1+)dX XXX1N =P` (x0 , s) < |S|b` +` .n`=d sS`=d`=dn sufficiently large, 2. Thus,1 (1+)dN < |S|2b+ 2(1+)d .nNote b(1+)d/n = O(1) =O(logb n). mentioned earlier (Theorem 6.1), b-regulargraphs 2 b 1 + o(1) 2 b. Assuming G spectral property = O(logb n),Theorem 6.2 givesN = O((1+)d ) = 2(1+)d b(1+)d/2 .cases, number node expansions G using (1 , 1 )-approximate heuristicO(d2(1+)d b(1+)d/2 ), implies effective branching factor roughly bounded21+ b(1+)/2 < 8b(1+)/2 .702fiThe Time Complexity Approximate Heuristics7. Experimental Resultsdiscussed introduction, bounds established thus far guarantee E, numbernodes expanded using -accurate heuristic, satisfiesE 2bd + dN cbdassumption N bd . (Here, before, b branching factor, optimalsolution depth, c constant.) suggests hypothesis hard combinatorialproblems suitably sparse near-optimal solutions,log E log b + .(10)constant determined search space heuristic independent .particular, suggests linear dependence log E . experimentally investigatedhypothesized relationship family results involving Knapsack problem partialLatin square problem. far aware, first experimental results specificallyinvestigating dependence.remark order experimental framework really cast light boundspresented , one must able furnish heuristic known approximation guarantees.7.1 Search Knapsackbegin describing family experimental results search coupled approximateheuristics solving Knapsack problem. problem extremely well-studiedwide variety fields including finance, operations research, cryptography (Kellerer, Pferschy,& Pisinger, 2004). Knapsack problem NP-hard (Karp, 1972), efficient algorithmsolve exactly unless NP = P. Despite that, problem admits FPTAS (Vazirani, 2001, p.70), algorithm return -approximation optimal solution time polynomial1/ input size. use FPTAS construct approximate admissible heuristicssearch, yields exact algorithm Knapsack may expand far fewer nodesstraightforward exhaustive search. (Indeed, resulting algorithm is, general, efficientexhaustive search.)7.1.1 Search Model KnapsackConsider Knapsack instance given n items, let [n] = {1, . . . , n}. item [n]weight wi > 0 profit pi > 0. knapsack capacity c > 0. task find set itemsmaximal total profit total weight c. Knapsack instancedenoted tuple h[n], p, w, ci. Knapsack instance restricted subset X [n] denotedhX, p, w, ci. subset X [n], let w(X) Pp(X) denote Ptotal weighttotal profit, respectively, items X, i.e., w(X) = iX wi p(X) = iX pi .Search Space. represent Knapsack instance h[n], p, w, ci search space follows.state (or node) search space nonempty subset X [n]. move (or edge) one stateX another state taken removing item X. cost move profitremoved item. state X [n] designated solution w(X) c. initial stateset [n]. See Figure 2 example search space n = 4.search space irregular directed graph whose out-degrees span wide range, 2n 1. Moreover, two states X1 , X2 X2 X1 [n], |X1 \ X2 |! pathssearch graph X1 X2 . Moreover, every path X1 X2 cost equalp(X1 ) p(X2 ). feature search graph makes behave like spanning subtreegraph: state search graph reopened. Hence, state X [n], cost703fiDinh, Dinh, Michel, & Russell{1, 2, 3, 4}{1, 2, 3}{1, 2}{1, 2, 4}{1, 3}{1, 3, 4}{1, 4}{1}{2, 3}{2}{2, 3, 4}{2, 4}{3}{3, 4}{4}Figure 2: search space Knapsack instance given set 4 items {1, 2, 3, 4}. Solutionstates edge costs indicated figure.path starting state Xg(X) = p([n] \ X) = p([n]) p(X) ,cheapest cost reach solution state X [n]h (X) = p(X) Opt(X) ,Opt(X) total profit optimal solution Knapsack instance hX, p, w, ci, i.e.,Opt(X) = max {p(X 0 ) | X 0 X w(X 0 ) c} .defObserve solution state X [n] search space h[n], p, w, ci optimalg(X ) minimal, equivalently, p(X ) maximal, means X optimal solutionKnapsack instance h[n], p, w, ci.Heuristic Construction. Fix constant (0, 1). order prove linear dependencelog E , wish efficient -accurate heuristic H aforementioned Knapsacksearch space h[n], p, w, ci. Moreover, order guarantee solution returnedsearch optimal, insist H admissible, H must satisfy:(1 )h (X) H (X) h (X) X [n] .main ingredient constructing heuristic FPTAS described book Vazirani(2001, p. 70). FPTAS algorithm, denoted A, returns solution totalprofitleast (1 )Opt(X) Knapsack instance hX, p, w, ci runs time |X|3 / ,(0, 1). nonempty subset X [n], let (X) denote total profit solutionreturned algorithm error parameter Knapsack instance hX, p, w, ci.(0, 1),(1 )Opt(X) (X) Opt(X) ,impliesp(X)(X)h (X) p(X) (X) .1(11)(X)Thus may work heuristic H (X) = p(X) A1, guarantees admissibility.However, definition H guarantee -approximation H : definition,condition (1 )h (X) H (X) equivalent(1 )h (X) p(X)704(X),1(12)fiThe Time Complexity Approximate Heuristicsalways hold. Since h (X) p(X) (X), condition (12) satisfied(1 )(p(X) (X)) p(X)(X).1(13)(X)Equation (13) holds. Otherwise, defineHence, define H (X) = p(X) A1H (X) differently, still ensuring -approximate admissible. Note Xsolution, least one item X must removed order reach solution contained X, thush (X) = p(X) Opt(X) m, smallest profit items. gives another optiondefine H (X) guarantee admissibility. summary, define heuristic functionH follows: non-solution state X,((X)(13) holdsp(X) A1def(14)H (X) =otherwise,determined later H -approximate. X solution, simply setH (X) = 0, h (X) = 0 case. H admissible, regardless .make sure H -approximate, remains consider case (13) hold,(X)i.e., p(X) A1< (1 )(p(X) (X)), non-solution state X. case,p(X) (X)(X)(p([n]) m) .(1 )(1 )(15)last inequality due assumption X solution. want choose(p([n]) m)(16)(1 )1which, combining (11) (15), imply (1 )h (X) = H (X). Therefore,choose1 = 1 + 1 1 (p([n])/m 1) .3 1Since running timecompute(X)|X|, running time compute H (X)3 1|X| p([n])/m , polynomial n 1 profits boundedrange [m, poly(n)m]. search using heuristic H given Knapsack spaceh[n], p, w, ci described Algorithm 3 below.7.1.2 Experimentsorder avoid easy instances, focus two families Knapsack instances identified studiedPisinger (2005) difficult existing exact algorithms, including dynamic programmingalgorithms branch-and-bound algorithms:Strongly Correlated: item [n], choose weight wi random integerrange [1, R] set profit pi = wi + R/10. correlation weights profitsreflects real-life situation profit item proportional weight plusfixed charge.Subset Sum: item [n], choose weight wi random integer range [1, R]set profit pi = wi . Knapsack instances type instances subset sumproblem.testsP set data range parameter R := 1000 choose knapsack capacityc = (t/101) i[n] wi , random3 integer range [30, 70].3. paper Pisinger (2005), fixed integer 1 100, average runtime testscorresponding values reported.705fiDinh, Dinh, Michel, & RussellAlgorithm 3 Search KnapsackInput: hn, p, w, c, i; n number items, pi wi profit weight item[n], c capacity knapsack, (0, 1) error parameter heuristic.Oracle: FPTAS algorithm Knapsack problem(2001, p. 70).P described VaziraniPNotation: subset X [n] items, let p(X) = iX pi , w(X) = iX wi .Output: subset X [n] items w(X ) c p(X ) maximal.1. Put start node [n] Open. Let = min1in pi . Set1 = 1 + 1 1 (p([n])/m 1) .2. Repeat Open empty:(a) Remove Open place Closed node X g(X) + h(X) minimum.(b) w(X) c, exit success return X, optimal solution.(c) Otherwise, expand X: item X, let X 0 = X \ {i},i. X 0 Open Closed, set g(X 0 ) := g(X) + p(i) = p([n]) p(X 0 ),compute heuristic h(X 0 ) follows:A. X 0 solution, set h(X 0 ) := 0.B. Otherwise, run algorithm Knapsack input hX 0 , p, w, ci error parameter , let A(X 0 ) denote total profit solution returned algorithmA. set(00))00p(X 0 ) A(Xp(X 0 ) A(X011 (1 )(p(X ) A(X ))h(X ) :=otherwise.put X 0 Open pointer back X.ii. Otherwise (X 0 Open Closed, g(X 0 ) calculated), g(X)+p(i) <g(X 0 ), direct pointer X 0 back X reopen X 0 Closed.[Remark: Since paths starting node X 0 cost,condition g(X) + p(i) < g(X 0 ) never holds. fact, step discarded.]3. Exit failure.706fiThe Time Complexity Approximate Heuristicsgenerating Knapsack instance h[n], p, w, ci either type described above, run seriessearch using given heuristic H , various values , well breath first search(BFS), solve Knapsack instance. search finishes, values E reported,E number nodes (states) expanded search, depth optimalsolution found search. Knapsack search space, k equals number items removedoriginal set [n] obtain optimal solution found search. overall runtimesearch, including time computing heuristic, also reported. addition, reportoptimal value h ([n]) minimal edge cost (i.e., minimal profit) search spaceKnapsack instance tested.specify appropriate size n Knapsack instance type, ran exploratory experiments identified largest possible value n search instances would finishwithin hours. chose values n (n = 23 Strongly Correlated type,n = 20 Subset Sum type) final experiments. Observing optimal solutiondepths resulted Knapsack instances sizes fairly small, ranging 5 15,selected sample points high interval [0.5, 1) distance two consecutivepoints large enough sensitiveness E seen. particular, selected eightsample points 8/16 = 0.5 15/16 = 0.9375 distance 1/16 = 0.0625two consecutive points. final experiments, generated 20 Knapsack instances typeselected parameters n .Experimental Results. Results final experiments shown Tables 1, 2, 3, 4, 5, 6,rows corresponding breath first search indicated BFS column. data show, expected, search outperforms breath first search termsnumber nodes expanded and, naturally, smaller , fewer nodes expands.result, effective branching factor decrease decreases (as long optimalsolutions given search space located depth). Recall expands Enodes finds solution depth d, effective branching factor branching factoruniform tree depth E nodes (Russell & Norvig, 1995, p. 102), i.e., number b satisfyingE = 1 + b + (b )2 + + (b )d . Clearly, (b )d E and, b 2, E 2(b )d . shallfocus solely values b 2, simply use E 1/d proxy effective branching factor, contentdiffers actually quantity factor 21/d . (Of course, b growserror decays even further). effective branching factors, calculated E 1/d , searchbreath first search Knapsack instances type Strongly Correlated shown Tables 1,2, 3. Note Knapsack instances Subset Sum type, one cannot directly compareeffective branching factors, optimal solutions found different search instances appeardifferent depths.primary goal experiments investigate proposed linear dependence which,case non-uniform branching factors non-uniform edge costs, may expresslog E log bBFS + ,(17)average optimal solution depth, bBFS effective branching factor breath firstsearch, constant depending . examine extend data supportshypothesis, calculate least-squares linear fit (or linear fit short) log E (forKnapsack instance, varying ) using least-squares linear regression model, measurecoefficient determination R2 . experiments, 17 20 Knapsack instances typeStrongly Correlated 20 Knapsack instances type Subset Sum R2 value least0.9. instances, 90% variation log E depends linearly , remarkable fit.See Figure 5 detailed histograms R2 values Knapsack instances. median R20.9534 Knapsack instances type Strongly Correlated, 0.9797 type SubsetSum. Graphs log E linear fit Knapsack instances median R2 amongtype shown Figures 3 4. Note even number instances707fiDinh, Dinh, Michel, & Russelltype, single instance median value. instances shown graphsactually R2 value median.Knapsack instance type Strongly Correlated median R2Instance 17Linear fitBFSlog10 E654320.50.60.70.80.9Heuristic error1Figure 3: Graph log10 E least-squares linear fit Knapsack instance type StronglyCorrelated median R2 (see data Table 3).Knapsack instance type Subset Sum median R25.3Instance 14Linear fitBFSlog10 E5.255.25.150.50.60.70.80.9Heuristic error1Figure 4: Graph log10 E least-squares linear fit Knapsack instance type SubsetSum median R2 (see data Table 5).Remark course, may instances poorly fit prediction linear dependence,instance #20 Strongly Correlated type whose R2 value 0.486, though instances708fiThe Time Complexity Approximate Heuristicsrarely show experiments. instance, search using heuristic function Hmay explore even fewer nodes search using H does, small > 0.phenomenon explained degree control accuracy heuristicfunction H . particular, guarantee H admissible -approximate,reality may provide approximation better nodes opened. Note Hproportional (1 ). Hence, H may occasionally accurate Hsmall > 0, resulting fewer nodes expanded.FrequencyHistograms R2 values Knapsack instance familiesStrongly correlatedSubset Sum105[0.97, )[0.93, )[0.90, )[0.87, )[0.83, )[0.80, )[0.77, )[0.73, )[0.70, )[0.67, )[0.63, )[0.60, )[0.57, )[0.53, )[0.50, )[0.47, )[0.43, )[0.40, )0R2 value bin limitsFigure 5: Histograms R2 values Knapsack instances.analyze deeply data fit model Equation (17), calculate slopeleast-squares linear fit log10 E Knapsack instance type Strongly Correlated. Noteinstance, every search optimal solution depth, denoted d, thus,= d. data, given Figure 6, show one instance worst R2 value,slope linear fit log10 E fairly close log10 bBFS , slope hypothesizedline given Equation (17). Specifically, Knapsack instance type Strongly Correlated,except instance #20,0.73d log10 bBFS 1.63d log10 bBFS .7.2 Search Partial Latin Square Completionexperimental results discussed Knapsack problem support hypothesis linearscaling (cf., Equation (1) (10)). However, several structural features search spaceheuristic unknown: example, cannot rule possibility approximationalgorithm, asked produce -approximation, fact produce significantlybetter approximation; likewise, explicit control number near-optimal solutions.order explore hypothesis detail, experimentally analytically investigatesearch space partial Latin square completion problem provide precise analyticcontrol heuristic error well number -optimal solutions N .7.2.1 Partial Latin Square completion (PLS) ProblemLatin square order n n n table row column permutationset [n] = {1, . . . , n}. cells n n table filled values [n]709fiDinh, Dinh, Michel, & RussellKnapsack instance type: Strongly CorrelatedInstance1234567891011121314151617181920Optimalsolutiondepth119677678697575697877Effective branchingfactor breath firstsearch bBFS4.20925.392810.85518.23808.019410.67808.70686.774211.41025.54128.326018.04868.030815.096410.00705.78638.31556.91068.36027.0964Slopelinear fita/(d log10 bBFS )Coefficientdetermination R27.65834.896610.10386.42795.08826.45117.90876.56168.68476.36909.76857.78486.03767.30044.42197.18159.17389.28377.18071.00551.11540.74351.62601.00270.80401.04541.20210.98721.36900.95171.51611.23920.95331.23850.73681.04661.42471.38231.11230.16880.93950.91830.96470.97100.91610.96960.94360.97820.95710.94610.96890.93140.96460.96760.87880.86980.94980.97290.97700.4860Figure 6: Slopes least-squares linear fits log10 E (varying ) Knapsack instancestype Strongly Correlated. Details least-squares linear fits given Tables 1, 2, 3.R2 values Knapsack instances also included figure.way value appears twice single row column, table called partial Latinsquare. completion partial Latin square L Latin square obtained fillingempty cells L, see Figure 7 example. Note every partial Latin squarecompletion. Since problem determining whether partial Latin square completionNP-complete (Colbourn, 1984), search version (denoted PLS), i.e., given partial Latin squareL find completion L one exists, NP-hard.12531412123454323514354214125354132Figure 7: 5 5 partial Latin square (right) unique completion (left).PLS problem (also known partial quasi-group completion) used recentpast source benchmarks evaluation search techniques constraint satisfactionBoolean satisfiability (Gomes & Shmoys, 2002). Indeed, partially filled Latin squares carryembedded structures trademark real-life applications scheduling time-tabling.Furthermore, hard instances partially filled Latin square trigger heavy-tail behaviorsbacktrack search algorithms common-place real-life applications require randomization restarting (Gomes, Selman, & Kautz, 1998). Additionally, PLS problem exhibitsstrong phase transition phenomena satisfiable/unsatisfiable boundary (when 42%cells filled) exploited produce hard instances. remark underlying710fiThe Time Complexity Approximate Heuristicsstructure Latin squares found real-word applications including scheduling, timetabling (Tay, 1996), error-correcting code design, psychological experiments design wavelengthrouting fiber optics networks (Laywine & Mullen, 1998; Kumar, Russell, & Sundaram, 1996).7.2.2 Search Model PLSFix partial Latin square L order n c > 0 completions. divide cells n n tabletwo types: black cells, filled L, white cells,left blank L. Let k number white cells. white cells indexed 0 k 1fixed order, e.g., left right top bottom table. task search findcompletion L. Hard instances obtained white cells uniformly distributed withinevery row every column density black cells (n2 k)/n2 42% tapphase transition. insure number completions c = O(1) (c exactly 1experiments).structure search space problem, place white cells virtual circlewhite cells index (i + 1) mod k adjacent. move along circle, stepeither forward (from white cell index cell index (i + 1) mod k) backward (fromwhite cell index cell index (i 1) mod k) may set content current cell.Formally, define search graph, denoted GL , PLS instance given L follows:state (or node) GL pair (, p), p {0, . . . k 1} indicates index currentwhite cell, : {0, . . . , k 1} {0, . . . , n} function representing current assignmentvalues white cells (we adopt convention (j) = 0 means white cell index jfilled). directed link (or edge) state (, p) state (, q) searchgraph GL q = (p 1) mod k, (q)6= 0, (j) = (j) j 6= q. words,link state (, p) state (, q) represents step consisting moving whitecell index p white cell index q, setting value (q) white cell index q.Figure 8 illustrates links one state another GL . cost every link GL unit.Obviously, search graph regular (out-)degree 2n.starting state (0 , 0) 0 (j) = 0 j. goal state (or solution) form( , p), assignment corresponding completion L, p {0, . . . , k 1}. So,solution cover tree GL path search graph GL starting stategoal state, length optimal solution equal k. show number-optimal solutions cover tree GL large.Lemma 7.1. Let L n n partial Latin square k white cells. Let assignmentcorresponding completion L. 0 < k, number pathslength k + GLstarting state goal state form ( , ) 2 + 2 + k+tnt .Proof. represent path GL length k + starting state pair hP, ~v i,P (k + t)-length path circle white cells starting white cell index 0,~v = (v1 , . . . , vk+t ) sequence values [n] vi value assigned white cellvisited ith step path P . Consider pair hP, ~v represents path GL endinggoal state ( , ). Since (j) 6= 0 j, every white cell must visited non-zero stepP . Let sj > 0 last step white cell index j visited. mustvsj = (j) j {0, . . . , k 1}. Given path P , nt ways assigning valueswhite cells order eventually obtain assignment . Thus, number (k + t)-lengthpaths GL starting state goal state ( , ) equal |Pt |nt , Pt set(k + t)-length paths circle white cells start white cell index 0 visit everywhite cell.remains upper bound |Pt |. Consider path P Pt ; strategy bound numberbackward (or forward) steps P . < k, least k 1 white cells visited exactly711fiDinh, Dinh, Michel, & Russell0k15k221423310k15k221431, vh2343232235155210k1k2553p42p411p1v43h+1, v14234113pv2p+125351Figure 8: links connecting states PLS search graph. label h+1, vi (resp., h1, vi)links means moving forward (resp., backward) setting value v [n] next white cell.P . Let w index white cell visited exactly P let stepwhite cell w visited.Assume step forward step, i.e., white cell visited step 1 (w 1) mod k.Let w0 farthest white cell w backward direction visited step s.Precisely, w0 = (w `) mod k, ` maximal number {0, . . . , k 1} whitecell (w `) mod k visited step s. Let wj = (w0 + j) mod k, j = 0, . . . , k 1. Notew` = w. set white cells visited first steps {w0 , w1 , . . . , w` }, deletingsteps among first steps P obtain path (w0 , w1 , . . . , w` ) w0 w`forward direction. white cells w`+1 , . . . , wk1 must visited step stepalso forward direction white cell w` visited forwardstep. Thus, deleting steps P obtain path visiting white cells w0 , w1 , . . . , wk1forward direction. Let s0 , . . . , sk1 steps P deleted, wj visitedstep sj P , 1 s0 < s1 < . . . < sk1 k + t. steps s1 , . . . , sk1 forward steps(step s0 forward backward). Moreover, number backward steps numberforward steps sj1 sj must equal j = 1, . . . , k 1. Let numberdeleted steps s0 sk1 exactly (t )/2 backward stepss0 sk . shows+ 1 + (t )/2 = 1 + (t + )/2 + 1 backward stepsP . Note k+tpaths Pt exactly j backward steps. Path Pj+ 1 backward steps = (and thus sj = sj1 + 1 j = 1, . . . , k 1) everystep 1 s0 sk1 backward. + 1 paths Pt , correspondingchoice s0 {1, . . . , + 1}.Similarly, step backward step, + 1 forwardsteps P . Also,+ 1 paths Pt exactly + 1 forward steps, k+tpaths Ptj712fiThe Time Complexity Approximate Heuristicsexactly j forward steps. Hence,|Pt | 2 + 1 +Xk+tj=0last inequality holds since coefficientj2 t+2+t k+t.k+tjincreases j increases j < (k + t)/2.upper bound Lemma 7.1 achieved = 0. fact, four ways visitevery white cell k steps starting white cell 0: taking either k forward steps k backwardsteps one backward step followed k 1 forward steps one forward steps followed k 1backward steps. number optimal solutions cover tree GL equal 4c, sincec completions initial partial Latin square.Theorem 7.1. Let L n n partial Latin square k white cells c completions.0 < < 1, number nodes expanded search GL -accurate heuristicB(),B() =(2(2n)k + 4ck2(2n)kk < 1 ,+ 4ck bkc + 2 + bkck+bkcbkcnbkck 1 .Proof. Lemma 3.1, number nodes expanded search GL -accurate heuristicupper-bounded 2(2n)k + kN , N number -optimal solutions cover treeGL . So, need bound N .k < 1, N equals number optimal solutions, implies upper bound2(2n)k + 4ck number expanded nodes .general case, let ` = bkc. Since k < k, Lemma 7.1,`Xk+t2 t+2+tntt=0!X``Xk+`2c(t + 2)n + 2ctn`t=0t=0k+``4c(` + 2)n + 4c`n` .`N csecond inequality holds k+tk+``. last inequality obtained`P`P``applying fact t=0 tn 2`n t=0 nt 2n` integers n 2 ` 0,proved easily induction `. Hence, number nodes expandedk+`2(2n)k + 4ck ` + 2 + `n` .`Corollary 7.1. Suppose 0 < < 1. number nodes expanded search GL-accurate heuristickkk 3/2 (1 + ) (1 + 1/) nk .Proof. Theorem 7.1, need upper bound binomial coefficient k+`large k,`` = bkc. Since k ` large, bound binomial coefficient using Stirlings713fiDinh, Dinh, Michel, & Russellnformula, asserts n! 2n ne . precisely, write n! = 2nn .k+`(k + `)!=`k!`!pk+`2(k + `) k+`k+`e=`k2k ke k 2` e` `k + ` (k + `)k+`k+`.=k `k k ``2k`n nen , n 1Stirlings formula, term k+` /k ` O(1). Sincek+`kk=1+1+1 + 2/`bkck 1k > 2/, term k + `/ 2k` O(1/ k). remaining term(k + `)k+`=k k ``k`k`k1k1+1+(1 + ) 1 +k`xsince ` k function g(x) = (1 + k/x) monotonically increases x > 0. Hence,k !k+`11= (1 + )k 1 +.`kTheorem 7.1, number nodes expandedk + ` kB() = 2(2n)k + k 2n`!k13/2kk= k (1 + ) 1 +n.follows corollary effective branching factor using -accurateheuristic GL asymptotically (1 + ) (1 + 1/) n , significantly smallerbrute-force branching factor 2n, since (1 + )n (1 + 1/) converge 1 0.7.2.3 ExperimentsGiven search model PLS problem described above, provide experimental resultssearch PLS instances, determined large partial Latin squaresingle completion. PLS instance experiments, run search differentheuristics form (1 )h given various values [0, 1). emphasize dominance property admissible heuristics, number nodes expanded using admissible-accurate heuristic strictly larger (1 )h less equal usingheuristic (1 )h . words, heuristic (1 )h worse admissible -accurateheuristics.build oracle heuristic (1 )h search graph GL , use informationcompletion partial Latin square L compute h . Consider partial Latin squareL k white cells, arbitrary state (, p) GL . show compute optimal714fiThe Time Complexity Approximate Heuristicscost h (, p) reach goal state GL state (, p). Let X() set white cellsdisagrees completion L, h (, p) equal length shortestpaths cycle starting p visiting every point X(). case|X() \ {p} | 1 easy handle, shall assume |X() \ {p} | 2 on. particular,suppose X() \ {p} = {p1 , . . . , p` } ` > 1, pj j th point X() \ {p} visitedmoving forward (clockwise) p; see Figure 9. two types paths cyclestarting p visiting every point X() \ {p}: type includes visit p, typeII includes visiting p. Let `1 `2 length shortest paths type type II,respectively.(min {`1 , `2 }p 6 X() ,h (, p) =min {`1 + 2, `2 } p X() .need compute `1 `2 . Computing `1 straightforward: realized eithermoving forward p p` moving backward p p1 .`1 = min {p` p, p p1 }defz = z mod k integer z. compute `2 , consider two options j: option(a) moving forward p pj moving backward pj pj+1 , option (b) movingbackward p pj+1 moving forward pj+1 pj . Thus,`2 = min min {pj p, p pj+1 } + pj pj+1 .1j<`moving forwardp1pp+1p1pmp2pm1pj+1pjFigure 9: Layout points X().describe experiments detail. generate six partial Latin squares orders10 20 following way. Initially, generate several partial Latin squares obtainednear phase transition white cells uniformly distributed within every row column.instance generated complete Latin square suitably chosen random subsetscells cleared. candidate partial Latin square solved exhaustive backtrackingsearch method find completions. subset candidates exactly one completionretained experiments. partial Latin square L chosen value , recordtotal number E nodes expanded search graph GL (1 )h heuristic.Then, Knapsack experiments, effective branching factor calculated E 1/k ,since optimal solution depth GL equals number white cells L. first purposecompare effective branching factors obtained experiments upper bound obtained715fiDinh, Dinh, Michel, & Russelltheoretical analysis. Recall Theorem 7.1 E B(), caseB() =(2(2n)k + 4k2(2n)kk < 1 ,+ 4k bkc + 2 + bkck+bkcbkcnbkck 1 .1/kTherefore, calculate theoretical upper bound B()effective branching factor E 1/k .1/kdeeper comparison, calculate multiplicative gap B() /E 1/k theoreticalbound actual values. empirical results given Tables 7 8, multiplicativegaps close 1 small k large. Notice given k, upper boundsB() almost value bkc. multiplicativegaps sometimes increase decrease. However, multiplicative gaps decreasebkc decreases, fixed k. upper bounds cases k < 1 much tighterothers (with k) cases k < 1 compute number-optimal solutions exactly. Also observe that, fixed , multiplicative gaps decrease kincreases. Finally, experiments show dramatic gap effective branching factorscorresponding brute-force branching factor, equals 2n. fact, instance,1/keffective branching factor E 1/k theoretical upper bound B()approach 1 approaches0.experiments Knapsack problem, data partial Latin square problemalso support linear dependance log E . particular, one partial Latin squareinstances R2 larger 0.9 (the worst one R2 value equal 0.8698). median R2value partial Latin square instances 0.9304. graph instance medianR2 shown Figure 10.Partial Latin Square instance median R2Instance 4Linear fit6log10 E543200.511.52Heuristic error2.53102Figure 10: Graph log10 E least-squares least-squares linear fit (or Linear fit)partial Latin square instance median R2 (see data Table 8).also investigate slope least-squares linear fit log E approximates slopelog b hypothesized linear dependence Equation (10). Recall case,branching factor b = 2n optimal solution depth = k. Figure 11 shows that, everyPLS instance experiment, slope least-squares linear fit log10 E approximates716fiThe Time Complexity Approximate Heuristicsk log10 (2n) factor 0.8, i.e., 0.8k log10 (2n). words, experimental resultsPLS indicate following relationship:log10 E 0.8k log10 (2n) + ,equivalently, E (2n)0.8k .Thus, empirically, effective branching factor search using heuristic (1)h givenPLS search space approximates (2n)0.8 . dominance property admissible heuristics,also empirical upper bound effective branching factor using admissible-accurate search space.Instance #nk123456101214161820446386113143176Slope linearfit line43.390173.752798.3613142.7056179.1665225.4152/(k log10 (2n))0.75800.84820.79030.83900.80500.7995Figure 11: Slopes least-squares linear fits log10 E partial Latin square instances.8. Reduction Depth vs. Branching Factor; Comparison PreviousWorksection compare results obtained Korf et al. (Korf & Reid, 1998; Korfet al., 2001). mentioned introduction, concluded effect heuristic functionreduce effective depth search rather effective branching factor. Consideringstriking qualitative difference findings ours, seems interesting discussconclusions apply accurate heuristics.study b-ary tree search model, above, permit multiple solutions. However,analysis depends critically following equilibrium assumption:Equilibrium Assumption: number nodes depth heuristic value exceeding `bi P (`), P (`) probability h(v) ` v chosen uniformly random amongnodes given depth, limit large depth.remark equilibrium assumption strong structural requirement, holdsexpectation rich class symmetric search spaces. specific, state-transitivesearch space,4 like Rubiks cube, quantity bi P (`) precisely expected number verticesdepth h(v) ` goal state (or initial state) chosen uniformly random. Korfet al. (2001) observe equilibrium assumption, one directly control numberexpandedP nodes total weight `, quantity denote E(`): indeed, caseE(`) = i` bi P (` i). hand, consider ratioP`P`bi1 P (` i)E(`)i=0 b P (` i)= P`1= b Pi=0b,(18)`i1 P (` i)E(` 1)i=0 b P (` 1 i)i=1 bconclude E(d) bd1 E(1); thus effective branching factorqpbd1 E(1) b E(1)4. say search space state-transitive structure search graph independent startingnode. Note Cayley graph property, natural search spaces formed algebraic problemslike Rubiks cube 15-puzzle, right choice generators, property.717fiDinh, Dinh, Michel, & Russelloptimal solution lies depth d.difficulty approach even presence mildly accurate heuristic satisfying, example,h(v) h (v) small, constant, > 0 ,actual values quantities satisfyE(1) = E(2) = = E(t) = 0d. (Even root tree h(root) d.) Observe,then,E(d) = 1pargument actually results effective branching factor bdd E(d) = b(1)d = b1 ,yielding reduction branching factor. Indeed, applying technique infer estimatescomplexity , even assuming equilibriumP assumption, appears require controlthreshold quantity `0 quantitiesb P (`0 i) become non-negligible. course,equilibrium assumption may well apply heuristics weaker or, example, nonuniformaccuracy.One perspective issue obtained considering case search b-regular(non-bipartite, connected) graph G = (V, E) observing selection node uniformlyrandom nodes given depth, limit large depth is, case, equivalentselection random node graph. consider mildly accurate heuristic hwhich, say, h(v) h (v) small constant , bi P (`) bi Prv [ dist(v, S) `], vchosen uniformly random graph, set solution nodes, dist(v, S) denoteslength shortest path v node S.|S| b`|{v | dist(v, S) `/}|Pr[dist(v, S) `/] =v|V ||V |1b-regular graph, expect relation equation (18) hold past thresholdvalue `0 logb (|S|/|V |).Acknowledgmentswish thank anonymous reviewers constructive comments. Author Hang Dinhsupported IU South Bend Faculty Research Grant. Author Laurent Michel supportedNSF grant #0642906. Author Alexander Russell supported NSF grant#1117426.718fiThe Time Complexity Approximate HeuristicsAppendix A: Tables Experimental Results#nHeuristicerror12323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFSTotalnodeexpansionsE56275882167660211946772257147013561182557154310734774822323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFS32323232323232323234OptimalsolutiondepthSearchtime(seconds)1111111111111111111251018587441341131820251653110110887/20010887/20010887/20010887/20010887/20010887/20010887/20010887/2004448145537372163474221135873525081343508255356905238575979999999996225071497129317511734146910475667820/1577820/1577820/1577820/1577820/1577820/1577820/1577820/1570.50.56250.6250.68750.750.81250.8750.9375BFS9412555289002318001090808798841477032163609366666666667981052063017075602242323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFS369621847441665346425332176079219751952317663257487677777777752323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFS236453050172597308417968504168102618338721833644213297762323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFS72323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFSh ([n])/mEffectivebranchingfactorE2.1924732.2013252.9850263.0493153.429663.6363764.1396744.1989684.209164log10 ELinearfitlog10 E3.75033.76955.22445.32625.88786.16746.78666.85466.86623.79564.27424.75295.23155.71026.18886.66747.14613.2844593.2930334.1588224.2723274.8024125.1408985.3362035.34645.3927834.64824.65845.57075.67606.13316.39946.54516.55266.58634.70185.00785.31395.61995.92596.23206.53806.84415991/1215991/1215991/1215991/1215991/1215991/1215991/1215991/1212.1323312.2360684.2049554.5609625.6286546.9123269.78898310.67165210.8551211.97312.09693.74263.95434.50245.03775.94446.16946.21381.96742.59893.23043.86194.49345.12485.75636.3878862563032585537889576943836343/1546343/1546343/1546343/1546343/1546343/1546343/1546343/1543.2335234.167864.6087594.736285.9149776.9211917.931828.1150828.238013.56774.33944.64514.72815.40375.88136.29566.36516.41083.74714.14894.55064.95245.35415.75586.15766.5593777777777305285429754107410478235853066785/2056785/2056785/2056785/2056785/2056785/2056785/2056785/2054.2152174.3713574.9478556.0836287.1640297.7511797.8481457.8480058.0193824.37374.48434.86095.48915.98616.22566.26346.26336.32904.38034.69835.01635.33435.65235.97036.28836.60641981123162169926575131561395118108031412822061482293666666666461391511312904315474092195012/1485012/1485012/1485012/1485012/1485012/1485012/1485012/1483.5438944.805575.2812895.4627617.1316158.56619210.12958510.42300610.6779783.29694.09054.33644.42455.11915.59676.03366.10806.17093.46453.86774.27094.67415.07735.48055.88376.28691834195620392327530974173886675468344075937932047777777775143361591383325269845686187/1226187/1226187/1226187/1226187/1226187/1226187/1226187/1222.9254992.9525382.9701194.205734.3809785.6054346.804578.5863338.7067893.26343.29143.30944.36694.49105.24035.82966.53676.57902.81103.30533.79964.29394.78825.28255.77686.2711R20.93950.91830.96470.97100.91610.96960.9436Table 1: Results Knapsack instances type Strongly Correlated.719fiDinh, Dinh, Michel, & Russell#nHeuristicerror82323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFSTotalnodeexpansionsE82998741584559350021641353671325699554096150443469792323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFS1023232323232323232311OptimalsolutiondepthSearchtime(seconds)888888888129105335332479558106610276556400/1536400/1536400/1536400/1536400/1536400/1536400/1536400/1534304605313950711268881587904022008558220680566666666619168491752296466733345835/1215835/1215835/1215835/1215835/1215835/1215835/1215835/1210.50.56250.6250.68750.750.81250.8750.9375BFS1416215321178178214332872080212866139429384543001492499299999999919216266957410521306137911187212323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFS315330974223742688319946478386325794232773773777777777122323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFS1029116313103968148207533336326317109351915195132323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFS142323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFSh ([n])/mEffectivebranchingfactorE3.0894293.1095333.9432354.1816864.6441955.2025686.3276246.7072886.7742log10 ELinearfitlog10 E3.91903.94164.76684.97085.33535.72976.40996.61246.64693.77534.18544.59555.00565.41575.82586.23596.64602.7473342.7783884.1772454.6026434.7348676.6712979.61556211.23261111.4102192.63352.66283.72533.97804.05184.94535.89786.30296.34382.37492.91773.46054.00334.54615.08895.63176.17456762/1716762/1716762/1716762/1716762/1716762/1716762/1716762/1712.8922522.9176413.8320243.9114974.5715335.0480425.4059115.4916745.5411594.15114.18535.25095.33115.94066.32816.59586.65736.69244.16184.55994.95795.35605.75416.15216.55026.94821915292321955147518804066465/1066465/1066465/1066465/1066465/1066465/1066465/1066465/1062.2745822.2897482.6726194.1820764.2932145.7164126.9507928.2400878.3260442.49832.51852.98864.34974.42955.29995.89426.41156.44312.16192.77243.38303.99354.60405.21465.82516.435655555555535292550922123805892835073/1065073/1065073/1065073/1065073/1065073/1065073/1065073/1064.0038994.1031364.2019835.2446246.8260539.44924412.94327717.64601718.0485623.01243.06563.11733.59864.17084.87705.56026.23326.28222.50152.98803.47463.96114.44774.93425.42085.9073670170844351471911854273763211441947196347521542807777777771541273793833135738626553246072/1226072/1226072/1226072/1226072/1226072/1226072/1226072/1223.5203953.5484594.5989784.9411485.0642326.2590497.5831547.9250798.0307753.82613.85034.63864.85684.93165.57566.15896.29306.33333.69574.07304.45044.82775.20505.58245.95976.337141836297016850351480178163550403668276784088555555555156374621622583522461104636/1404636/1404636/1404636/1404636/1404636/1404636/1404636/1403.3437615.1517815.8778426.1082178.75644311.2244114.06488414.62147515.0963852.62123.55983.84613.92964.71165.25085.74075.82505.89442.83863.29493.75124.20754.66375.12005.57636.0326R20.97820.95710.94610.96890.93140.96460.9676Table 2: Results Knapsack instances type Strongly Correlated.720fiThe Time Complexity Approximate Heuristics#nHeuristicerror152323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFSTotalnodeexpansionsE15713176581262611729365113978098848147748143891004228162323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFS1723232323232323232318OptimalsolutiondepthSearchtime(seconds)6666666662181845364666476004352911405825/2115825/2115825/2115825/2115825/2115825/2115825/2115825/21118511870250425512297643228267829279874672707159999999994436352911312228384211047275/1177275/1177275/1177275/1177275/1177275/1177275/1177275/1170.50.56250.6250.68750.750.81250.8750.9375BFS6566657111714328608190546844063255899027493817777777773326211921945148138954052323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFS6837721819024869136138308550231152848055685201719888888888192323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFS28543140115003817051667270043110777626007472854529202323232323232323230.50.56250.6250.68750.750.81250.8750.9375BFS158012505837589456700571682245682583682855682455906305h ([n])/mEffectivebranchingfactorE5.0046825.1029777.0829077.4641598.9425159.6546639.6643559.66359310.007034log10 ELinearfitlog10 E4.19634.24695.10135.23795.70885.90845.91105.91086.00184.31044.58684.86315.13955.41595.69225.96866.24502.3069872.3096062.3857562.3906913.0520113.2740484.0095475.2039045.7862543.26743.27183.39863.40674.36134.63585.42796.44706.86162.70613.15493.60384.05264.50154.95035.39925.84806501/1026501/1026501/1026501/1026501/1026501/1026501/1026501/1022.5258922.5308142.5551124.0259614.3315275.6791817.0246558.230738.3155452.81692.82282.85194.23414.45655.28005.92646.40816.43922.34282.91623.48954.06294.63635.20965.78306.3564141311410728032388910837906012/1646012/1646012/1646012/1646012/1646012/1646012/1646012/1642.2610112.2958963.4078393.5437034.3827574.8547376.2443526.8425526.9106412.83442.88764.25984.39575.13405.48936.36396.68176.71612.72503.30523.88554.46575.04595.62626.20646.786677777777765561212101854126827724155503/1195503/1195503/1195503/1195503/1195503/1195503/1195503/1193.1162793.1590853.8027674.513694.7132035.9692397.3028638.2497848.3602493.45553.49694.06074.58174.71325.43146.04456.41516.45553.20413.65294.10174.55054.99935.44815.89696.345777777777786611739657976314843572351236592/2956592/2956592/2956592/2956592/2956592/2956592/2956592/2955.5292986.529186.6734476.8401346.8142816.8147636.8151516.8145817.0964185.19875.70405.77055.84555.83395.83425.83435.83415.95735.51195.57485.63765.70055.76335.82625.88905.9518R20.87880.86980.94980.97290.97700.4860Table 3: Results Knapsack instances type Strongly Correlated.721fiDinh, Dinh, Michel, & RussellLinear fitlog10 E5.86875.88035.89195.90365.91525.92685.93845.9500#nTotalnodeexpansions E731425761013782339805295828252845545865626885943900630Optimalsoln. depth111512121210111413Search time,seconds109087871657946336026717980log10 E202020202020202020Heuristicerror0.50.56250.6250.68750.750.81250.8750.9375BFSh ([n])/m15509/285509/285509/285509/285509/285509/285509/285509/285.86425.88145.89345.90605.91825.92715.93735.94745.954522020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS6716471824766278061484553901669650699536104144697889778259208168136107825835102984/282984/282984/282984/282984/282984/282984/282984/284.82714.85634.88444.90644.92714.95504.98464.99805.01764.83114.85584.88044.90504.92974.95434.97905.003632020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS22229323298924487125625026623527405627989028316029123911128998119953343235328522617312681283687/263687/263687/263687/263687/263687/263687/263687/265.34695.36735.38895.40875.42535.43785.44705.45205.46425.35525.37065.38615.40155.41705.43245.44795.463342020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS2906083049743135983234773312353366653408743426443608371010999109983292722251851511219264333883/563883/563883/563883/563883/563883/563883/563883/565.46335.48435.49645.50985.52015.52725.53265.53485.55735.47345.48345.49355.50355.51365.52375.53375.543852020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS851515873968893378912734927408940724950209958343967863111412141312121312740609498410335267206142887731/777731/777731/777731/777731/777731/777731/777731/775.93025.94155.95105.96035.96735.97355.97785.98155.98585.93485.94215.94945.95675.96415.97145.97875.986062020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS7585881410884949458510032910640911065611460111749610869759944883632872251771349455112327/112327/112327/112327/112327/112327/112327/112327/114.88004.91074.94694.97585.00145.02705.04405.05925.07004.88954.91554.94164.96764.99365.01975.04575.071772020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS7121387480957785657993788232368449258701758974079090751112151113131312141178947765618490378280185806456/336456/336456/336456/336456/336456/336456/336456/335.85265.87405.89135.90285.91555.92685.93965.95305.95865.85905.87275.88645.90015.91385.92755.94125.9549R20.99180.99590.96490.93690.96870.98330.9895Table 4: Results Knapsack instances type Subset Sum.722fiThe Time Complexity Approximate HeuristicsLinear fitlog10 E5.41135.44165.47195.50235.53265.56295.59325.6236#nTotalnodeexpansions E252054279643299328324182340530361756385942423848454094Optimalsoln. depth10129119101099Search time,seconds22741607115987866649434420142log10 E202020202020202020Heuristicerror0.50.56250.6250.68750.750.81250.8750.9375BFSh ([n])/m83514/73514/73514/73514/73514/73514/73514/73514/75.40155.44665.47615.51085.53225.55845.58655.62725.657192020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS2841463013013183083309243385903453353510273563743690949879991010862850741233426320314692344494/344494/344494/344494/344494/344494/344494/344494/345.45355.47905.50285.51975.52975.53825.54535.55195.56715.46775.48125.49475.50835.52185.53535.54895.5624102020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS8128288525398816579033899234509412779548619708719855261113151215111414121078874711579466356266180886963/396963/396963/396963/396963/396963/396963/396963/395.91005.93075.94535.95595.96545.97375.97995.98725.99375.91935.92985.94035.95085.96135.97175.98225.9927112020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS872387892404907719920529930373939495945766948094961185121312121213121111527441366306260214169125857270/1027270/1027270/1027270/1027270/1027270/1027270/1027270/1025.94075.95065.95805.96405.96875.97295.97585.97695.98285.94565.95075.95585.96095.96605.97115.97625.9813122020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS5447495725925967326228266448366621456822577058667208271381291112111113997804656528420329242158645752/355752/355752/355752/355752/355752/355752/355752/355.73625.75785.77585.79445.80945.82105.83395.84875.85785.74225.75795.77365.78935.80505.82075.83645.8521132020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS592766628513662306684651713728745263781953824260861415101011131210111111182413191040828645487344216747445/307445/307445/307445/307445/307445/307445/307445/305.77295.79835.82115.83555.85355.87235.89325.91615.93525.77675.79635.81595.83555.85525.87485.89445.9140142020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS1373681489331577931653681723831799831860681914261976348710997981056145036328922617312373183509/223509/223509/223509/223509/223509/223509/223509/225.13795.17305.19815.21855.23655.25525.26975.28205.29595.15135.17135.19135.21135.23145.25145.27145.2914R20.99250.92820.95930.94090.99010.99630.9761Table 5: Results Knapsack instances type Subset Sum.723fiDinh, Dinh, Michel, & RussellLinear fitlog10 E4.53114.57604.62094.66584.71074.75564.80064.8455#nTotalnodeexpansions E349373861741757450364923154428624097560284284Optimalsoln. depth96109107978Search time,seconds1022772529353272186128728log10 E202020202020202020Heuristicerror0.50.56250.6250.68750.750.81250.8750.9375BFSh ([n])/m153124/93124/93124/93124/93124/93124/93124/93124/94.54334.58684.62074.65364.69224.73584.79524.87854.9257162020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS476547498939523867558927592373626403668497725325768536101110109101212133224209715361181911675468281715442/115442/115442/115442/115442/115442/115442/115442/115.67815.69805.71925.74745.77265.79695.82515.86055.88575.67185.69765.72355.74935.77515.80105.82685.8527172020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS64154466683770203273789377240581008985227190222796489715111214141414121437512791199114951124827570337867157/117157/117157/117157/117157/117157/117157/117157/115.80725.82405.84645.86805.88785.90855.93065.95535.98455.80455.82565.84685.86795.88915.91025.93135.9525182020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS321490338267358571379827399061419052443204486366508524910910910910101215952760600466356252157474631/204631/204631/204631/204631/204631/204631/204631/205.50725.52935.55465.57965.60105.62235.64665.68705.70635.50475.52935.55405.57865.60335.62795.65255.6772192020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS10469811084511689312271012839813188713365813420514234878108691058251206169137110846037133373/443373/443373/443373/443373/443373/443373/443373/445.01995.04475.06785.08895.10865.12025.12605.12785.15345.03225.04825.06415.08005.09595.11195.12785.1437202020202020202020200.50.56250.6250.68750.750.81250.8750.9375BFS275501286961296924305914315286322234324077324471348398109979891093522922401961591269465325262/945262/945262/945262/945262/945262/945262/945262/945.44015.45785.47265.48565.49875.50825.51065.51125.54215.44895.45945.46995.48045.49095.50135.51185.5223R20.97390.99470.99880.99320.93490.9299Table 6: Results Knapsack instances type Subset Sum.724fiThe Time Complexity Approximate HeuristicsEffectivebranchingfactorE 1/k1.106827611.106827611.106827611.106827611.106827611.106827611.106827611.106827611.106827611.106827611.117935321.124838831.130295271.134811291.137442621.139835701.142030511.143063421.144057701.153277891.194933261.203449421.217240421.228429281.233354961.242221701.246158951.249885161.256898941.266975711.281544061.288380001.294466891.299053041.304208521.308951501.312679201.316827681.327484521.34592652UpperboundB(d)1/kB(d)1/k00.00250.0050.00750.010.01250.0150.01750.020.02250.0250.02750.030.03250.0350.03750.040.04250.0450.04750.050.05250.0550.05750.060.06250.0650.06750.070.07250.0750.07750.080.08250.0850.08750.090.09250.0950.0975TotalnodeexpansionsE878787878787878787871351772192612893173453593735312530345857098539101831395616041182932340033251549896949285507999041189241395201581171816662589984752691.124982871.125094761.125249531.125463201.125757401.126161021.126712031.127459361.128464211.129800271.294130231.294137561.294147751.294161901.294181581.294208901.294246851.294299541.294372641.485495101.485495481.485496011.485496741.485497751.485499171.485501131.485503861.485507661.681670211.681670241.681670291.681670361.681670461.681670591.681670771.681671031.681671381.887267701.887267711.8872677100.00250.0050.00750.010.01250.0150.01750.020.02250.0250.02750.030.03250.0350.03750.040.04250.0450.04750.050.05250.0550.05750.060.06250.0650.06750.07125125125125125125125295599789979109312071759800618159318293989853605639341514702402174182625696638239421.03E+061.39E+063.35E+066.43E+061.079653221.079653221.079653221.079653221.079653221.079653221.079653221.094469151.106843301.111694221.115508131.117460211.119221361.125931981.153344311.168435201.178890261.183125921.188684911.192014281.208446441.217324631.228087581.234124621.241375361.245806971.251724831.269293961.282517191.091872591.091961021.092105931.092342401.092725691.093340291.094309561.214045341.214049451.214056221.214067381.214085791.214116111.348434341.348434461.348434661.348435001.348435551.348436471.348437971.482711411.482711421.482711441.482711471.482711521.482711611.620310361.620310361.62031037#nkHeuristicerror11010101010101010101010101010101010101010101010101010101010101010101010101010101044444444444444444444444444444444444444444444444444444444444444444444444444444444212121212121212121212121212121212121212121212121212121212126363636363636363636363636363636363636363636363636363636363log10 ELinearfitlog10 E1.016402971.016504061.016643901.016836941.017102751.017467411.017965241.018640441.019548301.020755411.157607431.150509321.144964311.140420361.137799441.135434611.133285711.132307721.131387551.288063451.243161891.234365141.220380721.209265991.204437661.195842201.192066121.188515321.337951811.327310561.312221991.305259601.299122031.294535741.289418631.284746631.281098521.433192611.421687171.402207091.93951.93951.93951.93951.93951.93951.93951.93951.93951.93952.13032.24802.34042.41662.46092.50112.53782.55512.57172.72513.40313.53883.75663.93144.00794.14484.20524.26234.36924.52184.74034.84194.93204.99965.07535.14465.19905.25935.41335.67691.26741.37581.48431.59281.70131.80971.91822.02672.13522.24362.35212.46062.56912.67752.78602.89453.00303.11143.21993.32843.43693.54543.65383.76233.87083.97934.08774.19624.30474.41324.52164.63014.73864.84714.95555.06405.17255.28105.38945.49791.011317861.011399771.011533991.011753011.012108021.012677281.013575041.109254971.096857561.092077471.088353681.086468921.084786401.197616161.169151701.154051731.143817231.139722771.134393521.131226361.226956661.218008241.207333631.201427681.194410301.190161591.294462111.276544611.263382962.09692.09692.09692.09692.09692.09692.09692.46982.77742.89712.99083.03863.08173.24533.90344.25914.50284.60104.72924.80575.18035.38065.62145.75565.91596.01346.14316.52446.80801.39531.57971.76411.94852.13282.31722.50162.68602.87043.05473.23913.42353.60793.79233.97674.16104.34544.52984.71424.89865.08295.26735.45175.63615.82056.00496.18926.37366.5580E 1/kR20.94820.9693Table 7: Results partial Latin square instances.725fiDinh, Dinh, Michel, & RussellEffectivebranchingfactorE 1/k1.061610171.061610171.061610171.061610171.061610171.066159201.073025321.076243111.078546001.079798311.108359831.126214851.135821481.141981311.145987741.155969511.163811381.168729051.173209071.17776024UpperboundB(d)1/kB(d)1/k00.00250.0050.00750.010.01250.0150.01750.020.02250.0250.02750.030.03250.0350.03750.040.04250.0450.0475TotalnodeexpansionsE17117117117117124742955566773769592750657104909231228792590534633446658719253061.29E+061.070345881.070420981.070573351.070879621.071484291.162911121.162913471.162918271.162928111.162948211.262741581.262741661.262741821.262742141.360836471.360836481.360836481.360836491.360836511.4598517911311311311311311311311311311311311311300.00250.0050.00750.010.01250.0150.01750.020.02250.0250.02750.03225225225225799171923172731502361447972587355169421.97E+061.049097311.049097311.049097311.049097311.060928841.068146351.070971981.072531181.100530041.110888421.116609641.123469881.1368680518181818181818181814314314314314314314314314300.00250.0050.00750.010.01250.0150.01750.0228528528574325793659391372463385359322020202020202017617617617617617617600.00250.0050.00750.010.01250.01535135135124254125107153619190#nkHeuristicerror31414141414141414141414141414141414141414868686868686868686868686868686868686868641616161616161616161616161656log10 ELinearfitlog10 E1.008228731.008299481.008443001.008731501.009301081.090748101.083770771.080534931.078236911.077005031.139288481.121226261.111743211.105746761.187479081.177225231.169292981.164372951.159926691.239515262.23302.23302.23302.23302.23302.39272.63252.74432.82412.86753.84254.43944.75674.95875.08955.41345.66595.82345.96636.11091.49861.74451.99042.23632.48222.72812.97403.21993.46583.71173.95764.20354.44944.69534.94125.18715.43305.67895.92486.17071.055634971.055703121.055882171.056342851.128380871.128382841.128388081.128402021.205726501.205726561.205726711.280882031.280882031.006231701.006296661.006467331.006906451.063578281.056393481.053611211.052092511.095587081.085371441.079810411.140112481.126676072.35222.35222.35222.35222.90253.23533.36493.43634.70105.16085.41295.71346.29521.67722.03402.39072.74753.10423.46103.81784.17454.53134.88815.24485.60165.95841.040319521.040319521.040319521.047313851.056467891.059055251.076752771.090694231.096639041.045425501.045491451.045724131.104630101.104631341.104635801.166936541.166936561.166936651.004908091.004971481.005195151.054726911.045589121.043038871.083755321.069902571.064102782.45482.45482.45482.87103.41153.56344.59265.39155.72911.86652.31442.76233.21033.65824.10614.55405.00195.44981.033860571.033860571.033860571.045276811.048436621.068020451.078718411.037973801.038041391.038372631.087391441.087394021.138998601.138998621.003978511.004043891.004364281.040290401.037157611.066457661.055881322.54532.54532.54533.38473.61545.03005.79181.94622.50983.07333.63684.20044.76395.3275E 1/kR20.93350.92740.91200.8698Table 8: Results partial Latin square instances.726fiThe Time Complexity Approximate HeuristicsReferencesAaronson, S. (2004). Lower bounds local search quantum arguments. Proceedings36th Annual ACM Symposium Theory Computing (STOC). ACM Press.Babai, L. (1991). Local expansion vertex-transitive graphs random generation finite groups.Proceedings 23rd annual ACM symposium Symposium Theory Computing,pp. 164174.Chernoff, H. (1952). measure asymptotic efficiency tests hypothesis based sumobservations. Annals Mathematical Statistics, 23, 493507.Chung, F. (2006). diameter Laplacian eigenvalues directed graphs. Electronic JournalCombinatorics, 13 (4).Chung, F. R. K. (1997). Spectral Graph Theory. American Mathematical Society.Colbourn, C. J. (1984). complexity completing partial Latin squares. Discrete AppliedMathematics, 8 (1), 2530.Davis, H., Bramanti-Gregor, A., & Wang, J. (1988). advantages using depth breadthcomponents heuristic search. Ras, Z., & Saitta, L. (Eds.), Proceedings ThirdInternational Symposium Methodologies Intelligent Systems, pp. 1928, North-Holland,Amsterdam. Elsevier.Dechter, R., & Pearl, J. (1985). Generalized best-first search strategies optimality A*. J.ACM, 32 (3), 505536.Demaine, E. D. (2001). Playing games algorithms: Algorithmic combinatorial game theory.Proc. 26th Symp. Math Found. Comp. Sci., Lect. Notes Comp. Sci., pp. 1832.Springer-Verlag.Dinh, H., Russell, A., & Su, Y. (2007). value good advice: complexity A*accurate heuristics. Proceedings Twenty-Second Conference Artificial Intelligence(AAAI-07), pp. 11401145.Edelkamp, S. (2001). Prediction regular search tree growth spectral analysis. ProceedingsJoint German/Austrian Conference AI: Advances Artificial Intelligence, KI 01,pp. 154168, London, UK, UK. Springer-Verlag.Felner, A., Korf, R. E., & Hanan, S. (2004). Additive pattern database heuristics. JournalArtificial Intelligence Research, 22, 279318.Friedman, J. (2003). proof Alons second eigenvalue conjecture. STOC 03: Proceedingsthirty-fifth annual ACM symposium Theory computing, pp. 720724, New York, NY,USA. ACM.Gaschnig, J. (1979). Perfomance measurement analysis certain search algorithms. Ph.D.thesis, Carnegie-Mellon University, Pittsburgh, PA.Gomes, C., & Shmoys, D. (2002). Completing quasigroups Latin squares: structured graphcoloring problem. Johnson, D. S., Mehrotra, A., & Trick, M. (Eds.), ProceedingsComputational Symposium Graph Coloring Generalizations, pp. 2239, Ithaca, NewYork, USA.Gomes, C. P., Selman, B., & Kautz, H. (1998). Boosting combinatorial search randomization. AAAI 98/IAAI 98: Proceedings fifteenth national/tenth conference Artificial intelligence/Innovative applications artificial intelligence, pp. 431437, Menlo Park,CA, USA. American Association Artificial Intelligence.Hart, P., Nilson, N., & Raphael, B. (1968). formal basis heuristic determination minimumcost paths. IEEE Transactions Systems Science Cybernetics, SCC-4 (2), 100107.727fiDinh, Dinh, Michel, & RussellHelmert, M., & Roger, G. (2008). good almost perfect?. Proceedings AAAI-08.Hochbaum, D. (1996). Approximation Algorithms NP-hard Problems. Brooks Cole.Horn, R., & Johnson, C. (1999). Matrix Analysis. Cambridge University Press, Cambridge, UK.Huyn, N., Dechter, R., & Pearl, J. (1980). Probabilistic analysis complexity A*. ArtificialIntelligence, 15, 241254.Ibarra, O. H., & Kim, C. E. (1975). Fast approximation algorithms knapsack sumsubset problems. Journal ACM, 22 (4), 463468.Karp, R. M. (1972). Reducibility among combinatorial problems. Miller, R. E., & Thatcher,J. W. (Eds.), Complexity Computer Computations, p. 85103. New York: Plenum.Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack problems. Springer.Korf, R. (1985). Depth-first iterative deepening: optimal admissible tree search. ArtificialIntelligence, 27, 97109.Korf, R., & Reid, M. (1998). Complexity analysis admissible heuristic search. ProceedingsNational Conference Artificial Intelligence (AAAI-98), pp. 305310.Korf, R., Reid, M., & Edelkamp, S. (2001). Time complexity iterative-deepening-A*. ArtificialIntelligence, 129 (1-2), 199218.Korf, R. E. (2000). Recent progress design analysis admissible heuristic functions.AAAI/IAAI 2000, pp. 11651170. Also SARA 02: Proceedings 4th InternationalSymposium Abstraction, Reformulation, Approximation.Kumar, R., Russell, A., & Sundaram, R. (1996). Approximating Latin square extensions. COCOON 96: Proceedings Second Annual International Conference ComputingCombinatorics, pp. 280289, London, UK. Springer-Verlag.Laywine, C., & Mullen, G. (1998). Discrete Mathematics using Latin Squares. Interscience SeriesDiscrete mathematics Optimization. Wiley.Lenstra, A. K., Lenstra, H. W., & Lovasz, L. (1981). Factoring polynomials rational coefficients.Tech. rep. 82-05, Universiteit Amsterdam.Motwani, R., & Raghavan, P. (1995). Randomized Algorithms. Cambridge University Press.Parberry, I. (1995). real-time algorithm (n2 1)-puzzle. Inf. Process. Lett, 56, 2328.Pearl, J. (1984). Heuristics: Intelligent Search Strategies Computer Problem Solving. AddisonWesley, MA.Pisinger, D. (2005). hard knapsack problems?. Computers Operations Research,32, 22712284.Pohl, I. (1977). Practical theoretical considerations heuristic search algorithms. Elcock,W., & Michie, D. (Eds.), Machine Intelligence, Vol. 8, pp. 5572. Ellis Horwood, Chichester.Ratner, D., & Warmuth, M. (1990). (n2 1)-puzzle related relocation problems. JournalSymbolic Computation, 10 (2), 111137.Russell, S., & Norvig, P. (1995). Artificial Intelligence - Modern Approach. Prentice Hall, NewJersey.Sen, A. K., Bagchi, A., & Zhang, W. (2004). Average-case analysis best-first search tworepresentative directed acyclic graphs. Artif. Intell., 155 (1-2), 183206.Tay, T.-S. (1996). results generalized Latin squares. Graphs Combinatorics, 12, 199207.Vazirani, V. (2001). Approximation Algorithms. Springer-Verlag.728fiThe Time Complexity Approximate HeuristicsVazirani, V. (2002). Primal-dual schema based approximation algorithms. Theoretical AspectsComputer Science: Advanced Lectures, pp. 198207. Springer-Verlag, New York.Zahavi, U., Felner, A., Schaeffer, J., & Sturtevant, N. (2007). Inconsistent heuristics. ProceedingsAAAI-07, pp. 12111216.Zhang, Z., Sturtevant, N. R., Holte, R., Schaeffer, J., & Felner, A. (2009). A* search inconsistentheuristics. Proceedings 21st international jont conference Artifical intelligence,IJCAI09, pp. 634639, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.729fiJournal Artificial Intelligence Research 45 (2012) 165-196Submitted 06/12; published 10/12Coalition Structure Generation GraphsThomas Voicetdv@ecs.soton.ac.ukSchool Electronics Computer Science,University Southampton, UKMaria Polukarovmp3@ecs.soton.ac.ukSchool Electronics Computer Science,University Southampton, UKNicholas R. Jenningsnrj@ecs.soton.ac.ukSchool Electronics Computer Science,University Southampton, UKDepartment Computing Information Technology,King Abdulaziz University, Saudi ArabiaAbstractgive analysis computational complexity coalition structure generationgraphs. Given undirected graph G = (N, E) valuation function v : P(N ) Rsubsets nodes, problem find partition N connected subsets,maximises sum components values. problem generally NPcomplete;particular, hard defined class valuation functions independentdisconnected membersthat is, two nodes effect others marginal contribution vertex separator. Nonetheless, functions provide boundscomplexity coalition structure generation general minorfree graphs.proof constructive yields algorithms solving corresponding instancesproblem. Furthermore, derive linear time bounds graphs bounded treewidth.However, show, problem remains NPcomplete planar graphs, hence,Kk minorfree graphs k 5. Moreover, 3-SAT problem clausesrepresented coalition structure generation problem planar graphO(m2 ) nodes. Importantly, hardness result holds particular subclass valuationfunctions, termed edge sum, value subset nodes simply determinedsum given weights edges induced subgraph.1. IntroductionCoalition structure generation (CSG) equivalent complete set partitioningproblem (Yeh, 1986)one fundamental problems combinatorial optimisation,applications many fields, political sciences economics, operations researchcomputer science. CSG problem, set N n elements valuationfunction v : P(N ) R, P(N ) denotes power set N , problemdivide given set Pdisjoint exhaustive subsets (or, coalitions) N1 , . . . , Nmtotal sum values,i=1 v(Ni ), maximised. Thus, seek valuable partition (or,coalition structure) N .c2012AI Access Foundation. rights reserved.fiVoice, Polukarov, & JenningsPartitioning structure problems arise wide range practical domains including delivery management, scheduling, routing location problems, one wishes assureevery customer served one (and one) location, vehicle person (server).Commonly cited problems kind include crew-scheduling problem every flightleg airline must scheduled exactly one cockpit crew, political districting problem whereby regions must divided voting districts every citizen assignedexactly one district, coalition formation problem political parties (Balas &Padberg, 1976). Recently, CSG become major research topic artificial intelligencemulti-agent systems, tool autonomous agents form effective teams.example, electronic commerce buyer agents may pool demands order obtaingroup discounts (Tsvetovat, Sycara, Chen, & Ying, 2001); e-business coalitions may formorder satisfy certain market niches respond diverse ordersindividual agents (Norman, Preece, Chalmers, Jennings, Luck, Dang, Nguyen, Deora, Gray,& Fiddian, 2004); distributed vehicle routing coalitions delivery companiesreduce transportation costs sharing deliveries (Sandholm & Lesser, 1997). important applications include information gathering several information servers cometogether answer queries (Klusch & Shehory, 1996), multi-sensor networks sensorsform dynamic coalitions wide-area surveillance scenarios (Dang, Dash, Rogers, & Jennings, 2006), grid computing multi-institution virtual organisations viewedcentral coordinated resource sharing problem solving (Yong, Li, Weiming,Jichang, & Changying, 2003).However, classic CSG model assumes structure primitive set elements.considerable shortcoming, various contexts interest computer scientists,elements represent agents (either human automated) resources (e.g., machines,computers, service providers communication lines), typically embeddedsocial computer network. Moreover, many scenarios elementsdisconnected effect others performance potential contributioncoalition, connected intermediaries, may able cooperate all.example, consider communication network edge channel, capacityindicating amount information transmitted it. Thus,aforementioned contexts e-commerce, multi-sensor networks grid computing,network connects sellers buyers, sensors agents working computationaltasks, respectively. subset nodes network produces value proportionaltotal capacity subnetwork induced nodes. scenario, twonodes connected direct link network, affect othersmarginal contribution coalition nodes separates them. Or, also typicale-commerce e-business domains, assume edge represents trust linkreputation system, two nodes participate coalition trustdistance given length path them, finite (that is, coalition inducesconnected subgraph trust network). Suppose value coalition givennumber pairs mutually trusted membersi.e., edges inducedsubgraph. Then, contribution particular node depend another nodej trusts members coalition trust directly,edge j. Additional natural examples arise multi-agent systems domains,agents come together complete tasks. Typically, pair agents associ166fiCoalition Structure Generation Graphsated weight indicating potential mutual (in)efficiency task execution(e.g., due skill/expertise equipment complementarity, interpersonal (in)compatibility,(dis)agreements, spatial constraints). value coalition measuredtotal coalitional weight given sum weights links whose ends participate coalition. Importantly, weights positive negative, representingdifferent relations among agents, thus corresponding effects coalitionalvalue. Note agents zero weight links affect others contributioncoalition. Finally, correlation clusteringa well-known clustering technique motivatedproblem clustering large corpus objects, documents (e.g., web pagesweblog data given content/access patterns), customers service providers (withgiven properties past buying/selling records) biological species (plants animalsgiven features)operates setting elements need partitioned clusters (by topic, location, behaviour etc.) characterised similarity(and/or difference) relations among them. aim usually maximise overallagreementi.e., correlationof clusters. example, given signed graph edgelabel indicates whether two nodes similar (+) different (), task clusternodes similar objects grouped together, different onesseparately. Thus,value cluster C given total sum positive intra-cluster edges negative inter-cluster edges one end C. cases, connected (either positivelynegatively) members impact cluster values.background, paper extend CSG problem connected sets.precisely, introduce independence disconnected members consider coalition structures node set graph, endowed valuation functionproperty. formally defined Section 2 below, also give necessary graphtheoretic notation summarise main contributions. Then, Sections 3, 4 5,discuss results great detail present proofs. Specifically, Section 3 providescomputational bounds coalition structure generation general graphs, Section 4introduces technique solving problem using tree decompositions. technique,particular, allows us show linear time solvability graphs bounded treewidth.Section 5, apply derive upper bounds graphs separator theorems and,particular, planar graphs minorfree graphs. also present negative resultshowing NPhardness problem planar graphs hence, Kk minorfreegraphs, even simple, called edge sum, valuation function. discuss relatedliterature Secion 6. Finally, Section 7 concludes paper.2. Coalition Structure Generation Graphssection, formalise concepts independence disconnected membersgraph coalition structure generation, list main contributions. completeness,first provide graphtheoretic definitions notation necessary presentationresults following sections.2.1 GraphTheoretic Definitions NotationLet N set elements let Pk (N ) stand set k-element subsetsset N . simple undirected graph G pair G = (N, E) N finite set elements,167fiVoice, Polukarov, & Jenningscalled vertices (or, nodes) G, E subset P2 (N )i.e., E collectiontwo-element subsets N representing connections nodes, called edges G.complete graph graph pair nodes connected edge.complete graph n nodes denoted Kn . graph G bipartite graph verticesdivided two disjoint sets N1 N2 every edge connects vertexN1 one N2 . complete bipartite graph, G = (N1 N2 , E), bipartite graphtwo vertices, n1 N1 n2 N2 , {n1 , n2 } edge G. completebipartite graph |N1 | = |N2 | = n, denoted Km,n .undirected graph H called minor graph G H obtained Gseries vertex deletions, edge deletions and/or edge contractions (removing edgegraph simultaneously merging together two vertices used connect).graph G H minorfree H minor G. graph G planar K5 minorfreeK3,3 minorfree. important property planar graph embeddedplane, i.e., drawn way edges cross other. familiarspecial case planar graphs class grids: finite grid graph, verticesassociated two indices 1 r 1 j c, edge connectingnode ni,j nodes ni+1,j ni,j+1 (if exist)thus, r rows ccolumns graph, number nodes n = rc.subgraph H graph G induced pair nodes x H, {x, y}edge H edge G. words, H induced subgraphG exactly edges appear G vertex set. vertex setH subset N vertex set G, H said induced S.path graph sequence nodes node edgenext node sequence, path called simple contains repeated nodes.graph said connected path every pair nodes graph.tree graph two nodes connected exactly one simple path.Many algorithms graphs become easy input graph tree tree-like.notion tree-like formalised using concept treewidth: treewidthgraph small, tree-likein particular, tree treewidth 1. Treewidthdefined using concept tree decompositiona mapping graph tree.Formally, tree decomposition G = (N, E) pair (X, ), X = {X1 , . . . , Xm }n = |N | family subsets N , tree whose nodes subsets Xi ,satisfying following properties: (i) union sets Xi equals N is, graphvertex associated least one tree node; (ii) every edge {x, y} graph,subset Xi contains x y; (iii) Xi Xj contain vertex x,nodes Xk tree (unique) path Xi Xj contain x welli.e.,nodes associated vertex x form connected subset (equivalently, Xi , XjXk nodes, Xk path Xi Xj , Xi Xj Xk ). widthtree decomposition size largest set Xi minus one. Finally, treewidthgraph G minimum width among possible tree decompositions G.Given notation, formally define problem coalition structuregeneration graphs.168fiCoalition Structure Generation Graphs2.2 ModelRecall coalition structure set elements N defined collectiondisjoint exhaustive subsets N1 , . . . , Nm Ni Nj = 1 i, ji=1 Ni = N . Given setting finite set elements N connected undirectedgraph G = (N, E) coalition valuation function v : P(N ) R subsets N ,v() = 0, consider class coalition structure generation problems N .Accordingly, make following definitions.Definition 1 graph G = (N, E), function v : P(N ) R independent disconnected members (IDM) i, j N (i, j)/ E, coalition C i, j/ C,v(C {i}) v(C) = v(C {i, j}) v(C {j}).means agent contributes coalition C exactly amountcoalition C {j} j directly connected. is, presence agent jaffect marginal contribution agent separating coalition. NoteDefinition 1 generally restrict effects agents mayconnected.give example, suppose edge {i, j} E associated constantweight vi,j R. Then, coalition valuation functionv(C) =Xvi,j{i,j}E:i,jCIDM property. shall term function edge sum coalition valuation function. function important naturally arises many application scenarios (e.g.,communication networks, information multi-agent systems) simple representation. work Deng Papadimitriou (1994), function studied contextcomplexity cooperative game-theoretic solution concepts.functions type arise familiar clustering settings. example,suppose edge {i, j} labeled + depending whether jdeemed similar different. coalition (or, cluster) C N , let E + (C) ={{i, j} = + | i, j C} denote set positive intra-cluster edges, let E (C) ={{i, j} = | C, j/ C} set negative inter-cluster edges one end C.Then, correlation coalition valuation function definedv(C) = |E + (C)| + |E (C)|satisfies IDM condition. Note function takes account intra-intercoalitional connections, thus different edge sum, considersintracoalitional links. Maximising sum coalitional values coalition structures,produces partition nodes agrees much possible edge labels.objective pursued paper Bansal, Blum Chawla (2003) showNP-completeness problem complete graphs provide several approximationresults.Yet another example IDM function found multi-agent scenarios coalitions agents work different parts global project. settings, memberscoalition must make joint decisions communicate coalitions agentscoordinate actions. Furthermore, collaboration communication possible169fiVoice, Polukarov, & Jenningsclosely connected agents, important coalition includes agentsmutual neighbours outside coalition, decisions made coordinatedcoalitions. Given this, coalition valuation functionv(C) =Xni (C)iCni (C) number agent pairs (j, k) N N j C, k/ C{i, j}, {i, k} E, IDM property. shall term function coordination coalitionvaluation function. Obviously, considering intercoalitional links, function differentedge sum. However, note also difference coordinationcorrelation functions. latter, effect link two agents valuecoalition determined link label whether agentsbelong coalition. contrast, coordination function accounts fact 3-agentcliques, two agents members coalition one outsider.analysis, however, restricted particular valuation function rathercovers class functions characterised Definition 1. define graph coalitionstructure generation (GCSG) problem follows.Definition 2 Given connected undirected graph G = (N, E) coalition valuationfunction v : P(N ) R independent disconnected members,graph coalitionPstructure generation problem G maximise v(C) = CC v(C) C coalitionstructure N .GCSG posed clustering graph partitioning problem sum clustervalues, given IDM valuation function, maximised. instance,aforementioned correlation clustering special case GCSG. Note, however,clustering problems general necessarily fit model: indeed,objectives admit IDM property; hand, clusteringproblems additional restrictions feasible graph partitions. example, onenatural objectives domain maximise modularity clusters (Brandes,Delling, Gaertler, Gorke, Hoefer, Nikoloski, & Wagner, 2008) given sum clus2|(E(C)|+|E(C)|ter values defined follows. cluster C, let v(C) = |E(C)|,|E|2|E|E(C) = {{i, j)} E : i, j C} set intra-cluster edges C E(C) ={{i, j} E : C, j/ C} set inter-cluster edges. Notice second termvaluation funciton squared, implies violation IMD property. Another related setting weighted graph partitioning problem nodes edges(non-negative) weights aim divide graph k disjoint partsparts approximately equal weight size edge cut minimised. Crucially,unlike model, case number subsets feasible partition fixed.2.3 Main ResultsHere, main results paper summarised. start observingGCSG problem NPcomplete general graphs, even edge sum valuation functions(Section 3). Alongside hardness result, show thata general instance |N | = nnodes |E| = e edges solved time n2 e+n(see Theorem 3).n170fiCoalition Structure Generation Graphsorder improve time required solving problem, make use treedecompositions. show graph n nodes tree decomposition widthw, GCSG problem O(ww+O(1) n). allows us derive upper boundcomputational complexity GCSG certain classes graphs, namely graphs boundedtreewidth, graphs separator theorems and, particular, planar graphs minorfreegraphs. also show subclass edge sum GCSG problems NPhard planargraphs hence, Kk minorfree graphs k 5 (see Section 5.1).Planar graphs exceptional family graph drawn planewithout edge crossing. Apart interesting mathematical properties as,example, 4colourability 3path separability, planar graphs many practicalapplications, including design problems circuits, subways utility lines. networkcrossing connections, usually means edges must run different heights.big issue electrical wires, would create extra expensestypes linese.g., burying one subway tunnel another (and therefore deeperone would normally need). Circuits, particular, easier manufactureconnections live fewer layers. Importantly, one may determine graphs planarity usingcalled forbidden minor characterisation, graph planarcontain complete graph K5 complete bipartite graph K3,3minor (Wagner, 1937).1 Remarkably, forbidden minor characterisations exist several graph families vary nature forbidden, utilisedcombinatorial algorithms, often identifying structure (Robertson & Seymour, 1983,1995, 2004). motivates particular interest classes minorfree graphs.next theorem main technical result.Theorem 1 general instance graph coalition structure generation problemgraph G n nodes known tree decomposition width w solvedO(ww+O(1) n) computational steps.gives us immediate corollary.Corollary 1 fixed w, GCSG problem graph G n nodes maximum treewidth w solved O(n) computational steps.proof results presented Section 4. Coupled known results regardingseparator theorems gives base following contributions (see Section 5proofs).Corollary 2 graph H k vertices, instance graph coalition structuren+O(1) )generation problem HminorfreegraphGnnodesrequiresO(npcomputation steps = 0.5k k/(1 2/3).Corollary 3 general instance graph coalitionstructure generation problemn+O(1) ) computation steps, =planargraphp G n nodes solved O(n2/(1 2/3).1. characterisation Wagners theorem closely related (but equivalent) Kuratowskis theorem, states graph planar contain subgraph subdivisionK5 K3,3 (Kuratowski, 1930).171fiVoice, Polukarov, & JenningsHowever, planar graphs also prove following hardness result.Theorem 2 class edge sum graph coalition structure generation problems planar graphs NPcomplete. Moreover, 3-SAT problem clauses representedGCSG problem planar graph O(m2 ) nodes.Note Theorem 2 holds Kk minorfree graphs k 5, planar graphsspecial case. means expect take time exponential n solveGCSG problem graphs size n. suggests methods givenCorollaries 2 3, solve problems time exponential log(n) n, closebest possible.background, main contribution work shows significantimprovement complexity exact algorithms general class coalition structure generation problems characterised single assumption independence disconnectedmembers valuation functions. particular, results especially valuablegraphs tree decomposition (low) width assessed.remaining sections describe main results techniques detailcontain proofs.3. General Graphssection, examine complexity coalition structure generation generalgraphs. first step, make technical observation showing without lossgenerality problem restricted subset coalition structures follows.Definition 3 graph G = (N, E), coalition structure C N connectedinduced subgraph G C connected C C.Lemma 1 imply GCSG problem equivalent maximisingobjective function connected coalition structures Definition 3. notelemma follows directly Definition 1 IDM property provide full proofappendix.Lemma 1 Given graph G = (N, E) coalition valuation function v() IDMproperty, A, B N edges G \ B B \ A,v(A) v(A B) = v(A B) v(B).Note, Definition 1, v() IDM two coalitions B Cdisconnected, Lemma 1, v(B C) = v(B) + v(C). So, coalition C, valuev(C) equal sum v() connected components. deduce that,coalition structure C exists coalition structure v(C) = v(D)coalitions connected subgraphs. Thus, without loss generality, restrictattention connected coalition structures. Moreover, G connected graph,solve coalition structure problem G IDM coalition valuationfunction finding optimal coalition structure connected component Gcombining results. operation testing connectivity finding connected172fiCoalition Structure Generation Graphscomponents computationally tractable polynomial time (Hopcroft & Tarjan, 1973),so, without loss generality, restrict attention connected graphs G.(connected) graph G = (N, E) set nodes N set edges E,denote |N | = n |E| = e. Next, present simple algorithm constructing optimalcoalition structures N , based following observation. Note everyconnected coalition structure N expressed connected componentssubgraph G0 = (N, E 0 ) G, E 0 E. Moreover, connected componentspanning subtree, restrict attention acyclic subgraphs G. Given this,Algorithm 1 runs acyclic subgraphs G connected components,correspond connected coalition structures set nodes N . would likeremark order subgraphs G checked, effectoutcome, chosen arbitrarily. Thus, w.l.o.g., initialise procedurecoalition structure C = ({n1 }, . . . , {nn }) corresponds connected componentssubgraph G0 = (N, ) G.Algorithm 1 algorithm coalition structure generation general graphs.1:INPUT: connected undirected graph G = (N, E);2:IDM coalition valuation function v : P(N ) R3:OUTPUT: optimal connected coalition structure N w.r.t. v4:C ({n1 }, . . . , {nn })5:E 0 E G0 = (N, E 0 ) acyclic6:find C(G0 ) = ({C1 }, . . . , {Ck0 })the collection connected components G0P 07:v (C(G0 )) = ki=1 v(Ci ) > v(C)8:C C(G0 )9:end10: endshow following.Theorem 3 Algorithm 1 solves general instance GCSG problem n2steps, using O(n log n) sized memory.e+nn000Proof : acyclic subgraphE,aata+1n1aedges,Pn1 e G = (N, E ) G, Ea+1k=0 k subgraphs. Since b + b1 = b b b ,2sum bounded e+ndetermine connectedn . Now, takes O(n ) stepse+n2components subgraph, and, thus, n nsteps needed checkcoalition structure. Finally, takes O(n log n) sized memory storecoalition checked.2Coupled Corollary 2.3 paper P. Stanica (2001), Theorem 3 impliesfollowing result sparse graphs.Corollary 4 sparse graphs e = cn edges, c constant, GCSG problemc+1.n3/2 n constant = (c+1)cceasy particularly promising result, may exponential n log(n)exponential n even sparse graphs. Indeed, class graph coalition structure173fiVoice, Polukarov, & Jenningsgeneration problems NPhard: contains subclass GCSG problems completegraphs, equivalent NPcomplete class standard coalition structure generation problems node sets. Importantly, problem remains hard even simplecoalition valuation functions, correlation function (Bansal et al., 2003). noteholds edge sum function well: result seen corollaryTheorem 2 showing hardness edge sum GCSG planar graphs.4. Tree Decompositionsconsider solving GCSG problem graphs known tree decompositions.Specifically, prove main technical result (Theorem 1) giving general boundGCSG graphs, derive Corollary 1 regarding graphs boundedtreewidth. proof follows recursively calculating potential marginal contributions total coalition structure valuation branch tree decomposition (seeAlgorithm 2). build intuition, first derive two technical lemmas. brevityexposition, proofs presented Appendix.Lemma 2 Let G = (N, E) graph tree decomposition (X, ), X ={X1 , . . . , Xm } n = |N | tree X. Suppose Xinumbered order shortest distance X1 , X1 may chosen arbitrarily.Then, C N ,v(C) =Xv(C Xi ) v C Xii=1[Xj .j<iLemma 2 allow us calculate value total coalition structure localstructures defined branches tree decomposition. discuss constructtotal structure local ones. need following notation.graph G = (N, E), P, Q N , P coalition structure PQ coalition structure Q, defineU (P, Q) = {A P : (P \Q)}{B Q : B (Q\P )}{AB : P, B Q, AB 6= }.is, U (P, Q) collection subsets P Q agrees P P \ QQ Q \ P , contains pairwise unions subsets P B Q non-emptyintersections. Note U (P, Q) necessarily coalition structure P Q,union coalitions B, P, B Q, need disjoint.Furthermore, graph G = (N, E) coalition structure P subsetnodes P N , subset P 0 P denote P(P 0 ) coalition structureP 0 defined follows:P(P 0 ) = {C P 0 : C P}.is, x, P 0 P , belong coalition P(P 0 )belong coalition P.illustration, consider following example. Let N = {1, 2, 3, 4, 5}, take two subsetsP = {1, 2, 3} Q = {3, 4, 5} N , define coalition structures P = {{1}, {2, 3}}Q = {{3, 4}, {5}} P Q, respectively. Note {1} P subset174fiCoalition Structure Generation GraphsP \ Q, {5} Q subset Q \ P , ({2, 3} P) ({3, 4} Q) = {3}. Then,U (P, Q) = {{1}, {5}, {2, 3} {3, 4}} = {{1}, {5}, {2, 3, 4}}. Now, let P 0 = {1, 2} PQ = {4, 5} Q. Then, P(P 0 ) = {{1} {1, 2}, {2, 3} {1, 2}} = {{1}, {2}}Q(Q0 ) = {{3, 4} {4, 5}, {5} {4, 5}} = {{4}, {5}}.Lemma 3 graph G = (N, E), P, Q N , P coalition structureP Q coalition structure Q, P(P Q) = Q(P Q), E = U (P, Q)coalition structure P Q P 0 P , Q0 Q, E(P ) = P(P 0 )E(Q) = Q(Q0 ).ready prove Theorem 1. end, present Algorithm 2 that,given graph known tree decomposition, finds best coalition structure nodeset recursively calculating potential marginal contributions total coalition structure valuation branch given tree decomposition. Lemma 4 provesvalidity computational bounds.Algorithm 2 algorithm coalition structure generation graphs known treedecompositions.1:INPUT: connected undirected graph G = (N, E);2:tree decomposition (X, ) G, X = {X1 , . . . , Xm } n,3:tree X, 1 < j dT (Xi , X1 ) dT (Xj , X1 ),4:1 m, dT (Xi , X1 ) distance Xi X15:IDM coalition valuation function v : P(N ) R6:OUTPUT: optimal connected coalition structure N w.r.t. v7:18:Yi Xi \ j<i Xj9:Zi Xi \ Yi10:Di {j > : (Xi , Xj ) }11: = m, 1, . . . , 112:CcoalitionP structures Zi P13:vi (C) maxE CE v(C) v(C \ Yi ) + jDi vj (E(Zj )),14:E coalition structures Xi E(Yi ) = C15: end16: C0 arg maxC v1 (C) C colition structures Z117: k = 1, . . . ,18:Ck U (Ck1 , Ek ),19:Ek coalitionP structure XkP Ek (Zk ) = Ck1 (Zk )20:vk (Ck1 (Zk )) = CEk v(C) v(C \ Yk ) + jDk vj (Ek (Zj ))21: end22: output CmLemma 4 Algorithm 2 solves general instance graph coalition structure generation problem graph G n nodes known tree decomposition width wO(ww+O(1) n) computational steps.175fiVoice, Polukarov, & JenningsProof : given graph G = (N, E) tree decomposition (X, ),X = {X1 , . . . , Xm } n = |N | tree X. Suppose w, |Xi | < wi. assume without loss generality Xi numbered order shortestdistance X1 , X1 may chosen arbitrarily. Thus, > 1, Xiexactly one link connects Xj j < i. define YiXi \ j<i Xj Zi Xi \ Yi . Note, > 1 exists single j <Zi Xj , hence Zi = (Xj Xi ). Since every node must least one Xi ,union Yi N . Finally, i, Di set j > (Xi , Xj )edge .Now, = m, 1, . . . , 1, algorithm recursively define functions vi ()give real values coalition structure Zi . C, coalition structure Zi ,let vi (C) maximumXXvj (E(Zj )),v(C) v(C \ Yi ) +jDiCEcoalition structures E Xi E(Yi ) = C. Note j Di , Zj =(Xi Xj ), hence, coalition structure E Xi , E(Zj ) forms coalition structureZj .Now, suppose C coalition structure G. show v(C) v1 (C(Z1 )).showing inductively that, k 1,v1 (C(Z1 ))kXX(v(C) v(C \ Yi )) +i=1 CC(Xi )Xvj (C(Zj )).(1)jDi :j>kk = 1 follows definition v1 (), C(X1 ) coalition structure X1 .sufficient show right hand side (1) increase k increases.general k change right hand side (1) preceeding iterationXX(v(C) v(C \ Yk )) +vj (C(Zj )) vk (C(Zk )).jDkCC(Xk )follows definition vk () value non-positive, (C(Xk )) coalitionstructure Xk . Hence, inductive proof complete. Thus, shownv1 (C(Z1 ))XX(v(C) v(C \ Yi )) = v(C),i=1 CC(Xi )Lemma 2. So, maximum v1 (E) coalition structures E Z1 greaterequal maximum value v(C) coalition structures C G.Now, let C0 coalition structure Z1 maximises v1 (C). algorithmrecursively defines coalition structures C1 , C2 , . . . Cm setting, 1 < k m, Ck =U (Ck1 , Ek ), Ek coalition structure Xk Ek (Zk ) = Ck1 (Zk )XXvk (Ck1 (Zk )) =v(C) v(C \ Yk ) +vj (Ek (Zj )).CEkjDk176fiCoalition Structure Generation Graphswant showv1 (C(Z1 )) =kXX(v(C) v(C \ Yi )) +i=1 CCk (Xi )Xvj (Ck (Zj )).(2)jDi :j>kAgain, use induction. k = 1, follows definition v1 (), notingsince C1 = U (C0 , E1 ), Lemma 3 implies C1 (X1 ) = E1 (X1 ), since coalitionstructures X1 , must C1 = E1 .Now, general k, since Ck = U (Ck1 , Ek ), must have, < k, Ck (Xi ) =Ck1 (Xi ), j Di j k, since Zj Xi , have, Ck (Zj ) = Ck1 (Zj ).Thus, change right hand side (2) previous increment equalXX(v(C) v(C \ Yi )) +vj (Ck (Zj )) vk (Ck (Zk ))=CCk (Xk )jDkXX(v(C) v(C \ Yi )) +vj (Ek (Zj )) vk (Ck1 (Zk )) = 0,jDkCEk (Xk )definition Ck Ek . completes inductive proof.shownv1 (C(Z1 )) =XX(v(C) v(C \ Yi )) = v(Cm ).i=1 CCm (Xi )Since v1 (C(Z1 )) upper bound v() coalition structures N , mustCm solution coalition valuation problem.Finally, order solve coalition valuation problem, needs donefully calculate vk () k 1, recording corresponding optimal coalitionstructures value, optimise v1 (). this, k, gocoalition structure E Xk , calculateXXv(C) v(C \ Yi ) +vj (E(Zj )).CEjDigreater currently held value vk (E(Zk )), replace value alsorecord E. requires polynomial (in w) calculations possible coalition structurenode Xk , gives O(ww+O(1) ) calculations Xk thus O(ww+O(1) n)calculations total.2Theorem 1 follows immediately Algorithm 2 Lemma 4. Now, given w,class graphs maximum treewidth w, tree decomposition widthw may found linear time (Bern, Lawlerand, & Wong, 1987). Given this, Corollary 1directly implied Theorem 1.Corollary 1 fixed w, GCSG problem graph G n nodes maximum treewidth w solved O(n) computational steps.177fiVoice, Polukarov, & Jenningsset w = 1, result applies acyclic graphs, related results Demange (2004) regarding coalition structure generation trees. However, Demange (2004)make IMD assumption. resulting algorithm complexpotentially exponential running time. expected, without independence disconnected members, coalition structure generation problem starnetworks necessarily exponential.Note, set w = 2, class graphs consideration becomes classK4 minorfree graphs. Likewise, class graphs treewidth 1 may characterisedK3 minorfree. results sharp contrast Theorem 2 shows NPcompleteness edge sum GCSG problem planar graphs, subsetclass K5 minorfree graphs. give proof Theorem 2 next section.5. Separator Theoremssection, prove computational bounds GCSG problem minorfreeplanar graphs. graphs guaranteed contain vertex separators, formalisedDefinition 4 below. Intuitively, means graphs corresponding classsplit smaller pieces removing small number vertices. general,Definition 4 class graphs G satisfies f (n)-separator theorem constant < 1G = (N, E) G |N | = n exists subset N |S| f (n)N \ = B disjoint B where, |A| n, |B| n, existsx B (x, y) E.illustrate this, consider example grid graph G r rows c columns,n = rc number nodes. r odd, single central row, otherwise,two rows equally close center; similarly, c odd, singlecentral column, otherwise, two columns equally close center. Letnode subset central rows columns. Removing graphdivide two smaller disjoint components, B, n/2vertices. r c, central column defines separator r n vertices,similarly, c r, central row separator n vertices. Thus,grid graph separator size n, removal splits graphtwo connected components, size n/2. is, class grid graphssatisfy n-separator theorem constant = 1/2.use Theorem 1 derive Algorithm 3 Lemma 5, provide usgeneral result classes graphs satisfy separator theorems. applyresult classes minorfree planar graphs, coupled correspondingseparator theorems, obtain computational bounds coalition structure generationgraphs.Suppose class graphs G closed taking subgraphs satisfiesf (n)-separator theorem constant < 1, f (n) = nc constants, c, exists algorithm find separator G G n nodespolynomial time. Given this, graph G G, Algorithm 3 findstree decomposition treewidth w nc /(1 c ) polynomial time. procedurebased proof Theorem 20 work Bodlaender (1998), states178fiCoalition Structure Generation Graphsclass graphs G, treewidth G G n nodes O(f (n)).apply Algorithm 2 solve GCSG problem G tree decompositionO(ww+O(1) n) computational steps, finally provides us computational boundcO(n 1c nc +O(1)) time, stated Lemma 5 below.Algorithm 3 algorithm coalition structure generation graphs separatortheorems.1:INPUT: graph G = (N, E) G; IDM coalition valuation function v : P(N ) R2:OUTPUT: optimal connected coalition structure N w.r.t. v3:n = 14:X ({x}), x N node G5:T0 G6:otherwise7:find S, nc separator G N \ = B |A| n |B| nc nc8:find tree decompositions (X , ) (X B , B ) B, width1c9:0 B {e0 } e0 = {x, y} E, x A, B10:X {X : X X }11:X B {X : X X B }12:X X X B13: end14: apply Algorithm 2 G tree decomposition (X, 0 )Lemma 5 Let G class graphs closed taking subgraphs satisfiesf (n)-separator theorem constant < 1, f (n) = nc constants , c,exists algorithm find separator G G n nodes polynomialtime. Then, Algorithm 3 solves GCSG problem graph G G n nodescO(nn +O(1) ) time,c=.1 cProof : Suppose class graphs G satisfying statement lemma.must exist constants K > log (2) G G n nodes, findnc separator, constant , Knd computational steps. proof proceedsshowing that, graph G G n nodes, Algorithm 3 (steps 313) finds treedecomposition width less equal nc /(1 c ) Knd /(1 2d )computational steps. prove result induction.n = 1 computational steps required G already tree form. nthinductive step, suppose G = (N, E) G |N | = n. Knd computationalsteps find S, nc separator G N \ = B |A| n|B| n. inductive hypothesis, apply steps 313 Algorithm 3 find treedecompositions (X , ) (X B , B ) subgraphs B respectively, takingtotal time 2Kd nd /(1 2d ), (X , ) (X B , B ) maximal widthc nc /(1 c ). Now, let X = {X : X X }, let X B = {X : X X B },let 0 tree formed connecting B single edge. Then, claim179fiVoice, Polukarov, & Jennings(X X B , 0 ) tree decomposition G. \ S, set elementsX appears in, forms subtree , thus, set elements X X Bappears in, must form subtree 0 . symmetry, holds B \ S.Further, S, appears every element X X B . Lastly, pair nodesconnected edge G, nodes lie inside B,element X X B respectively, otherwise least one nodes must lieS, must member every element X X B . proves claim.tree decomposition (X X B , 0 ) tookKnd +2Kd ndKnd=1 2d1 2dcomputational steps find widthnc +c ncnc=,1 c1 crequired. completes inductive proof.Thus, G G n nodes, find tree decomposition G treewidthnc /(1 c ) polynomial time. apply Algorithm 2, solveGCSG problem graph G G n nodes O(ww+O(1) n) computational steps,w = nc /(1 c ). However,ww =cccncn /(1 ) = O(nn ),1 c2statement lemma follows.result allows us obtain computational bounds GCSG problem minorfree planar graphs follows.Corollary 2 graph H k vertices, instance graph coalitionstruc n+O(1) )ture generation problem anH minorfreegraphGnnodesrequiresO(npcomputation steps = 0.5k k/(1 2/3).Proof : apply Lemma 5 using main result paper Alon,SeymourThomas (1990) shown class graphs satisfies k kn-separatorn+O(1) )theoremp Thus, solve general instance problem O(n= 2/3.= k k/2(1 2/3), required.2noted Proposition 4.5 Alon, Seymour Thomas (1990) givesbound k kn treewidth class graphs, constructive, cannotcombined Theorem 1 requires tree decomposition available.planar graphs, Corollary 3 provides stronger result.Corollary 3 general instance graph coalition structuregeneration problemplanar graph G n nodes solved O(n n+O(1) ) computation steps,180fiCoalition Structure Generation Graphs=2/(1p2/3).Proof : apply Lemma 5 using main result workof Lipton Tarjan (1979)shown class graphs satisfies 2 2n-separator theoremn+O(1)=2/3. Thus,)p solve general instance problem O(n= 2/(1 2/3), required.2Recall class planar graphs equivalent class K3,3 K5 minorfreegraphs. graphs, Theorem 2 shows graph coalition structure generationproblem NPcomplete, even simple, edge sum, coalition valuation functions (the prooftheorem presented 5.1 below). However, mentioned previous section,GCSG smaller minorfree instances solved linear time.5.1 Planar Graphsprove NP-hardness result planar graphs. Since planar graphs K5 minor free,hardness result must hold class Kk minorfree graphs k 5.proof proceeds finding representation general 3-SAT problem GCSG problemplanar graph.Theorem 2 class edge sum graph coalition structure generation problems planar graphs NPcomplete. Moreover, 3-SAT problem clauses representedGCSG problem planar graph O(m2 ) nodes.Proof : Suppose 3-SAT problem clauses C1 , . . . Cm . constructedge sum graph coalition structure generation problem planar graph O(m2 ) nodeswhich, solved, reveals solution 3-SAT problem one exists. useseries diagrams define components construct appropriateedge sum graph. diagrams denote edge values using symbols given keyFigure 1.first component given Figure 2. use symbol Subfigure 2brepresent three nodes surround subgraph edge values given Subfigure 2a.subgraph edge sum problem graph, contribution edge valuesmake valuation coalition structure 2, equality inducedstructure three outer nodes shown one Subfigure 2c, Subfigure 2dSubfigure 2e. induced coalition structure three nodes onetwo structures, contribution less 2. similarly describe twotriangular components Figures 2, 4 5. planar graph edge sum problemconstruct created components, connected edgesvalue 1, others overlap, sense share nodes.components sharing nodes other, share edges. Moreover,components share nodes form triangle borders component.two components share pair nodes, represent symbolically drawingsymbols adjacent along corresponding side triangular181fiVoice, Polukarov, & Jennings10-2(a) Edge valuesFigure 1: Edgevalue key.(b) Symbol(c) Optimum 1(d) Optimum 2(e) Optimum 3Figure 2: Edge sum problem component.(a) Edge values (b) Symbol (c) Optimum 1 (d) Optimum 2(a) Edge values(b) Symbol(c) OptimumFigure 4: Edge sum problem component.Figure 3: Edge sum problem component.symbols. So, edges symbols components touch, meancomponents share edge within graph.graph consisting components, constructed way, saycoalition structure locally optimal induced structure every componentoptimal component every connecting edge part componentlies inside coalition. every coalition structure, component, contributionedges component make value coalition structure boundedlocal optimum. Thus, coalition structure locally optimal mustoptimal. Furthermore, coalition value coalition structure straightforwardcalculate - simply sum local optimums component connecting edge. Note,value obtained always represents upper bound total valuationcoalition structure, thus locally optimal structure exists, optimal coalitionstructures must locally optimal. However, guaranteed locally optimalstructure exist.mind, possible provide intuition regarding components.component Figure 5 coalition structure locally optimalunless three nodes form outer triangle either lie coalitiondifferent coalitions. component Figure 3 coalition structurelocally optimal unless exactly one bottom two nodes coalitiontop node. component Figure 2 similar Figure 3, except allowsaddition possibility locally optimal coalition structure three outer nodesdifferent coalitions. component Figure 4, coalition structurelocally optimal bottom two node coalition, coalitioncontain top node. describe constructs madedescribed components. first given Figure 6. locallyoptimal coalition structure, nodes X always coalition pairnodes labelled lie coalition pair nodes182fiCoalition Structure Generation GraphsX(a)Construc-XBB(b) Symbol(a) Constructiontion(b) Optimum 1X(c) Optimum 1(d) Optimum 2Figure 5: Edge sum problemcomponent.B(c) Optimum 2Figure 6: Edge sum problem construct.labelled B lie coalition other. reduction 3-SAT problems,representing logical states whether certain pairs agents liecoalition locally optimal coalition structure. construct allows us enforcetwo pairs represent logical state whilst also allowing coalition passesplane.second third constructs given Figures 7 8. second construct,locally optimal coalition structure, pair nodes labelled togethercoalition, pair nodes labelled B coalition, similarlypair nodes labelled C. pair nodes labelled coalition,pair nodes labelled B coalition, similarly pairnodes labelled C. Thus, representation 3-SAT problem, locally optimalsolution pairs nodes labelled A, B C always represent logical state.third construct similar, except locally optimal coalition structure,state whether pair nodes labelled C coalitionopposite state two pairs nodes. Thus, representation3-SAT problem, locally optimal solution pairs nodes labelled Brepresent logical state, C represent negation state. lastconstruct given Figure 9. complex constructs, shallfirst examine three subgraphs it. first part, AX, consists subgraph threecomponents pair nodes labelled pair nodes labelled X, second,covers three components B CZ consists bottom two components.Note middle triangle diagram edges X, Y, Z, component, merely183fiVoice, Polukarov, & JenningsBBBCC(a) ConstructionC(b) Optimum 1(c) Optimum 2Figure 7: Edge sum construct.BBBCC(a) Construction(b) Optimum 1C(c) Optimum 2Figure 8: Edge sum construct.empty space. Subfigures 9b9h show locally optimal coalition structuresthree parts (with outer nodes component shown). Sinceconstruct union three parts, coalition structure locally optimalsubgraphs, locally optimal whole construct. However,every combination local optimums possible. For, coalition structure inducesSubfigure 9b AX Subfigure 9d three node triangle XYZ must liecoalition, possible coalition structure induce Subfigure 9f.locally optimal coalition structure, cannot true node A, B C liesdifferent coalition node paired with. Suppose think pair nodesrepresenting false state lie coalition true statedifferent coalitions. Then, locally optimal coalition structure construct,least one A, B C must represent true state. straightforward checkexist locally optimal coalition structures construct induce every possiblecombination states besides A, B C represent falsehood. Thus,construct enforces logical within 3-SAT solution representation. constructedge sum problem represent general 3-SAT problem follows. create copyconstruct Figure 9 clause problem. three pairs labelled A, B, Cidentified three literals corresponding clause. identify coalitionstructure constructs set logical values literals clausessaying literal associated pair node set true nodeslie inside single coalition. variable create path copies constructs184fiCoalition Structure Generation GraphsXBZXXC(a) Construction(b) AX: Optimum 1(c) AX: Optimum 2Z(d) BY: Optimum 1BZZBC(e) BY: Optimum 2CC(f) CZ: Optimum 1 (g) CZ: Optimum 2 (h) CZ: Optimum 3Figure 9: Edge sum construct.Figures 7 8, pair nodes labelled B one component sharedlabelled following component. path include copy constructFigure 7 literal representation variable, copy constructFigure 8 literal representation variables negation. connectpair nodes represents literal representation variable negationpair nodes labelled C corresponding construct path, using parallel pairconnecting edges, value 1. ensures locally optimal coalition structureassign consistent logical values literal representations variablenegative. ensure resulting graph planar, replace two parallelpairs connecting edges cross two copies constructFigure 6. For, two copies construct Figure 6 first copy sharesnodes labelled B nodes labelled second copy, then, locallyoptimal coalition structure, logical value represented nodes labelledfirst construct equal logical value represented nodes labelled B secondconstruct. Furthermore logical value represented nodes labelled X twoconstructs equal logical value nodes labelled two constructs.allows logical values pass plane.construction, locally optimal coalition structure exists, original 3-SATproblem must satisfiable. Furthermore, 3-SAT problem satisfiable,simply set construct locally optimal coalition structure agreeslogical value variables literals, create coalition structureentire graph taking union overlapping coalitions. Note, always possibleconstruction. constructs Figures 7 9 designed inducedoptimums, nodes never coalition node B C,nodes B never coalition node C. Moreover, constructFigure 6 locally optimal structure, coalition XY always disjointnodes B. means combining two locally optimal coalition structuresagree across pairs create coalitions local two pairs nodesconnected edges used connect them. Thus, combining severalconnections always possible without contradiction.185fiVoice, Polukarov, & JenningsBB!ABBBB!BB!A!AC!B!C!CCCCB!ACCFigure 10: Reduction (A B B) (!A!B!C) (!A B C).So, locally optimal coalition structure exists original 3-SAT problemsatisfiable, given locally optimal coalition structure, identify solution3-SAT problem. Furthermore, locally optimal coalition structure exists,coalition structure optimal locally optimal. size graphO(m2 ) thus result follows. example reduction process shownFigure 10 3-SAT problem (A B B) (!A!B!C) (!A B C).26. Related Worksection, give overview related work, broadly classifiedtwo main categories: clustering algorithms algorithms coalition structuregeneration (CSG). former relevant work deals partitioninggraph structures subgraphs; however, unlike case, values partitionsdetermined certain, problemspecific valuation function. latter,hand, considers general valuation functions, allows structure primitiveset elements.detail, clustering one primery tools machine learning dealsfinding structure collection unlabeled data. goal organise objectsgroupsclusterswhose members similar dissimilarobjects belonging clusters. certain relevant scenarios, instead actual description objects, relationships known. Thus, likework, objects typically represented node set signed graph,edge labels indicate whether two connected nodes similar different. However,clustering algorithms usually designed solving problems associated particularobjectives (and hence, valuation functions)e.g., correlation modularity mentioned previous sections. contrast, work concerned general classvaluation functions, characterised single assumption independence disconnected members. Thus, particular, Corollary 1 viewed generalisationresult Xin (2011) providing linear time algorithm correlation clusteringgraphs bounded treewidth. sense, literature CSG problemsurvey below, perhaps relevant research, deals designing universal186fiCoalition Structure Generation Graphsalgorithms, valuation function part input. However, hand,works assume structure primitive set elements.several algorithms developed CSG. Sandholm, Larson, Andersson,Shehory TohmeIn (1999), proposed anytime procedure worst case guarantees;however, reaches optimal solution checking possible coalition structures,runs time O(nn ). Specifically, given graph node set represents coalitionstructures, connected edge belong two consequtivelevels coalition structure level (i 1) obtained one levelmerging two coalitions one, algorithm firstly searches two bottom levels,explores remaining levels one one, starting top moving downwards.similar algorithm proposed Dang Jennings (2004): searching twobottom one top level, algorithm goes certain subsets remaininglevels (as determined sizes coalitions present corresponding structures),instead searching levels one one. hand, algorithms based dynamicprogramming (DP) (Yeh, 1986; Rothkopf, Pekec, & Harstad, 1998) work iteratingcoalition structures size 1, size 2, size n:every coalition C, value coalition compared value could possiblyobtained splitting C two coalitions. Visualising process graphcoalition structures before, start bottom node move upwardsseries connected nodes (a path) optimal node reached. Importantly,multiple paths lead optimal node, DP reachpaths. Based observation, improved dynamic programming algorithm(IDP) developed Rahwan Jennings (2008b). main idea IDP removeedges coalitions structure graph disregard many splittings coalitionspossible, yet without losing guarantee path leads every nodegraph. avoids counting approximately 2/3 operations compared DPevaluates every edge coalition structure graph, meaning IDP find optimalsolution O(3n ) time. However, DP IDP algorithms anytimethat is,allow trade computation time solution quality. end, Rahwan, Ramchurn,Giovannucci Jennings (2009) developed integer partition (IP) algorithm,anytime. works dividing search space regions, according coalitionstructure configurations based sizes coalitions contain, performingbranch-and-bound search. Although procedure worst case complexity O(nn ),practice, much faster DP based algorithms. Furthermore, IP algorithmimproved upon, using DP preprocessing (Rahwan & Jennings, 2008a). date,combined algorithm, termed IDP-IP, fastest anytime algorithm, capablefinding optimal solution O(3n ) time.CSG problem also tackled heuristic methods. particular, SenDutta (2000) gave genetic algorithm starts initial, randomly generated, setcoalition structures, called population, repeatedly evaluates every membercurrent population, selects members based evaluation, constructs newmembers selected ones exchanging and/or modifying contents. Keinnen (2009), based process Simulated Annealinga generic, stochastic local searchtechnique: iteration, algorithm explores different neighbourhoods certaincoalition structure, every neighbourhood defined according different criterion.187fiVoice, Polukarov, & Jenningshand, Shehory Kraus (1998) proposed decentralised greedy procedureiteration, best candidate coalitions (those overlapcoalitions currently present coalition structure) added structure,search done distributive fashioni.e., agents negotiate onesearches coalitions. significantly improved distribution mechanism laterproposed Rahwan Jennings (2007). Another greedy algorithm (Mauro, Basile, Ferilli, & Esposito, 2010) based GRASPa general purpose greedy algorithm that,iteration, performs quick local search try improve solution (Feo & Resende,1995). CSG version GRASP, coalition structure constructed iteratively,every iteration consists two steps: first add best candidate coalitionstructure, second explore different neighbourhoods current structure.two iterations repeated whole set agents covered, wholeprocess repeated achieve better solutions. However, heuristic techniquesguarantee optimal value reached point, givemeans evaluating quality coalition structure selected.alternative approach CSG problem utilise compact representation schemesvaluation functions proposed (Ohta, Conitzer, Ichimura, Sakurai, Iwasaki, & Yokoo,2009). Indeed, practice, functions often display significant structure,several methods developed represent concisely (e.g., set rulescompute function terms skills possessed agents types determining possible contribution coalition). Thus, marginal contribution nets,MC-nets (Ieong & Shoham, 2005), CSG problem formulated mixed integerprogram (MIP) (Ohta et al., 2009), solved reasonably well comparedIP algorithm, make use compact representations. However, generalproblem stays NP-hard, also shown compact representationssynergy coalition groups (Conitzer & Sandholm, 2006) skill games (Ohta, Iwasaki,Yokoo, Maruono, Conitzer, & Sandholm, 2006; Bachrach, Meir, Jung, & Kohli, 2010) (forlatter, authors also able define subclass instances problemsolved time polynomial number agents n number skills k).agent-type representation, two dynamic programming algorithms proposed solveCSG problem (Aziz & de Keijzer, 2011; Ueda, Kitaki, Iwasaki, & Yokoo, 2011),run O(n2t ) time, number different types.Another interesting direction look coalition structure generation framework distributed constraint optimisation problems (DCOPs) recently becomepopular approach modeling cooperative agents (Modi, 2003). Thus, Ueda, Iwasaki,Yokoo, Silaghi Matsui (2010) consider CSG problem multi-agent system represented one big DCOP, every coalitions value computed optimal solutionDCOP among agents coalition. Instead solving O(2n ) DCOPs,authors suggest modifying big DCOP solving using existing algorithms, e.g.,ADOPT (Modi, 2003) DPOP (Petcu & Faltings, 2005).hand, Rahwan, Michalak, Elkind, Faliszewski, Sroka, Wooldridge Jennings (2011) proposed constrained coalition formation (CCF) framework,constraints coalition structures formed. particular, CCF problem given set agents, set feasible coalition structures characteristicfunction assigning values coalitions appear feasible coalition structures.188fiCoalition Structure Generation GraphsAlthough general case, notion feasibility defined coalition structures,many settings interest constraints implied coalition structures reducedconstraints individual coalitionssuch settings termed locally constrained. represent constraints succinctly, authors propose use propositional logic.define natural subclass locally constrained CCF problems developalgorithm solve CSG problem based divide-and-conquer techniques.Finally, couple recent papers considered problem coalition structure generation combinatorial structuresi.e., graphs. Thus, Aziz de Keijzer (2011) showedpolynomial time bounds coalition structure generation contexts spanning treegames, edge path coalitional games vertex path coalitional games, valuecoalition nodes either 1 0, depending whether contains spanningtree, edge path vertex path, respectively. authors also prove NP-hardnessGCSG problem general graphs edge sum valuation function. paper, present stronger result showing hardness problem planar graphs.Independently, Bachrach, Kohli, Kolmogorov Zadimoghaddam (2011) showedcoalition structure generation problem intractable planar graphs edge sumvaluation function, also provided algorithms constant factor approximationsplanar, minorfree bounded degree graphs. However, aforementioned papers,like classic literature clustering, problem considered particular context(i.e., associated specific valuation function). contrast, results presentedapply general class valuation functions, characterised single assumptionindependence disconnected members.7. Conclusionskey organisational form multi-agent systems involves members coalitioncoordinating actions achieve common goals. agents organised effectively,cooperation significantly improve performance individual systemwhole, especially cases single agents insufficient skills resources complete given tasks own. reason, generating good coalitional structuresone fundamental problems studied AI.However, many real-life scenarios, certain subsets agents able cooperateapply joint actions. Indeed, act collectively, group agents 1) find (most)beneficial plan action, 2) agree it, 3) coordinate actions among membersgroup. Now, may achievable arbitrary subset agentsconnected related other. Therefore, study coalition formationtaking consideration social (or, communication) structure set participants,besides natural interesting research direction, may provide key manypositive results terms problem tractability, well quality stabilitysolutions. Moreover, approach obviously much appealing practicalperspective considering agents interacting vacuum.end, paper studies problem coalition structure generation graphs(GCSG) provides foundation analysis computational complexity.work stands existing literature graph partitioning (or, clustering)focus specific coalition valuation function, rather looks general189fiVoice, Polukarov, & Jenningsclass functions characterised single assumption independence disconnectedmembers (IDM).results show certain important cases indeed valuable identifyvaluation function satisfies IDM property, significantly reduces complexityGCSG problem one faces. particular,Algorithm 1 uses simple search procedureguaranteed bound n2 e+ncomputationalsteps general graphs nnnodes e edges. Hence, whenever graph sparse bound gets lower3n number steps required solve coalition structure generation problemunstructured set elementsutilising graph structure beneficial.graph n nodes known tree decomposition width w, Algorithm 2 requiresO(ww+O(1) n) computational steps, implying problem solved linear timebounded treewidth graphs! addition, coupling Algorithm 2 existing separatortheorems minorfree planar graphs, provides improved computational boundscoalition structure generation important graph classes, although, showTheorem 2, problem remains NPcomplete even planar graphs simple edge sumvaluation functions.work suggests several directions future research topic. First, althoughtheoretical bounds give complexity problem minorfree planar graphsclose best possible, tight. Closing gap would complete results.Second, perhaps main direction study, exploring approximabilityGCSG problem interesting graph classes, developing approximationschemes applicable. line, partial results provided Bachrach, Kohli,Kolmogorov Zadimoghaddam (2011) give algorithms constant factor approximations planar, minorfree bounded degree graphs endowed edge sumvaluation function. challenging task see results extendgeneral class IDM functions. Finally, would interesting incorporate ideascompact representation (Ohta et al., 2009) constrained coalition formation (Rahwanet al., 2011) graph coalition structure generation.AppendixLemma 1 Given graph G = (N, E) coalition valuation function v() IDMproperty, A, B N edges G \ B B \ A,v(A) v(A B) = v(A B) v(B).(3)Proof : B \ = B = B B = A, result holds. Now, let usshow holds kB \ Ak = 1. Suppose otherwise, let BkA \ Bk minimal B kB \ Ak = 1 (3) violated. cannot\ B = , otherwise B = B = B, would imply (3) holds. Letx element \ B. Then, IDM property,v(A) v(A \ {x}) = v(A B) v((A B) \ {x}),190(4)fiCoalition Structure Generation Graphsbut, choice B, set \ {x} must satisfy (3), since x B,v(A \ {x}) v(A B) = v((A B) \ {x}) v(B).(5)Adding (4) (5) gives us v(A) v(A B) = v(A B) v(B), contradiction.show result holds general. Suppose otherwise, letB kB \ Ak minimal B (3) violated. Let xelement B \ let A0 = (B \ {x}). Now, A0 B = B \ {x} A0 B = B.Furthermore, B \ A0 = {x}, applying results proven far pair A0 , B,getv(A0 ) v(A0 B) = v(A0 B) v(B),meaningv(A (B \ {x})) v(B \ {x}) = v(A B) v(B).Furthermore, choice B, since x A,v(A) v(A B) = v(A (B \ {x})) v(B \ {x}).two relations prove result holds B, contradiction.completes proof.2Lemma 2 Let G = (N, E) graph tree decomposition (X, ), X ={X1 , . . . , Xm } n = |N | tree X. Suppose Xinumbered order shortest distance X1 , X1 may chosen arbitrarily.Then, C N ,v(C) =Xv(C Xi ) v C Xii=1[Xj .(6)j<iProof : Towards contradiction, let us suppose result hold G.Let (X, ) tree decomposition minimal = kXk (6) violated.= 1, X = {N } equation (6) becomesv(C) = v(C N ) v(C N ),trivially true. must > 1. choice numbering, Xm mustleaf node . Let k Xk node Xm connected . SinceXm \ Xk disjoint Xi 6= m, edges G elementsXm \ XkXk \ Xm . Furthermore, < 6= k, Xm Xi Xm Xk ,Xm j<m Xj = Xm XkXk[Xj = (Xm Xk )j<k[j<k191Xj .fiVoice, Polukarov, & JenningsThus, C N ,v(C Xm ) v(C Xm[Xj ) + v(C Xk ) v(C Xkj<m[Xj )j<k= v(C Xm ) v(C Xm Xk ) + v(C Xk ) v(C (Xm Xk )[Xj )j<k= v(C (Xm Xk )) v(C (Xm Xk )[Xj ),j<kLemma 1. Furthermore, < k,[[XiXj = Xi XmXj ,j<ij<iso,Xv(C Xi ) v C Xii=1=m1X[Xjj<iv(C Yi ) v C Yii=1[Yj ,j<iYi = Xi 6= k Yk = Xk Xm . However, Yi form tree decompositionG 1 nodes, (with tree topology Xm leaf removed),thus sum must equal v(C) choice m. Since C chosen arbitrarily,leads us contradiction, result must hold general.2Lemma 3 graph G = (N, E), P, Q N , P coalition structureP Q coalition structure Q, P(P Q) = Q(P Q), E = U (P, Q)coalition structure P Q P 0 P , Q0 Q, E(P ) = P(P 0 )E(Q) = Q(Q0 ).Proof : Firstly, P, either (P \ Q) B QB 6= . Thus, union sets E covers P . symmetry, unionsets E must also cover Q.Now, P 0 P ,E(P 0 ) = {(A P 0 ) : P, (P \ Q)} {(A B) P 0 : P, B Q, B 6= }.However, P, B Q B 6= , since P(P Q) = Q(P Q) coalitionstructure P Q, must (P Q) = B (P Q). B Q, B P 0equal (B (P Q)) P 0 P 0 . Thus,E(P 0 ) = {(A P 0 ) : P} = P(P 0 ).symmetry, Q0 Q, E(Q0 ) = Q(Q0 ).192fiCoalition Structure Generation Graphsremains show E coalition structure. Towards contradiction, supposeA, B E B 6= 6= B. Then, since E(P ) = P, Pcoalition structure, must either P = B P P B P disjoint.Likewise, either Q = B Q B Q = . Now, P = B P Q = B Q,= B B P = B Q = , B = , contradictions.Suppose P = B P B Q = . implies P = B P non-empty,B 6= , also implies P Q = B P Q = , means, Pelement P subset P \ Q. However, element E wouldP subset would P itself, meaning = B = P , another contradiction.symmetry, Q = B Q B P = also leads contradiction,therefore scenario impossible. Thus, shown E coalition structure,required.2ReferencesAlon, N., Seymour, P., & Thomas, R. (1990). separator theorem graphsexcluded minor applications. Proceedings 22nd ACM SymposiumTheory Computing, pp. 293299.Aziz, H., & de Keijzer, B. (2011). Complexity coalition structure generation.10th International Joint Conference Autonomous Agents Multi-Agent Systems(AAMAS), pp. 191198.Bachrach, Y., Kohli, P., Kolmogorov, V., & Zadimoghaddam, M. (2011). Optimal coalitionstructures graph games. http://arxiv.org/abs/1108.5248.Bachrach, Y., Meir, R., Jung, K., & Kohli, P. (2010). Coalitional structure generationskill games. 24th AAAI Conference Artificial Intelligence (AAAI), pp.703708.Balas, E., & Padberg, M. W. (1976). Set partitioning: survey. SIAM Rev., 18 (4), 710760.Bansal, N., Blum, A., & Chawla, S. (2003). Correlation clustering. Machine LearningJournal, 56 (1-3), 89113.Bern, M. W., Lawlerand, E. L., & Wong, A. L. (1987). Linear-time computation optimalsubgraphs decomposable graphs. Journal Algorithms, 8 (2), 216235.Bodlaender, H. L. (1998). partial k-arboretum graphs bounded treewidth. Theoretical Computer Science, 209 (1-2), 145.Brandes, U., Delling, D., Gaertler, M., Gorke, R., Hoefer, M., Nikoloski, Z., & Wagner,D. (2008). modularity clustering. IEEE Transactions Knowledge DataEngineering, 20 (2), 172188.Conitzer, V., & Sandholm, T. (2006). Complexity constructing solutions corebased synergies among coalitions. Artificial Intelligence, 170 (6), 607619.Dang, V. D., Dash, R. K., Rogers, A., & Jennings, N. R. (2006). Overlapping coalitionformation efficient data fusion multi-sensor networks. AAAI-06, pp. 635640.193fiVoice, Polukarov, & JenningsDang, V. D., & Jennings, N. R. (2004). Generating coalition structures finite boundoptimal guarantees. 3rd International Joint Conference AutonomousAgents Multi-Agent Systems (AAMAS), pp. 564571.Demange, G. (2004). group stability heirarchies networks. Journal PoliticalEconomy, 112 (4), 754778.Deng, X., & Papadimitriou., C. (1994). complexity cooperative solution concepts.Mathematics Operations Research, 19 (2), 257266.Feo, T. A., & Resende, M. G. (1995). Greedy randomized adaptive search precedures.Journal Global Optimization, 6, 109133.Hopcroft, J., & Tarjan, R. (1973). Efficient algorithms graph manipulation. Communications ACM, 16 (6), 372378.Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: compact representationscheme coalitional games. Proceedings 6th ACM Conference ElectronicCommerce (ACM EC), pp. 193202.Keinanen, H. (2009). Simulated annealing multi-agent coalition formation. 3rdKES International Symposium Agent Multi-Agent Systems (KES-AMSTA),pp. 3039.Klusch, M., & Shehory, O. (1996). polynomial kernel-oriented coalition formation algorithm rational information agents. 2nd International Conference MultiAgent Systems (ICMAS), pp. 157164.Kuratowski, K. (1930). Sur le probleme des courbes gauches en topologie. FundamentaMathematicae, 15, 271283.Lipton, R. J., & Tarjan, R. E. (1979). separator theorem planar graphs. JournalApplied Mathematics, 36 (2), 177189.Mauro, N. D., Basile, T. M. A., Ferilli, S., & Esposito, F. (2010). Coalition structuregeneration grasp. 14th International Conference Artificial Intelligence:Methodology, Systems, Applications (AIMSA), pp. 111120.Modi, P. J. (2003). Distributed constraint optimization multiagent systems. Ph.D. thesis,University Southern California, Los Angeles, CA, USA.Norman, T. J., Preece, A. D., Chalmers, S., Jennings, N. R., Luck, M., Dang, V. D., Nguyen,T. D., Deora, J. S. V., Gray, W. A., & Fiddian, N. J. (2004). Agent-based formationvirtual organisations. International Journal Knowledge Based Systems, 17 (2-4),103111.Ohta, N., Conitzer, V., Ichimura, R., Sakurai, Y., Iwasaki, A., & Yokoo, M. (2009). Coalition structure generation utilizing compact characteristic funciton representations.Proceedings 15th International Joint Conference Principles PracticeConstraint Programming, pp. 623638.Ohta, N., Iwasaki, A., Yokoo, M., Maruono, K., Conitzer, V., & Sandholm, T. (2006). compact representation scheme coalitional games open anonymous environments.21st National Conference Artificial Intelligence (AAAI), pp. 697702.194fiCoalition Structure Generation GraphsPetcu, A., & Faltings, B. (2005). scalable method multiagent constraint optimization.19th International Joint Conference Artificial Intelligence (IJCAI), pp. 266271.Rahwan, T., & Jennings, N. R. (2007). algorithm distributing coalitional valuescalculations among cooperative agents. Artificial Intelligence, 171 (8-9), 535567.Rahwan, T., & Jennings, N. R. (2008a). Coalition structure generation: Dynamic programming meets anytime optimisation. 23rd AAAI Conference ArtificialIntelligence (AAAI), pp. 156161.Rahwan, T., & Jennings, N. R. (2008b). improved dynamic programming algorithmcoalition structure generation. Proceedings 7th International ConferenceAutonomous Agents Multi-Agent Systems, pp. 14171420.Rahwan, T., Michalak, T. P., Elkind, E., Faliszewski, P., Sroka, J., Wooldridge, M., &Jennings, N. R. (2011). Constrained coalition formation. 25th AAAI ConferenceArtificial Intelligence (AAAI), pp. 719725.Rahwan, T., Ramchurn, S. D., Giovannucci, A., & Jennings, N. R. (2009). anytimealgorithm optimal coalition structure generation. Journal Artificial IntelligenceResearch (JAIR), 34, 521567.Robertson, N., & Seymour, P. (1983). Graph minors. i. excluding forest. JournalCombinatorial Theory, Series B 35 (1), 3961.Robertson, N., & Seymour, P. (1995). Graph minors. xiii. disjoint paths problem.Journal Combinatorial Theory, Series B 63 (1), 65110.Robertson, N., & Seymour, P. (2004). Graph minors. xx. wagners conjecture. JournalCombinatorial Theory, Series B 92 (2), 325357.Rothkopf, M. H., Pekec, A., & Harstad, R. M. (1998). Computationally manageable combinatorial auctions. Management Science, 44 (8), 11311147.Sandholm, T., Larson, K., Andersson, M., Shehory, O., & Tohme, F. (1999). Coalitionstructure generation worst case guarantees. Artificial Intelligence, 111 (1-2),209238.Sandholm, T., & Lesser, V. R. (1997). Coalitions among computationally bounded agents.Artificial Intelligence, 94 (1-2), 99137.Sen, S., & Dutta, P. (2000). Searching optimal coalition structures. 6th International Conference Multi-Agent Systems (ICMAS), pp. 286292.Shehory, O., & Kraus, S. (1998). Methods task allocation via agent coalition formation.Artificial Intelligence, 101 (1-2), 165200.Stanica, P. (2001). Good lower upper bounds binomial coefficients. JournalInequalities Pure Applied Mathematics, 2 (3), Art. 30.Tsvetovat, M., Sycara, K. P., Chen, Y., & Ying, J. (2001). Customer coalitionselectronic market place. AA-01, pp. 263264.Ueda, S., Iwasaki, A., Yokoo, M., Silaghi, M. C., & Matsui, T. (2010). Coalition structuregeneration based distributed constraint optimization. 24th AAAI ConferenceArtificial Intelligence (AAAI), pp. 197203.195fiVoice, Polukarov, & JenningsUeda, S., Kitaki, M., Iwasaki, A., & Yokoo, M. (2011). Concise characteristic functionrepresentations coalitional games based agent types. 22nd InternationalJoint Conference Artificial Intelligence (IJCAI), pp. 393399.Wagner, K. (1937). Uber eine eigenschaft der ebenen komplexe. Mathematische Annalen,114 (1), 570590.Xin, X. (2011). FPT algorithm correlation clustering problem. Key EngineeringMaterials, Advanced Materials Computer Science(474-476), 924927.Yeh, D. Y. (1986). dynamic programming approach complete set partitioningproblem. BIT Numerical Mathematics, 26 (4), 467474.Yong, G., Li, Y., Weiming, Z., Jichang, S., & Changying, W. (2003). Methods resourceallocation via agent coalition formation grid computing systems. Proceedings IEEE International Conference Robotics, Intelligent Systems SignalProcessing, Vol. 1, pp. 295300.196fiJournal Artificial Intelligence Research 45 (2012) 443-480Submitted 3/12; published 11/12New Look BDDs Pseudo-Boolean ConstraintsIgnasi AboRobert NieuwenhuisAlbert OliverasEnric Rodrguez-Carbonelliabio@lsi.upc.eduroberto@lsi.upc.eduoliveras@lsi.upc.eduerodri@lsi.upc.eduTechnical University Catalonia (UPC), Barcelona.Valentin Mayer-Eichbergermayereichberger@gmail.comAbstractPseudo-Boolean constraints omnipresent practical applications, thus significant effort devoted development good SAT encoding techniquesthem. encodings first construct Binary Decision Diagram (BDD)constraint, encode BDD propositional formula. BDD-based approaches important advantages, dependent sizecoefficients, able share BDD representing many constraints.first focus size resulting BDDs, considered open problemresearch community. report previous work provedPseudo-Boolean constraints polynomial BDD exists. also give alternative simpler proof assuming NP different Co-NP. interestingly,also show overcome possible exponential blowup BDDs coefficient decomposition. allows us give first polynomial generalized arc-consistentROBDD-based encoding Pseudo-Boolean constraints. Finally, focus practicalissues: show efficiently construct ROBDDs, encodeSAT 2 clauses per node, present experimental results confirmapproach competitive encodings state-of-the-art Pseudo-Boolean solvers.1. Introductionpaperwe study Pseudo-Boolean constraints (PB constraints short), is, constraints form a1 x1 + + xn # K, ai K integer coefficients,xi Boolean (0/1) variables, relation operator # belongs {<, >, , , =}.assume # ai K positive since cases easilyreduced one (see Een & Sorensson, 2006).constraint ( positive coefficients) Boolean function C : {0, 1}n {0, 1}monotonic decreasing sense solution C remains solutionflipping inputs 1 0. Therefore constraints expressed setclauses negative literals. example, clause could simply define (minimal)subset variables cannot simultaneously true. Note however everymonotonic function PB constraint. example, function expressed twoclauses x1 x2 x3 x4 (single) equivalent PB constraint a1 x1 + + a4 x4 K(since without loss generality a1 a2 a3 a4 , also x1 x3 needed).Hence, even among monotonic Boolean functions, PB constraints rather restrictedclass (see also Smaus, 2007).c2012AI Access Foundation. rights reserved.fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-EichbergerPB constraints omnipresent practical SAT applications, typical 0-1linear integer problems, also ingredient new SAT approaches to, e.g., cumulative scheduling (Schutt, Feydy, Stuckey, & Wallace, 2009), logic synthesis (Aloul, Ramani,Markov, & Sakallah, 2002) verification (Bryant, Lahiri, & Seshia, 2002),surprising significant number SAT encodings constraints proposed literature. interested encoding PB constraint C clauseset (possibly auxiliary variables) equisatisfiable, also generalizedarc-consistent (GAC): given partial assignment A, xi false every extensionsatisfying C, unit propagating sets xi false.knowledge, polynomial GAC encoding far given Bailleux,Boufkhad, Roussel (2009). existing encodings based building (formsof) Binary Decision Diagrams (BDDs) translating CNF. Although approach Bailleux et al. BDD-based, main motivation revisit BDD-basedencodings following:Example 1. Let us consider two Pseudo-Boolean constraints: 3x1 + 2x2 + 4x3 530001x1 + 19999x2 + 39998x3 50007. clearly equivalent: Boolean functionrepresent expressed, e.g., clauses x1 x3 x2 x3 . However, encodingslike one Bailleux et al. (2009) heavily depend concrete coefficientsconstraint, generate significantly larger SAT encoding second one. Since,given variable ordering, ROBDDs canonical representation Boolean functions(Bryant, 1986), i.e., Boolean function unique ROBDD, ROBDD-based encodingtreat constraints equivalently.Another reason revisiting BDDs practical problems numerous PB constraints exist share variables among other. Representing singleROBDD potential generating much compact SAT encoding moreover likely better propagation properties.mentioned, BDD-based approaches already studied literature. good example work Een Sorensson (2006), GAC encodingusing six three-literals clauses per BDD node given. However, comes studyBDD size, page 9 cite work Bailleux, Boufkhad, Roussel (2006)say proven general PB-constraint generate exponentially sizedBDD. Section 7 explain approach Bailleux et al use ROBDDs,prove example use show exponentiality method turnspolynomial ROBDDs. Somewhat surprisingly, probably due different namesPB constraints receive (0-1 integer linear constraints, linear threshold functions, weightconstraints, knapsack constraints), work Hosaka, Takenaga, Yajima (1994)remained unknown research community. paper, provedPB constraints polynomial-sized ROBDDs exist. self-containednessarticle, bring interesting result knowledge research community,include family PB constraints prove that, regardless variable ordering,corresponding ROBDD always exponential size.444fiA New Look BDDs Pseudo-Boolean ConstraintsMain contributions organization paper:Subsection 3.2: reproduce family PB constraints proposed Hosaka et al.(1994), polynomial-size ROBDD exist. self-containedness, giveclearer alternative proof original paper.Subsection 3.3: simple proof that, unless NP=co-NP, PB constraintsadmit polynomial-size ROBDD, independently variable order.Subsection 4.1: proof PB constraints whose coefficients powers twoadmit polynomial-size ROBDDs.Subsections 4.2 4.3: GAC polynomial (size O(n3 log amax )) ROBDD-basedencoding PB constraints.Section 5: algorithm construct ROBDDs Pseudo-Boolean constraintspolynomial time w.r.t. size final ROBDD.Section 6: GAC SAT encoding BDDs monotonic functions, generalclass Boolean functions PB constraints. encoding uses one binaryone ternary clause per node (the standard if-then-else encoding BDDs usedin, e.g., Een & Sorensson, 2006, requires six ternary clauses per node). Moreover,translation works BDD variable ordering.Section 7: related work section, summarizing important ingredientsexisting encodings Pseudo-Boolean constraints SAT.Section 8: experimental evaluation comparing approach encodingstools.article extends shorter preliminary paper BDDs Pseudo-Boolean Constraints Revisited (Abo, Nieuwenhuis, Oliveras, & Rodrguez-Carbonell, 2011),presented SAT 2011 conference. Extensions include: (i) proofs technicalresults, (ii) multiple examples illustrating various concepts algorithms presented,(iii) PB constraint family Hosaka et al. (1994) polynomial ROBDD exists, (iv) algorithm efficiently construct ROBDDs Pseudo-Boolean constraints, (v)detailed related work section, (vi) extensive experimental results comparing encodingapproaches (vii) brief report experience trying take advantagesharing potential BDDs.2. PreliminariesLet X = {x1 , x2 , . . .} fixed set propositional variables. x X x xpositive negative literals, respectively. negation literal l, written l, denotes xl x, x l x. clause disjunction literals x1 . . . xp xp+1 . . . xn ,sometimes written x1 . . . xp xp+1 . . . xn . CNF formula conjunction clauses.(partial) assignment set literals {x, x} x, i.e.,contradictory literals appear. literal l true l A, false l A,undefined otherwise. Sometimes write set pairs x = v, v445fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichbergerx1x1100x20x30x21x31011 0x31x21000x201x310x1x3011 0x3x2101100x210x3x2110x3010x11011x3100110Figure 1: Construction BDD 2x1 + 3x2 + 5x3 61 x true 0 x false A. clause C true least oneliterals true A. formula F true clauses true A.case, model F . Systems decide whether formula F model calledSAT-solvers, main inference rule implement unit propagation: given CNFF assignment A, find clause F literals false exceptone, say l, undefined, add l repeat process reaching fixpoint.Pseudo-Boolean constraints (PB constraints short) constraints form a1 x1 ++ xn # K, ai K integer coefficients, xi Boolean (0/1)variables, relation operator # belongs {<, >, , , =}. assume #ai K positive, since cases easily reduced one 1 :(i) changing straightforward coefficients negative; (ii) replacing axa(1 x) a; (iii) replacing (1 x) x. Negated variables like x handled positiveones or, alternatively, replaced fresh x0 adding clauses x x0 x x0 .particular case Pseudo-Boolean constraints one cardinality constraints,coefficients ai equal 1.main goal find CNF encodings PB constraints. is, given PBconstraint C, construct equisatisfiable clause set (a CNF) modelrestricted variables C model C viceversa. Two extra propertiessought: (i) consistency checking unit propagation simply consistency: wheneverpartial assignment cannot extended model C, unit propagationproduces contradiction (a literal l negation l); (ii) generalized arc-consistencyGAC (again unit propagation): given assignment extendedmodel C, {x} cannot, unit propagation produces x.concretely, use ROBDDs finding encodings. ROBDDs introducedmeans following example.Example 2. Figure 1 explains (one method for) construction ROBDD PBconstraint 2x1 + 3x2 + 5x3 6 ordering [x1 , x2 , x3 ]. root node selectorvariable x1 . false child represents PB constraint assuming x1 = 0 (i.e., 3x2 +5x3 6)true child represents 2+3x2 +5x3 6, is, 3x2 +5x3 4. two childrennext variable ordering (x2 ) selector, process repeated reach1. =-constraint split -constraint -constraint. consider (generalizedarc-)consistency latter two isolatedly, original =-constraint.446fiA New Look BDDs Pseudo-Boolean Constraintslast variable sequence. Then, constraint form 0 K True node(1 figure) K 0 positive, False node (0) K < 0. construction(leftmost figure), known Ordered BDD. obtaining Reduced OrderedBDD (ROBDD short rest paper), two reductions applied fixpoint:removing nodes identical children (as done leftmost x3 node second BDDfigure), merging isomorphic subtrees, done x3 third BDD. fourthfinal BDD fixpoint. given ordering, ROBDDs canonical representationBoolean functions: Boolean function unique ROBDD. BDDs encodedCNF introducing auxiliary variable every node. selector variablenode x auxiliary variables false true child f t, respectively,add if-then-else clauses:x fx fxxfffollows, size BDD number nodes. say BDD represents PB constraint represent Boolean function. Given assignmentvariables BDD, define path induced path startsroot BDD step, moves false (true) child nodeselector variable false (true) A.3. Exponential ROBDDs PB Constraintssection study size ROBDDs PB constraints. start definingnotion interval PB constraint. Then, Section 3.2 consider two families PBconstraints study ROBDD size: first prove example given Bailleuxet al. (2006) polynomial ROBDDs, reproduce example Hosaka et al.(1994) exponential ROBDDs regardless variable ordering. Finally, relateROBDD size PB constraint well-known subset sum problem.3.1 Intervalsformally defining notion interval PB constraint, let us first giveintuitive explanation.Example 3. Consider constraint 2x1 + 3x2 + 5x3 6. Since combinationcoefficients adds 6, constraint equivalent 2x1 + 3x2 + 5x3 < 6, hence2x1 + 3x2 + 5x3 5. process cannot repeated since 5 obtainedexisting coefficients.Similarly, could try increase right-hand side constraint. However,combination coefficients adds 7, implies constraintequivalent 2x1 + 3x2 + 5x3 7. all, state constraint equivalent2x1 + 3x2 + 5x3 K K [5, 6]. trivial see set valid Ksalways interval.Definition 4. Let C constraint form a1 x1 + + xn K. interval Cconsists integers a1 x1 + + xn , seen Boolean function,equivalent C.447fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-EichbergerSimilarly, given ROBDD representing PB constraint node selectorvariable xi ,we refer interval integers constraintai xi + xn represented (as Boolean function) ROBDD rooted .following, unless stated otherwise, ordering used ROBDD[x1 , x2 , . . . , xn ].Proposition 5. [, ] interval node selector variable xi then:1. assignment {xj = vj }nj=i ai vi + + vn = .2. assignment {xj = vj }nj=i ai vi + + vn = + 1.3. assignment {xj = vj }i1j=1 K a1 v1 a2 v2 ai1 vi1 [, ]4. Take h < . exists assignment {xj = vj }nj=i ai vi + + vn > hpath goes True.5. Take h > . exists assignment {xj = vj }nj=i ai vi + + vn hpath goes False.6. interval True node [0, ).7. interval False node (, 1]. Moreover, intervalnegative values.Proof.1. Since 1 belong interval , constraintsai xi + ai+1 xi+1 + + xn 1ai xi + ai+1 xi+1 + + xndifferent. means partial assignment satisfying second onefirst one.2. proof analogous previous one.3. Take partial assignment {x1 = v1 , . . . , xi1 = vi1 } whose path goes root. Therefore, definition ROBDD, ROBDD constraintai xi + ai+1 xi+1 + + xn K a1 v1 ai1 vi1 .Therefore, definition interval ,K a1 v1 a2 v2 ai1 vi1 [, ].4. Intuitively, property states that, h interval ,assignment satisfies ROBDD rooted constraint ai xi + +xn h.Since h belong interval , ROBDDC 0 : ai xi + + xn h. Therefore, exists assignment either448fiA New Look BDDs Pseudo-Boolean Constraints[5, 6][5, 6]x1x10[5, 7]01x20[0, )100x3 [0, 4]0x2[5, 7]1[5,) x31(, 1]1[0, )x21[3, 4]10x3 [0, 4]100110(, 1]Figure 2: Intervals ROBDD 2x1 + 3x2 + 5x3 6(i) goes False satisfies C 0 ;(ii) goes True satisfy C 0 .want prove assignment satisfies (ii). Assume satisfies (i). Sincegoes False belongs interval , holdsai vi + + vn > .Since > h, assignment satisfy C 0 , contradiction. Therefore,assignment satisfies (ii).5. Take assignment second point proposition. Since + 1belong interval, path assignment goes False. Moreover,ai vi + + vn = + 1 h.6. True node ROBDD tautology. Therefore, represents PBconstraint 0 h h [0, ).7. False node ROBDD contradiction. Therefore, represents PBconstraint 0 h h (, 1]. Moreover, ai xi + + xn < 0 alsocontradiction, hence constraint also represented False node. Therefore,node interval negative values.prove that, given ROBDD PB constraint, one easily computeintervals every node bottom-up. first start motivating example.Example 6. Let us consider constraint 2x1 + 3x2 + 5x3 6. Assumevariables appear every path root leaves (otherwise, add extra nodesrightmost BDD Figure 2). Assume computed intervals twochildren root (rightmost BDD Figure 2). means false child rootBDD 3x2 +5x3 [5, 7] true child BDD 3x2 +5x3 [3, 4]. Assumingx1 false, false child would also represent constraint 2x1 +3x2 +5x3 [5, 7],assuming x1 true, true child would represent constraint 2x1 +3x2 +5x3 [5, 6].Taking intersection two intervals, infer root node represents2x1 + 3x2 + 5x3 [5, 6].449fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichbergerformally, interval every node computed follows:Proposition 7. Let a1 x1 + a2 x2 + + xn K constraint, let B ROBDDorder [x1 , . . . , xn ]. Consider node selector variable xi , false child f (withselector variable xf interval [f , f ]) true child (with selector variable xtinterval [t , ]), shown Figure 3. interval [, ], with:= max{f + ai+1 + + af 1 , + ai + ai+1 + + at1 },= min{f , + ai }....xi [, ]0xf[f , f ]1xt[t , ]Figure 3: interval node computed childrens intervals.moving proof, want note every path rootleaves ROBDD variables present, definition would much simpler( = max{f , + ai }). coefficients necessary account variablesremoved due ROBDD reduction process.Proof. Let us assume [, ] interval . One following statementshold:1. exists h [, ] belong interval .2. exists h < belonging interval .3. exists h > belonging interval .prove none cases hold.1. Let us defineC 0 : ai xi + + xn h.h belong interval, exists assignment {xj = vj }nj=ieither satisfies C 0 path goes False satisfy C 0path goes True. Assume assignment satisfies C 0 path goesFalse (the case similar). two possibilities:assignment satisfies vi = 0. Since h , holdsh ai+1 vi+1 af 1 vf 1 ai+1 vi+1 af 1 vf 1ai+1 af 1 f .450fiA New Look BDDs Pseudo-Boolean Constraintshand, since h ,h ai+1 vi+1 af 1 vf 1 h f .Therefore, h ai+1 vi+1 af 1 vf 1 belongs interval f . Sinceassignment {xf = vf , . . . , xn = vn } goes f False, have:af vf + + vn > h ai+1 vi+1 af 1 vf 1ai+1 vi+1 + + af vf + vn > hHence, adding ai vi sum one see assignment satisfyC 0 , contradiction.case vi = 1 gives similar contradiction.2. definition , either h < f + ai+1 + + af 1 h < + ai + ai+1 + +at1 . consider first case, since one similar. Therefore,h ai+1 af 1 < f . Due point 4 Proposition 5, exists assignment{xf = vf , . . . xn = vn }af vf + vn > h ai+1 af 1path goes f True. Hence, assignment{xi = 0, xi+1 = 1, . . . , xf 1 = 1, xf = vf , . . . , xn = vn }satisfy constraint ai xi + + xn h path goes True.definition interval, h cannot belong interval .3. case similar previous one.proposition gives natural way computing intervals ROBDD bottomup fashion. procedure initialized computing intervals terminal nodesdetailed Proposition 5, points 6 7.Example 8. Let us consider constraint 2x1 + 3x2 + 5x3 6. ROBDDshown left-hand side Figure 2, together intervals. computation,first compute intervals True False nodes, [0, ) (, 1]virtue Proposition 5. Then, compute interval node x3selector variable previous propositions formula: 3 = max{0, + 5} = 0, 3 =min{, 1 + 5} = 4. Therefore, interval [0, 4].next step, compute interval node selector variable x2 : 2 =max{0 + 5, 0 + 3} = 5, 2 = min{, 4 + 3} = 7. Thus, interval [5, 7]. Finally,compute roots interval: 1 = max{5, 0 + 2 + 3} = 5, 1 = min{7, 4 + 2} = 6,is, [5, 6].451fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger3.2 Families PB Constraints ROBDD Sizestart revisiting family PB constraints given Bailleux et al. (2006),proved that, concrete variable ordering, non-reduced BDDs grow exponentially family. prove ROBDDs polynomial family, even independent variable ordering. family definedPconsidering a, b n positive integers ni=1 bi < a. coefficients= + bi right-hand side constraint K = n/2. first proveconstraint C : 1 x1 + + n xn K equivalent cardinality constraintC 0 : x1 + + xn n/2 1. simplicity, assume n even.Take assignment satisfying C 0 . case, n/2 1 variables xiassigned true, assignment also satisfies C since:1 x1 + + n xnnXi=n/2+2= (n/2 1)a +nXi=n/2+2bi < K +nXbi < K.i=1Consider assignment satisfying C 0 . case, least n/2true variables assignment satisfy C either:1 x1 + + n xnn/2Xi=1= (n/2) +n/2Xi=1bi > (n/2) = K.Since two constraints equivalent ROBDDs canonical, ROBDD representation C C 0 same. ROBDD C 0 known quadraticsize cardinality constraint (see, instance, Bailleux et al., 2006).following, present family PB constraints admit exponentialROBDDs. example first given Hosaka et al. (1994), clearer alternativeproof given next. First all, prove lemma that, certain technical conditions,gives lower bound number nodes ROBDD PB constraint.Lemma 9. Let a1 x1 + + xn K PB constraint, let integer1 n. Assume every assignment {x1 = v1 , x2 = v2 , . . . , xi = vi } admitsextension {x1 = v1 , . . . , xn = vn } a1 v1 + + vn = K. Let numberdifferent results obtain adding subset coefficients a1 , a2 , . . . , ai , i.e.,= |{Xj=1aj bj : bj {0, 1}}|. Then, ROBDD size ordering [x1 , x2 , . . . , xn ]least .Proof. Let us consider PB constraint satisfies conditions lemma.prove ROBDD least distinct nodes showing two assignmentsform {x1 = v1 , . . . , xi = vi } {x1 = v10 , . . . , xi = vi0 } a1 v1 + + ai vi 6=a1 v10 + + ai vi0 lead different nodes ROBDD.Assume true: two assignments {x1 = v1 , . . . , xi = vi }{x1 = v10 , . . . , xi = vi0 } a1 v1 + + ai vi < a1 v10 + + ai vi0 paths endnode. Take extended assignment = {x1 = v1 , . . . , xn = vn }a1 v1 + vn = K. Since satisfies PB constraint, path defines ends452fiA New Look BDDs Pseudo-Boolean Constraintstrue node. However, assignment A0 = {x1 = v10 , . . . , xi = vi0 , xi+1 = vi+1 , . . . , xn = vn }satisfy constraint, sincea1 v10 + ai vi0 + ai+1 vi+1 + vn > a1 v1 + + vn = K.However, nodes defined {x1 = v1 , . . . , xi = vi } {x1 = v10 , . . . , xi = vi0 }same, path defined A0 must also end true node, contradiction.show family PB constraints admits exponential ROBDDs.Theorem 10. Let n positive integer, let us define ai,j = 2j1 + 22n+i11 i, j 2n; K = (24n 1)n. Then, PB constraint2n X2nXi=1 j=1ai,j xi,j Kleast 2n nodes variable ordering.Proof. convenient describe coefficients binary notation:2nz}|2n{0 10 1z0 00 0}|01...10{a1,1 = 0a1,2 = 000a1,2n1 = 0a1,2n = 0000 10 10 11 00000a2,1 = 0a2,2 = 0001 01 00 00 010a2,2n1 = 0a2,2n = 001...001 01 0...0 11 00000a2n,2n = 1 00 01 000K/n = 1 11 11 111First all, one see sum 2K.Let us take arbitrary bijectionF = (F1 , F2 ) : {1, 2, . . . , 4n2 } {1, 2, . . . , 2n} {1, 2, . . . , 2n},consider ordering defined it: [xF (1) , xF (2) , . . . , xF (4n2 ) ], xF (k) = xF1 (k),F2 (k)every k. want prove ROBDD PB constraint orderingleast 2n nodes.proof consist showing hypotheses Lemma 9 hold. is, firstshow variable ordering, find integer assignment453fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichbergerfirst variables extended full assignment adds K. Then, proveleast 2n different values add first coefficients, requiredLemma 9.Let us define bk 1 k 2n position k-th different value tuple(F1 (1), F1 (2), . . . , F1 (4n2 )). formally,bk =1k = 1,nmin r : F1 (r) 6 {F1 (b1 ), F1 (b2 ), . . . , F1 (bk1 )}k > 1.Analogously, let us define c1 , . . . , c2nck =1k = 1,nmin : F2 (s) 6 {F2 (c1 ), F2 (c2 ), . . . , F2 (ck1 )}k > 1.Let us denote ir = F1 (br ) js = F2 (cs ) 1 r, 2n. Notice{i1 , i2 , . . . , i2n } {j1 , j2 , . . . , j2n } permutations {1, 2, . . . , 2n}. Assume bn cn(the case analogous), take arbitrary assignment {xF (1) = vF (1) , xF (2) =vF (2) , . . . , xF (cn ) = vF (cn ) }. want extend complete assignment24nXaF (k) vF (k) = K.k=1Figure 4 represents initial assignment. values top-left square sinceassignment undefined xir ,js r > n > n. Extending assignmentsum K amounts completing table way exactly nones every column row.assignment completed following way: first, complete top leftsquare way, instance, adding zeros every non-defined cell. Then, copysquare bottom-right square and, finally, add complementary squaretwo squares (i.e., write 0 instead 1 1 instead 0). Figure 5 shows extendedassignment example.formally, assignment completed follows:vir ,js =0virn ,jsvir ,jsnvirn ,jsnr, n vir ,js undefined,r > n n,> n r n,r, > n,0 = 1 1 = 0.Now, let us prove satisfies requirements, i.e., coefficients correspondingtrue variables assignment add exactly K. Let us fix r, n. Denote = ir ,j = js , i0 = ir+n j 0 = js+n .vi,j = 0, definition vi0 ,j = vi,j 0 = 1 vi0 ,j 0 = 0. Therefore,ai,j vi,j + ai0 ,j vi0 ,j + ai,j 0 vi,j 0 + ai0 ,j 0 vi0 ,j 0= ai0 ,j + ai,j 000= 22n+i 1 + 2j1 + 22n+i1 + 2j 1ai,j + ai0 ,j + ai,j 0 + ai0 ,j 0=.2454fiA New Look BDDs Pseudo-Boolean Constraintsi1i2j111j20...jn1...in+1in+2...i2n01jn+1jn+2...j2nFigure 4: arbitrary assignment. 0, 1 nothing position (ir , js ) dependingwhether xir ,js false, true unassigned.455fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichbergeri1i2...in+1in+2...i2nj111000011j200001111...01001011jn00101101jn+100111100jn+211110000...10110100j2n11010010Figure 5: Extended assignment. exactly n ones every column row.Analogously, vi,j = 1,ai,j vi,j + ai0 ,j vi0 ,j + ai,j 0 vi,j 0 + ai0 ,j 0 vi0 ,j 0 =ai,j + ai0 ,j + ai,j 0 + ai0 ,j 0.2Therefore,224nXk=1aF (k) vF (k)4n1X= K.=2 k=1 F (k)Lemma 9, number nodes ROBDD least number differentresults obtain adding subset coefficients aF (1) , aF (2) , . . . , aF (cn ) . Consider set aF (c1 ) , aF (c2 ) , . . . , aF (cn ) . see different subsets adddifferent values, hence ROBDD size least 2n .sum subset {aF (c1 ) , aF (c2 ) , . . . , aF (cn ) }= aF (c1 ) v1 + aF (c2 ) v2 + + aF (cn ) vn ,vr {0, 1}.Let us look 2n last bits binary notation: digits 0 exceptpositions F2 (c1 ), F2 (c2 ), . . . , F2 (cn ), v1 , v2 , . . . , vn . Therefore, two subsetsadd same, 2n last digits sum same. means values(v1 , . . . , vn ) same, thus subset.456fiA New Look BDDs Pseudo-Boolean Constraints3.3 Relation Subset Sum Problem ROBDD Sizesection, study relationship ROBDD size PB constraintsubset sum problem. allows us to, assuming NP co-NP different,give much simpler proof exist PB constraints admit polynomialROBDDs.Lemma 9 exponential ROBDD example Theorem 10 suggestrelationship size ROBDDs number ways obtain Kadding coefficients PB. seems K obtained lotdifferent ways, ROBDD large.section explore another relationship problem adding Ksubset coefficients size ROBDDs. sense, give proofconverse last paragraph true: NP co-NP different,exponentially-sized ROBDDs PB constraints subsets coefficients addingK. Let us start defining one version well-known subset sum problem.Definition 11. Given set positive integers = {a1 , . . . , } integer K,subset sum problem (K, S) consists determining whether exists subset{a1 , . . . , } sums exactly K.well-known subset sum problem NP-complete K 2n ,polynomial algorithms n K also polynomial n. given subsetsum problem (K, S) = {a1 , . . . , }, construct associated PB constrainta1 x1 + +an xn K. previous section seen one PB constraint family whosecoefficients add K exponential number ways, thus generating exponentialROBDD. Now, assuming NP co-NP different, see existsPB constraint family exponential ROBDDs ordering coefficientscannot add K. First, show ROBDDs act unsatisfiability certificatessubset sum problem.Theorem 12. Let (K, S) UNSAT subset sum problem. Then, ROBDDassociated PB constraint polynomial size, act polynomial unsatisfiabilitycertificate (K, S).Proof. need show how, polynomial time, check whether ROBDDunsatisfiability certificate (K, S). that, note subset sum problemUNSAT PB constraintsa1 x1 + + xn K,a1 x1 + + xn K 1equivalent, happens ROBDDs same. Therefore,show, polynomial time, given ROBDD represents constraints.done, instance, building ROBDD (using Algorithm 1 Section 5)comparing ROBDDs.key point that, assume NP different co-NP, existsfamily UNSAT subset sum problems polynomial-sized unsatisfiability certificate.Hence, family consisting associated PB constraints admit polynomialROBDDs.457fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-EichbergerHence, PB constraints associated difficult-to-certify UNSAT subset sum problemsproduce exponential ROBDDs. However, subset sum NP-complete K 2n .PB constraints industrial problems usually K nr r, could expectnon-exponential ROBDDs constraints.4. Avoiding Exponential ROBDDssection introduce positive results. restrict particular classPB constraints, coefficients powers two. show below,constraints admit polynomial ROBDDs. Moreover, PB constraint reducedclass means coefficient decomposition.Example 13. Let us take PB constraint 9x1 + 8x2 + 3x3 10. Considering binaryrepresentation coefficients, constraint rewritten (23 x3,1 + 20 x0,1 ) +(23 x3,2 ) + (21 x1,3 + 20 x0,3 ) 10 add binary clauses expressing xi,r = xrappropriate r.4.1 Power-of-two PB Constraints Polynomial-size ROBDDsLet us consider PB constraint form:C : 20 0,1 x0,1 + 20 0,2 x0,2 + + 20 0,n x0,n21 1,1 x1,1 + 21 1,2 x1,2 + + 21 1,n x1,n...2 m,1 xm,1 + 2 m,2 xm,2 + + 2m m,n xm,n+++K,i,r {0, 1} r. Notice every PB constraint whose coefficientspowers 2 expressed way. Let us consider ROBDD representationordering [x0,1 , x0,2 , . . . , x0,n , x1,1 , . . . , xm,n ].Lemma 14. Let [, ] interval node selector variable xi,r . 2i divides0 < (n + r 1) 2i .Proof. Proposition 5.1, expressed sum coefficientsmultiples 2i , hence multiple 2i . Proposition 5.7, nodewhose interval contains negative values False node, hence 0. Now, usingProposition 5.3, must assignment variables {x0,1 , . . . , xi,r1 }20 0,1 x0,1 + + 2i i,r1 xi,r1 belongs interval. Therefore:20 0,1 x0,1 + + 2i i,r1 xi,r1 20 + 20 + + 2i= n20 + n21 + + n2i1 + (r 1) 2i = n(2i 1) + 2i (r 1)< 2i (n + r 1)Corollary 15. number nodes selector variable xi,r bounded n + r 1.particular, size ROBDD belongs O(n2 m).458fiA New Look BDDs Pseudo-Boolean ConstraintsProof. Let 1 , 2 , . . . , nodes selector variable xi,r . Let [j , j ] intervalj . Note intervals pair-wise disjoint since non-empty intersection wouldimply exists constraint represented two different ROBDDs. Henceassume, without loss generality, 1 < 2 < < . Due Lemma 14, knowj j1 2i . Hence 2i (n + r 1) > t1 + 2i 1 + 2i (t 1) 2i (t 1)conclude < n + r.4.2 Consistent Encoding PB ConstraintsLet us take arbitrary PB constraint C : a1 x1 + xn K assumelargest coefficient. = log , rewrite C splitting coefficientspowers two shown Example 13:C :20 0,1 x0,121 1,1 x1,12m m,1 xm,120 0,2 x0,2 + + 20 0,n x0,n21 1,2 x1,2 + + 21 1,n x1,n...+ 2m m,2 xm,2 + + 2m m,n xm,n+++++K,m,r m1,r 0,r binary representation ar . Notice C C representconstraint add clauses expressing xi,r = xr appropriate r.process called coefficient decomposition PB constraint. similar idea givenBartzis Bultan (2003).important remark that, using consistent SAT encoding ROBDD C(e.g. one given Een & Sorensson, 2006, one presented Section 6) addingclauses expressing xi,r = xr appropriate r, obtain consistent encodingoriginal constraint C using O(n2 log ) auxiliary variables clauses.difficult see. Take assignment variables C cannotextended model C. coefficients corresponding variablestrue add K. Using clauses xi,r = xr , unit propagation produceassignment xi,r cannot extended model C. Since encodingC consistent, false clause found. Conversely, consider assignmentvariables C extended model C, assignment clearlyextended model C clauses expressing xi,r = xr . Hence, unit propagationclauses encoding C detect false clause.Example 16. Consider PB constraint C : 2x1 + 3x2 + 5x3 6. obtainingconsistent encoding presented, first rewrite C splitting coefficientspowers two:C 0 : 1x0,2 + 1x0,3 + 2x1,1 + 2x1,2 + 4x2,3 6.Next, encode C 0 ROBDD finally encode ROBDD SAT add clausesenforcing relations xi,j = xj . Or, instead that, replace xi,j xjROBDD, encode result SAT. Figure 6 shows decision diagramreplacement.4.3 Generalized Arc-consistent Encoding PB ConstraintsUnfortunately, previous approach result GAC encoding. intuitiveidea seen following example:459fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichbergerx200x311x1 10x1100x2101x310Figure 6: Decision Diagram 2x1 + 3x2 + 5x3 6 decomposing coefficientspowers two.Example 17. Let us consider constraint 3x1 + 4x2 6. splitting coefficientspowers two, obtain C 0 : x0,1 + 2x1,1 + 4x2,2 6. set x2,2 true, C 0 implieseither x0,1 x1,1 false, encoding cannot exploit factvariables receive truth value hence propagated. Addingclauses stating x0,1 = x1,1 help sense.order overcome limitation, follow method presented Bessiere, Katsirelos, Narodytska, Walsh (2009) Bailleux et al. (2009). Let C : a1 x1 + +an xnK arbitrary PB constraint. denote Ci constraint a1 x1 + + ai 1 + +xn K, i.e., constraint assuming xi true. every 1 n,encode Ci Section 4.2 and, addition, add binary clause ri xi , riroot ROBDD Ci . clause helps us preserve GAC: given assignment{xi } cannot extended model C, literal ri propagatedusing (because encoding Ci consistent). Hence added clause allow uspropagate xi .Example 18. Consider PB constraint C : 2x1 + 3x2 + 5x3 6. Let us defineconstraints C1 : 3x2 + 5x3 4, C2 : 2x1 + 5x3 3 C3 : 2x1 + 3x2 1. Now, encodeconstraints ROBDDs previous section, coefficient decomposition.Figure 7 shows resulting ROBDDs. Finally, need encode SAT consistently, add clauses ri xi , assuming variable associated rootROBDD Ci ri .encoding GAC: take instance assignment = {x1 = 1}. Constraint C3satisfied. Hence, consistency, r3 propagated. Therefore, clause r3 x3 propagatesx3 , wanted. propagation assignments similar.all, suggested encoding GAC uses O(n3 log(aM )) clauses auxiliaryvariables, largest coefficient.460fiA New Look BDDs Pseudo-Boolean Constraintsr1x2r2x3001x30x210001111x21010x301r3x110Figure 7: ROBDDs2 C1 , C2 C3 coefficient decomposition.5. Algorithm Constructing ROBDDs Pseudo-BooleanConstraintsLet us fix Pseudo-Boolean constraint a1 x1 + + xn K variable ordering[x1 , x2 , . . . , xn ]. goal section prove one construct ROBDDconstraint using ordering polynomial time respect ROBDD sizen.algorithm builds standard ROBDDs, used build ROBDDscoefficient decomposition: need first split coefficients and, secondly, applyalgorithm. Forthcoming Example 21 shows detail overall process. similarversion algorithm described Mayer-Eichberger (2008).key point algorithm label node ROBDDinterval. following, every {1, 2, . . . , n + 1}, use set Li consistingpairs ([, ], B), B ROBDD constraint ai xi + + xn K 0every K 0 [, ] (i.e., [, ] interval B). sets kept tupleL = (L1 , L2 , . . . , Ln+1 ).Note definition ROBDDs intervals, ([1 , 1 ], B1 ) ([2 , 2 ], B2 ) belongLi either [1 , 1 ] = [2 , 2 ] [1 , 1 ] [2 , 2 ] = . Moreover, first case holdsB1 = B2 . Therefore, Li represented binary search tree-like datastructure, insertions searches done logarithmic time. functionsearch(K, Li ) searches whether exists pair ([, ], B) Li K [, ].tuple returned exists, otherwise empty interval returned first componentpair. Similarly, also use function insert(([, ], B), Li ) insertions.overall algorithm detailed Algorithm 1 Algorithm 2:Theorem 19. Algorithm 1 runs O(nm log m) time (where size ROBDD)correct following sense:2. Actually, diagram replacing variables xi,j xj ROBDD. However, denoteROBDDs simplicity.461fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-EichbergerAlgorithm 1 Construction ROBDD algorithmRequire: Constraint C : a1 x1 + . . . + xn K 0Ensure: returns B ROBDD C.1: insuch 1 n + 12:Li(, 1], F alse , [ai + ai+1 + + , ), rue3: end4: L (L1 , . . . , Ln+1 ).5: ([, ], B) BDDConstruction(1, a1 x1 + . . . + xn K 0 , L).6: return B.Algorithm 2 Procedure BDDConstructionRequire: integer {1, 2, . . . , n + 1}, constraint C : ai xi + . . . + xn K 0 set LEnsure: returns [, ] interval C B ROBDD1: ([, ], B) search(K 0 , Li ).2: [, ] 6=3:return ([, ], B)4: else5:([F , F ], BF ) := BDDConstruction(i + 1, ai+1 xi+1 + . . . + xn K 0 , L).6:([T , ], BT ) := BDDConstruction(i + 1, ai+1 xi+1 + . . . + xn K 0 ai , L).7:[T , ] = [F , F ]8:B BT9:[, ] [T + ai , ]10:else11:B ite(xi , BT , BF ) // root xi , true branch BT false branch BF .12:[, ] [F , F ] [T + ai , + ai ]13:end14:insert(([, ], B), Li )15:return ([, ], B)16: end462fiA New Look BDDs Pseudo-Boolean Constraints1. K 0 belongs interval returned BDDConstruction(ai xi + +an xn K 0 , L).2. tuple ([, ], B) returned BDDConstruction consist BDD Binterval [, ].3. BDDConstruction returns ([, ], B), BDD B reduced.Proof. Let us first start three correctness statements:1. K 0 found Li line 1 Algorithm 2 statement obviously true. Otherwiselet us reason induction i. base case = n + 1, since Ln+1contains intervals (, 1] [0, ], search call line 1 succeedhence result holds. < n + 1 assume, induction hypothesis,K 0 [F , F ] K 0 ai [T , ]. two intervals coincide result obvious,otherwise also easy see K 0 [F , F ] [T + ai , + ai ].2. Let us prove every moment tuples L correct, i.e., containBDDs correct interval. Since returned value always elementLi , proves statement.Proposition 5.6 5.7, initial tuples L correct. provepreviously inserted intervals correct, current interval also correct.follows virtue Proposition 7.3. Let us prove tuples L contain reduced BDDs. before,initial BDDs L reduced. Let B BDD computed algorithm,children BT BF . induction hypothesis, reduced, B reducedtwo children equal. algorithm creates nodechildrens intervals different. Therefore, BT BF representBoolean constraint, different BDDs.Regarding runtime, since cost search insertion Li logarithmic size,cost algorithm O(log m) times number calls BDDConstruction. Hence,remains show O(nm) calls.Every call (but first one) BDDConstruction done exploringedge ROBDD. Notice edge explored twice, since edges exploredparent node whenever reach explored node recursive callsBDDConstruction. hand, every edge ROBDD make 2k 1calls, k length edge (if nodes joined edge variables xixj say length |i j|). Since ROBDD O(m) edges lengthO(n), number calls O(nm).Let us illustrate algorithm example:Example 20. Take constraint C : 2x1 + 3x2 + 5x3 6, let us apply algorithmobtain ROBDD ordering [x1 , x2 , x3 ]. Figure 8 represents recursive callsBDDConstruction returned parameters (the ROBDD interval).463fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger1. BDDConstruction(1, 2x1 + 3x2 + 5x3 6, L)x1 [5, 6]01x201x301102. BDDConstruction(2, 3x2 + 5x3 6, L)7. BDDConstruction(2, 3x2 + 5x3 4, L)x2 [5, 7]0x3 [2, 4]10x30113. BDDConstruction(3, 5x3 6, L)11004. BDDConstruction(3, 5x3 3, L)x301 [5, )15. BDDConstruction(4, 0 3, L)8. BDDConstruction(3, 5x3 4, L)[0, 4]x301109. BDDConstruction(3, 5x3 1, L)[0, 4]06. BDDConstruction(4, 0 2, L)1 [0, )0 (, 1]Figure 8: Recursive calls BDDConstruction, returned values.464x3 [0, 4]01110fiA New Look BDDs Pseudo-Boolean Constraintscalls number 3, 5, 6, 8 9, search function returns true intervalROBDD returned without computation.call number 7, two recursive calls return interval (and, therefore,ROBDD). Hence, ROBDD returned.call number 1 two recursive calls return two different ROBDDs, addsnode join two ROBDDs another one, returned. happenscalls number 2 4.overall process coefficient decomposition would work following example:Example 21. Let us take constraint C : 2x1 + 3x2 + 5x3 6. want buildROBDD coefficient decomposition using Algorithm 1, proceed follows:1. Split coefficients obtain C 0 : 1y1 + 1y2 + 2y3 + 2y4 + 4y5 6, x1 = y3 ,x2 = y1 = y4 x3 = y2 = y5 .2. Apply algorithm C 0 obtain ROBDD B 0 .3. Replace y1 x2 , y2 x3 , etc. nodes B 0 .6. SAT Encodings BDDs Monotonic Functionssection consider BDD representing monotonic function F wantencode SAT. expected, want encoding small possible GAC.encoding explained valid type BDDs, so, particular, validROBDDs. main differences Minisat+ encoding (Een & Sorensson, 2006)number clauses generated (6 ternary clauses per node versus one binary oneternary clauses per node) encoding GAC variable ordering.usual, encoding introduces auxiliary variable every node. Let nodeselector variable x auxiliary variable n. Let f variable false childvariable true child. two clauses per node needed:f nx n.Furthermore, add unit clause variable True node another onenegation variable False node.Theorem 22. encoding consistent following sense: partial assignmentcannot extended model F r propagated unit propagation,r root BDD.Proof. prove theorem induction number variables BDD.BDD variables, BDD either True node False noderesult trivial.Assume result true BDDs less k variables, let Ffunction whose BDD k variables. Let r root node, x1 selector variable465fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichbergerf, respectively false true children (note abuse notation identifynodes auxiliary variable). denote F1 function F|x1 =1 (i.e., Fsetting x1 true) F0 function F|x1 =0 .Let partial assignment cannot extended model F .Assume x1 A. Since cannot extended, assignment \ {x1 } cannotextended model F1 . definition BDD, function F1BDD. induction hypothesis, propagated, since x1 A, r alsopropagated.Assume x1 6 A. Then, assignment \ {x1 } cannot extended modelF0 . Since F0 f BDD, induction hypothesis f propagated,hence r also is.Let partial assignment, assume r propagated. Then, either falso propagated propagated x1 (note x1propagated appears one clause already true).Assume f propagated. Since f BDD F0 , inductionhypothesis assignment \ {x1 , x1 } cannot extended model F0 .Since function monotonic, neither \ {x1 , x1 } extended modelF . Therefore, cannot extended model F .Assume propagated x1 A. Since BDD F1 ,induction hypothesis \ {x1 } cannot extended model F1 , neitherextended model F .obtaining GAC encoding, add unit clause.Theorem 23. add unit clause forcing variable root node true,previous encoding becomes generalized arc-consistent.Proof. prove induction variables BDD. case zerovariables trivial, let us prove induction case.before, let r root node, x1 selector variable n auxiliary variable,f, false true children. denote F0 F1 functions F|x1 =0 F|x1 =1 .Let partial assignment extended model F . Assume{xi } cannot extended. want prove xi propagated.Let us assume x1 A. case, propagated due clause x1 nunit clause n. Since x1 {xi } cannot extended model F ,\ {x1 } {xi } neither extended assignment satisfying F1 . inductionhypothesis, since BDD function F1 , xi propagated.Let us assume x1 6 xi 6= x1 . Since F monotonic, {xi } cannotextended model F cannot extended model F0 . Noticef propagated thanks clause f n unit clause n. inductionhypothesis, method GAC F0 , xi propagated.466fiA New Look BDDs Pseudo-Boolean ConstraintsFinally, assume x1 6 xi = x1 . Since {x1 } cannot extendedmodel F , cannot extended model F1 . Theorem 22, propagatedand, due x1 n n, also x1 .finish section example illustrating suggested encoding BDDsSAT used different PB encoding methods presented paper.Example 24. Consider constraint C : 2x1 + 3x2 + 5x3 6. encode constraint SAT three methods: usual ROBDD encoding; consistentapproach ROBDDs splitting coefficients, explained Section 4.2;GAC approach ROBDDs splitting coefficients explained Section 4.3.1. BDD-1: method builds ROBDD C encodes SAT. Hencestart building ROBDD C, seen last picture Figure 1.Now, need encode SAT. Let y1 , y2 y3 fresh variables correspondingnodes ROBDD C respectively x1 , x2 x3 selector variable.node y1 , add clauses y2 y1 x1 y3 y1 .y2 , add clauses > y2 x2 y3 y2 , > tautologysymbol.y3 , add clauses > y3 x3 y3 , contradiction symbol.Moreover, add unit clauses >, y1 . all, removingunits tautologies, clauses obtained y1 , y2 , x1 y3 , x2 y3 x3 y3 .2. BDD-2: build ROBDD C coefficient decomposition Example 21.Figure 6 shows resulting ROBDD. introduce variables y1 , y2 , . . . , y6 everynode ROBDD. precisely, first x2 node (starting top-down) receivesvariable y1 , next x2 node gets y5 . first x3 receives y2 one y6 .Finally leftmost x1 node gets variable y3 one y4 . addfollowing clauses: y2 y1 , y4 x2 y1 , y3 y2 , y4 x3 y2 , > y3 , y5 x1 y3 ,y5 y4 , y6 x1 y4 , > y5 , y6 x2 y5 , > y6 , x3 y6 , unitclauses >, y1 .removing units clauses tautologies, obtain y1 , y2 , y3 , y4 x2 ,y4 x3 , y5 x1 , y5 y4 , y6 x1 y4 , y6 x2 y5 x3 y6 .Notice encoding consistent: assignment = {x2 , x3 },y4 propagated clause y4 x2 , turn propagates y5 due clause y5 y4finally y6 propagated clause y6 x2 y5 . contradiction foundclause x3 y6 .However, encoding GAC: partial assignment = {x1 } propagate y5 . However, x3 also propagated.3. BDD-3: let C1 , C2 C3 constraints setting respectively x1 , x2 x3 true.Figure 7 shows ROBDDs constraints. encode ROBDDs467fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichbergerusual, BDD-2, replacing unit clause r root r xi .case variables associated roots C1 , C2 C3 y1 , z1 w1respectively.removing units tautologies, clauses C1 y1 x1 , y2 y1 , y4 x2 y1 ,y3 y2 , y4 x3 y2 , y4 x2 y3 x3 y4 .Clauses C2 z1 x2 x3 z1 .Finally, clauses C3 w1 x3 , w2 w1 , x1 w1 x2 w2 .encoding GAC. Take, instance, assignment = {x1 }. case, w1propagated virtue x1 w1 x3 propagated clause w1 x3 .7. Related WorkDue ubiquity Pseudo-Boolean constraints success SAT solvers, problem encoding constraints SAT thoroughly studied literature.following review important contributions, paying special attentionbasic idea based, encoding size, propagation propertiesencodings fulfill. ease presentation, remaining section alwaysassume constraint want encode a1 x1 + . . . + xn k, maximumcoefficient amax .first encoding mention one proposed Warners (1998). nutshell,encoding uses several adders numbers binary representation. First all, left handside constraint split two halves, recursively treated computecorresponding partial sum. that, two partial sums added finalresult compared k . encoding uses O(n log(amax )) clauses variablesneither consistent GAC. surprising, since adders numbers binarymake extensive use xors, good propagation properties.Bailleux et al. (2006) introduced encoding close using BDDtranslating clauses. order understand differences construction BDDs let us introduce detail. First all, coefficients orderedsmall large. Then, root labeled variable Dn,k , expressing sumfirst n terms k. two children Dn1,k Dn1,kan ,correspond setting xn false true, respectively. process repeated nodescorresponding trivial constraints reached, encoded true false.node Di,b children Di1,b Di1,bai , following four clauses added:Di1,bai Di,bDi1,bai xi Di,bDi1,b Di,bDi1,b xi Di,bExample 25. encoding proposed Bailleux et al. (2006) 2x1 + + 2x10 + 5x11 +6x12 10 illustrated Figure 9. Node D10,5 represents 2x1 + 2x2 + + 2x10 5,whereas node D10,4 represents 2x1 + 2x2 + 2x10 4. method fails identifytwo PB constraints equivalent hence subtrees B C merged,yielding much larger representation ROBDDs.468fiA New Look BDDs Pseudo-Boolean ConstraintsD12,1001D11,100D11,410D10,10D10,5D10,4BC1D10,1falseFigure 9: Tree-like construction Bailleux et al. (2006) 2x1 + +2x10 +5x11 +6x12 10resulting encoding GAC, example PB constraint family givenkind non-reduced BDDs, concrete variable ordering exponentially large.However, shown Section 3.2, ROBDDs family polynomial.Several important new contributions presented paper MiniSATteam (Een & Sorensson, 2006). paper describes three encodings,implemented MiniSAT+ tool. first one standard ROBDD constructionPseudo-Boolean constraints. done two steps: first, suggest simple dynamicprogramming algorithm constructing non-reduced BDD, later reduced.result ROBDD, first step may take exponential time even final ROBDDpolynomial. ROBDD constructed, suggest encode SAT using6 ternary clauses per node. paper showed that, given concrete variable ordering,encoding GAC. Regarding ROBDD size, authors cite work Bailleux et al.(2006) state BDDs exponential worst case. seen before,citation correct Bailleux et al construct ROBDDs.second method similar one Warners (1998) sense construction relies network adders. First coefficients decomposed binaryrepresentation. bit i, bucket created variables whose coefficient bitset one. i-th bit left-hand side constraint computed using seriesfull adders half adders. Finally, resulting sum lexicographically compared k.resulting encoding neither consistent GAC uses number adders linearsum number digits coefficients.last method suggest use sorting networks. Numbers expressedunary representation coefficients decomposed using mixed radix representation.smaller number representation, smaller encoding. setting,sorting networks used play role adders, better propagationproperties. N smaller sum digits coefficients base 2, sizeencoding O(N log2 N ). Whereas encoding GAC arbitrary PseudoBoolean constraints, generalized arc-consistency obtained cardinality constraints.469fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-EichbergerEncodingWarnersNon-reduced BDDROBDDAddersSorting NetworksWatch Dog (WD)Gen. Arc-cons. WDReference(Warners, 1998)(Bailleux et al., 2006)(Een & Sorensson, 2006)(Een & Sorensson, 2006)(Een & Sorensson, 2006)(Bailleux et al., 2009)(Bailleux et al., 2009)ClausesO(n log amax )ExponentialExponential (6 per node)PO( log ai )PPO(( log ai ) log2 ( log ai )O(n2 log n log amax )O(n3 log n log amax )Consist.YESYESYESYESYESGACYESYESYESTable 1: Summary comparing different encodings.first polynomial GAC encoding Pseudo-Boolean constraints, called WatchDog, introduced Bailleux et al. (2009). uses O(n2 log n log amax ) variablesO(n3 log n log amax ) clauses. Again, numbers expressed unary representationtotalizers used play role sorting networks. order make comparisonright hand side trivial, left-hand side k incremented k becomespower two. Then, coefficients decomposed binary representationbit added independently, taking account corresponding carry. paper, another encoding consistent uses O(n log n log amax ) variablesO(n2 log n log amax ) clauses also presented.Finally, worth mentioning work Bartzis Bultan (2003). authorsdeal general case variables xi Boolean, boundedintegers 0 xi < 2b . suggest BDD-based approach similar flavormethod Section 4, instead decomposing coefficients do, decomposevariables xi binary representation. BDD ordering starts first bitx1 , first bit x2 , etc... that, second bit treated similarPfashion, on. resulting BDD O(n b ai ) nodes nothing mentionedpropagation properties. case Pseudo-Boolean constraints, i.e. b = 1,approach amounts standard BDDs.Table 1 summarizes different encodings PB constraints SAT.8. Experimental Resultsgoal section assess practical interest encodings presentedpaper. aim evaluate extent BDD-based encodings interestingpractical point view. us, means study whether competitiveexisting techniques, whether show good behavior general interestingspecific types problems, whether produce smaller encodings.purpose, first all, compare encodings SAT encodingsterms encoding time, number clauses number variables. that, alsoconsider total runtime (that is, encoding time plus solving time) encodingscompare runtime state-of-the-art Pseudo-Boolean solvers. Finally, brieflyreport experiments sharing, is, trying encode several Pseudo-Booleanconstraints single ROBDD.470fiA New Look BDDs Pseudo-Boolean Constraintsexperiments performed 2Ghz Linux Quad-Core AMD time limit1800 seconds per benchmark. benchmarks used experiments obtainedPseudo-Boolean Competition 2011 (http://www.cril.univ-artois.fr/PB11/),category optimization, small integers, linear constraints (DEC-SMALLINT-LIN).compactness clarity, grouped benchmarks come sourcefamilies. However, individual results found http://www.lsi.upc.edu/~iabio/BDDs/results.ods.8.1 Bergmann-Hommel Testorder summarize experiments make easier extract conclusions, every experiment accompanied Bergmann-Hommel non-parametric hypothesis test(Bergmann & Hommel, 1988) results confidence level 0.1.Bergmann-Hommel test way comparing results n different methodsmultiple independent data sets. gives us two interesting pieces information. Firstall, sorts methods giving real number 1 n,lower number better method. Moreover, indicates, pair methods,whether one method significantly improves upon other. example, Figure 10output Bergmann-Hommel test. BDD-1 best method significantdifference method BDD-2 (this illustrated thick line connectingmethods). hand, Bergmann-Hommel test indicates BDD-1significantly better Adder, since thick line connecting BDD-1 Adder.said BDD-1 WD-1, BDD-1 BDD-3, BDD-1 WD-2, BDD-2Adder, etc.give quick overview Bergmann-Hommel test computed.remaining section skipped reader interested detailstest. hand, detailed information, refer reader workBergmann Hommel (1988).Let us assume n methods data sets, let Ci,j result (time,number variables value) i-th method j-th benchmark.every data set, assign number every method: best method data set1, second 2, on. Then, every method, compute averagevalues different data sets. obtained value denoted Ri calledaverage rank i-th method. method smaller average rank bettermethod bigger one.average ranks make possible rank different methods. However,also interested detecting whether differences methods significantnot: computed second part test. that, need previousdefinitions.R RGiven i, j N = {1, 2, . . . , n}, denote pi,j p-value3 zi,j = jn(n1)/(6m)respect normal distribution N (0, 1). partition N = {1, 2, . . . , n} collection setsP = {P1 , P2 , . . . , Pr } (i) Pi subsets N , (ii) P1 P2 Pr = N3. p-value z respect normal distribution N (0, 1) probability p[ |Z| > |z| ],random variable Z N (0, 1).471fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger(iii) Pi Pj = every 6= j. Given P partition N , defineL(P ) =rX|Pi |(|Pi | 1)i=12p(P ) minimum pi,j j belong subset Pk P .Bergmann-Hommel test ensures (with significance level ) methodsj significantly different partition P p(P ) > L(P )j belong subset Pk P . Hence, time-consuming testsince number partitions large.case, data sets families benchmarks. use familiesinstead benchmarks data sets must independent.8.2 Encodings SATstart comparing different methods encoding Pseudo-Boolean constraintsSAT. focused time spent encoding, number auxiliary variablesused number clauses. Moreover, benchmark family, also reportnumber PB-constraints encoded SAT.encodings included experimental evaluation are: adder encoding presented Een Sorensson (2006) (Adder), consistent WatchDog encodingBailleux et al. (2009) (WD-1), GAC version (WD-2), encoding ROBDDs without coefficient decomposition, using Algorithm 1 encoding Section 6 (BDD1); encoding ROBDDs coefficient decomposition explained Section 4.2(BDD-2), Algorithm 1 encoding Section 6; GAC approachSection 4.3 (BDD-3 ), also Algorithm 1 encoding Section 6. NoticeBDD-1 method similar ROBDDs presented Een Sorensson (2006).However, since Algorithm 1 produces every node once, BDD-1 faster. Also,encoding Section 6 creates two clauses per BDD node, opposed six clausessuggested Een Sorensson.Table 2 shows number problems different methods could encode withouttiming out. first column corresponds family problems. second columnshows number problems family. third fourth columns contain average number SAT Pseudo-Boolean constraints problem. experiments,considered constraint SAT clause 3 variables. Small PBconstraints benefit encodings hence constraints naiveencoding SAT always used. remaining columns correspond numberencoded problems without timing out. Time limit set 1800 seconds per benchmark.Table 3 shows time spent encode benchmarks different methods.before, first columns correspond family problems, number problemsfamily average number SAT Pseudo-Boolean constraints problems. remaining columns correspond average encoding time (in seconds) perbenchmarks method. Timeouts counted 1800 seconds average computation.Table 4 shows average number auxiliary variables required encoding PBconstraints (SAT constraints counted). meaning first 4 columns472fiA New Look BDDs Pseudo-Boolean ConstraintsFamilylopesarmyblastcachechnldbstv30dbstv40dbstv50dlxelffpgaj30j60j90j120neosooopig-crdpig-clppprobin13queen11tsp11vdwPr2001289215553536171817284192020661001005TOTAL669SAT502,6711926,510181,1000326,200985,2002,552,00020,90746,446013,68530,83250,553104,1471,45195,2170161,15029,846082,6628,978PB592,7154511,2534,5071252,7014,80175018571,3996872703093375163,8314,487113581,0237619345267,840Adder1881289215553536171817284192020661001005WD-11881289215553536171817284192020661001005WD-2118128921000353617188114191820621001005BDD-11971289215553536171817284192020661001005BDD-21971289215553536171817284192020661001005BDD-31881289210003536171811184192020661001005657657540666666626Table 2: Number problems encoded (without timing out) different methods.6543WD221BDD1BDD2BDD3WD1AdderFigure 10: Statistical comparison results Table 3, time spent differentmethods encoding.before, others contain average number auxiliary variables (inthousands) benchmarks time out.Finally, Table 5 contains average number (in thousands) clauses needed encodeproblem. before, considered benchmarks timed out,clauses due encoding SAT constraints counted.Figures 10, 11 12 represent Bergmann-Hommel tests tables.show BDD-1, BDD-2 Adders best methods terms time, variablesclauses. worth mentioning BDD-1 BDD-2 faster use significantly lessclauses Adder. However, Adders uses significantly less auxiliary variables BDD-2.Notice BDD-1 GAC, BDD-2 consistent Adder consistent,least theoretically BDD-1 clauses unit propagation power BDD-2 clauses,BDD-2 clauses better Adder clauses. Hence, BDD-1 best method usingcriteria BDD-2 better Adder. Regarding methods, seems clear473fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-EichbergerFamilylopesarmyblastcachechnldbstv30dbstv40dbstv50dlxelffpgaj30j60j90j120neosooopig-crdpig-clppprobin13queen11tsp11vdwPr2001289215553536171817284192020661001005SAT502,6711926,510181,1000326,200985,2002,552,00020,90746,446013,68530,83250,553104,1471,45195,2170161,15029,846082,6628,978PB592,7154511,2534,5071252,7014,8017,5018571,3996872703093375163,8324,487113581,0247619345267,840AverageAdder335.230.373.8923.080.5457.77211.51547.303.737.371.903.646.8514.8119.2510.4313.480.977.736.1312.030.190.46170.33WD-1292.140.432.4518.741.0597.21210.25552.993.056.532.464.6210.6931.0247.6212.659.673.298.785.0967.410.450.51109.42WD-2996.0739.9840.4181.6587.088.4121.6869.9081.03466.071,277.281,305.55257.9771.20517.51284.1533.261,315.96100.2924.42441.21BDD-1165.750.192.2016.190.1345.85105.62272.022.765.190.303.138.1928.2021.683.467.760.227.353.172.940.140.3047.15BDD-2163.660.191.8915.740.1383.09165.96468.512.755.900.303.678.7727.7625.505.327.880.217.313.232.820.170.3346.32BDD-3316.3510.2623.1547.782.686.1913.423.7542.44252.691,155.18967.1077.0426.359.5210.799.83301.1118.486.30125.91110.5799.79510.4055.9057.66223.41Table 3: Average time spent encoding different methods.Familylopesarmyblastcachechnldbstv30dbstv40dbstv50dlxelffpgaj30j60j90j120neosooopig-crdpig-clppprobin13queen11tsp11vdwAveragePr2001289215553536171817284192020661001005SAT502,6711926,510181,1000326,200985,2002,552,00020,90746,446013,68530,83250,553104,1471,45195,2170161,15029,846082,6628,978PB592,7154511,2534,5071252,7014,8017,5018571,3996872703093375163,8324,487113581,0247619345267,840Adder1,744.054.6327.77145.668.39219.822,468.456,135.1310.420.3721.1518.1537.0365.4159.4373.74118.1515.267.6857.13171.672.23.371,895.39WD-13,566.1110.9662.22339.1824.55709.736,564.4416,365.3921.6242.7853.9650.8112.35217.01540185.59273.5450.7525.25141.49628.456.178.833,356.94WD-25,478.81245.491,394.52,393.971,007.9247.79571.381,074.031,190.594,775.726,543.525,713.753,542.942,248.342,966.581,984.06623.583,634.13461.54170.5912,818.51BDD-12,393.976.3636.74201.186.76441.864,282.1611,111.0612.4024.6213.2744.96157.92553.8612.1379.33162.2511.934.0181.57158.555.635.711,391.65BDD-22,3946.3639.67210.836.761,695.877,225.6319,723.3713.8928.1313.2759.82180.05553.76806.08122.53168.6111.934.0182.86158.557.086.511,391.65BDD-37,734.65479.83761.291,503.19184.59126.81306.76242.031,153.517,285.6919,793.4922,246.822,003.951,315.77632.33310.03382.6716,565.75791.43221.215,875.58591.351,266.231,876.33892.62998.443,807.34Table 4: Average number auxiliary variables (in thousands) used.474fiA New Look BDDs Pseudo-Boolean ConstraintsFamilylopesarmyblastcachechnldbstv30dbstv40dbstv50dlxelffpgaj30j60j90j120neosooopig-crdpig-clppprobin13queen11tsp11vdwAveragePr2001289215553536171817284192020661001005SAT502,6711926,510181,1000326,200985,2002,552,00020,90746,446013,68530,83250,553104,1471,45195,2170161,15029,846082,6628,978PB592,7154511,2534,5071252,7014,8017,5018571,3996872703093375163,8324,487113581,0247619345267,840Adder10,643.6326.09184.95980.5156.821,497.4117,184.6242,797.3865.25129.05139.45121.56253.16450.491,102.86471.47793.36104.6852.41392.61,185.9214.7323.3110,885.963,675.7WD-17,471.8934.5102.07550.68116.973,367.7516,916.644,310.8335.6871.28175.66164.78494.831,286.173,803.28594.72442.47367.03180.33271.666,916.3538.9125.356,564.142,970.54WD-222,082.472,155.72,108.483,651.914,936.16377.76881.023,615.213,889.5822,843.0534,136.726,205.1912,410.13,378.1620,711.1114,641.261,804.2919,694.865,068.351,335.624,274.218,297.1BDD-13,049.0610.8770.9272.2711.23857.395,526.614,400.1423.0446.315.6589.53311.151,106.221,186.55139.26219.4119.864.07100.89281.4210.847.761,662.731,174.38BDD-23,049.0210.8765.46275.2611.233,282.2211,259.2931,279.222.5946.0715.65116.04351.011,095.121,570.73220.45227.6119.864.07103.24281.4213.739.351,662.731,380.41BDD-39,746.32924.931,264.832,418.77285.67208.92507.46278.372,244.02114,355.239,112.0844,068.973,681.262,126.42958.65314.05654.4828,875.611,573.89433.597,262.885,843.53Table 5: Average number clauses (in thousands) used.65432WD2BDD31AdderBDD1WD1BDD2Figure 11: Statistical comparison results Table 4, number auxiliary variablesused different encodings.6543WD221BDD1BDD2BDD3WD1AdderFigure 12: Statistical comparison results Table 5, number clauses useddifferent methods.475fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-EichbergerFamilylopesarmyblastcachechnldbstv30dbstv40dbstv50dlxelffpgaj30j60j90j120neosooopig-crdpig-clppprobin13queen11tsp11vdwTOTALAdder42989350035251717171421522431001001WD-1541289355535361717171621921331001001WD-240789200035361717792162242100961BDD-1561089555535361717171621821331001001BDD-2571189555535361717171621921431001001BDD-3615893000353617178112171246100751bsolo396872155535361717171321419343100721MiniSAT6668835553533171717122152243100901SAT4J2368615553536171717162142244100931Wbo63686355535361717171621522331001001borg37108621555353617171716214205531001001SMT43589055535361717171621700441001001VBS77128921555353617171717219205661001002403443385444448391422429392440458419514Table 6: Number problems solved different methods.encoding n different constraints order obtain GAC, done WD-2BDD-3, good idea terms variables clauses.8.3 SAT vs. PBsection compare state-of-the-art solvers Pseudo-Boolean problemsencodings SAT. SAT approach, encoding done,SAT formula given SAT Solver Lingeling (Biere, 2010) version 276.considered SAT encodings previous section. Regarding Pseudo-Booleansolvers, considered MiniSAT+ (Een & Sorensson, 2006) best non-parallelsolvers optimization, small integers, linear constraints category PseudoBoolean Competition: borg (Silverthorn & Miikkulainen, 2010) version pb-dec-11.04.03,bsolo (Manquinho & Silva, 2006) version 3.2, wbo (Manquinho, Martins, & Lynce, 2010)version 1.4 SAT4J (Berre & Parrain, 2010) version 2.2.1. also includedSMT Solver Barcelogic (Bofill, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Rubio, 2008)PB constraints, couples SAT solver theory solver PB constraints.Table 6 shows number instances solved method. Table 7 shows averagetime spent methods. SAT encodings, times include encodingSAT solving time. before, time limit 1800 seconds per benchmark set,average computation, timeout counted 1800 seconds. tables includecolumn VBS (Virtual Best Solver), represents best solver every instance.gives idea speedup could obtain portfolio approach.Figure 13 shows result Bergmann-Hommel test: SMT best method,whereas Adder, BDD-3 WD-2 worst ones. significant differencemethods. main conclusion infer BDD encodingsdefinitely competitive method. Also, technique outperforms othersbenchmark families, hence portfolio strategies would make lot sense476fiA New Look BDDs Pseudo-Boolean ConstraintsFamilylopesarmyblastcachechnldbstv30dbstv40dbstv50dlxelffpgaj30j60j90j120neosooopig-crdpig-clppprobin13queen11tsp11vdwAdder1,5156606.122531,5431,0497.0613.8758616.713724.189781,0234791,6201,62463193847.5228.361,645WD-11,4201392.561231,5431283669354.8810.145.277.7911436.638549361901,6201,7151,0019211.648.291,568WD-21,5611,14146.783961,71625.7244.091131165511,3031,3641,4054931,6801,6936561,353264428.61,545BDD-11,4085432.4275.491,50891.661986294.297.970.925.9411339.728399101511,6201,7189069134.6323.861,493BDD-21,4014691.991151,5081923247924.3490.928.4211639.468519151761,6201,7188589134.5118.321,493BDD-31,4351,29827.633751,68119.5830.0337.6477.883981,2331,2621,0734881,7251,7216467196437311,612bsolo1,5091,0280.126530.5559.281872003.4728.580.276.531100.99671,1066451141,65860593654.828551,478MiniSAT1,3449130.513951,55132.672.254301.292.972424.61153.961,0311,2764531,6261,6239199712383691,448SAT4J1,6611,1270.846701,75199.819.7421.221.62.311.4714.571051.428491,0385751,7491,70560277818.925031,596Wbo1,3641,0840.086061,6731.545.6916.130.551.425.170.531010.418399014861,6851,7429019205.92291,441borg1,5554382.136363.789.8745.331213.1511.613.041.931043.328419765123.921,36939096320.3527.641,450SMT1,4641,0660.032661.284.4411.360.170.690.10.281010.158149252596016051.921.811,441VBS1,249860.0363.950.471.284.4411.360.170.690.070.281010.157569011261.921,3672104441.281.511,1867836699586676671,003764772849710613696475Av.Table 7: Time spent different methods solving problem (in seconds).121110987654WD232SMTBDD3WboAdderSAT4JMiniSATbsoloborgBDD1BDD2WD1Figure 13: Statistical comparison results Table 7, runtime different methods.4771fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichbergerarea, witnessed performance Borg, implements approach. Finally,also want mention possible exponential explosion BDDs rarely occurspractice hence, coefficient decomposition seem pay practical situations.Regarding Best Virtual Solver, SMT contributes 52% problems. 25%cases best solution given specific PB solver. Among them, Wbo contributes10% problems bsolo 8%. Finally, encoding methods give bestsolution 23% cases: 14% times due Watchdog methods 8%times due BDD-based methods.8.4 SharingOne possible advantages using ROBDDs encode Pseudo-Boolean constraintsROBDDs allow one encode set constraints, one. would seemnatural think two constraints similar enough, two individual ROBDDswould similar structure, merging single one would result ROBDDwhose size smaller sum two individual ROBDDs. However, maindifficulty decide constraints encoded together, since bad choice couldresult ROBDD whose size larger sum ROBDDs individualconstraints.performed initial experiments criteria similarity constraintstook account variables appeared constraints. first fixedinteger k chose constraint largest set variables. that, lookedconstraint k variables appeared first constraint. next steplook another constraint k variables appeared twoprevious constraints on, reaching fixpoint. Finally, selected constraintsencoded together.tried experiment benchmarks different values k rarely gaveadvantage. However, still believe could way encoding differentconstraints single ROBDD, different criteria selecting constraintsstudied. see possible line future research.9. Conclusions Future Worktheoretical practical contributions made. Regarding theoreticalpart, negatively answered question whether PB constraints admit polynomial BDDs citing work Hosaka et al. (1994) which, best knowledge,largely unknown research community. Moreover, given simpler proofassuming NP different co-NP, relates subset sum problemROBDDs size PB constraints.practical level, introduced ROBDD-based polynomial generalizedarc-consistent encoding PB constraints developed BDD-based generalized arcconsistent encoding monotonic functions uses two clauses per BDD node.also presented algorithm efficiently construct ROBDDs provedoverall method competitive practice state-of-the-art encodings tools.future work practical level, plan study type Pseudo-Boolean478fiA New Look BDDs Pseudo-Boolean Constraintsconstraints likely produce smaller ROBDDs encoded together ratherencoded individually.AcknowledgmentsUPC authors partially supported Spanish Min. Educ. ScienceLogicTools-2 project (TIN2007-68093-C02-01). Abo also partially supported FPUgrant.ReferencesAbo, I., Nieuwenhuis, R., Oliveras, A., & Rodrguez-Carbonell, E. (2011). BDDs pseudoboolean constraints: revisited. Proceedings 14th international conferenceTheory application satisfiability testing, SAT 11, pp. 6175, Berlin, Heidelberg.Springer-Verlag.Aloul, F. A., Ramani, A., Markov, I. L., & Sakallah, K. A. (2002). Generic ILP versusspecialized 0-1 ILP: update. Proceedings 2002 IEEE/ACM internationalconference Computer-aided design, ICCAD 02, pp. 450457, New York, NY, USA.ACM.Bailleux, O., Boufkhad, Y., & Roussel, O. (2006). Translation Pseudo Boolean Constraints SAT. Journal Satisfiability, Boolean Modeling Computation, JSAT,2 (1-4), 191200.Bailleux, O., Boufkhad, Y., & Roussel, O. (2009). New Encodings Pseudo-Boolean Constraints CNF. Kullmann, O. (Ed.), 12th International Conference TheoryApplications Satisfiability Testing, SAT 09, Vol. 5584 Lecture NotesComputer Science, pp. 181194. Springer.Bartzis, C., & Bultan, T. (2003). Construction efficient bdds bounded arithmeticconstraints. Proceedings 9th international conference Tools algorithms construction analysis systems, TACAS 03, pp. 394408, Berlin,Heidelberg. Springer-Verlag.Bergmann, B., & Hommel, G. (1988). Improvements general multiple test proceduresredundant systems hypotheses. Bauer, P., Hommel, G., & Sonnemann,E. (Eds.), Multiple Hypothesenprfung - Multiple Hypotheses Testing, pp. 100115.Springer-Verlag.Berre, D. L., & Parrain, A. (2010). Sat4j library, release 2.2. Journal Satisfiability,Boolean Modeling Computation, JSAT, 7 (2-3), 596.Bessiere, C., Katsirelos, G., Narodytska, N., & Walsh, T. (2009). Circuit complexitydecompositions global constraints. Proceedings 21st international jontconference Artifical intelligence, IJCAI 09, pp. 412418, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Biere, A. (2010). Lingeling, Plingeling, PicoSAT PrecoSAT SAT Race 2010. Tech.rep., Institute Formal Models Verification, Johannes Kepler University, Al479fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichbergertenbergerstr. 69, 4040 Linz, Austria. Technical Report 10/1, August 2010, FMVReports Series.Bofill, M., Nieuwenhuis, R., Oliveras, A., Rodrguez-Carbonell, E., & Rubio, A. (2008).Barcelogic SMT Solver. Computer-aided Verification (CAV), Vol. 5123 LectureNotes Computer Science, pp. 294298.Bryant, R. E. (1986). Graph-Based Algorithms Boolean Function Manipulation. IEEETransactions Computers, 35 (8), 677691.Bryant, R. E., Lahiri, S. K., & Seshia, S. A. (2002). Deciding CLU Logic Formulas viaBoolean Pseudo-Boolean Encodings. Proceedings International Workshop Constraints Formal Verification, CFV 02. Associated InternationalConference Principles Practice Constraint Programming (CP 02).Een, N., & Sorensson, N. (2006). Translating Pseudo-Boolean Constraints SAT. JournalSatisfiability, Boolean Modeling Computation, 2, 126.Hosaka, K., Takenaga, Y., & Yajima, S. (1994). Size Ordered Binary DecisionDiagrams Representing Threshold Functions. Algorithms Computation, 5thInternational Symposium, ISAAC 94, pp. 584592.Manquinho, V. M., Martins, R., & Lynce, I. (2010). Improving Unsatisfiability-Based Algorithms Boolean Optimization. Strichman, O., & Szeider, S. (Eds.), 13thInternational Conference Theory Applications Satisfiability Testing, Vol.6175 SAT 10, pp. 181193. Springer.Manquinho, V. M., & Silva, J. P. M. (2006). Using Cutting Planes Pseudo-BooleanOptimization. Journal Satisfiability, Boolean Modeling Computation, JSAT,2 (1-4), 209219.Mayer-Eichberger, V. (2008). Towards Solving System Pseudo Boolean ConstraintsBinary Decision Diagrams. Masters thesis, New University Lisbon.Schutt, A., Feydy, T., Stuckey, P. J., & Wallace, M. G. (2009). cumulative decomposition bad sounds. Proceedings 15th international conferencePrinciples practice constraint programming, CP09, pp. 746761, Berlin,Heidelberg. Springer-Verlag.Silverthorn, B., & Miikkulainen, R. (2010). Latent class models algorithm portfoliomethods. Proceedings Twenty-Fourth AAAI Conference Artificial Intelligence.Smaus, J. (2007). Boolean Functions Encodable Single Linear Pseudo-BooleanConstraint. Hentenryck, P. V., & Wolsey, L. A. (Eds.), 4th International ConferenceIntegration AI Techniques Constraint Programming, CPAIOR07, Vol. 4510 Lecture Notes Computer Science, pp. 288302. Springer.Warners, J. P. (1998). Linear-Time Transformation Linear Inequalities ConjunctiveNormal Form. Information Processing Letters, 68 (2), 6369.480fiJournal Artificial Intelligence Research 45 (2012) 601-640Submitted 8/12; published 12/12Irrelevant Independent Natural ExtensionSets Desirable GamblesGert de Coomangert.decooman@ugent.beGhent University, SYSTeMS Research GroupTechnologiepark 914, 9052 Zwijnaarde, BelgiumEnrique Mirandamirandaenrique@uniovi.esUniversity Oviedo, Dept. Statistics O.R.C-Calvo Sotelo, s/n, Oviedo 33007, SpainAbstractresults paper add useful tools theory sets desirable gambles,growing toolbox reasoning partial probability assessments. investigatecombine number marginal coherent sets desirable gambles joint set usingproperties epistemic irrelevance independence. provide formulas smallestjoint, called independent natural extension, study main properties.independent natural extension maximal coherent sets desirable gambles allows usdefine strong product sets desirable gambles. Finally, explore easy waygeneralise results also apply conditional versions epistemic irrelevanceindependence. set tools easily implemented computer programsclearly beneficial fields, like AI, clear interest coherent reasoninguncertainty using general robust uncertainty models require full specification.1. Introductionreasoning decision making uncertainty, little doubt probabilities play leading part. Imprecise probability models provide well-founded extensionprobabilistic reasoning, allow us deal incomplete probability assessments,indecision robustness issues.1Early imprecise probability models (going back to, amongst others, Bernoulli, 1713;Boole, 1952, 1961; Koopman, 1940) centered lower upper probabilities eventspropositions. later stages (see instance Smith, 1961; Williams, 1975b and,clearest statement, Walley, 1991, Section 2.7), became apparent languageevents lower probabilities lacking power expression, muchexpressive theory built using random variables lower previsions (or lower expectations), instead.2 However, even though quite successful,quite well developed, number problems lower prevision approach.mathematical complexity fairly high, especially conditioning independence1. get good idea field imprecise probabilities about, evolving, browseonline proceedings biennial ISIPTA conferences, found web site (www.sipta.org) Society Imprecise Probability: Theories Applications.2. contrast, precise probability models, expressive power probabilities expectationssame: linear prevision expectation set (bounded) real-valued maps uniquelydetermined restriction events (a finitely additive probability), vice versa.c2012AI Access Foundation. rights reserved.fiDe Cooman & Mirandaenter picture. Also, coherence requirements, specify basic rules properinference using (conditional) lower previsions, quite cumbersome, rather harderchop intuitive elementary building blocks precise-probabilistic counterparts, even though latter turn special instances former. Finally,case many approaches probability, see on,theory coherent lower previsions issues conditioning sets probability zero.attractive solution problems offered Walley (2000), formsets desirable gambles. Walleys work inspired earlier ideas Smith (1961)Williams (1975b), previous work along lines also done Seidenfeld,Schervish, Kadane (1995). approach, primitive notions probabilitiesevents, expectations random variables. Rather, starting point questionwhether gamble, risky transaction, desirable subject, i.e. strictly preferredzero transaction, status quo. basic belief model probability measure,lower prevision, set desirable gambles.Let us briefly summarise believe working sets desirable gamblesbasic belief models deserves attention AI community:Primo, number examples literature shown (Couso & Moral, 2011;De Cooman & Quaeghebeur, 2012; Moral, 2005), shall see (lookinstance Examples 1 2), working making inferences using set desirablegambles subjects uncertainty model general expressive. alsoarguably simpler elegant mathematical point view,intuitive geometrical interpretation (Quaeghebeur, 2012b).Secundo, shall see Sections 4 5 approach coherent marginalisationconditioning especially straightforward, issues conditioningsets probability zero.Tertio, argue Section 2.3, similarity acceptinggamble one hand, accepting proposition true other, workingsets desirable gambles leads account probabilistic inference logicalflavour; see work Moral Wilson (1995) early discussion idea.Quarto, working sets desirable gambles encompasses subsumes specialcases classical (or precise) probabilistic inference inference classical propositional logic; see Sections 2 5.finally, quinto, made clear discussion throughout, sets desirablegambles eminently suited dealing partial probability assessments, situationsexperts express beliefs, preferences behavioural dispositions using finitelymany assessments need determine unique probability measure. particular,discuss connection partial preferences Section 2.1.Let us try present preliminary defense sweeping claims examples. One particular perceived disadvantage working lower previsionsorprevisions probabilities matteris conditioning lower prevision needlead uniquely coherent results conditioning event lower upper probabilityzero; see instance work Walley (1991, Section 6.4). precise probabilities,difficulty circumvented using full conditional measures (Dubins, 1975).already mentioned, imprecise-probabilities context, working informative coherent sets desirable gambles rather lower previsions provides602fiIrrelevant Independent Natural Extension Sets Desirable Gambleselegant intuitively appealing way problem, already suggestedWalley (1991, Section 3.8.6 Appendix F), argued much detaillater work (Walley, 2000). connection full conditional measures maximalcoherent sets desirable gambles recently explored Couso Moral (2011):latter still general expressive.work De Cooman Quaeghebeur (2012) shown working setsdesirable gambles especially illuminating context modelling exchangeabilityassessments: exposes simple geometrical meaning notion exchangeability,leads simple particularly elegant proof significant generalisation deFinettis representation theorem exchangeable random variables (de Finetti, 1931).Exchangeability structural assessment, independence, quite commoncontext probabilistic graphical models, Bayesian (Pearl, 1985) credal networks (Cozman, 2000). Conditioning independence are, course, closely related.recent paper (De Cooman, Miranda, & Zaffalon, 2011), investigated notions epistemic independence finite-valued variables using coherent lower previsions, thus addingliterature assessments epistemic irrelevance independence studiedgraphical models (De Cooman, Hermans, Antonucci, & Zaffalon, 2010; Destercke & DeCooman, 2008), alternative often used notion strong independence.above-mentioned problems conditioning, fact coherence requirements conditional lower previsions are, honest, quite cumbersome work with,turned quite complicated exercise. reason why, presentpaper, intend show looking independence using sets desirable gambles leadselegant theory avoids complexity pitfalls working coherent lower previsions. this, build strong pioneering work epistemicirrelevance Moral (2005). focus symmetrised notion epistemicindependence, much seen application continuationideas.goal paper show local models variables, togetherindependence assessments, combined order produce joint model. jointmodel used draw inferences, done instance context Bayesiancredal networks (Antonucci, de Campos, & Zaffalon, 2012; Cozman, 2000; Pearl, 1985).One core ideas probabilistic graphical models provide representationjoint model less taxing computational point view.three main novelties approach: first allow imprecisionlocal modelsalthough precise models particular case; secondmodel local probability assessments means sets desirable gambles,above-mentioned advantages possess coherent lower previsions; thirdstress epistemic irrelevance independence rather commonassessment strong independence, reasons become clear onalthoughalso discuss strong independence.results paper adding useful tools growing toolboxreasoning partial probability assessments sets desirable gambles constitute,something already started work exchangeability (De Cooman & Quaeghebeur,2012) work epistemic irrelevance credal networks Moral (2005).regard, also interesting mention algorithms making inferences sets603fiDe Cooman & Mirandadesirable gambles recently established (Couso & Moral, 2011; Quaeghebeur,2012a). set tools easily implemented computer programsclearly beneficial field like AI, surely interested coherent reasoninguncertainty general robust uncertainty models require full specification. paper constitutes step direction, also allows ussee clearly main difficulties faced working sets desirablegambles. remain, however, number important situations dealt with,future lines research discussed number places paper, wellConclusion.Section 2 summarise relevant results existing theory sets desirablegambles. mentioning useful notational conventions Section 3, recall basicmarginalisation, conditioning extension operations sets desirable gambles Sections 4 5. use combine number marginal sets desirable gamblesjoint satisfying epistemic irrelevance (Section 6), epistemic independence (Section 7).Section 8, study particular case maximal coherent sets desirable gambles,derive concept strong product. Section 9 deals conditional independenceassessments.2. Coherent Sets Desirable Gambles Natural ExtensionLet us begin explaining basic uncertainty models, coherent sets desirablegambles, (more details found Augustin, Coolen, De Cooman, & Troffaes,2012; Couso & Moral, 2011; De Cooman & Quaeghebeur, 2012; Moral, 2005; Walley, 2000).Consider variable X taking values possibility space X, assumepaper finite.3 model information X means sets desirable gambles.gamble real-valued function X, denote set gambles X G(X ).linear space point-wise addition gambles, point-wise multiplicationgambles real numbers. subset G(X ), denote posi(A) setpositive linear combinations gambles A:posi(A) :=Xnk=1k fk : fk A, k > 0, n > 0 .call convex cone closed positive linear combinations, meaningposi(A) = A.two gambles f g X, write f g (x X )f (x) g(x),f > g f g f 6= g. gamble f > 0 called positive. gamble g 0 callednon-positive. G(X )6=0 denotes set non-zero gambles, G(X )>0 convex conepositive gambles, G(X )0 convex cone non-positive gambles.3. results section remain valid working general, possibly infinite, possibilityspaces, case gambles assumed bounded real-valued functions. make finiteness assumption avoid deal controversial issue conglomerability (Miranda,Zaffalon, & De Cooman, 2012; Walley, 1991), make discussion independencelater sections significantly easier, practically implementable inference systems AIfinitary case.604fiIrrelevant Independent Natural Extension Sets Desirable Gambles2.1 Coherence Avoiding Non-positivityDefinition 1 (Avoiding non-positivity coherence). say set desirable gambles G(X ) avoids non-positivity f 6 0 gambles f posi(D),words G(X)0 posi(D) = . called coherent satisfies followingrequirements:D1. 0/ D;D2. G(X )>0 D;D3. = posi(D).denote D(X) set coherent sets desirable gambles X.Requirement D3 turns convex cone. Due D2, includes G(X )>0 ; due D1D3,excludes G(X)0 , therefore avoids non-positivity:D4. f 0 f/ D, equivalently G(X )0 = .set G(X )>0 coherent, smallest subset G(X ). set representsminimal commitments part subject, sense knows nothinglikelihood different outcomes prefer zero gamblessure never decrease wealth possibility increasing it. Hence,usually taken model complete ignorance, called vacuous model.One interesting feature coherent sets desirable gambles linkedfield decision making incomplete preferences (Aumann, 1962; Dubra, Maccheroni,& Ok, 2004; Shapley & Baucells, 1998), formally equivalent strictversions partial preference orderings (Buehler, 1976; Giron & Rios, 1980). Givencoherent set desirable gambles D, define strict preference relationgamblesf g f g gambles f g G(X ).Indeed, due linearity utility scale, exchanging gamble g gamble ftransaction reward function f g, strictly preferring f g meansexchange strictly preferred status quo (zero). relation satisfiesfollowing conditions:SP1. f f f G(X )[irreflexivity]SP2. f > g f g f, g G(X )[monotonicity]SP3. f g g h f h f, g, h G(X )[transitivity]SP4. f g f + (1 )h g + (1 )h (0, 1] f, g, h G(X ) [mixtureindependence]Conversely, preference relation satisfying axioms determines coherent setdesirable gambles. Partial preference orderings provide foundation general decisiontheory imprecise probabilities imprecise utilities (Fishburn, 1975; Seidenfeld et al.,1995; Seidenfeld, Schervish, & Kadane, 2010). See also work Moral Wilson(1995), Walley (1991, 2000) Quaeghebeur (2012b, Section 2.4) information.605fiDe Cooman & Miranda2.2 Natural Extensionconsider anyTnon-empty family coherent sets desirable gambles Di , I,intersection iI Di still coherent. idea behind following result,brings fore notion coherent inference. subject gives us assessment,set G(X ) gambles X finds desirable, tells us exactlyassessment extended coherent set desirable gambles, constructsmallest set.Theorem 1 (De Cooman & Quaeghebeur, 2012). Consider G(X ), definenatural extension by:4\E(A) :={D D(X) : D} .following statements equivalent:(i) avoids non-positivity;(ii) included coherent set desirable gambles;(iii) E(A) 6= G(X );(iv) set desirable gambles E(A) coherent;(v) E(A) smallest coherent set desirable gambles includes A.(and hence all) equivalent statements hold,E(A) = posi G(X )>0 .shows assessment finite description, representnatural extension computer storing finite description extreme rays.Although general assessments need finite description (for instanceconsidered Eq. (3) need one), interestvast range practical situations. description many cases partialprobability assessments given finite description, efficient algorithmsverifying coherence computing natural extension set gambles, referwork Couso Moral (2011) Quaeghebeur (2012a).2.3 Connection Classical Propositional Logicdefinition coherent set desirable gambles, Theorem 1, make clear inference desirable gambles bears formal resemblance deduction classical propositionlogic: D3 production axiom states positive linear combinations desirablegambles desirable. exact correspondences listed following table:Classical propositional logiclogical consistencydeductively closeddeductive closure4. usual, expression, let= G(X ).606Sets desirable gamblesavoiding non-positivitycoherentnatural extensionfiIrrelevant Independent Natural Extension Sets Desirable Gamblesshall see inference sets desirable gambles (precise-)probabilisticinference, particular Bayess Rule, special case. easy see alsogeneralises (includes special case) classical propositional logic: proposition pidentified subset Ap Stone space X , accepting proposition p correspondsjudging gamble IAp 1 + desirable > 0.5 IAp so-calledindicator (gamble) Ap , assuming value 1 Ap 0 elsewhere. See work DeCooman (2005) detailed discussion.2.4 Helpful Lemmasorder prove number results paper, need following lemmas, oneconvenient version separating hyperplane theorem. rely heavilyassumption finite space X , easily extended general case.Lemma 2. Assume X finite, consider finite subset G(X ). 0/posi(G(X )>0 A) ifPand probability mass function p p(x) > 0x X xX p(x)f (x) > 0 f A.Proof. clearly suffices prove necessity. Since 0/ posi(G(X )>0 A), inferversion separating hyperplane theorem (Walley, 1991, Appendix E.1)linear functional G(X )(x X)(I{x} ) > 0 (f A)(f ) > 0.P(X ) = xX (I{x} ) > 0, let p(x) := (I{x} )/(XP ) > 0 x X,p probability mass function X (f )/(X ) = xX p(x)f (x) > 0f A.second lemma implies consider coherent set desirable gamblesinclude gamble opposite, always find coherent supersetincludes one two:Lemma 3. Consider convex cone gambles X max f > 0 f A.Consider non-zero gamble g X. g/ 0/ posi(A {g}).Proof. Consider non-zero gamble g/ A, assume ex absurdo 0 posi(A {g}).follows assumptions f > 0 0 =f + (g). Hence g A, contradiction.2.5 Maximal Coherent Sets Desirable Gambleselement D(X) called maximal strictly included elementD(X), words, adding gamble f makes sure longer extendset {f } set still coherent:(D D(X))(D = ).5. equivalent judging gamble IAp 1 desirable, case obtaincoherent set desirable gambles; gamble IAp 1 almost-desirable sense Walley(1991, Section 3.7.3).607fiDe Cooman & MirandaM(X ) denotes set maximal elements D(X).following proposition provides useful characterisation maximal elements.Proposition 4 (De Cooman & Quaeghebeur, 2012). Consider D(X ).maximal coherent set desirable gambles(f G(X )6=0 )(f/ f D).case classical propositional logic (see, instance, De Cooman, 2005),coherence inference described completely terms maximal elements.essence following important result, continues hold infinite X,constructive proof given case X finite, based argumentsuggested Couso Moral (2011).Theorem 5 (Couso & Moral, 2011; De Cooman & Quaeghebeur, 2012). setavoids non-positivity maximal M(X ) M.Moreover\E(A) =m(A),letm(A) := {M M(X ) : M} .(1)shows (coherent) sets desirable gambles instances so-called strongbelief structures described studied detail De Cooman (2005), strongbelief structures classical propositional logic embedded. guarantees amongstthings AGM-like (De Cooman, 2005; Gardenfors, 1988) account beliefexpansion revision possible objects.2.6 Coherent Lower Previsionsconclude section shedding light connection coherent setsdesirable gambles, coherent lower previsions, probabilities.Given coherent set desirable gambles D, functional P defined G(X )P (f ) := sup { : f D}(2)coherent lower prevision (Walley, 1991, Thm. 3.8.1), is, corresponds takinglower envelope expectations associated set finitely additive probabilities.conjugate upper prevision P defined P (f ) := inf { : f D} = P (f ).Many different coherent sets desirable gambles induce coherent lower prevision P , typically differ boundaries. sense, saysets desirable gambles informative coherent lower previsions: althoughgamble positive lower prevision always desirable one negative lowerprevision desirable, lower prevision generally provide informationdesirability gamble whose lower prevision equal zero. reasonneed consider sets desirable gambles want additional information.see clearly, consider following adaptation example Moral (2005,Example 1):608fiIrrelevant Independent Natural Extension Sets Desirable GamblesExample 1. Consider X 1 = X 2 = {a, b}, let P coherent lower previsionG(X 1 X 2 ) givenf (b, a) + f (b, b) f (b, a) + 3f (b, b):=P (f ),mingambles f X 1 X 2 .24coherent lower prevision induced following coherent sets desirablegambles means Eq. (2)::= {f : f > 0 P (f ) > 0}:= {f : f (b, a) = f (b, b) = 0 f (a, a) + f (a, b) > 0} .However, two sets encode different preferences, gamble g given g(a, a) = 2,g(a, b) = 1, g(b, a) = g(b, b) = 0, P (g) = 0, considered desirableD. coherent lower previsions able distinguish preferencesweak preferences, sets desirable gambles can. shall see Section 5differences come play considering conditioning.smallest set desirable gambles induces given coherent lower previsionanopen coneis called associated set strictly desirable gambles, given:= {f G(X ) : f > 0 P (f ) > 0} .(3)instance case set Example 1. Sets strictly desirable gamblesone-to-one relationship coherent lower previsions, sufferproblems conditioning sets (lower) probability zero, senseconditional models determine caseby means Eqs. (8) (10)Section 5are always vacuous (Zaffalon & Miranda, 2012; Quaeghebeur, 2012b).one reasons paper considering general model coherentsets (not necessarily strictly) desirable gambles. additional discussionsets desirable gambles informative coherent lower previsions, referWalley (2000) Quaeghebeur (2012b).lower upper prevision coincide gambles, functionalP defined G(X ) P(f ) := P (f ) = P (f ) f G(X ) linear prevision, i.e.,corresponds expectation operator respect finitely additive probability.happens particular maximal coherent set desirable gambles M:P (f ) = sup { : f M} = inf { : f/ M} = inf { : f M} = P (f );see second equality holds, observe f also f< , consequence set { : f M} interval unboundedbelow. third equality follows Proposition 4. Thus, boundary behaviour,precise probability models correspond maximal coherent sets desirable gambles; seework Couso Moral (2011, Section 5), Miranda Zaffalon (2010, Proposition 6)Williams (1975a) information. Moreover, coherent lower prevision Plower envelope credal set M(P ) induces, givenM(P ) := {P linear prevision : (f G(X ))P(f ) P (f )} .conclude point least basic representational aspects, modelsinvolving coherent sets desirable gambles generalise classical propositional logicprecise probability finitary approach championed de Finetti (1937, 1975).609fiDe Cooman & Miranda3. Basic Notationhighlighted basic facts general approach uncertainty modelling, ready turn independence. order talk this, needable deal models involving one variable. present section,introduce notational devices use make discussion elegant possible.on, consider number variables Xn , n N , taking valuesrespective finite sets X n . N finite non-empty index set.6every subset R N , denote XR tuple variables (with one component:= rR X r . Cartesianr R) takes values Cartesianproduct X Rproduct set maps xR R rR X r xr := xR (r) X rr R. Elements X R generically denoted xR zR , corresponding componentsxr := xR (r) zr := zR (r), r R.assume variables Xn logically independent, meanssubset R N , XR may assume values X R .denote G(X R ) set gambles defined X R . frequently resortsimplifying device identifying gamble X R gamble X N , namelycylindrical extension. give example, K G(X N ), trick allows us considerKG(X R ) set gambles K depend variable XR . anotherexample, device allows us identify gambles I{xR } I{xR }X N\R , thereforealso events {xR } {xR } X N \R . generally, event X R ,identify gambles IA IAX N\R , therefore also events X N \R .must paySparticular attention case R = . definition, X setmaps r X r = . contains one element x : empty map. meansuncertainty value variable X : assume onevalue (the empty map). Moreover IX = I{x } = 1. Also, identify G(X )set real numbers R. one coherent set desirable gambles X : setR>0 positive real numbers.One final notational convention handy used throughout: nindex, identify n {n}. take X {n} , G(X {n} ), D{n} also referX n , G(X n ) Dn , respectively. trick, amongst things, allows us considertwo disjoint index sets N1 N2 , consider sets constitute indexthemselves, leading new index set {N1 , N2 }. variables XN1 XN2combined joint variable X{N1 ,N2 } , course identified variableXN1 N2 : joint variables considered single variables, combined constitutenew joint variables.4. Marginalisation Cylindrical ExtensionSuppose set DN G(X N ) desirable gambles modelling subjects information uncertain variable XN .6. assumption finiteness spaces X n essential proofs results establishedlater on, Theorem 13 Proposition 18. also allows us simplify expressionssets gambles derived assumption epistemic irrelevance independence,derive instance Lemma 11 Proposition 14.610fiIrrelevant Independent Natural Extension Sets Desirable Gamblesinterested modelling information variable XO ,subset N . done using set desirable gambles belong DNdepend variable XO :margO (DN ) := {g G(X ) : g DN } = DN G(X ).(4)Observe DN coherent obtain marg (DN ) = G(X )>0 , identifiedset positive real numbers R>0 . Also, O1 O2 N :margO1 (margO2 (DN )) = g G(X O1 ) : g margO2 (DN )= {g G(X O1 ) : g G(X O2 ) DN }= {g G(X O1 ) : g DN } = margO1 (DN ).(5)Coherence trivially preserved marginalisation.Proposition 6. Let DN set desirable gambles X N , consider subsetN .(i) DN avoids non-positivity, margO (DN ).(ii) DN coherent, margO (DN ) coherent set desirable gambles X .look kind inverse operation marginalisation. Supposecoherent set desirable gambles G(X ) modelling subjects informationuncertain variable XO , want extend coherent set desirable gamblesX N , representing information. looking coherent set desirablegambles DN G(X N ) margO (DN ) = small possible:conservative coherent set desirable gambles X N marginalises .turns set always exists difficult find.Proposition 7. Let subset N let D(X ). conservative(smallest) coherent set desirable gambles X N marginalises givenextN (DO ) := posi(G(X N )>0 ).(6)called cylindrical extension set desirable gambles X N , clearlysatisfiesmargO (extN (DO )) = .(7)extension called weak extension Moral (2005, Section 2.1).7Proof. clear coherence requirements Eq. (4) coherent setdesirable gambles marginalises must include G(X N )>0 , thereforealso posi(G(X N )>0 ) = extN (DO ). therefore suffices prove posi(G(X N )>0 )coherent, marginalises .7. main difference result Morals excluding zero gamblecoherent set desirable gambles, Moral including it.611fiDe Cooman & Mirandaprove coherence, suffices prove avoids non-positivity, Theorem 1.obvious coherent set desirable gambles X .left prove margO (extN (DO )) = . Since g obviousg extN (DO ) g G(X ), see immediately margO (extN (DO )),concentrate proving converse inclusion. Consider f margO (extN (DO )),meaning f G(X ) f extN (DO ). latter means g ,h G(X N )>0 , non-negative max{, } > 0 f = g + h.Since need prove f , assume without loss generality > 0.h = (f g)/ G(X ) therefore also h G(X )>0 , whence indeed f ,coherence .5. ConditioningSuppose set DN G(X N ) desirable gambles modelling subjects information uncertain variable XN .Consider subset N , assume want update model DNinformation XI = xI . leads updated, conditioned, set desirable gambles:DN |xI := f G(X N ) : f > 0 I{xI } f DN .(8)technical reasons, mainly order streamline proofs much possible,also allow admittedly pathological case = . Since I{x } = 1, amountsconditioning all.Eq. (8) introduces conditioning operator | essentially used Walley (2000)Moral (2005). prefer slightly modified version , introduced De CoomanQuaeghebeur (2012). Since I{xI } f = I{xI } f (xI , ), characterise updated modelDN |xI setDN xI := g G(X N \I ) : I{xI } g DN G(X N \I ),specific sense g G(X N \I ):g DN xI I{xI } g DN I{xI } g DN |xI ,(9)f G(X N ):f DN |xI (f > 0 f (xI , ) DN xI ).equation shows, one-to-one correspondence DN |xIDN xI . prefer second operator find intuitive conditioninggamble xI X produces gamble depends remaining N \variables. useful instance combining conditional sets gambles,Proposition 24 later on.immediate prove conditioning preserves coherence:Proposition 8. Let DN coherent set desirable gambles X N , considersubset N . DN xI coherent set desirable gambles X N \I .612fiIrrelevant Independent Natural Extension Sets Desirable Gamblesorder marginalisation conditioning reversed, conditions:Proposition 9. Let DN coherent set desirable gambles X N , considerdisjoint subsets N . xI X :margO (DN xI ) = margIO (DN )xI .Proof. Consider h G(X N ) observe following chain equivalences:h margO (DN xI ) h G(X ) h DN xIh G(X ) I{xI } h DNh G(X ) I{xI } h margIO (DN )h G(X ) h margIO (DN )xIh margIO (DN )xI .end section, let us briefly look consequences type updatingcoherent lower previsions associated coherent sets desirable gambles.allow us back claim standard probability theory recoveredspecial case theory coherent sets desirable gambles, also allows us deriveBayess Rule.Let us denote P N lower prevision induced joint model DN , P (|xI )conditional lower prevision G(X N \I ) induced updated set DN xI .gamble g X N \I :P (g|xI ) = sup { : g DN xI } = sup : I{xI } [g ] DN .(10)allows us clarify sets desirable gambles indeed informative coherent lower previsions, using example Moral (2005):Example 2. Consider lower prevision P coherent sets desirable gambles Example 1. Consider event X1 = a, (upper) probability zero . conditioning event, obtain two differentupdated sets: one hand,D(X1 = a) = {g G(X 2 ) : g > 0} = G(X 2 )>0whereas(X1 = a) = {g G(X 2 ) : g(a) + g(b) > 0} .means sets represent different information conditioning eventprobability zero X1 = a. Indeed, apply Eq. (10) see first one inducesvacuous lower prevision P (g|X1 = a) = min{g(a), g(b)} gamble g X 2 ,second one induces uniform linear prevision: P (g|X1 = a) = g(a)+g(b).2lower previsions P N P (|xI ) satisfy condition called GeneralisedBayes Rule (this follows Williams, 1975b Miranda & Zaffalon, 2010, Thm. 8):P N (I{xI } [g P (g|xI )]) = 0.613(11)fiDe Cooman & Mirandarefer work Walley (1991, 2000) information rule. leadsBayess Rule special case joint model DN induces precise prevision PN .Indeed, let g = I{xN\I } generically denote probability mass p, inferEq. (11) linearity PN PN (I{xI } I{xN\I } ) = P (I{xN\I } |xI )PN (I{xI } ),words p(xN ) = p(xN \I |xI )p(xI ). See Section 2.6 details relationshipcoherent lower (and linear) previsions sets desirable gambles.Remark 1. lower prevision P also one-to-one correspondence so-calledset almost desirable gambles, namely:= {f : P (f ) 0} .set corresponds uniform closure coherent set desirable gamblesinduces P means Eq. (2). Although sets almost-desirable gambles interesting,allow us work non-strict preference relations (Walley, 1991, Section 3.7.6),opted considering general model coherent sets desirable gambles two(admittedly related) reasons. Like sets strictly desirable gambles, sets almost-desirablegambles permit elicit boundary behaviour, may important updating, discussed Example 2. Moreover, conditioning set almost desirablegambles may produce incoherent models sets probability zero involved (Miranda& Zaffalon, 2010, Proposition 5 Example 2): take instance X 1 = X 2 = {0, 1}linear prevision P mass function p(0, ) = 0 p(, 1) = 12 , associatedset almost desirable gambles is:= {f : f (1, 0) + f (1, 1) 0} ,use Eq. (8) condition set X1 = 0, get G(X 1,2 ),incoherent set. means sets almost desirable gambles, generallysets gambles associated non-strict preferences, conditioning operation mustmodified order avoid producing incoherences. turns unique waythis; see work Hermans (2012) details.6. Irrelevant Natural Extensionready look simplest type irrelevance judgement.Definition 2. Consider two disjoint subsets N . say XI epistemicallyirrelevant XO learning value XI influence change subjectsbeliefs XO .set DN desirable gambles X N capture type epistemic irrelevance? Observing XI = xI turns DN updated set DN xI desirable gamblesX N \I , come following definition:Definition 3. set DN desirable gambles X N said satisfy epistemic irrelevanceXI XOmargO (DN xI ) = margO (DN ) xI X .614(12)fiIrrelevant Independent Natural Extension Sets Desirable Gamblesbefore, technical reasons also allow empty. cleardefinition variable X , whose constant value certain,epistemically irrelevant variable XO . Similarly, see variable XIepistemically irrelevant variable X . seems accordance intuition.refer Levi (1980) Walley (1982, 1991) related notions terms coherentlower previsions credal sets.epistemic irrelevance condition reformulated trivially interestingslightly different manner.Proposition 10. Let DN coherent set desirable gambles X N , letdisjoint subsets N . following statements equivalent:(i) margO (DN xI ) = margO (DN ) xI X ;(ii) f G(X ) xI X : f DN I{xI } f DN .Proof. suffices take account f margO (DN ) f DNf G(X ), f margO (DN xI ) f G(X ) I{xI } f DN .Irrelevance assessments useful constructing sets desirable gamblesones. Suppose coherent set desirable gambles X , assessment XI epistemically irrelevant XO , disjoint index sets.combine structural irrelevance assessment coherent setdesirable gambles X IO , generally, X N , N O? seedone way conservative possible, introduce following setsAirrI{xI } g : g xI X(13)IO := posi= {h G(X IO )6=0 : (xI X )h(xI , ) {0}} .(14)irrClearly, quite important streamlining proofs, Airr= AI =G(X )>0 . intuition behind Eq. (13) consider cylindrical extensionsgambles space X IO , take natural extension resulting set.alternative expression (14) shows equivalent selecting gamblefinite number xI X , derive gamble X IO .Let us give two important properties sets:Lemma 11. Consider disjoint subsets N , coherent set desirablegambles X . AirrIO coherent set desirable gambles X IO .Proof.PD1. Assume ex absurdo n > 0, real k > 0 fk AirrIOnk=1 k fk = 0. follows assumptions{1,... , n}PnxI X f (xI , ) 6= 0. implies sum k=1 k fk (xI , ) = 0gambles k fk (xI , ) zero. Since non-zero ones belong , contradictscoherence .D2. Consider h G(X IO )>0 . clearly h(xI , ) 0 therefore h(xI , ){0} xI X . Since h 6= 0, follows indeed h AirrIO .D3. Trivial, using posi(posi(D)) = posi(D) set desirable gambles D.615fiDe Cooman & MirandaLemma 12. Consider disjoint subsets N , coherent set desirablegambles X . margO (AirrIO ) = .Proof. obvious Eq. (14) indeed:irrmargO (AirrIO ) = AIO G(X ) = {h G(X )6=0 : (xI X )h {0}}= {h G(X )6=0 : h {0}} = .Theorem 13. Consider disjoint subsets N , coherent set desirable gambles X . smallest coherent set desirable gambles X Nmarginalises satisfies epistemic irrelevance condition (12) XI XOirrgiven extN (AirrIO ) = posi(G(X N )>0 AIO ).Proof. Consider coherent set DN desirable gambles X N marginalisessatisfies irrelevance condition (12). implies margO (DN xI ) =xI X , g DN xI , therefore I{xI } g DN g , Eq. (9). inferirrcoherence AirrIO DN , therefore also posi(G(X N )>0 AIO ) DN .consequence, suffices prove (i) extN (AirrIO ) coherent, (ii) marginalises ,(iii) satisfies epistemic irrelevance condition (12). setdo.(i). Lemma 11, AirrIO coherent set desirable gambles X IO , Proposition 7irrimplies posi(G(X N )>0 AirrIO ) = extN (AIO ) coherent set desirable gamblesX N .(ii). Marginalisation leads to:irrirrmargO (extN (AirrIO )) = margO (margIO (extN (AIO ))) = margO (AIO ) = ,first equality follows Eq. (5), second Eq. (7), thirdLemma 12.(iii). follows Proposition 9 Eq. (7)irrirrmargO (extN (AirrIO )xI ) = margIO (extN (AIO ))xI = AIO xI ,shown (ii) margO (extN (AirrIO )) = , proving equalityirr )) amounts proving Airr x = .margO (extN (Airr)x)=marg(ext(ANIOIOIOirr x , concentrateobvious definition AirrIOIOirrconverse inclusion. Consider h AirrIO xI ; I{xI } h AIO , inferEq. (14) particular h {0}. since AirrIO coherent Lemma 11, seeh 6= 0 therefore indeed h .Theorem 13 mentioned briefly, hint proof, Moral (2005, Section 2.4).believe result trivial therefore decided include versionproof here. notion epistemic irrelevance called weak epistemic irrelevanceMoral. version epistemic irrelevance requires addition DNequal irrelevant natural extension , therefore smallest modelsatisfies (weak) epistemic irrelevance condition (12). feel comfortable616fiIrrelevant Independent Natural Extension Sets Desirable Gamblesreasons so, decided follow lead this. main reasontied philosophy behind partial assessments (or probabilityspecifications). assessment, local (e.g. stating gambles setdesirable) structural (e.g. imposing symmetry irrelevance), servesrestrict possible models, stage conservative (smallest possible)model considered one used, possibly refined additionalassessments. calling model irrelevant smallest weakly irrelevant modelwould, believe, conflict approach: larger models obtained later adding, say,symmetry assessments, would longer deserve called irrelevant (but wouldstill satisfy relevant conditions).infer Theorem 13 Eq. (13) extreme rays irrelevant naturalextension form I{xI } g, g extreme ray , representingfinding extension computer computational complexity linearnumber extreme rays linear number elements product set Xtherefore essentially exponential number |I| irrelevant variables Xi , I.generally, also case fairly general situation generatedfinite number so-called generalised extreme rays, described detail CousoMoral (2011, Section 4) Quaeghebeur (2012a, Section 3).7. Independent Natural Extensionturn independence assessments, constitute symmetrisation irrelevanceassessments.Definition 4. say variables Xn , n N epistemically independentlearning values number influence change beliefsremaining ones: two disjoint subsets N , XI epistemically irrelevantXO .set DN desirable gambles X N capture type epistemic independence?Definition 5. coherent set DN desirable gambles X N called independentmargO (DN xI ) = margO (DN ) disjoint subsets N , xI X .definition, allow empty too, leadsubstantive requirement, condition margO (DN xI ) = margO (DN ) triviallysatisfied empty.Independent sets interesting factorisation property, means producttwo desirable gambles depend different variables desirable, provided one gambles positive; refer work De Cooman et al. (2011) another paper factorisation considered somewhat unusual form. Factorisationfollows characterisation epistemic irrelevance given Proposition 10properties coherence.617fiDe Cooman & MirandaProposition 14 (Factorisation independent sets). Let DN independent coherent set desirable gambles X N . disjoint subsets Nf G(X ):f DN (g G(X )>0 )(f g DN ).(15)Proof. Fix arbitrary disjoint subsets N f G(X ); showEq. (15) holds. part trivial. part, assume Pf DN considerg G(X).showfg.Sinceg=NxI X I{xI } g(xI ), seeP >0f g = xI X g(xI )I{xI } f . since f margO (DN ), infer independenceDN Proposition 10 f DN xI therefore I{xI } f DN xI X .conclude f g positive linear combination elements I{xI } f DN , thereforebelongs DN coherence.Independence assessments useful constructing joint sets desirable gamblesmarginal ones. Suppose coherent sets Dn desirable gambles X n , n Nassessment variables Xn , n N epistemically independent.combine Dn structural independence assessment coherent setdesirable gambles X N way conservative possible? call independentproduct Dn independent DN D(X N ) marginalises Dn n N ,means looking smallest independent product.on, going prove smallest independent product always exists.elegantly, however, need preparatory work involvingparticular sets desirable gambles constructed Dn . Consider,special case Eq. (14), subset N N \ I:I{xI } g : g xI X(16)AirrI{o} := posi= h G(X I{o} )6=0 : (xI X )h(xI , ) {0} ,(17)use sets construct following set gambles X N :[[irrnN Dn := posi G(X N )>0Airr=posiN \{n}{n}N \{n}{n} ,nN(18)nNsecond equality holds set G(X N )>0 included AirrN \{n}{n} everyn N . set nN Dn gathers subsets G(X N ) derive different Dnmeans assumption epistemic irrelevance, considers natural extensionunion, minimal coherent superset (we shall show indeed coherentProposition 15 below). Observe that, quite trivially, Airr{n}\{n}{n} = Dn thereforem{n} Dm = Dn . prove number important properties nN Dn .Proposition 15 (Coherence). Let Dn coherent sets desirable gambles X n , nN . nN Dn coherent set desirable gambles X N .Proof. Let, ease notation, := nN AirrN \{n}{n} . follows Theorem 1prove avoids non-positivity. consider f posi(AN ),assume ex absurdo f 0. n 0 fn AirrN \{n}{n}618fiIrrelevant Independent Natural Extension Sets Desirable GamblesPirrf =nN n fn maxnN n > 0 [recall \{n}{n} convex cones,Lemma 11]. Fix arbitrary N . Let:= fm (xN \{m} , ) : xN \{m} X N \{m} , fm (xN \{m} , ) 6= 0 ,follows Eq. (17)finite non-empty subset Dm , coherenceDm , Theorem 1 Lemma 2 imply mass function pm Xexpectation operator Em (xm X )pm (xm ) > 0(xN \{m} X N \{m} )(fm (xN \{m} , ) 6= 0 Em (fm (xN \{m} , )) > 0).define gamble gN \{m} X N \{m} lettinggN \{m} (xN \{m} ) := Em (fm (xN \{m} , ))xN \{m} X N \{m} , gN \{m} > 0.Since weQcan N , define mass function pN X N lettingpN (xN ) := mN pm (xm ) > 0 xN X N . corresponding expectation operatorEN course product operator marginalsEm . thenPit followsPreasoning assumptions EN (f ) = mN EN (fm ) = mN EN (gm ) >0, whereas f 0 leads us conclude EN (f ) 0, contradiction.Lemma 16. Consider disjoint subsets I, R N \ (I R). f (xR , )irrAirrI{o} {0} f AIR{o} xR X R .:= f (xR , ) X I{o} .Proof. Fix f AirrIR{o} xR X R consider gamble gfollows assumptions xI X :g(xI , ) = f (xR , xI , ) {0},whence indeed g AirrI{o} {0}.Proposition 17 (Marginalisation). Consider coherent marginal sets desirable gambles Dn n N . Let R subset N , margR (nN Dn ) = rR Dr .Proof. Since interpreting gambles X R special gambles X N , clearirrEq. (17) r R, AirrR\{r}{r} \{r}{r} . Eqs. (6) (18) tell usextN (rR Dr ) nN Dn . invoke Eq. (7), leadsrR Dr = margR (extN (rR Dr )) margR (nN Dn ),concentrate converse inequality.Consider therefore f margR (nN Dn ) = (nN Dn ) G(X R ), assume exabsurdo f/ rR Dr .follows coherence nN Dn f 6= 0 [see Proposition 15]. Since fnN Dn , N , fs AirrN \{s}{s} , g G(X N ) g 0Pf = g + sS fs . Clearly \ R 6= , \ R = would imply that, xN \R619fiDe Cooman & MirandaPelement X N \R , f = f (xN \R , ) = g(xN \R , ) + sSR fs (xN \R , ) rR Dr , sinceinfer Lemma 16 fs (xN \R , ) AirrR\{s}{s} {0} R.follows coherence rR Dr [Proposition 15], f/ rR Dr Lemma 30/ posi({f } rR Dr ). Let, ease notation,SR := fs (zN \R , ) : R, zN \R X N \R , fs (zN \R , ) 6= 0 .SR clearly finite subset rR Dr [to see this, use similar argument above,involving Lemma 16], infer Lemma 2 mass function pRX R associated expectation operator ER(xR X R )pR (xR ) > 0(s R)(zN \R X N \R )ER (fs (zN \R , )) 0ER (f ) < 0.PPinfer f = f (zN \R , ) = g(zN \R , ) + sSR fs (zN \R , ) + sS\R fs (zN \R , )zN \R X N \R :0 > ER (f ) ER (g(zN \R , ))XER (fs (zN \R , ))sSR=XsS\RER (fs (zN \R , )) =XXpR (xR )fs (zN \R , xR ).sS\R xR X Rgambles fs (, xR ) X N \R [where xR X R \ R] clearly zero.non-zero ones belong sN \R Ds N \ R xR X R , Lemma 16,coherence set desirable gamblessN \R Ds [Proposition 15] guaranteesPPpositive linear combination h := sS\R xR X R pR (xR )fs (, xR ) also belongssN \R Ds . contradicts h 0. Hence indeed f rR Dr .Proposition 18 (Conditioning). Consider coherent marginal sets desirable gamblesDn n N , define nN Dn means Eq. (18). nN Dn independent:disjoint subsets N , xI X ,margO (nN Dn xI ) = margO (nN Dn ) = oO .could probably proved indirectly using semi-graphoid properties conditional epistemic irrelevance, proved Moral (2005); appears need reverse weakunion, reverse decomposition, contraction. give direct proof. Proposition 17also seen special case present result = .Proof. Fix arbitrary disjoint subsets N , arbitrary xI X . secondequality follows Proposition 17, concentrate proving margO (nN Dn xI )coincides oO . proof similar Proposition 17.first show oO nN Dn xI . Consider gamble f oO ,show I{xI } f nN Dn . assumption, non-negative realsP, gambles fo AirrgG(X)f=g+>0oO foO\{o}{o}620fiIrrelevant Independent Natural Extension Sets Desirable Gamblesmax{, maxoO } > 0. Fix let fo := I{xI } fo G(X N ). followsdefinition AirrO\{o}{o} fo (zN \{o} , ) = I{xI } (zI )fo (zO\{o} , ) {0}irrzN \{o} X N \{o} . Since fo 6= 0, definition AirrN \{o}{o} tells us fo \{o}{o} .:= I{xI } g G(X N ), g > 0. follows Eq. (18)Similarly, let g Pindeed I{xI } f = g + oO fo nN Dn .turn converse inclusion, nN Dn xI oO . Consider gamblef G(X ) I{xI } f belongs nN Dn assume ex absurdo f/ oO .Let, sake notational simplicity, C := N \ (I O).follows coherence nN Dn f 6= 0 [see Proposition 15]. Since I{xI } fnN Dn , N , fs AirrN \{s}{s} , g G(X N ) g 0PI{xI } f = g + sS fs . Clearly \ OP6= , \ = would imply that, xCelement X C , f = g(xI , xC , ) + sSO fs (xI , xC , ) oO , since Lemma 16 showsfs (xI , xC , ) AirrO\{s}{s} O.follows coherence oO [Proposition 15], f/ oO Lemma 30/ posi({f } oO ). Let, ease notation,:= {fs (xI , zC , ) : O, zC X C , fs (xI , zC , ) 6= 0} .clearly finite subset oO [to see this, use similar argument above,involving Lemma 16], infer Lemma 2 mass function pOX associated expectation operator EO(xO X )pO (xO ) > 0(s O)(zC X C )EO (fs (xI , zC , )) 0EO (f ) < 0.Since f = g(xI , zC , )+see that:PsSO fs (xI , zC , )+0 > EO (f ) EO (g(xI , zC , ))PsS\OXfs (xI , zC , ) choice zC X C ,EO (fs (xI , zC , ))sSO=XXEO (fs (xI , zC , )) =XpO (xO )fs (xI , zC , xO )).sS\O xO XsS\OSimilarly,zC PX C zI X \ {xI } infer 0 = g(zI , zC , ) +PsSO fs (zI , zC , ) +sS\O fs (zI , zC , ) that:0 EO (g(zI , zC , ))XEO (fs (zI , zC , ))sSO=XEO (fs (zI , zC , )) =XXpO (xO )fs (zI , zC , xO )).sS\O xO XsS\OHenceh :=XXpO (xO )fs (, , xO ) 0.sS\O xO X621fiDe Cooman & Mirandagambles fs (, , xO ) X IC [where xO X \ O] clearlyzero. non-zero ones belong sIC Ds , Lemma 16. coherenceset desirable gambles sIC Ds [Proposition 15] guarantees positive linearcombination h element cC Dc h 0, contradiction. Hence indeedf oO .Theorem 19 (Independent natural extension). Consider coherent sets Dn desirable gambles X n , n N . nN Dn smallest coherent set desirablegambles X N independent product coherent sets desirable gambles Dn ,n N.call nN Dn independent natural extension marginals Dn .Proof. follows Propositions 15, 17 18 nN Dn independent productDN Dn . prove smallest one, consider independent product DNDn . Fix n N . consider xN \{n} X N \{n} , margn (DN xN \{n} ) = Dn ,assumption. therefore consider g Dn , turn implies g DN xN \{n} ,therefore I{xN\{n} } g DN , Eq. (9). infer coherence AirrN \{n}{n} DN ,therefore also nN Dn DN .One useful properties independent natural extension, associativity: allows us construct extension modular fashion.Theorem 20 (Associativity independent natural extension). Let N1 N2disjoint non-empty index sets, consider Dnk D(X nk ), nk Nk , k = 1, 2. givenDN1 := n1 N1 Dn1 DN2 := n2 N2 Dn2 , holdsDN1 DN2 = nN1 N2 Dn .Proof. first prove DN1 DN2 nN1 N2 Dn . Fix gamble h Airr{N1 }{N2 }xN1 X N1 , h(xN1 , ) DN2 {0} Eq. (17). follows Eq. (18)gambles hnxN2 AirrN2 \{n2 }{n2 } {0} n2 N21h(xN1 , )XhnxN2 .1n2 N2Define, n2 N2 , gamble gn2 X N letting gn2 (xN \{n2 } , ) := hnxN2 (xN2 \{n2 } , )1xN X N . follows Eq. (17) gn2 (xN \{n2 } , ) Dn2 {0}xN X N , therefore gn2 AirrN \{n2 }{n2 } {0}. Moreover,h=XI{xN1 } h(xN1 , )XI{xN1 }xN1 X N1xN1 X N1Xn2 N2hnxN2 =1XXn2 N2 xN1 X N1I{xN1 } hnxN2 =1Xgn2 ,n2 N2therefore follows Eq. (18) h nN1 N2 Dn , since clearly h 6= 0Eq. (17). conclude Airr{N1 }{N2 } nN1 N2 Dn . Similarly, prove622fiIrrelevant Independent Natural Extension Sets Desirable Gamblesinclusion Airr{N2 }{N1 } nN1 N2 Dn , therefore also DN1 DN2 nN1 N2 Dn ,Eq. (18).Next, prove converse inclusion nN1 N2 Dn DN1 DN2 . Consider gambleh nN1 N2 Dn , Eq. (18) hn AirrN1 N2 \{n}{n} {0} n N1 N2XXXhhn = h1 + h2 , h1 :=hn1 h2 :=hn2 .n1 N1nNn2 N2Fix xN1 X N1 . n2 N2 , hn2 AirrN1 N2 \{n2 }{n2 } {0} impliesirrhn2 (xN1 , ) AN2 \{n2 }{n2 } {0} Lemma 16. Hence h2 (xN1 , ) DN2 {0} Eq. (18),irrtherefore h2 Airr{N1 }{N2 } {0} Eq. (17). Similarly, h1 A{N2 }{N1 } {0},therefore h DN1 DN2 Eq. (18), since clearly h 6= 0.conclude section, establish connection independent natural extension sets desirable gambles eponymous notion coherent lower previsions,studied detail De Cooman et al. (2011). Given coherent lower previsions P n G(X n ),n N , independent natural extension coherent lower prevision givenXE N (f ) := supmin f (zN )[hn (zN ) P n (hn (, zN \{n} ))](19)hn G(X N ) zN X NnNnNgambles f X N . point-wise smallest (most conservative) joint lowerprevision satisfies property coherence Walley (1991, ch. 7) marginalsP n given assessment epistemic independence variables Xn , n N .correspondence coherent lower previsions sets desirable gamblesmentioned Section 2.6; show next correspondencemarginals, also holds associated independent natural extensions.Theorem 21. Let Dn coherent sets desirable gambles X n n N , letnN Dn independent natural extension. Consider coherent lower previsionsP n G(X n ) given P n (fn ) := sup { R : fn Dn } fn G(X n ).independent natural extension E N marginal lower previsions P n , n N satisfiesE N (f ) = sup { R : f nN Dn } f G(X N ).Proof. Fix gamble f G(X N ). First, consider real number < E N (f ),Pfollows Eq. (19) > 0 hn G(X N ), n N fnN gn , defined gambles gn X N gn (zN ) := hn (zN )P n (hn (zN \{n} , ))+zN X N . follows definition P ngn (zN \{n} , ) = hn (zN \{n} , ) P n (hn (zN \{n} , )) + Dn zN \{n} X N \{n} .Since clearly gn 6= 0, Eq. (17) tells us gn AirrN \{n}{n} , inferPEq. (18) nN gn nN Dn , therefore also f nN Dn . guaranteesE N (f ) sup { R : f nN Dn }.623fiDe Cooman & Mirandaprove converse inequality, consider real number f nN Dn .infer using Eq. (18) gambles hn AirrN \{n}{n} , n N fPnN hn . n N zN \{n} X N \{n} , follows Eq. (17) hn (zN \{n} , )Dn {0}, therefore P n (hn (zN \{n} , )) 0, whenceXnNXhn (zN ) P n (hn (zN \{n} , ))hn (zN ) f (zN ) .nNinfer Eq. (19) E N (f ) find indeed also E N (f )sup { R : f nN Dn }.similar way irrelevant natural extension, infer Eqs. (16) (18)computational complexity finding representing independent natural extension number marginal models Dn linear number extreme raysDn , linear number elements sets X N \{n} therefore essentially exponential number |N | independent variables Xn , n N . Similar results holdgeneral case marginal sets desirable gambles characterisedusing finite number generalised extreme rays, described Couso Moral (2011)Quaeghebeur (2012a).8. Maximal Coherent Sets Desirable Gambles Strong Productsseen collection Dn , n N marginal coherent sets desirable gambles, always smallest independent product, called independentnatural extension nN Dn . proceeded way wayexcluding may other, larger, independent products. Indeed, showsection case. Using notions independent natural extension maximal coherent sets desirable gambles, consistently define specific independentproduct typically strictly includes independent natural extension. callstrong product, close spirit strong product used coherent lowerprevision theory (Couso, Moral, & Walley, 2000; Cozman, 2000, 2005; De Cooman et al.,2011), shall see Theorem 28.8.1 Independent Products Maximal Coherent Sets Desirable Gamblesbegin mentioning number interesting facts maximal coherent sets desirable gambles, independent products. following result already (essentially)proved Couso Moral (2011): updating coherent set desirable gambles preservesmaximality.Proposition 22. Let MN M(X N ), consider disjoint subsets N .margO (MN xI ) M(X ) xI X .Proof. Suppose xI X margO (MN xI ) maximal.means f G(X ) neither f f belong MN xI ,turn implies neither I{xI } f I{xI } f belong MN . contradictsmaximality MN .624fiIrrelevant Independent Natural Extension Sets Desirable Gambleshand, taking independent natural extension necessarily preserve maximality: Mn M(X n ) n N , necessarily holdnN Mn M(X N ), counterexample Section A.1 shows. Interestingly, example present isolated case: consider two binary variables, independent natural extension two maximal coherent sets desirable gambles never maximal,see next proposition. open problem whether negative resultextended finite set (not necessarily binary) variables.intuitive explanation result maximal sets gambleshalf-space excluding one two rays determining boundary,zero gamble desirable; apply notion independent naturalextension end missing three four parts boundary set gamblesproduct space, preventing product maximal.Proposition 23. Consider X 1 = X 2 = {0, 1}, let M1 M2 maximal coherent sets desirable gambles X 1 X 2 , respectively. independent naturalextension M1 M2 maximal coherent set desirable gambles.Proof. Let pk mass function linear prevision Pk determined Mk , k = 1, 2.deduce Theorem 21 lower prevision determined M1 M2 independent natural extension linear previsions P1 P2 , therefore equalindependent product P{1,2} linear previsions [see Proposition 25 De Coomanet al., 2011]. linear prevision G(X {1,2} ) mass function definedp{1,2} (x1 , x2 ) := p1 (x1 )p2 (x2 ) (x1 , x2 ) X {1,2} .really get proof tracks, make useful observation. maximalMk semi-plane origin excludes origin, includes boundaryone side origin, excludes boundary side. meansunique element ak X k elements fk included boundarythoseelements fk Mk Pk (fk ) zeroare positive fk (ak ) > 0. denote singleelement X k bk . words, expressMk = {fk : Pk (fk ) > 0} {fk : Pk (fk ) = 0, fk Mk },consider fk Mk Pk (fk ) = pk (ak )fk (ak ) + pk (bk )fk (bk ) = 0, fk (ak ) > 0cannot gk Mk Pk (gk ) = 0 gk (bk ) > 0: otherwise, zero gamblek)would convex combination fk gk [it would 0 = fk fgkk (b(bk ) gk ] would thusbelong Mk , contradiction coherence. Note reasoning assumeimplicitly pk (ak ) (0, 1); otherwise, instance pk (ak ) = 0, gamble fk satisfiesPk (fk ) = 0 fk (bk ) = 0, fk belong Mk fk (ak ) > 0.ready turn proof. number possibilities.First, assume pk (ak ) > 0 pk (bk ) > 0 k = 1, 2. Consider gamble hX {1,2} h(a1 , a2 ) = h(b1 , b2 ) = 0, min h < 0, max h > 0P{1,2} (h) = p1 (a1 )p2 (b2 )h(a1 , b2 ) + p1 (b1 )p2 (a2 )h(b1 , a2 ) = 0.course, always gamble, going show belongM1 M2 .625fiDe Cooman & MirandaAssume ex absurdo does, meaning h1 Airr{2}{1} h2irrA{1}{2} h h1 + h2 . definition, h1 (, x2 ) M1 {0} thereforeP1 (h1 (, x2 )) 0 x2 X 2 . Similarly, P2 (h2 (x1 , )) 0 x1 X 1 . Hence0 = P{1,2} (h) P{1,2} (h1 ) + P{1,2} (h2 ) 0, taking accountXXP{1,2} (h1 ) =p2 (x2 )P1 (h1 (, x2 )) 0 P{1,2} (h2 ) =p1 (x1 )P2 (h2 (x1 , )) 0.x2 X 2x1 X 1consequence, P{1,2} (h1 ) = P{1,2} (h2 ) = 0. turn implies P1 (h1 (, x2 )) =0 x2 X 2 P2 (h2 (x1 , )) = 0 x1 X 1 . Given observationsmade start proof, therefore come conclusion h1 (a1 , x2 ) 0x2 X 2 h2 (x1 , a2 ) 0 x1 X 1 . h(a1 , a2 ) = 0 impliesh1 (a1 , a2 ) = h2 (a1 , a2 ) = 0, turn implies h1 (b1 , a2 ) = h2 (a1 , b2 ) = 0,0 = P1 (h1 (, a2 )) = p1 (a1 )h1 (a1 , a2 ) + p1 (b1 )h1 (b1 , a2 ) 0 = P2 (h2 (a1 , )) =p2 (a1 )h1 (a1 , a2 ) + p2 (b1 )h1 (a1 , b2 ). eventually findh(b1 , a2 ) h1 (b1 , a2 ) + h2 (b1 , a2 ) 0 h(a1 , b2 ) h1 (a1 , b2 ) + h2 (a1 , b2 ) 0,contradicts min h < 0.Now, non-zero h h(a1 , a2 ) = h(b1 , b2 ) = 0 = P{1,2} (h) min h < 0max h > 0 belong M1 M2 , neither h, means M1 M2maximal.Next consider cases one marginal linear previsions degenerate.Assume instance p1 (a1 ) = 0 p2 (a2 ) (0, 1) [the cases onemarginals degenerate similar]. Consider non-zero gamble h2/ M2P2 (h2 ) = 0 [always possible]. h2 M2 follows observations madebeginning proof h2 (a2 ) < 0. consider gamble h definedh(b1 , a2 ) := h2 (a2 ) < 0,h(b1 , b2 ) := h2 (b2 ) 0,h(a1 , a2 ) = h(a1 , a2 ) := 1.follows P{1,2} (h) = P2 (h2 ) = 0. see h/ M1 M2 , assumeirrf1 Airrfhf21 + f2 . 0 = P{1,2} (h){2}{1}{1}{2}P{1,2} (f1 ) + P{1,2} (f2 ) 0 therefore 0 = P{1,2} (f1 ) = p2 (a2 )f1 (b1 , a2 ) + p2 (b2 )f1 (b1 , b2 ).hand, f1 Airr{2}{1} also implies P1 (f1 (, x2 )) 0 x2 X 2 ,therefore f1 (b1 , a2 ) 0 f1 (b1 , b2 ) 0. Hence f1 (b1 , ) = 0, therefore f2 (b1 , )h(b1 , ) = h2 since h2/ M2 , follows f2 (b1 , )/ M2 . mustdefinition f2 (b1 , ) M2 {0}, mean f2 (b1 , ) = 0, whence h2 0,contradicting h2 (a2 ) < 0. implies h cannot belong M1 M2 .Similarly, h belongs M1 M2 , must g1 Airr{2}{1} g2irrA{1}{2} h g1 + g2 . 0 = P{1,2} (h) P{1,2} (g1 ) + P{1,2} (g2 )0, whence 0 = P{1,2} (g1 ) = p2 (a2 )g1 (b1 , a2 ) + p2 (b2 )g1 (b1 , b2 ). g1 Airr{2}{1} alsoimplies P1 (g1 (, x2 )) 0 x2 X 2 , therefore g1 (b1 , a2 ) 0 g1 (b1 , b2 )0. Hence g1 (b1 , ) = 0, therefore find g1 (a1 , a2 ) 0 g1 (a1 , b2 ) 0 [if,say, g1 (a1 , a2 ) < 0 g1 (, a2 ) < 0 also g1 (b1 , a2 ) = 0, contradictsirrg1 (, a2 ) M1 {0}, consequence g1 Airr{2}{1} ]. Since, moreover, g2 A{1}{2}implies 0 P2 (g2 (a1 , )) = p2 (a2 )g2 (a1 , a2 ) + p2 (b2 )g2 (a1 , b2 ) therefore also626fiIrrelevant Independent Natural Extension Sets Desirable Gamblesg2 (a1 , a2 ) 0 g2 (a1 , b2 ) 0, follows h(a1 , a2 ) 0 h(a1 , b2 ) 0,contradicts h(a1 , a2 ) = h(a1 , b2 ) = 1 < 0. Hence, h belong M1 M2either, M1 M2 maximal.Finally, turn cases marginals degenerate. Assume instancep1 (a1 ) = p2 (a2 ) = 0 [the cases marginals degenerate, similar].Consider gamble h givenh(a1 , a2 ) = h(b1 , b2 ) = 0,h(b1 , a2 ) = 1,h(a1 , b2 ) = 1,P{1,2} (h) = p1 (b1 )p2 (b2 )h(b1 , b2 ) = 0. see h/ M1 M2 , assume ex absurdoirru1 Airruhu1 + u2 . u1 Airr2{2}{1}{1}{2}{2}{1}impliesu1 (b1 , b2 ) = P1 (u1 (, b2 )) 0 u1 (b1 , a2 ) = P1 (u1 (, a2 )) 0,similarly u2 Airr{1}{2} implies u2 (b1 , b2 ) = P2 (u2 (b1 , )) 0 u2 (a1 , b2 ) =P2 (u2 (a1 , )) 0. also follows P{1,2} (h) = 0, P{1,2} (u1 ) 0 P{1,2} (u2 ) 0u1 (b1 , b2 ) = P{1,2} (u1 ) = 0 u2 (b1 , b2 ) = P{1,2} (u2 ) = 0, consequencefind u1 (a1 , b2 ) 0 u2 (b1 , a2 ) 0 [if, say, u1 (a1 , b2 ) < 0 u1 (, b2 ) < 0also u1 (b1 , b2 ) = 0, contradicts u1 (, b2 ) M1 {0}, consequence u1 Airr{2}{1} ].consequence, 1 = h(a1 , b2 ) u1 (a1 , b2 ) + u2 (a1 , b2 ) 0, contradiction. Henceindeed, h belong M1 M2 .irrFinally, assume ex absurdo v1 Airr{2}{1} v2 A{1}{2}h v1 + v2 . v1 Airr{2}{1} impliesv1 (b1 , b2 ) = P1 (v1 (, b2 )) 0 v1 (b1 , a2 ) = P1 (v1 (, a2 )) 0,similarly v2 Airr{1}{2} implies v2 (b1 , b2 ) = P2 (v2 (b1 , )) 0 v2 (a1 , b2 ) =P2 (v2 (a1 , )) 0. also follows P{1,2} (h) = 0, P{1,2} (v1 ) 0 P{1,2} (v1 ) 0v1 (b1 , b2 ) = P{1,2} (v1 ) = 0 v2 (b1 , b2 ) = P{1,2} (v2 ) = 0, consequence findv1 (a1 , b2 ) 0 v2 (b1 , a2 ) 0 [if, say, v1 (a1 , b2 ) < 0 v1 (, b2 ) < 0 alsov1 (b1 , b2 ) = 0, contradicts v1 (, b2 ) M1 {0}, consequence v1 Airr{2}{1} ].consequence, 1 = h(b1 , a2 ) v1 (b1 , a2 ) + v2 (b1 , a2 ) 0, contradiction. showsh belong M1 M2 either, therefore set maximal.hand, Example A.2 Appendix shows independentproducts maximal coherent sets desirable gambles maximal; hence, independent natural extension maximal coherent sets independent product.Indeed, establish following result:Proposition 24. Consider maximal coherent sets desirable gambles M1 M(X 1 )M2 M(X 2 ).(i) Let D{1,2} coherent set desirable gambles X {1,2} M1 M2D{1,2} . D{1,2} independent marginals M1 M2 .(ii) consequence, maximal set gambles M{1,2} independent productmarginals M{1,2} x2 x2 X 2 M{1,2} x1x1 X 1 .627fiDe Cooman & MirandaProof. (i). every x1 X 1 M2 = (M1 M2 )x1 D{1,2} x1 ,equality follows Proposition 18. Since M2 maximal, implies M2 =D{1,2} x1 x1 X 1 , similar argument shows M1 = D{1,2} x2x2 X 2 . hand, follows Proposition 17 M2 = marg2 (M1 M2 )marg2 (D{1,2} ). Since M2 maximal, implies M2 = marg2 (D{1,2} ), similarargument shows M1 = marg1 (D{1,2} ). summary, see marg1 (D{1,2} ) =D{1,2} x2 x2 X 2 , marg2 (D{1,2} ) = D{1,2} x1 x1 X 1 , showingD{1,2} indeed independent.(ii). follows definition independent product necessaryM{1,2} x2 M{1,2} x1 x2 x1 , respectively. seealso sufficient condition M{1,2} independent product, notecase M{1,2} x1 M{1,2} x2 M{1,2} , sets M{1,2} x1 M{1,2} x2maximal, Proposition 22. hand, Proposition 17 impliesmarg1 (M{1,2} x1 M{1,2} x2 ) = M{1,2} x1 marg1 (M{1,2} ),sets equal. Similarly, deducemarg2 (M{1,2} x1 M{1,2} x2 ) = M{1,2} x2 marg2 (M{1,2} ),therefore marg1 (M{1,2} ) marg2 (M{1,2} ) M{1,2} . Invoking first partproposition, find M{1,2} independent product marginals.first part proposition provides us simple characterisationindependent products two maximal sets: simply coherent supersetsindependent natural extension; particular, means maximal supersetindependent natural extension independent product, two maximal setsalways maximal products (although differ independent naturalextension). second part implies sets conditional gambles coincideconditioning events, automatically agree marginal sets gambles,consequence set independent product.8.2 Strong Product Propertiesconsider case coherent marginal sets desirable gambles Dnn N . define strong product nN Dn set desirable gamblesproduct space X N given by:8\nN Dn :={nN Mn : Mn m(Dn ), n N } ,m(Dn ) given Eq. (1). strong product corresponds set desirablegambles determined notion independence restrictive epistemic irrelevance independence considered far: strong independence (Cousoet al., 2000; Cozman, 2012), sometimes called type-3 independence (de Campos & Moral,8. paper focusses independent natural extension, much direct behaviouraljustification, forgo discussing complexity computing strong product, which,face it, appears significantly higher independent natural extension.628fiIrrelevant Independent Natural Extension Sets Desirable Gambles1995). Strong independence means associated joint credal set convex hullset linear previsions stochastic independent products linear previsionsdominate marginals; or, equivalently, associated lower previsionlower envelope products linear previsions dominate marginals.clearer Theorem 28.maximal coherent sets desirable gambles Mn M(X n ), n N strong product independent natural extension coincide: nN Mn = nN Mn , clearlym(Mn ) = {Mn }. Taking account Proposition 23, deduce strong productmaximal coherent sets desirable gambles necessarily maximal; Example A.2Appendix shows independent products may strictly includestrong product.marginalisation properties strong product follow directlyindependent natural extension.Proposition 25 (Marginalisation). Consider coherent sets desirable gambles Dnn N . Let R subset N , margR (nN Dn ) = rR Dr .Proof. Consider f G(X R ) observe following chain equivalences:f nN Dn (Mn m(Dn ), n N )f nN Mn(Mn m(Dn ), n N )f rR Mr(Mr m(Dr ), r R)f rR Mrf rR Dr ,second equivalence follows Proposition 17.Next, show strong product coherent marginal sets desirable gamblesDn independent product marginals. order so, first establishfollowing simple yet powerful result:j, j J non-empty family independent coherent setsProposition 26. Let DNjdesirable gambles X N . intersection DN := jJ DNindependentcoherent set desirable gambles X N .Proof. Consider disjoint subsets N , xI X .jxI )h margO (DN xI ) (j J)h margO (DNj(j J)h margO (DN)h margO (DN ).Proposition 27. Consider coherent marginal sets desirable gambles Dn n N .strong product nN Dn independent product marginals.Proof. Taking account Proposition 26, need show sets Dnmarginals strong product nN Dn . immediate consequence Proposition 25.629fiDe Cooman & Mirandastrong product may strictly include independent natural extension,see example Section A.3. open question whether, like independent natural extension, strong product associative. Althoughable prove associativity general, difficult show suffices establish maximal sets desirable gambles, one inclusions, namelynN1 N2 Mn (nN1 Mn ) (nN2 Mn ) holds strong product always includes independent natural extension. suspect, able prove,converse inclusion also holds, strong product associative, takingaccount definition taking intersection sets gambles determinedassociative operator (the independent natural extension).conclude section, establish connection strong product setsdesirable gambles eponymous notion coherent lower previsions, studiedinstance De Cooman et al. (2011) (see also Cozman, 2012 comments corresponding notion terms credal sets, sometimes called strong extension).Given coherent lower previsions P n G(X n ), n N , strong product coherentlower prevision definedN (f ) := inf {nN Pn (f ) : (n N )Pn M(P n )}gambles f X N ; intuition behind notion, taking account correspondence coherent lower previsions sets desirable gambles discussedSection 2.6, intersection family sets desirable gambles closely relatedtaking lower envelope associated family coherent lower previsions.start linear previsions Pn G(X n ), strong product correspondslinear product nN Pn , coincides also independent natural extension EN .begin coherent lower previsions P n G(X n ), strong product Nlower envelope set strong products determined dominating linear previsions.Theorem 28. Let Dn coherent sets desirable gambles G(X n ) n N ,let nN Dn strong product. Consider coherent lower previsions P n G(X n )given P n (f ) := sup { R : f Dn }. strong product N marginallower previsions P n , n N satisfiesN (f ) = sup { R : f nN Dn } .Proof. Assume first Dn maximal coherent set desirable gambles nN . follows P n linear prevision, denote Pn , n N .strong product linear previsions Pn , n N coincides linear independentproduct nN Pn , also independent natural extension (use Proposition 10De Cooman et al., 2011). Since proved Theorem 21coherent lower prevision associated nN Dn = nN Dn , conclude strongproduct nN Dn associated strong product linear previsions Pn .move next general case. Fix gamble f X N . Consider real number< N (f ). n N , consider maximal coherent set desirable gamblesMn m(Dn ), associated linear prevision Pn , clearly Pn M(P n ). HencenN Pn (f ) N (f ) > , infer arguments necessarily630fiIrrelevant Independent Natural Extension Sets Desirable Gamblesf nN Mn . Hence f nN Dn . leads conclusion N (f )sup { R : f nN Dn }.Conversely, consider real number f nN Dn . Consider arbitraryPn M(P n ), n N , maximal coherent sets desirable gambles Mnm(Dn ) inducing them: let Dn set strictly desirable gambles induces Pn , givenEq. (3). set coherent Walley (1991, Thm. 3.8.1). Consider set Dn Dn ,let us show coherent. Condition D2 holds trivially satisfied Dn .see D3 holds, taking account Dn Dn coherent sets gambles,particular cones, suffices show gamble f Dn g Dn ,sum f + g belongs Dn Dn . Consider thus gambles f, g. f G(X n )>0 , alsobelongs Dn consequence f + g Dn ; hand, f Dn \ G(X n )>0 ,follows Pn (f ) > 0, whence Pn (f + g) = Pn (f ) + Pn (g) Pn (f ) + P n (g) > 0,therefore f + g Dn . Since Dn Dn coherent, deducecondition D1 also holds, consequence set Dn Dn indeed coherent.Now, Theorem 5 implies maximal coherent set desirable gamblesMn m(Dn Dn ) m(Dn ), Walley (1991, Thm. 3.8.3) deduce DnMn induce Pn means Eq. (2). f nN Mn ,therefore nN Pn (f ) , using argumentation above. Hence N (f ) , thereforeN (f ) sup { R : f nN Dn }.Together Theorem 21 fact strong product lower previsions maystrictly dominate independent natural extension (see Example 9.3.4 Walley, 1991),also shows strong product marginal sets desirable gambles may strictlyinclude independent natural extension. explicit example given Appendix A.3.9. Conditional Irrelevance Independencefinal step take paper, consists extending results irrelevanceindependence simple common form conditional irrelevance independence.Next variables XN X N , also consider another variable assuming valuesfinite set Y.Consider two disjoint subsets N . say XI epistemically irrelevantXO when, conditional , learning value XI influence changebeliefs XO .set desirable gambles X N capture type conditionalepistemic irrelevance? Clearly, require that:margO (DxI , y) = margO (Dy) xI X Y.before, technical reasons also allow empty. cleardefinition variable X , whose constant value certain,conditionally epistemically irrelevant variable XO . Similarly, seevariable XI conditionally epistemically irrelevant variable X . seemsaccordance intuition.631fiDe Cooman & MirandaAlso, singleton, uncertainty conditioningamounts conditioning all: epistemic irrelevance seen special caseconditional epistemic irrelevance.want argue that, conversely, specific definite wayconditional epistemic irrelevance statements reduced simple epistemic irrelevancestatements. crucial results allow us establish this, following conceptuallysimple theorem corollary.Theorem 29 (Sequential updating). Consider subset R N , coherent setdesirable gambles X N Y.(Dy)xR = (DxR )y = DxR ,xR X R Y.(20)Proof. Fix xR X R Y. Clearly, three sets Eq. (20) subsetsG(X N \R ). take gamble f X N \R , consider following chains equivalences:I{y} I{xR } f I{xR } f Dy f (Dy)xRI{y} I{xR } f I{y} f DxR f (DxR )yI{y} I{xR } f f DxR , y.Corollary 30 (Reduction). Consider disjoint subsets N , coherent set desirable gambles X N Y. following statements equivalent:(i) margO (DxI , y) = margO (Dy) xI X Y;(ii) margO ((Dy)xI ) = margO (Dy) xI X Y.tells us model (XN , ) represents epistemic irrelevance XI XO ,conditional possible value , model Dy XNrepresents epistemic irrelevance XI XO .suppose marginal conditional models Dn X n , n N . notationDn concise way representing family conditional models Dn y, Y.combine Corollary 30 Theorem 19, obtain following:Corollary 31. smallest conditionally independent product DY marginal modelsDn given nN (Dn ), meaning Y, Dy = nN (Dn y).also shows calculating conditionally independent natural extension has,comparison independent natural extension, additional factor computationalcomplexity simply linear number possible values conditioningvariable .10. ConclusionsSets desirable gambles informative coherent lower previsions,shown Section 2.6, helpful avoiding problems involving zero probabilities.Moreover, simple axiomatic definition, seen Section 2.1.632fiIrrelevant Independent Natural Extension Sets Desirable Gamblesoverlooked much development theory imprecise probabilities,last five six years effort devoted bringingsimplifying unifying notion fore.Working sets desirable gambles allows us show computational complexity checking whether gamble belongs independent natural extension compares favourably computing strong product, complexityexponential number variables.results also show model assessments epistemic independenceeasily using sets desirable gambles, derive existing resultslower previsions.Moreover, results Section 7 indicate constructing global joint models (i.e. coherent sets desirable gambles) local ones something easily efficiently done types credal networks (Cozman, 2000, 2005). interpretationgraphical structure credal networks usually taken following:node (variable), conditional parents, non-parent non-descendants stronglyindependent (Cozman, 2000, 2005). replace assumption strong independence weaker one epistemic irrelevance, work De Cooman et al.(2010), tends produce conservative independent products.9consider credal network made n unconnected nodes X1 , . . . , Xn , interpretation simple: variable Xk , remaining variables X1 , . . . , Xk1 ,Xk+1 , . . . Xn epistemically irrelevant it. expression (18) independentnatural extension nk=1 Dk , reasoning behind Section 7, show nk=1 Dksmallest (most conservative) coherent joint set desirable gambles expressesepistemic irrelevancies graph.Interestingly, make network slightly complicated looking developments Section 9, tell us conditionally independent natural extensionnk=1 Xk conservative (conditional) joint model reflects independenceconditions embedded following graphical structure:...X1X2Xn1Xnvariable Xk , remaining variables X1 , . . . , Xk1 , Xk+1 , . . . Xn epistemicallyirrelevant Xk , conditional .Now, tree built recursively using simple networks like onebuilding blocks: similarly done De Cooman et al. (2010, Section 4),use recursion leaves root, step conditionalmodel put together joint one using epistemic irrelevance/independenceassessments marginal extension theorem (Miranda & De Cooman, 2007), allowsus combine hierarchical information. suggests developments paper9. See also Section 8 details strong independence; surmise computationalcomplexity dealing strong products worse computing independent naturalextension.633fiDe Cooman & Mirandaused good advantage finding efficient algorithms inference credal treesepistemic irrelevance, using sets desirable gambles uncertainty models.approach could build ideas proposed De Cooman et al. (2010) DesterckeDe Cooman (2008) context credal trees lower previsions local uncertaintymodels, make general also directly amenable simple assessmentelicitation local models. would interesting applications dealinghidden Markov models imprecise transition emission models, are,course, special credal trees.expect generalising algorithms towards general credal networks(polytrees, . . . ) difficult, rely heavily pioneering workMoral (2005) graphoid properties epistemic irrelevance. sense, wouldinteresting model assumptions independence variables using setsdesirable gambles, instance intermediate assumptions epistemic irrelevanceindependence (that is, epistemic irrelevance pairs sets variables only).Moreover, algorithms computing irrelevant independent natural extension,well strong product, need devised.open problems would generalise work infinite sequences randomvariables, would allow us deal unbounded trees, and, alreadydiscussed paper, establish associativity strong product extendresults variables taking values infinite spaces.Acknowledgmentswork supported SBO project 060043 IWT-Vlaanderen, projectMTM2010-17844. would like thank reviewers helpful comments.Appendix A. Examplesappendix, gathered number examples counterexamples.A.1 Independent Natural Extension Need Preserve MaximalityLet X = {0, 1} let subset G(X ) given:= {f G(X ) : f (0) + f (1) > 0 f (0) = f (1) > 0} .easy see coherent set desirable gambles.1moreover maximal: non-zero f/ M, either f (0) + f (1) < 0,whence f (0) f (1) > 0 f > 0, f (0) = f (1) < 0f (0) = f (1) > 0, means f M.Let N = {1, 2}, X 1 = X 2 = X M1 = M2 = M. independentnatural extension M1 M2 givenirrM1 M2 := posi G(X {1,2} )>0 Airr{1}{2} A{2}{1}nirr= h1 + h2 : h1 Airr{0},h{0}\ {0},2{1}{2}{2}{1}6340fiIrrelevant Independent Natural Extension Sets Desirable Gamblesirrtaking account non-negative gambles belong Airr{1}{2} A{2}{1}irrirrAirr{1}{2} {0} A{2}{1} {0} convex cones. Recall h1 A{1}{2} {0}iff h1 (0, ) {0} h1 (1, ) {0}, similarly h2 Airr{2}{1} {0} iffh2 (, 0) {0} h2 (, 1) {0}. means gamble h M1 M2expressedh(0, 0) = + ,h(0, 1) = + ,h(1, 0) = + ,h(1, 1) = + ,, . . . , real numbers satisfying following constraints:+ > 0 = 0+ > 0 = 0+ > 0 = 0+ > 0 = 0max{, , , } > 0.gamble h given h(0, 0) = h(1, 1) = 1 h(0, 1) = h(1, 0) = 1 belongM1 M2 : since h(0, 0) + h(0, 1) + h(1, 0) + h(1, 1) = 0, = 0,= 0, = 0 = 0, implies h(0, 0) 0, contradiction.h belong M1 M2 either, h(0, 0) + h(0, 1) + h(1, 0) + h(1, 1) = 0similarly implies h(1, 1) 0. Hence, independent natural extension M1M2 maximal.A.2 Maximal Independent Product Maximal SetsNext, construct example independent product maximal setsmaximal.Consider spaces X 1 X 2 , maximal marginal coherent sets desirablegambles M1 M2 Section A.1. consider set desirable gamblesdefined:= {h G(X N ) : h(0, 0) + h(0, 1) + h(1, 0) + h(1, 1) > 0}{h G(X N ) : h(0, 0) + h(0, 1) + h(1, 0) + h(1, 1) = 0[h(0, 0) > 0 h(0, 0) = 0, h(0, 1) > 0 h(0, 0) = h(0, 1) = 0, h(1, 0) > 0]}.first show M1 M2 . According discussion Section A.1, gambleh M1 M2 satisfies h(0, 0) = + , h(0, 1) = + , h(1, 0) = + , h(1, 1) = + ,particularmin{ + , + , + , + } 0,whenceh(0, 0) + h(0, 1) + h(1, 0) + h(1, 1) = ( + ) + ( + ) + ( + ) + ( + )= ( + ) + ( + ) + ( + ) + ( + ) 0.h(0, 0) + h(0, 1) + h(1, 0) + h(1, 1) = 0, implies + = + = + = + = 0,therefore, looking characterisation M1 M2 Section A.1,635fiDe Cooman & Miranda= 0, = 0, = 0 = 0 implies particularh(0, 0) = + 0. see either h(0, 0) > 0, case h ,h(0, 0) = 0. implies = = = = 0. h(0, 1) = 0,either h(0, 1) > 0, case h , h(0, 1) = = = 0.follows conditions imposed , . . . , Section A.1 h(1, 0) = > 0,means h belongs . So, indeed, M1 M2 .show maximal coherent set desirable gambles. easysee coherent. show maximal, consider non-zero gamble hG(X {1,2} ); three possibilities. h(0, 0) + h(1, 0) + h(0, 1) + h(1, 1) > 0,h h/ . h(0, 0) + h(1, 0) + h(0, 1) + h(1, 1) < 0, h h/ .h(0, 0) + h(1, 0) + h(0, 1) + h(1, 1) = 0, exactly one h, h belongs .conclude, note independent product M1 M2 Proposition 24.A.3 Strong Product May Strictly Include Independent NaturalExtensionfollowing adaptation example Walley (1991, Example 9.3.4).Consider X = {0, 1} let P coherent lower prevision determined P ({0}) =2/5 P (1) = 1/2, f G(X ) that:1123P (f ) = minf (0) + f (1), f (0) + f (1) .2255P associate coherent set (strictly) desirable gambles Eq. (3)::= {f : f > 0 P (f ) > 0} .let N = {1, 2}, X 1 = X 2 = X D1 = D2 = D. Consider gamble h X {1,2}determined5149h(0, 0) = h(1, 1) =, h(0, 1) = h(1, 0) =.100100see D1 D2 strictly included D1 D2 , show h belongs D1 D2D1 D2 .irrlatter claim, consider gambles h1 Airr{1}{2} h2 A{2}{1} , assumeex absurdo h h1 +h2 . see (h1 +h2 )(0, 0) = +, (h1 +h2 )(0, 1) = +,(h1 + h2 )(1, 0) = + (h1 + h2 )(1, 1) = + , real numbers , . . . , mustsatisfy following constraints:11 23max{, } > 0 min+ , + 022 5511 23max{, } > 0 min+ , + 022 5511 23max{, } > 0 min+ , + 022 5511 23max{, } > 0 min+ , + 0.22 55636fiIrrelevant Independent Natural Extension Sets Desirable Gamblesconsequence,32( + ) + ( + + + + + )5523236 116 11= ( + ) + ( + ) + ( + ) + ( + ) 0,55555 225 22hand23( + ) + ( + + + + + )552339h(0, 0) + (h(0, 1) + h(1, 0) + h(1, 1)) =,55500contradiction. implies h belong D1 D2 .former claim, consider arbitrary maximal coherent set desirable gamblesM1 m(D1 ) M2 m(D2 ). follows discussion Section 2.6M1 induces linear prevision P1 P 1 , M2 induces linear prevision P2 P 2 .follows discussion Example 9.3.4 Walley (1991)(P1 P2 )(h)1> 0,100tells us h belongs set strictly desirable gambles induces P1 P2 ,Eq. (3). Since smallest coherent set desirable gambles inducesP1 P2 , since M1 M2 another set, Theorem 28, deduce hM1 M2 . follows indeed h D1 D2 .ReferencesAntonucci, A., de Campos, C., & Zaffalon, M. (2012). Probabilistic graphical models.Coolen, F., Augustin, T., De Cooman, G., & Troffaes, M. C. M. (Eds.), introductionimprecise probabilities, chap. 10. John Wiley & Sons. press.Augustin, T., Coolen, F. P. A., De Cooman, G., & Troffaes, M. C. M. (Eds.). (2012).Introduction Imprecise Probabilities. John Wiley & Sons. press.Aumann, R. J. (1962). Utility theory without completeness axiom. Econometrica, 30,445462.Bernoulli, J. (1713). Ars Conjectandi. Thurnisius, Basel.Boole, G. (1847, reprinted 1961). Laws Thought. Dover Publications, New York.Boole, G. (2004, reprint work originally published Watts & Co., London, 1952).Studies Logic Probability. Dover Publications, Mineola, NY.Buehler, R. J. (1976). Coherent preferences. Annals Statistics, 4, 10511064.Couso, I., & Moral, S. (2011). Sets desirable gambles: conditioning, representation,precise probabilities. International Journal Approximate Reasoning, 52 (7), 10341055.637fiDe Cooman & MirandaCouso, I., Moral, S., & Walley, P. (2000). survey concepts independence impreciseprobabilities. Risk Decision Policy, 5, 165181.Cozman, F. G. (2000). Credal networks. Artificial Intelligence, 120, 199233.Cozman, F. G. (2005). Graphical models imprecise probabilities. International JournalApproximate Reasoning, 39 (2-3), 167184.Cozman, F. G. (2012). Sets probability distributions, independence, convexity. Synthese, 186, 177200.de Campos, L. M., & Moral, S. (1995). Independence concepts convex sets probabilities. Besnard, P., & Hanks, S. (Eds.), Eleventh Conference UncertaintyArtificial Intelligence, pp. 108115. San Francisco, CA.De Cooman, G. (2005). Belief models: order-theoretic investigation. Annals Mathematics Artificial Intelligence, 45 (12), 534.De Cooman, G., Hermans, F., Antonucci, A., & Zaffalon, M. (2010). Epistemic irrelevancecredal nets: case imprecise Markov trees. International Journal ApproximateReasoning, 51 (9), 10291052.De Cooman, G., Miranda, E., & Zaffalon, M. (2011). Independent natural extension. Artificial Intelligence, 175 (1213), 19111950.De Cooman, G., & Quaeghebeur, E. (2012). Exchangeability sets desirable gambles.International Journal Approximate Reasoning, 53 (3), 363395. Special issuehonour Henry E. Kyburg, Jr.de Finetti, B. (1931). Sul significato soggettivo della probabilita. Fundamenta Mathematicae, 17, 298329.de Finetti, B. (1937). La prevision: ses lois logiques, ses sources subjectives. Annales delInstitut Henri Poincare, 7, 168. English translation (Kyburg Jr. & Smokler,1964).de Finetti, B. (1970). Teoria delle Probabilita. Einaudi, Turin.de Finetti, B. (19741975). Theory Probability: Critical Introductory Treatment. JohnWiley & Sons, Chichester. English translation (de Finetti, 1970), two volumes.Destercke, S., & De Cooman, G. (2008). Relating epistemic irrelevance event trees.Dubois, D., Lubiano, M., Prade, H., Gil, M., Grzegiorzewski, P., & Hryniewicz, O.(Eds.), Soft Methods Handling Variability Imprecision, pp. 6673. Springer.Dubins, L. E. (1975). Finitely additive conditional probabilities, conglomerability disintegrations. Annals Probability, 3, 8899.Dubra, J., Maccheroni, F., & Ok, E. (2004). Expected utility theory without completeness axiom. Journal Economic Theory, 115 (1), 118133.Fishburn, P. C. (1975). theory subjective expected utility vague preferences.Theory Decision, 6, 287310.Gardenfors, P. (1988). Knowledge Flux Modeling Dynamics Epistemic States.MIT Press, Cambridge, MA.638fiIrrelevant Independent Natural Extension Sets Desirable GamblesGiron, F. J., & Rios, S. (1980). Quasi-Bayesian behaviour: realistic approachdecision making?. Bernardo, J. M., DeGroot, M. H., Lindley, D. V., & Smith, A.F. M. (Eds.), Bayesian Statistics, pp. 1738. Valencia University Press, Valencia.Hermans, F. (2012). operational approach graphical uncertainty modelling. Ph.D.thesis, Faculty Engineering Architecture.Koopman, B. O. (1940). Axioms Algebra Intuitive Probability. AnnalsMathematics, Second Series, 41 (2), 269292.Kyburg Jr., H. E., & Smokler, H. E. (Eds.). (1964). Studies Subjective Probability. Wiley,New York. Second edition (with new material) 1980.Levi, I. (1980). Enterprise Knowledge. MIT Press, London.Miranda, E., & De Cooman, G. (2007). Marginal extension theory coherent lowerprevisions. International Journal Approximate Reasoning, 46 (1), 188225.Miranda, E., & Zaffalon, M. (2010). Notes desirability coherent lower previsions.Annals Mathematics Artificial Intelligence, 60 (34), 251309.Miranda, E., Zaffalon, M., & De Cooman, G. (2012). Conglomerable natural extension.International Journal Approximate Reasoning, 53 (8), 12001227.Moral, S. (2005). Epistemic irrelevance sets desirable gambles. Annals MathematicsArtificial Intelligence, 45 (12), 197214.Moral, S., & Wilson, N. (1995). Revision rules convex sets probabilities. Coletti,G., Dubois, D., & Scozzafava, R. (Eds.), Mathematical Models Handling PartialKnowledge Artificial Intelligence, pp. 113128. Plenum Press, New York.Pearl, J. (1985). Bayesian netowrks: model self-activated memory evidential reasoning. Proceedings 7th Conference Cognitive Science Society, pp.329334, Irvine, CA. University California.Quaeghebeur, E. (2012a). CONEstrip algorithm. Proceedings SMPS 2012, pp.4554. Springer.Quaeghebeur, E. (2012b). Desirability. Coolen, F., Augustin, T., de Cooman, G., &Troffaes, M. C. M. (Eds.), Introduction Imprecise Probabilities, chap. 2. JohnWiley & Sons. press.Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (1995). representation partiallyordered preferences. Annals Statistics, 23, 21682217. Reprinted (Seidenfeld,Schervish, & Kadane, 1999), pp. 69129.Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (1999). Rethinking FoundationsStatistics. Cambridge University Press, Cambridge.Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (2010). Coherent choice functionsuncertainty. Synthese, 172 (1), 157176.Shapley, L. S., & Baucells, M. (1998). theory multiperson utility. Discussion paper,department economics 779, UCLA.Smith, C. A. B. (1961). Consistency statistical inference decision. JournalRoyal Statistical Society, Series A, 23, 137.639fiDe Cooman & MirandaWalley, P. (1982). elicitation aggregation beliefs. Tech. rep., UniversityWarwick, Coventry. Statistics Research Report 23.Walley, P. (1991). Statistical Reasoning Imprecise Probabilities. Chapman Hall,London.Walley, P. (2000). Towards unified theory imprecise probability. International JournalApproximate Reasoning, 24 (23), 125148.Williams, P. M. (1975a). Coherence, strict coherence zero probabilities. ProceedingsFifth International Congress Logic, Methodology Philosophy Science,Vol. VI, pp. 2933. Dordrecht. Proceedings 1974 conference held Warsaw.Williams, P. M. (1975b). Notes conditional previsions. Tech. rep., School MathematicalPhysical Science, University Sussex, UK. Revised journal version: (Williams,2007).Williams, P. M. (2007). Notes conditional previsions. International Journal Approximate Reasoning, 44 (3), 366383. Revised journal version (Williams, 1975b).Zaffalon, M., & Miranda, E. (2012). Probability time. Submitted publication.640fiJournal Artificial Intelligence Research 45 (2012) 363-441Submitted 03/12; published 10/12Transforming Graph Data Statistical Relational LearningRyan A. Rossirrossi@purdue.eduDepartment Computer Science, Purdue UniversityWest Lafayette, 47907 USALuke K. McDowelllmcdowel@usna.eduDepartment Computer Science, U.S. Naval AcademyAnnapolis, MD 21402, USADavid W. Ahadavid.aha@nrl.navy.milNavy Center Applied Research Artificial IntelligenceNaval Research Laboratory (Code 5514)Washington, DC 20375, USAJennifer Nevilleneville@purdue.eduDepartment Computer Science, Purdue UniversityWest Lafayette, 47907 USAAbstractRelational data representations become increasingly important topic duerecent proliferation network datasets (e.g., social, biological, information networks)corresponding increase application Statistical Relational Learning (SRL)algorithms domains. article, examine categorize techniquestransforming graph-based relational data improve SRL algorithms. particular, appropriate transformations nodes, links, and/or features data dramaticallyaffect capabilities results SRL algorithms. introduce intuitive taxonomydata representation transformations relational domains incorporates link transformation node transformation symmetric representation tasks. specifically,transformation tasks nodes links include (i) predicting existence, (ii)predicting label type, (iii) estimating weight importance, (iv) systematically constructing relevant features. motivate taxonomy detailedexamples use survey competing approaches tasks. also discuss general conditions transforming links, nodes, features. Finally, highlightchallenges remain addressed.1. Introductionarticle, examine categorize techniques transforming relational data improve Statistical Relational Learning (SRL) algorithms. Below, Section 1.1 first introducesrelational data SRL. summarize primary types representations relationaldata, explain focus data represented graphs. Section 1.1 also describestransforming content (rather type) representation improve SRLanalysis. instance, predicting new links graph increase accuracy relationalnode classification. Section 1.2 identifies scope article. Finally, Section 1.3summarizes organization approach article, includes descriptiontaxonomy relational representation transformation.c2012AI Access Foundation. rights reserved.fiRossi, McDowell, Aha, & Neville1.1 Relational Data, SRL, Representation Choicesmajority research machine learning assumes independently identically distributed data. independence assumption often violated relational data, encode dependencies among data instances. instance, people often linked businessassociations, information one person highly informative predictiontask involving associate person. generally, relational data describedset nodes, connected one types relations (or links).Relational information seemingly ubiquitous; present domains Internetworld-wide web (Faloutsos, Faloutsos, & Faloutsos, 1999; Broder et al., 2000; Albert, Jeong, & Barabasi, 1999), scientific citation collaboration (McGovern et al., 2003;Newman, 2001b), epidemiology (Pastor-Satorras & Vespignani, 2001; Moore & Newman,2000; May & Lloyd, 2001; Kleczkowski & Grenfell, 1999) communication analysis (Rossi& Neville, 2010), metabolism (Jeong, Tombor, Albert, Oltvai, & Barabasi, 2000; Wagner& Fell, 2001), ecosystems (Dunne, Williams, & Martinez, 2002; Camacho, Guimera, &Nunes Amaral, 2002), bioinformatics (Maslov & Sneppen, 2002; Jeong, Mason, Barabasi,& Oltvai, 2001), fraud terrorist analysis (Neville et al., 2005; Krebs, 2002), manyothers. links data may represent citations, friendships, associations, metabolicfunctions, communications, co-locations, shared mechanisms, many explicit implicit relationships.Statistical relational learning (SRL) methods developed address problems reasoning learning domains complex relations probabilistic structure(Getoor & Taskar, 2007). particular, SRL algorithms leverage relational informationattempt learn models higher predictive accuracy. key characteristic manyrelational datasets correlation statistical dependence valuesattribute across linked instances (e.g., two friends likely share political viewstwo randomly selected people). relational autocorrelation provides unique opportunity increase accuracy statistical inferences (Jensen, Neville, & Gallagher,2004). Similarly, relational information exploited many reasoning tasksidentifying useful patterns optimizing systems (Easley & Kleinberg, 2010).Representation issuesincluding knowledge, model, data representationhaveheart artificial intelligence community decades (Amarel, 1968; Minsky, 1974;Russell & Norvig, 2009). important, focus data representation issues, simple examples include choices whether discretize continuousfeatures add higher-order polynomial features. decisions significanteffect accuracy efficiency AI algorithms. especially criticalperformance SRL algorithms because, relational domains, even larger spacepotential data representations consider. complex structure relational dataoften represented variety ways choice specific data representationimpact applicability particular models/algorithms performance.Specifically, two categories decisions need considered contextrelational data representation.First, consider type data representation use (cf., hierarchyDe Raedt, 2008, ch. 4). instance, relational data propositionalizedapplication standard, non-relational learning algorithms. often, order fully364fiTransforming Graph Data Statistical Relational Learningexploit relational information, SRL researchers chosen represent data eitherusing attributed graph relational database (see e.g., Friedman, Getoor, Koller, &Pfeffer, 1999), via logic programs (see e.g., Kersting & De Raedt, 2002).1 choicedifferent strengths. article, focus graph-based representation,common choice addressing growing interest network data applications analyzing electronic communication online social networks Facebook,Twitter, Flickr, LinkedIn (Mislove, Marcon, Gummadi, Druschel, & Bhattacharjee,2007; Ahmed, Berchmans, Neville, & Kompella, 2010). Specifically, assume graphbased data representation G = hV, E, XV , XE nodes V entities (e.g., people,places, events) links E represent relationships among entities (e.g., friendships, citations). XV set features entities V . Likewise, set featuresXE provides information relation links E.Next, given type representation, must consider specific content datarepresentation, large space choices. instance, features nodeslinks graph constructed using wide range aggregation functions, basedmultiple kinds links paths. SRL researchers already recognized importancedata representation choices (e.g., Getoor & Diehl, 2005), many separate studiesexamined techniques feature construction (Neville, Jensen, Friedland, & Hay, 2003),node weighting (Tang, Musolesi, Mascolo, & Latora, 2009), link prediction (Taskar, Wong,Abbeel, & Koller, 2003), etc. However, article first comprehensively surveyapproaches relational representation transformation graph-based data.Given set (graph-based) relational data, define relational representation transformation change space links, nodes, and/or features used representdata. Typically, goal transformation improve performancesubsequent SRL application. instance, Figure 1 original graph representation Gtransformed new representation G links, nodes, features (such linkweights) added, links removed. SRL algorithmanalysis applied new representation, instance classify nodesidentify anomalous links. particular transformations used produce Gvary depending upon intended application, sometimes substantially improveaccuracy, speed, complexity final application. instance, Gallagher, Tong,Eliassi-Rad, Faloutsos (2008) found adding links similar nodes could increase node classification accuracy 15% tasks. Similarly, NevilleJensen (2005) demonstrated adding nodes represent underlying groups enabledsimpler inference increased accuracy.1.2 Scope Articlearticle focuses examining categorizing various techniques changingrepresentation graph-based relational data. shown Figure 1, typically viewchanges pre-processing step enables increased accuracy speedtask, object classification. However, output techniquesvaluable. instance, administrators social network may interested1. latter case, applicable SRL algorithms often referred probabilistic inductive logicprogramming (ILP) (De Raedt & Kersting, 2008).365fiRossi, McDowell, Aha, & NevilleCCSRL Analysis /ApplicationRepresentationTransformationLLLGGResultFigure 1: Example Transformation Subsequent Analysis: original relational representation G transformed G dotted lines represent predicted links, squares represent predicted nodes, bold links represent linkweighting. Changes may based link structure, link features, node features (here, similar node shadings indicate similar feature values). SRLanalysis applied new representation. example, SRLanalysis produces label (C L) node, example task discussed Section 2.1. article focuses representation transformation(left side figure), subsequent analysis.link prediction predicted links presented users potential newfriendship links. Alternatively, techniques may also applied improvecomprehensibility model. example, prediction protein-protein interactionsprovides insights protein function (Ben-Hur & Noble, 2005). Thus, techniquessurvey may used multiple purposes, relevant publications may useddifferent contexts. Regardless original context, examine generalapplicability benefits technique. techniques applied,transformed data used (e.g., friendship suggestions), examined greaterunderstanding, used task (e.g., object classification), used recursivelyinput another representation change (e.g., object/node prediction followedlink prediction).attempt survey many methods could used SRL analysis(e.g., right side Figure 1), although relevant set methods analysisoverlaps set methods facilitate transformations consider. instance, collective classification (Neville & Jensen, 2000; Taskar, Abbeel, & Koller, 2002)important SRL application define Section 2 use running exampleSRL analysis task. output classification could also used createnew attributes nodes (a data representation change). discuss possibilitySection 6.2, focus cases node labeling particularly usefulpre-processing step (e.g., applying certain stacked algorithms), rather surveying wide range possible classification algorithms, whether collective not. Likewise,survey issues model knowledge representation, whether sta366fiTransforming Graph Data Statistical Relational Learningtistical dependencies nodes, links, features modeled StructuralLogistic Regression (Popescul, Popescul, & Ungar, 2003b) Markov Logic Network(Domingos & Richardson, 2004). consider issues briefly, Section 8.4.Furthermore, focus transformations change content graph datarepresentation. particular, examine transformations graph data modifyset links nodes, modify features. consider changing graph datadifferent type representation, e.g., propositionalizing data changinglogic program. However, transformations discuss, node linkfeature aggregation, form propositionalization. addition, Section 6.3.3 describesnumber techniques structure learning logic programs, techniquesclosely related analogous problem feature construction graph-based representations. Finally, many techniques discuss also applicablelogical representations. instance, link weighting could applied weight knownrelations using logic program detect anomalous objects. focus, however,methods useful transforming graph-based representations.1.3 Approach Organization Articlemany dimensions relational data transformation, complicate taskunderstanding selecting appropriate techniques. assist process,introduce simple intuitive taxonomy representation transformation identifies link transformation node transformation symmetric representation tasks.specifically, transformation tasks nodes links include (i) predictingexistence, (ii) predicting label type, (iii) estimating weight importance,(iv) constructing relevant features. addition, propose taxonomy constructing link node features consists non-relational features, topology features,relational node-value features, relational link-value features. relational transformation task, survey applicable techniques, examine necessary conditions,provide detailed examples comparisons.article organized follows. next section presents taxonomy relationalrepresentation transformation discusses motivating example. Section 3, reviewalgorithms link prediction, Section 4 examines task link interpretation(i.e., constructing link labels, link weights, link features). Sections 5 6 considercorresponding prediction interpretation tasks nodes instead links. Section7, summarize algorithms jointly transform nodes links. Section 8 discussesmethods evaluating representation transformations challenges future work,Section 9 concludes.2. Overview Motivating Examplesection first introduce running example based classification dataFacebook, describe relational algorithms could used perform task.Next, introduce taxonomy relational representation transformation explaintype transformation could aid Facebook classification task. Finally,formally define type relational representation transformation.367fiRossi, McDowell, Aha, & Neville2.1 Motivating SRL Analysis Example: Classification Taskexample, consider hypothetical data inspired Facebook (www.facebook.com),one popular online social networks. assume given graphG = hV, E, XV , XE nodes V users 2 links E represent friendshipsFacebook. XV set features users V gender, relationshipstatus, school, favorite movies, musical preference (though information may missingusers). Likewise, set features XE provides information friendshiplinks E time formation possibly contents messagesent link formation requested one users.example SRL analysis task (see Figure 1) predict political affiliation (liberal,moderate, conservative) every node (person) G. assume affiliation,call class label node, known people G.3Moreover, assume users political affiliation likely correlatedcharacteristics user (to lesser degree) users friends. next sectionsummarizes correlations used classification.example, assume links simple, binary friendship connections. However, link types could used represent kinds relationships. instance,link might indicate two people communicated via wall-post message,two people chosen join Facebook group. addition, notion friendship Facebook weak thus significant portion persons friends oftencasual acquaintances. Thus, representation changes link deletion weightingmay significant impact classification accuracy. notational purposes, addtilde top graph components symbol indicate undergonetransformation (e.g., modified link set E denoted E).2.2 Background: Features Methods Classificationpredict political affiliation Facebook users, conventional classification approacheswould ignore links classify user using information known user,gender location. assume information representedform non-relational features, features computed directlyXV without considering links E. refer classification basedfeatures non-relational classification. Alternatively, relational classification, linksexplicitly used construct additional relational features capture informationusers friends. instance, relational feature could compute, user,proportion friends male live particular region. Using relationalinformation potentially increase classification accuracy, though may sometimes decreaseaccuracy well (Chakrabarti, Dom, & Indyk, 1998). Finally, even greater (and usuallyreliable) increases occur class labels (e.g., political affiliations)linked users used instead derive relevant features (Jensen et al., 2004). instance,2. general, may one type node. instance, nodes citation network mayrepresent papers authors.3. Later, discuss representation change node labeling, also constructs estimated labelevery node. discussed Section 1.2, representation changes sometimes resemble outputSRL analysis, focus changes particularly useful pre-processingsubsequent SRL analysis.368fiTransforming Graph Data Statistical Relational Learningclass-label relational feature could compute, user, proportion friendsliberal views. However, using features challenging sincelabels initially unknown, thus typically must estimated iterativelyrefined way. process jointly inferring labels interrelated nodesknown collective classification (CC).CC requires models inference procedures use inferences one useraffect inferences related users. Many algorithms considered CC,including Gibbs Sampling (Jensen et al., 2004), relaxation labeling (Chakrabarti, Dom, &Indyk, 1998), belief propagation (Taskar et al., 2002), ICA (Neville & Jensen, 2000; Lu &Getoor, 2003), weighted neighbor techniques (Macskassy & Provost, 2007). Seework Sen et al. (2008) survey.concrete example SRL analysis, explain many techniques surveyterms Facebook classification task, special emphasis CC. However,features transformation techniques apply many SRL tasks data setsrelationship classification, anomalous link detection, entity resolution, groupdiscovery (Getoor & Diehl, 2005).2.3 Representation Transformation Tasks Improving SRLFigure 2 shows proposed taxonomy relational representation transformation.two main tasks taxonomy link transformation node transformation.find powerful elegant symmetry two tasks. particular,link node representation transformation tasks decomposed predictioninterpretation tasks. former task involves predicting existence new nodeslinks. latter task interpretation involves three parts: constructing weights,labels, features nodes links. Together, yields eight distinct transformation tasksshown leaves taxonomy Figure 2. Underneath eight tasksfigure, list primary graph component modified task (i.e., V , E, XV ,XE ), followed illustration possible representation change task.text below, summarize Figure 2, organized around four larger categories linkprediction, link interpretation, node prediction, node interpretation.First, link prediction adds new links graph. sample graph task(Figure 2A) shows link predicted similarity two nodesused predict new link them. Intuitively, Facebook users share valuesmany non-relational features may also share political affiliation. Thus, addinglinks people increase autocorrelation improve accuracy collective classification. many simple link prediction algorithms based similarity,neighbor properties, shortest path distances, infinite sums paths (i.e. random walks),strategies. Section 3 provides detail techniques.Second, several types link interpretation, involves constructingweights, labels, features existing links. instance, many graphs (includingFacebook data), links (or friendships) equal importance. Thus, Figure 2Bshows result performing link weighting. case, weights based similarity feature values pair linked nodes, assumptionhigh similarity may indicate stronger relationships. (Link prediction techniques may also369fiRossi, McDowell, Aha, & NevilleRelationalRepresentationTransformationinputEweighted linklabeled linkpredicted nodeweighted nodelabeled nodeNode TransformationLink InterpretationNode PredictionLink WeightingLink LabelingLink FeatureConstructionX EX EX EVNode InterpretationNode WeightingNode LabelingNode FeatureConstructionX VX VX VppB.CwwA.pLLink TransformationLink Predictionpredicted link2+33pC.-CD.E.F.LG..1 B.2L-.3 B.5.3H.Figure 2: Relational Representation Transformation Taxonomy: Link nodetransformation formulated symmetric tasks leading four main transformation tasks: predicting links, interpreting links, predicting nodes, interpreting nodes. task yields modified graph component: E, XE , V ,XV , respectively. Interpretation divided weighting, labeling,constructing features. Examples tasks relational representationtransformation shown leaves taxonomy. examplegraphs, nodes similar shadings similar feature values.370fiTransforming Graph Data Statistical Relational Learninguse similarity measures, identifying probable new links, rather weightingexisting links.) Alternatively, link labeling may used assign kind discrete labellink. instance, Figure 2C shows links might labeled either personal(p) work (w) related, e.g., based known feature values analysis communication events linked users. hand, links might instead labeledpositive negative influence (i.e., labeled +/). Finally, Figure 2D showslink feature construction used add general kinds feature valueslink. instance, link feature might count number communication eventsoccurred two people number friends common. Link weightinglabeling could perhaps viewed special cases link feature construction,separate later sections show useful techniques taskdiffer. three link interpretation tasks could help example classificationproblem. particular, model learned predict political affiliation might choose placespecial emphasis links highly weighted labeled personal.link features might used represent complex dependencies, instance modeling influence users work friendships, friendship links nodeslarge number friends common. details techniquesprovided Section 4.Third, node prediction adds additional nodes (and associated links) graph.instance, Figure 2E shows result relational clustering applieddiscover two latent groups graph, user connected one latentgroup node. discovered node Facebook might represent types social processes,influences, tightly knit group friends. clustering techniques usedidentify new nodes could designed identify people particularly similarrespect relevant characteristic, political affiliation. new nodesassociated links could used several ways. instance, though presentsmall example Figure 2E, nodes far away (in terms shortestpath length) original graph may much closer new graph. Thus, linkslatent node may allow influence propagate effectively algorithmCC applied. Alternatively, identification distinct latent groups may even enableefficient accurate algorithms applied separately group (Neville & Jensen,2005). Node prediction discussed Section 5.Finally, several types node interpretation, involves constructingweights, labels, feature values existing nodes. instance, links,nodes may influential others thus weight. Figure 2Fdemonstrates node weighting, weights might assigned based numbersfriends via PageRank/eigenvector techniques. See Section 6.1 details.Alternatively, Figure 2G shows example node labeling. graph representstraining graph, node given estimated label conservative (C),liberal (L), moderate (M). labels might estimated using non-relationalfeatures via textual analysis. classification algorithms learn model basedtrue labels training graph, approaches instead first compute estimatedlabels, learn model new representation (Kou & Cohen, 2007). Section 6.2discusses simplify inference. Finally, Figure 2H shows result node featureconstruction, arbitrary feature values added node. instance, suppose371fiRossi, McDowell, Aha, & Nevillefind users relatively Facebook friends often moderatemany friends often liberal. case, feature counting number friendsnode would useful. directly exploit autocorrelation, different feature mightcount proportion users friends conservative, common politicalaffiliation users friends. feature correlated political affiliation couldused improve performance classification algorithm example problem.Identifying and/or computing features essential performance SRLalgorithms challenging; Section 6.3 considers process.Table 2.3, summarize prominent techniques performingtasks link prediction, link interpretation, node prediction, node interpretation.Sections 3-6 provide detail category turn.2.4 Relational Representation Transformation: Definitions Terminologyassume initial relational data represented graph G = hV, E, XV , XEvi V corresponds node edge eij E corresponds(directed) link nodes j. XV set features nodes V ,XkV XV k th feature. Likewise, XE set features linksE, XkE k th feature. features XE could refer link weights,distances, types, among possibilities. preceding notation lets us identify,instance, values particular feature XkV nodes. Alternatively, xvi refersvector containing feature values particular node vi , xeij containsfeature values particular edge eij . Table 2.3 summarizes notation.Relational representation transformation process transforming originalgraph G new graph G = hV , E, XV , XE arbitrary set transformation techniques. process, nodes, links, weights, labels, general features mayadded, nodes links may removed. theory, transformation seeksoptimize objective function (for instance, maximize autocorrelation), althoughpractice objective function may completely specified guaranteed improved transformation. define specifically four primary partsrelational representation transformation:Definition 2.1 (Link Prediction) Given nodes V , observed links E and/or featureset X = (XE , XV ), link prediction task defined creation modified link setE E 6= E. Usually, involves adding new links present E,links may also deleted.Definition 2.2 (Link Interpretation) Given nodes V , observed links E and/orfeature set X = (XE , XV ), link interpretation task defined creation newlink feature XkE XkE/ XE . task may estimate feature value every link.Alternatively, values XkE may partially estimated, example, originalfeatures missing values additional links also introduced link prediction.Definition 2.3 (Node Prediction) Given nodes V , links E and/or feature setX = (XE , XV ), node transformation defined creation modified node set VV V . addition, many node prediction tasks simultaneously create new links,372fiTransforming Graph Data Statistical Relational LearningRelational Representation TransformationLinks?Prediction???Weighting????Labeling??FeatureNodesAdamic/Adar (Adamic &Adar, 2001), Katz (Katz, 1953),others (Liben-Nowell &Kleinberg, 2007)Text Feature Similarity(Macskassy, 2007)ClassificationviaRMN(Taskar et al., 2003) SVM(Hasan, Chaoji, Salem, & Zaki,2006)Latent Variable Estimation (Xiang, Neville, & Rogati,2010)Linear Combination Features (Gilbert & Karahalios,2009)Aggregating Intrinsic Information (Onnela, Saramaki,Hyvonen, Szabo, Lazer, Kaski,Kertesz, & Barabasi, 2007)LDA (Blei et al., 2003), PLSA(Hofmann, 1999),Link Classification via Logistic Regression (Leskovec, Huttenlocher, & Kleinberg, 2010),Bagged Decision Trees (Kahanda & Neville, 2009),LinkFeatureSimilarity(Rossi & Neville, 2010)Link Aggregations (Kahanda& Neville, 2009)?Graph Features (Lichtenwalter, Lussier, & Chawla, 2010)????Betweenness (Freeman, 1977),Closeness (Sabidussi, 1966)?HITs (Kleinberg, 1999), Prob.HITs (Cohn & Chang, 2000),SimRank (Jeh & Widom, 2002)PageRank (Page, Brin, Motwani, & Winograd, 1999), Topical PageRank (Haveliwala, 2003;Richardson & Domingos, 2002)LDA (Blei et al., 2003), PLSA(Hofmann, 1999),NodeClassificationviaStacked Model (Kou & Cohen, 2007) RN (Macskassy &Provost, 2003)?????Construction?Spectral Clustering (Neville& Jensen,2005),MixedMembershipRelationalClustering (Long et al., 2007)LDA (Blei, Ng, & Jordan, 2003),PLSA (Hofmann, 1999),Hierarchical Clustering viaEdge-betweenness (Newman &Girvan, 2004)MLN Structure Learning (Kok& Domingos, 2009, 2010)DatabaseQuerySearch(Popescul et al., 2003b), RPT(Neville, Jensen, Friedland, et al.,2003)FOIL, nFOIL (Landwehr, Kersting, & De Raedt, 2005), kFOIL(Landwehr, Passerini, De Raedt,& Frasconi, 2010), Aleph (Srinivasan, 1999),Table 1: Summary Techniques: summary prominent graph transformation techniques tasks predicting existence nodes links interpretingweighting, labeling, constructing general features.373fiRossi, McDowell, Aha, & NevilleSymbolDescriptionGInitial graphGTransformed graphEInitial link setVInitial node setEInitial set link featuresVInitial set node featuresXXXkEXkVxeijxvisymbols(vi )Initial link feature k (XkE XE ) (for one feature, values links)Initial node feature k (XkV XV ) (for one feature, values nodes)Initial feature vector eij (for one link, values link features)Initial feature vector vi (for one node, values node features)DescriptionAdjacency matrix graphNeighbors viCut-off valueTable 2: Summary Notation used Survey: top half table showssymbols sometimes written tilde top symbol, indicatingresult transformation. conciseness, table demonstratesnotation G G.e.g., initial node vi V predicted node vj V . Thus, task may alsoproduce modified link set E.Definition 2.4 (Node Interpretation) Given nodes V , observed links E and/orfeature set X = (XE , XV ), node interpretation task defined creation new/ XV . link interpretation, values XkV maynode feature XkV XkVestimated nodes. node feature XkV could represent node weights,labels, general features.Section 2.2 introduced notion non-relational feature, node featureXkV constructed without making use links (i.e., without using E XE ).features sometimes referred articles attributes intrinsic features.important terms also referred multiple different ways. aid reader,Table 2.4 summarizes key synonyms terms found oftenliterature.3. Link Predictionsection focuses predicting existence links Section 4 considers linkinterpretation. Given initial graph G = hV, E, XV , XE i, interested creatingmodified link set E, usually prediction new links present374fiTransforming Graph Data Statistical Relational LearningTermPotential synonymsNodesVertices, points, objects, entities, individuals, users, constants, ...LinksEdges, relationships, ties, arcs, events, interactions, predicatesTopologyLink/network/graph structure, relational informationFeaturesAttributes, variables, co-variates, queries, predicates, ...Graph MeasuresTopology-based metrics (such proximity, centrality, betweenness, ...)SimilarityDistance (the inverse similarity), likenessClustersClasses, communities, groups, roles, topicsNon-relational FeaturesIntrinsic attributes/features, local attributes/features, ...Relational FeaturesFeatures, link-based features, graph features, aggregates, queries, ...Structure LearningFeature generation/construction, hypothesis learningParameter LearningModel selection, function learningTable 3: Synonyms Literature: summary possible synonyms foundliterature important terms related relational data.E. task motivated several ways. instance, may needpredict missing links present E incomplete data collectionproblems. Similarly, may interested predicting hidden links,assume exists unobservable interactions goal discovermodel interactions. example, network representing criminals terroristactivity, may seek predict link two people (nodes) directlyconnected whose actions share common motivation cause. missinghidden links, predicting links may improve accuracy subsequent learnedmodel. Alternatively, may seek predict future links evolving network,new friendships connections formed next year. might also interestedpredicting links objects spatially related. Finally, may wish predictbeneficial links, instance, predicting pairs individuals likely successfulworking together.Figure 3 summarizes one general approach often used link predictiontasks. summary, scores weights computed every pair nodes graph,shown Figure 3(b). Predicted links weight greater threshold , alongoriginal links, used create new link set E + (shown Figure 3(e)). (Atstep, original links low weight could also deleted appropriate.)final step, weights predicted links often discarded, yielding new graphuniform link weights shown Figure 3(f).key challenge approach compute weight score possiblelink. information used computation provides natural way categorizelink prediction techniques. Below, Section 3.1 describes techniques usenon-relational features nodes (ignoring initial links), Section 3.2 describestopology-based techniques use graph structure (i.e., links relations).375fiRossi, McDowell, Aha, & Neville(a) Initial Graph G = hE, V(b) Weighted Links wij E(c) Predicted Links (E E)(d) Pruning Predicted Links(E > )(e) E + := E > + E(f) E + Uniform LinkWeightsFigure 3: Example Demonstrating General Approach Link Prediction:initial graph (a) used input link predictor, yielding completegraph (b) weights wij estimated pairs nodes.next step shows removal initial (observed) links consideration (c),followed pruning predicted links weight cut-off value(d). remaining predicted links combined initial links (e).Often, estimated weights initial predicted links discarded,leaving uniform weight graph (f).Finally, Section 3.3 describes hybrid techniques exploit node featuresgraph structure.3.1 Non-relational (Feature-Based) Link Predictionsection, consider link predictors exploit graph structure relational features derived using graph structure. given arbitrary pair nodes376fiTransforming Graph Data Statistical Relational Learningvi vj graph node represented feature vector xvixvj , respectively. Feature-based link prediction defined using arbitrary similaritymeasure S(xvi , xvj ) means estimate likelihood link exist vivj . Typically, link created similarity exceeds fixed cut-off value; anotherstrategy predict links among n% node pairs highest similarity.traditional approach simply define measure similarity two objects,possibly based knowledge application and/or problem-domain. manysimilarity metrics proposed mutual information, cosine similarity,many others (Lin, 1998). instance, Macskassy (2007) represents textual contentnode feature vector uses cosine similarity create new links nodesgraph. Macskassy showed combination initial links predictedtext-based links increased classification accuracy compared using initial linkstext-based links. addition leveraging textual information predict links,might use arbitrary set features combined proper measure similaritylink prediction. instance, many recommender systems implicitly predict linktwo users based similarity ratings items movies books(Adomavicius & Tuzhilin, 2005; Resnick & Varian, 1997). case, cosine similaritycorrelation commonly used similarity metrics.Alternatively, similarity measure learned predicting link existence. linkprediction problem transformed standard supervised classification problembinary classifier trained determine similarity two nodes basedfeature vectors. One approach work Hasan et al. (2006), usedSupport Vector Machines (SVMs) link prediction found non-relational feature(keyword match count) useful predicting links bibliographic network.many link prediction approaches (Taskar et al., 2003; Getoor, Friedman, Koller,& Taskar, 2003) apply traditional machine learning algorithms. However,use features based graph structure well non-relational featuresfocus section. Thus, discuss techniques Section 3.3.Finally, variants topic models used link prediction. types modelstraditionally use text documents (non-relational information) infer mixture latent topics document. Inter-document topic similarity usedsimilarity metric link prediction (Chang & Blei, 2009). However, many topicmodels capable performing joint transformation nodes links, defer fulldiscussion techniques Section 7.3.2 Topology-Based Link PredictionTopology-based link prediction uses local relational neighborhood and/or globalgraph structure predict existence unobserved links. Table 3.2 summarizescommon metrics used task. Below, discuss manyapproaches, starting simplest local metrics moving complextechniques based global measures and/or supervised learning. systematic studymany approaches applied social network data, see work Liben-NowellKleinberg (2007).377fiRossi, McDowell, Aha, & NevilleLocal Node MetricsDescriptionCommon NeighborsNumber common neighbors x y, w(x, y) = |(x)(y)| (Newman,2001a)Jaccards CoefficientProbability x share common neighbors (normalized),|(x)(y)|(Jaccard, 1901; Salton & McGill, 1983)|(x)(y)|Adamic/AdarSimilar toPcommon neighbors, assigns weight rare neighbors,1w(x, y) = z(x)(y) log |(z)|(Adamic & Adar, 2001)RAEssentially equivalent Adamic/Adar |(z)| small,P1w(x, y) = z(x)(y) |(z)|(Zhou, Lu, & Zhang, 2009)Preferential AttachmentProbability link x product degree x y,w(x, y) = |(x)| |(y)| (Barabasi & Albert, 1999)Cosine Similarity|(x)(y)|w(x, y) =(Salton & McGill, 1983)Sorensen Indexw(x, y) =(Green, 1972; Zhou et al., 2009)Hub IndexNodes large degree likely assigned higher score,w(x, y) =|(x)||(y)|2|(x)(y)||(x)|+|(y)||(x)(y)|min{|(x)|,|(y)|}w(x, y) =(Ravasz, Somera, Mongru, Oltvai, & Barabasi, 2002)|(x)(y)|max{|(x)|,|(y)|}(Ravasz et al., 2002)Hub Depressed IndexAnalogous Hub Index, w(x, y) =Leicht-Holme-NewmanAssigns large weight pairs many common neighbors, normalized|(x)(y)|expected number common neighbors, w(x, y) = |(x)||(y)| (Leicht,Holme, & Newman, 2006)Global Graph MetricsDescriptionGraph DistanceLength shortest path xKatzNumber paths x y, exponentially damped length therebyassigning weight shorter paths, w(x, y) = [(I A)1 ]xy (Katz, 1953)Hitting timeNumber steps required random walk starting x reach (Brightwell& Winkler, 1990)Commute TimeExpected number steps reach node starting x returning++back x, defined w(x, y) = L+xx + Lyy 2Lxy L Laplacian matrix(Gobel & Jagers, 1974)Rooted PageRankSimilar Hitting time, step probability randomwalk reset starting node x, w(x, y) = [(IP)1 ]xy P = D1(Page et al., 1999)SimRankx similar extent joined similar neighbors,Pw(x, y) =Pv(y) sim(u,v)|(x)||(y)|u(x)(Jeh & Widom, 2002)K-walksNumber walks length k x y, defined w(x, y) = [Ak ]xyMeta-ApproachesDescriptionLow-rank ApproximationCompute rank-k matrix Ak best approximates (hopefully reducingnoise), compute similarity Ak using local global metric(Eckart & Young, 1936; Golub & Reinsch, 1970)Unseen BigramsCompute initial scores using local global metric, augment scoresw(x, y) using values w(z, y) nodes z similar x (Essen &Steinbiss, 1992; Lee, 1999)ClusteringCompute initial scores using local global metric, discard linkslowest scores, re-compute scores modified graph (Johnson,1967; Hartigan & Wong, 1979)Table 4: Topology Metrics: Summary common metrics link prediction.Notation: Let (x) neighbors x adjacency matrix G.378fiTransforming Graph Data Statistical Relational Learning3.2.1 Metrics Based Local Neighborhood Nodessimplest approaches use local neighborhood nodes graph devisemeasure topology similarity, use pairwise similarities nodes predictlikely links. shown Table 3.2, numerous metrics, often basednumber neighbors two nodes share common, varying strategiesnormalization.Zhou et al. (2009) compares nine local similarity measures six datasets findssimplest link predictor, common neighbors, performs best overall. alsopropose new metric, RA, outperforms initial nine metrics two datasets.new metric similar Adamic/Adar metric, uses different normalization factor yields better performance networks higher average degree.also propose method uses additional two-hop information avoid degenerate caseslinks assigned similarity score. results highlight importanceselecting appropriate metrics specific problems datasets. another relatedinvestigation, Clauset, Moore, Newman (2008) evaluate hierarchical random graphpredictor local topology metrics common neighbors, Jaccards coefficientdegree product three types networks: metabolic, ecology social network.find baseline measure based shortest paths performs best metabolicnetwork, relationships homogeneous, hierarchical metricperforms best links create complex relationships, predator-preyrelationships found ecology network.Liu Lu (2010) proposed local random-walk algorithm efficient alternativeglobal random-walk predictors large networks. method evaluated alongsidemetrics (i.e., common neighbors, local paths, RA, random-walk variants)shown perform better networks efficiently globalrandom-walk models.3.2.2 Metrics Based Global Graph Structuresophisticated similarity metrics based global graph properties, often involvingweighted computation based number paths pair nodes.instance, Katz measure (1953) counts number paths pair nodes,shorter paths count computation. Rattigan Jensen (2005) demonstrated even fairly simple metric could effective task anomalous linkprediction, identification statistically unlikely links among linksinitial graph.related measure hitting time metric, average number stepsrequired random walk starting node x reach node y. Gallagher et al. (2008)use random walks restart estimate similarity every pair nodes.focus sparsely labeled networks unlabeled nodes may labelednodes support learning and/or inference relational classification. predictionnew links improves flow information labeled unlabeled nodes, leadingincrease classification accuracy 15%. Note adding teleportation probabilitiesrandom walk approach roughly yields PageRank algorithm saidheart Google search engine (Page et al., 1999).379fiRossi, McDowell, Aha, & NevilleSimRank metric (Jeh & Widom, 2002) proposes two nodes x similarlinked neighbors similar. Interestingly, show approachequivalent metric based time required two backwards, random walksstarting x arrive node. approaches basedrandom walks, metric could computed via repeated simulations,efficiently computed via recursive set-point approach.3.2.3 Meta-approaches Supervised Learning Approachesmetrics modified combined multiple ways. Liben-Nowell Kleinberg (2007) consider several meta-approaches use local global similaritymetric subroutine. instance, metrics discussed definedterms arbitrary adjacency matrix A. Given formulation, imagine firstcomputing low-rank approximation Ak matrix using technique singularvalue decomposition (SVD), computing local global graph metric usingmodified Ak . idea Ak retains key structure original matrix, noisereduced. Liben-Nowell Kleinberg also propose two meta-approachesbased removing spurious links suggested first round similarity computation (theclustering approach) based augmenting similarity scores node x basedscores nodes similar x (the unseen bigrams approach). compare performance three meta-approaches vs. multiple local global metricstask predicting future links social network. Katz measure metaapproaches based clustering low-rank approximation perform best threefive arXiv datasets, simple local measures common neighbors Adamic/Adaralso perform surprisingly well.Supervised learning methods also used combine augment similaritymetrics discussed. instance, Lichtenwalter et al. (2010) investigate severalsupervised methods link prediction sparsely labeled networks, using many metrics Table 3.2. metrics used features simple classifiers C4.5,J48, naive Bayes. find supervised approach leads 30% improvementAUC simple unsupervised link prediction metrics. Similarly, Kashima Abe(2006) propose supervised probabilistic model assumes biological networkevolved time, uses topological features estimate model parameters. evaluate proposed method protein-protein metabolic networksreport increased precision compared simpler metrics Adamic/Adar, PreferentialAttachment, Katz.3.2.4 Discussiongeneral, local topology metrics sacrifice amount accuracy computationalgains global graph metrics may perform better costly estimateinfeasible huge networks. appropriate, supervised methods combine multiplelocal metrics may offer promising alternative. next subsection discusses additionalwork link prediction used supervised methods.Link prediction using metrics especially sensitive characteristicsdomain application. instance, many networks biology, identification380fiTransforming Graph Data Statistical Relational Learninglinks costly, contain missing incomplete links, removal insignificant linkssignificant issue social networks. reason, researchers analyzedproposed many different metrics working domains web analysis (Kleinberg,1999; Broder et al., 2000), social network analysis (Zheleva, Getoor, Golbeck, & Kuter,2010; Xiang et al., 2010; Koren, North, & Volinsky, 2007), citation analysis (Borgman &Furner, 2002), ecology communities (Zhou et al., 2009), biological networks (Jeong et al.,2000), many others (Barabasi & Crandall, 2003; Newman, 2003).3.3 Hybrid Link Predictionsubsection, examine approaches perform link prediction usingattributes graph topology. approaches, two key questions. First,kinds features used? Second, information multiplefeatures combined single measure probability used prediction?first consider mix non-relational relational features used.expected, best features vary based domain specific network. instance,Taskar et al. (2003) studied link prediction network web pages found simplelocal topology metrics (which called transitivity similarity) importantnon-relational features based words presents pages. Similarly, Hasanet al. (2006) found another topology metric (shortest distance) usefulpredicting co-authorship links bibliographic network based DBLP.single metric/feature, hitting time, used link prediction,must ensure metric works well nodes yields consistent ranking.However, multiple feature values combined way, mayacceptable use wider range features, especially supervised learner later selectweight important features based training data. Thus, hybrid systemslink prediction tend diverse feature set. instance, Zheleva et al.(2010) propose new features based combining two different kinds networks (socialaffiliation networks). Features based groups topology constructedcombined network used along descriptive non-relational features, yieldingimprovement 15-30% compared system without combined-network features.second example complex features provided Ben-Hur Noble (2005),design new pairwise kernel predicting links proteins (protein-proteininteractions). pairwise kernel tensor-product two linear kernels originalfeature space, especially useful domains two nodes mightcommon features. approach also applied user preference predictionrecommender systems (Basilico & Hofmann, 2004). Vert Yamanishi (2005) proposerelated approach, supervised learning used create mapping originalnodes new euclidean space simple distance metrics used linkprediction.Given great diversity possible features link prediction, interesting approachsystem automatically searches relevant features use. example, Popescul,Popescul, Ungar (2003a) propose unique link prediction approach systematicallygenerates searches space relational features learn potential link predictors. use logistic regression link prediction consider search space covering381fiRossi, McDowell, Aha, & Nevilleequi-joins, equality selections, aggregation operations. approach, model selection algorithm continues add one feature time model long BayesianInformation Criterion (BIC) score training set improved. findsearch algorithm discovers number useful topology-based features, co-citationbibliographic coupling, well complex features. However, complexitysearching large feature space avoiding overfitting present challenges.next consider second key question: information multiplefeatures combined single measure used link prediction? priorwork taken supervised learning approach, non-relational topologybased metrics used features describe possible link. supervisedtechniques discussed Section 3.2, model learned training dataused predict unseen links.supervised approaches apply classifier separately possible link,using classifier support vector machine, decision tree, logistic regression(Popescul et al., 2003a; Ben-Hur & Noble, 2005; Hasan et al., 2006). approaches,flat feature representation link created, prediction madepossible link independent predictions.contrast, early work Relational Bayesian Networks (RBNs) (Getoor et al., 2003)Relational Markov Networks (RMNs) (Taskar et al., 2003) involved joint inferencecomputation link prediction, prediction could influenced nearby linkpredictions (and sometimes also newly predicted node labels). Using webpage networksocial network, Taskar et al. demonstrated joint inference using belief propagation could improve accuracy compared independent inference approach. However,approach computationally intensive, noted getting belief propagationalgorithm converge significant problem. possible solution computationalchallenge simpler approach presented Bilgic, Namata, Getoor (2007).method involved repeatedly predicting labels node, predicting linksnodes using available features (including predicted labels), re-predicting labelsnew links, forth. link prediction based independent inferencestep using logistic regression, simpler approaches discussed above. However,repeated application step allows possibility link feature values changingiterations based intermediate predictions, thus allowing link predictionsinfluence other.Recently, Backstrom Leskovec (2011) proposed novel approach supervised,final predictions based random walk rather directlyoutput learned classifier. Given particular target node v social network,along nodes known link v, study predictlinks v likely arise future (or recommended). definesimple link features based node profile similarity messaging behavior,use features estimate initial link weights. show learn weights(or transition probabilities) manner optimizes likelihood subsequentrandom walk, starting v, arrive nodes already known link v.random walk thus guided links already known exist, callprocess supervised random walk. argue learning process greatly reducesneed manually specify complex graph-based features, show outperforms382fiTransforming Graph Data Statistical Relational Learningsupervised approaches well unsupervised approaches Adamic/Adarmeasure.final approach link prediction use kind unsupervised dimensionalityreduction yields new matrix way reveals possible new links. instance,Hoff, Raftery, Handcock (2002) propose latent space approach initial linkinformation projected low-dimensional space. Link existence predictedbased spatial representation nodes new latent space. modelsperform kind factorization link adjacency matrix thus often referredmatrix factorization techniques. advantage models spatial representation enables simpler visualization human interpretation. Related approachesalso proposed temporal networks (Sarkar & Moore, 2005), mixed-membershipmodels (Nowicki & Snijders, 2001; Airoldi, Blei, Fienberg, & Xing, 2008), situationslatent vector representing node usefully constrained binary (Miller,Griffiths, & Jordan, 2009). Typically, models capability includingattributes covariates affect link prediction directly part latentspace representation. However, Zhu, Yu, Chi, Gong (2007) demonstrated attributes also represented related distinct latent space. recently, MenonElkan (2011) showed matrix factorization technique link prediction scalemuch larger graphs training stochastic gradient descent instead MCMC.3.4 DiscussionLink prediction remains challenge, part large number possiblelinks (i.e., N 2 possible links given N observed nodes), widely varying datacharacteristics. Depending domain, best approach may use single nonrelational metric topology metric, may use richer set features evaluatedlearned model. Future work may also wish consider using ensemble linkpredictors yield even better accuracy.discussion link prediction focused predicting new links based existinglinks properties nodes. context web, however, link predictionsometimes taken forms. instance, Sarukkai (2000) used web server tracespredict next page user visit, given recent browsing history. particular,use Markov chains, related random walks discussed Section 3.2,task also call link prediction. recently, DuBois Smyth (2010)model relational events (i.e., links) using latent classes event/link ariseslatent class properties event (i.e. sender, receiver, type) chosendistributions nodes conditioned assigned class. work, localcommunity node influences distribution computed node, way relatedcomputations stochastic block modeling (Airoldi et al., 2008). DuBois & Smythstask also form link prediction, goal predict presenceabsence static link, frequency occurrence possible event/link.One might also interested deleting pruning away noisy, less informative links.instance, friendship links Facebook usually extremely noisy since costadding friendship links insignificant. techniques used section could383fiRossi, McDowell, Aha, & Nevillealso used remove existing links wherever link prediction algorithm yieldslow score (or weight) observed link original graph.Indeed, since link prediction algorithms effectively assign score every possiblelink, could also used assign weight set initial links G.link weighting one three subtasks link interpretation shown taxonomyFigure 2. However, practice weights needed initial links, differentfeatures algorithms often possible and/or effective. next sectiondiscusses link weighting algorithms, well link interpretation general. Also,Section 7 discuss additional methods link prediction seek jointlytransform nodes links.4. Link InterpretationLink interpretation process constructing weights, labels, general featureslinks. three tasks link interpretation related somewhat overlapping. First,link weighting task assigning weight link. weights may representrelevance importance link, typically expressed continuous values.Thus weights provide explicit order links. Second, link labeling similar,except usually assigns discrete values link. could represent positivenegative relationship, could used, instance, assign one five topics emailcommunication flows. Finally, link feature construction process generating setdiscrete continuous features links. instance, features might countfrequency particular words appeared messages two nodes connectedlink, simply count number messages.sense, link feature construction subsumes link weighting labeling, sinceweights labels viewed simply possible link features discovered. However, many tasks makes sense compute one particular feature summarizesrelevance link (the weight) and/or one particular feature summarizes typelink (the label). weights labels may especially useful later processing, example collective classification. Moreover, techniques used generalfeature construction tend toward simpler approaches aggregation discretization, whereas best techniques computing weights labels may involve muchcomplexity, including global path computations supervised learning. reason,treat link weighting (Section 4.1) link labeling (Section 4.2) separately generallink feature construction (Section 4.3).4.1 Link WeightingGiven initial graph G = hV, E, XV , XE i, task assign continuous value (theweight) existing link G, representing importance influence link.previously discussed, link weighting could potentially accomplished applying linkprediction technique simply retaining computed scores link weights. instance,Lassez, Rossi, Jeev (2008) perform link prediction weighting applying singularvalue decomposition adjacency matrix, retaining k significantsingular-vectors (similar low-rank approximation techniques discussed Section 3.2).384fiTransforming Graph Data Statistical Relational Learningshow querying (e.g., PageRank) resultant weighted graph yieldrelevant results compared unweighted graph.Unlike link prediction, however, link weighting techniques designedwork links already exist graph. techniques dont workpredicting unseen links weight links based known properties/featuresexisting links, compute additional link features yieldsensible results links already exist.simplest case, link weighting aggregating intrinsic property links.example, Onnela et al. (2007) defines link weights based aggregated durationphone calls individuals mobile communication network. cases, simplycounting number interactions two nodes may appropriate.Thus, link features like duration, direction, frequency known,aggregated way generate link weights. actual link weights already knownlinks, supervised methods used weight prediction, usingknown weights training data. instance, Kahanda Neville (2009) predict linkstrength within Facebook dataset, stronger relationships identified basedusers explicit identification top friends via popular Facebook application.Gilbert Karahalios (2009) also predict link strength Facebook, form training data survey data collected 35 participants (yielding strength ratings2000 links). algorithms generate large number (50-70) featureslink network, learn predictive model via regression technique bagged decision trees, Kahanda Neville finds performs best amongseveral alternatives. Gilbert Karahalios generate features based profile similarity(e.g., two users similar education levels?) based user interactions (e.g.,frequently topics two users communicate?). find interaction features helpful, especially feature based number days sincelast communication event. Kahanda Neville use similar kinds features,term attribute-based transactional features, also add topological features (suchAdamic/Adar discussed Section 3.2) network-transactional (NTR) features.NTR features based communications users (e.g., numberemail messages exchanged) moderated way larger network context.moderation often takes form normalization, instance dampen influencenode sent large number messages many different friends. findNTR features far helpful prediction, manyfeatures also contribute overall predictive accuracy.training data sample link weights available, approaches basedparameterized probabilistic model still possible. However, since candidate link featureslonger evaluated training data, approaches must (manually)choose features use much carefully. instance, Xiang et al. (2010)examine link weight prediction two social network datasets (Facebook LinkedIn),use 5-11 features link. hypothesize relationship strength hiddencause user interactions, propose link-based latent variable model capturedependence. inference, use coordinate ascent optimization procedure predictstrength link. Since actual strength link known, predictiontasks domain cannot directly evaluate accuracy. However, Xiang et al. demonstrate385fiRossi, McDowell, Aha, & Nevilleusing link strengths produced method leads higher autocorrelationhigher collective classification accuracy predicting user attributes genderrelationship status.number researchers considered importance recency evaluating linkweight, assumption events interactions occurred recentlyweight. instance, Roth et al. (2010) propose Interactions Rank metricweighting link based messages two nodes. formula separatelyweights incoming outgoing messages link, imposes exponential decayimportance message based old is. Roth et al. use metricweight links call implicit social network, node representsgroup users. demonstrate metric used accurately predict usersmissing email distribution list. However, basic metric simplecompute could applied many tasks.Interactions Rank metric weights link heavily connects two nodesfrequently and/or recently communicated. Alternatively, Sharan Neville (2008)considered weight links graph links (such hyperlinksfriendships) may appear disappear time. particular, constructsummarized graph nodes links ever existed past present.link new graph weighted based kernel function provideweight links present often recently past. explainmodify standard relational classifiers use weighted links, demonstratevariety kernels (including exponential linear decay kernels) produce weightedlinks yield higher classification accuracy compared non-weighted graph.recently, Rossi Neville (2012) extended work handle time-varying attributevalues, may serve basis incorporating temporal dynamics additionaltasks.4.2 Link LabelingGiven initial graph G = hV, E, XV , XE i, task construct discrete labelone links G. labels used describe type relationshiplink represents. instance, Facebook example, link labeling algorithm maycreate labels representing work personal relationships. labels would enablesubsequent classification models separately account influence differentkinds relationships.prior work link labeling assumed text (such message) describes link, based unsupervised textual analysis techniquesLatent Dirichlet Allocation (LDA) (Blei et al., 2003), Latent Semantic Analysis (LSA)(Deerwester, Dumais, Furnas, Landauer, & Harshman, 1990), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999). Traditionally, techniques usedassign one latent topics document collection documents.topics formed defined implicitly probability distribution likelyword appear, given topic associated document. topicsalways semantically meaningful, often manual inspection revealsprominent topics represent sensible concepts advertising government re386fiTransforming Graph Data Statistical Relational Learninglations. However, even semantic associations obvious, inferringtopics set links still aid analysis, since topics identify linksrepresent similar kinds relationships.textual analysis techniques developed independent documents mind,inter-linked nodes, adapted label links several ways. instance,Rossi Neville (2010) examined messages developers contributing opensource software project. treat message separate document, use LDAinfer single likely latent topic message (i.e., link label). techniquecould used graph textual content associated links. Rossi Nevillealso go further, consider impact time-varying topics time-varying topic/wordassociations, running multiple iterations LDA, one per time epoch. Using model,study problem predicting effectiveness different developers (nodes)network. demonstrate accuracy predictions significantly improvedmodeling temporal evolution communication topics.McCallum, Wang, Corrada-Emmanuel (2007) describe alternative way extending LDA-like approaches link labeling. LDA essentially Bayesian networkmodels probabilistic dependencies documents, associated topics, words associated topics. propose extend model Author-RecipientTopic (ART) model, choice topic document (message) dependsauthor recipient message. parameters learnedmodel, inference (e.g., Gibbs sampling) used infer likely latenttopics message. make use topics assign roles people emailcommunication network, demonstrate outperforms simpler models.Supervised techniques also used link labeling. instance, Taskar et al.(2003) study academic webpage network consider predict node labels (suchStudent Professor) simultaneously predicting link labels (such adviserof). Given labeled training graph, learn complex Relational Markov Network(RMN) predict labels existence new links. make linkprediction tractable, candidate new links considered, linkssuggested textual reference, inside page, entity graph.RMN utilizes text-based features, instance based anchor text known linksheading HTML section possible link reference found.demonstrate RMNs joint inference nodes links improves performancecompared separate inference. However, learning inference RMNs oftensignificant challenge, practice limits number types featureconsidered.RMN approach learns training data uses joint inferenceentire graph. simpler supervised approach create set features linkuse features learning inference arbitrary classifier treatslink separately. Leskovec, Huttenlocher, Kleinberg (2010) study particular formapproach two link labels, representing positive negative relationship (such friendship vs. animosity). create link features based (signed)degree nodes involved link also based transitivity-like properties computed known labels nearby links. demonstrate approach using dataEpinions, Wikipedia, Slashdot, users manually indicated positive387fiRossi, McDowell, Aha, & Nevillenegative relationships users. Given network almost edges labeled,label classifier able predict label (positive negative) single unlabeled edgehigh accuracy. Interestingly, show classifiers predictive accuracyparticular dataset decreases slightly classifier trained different datasetvs. trained dataset used predictions. argue theoriesbalance status social psychology partially explain ability predictivemodels generalize across datasets. Unlike techniques discussedsection, work make use text-based features. However, general problempredicting sign link related sentiment analysis (or opinion mining)natural language processing (Godbole, Srinivasaiah, & Skiena, 2007; Pang & Lee, 2008).sentiment analysis algorithms could reformulated predict label (suchpositive negative) link given associated text.link two nodes established based many different kindsrelationships, many types algorithms could potentially usedlabeling links, even original algorithm designed purpose. instance,Markov Logic Networks (MLNs) used extract semantic networks text,yielding graph nodes represent objects concepts (Kok & Domingos, 2008).process produces relations teaches written nodes,could used link labels analysis. Another example Group-Topic(GT) model proposed McCallum, Wang, Mohanty (2007), which, like previouslymentioned ART model, Bayesian network. model intended graphs twonodes (such people) become connected participate event,voting yes political bill. Rather directly labeling links (likeART), GT model clusters nodes (such people) latent groups basedtextual descriptions events/votes. However, GT model also simultaneously infersset likely topics event, could used label implicit linksnodes. results model could also used add new nodes graphrepresent latent groups discovered.4.3 Link Feature ConstructionLink feature construction systematic construction features links, typicallypurpose improving accuracy understandability SRL algorithms. Link featureconstruction important many prediction tasks, received considerablyless attention node feature construction literature. Fortunately, manycomputations developed node feature construction also apply linkfeatures. avoid redundancy, defer analysis feature constructiondiscussion node feature construction Section 6.3. section briefly discussestechniques node feature construction applied links, summarizesmajor types link features computed.Section 6.3 later describe feature values relational data often basedaggregating values multiple nodes. instance, feature might computeaverage common feature value among neighbors particular node.aggregation-based features help account varying number neighborsnode may have. links, aggregation less essential, since (usually) link precisely388fiTransforming Graph Data Statistical Relational Learning(a) link-aggregation(b) link-aggregationFigure 4: Link Feature Aggregation Example: figure demonstrates unknown link feature value computed aggregating link feature valuessurrounding links. aggregation operator Mode.two endpoint nodes. However, aggregation still useful computing featurescollect information larger area graph. instance, Figure 4, link featurevalue computed link center subgraph (the target link).computation considers feature values (positive negative signs) linksadjacent target link. case, aggregation operator Mode,result new link feature value. example used link features input, nodefeature values (e.g., lightly-shaded nodes Figure 4) could also aggregatedform new link feature. way, aggregation operators discussed nodesSection 6.3 also applied links.Figure 5 summarizes kinds features constructed link. figureorganized around sources information go computing single link feature(i.e., inputs), rather details feature computation (such typeaggregation function used). bottom figure shows four types linkfeatures, represented subgraph. case, emphasized link bottomsubgraph target link new feature value computed.subgraphs shows varying amounts information displaysfeatures, nodes, and/or links used inputs kind link feature.simplest type non-relational link feature, computedlink solely information already known link. Thus, Figure 5A showsfeature values already known target link, usedconstruct new feature value. instance, message associated link,link feature could count number times certain word occurs, numberdistinct words. Alternatively, date associated link, feature mightcompute number months since link formed. Onnela et al. (2007) computedkind feature aggregated duration phone calls two peopleform new link feature (which also used link weight).remaining feature types relational, meaning depend waygraph (not single link). First, topology features (Figure 5B)computed using topology graph. feature might, instance,compute total number links adjacent target link. Likewise, KahandaNeville (2009) computed clustering coefficient pair linked nodes,measures extent two nodes neighbors common (Newman, 2003),well topological features Adamic/Adar measure discussed Section 4.1.389fiRossi, McDowell, Aha, & NevilleinputV,E,XV,XEtarget linkLink FeatureConstructionEXlink-valuepV,E,XV,XEnode-valueLNon-relationalLink FeaturesRelational FeaturesV,E V,E,XE V,E,XVTopologyFeaturesLink-valueFeaturesNode-valueFeaturesCpwCLwLA.B.EC.LD.XFigure 5: Link Feature Taxonomy: link feature classes non-relational features,topology features, relational link-value features, relational node-value features.subgraphs bottom, information potentially usedclass link feature (i.e., nodes V , links E, node features X V , and/or linkfeatures X E ) shown. emphasized link represents feature valuecomputed (i.e., target link).used link features help predict link strength, could also usedtasks.Next, relational link-value features computed using feature valuesnearby links. instance, Figure 5C shows link labels personal (p) work(w) might identified links adjacent target link. new link feature couldformed representing distribution labels, taking commonlabel, (when link features numeric) averaging. Leskovec, Huttenlocher,Kleinberg (2010) used link-value features working graphs linksign feature positive negative (as Figure 4). computed featuresbased signed-degree two nodes connected target link wellcomplex measures based paths two nodes (e.g., measure signtransitivity).390fiTransforming Graph Data Statistical Relational LearningFinally, relational node-value features computed using featurevalues nodes close attached target link. instance,Figure 5D shows node labels conservative (C) liberal (L) might identifiednodes close target link. link-value features, labels could usedcreate new feature value summarization aggregation. Often, two nodesdirectly attached target link used. instance, work GilbertKarahalios (2009) Kahanda Neville (2009) construct link features basedsimilarity two nodes social network profiles. However, feature values distantnodes could also used, instance compute new link feature based similarfriends two people (nodes) are.5. Node PredictionNode transformation includes node prediction (e.g., predicting existence new nodes)node interpretation (e.g., constructing node weights, labels, features). sectionfocuses node prediction, Section 6 considers node interpretation.Given graph existing nodes V , node prediction used two distinct ways.First, node prediction algorithm could used discover additional nodestype already present V . instance, given set peoplecommunicate via email, simple algorithm might used create new nodesrepresent email recipients implied messages, explicitly representedoriginal graph. Alternatively, supervised unsupervised machine learning techniquescould used discover, instance, new research papers people informationavailable web (Craven et al., 2000; Cafarella, Wu, Halevy, Zhang, & Wang, 2008).techniques valuable, certainly used add new nodes graph.However, work examined context general knowledge baseconstruction, rather relational learning.4focus second type node prediction, involves predicting nodesdifferent type already present graph. new nodes mightrepresent locations, communities (Kleinberg, 1999), roles (McCallum, Wang, & CorradaEmmanuel, 2007; Rossi, Gallagher, Neville, & Henderson, 2012), shared characteristics,social processes (Tang & Liu, 2009; Hoff et al., 2002), functions (Letovsky & Kasif, 2003),kind relationship. instance, running Facebook example,newly discovered node may represent common interest hobby multiple peopleshare. nodes usually referred latent nodes (and nodes connectednode form latent group).5 meaning nodes depend uponfeatures and/or links included input node prediction algorithm.instance, including work-based friendships lead different groupspersonal friendships considered.4. recent work Kim Leskovec (2011) exception. technique uses EM inferexistence missing nodes links based known topology graph.5. Prior work sometimes refers nodes hidden nodes, especially thoughtrepresent concrete characteristics, geographic location, could measured were,reason, observed data.391fiRossi, McDowell, Aha, & NevilleFigure 6: Alternative Representations Newly Predicted Groups: leftfigure shows new feature (with value X Y) could added node,right figure demonstrates creation two new nodes representgroups.many advantages type representation change regards accuracy understandability. instance, nodes directly connectedoriginal graph similar way become, links new nodes,closer graph space. Intuitively, nodes connected high level concept sharelatent properties representing latent structure directly impact classification,network analysis, many tasks. instance, reducing path lengthsimilar nodes enables influence nodes propagate effectively collectiveclassification (CC) performed nodes. model still learn exploitnew nodes relationships, even semantic meaning new nodesprecisely understood.popular methods predicting new nodes based clustering,context means grouping nodes nodes within group similarnodes groups. Typically, one new node createdgroup, links added existing node correspondinggroup node (see right side Figure 6). techniques may also associate nodemultiple groups, link weights representing affinity group.new groups discovered, whether via clustering via technique,alternative creating new nodes links simply add new feature(s) noderepresent group information. left side Figure 6 demonstrates alternative.instance, new node feature might represent running hobby, may simplyrepresent belonging discovered group #17, unknown meaning. PopesculUngar (2004) use CiteSeer dataset demonstrate technique derive featuresimprove predictive accuracy. advantage approach, opposed addingnew nodes, potentially enables simpler, non-relational algorithms make usenew information. potential disadvantage, though, also allowalgorithms CC propagate influence newly connected nodes, discussedabove. However, methods use general strategy generate much larger392fiTransforming Graph Data Statistical Relational Learningnumbers latent features used classification (Tang & Liu, 2009; Menon &Elkan, 2010). Tang & Liu demonstrate that, cases, resultant large numberlink-based features may make collective inference unnecessary obtaining good accuracy.Naturally, whether information discovered clusterings best representedvia new nodes new features depend upon dataset inference task.section, simplicity discuss algorithm assuming new nodescreated (even algorithm originally described terms creating new features).discussion link prediction, organize discussion around kindsinformation used prediction. Section 5.1 discusses non-relational (attributebased) node prediction, Section 5.2 discusses topology-based node prediction, Section 5.3 discusses hybrid approaches use node feature values topologygraph.5.1 Non-relational (Attribute-Based) Node Predictionmany clustering algorithms used cluster existing nodes usingnon-relational features (attributes), used add new nodesgraph. two primary types hierarchical clustering algorithms (e.g., agglomerativedivisive clustering) partitioning algorithms k-means, k-medoids (Berkhin, 2006;Zhu, 2006), EM-based algorithms, self-organizing maps (Kohonen, 1990).discuss algorithms since well studied non-relational dataeasily applied relational data clustering based attribute valuesdesired.5.2 Topology-Based Node Predictiontechniques described section link existing nodes one new nodes (i.e.,latent groups), based original link structure graph. cases, findinggrouping depends upon computing kind similarity metric every pairnodes. Two key questions thus serve identify techniques. First, kindsimilarity metric used? Second, metric used predictgroupings? address question turn.5.2.1 Types Metrics Group Predictiontype topology-based link weighting metric (see Table 3.2) could conceivably usedlatent node prediction. metric suitable long produces high valuespairs nodes belong group lower values pairs.instance, high value Katz metric (see Section 3.2) indicates two nodesmany short paths them, thus may belong group. Metricsrepresenting distance rather similarity also used negating metric.instance, Girvan Newman (2002) focus detecting community structure extendingconcept node-betweenness links. Intuitively, network contains latent groupsloosely connected intergroup links, shortest pathsdifferent groups must go along links. links connect different groupsassigned high link-betweenness value (which corresponds low similarity value).393fiRossi, McDowell, Aha, & Nevilleunderlying group structure trivially revealed removing linkshighest betweenness.idea using link-betweenness relational clustering extendednumber directions. instance, Newman Girvan (2004) introduced random-walkbetweenness, expected number times random walk pairnodes pass particular link. addition, Radicchi, Castellano, Cecconi, Loreto,Parisi (2004) proposed using link-based clustering coefficient metric. showedmetric performs comparably original link-betweenness metric GirvanNewman, much faster local graph measure instead global graphmeasure.Zhou (2003) describes new metric, dissimilarity index, computedfollows. node i, compute vector di value dij representsdistance node node j (Zhou measures distance based average numbersteps needed random walk starting node reach node j, distance metriccould used). nodes k similar, similar distancevectors. Thus, dissimilarity index nodes k defined based Euclidean-likedistance computation vectors di dk . Zhou demonstrates techniqueoutperforms link-betweenness approach Girvan & Newman random modularnetworks.Relatively simple metrics often lead useful results. instance, Ravasz et al.(2002) used simple clustering coefficient metric study metabolic networks. studyreveals metabolic networks forty-three organisms organized many small,highly-connected modules. Furthermore, find E. coli, hidden hierarchicalmodularity closely overlaps known metabolic functions.5.2.2 Using Metrics Group Predictionsimplest techniques identifying new groups perform kind hierarchicalclustering. instance, similarities weights computed every pairnodes, links removed graph. Next, weighted links placednodes one one, ordered weights. intuition varying degreesclusters formed links added. particular, approach forms hierarchicaltree leaves represent finest granularity clustering every nodeseparate cluster. move tree larger clusters formed, reach topnodes joined one large cluster. type hierarchical approachused work Zhou (2003). Girvan Newman (2002) use similar strategy,start instead original graph iteratively remove less similar linksgraph reveal underlying community structure. challenge approaches,clustering general, select appropriate number final clusters,corresponds selecting level clustering tree.Spectral clustering (Dhillon, 2001; Ng, Jordan, & Weiss, 2001; Kamvar, Klein, & Manning, 2003) also used group identification. Spectral clustering relies upon computing similarity matrix describes data points, transforming matrixway yields new matrix U clustering rows U using simple clusteringalgorithm (such k-means) trivially identify interesting groups data.394fiTransforming Graph Data Statistical Relational Learningmatrix transformation several variants, involves computing kind LaplacianS, computing eigenvectors resultant matrix using eigenvectors represent original data. motivation transformation seenidentifying good graph cuts original graph (those yield good separationshighly-connected nodes groups) identifying nodes closely relatedterms random walks; see work von Luxburg (2007) overview. Spectralclustering originally applied non-relational data, but, hierarchical techniques described above, applied relational data using link-based metricscomputing similarity matrix. instance, Neville Jensen (2005) use nodeadjacency matrix spectral clustering technique described Shi Malik (2000)identify latent groups graphs. show technique enables simplerinference (since group handled separately), ultimately yields accurateclassification compared approaches ignore group structure. Tang Liu (2011)also use spectral clustering link graph, order create much largernumber latent features used learn supervised classifier. Unlikelatent groups work Neville Jensen, technique allows nodeassociated one cluster output spectral clustering, Tang& Liu claim leads improved classification accuracy. Spectral clustering also usedcomplex similarity metrics, described next subsection.Techniques borrowed web search also useful node prediction. instance, given adjacency matrix webpage graph, Hits algorithm (Kleinberg,1999) computes first eigenvectors AAT A, representauthoritative nodes (the authorities) well prominent nodes point (thehubs). Normally, algorithm used find single prominent community authorities hubs (to assist web search), secondary communitiesdiscovered also considering non-principal eigenvectors AAT (Gibson, Kleinberg, & Raghavan, 1998). node prediction algorithm could treatcommunity latent group add new node links represent group.techniques may especially useful detecting patterns influence graphadding explicit links represent influence.5.3 Hybrid Node Predictiontechniques previous section added new nodes graph, often basedclustering, using topology graph. principle, technique also usednodes attributes produce meaningful latent groups/nodes. sectionconsiders add attribute information techniques node prediction.simple approach define kind similarity metric combines nonrelational topology-based similarity single value, provide similaritymetric one previously mentioned clustering algorithms. instance, Neville,Adler, Jensen (2004) use weighted combination attribute link information1XS(i, j) =sk (i, j) + (1 ) lkkmetric, sk (i, j) = 1 iff nodes j value kth attribute,l = 1 iff link exists j. constant controls relative395fiRossi, McDowell, Aha, & Nevilleimportance attributes vs. links. use metric NCut spectralclustering technique add new nodes graph, demonstrate additionalnodes increase performance relational classification. similar weighted combinationattribute link-based similarity used Bhattacharya Getoor (2005) entityresolution.Attribute-based information also incorporated ad-hoc basis. instance,Adibi, Chalupsky, Melz, Valente, et al. (2004) describe group finding algorithminitial seed set clusters formed based handcrafted set logical rules,clusters refined using probabilistic system based mutual information.system, logic-based component primarily uses attributes node (person),probabilistic system primarily uses links describe connectionspeople. However, components make use attributes links.principled approach define kind generative model representsdependence observed attributes links latent group nodes, usemodel estimate group membership. instance, Kubica, Moore, Schneider,Yang (2002) define generative model node belongs one groups,group members tend link other. particular, use group membershipchart track whether node belongs group, local search possiblestates chart (using stochastic hill climbing) try identify membership changeswould better explain known data. step, maximum likelihood usedestimate parameters model. demonstrate usefulness techniquenews articles, webpages, synthetic data.Generative models also used sophisticated inference. example,Taskar, Segal, Koller (2001) treat group membership latent variable usesloopy belief propagation implicitly perform clustering nodes. Likewise, MixedMembership Relational Clustering (MMRC) (Long et al., 2007) uses EM variants estimate group memberships. particular, uses first round hard clustering (whereobject assigned exactly one cluster), following round soft clustering continuous strength values associated membership assignment. Mixed membership stochastic blockmodels (Airoldi et al., 2008) also assign continuous group membershipvalues node, use topological information (not attributes) groupassignments use variational inference techniques generative model. Finally,Long, Zhang, Wu, Yu (2006) demonstrate node clustering performed instead using spectral clustering, focuses particularly simultaneously clustermultiple types nodes (e.g., simultaneously cluster web pages web users twodistinct sets groups).group prediction algorithms assume links likely connect nodesbelong group. exception work Anthony desJardins (2007),also use generative model links attributes depend latent groupmemberships, types links likely occur nodesbelong group. instance, note groups social networkdefined gender, link representing dating likely connect two nodesdifferent groups.396fiTransforming Graph Data Statistical Relational LearningFigure 7: Lifted Graph Representation: initial graph G clustered transformed lifted graph representation G. lifted graph representationcreated clustering nodes, links, both.5.4 Discussiontechniques described produce single clustering nodes, usuallybased assigning every node single group. contrast, multi-clustering emergingresearch area aims provide multiple orthogonal clusterings complex data (Strehl& Ghosh, 2003; Topchy, Law, Jain, & Fred, 2004). instance, individuals Facebookmight clustered multiple ways latent node types might represent friend groups,work relations, socioeconomic status, locations, family circles. type multi-clusteringperformed McCallum, Wang, Corrada-Emmanuel (2007) latent nodescreated based roles topics. addition, Kok Domingos (2007) propose Statistical Predicate Invention (SPI), node transformation approach based Markov LogicNetworks (Richardson & Domingos, 2006). SPI clusters nodes, features links forming basis prediction predicates (or potential nodes). SPI considers multiplerelational clusterings based observation multiple distinct clusterings maynecessary to, instance, group individuals based friendships work relationships. demonstrate MLN inference estimate clusters improvesperformance compared two simpler baselines. similar node prediction approach appliesMLNs role labeling (Riedel & Meza-Ruiz, 2008).Node deletion may also useful cases. instance, node deletion mightbeneficial removing outdated spurious nodes graph. Alternatively,may multiple nodes represent real-world object concept, casedeletion purposes entity resolution important (Pasula, Marthi, Milch,Russell, & Shpitser, 2003; Bhattacharya & Getoor, 2007; Singla & Domingos, 2006).Finally, node representation changes used improve accuracy,also yield graphs processed efficiently desirableproperties. Section 5.2 already discussed Neville Jensen (2005) used additionlatent nodes enable simpler inference. Another possibility creation super-nodesrepresent one original nodes. instance, Figure 7 demonstratesfive original nodes can, clustering, collapsed three super-nodes, yielding liftedgraph representation. kind representation change used efficient397fiRossi, McDowell, Aha, & Nevilleinference Markov Logic Networks (see Section 6.3) network anonymization (seeSection 8.6).6. Node InterpretationNode interpretation process constructing weights, labels, general featuresnodes. symmetric tasks link interpretation, node weighting seeksassign continuous value node, representing nodes importance, nodelabeling seeks assign discrete value link, representing type, group, classnode. Likewise, node feature construction process systematically generatinggeneral-purpose node features based on, instance, aggregation, dimensionality reduction,subgraph patterns.discussed Section 4 links, node feature construction could viewed subsuming node weighting node labeling, since general feature construction could alwaysused construct feature values treated weights labels nodes.practice, however, techniques used tend rather different. instance, PageRankoften used node weighting supervised classification often used node labeling,techniques rarely used general feature construction. Nonetheless, nodeinterpretation (more link interpretation) substantial overlap techniques actually used weighting labeling vs. used generalfeature construction. Below, first discuss node weighting Section 6.1 labelingSection 6.2. Section 6.3 discusses node feature construction, mentioning brieflyrelevant techniques previously discussed weighting labeling.6.1 Node WeightingGiven initial graph G = hV, E, XV , XE i, task assign continuous value (theweight) existing node G, representing importance influence node.Node weighting techniques used information retrieval, search engines, socialnetwork analysis, many domains way discover important nodesrespect defined measure. node prediction classified basedwhether use node attributes, graph topology, constructweighting.6.1.1 Non-relational (Attribute-Based) Node Weightingsimplest node weighting techniques use node features XV (i.e., attributes).instance, nodes representing documents might weighted based numberquery-relevant words contain, nodes representing companies might rankedbased gross annual sales. Many sophisticated strategies also considered. instance, Latent Semantic Indexing (Deerwester et al., 1990) usedidentify important semantic concepts corpus text, nodes rankedbased connection concepts. methods extensively appliedquantify rank importance scientific publications (Egghe & Rousseau, 1990).However, techniques extensively studied elsewhere also ignoregraph structure (such citations), discuss here.398fiTransforming Graph Data Statistical Relational Learning6.1.2 Topology-Based Node WeightingSeveral node weighting algorithms use topology graph developedsupport early search engines. Examples kind algorithm include PageRank(Page et al., 1999), Hits (Kleinberg, 1999), SALSA (Lempel & Moran, 2000).algorithms rank relative importance web sites, conceptually basedkind eigenvector analysis (Langville & Meyer, 2005), though practice iterative computation may used. instance, PageRank models web Markov Chainimplemented systematically computing principal eigenvector limk Ak eadjacency matrix e unit vector. Hits, previously described, instead computes principal eigenvectors AAT A. algorithms continueimportant webpage ranking, also applied many kindsgraphs (Kosala & Blockeel, 2000).social network analysis, objective topology-based node weighting typicallyidentify influential significant individuals social network.variety centrality measures devised use local global network structure characterize importance individuals (Wasserman & Faust, 1994). Examplesmetrics include node degree, clustering coefficient (Watts & Strogatz, 1998), betweenness (Freeman, 1977), closeness (i.e., distance/shortest paths), eigenvector centrality (Bonacich & Lloyd, 2001), many others (Jackson, 2008; Newman, 2010; Sabidussi,1966). addition, White Smyth (2003) considered compute relative noderankings, i.e., rankings relative set particularly interesting nodes. showcompute relative rankings metrics based shortest paths wellMarkov chain-based techniques (e.g., produce PageRank priors). addition,similarity metrics described Table 3.2 alternatively formulatedcomputing weights nodes.recently, node weighting techniques extended measure relativeimportance nodes temporally-varying data. instance, Kossinets, Kleinberg,Watts (2008) Tang et al. (2009) define notions temporal distance basedanalysis frequently information exchanged nodes. informationused define range new graph metrics, global temporal efficiency, local temporal efficiency, temporal clustering coefficient (Tang et al., 2009). recently,Tang, Musolesi, Mascolo, Latora, Nicosia (2010) define notions temporal betweennesstemporal closeness. argue incorporating temporal informationmetrics provides better understanding dynamic processes networkaccurately identifies important nodes (people). metrics primarily concern networks time-varying interactions (e.g., communications people),could also applied types data intermittent interactionsnodes nodes/link join leave network time. metrics alsoapply links, could possibly used improve link prediction algorithms.6.1.3 Hybrid Node Weightingalso hybrid node weighting approaches use attributes graphtopology (Bharat & Henzinger, 1998; Cohn & Hofmann, 2001). instance,various approaches modify Hits (Chakrabarti, Dom, Raghavan, et al., 1998; Bharat399fiRossi, McDowell, Aha, & Neville& Henzinger, 1998) PageRank (Haveliwala, 2003) construct node weights basedcontent links. Topic-Sensitive PageRank (Haveliwala, 2003) seeks computebiased set PageRank vectors using set representative topics. Alternatively, Kolda,Bader, Kenny (2005) propose TOPHITS, hybrid approach adds anchor text (i.e.,clickable text hyperlink) adjacency matrix representation used Hits.use higher-order analogue SVD known Parallel Factors (PARAFAC)decomposition (Harshman, 1970) identify key topics graph wellimportant nodes. hybrid approaches proposed SimRank (Jeh& Widom, 2002), Topical methods (Haveliwala, 2003; Nie, Davison, & Qi, 2006; Kolda& Bader, 2006), Probabilistic HITs (Cohn & Chang, 2000), many others (Richardson& Domingos, 2002; Lassez et al., 2008). Section 7 discusses relevant workcontext joint node link transformation techniques.Recently, node weighting approaches applied Adversarial Information Retrieval (AIR) detect moderate influence spam web sites. Typically, techniques produce weights using topology graph information,necessarily kind attribute information used techniques discussedabove. instance, TrustRank (Gyongyi, Garcia-Molina, & Pedersen, 2004) basedPageRank uses set trusted sites evaluated humans propagate trustlocally reachable sites. hand, SpamRank (Benczur, Csalogany, Sarlos, &Uher, 2005) measures amount undeserved PageRank analyzing backlinkssite. algorithms try identify link farms link spam alliances (Wu& Davison, 2005), given seed set known link farm pages. Among AIR methods,TrustRank widely known suffers biases human-selected settrustworthy sites may favor certain communities others.6.2 Node LabelingGiven initial graph G = hV, E, XV , XE i, task assign discrete labelnodes G. first discuss labeling techniques based classification,consider unsupervised textual analysis techniques.many cases, node labeling may considered end itself. instance,running Facebook example, stated goal predict political affiliationnode label already known. cases, however, node labelingproperly understood representation change supports desired task.instance, definitions anomalous link detection (Rattigan & Jensen, 2005),estimated node labels would allow us identify links nodes whose labels indicaterarely, ever, connected. Alternatively, datasets estimating nodelabels may enable us subsequently partition data based node type, enabling uslearn accurate models type node.Even node labeling final goal, Facebook example, intermediatelabel estimation may still useful representation change. particular, Kou Cohen(2007) describe stacked model relational classification relabels training setestimated node labels using non-relational classifier. use estimatedlabels learn new classifier (one uses attributes relational features),use new classifier perform relational classification test graph. approach400fiTransforming Graph Data Statistical Relational Learningyields high accuracy, comparable much complex algorithms collectiveclassification (CC). Fast Jensen (2008) analyze result discussexplained natural bias CC algorithms: training performed givennode labels inference depends part estimated labels (McDowell, Gupta, &Aha, 2009). Stacked models compensate bias instead training relabeled(estimated) training set. addition, inference new classifier needs singlepass test graph, yielding much faster inference CC techniques like Gibbssampling belief propagation. recently, Maes, Peters, Denoyer, Gallinari (2009)extend ideas node relabeling order generate larger training set via multiplesimulated iterations classification. show cases approachoutperform stacked models CC algorithms like Gibbs sampling.Thus, multiple reasons creating new labels nodes graph.labeling accomplished relational-aware algorithms like describedwell earlier algorithms used relational collective classification (Chakrabarti,Dom, & Indyk, 1998; Neville & Jensen, 2000; Taskar et al., 2001; Lu & Getoor, 2003;Macskassy & Provost, 2003). Node labeling course also done traditional,non-relational algorithms SVM, decision trees, kNN, logistic regression, NaiveBayes, among various others (Lim, Loh, & Shih, 2000; Michie, Spiegelhalter, Taylor, &Campbell, 1994; Burges, 1998; Cristianini & Shawe-Taylor, 2000; Joachims, 1998).methods simply use features XV exploit topology link-structure.techniques assign new labels via supervised learning. Labels alsoassigned via unsupervised techniques textual analysis. many networksreal-world contain textual content social networks, email/communicationnetworks, citation networks, many others. Traditional textual analysis modelsLSA (Deerwester et al., 1990), PLSA (Hofmann, 1999) LDA (Blei et al., 2003)used assign node topic representing abstraction textual information.recent techniques Link-LDA (Erosheva, Fienberg, & Lafferty, 2004) LinkPLSA (Cohn & Hofmann, 2001) aim incorporate link structure traditionaltechniques order accurately discover nodes type.6 particular, workCohn Hofmann demonstrate technique produce accurate nodelabels techniques use node attributes link topology.also sophisticated topic models developed specific taskssocial tagging (Lu, Hu, Chen, & ran Park, 2010) temporal data (Huh & Fienberg,2010; & Parker, 2010).6.3 Node Feature ConstructionNode feature construction systematic construction features nodes, typicallypurpose improving accuracy understandability SRL algorithms. Featureconstruction common relational representation change, frequentlydone performing task classification. instance, performing CCclassify nodes example Facebook political affiliation task, likely compute6. names Link-LDA Link-PLDA come work Nallapati, Ahmed, Xing, Cohen(2008), original papers describing techniques.401fiRossi, McDowell, Aha, & Nevillenew features representing information node (e.g., age bracket?)known information nodes neighbors (e.g., many liberal?).Different techniques node feature construction described many previousinvestigations, though feature construction necessarily focus manyinvestigations. section, summarize explain different aspects featureconstruction. particular, Section 6.3.1 presents discusses taxonomy featuresbased kinds inputs, topology information link feature values,use computing new feature values. Next, Section 6.3.2 describes possible operators, aggregation discretization, applied inputs. Finally,Section 6.3.3 examines perform automatic feature search selection supportdesired computational task.6.3.1 Relational Feature Inputsnode feature categorized according types information usescomputing feature values. possible information use includes set nodes Vlinks E, node features XV , link features XE . Figure 8 shows taxonomynode features based sources information (the inputs) use.taxonomy consistent distinctions previously made literature (e.g., non-relational relational features), best knowledgecomplete taxonomy never previously described. taxonomy consistsfour basic types: non-relational features three types relational features (topology features, relational link-value features, relational node-value features).describe give examples each.Non-relational Features: node feature considered non-relational featurevalue feature particular node computed using non-relationalfeatures (i.e., attributes) node, ignoring link-based information. instance, Figure 8A shows node corresponding nodes feature vector. newfeature value might constructed vector using kind dimensionality reduction, adding together several feature values, thresholding particularvalue, etc.Topology Features: feature considered topology-based feature valuesfeature computed using nodes V links E, ignoring existingnode link feature values. instance, Figure 8B, new feature valuecomputed node bottom left figure (the target node), usingtopological information shown. particular, new feature value might countnumber adjacent nodes, count many shortest paths graph passtarget node.Relational Link-value Features: feature considered relational link-valuefeature feature values links adjacent target nodeused computing new feature. Typically, kind aggregation operatorapplied values, count, mode, average, proportion, etc. instance,Figure 8C, values links shown represent communication topics (workpersonal), new link-value feature might compute mode values (p).402fiTransforming Graph Data Statistical Relational LearninginputV,E,XV,XEtarget nodeNode FeatureConstructionXVlink-valuepV,E,XV,XEnode-valueLNon-relationalNode FeaturesRelational FeaturesV,E V,E,XE V,E,XVTopologyFeaturesLink-valueFeaturesNode-valueFeaturesCpLp.5 PA.wB.VC.LD.XFigure 8: Node Features Taxonomy Based Inputs Used: classes nodefeatures non-relational features, topology features, relational link-value features, relational node-value features. classes defined respectrelational information used construction features (i.e., nodesV , links E, node features XV , link features XE ). double-lined target noderepresents new feature value computed. Parts C showsingle feature value link node simplicity, generalone feature may exist used.Usually computation include links directly connected targetnode, links hops away could also used.Relational Node-value Features: feature considered relational node-valuefeature feature values nodes linked target node used construction. Links used identifying nodes, although nodesone hop away target node may also included. instance, Figure 8Dshows feature values adjacent nodes (C L) could, instance,used compute new node-value feature based mode (L) values.Alternatively, one feature might count number adjacent C nodes anothermight count number adjacent L nodes.403fiRossi, McDowell, Aha, & NevilleFeature computation may also applied recursively. instance, ReFeX system (Henderson, Gallagher, Li, Akoglu, Eliassi-Rad, Tong, & Faloutsos, 2011) first computes features every node based degree (a topology-based feature), considers recursive combinations features (such mean out-degree nodesneighbors). Henderson et al. show recursive features often improve classification accuracy datasets network structure predictive. Alternatively,topology-based feature betweenness might computed, relational nodevalue feature might compute average betweenness nodes neighborstarget label C. example hybrid feature usesnode-value topology-based information.Another interesting aspect relational features potential feature value recomputation. particular, many techniques collective classification involve computingnode feature (such number neighbors currently labeled C) featuredepends feature values estimated (e.g., predicted node labels)thus may change (Jensen et al., 2004; Sen et al., 2008). addition, McDowell, Gupta,Aha (2010) describe features similar need recomputation,meta-features use depend upon estimated label probabilities nodeneighborhood target node. contrast, kind feature re-computation muchless applicability non-relational data, nodes assumed independentother. However, occur techniques semi-supervised learningco-learning.6.3.2 Relational Feature Operatorsprevious section described features according different kinds inputsuse feature value computation, whereas section describes different operatorsused computation. Table 5 summarizes operators.cases, operator used many different types relational input. instance,aggregation operators computed using graph topology, relational node-valueinputs, and/or relational link-value inputs, indicated appropriate checkmarksTable 5. contrast, path walk-based operators generally use graph topology;operators, lighter colored checkmarks Table 5 indicate path/walk-basedoperators could sensibly used conjunction relational link-value node-valuesinputs, rarely ever done. discuss operatorsTable 5 detail.Relational Aggregates: Aggregation refers function returns single valuecollection input values set, bag, list. classical statisticalaggregation operators Average, Mode, Exists, Count, Max, Min, Sum (Neville& Jensen, 2000; Lu & Getoor, 2003). SRL, another frequent operator Proportion,computes, instance, fraction nodes neighbors meet criterialabel C (McDowell, Gupta, & Aha, 2007). operators may alsocombined thresholds, e.g., evaluate whether Count nodes neighborslabeled C least 3. thresholding turns numerical aggregate Booleanfeature, needed tree-based algorithms (Neville, Jensen, Friedland, et al., 2003).Perlich Provost (2003) describe set complex relational aggregates depend404fiTransforming Graph Data Statistical Relational LearningRelational aggregatesMode, Average, Count, Proportion, Degree, ...Temporal aggregatesExponential/linear decay, union, ...XSet operatorsUnion, intersection, multiset, ...XClique potentialsDirect link cliques, co-citation cliques, triads, ...Subgraph patternsTwo star, three-star, triangle (i.e., transitivity), ...Dimensionality reductionPCA, SVD, Factor Analysis, Principal Factor Analysis, Independent Component Analysis, ...Path/walk-based measuresBetweenness, common neighbors, Jaccards coefficient, Adamic/Adar, shortest paths, random-walks,...Textual analysisLSA, LDA, PLSA, Link-LDA, Link-PLSA, ...XRelational clusteringSpectral partitioning, Hierarchical clustering, Partitioning relocation methods (k-means, k-medoids),...XXRelational Node-valueExample TechniquesRelational Link-valueRelational OperatorsTopologyNon-relationalInputsXXXXXXXXXXXXXXXXXXXXXXXXTable 5: Relational Feature Operators: Summary popular types relational feature operators. check used indicate classes inputs (seeSection 6.3.1) operator naturally uses constructing feature values, lighter check indicates operator could sensibly usedinput combination rarely ever used.405fiRossi, McDowell, Aha, & Nevilledistribution attribute values associated node (e.g., via linksrelational join). instance, aggregates may use function edit distancecompare nodes distribution reference distribution computed trainingdata. Perlich Provost demonstrate aggregations cases improveperformance compared simpler alternatives. also aggregate operators usetopology-based information. instance, operator Degree, simply countsnumber adjacent links, predictive feature, applied carefullyrelational data avoid bias (Jensen, Neville, & Hay, 2003).Temporal Aggregates: Relational information might also contain temporal informationform timestamps durations links, node, features. general, datahandled defining special temporal-aggregation features computed rawdata (McGovern, Collier, Matthew Gagne, Brown, & Rodger, 2008) defining graphsummarizes temporal information (usually decreasing importanceless recent information) (Sharan & Neville, 2008; Rossi & Neville, 2010). Rossi Nevillediscuss example latter approach, explore impact using varioustemporal-relational information various kernels summarization. Alternatively, Section 6.1 discusses notions temporal distance used modify path/walk-basedmetrics node betweenness closeness.Set Operators: traditional domain-independent set operators set union,intersection, difference applied construct features (Kohavi & John, 1997).instance, two attributes represent presence wordpage (node), new feature might represent case page containswords (i.e., feature intersection). relational data, complex set-basedfeatures possible. instance, feature collective classification might representunion class labels nodes adjacent target node. Neville, Jensen,Gallagher (2003) propose complex approach feature value multisetrepresents complete distribution adjacent nodes labels (e.g., {3C, 2M, 5L}indicate labels ten adjacent nodes). Using feature representation, showindependent-value approach assumes labels independentlydrawn distribution yields effective relational classification. Recently,McDowell et al. (2009) showed that, CC, multiset approach usually outperformedtypes features proportion count-based aggregates discussed above.Clique Potentials: probabilistic models Relational Markov Networks (RMNs)(Taskar et al., 2002) perform inference related nodes without computing aggregates.Instead, use clique-specific potential functions represent probabilistic dependencies, product term probability computation naturally expands accommodatevarying number neighbors node. one sense, featureless approach,since need choose relational aggregation function. However, different kindsdependencies still represented different cliques. instance, Taskar et al.consider different sets cliques webpage classification: one based hyperlinks,including information based links appear within page. Likewise, laterwork added additional types cliques enable link prediction (Taskar et al., 2003). Thus,even models remain important feature choices made.406fiTransforming Graph Data Statistical Relational LearningFigure 9: Subgraph Patterns Link Labels. subgraph represents possiblepattern particular feature could look relation target node (thebottom-left node case).probabilistic models also use link-based information without computing explicitfeatures, random walk-based classifier Lin Cohen (2010) weightedneighbor approach Macskassy Provost (2007). Even cases, however, choicesremain types links use. instance, webpage graphs, co-citationlinks may predictive class labels direct links (Macskassy & Provost, 2007;McDowell et al., 2009).Subgraph Patterns: subgraph pattern feature one based existenceparticular pattern graph adjacent target node. feature might countmany times particular pattern exists target node, produce value trueleast one pattern exists. simplest pattern called reciprocity; truetarget node links node j j links back i. cases, however, patternscomplex involve nodes. Robins, Pattison, Kalish, Lusher (2007)define many patterns including two-star (a node least two links), three-star (anode least three links), triangle (also known transitivity, j kk). patterns defined directed undirected links.Many patterns possible. instance, Robins, Snijders, Wang, Handcock(2006) use subgraph patterns probabilistically modeling graphs. argue usingcomplex patterns alternating k-triangle (based finding k trianglesshare common side) help avoid degeneracy might otherwise arisegraph generation. Furthermore, subgraph patterns also extended exploit labelslinks and/or nodes. instance, assume links labeled 1 2 (representing different topics) links labeled plus minus sign (representingpositive negative relationships). Figure 9 demonstrates three possible subgraph patterns,based different link labelings, relative target node shown bottom leftsubgraph. subgraph feature could compute, node, number matchesone patterns, feature could used later analysis.Dimensionality Reduction goal dimensionality reduction find lower kdimensional representation initial n features (Sarwar, Karypis, Konstan, & Riedl,2000; Fodor, 2002). formally, given initial n-dimensional feature vector x ={x1 , x2 , ..., xn }, find lower k-dimensional representation x x = {x1 , x2 , ..., xk }k n significant information original data captured, according criterion. many dimensionality reduction methods Principal407fiRossi, McDowell, Aha, & NevilleComponent Analysis (PCA), Principal Factor Analysis (PFA), Independent ComponentAnalysis (ICA).Dimensionality reduction techniques applied adjacency matrixgraph G create low-dimensionality graph representation; Section 3.3 explainedused link prediction. techniques also useful feature computation.instance, Bilgic, Mihalkova, Getoor (2010) investigate active learning improveaccuracy collective classification. technique involves non-relationalrelational features, demonstrate first applying dimensionality reduction (withPCA) non-relational features simplifies learning, leading substantial gains accuracy.Operators: mention briefly operators already discussed extensively elsewhere. Path-based measures (such betweenness distance)walk-based measures (such PageRank) discussed Sections 6.1.types measures used features classifier predict links (Lichtenwalteret al., 2010) well validating relational sampling techniques (Leskovec, Chakrabarti,Kleinberg, Faloutsos, & Ghahramani, 2010; Moreno & Neville, 2009; Ahmed, Neville, &Kompella, 2012a, 2012b). measures typically use topology (not features), one could easily imagine computing metrics based, instance, pathsedge particular label type. Textual analysis techniques discussedSections 4.2 6.2, relational clustering techniques discussed Section 5.operators used specifically node/link prediction, weighting, labeling,also used general feature construction.Finally, operators based similarity measures. Similarity twonodes often computed, instance link prediction (Section 3) weighting (Section 4.1). computations easily lead feature value link, since linkobviously refers two endpoint nodes compared. However, computingnode feature value, usually obvious node comparison, similarity measures typically used node feature values. measures can, however, usednode prediction, Section 5 discusses cases newly discovered nodes/groupsused create new node features. particular instance relational similarityfunctions, graph kernels structured data (Gartner, 2003) also used. kernelsused either nodes single graph (Kondor & Lafferty, 2002)compute similarity two graphs (Vishwanathan, Schraudolph, Kondor, & Borgwardt, 2010). instance, former type kernel another technique could alsoused link group prediction.Discussion: Many feature operators discussed naturally used compute feature values links additions nodes. instance, textual analysis appliedlinks text associated link, node-centered path-based measuresanalogous formulations links. One difference nodes naturally may linkmany nodes, whereas assume links two endpoints. Thus, relational aggregates Count initially seem useful computing link features. However,Figure 4 previously demonstrated link-aggregation accomplished broadening computation include multiple links nodes logically connectedendpoint node target link. Naturally, feature inputs operators408fiTransforming Graph Data Statistical Relational Learningbetter suited computing node features vs. computing link features. next sectionexamines select appropriate features given task.6.3.3 Searching, Evaluating, Selecting Relational FeaturesGiven large number possible features could used task (suchexample Facebook classification task), features actually used learnmodel? cases, selection done manually based prior experience trialerror. many situations, though, automatic feature selection desirable.non-relational data, widely studied topic machine learning (Guyon &Elisseeff, 2003; Koller & Sahami, 1996; Yang & Pedersen, 1997; Dash & Liu, 1997; Jain& Zongker, 1997; Pudil, Novovicova, & Kittler, 1994), selecting relational featuresreceived considerably less attention. Given large number possible features, efficientstrategies searching evaluating possible features needed. section,first summarize two key problems feature search feature evaluation,give examples issues resolved actual SRL systems.Search: first step searching relational features define possiblerelational feature space specifying possible raw feature inputs (e.g., node linkfeature values) operators consider. possible operators include domainindependent operators (e.g., mode, count) and/or problem-specific operators (e.g., countnumber friends divided number groups). Domain-independent operatorsobviously general easier apply, problem-specific operators reducenumber possibilities must considered require effort expertknowledge. However, approaches vulnerable selection biases (Jensen et al., 2003;Jensen & Neville, 2002). second step pick appropriate search strategy, usuallyeither exhaustive, random, guided. exhaustive strategy consider featurespossible given specified inputs operators, random strategyconsider fraction space. guided strategy use heuristic subsystem identify features considered. three cases, featureconsidered subjected evaluation strategy assesses usefulness;strategies described next.Evaluation Selection: feature considered must evaluatedway determine retained use final model. instance, candidatefeature may evaluated adding current classification model; improvesaccuracy holdout set, immediately (and greedily) added set retainedfeatures (Davis, Burnside, Castro Dutra, Page, & Costa, 2005; Davis, Ong, Struyf, Burnside,Page, & Costa, 2007). cases, every candidate feature assigned scorebest scoring feature retained (Neville, Jensen, Friedland, et al., 2003),features added model based decreasing score, long new featurescontinue improve model (Mihalkova & Mooney, 2007). Simpler techniquesrequire evaluating overall model also used. instances, metricscorrelation mutual information used estimate useful featuredesired task. metrics strategies could used include Akaikes informationcriterion (AIC) (Akaike, 1974), Mallows Cp (Mallows, 1973), Bayesian information criterion(BIC) (Hannan & Quinn, 1979; Schwarz, 1978) many others (Shao, 1996; George &409fiRossi, McDowell, Aha, & NevilleProposed SystemSearch methodFeature EvaluationExhaustiveChi-square statistic/p-valueRDN-Boosting (Natarajan, Khot, Kersting, Gutmann, & Shavlik, 2012; Khot,Natarajan, Kersting, & Shavlik, 2011)ExhaustiveWeighted varianceReFeX (Henderson et al., 2011)ExhaustiveLog-binning disagreementRandomChi-square statistic/p-valueSAYU (Davis et al., 2005)AlephAUC-PRnFOIL (Landwehr et al., 2005)FOILConditional Log-LikelihoodSAYU-VISTA (Davis et al., 2007)AlephAUC-PRProbFOIL (De Raedt & Thon, 2010)FOILm-estimatekFOIL (Landwehr et al., 2010)FOILKernel target alignmentGreedy hill-climbingBayesian model selectionBeam searchWPLLTemplate-basedWPLLLevel-wise searchPseudo-likelihoodAleph++m-estimateRPT (Neville, Jensen, Friedland, et al.,2003)SpatiotemporalRPT(McGovernet al., 2008)PRM struct. learning (Getoor, Friedman, Koller, & Taskar, 2001)TSDL (Kok & Domingos, 2005)BUSL (Mihalkova & Mooney, 2007)PBNLearn-And-Join(Khosravi,Tong Man, Xu, & Bina, 2010)Discriminative MLN structurelearning (Huynh & Mooney, 2008; Biba,Ferilli, & Esposito, 2008)Table 6: Systems Searching Selecting Node Features: summarysystems used automatically search selectappropriate features given task. Note that, depending context,papers may describe function terms learning best rulessystem learning structure (e.g., MLN). MLNbased systems described; these, WPLL weighted pseudolog-likelihood.McCulloch, 1993). Frequently, possible feature may particular parameter whosevalue must set (such threshold); selecting best value given featureuse evaluation metrics may use simpler estimation technique, e.g., basedmaximum likelihood.Examples: Table 6 summarizes strategies used number SRL systems automatically search features. columns table describe system searches410fiTransforming Graph Data Statistical Relational Learningfeatures features evaluated. instance, Relational Probability Trees(RPTs) (Neville, Jensen, Friedland, et al., 2003) extension probability estimationtrees relational data use exhaustive search strategy feature selection. particular, RPT learning involves automatically searching space possible featuresusing aggregation functions Mode, Average, Count, Proportion, Min, Max,Exists, Degree. aggregations involve node link feature values (e.g.,Average) topology information (e.g., Degree). features usedclassification tasks, predicting class label document. featureevaluated based using chi-square statistic measure correlationfeature class label; yields feature score associated p-value. Featuresp-values level statistical significance discarded, remainingfeature highest score chosen inclusion model. selection processalso extended use randomization tests adjust biases commonrelational data (Jensen et al., 2003; Jensen & Neville, 2002). RPTs also extendedtemporal domains (Sharan & Neville, 2008; Rossi & Neville, 2012).RPTs represent conditional probability distributions using single tree. contrast,Natarajan et al. (2012) propose using gradient boosting (Friedman, 2001)conditional probability distribution represented weighted sum regression treesgrown stage-wise optimization. features tree selected via depthlimited, exhaustive search, though note domain knowledge could also usedguide search. Natarajan et al. argue resultant set multiple, relatively shallowtrees allows efficient learning complex structures, demonstrate techniqueoutperform alternatives based single trees Markov Logic Networks discussedbelow.Another system uses exhaustive search ReFeX (Henderson et al., 2011),uses aggregates Sum Mean operators recursively generate features baseddegree node local neighborhood. prune resultant large set, ReFeX useslogarithmic binning feature values, clusters features based similaritybinned space, retains one feature cluster. logarithmic binningchosen favors features discriminative high-degree nodes.recursive approach also modified constructing features dynamicnetworks (Rossi, Gallagher, Neville, & Henderson, 2012).Alternatively, spatiotemporal RPTs (McGovern et al., 2008) use random search strategy. particular, RPTs add temporal spatial-based features set possiblefeatures. resultant feature space large exhaustive search, instead randomsampling used. pre-defined number features considered, bestscored feature added model.remaining systems discuss use guided search strategy,heuristic sub-system provides candidate features considered. instance,several systems (Davis et al., 2005; Landwehr et al., 2005) use ILP systemgenerate candidate features, evaluate features select ultimate use.particular, SAYU (Davis et al., 2005) uses ILP system Aleph (Srinivasan, 1999)generate candidate feature (which consider new view original data).Aleph creates candidates features based positive examples, training data,concept predicted. proposed feature evaluated learning411fiRossi, McDowell, Aha, & Nevillenew model includes feature computing area precision-recallcurve (AUC-PR). feature improves AUC-PR score, permanently addedmodel feature search continues. SAYU-VISTA (Davis et al., 2007) retainsgeneral approach extends types features considered, particularadding ability dynamically link together objects different types recursivelybuild new features constructed features. Davis et al. demonstrate linkconnections especially helpful improving performance compared original SAYUsystem. Landwehr et al. (2005) describe nFOIL system similar SAYUdeveloped independently, De Raedt Thon (2010) describe ProbFOILupgrades deterministic rule learner like FOIL probabilistic. Landwehr et al. (2010)describe related kFOIL system integrates FOIL kernel methods. alsoconsider impact several different feature scoring functions.number systems considered perform structure learning Probabilistic Relational Models (PRMs) (Getoor et al., 2001) Markov Logic Networks(MLNs) (Domingos & Richardson, 2004), general case feature selection problems described above. instance, MLN weighted set first-order formulas;structure learning corresponds learning formulas weight learning correspondslearning associated weights. first MLN structure learning approaches systematically construct candidate clauses starting empty clause, greedily adding literalsit, testing resulting clauses fit training data using statistical measure (Kok& Domingos, 2005; Biba et al., 2008). However, top-down approaches inefficientinitial proposal clauses ignores training data, resulting large numberpossible features considered possible problems local minima. response,number bottom-up approaches proposed. particular, MihalkovaMooney (2007) use propositional Markov network structure learner construct templatenetworks guide construction features based training data. recentwork examined enable bottom-up approaches learn longer clauses basedconstraining search consider features consistent certain patterns motifs (Kok & Domingos, 2010), clustering input nodes create lifted graphrepresentation, enabling feature search smaller graph (Kok & Domingos, 2009).Khosravi et al. (2010) perform MLN structure learning first learning structuresimpler Parametrized Bayes Net (PBN) (Poole, 2003), converting resultMLN. data contains significant number descriptive attributes, showapproach dramatically improves runtime structure learning also improvespredictive accuracy. Schulte (2011) given theoretical justification approach.Another alternative, proposed Khot et al. (2011), extend previously mentionedwork Natarajan et al. (2012) gradient boosting MLNs. Essentially, problemlearning MLNs transformed series relational regression problemsfunctional gradients represented clauses trees. several datasets demonstrate faster MLN structure learning accurate better baselines includingalgorithms Mihalkova Mooney (2007) Kok Domingos (2010).techniques MLNs seek learn network structure best explainstraining data whole. contrast, situations prediction specific predicate desired (e.g., predict political affiliation Facebook example),Huynh Mooney (2008) Biba et al. (2008) propose discriminative approaches412fiTransforming Graph Data Statistical Relational LearningMLN structure learning. instance, Huynh Mooney use modified versionAleph (Srinivasan, 1999) compute large number candidate clauses, use formL1 -regularization force weights subsequently learned clauseszero clause helpful predicting predicate. regularization,conjunction appropriate optimization function, effectively leads selectingsmaller set features useful desired task.Discussion: focus article graph-based data representations (see Section 1.2).However, many examples discussed use logical representation instead.include section techniques used constructing searchingfeatures rules similar settings. instance, RPTs (a graphbased approach) RDN-Boosting (a logical approach) use exhaustive searchprobabilistic decision trees, different feature scoring strategies.Popescul et al. (2003a) examine automatically learn new relational featureslinks (to support link prediction), techniques could also applied constructingnode features. particular, treat feature relational database query, useconcept refinement graphs (Shapiro, 1982) consider refining initial queryequi-joins, equality selections, statistical aggregates. refinement,refinements considered; search guided sampling possible refinements proceeding results particular refinement type seemspromising. features chosen combined logistic regression classifier. evaluation specific features, use Bayesian Information Criterion (BIC) (Schwarz,1978), includes term penalizes feature complexity reduce dangeroverfitting.discussed multiple systems include notions aggregation including RPTs,SAYU-VISTA, work Popescul et al. (2003a) discussed above. alsoaggregate-based learning approaches Crossmine (Yin, Han, Yang, & Yu, 2006),CLAMF (Frank, Moser, & Ester, 2007), Multi-relational Decision Trees (MRDTL) (Leiva,Gadia, & Dobbs, 2002), Confidence-based Concept Discovery (C2 D) (Kavurucu, Senkul, &Toroslu, 2008), many others (Perlich & Provost, 2006; Krogel & Wrobel, 2001; Knobbe,Siebes, & Marseille, 2002). also possibilities feature evaluation.instance, GleanerSRL (Goadrich & Shavlik, 2007) uses Aleph (Srinivasan, 1999) searchclauses uses metric precision recall evaluating clauses.7. Jointly Transforming Nodes Linksprevious sections, primarily discussed relational representation transformationtechniques applied independently one another. instance, one techniquemight used predict links, another builds transformed representationapplying node labeling technique. section instead examines joint transformationtasks combine node link transformation way, instance label nodesweight links simultaneously. techniques may enable subtask influencehelpful ways, avoids bias might introduced requiringserialization two tasks (such link weighting node labeling) might usefullyperformed jointly.413fiRossi, McDowell, Aha, & NevilleOne recent approach proposed Namata, Kok, Getoor (2011) collectively performs link prediction, node labeling, entity resolution (which seen formnode deletion/merging). present iterative algorithm solves three taskssimultaneously propagating information among solutions three tasks.particular, introduce notion inter-relational features, relational features one task depend upon predicted values another. results showusing features improve accuracy, inferring predicted valuesthree tasks simultaneously significantly improve accuracy compared performingthree tasks sequence, even possible orderings considered.Techniques model full distribution across links attributes RMNs(Taskar et al., 2002), PRMs (Friedman et al., 1999), MLNs (Domingos & Richardson,2004) also used scenario, instance jointly predict node link labels.section, however, focus particularly recent techniques presumeexistence textual content associated nodes links graph(although basic algorithms would also work kinds features). considerthree types techniques, based kind input text use: stand-alone textdocuments (e.g., legal memos links), text documents connected links (e.g.,webpages hyperlinks), entities connected links associated text (e.g.,people connected email messages). Table 7 lists prominent models,grouped according three types. columns table indicate kindsinput models use (middle section) types transformation perform(right-hand section). text documents corresponds node features table,text associated links yields link features. discuss three typestechniques detail.7.1 Using Text Documents LinksFirst, many techniques used assign topics labels nodes nodes(such documents) associated text. instance, first row Table 7 indicatesLDA PLSA use nodes node features perform node prediction,weighting, labeling. Section 6 already mentioned techniques usedlabel node one discovered topics, typical use. However,techniques also perform node weighting (using weights associatedtopics) and/or node prediction (by converting discovered topics new latent nodesdiscussed introduction Section 5). Table 7, use lighter checkmarksrepresent kind situations transformation task could performedparticular model primary use/output.LDA PLSA treat document bag words seek assign onetopics (labels) document based words. contrast, Nubbi (Chang, BoydGraber, & Blei, 2009) designs approach based LDA graph defined basedobjects (nodes) referenced set documents, links predicted basedrelationships implied text documents. addition, nodeslinks associated likely topic(s) based relationships. Thus,model simultaneously performs link prediction, link labeling, node labeling. similar414fiTransforming Graph Data Statistical Relational LearningLink LabelingNode PredictionNode WeightingNode LabelingEXEXEVXVXVVXVLDA/PLSAXXNubbiXXXJoint Transformation ModelEXELink WeightingInputNodesLink PredictionLinksXXXXXXXLink-LDA, Link-PLSAXXXXXXXXPairwise-Link-LDAXXXXXXXXLink-PLSA-LDAXXXXXXXXRelational Topic Model (RTM)XXXXXXXXTopic-Link LDAXXXXXXXXGroup-Topic (GT)XXXXXXXXAuthor-Recipient-Topic (ART)XXXXXBlock-LDAXXXXXXXXXTable 7: Summary Joint Transformation Models: middle sectiontable indicates types graph features used inputs model,right side table indicates types link node transformationperformed model. Lighter checkmarks indicate outputmodel transformed perform particular transformation task (e.g.,use node labels create new latent group nodes), taskprimary goal specified model.415fiRossi, McDowell, Aha, & Nevilleresult produced semantic network extraction Kok Domingos (2008)discussed Section 4.2.7.2 Using Text Document Linkssecond type joint transformation also uses text documents, adds known linksdocuments model. instance, Section 6 discussed Link-LDALink-PLSA add link modeling LDA PLSA order perform node labeling;discussed LDA PLSA modified also achieve node predictionweighting. shown Table 7, Link-LDA Link-PLSA also used linkprediction weighting learning model training graph usingpredict unseen links new test graph (Nallapati et al., 2008).Link-LDA Link-PLSA model links way similar modelpresence words document (node). instance, Link LDAs generative model,generate one word, document chooses topic, chooses word topicspecific multinomial. identical process (using topic-specific multinomial) usedgenerate, particular document, one target document link to. Thus, Link-LDALink-PLSA directly extend original LDA PLSA models add links.Nallapati et al. (2008) argue Link-LDAs Link-PLSAs extensions links,pragmatic, adequately capture topical relationship two documentslinked together. Instead, propose two alternatives. first, Pairwise LinkLDA, replaces link model Link-LDA model based mixed membershipstochastic blockmodels (Airoldi et al., 2008), possible link modeledBernoulli variable conditioned topic chosen based topic distributionstwo endpoints link. second approach, Link-PLSA-LDA, retainslink generation model Link-LDA, changes word generation modeldocuments (the ones incoming links) words document dependtopics documents link it. downside latter approachworks nodes divided set outgoing links setincoming links. However, Nallapati et al. argue limitation largelyovercome duplicating nodes incoming outgoing links. Moreover,approach much faster scalable Pairwise Link-LDA. Nallapati et al.demonstrate models outperform Link-LDA likelihood ranking task,Link-PLSA-LDA also outperforms Link-LDA link prediction task. also showLink-PLSA-LDA Link-LDA comparable terms execution time,Pairwise Link-LDA much slower.Changes generative model used approaches encode different assumptions data lead significant performance differences. instance,Chang Blei (2009) introduce Relational Topic Model (RTM) comparePairwise Link-LDA model discussed above. models allow similar flexibility termslinks defined, Chang Blei argue model forces topicassignments used generate words documents also generatelinks, true Pairwise Link-LDA. demonstrate RTM providesaccurate predictions link suggestions Pairwise Link-LDA severalbaselines.416fiTransforming Graph Data Statistical Relational LearningAnother possible change model add types objects. instance,Topic-Link LDA (Liu, Niculescu-Mizil, & Gryc, 2009) models documents, links,likely topics associated document, also explicitly considersauthor document clusters authors multiple communities. Creatingnew clustering equivalent finding per-document topics authorassociated one document. argue approach analogousunifying separate tasks (1) assigning topics documents (2) analyzingsocial network authors. show approach cases outperformLDA Link-LDA.7.3 Using Text Associated Linksfinal type joint transformation techniques form link features based text associatedlinks, text email messages (McCallum, Wang, & Corrada-Emmanuel,2007) scientific abstracts relate particular protein-protein interaction (Balasubramanyan & Cohen, 2011). Several techniques discussed previouslycontext link interpretation. instance, Section 4.2 discussed modelsAuthor-Recipient-Topic (ART) model (McCallum, Wang, & Corrada-Emmanuel, 2007)Group-Topic (GT) model (McCallum, Wang, & Mohanty, 2007) extend LDA performlink labeling; strength predicted labels (topics) also used weightlinks. addition, GT model directly assigns nodes groups (i.e., node labeling),labels ART associates link could also used label associatednodes. RART model (McCallum, Wang, & Corrada-Emmanuel, 2007) extends ARTallowing node multiple roles. recently, Block-LDA (Balasubramanyan& Cohen, 2011) merges ideas latent variables models stochastic blockmodels. specifically, Block-LDA shares information three components:link model shares information block structure shared topicmodel. Unlike GT ART, however, Block-LDA focuses labeling nodes ratherlinks. Balasubramanyan Cohen evaluate Block-LDA protein datasetEnron email corpus demonstrate outperforms Link-LDA severalbaselines task protein functional category prediction.7.4 Discussiontechniques discussed variants latent group models focusnode and/or link label prediction, also used node predictionnew nodes represent newly discovered topics latent groups. models alsoextended incorporate notions time (Dietz, Bickel, & Scheffer, 2007; Wang, Blei, &Heckerman, 2008; Wang & McCallum, 2006), topic hierarchies (Li & McCallum, 2006),correlations topics (Blei & Lafferty, 2007). addition, links usually assumedgenerated based overall topic(s) node link. contrast, LatentTopic Hypertext Model (LTHM) (Gruber, Rosen-Zvi, & Weiss, 2008) models linkoriginating specific word document. Somewhat surprisingly, showapproach leads model fewer parameters models like Link-LDA,demonstrate approach outperforms Link-LDA Link-PLSAevaluated link prediction task.417fiRossi, McDowell, Aha, & Neville(a) Initial Graph(b) Joint TransformationFigure 10: Example Joint Transformation: example, new latent nodesadded represent discovered topics, weighted links addedoriginal node new latent node. addition, weighted links addedlatent nodes, representing connection strength topics.Finally, new links original nodes may also predicted. Noteexample adapted results found work Nallapati et al. (2008).new nodes added graph represent discovered topics, linksinvariably added connect existing nodes new nodes. However, models mayalso learn information discovered topics related other.instance, Figure 10 shows two new topics discovered graphconnected existing nodes. addition, topics connectednew links weight link represents frequently documenttopic cites document representing different topic. Adding additional linksgraph lets original nodes connected closely primary topicsalso related topics.8. Discussion Challengessection discuss additional issues related relational representationtransformation highlight important challenges future work.8.1 Guiding Evaluating Representation Transformationgoal representation transformation often improve data representationway leads better results subsequent task possibly understandable representation. evaluate whether particular transformation techniqueaccomplished goal? first address question, consider finalgoal used directly guide initial transformation.tasks, representation evaluation straightforward provided ground truthvalues known hold-out data set. instance, test technique link418fiTransforming Graph Data Statistical Relational Learningprediction effective, accuracy measured links predicted hold-out set(Taskar et al., 2003; Liu et al., 2009). particular evaluation metric modifiedappropriate domain. instance, Chang Blei (2009) evaluate precisiontwenty highest-ranked links suggested document, Nallapati et al. (2008)consider custom metric called RKL measures rank last true link suggestedmodel. Likewise, desired task involves classification, classificationalgorithm run hold-out data, without representation change,see change increases classification accuracy.cases, may difficult directly measure well representation changeperformed, classification used surrogate measure: accuracy increases,change assumed beneficial. instance, classification used evaluate link prediction (Gallagher et al., 2008), link weighting (Xiang et al., 2010), link labeling (Rossi & Neville, 2010; Macskassy, 2007), node prediction (Neville & Jensen,2005). addition, node labeling naturally classification problem, node weightingusually evaluated ways, e.g., based query relevance.techniques used direct evaluation feasible, existsmetric believed related. instance, higher autocorrelationgraph associated presence sensible links, algorithmscollective classification typically perform better level autocorrelation higher.Thus, Xiang et al. (2010) demonstrate success technique estimating relationship strengths (link weights) based part showing increase autocorrelationmeasured several attributes social network. Likewise, increased information gainattributes could used demonstrate improved representation (Lippi,Jaeger, Frasconi, & Passerini, 2009), link perplexity could used assess topic labelings (Balasubramanyan & Cohen, 2011). Naturally, appropriate evaluationtechniques vary based upon task, comparison transformation techniques mayyield different results depending upon metric chosen.Ideally, representation transformation would guided directly final goalexecuted, rather evaluated transformation complete.often case feature selection structure learning algorithms discussedSection 6.3: task accuracy (or surrogate measure) evaluated particular featureadded, retained accuracy improved. cases, transformationeven directly specified desired end goal. instance, supervised randomwalk approach discussed Section 3.3 uses gradient descent method obtain new linkweights links predicted subsequent random walk (their final goal)accurate. Likewise, Menon Elkan (2010) show add supervision methodsgenerating latent features (see introduction Section 5) features learnedwould relevant final classification task. show, however, addingsupervision always helpful. final example, Shi, Li, Yu (2011) usequadratic program optimize linear combination link weights final linkweights lead directly accurate classification via label propagation algorithm.general, ensuring particular transformation improve performancefinal SRL task remains challenging. Many transformations cannot directly guidedfinal goal, either suitable supervised data available, clear419fiRossi, McDowell, Aha, & Nevillemodify transformation algorithms use information (e.g., latenttopic models Section 7 group detection algorithms Section 5).8.2 Causal DiscoveryCausal discovery refers identifying cause-and-effect relationships (i.e., smoking causescancer) either online experimentation (Aral & Walker, 2010) observationaldata. challenge distinguish true causal relationships mere statistical correlations. One approach use quasi-experimental designs (QEDs), take advantagecircumstances non-experimental data identify situations provide equivalentexperimental control randomization. Jensen, Fast, Taylor, Maier (2008) proposesystem discover knowledge applying QEDs discovered automatically.recently, Oktay, Taylor, Jensen (2010) apply three different QEDs demonstrateone gain causal understanding social media system. also another causaldiscovery technique linear models proposed Wang Chan (2010). challengeremains extend techniques apply broader range relational data.8.3 Subgraph Transformation Graph Generationmajority article focused transformation tasks centered around nodeslinks graphs. However, also useful tasks subgraph transformationseek identify frequent/informative substructures set graphs create featuresclassify subgraphs (Inokuchi, Washio, & Motoda, 2000; Deshpande, Kuramochi,Wale, & Karypis, 2005). instance, Kong Yu (2010) consider use semisupervised techniques perform feature selection subgraph classification givenlabeled subgraphs. nodes links, subgraphs tasks prediction,labeling, weighting, feature generation described. Many techniquesdescribed node-centered features also used context, fulldiscussion subgraph transformation beyond scope article.Recently, graph generation algorithms attracted significant interest. algorithms use model represent family graphs, present way generate multiple samples family. Two prominent models Kronecker Product Graph Models(KPGMs) (Leskovec, Chakrabarti, et al., 2010) based preferential attachment(Price, 1976; Barabasi & Albert, 1999). graph generation methods take advantageglobal (with KPGMs) local (with preferential attachment models) graph propertiesgenerate distribution graphs potentially include attributes. Samplingmodels useful creating robust algorithms, instance trainingclassifier family related graphs instead single graph. Newman (2003) surveysadditional network models properties relevant graph generation.8.4 Model RepresentationSRL also notion model representation: kind statistical modellearned represent relationship nodes, links, features?prominent models SRL Probabilistic Relational Models (PRMs) (Friedmanet al., 1999), Relational Markov Networks (RMNs) (Taskar et al., 2002), Relational Depen420fiTransforming Graph Data Statistical Relational Learningdency Networks (RDNs) (Neville & Jensen, 2007), Structural Logistic Regression (Popesculet al., 2003b), Conditional Random Fields (CRFs) (Lafferty, McCallum, & Pereira, 2001),Markov Logic Networks (MLNs) (Domingos & Richardson, 2004; Richardson & Domingos, 2006); full discussion models beyond scope article. many casestechniques relational representation transformation, link prediction, performed regardless kind statistical model subsequently used. However,choice statistical model strongly interact kinds node link featuresuseful (or even possible use); Section 6.3 describes connections.number relevant comparisons already published (Jensen et al., 2004; Neville& Jensen, 2007; Macskassy & Provost, 2007; Sen et al., 2008; McDowell et al., 2009; Crane& McDowell, 2011), work needed evaluate interaction choicestatistical model feature selection, evaluate statistical models work bestdomains certain characteristics.8.5 Temporal Spatial Representation Transformationappropriate, already discussed multiple techniques incorporatetemporal information graph data (see especially Sections 4.2, 6.1, 6.3).techniques focused solving particular problems node classification, dealingdata invariably requires studying represent time-varying elements.However, work needed examine general tradeoffs involved differenttemporal representations. instance, Hill, Agarwal, Bell, Volinsky (2006) providegeneric framework modeling temporal dynamic network central goalbuild approximate representation satisfies pre-specified objectives. focussummarization (representing historical behavior two nodes concise manner),simplification (removing noise edges nodes, spurious transactions, stale relationships), efficiency (supporting fast analysis updating), predictive performance(optimizing representation maximize predictive performance). work providesnumber useful building blocks, comparisons needed to, instance, evaluate merits using summarized networks general-purpose algorithms vs. usingspecialized algorithms data maintains temporal distinctions.Temporal data one particular kind data represented relationalsequence. Kersting, De Raedt, Gutmann, Karwath, Landwehr (2008) survey arearelational sequence learning explains multiple tasks related data,sequence mining alignment. tasks often involve need identify relevantfeatures structure, identifying frequent patterns useful similarity functions.Thus, set useful techniques feature construction search domain overlapdiscussed Section 6.3.8.6 Privacy Preserving Representationsometimes desire make private graph-based data publicly available (e.g.,support research public policy) way preserves privacy individualsdescribed data. goal privacy preserving representation transformdata way minimizes information loss maximizing anonymization, e.g.,prevent individuals anonymized network identified. Naive approaches421fiRossi, McDowell, Aha, & Nevilleanonymization operate simply replacing individuals name (or attributes)arbitrary meaningless unique identifiers. However, social networks manyadversarial methods true identity user often discoveredanonymized network. particular, adversarial methods use networkstructure and/or remaining attributes discover identities users within network(Liu & Terzi, 2008; Zhou, Pei, & Luk, 2008; Narayanan & Shmatikov, 2009).early approach Zheleva Getoor (2007) examines graph may modifiedprevent sensitive relationships (a particular kind labeled link) disclosed.describe approach terms node anonymization edge anonymization.Node anonymization clusters nodes equivalence classes based node attributesonly, edge anonymization approaches based cleverly removingsensitive edges. Backstrom, Dwork, Kleinberg (2007) address related family attacksadversary able learn whether edge exists targeted pairs nodes.recently, Hay, Miklau, Jensen, Towsley, Weis (2008) study privacy issuesgraphs contain attributes. goal prevent structural re-identification(i.e., identity reconstruction using graph topology information) anonymizing graph viacreating aggregate network model allows samples drawn model.approach generalizes graph partitioning nodes summarizing graphpartition level. approach differs approaches describeddrastically changes representation opposed making incrementalchanges. However, method enforces privacy still preserving enough networkproperties allow wide variety network analyses performed.investigations key factors information available graph,resources attacker, type attacks must defended against.addition, attacker possibly obtain additional information related graphsources, challenges even difficult. work neededprovide strong privacy guarantees still enabling partial public release graph-basedinformation.9. ConclusionGiven increasing prevalence importance relational data, article surveyedsignificant current issues relational representation transformation. presenting new taxonomy important transformation tasks Section 2, nextdiscussed four primary tasks link prediction, link interpretation, node prediction,node interpretation. Section 7 considered tasks accomplishedsimultaneously via techniques joint transformation. Finally, Section 8 consideredperform representation evaluation key challenges future work.additional possible representation transformations spacediscuss, fit cleanly taxonomy Figure 2. instance, bipartitegraph customers products, may useful eliminate product nodes, replacinginformation content new links among customers purchasedproduct. somewhat related group discovery techniques Section 5.also considered depth potential transforming nodes edges422fiTransforming Graph Data Statistical Relational Learningvice versa (though representation choices Figure 6 also relevant here),technique sometimes useful pre-processing step.taxonomy presented Section 2 highlighted symmetry possibletransformation tasks links nodes. symmetry helped organizesurvey, also suggests areas techniques developed one entitiesused analogous task other. instance, Liben-Nowell Kleinberg(2007) reformulated traditional node weighting algorithms weight links. Likewise, topicdiscovery techniques based LDA used node labeling link labeling.Finally, many techniques used create node features also used create linkfeatures, vice versa, although node features studied much thoroughly.discussed Section 8, remains much work do. instance, link predictionremains difficult problem, especially general case two arbitrarynodes might connected together. Even significantly, describedwide range techniques address transformation tasks, endday practitioner left wide range choices without many guaranteesmight work best. instance, node weighting may improve classification accuracyone dataset decrease another. challenge made difficulttechniques described come wide range areas, includinggraph theory, social network analysis, numerical linear algebra (e.g., matrix factorization),metric learning, information theory, information retrieval, inductive logic programming,statistical relational learning, probabilistic graphical models. breadthtechniques relevant relational transformation wonderful resource, also meansevaluating representation change techniques relevant particular tasktime-consuming, technically challenging, incomplete process. Therefore, muchwork needed establish theoretical understanding different representationchanges affect data, different data characteristics interact process,combination techniques data characteristics affect final resultsanalysis relational data.Acknowledgmentsthank reviewers many helpful suggestions feedback. majoritywork completed Naval Research Laboratory, Ryan Rossi supportedASEE/ONR NREIP summer internship 2010 NSF Graduate ResearchFellowship (at Purdue University). Luke McDowell supported part NSF awardnumber 1116439 grant ONR. research also partly supportedNSF contract number IIS-1149789. views conclusions containedherein authors interpreted necessarily representingofficial policies endorsements, either expressed implied, ONR, NSF, U.S.Government.ReferencesAdamic, L. A., & Adar, E. (2001). Friends neighbors web. Social Networks,25 (3), 211230.423fiRossi, McDowell, Aha, & NevilleAdibi, J., Chalupsky, H., Melz, E., Valente, A., et al. (2004). KOJAK group finder:Connecting dots via integrated knowledge-based statistical reasoning.Proceedings 16th Conference Innovative Applications Artifical Intelligence,pp. 800807.Adomavicius, G., & Tuzhilin, A. (2005). Toward next generation recommendersystems: survey state-of-the-art possible extensions. IEEE TransactionsKnowledge Data Engineering, 17 (5), 734749.Ahmed, N., Neville, J., & Kompella, R. (2012a). Network sampling designs relationalclassification. Proceedings 6th International AAAI Conference WeblogsSocial Media.Ahmed, N., Neville, J., & Kompella, R. (2012b). Space-efficient sampling social activitystreams. BigMine, pp. 18.Ahmed, N., Berchmans, F., Neville, J., & Kompella, R. (2010). Time-based samplingsocial network activity graphs. Proceedings 8th Workshop MiningLearning Graphs, pp. 19.Airoldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed membershipstochastic blockmodels. Journal Machine Learning Research, 9, 19812014.Akaike, H. (1974). new look statistical model identification. IEEE TransactionsAutomatic Control, 19 (6), 716723.Albert, R., Jeong, H., & Barabasi, A. (1999). Internet: Diameter world-wide web.Nature, 401 (6749), 130131.Amarel, S. (1968). representations problems reasoning actions. MachineIntelligence, 3, 131171.Anthony, A., & desJardins, M. (2007). Data clustering relational push-pull model.Proceedings Seventh IEEE International Conference Data Mining Workshops, ICDMW 07, pp. 189194.Aral, S., & Walker, D. (2010). Creating Social Contagion Viral Product Design:Randomized Trial Peer Influence Networks. Proceedings 31st International Conference Information Systems.Backstrom, L., & Leskovec, J. (2011). Supervised random walks: predicting recommending links social networks. Proceedings 4th International ConferenceWeb Search Data Mining, pp. 635644.Backstrom, L., Dwork, C., & Kleinberg, J. M. (2007). Wherefore art thou r3579x?:anonymized social networks, hidden patterns, structural steganography. Proceedings 16th International World Wide Web Conference, pp. 181190.Balasubramanyan, R., & Cohen, W. (2011). Block-LDA: Jointly modeling entity-annotatedtext entity-entity links. Proceedings 7th SIAM International ConferenceData Mining.Barabasi, A., & Albert, R. (1999). Emergence scaling random networks. Science,286 (5439), 509512.424fiTransforming Graph Data Statistical Relational LearningBarabasi, A., & Crandall, R. (2003). Linked: new science networks. American journalPhysics, 71 (4), 409410.Basilico, J. B., & Hofmann, T. (2004). Unifying collaborative content-based filtering.Proceedings 21st International Conference Machine Learning, pp. 6572.Ben-Hur, A., & Noble, W. (2005). Kernel methods predicting protein-protein interactions. Bioinformatics, 21 (Suppl. 1), 3846.Benczur, A., Csalogany, K., Sarlos, T., & Uher, M. (2005). Spamrankfully automatic linkspam detection. Adversarial Information Retrieval Web, pp. 2538.Berkhin, P. (2006). Survey clustering data mining techniques. Grouping MultidimensionalData: Recent Advances Clustering, 10, 2571.Bharat, K., & Henzinger, M. (1998). Improved algorithms topic distillation hyperlinked environment. Proceedings 21st International SIGIR ConferenceResearch Development Information Retrieval, pp. 104111.Bhattacharya, I., & Getoor, L. (2005). Relational clustering multi-type entity resolution.Proceedings 4th International workshop Multi-relational Mining, pp. 312.Bhattacharya, I., & Getoor, L. (2007). Collective entity resolution relational data. Transactions Knowledge Discovery Data, 1 (1), 136.Biba, M., Ferilli, S., & Esposito, F. (2008). Discriminative structure learning Markovlogic networks. Inductive Logic Programming, 5194, 5976.Bilgic, M., Mihalkova, L., & Getoor, L. (2010). Active learning networked data.Proceedings 27th International Conference Machine Learning.Bilgic, M., Namata, G. M., & Getoor, L. (2007). Combining collective classification linkprediction. Proceedings 7th IEEE International Conference Data MiningWorkshops, pp. 381386.Blei, D., Ng, A., & Jordan, M. (2003). Latent Dirichlet allocation. Journal MachineLearning Research, 3, 9931022.Blei, D., & Lafferty, J. (2007). correlated topic model science. Annals AppliedStatistics, 1 (1), 1735.Bonacich, P., & Lloyd, P. (2001). Eigenvector-like measures centrality asymmetricrelations. Social Networks, 23 (3), 191201.Borgman, C., & Furner, J. (2002). Scholarly communication bibliometrics. AnnualReview Information Science Technology, 36, 372.Brightwell, G., & Winkler, P. (1990). Maximum hitting time random walks graphs.Random Structures & Algorithms, 1 (3), 263276.Broder, A., Kumar, R., Maghoul, F., Raghavan, P., Rajagopalan, S., Stata, R., Tomkins,A., & Wiener, J. (2000). Graph structure web. Computer networks, 33 (1-6),309320.Burges, C. (1998). tutorial support vector machines pattern recognition. Datamining knowledge discovery, 2 (2), 121167.425fiRossi, McDowell, Aha, & NevilleCafarella, M. J., Wu, E., Halevy, A., Zhang, Y., & Wang, D. Z. (2008). Webtables: Exploringpower tables web. Proceedings VLDB, pp. 538549.Camacho, J., Guimera, R., & Nunes Amaral, L. (2002). Robust patterns food webstructure. Physical Review Letters, 88 (22), 228102: 14.Chakrabarti, S., Dom, B., & Indyk, P. (1998). Enhanced hypertext categorization usinghyperlinks. Proceedings ACM SIGMOD International Conference Management Data, pp. 307318.Chakrabarti, S., Dom, B., Raghavan, P., Rajagopalan, S., Gibson, D., & Kleinberg, J.(1998). Automatic resource compilation analyzing hyperlink structure associated text. Computer Networks ISDN Systems, 30 (1-7), 6574.Chang, J., & Blei, D. (2009). Relational topic models document networks.9th International Conference Artificial Intelligence Statistics (AISTATS), pp.8188.Chang, J., Boyd-Graber, J., & Blei, D. (2009). Connections lines: augmentingsocial networks text. Proceedings 15th ACM SIGKDD InternationalConference Knowledge Discovery Data Mining, pp. 169178.Clauset, A., Moore, C., & Newman, M. (2008). Hierarchical structure predictionmissing links networks. Nature, 453 (7191), 98101.Cohn, D., & Chang, H. (2000). Learning probabilistically identify authoritative documents. Proceedings 17th International Conference Machine Learning, pp.167174.Cohn, D., & Hofmann, T. (2001). missing link-a probabilistic model document content hypertext connectivity. Advances Neural Information Processing Systems,13, 430436.Crane, R., & McDowell, L. K. (2011). Evaluating markov logic networks collectiveclassification. Proceedings 9th MLG Workshop 17th ACM SIGKDDConference Knowledge Discovery Data Mining.Craven, M., DiPasquo, D., Freitag, D., McCallum, A., Mitchell, T., Nigam, K., & Slattery,S. (2000). Learning construct knowledge bases World Wide Web. ArtificialIntelligence, 118 (1-2), 69113.Cristianini, N., & Shawe-Taylor, J. (2000). Introduction Support Vector Machineskernel-based learning methods. Cambridge University Press.Dash, M., & Liu, H. (1997). Feature selection classification. Intelligent data analysis,1 (3), 131156.Davis, J., Burnside, E., Castro Dutra, I., Page, D., & Costa, V. (2005). integratedapproach learning Bayesian networks rules. Proceedings EuropeanConference Machine Learning, pp. 8495.Davis, J., Ong, I., Struyf, J., Burnside, E., Page, D., & Costa, V. S. (2007). Change representation statistical relational learning. Proceedings 20th InternationalJoint Conference Artificial Intelligence, pp. 27192725.426fiTransforming Graph Data Statistical Relational LearningDe Raedt, L. (2008). Logical relational learning. Springer.De Raedt, L., & Kersting, K. (2008). Probabilistic inductive logic programming. SpringerVerlag.De Raedt, L., & Thon, I. (2010). Probabilistic rule learning. Inductive Logic Programming,6489, 4758.Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990).Indexing latent semantic analysis. Journal American Society InformationScience, 41, 391407.Deshpande, M., Kuramochi, M., Wale, N., & Karypis, G. (2005). Frequent substructurebased approaches classifying chemical compounds. IEEE Transactions Knowledge Data Engineering, 13, 10361050.Dhillon, I. (2001). Co-clustering documents words using bipartite spectral graph partitioning. Proceedings seventh ACM SIGKDD International ConferenceKnowledge Discovery Data Mining, pp. 269274.Dietz, L., Bickel, S., & Scheffer, T. (2007). Unsupervised prediction citation influences.Proceedings 24th International Conference Machine Learning, pp. 233240.Domingos, P., & Richardson, M. (2004). Markov logic: unifying framework statisticalrelational learning. Proceedings ICML Workshop Statistical RelationalLearning, pp. 4954.DuBois, C., & Smyth, P. (2010). Modeling Relational Events via Latent Classes. Proceedings 16th ACM SIGKDD International Conference Knowledge DiscoveryData Mining, pp. 803812.Dunne, J., Williams, R., & Martinez, N. (2002). Food-web structure network theory:role connectance size. Proceedings National Academy SciencesUnited States America, 99 (20), 12917.Easley, D., & Kleinberg, J. (2010). Networks, Crowds, Markets: ReasoningHighly Connected World. Cambridge University Press.Eckart, C., & Young, G. (1936). approximation one matrix another lower rank.Psychometrika, 1 (3), 211218.Egghe, L., & Rousseau, R. (1990). Introduction informetrics. Elsevier Science Publishers.Erosheva, E., Fienberg, S., & Lafferty, J. (2004). Mixed-membership models scientificpublications. Proceedings National Academy Sciences United StatesAmerica, 101 (Suppl 1), 5220.Essen, U., & Steinbiss, V. (1992). Cooccurrence smoothing stochastic language modeling. Proceedings International Conference Acoustics, Speech, SignalProcessing, pp. 161164.Faloutsos, M., Faloutsos, P., & Faloutsos, C. (1999). power-law relationshipsinternet topology. Proceedings ACM SIGCOMM International ConferenceApplications, Technologies, Architectures, Protocols Computer Communication, pp. 251262.427fiRossi, McDowell, Aha, & NevilleFast, A., & Jensen, D. (2008). stacked models perform effective collective classification.Proceedings IEEE International Conference Data Mining, pp. 785790.Fodor, I. (2002). Survey Dimension Reduction Techniques. US DOE Office ScientificTechnical Information, 18.Frank, R., Moser, F., & Ester, M. (2007). method multi-relational classificationusing single multi-feature aggregation functions. Proceedings PrinciplesPractice Knowledge Discovery Databases, 1, 430437.Freeman, L. C. (1977). set measures centrality based betweenness. Sociometry,40, 3541.Friedman, J. (2001). Greedy function approximation: gradient boosting machine..Annals Statistics, 29 (5), 11891232.Friedman, N., Getoor, L., Koller, D., & Pfeffer, A. (1999). Learning probabilistic relational models. Proceedings 16th International Joint Conference ArtificialIntelligence, pp. 13001309. Springer-Verlag.Gallagher, B., Tong, H., Eliassi-Rad, T., & Faloutsos, C. (2008). Using ghost edgesclassification sparsely labeled networks. Proceedings 14th ACM SIGKDDInternational Conference Knowledge Discovery Data Mining, pp. 256264.Gartner, T. (2003). survey kernels structured data. ACM SIGKDD ExplorationsNewsletter, 5 (1), 4958.George, E., & McCulloch, R. (1993). Variable selection via Gibbs sampling. JournalAmerican Statistical Association, 88, 881889.Getoor, L., Friedman, N., Koller, D., & Taskar, B. (2003). Learning probabilistic modelslink structure. Journal Machine Learning Research, 3, 679707.Getoor, L., & Taskar, B. (Eds.). (2007). Introduction Statistical Relational Learning.MIT Press.Getoor, L., & Diehl, C. P. (2005). Link mining. SIGKDD Explorations, 7, 312.Getoor, L., Friedman, N., Koller, D., & Taskar, B. (2001). Learning probabilistic modelsrelational structure. Proceedings International Conference Machine Learning,pp. 170177.Gibson, D., Kleinberg, J., & Raghavan, P. (1998). Inferring web communities linktopology. Proceedings 9th ACM Conference Hypertext Hypermedia,pp. 225234.Gilbert, E., & Karahalios, K. (2009). Predicting tie strength social media. Proceedings 27th CHI International Conference Human Factors ComputingSystems, pp. 211220.Girvan, M., & Newman, E. J. (2002). Community structure social biological networks.Proceedings National Academy Sciences, 99 (12), 78217826.Goadrich, M., & Shavlik, J. (2007). Combining clauses various precisions recallsproduce accurate probabilistic estimates. Proceedings 17th InternationalConference Inductive Logic Programming, pp. 122131.428fiTransforming Graph Data Statistical Relational LearningGobel, F., & Jagers, A. (1974). Random walks graphs. Stochastic processesapplications, 2 (4), 311336.Godbole, N., Srinivasaiah, M., & Skiena, S. (2007). Large-scale sentiment analysis newsblogs. Proceedings International Conference Weblogs SocialMedia.Golub, G., & Reinsch, C. (1970). Singular value decomposition least squares solutions.Numerische Mathematik, 14 (5), 403420.Green, J. (1972). Latitudinal variation associations planktonic Rotifera. Journalzoology, 167 (1), 3139.Gruber, A., Rosen-Zvi, M., & Weiss, Y. (2008). Latent topic models hypertext.Proceedings 24th Conference Uncertainty Artificial Intelligence, pp. 230239.Guyon, I., & Elisseeff, A. (2003). introduction variable feature selection. JournalMachine Learning Research, 3, 11571182.Gyongyi, Z., Garcia-Molina, H., & Pedersen, J. (2004). Combating web spam trustrank.Proceedings VLDB, pp. 576587.Hannan, E., & Quinn, B. (1979). determination order autoregression.Journal Royal Statistical Society. Series B (Methodological), 41, 190195.Harshman, R. (1970). Foundations PARAFAC procedure: Models conditionsexplanatory multi-modal factor analysis. UCLA Working Papers Phonetics,16 (1), 84.Hartigan, J., & Wong, M. (1979). k-means clustering algorithm. Journal RoyalStatistical Society. Series C, Applied statistics, 28, 100108.Hasan, M. A., Chaoji, V., Salem, S., & Zaki, M. (2006). Link prediction using supervisedlearning. Proceedings SDM Workshop Link Analysis, CounterterrorismSecurity.Haveliwala, T. (2003). Topic-sensitive pagerank: context-sensitive ranking algorithmweb search. IEEE transactions knowledge data engineering, 15, 784796.Hay, M., Miklau, G., Jensen, D., Towsley, D., & Weis, P. (2008). Resisting structural reidentification anonymized social networks. Proceedings VLDB, pp. 102114.He, D., & Parker, D. (2010). Topic Dynamics: alternative model Bursts StreamsTopics. Proceeding 16th ACM SIGKDD International ConferenceKnowledge Discovery Data Mining, pp. 443452.Henderson, K., Gallagher, B., Li, L., Akoglu, L., Eliassi-Rad, T., Tong, H., & Faloutsos,C. (2011). Know: Graph Mining Using Recursive Structural Features.Proceedings 17th ACM SIGKDD International Conference KnowledgeDiscovery Data Mining, pp. 110.Hill, S., Agarwal, D., Bell, R., & Volinsky, C. (2006). Building effective representationdynamic networks. Journal Computational Graphical Statistics, 15 (3),584608.429fiRossi, McDowell, Aha, & NevilleHoff, P., Raftery, A., & Handcock, M. (2002). Latent space approaches social networkanalysis. Journal American Statistical Association, 97 (460), 10901098.Hofmann, T. (1999). Probabilistic latent semantic analysis. Proceedings UncertaintyArtificial Intelligence, pp. 289296.Huh, S., & Fienberg, S. (2010). Discriminative Topic Modeling based Manifold Learning.Proceeding 16th ACM SIGKDD International Conference KnowledgeDiscovery Data Mining, pp. 653661.Huynh, T., & Mooney, R. (2008). Discriminative structure parameter learningmarkov logic networks. Proceedings 25th International Conference Machine Learning.Inokuchi, A., Washio, T., & Motoda, H. (2000). apriori-based algorithm miningfrequent substructures graph data. Principles Data Mining KnowledgeDiscovery, pp. 1323.Jaccard, P. (1901). Etude comparative de la distribution florale dans une portion des Alpeset du Jura. Impr. Corbaz.Jackson, M. (2008). Social economic networks. Princeton Univ Press.Jain, A., & Zongker, D. (1997). Feature selection: Evaluation, application, small sample performance. IEEE Transactions Pattern Analysis Machine Intelligence,19 (2), 153158.Jeh, G., & Widom, J. (2002). SimRank: measure structural-context similarity.Proceedings eighth ACM SIGKDD International Conference Knowledge Discovery Data Mining, pp. 538543.Jensen, D., & Neville, J. (2002). Linkage autocorrelation cause feature selection biasrelational learning. Proceedings 19th International Conference MachineLearning, pp. 259266.Jensen, D., Neville, J., & Gallagher, B. (2004). collective inference improves relationalclassification. Proceedings 10th ACM SIGKDD International ConferenceKnowledge Discovery Data Mining, pp. 593598.Jensen, D., Neville, J., & Hay, M. (2003). Avoiding bias aggregating relational datadegree disparity. Proceedings 20th International Conference MachineLearning, pp. 274281.Jensen, D., Fast, A., Taylor, B., & Maier, M. (2008). Automatic identification quasiexperimental designs discovering causal knowledge. Proceeding 14thACM SIGKDD International Conference Knowledge Discovery Data Mining,pp. 372380.Jeong, H., Mason, S., Barabasi, A., & Oltvai, Z. (2001). Lethality centrality proteinnetworks. Nature, 411 (6833), 4142.Jeong, H., Tombor, B., Albert, R., Oltvai, Z., & Barabasi, A. (2000). large-scaleorganization metabolic networks. Nature, 407 (6804), 651654.430fiTransforming Graph Data Statistical Relational LearningJoachims, T. (1998). Text categorization support vector machines: Learning manyrelevant features. Proceedings European Conference Machine Learning,pp. 137142.Johnson, S. (1967). Hierarchical clustering schemes. Psychometrika, 32 (3), 241254.Kahanda, I., & Neville, J. (2009). Using transactional information predict link strengthonline social networks. Proceedings 4th International Conference WeblogsSocial Media, pp. 106113.Kamvar, S., Klein, D., & Manning, C. (2003). Spectral learning. Proceedings 18thInternational Joint Conference Artificial Intelligence, pp. 561566.Kashima, H., & Abe, N. (2006). parameterized probabilistic model network evolutionsupervised link prediction. Proceedings IEEE International ConferenceData Mining, pp. 340349.Katz, L. (1953). new status index derived sociometric analysis. Psychometrika,18 (1), 3943.Kavurucu, Y., Senkul, P., & Toroslu, I. (2008). Aggregation confidence-based concept discovery multi-relational data mining. Proceedings IADIS European ConferenceData Mining, pp. 4352.Kersting, K., & De Raedt, L. (2002). Basic principles learning Bayesian logic programs.Tech. rep. 174, Institute Computer Science, University Freiburg.Kersting, K., De Raedt, L., Gutmann, B., Karwath, A., & Landwehr, N. (2008). Relationalsequence learning. Probabilistic inductive logic programming, 4911, 2855.Khosravi, H., Tong Man, O., Xu, X., & Bina, B. (2010). Structure learning markov logicnetworks many descriptive attributes. Proceedings 24th ConferenceArtificial Intelligence, pp. 487493.Khot, T., Natarajan, S., Kersting, K., & Shavlik, J. (2011). Learning markov logic networks via functional gradient boosting. Data Mining (ICDM), 2011 IEEE 11thInternational Conference on, pp. 320329. IEEE.Kim, M., & Leskovec, J. (2011). network completion problem: Inferring missing nodesedges networks. Proceedings SIAM International Conference DataMining.Kleczkowski, A., & Grenfell, B. (1999). Mean-field-type equations spread epidemics:small worldmodel. Physica A: Statistical Mechanics Applications, 274 (12), 355360.Kleinberg, J. (1999). Authoritative sources hyperlinked environment. JournalACM, 46 (5), 604632.Knobbe, A., Siebes, A., & Marseille, B. (2002). Involving aggregate functions multirelational search. Principles Data Mining Knowledge Discovery, pp. 145168.Kohavi, R., & John, G. (1997). Wrappers feature subset selection. Artificial intelligence,97 (1-2), 273324.431fiRossi, McDowell, Aha, & NevilleKohonen, T. (1990). self-organizing map. Proceedings IEEE, 78 (9), 14641480.Kok, S., & Domingos, P. (2005). Learning structure Markov logic networks.Proceedings 22nd International Conference Machine Learning, pp. 441448.Kok, S., & Domingos, P. (2007). Statistical predicate invention. Proceedings 24thInternational Conference Machine Learning, pp. 433440.Kok, S., & Domingos, P. (2008). Extracting semantic networks text via relational clustering. Proceedings European Conference Machine Learning PrinciplesPractice Knowledge Discovery Databases, pp. 624639.Kok, S., & Domingos, P. (2009). Learning markov logic network structure via hypergraphlifting. Proceedings 26th International Conference Machine Learning, pp.505512.Kok, S., & Domingos, P. (2010). Learning markov logic networks using structural motifs.Proceedings 27th International Conference Machine Learning.Kolda, T. G., Bader, B. W., & Kenny, J. P. (2005). Higher-order web link analysis usingmultilinear algebra. Proceedings IEEE International Conference DataMining, pp. 242249.Kolda, T., & Bader, B. (2006). TopHITS model higher-order web link analysis.Proceedings SIAM Data Mining Conference Workshop Link Analysis,Counterterrorism Security, pp. 2629.Koller, D., & Sahami, M. (1996). Toward optimal feature selection. Proceedings13th International Conference Machine Learning, pp. 284292.Kondor, R., & Lafferty, J. (2002). Diffusion kernels graphs discrete structures.Proceedings 19th International Conference Machine Learning, pp. 315322.Kong, X., & Yu, P. (2010). Semi-supervised feature selection graph classification. Proceeding 16th ACM SIGKDD International Conference Knowledge DiscoveryData Mining, pp. 793802.Koren, Y., North, S., & Volinsky, C. (2007). Measuring extracting proximity graphsnetworks. Transactions Knowledge Discovery Data (TKDD), 1 (3), 12:112:30.Kosala, R., & Blockeel, H. (2000). Web Mining Research: Survey. ACM SIGKDDExplorations Newsletter, 2 (1), 115.Kossinets, G., Kleinberg, J., & Watts, D. (2008). structure information pathwayssocial communication network. Proceeding 14th ACM SIGKDD InternationalConference Knowledge Discovery Data Mining, pp. 435443.Kou, Z., & Cohen, W. (2007). Stacked graphical models efficient inference markovrandom fields. Proceedings 7th SIAM International Conference DataMining, pp. 533538.Krebs, V. (2002). Mapping networks terrorist cells. Connections, 24 (3), 4352.432fiTransforming Graph Data Statistical Relational LearningKrogel, M., & Wrobel, S. (2001). Transformation-based learning using multirelational aggregation. Inductive logic programming, 2157, 142155.Kubica, J., Moore, A., Schneider, J., & Yang, Y. (2002). Stochastic link group detection.Proceedings 18th AAAI Conference Artificial Intelligence, pp. 798806.Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilistic models segmenting labeling sequence data. Proceedings 18thInternational Conference Machine Learning, pp. 282289.Landwehr, N., Kersting, K., & De Raedt, L. (2005). nFOIL: Integrating nave bayesFOIL. Proceedings 20th AAAI Conference Artificial Intelligence, pp.275282.Landwehr, N., Passerini, A., De Raedt, L., & Frasconi, P. (2010). Fast learning relationalkernels. Machine learning, 78 (3), 305342.Langville, A., & Meyer, C. (2005). Survey Eigenvector Methods Web InformationRetrieval. SIAM Review, 47 (1), 135161.Lassez, J.-L., Rossi, R., & Jeev, K. (2008). Ranking links web: Search surfengines. IEA/AIE, pp. 199208.Lee, L. (1999). Measures distributional similarity. Proceedings 37th annual meeting Association Computational Linguistics Computational Linguistics,pp. 2532.Leicht, E., Holme, P., & Newman, M. (2006). Vertex similarity networks. Physical ReviewE, 73 (2), 026120.Leiva, H. A., Gadia, S., & Dobbs, D. (2002). Mrdtl: multi-relational decision tree learningalgorithm. Proceedings 13th International Conference Inductive LogicProgramming, pp. 3856.Lempel, R., & Moran, S. (2000). stochastic approach link-structure analysis(SALSA) TKC effect. Computer Networks, 33 (1-6), 387401.Leskovec, J., Chakrabarti, D., Kleinberg, J., Faloutsos, C., & Ghahramani, Z. (2010). Kronecker graphs: approach modeling networks. Journal Machine LearningResearch, 11, 9851042.Leskovec, J., Huttenlocher, D., & Kleinberg, J. (2010). Predicting positive negativelinks online social networks. Proceedings 19th International World WideWeb Conference, pp. 641650.Letovsky, S., & Kasif, S. (2003). Predicting protein function protein/protein interactiondata: probabilistic approach. Bioinformatics, 19 (Suppl 1), i197.Li, W., & McCallum, A. (2006). Pachinko allocation: DAG-structured mixture modelstopic correlations. Proceedings 23rd International Conference MachineLearning, pp. 577584.Liben-Nowell, D., & Kleinberg, J. (2007). link-prediction problem social networks.Journal American Society Information Science Technology, 58 (7), 10191031.433fiRossi, McDowell, Aha, & NevilleLichtenwalter, R., Lussier, J., & Chawla, N. (2010). New Perspectives Methods LinkPrediction. Proceeding 16th ACM SIGKDD International ConferenceKnowledge Discovery Data Mining, pp. 243252.Lim, T., Loh, W., & Shih, Y. (2000). comparison prediction accuracy, complexity,training time thirty-three old new classification algorithms. Machine Learning,40 (3), 203228.Lin, D. (1998). information-theoretic definition similarity. Proceedings 15thInternational Conference Machine Learning, pp. 296304.Lin, F., & Cohen, W. W. (2010). Semi-supervised classification network data usinglabels. Proceedings International Conference Advances SocialNetwork Analysis Mining.Lippi, M., Jaeger, M., Frasconi, P., & Passerini, A. (2009). Relational information gain.Machine Learning, 83 (2), 121.Liu, K., & Terzi, E. (2008). Towards identity anonymization graphs. ProceedingsACM SIGMOD International Conference Management Data, pp. 93106.Liu, W., & Lu, L. (2010). Link prediction based local random walk. Europhysics Letters,89, 58007.Liu, Y., Niculescu-Mizil, A., & Gryc, W. (2009). Topic-link LDA: joint models topicauthor community. Proceedings 26th International Conference MachineLearning, pp. 665672.Long, B., Zhang, Z., & Yu, P. (2007). probabilistic framework relational clustering.Proceedings 13th ACM SIGKDD International Conference KnowledgeDiscovery Data Mining, pp. 470479.Long, B., Zhang, Z., Wu, X., & Yu, P. S. (2006). Spectral clustering multi-type relationaldata. Proceedings 23rd International Conference Machine Learning, pp.585592.Lu, C., Hu, X., Chen, X., & ran Park, J. (2010). Topic-Perspective Model SocialTagging Systems. Proceeding 16th ACM SIGKDD International ConferenceKnowledge Discovery Data Mining, pp. 683692.Lu, Q., & Getoor, L. (2003). Link-based classification. Proceedings 20th International Conference Machine Learning, pp. 496503.Macskassy, S., & Provost, F. (2003). simple relational classifier. ProceedingsSIGKDD 2nd Workshop Multi-Relational Data Mining, pp. 6476.Macskassy, S., & Provost, F. (2007). Classification networked data: toolkitunivariate case study. Journal Machine Learning Research, 8, 935983.Macskassy, S. A. (2007). Improving learning networked data combining explicitmined links. Proceedings 22nd AAAI Conference Artificial Intelligence,pp. 590595.Maes, F., Peters, S., Denoyer, L., & Gallinari, P. (2009). Simulated iterative classification new learning procedure graph labeling. Machine Learning KnowledgeDiscovery Databases, 5782, 4762.434fiTransforming Graph Data Statistical Relational LearningMallows, C. (1973). comments Cp . Technometrics, 42 (1), 8794.Maslov, S., & Sneppen, K. (2002). Specificity stability topology protein networks.Science, 296 (5569), 910913.May, R., & Lloyd, A. (2001). Infection dynamics scale-free networks. Physical ReviewE, 64 (6), 66112.McCallum, A., Wang, X., & Corrada-Emmanuel, A. (2007). Topic role discoverysocial networks experiments enron academic email. Journal ArtificialIntelligence Research, pp. 249272.McCallum, A., Wang, X., & Mohanty, N. (2007). Joint group topic discoveryrelations text. Statistical Network Analysis: Models, Issues New Directions,Lecture Notes Computer Science 4503, pp. 2844.McDowell, L., Gupta, K., & Aha, D. (2009). Cautious collective classification. JournalMachine Learning Research, 10, 27772836.McDowell, L., Gupta, K., & Aha, D. (2007). Cautious inference collective classification.Proceedings 22nd AAAI Conference Artificial Intelligence.McDowell, L., Gupta, K., & Aha, D. (2010). Meta-Prediction Collective Classification.Proceedings 23rd International FLAIRS Conference.McGovern, A., Friedland, L., Hay, M., Gallagher, B., Fast, A., Neville, J., & Jensen, D.(2003). Exploiting relational structure understand publication patterns highenergy physics. SIGKDD Explorations, 5 (2), 165172.McGovern, A., Collier, N., Matthew Gagne, I., Brown, D., & Rodger, A. (2008). Spatiotemporal Relational Probability Trees: Introduction. Eighth IEEE InternationalConference Data Mining, ICDM., pp. 935940.Menon, A., & Elkan, C. (2011). Link prediction via matrix factorization. ProceedingsEuropean Conference Machine Learning Principles Practice Knowledge Discovery Databases, pp. 437452.Menon, A., & Elkan, C. (2010). Predicting labels dyadic data. Data MiningKnowledge Discovery, 21 (2), 327343.Michie, D., Spiegelhalter, D., Taylor, C., & Campbell, J. (1994). Machine Learning, NeuralStatistical Classification. Ellis Horwood Limited.Mihalkova, L., & Mooney, R. (2007). Bottom-up learning Markov logic network structure.Proceedings 24th International Conference Machine Learning, pp. 625632.Miller, K., Griffiths, T., & Jordan, M. (2009). Nonparametric latent feature models linkprediction. Advances Neural Information Processing Systems (NIPS), 10, 12761284.Minsky, M. (1974). framework representing knowledge. Tech. rep., MassachusettsInstitute Technology, Cambridge, MA, USA.Mislove, A., Marcon, M., Gummadi, K., Druschel, P., & Bhattacharjee, B. (2007). Measurement analysis online social networks. Proceedings 7th ACM SIGCOMMConference Internet measurement, pp. 2942.435fiRossi, McDowell, Aha, & NevilleMoore, C., & Newman, M. (2000). Epidemics percolation small-world networks.Physical Review E, 61 (5), 56785682.Moreno, S., & Neville, J. (2009). investigation distributional characteristicsgenerative graph models. Proceedings 1st Workshop InformationNetworks.Nallapati, R., Ahmed, A., Xing, E., & Cohen, W. (2008). Joint latent topic models textcitations. Proceeding 14th ACM SIGKDD International ConferenceKnowledge Discovery Data Mining, pp. 542550.Namata, G., Kok, S., & Getoor, L. (2011). Collective graph identification. Proceedings17th ACM SIGKDD International Conference Knowledge Discovery DataMining, pp. 8795. ACM.Narayanan, A., & Shmatikov, V. (2009). De-anonymizing social networks. Proceedings30th IEEE Symposium Security Privacy, pp. 173187.Natarajan, S., Khot, T., Kersting, K., Gutmann, B., & Shavlik, J. (2012). Gradient-basedboosting statistical relational learning: relational dependency network case.Machine Learning, 86, 2556.Neville, J., Adler, M., & Jensen, D. (2004). Spectral clustering links attributes.Tech. rep. 04-42, Dept Computer Science, University Massachusetts Amherst.Neville, J., & Jensen, D. (2000). Iterative classification relational data. ProceedingsWorkshop SRL, 17th AAAI Conference Artificial Intelligence, pp. 4249.Neville, J., & Jensen, D. (2005). Leveraging relational autocorrelation latent groupmodels. Proceedings 5th IEEE International Conference Data Mining,pp. 322329.Neville, J., & Jensen, D. (2007). Relational dependency networks. Journal MachineLearning Research, 8, 653692.Neville, J., Jensen, D., Friedland, L., & Hay, M. (2003). Learning relational probabilitytrees. Proceedings 9th ACM SIGKDD International Conference KnowledgeDiscovery Data Mining, pp. 625630.Neville, J., Jensen, D., & Gallagher, B. (2003). Simple estimators relational Bayesianclassifers. Proceedings 3rd IEEE International Conference Data Mining,pp. 609612.Neville, J., Simsek, O., Jensen, D., Komoroske, J., Palmer, K., & Goldberg, H. (2005). Usingrelational knowledge discovery prevent securities fraud. Proceedings 11thACM SIGKDD International Conference Knowledge Discovery Data Mining,pp. 449458.Newman, M. (2010). Networks: Introduction. Oxford Univ Press.Newman, M. E. J. (2003). structure function complex networks. SIAM Review,45, 167256.Newman, M. (2001a). Clustering preferential attachment growing networks. PhysicalReview E, 64 (2), 025102.436fiTransforming Graph Data Statistical Relational LearningNewman, M. (2001b). structure scientific collaboration networks. ProceedingsNational Academy Sciences, 98 (2), 404409.Newman, M., & Girvan, M. (2004). Finding evaluating community structure networks. Physical review E, 69 (2), 26113.Ng, A., Jordan, M., & Weiss, Y. (2001). spectral clustering: Analysis algorithm.Advances Neural Information Processing Systems, pp. 849856.Nie, L., Davison, B., & Qi, X. (2006). Topical link analysis web search. Proceedings29th International ACM SIGIR Conference Research DevelopmentInformation Retrieval, p. 98.Nowicki, K., & Snijders, T. (2001). Estimation prediction stochastic blockstructures.Journal American Statistical Association, 96, 10771087.Oktay, H., Taylor, B., & Jensen, D. (2010). Causal Discovery Social Media Using QuasiExperimental Designs. Proceedings ACM SIGKDD 1st Workshop SocialMedia Analytics (SOMA-KDD).Onnela, J.-P., Saramaki, J., Hyvonen, J., Szabo, G., Lazer, D., Kaski, K., Kertesz, J., &Barabasi, A.-L. (2007). Structure tie strengths mobile communication networks.Proceedings National Academy Sciences, 104, 73327336.Page, L., Brin, S., Motwani, R., & Winograd, T. (1999). pagerank citation ranking:Bringing order web. Tech. rep., Technical Report, Stanford Digital LibraryTechnologies Project.Pang, B., & Lee, L. (2008). Opinion mining sentiment analysis. Foundations TrendsInformation Retrieval, 2 (1-2), 1135.Pastor-Satorras, R., & Vespignani, A. (2001). Epidemic spreading scale-free networks.Physical Review Letters, 86 (14), 32003203.Pasula, H., Marthi, B., Milch, B., Russell, S., & Shpitser, I. (2003). Identity uncertaintycitation matching. NIPS. MIT Press.Perlich, C., & Provost, F. (2003). Aggregation-based feature invention relational concept classes. Proceedings 9th ACM SIGKDD International ConferenceKnowledge Discovery Data Mining, pp. 167176.Perlich, C., & Provost, F. (2006). Acora: Distribution-based aggregation relationallearning identifier attributes. Machine Learning, 62, 65105.Poole, D. (2003). First-order probabilistic inference. International Joint ConferenceArtificial Intelligence, pp. 985991.Popescul, A., Popescul, R., & Ungar, L. H. (2003a). Statistical relational learninglink prediction. Proceedings Workshop Learning Statistical ModelsRelational Data IJCAI.Popescul, A., Popescul, R., & Ungar, L. H. (2003b). Structural logistic regression linkanalysis. Proceedings Second International Workshop Multi-RelationalData Mining, pp. 92106.437fiRossi, McDowell, Aha, & NevillePopescul, A., & Ungar, L. H. (2004). Cluster-based concept invention statistical relational learning. Proceedings 10th ACM SIGKDD International ConferenceKnowledge Discovery Data Mining, pp. 665670.Price, D. (1976). general theory bibliometric cumulative advantage processes.Journal American Society Information Science, 27 (5), 292306.Pudil, P., Novovicova, J., & Kittler, J. (1994). Floating search methods feature selection.Pattern recognition letters, 15 (11), 11191125.Radicchi, F., Castellano, C., Cecconi, F., Loreto, V., & Parisi, D. (2004). Definingidentifying communities networks. Proceedings National Academy Sciences,101 (9), 26582663.Rattigan, M. J., & Jensen, D. (2005). case anomalous link discovery. SIGKDDExplorations Newsletter, 7 (2), 4147.Ravasz, E., Somera, A., Mongru, D., Oltvai, Z., & Barabasi, A. (2002). Hierarchical organization modularity metabolic networks. Science, 297 (5586), 15511555.Resnick, P., & Varian, H. (1997). Recommender systems. Communications ACM,40 (3), 5658.Richardson, M., & Domingos, P. (2002). intelligent surfer: Probabilistic combinationlink content information pagerank. Advances Neural InformationProcessing Systems, pp. 14411448.Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine Learning, 62 (1),107136.Riedel, S., & Meza-Ruiz, I. (2008). Collective semantic role labelling markov logic.Proceedings Twelfth Conference Computational Natural Language Learning,pp. 193197.Robins, G., Pattison, P., Kalish, Y., & Lusher, D. (2007). introduction exponentialrandom graph (p*) models social networks. Social Networks, 29, 173191.Robins, G., Snijders, T., Wang, P., & Handcock, M. (2006). Recent developments exponential random graph (p*) models social networks. Social Networks, 29, 192215.Rossi, R., Gallagher, B., Neville, J., & Henderson, K. (2012). Dynamic behavioral mixedmembership model large evolving networks. Arxiv preprint arXiv:1205.2056,pp. 117.Rossi, R., & Neville, J. (2010). Modeling evolution discussion topics communication improve relational classification. Proceedings ACM SIGKDD 1stWorkshop Social Media Analytics (SOMA-KDD), pp. 110.Rossi, R., Gallagher, B., Neville, J., & Henderson, K. (2012). Role-Dynamics: Fast MiningLarge Dynamic Networks. LSNA-WWW, pp. 19.Rossi, R., & Neville, J. (2012). Time-evolving relational classification ensemble methods.Proceedings 16th Pacific-Asia Conference Knowledge Discovery DataMining, pp. 112.438fiTransforming Graph Data Statistical Relational LearningRoth, M., Ben-David, A., Deutscher, D., Flysher, G., Horn, I., Leichtberg, A., Leiser, N.,Merom, R., & Mattias, Y. (2010). Suggesting Friends Using Implicit Social Graph.Proceeding 16th ACM SIGKDD International Conference KnowledgeDiscovery Data Mining, pp. 233242.Russell, S. J., & Norvig, P. (2009). Artificial Intelligence: Modern Approach (3rd International Edition edition). Prentice Hall.Sabidussi, G. (1966). centrality index graph. Psychometrika, 31 (4), 581603.Salton, G., & McGill, M. (1983). Introduction modern information retrieval, Vol. 1.McGraw-Hill New York.Sarkar, P., & Moore, A. (2005). Dynamic Social Network Analysis using Latent SpaceModels. SIGKDD Explorations Newsletter, 7 (2), 3140.Sarukkai, R. R. (2000). Link prediction path analysis using markov chains. Proceedings 9th International World Wide Web Conference Computer Networks:International Journal Computer Telecommunications Networking, pp. 377386.Sarwar, B., Karypis, G., Konstan, J., & Riedl, J. (2000). Application dimensionalityreduction recommender systema case study. ACM WebKDD 2000 Web MiningE-Commerce Workshop.Schulte, O. (2011). tractable pseudo-likelihood function bayes nets applied relationaldata. SDM, pp. 462473.Schwarz, G. (1978). Estimating dimension model. annals statistics, 6 (2),461464.Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., & Eliassi-Rad, T. (2008). Collective classification network data. AI Magazine, 29 (3), 93.Shao, J. (1996). Bootstrap model selection. Journal American Statistical Association,91 (434), 655665.Shapiro, E. (1982). Algorithmic Program Debugging. ACM Distinguished Dissertation..Sharan, U., & Neville, J. (2008). Temporal-relational classifiers prediction evolvingdomains. Proceedings 8th IEEE International Conference Data Mining,pp. 540549.Shi, J., & Malik, J. (2000). Normalized cuts image segmentation. IEEE TransactionsPattern Analysis Machine Intelligence, 22 (8), 888905.Shi, X., Li, Y., & Yu, P. (2011). Collective prediction latent graphs. Proceedings20th ACM Conference Information Knowledge Management, pp. 11271136.Singla, P., & Domingos, P. (2006). Entity resolution markov logic. Proceedings6th IEEE International Conference Data Mining, pp. 572582.Srinivasan, A. (1999). aleph manual. Computing Laboratory, Oxford University, 1.Strehl, A., & Ghosh, J. (2003). Cluster ensemblesa knowledge reuse framework combining multiple partitions. Journal Machine Learning Research, 3, 583617.439fiRossi, McDowell, Aha, & NevilleTang, J., Musolesi, M., Mascolo, C., & Latora, V. (2009). Temporal distance metricssocial network analysis. Proceedings 2nd ACM workshop Online socialnetworks, pp. 3136.Tang, J., Musolesi, M., Mascolo, C., Latora, V., & Nicosia, V. (2010). Analysing informationflows key mediators temporal centrality metrics. Proceedings3rd Workshop Social Network Systems, pp. 16.Tang, L., & Liu, H. (2009). Relational learning via latent social dimensions. Proceedings15th ACM SIGKDD International Conference Knowledge DiscoveryData Mining, pp. 817826.Tang, L., & Liu, H. (2011). Leveraging social media networks classification. JournalData Mining Knowledge Discovery, 23, 447478.Taskar, B., Abbeel, P., & Koller, D. (2002). Discriminative probabilistic models relationaldata. Eighteenth Conference Uncertainty Artificial Intelligence, pp. 485492.Taskar, B., Segal, E., & Koller, D. (2001). Probabilistic classification clusteringrelational data. Proceedings 17th International Joint Conference ArtificialIntelligence, pp. 870878.Taskar, B., Wong, M., Abbeel, P., & Koller, D. (2003). Link prediction relational data.Advances Neural Information Processing Systems.Topchy, A., Law, M., Jain, A., & Fred, A. (2004). Analysis consensus partition clusterensemble. Proceedings 4th IEEE International Conference Data Mining,pp. 225232.Vert, J., & Yamanishi, Y. (2005). Supervised graph inference. Advances Neural Information Processing Systems, 17, 14331440.Vishwanathan, S., Schraudolph, N., Kondor, R., & Borgwardt, K. (2010). Graph kernels.Journal Machine Learning Research, 11, 12011242.von Luxburg, U. (2007). tutorial spectral clustering. Statistics Computing, 17 (4),395416.Wagner, A., & Fell, D. (2001). small world inside large metabolic networks. ProceedingsRoyal Society London. Series B: Biological Sciences, 268 (1478), 18031810.Wang, C., Blei, D., & Heckerman, D. (2008). Continuous time dynamic topic models.Proceedings Uncertainty Artificial Intelligence.Wang, X., & McCallum, A. (2006). Topics time: non-Markov continuous-time modeltopical trends. Proceedings 12th ACM SIGKDD International ConferenceKnowledge Discovery Data Mining, pp. 424433.Wang, Z., & Chan, L. (2010). efficient causal discovery algorithm linear models.Proceeding 16th ACM SIGKDD International Conference KnowledgeDiscovery Data Mining, pp. 11091118.Wasserman, S., & Faust, K. (1994). Social network analysis: Methods applications.Cambridge University Press.440fiTransforming Graph Data Statistical Relational LearningWatts, D., & Strogatz, S. (1998). Collective dynamics small-worldnetworks. Nature,393 (6684), 440442.White, S., & Smyth, P. (2003). Algorithms estimating relative importance networks.Proceedings ninth ACM SIGKDD International Conference KnowledgeDiscovery Data mining, pp. 266275.Wu, B., & Davison, B. (2005). Identifying link farm spam pages. Special interest tracksposters 14th International Conference World Wide Web, pp. 820829.Xiang, R., Neville, J., & Rogati, M. (2010). Modeling relationship strength online socialnetworks. Proceedings 19th International World Wide Web Conference, pp.981990.Yang, Y., & Pedersen, J. (1997). comparative study feature selection text categorization. Proceedings 14th International Conference Machine Learning,pp. 412420.Yin, X., Han, J., Yang, J., & Yu, P. (2006). Crossmine: Efficient classification across multipledatabase relations. Transactions Knowledge Data Engineering, 18 (6), 770783.Zheleva, E., Getoor, L., Golbeck, J., & Kuter, U. (2010). Using friendship ties familycircles link prediction. Advances Social Network Mining Analysis, pp.97113.Zheleva, E., & Getoor, L. (2007). Preserving privacy sensitive relationships graphdata. PinKDD, pp. 153171.Zhou, B., Pei, J., & Luk, W. (2008). brief survey anonymization techniques privacypreserving publishing social network data. SIGKDD Explorations, 10 (2), 1222.Zhou, H. (2003). Distance, dissimilarity index, network community structure. Physicalreview e, 67 (6), 61901.Zhou, T., Lu, L., & Zhang, Y. (2009). Predicting missing links via local information.European Physical Journal B-Condensed Matter Complex Systems, 71 (4), 623630.Zhu, S., Yu, K., Chi, Y., & Gong, Y. (2007). Combining content link classificationusing matrix factorization. Proceedings 30th Annual International ACMSIGIR Conference Research Development Information Retrieval, pp. 487494. ACM.Zhu, X. (2006). Semi-supervised learning literature survey. Computer Science Tech Reports,1530, 160.441fiJournal Artificial Intelligence Research 45 (2012) 257-286Submitted 6/12; published 10/12Generating Approximate Solutions TravelingTournament Problem using Linear Distance RelaxationRichard HoshinoKen-ichi Kawarabayashirichard.hoshino@gmail.comk keniti@nii.ac.jpNational Institute InformaticsJST ERATO Kawarabayashi Project2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, JapanAbstractdomestic professional sports leagues, home stadiums located citiesconnected common train line running one direction. instances,incorporate geographical information determine optimal nearly-optimal solutionsn-team Traveling Tournament Problem (TTP), NP-hard sports scheduling problem whose solution double round-robin tournament schedule minimizes sumtotal distances traveled n teams.introduce Linear Distance Traveling Tournament Problem (LD-TTP), solven = 4 n = 6, generating complete set possible solutions elementarycombinatorial techniques. larger n, propose novel expander constructiongenerates approximate solution LD-TTP. n 4 (mod 6), showexpander construction produces feasible double round-robin tournament schedule whosetotal distance guaranteed worse 43 times optimal solution, regardlessn teams located. 34 -approximation LD-TTP strongercurrently best-known ratio 35 + general TTP.conclude paper applying linear distance relaxation general (nonlinear) n-team TTP instances, develop fast approximate solutions simplyassuming n teams lie straight line solving modified problem.show technique surprisingly generates distance-optimal tournamentbenchmark sets 6 teams, well close-to-optimal schedules larger n, eventeams located around circle positioned three-dimensional space.1. Introductionpaper, introduce simple yet powerful technique develop approximate solutionsTraveling Tournament Problem(TTP), assuming n teams locatednstraight line, thereby reducing 2 pairwise distance parameters n 1 variables,solving relaxed problem.Traveling Tournament Problem (TTP) inspired real-life problemoptimizing regular-season schedule Major League Baseball. goal TTPdetermine optimal double round-robin tournament schedule n-team sportsleague minimizes sum total distances traveled n teams. Since problemfirst proposed (Easton, Nemhauser, & Trick, 2001), TTP attracted significantamount research (Kendall, Knust, Ribeiro, & Urrutia, 2010), numerous heuristicsdeveloped solving hard TTP instances, local search techniques well integerconstraint programming.c2012AI Access Foundation. rights reserved.fiHoshino & Kawarabayashimany ways, TTP variant well-known Traveling Salesman Problem(TSP), asking distance-optimal schedule linking venues close one another.computational complexity TSP NP-hard; recently, shown solvingTTP strongly NP-hard (Thielen & Westphal, 2010).Linear Distance Traveling Tournament Problem (LD-TTP), assume nteams located straight line. straight line relaxation natural heuristicn teams located cities connected common train line running onedirection, modelling actual context domestic sports leagues countries Chile,Sweden, Italy, Japan. example, Figure 1 illustrates locations six homestadiums Nippon Pro Baseballs Central League, situated close proximity majorstations Japans primary bullet-train line.Figure 1: six Central League teams Japanese Pro Baseball.Section 2, formally define TTP. Section 3, solve LD-TTP n = 4list 18 non-isomorphic tournament schedules achieving optimal distance.Section 4, solve LD-TTP n = 6 show 295 non-isomorphictournament schedules attain one seven possible values optimaldistance. Section 5, provide expander construction produce feasible doubleround-robin tournament schedule tournament n = 6m 2 teams, prove34 -approximation distance-optimal schedule, 1. Section 6,apply theories known (non-linear) 6-team benchmark sets (Trick, 2012),show cases, optimal solution appears list 295. also applyexpander construction various benchmark sets 10 16 teams, showingoptimality gap actually far lower theoretical maximum 33.3%. Section 7,in-depth analysis optimality gap, conclude paper Section 8open problems directions future research.2. Traveling Tournament ProblemLet {t1 , t2 , . . . , tn } n teams sports league, n even. Let n ndistance matrix, entry Di,j distance home stadiums teams titj . definition, Di,j = Dj,i 1 i, j n, diagonal entries Di,i zero.assume distances form metric, i.e., Di,j Di,k + Dk,j i, j, k.258fiGenerating Approximate Solutions TTP using Linear Distance RelaxationTTP requires tournament lasting 2(n 1) days, every team exactly onegame scheduled day byes days (this explains n must even.)objective minimize total distance traveled n teams, subject followingconditions:(a) each-venue: pair teams plays twice, others home venue.(b) at-most-three: team may home stand road trip lasting threegames.(c) no-repeat: team cannot play opponent two consecutive games.calculating total distance, assume every team begins tournamenthome returns home playing last away game. Furthermore, whenever teamroad trip consisting multiple away games, team doesnt return homecity rather proceeds directly next away venue.illustrate specific example, Table 1 lists distance-optimal schedule (Eastonet al., 2001) bechmark set known NL6 (six teams Major League BaseballsNational League). schedule, subsequent schedules presented paper,home games marked bold.TeamFlorida (FLO)Atlanta (ATL)Pittsburgh (PIT)Philadelphia (PHI)New York (NYK)Montreal (MON)12345ATL PHI NYK PIT NYKFLO NYK PITPHI MONNYK MON ATL FLO PHIMON FLO MON ATL PITPIT ATL FLO MON FLOPHI PITPHI NYK ATL6MONPITATLNYKPHIFLO78PIT PHIPHI MONFLO NYKATL FLOMON PITNYK ATL9MONNYKPHIPITATLFLO10ATLFLOMONNYKPHIPITTable 1: optimal TTP solution NL6.example, total distance traveled Florida DFLO,ATL +DATL,PHI +DPHI,FLO +DFLO,NYK + DNYK,MON + DMON,PIT + DPIT,FLO . Based NL6 distance matrix (Trick,2012), tournament schedule Table 1 requires 23916 miles total team travel,shown minimum distance possible.3. 4-Team LD-TTPFigure 2: general instance LD-TTP n = 4.Linear Distance TTP, assume n home stadiums lie straight line,t1 one end tn other. Thus, Di,j = Di,k + Dk,j triplets (i, j, k)259fiHoshino & Kawarabayashi1 < k < j n. Since TriangleInequality replaced Triangle Equality,longer need consider n2 entries distance matrix D; tournaments totaltravel distance function n 1 variables, namely set {Di,i+1 : 1 n 1}.notational convenience, denote di := Di,i+1 1 n 1.Teamt1t2t3t41t4t3t2t12t3t4t1t23t2t1t4t34t4t3t2t15t3t4t1t26t2t1t4t3Table 2: optimal LD-TTP solution n = 4.Table 2 gives feasible solution 4-team LD-TTP. claim solutionoptimal, possible 3-tuples (d1 , d2 , d3 ). see so, define ILBtiindependent lower bound team ti , minimum possible distance traveledti order complete games, independentteams schedules.Ptrivial lower bound total travel distance LB ni=1 ILBti .Recall calculating ti travel distance, assume ti begins tournament home returns home playing last away game. Since ti must play roadgame three teams, ILBti = 2(d1 + d2 + d3 ) 1 4.implies LB 8(d1 + d2 + d3 ). Since Table 2 tournament schedule whose totaldistance trivial lower bound, completes proof.remark Table 2 unique solution - example, generate anotheroptimal schedule simply reading Table 2 right left. Assuming first matcht1 t2 occurs home city t2 , straightforward computer search finds 18different schedules total distance 8(d1 + d2 + d3 ), provided Table 3 below.(For readability, replaced occurrence ti single index i.) Thus,symmetry, 36 optimal schedules 4-team LD-TTP. interested reader,Appendix provides actual Maplesoft code generated optimal schedules.234234143143412412321321234243143134412421321312234342143431412124321213243234134143421412312321243243134134421421312312243432134341421214312123342342431431124124213213342432431341124214213123432342341431214124123213432432341341214214123123342342431431124124213213342432431341124214213123342342431431124124213213342432431341124214213123432342341431214124123213432432341341214214123123432342341431214124123213432432341341214214123123Table 3: eighteen non-isomorphic optimal LD-TTP solutions n = 4.completes analysis 4-team Linear Distance TTP.260fiGenerating Approximate Solutions TTP using Linear Distance Relaxation4. 6-Team LD-TTPUnlike previous section, analysis 6-team LD-TTP requires work.Figure 3: general instance LD-TTP n = 6.6-team instance LD-TTP represented 5-tuple (d1 , d2 , d3 , d4 , d5 ).define = 14d1 + 16d2 + 20d3 + 16d4 + 14d5 . claim following:Theorem 1. Let 6-team instance LD-TTP. optimal solutionschedule total distance + 2 min{d2 + d4 , d1 + d4 , d3 + d4 , 3d4 , d2 + d5 , d2 + d3 , 3d2 }.prove Theorem 1 elementary combinatorial arguments, thus demonstrating utility linear distance relaxation presenting new techniques attackgeneral TTP ways differ integer/constraint programming. prooffollow several lemmas, prove one one.Lemma 1. feasible schedule must total distance least S.Proof. 1 k 5, define ck total number times team crossesbridge length dk , connecting home stadiums teamsPt5k tk+1 . Let Ztotal travel distance schedule. Since linear, Z = k=1 ck dk . Since teamcrosses every bridge even number times, ck always even.Let Lk home venues {t1 , t2 , . . . , tk } Rk home venues {tk+1 , . . . , t6 }.each-venue condition, every team Lk plays road game every team Rk .at-most-three condition, every team Lk must make least 2d 6k3 e trips acrossbridge, half trips direction. Similarly, every team Rk must makekleast 2d k3 e trips across bridge, implying ck 2kd 6k3 e + 2(6 k)d 3 e.Thus, c1 14, c2 16, cP4 16, c5 14. show c3 20,complete proof Z = ck dk 14d1 + 16d2 + 20d3 + 16d4 + 14d5 = S.Since n = 6 teams, 2(n 1) = 10 days games. 1 9,let Xi,i+1 total number times d3 -length bridge crossed teams movegames ith day games (i + 1)th day. Let Xstart,1 X10,endrespectively number times teams cross bridge playPtheir first game,return home played last game. c3 = Xstart,1 + 9i=1 Xi,i+1 +X10,end .1 9, let f (i) denote number games played L3 day i. Thus,day i, exactly 2f (i) teams left bridge 6 2f (i) teamsright. f (i) {0, 1, 2, 3} i. Since |L3 | |R3 | odd, Xstart,1 1X10,end 1.f (i) < f (i + 1), Xi,i+1 2, least two teams played R3 daymust cross play next game L3 . Similarly, f (i) > f (i + 1), Xi,i+1 2.f (i) = f (i + 1) = 1, day i, two teams p q play L3 fourteams play R3 . Xi,i+1 = 0 team crosses bridge day i, forcing p261fiHoshino & Kawarabayashiq play day + 1, thus violating no-repeat condition. Thus,least one p q must cross bridge, exchanging positions least one teammust cross play L3 . Thus, Xi,i+1 2. Similarly, f (i) = f (i + 1) = 2,Xi,i+1 2.f (i) = f (i + 1) = 0, teams play R3 days + 1. Xstart,1 = 3= 1 X10,end = 3 = 9. 2 8, {t1 , t2 , t3 } must play homegame either day 1 day + 2, order satisfy at-most-three condition. Thus,one two days, least two teams {t1 , t2 , t3 } play home, implying leastfour teams L3 . Therefore, must Xi1,i 4 Xi+1,i+2 4.derive results f (i) = f (i + 1) = 3. Xstart,1 = 3 = 1,X10,end = 3 = 9, either Xi1,i 4 Xi+1,i+2 4 2 8.double round-robin schedule, sequence{f (1), . . . , f (10)} pairP9consecutive 0s consecutive 3s, c3 = Xstart,1 + i=1 Xi,i+1 +X10,end 1+92+1 = 20.case, still c3 P20 results previous twoparagraphs. therefore proven Z =ck dk S.Lemma 2. Consider feasible schedule total distance Z =teams t1 t2 must play Days 1 10.Pck dk . c2 = 16,Proof. Lemma 1, 1 9 define Xi,i+1total numbertimes d2 -length bridge crossed teams move games ithday games (i + 1)th day. Similarly define Xstart,1X10,endP9c2 = Xstart,1 + i=1 Xi,i+1 + X10,end .Pprove 9i=1 Xi,i+116. this, 1 10, let g(i) denotenumber games played L2 (i.e., home stadiums t1 t2 ) day i.day i, exactly 2g(i) teams left d2 -length bridge 6 2g(i) teamsright. Clearly, 0 g(i) 2 1 10.|g(i + 1) g(i)| = 1, Xi,i+12, least two teams played dayone side bridge must cross play next game side.|g(i + 1) g(i)| = 2, Xi,i+1= 4.g(i) = g(i + 1) = 1, day i, two teams p q play L2 fourteams play R2 . Xi,i+1= 0 team crosses bridge day i, forcing pq play day + 1, thus violating no-repeat condition. Thus,least one p q must cross bridge, exchanging positions least one teammust cross play L2 . Thus, Xi,i+12. Similarly, g(i) = g(i + 1) = 2, twoteams p q play R2 four teams play L2 , applyargument show Xi,i+12. remaining case consider g(i) = g(i + 1) = 0,case Xi,i+1 could equal 0.Suppose days g(i) = 0, b days g(i) = 1, c days g(i) = 2.+ b + c = 10. Since t1 t2 play five home games, implies b + 2c = 10.this, see = c six possible triplets (a, b, c), namely(0, 10, 0), (1, 8, 1), (2, 6, 2), (3, 4, 3), (4, 2, 4), (5, 0, 5).= 0 = 1, Pexist index g(i) = g(i + 1) = 0, implyingXi,i+12 1 9. Hence, 9i=1 Xi,i+19 2 = 18 cases. = 2,Pone index g(i) = g(i + 1) = 0, implying 9i=1 Xi,i+116.262fiGenerating Approximate Solutions TTP using Linear Distance RelaxationPSuppose 9i=1 Xi,i+1< 16. must 3 5, two indices satisfying g(i) = g(i + 1) = 0. example,Pone 10-tuple (g(1), g(2), . . . , g(9), g(10)) =(1, 0, 0, 0, 1, 1, 1, 2, 2, 2), 9i=1 Xi,i+1= 14. simple case analysis(a, b, c) {(3, 4, 3), (4, 2, 4), (5, 0, 5)} shows bad 10-tuples violate atmost-three condition; example, 10-tuple above, either t1 t2 must play fourconsecutive road games start tournament, contradiction.P9Therefore, proveni=1 Xi,i+1 16 cases. implies= X10,end= 0. Hence, Days 1 10, t1 t2 stay L2c2 = 16, Xstart,1four teams stay R2 . Since t1 t2 teams L2 , clearly forcestwo teams play other, begin end tournament.Lemma 3. Let S1 set tournament schedules distance + 2(d2 + d4 ), S2distance + 2(d1 + d4 ), S3 distance + 2(d3 + d4 ), S4 distance + 6d4 , S5distance + 2(d2 + d5 ), S6 distance + 2(d2 + d3 ), S7 distance + 6d2 .set {S1 , S2 , . . . , S7 } non-empty.Proof. seven sets, suffices find one feasible scheduledesired total distance. {S1 , S2 , S3 , S4 }, least one set appearedpreviously literature, solution 6-team benchmark setcontext. (As see following section, label six teams NL6benchmark set Table 1 element S4 .)t1t2t3t4t5t61t2t1t4t3t6t52t3t6t1t5t4t23t4t5t6t1t2t34t6t4t5t2t3t15t3t6t1t5t4t26t5t3t2t6t1t47t6t4t5t2t3t18t4t5t6t1t2t39t5t3t2t6t1t410t2t1t4t3t6t5d1422222d2444222d3424422d4222444d5222224Table 4: Optimal CIRC6 solution, distance + 2(d2 + d4 ) = 14d1 + 18d2 + 20d3 + 18d4 + 14d5 .solution CIRC6 (Trick, 2012), Di,j = min{j i, 6 (j i)} 1 <j 6, element S1 . Table 4 provides schedule. 1 k 5, listnumber times dk bridge crossed six teams.conclude proof noting |Si+3 | = |Si | 2 4, labelteams backward t6 t1 create feasible schedule distance dk replacedd6k . Therefore, shown Si non-empty.ready prove Theorem 1, optimal solution 6-team instanceschedule appears S1 S2 . . . S7 . note seven optimaldistances minimum, depending 5-tuple (d1 , d2 , d3 , d4 , d5 ).PProof. Suppose optimal solution total distance Z =ck dk . Lemma 1,c1 , c5 14, c2 , c4 16, c3 20. Recall coefficient ck even.263fiHoshino & KawarabayashiLemma 3, S1 non-empty, schedule cannot optimal Z > +2(d2 +d4 ).Thus, c2 , c4 18, must (c1 , c2 , c3 , c4 , c5 ) = (14, 18, 20, 18, 14) Z =+ 2(d2 + d4 ), forcing schedule set S1 .Suppose c2 c4 , suffices check possibility c2 = 16. Lemma 2,t1 t2 must play Days 1 10. three cases:Case 1: c2 = 16, c1 = 14.Case 2: c2 = 16, c1 16, c4 = 16.Case 3: c2 = 16, c1 16, c4 18.Case 1, every team must travel minimum number times across d1 -d2 -bridges: team t1 take two road trips, team t2 take two road tripsplay {t3 , t4 , t5 , t6 }, {t3 , t4 , t5 , t6 } must play road games t1 t2consecutive days.symmetry, may assume first match t1 t2 occurs homecity t2 (i.e., road game t1 ). Lemma 2, schedule team t1 must onefollowing four cases, permutation {p, q, r, s} {3, 4, 5, 6}.Case#A1#A2#A3#A4Teamt1t1t1t11t2t2t2t22t?t?t?t?3tpt?tpt?4tqtptqtp5t?tqtrtq6t?t?t?tr7t?t?t?t?8trtrt?t?9tstststs10t2t2t2t2four cases, t1 plays home game ts day 9. words, ts playsroad t1 day 9, forcing ts road game t2 take place eitherday day after. latter possible, t2 already game scheduledt1 day 10; thus, ts must play road t2 day 8.Hence, t2 plays home game ts day 8 road game t1 day 10.suppose t2 home game day 9. t2 opponent day must tr ,must either Case #A1 #A2 above. (This way ensure trplays road games t1 t2 consecutive days.)Teamt1t21t1t12345678trts9tstr10t2t1six teams tournament, days 8 9, set four teamsassigned game. table, clear teams tp tq mustplay day 8 day 9, violation no-repeat condition.contradiction, therefore t2 must play road game day 9, team{t3 , t4 , t5 , t6 }.mentioned earlier, t2 take two road trips play four teams {t3 , t4 , t5 , t6 },forces one following two scenarios:264fiGenerating Approximate Solutions TTP using Linear Distance RelaxationCase#B1#B2Teamt2t21t1t12tptp3tqt?4t?t?5t?t?6t?tq7trtr8tsts9t?t?10t1t14 2 = 8 pairs matching cases t1 cases t2 , checkwhether exists feasible schedule team {t3 , t4 , t5 , t6 } plays roadgames t1 t2 consecutive days. quick check shows possibilitypairing Case #A1 Case #B1, leading following schedule firsttwo teams:Teamt1t21t2t12t?tp3tptq4tqt?5t?t?6t?t?7t?tr8trts9tst?10t2t1structural characterization reduces search space considerably,(see Appendix B) show either (i) c4 22, (ii) c3 22 c4 18. Lemma3, latter implies Z = + 2(d3 + d4 ) former implies Z = + 6d4 . Therefore,optimal schedule must S3 S4 .Case 2, demonstrate structural characterization exists c2 = c4 = 16.this, use Lemma 2 (for c2 = 16) symmetric analogue (for c4 = 16) showorder violate at-most-three no-repeat conditions, t3 t4 must playDays 1 10, well Day 2 9.violates each-venue condition. Hence, may eliminate case.Case 3, c1 16 c4 18, Z least + 2(d1 + d4 ). Lemma 3,must Z = + 2(d1 + d4 ) optimal schedule must S2 .shown c2 = 16, schedule appears S2 S3 S4 .symmetry, c4 = 16, schedule appears S5 S6 S7 . Finally, c2 , c4 18,schedule appears S1 . concludes proof.Theorem 1, seven possible optimal distances. optimal distance, enumerate set tournament schedules distance, thus producingcomplete set possible LD-TTP solutions, instances, case n = 6.Theorem 2. Consider set feasible tournaments first gamet1 t2 occurs home city t2 . 295 schedules whose total distanceappears S1 S2 . . . S7 , grouped follows:Total Distance# SchedulesS1223S24S38S424S54S68S724derive Theorem 2 computer search. {S1 , S2 , S3 , S4 }, developstructural characterization theorem, similar Case 1 above, shows feasibleschedule set must certain form. characterization reduces searchspace, brute-force search (using Maplesoft) enumerates possible schedules.took several days enumerate 223 schedules S1 , Maplesoft took less100 seconds enumerate set schedules S3 S4 . noted earlier,265fiHoshino & Kawarabayashiset schedules Si (for 2 4), immediately set schedules Si+3symmetry. full details case, refer reader Appendix B.Let us briefly explain |S1 | odd. schedule S, let (S) denote scheduleproduced playing games backwards (i.e., ti hosts tj day iff ti hosts tjday (11 d) (S).) let (S) denote schedule produced labelling sixteams reverse order (i.e., ti hosts tj day iff t7i hosts t7j day (S).)schedule S, clearly 6= (S) 6= (S).schedule S1 , exactly one (S ) ((S )) belongs S1 , since wevestipulated first game t1 t2 occurs home city t2 . Sincemapping functions () involutions, schedules S1 groupedpairs. However, 13 exceptional cases, schedule S1 pair, since= ((S )). One example given Table 5.t1t2t3t4t5t61t3t4t1t2t6t52t6t5t4t3t2t13t5t4t6t2t1t34t4t3t2t1t6t55t3t6t1t5t4t26t5t3t2t6t1t47t2t1t6t5t4t38t4t6t5t1t3t29t6t5t4t3t2t110t2t1t5t6t3t4Table 5: schedule S1 property = ((S )).schedule, pair (i, j), ti hosts tj day iff t7i hosts t7j day11 d. thirteen exceptions justify odd parity |S1 |. 2 7,schedule = ((S )), explains |Si | even cases.5. Approximation Algorithmsolved LD-TTP n = 4 n = 6, cases, determinedcomplete set schedules attaining optimal distances. natural follow-up questionwhether techniques scale larger values n. give partial answer question,show n 4 (mod 6), develop solution n-team LD-TTP whosetotal distance 33% higher optimal solution, although practiceoptimality gap actually much lower.construction 34 -approximation, note ratio strongercurrently best-known ( 53 + )-approximation general TTP (Yamaguchi, Imahori,Miyashiro, & Matsui, 2011). schedule based expander construction,completely different previous approaches generate approximate TTP solutions.describe construction, apply benchmark instances 10 teams16 teams.Let positive integer. first create single round-robin tournament U 2mteams, expand double round-robin tournament n = 6m 2 teams.use variation Modified Circle Method (Fujiwara, Imahori, Matsui, & Miyashiro,2007) build U , single round-robin schedule. Let {u1 , u2 , . . . , u2m1 , x} 2mteams. team plays 2m 1 games, according three-part construction:266fiGenerating Approximate Solutions TTP using Linear Distance Relaxation(a) 1 k m, team k plays teams following order: {2m k +1, 2m k + 2, . . . , 2m 1, 1, 2, . . . , k 1, x, k + 1, k + 2, . . . , 2m k}.(b) + 1 k 2m 1, team k plays teams following order:{2m k + 1, 2m k + 2, . . . , k 1, x, k + 1, k + 2, . . . , 2m 1, 1, 2, . . . , 2m k}.(c) Team x plays teams following order: {1, + 1, 2, + 2, . . . ,1, 2m 1, m}.u1u2u3u4u5u6u7x1xu7u6u5u4u3u2u12u2u1u7u6xu4u3u53u3xu1u7u6u5u4u24u4u3u2u1u7xu5u65u5u4xu2u1u7u6u36u6u5u4u3u2u1xu77u7u6u5xu3u2u1u4Table 6: single round-robin construction 2m = 8 teams.games involving team x, designate one home team one road teamfollows: 1 k m, uk plays road games meets team x, finishingremaining games home. + 1 k 2m 1, opposite scenario,uk plays home games meets team x, finishing remaining gamesroad. example, Table 6 provides single round-robin schedule case= 4.construction ensures 1 i, j 2m1, match ui ujexactly one home team one road team. verify this, note ui home teamuj road team iff occurs j set {1, 2m 1, 2, 2m 2, . . . , 1, + 1, m}.expand single round-robin tournament U 2m teams doubleround-robin tournament n = 6m 2 teams. accomplish this, keep xtransform uk three teams, {t3k2 , t3k1 , t3k }, set teams precisely{t1 , t2 , t3 , . . . , t6m5 , t6m4 , t6m3 , x}.t3i2t3i1t3it3j2t3j1t3j6r 5t3j1t3jt3j2t3it3i2t3i16r 4t3jt3j2t3j1t3i1t3it3i26r 3t3j2t3j1t3jt3i2t3i1t3i6r 2t3j1t3jt3j2t3it3i2t3i16r 1t3jt3j2t3j1t3i1t3it3i26rt3j2t3j1t3jt3i2t3i1t3iTable 7: Expanding one time slot U six time slots .267fiHoshino & KawarabayashiSuppose ui home team game uj , played time slot r.expand time slot U six time slots , namely slots 6r 5 6r.describe match assignments Table 7.proceeding further, let us explain idea behind construction. Recalleach-venue condition, team must visit every opponents home stadiumexactly once, at-most-three condition, road trips three games.build tournament maximizes number three-game road trips, ensuremajority road trips involve three venues closely situated one another,minimize total travel. Table 7 above, {t3j2 , t3j1 , t3j } located region,teams {t3i2 , t3i1 , t3i } play three road gamesteams highly-efficient manner.explain expand time slots games involving team x.1 k m, consider game uk x. expand time slot U sixtime slots , described Table 8.t3k2t3k1t3kx6r 5xt3kt3k1t3k26r 4t3kxt3k2t3k16r 3t3k1t3k2xt3k6r 2xt3kt3k1t3k26r 1t3kxt3k2t3k16rt3k1t3k2xt3kTable 8: six time slot expansion 1 k m.+ 1 k 2m 1, consider game uk x. expandtime slot U six time slots , described Table 9.t3k2t3k1t3kx6r 5xt3kt3k1t3k26r 4t3kxt3k2t3k16r 3t3k1t3k2xt3k6r 2xt3kt3k1t3k26r 1t3kxt3k2t3k16rt3k1t3k2xt3kTable 9: six time slot expansion + 1 k 2m 1.construction builds double round-robin tournament n = 6m 2 teams2n 2 = 12m 6 time slots. give example, Table 10 provides case= 2.straightforward verify tournament schedule n = 6m 2 teamsfeasible 1, i.e., satisfies each-venue, at-most-three, no-repeat conditions.show expander construction gives 34 -approximation LD-TTP,regardless values distance parameters d1 , d2 , . . . , dn1 .Let n-team instance LD-TTP, n = 6m 2 1. Lettotal distance optimal solution . Using expander construction, generatefeasible tournament total distance less 43 S. gives 34 -approximationLD-TTP.268fiGenerating Approximate Solutions TTP using Linear Distance Relaxationt1t2t3t4t5t6t7t8t9x1xt3t2t9t7t8t5t6t4t12t3xt1t8t9t7t6t4t5t23t2t1xt7t8t9t4t5t6t34xt3t2t9t7t8t5t6t4t15t3xt1t8t9t7t6t4t5t26t2t1xt7t8t9t4t5t6t37t5t6t4t3t1t2xt9t8t78t6t4t5t2t3t1t9xt7t89t4t5t6t1t2t3t8t7xt910 11t5 t6t6 t4t4 t5t3 t2t1 t3t2 t1x t9t9 xt8 t7t7 t812 13 14 15 16 17 18t4 t8 t9 t7 t8 t9 t7t5 t9 t7 t8 t9 t7 t8t6 t7 t8 t9 t7 t8 t9t1 x t6 t5 x t6 t5t2 t6 x t4 t6 x t4t3 t5 t4 x t5 t4 xt8 t3 t2 t1 t3 t2 t1t7 t1 t3 t2 t1 t3 t2x t2 t1 t3 t2 t1 t3t9 t4 t5 t6 t4 t5 t6Table 10: case = 2, producing 10-team tournament.Let y1 , y2 , . . . , yn n = 6m 2 teams , order, dkdistance yk yk+1 1 k n 1. map set {t1 , t2 , . . . , tn1 , x}{y1 , y2 , . . . , yn } follows: ti = yi 1 3m 3, x = y3m2 , ti = yi+13m 2 6m 3. Figure 4 below, illustrate mapping case = 2,n = 6m 2 teams divided three triplets singleton x:Figure 4: labeling n = 6m 2 teams, = 2.apply labeling expander construction create feasible n-teamtournament , n = 6m 2 1. following theorem tells us totaldistance tournament, function n 1 distance parameters d1 , d2 , . . . , dn1 .Theorem 3. Let n-team double round-robin tournament created expanderconstruction, n = 6m 2. 1 k 6m 3, let fk totalnumberPn1times dk -length bridge crossed, total distancek=1 fk dk .value fk given Table 11. addition, f1 = (8n 8)/3, f2 = 4n 4,f3m2 = fn/21 = (n2 + 6n 16)/3, f3m1 = fn/2 = (n2 + 9n 22)/3, f3m = fn/2+1 =(n2 + 9n 34)/3, f6m3 = fn1 = (8n 2)/3.Case(a)(b)(c)(d)(e)(f)kk = 4, 7, 10, . . . , 3m 5k = 5, 8, 11, . . . , 3m 4k = 3, 6, 9, 12, . . . , 3m 3k = 3m + 1, 3m + 4, . . . , 6m 8, 6m 5k = 3m + 2, 3m + 5, . . . , 6m 7, 6m 4k = 3m + 3, 3m + 6, . . . , 6m 6fk4k(n k)/3 + (6n + 8k 20)/34k(n k)/3 + (4n + 12k 20)/34k(n k)/3 + (4n + 6k 16)/34k(n k)/3 + (8n 4k 22)/34k(n k)/3 + (14n 10k 16)/34k(n k)/3 + (4n 2k 4)Table 11: formulas fk function n k.269fiHoshino & KawarabayashiProof. six cases, carefully enumerate number times teamcrosses bridge, considering activity team tournament schedule .(a) k teams left dk -length bridge, one team crosses bridge 2(nk)/3times, (k + 5)/3 teams cross bridge 2(n k + 3)/3 times (2k 8)/3 teamscross bridge 2(n k + 6)/3 times. n k 1 teams rightbridge (not including team x), (2n 3k 5)/3 teams cross bridge2(k + 2)/3 times remaining (n + 2)/3 teams cross bridge 2(k + 5)/3 times.Finally, team x crosses bridge (4k + 2)/3 times. there, sum casesdetermine fk = 4k(n k)/3 + (6n + 8k 20)/3.(b) k teams left dk -length bridge, one team crosses bridge 2(nk + 1)/3 times, (k + 4)/3 teams cross bridge 2(n k + 4)/3 times (2k 7)/3teams cross bridge 2(n k + 7)/3 times. n k 1 teams rightbridge (not including team x), (2n 3k 2)/3 teams cross bridge2(k + 1)/3 times, (n 4)/3 teams cross bridge 2(k + 4)/3 times, one teamcrosses 2(k + 7)/3 times. Finally, team x crosses bridge (4k 2)/3 times.there, sum cases determine fk = 4k(n k)/3 + (4n + 12k 20)/3.(c) k teams left dk -length bridge, (k + 6)/3 teams crossbridge 2(n k + 2)/3 times, remaining (2k 6)/3 teams cross bridge2(n k + 5)/3 times. n k 1 teams right bridge (notincluding team x), (n k 1)/3 teams cross bridge 2k/3 timesremaining 2(n k 1)/3 teams cross bridge (2k + 6)/3 times. Finally, team xcrosses bridge 4k/3 times. there, sum cases determinefk = 4k(n k)/3 + (4n + 6k 16)/3.(d) k 1 teams left dk -length bridge (not including team x), (k + 5)/3teams cross bridge 2(n k)/3 times, remaining (2k 8)/3 teams crossbridge 2(n k + 3)/3 times. n k teams right bridge,(nk +3)/3 cross bridge 2(k +2)/3 times remaining (2n2k 3)/3 teamscross bridge 2(k + 5)/3 times. Finally, team x crosses bridge 2(n k)/3 times.there, sum cases determine fk = 4k(nk)/3+(8n4k22)/3.(e) k 1 teams left dk -length bridge (not including team x), (3kn + 4)/3 teams cross bridge 2(n k + 1)/3 times, remaining (n 7)/3teams cross bridge 2(n k + 4)/3 times. n k teams rightbridge, (n k + 4)/3 cross bridge 2(k + 4)/3 times remaining(2n 2k 4)/3 teams cross bridge 2(k + 7)/3 times. Finally, team x crossesbridge 2(n k + 4)/3 times. there, sum cases determinefk = 4k(n k)/3 + (14n 10k 16)/3.(f) k 1 teams left dk -length bridge (not including team x), (3kn + 1)/3 teams cross bridge 2(n k + 2)/3 times, remaining (n 4)/3teams cross bridge 2(n k + 5)/3 times. n k teams rightbridge, (n k + 2)/3 cross bridge 2(k + 3)/3 times remaining(2n 2k 2)/3 teams cross bridge 2(k + 6)/3 times. Finally, team x crosses270fiGenerating Approximate Solutions TTP using Linear Distance Relaxationbridge 2(n k + 2)/3 times. there, sum cases determinefk = 4k(n k)/3 + (4n 2k 4).Finally, clear exceptional cases. k = 1, team t1 crosses bridge 2(n 1)/3times, remaining n 1 teams cross twice. Thus, f1 = 2(n 1)/3 + 2(n 1) =(8n 8)/3. k = 2, team t1 crosses bridge 2(n 1)/3 times, team t2 crosses 2(n + 2)/3times, (2n 5)/3 teams cross twice, (n 1)/3 teams cross four times. Thus, f2 =2(n 1)/3 + 2(n + 2)/3 + (4n 10)/3 + (4n 4)/3 = 4n 4. k = n 1, team tncrosses bridge 2(n + 2)/3 times, remaining n 1 teams cross twice. Thus,fn1 = 2(n + 2)/3 + 2(n 1) = (8n 2)/3.k = n2 1, formula fk case (d), except one teammakes additional trip across bridge. k = n2 1, formula fkcase (e), except one team makes one fewer trip across bridge. Finally,k = n2 + 1, formula fk case (f), except two teamsmake one additional trip across bridge. straightforward calculation resultsverifying f3m2 = fn/21 = (n2 + 6n 16)/3, f3m1 = fn/2 = (n2 + 9n 22)/3,f3m = fn/2+1 = (n2 + 9n 34)/3. completes proof.example, case = 2 (see Table 10), n = 10, total traveldistance 24d1 + 36d2 + 42d3 + 48d4 + 56d5 + 52d6 + 38d7 + 36d8 + 26d9 .Pn1Let = k=1lk dk trivial lower bound , found adding independentlower bounds team ti . described proof Lemma 1, lk =k2kd nk3 e + 2(n k)d 3 e k teams left dk bridge must makeleast 2d nk3 e trips across bridge, n k teams right bridge mustmake least 2d k3 e trips across.3, straightforward verify flkk < 34 1 k n 1, thusestablishing 34 -approximation LD-TTP. ratio 43 best possible duel3 = 4n 8, implying fl33 34 n .case k = 3, f3 = 16n343worst-case scenario achieved dk = 0 k 6= 3, i.e., teams {t1 , t2 , t3 }located one vertex, remaining n 3 teams located another vertex.natural question whether exist similar constructions n 0 n 2(mod 6). cases, addition n 4 case analyzed, ask whether43 -approximation best possible. one many open questions arisingwork.6. Application Benchmark Setsapply theories various benchmark TTP sets. start case n = 6,apply Theorems 1 2 known 6-team TTP benchmarks. addition NL6,examine six-team set Super Rugby League (SUPER6), six galaxy stars whosecoordinates appear three-dimensional space (GALAXY6), earlier six-team circulardistance instance (CIRC6), trivial constant distance instance (CON6)pair teams distance one unit.benchmark sets, first order six teams approximatestraight line, either formal line best fit informal check inspection.271fiHoshino & Kawarabayashiordered six teams, determinefive-tuple (d1 , d2 , d3 , d4 , d5 ) distance matrix ignore 62 5 = 10 entries. Modifying benchmark setassuming six teams lie straight line, solve LD-TTP via Theorem 1. UsingTheorem 2, take set tournament schedules achievingoptimal distance6apply actual distance matrix benchmark set (with 2 entries)optimal schedules output tournament minimum total distance.simple process, taking approximately 0.3 seconds computation time perbenchmark set, generates feasible solution 6-team TTP. surprise,algorithm outputs distance-optimal schedule five benchmark sets.unexpected result, given non-linearity data sets: example, CIRC6teams arranged circle, GALAXY6 uses three-dimensional distances.illustrate theory, let us begin NL6, ordering six teams south north:Figure 5: Location six NL6 teams.Thus, Florida t1 , Atlanta t2 , Pittsburgh t3 , Philadelphia t4 , New York t5 ,Montreal t6 . NL6 distance matrix (Trick, 2012), (d1 , d2 , d3 , d4 , d5 ) =(605, 521, 257, 80, 337).Since 2 min{d2 + d4 , d1 + d4 , d3 + d4 , 3d4 , d2 + d5 , d2 + d3 , 3d2 } = 6d4 = 480, Theorem1 tells us optimal LD-TTP solution total distance + 6d4 = 14d1 + 16d2 +20d3 + 22d4 + 14d5 = 28424. Theorem 2, 24 schedules set S4 , totaldistance + 6d4 . Two 24 schedules presented Table 12.t1t2t3t4t5t61t2t1t5t6t3t42t4t5t6t1t2t33t5t3t2t6t1t44t3t4t1t2t6t55t5t6t4t3t1t26t6t3t2t5t4t17t3t4t1t2t6t58t4t6t5t1t3t29t6t5t4t3t2t110t2t1t6t5t4t3t1t2t3t4t5t61t2t1t6t5t4t32t5t6t4t3t1t23t6t3t2t5t4t14t3t5t1t6t2t45t6t4t5t2t3t16t4t3t2t1t6t57t3t5t1t6t2t48t5t4t6t2t1t39t4t6t5t1t3t210t2t1t4t3t6t5Table 12: Two LD-TTP solutions total distance + 6d4 .Removing straight line assumption, apply actual NL6 distance matrixdetermine total distance traveled 24 schedules set S4 ,naturally produce different sums. left schedule Table 12 best among24 schedules, total distance 23916, right schedule worst, total272fiGenerating Approximate Solutions TTP using Linear Distance Relaxationdistance 24530. note left schedule, achieving optimal distance 23916miles, identical Table 1.repeat analysis four benchmark sets. each, marksets {S1 , S2 , . . . , S7 } produced optimal schedule.BenchmarkData SetNL6SUPER6GALAXY6CIRC6CON6OptimalSolution2391613036513656443LD-TTPSolution2391613036513656443OptimalScheduleS4S3S1S1S1Table 13: Comparing LD-TTP TTP benchmark data sets.sophisticated branch-and-price heuristic (Irnich, 2010) solved NL6 oneminute, yet required three hours solve CIRC6. latter problem considerablydifficult due inherent symmetry data set, required branching.However, LD-TTP approach, problems solved optimalityamount time approximately 0.3 seconds.Based results Table 13, ask whether exists 6-team instanceoptimal TTP solution different optimal LD-TTP solution. questionanswered following section.conclude section, apply 34 -approximation produced expanderconstruction various (non-linear) benchmark sets n 4 (mod 6). applyconstruction 10-team 16-team instances earlier examples (Trick, 2012).InstanceCONS10CIRC10NL10SUPER10GALAXY10CONS16CIRC16NL16GALAXY16Optimal124242594363163294535327[846, 916][249477, 261687][13619, 14900]Solution12827663850361924486233499428643915429Percentage Gap3.2%14.0%7.4%14.4%7.2%2.1%[8.5%, 17.5%][9.5%, 14.8%][3.6%, 13.3%]Table 14: Comparing construction optimal solution nine benchmark sets.GALAXY, NL, SUPER instances, first need arrange n teamsapproximate straight line. this, apply simple algorithm first randomlyassigns n teams {t1 , t2 , . . . , tn1 , x}, calculates sum total distancesadjacent pair teams. generate local line-of-best-fit recursively selectingtwo teams ti tj switching positions reduces sum n1 distances.algorithm terminates permutation n teams {t1 , t2 , . . . , tn1 , x}273fiHoshino & Kawarabayashilocally optimal (but perhaps globally), apply expander constructioncalculate total travel distance n-team tournament.Instead time-consuming process enumerates n! permutations teams,simple algorithm generates fast solution benchmark instances less2 seconds total computation time. Despite simplicity approach, seeTable 14 optimality gap extremely small constant instances (CONS),quite reasonable (non-linear) instances.7. Optimality GapTable 13, five 6-team benchmark instances produced identical solutionsTTP LD-TTP. natural question whether always case. showTTP LD-TTP solutions must identical n = 4, necessarily n = 6.instance n teams, define X total distance optimal TTPsolution, X total distance optimal LD-TTP solution. Define OGnX Xmaximum optimality gap, largest value X taken instances .Theorem 4. instance n = 4 teams, optimal TTP solution optimalLD-TTP solution. words, OG4 = 0%.Proof. Table 3, showed 18 non-isomorphic schedules total distance8(d1 + d2 + d3 ), i.e., 18 different solutions LD-TTP. 18 schedules,remove linear distance assumption determine total travel distancefunction six distance parameters (i.e., variables {Di,j : 1 < j 4}).example, schedule Table 2 total distance 4D1,2 + 2D1,3 + 2D1,4 + 3D2,3 + D2,4 +5D3,4 represent 6-tuple (4, 2, 2, 3, 1, 5). Considering 4! permutations{t1 , t2 , t3 , t4 }, 18 24 tournament schedules, producing 36 unique 6-tuples,including (4, 2, 2, 3, 1, 5). Denote L set thirty-six 6-tuples.brute-force enumeration finds 1920 feasible 4-team tournaments.1920 tournaments, determine 6-tuple representing total travel distance, find246 unique 6-tuples, denote set A. definition, L A.prove OG4 = 0, must verify set {D1,2 , D1,3 , D1,4 , D2,3 , D2,4 , D3,4 }satisfying Triangle Inequality, optimal solutions TTP LD-TTPsame, i.e., optimal solution among schedules (whose six-tuples given A)appears subset linear-distance schedules (whose six-tuples given L).establish this, first use Triangle Inequality verify 204 24636 = 210elements A\L, corresponding schedule dominated least one elementsL.example, six-tuple (3, 4, 3, 4, 1, 4) one 210 elements A\L. Comparingsix-tuple (4, 2, 2, 3, 1, 5) L, see corresponding schedule A\Ltotal distance 2D1,3 +D1,4 +D2,3 D1,2 D3,4 = (D1,3 +D1,4 D3,4 )+(D1,3 +D2,3 D1,2 ) 0corresponding schedule L, given Table 2.computer search shows 204 210 elements A\L handledapplying Triangle Inequality way, showing dominated least one element L. six exceptions, namely 6-tuples set {(2, 3, 3, 3, 3, 4),(3, 2, 3, 3, 4, 3), (3, 3, 2, 4, 3, 3), (3, 3, 4, 2, 3, 3), (3, 4, 3, 3, 2, 3), (4, 3, 3, 3, 3, 2)}.274fiGenerating Approximate Solutions TTP using Linear Distance Relaxationcases, analysis slightly harder. Consider six-tuple (2, 3, 3, 3, 3, 4); resthandled way, symmetry.twelve 6-tuples L 17 total trips, D1,2 coefficientstrictly less D4,5 coefficient. (An example one 6-tuple (4, 2, 2, 3, 1, 5).)Taking average, derive 6-tuple (7/3, 17/6, 17/6, 17/6, 17/6, 10/3), implyingexistence least one LD-TTP schedule whose total distance X (14D1,2 +17D1,3 + 17D1,4 + 17D2,3 + 17D2,4 + 20D3,4 )/6. Let distance represented6-tuple (2, 3, 3, 3, 3, 4). Triangle Inequality,6(Y X) = 2D1,2 + D1,3 + D1,4 + D2,3 + D2,4 + 4D3,4= (D1,3 + D3,4 + D4,2 D2,1 ) + (D1,4 + D4,3 + D3,2 D2,1 ) + 2D3,40 + 0 + 2 0 = 0.words, shown every element A\L dominated leastone element L. Therefore, choice {D1,2 , D1,3 , D1,4 , D2,3 , D2,4 , D3,4 } satisfyingTriangle Inequality, optimal solutions TTP LD-TTP same, i.e.,OG4 = 0%.earlier paper (Hoshino & Kawarabayashi, 2012), authors conjecturedOG6 > 0%, although unable find 6-team instance positive optimality12.3%.gap. present simple instance show OG6 43Let6-team instance D1,2 = D5,6 = 2 Di,j = 1. Clearly62 = 15 distances satisfy Triangle Inequality. show X = 431. Consider Table 15, solution TTPX = 44, thus proving OG6 43(but LD-TTP) 43 trips.t1t2t3t4t5t61t5t6t4t3t1t22t2t1t6t5t4t33t3t5t1t6t2t44t5t6t4t3t1t25t3t5t1t6t2t46t4t3t2t1t6t57t6t4t5t2t3t18t4t3t2t1t6t59t2t1t6t5t4t310t6t4t5t2t3t1# Trips778777Table 15: optimal 43-trip TTP solution beats optimal LD-TTP solution.inspection, see team travels along bridge connecting stadiumst1 t2 , along bridge connecting stadiums t5 t6 . Thus, total traveldistance must 43 1 = 43, since 2-unit distances D1,2 D5,6 appeartotal sum. Since every 6-team tournament must least 43 total trips (see Table 13),proves X = 43.295 potentially-optimal LD-TTP schedules Theorem 2, consider6! = 720 permutations (t1 , t2 , t3 , t4 , t5 , t6 ) see tournament totaldistance 43. computer search shows 36 295 schedules total distance44, none distance 43. proves X = 44 optimal LD-TTP traveldistance instance .275fiHoshino & Kawarabayashi12.3%. ask whetherTherefore, maximum optimality gap OG6 least 43gap made larger, propose following question.Problem 1. Determine value OGn n 6.Suppose OG6 = 5%. one 295 LD-TTP solutions Theorem 25% higher optimal TTP solution, found fraction computationalcost. course, necessary case n = 6 use integer constraintprogramming output TTP solution reasonable amount time. However,larger values n, linear distance relaxation technique would allow us quickly generateclose-to-optimal solutions exact optimal total distance unknown difficultcomputationally. hopeful approach help us develop better upperbounds large unsolved benchmark instances.8. Conclusionmany professional sports leagues, teams divided two conferences, teamsintra-league games within conference well inter-league gamesteams conference. TTP models intra-league tournament play. NPcomplete Bipartite Traveling Tournament Problem (Hoshino & Kawarabayashi, 2011) models inter-league play, would interesting see whether linear distance relaxationalso applied bipartite instances help formulate new ideas inter-league tournament scheduling.conclude paper proposing two new benchmark instances TravelingTournament Problem, well open problem conjecture Linear DistanceTTP. first begin benchmark instances.n 4, define LINEn instance n teams locatedstraight line, distance one unit separating pair adjacent teams, i.e., dk = 11 k n 1. define INCRn increasing-distance scenarion teams arranged dk = k 1 k n 1. Figure 6 illustrates locationteam INCR6.Figure 6: instance INCR6.definition, TTP solution matches LD-TTP solution twoinstances. Theorem 1, optimal solutions LINE6 INCR6 total distance84 250, respectively. naturally motivates following problem:Problem 2. Solve TTP instances LINEn INCRn, n 8.conclude one problem, inspired Theorem 2 listed sevenpossible optimal distances 6-team LD-TTP:276fiGenerating Approximate Solutions TTP using Linear Distance RelaxationProblem 3. Let P Dn denote number possible distances solutionn-team LD-TTP. example, P D4 = 1 P D6 = 7. Prove disprove P Dnexponential n.Acknowledgmentsresearch partially supported Japan Society Promotion Science(Grant-in-Aid Scientific Research), C & C Foundation, Kayamori Foundation,Inoue Research Award Young Scientists. authors thank Brett StevensCarleton University suggesting idea Linear Distance TTP 2010Winter Meeting Canadian Mathematical Society.Appendixused Maplesoft (www.maplesoft.com) generate set optimal LD-TTP schedulesn = 4 n = 6. appendix, explain process generated36 optimal schedules case n = 4.simplify notation, used numbers 1 4 represent team numbersopponents road games, numbers 11 14 represent team numbersopponents home games. Thus, notation, schedule left (from Table 2)identical 4 6 matrix right.Teamt1t2t3t41t4t3t2t12t3t4t1t23t2t1t4t34t4t3t2t15t3t4t1t26t2t1t4t314 13 2 4 3 1213 14 11 3 4 12 1 14 12 11 41 2 3 11 12 13produce set 36 schedules, following code used:restart: with(combinat):A1 :=A3 :=B1 :=B3 :=C1 :=C3 :=Z[{1,Z[{1,<,>(12, 1, 14, 3): A2<,>(2, 11, 14, 3): A4<,>(13, 14, 1, 2): B2<,>(3, 14, 11, 2): B4<,>(14, 13, 2, 1): C2<,>(4, 13, 2, 11): C42}] := d1: Z[{2, 3}] :=3}] := d1+d2: Z[{1, 4}]:= <,>(12, 1, 4, 13)::= <,>(2, 11, 4, 13)::= <,>(13, 4, 1, 12)::= <,>(3, 4, 11, 12)::= <,>(14, 3, 12, 1)::= <,>(4, 3, 12, 11):d2: Z[{3, 4}] := d3::= d1+d2+d3: Z[{2, 4}] := d2+d3:dist := proc (myinput, k)local i, myseq, x; x := 0; myseq :=77<= myseq[i] 7<= myseq[i+1]elif 7<= myseq[i] myseq[i+1]<7elif myseq[i]<7 7<= myseq[i+1][7, op(myinput), 7];x:=xx:=x+Z[{myseq[i+1],k}]x:=x+Z[{myseq[i],k}]277fiHoshino & Kawarabayashielif myseq[i]<7 myseq[i+1]<7 x:=x+Z[{myseq[i+1],myseq[i]}]else RETURN(ERROR)fi:od: x: end:checker := proc (my720)local k, flag, x, y, goodlist, temp; goodlist := NULL;k 720temp := Matrix([seq(my720[k][t], = 1 .. 6)]); flag := 0;x 4 5abs(temp[x][y]-temp[x][y+1])=10 flag:=1 fi:od: od:x 4dist([seq(temp[x,k],k = 1..6)],x)<>2*(d1+d2+d3) flag := 1 fi:od:flag = 0 goodlist := goodlist, temp: fi:od: goodlist: end:my720 := permute([A1, A4,my720 := permute([A1, A4,my720 := permute([A1, A4,my720 := permute([A1, A4,my720 := permute([A2, A3,my720 := permute([A2, A3,my720 := permute([A2, A3,my720 := permute([A2, A3,finallist := [set1, set2,B1, B4, C1,B1, B4, C2,B2, B3, C1,B2, B3, C2,B1, B4, C1,B1, B4, C2,B2, B3, C1,B2, B3, C2,set3, set4,C4]):C3]):C4]):C3]):C4]):C3]):C4]):C3]):set5,set1 := checker(my720):set2 := checker(my720):set3 := checker(my720):set4 := checker(my720):set5 := checker(my720):set6 := checker(my720):set7 := checker(my720):set8 := checker(my720):set6, set7, set8];Appendix Bprovide Maplesoft code generated 295 non-isomorphic schedules Theorem 2. Due symmetry, need consider cases S1 , S2 , S3 , S4 .authors would happy provide full set 295 schedules (available simple .txtfile upon request), and/or answer questions explain code generatescomplete set optimal schedules n = 6 case LD-TTP.restart: with(combinat):Z := Matrix(6, 6, 0):Z[1, 2] := a: Z[1, 3] := a+b: Z[1, 4] := a+b+c: Z[1, 5] := a+b+c+d:Z[1, 6] := a+b+c+d+e: Z[2, 3] := b: Z[2, 4] := b+c: Z[2, 5] := b+c+d:Z[2, 6] := b+c+d+e: Z[3, 4] := c: Z[3, 5] := c+d: Z[3, 6] := c+d+e:Z[4, 5] := d: Z[4, 6] := d+e: Z[5, 6] := e:6 j i+1 6 Z[j, i] := Z[i, j] od: od:all252 := choose(10, 5): combos := []:278fiGenerating Approximate Solutions TTP using Linear Distance Relaxation252test := all252[i]: flag := 0:j 2 test[j+3]-test[j] <= 3 flag := 1: fi: od:j 4 test[j+1]-test[j] >= 5 flag := 1: fi: od:or(test[1] >= 5, test[5] <= 6) flag := 1 fi:flag = 0 combos := [op(combos), test]: fi:od:totaldist := proc (myinput, k)local i, myseq, y; := 0; myseq := [7, op(myinput), 7];117<= myseq[i] 7<= myseq[i+1] y:=yelif 7<= myseq[i] myseq[i+1]<7 y:=y+Z[myseq[i+1],k]elif myseq[i]<7 7<= myseq[i+1] y:=y+Z[myseq[i],k]elif myseq[i]<7 myseq[i+1]<7 y:=y+Z[myseq[i+1],myseq[i]]else RETURN(ERROR)fi:od: y: end:getseq := proc (myfive, k)local myperm, myseq, mylist, i, j;mylist := []; myperm := permute(minus({1, 2, 3, 4, 5, 6}, {k}));120 myseq := [seq(7, = 1 .. 10)];j 5 myseq[myfive[j]] := myperm[i][j]: od:mylist := [op(mylist), myseq]od:mylist: end:checkdup := proc (tryj, j, tryk, k)local i, val1, val2, x; x := 0;i:=0: x=0 i<10i:=i+1; tryj[i]=tryk[i] tryj[i]<7 x:=1: fi: od:i:=0: x=0 i<10i:=i+1; tryj[i]=k tryk[i]<7 x:=1: fi: fi: od:i:=0: x=0 i<10i:=i+1; tryk[i]=j tryj[i]<7 x:=1: fi: fi: od:i:=0: x=0 i<10i:=i+1; tryk[i]=j val1:=i fi: tryj[i]=k val2:=i: fi: od:x = 0 abs(val1-val2) <= 1 x := 1: fi: fi:x: end:fivetuple := proc (myset)[coeff(myset,a),coeff(myset,b),coeff(myset,c),coeff(myset,d),coeff(myset,e)]:end:279fiHoshino & Kawarabayaship 5 q 5 r 55 5 k 6S[k, [2*p, 2*q, 2*r, 2*s, 2*t]] := NULL:od: od: od: od: od: od:kk 6 allvals[kk] := {}: od:r 194 x := getseq(combos[r], 1):120 in(2, {seq(x[s][k], k = 6 .. 10)}):= fivetuple(totaldist(x[s], 1));allvals[1] := {y, op(allvals[1])}; S[1, y] := S[1, y], x[s] fi: od: od:r 194 x := getseq(combos[r], 2):120 := fivetuple(totaldist(x[s], 2));allvals[2] := {y, op(allvals[2])}; S[2, y] := S[2, y], x[s] od: od:r 194 x := getseq(combos[r], 3);120 := fivetuple(totaldist(x[s], 3));allvals[3] := {y, op(allvals[3])}; S[3, y] := S[3, y], x[s] od: od:r 194 x := getseq(combos[r], 4);120 := fivetuple(totaldist(x[s], 4));allvals[4] := {y, op(allvals[4])}; S[4, y] := S[4, y], x[s] od: od:r 194 x := getseq(combos[r], 5);120 := fivetuple(totaldist(x[s], 5));allvals[5] := {y, op(allvals[5])}; S[5, y] := S[5, y], x[s] od: od:r 194 x := getseq(combos[r], 6);120 := fivetuple(totaldist(x[s], 6));allvals[6] := {y, op(allvals[6])}; S[6, y] := S[6, y], x[s] od: od:pp 10 qq 10 rr 10 ss 10triplet1[[2*pp, 2*qq, 2*rr, 2*ss, 6]] := []: od: od: od: od:pp 10 qq 10 rr 10 ss 10triplet2[[6, 2*ss, 2*rr, 2*qq, 2*pp]] := []: od: od: od: od:pp nops(allvals[1])qq nops(allvals[2])rr nops(allvals[3])val := allvals[1][pp]+allvals[2][qq]+allvals[3][rr];triplet1[val] := [op(triplet1[val]), [pp, qq, rr]]od: od: od:pp nops(allvals[4])280fiGenerating Approximate Solutions TTP using Linear Distance Relaxationqq nops(allvals[5])rr nops(allvals[6])val := allvals[4][pp]+allvals[5][qq]+allvals[6][rr];triplet2[val] := [op(triplet2[val]), [pp, qq, rr]]od: od: od:getnext := proc (inputset, x, setx)local i, k1, k2, k3, candx, mylist;mylist := NULL;k1 := op(minus({1, 2, 3, 4, 5, 6, 7}, {op(inputset[1])}));k2 := op(minus({1, 2, 3, 4, 5, 6, 7}, {op(inputset[2])}));k3 := op(minus({1, 2, 3, 4, 5, 6, 7}, {op(inputset[3])}));nops(setx) candx := setx[i];checkdup(candx, x, inputset[1], k1) = 0checkdup(candx, x, inputset[2], k2) = 0checkdup(candx, x, inputset[3], k3) = 0mylist := mylist, candx fi: fi: fi: od:[mylist]: end:getpos := proc (aval, bval, cval, dval)local pos3, pos4, pos5, pos6;aval = 3 pos3 := 2 elif aval = 4 pos4 := 2 elif aval =pos5 := 2 elif aval = 6 pos6 := 2 else RETURN(ERROR) end if;bval = 3 pos3 := 3 elif bval = 4 pos4 := 3 elif bval =pos5 := 3 elif bval = 6 pos6 := 3 else RETURN(ERROR) end if;cval = 3 pos3 := 7 elif cval = 4 pos4 := 7 elif cval =pos5 := 7 elif cval = 6 pos6 := 7 else RETURN(ERROR) end if;dval = 3 pos3 := 8 elif dval = 4 pos4 := 8 elif dval =pos5 := 8 elif dval = 6 pos6 := 8 else RETURN(ERROR) end if;[pos3, pos4, pos5, pos6]: end:5555firsttwo := proc (aval, bval, cval, dval,new1,new2)local pairs12, p, q, i, t1, t2, flag; pairs12 := {};p nops(new1) q nops(new2)checkdup(new1[p], 1, new2[q], 2) = 0new1[p][4] <> bval new1[p][6] <> cval new1[p][9] <> dvalnew2[q][2] <> aval new2[q][5] <> bval new2[q][7] <> cvalflag := 0;t1:=[2,aval,bval,new1[p][4],new1[p][5],new1[p][6],cval,dval,new1[p][9],2];t2:=[1,new2[q][2],aval,bval,new2[q][5],new2[q][6],new2[q][7],cval,dval,1];9 t1[i]=t2[i+1] t1[i+1]=t2[i] flag:=1 fi: od:flag = 0 pairs12 := {op(pairs12), [new1[p], new2[q]]} fi: fi: fi:od: od:pairs12: end:281fiHoshino & Kawarabayashifirstthree := proc (pairs12, sixpos, new6)local last6, mytry, trips126, p, q, k; last6 := {}; trips126 := {};k nops(new6) mytry := new6[k];mytry[sixpos] = 1 mytry[sixpos+1] = 2last6 := {op(last6), mytry} fi:od:p nops(pairs12) q nops(last6)checkdup(pairs12[p][1], 1, last6[q], 6) = 0checkdup(pairs12[p][2], 2, last6[q], 6) = 0trips126 := {op(trips126), [op(pairs12[p]), last6[q]]}: fi:od: od:trips126: end:steps126 := proc (new1, new2, new6)local i, j, k, finalsol; finalsol := NULL;nops(new1) j nops(new2)checkdup(new1[i], 1, new2[j], 2) = 0k nops(new6)checkdup(new1[i], 1, new6[k], 6) = 0checkdup(new2[j], 2, new6[k], 6) = 0finalsol := finalsol, [new1[i], new2[j], new6[k]]:fi: od: fi: od: od:[finalsol]: end:steps345 := proc (iset, new3, new4, new5)local mylist, tryd, trye, tryf, candd, cande, candf, a, b, c;mylist := NULL; tryd := getnext(iset, 3, new3);tryd <> [] trye := getnext(iset, 4, new4);trye <> [] tryf := getnext(iset, 5, new5);tryf <> [] nops(tryd) candd := tryd[a];b nops(trye) cande := trye[b];checkdup(candd, 3, cande, 4) = 0c nops(tryf) candf := tryf[c];checkdup(candf, 5, candd, 3) = 0checkdup(candf, 5, cande, 4) = 0mylist := mylist, [iset[1], iset[2], candd, cande, candf, iset[3]]:fi: fi: od: fi: od: od: fi: fi: fi:[mylist]: end:allsix := proc (my126,aval,bval,cval,dval,new3,new4,new5)local last3, last4, last5, k, mytry, tempval, pos3, pos4, pos5,finalresult, p, q, r, s, trips345, my345, valnext, finalans;last3:={}; last4:={}; last5:={}; finalresult:={}; trips345:={};tempval := getpos(aval, bval, cval, dval);pos3 := tempval[1]; pos4 := tempval[2]; pos5 := tempval[3];282fiGenerating Approximate Solutions TTP using Linear Distance Relaxationk nops(new3) mytry := new3[k];mytry[pos3] = 1 mytry[pos3+1] = 2checkdup(my126[1], 1, mytry, 3) = 0checkdup(my126[2], 2, mytry, 3) = 0checkdup(my126[3], 6, mytry, 3) = 0last3 := {op(last3), mytry} fi: fi:od:k nops(new4) mytry := new4[k];mytry[pos4] = 1 mytry[pos4+1] = 2checkdup(my126[1], 1, mytry, 4) = 0checkdup(my126[2], 2, mytry, 4) = 0checkdup(my126[3], 6, mytry, 4) = 0last4 := {op(last4), mytry} fi: fi:od:k nops(new5) mytry := new5[k];mytry[pos5] = 1 mytry[pos5+1] = 2checkdup(my126[1], 1, mytry, 5) = 0checkdup(my126[2], 2, mytry, 5) = 0checkdup(my126[3], 6, mytry, 5) = 0last5 := {op(last5), mytry} fi: fi:od:p nops(last3) q nops(last4)checkdup(last3[p], 3, last4[q], 4) = 0r nops(last5)checkdup(last3[p], 3, last5[r], 5) = 0checkdup(last4[q], 4, last5[r], 5) = 0trips345 := {op(trips345), [last3[p], last4[q], last5[r]]}:fi: od: fi: od: od:nops(trips345) my345 := trips345[s];finalresult := {op(finalresult),[my126[1], my126[2], my345[1], my345[2], my345[3], my126[3]]}:od:finalresult: end:checkallsolutions := proc(sixtuples)local k,rr,mysolutions,cand1,cand2,cand3,cand4,cand5,cand6,new1,new2,new3,new4,new5,new6,mytry,firsthalf,val:mysolutions := []:rr nops(sixtuples)cand1 := [S[1, allvals[1][sixtuples[rr][1]]]];cand2 := [S[2, allvals[2][sixtuples[rr][2]]]];cand3 := [S[3, allvals[3][sixtuples[rr][3]]]];cand4 := [S[4, allvals[4][sixtuples[rr][4]]]];cand5 := [S[5, allvals[5][sixtuples[rr][5]]]];cand6 := [S[6, allvals[6][sixtuples[rr][6]]]];283fiHoshino & Kawarabayashinew1 := {}; new2 := {}; new3 := {}; new4 := {}; new5 := {}; new6 := {};k nops(cand1) mytry := cand1[k]; new1 := {mytry, op(new1)}: od:k nops(cand2) mytry := cand2[k]; new2 := {mytry, op(new2)}: od:k nops(cand3) mytry := cand3[k]; new3 := {mytry, op(new3)}: od:k nops(cand4) mytry := cand4[k]; new4 := {mytry, op(new4)}: od:k nops(cand5) mytry := cand5[k]; new5 := {mytry, op(new5)}: od:k nops(cand6) mytry := cand6[k]; new6 := {mytry, op(new6)}: od:firsthalf := steps126(new1, new2, new6);k nops(firsthalf) val:=steps345(firsthalf[k],new3,new4,new5);val <> [] mysolutions := [op(mysolutions), op(val)]: fi:od: od:mysolutions: end:generatesolutions := proc(sixtuples)local cand1,cand2,cand3,cand4,cand5,cand6,new1,new2,new3,new4,new5,new6,k,rr,x,y,mytry,flag,aval,bval,cval,dval,pairs12,sixpos,trips126,my24,sols:sols := {}: my24 := permute([3,4,5,6]):rr nops(sixtuples)cand1 := [S[1, allvals[1][sixtuples[rr][1]]]];cand2 := [S[2, allvals[2][sixtuples[rr][2]]]];cand3 := [S[3, allvals[3][sixtuples[rr][3]]]];cand4 := [S[4, allvals[4][sixtuples[rr][4]]]];cand5 := [S[5, allvals[5][sixtuples[rr][5]]]];cand6 := [S[6, allvals[6][sixtuples[rr][6]]]];new1:={}; new2:={}; new3:={}; new4:={}; new5:={}; new6:={};k nops(cand1) mytry := cand1[k];{mytry[1], mytry[2], mytry[3], mytry[7], mytry[8]} = {7}mytry[10] = 2 new1 := {mytry, op(new1)}: fi: od:k nops(cand2) mytry := cand2[k];{mytry[3], mytry[4], mytry[8], mytry[9], mytry[10]} = {7}mytry[1] = 1 new2 := {mytry, op(new2)}: fi: od:k nops(cand3) mytry := cand3[k]; flag := 0;and(mytry[1] > 2, mytry[10] > 2)mytry[2] = 1 mytry[3] = 2 flag := 1 fi:mytry[3] = 1 mytry[4] = 2 flag := 1 fi:mytry[7] = 1 mytry[8] = 2 flag := 1 fi:mytry[8] = 1 mytry[9] = 2 flag := 1 fi:flag = 1 new3 := {mytry, op(new3)}: fi: fi: od:k nops(cand4) mytry := cand4[k]; flag := 0;and(mytry[1] > 2, mytry[10] > 2)mytry[2] = 1 mytry[3] = 2 flag := 1 fi:mytry[3] = 1 mytry[4] = 2 flag := 1 fi:mytry[7] = 1 mytry[8] = 2 flag := 1 fi:mytry[8] = 1 mytry[9] = 2 flag := 1 fi:flag = 1 new4 := {mytry, op(new4)}: fi: fi: od:284fiGenerating Approximate Solutions TTP using Linear Distance Relaxationk nops(cand5) mytry := cand5[k]; flag := 0;and(mytry[1] > 2, mytry[10] > 2)mytry[2] = 1 mytry[3] = 2 flag := 1 fi:mytry[3] = 1 mytry[4] = 2 flag := 1 fi:mytry[7] = 1 mytry[8] = 2 flag := 1 fi:mytry[8] = 1 mytry[9] = 2 flag := 1 fi:flag = 1 new5 := {mytry, op(new5)}: fi: fi: od:k nops(cand6) mytry := cand6[k]; flag := 0;and(mytry[1] > 2, mytry[10] > 2)mytry[2] = 1 mytry[3] = 2 flag := 1 fi:mytry[3] = 1 mytry[4] = 2 flag := 1 fi:mytry[7] = 1 mytry[8] = 2 flag := 1 fi:mytry[8] = 1 mytry[9] = 2 flag := 1 fi:flag = 1 new6 := {mytry, op(new6)}: fi: fi: od:x 24aval := my24[x][1]; bval := my24[x][2];cval := my24[x][3]; dval := my24[x][4];pairs12 := firsttwo(aval, bval, cval, dval,new1,new2);sixpos := getpos(aval, bval, cval, dval)[4];trips126 := firstthree(pairs12, sixpos, new6);nops(trips126)sols := {op(sols),op(allsix(trips126[y],aval,bval,cval,dval,new3,new4,new5))}:od:od:od:sols: end:S4cases := []:pp 8 qq 8xx := triplet1[[8, 10, 2*pp, 2*qq, 6]];yy := triplet2[[6, 6, 20-2*pp, 22-2*qq, 8]];u xx v yy S4cases := [op(S4cases), [op(u), op(v)]]:od: od: od: od:SolutionsForS4 := generatesolutions(S4cases):S3cases := []:pp 2 8 qq 8xx := triplet1[[8, 10, 2*pp, 2*qq, 6]];yy := triplet2[[6, 6, 22-2*pp, 18-2*qq, 8]];u xx v yy S3cases := [op(S3cases), [op(u), op(v)]]:od: od: od: od:SolutionsForS3 := generatesolutions(S3cases):285fiHoshino & KawarabayashiS2cases := []:pp 9 qq 9xx := triplet1[[10, 10, 2*pp, 2*qq, 6]];yy := triplet2[[6, 6, 20-2*pp, 18-2*qq, 8]];u xx v yy S2cases := [op(S2cases), [op(u), op(v)]]:od: od: od: od:SolutionsForS2 := checkallsolutions(S2cases):S1cases := []:pp 8 qq 8 rr 8xx := triplet1[[8, 2*pp, 2*qq, 2*rr, 6]]:yy := triplet2[[6, 18-2*pp, 20-2*qq, 18-2*rr, 8]]:u xx v yy S1cases := [op(S1cases), [op(u), op(v)]]:od: od: od: od: od:SolutionsForS1 := checkallsolutions(S1cases):ReferencesEaston, K., Nemhauser, G., & Trick, M. (2001). traveling tournament problem: description benchmarks. Proceedings 7th International Conference PrinciplesPractice Constraint Programming, 580584.Fujiwara, N., Imahori, S., Matsui, T., & Miyashiro, R. (2007). Constructive algorithmsconstant distance traveling tournament problem. Lecture Notes ComputerScience, 3867, 135146.Hoshino, R., & Kawarabayashi, K. (2011). Scheduling bipartite tournaments minimizetotal travel distance. Journal Artificial Intelligence Research, 42, 91124.Hoshino, R., & Kawarabayashi, K. (2012). linear distance traveling tournament problem. Proceedings 26th AAAI Conference Artificial Intelligence, 17701778.Irnich, S. (2010). new branch-and-price algorithm traveling tournament problem.European Journal Operational Research, 204, 218228.Kendall, G., Knust, S., Ribeiro, C., & Urrutia, S. (2010). Scheduling sports: annotatedbibliography. Computers Operations Research, 37, 119.Thielen, C., & Westphal, S. (2010). Complexity traveling tournament problem.Theoretical Computer Science, 412, 345351.Trick, M. (2012). Challenge traveling tournament problems.. [Online; accessed 9-June-2012].Yamaguchi, D., Imahori, S., Miyashiro, R., & Matsui, T. (2011). improved approximation algorithm traveling tournament problem. Annals Operations Research,61(4), 10771091.286fiJournal Artificial Intelligence Research 45 (2012) 515-564Submitted 8/12; published 12/12Safe Exploration State Action SpacesReinforcement LearningJavier GarcaFernando Fernandezfjgpolo@inf.uc3m.esffernand@inf.uc3m.esUniversidad Carlos III de Madrid,Avenida de la Universidad 30,28911 Leganes, Madrid, SpainAbstractpaper, consider important problem safe exploration reinforcementlearning. reinforcement learning well-suited domains complex transitiondynamics high-dimensional state-action spaces, additional challenge posedneed safe efficient exploration. Traditional exploration techniquesparticularly useful solving dangerous tasks, trial error process may leadselection actions whose execution states may result damagelearning system (or system). Consequently, agent begins interactiondangerous high-dimensional state-action space, important question arises;namely, avoid (or least minimize) damage caused explorationstate-action space. introduce PI-SRL algorithm safely improves suboptimalalbeit robust behaviors continuous state action control tasks efficientlylearns experience gained environment. evaluate proposed methodfour complex tasks: automatic car parking, pole-balancing, helicopter hovering,business management.1. IntroductionReinforcement learning (RL) (Sutton & Barto, 1998) type machine learning whosemain goal finding policy moves agent optimally environment, generally formulated Markov Decision Process (MDP). Many RL methods usedimportant complex tasks (e.g., robot control see Smart & Kaelbling, 2002; Hester,Quinlan, & Stone, 2011, stochastic games see Mannor, 2004; Konen & Bartz-Beielstein,2009 control optimization complex dynamical systems see Salkham, Cunningham,Garg, & Cahill, 2008). RL tasks focused maximizing long-term cumulative reward, RL researchers paying increasing attention long-termreward maximization, also safety approaches Sequential Decision Problems(SDPs) (Mihatsch & Neuneier, 2002; Hans, Schneegass, Schafer, & Udluft, 2008; Martn H.& Lope, 2009; Koppejan & Whiteson, 2011). Well-written reviews matters alsofound (Geibel & Wysotzki, 2005; Defourny, Ernst, & Wehenkel, 2008). Nevertheless,important ensure reasonable system performance consider safetyagent (e.g., avoiding collisions, crashes, etc.) application RL dangeroustasks, exploration techniques RL offer guarantees issues. Thus,using RL techniques dangerous control tasks, important question arises; namely,ensure exploration state-action space cause damage injuryc2012AI Access Foundation. rights reserved.fiGarca & Fernandezwhile, time, learning (near-)optimal policies? matter, words,one ensuring agent able explore dangerous environment safelyefficiently. many domains exploration/exploitation process may leadcatastrophic states actions learning agent (Geibel & Wysotzki, 2005).helicopter hovering control task one case involving high risk, since policiescrash helicopter, incurring catastrophic negative reward. Exploration/exploitationstrategies greedy may even result constant helicopter crashes (especiallyhigh probability random action selection). Another example foundportfolio theory analysts expected find portfolio maximizes profitavoiding risks considerable losses (Luenberger, 1998). Since maximization expectedreturns necessarily prevent rare occurrences large negative outcomes, differentcriteria safe exploration needed. exploration process new policiesevaluated must conducted extreme care. Indeed, environments, methodrequired explores state-action space, safe manner.paper, propose Policy Improvement Safe Reinforcement Learning(PI-SRL) algorithm safe exploration dangerous continuous control tasks.method requires predefined (and safe) baseline policy assumed suboptimal(otherwise, learning would pointless). Predefined baseline policies useddifferent ways approaches. work Koppejan Whiteson (2011), singlelayers perceptrons evolved, albeit starting prototype network whose weights correspond baseline policy provided helicopter control task competition software (Abbeel,Coates, Hunter, & Ng, 2008). approach viewed simple form population seeding proven advantageous numerous evolutionary methods(e.g. see Hernandez-Daz, Coello, Perez, Caballero, Luque, & Santana-Quintero, 2008; Poli& Cagnoni, 1997). work Martn de Lope (2009), weights neural networks also evolved inserting several baseline policies (including providedhelicopter control task competition software) initial population. minimizepossibility evaluating unsafe policies, approach prevents crossover mutationoperators permitting anything tiny changes initial baseline policies.paper, present PI-SRL algorithm, novel approach improving baselinepolicies dangerous domains using RL. PI-SRL algorithm composed two different steps. first, baseline behavior (robust albeit suboptimal) approximatedusing behavioral cloning techniques (Anderson, Draper, & Peterson, 2000; Abbott, 2008).order achieve goal, case-based reasoning (CBR) techniques (Aamodt & Plaza,1994; Bartsch-Sprl, Lenz, & Hbner, 1999) used successfully appliedimitation tasks past (Floyd & Esfandiari, 2010; Floyd, Esfandiari, & Lam, 2008).second step, PI-SRL algorithm attempts safely explore state-action spaceorder build accurate policy previously-learned behavior. Thus, setcases (i.e., state-action pairs) obtained previous phase improvedsafe exploration state-action space. perform exploration, small amountsGaussian noise randomly added greedy actions baseline policy approach.exploration strategy used successfully previous works (Argall, Chernova,Veloso, & Browning, 2009; Van Hasselt & Wiering, 2007).novelty present study use two new, main components: (i) riskfunction determine degree risk particular state (ii) baseline behavior516fiSafe Exploration State Action Spaces Reinforcement Learningcapable producing safe actions supposedly risky states (i.e., states leaddamage injury). addition, present new definition risk basedagent unknown known space. described Section 5 greater detail,new definition completely different traditional definitions risk found literature (Geibel, 2001; Mihatsch & Neuneier, 2002; Geibel & Wysotzki, 2005). paper alsoreports experimental results obtained application new approach fourdifferent domains: (i) automatic car parking (Lee & Lee, 2008), (ii) pole-balancing (Sutton& Barto, 1998), (iii) 2009 RL Competition helicopter hovering (Ng, Kim, Jordan, & Sastry,2003) (iv) business management (Borrajo, Bueno, de Pablo, Santos, Fernandez, Garca,& Sagredo, 2010). domain, propose learning near-optimal policy which,learning phase, minimize car crashes, pole disequilibrium, helicopter crashescompany bankruptcies, respectively. important note comparisonapproach agent optimal exploration policy possible since,proposed domains (each high-dimensional continuous state action space,well complex stochastic dynamics), know optimal exploration policyis.Regarding organization remainder paper, Section 2 introduces keydefinitions, Section 3 describes detail learning approach proposed. Section 4,evaluation performed four mentioned domains presented. Section 5discusses related work Section 6 summarizes main conclusions study.sections,term return used refer expected cumulative future discountedPreward R = t=0 rt , term reward used refer single real value usedevaluate selection action particular state denoted r.2. Definitionsillustrate concept safety used approach, navigation problem presentedFigure 1. navigation problem presented Figure 1, control policy mustlearned get particular start state goal state, given set demonstrationtrajectories. environment, assume task difficult due stochasticcomplex dynamic environment (e.g., extremely irregular surface caserobot navigation domain wind effects case helicopter hover task).stochasticity makes impossible complete task using exactly trajectoryevery time. Additionally, problem supposes set demonstrations baselinecontroller performing task (the continuous black lines) also given. setdemonstrations composed different trajectories covering well-defined regionstate space (the region within rectangle).approach based addition small amounts Gaussian noise perturbations baseline trajectories order find new better ways completingtask. noise affect baseline trajectories different ways, dependingamount noise added which, turn, depends amount risk taken. riskdesired, noise added baseline trajectories 0 and, consequently, newimproved behavior discovered (nevertheless, robot never fall cliffhelicopter never crash). If, however, intermediate level risk desired,small amounts noise added baseline trajectories new trajectories (the517fiGarca & FernandezFigure 1: Exploration strategy based addition small amounts noise baselinepolicy behavior. Continuous lines represent baseline behavior, newlyexplored behaviors indicated dotted dashed lines.dotted blue lines) complete task discovered. cases, exploration newtrajectories leads robot unknown regions state space (the dashed red lines).robot assumed able detect situations risk function usebaseline behavior return safe, known states. If, instead, high risk desired,large amounts noise added baseline trajectories, leading discoverynew trajectories (but also higher probability robot gets damaged).iteration process leads robot progressively safely explore stateaction spaces order find new improved ways complete task. degreesafety exploration, however, depend risk taken.2.1 Error Non-Error Statespaper, follow far notation presented Geibel et al. (2005)definition concept risk. study, Geibel et al. associate risk errorstates non-error states, former understood state consideredundesirable dangerous enter.Definition 1 Error non-error states. Let set states seterror states. state undesirable terminal state controlagent ends reached damage injury agent, learning systemexternal entities. set considered set non-error terminal states= control agent ends normally without damage injury.terms RL, agent enters error state, current episode ends damagelearning system (or systems); whereas enters non-error state, episodeends normally without damage. Thus, Geibel et al. define risk respectpolicy , (s), probability state sequence (si )i0 s0 = s, generatedexecution policy , terminates error state s0 . definition, (s) = 1. , (s) = 0 = . states/ , risktaken depends actions selected policy . definitions,518fiSafe Exploration State Action Spaces Reinforcement Learningtheoretical framework introduce definition risk associatedknown unknown states.2.2 Known Unknown States Continuous Action State Spacesassume continuous, n-dimensional state space <n state = (s1 , s2 , . . . ,sn ) vector real numbers dimension individual domain Dis <.Similarly, assume continuous m-dimensional action space <maction = (a1 , a2 , . . . , ) vector real numbers dimensionindividual domain Dia <. Additionally, agent considered endowedmemory, case-base B, size . memory element represents state-action pair,case, agent experienced before.Definition 2 (Case-base). case-base set cases B = {c1 . . . , c }. Every caseci consists state-action pair (si , ai ) agent experienced pastassociated value V (si ). Thus, ci =< si , ai , V (si ) >, first element representscases problem part corresponds state si , following element ai depicts casesolution (i.e., action expected agent state si ) final elementV (si ) value function associated state si . state si composed ncontinuous state variables action ai composed continuous action variables.agent receives new state sq , first retrieves nearest neighbor sqB according given similarity metric performs associated action.paper, use Euclidean distance similarity metric (Equation 1).vuXu nd(sq , si ) = (sq,j si,j )2(1)j=0Euclidean distance metric useful value function expected continuous smooth throughout state space (Santamara, Sutton, & Ram, 1998). However,since value function unknown priori Euclidean distance metric particularly suitable many problems, many researchers begun ask distancemetric learn adapt order achieve better results (Taylor, Kulis, & Sha,2011). use distance metric learning techniques would certainly desirableorder induce powerful distance metric specific domain, considerationlies outside scope present study. paper, therefore, focuseddomains Euclidean distance proven successful (i.e., successfully applied car parking (Cichosz, 1995), pole-balancing (Martin H & de Lope, 2009),helicopter hovering control (Martin H & de Lope, 2009) SIMBA (Borrajo et al., 2010).Traditionally, case-based approaches use density threshold order determinenew case added memory. distance nearest neighborsq greater , new case added. sense, parameter defines sizeclassification region case B (Figure 2). new case sq withinclassification region case ci , considered known state. Hence, casesassociated value function V BB describe case-based policy agent B.519fiGarca & FernandezFigure 2: Known Unknown states.Definition 3 (Known/Unknown states). Given case-base B = {c1 . . . , c } composedcases ci = (si , ai , V (si )) density threshold , state sq considered knownmin1i d(sq , si ) unknown cases. Formally, set knownstates, set unknown states = = S.Definition 3, states identified known unknown. agentreceives new state , performs action ai case ci d(s, si ) =min1j d(s, sj ). However, agent receives state where, definition,distance state B larger , case retrieved. Consequently, actionperformed state unknown agent.Definition 4 (Case-Based risk function). Given case base B = {c1 . . . , c } composedcases ci = (si , ai , V (si )), risk state defined Equation 2.B%(s) =01min1j d(s, sj ) <otherwise(2)Thus, %B (s) = 1 holds (i.e., unknown), stateassociated case and, hence, action performed given situationunknown. , %B (s) = 0.derived caseDefinition 5 (Safe case-based policy). case-based policy Bbase B = {c1 . . . . , c } safe when, initial known state s0 respect B,always produces known non-error states respect B.execution BBs0 | %B (s0 ) = 0, (si )i>0%B (si ) = 0(3)Additionally, assumed probability state sequence (si )i0, terminates error stateknown state s0 , generated executing policy BB (s0 ) = 0 (i.e., = ).520fiSafe Exploration State Action Spaces Reinforcement LearningDefinition 6 (Safe case-based coverage). coverage single state respectsafe case-base B = {c1 . . . . , c } defined state si min1i d(s, si ) .Therefore, assume safe case-based provide actions entire statespace, rather known states .Figure 3 graphically represents relationship known/unknown error/non learnt,error states. green area image denotes safe case-based policy Barea state space corresponding initial known space. agent followingalways green area resulting episodes end withoutpolicy Bdamages. Consequently, subset non-error states also form part known space.Formally, let subsets non-error states belonging known unknownspaces, respectively, = . . yellow area Figure,contrast, represents unknown space . space found error states,well subset remaining non-error states. Formally, .Understood way, PI-SRL algorithm summed follows:.first step, learn known space (green area) safe case-based policy Bsecond step, adjust known space (green area) unknown space (yellowarea) order explore new improved behaviors avoiding error states (redarea). process adjusting known space space used safebetter policies, algorithm forget ineffectual known states, shownSection 4.Figure 3: Known/unknown error/non-error states given Case Base B.2.3 Advantages Using Prior Knowledge PredeterminedExploration Policiespresent subsection, advantages using teacher knowledge RL, namely (i)provide initial knowledge task learned (ii) support explorationprocess, highlighted. Furthermore, explain believe knowledge521fiGarca & Fernandezindispensable RL tackling highly complex realistic problems large, continuousstate action spaces particular action may result undesirableconsequence.2.3.1 Providing Initial Knowledge TaskRL algorithms begin learning without previous knowledge tasklearnt. cases, exploration strategies greedy used. applicationstrategy results random exploration state action spaces gatherknowledge task. enough information discovered environment algorithms behavior improve. random exploration policies, however,waste significant amount time exploring irrelevant regions state actionspaces optimal policy never encountered. problem compoundeddomains extremely large continuous state action spaces randomexploration never likely visit regions spaces necessary learn (near-)optimalpolicies. Additionally, many real RL tasks real robots, random explorationgather information environment cannot even applied. real robots,considered sufficient information much information real robotgather environment. Finally, impossible avoid undesirable situationshigh-risk environments without certain amount prior knowledge task,use random exploration would require undesirable state visitedlabeled undesirable. However, visits undesirable states may result damageinjury agent, learning system external entities. Consequently, visitsstates avoided earliest steps learning process.Mitigating difficulties described above, finite sets teacher-provided examplesdemonstrations used incorporate prior knowledge learning algorithm.teacher knowledge used two general ways, either (i) bootstrap learning algorithm (i.e., sort initialization procedure) (ii) derive policyexamples. first case, learning algorithm provided examples demonstrations bootstrap value function approximation lead agentrelevant regions space. second way teacher knowledgeused refers Learning Demonstration (LfD) approaches policy derived finite set demonstrations provided teacher. principal drawbackapproach, however, performance derived policy heavily limitedteacher ability. one way circumvent difficulty improve performanceexploring beyond provided teacher demonstrations, raisesquestion agent act encounters state demonstrationexists (an unknown state).2.3.2 Supporting Exploration Processfurnishing agent initial knowledge helps mitigate problems associatedrandom exploration, alone sufficient prevent undesirable situationsarise subsequent explorations undertaken improve learner ability. additional mechanism necessary guide subsequent exploration process wayagent may kept far away catastrophic states. paper, teacher,522fiSafe Exploration State Action Spaces Reinforcement Learningrather policy derived current value function approximation usedselection actions unknown states. One way prevent agent encounteringunknown states exploration process would requesting beginningteacher demonstration every state state space. However, strategypossible due (i) computational infeasibility given extremely large number statesstate space (ii) fact teacher forced give actionevery state, given many states ineffectual learning optimal policy.Consequently, PI-SRL requests teacher action action actually required(i.e., agent unknown state).paper supposes teacher available task learned,teacher taken baseline behavior. Although studies examined userobotic teachers, hand-written control policies simulated planners, great majoritydate made use human teachers. paper uses suboptimal automatic controllersteachers, taken teachers policy.Definition 7 (Baseline behavior). Policy considered baseline behaviorthree assumptions made: (i) able provide safe demonstrationstask learnt prior knowledge extracted; (ii) able supportsubsequent exploration process, advising suboptimal actions unknown states reduceprobability entering error states return system known situation;(iii) performance far optimal.optimal baseline behaviors certainly ideal behave safely, non-optimal behaviors often easy (or easier) implement generate optimal ones. PI-SRLalgorithm uses baseline behavior two different ways. First, uses safe demonstrations provide prior knowledge task. step, algorithm buildsinitial known space agent derived safe case-based policy Bpurpose mimicking B . second step, PI-SRL uses supportsubsequent exploration process conducted improve abilities previously-learnt. exploration process continues, action requested required,Bis, agent unknown state (Figure 4). step, acts backuppolicy case unknown state intention guiding learning awaycatastrophic errors or, least, reducing frequency. important notebaseline behavior cannot demonstrate correct action every possible state. However,baseline behavior might able indicate best action cases,action supplies should, least, safer obtained randomexploration.2.4 Risk Parameterorder maximize exploration safety, seems advisable movementstate space arbitrary, rather known space expanded graduallystarting known state. exploration carried perturbation. Perturbation trajectoriesstate-action trajectories generated policy Baccomplished addition Gaussian random noise actions B orderobtain new ways completing task. Thus, Gaussian exploration takes place523fiGarca & FernandezFigure 4: exploration process PI-SRL requests actions baseline behavior, ,really required.around current approximation action ai current known state sc ,ci = (si , ai , V (si )) d(sc , si ) = min1j d(s, sj ). action performed sampledGaussian distribution mean action output given instance selectedB. ai denotes algorithm action output, probability selecting action a0i ,(s, a0i ) computed using Equation 4.(s, a0i ) =0221 e(ai ai ) /22 22 > 0.(4)shape Gaussian distribution depends parameter (standard deviation).study, used width parameter. large values imply wide bellshaped distribution, increasing probability selecting actions a0i differentcurrent action ai , small value implies narrow bell-shaped distribution, increasingprobability selecting actions a0i similar current action ai . 2 = 0,assume (s, ai ) = 1. Hence, value directly related amount perturbation. Higher values implyadded state-action trajectories generated policy Bgreater perturbations (more Gaussian noise) greater probability visiting unknownstates.Definition 8 (Risk Parameter). parameter considered risk parameter. Largevalues increase probability visiting distant unknown states and, hence, increaseprobability reaching error states.exploratory actions drive agent edge known space forcego slightly beyond, unknown space, search better, safer behaviors.period time, execution exploratory actions increases known space. riskimproves abilities previously-learned safe case-based policy Bparameter , well , design parameters must selected user.Section 3.3, guidelines selection offered.important note approach proposed study based two logicalassumptions RL derived following generalization principles (Kaelbling, Littman,& Moore, 1996; Sutton & Barto, 1998):524fiSafe Exploration State Action Spaces Reinforcement Learning(i) Nearby states similar optimal actions. continuous state spaces,impossible agent visit every state store value (or optimal action)table. generalization techniques needed. large, smooth state spaces,similar states expected similar values similar optimal actions. Therefore,possible use experience gathered environment limited subsetstate space produce reliable approximation much larger subset (Boyan, Moore,& Sutton, 1995; Hu, Kostiadis, Hunter, & Kalyviotis, 2001; Fernandez & Borrajo, 2008).One must also note that, proposed domains, optimal action also consideredsafe action sense never produces error states (i.e., action consideredoptimal leads agent catastrophic situation).(ii) Similar actions similar states tend produce similar effects. Considering deterministic domain, action performed state st always producesstate st+1 . stochastic domain, understood intuitively executionaction state st produce similar effects (i.e., produces states {s1t+1 , s2t+1 , s3t+1 , . . .}i, j 6= j dist(sit+1 , sjt+1 ) 0). Additionally, execution action a0tstate s0t st produces states {s0 1t+1 , s0 2t+1 , s0 3t+1 , . . .} i, j dist(s0 it+1 , sjt+1 ) 0.explained earlier, present study uses Euclidean distance similarity metric,proven successful proposed domains. result assumption,approximation techniques used, actions generate similar effectsgrouped together one action (Jiang, 2004). continuous action spaces, needgeneralization techniques even greater (Kaelbling et al., 1996). paper,assumption also allows us assume low values increase probability visitingknown states and, hence, exploring less taking less risks, greater valuesincrease probability reaching error states.3. PI-SRL AlgorithmPI-SRL algorithm composed two main steps described detail below.3.1 First Step: Modeling Baseline Behaviors CBRfirst step PI-SRL approach behavioral cloning, using CBR allow softwareagent behave similar manner teacher policy (baseline behavior) (Floyd et al.,2008). Whereas LfD approaches named differently according learned (Argall et al., 2009), prevent terminological inconsistencies here, consider behavioralcloning (also known imitation learning) area LfD whose goal reproduction/mimicking underlying teacher policy (Peters, Tedrake, Roy, & Morimoto,2010; Abbott, 2008).using CBR behavioral cloning, case built using agents statereceived environment, well corresponding action command performedteacher. PI-SRL, objective first step properly imitate behaviorusing cases stored case-base. point, important question arises; namely,case-base B learnt using sample trajectories provided that,end learning process, resulting policy derived B mimics behavior? Baseline behavior function maps states actions : or,525fiGarca & Fernandezwords, function that, given state si S, provides corresponding action ai A.paper, want build policy B derived case-base composed cases (sj , aj )that, new state sq , case minimum Euclidean distance dist(sq , sj )retrieved corresponding action aj returned. Intuitively, assumedB built simply storing cases (si , ai ) gathered one interactionenvironment limited number episodes K. end K episodes,one expects resulting B able properly mimic behavior . However,informal experimentation helicopter hovering domain shows case(Section 4.3). helicopter hovering, K = 100 episodes prohibitive number600,000 cases stored, policy derived case-base B unable correctlyimitate baseline behavior and, instead, continuously crashes helicopter. Indeed,order B mimic large continuous stochastic domains, approachrequires larger number episodes and, consequently, prohibitive number cases.fact, perfectly mimic domains, infinite number cases would required.Figure 5 attempts explain believe learning process work.it, region space represented simply storing cases derived formc = (s, a) shown. stored case (red circles) covers area space representscentroid Voronoi region.Figure 5: Effects storing training cases.previously-learned policy B used new state sq presented, action aj performed, corresponding case cj = (sj , aj ) Euclidean distancedist(sq , sj ) less stored cases. However, use policyprovide action situation sq , action ai provided different aj .point, policy B said classify state sq obtained class aj ,policy said classify state sq desired class ai (insofarpolicy mimicked), |ai aj | > 0. Furthermore, |ai aj | understoodclassification error. case-base stored possible pairs (si , ai )able generate domain, actions aj ai would always identical,dist(sq , sj ) = 0 |ai aj | = 0. However, stochastic large, continuous domain,impossible store cases. sum classification errors episode526fiSafe Exploration State Action Spaces Reinforcement Learningleads visiting unexplored regions case space (i.e., regions newstate sq received environment Euclidean distance dist(sq , sj ) >>respect closest case cj = (sj , aj ) B). unexplored regions visited,difference obtained class derived B desired class derivedlarge (i.e., |ai aj | >> 0) probability error states might visitedgreatly increases.may concluded, therefore, simply storing pairs c = (s, a) generatedsufficient properly mimic behavior. reason, algorithm Figure 6proposed.CBR Approach Behavioral Cloning00010203GivenGivenGiven1. Set040506072. RepeatSet k = 0k < maxEpisodeLengthCompute case < sc , ac , 0 > closest current state sk08091011121314151617181920baseline behaviordensity thresholdmaximum number casescase-base B =%B (sk ) = 0 // equation 2Set ak = acelseSet ak using baseline behaviorCreate new case cnew = (sk , ak , 0)B := B cnewExecute ak , receive sk+1Set k = k + 1endkBk >Remove kBk least-frequently-used cases Bstop criterion becomes true3. Return B performing safe case-based policy BFigure 6: CBR algorithm behavioral cloning.first step algorithm, state-value function V B (si ) initialized 0 (seeline 07). value V B (si ) case computed second step algorithmSection 3.2. Additionally, step uses case-based risk function (Equation 2)determine whether new state sk considered risky (line 08). new staterisky (i.e., known state sk ), 1-nearest neighbor strategy followed (line09). Otherwise, algorithm performs action ak using baseline behaviornew case cnew = (sk , ak , 0) built added case-base B (line 13). Startingempty case-base, learning algorithm continuously increases competence storingnew experiences. However, number reasons inflow new caseslimited. Large case-bases increase time required find closest cases newexample. may partially solved using techniques reduce retrieval time(e.g., k-d trees used work), nevertheless reduce storage527fiGarca & Fernandezrequirements. Several approaches removal ineffectual cases training exist,including Ahas IBx algorithms (Aha, 1992) nearest prototype approach (Fernandez& Isasi, 2008). number cases stored B exceeds critical value kBk >realization retrieval within certain amount time cannot guaranteed,removal cases inevitable. efficient approach problemremoval least-frequently-used elements B (line 18).result step constrained case-base B describing safe case-based policyB mimics baseline behavior , though perhaps deviation (line 20).Formally, let U (T ) estimate utility baseline behavior computed) U ( ).averaging sum rewards accumulated NT trials. Then, U (B3.2 Second Step: Improving Learned Baseline Behaviorlearned previousstep PI-SRL algorithm, safe case-based policy Bstep improved safe exploration state-action space. First, case ciB, state-value function V B (si ) computed following Monte Carlo (MC) approach(Figure 7).MC Algorithm Adapted CBR00010203Given case-base B1. Initialize, ci BV (s) arbitraryReturns(s) empty list0405060708092. k < maxN umberEpisodesGenerate episode using Bappearing episode < s, a, V (s) > BR return following first occurrenceAppend R Returns(s)V(s) average(Returns(s))1011Set k = k + 13. Return BFigure 7: Monte Carlo algorithm computation state-value function case.algorithm similar spirit first-visit MC method V (Sutton & Barto,1998), adapted paper work policy given case-base. algorithmshown Figure 7, returns state si B accumulated averaged, followingderived case base B (see line 09). important notepolicy Balgorithm term return following first occurrence refers expected return(i.e., expected cumulative future discounted reward starting state), whereasReturns refers list composed return different episodes. Oneprincipal reasons using MC method allows us quickly easily estimatestate values V B (si ) case ci B. addition, MC methods shownsuccessful wide variety domains (Sutton & Barto, 1998). state-valuefunction V B (si ) computed case ci B, small amounts Gaussian noiseorder obtain new improved waysrandomly added actions policy B528fiSafe Exploration State Action Spaces Reinforcement Learningcomplete task. algorithm used improve baseline behavior learnedprevious step depicted Figure 8. algorithm composed four steps performedepisode.- (a) Initialization step. algorithm initializes list used store cases occurringepisode sets cumulative reward counter episode 0.- (b) Case Generation. algorithm builds case step episode.new state sk , closest case < s, a, V (s) > B computed using Euclideandistance metric Equation 1 (see line 09 algorithm Figure 8). order determineperceived degree risk new state sk , case-based risk function used (line10). %B (sk ) = 0, sk (known state). case, action ak performedcomputed using Equation 4 new case cnew =< s, ak , V (s) > built addedlist cases occurred episode (line 13). important notenew case < s, ak , V (s) > built replacing action corresponding closest case< s, a, V (s) > B, new action ak resulting application randomGaussian noise Equation 4. Thus, algorithm produces smooth changescases B ak a. If, however, %B (sk ) = 1, state sk (i.e., unknownstate [line 14]). unknown states, action ak performed suggested baselinebehavior defines safe behavior (line 15). new case < sk , ak , 0 > builtadded list cases episode actions performed usingagent known state. Finally, reward obtained episode accumulated,r(sk , ak ) immediate reward obtained action ak performed state sk(line 18).- (c) Computing state-value function unknown states. step,state-value function states considered unknown previous stepcomputed. previous step (line 17), state-value function states set0. algorithm proceeds manner similar first-visit MC algorithm Figure 7.case, return unknown state si computed, averaged sinceone episode considered (line 24 25). return si computed, takingaccount first visit state si episode (each occurrence state episodecalled visit si ), although state si could appear multiple times restepisode.- (d) Updating cases B using experience gathered. Updates Bmade cases gathered episodes cumulative reward similarbest episode found point using threshold (line 27). way, good sequencesprovided updates since shown sequences experiencescause adaptive agent converge stable useful policy, whereas bad sequences maycause agent converge unstable bad policy (Wyatt, 1997). also preventsdegradation initial performance B computed first step algorithmuse bad episodes, episodes errors, updates. step, two typesupdates appear, namely, replacements additions new cases. Again, algorithmiterates case ci = (si , ai , V (si )) listCasesEpisode (line 29). si known state(line 30), compute case < si , a, V (si ) > B corresponding state si (line 31).One note case ci = (si , ai , V (si )) listCasesEpisode built line 13algorithm, replacing action corresponding case < si , a, V (si ) > Bnew action ai resulting application random Gaussian noise action529fiGarca & FernandezPolicy Improvement Algorithm00010203040506070809Given case-base B, maximum number casesGiven baseline behaviorGiven update threshold1. Set maxT otalRwEpisode = 0, maximum cumulative reward reached episode2. Repeat(a) Initialization step:set k = 0, listCasesEpisode , totalRwEpisode = 0(b) Case generation:k < maxEpisodeLengthCompute case < s, a, V (s) > B closest current state sk10111213%B (sk ) = 0 // known stateChose action ak using equation 4Perform action akCreate new instance cnew := (s, ak , V (s))141516171819else // unknown stateChose action ak usingPerform action akCreate new instance cnew := (sk , ak , 0)totalRwEpisode := totalRwEpisode + r(sk , ak )listCasesEpisode := listCasesEpisode cnew20212223242526272829Set k = k + 1(c) Computing state-value function unknown states:instance ci listCasesEpisode%B (si ) = 1 // unknown statePreturn(si ) := kj=n jn r(sj , aj ) // n first ocurrence si episodeV (si ) := return(si )(d) Updating cases B using experience gathered :totalRwEpisode > (maxT otalRwEpisode )maxT otalRwEpisode := max (maxT otalRwEpisode, totalRwEpisode)case ci =< si , ai , V (si ) > listCasesEpisode303132%B (si ) = 0 // known stateCompute case < si , a, V (si ) > B corresponding state siCompute = r(si , ai ) + V (si+1 ) V (si )3334353637> 0Replace case < si , a, V (si ) > B case < si , ai , V (si ) > listCasesEpisodeV (si ) = V (si ) +else // unknown stateB := B ci38394041kBk >Remove kBk least-frequently-used cases Bstop criterion becomes true3. Return BFigure 8: Description step two PI-SRL algorithm.Equation 4. Then, temporal distance (TD) error computed (line 32). > 0,performing action ai results positive change value state. action,530fiSafe Exploration State Action Spaces Reinforcement Learningturn, could potentially lead higher return and, thus, better policy. Van HasseltWiering (2007) also update value function using actions potentially leadhigher return. TD error positive, ai considered good selectionreinforced. algorithm, reinforcement carried updating outputcase < si , a, V (si ) > B ai (line 34). Therefore, update case-base occursTD error positive. similar linear reward-inaction update learningautomata (Narendra & Thathachar, 1974, 1989) sign TD error usedmeasure success. PI-SRL updates case-base actual improvementsobserved, thus avoiding slow learning plateaus value spaceTD errors small. shown empirically procedure resultbetter policies step size depends size TD error (Van Hasselt &Wiering, 2007). important note replacements produce smooth changescase-base B since action replaced ai results higher V (si ) ai a.form updating understood risk-seeking approach, overweightingtransitions successor states promise above-average return (Mihatsch & Neuneier,2002). Additionally, prevents degradation B, ensuring replacements madeaction potentially lead higher V (si ).If, instead, si known state, case ci added B (line 37). Finally,algorithm removes cases B necessary (line 39). Complex scoring metrics calculatecases removed given moment proposed several authors.Forbes Andres (2002) suggest removal cases contribute least overallapproximation, Driessens Ramon (2003) pursue error-oriented viewpropose deletion cases contribute prediction error examples.principal drawback sophisticated measures complexity.determination case(s) removed involves computation score valueci B, turn requires least one retrieval regression, respectively,cj B (j 6= i). entire repeated sweeps case-base entail enormouscomputational load. Gabel Riedmiller (2005) compute different score metricci B, requiring computation set k-nearest neighbors around ci .approaches well-suited systems learning adjusted time requirementshigh-dimensional state space, requiring use larger case-basesproposed here. Rather, paper, propose removal least-frequently-usedcases. idea seems intuitive insofar least-frequently-used cases usually containworse estimates corresponding states value; although strategy might leadfunction approximator forgets valuable experience made past(e.g., corner cases). Despite this, PI-SRL performs successfully domains proposedusing strategy, demonstrated Section 4. Thus, ability forget ineffectualknown states described Section 2 result algorithm removing kBk casesleast-frequently-used cases B.3.3 Parameter Setting DesignOne main difficulties applying PI-SRL algorithm given problemdecide appropriate set parameter values threshold , risk parameter ,update threshold maximum number cases . incorrect value531fiGarca & Fernandezparameter lead mislabeling state known really unknown, potentiallyleading damage injury agent. case risk parameter , high valuescontinuously result damage injury; low values safe, allowexploration state-action space sufficient reaching near-optimal policy. Unlike, parameter related risk, instead directly relatedperformance algorithm. Parameter used determine good episodemust respect best episode obtained, since best episodes usedupdate case-base B. value large, bad episodes may used update B(influencing convergence performance algorithm). If, instead, low,number updates B may insufficient improving baseline behavior. Finally,high value allows large case-bases, increasing computational effortretrieval degrading efficiency system. contrast, low value mightexcessively restrict size case-base thus negatively affect final performancealgorithm. subsection, solid perspective given automatic definitionparameters. parameter setting proposed taken suitable setheuristics tested successfully wide variety domains (Section 4).Parameter : parameter domain-dependent related average sizeactions. paper, value parameter establishedcomputing mean distance states execution baselinebehavior . Expressed another way, execution policy providesstate-action sequence form s1 a1 s2 a2 . . . sn . Thus, valuecomputed using Equation 5.=dist(s1 , s2 ) + . . . + dist(sn1 , sn )n1(5)Parameter : Several authors agree impossible completely avoidaccidents (Moldovan & Abbeel, 2012; Geibel & Wysotzki, 2005). importantnote PI-SRL completely safe first step algorithm executed.However, proceeding way, performance algorithm heavilylimited abilities baseline behavior. running subsequentexploratory process inevitable learner performance improved beyondbaseline behavior. Since agent operates state incomplete knowledgedomain dynamic, inevitable exploratory processunknown regions state space visited agent may reach errorstate. However, possible adjust risk parameter determine levelrisk assumed exploratory process. paper, start lowvalues (low risk) gradually increase. Specifically, propose beginning= 9 107 increasing value iteratively either accurate policyobtained amount damage injury high.Parameter : value parameter set relative best episode obtained.paper, value set 5% cumulative reward best episodeobtained.532fiSafe Exploration State Action Spaces Reinforcement LearningFigure 9: Trajectories generated baseline policy deterministic, slightlystochastic highly stochastic domain.Parameter : Previously, estimated maximum number cases storedcase-base estimated maximum number cases required properly mimic baseline behavior . follows description valuecomputed. Figure 9 presents trajectories (sequences states) followed baseline policy three different domains: deterministic, slightly stochastic highlystochastic. domain, different sequences states producedrepresented {s00 , s01 , s02 , . . . , s0n }, {s00 , s11 , s12 , . . . , s1n },. . ., {s00 , sm1 , sm2 , . . . , smn },sji i-th state, s00 initial state sjn final state resultingtrajectory episode j. deterministic domain, different executionsalways result trajectory. case, set maximum numbercases = n cases computed episode stored.slightly stochastic domain, trajectories produced different episodesdifferent, slightly so. Here, suppose case-base beginningempty. Additionally, assume states {s00 , s01 , s02 , . . . , s0n } corresponding first trajectory produced domain stored case-base.Furthermore, domain execute different episodes, obtaining differenttrajectories. Following execution episodes, compute maximum distance i-th state first trajectory (previously added case-base)i-th state produced trajectory j max1jm d(s0i , sji ).slightly stochastic domain, maximum distance exceed thresholdcase max1jm d(s0i , sji ) < . point, assume i-th statetrajectory j least one neighbor distance less (correspondingstate s0i ). Thus, i-th state j added case-base.contrast, highly stochastic domain, maximum distance greatly exceedsthreshold cases max1jm d(s0i , sji ) >> . domain,estimate total number cases added case-base following533fiGarca & Fernandezway. i-th state sequencej first trajectory,k estimate numbermax1jm d(s0i ,sji )cases added case-baseor, words,compute number intervals range [0, max1jm d(s0i , sji )] width(the threshold used decide whether new case added casebase). Consequently, estimated number cases addedjto case-base, takingkPnmax1jm d(s0i ,sji )account states sequence, computed i=0. Finally,estimated maximum number cases computed shown Equation 6.=n+nXmax1jm d(s0i , sji )i=0!(6)important remember deterministic domain, summation equation 6 equal 0 that, therefore, = n. increase value elementrelated increase stochasticity environment, insofar greaterstochasticity environment increases number cases required. Finally,number cases large nearly infinite, threshold increasedmake restrictive addition new cases case-base. However,increase may also adversely affect final performance algorithm.4. Experimental Resultssection presents experimental results collected use PI-SRL policylearning four different domains presented order increasing complexity (i.e., increasing number variables describing states actions): car parking problem (Lee &Lee, 2008), pole-balancing (Sutton & Barto, 1998), helicopter hovering (Ng et al., 2003)business simulator SIMBA (Borrajo et al., 2010). domains,proposed learning near-optimal policy minimizes car accidents,pole disequilibrium, helicopter crashes company bankruptcies, respectively,learning phase. four domains stochastic experimentation.helicopter hovering business simulator SIMBA are, themselves, stochastic and,additionally, generalized domains, made car parking pole-balancing domains stochastic intentional addition random Gaussian noise actionsreward function. results PI-SRL four domains compared yieldedtwo additional techniques, namely, evolutionary RL approach selected winnerhelicopter domain 2009 RL Competition (Martn H. & Lope, 2009) GeibelWysotzkis risk-sensitive RL approach (Geibel & Wysotzki, 2005). evolutionary approach, several neural networks cloning error-free teacher policies added initialpopulation (guaranteeing rapid convergence algorithm near-optimal policy and,indirectly, minimizing agent damage injury). Indeed, winner helicopterdomain agent highest cumulative reward, winner must also indirectlyminimize helicopter crashes insofar incur large catastrophic negative rewards.hand, risk-sensitive approach defines risk probability (s) reachingterminal error state (e.g., helicopter crash ending agent control), starting initial534fiSafe Exploration State Action Spaces Reinforcement Learningstate s. case, new value function weighted sum risk probability,, value function, V , used (Equation 7).V (s) = V (s) (s)(7)parameter 0 determines influence V (s)-values compared (s)values. = 0, V corresponds computation minimum risk policies. largevalues, original value function multiplied dominates weighted criterion.Geibel Wysotzki (2005) consider finite (discretized) action sets study,algorithm adapted continuous action sets. use CBR valuerisk function approximation Gaussian exploration around current action.experiments, domain, three different values used, modifying influenceV -values compared -values. cases, goal improve controlpolicy while, time, minimizing number episodes agent damageinjury. domain, establish different risk levels modifying risk parametervalues according procedure described subsection 3.3. important noteone baseline behavior used initialize evolutionary RL approach exactlyused subsequently first second step PI-SRL. Furthermore, case-baserisk-sensitive approach begin scratch since initialized safe. makes comparison performances fair possible,case-based policy Btaking account different techniques make use baseline behaviors.4.1 Car Parking Problemcar parking problem represented Figure 10 originates RL literature (Cichosz, 1996). car, represented rectangle Figure 10, initially locatedinside bounded area, represented dark solid lines, referred driving area.goal learning agent navigate car initial position garage,car entirely inside, minimum number steps. car cannot moveoutside driving area. Figure 10 (b) shows two possible paths car takestarting point garage obstacle order correctly performtask. consider optimal policy domain reaches goalstate shortest time which, time, free failures.state space domain described three continuous variables, namely,coordinates center car xt yt angle cars axisX coordinate system. car modeled essentially two controlinputs, speed v steering angle , let us suppose car controlledsteering angle (i.e., moves constant speed). Thus, action space described onecontinuous variable [1, 1] corresponding turn radius, used equationsbelow. agent receives positive reward value r = (1 (dist(Pt , Pg ))) 10,Pt = (xt , yt ) center car, Pg = (xg , yg ) center garage (i.e., goalposition) normalizing function scaling Euclidean distance dist(Pt , Pg )Pt Pg range [0, 1] car inside garage (i.e., reward value greatercar parked correctly center garage). agent receives reward-1 whenever hits wall obstacle. steps receive reward -0.1. Thus,difficulty problem lies reinforcement delay, also fact535fiGarca & FernandezFigure 10: Car Parking Problem: (a) Model car parking problem. (b) Examplestrajectories generated agent park car garage.punishments much frequent positive rewards (i.e., much easier hitwall park car correctly). motion car described followingequations (Lee & Lee, 2008)t+1 = + v /(l/2) tan( ),(8)xt+1 = xt + v cos(t+1 ),(9)yt+1 = yt + v sin(t+1 ),(10)v linear velocity car (assumed constant value),maximum steering angle (i.e., car change position maximum angledirections) simulation time step. Gaussian noise addedactions rewards standard deviation 0.1, since noisy interactions inevitablereal-world applications. Adding noise actuators environment,transform deterministic domain stochastic domain. important notenoise added transform domain stochastic domain independentGaussian noise standard deviation (risk parameter) used explore stateaction space second step PI-SRL algorithm. case, Gaussian noisestandard deviation used exploration added noise previously addedactuators. paper, l = 4 (m), v = 1.0 (m/s), = 0.78 (rad) = 0.5 (s)(the driving area obstacle dimensions detailed Figure 10 [a]). initial positioncar fixed xs = 4.0, ys = 4.0 = 0.26 (rad), goal positionxg = 22.5 yg = 13.5. domain, designed baseline behavioraverage cumulative reward per trial 4.75.order perform PI-SRL algorithm, modeling baseline behavior step exe learned demonstrationscuted. result step safe case-based policy Bprovided baseline behavior (see subsection 3.1). computed followingprocedure described subsection 3.3 resulting values 0.01 207, respectively.536fiSafe Exploration State Action Spaces Reinforcement LearningFigure 11: Car Parking Task Modeling Baseline Behavior Step: (a) Number steps pertrial executed Case Base B baseline behavior . (b) Cumulativereward per trial baseline behavior , learned Safe Case Based PolicyIBL approach.BFigure 11 (a) graphically represents execution modeling baseline behavior step.it, two different learning processes presented and, one, number steps pertrial executed baseline behavior (continuous red lines) cases B (dashedgreen lines) shown. beginning learning process empty case-base B,steps performed using baseline behavior . learning process continues,learned. around trialsnew cases added B safe case-based policy B40-50, practically steps performed using cases B rarely used,means safe case-based policy learned. two learning processes shownFigure 11 (a), modeling baseline behavior step performed without collisionswall obstacle. words, baseline behavior cloned safely withouterrors. Figure 11 (b) shows cumulative reward three different execution processes:first (continuous red lines) corresponding performance baseline behavior, second (dashed green lines) corresponding previously-learned safe case-based(derived B ) third (dashed blue lines) corresponding instancepolicy Bbased learning (IBL) approach consisting storing cases memory. IBL approach,new items classified examining cases stored memory determiningsimilar case(s) given particular similarity metric (Euclidean distance used paper). classification nearest neighbor (or nearest neighbors) takenclassification new item using 1-nearest neighbor strategy (Aha & Kibler, 1991).approach, two different executions carried out. IBL approach, trainingprocess performed saving training cases produced baseline behavior50 trials (so consider approach IB1 algorithm sense saves everycase training phase, see Aha & Kibler, 1991). Figure 11 (b) shows safecase-based policy B almost perfectly mimics behavior baseline behavior .domain, performance IB1 approach also similar.Figure 12 (a) shows results different risk configurations obtained improvinglearned baseline behavior step. risk configuration, two different learning pro537fiGarca & FernandezFigure 12: Improving learned baseline behavior step car parking problem: (a) Cumulative reward per episode different risk configurations () obtainedPI-SRL. (b) Cumulative reward per episode evolutionary RL risksensitive RL approaches. cases, episode ending failure marked.cesses performed. trials ending failure (car hits wall obstacle) marked(blue triangles). learning processes Figure 12 (a) demonstrate numberfailures increases increase parameter . low level risk ( = 9 104 ),although failures produced, performance nevertheless weak (around baseline behavior ) constant throughout whole learning process. Additionalexperiments demonstrated increasing value = 9102 increasesnumber failures without improving performance. Figure 12 (b) shows resultsevolutionary risk-sensitive RL approaches different values. Regarding former,number failures higher obtained PI-SRL approach, finalperformance similar. case latter, performance higher = 1.0 (valuemaximization), yet agent consistently crashes car wall.Figure 13 shows mean number failures (i.e., car collisions) cumulative rewardapproach 500 trials red circles corresponding PI-SRL algorithm,black triangles risk-sensitive approach blue square evolutionaryRL approach. Additionally, Figure 13 shows two asymptotes. horizontal asymptoteestablished according cumulative reward obtained highest value.horizontal asymptote indicates higher values increase number failures withoutimproving cumulative reward (which may, fact, get worse). vertical asymptoteF ailures = 0 indicates reducing risk parameter reduce numberfailures. Figure 13 also shows performance two additional risk levels,high level risk ( = 9 101 ) low level risk ( = 0), respectFigure 12. using low level risk = 0, additional random Gaussiannoise added actions algorithm free failures, although performancelearned first stepimprove respect safe case-based policy Balgorithm. PI-SRL medium level risk ( = 9 104 ) also freefailures, yet performance also slightly improved. PI-SRL algorithm high levelrisk ( = 9 102 ) obtains highest cumulative reward, 3053.37, mean538fiSafe Exploration State Action Spaces Reinforcement LearningFigure 13: Mean number failures (car collisions) cumulative reward 500 trialsapproach car parking task. means computed 10different executions.78.8 failures. However, using high level risk ( = 9 101 ), numberfailures greatly increases and, consequently, cumulative reward decreases. shownFigure 12, PI-SRL high risk ( = 9 102 ) evolutionary RL approach obtainsimilar performance, PI-SRL demonstrates faster convergence (thus, Figure 13,cumulative reward obtained PI-SRL higher). Pareto comparison criterionused compare solutions Figure 13. Using principle, one solution strictlydominates (or preferred to) solution parameter strictly worsecorresponding parameter least one parameter strictly better.written y, indicating strictly dominates y. accordance Paretoprinciple, assume points Figure 13 corresponding PI-SRL solutions,save PI-SRL high level risk, Pareto frontier, since pointsstrictly dominated solution (i.e., solution has, time,higher cumulative reward lower number failures PI-SRL). domain,solution PI-SRL medium level risk strictly dominates (or preferred to)risk-sensitive solutions (PI-SRL = 9 103 risk-sensitive) solution PI-SRLhigh level risk strictly dominates solution evolutionary RL solution(PI-SRL = 9 102 evolutionary RL).Nevertheless, important note ultimate decision approachFigure 13 best depends criteria researcher. If, instance, minimization number failures deemed important optimization criterion(independently improvement obtained respect baseline behavior ),best approach PI-SRL low level risk ( = 9 104 ). Similarly,maximization cumulative reward instead judged important optimization criterion (independently number failures generated), best approachPI-SRL high level risk ( = 9 102 ).Figure 14 shows evolution cases case-base B (known space) differenttrials high-risk learning process. graph presents set known states (green539fiGarca & FernandezFigure 14: Car parking problem: Evolution known space different trials = 0(a), = 50 (b), = 100 (c) = 200 (d) high-risk learning process( = 9 102 ). graph corresponds situation state spaceaccordance case-base B trial .area), error states (red area), unknown states (yellow area) non-error states(orange circles). PI-SRL adapts known space order find safer better policiescomplete task. Figure 14 (a) shows initial situation B (corresponding). robust sense never resultspreviously-learned safe case-based policy Bcollisions, suboptimal (it selects longest parking path driving around upperside obstacle). learning process progresses (Figure 14 (b)), PI-SRL findsshorter path park car garage along upper side obstacle (increasingperformance), comes closer obstacle (increasing probabilitycollisions). Figure 14 (c), PI-SRL finds new even shorter path, time alonglower side obstacle. However, still cases case-base B correspondingolder path along upper side obstacle (so Figure 14 (c) indicates two pathspark car). Finally, Figure 14 (d), cases corresponding suboptimal pathalong upper side obstacle removed B replaced new casescorresponding safe improved path along lower side obstacle.words, PI-SRL adapts known space exploration unknown spaceorder find new improved behaviors. process adjusting known space540fiSafe Exploration State Action Spaces Reinforcement Learningsafe better policies, algorithm forgets previously-learned, yet ineffectiveknown states.following experiment, becomes apparent domain noisy enough, eventaking risk (i.e., noise added actuator exploration),agent could nevertheless perform poorly constantly produce collisions. experimentalso serves explain domain noise never sufficient efficient explorationspace without action selection noise. experiment, intentionally addednoise actuators performed second step PI-SRL again, howevertime taking risk (i.e., = 0). test, added random Gaussian noisestandard deviation 0.3, rather standard deviation 0.1 used previously,actuators. Figure 15 shows two executions second step (improving learnedbaseline policy) PI-SRL algorithm x-axis indicating number trials,y-axis cumulative reward per episode failures (i.e., collisions) marked bluetriangles. experiments Figure 12 (b), case-based policy B low levelrisk ( = 9 104 ) never produces failures. contrast, experiments shownFigure 15, case-based policy B continually collides wall althoughrisk parameter set 0 ( = 0). Furthermore, increase performance alsodetected.Figure 15: Improving learned baseline behavior step car parking task: Two learning processes risk configuration = 0 increase noiseactuators.increase noise actuators second step algorithm respectfirst step (the case-based policy B learned first step using Gaussian randomnoise actuator standard deviation 0.1, second step performedusing Gaussian random noise actuator standard deviation 0.3) takesagent beyond known space case-base B learnt first step PI-SRLallows find new trajectories parking car garage. new situation,exploration process guided follows. known state reached, agent performsaction retrieved B without addition Gaussian noise, since risk parameter= 0 (see line 11 Figure 8 algorithm). unknown state reached, agent performs541fiGarca & Fernandezaction advised baseline behavior (see line 15). Using explorationprocess, new better trajectory found parking car garage,resulting cases episode corresponding unknown states added case-base(see line 37), slightly improving performance Figure 15. important notereplacements cases (see line 34) change actions B, sincereplaced action previously retrieved B plus certain amount Gaussiannoise standard deviation (see line 11). Nevertheless, given risk parameterset 0, actions retrieved case-base replaced. explorationprocess, however, = 0 (i.e., taking risk) lead optimal behavior since:actions performed unknown situations added case-base B performed using baseline behavior supposed perform suboptimal actions(see definition baseline behavior).actions cases B replaced improved actions. Gaussiannoise standard deviation used explore different better actionsprovided B ; however, case, risk parameter set = 0new better actions discovered.Additional experiments demonstrate PI-SRL behaves much worse highervalue noise used actuators (with collisions episodes). assume takingrisk (i.e., = 0) implies always performing actions discoveringnewer better actions provided learned case-base B baselinebehavior . PI-SRL, replacements case-base executed towardspromising action which, case, guarantees higher return.exploration necessary order obtain (near-)optimal behavior, since withoutexploration, new better actions discovered PI-SRL performance limitedcase-based policy learned first step B baseline behaviorwhich, one must remember, intended perform suboptimal policies.4.2 Pole-Balancingname suggests, objective pole-balancing problem balance polevertically top moving cart (Sutton & Barto, 1998). state description consistsfour-dimensional vector containing angle , radial speed 0 , cart positionx speed x0 . action consists real-valued force used push cart.study, reward computed encourage actions keep pole uprightpossible cart cart centered possible track. Thus, rewardstep computed rt = 1 ((t ) + (xt ))/2, normalizing functionsscaling angle position xt range [0, 1]. episode composed 10,000steps, although may nevertheless end prematurely pole becomes unbalanced (i.e.,inclination twelve degrees either direction) cart fallstrack (i.e., 2.4m center track),considered failures. car parking problem, Gaussian noise added actionsrewards, time standard deviation 104 . pole-balancing domainbecomes stochastic addition noise actuators reward function.542fiSafe Exploration State Action Spaces Reinforcement LearningFigure 16: Modeling baseline behavior step pole-balancing task: (a) Number steps pertrial executed case-base B baseline behavior . (b) Cumulative rewardIBL approach.per trial , learned safe case-based policy Bhand-made baseline behavior demonstrates execution safe, yet suboptimalpolicy, average cumulative reward per episode/trial 9292.learntmodeling baseline behavior step PI-SRL, safe case-based policy Bdemonstrations provided baseline behavior . computed followingprocedure described subsection 3.3, values 0.02 12572, respectively.Figure 16 (a) shows two different learning processes modeling baseline behavior step.learning process, Figure 16 (a) shows number steps per trial executedbaseline behavior (continuous red lines) case-base B (dashed green lines).beginning learning process, case-base B empty steps performedusing baseline behavior . learning process progresses, however, B filledlearnt. end learning process (after aroundsafe case-based policy B45-50 trials), almost steps performed using cases B rarely used.important note modeling baseline behavior step performed withoutfailures (i.e., pole disequilibrium cart track) case. previoustask, Figure 16 (b) represents three independent execution processes using previously (derived B indicated dashed green lines),learned safe case-based policy Bbaseline behavior (indicated continuous red lines) approach basedIBL (indicated dashed blue lines) (Aha & Kibler, 1991). average cumulative9230 (Figure 16 [b]). almost perfectly clones ,reward per episode BBIB1 approach which, cases, results pole disequilibrium cart fallingtrack averages cumulative reward per episode 8055.Figure 17 (a) shows results PI-SRL different risk configurations.configuration, learning curves shown two different learning processes performed.Additionally, episode ending failure marked (blue triangles). increaserisk increases probability failure, policy obtained nevertheless better termscumulative reward. Nevertheless, much greater risk values ( = 9 105 ) producefailures without accompanying increase cumulative reward. Figure 17 (b) showsresults evolutionary risk-sensitive RL approaches, former543fiGarca & FernandezFigure 17: Improving learned baseline behavior step pole-balancing task: (a) Cumulative reward per episode different risk configurations () obtained PISRL. (b) Cumulative reward per episode obtained evolutionary risksensitive RL approaches. cases, episode ending failure marked.clearly algorithm greatest number failures. risk-sensitive approach,= 2.0 (value maximization), agent selects actions result higher value,also higher risk. contrary, = 0 (risk minimization), agentlearns risk function (at around episode 6000), selects actions lower risk (andlower number failures), also considerably weak performance. value = 0.1produces intermediate policy. Consequently, concluded PI-SRL highlevel risk obtains better policies less failures evolutionary risk-sensitiveRL approaches. Figure 18 reinforces previous conclusions.Figure 18: Mean number failures (pole disequilibrium cart track) cumulative reward 500 trials approach pole-balancing task.means computed 10 different executions.544fiSafe Exploration State Action Spaces Reinforcement Learningit, mean number failures cumulative reward 12,000 trials shown,red circles corresponding PI-SRL, black triangles corresponding risksensitive approach blue square corresponding evolutionary RL approach.figure also shows performance two additional risk levels, high level risk( = 9 104 ) low level risk ( = 0), respect Figure 17. cumulativereward number failures increase high level risk ( = 9 105 ).risk level represents inflection point higher levels risk produce failureswithout accompanying improvement cumulative reward. fact, high levelrisk ( = 9 104 ) results reduction cumulative reward comparedhigh level risk ( = 9 105 ). Again, Pareto comparison criterion may usedcompare solutions Figure 18. domain, solution PI-SRLlow level risk strictly dominates risk-sensitive solutions = 0.0 = 0.1,PI-SRL = 9 107 risk-sensitive = 0.0 = 0.1. Additionally,solution PI-SRL high level risk strictly dominates evolutionary RL solution,PI-SRL = 9 105 evolutionary RL.Lastly, Figure 19 shows evolution known space derived case-baseB different trials high-risk learning process. graph, error states (redarea), set unknown states (yellow area), set known states (green area)set non-error states (orange circles) represented. known spacegraph computed taking cases B trials = 0, 3000, 6000 8000.graph, non-error states computed 10 different executions Btrial (the orange circles representing terminal states executions).first graph (Figure 19 [a]) presents initial known space resulting modelingbaseline behavior step. evolution Figure 19 demonstrates two different points. First,PI-SRL progressively adapts known space order encounter better behaviorknown space tends compressed toward center coordinates.due fact reward greater angle pole cartposition x 0 (i.e., pole upright possible cart cart centeredtrack). Second, risk failure pole-balancing domain greaterearly trials learning process. beginning learning process (Figure 19 [a]),= 0), regions known space close error space. situation,slight modifications actions consistently produce visits states (i.e., poledisequilibrium cart falling track). learning process advances (Figure 19[b], [c] [d]), known space compressed toward origin coordinates awayerror space. Consequently, probability visiting error states decreases.example, returning Figure 17 (a), high-risk learning processes, 52% failures(126) occur first 4000 trials, remaining 48% (117) occur last 8000trials.4.3 Helicopter Hoveringsuggested name, objective domain make helicopter hover closepossible defined position duration established episode. task challenging two main reasons. Firstly, state action spaces high-dimensionalcontinuous (more specifically, state space 12-dimensional action space545fiGarca & FernandezFigure 19: Pole-balancing task: Evolution known space different trials = 0 (a),= 3000 (b), = 6000 (c) = 8000 (d) high-risk learning process( = 9 105 ). graph corresponds situation state spaceaccording case-base B trial .4-dimensional). Secondly, generalized domain whose behavior modified windfactor. helicopter episode composed 6000 steps, although may end prematurelyhelicopter crashes. first step PI-SRL performed order imitate baselinebehavior . computed following procedure described subection 3.3values 0.3 49735, respectively. step performed, resultingable properly imitate baseline behavior .safe case-based policy BFigure 20 (a) shows two learning processes modeling baseline behavior step.Similar previous tasks, learning processes progress, number steps executedbaseline behavior reduced number steps using case-base Bincreases. end learning process, case-base B stores safe case-based. Figure 20 (b) compares performance (in terms cumulative reward perpolicy BIB1 approach. Regardingepisode) , learned case-based policy Bfirst two, average cumulative reward per episode -78035.93, obtained-85130.11. Although perfectly mimic baseline behavior ,BB546fiSafe Exploration State Action Spaces Reinforcement LearningFigure 20: Modeling baseline behavior step helicopter hovering task: (a) Numbersteps per trial executed case-base B baseline behavior . (b) Cumula IBLtive reward per trial , learned safe case-based policy Bapproach.nevertheless performs safe policy without crashing helicopter. regardtraining process IB1 approach, every case produced 15 episodes baselinebehavior stored. Figure 20 (b) demonstrates IB1 approach consistently resultshelicopter crashes, performance extremely far learned safe case . Improvement policy begins state-action space safelybased policy BBexplored execution step two PI-SRL.Figure 21 (a) shows results different risk levels. PI-SRL low mediumlevels risk levels produce helicopter crashes PI-SRL, performance neverthelessquite weak.Figure 21: Improving learned baseline behavior step helicopter hovering task: (a)Cumulative reward per episode different risk configurations obtained PISRL. (b) Cumulative reward per episode obtained evolutionary risksensitive RL approaches. cases, episode ending failure marked.547fiGarca & FernandezConversely, high level risk established produces near-optimal policylow number collisions. Extensive experimentation demonstrates increasing riskparameter = 9 103 also increases number crashes without accompanyingimprovement cumulative reward. Figure 21 (b) shows results evolutionaryRL approach which, remembered, selected winner RL Competition2009 domain (Martn H. & Lope, 2009), well risk-sensitive RL algorithmdifferent values. comparison results evolutionary RL approachPI-SRL shows similar cumulative reward, also significantly higher numbercrashes former latter. evolutionary approach, crashesoccur early steps learning process; PI-SRL, accidents occuradvanced steps learning process. case risk-sensitive RL algorithm,= 0 = 0.01 risk function learned around episode 3000. point,agent selects lower-risk actions number crashes considerably reduced.= 0.4 agent selects actions resulting higher values without taking riskaccount, performance improves, expense increased number accidents.Nevertheless whatever value, number crashes higher performanceworse PI-SRL.Figure 22: Mean number failures (helicopter crashes) cumulative reward 5000episodes approach helicopter hovering task. meanscomputed 10 different executions.information Figure 22, indicating mean number failures cumulativereward 5000 episodes approach, complements conclusions made above.data computed 10 independent executions approach.previous domains, PI-SRL indicated red circles, risk-sensitive approachblack triangles evolutionary RL approach blue square. Figure 22 also showsperformance two additional risk levels, high level risk ( = 9 102 )low level risk ( = 0), respect Figure 21. Figure 22 demonstratesevolutionary RL approach obtains highest cumulative reward (7.13 107 ),followed closely PI-SRL (7.57 107 ). approaches far results.Regarding number failures (i.e., helicopter crashes), PI-SRL low levelrisk ( = 0), low level risk ( = 9 105 ) medium level risk ( = 9 104 )548fiSafe Exploration State Action Spaces Reinforcement Learningproduces collisions, PI-SRL algorithm medium risk preferable inasmuchcumulative reward higher (18.01 107 ). Using Pareto comparison criterion,PI-SRL solution high level risk strictly dominates solutions risksensitive approach (PI-SRL = 9 103 risk-sensitive). Moreover, PI-SRL strictlydominated solution.Figure 23: Evolution known space different episodes helicopter hoveringtask. (a) Example representation single known state radar chart. (b),(c), (d) Known states episodes = 0, = 500 = 4000, respectively,high-risk learning process ( = 9 103 ). graph correspondssituation known space according case-base B episode .pole-balancing domain, Figure 23 shows evolution known spaceaccording case-base B different episodes high-risk learning process.case, radar charts used due high number features describing states.radar chart graphical method displaying multivariate data two-dimensionally.Figure, axis represents one features state and, preserve simplicityrepresentation, charts generated normalizing absolute values features0 1. Figure 23 (a) example representation single known state.549fiGarca & Fernandezvalue axis corresponds value individual feature stateline drawn connecting feature values axis. line Figure 23 (a)represents single state, Figures 23 (b), (c) (d) show known space accordingcase-base B episodes 0, 500 4000, respectively. three charts representsingle state, rather states B corresponding episode. Thus,graph, set known states marked (green area). state considered error statesingle feature value state greater 1. limits (marked red linegraphs) computed taking account helicopter crashes (i)velocity along main axes exceeds 5 m/s, (ii) position helicopter20 m, (iii) angular rate around main axes exceeds 2 2 rad/s(iv) orientation 30 degrees target orientation. previoustasks, Figure 23 indicates two different matters. First, learning proceeds, knownspace derived B adjusted space used better safer policies.helicopter domain, agent tries hover helicopter close possible targetposition (i.e., origin coordinates), since immediate rewards greater closerhelicopter hovers origin. Thus, known space starts expand (Figure 23[b]) and, progressively, concentrated origin coordinates (Figure 23 [c] [d]).regard second matter, probability crashing low since,beginning, known space already appears concentrated origin farerror space (Figure 23 [b]). words, beginning, featuresknown space (i.e., forward, sideways downward velocities; x, y, z coordinates; x,z angular-rates; x, z quaternation) far error space limits,decreasing probability visiting error state.previous experiments, second step PI-SRL performed usinginitial case-base B free failures built first step algorithm.following experiments show performance second step PI-SRL differentinitial policies used. Figure 24 (a) shows performance policies used initialpolicies. continuous black line indicates performance initial safe case-basedpolicy B , average cumulative reward per episode -85,130.11, used previousexperiments prior execution step two algorithm. remaining linesFigure correspond performance three different initializations case-base Bused new experiments, prior execution step two algorithm. Usingpoor initial policy (dashed green lines) helicopter crashed nearlyepisodes, average cumulative reward per episode calculated -108,548.03.Using different poor (albeit less poor) initial policy (continuous red lines)helicopter crashed occasionally, average cumulative reward per episode -91,723.89.Finally, near-optimal policy (dashed blue lines) whereby helicopter hovering freefailures yields average cumulative reward per episode -13,940.1.Figure 24 (b) shows performance second step (improving baseline behaviorstep) PI-SRL, starting case-base B corresponding poor, poornear-optimal policies presented Figure 24 (a). Figure 24 (b), dashed blue linescorrespond use case-base B containing near-optimal policy, continuousred lines correspond use case-base B containing poor policy dashedgreen lines correspond use case-base B containing poor policy.experiments Figure conducted using high level risk domain550fiSafe Exploration State Action Spaces Reinforcement LearningFigure 24: (a) performance different initial policies helicopter hovering task.(b) performance different executions second step PI-SRL,starting case-base B containing policy three different types:poor, poor near-optimal.( = 9 103 ). graph indicates use near-optimal policy initialpolicy high level risk level, case-base worsen performance which,fact, appears improve slightly. second step PI-SRL prevents degradationinitial performance B, since updates cases case-base made using badepisodes. words, updates B made cases gathered episodescumulative reward similar best episode found particular pointusing threshold (whose value set 5% cumulative reward best episode).example, cumulative reward best episode -13,940.1, episodescumulative reward higher -14,637 used update case-base (discardingbad episodes episodes failures). way, good sequences experiencesprovided updates, since proven good sequences experiencescause adaptive agent converge stable useful policy, bad sequencesmay cause agent converge unstable poor policy (Wyatt, 1997). solid redlines Figure 24 (b) show using poor policy failures initial policy produceshigher number failures using initial policy free failures. Howeverdespite poor initialization, PI-SRL nevertheless able learn near-optimal policywell policy free failures used initialize B (see lines corresponding highlevel risk, = 9 103 , Figure 21 (a)). Finally, dashed green lines Figure 24(b) show use poor initial policy many failures results decreasedperformance higher number failures produced, even though nevertheless ablelearn better behavior. case, algorithm falls local minimum, probablybiased poor initialization. cases poor policies, numberfailures higher beginning learning process decreases learningprocess proceeds. poor poor initial policies closeerror space, stark contrast initial policy shown Figure 23 which,beginning, already appears concentrated origin, far error space.551fiGarca & Fernandezlearning process proceeds, different policies compressed away errorspace number failures decreases.4.4 SIMBABusiness simulators powerful tools improving management decision-making processes. example tool SIMulator Business Administration (SIMBA)(Borrajo et al., 2010). SIMBA competitive simulator, since agents competeagents management different virtual companies. simulator,result twenty years experience university students businessexecutives, emulates business realities using variables, relationships eventspresent business world. objective provide users integrated visioncompany, using basic techniques business management, simplifying complexityemphasizing content principles greatest educational value (Borrajo et al.,2010). experiments performed here, learning agent competes five handcoded agents (Borrajo et al., 2010). Decision-making SIMBA episodic taskdecisions made sequentially. make business decision, state must studied10 continuous decision variables (e.g., selling price, advertising expenses, etc.) mustset, followed study state composed 12 continuous variables (e.g., materialcosts, financial expenses, economic productivity, etc.) (Borrajo et al., 2010). episodecomposed 52 steps, although may prematurely company goes bankrupt (i.e.,losses higher 10% net assets).Figure 25: Modeling baseline behavior step SIMBA Task: (a) Number steps per trialexecuted case-base B baseline behavior . (b) Cumulative reward perIBL approach.trial , learned safe case-based policy BFigure 25 (a) shows evolution number steps executed baseline behavior case-base B two learning processes performing modeling baselinebehavior step. computed following procedure described subsection 3.3values 1 102 513, respectively. episodes (approximately 25),learned. Figure 25 (b) shows performance previouslysafe case-based policy Blearned B , IB1 approach. study, mean profits per episode552fiSafe Exploration State Action Spaces Reinforcement LearningFigure 26: Improving learned baseline behavior step SIMBA task: (a) meanprofits per episode different risk configurations obtained PI-SRL agentfive hand-coded agents. (b) mean profits per episode obtainedevolutionary risk-sensitive RL agent five hand-coded agents.cases, episode ending failure (bankruptcy) noted.4.02 million Euros. IB15.24 million Euros, obtained Bapproach, cases generated using baseline behavior 25 episodes stored.experiments demonstrate SIMBA, contrast previous domains, storing cases sufficient obtaining safe policy performance similar usingmodeling baseline behavior step (with mean profits per episode 3.98 million Euros).learned, execute improving learned baselinesafe case-based policy Bbehavior step.Similar findings earlier tasks, Figure 26 (a) indicates low mediumlevels risk produce bankruptcies, performance nevertheless weak. highestlevel risk produces near-optimal policy low number number failures.contrast, Figure 26 (b) presents results evolutionary risk-sensitive RL approaches, former clearly yields highest number failures.risk-sensitive case, number bankruptcies cases insufficient learning risk function . comparative results Figure 26 show PI-SRL= 9 101 obtains better policies less failures evolutionary risk-sensitiveRL approaches.Figure 27 shows graphical representation different solutions domain.shows mean number failures cumulative reward different approaches100 episodes, data computed 10 independent executions approach.Figure, red circles correspond PI-SRL algorithm, black triangles correspondrisk-sensitive approach blue square corresponds evolutionary RL approach.Figure 27 also shows performance two additional risk levels, high ( = 9 102 )low ( = 0), respect Figure 26. experiments Figure 27 demonstratePI-SRL high level risk ( = 9 101 ) obtains highest cumulative reward,6693.58. Additionally, PI-SRL low level risk ( = 0), low level risk( = 9 101 ) medium level risk ( = 9 100 ) approaches lowest553fiGarca & FernandezFigure 27: Mean number failures (company bankruptcies) cumulative reward100 episodes approach SIMBA task. meanscomputed 10 different executions.mean number failures, 0.0. However, PI-SRL medium level risk preferredinasmuch performance superior terms cumulative reward. PI-SRLhigh level risk ( = 9 102 ) increases number failures obtains lower cumulativereward compared PI-SRL high level risk. Using Pareto comparisoncriterion, PI-SRL high level risk strictly dominates solutions (PI-SRL= 9101 risk-sensitive PI-SRL = 9101 evolutionary RL), approachstrictly dominated solution.Due difficulty representing high-dimensional state action spaceSIMBA domain, graphs provided evolution known space.5. Related WorkReinforcement learning (RL) case-based reasoning (CBR) techniques combined literature different ways. work Bianchi et al. (2009), new approachpresented permitting use cases heuristics speed RL algorithms. Additionally, Sharma et al. (2007) use combination CBR RL (called CARL) achievetransfer playing Game AI across variety scenarios MadRTS TM,commercial Real Time Strategy game. CBR also used state value functionapproximation RL (Gabel & Riedmiller, 2005). However, present study is,knowledge, first time CBR RL used conjunction safe exploration dangerous domains. field safe reinforcement learning, three principaltrends observed: (i) approaches based return variance, (ii) risk-sensitiveapproaches based definition error states (iii) approaches using teachers.5.1 Approaches Based Return Varianceliterature, long known optimal policy optimal expectedreturn MDP quite sensitive parameter variations (even optimal policy may554fiSafe Exploration State Action Spaces Reinforcement Learningperform badly cases due stochastic nature problem). mitigateproblem, agent try maximize return associated worst-case scenario,even though case may highlyunlikely. Thus, trend, risk refers worstPr variance. example approachoutcomes return R =t=0worst-case control worst possible outcome R optimized (Coraluppi& Marcus, 1999; Heger, 1994). worst case control strategies, optimality criterionexclusively focused risk-avoiding policies. policy considered optimalworst-case return superior. approach, however, restrictive inasmuch takesrare scenarios fully account.value return introduced Heger (1994) seen extensionworst case control MDPs. concept establishes returns R <policy occur probability lower neglected. algorithm lesspessimistic pure worst case control, given extremely rare scenarios effectpolicy. work Heger et al., idea weighting return risk, namelyexpected value-variance criterion, also introduced.risk-sensitive control based use exponential utility functions, return Rtransformed reflect subjective measure utility. Instead maximizing expectedvalue R, objective maximize U = 1 logE(eR ), parameterR usual return. shown that, depending parameter , policieshigh variance V (R) penalized ( < 0) enforced ( > 0). Instead, NeuneierMihatsch (2002) consider worst-case-outcomes policy, (i.e., risk relatedvariability return). study, authors demonstrate learning algorithminterpolates risk-neutral worst-case criterion limitingbehavior exponential utility functions. noted approaches basedvariability return worst possible outcomes suited problemspolicy small variance produce large risk (Geibel & Wysotzki, 2005).view risk present study, however, concerned variancereturn worst possible outcome, instead fact processes generallypossess unsafe states avoided. Consequently, address different classproblems dealt approaches focusing variability return.5.2 Risk-sensitive Approaches based Error States.second trend approaches, concept risk based definition errorstates fatal transitions. Thus, Geibel et al. (2005) , instance, establish riskfunction probability entering error state. Instead, Hans et al (2008) considertransition fatal corresponding reward less given threshold .first case demonstrated Section 4, learned TD methods requireerror states (i.e., car collisions, pole-balancing disequilibrium, helicopter crashescompany bankruptcies) visited repeatedly order approximate risk functionand, subsequently, avoid dangerous situations. second case, concept riskjoined reward. Moreover, mentioned studies either (i) assumesystem dynamics known, (ii) tolerate undesirable states explorationor, contrast paper, (iii) deal problems high-dimensionalcontinuous state-action spaces. Regarding latter, Geibel et al. write555fiGarca & Fernandezapproach also extended continuous action sets (e.g., using actor-criticmethod), give details may done entirely continuousproblems. Section 4, present approach solves problem.5.3 Approaches Using Teacherslast trend approaches based use teachers three different ways:(i) bootstrap learning algorithm (i.e., initialization procedure), (ii) derivepolicy finite demonstration set (iii) guide exploration process.5.3.1 Bootstrapping Learning Algorithmwork Driessens Szeroski (2004), bootstraping procedure used relational RL finite set demonstrations recorded human expertlater presented regression algorithm. allows regression algorithm buildpartial Q-function later used guide exploration state spaceusing Boltzmann exploration strategy. Smart Kaelbling (2000) also use examples,training runs bootstrap Q-learning approach HEDGER algorithm.initial knowledge bootstrapped Q-learning approach allows agent learneffectively helps reduce time spent random actions. Teacher behaviorsalso used form population seeding neuroevolution approaches (Yao, 1999; Siebel &Sommer, 2007). Evolutionary methods used optimize weights neural networks,starting prototype network whose weights correspond teacher (or baselinepolicy). Using technique, RL Competition helicopter hovering task winners Martin etal. (2009) developed evolutionary RL algorithm several teachers providedinitial population. algorithm restricts crossover mutation operators, allowingslight changes policies given teachers. Consequently, rapid convergence algorithm near-optimal policy ensured, indirect minimizationdamage agent. However, teachers included initial population resultingad-hoc training regimen conducted competition. Consequently, proposedapproach seems somewhat ad-hoc easily generalizable arbitrary RL problems.work Koppejan et al. (2009, 2011), neural networks also evolved, beginningone whose weights corresponds teacher behavior. approachproven advantageous numerous applications evolutionary methods (Hernandez-Dazet al., 2008; Koppejan & Whiteson, 2009), Koppejans algorithm nevertheless also seemssomewhat ad-hoc designed specialized set environments.5.3.2 Deriving Policy Finite Set Demonstrationsapproaches falling category framed according field LearningDemonstration (LfD) (Argall et al., 2009). Highlighting study Abbeel et al. (2010)based apprenticeship learning, approach composed three distinct steps.first, teacher demonstrates task learned state-action trajectoriesteachers demonstration recorded. second step, state-action trajectories seenpoint used learn dynamics model system. model, (near)optimal policy found using reinforcement learning (RL) algorithm. Finally,policy obtained tested running real system. work Tang et556fiSafe Exploration State Action Spaces Reinforcement Learningal. (2010), algorithm based apprenticeship learning also presented automaticallygenerating trajectories difficult control tasks. proposal based learningparameterized versions desired maneuvers multiple expert demonstrations. Despiteapproachs potential strengths general interest, inherently linkedinformation provided demonstration dataset. result, learner performanceheavily limited quality teachers demonstrations.5.3.3 Guiding Exploration ProcessDriessens Szeroski (2004), context relational RL, also use given teacherspolicy, rather policy derived current Q-function hypothesis (whichinformative early learning stages), selection actions. approach,episodes performed teacher interleaved normal exploration episodes.mixture teacher normal exploration make easier regression algorithmdistinguish beneficial poor actions. context LfD,approaches include teacher advice (Argall et al., 2009). advice used improvelearner performance, offering information beyond provided demonstrationdataset. approach, following initial task demonstration teacher, agentdirectly requests additional demonstration teacher different statespreviously demonstrated states single action cannot selectedcertainty (Chernova & Veloso, 2007, 2008).works mentioned trend, explicit definition risk ever given.6. Conclusionswork, PI-SRL, algorithm policy improvement safe reinforcementlearning high-risk tasks, described. main contributions algorithmdefinitions novel case-based risk function baseline behavior safe explorationstate-action space. use case-based risk function presented possibleinasmuch policy stored case-base. represents clear advantageapproaches, e.g., evolutionary RL (Martn H. & Lope, 2009; Koppejan & Whiteson,2011) extraction knowledge known space agent impossibleusing weights neural-networks. Additionally, completely different notionrisk others found literature presented. According notion, riskindependent variance return reward function, requireidentification error states learning risk functions. Rather, conceptrisk described paper based distance known unknownspace and, therefore, domain-independent parameter (in sense, proposal allowsapplication parameter-setting method described subsection 3.3).Koppejan et al. (2011) also use function identify dangerous states, contrastapproach, definition function requires strong previous knowledge domain.Furthermore, approaches risk found literature tackle problemsentirely continuous (Geibel & Wysotzki, 2005) report resultsone continuous domain (Koppejan & Whiteson, 2011). Consequently, difficult knowcertain approaches literature generalize easily arbitrary domains.557fiGarca & Fernandezpaper presents PI-SRL algorithm great detail demonstrates effectiveness four entirely different continuous domains: car parking problem, pole-balancing,helicopter hovering business management (SIMBA). experiments presentedpaper demonstrate different characteristics learning capabilities PI-SRLalgorithm.(i) PI-SRL obtains higher quality solutions. experiments Section 4 demonstratethat, save helicopter hovering task, PI-SRL obtains cases best cumulativereward per episode least number failures. Additionally, using Pareto comparison criterion said that, save high risk configuration car parkingproblem, approach strictly dominated approach.(ii) PI-SRL adjusts initial known space safe better policies. initial knownspace resulting first step PI-SRL, modeling baseline behavior, adjustedimproved second step algorithm, improving learned baseline behavior.Additionally, experiments demonstrate adjustment process compressknown space away error space (e.g., pole-balancing domain, subsection 4.2,helicopter hovering domain, subsection 4.3) or, occasions, require knownspace move closer error space (e.g., car parking problem, subsection 4.1)event better policies found there.(iii) PI-SRL works well domains differently structured state-action spacesvalue function vary sharply. Although car parking problem, polebalancing domain, helicopter hovering task business simulator representdifferently structured problems, experiments study nevertheless demonstratePI-SRL performs well each. Furthermore, even domains car parkingproblem value function varies sharply due presence obstacle,experimental results demonstrate PI-SRL nevertheless successfully handledifficulty. However, impossible avoid failures known space edgeedge error states algorithm would often explore error states.(iv) number failures depends distance known spaceerror space. experiments pole-balancing helicopter hovering domains demonstrate number failures depends close known space errorspace. Due structure domains, improving learned baseline behaviorstep algorithm tends concentrate known space origin coordinatesaway error space. greater distance known space errorspace, lower number failures. Additionally, helicopter hovering, knownspace is, beginning, far error space (consequently, number failures also low beginning). Therefore, initial distribution known spacelearned baseline policy later influences number failures obtainedsecond step PI-SRL.(v) PI-SRL completely safe first step algorithm executed. However,proceeding way, algorithm performance would heavily limitedcapabilities baseline behavior. learner performance improved beyondperformance baseline behavior, subsequent exploratory processsecond step PI-SRL must carried out. Since complete knowledge domaindynamic possessed, however, also inevitable that, exploratory558fiSafe Exploration State Action Spaces Reinforcement Learningprocess, unknown regions state space visited agent may reach errorstates.(vi) risk parameter allows user configure level risk assumed.algorithm, user gradually increase value risk parameter orderobtain better policies, also assuming greater likelihood damage learningsystem.(vii) PI-SRL performs successfully even poor initial policy failures used.experiments Figure 24 helicopter hovering domain demonstrate PI-SRLable learn near-optimal policy despite poor initialization, policyfree failures used initialize case-base B. However, Figure also showspoor initial policy many failures used, PI-SRL decreases performanceproduces higher number failures, although better behavior still learnt.case, algorithm falls local minimum, likely biased poor initialization.follows, applicability method discussed, allowing readerclearly understand scenarios proposed PI-SRL approach may applicable.applicability restricted domains following characteristics.(i) mandatory scenario satisfy two assumptions described Section 2.According first assumption, nearby states domain must necessarily similar actions. According other, similar actions similar states produce similareffects. fact similar actions lead similar states assumes degree smoothness dynamic behavior system which, certain environments, may hold.However, clearly explain Section 2, consider assumptions logicalassumptions derived generalization principles RL literature (Kaelbling et al.,1996; Jiang, 2004).(ii) applicability method limited size case-base B requiredmimic baseline behavior. possible apply proposed approach tasks when,first step PI-SRL algorithm, modeling baseline behavior, prohibitively largenumber cases required properly mimic complex baseline behaviors. case,threshold increased restrict addition new cases casebase. However, increase may adversely affect final performance algorithm.Nevertheless, experiments performed Section 4 demonstrate relatively simplebaseline behaviors mimicked almost perfectly using manageable number cases.(iii) PI-SRL algorithm requires presence baseline behavior. proposedmethod requires presence baseline behavior safely demonstrates tasklearned. baseline behavior conducted human teacher hand-codedagent. important note, nevertheless, presence baseline behaviorguaranteed domains.Finally, logical continuation present study would take account automaticgraduation risk parameter along learning process. example, wouldparticularly interesting exploit fact known space far away errorspace order increase risk parameter or, contrary, reduceclose. future work aims deploy algorithm real environments, inasmuchuncertainty real environments presents biggest challenge autonomousrobots. Autonomous robotic controllers must deal large number factorsrobotic mechanical system electrical characteristics, well environmental559fiGarca & Fernandezcomplexity. However, use PI-SRL algorithm (or risk-sensitive approaches)learning processes real environments could reduce amount damage incurredand, consequently, allow lifespan robots extended. might worthwhileadd mechanism algorithm detect known state lead directlyerror state. problems currently investigated.Acknowledgmentsstudy partially supported Spanish MICIIN projects TIN2008-06701-C0303, TRA2009-0080 CCG10-UC3M/TIC-5597. offer gratitude special thanksRaquel Fuentetaja Pizan, Assistant Professor Universidad Carlos III de MadridPlanning & Learning Group (PLG), generous invaluable commentsrevision paper. would also like thank Jose Antonio Martn, AssistantProfessor Universidad Complutense de Madrid, invaluable comments regardingevolutionary RL algorithm.ReferencesAamodt, A., & Plaza, E. (1994). Case-Based Reasoning; Foundational Issues, Methodological Variations, System Approaches. AI Communications, 7 (1), 3959.Abbeel, P., Coates, A., Hunter, T., & Ng, A. Y. (2008). Autonomous AutorotationRC Helicopter. ISER, pp. 385394.Abbeel, P., Coates, A., & Ng, A. Y. (2010). Autonomous helicopter aerobaticsapprenticeship learning. I. J. Robotic Res., 29 (13), 16081639.Abbott, R. G. (2008). Robocup 2007: Robot soccer world cup xi.. chap. Behavioral CloningSimulator Validation, pp. 329336. Springer-Verlag, Berlin, Heidelberg.Aha, D. W. (1992). Tolerating Noisy, Irrelevant Novel Attributes Instance-BasedLearning Algorithms. International Journal Man-Machine Studies, 36 (2), 267287.Aha, D. W., & Kibler, D. (1991). Instance-based learning algorithms. Machine Learning,pp. 3766.Anderson, C. W., Draper, B. A., & Peterson, D. A. (2000). Behavioral cloning studentpilots modular neural networks. Proceedings Seventeenth InternationalConference Machine Learning, pp. 2532. Morgan Kaufmann.Argall, B., Chernova, S., Veloso, M., & Browning, B. (2009). Survey Robot LearningDemonstration. Robotics Autonomous Systems, 57 (5), 469483.Bartsch-Sprl, B., Lenz, M., & Hbner, A. (1999). Case-based reasoning: Survey futuredirections.. Puppe, F. (Ed.), XPS, Vol. 1570 Lecture Notes Computer Science,pp. 6789. Springer.Bianchi, R., Ros, R., & de Mantaras, R. L. (2009). Improving reinforcement learningusing case-based heuristics.. Vol. 5650, pp. 7589. Lecture Notes Artificial Intelligence, Springer, Lecture Notes Artificial Intelligence, Springer.560fiSafe Exploration State Action Spaces Reinforcement LearningBorrajo, F., Bueno, Y., de Pablo, I., Santos, B. n., Fernandez, F., Garca, J., & Sagredo, I.(2010). SIMBA: Simulator Business Education Research. Decission SupportSystems, 48 (3), 498506.Boyan, J., Moore, A., & Sutton, R. (1995). Proceedings workshop value functionapproximation, machine learning conference 1995... Technical Report CMU-CS-95206.Chernova, S., & Veloso, M. (2007). Confidence-based policy learning demonstrationusing gaussian mixture models. Joint Conference Autonomous AgentsMulti-Agent Systems.Chernova, S., & Veloso, M. (2008). Multi-thresholded approach demonstration selectioninteractive robot learning. Proceedings 3rd ACM/IEEE internationalconference Human robot interaction, HRI 08, pp. 225232, New York, NY, USA.ACM.Cichosz, P. (1995). Truncating temporal differences: efficient implementationtd(lambda) reinforcement learning. Journal Artificial Intelligence Research(JAIR), 2, 287318.Cichosz, P. (1996). Truncated temporal differences function approximation: Successful examples using cmac. Proceedings Thirteenth European SymposiumCybernetics Systems Research (EMCSR-96).Coraluppi, S. P., & Marcus, S. I. (1999). Risk-Sensitive Minimax Control DiscreteTime, Finite-State Markov Decision Processes. AUTOMATICA, 35, 301309.Defourny, B., Ernst, D., & Wehenkel, L. (2008). Risk-aware decision making dynamicprogramming. NIPS 2008 Workshop Model Uncertainty Risk RL.Driessens, K., & Ramon, J. (2003). Relational instance based regression relational rl.International Conference Machine Learning (ICML), pp. 123130.Driessens, K., & Dzeroski, S. (2004). Integrating guidance relational reinforcementlearning. Machine Learning, 57 (3), 271304.Fernandez, F., & Isasi, P. (2008). Local feature weighting nearest prototype classification.Neural Networks, IEEE Transactions on, 19 (1), 4053.Fernandez, F., & Borrajo, D. (2008). Two steps reinforcement learning. InternationalJournal Intelligent Systems, 23 (2), 213245.Floyd, M. W., & Esfandiari, B. (2010). Toward domain-independent case-based reasoningapproach imitation: Three case studies gaming. Workshop Case-BasedReasoning Computer Games 18th International Conference Case-BasedReasoning (ICCBR), pp. 5564.Floyd, M. W., Esfandiari, B., & Lam, K. (2008). Case-Based Reasoning ApproachImitating Robocup Players. Proceedings 21st International Florida ArtificialIntelligence Research Society Conference, pp. 251256.Forbes, J., & Andre, D. (2002). Representations learning control policies.University New South, pp. 714.561fiGarca & FernandezGabel, T., & Riedmiller, M. (2005). Cbr state value function approximation reinforcement learning. Proceedings 6th International Conference Case-BasedReasoning (ICCBR 2005, pp. 206221. Springer.Geibel, P. (2001). Reinforcement Learning Bounded Risk. Proceedings 18thInternational Conference Machine Learning, pp. 162169. Morgan Kaufmann.Geibel, P., & Wysotzki, F. (2005). Risk-sensitive Reinforcement Learning Applied ControlConstraints. Journal Artificial Intelligence Research (JAIR), 24, 81108.Hans, A., Schneegass, D., Schafer, A. M., & Udluft, S. (2008). Safe Exploration Reinforcement Learning. European Symposium Artificial Neural Network, pp.143148.Heger, M. (1994). Consideration Risk Reinforcement Learning. 11th InternationalConference Machine Learning, pp. 105111.Hernandez-Daz, A. G., Coello, C. A. C., Perez, F., Caballero, R., Luque, J. M., & SantanaQuintero, L. V. (2008). Seeding initial population multi-objective evolutionary algorithm using gradient-based information. IEEE Congress EvolutionaryComputation, pp. 16171624. IEEE.Hester, T., Quinlan, M., & Stone, P. (2011). real-time model-based reinforcement learningarchitecture robot control. Tech. rep. arXiv e-Prints 1105.1749, arXiv.Hu, H., Kostiadis, K., Hunter, M., & Kalyviotis, N. (2001). Essex wizards 2001 teamdescription. Birk, A., Coradeschi, S., & Tadokoro, S. (Eds.), RoboCup, Vol. 2377Lecture Notes Computer Science, pp. 511514. Springer.Jiang, A. X. (2004). Multiagent reinforcement learning stochastic games continuousaction spaces..Kaelbling, L., Littman, M., & Moore, A. (1996). Reinforcement learning: survey. JournalArtificial Intelligence Research (JAIR), 4, 237285.Konen, W., & Bartz-Beielstein, T. (2009). Reinforcement learning games: failuressuccesses. Proceedings 11th Annual Conference Companion GeneticEvolutionary Computation Conference: Late Breaking Papers, GECCO 09, pp. 26412648, New York, NY, USA. ACM.Koppejan, R., & Whiteson, S. (2009). Neuroevolutionary reinforcement learning generalized helicopter control. GECCO 2009: Proceedings Genetic EvolutionaryComputation Conference, pp. 145152.Koppejan, R., & Whiteson, S. (2011). Neuroevolutionary reinforcement learning generalized control simulated helicopters. Evolutionary Intelligence, 4, 219241.Lee, J.-Y., & Lee, J.-J. (2008). Multiple Designs Fuzzy Controllers Car Parking UsingEvolutionary Algorithm, pp. 16. No. May.Luenberger, D. G. (1998). Investment science. Oxford University Press.Mannor, S. (2004). Reinforcement learning average reward zero-sum games. ShaweTaylor, J., & Singer, Y. (Eds.), COLT, Vol. 3120 Lecture Notes Computer Science,pp. 4963. Springer.562fiSafe Exploration State Action Spaces Reinforcement LearningMartin H, J., & de Lope, J. (2009). Exa: effective algorithm continuous actionsreinforcement learning problems. Industrial Electronics, 2009. IECON 09. 35thAnnual Conference IEEE, pp. 2063 2068.Martn H., J. A., & Lope, J. (2009). Learning Autonomous Helicopter Flight Evolutionary Reinforcement Learning. 12th International Conference ComputerAided Systems Theory (EUROCAST), pp. 7582.Mihatsch, O., & Neuneier, R. (2002). Risk-Sensitive reinforcement learning. Machine Learning, 49 (2-3), 267290.Moldovan, T. M., & Abbeel, P. (2012). Safe exploration markov decision processes.CoRR, abs/1205.4810.Narendra, K. S., & Thathachar, M. A. L. (1974). Learning automata - survey. IeeeTransactions Systems Man Cybernetics, SMC-4 (4), 323334.Narendra, K. S., & Thathachar, M. A. L. (1989). Learning automata: introduction.Prentice-Hall, Inc., Upper Saddle River, NJ, USA.Ng, A. Y., Kim, H. J., Jordan, M. I., & Sastry, S. (2003). Autonomous Helicopter Flightvia Reinforcement Learning. Thrun, S., Saul, L. K., & Scholkopf, B. (Eds.), NIPS.MIT Press.Peters, J., Tedrake, R., Roy, N., & Morimoto, J. (2010). Robot learning. Sammut, C.,& Webb, G. I. (Eds.), Encyclopedia Machine Learning, pp. 865869. Springer.Poli, R., & Cagnoni, S. (1997). Genetic programming user-driven selection: Experiments evolution algorithms image enhancement. Genetic Programming1997: Proceedings Second Annual Conference, pp. 269277. Morgan Kaufmann.Salkham, A., Cunningham, R., Garg, A., & Cahill, V. (2008). collaborative reinforcement learning approach urban traffic control optimization. Web IntelligenceIntelligent Agent Technology, 2008. WI-IAT 08. IEEE/WIC/ACM InternationalConference on, Vol. 2, pp. 560566.Santamara, J. C., Sutton, R. S., & Ram, A. (1998). Experiments reinforcementlearning problems continuous state action spaces. Adaptive Behavior, 6,163218.Sharma, M., Holmes, M., Santamaria, J., Irani, A., Isbell, C., & Ram, A. (2007). Transferlearning real-time strategy games using hybrid cbr/rl. ProceedingsTwentieth International Joint Conference Artificial Intelligence.Siebel, N. T., & Sommer, G. (2007). Evolutionary reinforcement learning artificial neuralnetworks. International Journal Hybrid Intelligent Systems, 4, 171183.Smart, W. D., & Kaelbling, L. P. (2000). Practical reinforcement learning continuousspaces. Artificial Intelligence, pp. 903910. Morgan Kaufmann.Smart, W. D., & Kaelbling, L. P. (2002). Effective reinforcement learning mobile robots.ICRA, pp. 34043410. IEEE.Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MITPress.563fiGarca & FernandezTang, J., Singh, A., Goehausen, N., & Abbeel, P. (2010). Parameterized maneuver learning autonomous helicopter flight. International Conference RoboticsAutomation (ICRA).Taylor, M. E., Kulis, B., & Sha, F. (2011). Metric learning reinforcement learning agents.Proceedings International Conference Autonomous Agents MultiagentSystems (AAMAS).Van Hasselt, H., & Wiering, M. A. (2007). Reinforcement Learning Continuous ActionSpaces. Approximate Dynamic Programming Reinforcement Learning, 2007.ADPRL 2007. IEEE International Symposium on, pp. 272279.Wyatt, J. (1997). Exploration Inference Learning Reinforcement. UniversityEdinburgh.Yao, X. (1999). Evolving artificial neural networks. PIEEE: Proceedings IEEE, 87,14231447.564fiJournal Artificial Intelligence Research 45 (2012) 1-45Submitted 11/11; published 9/12Interactions Knowledge TimeFirst-Order Logic Multi-Agent Systems:Completeness ResultsF. BelardinelliA. Lomusciof.belardinelli@imperial.ac.uka.lomuscio@imperial.ac.ukDepartment ComputingImperial College London, UKAbstractinvestigate class first-order temporal-epistemic logics reasoning multiagent systems. encode typical properties systems including perfect recall, synchronicity, learning, unique initial state terms variants quantified interpreted systems, first-order extension interpreted systems. identify several monodicfragments first-order temporal-epistemic logic show completeness respectcorresponding classes quantified interpreted systems.1. Introductionreactive systems (Pnueli, 1977) traditionally specified using plain temporal logic,well-established tradition Artificial Intelligence (AI) and, particular, MultiAgent Systems (MAS) research adopt expressive languages. Much traditioninspired earlier, seminal work AI McCarthy (1979, 1990) others aimedadopting intentional stance (Dennett, 1987) reasoning intelligent systems.Specifically, logics knowledge (Fagin, Halpern, Moses, & Vardi, 1995), beliefs, desires,intentions, obligations, etc., put forward represent informationalmotivational attitudes agents system. Theoretical explorations focusedsoundness completeness number axiomatisations well decidabilitycomputational complexity corresponding logics.great majority work lines focuses propositional languages. Yet, specifications supporting quantification increasingly required applications. example,often necessary refer different individuals different instances time.Quantified modal languages (Garson, 2001) long attracted considerable attention.Early work included analysing philosophical logical implications different setups quantification domains, particularly combination temporal concepts.recently, considerable attention given identifying suitable fragmentspreserve completeness decidability, studying resulting computational complexity satisfiability problem. article follows direction.detail, investigate meta-theoretical properties monodic fragmentsquantified temporal-epistemic logic interactions quantifiers, time,knowledge agents present. deep-rooted interest (Fagin et al., 1995;Meyden, 1994) understanding implications interaction axioms context,often express interesting properties MAS, including perfect recall, synchronicity,c2012AI Access Foundation. rights reserved.fiBelardinelli & Lomusciolearning. features well-understood propositional level (Fagin,Halpern, & Vardi, 1992; Halpern, van der Meyden, & Vardi, 2004) commonly usedseveral application areas. technical question paper aims resolve whethersimilar range results provided presence (limited forms of) quantification.shall demonstrate, answer question largely positive.1.1 State Artanalysis application temporal-epistemic logic first-order settingestablished tradition AI. One early contributions work Moore (1990),presents theory action takes consideration epistemic preconditionsactions effects knowledge. recently, number first-order temporalepistemic logics reasoning MAS introduced Wooldridge et al. (2002,2006, 1999), often context MABLE programming language agents.authors introduced first-order branching time temporal logic MAS (Wooldridge& Fisher, 1992), developed series papers (Wooldridge et al., 2002, 2006).First-order multi-modal logics also constitute conceptual base numberagent theories, BDI logics (Rao & Georgeff, 1991), KQML framework (Cohen& Levesque, 1995), LORA framework (Wooldridge, 2000b). includeoperators mental attitudes (e.g., knowledge, belief, intention, desire, etc.), welltemporal dynamic operators form quantification. However,current literature far fallen short systematic analysis formal propertiesframeworks. frameworks rich unlikelyfinitely axiomatisable, let alone decidable. Still, earlier contributions inspirationpresent investigation, among explicitly addressedsubject first-order temporal-epistemic languages MAS setting.purely theoretical level, first-order temporal epistemic logics also received increasing attention range contributions axiomatisability (Degtyarevet al., 2003; Sturm et al., 2000; Wolter & Zakharyaschev, 2002), decidability (Degtyarevet al., 2002; Hodkinson et al., 2000; Wolter & Zakharyaschev, 2001), complexity (Hodkinson, 2006; Hodkinson et al., 2003). Wolter Zakharyaschev (2001) introducedmonodic fragment quantified modal logic, modal operators restrictedformulas one free variable, proved decidability various fragments. Similar results obtained monodic fragments first-order temporallogic (Hodkinson, 2002; Hodkinson et al., 2000), computational complexityformalisms analysed (Hodkinson, 2006; Hodkinson et al., 2003). Further, WolterZakharyaschev (2002) provided complete axiomatisation monodic first-ordervalidities natural numbers. monodic fragment first-order epistemic logicalso explored (Sturm et al., 2000; Sturm, Wolter, & Zakharyaschev, 2002),axiomatisation including common knowledge provided. lines researchconstitute theoretical background research set.contributions discussed previously used plain Kripke models underlying semantics. However, argued though applications computationallygrounded semantics (Wooldridge, 2000a) preferable, enables systems modelled directly. introduced quantified interpreted systems (QIS) fill gap (Belar2fiInteractions Knowledge Time First-Order Logic MASdinelli & Lomuscio, 2009). enabled us provide complete axiomatisationmonodic fragment quantified temporal-epistemic logic linear time (Belardinelli & Lomuscio, 2011). However, interaction temporal epistemic modalitiesstudied. Preliminary investigations interactions temporal epistemicoperators first-order setting already appeared (Belardinelli & Lomuscio, 2010).paper extend previous results also consider epistemic languages containingcommon knowledge operator.1.2 Present Contributionpaper extends current state art first-order temporal-epistemic logicintroducing family provably complete calculi variety quantified interpreted systems characterising range properties including perfect recall, learning, synchronicity,unique initial state. prove completeness presented first-ordertemporal-epistemic logics via quasimodel construction, previously used(Hodkinson, Wolter, & Zakharyaschev, 2002; Hodkinson et al., 2000) prove decidabilitymonodic fragments first-order temporal logic (FoTL). Quasimodels alsoapplied first-order temporal well epistemic logic (Sturm et al., 2000; Wolter & Zakharyaschev, 2002). Wolter et al. (2002) present complete axiomatisation monodicfragment FoTL natural numbers; similar result variety first-order epistemic logics common knowledge also appeared (Sturm et al., 2000). However,interaction temporal epistemic modalities first order settingtaken account yet, interpreted systems semantics. Nonetheless,features essential applications multi-agent systems subjectanalysis here.1.2.1 Structure Paper.Section 2 first introduce first-order temporal-epistemic languages Lm LCmcommon knowledge set Ag = {1, . . . , m} agents. present relevantclasses QIS well monodic fragments Lm LCm . Sections 3 introduceaxiomatisations classes QIS, details completeness proofspresented Sections 4 5. Finally, Section 6 elaborate results obtaineddiscuss possible extensions future work.2. First-Order Temporal-Epistemic LogicsInterpreted systems standard semantics interpreting temporal-epistemic logicsmulti-agent setting (Fagin et al., 1995; Parikh & Ramanujam, 1985). extend interpretedsystems first-order case enriching structures domain individuals.first investigated static quantified interpreted systems, accountevolution system given (Belardinelli & Lomuscio, 2008, 2009). Then, fully-fledgedQIS language also temporal modalities introduced (Belardinelli & Lomuscio,2010, 2011). follow definition QIS provided references.3fiBelardinelli & Lomuscio2.1 First-Order Temporal-Epistemic LanguagesGiven set Ag = {1, . . . , m} agents, first-order temporal-epistemic language Lmcontains individual variables x1 , x2 , . . ., individual constants c1 , c2 , . . ., n-ary predicate constants P1n , P2n , . . ., n N, propositional connectives , quantifier ,linear time operators U, epistemic operator Ki agent Ag.language LCm also contains common knowledge operator C (Fagin et al., 1992).simplicity consider one group agents common knowledge modality,is, whole Ag; C really tantamount CAg . extension proper non-emptysubsets Ag problematic.languages Lm LCm contain symbol functions; terms t1 , t2 , . . .languages either individual variables constants.Definition 1. Formulas Lm defined Backus-Naur form follows:::= P k (t1 , . . . , tk ) | | 0 | x | | U 0 | Kilanguage LCm extends Lm following clause:formula LCm , also C formula LCm .formulas U0 read next step eventually 0respectively. formula Ki represents agent knows , C standscommon knowledge set Ag agents.define symbols , , , , G (always future), F (some time future)standard. Further, introduce abbreviations. Voperator Ki dual Ki ,is, Ki defined Ki , E shorthand iAg Ki . k N, E kdefined follows: E 0 = E k+1 = EE k . formulas Ki E readagent considers possible every agent knows respectively.Free bound variables defined standard. [~y ] mean ~y = y1 , . . . , ynfree variables . Additionally, [~y /~t] formula obtained substituting simultaneously some, possibly all, free occurrences ~y ~t = t1 , . . . , tnrenaming bound variables. sentence formula free variables.2.2 Quantified Interpreted Systemsintroduce quantified interpreted systems assume set Li local states li , li0 , . . .agent Ag multi-agent system. consider set Le local statesenvironment e well. set Le L1 . . .Lm contains global statesmulti-agent system. represent temporal evolution MAS considerflow time N natural numbers; run function r : N S. Intuitively, runrepresents one possible evolution MAS assuming N flow time. Givenabove, define quantified interpreted systems languages Lm LCm follows:Definition 2 (QIS). quantified interpreted system triple P = hR, D, Ii where:R non-empty set runs;non-empty set individuals;4fiInteractions Knowledge Time First-Order Logic MASr R, n N, first-order interpretation, is, functionevery constant c, I(c, r(n)) D,every predicate constant P k , I(P k , r(n)) k-ary relation D.Further, every r, r0 R, n, n0 N, I(c, r(n)) = I(c, r0 (n0 )).Notice assume unique domain interpretation, well fixed interpretationindividual constants; simply write I(c). Following standard notation (Fagin et al.,1995), r R n N, pair (r, n) point P. r(n) = hle , l1 , . . . , lmglobal state point (r, n) (n) = le ri (n) = li environments agentlocal state (r, n) respectively. Further, Ag epistemic equivalence relationdefined (r, n) (r0 , n0 ) iff ri (n) = ri0 (n0 ). Clearly, equivalencerelation. Two points (r, n) (r0 , n0 ) said epistemicallyreachable, simplyreachable, (r, n) (r0 , n0 ) transitive closure iAg .paper consider following classes QIS.Definition 3. quantified interpreted system P satisfiessynchronicityiffevery Ag, points (r, n), (r0 , n0 ),(r, n) (r0 , n0 ) implies n = n0perfect recall agentiffpoints (r, n), (r0 , n0 ), (r, n) (r0 , n0 ) n > 0either (r, n 1) (r0 , n0 )k < n0 (r, n 1) (r0 , k)k 0 , k < k 0 n0 implies (r, n) (r0 , k 0 )learning agentiffpoints (r, n), (r0 , n0 ), (r, n) (r0 , n0 )either (r, n + 1) (r0 , n0 )k > n0 (r, n + 1) (r0 , k)k 0 , k > k 0 n0 implies (r, n) (r0 , k 0 )unique initial stateiffr, r0 R, r(0) = r0 (0)conditions extensively discussed literature (Halpern et al., 2004)together equivalent formulations. Intuitively, QIS synchronous time partlocal state agent. QIS satisfies perfect recall agent local state recordseverything happened (from agents point view) far run.learning dual perfect recall: agent acquire new knowledge run.Finally, QIS unique initial state runs start global state.QIS P satisfies perfect recall (resp. learning) P satisfies perfect recall (resp.learning) agents. denote class QIS agents QIS ; superscriptspr, nl, sync, uis denote subclasses QIS satisfying perfect recall, learning,synchronicity, unique initial state respectively. instance, QIS sync,uisclass synchronous QIS agents unique initial state.assign interpretation formulas Lm LCm means quantifiedinterpreted systems. Let assignment variables individuals D, valuation5fiBelardinelli & Lomuscio(t) term defined (y) = y, (t) = I(c) = c. variant axassignment assigns x agrees variables.Definition 4. satisfaction formula Lm point (r, n) P assignment, denoted (P , r, n) |= , defined inductively follows:(P , r, n) |= P k (t1 , . . . , tk )(P , r, n) |=(P , r, n) |= 0(P , r, n) |= x(P , r, n) |=(P , r, n) |= U 0iffiffiffiffiffiff(P , r, n) |= KiiffhI (t1 ), . . . , (tk )i I(P k , r(n))(P , r, n) 6|=(P , r, n) 6|= (P , r, n) |= 0xD, (P , r, n) |=(P , r, n + 1) |=n0 n (P , r, n0 ) |= 0(P , r, n00 ) |= n n00 < n0r0 , n0 , (r, n) (r0 , n0 ) implies (P , r0 , n0 ) |=LCm consider also case common knowledge operator:(P , r, n) |= Ciffk N, (P , r, n) |= E ktruth conditions , , , , G F defined above.definition follows (P , r, n) |= C iff (r0 , n0 ) reachable (r, n),(P , r0 , n0 ) |= .formula true point (r, n) satisfied (r, n) every assignment ;true QIS P true every point P; valid class C QIS trueevery QIS C. Further, formula satisfiable QIS P satisfiedpoint P, assignment ; satisfiable class C QIS satisfiableQIS C.considering combinations pr, nl, sync uis obtain 16 subclasses QISN. independent, axiomatisable. Indeed,axiomatisable even propositional level (Halpern & Moses, 1992; Halpern &Vardi, 1989). first column Table 1 group together classes QIS shareset validities languages Lm LCm . proofs equivalencessimilar propositional case (Halpern et al., 2004) reportedhere. Further, define languages PLm PLCm propositional fragmentsLm LCm respectively (formally, PLm PLCm obtained restricting atomicformulas 0-ary predicate constants p1 , p2 , . . .). Table 1 summarises results Halpernet al. (2004) concerning axiomatisability propositional validities PLm PLCm .Observe that, regards language PLm , = 1 sets validities variousclasses QIS axiomatisable, 2 axiomatisation givenQIS nl,uisQIS nl,pr,uis(Halpern & Vardi, 1986, 1989). language PLCm ,restrict case 2, = 1 PLCm expressive powerPLm . 2 class validities PLCm recursive axiomatisation QIS ,uissync,uisQIS nl,sync,uis, QIS nl,pr,sync,uis.QIS sync, QIS , QISnext section show axiomatisability results propositional levellifted monodic fragment languages Lm LCm .6fiInteractions Knowledge Time First-Order Logic MASQISsync,uisQIS , QIS sync,QIS uis, QISpr,uisQIS pr, QISpr,syncQIS, QIS pr,sync,uisnlQISQIS nl,syncQIS nl,prQIS nl,pr,syncQIS nl,uisQIS nl,pr,uisQIS nl,sync,uis, QIS nl,pr,sync,uisPL1XXXXXXXXXXPLm , 2XXXXXXX77XPLCm , 2X77777777XTable 1: Equivalences classes QIS axiomatisability results propositional fragments PLm PLCm . sign X indicates set validitiesspecific class axiomatisable; 7 indicates not.2.3 Monodic Fragmentrest paper show sufficient condition lifting resultsTable 1 first-order case restrict languages Lm LCm monodicfragments.Definition 5. monodic fragment L1m set formulas Lm subformula form Ki , , 1 U2 contains one free variable. Similarly,1 set formulas LCmonodic fragment LCmsubformulaform Ki , C, , 1 U2 contains one free variable.monodic fragments number first-order modal logics thoroughlyinvestigated literature (Hodkinson et al., 2000, 2003; Wolter & Zakharyaschev, 2001,2002). case Lm LCm fragments quite expressive containformulas like following:C(zAvailable(y, z)UxRequest(x, y))(1)Ki xyz(Request(x, y) Available(y, z))Ki xyz(Request(x, y) Available(y, z))(2)Formula (1) intuitively states common knowledge every resourceeventually requested somebody, time resource remains availableeverybody. Notice free variable within scope modal operators UC. Formula (2) represents agent knows next step every resourceavailable whenever requested, next step agent knows indeedcase. However, note formulaxKi (Process(x) yF Access(x, y))7fiBelardinelli & Lomusciointuitively means agent knows every process eventually try accessevery resource, L1m x occur free within scope modal operatorF . Still, monodic fragments Lm LCm quite expressive containde dicto formulas, i.e., formulas free variable appears scope modaloperator, (2). So, limitation really de formulas.stress fact formulas propositional equivalentcase interepreted quantified interpreted systems domainquantification infinite, cardinality cannot bounded advance.Finally, observe Barcan formulas x x Ki x xKitrue quantified interpreted systems, QIS includes unique domainquantification. implies universal quantifier commutes temporalmodality epistemic modality Ki . Thus, case formulas, 0 Lm , 0 validity, L1m 0/ L1m . instance,consider = xP (x, y) 0 = x P (x, y). see remarkinterfere results.3. Axiomatisationssection present sound complete axiomatisations sets monodic validities classes quantified interpreted systems Section 2. First, introducebasic system QKTm extends first-order case multi-modal epistemic logicS5m combined linear temporal logic LTL.Definition 6. system QKTm contains following schemes axioms rules,, formulas L1m = inference relation.First-order logicTemporal logicEpistemic logicTautMPExGenKT1T2NecT3K45Necclassical propositional tautologies, =x [x/t][x/t] = x, x free( ) ( )U ( (U))== (U)Ki ( ) (Ki Ki )KiKi Ki KiKi Ki Ki= Kioperator Ki S5 modality, next U operators axiomatised linear-time modalities (Fagin et al., 1995). add classical postulatesEx Gen quantification, sound consider unique domainindividuals quantified interpreted systems.8fiInteractions Knowledge Time First-Order Logic MASDefinition 7. system QKT Cm extends QKTm following schemes axioms1 = inferencecommon knowledge, , formulas LCmrelation.C1C2C ( EC)( E) = Cconsider standard definitions proof theorem; `S means formulatheorem formal system S. remark Barcan formula (BF ) 2xx2 provable unary modal operator 2 axioms K Ex, rulesP Gen. notions soundness completeness system respectclass C QIS defined standard: sound w.r.t. C , ` implies C |= .Similarly, complete w.r.t. C , C |= implies ` .paper focus schemes axioms Table 2 specify key interactionstime knowledge (Halpern et al., 2004). use 1, . . . , 5 superscripts denoteKT1KT2KT3KT4KT5Ki (Ki Ki ) Ki ((Ki )U((Ki )U))Ki Ki(Ki )UKi Ki ((Ki )UKi )Ki KiKi KjTable 2: axioms KT1-KT5.systems obtained adding QKTm QKTCm combination KT1-KT5.instance, system QKTC2,3extends QKTCm axioms KT2 KT3.straightforward check axioms QKTm QKTCm valid everyQIS inference rules preserve validity. However, axioms KT1-KT5 validspecific classes QIS stated following Remark.Remark 1. QIS P satisfies axioms KT1-KT5 first column P satisfiescorresponding semantical condition second column.AxiomKT1KT2KT3KT4KT5Condition QISperfect recallperfect recall, synchronicitylearninglearning, synchronicityagents share knowledge, i.e.,i, j Ag, (r, n) (r0 , n0 ) iff (r, n) j (r0 , n0 ).results shown similar way propositional case (Halpern et al.,2004); proofs omitted.using Remark 1 prove soundness results first-order temporalepistemic systems.Theorem 1 (Soundness). systems reported first second column following table sound w.r.t. corresponding classes QIS third column.9fiBelardinelli & LomuscioSystemsQKTmQKT Cm11QKTmQKT Cm22QKTmQKT Cm33QKTmQKT Cm44QKTmQKT Cm2,32,3QKTmQKT Cm1,41,4QKTmQKT Cm1,4,51,4,5QKTmQKT CmQISsync,uisQIS , QIS sync,QIS uis, QISpr,uisQIS pr, QISpr,syncQIS, QIS pr,sync,uisnlQIS , QIS nl,uisQIS nl,syncnl,pr,uisQIS nl,pr, QISnl,pr,syncQISQIS nl,sync,uis, QIS nl,pr,sync,uisProof. results follow Remark 1 line reasoning similar usedpropositional case (Fagin et al., 1995; Halpern et al., 2004). Notice quantifiedinterpreted systems P satisfies learning, synchronicity, unique initial state,P satisfies also perfect recall, is, P QIS nl,sync,uisimplies P QIS nl,pr,sync,uis.Further, agents share knowledge, therefore KT5 holds P.anticipated above, calculi complete w.r.t. corresponding classesquantified interpreted systems Theorem 1. next theorem summarisecompleteness results proved rest paper.Theorem 2 (Completeness). systems reported first second columnfollowing table complete w.r.t. corresponding classes QIS third column.SystemsQKTmQKT Cm1QKTm2QKTm3QKTm4QKTm2,3QKTm1,4QKTmQKT12,31,4,51,4,5QKTmQKT CmQISsync,uisuisQIS , QIS sync, QIS , QISprpr,uisQIS , QISQIS pr,sync, QIS pr,sync,uisnlQISQIS nl,syncQIS nl,prQIS nl,pr,syncQIS nl,uis, QIS nl,pr,uis11nl,pr,sync,uisQIS nl,sync,uis,QISobserve that, regards language L1m , sets monodic validities axiomatisable classes introduced QIS nl,uisQIS nl,pr,uis. However, L11QIS nl,uisQIS nl,pr,uisequivalent QIS nl,pr. Thus, sets monodic111nl,pr,uisnl,uisvalidities QIS 1QIS 1axiomatised QKT2,31 .1regards language LCm , set monodic validities QIS , QIS sync,sync,uisnl,sync,uisnl,pr,sync,uisQIS uis,QISaxiomatisable,wellQISQIS.classes recursively axiomatisable, case already propositional level (Halpern & Moses, 1992; Halpern & Vardi, 1986, 1989).proving completeness results reported introduce Kripke modelsgeneralisation quantified interpreted systems.10fiInteractions Knowledge Time First-Order Logic MAS3.1 Kripke Modelsprove completeness results Theorem 2, first introduce appropriate classKripke models generalisation QIS prove completeness models.apply correspondence result Kripke models QIS obtain desiredresults.Definition 8 (Kripke model). Kripke model tuple = hW, RW , {i }iAg , D, IiW non-empty set states;RW non-empty set functions r : N W ;every agent Ag, equivalence relation W ;non-empty set individuals;every w W , first-order interpretation, is, functionevery constant c, I(c, w) D,every predicate constant P k , I(P k , w) k-ary relation D.Further, every w, w0 W , I(c, w) = I(c, w0 ).Notice Def. 8 differs notions Kripke model includesset RW functions guarantee correspondence Kripke modelsQIS one-to-one. also assume unique domain interpretation, well fixedinterpretation individual constants, also case simply write I(c). Kripkemodels generalisation QIS specify inner structure statesW . Also Kripke models introduce points pairs (r, n) r RW n N.point derives properties corresponding state; instance, (r, n) (r0 , n0 )r(n) r0 (n0 ).consider Kripke models satisfying synchronicity, perfect recall, learning,unique initial state. definition subclasses analogous Def. 3.Definition 9. Kripke model satisfiessynchronicityiffevery Ag, points (r, n), (r0 , n0 ),(r, n) (r0 , n0 ) implies n = n0perfect recall agentiffpoints (r, n), (r0 , n0 ), (r, n) (r0 , n0 ) n > 0either (r, n 1) (r0 , n0 )k < n0 (r, n 1) (r0 , k)k 0 , k < k 0 n0 implies (r, n) (r0 , k 0 ).learning agentiffpoints (r, n), (r0 , n0 ), (r, n) (r0 , n0 )either (r, n + 1) (r0 , n0 )k > n0 (r, n + 1) (r0 , k)k 0 , k > k 0 n0 implies (r, n) (r0 , k 0 ).unique initial stateiffr, r0 RW , r(0) = r0 (0).11fiBelardinelli & Lomusciolet Km class Kripke models agents. Hereafter adoptsync,uisnaming conventions QIS; instance, Kmclass synchronous Kripkemodels agents unique initial state. Further, inductive clausessatisfaction relation |= respect assignment straightforwardly definedQIS, well notions truth validity.Definition 10. satisfaction formula Lm (resp. LCm ) point (r, n)assignment , (M , r, n) |= , inductively defined follows:(M , r, n) |= P k (t1 , . . . , tk )(M , r, n) |=(M , r, n) |= 0(M , r, n) |= x(M , r, n) |=(M , r, n) |= U 0iffiffiffiffiffiff(M , r, n) |= Ki(M , r, n) |= CiffiffhI (t1 ), . . . , (tk )i I(P k , r(n))(M , r, n) 6|=(M , r, n) 6|= (M , r, n) |= 0xD, (Ma , r, n) |=(M , r, n + 1) |=n0 n (M , r, n0 ) |= 0n n00 < n0 implies (M , r, n00 ) |=r0 , n0 , (r, n) (r0 , n0 ) implies (M , r0 , n0 ) |=k N, (M , r, n) |= E kformula true point (r, n) satisfied (r, n) every assignment ;true Kripke model true every point M; valid class CKripke models true every Kripke model C. Further, formula satisfiableKripke model satisfied point M, assignment ;satisfiable class C Kripke models satisfiable Kripke model C.relate Kripke models quantified interpreted systems means map g :Km QIS (Lomuscio & Ryan, 1998). Let = hW, RW , {i }iAg , D, Ii Kripkemodel. every agent Ag, (r, n) M, let equivalence class [(r, n)]i = {(r0 , n0 ) |(r, n) (r0 , n0 )} local state agent i; (r, n) local stateenvironment. define g(M) tuple hR0 , D, 0 R0 contains runs rrr RW rr (n) = h(r, n), [(r, n)]1 , . . . , [(r, n)]m i. Further, M,every constant c, 0 (c, rr (n)) = I(c, r(n)), 0 (P k , rr (n)) = I(P k , r(n)).structure g(M) QIS satisfies following result:Lemma 1. every Lm (resp. LCm ),(M , r, n) |= iff (g(M) , rr , n) |=Proof. proof induction structure . atomic formula P k (t1 , . . . , tk ), (M , r, n) |= iff hI (t1 ), . . . , (tk )i I(P k , r(n)), iffhI 0 (t1 ), . . . , 0 (tk )i 0 (P k , rr (n)), iff (g(M) , rr , n) |= . inductive casespropositional connectives quantifiers straightforward, well temporal operators U. = Ki , (M , r, n) |= iff (r, n) (r0 , n0 )0implies (M , r0 , n0 ) |= , (r, n) (r0 , n0 ) iff rri (n) = rri (n0 ). Thus, (M , r, n) |= iff0(rr , n) 0i (rr , n0 ) implies (M , r0 , n0 ) |= . Again, induction hypothesis (M , r, n) |=00iff (rr , n) 0i (rr , n0 ) implies (g(M) , rr , n0 ) |= , i.e., iff (g(M) , rr , n) |= . case= C treated similarly considering epistemic reachability relation.12fiInteractions Knowledge Time First-Order Logic MASNotice satisfies synchronicity, perfect recall, learning, uniqueinitial state, also g(M) satisfies property. follows fact0(r, n) (r0 , n0 ) iff (rri , n) 0i (rr , n0 ). Thus, g defines map 16 subclassesKm outlined Def. 9 corresponding subclass QIS obtain followingcorollary Lemma 1.Corollary 1. Let X subset {pr, nl, sync, uis}. every monodic formula L1m1 ), satisfiable KX , satisfiable QIS X .(resp. LCmreasoning monodic fragments Lm LCm dealinglearning perfect recall, introduce following class monodic friendly Kripkemodels. structures motivated fact KT1 KT3 weakenforce either perfect recall learning Kripke models axiomsrestricted monodic formulas. However, suffice monodic friendly structures.following, also prove satisfiability Kripke models equivalent satisfabilitymonodic friendly structures restrict languages monodic formulas.Definition 11 (mf-model). monodic friendlyMmf = hW, RW , {i,a }iAg,aD , D, IiKripkemodeltupleW , RW , defined Kripke models;Ag, D, i,a equivalence relation W .define synchronicity, perfect recall, learning, unique initial statealso mf-models parametrising Def. 9 relation i,a . instance, mf-modelsatisfies perfect recall agent points (r, n), (r0 , n0 ), D, whenever(r, n) i,a (r0 , n0 ) n > 0 either (r, n 1) i,a (r0 , n0 ) k < n0(r, n 1) i,a (r0 , k) k 0 , k < k 0 n0 implies (r, n) i,a (r0 , k 0 ). regardssubclasses class MF mf-models agents, adopt namingconventions QIS Kripke models. Notice Kripke models seenmf-models Ag, a, b D, i,a equal i,b .1 ) mf-modelFinally, satisfaction relation |= L1m (resp. LCmmfdefined way Kripke models, except epistemic operators:(Mmf , r, n) |= Ki [y]iffr0 , n0 , (r, n) i,(y) (r0 , n0 ) implies (Mmf , r0 , n0 ) |= [y]appears free . Notice sentence, (Mmf , r, n) |= Kiiff (r, n) i,a (r0 , n0 ) implies (Mmf , r0 , n0 ) |= D. case commonknowledge operator C straightforward definition E k . particular, two points (r, n)0 0(r0 , n0 ) epistemically reachableD, simply reachable, (r, n) (r , n ),transitive closure iAg i,a .remark converse Barcan formula, CBF , Ki x xKi holdsmf-models; Barcan formula, BF , xKi Ki x not. checkconsider mf-model = hW, RW , {i,a }iAg,aD , D, Ii Fig.1(a)- W = {w, w0 , w00 }- RW = {r, r0 , r00 } r(0) = w, r0 (0) = w0 , r00 (0) = w0013fiBelardinelli & Lomuscio............i,bi,a......i,di,cw0ww00v0vP (a)P (a), P (b)P (b)Q(c)Q(c)v 00(b) mf-model M0 .(a) mf-model M.Figure 1: Arrows represent system runs; epistemically related states groupedtogether.- = {a, b}- I(P 1 , r(0)) = {a, b}, I(P 1 , r0 (0)) = {a} I(P 1 , r00 (0)) = {b}- i,a i,b equivalence relations (r, 0) i,a (r0 , 0) (r, 0) i,b (r00 , 0).see (M, r, 0) |= xKi P (x), (M, r, 0) 6|= Ki xP (x) (r, 0) i,a (r0 , 0)(M , r0 , 0) 6|= P (x) (x) = b.Furthermore, K axiom Ki ( 0 ) (Ki Ki 0 ) valid mf-modelseither. fact, consider mf-model M0 = hW 0 , R0W 0 , {0i,a }iAg,aD0 , D0 , 0 Fig.1(b)- W 0 = {v, v 0 , v 00 }- R0W 0 = {q, q 0 , q 00 } q(0) = v, q 0 (0) = v 0 , q 00 (0) = v 00- D0 = {c, d}- 0 (Q1 , q(0)) = {c}, 0 (Q1 , q 0 (0)) = {c} 0 (Q1 , q 00 (0)) =- i,c i,d equivalence relations (q, 0) i,c (q 0 , 0) (q, 0) i,d (q 00 , 0).Finally, let (x) = c. check (M , q, 0) |= (Q(x) xQ(x)) Q(x)(M , q 0 , 0) |= (Q(x) xQ(x)) Q(x). Thus, (M , q, 0) |= Ki (Q(x) xQ(x)) Ki Q(x).(M, q 00 , 0) 6|= xQ(x), (M , q, 0) 6|= Ki xQ(x).prove following lemma, used completeness proofsystems satisfying perfect recall learning. lemma states that, dealsatisfability monodic formulas, mf-models suffice.Lemma 2. Let MF K,BFclass mf-models validating formulas K BF .1 ),every monodic formula L1m (resp. LCmKm |= iff MF K,BF|=14fiInteractions Knowledge Time First-Order Logic MASProof. implication right left follows fact class KmKripke models isomorphic subclass monodic friendly Kripke modelsAg, a, b D, i,a equal i,b . words, given Kripke model =hW, RW , {i }iAg , D, Ii define mf-model M0 = hW, RW , {i,a }iAg,aD , D, Ii,every D, i,a equal . straightforward see M0 validatesK BF (in particular, counterexamples Fig. 1 ruled out). Further,6|= M0 6|= . Thus, MF K,BF|= , Km |= .implication left right, assume Mmf = hW, RW ,{i,a }iAg,aD , D, Ii mf-model validating K BF (Mmf , r, n) 6|=point (r, n) assignment . build Kripke model M0 =hW 0 , R0W 0 , {0i }iAg , D0 , 0 Mmf (M0 , r, n) 6|= follows. startassuming W 0 = W , SR0 = R D0 = D. Further, Ag, define 0itransitive closure aD i,a . Finally, set 0 = I. check Kripkemodel M0 well defined validate .First all, point following issue associated construction above:case point (q, k) monodic formula [x], happens(Mmf , q, k) |= Ki [x], (q, k) i,(x) (q 0 , k 0 ) (q, k) i,(y) (q 00 , k 00 ) (x) 6= (y).Further, suppose (Mmf , q 00 , k 00 ) 6|= [x], obviously (Mmf , q 0 , k 0 ) |=[x]. definition 0i (M0 , q, k) 6|= Ki [x]; two modelssatisfy formulas. solve problem modifying interpretationaccording structure monodic formula [x], keeping truth value[x] point (q, k). consider relevant cases according structure [x];induction hypothesis consists fact able find interpretationsubformulas [x].[x] = P (x) simply assume (x) I(P, q 00 (k 00 )), (M0 , q 00 , k 00 ) |=[x] (M0 , q, k) |= Ki [x]. Note change truth valueepistemic formula (q, k) assumed (q, k) 6i,(x) (q 00 , k 00 ) (otherwise [x] wouldsatisfied (q 00 , k 00 )). cases propositional connectives modal operatorssimilarly dealt applying induction hypothesis. [x] = y[x, y]b, q 00 , k 00 ) 6|= [x, y].(Mmf , q 00 , k 00 ) 6|= [x], therefore exists b (Mmfconsider 4 different cases depending whether (q, k) satisfies4 formulas:Ki x [x, y](3)Ki x [x, y](4)Ki x [x, y](5)Ki x [x, y](6)using axioms inference rules QKTm Formula (3) showfollows (where used entailment):(Mmf , q, k) |= Ki y[x, y] Ki x[x, y](Mmf , q, k) |= xKi y[x, y] yKi x[x, y] Ex(Mmf , q, k) |= Ki xy[x, y] Ki yx[x, y] zKi Ki z(Mmf , q, k) |= Ki (xy[x, y] yx[x, y]) Ki ( ) Ki Ki15fiBelardinelli & Lomuscio(Mmf , q, k) |= Ki (xu[x, u] yv[v, y])(Mmf , q, k) |= Ki xyuv([x, u] [v, y])(Mmf , q, k) |= Ki xy([x, y] [x, y])change variablesprefixingExlast formula contradiction; (3) cannot hold (q, k). Similarly, Formula(4) cannot hold (q, k) either because:(Mmf , q, k) |= Ki y[x, y] Ki x[x, y](Mmf , q, k) |= xKi y[x, y] Ki x[x, y](Mmf , q, k) |= Ki xy[x, y] Ki x[x, y](Mmf , q, k) |= Ki xy[x, y] Ki yx[x, y](Mmf , q, k) |= Ki (xy[x, y] yx[x, y])(Mmf , q, k) |= Ki xy([x, y] [x, y])ExzKi Ki zz Ki Ki zKi Ki Ki ( )similarlyNote derivations make use formulas K BF (for instance,prove theorems Ki ( ) Ki Ki zKi Ki z). Finally, satifyFormulas (5) (6) (q, k), guarantee existence individual xavoiding clash (x). So, introduce new individual a0 domain D0a0 (x) satisfy formulas points. Thus, a0 seen copy(x). Finally, induction hypothesis modify interpretation 0(M0 , q 00 , k 00 ) |= [x, y].case common knowledge operator derives one Ki . result,obtain Kripke model M0 (M0 , r, n) 6|= .Moreover, procedure described above, Mmf satisfies perfect recall, learning, synchronicity, unique initial state, also M0 satisfies property.Thus, Lemma 2 prove following result.Corollary 2. Let X subset {pr, nl, sync, uis}. every monodic formula L1m1 ), satisfiable MF X also validating formulas K BF ,(resp. LCmX.satisfiable KmProof. easy see Mmf satisfies either synchronicity uniqueinitial state, M0 well way defined. Further, suppose Mmfsatisfies perfect recall, (r, n) 0i (r0 , n0 ) n > 0. means sequencea1 , . . . , ak individuals sequence (q1 , m1 ), . . . , (qk , mk ) points (i)(r, n) = (q1 , m1 ) (r0 , n0 ) = (qk , mk ); (ii) (qj , mj ) i,aj (qj+1 , mj+1 ) j < k.show result k = 3, case k > 3 follows straightforward generalisation.(q1 , m1 1) i,a1 (q2 , m2 ), definition 0i (q1 , m1 1) 0i(q3 , m3 ) well. Hence, Mmf satisfies perfect recall. Otherwise, suppose perfectrecall l2 < m2 (q1 , m1 1) i,a1 (q2 , l2 ), l20 , l2 < l20 m2implies (q1 , m1 ) i,a1 (q2 , l20 ). consider l20 (h) = m2 h, 0 h < m2l2 . perfect recall, either (i) exists p3 (h) (q2 , l20 (h)) i,a2 (q3 , p3 (h)),p03 (h), p3 (h) < p03 (h) p3 (h 1) implies (q2 , l2 (h)) i,a2 (q3 , p03 (h)), (ii)(q2 , l20 (h) 1) i,a2 (q3 , p3 (h 1)), p3 (1) = m3 . Notice cases,definition 0i , (q1 , m1 ) 0i (q3 , p03 (h)) 0 h < m2 l2 , is,(q1 , m1 ) 0i (q3 , p03 ) p3 [l2 + 1] < p03 m3 . Further, l2 , either (i) exists l316fiInteractions Knowledge Time First-Order Logic MAS(q2 , l2 1) i,a2 (q3 , l3 ), l30 , l3 < l30 p3 [l2 + 1] implies (q2 , l2 ) i,a2 (q3 , l30 ),(ii) (q2 , l2 1) i,a2 (q3 , p3 [l2 + 1]). first case, l30 strictly lessm3 , l30 (q1 , m1 1) 0i (q3 , l30 ) l300 , l30 < l300 m3 implies(q1 , m1 ) 0i (q3 , l300 ). Otherwise, (q1 , m1 1) 0i (q3 , m3 ). Hence, Mmf satisfiesperfect recall.proof learning similar.Finally, combining Corollaries 1 2 immediately obtain following result.Corollary 3. Let X subset {pr, nl, sync, uis}. every monodic formula L1m1 ), satisfiable MF X also validating formulas K BF ,(resp. LCmsatisfiable QIS Xm.next section show indeed possible build mf-model.4. Completeness Proofsection outline main steps completeness proof, basedquasimodel construction (Gabbay, Kurucz, Wolter, & Zakharyaschev, 2003; Hodkinsonet al., 2000). Differently contributions, explicitly take accountinteraction temporal epistemic modalities. Intuitively, quasimodelmonodic formula relational structure whose points sets sets subformulas. set sets subformulas describes possible state affairs, containssets subformulas defining individuals state. formally, given formulaLCn1 definesubC = sub {EC | C sub} {Ki C | C sub, Ag}sub set subformulas . L1n , subC simply sub. Further,definesubC = subC { | subC } { | subC } { | subC }Observe subC closed negation modulo equivalences 1,is, subC , form subC ; otherwise,subC . Finally, let subn subset subC containing formulasn free variables. So, sub0 set sentences subn . x variable occurring, define subx = {[y/x] | [y] sub1 }. Clearly, x free variableformulas subx . con denote set constants occurring . Table 3report set suby equal Formula (1) thus abbreviated:C(zAv(y, z)UxReq(x, y))Further, k N define closures clk clk,i mutual recursion.Definition 12. Let cl0 = subx k 0, clk+1 = iAg clk,i . k 0, Ag,clk,i = clk {Ki (1 . . . n ), Ki (1 . . . n ) | 1 , . . . , n clk }.17fiBelardinelli & Lomusciosuby{, C(zAv(y, z)UxReq(x, y)), EC(zAv(y, z)UxReq(x, y)),{Ki C(zAv(y, z)UxReq(x, y))}iAg , zAv(y, z)UxReq(x, y), zAv(y, z), xReq(x, y),, C(zAv(y, z)UxReq(x, y)), EC(zAv(y, z)UxReq(x, y)),{Ki C(zAv(y, z)UxReq(x, y))}iAg , zAv(y, z)UxReq(x, y), zAv(y, z),xReq(x, y),, C(zAv(y, z)UxReq(x, y)), EC(zAv(y, z)UxReq(x, y)),{ Ki C(zAv(y, z)UxReq(x, y))}iAg , zAv(y, z)UxReq(x, y), zAv(y, z),xReq(x, y),, C(zAv(y, z)UxReq(x, y)), EC(zAv(y, z)UxReq(x, y)),{ Ki C(zAv(y, z)UxReq(x, y))}iAg , zAv(y, z)UxReq(x, y), zAv(y, z),xReq(x, y)}Table 3: set suby equal Formula (1).{, C(zAv(y, z)UxReq(x, y)), EC(zAv(y, z)UxReq(x, y)),{Ki C(zAv(y, z)UxReq(x, y))}iAg , zAv(y, z)UxReq(x, y), zAv(y, z), xReq(x, y),, C(zAv(y, z)UxReq(x, y)), EC(zAv(y, z)UxReq(x, y)),{ Ki C(zAv(y, z)UxReq(x, y))}iAg , zAv(y, z)UxReq(x, y), zAv(y, z),xReq(x, y)}Table 4: type cl0 , equal Formula (1).define ad() greatest number alternations distinct Ki along branchparse tree (Halpern et al., 2004). Further, index finite sequence = i1 , . . . , ikagents 6= in+1 , 1 n < k; length denoted ||. Also, ]iabsorptive concatenation indexes ]i = ik = i. Finally, Kshorthand Ki1 . . . Kik . let index || ad(). emptysequence cl = clad() . = 0 ]i, cl = clk,i k = ad() ||.introduce types quasimodels, intuitively seen individuals describedmaximal consistent sets formulas.Definition 13 (Type). -type maximal consistent subset cl , i.e.,every monodic formulas 0 cl ,(i) iff/ t;(ii) 0 iff , 0 t.Two -types t, t0 said agree sub0 sub0 = t0 sub0 , i.e., sharesentences. Given -type constant c con, ht, ci indexed type, abbreviated tc . Table 4 report type cl0 , equal Formula (1).introduce state candidates, intuitively represent states quasimodel.Definition 14 (State candidate). -state candidate pair C = hT, con(i) set -types agree sub0 ;(ii) con set containing c con indexed type tc .18fiInteractions Knowledge Time First-Order Logic MASalso introduce notion point, describes state candidate perspective particular type.Definition 15 (Point). -point pair P = hC, ti(i) C = hT, con -state candidate ;(ii) -type.Note that, slight abuse notation, call points pairs (r, n) QISpairs P = hC, ti. consistent previous work (Fagin et al., 1995; Halpernet al., 2004); context disambiguate. Also, write C C = hT, con. Similarly P. Given -state candidate C = hT, con point P = hC, tidefine formulas C P follows:C :=^xt[x] xtT_tTt[x]^t[x/c]tc conP := Cdistinguish type conjuction formulas contains.-state candidate C S-consistent formula C consistent w.r.t. system S,i.e., 0S C . Similarly, -point P S-consistent formula P consistent w.r.t. S.refer plain consistency whenever system reference understood. Consistentstate candidates represent states quasimodels. define relationssuitability constitute relational part quasimodels.Definition 16.1 -type t1 2 -type t2 -suitable, t1 t2 , iff 1 = 2t1 t2 consistent. i-suitable, t1 t2 , iff 1 ]i = 2 ]i t1 Ki t2consistent.1 -state candidate C1 2 -state candidate C2 -suitable, C1 C2 , iff1 = 2 C1 C2 consistent. i-suitable, C1 C2 , iff 1 ]i = 2 ]iC1 Ki C2 consistent.1 -point P1 2 -point P2 -suitable, P1 P2 , iff 1 = 2 P1P2 consistent. i-suitable, P1 P2 , iff 1 ]i = 2 ]i P1 Ki P2consistent.1 -point P1 = hC1 , t1 2 -point P2 = hC2 , t2 -suitable constantc con, P1 c P2 , iff P1 P2 , tc1 T1con tc2 T2con . i-suitablec, P1 ci P2 , iff P1 P2 , tc1 T1con tc2 T2con .using axioms , 4 5 shown relation reflexive,transitive symmetric, is, equivalence relation. Also, relation serial.following lemma list properties relations usefulfollows.Lemma 3.(i) Let subx , t1 t2 t1 iff t2 .19fiBelardinelli & Lomuscio(ii) Let Ki subx let -type, Ki iff -types t0 , t0 impliest0 . Moreover, let |]i| ad(), Ki iff ]i-types t0 , t0 impliest0 .Proof.(i) proof similar one Lemma 9(i) work Wolter et al. (2002).t1/ t2 t2 since t1 t2 consistent, alsoconsistent, contradiction. right left, t2/ t1t1 . Since t1 t2 consistent, also consistent,contradiction.(ii) left right, Ki/ t0 t0 since Ki t0 consistent,also Ki Ki consistent, contradiction. right left,Ki/ extend set {} { | Ki t} -type t0 . particular,t0 t0 . Moreover, |]i| ad() similarly prove Kiiff ]i-types t0 , t0 implies t0 .present frame underlying quasimodel .Definition 17 (Frame). frame F tuple hR, D, {i,a }iAg,aD , fi(i) R non-empty set indexes r, r0 , . . .;(ii) non-empty set individuals;(iii) every Ag, D, i,a equivalence relation set points (r, n)r R n N;(iv) f partial function associating point (r, n) consistent state candidatef(r, n) = Cr,n(a) domain f empty;(b) f defined (r, n), defined (r, n + 1);(c) f defined (r, n) (r, n) i,a (r0 , n0 ), f defined (r0 , n0 ).function f partial take consideration case synchronous systems. Also,straightforward introduce frames satisfying perfect recall, learning, synchronicity,unique initial state, following definitions given mf-models. Next,provide definition objects, correspond runs Gabbay et al. (2003).choose terminology avoid confusion runs QIS.Definition 18 (Object). Given individual D, object frame F mapconassociating type (r, n) Tr,n every (r, n) Dom(f) f(r, n) = Cr,n = hTr,n , Tr,n1. (r, n) (r, n + 1)2. (r, n) i,a (r0 , n0 ) (r, n) (r0 , n0 )20fiInteractions Knowledge Time First-Order Logic MAS3. U (r, n) iff n0 n (r, n0 ) n00 , n n00 < n0implies (r, n00 )4. (r, n) -types, (r0 , n0 ), (r, n) i,a (r0 , n0 ) (r0 , n0 ) =5. C (r, n) exists point (r0 , n0 ) reachable (r, n)(r0 , n0 )object+ satisfies (1), (2), (3), (5) (40 ) instead (4).(40 ) (r, n) -type, ]i-type (r, n) t, (r0 , n0 ) i,a (r, n),(r0 , n0 ) = t.Intuitively, object identifies individual, represented types, acrossdifferent state candidates. elements give definition quasimodel.Definition 19 (Quasimodel). quasimodel tuple Q = hR, O, {i, }iAg,O , fihR, O, {i, }iAg,O , fi frame,1. Tr,n Tr,n Cr,n2. Cr,n Cr,n+13. (r, n) i, (r0 , n0 ) (r, n) (r0 , n0 )4. every Tr,n exists object (r, n) =con object O.5. every c con, function c c (r, n) = tc Tr,nquasimodel+ defined quasimodel clauses (4) (5) refer objects+rather objects. define quasimodels (resp. quasimodel+ ) satisfying perfect recall,learning, synchronicity, unique initial state, assuming correspondingcondition underlying frame. difference objects (resp. quasimodel)objects+ (resp. quasimodel+ ) purely technical. particular, latter neededsystems satisfying perfect recall learning become apparent Section 5.following lemma list properties quasimodels usefulfollows.Lemma 4. every quasimodel Q, every object O,(i) Ki (r, n) iff (r0 , n0 ), (r0 , n0 ) i, (r, n) implies (r0 , n0 ).(ii) C (r, n) iff points (r0 , n0 ) reachable (r, n) (r0 , n0 ).Proof.(i) implication left right follows fact (r0 , n0 ) i, (r, n) implies(r, n) (r0 , n0 ). implication right left, Ki/ (r, n)Lemma 3(ii) -type (r, n) t. Definition 18(r0 , n0 ), (r, n) i, (r0 , n0 ) (r0 , n0 ) = t.21fiBelardinelli & Lomuscio(ii) implication left right proved induction length path(r, n) (r0 , n0 ). base case inductive step follow axiom C1.implication right left follows Definition 18.state main result section, is, satisfability quasimodels impliessatisfability mf-models. follows quasimodel Q validates formula belongsevery type every state-candidate Q.Theorem 3. quasimodel (resp. quasimodel+ ) Q monodic formula ,satisfiable mf-model Mmf . Moreover, Q validates formulas K BF ,Mmf . Finally, Q satisfies perfect recall, learning, synchronicity,unique initial state, Mmf .Proof. proof inspired Lemmas 11.72 12.9 work Gabbayet al. (2003), consider monodic friendly Kripke models rather standardKripke models. First, every monodic formula form Ki , C, 1 U2introduce k-ary predicate constant Pk k equal 0 1, depending whether0 1 free variables . formula Pk (x) called surrogate . Givenmonodic formula denote formula obtained substituting modalsubformulas within scope another modal operator surrogates.Since every state candidate C quasimodel Q consistent systemfirst-order temporal-epistemic logic considered Section 3 based classical first-orderlogic, formula C consistent respect first-order (non-modal) logic. Godelscompleteness theorem first-order structure = hI, Di, non-empty setindividuals first-order interpretation D, satisfies C , i.e., |= Cassignment variables elements D. intend build mf-modeljoining first-order structures. However, possible structuresdifferent domains different cardinalities. solve problem, consider cardinalnumber 0 greater cardinality set objects Q define= {h, | O, < }Then, (r, n) Q, -type Tr,n|{h, | (r, n) = t}| =method described Claim 11.24 Gabbay et al. (2003), expandfirst-order structure obtain structure Ir,n = hIr,n , Di domain Ir,nsatisfies Cr,n|{a | (x) = Ir,n|= t[x]}| =So, assume without loss generality first-order structures Ir,n sharedomain D, every Tr,n , h, D,(r, n) = iff Ir,n|= t[x](x) = h, i. Equivalently, Tr,n , (x) = h, D,(r, n) = { cl | Ir,n|= [x]}22(7)fiInteractions Knowledge Time First-Order Logic MASMoreover, Ir,n (c) = hc , 0i every c con.define mf-model Mmf tuple hW, R, {i,a }iAg,aD , D, Ii Wset points (r, n) r R Q n N; R set runs N Wr(n) = (r, n); defined above; Ag h, D, i,h,i defined i, ;interpretation obtained joining various first-order interpretations Ir,n ,i.e., I(P, r(n)) = Ir,n (P ) every predicate constant P . prove followingresult Mmf .Lemma 5. mf-model Mmf obtained quasimodel Q described above,every subx ,Ir,n|= iff (Mmf , r, n) |=Moreover, Q quasimodel+ , f(r, n) -state candidate ad(K ) ad()Ir,n|= iff (Mmf , r, n) |=Proof. proof similar Lemma 12.10 work Gabbay et al. (2003).begin first part. base case induction follows definition interpretation mf-model. step propositional connectives quantifiers followsinduction hypothesis equations 1 2 = 1 2 , 1 = 1 , x1 = x1 .let = [x] assume (x) = h, i, have:Ir,n|= [x]iff[x] (r, n)(8)iff[x] (r, n + 1)(9)iffIr,n+1|= [x](Mmf , r, n + 1) |= [x](Mmf , r, n) |= [x]iffiff(10)(11)Steps (8) (10) follow Equation (7). Step (9) motivated Lemma 3(i),step (11) follows induction hypothesis.Let = (U0 )[x] (x) = h, i, have:Ir,n|= (U0 )[x] iffiff(U0 )[x] (r, n)0(12)00n n [x] (r, n )[x] (r, n00 ) n n00 < n0iff0n nIr,n00Ir,n0|=(13)0 [x]00|= [x] n n < n00iffn niff(Mmf , r, n00 ) |= [x](Mmf , r, n) |= U0 [x](Mmf , r, n0 )(14)0|= [x]00n n < n0(15)Steps (12) (14) follow Equation (7). Step (13) motivated Def. 18, step(15) follows induction hypothesis.23fiBelardinelli & LomuscioLet = Ki [x] (x) = h, i, have:Ir,n|= Ki [x]iffiffKi [x] (r, n)0000(16)00(r , n ) i, (r, n), [x] (r , n )(17)(r , n ) i,h,i (r, n), Ir0 ,n0 |= [x](r0 , n0 ) i,h,i (r, n), (Mmf , r0 , n0 )iffiffiff(Mmf , r, n)(18)|= [x](19)|= Ki [x]Steps (16) (18) follow Equation (7). Step (17) motivated Lemma 4(i), step(19) follows induction hypothesis.Let = C[x] (x) = h, i, have:Ir,n|= C[x]iffiffiffC[x] (r, n)000000(20)00(r , n ) reachable (r, n), [x] (r , n )(r , n ) reachable(r, n), Ir0 ,n0(21)|= [x](22)0(23)0iff(r , n ) reachable (r, n), (M, r , n ) |= [x]iff(M, r, n) |= C[x]Steps (20) (22) follow Equation (7). Step (21) motivated Lemma 4(ii),step (23) follows induction hypothesis.prove second part lemma. cases identical first part,except = Ki . Suppose f(r, n) -state candidate ad(K ) ad().implication left right, (r, n) i, (r0 , n0 ) (r0 , n0 ) 0 -type]i = 0 ]i. Thus, ad(K0 ) ad(K]i ) ad(K Ki ) ad(). So, applyinduction hypothesis. implication right left, ad(K Ki ) ad()|]i| ad() Lemma 3(ii) ]i-type (r, n)t. Def. 18 (r0 , n0 ) (r, n) i, (r0 , n0 ) (r0 , n0 ) = t. Sincead(K]i ) = ad(K Ki ) ad() apply induction hypothesis.complete proof Theorem 3, definition quasimodelTr,n Tr,n Cr,n . Therefore, satisfied mf-model Mmf point (r, n).also remark Q validates formulas K BF , Mmf . caseas, K BF belong every type every state-candidate Q, Lemma 5Mmf validates K BF well.Finally, Q satisfies perfect recall, learning, synchronicity, uniqueinitial state, mf-model obtained Q satisfies corresponding constraintsconstruction. show fact perfect recall: (r, n) i,h,i (r0 , n0 ) n > 0,particular (r, n) i, (r0 , n0 ). Since Q satisfies perfect recall, either (r, n 1) i, (r0 , n0 ),k < n0 (r, n 1) i, (r0 , k) k 0 , k < k 0 n0 implies(r, n) i, (r0 , k 0 ). definition i,h,i obtain either (r, n 1) i,h,i (r0 , n0 ),k < n0 (r, n 1) i,h,i (r0 , k) k 0 , k < k 0 n0 implies(r, n) i,h,i (r0 , k 0 ), is, Mmf satisfies perfect recall well.next show existence quasimodels monodic .24fiInteractions Knowledge Time First-Order Logic MAS5. Dealing Systemsection consider completeness proof system Theorem 2. particular, show monodic formula consistent respect system S,build quasimodel (or quasimodel+ specific cases) based frameS. following sections symbol ` represents provability appropriate systemS. start lemmas useful construction quasimodelsystem.Lemma 6. (i) consistent monodic formula consistent -state candidate C = hT, con .(ii) Let P = hC, ti consistent -point C = hT, con i, let c con.Then,(a) C C0 exists -point P0 = hC0 , t0 P P0 .(b) tc con C C0 exists -point P0 = hC0 , t0 P c P0 .(c) 1 U2 sequence -points Pj = hCj , tj j krealises 1 U2 , i.e., P = P0 . . . Pk , 2 tk 1 tj j < k.(d) 1 U2 tc sequence -points Pj = hCj , tj j kc-realises 1 U2 , i.e., sequence realises 1 U2 P0 c . . . c Pk .(e) Ki -point P0 = hC0 , t0 P P0 t0 .(f ) Ki tc -point P0 = hC0 , t0 P ci P0 t0 .(g) C sequence -points Pj = hCj , tj j kP = P0 i0 . . . ik1 Pk tk .(h) C tc sequence -points Pj = hCj , tj j kP = P0 ci0 . . . cik1 Pk tk .Proof. proof similar one Claims 11.75, 11.76 12.13 workGabbay et al. (2003), consider -state candidates -points. Letdisjunction formulas P -points P . Consider formula ,obtained substituting subformulas form Ki , C, 1 U2within scope another modal operator surrogates. checktrue (non-modal) first-order structures. Since QKTm QKTCmextend first-order logic, semantical completeness first-order logic`(24)W(i) Notice that, previous remark, ` also = {P|P -point } P .Moreover, consistent (24) also consistent. Therefore,disjunct P P consistent. So, P = hC, ti.(a) (24) Nec ` . So, P consistent must-point P0 P P0 also consistent.(b) proof similar (a).25fiBelardinelli & Lomuscio(c) proof contradiction. Let U set -points P0W exist points Pj = hCj , tj j < k P = P0 . . . Pk = P0 . Let = {P0 |P0 U } P0 .show` 2(25)otherwise, would sequence realising 1 U2 . Moreover, definitionU,`(26)(25) obtain` G G2together (25) (26) derive` (2 G2 )(27)consider P1 U P P1 . (27)`P1 (2 G2 )`P1 G2`(P P1 ) G2(28)hand, since 1 U2` (P P1 ) F 2(29)(28) (29) contradict fact P P1 .(d) proof similar (c).(e) First remark P Ki ( ) consistent. Thus, exists -pointP0 = hC0 , t0 P Ki (P0 ) consistent. Hence, P P0 t0 .(f) proof similar (e).(g) proof contradiction. Let V minimal set -points DWsuch (i)P V ; (ii) V D0 Ag, D0 V . Let = {D|DV } .show`(30)(30) hold, would sequence specified lemma. Moreover,definition V ,` Ki(31)Ag. (30) (31) obtain` ( E)axiom C2,` Cdefinition P,` P Ccontradicts (32).26(32)fiInteractions Knowledge Time First-Order Logic MAS(h) proof similar (g).following result always possible extend -suitability relationtypes -suitability points.Lemma 7. Suppose t0 -types t0 , -pointsP = hC, ti P0 = hC0 , t0 P P0 . particular, c con,-points P = hC, ti P0 = hC0 , t0 P c P0 .WProof. Lemma 6 ` ` = {P|P -point } P .Since t0 , ( t0 ) consistent. Thus, must -points PP0 P (P0 t0 ) consistent. Then, case P = hC, tiP0 = hC0 , t0 -state candidates C C0 . result, P P0 . second partlemma proved similarly first observing t0 t[x/c] t[x/c]consistent. Hence, also t[x/c] ( t0 [x/c]) consistent. Thus, must-points P P0 P t[x/c] (P0 t0 [x/c]) consistent. So, tc cont0c 0con , is, P c P0 .According Lemma 7 always extend possibly infinite sequence -types t0t1 . . . possibly infinite sequence -points P0 P1 . . . Pk = hCk , tk i.Definition 20. Let -sequence possibly infinite sequence C0 C1 . . . -statecandidates. -sequence acceptable k 0,(i) 1 U2 tk , tk Ck , 1 U2 realised sequence -points Pj =hCj , tj k j n;(ii) 1 U2 tck , tck Ck , 1 U2 c-realised sequence -points Pj =hCj , tj k j n.following lemma entails completeness result.Lemma 8. Every finite -sequence -state candidates extended infiniteacceptable -sequence.Proof. Assume C0 . . . Cn finite -sequence 1 U2 tk Ckk n. Either 1 U2 realised C0 . . . Cn , Lemma 6(ii)(c) extend-sequence 0 realises 1 U2 . procedure repeated formulasform 1 U2 appearing point -sequence. Thus, obtain (possiblyinfinite) -sequence C0 C1 . . . property (i) Definition 20 satisfied.also satisfy property (ii) reason similarly using Lemma 6(ii)(d) instead.let X new object, sequence X, . . . , X, Cn , Cn+1 , . . . acceptable nstarts n copies X Cn , Cn+1 , . . . acceptable -sequence.consider completeness proof single class QIS.uissync,uis5.1 Classes QIS , QIS sync, QIS QISstart completeness proof systems QKTm QKTCm ,interaction temporal epistemic operators.27fiBelardinelli & Lomusciomonodic formula consistent, Lemma 6(i) consistent -statecandidate C = hT, con type . Also, Lemma 8extend C infinite acceptable -sequence. So, set infinite acceptable sequences non-empty. Let R set -sequences acceptable n,n N. r R, k N, define partial function f R N f(r, k) = Ck r-sequence X, . . . , X, Cn , Cn+1 , . . . acceptable n k n, undefined otherwise.Finally, let set functions associating every (r, n) Dom(f) type(r, n) Tr,n(A) (r, n) (r, n + 1);(B) U (r, n) iff n0 n (r, n0 ) (r, n00 )n n00 < n0 ;(C) (r, n) -types, (r0 , n), (r0 , n) = t;(D) C (r, n) exists point (r0 , n) sequence -points Pj =hCj , tj j k, hf(r, n), (r, n)i = P0 i0 . . . ik1 Pk , tk , f(r0 , n) =Ck (r0 , n) = tk .show non-empty. Condition (A) guaranteed Lemma 6(ii)(a),condition (B) fact r acceptable -sequence. regards (C) remark(r, n) find consistent -point P = hC, ti reasoning similarlyLemma 6(i), Lemma 8, C extended -sequence r0 acceptablen. Finally, set (r0 , n) = t. (D) observe C (r, n)Lemma 6(ii)(g) exists sequence -points Pj = hCj , tj j k,hf(r, n), (r, n)i = P0 i0 . . . ik1 Pk tk . Now, Ck extended sequence r0 acceptable n (r0 , n) = tk . Finally, Ag, O, define(r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ) n = n0 .Lemma 9. tuple hR, O, {i, }iAg,O , fi synchronous frame.Proof. previously shown R non-empty. Also, i,equivalence relation definition, f satisfies conditions Definition 17. Further,frame synchronous definition i, .prove main result.Lemma 10. tuple hR, O, {i, }iAg,O , fi synchronous quasimodelvalidates formulas K BF .Proof. previous lemma, remains prove functions objects.Conditions (1), (3), (4) (5) objects safisfied remarks (A)-(D) above. Condition (2) satisfied definition i, . Furthermore, conditions (1), (2) (3)quasimodels satisfied definitions R, f i, . regards (4), extendfunction (r, n) = Dom(f) using Lemma 6(ii)(a), (c), (e) (g). (5)function c c (r, n) = tc object Lemma 6(ii)(b), (d), (f) (h).Finally, Q validates formulas K BF , C, C Q, consistentQKTm (resp. QKTCm ).28fiInteractions Knowledge Time First-Order Logic MAScompleteness QKTm QKTCm respect classes QIS QIS syncquantified interpreted systems directly follows Lemma 10 together Theorem 3.Thus, obtain following item Theorem 2.Theorem 4 (Completeness). system QKTm (resp. QKTCm ) complete w.r.t.classes QIS QIS syncQIS.sync,uisprove completeness QIS uisuse next result,QISextension propositional case (Halpern et al., 2004).1 ) satisfiableRemark 2. Suppose X subset {pr, sync}. L1m (resp. LCmXX,uisQIS also satisfiable QIS .Thus, system QKTm (resp. QKTCm ) also complete w.r.t. classes QIS uissync,uisQISQIS.pr,uis5.2 Classes QIS prQISbegin investigate systems interactions time knowledgepr,uispresent. completeness proof QKT1m respect QIS prreliesQISfollowing lemma.Lemma 11. -points P1 = hC1 , t1 i, P2 = hC2 , t2 ]i-type t02 , P1 P2t2 t02 ]i-point P02 = hC02 , t02 -sequence S1 . . . Sn = P02]i-points Sk = hDk , sk i, s1 t1 sk t2 1 < k n. Further,k n.P1 c P2 sck TDconkProof. extend proof Halpern et al. (2004, Lemma 5.5) deal statecandidates monodic friendly Kripke frames. cited result provet1 t2 t2 t02 sequence ]i-types s1 . . . sn = t02s1 t1 sk t2 1 < k n. Lemma 7 extend sequence]i-types sequence ]i-points S1 . . . Sn Sk = hDk , sklemmas statement satisfied. particular, P1 c P2 also Lemma 7 assumewithout loss generality sck TDconk n.kconsistent L1m define quasimodel+ establish completenessQKT1m respect QIS pr. Let R set acceptable -sequences, definef f(r, k) = Ck r -sequence C0 , C1 , . . . . Finally, let setfunctions associating every (r, n) Dom(f) type (r, n) Tr,n conditions(A) (B) satisfied(C) (r, n) -type, ]i-type (r, n) t, (r0 , n0 ), (r0 , n0 ) = t.(E) (r, n) (r0 , n0 ) n > 0 either (a) (r, n 1) (r0 , n0 ) (b)k < n0 (r, n 1) (r0 , k) k 0 , k < k 0 n0 implies (r, n)(r0 , k 0 ).Finally, Ag, O, define (r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ).following lemma shows set non-empty. particular, conditions (C)(E) satisfied functions O.29fiBelardinelli & LomuscioLemma 12. set functions satisfies conditions (A), (B), (C) (E)non-empty.Proof. Conditions (A) (B) follow respectively Lemma 6(ii)(a) factr acceptable -sequence. regards (C) (E), proof proceeds inductionn. result n = 0 immediate, take r0 acceptable -sequencestarting C C. Further, define (r0 , 0) = t. Thus, (r0 , 0) (r, 0)(C) (E) satisfied.suppose n > 0 result holds n 1. Since f(r, n 1) f(r, n)(r, n) t, follows Lemma 11 ]i-point P = hC, ti -sequence]i-points P0 S0 . . . Sk = P Sk0 = hDk0 , sk0 sk0 (r, n) k 0 k.induction hypothesis, exists every ]i-type (r, n1) point(r0 , n0 ) (r0 , n0 ) = s. case (a), take = t; (r, n 1) (r0 , n0 )(r0 , n0 ) = t. Thus, also case (r, n) (r0 , n0 ). case (b), take = t0 .Hence, (r, n 1) (r0 , n0 ) (r0 , n0 ) = t0 . suppose r0 derivedacceptable -sequence v0 , v1 , . . .. Let r00 run derived acceptable sequenceinitial segment v0 , . . . , vn0 , D0 , . . . , Dk . Again, run exists Lemma 8.define (r00 , n0 + k + 1) = sk = t. Thus, (r, n) (r00 , n0 + k + 1) (C)(E) satisfied.prove following lemma.Lemma 13. tuple hR, O, {i, }iAg,O , fi frame satisfies perfect recall.Proof. Lemmas 6(i), 8 12 sets R non-empty. Also, f satisfiesconditions Definition 17. Finally, i, equivalence relation definition,satisfies perfect recall definition functions O.Finally, prove main result section.Lemma 14. tuple hR, O, {i, }iAg,O , fi quasimodel+ perfect recallvalidates formulas K BF .Proof. previous lemma hR, O, {i, }iAg,O , fi frame satisfying perfectrecall; left prove functions objects+ . Conditions (1)-(4)objects+ safisfied remarks (A)-(E) definition i, . Furthermore, conditions(1), (2) (3) quasimodels+ satisfied definitions R, f i, .regards (4), follows Lemma 11. Finally, condition (5) quasimodels+ holdsLemma 6(ii)(b), (d), (f) (h) Lemma 11. Finally, Q validates formulas KBF , C, C Q, consistent QKTm .completes proof QIS pr. Thus, obtain following item Theorem 2.Theorem 5 (Completeness). system QKT1m complete w.r.t. class QIS prQIS.completeness QKT1m respect QIS pr,uisfollows Remark 2.5.3 Classes QIS pr,syncQIS pr,sync,uiscompleteness QKT2m respect QIS pr,syncproved similarly previouscase using following lemma instead Lemma 11.30fiInteractions Knowledge Time First-Order Logic MASLemma 15. -state candidates C1 , C2 ]i-state candidate C02 , ]i-statecandidate C01C1 C2 C2 C02 C1 C01 C01 C02 .c con, P1 = hC1 , t1 i, P2 = hC2 , t2 P02 = hC02 , t02 i, P1 c P2P2 ci P02 P01 = hC01 , t01 i, P1 ci P01 P01 c P02 .Proof. C1 C2 C2 C02 exist t1 C1 , t2 C2 t02 C02t1 t2 t2 t02 . Moreover, without loss generality assume0con . Following proof Halpern etc con, tc1 T1con , tc2 T2con t0c2 T20al. (2004, Lemma 5.8) find ]i-type t1 t1 t01 t01 t02 . Define T10000conset t01 T10con set t0c1 . show C1 = hT1 , T1000consistent ]i-state candidate C1 C1 , C1 C2 , c con, P1 ci P01P01 c P02 .consistent L1m define quasimodel+ establish complete. Let R set -sequences acceptness QKT2m respect QIS pr,syncable n, n N, define f f(r, k) = Ck r -sequenceX, . . . , X, Cn , Cn+1 , . . . acceptable n k n, undefined otherwise. Finally, letset functions associating every (r, n) Dom(f) type (r, n) Tr,nconditions (A) (B) Section 5.1 satisfied(C) (r, n) -type, ]i-type (r, n) t, (r0 , n), (r0 , n) = t.(F) (r, n) (r0 , n) n > 0 (r, n 1) (r0 , n 1).Finally, Ag, O, define (r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ) n = n0 .following remark shows set non-empty. particular, conditions (C)(F) satisfied functions O.Lemma 16. set functions satisfies condition (A), (B), (C) (F)non-empty.Proof. Conditions (A) (B) follow Lemma 6(ii)(a) fact racceptable -sequence. regards (C) (F), assume (r, n) f(r, n) -type,]i-type (r, n) t. f(r, n) different (r, n) consider setU = { | Ki s}. check U consistent extended ]i-type s0s0 . define 0 collection s0 . Further, sc con ,set s0c 0con . Let C0 = hT 0 , 0con i. Clearly, C C0 hC, si ci hC0 , s0 i. Lemma 15construct -sequence C0 . . . Cn Cn = C0 f(r, k) Ckk n. Lemma 8 extend -sequence infinite acceptable -sequencer0 . particular, function extended k n, (r, k) (r0 , k)(r0 , n) = t. Thus, (C) (F) satisfied.show following lemma.Lemma 17. tuple hR, O, {i, }iAg,O , fi frame satisfies perfect recallsynchronicity.31fiBelardinelli & LomuscioProof. Lemmas 6(i), 8 16 sets R non-empty. Also, f satisfiesconditions Definition 17. Finally, i, equivalence relation definition,satisfies perfect recall synchronicity definition functions O.prove main result.Lemma 18. tuple hR, O, {i, }iAg,O , fi quasimodel+ perfect recallsynchronicity, validates formulas K BF .Proof. previous lemma hR, O, {i, }iAg,O , fi frame satisfying perfectrecall synchronicity; left prove functions objects+ .Conditions (1)-(4) objects+ safisfied remarks (A)-(F) definitioni, . Furthermore, conditions (1), (2) (3) quasimodels+ satisfieddefinitions R, f i, . regards condition (4), make use Lemma 15show holds. Additionally, (5) holds Lemma 6(ii)(b), (d), (f) (h) Lemma15. Finally, Q validates formulas K BF , C, C Q,consistent QKTm .completes proof QKT2m . Thus, obtain following item Theorem 2.Theorem 6 (Completeness). system QKT2m complete w.r.t. class QIS pr,syncQIS.follows Remark 2.completeness QKT2m respect QIS pr,sync,uis5.4 Class QIS nlFirst, give following definitions, used completeness proof.Definition 21. -type, t,i conjunction -types t0 t0 .Similarly, P -point, P,i set -points P0 P P0 .Definition 22. Two sequences types 0 -concordant n N(or n may ) non-empty consecutive intervals 1 , . . . , n 01 , . . . , 0n 0j s0 0j s0 j n.Two sequences 0 state candidates -concordant C, eitherC C 0 , two sequences 0 types 0 respectively-concordant.prove completeness QKT3m respect QIS nlneed following lemma,dual Lemma 11.Lemma 19. -points P1 = hC1 , t1 i, P2 = hC2 , t2 ]i-type t01 , P1 P2t1 t01 exists ]i-point P01 = hC01 , t01 -sequence P01 = S1 . . . Sn]i-points Sk = hDk , sk i, sk t1 k < n, t2 sn . Further, P1 c P2sck TDconk n.kProof. adapting result Halpern et al. (2004, Lemma 5.11) typesprove t1 t2 t1 t01 sequence ]i-types t01 = s0 . . . snsk t1 k < n sn t2 . Lemma 7 extend sequence]i-types sequence ]i-points S1 . . . Sn Sk = hDk , sk i. So,32fiInteractions Knowledge Time First-Order Logic MASstatement lemma satisfied. particular, P1 c P2 Lemma 7assume without loss generality sck TDconk n.kpointed Halpern et al. (2004), Lemma 19 sufficient constructquasimodel+ satisfies learning. fact, given -sequence = C0 , C1 , . . . state candidates ]i-type t00 t0 t00 t0 C0 , Lemma 19 find-sequence 0 = C00 , C01 , . . . t00 C00 learning satisfied. However,follow acceptability 0 also acceptable. So, propositionalcase, work trees state candidates. Hereafter extend definitionsgiven Halpern et al. (2004) able deal points monodic friendly Kripkemodels.Definition 23. Let k ad(). k-tree state candidates set -statecandidates || k contains unique -state candidate, i.e., root,every -point C ,t0 ]i-type t0 |]i| k ]i-state candidateC0 t0 C0 ;= 0 ]i 0 -state candidate C0 0 -type t0 C0t0 .Similarly, define k-tree points set -points || kcontains unique -point, every -point P = hC, ti ,t0 ]i-type t0 |]i| k, ]i-point P0 =hC0 , t0 ;= 0 ]i 0 -point P0 = hC0 , t0 t0 .Intuitively, k-tree view epistemic state quasimodel particulartype t, k steps t. extend -suitability relation k-trees.Definition 24. Let 0 k-trees state candidates . say f 0whenever f function associating -state candidate C -type Cfinite -sequences -state candidates 0 -types that:1. f (C) = C0 . . . Ck (a) C = C0 (b) Cj j < k Ck 0 .Similarly, f (t) = t0 . . . tk (a) = t0 (b) tj Cj j < kk Ck .2. Let C t0 C0 C, C0 . t0 f (t) f (t0 ) concordant;3. least one C sequence f (C) length least 2.Further, let 0 k-trees points . say f 0 whenever ffunction associating -point P finite -sequence -points 0that:1. f (P) = P0 . . . Pk (a) P = P0 (b) Pj j < k Pk 0 ;33fiBelardinelli & Lomuscio2. Let P = hC, ti P0 = hC0 , t0 . t0 f (t) f (t0 ) -concordant;3. least one P sequence f (P) length least 2.Finally, constant c con, say cf 0 whenever f 0 f (P) =P0 c . . . c Pk .Notice given k-tree state candidates root C C, obtaink-tree points P0 = hC0 , t0 iff C0 . Also, , 0 k-tree statecandidates f 0 , also f 0 0 k-trees pointsbased 0 respectively.show obtain acceptable sequences state candidates sequencestrees. Given two sequences -state candidates = C0 , . . . , Ck = C00 , . . .,finite, fusion defined C0 , . . . , Ck1 , C00 , . . . Ck = C00 . Further,given infinite sequence = 0 f0 1 f1 . . . k-trees, say sequence-state candidates compatible exists h N -state candidatesCh , Ch+1 , . . ., Cj j j h, = fh (Ch ) fh+1 (Ch+1 ) . . .. sequenceacceptable every -sequence compatible infinite acceptable.basic idea completeness proof define quasimodel+ startingacceptable sequence . Next introduce definitions lemmas essentialcompleteness proof.Given k-tree -point P inductively define formula tree,Pdescribes k-tree viewpoint P.Definition 25. P -point, tree,P ::= P . P 0 ]i-point 0 6= 0 ]i^tree,P = PKi tree,P0{0 point P0 |t0 t}0 k-trees, P P0 0 , write (, P) + (0 , P0 )sequence k-trees 0 , . . . , l functions f0 , . . . , fl1 (a) = 0 f0 . . . fl1l = 0 ; (b) fj (P) = P j l 2 fl1 (P) = (P, P0 ). Similarly, (, P) c+ (0 , P0 )(, P) + (0 , P0 ) (a) = 0 cf0 . . . cfl1 l = 0 .prove following lemma, extends result Halpern et al. (2004, Lemma 5.12)points.Lemma 20. Suppose k-tree points P = hC, ti -point || = k,(a) t0 -type tree,P (t0 ) consistent, k-tree 0-point P0 = hC0 , t0 0 (, P) + (0 , P0 ) tree0 ,P0 consistent.Further, tc con (, P) c+ (0 , P0 ).W(b) ` tree,P {(0 ,P0 )|(,P)+ (0 ,P0 )} tree0 ,P0(c) tree,P U 0 consistent, sequence 0 , . . . , l k-treespoints P0 , . . . , Pl (i) Pj j j l; (ii) (0 , P0 ) = (, P); (iii)(j , Pj ) + (j+1 , Pj+1 ) j < l; (iv) treej ,Pj consistent j < l; (v)treel ,Pl 0 consistent. Further, tc con (iii) (j , Pj ) c+ (j+1 , Pj+1 )j < l.34fiInteractions Knowledge Time First-Order Logic MASProof. proceed induction k. case k = 0 immediate using standardarguments tree,P P .Assume k > 0 = 0 ]i 6= 0 . first prove part (a) = Ki 0 ,part (b), general case (a), finally (c).regards part (a) = Ki 0 , note tree,P (t0 Ki 0 ) impliestree,P Ki P,i UKi ( 0 t0 ,i )definition k-tree 0 -point P . Let(k 1)-tree consisting -points | | k 1. axiom KT3 alsotree ,P Ki P,i UKi ( 0 t0 ,i ) consistent, part (c) sequence 0 , . . . , l(k 1)-trees points P0 , . . . , Pl (i) Pj j j l; (ii) (0 , P0 ) =( , P ); (iii) (j , Pj ) + (j+1 , Pj+1 ) j < l; (iv) treej ,Pj Ki P,i consistentj < l; (v) treel ,Pl Ki ( 0 t0 ,i ) consistent.Again, definition relation + sequence (k 1)-trees 0 , . . . ,functions f0 , . . . , fm1 (a) = 0 = 0 f0 . . . fm1 = l . Moreover,(k 1)-points u0 , . . . , um u0 = P , um = Pl , j < m, uj = Pj 0j 0 j, uj = uj+1 fj (uj ) = uj , uj 6= uj+1 fj (uj ) = (uj , uj+1 ).show define k-tree 0j extending j j < m. (iv)uj Ki P,i consistent j < m, uj P. P 0j . Similarly,um Ki t0 ,i consistent; exists P0 = hC0 , t0 P0 0m . Further,saturate 0j conditions k-trees satisfied particular 00 = .show construct fj0 j < m. point S0 = hD0 , s0 0j \ j mustexist point = hD, si j agent j 0 Ag j 0 s0 . Lemma 19follows exists sequence S0 starting S0 j 0 -concordant fj (S).Moreover, take Pj = (P) j < 1, Pm1 = (P, P0 ). define fj0agrees fj j , S0 0j \ j fj0 (S0 ) = S0 .Notice 00 = construction. > 0 follows immediately definition(, P) + (m , P0 ) treem ,P0 Ki 0 consistent. = 0 easilycheck P0 t0 . Since also t, follows t0 .define f f (u) = u every u 6= P f (P) = (P, P0 ). (, P) f (, P0 ).Since also P P0 (, P) + (, P0 ).second part (a) follows similar line reasoning.prove part (b), contradiction assume_0 tree,Ptree0 ,P0{(0 ,P0 )|(,P)+ (0 ,P0 )}Vtree,P {(0 ,P0 )|(,P)+ (0 ,P0 )} tree0 ,P0 consistent. temporal reasoningmust point u^tree,P (utree0 ,P0 )(33){(0 ,P0 )|(,P)+ (0 ,P0 )}Wconsistent. Note tree0 ,P0 equivalent P0 {0 point P 0 |t t0 } Ki tree0 ,P .Thus, consistency (33) implies tree 0 (, P) + (0 , u)35fiBelardinelli & Lomuscioexists 0 -point P0 = hC0 , t0 t0 tu^tree,P (u Ki (tree0 ,P0 ))(34){0 |(,P)+ (0 ,P0 )}+consistent. part (a)Vthere exists k tree P (, P) ( , P )tree ,P u Ki ( {0 |(,P)+ (0 ,P 0 )} tree0 ,P0 ) consistent. meansP = u. Thus contradiction, since tree ,u Ki tree ,P inconsistent.general case (a) follows (b). Part (c) also follows (b).following lemma correspondent Lemma 8 k-trees.Lemma 21. L1m consistent QKT3m , exists acceptable sequencead()-trees state candidates belongs root first tree.Proof. Lemma 8 key part proof consists showing that, given finitesequence 0 f0 . . . fl1 l d-trees points -point P = hC, ti lU 0 (resp. t), Lemmas 19 20 extend sequence treessatisfy acceptability. Specifically, suppose U 0 t. Let include P 0 -pointsP0 = hC0 , t0 l |0 | k = ||. Note k-tree. Further, Lemma 20find sequence 0 , . . . , n k-trees points P0 , . . . , Pn (i) Pj jj n; (ii) (0 , P0 ) = (, P); (iii) (j , Pj ) + (j+1 , Pj+1 ) j < l; (iv) treej ,Pjconsistent j < l; (v) treen ,Pl 0 consistent. using Lemma 19extend sequence ad()-trees starting l satisfies U 0 proofLemma 20(a). argument similar. Since consistent, musttree root C C; extendcomplete proof.consistent L1m define quasimodel+ establish completenessQKT3m respect QIS nl. Let R consist acceptable -sequences compatiblead()-tree , function f given f(r, k) = Ck r acceptable-sequence C0 , C1 , . . .. Further, let set functions associating every (r, n)Dom(f) type (r, n) Tr,n conditions (A), (B) (C) given previouslysatisfied following holds:(G) (r, n) (r0 , n0 ) either (r, n + 1) (r0 , n0 ) exists k > n0(r, n + 1) (r0 , k) k 0 , k > k 0 n0 implies (r, n) (r0 , k 0 ).Finally, Ag, O, (r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ).previous cases following.Lemma 22. set functions satisfies conditions (A), (B), (C) (G)non-empty.Proof. Conditions (A) (B) guaranteed Lemma 6(ii) fact racceptable -sequence respectively. regards (C) (G), assume (r, n)-type, ]i-type (r, n) t. using proofs Lemmas 20 19find acceptable -sequence r0 compatible d-tree f(r0 , 0)(G) satisfied.show following lemma.36fiInteractions Knowledge Time First-Order Logic MASLemma 23. tuple hR, O, {i, }iAg,O , fi frame satisfies learning.Proof. Lemmas 6(i), 21 22 sets R non-empty. Also, f satisfiesconditions Definition 17. Further, i, equivalence relation definition.Finally, learning condition satisfied definition functions O.Lemma 24. tuple hR, O, {i, }iAg,O , fi quasimodel+ satisfieslearning validates formulas K BF .Proof. previous lemma hR, O, {i, }iAg,O , fi frame satisfying learning;left prove functions objects+ . Conditions (1)-(4) objects+safisfied remarks (A)-(G) definition i, . Furthermore, conditions (1), (2)(3) quasimodels+ satisfied definitions R, f i, . regards (4)use Lemma 19 show holds. Finally, (5) holds Lemma 6(ii)(b), (d), (f)(h) Lemma 19. Finally, Q validates K BF , C, C Q,consistent QKTm .completes proof QKT3m . Thus, obtain following item Theorem 2.Theorem 7 (Completeness). system QKT3m complete w.r.t. class QIS nlQIS.5.5 Class QIS nl,syncshow QKT4m complete axiomatisation QIS nl,sync, analogously Lemma 15,need following.Lemma 25. -state candidate C1 , C2 ]i-state candidate C01 exists ]i-statecandidate C02C1 C2 C1 C01 C01 C02 C2 C02 .c con, P1 = hC1 , t1 i, P2 = hC2 , t2 P01 = hC01 , t01 i, P1 c P2P1 ci P01 P01 c P02 P2 ci P02 .Proof. proof similar Lemma 15. C1 C2 C1 C01 existt1 C1 , t2 C2 t01 C01 t1 t2 t1 t01 . Moreover, without loss0con .generality, assume c con, tc1 T1con , tc2 T2con t0c1 T1adapting proof Halpern et al. (2004, Lemma 5.18) find ]i-type t02t2 t02 t01 t02 . define T20 set t02 T10con set t0c2.Clearly, C02 = hT20 , T20con consistent ]i-state candidate C2 C02 , C01 C02 ,c con, P2 ci P02 P01 c P02 .systems including axiom KT4m define synchronous version relationk-trees.Definition 26. 0 k-trees state candidates sync0 ifff0f C , f (C) exactly length 2. Similarly, 0k-trees points sync0 iff f 0 P , f (P) exactlyflength 2.37fiBelardinelli & Lomuscioc con, relation cf sync defined similarly. define sync-acceptablesequence trees acceptable sequence relation substitutedrelation sync , is, sequence acceptable every sync -sequence compatibleinfinite acceptable. Similarly, given relations + c+ c con,definitions sync,+ c sync,+ straightforward. state following result,simplified version Lemma 20. proof analogous Lemma 20,Lemma 25 used instead Lemma 19.Lemma 26. Let k-tree points P -point || = k,(a) t0 -type tree,P (t0 ) consistent, exists k-tree 0-point P0 = hC0 , t0 0 (, P) sync,+ (0 , P0 ) tree0 ,P0consistent. Further, tc con (, P) c sync,+ (0 , P0 ).W(b) ` tree,P {(0 ,P0 )|(,P)sync,+ (0 ,P0 )} tree0 ,P0(c) tree,P U 0 consistent, exists sequence 0 , . . . , l k-treespoints P0 , . . . , Pl (i) Pj j j l; (ii) (0 , P0 ) = (, P); (iii)(j , Pj ) sync,+ (j+1 , Pj+1 ) j < l; (iv) treej ,Pj consistent j < l;(v) treel ,Pl 0 consistent. Further, tc con (iii) (j , Pj ) c sync,+(j+1 , Pj+1 ) j < l.Further, make use Lemma 26 adapt Lemma 21 obtain following result.Lemma 27. L1m consistent QKT4m , exists sync-acceptablesequence ad()-trees state candidates belongs root first tree.consistent L1m define quasimodel+ establish completeness QKT4m respect QIS nl,sync. Let X new object, sequenceX, . . . , X, Cn , Cn+1 , . . . sync-acceptable n starts n copies X Cn , Cn+1 , . . .sync-acceptable -sequence compatible ad()-tree . Let R consist sequences sync-acceptable n, n N. function f defined f(r, k) = Ckr -sequence X, . . . , X, Cn , Cn+1 , . . . sync-acceptable n k n; f(r, k) undefined otherwise. Further, Let set functions associating every (r, n) Dom(f)type (r, n) Tr,n conditions (A), (B) (C) satisfied followingholds:(H) (r, n) (r0 , n0 ) (r, n + 1) (r0 , n0 + 1).Finally, Ag, O, (r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ) n = n0 .Similarly Lemma 22, show following.Lemma 28. set functions satisfies conditions (A), (B), (C) (H)non-empty.Moreover, following result follows Lemmas 6(i), 27 28.Lemma 29. tuple hR, O, {i, }iAg,O , fi frame satisfies learningsynchronicity.38fiInteractions Knowledge Time First-Order Logic MASFinally, adapting proof Lemma 24 state following result.Lemma 30. tuple hR, O, {i, }iAg,O , fi quasimodel+ learningsynchronicity, validates formulas K BF .completes proof QKT4m . Thus, obtain following item Theorem 2.Theorem 8 (Completeness). system QKT4m complete w.r.t. class QIS nl,syncQIS.5.6 Classes QIS nl,prQIS nl,pr,uis1obtain completeness proof QIS nl,prcombine results shown QIS prnlQIS .2,3L1m consistent QKTmLemma 21 exists acceptablesequence ad()-trees belongs root first tree. Let Rset acceptable -sequences suffix compatible ,function f defined Section 5.2. Further, set functions associatingevery (r, n) Dom(f) type (r, n) Tr,n satisfies conditions (A), (B), (C),(E) (G). Finally, Ag, O, (r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ).Lemma 31. set functions satisfies conditions (A), (B), (C), (E) (G)non-empty.Proof. show conditions (C) satisfied similarly casesnlQIS prQIS . (C), suppose (r, n) -type f(r, n) ]i-type.Also, sequence ad()-trees 0 f0 1 f1 . . . state candidates. run rderived definition -sequence C0 , C1 , . . . suffix CN , CN +1 , . . .compatible , f(r, n) = Cn . consider two cases.n N , exists k N Cn k . Lemma 11 exists-sequence S0 . . . Sh ]i-state candidates Sh S0 , . . . , Sh-concordant C0 , . . . , Cn . Further, assume Sh k let Sh , Sh+1 , . . .sequence compatible . consider -sequence S0 S1 . . ..construction run r0 derived sequence R assume(r0 , h) = t.n < N , Lemma 11 exists -sequence S0 . . . Sh ]i-statecandidates Sh S0 , . . . , Sh -concordant C0 , . . . , Cn . Lemma 19extend sequence -sequence S0 . . . Sk -concordantC0 , . . . , CN . Since CN N, assume Skwell. Let Sh , Sh+1 , . . . sequence compatible , consider -sequenceS0 S1 . . .. previous case, run r0 derived sequence Rconstruction assume (r0 , h) = t.Lemmas 6(i), 21 31 obtain next result.Lemma 32. tuple hR, O, {i, }iAg,O , fi frame satisfies perfect recalllearning.Finally, state following lemma, whose proof follows lines correspondingnlproofs QIS prQIS Lemma 31.39fiBelardinelli & LomuscioLemma 33. tuple hR, O, {i, }iAg,O , fi quasimodel+ satisfies perfectrecall learning, validates formulas K BF .establishes completeness QKT2,3 . Thus, obtain following itemTheorem 2.nl,prTheorem 9 (Completeness). system QKT2,3complete w.r.t. class QISQIS.completeness QKT2,3respect QIS nl,pr,uisfollows following11remark, whose proof analogous propositional case.Remark 3. formula L11 satisfiable QIS nl,pr(resp. QIS nl,pr,sync) iff11nl,pr,uisnl,pr,sync,uissatisfiable QIS 1(resp. QIS 1).5.7 Class QIS nl,pr,sync1,4prove completeness QKTmrespect QIS nl,pr,synccombine resultsnl,probtained QIS previous section QIS nl,syncQIS pr,sync.1,41Specifically, Lm consistent QKTm Lemma 27 construct syncacceptable sequence ad()-trees belongs root first tree. Let Rset sync-acceptable -sequences suffixes compatible ;function f defined Section 5.2. Further, set functions associatingevery (r, n) Dom(f) type (r, n) Tr,n satisfies conditions (A), (B), (C),(F) (H). Finally, Ag, O, (r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ) n = n0 .adapting proof Lemma 31 means Lemmas 15 25 showfollowing result.Lemma 34. set functions satisfies conditions (A), (B), (C), (F) (H)non-empty.Lemmas 6(i), 27 34 obtain following result.Lemma 35. tuple hR, O, {i, }iAg,O , fi frame satisfies perfect recall,learning synchronicity.Finally, state following lemma whose proof follows lines correspondingproofs QIS pr,sync, QIS nl,syncLemma 34.Lemma 36. tuple hR, O, {i, }iAg,O , fi quasimodel+ satisfies perfectrecall, learning synchronicity, validates formulas K BF .completes proof QKT1,4. Thus, obtain following item Theorem 2.1,4Theorem 10 (Completeness). system QKTmcomplete w.r.t. class QIS nl,pr,syncQIS.40fiInteractions Knowledge Time First-Order Logic MAS5.8 Classes QIS nl,sync,uisQIS nl,pr,sync,uis1,4,5show system QKTmcomplete respect classes QIS nl,sync,uisnl,pr,sync,uisQIS. completeness result follows next remark.Remark 4. formula Lm valid QIS nl,sync,uisiff valid QIS nl,pr,sync,uis.proof straightforward extension first-order result Halpern et al. (2004,Proposition 5.22). Given remark axiom KT5 sufficient provecompleteness QKT1,4respect QIS nl,pr,sync,uis. result previous111,4nl,pr,syncsection, QKT1 indeed complete respect QIS 1. desired result followsRemark 3. Thus, obtain following item Theorem 2.1,4Theorem 11 (Completeness). system QKTmcomplete w.r.t. classes QIS nl,sync,uisQIS nl,pr,sync,uisQIS.6. Conclusions Workpaper investigated interaction axioms context monodic first-ordertemporal-epistemic logic. Specifically, explored classes quantified interpreted systemssatisfying conditions synchronicity, learning, perfect recall, uniqueinitial state. contribution article concerns provably complete axiomatisationclasses.results presented extend previous contributions first-order epistemic temporal logic interactions (e.g., see Belardinelli & Lomuscio, 2011, Sturm et al., 2000,Wolter & Zakharyaschev, 2002), direction previously exploredpropositional level (Halpern et al., 2004). findings show characterisationaxioms considered propositional level extended first-order monodicsetting.temporal-epistemic logic first-order context far mostly attracted theoretical contributions, evidence literature increasingly embracedapplications. instance, active interest verifying artifact-centric systemsfirst-order modal specifications (Belardinelli, Lomuscio, & Patrizi, 2011a, 2011b;Deutsch, Hull, Patrizi, & Vianu, 2009; Deutsch, Sui, & Vianu, 2007; Calvanese, Giacomo,Lenzerini, & Rosati, 2012; Hariri, Calvanese, Giacomo, Masellis, & Felli, 2011).Given this, remains importance investigate questions pertaining computational aspects formalisms introduced, including decidability computational complexity satisfiability model checking problems. Work far (includingBelardinelli & Lomuscio, 2011; Hodkinson al., 2000; Wolter & Zakharyaschev 2001)focused fragments interaction present, know literature (Halpern et al., 2004) interactions make problems harder. leavework, particularly connection addition epistemic modalities(e.g., explicit algorithmic knowledge, see Halpern & Pucella, 2005), branching-timemodalities. Epistemic variants branching-time CTL well understood propositional level (Meyden & Wong, 2003) first-order extensions yetexplored.41fiBelardinelli & LomuscioAcknowledgmentsresearch presented supported European Commission MarieCurie Fellowship FoMMAS (grant n. 235329) STREP Project ACSI (grantn. 257593), UK Engineering Physical Sciences Research Council LeadershipFellowship Trusted Autonomous Systems (grant n. EP/I00520X/1).would like thank anonymous reviewers Mr. Andrew V. Jones valuablecomments paper.ReferencesBelardinelli, F., & Lomuscio, A. (2009). Quantified epistemic logics reasoningknowledge multi-agent systems. Artificial Intelligence, 173 (9-10), 9821013.Belardinelli, F., & Lomuscio, A. (2011). First-order linear-time epistemic logic groupknowledge: axiomatisation monodic fragment. Fundamenta Informaticae,106 (2-4), 17590.Belardinelli, F., & Lomuscio, A. (2008). complete quantified epistemic logic reasoningmessage passing systems. Computational Logic Multi-Agent Systems, 8thInternational Workshop, CLIMA VIII. Revised Selected Invited Papers, Vol. 5056Lecture Notes Computer Science, pp. 248267. Springer.Belardinelli, F., & Lomuscio, A. (2010). Interactions time knowledge firstorder logic multi-agent systems. Principles Knowledge RepresentationReasoning: Proceedings 12th International Conference, KR 2010. AAAI Press.Belardinelli, F., Lomuscio, A., & Patrizi, F. (2011a). computationally-grounded semantics artifact-centric systems abstraction results. Proceedings 22ndInternational Joint Conference Artificial Intelligence, IJCAI 2011, pp. 738743.AAAI Press.Belardinelli, F., Lomuscio, A., & Patrizi, F. (2011b). Verification deployed artifact systems via data abstraction. Service-Oriented Computing: Proceedings 9thInternational Conference, ICSOC 2011, Vol. 7084 Lecture Notes Computer Science, pp. 142156. Springer.Calvanese, D., Giacomo, G. D., Lenzerini, M., & Rosati, R. (2012). View-based queryanswering description logics: Semantics complexity. Journal ComputerSystem Sciences, 78 (1), 2646.Cohen, P., & Levesque, H. (1995). Communicative actions artificial agents. Proceedings 1st International Conference Multi-Agent Systems, ICMAS 1995, pp.6572. AAAI Press.Degtyarev, A., Fisher, M., & Konev, B. (2003). Monodic temporal resolution. AutomatedDeduction: Proceedings 19th International Conference Automated Deduction,CADE-19, Vol. 2741 Lecture Notes Computer Science, pp. 397411. Springer.Degtyarev, A., Fisher, M., & Lisitsa, A. (2002). Equality monodic first-order temporallogic. Studia Logica, 72 (2), 147156.Dennett, D. (1987). Intentional Stance. MIT Press.42fiInteractions Knowledge Time First-Order Logic MASDeutsch, A., Hull, R., Patrizi, F., & Vianu, V. (2009). Automatic verification datacentric business processes. Database Theory: Proceedings 12th InternationalConference, ICDT 2009, Vol. 361 ACM International Conference Proceeding Series,pp. 252267. ACM Press.Deutsch, A., Sui, L., & Vianu, V. (2007). Specification verification data-driven webapplications. Journal Computer System Sciences, 73 (3), 442474.Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning Knowledge.MIT Press.Fagin, R., Halpern, J. Y., & Vardi, M. Y. (1992). machines know?properties knowledge distributed systems. Journal ACM, 39 (2), 328376.Gabbay, D., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2003). Many-Dimensional ModalLogics: Theory Applications, Vol. 148 Studies Logic. Elsevier.Garson, J. (2001). Quantification modal logic. Gabbay, D., & Guenthner, F. (Eds.),Handbook Philosophical Logic, Vol. 3, pp. 267323. Reidel.Halpern, J., & Moses, Y. (1992). guide completeness complexity modal logicsknowledge belief. Artificial Intelligence, 54, 319379.Halpern, J., van der Meyden, R., & Vardi, M. (2004). Complete axiomatizations reasoning knowledge time. SIAM Journal Computing, 33 (3), 674703.Halpern, J., & Vardi, M. (1986). complexity reasoning knowledge time.ACM Symposium Theory Computing, STOC 1986, pp. 304315. ACM Press.Halpern, J., & Vardi, M. (1989). complexity reasoning knowledge time1: lower bounds. Journal Computer System Sciences, 38 (1), 195237.Halpern, J., & Pucella, R. (2005). Probabilistic algorithmic knowledge. Logical MethodsComputer Science, 1 (3).Hariri, B. B., Calvanese, D., Giacomo, G. D., Masellis, R. D., & Felli, P. (2011). Foundationsrelational artifacts verification. Business Process Management: Proceedings9th International Conference, BPM 2011, Vol. 6896 Lecture Notes ComputerScience, pp. 379395. Springer.Hodkinson, I. (2002). Monodic packed fragment equality decidable. Studia Logica,72, 185197.Hodkinson, I. (2006). Complexity monodic guarded fragments linear real time.Annals Pure Applied Logic, 138, 94125.Hodkinson, I., Kontchakov, R., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2003).computational complexity decidable fragments first-order linear temporal logics.Proceedings 10th International Symposium Temporal RepresentationReasoning / 4th International Conference Temporal Logic, TIME-ICTL 2003, pp.9198. IEEE Computer Society Press.Hodkinson, I., Wolter, F., & Zakharyaschev, M. (2000). Decidable fragment first-ordertemporal logics. Annals Pure Applied Logic, 106 (1-3), 85134.43fiBelardinelli & LomuscioHodkinson, I., Wolter, F., & Zakharyaschev, M. (2002). Decidable undecidable fragments first-order branching temporal logics. Proceedings 17th IEEE Symposium Logic Computer Science, LICS 2002, pp. 393402. IEEE ComputerSociety Press.Lomuscio, A., & Ryan, M. (1998). relation interpreted systems Kripkemodels. Agent Multi-Agent Systems: Proceedings AI97 Workshoptheoretical practical foundations intelligent agents agent-oriented systems,Vol. 1441 Lecture Notes Artificial Intelligence, pp. 4659. Springer.McCarthy, J. (1979). Ascribing mental qualities machines. Ringle, M. (Ed.), Philosophical Perspectives Artificial Intelligence, pp. 161195. Harvester Press.McCarthy, J. (1990). Artificial intelligence, logic formalizing common sense. Thomason, R. (Ed.), Philosophical Logic Artificial Intelligence, pp. 161190. KluwerAcademic.Meyden, R. (1994). Axioms knowledge time distributed systems perfectrecall. Proceedings 9th Annual IEEE Symposium Logic ComputerScience, LICS 1994, pp. 448457. IEEE Computer Society Press.Meyden, R. v., & Wong, K. (2003). Complete axiomatizations reasoning knowledgebranching time. Studia Logica, 75 (1), 93123.Moore, R. C. (1990). formal theory knowledge action. Allen, J., Hendler, J., &Tate, A. (Eds.), Readings Planning, pp. 480519. Kaufmann.Parikh, R., & Ramanujam, R. (1985). Distributed processes logic knowledge.Logics Programs, Conference Proceedings, Vol. 193 Lecture Notes ComputerScience, pp. 256268. Springer.Pnueli, A. (1977). temporal logic programs. Proceedings 18th InternationalSymposium Foundations Computer Science, FOCS 1977, pp. 4657.Rao, A., & Georgeff, M. (1991). Deliberation role formation intentions.Proceedings 7th Conference Uncertainty Artificial Intelligence, pp.300307. Kaufmann.Sturm, H., Wolter, F., & Zakharyaschev, M. (2000). Monodic epistemic predicate logic.Logics Artificial Intelligence, European Workshop, JELIA 2000, Vol. 1919Lecture Notes Computer Science, pp. 329344. Springer.Sturm, H., Wolter, F., & Zakharyaschev, M. (2002). Common knowledge quantification.Economic Theory, 19, 157186.Wolter, F., & Zakharyaschev, M. (2001). Decidable fragments first-order modal logics.Journal Symbolic Logic, 66 (3), 14151438.Wolter, F., & Zakharyaschev, M. (2002). Axiomatizing monodic fragment first-ordertemporal logic. Annals Pure Applies Logic, 118 (1-2), 133145.Wooldridge, M. (2000a). Computationally grounded theories agency. ProceedingsInternational Conference Multi-Agent Systems, ICMAS 2000, pp. 1322. IEEEComputer Society Press.44fiInteractions Knowledge Time First-Order Logic MASWooldridge, M. (2000b). Reasoning Rational Agents. MIT Press.Wooldridge, M., & Fisher, M. (1992). first-order branching time logic multi-agentsystems. Proceedings 10th European Conference Artificial Intelligence,ECAI 1992, pp. 234238. John Wiley Sons.Wooldridge, M., Fisher, M., Huget, M., & Parsons, S. (2002). Model checking multi-agentsystems MABLE. Proceedings 1st International Conference Autonomous Agents Multiagent Systems, AAMAS 2002, pp. 952959. ACM Press.Wooldridge, M., Huget, M., Fisher, M., & Parsons, S. (2006). Model checking multiagentsystems: MABLE language applications. International Journal ArtificialIntelligence Tools, 15 (2), 195226.Wooldridge, M. (1999). Verifying agents implement communication language.Proceedings 16th National Conference Artificial Intelligence 11th Conference Innovative Applications Artificial Intelligence, pp. 5257. AAAI Press.45fiJournal Artificial Intelligence Research 45 (2012) 761-780Submitted 08/12; published 12/12Evaluating Indirect Strategies ChineseSpanishStatistical Machine TranslationMarta R. Costa-jussavismrc@i2r.a-star.edu.sgInstitute Infocomm Research,Singapore 138632Carlos A. Henrquez Q.carlos.henriquez@upc.eduUniversitat Politecnica de Catalunya,08034 BarcelonaRafael E. Banchsrembanchs@i2r.a-star.edu.sgInstitute Infocomm Research,Singapore 138632AbstractAlthough, Chinese Spanish two spoken languages world,much research done machine translation language pair.paper focuses investigating state-of-the-art Chinese-to-Spanish statistical machinetranslation (Smt), nowadays one popular approaches machinetranslation. purpose, report details available parallel corpusBasic Traveller Expressions Corpus (Btec), Holy Bible United Nations (Un).Additionally, conduct experimental work largest three corporaexplore alternative Smt strategies means using pivot language. Three alternativesconsidered pivoting: cascading, pseudo-corpus triangulation. pivot language,use either English, Arabic French. Results show that, phrase-based Smt system,English best pivot language Chinese Spanish. propose systemoutput combination using pivot strategies capable outperforming directtranslation strategy. main objective work motivating involvingresearch community work important pair languages given demographicimpact.1. IntroductionChinese Spanish distant languages many aspects. However, come closetogether ranking spoken languages world (Ethnologue, 2012).Web 2.0 era, content produced users, number nativespeakers excellent indicator actual relevance machine translation twolanguages. course, factors literacy, amount text published strengthcommercial relationships also taken account, factors actuallysupport idea strategic importance developing machine translationtechnologies Chinese Spanish. huge increase volume online contentsChinese last years, well steady increase commercial relationshipsSpanish speaking Latin American countries China two basic examplessupporting fact. Needless say, languages involve many economical interestsc2012AI Access Foundation. rights reserved.fiCosta-jussa, Henrquez & Banchs(Zapatero, 2010). Nevertheless, two languages seem become far apartlooking bilingual resources.recently interested gathering collecting ChineseSpanish bilingualresources research machine translation application purposes. amount bilingual resources currently available specific language pair surprisingly low.Similarly, related amount work found, within computational linguisticcommunity, reduced small set references (Banchs, Crego, Lambert, &Marino, 2006; Banchs & Li, 2008; Bertoldi, Cattoni, Federico, & Barbaiani, 2008; Wang,Wu, Hu, Liu, Li, Ren, & Niu, 2008). Apart Btec1 corpus available International Workshop Spoken Language Translation (Iwslt) competition (Bertoldi et al.,2008) Holy Bible datasets (Banchs & Li, 2008), aware ChineseSpanish parallel corpus suitable training phrase-based (Koehn, Och, & Marcu, 2003)2statistical machine translation systems two languages, six-languageparallel corpus (including Chinese Spanish) United Nations releasedresearch purposes (Rafalovitch & Dale, 2009).Using recently released United Nations parallel corpus starting point, workfocuses problem developing Chinese-to-Spanish phrase-based machine translationtechnologies limited set bilingual resources. explore evaluate differentalternatives problem hand means pivot-language strategieslanguages available United Nations parallel corpus, Arabic, EnglishFrench 3 . Existing strategies system cascading, pseudo-corpus generation triangulation implemented compared baseline system built directtranslation approach. follows, briefly describe pivot approaches:cascaded approach generates Chinese-to-Spanish translations concatenatingsystem translates Chinese pivot language system translatespivot language Spanish.pseudo-corpus approach builds synthetic ChineseSpanish corpus eithertranslating Spanish pivot side Chinesepivot corpus translatingChinese pivot side PivotSpanish corpus.triangulation approach implements Chinese-to-Spanish translation systemcombining translation table probabilities Chinesepivot system PivotSpanish system.Additionally, implement evaluate system combination three pivot strategies based minimum Bayes risk (Mbr) (Kumar & Byrne, 2004) technique.combination strategy capable outperforming direct system.Besides experimenting different pivot languages compare mentioned approaches, also wanted determine pivot alone gives best results why.1. Basic Traveller Expressions Corpus.2. Note phrase-based commonly used refer statistical machine translation systems,term phrase refers segments one one word usual meaningmulti-word syntactical consitutent, linguistics.3. Although Russian available Un corpus, discard use properpreprocessing tools it.762fiEvaluating Indirect Strategies ChineseSpanish SMTHence, present short comparison amount reordering vocabulary sizespivot languages, following study presented Birch et al. (2008) identifiedtwo properties key elements predicting machine translation quality. resultscomparisons, together translation quality obtained different approaches, show English best pivot language Chinese-to-Spanish translationpurposes experimental framework.paper structured follows. Section 2 motivates work intendedbring light investigation Chinese-to-Spanish translation task. Section 3presents related work Chinese-to-Spanish translation task. Section 4 reportsdetails main parallel corpora available translation task. Next, section5 describes main strategies performing Chinese-to-Spanish translationtested work: direct, cascade, pseudo-corpus triangulation. Section 6 presentsevaluation framework includes corpus statistics, system evaluationdetails. Then, section 7 reports experiments (including system combination)results. Finally, section 8 concludes work proposes new research directionsarea.2. MotivationAlthough current web translation systems allow performing translationsChinese Spanish, quality current Chinese-to-Spanish translations still wellquality achieved language pairs, English Spanish. farknow, much research translation task. main reason maylack parallel corpora. study intends make progress involve researchersarea ChineseSpanish statistical machine translation by:1. Listing available parallel corpora ChineseSpanish.2. Comparing different methodologies performing statistical machine translation: cascaded (Wang et al., 2008), pseudo-corpus generation (Banchs et al., 2006; de Gispert& Marino, 2006) triangulation (Wu & Wang, 2007).3. Evaluating best language (among Arabic, English French) generating cascade, pseudo-corpus triangulation Mt ChineseSpanish.4. Performing output system combination explore new ways improving Chineseto-Spanish translation.3. Related WorkOne first works dealing ChineseSpanish statistical machine translationpresented Banchs et al. (2006). Authors experimented two independent corporaChineseEnglish EnglishSpanish translate Chinese Spanish. builttranslation systems using so-called Ngram-based approach, differsphrase-based system mainly translation reordering model (Marino, Banchs,Crego, de Gispert, Lambert, Fonollosa, & Costa-jussa, 2006).763fiCosta-jussa, Henrquez & Banchsresearch event recently performed language pair 2008 Iwsltevaluation campaign (Paul, 2008). evaluation organized two Chinese-to-Spanish tracks.One focused direct translation one pivot translationEnglish. best translation results accordingly manual evaluation obtainedfar pivot task.best systems tracks developed Wang et al. (2008). Regardingdirect system, used standard phrase-based Smt system. makes differentparticipating systems provide Chinese segmentationLdc (Linguistic Data Consortium) bilingual dictionary. Regarding pivot task,compared two different approaches. first one, referred triangulation, consistedtraining two translation models ChineseEnglish corpus EnglishSpanish corpus,building new translation model ChineseSpanish translation combiningtwo previous models proposed Wu & Wang (2007); second one obtained betterresults based cascaded approach. idea translate ChineseEnglish English Spanish, means performing two translations.participants also proposed cascaded methodology. approximationdone n-best translations (Khalilov, Costa-Jussa, Henrquez, Fonollosa, Hernandez,Marino, Banchs, Chen, Zhang, Aw, & Li, 2008).Another proposal generate pseudo-corpus means translate eitherEnglish Chinese Spanish, creating parallel ChineseSpanish corpus.pseudo-corpus used train ChineseSpanish translation (Bertoldi et al., 2008).mentioned aboved, comparison performed Wang et al. (2008) showedcascaded approach performed better phrase-table combination ChineseSpanish pivot task.Finally, previous work (Costa-jussa, Henrquez, & Banchs, 2011b) compared twostandard pivot approaches (pseudo-corpus cascaded) using English direct system. Experiments work showed quality direct systempivot systems differ much. Additionally, cascaded system presented slightlybetter results pseudo-corpus system. previous work (Costa-jussa,Henrquez, & Banchs, 2011a), compared two pivot approaches (pseudo-corpuscascaded) using Arabic, French English pivot languages direct system.concluded English best pivot language.present work, extending two previous studies by: (1) using pivotstrategies (including triangulation strategy); (2) introducing measure pre-evaluatequality pivot approaches; (3) extending pivot combination experiments; (4)providing evaluation.Note working United Nations (Un) corpus ratherBtec corpus (the one used Iwslt). former freely available largerlatter.4. ChineseSpanish Parallel Corporalimited resources language pair ChineseSpanish comparisonnumber native speakers languages. practice, also common translateChinese Spanish English even manual translations conducted.764fiEvaluating Indirect Strategies ChineseSpanish SMTparallel corpus sentence level, Basic Travel Expressions Corpus(Btec) (Paul, Yamamoto, Sumita, & Nakamura, 2009), collection sentencesbilingual travel experts consider useful people going coming anothercountry. corpus contains around 160,000 parallel sentences around 20,000sentences 180,000 words actively used Mt purposes Iwslt evaluationcampaign. full corpus freely available, 20,000 version availableparticipation purposes 2008 Iwslt evaluation campaign.Another parallel corpus Holy Bible, proved good resourceCLIR (Cross-language information retrieval) (Chew, Verzi, Bauer, & McClain, 2006).corpus contains around 28,000 parallel sentences around 800,000 tokens per language.main advantages using corpus worlds translated book;covers variety literary styles including narrative, poetry, correspondence; great caretaken translations; and, perhaps surprisingly, vocabulary appearshigh rate coverage (as much 85%) modern-day language.Finally, United Nations multilanguage corpus (Rafalovitch & Dale, 2009),freely available online research purposes. Among others, contains paralleltexts sentence level following languages: Chinese, English, Spanish, FrenchArabic. consists 2100 United Nations General Assembly resolutions translationsix official languages United Nations, average around 3 million tokens perlanguage. material using work. Table 1 shows statisticsthree different corpora corresponding languages.CorpusBtecHolyBibleUnLang.ChineseEnglishSpanishChineseEnglishSpanishChineseEnglishSpanishArabicFrenchSent.2020203030306060606060Words1641821478149088361,7502,0802,3802,7202,380Vocab.88171312271815201718Avg. sent. length6792629272834394439Table 1: Available corpora ChineseSpanish (all figures given thousands, exceptaverage sentence length)Additionally, surf web find several publications availableChinese Spanish e.g. Global Asia Magazine (2012), additional material consists mainly comparable corpora rather parallel corpora. comparable materialcannot directly used statistical machine translation system. However,many nice algorithms extract parallel corpora comparable corpora (Moore,2002; Senrich, 2010; Abdul-Rauf, Fishel, Lambert, Noubours, & Sennrich, 2012).765fiCosta-jussa, Henrquez & Banchs5. Direct Pivot Statistical Machine Translation Approachesseveral strategies follow translating pair languagesstatistical machine translation (Smt). section present details onesusing work.general, statistical machine translation system relies translation sourcelanguage sentence target language sentence t. Among possible target languagesentences choose one highest probability, show equation (2):= arg max [P (t|s)](1)= arg max [P (t) P (s|t)](2)probability decomposition based Bayes theorem known source-channelapproach statistical machine translation (Brown, Cocke, Della Pietra, Della Pietra, Jelinek, Lafferty, Mercer, & Roossin, 1990). allows model independently targetlanguage model P (t) source translation model P (s|t). one hand, translation model weights likely words foreign language translation wordssource language; language model, hand, measures fluency hypothesist. search process represented arg max operation.Later on, variation proposed Och & Ney (2002) named log-linear model.allows using two models features weight independentlyseen equation (3):= arg max"X#hm (s, t)(3)m=1equation interpreted maximum-entropy framework. see eq.(2) special case eq. (3). fact, logarithm (2) would similar(3). Then, identify h1 (s, t) log(p(t)) h2 (s, t) log(p(s|t)),taking = 2 (two models) 1 = 2 = 1. general case, obtainedmaximizing objective function held-out set (development set).Among additional features used log-linear model lexicalmodels, word bonus, reordering model. lexical models particularly usefulcases translation model may sparse. example, phrases mayappeared times translation model probability may well estimated. Then,lexical models provide probability among words (Koehn et al., 2003). word bonusused compensate language model benefits shorter outputs. reorderingmodel used provide reordering phrases. not, reordering wouldtreated internally phrase. Finally, mentioned name log-linearclearly misnomer many features logarithms all.regards reordering model, standard way implementing distancebased model gives linear cost depending reordering distance. instance,consecutive target words t1 , t2 come translating source words s1 s5 ,sub-scripts indicate word position corresponding sentences, movement766fiEvaluating Indirect Strategies ChineseSpanish SMTFigure 1: Word alignment two sentences= 5 1 = 4 words taken place cost double movement= 2 words. visual representation phrases seen Figure 1Besides traditional distance-based reordering mentioned before, state-of-the-art systems implement additional lexicalized reordering model (Tillman, 2004). lexicalizedreordering model classifies phrases movement make relative previous usedphrase, i.e., phrase model learns likely followed previous phrase(monotone), swapped (swap) connected (discontinuous). instance,considering sub-scripts word positions corresponding sentences, Figure1 bilingual phrases (s1 t1 ) (s5 t2 ) connected, (s7 t6 ) followed(s6 t5 ) (s2 t4 ) swapped (s3 s4 t3 ).5.1 Direct Systemdirect system uses phrase-based translation approach (Koehn et al., 2003).basic idea segment given source sentence segments one words,source segment translated using bilingual phrase obtained trainingcorpus finally compose target sentence phrase translations. bilingualphrase pair source words n target words extracted parallel sentencebelongs bilingual corpus previously aligned words. extraction, considerwords consecutive source target sides consistentword alignment. consider phrase consistent word alignmentword inside phrase aligned one word outside phrase.Regarding segmentation sentence K phrases, assume possiblesegmentations (which considered hidden variable ) probability(t):P (s|t) =XP (s, |t)(4)P (M |t)P (s|t)(5)=X= (t)XP (s|t)(6)Then, consider monotone translations phrase sk produced tk (Zens,Och, & Ney, 2002).P (s|t) =Kk=1767p(sk , tk )(7)fiCosta-jussa, Henrquez & BanchsFinally, phrase translation probabilities estimated relative frequenciesbilingual phrases corpus.p (s|t) =N (s, t)N (t)(8)N (s, t) counts number times phrase translated N (t)number times phrase target language appears training corpus.5.2 Pivot-Based Systemscascaded approach handles sourcepivot pivottarget system independently. built tuned improve local translation qualitycomposed translate source language target language two steps: first,translation output source pivot computed used obtaintarget translation output.pseudo-corpus approach translates pivot section sourcepivot parallel corpus target language using pivottarget system built previously. Then,sourcetarget Smt system built using source side translated pivot sidesourcepivot corpus. pseudo-corpus system tuned using original sourcetargetdevelopment corpus, since available.triangulation approach combines sourcepivot (P (s|p) P (p|s)) pivottarget (P (p|t) P (t|p)) relative frequencies following strategy proposed Cohn &Lapata (2007) order build sourcetarget translation model. translation probabilities computed assuming independence source target phrasesgiven pivot phrase.P (s|t) =XP (s|p)P (p|t)(9)P (t|p)P (p|s)(10)pP (t|s) =Xps, t, p represent phrases source, target pivot language respectively.lexical weights computed similar manner, following strategy proposedCohn & Lapata (2007). approach handle lexicalized reorderingpivot strategies therefore represents limitation potential. Instead,simple distance-based reordering applied decoding. model gives cost linearreordering distance. instance, skipping two words costs twice muchskipping one word.corresponding translation model obtained, sourcetarget systemtuned using original sourcetarget development corpus mentioned previousapproach.768fiEvaluating Indirect Strategies ChineseSpanish SMT6. Evaluation Frameworkfollowing section introduces details evaluation framework. reportstatistics Un corpus, description built systems evaluationdetails.6.1 Corpus Statisticsfar know, discussed section 4, three parallel corpora availableChineseSpanish language pair: Btec, Holy Bible Un.4 former used2008 Iwslt complete experiments pivot strategies reported worksBertoldi et al. (2008). Holy Bible used similar purposes Henrquez,Banchs & Marino (2010).study decide use Un corpus taking advantage factlargest corpus (among three) contains sentences six languages,therefore experiment different pivot languages.experimenting different pivot languages, order make systemscomparable possible, first sentence selection corpus systemsbuilt exactly training, tuning testing sets. selection processfollows:1. corpora tokenized, using standard tokenizer available Moses (Koehn,Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer,Bojar, Constantin, & Herbst, 2007) Spanish, English French; ictclass (Zhang,Yu, Xiong, & Liu, 2003) Chinese; Mada+Tokan (Habash & Rambow, 2005)Arabic.2. Spanish, English French corpora lowercased.3. sentence 100 words language, deleted corpora.4. sentence pair word ratio larger three Chinesepivot pivotSpanish parallel corpora, deleted corpora.5. extract tuning test sets identified sentences ocurringcorpora languages. tuning testing sets drawn sentencesassure appear training corpus. Additionally, sentences,want select differ sentences training setlowest out-of-vocabulary rate. order this, perplexityEnglish language model computed sentence-by-sentence basis usingleave-one-out strategy; then, selected two thousand sentenceshighest perplexity lowest ratio out-of-vocabulary words constructingtuning testing sets. highest perplexity criterion used orderavoid tuning test sentences similar ones training set.lowest out-of-vocabulary words criterion used minimize number outof-vocabulary words tuning test translation. two criteria used4. review process paper, aware new corpus KDE (K Desktop Environment), available recent OPUS project 5769fiCosta-jussa, Henrquez & Banchssequentially, first selected sentences highest perplexity, amongthem, selected lowest ratio out-of-vocabulary words.Table 2 shows main statistics corpora used divided experimentation.DatasetTrainDev.TestLang.ChineseSpanishEnglishArabicFrenchChineseSpanishEnglishArabicFrenchChineseSpanishEnglishArabicFrenchSent.58585858581111111111Words1,7002,3002,0002,6002,30033.043.437.448.844.133.744.238.149.344.9Vocab.1720141718354.24.653.854.24.65Table 2: Un Corpus Statistics used research (all figures given thousands)6.2 System Implementation Evaluation Detailssystems build using revision 4075 Moses (Koehn et al., 2007). systems,used default Moses parameters includes grow-diagonal-final-and wordalignment symmetrization, lexicalized reordering (where possible), relative frequencies,lexical weights phrase bonus translation model (with phrases length 10),5-gram language model using Kneser-Ney smoothing word penalty model. Therefore,14 different features combined equation (3). language model built usingSrilm (Stolcke, 2002) version 1.5.12. optimization done using Mert (Och, 2003).word aligning used Giza++ (Och & Ney, 2000) version 1.0.5.order evaluate translation quality, used Bleu (Papineni, Roukos, Ward,& Zhu, 2001), Ter (Snover, Dorr, Schwartz, Micciulla, & Makhoul, 2006) Meteor(Banerjee & Lavie, 2005) automatic evaluation metrics.Additionally, significance tests performed study system betterother. tests followed pair bootstrap resampling method presented Koehn(2004): Given two translation outputs coming two different systems, created twonew virtual test sets drawing sentences replacement translation outputs.obtained them, computed Bleus observed system performsbetter. procedure repeated 1, 000 times. end, one systems outperformed 99% time, concluded indeed better Bleu score99% statistical significance.770fiEvaluating Indirect Strategies ChineseSpanish SMT7. ChineseSpanish Machine Translation StrategiesGiven different languages available Un corpora, tested three different languagepivots. Additionally, compared cascaded, pseudo-corpus triangulation pivotstrategies. Finally, tried combine system outputs improve translation.7.1 Experimenting Different Pivot Languagesbuilt compared several translation approaches order study impactdifferent pivot languages translating Chinese Spanish. Moreover,evaluated quality pivot approaches differs direct system. builtpivot systems using five languages available Un parallel corpus: English,Spanish, Chinese, Arabic French, built direct system ChineseSpanishparallel corpus.particular, experimented following Chinese-to-Spanish systems: directChinese-to-Spanish system quality upper bound; three cascaded, three pseudo-corpusthree triangulation approaches, using English, Arabic French pivots. orderbuild pivot systems, need corresponding Chinesepivot pivotSpanishsystems.Table 3 shows Bleu, Ter Meteor scores achieved intermediatesystems trained Un Corpus later used built different pivot approaches. Meteor score Chinese-to-Arabic system shownpostprocessing tools required language.ChineseEnglishChineseArabicChineseFrenchEnglishSpanishArabicSpanishFrenchSpanishBleu35.6746.1128.3151.2241.7946.42Ter51.0756.1263.2132.0244.3740.25Meteor36.7747.3570.1260.2264.76Table 3: Pivot Systems.Table 4 shows results Chinese-to-Spanish configurations Un corpus.see best pivot system used pseudo-corpus approach Englishpivot language.Chinese-to-Spanish, fact pseudo-corpus English outperformscascaded English according Bleu score statistically significant,99% confidence (Koehn, 2004). results, however, coherent previous worksusing language pair (Bertoldi et al., 2008; Henrquez Q. et al., 2010) alsoreported pseudo-corpus strategy better cascaded strategy. cascadedpseudo-corpus approaches English statistically significantly bettertriangulation approach, 99% confidence. best knowledge, reasonsone pivot approach better reported literature. Moreover,given difference among approaches pseudo-corpus cascaded approaches771fiCosta-jussa, Henrquez & BanchsLanguagesSystemBleuTerMeteorPivot vocab.ChineseSpanishChineseEnglishSpanishChineseFrenchSpanishChineseArabicSpanishChineseEnglishSpanishChineseFrenchSpanishChineseArabicSpanishChineseEnglishSpanishChineseFrenchSpanishChineseArabicSpanishdirectcascadedcascadedcascadedpseudopseudopseudotriangulationtriangulationtriangulation33.0632.9030.3728.8832.9732.6132.2332.0530.4130.6157.3256.6760.3360.3757.3957.4357.4757.9159.7059.5353.9654.0650.9650.1553.9953.5553.2753.3751.5151.4314k18k17k14k18k17k14k18k17kTable 4: Chinese-to-Spanish cascaded, pseudo-corpus triangulation approaches.significant, better perform experiments particular task languagepair.three approaches, according scores table 4 English best pivotlanguage, statistical signicance 99%, coherent pivotSpanishresults table 3.follows, use procedure predict suitable pivot language justifylanguage may better pivot another. example, pivot vocabularysizes play important role. Birch et al. (2008) concluded study targetvocabulary size negative impact translation quality measured Bleuscore seen Arabic French larger vocabulary sizeEnglish.Apart vocabulary size, research mentioned also measured successmachine translation terms word reordering, i.e., differences word order occurparallel corpus, mainly driven syntactic differences languages.order measure reordering translation assumed reordering occurstwo adjacent blocks source side. simplification allowed detectextract reordering deterministic way.block defined Birch et. al. (2008) segment consecutive source words(source span) aligned set target words. target words also form block. definition block set, formally defined reordering r two blocksB adjacent source, relative order blocks sourcereversed target reordering consistent. reordering blocksBs consistent block Cs , consisting union blocks Bs , alsoconsistent. block said consistent span defined correspondingtarget block contain words aligned source words outside .definition consistent block equivalent definition phrase phrase-basedmachine translation paradigm. Finally, set reorderings r sentence definedR unique given pair sentences. Summarizing, concept reorderingequivalent swap movement described lexicalized reordering endsection 5.772fiEvaluating Indirect Strategies ChineseSpanish SMTLanguagesChineseEnglishSpanishChineseFrenchSpanishChineseArabicSpanishSourcePivot0.39550.62000.6921PivotTarget0.21240.01700.0908Average0.30390.31850.3914Table 5: Chinese-to-Spanish RQuantity metrics depending pivot used.concepts defined, developed metric called RQuantity, definedsentence level metric averaged corpus:rR |rAs |PRQuantity =+ |rBs |(11)R set reorderings sentence, source sentence length, Btwo blocks involved reordering, |rAs | size span blocksource side |rBs | size span block B source side (Birch et al., 2008).objective RQuantity measure amount reordering needtranslating source language target language. minimum RQuantitygiven sentence 0 translation involve word movement maximumP( Ii=2 i)/I words translation inverted compared ordersource sentence.computed RQuantity different language pairs involved pivotapproaches. seen table 5 English appears best pivotlowest average RQuantity three, i.e. pivot needsleast amount reordering average achieve final translation. French Arabicrequired less movements translate Spanish English, lot reorderingneeded obtain first step Chinese, hence penalizing average. resultcoherent conclusion obtained Birch et al. (2008), says amountreordering also negative impact Bleu score.results support intuitive idea English works good intermediatestep Chinese Spanish. French Arabic vocabularycloser size Spanish reorderings also complex Englishfirst step, making source-to-pivot translation harder candidates.gradual increase difficulty (measured target vocabulary size reordering) presentedEnglish seems benefit global result.Nevertheless, also possible Un texts authored English, then,translated languages. would also favour English best pivotlanguage.order observe benefits pivot language direct translation,table 6 presents three examples Bleu scores pivot approach betterdirect approach. Notice phrases disappeareddirect translation correctly appear pseudo-corpus approach.773fiCosta-jussa, Henrquez & BanchsDIRECTPSEUDOREFEN REFDIRECTPSEUDOREFEN REFDIRECTPSEUDOREFEN REFcuestiones como que consideren seriamente la posibilidad de ratificar la tortura otros tratospenas crueles , inhumanos degradantescomo cuestiones que consideren seriamente la posibilidad de ratificar la convencion contrala tortura otros tratos penas crueles , inhumanos degradantesconsidere seriamente la posibilidad de ratificar , con caracter prioritario , la convencioncontra la tortura otros tratos penas crueles , inhumanos degradantesseriously consider ratifying , matter priority , convention torturecruel , inhuman degrading treatment punishmenthabiendo examinado el segundo informe de la comision la recomendacion que figura en elhabiendo examinado el segundo informe de la comision de verificacion de poderes lasrecomendaciones que figuran en elhabiendo examinado el segundo informe de la comision de verificacion de poderes larecomendacion que figura en elconsidered second report credentials committee recommendationcontained thereinpide al secretario general que prepare un informe sobre la aplicacion de esta resolucion laasamblea general , quincuagesimo sexto perodo de sesionespide al secretario general que prepare un informe sobre la aplicacion de la presente resolucionpara su examen por la asamblea general en su quincuagesimo sexto perodo de sesionespide al secretario general que prepare un informe sobre la aplicacion de la presente resolucion ,que sera examinado por la asamblea general en su quincuagesimo sexto perodo de sesionesrequests secretary-general prepare report implementation present resolutionconsideration general assembly fifty-sixth session .Table 6: Chinese-to-Spanish examples pseudo-corpus system (through English) better direct system. En ref English referencesentence7.2 Pivot CombinationUsing 1-best translation output different pivot strategies, built n-bestlist computed final translation using minimum Bayes risk (Mbr) (Kumar & Byrne,2004).translating sentence s, obtain translation t0 evaluatedreference measure systems performance. Mbr focuses finding bestperformance possible translations. so, uses loss function LF(t, t0 )measures loss obtaining hypothesis t0 instead real translation t. BayesRisk defined expected value loss function possible hypotheses t0translations t.E(LF) =XLF(t, t0 )p(t0 |s)(12)t,t0p(t0 |s) translation probability hypothesis t0 given source sentenceobtained decoder, approximation real probability distribution.objective finding best performance possible translation thereforeminimize Bayes Risk. Given loss function distribution, decision ruleminimizes Bayes Risk (Bickel & Doksum, 1977; Goel & Byrne, 2000) given by:= arg min0XLF(t, t0 )p(t0 |s)774(13)fiEvaluating Indirect Strategies ChineseSpanish SMTMbr used literature decoding (Ehling, Zens, & Ney, 2007)postprocess n-best list. instance, last approach used Khalilov et al.(2008) together cascaded approach order obtain best ChineseSpanishtranslation. current version Moses toolkit includes implementations.Mbr algorithm implemented Moses postprocess uses 1Bleu(t, t0 ) lossfunction. experiment, consider hypothesis equally likely thereforep(t0 |s) positive constant therefore could discarded. end, Mbr chooseshypotheses fullfills:= arg min0X1 Bleu(t, t0 )(14)t6=t0Different n-best lists built compare different Mbr outputs: cascaded Mbrusing three pivot languages (hence n = 3, one hypothesis per pivot), pseudo-corpusMbr using three pivot languages (n = 3), triangulation Mbr using threelanguages (n = 3), combination cascaded, pseudo-corpus triangulation outputsusing two languages (n = 6, one hypothesis per pivot strategy) another using(n = 9). important mention n-best lists must least 3 hypothesisper sentence. two hypothesis would work expected LossFunction would always choose longest one, explained definitionBleu:NX!pn (t, t0 )Bleu(t, t0 ) = explog(t, t0 )Nn=1(15)pn (t, t0 ) precision n-grams hypothesis t0 reference t; (t, t0 )brevity penalty disfavouring translation t0 shorter reference t.pn (t, t0 ) = pn (t0 , t)t, t0 : length(t) > length(t0 )(16)1 Bleu(t, ) 1 Bleu(t , t)(17)00Tables 7 8 show results different ChineseSpanish output systems (fromtable 4) combined Mbr technique. tables, observedcombinations pivot strategies obtained better results metrics directapproach. case Ar + Fr, combination statistically significantlybetter direct system terms Bleu score (with 99% confidence).Mbr cascaded triangulation approach (1st row, 2nd 4th column, respectively, table 8) outperform direct system.Finally, D+A (which combine languages pivot system outputstable 4 including direct approach) best Chinese-to-Spanish systems.experimented reverse translation direction (from Spanish Chinese) would unable assess subjective evaluations resulting translationoutputs. However, reversed direction, intuition reordering difficultiesmoved pivottarget step cascade system.775fiCosta-jussa, Henrquez & BanchsEn+FrEn+ArAr+Fr33.58*/56.34/54.4833.53*/56.15/54.6333.14/56.83/53.95Table 7: Chinese-to-Spanish percent Bleu/Ter/Meteor scores system combinationstwo languages pivot approaches using Mbr. (*) statistically significantbetter Bleu direct system.D+ACascaded32.66/57.27/53.3433.60*/56.72/54.38Pseudo33.30*/57.04/53.9133.77*/56.52/54.47Triangulation31.84/58.12/53.0532.90/57.01/54.0333.97*/56.00/54.8734.09*/55.88/55.02Table 8: Chinese-to-Spanish percent Bleu/Ter/Meteor scores system combinationsEn + Fr + Ar languages (A), direct system (D) pivot approaches usingMbr. (*) statistically significant better Bleu direct system.Regarding fact English best pivot language task consideration, argue English might constitute better intermediate stepSpanish Chinese, rather French Arabic, based assumption Spanishcloser French (both romance languages derived Latin) Arabic (theIberian peninsula occupied Arabic culture 500 years, Spanishstrong influence Arabic) Chinese French Arabic. sense, Englishseems represent optimal intermediate point Chinese Spanish,translation complexity divided two phases. reordering burden resolvedChinese-to-English phase morphology generation burden resolvedEnglish-to-Spanish phase. Thinking translation-space non-conservative field,say English middle way Chinese Spanish,passing French Arabic implies larger path kind detour proximities Spanish. conjecture, course, nicely explainsobserving. Definitively, research needed better understand happening.8. Conclusionswork provided brief survey state-of-the-art ChineseSpanish Smt. Firstall, language pair great interest economically culturally takeaccount high number Chinese Spanish speakers. Besides, statistical machinetranslation popular approach field Mt given shown greatquality international evaluation campaigns Nist (2009) Wmt (2012).main points covered study were:mainly three ChineseSpanish parallel corpora (Btec, Holy Bible Un)freely available research purposes.776fiEvaluating Indirect Strategies ChineseSpanish SMTEnglish best pivot language conducting Chinese-to-Spanish translationscompared languages French Arabic. system built using Englishpivot significantly better ones built French Arabic, 99%confidence comparisons.preference pivot language appears correlated proposedtranslation-quality prediction metrics differences vocabulary sizesamount reordering. According conclusion, best pivot languageEnglish lowest increase vocabulary size lowest increasereordering complexity.significant difference found among best cascaded pseudo-corpus pivotapproaches, pseudo-corpus strategy best pivot strategy Chineseto-Spanish. Additionally, pseudo-corpus cascaded approaches significantlybetter triangulation approach.output combination using Mbr able improve direct system 1 Bleupoint best case. improvement significantly better 99% confidencecoherent improvements evaluation metrics studied.future research plan work problem automatically extracting parallelcorpus comparable corpora collected web. Additionally, intend developfreely available ChineseSpanish translation system would allow collecting userfeedback. Then, work special techniques incorporate knowledgeSmt system.Acknowledgmentsauthors would like specially thank reviewers comments helpedlot improve work. Additionally, authors would like thank UniversitatPolitecnica de Catalunya Institute Infocomm Research supportpermission publish research.work partially funded Seventh Framework Program EuropeanCommission International Outgoing Fellowship Marie Curie Action (IMTraP2011-29951); Spanish Ministry Economy Competitiveness FPIScholarship BES-2008-003851 Ph.D. students AVIVAVOZ project (TEC200613694-C03-01); BUCEADOR project (TEC2009-14094-C04-01).ReferencesAbdul-Rauf, S., Fishel, M., Lambert, P., Noubours, S., & Sennrich, R. (2012). Extrinsicevaluation sentence alignment systems. Proceedings LREC workshop Creating Cross-language Resources Disconnected Languages Styles, CREDISLAS,Istambul.Banchs, R. E., Crego, J. M., Lambert, P., & Marino, J. B. (2006). Feasibility StudyChinese-Spanish Statistical Machine Translation. Proc. 5th Int. Symposium777fiCosta-jussa, Henrquez & BanchsChinese Spoken Language Processing (ISCSLP)CONLL, pp. 681692, Kent Ridge,Singapore.Banchs, R. E., & Li, H. (2008). Exploring Spanish Morphology effects Chinese-SpanishSMT. MATMT 2008: Mixing Approaches Machine Translation, pp. 4953,Donostia-San Sebastian, Spain.Banerjee, S., & Lavie, A. (2005). METEOR: Automatic Metric MT EvaluationImproved Correlation Human Judgments. Proceedings ACL WorkshopIntrinsic Extrinsic Evaluation Measures MT and/or Summarization.Bertoldi, N., Cattoni, R., Federico, M., & Barbaiani, M. (2008). FBK @ IWSLT-2008.Proc. International Workshop Spoken Language Translation, pp. 3438,Hawaii, USA.Bickel, P. J., & Doksum, K. A. (1977). Mathematical: Basic Ideas Selected topics.HoldenDay Inc., Oakland, CA, USA.Birch, A., Osborne, M., & Koehn, P. (2008). Predicting Success Machine Translation.Proceedings 2008 Conference Empirical Methods Natural LanguageProcessing, pp. 745754, Honolulu, Hawaii. Association Computational Linguistics.Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Lafferty, J. D.,Mercer, R. L., & Roossin, P. S. (1990). Statistical Approach Machine Translation.Computational Linguistics, 16 (2), 7985.Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., & Specia, L. (2012). Findings2012 workshop statistical machine translation. Proceedings SeventhWorkshop Statistical Machine Translation, pp. 1051, Montreal, Canada.Chew, P. A., Verzi, S. J., Bauer, T. L., & McClain, J. T. (2006). Evaluation BibleResource Cross-language Information Retrieval. Proceedings WorkshopMultilingual Language Resources Interoperability, pp. 6874.Cohn, T., & Lapata, M. (2007). Machine Translation Triangulation: Making EffectiveUse Multi-Parallel Corpora. Proc. ACL.Costa-jussa, M., Henrquez, C., & Banchs, R. E. (2011a). Enhancing Scarce-resource Language Translation Pivot Combinations. 5th International Joint ConferenceNatural Language Processing, IJCNLP, Chiang Mai, Thailand.Costa-jussa, M., Henrquez, C., & Banchs, R. E. (2011b). Evaluacion de estrategias parala traduccion automatica estadstica de chino castellano con el ingles como lenguapivote. XXVII edicion del Congreso Anual de la Sociedad Espanola para el Procesamiento del Lenguaje Natural, SEPLN, Huelva.de Gispert, A., & Marino, J. (2006). Catalan-English Statistical Machine Translation without Parallel Corpus: Bridging Spanish. Proc. LREC 5th WorkshopStrategies developing Machine Translation Minority Languages (SALTMIL06),pp. 6568, Genova.EastAsiaFoundation (2012). Global asia magazine.. [Online; accessed 12-December-2012].778fiEvaluating Indirect Strategies ChineseSpanish SMTEhling, N., Zens, R., & Ney, H. (2007). Minimum Bayes Risk Decoding BLEU. Proceedings 45th Annual Meeting Association Computational Linguistics Companion Volume Proceedings Demo Poster Sessions, pp. 101104,Prague, Czech Republic. Association Computational Linguistics.Ethnologue (2012). Ranking spoken languages.. [Online; accessed 12-December2012].Goel, V., & Byrne, W. (2000). Minimum Bayes-risk Automatic Speech Recognition. Computer Speech Language, 14 (2), 115135.Habash, N., & Rambow, O. (2005). Arabic Tokenization, Part-of-Speech Tagging Morphological Disambiguation One Fell Swoop. Proc. 43rd Annual MeetingAssociation Computational Linguistics, pp. 573580, Ann Arbor, MI. Association Computational Linguistics.Henrquez Q., C. A., Banchs, R. E., & Marino, J. B. (2010). Learning Reordering ModelsStatistical Machine Translation Pivot Language. Internal Report TALP-UPC.Khalilov, M., Costa-Jussa, M. R., Henrquez, C. A., Fonollosa, J. A. R., Hernandez, A.,Marino, J. B., Banchs, R. E., Chen, B., Zhang, M., Aw, A., & Li, H. (2008).TALP & I2R SMT Systems IWSLT 2008. Proc. International WorkshopSpoken Language Translation, pp. 116123, Hawaii, USA.Koehn, P. (2004). Statistical Significance Tests Machine Translation Evaluation.Proceedings EMNLP, Vol. 4, pp. 388395.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B.,Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.(2007). Moses: Open source toolkit statistical machine translation. Proc.ACL, pp. 177180, Prague, Czech Republic.Koehn, P., Och, F., & Marcu, D. (2003). Statistical Phrase-Based Translation. Proc.41th Annual Meeting Association Computational Linguistics.Kumar, S., & Byrne, W. (2004). Minimum Bayes-Risk Decoding Statistical MachineTranslation. Proceedings Human Language Technology North AmericanAssociation Computational Linguistics Conference (HLT/NAACL04), pp. 169176, Boston, USA.Marino, J., Banchs, R. E., Crego, J. M., de Gispert, A., Lambert, P., Fonollosa, J. R.,& Costa-jussa, M. R. (2006). N-gram Based Machine Translation. ComputationalLinguistics, 32 (4), 527549.Moore, R. (2002). Fast Accurate Sentence Alignment Bilingual Corpora. Proc.AMTA, pp. 135144, London.Nist (2009). NIST machine translation evaluation campaign..December-2012].[Online; accessed 12-Och, F. J., & Ney, H. (2000). Improved Statistical Alignment Models. Proc.38th Annual Meeting Association Computational Linguistics, pp. 440447,Hongkong, China.779fiCosta-jussa, Henrquez & BanchsOch, F. (2003). Minimum Error Rate Training Statistical Machine Translation. Proc.41th Annual Meeting Association Computational Linguistics, pp.160167.Och, F., & Ney, H. (2002). Dicriminative training maximum entropy models statistical machine translation. Proc. 40th Annual Meeting AssociationComputational Linguistics, pp. 295302, Philadelphia, PA.Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2001). BLEU: Method AutomaticEvaluation Machine Translation. IBM Research Report, RC22176.Paul, M. (2008). Overview iwslt 2008 evaluation campaign. Proc. International Workshop Spoken Language Translation, pp. 117, Hawaii, USA.Paul, M., Yamamoto, H., Sumita, E., & Nakamura, S. (2009). Importance PivotLanguage Selection Statistical Machine Translation. HLT-NAACL (Short Papers), pp. 221224.Rafalovitch, A., & Dale, R. (2009). United Nations General Assembly Resolutions: SixLanguage Parallel Corpus. Proc. MT Summit XII, pp. 292299, Ottawa.Senrich, R. (2010). MT-based Sentence Alignment OCR-generated Parallel Texts.Proc. AMTA, Denver.Snover, M., Dorr, B., Schwartz, R., Micciulla, L., & Makhoul, J. (2006). Study Translation Edit Rate Targeted Human Annotation. Proceedings AssociationMachine Translation Americas.Stolcke, A. (2002). SRILM: extensible language modeling toolkit.. Proc. Int.Conf. Spoken Language Processing, pp. 901904, Denver, CO.Tillman, C. (2004). Block Orientation Model Statistical Machine Translation.HLT-NAACL.Wang, H., Wu, H., Hu, X., Liu, Z., Li, J., Ren, D., & Niu, Z. (2008). TCH MachineTranslation System IWSLT 2008. Proc. International WorkshopSpoken Language Translation, pp. 124131, Hawaii, USA.Wu, H., & Wang, H. (2007). Pivot Language Approach Phrase-Based Statistical MachineTranslation. Proc. ACL, pp. 856863, Prague.Zapatero, J. R. (2010). China top priority spanish economy; companieswell aware that.. [Online; accessed 12-December-2012].Zens, R., Och, F., & Ney, H. (2002). Phrase-based statistical machine translation. Verlag,S. (Ed.), Proc. German Conference Artificial Intelligence (KI).Zhang, H., Yu, H., Xiong, D., & Liu, Q. (2003). HHMM-based chinese lexical analyzerICTCLAS. Proc. 2nd SIGHAN Workshop Chinese language processing,pp. 184187, Sapporo, Japan.780fiJournal Artificial Intelligence Research 45 (2012) 641-684Submitted 11/12; published 12/12Learning Predict Textual DataKira RadinskySagie DavidovichShaul Markovitchkirar@cs.technion.ac.ilmesagie@gmail.comshaulm@cs.technion.ac.ilComputer Science DepartmentTechnionIsrael Institute TechnologyHaifa 32000, IsraelAbstractGiven current news event, tackle problem generating plausible predictionsfuture events might cause. present new methodology modeling predictingfuture news events using machine learning data mining techniques. Punditalgorithm generalizes examples causality pairs infer causality predictor. obtainprecisely labeled causality examples, mine 150 years news articles apply semanticnatural language modeling techniques headlines containing certain predefined causalitypatterns. generalization, model uses vast number world knowledge ontologies.Empirical evaluation real news articles shows Pundit algorithm performswell non-expert humans.1. IntroductionCausality studied since antiquity, e.g., Aristotle, modern perceptionscausality influenced, perhaps, work David Hume (17111776),referred causation strongest important associative relation,lies heart perception reasoning world, constantlysupposed connection present fact inferredit.Causation also important designing computerized agents. agent, situated complex environment, plans actions, reasons future changesenvironment. changes result actions, many othersresult various chains events necessarily related agent. processobserving event, reasoning future events might caused it, calledcausal reasoning.past, computerized agents could operate complex environments duelimited perceptive capabilities. proliferation World Wide Web, however,changed that. intelligent agent act virtual world Web, perceivingcurrent state world extensive sources textual information, including Webpages, tweets, news reports, online encyclopedias, performing various taskssearching, organizing, generating information. act intelligently complexvirtual environment, agent must able perceive current state reasonfuture states causal reasoning. reasoning ability extremely helpfulconducting complex tasks identifying political unrest, detecting trackingc2012AI Access Foundation. rights reserved.fiRadinsky, Davidovich & Markovitchsocial trends, generally supporting decision making politicians, businesspeople,individual users.many works devoted extracting information text (e.g., Banko,Cafarella, Soderl, Broadhead, & Etzioni, 2007; Carlson, Betteridge, Kisiel, Settles, Hruschka, & Mitchell, 2010), little done area causality extraction,works Khoo, Chan, Niu (2000) Girju Moldovan (2002) notableexceptions. Furthermore, algorithms developed causality extraction try detectcausality cannot used predict it, is, generate new events given eventmight cause.goal paper provide algorithms perform causal reasoning, particular causality prediction, textually represented environments. developedcausality learning prediction algorithm, Pundit, that, given event representednatural language, predicts future events cause. algorithm trained examplescausality relations. uses large ontologies generalize causality pairsgenerate prediction model. model represented abstraction tree, that, giveninput cause event, finds appropriate generalization, uses learned rulesoutput predicted effect events.implemented algorithm applied large collection news reportslast 150 years. extract training examples news corpus,use correlation, means causality often misidentified. Instead, use textualcausality patterns (such X X causes Y), applied news headlines,identify pairs structured events supposedly related causality. resultsemantically-structured causality graph 300 million fact nodes connectedone billion edges. evaluate method, tested news archive 2010,used training. results judged human evaluators.give intuition type predictions algorithm generates, presenttwo examples actual predictions made system. First, given event Magnitude 6.5 earthquake rocks Solomon Islands, algorithm predicted tsunamiwarning issued Pacific Ocean. learned past examplestrained, oneh7.6 earthquake strikes island near India, tsunami warning issued Indian Oceani.Pundit able infer earthquake occurring near island would resulttsunami warning issued ocean. Second, given event Cocaine foundKennedy Space Center, algorithm predicted people arrested.partially based past example hpolice found cocaine lab 2 people arrested i.contributions work threefold: First, present novel scalable algorithms generalizing causality pairs causality rules. Second, provide new methodusing casualty rules predict new events. Finally, implement algorithmslarge scale system perform empirical study realistic problems judged humanraters. make extracted causality information publicly available researchfield 1 .1. http://www.technion.ac.il/~kirar/Datasets.html642fiLearning Predict Textual Data2. Learning Predicting Causalitysection, describe Pundit algorithm learning predicting causality.start overview learning prediction process. training, learningalgorithm receives causality event pairs, extracted historical news archives (Section3). algorithm generalizes given examples using world knowledgeproduces abstraction tree (AT)(Section 2.4). node AT, predictionrule generated examples node (Section 2.5). Then, predictionphase, algorithm matches given new event nodes AT, associatedrule applied produce possible effect events (Section 2.6). eventsfiltered (Section 2.7) effect event output. output event also givennatural language, sentence-like form. process illustrated Figure 1.Implausibleevent lterFigure 1: Structure Pundit prediction algorithm2.1 Event Representationbasic element causal reasoning event. Topic Tracking Detection (TDT)community (Allan, 2002) defined event particular thing happens specific time place, along necessary preconditions unavoidable consequences.643fiRadinsky, Davidovich & Markovitchphilosophical theories consider events exemplifications properties objectstimes (Kim, 1993). example, Caesars death 44 BC Caesars exemplificationproperty dying time 44 BC. theories impose structure events,change one elements yields different event. example, Shakespears deathdifferent event Caesars death, objects exemplifying property different.section, discuss way represent events following Kims (1993) exemplification theory allow us easily compare them, generalize them, reasonthem.three common approaches textual event representation: first approachdescribes event sentence level running text individual terms (Blanco, Castell,& Moldovan, 2008; Sil, Huang, & Yates, 2010). Event similarity treated distancemetric two events bag words. approaches useful,often fail perform fine-grained reasoning. Consider, example, three events: US Armybombs warehouse Iraq, Iraq attacks US base, Terrorist base attackedUS Marines Kabul. Representing events individual terms alone might yieldfirst two similar first last wordscommon. However, approaches disregard fact actors first last eventmilitary groups Kabul Iraq event locations. factstaken account, first last events clearly similar firstsecond.second approach describes events syntax-driven manner, event texttransformed syntax-based components, noun phrases (Garcia, 1997; Khoo et al.,2000; Girju & Moldovan, 2002; Chan & Lam, 2005). example, representationerroneously finds second third events similar due syntacticsimilarity them. Using first two approaches, hard make practicalgeneralizations events compare way takes accountsemantic elements compose them.third approach semantic (similar representation Cyc; Lenat & Guha,1990), maps atomic elements event semantic concepts. approachprovides grounds canonic representation events comparable generalizable. work, follow third approach represent events semantically.Given set entities represent physical objects abstract conceptsreal world (e.g., people, instances, types), set actions P , define eventordered set e = hP, O1 , . . . , O4 , ti, where:1. P temporal action state events objects exhibit.2. O1 actor performed action.3. O2 object action performed.4. O3 instrument action performed.5. O4 location event.6. time-stamp.644fiLearning Predict Textual Dataexample, event U.S Army destroyed warehouse Iraq explosives,occurred October 2004, modeled as: Destroy (Action); U.S Army (Actor);warehouse (Object); explosives (Instrument); Iraq (Location); October 2004 (Time).approach inspired Kims (1993) property-exemplification events theory.2.2 Learning Problem Definitiontreat causality inference supervised learning problem. Let Ev universepossible events. Let f : Ev Ev {0, 1} function(1 e1 causes e2 ,f (e1 , e2 ) =0 otherwise.denote f + = {(e1 , e2 )|f (e1 , e2 ) = 1}. assume given set possible positiveexamples E f + .goal merely test whether pair events plausible cause-effect pairf , generate given event e events cause. purpose defineg : Ev 2Ev g(e) = {e0 |f (e, e0 ) = 1}; is, given event, output set eventscause. wish build predictor g using examples E.Learning f E could solved standard techniques concept learningpositive examples. requirement learn g, however, presents challenging taskstructured prediction positive examples.2.3 Generalizing Objects Actionsgoal develop learning algorithm automatically produces causality functionbased examples causality pairs. inferred causality function ablepredict outcome given event, even never observed before. example, giventraining examples hearthquake Turkey, destructioni hearthquake Australia,destructioni, current new event earthquake Japan, reasonable predictionwould destruction. able handle predictions, must endow learningalgorithm generalization capacity. example, scenario, algorithmmust able generalize Australia Turkey countries, infer earthquakescountries might cause destruction. type inference knowledge Japanalso country enables algorithm predict effects new events using patternspast.generalize set examples, consisting pair events, performgeneralization components events. two types componentsobjects actions.generalize objects, assume availability semantic network Go = (V, E),nodes V objects universe, labels edgesrelations isA, partOf CapitalOf. work, consider one largestsemantic networks available, LinkedData ontology (Bizer, Heath, & Berners-Lee, 2009),describe detail Section 3.define two objects similar relate third object way.relation label sequence labels semantic network. example,645fiRadinsky, Davidovich & MarkovitchParis London considered similar nodes connected pathCapitalof Incontinentnode Europe. formally define idea.Definition 1. Let a, b V . sequence labels L = l1 , . . . , lk generalization patha, b, denoted GenPath(a,b), exist two paths G, (a, v1 , l1 ), . . . (vk , vk+1 , lk )(b, w1 , l1 ), . . . (wk , wk+1 , lk ), s.t. vk+1 = wk+1 .Overgeneralization events avoided e.g., given two similar events, oneoccurring Paris one London, wish produce generalization city EuropeCapitalof Incontinent( Europe) rather abstract generalization cityCapitalof Incontinent IsAcontinent ( Continent). wish generalizationspecific possible. call minimal generalization objects.Definition 2. minimal generalization path, denoted GenP ath(a, b), definedset containing shortest generalization paths. denote distGen (a, b) lengthGenP ath(a, b).Path-based semantic distances one shown successfulmany NLP applications. example, semantic relatedness two words measuredmeans function measured distance words taxonomy (Rada,Mili, Bicknell, & Blettner, 1989; Strube & Ponzetto, 2006). build metricexpand handle events structured contain several objects differentontologies.efficiently produce GenP ath, designed algorithm (described Figure 2),based dynamic programming, computes GenP ath object pairs G.simplicity, describe algorithm computes single path two nodesb, rather set shortest paths. step 1 queue holds nodesgeneralization initialized. step 2, algorithm identifies nodes (a, b)common node (c) connecting via type edge (l). cthought generalization b. gen structure maps pair nodesgeneralization (M gen.Gen) generalization path (M Gen.P red). step 3,dynamic programming manner, algorithm iterates nodes (a, b) genfound minimal generalization previous iterations, finds two nodes one(x) connecting one (y) connecting b via type edge l (step 3.4). Thus,minimal generalization x minimal generalization b, pathGenP ath a, b addition edge type l. update performedsteps 3.4.13.4.4. Eventually, nodes minimal generalizationexpanded (i.e., algorithm cannot find two nodes connect via edgetype), stops returns gen (step 4). prediction, several gen exists,consider prediction corresponding GenP ath.define distance actions using ontology Gp , similarly waydefined distance objects. Specifically, use VerbNet (Kipper, 2006) ontology,one largest English verb lexicons. mapping many onlineresources, Wordnet (Miller, 1995). ontology hierarchical basedclassification verbs Levin classes (Dang, Palmer, & Rosenzweig, 1998).resource widely used many natural language processing applications (Shi &646fiLearning Predict Textual DataProcedure Minimal Generalization Path(G)(1) Q new Queue(2) Foreach {(a, c, l), (b, c, l) E(G)|a, b, c V (AT ), l Lables}(2.1) gen(a, b).Gen = c(2.2) gen(a, b).P red = l(2.3) gen(a, b).Expanded = f alse(2.4) Q.enqueue((a, b))(3) Q 6= :(3.1) (a, b) Q.dequeue()(3.2) gen(a, b).Expanded 6= true:gen(a, b).Expanded = true(3.4) Foreach {(x, a, l), (y, b, l) E(AT )|x, V (AT ), l Lables}(3.4.1) gen(x, y).Gen = gen(a, b).Gen(3.4.2) gen(x, y).P red = gen(a, b).P red||l(3.4.3) gen(x, y).Expanded = f alse(3.4.4) Q.enqueue((x, y))(4)Return genFigure 2: Procedure calculating minimal generalization path object pairsMihalcea, 2005; Giuglea & Moschitti, 2006). Using ontology describe connectionsverbs. Figure 10 shows node ontology generalizes actions hitkick.2.4 Generalizing Eventsorder provide strong support generalization, wish find similar eventsgeneralized single abstract event. example, wish inferhearthquake Turkey, destructioni hearthquake Australia, destructioni examplesgroup events. Therefore, wish cluster events wayevents similar causes effects clustered together. clustering methods,distance measure objects defined. Let ei = hP , O1i , . . . , O4i , tiej = hP j , O1j , . . . , O4j , tj two events. previous subsection defined distancefunction objects (and actions). Here, define similarity two eventsei ej function distances objects actions:GpjjGoSIM (ei , ej ) = f distGen(P , P j ), distGGen (O1 , O1 ), . . . , distGen (O4 , O4 ) ,(1)where, distGGen distance function distGen graph G, f aggregationfunction. work, mainly use average aggregation function, alsoanalyze several alternatives.647fiRadinsky, Davidovich & MarkovitchLikewise, similarity two pairs cause-effect events hci , ei hcj , ejdefined as:SIM (hci , ei i, hcj , ej i) = f SIM (ci , cj ), SIM (ei , ej ) .(2)Using similarity measure suggested above, clustering process thoughtgrouping training examples way low varianceeffects high similarity cause. similar information gain methodsexamples clustered class. use HAC hierarchical clustering algorithm(Eisen, Spellman, Brown, & Botstein, 1998) clustering method. algorithm startsjoining closest event pairs together cluster. keeps repeating processjoining closest two clusters together elements linked hierarchicalgraph events call abstraction tree (AT). Distance clusters measureddistance representative events. allow this, assign noderepresentative cause event, event closest centroid nodes causeevents. prediction phase, input cause event matched onecreated clusters, i.e., closest representative cause event cluster.2.5 Causality Prediction Rule Generationlast phase learning creation rules allow us, given cause event,generate prediction it. input cause event matched nodecentroid, naive approach would return effect event matched centroid. This,however, would provide us desired result. Assume event ei =Earthquakehits Haiti occurred today, matched node represented centroid:Earthquake hits Turkey, whose effect Red Cross help sent Ankara. Obviously,predicting Red Cross help sent Ankara earthquake Haitireasonable. would like able abstract relation past causepast effect learn predicate clause connects them, example Earthquake hits[Country Name] yielding Red Cross help sent [Capital Country]. prediction,clause applied input event ei , producing predicted effect.example, logical predicate clause would CapitalOf, CapitalOf(Turkey)= Ankara.applied current event ei , CapitalOf(Haiti) = Port-au-Prince, outputRed Cross help sent Port-au-Prince. Notice clausesapplied certain types objects case, countries. clauses length,e.g., pair hsuspect arrested Brooklyn, Bloomberg declares emergencyi producesclause Mayor(BoroughOf(x)), Brooklyn borough New York, whose mayorBloomberg.show learn clauses node graph. Recallsemantic network graph GO edge-labeled graph, edge triplethv1 , v2 , li, l predicate (e.g., CapitalOf). rule-learning procedure dividedtwo main steps. First, find undirected path pi length k GOobject cause event object effect event. Notenecessarily look paths two objects role. example,found path location cause event (Brooklyn) actoreffect event (Bloomberg). Second, construct clause using labels path pi648fiLearning Predict Textual Datapredicates. call predicate projection size k, pred = l1 , . . . , lkevent ei event ej . prediction, projections applied new evente = hP , O1 , . . . , O4 , ti finding undirected path GO Oi sequencelabels pred. k unknown, algorithm, training example hct , et nodeAT, finds possible predicate paths increasing sizes k objectsct objects et GO graph. path weighted number timesoccurred node, support rule. full predicate generation proceduredescribed Figure 3. function LearnP redicateClause calls inner functionF indP redicateP ath different k sizes different objects given causeeffect events. F indP redicateP ath recursive function tries find pathtwo objects graph length k. returns labels path found.rule generated template generating prediction future event given causeevent. example rule seen Figure 4. Rules return NULLdisplayed figure. example, generate object O1 future event,l1 l2try apply pathobject O4 cause, thus generating possible objectsl1 l2object O1 prediction (see Section 2.6). Similarly, pathappliedO2 , generating possible objects. object O2 prediction, simple pathone label generated. Therefore, prediction, possible objects O2ones connect O4cause label l8 (if any). object O3 prediction,use O3cause . O4 special rule generated (F indP redicateP ath returned NULLobjects), final prediction O4ef f ect .2.6 PredictionGiven trained model g, applied new event e = hP , O1 , . . . , O4 , ti orderproduce effects. process divided two main steps propagating eventretrieve set matched nodes, applying rules matched nodeproduce possible effects.Given new event, Pundit traverses starting root. nodesearch frontier, algorithm computes similarity (SIM (ei , ej )) input eventcentroid children node, expands children bettersimilarity parent. idea stated intuitively attempt findnodes least general still similar new event. full algorithmillustrated Figure 5. illustration process seen Figure 6. Here, eventbombing Baghdad received input. system searches least generalcause event observed past (for simplicity show short notationcause events AT). case, generalized cluster: bombing city.candidates selected military communication cluster bombing cluster(as node bombing worship area lower score bombing).node retrieved previous stage, predicate projection, pred, appliednew event e = hP , O1 , . . . , O4 , ti. Informally, say pred appliedfinding undirected path GO Oi labels pred. rule generatespossible effect event retrieved node. projection results reachedobjects vertex. formal explanation pred applied V0 :V0 , V1 . . . Vk : (V0 , V1 , l1 ), . . . (Vk1 , Vk , lk ) Edges(GO ). projection results649fiRadinsky, Davidovich & MarkovitchProcedure FindPredicatePath(curEntity, goalEntity, depth)curEntity = goalEntity ReturnElsedepth = 0 Return N UElseForeach relation outEdges(curEntity)solution FindPredicatePath(relation(curEntity), goalEntity, depth 1)solution 6= N UForeach existingSolutionSolutions :Return Solutions (existingSolution||relation||solution)Return SolutionsProcedure LearnPredicateClause(hP c , O1c , . . . , O4c , tc i, hP e , O1e , . . . , O4e , te i, depth)Foreach Oic Oc , Oje Oe , k {1 . . . depth}rule(j)Foreach Oic Oc , Oje Oe , k {1 . . . depth}rule(j) rule(j) {hOic , FindPredicatePath(Oic , Oje , k)i}Return ruleFigure 3: Procedure generating rule two events inferring paths twoevents causality graph.objects Vk . projection results nodes weighted similaritytarget cause node Gen ranked support rule (for tiebreaking). several Gen exists, highest similarity considered. See Figure 7complete formal description algorithm. example (Figure 6), candidatebombing [city] following rules:1. P ef f ect = happen, O1ef f ect = riot , O4ef f ect = O4cause2. P ef f ect = happen, O1ef f ect = riot , O4ef f ect = O4causemainstreetof3. P ef f ect = kill, O2ef f ect = people4. P ef f ect = condemn, O1ef f ect = O4mayorof boroughof, O2ef f ect = attackclarity, objects rule applied (the rule object NULL),use effect concept matched training example.event Baghdad Bombing (O1 = Bombing, O4 = Baghdad), applying rulesyields following:1. Baghdad Riots (P ef f ect = happen, O1ef f ect = riot , O4ef f ect = Baghdad).2. Caliphs Street Riots (P ef f ect = happen, O1ef f ect = riot , O4ef f ect = Caliphs StreetmainstreetofO4cause ).650fiLearning Predict Textual DataRule(cause, ef f ect) =O1O4causel1O2{O4causel8O3{O3cause ;}l2! !,O2causel1l3! !!}O4Figure 4: example generated rule3. People killed (P ef f ect = kill, O2ef f ect = people).4. rule cannot applied given event, outgoing edge typeborough-of node Baghdad.2.7 Pruning Implausible Effectscases, system generated implausible predictions. example, eventhlightning kills 5 peoplei, system predicted hlightning arrestedi.prediction based generalized training examples people killedpeople got arrested, as: hMan kills homeless man, man arrested i. coulddetermine logical event is, could avoid false predictions. sectiondiscuss filter out.goal filtering component different predictor.predictors goal generate predictions future events, components goalmonitor predictions. predictor learns causality relation events,component learns plausibility.right way perform filtering utilize common sense knowledgeaction. knowledge would state type actor object performaction, possible instruments action preformed possiblelocations. knowledge would existed, would identified actionarrest object human. However, common sense knowledge currentlyavailable. Therefore, resort common practice using statisticalmethods.651fiRadinsky, Davidovich & MarkovitchProcedure Propagation(e = hP , O1 , . . . , O4 , ti)(1) Candidates {}(2) Q new Queue(3) Q.enqueue(G.root)(4) Q 6= :(4.1) cur Q.dequeue()(4.2) Foreach edge cur.OutEdges:SIM (e, edge.Source) > SIM(e, edge.Destination):Candidates Candidates{(edge.Source, SIM (e, edge.Source))}Else :Q.enqueue(edge.Destination)(5) Return CandidatesFigure 5: Procedure locating candidates prediction. algorithm saves setpossible matched results (Candidates), queue holding search frontier(Q). step 4, algorithm traverses graph. step 4.2, edge,algorithm tests whether similarity new event e parent node(edge.Source) higher child node (edge.Destination). testsucceeds, parent node, similarity score, added possibleresults. edges exposed, algorithm returns possible resultsstep 5.Baghdadbombingmilitary0.2militarycommunicationbombing0.7bombing0.8city0.3bombing0.65 worship area0.20.75bombing KabulFigure 6: event bombing Baghdad received input. system searchesleast general cause event observed past. casegeneralized cluster: bombing city. rule node appliedBaghdad bombing generate prediction.652fiLearning Predict Textual DataProcedure FindPredicatePathObjects(entity, path = hl1 . . . lk i)(1) Candidates {}(2) Q new Queue(3) Q.enqueue(entity)(4) labelIndexInP ath = 1(5) path.Count == 0: Return {entity}(6) Q 6= :cur Q.dequeue()Foreach edge {edge cur.OutEdges|edge.label = path[labelIndexInP ath]}:labelIndexInP ath = k :Candidates Candidates {edge.Destination}Else :labelIndexInP ath > k: Return CandidatesQ.enqueue(edge.Destination)labelIndexInP ath labelIndexInP ath + 1(7) Return CandidatesProcedure ApplyPredicateClause(hP, O1 , . . . , O4 , ti, rule)Foreach = 1...4OipredictionForeach path = {Oj , {l1 . . . lk }} rule(i)Oiprediction FindPredicatePathObjects(Oj , hl1 . . . lk i)Return hO1prediction . . . O4predictionFigure 7: Procedure applying rule new given event. main procedure ApplyPredicateClause. procedure generates objects predicted event O1 . . . O4 givenrule. rule list lists tuples. tuple concept path.tuple function FindPredicatePathObjects applied. procedure findsobjects path whose labels connect given concept. objectsstored Candidates (step 1). algorithm holds queue Q frontier (step2). queue first holds given entity (step 3). procedure holds counter indicating whether followed entire given path (step 4). algorithm checkswhether edge label path[labelIndexInPath] going objecthead frontier. algorithm reaches end given path(labelIndexInP ath = k), returns candidates.information extraction literature, identifying relationship factsplausibility widely studied. methods usually estimate prior probability relation examining frequency pattern large corpus,Web (Banko et al., 2007). example, relation hPeople,arrest,Peopleimethods return phrase mentioned 188 times Web, relationhPeople,arrest,[Natural Disaster]i mentioned 0 times. Similarly, estimate priorprobability event occur prior appearance New York Times,653fiRadinsky, Davidovich & MarkovitchProcedure Pruning Implausible Effects(ev = hPi , O1 , . . . , O4 , ti, generalizationBound)(1) Foreach j 1 . . . 4 :generalizationPath = {}0 . . . generalizationBoundGen(Oi ) F indP redicateP athObjects(OjS, generalizationP ath)generalizationPath generalizationP ath {IsA}(2) Return Averagei,j,i6=j (M axo1 Gen(Oi ),o2 Gen(Oj ) P CI(o1 , o2 , i, j))Figure 8: procedure calculating PMCI event. procedure, step 1,first generates generalizations type IsA object (with path whoselength generalizationBound). purpose uses functionFindPredicatePathObjects (defined Figure 7). generalization procedurerepeated objects comprising event ev, result storedGen. final result algorithm calculated step 2. two objects(o1 , o2 ) generalization (Gen), also contains original objects,find maximum PMCI. compute final result averagingmaximum PMCI.primary source news headlines. filter events that, priori, lowprobability occur.present algorithm Figure 8. calculated many times semantic concepts representing event, immediate generalizations, actually occurred togetherpast semantic roles. example, check many times lightningnatural phenomena arrested. Formally, define point-wise mutual conceptinformation (PMCI) two entities verbs oi , oj (e.g., lightning arrest) rolesri , rj (e.g., actor action) definedP CI(oi , oj , ri , rj ) = logp(oi @ri , oj @rj ).p(oi @ri )p(oj @rj )(3)Given event, calculate average PMCI components. algorithm filterspredicted events low average PMCI. assume cause effectexamples training ground truth, yield high PMCI. Therefore,evaluate threshold filtering training data. is, collectedeffects observed training data estimated average PMCI entireNYT dataset.reader note applying rules might create problem. pastearthquake occurred Tokyo, pruning procedure might return low plausibility.handle type errors, calculate PMCI upper level categories entities(e.g., natural disasters) rather specific entities (e.g., earthquakes). therefore restricttwo upper level categories.654fiLearning Predict Textual Data3. Implementation Detailsprevious section, presented high-level algorithm requires training examples, knowledge entities GO , event action classes P . One main challengeswork build scalable system meet requirements.present system mines news sources extract events, constructs canonicalsemantic model, builds causality graph top events. system crawled,4 months, several dynamic information sources (see Section 3.1 details).largest information source NYT archive, optical character recognition(OCR) performed. overall gathered data spans 150 consecutive years(1851 2009).generalization objects, system automatically reads Web content extracts world knowledge. knowledge mined structured semi-structuredpublicly available information repositories. generation causality graph distributed 20 machines, using MapReduce framework. process efficiently unitesdifferent sources, extracts events, disambiguates entities. resulting causality graphcomposed 300 million entity nodes, one billion static edges (connecting different objects encountered events), 7 million causality edges (connectingevents found Pundit cause other). rule generatedusing average 3 instances standard deviation 2.top causality graph, search indexing infrastructure built enablesearch millions documents. highly scalable index allows fast walkgraph events, enabling efficient inference capabilities prediction phasealgorithm.3.1 World Knowledge Miningentity graph Go composed concepts Wikipedia, ConceptNet (Liu & Singh,2004), WordNet (Miller, 1995), Yago (Suchanek, Kasneci, & Weikum, 2007), OpenCyc;billion labeled edges graph Go predicates ontologies.section describe process knowledge graph created searchsystem built upon it.system creates entity graph collecting content, processing feeds,processing formatted data sets (e.g., Wikipedia). crawler archives documents raw format, transforms RDF (Resource Description Framework)format (Lassila, Swick, Wide, & Consortium, 1998). concepts interlinked humans part Linked Data project (Bizer et al., 2009). goal Bizer et al.s(2009) Linked Data project extend Web interlinking multiple datasets RDFsetting RDF links data items different data sources. Datasets includeDBPedia (a structured representation Wikipedia), WordNet, Yago, Freebase, more.September 2010 grown 25 billion RDF triples, interlinked around 395million RDF links.use SPARQL queries way searching knowledge graph. Experimentsperformance queries Berlin benchmark (Bizer & Schultz, 2009) providedevidence superiority Virtuoso open source triple structures task.655fiRadinsky, Davidovich & Markovitch3.2 Causality Event Mining Extractionsupervised learning algorithm requires many learning examples able generalizewell. amount temporal data extremely large, spanning millions articles,goal obtaining human annotated examples becomes impossible. therefore provideautomatic procedure extract labeled examples learning causality dynamiccontent. work, used NYT archives years 1851 2009, WikiNews,BBC 14 million articles total (see data statistics Table 1). Extractingcausal relations events text hard task. state-of-the-art precisiontask around 37% (Do, Chan, & Roth, 2011). hypothesisinformation regarding event found headlines. structuredtherefore easier analyze. Many times headline contain causeeffect event. assume headlines describing eventsdeveloped extraction algorithm identify headlines extract eventsthem. News headlines quite structured, therefore accuracy stage(performed representative subset data) 78% (see Section 4.2.1). systemmines unstructured natural language text found headlines, searches causalgrammatical patterns. construct patterns using causality connectors (Wolff, Song,& Driscoll, 2002; Levin & Hovav, 1994). work used following connectors:1. Causal Connectives: words because, as, connectors.2. Causal prepositions: words due of.3. Periphrastic causative verbs: words cause lead to.constructed set rules extracting causality pair. rule structured as:hPattern, Constraint, Priorityi, Pattern regular expression containing causalityconnector, Constraint syntactic constraint sentence patternapplied, Priority priority rule several rules matched.following constraints composed:1. Causal Connectives: pattern [sentence1] [sentence2] used following constraints: [sentence1] cannot start when, how, where, [sentence2]cannot start all, hours, minutes, years, long, decades. pattern[sentence1], [sentence2] add constraint [sentence1] cannot startnumber. pattern match sentence Afghan vote, complaintsfraud surface match sentence 10 years Lansing, statelawmaker Tom George returns. pattern [sentence1] [sentence2] usedconstraint [sentence2] verb. Using constraint, patternmatch sentence Nokia cut jobs tries catch rivals,sentence civil rights photographer unmasked informer.2. Causal prepositions: pattern [sentence1][because of, due to] [sentence2]required constraints [sentence1] start when, how, where.3. Periphrastic causative verbs: pattern [sentence1] [leads to, Leads to, leadto, Lead to, led to, Led to] [sentence2] used, [sentence1] cannot con656fiLearning Predict Textual Datatain when, how, where, prefix cannot study studies. Additionally, consider periphrastic causative verbs, allow additional verbs[sentence1] [sentence2].result rule application pair sentences one tagged cause, onetagged effect.Given natural-language sentence (extracted article headline), representingevent (either learning prediction), following procedure transformsstructured event:1. Root forms inflected words extracted using morphological analyzer derivedWordNet (Miller, 1995) stemmer. example, article headline10/02/2010: U.S. attacks kill 17 militants Pakistan, words attacks, killedmilitants transformed attack, kill, militant respectively.2. Part-Of-Speech tagging (Marneffe, MacCartney, & Manning, 2006) performed,verb identified. class verb identified using VerbNet vocabulary(Kipper, 2006), e.g., kill belongs P =murder class.3. syntactic template matching verb applied extract semantic relationsthus roles words (see example Figure 10). templates basedVerbNet, supplies verb class set syntactic templates.templates match syntax thematic roles entities sentence.match templates even continuous sentence tree. allowsmatch sentence even auxiliary verb subjectmain transitive verb. example, template NP1 V NP2,transforms NP1 Agent, NP2 Patient. Therefore, match U.S. attacksActor, militant Patient . template matched,sentence transformed typed-dependency graph grammatical relations(Marneffe et al., 2006). example, U.S. attacks identified subjectsentence (candidate Actor), militants object (candidate Patient),Pakistan preposition (using Locations lexicons). Using analysis,identify Location Pakistan.4. word Oi mapped Wikipedia-based concept. word matchesone concept, perform disambiguation computing cosine similaritybody news article body Wikipedia article associatedconcept. example, U.S matched several concepts, UnitedStates, University Salford, Us (Brother Ali album). similarcontent Wikipedia concept United States. word Oi foundWikipedia, treated constant, i.e., generalization applied it,used similarity calculation. is, distGen (const1 , const2 ) = 0const1 = const2 , distGen (const1 , const2 ) = k otherwise. experiments,set k = 4, length longest distance found two conceptsGO .5. time event time publication article news, e.g.,=10/02/2010.657fiRadinsky, Davidovich & MarkovitchData SourceNYTBBCWikiNewsNumber Titles Extracted14,279,387120,44525,808Table 1: Data Summary.Time5QuanGerkillAcGonTroopsAPributeAfghan1/2/198711:15AM +(3h)Time-frameEvent25 Afghan troops killedArmybombard1/2/198711:00AM +(2h)US ArmyTime-frameWeaponswarehouseAcGonUSEvent1LocaGonKabulInstrumentUS Army bombards weaponswarehouse Kabul missilesMissilesFigure 9: pair events causality graph. first represents cause eventsecond represents effect event. extracted headline published 1/2/1987: 5 Afghan troops killed US army bombards warehouseKabul.example, final result event e = hMurder-Class, United States America,Militant, NULL, Pakistan, 10/02/2010i . final result stage causality graphcomposed causality event pairs. events structured described Section 2.1.illustrate pair Figure 9.certain cases, additional heuristics needed order deal brevitynews language. used following heuristics:1. Missing Context McDonalds recalls glasses due cadmium traces, extracted event cadmium traces needs additional context Cadmium traces [in McDonalds glasses]. object missing, first sentence ([sentence1]) subjectused.658fiLearning Predict Textual DataClass Hit-18.1Roles Restrictions:Agent[int control] Patient[concrete] Instrument[concrete]Members: bang, bash, hit, kick, . . .Frames:ExampleSyntaxSemanticscause(Agent, E)manner(during(E),directedmotion, Agent)!contact(during(E),Paula hit ball Agent V PatientAgent, Patient)manner(end(E),forceful,Agent)contact(end(E), Agent,Patient)Figure 10: VerbNet Template.2. Missing entities verbs text 22 dead structured event 22[people] [are] dead. number appears subject, word people addedused subject, added verb.3. Anaphora resolution text boy hangs sees reports Husseinsexecution modeled [boy1 ] sees reports Husseins execution causes [boy1 ]hangs [boy1 ] (Lappin & Leass, 1994).4. Negation text Matsui still playing despite struggles modeled as:[Matsui] struggles causes event Matsui [not] playing. Modeling preventiveconnectors (e.g., despite) requires negation modeled event.4. Experimental Evaluationsection, describe set experiments performed evaluate abilityalgorithms predict causality. first evaluate predictive precision algorithm,continue analyzing part algorithm separately, conclude qualitative evaluation.4.1 Prediction Evaluationprediction algorithm trained using news articles period 1851 2009.world knowledge used algorithm based Web resource snapshots (Section 3)dated 2009. evaluation performed separate data Wikinews articlesyear 2010. refer data test data.task tackled algorithm addressed before, could findbaseline algorithm compare against. therefore decided compare algorithmsperformance human predictors. algorithm human competitorsassigned basic task predicting event given event might cause. evaluateprediction using two metrics. first metric accuracy: whether predicted659fiRadinsky, Davidovich & Markovitchevent actually occurred real world. two possible problems metric.First, predicted event, though plausible, still might actually occurred realworld. Second, predicted event might happened real worldcaused given event, example, trivial predictions always true (thesun rise). therefore use additional metric, event quality, likelihoodpredicted event caused given event.experiments conducted follows:1. Event identification algorithm assumes input predictor hevent. find news headlines represent event, randomly sample n = 1500headlines test data. headline, human evaluator requesteddecide whether headline event cause events. denoteset headlines labeled events E. randomly sample k = 50 headlinesE. denote group C.2. Algorithm event prediction headline ci C, Pundit performs event extraction, produces event P undit(ci ) highest score causedevent represented ci . Although system provides ranked list results,simplify human evaluation theses results, consider highest scoreprediction. tie top score, pick one random. resultsstage pairs: {(ci , P undit(ci ))|ci C}.3. Human event prediction event ci C, human evaluator asked predictevent might cause. evaluator instructed read given headlinepredict likely outcome, using online resource time limit.evaluators presented empty structured forms 5 fieldsoutput event need provide. human result denoted human(ci ).results stage pairs: {(ci , human(ci ))|ci C}.4. Human evaluation results(a) Quality: present = 10 people triplet (ci , human(ci ), P undit(ci )).human evaluators asked grade (ci , human(ci )) (ci , P undit(ci ))scale 0-4 (0 highly implausible prediction 4 highly plausibleprediction). allowed use resource limited time.human evaluators different performed predictions.(b) Accuracy: predicted event, checked news (and Web resources), year time cause event, see whetherpredicted events reported.Human evaluation conducted using Amazon Mechanical Turk, emerging utilityperforming user study evaluations, shown precise certain tasks(Kittur, Chi, & Suh, 2008). evaluation, tasks created routing questionrandom users obtaining answers. filtered raters using CAPTCHA.restricted US-based users, events used system extractedNYT. perform manual filtering results. average timeshuman tasks reported table 2. observed time-consuming660fiLearning Predict Textual DataHuman EventIdentification1 min 26 secHuman EventPrediction4 min 10 secHuman Evaluation(Quality)1 min 44 secHuman Evaluation(Accuracy)6 min 24 secTable 2: Response times human evaluators different evaluation tasks.PunditHumans[0-1)[1-2)[2-3)[3-4]AverageQuality0023192429233.082.86Table 3: Quality results. histogram rankings users humansalgorithm.task humans verify event indeed happened past. timeconsuming task Human Event Prediction. surprising, cases requireduse external resources, whereas quality evaluation measured whetherevents make sense. Additionally, manually investigated human evaluationscategory, find correlation response time quality humanprediction. used Mechanical Turk, know external resourcesevaluators used. measured inter-rater reliability using Fleiss kappa statistical test,measures consistency ratings. raters test, obtained= 0.3, indicates fair agreement (Landis & Koch, 1977; Viera & Garrett, 2005).result quite significant, following reasons:1. Conservativeness measure.2. Subjectivity predictions asking people whether prediction makes sense oftenleads high variance responses.3. Small dataset tests performed 10 people asking categorize 5different scales plausibility 50 examples.4. Lack formal guidelines evaluating plausibility prediction instructions given human evaluators regarding considered plausible not.Additionally, comparison, similar tasks natural language processing, sentenceformality identification (Lahiri, Mitra, & Lu, 2011), usually reach kappa values 0.1 0.2.quality evaluation yielded Pundits average predictive precision 3.08/4 (3plausible prediction), compared 2.86/4 humans. event, averageresults rankers, producing average score algorithms performanceevent, averaged score human predictors (see Table 3). performedpaired t-test k paired scores. advantage algorithm humanevaluators found statistically significant, p 0.05.661fiRadinsky, Davidovich & MarkovitchAlgorithmPunditHumansAverage Accuracy63%42%Table 4: Prediction accuracy human algorithm.accuracy results reported Table 4. performed Fishers exact test (asresults binary) k paired scores. results found statisticallysignificant, p 0.05.4.2 Component Analysissection, report results empirical analysis different partsalgorithm.4.2.1 Evaluation Extraction ProcessSection 3.1, described process extracting causality pairs news.pairs used training set learning algorithm. process consists twomain parts: causality identification event extraction. perform set experimentsprovide insights extracted training data quality.Causality Extraction Experiment first step building training set consistsusing causality patterns extract pairs sentences causality relation holds.assess quality process, randomly sampled 500 pairs trainingset presented human evaluators. pair evaluated 5 humans.filtered raters using CAPTCHA filtered outliers. evaluators showntwo sentences system believed causally related asked evaluateplausibility relation scale 0-4.results show averaged precision extracted causality events 3.114 (78%), 3 means plausible causality relation, 4 means highly plausiblecausality relation. example, causality pair: pulling car 2 New Jerseypolice officers shot, got high causality precision score, plausible causeeffect relation, system extracted headline 2 New Jersey Police OfficersShot Pulling Car.comparison, temporal rule extraction systems (Chambers, Wang, & Jurafsky,2007) reach precision 60%. better performance system explaineduse specially crafted templates (we attempt solve general problemtemporal information extraction).causality pairs extracted judged high quality. main reasonerrors events, although reported news matching templatesdescribed, common-sense causality knowledge. example, Aborted landingBeirut Hijackers fly airliner Cyprus, rated unlikely causally related,although event took place April 09, 1988.662fiLearning Predict Textual DataQuality PrecisionAction93%Actor74%Object76%Instrument79%Location79%Time100%Table 5: Extraction precision 5 event components using causality patterns.ActorMatching84%ObjectMatching83%InstrumentMatching79%LocationMatching89%ActionMatching97%Table 6: Entity-to-ontology matching precision.Event Extraction Experiment pair sentences determined casualty relation, algorithm extracts structured event sentences.event includes following roles: action, actor, object, instrument, time.assess quality process, used 500 pairs previous experiment presented 1000 associated sentences 5 human evaluators.evaluators shown sentence together extracted roles: action, actor, object,instrument, time, asked mark role assignment rightwrong.Table 5 shows precision extracted event components ranges 74100%. comparison, works (Chambers & Jurafsky, 2011) extracting entitiesdifferent types relations reach 42 53% precision. higher precision resultsmainly due use domain-specific templates.performed additional experiments evaluate matching every entityexperiment world-knowledge ontology. matching based semanticsimilarity. ranker asked indicate whether extracted entity mappedcorrectly Wikipedia URI. results summarized Table 6.4.2.2 Evaluation Event Similarity Algorithmlearning prediction algorithms strongly rely event similarity functiondist described Section 2.4. evaluate quality function, randomly sampled30 events training data found similar event entirepast data (according similarity function). human evaluator askedevaluate similarity events scale 15. repeated experiment,replacing average aggregator function f minimum maximum functions.results presented Table 7. general precision average functionhigh (3.9). Additionally, average function performed substantially (confirmedt-test) better minimum maximum. result indicates distance functions aggregate several objects structured event (ratherselecting minimum maximum one events) yield highest performance.663fiRadinsky, Davidovich & MarkovitchMinimum1.9Maximum3.5Average3.9Table 7: Comparison different aggregations event-similarity f .4.2.3 Importance AbstractionGiven cause event whose effect wish predict, use algorithm describedSection 2.4 identify similar generalized events. evaluate importance stage,compose alternative matching algorithm, similar nearest-neighbor approach(as applied work Gerber, Gordon, & Sagae, 2010), matches cause eventcause events training data. Instead building abstraction tree, algorithmsimply finds closest cause past based text similarity. rank matchedresults using TF-IDF measure.applied original algorithm baseline algorithm 50 eventsused prediction. event, asked human evaluator compare predictionoriginal baseline algorithm. results showed 83% casespredictions generalization rated plausible nearestneighbor approach without generalization.4.2.4 Analysis Rule Generation Applicationorder generate appropriate prediction respect given cause event,learned rule applied, described Section 2.5. observe 31% predictions, non-trivial rule generated applied (that is, non-NULL rulesimply output effect observed matched past cause-effect pair example).those, application predicted correctly 90% cases generatedplausible object effect. results indicate generalization rule-generationtechniques essential performance algorithm.4.2.5 Analysis Pruning Implausible Causationeliminate situations generated prediction implausible, devised algorithm (Section 2.7) prevents implausible predictions. randomly selected 200predictions algorithm predictions based human-labeled events extractedWikinews articles (see Section 4.1). human rater requested label predictions considered implausible. applied filtering rules 200predictions well. algorithm found 15% predictions implausible 70%precision 90% recall respect human label. qualitative example filteredprediction Explosion surrender cause event Explosion Afghanistan killstwo.4.3 Qualitative Analysisbetter understanding algorithms strengths weaknesses presentexamples results. Given event Louisiana flood, algorithm predicted[number] people flee. prediction process illustrated Figure 11.664fiLearning Predict Textual Data1. Raw data:prediction based following raw news articles:(a) 150000 flee hurricane nears North Carolina coast.(b) million flee huge storm hits Texas coast.(c) Thousands flee storm whips coast Florida.(d) Thousands Dallas Flee Flood Severe Storms Move Southwest.2. Causality pair extraction:template used process headlines following structured events:(a) Cause Event: near (Action); hurricane (Actor); Coast(Object); North Carolina(Object Attribute) ; (Instrument); Carolina (Location); 31 Aug 1993 (Time).Effect Event: flee (Action); People (Actor); 150000(Actor Attributes); Carolina(Location); 31 Aug 1993 (Time).(b) Cause Event: hit (Action); Storm (Actor); Huge (Actor Attributes); Coast(Object);Texas (Object Attribute); Texas (Location); 13 Sep 2008 (Time).Effect Event: flee (Action); People (Actor); million(Actor Attributes); Texas(Location); 13 Sep 2008 (Time).(c) Cause Event: whip (Action); Storm (Actor); Coast(Object); Florida (ObjectAttribute); Florida (Location); March 19, 1936 (Time).Effect Event: flee (Action); People (Actor); thousands(Actor Attributes); Florida(Location); March 19, 1936 (Time).(d) Cause Event: move (Action); Storm (Actor); Severe (Actor Attributes); Dallas(Location); May 27, 1957 (Time).Effect Event: flee (Action); People (Actor); thousands(Actor Attributes);Flood(Object); Dallas (Location); May 27, 1957 (Time).3. Learning abstraction tree:four events clustered together AT. clusterednode causes found similar: actors weatherhazards location state United States. effects foundsimilar actions actors similar across events, actorattributes numbers. generalization, following world knowledgeused:(a) Storm, hurricane flood weather hazards (extracted in-categoryrelation Wikipedia).(b) Carolina, Texas, California located United States (extractedlocated-in relation Yago).4. Prediction:665fiRadinsky, Davidovich & Markovitchprediction, event Louisiana flood (which occur trainingexamples) found similar node, node rule output[number] people flee.150000 flee hurricanenears north Carolina coast.million fleehuge storm hitsTexas coast.Cause Event: near (Action);hurricane (Actor);Coast(Object);North Carolina (Object Attribute) ;(Instrument); Carolina (Location);31 Aug 1993 (Time).Effect Event: flee (Action);People (Actor);150000(Actor Attributes);Carolina (Location);31 Aug 1993 (Time).Storm, Hurricane Flood`Weather hazards''Carolina, Texas, California, Nebraska``United States''Louisiana(Location);Flood (Actor)people (Actor);flee (Action)Implausibleevent lterFigure 11: Examples predictionanother example, given event 6.1 magnitude aftershock earthquake hits Haiti,highest matching predictions were: [number] people dead, [number] peoplemissing, [number] magnitude aftershock earthquake strike island near Haitiearthquake turn United States Virgin Islands. first three predictions seemreasonable. example, third prediction came rule natural disastershitting countries next shore tend affect nearby countries. case predictedearthquake affect United States Virgin Islands, geographicallyclose Haiti. fourth prediction, however, realistic earthquake cannotchange course. created match past example tornado hittingcountry coast. implausible causation filters prediction, lowPMCI, output system [number] people dead. examplealso interesting, issues prediction using spatial locality (the United States VirginIslands [near] Haiti).Additional examples 50 test predictions seen Table8.666fiLearning Predict Textual DataCause eventHuman-predicted effect eventAl-Qaida demands hostageexchangeAfghanistans parliament rejects Karzais cabinet nominationsRemains 1912 expeditionplane found AntarcticaAl-QaidaexchangeshostageParliamentacceptsKarzais cabinet nominationsEurope museums vie remainsNorth Korea seeks diplomaticrelations USVolcano erupts DemocraticRepublic CongoIcelands President vetoes repayment Ice save lossesDeath toll Brazil mudslides rises sixty7.0 magnitude earthquakestrikes Haitian coast2 Palestinians reportedly shotdead Israeli troopsProfessor Tehran University killed bombingUN officials offer mediationservicesScientists RepublicCongo investigate lava bedsBanks Reykjavik reportrecord withdrawalsRescuers Brazil abandon rescue effortsTsunami Haiti affectscoastIsraeli citizens protestPalestinian leadersTehran students rememberslain professor memorialserviceMafia kills people gunstownIslamist group would adoptanother name UKGerman officials suspend tariffsSomeone firedAlleged drug kingpin arrestedMexicoUK bans Islamist groupChina overtakes Germanyworlds biggest exporterCocaine found KennedySpace CenterAlgorithm-predicted effect eventcountry refusedemandMany critics rejectionEnduringmysterysolvedAntarcticaNorth Korea riftgrowThousands peopleflee CongoOfficial administration reaction issuedTestimoniesheardTsunami warningissuedIsraeli troopsface scrutinyProfessorfuneralheldKingpin sentprisonGroup growWheat price fallPeople arrestedTable 8: Human algorithm predictions events. Predictions bold labeledevaluators correct predictions.4.4 Discussionexperiments report precision algorithms. experimentsmeasuring recall system necessary. However, experiments validation step required human intervention. example, validating prediction occurredfuture news. order perform full recall experiment one apply algorithmnews headlines reported certain day measure appearance667fiRadinsky, Davidovich & Markovitchcorresponding predictions future news. Unfortunately, performing human validationlarge prediction space hard. leave task performing experimentsprovide rough estimate recall future work.common practice compare system performance previous systems tacklingproblem. However, ambitious task tackled work immediatebaselines compare with. is, comparable system neither scaleability take arbitrary cause event natural language output effect eventnatural language. Instead, compared agents know capable performingtask humans.Although results indicate superiority system human agents,claim system predictions perform better humans. rather provideevidence system provides similar predictions humans, sometimeseven outperforms human ability predict, supported superioritysystem accuracy evaluation.fully support claim superiority system humans, wider experimentsperformed. Experiments larger order magnitude provide resultshigher agreement raters shed light different types eventssystems performance better. Additionally, experiments comparing systemperformance experts fields individual prediction valuablewell. point, assume performance experts would higheralgorithm. main reason causality knowledge used trainalgorithms. knowledge extracted headlines tend simple causalitycontents, easily understandable general population. type knowledgelimits complexity predictions made Pundit. Pundit predictionstherefore tend closer common knowledge average human. orderpredict complex events would need rely better training examples newsheadlines alone.evaluation presented section provides evidence quality predictions system provide. results impressive sensecomparable humans, thus providing evidence ability machineperform one desirable goals general AI.5. Related Workaware work attempts perform task face: receive arbitrarynews events natural language representation predict events cause. Severalworks, however, deal related tasks. general, work focus better information extraction causality extraction techniques, rather informationleveraged prediction. present novel methods combining world knowledgeevent extraction methods represent coherent events, present novel methodsrule extraction generalization using knowledge.5.1 Prediction Web Behavior, Books Social MediaSeveral works focused using search-engine queries prediction traditionalmedia (Radinsky, Davidovich, & Markovitch, 2008) blogs (Adar, Weld, Bershad, &668fiLearning Predict Textual DataGribble, 2007). Ginsberg et al. (2009) used queries predicting H1N1 influenza outbreaks. context causality recognition, Gordon, Bejan, Sagae (2011) presentmethodology mining blogs extract common-sense causality. evaluation donehuman-labeled dataset test consists fact two possible effects. Applying point-mutual information personal blog stories, authors select best predictioncandidate. work differs authors focus personal commonsense mining consider whether predictions actually occurred. worksfocused predicting Web content change itself. example, Kleinberg (2002, 2006) developed general techniques summarizing temporal dynamics textual contentidentifying bursts terms within content. Similarly, Amodeo, Blanco, Brefeld (2011)built time series model publication dates documents relevant query orderpredict future bursts. Social media used predict riots (Kalev, 2011) movie boxoffice sales (Asur & Huberman, 2010; Joshi, Das, Gimpel, & Smith, 2010; Mishne, 2006).works (Jatowt & Yeung, 2011; Yeung & Jatowt, 2011; Michel, Shen, Aiden, Veres,Gray, Google Books Team, Pickett, Hoiberg, Clancy, Norvig, Orwant, Pinker, Nowak, &Aiden, 2011) explored use text mining techniques news books explainculture develops, peoples expectations memories are.work differs several ways: First, present general-purposeprediction algorithm rather domain-specific one. Second, unlike works,combines variety heterogenous online sources, including world knowledge minedWeb. Finally, focus generation future event predictions representedentirely natural language, provide techniques enrich generalize historicalevents purpose future event prediction.5.2 Textual Entailmentrelated topic work textual entailment (TE) (Glickman, Dagan, & Koppel,2005). text said entail textual hypothesis h people reading agreemeaning implies truth h. TE divided three main categories:recognition, generation, extraction. section, provide short summaryfirst two categories. detailed overview refer reader surveyAndroutsopoulos Malakasiotis (2010). discuss specific task causalityextraction text Section 5.3.4.5.2.1 Textual Entailment recognitiontask, pairs texts given input, output whether TE relations holdpair. approaches map text logical expressions (with semantic enrichment, using WordNet, example) perform logical entailment checks, usually usingtheorem provers (Raina, Ng, & Manning, 2005; Bos & Markert, 2005; Tatu & Moldovan,2005). approaches map two texts vector space model, wordmapped strongly co-occurring words corpus (Mitchell & Lapata, 2008),similarity measures vectors applied. measure syntactic similarity applying graph similarity measure syntactic dependency graphs twotexts (Zanzotto, Pennacchiotti, & Moschitti, 2009). Similarly, methods measuresemantic distance similarity words text (Haghighi, 2005), usually exploiting669fiRadinsky, Davidovich & Markovitchresources WordNet well. last set approaches represents two textssingle feature vector trains machine learning algorithm, later, given twonew texts represented via vector, determine whether entail (Bos &Markert, 2005; Burchardt, Pennacchiotti, Thater, & Pinkal, 2009; Hickl, 2008). example, Glickman et al. (2005) show naive Bayes classifier trained lexical features, i.e.,number times words appeared words h. features usually includepolarity (Haghighi, 2005), whether theorem prover managed prove entailment (Bos& Markert, 2005), tagging named entities categories people, organizations,locations.5.2.2 Textual Entailment Generationdiscuss TE generation, where, given expression, system outputset expressions entailed input. task closely relatedone presented work: TE generation, text received entailed textgenerated output. Androutsopoulos Malakasiotis (2010) mention benchmarksexist evaluate task, common costly approach evaluate usinghuman judges. also encountered difficulty task, performed humanevaluation.TE generation methods divided two types: use machine translation techniques use template-based techniques. use machinetranslation techniques try calculate set transformations highest probability, using training corpus. Quirk, Brockett, Dolan (2004) cluster news articlesreferring event, select pairs similar sentences, apply aforementionedtechniques. methods use template-based approaches large corpora,Web. methods (Idan, Tanev, & Dagan, 2004) start initial seed sentences(composed entities), use search engine find entities entailment relations hold. relations used templates. find additional entitiesrelations hold, relations searched again. TE generation system, given text, matches template outputs texts matchedtemplate. Others (Ravichandran, Ittycheriah, & Roukos, 2003) also add additionalfiltering techniques templates.work closely related template-based approach. craftednew set templates extract causality pairs news.5.3 Information ExtractionInformation Extraction study automatic extraction information unstructured sources. categorizes types information extracted three types: entities,relationships entities, higher-order structures tables lists.closely related tasks entity extraction relation extraction;rest refer reader survey Sarawagi (2008). former task, similarprocess extracting concepts, deals extracting noun phrases text.latter task, given document relation input, problem extract entitypairs document relation holds. Whereas works dealone element problem extraction information needed understand given670fiLearning Predict Textual Datacausality, deal actual causality prediction. claim createprecise information extraction methods, rather try leverage knowledgeperform important AI task future event prediction.5.3.1 Entity Extractionentity extraction, two categories methods exist rule-based statistical methods.Rule-based methods (Riloff, 1993; Riloff & Jones, 1999; Jayram, Krishnamurthy, Raghavan,Vaithyanathan, & Zhu, 2006; Shen, Doan, Naughton, & Ramakrishnan, 2007; Ciravegna,2001; Maynard, Tablan, Ursu, Cunningham, & Wilks, 2001; Hobbs, Bear, Israel, & Tyson,1993) define contextual patterns consisting regular expression features entitiestext (e.g., entity word, part-of-speech tagging). rules either manuallycoded domain expert learned using bottom-up (Ciravegna, 2001; Califf & Mooney,1999) top-down learners (Soderland, 1999). Others follow statistical methods definenumerous features sentence treat problem classification problem,applying well-known machine learning algorithms (e.g., Hidden Markov Models; Agichtein& Ganti, 2004; Borkar, Deshmukh, & Sarawagi, 2001). system dealmany challenges field, propose large scale domain-specific approach drivenspecific extraction templates.5.3.2 Relation ExtractionRelation extraction developed widely last years large text corpora(Schubert, 2002) and, particular, different Web resources, general Webcontent (Banko et al., 2007; Carlson et al., 2010; Hoffmann, Zhang, & Weld, 2010), blogs(Jayram et al., 2006), Wikipedia (Suchanek et al., 2007), news articles (e.g., topicdetection tracking task (Section 5.3.3)). Given two entities, first task domain classify relationship. Many feature-based methods (Jiang & Zhai, 2007;Kambhatla, 2004; Suchanek, 2006) rule-based methods (Aitken, 2002; Mcdonald, Chen,Su, & Marshall, 2004; Jayram et al., 2006; Shen et al., 2007) developedtask. methods use different features extracted text, words,grammar features, parse tree dependency graphs, features extra ionexternal relation repositories (e.g., Wikipedia Infobox) add additional features (Nguyen& Moschitti, 2011; Hoffmann, Zhang, Ling, Zettlemoyer, & Weld, 2011). Labeled trainingexamples, feature extracted, fed machine learning classifier, sometimes using transformations kernels (Zhao & Grishman, 2005; Zhang,Zhang, Su, & Zhou, 2006; Zelenko, Aone, & Richardella, 2003; Wang, 2008; Culotta &Sorensen, 2004; Bunescu & Mooney, 2005; Nguyen, Moschitti, & Riccardi, 2009), which,given new unseen entities, able classify categories.Given relation, second common task domain find entities satisfyrelation. information extraction tasks, task relevant ours,try find structured events causality relation holds. worksdomain focus large collections, Web, labeling entities relationsinfeasible (Agichtein & Gravano, 2000; Banko et al., 2007; Bunescu & Mooney, 2007;Rosenfeld & Feldman, 2007; Shinyama & Sekine, 2006; Turney, 2006). Usually seed entitydatabases used, along manual extraction templates, expanded671fiRadinsky, Davidovich & Markovitchfiltered iteratively. Sarawagi states spite extensive research topic,relationship extraction means solved problem. accuracy values still rangeneighborhood 50%70% even closed benchmark datasets . . . open domainslike Web, state-of-the-art systems still involve lot special case handlingcannot easily described principled, portable approaches. (Sarawagi, 2008, p. 331).Similarly, task size corpus allow us assume labeled sets.Instead, like common approaches presented here, also start predefined setpatterns.5.3.3 Temporal Information Extractiontemporal information extraction task deals extraction ordering eventsmany events time. Temporal information extraction categorized threemain subtasks predicting temporal order events time expressions describedtext, predicting relation events, identifying documentwritten. task found important many natural language processingapplications, question answering, information extraction, machine translationtext summarization, require mere surface understanding.approaches (Ling & Weld, 2010; Mani, Schiffman, & Zhang, 2003; Lapata & Lascarides, 2006; Chambers et al., 2007; Tatu & Srikanth, 2008; Yoshikawa, Riedel, Asahara,& Matsumoto, 2009) learn classifiers predict temporal order pair eventspredefined features pair.related works deal topic detection tracking (Cieri, Graff, Liberman,Martey, & Strassel, 2000). area includes several tasks (Allan, 2002). them,multiple, heterogenous new sources used, including audio. story segmentation taskaims segment data constituent stories. topic tracking task e.g., workShahaf Guestrin (2010) aims find stories discussing certain topic. subtasklink detection task which, given pair stories, aims classify whethertopic. topic detection task e.g. works Ahmed, Ho,Eisenstein, Xing, Smola, Teo (2011) Yang, Pierce, Carbonell (1998) aimsdetect clusters topic-cohesive stories stream topics. first-story detection taskaims identify first story topic (Jian Zhang & Yang, 2004). paper,focused short text headlines extraction events them. work differstemporal information extraction, generate predictions futureevents, whereas temporal information extraction tasks focus identifying clusteringtext corpus topics.5.3.4 Causality Pattern Extraction Recognitionfirst stage learning process extract causality pairs text. Causalityextraction discussed literature past, dividedfollowing subgroups:1. Use handcrafted domain-specific patterns. studies deal causality extraction using specific domain knowledge. Kaplan Berry-Rogghe (1991) used scientifictexts create manually designed set propositions later applied672fiLearning Predict Textual Datanew texts extract causality. methods require handcrafted domain knowledge,problematic obtain real-world tasks, especially large amounts.2. Use handcrafted linguistic patterns. works use general approachapplying linguistic patterns. example, Garcia (1997) manually identified 23causative verb groups (e.g., result in, lead to, etc.). sentence contained oneverbs, classified containing causation relation. precision 85%reported. Khoo et al. (2000) used manually extracted graphical patterns basedsyntactic parse trees, reporting accuracy 68% English medicaldatabase. Similarly, Girju Moldovan (2002) defined lexicon-syntactic patterns(pairs noun phrases causative verb between) additional semanticconstraints.3. Semi-supervised pattern learning approaches. set approaches uses supervisedmachine learning techniques identify causality text. example, Blanco et al.(2008) Sil et al. (2010) use syntactic patterns features later fedclassifiers, whose output whether text implies causality cause effectthemselves.4. Supervised pattern learning approaches. many works designinference rules discover extraction patterns given relation using training examples (Riloff, 1996; Riloff & Jones, 1999; Agichtein & Gravano, 2000; Lin & Pantel,2001). Specifically, Chan Lam (2005) dealt problem creating syntacticpatterns cause-effect extraction.domain causality pattern extraction, work resembles handcraftedlinguistic patterns pattern approaches. evaluated performance specificdomain. Since goal obtain precise set examples feedlearning, chose follow approach well.5.4 Learning Causalitydrawn algorithmic motivation work machine learningcommunity. section, give partial review main areas machine learningrelevant work.5.4.1 Bayesian Causal Inferencefunctional causal model (Pearl, 2000) assumes set observables X1 . . . Xn ,vertices directed acyclic graph G. semantics graph parentsnode directed causes. shown satisfy Reichenbachs common cause principle,states node Z children X, , X statistically dependent,Z causally influencing both. model, similar Bayesian network,satisfies several conditions: (1) Local Causal Markov condition: node statisticallyindependent non-descendants, given parents; (2) Global CausalQ Markov condition: dseparation criterion; (3) Factorization criterion: P (X1 , . . . , Xn ) = P (Xi |P arents(Xi )).theoretical literature inference learning causality models extensive.models resemble work use structural models. literature inference673fiRadinsky, Davidovich & Markovitchlearning causality models extensive, knowledge solutionsscale scope tasks discussed paper. contrast Bayesian approach,causality graph work contains less detailed information. work combinesseveral linguistic resources learned data several heuristics buildcausality graph.5.4.2 Structured Learningimportant problem machine learning field structured learning, inputoutput classifier complex structure, relational domain,object related another, either time features. task resembles structuredlearning also use structured input (structured events given input) producestructured event output.Many generative models developed, including hidden Markov models, Markovlogic networks, conditional random fields, among others. approaches use transformations, kernels, unite objects, ignoring structure, feedstandard structured classifier, e.g., kernelized conditional random fields (Lafferty,Zhu, & Liu, 2004), maximum margin Markov networks (Taskar, Guestrin, & Koller, 2003),others (Bakir, Hofmann, Scholkopf, Smola, Taskar, & Vishwanathan, 2007).dealing complex output, annotated parse trees natural language problems,approaches define distance metric label space objects,apply standard machine learning algorithms, e.g., structured support vector machines(Joachims, 2006).5.4.3 Learning Positive Examples (One Class Classification)system fed examples sort causes b, examples sortcause b, must deal problem learning positive examples only.challenge multi-class learning mechanisms, require negativepositive examples. theoretical studies possibility learningpositive unlabeled data provided work Denis (1998) (probably approximatelycorrect (PAC) learning) Muggleton (1996) (Bayesian learning).works (Tax, 2001; Manevitz & Yousef, 2000; Manevitz, Yousef, Cristianini, ShaweTaylor, & Williamson, 2001) domain develop algorithms use one-class SVM(Vapnik, 1995) learn support using positive distribution. constructdecision boundaries around positive examples differentiate possiblenegative data. Tax Duin (1991) use hyper-sphere defined radius aroundpositive class points (support vector data description method). also usekernel tricks finding sphere (Tax, 2001). Scholkopf et al. (1999, 2000) developmethods try separate surface region positive labeled data regionunlabeled data.6. ConclusionsMuch research carried information extraction ontology building.work, discuss leverage knowledge large-scale AI problem event674fiLearning Predict Textual Dataprediction. present system trained predict future events, using cause eventinput. event represented tuple one predicate 4 general semantic roles.event pairs used training extracted automatically news headlines usingsimple syntactic patterns. Generalization unseen events achieved by:1. Creating abstraction tree (AT) contains entities observed events togethersubsuming categories extracted available online ontologies.2. Finding predicate paths connecting entities cause events entities effectevents, paths extracted available ontologies.discuss many challenges building system: obtaining large enough dataset,representing knowledge, developing inference algorithms requiredtask. perform large-scale mining apply natural language techniques transformraw data 150 years history archives structured representation events,using mined Web-based object hierarchy action classes. shows scalabilityproposed method, crucial method requires large amountsdata work well. However, engineering design analysis performedscale entire knowledge web provide real-time alerts. also shownumerous resources built different people different purposes (e.g., differentontologies) fact merged via concept-graph build system work wellpractice.perform large-scale learning large data corpus present novel inferencetechniques. consider rule extraction generalization. propose novel methodsrule generalization using existing ontologies, believe useful manyrelated tasks. Tasks entailment topic tracking benefitconcepts understanding sequences generalizations.work scratch surface real-time fully functionalprediction system. Due complexity problem, size systemmany components, errors unavoidable. example, errors due noise eventextraction, noise similarity calculation events, etc. Although performexperiments analyzing different components system errors additionoverall system performance, believe additional training examples bettersources knowledge deeper ontologies bring many improvements algorithms.future work, suggest following directions extensions:1. Better event extraction event matching Event extraction techniques, e.g.,proposed et al. (2011) provide higher analysis data entiretext rather titles. Event similarity enriched many ways, e.g.,work compared three aggregation functions f , however, coherentway learning weights Oi past data applied.2. Analysis knowledge sources believe in-depth analysis differenttypes knowledge obtained Web individual contributionsstudied. work, explore sensitivity system initialnoise conceptual networks, believe proper analysisbetter networks provide higher prediction accuracy, already carriedLinkedData community.675fiRadinsky, Davidovich & Markovitch3. Large scale experiments Performance larger experiments humans largerperiods times, even comparison experts provide insightsperformance reliability system. Automation experiments withouthuman involvement measure accuracy predictions make possible providericher metrics performance, recall.4. Time effect work, events treated similarly, even events 100years ago. future directions, wish investigate give decaying weightinformation events system, causality learned eventtook place 1851 might less relevant prediction 2010. However, muchcommon-sense knowledge still used even learned events happenedlong time ago. example, headlines Order Restored Riots (1941)Games Suspended Riot (1962) still relevant today.experimental evaluation showed predictions Pundit algorithmleast good non-expert humans. believe work one firstharness vast amount information available Web perform event predictiongeneral purpose, knowledge based, human-like.ReferencesAdar, E., Weld, D. S., Bershad, B. N., & Gribble, S. D. (2007). search: visualizingpredicting user behavior. Proceedings International ConferenceWorld Wide Web (WWW).Agichtein, E., & Ganti, V. (2004). Mining reference tables automatic text segmentation.Proceedings Tenth ACM SIGKDD International Conference KnowledgeDiscovery Data Mining (KDD).Agichtein, E., & Gravano, L. (2000). Snowball: extracting relations large plain-textcollections. Proceedings Joint Conference Digital Libraries (JCDL), pp. 8594.Ahmed, A., Ho, Q., Eisenstein, J., Xing, E. P., Smola, A. J., & Teo, C. H. (2011). Unifiedanalysis streaming news. Proceedings International ConferenceWorld Wide Web (WWW).Aitken, J. (2002). Learning information extraction rules: inductive logic programmingapproach. Proceedings 15th European Conference Artificial Intelligence(ECAI), pp. 355359.Allan, J. (Ed.). (2002). Topic Detection Tracking: Event-based Information Organization, Vol. 12. Kluwer Academic Publishers, Norwell, MA, USA.Amodeo, G., Blanco, R., & Brefeld, U. (2011). Hybrid models future event prediction.Proceedings ACM Conference Information Knowledge Management(CIKM).Androutsopoulos, I., & Malakasiotis, P. (2010). survey paraphrasing textualentailment methods. Journal Artificial Intelligence Research (JAIR), 38, 135187.Asur, S., & Huberman, B. A. (2010). Predicting future social media. ArxiV.676fiLearning Predict Textual DataBakir, G. H., Hofmann, T., Scholkopf, B., Smola, A. J., Taskar, B., & Vishwanathan, S.V. N. (2007). Predicting Structured Data. MIT Press.Banko, M., Cafarella, M. J., Soderl, S., Broadhead, M., & Etzioni, O. (2007). Open information extraction web. Proceedings International Joint ConferencesArtificial Intelligence (IJCAI).Bizer, C., Heath, T., & Berners-Lee, T. (2009). Linked data story far. InternationalJournal Semantic Web Information Systems (IJSWIS).Bizer, C., & Schultz, A. (2009). berlin sparql benchmark. International JournalSemantic Web Information Systems (IJSWIS).Blanco, E., Castell, N., & Moldovan, D. (2008). Causal Relation Extraction. ProceedingsInternational Conference Language Resources Evaluation (LREC).Borkar, V., Deshmukh, K., & Sarawagi, S. (2001). Automatic text segmentation extracting structured records. Proceedings ACM SIGMOD International ConferenceManagement Data (KDD).Bos, J., & Markert, K. (2005). Recognising textual entailment logical inference.Proceedings Human Language Technology Conference Conference EmpiricalMethods Natural Language Processing (HLT EMNLP).Bunescu, R., & Mooney, R. (2007). Learning extract relations web usingminimal supervision. Proceedings 45th Annual Meeting AssociationComputational Linguistics (ACL), pp. 576583.Bunescu, R. C., & Mooney, R. J. (2005). shortest path dependency kernel relationextraction. Proceedings Conference Human Language TechnologyEmpirical Methods Natural Language Processing (HLT EMNLP), pp. 724731.Burchardt, A., Pennacchiotti, M., Thater, S., & Pinkal, M. (2009). Assessing impactframe semantics textual entailment. Natural Language Engineering, 15, 527550.Califf, M. E., & Mooney, R. J. (1999). Relational learning pattern-match rules information extraction. Proceedings Sixteenth National Conference ArtificialIntelligence (AAAI), pp. 328334.Carlson, A., Betteridge, J., Kisiel, B., Settles, B., Hruschka, E., & Mitchell, T. (2010).Toward architecture never-ending language learning. ProceedingsAssociation Advancement Artificial Intelligence (AAAI).Chambers, N., & Jurafsky, D. (2011). Template-Based Information Extraction withoutTemplates. Proceedings Annual Meeting Association ComputationalLinguistics (ACL).Chambers, N., Wang, S., & Jurafsky, D. (2007). Classifying temporal relationsevents. Proceedings Annual Meeting Association ComputationalLinguistics (ACL) (Poster).Chan, K., & Lam, W. (2005). Extracting causation knowledge natural language texts.International Journal Information Security (IJIS), 20, 327358.677fiRadinsky, Davidovich & MarkovitchCieri, C., Graff, D., Liberman, M., Martey, N., & Strassel, S. (2000). Large, multilingual,broadcast news corpora cooperative research topic detection tracking:tdt-2 tdt-3 corpus efforts. Proceedings International ConferenceLanguage Resources Evaluation (LREC).Ciravegna, F. (2001). Adaptive information extraction text rule inductiongeneralisation. Proceedings 17th International Joint Conference ArtificialIntelligence (IJCAI).Culotta, A., & Sorensen, J. (2004). Dependency tree kernels relation extraction.Proceedings 42nd Meeting Association Computational Linguistics(ACL), pp. 423429.Dang, H. T., Palmer, M., & Rosenzweig, J. (1998). Investigating regular sense extensionsbased intersective levin classes. Proceedings International ConferenceComputational Linguistics (COLING).Denis, F. (1998). PAC learning positive statistical queries. ProceedingsInternational Conference Algorithmic Learning Theory (ALT), pp. 112126.Do, Q., Chan, Y., & Roth, D. (2011). Minimally supervised event causality identification.Proceedings Conference Empirical Methods Natural Language Processing(EMNLP).Eisen, M. B., Spellman, P. T., Brown, P. O., & Botstein, D. (1998). Cluster analysisdisplay genome-wide expression patterns. PNAS, 95, 1486314868.Garcia, D. (1997). Coatis, NLP system locate expressions actions connectedcausality links. Proceedings Knowledge Engineering Knowledge ManagementMasses (EKAW).Gerber, M., Gordon, A. S., & Sagae, K. (2010). Open-domain commonsense reasoning usingdiscourse relations corpus weblog stories. Proceedings FormalismsMethodology Learning Reading, NAACL-2010 Workshop.Ginsberg, J., Mohebbi, M. H., Patel, R. S., Brammer, L., Smolinski, M. S., & Brilliant, L.(2009). Detecting influenza epidemics using search engine query data. Nature, 457,10121014.Girju, R., & Moldovan, D. (2002). Text mining causal relations. ProceedingsAnnual International Conference Florida Artificial Intelligence ResearchSociety (FLAIRS), pp. 360364.Giuglea, A.-M., & Moschitti, A. (2006). Shallow semantic parsing based framenet, verbnet propbank. Proceedings 17th European Conference ArtificialIntelligence (ECAI 2006).Glickman, O., Dagan, I., & Koppel, M. (2005). probabilistic classification approachlexical textual entailment. Proceedings Association AdvancementArtificial Intelligence (AAAI).Gordon, A. S., Bejan, C. A., & Sagae, K. (2011). Commonsense causal reasoning usingmillions personal stories. Proceedings Association AdvancementArtificial Intelligence (AAAI).678fiLearning Predict Textual DataHaghighi, A. D. (2005). Robust textual inference via graph matching. ProceedingsHuman Language Technology Conference Conference Empirical Methods NaturalLanguage Processing (HLT EMNLP).Hickl, A. (2008). Using discourse commitments recognize textual entailment. Proceedings International Conference Computational Linguistics (COLING).Hobbs, J. R., Bear, J., Israel, D., & Tyson, M. (1993). Fastus: finite-state processorinformation extraction real-world text. Proceedings 13th InternationalJoint Conference Artificial Intelligence (IJCAI), pp. 11721178.Hoffmann, R., Zhang, C., Ling, X., Zettlemoyer, L., & Weld, D. S. (2011). Knowledge-basedweak supervision information extraction overlapping relations. Proceedings49th Annual Meeting Association Computational Linguistics: HumanLanguage Technologies (HLT).Hoffmann, R., Zhang, C., & Weld, D. S. (2010). Learning 5000 relational extractors. Proceedings 48th Annual Meeting Association Computational Linguistics(ACL).Idan, I. S., Tanev, H., & Dagan, I. (2004). Scaling web-based acquisition entailmentrelations. Proceedings Conference Empirical Methods Natural LanguageProcessing (EMNLP), pp. 4148.Jatowt, A., & Yeung, C. (2011). Extracting collective expectations futurelarge text collections. Proceedings ACM Conference InformationKnowledge Management (CIKM).Jayram, T. S., Krishnamurthy, R., Raghavan, S., Vaithyanathan, S., & Zhu, H. (2006).Avatar information extraction system. IEEE Data Engineering Bulletin, 29, 4048.Jian Zhang, Z. G., & Yang, Y. (2004). probabilistic model online document clusteringapplication novelty detection. Proceedings Annual ConferenceNeural Information Processing Systems (NIPS).Jiang, J., & Zhai, C. (2007). systematic exploration feature space relationextraction. Proceedings Human Language Technologies ConferenceNorth American Chapter Association Computational Linguistics (HLTNAACL), pp. 113120.Joachims, T. (2006). Structured output prediction support vector machines. Yeung,D.-Y., Kwok, J., Fred, A., Roli, F., & de Ridder, D. (Eds.), Structural, Syntactic,Statistical Pattern Recognition, Vol. 4109 Lecture Notes Computer Science, pp.17. Springer Berlin / Heidelberg.Joshi, M., Das, D., Gimpel, K., & Smith, N. A. (2010). Movie reviews revenues:experiment text regression. Proceedings North American ChapterAssociation Computational Linguistics - Human Language Technologies (NAACLHLT).Kalev (2011). Culturomics 2.0: Forecasting large-scale human behavior using global newsmedia tone time space. First Monday, 15 (9).679fiRadinsky, Davidovich & MarkovitchKambhatla, N. (2004). Combining lexical, syntactic semantic features maximumentropy models information extraction. Proceedings Annual MeetingAssociation Computational Linguistics (ACL), pp. 178181.Kaplan, R., & Berry-Rogghe, G. (1991). Knowledge-based acquisition causal relationshipstext. Knowledge Acquisition, 3, 317337.Khoo, C., Chan, S., & Niu, Y. (2000). Extracting causal knowledge medical databaseusing graphical patterns. Proceedings Annual Meeting AssociationComputational Linguistics (ACL), pp. 336343.Kim, J. (1993). Supervenience mind. Selected Philosophical Essays.Kipper, K. (2006). Extending verbnet novel verb classes. Proceedings International Conference Language Resources Evaluation (LREC).Kittur, A., Chi, H., & Suh, B. (2008). Crowdsourcing user studies mechanical turk.Proceedings ACM CHI Conference Human Factors Computing Systemspremier International Conference human-computer interaction (CHI).Kleinberg, J. (2006). Temporal dynamics on-line information systems. Data StreamManagement: Processing High-Speed Data Streams. Springer.Kleinberg, J. (2002). Bursty hierarchical structure streams. ProceedingsAnnual ACM SIGKDD Conference (KDD).Lafferty, J., Zhu, X., & Liu, Y. (2004). Kernel conditional random fields: Representationclique selection. 21st International Conference Machine Learning (ICML).Lahiri, S., Mitra, P., & Lu, X. (2011). Informality judgment sentence level experiments formality score. Proceedings 12th International ConferenceComputational Linguistics Intelligent Text Processing (CICLing).Landis, & Koch (1977). measurement observer agreement categorical data.Biometrics, 33 (1), 74159.Lapata, M., & Lascarides, A. (2006). Learning sentence-internal temporal relations. JournalArtificial Intelligence Research (JAIR), 27, 85117.Lappin, S., & Leass, H. (1994). algorithm pronominal anaphora resolution. Computational Linguistics, 20, 535561.Lassila, O., Swick, R. R., Wide, W., & Consortium, W. (1998). Resource description framework (rdf) model syntax specification..Lenat, D. B., & Guha, R. V. (1990). Building Large Knowledge-Based Systems: Representation Inference Cyc Project. Addison-Wesley.Levin, B., & Hovav, M. R. (1994). preliminary analysis causative verbs english.Lingua, 92, 3577.Lin, D., & Pantel, P. (2001). Dirt-discovery inference rules text. ProceedingsAnnual ACM SIGKDD Conference (KDD).Ling, X., & Weld, D. (2010). Temporal information extraction. ProceedingsAssociation Advancement Artificial Intelligence (AAAI).680fiLearning Predict Textual DataLiu, H., & Singh, P. (2004). Conceptnet: practical commonsense reasoning toolkit. BTTechnology Journal, 22, 211226.Manevitz, L. M., & Yousef, M. (2000). Document classification neural networks usingpositive examples. Proceedings 23rd Annual International ACM SIGIRConference Research Development Information Retrieval (SIGIR), pp. 304306.Manevitz, L. M., Yousef, M., Cristianini, N., Shawe-Taylor, J., & Williamson, B. (2001).One-class svms document classification. Journal Machine Learning Research,2, 139154.Mani, I., Schiffman, B., & Zhang, J. (2003). Inferring temporal ordering events news.Proceedings North American Chapter Association ComputationalLinguistics - Human Language Technologies (NAACL HLT).Marneffe, M., MacCartney, B., & Manning, C. (2006). Generating typed dependency parsesphrase structure parses. Proceedings International Conference Language Resources Evaluation (LREC).Maynard, D., Tablan, V., Ursu, C., Cunningham, H., & Wilks, Y. (2001). Named entityrecognition diverse text types. Recent Advances Natural Language Processing Conference (RANLP), pp. 11721178.Mcdonald, D. M., Chen, H., Su, H., & Marshall, B. B. (2004). Extracting gene pathwayrelations using hybrid grammar: arizona relation parser. Bioinformatics, 20,33703378.Michel, J., Shen, Y., Aiden, A., Veres, A., Gray, M., Google Books Team, Pickett, J.,Hoiberg, D., Clancy, D., Norvig, P., Orwant, J., Pinker, S., Nowak, M., & Aiden, E.(2011). Quantitative analysis culture using millions digitized books. Science,331, 176182.Miller, G. (1995). Wordnet: lexical database english. Journal CommunicationsACM (CACM), 38, 3941.Mishne, G. (2006). Predicting movie sales blogger sentiment. ProceedingsAssociation Advancement Artificial Intelligence (AAAI) Spring Symposium.Mitchell, J., & Lapata, M. (2008). Vector-based models semantic composition. Proceedings Annual Meeting Association Computational Linguistics (ACL).Muggleton, S. (1996). Learning positive data. Proceedings Inductive LogicProgramming Workshop, pp. 358376.Nguyen, T.-V. T., & Moschitti, A. (2011). Joint distant direct supervision relationextraction. Proceedings 5th International Joint Conference NaturalLanguage Processing (IJCNLP).Nguyen, T.-V. T., Moschitti, A., & Riccardi, G. (2009). Convolution kernels constituent,dependency sequential structures relation extraction. Proceedings2009 Conference Empirical Methods Natural Language Processing (EMNLP).Pearl, J. (2000). Causality: Models, Reasoning, Inference. Cambridge University Press.681fiRadinsky, Davidovich & MarkovitchQuirk, C., Brockett, C., & Dolan, W. (2004). Monolingual machine translation paraphrase generation. Proceedings Conference Empirical Methods NaturalLanguage Processing (EMNLP), pp. 142149.Rada, R., Mili, H., Bicknell, E., & Blettner, M. (1989). Development applicationmetric semantic nets. IEEE Transactions Systems, Man Cybernetics, 19 (1),1730.Radinsky, K., Davidovich, S., & Markovitch, S. (2008). Predicting news tomorrowusing patterns web search queries. Proceedings IEEE/WIC InternationalConference Web Intelligence (WI).Raina, R., Ng, A. Y., & Manning, C. D. (2005). Robust textual inference via learningabductive reasoning. Proceedings Association AdvancementArtificial Intelligence (AAAI).Ravichandran, D., Ittycheriah, A., & Roukos, S. (2003). Automatic derivation surface textpatterns maximum entropy based question answering system. ProceedingsNorth American Chapter Association Computational Linguistics: ShortPapers (NAACL Short), pp. 8587.Riloff, E. (1993). Automatically constructing dictionary information extractiontasks. Proceedings Association Advancement Artificial Intelligence(AAAI), pp. 811816.Riloff, E. (1996). Automatically Generating Extraction Patterns Untagged Text.Proceedings Association Advancement Artificial Intelligence (AAAI).Riloff, E., & Jones, R. (1999). Learning dictionaries information extraction multi-levelbootstrapping. Proceedings Association Advancement ArtificialIntelligence (AAAI).Rosenfeld, B., & Feldman, R. (2007). Using corpus statistics entities improve semisupervised relation extraction web. Proceedings 45th Annual MeetingAssociation Computational Linguistics (ACL), pp. 600607.Sarawagi, S. (2008). Information extraction. Foundations Trends Databases, 1 (3),261377.Scholkopf, B., Williamson, R., Smola, A., Shawe-Taylor, J., & Platt, J. (2000). Supportvector method novelty detection. Proceedings Annual ConferenceNeural Information Processing Systems (NIPS), pp. 582588.Scholkopf, B., Williamson, R. C., Smola, A., & Shawe-Taylor, J. (1999). Sv estimationdistributions support. Proceedings Annual Conference Neural InformationProcessing Systems (NIPS).Schubert, L. (2002). derive general world knowledge texts?. ProceedingsSecond Conference Human Language Technology (HLT).Shahaf, D., & Guestrin, C. (2010). Connecting dots news articles. ProceedingsAnnual ACM SIGKDD Conference (KDD).682fiLearning Predict Textual DataShen, W., Doan, A., Naughton, J. F., & Ramakrishnan, R. (2007). Declarative informationextraction using datalog embedded extraction predicates. ProceedingsConference Large Data Bases (VLDB), pp. 10331044.Shi, L., & Mihalcea, R. (2005). Putting pieces together: Combining framenet, verbnetwordnet robust semantic parsing. Proceedings Sixth InternationalConference Intelligent Text Processing Computational Linguistics (CICLing),pp. 100111.Shinyama, Y., & Sekine, S. (2006). Preemptive information extraction using unrestrictedrelation discovery. Proceedings North American Chapter AssociationComputational Linguistics - Human Language Technologies (NAACL HLT).Sil, A., Huang, F., & Yates, A. (2010). Extracting action event semantics webtext. Proceedings Association Advancement Artificial Intelligence(AAAI) Fall Symposium Commonsense Knowledge.Soderland, S. (1999). Learning information extraction rules semi-structured freetext. Machine Learning, 34.Strube, M., & Ponzetto, S. P. (2006). Wikirelate! computing semantic relatedness usingwikipedia. Proceedings Association Advancement Artificial Intelligence (AAAI).Suchanek, F. M. (2006). Combining linguistic statistical analysis extract relationsweb documents. Proceedings 12th ACM SIGKDD International Conference Knowledge Discovery Data Mining (KDD), pp. 712717.Suchanek, F. M., Kasneci, G., & Weikum, G. (2007). Yago: core semantic knowledge.Proceedings International Conference World Wide Web (WWW).Taskar, B., Guestrin, C., & Koller, D. (2003). Max-margin markov networks. ProceedingsAnnual Conference Neural Information Processing Systems (NIPS).Tatu, M., & Moldovan, D. (2005). semantic approach recognizing textual entailment.Proceedings Human Language Technology Conference Conference EmpiricalMethods Natural Language Processing (HLT EMNLP).Tatu, M., & Srikanth, M. (2008). Experiments reasoning temporal relationsevents. Proceedings International Conference Computational Linguistics(COLING).Tax, D. (2001). One class classification. PhD thesis, Delft University Technology.Tax, D. M. J., & Duin, R. P. W. (1991). Support vector domain description. PatternRecognition Letters, 20, 11911199.Turney, P. D. (2006). Expressing implicit semantic relations without supervision. Proceedings 44th Annual Meeting Association Computational Linguistics(ACL).Vapnik, V. (1995). Nature Statistical Learning Theory. Springer-Verlag, NY, USA.Viera, A. J., & Garrett, J. M. (2005). Understanding interobserver agreement: kappastatistic. Family Medicine, 37 (5), 360363.683fiRadinsky, Davidovich & MarkovitchWang, M. (2008). re-examination dependency path kernels relation extraction.Proceedings Third International Joint Conference Natural Language Processing (ACL IJCNLP).Wolff, P., Song, G., & Driscoll, D. (2002). Models causation causal verbs.Proceedings Annual Meeting Association Computational Linguistics(ACL).Yang, Y., Pierce, T., & Carbonell, J. (1998). study retrospective online eventdetection. Proceedings ACM SIGIR Special Interest Group Information Retrieval (SIGIR).Yeung, C., & Jatowt, A. (2011). Studying past remembered: Towards computational history large scale text mining. Proceedings ACM ConferenceInformation Knowledge Management (CIKM).Yoshikawa, K., Riedel, S., Asahara, M., & Matsumoto, Y. (2009). Jointly identifying temporal relations markov logic. Proceedings Third International JointConference Natural Language Processing (ACL IJCNLP).Zanzotto, F. M., Pennacchiotti, M., & Moschitti, A. (2009). machine learning approachtextual entailment recognition. Natural Language Engineering, 15, 551582.Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods relation extraction.Journal Machine Learning Research, 3, 10831106.Zhang, M., Zhang, J., Su, J., & Zhou, G. (2006). composite kernel extract relationsentities flat structured features. Proceedings 21stInternational Conference Computational Linguistics 44th Annual MeetingAssociation Computational Linguistics, pp. 825832.Zhao, S., & Grishman, R. (2005). Extracting relations integrated information usingkernel methods. Proceedings 43rd Annual Meeting AssociationComputational Linguistics (ACL), pp. 419426.684fiJournal Artificial Intelligence Research 45 (2012) 731-759Submitted 06/12; published 12/12Tractable Set ConstraintsManuel Bodirskybodirsky@lix.polytechnique.frEcole Polytechnique, LIX(UMR 7161 du CNRS)91128 Palaiseau, FranceMartin Hilshils@math.univ-paris-diderot.frInstitut de Mathematiques de Jussieu(UMR 7586 du CNRS)Universite Paris Diderot Paris 7UFR de Mathematiques75205 Paris Cedex 13, FranceAbstractMany fundamental problems artificial intelligence, knowledge representation,verification involve reasoning sets relations sets modeledset constraint satisfaction problems (set CSPs). problems frequently intractable,several important set CSPs known polynomial-time tractable.introduce large class set CSPs solved quadratic time. class,call EI, contains previously known tractable set CSPs, also newones crucial importance example description logics. class EI setconstraints elegant universal-algebraic characterization, use showevery set constraint language properly contains EI set constraints alreadyfinite sublanguage NP-hard constraint satisfaction problem.1. IntroductionConstraint satisfaction problems computational problems where, informally, inputconsists finite set variables finite set constraints imposed variables;task decide whether assignment values variablesconstraints simultaneously satisfied. Set constraint satisfaction problems specialconstraint satisfaction problems values sets, constraints might,instance, force one set includes another set x, one set x disjoint anotherset y. constraints might also ternary (or, generally, finite arity),constraint intersection two sets x contained z, symbolspx X yq z.systematically study computational complexity constraint satisfaction problems, turned fruitful approach consider constraint satisfaction problemsCSPpq set allowed constraints formed fixed finite set relationsR Dk (possibly infinite) common domain D. example, equals Q,rational numbers, tu usual order rationals, CSPpqproblem deciding whether given set (binary) constraints form xcommon solution rational numbers. way parametrizing conc2012AI Access Foundation. rights reserved.fiBodirsky & Hilsstraint satisfaction problem constraint language led many strong algorithmicresults (e.g., Bulatov & Dalmau, 2006; Idziak, Markovic, McKenzie, Valeriote, & Willard,2010; Barto & Kozik, 2009; Bodirsky & Kutz, 2007; Bodirsky & Kara, 2009), manypowerful hardness conditions large classes constraint satisfaction problems (Schaefer,1978; Bulatov, Krokhin, & Jeavons, 2005; Bulatov, 2003, 2006; Bodirsky & Kara, 2009).set constraint language set relations R pP pNqqk common domainP pNq set subsets natural numbers; moreover, requirerelation R defined Boolean combination equations signature [, \, c,0, 1, function symbols intersection, union, complementation, emptyfull set, respectively. Details formal definition set constraint languagesfound Section 3. Section 4, give many examples set constraint languages.choice N notational convenience; could selected infinite setpurposes, e.g., Rn instead N, natural choice spatial reasoning. One may evenreplace P pNq infinite Boolean algebra (see Theorem 28).following, set constraint satisfaction problem (set CSP) problem formCSPpq finite set constraint language . shown Marriott Oderskyset CSPs contained NP.Drakengren Jonsson (1998) initiated search set CSPs solvedpolynomial time. showed CSPpt, ||, uq solved polynomial time,x holds iff x subset equal y;x || holds iff x disjoint sets;x holds iff x distinct sets.also showed CSPpq solved polynomial time relationsdefined formulas formx1y1 _ _ xk yk _ x0 y0x1y1 _ _ xk yk _ x0 || y0formx0 , . . . , xk , y0 , . . . , yk necessarily distinct variables (Drakengren & Jonsson,1998, Thm. 20). call set relations defined way Drakengren Jonssons set constraint language. easy see algorithmpresent runs time quadratic size input. hand, also showcontains relation defined x1 y1 _ x2 y2 relation definedx1 x0 _ x2 x0 problem CSPpq NP-hard (Drakengren & Jonsson, 1998,Thm. 22).1.1 Contributions Outline.present significant extension Drakengren Jonssons (1998) set constraint language (Section 4) whose CSP still solved quadratic time input size (Section 5); call set constraint language EI. Unlike Drakengren Jonssons set732fiTractable Set Constraintsconstraint language, language also contains ternary relation defined px X q z,relation particular interest description logics discussbelow. Moreover, show extension EI contains finite sublanguageNP-hard set CSP (Section 6), using concepts model theory universalalgebra. sense, present maximal tractable class set constraint satisfactionproblems.algorithm based concept independence constraint languagesdiscovered several times independently 90s (Lassez & McAloon, 1989; Jonsson &Backstrom, 1998; Marriott & Odersky, 1996, see also Koubarakis, 2001; Broxvall, Jonsson,& Renz, 2002; Cohen, Jeavons, Jonsson, & Koubarakis, 2000); however, applyconcept twice novel, nested way, leads two level resolution procedureimplemented run quadratic time. technique use prove correctnessalgorithm also important contribution paper, believe similarapproach applied many contexts; technique inspired alreadymentioned connection universal algebra.1.2 Application Areas Related Literaturemention three different contexts set constraints appeared literature.1.2.1 Set Constraints Programming Languages.Set constraints find applications program analysis; here, set constraint formX , X set expressions. Examples set expressions 0 (denotingempty set), set-valued variables, union intersection sets, also expressionsform f pZ1 , Z2 q f function symbol Z1 , Z2 set expressions.Unfortunately, worst-case complexity reasoning tasks consideredsetting high, often EXPTIME-hard (for survey this, see Aiken, 1994).recently, shown quantifier-free combination set constraints (withoutfunction symbols) cardinality constraints (quantifier-free Pressburger arithmetic)satisfiability problem NP (Kuncak & Rinard, 2007). logic (called QFBAPA)interesting program verification (Kuncak, Nguyen, & Rinard, 2006).1.2.2 Tractable Description Logics.Description logics family knowledge representation formalisms usedformalize reason concept definitions. computational complexitycomputational tasks studied various formalisms usually quite high.However, last years series description logics, example EL, EL , Horn-FL0 ,various extensions fragments (Kusters & Molitor, 2002; Baader, 2003; Baader,Brandt, & Lutz, 2005; Krotzsch, Rudolph, & Hitzler, 2006), discoveredcrucial tasks e.g. entailment, concept satisfiability knowledge base satisfiabilitydecided polynomial time.Two basic assertions made ELHorn-FL0 C1 ||C2 (thereC1 also C2 ) C1 X C2 C3 (every C1 C2 also C3 ), conceptnames C1 , C2 , C3 . EI set constraints, latter treated733fiBodirsky & Hilsframework Drakengren Jonsson. None description logics tractableknowledge base satisfiability problem contains EI set constraints.1.2.3 Spatial Reasoning.Several spatial reasoning formalisms (like RCC-5 RCC-8) closely related set constraint satisfaction problems. formalisms allow reason relationsregions; fundamental formalism RCC-5 (see, e.g., Jonsson & Drakengren, 1997),one think region non-empty set, possible (binary) relationships containment, disjointness, equality, overlap, disjunctive combinations thereof. Thus,exclusion empty set prominent difference set constraint languages studied Drakengren Jonsson (1998) contained class set constraintlanguages considered RCC-5 fragments. see Section 3CSP RCC-5 CSPs reducts set CSPs (Proposition 2).2. Constraint Satisfaction Problemsuse existing terminology logic model theory, convenient describeconstraint languages structures (see, e.g., Hodges, 1993). structure tuplepD; f1, f2, . . . , R1, R2, . . . q set (the domain ), fi functionDki (where ki called arity fi ), Ri relation D, i.e., subsetDli (where li called arity Ri ). function fi assumefunction symbol denote fi , relation Ri relation symboldenote Ri . Constant symbols treated 0-ary function symbols.set relation function symbols structure called signature ,also say -structure. signature contains relation symbolsfunction symbols, also say relational structure. contextconstraint satisfaction, relational structures also called constraint languages,constraint language 1 called sublanguage (or reduct) constraint languagerelations 1 subset relations (and called expansion 1 ).Let relational structure domain finite signature . constraintsatisfaction problem following computational problem, also denoted CSPpq:given finite set variables V conjunction finitely many atomic formulasform Rpx1 , . . . , xk q, x1 , . . . , xk P V R P , satisfiable ; is,exist assignment : V every constraint Rpx1 , . . . , xk q inputpspx1 q, . . . , spxk qq P R ?mapping also called solution instance CSPpq, conjunctscalled constraints. Note introduce constraint satisfaction problemsCSPpq finite constraint languages, i.e., relational structures finite relationalsignature.Example 1. problem CSPppQ; qq problem deciding whether given setconstraints form x solution simultaneously satisfies constraints.734fiTractable Set Constraints3. Set Constraint Languagessection, give formal definitions set constraint languages. Let structure domain P pNq, set subsets natural numbers, signaturet[, \, c, 0, 1u,[ binary function symbol denotes intersection, i.e., [S X;\ binary function symbol union, i.e., \S Y;c unary function symbol complementation, i.e., cS function mapsN NzS;0 1 constants (treated 0-ary function symbols) denoting empty setfull set N, respectively.HSometimes, simply write [ function [S \ function \S , i.e.,distinguish function symbol respective function. use symbols[, \ symbols X, prevent confusion meta-mathematical usages Xtext.set constraint language relational structure whose relations quantifier-freedefinition S. always allow equality first-order formulas, equality symbolalways interpreted true equality relation domain structure.write x abbreviation x [ x.Example 2. ternary relation px, y, z qdefinition z [ px [ q x [ S.P P pN q3 | x [ z(quantifier-freeTheorem 1 (See Marriott & Odersky, 1996, Proposition 5.8). Let set constraintlanguage finite signature. CSPpq NP.easy see NP-hard set CSPs, shown next example.Example 3. Consider set constraint language contains eight relationstpx, y, zq | x 1 _ 1 _ z 1utpx, y, zq | x 1 _ 1 _ z 1utpx, y, zq | x 1 _ 1 _ z 1utpx, y, zq | x 1 _ 1 _ z 1utpx, y, zq | x 1 _ 1 _ z 1utpx, y, zq | x 1 _ 1 _ z 1utpx, y, zq | x 1 _ 1 _ z 1utpx, y, zq | x 1 _ 1 _ z 1u .set CSP relations well-known 3-SAT problem, NPcomplete (Garey & Johnson, 1978).well-known structure pP pNq; \, [, c, 0, 1q Boolean algebra,735fiBodirsky & Hils0 playing role false, 1 playing role true;c playing role;[ \ playing role ^ _, respectively.refer work Koppelberg (1989) background Boolean algebras.confuse logical connectives connectives Boolean algebras, alwaysuse symbols [, \, c instead usual function symbols ^, _, Booleanalgebras. facilitate notation, also write x instead cpxq, x insteadp x q.assume terms functional signaturet[, \, c, 0, 1u written(inner) conjunctive normal form (CNF), i.e., ni1 nj 1 lij lij eitherform x form x variable x. Note every term t[, \, c, 0, 1u rewritten equivalent term form, using usual laws Boolean algebras (Boole,1847). allow specialcase n 0 (in case becomes 1), special casenini 0 (in case j 1 lij becomes 0). refer ci : tlij | 1 j ni u(inner) clause t, lij (inner) literal ci . say set inner clausessatisfiable exists assignment V P pNq inner clauses,union evaluation literals equals N (this case formulani1 ci 1 satisfying assignment).Example 4. Inequality x P pNq equivalently written py \ xq[px \ q 1;formula, two inner clauses, positive negative inner literal.assume quantifier-free formulas signaturet[, \, c, 0, 1u writtenmi(outer) conjunctive normal form (CNF), i.e.,j 1 Lij Lij eitheri1form 1 (a positive (outer) literal ) form 1 (a negative (outer) literal ).Again, well-known easy see every quantifier-free formula findformula form equivalent every Boolean algebra. referCi : tLij | 1 j mi u (outer) clause , Lij (outer) literal Ci .Whenever convenient, identify set clauses.Example 5. Consider formula px ^ z q _ px ^ z q. rewrittenpx _ z q^px _ z q. subsequently replace inequality literals xpy \ xq [ px \ q 1 (see Example 4), arrive formula discussednormal form: two outer clauses, one two positive outer literals,two negative outer literals.mentioned introduction, CSPs reducts RCC-5 (and therefore alsomany reducts RCC-8) formulated set CSPs. network satisfaction problem RCC-5 seen CSP following structure RCC-5 domainP pNqztHu binary relations given follows: relation R RCC-5exists quantifier-free t\, [, c, 0, 1u-formula px1 , x2 q ps1 , s2 q P Rs1 s2 non-empty, ps1 , s2 q true S. clearfinitely many inequivalent quantifier-free formulas language t\, [, c, 0, 1u,hence RCC-5 finitely many relations.736fiTractable Set ConstraintsProposition 2. Let reduct RCC-5. exists set constraint languageCSPpq CSPpq problem.Proof. Let 1 , . . . , n formulas define relations (which domainP pNqztHu). Let structure domain P pNq relations R1 , . . . , Rn defined1 , . . . , n S. is, difference additional elementtHu, appears none relations . prove every finiteconjunction atomic formulas form Ri px1 , . . . , xk q satisfiableconjunction satisfiable . satisfiable clearly also satisfiablesince induced substructure . Conversely, satisfiable ,solution must property spxi q H, xi appearconstraint (since constraints force arguments non-empty). Hence, settingspxi q non-empty subset N still solution, implies claim.Proposition 2 shows class set CSPs contains class CSPs reductsRCC-5. inclusion clearly strict: set CSPs cannot formulatedCSPs reducts RCC-5, since relations set constraint languages arbitraryarity, reducts RCC-5 contain binary relations.4. Horn-Horn Set Constraintssection, study Horn-Horn set constraints, class set constraints admitsintuitive syntactic description thus easy define. Universal algebraicconsiderations class Section 4.2 lead us another class set constraints, calledEI introduced Section 4.3. class strictly contains class Horn-Horn setconstraints. universal algebraic description key maximality resultprove Section 6. tractability, set constraint languages EI allow(linear-time) reduction satisfiability Horn-Horn clauses (see Proposition 22). noteclasses set constraints (Horn-Horn EI) studied before.4.1 Horn-Horn RelationsDefinition 3. quantifier-free formula called Horn-Horn1. every outer clause outer Horn, i.e., contains one positive outer literal,2. every inner clause positive outer literals inner Horn, i.e., contains onepositive inner literal.relation R P pNqk calledouter Horn defined conjunction outer Horn clauses;inner Horn defined formula form pc1 [ [ ck qci inner Horn;1Horn-Horn defined Horn-Horn formula S.Example 6. InequalityHorn-Horn: recall may defined py \ xq[px \ yq 1.737fiBodirsky & HilsExample 7. Using previous example, relationeasily seen Horn-Horn, too.Example 8. ternary relation tpx, y, z q | x [Horn-Horn definition x \ \ z 1.tpx, y, u, vq | x _ u vuzu, encountered above,Proposition 4. Drakengren Jonssons set constraint language contains Horn-Hornrelations.Proof. disjointness relation || definition x \ 1, inner Horn.inequality relation Horn-Horn since inner clauses definition py \ xq[px \ q1 one positive inner literal. inclusion relation x definition\ x 1, inner Horn.Horn-Horn preserved adding additional outer disequality literals outerclauses, relations considered Drakengren Jonssons language Horn-Horn.4.2 Universal Algebraic Preliminariessee section, class Horn-Horn formulas preserved severalimportant functions defined set subsets natural numbers.Definition 5.Let : pP pNqq2 P pNq function maps pair sets pS1 , S2 qset t2a | P S1 u t2a 1 | P S2 u;denote Fin set finite non-empty subsets N, let F : P pNq P pFin q,F pS q : tS0| S0 finite non-emptyu ;let G : N Fin bijection (since sets countable, bijection exists);let e : P pNq P pNq definedepS q tG1 pT q |P F pS qu ;let ei function defined eipx, q epipx, qq.Intuitively, functions e ei designed forget unions (thisformalized Definition 34), preserving basic operations (see Lemma11 Proposition 8 case e).Definition 6. Let f : pP pNqqk P pNq function, R P pNql relation.say f preserves R following holds: a1 , . . . , ak P pP pNqqlpf pa11, . . . , ak1 q, . . . , f pa1l , . . . , akl qq P R ai P R k. f preserve R,also say f violates R. say f strongly preserves R a1 , . . . , ak P pP pNqqlpf pa11 , . . . , ak1 q, . . . , f pa1l , . . . , akl qq P R ai P R k.first-order formula defines relation R S, f preserves (strongly preserves)R, also say f preserves (strongly preserves) . Finally, g : pP pNqql P pNqfunction, say f preserves (strongly preserves) g preserves (stronglypreserves)(graph g, i.e., relation px1 , . . . , xl , g px1 , . . . , xl qq | x1 , . . . , xl N .738fiTractable Set ConstraintsNote injective function f preserves function g, also strongly preservesg.Example 9. Consider function f : pP pNqq2 P pNq, px, q x \ y. f preserves, since x \ x1 \ y1 whenever x x1 y1 y. hand, f stronglypreserve , shown f pN, Hq f pH, Nq.Fact 7. mapping isomorphism S2 S.Proof. mappinginverted mapping sends N ta | 2a Pu, ta | 2a 1 P u . straightforward verify strongly preserves 0, 1, c, \, [.Clearly, ipx, q H xH.Similarly, since natural numbers partitioned even odd numbers,ipx, q N x N.Let S1 S2 subsets N. verify preserves c showipcpS1 , S2 qq, definition equal ipS1 , S2 q, equals cpipS1 , S2 qq. Suppose2a1 . Then:P pS 1 , 2 q 1P S1a1 R S12a1 R ipS1, S2qP ipS1, S2qargument 2a1even strongly preserves c.1 analogous. Thus, preserves c. Since injective,Let pS1 , S2 q pT1 , T2 q pP pNq2 . show ippS1 , S2 q\pT1 , T2 qq,definition equal ipS1 \ T1 , S2 \ T2 q, equals ipS1 , S2 q \ ipT1 , T2 q.2a1 before:P ipS1 \ T1 , S2 \ T2 q a1P pS1 \ T1q2a1 P ipS1, S2q \ ipT1, T2qargument 2a1 1 analogous. Thus, preservesinjective, even strongly preserves \.verification\.Since[ similar \.Proposition 8. function e following properties.e injective,[,x, y, z P P pNq x \epxq \ epy q epz q.e strongly preserves 1, 0,z, x739y,x,fiBodirsky & HilsProof. verify properties one one. Since G bijective, epxq epy qx finite subsets. case x y, hence einjective. Thus, prove e strongly preserves 1, 0, [, suffices check epreserves 1, 0, [.Since G bijective, GpNq equals set finite subsets N,hence epNq N, shows e preserves 1. also compute epHq G1 pF pHqqG1 pHq H.Next, verify x, P P pNq epxq [ epy q epx [ q. Let P Narbitrary. P epxq [ epy q Gpaq P F pxq X F py q. definition Fsince Gpaq finite subset N, case Gpaq P F px [ q.case P epx [ q, concludes proof e preserves [.verify x \ z, x y, x, epxq \ epy q epz q. Firstobserve u, v N u v epuq epv q since e preserves [.implies epxq \ epy q epz q. Since x x, a, b P x,R y, b P y, b R x. ta, bu P F pz q, ta, bu R F pxq F py q. Hence,G1 pta, buq P epz q, G1 pta, buq R epxq \ epy q. shows epz q epxq \ epy q.Note particular e preserves , , ||. Moreover, epcpxqq cpepxqq:follows preservation ||, since x||cpxq, therefore epxq||epcpxqq, equivalentinclusion above. e strongly preserve [, 0, 1, therefore also eistrongly preserves [, 0, 1.following direct consequence fact isomorphisms kpreserve Horn formulas ; since simple proof instructive follows,give special case relevant here.Proposition 9. Outer Horn relations preserved i.Proof. Let conjunction outer Horn clauses variables V . Let tt0 1, t11, . . . , tk 1u outer clause . Let u, v : V P pNq two assignments satisfyclause. Let w : V P pNq given x ipupxq, v pxqq. Suppose w satisfiestj 1 1 j k. Since injective must tj 1 u v1 j k, therefore neither assignment satisfies negative literals. Hence, u vmust satisfy t0 1. Since isomorphism S2 S, preserves particulart0 1, hence w also satisfies t0 1.Proposition 10. Inner Horn relations strongly preserved e.Proof. Observe x \ p j j q 1 equivalent x [ p j yj q j yj , stronglypreserved e since e strongly preserves [. clearly implies statement.Note Proposition 9 Proposition 10 imply ei strongly preserves inner Hornrelations. later also need following.N, k 1. following equivalent.epx1 q \ \ epxk q \ epy1 q \ \ epyl q 1.exists k xi \ p j j q 1.Lemma 11. Let x1 , . . . , xk , y1 , . . . , yl1.2.740fiTractable Set Constraints3. exists k epxi q \ pjepyj qq 1.0, j yjl 1 jl epyj q 1.Proof. implicationp1q p2q, suppose every k ai P Nai R Xi : xi \ p j j q. Let c G1 ta1 , a2 , . . . , ak u . k,c R epxi q \ j l epyj q. see this, first observe ai P j l yj [ xi . Therefore,ta1, . . . , ak u P jl F pyj q [ F pxiq k. conclude c R epx1q \ \ epxk q \epy1 q \ \ epyl q.implication p2q p3q follows directly Proposition 10. implication p3qp1q trivial. second statement direct consequence Proposition 10.kProposition 12. Every Horn-Horn relation preserved e i, particularei.Proof. Suppose R Horn-Horn definition variables V . Since Rparticular outer Horn, preserved Proposition 9.verify R preserved e. Let u : V P pNq assignmentsatisfies . is, u satisfies least one literal outer clause . sufficesshow assignment v : V P pNq defined x epupxqq satisfies outerliteral. Suppose first outer literal positive; Horn-Horn,form x \ y1 \ \ yl 1 form y1 \ \ yl 1, preserved eLemma 11.Now, suppose outer literal negative, is, form x1 \ \ xk \ y1 \\ yl 1 k 0. treat case k 1, case similar.Suppose contradiction v px1 q \\ vpxk q \ vpy1q \ \ vpyl q 1. Lemma 11,exists k upxi q \ p j upy j qq 1. particularupx1 q \ \ upxk q \ upy1 q \ \ upyl q 1, contradiction assumption usatisfies .4.3 EI Set Constraintssection introduce class EI set constraints, show strictly containsHorn-Horn relations, give several examples non-examples. presentalgorithmic reduction CSPs EI set constraints satisfiability finite setsHorn-Horn clauses.Definition 13. set relations quantifier-free definitionpreserved operation ei denoted EI.Remark. Note definition operation ei (Definition 5) involved bijection GN Fin ; see later (Proposition 36 Proposition 37) classEI independent precise choice G.Recall Proposition 12 EI contains Horn-Horn relations. presentexamples relations EI, examples relations EIHorn-Horn.741fiBodirsky & HilsExample 10. give example relation clearly EI. relationR tpx, q | x \ 1u violated ei: consider S1 t2a | P Nu S2 t2a1 | P Nu. pS1 , S2 q P R, since isomorphism S2also pipS1 , S1 q, ipS2 , S2 qq P R. Since neither ipS1 , S1 q ipS2 , S2 q ipS2 , S2 qipS1 , S1 q, get epipS1 , S1 qq \ epipS2 , S2 qq ep1q 1 Proposition 8. Therefore,peipS1, S1q, eipS2, S2qq R R wanted show.Example 11. relation R tpx, y, z q | px q _ py z qu also preservedei: note p0, 1, 1q, p0, 0, 1q P R, eip0, 0q, eip1, 0q, eip1, 1q pairwise distinctsince ei injective.Example 12. formulap x [ xq^ px [ yq^ pv 1 _ u 1 _ x \ 1qclearly Horn-Horn. However, relation defined formula EI:px1, y1, u1, u2q und px2, y2, u2, v2q relation, neither ipx1, x2q ipy1, y2qipy1 , y2 q ipx1 , x2 q. Proposition 8, peipx1 , x2 q, eipy1 , y1 q, eipu1 , u2 q, eipv1 , v2 qq satisfiesformula. equivalent Horn-Horn formula, since formula preservedi.Example 13. formula ppx \ 1q _ pu \ v 1qq ^ px \ 1q ^ px \ 1qHorn-Horn. However, preserved e i: reason one clausesnegative literal x \ 1, conjuncts tx \ 1u tx \ 1u. Therefore,every tuple P R tuple eptq satisfies x \ 1 R well. Proposition 9,R preserved i. case, authors suspect equivalent Horn-Hornformula. generally, open whether exist formulas preserved ei, equivalent Horn-Horn formula.Corollary 14. class Horn-Horn relations proper subclass EI.Proof. Proposition 12 shows EI contains Horn-Horn relations. Example 12 showsinclusion strict.prepare results viewed partial converse Proposition 12.Definition 15. quantifier-free formula (in syntactic form described endSection 3) called reduced every formula obtained removing outer literalequivalent S.note slightly different notion reduced formula introducedBodirsky, Chen, Pinsker (2010). variant using better suitedpurposes.Lemma 16. structure S, every quantifier-free formula equivalent reducedformula.742fiTractable Set ConstraintsProof. clear every quantifier-free formula written formula CNFform discussed Theorem 1. remove successively outerliterals long results equivalent formula.first prove partial converse Proposition 9.Proposition 17. Let reduced formula preserved i. outer clauseHorn.Proof. Let V set variables . Assume contradiction contains outerclause two positive literals, t1 1 t2 1. remove literal t1 1clause C, resulting formula inequivalent , hence assignments1 : V P pNq satisfies none literals C except t1 1. Similarly,assignment s2 : V P pNq satisfies none literals C except t2 1.injectivity i, since strongly preserves c, [, \, 1, assignment : V P pNqdefined x ips1 pxq, s2 pxqq satisfy two literals t1 1 t2 1. Sincestrongly preserves c, \, [, none literals C satisfied mappingswell, contradiction assumption preserved i.Definition 18. Let V set variables, : V P pNq mapping.function V P pNq form x epspxqq called core assignment.Lemma 19. every quantifier-free formula exists formula innerclauses inner Horn, satisfying core assignments.preserved ei, set satisfying core assignments closedei.Proof. Suppose outer clause C positive outer literal 1contains inner clause c : x1 \ \ xk \ 1 \ \ l Horn, i.e., k 2.replace outer literal 1 k literals t1 1, . . . , tk 1 ti obtainedreplacing c xi \ 1 \ \ l .claim resulting formula 1 set satisfying core assignments.Observe xi \ 1 \ \ l c, hence ti 1 implies 1. arbitrary satisfyingassignment 1 satisfies either one positive outer literals ti 1, caseobservation shows also satisfies , satisfies one outer literals C,case also satisfies literal . Hence, 1 implies . Conversely, letsatisfying core assignment . satisfies literal C 1, alsosatisfies literal 1 , satisfies 1 . Otherwise, must satisfy 1, hencespx1 q\ \ spxk q\ spy1 q\ \ spyl q 1. Since core assignment, Lemma 11 impliesexists k spxi q \ spy1 q \ \ spyl q 1. satisfies 1 .Suppose outer clause C negative outer literal 1contains inner clause c : x1 \ \ xk \ 1 \ \ l Horn, i.e., k 2.replace clause C k clauses C1 , . . . , Ck Ck obtained Creplacing c xi \ 1 \ \ l .claim resulting formula 1 set satisfying core assignments.Observe x1 \ \ xk \ 1 \ \ l 1 implies xi \ 1 \ \ l 1, everyk. observation shows arbitrary assignment also assignment 1 .743fiBodirsky & HilsConversely, let satisfying core assignment 1 . satisfies one literalsC 1, satisfies . Otherwise, must satisfy xi \ 1 \ \ l 1k, Lemma 11 also satisfies x1 \ \ xk \ 1 \ \ l 1.perform replacements obtain formula 1 inner clausesHorn; formula satisfies requirements first statement lemma.prove second statement, let u, v : V P pNq two satisfying core assignments1 . Since 1 satisfying core assignments, u v also satisfy .mapping w : V P pNq given x eipupxq, v pxqq core assignment,ei preserves , mapping w satisfies . Since 1 core assignments,w also satisfying assignment 1 , proves statement.single technical condition guarantees, extra condition(see Proposition 21) formulas satisfying certain universl algebraic property HornHorn. allow us perform reduction CSP associated (finite) setconstraint languages EI satisfiability Horn-Horn clauses.Definition 20. quantifier-free formula (in syntactic form described endSection 3) called strongly reduced every formula obtained removing outerliteral set satisfying core assignments S.Proposition 21. Let strongly reduced formula whose inner clauses Horn.set satisfying core assignments closed ei, Horn-Horn.Proof. Let V set variables . suffices show clauses outerHorn. Assume contradiction contains outer clause two positive literals,t1 1 t2 1. remove literal t1 1 clause C, resulting formulastrictly less satisfying core assignments; shows existence core assignments1 : V P pNq satisfies none literals C except t1 1. Similarly, existscore assignment s2 : V P pNq satisfies none literals C except t2 1.assumption, inner clauses t1 t2 Horn. claim assignment: V P pNq defined x eips1 pxq, s2 pxqq satisfy clause C. Since eistrongly preserves inner Horn clauses, satisfy t1 1 _ t2 1.reasons satisfy literals C; contradicts assumptionsatisfying core assignments preserved ei.Satisfiability Horn-Horn clauses computational problem decide whether,given finite set Horn-Horn clauses, satisfying assignment S.Proposition 22. Let finite set constraint language EI. CSPpqreduced linear time satisfiability Horn-Horn clauses.Proof. Let instance CSPpq, let V set variables appear. constraint Rpx1 , . . . , xk q , let R definition R S.Lemma 19, exists formula R satisfying core assignments Rinner clauses Horn; moreover, since R preserved ei, lemmaasserts set satisfying core assignments R preserved ei.assume without loss generality R strongly reduced; seen similarlyLemma 16. Proposition 21, formula R Horn-Horn.744fiTractable Set ConstraintsLet set Horn-Horn clauses formulas R px1 , . . . , xk q obtained constraints Rpx1 , . . . , xk q described manner. claim satisfiable instanceCSPpq satisfiable. follows fact constraintRpx1 , . . . , xk q , formulas R R satisfying core assignments,R R preserved ei (for R follows Proposition 12),particular function x eipx, xq.Note Proposition 22 reduce satisfiability EI satisfiability propersubclass Horn-Horn set constraints: general Horn-Horn set constraints allowinner clauses negative outer literals Horn, reduction produces HornHorn clauses inner clauses Horn.5. Algorithm Horn-Horn Set Constraintspresent algorithm takes input set Horn-Horn clauses decidessatisfiability pP pNq; \, [, c, 0, 1q time quadratic length input.Proposition 22, section therefore conclude proof CSPpq tractablerelations EI.mentioned introduction, algorithm based two procedures,resolution-like. inner procedure essentially well-known positive unit resolutionprocedure Horn-SAT, outer procedure basically algorithmused literature independence constraint satisfaction (see, e.g., Jonsson &Backstrom, 1998; Koubarakis, 2001; Broxvall et al., 2002; Cohen et al., 2000). contribution section way nest two algorithms obtain polynomial-timedecision procedure satisfiability Horn-Horn clauses.start discussing first procedure algorithm, call innerresolution algorithm. case Boolean positive unit resolution (Dowling & Gallier,1984) one implement procedure Inner-Res runs linear timeinput size.Lemma 23. Let finite set inner Horn clauses. following equivalent.1.1 satisfiable S.2. Inner-Respq Figure 1 accepts.3.1 solution whose image containedtH, Nu.Proof. obvious 1 unsatisfiable Inner-Respq rejects; fact,inner clauses c derived Inner-Res , formula c 1 logically implied1. Conversely, algorithm accepts set eliminated variablesN remaining variables H, satisfies clauses: removed clausespositive literal satisfied, remaining clauses least one negative literalfinal stage algorithm, clauses negative literals final stagealgorithm satisfied.proof previous lemma shows 1 satisfiable1 satisfiable two-element Boolean algebra. see following,745fiBodirsky & HilsInner-Res()// Input: finite set inner Horn clauses// Accepts iff 1 satisfiableentire algorithm:contains empty clause, reject.Repeat := trueRepeat = trueRepeat := falsecontains positive unit clause txuRepeat := trueRemove clauses literal x occurs.Remove literal x clauses.EndLoopAcceptFigure 1: Inner Resolution Algorithm.holds generally (and inner Horn clauses). followingwell-known, shown proof given Koppelberg (1989)weaker Proposition 2.19 there. give proof convenience reader.Fact 24. Let t1 , t2 termst[, \, c, 0, 1u. following equivalent:1 ^ t2 1 satisfiable two-element Boolean algebra;t1 1 ^ t2 1 satisfiable Boolean algebras;t1 1 ^ t2 1 satisfiable Boolean algebra;t1 1 ^ t2 1 satisfiable finite Boolean algebra.1. t12.3.4.Proof. Obviously, (1) implies (2), (2) implies (3).(3) implies (4), assume t1 1 ^ t2 1 satisfying assignmentBoolean algebra C. Let x1 , . . . , xn variables occur t1 t2 , let xi cisatisfying assignment. t1 1 ^ t2 1 satisfiable Boolean sub-algebranC1 C generated tc1 , . . . , cn u, C1 finite (it 2p2 q elements).(4) implies (1), first note finite Boolean algebra isomorphic Booleanalgebra pP pX q; [, \, c, 0, 1q subsets finite set X. x P X, consider map hx :P pX q t0, 1u, hpY q : 1 x P , hpY q 0 otherwise. hx homomorphismBoolean algebras. particular, shows every non-zero element finiteBoolean algebra C, homomorphism h C two-element Boolean algebrahpaq 0. suppose (4), assume t1 1 ^ t2 1 satisfyingassignment finite Boolean algebra C. Let c element denoted t2 Cassignment, c 1. let h homomorphism C t0, 1uhpcq 0, i.e. hpcq 1. construction, image satisfying assignment hsatisfying assignment t1 1 ^ t2 1 t0, 1u.746fiTractable Set Constraintsstatement t1 1 instead t1 1 ^ t2 1 given Proposition2.19 (Koppelberg, 1989). Fact 24 following consequence crucial wayuse inner resolution procedure algorithm.Lemma 25. Let finite set inner Horn clauses. following equivalent:1. Inner-RespYtx1 , . . . , xk , y0 , . . . , yl uq rejects.1 S.Proof.1 implies x1 \ \ xk \ 1 \ \ l 11 ^ x1 \\ xk \ y1 \ \ yl 1 unsatisfiable S. Fact 24, case1 ^ x1 \ \ xk \ 1 \ \1 unsatisfiable 2-elementlBoolean algebra, case 1 ^ x1 \ \ xk \ 1 \ \ l 02.1 implies x1 \ \ xk \ 1 \ \ lunsatisfiable two-element Boolean algebra. seen Lemma 23,turn holds Inner-RespYtx1 1, . . . , xk 1, y1 1, . . . , yl 1uq rejects.Outer-Res()// Input: finite set Horn-Horn clauses// Accepts iff satisfiable pP pNq; [, \, c, 0, 1qentire algorithm:contains empty clause, reject.Repeat := trueRepeat = trueRepeat := falseLet set inner Horn clauses termspositive unit clauses tt 1u .Inner-Res rejects , reject.negative literal 1 clausesinner clause tx1 , . . . , xk , 1 , . . . , l uCall Inner-Restx1 1, . . . , xk 1, y0 1, . . . , yl 1uInner-Res rejects remove clauseEndclauses removed,Remove outer literal 1 clauseRepeat := trueEndLoopAcceptFigure 2: Outer Resolution Algorithm.Theorem 26. algorithm Outer-Res Figure 2 decides satisfiability sets HornHorn clauses quadratic time.Proof. first argue algorithm rejects , indeed solution. Firstnote whole argument, set clauses satisfying tuples747fiBodirsky & Hils(i.e., corresponding formulas equivalent): Observe negative literals getremoved clauses, negative literal 1 gets removed clauseInner-Res rejects tx1 1, . . . , xk 1, y0 1, . . . , yl 1u inner clausetx1, . . . , xk , y1, . . . , yl u t. Lemma 25, Inner-Res rejects Ytx1 1, . . . , xk 1, y01, . . . , yl 1u implies x1 \ \ xk \ 1 \ \ l 1. Hence, positiveunit clauses imply 1 therefore literal 1 removed clausewithout changing set satisfying tuples. algorithm rejects either Inner-Resrejects derives empty clause. cases clear satisfiable.Thus, suffices construct solution algorithm accepts. Let setinner clauses terms positive unit clauses final stage, algorithmaccepts. remaining negative outer literal tt 1u remaining inner clausetx1 , . . . , xk , 1 , . . . , l u exists assignment V P pNq satisfiesYtx1 \ \ xk \ 1 \ \ l 1u: otherwise, Lemma 25, inner resolution algorithmwould rejected tx1 1, . . . , xk 1, y0 1, . . . , yl 1u, would removedinner clause t. Let D1 , . . . , Ds enumeration remaining inner clausesappear remaining negative outer literals.Write s-ary operation defined px1 , . . . , xs q ipx1 , ipx2 , . . . , ipxs1 , xs q qq(where Fact 7). claim : V P pNq givenx pD1 pxq, . . . , Ds pxqqsatisfies clauses . Let C clause . assumption, final stagealgorithm, clause C still non-empty. Also note since formulas inputHorn-Horn, contain one positive literal. holds particular C,therefore distinguish following cases:final state algorithm, C still contains negative literal 1. Since 1removed, remaining inner clause tx1 , . . . , xk , 1 , . . . , l ut. Observe spx1 q \ \ spxk q \ spy1 q \ \ spyl q 1 Dj px1 q \\ Dj pxk q \ Dj py1q \ \ Dj pyl q 1 1 j s. Hence, sincepx1 q \ \ pxk q \ py1 q \ \ pyl q 1, satisfies 1. showssatisfies C.negative literals removed C algorithm. positiveliteral t0 1 C inner clauses t0 Horn. part, therefore t0 1 satisfied s. Indeed, assumption assignments Djsatisfy , preserved i.conclude solution . inner resolution algorithm linear timecomplexity; outer resolution algorithm performs linear number callsinner resolution algorithm, straightforward implement necessary datastructures outer resolution obtain running time quadratic inputsize.Combining Proposition 22 Theorem 26, obtain following.Theorem 27. Let finite set constraint language EI. CSPpqsolved quadratic time.748fiTractable Set Constraints6. Maximal Tractabilitysection show class EI maximal tractable set constraint language.specifically, let set constraint language strictly contains EI relations.show contains finite set relations 1 already problemCSPp1 q NP-hard (Theorem 40).6.1 Universal-Algebraic Approachproof use so-called universal-algebraic approach complexity constraint satisfaction problems, requires re-formulate set CSPs constraintsatisfaction problems -categorical structures. detailed introductionuniversal-algebraic approach -categorical structures (see Bodirsky, 2012). structurecountable domain called -categorical countable structures satisfyfirst-order sentences isomorphic (see, e.g., Hodges, 1993).theorem Ryll-Nardzewski, countable signatures, equivalent requiringevery relation preserved automorphisms1 first-order definable(see, e.g., Hodges, 1993). useful consequence -categorical structure , whenever two tuples c pc1 , . . . , cn q pd1 , . . . , dn q satisfyfirst-order formulas, automorphism maps c d.example -categorical structure pQ; q (by Cantors theorem), nonexample given pZ; q. Note pQ; q pZ; q CSP; indeed, twoinfinite linear orders share CSP, since even finite substructures.characterisation infinite structures -categorical structureCSP given Bodirsky, Hils, Martin (2011). Empirically,observed constraint satisfaction problems studied temporal spatialreasoning typically called qualitative formulated-categorical template.Set constraint languages general -categorical (this follows easilymentioned theorem Ryll-Nardzewski). However, every set CSP formulatedCSP -categorical structure. see this, first recall basic factsBoolean algebras. countable atomless2 Boolean algebras isomorphic (Koppelberg,1989, Corollary 5.16; see also Hodges, 1993, Example 4 page 100). Let denotecountable atomless Boolean algebra, let denote domain A. Again, use [\ denote join meet A, respectively. Since axioms Boolean algebrasproperty atoms written first-order sentences, follows-categorical. structure B quantifier elimination every first-order formulaB equivalent quantifier-free formula. well-known quantifier elimination(see Hodges, 1993, Exercise 17 page 391). also make use following.Theorem 28 (Marriott & Odersky, 1996, Corollary 5.7). quantifier-free formula satisfiable infinite Boolean algebra satisfiable infinite Booleanalgebras.1. isomorphism structure called automorphism .2. atom Boolean algebra element x 0 x X0. Boolean algebra contains atoms, called atomless.749xfiBodirsky & Hilsparticular, B infinite Boolean algebra 1 , . . . , n quantifier-freeformulas signature t[, \, c, 0, 1u, relational structure signature tR1 , . . . , Rn u Ri n defined B, CSPpqdepend choice B.fundamental concept complexity theory constraint satisfaction problemsnotion primitive positive definitions. first-order formula called primitive positive(pp) formDx1, . . . , xn p1 ^ ^ mqformula form Rpy1 , . . . , yl q form y1 y2 ,R relation symbol y1 , y2 , . . . , yl either free variables tx1 , . . . , xn u.say k-ary relation R Dk primitive positive definable (pp definable)-structure domain iff exists primitive positive formula px1 , . . . , xk qk free variables x1 , . . . , xk tuple pb1 , . . . , bk q R pb1 , . . . , bk qtrue .Example 14. relation tpx, q P P pNq2 | x u pp definable pP pNq; S, qtpx, y, z q | x [ z u. pp definition px, x, q ^ x (the definition evenquantifier-free).Example 15. relation tpx1 , x2 , x3 , q P P pNq4 | x1 [ x2 [ x3 u pp definablepP pNq; q tpx, y, z q | x [ z u. pp definition Du pS px1 , x2 , uq ^pu, x3 , qq.every relation structure preserved operation f , f calledpolymorphism . Note polymorphisms also preserve relations ppdefinition . following shown finite domain constraint satisfactionBulatov et al. (2005); easy proof also works infinite domain constraint satisfaction.Lemma 29. Let R relation primitive positive definition structure .CSPpq CSP expansion relation R polynomial-time equivalent.following theorem one reasons useful work -categoricaltemplates (when possible).Theorem 30 (Bodirsky & Nesetril, 2006). Let -categorical structure. Rprimitive positive definable R preserved polymorphisms .previous next result together used translate questionsprimitive positive definabilitypurely operational questions. Let set, let Opnqpnq set operations finite arity.Dn D, let 8n1pnqoperation P called projection fixed P t1, . . . , nu n-tuplespx1, . . . , xnq P Dn identity px1, . . . , xnq xi. composition k-aryoperation f k operations g1 , . . . , gk arity n n-ary operation definedpf pg1, . . . , gk qqpx1, . . . , xnqf g1px1, . . . , xnq, . . . , gk px1, . . . , xnq .750fiTractable Set ConstraintsDefinition 31. say F locally generates f : Dn every finite subsetoperation g : Dn obtained operations Fprojection maps composition f paq g paq P .Theorem 32 (see Szendrei, 1986, Corollary 1.9; also Bodirsky, 2012, Proposition 5.2.1). LetF set operations domain D. operation f : Dk preservesfinitary relations preserved operations F F locally generatesf.set automorphisms structure denoted Autpq. following,always consider sets operations F contain AutpAq, therefore make followingconvention. F O, say F generates f P F AutpAq locally generates f .6.2 EI Set Constraints Atomless Boolean Algebraprevious subsection seen set CSPs formulated CSPs-categorical structures. section, describe -categorical templatescorrespond set CSPs EI set constraints. order so, define analogsoperations e i, defined instead P pNq.Proposition 33. isomorphism A2 A.Proof. straightforward verify A2 countable atomless Boolean algebra.Motivated properties e described Lemma 11, make following definition.Definition 34. Let B B1 two arbitrary Boolean algebras domains B B 1 ,respectively, let g : B B 1 function strongly preserves [, 0, 1. sayg forgets unions k 1, l 0, x1 , . . . , xk , y1 , . . . , yl P Bepx1 q \ \ epxk q \ epy1 q \ \ epyl q 1exists k xi \ y1 \ \ yl1.Proposition 35. exists injection e : strongly preservesA, forgets unions.[, 0, 1Proof. construction e standard application Konigs tree lemma categorical structures (see, e.g., Bodirsky & Dalmau, 2012, Lemma 2); suffices showinjection f every finite induced substructure B fstrongly preserves [, 0, 1, forgets unions.let B finite substructure A, let B domain B. Let CpP pB q; [, \, c, 0, 1q Boolean algebra subsets B. claim g : B P pB qgiven g p1q 1 g pxq tz | z 0 ^ z B xu x 1preserves 0 1: definition;751fiBodirsky & Hilspreserves[: x, P B (including case x 1 1)g pxq [C g py q tz | z 0 ^ z B x ^ z B u(z | z 0 ^ z B px [B qg p x [B q ;injective: x,x y;strongly preservesP B gpxq gpyq, x B B x, hence[: follows previous two items;forgets unions: shown analogously proof Lemma 11.Indeed, one xi \ y1 \ \ yl 1 iff xi B j yj iff xi [ j yj j yj iffg pxi q[ j g pyj q j g pyj q iff g pxi q\ g py1 q\ \ g pyl q 1. Thus, xi \ y1 \ \ yl 11 k implies g px1 q \ \ g pxk q \ g py1 q \ \ g pyl q 1.prove converse, use finite Boolean algebra B may identifiedpP pAq; [, \, c, 0, 1q finite set A. Xi : xi \ y1 \ \ yl 1 1, . . . , k,may choose ai P AzXi , i.e. ai P j yj [ xi , 1, . . . , k. Let C : ta1 , . . . , ak uA, C P B. construction, k one tC u R g pxi q \ g py1 q \ \ g pyl q.particular, follows g px1 q \ \ g pxk q \ g py1 q \ \ g pyl q 1.Clearly, embedding h C A. f : hpg q homomorphismB forgets unions.Proposition 36. Let quantifier-free formula signature t[, \, c, 0, 1u.e preserves e preserves A. Moreover, every operationstrongly preserves [, 0, 1 forgets unions generates e, generatede.Proof. Let tuple elements A. Clearly, exists tuple b elementsP pNq b satisfy set quantifier-free formulas; followsfact every finite Boolean algebra Boolean algebra subsets finite set.observe whether tuple epbq satisfies quantifier-free formuladepends , Lemma 11. Since e strongly preserves [, 0, 1, forgets unions,true quantifier-free formulas hold epaq. Hence, e preservese preserves S.prove second part statement, use Theorem 32. Suppose ctuples (of length) elements satisfy quantifier-freeformulas. Since quantifier-elimination, follows c satisfy firstorder formulas A. consequence theorem Ryll-Nardzewski mentionedbeginning Section 6.1, exists automorphism maps c d.observations Theorem 32, implies operations strongly preserve[, 0, 1, forget unions generate other.r operation px, q epipx, qq.Let ei752fiTractable Set ConstraintsProposition 37. Let quantifier-free formula signature t[, \, c, 0, 1u.r preserves A. Moreover, every binary operationei preserves eir generatedg strongly preserves [, 0, 1, forgets unions generates ei,rei.Proof. arguments similar ones given proof Proposition 36. a1a2 n-tuples elements A, n-tuples b1 , b2 elements P pNqpa1 , a2 q pb1 , b2 q satisfy set quantifier-free formulas. Whethereipb1 , b2 q satisfies quantifier-free formula depends , ei strongly preserves [,r pa1 , a2 q, eir preserves0, 1, forgets unions. holds eiei preserves S.proof second part statement identical one Proposition 36.6.3 Central Argumentgive central argument maximal tractability EI, stated universalalgebraic language. say operation Ak depends argumentP t1, . . . , k u pk 1q-ary operation f 1 x1 , . . . , xk Pf px1 , . . . , xk q f 1 px1 , . . . , xi1 , xi1 , . . . , xkq.equivalently characterize k-ary operations depend i-th argumentrequiring x1 , . . . , xk P x1i Pf px1 , . . . , xk q f px1 , . . . , xi1 , x1i , xi1 , . . . , xkq.following general fact injective maps.Lemma 38. Let f : Ak function depends arguments,locally generated set injective operations F. f injective.Proof. first prove every term px1 , . . . , xn q formed operations Fvariables x1 , . . . , xn every variable appears least defines injectivemap. prove induction term structure. case n 1x1 nothing show. Otherwise, form f pT1 , . . . , Tk q k-ary f P FTj Tj pxi1 , . . . , ximpj q q j k term operations F variablesxi1 , . . . , ximpj q appears least Tj . suppose a1 , . . . , Pb1 , . . . , bn P pa1 , . . . , q pb1 , . . . , bn q. want show ai bin. Since f injective must Tj pai1 , . . . , aimpj q q Tj pbi1 , . . . , bimpj q qj k. Since every variable x1 , . . . , xn appears least once, variablexi must appear Tj , j k. Since Tj defines injective operation inductiveassumptions, must ai bi . follows defines injective map.suppose f operation locally generated F dependsarguments. Thus, ci1 , . . . , cin di f pci1 , . . . , cin qf pci1 , . . . , cii1 , di , cii 1 , . . . , cin q. Let a1 , . . . , , b1 , . . . , bn P f pa1 , . . . , qf pb1 , . . . , bn q. show a1 b1 , . . . , bn . Since f locally generated753fiBodirsky & HilsF, exists term px1 , . . . , xn q composed variables x1 , . . . , xn operations F pe1 , . . . , en q f pe1 , . . . , en q elements e1 , . . . , enset ta1 , . . . , , b1 , . . . , bn , c11 , . . . , cnn , d1 , . . . , dn u. i, variable xi must appearpx1 , . . . , xn q pci1 , . . . , cin q pci1 , . . . , cii1 , di , cii 1 , . . . , cin q. Hence, argumentbeginning proof shows px1 , . . . , xn q defines injective map,therefore a1 b1 , . . . , bn . shown f injective.r u. either tf u generates ei,r fTheorem 39. Let f operation generated teigenerated teu.r u.Proof. show statement theorem, let f k-ary operation generated teisake notation, let x1 , . . . , xl arguments f depends, l k.Let f 1 : Al operation given f 1 px1 , . . . , xl q f px1 , . . . , xl1 , xl , xl , . . . , xl q.Observe f 1 depends arguments, locally generated injective operations;Lemma 38, f 1 injective. Since f 1 generated operations preserve 0, 1,[, also f 1 preserves them. f 1 injective, even strongly preserves 0, 1, [.Consider first case l 1, i.e., f 1 unary. finite subsets A,operation f 1 equals automorphism A, f generated AutpAqnothing show. assume otherwise; is, assume finite setP AutpAq f 1 pxq apxq x P S. claim f 1 forgetsunions. see this, let u1 , . . . , um , v1 , . . . , vn f 1 pu1 q \ \ f 1 pum q \u, term pxq composedf 1 pv1 q \ \ f 1 pvn q 1. Since f 1 generated teirei, automorphisms A, single variable x f 1 pxq pxqx P tu1 , . . . , um , v1 , . . . , vn u. choice S, term cannot composedautomorphisms alone, hence must P AutpAq operational terms T1 , T2r f 1 pxq apeir pT1 pxq, T2 pxqqqcomposed automorphisms eirx P S. ei forgets unions, exists k T1 pui q\ T1 pv1 q\ \ Tl pvn q 1.Since T1 strongly preserves [, means ui \ v 1 \ \ v n 1 (see proofProposition 10), wanted show. Proposition 36 follows f 1generated e. f generated e well.Next, consider case l 1. Let g binary operation defined g px, q1f px, y, . . . , q. functions depends arguments, cannot generatedautomorphisms alone. Hence, term formr pT1 px, q, T2 px, qqqpx, q apeiP AutpAq,r automorphisms A,T1 T2 operational terms composed ei,two variables x y,g px, q px, q px, q P tu1 , . . . , um , v1 , . . . , vn u.claim g forgets unions. Assume g pu1 q\ \ g pum q\ g pv1 q\ \ g pvn q 1elements u1 pu11 , u21 q, . . . , um pu1m , u2m q, v1 pv11 , v12 q, . . . , vn pvn1 , vn2 q A2 .r forgets unions, exists k T1 pui q\ T1 pv1 q\ \ T1 pvn q 1Since ei754fiTractable Set ConstraintsT2 pui q\T2 pv1 q\ \T2 pvn q 1. Suppose first T1 depends arguments. T1defines injective operation strongly preserves [. follows ui \ v 1 \ \ v n 1A2 since equations inner Horn. argue similarly T2 dependsarguments, cases established g forgets unions. SupposeT1 T2 depend arguments. Consider first caseT1 depends first argument. function x T1 px, xq injectivestrongly preserves [, T1 pui q \ T1 pv1 q \ \ T1 pvn q 1 deriveu1i \ v11 \ \ vn1 1 holds A. case, T2 must depend second argument,since depends arguments. therefore also u2i \ v12 \ \ vn2 1 holdsA. situation T1 depends second argument T2 dependsrfirst argument analogous. g forgets unions. Proposition 37, g generates ei.rConsequently, also f generates ei.Theorem 40. Let set constraint language. Suppose contains relationsEI, also contains relation EI. finite sublanguage 1CSPp1 q NP-hard.Proof. R1 , R2 , . . . relations , let 1 , 2 , . . . quantifier-free formulasdefine R1 , R2 , . . . pP pNq; \, [, c, 0, 1q. Let R1A , R2A , . . . relations defined1 , 2 , . . . A, let relational structure domain exactlyr containsrelations. Proposition 37, contains relation preserved ei,r Consider set F polymorphisms .relations preserved ei.Theorem 32, operations F locally generated eri.set F contain eri, since would contradict Theorem 32 factr Since F locally closed, followscontains relation preserved ei.Theorem 39 operations f P F generated e. relationtpx, y, zq | x z _ x zu preserved operations F (we already seenrelation Example 5), hence pp definable Theorem 30. relationNP-complete CSP (Bodirsky & Kara, 2008). Let 1 reduct containsexactly relations appear pp definition tpx, y, z q | x z _ x z u. Clearly, finitely many relations; denote corresponding relationsymbols 1 . Lemma 29, CSPp1 q NP-hard.establishes also hardness CSPpq: let 1 1 -reduct . claimCSPp1 q CSPp1 q computational problem. showconjunction atomic 1 -formulas satisfiable 1 true 1 .Replacing atomic 1 -formula quantifier-free definition, followsTheorem 28.7. Concluding Remarksintroduced powerful set constraint language EI set constraints,particular contains Horn-Horn set constraints previously studied tractable setconstraint languages. Constraint satisfaction problems EI solved polynomialeven quadratic time. tractability result complemented complexity resultshows tractability EI set constraints best-possible within large classset constraint languages.755fiBodirsky & Hilswould also like remark algorithm test whether given finite setconstraint language (where relations language given quantifier-free formulassignature t\, [, c, 0, 1u) contained EI. means so-called metaproblem EI set constraints decided effectively.Proposition 41. algorithm test whether given quantifier-free formulasignature t\, [, c, 0, 1u defines relation EI.Proof. clear effectively transformed normal form describedSection 3, assume conjunction outer clauses,atomic formula form 1 inner conjunctive normal form.Let n number variables . test two n-tuples u1 , u2elements P pNq satisfy , n-tuple eipu1 , u2 q satisfies well. Notewhether tuple satisfies depends Boolean algebra generatedentries tuple S. Boolean algebra generated n elements sizen22 ; therefore, finitely many cases check. pair Booleanalgebras generating tuples u1 , u2 , check whether eipu1 , u2 q satisfies follows.Lemma 11, eipu1 , u2 q satisfies atomic formula 1 every inner clausex1 \\ xk \ epy1q \ \ epy1q exists k ipu1, u2 q satisfiesxi \ j j 1. turn true u1 u2 satisfy xi \ j j 1.truth value non-atomic formulas tuple eipu1 , u2 q computedtruth value atomic formulas usual way.Finally would also like remark one analogously obtain tractabilityclass constraints inner clauses positive outer literals dual Horn(i.e., one negative literal). statements proofs respective resultobtained dualizing following formal sense: dual relation Rdefinable Boolean algebra relation tcptq | P Ru. dual k-ary operationf domain operation px1 , . . . , xk q cpf pcpx1 q, . . . , cpxk qqq. proofstranslate literally proofs dualized versions statements.Acknowledgmentsextended abstract article appeared proceedings IJCAI11 (Bodirsky, Hils,& Krimkevitch, 2011)3 . want thank Francois Bossiere pointed mistakesconference version paper. One mistake concerned reduction CSPlanguages EI satisfiability Horn-Horn clauses; concerned problemprevious proof Theorem 26.Manuel Bodirsky received funding ERC European CommunitysSeventh Framework Programme (FP7/2007-2013 Grant Agreement no. 257039).3. third author conference version left author team preparation journal version.756fiTractable Set ConstraintsReferencesAiken, A. (1994). Set constraints: Results, applications, future directions. ProceedingsSecond Workshop Principles Practice Constraint Programming,pp. 326335.Baader, F. (2003). Least common subsumers specific concepts description logicexistential restrictions terminological cycles. Proceedings InternationalJoint Conferences Artificial Intelligence (IJCAI), pp. 319324.Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. International JointConferences Artificial Intelligence (IJCAI), pp. 364369.Barto, L., & Kozik, M. (2009). Constraint satisfaction problems bounded width.Proceedings Annual Symposium Foundations Computer Science (FOCS),pp. 595603.Bodirsky, M. (2012). Complexity classification infinite-domain constraint satisfaction.Memoire dhabilitation diriger des recherches, Universite Diderot Paris 7. AvailablearXiv:1201.0856.Bodirsky, M., Chen, H., & Pinsker, M. (2010). reducts equality primitivepositive interdefinability. Journal Symbolic Logic, 75 (4), 12491292.Bodirsky, M., & Dalmau, V. (2012). Datalog constraint satisfaction infinite templates. appear Journal Computer System Sciences. preliminaryversion appeared proceedings Symposium Theoretical AspectsComputer Science (STACS05).Bodirsky, M., Hils, M., & Krimkevitch, A. (2011). Tractable set constraints. ProceedingsInternational Joint Conferences Artificial Intelligence (IJCAI), pp. 510515.Bodirsky, M., Hils, M., & Martin, B. (2011). scope universal-algebraic approach constraint satisfaction. appear Logical Methods Computer Science (LMCS), 9099. Available arXiv:0909.5097v3. extended abstractannounced results appeared proceedings Logic ComputerScience (LICS10).Bodirsky, M., & Kara, J. (2008). complexity equality constraint languages. TheoryComputing Systems, 3 (2), 136158. conference version appeared proceedingsComputer Science Russia (CSR06).Bodirsky, M., & Kara, J. (2009). complexity temporal constraint satisfaction problems. Journal ACM, 57 (2), 141. extended abstract appearedProceedings Symposium Theory Computing (STOC08).Bodirsky, M., & Kutz, M. (2007). Determining consistency partial tree descriptions.Artificial Intelligence, 171, 185196.Bodirsky, M., & Nesetril, J. (2006). Constraint satisfaction countable homogeneoustemplates. Journal Logic Computation, 16 (3), 359373.Boole, G. (1847). Investigation Laws Thought. Walton, London. ReprintedPhilisophical Library, New York, 1954.757fiBodirsky & HilsBroxvall, M., Jonsson, P., & Renz, J. (2002). Disjunctions, independence, refinements.Artificial Intelligence, 140 (1/2), 153173.Bulatov, A. A. (2003). Tractable conservative constraint satisfaction problems. Proceedings Symposium Logic Computer Science (LICS), pp. 321330, Ottawa,Canada.Bulatov, A. A. (2006). dichotomy theorem constraint satisfaction problems3-element set. Journal ACM, 53 (1), 66120.Bulatov, A. A., & Dalmau, V. (2006). simple algorithm Maltsev constraints. SIAMJournal Computing, 36 (1), 1627.Bulatov, A. A., Krokhin, A. A., & Jeavons, P. G. (2005). Classifying complexityconstraints using finite algebras. SIAM Journal Computing, 34, 720742.Cohen, D., Jeavons, P., Jonsson, P., & Koubarakis, M. (2000). Building tractable disjunctiveconstraints. Journal ACM, 47 (5), 826853.Dowling, W. F., & Gallier, J. H. (1984). Linear-time algorithms testing satisfiabilitypropositional Horn formulae. Journal Logic Programming, 1 (3), 267284.Drakengren, T., & Jonsson, P. (1998). Reasoning set constraints applied tractableinference intuitionistic logic. Journal Logic Computation, 8 (6), 855875.Garey, M., & Johnson, D. (1978). guide NP-completeness. CSLI Press, Stanford.Hodges, W. (1993). Model theory. Cambridge University Press.Idziak, P. M., Markovic, P., McKenzie, R., Valeriote, M., & Willard, R. (2010). Tractabilitylearnability arising algebras subpowers. SIAM Journal Computing, 39 (7), 30233037.Jonsson, P., & Backstrom, C. (1998). unifying approach temporal constraint reasoning.Artificial Intelligence, 102 (1), 143155.Jonsson, P., & Drakengren, T. (1997). complete classification tractability RCC-5.Journal Artificial Intelligence Research, 6, 211221.Koppelberg, S. (1989). Projective boolean algebras. Handbook Boolean Algebras,Vol. 3, pp. 741773. North Holland, Amsterdam-New York-Oxford- Tokyo.Koubarakis, M. (2001). Tractable disjunctions linear constraints: Basic results applications temporal reasoning. Theoretical Computer Science, 266, 311339.Krotzsch, M., Rudolph, S., & Hitzler, P. (2006). complexity Horn descriptionlogics. OWL: Experiences Directions Workshop.Kuncak, V., Nguyen, H. H., & Rinard, M. C. (2006). Deciding boolean algebra presburger arithmetic. Journal Automatic Reasoning, 36 (3), 213239.Kuncak, V., & Rinard, M. C. (2007). Towards efficient satisfiability checking booleanalgebra presburger arithmetic. Proceedings International Conferenceautomated deduction (CADE), pp. 215230.Kusters, R., & Molitor, R. (2002). Approximating specific concepts descriptionlogics existential restrictions. AI Communications, 15 (1), 4759.758fiTractable Set ConstraintsLassez, J.-L., & McAloon, K. (1989). Independence negative constraints. InternationalJoint Conference Theory Practice Software Development (TAPSOFT), Volume 1, pp. 1927.Marriott, K., & Odersky, M. (1996). Negative Boolean constraints. Theoretical ComputerScience, 160 (1&2), 365380.Schaefer, T. J. (1978). complexity satisfiability problems. ProceedingsSymposium Theory Computing (STOC), pp. 216226.Szendrei, A. (1986). Clones universal algebra. Seminaire de Mathematiques Superieures.Les Presses de lUniversite de Montreal.759fiJournal Artificial Intelligence Research 45 (2012) 305-362Submitted 4/12; published 10/12Tutorial Dual Decomposition Lagrangian RelaxationInference Natural Language ProcessingAlexander M. RushSRUSH @ CSAIL . MIT. EDUComputer Science Artificial Intelligence LaboratoryMassachusetts Institute TechnologyCambridge, 02139, USAMichael CollinsMCOLLINS @ CS . COLUMBIA . EDUDepartment Computer ScienceColumbia UniversityNew York, NY 10027, USAAbstractDual decomposition, generally Lagrangian relaxation, classical method combinatorial optimization; recently applied several inference problems natural language processing (NLP). tutorial gives overview technique. describe example algorithms, describe formal guarantees method, describe practical issues implementingalgorithms. examples predominantly drawn NLP literature, materialgeneral relevance inference problems machine learning. central themetutorial Lagrangian relaxation naturally applied conjunction broad class combinatorial algorithms, allowing inference models go significantly beyond previous workLagrangian relaxation inference graphical models.1. Introductionmany problems statistical natural language processing, task map input x (e.g.,string) structured output (e.g., parse tree). mapping often defined= argmax h(y)(1)yYfinite set possible structures input x, h : R function assignsscore h(y) Y. example, part-of-speech tagging, x would sentence,would set possible tag sequences x; parsing, x would sentence wouldset parse trees x; machine translation, x would source-language sentencewould set possible translations x. problem finding referreddecoding problem. size typically grows exponentially respect sizeinput x, making exhaustive search intractable.paper gives overview decoding algorithms NLP based dual decomposition,generally, Lagrangian relaxation. Dual decomposition leverages observationmany decoding problems decomposed two sub-problems, together linearconstraints enforce notion agreement solutions different problems.sub-problems chosen solved efficiently using exact combinatorialc2012AI Access Foundation. rights reserved.fiRUSH & C OLLINSalgorithms. agreement constraints incorporated using Lagrange multipliers, iterativealgorithmfor example, subgradient algorithmis used minimize resulting dual. Dualdecomposition algorithms following properties:typically simple efficient. example, subgradient algorithms involve twosteps iteration: first, sub-problems solved using combinatorial algorithm; second, simple additive updates made Lagrange multipliers.well-understood formal properties, particular connections linear programming (LP) relaxations.cases underlying LP relaxation tight, produce exact solutionoriginal decoding problem, certificate optimality.1 cases underlying LPtight, heuristic methods used derive good solution; alternatively, constraintsadded incrementally relaxation tight, point exact solutionrecovered.Dual decomposition, two combinatorial algorithms used, special caseLagrangian relaxation (LR). useful also consider LR methods make use singlecombinatorial algorithm, together set linear constraints incorporated usingLagrange multipliers. use single combinatorial algorithm qualitatively differentdual decomposition approaches, although techniques closely related.Lagrangian relaxation long history combinatorial optimization literature, going backseminal work Held Karp (1971), derive relaxation algorithm travelingsalesman problem. Initial work Lagrangian relaxation/dual decomposition decoding statistical models focused MAP problem Markov random fields (Komodakis, Paragios, &Tziritas, 2007, 2011). recently, decoding algorithms derived several modelsstatistical NLP, including models combine weighted context-free grammar (WCFG)finite-state tagger (Rush, Sontag, Collins, & Jaakkola, 2010); models combine lexicalizedWCFG discriminative dependency parsing model (Rush et al., 2010); head-automata modelsnon-projective dependency parsing (Koo, Rush, Collins, Jaakkola, & Sontag, 2010); alignmentmodels statistical machine translation (DeNero & Macherey, 2011); models event extraction(Riedel & McCallum, 2011); models combined CCG parsing supertagging (Auli & Lopez,2011); phrase-based models statistical machine translation (Chang & Collins, 2011); syntaxbased models statistical machine translation (Rush & Collins, 2011); models semantic parsing (Das, Martins, & Smith, 2012); models parsing tagging make use document-levelconstraints (Rush, Reichart, Collins, & Globerson, 2012); models coordination problemnatural language parsing (Hanamoto, Matsuzaki, & Tsujii, 2012); models based intersection weighted automata (Paul & Eisner, 2012). give overview severalalgorithms paper.focus examples natural language processing, material tutorialgeneral relevance inference problems machine learning. clear relevanceproblem inference graphical models, described example Komodakis et al.(2007, 2011); however one central theme tutorial Lagrangian relaxation naturally1. certificate optimality information allows proof optimality solution constructed polynomial time.306fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGapplied conjunction much broader class combinatorial algorithms max-productbelief propagation, allowing inference models go significantly beyond graphical models.remainder paper structured follows. Section 2 describes related work. Section 3gives formal introduction Lagrangian relaxation. Section 4 describes dual decompositionalgorithm (from Rush et al., 2010) decoding model combines weighted context-freegrammar finite-state tagger. algorithm used running example throughoutpaper. Section 5 describes formal properties dual decomposition algorithms. Section 6 givesexamples algorithms, section 7 describes practical issues. Section 8 gives overviewwork alternative optimization methods subgradient methods described tutorial.Finally, section 9 describes relationship LP relaxations, describes tightening methods.2. Related Worktutorial draws ideas fields combinatorial optimization, machine learning,natural language processing. section, give summary work fieldsrelevant methods describe.2.1 Combinatorial OptimizationLagrangian relaxation (LR) widely used method combinatorial optimization, going backseminal work Held Karp (1971) traveling salesman problem. See workLemarechal (2001) Fisher (1981) surveys LR methods, textbook KorteVygen (2008) background combinatorial optimization. Decomposing linear integerlinear programs also fundamental technique optimization community (Dantzig & Wolfe,1960; Everett III, 1963). direct relationship LR algorithms linearprogramming relaxations combinatorial optimization problems; again, see textbook KorteVygen.2.2 Belief Propagation, Linear Programming Relaxations Inference MRFslarge amount research MAP inference problem Markov random fields(MRFs). tree-structured MRFs, max-product belief propagation (max-product BP) (Pearl, 1988)gives exact solutions. (Max-product BP form dynamic programming, closely related Viterbi algorithm.) general MRFs underlying graph may contain cycles,MAP problem NP-hard: led researchers consider number approximate inference algorithms. Early work considered loopy variants max-product BP (see exampleFelzenszwalb & Huttenlocher, 2006, application loopy max-product BP problemscomputer vision); however, methods heuristic, lacking formal guarantees.recent work considered methods based linear programming (LP) relaxationsMAP problem. See work Yanover, Meltzer, Weiss (2006), section 1.6 workSontag, Globerson, Jaakkola (2010), description. Methods based LP relaxationsbenefit stronger guarantees loopy belief propagation. Inference cast optimizationproblem, example problem minimizing dual. Since dual problem convex, convergence results convex optimization linear programming leveraged directly. Oneparticularly appealing feature methods certificates optimality givenexact solution MAP problem found.307fiRUSH & C OLLINSKomodakis et al. (2007, 2011) describe dual decomposition method provably optimizesdual LP relaxation MAP problem, using subgradient method. workcrucial reference tutorial. (Note addition, Johnson, Malioutov, & Willsky, 2007, alsodescribes LR methods inference MRFs.)tutorial focus subgradient algorithms optimization dual objective. Seesection 8 discussion alternative optimization approaches developed withinmachine learning community.2.3 Combinatorial Algorithms Belief Propagationcentral idea algorithms describe use combinatorial algorithms maxproduct BP. idea closely related earlier work use combinatorial algorithms withinbelief propagation, either MAP inference problem (Duchi, Tarlow, Elidan, & Koller, 2007),computing marginals (Smith & Eisner, 2008). methods generalize loopy BP wayallows use combinatorial algorithms. Again, argue methods based Lagrangianrelaxation preferable variants loopy BP, stronger formal guarantees.2.4 Linear Programs Decoding Natural Language ProcessingDual decomposition Lagrangian relaxation closely related integer linear programming(ILP) approaches, linear programming relaxations ILP problems. Several authorsused integer linear programming directly solving challenging problems NLP. Germann, Jahr,Knight, Marcu, Yamada (2001) use ILP test search error greedy phrase-based translation system short sentences. Roth Yih (2005) formulate constrained sequence labelingproblem ILP decode using general-purpose solver. Lacoste-Julien, Taskar, Klein,Jordan (2006) describe quadratic assignment problem bilingual word alignment decode using ILP solver. work Riedel Clarke (2006) Martins, Smith, Xing(2009) formulates higher-order non-projective dependency parsing ILP. Riedel Clarke decode using ILP method constraints added incrementally. Martins et al. solve LPrelaxation project valid dependency parse. Like many works, method presentedtutorial begins ILP formulation decoding problem; however, instead employing general-purpose solver aim speed decoding using combinatorial algorithmsexploit underlying structure problem.3. Lagrangian Relaxation Dual Decompositionsection first gives formal description Lagrangian relaxation, gives descriptiondual decomposition, important special case Lagrangian relaxation. descriptionsgive deliberately concise. material section essential remainderpaper, may safely skipped reader, returned second reading. Howeverdescriptions may useful would like immediately see formal treatmentLagrangian relaxation dual decomposition. algorithms paper special casesframework described section.308fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING3.1 Lagrangian Relaxationassume finite set Y, subset Rd . score associatedvectorh(y) =also vector Rd . decoding problem find= argmax h(y) = argmaxyY(2)yYdefinitions, structure represented d-dimensional vector, scorestructure linear function, namely . practice, structured prediction problemsoften binary vector (i.e., {0, 1}d ) representing set parts2 present structure y.vector assigns score part, definition h(y) = implies scoresum scores parts contains.assume problem Eq. 2 computationally challenging. cases,might NP-hard problem. cases, might solvable polynomial time,algorithm still slow practical.first key step Lagrangian relaxation choose finite set 0 Rdfollowing properties:0 . Hence 0 contains vectors found Y, addition contains vectorsY.value Rd , easily findargmaxyY 0(Note replaced Eq. 2 larger set 0 .) easily meanproblem significantly easier solve problem Eq. 2. example, problemEq. 2 might NP-hard, new problem solvable polynomial time;problems might solvable polynomial time, new problem significantlylower complexity.Finally, assume= {y : 0 Ay = b}(3)Rpd b Rp . condition Ay = b specifies p linear constraints y.assume number constraints, p, polynomial size input.implication linear constraints Ay = b need added set 0 ,constraints considerably complicate decoding problem. Instead incorporating hardconstraints, deal constraints using Lagrangian relaxation.2. example, context-free parsing part might correspond tuple hA B C, i, k, ji B Ccontext-free rule, i, k, j integers specifying non-terminal spans words . . . j input sentence,non-terminal B spans words . . . k, non-terminal C spans words (k + 1) . . . j. finite-state taggingbigram tagging model part might tuple hA, B, ii A, B tags, integer specifying tagB seen position sentence, tag seen position (i 1). See work Rush et al. (2010)detailed treatment examples.309fiRUSH & C OLLINSintroduce vector Lagrange multipliers, u Rp . LagrangianL(u, y) = + u (Ay b)function combines original objective function , second term incorporateslinear constraints Lagrange multipliers. dual objectiveL(u) = max0 L(u, y)yYdual problem findminp L(u)uRcommon approachwhich used algorithms paperis use subgradientalgorithm minimize dual. set initial Lagrange multiplier values u(0) = 0.k = 1, 2, . . . perform following steps:(k) = argmax L(u(k1) , y)(4)u(k) = u(k1) k (Ay (k) b)(5)yY 0followedk > 0 step size kth iteration. Thus iteration first find structure(k) , update Lagrange multipliers, updates depend (k) .crucial point (k) found efficiently,argmax L(u(k1) , y) = argmax + u(k1) (Ay b) = argmax 0yY 0yY 0yY 00 = + A> u(k1) . Hence Lagrange multiplier terms easily incorporatedobjective function.state following theorem:Theorem 1 following properties hold Lagrangian relaxation:a). u Rp , L(u) maxyY h(y).b). suitable choice step sizes k (see section 5), limk L(u(k) ) = minu L(u).c). Define yu = argmaxyY 0 L(u, y). u Ayu = b, yu = argmaxyY (i.e.,yu optimal).particular, subgradient algorithm described above, k Ay (k) = b,(k) = argmaxyY .d). minu L(u) = maxQ , set Q defined below.310fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGThus part (a) theorem states dual value provides upper bound scoreoptimal solution, part (b) states subgradient method successfully minimizes upperbound. Part (c) states ever reach solution (k) satisfies linear constraints,solved original optimization problem.Part (d) theorem gives direct connection Lagrangian relaxation methodLP relaxation problem Eq. 2. define set Q. First, define setdistributions set 0 :0= { : R|Y | ,XyY 0= 1, 0 1}convex hull 0 definedConv(Y 0 ) = { Rd : s.t. =Xy}yY 0Finally, define set Q follows:Q = {y : Conv(Y 0 ) Ay = b}Note similarity Eq. 3: simply replaced 0 Eq. 3 convex hull 0 . 0subset Conv(Y 0 ), hence subset Q. consequence Minkowski-Weyl theorem(Korte & Vygen, 2008, Thm. 3.31) Conv(Y 0 ) polytope (a bounded set specifiedintersection finite number half spaces), Q therefore also polytope. problemmaxQtherefore linear program, relaxation original problem, maxyY .Part (d) theorem 1 direct consequence duality linear programming.following implications:minimizing dual L(u), recover optimal value maxQ LPrelaxation.maxQ = maxyY say LP relaxation tight. casesubgradient algorithm guaranteed3 find solution original decoding problem,= argmax = argmaxQyYcases LP relaxation tight, methods (e.g., see Nedic & Ozdaglar,2009) allow us recover approximate solution linear program, = argmaxQ. Alternatively, methods used tighten relaxation exact solution obtained.3. assumption unique solution problem maxyY ; solution uniquesubtleties may arise.311fiRUSH & C OLLINS3.2 Dual Decompositiongive formal description dual decomposition. see, dual decompositionspecial case Lagrangian relaxation;4 however, important enough purposestutorial warrant description. Again, section deliberately concise, may safelyskipped first reading.assume finite set Rd . vector associatedscoref (y) = (1)0(1) vector Rd . addition, assume second finite set Z Rd , vectorz Z associated scoreg(z) = z (2)decoding problem findargmax (1) + z (2)yY,zZAy + Cz = b0Rpd , C Rpd , b Rp .Thus decoding problem find optimal pair structures, linear constraintsspecified Ay + Cz = b. practice, linear constraints often specify agreement constraintsz: is, specify two vectors sense coherent.convenience, make connection Lagrangian relaxation clear, definefollowing sets:W = {(y, z) : Y, z Z, Ay + Cz = b}W 0 = {(y, z) : Y, z Z}follows decoding problem findargmax (1) + z (2)(6)(y,z)WNext, make following assumptions:4. Strictly speaking, Lagrangian relaxation also viewed special case dual decomposition: formulation section set Z = Y, (2) = 0, Ci,j = 0 i, j, thus recovering Lagrangian relaxationproblem previous section. sense Lagrangian relaxation dual decomposition equivalent (wetransform Lagrangian relaxation problem dual decomposition problem, vice versa). However,view dual decomposition naturally viewed special case Lagrangian relaxation, particularmethods described tutorial go back work Held Karp (1971) (see section 6.3), makesuse single combinatorial algorithm. addition, Lagrangian relaxation appears standard termcombinatorial optimization literature: example textbook Korte Vygen (2008) descriptionLagrangian relaxation mention dual decomposition; several tutorials Lagrangian relaxationcombinatorial optimization literature (e.g., see Lemarechal, 2001; Fisher, 1981), founddifficult find direct treatments dual decompositon. Note however recent work machine learningcomputer vision communities often used term dual decomposition (e.g., Sontag et al., 2010; Komodakis et al.,2007, 2011).312fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGvalue (1) Rd , easily find argmaxyY (1) . Furthermore,0value (2) Rd , easily find argmaxzZ z (2) . follows (1) Rd ,0(2) Rd , easily find(y , z ) = argmax (1) + z (2)(7)(y,z)W 0setting= argmax (1) ,yYz = argmax z (2)zZNote Eq. 7 closely related problem Eq. 6, W replaced W 0 (i.e.,linear constraints Ay + Cz = b dropped). easily meanoptimization problems significantly easier solve original problem Eq. 6.clear problem special case Lagrangian relaxation setting,described previous section. goal involves optimization linear objective,finite set W, given Eq. 6; efficiently find optimal value set W 0 Wsubset W 0 , W 0 dropped linear constraints Ay + Cz = b.dual decomposition algorithm derived similar way before. introducevector Lagrange multipliers, u Rp . LagrangianL(u, y, z) = (1) + z (2) + u (Ay + Cz b)dual objectiveL(u) = max 0 L(u, y, z)(y,z)Wsubgradient algorithm used find minuRp L(u). initialize Lagrange multipliers u(0) = 0. k = 1, 2, . . . perform following steps:(y (k) , z (k) ) = argmax L(u(k1) , y, z)(y,z)W 0followedu(k) = u(k1) k (Ay (k) + Cz (k) b)k > 0 stepsize.Note solutions (k) , z (k) iteration found easily, easily verified!(k1)argmax L(u, y, z) =(y,z)W 0argmaxyY0(1), argmax z0(2),zZ0(1) = (1) + A> u(k1) 0(2) = (2) + C > u(k1) . Thus dual decomposes twoeasily solved maximization problems.formal properties dual decomposition similar stated theorem 1.particular, shownminp L(u) = max (1) + (2)uR(,)Q313fiRUSH & C OLLINSNPVPNVUnitedfliesNPNlargejetFigure 1: example parse tree.set Q definedQ = {(, ) : (, ) Conv(W 0 ) + C = d}problemmax (1) + (2)(,)Qlinear programming problem, L(u) dual linear program.descriptions Lagrangian relaxation dual decomposition givensufficient level generality include broad class algorithms, including introduced paper. remainder paper describes specific algorithms developed withinframework, describes experimental results practical issues arise, elaboratestheory underlying algorithms.Note section described dual-decomposition approach two components. generalization two components relatively straightforward; examplesee work Komodakis et al. (2007, 2011), see also work Martins, Smith, Figueiredo,Aguiar (2011).4. Example: Integration Parser Finite-State Taggernext describe dual decomposition algorithm decoding model combinesweighted context-free grammar finite-state tagger. classical approach problemuse dynamic programming algorithm, based construction Bar-Hillel, Perles,Shamir (1964) intersection context-free language finite-state language. dualdecomposition algorithm advantages exhaustive dynamic programming, termsefficiency simplicity. use dual decomposition algorithm running examplethroughout tutorial.first give formal definition problem, describe motivation problem, describe classical dynamic programming approach. describe dual decompositionalgorithm.314fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING4.1 Definition ProblemConsider problem mapping input sentence x parse tree y. Define setparse trees x. parsing problem find= argmax h(y)(8)yYh(y) score parse tree Y.consider case h(y) sum two model scores: first, scoreweighted context-free grammar; second, score part-of-speech (POS) sequencefinite-state part-of-speech tagging model. formally, define h(y)h(y) = f (y) + g(l(y))(9)functions f , g, l defined follows:1. f (y) score weighted context-free grammar (WCFG). WCFG consistscontext-free grammar set rules G, scoring function : G R assignsreal-valued score rule G. score entire parse tree sum scoresrules contains. example, consider parse tree shown Figure 1; tree,f (y) = (S NP VP) + (NP N) + (N United)+(VP V NP) + . . .remain agnostic scores individual context-free rules defined. oneexample, probabilistic context-free grammar, would define ( ) = log p(|). second example, conditional random field (CRF) (Lafferty, McCallum, &Pereira, 2001) would define ( ) = w ( ) w Rq parametervector, ( ) Rq feature vector representing rule .2. l(y) function maps parse tree sequence part-of-speech tags y.parse tree Figure 1, l(y) would sequence N V N.3. g(z) score part-of-speech tag sequence z mth-order finite-state taggingmodel. model, zi = 1 . . . n ith tag z,g(z) =nX(i, zim , zim+1 , . . . , zi )i=1(i, zim , zim+1 , . . . , zi ) score sub-sequence tags zim , zim+1 , . . . , ziending position sentence.5remain agnostic terms defined. one example, g(z) mightlog-probability z hidden Markov model, case(i, zim . . . zi ) = log p(zi |zim . . . zi1 ) + log p(xi |zi )5. define zi 0 special start POS symbol.315fiRUSH & C OLLINSxi ith word input sentence. another example, CRF would(i, zim . . . zi ) = w (x, i, zim . . . zi )w Rq parameter vector, (x, i, zim . . . zi ) feature-vector representationsub-sequence tags zim . . . zi ending position sentence x.motivation problem follows. scoring function h(y) = f (y) + g(l(y))combines information parsing model tagging model. two models capturefundamentally different types information: particular, part-of-speech tagger captures information adjacent POS tags missing f (y). information may improveparsing tagging performance, comparison using f (y) alone.6definition h(y), conventional approach finding Eq. 8 constructnew context-free grammar introduces sensitivity surface bigrams (Bar-Hillel et al., 1964).Roughly speaking, approach (assuming first-order tagging model) rulesVP V NPreplaced rulesVPN,N VN,V NPV,N(10)non-terminal (e.g., NP) replaced non-terminal tracks preceding lastPOS tag relative non-terminal. example, NPV,N represents NP dominates sub-treewhose preceding POS tag V, whose last POS tag N. weights new rulescontext-free weights f (y). Furthermore, rulesV fliesreplaced rulesVN,V fliesweights rules context-free weights f (y) plus bigram tag weightsg(z), example bigram N V. dynamic programming parsing algorithmfor exampleCKY algorithmcan used find highest scoring structure new grammar.approach guaranteed give exact solution problem Eq. 8; howeveroften inefficient. greatly increased size grammar introducing refinednon-terminals, leads significantly slower parsing performance. one example, considercase underlying grammar CFG Chomsky-normal form, G non-terminals,use 2nd order (trigram) tagging model, possible part-of-speech tags. Definen length input sentence. Parsing grammar alone would take O(G3 n3 )time, example using CKY algorithm. contrast, construction Bar-Hillel et al. (1964)6. assumed sensible, theoretical and/or empirical sense, take sum scores f (y)g(l(y)). might case, example, f (y) g(z) defined structured prediction models (e.g.,conditional random fields), parameters estimated jointly using discriminative methods. f (y) g(z)log probabilities PCFG HMM respectively, strict probabilistic sense makesense combine scores way: however practice may work well; example, type log-linearcombination probabilistic models widely used approaches statistical machine translation.316fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGresults algorithm run time O(G3 6 n3 ).7 addition tagging model leadsmultiplicative factor 6 runtime parser, significant decreaseefficiency (it uncommon take values say 5 50, giving values 6 larger15, 000 15 million). contrast, dual decomposition algorithm describe nexttakes O(k(G3 n3 + 3 n)) time problem, k number iterations requiredconvergence; experiments, k often small number. significant improvementruntime Bar-Hillel et al. method.4.2 Dual Decomposition Algorithmintroduce alternative formulation problem Eq. 8, lead directlydual decomposition algorithm. Define set POS tags. Assume input sentencen words. parse tree y, position {1 . . . n}, tag , definey(i, t) = 1 parse tree tag position i, y(i, t) = 0 otherwise. Similarly, tagsequence z, define z(i, t) = 1 tag sequence tag position i, 0 otherwise.example, following parse tree tag sequence y(4, A) = 1 z(4, A) = 1:NPNVPVUnited fliesNPNNlarge jetVUnited1 flies2some3 large4Njet5addition, define Z set possible POS tag sequences input sentence.introduce following optimization problem:Optimization Problem 1 Findargmax f (y) + g(z)(11)yY,zZ{1 . . . n}, , y(i, t) = z(i, t).Thus find best pair structures z share POS sequence.define (y , z ) pair structures achieve argmax problem. crucial7. precise, assume finite-state automaton Q states context-free chart rule productionshA B C, i, k, ji A, B, C G 1 < k < j n well productions hA wi , ii G{1 . . . n}. (Here use wi refer ith word sentence, set G refer set nonterminals grammar. follows G = |G|.) Applying Bar-Hillel intersection gives new rule productionshAs1 ,s3 Bs1 ,s2 Cs2 ,s3 , i, k, ji s1 , s2 , s3 {1 . . . Q} well hAs,t wi , ii s, {1 . . . Q}(s, t) valid state transition FSA. intersection, count free variables seeO(G3 n3 Q3 ) rule productions, implies CKY algorithm find best parse O(G3 n3 Q3 ) time.case tagging, 2nd-order tagging model represented FSA |T |2 states, staterepresents previous two tags. intersection, yields O(G3 n3 |T |6 ) time algorithm.317fiRUSH & C OLLINSclaim, easily verified, also argmax problem Eq. 8. sense,solving new problem immediately leads solution original problem.make following two assumptions. Whether assumptions satisfieddepend definitions f (y) (for assumption 1) definitions Z g(z)(for assumption 2). assumptions hold f (y) WCFG g(z) finite-state tagger,generally may hold parsing tagging models.Assumption 1 Assume introduce variables u(i, t) R {1 . . . n}, .assume value variables, findargmax f (y) +yYXu(i, t)y(i, t)i,tefficiently.example. Consider WCFG grammar Chomsky normal form. scoringfunction definedf (y) =XXY Zc(y, X Z)(X Z) +Xi,ty(i, t)(t wi )write c(y, X Z) denote number times rule X Z seenparse tree y, y(i, t) = 1 word POS t, 0 otherwise (note y(i, t) = 1 impliesrule wi used parse tree). highest scoring parse tree f (y)found efficiently, example using CKY parsing algorithm.argmax f (y) +yYXu(i, t)y(i, t) =i,targmaxyYXXY Zc(y, X Z)(X Z) +Xi,ty(i, t)((t wi ) + u(i, t))argmax found easily using CKY algorithm, scores (t wi )simply replaced new scores defined 0 (t wi ) = (t wi ) + u(i, t).Assumption 2 Assume introduce variables u(i, t) R {1 . . . n}, .assume value variables, findargmax g(z)zZXu(i, t)z(i, t)i,tefficiently.example. Consider 1st-order tagging model,g(z) =nX(i, zi1 , zi )i=1318fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGInitialization: Set u(0) (i, t) = 0 {1 . . . n},k = 1 KPP(k) argmaxyY f (y) +z (k) argmaxzZ g(z)(k1) (i, t)y(i, t) [Parsing]i,t u(k1) (i, t)z(i, t) [Tagging]i,t u(k) (i, t) = z (k) (i, t) i, Return (y (k) , z (k) )Else u(k+1) (i, t) u(k) (i, t) k (y (k) (i, t) z (k) (i, t))Figure 2: dual decomposition algorithm integrated parsing tagging. k k = 1 . . . Kstep size kth iteration.argmax g(z)zZXu(i, t)z(i, t)i,tnXXu(i, t)z(i, t)= argmax (i, zi1 , zi )zZ= argmaxzZi,ti=1nX0(i, zi1 , zi )i=10 (i, zi1 , zi ) = (i, zi1 , zi ) u(i, zi )argmax found efficiently using Viterbi algorithm, new 0 termsincorporate u(i, t) values.Given assumptions, dual decomposition algorithm shown Figure 2. algorithmmanipulates vector variables u = {u(i, t) : {1 . . . n}, }. soon seevariable u(i, t) Lagrange multiplier enforcing constraint y(i, t) = z(i, t) optimizationproblem. iteration algorithm finds hypotheses (k) z (k) ; assumptions 1 2step efficient. two structures POS sequence (i.e., (k) (i, t) = z (k) (i, t)(i, t)) algorithm returns solution. Otherwise, simple updates madeu(i, t) variables, based (k) (i, t) z (k) (i, t) values.moment well give example run algorithm. First, though, give importanttheorem:Theorem 2 iteration algorithm Figure 2 (k) (i, t) = z (k) (i, t)(i, t), (y (k) , z (k) ) solution optimization problem 1.319fiRUSH & C OLLINStheorem direct consequence theorem 5 paper.Thus reach agreement (k) z (k) , guaranteed optimalsolution original problem. Later tutorial give empirical results various NLPproblems showing often, quickly, reach agreement. also describetheory underlying convergence; theory underlying cases algorithm doesnt converge;methods used tighten algorithm goal achieving convergence.Next, consider efficiency algorithm. concrete, consider casef (y) defined weighted CFG, g(z) defined finite-state tagger.iteration algorithm requires decoding two models. numberiterations k relatively small, algorithm much efficient using constructionBar-Hillel et al. (1964). discussed before, assuming context-free grammar Chomskynormal form, trigram tagger tags, CKY parsing algorithm takes O(G3 n3 ) time,Viterbi algorithm tagging takes O(T 3 n) time. Thus total running time dualdecomposition algorithm O(k(G3 n3 + 3 n)) k number iterations requiredconvergence. contrast, construction Bar-Hillel et al. results algorithm runningtime O(G3 6 n3 ). dual decomposition algorithm results additive cost incorporatingtagger (a 3 n term added run time), whereas construction Bar-Hillel et al. resultsmuch expensive multiplicative cost (a 6 term multiplied run time). (Smith &Eisner, 2008, makes similar observation additive versus multiplicative costs contextbelief propagation algorithms dependency parsing.)4.3 Relationship Approach Section 3easily verified approach described instance dual decompositionframework described section 3.2. set set parses input sentence; setZ set POS sequences input sentence. parse tree Rd representedvector f (y) = y(1) (1) Rd : number ways representing parsetrees vectors, see work Rush et al. (2010) one example. Similarly, tag sequence00z Rd represented vector g(z) = z (2) (2) Rd . constraintsy(i, t) = z(i, t)(i, t) encoded linear constraintsAy + Cz = bsuitable choices A, C, b, assuming vectors z include components y(i, t)z(i, t) respectively.4.4 Example Run Algorithmgive example run algorithm. simplicity, assume step size kequal 1 iterations k. take input sentence United flies large jet. Initially,algorithm sets u(i, t) = 0 (i, t). example, decoding initial weightsleads two hypotheses320fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGNPNVPVNUnited flies large jetVUnited1 flies2 some3 large4Njet5two structures different POS tags three positions, highlighted red; thus twostructures agree. update u(i, t) variables based differences, giving newvalues follows:u(1, A) = u(2, N ) = u(5, V ) = 1u(1, N ) = u(2, V ) = u(5, N ) = 1u(i, t) values shown still value 0. decode new u(i, t) values,giving structuresVPNPNVUnited fliesNPNlarge jetNUnited1 flies2 some3 large4Njet5Again, differences structures shown red. update u(i, t) valuesobtain new values follows:u(5, N ) = 1u(5, V ) = 1u(i, t) values 0. (Note updates reset u(1, A), u(1, N ), u(2, N )u(2, V ) back zero.)decode again, new u(i, t) values; time, two structures321fiRUSH & C OLLINS100% examples converged80604020050<=20<=10<=4<=3<=2<=1<=number iterationsFigure 3: Convergence results work Rush et al. (2010) integration probabilisticparser POS tagger, using dual decomposition. show percentage examplesexact solution returned algorithm, versus number iterationsalgorithm.NPNVPVUnited fliesNPNlarge jetNVUnited1 flies2 some3 large4Njet5two structures identical sequences POS tags, algorithm terminates,guarantee solutions optimal.Rush et al. (2010) describe experiments using algorithm integrate probabilistic parserCollins (1997) POS tagger Toutanova, Klein, Manning, Singer (2003). (Inexperiments stepsize k held constant, instead set using strategy describedsection 7.2 paper.) Figure 3 shows percentage cases exact solutions returned(we agreement (k) z (k) ) versus number iterations algorithm.algorithm produces exact solutions 99% examples. 94% examplesalgorithm returns exact solution 10 iterations fewer. models least,dual decomposition algorithm guaranteed give exact solution, casesuccessful achieving goal.322fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING5. Formal Propertiesgive formal properties algorithm described previous section. firstdescribe three important theorems regarding algorithm, describe connectionsalgorithm subgradient optimization methods.5.1 Three TheoremsRecall problem attempting solve (optimization problem 1)argmax f (y) + g(z)yY,zZ= 1 . . . n, ,y(i, t) = z(i, t)first step introduce Lagrangian problem. introduce Lagrangemultiplier u(i, t) equality constraint y(i, t) = z(i, t): write u = {u(i, t) :{1 . . . n}, } denote vector Lagrange mulipliers. Lagrange multiplier takepositive negative value. LagrangianL(u, y, z) = f (y) + g(z) +Xi,tu(i, t) (y(i, t) z(i, t))(12)Note grouping terms depend z, rewrite LagrangianL(u, y, z) = f (y) +Xi,tu(i, t)y(i, t) + g(z)Xu(i, t)z(i, t)i,tdefined Lagrangian, dual objectiveL(u) =max L(u, y, z)yY,zZ= max f (y) +yYXi,tu(i, t)y(i, t) + max g(z)zZXu(i, t)z(i, t)i,tassumptions 1 2 described above, dual value L(u) value u calculatedefficiently: simply compute two maxs, sum them. Thus dual decomposesconvenient way two efficiently solvable sub-problems.Finally, dual problem minimize dual objective, is, findmin L(u)usee shortly algorithm Figure 2 subgradient algorithm minimizing dualobjective.Define (y , z ) optimal solution optimization problem 1. first theoremfollows:Theorem 3 value u,L(u) f (y ) + g(z )323fiRUSH & C OLLINSHence L(u) provides upper bound score optimal solution. proof simple:Proof:L(u) ==max L(u, y, z)yY,zZ(13)maxL(u, y, z)(14)maxf (y) + g(z)(15)yY,zZ:y=zyY,zZ:y=z= f (y ) + g(z )(16)use shorthand = z state y(i, t) = z(i, t) (i, t). Eq. 14 followsadding constraints = z, optimizing smaller set (y, z) pairs, hencemax cannot increase. Eq. 15 follows = z,Xi,tu(i, t) (y(i, t) z(i, t)) = 0hence L(u, y, z) = f (y) + g(z). Finally, Eq. 16 follows definition z .property L(u) f (y ) + g(z ) value u often referred weak duality.value inf u L(u) f (y ) g(z ) often referred duality gap optimal dualitygap (see example Boyd & Vandenberghe, 2004).Note obtaining upper bound f (y ) + g(z ) (providing relatively tight)useful goal itself. First, upper bounds form used admissible heuristicssearch methods A* branch-and-bound algorithms. Second, methodgenerates potential solution (y, z), immediately obtain upper bound far solutionoptimal,(f (y ) + g(z )) (f (y) + g(z)) L(u) (f (y) + g(z))Hence L(u) (f (y) + g(z)) small, (f (y ) + g(z )) (f (y) + g(z)) must small. Seesection 7 discussion.second theorem states algorithm Figure 2 successfully converges minu L(u).Hence algorithm successfully converges tightest possible upper bound given dual.theorem follows:Theorem 4 Consider algorithm Figure 2. sequence 1 , 2 , 3 , . . . k > 0k 1,lim k = 0kXk=1k = ,lim L(uk ) = min L(u)ukProof: See work Shor (1985). See also Appendix A.3.algorithm actually subgradient method minimizing L(u): return pointsection 5.2. though, important point algorithm successfully minimizes L(u).final theorem states ever reach agreement algorithm Figure 2,guaranteed optimal solution. first need following definitions:324fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGDefinition 1 value u, define(u) = argmax f (y) +yYXu(i, t)y(i, t)i,tz (u) = argmax g(z)zZXu(i, t)z(i, t)i,ttheorem then:Theorem 5 u(u) (i, t) = z (u) (i, t)i, t,f (y (u) ) + g(z (u) ) = f (y ) + g(z )i.e., (y (u) , z (u) ) optimal.Proof: have, definitions (u) z (u) ,L(u) = f (y (u) ) + g(z (u) ) +Xi,t= f (y(u)) + g(z(u)u(i, t)(y (u) (i, t) z (u) (i, t)))second equality follows (u) (i, t) = z (u) (i, t) (i, t). L(u) f (y ) +g(z ) values u, hencef (y (u) ) + g(z (u) ) f (y ) + g(z )z optimal, alsof (y (u) ) + g(z (u) ) f (y ) + g(z )hence mustf (y (u) ) + g(z (u) ) = f (y ) + g(z )Theorems 4 5 refer quite different notions convergence dual decompositionalgorithm. remainder tutorial, avoid confusion, explicitly use followingterms:d-convergence (short dual convergence) used refer convergence dualdecomposition algorithm minimum dual value: is, property limk L(u(k) ) =minu L(u). theorem 4, assuming appropriate step sizes algorithm, alwaysd-convergence.e-convergence (short exact convergence) refers convergence dual decomposition algorithm point y(i, t) = z(i, t) (i, t). theorem 5, dualdecomposition algorithm e-converges, guaranteed provided optimal solution. However, algorithm guaranteed e-converge.325fiRUSH & C OLLINS5.2 Subgradientsproof d-convergence, defined theorem 4, relies fact algorithm Figure 2subgradient algorithm minimizing dual objective L(u). Subgradient algorithmsgeneralization gradient-descent methods; used minimize convex functionsnon-differentiable. section describes algorithm Figure 2 derived subgradientalgorithm.Recall L(u) defined follows:L(u) =max L(u, y, z)yY,zZ= max f (y) +yYXi,tu(i, t)y(i, t) + max g(z)zZXu(i, t)z(i, t)i,tgoal find minu L(u).First, note L(u) following properties:L(u) convex function. is, u(1) Rd , u(2) Rd , [0, 1],L(u(1) + (1 )u(2) ) L(u(1) ) + (1 )L(u(2) )(The proof simple: see Appendix A.1.)L(u) differentiable. fact, easily shown piecewise linear function.fact L(u) differentiable means cannot use gradient descent methodminimize it. However, nevertheless convex function, instead use subgradientalgorithm. definition subgradient follows:Definition 2 (Subgradient) subgradient convex function L : Rd R u vector (u)v Rd ,L(v) L(u) + (u) (v u)subgradient (u) tangent point u gives lower bound L(u): sensesimilar8 gradient convex differentiable function.9 key idea subgradientmethods use subgradients way would use gradients gradient descentmethods. is, use updates formu0 = u (u)u current point search, (u) subgradient point, > 0 step size,u0 new point search. suitable conditions stepsizes (e.g., see theorem 4),updates successfully converge minimum L(u).8. precisely, function L(u) convex differentiable, gradient point u subgradientu.9. noted, however, given point u, may one subgradient: occur,example, piecewise linear function points gradient defined.326fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGS(flies)NP(United)VP(flies)NVUnitedfliesNP(jet)Nlarge jet*0United1 flies2some3 large4jet5Figure 4: lexicalized parse tree, dependency structure.calculate subgradient L(u)? turns convenientform. (see definition 1), define (u) z (u) argmaxs two maximizationproblems L(u). define vector (u)(u) (i, t) = (u) (i, t) z (u) (i, t)(i, t), shown (u) subgradient L(u) u. updatesalgorithm Figure 2 take formu0 (i, t) = u(i, t) (y (u) (i, t) z (u) (i, t))hence correspond directly subgradient updates.See Appendix A.2 proof subgradients take form, Appendix A.3 proofconvergence subgradient optimization method.6. Examplessection describe examples dual decomposition algorithms. first example,also work Rush et al. (2010), dual decomposition algorithm combines twoparsing models. second example, work Komodakis et al. (2007, 2011), dualdecomposition algorithm inference Markov random fields. Finally, describe algorithmHeld Karp (1971) traveling salesman problem, algorithm Chang Collins(2011) decoding phrase-based translation models.6.1 Combined Constituency Dependency ParsingRush et al. (2010) describe algorithm finding highest scoring lexicalized context-freeparse tree input sentence, combination two models: lexicalized probabilisticcontext-free grammar, discriminative dependency parsing model.Figure 4 shows example lexicalized context-free tree. take setlexicalized trees input sentence, f (y) score tree lexicalizedparsing modelspecifically, f (y) log-probability model Collins (1997).model, lexicalized rule receives score log probability, logprobability sum log probabilities rules contains.327fiRUSH & C OLLINS100% examples converged80604020050<=20<=10<=4<=3<=2<=1<=number iterationsFigure 5: Convergence results work Rush et al. (2010) integration lexicalizedprobabilistic context-free grammar, discriminative dependency parsing model.show percentage examples exact solution returned algorithm,versus number iterations algorithm.second model dependency parsing model. example dependency parse also shownFigure 4. set possible dependency parses sentence Z; parse z receivesscore g(z) dependency parsing model. use discriminative dependency parsingmodel Koo, Carreras, Collins (2008) (see also McDonald, 2006).lexicalized parse tree y, mapping underlying dependency structure l(y).decoding problem consider findargmax f (y) + g(l(y))(17)yYmotivation problem allow us inject information dependencyparsing model g(z) lexicalized parsing model Collins (1997); Rush et al. (2010) showgives significant improvements parsing accuracy.problem solved exactly using dynamic programming approach,dynamic program created intersection two models (there clear analogyBar-Hillel et al. (1964) method construction dynamic program intersectionPCFG HMM). However dynamic program relatively inefficient.develop dual decomposition algorithm similar way before. dependency(i, j) {0 . . . n} head word (we use 0 denote root symbol) j {1 . . . n},j 6= i, modifier, define y(i, j) = 1 contains dependency (i, j), y(i, j) = 0otherwise. define similar variables z(i, j) dependency structures. reformulateproblem Eq. 17 as:Optimization Problem 2 Findargmax f (y) + g(z)yY,zZ(i, j), y(i, j) = z(i, j).328(18)fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGLagrangian introduced problem, similar Eq. 12,subgradient algorithm used minimize resulting dual. introduce Lagrange multipliersu(i, j) dependencies (i, j), whose initial values u(0) (i, j) = 0 i, j. iterationalgorithm find(k) = argmax f (y) +yYXu(k1) (i, j)y(i, j)i,jusing dynamic programming algorithm lexicalized context-free parsing (a trivial modificationoriginal algorithm finding argmaxy f (y)). addition findz (k) = argmax g(z)zZXu(k1) (i, j)z(i, j)i,jusing dynamic programming algorithm dependency parsing (again, requires trivial modification existing algorithm). (k) (i, j) = z (k) (i, j) (i, j) algorithme-converged, guaranteed solution optimization problem 2. Otherwise,perform subgradient updatesu(k) (i, j) = u(k1) (i, j) k (y (k) (i, j) z (k) (i, j))(i, j), go next iteration.Rush et al. (2010) describe experiments algorithm. method e-converges99% examples, 90% examples e-converging 10 iterations less. Figure 5 showshistogram number examples e-converged, versus number iterationsalgorithm. method gives significant gains parsing accuracy model Collins (1997),significant gains baseline method simply forces lexicalized CFG parserdependency structure first-best output dependency parser.106.2 MAP Problem Pairwise Markov Random FieldsMarkov random fields (MRFs), generally graphical models, widely used machinelearning statistics. MAP problem MRFs problem finding likely settingrandom variables MRFis inference problem central importance. sectiondescribe dual decomposition algorithm work Komodakis et al. (2007, 2011)finding MAP solution pairwise, binary, MRFs. Pairwise MRFs limited casepotential functions consider pairs random variables, opposed larger subsets; however,generalization method non-pairwise MRFs straightforward.commonly used approach MAP problem MRFs use loopy max-product belief propagation. dual decomposition algorithm advantages terms stronger formalguarantees, described section 5.10. Note Klein Manning (2002) describe method combination dependency parser constituentbased parser, score entire structure sum scores two models. approachA* algorithm developed, admissible estimates within A* method computed efficiently usingseparate inference two models. interesting connections A* approach dualdecomposition algorithm described section.329fiRUSH & C OLLINSMAP problem follows. Assume vector variables y1 , y2 , . . . , yn ,yi take two possible values, 0 1 (the generalization two possible valuesvariable straightforward). 2n possible settings n variables. MRFassumes underlying undirected graph (V, E), V = {1 . . . n} set verticesgraph, E set edges. MAP problem findargmax h(y)(19)y{0,1}nXh(y) =i,j (yi , yj ){i,j}Ei,j (yi , yj ) local potential associated edge {i, j} E, returns realvalue (positive negative) four possible settings (yi , yj ).underlying graph E tree, problem Eq. 19 easily solved using max-productbelief propagation, form dynamic programming. contrast, general graphs E, maycontain loops, problem NP-hard. key insight behind dual decomposition algorithmdecompose graph E trees T1 , T2 , . . . , Tm . Inference treeperformed efficiently; use Lagrange multipliers enforce agreement inferenceresults tree. subgradient algorithm used, iteration first performinference trees T1 , T2 , . . . , Tm , update Lagrange multipliers casesdisagreements.simplicity, describe case = 2. Assume two treesT1 E, T2 E, T1 T2 = E.11 Thus trees contains subset edges(1)E, together trees contain edges E. Assume define potential functions i,j(2)(i, j) T1 i,j (i, j) T2Xi,j (yi , yj ) ={i,j}E(1)X{i,j}T1(2)Xi,j (yi , yj ) +i,j (yi , yj ){i,j}T2easy do: example, definei,j(yi , yj ) =i,j (yi , yj )#(i, j)= 1, 2 #(i, j) 2 edge {i, j} appears trees, 1 otherwise.define new problem equivalent problem Eq. 19:Optimization Problem 3 FindargmaxXy{0,1}n ,z{0,1}n{i,j}T1(1)i,j (yi , yj ) +X(2)i,j (zi , zj ){i,j}T2yi = zi = 1 . . . n.11. may always possible decompose graph E 2 trees way. Komodakis et al. (2007, 2011)describe algorithm general case 2 trees.330fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGNote similarity previous optimization problems. goal find pair structures,{0, 1}n z {0, 1}n . objective function writtenf (y) + g(z)(1)Xf (y) =i,j (yi , yj ){i,j}T1(2)Xg(z) =i,j (zi , zj ){i,j}T2set constraints, yi = zi = 1 . . . n, enforce agreement z.proceed beforewe define Lagrangian Lagrange multiplier uiconstraint:(1)XL(u, y, z) ={i,j}T1(2)Xi,j (yi , yj ) +i,j (zi , zj ) +nXi=1{i,j}T2ui (yi zi )minimize dualL(u) = max L(u, y, z)y,z(0)using subgradient algorithm. algorithm initialized uiiteration algorithm findX(k) = argmaxy{0,1}n= 0 = 1 . . . n.X(k1)(1)uyi(yi , yj ) +i,j{i,j}T1z (k) = argmaxz{0,1}nX(2)i,j (zi , zj )X (k1)uzi{i,j}T2steps achieved efficiently, T1 T2 trees, hence max-product belief propP (k1)P (k1)agation produces exact answer. (The Lagrangian terms uiyi uizi easily(k)(k)incorporated.) yi = zi algorithm e-converged, guaranteedsolution optimization problem 3. Otherwise, perform subgradient updates form(k)ui(k1)= ui(k)k (yi(k)zi )= {1 . . . n}, go next iteration. Intuitively, updates bias two inferenceproblems towards agreement other.Komodakis et al. (2007, 2011) show good experimental results method. algorithmparallels max-product belief propagation, ui values interpretedmessages passed sub-problems.331fiRUSH & C OLLINS15135743674262Figure 6: illustration approach Held Karp (1971). left tourvertices 1 . . . 7. right 1-tree vertices 1 . . . 7. 1-tree consists treevertices 2 . . . 7, together 2 additional edges include vertex 1. tourleft also 1-tree (in fact every tour also 1-tree).6.3 Held Karp Algorithm TSPsnext example approach Held Karp (1971) traveling salesman problems (TSPs),notable original paper Lagrangian relaxation. algorithminstance dual decomposition. Instead leveraging two combinatorial algorithms,combination agreement constraints, makes use single combinatorial algorithm, togetherset linear constraints incorporated using Lagrange multipliers. usetwo combinatorial algorithms, seen dual decomposition, useful technique,broadening scope algorithms make use single combinatorial algorithmuseful. NLP decoding algorithms leverage single combinatorial algorithm, see algorithm Chang Collins (2011) decoding phrase-based translation models (we describealgorithm next section), algorithm Rush Collins (2011) decodingsyntax-based translation models.TSP defined follows. undirected graph (V, E) vertices V = {1, 2, . . . , n},edges E. edge e E score e R. subset edges E representedvector = {ye : e E}, ye = 1 edge subset, ye = 0 otherwise. Thusvector {0, 1}|E| . tour graph subset edges corresponds pathgraph begins ends vertex, includes every vertex exactlyonce. See Figure 6 example tour. use {0, 1}|E| denote set possibletours. traveling salesman problem findargmaxXyYeEye eproblem well-known NP-hard.12key idea work Held Karp (1971) 1-tree, which, like tour, subsetE. Held Karp define 1-tree follows:1-tree consists tree vertex set {2, 3, . . . , n}, together two distinctedges vertex 1... Thus, 1-tree single cycle, cycle contains vertex 1,vertex 1 always degree two.12. many presentations traveling salesman problem goal find minimum cost tour: consistencyrest tutorial presentation considers maximization problem, equivalent.332fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGFigure 6 shows example 1-tree. define 0 set possible 1-trees. followssubset 0 , every tour also 1-tree.Crucially, possible findXargmaxye eyY 0eEusing efficient algorithm. first step, find maximum scoring spanning treevertices {2, 3, . . . , n}, using maximum spanning tree algorithm. second step, addtwo highest scoring edges include vertex 1. simple show resulting 1-treeoptimal. Thus search set NP-hard, search larger set 0 performedeasily. Lagrangian relaxation algorithm explicitly leverage observation.Next, note= {y : 0 , {1, 2, . . . , n},e:ie yeP= 2}constraint formXye = 2(20)e:iecorresponds property ith vertex exactly two incident edges. Thusadd constraint vertex exactly two incident edges, go set 1-treesset tours. Constraints form Eq. 20 linear ye variables, thereforeeasily incorporated Lagrangian.Held Karp introduce following optimization problem:Optimization Problem 4 FindXargmaxyY 0{1, 2, . . . , n},e:ie yePye eeE= 2.clear equivalent finding highest scoring tour graph.before, deal equality constraints using Lagrange multipliers. Define Lagrange multipliers vector u = {ui : {1 . . . n}}. LagrangianL(u, y) =Xye e +nX!uie:iei=1eEXye 2dual objectiveL(u) = max0 L(u, y)yY(0)subgradient algorithm takes following form. Initially set uiiteration find(k) = argmaxyY 0Xye e +eEnX(k1)Xi=1e:ieuiconstraints satisfied, i.e.,Xye(k) = 2e:ie333= 0 = 1 . . . n.!!ye 2(21)fiRUSH & C OLLINSalgorithm terminates, guarantee structure (k) solution optimizationproblem 4. Otherwise, subgradient step used modify Lagrange multipliers.shown subgradient L(u) u vector g (u) definedg (u) (i) =Xe:ieye(u) 2(u) = argmaxyY 0 L(u, y). Thus subgradient step {1 . . . n},!(k)ui=(k1)uikXe:ieye(k) 2(22)Note problem Eq. 21 easily solved. equivalent findingX(k) = argmaxyY 0ye e0eEmodified edge weights e0 : edge e = {i, j}, define(k1)e0 = e + ui(k1)+ ujHence new edge weights incorporate Lagrange multipliers two vertices edge.subgradient step Eq. 22 clear intuition. vertices greater 2 incidentedges (k) , value Lagrange multiplier ui decreased, effectpenalising edges including vertex i. Conversely, vertices fewer 2 incident edges,ui increase, edges including vertex preferred. algorithm manipulatesui values effort enforce constraints vertex exactly two incident edges.note qualitative difference example previous algorithms.previous algorithms employed two sets structures Z, two optimization problems,equality constraints enforcing agreement two structures. TSP relaxation instead involves single set Y. two approaches closely related, however, similar theoremsapply TSP method (the proofs trivial modifications previous proofs).L(u)Xye eeEu, optimal tour. appropriate step sizes subgradient algorithm,lim L(u(k) ) = min L(u)uk(k)Finally, ever find structuresatisfies linear constraints, algorithme-converged, guaranteed solution traveling salesman problem.6.4 Phrase-Based Translationnext consider Lagrangian relaxation algorithm, described work Chang Collins(2011), decoding phrase-based translation models (Koehn, Och, & Marcu, 2003). inputphrase-based translation model source-language sentence n words, x = x1 . . . xn .output sentence target language. examples section use Germansource language, English target language. use German sentence334fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGwir mussen auch diese kritik ernst nehmenrunning example.key component phrase-based translation model phrase-based lexicon, pairssequences words source language sequences words target language.example, lexical entries relevent German sentence shown include(wir mussen, must)(wir mussen auch, must also)(ernst, seriously)on. phrase entry associated score, take value reals.introduce following notation. phrase tuple (s, t, e), signifying subsequence xs . . . xt source language sentence translated target-language string e,using entry phrase-based lexicon. example, phrase (1, 2, must) would specify sub-string x1 . . . x2 translated must. phrase p = (s, t, e) receivesscore (p) R model. given phrase p, use s(p), t(p) e(p) referthree components. use P refer set possible phrases input sentence x.derivation finite sequence phrases, p1 , p2 , . . . pL . length Lpositive integer value. derivation use e(y) refer underlying translation definedy, derived concatenating strings e(p1 ), e(p2 ), . . . e(pL ). example,= (1, 3, must also), (7, 7, take), (4, 5, criticism), (6, 6, seriously)(23)e(y) = must also take criticism seriouslyscore derivation definedh(y) = g(e(y)) +X(p)pyg(e(y)) score (log-probability) e(y) n-gram language model.set valid derivations defined follows. derivation y, define y(i)= 1 . . . n number times source word translated derivation.formally,Xy(i) =[[s(p) t(p)]]py[[]] 1 statement true, 0 otherwise. set valid derivations= {y P : = 1 . . . n, y(i) = 1}P set finite length sequences phrases. Thus derivation valid,source-language word must translated exactly once. definition, derivation Eq. 23valid. decoding problem find= argmax h(y)yY335(24)fiRUSH & C OLLINSproblem known NP-hard. useful intuition follows. dynamic programmingapproach problem would need keep track bit-string length n specifyingn source language words havent translated point dynamic program.2n bit-strings, resulting dynamic program exponential numberstates.describe Lagrangian relaxation algorithm. before, key idea defineset 0 subset 0 ,argmax h(y)(25)yY 0found efficiently. defining0 = {y P :Pni=1 y(i)= n}Thus derivations 0 satisfy weaker constraint total number source words translatedexactly n: dropped y(i) = 1 constraints. one example, following derivationmember 0 , member Y:= (1, 3, must also), (1, 2, must), (3, 3, also), (6, 6, seriously)(26)case y(1) = y(2) = y(3) = 2, y(4) = y(5) = y(7) = 0, y(6) = 1. Hencewords translated once, words translated 0 times.definition 0 , problem Eq. 25 solved efficiently, using dynamicprogramming. contrast dynamic program Eq. 24, keeps track bit-stringlength n, new dynamic program merely needs keep track many source language wordstranslated point search.proceed follows. Note= {y : 0 , = 1 . . . n, y(i) = 1}introduce Lagrange multiplier u(i) constraint y(i) = 1. LagrangianL(u, y) = h(y) +nXi=1u(i) (y(i) 1)subgradient algorithm follows. Initially set u(0) (i) = 0 i. iterationfind(k) = argmax L(u(k1) , y)(27)yY 0perform subgradient stepu(k) (i) = u(k1) (i) k (y (k) (i) 1)(28)point (k) (i) = 1 = 1 . . . n, guaranteed optimalsolution original decoding problem.336fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGproblem Eq. 27 solved efficiently,argmax L(u(k1) , y)yY 0= argmax g(e(y)) +yY 0X(p) +pynXi=1u(k1) (i) (y(i) 1)= argmax g(e(y)) +yY 0X0 (p)py0 (p) = (p) +t(p)Xu(k1) (i)i=s(p)0 (p),Thus new phrase scores,take account Lagrange multiplier valuespositions s(p) . . . t(p). subgradient step Eq. 28 clear intuition. sourcelanguage word translated once, associated Lagrange multiplier u(i)decrease, causing phrases including word penalised next iteration. Conversely,word translated 0 times Lagrange multiplier increase, causing phrases includingword preferred next iteration. subgradient method manipulates u(i) valuesattempt force source-language word translated exactly once.description given sketch: Chang Collins (2011) describe detailsmethod, including slightly involved dynamic program gives tighter relaxationmethod described here, tightening method incrementally adds constraintsmethod initially e-converge. method successful recovering exact solutionsphrase-based translation model, far efficient alternative approaches basedgeneral-purpose integer linear programming solvers.7. Practical Issuessection reviews various practical issues arise dual decomposition algorithms.describe diagnostics used track progress algorithm minimizing dual,providing primal solution; describe methods choosing step sizes, k , algorithm;describe heuristics used cases algorithm provide exactsolution. continue use algorithm section 4 running example, althoughobservations easily generalized Lagrangian relaxation algorithms.first thing note iteration algorithm produces number useful terms,particular:solutions (k) z (k) .current dual value L(u(k) ) (which equal L(u(k) , (k) , z (k) ).addition, cases function l : Z maps structurestructure l(y) Z, alsoprimal solution (k) , l(y (k) ).337fiRUSH & C OLLINS-13-14Value-15-16-17-18Current PrimalCurrent Dual-190102030Round405060Figure 7: Graph showing dual value L(u(k) ) primal value f (y (k) ) + g(l(y (k) )), versusiteration number k, subgradient algorithm translation example workRush Collins (2011).primal value f (y (k) ) + g(l(y (k) )).primal solution mean pair (y, z) satisfies constraints optimization problem.example, optimization problem 1 (the combined HMM PCFG problem section 4)primal solution properties Y, z Z, y(i, t) = z(i, t) (i, t).one example, algorithm Figure 2, iteration produce parse tree (k) .simple recover POS sequence l(y (k) ) parse tree, calculate scoref (y (k) ) + g(l(y (k) )) combined model. Thus even (k) z (k) disagree, still use(k) , l(y (k) ) potential primal solution. ability recover primal solution value(k) always holdbut cases hold, useful. allow us,example, recover approximate solution cases algorithm hasnt e-convergedexact solution.describe various items described used practical applicationsalgorithm.7.1 Example Run AlgorithmFigure 7 shows run subgradient algorithm decoding approach machine translationdescribed work Rush Collins (2011). behavior typical casesalgorithm e-converges exact solution. show dual value L(u(k) ) iteration,value f (y (k) ) + g(l(y (k) )). important points follows:L(u) provides upper bound f (y ) + g(z ) value u,L(u(k) ) f (y (k) ) + g(l(y (k) ))every iteration.example e-convergence exact solution, pointL(u(k) ) = f (y (k) ) + g(z (k) ))338fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING-134Gap3.5-143ValueValue-15-162.521.5-171-18Best PrimalBest Dual-190102030Round40500.50600102030Round405060Figure 8: graph left shows best dual value Lk best primal value pk , versusiteration number k, subgradient algorithm translation example workRush Collins (2011). graph right shows Lk pk plotted k.(y (k) , z (k) ) guaranteed optimal (and addition, z (k) = l(y (k) )).dual values L(u(k) ) monotonically decreasingthat is, iterationsL(u(k+1) ) > L(u(k) )even though goal minimize L(u). typical: subgradient algorithmsgeneral guaranteed give monotonically decreasing dual values. However, seeiterations dual decreasesthis typical.Similarly, primal value f (y (k) ) + g(z (k) ) fluctuates (goes down) coursealgorithm.following quantities useful tracking progress algorithm kth iteration:L(u(k) ) L(u(k1) ) change dual value one iteration next.soon see useful choosing step size algorithm (if valuepositive, may indication step size decrease).0Lk = mink0 k L(u(k ) ) best dual value found far. gives us tightest upper boundf (y ) + g(z ) k iterations algorithm.00pk = maxk0 k f (y (k ) ) + g(l(y (k ) )) best primal value found far.Lk pk gap best dual best primal solution found far algorithm.Lk f (y ) + g(z ) pk ,Lk pk f (y ) + g(z ) pkhence value Lk pk gives us upper bound difference f (y ) + g(z )pk . Lk pk small, guarantee primal solution closeoptimal.Figure 8 shows plot Lk pk versus number iterations k previous example, addition shows plot gap Lk pk . graphs are, surprisingly, muchsmoother graph Figure 7. particular guaranteed values Lk pkmonotonically decreasing increasing respectively.339fiRUSH & C OLLINS-130.010.0050.0005-13.5Value-14-14.5-15-15.5-1605101520Round25303540Figure 9: Graph showing dual value L(u(k) ) versus number iterations k, differentfixed step sizes.7.2 Choice Step Sizes kFigure 9 shows convergence algorithm various choices step size, chosenkeep stepsize constant iteration. immediately see potential dilemma arising.small step size ( = 0.0005), convergence smooththe dual value monotonicallydecreasingbut convergence slow. large step size ( = 0.01), convergence muchfaster initial phases algorithm, dual fluctuates quite erratically. practiceoften difficult choose constant step size gives good convergence propertiesearly late iterations algorithm.Instead, found often find improved convergence properties choicestep size kdecreases increasing k. One possibility use definition k = c/kk = c/ k c > 0 constant. However definitions decrease step sizerapidlyin particular, decrease step size iterations, even cases progressmade decreasing dual value. many cases found effectivedefinitionck =t+1c > 0 constant, < k number iterations prior k dual value00increases rather decreases (i.e., number cases k 0 k L(u(k ) ) > L(u(k 1) )).definition step size decreases dual value moves wrong direction.7.3 Recovering Approximate SolutionsFigure 10 shows run algorithm fail get e-convergence exact solution.section 9.4 describe one possible strategy, namely tightening relaxation,used produce exact solution cases. Another obvious strategy, approximate,simply choose best primal solution generated k iterations algorithm,00fixed k: i.e., choose (k ) , l(y (k ) )00k 0 = argmax f (y (k ) ) + g(l(y (k ) ))k0 k340fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING0Best Primal-5Value-10-15-20-25Current PrimalCurrent Dual-30010203040Round5060701001009090PercentagePercentageFigure 10: Graph showing dual value L(u(k) ) primal value f (y (k) ) + g(l(y (k) )), versusiteration number k, subgradient algorithm translation examplework Rush Collins (2011), method e-converge exactsolution.8070600200400600800Maximum Number Dual Decomposition Iterations7060% validation UAS% certificates% match K=50005080f score% certificates% match K=50501000010203040Maximum Number Dual Decomposition Iterations50Figure 11: Figures showing effects early stopping non-projective parsing algorithmKoo et al. (2010) (left graph) combined constituency dependency parsing (rightgraph). case plot three quantities versus number iterations, k: 1)accuracy (UAS f-score); 2) percentage cases algorithm e-convergesgiving exact solution, certificate optimality; 3) percentage casesbest primal solution kth iteration running algorithme-convergence.described before, use Lk pk upper bound difference approximate solution optimal solution.7.4 Early Stoppinginteresting also consider strategy returning best primal solution early algorithm cases algorithm eventually e-converge exact solution. practice,strategy sometimes produce high quality solution, albeit without certificate optimality, faster running algorithm e-convergence. Figure 11 shows graphs two problems:non-projective dependency parsing (Koo et al., 2010), combined constituency dependency341fiRUSH & C OLLINSparsing (Rush et al., 2010). case show three quantities vary numberiterations algorithm. first quantity percentage cases algorithm econverges, giving exact solution, certificate optimality. combined constituencydependency parsing takes roughly 50 iterations (over 95%) cases e-converge;second algorithm takes closer 1000 iterations.addition, show graphs indicating quality best primal solution generatediteration k algorithm, versus number iterations, k. early stopping strategy wouldpick fixed value k, simply return best primal solution generated first kiterations algorithm. first plot accuracy (f-score, dependency accuracy respectively)two models early stopping: see accuracy quickly asymptotesoptimal value, suggesting returning primal solution e-convergence often yield highquality solutions. also plot percentage cases primal solution returned factidentical primal solution returned algorithm run e-convergence. seecurve asymptotes quickly, showing many cases early stopping strategyfact produce optimal solution, albeit without certificate optimality.8. Alternatives Subgradient Optimizationtutorial focused subgradient methods optimization dual objective. Severalalternative optimization algorithms proposed machine learning literature;section give overview approaches.Wainwright, Jaakkola, Willsky (2005) describe early important algorithm Markovrandom fields (MRFs) based LP relaxations, tree-reweighted message passing (TRW). Following work Kolmogorov (2006), use TRW-E refer edge-based variant TRW,TRW-T refer tree-based algorithm. Kolmogorov (2006) derives variant, TRW-S(the refers sequential nature algorithm). three algorithmsTRW-E, TRW-T,TRW-Sare motivated LP relaxation MRFs, none guaranteeconverging optimal value LP. TRW-S strongest guarantee three algorithms, namely monotonically improves dual value, may converge optimaldual value.Yanover et al. (2006) describe experiments comparing TRW-based algorithms generic LPsolvers MRF problems (specifically, LP solver use CPLEX13 ). TRW-based algorithms considerably efficient CPLEX, due fact TRW-based methodsleverage underlying structure MRF problem. various Lagrangian relaxation algorithms described current paper viewed specialized algorithms solving LPrelaxations, explicitly leverage combinatorial structure within underlying problem.Komodakis et al. (2007, 2011) give experiments comparing subgradient method TRWS TRW-T algorithms. experiments TRW-S generally performs better TRW-T.several cases TRW-S finds optimal dual solution faster subgradient method;cases TRW-S appears get stuck (as expected given lack convergence guarantee),subgradient method finds global optimum. Overall, subgradient method competitiveTRW-S: may initially make slower progress dual objective, benefit guaranteedconvergence global optimum LP relaxation.13. http://www.ilog.com/products/cplex/342fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGAnother important class algorithms optimizing dual LP block coordinatedescent algorithms: example MPLP algorithm Globerson Jaakkola (2007). Seework Sontag et al. (2010) discussion methods. Like TRW-S, MPLP algorithmguaranteed monotonically improve dual value, guaranteed convergeglobal optimum MRF LP. several experimental settings, MPLP algorithm producesbetter dual values early iterations subgradient methods, get stuck non-optimalsolution (Jojic, Gould, & Koller, 2010; Martins, Figueiredo, Aguiar, Smith, & Xing, 2011; Meshi& Globerson, 2011). Another complication MPLP requires computing max-marginalssub-problems iteration instead MAP assignments. Max-marginals may slowercompute practice, combinatorial problems computation may asymptoticallyslower. (For example, directed spanning tree models Koo et al., 2010, MAP problemsolved O(n2 ) time n length input sentence, awarealgorithm solves max-marginal problem better O(n4 ) time.)work, Jojic et al. (2010) describe accelerated method MRF inference, usingmethod Nesterov (2005) smooth objective underlying decomposition. methodrelatively fast rate convergence (O(1/) time reach solution -close optimal).Experiments work Jojic et al. (2010) show decrease number iterations requiredcompared subgradient; however work Martins et al. (2011) accelerated methodrequires iterations subgradient algorithm. sets experiments, MPLP makesinitial progress either method. Accelerated subgradient also requires computing subproblem marginals, similar disadvantages MPLPs requirement max-marginals.Recently, Martins et al. (2011) proposed augmented Lagrangian method inference usingalternating direction method multipliers (ADMM). See tutorial Boyd, Parikh, Chu,Peleato, Eckstein (2011) ADMM. augmented Lagrangian method extendsobjective quadratic penalty term representing amount constraint violation. ADMMmethod optimizing augmented problem able maintain similar decomposibility properties dual decomposition. Like subgradient method, ADMM guaranteed findoptimum LP relaxation. Martins et al. (2011) show empirically ADMM requirescomparable number iterations MPLP find good primal solution, still guaranteed optimize LP. challenge ADMM extra quadratic term may complicatesub-problem decoding, example clear directly decode parsing problems presented work quadratic term objective. Several alternative approachesproposed: Martins et al. (2011) binarize combinatorial sub-problems binary-valued factorgraphs; Meshi Globerson (2011) avoid problem instead applying ADMM dualLP; Martins (2012) Das et al. (2012) use iterative active set method utilizes MAPsolutions original sub-problems solve quadratic version. Martins (2012) also describesrecent results ADMM give O(1/) bound relaxed primal convergence.9. Relationship Linear Programming Relaxationssection describes close relationship dual decomposition algorithm linearprogramming relaxations. connection useful understanding behavioralgorithm, particular understanding cases algorithm e-convergeexact solution. addition, suggest strategies tightening algorithm exactsolution found.343fiRUSH & C OLLINS9.1 Linear Programming Relaxationcontinue use algorithm section 4 example; generalization problemsstraightforward. First, define set= { : R|Y| ,X= 1, 0 1}Thus simplex, corresponding set probability distributions finite set Y.Similarly, defineXz = { : R|Z| ,z = 1, z 0 z 1}zset distributions set Z.define new optimization problem, follows:Optimization Problem 5 Findmax,zXf (y) +Xz g(z)(29)zi, t,Xy(i, t) =Xz z(i, t)(30)zoptimization problem linear program: objective Eq. 29 linear variables; constraints Eq. 30, together constraints definitions z ,also linear variables.optimization problem similar original problem, optimization problem 1.see this, define 0y follows:0y = { : R|Y| ,X= 1, {0, 1}}Thus 0y subset , constraints 0 1 replaced {0, 1}.Define 0z similarly. Consider following optimization problem, replace zEq. 29 0y 0z respectively:Optimization Problem 6 Findmax0X,0zf (y) +Xz g(z)(31)zi, t,Xy(i, t) =Xz344z z(i, t)(32)fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGnew problem equivalent original problem, optimization problem 1: choosingvectors 0y 0z equivalent choosing single parse Y, single POSsequence z. sense, optimization problem 5 relaxation original problem,constraints form {0, 1} z {0, 1} replaced constraints form0 1 0 z 1.Note optimization problem 6 integer linear program, objectivelinear variables, constraints variables combine linear constraintsinteger constraints (that z must either 0 1). also worth notingactually convex hull finite set 0y . points 0y form vertices polytope.useful theorem, central relationship linear programming combinatorial optimization problems, following:Theorem 6 finite set Y, function f : R,max f (y) = maxyYXf (y)yYdefined above.proof simple, given Appendix A.4.9.2 Dual New Optimization Problemdescribe dual problem linear program Eqs. 29 30.function (u) vector dual variables u = {u(i, t) : {1 . . . n}, }. crucial resulttwo dual functions (u) L(u) identical.new Lagrangian!(u, , ) =Xf (y) +=XXz g(z) +zu(i, t)Xi,ty(i, t)Xz z(i, t)zXXXy(i, t)f (y) +u(i, t)i,tXXX+ z g(z)u(i, t)z z(i, t)zzi,tnew dual objective(u) =max,z(u, , )Note simply maximized primal ( ) variables, ignoringconstraints Eq. 30. dual problem findmin (u)uTwo theorems regarding dual problem follows:345fiRUSH & C OLLINSTheorem 7 Define ( , ) solution optimization problem Eqs. 29 30.min (u) =Xuf (y) +Xz g(z)zProof. follows immediately results linear programming duality see textbookKorte Vygen (2008) details.Note equality above, contrast previous result,min L(u) f (y ) + g(z )udual function gave upper bound best primal solution.second theorem follows:Theorem 8 value u,(u) = L(u)Thus two dual functions identical. Given subgradient algorithm described minimizes L(u), therefore also minimizes dual linear program Eqs. 29 30.Proof.(u) =XXXy(i, t) +u(i, t)max f (y) +i,tXXXz z(i, t)u(i, t)max z g(z)zzzi,t= max f (y) +yYXu(i, t)y(i, t) +i,tmax g(z)zZXu(i, t)z(i, t)i,t= L(u)used theorem 6 giveXXXXu(i, t)y(i, t)max f (y) +u(i, t)y(i, t) = max f (y) +i,tyYi,tused similar result replace max z max Z.346fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGy1y2y3xxxZbbccz1z2z3bbccFigure 12: simple example three possible parse trees three possible POS sequences.Zy1y2y3xxxbbccy1y2y3xxxbbccz1z2z3z1z2z3bbccZbbccFigure 13: Illustration two solutions satisfy constraints Eq. 30. left, solution= [0, 0, 1], = [0, 0, 1] puts weight 1 y3 z3 . right, fractionalsolution = [0.5, 0.5, 0] = [0.5, 0.5, 0] puts 0.5 weight y1 /y2 z1 /z2 .347fiRUSH & C OLLINS9.3 Examplegive example illustrates ideas. example also illustratehappens algorithm fails e-converge.assume three possible parse trees, = {y1 , y2 , y3 }, three possible tagsequences, Z = {z1 , z2 , z3 }, shown Figure 12. write distributions setsvectors = [0, 0, 1] = [0.5, 0.5, 0].consider pairs vectors (, ) satisfy constraints Eq. 30. Figure 13 illustratestwo possible solutions. One pair, denote (1 , 1 ), 1 = [0, 0, 1], 1 =[0, 0, 1]. easily verified definitionXy1 y(1, c) =yYXz1 z(1, c) =zYXy1 y(2, c) =yYXz1 z(2, c) = 1zYexpected values equal 0: hence (1 , 1 ) satisfies constraints. potential solution integral, puts weight 1 single parse tree/POS-tag sequence,structures weight 0.second pair satisfies constraints 2 = [0.5, 0.5, 0], 2 = [0.5, 0.5, 0].definitions,Xy2 y(1, a) =Xz2 z(1, a) =y2 y(1, b) =Xz2 z(1, b) = 0.5zYyYzYyYXXyYy2 y(2, a) =Xz2 z(2, a) =XyYzYy2 y(2, b) =Xz2 z(2, b) = 0.5zYexpected values equal 0. pair (2 , 2 ) fractional solution,puts fractional (0.5) weight structures.Next, consider different definitions functions f (y) g(z). Consider first definitionsf = [0, 0, 1] g = [0, 0, 1] (we write f = [0, 0, 1] shorthand f (y1 ) = 0, f (y2 ) = 0,f (y3 ) = 1). solution problem Eqs. 29 30 pair (1 , 1 ).Alternatively, consider definitions f = [1, 1, 2] g = [1, 1, 2]. case followingsituation arises:pair (1 , 1 ) achieves score 0 objective Eq. 29, whereas pair (2 , 2 )achieves score 2. Thus solution problem Eqs. 29 30 (2 , 2 ),fractional solution.theorem 7, minu (u) equal value optimal primal solution, i.e., minu (u) =2. Hence minu L(u) = 2.contrast, solution original optimization problem 1 (y , z ) = (y3 , z3 ): fact,(y3 , z3 ) pair structures satisfies constraints y(i, t) = z(i, t) (i, t).Thus f (y ) + g(z ) = 0.min L(u) = 2 > f (y ) + g(z ) = 0uThus clear gap minimum dual value, score optimalprimal solution.348fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING5Dual43210012345678910k123456789(k)y3y2y1y1y2y1y2y1y2z (k)z2z1z1z1z2z1z2z1z2RoundFigure 14: Figures showing progress subgradient algorithm f = [1, 1, 2] g =[1, 1, 2]. graph shows dual value L(u(k) ) versus number iterations k.table shows hypotheses (k) z (k) versus number iterations k. later iterationsmethod alternates hypotheses (y1 , z1 ) (y2 , z2 ).Figure 14 shows trace subgradient method problem. nine iterationsmethod reached L(u(9) ) = 2.06, close optimal dual value. case, however,algorithm reach agreement structures (k) z (k) . Instead, reachespoint alternates solutions (y (1) , z (1) ), (y (2) , z (2) ). Thus dual d-convergesminimum value, primal solutions generated alternate structures y1 , y2 , z1 , z2greater 0 weight fractional solution (2 , 2 ). behavior typical casesduality gap, i.e., minu L(u) strictly greater f (y ) + g(z ).9.4 Fixing E-Convergence: Tightening Approachesdescribe tightening approach used fix issue non-convergence givenprevious example.Consider problem integrated CFG parsing HMM tagging. Assume inputsentence length n. first approach follows. introduce new variables y(i, t1 , t2 )= 1 . . . (n 1), t1 , t2 , y(i, t1 , t2 ) = 1 y(i, t1 ) = 1 y(i + 1, t2 ) = 1, 0otherwise. Thus new variables track tag bigrams. Similarly, introduce variables z(i, t1 , t2 )tag sequences z Z. define set constraintsy(i, t) = z(i, t){1 . . . n}, (the constraints before), additiony(i, t1 , t2 ) = z(i, t1 , t2 ){1 . . . n 1}, t1 , t2 .proceed before, using Lagrange multipliers u(i, t) enforce first set constraints, Lagrange multipliers v(i, t1 , t2 ) enforce second set constraints. dual349fiRUSH & C OLLINSdecomposition algorithm require us find(k) = argmax f (y) +XyYi,tz (k) = argmax g(z)Xu(k1) (i, t)y(i, t) +Xv (k1) (i, t1 , t2 )y(i, t1 , t2 )(33)v (k1) (i, t1 , t2 )z(i, t1 , t2 )(34)i,t1 ,t2zZi,tu(k1) (i, t)z(i, t)Xi,t1 ,t2iteration, followed updates formu(k) (i, t) u(k1) (i, t) (y (k) (i, t) z (k) (i, t))v (k) (i, t1 , t2 ) v (k1) (i, t1 , t2 ) (y (k) (i, t1 , t2 ) z (k) (i, t1 , t2 ))shown g(z) defined bigram HMM model, methodguaranteed e-converge exact solution. fact, underlying LP relaxation tight,integral solutions possible.problem approach finding argmax Eq. 33 expensive, duev(i, t1 , t2 )y(i, t1 , t2 ) terms: fact, requires exact dynamic programming algorithmintersection bigram HMM PCFG. Thus end algorithm leastexpensive integration bigram HMM PCFG using construction Bar-Hillel et al.(1964).14second approach, may efficient, follows. Rather introducingconstraints form Eq. 33, might introduce selected constraints. example,previous non-convergent example might add single constrainty(1, a, b) = z(1, a, b)single Lagrange multiplier v(1, a, b) new constraint, dual decompositionalgorithm requires following steps iteration:(k) = argmax f (y) +XyYi,tz (k) = argmax g(z)Xu(k1) (i, t)y(i, t) + v (k1) (1, a, b)y(1, a, b)(35)u(k1) (i, t)z(i, t) v (k1) (1, a, b)z(1, a, b)(36)zZi,tupdatesu(k) (i, t) u(k1) (i, t) (y (k) (i, t) z (k) (i, t))v (k) (1, a, b) v (k1) (1, a, b) (y (k) (1, a, b) z (k) (1, a, b))Figure 15 shows run subgradient algorithm single constraint added.fractional solution (2 , 2 ) eliminated, method e-converges correct solution.Two natural questions arise:14. g(z) defined bigram HMM, clearly nothing gained efficiency Bar-Hillel et al.(1964) method. g(z) complex, example consisting trigram model, dual decomposition methodmay still preferable.350fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING5Dual4321078910111213141516k78910111213141516(k)y3y2y1y3y2y1y3y2y1y3z (k)z2z3z2z1z3z2z1z3z2z3RoundFigure 15: Figures showing progress subgradient algorithm f (y) = [1, 1, 2] g(z) =[1, 1, 2], additional constraint y(1, a, b) = z(1, a, b) incorporated Lagrangian. graph shows dual value L(u(k) ) versus number iterations k.table shows hypotheses (k) z (k) versus number iterations k.constraints added? One strategy first run subgradient methodbasic constraints, shown Figure 14. heuristic used determinedual longer decreasing significant rate. point, determinedalgorithm oscillating solutions (y1 , z1 ) (y2 , z2 ), additionalconstraint y(1, a, b) = z(1, a, b) would rule solutions; hence constraint added.efficient adding constraints Eq. 33? toy examplesimple illustrate benefit adding selected constraints. understand benefit,consider case sentence length n reasonably large. case, mightadd bigram constraints positions sentence: practice CKY decodingalgorithm need introduce Bar-Hillel et al. (1964) machinery selectedpoints, much efficient introducing constraints.examples methods tighten dual decomposition/Lagrangian relaxation techniques using additional constraints, see work Sontag, Meltzer, Globerson, Jaakkola, Weiss (2008),Rush Collins (2011), Chang Collins (2011), Das et al. (2012). related previous work non-projective dependency parsing (Riedel & Clarke, 2006) incrementally addsconstraints integer linear program solver.9.5 Compact Linear ProgramsLP relaxations described large set variables: is, one variablemember Z. cases interest, sets Z exponential size.section describe derive equivalent linear programs far fewer variables.problem practical interest: many problems, found beneficial implementunderlying LP relaxation within generic LP solver, way debugging dual decomposition351fiRUSH & C OLLINSalgorithms. practical compact LPs describe section, clearlyimpractical exponential-size linear programs described previous section.First, consider abstract description Lagrangian relaxation given section 3. LPrelaxationargmaxQQ = {y : Conv(Y 0 ) Ay = b}Rpd b Rp . Recall Conv(Y 0 ) convex hull set 0 . Next, assumeConv(Y 0 ) defined polynomial number linear constraints: is,Conv(Y 0 ) = {y Rd : Cy = e, 0}(37)C Rqd e Rq , number constraints, q, polynomial. caseexplicit characterization set QQ = {y Rd : Cy = e, 0 Ay = b}d, p, q polynomial size, resulting linear program polynomial size.sense compact.remaining question whether characterization form Eq. 37 exists, so,defined. Recall made assumption value ,argmax(38)yY 0found using combinatorial algorithm. many combinatorial algorithms, LPformulations polynomial size: formulations lead directly definitions Ce.15 example, Martin, Rardin, Campbell (1990) give construction dynamic programming algorithms, includes parsing algorithms weighted context-free grammars,Viterbi algorithm, dynamic programs used NLP. Martins et al. (2009) make useconstruction directed spanning trees (see also Magnanti & Wolsey, 1994), apply nonprojective dependency parsing. Korte Vygen (2008) describe many constructions.short, given combinatorial algorithm solves problem Eq. 38, often straightforwardfind recipe constructing pair (C, e) completely characterizes Conv(Y 0 ).straightforward extend idea LP dual decomposition. Considerrunning example, (optimization problem 1),argmax f (y) + g(z)yY,zZ= 1 . . . n, ,y(i, t) = z(i, t)Rush et al. (2010) give full description compact LP problem: give sketch here.15. one subtlety here: cases additional auxilliary variables may need introduced. See example spanning tree construction Magnanti Wolsey (1994). However number auxilliary variablesgenerally polynomial number, hence benign.352fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGDefine vector {0, 1}d specifies context-free rules contains.follows subset {0, 1}d .f (y) =Rd vector specifying weight rule. Similarly, define z vector0{0, 1}d specifies trigrams z contains (assuming g(z) trigram tagging model).0follows Z subset {0, 1}d . writeg(z) = z 000 Rd . compact LPargmax+ 0Conv(Y),Conv(Z)= 1 . . . n, ,0(i, t) = (i, t)Again, existence combinatorial algorithms problems argmaxyY argmaxzZ zimplies explicit representationsConv(Y) = { Rd : = b, 0}0Conv(Z) = { Rd : C = e, 0}A, b, C e polynomial size. Rush et al. (2010) describe construction detailcase weighted CFG combined finite-state tagger.9.6 Summarysummarize, key points section follows:introduced linear programming problem relaxation original problem.function L(u) shown dual linear programming relaxation.cases optimal solution underlying LP fractional, subgradient methodstill d-converge minu L(u). However primal solutions (y (k) , z (k) ) alternatedifferent solutions satisfy y(i, t) = z(i, t) constraints.practice, tightening methods used improve convergence. methods selectively introduce constraints effort improve convergence method, costincreased complexity finding (k) and/or z (k) . precise constraints addedchosen identifying constraints frequently violated subgradient method.Finally, described methods construct compact linear program equivalentoriginal LP relaxation. linear program often small enough solved genericLP solver; useful debugging dual decomposition Lagrangian relaxation algorithms.353fiRUSH & C OLLINS10. Conclusionsbroad class inference problems statistical NLP areas machine learningamenable Lagrangian relaxation (LR) methods. LR methods make use combinatorial algorithms combination linear constraints introduced using Lagrange multipliers: iterative methods used minimize resulting dual objective. LR algorithms simpleefficient, typically involving repeated applications underlying combinatorial algorithm,conjunction simple additive updates Lagrange multipliers. well-understoodformal properties: dual objective upper bound score optimal primal solution;close connections linear programming relaxations; crucially, potentialproducing exact solution original inference problem, certificate optimality. Experiments several NLP problems shown effectiveness LR algorithms inference:LR methods often considerably efficient existing exact methods, strongerformal guarantees approximate search methods often used practice.Acknowledgmentsthank anonymous reviewers helpful comments. Tommi Jaakkola David Sontag introduced us dual decomposition Lagrangian relaxation inference probabilistic models;work would happened without them. benefited many discussionsYin-Wen Chang, Terry Koo, Roi Reichart, Tommi David collaboratorswork dual decomposition/Lagrangian relaxation NLP. also thank Shay Cohen, YoavGoldberg, Mark Johnson, Andre Martins, Ryan McDonald, Slav Petrov feedback earlierdrafts paper. Columbia University gratefully acknowledges support Defense Advanced Research Projects Agency (DARPA) Machine Reading Program Air Force ResearchLaboratory (AFRL) prime contract no. FA8750-09-C-0181. Alexander Rush supportedNational Science Foundation Graduate Research Fellowship.Appendix A. Proofssection derive various results combined parsing tagging problem. Recallcase Lagrangian definedL(u, y, z) = f (y) + g(z) +Xi{1...n},tTu(i, t)(y(i, t) z(i, t))dual objective L(u) = maxyY,zZ L(u, y, z). n number wordssentence, finite set part-of-speech tags.first prove L(u) convex function; derive expression subgradientsL(u); give convergence theorem algorithm Figure 2, subgradientalgorithm minimization L(u).Finally, give proof theorem 6.A.1 Proof Convexity L(u)theorem follows:354fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGTheorem 9 L(u) convex. is, u(1) Rd , u(2) Rd , [0, 1],L(u(1) + (1 )u(2) ) L(u(1) ) + (1 )L(u(2) )Proof: Define(y , z ) = arg max L(u , y, z)yY,zZu = u(1) + (1 )u(2) . followsL(u ) = L(u , , z )addition, noteL(u(1) , , z ) max L(u(1) , y, z) = L(u(1) )yY,zZsimilarlyL(u(2) , , z ) L(u(2) )followsL(u(1) , , z ) + (1 )L(u(2) , , z ) L(u(1) ) + (1 )L(u(2) )Finally, easy showL(u(1) , , z ) + (1 )L(u(2) , , z ) = L(u , , z ) = L(u )henceL(u ) L(u(1) ) + (1 )L(u(2) )desired result.A.2 Subgradients L(u)value u Rd , define(y (u) , z (u) ) = argmax L(u, y, z)yY,zZequivalently,(u) = argmax f (y) +XyYu(i, t)y(i, t)i,tz (u) = argmax g(z)XzZu(i, t)z(i, t)i,tdefine (u) vector components(u) (i, t) = (u) (i, t) z (u) (i, t)355fiRUSH & C OLLINS{1 . . . n}, , (u) subgradient L(u) u.result special case following theorem:16Theorem 10 Define function L : Rd RL(u) = max (ai u + bi )i{1...m}ai Rd bi R {1 . . . m}. value u,j = argmax (ai u + bi )i{1...m}aj subgradient L(u) u.Proof: aj subgradient point u, need show v Rd ,L(v) L(u) + aj (v u)Equivalently, need show v Rd ,max (ai v + bi ) max (ai u + bi ) + aj (v u)i{1...m}i{1...m}(39)show this, first noteaj u + bj = max (ai u + bi )i{1...m}hencemax (ai u + bi ) + aj (v u) = bj + aj v max (ai v + bi )i{1...m}i{1...m}thus proving theorem.A.3 Convergence Proof Subgradient MethodConsider convex function L : Rd R, minimizer u (i.e., u = argminuRd L(u)).subgradient method iterative method initializes u value u(0) Rd ,setsu(k+1) = u(k) k gkk = 0, 1, 2, . . ., k > 0 stepsize kth iteration, gk subgradient u(k) :is, v Rd ,L(v) L(u(k) ) + gk (v u(k) )following theorem useful proving convergence method (thetheorem proof taken Boyd & Mutapcic, 2007):16. specific, definition L(u) written form maxi{1...m} (ai u + bi ) follows. Defineinteger |Y| |Z|. Define (y (i) , z (i) ) {1 . . . m} list possible pairs (y, z)z Z. Define bi = f (y (i) ) + g(z (i) ), ai vector components ai (l, t) = (i) (l, t) z (i) (l, t)l {1 . . . n}, . verifed L(u) = maxi{1...m} (ai u + bi ).356fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGTheorem 11 Assume k, ||gk ||2 G2 G constant. k 0,||u(0) u ||2 + G2min L(u ) L(u ) +Pi{0...k}2 ki=0(i)Pk2i=0Proof: First, given updates u(k+1) = u(k) k gk , k 0,||u(k+1) u ||2 = ||u(k) k gk u ||2= ||u(k) u ||2 2k gk (u(k) u ) + k2 ||gk ||2subgradient property,L(u ) L(u(k) ) + gk (u u(k) )hencegk (u(k) u ) L(u ) L(u(k) )Using inequality, together ||gk ||2 G2 , gives||u(k+1) u ||2 ||u(k) u ||2 + 2k L(u ) L(u(k) ) + k2 G2Taking sum sides = 0 . . . k giveskXi=0kX||u(i+1) u ||2i=0||u(i) u ||2 + 2kXi=0kXL(u ) L(u(i) ) +i2 G2i=0hence||u(k+1) u ||2 ||u(0) u ||2 + 2Finally, using||u(k+1)kXi=0u ||20(i)L(u ) L(u )kXi=0kXL(u ) L(u(i) ) +kXi=0!i=0i2 G2!(i)L(u ) min L(u )i{0...k}gives(0)0 ||u2u || + 2kX!i=0!(i)L(u ) min L(u ) +i{0...k}kXi2 G2i=0Rearranging terms gives result theorem.theorem number consequences. one example, constant step-size, k = hh > 0,!P||u(0) u ||2 + G2 ki=1 i2Ghlim=Pkk22 i=1hence limit valuemin L(u(i) )i{1...k}within Gh/2 optimal solution. slightly involved argument showsPassumptions k > 0, limk k = 0,k=0 k = ,limk||u(0) u ||2 + G2P2 ki=1See Boyd Mutapcic full derivation.357Pk2i=1!=0fiRUSH & C OLLINSA.4 Proof Theorem 6Recall goal proveXmax f (y) = maxyYf (y)yYshow proving: (1) maxyY f (y) maxy yY f (y), (2) maxyY f (y)Pmaxy yY f (y).First, consider case (1). Define memberPf (y ) = max f (y)yYNext, define = 1, = 0 6= .Xf (y) = f (y )yYHence found setting variablesXf (y) = max f (y)yYyYfollowsmaxXyYf (y) max f (y)yYNext, consider case (2). Define setting variablesXf (y) = maxXf (y)yYNext, define memberf (y ) = maxf (y)y:y >0easily verifiedf (y )Xf (y)Hence foundf (y ) maxXf (y)followsmax f (y) maxyY358Xf (y)fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGReferencesAuli, M., & Lopez, A. (2011). comparison loopy belief propagation dual decompositionintegrated ccg supertagging parsing. Proceedings 49th Annual MeetingAssociation Computational Linguistics: Human Language Technologies, pp. 470480,Portland, Oregon, USA. Association Computational Linguistics.Bar-Hillel, Y., Perles, M., & Shamir, E. (1964). formal properties simple phrase structuregrammars. Language Information: Selected Essays Theory Application,pp. 116150.Boyd, S., Parikh, N., Chu, E., Peleato, B., & Eckstein, J. (2011). Distributed optimizationstatistical learning via alternating direction method multipliers. Publishers.Boyd, S., & Vandenberghe, L. (2004). Convex optimization. Cambridge Univ Pr.Boyd, S., & Mutapcic, A. (2007). Subgradient Methods. Course Notes EE364b, StanfordUniversity, Winter 2006-07.Chang, Y., & Collins, M. (2011). Exact Decoding Phrase-based Translation ModelsLagrangian Relaxation. appear proc. EMNLP.Collins, M. (1997). Three Generative, Lexicalised Models Statistical Parsing. Proc. ACL, pp.1623.Dantzig, G., & Wolfe, P. (1960). Decomposition principle linear programs. Operationsresearch, Vol. 8, pp. 101111.Das, D., Martins, A., & Smith, N. (2012). exact dual decomposition algorithm shallowsemantic parsing constraints. Proceedings of* SEM.[ii, 10, 50].DeNero, J., & Macherey, K. (2011). Model-Based Aligner Combination Using Dual Decomposition. Proc. ACL.Duchi, J., Tarlow, D., Elidan, G., & Koller, D. (2007). Using combinatorial optimization within maxproduct belief propagation. Advances Neural Information Processing Systems (NIPS).Everett III, H. (1963). Generalized lagrange multiplier method solving problems optimumallocation resources. Operations Research, 399417.Felzenszwalb, P., & Huttenlocher, D. (2006). Efficient belief propagation early vision. International journal computer vision, 70(1), 4154.Fisher, M. L. (1981). lagrangian relaxation method solving integer programming problems.Management Science, 27(1), pp. 118.Germann, U., Jahr, M., Knight, K., Marcu, D., & Yamada, K. (2001). Fast decoding optimaldecoding machine translation. Proceedings 39th Annual Meeting AssociationComputational Linguistics, pp. 228235. Association Computational Linguistics.Globerson, A., & Jaakkola, T. (2007). Fixing max-product: Convergent message passing algorithmsmap lp-relaxations. NIPS.Hanamoto, A., Matsuzaki, T., & Tsujii, J. (2012). Coordination structure analysis using dual decomposition. Proceedings 13th Conference European Chapter AssociationComputational Linguistics, pp. 430438, Avignon, France. Association ComputationalLinguistics.359fiRUSH & C OLLINSHeld, M., & Karp, R. M. (1971). traveling-salesman problem minimum spanning trees:Part ii. Mathematical Programming, 1, 625. 10.1007/BF01584070.Johnson, J., Malioutov, D., & Willsky, A. (2007). Lagrangian relaxation map estimationgraphical models. 45th Annual Allerton Conference Communication, Control Computing.Jojic, V., Gould, S., & Koller, D. (2010). Fast smooth: Accelerated dual decompositionMAP inference. Proceedings International Conference Machine Learning (ICML).Klein, D., & Manning, C. (2002). Fast exact inference factored model natural languageparsing. Advances neural information processing systems, 15(2002).Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. Proceedings2003 Conference North American Chapter Association ComputationalLinguistic Human Language Technology, NAACL 03, pp. 4854.Kolmogorov, V. (2006). Convergent tree-reweighted message passing energy minimization.Pattern Analysis Machine Intelligence, IEEE Transactions on, 28(10), 15681583.Komodakis, N., Paragios, N., & Tziritas, G. (2007). MRF Optimization via Dual Decomposition:Message-Passing Revisited. Proc. ICCV.Komodakis, N., Paragios, N., & Tziritas, G. (2011). Mrf energy minimization beyond via dualdecomposition. Pattern Analysis Machine Intelligence, IEEE Transactions on, pp. 11.Koo, T., Carreras, X., & Collins, M. (2008). Simple semi-supervised dependency parsing. Proc.ACL/HLT.Koo, T., Rush, A. M., Collins, M., Jaakkola, T., & Sontag, D. (2010). Dual decompositionparsing non-projective head automata. EMNLP.Korte, B., & Vygen, J. (2008). Combinatorial Optimization: Theory Algorithms. SpringerVerlag.Lacoste-Julien, S., Taskar, B., Klein, D., & Jordan, M. (2006). Word alignment via quadratic assignment. Proceedings main conference Human Language Technology ConferenceNorth American Chapter Association Computational Linguistics, pp. 112119.Association Computational Linguistics.Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional Random Fields: Probabilistic ModelsSegmenting Labeling Sequence Data. Proc. ICML, pp. 282289.Lemarechal, C. (2001). Lagrangian Relaxation. Computational Combinatorial Optimization, Optimal Provably Near-Optimal Solutions [based Spring School], pp. 112156, London,UK. Springer-Verlag.Magnanti, T. L., & Wolsey, L. A. (1994). Optimal trees. Tech. rep. 290-94, Massachusetts InstituteTechnology, Operations Research Center.Martin, R., Rardin, R., & Campbell, B. (1990). Polyhedral characterization discrete dynamicprogramming. Operations research, 38(1), 127138.Martins, A. (2012). Geometry Constrained Structured Prediction: Applications InferenceLearning Natural Language Syntax. Ph.D. thesis.360fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSINGMartins, A., Figueiredo, M., Aguiar, P., Smith, N., & Xing, E. (2011). augmented lagrangianapproach constrained map inference. International Conference Machine Learning.Martins, A., Smith, N., & Xing, E. (2009). Concise Integer Linear Programming FormulationsDependency Parsing. Proc. ACL, pp. 342350.Martins, A., Smith, N., Figueiredo, M., & Aguiar, P. (2011). Dual decomposition many overlapping components. Proceedings 2011 Conference Empirical Methods NaturalLanguage Processing, pp. 238249, Edinburgh, Scotland, UK. Association ComputationalLinguistics.McDonald, R. (2006). Discriminative Training Spanning Tree Algorithms DependencyParsing. Ph.D. thesis, University Pennsylvania, Philadelphia, PA, USA.Meshi, O., & Globerson, A. (2011). alternating direction method dual map lp relaxation.Machine Learning Knowledge Discovery Databases, 470483.Nedic, A., & Ozdaglar, A. (2009). Approximate primal solutions rate analysis dual subgradient methods. SIAM Journal Optimization, 19(4), 17571780.Nesterov, Y. (2005). Smooth minimization non-smooth functions. Mathematical Programming,103(1), 127152.Paul, M. J., & Eisner, J. (2012). Implicitly intersecting weighted automata using dual decomposition. Proc. NAACL.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference(2nd edition). Morgan Kaufmann Publishers.Riedel, S., & Clarke, J. (2006). Incremental integer linear programming non-projective dependency parsing. Proceedings 2006 Conference Empirical Methods NaturalLanguage Processing, pp. 129137. Association Computational Linguistics.Riedel, S., & McCallum, A. (2011). Fast robust joint models biomedical event extraction.Proceedings 2011 Conference Empirical Methods Natural Language Processing,pp. 112, Edinburgh, Scotland, UK. Association Computational Linguistics.Roth, D., & Yih, W. (2005). Integer linear programming inference conditional random fields.Proceedings 22nd international conference Machine learning, pp. 736743. ACM.Rush, A., Reichart, R., Collins, M., & Globerson, A. (2012). Improved parsing pos taggingusing inter-sentence consistency constraints. Proceedings 2012 Joint ConferenceEmpirical Methods Natural Language Processing Computational Natural LanguageLearning, pp. 14341444, Jeju Island, Korea. Association Computational Linguistics.Rush, A., & Collins, M. (2011). Exact Decoding Syntactic Translation Models Lagrangian Relaxation. Proc. ACL.Rush, A., Sontag, D., Collins, M., & Jaakkola, T. (2010). Dual Decomposition LinearProgramming Relaxations Natural Language Processing. Proc. EMNLP.Shor, N. Z. (1985). Minimization Methods Non-differentiable Functions. Springer SeriesComputational Mathematics. Springer.Smith, D., & Eisner, J. (2008). Dependency parsing belief propagation. Proc. EMNLP, pp.145156.361fiRUSH & C OLLINSSontag, D., Globerson, A., & Jaakkola, T. (2010). Introduction dual decomposition inference.Sra, S., Nowozin, S., & Wright, S. J. (Eds.), Optimization Machine Learning. MITPress.Sontag, D., Meltzer, T., Globerson, A., Jaakkola, T., & Weiss, Y. (2008). Tightening LP relaxationsMAP using message passing. Proc. UAI.Toutanova, K., Klein, D., Manning, C. D., & Singer, Y. (2003). Feature-rich part-of-speech taggingcyclic dependency network. HLT-NAACL.Wainwright, M., Jaakkola, T., & Willsky, A. (2005). MAP estimation via agreement trees:message-passing linear programming.. IEEE Transactions Information Theory,Vol. 51, pp. 36973717.Yanover, C., Meltzer, T., & Weiss, Y. (2006). Linear Programming Relaxations BeliefPropagationAn Empirical Study. Journal Machine Learning Research, Vol. 7,p. 1907. MIT Press.362fiJournal Artificial Intelligence Research 45 (2012) 79-124Submitted 03/12; published 09/12Approximative Inference Method SolvingSatisfiability ProblemsHanne VlaeminckJoost VennekensMarc DeneckerMaurice Bruynooghehanne.vlaeminck@cs.kuleuven.bejoost.vennekens@cs.kuleuven.bemarc.denecker@cs.kuleuven.bemaurice.bruynooghe@cs.kuleuven.beDepartment Computer Science,Celestijnenlaan 200a3001 Heverlee, BelgiumAbstractpaper considers fragment second-order logic. Many interesting problems, conformant planning, naturally expressed finite domain satisfiability problems logic. satisfiability problems computationally hard (P2 )many problems often solved approximately. paper, develop generalapproximative method, i.e., sound incomplete method, solving satisfiabilityproblems. use syntactic representation constraint propagation method firstorder logic transform satisfiability problem SO(ID) satisfiabilityproblem (second-order logic, extended inductive definitions). finite domain satisfiability problem latter language NP handled several existingsolvers. Inductive definitions powerful knowledge representation tool, motivates us also approximate SO(ID) problems. order this, first showperform propagation inductive definitions. Next, use approximateSO(ID) satisfiability problems. provides general theoretical frameworknumber approximative methods literature. Moreover, also showuse framework solving practical useful problems, conformant planning,effective way.1. IntroductionFinite model generation logical paradigm solving constraint problems. successfulinstance field SAT, efficient solvers low level CNF language developed. instances, expressive languages, Answer Set Programming(ASP) (Baral, 2003) model expansion (MX) (extensions of) first order logic. ASP,e.g., finite Herbrand models answer set program computed (Baral, 2003). Modelexpansion (MX) (Mitchell & Ternovska, 2005) generalizes Herbrand model generationaims computing one models theory expand finite interpretation I0(possibly empty) subset symbols . MX first order logic (MX(FO)) formallyequivalent finite domain satisfiability checking problem existential second-orderlogic (SAT (SO))1 known Fagins celebrated theorem capture NP (Fagin,1974). is, problems NP exactly precise sense equivalentsatisfiability problem, hence MX(FO) problem. range solvers exists1. specifically, search problem witness problem.c2012AI Access Foundation. rights reserved.fiVlaeminck, Vennekens, Denecker & Bruynooghefinite model generation, e.g., overview state-of-the-art ASP MX(FO()) solvers(here, FO() refers family extensions FO) found reportsASP competition (e.g., Denecker, Vennekens, Bond, Gebser, & Truszczynski, 2009).Example 1.1. bounded planning problem modeled Finite Model Generationproblem. problem deliberately kept simple serve running examplepaper: glass may clean not, cleaned action wiping.represent dynamic domain following FO theory Tact :: (Clean(t + 1) Clean(t) W ipe(t)).Clean(0) InitiallyClean.(1)(2)bounded planning problem considering turn dirty glass cleanone n steps, n N given constant. indeed formulatedModel Expansion problem: find model satisfies Tact InitiallyClean false,Clean(n) true. formulate problem equivalently finite domainsatisfiability problem, namely satisfiability problem range [0 . . . n] timepoints following formula:W ipe, Clean, InitiallyClean : (act InitiallyClean Clean(n)),(3)act denote conjunction sentences Tact . n > 0, formulaindeed satisfiable suitable interpretation constants 0 n binary function+, and, moreover, witness W satisfiability provides plan. instance, wipingtime point 0 job, verified witness W W ipeW = {0}CleanW = {1, . . . , n}.large number search problems indeed seen Finite Model Generationproblems, also number problems higher complexity NP,consequently cannot formulated MX(FO) problem. Indeed, paperinterested NP, next level P2 polynomial hierarchy. Perhapsprototypical problem finite domain satisfiability SO: satisfaction finiteinterpretations P2 every sentence P2 -hard sentences(Immerman, 1998). interesting P2 problem conformant planning,discuss detail Section 7, already introduce next example.Example 1.2. Extending Example 1.1, suppose know whether objectinitially clean dirty, still want plan guaranteed make clean, matterinitial situation was. longer standard planning problem, calledconformant planning problem. formulate following satisfiabilityproblem:W ipe InitiallyClean, Clean : (act Clean(n)).(4)words, need assignment action W ipe goal Clean(n) satisfiedevery initial situation InitiallyClean fluent Clean satisfy action theory.Solving problems like would require us make choice existentially quantified predicates check implication satisfied every interpretation80fiAn Approximative Inference Methoduniversally quantified predicates. done principle, practiceoften expensive. paper, explore alternative based propagationmethod first order logic developed Wittocx, Denecker, Bruynooghe (2010).method computes polynomial time, given theory partial interpretation I,approximation certainly true (= true models expandI) certainly false (= false models). Now, interesting propertypropagation method syntactically represented monotone inductivedefinition (Denecker & Ternovska, 2008) defines (in approximative way) underestimates predicates complements. monotone inductivedefinition essentially set propagation rules, similar definite logic program,interpreted least fixpoint immediate consequence operator.given theory obtain approximating definition linear transformationoriginal FO formula.Returning example, need find interpretation action predicates, every interpretation predicates, implication actClean(n) satisfied, i.e., without knowing anything predicates, already certain implication satisfied. basic idea behindmethod approximate problem form P Q , using approximate definition Wittocx et al. check whether interpretation existentiallyquantified predicates P property making true, regardless predicates Q.Essentially, reduces problem SO(ID) problem (where SO(ID)refer extended inductive definitions).Section 5, extend method SO(ID) problems. argued DeneckerTernovska, inductive definitions useful tool knowledge representation.example, many dynamic domains formulated naturally modular way usinginductive definitions, quite tedious FO. already mentioned conformant planning typical satisfiability problem. Typically, conformantplanning problems require modeling dynamic domain. come backsyntax semantics inductive definitions next section, dynamic domainExample 1.1 can, alternative action theory Tact , formulated followinginductive definition act :Clean(t + 1) Clean(t).Clean(t + 1) W ipe(t).(5)Clean(0)InitiallyClean.conformant planning problem formulated alternatively satisfiability problem formula W ipe InitiallyClean, Clean : (act Clean(n)). However,longer satisfiability problem, SO(ID) satisfiability problem.motivates us see extend approximation method SO(ID)satisfiability problems. purpose, first show symbolically represent propagation inductive definitions. Next, show use togetherrepresentation propagation FO approximate finite domain SO(ID) satisfiabilityproblems.approximation method number benefits. First all, general method,applied automatically approximately solve problem. Second,81fiVlaeminck, Vennekens, Denecker & Bruynoogherequired computation carried off-the-shelf MX(FO(ID)) solver, allowingmethod benefit effortlessly improvements solver technology,IDP system Marien, Wittocx, Denecker (2006). Finally, show Section 7,method elegantly generalizes number approximate reasoning methodsliterature (e.g., Baral, Gelfond, & Kosheleva, 1998; Son, Tu, Gelfond, & Morales, 2005;Denecker, Cortes Calabuig, Bruynooghe, & Arieli, 2010; Doherty, Magnusson, & Szalas,2006; Son et al., 2005).Parts work already presented JELIA 2011 conference (Vlaeminck, Wittocx, Vennekens, Denecker, & Bruynooghe, 2010).2. Preliminariesassume familiarity classical first-order logic (FO) second-order logic (SO).section introduce notations conventions used throughout paper.2.1 First-order Logicvocabulary finite set predicate symbols P function symbols F ,associated arity. Constants function symbols arity 0. often denote symbolarity n S/n. interpretation consists domain assignmentrelation P Dn predicate symbol P/n assignment functionF : Dn function symbol F/n . assume P contains equalitypredicate = interpreted identity relation. pre-interpretation consistsdomain interpretation function symbols. -interpretation 0 ,denote I|0 restriction symbols 0 . 1 2 two disjointvocabularies, 1 -interpretation domain J 2 -interpretationdomain, + J denotes unique (1 2 )-interpretation domain(I + J)|1 = (I + J)|2 = J.Terms formulas vocabulary defined usual. expression formP n-ary predicate Dn called domain atom. domain literalP (d)negation P (d)thereof. usual, denote formuladomain atom P (d)[x] indicate set free variables subset x. formula withoutfree variables called sentence. satisfaction relation |= defined usual.interpretation I, formula n free variables tuple n domain elements d,use |= [d] shorthand I, [x : d] |= [x], variable assignment,variable assignment except maps variables[x : d]define truth evaluation function ([d])follows:x domain elements d.([d]) = iff |= [d] ([d]) = f otherwise. say formula negationnormal form contains implications equivalences, negations occur directlyfront atoms. define inverse truth values follows: (f )1 = (t)1 = f .also define following strict order truth values: f <t t. truth orderpoint-wise extends interpretations: J two -interpretations, sayJ every predicate symbol P tuple domain elements holdsP J (d).P (d)Similar real number r approximated interval [l, u] l ru, paper approximate -interpretations K pair (I, J) -interpretations,82fiAn Approximative Inference MethodK J. denote [I, J] interval interpretations K.interval empty 6t J. follows easily well-known monotonicityresults, evaluate positive occurrences (i.e., scope even numbernegations) atoms formula I, negative occurrences (i.e., scopeodd number negations) J, underestimating truth interval[I, J]. Conversely, evaluate positive occurrences J negative occurrences I,overestimating truth [I, J]. state property formally,introduce following notation.Definition 2.1 (Pos-neg evaluation relation +IJ ). Let -formula let J-interpretations. define pos-neg evaluation J, denoted +IJ ,induction size :atom = P (t), +IJ = ;= , +IJ = ( +JI )1 ;= 1 2 , +IJ = iff i+IJ = = 1, 2;= x , +IJ = iff +I[x/d]J[x/d] = t.indeed that, K [I, J], +IJ K +JI . Also,K = +KK .intimate connection approximation interpretationpair interpretations Belnaps four-valued logic (1977). denote truth valuestrue, false, unknown inconsistent four-valued logic respectively t, f , u i.truth values, truth order precision order p defined shown Figure1.four-valued relation arity n domain function Dn {t, f , u, i}.four-valued interpretation vocabulary consists pre-interpretation P ,four-valued relation arity n predicate symbol P/n . Again,precision order pointwise extends interpretations: J two -interpretations,say p J every predicate symbol P tuple domain elementsp P J (d).Similarly, also truth order extended interpretations.holds P (d)@t^<t<p<tu^@i<pf_<t<t@i^?t<p<pufFigure 1: truth precision order83fiVlaeminck, Vennekens, Denecker & Bruynooghenatural isomorphism Belnaps four truth values pairs twostandard truth values:(t, t) = t;(f , t) = u;(t, f ) = i;(f , f ) = f .Intuitively, mapping interprets first argument underestimate realtruth value, second argument overestimate: underestimate foverestimate t, real truth value indeed unknown; whereas, underestimateoverestimate f , cannot exist real truth value, since 6 f ,end inconsistency. isomorphism extends obvious way isomorphismpairs (I, J) two-valued interpretations four-valued interpretationsshare pre-interpretations: (I, J) isomorphic iff, predicate= (P (d),P J (d)).also denote isomorphism .P/n tuple Dn , P (d)tight link pos-neg evaluation function +IJ Belnapevaluation := (+IJ , +JI ), (I, J) = I.three-valued structure (i.e., never assigns i) corresponds standardKleene evaluation (1952). rest paper, often omit isomorphism ,and, e.g., simply denote four-valued truth value formula pair interpretations(I, J) (I,J) . important property, already stated different notation,(I,J) p K K [I, J].natural well-known alternative way using interval [I, J]J assign truth value formula : supervaluation (van Fraassen, 1966).Definition 2.2 (Supervaluation sv(I,J) (.)). supervaluation sv(I,J) () sentencepair interpretations (I, J) (or equivalently, three-valued interpretation (I, J))definedsv(I,J) () = glbp ({K |K [I, J]}).easy see always sv(I,J) () p (I,J) . inequality may strict.instance, take = Q Q interpretations J Q(I,J) = u,sv(I,J) () = t, (I,J) = u. supervaluation following interesting property.Let interpretation free vocabulary formula = Q , let (J1 , J2 )=uleast precise pair interpretations Q domain (i.e., Q(J1 ,J2 ) (d)nQ/n Q ). sv(I+J1 ,I+J2 ) () = = t.Key approach simulate four-valued truth evaluation pairsinterpretations encoding certainly true certainly false, using singletwo-valued structure tf new vocabulary tf . show next section,gives us convenient vocabulary syntactically represent constructionapproximation. new vocabulary tf contains function symbols F and,predicate P P , two symbols P ct P cf . interpretations P ct P cfcertainly truetf contain, respectively, tuples P (d)certainly false. Formally, vocabulary four-valued -interpretation84fiAn Approximative Inference Method= (I, J), tf -interpretation tf pre-interpretation I, definedby:(P ct )Itf(d)p t} = P ,= {d|P(P cf )Itf(d)p f } = Dn \ P J .= {d|Ptftfinterpretation three-valued iff (P ct )I (P cf )I disjoint P .tftftwo-valued iff (P ct )I (P cf )I others complement Dn . Also, p J ,tftftftfthen, P , (P ct )I (P ct )J (P cf )I (P cf )J .Definition 2.3 (ct cf ). given -formula [x], let ct [x] tf -formulaobtained first reducing negation normal form replacing occurrencespositive literals P (t) P ct (t) negative literals P (t) P cf (t), let cf [x]formula ([x])ct .interesting property formulas ct cf contain negations.Also, following proposition well-known.Proposition 2.1 (Feferman, 1984). every -formula interpretation I, holdsp [d]+IJ = (ct [d])tf = t. Also, [d]p f[d]tf+JIcf[d]= f ( [d])= t.2.2 FO(ID)subsection recall FO(ID) (Denecker & Ternovska, 2008), extension FOconstruct respresent common forms inductive definitions,monotone induction, induction well-founded order iterated induction.illustrated Denecker Ternovska, FO(ID) used represent different sortscommon knowledge, temporal dynamic domains, closed world assumption, defaults, causality, etc. paper, use definitions symbolically representpropagation, FO formulas, already mentioned introduction, alsopropagation inductive definitions themselves.definitional rule vocabulary expression form x P (t)P (t) atomic formula FO formula. symbol new connective, calleddefinitional implication, distinguished FO material implication symbol(or converse ). definition finite set definitional rules. predicatesymbol P head rule called defined predicate; predicatefunction symbols called open symbols parameters definition;set defined predicates denoted Def (), remaining symbols Open() (noteOpen() therefore also includes F ).Given interpretation open predicates, definition unique model,constructed firing rules appropriate order. definingformally, first consider example.Example 2.1. Reachability graph expressible FO. is, FOformula vocabulary consisting two predicates Edge/2 Reach/2model , (d1 , d2 ) ReachM iff non-empty path d1 d285fiVlaeminck, Vennekens, Denecker & Bruynooghegraph represented EdgeM . represent reachability inductive definitionhowever. following definition defines predicate Reach terms open predicateEdge.()xy Reach(x, y) Edge(x, y).xy Reach(x, y) z(Reach(x, z) Reach(z, y)).). definition given Open()-interpretation O,Definition 2.4 (Operatortwo-valued Def ()-interpretationsdefine immediate consequence operator(I) = J iff defined predicate P/n tuple Dn , holds= iff exists rule x P (t) [x], t(O+I) = (O+I) [d]= t.P J (d)model positive definition (i.e., defined predicates occur negativelybody rules) defined least fixpoint immediate consequence operator.use odI|Open() () denote model definition extending restrictionopen predicates function symbols . open predicates,omit subscript simply use od(). postpone going detailconstruct model general (non-monotonic) inductive definition Section5. next two sections, use positive definitions.FO(ID) formulas inductively defined rules standard FO formulas,augmented one extra case:definition FO(ID) formula (over )).Note rule bodies contain definitions, rules occur inside definitionsFO(ID) formulas whereas definitions used FO(ID) formulasanywhere atoms used.define satisfaction relation |= FO(ID) using standard inductiverules FO, augmented one extra rule:|= = odI|Open() (),on, assume without loss generality definition , holdsevery defined predicate P Def () defined exactly one rule, denoted x(P (x)P [x]). Indeed, definition brought form process similarpredicate completion. transformation consists first transforming rules formx(P (t) ) equivalent rules y(P (y) x(y = )). Next, one merges rulesform x(P (x) [x]) x(P (x) 1 [x] . . . n [x]).3. Propagation FOsection give general, symbolic representation propagation first-order logic.this, base work Wittocx et al. (2010). come backprecise relation material presented section work endsection.Suppose FO theory vocabulary , pre-interpretation ,finite three-valued interpretation represents (incomplete) knowledge86fiAn Approximative Inference Method(Clean(t + 1) (Clean(t) W ipe(t))).A1Act1Clean(t + 1) (Clean(t) W ipe(t))= 3Clean(t + 1)f = 3A2 (t)Act2 (3)Clean(t) W ipe(t)= 3Clean(t)= 3A3 (t)Act3 (3)Clean(t + 1)Cleancf (4)W ipe(t)= 3Clean(t)Cleancf (3)W ipe(t)W ipecf (3)Figure 2: Propagation FO.predicates . would like know implications knowledge, assumingtheory satisfied context I. find out, look setmodels complete three-valued interpretation, i.e., = {M | |=p }. Given partial information I, everything truemust certainly true according , everything false mustcertainly false according . words, information allows usderive captured greatest lower bound G = glbp M.general, computing greatest lower bound may expensive (the data complexity P2 ) practical use. However, may still achieve useful resultscomputing approximation p p G. computeapproximation propagating three-valued interpretation parse tree. illustrate following example.Example 3.1. Consider sentence : Clean(t + 1) Clean(t) W ipe(t). Rewritingnegation normal form, becomes:Clean(t + 1) (Clean(t) W ipe(t)).Now, assume satisfied, know Clean false timepoint 4.knowledge satisfied, immediately follows that, timepoints t,disjunctive formula Clean(t + 1) (Clean(t) W ipe(t)) satisfied. Using factClean false timepoint 4, deduce conjunction (Clean(t)W ipe(t)) true timepoint 3. Therefore, models Clean falsetimepoint 4, W ipe Clean false timepoint 3. reasoning processillustrated left part Figure 2.construct symbolic representation propagation process. First,introduce additional vocabulary Aux refer different nodes parsetree process operates. use additional vocabulary transformFO formula equivalence normal form formula. similar Tseitintransformation (1968) propositional logic.Definition 3.1 (EN F ()). FO formula negation normal form, introducenew predicate symbol arity n non-literal subformula [x] n87fiVlaeminck, Vennekens, Denecker & Bruynooghefree variables. denote set new predicates Aux(). newpredicate symbols defined formula Eq(A ) follows. make notation simplerassume 1 , . . . , n non-literal subformula. definitions analogouswhenever literal, instead Ai literal used bodydefinition.[x] subformula form 1 [x1 ] 2 [x2 ] . . . n [xn ], Eq(A )x (A (x) A1 (x1 ) A2 (x2 ) . . . (xn )).[x] subformula form 1 [x1 ] 2 [x2 ] . . . n [xn ], Eq(A )x (A (x) A1 (x1 ) A2 (x2 ) . . . (xn )).[x] subformula form 1 [x, y], Eq(A ) equals x (A (x)A1 (x, y)).[x] subformula form 1 [x, y], Eq(A ) equals x (A (x)A1 (x, y)).define equivalence normal form set Eq(A ), denoteEN F ().Example 3.2. According definition, EN F () theory Example 3.1 is:A1 A2 (t).A2 (t) Clean(t + 1) A3 (t).A3 (t) Clean(t) W ipe(t).illustrated right side Figure 2.Using auxiliary vocabulary, write propagations shownFigure 2 following implications.A1A2 (3) Clean(4)A3 (3)A3 (3)A2 (3).A3 (3).Clean(3).W ipe(3).Note rules top-down rules, is, implications propagate informationsubformula parse tree, component subformula (possiblyusing also information components subformula, implicationA2 (3)Clean(4) A3 (3)). general, also bottom-up propagations course possible.instance, Clean(4) could derive A2 (3). every predicate , deriveEq(A ) set implications 1 2 , propagation correspondsderiving consequent 2 antecedent 1 (so, different implicationslogically equivalent). defined Table 1. last column table indicateswhether rule top-down (TD) bottom-up (BU).Definition 3.2 (IN F ()). Given equivalence EN F () certain formula ,denote Imp() set implications obtained Table 1.define implication normal form , denoted F (), follows: F () = EN F () Imp().88fiAn Approximative Inference Method= Eq(A )Imp()x (L L1 . . . Ln ).xxxx(L1 . . . Ln L).(Li L).(L Li ).(L L1 . . . Li1 Li+1 . . . Ln Li ).1in1in1inBUBUTDTDx (L L1 . . . Ln ).xxxx(L1 . . . Ln L).(Li L).1in(L Li ).1in(L L1 . . . Li1 Li+1 . . . Ln Li ). 1 nBUBUTDTDx (L[x] L0 [x, y]).x ((y L0 [x, y]) L[x]).x(y L0 [x, y]) L[x]).xy (L[x] L0 [x, y]).xy ((L[x] z (y 6= z L0 [x, y][y/z])) L0 [x, y]).BUBUTDTDx (L[x] L0 [x, y]).x ((y L0 [x, y]) L[x]).x(y L0 [x, y]) L[x]).xy (L[x] L0 [x, y]).xy ((L[x] z (y 6= z L0 [x, y][y/z])) L0 [x, y]).BUBUTDTDTable 1: ENF INFwork Wittocx et al. (2010) proven models models F ()true correspond, sense restriction model F ()also model , vice versa, every model extended modelF () . implications form core approximation method.approximation could made complete adding implicationsF (), definition tries strike balance completeness easeautomatically deriving implications.Example 3.3. three formulas EN F () Example 3.2, followingtable shows corresponding set implications Imp(). complete theory F ()consists union three sets.A1 A2 (t).(t A2 (t)) A1 .(t A2 (t)) A1 .(A1 A2 (t)).((A1 t0 (t 6= t0A2 (t0 ))) A2 (t)).(A2 (t) Clean(t + 1) A3 (t)).(Clean(t + 1) A3 (t) A2 (t)).(Clean(t + 1) A2 (t)).(A3 (t) A2 (t)).(A2 (t) Clean(t + 1)).(A2 (t) A3 (t)).(A2 (t) A3 (t) Clean(t + 1)).(A2 (t) Clean(t + 1) A3 (t)).(A3 (t) (Clean(t) W ipe(t))).(Clean(t) W ipe(t) A3 (t)).(Clean(t) A3 (t)).(W ipe(t) A3 (t)).(A3 (t) Clean(t)).(A3 (t) W ipe(t)).(A3 (t) W ipe(t) Clean(t)).(A3 (t) Clean(t) W ipe(t)).reader verify four implications representing propagation Example 3.1indeed belong F ().propagation process Example 3.1 described least fixpointcomputation, apply implications (i.e., infer head bodyalready inferred), longer infer new information. representfixpoint computation inductive definition syntax FO(ID). However,two complications.89fiVlaeminck, Vennekens, Denecker & BruynoogheFirst, paper, always need implications F (). Indeed,typically subset symbols already knowknow. conformant planning example, instance, caseexistentially quantified predicate W ipe/2, simply use model expansionsystem guess complete interpretation predicate. job propagationprocess figure consequences particular guess. this, implications predicate (i.e., W ipe/2) head obviously needed.Second, fixpoint computation needs infer atoms true alsofalse. However, syntax FO(ID) allow negative literalsheads rules. Therefore, definition contain rules predicates Poriginal vocabulary head, instead use predicates P ct P cftf -vocabulary. Since need rules fully known predicates head,introduce P ct P cf predicates P \ .ctctgiven formula , therefore define ctformula (see Definition 2.3) Pcfreplaced P P P every predicate P .Definition 3.3 (Approx ()). formula , define Approx ()inductive definition contains, every sentence x ( L[x]) F () Lctliteral predicate , definitional rule x(L[x]ct). also defineTDApproxBU() (and Approx ()) way Approx (), containingdefinitional rules coming bottom-up (respectively, top-down) rules F ().often assume without loss generality = . Whenever casedrop use Approx() rather Approx (), denote approximativedefinition.Example 3.4. Using implications F () Example 3.3, obtain definitionshown Figure 3 Approx(). take = {W ipe}, get definitionApprox () Figure 3, apart last seven definitional rules replacedfollowing five definitional rules....ctcf(t)Clean(t)Wipe(t).3cfctA3 (t)Clean (t).W ipe(t).Acf3 (t)cfctClean(t)(t).3cfctClean (t) A3 (t) W ipe(t).contrast Approx() definition longer approximates predicate W ipe.definition Approx () used find certainly holds holds given twovalued interpretation predicates .Example 3.5. larger example, look Example 1.1. Let us take= act = {W ipe}. definition Approx () found AppendixA.approximative definition useful properties, formulatenext two theorems. first property that, using approximative definition90fiAn Approximative Inference MethodctA1cf1ctA2 (t)Acf2 (t)Acf2 (t)ct2 (t)ctA2 (t)Cleancf (t + 1)cfA3 (t)Cleanct (t + 1)Act3 (t)Act3 (t)cf3 (t)cf3 (t)cfClean(t)cfWipe(t)Cleanct (t)W ipect (t)Act2 (t).Acf(t).2ctA1 .cf00ct 0A1 (t 6= A2 (t )).cfcfClean (t + 1) A3 (t).ctClean (t + 1).ctA3 (t).cfA2 (t).cfA2 (t).cfAct2 (t) A3 (t).ctA2 (t) Cleancf (t + 1).cfcfClean (t) W ipe (t).Cleanct (t).W ipect (t).Act(t).3Act(t).3cfcfA3 (t) W ipe (t).cfcfA3 (t) Clean (t).A1A2 (t)A3 (t)Clean(t + 1)Clean(t)W ipe(t)Figure 3: Example approximative definitiontogether encoding three-valued interpretation original vocabulary,give exact characterization approximative definition computes. Indeed,setting, ApproxBU () actually encodes three-valued Kleene evaluationI. Moreover, adding top-down rules change this, since computeactually anything, long information original vocabulary providedinput. formally state property, need define encodefour-valued interpretation definition. on, assume vocabulary-pre-interpretation I, contains constant symbol Cd every domain elementdomain I, pre-interpretation holds (Cd )I = d.allows us identify Cd therefore, abusing notation, use denote Cdfollows.Definition 3.4 (I ). Given four-valued -interpretation I, definition associateddenoted defined=| P (d)p t}{P ct (d)cf{P (d) | P (d) p f }Theorem 3.1. Given -formula four-valued -interpretation I, followingholds:a) case three-valued holds Approx() logically equivalentApproxBU () , is, od(Approx() ) = od(ApproxBU () )91fiVlaeminck, Vennekens, Denecker & Bruynoogheb) Let od(Approx() ), v1 truth value Actv2 truthvalue Acf, (v1 , v2 ) corresponds four-valued truth value , i.e.,= (v1 , v2 ).Proof. See Appendix B.summary, theorem says that, first all, approximation alwayscomputes four-valued Belnap evaluation four-valued structure I. Moreover,computation done bottom-up rules approximation alone. threevalued, top-down rules actually effect all. four-valued,may still serve purpose, however: bottom-up rules derivedsubformula inconsistent, propagate information derive smallerformulas also inconsistent. see this, consider following formula P Q, takefour-valued interpretation P = Q = t. one verifycfbottom-up rules Approx() infer ActP Q AP Q true.However, top-down rules also infer Qcf true.theorem information add approximative definitionform definition , i.e., assert truth, resp. falsity domain atoms.following definition allows us assert truth falsity groundedsubformula [d].Definition 3.5 ( ). Given -formula , -pre-interpretation I, set[x] subformula arity n Dnformulas ()[d],domain I, define follows:cf= {Act(d) | [d] } {A (d) | [d] }.assert way truth (or falsity) set grounded subformulas ,obtain approximation everything holds (respectively, hold)models . However, opposed theorem above, next theoremgive exact characterization approximation get.Theorem 3.2. Given -formula , set defined subformula 0 [x0 ]cf 00 00. Let od(Approx() ). |= Act0 (d ) (resp. A0 (d )), |= [d ](resp. |= 0 [d0 ]).Note interesting special case theorem take equal {}thus add ActApprox(). definition gives approximation everythingcertainly true resp. certainly false models .Returning exact relationship work Wittocx et al. (2010)content section, see Wittocx et al. interested specialcase, i.e., approximating models theory. reason transformationformula EN F () already includes formula t, cause ruleActalways included approximating definition. soundness resultsalso formulated proven setting. However, difficult seeproofs trivially adapted proof Theorem 3.2 general settingused section.92fiAn Approximative Inference Method4. Approximating SO-Satisfiability Problemsuse approximate definition previous section approximate following problem. Take formula F = P Q : . ease presentation,assume second-order formulas paper contain free predicate symbols,results generalize setting free predicate symbols. also assumeQ contains predicate symbols. follows, denote vocabulary. question want answer whether formula F satisfied givenfinite-domain pre-interpretation constant function symbols formula.satisfiability problem boils deciding whether find witnesssatisfiability formula, following sense.Definition 4.1 (Witness). call J witness satisfiability formula P Q :given finite pre-interpretation I, J interpretation \ Q extending (i.e., Jinterpretation whole vocabulary without universally quantified predicates)Q : satisfied J. Equivalently, J witness three-valued holdsinterpretation J expands J assigning u domain atom Q(d),svJ () = t.goal section approximate satisfiability problemssatisfiability problem following sense.Definition 4.2 (Sound approximation). Consider satisfiability problemformula P Q : , FO formula alphabet . SO(ID) formulaform P R : 0 , 0 FO(ID) formula alphabet \ Q R, soundapproximation satisfiability problem if, whenever J witness satisfiabilityP R : 0 , J|\Q witness satisfiability P Q : .words, sound approximation G satisfiability problemformula F stronger SO(ID) formula, i.e., one fewer witnesses P .4.1 Naive Methoduse results Theorem 3.1 construct sound approximation givenformula.Definition 4.3 (APP(F )). Given formula F = P Q : . Take alphabetfunction symbols predicates P . define APP(F ) formulatfP R : Approx () Actvocabulary R, R = (Q Aux()) .intuition -interpretation I, Approx () give resultfour-valued evaluation -interpretation expands assigning unknown universally quantified predicates Q. entire FO formula evaluatestrue four-valued interpretation, know satisfied interpretation expands (in words, every interpretation Q predicates), thuswitness satisfiability entire formula F . auxiliary predicatesAux() introduced transformation ENF needed waypropagation works, value completely determined P .93fiVlaeminck, Vennekens, Denecker & BruynoogheProposition 4.1. formula F form P Q : , holds APP(F )sound approximation F .Proof. follows immediately Theorem 3.1, take three-valued inter = u Q Q Dn ,pretation, interpretation (Q(d))= (P (d))P P Dn , domain I.(P (d))example, take F formula P Q : = P Q, APP(F )becomes:ctctPQAcf P QcfcfctcfctP, Q , Q , , :Act.ct Act PQcfQ Acfstart interpretation open predicate P definition Approx{P } ().Let us take interpretation makes P true. unique model definitionctcfextends interpretation assigns true Actfalse Q , QctAcf. Therefore, satisfies Approx{P } () . Hence, witnesssatisfiability APP(F ), and, indeed, also witness satisfiability originalformula P Q : P Q.approximation method sound, many applications still incomplete.Indeed, let us look following formula: F = Q : Q Q. APP(F ) becomes:Qct QcfActcfcf QctQQct Act Qctcf:,Qct , Qcf , ActAct.cf AcfQcfQcf ActQQct Acfdefinition entail Act, APP(F ) unsatisfiable, even though original formula F clearly always satisfied. problem that, showedprevious section, definition encodes three-valued Kleene evaluation,strong enough find formula F satisfied. this, need strongersupervaluation.Recall preliminaries saw supervaluation Kleene evaluationgeneral equal. However, formulas equal. literature, severalclasses formulas agree proposed, e.g., context locallyclosed databases (Denecker et al., 2010), context reasoning incompletefirst-order knowledge (Liu & Levesque, 1998). latter introduces normal form N Ffirst-order formulas, supervaluation coincides Kleene evaluation,proves certain classes formulas normal form N F. One classCNF formulas every two literals conflict-free: pair literalsconflict-free either polarity, use different predicates,use different constants argument position. immediately follows94fiAn Approximative Inference Methodapproximation complete formulas first-order formula satisfiescondition.Proposition 4.2. formula F form P Q : , N Fnormal form (according Liu & Levesque, 1998) satisfiable respect given finitepre-interpretation SO-formula APP(F ) satisfiable w.r.t. I.Proof. follows immediately results Liu Levesque Theorem 3.1.4.2 Complete MethodUnfortunately, many applications give rise formulas first-order part fallsoutside class N F, means completeness method guaranteed.Particularly troublesome practice formulas common form P Q : 1 2 .formulas, naive approximation method previous section tries findinterpretations P implication = (1 2 ) holds Q. However,look details approximative definitions, find Actdefined rulecfctbody 1 2 . words, approximation derive holdsQ either case 1 false Q 2 true Q. However,rarely case. practical applications, witnesses interesttypically satisfy implication 1 2 always falsify 1 alwayssatisfy 2 , rather interpretation Q satisfies 1 also satisfies 2.instance, conformant planning example, always interpretationsfluents satisfy action theory act , arbitrarily assignfluent value wrong initial value actions performed. Evenset actions completely correct conformant plan, therefore cannot make goalcertainly true, still unsatisfied wrong interpretationsfluents. course, bother good method finding conformant plans.thing matter goal satisfied interpretationsfluents satisfy action theory.Luckily, approximation method also used discover kind witnesses.thing required add approximative definition = Approx ()rule Act1 t. do, seed approximation assumption 1holds. Starting assumption, top-down rules derive propertiespredicates Q shared interpretations Q actually satisfy 1 .bottom-up rules propagate information upwards discover whetherproperties suffice ensure 2 also holds. do, know 2 indeedmust hold every interpretation Q satisfies 1 therefore foundwitness formula.want find witnesses kind degenerate witnesses either make1 false Q 2 true Q, could simply combine new method oldctctone check either whether Act2 holds according {A1 t} whether holdsaccording itself. However, turns necessary: achievecteffect checking whether {Act1 t} implies . because, first,ctdefinition {Act1 t} able derive whenever can:ctctderive A2 {A1 t} obviously still able so; would95fiVlaeminck, Vennekens, Denecker & Bruynooghectable derive Acf1 , {A1 t} also able so, simplyapproximation flow information ct cf variantsformula, additional assumption Act1 holds change original derivationctctctAcf1 . Second, {A1 t} derive A2 , also derives , simplycfctcontains rule ActA1 A2 . Therefore, find kinds witnessesctchecking whether Actimplied single definition {A1 t}.Definition 4.4 (APP (F )). formula F = P Q : ,definitionform 1 2 , define APP (F ) P R : Act,ctApprox (1 2 ) {A1 t}.Note obtain Definition 4.3 special case taking trivial formula1 . approximation method still sound, following proposition states.Proposition 4.3. Given formula F form P Q : , = 1 2 ,SO(ID) formula APP (F ) sound approximation F .Proof. See Appendix C.Since approximative definition APP (F ) contains rules Approx(F ),hard see new approximation method least complete oneusing APP(F ) (Definition 4.3). Moreover, seen following example,also strictly complete.Example 4.1. Let us consider following formula F = P Q : (Q P ) Q.P = clearly witness satisfiability problem. denote (Q P ) Q1 (Q P ) 2 , APP(F ) following formula.Act1cfct1A2P R :Acf2ctQcfQctAcf2 QctA2 QcfP QctP QcfAct2 PAcf2Act1 .Now, even P = t, definition body formula entail Act1 = t.Therefore, APP(F ) satisfiable. hand, APP (F ) formulaabove, apart definition contains one rule, is, rule Act2 t.easy verify APP (F ) satisfiable, indeed P = witness.Obviously, new method still complete formulas 1 2 , 1 2satisfies normal form N F. However, method also works many formulas outsideclass. Unfortunately, difficult characterize precisely much completenew method is. instance, one source loss completeness comes factcurrent translation ENF cannot recognize multiple occurrences subformula,introduce different Tseitin predicate occurrence. Even though cannot96fiAn Approximative Inference Methodguarantee completeness method general, always found solutionsconformant planning benchmarks considered Section 6.final remark method approximative definition Approx (12 ) contains number rules superfluous context. Indeed, method,definition takes input interpretation P together assumption1 certainly true. uses bottom-up top-down rules derived 1compute effect inputs predicates Q. Finally, rules derived 2compute whether derived information Q suffices make 2 certainly true.However, know Theorem 3.1, bottom-up rules 2 neededthis. Therefore, top-down rules 2 actually contribute nothing couldTDwell removed. Adapting Definition 4.4 useBU = \ Approx (2 ) insteadleads following definition.Definition 4.5 (APPBU (F )). formula F = P Q : ,ctform 1 2 , define APPBU (F ) P R : BU ,TDctBU = Approx (1 2 ) \ Approx (2 ) {A1 t}.follows directly Theorem 3.1 Proposition 4.3 sound approximation. removed top-down rules approximation 2 , remainingrules serve, already know, compute Kleene evaluation 2 .computing Kleene evaluation subformula 2 , useptTseitin predicates Act. alternative avoid Tseitin predicatesdefining Act2 directly single rule:ctAct2 (2 )variant summarized following definition.Definition 4.6 (APPBU,U nf (F )). formula F = P Q : ,ct1 2 , define APPBU,U nf (F ) P R : BU,U nf ,ctctctBU,U nf = Approx () \ Approx (2 ) {A2 (2 ) } {A1 t}.approximation actually equivalent Def. 4.5. followsfact bottom-up rules 2 positive non-recursive, allows useliminate Tseitin predicates introduced parse tree 2 applying unfoldingprocedure Tamaki Sato (1984). iteratively applying equivalence preservingprocedure, reduce rules generated approximate 2ctsingle rule Act2 (2 ) .5. Approximating SO(ID)-Satisfiability ProblemsInductive definitions important knowledge representation. argued DeneckerTernovska (2008), inductive definitions used represent mathematicalconcepts, also sort common sense knowledge often represented logicprograms, dynamic domains, closed world assumption, defaults, causality, etc.97fiVlaeminck, Vennekens, Denecker & BruynoogheTherefore, inductive definitions make task representing problem logic considerably easier. example use inductively defined Situation Calculusreasoning actions (Denecker & Ternovska, 2007). Recall introductionshowed represent Tact Example 1.1 inductive definition act :Clean(t + 1) Clean(t).Clean(t + 1) W ipe(t)..Clean(0)InitiallyClean.associated conformant planning problem expressed SO(ID) satisfiability problem:W ipe Clean, InitiallyClean : act Clean(n).show detail Section 7, general conformant planning problemseen satisfiability problem formF : (act init ) (prec goal ),AIpredicates represent actions, initial fluents F fluents.definition act defines fluents change terms action, init firstorder formula initial situation, prec describes preconditions actionsgoal goal. motivates extension approximation method formulasincluding definitions. However, analyze general case definitions mayappear arbitrary locations formula, instead restrict attention formulasformP Q : ( 1 ) 2 ,definition Def () Q 1 2 FO formulas. Eventhough restrictions strictly necessary, allow us keep technicaldetails relatively simple (in particular, avoid need approximation rules inferdefinition whole certainly true/false), still covering waydefinitions typically used: assumption predicates indeeddefinition formula 1 say be, 2 states propertiessatisfy.extend approximative method ( 1 ) 2 satisfiability problems,need syntactic representation (i.e., approximative definition) describes soundinferences made definition three-valued context. sectionpropose two ways obtain approximative definition, accordingly, two waysapproximate ( 1 ) 2 satisfiability problems. continue, firstneed recall preliminaries.5.1 Preliminaries Well-founded Semantics Inductive DefinitionsEarlier, defined model positive inductive definition given two-valued interpretation open predicates. on, inductive definitions longerpositive definitions, model definition longer always computedleast fixpoint immediate consequence operator introduced Section 2. Moreover,98fiAn Approximative Inference Methodfollows want use inductive definitions together four-valued informationopen predicates (for example, information obtained propagationfirst order theory). Therefore, recall (see, e.g., Denecker & Ternovska, 2008)define well-founded model non-monotone inductive definition (that is, negationbody rules allowed), given four-valued information open predicates, denote W F MO (). order this, first need defineadditional concepts. Recall P denotes body unique rule predicate Phead.Definition 5.1 (Operator ). definition given (potentially 4-valued)Open()-interpretation O, define operator 4-valued Def ()-interpretationsdomain Open() (I) = J iff defined predicate P/nn-tuple Dn , holds= O+I [d]P J (d)PRecall preliminaries defined isomorphism maps pairinterpretations (I, J) corresponding four-valued interpretation I.Definition 5.2 (W F MO ()). define well-founded model 4-valuedO,interpretation (I, J) (I, J) maximal oscillation pair operator STST operator J(lf p(K(T (K, J)1 )). I.e., (I, J) least precise pair2-valued interpretations= ST(J) J = ST(I).explanation order. First look operator K(TO (K, J)1 ). operator takes Def ()-structure K, turns 4-valued one combining J,applies operator , projects result first argument. seeK(TO (K, J)1 ) = L iff defined predicate P/n n-tuple Dn , holds= iff (P [d])+(O1 +L)(O2 +J) =P L (d)words, positive occurrences atoms evaluated O1 + L, negative occurrencesO2 + J. J, operator K(TO (K, J)1 ) monotone, thereforemaps J least fixpoint. provenleast fixpoint. operator SToperator antimonotone, therefore maximal oscillation pair. definitionnonof well-founded model maximal oscillation pair operator STconstructive definition. maximal oscillation pair constructed iteratingfollowing operator, starting least precise interpretation extends O,reaches least fixpoint.Definition 5.3 (Stable operator ST). define operator ST pairs interpretations as:ST(I, J) = (ST (J), ST (I)).stable operator monotone w.r.t. precision order p pairs interpretationsfixpoints therefore form complete lattice. fixpoints operator called99fiVlaeminck, Vennekens, Denecker & Bruynooghefour-valued stable fixpoints , least precise fixpoints preciselywell-founded model given O.define semantics inductive definitions general case.reader easily verify indeed generalizes definition odO () positivedefinitions gave Section 2.2.Definition 5.4 (Satisfaction relation definitions). |= iff (I|Def () , I|Def () )well-founded model I|Open() .Note definition three-valued well-founded model every possibleinterpretation open predicates Open(), definition model (i.e.exists interpretation |= ). call definition totaltwo-valued well-founded model every possible two-valued interpretation openpredicates.definitions generalize rather obvious way standard well-foundedsemantics propositional logic programs strongly linked stable semantics (Gelfond & Lifschitz, 1988). case propositional logic program ,Open() = {}, operator K(T (K, J)1 ) nothing else immediate consequence operator TJ Gelfond Lifschitz reduct J , operator mapsJ lf p(K(T (K, J)1 ) stable operator . shown Van Gelder (1993),maximal oscillation pair indeed well-founded model .5.2 Approximating SO(ID)-Satisfiability ProblemsAssume formula , FO(ID) formula instead FO.concepts witness (Definition 4.1) sound approximation (Definition 4.2) straightforwardly generalised FO(ID) formula. allows us develop twoapproaches. first one, Section 5.2.1, replaces definition completionapplies method Section 4. However, completion weakerdefinition. Therefore, Section 5.2.2, develop another approach constructsapproximation conjunction definition FO formula.5.2.1 Using Completion Definitionfirst approach based use completion (Clark, 1978). completiondefinition conjunction equivalences x P (x) P (x) predicatesP Def (), P (x) body unique rule P head. usefulproperty definition implies completion compl(). Moreover, nonrecursive, two actually equivalent. Replacing definition completion( 1 ) 2 obtain formula (comp() 1 ) 2 . every modelmodel comp(), every model (comp() 1 ) 2 model ( 1 ) 2every witness (compl() 1 ) 2 satisfiability problem witness( 1 ) 2 satisfiability problem. Hence use results Section 4formulate following proposition.Proposition 5.1. formula APPBU ((compl() 1 ) 2 ) sound approximationP Q( 1 ) 2 .100fiAn Approximative Inference Methoddisadvantage using completion matter complete approximation method defined Definition 4.4 is, never able infer somethingfollows compl(). instance, inductive definition {P P }entails P , completion P P not.Denecker Ternovska (2008) proven that, addition non-recursive definitions, class recursive definitions equivalent completion. particular,case definitions strict well-founded order 2 . therefore replacedefinitions completion without losing precision. theory Tact Example1.1 actually completion definition act . Since act recursive definitionstrict well-founded order (we make use time argument predicatesconstruct well-founded order), act Tact equivalent.Gaspipe conformant planning problem (Son et al., 2005), hand, usesdynamic domain completion suffice. Summarized, objectiveconformant planning problem start flame burner connectedgas tank pipe line. pipe line consists sections connectedvalves. valve opened gas one side other, gasspread far possible. formalized inductive definitionreachability relation pipe line:()x, Gas(x, t) Gas(y, t) v Connected(x, y, v) Open(v, t).x, Gas(x, t) ank(x).reachability definitions equivalent completion. Therefore, approximative method presented subsection work. problem completioncase correctly minimize defined predicates presencerecursion, would allow models loop pipe line filled gas evenconnected tank. missing, therefore, unfounded set reasoning allows well-founded semantics correctly minimize defined predicates.5.2.2 Using Certainly True/Possibly True Approximationapproximative definition Approx () used Section 4 nice propertydefines, subformula (including itself), whether certainly true certainlyfalse. property allowed us find witnesses simply asserting Acthold according definition. want apply method formulascontain definition , construct approximative definition defineswhether subformulas (including itself) certainly true certainlyfalse. Section 5.2.1, naive method managed simply replacingcompletion. want improve method constructing approximationalso takes account unfounded set reasoning performed well-foundedsemantics.also take aspect well-founded semantics account, however,becomes difficult define definition whole certainly true certainlyfalse. Luckily, needed stick assumption definitions appearantecedent implication . Indeed, approximate implications2. order < well-founded infinite descending chains . . . < xn < xn1 < . . . < x1101fiVlaeminck, Vennekens, Denecker & Bruynoogheassuming antecedent certainly true (Definition 4.4), really needapproximation consequences definition. end, transformoriginal definition approximative definition 0 well-foundedmodel 0 , given approximation open predicates , approximateswell-founded models given interpretation open predicatesapproximated O. words, construct approximative definition 0 whosetwo-valued well-founded model encodes potentially four-valued well-founded modeloriginal definition , given potentially four-valued interpretation predicates. therefore represent four-valued interpretation orginal vocabularytwo-valued interpretation larger vocabulary 0 . However, instead introducing,predicate P , predicate P ct (P certainly true) P cf (P certainly false),before, introduce predicates P ct P pt (P possibly true, i.e., Pcertainly false). Let ct/pt denote vocabulary F {P ct | P } {P pt | P }.four-valued -interpretation I, define corresponding ct/pt -interpretation ct/ptct/ptinterpretation pre-interpretation (P ct )I= {d |ct/ptptP (d) p t} (P )= {d | (P (d) p t)}.Also, -formula , define formula ct/pt formula obtainreplacing positive occurrences predicate P P ct , negative occurrencesP pt , finally reducing negational normal form. easy see ct/pt alsoobtained ct replacing, every predicate P , occurrences P cf P pt .Unlike ct , ct/pt always positive formula contain negations. particular,P ct occurs positively P pt occurs negatively. subvocabulary ,ct/ptdenotes ct/pt P ct P pt replaced P every predicateP . Again, follows use denote predicates needapproximated two-valued information them.ct/ptct/ptDefinition 5.5 (App ()). definition , define App{Rct Rpt } Rct consists rules() definitionx(P ct (x) ct/pt)Rpt consists rulesx(P pt (x) ()ct/pt)every definitional rule x (P (x) ) .assume rest paragraph without loss generality empty,ct/ptdrop notation App .Example 5.1. Consider following inductive definition.B BAssume = {}.Appct/ptctBct=ptBpt102B ct AptDptB pt ActDct.fiAn Approximative Inference Methodsee three-valued interpretation {D = u}, translates {Dct = f , Dpt =t}, approximative definition correctly infer B ct false B pt true.take {D = f } interpretation open predicate D, see approximativedefinition correctly infers B ct B pt false. example unfoundedset reasoning: known true, approximation detects B couldderived B therefore must false. kind reasoning could doneprevious, completion-based approximation method, since sound w.r.t.semantics definition itself, w.r.t. weaker completion.example also demonstrates use vocabulary ct/pt insteadct/cf , since latter would yielded definition:ctBB ct AcfctDcf.B cf B cf ActcfDct{D = f }, definition would fail infer B cf . Intuitively, reasonunfounded set reasoning well-founded semantics tries minimize extensiondefined predicates making many atoms false possible. Using ct/cfvocabulary, well-founded semantics approximating definition therefore attemptspossible, actually corresponds maximizingfalsify many atoms P cf (d)possible extension original predicates P , instead minimizing well-foundedsemantics original definition does.two-valued interpretation double vocabulary ct/pt corresponds fourvalued interpretation original vocabulary . want establish linkct/ptwell-founded model original definition App . complict/ptcating factor that, example shows, definition Applongermonotone therefore longer guaranteed two-valued well-founded model.three-valued interpretation ct/pt longer corresponds even four-valuedinterpretation original vocabulary , prove correspondencect/ptwell-founded model Apptwo-valued.Theorem 5.2. Let four-valued interpretation open predicates definition. Appct/pt () two-valued well-founded model given Oct/ptunique four-valued stable fixpoint given O. Moreover, Appct/pt () two-valued wellfounded model I, unique four-valued stable fixpoint unique interpretationct/pt = I.Proof. See Appendix D.theorem requires unique four-valued stable model four-valuedinput interpretation O. stronger requirement common conditiontotality, requires definition two-valued well-founded model giventwo-valued input interpretation O. following example shows, stronger conditionindeed necessary.103fiVlaeminck, Vennekens, Denecker & BruynoogheExample 5.2. Consider following definition:B.B C.C O.definition total, because, two-valued interpretation open predicateO, ({A}, {A}) two-valued well-founded model. However, three-valuedinterpretation ({}, {O}) (i.e., unknown) open predicate O, three-valuedwell-founded model ({}, {O, A, B, C}) unique three-valued stable fixpoint, since({A}, {O, A, C}) also fixpoint. indeed, findctB ctC ctct/ptApp() =AptB ptptCB pt .Apt C ct .ctpt.B ct .ctptC .ptct.two-valued well-founded model given {Opt }. easiest way seefill fact know Opt Oct f propagate information:ctB pt .ctptctB C .C ct f t.Apt B ct .ptctptBC.ptC f .ctB pt .ctptB f .ctB pt .ctB pt .Apt B ct .ptctBt.ptCApt f .ptctB.ptCAptptctB.ptCSo, left loop negation, means Act B pt remain unknownthree-valued well-founded model ({Apt , C pt }, {Act , Apt , C pt , B pt }) definition.computing three-valued well-founded model given O, approximative definition Appct/pt () produce precise results approximation compl();particular detect atoms unfounded set must false, illustratedExample 5.2 above. use Appct/pt () approximation () 2 -problem,still need show combined approximation Approx ()produce sound approximation . this, need combine one definitionct/cf-predicates another definition ct/pt-predicates. achieve first mergingtwo definitions adding rules copy information one vocabularyother.104fiAn Approximative Inference MethodDefinition 5.6 (D ). Given vocabulary , subvocabulary , inductivedefinition first-order formula . define following inductive definition.ct/ptApp{Ocf()Approx () {Actt}{Opt Ocf | every predicate Open() \ }{P cf P pt | every predicate P Def () \ }ctf , f | every predicate Open() \ occur }definition indeed contains rules approximation rulesapproximation , is, Appct/pt () Approx() {Actt}, respectively,also number extra rules make connection two approximations.approximate defined predicate Q approximation uses pair predicatesQct Qpt approximation uses Qct Qcf . Hence, number extrarules needed transfer information predicates Qpt Qcf . rules{Opt Ocf } transfer information approximation derivedtruth open predicate (by means Ocf ) corresponding predicate Optapproximation definition. rules {P cf P pt } turn propagate informationderived truth defined predicate approximation definitioncorresponding predicate P cf approximation . Finally, rules {Ocf f , Octf } make sure Ocf Oct defined atoms (instead open ones)default value u. following proposition relates well founded modelmodels .Proposition 5.3. Given vocabulary , subvocabulary FO(ID) formula .(resp. P cf (d)),holdsThen, every -interpretation I, W F MI (D ) |= P ct (d)(resp. |= P (d)).every model extending |= P (d)Proof. See Appendix E.proposition analogue inductive definitions result Theorem 3.2states FO formulas. One difference two results proposition always assumes definition holds, Theorem 3.2 makesassumption FO formula approximated. discussed beginningsection, restriction problem, way approximateimplications allow definitions appear antecedent. seconddifference Theorem 3.2 applies arbitrary subformulas ,is, however, easy corrolary propositionproposition considers atoms P (d).result fact holds formula contains predicates defined(or cf (d)),every model, i.e., whenever W F MI (D ) |= ct (d)(resp. |= (d)).extending I, holds |= (d)introduced approximation Appct/pt () aim completecompletion-based approximation. long single definition considered105fiVlaeminck, Vennekens, Denecker & Bruynoogheisolation, succeeded goal. However, also incorporatedadditional formula , longer case. instance, consider following FO(ID)theory:Q P Q.Here, approximation cannot derive P certainly true, simplyct/pt-approximation Appct/pt () contain rules head-to-body propagation, i.e.,rules infer something body definitional rule, given informationhead. contrast, approximation completion contain rulestherefore problems reaching conclusion. motivates us useuse D(compl()) instead. definition implies completion,sound.obtain sound approximation SO(ID) formula P Q = () 2 , need plug approximation D(compl())suitable SO(ID) formula, similar one defined Definition 4.4formula P Q 1 2 . small complication, however, that, discussed previously,cfapproximation definition define predicates Acttell usdefinition whole certainly true certainly false. Therefore, longeruse normal approximation entire implication certainly true.present approximation SO(ID), let us first introduce reformulationoriginal approximation SO, avoids use Act.Proposition 5.4. formula F = P Q : , form 1 2 ,approximation defined Definition 4.4, i.e., formulactP R : (Approx (1 2 ) {Act1 t})equivalentcfctP R : (Approx (1 ) Appox (2 ) {Act1 t}) (A1 A2 )Proof. obvious fact difference Approx (1 2 )Approx (1 ) Appox (2 ) precisely set rules ensure ActequivalentctAcf1 A2 .approximation SO(ID) essentially consists replacing Approx (1 )compl()cf{ActAcfform.1 t} A1 respectivelyProposition 5.5. Given SO(ID) formula F = P Q ( ) 2 . defineAPP wf (F ) following SO(ID) formula.ctP R : (Dcompl() Approx (2 )) (AcfA2 ).APP wf (F ) sound approximation F .compl()cases, approximatinginstead gain us anything. instance, consider P Q( ) 2 problem, contains openpredicates (as case conformant planning problems consider next106fiAn Approximative Inference Methodsections). case, never need head-to-body propagation, thereforecompl()complete, therefore better using former.case approximation method , well rulesdefinition APP wf necessary. Indeed, bottom-up rules Approx (2 )needed, unfolded single rule. Therefore, define twovariants Definition 5.5.Definition 5.7 (APP wfBU (F )). Given SO(ID) formula F = P Q ( ) 2 .wfdefine APP BU (F ) following SO(ID) formula,cfctP R : (Dcompl() ApproxBU(2 )) (A A2 ),define APP wfBU,U nf (F ) following SO(ID) formula,ctcfctP R : (Dcompl() {Act2 (2 ) }) (A A2 ).6. Experimental Evaluationpaper seen number methods approximate SO(ID)satisfiability problems. subsection, explore, number experiments,use methods solve practically useful problems fast possible.performed experiments number conformant planning benchmarkspaper Son et al. (2005). show Section 7, benchmarksform F = , stratified definition, therefore equivalentcompletion. Therefore, F equivalent formula compl() ,denote F cp . experiments run dual core 2.4 Ghz CPU, 2.8 Gb RAMLinux machine, using IDP model expansion system FO(ID) (Marien et al., 2006).time-out twenty minutes used.first question want answer whether definitions, completionbased approximation faster ct/pt approximation. hard see that, eventhough Approx(compl()) linear size parse tree compl(), definitionmay contain rules Appct/pt (), moreover, rules may contain lotrecursion. pose challenge current solvers, suggests likelyefficient use ct/pt approximation definitions. first column Table 2shows times using completion definition , is, APP (F cp ),second column, ct/pt-approximation used, is, APP wf (F ). expected,ct/pt-approximation consistently faster.Table 2 also compares solving times full completion-based approximative definitioncp(in first column) approximation APPBU (F ) (Def. 4.5), topdown propagation rules removed (third column). see BTBTC benchmarks get order magnitude improvement. fourth columncpTable 2 shows timings unfolded approximation 2 , APPBU,U nf (F ),intermediate Tseitin predicates removed (Def. 4.6). see unfoldingconsistently provides speed-up.results suggest combining techniques, is, using ct/ptapproximation unfolding bottom-up approximation 2 together,107fiVlaeminck, Vennekens, Denecker & BruynoogheAPP (F cp )APP wf (F )cpAPPBU (F )cpAPPBU,U nf (F )APP wfBU,U nf (F )BT(2,2)BT(4,2)BT(6,2)BT(8,4)BT(10,4)0,1513,40438,93-0,1093,49314,76-0,1150,3120,87632,91-0,0650,1530,4091,774-0,0310,0640,1130,4621,643BTC(2,2)BTC(4,2)BTC(6,2)BTC(8,4)0,210-0,131-0,17140,081-0,1168,408-0,0370,1090,33541,8940,3901,1016,59731,275-0,0260,0360,0670,1200,2310,5071,2508,99542,583-0,4731,2666,99728,387-0,0490,0520,1280,3961,5307,023-0,374-5,217-2,792-0,1000,3586,650193,2902485ProblemDomino(100)Domino(200)Domino(500)Domino(1000)Domino(2000)Ring(2)Ring(4)Ring(6)Ring(8)Ring(10)Table 2: first column gives name benchmark ones differentexecution times. second column gives execution time approximationcompletion third cp/pt approximation. fourth fifth column usevariants completion approximation. fourth column, top-down rules2 removed addition, fifth column, remaining bottom-up rulesunfolded. last column combines cp/pt approximation changes. -means execution interrupted 20 minutes.give us fastest way approximating ( 1 ) 2 satisfiability problems. Indeed,formula APP wfBU,U nf (F ) (Definition 5.7) does, results methodshown last column Table 2. expected, far fastest method.7. Applications Related Workliterature, many examples found approaches perform kindapproximate reasoning models logical theory. Often, approaches,specific problem hand, seem boil instantiation generalmethods presented here. section give examples.7.1 Conformant Planninggeneral, conformant planning problem planning problem non-deterministicdomain initial state may fully known. goal comeplan (i.e., sequence actions) nevertheless guaranteed work. hardproblem: decision problem deciding whether conformant plan fixed lengthk exists P2 -complete3 (Baral, Kreinovich, & Trejo, 2000; Turner, 2002). Therefore, one3. planning domains executability actions given state cannot determined polynomially, even P3 (Turner, 2002)108fiAn Approximative Inference Methodtypically attempts solve approximately. section, show applyapproximative methods solve conformant planning problems.Example 7.1. Let us consider Clogged Bombs Toilet domain (McDermott, 1987;Son et al., 2005). number packages toilet. packages maycontain bomb disarmed dunking package toilet. Dunkingpackage toilet also clogs toilet cannot throw package clogged toilet.Flushing toilet unclogs it. effects actions fluents modeledfollowing definition act , preconditions conjunction prec sentencesTprec .actClogged(0) Init Clogged.Clogged(t + 1) p : Dunk(p, t) (Clogged(t) F lush(t)).=Armed(p, 0) Init Armed(p).Armed(p, + 1) Armed(p, t) Dunk(p, t).Tprecp : Dunk(p, t) Clogged(t).( p : Dunk(p, t) F lush(t)).=p p2 : Dunk(p, t) Dunk(p2 , t) p = p2 .p t2 : Dunk(p, t) Dunk(p, t2 ) = t2 .consider following regular planning problem: given completely specified initialsituation (specified formula init ), find plan packages disarmed.formulate problem following formula:A, F , : act prec init (t p Armed(p, t)),A, denote action predicates {Dunk/2, F lush/1}, F denotefluent predicates {Armed/2, Clogged/1} denote predicates useddescribe initial situation {Init Clogged/0, Init Armed/1}. imagine initialsituation specified, want find plan works possible initialsituations, words conformant plan. formulate problem findingplan follows.F , : act (prec p Armed(p, t)).formalized general follows.Definition 7.1 (Conformant planning). Let vocabulary, consisting setpredicates A, denoting actions, I, denoting initial fluents, F denoting fluents. LetTact FO(ID) theory Tinit , Tprec Tgoal FO theories, , Tactspecifies values fluents given interpretation actions initial fluents,Tinit theory specifying initial situation, Tprec contains preconditions actions,Tgoal specifies goal planning problem. act denote conjunctionsentences possibly definitions Tact similarly theories.problem conformant planning decide satisfiability following formula:F : (act init ) (prec goal ).109(6)fiVlaeminck, Vennekens, Denecker & BruynoogheAR :Cloggedct (0)ct (t + 1)CloggedArmedct (p, 0)Armedct (p, + 1)Cloggedpt (0)Cloggedpt (t + 1)Armedpt (p, 0)Armedpt (p, + 1)Init CloggedctInitArmedct (p)Init CloggedptInit CloggedcfInitArmedpt (p)ArmedcfInitcf (t)Cloggedcf (p, t)ArmedAct2Init Cloggedct .p : Dunk(p, t) (Cloggedct (t) F lush(t)).Init Armedct (p).Armed(p, t) Dunk(p, t).Init Cloggedpt .p : Dunk(p, t) (Cloggedpt (t) F lush(t)).Init Armedpt (p).Armed(p, t) Dunk(p, t).f.f.Init Cloggedcf .f.Init Armedcf (p).f.Cloggedpt (t).Armedpt (p, t).pt : Dunk(p, t) Cloggedcf (t)(pt : Dunk(p, t) F lush(t))p1 p2 : Dunk(p, t) Dunk(p2 , t) p1 = p2pt1 t2 : Dunk(p, t1 ) Dunk(p, t2 ) t1 = t2tp : Armedcf (p, t).Act2 .Figure 4: complete approximation Clogged Bombs Toilet example.words, must plan (A), matter nondeterministicF ), long specification effects actions (act )aspects turn (I,(partial) specification initial situation (init ) obeyed, planexecutable (prec ) achieve goal (goal ).Formula 6 exactly form assumed above, thus use onemethods approximate conformant planning problems.Example 7.1. (continued) Continuing Clogged Bombs Toilet example,using ct/pt-approximation definition, unfolding constraint (precp Armed(p, t))ct , get approximating formula APP wfBU,U nf (Definition 5.7),shown Figure 4, R ct- cf-predicates introduced approximationmethod.result applying general approximation method conformant planningproblem, specified Tact , Tprec , Tgoal Tinit above, similar approximation AL action theory logic program work Son et al. (2005).However, small differences details make difficult formallycompare two. Nevertheless, experiments discussed section, methodalways finds correct solution (unless times out), method Son et al.Moreover, two approaches also found solutions comparable execution times.detail, Table 3 presents following results. implemented conformantplanner iteratively calling IDP model generator FO(ID) (Marien et al., 2006)approximation, giving increasing number timesteps either plan110fiAn Approximative Inference MethodProblemIDPSmodelsCmodelsBT(2,2)BT(4,2)BT(6,2)BT(8,4)BT(10,4)0.4380.5131.0501.552.800.1990.2190.58730.9-0.1450.2120.4252.395.80BTC(2,2)BTC(4,2)BTC(6,2)BTC(8,4)0.2730.8441.6043.70.1360.4123.88-0.1390.3891.23102Cleaner(2,2)Cleaner(2,5)Cleaner(2,10)Cleaner(4,2)Cleaner(4,5)Cleaner(4,10)Cleaner(6,2)Cleaner(6,5)0.6441.571.5524608.30-0.22672.513.8-0.3761.361.136.16-Domino(100)Domino(200)Domino(500)Domino(1000)Domino(2000)0.1760.1810.2120.2360.3390.0960.1140.3240.6181.220.0900.1510.3540.6601.320.6551.567.3515715370.2852.09219.1-0.2960.9373.54219.860232Ring(2)Ring(4)Ring(6)Ring(8)Ring(10)Table 3: Comparison IDP vs Cmodels vs Smodelsfound maximum number timesteps reached. compared plannerCPASP conformant planner (Son et al., 2005), using experimental setupSection 6. CPASP takes action theory action language AL, encodesapproximation transition diagram corresponding action theory, meansanswer set program. answer set solver used find conformant plans.Son et al., used ASP solver behind CPASP CModels (E. Giunchiglia &Maratea, 2011) SModels (Niemela, Simons, & Syrjanen, 2000). Table 3 shows,combination approximation IDP system comparable to, overall slightlyworse, combination CModels Son et al.s approximation. comparedapproximation given SModels, method tends bit better.results line results ASP competition (Denecker et al., 2009) concerningperformance SModels, CModels IDP general, suggesting that, conformantplanning, approximation Son et al. comparable quality.Another approximative method solving conformant planning problems foundwork Palacios Geffner (2009). paper, authors consider conformantplanning problems, specified language Strips extended conditional effectsnegation. define transformation K0 transforms conformant planningproblem classical planning problem sound incomplete way. fluentliteral L conformant planning specification, two new literals KL KL created,111fiVlaeminck, Vennekens, Denecker & Bruynooghedenoting L known true, resp. known true, initial situation,action preconditions effects translated initial situation, preconditionseffects reference new knowledge literals. hard verifyapproximation method generalizes transformation: take encodingconformant planning problem P , approximation obtained methodinterpreted classical planning problem ct/cf vocabulary. planning problemexactly planning problem specified K0 (P ) (i.e., action preconditionseffects correspond), apart initial situation. K0 transformationpropagation knowledge initial situation: given initial situation (specifiedset clauses), K0 (I) consists literals KL L unit clauseI. means that, e.g., initial situation = {P Q, P }, K0 (I) includeliteral KQ, method able infer Qct holds (which meansapproximation method complete K0 transformation).general method, allow solving conformant planning problems, also allows approximating number related problems temporal domains.Consider, example, following problem: Given certain action happenstimepoint t, certainly lead property true ? formalizedfollowing satisfiability problem, method applies again.AIF : ((act init prec A(t)) ).formula true possible plans A(t) happens, property holds.variant problem so-called projection problem: Given exactly knowactions happened (we thus assume preconditions satisfied),property hold ? order formulate problem satisfiability problem,need express actions happened. done, example,using inductive definition . projection problem expressedAIF : ((act init ) ) satisfiability problem. Another variant followingproblem: property 1 holds certain plan, property 2 also hold?,expressed AIF ((act init prec 1 ) 2 ) satisfiability problem.7.2 Querying Reasoning Open DatabasesApproximate methods similar used context databases withoutcomplete information, particular databases without CWA, open databases(Baral et al., 1998; Liu & Levesque, 1998) databases make forms local closedworld assumptions (Denecker et al., 2010; Doherty et al., 2006). papersgoal compute certain possible answers queries. taskhigh complexity (from CoNP locally closed database without integrity constraintspossibly P2 databases first-order constraints - assuming given finite domain),approximate methods presented translate FO query approximate FOFO(FP)4 query solved directly database tables using standard(polynomial) query methods.method presented paper provide similar functionality. Let DBset ground literals, representing incomplete database. Let background theory:4. FO(FP) extension FO least greatest fixpoint constructs.112fiAn Approximative Inference Methodmay contain integrity constraints, view definitions (datalog view programs specialcase FO(ID) definitions), local closed world statements expressed FO, etc. givenholds Herbrand modelsFO query Q [x], goal find tuples Q [d]DB . problem deciding whether given tuple answer correspondssatisfiability problem formulaR(DB Q [d]),(7)directly use approximation method problem. allows usanswer yes/no queries well decide whether given tuple certain answerquery, approximation method directly provide method compute (anapproximation of) tuples.However, let us look following satisfiability problem.R0 : DDB ApproxBU (Q [x]),looks much like approximation ( 1 ) 2 satisfiability problems (asformulated Proposition 5.5). definition DDB approximatingdatabase DB background knowledge (note possibly contains definitions),bottom evaluation query, constraint ActQ dropped.definition DDB ApproxBU (Q [x]) consists rules describing propagations allowed database theory , rules defining predicate symbol ActQ ,AQ Tseitin predicate representing query Q [x]. unique Herbrand modeldefinition, interpretation ActQ contains tuples propagationderive certainly satisfy query sound approximation full setanswers!work Denecker et al. (2010), locally closed database LCDB assumed.locally closed database consists standard database DB, i.e. set atoms, togetherset local closed world assumptions LCWA(P (x), [x]). LCWAstatements expresses databases knowledge P complete tuples xtherefore true DB falsesatisfy formula [x]. atom P (d)holds domainDB LCWA(P (x), [x]) [d]discourse; otherwise unknown. authors present approximate reasoningmethod query answering locally closed databases show approximatequery answering formulated fixpoint query. Basically, boilsfollowing. One constructs following definition...P ct (x) P (x),LCWA =P cf (x) P ct (x) ctP [x]...every relation P every local closed world assumption LCWA(P (x), [x]). Althoughauthors phrase form, method finding approximationcertain answers query Q [x] actually boils solving following satisfiabilityproblem:R0 : DB CW A(DB) LCWA ApproxBU (Q [x]),113fiVlaeminck, Vennekens, Denecker & BruynoogheR0 denotes predicates auxiliary predicates occurring body existential formula. CW A(DB), denote formula expressing closed world assumptiondatabase DB. presence closed world assumption might seem strangefirst sight, since whole idea behind locally closed world databases assume CWAper default. However, order correctly apply local closed world assumptions,need exact specification database not, preciselyexpressed DB CW A(DB). Indeed, given DB CW A(DB), LCWAseen approximative definition certainly true false contextlocally closed world assumptions. predicate ActQ contain approximateanswer query Q [x], i.e., lower bound tuples query Q [x]certainly true. Similarly, predicate AcfQ contain lower bound tuplesquery false.limitation approach Denecker et al. extend methodone type integrity constraints, namely functional dependencies. wayfunctional dependencies handled extending LCWA extra propagation rulestaking functional dependencies account. contrast, general methodused easily extend arbitrary integrity constraints. works follows.Let Tint set first-order integrity constraints. approximate problemfinding certain queries following satisfiability problem.BUct t} DB CW A(DB) LCWA Approx(Q [t]).R0 : Approx(Tint ) {ATintAgain, predicate ActQ contain approximate answer query Q [x].Doherty et al. (2006), propose yet another approach asking queries incompletedatabase. authors use term approximate database denote database, consistingtwo layers: extentional intensional layer. layers externalrepresentation towards user, internal representation.extentional database consists positive negative literals, internallystored classical database, using Feferman transformation. example, extentional database (EDB), entered user,Color(Car1, Black), Color(Car1, Red), Color(Car2, Red),internally storedColorct (Car1, Black), Colorcf (Car1, Red), Colorct (Car2, Red).intentional database consists rules infer additional information factsEDB. user write rules form ()P1 (x1 ). . .()Pn (xn ) ()P (x)),internally stored (()P1 (x1 ))ct . . . (()Pn (xn ))ct (()P (x)))ct .example IDB rule following ruleColor(x, y1 ) y1 6= y2 Color(x, y2 ),internally storedColorct (x, y1 ) y1 6= y2 Colorcf (x, y2 ).114fiAn Approximative Inference Methodevaluate query, naive algorithm based exhaustively applying rules EDBused.rules IDB resemble F formulas sense describe validinferences made based incomplete information. internal representationIDB indeed similar representation F formulas definitional rules.However, key difference approach Doherty et al., user wants addproperty database (e.g., car one color), writeinferences valid according property, approach inferencerules automatically generated property itself. Manually writing validinferences sanctioned property easy task. example, take propertycar inspected suspect black paper Dohertyet al.. expressed FO formula = c(Suspect(c) Color(c, Black)Investigate(c)). While, method, Approx() constructs approximation validinferences made formula, user write followingrules Doherty et al.s approach:Suspect(c) Color(c, Black) Investigate(c)Suspect(c) Investigate(c) Color(c, Black)Suspect(c) Investigate(c)...method therefore generalizes work Doherty et al. deriving rules automatically general first-order theory.Liu Levesque (1998) propose another type reasoning open databases.consider simple form first order knowledge bases, called proper knowledge bases.interesting feature knowledge bases easy obtain completecharacterization certainly true, resp. certainly false. terminology,|= P ct (d)means one construct definition , KB |= P (d)cfKB |= P (d) |= P . holds every two valued extensionthree valued interpretation encoded model KB. Levesque et al. useevaluation procedure based three-valued Kleene-evaluation check whetherquery holds knowledge base. mentioned earlier, also define normal formN F queries, prove Kleene-evaluation complete. workextends work, sense take general first order knowledge baseapproximately solve queries, shown above. course, since generallonger guarantee complete characterization certainly true/false,longer guarantee completeness, even query normal form N F. Anotherdifference work Liu Levesque work here, assumefixed countable infinite domain, assume fixed finite domain. indeedtheoretical difference, practice make difference, since evaluationmethod considers finite set domain elements determined up-front.8. Conclusions Future WorkEven problem computationally hard general, specific instances might stillsolved efficiently. approximate methods important: cannot solve115fiVlaeminck, Vennekens, Denecker & Bruynoogheevery instance, instances solve, solve quickly. computationallogic, hard problems arise quite readily. therefore surprising literaturecontains numerous examples algorithms perform approximate reasoning tasksvarious logical formalisms various specific contexts. Since many algorithms sharecommon ideas, natural question whether seen instancesgeneral method general language.paper presents method. start propagation method FO()developed Wittocx, Marien, Denecker (2008) symbolic expression (Wittocx,2010) generalize method approximating P2 -complete SO(ID) satisfiability problem solving NP problem. Importantly, syntactic methodtransforms SO(ID) formula SO(ID) formula. affords us freedomuse off-the-shelf solver language perform approximative reasoning.Moreover, also makes significantly easier update method adding (or removing)specific propagations.Since method approximation, necessarily incomplete. Nevertheless,experiments shown that, practice, often manage find solution.interesting topic future work determine classes problems, methodshown complete.summary, contributions paper (1) extended logicalrepresentation describing propagation process general method approximatingSAT (SO) problems; (2) shown approximate inductive definitions, useapproximate class useful SAT (SO(ID))-problems; (3) examinedexisting approximation methods fit general framework.Acknowledgmentswork supported Research Foundation - Flanders FWO-Vlaanderen, projectsG.0489.10 G.035712, Research Fund KULeuven, project GOA/08/008. HanneVlaeminck supported IWT-Vlaanderen.Appendix A. Example ApproximationFigure 5 shows full approximation act Example 1.1.Appendix B. Proof Theorem 3.1Proof. First, remark Feferman (1984) showed four-valued evaluationformula interpretation simulated computing standard two-valuedevaluation ct cf tf . easy verify bottom-up rules Approx()inductively encode evaluation. split proof two parts. First assumethree-valued. show case bottom-up rules used, i.e., leavingtop-down rules change model definition. proves firstpart theorem, together remark also proves second parttheorem case three-valued. Then, left prove,second part theorem also holds four-valued I.116fiAn Approximative Inference MethodActactAcfactAcfactAct0Act8Acf0Acf8Act0Acf0Act1 (t)Acf1 (t)Acf1 (t)Acf1 (t)Act1 (t)Act2 (t)Acf2 (t)Act5 (t)Acf5 (t)Act2 (t)Act2 (t)Acf2 (t)Cleanct (t + 1)Cleancf (t + 1)Acf4 (t)Act4 (t)Act4 (t)Act4 (t)Acf4 (t)Cleancf (t)Cleanct (t)Act5 (t)Act5 (t)Acf5 (t)Acf6 (t)Act6 (t)Cleancf ((t + 1))Cleanct ((t + 1))Acf6 (t)Acf6 (t)Act6 (t)Cleancf (t)Cleanct (t)Acf8Acf8Act8Act9Acf9Act11Acf11Act9(t : ActAct1 (t)).9(t : AcfAcf1 (t)).9ActCleanct (0)0.ctcf(Acf0 (t1 : (t1 = A1 (t1 )))). Clean (0)cfAcf(t).InitiallyClean2cfA5 (t).InitiallyCleanctctct(A2 (t) A5 (t)).Act11Act(t).Act111ct(AcfAcf1 (t) A5 (t)).11ctA1 (t).Acf12ct(AcfAct121 (t) A2 (t)).Cleancf ((t + 1)).Cleancf (0)Act(t).Cleanct (0)4cfct(Clean (t + 1)) A4 (t)).Act12cfA2 (t).Acf12cf(ActInitiallyCleancf2 (t) A4 (t)).cfA2 (t).InitiallyCleanctct(Act2 (t) Clean (t + 1)).Cleanct (t).W ipe(t).(Cleancf (t) W ipe(t)).Acf4 (t).(Act4 (t) W ipe(t)).Act6 (t).Cleanct (t + 1).cf(Acf6 (t) Clean (t + 1)).cfA5 (t).cf(Act5 (t) Clean (t + 1)).cfA5 (t).cf(Act5 (t) A6 (t)).ctClean (t).W ipe(t).(Cleancf (t) W ipe(t)).Act6 (t).(Acf6 (t) W ipe(t)).ctAct0 A8 .cfA0 .Acf8 .Actact .Actact .ctAcfact A8 .cfAact Act0.Acf9 .Acf11 .ct(Act9 A11 ).ctA8 .ct(Acf8 A11 ).ctA8 .ct(Acf8 A9 ).cfClean (0).InitiallyCleanct .(Cleanct (0) InitiallyCleancf ).Acf9 .cf(Act9 InitiallyClean ).Acf.9ct(Act9 Clean (0).ctA12 .Cleanct (0).cf(Acf12 Clean (0)).cfA11 .cf(Act11 Clean (0)).Acf.11cf(Act11 A12 ).InitiallyCleancf .InitiallyCleanct .Act12 .Acf12 .Figure 5: Approx{W ipe} (act ), act taken Example 1.1.let us assume three-valued. prove od(ApproxBU () ) =od(Approx() ) contradiction. Assume predicate Act(the proof goescfctanalogously ) od(Approx() ) |= od(ApproxBU ()) 6|= Act. preliminaries recalled model positive inductive definition. modelleast-fixpoint immediate consequence operator117fiVlaeminck, Vennekens, Denecker & Bruynooghedefinition thus limit sequence applications immediate consequenceoperator. One prove (see, e.g., Denecker & Ternovska, 2004)apply immediate consequence operator complete definition every step. I.e.,applying immediate consequence operator subset definition,longer exists immediate consequence operator give something new, givesmodel. Suppose take sequence first apply bottom-uprules, bottom-up rules applicable try apply top-down rules.Suppose Actfirst atom infer top rules sequence. ObviouslyActcannottop-level atom Act, since top-down rules this.case study type (sub)formula occurs in, e.g., assume subformulaformula = 0 . fact Acttrue, follows bodycfcfctcttop rule A0 true, thus ActA0 true.ctSince Actfirst atom inferred top-down rule, sincectctcttrue, also A0 must true. since became true last stepsequence, Act0 must true already. means applyingcfbottom-up rules Act0 A0 true, contradiction factthree-valued bottom-up rules encode four-valued evaluation.proof analogous types subformulas.case four-valued, three-valued longer casebottom-up rules contribute model (i.e., od(ApproxBU () ) 6=od(Approx() )). see this, consider following formula P Q, takefour-valued interpretation P = Q = t. one verifycfbottom-up rules Approx() infer ActP Q AP Q true.However, top-down rules also infer Qcf true. happensinconsistency inferred certain subformula, propagates backparse-tree. However, similar above, case study structurecfprove (for top formula ) od(ApproxBU () ) |= ActcfBU () clearly directod(Approx() ) |= Act. since since Approxencoding four-valued evaluation, concludes proof.Appendix C. Proof Proposition 4.3Proof. Take witness satisfiability APP (F ). First let us remarkOpen(Approx (1 2 ) {Act1 t}) = . fact witnesssatisfiability APP (F ) know model definition extending concftains Actconstruction Approx (1 2 ) must also contain either A1Act2 .Assume first Acf1 true . application Theorem 3.2 (where take= {1 } 0 = 1 ) gives: extends |= 1 , 6|= 1 , assumption|= 1 results contradiction hence 6|= 1 , case 1 2 holdsevery extending I, thus witness satisfiability F .Next, assume Act2 true . applying Theorem 3.2 (where time= {1 } 0 = 2 ) gives: extends |= 1 |= 2 , hence also118fiAn Approximative Inference Methodcase 1 2 hold every extending I, means witnesssatisfiability F .Appendix D. Proof Theorem 5.2key ingredient proof Theorem 5.2 following property Appct/pt ().Oct/ptimmediate consequence operator TAppct/pt () two-valued interpretations simulates(O ,O )immediate consequence operator 1 2 four-valued interpretations originaldefinition. made precise following lemma.Definition D.1. pair -interpretations (I, J), use t(I, J) denote ct/pt interpretation (I, J)ct/pt .Lemma D.1. (O1 , O2 ) (I, J),(O1 ,O2 )(O1 ,O2 )Proof. Let (I 0 , J 0 )t(O ,O )(I, J) = t1 (TApp1ct/pt2 () (t(I, J))).t(O ,O )(I, J) let F = TApp1ct/pt2 () (t(I, J)). first showF |ct = 0 . Since F |ct depends rules Appct/pt () predicate ct ,discard rules head pt . result, left single copypositive occurrences atoms replaced ct variant negativeones pt variant. implies evaluation bodies remainingrules according t(O1 I, O2 J) identical evaluation bodiesoriginal rules (O1 I, O2 J) construction 0 , thus proving equality.proof remaining equality F |pt = J 0 analoguous.Proof Theorem 5.2. First, recall that, given partial knowledge (O1 , O2 ), threevalued well-founded models , resp. Appct/pt () least fixpoints operators(O ,O )t(O ,O2 )ST 1 2 resp. ST App1ct/pt(note since t(O1 , O2 ) two-valued, abuse notation()rest proof denote two-valued pair (t(O1 , O2 ), t(O1 , O2 ))t(O1 , O2 )).Now, latter operator rather peculiar, sense actually juggling fourdifferent interpretations original alphabet . detail, elementdomain looks like this:IctJct(I, J) = , .IptJptIct Jct interpret alphabet ct , Ipt Jpt interpret pt . applyt(O ,O2 )operator ST App1ct/pt, obtain new pair:()00IctJct(I 0 , J 0 ) = , .00IptJpt119fiVlaeminck, Vennekens, Denecker & Bruynooghe00general definition STconstruction, obvious Ict Ipt dependsJct Jpt . However, particular case, operator exhibits even structure.(O ,O2 )operator STApp1ct/pt(J) uses argument J fixed interpretation negative()occurrences, remains constant throughout least fixpoint computationpositive occurrences. Now, Appct/pt () contains two copies interactnegative occurrences (that is, occurrences pt predicate body rulect predicate head always negative ones, vice versa). meanslong keep interpretation negative occurrence fixed constant value J,0 , discardtwo copies interact all. Consequently, construct Ictrules pt predicate head. means left rules whose headct predicate whose body contains positive occurrences ct predicates0 depends J . Moreover,negative occurrences pt predicates. Therefore, Ictpt0 map symbols back original alphabet (letvalue Ict0 ) = ST (O1 ,O2 ) (orig(J )). Similarly,orig( ct ) = orig( pt ) = ), orig(Ictptalso obtain that:(O1 ,O2 )(orig(Jct )),(O1 ,O2 )(orig(Ipt )),(O1 ,O2 )(orig(Ict )).0orig(Ipt) = ST0orig(Jct) = ST0orig(Jpt) = STwords,(O ,O2 )00(orig(Ict), orig(Jpt)) = ST 100(orig(Ipt), orig(Jct)) = ST(orig(Ict ), orig(Jpt )),(O1 ,O2 )(orig(Ipt ), orig(Jct )).Now, consider construction well-founded model Appct/pt (),sequence form:120012IctJctIctJctIctJctIctJct, 7 , 7 , 7 7 , .001122IptJptIptJptIptJptIptJptLet (I , J )i0 well-founded model construction original definition , i.e.,0 = fpredicates P/n Def () domain tuples Dn (P (d))J 0 = t, (I n+1 , J n+1 ) = ST (O1 ,O2 ) (I n , J n ) n 0. easy see(P (d))0 J 0 ) = (I , J ). provides base case, equation providest1 (Ict0 0ptJ ). words,inductive step prove that, i, (I , J ) = t1 (Ictptwell-founded model construction original definition tracked elementswell-founded model construction Appct/pt ():012IctIctIctIct, 7 , 7 , 7 7 , .012JptJptJptJptdiagonal? t1 (J0ct I0ct ) = (>, ),precise element >p lattice pairs interpretations. Therefore, find120fiAn Approximative Inference Method(O ,O )diagonal actually tracks construction greatest fixpoint ST 1 2 .Combining two results, see (L1 , L2 ) (G1 , G2 ) least greatestfixpoint, respectively, well-founded model Appct/pt () looks like this:L1G1, .G2L2Note unique four-valued stable fixpoint least greatest stable fixpointhence also well-founded fixpoint. immediately concludes proof.Appendix E. Proof Proposition 5.3Lemma E.1. Given -definition , FO-formula . Let subset 0 (arenamed copy ). Consider definition= Appct/pt () {P ct P 0ct }P {Opt O0pt }OOpen(0 ) .Assume three-valued interpretation approximates models .holds W F MI ct/pt (D) also approximates , i.e.,W F ct/pt (D), |= ( ) P (d),- P ct (d)6 W F ct/pt (D), |= ( ) P (d).- P pt (d)Proof. prove induction well-founded model construction alternativeone described paper, found work DeneckerVennekens (2007). Assume induction sequence (Ii )0in , four-valued Def (D)interpretations I0 interpretation everything completely unknown,= W F MI ct/pt (D). prove every Ii sound approximation .trivially case n = 0. assume Ik sound approximation. prove also case Ik+1 . need prove two thingscannot true Ik+1 P (d)true models ,this: first, atom P ct (d)ptcannot false Ik+1 false modelssecond, atom P (d). prove cases contradiction.start first case. assume k-th well-founded induction step,incorrectly deduced, i.e.,certain predicate P domain tuple D, P ct (d)inferred k-th step,exists model |= , s.t. 6|= P (d). Since P ct (d)ctis, (P )ct [d]already mademeans body rule defining P (d),true previous step. induction hypothesis tells us every modelsemantics inductive definitions saysholds |= P [d],also |= P , contradiction assumption.Next, consider second case. time, assume k-th well-foundedinferred false, exists model , s.t.induction step, P pt (d)|= P (d). Now, using alternative version well-founded semantics,two ways domain atom become false. Indeed, domain atom become false121fiVlaeminck, Vennekens, Denecker & Bruynooghebody defining rule already false, part unfoundedset.made false body defining rule (P )pt already false,P pt (d)argument completely analogous one - using induction hypothesis -gives contradiction assumption. now, left prove, P pt (d)ptcannot incorrectly made false application unfounded set rule. P (d)made false application unfounded set rule, meansset U atoms, unknown Ik , made false bodiesrules defining atoms, Kleene evaluation bodies returns false. possibleverify always find U contains pt -atoms.let us take model . obviously also model. Consider corresponding set U 0 , consisting domain atoms P (d),ptptptP (d) U S. every atom Q [d] body (P ) set U S, induction(Qpt [d])Ik . Similarly, every atom Qcthypothesis actually tells us (Q[d])ptctk (Q[d]). Now, since Qpt atoms occurbody (P ) , says (Q [d])positively Qct negatively (P )pt , follows interprets literalsU 0 false way Ik . Thus, U 0 also unfounded set (indeed,turning atoms U 0 false make bodies defining rules false),contradiction assumption, concludesthus |= P (d),proof lemma.Proof Proposition 5.3. proof proposition easy proof inductionconstruction well-founded model , using lemma above,soundness Approx().ReferencesBaral, C. (2003). Knowledge Representation, Reasoning, Declarative Problem Solving.Cambridge University Press, New York, NY, USA.Baral, C., Gelfond, M., & Kosheleva, O. (1998). Expanding queries incomplete databasesinterpolating general logic programs. J. Log. Program., 35 (3), 195230.Baral, C., Kreinovich, V., & Trejo, R. (2000). Computational complexity planningapproximate planning presence incompleteness. Artif. Intell., 122 (1-2), 241267.Belnap, N. D. (1977). useful four-valued logic. Dunn, J. M., & Epstein, G. (Eds.),Modern Uses Multiple-Valued Logic, pp. 837. Reidel, Dordrecht. Invited papersFifth International Symposium Multiple-Valued Logic, held IndianaUniversity, Bloomington, Indiana, May 13-16, 1975.Clark, K. L. (1978). Negation failure. Logic Data Bases, pp. 293322. PlenumPress.Denecker, M., Cortes Calabuig, A., Bruynooghe, M., & Arieli, O. (2010). Towards logicalreconstruction theory locally closed databases. ACM Transactions DatabaseSystems, 35 (3), 22:122:60.122fiAn Approximative Inference MethodDenecker, M., & Ternovska, E. (2004). logic non-monotone inductive definitionsmodularity properties. Lifschitz, V., & Niemela, I. (Eds.), LPNMR, Vol. 2923LNCS, pp. 4760. Springer.Denecker, M., & Ternovska, E. (2007). Inductive situation calculus. Artificial Intelligence,171 (5-6), 332360.Denecker, M., & Ternovska, E. (2008). logic nonmonotone inductive definitions. ACMTransactions Computational Logic (TOCL), 9 (2), Article 14.Denecker, M., & Vennekens, J. (2007). Well-founded semantics algebraic theorynon-monotone inductive definitions. Baral, C., Brewka, G., & Schlipf, J. S. (Eds.),LPNMR, Vol. 4483 LNCS, pp. 8496. Springer.Denecker, M., Vennekens, J., Bond, S., Gebser, M., & Truszczynski, M. (2009). secondAnswer Set Programming competition. Erdem, E., Lin, F., & Schaub, T. (Eds.),LPNMR, Vol. 5753 LNCS, pp. 637654. Springer.Doherty, P., Magnusson, M., & Szalas, A. (2006). Approximate databases: support toolapproximate reasoning. Journal Applied Non-Classical Logics, 16 (1-2), 87118.E. Giunchiglia, Y. L., & Maratea, M. (2011). Cmodels homepage. http://www.cs.utexas.edu/users/tag/cmodels.html.Fagin, R. (1974). Generalized first-order spectra polynomial-time recognizable sets.Complexity Computation, 7, 4374.Feferman, S. (1984). Toward useful type-free theories. Journal Symbolic Logic, 49 (1),75111.Gelfond, M., & Lifschitz, V. (1988). stable model semantics logic programming.Kowalski, R. A., & Bowen, K. A. (Eds.), ICLP/SLP, pp. 10701080. MIT Press.Immerman, N. (1998). Descriptive Complexity. Springer Verlag.Kleene, S. C. (1952). Introduction Metamathematics. Van Nostrand.Liu, Y., & Levesque, H. J. (1998). completeness result reasoning incompletefirst-order knowledge bases. KR, pp. 1423.Marien, M., Wittocx, J., & Denecker, M. (2006). IDP framework declarative problemsolving. Search Logic: Answer Set Programming SAT, pp. 1934.McDermott, D. (1987). critique pure reason. Computational Intelligence, 3, 151160.Mitchell, D. G., & Ternovska, E. (2005). framework representing solving NPsearch problems. Veloso, M. M., & Kambhampati, S. (Eds.), AAAI, pp. 430435.AAAI Press / MIT Press.Niemela, I., Simons, P., & Syrjanen, T. (2000). Smodels: system answer set programming. Proceedings 8th International Workshop Non-Monotonic Reasoning,Breckenridge, Colorado, USA. CoRR, cs.AI/0003033.Palacios, H., & Geffner, H. (2009). Compiling uncertainty away conformant planningproblems bounded width. Journal Artificial Intelligence Research (JAIR), 35,623675.123fiVlaeminck, Vennekens, Denecker & BruynoogheSon, T. C., Tu, P. H., Gelfond, M., & Morales, A. R. (2005). approximation actiontheories application conformant planning. Baral, C., Greco, G., Leone,N., & Terracina, G. (Eds.), LPNMR, Vol. 3662 LNCS, pp. 172184. Springer.Tamaki, H., & Sato, T. (1984). Unfold/fold transformations logic programs. ICLP,pp. 127138.Tseitin, G. S. (1968). complexity derivation propositional calculus. Slisenko,A. O. (Ed.), Studies Constructive Mathematics Mathematical Logic II, pp. 115125. Consultants Bureau, N.Y.Turner, H. (2002). Polynomial-length planning spans polynomial hierarchy. JELIA,pp. 111124.van Fraassen, B. (1966). Singular terms, truth-value gaps free logic. Journal Philosophy, 63 (17), 481495.Van Gelder, A. (1993). alternating fixpoint logic programs negation. JournalComputer System Sciences, 47 (1), 185221.Vlaeminck, H., Wittocx, J., Vennekens, J., Denecker, M., & Bruynooghe, M. (2010).approximate method solving problems. Fisher, M., van der Hoek, W.,Konev, B., & Lisitsa, A. (Eds.), JELIA, Lecture Notes Computer Science, pp.326338. Springer.Wittocx, J. (2010). Finite Domain Symbolic Inference Methods Extensions FirstOrder Logic. Ph.D. thesis, Department Computer Science, K.U.Leuven, Leuven,Belgium.Wittocx, J., Denecker, M., & Bruynooghe, M. (2010). Constraint propagation extendedfirst-order logic. CoRR, abs/1008.2121.Wittocx, J., Marien, M., & Denecker, M. (2008). Approximate reasoning first-order logictheories. Brewka, G., & Lang, J. (Eds.), KR, pp. 103112. AAAI Press.124fiJournal Artificial Intelligence Research 45 (2012) 197255Submitted 12/11; published 10/12Reasoning Ontologies Hidden Content:Import-by-Query ApproachBernardo Cuenca GrauBoris Motikbernardo.cuenca.grau@cs.ox.ac.ukboris.motik@cs.ox.ac.ukDepartment Computer Science, Oxford UniversityWolfson Building, Parks Road, Oxford OX1 3QD UKAbstractcurrently growing interest techniques hiding parts signatureontology Kh reused another ontology Kv . Towards goal,paper propose import-by-query framework, makes content Khaccessible limited query interface. Kv reuses symbols Kh certainrestricted way, one reason Kv Kh accessing Kv query interface.map landscape import-by-query problem. particular, outlinelimitations framework prove certain restrictions expressivity Khway Kv reuses symbols Kh strictly necessary enable reasoningsetting. also identify cases reasoning possible present suitableimport-by-query reasoning algorithms.1. IntroductionOntologiesformal conceptualizations domain interesthave become increasinglyimportant computer science. play central role many applications,Semantic Web biomedical information systems. widely used ontology languages Web Ontology Language (OWL) (Horrocks, Patel-Schneider, & van Harmelen, 2003) revision OWL 2 (Cuenca Grau, Horrocks, Motik, Parsia, Patel-Schneider,& Sattler, 2008), standardized World Wide Web Consortium (W3C).formal underpinning OWL family languages provided description logics (DLs) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2007)knowledgerepresentation formalisms well-understood computational properties.Constructing ontologies labor-intensive task, reusing (parts of) well-establishedontologies seen key reducing ontology development cost. Consequently, problemontology reuse recently received significant attention (Stuckenschmidt, Parent, &Spaccapietra, 2009; Lutz & Wolter, 2010; Lutz, Walther, & Wolter, 2007; Cuenca Grau,Horrocks, Kazakov, & Sattler, 2008, 2007; Doran, Tamma, & Iannone, 2007; Jimenez-Ruiz,Cuenca Grau, Sattler, Schneider, & Berlanga Llavori, 2008).discuss problems ontology reuse means example health-caredomain. particular, ontologies currently used several countries describeelectronic patient records (EPR). representation patients data typically involvesontological descriptions human anatomy, medical conditions, drugs treatments,on. latter domains already described well-established reference ontologies, SNOMED-CT, GALEN, Foundational Model Anatomy (FMA).order save resources, increase interoperability applications, rely expertsc2012AI Access Foundation. rights reserved.fiCuenca Grau & Motikknowledge, reference ontologies reused whenever possible.example, assume reference ontology Kh describes concepts ventricular septum defect; then, one might reuse terms Kh order define ontologyKv concepts patients ventricular septum defect, mightembedded EPR application.enable ontology reuse, OWL provides importing mechanism: ontology Kvimport another ontology Kh , result logically equivalent Kv Kh . OWLreasoners deal imports loading ontologies merging contents, thusrequiring physical access axioms Kh . vendor Kh , however, may reluctantdistribute (parts of) contents Kh , might allow competitors plagiarizeKh . Moreover, Kh may contain information sensitive privacy point view.Finally, one may want impose varying cost reuse dierent parts Kh .Rather publishing entire ontology, vendor Kh might want freelydistribute symbols describe organs medical conditions, without distributingaxioms describing symbols. Furthermore, vendor might want completelyhide sensitive information Kh , information treatments. should,however, possible reuse published part Kh without aecting ontologysconsequences; is, part Kh used construct ontology Kv , queryq mentioning symbols Kv answered Kv respective partKh way would done Kv Kh . stipulate Khpublicly available, call ontology Kh hidden and, analogy, call Kv visible.Motivated scenarios, several approaches hiding subset signatureKh developed. example, one possible approach publish -interpolantKh ontology contains symbols coincides Kh logicalconsequences formed using symbols (Konev, Walther, & Wolter, 2009; Wang,Wang, Topor, Pan, & Antoniou, 2009; Wang, Wang, Topor, & Pan, 2008; Wang, Wang,Topor, & Zhang, 2010; Wang et al., 2008; Lutz & Wolter, 2011; Nikitina, 2011). Publishinginterpolant ensures sensitive information Kh (i.e., informationsymbols Kh mentioned interpolant) exposed way; furthermore,interpolants preserve consequences symbols additional advantagedevelopers Kv reason union Kv interpolant using o-theshelf reasoners. interpolation approach may, however, exhibit several drawbacks. First,interpolant may exist Kh expressed relatively weak ontology languagesatisfies certain syntactic conditions (Konev et al., 2009). Second, although interpolantspreserve logical consequences formed using symbols , robustreplacement (Sattler, Schneider, & Zakharyaschev, 2009)that is, union Kv-interpolant Kh guaranteed yield consequences Kh Kv queryq involving symbols Kv . Finally, -interpolant Kh exponentiallylarger Kh , may reveal information strictly needed. referreader Section 7 detailed discussion related work.paper, propose novel approach ontology reuse addresses problemsoutlined making Kh accessible via limited query interface called oracle.oracle advertises public subset signature Kh (e.g., symbols describing organsmedical conditions), answer queries Kh expressed particularquery language use symbols . certain assumptions, so198fiReasoning Ontologies Hidden Contentcalled import-by-query algorithm reason Kv Kh (e.g., determine satisfiabilityKv Kh ) posing queries oracle Kh , without accessing axiomsKh . Furthermore, reasoning performed without making axioms Kvavailable Kh , beneficial Kv might also contain sensitive informationprivacy point view. Finally, framework applicable even casesrelevant interpolant Kh exist.order achieve benefits, however, Kv must reuse symbolssyntactically restricted way, formal properties import-by-query algorithmsspecific restrictions necessary import-by-query algorithm exist dependoracle query language ontology languages used express Kv Kh .paper, explore properties import-by-query reasoning languages ranginglightweight description logic EL (Baader, Brandt, & Lutz, 2005) expressive logicALCHOIQ (Horrocks & Sattler, 2005), combined following types oracles.Queries concept satisfiability oracles concepts constructed using symbolsexpressed particular DL; query, oracle decides satisfiabilityquery concept w.r.t. Kh .Queries ABox satisfiability oracles ABoxes constructed using symbols; query, oracle decides satisfiability query ABox w.r.t. Kh .Queries ABox entailment oracles consist ABox assertion, constructed using symbols ; query, oracle determines whetherassertion entailed Kh query ABox.Concept satisfiability, ABox satisfiability, ABox entailment implementedstate-of-the-art DL reasoners, mentioned query languages seem likenatural foundation practical implementations framework.main contributions paper follows:1. present import-by-query framework, formalize notions oracleimport-by-query algorithm, establish connections import-by-queryalgorithms based dierent types oracles.2. explore limitations framework wide range description logicsformulate precise conditions import-by-query algorithms fail exist.3. identify sucient conditions visible ontology Kv import-byquery algorithm obtained.4. present general hypertableau-based (Motik, Shearer, & Horrocks, 2009) importby-query algorithm relies ABox satisfiability oracles applicableKv Kh given expressive description logic ALCHIQ (Horrocks & Sattler,1999), provided Kv satisfies sucient conditions.5. general algorithm, however, unlikely suitable practice due highdegree nondeterminism. Therefore, present practical (goal-oriented) variantapplicable whenever Kh expressed Horn DL. algorithm199fiCuenca Grau & Motikreadily applied ontologies expressed lightweight description logic EL,guaranteed computationally optimal. Therefore, also presentpractical computationally optimal algorithm used Kv Khexpressed EL.6. establish lower bounds size number queries importby-query algorithm may need ask oracle order solve reasoning task.results provide flexible useful ways ontology designers ensure selectiveaccess ontologies, well family reasoning algorithms provide startingpoint implementation optimization. Furthermore, believe techniquesalso adapted settings, distributed ontology reasoning, collaborativeontology development scenarios ontology developers restricted accessparts ontology developed others.2. Preliminariessection, recapitulate description logic notation used paper, presentoverview various hypertableau reasoning algorithms description logics (Motik et al.,2009), recapitulate various notions modular ontology reuse (Lutz, Walther, &Wolter, 2007; Cuenca Grau, Horrocks, Kazakov, & Sattler, 2008; Konev, Lutz, Walther, &Wolter, 2008).2.1 Description Logicssyntax description logic ALCHOIQ defined w.r.t. pairwise-disjoint countablyinfinite sets atomic concepts NC , atomic roles NR , named individuals NI . Set NCcontains distinguished infinite subset NC nominal concepts (or simply nominals).role either atomic role inverse role R R atomic role.set concepts smallest set containing , A, C, C1 C2 , R.C (existentialrestriction), n R.C (cardinality restriction), atomic concept, C, C1 ,C2 concepts, R role, n nonnegative integer. Furthermore, , C1 C2 , R.C,n R.C abbreviations , (C1 C2 ), (R.C), ( n+1 R.C),respectively. also often treat concepts form R.C abbreviations 1 R.C.concept inclusion axiom form C1 C2 C1 C2 concepts, conceptequivalence C1 C2 abbreviation C1 C2 C2 C1 , concept definitionconcept equivalence form C atomic concept. role inclusionaxiom form R1 R2 R1 R2 roles. TBox axiom either conceptinclusion axiom role inclusion axiom. TBox finite set TBox axioms.assertion form C(a), R(a, b), R(a, b), b, b, C concept, R role,b individuals. ABox finite set assertions. ABox normalizedcontains assertions form A(a), A(a), R(a, b), R(a, b), b,atomic concept R atomic role. axiom either TBox axiomassertion. knowledge base K = consists TBox ABox A.200fiReasoning Ontologies Hidden ContentTable 1: Model-Theoretic Semantics ALCHOIQInterpretation Roles(R )I = {y, x | x, RI }Interpretation Concepts=(C)I = \ C(C1 C2 )I = C1I C2I(R.C)I = {x | : x, RI C }( n R.C)I = {x | {y | x, RI C } n}Satisfaction Axioms Interpretation|= C|= R1 R2|= C(a)|= R(a, b)|= R(a, b)|= b|= bC DIR1I R2IaI CaI , bI RIaI , bI/ RI=baI = bIsignature set atomic concepts atomic roles. concept, role,axiom, set axioms, signature , written sig(), set atomic conceptsatomic roles occurring .1cardinality set written S. interpretation = (I , ) consistsnonempty domain set function assigns object aI individuala, set AI atomic concept implies AI = 1,relation RI atomic role R. Table 1 defines extension rolesconcepts, well satisfaction axioms I. interpretation modelK, written |= K, satisfies axioms K; exists, K satisfiable.concept C satisfiable w.r.t. K model K exists C = .Sometimes, nominal concepts defined form {a} individual,concept interpreted ({a})I = {aI }; is, nominal concept contains preciselygiven individual. drawback definition blurs distinctionconcepts individuals syntactic level. distinction importantimport-by-query framework since framework supports sharing concepts,individuals. paper thus use given alternative definition, nominalsspecial atomic concepts singleton interpretation. well knowntwo definitions equally expressive (Baader et al., 2007).results use general notion description logic. Formally, definedescription logic DL pair consisting set concepts set knowledge bases.call elements former set DL-concepts elements latter set DLknowledge bases. concept DL-knowledge base must DL-concept. DL-TBox(resp. DL-ABox ) DL-knowledge base containing assertions (resp. TBox axioms).1. Note treating nominals special atomic concepts (and individuals); hence, sig()includes nominals, individuals occurring .201fiCuenca Grau & MotikDL-TBox axiom (resp. DL-assertion) TBox axiom (resp. assertion) occursDL-knowledge base. description logic DL1 fragment DL2 (or, conversely,DL2 extends DL1 ) DL1 -concept DL2 -concept DL1 -knowledge baseDL2 -knowledge base. Since unqualified notions concept knowledge basedefined ALCHOIQ, definitions imply description logic consideredpaper fragment ALCHOIQ.Let DL1 DL2 description logics. say DL1 allows DL2 -definitionsif, DL1 -knowledge base K, atomic concept A, DL2 -concept C,K {A C} DL1 -knowledge base. Furthermore, DL1 finite modelproperty satisfiable DL1 -knowledge base model finite domain.description logic ALC obtained ALCHOIQ disallowing nominal concepts(O), inverse roles (I), role inclusion axioms (H), cardinality restrictions (Q).description logics ALC ALCHOIQ named appending combinationsletters O, H, I, Q ALC.DL EL (Baader et al., 2005) (resp. FL0 , see Baader et al., 2007) obtainedALC allowing concepts form , , A, C1 C2 , R.C (resp. R.C)R atomic, allowing assertions form C(a) R(a, b), CEL (resp. FL0 ) concept R atomic role. recent years, significant eortdevoted development DL languages good computational properties,EL, DL-Lite (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007), Horn-SHIQ(Hustadt, Motik, & Sattler, 2005). ALCHIQ knowledge base Horn expressedHorn-SHIQ fragment ALCHIQ.ABox A, G(A) denote graph whose nodes precisely individualsoccurring A, contains undirected edge individuals b= b b occur together assertion A. Individuals bconnected b connected G(A); furthermore, connected pairsindividuals occurring connected. ABox connected componentG(A ) connected component G(A).2.2 Hypertableau Reasoning Algorithmhypertableau calculus Motik et al. (2009) decides satisfiability ALCHOIQknowledge base K. show Section 4.1, presence nominals precludesexistence import-by-query algorithm; hence, section present overviewsimplified version algorithm applicable K ALCHIQ knowledge base.algorithm first preprocesses K set rules Rimplications interpreted first-order semanticsand normalized ABox K equisatisfiableR A. Preprocessing consists three steps. First, transitivity axioms eliminatedK encoding using concept inclusions. Second, axioms normalizedcomplex concepts replaced atomic ones way similar structural transformation first-order logic. Third, normalized axioms translated rulesusing correspondence description first-order logic. omit detailspreprocessing sake brevity; Motik et al. (2009) present relevantdetails. Preprocessing produces so-called HT-rulessyntactically restricted rules202fiReasoning Ontologies Hidden Contenthypertableau calculus guaranteed terminate; precise syntactic form HT-rulesdescribed Section 2.2.1.preprocessing, satisfiability RA decided using hypertableau calculus,described Section 2.2.2.2.2.1 HT-RulesLet NV set variables disjoint set individuals NI . atom expressionform C(s) (a concept atom), R(s, t) (a role atom), (an equality atom),s, NV NI , C concept, R role. rule expression form(1)U1 . . . Um V1 . . . VnUi Vj atoms, 0, n 0. Conjunction U1 . . . Um called body,disjunction V1 . . . Vn called head rule. empty body emptyhead written , respectively. Rules interpreted universally quantifiedFOL implications usual way. rule Horn contains one head atom.HT-rule rule formAi (x) Rij (x, yi ) Sij (yi , x)Bij (yi )(2)Ci (x) Rij (x, yi ) Sij (yi , x) Dij (yi ) yi yj, atomic roles; , B , atomic concepts;Rij , Sij , RijijijijCi either atomic concepts concepts form n R.A n R.A. addition,variable yi occurring HT-rule required occur body atom formRij (x, yi ) Sij (yi , x). Intuitively, body head HT-rules seenstar-shaped: center variable x represents center star, branch variables yiconnected center role atoms. shape ensures satisfiableHT-rules always tree-like modela property used explaingood computational properties many DLs.Motik et al. (2009) shown, preprocessing K produces equisatisfiableset HT-rules normalized ABox; furthermore, K Horn, resulting setcontains Horn HT-rules. Furthermore, certain description logic constructorsused K, R satisfies certain syntactic restrictions discussed next.K use cardinality restrictions, HT-rule R contains atomform yi yj head.K use inverse roles, HT-rule R contains atom form(y , x) head atom form (y , x) body.SijijK use role hierarchies, HT-rule R contains role atomhead.example, consider following knowledge base K corresponding setHT-rules R obtained K.R.BR.CA(x) R.B(x)A(x) R.C(x)203(3)(4)fiCuenca Grau & Motik1 R.R(x, y1 ) R(x, y2 ) y1 y2BCB(x) C(x) D(x)R.D ER(x, y) D(y) E(x)(5)(6)(7)Note R set Horn HT-rules. Note also K uses cardinality restriction1 R., R contains rule equality atom head. Furthermore, Kuse role hierarchies, rule R contains role atom head. Finally, Kuse inverse roles, role atom occurring body rule R contains centervariable x first position branch variable yi second position.applied EL knowledge base, transformation Motik et al. (2009)produces EL-rulesHT-rules form (8) C either atomic conceptconcept form R.A atomic concept.mikRi (x, yi )Ai (x)Bij (yi ) C(x)(8)i=1i=1j=1Note rules previous example except third one (which uses equalityhead) EL-rules.2.2.2 Hypertableau Calculus HT-RulesGiven arbitrary set HT-rules R normalized ABox A, satisfiability Rdecided using calculus described Definition 1.Definition 1. Individuals. set named individuals NI , set individualsNX inductively defined smallest set NI NX and, x NX ,x.i NX integer i. individuals NX \ NI unnamed. individual x.isuccessor x, x predecessor x.i; descendant ancestor transitiveclosures successor predecessor, respectively.Pairwise Anywhere Blocking. label LA (s) individual labelLA (s, t) individual pair s, ABox defined follows:LA (s) = {A | A(s) atomic}LA (s, t) = {R | R(s, t) A}Let strict ordering NX containing ancestor relation. induction ,assign individual blocking status follows.Individual directly blocked individual following conditions hold,predecessors t, respectively:unnamed, blocked, s;2LA (s) = LA (t) LA (s ) = LA (t );LA (s, ) = LA (t, ) LA (s , s) = LA (t , t).2. blocking used ALCHOIQ knowledge bases, individuals also requiredunnamed; however, restriction needed ALCHIQ knowledge bases.204fiReasoning Ontologies Hidden ContentHyp-rule-rule-rule-ruleTable 2: Hypertableau Derivation RulesDerivation Rules HT-rules1. R form (1)2. mapping variables individuals exists2.1 (x) indirectly blocked variable x ,2.2 (Ui ) 1 m,2.3 (Vj ) 1 j n,A1 = {} n = 0;Aj := {(Vj )} 1 j n otherwise.1. n R.C(s) blocked2. individuals u1 , . . . , un exist{ar(R, s, ui ), C(ui ) | 1 n} {ui uj | 1 < j n} A,A1 := {ar(R, s, ti ), C(ti ) | 1 n} {ti tj | 1 < j n}t1 , . . . , tn fresh successors s.1. = neither indirectly blockedA1 := mergeA (s t) named descendant t,A1 := mergeA (t s) otherwise.1. {A(s), A(s)} {R(s, t), R(s, t)}neither indirectly blocked2.A1 := {}.-rule EL-rulesR.A(s) {R(s, aA ), A(aA )}-ruleA1 := {R(s, aA ), A(aA )}Individual indirectly blocked predecessor blocked.Individual blocked either directly indirectly blocked.Pruning Merging. ABox pruneA (s) obtained removing assertions containing descendant s. ABox mergeA (s t) obtained pruneA (s)replacing assertions.Clash. ABox contains clash A; otherwise, clash-free.Derivation Rules. derivation rules consist Hyp-, -, -, -ruleTable 2, which, given R clash-free ABox A, derive ABoxes A1 , . . . , .Hyp-rule, (U ) obtained U replacing variable x (x). role Rindividuals t, function ar(R, s, t) returns assertion R(s, t) R atomic, assertionS(t, s) R inverse role R = .Derivation. derivation R pair (T, ) finitely branchingtree labels nodes ABoxes ( i) () = root,( ii) node t, derivation rule applicable R (t), childrent1 , . . . , tn (t1 ), . . . , (tn ) result applying one derivation rule R(t). algorithm returns derivation R leaf node labeledclash-free ABox, f otherwise.205fiCuenca Grau & MotikHyp-rule similar one hypertableau calculus first-order logic: givenHT-rule form (1) ABox A, Hyp-rule tries unify atoms U1 , . . . , Umsubset assertions A; unifier found, rule nondeterministicallyderives (Vj ) 1 j n. example, given rule A(x) R.C(x) D(x)assertion A(a), Hyp-rule derives either R.C(a) D(a). -rule dealsexistential quantifiers; example, given R.C(a), rule introduces fresh individualderives R(a, t) C(t). -rule deals equality; example, given b,rule replaces individual assertions individual b. Finally, -ruledetects obvious contradictions A(a) A(a), R(a, b) R(a, b), a.Since ALCHIQ allows cyclic concept inclusions form C R.C, terminationhypertableau calculus requires blocking mechanism prevent -rulegenerating infinite sequences successors. individual directly blockedanother individual t, -rule longer applicable s, prevents introductionfresh successors s. Furthermore, descendants indirectly blocked,prevents application rules Table 2 descendants s.derivation R exists leaf node labeled clash-free ABox, model R constructed via well-known technique calledunraveling. Models R obtained way called canonical forest models,Motik et al. (2009) discuss depth properties models.Let R set HT-rules (3)(7) given Section 2.2.1, let = {A(a), E(a)};next show demonstrate using hypertableau algorithm R unsatisfiable. applying Hyp-rule A(a), derive R.B(a) R.C(a). Next,applying -rule R.B(a) derive R(a, t1 ) B(t1 ); applying -ruleR.C(a) derive R(a, t2 ) C(t2 ). Individuals t1 t2 fresh successorsactually form s.1 s.2; however, clarity write simply t1t2 . applying Hyp-rule R(a, t1 ) R(a, t2 ), derive t1 t2 . Furthermore,apply -rule t1 t2 , must replace t1 t2 assertions; thus, replaceR(a, t1 ) B(t1 ) R(a, t2 ) B(t2 ), respectively. Next, applying Hyp-ruleB(t2 ) C(t2 ) derive D(t2 ). Next, applying Hyp-rule R(a, t2 ) D(t2 )derive E(a). Finally, applying -rule E(a) E(a) derive .thus constructed derivation R whose (only) leaf contains clash,R unsatisfiable.2.2.3 Hypertableau Algorithm EL-rulesSince EL knowledge base ALCHIQ knowledge base well, hypertableaualgorithm straightforwardly applied EL KBs. Motik Horrocks (2008) showed,however, worst-case optimal algorithm obtained modifying -rule.modified algorithm works set R EL-rules.following algorithm checks satisfiability R A, R set EL-rulesnormalized ABox.Definition 2. named individual NI atomic concept NC , let aAfresh individual uniquely associated A. hypertableau algorithmEL one described Definition 1, derivation rules includeHyp-, -, -rule Table 2.206fiReasoning Ontologies Hidden Content2.3 ModularityLet Kv knowledge base reuses knowledge base Kh , let subsetsig(Kh ) reused Kv is, = sig(Kh ) sig(Kv ). often beneficialKv reuses Kh modular way; intuitively, case knowledge base Kvaect meaning symbols (Lutz, Walther, & Wolter, 2007; Cuenca Grau,Horrocks, Kazakov, & Sattler, 2008; Konev, Lutz, Walther, & Wolter, 2008). Two dierentnotions modularity considered literature, providing dierent formalaccount means Kv aect meaning symbols .knowledge base Kv deductively modular w.r.t. signature if, concepts Cexpressed description logic Kv sig(C) sig(D) ,Kv |= C implies |= C D. is, axioms Kv must giverise nontrivial logical consequences involve symbols .knowledge base Kv semantically modular w.r.t. signature if, interpretation = (I , ) symbols , exists interpretation J = (J , J )= J , X = X J X , J |= Kv . is, axioms Kvallowed impose constraints interpretation symbols .Semantic modularity stronger deductive one: Kv semantically modularw.r.t. , also deductively modular w.r.t. ; converse hold necessarily.Deciding whether knowledge base Kv deductively semantically modular w.r.t.signature hard computational problem DLs, often undecidable(Lutz et al., 2007; Konev et al., 2008). Cuenca Grau, Horrocks, Kazakov, Sattler(2008) defined several practically useful sucient syntactic conditions guaranteesemantic modularity.3. Import-by-Query Frameworksection introduce framework. first present motivating example,proceed formalization import-by-query problem.Consider medical research company (MRC) developed knowledge basehuman anatomy. knowledge base contains concepts describing organs HeartTV (tricuspid valve); medical conditions CHD (congenital heart defect), VSD(ventricular septum defect), (aortic stenosis); treatments Surgery.roles part, con, treatment relate organs parts, medical conditions,treatments, respectively, used define concepts VSD Heart (aheart ventricular septum defect) Sur Heart (a heart requires surgicaltreatment). focus reusing schema knowledge, assume knowledgebase consists TBox Th , shown Table 3. Assume MRC wantsfreely distribute information organs conditions, hide informationtreatments. Thus, MRC identifies set public symbols Th ; write symbolsbold, remaining private symbols sans serif. MRC want distributeaxioms Th , might allow competitors copy parts Th ; therefore, sayknowledge base Th hidden.Consider also health-care provider (HCP) reuses Th describe types patientsVSD Patient (patients ventricular septum defect), HS Patient (patientsrequiring heart surgery), Patient (patients aortic stenosis), EA Patient (patients207fiCuenca Grau & MotikTable 3: Example Knowledge BasesHidden Knowledge Base Th1Heart Organ part.TV2VSD CHD3CHD4 VSD Heart Heart con.VSD5 VSD Heart treatment.Surgery6 Sur Heart Heart treatment.SurgeryVisible Knowledge Base Kv1 VSD Patient Patient hasOrg.VSD Heart2 HS Patient Patient hasOrg.Sur Heart3 Patient Patient hasOrg.(Heart con.AS)4Ab TV TV5Dis TV Ab TV6EA Heart VSD Heart part.Dis TV7 EA Patient Patient hasOrg.EA Heart8 Ab TV Heart Heart part.Ab TV9 TVD Patient Patient hasOrg.Ab TV HeartEbsteins anomaly), TVD Patient (patients tricuspid valve defect). SinceTBox Th describe Ebsteins anomaly, HCP defines EA Heart heartventricular septum defect displaced tricuspid valve Dis TV ; furthermore,defines displaced tricuspid valve abnormal, Ab TV Heart heart abnormal tricuspid valve. general, HCPs knowledge base could contain ABox assertions,denote knowledge base Kv call visible. axioms Kv shownTable 3, private symbols Kv written italic. HCP use combinedknowledge base Kv Th deduce VSD Patient HS Patient (patients ventricular septum defect require heart surgery) EA Patient TVD Patient (patientsEbsteins anomaly kind patients tricuspid valve defect).support scenarios, propose import-by-query framework. Instead publishing (a subset of) axioms Th , MRC publish oracle Th serviceadvertises set public symbols Th query language L, answerL-queries Th provided queries use symbols . so-called import-byquery algorithm reason Kv Th (e.g., determine satisfiability Kv Th )without physical access contents Th , asking queries oracle.existence algorithm, however, depends oracles query language,DLs used express Kv Th , way symbols reused Kv .One popular query languages description logics concept satisfiability,available DL reasoners known us. thus natural consider conceptsatisfiability oracles, advertise signature check satisfiability w.r.t. Th(not necessarily atomic) concepts formed using symbols . Later showimport-by-query algorithms based concept satisfiability oracles exist rather strong208fiReasoning Ontologies Hidden Contentrestrictions imposed way Kv reuses symbols ; roughly speaking,possible mix roles concepts private Kv existential universalrestrictions. example, means axioms 6 8 Table 3 wouldallowed Kv . overcome limitations concept satisfiability oracles, consider twoadditional types (closely related) oracles powerful oracles basedconcept satisfiability. ABox satisfiability oracle given ABox sig(A) ,checks satisfiability Th . ABox entailment oracle given ABoxassertion sig(A) sig() , checks whether Th |= . ABoxsatisfiability entailment implemented state-of-the-art DL reasoners,oracles based inferences seem natural.practice, natural express oracle queries DL Th ; however,sake generality allow queries expressed arbitrary description logic L.Intuitively, allows Kv learn structure models Th ,allows us obtain general results nonexistence import-by-query algorithms.Definition 3 formally introduces dierent types oracles.Definition 3. Let Th TBox, let signature, let L description logic.concept satisfiability oracle Th , , L Boolean function cTh ,,L that,L-concept C sig(C) , returns C satisfiable w.r.t. Th .ABox satisfiability oracle Th , , L Boolean function aTh ,,L that,connected L-ABox sig(A) , returns Th satisfiable.ABox entailment oracle Th , , L Boolean function eTh ,,L that,connected L-ABox sig(A) L-assertion mentionsindividuals sig() , returns Th |= .use generic term oracle either concept satisfiability, ABox satisfiability,ABox entailment oracle. Furthermore, L description logic Th ,abbreviate Th ,,L Th , . Finally, often refer oracle arguments (i.e.,concepts C, ABoxes A, pairs A, case concept satisfiability, ABoxsatisfiability, ABox entailment oracles, respectively) oracle queries.next formally define import-by-query algorithms using well-known notionoracle Turing machine. precise definition latter given Papadimitriou (1993);next present informal overview main ideas. oracle Turing machineseparate query tape, write arbitrary strings given alphabet.point time, enter special state q? , upon black-box oracle checkswhether string currently written query tape belongs language associated; case, enters special state qyes , otherwise enters specialstate qno . allows oracles answers aect computation . combinationusually written . definition assumes computationdepends input oracles answers; is, 1 2 two distinctoracles, computations 1 indistinguishable computations 21 2 return answers queries encountered computations. restpaper, make assumptions type : reasonable Turingmachine model used. merely assume equipped suitable notionrun captures computation input. run (butneed to) accept reject input.209fiCuenca Grau & MotikDefinition 4. class inputs C class triples form C , KvC , ThC Csignature, KvC knowledge base, ThC TBox sig(KvC ) sig(ThC ) C .triple C called input.import-by-query algorithm description logic L class inputs C basedoracles type x {a, e, c} oracle Turing machine ibqx combinedoracle type x. input , Kv , Th C following properties must satisfied,ibqx [Th , , L] combination ibqx oracle xTh ,,L :1. whenever ibqx [Th , , L] enters state q? run, string query tape encodesquery accepted xTh ,,L ;2. ibqx [Th , , L] accepting run Kv Kv Th satisfiable;3. run ibqx [Th , , L] Kv finite.Intuitively, transition relation ibqx takes account possible answersoracle type x, ibqx executable actual oracle unknown. Thus,ibqx seen computer program particular subroutine missing. Giveninput , Kv , Th C, parameterize ibqx xTh ,,L obtain ibqx [Th , , L],latter Turing machine freely applied Kv .rest paper, whenever oracle type explicitly given, discussionapplies oracle types. consider various classes inputs,defined using following formulation:C largest class triples C , KvC , ThC sig(KvC ) sig(ThC ) CC , KvC , ThC satisfy condition.Usually, however, abbreviate formulations follows:C[C , KvC , ThC ] class inputs C , KvC , ThC satisfy condition.Definition 4 straightforwardly implies following property, essentially reformulates idea runs Turing machine determined oraclesanswers, oracles themselves.Proposition 1. Let ibq import-by-query algorithm description logic Lclass inputs C, let , Kv , Th1 arbitrary input C, let Q1 , . . . , Qnoracle queries encountered possible runs ibq[Th1 , , L] Kv . Then, Th2, Kv , Th2 C 1 ,,L (Qi ) = 2 ,,L (Qi ) 1 n, runhhibq[Th1 , , L] Kv run ibq[Th2 , , L] Kv vice versa.Section 4 identify DLs defining oracle query language classesinputs import-by-query algorithm based oracles particular type exists.following proposition shows suces prove nonexistence resultsexpressive DL smallest class inputs; then, analogous results holdweaker DL larger class inputs.Proposition 2. Let L1 description logic let L2 fragment L1 ; let C1 C2classes inputs triple C1 also belongs C2 ; let x {a, c, e}oracle type. import-by-query algorithm L1 C1 based oracles typex, also import-by-query algorithm L2 C2 based oracles type x.210fiReasoning Ontologies Hidden ContentProof. prove contrapositive claim. Let ibqx import-by-query algorithmL2 C2 . Since triple C1 also contained C2 , ibqx clearly import-byquery algorithm L2 C1 . Let , Kv , Th C1 arbitrary input, let Qarbitrary L2 -query encountered run ibqx [Th , , L] Kv . Since L2 fragmentL1 , Q L1 -query well. Thus, ibqx import-by-query algorithm L1 C1 .following theorem shows oracles certain types simulate oraclestypes. important 1 simulate 2 show importquery algorithm exists particular class inputs applicable 1 , alsoalgorithm exists applicable 2 .Theorem 1. Let smallest partial order class oracles satisfiesfollowing conditions TBox Th , signature , description logic L:1. cTh ,,L aTh ,,L eTh ,,L ;2. L-ABox L-assertion {} L-ABox,eTh ,,L aTh ,,L holds well.Let L description logic, let C class inputs, let x1 , x2 {a, c, e} oracletypes xTh1 ,,L xTh2 ,,L , Kv , Th C. Then, import-by-query algorithm ibqx1 L C transformed import-by-query algorithm ibqx2L C that, input , Kv , Th C, ibqx1 [Th , , L] run Kv noracle queries ibqx2 [Th , , L] run Kv n oracle queries.Proof. Let ibqx1 arbitrary import-by-query algorithm L C, considerarbitrary input , Kv , Th C. Conditions 1 2 ensure xTh1 ,,L reduciblexTh2 ,,L sense computable total function f exists domainxTh1 ,,L domain xTh2 ,,L query Q accepted xTh1 ,,L ,xTh1 ,,L (Q) = xTh2 ,,L (f (Q)). particular, ABox satisfiability oracle reducibleABox entailment oracle via f (A) = (A, ) ABox A. Furthermore, Condition2 holds, ABox entailment oracle reducible ABox satisfiability oracle viaf (A, ) = {}. Finally, concept satisfiability oracle reducible ABox satisfiability oracle via f (C) = {C(a)} fresh individual.Algorithm ibqx2 simply simulate ibqx1 input , Kv , Th C; furthermore, whenever ibqx1 [Th , , L] poses query Q xTh1 ,,L , ibqx2 [Th , , L] computesf (Q) poses query f (Q) xTh2 ,,L . Since ibqx1 import-by-query algorithmL C, ibqx2 . Furthermore, input, one-to-one correspondence runs algorithms corresponding runs posing exactly numberoracle queries.next show that, shared signature contains atomic concepts,close correspondence ABox concept satisfiability oracles.Theorem 2. Let L description logic let C[C , KvC , ThC ] class inputs Ccontains atomic concepts. Then, import-by-query algorithm ibqa L Ctransformed import-by-query algorithm ibqc L C followingstatements hold input , Kv , Th C.211fiCuenca Grau & Motikrun ibqa [Th , , L] Kv n oracle queries maximum numberindividuals query ABox, run ibqc [Th , , L] Kv n oraclequeries exists.run ibqc [Th , , L] Kv n oracle queries, run ibqa [Th , , L]Kv n oracle queries exists.Proof. Let ibqa import-by-query algorithm L C. define ibqc that,input , Kv , Th C, algorithm ibqc [Th , , L] simulates steps algorithmibqa [Th , , L]; furthermore, ibqa [Th , , L] queries aTh ,,L ABox A, algorithmibqc [Th , , L] proceeds follows.1. algorithm transforms ABox iterating assertionsform b and, assertion, replacing one individual (say a)one (say b) assertions.2. contains individual cTh ,,L (B1 . . . Bn ) = fB1 , . . . , Bn concepts Bi (a) , ibqc [Th , , L] proceedsway ibqa [Th , , L] aTh ,,L (A) = f; otherwise, ibqc [Th , , L] proceedsway ibqa [Th , , L] aTh ,,L (A) = t.obvious correspondence runs ibqa [Th , , L] ibqc [Th , , L]Kv ; furthermore, whenever ibqa [Th , , L] issues query aTh ,,L , ibqc [Th , , L] issuesqueries cTh ,,L order determine proceed. Finally, notesecond statement theorem directly follows Theorem 1.finally show without loss generality assume Kv contain conceptcon.AS axiom 3 Table 3.Definition 5. Let signature. concept C -modal sig(C) Cform R.D, R.D, n R.D, n R.D.Intuitively, -modal concepts always treated atomic point viewKv , rely oracle compute relevant consequences concepts.Theorem 3. Let L, DL1 , DL2 description logics DL1 -conceptalso L-concept DL2 allows DL1 -definitions; let x {a, c, e}; let C[C , KvC , ThC ]class inputs KvC DL1 -knowledge base ThC DL2 -TBox; letD[D , KvD , ThD ] class inputs consisting triples , Kv , Th C[C , KvC , ThC ]Kv contains -modal concepts. Then, import-by-query algorithm ibqx2 Ltransformed import-by-query algorithm ibqx1 L C.Proof. signature, C concept, concept, axiom, knowledge base,say C -outermost C -modal C occur propersubconcept another -modal concept.Let , Kv , Th C arbitrary input C, let set -outermost conceptsKv , let XC fresh atomic concept uniquely associated C S. define, Th , Kv follows: = {XC | C S}; Kv obtained Kv replacingC XC ; Th = Th {XC C | C S}. Clearly, Kv Th equisatisfiable212fiReasoning Ontologies Hidden ContentKv Th , , Kv , Th D. Let ibqx2 arbitrary import-by-query algorithm LD. define ibqx1 algorithm , Kv , Th C simulates stepsibqx2 input , Kv , Th D, following modifications:ibqx1 [Th , , L] treats concepts atomic;whenever ibqx2 [Th , , L] queries xT , ,L query Q , ibqx1 [Th , , L] querieshxTh ,,L query Q obtained Q replacing occurrence XC C.obvious correspondence runs ibqx2 [Th , , L] ibqx1 [Th , , L]Kv , ibqx1 import-by-query algorithm L C.4. Limitations Import-by-Query Frameworksection, explore limitations import-by-query framework showimport-by-query algorithms exist certain conditions. negative resultsapply classes input Kv Th expressed description logic DLlightweight possible, oracle based ABox satisfiability, oracle acceptsqueries expressed description logic L expressive possible. Theorem 1Proposition 2, results also apply oracle types, queries expressedfragment L, classes input Kv Th expressed description logicextends DL.particular, Section 4.1 establish following general limitations importby-query framework.presence nominals Th may preclude existence import-by-queryalgorithm even = (cf. Theorem 4).Deductive modularity TBox Kv w.r.t. necessary conditionexistence import-by-query algorithm (cf. Theorem 5).Deductive modularity, however, sucient, even Kv Th ELallowed contain atomic concepts (cf. Theorem 6).response negative results, import-by-query algorithms proposed papersubjected following restrictions:R1. Th allowed contain nominals.R2. TBox Kv required semantically modular w.r.t. .show Section 5.1 two restrictions sucient guarantee existenceimport-by-query algorithm Kv ALCHIQ Th ALCHIQ, providedcontains atomic concepts.Section 4.2, however, show restrictions input necessaryallowed contain atomic roles. Roughly speaking, restrictions R1 R2 insucientsince axioms Kv arbitrarily propagate information symbols privateKv via role hidden part canonical model Kv Th (that is, partcanonical model cannot constructed using axioms Kv ); propagation213fiCuenca Grau & Motikoccur via existential (cf. Theorem 7) universal quantification (cf. Theorem 8).overcome negative results, define Section 5.1 HT-safety condition that,one hand, ensures semantic modularity and, hand, prevents arbitrarytransfer information symbols private Kv hidden parts canonicalmodel via role . condition, however, still insucient enable import-byquery reasoning Th contains universal quantifiers, inverse roles, functional roles,Kv entails cyclic axioms form R.A R (cf. Theorem 9).overcome negative result, Section 5.1 introduce acyclicity conditiontogether HT-safety guarantees existence import-by-query algorithm basedABox satisfiability oracles Kv Th expressed ALCHIQ.Finally, Section 4.3 show import-by-query algorithm based conceptsatisfiability oracles exists class inputs C[C , KvC , ThC ] KvC ELsatisfies HT-safety condition, ThC EL (cf. Theorem 10). Section 5.2.2,however, present algorithm based ABox entailment oracles appliesclass inputs C. Thus, practically relevant cases exist import-by-query reasoningimpossible concept satisfiability oracles, becomes feasible ABox oracles.4.1 General Limitationsfirst show presence nominals hidden knowledge base precludesexistence import-by-query algorithm visible knowledge base satisfiableinfinite models. Expressive DLs used practice often finite model property,negative result holds even shared signature empty; thus, restpaper consider DLs nominals, leave investigationconditions enable import-by-query reasoning DLs future work.Theorem 4. description logic DL without finite model property, importby-query algorithm based ABox satisfiability oracles exists L = ALCHOIQclass inputs C[C , KvC , ThC ] C = , KvC DL-knowledge base, ThCALCHOIQ-TBox.Proof. Let C arbitrary class inputs let ibqa arbitrary import-by-queryalgorithm C ibqa satisfy theorems assumptions. Furthermore, let, Kv , Th1 C arbitrary input KvC satisfiable infinite models, = ,Th1 = . Since runs ibqa [Th1 , , L] Kv finite, number individualsoccurring query ABox run bounded integer n. Let Th2follows, O1 , . . . , fresh nominal concepts:Th2 = { O1 . . . }(9)Clearly, Kv Th1 satisfiable, Kv Th2 not. Consider arbitrary query ABoxoccurring run ibqa [Th1 , , L]. Since = , consists assertions formb b; furthermore, contains n individuals, 1 , (A) = impliesh2 , (A) = t, converse holds monotonicity first-order logic. then,hProposition 1, runs ibqa [Th1 , , L] Kv coincide runs ibqa [Th2 , , L]Kv , contradicts fact Kv Th1 satisfiable Kv Th2 not.214fiReasoning Ontologies Hidden Contentnext present strong result: deductive modularity necessary requirementexistence import-by-query algorithm; is, import-by-query algorithmexists class inputs contains triple , Kv , Th TBox Kvdeductively modular w.r.t. . Intuitively, without deductive modularity, Kvarbitrarily influence consequences Th , oracle cannot take accountsince access axioms Kv . sake generality,impose conditions .Theorem 5. Let DL1 arbitrary fragment ALCHIQ; let DL2 arbitrarydescription logic extends EL allows DL1 -definitions; let arbitrarysignature; let Kv arbitrary satisfiable DL1 -knowledge base whose TBoxdeductively modular w.r.t. . Then, import-by-query algorithm based ABox satisfiability oracles exists L = ALCHIQ class inputs C[C , KvC , ThC ] C = ,KvC = Kv , ThC DL2 -TBox.Proof. Let C class inputs satisfying theorems conditions, let , Kv , Th1 Cinput Th1 = . Since Kv deductively modular w.r.t. , possibly complex DL1 concepts C1 C2 exist sig(C1 ) , sig(C2 ) , Tv |= C1 C2 ,|= C1 C2 . Let ibqa import-by-query algorithm L = ALCHIQ C. Finally,let Th2 follows, A, B1 , B2 , R occur .Th2 = { B1 C1 , B2 C2 , R.(A B1 ), B2 }(10)Clearly, Kv Th1 satisfiable, Kv Th2 not. Consider arbitrary L-ABoxsig(A) . Th1 unsatisfiable, Th2 . Conversely, assumeTh1 satisfiable model = (I , ). Since |= C1 C2 , interpretation= (I , ) domain element x exist x C1I x C2I .loss generality assume = . Let following interpretation:aIAIB1IB2IRIXI== aI individual occurring= {x}= C1I C1I= C2I C2I= {o, x | }= X X atomic concept role XALCHIQ-concept E sig(E) , since disjoint,straightforward induction structure E one show E = EE = E . Furthermore, atomic role . Thus |= A,straightforward check |= Th2 . Consequently, 1 ,,L (A) = 2 ,,L (A) LhhABox sig(A) . Hence, Proposition 1, runs ibqa [Th1 , , L] Kv coincideruns ibqa [Th2 , , L] Kv , contradicts fact Kv Th1 satisfiableKv Th2 not.Theorem 5 shows deductive modularity necessary requirementimport-by-query algorithm exist, following theorem shows sucient215fiCuenca Grau & Motikrequirement, even contains atomic concepts, Kv EL-knowledge base, ThEL-TBox.Theorem 6. import-by-query algorithm based ABox satisfiability oracles existsL = ALCHIQ class inputs C[C , KvC , ThC ] C contains atomic concepts,KvC ThC EL, TBox KvC deductively modular w.r.t. C .Proof. Let ibqa import-by-query algorithm satisfying theorems assumptions, let= {A, B, C}, let Kv , Th1 , Th2 following EL knowledge bases:Kv = { A(a), B R.C }(11)= Th1 { S.B }(13)Th1Th2={C}(12)TBox Kv clearly deductively modular w.r.t. , , Kv , Thi C {1, 2};furthermore, Kv Th1 satisfiable, whereas Kv Th2 not. Consider arbitraryquery ABox sig(A) ; since contains assertions form X(a),X(a), b, b sig(X) , 1 ,,L (A) = 2 ,,L (A). then,hhProposition 1, runs ibqa [Th1 , , L] Kv coincide runs ibqa [Th2 , , L]Kv , contradicts fact Kv Th1 satisfiable Kv Th2 not.deductive modularity sucient, semantic modularity sucientcases: Section 5.1 present import-by-query algorithm appliedcase contains atomic concepts, Kv Th ALCHIQ, TBoxKv semantically modular w.r.t. .4.2 Limitations Importing Atomic Rolessection, establish limitations import-by-query framework casesallowed contain atomic roles. particular, show semantic modularitysucient guarantee existence import-by-query algorithm.Theorems 7 8 demonstrate problems arise due certain fundamental limitations oracle query languages. understand intuition behind results,assume shared signature contains one atomic role R. Even relatively simpleDL EL, knowledge base Th imply existence arbitrarily long R-chains using axiomC R.C. oracle languages consider, however, examinebounded prefixes chains. example, assume use ABox satisfiability oracle query language based ALCHIQ. concept query ABox correspondsfirst-order formula, well known satisfiability formulafirst-order interpretation depends formulas quantifier depth. Since numberoracle calls run import-by-query algorithm must bounded, import-by-queryalgorithm examine bounded prefix model Th . leads usfundamental problem: Th changed interesting consequencesdetected examining longer R-chains, consequences go undetectedalgorithm render algorithm incorrect. Theorem 7 exploits fact216fiReasoning Ontologies Hidden Contentinteresting consequences Th detected Kv using axioms existentially quantified concepts (i.e., proof uses axiom R.B2 B2 ), whereas Theorem (8) analogouslyuses axioms universally quantified concepts (i.e., B R.B).alternative intuitive explanation results Theorems 7 8 thinkculprit axioms R.B2 B2 B R.B Kv propagating informationKv Th . order miss interesting consequences Th , import-by-queryalgorithm must examine suciently large portion hidden part canonical modelKv Th order correctly evaluate culprit axioms. This, however, impossiblebound portion size determined algorithms inputs.Theorem 7. import-by-query algorithm based ABox satisfiability oracles existsL = ALCHIQ class inputs C[C , KvC , ThC ] C arbitrary, KvC ThCexpressed EL, TBox KvC semantically modular w.r.t. C .Proof. Let ibqa import-by-query algorithm satisfying theorems assumptions, let= {A1 , A2 , R}, let Kv following EL knowledge base:Kv = { B1 (a), B1 S.A1 , A2 B2 , R.B2 B2 , S.B2 }(14)TBox Kv semantically modular w.r.t. : interpretation symbols, interpretation J X J = X X , B1J = , B2J = J ,J = model TBox Kv . Let Th1 following EL TBox:Th1 = { A1 C, C R.C }(15)Since run ibqa [Th1 , , L] Kv finite, integer n exists queryABox occurring run contains concepts quantifier depth n. Let Th2following EL TBox:Th2 = {A1 R. . R .A2 }.n + 1 times(16)Clearly, Kv Th1 satisfiable, whereas Kv Th2 not. Consider arbitrary query ABoxoccurring run ibqa [Th1 , , L]. next show 1 ,,L (A) = 2 ,,L (A).hhAssume Th1 satisfiable. Since expressed ALCHIQ Th1 EL,canonical forest model = (I , ) Th1 exists (e.g., model obtainedapplying hypertableau algorithm Th1 A). Due (15), x AI1 , infinitex RI 0 i.sequence {0x , 1x , 2x , . . .} exists 0x = x ix , i+1JJLet J = ( , ) interpretation defined follows:J =xAJ2 = AI2 {n+1| x AI1 }X J = X X = A2Clearly, J |= Th2 . Furthermore, since |= A, contains concepts quantifier depthn, J coincide depth n, J |= A. Thus, Th2 satisfiable.Assume Th2 satisfiable. canonical forest model = (I , ) Th2x } existsexists. Due (16), x AI1 , finite sequence {0x , 1x , 2x , . . . , n+1217fiCuenca Grau & Motikx RI 0 < n. Let J = (J , J ) interpresuch 0x = x ix , i+1tation defined follows:J =xC J = {0x , . . . , n+1| x AI1 }x , x | x AI }RJ = RI {n+1n+11X J = X X {R, C}Clearly, J |= Th1 . Furthermore, since |= A, C sig(A), contains conceptsquantifier depth n, J coincide depth n, J |= A. Thus,Th1 satisfiable.Proposition 1, runs ibqa [Th1 , , L] Kv coincide runs ibqa [Th2 , , L]Kv , contradicts fact Kv Th1 satisfiable Kv Th2 not.Theorem 8. import-by-query algorithm based ABox satisfiability oracles existsL = ALCHIQ class inputs C[C , KvC , ThC ] C arbitrary, KvC expressedFL0 , ThC expressed EL, TBox KvC semantically modular w.r.t. C .Proof. Let ibqa import-by-query algorithm satisfying theorems assumptions, let= {A1 , A2 , R}, let Kv following FL0 knowledge base.Kv = { A1 (a), B(a), B R.B, A2 B }(17)TBox Kv semantically modular w.r.t. : interpretation ,interpretation J X J = X X B J = model TBoxKv . Let Th1 EL TBox (15) given proof Theorem 7. Since runibqa [Th1 , , L] Kv finite, integer n exists query ABox occurringrun contains concepts quantifier depth n. Let Th2 EL TBox (16)Theorem 7. Clearly, Kv Th1 satisfiable, whereas Kv Th2 not. Using argumentsanalogous proof Theorem 7, one show 1 ,,L (A) = 2 ,,L (A)hhquery ABox occurring run ibqa [Th1 , , L]. Proposition 1, runsibqa [Th1 , , L] Kv coincide runs ibqa [Th2 , , L] Kv , contradictsfact Kv Th1 satisfiable Kv Th2 not.possible way overcome negative results prevent axioms Kvpropagating information via roles hidden part canonical modelKv Th . Section 5.1, achieve requiring Kv HT-safe. Roughly speaking,Kv semantically modular w.r.t. , but, addition, translated setHT-rules Rv variables x role atom form R(x, y) Rguarded suitable concepts. example, although knowledge base Kv (17)semantically modular w.r.t. = {A1 , A2 , R}, axiom B R.B Kv violates HTsafety condition since body corresponding HT-rule B(x) R(x, y) B(y)contain guard concept atom variable y. order streamline presentationensure notions needed enable import-by-query reasoning defined oneplace, formalize HT-safety Definition 6 Section 5.1. Unfortunately, Theorem 9shows, HT-safety alone ensure existence import-by-query algorithm.Theorem 9. import-by-query algorithm based ABox satisfiability oracles existsL = ALCHIQ class inputs C[C , KvC , ThC ] C arbitrary, KvC expressedEL, ThC expressed Horn-ALCIF, TBox KvC HT-safe w.r.t. C .218fiReasoning Ontologies Hidden ContentProof. Let ibqa import-by-query algorithm satisfying theorems assumptions, let= {B, R}, let Kv following EL knowledge base:Kv = { A(a), B(a), R.A }(18)TBox Kv semantically modular w.r.t. : interpretation ,interpretation J X J = X X AJ = model TBoxKv . According Definition 6, TBox Kv HT-safe well. Let Th1following Horn-ALCIF TBox:Th1 = { B C , B R.C, C R.C, 1 R }(19)Since run ibqa [Th1 , , L] Kv finite, integers n exist queryABox occurring run contains n individuals concepts quantifier depthm. Let k = n + let D0 , . . . , Dk distinct fresh atomic concepts. Let Th2following Horn-ALC TBox:Th2 = Th1 { Di Dj , | 0 < j k } { Dj1 R.Dj | 1 k }{ B D0 , Dk }(20)Clearly, Kv Th1 satisfiable, whereas Kv Th2 not. Consider arbitrary queryABox occurring run ibqa [Th1 , , L]. next show 1 ,,L (A) = 2 ,,L (A).hhclearly holds Th1 unsatisfiable, assume Th1 satisfiable. Then,exists canonical forest model = (I , ) Th1 A. Consider arbitrary domainelement x B . say domain element reachable x stepssequence domain elements 0 = x, 1 , 2 , . . . , = exist , i+1 RI1 < . x y, axioms Th1 ensure following properties:1. sequence unique consists unique domain elements.RI inverse-functional relation so, 0 < , domain elementelement , i+1 RI , = j 0 < < j ; furthermore,0 B 0 C , C 0 < , ensures 0 = 0 < .2. x B distinct x exists reachable form x .xi B 0 < RI inverse-functional.3. < k. contains n individuals conceptsquantifier depth m.Let J = (J , J ) interpretation defined follows:J =X J = X X sig(Th1 )DkJ =DiJ ={y | reachable x steps } 0 < kxBInterpretations J coincide symbols Th1 , J |= Th1 . Furthermore,DiI < k, properties 13 DjI j = i, J |= Th2 . then,Proposition 1, runs ibqa [Th1 , , L] Kv coincide runs ibqa [Th2 , , L]Kv , contradicts fact Kv Th1 satisfiable Kv Th2 not.219fiCuenca Grau & Motikproof Theorem 9 uses Kv implies Rn .A arbitrary n,R . Furthermore, axioms Th containing universally quantified concepts propagate information along R-chain unknown level m. import-by-query algorithmcannot determine depth must examine model Kv , precludestermination requirement Definition 4. Section 5.1, present sucient acyclicityrestriction Kv bounds n ensures existence import-by-query algorithm.4.3 ABox vs. Concept Satisfiability Oraclessection show that, Kv EL-knowledge base Th EL-TBox,import-by-query algorithm based concept satisfiability oracles exists, even Kv HTsafe w.r.t. . interesting Section 5.2.2 present algorithm basedABox entailment oracles handle case. Thus, ABox oracles strictlyexpressive concept satisfiability oracles.Theorem 10. import-by-query algorithm based concept satisfiability oracles existsL = ALCHIQ class inputs C[C , KvC , ThC ] C arbitrary, KvC ThCexpressed EL, TBox KvC HT-safe w.r.t. C .Proof. Let ibqc import-by-query algorithm satisfying theorems assumptions, let= {R}, let Kv following EL knowledge base:Kv = { A(a), R.A }(21)Definition 6, TBox Kv HT-safe. Let Th1 = . run ibqc [Th1 , , L] Kvfinite, integer n exists query concept occurring run containsconcepts quantifier depth n. Let Th2 following EL TBox:Th2 = { R.. . . R . }n + 1 times(22)Clearly, Kv Th1 satisfiable, whereas Kv Th2 not. Furthermore, straightforwardsee that, ALCHIQ concept C quantifier depth n sig(C) ,Th1 |= C Th2 |= C , cT 1 , (C) = cT 2 , (C). then,hhProposition 1, runs ibqc [Th1 , , L] Kv coincide runs ibqc [Th2 , , L]Kv , contradicts fact Kv Th1 satisfiable Kv Th2 not.Note knowledge base Kv used proof Theorem 10 analogousone proof Theorem 9that is, entails cyclic axiom form R.AR . negative result Theorem 9, however, applycase Th expressed EL. algorithm presented Section 5.2.2 handleknowledge bases via ABox entailment oracle. Intuitively, ABoxesencode cyclic structures, whereas concepts cannot.5. Import-by-Query Algorithmssection, identify several cases import-by-query algorithms exist.simplicity, throughout section assume Kv contain -modal concepts;Theorem 3 without loss generality.220fiReasoning Ontologies Hidden Contentovercome negative results Section 4, Sections 5.1.1 5.1.2 introduceHT-safety acyclicity conditions, respectively, Kv must satisfy order prevent undesirable interactions axioms Kv Th . Furthermore, restpaper assume Kv preprocessed described Motik et al. (2009)corresponding set HT-rules Rv ABox Av ; convenient HT-rulescontain nested quantifiers. thus formulate HT-safety acyclicity termsRv Av , define Kv HT-safe (resp. acyclic) corresponding RvAv HT-safe (resp. acyclic). algorithms take inputs Rv Av , specifyallowed inputs using classes C[C , RCv ACv , ThC ] triples C , RCv ACv , ThC Rvset HT-clauses, Av normalized ABox, sig(RCv ACv ) sig(ThC ) C .Section 5.1 present general import-by-query algorithm based ABox satisfiability oracles applicable case Kv imports atomic conceptsroles, Kv Th expressed ALCHIQ. order algorithm applicable,however, Kv must HT-safe acyclic. contains atomic concepts,acyclicity vacuously satisfied Kv HT-safety becomes equivalent semanticmodularity; thus, atomic concepts shared, algorithms applicable wheneverKv semantically modular w.r.t. .algorithm Section 5.1, however, unlikely suitable practice duehigh degree nondeterminism. Therefore, Section 5.2.1 present import-by-queryalgorithm based ABox entailment oracles that, believe, suited implementationoptimization. algorithm requires Th Horn knowledge base, allowsalgorithm goal-oriented.practical algorithm Section 5.2.1 readily applied EL knowledge bases,guaranteed optimal. Therefore, Section 5.2.2 present EL-specificimport-by-query algorithm case Kv Th expressed EL. additionoptimal EL knowledge bases, EL-specific algorithm require Kvacyclic somewhat relaxes HT-safety requirement.5.1 Import-by-Query ALCHIQnext present import-by-query algorithm based ABox satisfiability oraclesapplicable set HT-rules Rv TBox Th ALCHIQ. assumptions madetype symbols : Rv reuse atomic concepts roles Th .5.1.1 HT-Safetydefine HT-safety condition allows us overcome negative resultsTheorems 7 8, also guarantees semantic modularity required overcomenegative results Theorems 5 6. contains atomic concepts, HT-safetyreduces semantic modularity Rv w.r.t. .notion HT-safety Rv consists following building blocks. first identitysafe conceptsthat is, concepts private Rv propagatedmodels Th . Next, transform Rv reduct replacing Rv safe concepts, require reduct semantically modular w.r.t. . latter propertyensures interpretation symbols extended interpretationsymbols Rv interpreting safe concepts empty set. Finally, motivated221fiCuenca Grau & MotikSection 4.2, impose syntactic restriction HT-rule Rv : body atomR(x, y) R , require variables x guarded safe concept.Definition 6. Let Rv set HT-rules let signature. set safe conceptsRv smallest set safe(Rv , ) that, HT-rule Rv whose bodycontains atom form R(x, yi ) R(yi , x) R atom form A(x)A(yi ) , safe(Rv , ).reduct Rv w.r.t. set rules obtained Rv removing rulecontaining concept safe(Rv , ) body, removing headremaining rules atom containing concept safe(Rv , ).set Rv HT-safe w.r.t.1. reduct Rv w.r.t. semantically modular w.r.t. ,2. rule Rv body atom form R(x, yi ) R(yi , x)R , body contains atoms A(x) B(yi ) A, B safe(Rv , ).HT-safety invalidates proofs Theorems 7 8: knowledge bases Kv usedproofs two theorems HT-safe w.r.t. respective signatures .particular, consider Kv used proof Theorem 7. set HT-rules Rv obtainedTBox Kv shown below.B1 S.A1A2 B2R.B2 B2S.B2B1 (x) S.A1 (x)A2 (x) B2 (x)R(x, y) B2 (y) B2 (x)S(x, y) B2 (y)(23)(24)(25)(26)safe(Rv , ) = {B2 }. straightforward see reduct Rv w.r.t. , shownbelow, semantically modular w.r.t. = {A1 , A2 , R}.B1 (x) S.A1 (x)(27)A2 (x)(28)Consider Kv used proof Theorem 8. set HT-rules Rv obtainedTBox Kv shown below.B R.BA2 BB(x) R(x, y) B(y)A2 (x) B(x)(29)(30)safe(Rv , ) = {B}, reduct Rv w.r.t. empty thus semantically modularw.r.t. = {A1 , A2 , R}; however, first HT-rule satisfy Condition 2 Definition 6 since rule body contain atom form A(y) safe(Rv , ).Note that, contains atomic concepts, safe(Rv , ) = . reduct Rvw.r.t. equal Rv , Condition 1 Definition 6 holds Rvsemantically modular w.r.t. ; furthermore, Condition 2 vacuously holds Rv . Thus,HT-safety reduces semantic modularity w.r.t. atomic concepts shared.following proposition shows that, given interpretation symbols , obtainmodel Rv interpreting safe concepts empty set.222fiReasoning Ontologies Hidden ContentProposition 3. Let Rv set HT-rules HT-safe w.r.t. . Then,interpretation symbols , model J Rv exists J = , X J = Xsymbol X , X J = atomic concept X safe(Rv , ).Proof. Let interpretation symbols , let Rv reduct Rv w.r.t. Since Rv semantically modular w.r.t. , model Rv exists =X = X symbol X . Let J interpretation obtainedsetting X J = X safe(Rv , ). Consider arbitrary HT-rule Rv .safe(Rv , ) occurs body , AJ = clearly implies J |= . Otherwise,let Rv rule obtained removing head atoms contain safe concept;|= clearly implies J |= . Consequently, J |= Rv .Finally, note HT-safety syntactic condition; fact, checking HT-safetyundecidable general requires checking semantic modularity set HT-rulesw.r.t. signature. mentioned Section 2.3, however, several practically useful syntacticconditions known guarantee semantic modularity (Cuenca Grau, Horrocks, Kazakov, & Sattler, 2008), condition used obtain purely syntacticHT-safety notion.5.1.2 Acyclicitynegative result Theorem 9 relies Kv containing cyclic axiom R.AR . next present sucient condition detect cyclespolynomial time.test involves set function-free first-order formulae equality D(Rv , Av )whose consequences summarize models Rv Th Av ; precisely, projectioncanonical model Rv Th Av symbols sig(Rv ) homomorphicallyembedded set ground facts entailed D(Rv , Av ). Intuitively, since axiomsTh available, facts entailed D(Rv , Av ) reflect possible consequencesTh information derived using Rv Av . Theory D(Rv , Av ) also keepstrack paths visible part canonical models Rv Th Av usingtwo special binary predicates: Succ keeps track successorship relation domainelements, -Desc keeps track descendant relation via roles contained .acyclicity condition checks whether -Desc relation entailed D(Rv , Av )cyclic; case, establish bound length paths roles .Definition 7. Let Rv set HT-rules, let Av ABox, let signature.atomic concept sig(Rv ) sig(Av ), let vA vA individuals uniquely associatedA, respectively; furthermore, let Succ -Desc binary predicatesoccurring Rv Av . Function tt() maps atom occurring Rv Avconjunction atoms follows, z arbitrary term:tt(A(z)) = ;tt( n R.C(z)) = ar(R, z, vC ) tt(C(vC )) Succ(z, vC );tt() = atom form covered two cases.223fiCuenca Grau & MotikFurthermore, D(Rv , Av ) set function-free formulas first-order logicequality defined follows, variables implicitly universally quantified.assertion Av , set D(Rv , Av ) contains tt().individual c occurring Av atomic concept , set D(Rv , Av )contains A(c).HT-rule Rv form (1) 1 j n, set D(Rv , Av ) containsfollowing formula:tt(U1 ) . . . tt(Um ) tt(Vj )(31)atomic concept , set D(Rv , Av ) contains following formula:Succ(z1 , z2 ) A(z2 )(32)atomic roles R, R , set D(Rv , Av ) contains following formulae:R (z1 , z2 ) R(z1 , z2 )R (z1 , z2 ) R(z2 , z1 )(33)(34)(35)(36)(37)R(z, z1 ) R (z, z2 ) z1 z2R(z1 , z) R (z2 , z) z1 z2R(z1 , z) R (z, z2 ) z1 z2atomic role R , set D(Rv , Av ) contains following formulae:Succ(z1 , z2 ) R(z1 , z2 ) -Desc(z1 , z2 )-Desc(z1 , z2 ) -Desc(z2 , z3 ) -Desc(z1 , z3 )(38)(39)Set D(Rv , Av ) contains harmful cycle D(Rv , Av ) |= -Desc(vC , vC ) vC . Furthermore, Rv Av acyclic w.r.t. D(Rv , Av ) contain harmful cycle.set formulae D(Rv , Av ) straightforwardly transformed equivalent datalog program equality using well-known equivalences first-order logic;therefore, often refer D(Rv , Av ) datalog program.Acyclicity allows us express axioms 6 8 Table 3. Intuitively, acyclicityensures visible parts canonical forest models Rv Th Av containinfinite chains roles ; use property algorithm define suitableblocking condition. explain intuition means example. Let = {C, R, U }C concept R U roles, Av = {A(a)}, Rv contains following HTrules; corresponding formulae D(Rv , Av ) shown symbol. NoteRv HT-safe w.r.t. .A(x) R.B(x)A(x) S.C(x)A(x) R(x, vB ) B(vB ) Succ(x, vB ) (40)A(x) S(x, vC ) C(vC ) Succ(x, vC ) (41)224fiReasoning Ontologies Hidden ContentB, CUUB, C,EbURbA, CeRRcC,C,c(a) Model(b) Extended ModelB, CB, C,vBvB vCvDR RU UA, CB, C, EvBR RU UvAR RU UC,A, CvCvD(c) Acyc. CheckvAA, CvAC,(d) Acyc. Check (I)vCvD(e) Acyc. Check (II)Figure 1: Canonical Models AcyclicityA(x) S.D(x)S(x, y1 ) S(x, y2 ) y1 y2A(x) S(x, vD ) D(vD ) Succ(x, vD ) (42)S(x, y1 ) S(x, y2 ) y1 y2(43)C(x) D(x) S.A(x) C(x) D(x) S(x, vA ) A(vA ) Succ(x, vA ) (44)Consider also following hidden TBox expressed ALCHIQ:Th = { 1 R., R U , U. C }(45)Figure 1(a) shows canonical model Rv Th Av . Furthermore, Figure 1(c) showsground atoms entailed D(Rv , Av ) represented graph G solid arrows showroles R, U , S, dashed arrows show special predicate -Desc; clarity,atoms involving special predicate Succ included followingfigures. Note D(Rv , Av ) entails R(vA , vB ) R(a, vB ); atoms, togetherrules (34) (35), entail vA va ; consequently, vA va represented Figure 1(c)node. Structure G summarizes sense homomorphicallyembedded G. repetitive structure represented G cycle nodes vAvC via S; however, since shared symbol (i.e., ), give riseharmful cycle. Consequently, Rv Av acyclic w.r.t. , guaranteesvisible part model Rv Th Av contain R-chains unbounded length,regardless contents Th . Accordingly, canonical model Rv Th Av shownFigure 1(a) contains R-chains.225fiCuenca Grau & MotikNote, however, G overestimates canonical model I; example, G containsindividual vA instance C, reflected I. let usassume Rv Rv extended following HT-rule:A(x) C(x) R.C(x)A(x) C(x) R(x, vC ) C(vC ) Succ(x, vC )(46)canonical model Rv Th Av clearly Rv Th Av ; however,D(Rv , Av ) contains harmful cycle, shown Figure 1(d). Intuitively, D(Rv , Av ) providesus conservative overestimate canonical models, cases detectcycles really exist canonical models. necessary consequencefact acyclicity checked polynomial time.Definition 7, however, provides us sucient check. example, let Rv Rvextended following HT-rules:A(x) R.E(x)B(x) C(x) E(x) R.A(x)A(x) R(x, ) E(vE ) Succ(x, )B(x) C(x) E(x)R(x, vA ) A(vA ) Succ(x, vA )(47)(48)canonical model Rv Th Av ground atoms entailed D(Rv , Av ) shownFigures 1(b) 1(e), respectively. HT-rules Rv \ Rv enforce existenceinfinite R-chain, reflected harmful cycle (e.g., self-loop vA ).Acyclicity indeed checked polynomial time, shown next.Proposition 4. Acyclicity Rv Av w.r.t. checked polynomial time.Proof. Let D(Rv , Av ) specified Definition 7. number fresh individualsform vA vA clearly linear size Rv , Av , , size D(Rv , Av )polynomial size Rv , Av , .compute set positive ground atoms follow D(Rv , Av )polynomial time using forward chaining. predicates D(Rv , Av ) bounded arity,number atoms polynomial size D(Rv , Av ). straightforwardly implies claim proposition show that, given set facts ruleD(Rv , Av ), compute set entailed facts polynomial time. Rulesform (31) contain bounded number variables, set entailed factscomputed polynomial time simply considering possible mappings variablesindividuals. Assume form (31). number variables linearsize Rv , exponentially many mappings variables individuals.can, however, determine values x yi make body true follows.variable yi , let Pi binary relation initially contains pairs individualsoccurring D(Rv , Av ); relation eventually contain pairs values x yimake body true. remove Pi pairs satisfybody atoms contain variables x yi . Next, Pi Pj , removepairs c, c Pi c exists c, c Pj . considerconsequent atom ; contains variables x yi , infer ground atomsobtained replacing x c yi c c, c Pi ; contains variablesyi yj , infer ground atoms c c individual c exists c, c Pic, c Pj . clearly done polynomially many steps numberindividuals D(Rv , Av ) maximal number variables rule Rv .226fiReasoning Ontologies Hidden ContentTable 4: Additional Derivation Rulesindividual atomic concept exist1. indirectly blocked2. {A(s), A(s)} =A1 := {A(s)} A2 := {A(s)}.individuals atomic roles R, R exist1. neither indirectly blocked A,2. R (s, t) A,3. {R(s, t), R(s, t)} =A1 := {R(s, t)} A2 := {R(s, t)}.individuals atomic roles R, R exist1. neither indirectly blocked A,2. R (s, t) A,3. {R(t, s), R(t, s)} = ,A1 := {R(t, s)} A2 := {R(t, s)}.individuals s, s1 , s2 exist1. none s, s1 , s2 indirectly blocked A,2. {s1 s2 , s1 s2 } = ,3. atomic roles R, R exist3.1 {R(s, s1 ), R (s, s2 )}3.2 {R(s1 , s), R (s2 , s)}3.2 {R(s1 , s), R (s, s2 )}A1 := {s1 s2 } A2 := {s1 s2 }.1.2. connected component A| exists aTh , (A ) = fA1 := {}.A-cutR-cutR -cut-cut-rule5.1.3 Import-by-Query Algorithmnext present import-by-query algorithm applicable Rv Av HT-safeacyclic w.r.t. . algorithm modifies standard hypertableau algorithm follows.First, several cut rules nondeterministically guess relevant assertions involvingsymbols . Second, -rule checks whether guesses indeed consistentTh . Third, relaxed blocking condition ensures termination.Definition 8. Let C[C , RCv ACv , ThC ] class inputs RCv ACv acyclic w.r.t.C , RCv HT-safe w.r.t. C , ThC ALCHIQ TBox. ALCHIQ -algorithmtakes triple , Rv Av , Th C obtained modifying Definition 1 follows.Blocking. unnamed individual blocking-relevant if, predecessors,LA (s, ) = LA (s , s) = .Then, individual ABox assigned blocking status wayDefinition 1, dierence directly blocked if, addition conditionsDefinition 1, blocking-relevant.227fiCuenca Grau & MotikBAnmbRA, CBeRbRCAdcC,CA, CcfAcC,C(a) Clash-free ABoxf(b) ABox A|AnmfinAcfinC, EbRC, ECeRAdfinceC, EC, ERC, EC, EC, EC, EC, E(c) Saturation via RhBbRA, C, EBC, EC, EA, C, E RC, D, EcC, EC, EC, EeC, D, EfC, E(d) Extended ABox AfinFigure 2: Completeness ALCHIQ -algorithmDerivation Rules. derivation rules given Tables 2 4, A|ABox obtained removing assertion containing indirectly blocked individualassertion sig() .Section 5.1.4 show cut rules Table 4 needed knowTh expressed description logic ALC ALCHIQ. algorithmindeed import-by-query algorithm, show next.Theorem 11. ALCHIQ -algorithm import-by-query algorithm based ABoxsatisfiability oracles class inputs C[C , RCv ACv , ThC ] Definition 8. algorithm implemented runs N2ExpTime N , total numberoracle queries size query exponential N ,N = |Rv Av | + || input Rv , Av , .proof Theorem 11 lengthy quite technical, defer appendixnext discuss intuitions. derivation rules Table 2 clearly sound.228fiReasoning Ontologies Hidden ContentFurthermore, due acyclicity, chains assertions involving roles boundedlength, enables blocking ensures termination. next sketch completenessargument. particular, completeness need show existence clash-freeABox derivation rule applicable implies satisfiability input.Let clash-free ABox labeling leaf derivation , Rv Av , Th , let Rhset HT-rules corresponding Th . model Rv Th extendedmodel Rv Av Th , suces show satisfiability Rv Th .end, extend clash-free ABox Afin derivation rule standardhypertableau algorithm applicable Rv Rh Afin ; thus, Rv Afin Th satisfiable,since Afin Rv Th monotonicity. construction Afin proceedsfollows:1. split projection A| . particular, define Anm ABox containing assertions A| involving individuals reachable named individual;furthermore, nonblocked blocking-relevant individual t, defineABox containing assertions A| involving individuals reachable t.2. apply standard hypertableau algorithm Rh connectedcomponents Anm , Rh ; let Anmfin Afin clash-free ABoxeslabeling leaves respective derivations. -rule applicableABoxes exist.3. define Afin union A, Anmfin , Afin , assertions C(s)blocked blocker , C(s ) Afin , sig(C) sig(Rh ).Let us call individuals old, individuals introduced second stepnew ; observe following. (1) Due cut rules, second step cannotderive fresh assertions involving old individuals symbols without leadingcontradiction. (2) connected components Anm disjoint,HT-rules Rh applied Afin subsets correspond connectedcomponent Anm . (3) Due (1), HT-rule Rv become applicableassertions involving old individuals. (4) Due HT-safety, HT-rule Rvbecome applicable assertion Afin involves new individual. (5) Due (1)third step construction above, individual blocked A, Anmfin , Afin ,blocked Afin well. Then, (1)(5) imply derivation rule standardhypertableau algorithm applicable Rv Rh Afin , proves completeness.explain intuition example = {C, R}, Av = {A(a)}, Rv consistsHT-rules (40)(44), Th defined follows:Th = { R. C, C T.C, C E }(49)shown Section 5.1.2, Rv Av acyclic w.r.t. , ALCHIQ -algorithmapplicable. algorithm produces derivation leaf labeledABox shown Figure 2(a); readability, show neither negative assertionsassertions involving complex concepts. Individual f directly blocked c A,assertions C(a) C(d) introduced A-cut rule. construct Afin ,assertions containing symbol removed, resulting ABox A| shown229fiCuenca Grau & MotikR, UA, CB, CvBR, URURUC,RUvCvDRUE, CR, UvAA, CR, UFigure 3: Acyclicity Check Th ALCHIFigure 2(b). ABox split connected components Anm , Ac , Ad ; notec nonblocked blocking-relevant individuals. Next, Anm , Ac , Adcompleted w.r.t. Rh using standard hypertableau algorithm; Figure 2(c) showscresulting ABoxes Anmfin , Afin , Afin . Note C(a) C(d) consistentaxiom R. C Th . Finally, Afin obtained taking union A, Anmfin ,Acfin , Adfin , adding E(f ); latter f blocked c E(c) Acfin .result shown Figure 2(d); clearly, derivation rule standard hypertableaualgorithm applicable Afin .5.1.4 Hidden Ontology DLs ALC ALCHIQmain limitation acyclicity condition Definition 7 stems factmust anticipate possible consequences Th . acyclicity conditionsderivation rules Table 4 simplified hidden ontology knownexpressed description logic ALC ALCHIQ.Th known use cardinality restrictions, omit rules (35)(37)definition D(Rv , Av ), -cut rule Table 4 required.Th known use inverse roles, omit rules (34), (36), (37)definition D(Rv , Av ), R -cut rule required, Conditions 3.23.3 removed -cut rule.Th known use role hierarchies, omit rules (33) (34)definition D(Rv , Av ), R-cut Table 4 required, R -cut ruleneed applied R R same.simplifications allow approach applied wider range visible ontologies. example, consider set Rv consisting HT-rules (40)(44) (47)(48),obtained harmful cycle w.r.t. = {C, R, U }, shown Figure 1(e).Th known expressed ALCHI (and use cardinality restrictions),230fiReasoning Ontologies Hidden Contentomit formulas form (35)(37) definition D(Rv , Av ); ground atomsentailed D(Rv , Av ) shown Figure 3. change makes Rv Av acyclicw.r.t. : sure that, arbitrary hidden TBox expressed ALCHI,infinite R-chains need considered reasoning Rv .5.2 Practical Import-by-Query Algorithmsalgorithm presented Section 5.1.3 suited practical implementationderivation rules Table 4 incur huge amount nondeterminism. section,present practical import-by-query algorithms nondeterministic rules replaceddemand oracle calls, makes algorithms goal-oriented.algorithms make assumptions kinds symbols contained : atomicconcepts roles shared.5.2.1 Importing Horn Ontologiessection, present practical algorithm applies Th expressedHorn-ALCHIQ fragment ALCHIQ. well known Th transformedset Horn HT-rules. allows us eliminate nondeterministic cut rules,use ABox entailment oracle instead ABox satisfiability oracle, define oraclequery rules deterministically complete query ABox missing assertionsentailed Th A. algorithm issues oracle queries demand, goal orientedthus amenable implementation.Definition 9. Let C[C , RCv ACv , ThC ] class inputs RCv ACv acyclic w.r.t.C , RCv HT-safe w.r.t. C , ThC Horn-ALCHIQ TBox. Horn-ALCHIQ e algorithm takes triple , Rv Av , Th C obtained Definition 8 replacingderivation rules Table 4 Table 5.algorithm indeed import-by-query algorithm worst-case complexity algorithm non-Horn case.Theorem 12. Horn-ALCHIQ e -algorithm import-by-query algorithm basedABox entailment oracles class inputs C[C , RCv ACv , ThC ] Definition 9.algorithm implemented runs N2ExpTime N , total numberoracle queries size query also exponential N ,N = |Rv Av | + || input Rv , Av , .proof Theorem 12 obtained modification one Theorem 11given appendix.5.2.2 Import-by-Query ELsection, present import-by-query algorithm based ABox entailment oracleshandle case Kv Th expressed EL. setting,Theorems 5 7 provide clues features hinder existence import-by-queryalgorithm. particular, longer necessary Kv acyclic.algorithm based hypertableau framework, Kv first convertedset Rv EL-rules normalized ABox Av . Since EL allow inverse roles231fiCuenca Grau & MotikTable 5: Additional Derivation Rules Horn KBsconnected component A| , individual ,atomic concept {} exist1. indirectly blocked A,2. A(s) A,3. eTh , (A , A(s)) =A1 := {A(s)}connected component A| , individuals ,atomic roles R, R exist1. neither indirectly blocked A,2. R (s, t) R (t, s) ,3. R(s, t) A,4. eTh , (A , R(s, t)) =A1 := {R(s, t)}connected component A| individuals s, s1 , s2exist1. none s, s1 , s2 indirectly blocked A,2. s1 s2 A,3. atomic roles R, R exist3.1 {R(s, s1 ), R (s, s2 )}3.2 {R(s1 , s), R (s2 , s)}3.3 {R(s1 , s), R (s, s2 )} A,4. eTh , (A , s1 s2 ) =A1 := {s1 s2 }e -concepte -rolee -universal quantification, danger information propagating successorpredecessor; therefore, relax HT-safety condition shown Definition 10.Definition 10. Let Rv set EL-rules, let signature Then, Rv EL-safew.r.t.satisfies Condition 1 Definition 6,rule Rv body atom form R(x, yi ) R ,body contains atom B(yi ) B safe(Rv , ).algorithm takes set Rv EL-safe rules normalized ABox Av . appliesstandard EL hypertableau derivation rules; furthermore, like Horn-ALCHIQe -algorithm Section 5.2.1, uses oracle complete ABoxes encounteredderivation relevant concept assertions.Definition 11. Let C[C , RCv ACv , ThC ] class inputs RCv set EL-rulesEL-safe w.r.t. C , ACv normalized ABox, ThC EL TBox. ELe -algorithm takes triple , Rv Av , Th C obtained extending algorithmDefinition 2 e -concept derivation rule shown Table 5.232fiReasoning Ontologies Hidden Contentalgorithm indeed import-by-query algorithm, implemented runpolynomial time, shown following theorem. contrast algorithmspresented thus far, EL e -algorithm optimal amenable implementation.Theorem 13. EL e -algorithm import-by-query algorithm based ABox entailment oracles class inputs C[C , RCv ACv , ThC ] Definition 11. algorithmimplemented runs PTime N polynomial number Ncalls eTh , , N = |Rv Av | + || input Rv , Av , .proof Theorem 13 rather technical lengthy, given appendix.intuition behind proof, however, case ALCHIQ algorithm, dierences due fact ABoxes produced ELe -algorithm specific shape.6. Lower Bound Complexity Import-by-Query Reasoningsection show import-by-query algorithm handles inputALCHIQ -algorithm make polynomial number (in ||) queriespolynomial size (in ||). result applies already contains atomicconcepts, requirement ALCHIQ -algorithm applicableTBox Kv semantically modular w.r.t. .Theorem 14. Let C[C , KvC , ThC ] class inputs C contains atomic concepts, KvC ALCHIQ knowledge base semantically modular C , ThCALCHIQ TBox. Then, import-by-query algorithm ibqa based ABox satisfiabilityoracles L = ALCHIQ C exists that, input , Kv , Th C, totalnumber oracle queries possible runs ibqa [Th , , L] Kv , well sizequery, polynomial ||.Proof. Assume ibqa algorithm satisfies theorems assumptions; then,integers c1 c2 exist that, input , Kv , Th C, total number oraclequeries possible runs ibqa [Th , , L] Kv smaller equal ||c1 ,maximal size query ABox smaller equal ||c2 .next construct particular input C show ibqa violatesassumption. Let k arbitrary integer k c1 +c2 < 2k ; k exists since c1c2 fixed. Let = {A1 , . . . , Ak } arbitrary atomic concepts, let Z, B, C1 , . . ., Ck ,C 1 , . . ., C k atomic concepts occurring . Then, define Kv = Tv AvAv = {Z(a)} Tv contains following axioms:(50)B R.BZ B C1 . . . Ck(51)(C1 R.C 1 ) (C 1 R.C1 )(53)Cj C j1jkCj1 R.C j1 (Cj R.C j ) (C j R.Cj ) 1 < j kC j1 (Cj1 R.Cj1 ) (Cj R.Cj ) (C j R.C j ) 1 < j kCi Ai1ik233(52)(54)(55)(56)fiCuenca Grau & MotikC Ai1ik(57)TBox Tv uses well-known integer counting technique (Tobies, 2000). Considerarbitrary model Kv . Domain elements assigned integers 02k 1 means 2k atomic concepts C1 , . . ., Ck , C 1 , . . ., C k . Axiom (51) impliesaI (C k . . . C 1 )I , initializes counter 0. Axiom (50) ensures aIorigin infinite R-chain. Axioms (52) ensure domain element chainlabeled Cj C j . Axioms (53), (54), (55) increment counter R.Finally, axioms together axioms (56) (57) ensure possible number0 2k 1 assigned domain element R-chain. Clearly, Tvsemantically modular w.r.t. since extend interpretation symbolsmodel Tv interpreting symbols empty set.Let Th1 = , let A1 , . . . , query ABoxes occurring possible runsibq [Th1 , , L] Kv , let n maximal size Ai 1 m. assumptions,k c1 n k c2 , implies n = k c1 +c2 < 2k due way chosek. 1 m, let Ai following ABox equivalent Ai :Ai unsatisfiable, Ai = {}.Ai satisfiable, let Ai ABox contains individual exactly oneconcept assertion form D(s) disjunctive normal form; is,expressed disjunction concepts form ()A1 . . . ()Ak . Aiobtained Ai applying de Morgans laws.Let D1 , . . . , disjunctive concepts occur satisfiable ABox Ai . Aicontains n concepts, 1 n. Furthermore, let U subset{D1 , . . . , } containing precisely Di exactly one disjunct. Finally, letconcept form ()A1 . . . ()Ak occur U ; existsn < 2k . let Th2 following TBox:Th2 = {S }(58)next show that, 1 j , concept Dj satisfiable w.r.t. Th2 .claim trivial Dj contain S; otherwise, Dj contains disjunct = S,interpretation satisfying Th2 Dj obtained interpreting nonempty set.next show 1 ,,L (Ai ) = 2 ,,L (Ai ) 1 m; since Ai Aihhequivalent, 1 ,,L (Ai ) = 2 ,,L (Ai ) well. statement clearly holds Aihhunsatisfiable, assume Ai satisfiable. Since Ai consists assertions formD(s) satisfiable w.r.t. Th2 , interpretation satisfying Ai Th2 obtaineddisjoint union interpretations satisfying D.Proposition 1, runs ibqa [Th1 , , L] Kv coincide runsibq [Th2 , , L] Kv ; however, straightforward see Kv Th1 satisfiable, whereasKv Th2 unsatisfiable, contradiction.7. Related Workcurrently growing interest techniques hiding parts ontology Th . Onepossible approach hide subset signature Th first extracting Th 234fiReasoning Ontologies Hidden Contentmodule subset Th preserves -consequences (i.e., logical consequencesformed using symbols )and publishing ontology Th \ . orderensure sensitive information disclosed, moduledepleting (Kontchakov, Pulina, Sattler, Schneider, Selmer, Wolter, & Zakharyaschev,2009)that is, ontology Th \ indistinguishable empty ontologyw.r.t. -consequences. approach ensures -consequences disclosedexternal applications oers additional advantage one reason unionKv Th \ using o-the-shelf DL reasoners. Finally, although determining whethersubset ontology depleting module signature undecidable problemmany DLs (and hence extraction minimal depleting modules often computationallyinfeasible), several practical techniques extracting (not necessarily minimal) depletingmodules known (Cuenca Grau, Horrocks, Kazakov, & Sattler, 2008).important disadvantage approach module may also containrelevant information sensitive (e.g., may entail consequences symbols) hence union Kv (which may use symbols ) Th \ maycontain enough information answer relevant queries. Furthermore, adoptingapproach, vendor Th would distribute subset axioms Th , may allowcompetitors plagiarize parts Th . Finally, published axioms might mention symbols(even entail -consequence) external applications wouldaware presence symbols ontology.drawbacks overcome publishing -interpolant Thontology contains symbols coincides Th logical consequences formed using symbols (Konev et al., 2009; Wang et al., 2009, 2008;Lutz & Wolter, 2011; Nikitina, 2011). contrast module extraction approach, publishing interpolant ensures sensitive information Th (i.e., informationsymbols Th mentioned interpolant) exposed wayexternal applications; furthermore, interpolants preserve consequences symbols. Similarly module extraction approach, using interpolation additionaladvantage developers Kv reason union Kv interpolantusing o-the-shelf DL reasoners.interpolation approach may, however, several drawbacks. First, interpolantmay exist Th expressed relatively weak DL satisfies certain syntacticconditions (Konev et al., 2009). contrast, import-by-query often possible eveninterpolant Th signature interest exist.Second, although interpolants preserve logical consequences formed using symbols, robust replacement (Sattler et al., 2009)that is, union Kv-interpolant Th guaranteed yield consequences Th Kvquery q involving symbols . example, given = {R} Th = {A R.B},empty ontology -interpolant (it preserves consequences form CC arbitrary boolean concepts signature {A, B}); however, Kv = {B }Kv Th entails consequence , whereas union Kv(empty) interpolant not. Thus, interpolant published, cannotimported Kv guarantee relevant consequences preserved, unlesssuitable restrictions imposed Kv .235fiCuenca Grau & MotikFinally, -interpolant Th exponentially larger Th , may revealinformation strictly needed. Although import-by-query algorithmsalso formulate worst-case exponentially many queries oracle, algorithmsmay limit flow irrelevant information Th Kv , especially Th expressedHorn DL, case import-by-query algorithms issue queries demand.example, = {R, C}, = , Kv = {A R.B, B C} Th = {R.R.C C},-interpolant equal Th thus publishing interpolant reveals entire contents Th .contrast, import-by-query algorithm EL would reveal positive informationTh , would disclose fact ABox form {R(a, b), C(b)}satisfiable w.r.t. Th .idea accessing ontology oracle similar spirit proposalCalvanese, De Giacomo, Lembo, Lenzerini, Rosati (2004) query answering peerto-peer setting. authors consider problem answering conjunctive query qKBs Kv Kh mappings reformulating q queries evaluatedKv Kh isolation. query reformulation algorithm accesses Kv , qanswered using oracle Kh . setting, however, focus reusedata, rather schema. Since satisfiable Kh cannot aect subsumption conceptsKv , results Calvanese et al. (2004) applicable schema reasoning.8. Conclusionpaper, proposed studied import-by-query framework. resultsprovide flexible way ontology designers ensure selective access ontologies.framework thus provides key theoretical insights issues surrounding ontologyprivacy. Furthermore, believe algorithms practicable applied Hornontologies; thus, results provide starting point development practical importby-query systems.problem import-by-query novel, see many open questions. example,problem relevant theory practice allow hidden ontologyselectively export data schema statements.Acknowledgmentsextended version paper Import-by-Query: Ontology ReasoningAccess Limitations Bernardo Cuenca Grau, Boris Motik, Yevgeny Kazakov published IJCAI 2009 paper Pushing Limits Reasoning OntologiesHidden Content Bernardo Cuenca Grau Boris Motik published KR 2010.research supported Royal Society EPSRC projects ExODA(EP/H051511/1) HermiT (EP/F065841/1).236fiReasoning Ontologies Hidden ContentAppendix A. Proof Theorem 11use following definitions intermediate results prove theorem.Definition 12. ABox HT-ABox assertions satisfy followingconditions, B atomic negated atomic concept, role, R atomic role,b named individuals, individual, j integers.1. concept assertion form B(s) n S.B(s).2. role assertion form R(a, b), R(s, s.i), R(s.i, s).3. individual s.i occurs assertion A, contains role assertionform R(s, s.i) R(s.i, s).4. equality form s.i s.j, s.i.j s, s, b.i.Furthermore, extended HT-ABox additionally allowed contain assertionsform R(s, s) s.i s.Lemma 1. Let R set HT-rules let ABox. Then, ABox labelingnode derivation R HT-ABox.Proof. proof straightforward modification proof Lemma 4 Motik et al.(2009), due following observations: since HT-rules allow atomsform R(x, x) head, one cannot derive atoms form R(s, s); this, turn,guarantees one cannot derive equalities form s.i s.Lemma 2. (Motik et al., 2009, Lemma 6) Let R set HT-rules let clashfree extended HT-ABox containing indirectly blocked individuals. derivation ruleapplicable R A, R satisfiable.Definition 13. weakened pairwise anywhere blocking, abbreviated w-blocking,Definition 1, dierence following condition used insteadLA (s ) = LA (t ):HT-rule R containing body atom form R(x, y) R(y, x)R atomic role R LA (s, ) LA (s , s), atomicconcept occurring , LA (s ) LA (t ).Lemma 3. Lemma 2 holds even derivation R uses w-blocking.Proof (Sketch). Let ABox labeling leaf derivation R A; letindividual blocked w-blocking; let parentst. proof Motik et al. (2009, Lemma 6) hold, must show HT-ruleapplicable interpretation obtained unraveling . Let R arbitraryHT-rule. contain body role atom role R LA (s, ) LA (s , s),Hyp-rule cannot applied mapping (x) = s. Furthermore,contain atomic concept A, fact LA (s ) LA (t ) vice versacannot aect applicability . Thus, straightforward modification proofMotik et al. (2009, Lemma 6), construct model R unraveling .straightforward see derivation rules Table 4 invalidate Lemma1that is, given HT-ABox, always produce HT-ABox.237fiCuenca Grau & MotikA.1 Terminationfirst show logical consequences datalog program D(Rv , Av ) Definition 7 overestimate ABoxes produced hypertableau algorithm; is,show ABox (t) labeling derivation node homomorphically embeddedset ground facts entailed D(Rv , Av ).= s.i either R(s, ) (t) R(s , s) (t) R , say-successor s.Lemma 4. Let Rv set HT-rules, let Av ABox, let signature, letD(Rv , Av ) given Definition 7, let Th ALCHIQ TBox, let (T, )derivation , Rv Av , Th . Then, derivation node , mappingindividuals (t) individuals D(Rv , Av ) exists satisfying followingproperties individuals occurring (t):1. A(s) (t) atomic concept implies D(Rv , Av ) |= A((s)).2. R(s, ) (t) implies D(Rv , Av ) |= R((s), (s )).3. successor (t), D(Rv , Av ) |= Succ((s), (s )).4. -successor (t), D(Rv , Av ) |= -Desc((s), (s )).5. n R.C(s) (t) R possibly inverse role, following conditions hold:(a) C atomic concept, D(Rv , Av ) |= C(vC );(b) D(Rv , Av ) |= ar(R, (s), vC );(c) D(Rv , Av ) |= Succ((s), vC );(d) R , D(Rv , Av ) |= -Desc((s), vC ).6. (t), D(Rv , Av ) |= (s) (s ).7. unnamed individual (t), atomic concept sig(Rv ) sig(Av ) exists(s) = vA (s) = vA .Proof. prove lemma induction structure derivation.root node derivation, let map individual Av itself. ABox () = Avtrivially satisfies Properties 3, 4, 7 since Av contains named individuals. Properties5 6 also hold trivially () normalized ABox hence containassertions form n R.C(s) form . Finally, Properties 1 2 hold() D(Rv , Av ).induction step, assume that, derivation node , ABox (t) satisfiesclaim mapping . child node , consider possibleways (t ) derived (t).-rule: properties hold trivially (t ) .238fiReasoning Ontologies Hidden ContentA-cut: properties hold trivially (t ) except Property 1 case(t ) = (t) {A(s)} . named individual (t), occurs Av Property 1 holds D(Rv , Av ) contains assertion A(s)occurring Av . unnamed, successor individual (t); induction hypothesis (Property 3)D(Rv , Av ) |= Succ((s ), (s)); however, D(Rv , Av ) contains formula (32), D(Rv , Av ) |= A((s)), required.R-cut: properties hold trivially (t ) except Property 2 case(t ) = (t) {R(s, )} R . Condition 2 R-cut R (s, ) (t)atomic role R , D(Rv , Av ) |= R ((s), (s )) inductionassumption. Since R, R D(Rv , Av ) contains formulae (33) roles ,D(Rv , Av ) |= R((s), (s )), (t ) satisfies Property 2 .R -cut: properties hold trivially (t ) except Property 2 case(t ) = (t) {R(s , s)} R . Condition 2 R -cut R (s, ) (t)atomic role R , D(Rv , Av ) |= R ((s), (s )) inductionassumption. Since R, R D(Rv , Av ) contains formulae (34) roles ,D(Rv , Av ) |= R((s ), (s)), (t ) satisfies Property 2 .-rule: properties hold trivially (t ) .-rule: Assume (t ) defined follows, n R.C(s) (t), si freshsuccessors s, C possibly negated atomic concept:(t ) = (t) { ar(R, s, si ), C(si ) | 1 n } { si sj | 1 < j n }Let = {si vC | 1 n}. Properties 5 6 hold trivially (t ) ,obvious Property 7 holds well. Hence, focus showing Properties 14. Property 1, assume C atomic concept; since Property5(a) holds (t) induction assumption, D(Rv , Av ) |= C(vC ),required. Property 2, since Property 5(b) holds (t) induction assumption, D(Rv , Av ) |= ar(R, (s), vC ), required. Property3, since Property 5(c) holds (t) induction assumption,D(Rv , Av ) |= Succ((s), vC ), Property 3 holds (t ) . Property 4,assume R ; Property 5(d) holds (t) induction assumption,D(Rv , Av ) |= -Desc((s), vC ), Property 4 holds (t ) .Hyp-rule: Assume (t ) = (t) {} head atom HT-ruleform (2). Properties 3, 4, 7 hold trivially (t ) , focusremaining properties. Condition 2 Hyp-rule, (t) contains individualss, s1 , . . . , sn statements left column following tableholds. then, induction assumption, statements right columnhold well.Ai (s) (t)Rij (s, si ) (t)Sij (si , s) (t)Bij (si ) (t)D(Rv , Av ) |= Ai ((s))D(Rv , Av ) |= Rij ((s), (si ))D(Rv , Av ) |= Sij ((si ), (s))D(Rv , Av ) |= Bij ((si ))239fiCuenca Grau & MotikHT-rule , datalog program contains rule (31). Thus, statementsfollowing table hold well:D(Rv , Av ) |= tt(Ci ((s)))((s), (s ))D(Rv , Av ) |= Rij((s ), (s))D(Rv , Av ) |= SijD(Rv , Av ) |= Dij ((si ))D(Rv , Av ) |= (si ) (sj )Consequently, Properties 2 6 clearly hold; Property 1 also holds sinceatomic concept atom tt() = . show Property 5, assume Ci ((s))form n R.C((s)),tt(Ci ((s))) = ar(R, (s), vC ) tt(C(vC )) Succ((s), vC ).Then, following holds:D(Rv , Av ) |= ar(R, (s), vC )D(Rv , Av ) |= tt(C(vC ))D(Rv , Av ) |= Succ((s), vC )Thus, Properties (5a), (5b), (5c) hold. Finally, R , Property (5d) holdsdatalog program entails assertion Succ((s), vC ), contains formulae(38) (34) roles .-cut rule: Assume (t ) = (t) {} assertion form s1 s2s1 s2 . Then, (t) trivially satisfies Properties 15 7 . Property 6 alsoholds trivially form s1 s2 , assume form s1 s2 .preconditions -cut rule, individual (t) atomic roles R, Rexist{ R(s, s1 ), R (s, s2 ) } (t){ R(s1 , s), R (s2 , s) } (t){ R(s1 , s), R (s, s2 ) } (t).induction hypothesis (Property 2),D(Rv , Av ) |= { R((s), (s1 )), R ((s), (s2 )) }D(Rv , Av ) |= { R((s1 ), (s)), R ((s2 ), (s)) }D(Rv , Av ) |= { R((s1 ), (s)), R ((s), (s2 )) }.then, since datalog program contains formulas (35)(37) roles ,D(Rv , Av ) |= (s1 ) (s2 ), required.-rule: Assume (t ) = merge(t) (s ). Then, Conditions 1 2-rule, (t) = . Furthermore, induction assumption,D(Rv , Av ) |= (s) (s ). Since merging merely replaces , semanticsequality (t ) satisfies required properties.next use Lemma 4 prove length chains role assertions involvingrole bounded.240fiReasoning Ontologies Hidden ContentLemma 5. Let Rv , Av , , D(Rv , Av ), (T, ) Lemma 4 additionalrestriction Rv Av acyclic w.r.t. . Let N number individualsform vC occurring D(Rv , Av ), let arbitrary derivation node (T, ), lets1 , . . . , unnamed individuals occurring (t) si+1 -successor si1 < . Then, N .Proof. Assume that, integer > N , unnamed individuals s1 , . . . , satisfyingconditions lemma exist, let mapping satisfying Lemma 4. Property 7Lemma 4, 1 (si ) = vCi Ci (because si unnamed).Furthermore, Property 4 Lemma 4, also D(Rv , Av ) |= -Desc((si ), (si+1 ))1 < . then, since > N predicate -Desc(x, y) axiomatizedtransitive formula (39) D(Rv , Av ), clearly obtain harmful cycle,contradiction.ready prove main claim.Lemma 6 (Termination). Let Rv , Av , , D(Rv , Av ), N , (T, ) Lemma 5.Then, (T, ) finite.Proof. Let depth individual number ancestors, let c rnumbers atomic concepts roles, respectively, occurring Rv Av ; finally, let= (22cr + 1)(N + 1) + 1. Consider arbitrary derivation node . Letindividual (t) depth i(N + 1) + 1. simple induction i, one showleast ancestors blocking-relevant. induction base straightforward= 0; furthermore, induction step holds because, Lemma 5 fact(t) HT-ABox, depth nearest blocking-relevant ancestorN + 1 less depth s. Thus, individual depth least 22cr + 1blocking-relevant ancestors; since 22cr possible concept role labelingsindividual predecessor, one blocking ancestors blocked duedefinition blocking; hence, either directly indirectly blocked (t). restproof claim analogous proof Lemma 7 Motik et al. (2009).A.2 SoundnessLemma 7 (Soundness). Let Rv set HT-rules, let Th ALCHIQ TBox, letABox Rv Th satisfiable, let A1 , . . . , ABoxes obtainedapplying derivation rule Table 2 4 Rv A. Then, Rv Th Ai satisfiable1 n.Proof. Let model Rv Th A, let us consider possible derivation rulesderive A1 , . . . , . cases Hyp-, -, -, -ruleproof Motik et al. (2009, Lemma 5). Furthermore, law excluded middlefirst-order logic, claim true A, R-cut, R -cut -cut rules. Assume-rule derives is, Th unsatisfiable connected componentA| . then, since A| A, monotonicity first-order logic Rv Thunsatisfiable well, contradiction.241fiCuenca Grau & MotikA.3 CompletenessDefinition 14 Proposition 5 show part model implied Thalways extended model Rv . say assertion atomic formA(a) atomic concept, R(a, b) R atomic role.Definition 14. Let signature, let Rv set HT-rules, let nonemptyclash-free ABox containing exactly one individual sig(A) . ABoxRv -extension w.r.t. following conditions hold:1. contains exactly one individual, | = A, sig(A ) sig(Rv );2. derivation rule Table 2 applicable Rv ;3. contain assertion form A(s) safe(Rv , ).Proposition 5. , Rv , Definition 14 Rv additionally HTsafe, least one Rv -extension w.r.t. exists.Proof. Let individual occurring A, let = (I , ) interpretationsymbols defined follows:{s} A(s){s, s} R(s, s)= {s}AI =RI =otherwiseotherwiseSince Rv HT-safe w.r.t. sig(A) , Proposition 3 model J Rv existsJ = , X J = X symbol X , X J = X safe(Rv , ).define ABox follows:= {s s}{A(s) | AJ sig(Rv )}{A(s) | AJ sig(Rv )}{R(s, s) | s, RJ R sig(Rv )}{ 1 R.A(s) | ( 1 R.A)J {R, A} sig(Rv )}{ 1 R.A(s) | ( 1 R.A)J {R, A} sig(Rv )}show Rv -extension w.r.t. . Since J coincidesinterpretation atomic concepts roles , satisfies Properties 1 3Definition 14. next show hypertableau derivation rule applicableRv . - -rule clearly applicable . Furthermore, constructionensures 1 R.C(s) {R(s, s), C(s)} , -ruleapplicable either. Finally, assume Hyp-rule applicable HT-ruleRv mapping . Since contains individual s, mappingmaps variables s. Since J |= Rv , rule contains head atom VjJ |= (Vj ). Note Vj form n R.C, n = 1 since J containsone element. Thus, (Vj ) form A(s), R(s, s), 1 R.C(s), s,sig(Rv ), R sig(Rv ), sig(C) sig(Rv ). then, construction(Vj ) , contradicts assumption Hyp-rule applicable Rv.242fiReasoning Ontologies Hidden Contentready prove main claim section.Lemma 8 (Completeness). Let , Rv Av , Th input ALCHIQ -algorithm.derivation , Rv Av , Th contains leaf node labeled clash-free ABox,Rv Av Th satisfiable.Proof. Let ABox obtained clash-free ABox labeling leaf derivation, Rv Av , Th removing assertions involving indirectly blocked individual. SinceRv Av acyclic w.r.t. , ABox finite Lemma 6. Furthermore, clearly HTABox derivation rule applicable Rv , A, aTh , . Finally, straightforwardsee mapping h individuals Av individuals existsh(a) = individual occurring A, C(a) Av implies C(h(a)) A,R(a, b) Av implies R(h(a), h(b)) A. Hence, model Rv Th extendedmodel Rv Av Th interpreting individual occurring Avway h(a). Thus, prove lemma showing Rv Th satisfiable.Let Rh result transforming Th set HT-rules described Motiket al. (2009); then, Rv Av Th equisatisfiable Rv Av Rh , modellatter model former well. Therefore, rest proof extendclash-free extended HT-ABox Afin derivation rule Table 2 applicableRv Rh Afin . Lemma 3, Rv Afin Rh satisfiable, which, togetherAfin , implies satisfiability Rv Rh . proceeding constructionAfin , next introduce several useful definitions notational conventions.Let v = sig(Rv ) sig(Av ) let h = sig(Rh ).proof, term blocking refers version blocking given Definition8; term w-blocking refers version blocking Definition 13; term sblocking refers standard blocking given Definition 1 additionalrequirement individuals s, , t, unnamed.blocked individual s, pick arbitrary fixed individual blockss, call blocker s.modified hypertableau algorithm standard hypertableau algorithm Definition 1 dierence uses s-blockingapplied ABoxes contain unnamed individuals; individualstreated algorithm named. modified hypertableau algorithm clearly sound, complete, terminating.projection ABox set individuals ABox consisting exactlyassertions contain individuals S.proceed construction Afin . end, split A| ABoxesAnm follows; use ABoxes later construct Afin .ABox Anm projection A| set containing named individualsunnamed individuals connected named individual A| .243fiCuenca Grau & Motiknonblocked blocking-relevant individual A, ABox projectionA| set containing (unnamed) individuals connected A| .Let Anmder result taking clash-free ABox labeling leaf derivationRh Anm modified hypertableau algorithm removing assertionscontaining indirectly blocked individual; furthermore, nonblocked blockingrelevant individual A, let Atder obtained analogous way. ABoxes AnmderAtder exist aTh , (A ) = connected component Anm , aTh , (At ) =t, modified hypertableau algorithm sound, complete, terminating.Since supply unnamed individuals unlimited, assume without loss generality-rule always introduces individuals globally freshthat is,occur ABox.next extend Anmder Ader assertions necessary satisfy Rv . Letnmnm(resp. ) let Ader Ader (resp. corresponding Ader ). sayindividual u fresh Ader u occurs Ader . freshindividual u Ader , define Ader [u] Rv -extension projection Ader | {u};without loss generality, assume Ader [u1 ] = Ader [u2 ] u1 u2projections Ader | {u1 } {u2 } isomorphic (i.e., identical renamingindividuals). Finally, let Afin union Ader Ader [u] u freshAder ; thus, obtain ABoxes Anmfin Afin . Condition 1 Definition 14, atomicassertions Ader |sig(Rh ) coincide atomic assertions |sig(Rh ) . Furthermore,since individuals involved s-blocking required unnamed isomorphicindividuals extended way, construction aect s-blockingthatis, u s-blocked u s-blocked Ader .define Afin ABox obtained1. taking union A, Anmfin , Afin nonblocked blocking-relevant individualA,2. adding A(s) blocked individual blocker A(s ) Asfinh .3Lemma 1, Anmfin Afin HT-ABoxes, Afin clearly extended HT-ABox.next show hypertableau derivation rule applicable Rv Rh Afin .end, first show Afin satisfies following property (*): Afinatomic assertion assertion form b sig() v individualsmentioned occur A, A. particular, note extension AnmdernmAder Afin Afin , respectively, introduce atomic assertion involvesindividual sig() (v \ ) = ; hence, possibilityAfin , A, sig() v Anmder Ader t. consider nextformer case; latter one analogous. prove (*) induction applicationderivation rules construction Anmder . end, show ABoxderivation Anm Rh satisfies following properties:3. Note that, since blocked, blocking-relevant.244fiReasoning Ontologies Hidden Content1. atomic assertion assertion form b sig() vindividuals mentioned occur A, A.2. R(a, b) b occur R h \ , existsS(a, b) S(b, a) A.3. b occurs A, R individual c occurringexist R(a, c) R(c, a) A.base case trivial. next consider ways assertion derived.application -rule -rule clearly preserves (1)(3). application-rule, modified hypertableau algorithm treats individuals named;furthermore, b b occur A, (1) b A, = b since-rule applicable A; then, straightforward see (1)(3) remainpreserved. Finally, following types assertions relevant applicationHyp-rule HT-rule Rh :A(a) . Since A-cut rule applicable A,A(a) A(a) A, (1) holds.R(a, b) b A. body contains atom matchedassertion R (a, b) R (b, a) R v satisfies inductionassumption; thus, exists S(a, b) S(b, a) A, (2) holds.Furthermore, R , assertion satisfies preconditions R-cutR -cut rule; since rules applicable A, R(a, b)R(a, b) A, (1) holds.b A. body contains atom matched assertionR (a, c) R (c, a) R v satisfies induction assumption;thus, exists S(a, c) S(c, a) A, (3) holds. Furthermore,b A, body also contains atom matched assertionR (a, c) R (c, a) satisfies induction assumption; thus,exists (a, c) (c, a) A. precondition -cut rulesatisfied and, since rule applicable A, b b A,(1) holds.completes proof (1)(3). Property (*) straightforward consequence (1):derivation assertion sig() v individuals mentioned occureither makes dierence leads contradiction. straightforward consequence(*) (59) (60) hold individuals u v occur A:LAfin (u) v = LA (u)LAfin (u, v) v = LA (u, v)(59)(60)show derivation rule hypertableau algorithm w-blockingapplicable Rv Rh Afin . considering possible derivation rules.(-rule) Assume -rule applicable assertion n R.C(s) Afin ,w-blocked Afin . show blocked A, s-blockedAnmfin , s-blocked Afin . following cases.245fiCuenca Grau & Motikn R.C(s) A. Assume blocked blocker t, letpredecessors t, respectively. definition blocking, (61)(65) hold:LA (s) = LA (t)(61)LA (s ) = LA (t )(62)LA (s, ) = LA (t, )LA (s , s) = LA (t , t)LA (s, ) LA (s , s) v \(63)(64)(65)(59) (60), following properties hold well:LAfin (s) v = LAfin (t) vLAfin (s ) v = LAfin (t ) v(66)(67)Furthermore, second item construction Afin ensures LAfin (s)LAfin (t) coincide concept C h , ensures following property:LAfin (s) = LAfin (t)(68)(65), A| contains assertion involving individuals , individuals . construction Afin , following properties hold:LAfin (s, ) = LAfin (t, )LAfin (s , s) = LAfin (t , t)(69)(70)Consider rule Rv Rh . Rh , role body occursLAfin (s, ) LAfin (s , s), satisfies condition weakened pairwise anywhereblocking. Rv , satisfies condition weakened pairwise anywhereblocking due (67). Together (68)(70), implies w-blocked t,contradiction. Consequently, blocked A.n R.C(s) Anmfin n R.C(s) A. occurs successorindividual occurs A, s-blocked Anmfin since modified hypertableau algorithm treats individuals occurring named individualscannot s-blocked. Otherwise, construction Afin , LAfin (u) = LAnm(u)finnm A;LAfin (u, v) = LAnm(u,v)individualsuvoccurringfinfinagain, s-blocked Anmfin .n R.C(s) Atfin n R.C(s) A. case completely analogousprevious one.Let ABox property holds; note n R.C(s) .-rule applicable , contains individuals u1 , . . . , un{ar(R, s, ui ), C(ui ) | 1 n} {ui uj | 1 < j n} .construction Afin Afin , contradicts assumption-rule applicable Afin .246fiReasoning Ontologies Hidden Content(-rule, first variant) Property (59) holds individual occurring A, (71)(72) hold individual occurring Anmfin Afin , respectively.LAfin (s) h = LAnm(s) hfinLAfin (s) h = LAt (s) hfin(71)(72)Thus, {A(s), A(s)} Afin implies {A(s), A(s)} , A, Anmfin ,Afin . Since first variant -rule applicable , applicableAfin either.(-rule, second variant) Property (60) holds pair individuals occurringA. Furthermore, Anmfin Afin contain negative assertions alreadypresent A. Since second variant -rule applicable A, Anmfin ,Afin , applicable Afin either.(-rule, third variant) Suppose -rule applicable assertion formAfin . construction Afin , A, Anmfin , Afint. then, since -rule applicable , applicable Afin either.(-rule) Assume -rule applicable Afin . Then, assertionAfin exists = . construction Afin , , = A,= Anmfin , = Afin t. then, since -rule applicable ,applicable Afin either.(Hyp-rule) Assume Hyp-rule applicable Afin HT-rule Rv Rhform (2). Thus, mapping variables individuals Afin exists(Ui ) Afin 1 m, (Vj ) Afin 1 j n. Let = (x)ui = (yi ). following possibilities:Rh . Let ABox chosen among Anmfin Afin containing individual s.Consider ui . contains atom form Rij (x, yi ) Rij (yi , x)Rij h , Afin contains assertion form Rij (s, ui ) Rij (ui , s).definition blocking, pair individuals u v belong dierent Anm, ABox contain assertion form (u, v) h ;then, construction Afin , u v belong dierent Anmfin Afin ,ABox Afin contain assertion either. Thus, ui occur ,Hyp-rule applicable , contradiction.Rv . first show following property (**): ui occurA, = uj uj . consider first case occur A.Consider arbitrary uj . Since HT-rule, body contains atomform Rjk (x, yj ) Rjk (yj , x), Afin contains assertion form Rjk (s, uj )Rjk (uj , s). following two possibilities Rjk .Rjk v \. construction Afin , assertion Rjk (s, uj ) Rjk (uj , s)occurring must introduced via Rv -extension, uj = s.Rjk . Since HT-safe w.r.t. , contains atom form A(x)safe(Rv , ) body. Condition 3 Definition 14, A(s) Afin ,contradiction.247fiCuenca Grau & Motikcase ui occur symmetric; dierencecase Rjk consider body atom B(yj ) B safe(Rv , ).Let = occurs A, let ABox contains otherwise.straightforward consequence (**) (Ui ) 1 m; furthermore, Afin (Vj ) Afin imply (Vj ) 1 j n. then,Hyp-rule applicable , contradiction.Thus, derivation rule hypertableau algorithm w-blocking applicableRv Rh Afin , Rv Rh Afin satisfiable Lemma 3. explained earlier,proves claim lemma.Lemmas 6, 7, 8 immediately imply Theorem 11.Appendix B. Proof Theorem 12termination argument Horn-ALCHIQ e -algorithm analogous nonHorn case: derivation , Rv Av , Th , node derivation,find embedding Lemma 4; proof straightforward variantproof given non-Horn case. Termination follows exactly non-Horncase. Soundness consequence soundness standard hypertableau algorithmtogether following lemma.Lemma 9. Let Rv set HT-rules, let Th Horn-ALCHIQ TBox, letABox Rv Th satisfiable. Furthermore, let A1 ABox obtainedapplying derivation rule Table 5 Rv A. Then, Rv Th A1 satisfiable.Proof. Let model Rv Th A, let us assume derivation ruleTable 5 derives A1 = {}. preconditions e -concept, e -role, e -rules, eTh , (A , ) = connected component A| , Th |= . SinceA| A, |= Th , |= , implies claim.show completeness algorithm. set HT-rules R Horn,derivation hypertableau algorithm contains exactly one leaf node,identify derivation sequence ABoxes A0 , . . . , . following propositionstraightforward consequence fact R Horn set HT-rules.Proposition 6. Let R set Horn HT-rules, let ABox, let A0 , . . . ,derivation R A. Then, assertion mentions individualsAi 1 n, R |= .Lemma 10 (Completeness). Let , Rv Av , Th input Horn-ALCHIQ e algorithm. derivation , Rv Av , Th contains leaf node labeled clash-freeABox, Rv Av Th satisfiable.Proof. proof analogous proof Lemma 8: given ABox labelingderivation leaf, construct ABox Afin derivation rule hypertableaualgorithm w-blocking applicable Rv Rh Afin . construction248fiReasoning Ontologies Hidden Contentbulk proof exactly Lemma 8, next prove propertiesaected dierence derivation rules.preconditions derivation rules Table 5 clearly ensure that, wheneverderivation rule applied HT-ABox, result also HT-ABox; consequently, Afinextended HT-ABox.next show property (*) holds despite change derivation rules:Afin atomic assertion assertion form b sig() vindividuals mentioned occur A, A. particular, noteconstruction Afin introduce atomic assertion involves individualsig() (v \ ) = . Assume sig() individualsoccur A. Proposition 6 Rh |= . Furthermore, sayLemma 8 one show preconditions e -concept, e -role, e - rulesatisfied A; since relevant rule applicable A, A, provesclaim.rest proof exactly Lemma 8.Theorem 12 follows immediately Lemmas 9 10.Appendix C. Proof Theorem 13set EL-rules R ABox A, derivation EL hypertableau algorithm contains exactly one leaf node, identify derivation sequence ABoxesA0 , A1 , . . . , . Since Aj1 Aj 1 n, ABox labeling derivation leafuniquely defined R A. following lemma captures relevant propertiesstandard EL hypertableau algorithm, proved slight variationproofs Motik Horrocks (2008) Baader et al. (2005).Lemma 11. Let R set EL-rules, let ABox containing named individuals, let Af ABox labeling leaf derivation R A. followingproperties hold pair atomic concepts A, B sig(R) individual A:1. A(s) Af R |= A(s).2. B(aA ) Af R |= B.3. R R, Af Af , Af ABox labelingleaf derivation R .like EL hypertableau algorithm, derivation EL e -algorithmcontains exactly one leaf node, ABox labeling derivation leaf uniquely defined, Rv Av , Th . next show several useful properties algorithm.Lemma 12. Let , Rv Av , Th input EL e -algorithm let AeABox labeling leaf derivation , Rv Av , Th . following holds.1. Let Rh set EL-rules corresponding Th described Motik et al. (2009),let AEL ABox labeling leaf derivation standard EL hypertableaualgorithm Rv Rh Av ; then, Ae AEL .249fiCuenca Grau & Motik2. Ae B(aA ) Ae , B safe(Rv , ).Proof. (Claim 1) Let A0 , . . . , derivation EL e -algorithm Rv , Av ,eTh , A0 = Av = Ae . prove claim inductively showingAj AEL 0 j n. induction base, clearly A0 AEL . AssumeAj1 AEL let Aj obtained Aj1 applying derivation ruleEL e -algorithm. Hyp-rule applied Aj1 Rv , Rv Rh ,Aj1 AEL , fact Hyp-rule applicable AEL imply Aj AEL .argument analogous -rule. Finally, assume e -concept rule derivesA(s) {} Aj1 . preconditions e -concept rule,eTh , (A , A(s)) = connected component Aj1 | . Property 1 Lemma11 A(s) , ABox labeling leaf derivation standard ELhypertableau algorithm Rh . AEL , Rh Rh Rv , Property 3Lemma 11 imply AEL ; consequently, A(s) AEL Aj AEL .(Claim 2) Consider arbitrary individual form aA arbitraryassertion B(aA ) Ae . Claim 1, B(aA ) AEL , Property 2 Lemma 11Av Rv Rh |= B. Since Rv Rh EL-rules, Av aect subsumptioninferences, Rv Rh |= B. Since Ae , interpretation exists AI =|= Rh . Assume B safe(Rv , ). Proposition 3 fact RvEL-safe, model J Rv exists X J = X X sig(Rh ), B J = .Thus, J |= Rv Rh , contradicts fact Rv Rh |= B.Soundness EL e -algorithm follows immediately Property 1 Lemma 12fact standard EL hypertableau algorithm sound. next provealgorithm complete.Lemma 13 (Completeness). Let , Rv Av , Th input EL e -algorithm letTh EL TBox, let Ae ABox labeling leaf derivation , Rv Av , Th .Ae , Rv Av Th satisfiable.Proof. Let Rh result transforming Th set EL-rules described Motiket al. (2009); then, Rv Av Th equisatisfiable Rv Av Rh , modellatter model former well. Therefore, rest proof extendclash-free ABox Afin derivation rule Table 2 applicable Rv RhAfin . Lemma 3, Rv Afin Rh satisfiable, which, together Afin ,implies satisfiability Rv Rh . Let v = sig(Rv ) sig(Av ) h = sig(Rh ).next present construction Afin . first step extend Aesatisfies Rh , achieve applying EL hypertableau algorithm Rh Ae .assume individuals Ae form aA reused whenever v .call individuals Ae old freshly introduced individuals new, callindividual -relevant form aA .next show ABox Aj derivation Rh Ae satisfies followingproperties (*):1. Aj implies Ae whenever form(a) C(s) sig(C) v old individual,250fiReasoning Ontologies Hidden Content(b) R(s, t) R v old individuals.2. C(s) Aj , following properties hold:(a) sig(C) v sig(C) h ,(b) new individual, sig(C) h .3. R(s, t) Aj , following properties hold:(a) new individual, R h ,(b) new old, -relevant R h .proof (*) induction application derivation rules.induction base, A0 = Ae . Statements (1) (2a) hold trivially, (2b) (3)vacuously true since Ae contains old individuals. Assume (1)(3) holdAj1 consider application derivation rule derives Aj .Assume -rule applied R.A(s) Aj1 , deriving R(s, aA ) A(aA ).{R, A} v old, R.A(s) Ae induction assumption; since rule applicable Ae , {R(s, aA ), A(aA )} Ae , (1) holds. Furthermore,v \ , old (2b), R v (2a); R.A(s) Ae , -rulecannot applicable R.A(s) Aj1 . Consequently, {R, A} h , A(aA )clearly satisfies (2), R(s, aA ) clearly satisfies (3a). Finally, aA old ,R(s, aA ) clearly satisfies (3b).Assume Hyp-rule applied EL-rule Rh form (8), derivingC(s). Then, individuals t1 , . . . , tm Aj1 exist Ai (s) Aj1 1 k{Ri (s, ti ), Bi,1 (ti ), . . . , Bi,mi (ti )} Aj1 1 m. ABox Aj trivially satisfies(1b) (3), satisfies (2) Rh , sig(C) h . show (1a), assumeold individual sig(C) v ; since Rh , sig(C) . Property 1Lemma 11, Rh Ae |= C(s). Since sig(C) , Rh Ae | |= C(s). Sincee -concept rule applicable Ae , C(s) Ae , Aj satisfies (1a).completes proof (*). Let Ader ABox labeling leaf derivationEL hypertableau algorithm Rh Ae . Ader clash-free since/ Aee -rule applicable Ae ; furthermore, Ader satisfies (*).extend Ader EL-rules Rv satisfied matchednew individuals. end, new individual u Ader , let Ader [u] Rv extension w.r.t. projection Ader {u}; Ader [u] exists Proposition 5fact Rv EL-safe. Let Afin union Ader Ader [u]. SinceAv Ae Ae Afin , Av Afin . Furthermore, since Ader Ader [u]u new Ader , Afin . Finally, (*), Property 2 Lemma12, fact Ader [u] contains one individual safe concepts, Afinsatisfies following properties (**):1. B(s) Afin -relevant new, B safe(Rv , ).2. R(s, t) Afin , following properties hold:(a) new individual = t, R h ,251fiCuenca Grau & Motik(b) new old, -relevant R h .complete proof lemma, show derivation rule hypertableaualgorithm applicable Afin Rv Rh .(-rule) Consider arbitrary R.C(s) Afin . R.C(s) Ader , since -ruleapplicable Ader , {R(s, t), C(t)} Ader Afin . R.C(s) Ader [u]individual u new Ader , Definition 14 -rule applicable Ader [u],{R(s, t), C(t)} Ader [u] Afin . Either way, -rule applicable R.C(s) Afin .(Hyp-rule) Assume Hyp-rule applicable EL-rule Rv Rhform (8), deriving C(s). Then, individuals t1 , . . . , tm Afin exist Ai (s) Afin1 k {Ri (s, ti ), Bi,1 (ti ), . . . , Bi,mi (ti )} Afin 1 m.following possibilities:Rh . new individual u assertion Ader [u] \ Ader , Definition 14 either sig() v \ form R.C. Thus, Ai (s) Ader1 k {Ri (s, ti ), Bi,1 (ti ), . . . , Bi,mi (ti )} Ader 1 m,Hyp-rule applicable Ader , contradiction.Rv . first show following property (***): ti new, = tjtj . following cases.Assume new consider arbitrary 1 m. Clearly, Ri v ;furthermore, Ri , since EL-safe, body contains atommatched Bij (ti ) Afin Bij safe(Rv , ). Assumeti = s. ti new, Ri Statement (2a) (**); furthermore, tiold, Ri ti -relevant Statement (2b) (**); consequently,Ri ti either new -relevant. then, Statement (1) (**)Property 3 Definition 14, Bij (ti ) Afin , contradiction. Hence,conclude = ti .Assume ti new 1 m. ti = s, Statement (2a) (**)Ri . Since EL-safe, body contains atommatched Bij (ti ) Afin Bij safe(Rv , ). Statement (1)(**) implies Bij (ti ) Afin , contradiction. Hence, conclude= ti ; previous case = tj 1 j m.Let = Ae old, = Ader [s] otherwise. straightforward consequence(***) Ai (s) 1 k {Ri (s, ti ), Bi,1 (ti ), . . . , Bi,mi (ti )}1 m. Hyp-rule applicable , C(s) . SinceAfin , C(s) Afin , contradicts assumption Hyp-ruleapplicable Afin .completes proof lemma.Finally, prove EL e -algorithm optimal import-by-query algorithm.252fiReasoning Ontologies Hidden ContentTheorem 13. EL e -algorithm import-by-query algorithm based ABox entailment oracles class inputs C[C , RCv ACv , ThC ] Definition 11. algorithmimplemented runs PTime N polynomial number (in N )calls eTh , , N = |Rv Av | + || input Rv , Av , .Proof. EL e -algorithm import-by-query algorithm straightforwardconsequence Lemmas 12 13. estimate algorithms running time, noteapplication derivation rule adds assertion form C(a) R(a, b)C v {}, b individuals occurring either Av form aAsig(Rv ). Clearly, maximal number individuals occurring ABoxderivation polynomial size Av , Rv , , maximal numberassertions. Furthermore, derivation rule removes assertions ABox,number assertions ABox monotonically increases course derivation.Consequently, number rule applications polynomial size Av , Rv , .way standard EL hypertableau algorithm (Motik & Horrocks, 2008),one show derivation rule applied polynomial time, impliesclaim theorem.ReferencesBaader, F., Brandt, S., & Lutz, C. (2005). Pushing EL Envelope. Kaelbling, L. P., &Saotti, A. (Eds.), Proc. 19th Int. Joint Conference Artificial Intelligence(IJCAI 2005), pp. 364369, Edinburgh, UK. Morgan Kaufmann Publishers.Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).(2007). Description Logic Handbook: Theory, Implementation Applications(2nd edition). Cambridge University Press.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). TractableReasoning Ecient Query Answering Description Logics: DL-Lite Family.Journal Automated Reasoning, 9 (3), 385429.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2004).Ask Peer: Ontolgoy-based Query Reformulation. Dubois, D., Welty, C. A., &Williams, M.-A. (Eds.), Proc. 9th Int. Conf. Principles KnowledgeRepresentation Reasoning (KR 2004), pp. 469478. AAAI Press.Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2007). Logical FrameworkModularity Ontologies. Veloso, M. M. (Ed.), Proc. 20th Int. Joint Conf.Artificial Intelligence (IJCAI 2007), pp. 298303, Hyderabad, India.Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2008). Modular Reuse Ontologies: Theory Practice. Journal Artificial Intelligence Research, 31, 273318.Cuenca Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P., & Sattler, U.(2008). OWL 2: next step OWL. Journal Web Semantics: Science, ServicesAgents World Wide Web, 6 (4), 309322.Doran, P., Tamma, V. A. M., & Iannone, L. (2007). Ontology module extraction ontologyreuse: ontology engineering perspective. Silva, M. J., Laender, A. H. F., BaezaYates, R. A., McGuinness, D. L., Olstad, B., Olsen, . H., & Falcao, A. O. (Eds.), Proc.253fiCuenca Grau & Motik16th ACM Conference Information Knowledge Management (CIKM2007), pp. 6170, Lisbon, Portugal. ACM.Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDFOWL: making Web Ontology Language. Journal Web Semantics, 1 (1),726.Horrocks, I., & Sattler, U. (1999). Description Logic Transitive Inverse RolesRole Hierarchies. Journal Logic Computation, 9 (3), 385410.Horrocks, I., & Sattler, U. (2005). Tableaux Decision Procedure SHOIQ. Proc.19th Int. Joint Conf. Artificial Intelligence (IJCAI 2005), pp. 448453,Edinburgh, UK. Morgan Kaufmann Publishers.Hustadt, U., Motik, B., & Sattler, U. (2005). Data Complexity Reasoning Expressive Description Logics. Proc. 19th Int. Joint Conf. Artificial Intelligence(IJCAI 2005), pp. 466471, Edinburgh, UK. Morgan Kaufmann Publishers.Jimenez-Ruiz, E., Cuenca Grau, B., Sattler, U., Schneider, T., & Berlanga Llavori, R.(2008). Safe Economic Re-Use Ontologies: Logic-Based MethodologyTool Support. Bechhofer, S., Hauswirth, M., Homann, J., & Koubarakis, M.(Eds.), Proc. 5th European Semantic Web Conference (ESWC 2008), Vol. 5021LNCS, pp. 185199, Tenerife, Spain. Springer.Konev, B., Lutz, C., Walther, D., & Wolter, F. (2008). Semantic Modularity ModuleExtraction Description Logics. Ghallab, M., Spyropoulos, C. D., Fakotakis, N.,& Avouris, N. M. (Eds.), Proc. 18th European Conf. Artificial Intelligence(ECAI 2008), Vol. 178 FAIA, pp. 5559, Patras, Greece. IOS Press.Konev, B., Walther, D., & Wolter, F. (2009). Forgetting Uniform InterpolationLarge-Scale Description Logic Terminologies. Boutilier, C. (Ed.), Proc. 21stInt. Joint Conf. Artificial Intelligence (IJCAI 2009), pp. 830835, Pasadena, CA,USA.Kontchakov, R., Pulina, L., Sattler, U., Schneider, T., Selmer, P., Wolter, F., & Zakharyaschev, M. (2009). Minimal Module Extraction DL-Lite Ontologies UsingQBF Solvers. Boutilier, C. (Ed.), Proc. 21st Int. Joint Conf. ArtificialIntelligence (IJCAI 2009), pp. 836841, Pasadena, CA, USA.Lutz, C., Walther, D., & Wolter, F. (2007). Conservative Extensions Expressive Description Logics. Veloso, M. M. (Ed.), Proc. 20th Int. Joint Conf. ArtificialIntelligence (IJCAI 2007), pp. 453458, Hyderabad, India.Lutz, C., & Wolter, F. (2010). Deciding inseparability conservative extensionsdescription logic EL. Journal Symbolic Computation, 45 (2), 194228.Lutz, C., & Wolter, F. (2011). Foundations Uniform Interpolation ForgettingExpressive Description Logics. Walsh, T. (Ed.), Proc. 22nd Int. Joint Conf.Artificial Intelligence (IJCAI 2011), pp. 989995, Barcelona, Spain.Motik, B., & Horrocks, I. (2008). Individual Reuse Description Logic Reasoning.Armando, A., Baumgartner, P., & Dowek, G. (Eds.), Proc. 4th Int. Joint Conf.Automated Reasoning (IJCAR 2008), Vol. 5195 LNAI, pp. 242258, Sydney,NSW, Australia. Springer.254fiReasoning Ontologies Hidden ContentMotik, B., Shearer, R., & Horrocks, I. (2009). Hypertableau Reasoning DescriptionLogics. Journal Artificial Intelligence Research, 36, 165228.Nikitina, N. (2011). Forgetting General EL Terminologies. Rosati, R., Rudolph, S.,& Zakharyaschev, M. (Eds.), Proc. 24th Int. Workshop Description Logics(DL 2011), Vol. 745 CEUR Workshop Proceedings, Barcelona, Spain.Papadimitriou, C. H. (1993). Computational Complexity. Addison Wesley.Sattler, U., Schneider, T., & Zakharyaschev, M. (2009). Kind ModuleExtract?. Cuenca Grau, B., Horrocks, I., Motik, B., & Sattler, U. (Eds.), Proc.22nd Int. Workshop Description Logics (DL 2009), Vol. 477 CEUR WorkshopProceedings, Oxford, UK.Stuckenschmidt, H., Parent, C., & Spaccapietra, S. (Eds.). (2009). Modular Ontologies:Concepts, Theories Techniques Knowledge Modularization, Vol. 5445 LNCS.Springer.Tobies, S. (2000). Complexity Reasoning Cardinality Restrictions NominalsExpressive Description Logics. Journal Artificial Intelligence Research, 12, 199217.Wang, K., Wang, Z., Topor, R. W., Pan, J. Z., & Antoniou, G. (2009). Concept RoleForgetting ALC Ontologies. Bernstein, A., Karger, D. R., Heath, T., Feigenbaum,L., Maynard, D., Motta, E., & Thirunarayan, K. (Eds.), Proc. 8th Int. SemanticWeb Conference (ISWC 2009), Vol. 5823 LNCS, pp. 666681, Chantilly, VA, USA.Springer.Wang, Z., Wang, K., Topor, R. W., & Pan, J. Z. (2008). Forgetting Concepts DL-Lite.Bechhofer, S., Hauswirth, M., Homann, J., & Koubarakis, M. (Eds.), Proc.5th European Semantic Web Conference (ESWC 2008), Vol. 5021 LNCS, pp.245257. Springer.Wang, Z., Wang, K., Topor, R. W., & Zhang, X. (2010). Tableau-based ForgettingALC Ontologies. Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proc.19th European Conference Artificial Intelligence, Vol. 215 Frontiers ArtificialIntelligence Applications, pp. 4752, Lisbon, Portugal. IOS Press.255fiJournal Artificial Intelligence Research 45 (2012) 481514Submitted 06/12; published 11/12Complexity Judgment AggregationUlle EndrissUmberto GrandiDaniele Porelloulle.endriss@uva.nlumberto.uni@gmail.comdanieleporello@gmail.comInstitute Logic, Language ComputationUniversity AmsterdamPostbus 94242, 1090 GE AmsterdamNetherlandsAbstractanalyse computational complexity three problems judgment aggregation:(1) computing collective judgment profile individual judgments (the winnerdetermination problem); (2) deciding whether given agent influence outcomejudgment aggregation procedure favour reporting insincere judgments (thestrategic manipulation problem); (3) deciding whether given judgment aggregationscenario guaranteed result logically consistent outcome, independentlyjudgments supplied individuals (the problem safety agenda).provide results specific aggregation procedures (the quota rules, premisebased procedure, distance-based procedure) classes aggregation procedurescharacterised terms fundamental axioms.1. IntroductionJudgment aggregation (JA) branch social choice theory studies propertiesprocedures amalgamating several agents individual judgments regarding truthfalsity set inter-related propositions collective judgment reflecting viewsgroup agents whole (List & Pettit, 2002; List & Puppe, 2009). classicexample due Kornhauser Sager (1993): Suppose three judges decidelegal case involving possible breach contract. Two relevant propositionsreally binding contract rather informal promise (proposition p)defendant broke promise (proposition q). defendant pronouncedguilty conjunction two propositions found true (p q). judgestake following views matter:Judge 1Judge 2Judge 3MajoritypYesYesYesqYesYesYespqYesNote position individual judge logically consistent. However,aggregate information using majority rule (i.e., accept propositionstrict majority judges do), arrive collective judgment setinconsistent. paradoxical outcome, variations known doctrinalc2012AI Access Foundation. rights reserved.fiEndriss, Grandi, & Porelloparadox (Kornhauser & Sager, 1993) discursive dilemma (Pettit, 2001), inspiredimportant fast growing literature JA, starting seminal contributionList Pettit (2002), showed fact aggregation procedure satisfying certainaxioms encoding natural desiderata avoid kind paradox.literature JA largely developed outlets associated Philosophy, Economic Theory, Political Science, Logic, recently JA also come recognisedrelevant AI, particularly design analysis multiagent systems.reasons clear: multiagent system, different autonomous software agents maydifferent opinions issues (maybe due difference access relevantinformation, due different reasoning capabilities), joint course actionneeds extracted diverse views. Indeed, AI, related problem beliefmerging studied time (see, e.g., Konieczny & Pino Perez, 2002; MaynardZhang & Lehmann, 2003; Chopra, Ghose, & Meyer, 2006; Everaere, Konieczny, & Marquis,2007), interesting parallels literature JA (Pigozzi, 2006).JA also found relevant analysis abstract argumentation frameworkswidely studied AI (Caminada & Pigozzi, 2011; Rahwan & Tohme, 2010).Given relevance JA AI, important understand computational aspects.However, date, received relatively little attention literature.course explained origins field Law, Economics, Philosophy.domains social choice, particularly voting fair division, hand,recent focus computational aspects successful given risefield computational social choice (Chevaleyre, Endriss, Lang, & Maudet, 2007; Brandt,Conitzer, & Endriss, 2012).help bridge gap, paper shall analyse computational complexitythree important problems arise JA:Winner determination. winner determination problem problem computing result applying given aggregation procedure given profile individual judgment sets. immediate practical relevance applications JA.obtain positive negative results: two types aggregation procedures,namely quota rules premise-based procedure, winner determinationproblem easily seen polynomial, certain distance-based procedureobtain interesting intractability result, establishing completeness parallelaccess NP mirrors corresponding results voting theory Dodgson rule(Hemaspaandra, Hemaspaandra, & Rothe, 1997), Young rule (Rothe, Spakowski,& Vogel, 2003) Kemeny rule (Hemaspaandra, Spakowski, & Vogel, 2005).Strategic manipulation. agent may try influence result aggregationfavour reporting set judgments different truthfully heldbeliefs. manipulation problem asks, given aggregation procedure, givenprofile judgment sets, given agent, whether agent opportunitymanipulate successfully situation. one natural way defining preferencestop JA framework (namely terms Hamming distance)aggregation procedures independent monotonic, well-knownagents never incentive manipulate (Dietrich & List, 2007c).cases, interesting explore hard solve manipulation problem,482fiComplexity Judgment Aggregationhigh complexity might signal certain level immunity manipulation.context voting, kind question lead series interesting importantresults (Bartholdi, Tovey, & Trick, 1989; Faliszewski & Procaccia, 2010), evencareful over-interpret theoretical intractability results necessarilyproviding protection practice (Walsh, 2011). one widely used procedure (witheasy winner determination problem), namely premise-based procedure,able show NP-completeness manipulation problem.Safety agenda. paradox presented shows aggregation procedures possible obtain collective judgment set logicallyinconsistent, even though judgment sets supplied individualsconsistent. important parameter determining possibility paradoxagenda, set propositions pass judgment. given aggregation procedure, problem safety agenda asks whether given agendasafe sense profile individual judgment sets consistentever result collective judgment set inconsistent. various classesaggregation procedures, defined terms classical axioms, prove safety theoremsfully characterise agendas safe sense relate resultsknown possibility theorems JA literature (List & Puppe, 2009).study complexity deciding whether given agenda meets safety conditionsidentified find typically highly intractable problem locatedsecond level polynomial hierarchy.results build extend earlier work complexity judgment aggregation (Endriss, Grandi, & Porello, 2010a, 2010b).remainder paper organised follows. Section 2 introduce formalframework JA, including several concrete aggregation procedures importantaxioms used define desiderata procedures. Section 2 also includes proofssimple representation results characterise aggregation procedures satisfycertain combinations axioms. Section 3 devoted study complexitywinner determination problem Section 4 manipulationproblem. Section 5 introduce problem safety agenda, proveseveral agenda characterisation theorems establishing necessary sufficient conditionssafety, finally study complexity deciding whether conditions satisfied.Section 6 reviews related work computational aspects JA Section 7 concludesdiscussion possible avenues future work.Throughout paper, shall assume familiarity basics complexity theory notion NP-completeness. Helpful introductions include textbooksPapadimitriou (1994) Arora Barak (2009).2. Formal Framework Judgment Aggregationsection provide succinct exposition formal framework JA (List &Puppe, 2009), originally laid List Pettit (2002) sincerefined number authors, notably Dietrich (2007). also define threeconcrete (families of) aggregation procedures discuss important axiomatic483fiEndriss, Grandi, & Porelloproperties literature. Finally, prove number representation results,status folk theorems JA literature often play crucial roleproofs complex results, rarely stated explicitly.2.1 Notation TerminologyLet L set propositional formulas built finite set propositional variablesusing usual connectives , , , , , constants (true) (false).every formula , define complement , i.e., =negated, = = formula . say set closedcomplementation case whenever .Definition 1. agenda finite nonempty set L containdoubly-negated formulas closed complementation.is, slight departure common definition literature (List & Puppe,2009), allow tautologies contradictions agenda. reason wantstudy computational complexity JA, recognising tautology contradictioncomputationally intractable problem. write + set non-negatedformulas , i.e., = + { | + }.Definition 2. judgment set J agenda subset J .call judgment set J complete J J ; call complementfree 1 case complement J;call consistent exists assignment makes formulas J true. Let J ()denote set complete consistent subsets .shall occasionally interpret judgment set J (characteristic) function J :{0, 1} J() = 1 J J() = 0 6 J. Hamming distance H(J, J )two (complete complement-free) judgment sets J J numberpositive formulas differ:XH(J, J ) =|J() J ()|+Given set N = {1, . . . , n} n > 1 individuals (or agents), write J = (J1 , . . . , Jn )J ()n generic profile judgment sets, one individual.2 ease exposition,shall assume n odd (i.e., n > 3). write NJ = {i N | Ji } setindividuals accepting formula profile J .Definition 3. (resolute) judgment aggregation procedure agendaset individuals N n = |N | function F : J ()n 2 .1. property called weak consistency Dietrich (2007), consistency List Pettit (2002).choice terminology intended emphasise fact purely syntactic notion,involving model-theoretic concept, distinction believe worth stressing.2. previous work used general notation J ()N (i.e., set functions NJ ()) set admissible profiles (Endriss et al., 2010a). useful N might infinitenecessarily want associate set individuals set natural numbers,require level generality here.484fiComplexity Judgment Aggregationis, aggregation procedure maps profile individual (complete consistent)judgment sets single collective judgment set (an element powerset ).shall occasionally refer assumption individual judgment sets completeconsistent individual rationality. Note collective judgment set needcomplete consistent (that is, collective rationality need hold). kindprocedure defined called resolute, return single judgment setprofile. Later, shall also discuss irresolute JA procedures, may returnnonempty set judgment sets (that tied winning). Finally, note that, since Fdefined set profiles consistent complete judgment sets, implicitlymaking universal-domain assumption, sometimes stated separate property(List & Pettit, 2002).2.2 Axiomatic PropertiesDefinition 3 put constraints collective judgment set, outcomeaggregation process. role following definition:Definition 4. judgment aggregation procedure F , defined agenda , said be:(i) complete F (J ) complete every J J ()n ;(ii) complement-free F (J ) complement-free every J J ()n ;(iii) consistent F (J ) consistent every J J ()n .present several axioms provide normative framework statedesirable properties acceptable aggregation procedure be. Noteevery procedure satisfy every axiom. Rather, axioms model desiderataprocedures satisfy others not. first axiom basic requirement, restrictingpossible aggregators F terms fundamental properties outcomes produce.Weak Rationality (WR): F complete complement-free.3condition differs called collective rationality literatureJA (List & Puppe, 2009), require collective judgment set consistent.first reason include consistency basic notion rationalityrequirements (WR) purely syntactic notions easily checked automatically,case consistency. second reason consistency intrinsicaggregation function, rather depends properties agenda. pointmade precise Section 5, study consistency classaggregators depending agenda.following important axioms discussed literature JA (List &Pettit, 2002; Dietrich, 2007; List & Puppe, 2009; Nehring & Puppe, 2010):Unanimity (U): Ji agents N , F (J ).3. previous work (Endriss et al., 2010a), used definition weak rationality additioncompleteness complement-freeness also included (very weak) technical requirementcontradictory formula universally accepted profiles. consequence,results later stated slightly differently.485fiEndriss, Grandi, & PorelloAnonymity (A): profile J J ()n permutation : N N ,F (J1 , . . . , Jn ) = F (J(1) , . . . , J(n) ).Neutrality (N): two formulas , agenda profile J J ()n ,agents N Ji Ji , F (J ) F (J ).Independence (I): formula agenda two profiles J , JJ ()n , Ji Ji agents N , F (J ) F (J ).Systematicity (S): two formulas , agenda two profiles J ,J J ()n , Ji Ji agents N , F (J ) F (J ).Unanimity expresses idea individuals accept given judgment,collective.4 Anonymity states aggregation symmetric respectindividuals, i.e., individuals treated same. Neutrality symmetryrequirement propositions: subgroup accepts two propositions, eitherneither collectively accepted. Independence says propositionaccepted subgroup two otherwise distinct profiles, propositionaccepted either neither profile. Systematicity satisfiedneutrality independence are. axioms intuitivelyappealing, stronger may seem first, several impossibility theorems,establishing inconsistencies certain combinations axioms desiderata,proved literature. original impossibility theorem List Pettit(2002), instance, shows (under certain assumptions regarding agenda)complete consistent aggregation procedure satisfying (A) (S).important property monotonicity. introduce two different axiomsmonotonicity. first one commonly used literature (Dietrich & List, 2007a;List & Puppe, 2009). implicitly relies independence axiom. second, introducedprevious work (Endriss et al., 2010a), designed applied neutral procedures.systematic procedures two formulations equivalent.I-Monotonicity (MI ): formula agenda two profiles J , JJ ()n , Ji Ji agents N , N6 Js Js , F (J ) F (J ).N-Monotonicity (MN ): two formulas , agenda profile JJ ()n , Ji Ji agents N 6 Js JsN , F (J ) F (J ).is, (MI ) expresses collectively accepted (in J ) receives additionalsupport (in J , agent s), continue collectively accepted. Axiom(MN ) says collectively accepted accepted strict supersetindividuals accepting , also collectively accepted.Axioms used define different classes aggregation procedures: Givenagenda list desirable properties AX provided form axioms, defineF [AX] set procedures F : J ()n 2 satisfy axioms AX.4. notion unanimity stronger another common formulation requiring J = (J, . . . , J)imply F (J ) = J (List & Puppe, 2009), two equivalent assumption (I).486fiComplexity Judgment Aggregation2.3 Judgment Aggregation ProceduresNext, define three concrete types aggregation procedures.2.3.1 Uniform Quota Rules Majority Ruleaggregation procedure F n = |N | individuals quota rule every formulaexists quota q {0, . . . , n+1} F (J ) |NJ | > q .class quota rules studied depth Dietrich List (2007a). paper,interested particular class quota rules:Definition 5. Given {0, . . . , n+1} agenda , uniform quota rulequota aggregation procedure Fm Fm (J ) |NJ | > m.aggregation procedure satisfies (A), (I), (MI ), (N) uniform quotarule; fact follows immediately result Dietrich List (2007a), useslightly narrow definition quota rule. Provided 6= n + 1, uniform quota ruleFm also satisfies (U).quota rule special interest majority rule. majority rule uniformquota rule = n+12 ; accepts formula whenever individuals acceptingrejecting (recall assume n odd). Clearly, majorityrule uniform quota rule satisfies (WR).2.3.2 Premise-Based Procedureseen introduction, majority rule may fail produce consistentoutcome. Two basic aggregation procedures set way avoidproblem discussed JA literature beginning, namelypremise-based conclusion-based procedure (Kornhauser & Sager, 1993; Dietrich& Mongin, 2010). basic idea divide agenda premises conclusions.premise-based procedure, apply majority rule premisesinfer conclusions accept given collective judgments regarding premises;5conclusion-based procedure directly ask agents judgmentsconclusions leave premises unspecified collective judgment set. is,conclusion-based procedure result complete outcomes (indeed, strictly speaking,conform Definition 3), shall consider here. premise-basedprocedure, hand, set way guarantees consistent completeoutcomes, provides usable procedure practical interest.many JA problems, may natural divide agenda premisesconclusions. Let = p c agenda divided set premises p setconclusions c , closed complementation.5. commonly understood premise-based procedure. Dietrich Mongin (2010),call rule premise-based majority voting, also investigated general class premise-basedprocedures procedure used decide upon premises need majority rule.487fiEndriss, Grandi, & PorelloDefinition 6. premise-based procedure PBP p c function mappingprofile J = (J1 , . . . , Jn ) J ()n following judgment set:PBP(J ) = { c | |= },= { p | |NJ | >n+1}2is, set premises accepted (strict) majority; PBP returnset together conclusions logically follow ( |= ).want ensure PBP always returns judgment sets consistentcomplete, impose certain restrictions:want guarantee consistency, impose restrictionspremises. well-known majority rule guaranteed consistentagenda satisfies so-called median property, i.e., every inconsistent subset inconsistent subset size 6 2 (Nehring & Puppe, 2007;List & Puppe, 2009).6 result immediately transfers PBP: consistentset premises satisfies median property.want guarantee completeness, impose restrictionsconclusions: assignment truth values premises, truth valueconclusion fully determined.shall see Section 5 deciding whether set formulas satisfies medianproperty highly intractable. is, general form, deciding whetherPBP consistent aggregation procedure given agenda complex problem.meaningful analysis, therefore make two additional assumptions. First, assumeagenda closed propositional variables: p propositional variable poccurring within formulas . Second, equate set premisesset literals. Clearly, above-mentioned conditions consistency completenesssatisfied assumptions.So, summarise, instance PBP shall work paper definedfollows: assumption agenda closed propositional variables,PBP accepts literal individuals accept accept ;PBP accepts compound formula entailed accepted literals.consistent complete input profiles, assuming n odd, leads resoluteJA procedure consistent complete. downside, PBP violatesstandard axioms typically considered, (N) (I). even violates (U):Agent 1Agent 2Agent 3PBPpYesqYesrYespqrYesYesYesexample, three agents unanimously accept p q r, aggregate usingPBP, end rejecting p q r, three premises rejected.6. shall discuss result detail Section 5.488fiComplexity Judgment Aggregation2.3.3 Distance-Based Procedurebasic idea distance-based approach aggregation select outcome that,sense, minimises distance input profile. idea used extensivelypreference aggregation (Kemeny, 1959) belief merging (Konieczny & Pino Perez,2002). first example JA procedure based notion distance introducedPigozzi (2006), albeit restrictive assumption agenda closedpropositional variables compound formula either unanimously acceptedunanimously rejected agents. importantly, Pigozzis approach syntacticinformation contained agenda discarded moving aggregationlevel formulas level models. syntactic variant procedure laterdefined Miller Osherson (2009), authors call Prototype-Hammingrule. distance-based procedure shall define analyse here. irresoluteprocedure, returning (nonempty) set collective judgment sets.Definition 7. Given agenda , distance-based procedure DBP functionmapping profile J = (J1 , . . . , Jn ) J ()n following set judgment sets:DBP(J ) = argminXH(J, Ji )JJ ()collective judgment set DBP minimises amount disagreementindividual judgment sets (i.e., minimises sum Hamming distancesindividual judgment sets). Note cases majority rule leads consistentoutcome, outcome DBP coincides majority rule (makingresolute procedure profiles). combine DBP tie-breaking ruleobtain resolute procedure.DBP complete consistent design: judgment sets J () considered candidates searching solution. However, violates standardaxiomatic properties adapted case irresolute JA procedures (Lang,Pigozzi, Slavkovik, & van der Torre, 2011). particular, DBP independent; indeed, based idea correlations propositions exploitedrather neglected.2.4 Representation Resultsprove number representation results characterise aggregation procedures satisfy certain combinations axioms. results sectionknown results, butdespite usefulthey rarely stated explicitlyliterature.Observe aggregation procedure F satisfies (I) exists familysets winning coalitions W 2N , one formula , F (J )NJ W . Imposing additional axioms, top (I), forces additional structure ontofamily winning coalitions:F satisfies (I) (U) grand coalition belongs every set winningcoalitions: N W .489fiEndriss, Grandi, & PorelloF satisfies (I) (N), i.e., satisfies (S), exists single setwinning coalitions W 2N F (J ) NJ W.F satisfies (I) (A) collective acceptance formula dependsnumber individuals accepting it: C W |C| = |C | imply C W .One consequence latter two insights that, F satisfies (A) (S),|NJ | = |NJ | implies F (J ) F (J ). well-known fact; List Pettit(2002), instance, use proof impossibility theorem (for special caseJ = J ). Note (somewhat surprising) consequence fact that, case neven, exists aggregation procedure satisfies (A), (S), well (WR).see this, suffices consider (single) profile J exactly n2 agents accept n2J |, i.e., either must F (J ), contraagents accept . |NJ | = |Ndicting complement-freeness, neither must F (J ), time contradictingcompleteness. emphasise basic impossibility result involve notionlogical consistency.hand, n odd (which shall continue assume),axioms characterise relevant class aggregation procedures:Proposition 1. F F [WR, A, S] exists function h : {0, . . . , n}{0, 1} satisfying h(i) = 1 h(n i) N F (J ) h(|NJ |) = 1.Proof. already seen F satisfies (S) (A), |NJ | = |NJ | impliesF () F (J ). latter equivalent existence function h :{0, . . . , n} {0, 1} F (J ) h(|NJ |) = 1. additional requirement h(i) =1 h(n i) consequence (WR). direction immediate: acceptanceformula F depends number agents accepting it, F mustanonymous, neutral independent; condition h(i) = 1 h(n i) furthermore ensurescompleteness complement-freeness.Dropping either neutrality independence, obtain following representation results:Proposition 2. F F [WR, A, I] exists function h : {0, . . . , n}{0, 1} every formula satisfying h (i) = 1 h (n i) NF (J ) h (|NJ |) = 1.Proof. clear characterisation procedures satisfying (I) (A) termswinning coalitions given above, procedure always decide whethercollectively accepted looking cardinality coalition accepting. rest proof proceeds Proposition 1.Proposition 3. F F [WR, A, N] exists function hJ : {0, . . . , n}{0, 1} every profile J J ()n satisfying hJ (i) = 1 hJ (n i) NF (J ) hJ (|NJ |) = 1.Proof. drop (I), winning coalitions anymore associated formulas,depend profile J in. (N) merely ensures winning coalitionsalso depend formula question. (WR) forces symmetry requirementhJ (i) = 1 hJ (n i). opposite direction immediate.490fiComplexity Judgment Aggregationthree representation results above, add (U) list axioms,corresponds requiring h(n) = 1 characteristic functions h.Finally, recall seen Section 2.3.1, F F [A, S, MI ]F uniform quota rule F F [WR, A, S, MI ] F majorityrule. is, representation results stated concern natural weakeningscombination axioms characterising majority rule. particular, chose neverdrop anonymity axiom, find appealing uncontroversialJA. also consider unanimity weak rationality fundamental (although makeexceptions class quota rules). independence neutrality axioms,hand, much debatable, considered various optionseither including including (although always keep least one them,maintain minimal amount structure). is, classes aggregation procedurescovered representation results natural focus on.3. Winner Determinationsection define problem winner determination given JA proceduredecision problem, study computational complexity problemprocedures presented Section 2.3.3.1 Problem Definitionproblem winner determination voting theory computing electionwinner given profile preferences supplied voters. corresponding decisionproblem asks, given preference profile candidate, whether given candidatewinner election. JA, want compute F (J ) given profile J .resolute aggregation procedure F , formulate corresponding decision problemasking, given formula, whether belongs F (J ):WinDet(F )Instance: Agenda , profile J J ()n , formula .Question: element F (J )?solving WinDet formula agenda, compute collectivejudgment set input profile. Note asking instead whether given judgment setJ equal F (J ) lead appropriate formulation winner determinationproblem, actually compute winner would solve decisionproblem exponential number times (once possible J ).case irresolute JA procedures F adapt winner determinationproblem following way:WinDet (F )Instance: Agenda , profile J J ()n , subset L .Question: J L J J F (J )?see appropriate formulation decision problem correspondingtask computing winning set, note compute winner using polynomial491fiEndriss, Grandi, & Porellonumber queries WinDet follows. First, ask whether exists winning setincluding arbitrarily chosen first formula agenda 1 , i.e., L = {1 }. caseanswer positive, consider second formula 2 query WinDet L = {1 , 2 }.Use subset L = {1 , 2 } case negative answer. Continue processformulas agenda covered.73.2 Winner Determination Quota Rules Premise-Based Procedureimmediately clear winner determination polynomial problem quotarule, including majority rule.Fact 4. WinDet(Fm ) P uniform quota rule Fm .Winner determination also tractable premise-based procedure:Proposition 5. WinDet(PBP) P.Proof. Counting number agents accepting premises checkingpremise whether positive negative instance majority easy. determines collective judgment set far premises concerned. Deciding whethergiven conclusion accepted collective amounts model checkingproblem (is conclusion true model induced accepted premises/literals?),also done polynomial time.3.3 Winner Determination Distance-Based Procedurewant analyse complexity winner determination problemdistance-based procedure. DBP irresolute, study decision problemWinDet . shall see, WinDet (DBP) p2 -complete, thus showing rulehard compute. class p2 (also known p2 (O(log n)), PNP[log] PNP|| )class problems solved polynomial time asking logarithmic numberqueries NP oracle or, equivalently, solved polynomial time askingpolynomial number queries parallel (Wagner, 1987; Hemachandra, 1989).obtain result, first devise NP oracle used proofp2 -membership. shall use following problem:WinDetK (DBP)Instance: Agenda , profile J J ()n , subset L , PK N.Question: J J () L J H(J , Ji ) 6 K?is, ask whether exists judgment set J Hamming distanceprofile K accepts formulas L. words, rather aimingcomputing winning judgment set, problem merely allows us compute judgment set7. line recent work Hemaspaandra, Hemaspaandra, Menton (2012), therefore argueformulation winner determination problem correct decision problem associatedsearch problem actually computing winning judgment set.492fiComplexity Judgment Aggregationcertain minimal quality (where quality measured terms Hamming distance).show problem lies NP.8Lemma 6. WinDetK (DBP) NP.Proof. show WinDetK (DBP) modelled integer program (withoutobjective function). proves membership NP (Papadimitriou, 1981). Supposewant answer instance WinDetK (DBP). number subformulas propositionsoccurring agenda linear size (not cardinality) . introduce binarydecision variable subformulas: xi {0, 1} ith subformula.first write constraints ensure chosen outcome correspondconsistent judgment set (i.e., J J ()). Note rewrite formulaterms negation, conjunction, bi-implication without resulting superpolynomial(or even superlinear) increase size. need show encode constraintsconnectives. following table indicates write constraints:2 = 1x2 = 1 x13 = 1 2 x3 6 x1 x3 6 x2 x1 + x2 6 x3 + 13 = 1 2 x1 + x2 6 x3 + 1 x1 + x3 6 x2 + 1x2 + x3 6 x1 + 1 1 6 x1 + x2 + x3continue, consider following way rewriting sum distances featuringdefinition WinDetK (DBP):XH(J , Ji ) ===n XX|J () Ji ()|i=1 +nXX12|J () Ji ()|i=1nX1 XJi ()||n J ()2i=1need bound sum above. suppose variables xi indices{1, . . . , m} = || correspond propositions elements. Let ai = |NJi | number individuals accept ith proposition(under J ). compute winner DBP, need find consistent judgmentset J (characterised variables x1 , . . . , xm ) minimises sum |n x1 a1 | + +|n xm |. introducing additional set integer variables yi > 0= 1, . . . , m. ensure yi = |n xi ai | adding following constraints:9(i 6 m)(i 6 m)n x 6 yin x 6 yi8. proof establishes membership NP, also suggests implement solverdifficult problem. pointed one anonymous reviewer, also possible prove NP-membershipdirectly, using certificate consists J satisfying assignment J .9. precise, constraints ensure |n xi ai | 6 yi . However, next constraint forceyi minimal.493fiEndriss, Grandi, & PorelloPsum 21i=1 yi corresponds Hamming distance winning setprofile. ensure exceed K, add following constraint:1 Xyi 6 K2i=1Finally, need ensure formulas set L get accepted.adding one last set constraints:(for L)xi = 1Now, construction, integer program presented feasibleinstance WinDetK (DBP) started answered positive.completes proof.obtain upper bound winner determination problem DBP,use standard construction. first involves identifying best value K,deciding WinDetK (DBP) value K. latter done logarithmicnumber queries problem complexity analysed Lemma 6.Together, yields desired upper bound:Lemma 7. WinDet (DBP) p2 .Proof. problem WinDet (DBP) asks whether exists winning judgment setaccepts formulas given subset L . Since Hamming distancejudgment set profile bounded polynomial figure, solveproblem performing binary search K using logarithmic number queriesWinDetK (DBP).Pprecisely, since H(J , Ji )) 6 K = ||2 |N |, figure polynomialsize problem description, ask first query WinDetK (DBP)K = K2 empty subset designated formulas. case positive answer,continue search new K = K4 , otherwise move higher halfinterval querying WinDetK (DBP) K = 43 K . process ends logarithmicnumber steps, providing exact Hamming distance K w winning candidateprofile J consideration. sufficient run problem WinDetK (DBP)K = K w subset L original instance WinDet (DBP) wantedsolve. case answer positive, since cannot winning judgment setHamming distance strictly less K w , one winning judgment sets containsformulas L. hand, case negative answer judgment sets containingL Hamming distance bigger K w , thus cannot belong winning set.Next, show upper bound established Lemma 7 tight. exploitsimilarity DBP Kemeny rule preference aggregation build knownp2 -hardness result Hemaspaandra et al. (2005).Lemma 8. WinDet (DBP) p2 -hard.494fiComplexity Judgment AggregationProof. build reduction problem Kemeny Winner, defined workHemaspaandra et al. (2005).10 instance problem consists set candidates C,profile linear preference orders P = (P1 , . . . , Pn ) C, designated candidate cC. Define Kemeny score c following expression:PKemenyScore(c, P ) = min{ ni=1 dist(Pi , Q) | Q linear order top(Q) = c}Here, dist(Pi , Q) Hamming distance two linear orders (defined numberordered pairs candidates disagree) top(Q) preferredcandidate preference order Q. Kemeny Winner asks whether Kemeny scorec less equal Kemeny score candidates C.build instance WinDet (DBP) decide problem. Define agendaC following way. First, add propositional variables pab ordered pairsdistinct candidates a, b C; variables encode linear order C binaryrelation (where pab stands b). describe properties linear ordermeans formulas form pab pbc pac pab pba . includeformulas, a, b, c C, C . fact, include m2 + 1 syntactic variants (where= |C|) them.11 figure m2 + 1 chosen higher maximalHamming distance two linear orders (which m2 ).Given preference profile P , build judgment profile J P encodingorder Pi C judgment set JiP C . example, agent 1s preference orderb c, J1P include set {pab , pba , pbc , pcb , pac , pca }. addition,JiP include syntactic copies formulas encoding linear orders.Observe dist(Pi , Pj ) = H(JiP , JjP ) construction. therefore sufficientask query WinDet (DBP) using C agenda, J P profile, L ={pcd | C, c 6= d} set propositions accept sure, obtain answerinitial Kemeny Winner instance designated candidate c. winning rankingfeatures c top candidates (i.e., formulas pcd accepted candidates d),Kemeny score lower equal candidates, providingpositive answer original problem. key insight notice judgmentsets encoding relations linear orders considered minimisationprocess, since every disagreement one formulas encoding linear orders causemuch greater loss Hamming distance gained modifyingvariables encoding individual candidate rankings.Putting Lemma 7 8 together yields complete characterisation complexitywinner determination distance-based aggregation:Theorem 9. WinDet (DBP) p2 -complete.Theorem 9 shows DBP highly intractable. However, adapting efficient heuristics developed Kemeny rule (which, seen proof Lemma 8, closely relatedDBP) may possible obtain implementation DBP achievesacceptable performance practice (Conitzer, Davenport, & Kalagnanam, 2006).10. Hemaspaandra et al. (2005) work preferences weak orders, point resultsremain valid linear orders used instead. simplify presentation, work linear orders.11. instance, formula might use syntactic variants , , , forth.495fiEndriss, Grandi, & Porello4. Strategic Manipulationcontext voting, agent said able manipulate voting ruleexists situation voting manner truthfully reflect preferencesresult outcome prefers outcome would realisedvote truthfully (Gaertner, 2006). would constitute appropriate definitionmanipulation context JA immediately clear, JAnotion preference. However, fixing suitable notion closeness judgment sets,possible build preference ordering starting individuals initial judgmentset. approach followed Dietrich List (2007c) JA Everaereet al. (2007) related setting belief merging. builds assumptionagents individual judgment set also preferred outcome amongst twooutcomes prefer one closer preferred outcome.measure closeness using Hamming distance call aggregation procedureF manipulable permits situation agent change outcome get closertruthful judgment reporting untruthfully.main interest computational complexity deciding whether givenagent successfully manipulate given profile. context, result showingmanipulation computationally intractable would count positive result. Specifically, study problem premise-based procedure.family quota rules, (as shall see) impossible manipulate quotarule aforementioned sense. also study manipulation problemdistance-based procedure, (as seen) even much basic winnerdetermination problem already intractable procedure.4.1 Problem Definitionfirst need define preference ordering judgment sets agent N .principle, number ways this, one reasonable approachassume agent judgment set Ji also preferred outcomepreferences outcomes depend close Ji (Dietrich & List, 2007c).shall measure closeness using Hamming distance, distances could alsoused end (Duddy & Piggins, 2012). say agent prefers J JH(Ji , J) < H(Ji , J ).employ standard game-theoretical notation denote (J , Ji ) profilelike J , except judgment set agent replaced Ji .Definition 8. F manipulable profile J J ()n agent N , existsalternative judgment set Ji J () H(Ji , F (J , Ji )) < H(Ji , F (J )).is, reporting Ji rather truthful judgment set Ji , agent achieveoutcome F (J , Ji ) outcome closer (in terms Hamming distance)truthful (and preferred) set Ji outcome F (J ) would get realisedtruthfully report Ji . procedure manipulable profileagent called strategy-proof.Dietrich List (2007c) shown JA procedure strategy-proofsatisfies (I) (MI ). Indeed, follows immediately definitions: independence496fiComplexity Judgment Aggregationmeans would-be manipulator consider one proposition time; monotonicitymeans always best interest drive support formulasjudgment set reduce support judgment set, i.e.,best interest report judgment set truthfully.12aggregation procedures strategy-proofness cannot guaranteed, wantstudy algorithmic problem computing manipulating judgment set. end,formulate manipulation decision problem aggregation procedure F :Manip(F )Instance:Question:Agenda , profile J J ()n , agent N .Ji J () H(Ji , F (J , Ji )) < H(Ji , F (J ))?Note asking whether agent manipulate successfully, rather how.is, problem immediately correspond practical (and potentiallyharder) problem computing actual strategy manipulator. However, sinceinterest obtaining intractability results (to provide protection manipulation), safely concentrate formulation, provides lower boundcorresponding search problem.seen, uniform quota rules (including majority rule) independent monotonic, means also strategy-proof (so algorithmicproblem deciding Manip arise procedures). course, comesprice always producing outcomes consistent.4.2 Strategic Manipulation Premise-Based Procedureprove manipulating premise-based procedure intractable, thus showing existence kind jump computational complexity winnerdetermination manipulation desirable context.Theorem 10. Manip(PBP) NP-complete.Proof. first establish NP-membership. untruthful judgment set Ji yielding preferred outcome serve certificate. Checking validity certificate meanschecking (a) Ji actually complete consistent judgment set (b)outcome produced Ji better outcome produced truthful set Ji .(a), checking completeness easy. Consistency also decided polynomial time:every propositional variable p agenda, Ji must include either p p; admitssingle possible model; remains done checking compoundformulas Ji satisfied model.13 (b), need compute outcomesJi Ji (by Proposition 5, polynomial), compute Hamming distancesJi , compare two distances.Next, prove NP-hardness reducing Sat Manip(PBP). Suppose givenpropositional formula want check whether satisfiable. build12. Note contradict Gibbard-Satterthwaite Theorem voting theory (Gaertner,2006). theorem involves universal-domain assumption, manner usingHamming distance induce preferences judgment sets amounts domain restriction.13. is, point crucially rely assumption PBP defined agendasclosed propositional variables.497fiEndriss, Grandi, & Porellojudgment profile three agents third agent manipulate aggregationsatisfiable. Let p1 , . . . , pm propositional variables occurring ,let q1 , q2 two additional propositional variables. Define agenda containsatoms p1 , . . . , pm , q1 , q2 negation, well + 2 syntactic variantsformula q1 ( q2 ), well complements formulas. instance,= q1 ( q2 ), might use syntactic variants , , , forth.consider profile J (with rightmost column weight + 2):J1J2J3F (J )p11011p21011pm1011q10010q20100q1 ( q2 )??10judgments agents 1 2 regarding q1 ( q2 ) irrelevant argument,indicated ? table (but note determined polynomialtime; particular, J1 (q1 ( q2 )) = 0 ).agent 3 reports judgment set truthfully (as shown table), Hammingdistance J3 collective judgment set 1 + (m + 2) = + 3. Noteagent 3 decisive propositional variables (i.e., premises) except q1 (whichcertainly get rejected). Now:satisfiable, agent 3 report judgments regarding p1 , . . . , pm correspond satisfying assignment . furthermore accepts q2 , + 2copies q1 ( q2 ) get accepted collective judgment set. Thus,Hamming distance J3 new outcome + 2, i.e., agent 3manipulated successfully.satisfiable, way get m+2 copies q1 (q2 )accepted (and q1 get rejected case). Thus, agent 3 meansimproving Hamming distance + 3 guaranteereporting truthfully.Hence, satisfiable agent 3 manipulate successfully, reductionSat Manip(PBP) complete.Thus, manipulating PBP significantly harder using it, least terms worstcase complexity (and assumption P 6= NP).5. Safety Agendasection, introduce concept safety agenda: agenda safegiven aggregation procedure F , collective judgment set returned Fconsistent (consistent) input profile. course, question relevantaggregation procedures always consistent begin with,consider PBP DBP section. fact, main interestsafety agenda entire classes aggregation procedures, characterised set498fiComplexity Judgment Aggregationaxioms AX: safe class F [AX] aggregation procedures safe everyprocedure F F [AX].defining problem relating so-called agenda characterisation results (orpossibility theorems, shall call them) studied JA literature, characterise safeagendas number natural combinations axioms establish computationalcomplexity checking safety agenda cases.5.1 Problem Definitionperforming aggregation judgments, would like avoid paradoxical outcomes,i.e., would like ensure collective judgment set consistent. Whetherindeed case depends several factors: aggregation procedure,agenda, individual judgment sets. cannot control choices individualsmake. might even know aggregation procedure exactly goinguse; might know properties, i.e., might knowbelongs certain class procedures. nevertheless guarantee collectivejudgment set consistent? formalise question follows:Definition 9. agenda safe respect class aggregation procedures F,every procedure F consistent applied profiles judgment sets .example paradox presented introduction demonstrates unsafetyagenda {p, p, q, q, p q, (p q)} respect majority rule. agenda {p, p},hand, immediately seen safe respect full class weaklyrational aggregation procedures.question whether agenda safe closely related rich literature socalled agenda characterisation results (see, e.g., Nehring & Puppe, 2007; Dokow & Holzman,2010; Dietrich & List, 2007b; List & Puppe, 2009). authors asked followingkind question: given agenda given list axiomatic requirements (alwaysincluding requirement consistency), possible find aggregation proceduremeets requirements agenda? may rephrase question follows:given agenda list axioms AX (now excluding consistency), possiblefind procedure F [AX] consistent? distinguish results kindsafety theorems (which also agenda characterisations kind), shall referpossibility theorems. summarise: possibility theorem showsconsistent procedure F [AX], safety theorem shows proceduresF [AX] consistent.Note case class aggregation procedures consists single aggregationprocedure (e.g., F [WR, A, S, MI ] consists majority rule), possibility safetyresults coincide.Possibility theorems important point view mechanism designer:given certain axioms would like see satisfied, still possible designaggregation procedure meeting know certain characteristics kindagenda procedure used? is, question likelyask off-line situation once. Safety theorems, hand,likely play role on-line situation arguably particular interest499fiEndriss, Grandi, & Porelloapplications. reason actual users likely want assuranceaggregation consistent (provided certain axioms satisfied agendacertain properties) rather learn exists consistent form aggregation(satisfying certain axioms). instance, suppose want give certain guaranteesquality operations multiagent system, without full knowledge precisespecification every individual agent without full knowledge interactionprotocols going employ. might nevertheless sufficient informationsafety theorem apply, case check, given agenda, whether consistencyguaranteed. is, deciding whether safety holds question mightanswer again, many different agendas. computationalcomplexity problem relevant question.5.2 Agenda Propertiesshall see, agenda satisfies certain structural properties, mightsufficient condition ensure safety respect certain aggregation rules. turnstypes agenda properties help similar featureknown possibility theorems. Specifically, shall make use so-called median property,introduced Nehring Puppe (2007).14Definition 10. say agenda satisfies median property (MP), everyinconsistent subset inconsistent subset size 2.words, satisfies MP minimally inconsistent subset (mi-subset)2 elements. Note case known include tautologies(and thus contradictions), definition simplifies requiring mi-subset mustexactly size 2. generalise median property follows:Definition 11. Let k > 2. agenda satisfies k-median property (kMP), everyinconsistent subset inconsistent subset size k.is, MP 2MP property. Agendas satisfying MP alreadyquite simple, restriction made tighter requiring inconsistent subsetsparticular form. sequel, call inconsistent set nontrivially inconsistentcontain single formula contradiction.Definition 12. agenda satisfies simplified median property (SMP), everynontrivially inconsistent subset subset form {, } logicallyequivalent .simplification yields:Definition 13. agenda satisfies syntactic simplified median property(SSMP), every nontrivially inconsistent subset subset form {, }.14. name median property derives work Nehring Puppe (2007), analyse socialchoice functions class vector spaces called median spaces.500fiComplexity Judgment AggregationAgendas satisfying SSMP composed uncorrelated formulas, i.e., essentially equivalent agendas composed atoms alone. SMP less restrictive, allowinglogically equivalent syntactically different formulas.Observe every agenda satisfies SMP also satisfies MP. conversetrue: = {p, p, p q, (p q)} satisfies MP, SMP. Similarly, everyagenda satisfies SSMP also satisfies SMP. Again, converse true:= {p, p, p p, (p p)} satisfies SMP, SSMP.5.3 Safety Theorems: Linking Agenda Properties Axiomsprove several characterisation results safe aggregation judgments, concentrating classes procedures defined weakening axiomatisation majorityrule. begin safety theorem majority rule itself. fact, resultfamiliar literature (Nehring & Puppe, 2007), although presenteddifferent form. Despite fact known result, still provide proof,arguably simpler translating result Nehring Puppe setting.Theorem 11. agenda safe majority rule satisfies MP.Proof. Let F majority rule.() First, suppose satisfies MP. need show F (J ) consistentJ J ()n . sake contradiction, suppose not, let mi-subsetF (J ). F (J ) , 2 elements. Clearly, cannot caseF (J ) includes contradiction , would mean majority agentswould accepted . Hence, must set exactly two formulas, say, .means must accepted n+12 agents mustaccepted n+1agents.Hence,pigeon hole principle, least one agent2must accepted them, thereby contradicting individual rationality.() direction, suppose satisfy MP, i.e., mi-subsetsize k > 3. need show exists profile J F (J ) inconsistent.Let two distinct formulas . consider profile J followingproperties (recall assume n > 3): (1) first n12 agents accept formulasexcept ; (2) last n1agentsacceptformulasexcept ; (3)2n+1middlemost agent 2 accepts formula . is, individualagent accepts formulas , i.e., really build individually rationalprofile properties (note consistent subset always extendedcomplete consistent judgment set ). However, profileformulas majority get F (J ), i.e., F (J ) inconsistent.reason case able rely known result aforementioned factclasses aggregation procedures consisting single procedure, safetypossibility results coincide. Unfortunately, larger classes procedures, approachexploiting known possibility results cannot used.first establish two sufficient conditions safety agenda, two different(fairly large) classes aggregation procedures:Lemma 12. agenda satisfies SSMP, safe F [WR, U].501fiEndriss, Grandi, & PorelloProof. Consider aggregation procedure satisfies (WR) (U). Let agendasatisfies SSMP. Hence, way obtain inconsistent outcome wouldeither accept inconsistent formula accept formula syntactic complement. latter possibility excluded (WR). So, sake excluding alsoformer possibility, suppose inconsistent formula collectively accepted.individual rationality, get accepted agents. Hence, (U),collectively accepted, thus collectively rejected (WR).Lemma 13. agenda satisfies SMP, safe F [WR, U, N].Proof. Let F aggregation procedure satisfies (WR), (U) (N), letagenda satisfies SMP. sake contradiction, suppose exists profileJ J ()n F (J ) inconsistent. distinguish two cases:(1) exists set {, } F (J ) logically equivalent . givenindividual judgment sets assumed complete consistent,logically equivalent means every agent accepts also accept ,J . Together (N) entails F (J )vice versa, i.e., NJ = NF (J ). already know F (J ); thus, also get F (J ). alsoF (J ), obtained contradiction (WR).(2) exists inconsistent formula F (J ). argument usedproof Lemma 12, contradicts assumption F satisfying (U) (WR).is, obtain contradiction possible cases.Next, prove two results concerning necessary conditions safety agenda(now aim relatively narrowly defined classes aggregation procedures):Lemma 14. agenda safe F [WR, A, U, S], satisfies SMP.Proof. Let agenda violates SMP. need provide exampleaggregation procedure F satisfies (WR), (A), (U) (S) produceinconsistent outcome least one input profile. distinguish two cases:(1) Suppose violates SMP virtue mi-subset size greater 2.case also violates MP. Theorem 11 shows safemajority rule. majority rule satisfies (WR), (A), (U) (S), done.(2) possibility mi-subset consisting two formulaslogical complements, i.e., exists set form {, }|= 6|= .15 Consider following weakly rational, anonymous,unanimous systematic aggregation procedure Fh 3 individuals, defined usingnotation Proposition 1: h(0) = h(2) = 0 h(1) = h(3) = 1. is, Fhaccepts proposition accepted odd number individuals.16 Considerfollowing profile, restricted complements: J1 = {, },15. example, might p q might p.16. parity rule also used Dokow Holzman (2010) provide witness onepossibility results.502fiComplexity Judgment AggregationJ2 = {, }, J3 = {, }. Note sets consistent. However,profile (opportunely extended profile whole agenda) generateinconsistent outcome, since accepted exactly one individual.Hence, cases fails safe least one procedure F [WR, A, U, S].Lemma 15. agenda safe F [WR, A, U, I], satisfies SSMP.Proof. Let agenda violates SSMP. also violates SMP,Lemma 14 applies done.Otherwise, must two formulas |= 6= ,i.e., logical syntactic complements. Let F procedure accepts(and rejects ) least one agent accepts , accepts (and rejects )least one agent accepts , behaves like majority rule respectpropositions. F satisfies (WR), (A), (U) (I), safe F , case oneagent accepts another , collective judgment set include .ready state prove safety theorems:Theorem 16. agenda safe F [WR, A, U, S] satisfies SMP.Proof. One direction given Lemma 14. follows Lemma 13 togetherobservation F [WR, U, N] F [WR, A, U, S].characterisation safe agendas remains intact widen class aggregationprocedures consideration systematic neutral procedures:Theorem 17. agenda safe F [WR, A, U, N] satisfies SMP.Proof. One direction follows Lemma 14 together fact F [WR, A, U, S]F [WR, A, U, N]; Lemma 13 F [WR, U, N] F [WR, A, U, N].Indeed, Theorems 16 17 state safety results particularly natural classesaggregation procedures, argument easily see class FF [WR, A, U, S] F F [WR, U, N] case safe Fsatisfies SMP.drop neutrality F [WR, A, U, S] rather independence, obtaineven restrictive characterisation safe agendas:Theorem 18. agenda safe F [WR, A, U, I] satisfies SSMP.Proof. One direction given Lemma 15; follows Lemma 12 togetherF [WR, U] F [WR, A, U, I].Again, generalise result say that, class F F [WR, A, U, I]F F [WR, U], case safe F satisfies SSMP.Finally, uniform quota rules characterisation result kind seek availableliterature (albeit different name), least rules certain boundsimposed quota (Dietrich & List, 2007a). state interesting result follows(recall n number individuals):503fiEndriss, Grandi, & PorelloTheorem 19. Let k > 2. agenda safe class uniform quota rules Fmsatisfying constraint > n nk satisfies kMP.Theorem 19 reformulation Corollary 2(a) work Dietrich List (2007a)shall prove here.Let us conclude presentation safety theorems remark roleaxiom (U) results above. Recall made assumptionagenda including contradictory formulas (or complements, i.e., tautologies).make assumption (which common JA literature certainlyunreasonable), remove mentionings (U) safety results above.Indeed, ever used (U) proofs avoid situations contradiction getsunanimously rejected yet collectively accepted. wish make assumptionregarding absence contradictory formulas agenda, still removementionings (U) safety results above, provided replace mentioningsSMP property satisfying SMP including contradictoryformulas (and accordingly results involving SSMP).5.4 Membership Results Agenda Propertiesidentified conditions guarantee safety givenagenda, want know difficult decide whether conditions satisfied.shall see, problem p2 -complete classes aggregation proceduresconsidered. p2 (also known coNPNP coNP NP oracle) complexity class located second level polynomial hierarchy (Meyer & Stockmeyer, 1972;Stockmeyer, 1976; Arora & Barak, 2009). class decision problemscertificate negative answer verified polynomial time machineaccess oracle answering queries Sat (or NP-complete problem).prove problem p2 -complete, prove membership p2 p2 -hardness.begin proving membership p2 . so, need provide algorithmthat, provided certificate intended establish negative answer, verifycorrectness certificate polynomial time, assume algorithmaccess Sat oracle. sequel, shall write MP median propertyproblem deciding whether given agenda satisfies median property,similarly SMP, SSMP kMP.Lemma 20. MP, SMP, SSMP, kMP p2 .Proof. shall present proof kMP, intuitively difficult fourproblems. proofs three problems similar.need give algorithm decides correctness certificate violationkMP polynomial time, assuming access Sat oracle. given agenda(with = ||), certificate set (a) needs inconsistent(b) must inconsistent subsets size 6 k. Inconsistency checkedPsingle query Sat oracle. = ||, ki=1 mi nonemptysubsets size 6 k, polynomial (and thus also m).17 Hence, secondcondition checked polynomial number queries oracle.17. figure polynomial k, affect argument, k constant.504fiComplexity Judgment Aggregation5.5 Hardness Results Agenda PropertiesNext, want show MP, SMP, SSMP kMP p2 -hard. donegiving polynomial-time reduction problem already known p2 -hardproblem investigation. purpose, make use quantified booleanformulas (QBFs). QSat, satisfiability problem18 general QBFs, PSPACEcomplete, imposing suitable syntactic restrictions generate complete problemslevel polynomial hierarchy. Consider QBF following form:x1 xr y1 ys .(x1 , . . . , xr , y1 , . . . , ys )arbitrary propositional formula {x1 , . . . , xr } {y1 , . . . , ys } setpropositional variables occurring (that is, could QBFexistential quantifiers occur inside scope universal quantifiers). problemchecking whether formula form satisfiable (i.e., true), shall denoteSat, known p2 -complete (Arora & Barak, 2009). Below, shall abbreviateformulas type writing xy.(x, y).basic intuition MP related problems p2 -hard sharebasic structure Sat, asking question form subsetsinconsistent, exist subset certain property? Indeed, embedding,say, MP Sat relatively straightforward. However, require opposite:need show even though Sat may appear general MPagenda problems, actually reduced problems.first prove technical lemma. Let Sat2 problem checking whetherQBF following form true, given already know (i) tautology,(ii) contradiction, (iii) logically equivalent literal:xy.(x, y) xy.(x, y)Lemma 21. Sat2 p2 -hard.Proof. reduction Sat: Given QBF form xy.(x, y), showchecking satisfiability equivalent running Sat2 ( a) buniversally b existentially quantified, two new propositional variables boccurring , i.e., checking satisfiability formulaxayb.[((x, y) a) b] xayb.[((x, y) a) b].First, note ( a) b cannot tautology, contradiction, equivalent literal;side constraints specified definition Sat2 satisfied. Also notefirst conjunct true exactly original formula xy.(x, y) true.b always set true, original formula true wheneverset false (a falls scope universal quantifier). Therefore, positive answerSat2 instance entails positive answer original Sat instance.direction immediate, second conjuncts always satisfiable(by making b false).18. shall speak satisfiability problems QBFs, even though strictly speaking QBFsdistinction satisfiability, truth validity, every QBF closed formula.505fiEndriss, Grandi, & Porelloable prove p2 -hardness SSMP:Lemma 22. SSMP p2 -hard.Proof. shall give polynomial-time reduction Sat2 SSMP; claimfollows Lemma 21. Take instance Sat2 , i.e., question whetherxy.(x, y) xy.(x, y) true 6|= , 6|= , 6|=literals . Suppose x = hx1 , . . . , xr i, define agenda follows:19= {x1 , x1 , x2 , x2 , . . . , xr , xr , ( ), ( )}prove violates SSMP answer Sat2 -questionNO. see this, consider following facts. First, suppose violates SSMP.circumstances case? neither tautology contradiction,inconsistent subset must nontrivially inconsistent. Furthermore, construction(consisting largely literals), inconsistent subset including pairsyntactic complements must include either ( ) ( ), well (complementfree) subset {x1 , x1 , . . . , xr , xr }. is, way violating SSMPfind subset literals {x1 , x1 , . . . , xr , xr } make true forces either ( )( ) false. precisely situation instance Sat2requires negative answer.direction, suppose answer Sat2 -question NO. meansable find assignment variables x makes eitherunsatisfiable. W.l.o.g., suppose latter situation. Construct subset ,containing ( ), includes literal xi set true assignment ,xi otherwise. inconsistent subset , since neither tautologycontradiction, falsifies SSMP.Proving hardness SMP works similarly:Lemma 23. SMP p2 -hard.Proof. construction used proof Lemma 22. additionalinsight required observation special kind agenda constructedproof, SMP SSMP coincide: excluding formulas equivalentliterals, ensure agenda constructed previous proof containpairs logically equivalent formulas.MP give proof using reduction SSMP:Lemma 24. MP p2 -hard.Proof. show reduce problem deciding SSMP instance MP.Let agenda want test SSMP let + = {1 , . . . , }set non-negated formulas . build set + following way: copyformulas + times, every time renaming variables occurring , obtaining19. Using ( ) rather ensures agenda include doubly-negated formulas.506fiComplexity Judgment Aggregationformulas ji 1 6 i, j 6 m. every substitute ii ii pi , pi new variableoccurring ji . Finally, add p1 , . . . , pm + . obtain following set:+ = {p1 , 11 p1 , . . . , 1m ,p2 , 21 , 22 p2 , . . . , 2m ,...p ,1 , . . . , p }Define = + { | + }. show satisfies SSMPsatisfies MP. One direction immediate. Suppose satisfy SSMP.must mi-subset size k > 2.20 Let = {i1 , . . . , ik }. existssubset , namely = {pi1 , ii11 pi1 , ii12 , . . . , ii1k }, mi-set size k + 1 > 3,thereby falsifying MP.opposite direction, suppose satisfy MP. is,mi-subset size > 3. construction , know subset mustcontain formulas superscript complements (all formulasdifferent variables). subset contain pi pi , find copy, violates SSMP, case done. Clearly, cannot includepi pi , would contradict || > 3. left casesincludes either pi pi i. Then, minimality, also ii pi negationmust included. reason cases: (1) pi ii pi ,dropping disjunction still get inconsistent subset, assumptionminimality; (2) pi (ii pi ) cannot reason; (3) finally,pi together negation ii pi already inconsistent. Therefore, concludemust form {pi , ii pi }i , set (one more) formulassuperscript i. easy see set obtain removesuperscript {ii } mi-subset falsifies SSMP. particular,ii 6 , ii 6 construction, i.e., mi-subset obtainconsist two formulas logical complements.Finally, establish hardness kMP:Lemma 25. kMP p2 -hard every k > 2.Proof. k = 2, claim established Lemma 24. observeuse exactly construction proof Lemma 24 reduce instancekMP k > 2 instance corresponding (k+1)MP. Hence, simpleinductive argument, kMP must p2 -hard finite k > 2.5.6 Complexity Safety Agendashown deciding whether given agenda satisfies MP, SMP,SSMP, kMP p2 p2 -hard. Furthermore, Section 5.3 linkedproperties safety various combinations axioms. immediatecorollary results, obtain theorem concerning complexity decidingsafety agenda:20. fact cannot contain two formulas logical complements relevant proof.507fiEndriss, Grandi, & PorelloTheorem 26. Deciding problem safety agenda p2 -completefollowing classes aggregation procedures:(i)(ii)(iii)(iv)(v)F [WR, A, S, MI ], consisting majority rule;F [WR, A, U, S], systematic procedures;F [WR, A, U, N], neutral procedures;F [WR, A, U, I], independent procedures;class uniform quota rules Fm > n nk k > 2.Proof. Concerning p2 -hardness, (i) direct consequence Theorem 11 Lemma 24.way, (ii) derived Theorem 16 Lemma 23, (iii) Theorem 17Lemma 23, (iv) Theorem 18 Lemma 22. Finally, (v) follows Theorem 19together Lemma 25. Membership p2 follows Lemma 20 five cases.is, case safety agenda guaranteedstructurally simple agendas, deciding whether given agenda meets structuralconstraints highly intractable. negative result sense concernsproblem would like able solve efficiently. stressrender problem hopeless. Work QBF solvers seen lot progress recent years(see, e.g., Narizzano, Pulina, & Tacchella, 2006), tools could deployed checkwhether agenda satisfies given type median property.21 event, understandingnaturally arising question JA relates difficult well-studied algorithmicproblem Sat interesting worthwhile right.6. Related Work: Computational Perspectives Judgment AggregationStarting work List Pettit (2002), research JA focussed eitherphilosophical implications fact aggregation may result inconsistentoutcome derivation impossibility characterisation results. extensiveliterature field recently reviewed List Puppe (2009). workalso explored links JA preference aggregation (Dietrich & List, 2007b;Grossi, 2009; Porello, 2010; Grandi & Endriss, 2011) several recent contributionsfurthermore focussed definition analysis specific aggregation procedures (Dietrich & List, 2007a; Dietrich & Mongin, 2010; Miller & Osherson, 2009; Lang et al., 2011).shall instead concentrate contributions JA either computationalslant otherwise relevant AI.Besides previous work subject present paper (Endriss et al., 2010a,2010b), small number contributions computational social choicetaking computational perspective JA (Nehama, 2010; Slavkovik & Jamroga, 2011;Baumeister, Erdelyi, & Rothe, 2011; Baumeister, Erdelyi, Erdelyi, & Rothe, 2012):first example work Nehama (2010), proposes framework approximate JAgoal finding aggregation procedure never return inconsistent21. pointed one anonymous reviewer, Answer Set Programming may also useful frameworkreason safety problems. DLV System, instance, provides flexible tooldeciding arbitrary problems located second level polynomial hierarchy (Leone, Pfeifer,Faber, Eiter, Gottlob, Perri, & Scarcello, 2006).508fiComplexity Judgment Aggregationjudgment set replaced goal finding procedure returninginconsistent set highly unlikely. (negative) result obtained frameworkhowever extend range available procedures significant way. Second,Slavkovik Jamroga (2011) extend standard JA framework weights (to modeldifferences influence individuals) provide upper bound complexitywinner determination problem family distance-based aggregation procedures.Third, Baumeister et al. (2011) provide first study computational complexitybribery problem JA, asking whether possible obtain desired outcomek individual agents bribed change judgment set. Finally, Baumeisteret al. (2012) discuss complexity various forms controlling judgment aggregationprocesses, e.g., influencing outcome adding removing judges.clearest example work explores integration ideas JA ideascoming field traditionally studied AI recent work connectionsJA abstract argumentation frameworks (Rahwan & Tohme, 2010; Caminada & Pigozzi,2011): problem commonly studied abstract argumentation decideones set arguments mutually attack accept, reject,remain undecided. Rahwan Tohme (2010) study variantproblem group agents decide status award argument,problem naturally lends viewed lens JA. related work,Caminada Pigozzi (2011) proposed approach JA involves translationabstract argumentation framework, makes tools techniques abstractargumentation available aggregation judgments.field research within AI closely related JA belief merging (see, e.g.,Konieczny & Pino Perez, 2002; Maynard-Zhang & Lehmann, 2003; Chopra et al., 2006;Everaere et al., 2007). work Konieczny Pino Perez (2002), particular,inspired distance-based procedure JA used paper. JA beliefmerging modelled Konieczny Pino Perez share interesting features, ultimatelystudy different problems. JA individuals assumed submit consistent judgment sets, belief merging constraint enforced outcome. reflectsview consistency belief merging (modelled terms integrity constraint)feasibility requirement, JA amounts rationality assumption.7. Conclusions Future Workstudied computational complexity three problems JA: computingwinning judgment set given aggregation procedure, deciding whether manipulationwould beneficial given agent given aggregation procedure givenprofile, deciding safety agenda given class aggregation procedures.also proven several safety theorems link safety simple structural propertiesagenda provide interesting counterpart known possibility theorems.results show that, winner determination problem easy quota rulespremise-based procedure, intractable otherwise attractive distance-basedprocedure. Regarding strategic manipulation, seen manipulation NP-hardpremise-based procedure, positive result. also seenquota rules question manipulation complexity arise, least509fiEndriss, Grandi, & Porellomodel preferences used here. distance-based procedure, investigatedcomplexity manipulation problem, already winner determinationproblem found intractable. work safety agenda,derived characterisation results wide range procedures, defined terms commonlyused axioms. seen safety guaranteed relatively simple agendasalso seen deciding whether simplicity conditions met highlyintractable.work computational aspects JA far limited smallnumber interesting scattered contributions, strongly believe JAtaken important research topic AI computational social choice. Oneimportant direction pursue concerns practical algorithms problems studiedpaper (as well related problems naturally arising JA). already mentionedexisting work algorithms winner determination problem Kemenyrule preference aggregation (Conitzer et al., 2006) may provide starting pointworking implementation distance-based procedure work QBF solversautomated reasoning (Narizzano et al., 2006) work Answer Set Programming (Leoneet al., 2006) could prove helpful tackling challenges identified complexityresults regarding safety agenda.Alongside development practical algorithms, improving understandingalgorithmic aspects JA studying framework parameterised complexity wouldalso great interest. context voting, approach lead numberinsightful results (Betzler, 2010). Indeed, JA, initial steps direction alreadytaken Baumeister et al. (2011).Studying winner determination problem, complexity-theoretic practical terms, distance-based procedures proposed Miller Osherson (2009)Lang et al. (2011) also constitutes worthwhile direction future work.Recall analysed manipulation one particular way defining preferences,namely terms Hamming distance agents true set judgments. Thus,would interesting investigate extent changing definition manipulation(by altering notion induced preference) affects complexity result. Indeed,notions induced preference (and thus manipulation) conceivable. instance,would-be manipulator might interested status specific propositions (e.g.,conclusions) might use different notion distance, e.g., one recentlyproposed Duddy Piggins (2012).justified decision study complexity manipulationproblem distance-based procedure fact already much basicwinner determination problem p2 -complete. important question believerequires discussion research community whether indeed valid argument.context voting, initial idea Bartholdi et al. (1989) that, say,NP-hardness result manipulation problem particular voting rule might suggestrule immune manipulation practice. Recent work stronglysuggests case (Faliszewski & Procaccia, 2010), kindNP-hard problems encountered context algorithms perform well practicerelatively easy design (Walsh, 2011). question arises whetherstill true hardness results respect higher complexity classes. instance,510fiComplexity Judgment Aggregationconceivable possible design algorithms efficiently solvetypical instances winner determination problem distance-based procedure,might turn much difficult design similarly successful algorithmcorresponding manipulation problem. is, question arises whether hardnessof-manipulation studies need restricted problems winner determinationpolynomial, whether jump complexity desirable principle mightprovide level protection practice.Regarding safety agenda, given results natural combinations axioms correspond weakening majority rule, similar studycould also conducted combinations axioms. Indeed, would interestingexplore robust p2 -completeness results are. is, open question suggestswhether exists interesting relevant class aggregation proceduressafety problem falls different complexity class.Generally speaking, believe much work exploring obvious potentialJA AI multiagent systems needed. lead practical advancesdefinition interesting new theoretical problems. steps directionrecently taken Slavkovik (2012), concerning modelling collective decisionmaking multiagent systems, Caminada Pigozzi (2011) RahwanTohme (2010), concerning applications JA abstract argumentation.Acknowledgmentspaper builds earlier work complexity judgment aggregation presentedAAMAS-2010 (Endriss et al., 2010a) COMSOC-2010 (Endriss et al., 2010b).would like thank reviewers members audiences meetings, threereviewers Journal Artificial Intelligence Research, attendees workshop seminar talks given topic Amsterdam, Barcelona, Chongqing,Luxembourg, Moscow, New Delhi, Padova, Paris, Pisa, Tilburg many helpfulsuggestions received.ReferencesArora, S., & Barak, B. (2009). Computational Complexity: Modern Approach. CambridgeUniversity Press.Bartholdi, J. J., Tovey, C. A., & Trick, M. A. (1989). computational difficultymanipulating election. Social Choice Welfare, 6 (3), 227241.Baumeister, D., Erdelyi, G., Erdelyi, O. J., & Rothe, J. (2012). Control judgment aggregation. Proceedings 6th Starting AI Researchers Symposium (STAIRS-2012).IOS Press.Baumeister, D., Erdelyi, G., & Rothe, J. (2011). hard bribe judges?study complexity bribery judgment aggregation. Proceedings2nd International Conference Algorithmic Decision Theory (ADT-2011). SpringerVerlag.511fiEndriss, Grandi, & PorelloBetzler, N. (2010). Multivariate Complexity Analysis Voting Problems. Ph.D. thesis,University Jena.Brandt, F., Conitzer, V., & Endriss, U. (2012). Computational social choice. Weiss, G.(Ed.), Multiagent Systems. MIT Press. press.Caminada, M., & Pigozzi, G. (2011). judgment aggregation abstract argumentation.Autonomous Agents Multi-Agent Systems, 22 (1), 64102.Chevaleyre, Y., Endriss, U., Lang, J., & Maudet, N. (2007). short introduction computational social choice. Proceedings 33rd Conference Current TrendsTheory Practice Computer Science (SOFSEM-2007). Springer-Verlag.Chopra, S., Ghose, A., & Meyer, T. (2006). Social choice theory, belief merging,strategy-proofness. Information Fusion, 7 (1), 6179.Conitzer, V., Davenport, A. J., & Kalagnanam, J. (2006). Improved bounds computing Kemeny rankings. Proceedings 21st National Conference ArtificialIntelligence (AAAI-2006).Dietrich, F. (2007). generalised model judgment aggregation. Social ChoiceWelfare, 28 (4), 529565.Dietrich, F., & List, C. (2007a). Judgment aggregation quota rules: Majority votinggeneralized. Journal Theoretical Politics, 19 (4), 391424.Dietrich, F., & List, C. (2007b). Arrows theorem judgment aggregation. Social ChoiceWelfare, 29 (1), 1933.Dietrich, F., & List, C. (2007c). Strategy-proof judgment aggregation. EconomicsPhilosophy, 23 (3), 269300.Dietrich, F., & Mongin, P. (2010). premiss-based approach judgment aggregation.Journal Economic Theory, 145, 562582.Dokow, E., & Holzman, R. (2010). Aggregation binary evaluations. Journal EconomicTheory, 145 (2), 495511.Duddy, C., & Piggins, A. (2012). measure distance judgment sets. SocialChoice Welfare, 39 (4), 855867.Endriss, U., Grandi, U., & Porello, D. (2010a). Complexity judgment aggregation: Safetyagenda. Proceedings 9th International Conference AutonomousAgents Multiagent Systems (AAMAS-2010).Endriss, U., Grandi, U., & Porello, D. (2010b). Complexity winner determinationstrategic manipulation judgment aggregation. Proceedings 3rd International Workshop Computational Social Choice (COMSOC-2010).Everaere, P., Konieczny, S., & Marquis, P. (2007). strategy-proofness landscapemerging. Journal Artificial Intelligence Research (JAIR), 28 (1), 49105.Faliszewski, P., & Procaccia, A. D. (2010). AIs war manipulation: winning?. AIMagazine, 31 (4), 5364.Gaertner, W. (2006). Primer Social Choice Theory. LSE Perspectives EconomicAnalysis. Oxford University Press.512fiComplexity Judgment AggregationGrandi, U., & Endriss, U. (2011). Binary aggregation integrity constraints. Proceedings 22nd International Joint Conference Artificial Intelligence (IJCAI2011).Grossi, D. (2009). Unifying preference judgment aggregation.. Proceedings 8thInternational Conference Autonomous Agents Multiagent Systems (AAMAS2009).Hemachandra, L. A. (1989). strong exponential hierarchy collapses. Journal Computer System Sciences, 39 (3), 299322.Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (1997). Exact analysis Dodgsonelections: Lewis Carrolls 1876 system complete parallel access NP. JournalACM, 44 (6), 806825.Hemaspaandra, E., Hemaspaandra, L. A., & Menton, C. (2012). Search versus decisionelection manipulation problems. Tech. rep. URCS-TR-2012-971, UniversityRochester, Computer Science Department.Hemaspaandra, E., Spakowski, H., & Vogel, J. (2005). complexity Kemeny elections.Theoretical Computer Science, 349 (3), 382391.Kemeny, J. (1959). Mathematics without numbers. Daedalus, 88 (4), 577591.Konieczny, S., & Pino Perez, R. (2002). Merging information constraints: logicalframework. Journal Logic Computation, 12 (5), 773808.Kornhauser, L. A., & Sager, L. G. (1993). one many: Adjudication collegialcourts. California Law Review, 81 (1), 159.Lang, J., Pigozzi, G., Slavkovik, M., & van der Torre, L. (2011). Judgment aggregationrules based minimization. Proceedings 13th Conference TheoreticalAspects Rationality Knowledge (TARK-XIII).Leone, N., Pfeifer, G., Faber, W., Eiter, T., Gottlob, G., Perri, S., & Scarcello, F. (2006).DLV system knowledge representation reasoning. ACM TransactionsComputational Logic, 7 (3), 499562.List, C., & Pettit, P. (2002). Aggregating sets judgments: impossibility result. Economics Philosophy, 18 (1), 89110.List, C., & Puppe, C. (2009). Judgment aggregation: survey. Handbook RationalSocial Choice. Oxford University Press.Maynard-Zhang, P., & Lehmann, D. J. (2003). Representing aggregating conflictingbeliefs. Journal Artificial Intelligence Research (JAIR), 19, 155203.Meyer, A. R., & Stockmeyer, L. J. (1972). equivalence problem regular expressionssquaring requires exponential space. Proceedings 13th Annual Symposium Switching Automata Theory (SWAT/FOCS-1972). IEEE ComputerSociety.Miller, M., & Osherson, D. (2009). Methods distance-based judgment aggregation. SocialChoice Welfare, 32 (4), 575601.513fiEndriss, Grandi, & PorelloNarizzano, M., Pulina, L., & Tacchella, A. (2006). QBFEVAL web portal. Proceedings 10th European Conference Logics Artificial Intelligence (JELIA2006). Springer-Verlag.Nehama, I. (2010). Approximate judgment aggregation. Proceedings 3rd International Workshop Computational Social Choice (COMSOC-2010).Nehring, K., & Puppe, C. (2007). structure strategy-proof social choice. Part I: General characterization possibility results median spaces. Journal EconomicTheory, 135 (1), 269305.Nehring, K., & Puppe, C. (2010). Abstract Arrowian aggregation. Journal EconomicTheory, 145 (2), 467494.Papadimitriou, C. H. (1981). complexity integer programming. JournalACM, 28 (4), 765768.Papadimitriou, C. H. (1994). Computational Complexity. Addison Wesley.Pettit, P. (2001). Deliberative democracy discursive dilemma. Philosophical Issues,11 (1), 268299.Pigozzi, G. (2006). Belief merging discursive dilemma. Synthese, 152 (2), 285298.Porello, D. (2010). Ranking judgments Arrows setting. Synthese, 173 (2), 199210.Rahwan, I., & Tohme, F. (2010). Collective argument evaluation judgement aggregation. Proceedings 9th International Conference Autonomous AgentsMultiagent Systems (AAMAS-2010).Rothe, J., Spakowski, H., & Vogel, J. (2003). Exact complexity winner problemYoung elections. Theoretical Computer Science, 63 (4), 375386.Slavkovik, M. (2012). Judgment Aggregation Multiagent Systems. Ph.D. thesis, University Luxembourg.Slavkovik, M., & Jamroga, W. (2011). Distance-based judgment aggregation three-valuedjudgments weights. Proceedings IJCAI-2011 Workshop Social ChoiceArtificial Intelligence.Stockmeyer, L. J. (1976). polynomial-time hierarchy. Theoretical Computer Science,3 (1), 122.Wagner, K. W. (1987). complicated questions maxima minima,closures NP. Theoretical Computer Science, 51 (12), 5380.Walsh, T. (2011). hard manipulation problems?. Journal Artificial Intelligence Research (JAIR), 42, 129.514fiJournal Artificial Intelligence Research 45 (2012) 287-304Submitted 05/12; published 10/12Research NoteRemoving Redundant Messages N-ary BnB-ADOPTPatricia GutierrezPedro MeseguerPATRICIA @ IIIA . CSIC . ESPEDRO @ IIIA . CSIC . ESIIIA - CSIC, Universitat Autonoma de Barcelona08193 Bellaterra, SpainAbstractnote considers modify BnB-ADOPT, well-known algorithm optimally solvingdistributed constraint optimization problems, double aim: (i) avoid sendingredundant messages (ii) handle cost functions arity. messages exchangedBnB-ADOPT turned redundant. Removing redundant messages increasessubstantially communication efficiency: number exchanged messages casesleast three times fewer (keeping measures almost unchanged), terminationoptimality maintained. hand, handling n-ary cost functions addressedoriginal work, presence thresholds makes practical usage complex. issuesremoving redundant messages efficiently handling n-ary cost functionscombined, producing new version BnB-ADOPT+ . Experimentally, show benefitsversion original one.1. IntroductionDistributed Constraint Optimization Problems (DCOPs) used model many actual worldmultiagent coordination problems, meeting scheduling (Maheswaran, Tambe, Bowring,Pearce, & Varakantham, 2004), sensor network (Jain, Taylor, Tambe, & Yokoo, 2009), traffic control (Junges & Bazzan, 2008), coalition structure generation (Ueda, Iwasaki, & Yokoo, 2010).DCOPs include finite number agents, usual assumption agent holds one variable finite discrete domain. Variables related cost functions define costevery combination value assignments. costs represent preferences penalty relations.cost particular assignment sum cost functions evaluated assignment.goal find complete assignment minimum cost message passing.Considering distributed search DCOP solving, first proposed complete asynchronousalgorithm ADOPT (Modi, Shen, Tambe, & Yokoo, 2005). Later on, closely related BnBADOPT (Yeoh, Felner, & Koenig, 2010) presented. BnB-ADOPT changes naturesearch ADOPT best-first search depth-first branch-and-bound strategy, obtaining betterperformance. algorithms complete, compute optimum cost guaranteedterminate. ADOPT BnB-ADOPT similar communication strategy, using similar setmessages (with small differences processes). also share data structuressemantics store update internal tables.last years, several complete DCOP algorithms proposed. Following classification appears work Yeoh et al. (2010), search algorithm class mention (inaddition previously cited ADOPT BnB-ADOPT), Synchronous Branch Bound (SBB)(Hirayama & Yokoo, 1997), Commitment Branch Bound (NCBB) (Chechetka & Sycara,c2012AI Access Foundation. rights reserved.fiG UTIERREZ & ESEGUER2006) Asynchronous Forward Bounding (AFB) (Gershman, Meisels, & Zivan, 2009). distributed inference algorithms mention Dynamic Programming Optimization (DPOP) (Petcu &Faltings, 2005). interest ADOPT BnB-ADOPT comes fact requirepolynomial memory, asynchronous communication limited neighbors. Distributedsearch algorithms use polynomial memory, DPOP requires exponential memory worstcase. Asynchronous algorithms shown suitable distributed context,agents active time approach globally robust failures. SBB NCBBexamples synchronous algorithms. Limiting communication neighbors (no agent communicate agent constrained with) common requirement applications(for instance sensor networks). algorithms, AFB, restriction allow agents broadcast messages.ADOPT lesser extent BnB-ADOPT exchange large number messages. Often,major drawback practical applicability, despite good theoretical properties(soundness, optimality, termination). Aiming decreasing number exchanged messageswithout compromising optimality termination, paper contains results detecting redundant messages BnB-ADOPT. Using results generate new version avoids sendingredundant messages. Experimentally, seen removing redundantmessages significantly decreases communication costs (the new algorithm exchanges least threetimes fewer messages) several widely used DCOP benchmarks. proposed modificationsalso applied ADOPT (Gutierrez & Meseguer, 2010a), although detail resultssince according experiments BnB-ADOPT usually achieves better performance.DCOPs, somehow natural use binary cost functions, agent one-toone relation neighbors. Since agent holds single variable, naturallybrings binary functions. However, cases agent may cost function higherarity subset agents. original ADOPT (Modi et al., 2005) proposes way dealn-ary constraints, BnB-ADOPT takes exact strategy deal constraints arityhigher two. However, strategy may cause inefficiency applied BnB-ADOPT.provide simple way correct inefficiency, easily integrated BnB-ADOPT.combined improvements new version algorithm called BnB-ADOPT+ .Experimental results show benefits proposed approach respect original algorithm. comparison state-of-the-art DCOP algorithms also included.paper structured follows. First, provide basic definitions short description BnB-ADOPT algorithm Section 2. introduce detection redundant messagesSection 3. discuss generalize algorithm deal efficiently cost functionsarity Section 4. Using results, propose new version BnB-ADOPT, calledBnB-ADOPT+ . Section 5 report experimental results benefits BnB-ADOPT+respect previous version. addition, present results comparing BnB-ADOPT+DCOP algorithms. Finally, conclude Section 6.2. BackgroundSection provide basic DCOP definitions short description BnB-ADOPTalgorithm (Yeoh et al., 2010).288fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPT2.1 DCOPDistributed Constraint Optimization Problem (DCOP) (Modi et al., 2005), formally defined(X , D, F, A, ) where:X = {x1 , ..., xn } set n variables.= {D1 , ..., Dn } collection finite domains variable xi takes values Di .F set binary cost functions; fij C involving set variables var(f ) = {xi , xj }mapping Di Dj 7 N {0, } associates non-negative cost combinationvalues variables xi xj . scope f var(f ), arity |var(f )|.= {1, . . . , p} set p agents.: X A, maps variable one agent.cost function (also called soft constraint elsewhere) f evaluated particular value tuplegives cost one pay taking values variables var(f ). Completelypermitted tuples f 0 cost, completely forbidden tuples cost. Intermediatecosts associated tuples neither completely permitted completely forbidden.cost complete assignment sum evaluation cost functions assignment.solution complete assignment cost user threshold. solution optimalminimum cost. make usual simplifying assumption agent owns exactly onevariable, agents variables used interchangeably (we connect agents variablessubindexes, agent owns variable xi ). also assume cost function f among several variablesknown every agent owns variable var(f ) (Yokoo, Durfee, Ishida, & Kuwabara, 1998).Agents communicate messages never lost and, pair agents, messagesdelivered order sent.constraint graph represents DCOP instance, nodes graph correspond variables edges connect pairs variables appearing cost function. depth-first search(DFS) pseudo-tree arrangement nodes edges constraint graph satisfies(i) subset edges, called tree edges, form rooted tree (ii) two variablescost function appear branch tree. edges called backedges. Treeedges connect parent-child nodes, backedges connect node pseudo-parentspseudo-children. DFS pseudo-trees constructed using distributed algorithms (Petcu, 2007).2.2 BnB-ADOPTBnB-ADOPT (Yeoh et al., 2010) distributed algorithm optimally solves DCOPs usingdepth-first branch-and-bound search strategy. closely related ADOPT (Modi et al., 2005),maintaining data structures communication framework. BnB-ADOPT starts constructing DFS pseudo-tree arrangement agents. this, agent knows parent,pseudo-parents, children pseudo-children.2.2.1 DATA TRUCTURESexecution agent maintains: current value di ; current context Xi ,knowledge current value assignment ancestors; timestamp current value289fiG UTIERREZ & ESEGUERdi value assignment current context (so value recency compared); everyvalue Di context Xi , lower upper bound LBi (d) UBi (d); two bounds LBiUBi calculated following way:(d) =P(xj ,dj )Xifij (d, dj )PLBi (d) = (d) + P xc children lbi,c (d)UBi (d) = (d) + xc children ubi,c (d)LBi = mindDi {LBi (d)}UBi = mindDi {UBi (d)}(d) sum costs cost functions ancestors given assignsvalue ancestors assign respective values Xi . Tables lbi,c (d) ubi,c (d) store upperlower bounds children c, values Di current context Xi . LBi UBi lowerupper bounds optimal solution context Xi . Due memory limitations, agentstore lower upper bounds one context. Agents may reinitialize bounds timecontext change.goal every agent explore search space ultimately chooses valueminimizes LBi . agent BnB-ADOPT changes value assignment abledetermine optimal solution value provably better best solution foundfar current context. words, LBi (di ) UBi current value di .prune values search, agents use threshold value H, initially . Hroot remainsentire solving process. agents, threshold values calculated sentparent children following way. threshold th sent agent value child ccalculated as:th = min(T Hi , U Bi ) (d)Pchchildren,ch6=c lbi,ch (d)Thresholds represent estimated upper bound current context. Therefore, agentsprune values using thresholds sent parents. is, agent changes value di (prunesit) LBi (di ) min{THi , UBi }. LBi = U Bi , agent reached cost optimalsolution context Xi .2.2.2 C OMMUNICATIONcommunication needed BnB-ADOPT calculate global costs individual agentsassignments coordinate search towards optimal solution. BnB-ADOPT agents use threetypes messages: VALUE, COST TERMINATE, defined follows:VALUE(i; j; val; th): agent informs child pseudo-child j takes value valthreshold th;COST(k; j; context; lb; ub): agent k informs parent j context bounds lb, ub;TERMINATE(i; j): agent informs child j terminates.290fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPTmentioned above, BnB-ADOPT value assignment timestamped (both VALUECOST messages). permits VALUE COST messages update context receiveragent, values recent.Upon reception VALUE message, value val copied receiver context timestamprecent, threshold th updated sender parent receiver.Upon reception COST message child c, values context COST messagerecent receiver context copied receiver agent. receiver contextcompatible COST message context, agent updates lower upper boundslbi,c (d) ubi,c (d) lower upper bounds COST message, respectively. Otherwise, COST message discarded. Contexts compatible iff agree common agentvalue pairs. Every time context change, agents check bounds must reinitialized.2.2.3 E XECUTIONBnB-ADOPT agent executes following loop. First, reads processes incoming messages. completely processing message queue, changes value lower boundcurrent value surpasses min{THi , UBi }, case value proven suboptimal current context. Finally, agent sends following messages: VALUE per child,VALUE per pseudo-child COST parent. process repeats root agent rreaches termination condition LBr = UBr , means found minimum cost.sends TERMINATE message children terminates. Upon receptionTERMINATE message, agent sends TERMINATE messages children; agent terminatesLBi = U Bi .rest paper, assume reader familiarity BnB-ADOPT (fordetailed description, see original source, Yeoh et al., 2010).3. Redundant MessagesSection present results redundant messages considering BnB-ADOPT workingDCOP instances binary cost functions. following i, j k agents, executingBnB-ADOPT. Agent i, holding variable xi , takes value v assignment xi v madeinforms children, pseudo-children parent new value assignment. statedefined by: (i) value, (ii) context, contexti , set value assignments agents locatedbranch (timestamps considered part context), (iii) possiblevalue children c, lower upper bounds lbi,c (d)/ubi,c (d).message msg sent j redundant if, future time t, messages receivedj msg would cause effect, msg could avoided. messagemsg sent j containing assignment xi v timestamp updates contextj [i] (thatis, part contextj containing value xi ) timestamp t0 > t0 .Lemma 1 BnB-ADOPT, agent sends two consecutive VALUE messages agent j timestamps t1 t2 , message timestamp agent assignment t1 < < t2 .Proof. VALUE message timestamp t1 t2 i, since VALUEmessages consecutive sent agent i. COST messages build contextsinformation VALUE messages. Since VALUE contains timestamp t1 t2 i,COST contain i.2291fiG UTIERREZ & ESEGUERTheorem 1 BnB-ADOPT, agent sends agent j two consecutive VALUE messagesval, second message redundant.Proof. Let V1 V2 two consecutive VALUE messages sent agent agent jvalue val timestamps t1 t2 , t1 t2 . t1 = t2 , V2 always discarded (andtherefore V2 always redundant), concentrate second option, t1 < t2 .V1 V2 agent j may receive messages coming agents. V1 reaches j,following cases possible:1. V1 update contextj [i] (V1 discarded). V2 arrives j may happen:(a) V2 update contextj [i] (V2 discarded). Future messages processedV2 received, V2 redundant.(b) V2 updates contextj [i] timestamp t. two options: (i) t2 > >t1 (ii) t2 > = t1 . Option (i) impossible according Lemma 1. Option(ii) possible, since = t1 value contained V2 already contextj [i].future messages, every message accepted timestamp t2 contextj [i] wouldalso accepted timestamp t1 contextj [i]. Since messagestimestamp t1 t2 i, conclude V2 redundant.2. V1 updates contextj [i] val, timestamp t1 . V2 arrives j may happen:(a) V2 update contextj [i]; case (1.a).(b) V2 updates contextj [i]: since V1 updated contextj Lemma 1, timestampcontextj [i] must t1 . V2 change contextj [i] timestamp rewrittent2 . Since messages timestamp t1 t2 (Lemma 1),future message could update contextj t2 would also update t1 . V2redundant.considered threshold contained VALUE messages BnB-ADOPTcomplete terminates without use thresholds (they included increase efficiency).see this, enough realize BnB-ADOPT without using thresholds equivalent BnBADOPT thresholds always equal (equal initial value thresholdagent). results Section 5 work Yeoh et al. (2010) remain valid BnB-ADOPTworks thresholds.1 Therefore, BnB-ADOPT without thresholds terminates costoptimal solution.2Lemma 2 BnB-ADOPT, agent k sends two consecutive COST messages C1 C2context, k detected context change C1 C2 , messagetimestamp agent ones C1 C2 i, context incompatibleC1 C2 .Proof. time agent constrained agent k changes value sends VALUE messagechildren pseudo-children. Since context change C1 C2 , messagetimestamp ones C1 C2 contain context incompatible C1C2 ; otherwise agent k would necessarily detected context change.21. see this, enough realize thresholds used proof Section 5 proof Lemma 8.However, proof Lemma remains valid replacing threshold .292fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPTTheorem 2 BnB-ADOPT, agent k sends agent j two consecutive COST messagescontent (context, lower/upper bound) k detected context change, secondmessage redundant.Proof. Let C1 C2 two consecutive COST messages sent k j content,contextk changed sending them. message may arrive j C1C2 (coming agents). Upon reception, recent values C1 (and later C2 )copied contextj (by PriorityMerge, see Yeoh et al., 2010). j detects context change, tableslbj,c (d) ubj,c (d) could reinitialized. Otherwise, contextj compatible COSTcontext tables lbj,c (d) ubj,c (d) updated information contained COSTmessage.Copying C2 recent values contextj essential. Let us assume valuescopied. Since context change C1 C2 , message timestampsC1 C2 necessarily include context compatible C2 , accordingLemma 2. Therefore C2 arrives, C2 contains values recent timestampincompatible contextj , C2 updates timestamps.Now, let us consider possible lbj,c (d), ubj,c (d) reinitializations. Since C2 causecontext change j, updates timestamps (Lemma 2), cause lower/upperbound reinitialization j. that, proof concentrates update bounds.C1 arrives, may happen:1. C1 compatible contextj , bounds discarded. C2 arrives may happen:(a) C2 compatible contextj , bounds discarded. Since C2 = C1 (excepttimestamps), actions done detecting C2 compatiblealready done detecting C1 compatible. Thus, C2 redundant.(b) C2 compatible contextj , bounds included j. Since C1compatible, must least one agent j changed value, receivedj C1 C2 . one several VALUE messages its/their waytowards k k descendants. Upon reception, one several COST messagesgenerated. last sent k j updated bounds, C2could avoided updated COST arrive j. Consequently,C2 redundant.2. C1 compatible contextj , bounds included. C2 arrives j mayhappen:(a) C2 compatible contextj , bounds discarded. Bounds providedC2 useless, based outdated information. future updatedCOST reach j (same reasons previous case 1.b). C2 redundant.(b) C2 compatible contextj , bounds included. However, causeschange j bounds, unless bounds reinitialized. case leastone agent j changed value, information reached jC1 C2 . situation case (1.b). Hence, C2 redundant.2Temporarily, define BnB-ADOPT+ version BnB-ADOPT following changes:(i) second two consecutive VALUE messages i, j, val th sent,293fiG UTIERREZ & ESEGUER(ii) second two consecutive COST messages k, j, context, lb ubk detects context change sent. changes affect algorithms optimality,proved next.Theorem 3 BnB-ADOPT+ terminates cost optimal solution.Proof. Theorem 1 agent sends two consecutive VALUE messages valagent j, second redundant. However, differ th, also send second VALUEmessage efficiency purposes. Observe sending redundant messages causeincorrect behavior. Theorem 2, COST messages sent BnB-ADOPT+ redundanteliminated. BnB-ADOPT terminates cost optimal solution (Yeohet al., 2010), BnB-ADOPT+ also terminates cost optimal solution.2Experimentally, version caused minor benefits. realized ignored threshold management. Observe thresholds reinitialized contextchange (caused VALUE COST messages); causes special difficulty originalBnB-ADOPT algorithm time agent processes message queue, sends VALUEmessages children containing thresholds. Now, VALUE messagessent, children run algorithm threshold periods. avoid this, children way ask threshold parents reinitialization. doneusing COST messages, sent children parents. Thus, define BnB-ADOPT+BnB-ADOPT version following changes:1. Agent remembers last message sent neighbors.2. COST message j includes boolean hReq, set true j thresholdreinitialized.3. j send COST message equal (ignoring timestamps) last messagesent, new COST message sent j detected context changethem.4. send VALUE message j equal (ignoring timestamps) last messagesent, new VALUE message sent last COST message receivedj hReq = true.immediate see changes alter optimality termination propertiesBnB-ADOPT+ : original BnB-ADOPT, includes redundant messages, terminates costoptimal solution (Yeoh et al., 2010); sending redundant messages algorithmremains optimal terminates. proposed changes avoid redundant messages alsoapplied ADOPT algorithm (Gutierrez & Meseguer, 2010a).4. Dealing N-ary Cost Functionsdefined DCOPs using binary cost functions, although DCOP definition easily extended include cost functions arity. Similarly, ADOPT BnB-ADOPT extendeddeal cost functions arity. proposed BnB-ADOPT extension exactly294fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPTADOPT extension handle n-ary cost functions (Yeoh et al., 2010). extension, describedwork Modi et al. (2005), follows: 2...a ternary constraint fijk ... defined three variables xi , xj , xk ... Suppose xixj ancestors xk ... ternary constraint, xi xj send VALUEmessages xk . xk evaluates ternary constraint sends COST messagesback tree normal ... Thus, deal n-ary constraint assigningresponsibility evaluation lowest agent involved constraint.difference evaluation n-ary constraint binary one lowestagent must wait receive ancestors VALUE messages evaluating ...words (replacing constraint cost function), proposed extension agentsmust send VALUE messages lowest agent DFS pseudo-tree involved costfunction. case binary cost function, lower agent (of two involved costfunction) always receives VALUE messages. case n-ary cost functions (involvingtwo agents), intermediate agents receive VALUE messages rest agentsinvolved function. lowest agent xk must receive VALUE messages evaluatingcost function. this, called evaluator agent. Upon receptionmessages, xk evaluates function sends COST message parent, receivesprocesses message COST message. applying technique BnB-ADOPTissues appear, explained following.4.1 TERMINATE Messagesversion binary BnB-ADOPT, time agent changes value sends VALUE messageschildren pseudo-children. non-root agent terminates reaches terminationcondition LB = U B receiving TERMINATE message parent (for root agentTERMINATE message needed). side-effect, last value taken agentsoptimal value. feature appreciated distributed environments, optimal valuesdistributed among agents without requiring central agent charge whole solution.n-ary BnB-ADOPT, although root agent computes minimum cost, direct implementation may terminate optimal value assigned every agent. Let us consider ternarycost function among agents i, j k (as Figure 1); root DFS pseudo-tree,k evaluates cost function. Agent may explore last value, jump back best valuereaches termination condition LB = U B, sends VALUE message k TERMINATEmessage child j. Upon reception VALUE message, k send COST message j.COST message contains last assignment (optimal value) made i. However, j alreadyprocessed TERMINATE message i, ended without processing COST message, means j may end outdated context: 3 causes j end assignedvalue may optimal one, since truly minimize cost globalsolution. 42. must also assured agents involved n-ary cost function lie branch pseudo-tree.guaranteed since agents sharing n-ary cost functions form clique constraint graph. performingdepth first traversal construct DFS pseudo-tree, agents clique necessarily lie branch.3. would happen agent would sent VALUE messages every child j informing value changes,original BnB-ADOPT strategy deal n-ary cost functions.4. Technically speaking, j might terminate incorrect context.295fiG UTIERREZ & ESEGUERVALUExiTH=50xjTH=xknary constraintTH=min(, UB)(v)-child xc lb(v,child)xcxi, xj, xkFigure 1: Original BnB-ADOPT dealing n-ary constrains, use VALUE messagessimple way correct include TERMINATE messages last assignment madesender agent. way, receiver update context terminate valuetruly minimizes lower bound.4.2 VALUE MessagesADOPT strategy n-ary cost functions applied literally BnB-ADOPT, scenarios inefficiently solved. instance, consider Figure 1, variables agents i, j, kshare ternary cost function. Agent root DFS pseudo-tree k lowest agent(therefore evaluator) ternary cost function. Suppose xj constrained variablesproblem, represented gray subtree. Since VALUE messages sent jk inform value changes, VALUE messages sent j, agent jthreshold provided parent. Thresholds provided parents lowest upper boundsamong visited contexts (the cost best solution found far), thresholds computedagents upper bounds cost current context. Thresholds introduced BnB-ADOPT speed problem resolution increase pruning opportunities,tightest threshold agent j j subtree clearly detrimental algorithmperformance.simple way avoid issue send VALUE messages descendants (childrenpseudo-children). However, needed. avoid unnecessary messagessending VALUE messages lowest agent charge evaluating cost function generateCOST messages updated LB U B children propagate H valueDFS pseudo-tree. agent involved cost function i, neitherevaluator cost function child need receive VALUE messages i.proposed extension BnB-ADOPT deal n-ary cost functions. Observe296fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPTbinary case, proposal collapses existing operation algorithms. restpaper assume extension included version BnB-ADOPT, deal costfunctions arity.4.3 Correctness Completenessdefine n-ary BnB-ADOPT work Yeoh et al. (2010) (i.e., agent sends VALUEmessages evaluator agent cost functions involved in) plus agent sendsVALUE messages children pseudo-tree. easy show n-ary BnBADOPT version optimal terminates. First, prove agents send VALUE messageschildren pseudo-children, extended n-ary BnB-ADOPT terminates costoptimal solution. Second, show VALUE messages send pseudo-children redundant(except pseudo-child evaluator agent cost function involved sender). Third,demonstrate redundant messages binary case remain redundant n-ary case.Combining results obtain desired output: proof n-ary BnB-ADOPT+ terminatescost optimal solution.Theorem 4 N-ary BnB-ADOPT terminates cost optimal solution.Proof. Imagine extended version n-ary BnB-ADOPT agents send VALUE messagesdescendants (children pseudo-children) pseudo-tree. extended versionn-ary BnB-ADOPT working binary case: agent sends VALUE messagesdescendants (children pseudo-children) sends COST message parent.case, easy check results Section 5 paper Yeoh et al. (2010) apply(it long conceptually easy; observe result Section 5 paper Yeoh et al.uses fact cost functions binary). particular, Yeoh et al. proved binary BnBADOPT terminates cost optimal solution. Therefore, extended version n-aryBnB-ADOPT terminates cost optimal solution.Now, consider VALUE messages sent agent pseudo-childrenevaluators cost function involving agent i. show messages redundant.receiving VALUE message i:1. Agent j updates context.2. message comes parent, agent j rewrites threshold messagethreshold.know parent j, consider point (1) only. Agent j evaluatingagent cost function involving i; thus, another agent k branchj charge evaluation. agent k receive sure VALUEmessages coming ancestors, send COST messages tree.COST messages reach j, update context exactly way receiving jVALUE message i. Original VALUE messages redundant effectobtained COST messages arriving k. Therefore, remove VALUEmessages extended version n-ary BnB-ADOPT, algorithm terminatecost optimal solution. definition, algorithm n-ary BnB-ADOPT.2Next prove redundant messages binary case remain redundant n-ary case.297fiG UTIERREZ & ESEGUERLemma 3 VALUE COST messages found redundant binary BnB-ADOPT remain redundantn-ary BnB-ADOPT.Proof. prove Lemma, enough realize Theorems 1 2 remain valid n-aryBnB-ADOPT. Observe proofs Lemma 1, Theorems 1 2 required usebinary cost functions. proof Lemma 2 easily generalized n-ary case, simplyreplacing children pseudo-children children evaluator pseudochildren.2define n-ary BnB-ADOPT+ n-ary BnB-ADOPT removing redundant VALUE COSTmessages, Section 3.Corollary 1 N-ary BnB-ADOPT+ terminates cost optimal solution.Proof. Combining Theorem 4 Lemma 3 prove n-ary BnB-ADOPT sendingredundant VALUE COST messages terminates cost optimal solution.Section 3, child may ask parent resend threshold VALUE messagethreshold reinitialized. exceptions cause extra difficulty here.justification show sending messages question optimality terminationbinary case, remains fully valid n-ary case.2following make difference binary n-ary cases; insteaduse single algorithm, n-ary BnB-ADOPT+ , simply name BnB-ADOPT+ .5. Experimental Resultsexperimentally evaluated performance original BnB-ADOPT (binary n-ary versions)proposal n-ary BnB-ADOPT (Section 4) BnB-ADOPT+ (that includeschanges proposed Sections 3 4), using discrete event simulator. Performance evaluated terms communication cost (total number messages exchanged) computation effort(NCCCs, non-concurrent constraint checks, see Meisels, Kaplansky, Razgon, & Zivan, 2002).also consider number cycles number iterations simulator must performsolution found. cycle, agents read incoming messages message queue,process them, send outgoing messages required. DFS pseudo-tree generateddistributed form, following most-connected heuristic.First, evaluate impact removing redundant messages binary case, comparingoriginal BnB-ADOPT BnB-ADOPT+ . Second, evaluate performance non-binary instances. compare original BnB-ADOPT proposal n-ary BnB-ADOPT (changesSection 4) BnB-ADOPT+ (n-ary BnB-ADOPT saving redundant messages). Lastly,compare BnB-ADOPT+ two well-known algorithms DCOP solving: SynchronousBranch Bound (SBB) (Hirayama & Yokoo, 1997) Asynchronous Forward Bounding (AFB)(Gershman et al., 2009). SBB completely synchronous algorithm whereas AFB performs synchronous value assignments asynchronously computes bounds used pruning. SBBAFB maintain total order variables perform assignments BnB-ADOPT+ uses partialordering following DFS pseudo-tree structure. present last comparison provideoverall picture BnB-ADOPT+ asynchronous nature affects number messagesexchanged computation.Experiments performed three different benchmarks: random DCOPs (binary ternarycases), meeting scheduling sensor networks (both binary, obtained public DCOP298fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPTrepository, Yin, 2008). Random DCOPs characterized hn, d, p1 i, n numbervariables, domain size p1 network connectivity. Random generation assuresconnected problems, agents problem belong constraint graphDFS pseudo-tree. Binary instances contain p1 n(n 1)/2 binary cost functions, ternaryinstances contain p1 n(n 1)(n 2)/6 ternary cost functions. Costs selected randomlyset {0,..., 100}. meeting scheduling, variables represent meetings, domains represent timeslots assigned meetings, cost functions meetings share participants(Maheswaran et al., 2004). sensor networks, variables represent areas need observed,domains represent time slots, cost functions adjacent areas (Maheswaranet al., 2004).Results first experiment comparing BnB-ADOPT BnB-ADOPT+ appear Table 1Table 2. Table 1 shows results binary random problems averaged 50 instances. Table1(a) shows results varying network connectivity hn = 10, = 10, p1 = 0.2...0.8i. Table 1(b)shows results varying domain size hn = 10, = 6...12, p1 = 0.5i. Table 1(c) shows resultsvarying number variables hn = 6...12, = 10, p1 = 0.5i. Table 2 (a) shows meetingscheduling instances 4 cases different hierarchical scenarios: case (8 variables), B (10variables), C (12 variables) (12 variables). Table 2 (b) shows sensor network instances 4cases different topologies: cases (16 variables), B (16 variables), C (10 variables) (16variables). two last benchmarks, results averaged 30 instances.Experiments binary random DCOPs show algorithm BnB-ADOPT+ obtains important communication savings respect original BnB-ADOPT. number messagesreduced 3 6 times connectivity domain size increases, also showing consistent reduction, factor 4 5, increasing number variables. meetingscheduling instances, messages reduced factor 3 9, sensor networks,factor 5 8. standard deviation messages also decreases problemsconsidered.Regarding NCCCs, mean also moderately reduced instances (around 10%).binary random benchmark, standard deviation also slightly reduced. meeting schedulingsensor networks, standard deviation increases. However, looking every problemseparately, number NCCCs BnB-ADOPT+ always smaller every instance. number cycles remain practically unchanged. results clearly indicate that, binary case,removing redundant messages beneficial enhance communication, achieving alsomoderated gains computation.addition, took particular random instance hn = 10, = 10, p1 = 0.5i, solvedrepeatedly using original BnB-ADOPT BnB-ADOPT+ , varying order agentsactivated simulator (using DFS pseudo-tree executions). Results quitesimilar across executions. Regarding saved messages, BnB-ADOPT always required 4.34.4 times messages BnB-ADOPT+ (considering individual executions). results show activation order agents simulator impact message reductionachieved BnB-ADOPT+ .Results second experiment appear Table 3, contains results ternary randominstances hn = 8, = 5, p1 = 0.4...0.8i averaged 50 instances. First row contains resultsoriginal BnB-ADOPT (including modification Section 4.1). Second row contains resultsn-ary BnB-ADOPT proposal (Section 4), thresholds propagated children.299fiG UTIERREZ & ESEGUER(a) < n = 10, = 10, p1 >p10.20.30.40.50.60.70.8#Messages1068 (274)416 (74)39,158 (36,578)11,774 (10,105)270,379 (432,782)69,277 (92,291)2,273,768 (2,149,369)493,137 (422,360)11,439,563 (10,231,971)2,205,848 (1,802,655)60,221,283 (34,121,853)8,930,713 (5,092,602)161,327,710 (94,398,879)22,972,676 (13,464,530)#NCCC904 (23)881 (23)68,882 (62,180)62,031 (53,085)504,373 (796,625)475,534 (776,820)4,311,524 (3,923,577)4,112,299 (3,760,583)23,759,356 (22,468,476)22,783,209 (21,040,893)134,868,051 (90,469,274)129,143,706 (89,328,458)360,857,244 (212,464,295)353,180,585 (209,726,371)#Cycles62 (15)62 (15)1,751 (1,625)1,753 (1,629)10,313 (16,478)10,317 (16,483)73,715 (69,676)73,792 (69,808)331,947 (299,259)332,841 (300,784)1,526,394 (862,540)1,527,960 (865,974)3,752,164 (2,210,488)3,755,118 (2,213,631)(b) < n = 10, d, p1 = 0.5 >681012#Messages618,005 (573,704)119,841 (104,980)1,362,586 (951,900)288,422 (201,488)2,711,719 (2,929,759)597,325 (633,879)4,871,563 (9,725,100)1,015,541 (1,706,302)#NCCC701,352 (642,821)657,276 (593,830)2,090,231 (1,470,631)1,986,430 (1,398,420)5,092,387 (5,376,943)4,842,265 (5,133,731)10,969,641 (20,549,608)10,342,414 (18,679,208)#Cycles20,305 (18,869)20,342 (18,924)44,507 (31,209)44,562 (31,238)88,224 (96,033)88,329 (96,195)157,856 (314,908)157,994 (315,137)(c) < n, = 10, p1 = 0.5 >n681012#Messages4,388 (3,272)1,514 (1,020)72,783 (54,772)20,326 (12,971)2,603,727 (3,358,285)547,079 (656,709)111,436,193 (133,362,317)20,169,771 (23,877,564)#NCCC13,077 (13,214)12,221 (12,439)173,038 (126,743)159,698 (113,801)5,289,823 (6,844,174)5,005,774 (6,576,047)187,178,211(237,619,542)179,110,208 (228,862,664)#Cycles350 (259)350 (259)3,576 (2,679)3,581 (2,689)84,706 (112,469)84,816 (112,774)2,633,456 (3,148,339)2,636,675 (3,152,543)Table 1: Results (mean standard deviation parenthesis) random binary benchmarksvarying network connectivity, domain size number variables: BnB-ADOPT(first row), BnB-ADOPT+ (second row).Third row contains BnB-ADOPT+ results, enhances last version removing redundantmessages.Experiments ternary random DCOPs show assuring propagation threshold values children produces clear performance benefits (Table 3, second row). Agents sendextra messages children containing threshold values, sent original version,extra messages contribute better pruning. global effect, less communicationrequired overall search, significant reductions obtained metrics (messages,NCCCs cycles). Maintaining positive effect, remove redundant messages using BnBADOPT+ (Table 3, third row). Removing redundant messages causes savings one ordermagnitude number messages exchanged. result positive executiontime often dominated communication time. Observe number cycles little300fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPTvariation second third row. Also, savings NCCCs, althoughsignificant. results conclude that, n-ary case, proposal nary BnB-ADOPT yields clear benefits communication computation, removingredundant messages substantially reduces communication.Finally, present third experiment comparing BnB-ADOPT SBB AFB Figure2. experiments performed binary random (top), meeting scheduling (middle)sensor network (bottom) instances. SBB variables statically ordered using width heuristicdescribed Hirayama Yokoo (1997), AFB variables ordered following heuristic, BnB-ADOPT+ variables partially ordered using connected heuristicconstructing pseudo-tree. restrict solving time one hour; case SBB AFB(a) Meeting SchedulingBC#Messages178,899 (3,638)18,117 (627)65,556 (912)15,373 (426)62,707 (741)11,343 (347)41,282 (862)13,354 (455)#NCCC446,670 (2,786)413,507 (13,446)125,331 (1,963)120,900 (1,969)80,369 (54)74,518 (407)60,424 (460)49,878 (1,692)(b) Sensor Network#Cycles8,202 (302)8,203 (302)2,663 (43)2,665 (43)2,353 (34)2,355 (35)1,545 (48)1,547 (48)BC#Messages9,369 (99)1,103 (73)12,917 (116)1,569 (77)6,429 (59)1,177 (51)15,560 (145)2,155 (81)#NCCC7,241 (52)450 (232)11,054 (135)592 (879)8,786 (52)1,495 (2,490)12,641 (57)2,137 (3,552)#Cycles313 (3)307 (4)414 (4)409(4)340 (5)340 (6)477 (2)477 (2)Table 2: Results (mean standard deviation parenthesis) meeting schedulingsensor network instances: BnB-ADOPT (first row), BnB-ADOPT+ (second row).p10.20.30.40.50.60.70.8#Messages257,238 (294,137)147,379 (139,646)28,614 (23,822)648,217 (413,580)401,306 (239,271)63,778 (33,025)1,642,247 (975,131)1,210,143 (523,825)164,901 (68,876)2,321,729 (1,106,146)1,771,256 (678,893)225,836 (69,630)3,666,514 (1,316,782)2,973,239 (1,406,400)311,524 (93,062)4,013,891 (897,046)3,537,027 (1,119,242)348,199 (73,620)4,892,733 (788,897)4,616,032 (1,135,830)399,662 (70,572)#NCCC1,522,580 (1,863,696)829,594 (852,754)764,516 (783,253)4,029,045 (2,807,389)2,414,946 (1,641,836)2,237,786 (1,520,761)12,585,339 (8,483,693)9,194,309 (4,938,582)8,744,465 (4,813,577)19,424,669 (10,248,817)14,775,187 (6,678,667)14,337,952 (6,372,442)35,743,718 (15,729,105)29,252,469 (16,146,978)27,614,749 (14,316,979)41,469,157 (11,804,005)36,966,889 (13,604,359)35,314,718 (12,331,316)55,151,742 (11,209,964)52,221,369 (14,948,424)49,189,230 (13,197,765)#Cycles10,880 (12,295)5,793 (5,357)5,797 (5,360)24,026 (15,085)13,938 (8,271)13,943 (8,270)55,156 (32,553)39,373 (17,188)39,381 (17,197)74,279 (36,150)55,101 (21,280)55,103 (21,280)115,523 (43,001)92,020 (45,010)92,020 (45,010)124,408 (29,353)108,858 (36,028)108,858 (36,028)150,077 (25,114)140,472 (35,672)140,472 (35,672)Table 3: Results (mean standard deviation parenthesis) random ternary DCOPs:original BnB-ADOPT (first row), proposal n-ary BnB-ADOPT (second row),BnB-ADOPT+ (third row).301fiG UTIERREZ & ESEGUER81010107108106NCCCmessages10510410BnB-ADOPT+SBBAFB310BnB-ADOPT+SBBAFB4102100.261020.30.40.50.6p10.7100.890.20.30.40.50.6p10.70.812101081010710NCCCmessages10610510BnB-ADOPT+SBB (timeouts)AFB (timeouts)BnB-ADOPT+SBB (timeouts)AFB (timeouts)6104108104BCmeeting scheduling problems10BCmeeting scheduling problems91010108108107NCCCmessages10610610510410BnB-ADOPT+SBB (timeouts)AFB (timeouts)BnB-ADOPT+SBB (timeouts)AFB (timeouts)410310BCsensor network problemsBCsensor network problemsFigure 2: Comparison algorithms BnB-ADOPT+ , SBB, AFB. Top: binary random instances.Middle: meeting scheduling. Bottom: sensor networks.problem instance could solved amount time, present amount messagesexchanged NCCCs performed timeout.results observe random instances BnB-ADOPT+ significantlyefficient low connected problems, however tightly connected problems requiresmessages computational effort SBB AFB. explain behavior given BnBADOPT+ asynchronous algorithm, designed benefit pseudo-tree structure,non-connected agents lying different branches pseudo-tree explore search space302fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPTparallel. connectivity increases, width pseudo-tree decreases (in fully connectedproblem pseudo-tree single branch agents totally ordered). makesBnB-ADOPT+ asynchronous potential decrease. time, higher number reinitializations (bounds context) performed, since agents links ancestors, reducespruning effectiveness.meeting scheduling sensor network instances, see BnB-ADOPT+ several orders magnitude efficient SBB AFB. structured nature problems(with different topology every case A, B, C (Maheswaran et al., 2004) suitable buildbalanced pseudo-trees) allows BnB-ADOPT+ benefit asynchronous search. addition,observed instances variability costs smaller random problems:costs quite similar, others clearly larger. cases, upper bound closeoptimum cost reached early execution. However satisfy pruning condition,lower bounds contributions almost cost functions needed. observed SBBlesser extent AFB go deep search tree obtain contributions (pruningusually done last agents ordering) finally subject thrashing.hand, BnB-ADOPT+ computes local bounds every agent since agents assigned everymoment execution, specialized upper bounds propagated every node pseudotree. allows BnB-ADOPT+ perform pruning and, consequence, reduces searchspace faster AFB SBB. confirmed fact empirically testing instancessmall variability costs (even synthetic instances tuples cost).summary, see proposed BnB-ADOPT+ clearly efficient originalBnB-ADOPT. binary instances, BnB-ADOPT+ processes third (or less) total numbermessages required BnB-ADOPT (in instances, messages reduced factor 89), still reaches optimal solution almost number cycles. ternary instances,savings reach one order magnitude communication almost cases. Regardingcomparison SBB AFB, new BnB-ADOPT+ outperforms (in numbermessages NCCCs) low connected random instances, contrary occurs highlyconnected ones. Regarding meeting scheduling sensor network instances, BnB-ADOPT+ outperforms SBB AFB large margin. results indicate value BnB-ADOPT+optimal DCOP solving.6. Conclusionpresented two contributions increase performance BnB-ADOPT, reference algorithm optimally solve distributed constrained optimization problems. First, presented theoretical results detect redundant messages. Second, described difficulties dealingn-ary cost functions (the original algorithm presented detail binary case). Combining two contributions, generated new version BnB-ADOPT. new version, calledBnB-ADOPT+ , obtains substantial savings respect original algorithm, testedcommonly used benchmarks DCOP community.303fiG UTIERREZ & ESEGUERAcknowledgmentspaper extension previous publication (Gutierrez & Meseguer, 2010b), generalization n-ary cost functions, deeper explanations, detailed proofs, experiments.Authors sincerely thank reviewers useful criticisms.ReferencesChechetka, A., & Sycara, K. (2006). No-commitment branch bound search distributedconstraint optimization. Proc. AAMAS-06, 14271429.Gershman, A., Meisels, A., & Zivan, R. (2009). Asynchronous forward bounding distributedcops. Journal Artificial Intelligence Research, 34, 6188.Gutierrez, P., & Meseguer, P. (2010a). Saving messages adopt-based algorithms. Proc. 12th DCRworkshop AAMAS-10, 5364.Gutierrez, P., & Meseguer, P. (2010b). Saving messages BnB-ADOPT. Proc. AAAI-10, 12591260.Hirayama, K., & Yokoo, M. (1997). Distributed partial constraint satisfaction problem. Proc. CP97, 222236.Jain, M., Taylor, M., Tambe, M., & Yokoo, M. (2009). DCOPs meet realworld: Exploringunknown reward matrices applications mobile sensor networks. Proc. IJCAI-09, 181186.Junges, R., & Bazzan, A. L. C. (2008). Evaluating performance DCOP algorithms realworld dynamic problem. Proc. AAMAS-08, 599606.Maheswaran, R., Tambe, M., Bowring, E., Pearce, J., & Varakantham, P. (2004). Taking DCOPreal world: Efficient complete solutions distributed event scheduling. Proc. AAMAS04, 310317.Meisels, A., Kaplansky, E., Razgon, I., & Zivan, R. (2002). Comparing performance distributedconstraints processing algorithms. Proc. 3rd DCR workshop AAMAS-02, 8693.Modi, P. J., Shen, W.-M., Tambe, M., & Yokoo, M. (2005). ADOPT: Asynchronous distributedconstraint optimization quality guarantees. Artificial Intelligence, 161, 149180.Petcu, A. (2007). class algorithms Distributed Constraint Optimization. Ph.D. thesis.Petcu, A., & Faltings, B. (2005). scalable method multiagent constraint optimization. Proc.IJCAI-05, 266271.Ueda, S., Iwasaki, A., & Yokoo, M. (2010). Coalition structure generation based distributedconstraint optimization. Proc. AAAI-10, 197203.Yeoh, W., Felner, A., & Koenig, S. (2010). BnB-ADOPT: asynchronous branch-and-boundDCOP algorithm. Journal Artificial Intelligence Research, 38, 85133.Yin, Z. (2008). USC DCOP repository..Yokoo, M., Durfee, E., Ishida, T., & Kuwabara, K. (1998). distributed constraint satisfactionproblem: Formalization algorithms. IEEE Trans. Know. Data Engin., 10, 673685.304fiJournal Artificial Intelligence Research 45 (2012) 125-163Submitted 05/12; published 09/12Towards Unsupervised Learning TemporalRelations EventsSeyed Abolghasem MirroshandelGholamreza Ghassem-Sanimirroshandel@ce.sharif.edusani@sharif.eduComputer Engineering DepartmentSharif University TechnologyAzadi Avenue, Tehran 11155-9517, IranAbstractAutomatic extraction temporal relations event pairs important taskseveral natural language processing applications Question Answering, InformationExtraction, Summarization. Since existing methods supervised requirelarge corpora, many languages exist, concentrated effortsreduce need annotated data much possible. paper presents two differentalgorithms towards goal. first algorithm weakly supervised machine learningapproach classification temporal relations events. first stage,algorithm learns general classifier annotated corpus. Then, inspiredhypothesis one type temporal relation per discourse, extracts useful informationcluster topically related documents. show combining globalinformation cluster local decisions general classifier, bootstrappingcross-document classifier built extract temporal relations events.experiments show without additional annotated data, accuracy proposedalgorithm higher several previous successful systems. second proposedmethod temporal relation extraction based expectation maximization (EM)algorithm. Within EM, used different techniques greedy best-first searchinteger linear programming temporal inconsistency removal. thinkexperimental results EM based algorithm, first step toward fully unsupervisedtemporal relation extraction method, encouraging.1. IntroductionMuch progress made natural language processing (NLP) recent years. Combining statistical symbolic methods played significant role advances.result, tasks part-of-speech tagging (Sgaard, 2011), parsing (Petrov & Klein, 2007),named entity recognition (Mikheev, Grover, & Moens, 1998) addressedsatisfactory results. However, tasks temporal information processing, need deeper analysis meaning, achieved results yetsatisfactory.Temporal information encoded textual description events. Lately, increasing attention practical NLP applications question answering, summarization, information extraction resulted growing demand temporal informationprocessing (Tatu & Srikanth, 2008). question answering, one may expect systemanswer questions event occurred, chronological order bec2012AI Access Foundation. rights reserved.fiMirroshandel & Ghassem-Sanitween desired events. text summarization, especially multi-document type,knowing order events useful source correctly merging related information.Construction TimeBank corpus 2003 (Pustejovsky et al., 2003), providedopportunity applying different machine learning methods task temporal relationextraction. However, realized even six-class classification temporalrelations complicated task, even human annotators (Mani, Verhagen, Wellner,Lee, & Pustejovsky, 2006).paper presents two different approaches need annotated datatemporal relation learning reduced. first approach weakly supervised machinelearning algorithm classification temporal relations events. first stage,algorithm learns general classifier annotated corpus. Then, inspiredhypothesis one type temporal relation per discourse, extracts useful informationcluster topically related documents retraining model. combiningglobal information cluster local decisions general classifier, proposenovel bootstrapping cross-document classifier extract temporal relations events.experiments show without additional annotated data, accuracyproposed algorithm least 7% higher state-of-the-art statisticalmethods (Chambers, Wang, & Jurafsky, 2007).second introduced approach novel usage expectation maximization (EM) algorithm temporal relation learning. algorithm also employs Allens interval algebra(Allen, 1984) correction predicted relations. applying interval algebra, utilizetwo different approaches: 1) heuristic search method 2) integer linear programming(ILP). think experimental results EM based algorithm, first steptoward fully unsupervised temporal relation extraction method, encouraging.remainder paper organized follows: section 2 previous approaches temporal relation learning. Section 3 explains first proposed method,evaluated section 4. second algorithm explained section 5, evaluated section 6. Finally, section 7 includes conclusions possible futurework.2. Temporal Relation LearningAssuming access texts events time expressionsappropriately tagged, two different tasks pertaining temporal relation learning distinguished: 1) detecting whether exist relation given pair events/timeexpressions; 2) identifying relation type positive cases first task. firsttask hard evaluate, annotators may ignore many plausible existingrelations tagging corpora (Mani et al., 2006). Accordingly, paper likeexisting research, addressed second task, specificallydefined follows: given ordered pair components (x1 , x2 ), x1 x2annotated events and/or time expressions, temporal relation classifier identifies typerelation ri temporally links x1 x2 . shown Figure 1, temporal relation one fourteen types proposed TimeML (Pustejovsky et al., 2003).example, Powerful political pressures (event1 ) may convince (event2 ) Conservativegovernment keep (event3 ) so-called golden share, limits individual holding126fiUnsupervised Temporal Relation Learning EventsFigure 1: Different temporal relations TimeML.15%, restriction (event4 ) expires (event5 ) Dec. 31, 1990 (time1 ). (takendocument wsj 0745 TimeBank, see Pustejovsky et al., 2003). task automatically tag relations pairs (event1 , event2 ), (event3 , event5 ), (event5 , event4 ),(event5 , time1 ) BEFORE, ENDED BY, ENDS, INCLUDED, respectively(see Figure 2). Since automatic extraction event-event relations difficulttask, paper, focused particular task, left detectiontype temporal relations event-time time-time future work.many ongoing research focusing temporal relation learning. Additionally,two important shared tasks temporal information extraction: TempEval2007 (Verhagen et al., 2007) TempEval 2010 (Verhagen, Sauri, Caselli, & Pustejovsky,2010). TempEval 2007, three different tasks regarding temporal relationsclassification A) events times within sentence; B) creation timedocument events; C) main (verb) events adjacent sentences.TempEval 2010, six different tasks including A, B) Determining timeexpressions events input texts specified features; temporal relation classification C) events times within sentence; D) creation time documentevents; E) main events consecutive sentences; F) two events one eventsyntactically dominates event.Due focusing temporal relations event pairs, task C TempEval 2007plus tasks E F TempEval 2010 similar task tackle paper;however, tasks considered special cases ours. instance, task ETempEval 2010, event pairs consecutive sentences considered; whereas,task event pairs either sentence twosentences input text.research temporal relation learning divided different categories.paper, divide efforts three groups: 1) Statistical; 2) Rule-based, 3)127fiMirroshandel & Ghassem-SaniHybrid, explained following sections.Figure 2: Temporal relations sentence Powerful political pressures (event1 ) mayconvince (event2 ) Conservative government keep (event3 ) so-calledgolden share, limits individual holding 15%, restriction(event4 ) expires (event5 ) Dec. 31, 1990 (time1 ). Bold arrows show relationsevent pairs.2.1 Statistical Methodsstatistical methods, classification (or clustering) algorithm employed number tagged and/or extracted features input corpus. Maximum Entropy (Mani,Wellner, Verhagen, & Pustejovsky, 2007; Derczynski & Gaizauskas, 2010), Support VectorMachines (Chambers et al., 2007; Bethard & Martin, 2007; Hepple, Setzer, & Gaizauskas,2007; Cheng, Asahara, & Matsumoto, 2007; Mirroshandel, Ghassem-Sani, & Khayyamian,2009a, 2009b, 2011), Conditional Random Fields (Llorens, Saquete, & Navarro, 2010; Kolya,Ekbal, & Bandyopadhyay, 2010), Markov Logic Networks (UzZaman & Allen, 2010;Ha, Baikadi, Licata, & Lester, 2010) statistical techniquesapplied problem.MaxEnt one first approaches temporal relation learning, usesmaximum entropy classification algorithm (Mani et al., 2007). method, classifierassigns one six different temporal relation types event-event event-time pair.classifier relies number features including modality, polarity, tense, aspect,event class, hand-tagged corpus. addition features,also relies pairwise agreement two additional features: tense aspect. laterpropose new technique improve MaxEnt. results comparing proposed methodMaxEnt given section 4.128fiUnsupervised Temporal Relation Learning EventsUSFD2 (Derczynski & Gaizauskas, 2010) another method employs maximumentropy solving tasks C F TempEval 2010. method uses featuresMaxEnt plus features related so-called signals text. USFD2 achievedsecond highest score task C TempEval 2010. However, results task Fsatisfactory enough.state-of-the-art statistical methods analogous MaxEnt (Chambers et al.,2007). works two consecutive stages employs event-event features additionused MaxEnt. work, Support Vector Machines (SVM) usedclassification. Similar results reported using Naive Bayes classier instead SVM.Section 4 also includes results comparing work proposed algorithm.SVMs also used classification algorithm several research. CUTMP (Bethard & Martin, 2007) applied SVM solving three tasks TempEval 2007.also used gold-standard TimeBank features event time expressions plus partsderived parse trees input text. CU-TMP first solves task B usesresults tackle tasks C.USFD (Hepple et al., 2007) NAIST-Japan (Cheng et al., 2007) twoparticipants TempEval 2007 used SVM classification. NAIST-Japan,task defined sequence labeling model. task approached using HMMSVM, relying features dependency-parsed trees standard attributes targetevents/time expressions. result system slightly average tasksB, less average task C. shown extracted features dependencyparsed trees effective task C, contrast tasks B. USFD,temporal relation learning treated simple classification task (Hepple et al., 2007).used different classification algorithm WEKA machine learning workbench (Hallet al., 2009). task C, SVMs gained best result among participantsshared task.another work, corpus parallel temporal causal relations employed,SVMs used extract types relations (Bethard & Martin, 2008). Since existingcorpora provide parallel temporal causal annotations, 1000 conjoined event pairsannotated (Bethard & Martin, 2008; Bethard, 2007). shown causal relationinformation could helpful temporal relation extraction, too. also showntemporal relation information mutually positive effects causal relation extraction(Bethard & Martin, 2008).Bethard colleagues (2007a, 2007b) applied SVM classify event pairsfirst event verb second one head clausal argumentverb. used combination number event based features (e.g., tenseaspect) syntactic features (e.g., specific path parse tree).reported results shown high accuracy specific event pairs.also algorithms utilize grammatical information SVM usingconvolution tree kernels (Mirroshandel et al., 2009a, 2009b, 2011). showngrammatical aspects input text rich sources information temporal relationclassification. Argument Ancestor Path Distance (AAPD) convolution tree kernelsuccessful tree kernel used SVM classification. kernel similarCollinsDuffy tree kernel (Collins & Duffy, 2001). CollinsDuffy kernel effectivelycounts number common subtrees two comparing parse trees. kernel,129fiMirroshandel & Ghassem-Sanisubtrees importance, whereas AAPD, different weighting functionsused compute kernel value. Furthermore, AAPD, significance subtreesmeasured using distance so-called argument ancestor path (AAP). AAPancestor nodes argument (event). example node (NN) distancenode AAP shown Figure 3. AAPD, closer nodepath, less decayed weighting function. words, nodeslocated nearer path important farther away.improve accuracy AAPD, combined kernels,either linear polynomial (Zhang, Zhang, Su, & Zhou, 2006). However, polynomialcomposite kernels shown superior results (Mirroshandel et al., 2009b). Section 4includes results comparing AAPD AAPD Polynomial kernels.Figure 3: syntactic parse tree two argument ancestor paths events moveresigned plus distance node NN AAP resigned.Markov Logic Networks (MLN) another classification algorithm,used two participants TempEval 2010: TRIPS & TRIOS (UzZaman & Allen, 2010)NCSU (Ha et al., 2010). TRIPS TRIOS use number features produceddeep semantic parser, plus features extracted target pairs (i.e., event/timeexpression). contrast participants, TRIPS TRIOS operate raw texts.words, systems use tagged events/time expressions.outperformed teams two tasks (C E). TRIOS also gained second best results four remaining tasks. NCSU another participant TempEval 2010 usesMLN classification. relies basic annotated features, syntactic features extractedgenerated parse trees, lexical semantic features two external resources (Ver130fiUnsupervised Temporal Relation Learning EventsbOcean WordNet) (Ha et al., 2010). NCSU applied tasks C, D, E, Ftwo different settings: NCSU-indi NCSU-joint. NCSU-indi, independent MLNtrained task. hand, set global formulae also addedNCSU-joint ensure consistency among classification decisions four local MLNs(one task). NCSU-indi achieved best result task F second best resulttask C.One successful participants TempEval 2010 TIPSem basedConditional Random Field (CRF) models classification purpose (Llorens et al., 2010).TIPSem employs different morphological, syntactic, semantic features building CRFmodels. Spanish, achieved best results tasks. English, TIPSem achievedbest results Tasks B D; one best systems tasks.JU CSE TEMP another participant TempEval 2010 utilized CRF modelstemporal relation learning tasks (Kolya et al., 2010). system needs goldstandard features TimeBank time expressions and/or events. comparisonTIPSem, JU CSE TEMP achieved weaker results, shows importance featureengineering temporal relation learning.another approach applies different machine learning techniques detectintra-sentential events, builds corpus sentences two eventsleast one event triggered key time word (e.g., after, before, etc.). classifierbased number syntactic clausal ordering features (Lapata & Lascarides, 2006;Bramsen, Deshpande, Lee, & Barzilay, 2006).exist comprehensive study statistical methods, compares threedifferent interval based algebras terms classification accuracy, performance, expressiveness power (Denis & Muller, 2010). also algorithms exclusivelywork temporal relation classification events time expressions. Onealgorithms employs cascaded finite-state grammars (for temporal expression analysis, shallow syntactic parsing, feature generation) together machine learning componentcapable effectively using large amounts unannotated data (Boguraev & Ando, 2005).group statistical methods rely information argument fillers (calledanchors) every event expression valuable clue recognizing temporal relations.methods, looking set event expressions whose argument fillers similardistribution, analogous event expressions recognized. Algorithms DIRT (Lin &Pantel, 2001), TE/ASE (Szpektor, Tanev, Dagan, & Coppola, 2004), Pekarssystem (2006) examples type statistical method.DIRT unsupervised method based extended version so-called distributional hypothesis (Lin & Pantel, 2001). According hypothesis, words occurcontexts usually similar. Here, instead words, algorithm appliesdistributional hypothesis certain paths dependency trees parsed corpus.TE/ASE, too, unsupervised algorithm, two major phases. firstphase (called Anchor Set Extraction), algorithm extracts similar anchors. Then,second phase (called Template Extraction), system extracts templates resultinganchor sets. final part algorithm, post-processing transformationsapplied extracted templates remove inappropriate templates (Szpektor et al.,2004).131fiMirroshandel & Ghassem-SaniPekars approach (2006), co-occurrence two verbs inside locally coherent textused extract useful information. method three major steps. First, basedlocal discourse, identifies several pairs clauses related. Next, basedrelated clauses, tries create number templates verb pairs usinginformation syntactic behavior. last step, algorithm scores employstemplates relation extraction.2.2 Rule-Based Methodscommon idea behind rule-based methods find general patterns classifyingtemporal relations. works, rules (patterns) manually defined.Perhaps simplest rule-based method one developed using knowledgeresource called VerbOcean (Chklovski & Pantel, 2005). VerbOcean small numbermanually designed generic rules. style rules form * <Verb-X> * <VerbY> *. example, rules Verb-X Verb-Y, Verb-Xeventually Verb-Y, Verb-X later Verb-Y happens-before relationtype; also rules Verb-X even Verb-Y Verb-Y least Verb-Xso-called strength relation type. manually creating rules, numbersemantic relations (e.g., strength, antonymy, happens-before, etc.) eventsdetected. Several heuristics also employed filter inappropriate relations (Chklovski& Pantel, 2005).another rule-based method temporal relation learning focused biomedicaltexts (Mulkar-Mehta, Hobbs, Liu, & Zhou, 2009). shown existing methodstemporal relation learning effective texts. work, specificaxioms (rules) used predict temporal causal relations. pattern extractionalgorithm employed create system rules semi-automatic manner.XRCE-T, participant TempEval 2007, rule-based system relies syntacticsemantic features (e.g., deep syntactic analysis determination thematic roles)(Hagege & Tannier, 2007). XRCE-T fact used post-processing modulegeneral purpose linguistic analyzer.another study, rules temporal transitivity used increase training set.test accuracy enlarged corpus showed improvements (Mani et al., 2007).Reasoning pre-determined rules another approach rules usage. workTatu Srikanth (2008), rich set axioms (rules) created used firstorder logic based theorem prover find proof temporal relation refutation.set discourse rules used algorithm Muller Tannier (2004)establish possible relations every two consecutive events input text.rules based tenses event verbs. classical path-consistencyalgorithm (Allen, 1984) applied extracted relations first step.2.3 Hybrid Methodsshown one increase accuracy temporal relation classifiersmerging discussed methods. example work Chambers Jurafsky(2008), local decisions generated statistical method combined two typesimplicit global rule-based properties. properties included transitivity rule (e.g.,132fiUnsupervised Temporal Relation Learning EventsB B C implies C), time expressions normalization (e.g., lastmonth yesterday). constraints used create densely-connectednetwork events, global state consistency enforced incorporatingconstraints integer linear programming framework (Chambers & Jurafsky, 2008).Integer linear programming local classifiers shown appropriatecases number possible relations events restricted (Denis & Muller,2011). suggested translation constraints temporal intervals endpoints used handle significantly smaller set constraints. translation,temporal relations preserved. method shown rather high accuracy.also proposed graph decomposition technique improve accuracy.another algorithm, spiritually similar (Chambers & Jurafsky,2008), instead applying global constraints using integer linear programming, so-calledMarkov logic (ML) used (Yoshikawa, Riedel, Asahara, & Matsumoto, 2009). Globalconstraints easily captured adding weighted first order logic formulas.shown problem solved ML easily accuratelyILP.WVALI another hybrid system, enhanced classification process using rules particular knowledge base (Puscasu, 2007). system, differentheuristics temporal reasoning mechanism combined statistical data extracted training corpus. WVALI achieved best results tasks TempEval2007.LCC-TE another hybrid system TempEval 2007. combined different machinelearning models human rules temporal relation learning (Min, Srikanth, & Fowler,2007). LCC-TE uses gold-standard features available TimeBank, well numberderived extended features grammatical semantic features. evaluationsLCC-TE shown acceptable results three tasks.3. Bootstrapped Cross-Document Classification (BCDC)section, new method extracting temporal relations events introduced.call method Bootstrapped Cross-Document Classification (BCDC). resultsexperiments BCDC show significant improvement previous work termsaccuracy (see Tables 6 7). used SVM three different kernelslearning process. two novelties bootstrapping (self-training) method: 1)information retrieval based approach extracts useful information exclusivelyrelated documents. 2) builds specific model test document. describingBCDC, motivation briefly explained next section.3.1 Motivationregular corpus heterogeneous documents, verbs, often act event triggers,may different senses different documents. example, event firing maysense shooting gun document army, whereas may also senseending someones job different document company. However, clustertopically-related documents, distribution much less divergent. motivatedus apply so-called one sense per discourse hypothesis (Yarowsky, 1995)133fiMirroshandel & Ghassem-Saniproblem temporal relation classification, extend scope discourse singledocument cluster topically related documents. Also inspired another workproposed assumptions one event trigger sense one event argument role per discourse(Ji & Grishman, 2008), based work analogous assumption, calledone type temporal relation per discourse. words, assume similar eventpairs different places topically related documents likelytemporal relations. Although, later explained, explicitly employedassumption proposed algorithm, tried verify assumption consideringtemporal relations Opinion corpus (Mani et al., 2006). corpus, documentslocated four different directories specific topic. verification,considered documents within directory related. words, twodocuments considered related documents directory (i.e.,topic). verify assumption, selected event pairsappeared once. Opinion corpus, total number 2666 temporallyrelated event pairs (i.e., TLinks), which, 994 pairs appeared once1 .Table 1 shows results verification. Supporting samples event pairsappeared two related documents exact temporal relation.Even event pairs different relations unrelated documents, alsoregarded supporting samples. contrary, event pairs different relationsrelated documents, considered contradictory samples. shownTable 1, 95% samples supported assumption (i.e., one temporalrelation per discourse).Supporting SamplesContradictory SamplesTotalCount942(95%)52(5%)994Table 1: distribution assumption one type temporal relation per discourseOpinion corpus.example, following sentences, taken different documents topic (i.e., Kenya Tanzania Embassy bombings), event pair (blastkill) IBEFORE temporal relation sentences:Reports reaching said massive blast damaged U.S. embassy Nairobi ,killing 40 people wounding least 1,000 people.100 people killed 1,000 others woundedblasts next U.S. embassies Kenya Tanzania Friday.Dar es Salaam , laid wreath next crater left embassy blastkilled 10 people.1. counting number event pairs, applied lemmatizer event words.134fiUnsupervised Temporal Relation Learning Events3.2 Feature EngineeringBCDC, two types features used: basic extra event-event features. Basicfeatures simple features related individual events extra event-event featuresextracted two related events. next two sections, featuresexplained detail.3.2.1 Basic Featuressimple features extracted events. event, five temporalattributes, tagged standard corpora: 1) tense; 2) grammatical aspect; 3)modality; 4) polarity, 5) event class. Tense aspect define temporal locationevent structure; thus, necessary method temporal relation extraction.Modality polarity specify non-occurring hypothetical situations. event classshows type event. range values attributes based workPustejovsky et al. (2003), shown Table 2. attributes either annotatedinput corpus automatically extracted existing tools.AttributeTenseAspectModalityPolarityEvent ClassRange valuesnone, present, past, futurenone, prog, perfect, prog perfectnone, to, should, would, could, can, mightpositive, negativereport, aspectual, state, state, action, perception, occurrenceTable 2: range values five temporal attributes.addition five mentioned attributes, BCDC also employs string wordsconstitute event, part speech tags well number contextual features including pairwise agreement tenses aspects. Part speech tags eventseither annotated corpora determined existing POS taggers.example, sentence succeeds James A. Taylor, ..., succeeds eventfollowing features:[tense: present], [aspect: none], [modality: none], [polarity: positive], [event class: aspectual], [word: succeeds], [pos: verb]3.2.2 Extra Event-Event FeaturesExtra event-event features based two related events automatically extractedinput text. case, three types features, defined follows:Event-Event parse tree: events sentence, algorithmuse parse tree sentence learn useful syntactic properties domination. parse tree, event dominates event B, ancestor B. propertiesexplicit features, rather implicit properties extracted learnedSVM using appropriate tree kernels proposed work Mirroshan135fiMirroshandel & Ghassem-Sanidel et al. (2009b). Parse trees extracted statistical parser needTreebank.Prepositional phrase: preposition head often indicator temporal class.Thus, use new feature indicates event part prepositional phrase.information also extracted parse trees. example, sentencesaw earthquake, relation events saw earthquakeeasily determined word prepositional phrase earthquake.Event-Event distance: based idea strength relationshiptwo events inversely related textual distance events. meansrelationship becomes weaker distance increases (and vice versa). Accordingly, intra- inter-sentential events treated differently. train two separatemodels: one intra-sentential events one inter-sentential ones.3.3 Proposed AlgorithmBCDC applies novel usage bootstrapping classification temporal relationsevents. works two main stages. first stage, using standard corpus,general model learned. stage two, general model retrained testdocument based related information. Figure 4 shows flowchart proposedalgorithm, described detail following sections.3.3.1 Stage Onestage one, BCDC employs discussed features extracted standard corpustrain general model classification using SVM. end stage,model temporal relation classification. However, models, alsoproposed researchers, problem general.words, model specific information particular domainconsideration. better deal problem, BCDC extra bootstrappingphase training.3.3.2 Stage Twostage two, retrain general model produced stage one, test documentrelated information. order achieve goal, bootstrapping phaseBCDC proceeds according following steps:Step 1: first all, unprocessed test document randomly selected.Step 2: BCDC finds top N documents topically related selected test document large unannotated corpus. choice related documentsmade INDRI retrieval system (Strohman, Metzler, Turtle, & Croft, 2005). Notementioned large unannotated corpus different training test corpora.136fiUnsupervised Temporal Relation Learning EventsFigure 4: flowchart BCDC.137fiMirroshandel & Ghassem-SaniStep 3: step, extract events required features related documentsfound INDRI. events specified features section 3.2 automatically annotated EVITA (Saur, Knippen, Verhagen, & Pustejovsky, 2005). Althoughevents and/or features extracted EVITA may incorrect, experimental resultsshow still helpful. extra event-event features requiredfeatures extracted POS tagger statistical parser.Step 4: using existing model, temporal relations intrasentential event pairs related documents predicted. Besides, normalized measureconfidence computed relation. used SVM classificationpurpose. Therefore, designed confidence measure using SVM, explainedbelow.SVM binary classification, positive negative instances linearly partitionedhyper-plane (with maximum marginal distance instances) original higherdimensional feature space. order classify new instance X, distance hyperplane computed X assigned class corresponds sign computed distance. distance instance X hyper-plane H, eitherpositive negative value, supported support vectors X1 . . . Xl computedequation 1 (Han & Kambert, 2006):d(X, H) =lXyi Xi X + b0(1)i=1yi class label support vector Xk ; k b0 numeric parametersautomatically determined.used one-versus-one case multi-class classification classes,set (m 1) / 2 hyper-planes (i.e., one hyper-plane every class pair) denotedH defined. hyper-plane separates class j referred Hi,j . Hi useddenote subset 1 hyper-planes H separates class others.order classify new instance X, distance hyper-plane Hi,j computed.X assigned class j. end process, every instance X, classaccumulated certain number votes, represented Vi (X), numbertimes classifier assigned instance X class i. final class X, denotedC(X), one highest number votes.process described above, easy compute confidence values baseddistance measures equation 1 (i.e., closer case support vectors, lessconfident). precisely, multi-class classification, define confidenceinstance X sum distances class-separating hyper-planes:fififi Xfifififi(X) = fid(X, H)fififiHHC(x)fi(2)Based equation 2, larger value (X) shows X confident, viceversa.138fiUnsupervised Temporal Relation Learning EventsStep 5: step, BCDC chooses K confident temporal relationsdetected step 4.Step 6: retrain SVM injecting temporal relations selected step5. noted test document, original model retrainedconfident relations documents related test documenttest documents.model trained original training data plus confident predictedrelations relevant documents current test document predicted relations test documents.Steps 4-6 repeated one following two termination conditions satisfied: 1) unselected temporal relation, 2) predefined numberiterations reached.Step 7: retraining phase general model selected test documentfinished, temporal relations test document classified based newspecifically retrained model.Then, still unprocessed test documents, BCDC start step 1again; otherwise algorithm terminate.fundamental idea second stage BCDC obtain document-cluster-wide statistics temporal relations different types events,using information improve temporal relation identification.explained above, specific model learned test document, usingnumber unannotated text documents topically related test document(i.e., bootstrapping phase). However, test documents topically related, corresponding retrained models similar. sake efficiency,run bootstrapping phase one test documents, useretrained model rest. words, run steps 2 6 BCDC onemember set similar test documents, members, solely apply step 7.explained section 3.1, explicitly use assumption onetype temporal relation per discourse part BCDC. However, bootstrapping,somehow implicitly benefit assumption seeking topically related documents, likely include similar event pairs identical temporal relations.4. Experimental Results BCDCsection, specification employed corpora briefly explained. Then,accuracy BCDC analyzed.4.1 Characteristic Corporaused two standard corpora (i.e., TimeBank (v 1.2) Opinion, see Mani et al.,2006) experiments. TimeBank 183 newswire documents 64, 077 tokens,139fiMirroshandel & Ghassem-SaniOpinion 73 documents 38, 709 tokens. two datasets annotatedbased TimeML standard (Pustejovsky et al., 2003). mentioned before,fourteen temporal relation types (SIMULTANEOUS, IDENTITY, BEFORE, AFTER,IBEFORE, IAFTER, INCLUDES, INCLUDED, DURING, INV, BEGINS,BEGUN BY, ENDS, ENDED BY) TLink class TimeML. sake reducingdata sparseness problem, many others (Mani et al., 2006; Tatu & Srikanth, 2008;Mani et al., 2007; Chambers et al., 2007), used normalized versionrelation types including six following relations:SIMULTANEOUSOriginal RelationXX IAFTERX ENDEDX BEGUNX INCLUDEDXX IDENTITYX INVENDSIBEFOREBEGINSINCLUDESConverted RelationXIBEFORE XENDS XBEGINS XINCLUDES XINCLUDES XX SIMULTANEOUSX INCLUDESTable 3: normalization process temporal relation types.normalizing, inverse relations merged. conversions shownTable 3. first six conversions, relations easily converted swappingarguments. Relations IDENTITY SIMULTAENOUS collapsed, since IDENTITYsubtype SIMULTANEOUS (i.e., two events IDENTITY SIMULTANEOUS coreferential). Similarly, relations INV INCLUDES alsocollapsed INV subtype INCLUDES (i.e., identical AllensCONTAINS) based Allens interval algebra (Allen, 1984). clearusing conversions, information lost.Relation TypeIBEFOREBEGINSENDSSIMULTANEOUSINCLUDESTOTALTimeBank637711413045881335 (38.35%)3481OTC13116020815289503170 (51.57%)6147Table 4: normalized TLink class distribution TimeBank OTC.140fiUnsupervised Temporal Relation Learning Eventsexperiments, like previous work (Mani et al., 2006; Chambers et al., 2007;Chambers & Jurafsky, 2008), TimeBank Opinion corpora merged singlecorpus called Opinion TimeBank Corpus (OTC). Table 4 shows normalized TLink classdistribution (only Event-Event relations) TimeBank OTC. shown,relation frequent relation; thus forms majority class,used baseline experiments.comparison methods, also used English part TempEval2 corpus. part based TimeBank (Verhagen et al., 2010; Pustejovsky et al., 2003;Boguraev, Pustejovsky, Ando, & Verhagen, 2007). However, TimeBank annotationsreviewed based guidelines TempEval 2010 temporal relationsmodified according specific types shared task.Relation TypeOVERLAPBEFORE-OR-OVERLAPOVERLAP-OR-AFTERVAGUETOTALTask ETraining403279652 (41.19%) 12461511371583Test5838(48.63%)9620255Task FTrainingTest600 (35.46%)9229945518100 (33%)1115589117501692303Table 5: distribution temporal relation types TempEval-2 Corpus Task EF.two parts corpus: 1) training part including 163 documents53, 450 tokens; 2) test part 21 documents 4, 848 tokens. six different temporal relation types: BEFORE, AFTER, OVERLAP, BEFORE-OR-OVERLAP,OVERLAP-OR-AFTER, VAGUE. Among six different tasks TempEval 2010,focused tasks E F, similar problem tackledpaper. Tasks E F tasks consider exclusively relationstwo events. distribution temporal relation types tasks trainingtest parts corpus shown Table 5. majority classes underlinedtable.discussed section 3.3.2, text, retrieve number topicallyrelated texts using public domain software called INDRI. experiments, relatedtexts retrieved English part TDT5 multilingual news text corpus2 .total, TDT5 consists 407, 505 text documents English (278, 109 documents), MandarinChinese (56, 486 documents), modern standard Arabic (72, 910 documents). also250 different topics. Unlike previous TDT corpora, TDT5 contain broadcastnews data; sources newswires.2. TDT 2004: Annotation Manual, Available http://www.ldc.upenn.edu/Projects/TDT2004.141fiMirroshandel & Ghassem-Sani4.2 Experimentsused LIBSVM java source SVM classification (Chang & Lin, 2011).EVITA system (Saur et al., 2005) used event extraction. EVITA worksbased linguistic statistical information. addition event extraction, eventattributes (which described Table 2) also extracted EVITA. alsoused Stanford NLP package3 tokenization, sentence segmentation, part speechtagging, parsing. INDRI retrieval system (Strohman et al., 2005) employed obtain related documents. INDRI language model based search engineprovides state-of-the-art text search engine. English part TDT5 indexedINDRI, using search engine, texts highly related specifieddocuments retrieved.mentioned earlier, applied algorithm TimeBank, OTC, TempEval 2010 Corpora. randomly selected 20 documents (almost 10 percent totaldocuments) TimeBank development set. Based several experiments development set different number extracted related documents step 2 BCDC(i.e., N), number confident relations chosen step 5 (i.e., K), set N25 K 40.TimeBank OTC, results evaluated first excluding 20 documentsdevelopment set measuring accuracy using five-fold cross validationmethod. However, corpus TempEval-2, need cross validation,training test sets predetermined, reported accuracyBCDC test set.Table 6 shows results three different settings proposed algorithmseveral others TimeBank OTC. table, baseline majority classevent-event relations (i.e., relation) evaluated corpora. Manis methodregarded successful statistical approach temporal relation identification,exclusively uses gold standard features events (Mani et al., 2007). Methods proposedChambers Mani similar except Chambers also used number extrafeatures two step algorithm. method currently regarded state-of-the-artstatistical approaches TimeBank OTC. achieve higher accuracy, alsoused extra resources WordNet (Chambers et al., 2007).Argument ancestor path distance (AAPD) accurate convolution tree kerneluses parse trees event-event sentences temporal relation classification (Mirroshandel et al., 2009b). AAPD polynomial composite kernel combines simple eventkernel AAPD (Mirroshandel et al., 2009b). mentioned simple event kernel linear kernel exclusively uses features Manis method (Mirroshandelet al., 2009b). AAPD AAPD polynomial kernels designed appliedevent pairs within sentence. Accordingly, relations TimeBankOTC split two parts: 1) relations intra-sentential event pairs,2) relations inter-sentential event pairs. kernels appliedfirst part second part, used simple event kernel (i.e., Manis kernel).Table 6, results reported AAPD AAPD polynomial kernel factoutcome merging partial results two parts.3. Available http://nlp.stanford.edu/software/index.shtml142fiUnsupervised Temporal Relation Learning EventsMethodBaselineChambersMani (Event Kernel + Basic Features)Classic Bootstrapping + Event Kernel + Basic FeaturesBCDC + Event Kernel + Basic FeaturesAAPD Kernel + Extra Event-Event FeaturesClassic Bootstrapping + AAPD Kernel + Extra Event-EventFeaturesBCDC + AAPD Kernel + Extra Event-Event FeaturesAAPD Polynomial Kernel + Basic Features + Extra EventEvent FeaturesClassic Bootstrapping + AAPD Polynomial Kernel + BasicFeatures + Extra Event-Event FeaturesBCDC + AAPD Polynomial Kernel + Basic Features +Extra Event-Event FeaturesTimeBankCorpus38.3559.4350.9753.2159.715457.98OTCCorpus51.5765.4862.563.1265.1963.4464.5362.5657.0266.2965.9559.8366.5566.1868.07Table 6: accuracy proposed methods event-event temporal relation classificationTimeBank OTC.BCDC + Event Kernel + Basic Features bootstrapped algorithm,uses basic features, mentioned section 3.2.1, applying simple event kernel (Mirroshandel et al., 2011). BCDC + AAPD Kernel + Extra Event-Event Features,utilized extra event-event features AAPD kernel. Third setting (BCDC + AAPD Polynomial Kernel + Basic Features +Extra Event-Event Features) uses AAPD Polynomialkernel combine basic extra event-event features.better comparison, also applied classic bootstrapping methodSVM kernels features BCDC. reporting results, initial modeltrained standard corpus (i.e., like stage one BCDC). Then, iterative manner,confident samples documents (rather related documents) usedretrain model. Note case, need process retrieving relateddocuments, one model learned test documents. order find bestvalue K (i.e., number confident samples) classic bootstrapping method,performed several different experiments mentioned development set. Incidentally,experiments showed here, too, K set 40.Table 6 indicates, BCDC + AAPD Kernel + Extra Event-Event FeaturesBCDC + AAPD Polynomial Kernel + Basic Features + Extra Event-Event Featuresshow significant improvement state-of-the-art method (i.e., Chambers method).Comparison BCDC classical bootstrapping shows effectiveness proposed idea extracting retraining samples related documents.improvement TimeBank considerable OTC. seemsdifferent distributions temporal relations two corpora caused difference143fiMirroshandel & Ghassem-Saniimprovements. shown Table 4, OTC, majority class (i.e.,relation) larger part whole corpus. causes learning algorithmbecome biased towards relation, thus correct predictionrelations becomes harder. contrary, TimeBank, distribution less biasedthus BCDC shown improvement corpus.Method 1Method 2BCDC + Event Kernel + Basic FeaturesBCDC + AAPD Kernel + Extra Event-Event FeaturesBCDC + AAPD PolynomialKernel + Basic Features +Extra Event-Event FeaturesBCDC + Event Kernel + Basic FeaturesMani (Event Kernel + BasicFeatures)AAPD Kernel + Extra EventEvent FeaturesAAPD Polynomial Kernel+ Basic Features + ExtraEvent-Event FeaturesClassic Bootstrapping +Event Kernel + Basic FeaturesClassic Bootstrapping +AAPD Kernel + ExtraEvent-Event FeaturesClassic Bootstrapping +AAPD Polynomial Kernel+ Basic Features + ExtraEvent-Event FeaturesChambersBCDC + AAPD Kernel + Extra Event-Event FeaturesBCDC + AAPD PolynomialKernel + Basic Features +Extra Event-Event FeaturesBCDC + Event Kernel + Basic FeaturesBCDC + AAPD Kernel + Extra Event-Event FeaturesBCDC + AAPD PolynomialKernel + Basic Features +Extra Event-Event FeaturesP-ValueTimeBank0.0134P-ValueOTC0.02960.03830.02120.04220.03110.05140.02990.04410.04020.04910.07650.0878Chambers0.05030.0487Chambers0.04310.03970.0476Table 7: statistical significance test results proposed methods.testing statistical significance, applied type stratified shuffling,kind compute-intensive randomized test. null hypothesis (i.e., two modelsproduced observed results same) tested randomly shuffling generatedoutput event pair two models re-computing evaluationmetrics (i.e., accuracy case). difference particular metric shufflingequal greater original observed difference metric, counter (nc)metric incremented. Ideally, perform 2n possible shuffles,n shows number test cases (i.e., event pairs). But, case, impracticaln rather large number. Therefore, many others, tried 10, 000144fiUnsupervised Temporal Relation Learning Eventsiterations (nt). finishing iterations, p-value (likelihood incorrectly rejectingnull hypothesis) simply calculated (nc + 1)/(nt + 1) (Yeh, 2000). Table 7 showsresult significance test proposed methods. test, proposed methodcompared relevant method.shown Table 7, majority methods passed test (the p-value less0.05). four exceptions p-value slightly greater 0.05.Table 8 shows accuracy BCDC English part corpus used TempEval2010 tasks E F. JU-CSE, NCSU-indi, NCSU-joint, TIPSem, TIPSem-B, TRIOS,TRIPS participants TempEval 2010 shared task (Verhagen et al., 2010).methods Table 6. AAPD kernel applied event pairswithin sentence. Therefore, unable apply task E. inter-sententialevent pairs, AAPD Polynomial kernel almost similar simple event kernel,cannot use syntactic parse trees, appropriate sources information.MethodBaselineJU-CSENCSU-indiNCSU-jointTIPSemTIPSem-BTRIOSTRIPSEvent Kernel + Basic FeaturesBCDC + Event Kernel + Basic FeaturesAAPD Kernel + Extra Event-Event FeaturesBCDC + AAPD Kernel + Extra Event-Event FeaturesAAPD Polynomial Kernel + Basic Features + ExtraEvent-Event FeaturesBCDC + AAPD Polynomial Kernel + Basic Features+ Extra Event-Event FeaturesTask E495648515555565838.0244.3538.54Task F335666255960605933.2338.4440.7147.2043.5145.6250.41Table 8: accuracy proposed methods tasks E F TempEval 2010 sharedtask.seen Table 8, although BCDC shown improvementaccuracy temporal relation identification (i.e., comparison base methods),generally weaker almost participants TempEval 2010. thinkweakness due restricted feature set used BCDC. words,majority participants TempEval 2010 used richer feature sets different levels(e.g., lexical, syntactic, semantic), used simple event features (plussyntactic features task F). think verified yetricher set features, BCDC produce successful results TempEvals tasks, too.Besides, think replacing base method successful method TipSem145fiMirroshandel & Ghassem-SaniTRIPS, make BCDC competitive participants TempEval 2010. However,show this, first need find appropriate confidence measure step 4 BCDC4 ,requires investigation one directions future research.Figure 5: effects utilizing related documents vs. randomly selected documentsaccuracy BCDC TimeBank OTC.4.3 Analysisseen Tables 6 8, BCDC shown substantial improvementseveral different methods terms accuracy without using extra annotated data.Bootstrapping using number related documents following positive effects:1) knowing relation events, better predict relation typesanalogous events, may appear related documents.2) related documents, number sentences similar patterns increase,tree kernels extract confident information parse trees. Thus4. noted proposed confidence measure useful SVM classification technique.146fiUnsupervised Temporal Relation Learning Eventsway, SVM informative.3) used corpora rather small examples relation. datasparseness problem affect performance temporal relation identification method.BCDC, retrieving related documents extraction new temporal relationsdocuments increase number relations improve performance alleviatingdata sparseness problem.One remaining question impact choosing related documents?.words, randomly choose number unrelated documents bootstrappingphase BCDC. show effectiveness idea using related documents,repeated experiments N = 25 (i.e., original BCDC) randomly selecteddocuments. results experiments shown Figure 5. shown,although randomly selected documents slightly improved base methods, however,improvement comparable using related documents.5. Using EM Temporal Relation Learning (EMTRL)Since supervised even semi-supervised methods need annotated corpora, manylanguages and/or domains exist, here, propose unsupervised algorithmtemporal relation learning problem. Due encouraging results expectationmaximization (EM) algorithm unsupervised tasks natural language processingunsupervised grammar induction (Klein, 2005), unsupervised anaphora resolution(Cherry & Bergsma, 2005; Charniak & Elsner, 2009), unsupervised coreference resolution (Ng, 2008), decided evaluate EM unsupervised temporal relation extraction.Currently, reported work temporal relation extraction based EM. fact,yet attempt towards unsupervised approach temporal relationextraction. Here, explain EM successfully applied task temporalrelation extraction show performance EM encouraging task.that, first introduce definitions notations later used subsequentsections.5.1 EM AlgorithmEM general algorithm maximum likelihood estimation (MLE) (Dempster, Laird, &Rubin, 1977). algorithm used deal incomplete information.mentioned before, temporal relation learning, task determine typetemporal relation r two events e1 e2 . algorithm, context meanssentence (or sentences) containing pair events.5.2 Proposed ModelLet us call new proposed algorithm EMTRL, stands EM based temporalrelation learning. EMTRL operates corpus level, inducing valid temporal clusteringevent pairs given corpus. specifically, EMTRL induces probabilitydistribution maximize P (corpus) (the probability corpus). easily incorporate147fiMirroshandel & Ghassem-Sanilinguistic constraints, corpus represented event pairs (ei ej ). assume event pairsindependent:P (corpus) =P (ei ej )(3)ei ej corpusrewrite P (ei ej ) uses hidden variable Cipair ei ej ) influences observed variables (ei ej ):XP (ei ej ) =Cijj(temporal class eventP (ei ej , Ci j )(4)possible temporal classesprobability P (ei ej , Ci j ) rewritten as:P (ei ej , Ci j ) = P (ei ej | Ci j )P (T Ci j )(5)inducing temporal relations, EMTRL runs EM model. use uniformdistribution P (T Ci j ). clear could choose informative priordistribution P (T Ci j ), would benefits like better handle skewness distribution. applications EM, settings priordistribution. However, problem temporal relation learning, cannotprior distribution except uniform distribution; here, temporal relationtypes would seem equal learner.expand equation 5, pair ei ej represented features,potentially used determining temporal relation type events ei ej .Therefore, P (ei ej | Ci j ) rewritten using equation 6:P (ei ej | Ci j ) = P (ei e1j , ei e2j , ... ei ekj | Ci j )(6)ei elj value lth feature ei ej . features, similarmentioned work Chambers Jurafsky (2008), listed Table 9.reduce data sparseness problem improve probability estimation, conditional independence assumed features value generation. assumetense aspect dependent (i.e., tensei aspecti ), tense aspect definetemporal location event structure, thus considering features togetherrich source information temporal relation extraction system. conditionalindependence assumption, value P (ei ej | Ci j ) rewritten as:P (ei ej | Ci j ) =P (ei elj | Ci j )(7)f eatures lprobabilities (i.e. P (ei elj | Ci j ) ) regarded parameters proposedmodel. using them, likelihood different temporal classes determined.Based features Table 9 different temporal classes, P (ei elj | Ci j )defined. Four examples probabilities shown below:P (class(ei ) = OCCU RREN CE class(ej ) = P ERCEP ION | Ci j =BEF ORE)148fiUnsupervised Temporal Relation Learning EventsFeatureW ord1 & W ord2Lemma1 & Lemma2Synset1 & Synset2P OS1 & P OS2Event Government V erb1 &V erb2Event Government V erb1 &V erb2 POSAuxiliaryClass1 & Class2ense1 & ense2Aspect1 & Aspect2odality1 & odality2P olarity1 & P olarity2Tense MatchAspect MatchClass MatchTense PairAspect PairClass PairPOS pairP reposition1P reposition2Text orderDominatesEntity MatchDescriptiontext first second eventslemmatized first second events headsWordNet synset first second events headsPOS first second eventsverbs govern first second eventsverbs POS govern first second eventsauxiliary adverbs verbs modifies governing verbsClass first second eventstense first second eventsaspect first second eventsmodality first second eventspolarity first second eventstwo events tensetwo events aspecttwo events classPair two events tensePair two events aspectPair two events classPair two events POSfirst event prepositional phrasesecond event prepositional phrasefirst event occurs first documentfirst event syntactically dominates second evententity argument shared twoeventsTable 9: features events used EMTRL temporal relation learning.149fiMirroshandel & Ghassem-SaniP (ei dominates ej | Cij= AF ER )P (T ense(ei ) = P AST ense(ej ) = P AST Aspect(ei )N E Aspect(ej ) = P ROGRESSIV E | Ci j = OV ERLAP )P (P OS ei = V P OS ej = N | Cij== AF ER )5.3 Induction Algorithminduce temporal clustering corpus, EM applied proposed model.EMTRL, corpus (i.e., event pairs) temporal clustering C respectivelyobserved unobserved (the hidden) random variables. EM algorithm includes twomain steps expectation (E) maximization (M), task definedfollowing way iteratively estimate parameters model (i.e., P (ei elj | Ci j )):E-step: Fix current parameters model, assign probability, P (T Ci j | ei ej ),possible temporal class event pairs (ei ej ) corpus. probabilitycomputed following equation:P (ei ej , Ci j )P (ei ej )rewrite equation 8 using equations 4, 5, 7:P (T CiP (T Cijj| ei ej ) =lf eatures l P (ei ej | Ci j )Q0possible temporal classes P (T Ci j ) f eatures l P (eiP (T Ci j )| ei ej ) = P(8)Qelj | Ci0 j )(9)Using equation 9, event pair (ei ej ), temporal relation type (temporal class)highest probability selected. relations later used M-stepupdate parameters model.Ci0jM-step: fixing determined temporal relations E-step, parametersmodel, P (ei elj | Ci j ), updated step. achieving goal, different optimization algorithms conjugate gradient used. However, algorithmsslow costly. addition, difficult smooth methods desired manner.Therefore, used relative frequency method re-estimation parameters,using equation 10:P (ei elj | Ci j ) =N (ei elj , Ci j )N (T Ci j )(10)N () counts number times given items joint items appearedcorpus. example, updating probability P (ei dominates ej | Ci j = AF ER)done dividing N (ei dominates ej , Ci j = AF ER) (i.e., number timesei dominates ej temporal relation ei ej AFTER) N (T Ci j =AF ER) (i.e., number times relation event pairs corpus AFTER).150fiUnsupervised Temporal Relation Learning EventsSteps E repeated one following termination conditionssatisfied: 1) predefined number iterations reached, 2)changes P (ei elj | Ci j ). practice, EMTRL usually stopped 30 predefinediterations, final behavior apparent 15 22 iterations.finishing training phase, temporal relation (T Ci j ) requested eventpairs ei ej determined using following equation:Cij= arg maxT C 0possible temporal classes P (T C0| ei ej )(11)Now, EM algorithm begin either E-Step M-step. startinduction algorithm M-step. clear parameters model availablefirst iteration EM. Instead, initial distribution temporal clusteringused. important question: one initialize distribution?Initialization important task EM, EM guarantees find localmaximum likelihood. quality local maxima highly dependentinitial starting point. tested three different ways initialization:1) Random Initialization: uniform distribution temporal clusteringused; therefore, temporal clustering first step equal probability.2) 10% Supervised Initialization: used small part labeled corpus (10%relation type) task. Relations selected randomly.3) Rule-based Initialization: used specific rules initial estimation temporal relation types used initial estimation computing parameters model.rules combination so-called GTag rules (Mani et al., 2006), VerbOcean (Chklovski & Pantel, 2005), rules derived certain signal words (e.g., on,during, when, if) text. GTag contains 187 syntactic lexical rulesinferring labeling temporal relations event, document time, time expressions. rules, 169 event pairs, utilized EMTRL.169 rules either event pairs sentence two mainevents two consecutive sentences. example GTag rule shown below; rulesaccessible Blinker part TARSQI toolkit5 .conjBetweenEvents = ES &&isT heSameSentence = RU E &&event1 .class = (OCCU RREN CE|P ERCEP ION |ASP ECT U AL|I ACT ION ) &&event2 .class = ST E &&event1 .tense = P AST &&event2 .tense = P AST &&event1 .aspect = N E &&event2 .aspect = P ERF ECT &&event1 .pos = V ERB &&event2 .pos = V ERB5. Available http://www.timeml.org/site/tarsqi/index.html151fiMirroshandel & Ghassem-Sanirelation(event1 , event2 ) = AF ERVerbOcean contains lexical rules two verbs, mined usinglexical syntactic patterns. relation verb pairs one different semantic relations strength, enablement, antonymy, similarity, happens-before.extracted 4, 205 happens-before rules VerbOcean. Two examples rulesshown below:announce [happens-before] postpone :: 12.844086review [happens-before] recommend :: 9.049530rule contains two verbs, relation, strength value relationship.example, second rule shows relation happens-before review recommend strength 9.049530. also designed 23 rules based signalwords before, on, when. rules GTag format. examplegroup rules given below:isT heSameSentence = rue &&signal = bef ore &&signalBetweenT woEvents = ruerelation(event1 , event2 ) = bef oreLike many statistical NLP tasks, smoothing vital alleviate problemdata sparseness. particular, first iterations, much smoothing requiredlater iterations. experiments, used simply add-1 smoothing techniquecomputing equation 10.6. Experimental Results EMTRLLike experiments BCDC, TimeBank OTC also used experimentsEMTRL. However, order simplify task, used different normalized versioncorpora, contained three following temporal relations:OVERLAPmain reason simplification EMTRL reducing level supervision task temporal relation learning makes even difficult task,already considered hard one (Mani et al., 2006). normalize corporareduce number relation types three, adopted normalization approach like previous work (Bethard et al., 2007b), IBEFORE relationsmerged relations. Similarly, IAFTER relationsalso merged relations. remaining ten relation typescollapsed OVERLAP relations. Table 10 shows converted TLink class distributionTimeBank OTC.152fiUnsupervised Temporal Relation Learning EventsRelation TypeOVERLAPTotalTimeBank Corpus7066922083 (59.83 %)3481OTC Corpus236910732792 (44.79 %)6234Table 10: converted TLink class distribution TimeBank OTC.Beside TimeBank OTC, performance EMTRL also evaluatedtasks E F TempEval-2 corpus. tasks relations distributionshown Table 5.6.1 Results Discussionsexperiments, baselines majority class event pair relationsemployed corpora (i.e., OV ERLAP corpora). Note Manis methodfact supervised, exclusively uses gold-standard features (Mani et al., 2007).Chambers method similar Manis, except also uses external resourcesWordNet (Chambers et al., 2007). Here, result implementation ManiChambers methods different reported results, because, explainedbefore, considered three temporal relation types reported experiments,six relation types.Table 11, addition results employing EMTRL three different initializations, also reported results initializations stand-alone classifiers.Random Initialization EMTRL + Random Initialization, question may arisemethods determine label different classes. fact, methodsdistinguish three different classes (Class1 , Class2 , Class3 ). Among different possible ways unlabeled classes mapped BEF ORE, AF ER,OV ERLAP , choose mapping similarity predicted annotated temporal relations maximized.Method TypeBaselineManiChambersRandom InitializationEMTRL + Random Initialization10% Supervised InitializationEMTRL + 10% Supervised InitializationRule-based InitializationEMTRL + Rule-based InitializationTimeBank Corpus59.8361.5566.7935.9940.9239.3348.3138.9247.86OTC Corpus44.7960.5862.9437.2943.0241.1449.3441.0348.78Table 11: accuracy results different methods TimeBank OTC.153fiMirroshandel & Ghassem-SaniConsidering unsupervised nature EMTRL, results Table 11 encouraging. shown table, TimeBanks baseline well OTC.TimeBank highly biased towards OVERLAP. Accordingly, difficultlearning methods pass baseline TimeBank. performance Manismethod, fully supervised approach, slightly baseline.case, EMTRLs accuracy considerably baseline. However, case OTC,performance passed baseline.shown Table 11, EMTRLs accuracy three different initializations,respectively superior stand-alone counterparts. statisticalsignificance results table shows superiority EMTRLbaseline (i.e., case OTC) stand-alone initializations (i.e., corpora)verified stratified shuffling test significance level = 0.05.Table 11 shows best accuracy belongs Chambers method. However,noted method currently best-reported results TimeBankOTC among supervised temporal relation extraction methods.Table 11 also shows EMTRL + Randomized Initialization efficienteither corpora. may due fact randomized initialization hardproblem causes divergence probability distribution. hand, twoinitializations shown satisfactory results tackling problem. impliesinitialization critical factor EMTRL, even little source supervisioncrucial achieving satisfactory results.Method TypeBaselineNCSU-indiTRIPSRandom InitializationEMTRL + Random Initialization10% Supervised InitializationEMTRL + 10% Supervised InitializationRule-based InitializationEMTRL+ Rule-based InitializationTask E49485824.7527.9226.3532.2027.1732.76Task F33665919.1122.3323.7726.2923.6427.03Table 12: accuracy results different methods tasks E F TempEval-2Corpus.Table 12 shows results applying EMTRL corpus TempEval 2010.comparison accuracy kernels Table 8, EMTRL could achieve encouragingresults. case, EMTRLs accuracy three different initializations, alsorespectively superior stand-alone counterparts. TRIPS NCSU-indisuccessful supervised systems tasks E F TempEval 2010, respectively(Verhagen et al., 2010).154fiUnsupervised Temporal Relation Learning Events6.2 Inconsistency RemovalSince pair-wise relation learning system, relation pair eventspredicted without considering impact relations event pairs, system mayencounter inconsistencies among predicted relations. may happen selectingtemporal relations equation 9 E-step. also happen finding final class labelsequation 11. Figure 6 shows example inconsistent relation events A,B, C:EventBfter BB fter CEventEvent CB efore CFigure 6: contradiction temporal relations three events A, B, C.several ways eliminating inconsistencies (Mani et al., 2007; Tatu &Srikanth, 2008; Chambers & Jurafsky, 2008). work, used two differentapproaches: greedy best-first search strategy Integer Linear Programming (ILP)based method. details approaches given next.6.2.1 Greedy Best-First Search Strategyorder detect possible inconsistencies predicted relations, first build graphtext, node corresponds event, edge represents temporalrelation corresponding events. Then, existing contradiction among connectednodes graph discovered applying set rules (i.e., 640 rules) basedAllens interval algebra (Allen, 1984). example, consider following three rules:bef ore(x, y) && bef ore(y, z)bef ore(x, z)af ter(x, y) && bef ore(z, y)af ter(x, z)af ter(x, y) && includes(y, z)af ter(x, z)inconsistent relations graph stored sorted list named SL, basedcomputed confidence score (i.e., P (T Ci j | ei ej ) equation 9). Thus, SL, firstlast elements least confident relations, respectively.algorithm starts first relation SL, pops relation addsanother list named F L. adding new relation F L, algorithm verifiesconsistency among relations F L. new relation introduces inconsistency,155fiMirroshandel & Ghassem-Sanireplaced next confident relation corresponding events. replacementmay repeated new relation consistent relations existingF L. contradictions F L, algorithm move nextelement SL F L. operations iterated remain relationsSL. resultant consistent relations F L used subsequent M-stepfinal result EM.6.2.2 Integer Linear Programming (ILP)second approach, cast task finding probable temporal relationsoptimization problem. contrast previous method, approach, basedinteger linear programming (ILP) framework, finds optimal solution basedparameters model, P (ei elj | Ci j ). method similar ChambersJurafsky (2008).ILP framework, event pair (ei , ej ), relation type eiej denoted Ri jM . objective function framework defined follows:!maxX Xj >X(PijMRijM+ PjiMRjiM )(12)(Pi jM (i.e., P (M | ei ej ) equation 8) probability temporal relationtype ei ej . objective function maximizes sum probabilitiestemporal relations event pairs input text. also three followingconstraints (i.e., 13, 14, 15) objective function:j M, > j : RijM ,Constraint 13 implies RijMX(T Rij, > j :RjiM{0, 1}(13)variable either zero one.jM+ RjiM )=1(14)Constraint 14 ensures pair events (ei ej ), one Ri jMvariable set one, rest set zero. words, impossible pairevents two (or more) relations.RijM 1+ RjkM 2RikM 31(15)Constraint 15 guarantees transitivity conditions among event pairs, wherever relations Ri jM 1 Rj kM 2 entail relation Ri kM 3 . obvious transitivityconstraint effective event pairs connected one another. disconnected graph, constraint little effect. example, Figure 6, consideringconstraint relations RA BAf ter RB CAf ter , RA CAf ter possiblerelation events C.generating set constraints document, use ILP solver(SCIP6 ) solve problem. One important issue ILP technique6. ILP solver fastest existing noncommercial mixed integer programming solver. Availablehttp://scip.zib.de/156fiUnsupervised Temporal Relation Learning Eventseffective dense temporal graphs sparse ones.removing contradictions temporal relations, generated (consistent) relationseasily used updating probabilities model M-step. resultsTables 11 12 without applying greedy best-first search ILP. accuracyresults greedy ILP algorithms TimeBank OTC shown Table13. Table 14 shows accuracy results tasks E F corpus TempEval2010. One question may arise enforce transitivity constraints EM,labels Class1 , Class2 , Class3 , rather BEFORE, AFTER,OVERLAP. problem happens case EMTRL + Random Initialization,used prior assignment Class1 = AF ER, Class2 = BEF ORE,Class3 = OV ERLAP . two initializations (i.e., 10% SupervisedRule-base), problem occur, algorithm starts actual classlabels BEFORE, AFTER, OVERLAP.Method TypeEMTRL + Random InitializationEMTRL + 10% SupervisedInitializationEMTRL + Rule-based InitializationTimeBankBaseGreedyMethod40.9241.0941.03BaseMethod43.0248.3149.5450.3447.8650.8852.12ILPOTCGreedyILP42.9443.0049.3450.5251.4448.7849.9851.17Table 13: accuracy results applying greedy best-first search strategy ILPTimeBank OTC.Method TypeEMTRL + Random InitializationEMTRL + 10% SupervisedInitializationEMTRL + Rule-based InitializationTask EGreedyILP28.0328.04BaseMethod22.3332.2033.5433.9232.7633.4934.12BaseMethod27.92Task FGreedyILP22.3222.3026.2927.9427.9227.0328.3628.55Table 14: accuracy results applying greedy best-first strategy ILP tasksE F TempEval 2010.157fiMirroshandel & Ghassem-SaniTables 13 14 show impact utilizing greedy best first search ILPapproaches EMTRL base method. using strategies,inconsistencies may exist among predicted temporal relations, removed (in stepE EMTRL) make predicted relations reliable. result, step M,parameters model updated accurately thus accuracywhole algorithm iteratively increase.significance results depicted Tables 13 14 verifiedstratified shuffling significance level = 0.05. expected, resultsapproaches EMTRL + Random Initialization statistically significant.hand, majority tests EMTRL + 10% Supervised Initialization EMTRL +Rule-based Initialization, compared output greedy ILP algorithmsbase method, statistical significance results verified.7. Conclusion Future Workpaper, addressed problem temporal relation learning events,topic interest since early days statistical natural language processing.concentrated efforts reduce need annotated corpora muchpossible. Accordingly, paper, two new algorithms, weakly supervisedunsupervised, presented.first algorithm two-stage weakly supervised approach classificationtemporal relations. first stage algorithm, SVM based classifier trainedlearn temporal relations corpus. Then, second stage algorithm,cross-document bootstrapping technique employed iteratively improve modelproduced first stage. idea bootstrapping, inspiredhypothesis called one type temporal relation events per discourse,test document, global evidences cluster topically related documentsrefined local decisions made initial model. results experiment newtechnique showed significant improvement terms accuracy related work includingstate-of-the-art statistical methods.second proposed algorithm novel model used EM algorithminterval algebra reasoning temporal relation learning. compared worksuccessful fully supervised methods. experiments showed encouraging results,considering low level supervision provided algorithm.Currently, working finding ways improvement algorithms,time trying reduce supervision level. BCDC, extracting semanticfeatures related documents, may able improve performance. Inconsistencyremoval (i.e. ILP greedy best first search) algorithms also employed BCDC.Besides, employing hypothesis one type temporal relation events perdiscourse explicit constraint possible direction research.EMTRL, one use sources information like narrative information, relationsevents document times, relations events time expressionsbuild denser temporal graph. increases effectiveness greedy best firstsearch integer linear programming algorithms. also think, verified yet,using richer feature set may improve accuracy EMTRL.158fiUnsupervised Temporal Relation Learning EventsAcknowledgmentsauthors wish thank associate editor anonymous reviewersvaluable comments.ReferencesAllen, J. (1984). Towards general theory action time. Artificial intelligence, 23 (2),123154.Bethard, S., & Martin, J. (2007). Cu-tmp: Temporal relation classification using syntacticsemantic features. Proceedings 4th International Workshop SemanticEvaluations, pp. 129132. Association Computational Linguistics.Bethard, S., & Martin, J. (2008). Learning semantic links corpus parallel temporalcausal relations. Proceedings 46th Annual Meeting AssociationComputational Linguistics Human Language Technologies: Short Papers, pp.177180. Association Computational Linguistics.Bethard, S., Martin, J., & Klingenstein, S. (2007a). Finding temporal structure text:Machine learning syntactic temporal relations. International Journal SemanticComputing, 1 (4).Bethard, S., Martin, J., & Klingenstein, S. (2007b). Timelines text: Identificationsyntactic temporal relations. Semantic Computing, 2007. ICSC 2007. InternationalConference on, pp. 1118. IEEE.Bethard, S. (2007). Finding event, temporal causal structure text: machine learningapproach. Ph.D. thesis, University Colorado Boulder.Boguraev, B., & Ando, R. (2005). Timeml-compliant text analysis temporal reasoning.Proceedings IJCAI, Vol. 5, pp. 9971003.Boguraev, B., Pustejovsky, J., Ando, R., & Verhagen, M. (2007). Timebank evolutioncommunity resource timeml parsing. Language Resources Evaluation, 41 (1),91115.Bramsen, P., Deshpande, P., Lee, Y., & Barzilay, R. (2006). Inducing temporal graphs.Proceedings 2006 Conference Empirical Methods Natural LanguageProcessing, pp. 189198. Association Computational Linguistics.Chambers, N., & Jurafsky, D. (2008). Jointly combining implicit constraints improvestemporal ordering. Proceedings Conference Empirical Methods NaturalLanguage Processing, pp. 698706. Association Computational Linguistics.Chambers, N., Wang, S., & Jurafsky, D. (2007). Classifying temporal relationsevents. Proceedings 45th Annual Meeting ACL Interactive PosterDemonstration Sessions, pp. 173176. Association Computational Linguistics.Chang, C., & Lin, C. (2011). Libsvm: library support vector machines. ACM Transactions Intelligent Systems Technology (TIST), 2 (3).159fiMirroshandel & Ghassem-SaniCharniak, E., & Elsner, M. (2009). Em works pronoun anaphora resolution. Proceedings 12th Conference European Chapter Association Computational Linguistics, pp. 148156. Association Computational Linguistics.Cheng, Y., Asahara, M., & Matsumoto, Y. (2007). Naist. japan: Temporal relation identification using dependency parsed tree. Proceedings 4th International WorkshopSemantic Evaluations, pp. 245248. Association Computational Linguistics.Cherry, C., & Bergsma, S. (2005). expectation maximization approach pronoun resolution. Proceedings Ninth Conference Computational Natural LanguageLearning, pp. 8895. Association Computational Linguistics.Chklovski, T., & Pantel, P. (2005). Global path-based refinement noisy graphs appliedverb semantics. Natural Language ProcessingIJCNLP 2005, pp. 792803. Springer.Collins, M., & Duffy, N. (2001). Convolution kernels natural language. ProceedingsNIPS, Vol. 14, pp. 625632.Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data viaem algorithm. Journal Royal Statistical Society. Series B (Methodological),39, 138.Denis, P., & Muller, P. (2010). Comparison different algebras inducing temporalstructure texts. Proceedings 23rd International Conference Computational Linguistics, pp. 250258. Association Computational Linguistics.Denis, P., & Muller, P. (2011). Predicting globally-coherent temporal structures textsvia endpoint inference graph decomposition. Twenty-Second InternationalJoint Conference Artificial Intelligence.Derczynski, L., & Gaizauskas, R. (2010). Usfd2: Annotating temporal expresions tlinkstempeval-2. Proceedings 5th International Workshop Semantic Evaluation, pp. 337340. Association Computational Linguistics.Ha, E., Baikadi, A., Licata, C., & Lester, J. (2010). Ncsu: Modeling temporal relationsmarkov logic lexical ontology. Proceedings 5th International WorkshopSemantic Evaluation, pp. 341344. Association Computational Linguistics.Hagege, C., & Tannier, X. (2007). Xrce-t: Xip temporal module tempeval campaign.Proceedings fourth international workshop semantic evaluations (SemEval2007), pp. 492495.Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. (2009).weka data mining software: update. ACM SIGKDD Explorations Newsletter, 11 (1),1018.Han, J., & Kambert, M. (2006). Data Mining: Concepts Techniques (second edition).San Francisco: Morgan Kaufmann.Hepple, M., Setzer, A., & Gaizauskas, R. (2007). Usfd: preliminary exploration featuresclassifiers tempeval-2007 tasks. Proceedings SemEval, pp. 438441.Ji, H., & Grishman, R. (2008). Refining event extraction cross-document inference.Proceedings Joint Conference 46th Annual Meeting ACL, pp.254262. Association Computational Linguistics.160fiUnsupervised Temporal Relation Learning EventsKlein, D. (2005). Unsupervised Learning Natural Language Structure. Ph.D. thesis,Department Computer Science, Stanford University.Kolya, A., Ekbal, A., & Bandyopadhyay, S. (2010). Ju cse temp: first step towardsevaluating events, time expressions temporal relations. Proceedings5th International Workshop Semantic Evaluation, pp. 345350. AssociationComputational Linguistics.Lapata, M., & Lascarides, A. (2006). Learning sentence-internal temporal relations. JournalArtificial Intelligence Research, 27 (1), 85117.Lin, D., & Pantel, P. (2001). Dirt: discovery inference rules text. Proceedingsseventh ACM SIGKDD international conference Knowledge discoverydata mining, pp. 323328. ACM.Llorens, H., Saquete, E., & Navarro, B. (2010). Tipsem (english spanish): Evaluating crfssemantic roles tempeval-2. Proceedings 5th International WorkshopSemantic Evaluation, pp. 284291. Association Computational Linguistics.Mani, I., Verhagen, M., Wellner, B., Lee, C., & Pustejovsky, J. (2006). Machine learningtemporal relations. Proceedings 21st International Conference Computational Linguistics 44th annual meeting Association ComputationalLinguistics, pp. 753760. Association Computational Linguistics.Mani, I., Wellner, B., Verhagen, M., & Pustejovsky, J. (2007). Three approaches learningtlinks timeml. Tech. rep., Technical Report CS-07-268, Brandeis University.Mikheev, A., Grover, C., & Moens, M. (1998). Description ltg system used muc-7.Proceedings 7th Message Understanding Conference (MUC-7). Fairfax, VA.Min, C., Srikanth, M., & Fowler, A. (2007). Lcc-te: hybrid approach temporal relationidentification news text. Proceedings 4th International WorkshopSemantic Evaluations, pp. 219222. Association Computational Linguistics.Mirroshandel, S., Ghassem-Sani, G., & Khayyamian, M. (2009a). Event-time temporalrelation classification using syntactic tree kernels. Proceeding 4th LanguageTechnology Conference, pp. 300304.Mirroshandel, S., Ghassem-Sani, G., & Khayyamian, M. (2009b). Using tree kernelsclassifying temporal relations events. Proceedings 23th Pacific AsiaConference Language, Information Computation, pp. 355364.Mirroshandel, S., Ghassem-Sani, G., & Khayyamian, M. (2011). Using syntactic-based kernels classifying temporal relations. Journal Computer Science Technology,26 (1), 6880.Mulkar-Mehta, R., Hobbs, J., Liu, C., & Zhou, X. (2009). Discovering causal temporalrelations biomedical texts recognizing causal temporal relations. ProceedingsAAAI Spring Symposium, Stanford CA.Muller, P., & Tannier, X. (2004). Annotating measuring temporal relations texts.Proceedings 20th international conference Computational Linguistics, pp.5056. Association Computational Linguistics.161fiMirroshandel & Ghassem-SaniNg, V. (2008). Unsupervised models coreference resolution. Proceedings Conference Empirical Methods Natural Language Processing, pp. 640649. AssociationComputational Linguistics.Pekar, V. (2006). Acquisition verb entailment text. Proceedings main conference Human Language Technology Conference North American ChapterAssociation Computational Linguistics, pp. 4956. Association Computational Linguistics.Petrov, S., & Klein, D. (2007). Improved inference unlexicalized parsing. ProceedingsNAACL HLT 2007, pp. 404411.Puscasu, G. (2007). Wvali: Temporal relation identification syntactico-semantic analysis.Proceedings 4th International Workshop SemEval, pp. 484487.Pustejovsky, J., Hanks, P., Sauri, R., See, A., Gaizauskas, R., Setzer, A., Radev, D., Sundheim, B., Day, D., Ferro, L., & Lazo, M. (2003). timebank corpus. CorpusLinguistics, Vol. 2003, p. 40.Saur, R., Knippen, R., Verhagen, M., & Pustejovsky, J. (2005). Evita: robust event recognizer qa systems. Proceedings conference Human Language TechnologyEmpirical Methods Natural Language Processing, pp. 700707. AssociationComputational Linguistics.Sgaard, A. (2011). Semisupervised condensed nearest neighbor part-of-speech tagging. Proceedings 49th Annual Meeting Association ComputationalLinguistics: Human Language Technologies: short papers, Vol. 2, pp. 4852.Strohman, T., Metzler, D., Turtle, H., & Croft, W. (2005). Indri: language model-basedsearch engine complex queries. Proceedings International ConferenceIntelligent Analysis.Szpektor, I., Tanev, H., Dagan, I., & Coppola, B. (2004). Scaling web-based acquisitionentailment relations. Proceedings EMNLP, Vol. 4, pp. 4148.Tatu, M., & Srikanth, M. (2008). Experiments reasoning temporal relations events. Proceedings 22nd International Conference ComputationalLinguistics-Volume 1, pp. 857864. Association Computational Linguistics.UzZaman, N., & Allen, J. (2010). Trips trios system tempeval-2: Extracting temporalinformation text. Proceedings 5th International Workshop SemanticEvaluation, pp. 276283. Association Computational Linguistics.Verhagen, M., Gaizauskas, R., Schilder, F., Hepple, M., Katz, G., & Pustejovsky, J. (2007).Semeval-2007 task 15: Tempeval temporal relation identification. Proceedings4th International Workshop Semantic Evaluations, pp. 7580. AssociationComputational Linguistics.Verhagen, M., Sauri, R., Caselli, T., & Pustejovsky, J. (2010). Semeval-2010 task 13:Tempeval-2. Proceedings 5th International Workshop Semantic Evaluation, pp. 5762. Association Computational Linguistics.Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. Proceedings 33rd annual meeting Association ComputationalLinguistics, pp. 189196. Association Computational Linguistics.162fiUnsupervised Temporal Relation Learning EventsYeh, A. (2000). accurate tests statistical significance result differences.Proceedings 18th conference Computational linguistics-Volume 2, pp. 947953. Association Computational Linguistics.Yoshikawa, K., Riedel, S., Asahara, M., & Matsumoto, Y. (2009). Jointly identifying temporal relations markov logic. Proceedings Joint Conference 47thAnnual Meeting ACL 4th International Joint Conference NaturalLanguage Processing AFNLP: Volume 1-Volume 1, pp. 405413. AssociationComputational Linguistics.Zhang, M., Zhang, J., Su, J., & Zhou, G. (2006). composite kernel extract relationsentities flat structured features. Proceedings 21stInternational Conference Computational Linguistics 44th annual meetingAssociation Computational Linguistics, pp. 825832. Association Computational Linguistics.163fiJournal Artificial Intelligence Research 45 (2012) 4778Submitted 03/12; published 09/12Tractability CSP Classes Defined Forbidden PatternsDavid A. Cohendave@cs.rhul.ac.ukDepartment Computer ScienceRoyal Holloway, University LondonEgham, Surrey, UKMartin C. Coopercooper@irit.frIRITUniversity Toulouse III, 31062 Toulouse, FrancePaid Creedp.creed@qmul.ac.ukSchool Mathematical SciencesQueen Mary, University LondonMile End, London, UKDaniel Marxdmarx@cs.bme.huComputer Automation Research InstituteHungarian Academy Sciences (MTA SZTAKI)Budapest, HungaryAndras Z. Salamonandras.salamon@ed.ac.ukLaboratory Foundations Computer ScienceSchool Informatics, University Edinburgh, UKAbstractconstraint satisfaction problem (CSP) general problem central computerscience artificial intelligence. Although CSP NP-hard general, considerableeffort spent identifying tractable subclasses. main two approaches considerstructural properties (restrictions hypergraph constraint scopes) relationalproperties (restrictions language constraint relations). Recently, authorsconsidered hybrid properties restrict constraint hypergraph relationssimultaneously.key contribution novel concept CSP pattern classes problemsdefined forbidden patterns (which viewed forbidding generic sub-problems).describe theoretical framework used reason classes problemsdefined forbidden patterns. show framework generalises certain knownhybrid tractable classes.Although close obtaining complete characterisation concerningtractability general forbidden patterns, prove dichotomy special case: classesproblems arise forbid binary negative patterns (generic subproblems disallowed tuples specified). case show (finitesets of) forbidden patterns define either polynomial-time solvable NP-complete classesinstances.c2012AI Access Foundation. rights reserved.fiCohen, Cooper, Creed, Marx & Salamon1. Introductionconstraint satisfaction paradigm consider computational problemsassign values (from domain) variables, constraints. constraintlimits (simultaneous) values list variables (its scope) assigned.typical situation pair variables might represent starting times two jobsmachine shop scheduling problem. reasonable constraint would require minimum timegap values assigned two variables.Constraint satisfaction proved useful modelling tool variety contexts,scheduling, timetabling, planning, bio-informatics computer vision.led development number successful constraint solvers. Unfortunately, solvinggeneral constraint satisfaction problem (CSP) instances NP-hardsignificant research effort finding tractable fragments CSP.principle stratify CSP two quite distinct natural ways. structure constraint scopes instance CSP thought hypergraphvariables vertices, generally relational structure. findtractable classes restricting relational structure, allowing arbitrary constraintsresulting scopes (Dechter & Pearl, 1987). Sub-problems general constraintproblem obtained restrictions called structural. Alternatively, set allowed assignments variables scope seen relation. chooseallow specified kinds constraint relations, allow interact arbitrary structure (Jeavons, Cohen, & Gyssens, 1997). restrictions called relationallanguage-based.Structural subclasses defined specifying set hypergraphs (or relational structures) allowed structures CSP instances. shown tractablestructural classes characterised limiting appropriate (structural) width measures(Dechter & Pearl, 1989; Freuder, 1990; Gyssens, Jeavons, & Cohen, 1994; Gottlob, Leone,& Scarcello, 2002; Marx, 2010a, 2010b). example, tractable structural class binaryCSPs obtained whenever restrict constraint structure (which graphcase) bounded tree width (Dechter & Pearl, 1989; Freuder, 1990). fact,shown that, subject certain complexity-theoretic assumptions, structuresgive rise tractable CSPs bounded (hyper-)tree width (Dalmau,Kolaitis, & Vardi, 2002; Grohe, 2006, 2007; Marx, 2010a, 2010b).Relational subclasses defined specifying set constraint relations. complexity subclass arising restriction precisely determinedcalled polymorphisms set relations (Bulatov, Jeavons, & Krokhin, 2005; Cohen& Jeavons, 2006). polymorphisms specify that, whenever set tuplesconstraint relation, cannot case particular tuple (the result applyingpolymorphism) constraint relation. thus relationship allowed tuples disallowed tuples inside constraint relations key importancerelational tractability given class instances. Whilst general dichotomyyet proven relational case, many dichotomies sub-problemsobtained, instance Bulatov (2003), Bulatov et al. (2005) Bulatov (2006).48fiTractability CSP Classes Defined Forbidden PatternsUsing structural relational restrictions limits possible subclassesdefined. allowing restrictions structure relations ableidentify new tractable classes. call restrictions hybrid reasons tractability.Several hybrid results published binary CSPs (Jegou, 1993; Weigel & Bliek,1998; Cohen, 2003; Salamon & Jeavons, 2008; Cooper, Jeavons, & Salamon, 2010; Cooper &Zivny, 2011b). Instead looking set constraint scopes constraint language,results captured tractability based properties (coloured) microstructureCSP instances. microstructure binary CSP instance graph hV, Ei Vset possible assignments values variables E set pairs mutuallyconsistent variable-value assignments (Jegou, 1993). coloured microstructure,vertices representing assignment variable vi labelled colour representingvariable vi . maintains distinction assignments different variables.coloured microstructure CSP instance captures structurerelations CSP instance natural place look tractable classesneither purely structural purely relational. results (coloured) microstructureproperties, three particular note. First observed class instancesperfect microstructure tractable (Salamon & Jeavons, 2008). propergeneralisation well known hybrid tractable CSP class whose instances allow arbitraryunary constraints every pair variables constrained equal (Regin,1994; van Hoeve, 2001), hybrid class whose microstructure triangulated (Jegou,1993; Weigel & Bliek, 1998; Cohen, 2003). perfect microstructure property excludesinfinite set induced subgraphs microstructure.Secondly, Joint Winner Property (JWP) (Cooper & Zivny, 2011b) applied CSPsprovides different hybrid class also strictly generalises class CSP instancesdisequality constraint (6=) every pair variables arbitrary setunary constraints, forbidding single pattern (a subgraph) colouredmicrostructure. JWP generalized hierarchies soft non-binary constraints (Cooper & Zivny, 2011a), including, example, soft hierarchical global cardinalityconstraints, reduction minimum convex cost flow problem.Thirdly, called broken-triangle property properly extends structural notionacyclicity interesting hybrid class (Cooper et al., 2010). broken triangleproperty specified excluding particular pattern coloured microstructure.notion forbidden pattern study paper. therefore work directlyCSP instance (or equivalently coloured microstructure) rather microstructureabstraction simple graph. allows us introduce language expressinghybrid classes terms forbidden patterns, providing framework searchnovel hybrid tractable classes. case binary negative patterns ablecharacterise tractable (finite sets of) forbidden patterns. also state necessarycondition tractability (finite set of) general patterns.1.1 Contributionspaper generalise definition CSP instance CSP patternthree types tuple constraint relations, tuples explicitly al49fiCohen, Cooper, Creed, Marx & Salamonlowed/disallowed tuples labelled unknown1 . defining natural notioncontainment patterns CSP, able describe problems defined forbiddenpatterns: class CSP instances defined forbidding particular pattern exactlyinstances contain . use framework capture tractabilityidentifying local patterns allowed disallowed tuples (within small groups connectedconstraints) whose absence enough guarantee tractability.Using concept forbidden patterns, lay foundations theoryused reason classes CSPs defined hybrid properties. Since firstwork kind, primarily focus simplest case: binary patterns tupleseither disallowed unknown (called negative patterns). give large class binarynegative patterns give rise intractable classes problems and, using this, shownegative pattern defines tractable class problems must certainstructure. able prove structure also enough guarantee tractabilitythus providing dichotomy tractability defined forbidding binary negative patterns.Importantly, intractability results also allow us give necessary conditionform general tractable patterns.remainder paper structured follows. Section 2 define constraintsatisfaction problems, give definitions used paper. Then, Section 3,define notion CSP pattern describe classes problems defined forbiddenpatterns. give examples tractable classes defined forbidden patterns threevariables. Section 4 show one must take size patterns accountnotion maximal classes defined forbidding patterns. general, yet ablemake conjecture concerning dichotomy hybrid tractability defined generalforbidden patterns. However, Section 5 able give necessary conditionclass tractable Section 6 prove dichotomy negative patterns. Finally,Section 7 summarise results discuss directions future research.2. PreliminariesDefinition 2.1. CSP instance triple hV, D, Ci where:V finite set variables (with n = |V |).finite set called domain (with = |D|).C set constraints. constraint c C pair c = h, where:list distinct variables called scope c.relation arity || called relation c. set tuplesallowed c.solution CSP instance P = hV, D, Ci mapping : V where,h, C s() (where s() represents tuple resulting applicationcomponent-wise list variables ).1. viewed natural generalisation CSP three-valued logic.50fiTractability CSP Classes Defined Forbidden Patternssimplicity presentation, assume variables domains.Unary constraints used impose different domains different variables.arity CSP largest arity constraint scopes. long-termaim identify tractable subclasses CSP problem detectedpolynomial time. paper describe general theory forbidden patternsarbitrary arity consider implications new theory tractable classesarity two (binary) problems specified finite sets forbidden patterns. casescertain class membership decided polynomial time.CSP decision problem, asks whether particular CSP instance solution,already NP-complete binary CSPs. example, straightforward reductiongraph colouring problem set colours used domainCSP instance, vertices graph map CSP variables vi , edges {i, j} mapdisequality constraints vi 6= vj .sometimes convenient paper use equivalent functional formulationconstraint. alternative formulation scope constraint h, abstracted set variables possible assignment seen function f : D.constraint relation alternative view function set possibleassignments, , set {T, F } where, convention, tuples occurconstraint relation map . follows assignment setvariables allowed h, restriction mapped .Definition 2.2. function f : X X, notation f |S meansfunction domain satisfying f |S (x) = f (x) x S.Given set V variables domain D, constraint functional representationpair h, V : {T, F }. CSP instance functional representation triple hV, D, Ci C set constraints functional representation.solution (to CSP instance hV, D, Ci functional representation) mapping: V where, h, C (s| ) = .functional formulation clearly equivalent relational formulationuse whichever seems appropriate throughout paper. choice alwaysclear context.following notions standard study CSP. binary CSP instanceone maximum arity constraint scope two. subproblemvariables U V instance hU, D, CU CU set constraints h, CU . instance arc-consistent v1 , v2 V , solutionsubproblem {v1 } extended solution subproblem {v1 , v2 }.constraint graph binary CSP instance = hV, D, Ci graph verticesV edges set scopes binary constraints C. Since often convenientconsider (possibly irrelevant) constraint exists every pair variables,introduce refined notion true constraint graph.Definition 2.3. binary constraint v1 v2 improper allows every pairvalues allowed unary constraints v1 v2 , proper otherwise.true constraint graph binary CSP instance constraint graphinstance removing improper binary constraints.51fiCohen, Cooper, Creed, Marx & Salamonmay also sometimes need disregard unary constraints following.Definition 2.4. binary reduction CSP instance obtained removingconstraint set constraints whose scope arity two.3. Forbidden Patterns CSPpaper explain define classes CSP instances forbiddingoccurrence certain patterns. CSP pattern generalisation CSP instance.CSP pattern define relations relative three-valued logic {T, F, U }, meaningpattern seen representing set CSP instances undefined value U replaced either F . Forbidding CSP pattern equivalentsimultaneously forbidding instances sub-problems.Definition 3.1. define three-valued logic {T, F, U }, U stands unknownundefined. set {T, F, U } partially ordered U < U < F Fincomparable. Let finite set. k-ary three-valued relation function: Dk {T, F, U }. Given k-ary three-valued relations 0 , say realises 0x Dk (x) 0 (x).extend definition CSP constraint pattern include additional structure set variable names set domain values, set relationsset question. Adding structure makes patterns specific. therefore capture larger, hence interesting, tractable classes. example, domaintotally ordered define tractable max-closed class (Jeavons & Cooper, 1995);independent total order domain variable capturerenamable Horn class (Green & Cohen, 2003); placing order variablespattern allow us define class tree-structured CSP instances.Definition 3.2. CSP pattern quadruple = hV, D, C, Si, where:V set variables, associated relational structure universe V .domain, associated relational structure universe D.C set constraint patterns. constraint pattern c C pair c = h, i,V , scope c, list distinct variables : {T, F, U }three-valued relation (in functional representation) c. constraint patternnon-trivial three-valued relation maps least one tuple {T, F }.structure, set consisting relational structures associatedvariable set domain.arity CSP pattern maximum arity constraint pattern h, .basic type pattern one employs structure, empty.also frequently require patterns use disequality relation 6=, applied every pair52fiTractability CSP Classes Defined Forbidden Patternsspecified subset variables, allow several subsets variablesstructure.paper relations occurring structure arity two, interpretation limited selected binary relations representing disequality partialorder. structure variable set domain clear context,explicitly mention it. Different kinds structure imposed CSP patterns; indeedstructures specified general relations would interesting area future study.weakest structure consider allows us say two variablesdistinct. Thus structure CSP pattern simply set disequalitiessubsets variables. paper denote disequalities NEQ(v1 , . . . , vr ) meaningvariables v1 , v2 , . . . , vr pairwise distinct. pattern structurecalled flat. Indeed, paper mostly concerned flat patterns. twovariables occur together scope constraint pattern, also assumeimplicitly includes disequality NEQ(v1 , v2 ).Thus CSP patterns defined using relational structures three sorts: variables,domain values, variable-value assignments. constraint patterns CSPpattern three-valued relations sort variable-value assignments. CSPpattern flat structure specifies relations sort variables. partial ordervariables also relation sort variables, partial orders domainvalues relations sort domain values.simplicity presentation, assume throughout paper two constraintpatterns C scope (and that, case CSP instances, twoconstraints scope). represent binary CSP patterns simple diagrams. oval represents domain variable, dot domain value. tuplesconstraint patterns value F shown dashed lines, value solidlines value U depicted all.Definition 3.3. constraint pattern h, called negative never takesvalue . CSP pattern negative every constraint pattern negative.3.1 Patterns, CSPs OccurrenceCSP instance implicitly assumed variables domain valuesdistinct. equivalent existence implicit disequalities NEQ variablenames domain values. CSP instance CSP pattern (with structurevariables domain values distinct) three-valued relationsconstraint patterns never take value U . is, decide possible tuplewhether relation not. Furthermore, CSP instance, pairvariables assume constraint exists scope; explicit constraintgiven scope, assume relation complete, i.e. contains tuples.contrasted CSP patterns absence explicit constraintpair variables implies truth value tuple undefined.order define classes CSP instances forbidding patterns, require formaldefinition occurrence (containment) pattern within instance. definegeneral notion containment one CSP pattern within another pattern. Informally,names variables domain elements CSP pattern inconsequential53fiCohen, Cooper, Creed, Marx & Salamoncontainment allows renaming variables domain values variable.Thus, order define containment patterns, firstly require formal definitionrenaming. arbitrary renaming, unless explicitly prohibited disequalitystructure, two distinct variables may map variable two distinct domainvalues may map domain value. Furthermore, pattern occurs another,may use subset variables second pattern; hence notion requireknown renaming-extension.domain labelling set variables assignment domain valuesvariables. Variable domain renaming induces mapping domain labellingsscopes constraints: simply assign renamed domain values renamed variables. natural way extend mapping domain labellings mappingconstraint pattern: truth-value mapped domain labellingtruth-value original domain labelling. However, may occur two domainlabellings scope map domain labelling, instead resulting valuetaken greatest original truth-values. (In order processwell-defined, two domain labellings constraint mapped domain labelling, original truth-values must comparable.) leads followingformal definition renaming-extension first step towards definitioncontainment.Definition 3.4. Let = hV, D, C, Si 0 = hV 0 , D0 , C 0 , 0 CSP patterns.say 0 renaming-extension exist variable-renaming function : V V 0 domain-renaming function : V D0 s,assignment-renaming function F : V V 0 D0 induced (s, t) definedF (hv, ai) = hs(v), t(v, a)i satisfy:constraint pattern h, C, two domain labellings `, `0F (`) = F (`0 ), (`) (`0 ) comparable, F (`) denotesassignment f : s() D0 v , f (s(v)) = t(v, `(v)).C 0 = {hs(), 0 | h, C}, where, assignment f : s() D0 , 0 (f ) = UF (`) 6= f every ` , 0 (f ) = max {(`) | F (`) = f } otherwise.structure, s, F preserve structure. mapping induceshomomorphism relational structures variable-sets, mappinginduces homomorphism relational structures domains. (Inparticular, NEQ(v1 , v2 ) S, s(v1 ) 6= s(v2 ) NEQ(s(v1 ), s(v2 )) 0 .)use patterns define sets CSP instances forbidding occurrence (containment) patterns CSP instances. way able characterisetractable subclasses CSP. Informally, pattern said occur CSP instanceP find sub-problem Q P (formed taking subsets variables domains)realises . Q realises if, renaming variables domain values ,constraint pattern realised corresponding constraint Q. Definition 3.4,renaming-extension, extra variables, domain values disequalities introduced. Thus need combine notions renaming-extension realisationformally define mean pattern occurring another pattern (and, particular,CSP instance).54fiTractability CSP Classes Defined Forbidden PatternsDefinition 3.5. say CSP pattern occurs CSP pattern P = hV, D, C, Si(or P contains ), denoted P , renaming-extension hV, D, C 0 , Siwhere, every constraint pattern h, 0 C 0 constraint pattern h, C and,furthermore, realises 0 .Pattern 1d0bbcbcxzx(i)cx(ii)(iii)Pattern 2bcxExample 3.6. example describes three simple containments. Consider three CSPpatterns, Pattern 1(i)(iii). patterns occur in, contained in, Pattern 2mappings F1 , F2 , F3 , respectively, describe.F1 simply bijection. Although patterns different, valid containmentPattern 1(i) Pattern 2 three-valued relation Pattern 2 realisationthree-valued relation Pattern 1(i): replacing (b, d) 7 U (b, d) 7 F .F2 maps (x, a), (x, b), (y, c) themselves, maps (y, d) (y, d0 )(y, d). merging domain elements possible values three-valuedconstraint relation Pattern 1(ii) comparable tuples involving assignments (y, d)(y, d0 ) and, furthermore, restriction three-valued relation Pattern 1(ii)either two assignments realised three-valued constraint relationPattern 2: (b, d) 7 F (a, d) 7 . example, replacing (a, d0 ) 7 U(a, d) 7 . similar manner, Pattern 1(i) also contained Pattern 2 simplemapping F10 maps (x, b), (x, a) (x, b) (y, c), (y, d) (y, c).Finally, F3 maps (y, c) (y, d) themselves, maps (x, a) (z, b) Pattern 1(iii) (x, a) (x, b), respectively, Pattern 2. merging variables pos55fiCohen, Cooper, Creed, Marx & Salamonsible three-valued relations agree NEQ(x, z) structurePattern 1(iii).Pattern 3zbbzcxcxNEQ(x, z)(i)(ii)Throughout paper, use notation NEQ(v1 , . . . , vr ) denote factvariables v1 , . . . , vr CSP pattern distinct. worth discussing structureimplies far Definition 3.4 concerned. Structure source pattern mustpreserved target pattern. Thus Pattern 1(iii) occurs Pattern 3(i), Pattern 3(i)contained Pattern 1(iii) since structure NEQ(x, z) preserved targetpattern. structure NEQ(v1 , v2 ) considered preserved renaming-extension0 even explicitly given 0 implicit, example, due existencenon-trivial constraint pattern h, 0 v1 , v2 . example, considertwo CSP patterns, Pattern 3(i)(ii). Pattern 3(i) mapped Pattern 3(ii)simple bijection three-valued relation Pattern 3(ii) realisationthree-valued relation Pattern 3(i). structure NEQ(x, z) considered preservedmapping due existence non-trivial constraint pattern variablesx z Pattern 3(ii). Hence, Pattern 3(i) occurs Pattern 3(ii).continuing need define mean say class CSPinstances definable forbidden patterns.Definition 3.7. Let C class CSP instances maximum arity k. sayC definable forbidden patterns set patterns X setCSP instances maximum arity k none patterns X occur preciselyinstances C.Notation: Let X set CSP patterns maximum arity k. use CSP(X )denote set CSP instances element X occurs. X singleton{} use CSP() denote CSP({}).paper, consider classes CSP(X ) sets X CSP patternsbinary sense constraint patterns scope size exactly two.X patterns X binary, CSP(X ) closed arc consistency(in sense arc consistency closure instance CSP(X ) belongs56fiTractability CSP Classes Defined Forbidden PatternsCSP(X )) operation updates unary constraints. Indeed, changingunary constraints cannot introduce patterns X instance CSP(X ).3.2 Tractable Patternspaper define, forbidding certain patterns, tractable subclasses CSP.Furthermore, give examples truly hybrid classes (i.e. classes definablepurely relational purely structural properties).Definition 3.8. finite set patterns X intractable CSP(X ) NP-hard.tractable polynomial-time algorithm solve CSP(X ). single patterntractable (intractable) {} tractable (intractable). (We assume throughout paperP6= NP, therefore sets tractable intractable patterns disjoint.)worth observing classes CSP instances defined forbidding patternsfixed domain. Recall, however, CSP instance finite domain.structure present CSP instance assumed given part instance. particular,variables CSP instance assumed distinct. finite sets patterns X ,number possible renaming-extensions particular instance P polynomialsize P . Hence determine whether instance lies CSP(X ) exhaustivesearch polynomial time.need following simple lemmas proofs intractability results latersections paper.Lemma 3.9. 1 2 2 3 , 1 3 .Proof. 0 , constraint pattern h, maps constraint pattern h 0 , 00 realises . transitivity follows following facts:realisation operation transitive.1 2 2 3 , Definition 3.4, structure 1 preserved 2hence 3 .Lemma 3.10. Let X sets CSP patterns suppose every pattern, pattern X . CSP(X ) CSP(T ).Proof. Let P CSP(X ), 6 P X . cannot P, since would imply exists X P henceP Lemma 3.9. Hence, P CSP(T ).Corollary 3.11. Let X sets CSP patterns suppose every pattern, pattern X .CSP(T ) intractable CSP(X ) intractable conversely,CSP(X ) tractable whenever CSP(T ) tractable.Finally, give examples tractable patterns. first example negativepattern since truth-values relations F U .57fiCohen, Cooper, Creed, Marx & SalamonPattern 4 simple negative pattern.vxcc0wbNEQ(v, w, x)Example 3.12. Consider Pattern 4. defines class CSPs trivially tractable.Forbidding Pattern 4 ensures paths two variables trueconstraint graph. Thus, problem forbidding Pattern 4 decomposed setindependent sub-problems, two variables.Example 3.13. Cooper Zivny (2011b) showed forbidding pattern Negtransshown Pattern 5 describes tractable class CSP instances. seen generalisation well-known tractable class problems, AllDifferent+unary (Costa,1994; Regin, 1994; van Hoeve, 2001): instance class consists set variablesV , set arbitrary unary constraints V , constraint v 6= w defined pairdistinct variables v, w V . Forbidding Negtrans equivalent saying disallowedtuples form transitive relation, i.e. (hv, ai , hx, bi) (hx, bi , hw, ci) disallowed(hv, ai , hw, ci) must also disallowed. Thus Negtrans occur binary CSPinstance class AllDifferent+unary transitivity equality (equalityexactly disallowed).Pattern 5 Negative transitive pattern (Negtrans)vxwNEQ(v, w, x)Cooper Zivny (2011b) also recently showed tractable class definedforbidding Pattern 5 (Negtrans) extended soft constraint problems.58fiTractability CSP Classes Defined Forbidden Patterns3.3 Tractable Patterns Structurepaper primarily studies patterns weak structure conditionsimposed variables distinct. However, worth pointing addingstructure pattern allows us capture larger classes instances. Example 3.14show forbidden pattern capture class CSPs tree width 1adding variable-ordering Pattern 4. case pattern containment must preservetotal order. ordered pattern , consider unordered CSP PCSP() exists ordering variable set P forbidden.order define tractable class, must possible find ordering polynomialtime. case patterns Examples 3.14 3.15.Pattern 6 Tree structure pattern (Tree)v1v3v2v1 < v2 < v3Example 3.14. Consider pattern Tree, given Pattern 6. show classCSP(Tree) exactly set CSPs whose true constraint graph forest (i.e. treewidth 1). First, suppose P CSP(Tree). Then, exists ordering = (v1 , . . . , vn )variable shares proper constraint one variable precedingordering. hand, suppose P CSP whose true constraint graphtree. ordering vertices according pre-order traversal, obtain orderingvariable shares proper constraint one variable precedingordering (its parent); thus, P CSP(Tree).Example 3.15. Forbidding pattern BTP shown Pattern 7 known brokentriangle property (Cooper et al., 2010). order capture class forbiddenpattern impose total order pattern variables. Cooper et al. (2010)proved class CSP instances CSP(BTP) solved polynomial time and,indeed, CSP instances CSP(BTP) unknown total ordering variablesrecognised solved polynomial time.easy see Tree (shown Pattern 6) occurs BTP (with truthvalues U changed ). follows Lemma 3.10 CSP(Tree) CSP(BTP).Hence class CSP(BTP) includes CSP instances whose true constraint graph tree.However, CSP(BTP) also includes certain CSP instances whose true constraint graph59fiCohen, Cooper, Creed, Marx & SalamonPattern 7 Broken triangle pattern (BTP)bv1v3v2v1 < v2 < v3tree width r value r: consider, example, CSP instance r + 1 variablesidentical constraint every pair variables simply disallows singletuple h0, 0i.tractable forbidden pattern order imposed variables,obtain another tractable class considering problems forbidding pattern withoutordering condition. class obtained generally smaller, easier establishcontainment flat pattern. example, consider Pattern 4 flat versionPattern 6. seen forbidding Pattern 4 gives rise class CSP instancespaths length greater two true constraint graph.hand, forbidding Pattern 6 gives much larger class CSP instancestrue constraint graph tree width 1.case broken-triangle property, also obtain strictly smaller tractableclass forbidding Pattern 7 triples variables v1 , v2 , v3 irrespective order.easily exhibit CSP instance shows inclusion strict: example,3-variable CSP instance Boolean domains consisting two constraints v1 = v2 ,v1 = v3 variable ordering v1 < v2 < v3 . unordered version BTPrecently used obtain dichotomy patterns consisting 2 constraints (Cooper &Escamocher, 2012).4. Maximal Tractable Classes Defined Forbidden Patternsrelational tractability define maximal tractable sub-problem CSP problemgiven set possible relations. class relations maximal possibleadd even one relation without sacrificing tractability.case structural tractability picture less clear, since measurecomplexity infinite set hypergraphs (or, generally, relational structures).obtain tractability bound width measure structures. Whateverwidth measure chosen containment class width bounded k insideclass width bounded k +1 maximal class possible (althoughk unique maximal class structurally tractable instances). section,show case forbidden patterns situation similar.60fiTractability CSP Classes Defined Forbidden PatternsDefinition 4.1. Let = hV, D, C, Si = hV 0 , D0 , C 0 , 0 two flat CSP patterns.0 DD0 . Now, extend constraint patternform disjoint unions V V0 setting value tuple including elements D0C domain DD0.U , extend similarly constraint patterns C 0 : way define C C0 forming disjoint union 0 addingAlso define structure0disequalities NEQ(v, v ) v V v 0 V 0 . set disjoint union= hV V0 , DD0 , C C0 ,0 i.Lemma 4.2. Let flat non-empty (i.e. containing least one variable) binaryCSP patterns.).CSP() CSP( ) ( CSP() tractable whenever CSP() CSP( ) tractable.Moreover, CSP(Proof. begin showing strict inclusion).CSP() CSP( ) ( CSP(inclusion holds follows directly Lemma 3.10. Among patternsoccurs, let pattern smallest number variables. define similarly.see inclusion strict, observe occur CSP pattern whosedomain disjoint union , whose variable set size equallarger variable sets . CSP instance containing patternneither CSP() CSP( ). However, construct CSP instance containing), structureimposing disequalitiespattern contained CSP(variables means contained pattern:simply enough variables.). P CSP() CSP( ) P solved polynomialSuppose P CSP(time, tractability CSP() CSP( ).may suppose P . Choose particular occurrence P letdenote set variables used containment. Consider assignment :D. Let Pt denote problem obtained making assignment enforcingarc-consistency resulting problem. corresponds adding new unaryconstraints P .must occur P . see this, observeshow occurs Ptcontainment Pm naturally induces containment P extendsP , considering occurrence . Thus, concludecontainmentPt CSP( ), solved polynomial time.construction, solution Pt extends solution P adding assignmentvariables . Moreover, every solution P corresponds solution Pt: D. Since size fixed, iterate solutions polynomialtime. P solution, find solution Pt . findPt solution, know P solution. Thus, since solvePt polynomial time, also solve P polynomial time.Corollary 4.3. tractable class defined forbidding flat pattern maximal.61fiCohen, Cooper, Creed, Marx & Salamondefined disjointProof. Let tractable flat pattern. Consider patternunion two copies . Lemma 4.2 CSP()tractable also,CSP() ( CSP()hence CSP() maximal tractable class.follows cannot characterise tractable forbidden patterns exhibitingmaximal tractable classes defined forbidding pattern (or finite set patterns,since Lemma 4.2 finite set replaced single pattern). Indeed,consequence Lemma 4.2 construct infinite chain patterns,forbidding one gives rise slightly larger tractable class. Naturally, placeupper bound size patterns finitely many patternsconsider, maximal tractable classes defined forbidden patterns bounded sizenecessarily exist.5. Binary Flat Negative Patternsmoment, able make conjecture concerning complete characterisation complexity general forbidden patterns, although conjecturedichotomy exists. Nonetheless, restricting attention special case, forbiddenbinary flat negative patterns, able obtain dichotomy. Recall patternflat structure imposed variables distinct,negative constraint patterns h, i, never takes value .begin defining three particular patterns one infinite class patterns.use patterns characterise large class intractable patterns. provefinite set flat negative patterns class simple structure: onepatterns must contained one particular set patterns, call pivots.means tractable set patterns must include pattern occurspivot pattern. Furthermore, demonstrate forbidding pivot pattern gives risetractable class. leads simple characterisation tractability finitesets binary flat negative patterns.Pattern 8 Cycle(6)c0cv1v2v3v6v5NEQ(v1 , . . . , v6 )62v4fiTractability CSP Classes Defined Forbidden PatternsPattern 9 Valencyx1x01x2x02x3x03NEQ(x1 , x2 , x3 , x01 ) NEQ(x01 , x02 , x03 )Pattern 10 Pathv1v2v3w1w2w3NEQ(v1 , v2 , v3 , w1 ) NEQ(w1 , w2 , w3 )Definition 5.1 below, define concept neg-connected binary pattern.correspond binary patterns true constraint graph every realisationbinary CSP instance connected graph. first generalise notion trueconstraint graph CSP patterns. call resulting graph negative structure graph.Definition 5.1. Let binary pattern. vertices negative structuregraph G variables . pair vertices edge G formscope whose constraint pattern assigns least one tuple value F . saypattern neg-connected negative structure graph connected. casenegative patterns, use simpler term connected instead neg-connected.Pattern 9 (Valency), Pattern 10 (Path) Pattern 11 (Valency+Path)connected. Note pattern connected may occur connected pattern(and vice versa). Pattern 8 shows Cycle(6) connected. one examplegeneric pattern Cycle(k) k 2. structure Cycle(k)variables distinct, except special case k = 2 structure alsoincludes NEQ(c, c0 ). additional requirement means Cycle(2) composedsingle binary constraint pattern containing two distinct disallowed tuples. followingtheorem uses patterns show patterns intractable.63fiCohen, Cooper, Creed, Marx & SalamonPattern 11 Valency+Pathv1v2w1v3w2w3xNEQ(v1 , v2 , v3 ), NEQ(w1 , w2 , w3 ), NEQ(x, w2 )Theorem 5.2. Let X finite set neg-connected binary patterns. If, X ,least one Cycle(k) (for k 2), Valency, Path, Valency+Path occurs, X intractable.Proof. Let X finite set neg-connected negative binary patterns let `number variables largest element X .Assuming least one four patterns occurs X , constructclass CSPs element X occurs polynomial-timereduction well-known NP-complete problem 3SAT (Garey & Johnson, 1979).construction involve three gadgets, examples shown Figure 1.gadgets serve particular purpose:1. cycle gadget, shown Figure 1(a) special case 4 variables, enforcescycle Boolean variables (v1 , v2 , . . . , vr ) take value.2. clause gadget Figure 1(b) equivalent clause v1 v2 v3 , since vCvalue domain one three vi variables set true.obtain 3-clauses three variables inverting domains vivariables.3. line gadget Figure 1(c), imposes constraint v1 v2 . also usedimpose logically equivalent constraint v2 v1 .cycle gadget connected clause gadget via line gadgets. three typesgadgets specified ensure one negative edge adjacentvertex coloured microstructure, except cycle gadget connected linegadget.Now, suppose instance 3SAT n propositional variablesX1 , . . . , Xn clauses C1 , . . . , Cm .begin construction CSP instance P solve 3SAT instance usingn copies cycle gadget (Figure 1(a)), m(` + 1) variables. = 1, . . . , n,m(`+1)variables along ith copy cycle denoted (vi1 , vi2 , . . . , vi).64fiTractability CSP Classes Defined Forbidden Patternsv1Fv2vCv2v1v3Fv3Fv4(a)(b)Fv1v2(c)Figure 1: (a) Making copies variable (v1 = v2 = v3 = v4 ). (b) Imposingternary constraint vC = v1 v2 v3 . (c) line constraints length 4imposes v1 v2 .solution CSP instance P constraints,variables vij , j = 1, . . . , m(` + 1) must value, di . therefore considervij copy Xi .Consider clause Cw . eight cases consider similarshow details one case. Suppose Cw Xi Xj Xk .build clause gadget (Figure 1(b)) three Boolean variables ciw , cjw ckwinvert domain ckw since occurs negatively Cw . solutionconstructed CSP must satisfy s(ciw ) s(cjw ) s(ckw ) = .complete insertion Cw CSP instance adding line gadgetslength ` + 1 (Figure 1(c)). connect cycle gadgets corresponding Xi , Xj Xkw(`+1)clause gadget clause Cw since Xi , Xj Xk occur Cw . connect viw(`+1)ciw since Xi positive Cw , s(ciw ) = possible s(vi) = ,65fiCohen, Cooper, Creed, Marx & Salamonw(`+1)solution s. Similarly, connect vjcjw . Finally, since Xk occurs negativelyCw , impose line constraints direction. ensures s(ckw ) = Fw(`+1)possible s(vk) = F . Imposing constraints ensures solutionpossible least one cycles corresponding variables Xi , Xj , Xkassigned value would make corresponding literal Cw true.continue construction clause 3SAT instance. Since ` constant,clearly polynomial reduction 3SAT.show CSP instance P constructed manner described cannot contain pattern X . showing neg-connectedpattern containing Cycle(k) (for 2 k `), Valency, Path, Valency+Pathoccur instance. sufficient show CSP instance P containpatterns X .CSP instance P constraint contains one disallowed tuple. Thus,X Cycle(2) cannot occur P . Furthermore, P builtcycles length m(` + 1) paths length ` + 1, cannot contain cycles less` + 1 vertices. Thus, since ` maximum number vertices element X ,follows X Cycle(k) , k 3, occur P .define valency variable x number distinct variables shareconstraint pattern x. Suppose Valency , X neg-connected.possible require variable valency four , pairvariables valency three connected path length ` negative structuregraph . Certainly P variables valency four. Moreover, fact Pbuilt using paths length ` + 1 means two valency three variables joinedpath length `. Thus, X occur P Valency .Next, consider case Path , X neg-connected. musttwo distinct (but possibly overlapping) three-variable lines (with disallowed tuplesconstraint patterns match domain values) separated ` variables.place disallowed tuples meet P connect line gadgetcycle gadget. connection sites always distance greater `,conclude 6 P whenever Path .Finally, consider case Valency+Path , X neg-connected.Here, must variable valency least 3 path constraint patternsthree variables intersecting disallowed tuples, must connected pathless ` variables negative structure graph . observed above,places P disallowed tuples meeting line gadget meetscycle gadget, path least ` variables one pointsevery variable valency 3. Thus, 6 P whenever Valency+Path .remains consider sets negative binary patterns could tractable.this, need define pivot patterns, Pivot(r), contain every tractable negativebinary pattern.Definition 5.3. Let V = {p} {v1 , . . . , vr } {w1 , . . . , wr } {x1 , . . . , xr }, = {a, b}= {NEQ(p, v1 , . . . , vr , w1 , . . . , wr , x1 , . . . , xr )}. define pattern Pivot(r) =66fiTractability CSP Classes Defined Forbidden PatternsPattern 12 Pivot(3)v3v2v1w1w2w3px3x2x1bNEQ(p, v1 , v2 , v3 , w1 , w2 , w3 , x1 , x2 , x3 )hV, D, Cp Cv Cw Cx , Si,Cp = {h(p, v1 ), ab , h(p, w1 ), ab , h(p, x1 ), bb i}Cv = {h(vi , vi+1 ), ab | = 1, . . . , r 1}Cw = {h(wi , wi+1 ), ab | = 1, . . . , r 1}Cx = {h(xi , xi+1 ), ab | = 1, . . . , r 1}ab (a, b) = F , ab (s, t) = U (for (s, t) 6= (a, b)), bb (b, b) = F , bb (s, t) = U(for (s, t) 6= (b, b)). pattern Pivot(r) structure variablesdistinct. See Pattern 12 example, Pivot(3).say pattern variables v1 , . . . , vr distinct-variable patternstructure includes NEQ(v1 , . . . , vr ). following proposition characterises setsconnected binary flat negative distinct-variable patterns Theorem 5.2 proveintractable.Proposition 5.4. connected binary flat negative distinct-variable pattern eithercontains Cycle(k) (for k 3), Valency, Path, Valency+Path,occurs Pivot(r) integer r ||.Proof. Suppose contain patterns Valency, Cycle(k) (for k 3),Path, Valency+Path. Recall valency variable x number distinctvariables share constraint pattern x. Since contain Valencycontain one variable valency three variables must valencytwo. Moreover, since Cycle(k) 6 k 3, negative structure graphcontain cycles. Thus, since connected, negative structure graphconsists three disjoint paths joined single vertex. two disallowed tuples67fiCohen, Cooper, Creed, Marx & Salamondistinct scopes intersect, call union scopes footprintintersection. fact negative structure graph acycliccontain Path means pairs intersecting disallowed tuples mustfootprint. Moreover, fact contain Valency+Path meansintersections must occur variable valency 3, exists. factflat negative means renaming-extension pair disallowed tuples ha, bi,hc, di scope hu, vi merged domain-renaming function t,i.e. t(hu, ai) = t(hu, ci) t(hv, bi) = t(hv, di). follows occurs Pivot(r),r ||.Corollary 5.5. Let X finite set connected binary flat negative distinct-variablepatterns. CSP(X ) tractable X occurs Pivot(r),integer r ||.prove result patterns necessarily distinct-variable.Corollary 5.6. Let X finite set connected binary flat negative patterns.CSP(X ) tractable X occurs Pivot(r), integerr ||.Proof. connected binary flat negative pattern, let dv() denote set connectedbinary flat negative distinct-variable patterns occurs,domain || variables. use dv(X ) denote union sets dv()X .Lemma 3.10, CSP() CSP(dv()). every CSP instance P P ,P dv(). follows CSP(dv()) CSP(), henceCSP() = CSP(dv()). Since CSP(X ) intersection CSP() XCSP(dv(X )) intersection CSP(dv()) X , CSP(X ) =CSP(dv(X )).Corollary 5.5, CSP(dv(X )) tractable pattern dv(),X , occurs Pivot(r) r | |. But, definition dv(), occurs| | ||. Therefore, CSP(X ) tractable occurs Pivot(r) integerr ||.arbitrary (not necessarily flat negative) binary CSP pattern , denoteneg() flat negative pattern obtained replacing truth-valuesU constraint patterns ignoring structure beyond disequalitiesvariables. Recall structure flat pattern contains disequality relationsvariables, neg() flat pattern definition. set patterns X ,neg(X ) naturally defined set neg(X ) = {neg() : X }. Clearly CSP(neg(X ))CSP(X ). following result follows immediately Corollary 5.6. providesnecessary condition tractability general patterns.Corollary 5.7. Let X finite set binary patterns X , neg()connected. CSP(X ) tractable X neg() occursPivot(r), integer r ||.68fiTractability CSP Classes Defined Forbidden Patterns6. Pivot TheoremTheorem 6.1. Pivot(r) tractable r 1.theorem together Corollary 5.6 immediately provides dichotomy finitesets connected binary flat negative patterns. section devoted prooftheorem (which call pivot theorem). conclude section givingdichotomy finite sets flat negative patterns necessarily connected.need definitions graph theory.Definition 6.2. subdivision graph G graph obtained replacing edgesG simple paths.minor graph G graph obtained G deleting edges, contractingedges removing isolated vertices. graph H topological minor graph Gsubdivision H subgraph G.need use following well-known theorem Robertson Seymour (1986).Theorem 6.3. every planar graph H integer k > 0 graphcontain H minor, tree width k.particular, graph large tree width, contains large grid minor.section consider hexagonal grid minors instead (see Figure 2). reasonwell-known fact graph maximum degree three minor another graph,topological minor latter notion convenient proofs.illustrated Figure 2, h hexagonal grid graph composed hexagons honeycombpattern: width h number hexagons horizontal vertical directions.Definition 6.4. Let g : r N every graph tree width least g(r) contains3(r + 4) hexagonal grid topological minor.Let us observe following simple property first:Lemma 6.5. three degree three vertices hexagonal grid width 3r begin disjointpaths length r.Proof. vertex different row simply choose pathalong row (in direction away nearest boundary grid). See verticesa, b c Figure 2 visualise typical situation.Otherwise may possible, rotating grid 120 240 degrees getthree vertices lie different rows. cannot separate vertices rotatingcorners equilateral triangle, x, y, z p, q, r diagram.triangle interior row, Row 4 x, y, z diagram,extend two vertices along row drop third interior rowalong row. Thus, example, path beginning would drop one rowcontinue along Row 4.remaining case three vertices form equilateral triangle occupyingtwo adjacent rows, like p, q, r diagram. case orientationtwo vertices row lie along edge69fiCohen, Cooper, Creed, Marx & Salamongrid. diagram rotate either 120 240 degrees achieve p, q, r.extend two three vertices along row third shift awaycentre triangle order find empty row along pathextended.Row 6bxRow 5Row 4zRow 3cRow 2Row 1Row 0rpqFigure 2: hexagonal grid width 6 rows picked bold numbered.following combinatorial result crucial algorithm, interestingright:Lemma 6.6. Let G 3-connected graph tree width least g(r) let a, b, cdistinct vertices G. G contains 3 pairwise vertex disjoint paths starting a, b, c,respectively, length r.Proof. Let H 3(r + 4) hexagonal grid. definition g(r), graph G containsH topological minor. Note H contains vertices degree 2 boundary,cause complications proof. avoid complication, observeH subdivision graph H whose every vertex degree 3 focusgraph H instead.Let HGdenote subdivision H appearing G let denote verticesdegree three HG. Mengers theorem (Dirac, 1966) vertex-disjoint paths Pa ,Pb , Pc G a, b, c distinct vertices sa , sb , sc S, respectively. Choose pathsway total number edges used HGminimized.Let x, two vertices correspond adjacent vertices H . meansHGcontains path Q endpoints x whose internal vertices disjointS. Suppose that, say, Pa contains internal vertex Q. claim either (1)x, {sa , sb , sc } (2) sa {x, y} Pb , Pc disjoint Q. Suppose (1)hold, say, x 6 {sa , sb , sc }. Consider internal vertex q Q closest x used70fiTractability CSP Classes Defined Forbidden Patternsone paths. reroute path q x without using edgesoutside HG. would create new set paths smaller number edges outsideHG , unless rerouted path use edges outside HGq.possible path goes q Q. means one pathintersecting Q: path intersects Q q x definition qpath uses vertices q y. Thus case (2) holds.Lemma 6.5 three independent paths length r + 4 vertices sa , sbsc (non subdivided) hexagonal grid H, correspond paths Xa , Xb , XcHG. use paths create three independent paths length least r a, bc G. definition, path Xa go sb sc . Therefore, claimprevious paragraph, Xa disjoint Pb Pc : Xa uses path Q xy, sb , sc 6 {x, y} means neither (1) (2) happen Pb Pc intersectsQ. Xa disjoint Pa well, create new path Ta simply concatenatingPa Xa . Otherwise, way Xa intersect Pa first subdivided edgeH Xa goes (this place case (2) claim happen).case, create new path Ta following Pa meets subdivided edgefollowing Xa . edge H corresponds 4 edges H, path Ta couldmeet 4 edges H fewer Xa does. path Ta will, either case, meetleast r subdivided edges H length least r. build Tb Tcanalogous fashion.require following technical lemma proofs.Lemma 6.7. Let P binary CSP instance. Suppose assignment x,d0 extend solution P solution assigning xd0 y. path proper binary constraints x y. Furthermore,path first constraint along path disallows tuple hd, d1last constraint disallows tuple hd2 , d0 i.Proof. Let Sx solution P including assignment x similarly let Sysolution P including assignment d0 y.Define graph G variables P . edge x variable zassignment x incompatible domain value z. Similarly,edge variable z assignment d0 incompatibledomain value z. Finally edge two variablesx constraint proper.Let Cx component G containing x. Define assignment variablesP setting S(z) = Sx (z) z Cx S(z) = Sy (z) otherwise. solutionP since possible unsatisfied constraint would variable Cxvariable Cx , choice Cx cannot happen.hypothesis, know S(y) 6= d0 required path properbinary constraints.Note Lemma 6.7 binary constraint x forbidsassigning x d0 y, setting d1 = d0 d2 = yields required pathproper binary constraints, length one.first show CSP(Pivot(r)) tractable special case restricted structure.71fiCohen, Cooper, Creed, Marx & SalamonLemma 6.8. subclass CSP(Pivot(r)) consisting instances:arc-consistent binary reduction;unary constraints variables degree two constraint graph;true constraint graph subdivided three-connected graph,time complexity n3 dg(r)+1 .Proof. Let P instance satisfying conditions lemma. time nd2 ,join binary constraints along subdivided edge eliminating intermediate variablesgo, obtain instance P 0 , whose constraint graph three-connected,may improper binary constraints arbitrary unary constraints. Let G denotetrue constraint graph P G0 three-connected graph obtained Gcontracting subdivided edges. true constraint graph P 0 subgraph G0vertex-set.solve P 0 CSP(Negtrans) time n2 d3 (n + d) (Cooper & Zivny, 2011b).clearly n3 dg(r)+1 since g(r) 3 r. Furthermore, G0 tree widthg(r) P 0 solved time ndg(r)+1 (Dechter & Pearl, 1989). eithercase solution extended solution original instance P time O(nd).remaining case consider Negtrans occurs P 0 treewidth G0 also least g(r). complete proof deriving contradictionP 6 CSP(Pivot(r)), order show case cannot occur.Suppose Negtrans occurs P 0 variables a, b, c values da , db , dc :dadccdbbLemma 6.6, G0 contains 3 vertex-disjoint paths Ta , Tb , Tc starting a, b, c, respectively, length least r. Recall true constraint graph Goriginal instance P subdivided three-connected graph P 0 obtainedP joining binary constraints along subdivided edges. Let Ta , Tb , Tc denote pathsG corresponding Ta , Tb , Tc G0 . Recall also Negtrans occurs P 0 variablesa, b, c values da , db , dc .let c first vertices along subdivided edges b cG. embedding Negtrans P 0 shows hdb , da disallowed joinarc-consistent path b a. Since path is, construction, subdivisionedge P 0 know unary constraints occur internal vertices. also know,arc consistency binary constraints, assignments = da b = dbextend consistent assignment path b. So, Lemma 6.7 knowvalue da domain hdb , da disallowed P72fiTractability CSP Classes Defined Forbidden Patternsconstraint b a. Similarly value dc hdb , dc disallowed Pconstraint c. appending path b path Tapath b c Tc , together Tb , obtain three independent paths lengthleast r proper constraints P , beginning variable b, two beginning constraintsdisallowing tuple value db b. shown Pivot(r) indeed occurP done.CSP(Pivot(r)) places upper bound length chain dependenciesmay followed discard partial solution cannot extended solution.Informally speaking, forbidding Pivot(r) pattern bounds amount local searchmay done extending partial solution larger partial solution.amount effort may required increases length chains inference,worst-case behaviour quantified precisely following result. firstrequire definitions.Definition 6.9. Let G graph U subset vertices G. induced graphG[U ] G U graph vertex set U whose edges edges Gconnect two vertices U .graphs G = hV, Ei G0 = hV 0 , E 0 define G G0 = hV V 0 , E E 0 i.graph G say hU1 , U2 separation G = G[U1 ] G[U2 ] neitherU1 , U2 subset other. separator separation hU1 , U2 U1 U2order |U1 U2 |. minimal separator one minimal order.torso U1 separation hU1 , U2 obtained induced graph G[U1 ]adding every edge vertices separator hU1 , U2 i.Theorem 6.10. class Pivot(r)-free instances solvable time n3 dg(r)+3 .Proof. prove result induction number variables.base case straightforward. instance fewer g(r)+3 variables,clearly solve exhaustive search time n3 dg(r)+3 . inductive caseassume solve smaller instances n < k variables time n3 dg(r)+3 .Let P Pivot(r)-free instance n = k variables. First make P arc-consistenttime O(n2 d2 ) (Bessiere, Regin, Yap, & Zhang, 2005). Since P arc-consistent, unaryconstraints longer effect. Remove unary improper binary constraintsP time O(n2 d2 ). Let G true constraint graph resulting instance, P 0 .G separation order two either three-connected threevertices. three-connected case solve P 0 , hence P , time n3 dg(r)+1Lemma 6.8. P three variables trivial solve time O(d3 ).So, assume G separation order two. definition torsosize-2 separator torso size-2 separator G. Hence find size-2 separationhU1 , U2 torso U1 separation order two. assumetorso U1 either three-connected three vertices.consider separator = U1 U2 hU1 , U2 i. empty P 0 composed two smaller independentPivot(r)-free instances solved time3g(r)+33g(r)+3n1+ n2n1 + n2 = n 1 n1 , n2 < n. followssolve P time n3 dg(r)+3 , done.73fiCohen, Cooper, Creed, Marx & Salamon= {m} consider structure G[U1 ]. three-connected,vertex degree least three. case add unary constraintand, Lemma 6.8, solve instance time n3 dg(r)+1 . Hence find, timedn3 dg(r)+1 , values variable extend variables U1 . Adding restriction unary constraint variable leaves induced instance U2 Pivot(r)-freesee, induction, solve P 0 time dn3 dg(r)+1 + (n 1)3 dg(r)+3 =n3 dg(r)+3 , done.Finally must consider = {x, y}. Since minimal know G[U2 ]connected path x U2 . Denote Q CSP instanceinduced G[U1 ], together path U2 x y. constraint graphQ subdivision torso U1 either three-connected threevertices. latter case Q tree width two so, addition unaryconstraints x y, solved time O(n2 d3 ) (Dechter & Pearl, 1989). torsoU1 three-connected degrees x Q least three.additionunary constraints x can, Lemma 6.8, solve case timen3 dg(r)+1 . Hence solve Q possible unary constraints xallow one value x y, time d2 n3 dg(r)+1 .value variable x know whether extends solutionvariables Q. Similarly, value variable know whether extendssolution variables Q. express two restrictions unary constraints,u(x) u(y) x y. Lastly find binary constraint c(x, y) xspecifies precisely pairs values, allowed u(x) u(y), extend variablesU1 . obtain solving subdivided three-connected instance seeingpairs disallowed subdivided edge U2 pairs set constraintrelation c(x, y) F .u(x) allows values x P solution stop.consider instance R, induced P 0 U2 together constraintsu(x), u(y) c(x, y). construction, P solution R solution.Pivot(t) occurring R must use pair values disallowed c(x, y) since cannotoccur P 0 . Suppose hd, d0 disallowed c(x, y). follows assignmentx, d0 extend solution Q assigning x d0extend solution problem induced P 0 U1 . Lemma 6.7 pathproper constraints x G[U1 ]. Furthermore, first constraint alongpath disallows tuple hd, d1 last constraint disallows tuple hd2 , d0 i.follows cannot embed Pivot(r) instance R induced U2 togetherconstraint c(x, y) (otherwise would able embedinstance P ).Since R CSP(Pivot(r)), solve time n3 dg(r)+3 inductive hypothesis. Thus, final case, complexity d2 n3 dg(r)+1 + n3 dg(r)+3 = n3 dg(r)+3done.Theorem 6.1 important gives us tractable class CSPs defined forbiddingnegative pattern which, unlike CSP(Tree), contains problems unbounded tree width,cannot captured structural tractability. true even Pivot(1).example class CSP instances CSP(Pivot(1)) unbounded tree width,consider n-variable CSP instance Pn domain {1, . . . , n} whose constraint graph74fiTractability CSP Classes Defined Forbidden Patternscomplete graph and, pair distinct values i, j {1, . . . , n}, constraintvariables vi , vj disallows single pair assignments (hvi , ji , hvj , ii). Since assignment hvi , ji occurs single disallowed tuple, Pivot(1) occur Pn , hencePn CSP(Pivot(1)). produce example class instances CSP(Pivot(1))unbounded tree width CSP(Negtrans), modify Pnintroducing Boolean variable vij pair < j replacing constraintvariables vi , vj constraints vi , vij vj , vij : former disallowing single pairassignments (hvi , ji , hvij , 0i) latter pair assignments (hvj , ii , hvij , 0i).pattern Negtrans occurs triple assignments (hvi , ji , hvij , 0i , hvj , ii).dichotomy finite sets connected binary flat negative patterns followsdirectly Theorem 6.1 Corollary 5.6.Theorem 6.11. Let X finite set connected binary flat negative patterns. Xtractable X contained Pivot(r), integerr ||.Informally speaking, dichotomy states bounding length problematicPivot(r)-style inference chains leads tractability, moreover classinstances defined finite set forbidden flat patterns tractable, must avoidproblematic inference chains form.dichotomy easily extends patterns necessarily connected.negative pattern connected, decomposed connected patterns corresponding connected components negative structure graph . callpatterns connected components .Corollary 6.12. Let X finite set binary flat negative patterns. X tractableX , connected components containedPivot(r), integer r ||.Proof. Let X finite set binary flat negative patterns. Let CC() represent setconnected components pattern , CC(X ) union sets CC() ( X ).Suppose X tractable. Consider arbitrary subset X 0 CC(X )set X 0 contains exactly one connected component pattern X . Lemma 3.10CSP(X 0 ) CSP(X ), hence X 0 also tractable. Therefore, Corollary 5.6,pattern 0 X 0 occurs Pivot(r), integer r |0 |. waytrue possible choices X 0 X connectedcomponents occur Pivot(r), integer r ||.hand, suppose X , connected componentsX occurs Pivot(r), r ||. Let k number connected components. occurs disjoint union k copies Pivot(r). tractableTheorem 6.1 k 1 applications Lemma 4.2. follows , hence X ,tractable.7. Conclusionpaper described framework identifying classes CSPs terms forbiddenpatterns, used tool identifying tractable classes CSP. gave severalexamples small patterns used define tractable classes CSPs.75fiCohen, Cooper, Creed, Marx & Salamonsearch general result, restricted special case binarypatterns binary CSPs. Theorem 5.2 showed CSP(X ) NP-hard everypattern set X contains least one four patterns (Patterns 8, 9, 10, 11). Moreover,showed binary flat negative pattern contain patternsmust contained within (possibly several copies of) special type pattern calledpivot. Hence, contained (several copies of) pivot necessary conditionpattern tractable. showed forbidding pivot pattern definestractable class.Beyond dichotomy binary flat negative patterns, interesting seenew tractable classes defined general binary patterns non-binarypatterns. particular, important area future research determining maximaltractable classes problems defined patterns fixed size (given numbervariables number variable-value assignments). avenue future researchcharacterisation complexity patterns involving structure usesdisequalities groups variables, total ordering variables.Acknowledgmentsauthors acknowledge support ANR Project ANR-10-BLAN-0210, EPSRC grantsEP/F011776/1 EP/I011935/1, ERC Starting Grant PARAMTIGHT (No. 280152),EPSRC platform grant EP/F028288/1.ReferencesBessiere, C., Regin, J.-C., Yap, R. H. C., & Zhang, Y. (2005). optimal coarse-grainedarc consistency algorithm. Artificial Intelligence, 165 (2), pp. 165185. doi:10.1016/j.artint.2005.02.004.Bulatov, A., Jeavons, P., & Krokhin, A. (2005). Classifying complexity constraintsusing finite algebras. SIAM Journal Computing, 34 (3), pp. 720742. doi:10.1137/S0097539700376676.Bulatov, A. A. (2003). Tractable conservative constraint satisfaction problems. LICS 03:Proceedings 18th IEEE Symposium Logic Computer Science, pp. 321330.doi:10.1109/LICS.2003.1210072.Bulatov, A. A. (2006). dichotomy theorem constraint satisfaction problems 3element set. Journal ACM, 53 (1), pp. 66120. doi:10.1145/1120582.1120584.Cohen, D., & Jeavons, P. (2006). complexity constraint languages. Rossi et al.(Rossi et al., 2006), chap. 8, pp. 245280.Cohen, D. A. (2003). new class binary CSPs arc-consistency decisionprocedure. CP 03: Proceedings 9th International Conference PrinciplesPractice Constraint Programming, No. 2833 Lecture Notes ComputerScience, pp. 807811. Springer-Verlag. doi:10.1007/978-3-540-45193-8_57.Cooper, M. C., & Escamocher, G. (2012). Dichotomy 2-Constraint Forbidden CSPPatterns. AAAI 12: Proceedings Twenty-Sixth AAAI Conference Ar76fiTractability CSP Classes Defined Forbidden Patternstificial Intelligence. Available from: https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/4960/5225.Cooper, M. C., Jeavons, P. G., & Salamon, A. Z. (2010). Generalizing constraint satisfactiontrees: Hybrid tractability variable elimination. Artificial Intelligence, 174 (910), pp. 570584. doi:10.1016/j.artint.2010.03.002.Cooper, M. C., & Zivny, S. (2011a). Hierarchically nested convex VCSP. CP 11: Proceedings 17th International Conference Principles Practice ConstraintProgramming, pp. 187194. Springer-Verlag. doi:10.1007/978-3-642-23786-7_16.Cooper, M. C., & Zivny, S. (2011b). Hybrid tractability valued constraint problems.Artificial Intelligence, 175 (910), pp. 15551569. doi:10.1016/j.artint.2011.02.003.Costa, M.-C. (1994). Persistency maximum cardinality bipartite matchings. OperationsResearch Letters, 15 (3), pp. 143149. doi:10.1016/0167-6377(94)90049-3.Dalmau, V., Kolaitis, P. G., & Vardi, M. Y. (2002). Constraint satisfaction, boundedtreewidth, finite-variable logics. CP 02: Proceedings 8th International Conference Principles Practice Constraint Programming, No. 2470Lecture Notes Computer Science, pp. 310326. Springer-Verlag. doi:10.1007/3-540-46135-3_21.Dechter, R., & Pearl, J. (1987). Network-based heuristics constraint-satisfaction problems. Artificial Intelligence, 34 (1), pp. 138. doi:10.1016/0004-3702(87)90002-6.Dechter, R., & Pearl, J. (1989). Tree clustering constraint networks. Artificial Intelligence, 38 (3), pp. 353366. doi:10.1016/0004-3702(89)90037-4.Dirac, G. A. (1966). Short proof Mengers graph theorem. Mathematika, 13 (1), pp.4244. doi:10.1112/S0025579300004162.Freuder, E. C. (1990). Complexity K-Tree Structured Constraint Satisfaction Problems.AAAI 90: Proceedings Eighth National Conference Artificial Intelligence,pp. 49. Available from: http://www.aaai.org/Library/AAAI/1990/aaai90-001.php.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryNP-Completeness. W. H. Freeman, San Francisco, CA.Gottlob, G., Leone, N., & Scarcello, F. (2002). Hypertree decompositions tractablequeries. Journal Computer System Sciences, 64 (3), pp. 579627. doi:10.1006/jcss.2001.1809.Green, M. J., & Cohen, D. A. (2003). Tractability approximating constraint languages.CP 03: Proceedings 9th International Conference Principles PracticeConstraint Programming, Vol. 2833 Lecture Notes Computer Science, pp. 392406. Springer-Verlag. doi:10.1007/978-3-540-45193-8_27.Grohe, M. (2006). structure tractable constraint satisfaction problems. MFCS 06:Proceedings 31st Symposium Mathematical Foundations Computer Science, Vol. 4162 Lecture Notes Computer Science, pp. 5872. Springer-Verlag.doi:10.1007/11821069_5.77fiCohen, Cooper, Creed, Marx & SalamonGrohe, M. (2007). complexity homomorphism constraint satisfaction problemsseen side. Journal ACM, 54 (1), pp. 124. doi:10.1145/1206035.1206036.Gyssens, M., Jeavons, P. G., & Cohen, D. A. (1994). Decomposing constraint satisfactionproblems using database techniques. Artificial Intelligence, 66 (1), pp. 5789. doi:10.1016/0004-3702(94)90003-5.Jeavons, P., Cohen, D., & Gyssens, M. (1997). Closure properties constraints. JournalACM, 44 (4), pp. 527548. doi:10.1145/263867.263489.Jeavons, P. G., & Cooper, M. C. (1995). Tractable constraints ordered domains. ArtificialIntelligence, 79 (2), pp. 327339. doi:10.1016/0004-3702(95)00107-7.Jegou, P. (1993). Decomposition domains based micro-structure finiteconstraint-satisfaction problems. AAAI 93: Proceedings Eleventh National Conference Artificial Intelligence, pp. 731736. Available from: http://www.aaai.org/Library/AAAI/1993/aaai93-109.php.Marx, D. (2010a). beat treewidth?. Theory Computing, 6 (1), pp. 85112.doi:10.4086/toc.2010.v006a005.Marx, D. (2010b). Tractable hypergraph properties constraint satisfaction conjunctive queries. STOC 10: Proceedings 42nd ACM symposium Theorycomputing, pp. 735744. ACM. doi:10.1145/1806689.1806790.Regin, J.-C. (1994). filtering algorithm constraints difference CSPs. AAAI 94:Proceedings Twelfth National Conference Artificial Intelligence, Vol. 1, pp.362367. Available from: http://www.aaai.org/Library/AAAI/1994/aaai94-055.php.Robertson, N., & Seymour, P. D. (1986). Graph minors. V. Excluding planar graph. Journal Combinatorial Theory, Series B, 41, pp. 92114. doi:10.1016/0095-8956(86)90030-4.Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006). Handbook Constraint Programming.Foundations Artificial Intelligence. Elsevier.Salamon, A. Z., & Jeavons, P. G. (2008). Perfect constraints tractable. CP 08:Proceedings 14th International Conference Principles Practice Constraint Programming, Vol. 5202 Lecture Notes Computer Science, pp. 524528.Springer-Verlag. doi:10.1007/978-3-540-85958-1_35.van Hoeve, W. J. (2001). alldifferent Constraint: Survey. Proceedings 6thAnnual Workshop ERCIM Working Group Constraints. Available from:http://arxiv.org/abs/cs/0105015v1.Weigel, R., & Bliek, C. (1998). reformulation constraint satisfaction problems.ECAI 98: Proceedings 13th European Conference Artificial Intelligence, pp.254258.78fi