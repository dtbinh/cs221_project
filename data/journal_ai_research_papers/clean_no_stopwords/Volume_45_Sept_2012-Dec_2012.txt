Journal Artificial Intelligence Research 45 (2012) 565600

Submitted 6/12; published 12/12

Replanning Domains Partial Information Sensing Actions
Ronen I. Brafman

BRAFMAN @ CS . BGU . AC . IL

Department Computer Science
Ben-Gurion University Negev

Guy Shani

SHANIGU @ BGU . AC . IL

Department Information Systems Engineering
Ben-Gurion University Negev

Abstract
Replanning via determinization recent, popular approach online planning MDPs.
paper adapt idea classical, non-stochastic domains partial information
sensing actions, presenting new planner: SDR (Sample, Determinize, Replan). step
generate solution plan classical planning problem induced original problem.
execute plan long safe so. longer case, replan.
classical planning problem generate based translation-based approach conformant
planning introduced Palacios Geffner. state classical planning problem generated
approach captures belief state agent original problem. Unfortunately,
method applied planning problems sensing, yields non-deterministic planning
problem typically large. main contribution introduction state sampling
techniques overcoming two problems. addition, introduce novel, lazy, regressionbased method querying agents belief state run-time. provide comprehensive
experimental evaluation planner, showing scales better state-of-the-art CLG
planner existing benchmark problems, also highlighting weaknesses new domains.
also discuss theoretical guarantees.

1. Introduction
many real world scenarios agent must complete task required features
unknown, observed special sensing actions. Consider example Mars rover
must collect rock samples (Smith & Simmons, 2004). rover know
rocks surrounding contains interesting mineral, move closer rocks
activate sensor detects minerals. domains modeled planning
partially observability sensing actions (PPOS).
Planning partial observability sensing actions one hardest problems
automated planning. difficulty stems large number contingencies occur,
need take account planning. address contingencies, planner
must generate conditional plan, plan tree, rather linear plan. plan tree grow
exponentially number propositions problem description, making offline generation
complete plan tree impossible even moderately complex problems. difficulty
overcome, extent, using online planner generates next action only, given
current state. One technique online planning replanning (Zelinsky, 1992), made popular
FF-replan planner (Yoon, Fern, & Givan, 2007). replanning, state, agent finds
c
2012
AI Access Foundation. rights reserved.

fiB RAFMAN & HANI

plan based partial, possible inaccurate model, using simpler planning problem. executes
prefix it, replanning new information arrives.
key component replanning algorithm method generating solving
simpler problems. Recent replanners focus generating fully-deterministic classical planning
problems solving using off-the-shelf (Yoon et al., 2007), modified (Albore, Palacios, &
Geffner, 2009) classical planners. approach particularly beneficial given large array existing classical planners, ability immediately enjoy progress made intensively
studied area. However, still leaves open key question generate appropriate classical planning problem given agents current state. context probabilistic planning
full observability, current planners use multiple samples classical planning problems obtained
transforming stochastic actions deterministic actions selecting single effect action
instance (Yoon et al., 2007; Yoon, Fern, Givan, & Kambhampati, 2008; Kolobov, Mausam, & Weld,
2010).
context PPOS sophisticated translation scheme introduced CLG
planner (Albore et al., 2009). translation based techniques introduced Palacios
Geffner (2009) solving conformant planning representing agents belief state within
classical planners state. achieved extending language propositions form
Kp, Kp, denoting fact agent knows p true false, respectively.
ideas extended PPOS, result non-deterministic planning problems effect
sensing action agents belief state cannot known offline. CLG handles problem
relaxing problem number ways, using complex translation. key
aspect translation that, action precondition p, value p
sensed, CLG plan as-if p true (and thus, executed), provided action
senses p executed a. Thus, makes optimistic assumptions regarding future outcomes
sensing actions. This, however, imply CLG actually execute a,
actual outcome differs expected one, CLG replan using new information.
SDR planning algorithm propose paper follows similar high-level approach
based replanning. state, generate classical planning problem reflects information agents belief state. specifics approach, are, however bit different,
main aim provider better scalability, requires generating smaller classical planning
problems. achieve using state sampling. is, instead planning possible
initial states, sample small subset states, plan possible initial
states. also use sampling remove optimistic bias CLG: sample arbitrary initial
state sI , assume observation values obtained sI true initial state.
use sampling leads much smaller classical planing problems, hence, better scalability.
planner operates assumptions alway true (i.e., considers
subset initial states one possible set observations), possible preconditions
actions selected, well goal, hold possible worlds. Thus, agent must
maintain representation current set possible states, also known belief state,
order verify conditions. many methods maintaining belief state (Albore
et al., 2009; To, Pontelli, & Son, 2011; To, Son, & Pontelli, 2011a; To, Pontelli, & Son, 2009),
work well problems certain structure, well problem structures.
general, belief state maintenance difficult, use belief state limited,
suggest lazy belief state querying mechanism spirit CFF (Hoffmann & Brafman,
2006) require explicit representation update belief state following
566

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

action. maintain symbolic representation initial belief state, only. determine
literal holds current belief state, regress literal history actions
observations, check consistency regressed formula initial belief state.
augment regression process caching mechanism, call partially-specified states,
allows us keep regressed formulas compact.
resulting planner SDR (Sample, Determinize, Replan) compares favorably CLG
(Albore et al., 2009), current state-of-the-art contingent planner. existing
benchmarks generates plans faster solve problems CLG cannot solve, plans
similar slightly worse size.
paper contains comprehensive experimental evaluation aimed identifying strengths
especially weaknesses SDR replanning approach. end, formulated
number new benchmark domains, including domains sensing requires performing actions
path goal, domains dead-ends. addition, also evaluate
effectiveness new regression-based method maintaining information belief state
comparing closest lazy approach CFF (Hoffmann & Brafman, 2006). Finally,
describe theoretical guarantees associated SDR. First, show translation
scheme use sound complete whenever sampled initial state true initial state.
Then, show that, certain assumptions connectivity domain, SDR
complete description initial belief state reach goal, goal reachable.
paper organized follows: next section describe problem contingent
planning partial observability sensing. Then, describe idealized version
SDR planner ignores efficiency problems arise belief state large uses
entire belief state generate classical planning problem. provide theoretical analysis
correctness convergence properties idealized algorithm. Next, describe full
SDR algorithm. algorithm uses state sampling manage size belief state well
regression mechanism querying belief state. followed overview comparison
related work, followed empirical evaluation analyzing strengths weakness SDR.

2. Problem Definition
focus planning problems partial observability sensing actions (PPOS). shall
assume actions deterministic throughout paper.
Formally, PPOS problems described quadruple: P, A, , G, P set
propositions, set actions, propositional formula P describes set
possible initial states, G P set goal propositions. follows often abuse
notation treat sets literals conjunction literals set, well assignment
values propositions appearing set. example, {p, q} also treated p q
assignment true p false q.
state world, s, assigns truth value elements P , usually represented
using closed-world assumption via set propositions assigned true s. belief-state
set possible states, initial belief state, bI = {s : |= } defines set states
possible initially.
action, A, three-tuple, {pre(a),effects(a),obs(a)}. action preconditions, pre(a),
set literals must valid action executed. action effects, effects(a),
567

fiB RAFMAN & HANI

set pairs, (c, e), denoting conditional effects, c set (conjunction) literals e
single literal.
Finally, obs(a) set propositions, denoting propositions whose value observed
following execution a. assume consistent, is, (c, e) effects(a)
cpre(a) consistent, (c, e), (c , e ) effects(a) |= c c state s,
e e consistent.
current benchmark problems, either set effects set obs empty. is, actions
either alter state world provide information, pure sensing actions
alter state world, reason case general.
use a(s) denote state obtained executed state s.
satisfy literals pre(a) a(s) undefined. Otherwise, a(s) assigns proposition p
value s, unless exists pair (c, e) effects(a) |= c e assigns p
different value s. sequence actions, use a(s) denote resulting state, defined
analogously.
Observations affect agents belief state. assume observations deterministic
accurate, reflect state world prior execution action.1 Thus,
p obs(a) agent observe p p holds (i.e., prior effect), otherwise
observe p. Thus, true state world, b current belief state
agent, ba,s , belief state following execution state defined as:
ba,s = {a(s )|s b, agree obs(a)}
is, progression states agent would receive, following execution a, observation state s. Extending definition sequences
actions, sequence actions, ba,s denotes belief state reached b executing
starting state b.
Alternatively, define belief progression without explicit world state using:
ba,o = {a(s )|s b, observation world state o}.
contingent plan PPOS problem annotated tree = (N, E). nodes, N ,
labeled actions, edges, E, labeled observations. node labeled action
observations single child, edge leading labeled null observation
true. Otherwise, node one child possible observation value, i.e., one child
possible assignment observed propositions. edge leading child labeled
corresponding observation. plan executed follows: action root executed,
observation (possibly null) made, execution continues recursively child
corresponds edge labeled observation. assume actions observations
deterministic, single possible execution path along tree initial state.
use (s) denote state obtained executed starting state s. solution plan (plan
short) PPOS P = P, A, , G (s) |= G every |= .
Complete contingent plans consider possible future observations, prohibitively
large problems practical interest. paper, thus, concerned online planning
PPOS. is, stage, planner selects next action execute, executes it,
reconsiders.
1. One choose observations reflect state world following execution action
price slightly complicated notation below.

568

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

Figure 1: 4 4 Wumpus Domain

Example 1. illustrate definitions using 4 4 simplified Wumpus domain (Albore et al.,
2009), serve running example. domain, illustrated Figure 1, agent
navigates 4 4 grid bottom-left corner top-right corner (the goal) moving
four directions. squares around top two squares diagonal grid
contain monsters called Wumpus one every pair squares adjacent diagonal (e.g.,
theres either Wumpus square 3,4 4,3). agent move square safe.
is, contains Wumpus, specified precondition move action. Thus, agent
never enter square Wumpus die, dead-ends domain.2
initial state agent know location Wumpuses, directly observe
location. However, Wumpus emits stench drifts adjacent squares. Hence,
agent adjacent square Wumpus smell stench, although cannot determine
adjacent square Wumpus hiding. Thus, measurements different locations may
required determine precise position Wumpus. domain demonstrates complex
hidden state multiple sensing actions, conditional effects.
formalize problem follows:
set propositions at-x-y 1 x, 4, wumpus-at-2-3, wumpus-at-3-2, wumpusat-3-4, wumpus-at-4-3 stench-at-x-y 1 x, 4.
actions move-from-x1 -y1 -to-x2 -y2 adjacent x1 , y1 , x2 , y2 pairs, smell.
initial state is: at-1-1 at-1-2 at-4-4 (oneof wumpus-at-2-3 wumpus-at-3-2)
(oneof wumpus-at-4-3 wumpus-at-3-4).
2. Later on, introduce natural formalization domain require square safe move
it, thus, contains dead-ends.

569

fiB RAFMAN & HANI

goal at-4-4.
initial belief state consists four possible world-states, corresponding four possible truth assignments (wumpus-at-2-3 wumpus-at-3-2) (wumpus-at-4-3 wumpus-at3-4) addition known literals, at-1-1 adjacency propositions.

3. Idealized SDR Planner
describe idealized version Sample Determinize Replan (SDR) planner. samples
single distinguished state current belief, creates deterministic classical problem,
observations correspond derived initial state . solving classical problem using classical planner, applies resulting plan sensing action performed.
Then, belief state updated, removing states agree observed value,
process repeated. idealized version SDR complete explicit description
agents belief state maintained used generate classical planning problem. actual
algorithm modifies version using sampled belief state lazy belief-state maintenance;
described Section 5.
3.1 Replanning using Complete Translations
central component SDR translation PPOS classical planning problem.
well known planning uncertainty reduced planning belief space, i.e.,
move sets possible states world, modeling current knowledge.
sensing actions, conformant planning, results classical, deterministic
planning problem belief space. sensing actions exist, however, resulting problem
non-deterministic, effect sensing actions agents state knowledge depends
values observed online, depend true state world, hidden agent.
Since want generate deterministic classical planning problem solved
off-the-shelf classical planner, need determinize non-deterministic problem described
above, simplifying process. selecting one possible initial state assuming
observation values correspond obtained true initial state. Note
imply planner plans actual initial state world (which would
yield simple classical planning problem standard state space) planner
actually reasons belief state online true initial state, attempts reach
belief state goal known, i.e., possible states goal states.
cases, assumption true initial state turns incorrect.
planner learns online executes sensing action discovers outcome
different expected true initial world state. point, learned
(and possibly initial states) cannot possibly true initial world state. Thus,
uncertainty reduced, replan using new belief state new initial state sampled
it.
algorithm terminates cannot find solution classical problem generated,
implies goal cannot achieved set states indistinguishable
selected state , or, belief state indicates goal, G known, i.e., every state
b, |= G.
high-level SDR algorithm complete initial belief state described Algorithm 1.
570

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

Algorithm 1 SDR (Complete Translation)
Input: PPOS Problem: P = P, A, , G, Integer: size
1: b0 := initial belief state bI = {s : |= }
2: := 0
3: G hold states bi
4:
Select state bi
5:
Generate deterministic planning problem C given P, bi ,
6:
Find solution plan C
7:
solution exists
8:
return failure
9:
end
10:
=
11:
:=first()
12:
Execute a, observe
13:
bi+1 bia,o update belief given a,
14:
ii+1
15:
Remove
16:
inconsistent
17:
break
18:
end
19:
end
20: end

3.2 Generating Classical Planning Problem
Given input PPOS P = P, A, , G, current belief state b, selected state b (hypothesized current true system state), generate classical planning problem Pc (b, ) =
Pc (b), Ac (b), Ic (b, ), Gc . Notice influences definition classical initial state only,
b influences elements except goal. Pc (b, ) defined follows:
Propositions Pc (b) = P {Kp, Kp|p P } {p/s|p P, b} {Ks|s b} :
1. P set propositions appear original problem. value initialized
according distinguished state updated reflect current state
world given true initial state.
2. {Kp, Kp|p P } Propositions encoding knowledge obtained agent. Kp
holds agent knows p true, i.e., p holds possible states. knowledge
obtained sensing action p observed true
necessary consequence action. agent know p true (denoted Kp),
know p false (Kp), know value p (denoted Kp
Kp).
3. {p/s|p P, b} Propositions capture value p given true
initial state. use rule certain states. example, observed p
true, rule state true initial state p/s holds.
571

fiB RAFMAN & HANI

4. {Ks|s b} Propositions capture states ruled out.
concluding certain state initial state system, acquire Ks.
shall use Kc shorthand notation Kl1 Klm , c = l1 lm ,
Kc shorthand notation Kl1 Klm . also note number
propositions generated idealized translation exponentially large,
actual SDR planner, described Section 5, uses various approximations.
Actions every action A, Ac (b) contains action ac defined follows:
pre(ac ) = pre(a) {Kp|p pre(a)}. is, precondition action must hold
agent must know true prior applying action.
every (c, e) effects(a), effects(ac ) contains following conditional effects:
1. (c, e) original effect. conditions update state assumed
true state world.
2. {(c/s, e/s)|s b} above, conditioned states consistent b.
conditions update values propositions given possible states b.
3. (Kc, Ke) know condition c holds prior executing a, know
effect holds following a. condition allows us gain knowledge.
4. (Kc, Ke) c known false prior executing a, e known
false afterwards.
5. {(p, Kp), (p, Kp)|p obs(a)} observing value p, gain knowledge
form either Kp Kp, depending value p state assumed
true world state.
6. {(p p/s, Ks), (p p/s, Ks)|p obs(a), b} rule possible states b
inconsistent observation. sometimes known refutation
states.
addition, literal l (w.r.t. P ) merge action allows us conclude absolute
knowledge knowledge conditional states b. is, states agree
value proposition, know value proposition. exclude states
already refuted, i.e., found inconsistent observation.
pre(merge(l)) = {l/sKs|s b} state s, either agrees l, previously
refuted.
effects(merge(l)) = {(true,Kl)}.




Initial State Ic (b, ) = l:s |=l l
sb Ks:
sb,s|=l l/s
pP Kp Kp
1. {l : |= l} set propositions P appear original problem, initialized
value distinguished state . Notice element
translation affected choice hypothesized initial state.
2. {Kp Kp : p P } Knowledge propositions Kp Kp initialized
false, denoting initial knowledge value propositions.
572

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

3. {l/s : b, |= l} Propositions form p/s initialized value p
corresponding state s.
4. {Ks : b} Propositions Ks initialized false, know
initial state b impossible observing value proposition.
Goal Gc = KG, is, goal literals known true.
translation similar to, inspired KS0 translation introduced Palacios
Geffner (2009) conformant planning. adapt PPOS, chose determinize observation actions sampling distinguished initial state, , whose value track throughout
plan, use select outcome observation actions. Albore et al. (2009) provide different
translation contains propositions encode fact value proposition
sensed (denoted Ap). imply p true, agent knows value p.
Instead requiring Kp hold prior executing action precondition p, require Ap
hold. results optimistic choice values sensed propositions.
natural extension idea conditioning value propositions initial state,
capture p/s propositions use, condition value multiple initial states
p progresses identically. is, suppose differ value
proposition r, r appear actions affect value p. case, p/s
p/s always value. Consequently, simply maintain single proposition, p/{s, }. This, indeed, approach taken Palacios Geffner (2009) Albore
et al. (2009), sets states called tags. larger set states denoted tag,
fewer tags needed, smaller representation. fact, Palacios Geffner show
conformant planning problems require small set tags, linear number proposition.
use tags important optimization, lead exponential reduction size
generated planning problem. decided introduce tags would
complicate description translation, primary technique reducing problem
size state sampling discussed Section 5. Nevertheless, SDR could optimized
using tags instead states.
Example 2. demonstrate translation using small toy example identifying
treating disease. n possible diseases, uniquely identified using
single test, cured using unique treatment. applying treatment, must identify
disease, avoid applying wrong treatment, causing damage. PPOS hence
defined follows:
one proposition per disease, diseasei {1..n}, proposition result
test test-passed.
need n test actions testi , preconditions conditional effect (diseasei ,testpassed), n treatment actions treati precondition diseasei effect diseasei .
also one sensing action observe-test-result allowing us sense value test-passed.
initial state is: (oneof disease1 , ..., diseasen ) test-passed. initial belief state
consists n possible world-states, corresponding n possible diseases.

goal i[1,n] diseasei .
573

fiB RAFMAN & HANI

denote possible states using si {0, 1..n}, si patient
disease state s0 patient disease. Let us choose sk , is, choose
assume patient k th disease.
set propositions translation is:
original propositions diseasei test-passed.
Propositions representing unconditional knowledge:
Kdiseasei , Kdiseasei {1..n}.
Ktest-passed, Ktest-passed.
Propositions representing knowledge conditional upon initial states:
diseasei /sj , {1..n}, j {0..n}.
test-passed/sj , j {0..n}.
Ksj j {0..n}.
set actions is:
Test: testi action have:
preconditions.
effects:





(diseasei , test-passed).
(diseasei , test-passed).
(diseasei /sj , test-passed/sj ), j {0..n}.
(diseasei /sj , test-passed/sj ), j {0..n}.

Treat: treati action have:
precondition: diseasei
effects: diseasei Kdiseasei .
Observing test result:
preconditions:
Effects:
(test-passed, Ktest-passed) positive observations.
(test-passed test-passed/sj ), Ksj , j {0..n} refuting states
agree positive observation.
(test-passed, Ktest-passed) negative observations.
(test-passed test-passed/sj , Ksj ), j {0..n} refuting states
agree negative observation.
Merge: need merges diseasei proposition (see implementation note below):
574

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

preconditions:



j[0..n] diseasei /sj

Ksj

effects: Kdiseasei
actions merging diseasei :

preconditions: j[0..n] diseasei /sj Ksj
effects: Kdiseasei
initial state conjunction following conjuncts (we omit negations simplicity
presentation):
distinguished initial state: diseasek
Uncertainty current state: Ksj , j {0..n}.
Conditional knowledge:diseasei /si , diseasei /sj , {1..n}, j [0..n], = j.

Finally, goal i[1..n] Kdiseasei .
3.3 Notes Efficient Implementation
translation correct, propositions removed. many problems,
original propositions always known. example, Wumpus example,
location agent always known, identical possible current states. value
p always known, remove propositions Kp, Kp, p/s, use proposition p only.
propositions require merge actions, either.
refutation effect actions (item 6 list action effects above) moved
independent action, similar merge actions. create proposition p observable
action (i.e., p obs(a) A) state two refutation actions,
preconditions p p/s p p/s, identical effect Ks. reduces number
conditions action poses difficulty current classical planners.

4. Theoretical Guarantees
prove two important properties algorithm applied deterministic PPOS P.
first result proof correctness translation. show plan exists
original problem iff plan exists classical problems generate, assuming guessed
correct initial state. show that, standard assumptions, algorithm eventually
reach goal. note understanding results required following
sections.
begin section definitions notations used theorems below:
Classical planning notations: b belief state b (world) state recall
Ic (b, s) denotes initial state classical planning problem SDRs translation would
generate selected distinguished initial state. plan P b initial
belief state c corresponding plan classical planning problem Pc (b, s),
action replaced corresponding action generated classical
problem. addition, prior first action c , following translated actions,
575

fiB RAFMAN & HANI

merge actions inserted. assume modified version merge actions
preconditions instead, current precondition replaces condition part
conditional effect previously empty (i.e., = true). allows us insert arbitrary
merge actions without risking making plan undefined. Adding merges, plan c
forced make possible inferences following every action.
Sensing non-sensing actions: without loss generality, assume action
either makes observation changes state world, both. Actions
modeled consecutive pair actions; first, one changes world,
one makes observations.
Indistinguishable states: say s, indistinguishable applicable
iff applicable , observations generated executing
identical. Note s, may indistinguishable distinguishable
plan .
Applicability actions plans: say action applicable state
satisfies preconditions a. say action applicable belief state b
b satisfies preconditions a. Applicability generalized plans
natural manner. is, given plan = a1 , ..., state s, plan applicable
a1 applicable = a2 , ..., applicable a1 (s).
begin showing c achieves Kl literal l, belief resulting
executing plan belief space satisfy l.
Theorem 1. Let b belief state b true initial state. Let sequence actions
original problem. every literal l, b,s |= l iff c (Ic (b, s)) |= Kl.
Proof. prove induction || following conditions hold:
1. applicable b iff c applicable Ic (b, s)
2. every b indistinguishable every literal l: (s ) |= l iff
c (Ic (b, s)) |= l/s
3. every b, distinguishable iff c (Ic (b, s)) |= Ks
4. every literal l: b,s |= l iff c (Ic (b, s)) |= Kl
Base case. base case = c includes merge actions. Conditions 1, 2, 3
immediate construction. is, empty plan always applicable (because contains
actions), distinguishable states. Condition 4 consequence definition
merge action.
Inductive step. inductive step, assume conditions 1-4 hold consider
sequence = a. consider two cases:
sensing action: condition 1, observe given induction hypothesis,
need show applicable following iff ac applicable following c . (If
prefix inapplicable one case, know induction hypothesis also
576

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

inapplicable other). Suppose applicable, i.e., b ,s |= p every precondition
p a. condition 4 conclude c (Ic (b, s)) |= Kp corresponding
precondition ac . additional merge actions preconditions. direction
similar.
conditions 2 3 notice indistinguishable states become distinguishable observing literal holds one other. Thus, non-sensing action cannot
cause two states indistinguishable given become distinguishable following .
Hence, Condition 3 follows immediately induction hypothesis fact
non-sensing action effect form Ks. Condition 2: l
affected last action , follows induction hypothesis. Otherwise,
l added condition c holds . construction, (c/s , l/s )
conditional effect ac . Given induction hypothesis, c held prior execution
iff c/s held prior execution ac merge actions follow c (Ic (b, s))
c (Ic (b, s)). Consequently, l/s holds l holds.
Finally, Condition 4, b,s |= l iff every indistinguishable given ,
(s ) |= l. Condition 2 above, happens iff c (Ic (b, s)) |= l/s every .
Condition 3 guarantees c (Ic (b, s)) |= Ks every distinguishable
s. Thus, suitable merge action, included definition c conclude Kl.
direction, Kl holds, know either true affected
action, consequence merge action. former case, know using induction
hypothesis b ,s |= l. Since l influenced a, conclude b,s |= l.
latter case merge action, use Condition 2 3 conclude b,s |= l.
sensing action: states remain indistinguishable s, conditions 1,2
immediate: effect deduce Kl literal l. Condition 1 note
sensing actions always applicable. Condition 2 note sensing actions affect
state.
Condition 3; distinguishable given either distinguishable
before, case distinguishable following a, Ks never removed
deduced (Ks effect action translation). became distinguishable executed, construction ac Ks effect.
direction, indistinguishable given , must indistinguishable given
. Hence, induction hypothesis, c (Ic (b, s)) |= Ks . Since indistinguishable now, sensing action effect , ac add
Ks , construction.
Condition 4, first suppose b ,s |= l. induction hypothesis happens iff
c (Ic (b, s)) |= Kl. However, b ,s |= l c (Ic (b, s)) |= Kl affected a, thus
remain true well. Therefore, need consider case b ,s |= l
c (Ic (b, s)) |= Kl.
Suppose b ,s |= l b,s |= l. implies state b (s ) |= l
distinguishable . Thus, c (Ic (b, s)) |= Ks
using merge action, conclude c (Ic (b, s)) |= Kl.
577

fiB RAFMAN & HANI

Next, suppose b ,s |= l b,s |= l. Thus, exists b indistinguishable
given (s ) |= l. Since b ,s |= l merge action applied,
conclude b,s |= l.

result provides local soundness completeness proof (i.e., per replanning phase). Applying theorem goal G get following corollary:
Corollary 1. Let b belief state, state it, G goal. SDR (with sound complete
underlying classical planner) find (standard, sequential) plan executed
initial belief state b reach belief state satisfying G, iff one exists.
Proof. Theorem 1 b,s |= l iff c (Ic (b, s)) |= Kl literal l. Thus,
b,s |= G iff c (Ic (b, s)) |= KG. Thus, plan exists original problem iff plan exists
translated problem. (Recall KG shorthand Kg1 Kgm , gi literals
G = g1 gm .)
also use theorem deduce belief state reduced every replanning
episode:
Corollary 2. Let plan generated SDR (with sound complete underlying classical
planner) belief state b initial state b. Let real initial state. execute
b reach belief state satisfying G, = s.
Proof. Given Theorem 1, know SDR generate correct plan b s. Thus,
reach goal indistinguishable, consequently, must
different.
show next global version correctness. show standard,
admittedly strong assumptions, algorithm reach goal within finite number steps.
above, assume complete representation belief state maintained (as opposed
using sample, later).
Dead-ends well known pitfall replanning algorithms, also online algorithms generate complete solution problem, unlikely feasible
practice, may require exponential space. Thus, provide reasonable guarantees, analysis
focuses domains dead-ends. formally, say PPOS problem defines connected state-space, connected3 , state satisfying G reachable every state reachable
I.
Another problem case partial observability inability recognize goal
reached. Consider following simple example; single proposition p,
unobservable whose value unknown initial state. single action, flip,
flips value. goal p, evidently reached state. Yet, never know
p holds. is, never reach belief state states satisfy p.
Thus, deadends enough, must also require belief dead-ends, i.e.,
belief states belief state satisfying goal reachable. say PPOS
3. analogous term context RL algorithms MDPs ergodic.

578

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

belief-connected given belief state b reachable initial belief state, bI ,
world-state consistent b (i.e., b), exists sequence actions that,
true starting state, leads b b , b |= G. notation: ba,s |= G. Clearly,
belief-connectedness implies connectedness.
Theorem 2. Given belief-connected PPOS problem, SDR sound complete underlying
classical planner set tags corresponds possible initial states, reach goal
finite number steps.
Proof. proof based ideas PAC-RL algorithms E 3 (Kearns & Singh, 2002)
Rmax (Brafman & Tennenholtz, 2003) follows immediately Corrolary 2. Consider plan
generated assumption true initial state. plan successful
every initial state indistinguishable given . plan fails, recognize
fact maintain sound complete description current belief state. also
conclude neither initial state indistinguishable given possible,
hence, belief state reduced least one state. belief-connectedness,
still reach goal. continue process number times equal
size initial belief state, point belief state singleton, left
classical planning problem, underlying classical planner solve.
proof makes apparent many cases, polynomial number replanning
phases required, number initial states ruled could large, e.g.,
iteration learn initial value single proposition. Bonet Geffner (2011) identify one
case, which, consequently, requires linear number replanning phases succeed.

5. SDR Belief State Sampling Belief Tracking
size classical problem generated translation method suggested Section 3.2
depends cardinality belief state, i.e., number possible initial states: generate
one proposition original proposition possible initial state, conditional
effect every action generate conditional effect possible initial state. lead
exponential (in size P) larger classical planning problem. Thus, generated problems
size may become large current classical solvers handle. addition, belief state
exponentially large, explicit representation impractical. address
issues using sampled subset current belief state generate classical planning
problem, using implicit description belief state.
5.1 Sampling Belief State
address problem large belief state suggest using sampled subset belief state
generate classical planning problem. Conceptually technically, change required
method described Section 3.2 minor: select subset b generate classical
planning problem true belief state. also implies distinguished initial
state chosen . Thus given PPOS P = P, A, , G, sampled set state
|= , distinguished initial state S, simply generate classical planning
problem Pc (S , ).
579

fiB RAFMAN & HANI

sampling translation method similar complete translation, important semantic difference. complete translation Kp denoted knowing p true
possible states, sampling translation Kp denotes knowing p true sampled
states. Thus, upon execution, agent intends execute action, might preconditions whose value known true possible states. call actions unsafe.
must ensure unsafe actions never executed.
new translation uses instead b cannot guarantee action resulting
plan unsafe. reason, must maintain representation true belief state throughout
execution. use information check whether possible state exists
precondition next action hold. call state witness state. witness state
found, must sample replan. ensure generate another plan
executable witness state, add set sampled states, . new plan either
learn distinguish witness state rest states , choose different
path valid witness state.
Example 3. Returning Wumpus example, show sampled translation reduces
size translation. example, four possible initial states, denote
sll , slr , srl , srr , sll denotes initial state Wumpus left
diagonal, etc. select sll , slr sampled belief state , sll distinguished initial
state .
set propositions is:
original propositions:
at-x-y 1 x, 4 explained above, proposition always known
require conditional knowledge propositions.
wumpus-at-2-3, wumpus-at-3-2, wumpus-at-3-4, wumpus-at-4-3
stench-at-x-y 1 x, 4.
Propositions representing unconditional knowledge:
Kwumpus-at-2-3, Kwumpus-at-3-2, Kwumpus-at-3-4, Kwumpus-at-4-3.
Kwumpus-at-2-3, Kwumpus-at-3-2, Kwumpus-at-3-4, Kwumpus-at-4-3.
Kstench-at-x-y 1 x, 4.
Kstench-at-x-y 1 x, 4.
Propositions representing conditional knowledge:
sll conditional propositions:
wumpus-at-2-3/sll , wumpus-at-3-2/sll , wumpus-at-3-4/sll , wumpus-at-4-3/sll .
stench-x-y/sll 1 x, 4.
slr conditional propositions:
wumpus-at-2-3/slr , wumpus-at-3-2/slr , wumpus-at-3-4/slr , wumpus-at-4-3/slr .
stench-at-x-y/slr 1 x, 4.
Ksll , Kslr propositions denoting refuted states.
580

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

set actions is:
Move: move-from-x1 -y1 -to-x2 -y2 actions have:
preconditions: at-x1 -y1 wumpus-at-x2 -y2 Kwumpus-at-x2 -y2 )
effects 4 :
at-x1 -y1
at-x2 -y2
Smell: smell-stench-at-x-y action have:
preconditions: at-x-y
effects:







stench-at-x-y , Kstench-at-x-y
stench-at-x-y stench-at-x-y/sll , Ksll
stench-at-x-y stench-at-x-y/slr , Kslr
stench-at-x-y , Kstench-at-x-y
stench-at-x-y stench-at-x-y/sll , Ksll
stench-at-x-y stench-at-x-y/slr , Kslr

Merge: illustrate merges using stench-at-x-y proposition:
preconditions:(stench-at-x-y/sll Ksll ) ( stench-at-x-y/slr Kslr )
effects: Kstench-at-x-y
initial state conjunction following conjuncts (we omit negations simplicity presentation):
distinguished initial state:

at-1-1 x=1..4,y=1..4,x=1y=1 at-x-y
wumpus-at-2-3 wumpus-at-4-3 wumpus-at-3-2 wumpus-at-3-4
stench-at-1-3 stench-at-2-2 stench-at-2-4 stench-at-3-3 stench-at-4-2 stenchat-4-4
Uncertainty current state: Ksll Kslr .
Conditional knowledge:
state sll
wumpus-at-3-2/sll wumpus-at-2-3/sll wumpus-at-4-3/sll wumpus-at-3-4/sll .
stench-at-1-3/sll stench-at-2-2/sll stench-at-2-4/sll stench-at-3-3/sll stenchat-4-2/sll stench-at-4-4/sll stench-at-x-y/sll x locations.
state slr
4. domain conditional effects, list effect e directly rather writing (true, e).

581

fiB RAFMAN & HANI

wumpus-at-2-3/slr wumpus-at-3-2/slr wumpus-at-3-4/slr wumpus-at-4-3/slr .
stench-at-1-3/sll stench-at-2-2/sll stench-at-2-4/sll stench-at-3-3/sll stenchat-4-4/sll stench-at-x-y/sll x-y locations.
Finally, goal Kat-4-4.
5.2 Note Theory
theoretical guarantees Section 4 hold sampled translation. Specifically,
expected, translation longer sound: plan works states may work
states. However, since never apply illegal action (because maintain complete description
belief state plan execution), maintain assumption belief-connectedness,
goal always remains reachable. question whether ensure progress towards
goal. Unfortunately, planner may always come unsound plan, even beliefconnectedness, progress cannot guaranteed without additional assumptions. example,
assume sample size grows time plan unsound, i.e., accumulate
witness states discussed above, ensure progress made, eventually goal
reached, i.e., assured completeness.
5.3 Belief State Maintenance Regression
noted above, must maintain information true belief state. Belief state maintenance
difficult task various representations CNF, DNF, Prime Implicates, Prime Implicants,
(To et al., 2009; To, Son, & Pontelli, 2010, 2011b; et al., 2011, 2011a), work well
domains poorly domains. is, representation method suitable
family domains special features, work well domains
features exist. However, require belief state order answer two types
queries: (1) Sampling subset current possible states executed replanning
episode constructing classical problem, (2) Checking whether literal l holds
currently possible states executed prior action execution ensure unsafe,
order check whether goal reached.
propose method answering queries without maintaining explicit representation
belief space, regressing queries execution history towards initial belief
state. approach requires maintain initial belief state formula execution
history only, somewhat similarly situation calculus (McCarthy & Hayes, 1969; Reiter, 1991).
answer query current state, regress query entire history
compare initial state. improve performance, also cache limited information
intermediate belief states.
main benefit approach focused querys condition only, therefore
yields small formulas easier check satisfiability than, e.g., formula describing
complete history conjunction initial belief (Hoffmann & Brafman, 2006). However,
process must repeated every query.
5.3.1 Q UERYING C URRENT TATE P ROPERTIES
Throughout online process maintain symbolic initial state formula history
actions observations made. use information check, prior applying action a,
582

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

whether preconditions hold current state, is, whether action safe. must also
check whether goal conditions hold current state determine whether goal
achieved. check whether condition c holds, regress c current history, obtaining
cI . world state currently satisfies c iff result executing current history initial
state satisfying cI . cI inconsistent iff c holds states currently possible.
specifically, checking whether literal l (or set literals) holds current
belief state, regress negation current history, resulting formula . Next,
check, using SAT solver, whether satisfiable. satisfiable, know l
valid.
Recall c = regress(c, a) weakest condition state executing
yields state satisfying c. compute regress(c, a) using following recursive procedure:
regress(l, a) = false (true, l) effects(a).
regress(l, a) = pre(a) (true, l) effects(a). case pre(a) eliminated
regression, preconditions already regressed proven valid
prior applying a. Thus, (true, l) effects(a) regress(l, a) = true.


regress(l, a) = pre(a) (l (c,l)effects(a) c) (c,l) c. is, either l existed prior
action executed, added one conditions l effect. also
impossible conditions removing l apply. above, pre(a) eliminated
regression.
regress(c1 c2 , a) = regress(c1 , a) regress(c2 , a)
regress(c1 c2 , a) = regress(c1 , a) regress(c2 , a)5
Applying regression histories sequences actions, extend meaning regress operator:
regress(c, ) = c
regress(c, h a) = regress(regress(c, a),h)
maintain correct description set initial world states, must also update initial
belief-state formula whenever make observation. Thus, regress every obtained observation
h, obtaining regressed formula , conjoin bI . Thus, updated set initial
state described = . optimize, apply unit propagation maintain
semi-CNF form conjunction disjunctions xor (oneof ) statements. is, convert
newly added CNF form. current benchmarks find maintaining initial
belief formula CNF easy.
5.3.2 AMPLING B ELIEF TATE
SDR samples belief state generate subset possible states computing translation
classical problem. Using regression mechanism, maintain formula describing
possible set initial states given initial constraints possible states current history,
i.e., actions executed observations sensed.
5. Recall effects deterministic effect condition conjunction.

583

fiB RAFMAN & HANI

sample n states current belief state, begin finding n possible satisfying
assignments initial belief formula . running simple SAT solver,
picks propositions randomly set unassigned propositions formula, sets
value them, propagates value formula. formula unsolvable,
backtrack. current benchmarks structure initial belief formula simple, e.g.,
disjunctive set oneof statements, clauses, simple SAT solver finds solutions
rapidly.
n satisfying assignments represent set initial states consistent
current information (i.e., initial belief state observations made far). obtain sample
states current belief state, progress history.
5.3.3 PTIMIZATION : PARTIALLY-S PECIFIED TATES
step execution current plan maintain list literals known hold
belief state. propositions appear among literals list currently unknown.
call construct partially-specified belief state (PSBS), serves cache.
execute action a, propagate set forward update follows:
(c, l) effect a, c must true execution, add l remove l.
l l may true (that is, l unknown PSBS), deletes l l holds (possibly
conditional conditions necessarily hold), add l l (and
conditions possible) holds, conclude l must true execution a.
performing regression condition c, may learn literal l valid
intermediate PSBS. update PSBS successors, accordingly.
instance, observing test-passed Example 2, add last PSBS. Then,
regression action testi , obtain diseasei , add previous PSBS,
forth. Following regression, simplification techniques mentioned above,
may learn literal l true initially. progress l history add
information PSBSs. Example 1, instance, regressing stench observation
initial state formula, may learn wumpus-at-3-4 holds, progress
history add learned information PSBS.
PSBS may reduce formulas constructed regression. example, suppose
regressed formula intermediate belief state b form l, l literal
belongs current PSBS, i.e., known hold b. Then, need regress back
b. Or, know l holds b, immediately conclude regressed formula
inconsistent initial belief state.
conclude, belief state representation uses initial belief, represented symbolic
formula (initialized given PPOS definition), sets literals shown hold
time step. update initial belief adding formulas resulting regressing
observations, thus adding constraints set possible states, reducing cardinality
initial belief state. Whenever discover literal holds time step, either
action (unconditional) effect, regressing observation, cache this. regression
mechanism uses cached facts whenever possible reduce size regressed formula.
584

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

5.4 Complete SDR Planner Algorithm
explaining essential components SDR planner, i.e., sampling, translation,
regression-based belief maintenance, present complete SDR algorithm (Algorithm 2).
Algorithm 2 SDR (Sampling Translation)
Input: PPOS Problem: P = P, A, , G, Integer: size number states sampled SI
1: h := , empty history
2: G known current belief state
3:
Select distinct set states SI consistent bI , finding satisfying assignments
s.t. |S | size
4:
Select distinguished state sI SI
5:
Propagate SI sI h, resulting
6:
Generate deterministic planning problem Pc (S , )
7:
Find solution C
8:
solution exists
9:
return failure
10:
end
11:
=
12:
:=first()
13:
Regress pre(a) h obtaining pre(a),h
14:
pre(a),h inconsistent bI
15:
break
16:
end
17:
Execute a, observe
18:
Append a, h
19:
Regress h obtaining o,h
20:
Update initial belief state formula given o,h : o,h
21:
Remove
22:
inconsistent
23:
break
24:
end
25:
Update current state: a(s )
26:
end
27: end
algorithm begins querying goal state already reached (line 2).
done regressing negation goal conditions current history, checking
whether negation consistent initial belief state. latter true, know
state goal conditions apply.
choose sub-sample SI bI (the initial belief state) sI SI (lines 3 4).
propagate states SI history applying executed actions h states
SI (line 5). Lines 3-5 thus equivalent sampling current belief state.
obtain , use translation Section 3.2 (replacing b ) generate
classical problem Pc (S , ), solve using classical planner (lines 6 7).
585

fiB RAFMAN & HANI

solution problem, means goal cannot obtained current
state, thus solution PPOS.
execute obtained plan; first check preconditions current action hold
current belief state. done regressing negation preconditions
history, checking regressed formula consistent bI (line 13-16). consistent,
i.e., state bI (regressed) negation preconditions hold,
must choose new SI replan.
Finding preconditions hold current belief state, execute current action, observing observation o. regress history update initial belief state using
regressed formula (lines 19 20). inconsistent (that is, o,h inconsistent
sI ) must sample replan (lines 22-24). Otherwise, even inconsistent
state sI SI , sI = sI , continue executing plan.
execution plan terminates check see goal met,
obtained, sample replan again.
5.4.1 B IAS BSERVATION K NOWLEDGE .
possible bias SDR planner make observations. crude simple method execute
sensing action sense unknown value without affecting state. context
current benchmarks improves planners run-time performance. refer version
SDR SDR-Obs.
focused method augment goal state requirement prove
distinguished initial state correct. refer version SDR state-refutation (SDR-SR).
Recall distinguished state determines value propositions initial state,
affect knowledge. Thus, Wumpus domain, sll distinguished state
wumpus-at-2-3 holds, Kwumpus-at-2-3 not. change goal Kat-4-4
Kslr Ksrl Ksrr planner generate plan knowledge effects
well. is, valid plan must prove, e.g., initial state srr , Wumpuses
right, invalid. state refutation achieved actions whose effects s,
effects obtained following differentiating observation, method encourages
planner take sensing actions.
distinguished initial state unlikely true initial state, plans generated
modified goal likely quickly identify fact. This, turn, cause replanning
trigger sooner, information. course, necessarily need know
identity initial state succeed, may also add sensing actions may
required optimal plan.

6. Related Work
SDR borrows extends ideas various related planners, notably: replanning/online
planning, translation-based techniques, lazy belief state representation. briefly discuss
here.
Replanning recently become popular online planning uncertainty FFReplan (Yoon et al., 2007) MDP solver. Replanning technique planning uncertainty
online, stage, planner solves simplified problem typically removes uncertainty, e.g., making assumptions value unknown variables effects actions.
586

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

planner executes obtained solution receives information contradicts assumptions, updates model new information, repeats process. example, FF-Replan
assumes certain deterministic effects stochastic actions, obtaining classical planning problem.
replanning essentially ignores certain aspects model, runs risk getting stuck
dead-ends, regions state space difficult reach goal. However, combined smart sampling techniques recently developed stochastic planning problems,
UCT (Kocsis & Szepesvari, 2006) , replanning becomes powerful technique. example, FFReplan later improved using idea hindsight optimization (Yoon et al., 2008; Yoon, Ruml,
Benton, & Do, 2010), multiple, non-stationary determinizations MDP examined.
choice next action guided solution multiple resulting classical planning
problems, enabling planner account different possible future dynamics.
noted above, essential element replanning reduction current problem
simpler problem. SDR builds translation-based approach conformant planning introduced Palacios Geffner (2009) generate classical planning problem, specifically,
KS0 translation. conformant planning, resulting classical planning problem equivalent
original problem (has set solutions), may much larger size. Applied
contingent planning, translation methods generates non-deterministic, fully observable, planning problem. make problem deterministic, SDR simplifies assuming observations
conform specific initial state. reduce size, SDR samples subset initial
states.
Palacios Geffner (2009) suggest different technique controlling problem size.
SDR maintain value propositions conditioned initial state, planner maintains value proposition conditioned sets initial states, called tags. Ideally, tags
proposition p contain initial states differ value propositions whose initial
value affect future value p. sets quite large, implying fewer tags
required. This, turn, leads considerable savings size generated problem.
set tags required complete translation large, one may use tags deterministic
following sense: proposition may different values different states belonging
tag. soundness still maintained case, one must sacrifice completeness.
CLG planner (Albore et al., 2009) takes different approach extending ideas Palacios Geffner contingent planning. P original contingent planning problem, denote
X(P ) fully-observable non-deterministic problem obtained using transformation Palacios Geffner. CLG solves X(P ) obtaining heuristics relaxed version problem
moves precondition condition effects action similar way done
CFF (Hoffmann & Brafman, 2006), drops non-deterministic effects, introduces propositions
form Ap, roughly say p observed. effect sensing action senses
p thus Ap, rather Kp (which would erroneously imply offline know value
sensed). Actions require p precondition, require Ap classical problem generated.
forces classical solver insert sensing action senses p insert action
requires p. course, actual value sensed online may correspond assumptions
made rest plan. why, following every sensing action, CLG (in online version) replans. Thus, plan executed first sensing action. initial belief state
correctly computed, translation ensures fragment plan executed. Thus,
CLG SDR use replanning translation classical planning replanning phase.
However, SDR uses translation simpler two ways. use Ap type proposi587

fiB RAFMAN & HANI

tions, instead uses sampling determinize sensing actions. addition, SDR uses sampling
select subset possible initial states. leads much smaller classical planning problems
faster generate solve, less informed. Consequently, often SDR takes
steps reach goal, able scale better CLG.
Recently, Bonet et. al. introduced replanning-based contingent planner (Bonet & Geffner,
2011) called K-planner, similar SDR, focuses special class domains
handled efficiently. domains hidden variables static, i.e., value
hidden propositions change throughout execution plan. example,
Wumpus domain, Wumpuses move, location Wumpuses along diagonal
static. Localize domain, however, hidden current wall configuration changes every
move action. Thus, K-planner unsuitable domain. addition, K-planner assumes
value observable variables always observed following action i.e., explicit
sensing actions.
K-planner SDR developed parallel, share many ideas, provide similar theoretical guarantees. K-planner uses replanning translation classical planning much
like SDR (and CLG). Whereas SDRs translation assumes sensed value correspond
sampled initial state, K-planner actions correspond making assumptions sensed
values. is, real sensing actions translated multiple classical actions lead knowledge different values sensed variable. Thus, classical planner may choose value
would like sensor sense. essentially allows planner make optimistic assumptions ensuring goal focused sensing. However, multiple sensing actions plan may
assumed effects cannot realized initial state. may affect quality
classical plan generated, lead unsound behavior because, online, plan executed
first sensing action, belief state updated replanning takes place.
SDRs use select initial state ensures sensed values consistent, may
pessimistic cannot conceive sensed value different dictated
special initial state selected.
noted above, K-planner makes certain assumptions nature uncertainty
domain, one important contribution makes showing properties leveraged
provide efficient representation stronger guarantees. Essentially, static hidden
variable assumption made K-planner viewed saying uncertainty domain
essentially value multi-value variable, conditional effects depend
value variable. case, one represent belief state compactly one
generate compact translations classical planning. Bonet Geffner prove soundness
completeness K-planner similar assumptions made here, also show
additional assumption nature uncertainty domain discussed above,
achieve properties generating classical planning problems linear size
original planning problem.
Like planners operate uncertainty, SDR maintains form belief state.
SDR taken lazy approach belief state maintenance extreme. lazy, implicit
approach belief state maintenance introduced CFF (Hoffmann & Brafman, 2006). CFF
maintains single complete formula describes history initial state jointly.
requires using different copy state variables time point. action applied step
conjoined formula clauses describe value variables time + 1
function value time t. determine whether condition c holds current belief state,
588

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

current formula conjoined c. resulting formula consistent, know
possible states c holds. Otherwise, know c valid. example, c
goal condition, know plan found. c precondition action a,
know applied safely. CFF also caches information discovered simplifying
formula whenever conclusion obtained, via unit propagation. regression method
use thought constructing, query, part CFF formula
needed answering current query. downside approach could,
principle, reconstruct formula, parts formula repeatedly. advantage
formulas construct much smaller easier satisfy complete CFF formula.
Many belief state representations explored literature, including binary decision
diagrams (Bryce, Kambhampati, & Smith, 2006), DNF CNF representations, Prime Implicates Prime Implicants (To et al., 2011b, 2011, 2011a, 2009, 2010). Overall, (2011)
concludes different domains require different representations. is, belief representation method works well domains certain structure actions, well
domains. would interesting compare various belief representation methods lazy
regression based technique large set benchmarks, leave future research.
et al. also suggest number contingent planners, built using belief representations AND/OR forward search algorithm, generate complete plan trees contingent
problems. planners CFF, POND (Bryce et al., 2006), CLG offline mode also
compute complete plan trees. general, plan trees exponential, must produce
different plans possible initial state worst case. Thus, offline contingent planning
inherently difficult scale larger domains many possible initial states.

7. Experimental Results
demonstrate power replanning approach compared SDR state art
contingent planner CLG (Albore et al., 2009). use CLG so-called execution mode,
becomes online planner. compare SDR CLG domains CLG paper:
Wumpus: grid-based navigation problem, agent must move lowest left
corner upper right corner. diagonal grid surrounded squares containing
either monsters called Wumpuses, pits. agent must ensure square safe, i.e.
monster pit, entering it. Squares cannot directly checked safety. Instead,
squares surrounding Wumpus agent smell stench, squares surrounding
pit agent feel breeze. agent know, though, given breeze stench
neighboring squares unsafe. ensure safety agent must use information
collected various squares around suspected squares. domain demonstrates
complex hidden state multiple sensing actions, conditional effects,
key bottleneck CLG translation. Thus, domain unknown features
environment fixed, uncertainty propagate propositions , i.e.
known features become unknown. type domains K-planner
handle.
Doors: grid-based navigation task, agent must move left
grid right. Along way walls passage (unlocked door)
single location. sense whether door unlocked agent must try it. naive
589

fiB RAFMAN & HANI

strategy hence move along wall try doors unlocked one found.
Then, agent start new wall. domain exhibits simple strategies
conditional effects. Like Wumpus domain, uncertainty propagated here.
Color-balls: domain several colored balls hidden grid, agent must find
balls transfer bin appropriate color. domain large state
space, multiple sensing actions, conditional effects. Here, again, propositions
whose values known cannot become unknown.
Unix: domain agent must find file folder tree transfer root.
smart technique searching tree agent must exhaustively check
every subfolder. domain contain conditional effects. domain
uncertainty propagate.
Localize: yet another grid-based navigation problem, agent must reach
top right corner grid. However, agent unaware position within grid.
agent sense nearby walls, hence deduce position within grid. domain
presents complex conditional effects domains, thus difficult
scaling using CLG translation. domain, due conditional effects
unknown features, uncertainty propagate, i.e. known features may change value
depending value variables whose value unknown, causing them, turn, become
unknown. example, sense wall right, move knowledge
observing wall right lost. domain unsuitable K-planner.
clear description benchmark domains somewhat limited. Specifically, domains use conditional effects, challenging partial
observability. dead-ends. come back issue Section 7.5. Nevertheless,
assess performance SDR comparison current state art, evaluate
domains, allow scaling using larger grids, balls forth.
implemented SDR algorithm using C#. experiments conducted Windows
Server 2008 machine 24 2.66GHz cores (although experiment uses single core)
32GB RAM. used FF (Hoffmann & Nebel, 2001) compiled Cygwin environment
solving deterministic problems, used Minisat SAT solver (Een & Sorensson,
2003) search satisfying assignments (or lack of). Minisat provide random
assignments, also implemented naive solver generates random assignments true
environment states.
7.1 Comparing Plan Quality Execution Time
begin comparing SDR variants CLG number benchmarks. compute average
number actions average time several (25) iterations problem instance,
iteration initial state uniformly sampled. Arguably, computing averages case
contingent planning perhaps incorrect estimation, averages assume uniform sampling
conditions (initial states case), part PPOS formalism. Indeed, defining
reasonable comparison metric online contingent planners still open question, lack
better measure, report measured averages.
590

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

Name
clog
huge
elog
7
ebtcs
70
CB
9-1
CB
9-3
CB
9-5
CB
9-7
doors
5
doors
7
doors
9
doors
11
doors
13
doors
15
doors
17
localize
3
localize
5
localize
9
localize
11
localize
13
localize
15
localize
17

#Actions
82.42
( 0.64 )
22.08
( 0.05 )
33.96
( 0.8 )
244
( 6.16 )
841.44
( 7.23 )
1068.6
( 4.44 )
Failed
21.6
( 0.22 )
47.76
( 0.52 )
97.76
( 1.1 )
148.44
( 1.3 )
229.04
( 1.7 )
343.26
( 2.77 )
519.13
( 4.7 )
8
( 0.12 )
14.56
( 0.24 )
28.52
( 0.42 )
34.67
( 0.61 )
37.52
( 0.62 )
40.08
( 0.61 )
45
( 0.86 )

SDR
Time(secs)
321.28
( 9.32 )
1.81
( 0.02 )
17.4
( 0.38 )
214.1
( 7.65 )
912.73
( 18.7 )
1356
( 10.7 )

3.76
( 0.05 )
18
( 0.26 )
72.5
( 0.87 )
216.52
( 4.4 )
524.03
(7)
1086
( 20 )
1582
( 26 )
1.77
( 0.03 )
7.12
( 0.1 )
72.69
( 1.43 )
155.6
( 3.87 )
396.76
( 10.72 )
667.22
( 19.7 )
928.56
( 33.2 )

SDR-obs
#Actions Time(secs)
61.17
117.13
( 0.44 )
( 4.19 )
21.76
0.85
( 0.07 )
( 0.01 )
35.52
3.18
( 0.75 )
( 0.07 )
124.56
71.02
( 2.49 )
( 1.57 )
247.28
245.87
( 2.91 )
( 4.03 )
392.16
505.48
( 2.81 )
( 8.82 )
487.04
833.52
( 2.95 )
( 15.82 )
18.04
2.14
( 0.18 )
( 0.03 )
35.36
9.29
( 0.41 )
( 0.1 )
51.84
28
( 0.55 )
( 0.31 )
88.04
79.75
( 0.91 )
( 1.04 )
120.8
158.54
( 0.93 )
( 2.01 )
143.24
268.16
( 1.36 )
( 3.78 )
188
416.88
( 1.64 )
( 6.16 )
8.88
0.81
( 0.11 )
( 0.01 )
15.32
2.87
( 0.21 )
( 0.04 )
29.44
26.61
( 0.47 )
( 0.48 )
41.2
77.11
( 0.83 )
( 1.97 )
56.96
159.53
( 0.69 )
( 4.18 )
68.44
352.36
( 0.9 )
( 9.72 )
81.24
527.53
( 1.16 )
( 15.25 )

SDR-SR
#Actions Time(secs)
82
786.17
( 0.42 )
( 31.19 )
21.52
1.67
( 0.05 )
( 0.02 )
35
25.18
( 0.64 )
( 0.39 )
264.24
140.64
( 5.65 )
( 5.21 )
665.4
565
( 6.37 )
( 18.1 )
918.07
716
( 4.72 )
( 15.3 )
Failed
16.64
( 0.23 )
40.52
( 0.45 )
77.68
( 0.92 )
125.08
(1)
185.64
( 1.88 )
252.24
( 2.04 )
299.28
( 2.78 )
8.88
( 0.14 )
13.08
( 0.22 )
22.12
( 0.43 )
31.12
( 0.6 )
39.96
( 0.54 )
50.63
( 0.59 )
59.48
( 0.81 )

3.05
( 0.04 )
17.87
( 0.22 )
61.57
( 0.9 )
174.04
( 1.75 )
383.88
( 5.42 )
725.76
( 8.52 )
1089
( 17 )
1.95
( 0.02 )
8.24
( 0.16 )
81.44
( 1.12 )
199.35
( 3.84 )
387.75
( 7.01 )
721.53
( 13.33 )
1031
( 25 )

#Actions
51.76
( 0.33 )
20.12
( 0.05 )
36.52
( 0.86 )
94.36
( 1.83 )
252.76
( 2.66 )
PF

CLG
Time(secs)
8.25
( 0.08 )
1.4
( 0.08 )
73.96
( 0.14 )
129.3
( 0.26 )
819.52
( 0.47 )

TF
16.44
( 0.18 )
30.4
( 0.24 )
50.48
( 0.5 )
71.68
( 0.79 )
105.48
( 0.89 )
PF

2.4
( 0.1 )
20.44
( 0.02 )
38.52
( 0.06 )
126.59
( 0.1 )
330.73
( 0.21 )

PF
CSU
CSU
CSU
PF
PF
PF
PF

Table 1: Comparing CLG (execution mode) various SDR methods. domains conditional actions
(localize) CLG execution cannot simulated. TF denotes CLG translation failed; CSU denotes
CLG cannot run simulation uniform distribution; PF denotes CLG planner failed, either due
many propositions due timeout.

Table 1 Table 2 lists results various SDR methods CLG. report results
pure SDR, SDR observation bias (denoted SDR-obs), SDR state refutation added
goal (denoted SDR-SR). method report average number actions
591

fiB RAFMAN & HANI

Name
unix
1
unix
2
unix
3
unix
4
wumpus
5
wumpus
10
wumpus
15
wumpus
20

SDR
#Actions Time(sec)
9.4
0.46
( 0.16 )
( 0.01 )
31.04
2.01
( 0.79 )
( 0.05 )
78.48
9.61
( 2.23 )
( 0.28 )
195.8
53.1
( 4.73 )
( 1.38 )
27.19
9.8
( 0.39 )
( 0.16 )
45.18
102.08
( 1.57 )
( 2.17 )
61.2
464.74
( 1.01 )
( 10.05 )
80.33
1296
( 1.47 )
( 21 )

SDR-obs
#Actions Time(sec)
12.2
0.48
( 0.16 )
( 0.01 )
26.44
1.41
( 0.72 )
( 0.03 )
56.32
5.47
( 1.72 )
( 0.18 )
151.72
35.22
( 4.12 )
( 0.94 )
34.72
6.51
( 0.3 )
( 0.07 )
70.64
65.89
( 1.13 )
( 1.13 )
120.14
324.32
( 2.4 )
( 7.14 )
173.21
773.01
( 3.4 )
( 20.78 )

SDR-SR
#Actions Time(sec)
9.32
5.28
( 0.18 )
( 0.95 )
29.28
2.33
( 0.7 )
( 0.05 )
100
21.19
( 2.12 )
( 1.04 )
202.24
78.81
( 6.02 )
( 2.38 )
26.48
9.37
( 0.24 )
( 0.1 )
39.72
54.21
( 0.49 )
( 0.72 )
72.32
368.53
( 1.04 )
( 12.48 )
112.33
952.52
( 3.12 )
( 17.62 )

CLG
#Actions Time(sec)
11.68
0.35
( 0.23 )
( 0.01 )
19.88
2.69
( 0.47 )
( 0.01 )
51.32
18.56
( 0.97 )
( 0.05 )
90.8
189.41
( 2.12 )
( 0.6 )
24.12
2.38
( 0.1 )
( 0.09 )
40.44
36.29
( 0.18 )
( 0.04 )
101.12
330.54
( 0.67 )
( 0.25 )
155.32
1432
( 0.95 )
( 0.47 )

Table 2: Comparing CLG (execution mode) various SDR methods. results averaged 25
executions, standard error reported brackets.

Name
logistics
CB
doors
localize
unix
Wumpus
Overall
Largest

SDR
#Actions Time
1
0
0
0
0
0
0
0
1
0
2
0
4
0
2
0

SDR-obs
#Actions Time
0
2
3
4
3
7
0
7
0
3
0
2
6
25
4
10

SDR-SR
#Actions Time
0
0
0
0
1
0
6
0
1
0
1
0
9
0
2
0

CLG
#Actions Time
2
1
1
0
5
0


3
1
1
2
12
4
3
1

Table 3: Counting number times method performed best category. bottom row
shows results two largest problems category (except first category cloghuge
considered 11 problems overall).

average time (seconds) goal reached 25 iterations (standard error reported brackets). Different executions correspond case different selections initial states,
deterministic PPOS, initial state governs complete observation behavior. SDR variants,
various executions also correspond different possible samplings states current belief
state.
offline contingent planners compute complete plan tree, execution time (as opposed
planning time) negligible. case online replanning algorithms, however, execution time
encapsulates various important aspects performance, belief update computation,
time required replanning episodes. real applications, controlling robots, would
translate time required system deciding next action. time
considerable, possible robot would stop operating order compute
next action. Thus, execution time important factor deciding online replanning
approach appropriate.
592

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

Execution time CLG includes translation time domain, CLG execution, plan verification time environment, adds seconds execution. translation
timeout 20 minutes, CLG execution also stopped 30 minutes. allowed FF
timeout 2 minutes replanning episode, timeout never reached. cases
FF solves deterministic plan seconds. domains CLGs simulator support uniform sampling initial states (denoted CSU Table 1). domains SDR scales
larger instances CLG, problem crucial comparison. domain
bolded shortest plan fastest execution.
see Table 3, SDR SDR-obs typically faster CLG, difference
grows scale larger instances. SDR variants also scale larger problems CLG
fails. benchmark domains, observation bias SDR-obs resulted faster execution,
typically beneficial SDR variants learn much concerning
hidden state early possible, replan accordingly.
domains planners solve, efficiency (in terms avg. steps reach goal)
mixed. CLG clearly generates shorter plans Unix Doors, many instances SDRobs better, larger Wumpus instances, SDR SDR-SR better. surprising
general CLG produces shorter plans, considers contingencies (the complete
belief state), also reason difficulty scaling larger domains. CLG
produces plans lesser quality, speculate heuristic search belief
space embedded CLG, due problems translation approach.
SDR also computes much smaller translated domain descriptions, ranging 10KB 200KB.
However, direct comparison CLG impossible SDR generates parameterized domains CLG generates grounded translations, hence provide detailed results
model sizes.
conclusion, expected, one interested shorter plans, CLG seems
appropriate candidate, one wishes scale larger domains, SDR variants better
choice.
7.2 K-Planner
K-Planner (Bonet & Geffner, 2011) state art planner handles PPOS problems
static hidden variables use explicit sensing actions, assumes possible
observations immediately available upon entering state. domains maintenance
belief state especially simple, translation relatively easy generate. Thus,
surprise K-Planner performs much better SDR CLG domains.
Looking Table 4 one see differences especially pronounced case
Wumpus domains, SDR-OBS significant overhead updating belief. Furthermore,
K-Planner employs optimistic heuristic, especially appropriate Wumpus domain.
K-Planner simply assumes top-right square safe, goes immediately.
square safe, traces back finds passage top-left square, goes directly.
domains, optimistic heuristic successful.
conclude, K-Planner far best approach domains static hidden variables,
difficult see direct extension K-Planner handles types PPOS problems.
593

fiB RAFMAN & HANI

Name
CB-9-1
CB-9-3
CB-9-5
CB-9-7
doors5
doors7
doors9
doors11
doors13
doors15
doors17
unix1
unix2
unix3
unix4
Wumpus05
Wumpus10
Wumpus15
Wumpus20

SDR-obs
#Actions
Time(secs)
124.56 ( 2.49 ) 71.02 ( 1.57 )
247.28 ( 2.91 ) 245.87 ( 4.03 )
392.16 ( 2.81 ) 505.48 ( 8.82 )
487.04 ( 2.95 ) 833.52 ( 15.82 )
18.04 ( 0.18 )
2.14 ( 0.03 )
35.36 ( 0.41 )
9.29 ( 0.1 )
51.84 ( 0.55 )
28 ( 0.31 )
88.04 ( 0.91 )
79.75 ( 1.04 )
120.8 ( 0.93 )
158.54 ( 2.01 )
143.24 ( 1.36 ) 268.16 ( 3.78 )
188 ( 1.64 )
416.88 ( 6.16 )
12.2 ( 0.16 )
0.48 ( 0.01 )
26.44 ( 0.72 )
1.41 ( 0.03 )
56.32 ( 1.72 )
5.47 ( 0.18 )
151.72 ( 4.12 ) 35.22 ( 0.94 )
34.72 ( 0.3 )
6.51 ( 0.07 )
70.64 ( 1.13 )
65.89 ( 1.13 )
120.14 ( 2.4 )
324.32 ( 7.14 )
173.21 ( 3.4 )
773.01 ( 20.78 )

K-Planner
#Actions
Time(secs)
117.04 ( 10.99 ) 34.83 ( 3.9 )
219.6 ( 10.09 )
60.63 ( 3.05 )
358.08 ( 15.8 )
94.18 ( 3.31 )
458.36 ( 14.64 ) 116.63 ( 3.24 )
17 ( 1.05 )
4.57 ( 0.35 )
33.2 ( 1.67 )
9.01 ( 0.55 )
52.12 ( 2.61 )
15.04 ( 0.95 )
80.8 ( 3.04 )
25.82 ( 1.2 )
109.72 ( 4.76 )
37.96 ( 1.72 )
150.88 ( 4.7 )
55.24 ( 2 )
188.8 ( 5.79 )
79.24 ( 2.62 )
9.68 ( 0.85 )
3.71 ( 0.25
22.04 ( 2.27 )
8.13 ( 0.71 )
45.48 ( 4.59 )
16.87 ( 1.56 )
87.04 ( 8.54 )
38.81 ( 3.53 )
35.76 ( 1.53 )
2.45 ( 0.21 )
90.52 ( 6.4 )
5.39 ( 0.61 )
107.64 ( 4.6 )
7.17 ( 0.6 )
151.52 ( 6.29 )
16.03 ( 1 )

Table 4: Comparing SDR-OBS K-Planner, domains static hidden variables.

7.3 Effects |SI |
important parameter algorithm size SI number initial states
deterministic planner recognizes. number grows, plan must distinguish
various states hence becomes accurate. However, states use larger
translation deterministic problem, difficult FF handle. examine
impact parameter tested performance SDR function size SI .
show Figure 2, plan quality (the number actions reach goal) SDR
change considerably number states. domain significant benefit
adding states Wumpus, one sees farther significant improvement beyond
8 states. expected, running time grows growth number states.
conclude, hence, that, least current domains, need use handful
states.
7.4 Belief Maintenance
examine efficiency proposed lazy belief maintenance method. natural candidate compare belief maintenance method CFF (Hoffmann & Brafman, 2006,
2005) introduced lazy belief-state maintenance method motivated considerations guided us. CFF maintains propositional formula sets propositions represent
value original proposition every time point, thus, p(t) represents value proposition p time t. Initially, formula contains formula describing initial belief state
using propositions form p(0). CFF updates formula following execution action.
executed time certain constraints propositions time + 1 must
satisfied. constraints capture add delete effects a, well frame axiom.
Given formula, check whether literal l holds possible states time t, CFF checks
594

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

Figure 2: Effect |SI | number initial states (tags) number actions (solid) execution
time (dashed) Wumpus10, doors 9, localize 9, unix 2.

whether l(t) consequence current formula. conclusions cached adding
formula simplifying using unit propagation.
Although update process used CFF quite simple, still needs maintain potentially
large formula perform satisfiability checks it. SDRs method even lazier reconstructs
formula every query. formula focused literal question, much
smaller reconstructed query, although noted above, information cached.
natural ask approach provides better trade-off.
compare two methods ran SDR belief maintenance method CFF
new method. rest algorithm remains same, two experiments
executed using random seeds, differences runtime stem different
belief maintenance method. experimented domains, report domains
belief maintenance CFF could solve.
Table 5 shows, belief maintenance method CFF scales poorly compared lazy
formula construction method. domain differences substantial localize, mainly domain planner quickly learns value propositions
quickly decouple formula set formulas disjoint sets propositions.
(2011) suggests number approaches belief state maintenance update. Unfortunately, planners available online6 belief update mechanism deeply tied
planning mechanism, currently unable independently measure performance
various belief update methods compare lazy regression approach, although
comparison interesting.
6. http://www.cs.nmsu.edu/sto/

595

fiB RAFMAN & HANI

Domain
cloghuge
ebtcs-70
elog7
doors5
doors7
doors9
localize3
localize5
localize9
localize11
localize13
localize15
unix1
unix2
unix3
Wumpus05
Wumpus10

CFF
410.39 (4.94)
481.27 (15.89)
6.88 (0.94)
4.28 (0.06)
41.05 (1.02)
283.93 (6.86)
2.32 (0.04)
7.54 (0.18)
78.08 (1.69)
109 (1.53)
458.89 (10.06)
909.12 (32.91)
0.81 (0.01)
6.29 (0.19)
427.73 (13.22)
21.07 (0.27)
688.84 (29.46)

SDR
321.28 (9.32)
17.4 (0.38)
1.81 (0.02)
3.76 (0.05)
18 (0.26)
72.5 (0.87)
1.77 (0.03)
7.12 (0.1)
72.69 (1.43)
155.6 (3.87)
396.76 (10.72)
667.22 (19.7)
0.46 (0.01)
2.01 (0.05)
9.61 (0.28)
9.8 (0.16)
102.08 (2.17)

Table 5: Comparing belief maintenance methods CFF SDR. report time (in seconds)
solving domains different methods. reported models type largest
belief maintenance method CFF could handle within given timeout.

7.5 New Domains
noted earlier existing benchmark problems contingent planning focus problems
limited features. First, dead-ends, which, noted Little Thiebaux (2007),
problematic replanning based methods. Second, fact perform well sampled
initial belief state size 2 seems imply solution sensitive identity
initial state. related fact type amount conditional effects see
current domains quite limited. Finally, success sensing bias suggests
investigate domains sensing actions carry cost, sensing necessarily
separate action. Domains sensing actions require substantial effort attain preconditions may also provide interesting insights. Many domains above, colorballs,
doors, unix also problematic isnt smart exploration method reduces
belief space faster. domains agent must move location independently
query object interest (door, ball, file). agent cant, example, sense whether
door current position, thus cutting belief space half.
thus suggest number new benchmark domains, either variations
current domains, adaptations domains POMDP community. new domains
especially designed expose difficulties current planners, point towards needed
improvements. such, SDR CLG perform well many them, sometimes fail
utterly.
first explore variations interesting Wumpus problem. Wumpus requires smart exploration sensing policy, thus one challenging benchmarks. original
Wumpus definition requires cell safe entering it. removing precondition, changing move action agent alive wumpus pit exist cell,
create domain deadends. experimented domain and, expected, SDR
variations fail utterly handle it. CLG, however, solves domains without failing. suc596

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

ceeds fully represents belief state hence detects possible states
agent would dead, would avoid consequences. SDR, even complete belief representation, makes strong assumption single initial state, plans state specifically.
state agent dead resulting plan, execute without bothering
sense deadends. true world state assumed state, eventually enter
unsafe cell would die.
Name
Wumpus 4
Wumpus 8
Wumpus 16

SDR

SDR-OBS

SDR-SR

Fail
Fail
Fail

Fail
Fail
Fail

Fail
Fail
Fail

CLG
#Actions
Time (secs)
17.7 (0.04)
0.17 (0.001)
40.5 (0.31)
2.8 (0.01)
119.7 (0.91) 182.5 (1.73)

Table 6: Wumpus domains deadends

experimented Wumpus variation agent enters unsafe cell, runs
back start cell (at 1,1). thus cost sensing wumpus. cost introduces
interesting tradeoff agent close start cell, better enter cell without
verifying safety. However, agent far beginning, better sense
safety rather pay price going back. Here, CLG fails utterly underlying revised FF
unable solve translation within given timeout. SDR variations solve model,
see sensing bias reduces performance case. Still, closely looking
plans SDR executes, observe suboptimal SDR weigh costs
sensing vs. entering possibly unsafe square causing restart. Instead, always enters
possibly unsafe square pays restart price.
Name
Wumpus 4
Wumpus 8
Wumpus 16

#Actions
13.1
(1.13)
34.84
(0.67)
67.08
(1.1)

SDR
Time (secs)
3.2
(0.04)
50.3
(1.14)
178.9
(7.6)

SDR-OBS
#Actions Time (secs)
22.2
3.4
(0.1)
(0.03)
69.12
59.23
(0.7)
(0.74)
Fail

SDR-SR
#Actions Time (secs)
17
5.4
(0.14)
(0.1)
37.9
51.5
(0.7)
(1.15)
74.8
450.1
(1.14)
(14.2)

CLG
Fail
Fail
Fail

Table 7: Wumpus domains restarts

Next, introduce domain POMDP community, known RockSample (Smith &
Simmons, 2004), motivated Mars rover task. agent (the rover) sample minerals
set nearby visible rocks. agent knows rocks are, order know
whether rock sampled, must activate sensors. version problem,
agent sensor senses presence nearby minerals, set antenna. agent
extends antenna higher, sensor senses minerals farther of. antenna
completely folded, agent senses minerals immediate vicinity. Solving problem
smartly, without visiting rocks contain minerals, requires smart sensing strategy, involving raising lowering antenna order know rocks contain minerals.
SDR currently solves problem, smartly. example, SDR observation
bias (SDR-OBS) significant advantage get additional observations,
getting long-range observations requires preparation actions.
597

fiB RAFMAN & HANI

Name
RockSample 4
RockSample 8
RockSample 12
RockSample 14

#Actions
42.2
(0.4)
85.08
(0.65)
127.24
(0.68)
142.08
(0.8)

SDR
Time (secs)
37.2
(0.36)
109.3
109.3 (1.15)
113.4
(0.79)
146.75
(1.19)

SDR-OBS
#Actions Time (secs)
45.3
32.12
(0.41)
85.5
92
(0.62)
(0.93)
125.36
101.2
(0.81)
(0.75)
145.04
128.2
(0.63)
(0.8)

SDR-SR
#Actions Time (secs)
45.6
37.2
(0.39)
(0.38)
89.12
106.04
(0.63)
(1.12)
120.72
111.66
(0.64)
(0.79)
146.84
139.3
(0.86)
(1.14)

CLG
CSU
CSU
CSU
CSU

Table 8: RockSample domains 8 8 board 4 14 rocks. CLG properly simulate
underlying world state observations given conditional effects thus performance cannot
evaluated, even though manages solve domains (denoted CSU).

Finally, also experiment another domain explored POMDP community
well-known MasterMind game, agent must guess correctly order color k
hidden pegs n possible colors. agent guesses configuration, receives feedback
form number correctly guessed colors number correctly guessed locations.
problem interesting agent never directly observes crucial features state,
i.e., peg currently correctly guessed. Furthermore, exists optimal sensing strategy
provably solves game five guesses less 4 pegs 6 colors (MasterMind 6 4)
(Koyama & Lai, 1993). CLG cannot solve problem problem-width
1. SDR SDR-SR poorly task use simplest possible strategy
guess setting see guess correct, i.e., whether result n location hits, ruling
single state guess. SDR observation bias better, observes
number location color hits, thus ruling many states guess.
noteworthy underlying FF planner particularly badly translations generated
task, executing many guesses without observing results obvious reason.

Name
MasterMind 2 4
MasterMind 3 4
MasterMind 4 6

#Actions
25.6
(0.52)
63.5
(1.5)

SDR
Time (secs)
8.2
(0.16)
46.07
(1.09)
Fail

SDR-OBS
#Actions Time (secs)
14.48
2.68
(0.134)
(0.02)
26.76
9.44
(0.47)
(0.12)
52.72
74.18
(1.08)
(0.68)

SDR-SR
#Actions Time (secs)
28.48
8.2
(0.6)
(0.2)
66.8
52.9
(1.42)
(1.22)
Fail

CLG
TF
TF
TF
TF
TF
TF

Table 9: MasterMind color guessing game. MasterMind n k stands MasterMind n pegs k colors.
CLG cannot translate problem width 1.

8. Conclusion
described SDR, new contingent planner extends replanning approach domains
partial observability uncertainty initial state. SDR also introduces novel, lazy
method maintaining information querying current belief state, nice theoretical properties. empirical evaluation shows SDR improves state art current
598

fiR EPLANNING PARTIAL NFORMATION ENSING ACTIONS

benchmark domains, scaling much better CLG. However, success current simple
sampling techniques also highlights weakness current benchmark problems. highlight this,
generated new problem domains challenging current contingent planners,
serve measure progress area.
Acknowledgments
authors grateful Alexander Albore, Hector Geffner, Blai Bonet, Son help
understanding using systems anonymous referees many useful suggestions
corrections. Ronen Brafman partially supported ISF grant 1101/07, Paul Ivanier Center Robotics Research Production Management, Lynn William Frankel Center
Computer Science.

References
Albore, A., Palacios, H., & Geffner, H. (2009). translation-based approach contingent planning.
IJCAI, pp. 16231628.
Bonet, B., & Geffner, H. (2011). Planning partial observability classical replanning: Theory experiments. IJCAI, pp. 19361941.
Brafman, R. I., & Tennenholtz, M. (2003). Learning coordinate efficiently: model-based approach. Journal Artificial Intelligence Research (JAIR), 19, 1123.
Bryce, D., Kambhampati, S., & Smith, D. E. (2006). Planning graph heuristics belief space
search. JOURNAL AI RESEARCH, 26, 3599.
Een, N., & Sorensson, N. (2003). extensible sat-solver. SAT, pp. 502518.
Hoffmann, J., & Brafman, R. I. (2005). Contingent planning via heuristic forward search
implicit belief states. ICAPS, pp. 7180.
Hoffmann, J., & Brafman, R. I. (2006). Conformant planning via heuristic forward search: new
approach. Artif. Intell., 170(6-7), 507541.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristic
search. JAIR, 14, 253302.
Kearns, M. J., & Singh, S. P. (2002). Near-optimal reinforcement learning polynomial time.
Machine Learning, 49(2-3), 209232.
Kocsis, L., & Szepesvari, C. (2006). Bandit based monte-carlo planning. ECML, pp. 282293.
Kolobov, A., Mausam, & Weld, D. S. (2010). Classical planning MDP heuristics: little
help generalization. ICAPS, pp. 97104.
Koyama, M., & Lai, T. (1993). Circumscription: form non-monotonic reasoning. Journal
Recreational Mathematics, 25, 251256.
Little, I., & Thiebaux, S. (2007). Probabilistic planning vs. replanning. ICAPS Workshop IPC:
Past, Present Future.
McCarthy, J., & Hayes, P. J. (1969). philosophical problems standpoint artificial
intelligence. Meltzer, B., & Michie, D. (Eds.), Machine Intelligence, Vol. 4, pp. 463502.
599

fiB RAFMAN & HANI

Palacios, H., & Geffner, H. (2009). Compiling uncertainty away conformant planning problems
bounded width. JAIR, 35, 623675.
Reiter, R. (1991). frame problem situation calculus: simple solution (sometimes)
completeness result goal regression. Lifshitz, V. (Ed.), AI Mathematical Theory
Computation: papers honour John McCarthy, pp. 359380.
Smith, T., & Simmons, R. (2004). Heuristic search value iteration POMDPs. UAI 2004,
Banff, Alberta.
To, S. T. (2011). impact belief state representation planning uncertainty.
IJCAI, pp. 28562857.
To, S. T., Pontelli, E., & Son, T. C. (2009). conformant planner explicit disjunctive representation belief states. ICAPS.
To, S. T., Pontelli, E., & Son, T. C. (2011). effectiveness cnf dnf representations
contingent planning. IJCAI, pp. 20332038.
To, S. T., Son, T. C., & Pontelli, E. (2010). use prime implicates conformant planning.
AAAI.
To, S. T., Son, T. C., & Pontelli, E. (2011a). Conjunctive representations contingent planning:
Prime implicates versus minimal cnf formula. AAAI.
To, S. T., Son, T. C., & Pontelli, E. (2011b). effectiveness belief state representation
contingent planning. AAAI.
Yoon, S. W., Fern, A., & Givan, R. (2007). FF-Replan: baseline probabilistic planning.
ICAPS.
Yoon, S. W., Fern, A., Givan, R., & Kambhampati, S. (2008). Probabilistic planning via determinization hindsight. AAAI, pp. 10101016.
Yoon, S. W., Ruml, W., Benton, J., & Do, M. B. (2010). Improving determinization hindsight
on-line probabilistic planning. ICAPS, pp. 209217.
Zelinsky, A. (1992). mobile robot exploration algorithm. IEEE Trans. Robotics Automation, 8(6).

600

fiJournal Artificial Intelligence Research 45 (2012) 685-729

Submitted 08/12; published 12/12

Time Complexity Approximate Heuristics
Multiple-Solution Search Spaces
Hang Dinh

htdinh@iusb.edu

Department Computer & Information Sciences
Indiana University South Bend
1700 Mishawaka Ave. P.O. Box 7111
South Bend, 46634 USA

Hieu Dinh

hieu.dinh@mathworks.com

MathWorks
3 Apple Hill Drive
Natick, 01760-2098 USA

Laurent Michel
Alexander Russell

ldm@engr.uconn.edu
acr@cse.uconn.edu

Department Computer Science & Engineering
University Connecticut
371 Fairfield Way, Unit 2155
Storrs, CT 06269-2155 USA

Abstract


study behavior search algorithm coupled heuristic h satisfying
(1 1 )h h (1 + 2 )h , 1 , 2 [0, 1) small constants h denotes optimal
cost solution. prove rigorous, general upper bound time complexity search
trees depends accuracy heuristic distribution solutions.
upper bound essentially tight worst case; fact, show nearly matching lower bounds
attained even non-adversarially chosen solution sets induced simple stochastic
model. consequence rigorous results effective branching factor search
reduced long 1 + 2 < 1 number near-optimal solutions search tree
large. go provide upper bound search graphs context
establish bound running time determined spectrum graph.
experimentally explore extent rigorous upper bounds predict behavior
natural, combinatorially-rich search spaces. begin applying solve
knapsack problem near-accurate admissible heuristics constructed efficient approximation algorithm problem. additionally apply analysis search partial
Latin square problem, provide quite exact analytic bounds number nearoptimal solutions. results demonstrate dramatic reduction effective branching factor
coupled near-accurate heuristics search spaces suitably sparse solution sets.

1. Introduction
classical search procedure (Hart, Nilson, & Raphael, 1968) method bringing heuristic
information bear natural class search problems. One celebrated features
coupled admissible heuristic function, is, one always returns lower bound
distance solution, guaranteed find optimal solution. worst-case
behavior (even admissible heuristic function) better of, say, breadthfirst search, practice intuition suggest availability accurate heuristic
decrease running time. Indeed, methods computing accurate admissible heuristic functions
various search problems presented literature (see, e.g., Felner, Korf, & Hanan,
2004). article, investigate effect accuracy running time search;
2012 AI Access Foundation. rights reserved.

fiDinh, Dinh, Michel, & Russell

specifically, focus rigorous estimates running time coupled accurate
heuristics.
initial notion accuracy adopt motivated standard framework approximation algorithms: f () hard combinatorial optimization problem (e.g., permanent
matrix, value Euclidean traveling salesman problem, etc.), algorithm efficient -approximation f runs polynomial time (1 )f (x) A(x) (1 + )f (x),
inputs x, f (x) optimal solution cost input x A(x) solution cost
returned algorithm input x. approximation algorithms community developed
efficient approximation algorithms wide swath NP-hard combinatorial optimization problems and, cases, provided dramatic lower bounds asserting various problems cannot
approximated beyond certain thresholds (see Vazirani, 2001; Hochbaum, 1996, surveys
literature). Considering great multiplicity problems successfully addressed
way (including problems believed far outside NP, like matrix permanent), natural
study behavior coupled heuristic function possessing properties. Indeed,
interesting cases (e.g., Euclidean travelling salesman, matrix permanent, knapsack), hard
combinatorial problems approximated polynomial time within fixed constant > 0;
cases, polynomial depends constant . remark, also, many celebrated
approximation algorithms provable performance guarantees proceed iterative update methods coupled bounds local change objective value (e.g., basis reduction Lenstra,
Lenstra, & Lovasz, 1981, typical primal-dual methods Vazirani, 2002).
Encouraged possibility utilizing heuristics practice natural question
understanding structural properties heuristics (and search spaces) indeed guarantee
palatable performance part , study behavior provided heuristic
function -approximation cost cheapest path solution. certain natural
situations arise approximation quality asymmetric (i.e., case admissible heuristic),
slightly refine notion accuracy distinguishing multiplicative factors two sides
approximation: say heuristic h (1 , 2 )-approximation actual cost
function h , simply (1 , 2 )-approximate, (1 1 )h h (1 + 2 )h . particular, admissible
heuristics -approximation (, 0)-approximate. call heuristic -accurate
(1 , 2 )-approximate = 1 + 2 . detailed description appears Section 2.1.
1.1 Sketch Results
initially model search space infinite b-ary tree distinguished root. problem
instance determined set nodes treethe solutions problem. cost
associated solution simply depth. search procedure equipped (i.)
oracle which, given node n, determines n S, (ii.) heuristic function h, assigns
node n tree estimate actual length h (n) shortest (descending) path
solution. Let solution set first (and hence optimal) solution appears
depth d. establish family upper bounds number nodes expanded : h
(1 , 2 )-approximation h , finds solution cost worse (1 + 2 )d expands
2b(1 +2 )d + dN1 +2 nodes, N denotes number solutions depth less
(1 + )d. See Lemma 3.1 stronger results. emphasize bound applies
solution space generalized search models non-uniform branching factors
non-uniform edge costs (see Section 5).
go show upper bound essentially tight; fact, show bound
nearly achieved even non-adversarially determined solution spaces selected according simple
stochastic rule (see Theorems 3.1 4.1.). remark bounds running time fall
rapidly accuracy heuristics increases, long number near-optimal solutions
large (although may grow exponentially). instance, effective branching factor
guided admissible -accurate heuristic reduced b N = O(bd ). However,
686

fiThe Time Complexity Approximate Heuristics

worst cases, occur search space overwhelming number near-optimal
solutions, still expand almost many nodes brute-force does, regardless heuristic
accuracy. Likewise, strong guarantees < 1 are, general, necessary effect appreciable
changes average branching factor. discussed Theorem 4.2.
establishing bounds tree-based search model, examine time complexity
graph unrolling graph equivalent tree bounding number
near-optimal solutions tree lift solution original graph. appears
Section 6. Using spectral graph theory, show number N lifted solutions
tree corresponding b-regular graph G O((1+)d ), assuming optimal solution depth
O(logb |G|) number solutions G constant, second largest eigenvalue (in
absolute value) adjacency matrix G. particular,for almost b-regular graphs
b grow size graphs, 2 b, yields effective branching
factor search graphs roughly 8b(1+)/2 heuristic -accurate.
also experimentally evaluate heuristics.
Experimental Results Relationship Practice. course, upper
bounds interesting reflect behavior search problems practice. bounds
guarantee, general, E, number nodes expanded -accurate heuristic,
satisfies
E 2bd + dN .
plausible condition N bd , simply E cbd node expansions
constant c depend (c may depend k and/or properties search
space). suggests hypothesis hard combinatorial problems suitably sparse
near-optimal solutions,
E cbd

or, equivalently,

log E log c + log b .

(1)

particular, suggests linear dependence log E .
explore hypothesis, conducted battery experiments natural search-tree
presentation well-studied Knapsack Problem. obtain admissible -accurate heuristic applying Fully Polynomial Time Approximation Scheme (FPTAS) problem due
work Ibarra Kim (1975) (see also Vazirani, 2001, p. 70), provides us
convenient method varying without changing parameters search. remark
natural search space problem quite irregular edge-weighted directed graph
avoid reopening node. Thus, search space equivalent one spanning
subtrees terms behaviors. order focus computationally nontrivial examples,
generate Knapsack instances distributions empirically hard best known exact
algorithms (Pisinger, 2005). results experiments yield remarkably linear behavior (of
log E function ) quite wide window values: indeed, tests yield R2 correlation
coefficients (of least-square linear regression model) excess 90% range (.5, 1)
Knapsack instances. See Section 7.1 details.
experimental results discussed Knapsack problem support linear
scaling (1), several actual parameters search unknown: example, cannot rule
possibility approximation algorithm, asked produce -approximation,
fact produce significantly better approximation. seems far-fetched,
behavior could provide spurious evidence linear scaling. explore hypothesis
detail, additionally explore artificial search space partial Latin square completion
(PLS) problem provide precise control (and, fact, N ). PLS problem
featured number benchmarks local search complete search methods. Roughly,
problem finding assignment values empty cells partially filled n n table
row column completed table permutation set {1, . . . , n}.
formulation problem, search space 2n-regular graph, thus brute-force branching
687

fiDinh, Dinh, Michel, & Russell

factor 2n. search space, controlling N , prove asymptotic upper bound

(1 + ) (1 + 1/) n effective branching factor coupled -accurate heuristic.
also experimentally evaluate effective branching factor admissible -accurate
heuristic (1)h , expands nodes admissible -accurate heuristic
strictly larger (1 )h .
remark PLS problem well-studied natural, invent specific search
space structure problem allows us analytically control number near-optimal
solutions. Unlike Knapsack problem, construct efficient admissible -accurate
heuristic every fixed thanks given FPTAS, known approximation algorithms PLS
problem much weakerthey provide approximations specific constants (1/e). avoid
hurdle, construct instances PLS known solution, extract heuristics
(1 )h . Despite planted solutions contrived heuristics, infrastructure provides
example combinatorially rich search space known solution multiplicity heuristic
known quality, provides means experimentally measuring relationship
heuristic accuracy running time. empirical data results remarkable agreement
theoretical upper bounds. subtly, empirically analyzing linear dependence log E
, see effective branching factor using heuristic (1 )h given PLS
search space roughly (2n)0.8 ; see Section 7.2.
far aware, first experimental results explore relationship
E. Understanding heuristic accuracy solution space structure general (and
ensuing bounds running time) problems heuristics practical interest remains
intriguing open problem. remark problems (n2 1)-puzzle,
extensively used test cases , seems difficult find heuristics accuracy sufficient
significantly reduce average branching factor. best rigorous algorithms give rather
large constant guarantees (Ratner & Warmuth, 1990; Parberry, 1995): particular, Parberry (1995)
shows one quickly compute solutions (and hence approximate heuristics)
factor 19 worse optimal; situation somewhat better random instances,
establishes 7.5-factor. See Demaines (2001) work general discussion.
Observe search algorithm privy heuristic information requires (bd ) running
time, general, find solution. High probability statements kind made
solution space selected sufficiently rich family. pessimistic lower bounds exist even
situations search space highly structured (Aaronson, 2004). results suggest
accurate heuristic information dramatic impact search, even face substantial
solution multiplicity.
article expands conference article (Dinh, Russell, & Su, 2007) complexity
-approximate heuristic function studied trees. article, generalize
asymmetric approximation, develop analogous bounds general search spaces, establishing
connection algebraic graph theory, report battery supporting experimental results.
1.2 Motivation Related Work
algorithm subject enormous body literature, often investigating
behavior relation specific heuristic search problem combination, (e.g., Zahavi, Felner,
Schaeffer, & Sturtevant, 2007; Sen, Bagchi, & Zhang, 2004; Korf & Reid, 1998; Korf, Reid, &
Edelkamp, 2001; Helmert & Roger, 2008). space complexity (Korf, 1985) time complexity
addressed various levels abstraction. Abstract formulations, involving accuracy
guarantees like consider, studied, tree models search
space possesses single solution. single solution framework, Gaschnig (1979) given
exponential lower bounds (bd ) time complexity admissible -accurate heuristics,
def
b = b/(2) b (see also Pearl, 1984, p. 180), Pohl (1977) studied restrictive
(additive) approximation guarantees h result linear time complexity. Average-case
688

fiThe Time Complexity Approximate Heuristics

analysis based probabilistic accuracy heuristics also given single-solution
search spaces (Huyn, Dechter, & Pearl, 1980). previous analysis suggested effect
heuristic functions would reduce effective branching factor search, consistent
results applied single-solution model (the special case N = 1 > 0).
single solution model, however, appears inappropriate abstraction search
problems featuring multiple solutions, recognized . . . presence multiple
solutions may significantly deteriorate ability benefit improved precision. (Pearl, 1984,
p. 192) (emphasis added).
problem understanding time complexity terms structural properties h
multiple-solution spaces studied Korf Reid (1998), Korf et al. (2001), Korf
(2000), using estimate based distribution h() values. particular, studied
abstract search space given b-ary tree concluded effect heuristic function
reduce effective depth search rather effective branching factor (Korf & Reid,
1998; Korf et al., 2001). case accurate heuristics controlled solution multiplicity,
conclusion directly contradicts findings, indicate dramatic reduction effective branching
factor cases. explain discrepancy, observe analysis relies equilibrium assumption fails accurate heuristics (in fact, fails even much weaker heuristic
guarantees, h(v) h (v) small > 0). basic structure argument, however,
naturally adapted case accurate heuristics, case yields reduction
effective branching factor. give detailed discussion Section 8.
follow-up Korf Reid (1998), Korf et al. (2001), Korfs (2000) work, Edelkamp
(2001) examined (indeed, IDA ) undirected graphs, relying equilibrium assumption.
Edelkamps new technique use graph spectrum estimate number n(`) nodes
certain depth ` brute-force search tree (same cover tree). However, unlike spectral
analysis, original search graph G, Edelkamp analyzed spectrum related
equivalence graph, quite different structural properties. Specifically, Edelkamp found
asymptotic branching factor, defined ratio n(`) /n(`1) large `, equals largest
eigenvalue adjacency matrix equivalence graph certain Puzzle problems. compare,
spectral analysis depends second largest eigenvalue adjacency matrix AG
original search graph G, largest eigenvalue AG always equals branching factor,
assuming G regular.
Additionally, analyses Korf Reid (1998), Korf et al. (2001), Korf (2000) (and
therefore, Edelkamp, 2001) focus particular subclass admissible heuristics, called consistent
heuristics. remark heuristics used experiments Knapsack problem
admissible likely inconsistent. Zhang, Sturtevant, Holte, Schaeffer, Felner (2009) Zahavi
et al. (2007) discuss usages inconsistent heuristics practice.
work explores worst-case average-case time complexity search
trees graphs multiple solutions coupled heuristics possessing accuracy
guarantees. make assumptions regarding consistency admissibility heuristics, though
several results naturally specialized case. addition studying effect
heuristic accuracy, results also shed light sensitivity distribution solutions
combinatorial structure underlying search spaces (e.g., graph eigenvalues,
measure, among things, extent connectedness graphs). far aware,
first rigorous results combining search space structure heuristic accuracy single
framework predicting behavior .

2. Preliminaries
typical search problem defined search graph starting node set goal nodes
called solutions. instance search graph, however, simulated search
cover tree without reducing running time; discussed Section 6.1. Since number
689

fiDinh, Dinh, Michel, & Russell

expansions cover tree graph larger equal original graph,
sufficient upper bound running time search cover tree. justification,
begin considering algorithm search problems rooted tree.
Problem Definition Notations. Let tree representing infinite search space,
let r denote root . convenience, also use symbol denote set vertices
tree . Solutions specified nonempty subset nodes . edge
assigned positive number called edge cost. vertex v , let
SubTree(v) denote subtree rooted v,
Path(v) denote path root r v,
g(v) denote total (edge) cost Path(v),
h (v) denote cost least costly path v solution SubTree(v). (We write
h (v) = solution exists.)
objective value search problem h (r), cost cheapest path root r
solution. cost solution value g(s). solution cost equal h (r)
referred optimal.
algorithm best-first search employing additive evaluation function f (v) = g(v) +
h(v), h function heuristically estimates actual cost h . Given heuristic
function h : [0, ], algorithm using h defined search problem tree
described follows:
Algorithm 1 search tree
1. Initialize Open := {r}.
2. Repeat Open empty:
(a) Remove Open node v function f = g + h minimum.
(b) v solution, exit success return v.
(c) Otherwise, expand node v, adding children Open.
3. Exit failure.
known (e.g., Dechter & Pearl, 1985, Lemma 2) time terminates,
always vertex v present Open v lies solution path f (v) ,
min-max value defined follows:


def
= min
max f (u) .
(2)
sS

uPath(s)

fact leads following node expansion conditions:
vertex v expanded (with heuristic h) must f (v) . (cf., Dechter & Pearl,
1985, Thm. 3). say vertex v satisfying f (v) potentially expanded .
vertex v
max

f (u) <

uPath(v)

must expanded (with heuristic h) (cf., Dechter & Pearl, 1985, Thm. 5). particular,
function f monotonically increases along path root r v, node v
must expanded f (v) < .
690

fiThe Time Complexity Approximate Heuristics

value obtained solution path search terminates (Dechter &
Pearl, 1985, Lemma 3), implies upper bound cost solution found
search.
remark h reasonable approximation h along path optimal solution,
immediately provides control . particular:
Proposition 2.1. (See also Davis, Bramanti-Gregor, & Wang, 1988) Suppose 1,
h(v) h (v) vertices v lying optimal solution path; h (r).
Proof. Let optimal solution. v Path(s),
f (v) g(v) + h (v) = g(v) + (g(s) g(v)) g(s) .
Hence

max

f (v) g(s) = h (r).

vPath(s)

particular, = h (r) heuristic function satisfies h(v) h (v) v ,
case heuristic function called admissible. observation recovers fact
always finds optimal solution coupled admissible heuristic function (cf., Pearl,
1984, Thm. 2, 3.1). Admissible heuristics also possess natural dominance property (Pearl, 1984,
Thm. 7, p. 81): admissible heuristic functions h1 h2 , h1 informed
h2 , i.e., h1 (v) > h2 (v) v \S, using h1 dominates using h2 , i.e., every node
expanded using h1 also expanded using h2 .
2.1 Approximate Heuristics
Recall introduction shall focus heuristics providing (1 , 2 )-approximation
actual optimal cost reach solution:
Definition. Let 1 , 2 [0, 1]. heuristic function h called (1 , 2 )-approximate
(1 1 )h (v) h(v) (1 + 2 )h (v)

v .

(1 , 2 )-approximate heuristic simply called -approximate 1 2 .
heuristic function h (1 , 2 )-approximate, shall say h heuristic error 1 + 2 , h
(1 + 2 )-accurate.
see below, two approximation factors control performance search
rather different ways: 1 effects running time , 2 impact
running time quality solution found . Particularly, special case 2 = 0
corresponds admissible heuristics, always finds optimal solution. general,
Proposition 2.1, have:
Fact 1. h (1 , 2 )-approximate, (1 + 2 )h (r).
Hence, solution found using (1 , 2 )-approximate heuristic must cost
(1 + 2 )h (r) thus exceeds optimal cost multiplicative factor equal
2 .
Definition. Let 0. solution cost less (1 + )h (r) called -optimal solution.
Assumptions. simplify analysis now, assume search tree b-ary
every edge unit cost unless otherwise specified. case, cost g(v) simply
depth node v h (v) shortest distance v solution descendant v.
Throughout, parameters b 2 (the branching factor search space) 1 (0, 1], 2 [0, 1]
(the quality approximation provided heuristic function) fixed. rule case
1 = 0 simplicity.
691

fiDinh, Dinh, Michel, & Russell

3. Upper Bounds Running Time Trees
going establish upper bounds running time search tree model.
first show generic upper bound applies solution space. apply
generic upper bound natural stochastic solution space model.
3.1 Generic Upper Bound
mentioned introduction, begin upper bound time complexity
search depending weight distribution solution set, addition heuristics
approximation factors. shall, fact, upper bound number potentially expanded nodes,
clearly upper bound number nodes actually expanded :
Lemma 3.1. Let solution set whose optimal solutions lie depth d. Then, every 0,
number nodes expanded search tree (1 , 2 )-approximate heuristic

2b(1 +2 +1)d + (1 1 )dN1 +2
nodes, N number -optimal solutions.
presence independent parameter offers flexible way apply upper bound
Lemma 3.1. particular, applying Lemma 3.1 = 1 using fact 1 1 1,
arrive upper bound 2b(1 +2 )d + dN1 +2 mentioned introduction. bound works
best when1 N1 +2 = (b(1 +2 )d ). general, N1 +2 = O(b(1 +2 )d ), choose least
1 N1 +2 = O(b(1 +2 )d ). opposite case, N1 +2 = (b(1 +2 +c)d )
positive constant c 1 1 , obtain better bound choosing = 1 c/(1 1 ) < 1, since
N1 +2 dominates terms (b(1 +2 +1)d ) N1 +2 given choice .
Proof Lemma 3.1. Let = h (r) let = 1 + 2 . Consider node v lie
path root -optimal solution, h (v) (1 + )d g(v).
f (v) g(v) + (1 1 )[(1 + )d g(v)] = (1 1 )(1 + )d + 1 g(v) .
Recall node potentially expanded f -value less equal . Since
(1 + 2 )d, node v potentially expanded
(1 1 )(1 + )d + 1 g(v) > (1 + 2 )d .

(3)

Since 1 > 0, inequality (3) equivalent
g(v) > (2 /1 /1 + 1 + )d = (1 + 2 + 1 )d .
words, node depths range

(1 + 2 + 1 )d, (1 + 2 )d
potentially expanded lies path root -optimal solution.
hand, -optimal
solution path, (1 1 )d nodes depths

(1 + 2 + 1 )d, (1 + 2 )d . Pessimistically assuming nodes depth
(1 + 2 + 1 )d potentially expanded addition paths -optimal solutions
P`
yields statement lemma. (Note b 2, i=0 bi 2b` every potentially
expanded node v must depth g(v) f (v) (1 + 2 )d.)
1. Recall asymptotic notations: f (n) = (g(n)) means exist constants c1 , c2 > 0 c1 g(n)
f (n) c2 g(n) sufficiently large n; f (n) = (g(n)) means exists constant c > 0 cg(n) f (n)
sufficiently large n.

692

fiThe Time Complexity Approximate Heuristics

3.2 Upper Bound Natural Search Space Model
actual time complexity depend, course, precise structure h, show
bound essentially tight rich family solution spaces. consider sequence
search problems increasing difficulty, expressed terms depth optimal solution.
Stochastic Solution Space Model. parameter p [0, 1], consider solution set
obtained independently placing node probability p.
setting, random variable written Sp . solutions distributed according Sp ,
observe expected number solutions depth precisely pbd p = bd
optimal solution lies depth constant probability. reason, focus specific
values pd = bd consider solution set Spd > 0. Recall model,
likely optimal solutions lie depth and, generally, see
high probability optimal solutions particular subtree located near depth (with
respect root subtree). make precise below.
Lemma 3.2. Suppose solutions distributed according Spk . node v
> 0,
td
1 2btd Pr[h (v) > t] eb .
Pt
Proof. tree SubTree(v), n = i=0 bi = (bt+1 1)/(b 1) nodes depths less,
Pr[h (v) > t] = (1 bd )n .

1 nbd (1 bd )n exp nbd .
first inequality obtained applying Bernoullis inequality, last one implied
fact 1 x ex x. Observing
bt

bt+1 1
2bt
b1

b 2 completes proof.
Observe Spd model, conditioned likely event optimal solutions appear
depth d, expected number -optimal solutions (bd ). situation, according
Lemma 3.1, expands O(b(1 +2 +1)d ) + O(db(1 +2 )d ) vertices expectation,
0. leading exponential term bound equal
max {(1 + 2 + 1 )d, (1 + 2 )d} ,
minimal = 1. suggests best upper bound inferred
family bounds Lemma 3.1 poly(d)b(1 +2 )d (for Spd ).
discussing average-case time complexity search, record following wellknown Chernoff bound, used control tail bounds analysis later.
Lemma 3.3 (Chernoff bound, Chernoff, 1952). Let Z sum mutually independent indicator
random variables expected value = E [Z]. > 0,


e
Pr[Z > (1 + )] <
.
(1 + )1+
detailed proof found book Motwani Raghavan (1995). several cases
below, know exactly expected value variable wish apply
tail bound Lemma 3.3, compute sufficiently good upper bounds expected value.
order P
apply Chernoff
case, actually require monotonicity argument:
Pbound
n
n
0
0
Z =
X

Z
=
X
i=1
i=1 sums independent identically distributed (i.i.d.)
indicator random variables E [Xi ] E [Xi0 ], Pr[Z > ] Pr[Z 0 > ] .
argument applying Lemma 3.3 = e 1, obtain:
693

fiDinh, Dinh, Michel, & Russell

Corollary 3.1. Let Z sum n i.i.d. indicator random variables E [Z] n,
Pr[Z > e] < e .
Adopting search space whose solutions distributed according Spd , ready
bound running time average guided (1 , 2 )-approximate heuristic:
3

Theorem 3.1. Let sufficiently large. probability least 1 ed e2d , search
tree using (1 , 2 )-approximate heuristic function expands 12d4 b(1 +2 )d vertices
solutions distributed according random variable Spd .
Proof. Let X random variable equal total number nodes expanded
(1 , 2 )-approximate heuristic. course exact value of, say, E [X] depends h; prove
upper bounds achieved high probability (1 , 2 )-approximate h. Applying Lemma 3.1
= 1, conclude

X 2b(1 +2 )h (r) + (1 1 )h (r)N1 +2 .
Thus suffices control h (r) number N1 +2 (1 + 2 )-optimal solutions.
utilize fact Spd model, optimal solutions unlikely located
far depth d. end, let Efar event h (r) > + < set

later. Lemma 3.2 immediately gives Pr[Efar ] eb .
Observe conditioned Efar , h (r) d+ N1 +2 Z, Z random
variable equal number solutions depth (1 + 1 + 2 )(d + ).

(1+1 +2 )(d+)
= 2b(1 +2 )d+(1+1 +2 ) < 2b(1 +2 )d+3
E[Z] b 2b

and, applying Chernoff bound Corollary 3.1 control Z,
h



3
Pr Z > 2eb(1 +2 )d+3 exp 2b(1 +2 )d+3 e2b .
Letting Ethick event Z 6b(1 +2 )d+3 , observe
h

3
Pr[Ethick ] Pr Z > 2eb(1 +2 )d+3 e2b .
summarize: neither Efar Ethick occurs,
X 2b(1 +2 )(d+) + (1 1 )(d + )6b(1 +2 )d+3
6(d + )b(1 +2 )d+3
12db(1 +2 )d+3 .
Hence,
h


3
Pr X > 12db(1 +2 )d+3 Pr[Efar Ethick ] eb + e2b .
infer bound stated theorem, set b = b(1 +2 )d+3 = d3 b(1 +2 )d , completing
proof.
Remark similar methods, trade-offs error probability resulting bound
number expanded nodes obtained.
694

fiThe Time Complexity Approximate Heuristics

4. Lower Bounds Running Time Trees

establish upper bounds Theorem 3.1 tight within O(1/ d) term
exponent. begin recording following easy fact solution distances discrete
model.
Fact 2. Let h (r) nonnegative integer. every solution s, node v
Path(s) h (v) = .
Proof. Fix distance h (r). prove lemma induction depth solutions.
lemma clearly holds optimal solutions. Consider solution may optimal,
let v Path(s) node level far h (v) . h (v) < ,
must another solution s0 SubTree(v) closer v. induction assumption,
node v 0 Path(s0 ) h (v 0 ) = . node v 0 must ancestor v, since distance
v s0 less distance v 0 s0 least , completing
proof.
proceed lower bound.
Theorem 4.1.Let sufficiently large. solutions distributed according Spd , probability
least 1 b , exists (1 , 2 )-approximate heuristic function
h number
vertices expanded search tree using h least b(1 +2 )d4 /8.
Proof. plan define pathological heuristic function forces expand many
nodes possible. Note heuristic function allowed overestimate h . Intuitively,
wish construct heuristic function overestimates h nodes close solution
underestimates h nodes far solutions, leading astray whenever possible. Recall
every vertex v, likely solution lying depth SubTree(v). Thus use
quantity h (v) formalize intuitive notion node v close solution,
quantity < determined later. heuristic function h formally defined follows:
(
(1 + 2 )h (v) h (v) ,
h(v) =
(1 1 )h (v) otherwise.
Observe chance node overestimated small since, Lemma 3.2,
Pr[v overestimated] = Pr[h (v) ] 2b

(4)

node v. Also note node v overestimated ancestor, f
values monotonically increase along path root v.
Naturally, also wish ensure optimal solution close root. Let Eclose
event h (r) . Lemma 3.2,
Pr[Eclose ] 2b .
see conditioned event Eclose , means h (r) > , every
solution obscured overestimated node close solution. Concretely,
issues integrality, Fact 2 asserts every solution s, must node v
path root h (v) = , long < h (r).
Assume Eclose : whenever h (v) = , g(v) h (r) (d ) > 0 h(v) =
(1 + 2 )(d ), thus f (v) > (1 + 2 )(d ). Since every solution obscured
overestimated node whose f value larger (1 + 2 )(d ), > (1 + 2 )(d ),
min-max value defined (2). follows node v must expanded Path(v)
695

fiDinh, Dinh, Michel, & Russell

contain overestimated node f (v) (1 + 2 )(d ). Path(v) contain
overestimated node, f (v) = g(v) + (1 1 )h (v),
f (v) (1 + 2 )(d ) (1 1 )h (v) (1 + 2 )(d ) g(v) ,
since 1 < 1. Therefore, say node v required overestimated node Path(v)
(1 1 )h (v) (1 + 2 )(d ) g(v). recap, conditioned Eclose , set required nodes
subset set nodes expanded search using defined heuristic function. use
Chernoff bound control size R` denotes set non-required nodes depth
`.
Let v node depth ` < (1 + 2 )d. Equation (4) implies
Pr[ overestimated node Path(v)] 2`b < 1/16 .
last inequality holds sufficiently large d, long = poly(d). hand,
1 < 1,




(1 + 2 )(d ) `
Pr v R` = Pr h (v) >
1 1


(1+2 )(d)`

11
exp b
(by Lemma 3.2)


(1 +2 )d(1+2 )`
11
.
(5)
= exp b
set ` = (1 + 2 )d (1 + 2 ) logb 4. Equation (5) implies
logd 4


Pr v R` exp b 11 e4 1/16 .
case 1 = 1, event (1 1 )h (v) > (1 + 2)(d ) ` never
given value
fi
fi happens
` set. Hence, case, Pr v R` 1/8 E fiR` fi b` /8. Applying
Chernoff bound Corollary 3.1 yields
fi fi

Pr fiR` fi > eb` /8 exp(b` /8) .
fi fi
Let Ethin event fiR` fi b` /2. Since b` /2 > eb` /8,
Pr[Ethin ] exp(b` /8) .
Putting pieces together,


`
Pr expands less b` /2 nodes Pr[Eclose Ethin ] 2b + eb /8 .


Setting = 2 ` = (1 + 2 )d 2(1 + 2 ) logb 4, thus
h



Pr expands less b(1 +2 )d4 /8 nodes b
sufficiently large d.
contrast, explore behavior adversarially selected solution set;
achieves lower bound nearly tight (in comparison general upper bound
worst-case running time obtained setting = 0 bound Lemma 3.1 above).
696

fiThe Time Complexity Approximate Heuristics

Theorem 4.2. > 1, exists solution set whose optimal solutions lie depth
(1 , 2 )-approximate heuristic function h tree using h expands
least b(1+2 )d12 /1 nodes.
Proof. Consider solution set 2 -optimal solutions share ancestor u lying depth
1. Furthermore, contains every node depth (1 + 2 )d descendant u,
= h (r).
define (1 , 2 )-approximate heuristic h follows: h(u) = (1 + 2 )h (u) h(v) =
(1 1 )h (v) v 6= u. heuristic, every 2 -optimal solution hidden search
procedure ancestor u. Precisely, since f (u) = 1 + (1 + 2 )(d 1) = (1 + 2 )d 2 , every
2 -optimal solution (which descendant u)
max
vPath(s)

f (v) f (u) = (1 + 2 )d 2 .

Thus (1 + 2 )d 2 , min-max value defined Equation (2).
Let v node depth ` (1 + 2 )d lie inside SubTree(u). Note
f values monotonically increase along path root r v, implies node v must
expanded f (v) < . hand, since every non-descendant u depth (1 + 2 )d
solution, ` + h (v) (1 + 2 )d, thus
f (v) ` + (1 1 )[(1 + 2 )d `] = (1 1 )(1 + 2 )d + 1 ` .
Hence, node v must expanded (1 1 )(1 + 2 )d + 1 ` < (1 + 2 )d 2 , equivalent
` < (1 + 2 )d 2 /1 . follows number nodes expanded least
(1+2 )d12 /1

X
`=0

(1+2 )d22 /1

b`

X

b` = b(1+2 )d12 /1 .

`=0

According Theorem 4.2, set 2 = 0 let 1 arbitrarily small provided 1 > 0,
obtain near-accurate heuristic forces expand least many bd1 nodes.
lower bound partially explains perform poorly, even almost perfect
heuristic, certain applications (Helmert & Roger, 2008): adversarially-chosen solution set
given proof Theorem 4.2 overwhelming number near-optimal solutions. Indeed,
N+2 b(1+2 )d b(1+2 )d1 b(1+2 )d1
> 0.

5. Generalizations: Non-uniform Edge Costs Branching Factors
section, discuss generic upper bounds Lemma 3.1 generalized apply
natural search models non-uniform branching factors non-uniform
edge costs; Section 6, show extended general graph search models.
consider general search tree without assumptions uniform branching factor
uniform edge costs. argument given proof Lemma 3.1, derive assertion
heuristic (1 , 2 )-approximate, node cost (1 + 2 + 1 )c
potentially expanded lie (1 + 2 )-optimal solution path,
arbitrary nonnegative number c = h (r) optimal solution cost.
Hence, number nodes potentially expanded (1 , 2 )-approximate heuristic
bounded


F (1 + 2 + 1 )c + R (1 + 2 + 1 )c , 1 + 2 .
(6)
697

fiDinh, Dinh, Michel, & Russell

F () number nodes cost , call free nodes; R(, )
number nodes cost range (, (1 + 2 )c ] lie -optimal solution path,
call restricted nodes.
bound number free restricted nodes, respectively, assume branching
factors upper bounded edge costs lower bounded. Let B 2 maximal branching
factor let minimal edge cost. Since node cost must lie
depth larger /m,
F () 2B /m .
-optimal solution path, ((1 + 2 )c )/m nodes cost range
(, (1 + 2 )c ]. Thus,
(1 + 2 )c
N .
R(, )

Letting = (1 + 2 + 1 )c , = 1 + 2 , applying bounds F () R(, )
bound (6), obtain another upper bound number expanded nodes heuristic
(1 , 2 )-approximate:


2B (1 +2 +1)c

/m

+ N1 +2 (1 1 )c /m

(7)

0. equation (7) generalized version bound Lemma 3.1. Substituting
= 1 (7), arrive following simpler upper bound number expanded nodes:
2B (1 +2 )c



/m

+ N1 +2 (1 1 )c /m .

(8)

6. Bounding Running Time Graphs
previous parts, established bounds running time tree model.
apply bounds graph model. order that, first unroll
graph cover tree, bound number solutions lifted cover tree.
6.1 Unrolling Graphs Trees
preceding generic upper bounds developed tree-based models; section discuss
natural extension general graph search models. principal connection obtained unrolling graph tree expands least many nodes original
graph (including repetitions). specifically, given directed graph G starting node x0 G,
define cover tree (G) whose nodes one-to-one correspondence finite-length paths
G x0 . shall write path (x0 , . . . , x` ) G node (G). root (G)
(x0 ). parent node (x0 , x1 , . . . , x` ) (G) node (x0 , x1 , . . . , x`1 ), edge cost
two nodes (x0 , x1 , . . . , x`1 ) (x0 , x1 , . . . , x` ) (G) equals cost edge
(x`1 , x` ) G. Hence, node P (G), cost value g(P ) equal total edge
cost path P G. node (x0 , . . . , x` ) (G) designated solution whenever x`
solution G.
node (G) corresponds path ending node x G called copy x.
Observe solution G may lift multiple times solutions (G), node G may
multiple copies (G). Figure 1 illustrates example unrolling graph cover tree.
example, node solution graph first two copies cover tree correspond
paths (0, 3, s) (0, 5, 3, s), 0 starting node given graph.
search graph G described Algorithm 2 below, h(x) heuristic
node x, g(x) cost current path x0 x, c(x, x0 ) denotes cost
edge (x, x0 ) G. assume value h(x) depends x, i.e., h(x) depend
particular path x0 x. Unlike search tree, node x Open Closed,
698

fiThe Time Complexity Approximate Heuristics

0
1
6

1

3

5

2

2



3

6

1

1

5

2



2
0

5

3


Figure 1: Unrolling graph cover tree.
Algorithm 2 also keeps track current path P x0 x pointers,
current f -value x equal g(P ) + h(x). current path cheapest path x0 x
passes nodes expanded.
Algorithm 2 search graph (Pearl, 1984, p. 64)
1. Initialize Open := {x0 } g(x0 ) := 0.
2. Repeat Open empty.
(a) Remove Open place Closed node x function f = g + h
minimum.
(b) x solution, exit success return x.
(c) Otherwise, expand x, generating successors. successor x0 x,
i. x0 Open Closed, estimate h(x0 ) calculate f (x0 ) = g(x0 ) + h(x0 )
g(x0 ) = g(x) + c(x, x0 ), put x0 Open pointer back x.
ii. x0 Open Closed, compare g(x0 ) g(x) + c(x, x0 ). g(x) + c(x, x0 ) <
g(x0 ), direct pointer x0 back x reopen x0 Closed.
3. Exit failure.
consider search cover tree (G) graph G using heuristic function h:
node P (G), set heuristic value h(P ) equal h(x) P copy node
x G, i.e., P path G x0 x. Observe cover tree (G) graph G share
threshold (defined Equation (2)). Hence, whenever node x G expanded
current path P , must g(P ) + h(x) , implies P potentially expanded
search cover tree (G). shows following fact:
Fact 3. number node expansions G number nodes potentially
expanded (G) using heuristic.
Here, node expansion, mean execution expand step , i.e. Step (2c). Note
that, general, node G expanded many times along different paths.
Remark running time cover tree also used upper bound running time
iterative-deepening (IDA ) graph. Recall running time IDA dominated
last iteration. hand, last iteration IDA G merely depth-first search
699

fiDinh, Dinh, Michel, & Russell

cover tree (G) cost threshold . Hence, number expansions last
iteration IDA number nodes potentially expanded cover
tree.
So, upper-bound time complexity IDA graph, suffices unroll graph
cover tree apply upper bounds number nodes potentially expanded
cover tree. particular, bound Equation (7) applied directly search
G.
Note bounds applied directly, problem determining exactly
solutions G lift solutions cover tree depends delicate structural properties G
specifically, depends growth number distinct paths x0 solution
function length paths. particular, order obtain general results
complexity model, must invoke measure connectedness graph G.
show bound complexity terms spectral properties G. choose
approach offers single parameter notion connectedness (the second eigenvalue)
analytically tractable actually analyzed bounded many graphs
interest, including various families Cayley graphs combinatorial graphs methods
conductance.
6.2 Upper Bound via Graph Spectra
shall consider undirected2 graph G n vertices search space. Let x0 starting
node let set solutions G. simplicity, assume G b-regular (2 < b n)
edge costs uniformly equal one, cover tree (G) b-ary uniform edge cost.
assume, additionally, |S| treated constant n .
Fact 3 Lemma 3.1, number node expansions G (1 , 2 )approximate heuristic 2b(1 +2 )d + dN1 +2 , optimal solution cost,
equals optimal solution depth (G), N number -optimal solutions (G).
goal upper bound N (of cover tree (G)) terms spectral properties G.
introduce principal definitions spectral graph theory below, primarily set
notation. complete treatment spectral graph theory found work Chung
(1997).
Graph Spectra. graph G, let adjacency matrix G: A(x, y) = 1 x adjacent
y, 0 otherwise. real, symmetric matrix (AT = A) thus real eigenvalues
b = 1/b
b = 1 2 . . . n b, spectral theorem (Horn & Johnson, 1999). Let
b
denote normalized adjacency matrix G; eigenvalues 1 = 1 2 . . . n 1,
referred spectrum G, = /b. eigenvalues, along
associated eigenvectors, determine many combinatorial aspects graph G. applications
def
graph eigenvalues, however, critical value = (G) = max {|2 |, |n |} invoked
and, moreover, real parameter interest gap = /b largest eigenvalue
1 = 1 normalized adjacency matrix. Intuitively, measures connectedness G.
Sparsely connected graphs 1; n-cycle, example, = 1 O(1/n). hypercube
N = 2n vertices = 1 (1/ log N ). Similar bounds , known many
families Cayley graphs. Random graphs, even constant degree b 3, achieve = o(1)
high probability. fact, recent result Friedman (2003) strengthens this:
Theorem 6.1. (Friedman, 2003) Fix real c > 0 integer b 2. probability
1 o(1) (as n ),

(Gn,b ) 2 b 1 + c
2. one produce analogous cover tree directed case, spectral machinery apply next
section somewhat complicated presence directed edges. See work Chung (2006) Horn
Johnson (1999, Perron-Frobenius theorem) details.

700

fiThe Time Complexity Approximate Heuristics

Gn,b random b-regular graph n vertices.
remark non-bipartite connected graph diameter D, always
b 1/(Dn). stronger conditions, graph vertex-transitive (which say
pair v0 , v1 vertices G automorphism G sending v0 v1 ), one
b (1/D2 ) (Babai, 1991). vertex transitivity strong condition, satisfied
many natural algebraic search problems (e.g., 15-puzzle-like search spaces Rubiks cube).
principal spectral tool apply section described Lemma 6.1 below. begin
notation.
Notations. function G viewed column vector indexed vertices G
vice versa. vertex x G, let 1x denote function G value 1 x
0 everyPvertex x. real-valued functions , G, define
pinner product
h, = xG (x)(x). shall use k k denote L2 -norm, i.e., kk = h,
function G.
b symmetric real, spectral theorem (Horn & Johnson, 1999),
Recall since
exist associated eigenfunctions 1 , . . . , n form orthonormal basis space real-valued
b particular,
functions G, eigenfunction associated eigenvalue A.
b

Pn Ai = ki k = 1 i, hi , j = 0 6= j. basis, write
= i=1 h, real-valued function G.
Lemma 6.1. Let G undirected b-regular graph n vertices, = (G)/b.
probability distributions p q vertices G, integers s, 0,
fi
fiD


E
fi
fi
b p,
bt q 1 fi s+t kpk kqk 1 .
fi
fi
nfi
n
Pn

Pn
ai q = j=1 bj j ai = hp, , bj = hq, j i.
+
* n
n
n
n

E
X
X
X
X




b p,
bq =
s+t
ai bi .
ai bj si tj hi , j =
bj j j =

ai ,


Proof. Write p =

i=1

i=1

i,j=1

j=1

i=1

Cauchy-Schwartz inequality,
n
X

v
!
! n
u n
X
u X
2
2

|ai bi |
bi = kpk kqk .
ai

i=1

i=1

i=1
1/n

Without loss generality, assume 1 (x) =
vertices x G. Since p probability
distribution,
X
1
1 X
p(x) = .
a1 = hp, 1 =
p(x)1 (x) =
n
n
xG

Similarly, b1 =

1 .
n

Thus, a1 b1 =

xG

1
n.


fi n
fi
fiD
fi
E 1 fifi fiX
fi
fi
fi
s+t
b p,
bt q fi = fi
fi


b
fi



fi
fi
nfi fi
i=2

s+t

n
X

|ai bi |

(as = max |i |)
2in

i=2



s+t kpk kqk
completing proof lemma.
701

1
n


,

fiDinh, Dinh, Michel, & Russell

Lemma 6.1 hand, establish following bound number paths prescribed
length ` connecting pair vertices. apply control number -optimal solutions
cover tree G. Let P` (u, v) denote number paths G length ` u v.
Lemma 6.2. Let G undirected b-regular graph n vertices, = (G). vertices
u, v G ` 0,
fi
fi


`fi
fi
fiP` (u, v) b fi ` 1 1 < ` .
fi
nfi
n
Proof. Since P` (u, v) number `-length paths u v, P` (u, v) = b` p(`) (v),
p(`) (v) probability natural random walk G length
` starting
u ends

E


ff
P` (u,v)
`
(`)
`
(`)
(`)
b
b
= 1v , 1u . Applying Lemma 6.1
v. Since p = 1u p (v) = 1v , p ,
`
b

yields
fi
fi fi




E 1 fifi
fi P` (u, v)
1 fifi fifiD
`
b
fi
fi ` k1v k k1u k 1 = ` 1 1 .
1
,

1


=
v
u
fi b`
nfi fi
nfi
n
n
= /b, multiplying sides last inequality b` completes proof lemma.
major consequence Lemma 6.2 application following bound number
-optimal solutions (G).
Theorem 6.2. Let G undirected b-regular graph n vertices, = (G). sufficiently
large n 0, number -optimal solutions (G)
(1+)d

b
N < 2|S|
+ (1+)d ,
n
depth optimal solutions (G), set solution nodes G.
Proof. solution S, number copies level ` (G) equals P` (x0 , s),
less b`/n + ` Lemma 6.2. Hence, number solutions level ` (G)
`

X
b
P` (x0 , s) < |S|
+ ` .
(9)
n
sS

Summing sides (9) ` ranging (1 + )d,


(1+)d
(1+)d
(1+)d
X X
X
X
1
N =
P` (x0 , s) < |S|
b` +
` .
n
`=d sS

`=d

`=d

n sufficiently large, 2. Thus,


1 (1+)d
N < |S|
2b
+ 2(1+)d .
n

Note b(1+)d
/n = O(1) =O(logb n). mentioned earlier (Theorem 6.1), b-regular
graphs 2 b 1 + o(1) 2 b. Assuming G spectral property = O(logb n),
Theorem 6.2 gives


N = O((1+)d ) = 2(1+)d b(1+)d/2 .
cases, number node expansions G using (1 , 1 )-approximate heuristic
O(d2(1+)d b(1+)d/2 ), implies effective branching factor roughly bounded
21+ b(1+)/2 < 8b(1+)/2 .
702

fiThe Time Complexity Approximate Heuristics

7. Experimental Results
discussed introduction, bounds established thus far guarantee E, number
nodes expanded using -accurate heuristic, satisfies
E 2bd + dN cbd
assumption N bd . (Here, before, b branching factor, optimal
solution depth, c constant.) suggests hypothesis hard combinatorial
problems suitably sparse near-optimal solutions,
log E log b + .

(10)

constant determined search space heuristic independent .
particular, suggests linear dependence log E . experimentally investigated
hypothesized relationship family results involving Knapsack problem partial
Latin square problem. far aware, first experimental results specifically
investigating dependence.
remark order experimental framework really cast light bounds
presented , one must able furnish heuristic known approximation guarantees.
7.1 Search Knapsack
begin describing family experimental results search coupled approximate
heuristics solving Knapsack problem. problem extremely well-studied
wide variety fields including finance, operations research, cryptography (Kellerer, Pferschy,
& Pisinger, 2004). Knapsack problem NP-hard (Karp, 1972), efficient algorithm
solve exactly unless NP = P. Despite that, problem admits FPTAS (Vazirani, 2001, p.
70), algorithm return -approximation optimal solution time polynomial
1/ input size. use FPTAS construct approximate admissible heuristics
search, yields exact algorithm Knapsack may expand far fewer nodes
straightforward exhaustive search. (Indeed, resulting algorithm is, general, efficient
exhaustive search.)
7.1.1 Search Model Knapsack
Consider Knapsack instance given n items, let [n] = {1, . . . , n}. item [n]
weight wi > 0 profit pi > 0. knapsack capacity c > 0. task find set items
maximal total profit total weight c. Knapsack instance
denoted tuple h[n], p, w, ci. Knapsack instance restricted subset X [n] denoted
hX, p, w, ci. subset X [n], let w(X) P
p(X) denote P
total weight
total profit, respectively, items X, i.e., w(X) = iX wi p(X) = iX pi .
Search Space. represent Knapsack instance h[n], p, w, ci search space follows.
state (or node) search space nonempty subset X [n]. move (or edge) one state
X another state taken removing item X. cost move profit
removed item. state X [n] designated solution w(X) c. initial state
set [n]. See Figure 2 example search space n = 4.
search space irregular directed graph whose out-degrees span wide range, 2
n 1. Moreover, two states X1 , X2 X2 X1 [n], |X1 \ X2 |! paths
search graph X1 X2 . Moreover, every path X1 X2 cost equal
p(X1 ) p(X2 ). feature search graph makes behave like spanning subtree
graph: state search graph reopened. Hence, state X [n], cost
703

fiDinh, Dinh, Michel, & Russell

{1, 2, 3, 4}

{1, 2, 3}

{1, 2}

{1, 2, 4}

{1, 3}

{1, 3, 4}

{1, 4}

{1}

{2, 3}

{2}

{2, 3, 4}

{2, 4}

{3}

{3, 4}

{4}

Figure 2: search space Knapsack instance given set 4 items {1, 2, 3, 4}. Solution
states edge costs indicated figure.
path starting state X
g(X) = p([n] \ X) = p([n]) p(X) ,
cheapest cost reach solution state X [n]
h (X) = p(X) Opt(X) ,
Opt(X) total profit optimal solution Knapsack instance hX, p, w, ci, i.e.,
Opt(X) = max {p(X 0 ) | X 0 X w(X 0 ) c} .
def

Observe solution state X [n] search space h[n], p, w, ci optimal
g(X ) minimal, equivalently, p(X ) maximal, means X optimal solution
Knapsack instance h[n], p, w, ci.
Heuristic Construction. Fix constant (0, 1). order prove linear dependence
log E , wish efficient -accurate heuristic H aforementioned Knapsack
search space h[n], p, w, ci. Moreover, order guarantee solution returned
search optimal, insist H admissible, H must satisfy:
(1 )h (X) H (X) h (X) X [n] .
main ingredient constructing heuristic FPTAS described book Vazirani
(2001, p. 70). FPTAS algorithm, denoted A, returns solution total
profit
least (1 )Opt(X) Knapsack instance hX, p, w, ci runs time |X|3 / ,
(0, 1). nonempty subset X [n], let (X) denote total profit solution
returned algorithm error parameter Knapsack instance hX, p, w, ci.
(0, 1),
(1 )Opt(X) (X) Opt(X) ,
implies
p(X)

(X)
h (X) p(X) (X) .
1

(11)

(X)
Thus may work heuristic H (X) = p(X) A1
, guarantees admissibility.
However, definition H guarantee -approximation H : definition,
condition (1 )h (X) H (X) equivalent

(1 )h (X) p(X)
704

(X)
,
1

(12)

fiThe Time Complexity Approximate Heuristics

always hold. Since h (X) p(X) (X), condition (12) satisfied
(1 )(p(X) (X)) p(X)

(X)
.
1

(13)

(X)
Equation (13) holds. Otherwise, define
Hence, define H (X) = p(X) A1
H (X) differently, still ensuring -approximate admissible. Note X
solution, least one item X must removed order reach solution contained X, thus
h (X) = p(X) Opt(X) m, smallest profit items. gives another option
define H (X) guarantee admissibility. summary, define heuristic function
H follows: non-solution state X,
(
(X)
(13) holds
p(X) A1
def
(14)
H (X) =

otherwise,

determined later H -approximate. X solution, simply set
H (X) = 0, h (X) = 0 case. H admissible, regardless .
make sure H -approximate, remains consider case (13) hold,
(X)
i.e., p(X) A1
< (1 )(p(X) (X)), non-solution state X. case,
p(X) (X)



(X)
(p([n]) m) .
(1 )
(1 )

(15)

last inequality due assumption X solution. want choose



(p([n]) m)
(16)
(1 )
1
which, combining (11) (15), imply (1 )h (X) = H (X). Therefore,
choose

1 = 1 + 1 1 (p([n])/m 1) .

3 1
Since running time
compute

(X)


|X|

, running time compute H (X)


3 1
|X| p([n])/m , polynomial n 1 profits bounded
range [m, poly(n)m]. search using heuristic H given Knapsack space
h[n], p, w, ci described Algorithm 3 below.
7.1.2 Experiments
order avoid easy instances, focus two families Knapsack instances identified studied
Pisinger (2005) difficult existing exact algorithms, including dynamic programming
algorithms branch-and-bound algorithms:
Strongly Correlated: item [n], choose weight wi random integer
range [1, R] set profit pi = wi + R/10. correlation weights profits
reflects real-life situation profit item proportional weight plus
fixed charge.
Subset Sum: item [n], choose weight wi random integer range [1, R]
set profit pi = wi . Knapsack instances type instances subset sum
problem.
tests
P set data range parameter R := 1000 choose knapsack capacity
c = (t/101) i[n] wi , random3 integer range [30, 70].
3. paper Pisinger (2005), fixed integer 1 100, average runtime tests
corresponding values reported.

705

fiDinh, Dinh, Michel, & Russell

Algorithm 3 Search Knapsack
Input: hn, p, w, c, i; n number items, pi wi profit weight item
[n], c capacity knapsack, (0, 1) error parameter heuristic.
Oracle: FPTAS algorithm Knapsack problem
(2001, p. 70).
P described Vazirani
P
Notation: subset X [n] items, let p(X) = iX pi , w(X) = iX wi .
Output: subset X [n] items w(X ) c p(X ) maximal.
1. Put start node [n] Open. Let = min1in pi . Set

1 = 1 + 1 1 (p([n])/m 1) .
2. Repeat Open empty:
(a) Remove Open place Closed node X g(X) + h(X) minimum.
(b) w(X) c, exit success return X, optimal solution.
(c) Otherwise, expand X: item X, let X 0 = X \ {i},
i. X 0 Open Closed, set g(X 0 ) := g(X) + p(i) = p([n]) p(X 0 ),
compute heuristic h(X 0 ) follows:
A. X 0 solution, set h(X 0 ) := 0.
B. Otherwise, run algorithm Knapsack input hX 0 , p, w, ci error parameter , let A(X 0 ) denote total profit solution returned algorithm
A. set
(
0
0
)
)
0
0
p(X 0 ) A(X
p(X 0 ) A(X
0
1
1 (1 )(p(X ) A(X ))
h(X ) :=

otherwise.
put X 0 Open pointer back X.
ii. Otherwise (X 0 Open Closed, g(X 0 ) calculated), g(X)+p(i) <
g(X 0 ), direct pointer X 0 back X reopen X 0 Closed.
[Remark: Since paths starting node X 0 cost,
condition g(X) + p(i) < g(X 0 ) never holds. fact, step discarded.]
3. Exit failure.

706

fiThe Time Complexity Approximate Heuristics

generating Knapsack instance h[n], p, w, ci either type described above, run series
search using given heuristic H , various values , well breath first search
(BFS), solve Knapsack instance. search finishes, values E reported,
E number nodes (states) expanded search, depth optimal
solution found search. Knapsack search space, k equals number items removed
original set [n] obtain optimal solution found search. overall runtime
search, including time computing heuristic, also reported. addition, report
optimal value h ([n]) minimal edge cost (i.e., minimal profit) search space
Knapsack instance tested.
specify appropriate size n Knapsack instance type, ran exploratory experiments identified largest possible value n search instances would finish
within hours. chose values n (n = 23 Strongly Correlated type,
n = 20 Subset Sum type) final experiments. Observing optimal solution
depths resulted Knapsack instances sizes fairly small, ranging 5 15,
selected sample points high interval [0.5, 1) distance two consecutive
points large enough sensitiveness E seen. particular, selected eight
sample points 8/16 = 0.5 15/16 = 0.9375 distance 1/16 = 0.0625
two consecutive points. final experiments, generated 20 Knapsack instances type
selected parameters n .
Experimental Results. Results final experiments shown Tables 1, 2, 3, 4, 5, 6,
rows corresponding breath first search indicated BFS column
. data show, expected, search outperforms breath first search terms
number nodes expanded and, naturally, smaller , fewer nodes expands.
result, effective branching factor decrease decreases (as long optimal
solutions given search space located depth). Recall expands E
nodes finds solution depth d, effective branching factor branching factor
uniform tree depth E nodes (Russell & Norvig, 1995, p. 102), i.e., number b satisfying
E = 1 + b + (b )2 + + (b )d . Clearly, (b )d E and, b 2, E 2(b )d . shall
focus solely values b 2, simply use E 1/d proxy effective branching factor, content
differs actually quantity factor 21/d . (Of course, b grows
error decays even further). effective branching factors, calculated E 1/d , search
breath first search Knapsack instances type Strongly Correlated shown Tables 1,
2, 3. Note Knapsack instances Subset Sum type, one cannot directly compare
effective branching factors, optimal solutions found different search instances appear
different depths.
primary goal experiments investigate proposed linear dependence which,
case non-uniform branching factors non-uniform edge costs, may express
log E log bBFS + ,

(17)

average optimal solution depth, bBFS effective branching factor breath first
search, constant depending . examine extend data supports
hypothesis, calculate least-squares linear fit (or linear fit short) log E (for
Knapsack instance, varying ) using least-squares linear regression model, measure
coefficient determination R2 . experiments, 17 20 Knapsack instances type
Strongly Correlated 20 Knapsack instances type Subset Sum R2 value least
0.9. instances, 90% variation log E depends linearly , remarkable fit.
See Figure 5 detailed histograms R2 values Knapsack instances. median R2
0.9534 Knapsack instances type Strongly Correlated, 0.9797 type Subset
Sum. Graphs log E linear fit Knapsack instances median R2 among
type shown Figures 3 4. Note even number instances
707

fiDinh, Dinh, Michel, & Russell

type, single instance median value. instances shown graphs
actually R2 value median.
Knapsack instance type Strongly Correlated median R2
Instance 17
Linear fit
BFS

log10 E

6
5
4
3
2
0.5

0.6

0.7
0.8
0.9
Heuristic error

1

Figure 3: Graph log10 E least-squares linear fit Knapsack instance type Strongly
Correlated median R2 (see data Table 3).

Knapsack instance type Subset Sum median R2
5.3

Instance 14
Linear fit
BFS

log10 E

5.25

5.2

5.15
0.5

0.6

0.7
0.8
0.9
Heuristic error

1

Figure 4: Graph log10 E least-squares linear fit Knapsack instance type Subset
Sum median R2 (see data Table 5).

Remark course, may instances poorly fit prediction linear dependence,
instance #20 Strongly Correlated type whose R2 value 0.486, though instances
708

fiThe Time Complexity Approximate Heuristics

rarely show experiments. instance, search using heuristic function H
may explore even fewer nodes search using H does, small > 0.
phenomenon explained degree control accuracy heuristic
function H . particular, guarantee H admissible -approximate,
reality may provide approximation better nodes opened. Note H
proportional (1 ). Hence, H may occasionally accurate H
small > 0, resulting fewer nodes expanded.

Frequency

Histograms R2 values Knapsack instance families
Strongly correlated
Subset Sum

10

5

[0.97, )

[0.93, )

[0.90, )

[0.87, )

[0.83, )

[0.80, )

[0.77, )

[0.73, )

[0.70, )

[0.67, )

[0.63, )

[0.60, )

[0.57, )

[0.53, )

[0.50, )

[0.47, )

[0.43, )

[0.40, )

0

R2 value bin limits
Figure 5: Histograms R2 values Knapsack instances.
analyze deeply data fit model Equation (17), calculate slope
least-squares linear fit log10 E Knapsack instance type Strongly Correlated. Note
instance, every search optimal solution depth, denoted d, thus,
= d. data, given Figure 6, show one instance worst R2 value,
slope linear fit log10 E fairly close log10 bBFS , slope hypothesized
line given Equation (17). Specifically, Knapsack instance type Strongly Correlated,
except instance #20,
0.73d log10 bBFS 1.63d log10 bBFS .

7.2 Search Partial Latin Square Completion
experimental results discussed Knapsack problem support hypothesis linear
scaling (cf., Equation (1) (10)). However, several structural features search space
heuristic unknown: example, cannot rule possibility approximation
algorithm, asked produce -approximation, fact produce significantly
better approximation; likewise, explicit control number near-optimal solutions.
order explore hypothesis detail, experimentally analytically investigate
search space partial Latin square completion problem provide precise analytic
control heuristic error well number -optimal solutions N .
7.2.1 Partial Latin Square completion (PLS) Problem
Latin square order n n n table row column permutation
set [n] = {1, . . . , n}. cells n n table filled values [n]
709

fiDinh, Dinh, Michel, & Russell

Knapsack instance type: Strongly Correlated
Instance
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

Optimal
solution
depth
11
9
6
7
7
6
7
8
6
9
7
5
7
5
6
9
7
8
7
7

Effective branching
factor breath first
search bBFS
4.2092
5.3928
10.8551
8.2380
8.0194
10.6780
8.7068
6.7742
11.4102
5.5412
8.3260
18.0486
8.0308
15.0964
10.0070
5.7863
8.3155
6.9106
8.3602
7.0964

Slope

linear fit

a/(d log10 bBFS )

Coefficient
determination R2

7.6583
4.8966
10.1038
6.4279
5.0882
6.4511
7.9087
6.5616
8.6847
6.3690
9.7685
7.7848
6.0376
7.3004
4.4219
7.1815
9.1738
9.2837
7.1807
1.0055

1.1154
0.7435
1.6260
1.0027
0.8040
1.0454
1.2021
0.9872
1.3690
0.9517
1.5161
1.2392
0.9533
1.2385
0.7368
1.0466
1.4247
1.3823
1.1123
0.1688

0.9395
0.9183
0.9647
0.9710
0.9161
0.9696
0.9436
0.9782
0.9571
0.9461
0.9689
0.9314
0.9646
0.9676
0.8788
0.8698
0.9498
0.9729
0.9770
0.4860

Figure 6: Slopes least-squares linear fits log10 E (varying ) Knapsack instances
type Strongly Correlated. Details least-squares linear fits given Tables 1, 2, 3.
R2 values Knapsack instances also included figure.
way value appears twice single row column, table called partial Latin
square. completion partial Latin square L Latin square obtained filling
empty cells L, see Figure 7 example. Note every partial Latin square
completion. Since problem determining whether partial Latin square completion
NP-complete (Colbourn, 1984), search version (denoted PLS), i.e., given partial Latin square
L find completion L one exists, NP-hard.
1

2
5

3
1
4

1
2

1
2
3
4
5

4
3

2
3
5
1
4

3
5
4
2
1

4
1
2
5
3

5
4
1
3
2

Figure 7: 5 5 partial Latin square (right) unique completion (left).
PLS problem (also known partial quasi-group completion) used recent
past source benchmarks evaluation search techniques constraint satisfaction
Boolean satisfiability (Gomes & Shmoys, 2002). Indeed, partially filled Latin squares carry
embedded structures trademark real-life applications scheduling time-tabling.
Furthermore, hard instances partially filled Latin square trigger heavy-tail behaviors
backtrack search algorithms common-place real-life applications require randomization restarting (Gomes, Selman, & Kautz, 1998). Additionally, PLS problem exhibits
strong phase transition phenomena satisfiable/unsatisfiable boundary (when 42%
cells filled) exploited produce hard instances. remark underlying
710

fiThe Time Complexity Approximate Heuristics

structure Latin squares found real-word applications including scheduling, timetabling (Tay, 1996), error-correcting code design, psychological experiments design wavelength
routing fiber optics networks (Laywine & Mullen, 1998; Kumar, Russell, & Sundaram, 1996).
7.2.2 Search Model PLS
Fix partial Latin square L order n c > 0 completions. divide cells n n table
two types: black cells, filled L, white cells,
left blank L. Let k number white cells. white cells indexed 0 k 1
fixed order, e.g., left right top bottom table. task search find
completion L. Hard instances obtained white cells uniformly distributed within
every row every column density black cells (n2 k)/n2 42% tap
phase transition. insure number completions c = O(1) (c exactly 1
experiments).
structure search space problem, place white cells virtual circle
white cells index (i + 1) mod k adjacent. move along circle, step
either forward (from white cell index cell index (i + 1) mod k) backward (from
white cell index cell index (i 1) mod k) may set content current cell.
Formally, define search graph, denoted GL , PLS instance given L follows:
state (or node) GL pair (, p), p {0, . . . k 1} indicates index current
white cell, : {0, . . . , k 1} {0, . . . , n} function representing current assignment
values white cells (we adopt convention (j) = 0 means white cell index j
filled). directed link (or edge) state (, p) state (, q) search
graph GL q = (p 1) mod k, (q)6= 0, (j) = (j) j 6= q. words,
link state (, p) state (, q) represents step consisting moving white
cell index p white cell index q, setting value (q) white cell index q.
Figure 8 illustrates links one state another GL . cost every link GL unit.
Obviously, search graph regular (out-)degree 2n.
starting state (0 , 0) 0 (j) = 0 j. goal state (or solution) form
( , p), assignment corresponding completion L, p {0, . . . , k 1}. So,
solution cover tree GL path search graph GL starting state
goal state, length optimal solution equal k. show number
-optimal solutions cover tree GL large.
Lemma 7.1. Let L n n partial Latin square k white cells. Let assignment
corresponding completion L. 0 < k, number paths
length k + GL

starting state goal state form ( , ) 2 + 2 + k+t
nt .

Proof. represent path GL length k + starting state pair hP, ~v i,
P (k + t)-length path circle white cells starting white cell index 0,
~v = (v1 , . . . , vk+t ) sequence values [n] vi value assigned white cell
visited ith step path P . Consider pair hP, ~v represents path GL ending
goal state ( , ). Since (j) 6= 0 j, every white cell must visited non-zero step
P . Let sj > 0 last step white cell index j visited. must
vsj = (j) j {0, . . . , k 1}. Given path P , nt ways assigning values
white cells order eventually obtain assignment . Thus, number (k + t)-length
paths GL starting state goal state ( , ) equal |Pt |nt , Pt set
(k + t)-length paths circle white cells start white cell index 0 visit every
white cell.
remains upper bound |Pt |. Consider path P Pt ; strategy bound number
backward (or forward) steps P . < k, least k 1 white cells visited exactly
711

fiDinh, Dinh, Michel, & Russell

0

k1

5

k2

2

1

4

2

3

3

1
0

k1

5

k2

2

1

4

3

1, v
h

2

3

4



3

2

3

2

2

3

5

1

5

5

2

1

0

k1
k2

5

5
3

p

4
2

p

4

1

1

p1

v

4

3

h+

1, v


1

4

2

3

4

1

1

3

p

v

2

p+1

2

5
3

5

1

Figure 8: links connecting states PLS search graph. label h+1, vi (resp., h1, vi)
links means moving forward (resp., backward) setting value v [n] next white cell.

P . Let w index white cell visited exactly P let step
white cell w visited.
Assume step forward step, i.e., white cell visited step 1 (w 1) mod k.
Let w0 farthest white cell w backward direction visited step s.
Precisely, w0 = (w `) mod k, ` maximal number {0, . . . , k 1} white
cell (w `) mod k visited step s. Let wj = (w0 + j) mod k, j = 0, . . . , k 1. Note
w` = w. set white cells visited first steps {w0 , w1 , . . . , w` }, deleting
steps among first steps P obtain path (w0 , w1 , . . . , w` ) w0 w`
forward direction. white cells w`+1 , . . . , wk1 must visited step step
also forward direction white cell w` visited forward
step. Thus, deleting steps P obtain path visiting white cells w0 , w1 , . . . , wk1
forward direction. Let s0 , . . . , sk1 steps P deleted, wj visited
step sj P , 1 s0 < s1 < . . . < sk1 k + t. steps s1 , . . . , sk1 forward steps
(step s0 forward backward). Moreover, number backward steps number
forward steps sj1 sj must equal j = 1, . . . , k 1. Let number
deleted steps s0 sk1 exactly (t )/2 backward steps
s0 sk . shows
+ 1 + (t )/2 = 1 + (t + )/2 + 1 backward steps

P . Note k+t
paths Pt exactly j backward steps. Path P
j
+ 1 backward steps = (and thus sj = sj1 + 1 j = 1, . . . , k 1) every
step 1 s0 sk1 backward. + 1 paths Pt , corresponding
choice s0 {1, . . . , + 1}.
Similarly, step backward step, + 1 forwardsteps P . Also,
+ 1 paths Pt exactly + 1 forward steps, k+t
paths Pt
j
712

fiThe Time Complexity Approximate Heuristics

exactly j forward steps. Hence,

|Pt | 2 + 1 +



X
k+t
j=0

last inequality holds since coefficient

j






2 t+2+t k+t
.


k+t
j



increases j increases j < (k + t)/2.

upper bound Lemma 7.1 achieved = 0. fact, four ways visit
every white cell k steps starting white cell 0: taking either k forward steps k backward
steps one backward step followed k 1 forward steps one forward steps followed k 1
backward steps. number optimal solutions cover tree GL equal 4c, since
c completions initial partial Latin square.
Theorem 7.1. Let L n n partial Latin square k white cells c completions.
0 < < 1, number nodes expanded search GL -accurate heuristic
B(),
B() =

(
2(2n)k + 4ck
2(2n)

k

k < 1 ,

+ 4ck bkc + 2 + bkc

k+bkc
bkc



n

bkc

k 1 .

Proof. Lemma 3.1, number nodes expanded search GL -accurate heuristic
upper-bounded 2(2n)k + kN , N number -optimal solutions cover tree
GL . So, need bound N .
k < 1, N equals number optimal solutions, implies upper bound
2(2n)k + 4ck number expanded nodes .
general case, let ` = bkc. Since k < k, Lemma 7.1,



`
X
k+t
2 t+2+t
nt

t=0
!

X
`
`
X
k+`


2c
(t + 2)n + 2c
tn
`
t=0
t=0


k+`
`
4c(` + 2)n + 4c
`n` .
`

N c



second inequality holds k+t
k+`
`. last inequality obtained

`
P`
P`

`
applying fact t=0 tn 2`n t=0 nt 2n` integers n 2 ` 0,
proved easily induction `. Hence, number nodes expanded




k+`
2(2n)k + 4ck ` + 2 + `
n` .
`

Corollary 7.1. Suppose 0 < < 1. number nodes expanded search GL
-accurate heuristic


k
k
k 3/2 (1 + ) (1 + 1/) nk .

Proof. Theorem 7.1, need upper bound binomial coefficient k+`
large k,
`
` = bkc. Since k ` large, bound binomial coefficient using Stirlings
713

fiDinh, Dinh, Michel, & Russell



n
formula, asserts n! 2n ne . precisely, write n! = 2n
n .


k+`
(k + `)!
=
`
k!`!
p
k+`
2(k + `) k+`
k+`
e
=


`
k
2k ke k 2` e` `

k + ` (k + `)k+`
k+`

.

=
k `
k k ``
2k`


n n
e

n , n 1

Stirlings formula, term k+` /k ` O(1). Since
k+`
k
k
=1+
1+
1 + 2/
`
bkc
k 1



k > 2/, term k + `/ 2k` O(1/ k). remaining term
(k + `)k+`
=
k k ``


k
`

k
`
k
1
k
1+
1+
(1 + ) 1 +
k
`

x

since ` k function g(x) = (1 + k/x) monotonically increases x > 0. Hence,



k !
k+`
1
1
= (1 + )k 1 +
.
`

k
Theorem 7.1, number nodes expanded


k + ` k
B() = 2(2n)k + k 2
n
`
!

k
1
3/2
k
k
= k (1 + ) 1 +
n
.


follows corollary effective branching factor using -accurate

heuristic GL asymptotically (1 + ) (1 + 1/) n , significantly smaller

brute-force branching factor 2n, since (1 + )n (1 + 1/) converge 1 0.
7.2.3 Experiments
Given search model PLS problem described above, provide experimental results
search PLS instances, determined large partial Latin square
single completion. PLS instance experiments, run search different
heuristics form (1 )h given various values [0, 1). emphasize dominance property admissible heuristics, number nodes expanded using admissible
-accurate heuristic strictly larger (1 )h less equal using
heuristic (1 )h . words, heuristic (1 )h worse admissible -accurate
heuristics.
build oracle heuristic (1 )h search graph GL , use information
completion partial Latin square L compute h . Consider partial Latin square
L k white cells, arbitrary state (, p) GL . show compute optimal
714

fiThe Time Complexity Approximate Heuristics

cost h (, p) reach goal state GL state (, p). Let X() set white cells
disagrees completion L, h (, p) equal length shortest
paths cycle starting p visiting every point X(). case
|X() \ {p} | 1 easy handle, shall assume |X() \ {p} | 2 on. particular,
suppose X() \ {p} = {p1 , . . . , p` } ` > 1, pj j th point X() \ {p} visited
moving forward (clockwise) p; see Figure 9. two types paths cycle
starting p visiting every point X() \ {p}: type includes visit p, type
II includes visiting p. Let `1 `2 length shortest paths type type II,
respectively.
(
min {`1 , `2 }
p 6 X() ,

h (, p) =
min {`1 + 2, `2 } p X() .
need compute `1 `2 . Computing `1 straightforward: realized either
moving forward p p` moving backward p p1 .
`1 = min {p` p, p p1 }
def

z = z mod k integer z. compute `2 , consider two options j: option
(a) moving forward p pj moving backward pj pj+1 , option (b) moving
backward p pj+1 moving forward pj+1 pj . Thus,


`2 = min min {pj p, p pj+1 } + pj pj+1 .
1j<`

moving forward
p1

p

p+1

p1

pm

p2

pm1
pj+1

pj

Figure 9: Layout points X().
describe experiments detail. generate six partial Latin squares orders
10 20 following way. Initially, generate several partial Latin squares obtained
near phase transition white cells uniformly distributed within every row column.
instance generated complete Latin square suitably chosen random subsets
cells cleared. candidate partial Latin square solved exhaustive backtracking
search method find completions. subset candidates exactly one completion
retained experiments. partial Latin square L chosen value , record
total number E nodes expanded search graph GL (1 )h heuristic.
Then, Knapsack experiments, effective branching factor calculated E 1/k ,
since optimal solution depth GL equals number white cells L. first purpose
compare effective branching factors obtained experiments upper bound obtained
715

fiDinh, Dinh, Michel, & Russell

theoretical analysis. Recall Theorem 7.1 E B(), case
B() =

(
2(2n)k + 4k
2(2n)

k

k < 1 ,

+ 4k bkc + 2 + bkc

k+bkc
bkc



n

bkc

k 1 .

1/k

Therefore, calculate theoretical upper bound B()
effective branching factor E 1/k .
1/k
deeper comparison, calculate multiplicative gap B() /E 1/k theoretical
bound actual values. empirical results given Tables 7 8, multiplicative
gaps close 1 small k large. Notice given k, upper bounds
B() almost value bkc. multiplicative
gaps sometimes increase decrease. However, multiplicative gaps decrease
bkc decreases, fixed k. upper bounds cases k < 1 much tighter
others (with k) cases k < 1 compute number
-optimal solutions exactly. Also observe that, fixed , multiplicative gaps decrease k
increases. Finally, experiments show dramatic gap effective branching factors
corresponding brute-force branching factor, equals 2n. fact, instance,
1/k
effective branching factor E 1/k theoretical upper bound B()
approach 1 approaches
0.
experiments Knapsack problem, data partial Latin square problem
also support linear dependance log E . particular, one partial Latin square
instances R2 larger 0.9 (the worst one R2 value equal 0.8698). median R2
value partial Latin square instances 0.9304. graph instance median
R2 shown Figure 10.
Partial Latin Square instance median R2
Instance 4
Linear fit

6

log10 E

5
4
3
2
0

0.5

1
1.5
2
Heuristic error

2.5

3
102

Figure 10: Graph log10 E least-squares least-squares linear fit (or Linear fit)
partial Latin square instance median R2 (see data Table 8).
also investigate slope least-squares linear fit log E approximates slope
log b hypothesized linear dependence Equation (10). Recall case,
branching factor b = 2n optimal solution depth = k. Figure 11 shows that, every
PLS instance experiment, slope least-squares linear fit log10 E approximates
716

fiThe Time Complexity Approximate Heuristics

k log10 (2n) factor 0.8, i.e., 0.8k log10 (2n). words, experimental results
PLS indicate following relationship:
log10 E 0.8k log10 (2n) + ,

equivalently, E (2n)0.8k .

Thus, empirically, effective branching factor search using heuristic (1)h given
PLS search space approximates (2n)0.8 . dominance property admissible heuristics,
also empirical upper bound effective branching factor using admissible
-accurate search space.
Instance #

n

k

1
2
3
4
5
6

10
12
14
16
18
20

44
63
86
113
143
176

Slope linear
fit line
43.3901
73.7527
98.3613
142.7056
179.1665
225.4152

/(k log10 (2n))
0.7580
0.8482
0.7903
0.8390
0.8050
0.7995

Figure 11: Slopes least-squares linear fits log10 E partial Latin square instances.

8. Reduction Depth vs. Branching Factor; Comparison Previous
Work
section compare results obtained Korf et al. (Korf & Reid, 1998; Korf
et al., 2001). mentioned introduction, concluded effect heuristic function
reduce effective depth search rather effective branching factor. Considering
striking qualitative difference findings ours, seems interesting discuss
conclusions apply accurate heuristics.
study b-ary tree search model, above, permit multiple solutions. However,
analysis depends critically following equilibrium assumption:
Equilibrium Assumption: number nodes depth heuristic value exceeding `
bi P (`), P (`) probability h(v) ` v chosen uniformly random among
nodes given depth, limit large depth.
remark equilibrium assumption strong structural requirement, holds
expectation rich class symmetric search spaces. specific, state-transitive
search space,4 like Rubiks cube, quantity bi P (`) precisely expected number vertices
depth h(v) ` goal state (or initial state) chosen uniformly random. Korf
et al. (2001) observe equilibrium assumption, one directly control number
expanded
P nodes total weight `, quantity denote E(`): indeed, case
E(`) = i` bi P (` i). hand, consider ratio
P`
P`

bi1 P (` i)
E(`)
i=0 b P (` i)
= P`1
= b Pi=0
b,
(18)
`

i1 P (` i)
E(` 1)
i=0 b P (` 1 i)
i=1 b
conclude E(d) bd1 E(1); thus effective branching factor
q
p

bd1 E(1) b E(1)
4. say search space state-transitive structure search graph independent starting
node. Note Cayley graph property, natural search spaces formed algebraic problems
like Rubiks cube 15-puzzle, right choice generators, property.

717

fiDinh, Dinh, Michel, & Russell

optimal solution lies depth d.
difficulty approach even presence mildly accurate heuristic satisfying, example,
h(v) h (v) small, constant, > 0 ,
actual values quantities satisfy
E(1) = E(2) = = E(t) = 0
d. (Even root tree h(root) d.) Observe,
then,
E(d) = 1
p

argument actually results effective branching factor bdd E(d) = b(1)d = b1 ,
yielding reduction branching factor. Indeed, applying technique infer estimates
complexity , even assuming equilibrium
P assumption, appears require control
threshold quantity `0 quantities
b P (`0 i) become non-negligible. course,
equilibrium assumption may well apply heuristics weaker or, example, nonuniform
accuracy.
One perspective issue obtained considering case search b-regular
(non-bipartite, connected) graph G = (V, E) observing selection node uniformly
random nodes given depth, limit large depth is, case, equivalent
selection random node graph. consider mildly accurate heuristic h
which, say, h(v) h (v) small constant , bi P (`) bi Prv [ dist(v, S) `], v
chosen uniformly random graph, set solution nodes, dist(v, S) denotes
length shortest path v node S.
|S| b`
|{v | dist(v, S) `/}|

Pr[dist(v, S) `/] =
v
|V |
|V |

1

b-regular graph, expect relation equation (18) hold past threshold
value `0 logb (|S|/|V |).

Acknowledgments
wish thank anonymous reviewers constructive comments. Author Hang Dinh
supported IU South Bend Faculty Research Grant. Author Laurent Michel supported
NSF grant #0642906. Author Alexander Russell supported NSF grant
#1117426.

718

fiThe Time Complexity Approximate Heuristics

Appendix A: Tables Experimental Results
#

n

Heuristic
error

1

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

Total
node
expansions
E
5627
5882
167660
211946
772257
1470135
6118255
7154310
7347748

2

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

3

23
23
23
23
23
23
23
23
23

4

Optimal
solution
depth

Search
time
(seconds)

11
11
11
11
11
11
11
11
11

125
101
858
744
1341
1318
2025
1653
1101

10887/200
10887/200
10887/200
10887/200
10887/200
10887/200
10887/200
10887/200

44481
45537
372163
474221
1358735
2508134
3508255
3569052
3857597

9
9
9
9
9
9
9
9
9

622
507
1497
1293
1751
1734
1469
1047
566

7820/157
7820/157
7820/157
7820/157
7820/157
7820/157
7820/157
7820/157

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

94
125
5528
9002
31800
109080
879884
1477032
1636093

6
6
6
6
6
6
6
6
6

6
7
98
105
206
301
707
560
224

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

3696
21847
44166
53464
253321
760792
1975195
2317663
2574876

7
7
7
7
7
7
7
7
7

5

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

23645
30501
72597
308417
968504
1681026
1833872
1833644
2132977

6

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

7

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

Effective
branching


factor
E
2.192473
2.201325
2.985026
3.049315
3.42966
3.636376
4.139674
4.198968
4.209164

log10 E

Linear
fit
log10 E

3.7503
3.7695
5.2244
5.3262
5.8878
6.1674
6.7866
6.8546
6.8662

3.7956
4.2742
4.7529
5.2315
5.7102
6.1888
6.6674
7.1461

3.284459
3.293033
4.158822
4.272327
4.802412
5.140898
5.336203
5.3464
5.392783

4.6482
4.6584
5.5707
5.6760
6.1331
6.3994
6.5451
6.5526
6.5863

4.7018
5.0078
5.3139
5.6199
5.9259
6.2320
6.5380
6.8441

5991/121
5991/121
5991/121
5991/121
5991/121
5991/121
5991/121
5991/121

2.132331
2.236068
4.204955
4.560962
5.628654
6.912326
9.788983
10.671652
10.855121

1.9731
2.0969
3.7426
3.9543
4.5024
5.0377
5.9444
6.1694
6.2138

1.9674
2.5989
3.2304
3.8619
4.4934
5.1248
5.7563
6.3878

86
256
303
258
553
788
957
694
383

6343/154
6343/154
6343/154
6343/154
6343/154
6343/154
6343/154
6343/154

3.233523
4.16786
4.608759
4.73628
5.914977
6.921191
7.93182
8.115082
8.23801

3.5677
4.3394
4.6451
4.7281
5.4037
5.8813
6.2956
6.3651
6.4108

3.7471
4.1489
4.5506
4.9524
5.3541
5.7558
6.1576
6.5593

7
7
7
7
7
7
7
7
7

305
285
429
754
1074
1047
823
585
306

6785/205
6785/205
6785/205
6785/205
6785/205
6785/205
6785/205
6785/205

4.215217
4.371357
4.947855
6.083628
7.164029
7.751179
7.848145
7.848005
8.019382

4.3737
4.4843
4.8609
5.4891
5.9861
6.2256
6.2634
6.2633
6.3290

4.3803
4.6983
5.0163
5.3343
5.6523
5.9703
6.2883
6.6064

1981
12316
21699
26575
131561
395118
1080314
1282206
1482293

6
6
6
6
6
6
6
6
6

46
139
151
131
290
431
547
409
219

5012/148
5012/148
5012/148
5012/148
5012/148
5012/148
5012/148
5012/148

3.543894
4.80557
5.281289
5.462761
7.131615
8.566192
10.129585
10.423006
10.677978

3.2969
4.0905
4.3364
4.4245
5.1191
5.5967
6.0336
6.1080
6.1709

3.4645
3.8677
4.2709
4.6741
5.0773
5.4805
5.8837
6.2869

1834
1956
2039
23275
30974
173886
675468
3440759
3793204

7
7
7
7
7
7
7
7
7

51
43
36
159
138
332
526
984
568

6187/122
6187/122
6187/122
6187/122
6187/122
6187/122
6187/122
6187/122

2.925499
2.952538
2.970119
4.20573
4.380978
5.605434
6.80457
8.586333
8.706789

3.2634
3.2914
3.3094
4.3669
4.4910
5.2403
5.8296
6.5367
6.5790

2.8110
3.3053
3.7996
4.2939
4.7882
5.2825
5.7768
6.2711

R2

0.9395

0.9183

0.9647

0.9710

0.9161

0.9696

0.9436

Table 1: Results Knapsack instances type Strongly Correlated.

719

fiDinh, Dinh, Michel, & Russell

#

n

Heuristic
error

8

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

Total
node
expansions
E
8299
8741
58455
93500
216413
536713
2569955
4096150
4434697

9

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

10

23
23
23
23
23
23
23
23
23

11

Optimal
solution
depth

Search
time
(seconds)

8
8
8
8
8
8
8
8
8

129
105
335
332
479
558
1066
1027
655

6400/153
6400/153
6400/153
6400/153
6400/153
6400/153
6400/153
6400/153

430
460
5313
9507
11268
88158
790402
2008558
2206805

6
6
6
6
6
6
6
6
6

19
16
84
91
75
229
646
673
334

5835/121
5835/121
5835/121
5835/121
5835/121
5835/121
5835/121
5835/121

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

14162
15321
178178
214332
872080
2128661
3942938
4543001
4924992

9
9
9
9
9
9
9
9
9

192
162
669
574
1052
1306
1379
1118
721

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

315
330
974
22374
26883
199464
783863
2579423
2773773

7
7
7
7
7
7
7
7
7

12

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

1029
1163
1310
3968
14820
75333
363263
1710935
1915195

13

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

14

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

Effective
branching


factor
E
3.089429
3.109533
3.943235
4.181686
4.644195
5.202568
6.327624
6.707288
6.7742

log10 E

Linear
fit
log10 E

3.9190
3.9416
4.7668
4.9708
5.3353
5.7297
6.4099
6.6124
6.6469

3.7753
4.1854
4.5955
5.0056
5.4157
5.8258
6.2359
6.6460

2.747334
2.778388
4.177245
4.602643
4.734867
6.671297
9.615562
11.232611
11.410219

2.6335
2.6628
3.7253
3.9780
4.0518
4.9453
5.8978
6.3029
6.3438

2.3749
2.9177
3.4605
4.0033
4.5461
5.0889
5.6317
6.1745

6762/171
6762/171
6762/171
6762/171
6762/171
6762/171
6762/171
6762/171

2.892252
2.917641
3.832024
3.911497
4.571533
5.048042
5.405911
5.491674
5.541159

4.1511
4.1853
5.2509
5.3311
5.9406
6.3281
6.5958
6.6573
6.6924

4.1618
4.5599
4.9579
5.3560
5.7541
6.1521
6.5502
6.9482

19
15
29
232
195
514
751
880
406

6465/106
6465/106
6465/106
6465/106
6465/106
6465/106
6465/106
6465/106

2.274582
2.289748
2.672619
4.182076
4.293214
5.716412
6.950792
8.240087
8.326044

2.4983
2.5185
2.9886
4.3497
4.4295
5.2999
5.8942
6.4115
6.4431

2.1619
2.7724
3.3830
3.9935
4.6040
5.2146
5.8251
6.4356

5
5
5
5
5
5
5
5
5

35
29
25
50
92
212
380
589
283

5073/106
5073/106
5073/106
5073/106
5073/106
5073/106
5073/106
5073/106

4.003899
4.103136
4.201983
5.244624
6.826053
9.449244
12.943277
17.646017
18.048562

3.0124
3.0656
3.1173
3.5986
4.1708
4.8770
5.5602
6.2332
6.2822

2.5015
2.9880
3.4746
3.9611
4.4477
4.9342
5.4208
5.9073

6701
7084
43514
71911
85427
376321
1441947
1963475
2154280

7
7
7
7
7
7
7
7
7

154
127
379
383
313
573
862
655
324

6072/122
6072/122
6072/122
6072/122
6072/122
6072/122
6072/122
6072/122

3.520395
3.548459
4.598978
4.941148
5.064232
6.259049
7.583154
7.925079
8.030775

3.8261
3.8503
4.6386
4.8568
4.9316
5.5756
6.1589
6.2930
6.3333

3.6957
4.0730
4.4504
4.8277
5.2050
5.5824
5.9597
6.3371

418
3629
7016
8503
51480
178163
550403
668276
784088

5
5
5
5
5
5
5
5
5

15
63
74
62
162
258
352
246
110

4636/140
4636/140
4636/140
4636/140
4636/140
4636/140
4636/140
4636/140

3.343761
5.151781
5.877842
6.108217
8.756443
11.22441
14.064884
14.621475
15.096385

2.6212
3.5598
3.8461
3.9296
4.7116
5.2508
5.7407
5.8250
5.8944

2.8386
3.2949
3.7512
4.2075
4.6637
5.1200
5.5763
6.0326

R2

0.9782

0.9571

0.9461

0.9689

0.9314

0.9646

0.9676

Table 2: Results Knapsack instances type Strongly Correlated.

720

fiThe Time Complexity Approximate Heuristics

#

n

Heuristic
error

15

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

Total
node
expansions
E
15713
17658
126261
172936
511397
809884
814774
814389
1004228

16

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

17

23
23
23
23
23
23
23
23
23

18

Optimal
solution
depth

Search
time
(seconds)

6
6
6
6
6
6
6
6
6

218
184
536
466
647
600
435
291
140

5825/211
5825/211
5825/211
5825/211
5825/211
5825/211
5825/211
5825/211

1851
1870
2504
2551
22976
43228
267829
2798746
7270715

9
9
9
9
9
9
9
9
9

44
36
35
29
113
122
283
842
1104

7275/117
7275/117
7275/117
7275/117
7275/117
7275/117
7275/117
7275/117

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

656
665
711
17143
28608
190546
844063
2558990
2749381

7
7
7
7
7
7
7
7
7

33
26
21
192
194
514
813
895
405

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

683
772
18190
24869
136138
308550
2311528
4805568
5201719

8
8
8
8
8
8
8
8
8

19

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

2854
3140
11500
38170
51667
270043
1107776
2600747
2854529

20

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

158012
505837
589456
700571
682245
682583
682855
682455
906305

h ([n])/m

Effective
branching


factor
E
5.004682
5.102977
7.082907
7.464159
8.942515
9.654663
9.664355
9.663593
10.007034

log10 E

Linear
fit
log10 E

4.1963
4.2469
5.1013
5.2379
5.7088
5.9084
5.9110
5.9108
6.0018

4.3104
4.5868
4.8631
5.1395
5.4159
5.6922
5.9686
6.2450

2.306987
2.309606
2.385756
2.390691
3.052011
3.274048
4.009547
5.203904
5.786254

3.2674
3.2718
3.3986
3.4067
4.3613
4.6358
5.4279
6.4470
6.8616

2.7061
3.1549
3.6038
4.0526
4.5015
4.9503
5.3992
5.8480

6501/102
6501/102
6501/102
6501/102
6501/102
6501/102
6501/102
6501/102

2.525892
2.530814
2.555112
4.025961
4.331527
5.679181
7.024655
8.23073
8.315545

2.8169
2.8228
2.8519
4.2341
4.4565
5.2800
5.9264
6.4081
6.4392

2.3428
2.9162
3.4895
4.0629
4.6363
5.2096
5.7830
6.3564

14
13
114
107
280
323
889
1083
790

6012/164
6012/164
6012/164
6012/164
6012/164
6012/164
6012/164
6012/164

2.261011
2.295896
3.407839
3.543703
4.382757
4.854737
6.244352
6.842552
6.910641

2.8344
2.8876
4.2598
4.3957
5.1340
5.4893
6.3639
6.6817
6.7161

2.7250
3.3052
3.8855
4.4657
5.0459
5.6262
6.2064
6.7866

7
7
7
7
7
7
7
7
7

65
56
121
210
185
412
682
772
415

5503/119
5503/119
5503/119
5503/119
5503/119
5503/119
5503/119
5503/119

3.116279
3.159085
3.802767
4.51369
4.713203
5.969239
7.302863
8.249784
8.360249

3.4555
3.4969
4.0607
4.5817
4.7132
5.4314
6.0445
6.4151
6.4555

3.2041
3.6529
4.1017
4.5505
4.9993
5.4481
5.8969
6.3457

7
7
7
7
7
7
7
7
7

866
1173
965
797
631
484
357
235
123

6592/295
6592/295
6592/295
6592/295
6592/295
6592/295
6592/295
6592/295

5.529298
6.52918
6.673447
6.840134
6.814281
6.814763
6.815151
6.814581
7.096418

5.1987
5.7040
5.7705
5.8455
5.8339
5.8342
5.8343
5.8341
5.9573

5.5119
5.5748
5.6376
5.7005
5.7633
5.8262
5.8890
5.9518

R2

0.8788

0.8698

0.9498

0.9729

0.9770

0.4860

Table 3: Results Knapsack instances type Strongly Correlated.

721

fiDinh, Dinh, Michel, & Russell

Linear fit
log10 E
5.8687
5.8803
5.8919
5.9036
5.9152
5.9268
5.9384
5.9500

#

n

Total
node
expansions E
731425
761013
782339
805295
828252
845545
865626
885943
900630

Optimal
soln. depth
11
15
12
12
12
10
11
14
13

Search time,
seconds
1090
878
716
579
463
360
267
179
80

log10 E

20
20
20
20
20
20
20
20
20

Heuristic
error
0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

1

5509/28
5509/28
5509/28
5509/28
5509/28
5509/28
5509/28
5509/28

5.8642
5.8814
5.8934
5.9060
5.9182
5.9271
5.9373
5.9474
5.9545

2

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

67164
71824
76627
80614
84553
90166
96506
99536
104144

6
9
7
8
8
9
7
7
8

259
208
168
136
107
82
58
35
10

2984/28
2984/28
2984/28
2984/28
2984/28
2984/28
2984/28
2984/28

4.8271
4.8563
4.8844
4.9064
4.9271
4.9550
4.9846
4.9980
5.0176

4.8311
4.8558
4.8804
4.9050
4.9297
4.9543
4.9790
5.0036

3

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

222293
232989
244871
256250
266235
274056
279890
283160
291239

11
12
8
9
9
8
11
9
9

533
432
353
285
226
173
126
81
28

3687/26
3687/26
3687/26
3687/26
3687/26
3687/26
3687/26
3687/26

5.3469
5.3673
5.3889
5.4087
5.4253
5.4378
5.4470
5.4520
5.4642

5.3552
5.3706
5.3861
5.4015
5.4170
5.4324
5.4479
5.4633

4

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

290608
304974
313598
323477
331235
336665
340874
342644
360837

10
10
9
9
9
10
9
9
8

329
272
225
185
151
121
92
64
33

3883/56
3883/56
3883/56
3883/56
3883/56
3883/56
3883/56
3883/56

5.4633
5.4843
5.4964
5.5098
5.5201
5.5272
5.5326
5.5348
5.5573

5.4734
5.4834
5.4935
5.5035
5.5136
5.5237
5.5337
5.5438

5

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

851515
873968
893378
912734
927408
940724
950209
958343
967863

11
14
12
14
13
12
12
13
12

740
609
498
410
335
267
206
142
88

7731/77
7731/77
7731/77
7731/77
7731/77
7731/77
7731/77
7731/77

5.9302
5.9415
5.9510
5.9603
5.9673
5.9735
5.9778
5.9815
5.9858

5.9348
5.9421
5.9494
5.9567
5.9641
5.9714
5.9787
5.9860

6

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

75858
81410
88494
94585
100329
106409
110656
114601
117496

10
8
6
9
7
5
9
9
4

488
363
287
225
177
134
94
55
11

2327/11
2327/11
2327/11
2327/11
2327/11
2327/11
2327/11
2327/11

4.8800
4.9107
4.9469
4.9758
5.0014
5.0270
5.0440
5.0592
5.0700

4.8895
4.9155
4.9416
4.9676
4.9936
5.0197
5.0457
5.0717

7

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

712138
748095
778565
799378
823236
844925
870175
897407
909075

11
12
15
11
13
13
13
12
14

1178
947
765
618
490
378
280
185
80

6456/33
6456/33
6456/33
6456/33
6456/33
6456/33
6456/33
6456/33

5.8526
5.8740
5.8913
5.9028
5.9155
5.9268
5.9396
5.9530
5.9586

5.8590
5.8727
5.8864
5.9001
5.9138
5.9275
5.9412
5.9549

R2

0.9918

0.9959

0.9649

0.9369

0.9687

0.9833

0.9895

Table 4: Results Knapsack instances type Subset Sum.

722

fiThe Time Complexity Approximate Heuristics

Linear fit
log10 E
5.4113
5.4416
5.4719
5.5023
5.5326
5.5629
5.5932
5.6236

#

n

Total
node
expansions E
252054
279643
299328
324182
340530
361756
385942
423848
454094

Optimal
soln. depth
10
12
9
11
9
10
10
9
9

Search time,
seconds
2274
1607
1159
878
666
494
344
201
42

log10 E

20
20
20
20
20
20
20
20
20

Heuristic
error
0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

8

3514/7
3514/7
3514/7
3514/7
3514/7
3514/7
3514/7
3514/7

5.4015
5.4466
5.4761
5.5108
5.5322
5.5584
5.5865
5.6272
5.6571

9

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

284146
301301
318308
330924
338590
345335
351027
356374
369094

9
8
7
9
9
9
10
10
8

628
507
412
334
263
203
146
92
34

4494/34
4494/34
4494/34
4494/34
4494/34
4494/34
4494/34
4494/34

5.4535
5.4790
5.5028
5.5197
5.5297
5.5382
5.5453
5.5519
5.5671

5.4677
5.4812
5.4947
5.5083
5.5218
5.5353
5.5489
5.5624

10

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

812828
852539
881657
903389
923450
941277
954861
970871
985526

11
13
15
12
15
11
14
14
12

1078
874
711
579
466
356
266
180
88

6963/39
6963/39
6963/39
6963/39
6963/39
6963/39
6963/39
6963/39

5.9100
5.9307
5.9453
5.9559
5.9654
5.9737
5.9799
5.9872
5.9937

5.9193
5.9298
5.9403
5.9508
5.9613
5.9717
5.9822
5.9927

11

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

872387
892404
907719
920529
930373
939495
945766
948094
961185

12
13
12
12
12
13
12
11
11

527
441
366
306
260
214
169
125
85

7270/102
7270/102
7270/102
7270/102
7270/102
7270/102
7270/102
7270/102

5.9407
5.9506
5.9580
5.9640
5.9687
5.9729
5.9758
5.9769
5.9828

5.9456
5.9507
5.9558
5.9609
5.9660
5.9711
5.9762
5.9813

12

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

544749
572592
596732
622826
644836
662145
682257
705866
720827

13
8
12
9
11
12
11
11
13

997
804
656
528
420
329
242
158
64

5752/35
5752/35
5752/35
5752/35
5752/35
5752/35
5752/35
5752/35

5.7362
5.7578
5.7758
5.7944
5.8094
5.8210
5.8339
5.8487
5.8578

5.7422
5.7579
5.7736
5.7893
5.8050
5.8207
5.8364
5.8521

13

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

592766
628513
662306
684651
713728
745263
781953
824260
861415

10
10
11
13
12
10
11
11
11

1824
1319
1040
828
645
487
344
216
74

7445/30
7445/30
7445/30
7445/30
7445/30
7445/30
7445/30
7445/30

5.7729
5.7983
5.8211
5.8355
5.8535
5.8723
5.8932
5.9161
5.9352

5.7767
5.7963
5.8159
5.8355
5.8552
5.8748
5.8944
5.9140

14

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

137368
148933
157793
165368
172383
179983
186068
191426
197634

8
7
10
9
9
7
9
8
10

561
450
363
289
226
173
123
73
18

3509/22
3509/22
3509/22
3509/22
3509/22
3509/22
3509/22
3509/22

5.1379
5.1730
5.1981
5.2185
5.2365
5.2552
5.2697
5.2820
5.2959

5.1513
5.1713
5.1913
5.2113
5.2314
5.2514
5.2714
5.2914

R2

0.9925

0.9282

0.9593

0.9409

0.9901

0.9963

0.9761

Table 5: Results Knapsack instances type Subset Sum.

723

fiDinh, Dinh, Michel, & Russell

Linear fit
log10 E
4.5311
4.5760
4.6209
4.6658
4.7107
4.7556
4.8006
4.8455

#

n

Total
node
expansions E
34937
38617
41757
45036
49231
54428
62409
75602
84284

Optimal
soln. depth
9
6
10
9
10
7
9
7
8

Search time,
seconds
1022
772
529
353
272
186
128
72
8

log10 E

20
20
20
20
20
20
20
20
20

Heuristic
error
0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

15

3124/9
3124/9
3124/9
3124/9
3124/9
3124/9
3124/9
3124/9

4.5433
4.5868
4.6207
4.6536
4.6922
4.7358
4.7952
4.8785
4.9257

16

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

476547
498939
523867
558927
592373
626403
668497
725325
768536

10
11
10
10
9
10
12
12
13

3224
2097
1536
1181
911
675
468
281
71

5442/11
5442/11
5442/11
5442/11
5442/11
5442/11
5442/11
5442/11

5.6781
5.6980
5.7192
5.7474
5.7726
5.7969
5.8251
5.8605
5.8857

5.6718
5.6976
5.7235
5.7493
5.7751
5.8010
5.8268
5.8527

17

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

641544
666837
702032
737893
772405
810089
852271
902227
964897

15
11
12
14
14
14
14
12
14

3751
2791
1991
1495
1124
827
570
337
86

7157/11
7157/11
7157/11
7157/11
7157/11
7157/11
7157/11
7157/11

5.8072
5.8240
5.8464
5.8680
5.8878
5.9085
5.9306
5.9553
5.9845

5.8045
5.8256
5.8468
5.8679
5.8891
5.9102
5.9313
5.9525

18

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

321490
338267
358571
379827
399061
419052
443204
486366
508524

9
10
9
10
9
10
9
10
10

1215
952
760
600
466
356
252
157
47

4631/20
4631/20
4631/20
4631/20
4631/20
4631/20
4631/20
4631/20

5.5072
5.5293
5.5546
5.5796
5.6010
5.6223
5.6466
5.6870
5.7063

5.5047
5.5293
5.5540
5.5786
5.6033
5.6279
5.6525
5.6772

19

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

104698
110845
116893
122710
128398
131887
133658
134205
142348

7
8
10
8
6
9
10
5
8

251
206
169
137
110
84
60
37
13

3373/44
3373/44
3373/44
3373/44
3373/44
3373/44
3373/44
3373/44

5.0199
5.0447
5.0678
5.0889
5.1086
5.1202
5.1260
5.1278
5.1534

5.0322
5.0482
5.0641
5.0800
5.0959
5.1119
5.1278
5.1437

20

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

275501
286961
296924
305914
315286
322234
324077
324471
348398

10
9
9
7
9
8
9
10
9

352
292
240
196
159
126
94
65
32

5262/94
5262/94
5262/94
5262/94
5262/94
5262/94
5262/94
5262/94

5.4401
5.4578
5.4726
5.4856
5.4987
5.5082
5.5106
5.5112
5.5421

5.4489
5.4594
5.4699
5.4804
5.4909
5.5013
5.5118
5.5223

R2

0.9739

0.9947

0.9988

0.9932

0.9349

0.9299

Table 6: Results Knapsack instances type Subset Sum.

724

fiThe Time Complexity Approximate Heuristics

Effective
branching
factor
E 1/k
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.11793532
1.12483883
1.13029527
1.13481129
1.13744262
1.13983570
1.14203051
1.14306342
1.14405770
1.15327789
1.19493326
1.20344942
1.21724042
1.22842928
1.23335496
1.24222170
1.24615895
1.24988516
1.25689894
1.26697571
1.28154406
1.28838000
1.29446689
1.29905304
1.30420852
1.30895150
1.31267920
1.31682768
1.32748452
1.34592652

Upper
bound
B(d)1/k

B(d)1/k

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
0.0225
0.025
0.0275
0.03
0.0325
0.035
0.0375
0.04
0.0425
0.045
0.0475
0.05
0.0525
0.055
0.0575
0.06
0.0625
0.065
0.0675
0.07
0.0725
0.075
0.0775
0.08
0.0825
0.085
0.0875
0.09
0.0925
0.095
0.0975

Total
node
expansions
E
87
87
87
87
87
87
87
87
87
87
135
177
219
261
289
317
345
359
373
531
2530
3458
5709
8539
10183
13956
16041
18293
23400
33251
54989
69492
85507
99904
118924
139520
158117
181666
258998
475269

1.12498287
1.12509476
1.12524953
1.12546320
1.12575740
1.12616102
1.12671203
1.12745936
1.12846421
1.12980027
1.29413023
1.29413756
1.29414775
1.29416190
1.29418158
1.29420890
1.29424685
1.29429954
1.29437264
1.48549510
1.48549548
1.48549601
1.48549674
1.48549775
1.48549917
1.48550113
1.48550386
1.48550766
1.68167021
1.68167024
1.68167029
1.68167036
1.68167046
1.68167059
1.68167077
1.68167103
1.68167138
1.88726770
1.88726771
1.88726771

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
0.0225
0.025
0.0275
0.03
0.0325
0.035
0.0375
0.04
0.0425
0.045
0.0475
0.05
0.0525
0.055
0.0575
0.06
0.0625
0.065
0.0675
0.07

125
125
125
125
125
125
125
295
599
789
979
1093
1207
1759
8006
18159
31829
39898
53605
63934
151470
240217
418262
569663
823942
1.03E+06
1.39E+06
3.35E+06
6.43E+06

1.07965322
1.07965322
1.07965322
1.07965322
1.07965322
1.07965322
1.07965322
1.09446915
1.10684330
1.11169422
1.11550813
1.11746021
1.11922136
1.12593198
1.15334431
1.16843520
1.17889026
1.18312592
1.18868491
1.19201428
1.20844644
1.21732463
1.22808758
1.23412462
1.24137536
1.24580697
1.25172483
1.26929396
1.28251719

1.09187259
1.09196102
1.09210593
1.09234240
1.09272569
1.09334029
1.09430956
1.21404534
1.21404945
1.21405622
1.21406738
1.21408579
1.21411611
1.34843434
1.34843446
1.34843466
1.34843500
1.34843555
1.34843647
1.34843797
1.48271141
1.48271142
1.48271144
1.48271147
1.48271152
1.48271161
1.62031036
1.62031036
1.62031037

#

n

k

Heuristic
error

1

10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10

44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44

2

12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12

63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63

log10 E

Linear
fit
log10 E

1.01640297
1.01650406
1.01664390
1.01683694
1.01710275
1.01746741
1.01796524
1.01864044
1.01954830
1.02075541
1.15760743
1.15050932
1.14496431
1.14042036
1.13779944
1.13543461
1.13328571
1.13230772
1.13138755
1.28806345
1.24316189
1.23436514
1.22038072
1.20926599
1.20443766
1.19584220
1.19206612
1.18851532
1.33795181
1.32731056
1.31222199
1.30525960
1.29912203
1.29453574
1.28941863
1.28474663
1.28109852
1.43319261
1.42168717
1.40220709

1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
2.1303
2.2480
2.3404
2.4166
2.4609
2.5011
2.5378
2.5551
2.5717
2.7251
3.4031
3.5388
3.7566
3.9314
4.0079
4.1448
4.2052
4.2623
4.3692
4.5218
4.7403
4.8419
4.9320
4.9996
5.0753
5.1446
5.1990
5.2593
5.4133
5.6769

1.2674
1.3758
1.4843
1.5928
1.7013
1.8097
1.9182
2.0267
2.1352
2.2436
2.3521
2.4606
2.5691
2.6775
2.7860
2.8945
3.0030
3.1114
3.2199
3.3284
3.4369
3.5454
3.6538
3.7623
3.8708
3.9793
4.0877
4.1962
4.3047
4.4132
4.5216
4.6301
4.7386
4.8471
4.9555
5.0640
5.1725
5.2810
5.3894
5.4979

1.01131786
1.01139977
1.01153399
1.01175301
1.01210802
1.01267728
1.01357504
1.10925497
1.09685756
1.09207747
1.08835368
1.08646892
1.08478640
1.19761616
1.16915170
1.15405173
1.14381723
1.13972277
1.13439352
1.13122636
1.22695666
1.21800824
1.20733363
1.20142768
1.19441030
1.19016159
1.29446211
1.27654461
1.26338296

2.0969
2.0969
2.0969
2.0969
2.0969
2.0969
2.0969
2.4698
2.7774
2.8971
2.9908
3.0386
3.0817
3.2453
3.9034
4.2591
4.5028
4.6010
4.7292
4.8057
5.1803
5.3806
5.6214
5.7556
5.9159
6.0134
6.1431
6.5244
6.8080

1.3953
1.5797
1.7641
1.9485
2.1328
2.3172
2.5016
2.6860
2.8704
3.0547
3.2391
3.4235
3.6079
3.7923
3.9767
4.1610
4.3454
4.5298
4.7142
4.8986
5.0829
5.2673
5.4517
5.6361
5.8205
6.0049
6.1892
6.3736
6.5580

E 1/k

R2

0.9482

0.9693

Table 7: Results partial Latin square instances.

725

fiDinh, Dinh, Michel, & Russell

Effective
branching
factor
E 1/k
1.06161017
1.06161017
1.06161017
1.06161017
1.06161017
1.06615920
1.07302532
1.07624311
1.07854600
1.07979831
1.10835983
1.12621485
1.13582148
1.14198131
1.14598774
1.15596951
1.16381138
1.16872905
1.17320907
1.17776024

Upper
bound
B(d)1/k

B(d)1/k

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
0.0225
0.025
0.0275
0.03
0.0325
0.035
0.0375
0.04
0.0425
0.045
0.0475

Total
node
expansions
E
171
171
171
171
171
247
429
555
667
737
6959
27506
57104
90923
122879
259053
463344
665871
925306
1.29E+06

1.07034588
1.07042098
1.07057335
1.07087962
1.07148429
1.16291112
1.16291347
1.16291827
1.16292811
1.16294821
1.26274158
1.26274166
1.26274182
1.26274214
1.36083647
1.36083648
1.36083648
1.36083649
1.36083651
1.45985179

113
113
113
113
113
113
113
113
113
113
113
113
113

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
0.0225
0.025
0.0275
0.03

225
225
225
225
799
1719
2317
2731
50236
144797
258735
516942
1.97E+06

1.04909731
1.04909731
1.04909731
1.04909731
1.06092884
1.06814635
1.07097198
1.07253118
1.10053004
1.11088842
1.11660964
1.12346988
1.13686805

18
18
18
18
18
18
18
18
18

143
143
143
143
143
143
143
143
143

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02

285
285
285
743
2579
3659
39137
246338
535932

20
20
20
20
20
20
20

176
176
176
176
176
176
176

0
0.0025
0.005
0.0075
0.01
0.0125
0.015

351
351
351
2425
4125
107153
619190

#

n

k

Heuristic
error

3

14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14

86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86

4

16
16
16
16
16
16
16
16
16
16
16
16
16

5

6

log10 E

Linear
fit
log10 E

1.00822873
1.00829948
1.00844300
1.00873150
1.00930108
1.09074810
1.08377077
1.08053493
1.07823691
1.07700503
1.13928848
1.12122626
1.11174321
1.10574676
1.18747908
1.17722523
1.16929298
1.16437295
1.15992669
1.23951526

2.2330
2.2330
2.2330
2.2330
2.2330
2.3927
2.6325
2.7443
2.8241
2.8675
3.8425
4.4394
4.7567
4.9587
5.0895
5.4134
5.6659
5.8234
5.9663
6.1109

1.4986
1.7445
1.9904
2.2363
2.4822
2.7281
2.9740
3.2199
3.4658
3.7117
3.9576
4.2035
4.4494
4.6953
4.9412
5.1871
5.4330
5.6789
5.9248
6.1707

1.05563497
1.05570312
1.05588217
1.05634285
1.12838087
1.12838284
1.12838808
1.12840202
1.20572650
1.20572656
1.20572671
1.28088203
1.28088203

1.00623170
1.00629666
1.00646733
1.00690645
1.06357828
1.05639348
1.05361121
1.05209251
1.09558708
1.08537144
1.07981041
1.14011248
1.12667607

2.3522
2.3522
2.3522
2.3522
2.9025
3.2353
3.3649
3.4363
4.7010
5.1608
5.4129
5.7134
6.2952

1.6772
2.0340
2.3907
2.7475
3.1042
3.4610
3.8178
4.1745
4.5313
4.8881
5.2448
5.6016
5.9584

1.04031952
1.04031952
1.04031952
1.04731385
1.05646789
1.05905525
1.07675277
1.09069423
1.09663904

1.04542550
1.04549145
1.04572413
1.10463010
1.10463134
1.10463580
1.16693654
1.16693656
1.16693665

1.00490809
1.00497148
1.00519515
1.05472691
1.04558912
1.04303887
1.08375532
1.06990257
1.06410278

2.4548
2.4548
2.4548
2.8710
3.4115
3.5634
4.5926
5.3915
5.7291

1.8665
2.3144
2.7623
3.2103
3.6582
4.1061
4.5540
5.0019
5.4498

1.03386057
1.03386057
1.03386057
1.04527681
1.04843662
1.06802045
1.07871841

1.03797380
1.03804139
1.03837263
1.08739144
1.08739402
1.13899860
1.13899862

1.00397851
1.00404389
1.00436428
1.04029040
1.03715761
1.06645766
1.05588132

2.5453
2.5453
2.5453
3.3847
3.6154
5.0300
5.7918

1.9462
2.5098
3.0733
3.6368
4.2004
4.7639
5.3275

E 1/k

R2

0.9335

0.9274

0.9120

0.8698

Table 8: Results partial Latin square instances.

726

fiThe Time Complexity Approximate Heuristics

References
Aaronson, S. (2004). Lower bounds local search quantum arguments. Proceedings
36th Annual ACM Symposium Theory Computing (STOC). ACM Press.
Babai, L. (1991). Local expansion vertex-transitive graphs random generation finite groups.
Proceedings 23rd annual ACM symposium Symposium Theory Computing,
pp. 164174.
Chernoff, H. (1952). measure asymptotic efficiency tests hypothesis based sum
observations. Annals Mathematical Statistics, 23, 493507.
Chung, F. (2006). diameter Laplacian eigenvalues directed graphs. Electronic Journal
Combinatorics, 13 (4).
Chung, F. R. K. (1997). Spectral Graph Theory. American Mathematical Society.
Colbourn, C. J. (1984). complexity completing partial Latin squares. Discrete Applied
Mathematics, 8 (1), 2530.
Davis, H., Bramanti-Gregor, A., & Wang, J. (1988). advantages using depth breadth
components heuristic search. Ras, Z., & Saitta, L. (Eds.), Proceedings Third
International Symposium Methodologies Intelligent Systems, pp. 1928, North-Holland,
Amsterdam. Elsevier.
Dechter, R., & Pearl, J. (1985). Generalized best-first search strategies optimality A*. J.
ACM, 32 (3), 505536.
Demaine, E. D. (2001). Playing games algorithms: Algorithmic combinatorial game theory.
Proc. 26th Symp. Math Found. Comp. Sci., Lect. Notes Comp. Sci., pp. 1832.
Springer-Verlag.
Dinh, H., Russell, A., & Su, Y. (2007). value good advice: complexity A*
accurate heuristics. Proceedings Twenty-Second Conference Artificial Intelligence
(AAAI-07), pp. 11401145.
Edelkamp, S. (2001). Prediction regular search tree growth spectral analysis. Proceedings
Joint German/Austrian Conference AI: Advances Artificial Intelligence, KI 01,
pp. 154168, London, UK, UK. Springer-Verlag.
Felner, A., Korf, R. E., & Hanan, S. (2004). Additive pattern database heuristics. Journal
Artificial Intelligence Research, 22, 279318.
Friedman, J. (2003). proof Alons second eigenvalue conjecture. STOC 03: Proceedings
thirty-fifth annual ACM symposium Theory computing, pp. 720724, New York, NY,
USA. ACM.
Gaschnig, J. (1979). Perfomance measurement analysis certain search algorithms. Ph.D.
thesis, Carnegie-Mellon University, Pittsburgh, PA.
Gomes, C., & Shmoys, D. (2002). Completing quasigroups Latin squares: structured graph
coloring problem. Johnson, D. S., Mehrotra, A., & Trick, M. (Eds.), Proceedings
Computational Symposium Graph Coloring Generalizations, pp. 2239, Ithaca, New
York, USA.
Gomes, C. P., Selman, B., & Kautz, H. (1998). Boosting combinatorial search randomization. AAAI 98/IAAI 98: Proceedings fifteenth national/tenth conference Artificial intelligence/Innovative applications artificial intelligence, pp. 431437, Menlo Park,
CA, USA. American Association Artificial Intelligence.
Hart, P., Nilson, N., & Raphael, B. (1968). formal basis heuristic determination minimum
cost paths. IEEE Transactions Systems Science Cybernetics, SCC-4 (2), 100107.
727

fiDinh, Dinh, Michel, & Russell

Helmert, M., & Roger, G. (2008). good almost perfect?. Proceedings AAAI-08.
Hochbaum, D. (1996). Approximation Algorithms NP-hard Problems. Brooks Cole.
Horn, R., & Johnson, C. (1999). Matrix Analysis. Cambridge University Press, Cambridge, UK.
Huyn, N., Dechter, R., & Pearl, J. (1980). Probabilistic analysis complexity A*. Artificial
Intelligence, 15, 241254.
Ibarra, O. H., & Kim, C. E. (1975). Fast approximation algorithms knapsack sum
subset problems. Journal ACM, 22 (4), 463468.
Karp, R. M. (1972). Reducibility among combinatorial problems. Miller, R. E., & Thatcher,
J. W. (Eds.), Complexity Computer Computations, p. 85103. New York: Plenum.
Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack problems. Springer.
Korf, R. (1985). Depth-first iterative deepening: optimal admissible tree search. Artificial
Intelligence, 27, 97109.
Korf, R., & Reid, M. (1998). Complexity analysis admissible heuristic search. Proceedings
National Conference Artificial Intelligence (AAAI-98), pp. 305310.
Korf, R., Reid, M., & Edelkamp, S. (2001). Time complexity iterative-deepening-A*. Artificial
Intelligence, 129 (1-2), 199218.
Korf, R. E. (2000). Recent progress design analysis admissible heuristic functions.
AAAI/IAAI 2000, pp. 11651170. Also SARA 02: Proceedings 4th International
Symposium Abstraction, Reformulation, Approximation.
Kumar, R., Russell, A., & Sundaram, R. (1996). Approximating Latin square extensions. COCOON 96: Proceedings Second Annual International Conference Computing
Combinatorics, pp. 280289, London, UK. Springer-Verlag.
Laywine, C., & Mullen, G. (1998). Discrete Mathematics using Latin Squares. Interscience Series
Discrete mathematics Optimization. Wiley.
Lenstra, A. K., Lenstra, H. W., & Lovasz, L. (1981). Factoring polynomials rational coefficients.
Tech. rep. 82-05, Universiteit Amsterdam.
Motwani, R., & Raghavan, P. (1995). Randomized Algorithms. Cambridge University Press.
Parberry, I. (1995). real-time algorithm (n2 1)-puzzle. Inf. Process. Lett, 56, 2328.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies Computer Problem Solving. AddisonWesley, MA.
Pisinger, D. (2005). hard knapsack problems?. Computers Operations Research,
32, 22712284.
Pohl, I. (1977). Practical theoretical considerations heuristic search algorithms. Elcock,
W., & Michie, D. (Eds.), Machine Intelligence, Vol. 8, pp. 5572. Ellis Horwood, Chichester.
Ratner, D., & Warmuth, M. (1990). (n2 1)-puzzle related relocation problems. Journal
Symbolic Computation, 10 (2), 111137.
Russell, S., & Norvig, P. (1995). Artificial Intelligence - Modern Approach. Prentice Hall, New
Jersey.
Sen, A. K., Bagchi, A., & Zhang, W. (2004). Average-case analysis best-first search two
representative directed acyclic graphs. Artif. Intell., 155 (1-2), 183206.
Tay, T.-S. (1996). results generalized Latin squares. Graphs Combinatorics, 12, 199
207.
Vazirani, V. (2001). Approximation Algorithms. Springer-Verlag.
728

fiThe Time Complexity Approximate Heuristics

Vazirani, V. (2002). Primal-dual schema based approximation algorithms. Theoretical Aspects
Computer Science: Advanced Lectures, pp. 198207. Springer-Verlag, New York.
Zahavi, U., Felner, A., Schaeffer, J., & Sturtevant, N. (2007). Inconsistent heuristics. Proceedings
AAAI-07, pp. 12111216.
Zhang, Z., Sturtevant, N. R., Holte, R., Schaeffer, J., & Felner, A. (2009). A* search inconsistent
heuristics. Proceedings 21st international jont conference Artifical intelligence,
IJCAI09, pp. 634639, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.

729

fiJournal Artificial Intelligence Research 45 (2012) 165-196

Submitted 06/12; published 10/12

Coalition Structure Generation Graphs
Thomas Voice

tdv@ecs.soton.ac.uk

School Electronics Computer Science,
University Southampton, UK

Maria Polukarov

mp3@ecs.soton.ac.uk

School Electronics Computer Science,
University Southampton, UK

Nicholas R. Jennings

nrj@ecs.soton.ac.uk

School Electronics Computer Science,
University Southampton, UK
Department Computing Information Technology,
King Abdulaziz University, Saudi Arabia

Abstract
give analysis computational complexity coalition structure generation
graphs. Given undirected graph G = (N, E) valuation function v : P(N ) R
subsets nodes, problem find partition N connected subsets,
maximises sum components values. problem generally NPcomplete;
particular, hard defined class valuation functions independent
disconnected membersthat is, two nodes effect others marginal contribution vertex separator. Nonetheless, functions provide bounds
complexity coalition structure generation general minorfree graphs.
proof constructive yields algorithms solving corresponding instances
problem. Furthermore, derive linear time bounds graphs bounded treewidth.
However, show, problem remains NPcomplete planar graphs, hence,
Kk minorfree graphs k 5. Moreover, 3-SAT problem clauses
represented coalition structure generation problem planar graph
O(m2 ) nodes. Importantly, hardness result holds particular subclass valuation
functions, termed edge sum, value subset nodes simply determined
sum given weights edges induced subgraph.

1. Introduction
Coalition structure generation (CSG) equivalent complete set partitioning
problem (Yeh, 1986)one fundamental problems combinatorial optimisation,
applications many fields, political sciences economics, operations research
computer science. CSG problem, set N n elements valuation
function v : P(N ) R, P(N ) denotes power set N , problem
divide given set P
disjoint exhaustive subsets (or, coalitions) N1 , . . . , Nm
total sum values,
i=1 v(Ni ), maximised. Thus, seek valuable partition (or,
coalition structure) N .
c
2012
AI Access Foundation. rights reserved.

fiVoice, Polukarov, & Jennings

Partitioning structure problems arise wide range practical domains including delivery management, scheduling, routing location problems, one wishes assure
every customer served one (and one) location, vehicle person (server).
Commonly cited problems kind include crew-scheduling problem every flight
leg airline must scheduled exactly one cockpit crew, political districting problem whereby regions must divided voting districts every citizen assigned
exactly one district, coalition formation problem political parties (Balas &
Padberg, 1976). Recently, CSG become major research topic artificial intelligence
multi-agent systems, tool autonomous agents form effective teams.
example, electronic commerce buyer agents may pool demands order obtain
group discounts (Tsvetovat, Sycara, Chen, & Ying, 2001); e-business coalitions may form
order satisfy certain market niches respond diverse orders
individual agents (Norman, Preece, Chalmers, Jennings, Luck, Dang, Nguyen, Deora, Gray,
& Fiddian, 2004); distributed vehicle routing coalitions delivery companies
reduce transportation costs sharing deliveries (Sandholm & Lesser, 1997). important applications include information gathering several information servers come
together answer queries (Klusch & Shehory, 1996), multi-sensor networks sensors
form dynamic coalitions wide-area surveillance scenarios (Dang, Dash, Rogers, & Jennings, 2006), grid computing multi-institution virtual organisations viewed
central coordinated resource sharing problem solving (Yong, Li, Weiming,
Jichang, & Changying, 2003).
However, classic CSG model assumes structure primitive set elements.
considerable shortcoming, various contexts interest computer scientists,
elements represent agents (either human automated) resources (e.g., machines,
computers, service providers communication lines), typically embedded
social computer network. Moreover, many scenarios elements
disconnected effect others performance potential contribution
coalition, connected intermediaries, may able cooperate all.
example, consider communication network edge channel, capacity
indicating amount information transmitted it. Thus,
aforementioned contexts e-commerce, multi-sensor networks grid computing,
network connects sellers buyers, sensors agents working computational
tasks, respectively. subset nodes network produces value proportional
total capacity subnetwork induced nodes. scenario, two
nodes connected direct link network, affect others
marginal contribution coalition nodes separates them. Or, also typical
e-commerce e-business domains, assume edge represents trust link
reputation system, two nodes participate coalition trust
distance given length path them, finite (that is, coalition induces
connected subgraph trust network). Suppose value coalition given
number pairs mutually trusted membersi.e., edges induced
subgraph. Then, contribution particular node depend another node
j trusts members coalition trust directly,
edge j. Additional natural examples arise multi-agent systems domains,
agents come together complete tasks. Typically, pair agents associ166

fiCoalition Structure Generation Graphs

ated weight indicating potential mutual (in)efficiency task execution
(e.g., due skill/expertise equipment complementarity, interpersonal (in)compatibility,
(dis)agreements, spatial constraints). value coalition measured
total coalitional weight given sum weights links whose ends participate coalition. Importantly, weights positive negative, representing
different relations among agents, thus corresponding effects coalitional
value. Note agents zero weight links affect others contribution
coalition. Finally, correlation clusteringa well-known clustering technique motivated
problem clustering large corpus objects, documents (e.g., web pages
weblog data given content/access patterns), customers service providers (with
given properties past buying/selling records) biological species (plants animals
given features)operates setting elements need partitioned clusters (by topic, location, behaviour etc.) characterised similarity
(and/or difference) relations among them. aim usually maximise overall
agreementi.e., correlationof clusters. example, given signed graph edge
label indicates whether two nodes similar (+) different (), task cluster
nodes similar objects grouped together, different onesseparately. Thus,
value cluster C given total sum positive intra-cluster edges negative inter-cluster edges one end C. cases, connected (either positively
negatively) members impact cluster values.
background, paper extend CSG problem connected sets.
precisely, introduce independence disconnected members consider coalition structures node set graph, endowed valuation function
property. formally defined Section 2 below, also give necessary graph
theoretic notation summarise main contributions. Then, Sections 3, 4 5,
discuss results great detail present proofs. Specifically, Section 3 provides
computational bounds coalition structure generation general graphs, Section 4
introduces technique solving problem using tree decompositions. technique,
particular, allows us show linear time solvability graphs bounded treewidth.
Section 5, apply derive upper bounds graphs separator theorems and,
particular, planar graphs minorfree graphs. also present negative result
showing NPhardness problem planar graphs hence, Kk minorfree
graphs, even simple, called edge sum, valuation function. discuss related
literature Secion 6. Finally, Section 7 concludes paper.

2. Coalition Structure Generation Graphs
section, formalise concepts independence disconnected members
graph coalition structure generation, list main contributions. completeness,
first provide graphtheoretic definitions notation necessary presentation
results following sections.
2.1 GraphTheoretic Definitions Notation
Let N set elements let Pk (N ) stand set k-element subsets
set N . simple undirected graph G pair G = (N, E) N finite set elements,
167

fiVoice, Polukarov, & Jennings

called vertices (or, nodes) G, E subset P2 (N )i.e., E collection
two-element subsets N representing connections nodes, called edges G.
complete graph graph pair nodes connected edge.
complete graph n nodes denoted Kn . graph G bipartite graph vertices
divided two disjoint sets N1 N2 every edge connects vertex
N1 one N2 . complete bipartite graph, G = (N1 N2 , E), bipartite graph
two vertices, n1 N1 n2 N2 , {n1 , n2 } edge G. complete
bipartite graph |N1 | = |N2 | = n, denoted Km,n .
undirected graph H called minor graph G H obtained G
series vertex deletions, edge deletions and/or edge contractions (removing edge
graph simultaneously merging together two vertices used connect).
graph G H minorfree H minor G. graph G planar K5 minorfree
K3,3 minorfree. important property planar graph embedded
plane, i.e., drawn way edges cross other. familiar
special case planar graphs class grids: finite grid graph, vertices
associated two indices 1 r 1 j c, edge connecting
node ni,j nodes ni+1,j ni,j+1 (if exist)thus, r rows c
columns graph, number nodes n = rc.
subgraph H graph G induced pair nodes x H, {x, y}
edge H edge G. words, H induced subgraph
G exactly edges appear G vertex set. vertex set
H subset N vertex set G, H said induced S.
path graph sequence nodes node edge
next node sequence, path called simple contains repeated nodes.
graph said connected path every pair nodes graph.
tree graph two nodes connected exactly one simple path.
Many algorithms graphs become easy input graph tree tree-like.
notion tree-like formalised using concept treewidth: treewidth
graph small, tree-likein particular, tree treewidth 1. Treewidth
defined using concept tree decompositiona mapping graph tree.
Formally, tree decomposition G = (N, E) pair (X, ), X = {X1 , . . . , Xm }
n = |N | family subsets N , tree whose nodes subsets Xi ,
satisfying following properties: (i) union sets Xi equals N is, graph
vertex associated least one tree node; (ii) every edge {x, y} graph,
subset Xi contains x y; (iii) Xi Xj contain vertex x,
nodes Xk tree (unique) path Xi Xj contain x welli.e.,
nodes associated vertex x form connected subset (equivalently, Xi , Xj
Xk nodes, Xk path Xi Xj , Xi Xj Xk ). width
tree decomposition size largest set Xi minus one. Finally, treewidth
graph G minimum width among possible tree decompositions G.
Given notation, formally define problem coalition structure
generation graphs.
168

fiCoalition Structure Generation Graphs

2.2 Model
Recall coalition structure set elements N defined collection
disjoint exhaustive subsets N1 , . . . , Nm Ni Nj = 1 i, j

i=1 Ni = N . Given setting finite set elements N connected undirected
graph G = (N, E) coalition valuation function v : P(N ) R subsets N ,
v() = 0, consider class coalition structure generation problems N .
Accordingly, make following definitions.
Definition 1 graph G = (N, E), function v : P(N ) R independent disconnected members (IDM) i, j N (i, j)
/ E, coalition C i, j
/ C,
v(C {i}) v(C) = v(C {i, j}) v(C {j}).

means agent contributes coalition C exactly amount
coalition C {j} j directly connected. is, presence agent j
affect marginal contribution agent separating coalition. Note
Definition 1 generally restrict effects agents may
connected.
give example, suppose edge {i, j} E associated constant
weight vi,j R. Then, coalition valuation function
v(C) =

X

vi,j

{i,j}E:i,jC

IDM property. shall term function edge sum coalition valuation function. function important naturally arises many application scenarios (e.g.,
communication networks, information multi-agent systems) simple representation. work Deng Papadimitriou (1994), function studied context
complexity cooperative game-theoretic solution concepts.
functions type arise familiar clustering settings. example,
suppose edge {i, j} labeled + depending whether j
deemed similar different. coalition (or, cluster) C N , let E + (C) =
{{i, j} = + | i, j C} denote set positive intra-cluster edges, let E (C) =
{{i, j} = | C, j
/ C} set negative inter-cluster edges one end C.
Then, correlation coalition valuation function defined
v(C) = |E + (C)| + |E (C)|

satisfies IDM condition. Note function takes account intra-
intercoalitional connections, thus different edge sum, considers
intracoalitional links. Maximising sum coalitional values coalition structures,
produces partition nodes agrees much possible edge labels.
objective pursued paper Bansal, Blum Chawla (2003) show
NP-completeness problem complete graphs provide several approximation
results.
Yet another example IDM function found multi-agent scenarios coalitions agents work different parts global project. settings, members
coalition must make joint decisions communicate coalitions agents
coordinate actions. Furthermore, collaboration communication possible
169

fiVoice, Polukarov, & Jennings

closely connected agents, important coalition includes agents
mutual neighbours outside coalition, decisions made coordinated
coalitions. Given this, coalition valuation function
v(C) =

X

ni (C)

iC

ni (C) number agent pairs (j, k) N N j C, k
/ C
{i, j}, {i, k} E, IDM property. shall term function coordination coalition
valuation function. Obviously, considering intercoalitional links, function different
edge sum. However, note also difference coordination
correlation functions. latter, effect link two agents value
coalition determined link label whether agents
belong coalition. contrast, coordination function accounts fact 3-agent
cliques, two agents members coalition one outsider.
analysis, however, restricted particular valuation function rather
covers class functions characterised Definition 1. define graph coalition
structure generation (GCSG) problem follows.
Definition 2 Given connected undirected graph G = (N, E) coalition valuation
function v : P(N ) R independent disconnected members,
graph coalition
P
structure generation problem G maximise v(C) = CC v(C) C coalition
structure N .
GCSG posed clustering graph partitioning problem sum cluster
values, given IDM valuation function, maximised. instance,
aforementioned correlation clustering special case GCSG. Note, however,
clustering problems general necessarily fit model: indeed,
objectives admit IDM property; hand, clustering
problems additional restrictions feasible graph partitions. example, one
natural objectives domain maximise modularity clusters (Brandes,
Delling, Gaertler, Gorke, Hoefer, Nikoloski, & Wagner, 2008) given sum clus
2
|(E(C)|+|E(C)|
ter values defined follows. cluster C, let v(C) = |E(C)|

,
|E|
2|E|
E(C) = {{i, j)} E : i, j C} set intra-cluster edges C E(C) =
{{i, j} E : C, j
/ C} set inter-cluster edges. Notice second term
valuation funciton squared, implies violation IMD property. Another related setting weighted graph partitioning problem nodes edges
(non-negative) weights aim divide graph k disjoint parts
parts approximately equal weight size edge cut minimised. Crucially,
unlike model, case number subsets feasible partition fixed.
2.3 Main Results
Here, main results paper summarised. start observing
GCSG problem NPcomplete general graphs, even edge sum valuation functions
(Section 3). Alongside hardness result, show thata general instance |N | = n
nodes |E| = e edges solved time n2 e+n
(see Theorem 3).
n
170

fiCoalition Structure Generation Graphs

order improve time required solving problem, make use tree
decompositions. show graph n nodes tree decomposition width
w, GCSG problem O(ww+O(1) n). allows us derive upper bound
computational complexity GCSG certain classes graphs, namely graphs bounded
treewidth, graphs separator theorems and, particular, planar graphs minorfree
graphs. also show subclass edge sum GCSG problems NPhard planar
graphs hence, Kk minorfree graphs k 5 (see Section 5.1).
Planar graphs exceptional family graph drawn plane
without edge crossing. Apart interesting mathematical properties as,
example, 4colourability 3path separability, planar graphs many practical
applications, including design problems circuits, subways utility lines. network
crossing connections, usually means edges must run different heights.
big issue electrical wires, would create extra expenses
types linese.g., burying one subway tunnel another (and therefore deeper
one would normally need). Circuits, particular, easier manufacture
connections live fewer layers. Importantly, one may determine graphs planarity using
called forbidden minor characterisation, graph planar
contain complete graph K5 complete bipartite graph K3,3
minor (Wagner, 1937).1 Remarkably, forbidden minor characterisations exist several graph families vary nature forbidden, utilised
combinatorial algorithms, often identifying structure (Robertson & Seymour, 1983,
1995, 2004). motivates particular interest classes minorfree graphs.
next theorem main technical result.
Theorem 1 general instance graph coalition structure generation problem
graph G n nodes known tree decomposition width w solved
O(ww+O(1) n) computational steps.
gives us immediate corollary.
Corollary 1 fixed w, GCSG problem graph G n nodes maximum treewidth w solved O(n) computational steps.
proof results presented Section 4. Coupled known results regarding
separator theorems gives base following contributions (see Section 5
proofs).
Corollary 2 graph H k vertices, instance graph coalition structure

n+O(1) )
generation problem Hminorfree
graph
G

n
nodes
requires
O(n
p
computation steps = 0.5k k/(1 2/3).
Corollary 3 general instance graph coalitionstructure generation problem
n+O(1) ) computation steps, =
planar
graph
p G n nodes solved O(n

2/(1 2/3).
1. characterisation Wagners theorem closely related (but equivalent) Kuratowskis theorem, states graph planar contain subgraph subdivision
K5 K3,3 (Kuratowski, 1930).

171

fiVoice, Polukarov, & Jennings

However, planar graphs also prove following hardness result.
Theorem 2 class edge sum graph coalition structure generation problems planar graphs NPcomplete. Moreover, 3-SAT problem clauses represented
GCSG problem planar graph O(m2 ) nodes.
Note Theorem 2 holds Kk minorfree graphs k 5, planar graphs

special case. means expect take time exponential n solve
GCSG problem graphs size n. suggests methods given

Corollaries 2 3, solve problems time exponential log(n) n, close
best possible.
background, main contribution work shows significant
improvement complexity exact algorithms general class coalition structure generation problems characterised single assumption independence disconnected
members valuation functions. particular, results especially valuable
graphs tree decomposition (low) width assessed.
remaining sections describe main results techniques detail
contain proofs.

3. General Graphs
section, examine complexity coalition structure generation general
graphs. first step, make technical observation showing without loss
generality problem restricted subset coalition structures follows.
Definition 3 graph G = (N, E), coalition structure C N connected
induced subgraph G C connected C C.
Lemma 1 imply GCSG problem equivalent maximising
objective function connected coalition structures Definition 3. note
lemma follows directly Definition 1 IDM property provide full proof
appendix.
Lemma 1 Given graph G = (N, E) coalition valuation function v() IDM
property, A, B N edges G \ B B \ A,
v(A) v(A B) = v(A B) v(B).
Note, Definition 1, v() IDM two coalitions B C
disconnected, Lemma 1, v(B C) = v(B) + v(C). So, coalition C, value
v(C) equal sum v() connected components. deduce that,
coalition structure C exists coalition structure v(C) = v(D)
coalitions connected subgraphs. Thus, without loss generality, restrict
attention connected coalition structures. Moreover, G connected graph,
solve coalition structure problem G IDM coalition valuation
function finding optimal coalition structure connected component G
combining results. operation testing connectivity finding connected
172

fiCoalition Structure Generation Graphs

components computationally tractable polynomial time (Hopcroft & Tarjan, 1973),
so, without loss generality, restrict attention connected graphs G.
(connected) graph G = (N, E) set nodes N set edges E,
denote |N | = n |E| = e. Next, present simple algorithm constructing optimal
coalition structures N , based following observation. Note every
connected coalition structure N expressed connected components
subgraph G0 = (N, E 0 ) G, E 0 E. Moreover, connected component
spanning subtree, restrict attention acyclic subgraphs G. Given this,
Algorithm 1 runs acyclic subgraphs G connected components,
correspond connected coalition structures set nodes N . would like
remark order subgraphs G checked, effect
outcome, chosen arbitrarily. Thus, w.l.o.g., initialise procedure
coalition structure C = ({n1 }, . . . , {nn }) corresponds connected components
subgraph G0 = (N, ) G.
Algorithm 1 algorithm coalition structure generation general graphs.
1:
INPUT: connected undirected graph G = (N, E);
2:
IDM coalition valuation function v : P(N ) R
3:
OUTPUT: optimal connected coalition structure N w.r.t. v
4:
C ({n1 }, . . . , {nn })
5:
E 0 E G0 = (N, E 0 ) acyclic
6:
find C(G0 ) = ({C1 }, . . . , {Ck0 })the collection connected components G0
P 0
7:
v (C(G0 )) = ki=1 v(Ci ) > v(C)
8:
C C(G0 )
9:
end
10: end
show following.
Theorem 3 Algorithm 1 solves general instance GCSG problem n2
steps, using O(n log n) sized memory.

e+n
n



0
0
0
Proof : acyclic subgraph


E,a
ata+1
n1aedges,


Pn1 e G = (N, E ) G, E

a+1
k=0 k subgraphs. Since b + b1 = b b b ,

2
sum bounded e+n
determine connected
n . Now, takes O(n ) steps
e+n
2
components subgraph, and, thus, n n
steps needed check
coalition structure. Finally, takes O(n log n) sized memory store
coalition checked.
2

Coupled Corollary 2.3 paper P. Stanica (2001), Theorem 3 implies
following result sparse graphs.
Corollary 4 sparse graphs e = cn edges, c constant, GCSG problem

c+1
.
n3/2 n constant = (c+1)
cc
easy particularly promising result, may exponential n log(n)
exponential n even sparse graphs. Indeed, class graph coalition structure
173

fiVoice, Polukarov, & Jennings

generation problems NPhard: contains subclass GCSG problems complete
graphs, equivalent NPcomplete class standard coalition structure generation problems node sets. Importantly, problem remains hard even simple
coalition valuation functions, correlation function (Bansal et al., 2003). note
holds edge sum function well: result seen corollary
Theorem 2 showing hardness edge sum GCSG planar graphs.

4. Tree Decompositions
consider solving GCSG problem graphs known tree decompositions.
Specifically, prove main technical result (Theorem 1) giving general bound
GCSG graphs, derive Corollary 1 regarding graphs bounded
treewidth. proof follows recursively calculating potential marginal contributions total coalition structure valuation branch tree decomposition (see
Algorithm 2). build intuition, first derive two technical lemmas. brevity
exposition, proofs presented Appendix.
Lemma 2 Let G = (N, E) graph tree decomposition (X, ), X =
{X1 , . . . , Xm } n = |N | tree X. Suppose Xi
numbered order shortest distance X1 , X1 may chosen arbitrarily.
Then, C N ,
v(C) =


X

v(C Xi ) v C Xi

i=1

[


Xj .

j<i

Lemma 2 allow us calculate value total coalition structure local
structures defined branches tree decomposition. discuss construct
total structure local ones. need following notation.
graph G = (N, E), P, Q N , P coalition structure P
Q coalition structure Q, define
U (P, Q) = {A P : (P \Q)}{B Q : B (Q\P )}{AB : P, B Q, AB 6= }.
is, U (P, Q) collection subsets P Q agrees P P \ Q
Q Q \ P , contains pairwise unions subsets P B Q non-empty
intersections. Note U (P, Q) necessarily coalition structure P Q,
union coalitions B, P, B Q, need disjoint.
Furthermore, graph G = (N, E) coalition structure P subset
nodes P N , subset P 0 P denote P(P 0 ) coalition structure
P 0 defined follows:
P(P 0 ) = {C P 0 : C P}.
is, x, P 0 P , belong coalition P(P 0 )
belong coalition P.
illustration, consider following example. Let N = {1, 2, 3, 4, 5}, take two subsets
P = {1, 2, 3} Q = {3, 4, 5} N , define coalition structures P = {{1}, {2, 3}}
Q = {{3, 4}, {5}} P Q, respectively. Note {1} P subset
174

fiCoalition Structure Generation Graphs

P \ Q, {5} Q subset Q \ P , ({2, 3} P) ({3, 4} Q) = {3}. Then,
U (P, Q) = {{1}, {5}, {2, 3} {3, 4}} = {{1}, {5}, {2, 3, 4}}. Now, let P 0 = {1, 2} P
Q = {4, 5} Q. Then, P(P 0 ) = {{1} {1, 2}, {2, 3} {1, 2}} = {{1}, {2}}
Q(Q0 ) = {{3, 4} {4, 5}, {5} {4, 5}} = {{4}, {5}}.
Lemma 3 graph G = (N, E), P, Q N , P coalition structure
P Q coalition structure Q, P(P Q) = Q(P Q), E = U (P, Q)
coalition structure P Q P 0 P , Q0 Q, E(P ) = P(P 0 )
E(Q) = Q(Q0 ).
ready prove Theorem 1. end, present Algorithm 2 that,
given graph known tree decomposition, finds best coalition structure node
set recursively calculating potential marginal contributions total coalition structure valuation branch given tree decomposition. Lemma 4 proves
validity computational bounds.

Algorithm 2 algorithm coalition structure generation graphs known tree
decompositions.
1:
INPUT: connected undirected graph G = (N, E);
2:
tree decomposition (X, ) G, X = {X1 , . . . , Xm } n,
3:
tree X, 1 < j dT (Xi , X1 ) dT (Xj , X1 ),
4:
1 m, dT (Xi , X1 ) distance Xi X1
5:
IDM coalition valuation function v : P(N ) R
6:
OUTPUT: optimal connected coalition structure N w.r.t. v
7:
1
8:
Yi Xi \ j<i Xj
9:
Zi Xi \ Yi
10:
Di {j > : (Xi , Xj ) }
11: = m, 1, . . . , 1
12:
Ccoalition
P structures Zi P
13:
vi (C) maxE CE v(C) v(C \ Yi ) + jDi vj (E(Zj )),
14:
E coalition structures Xi E(Yi ) = C
15: end
16: C0 arg maxC v1 (C) C colition structures Z1
17: k = 1, . . . ,
18:
Ck U (Ck1 , Ek ),
19:
Ek coalition

P structure Xk
P Ek (Zk ) = Ck1 (Zk )
20:
vk (Ck1 (Zk )) = CEk v(C) v(C \ Yk ) + jDk vj (Ek (Zj ))
21: end
22: output Cm
Lemma 4 Algorithm 2 solves general instance graph coalition structure generation problem graph G n nodes known tree decomposition width w
O(ww+O(1) n) computational steps.
175

fiVoice, Polukarov, & Jennings

Proof : given graph G = (N, E) tree decomposition (X, ),
X = {X1 , . . . , Xm } n = |N | tree X. Suppose w, |Xi | < w
i. assume without loss generality Xi numbered order shortest
distance X1 , X1 may chosen arbitrarily. Thus, > 1, Xi
exactly one link connects Xj j < i. define Yi
Xi \ j<i Xj Zi Xi \ Yi . Note, > 1 exists single j <
Zi Xj , hence Zi = (Xj Xi ). Since every node must least one Xi ,
union Yi N . Finally, i, Di set j > (Xi , Xj )
edge .
Now, = m, 1, . . . , 1, algorithm recursively define functions vi ()
give real values coalition structure Zi . C, coalition structure Zi ,
let vi (C) maximum
X
X
vj (E(Zj )),
v(C) v(C \ Yi ) +
jDi

CE

coalition structures E Xi E(Yi ) = C. Note j Di , Zj =
(Xi Xj ), hence, coalition structure E Xi , E(Zj ) forms coalition structure
Zj .
Now, suppose C coalition structure G. show v(C) v1 (C(Z1 )).
showing inductively that, k 1,
v1 (C(Z1 ))

k
X
X

(v(C) v(C \ Yi )) +

i=1 CC(Xi )

X

vj (C(Zj )).

(1)

jDi :j>k

k = 1 follows definition v1 (), C(X1 ) coalition structure X1 .
sufficient show right hand side (1) increase k increases.
general k change right hand side (1) preceeding iteration
X
X
(v(C) v(C \ Yk )) +
vj (C(Zj )) vk (C(Zk )).
jDk

CC(Xk )

follows definition vk () value non-positive, (C(Xk )) coalition
structure Xk . Hence, inductive proof complete. Thus, shown
v1 (C(Z1 ))


X
X

(v(C) v(C \ Yi )) = v(C),

i=1 CC(Xi )

Lemma 2. So, maximum v1 (E) coalition structures E Z1 greater
equal maximum value v(C) coalition structures C G.
Now, let C0 coalition structure Z1 maximises v1 (C). algorithm
recursively defines coalition structures C1 , C2 , . . . Cm setting, 1 < k m, Ck =
U (Ck1 , Ek ), Ek coalition structure Xk Ek (Zk ) = Ck1 (Zk )
X
X
vk (Ck1 (Zk )) =
v(C) v(C \ Yk ) +
vj (Ek (Zj )).
CEk

jDk

176

fiCoalition Structure Generation Graphs

want show
v1 (C(Z1 )) =

k
X

X

(v(C) v(C \ Yi )) +

i=1 CCk (Xi )

X

vj (Ck (Zj )).

(2)

jDi :j>k

Again, use induction. k = 1, follows definition v1 (), noting
since C1 = U (C0 , E1 ), Lemma 3 implies C1 (X1 ) = E1 (X1 ), since coalition
structures X1 , must C1 = E1 .
Now, general k, since Ck = U (Ck1 , Ek ), must have, < k, Ck (Xi ) =
Ck1 (Xi ), j Di j k, since Zj Xi , have, Ck (Zj ) = Ck1 (Zj ).
Thus, change right hand side (2) previous increment equal
X
X
(v(C) v(C \ Yi )) +
vj (Ck (Zj )) vk (Ck (Zk ))
=

CCk (Xk )

jDk

X

X

(v(C) v(C \ Yi )) +

vj (Ek (Zj )) vk (Ck1 (Zk )) = 0,

jDk

CEk (Xk )

definition Ck Ek . completes inductive proof.
shown
v1 (C(Z1 )) =


X

X

(v(C) v(C \ Yi )) = v(Cm ).

i=1 CCm (Xi )

Since v1 (C(Z1 )) upper bound v() coalition structures N , must
Cm solution coalition valuation problem.
Finally, order solve coalition valuation problem, needs done
fully calculate vk () k 1, recording corresponding optimal coalition
structures value, optimise v1 (). this, k, go
coalition structure E Xk , calculate
X
X
v(C) v(C \ Yi ) +
vj (E(Zj )).
CE

jDi

greater currently held value vk (E(Zk )), replace value also
record E. requires polynomial (in w) calculations possible coalition structure
node Xk , gives O(ww+O(1) ) calculations Xk thus O(ww+O(1) n)
calculations total.
2
Theorem 1 follows immediately Algorithm 2 Lemma 4. Now, given w,
class graphs maximum treewidth w, tree decomposition width
w may found linear time (Bern, Lawlerand, & Wong, 1987). Given this, Corollary 1
directly implied Theorem 1.
Corollary 1 fixed w, GCSG problem graph G n nodes maximum treewidth w solved O(n) computational steps.

177

fiVoice, Polukarov, & Jennings

set w = 1, result applies acyclic graphs, related results Demange (2004) regarding coalition structure generation trees. However, Demange (2004)
make IMD assumption. resulting algorithm complex
potentially exponential running time. expected, without independence disconnected members, coalition structure generation problem star
networks necessarily exponential.
Note, set w = 2, class graphs consideration becomes class
K4 minorfree graphs. Likewise, class graphs treewidth 1 may characterised
K3 minorfree. results sharp contrast Theorem 2 shows NPcompleteness edge sum GCSG problem planar graphs, subset
class K5 minorfree graphs. give proof Theorem 2 next section.

5. Separator Theorems
section, prove computational bounds GCSG problem minorfree
planar graphs. graphs guaranteed contain vertex separators, formalised
Definition 4 below. Intuitively, means graphs corresponding class
split smaller pieces removing small number vertices. general,
Definition 4 class graphs G satisfies f (n)-separator theorem constant < 1
G = (N, E) G |N | = n exists subset N |S| f (n)
N \ = B disjoint B where, |A| n, |B| n, exists
x B (x, y) E.
illustrate this, consider example grid graph G r rows c columns,
n = rc number nodes. r odd, single central row, otherwise,
two rows equally close center; similarly, c odd, single
central column, otherwise, two columns equally close center. Let
node subset central rows columns. Removing graph
divide two smaller disjoint components, B, n/2

vertices. r c, central column defines separator r n vertices,

similarly, c r, central row separator n vertices. Thus,

grid graph separator size n, removal splits graph
two connected components, size n/2. is, class grid graphs

satisfy n-separator theorem constant = 1/2.
use Theorem 1 derive Algorithm 3 Lemma 5, provide us
general result classes graphs satisfy separator theorems. apply
result classes minorfree planar graphs, coupled corresponding
separator theorems, obtain computational bounds coalition structure generation
graphs.
Suppose class graphs G closed taking subgraphs satisfies
f (n)-separator theorem constant < 1, f (n) = nc constants
, c, exists algorithm find separator G G n nodes
polynomial time. Given this, graph G G, Algorithm 3 finds
tree decomposition treewidth w nc /(1 c ) polynomial time. procedure
based proof Theorem 20 work Bodlaender (1998), states
178

fiCoalition Structure Generation Graphs

class graphs G, treewidth G G n nodes O(f (n)).
apply Algorithm 2 solve GCSG problem G tree decomposition
O(ww+O(1) n) computational steps, finally provides us computational bound
c

O(n 1c n

c +O(1)

) time, stated Lemma 5 below.

Algorithm 3 algorithm coalition structure generation graphs separator
theorems.
1:
INPUT: graph G = (N, E) G; IDM coalition valuation function v : P(N ) R
2:
OUTPUT: optimal connected coalition structure N w.r.t. v
3:
n = 1
4:
X ({x}), x N node G
5:
T0 G
6:
otherwise
7:
find S, nc separator G N \ = B |A| n |B| n
c nc
8:
find tree decompositions (X , ) (X B , B ) B, width
1c
9:
0 B {e0 } e0 = {x, y} E, x A, B
10:
X {X : X X }
11:
X B {X : X X B }
12:
X X X B
13: end
14: apply Algorithm 2 G tree decomposition (X, 0 )

Lemma 5 Let G class graphs closed taking subgraphs satisfies
f (n)-separator theorem constant < 1, f (n) = nc constants , c,
exists algorithm find separator G G n nodes polynomial
time. Then, Algorithm 3 solves GCSG problem graph G G n nodes
c
O(nn +O(1) ) time,
c
=
.
1 c
Proof : Suppose class graphs G satisfying statement lemma.
must exist constants K > log (2) G G n nodes, find
nc separator, constant , Knd computational steps. proof proceeds
showing that, graph G G n nodes, Algorithm 3 (steps 313) finds tree
decomposition width less equal nc /(1 c ) Knd /(1 2d )
computational steps. prove result induction.
n = 1 computational steps required G already tree form. nth
inductive step, suppose G = (N, E) G |N | = n. Knd computational
steps find S, nc separator G N \ = B |A| n
|B| n. inductive hypothesis, apply steps 313 Algorithm 3 find tree
decompositions (X , ) (X B , B ) subgraphs B respectively, taking
total time 2Kd nd /(1 2d ), (X , ) (X B , B ) maximal width
c nc /(1 c ). Now, let X = {X : X X }, let X B = {X : X X B },
let 0 tree formed connecting B single edge. Then, claim
179

fiVoice, Polukarov, & Jennings

(X X B , 0 ) tree decomposition G. \ S, set elements
X appears in, forms subtree , thus, set elements X X B
appears in, must form subtree 0 . symmetry, holds B \ S.
Further, S, appears every element X X B . Lastly, pair nodes
connected edge G, nodes lie inside B,
element X X B respectively, otherwise least one nodes must lie
S, must member every element X X B . proves claim.
tree decomposition (X X B , 0 ) took
Knd +

2Kd nd
Knd
=
1 2d
1 2d

computational steps find width
nc +

c nc
nc
=
,
1 c
1 c

required. completes inductive proof.
Thus, G G n nodes, find tree decomposition G treewidth
nc /(1 c ) polynomial time. apply Algorithm 2, solve
GCSG problem graph G G n nodes O(ww+O(1) n) computational steps,
w = nc /(1 c ). However,
ww =


c
c
c
ncn /(1 ) = O(nn ),
1 c
2

statement lemma follows.

result allows us obtain computational bounds GCSG problem minor
free planar graphs follows.
Corollary 2 graph H k vertices, instance graph coalition
struc n+O(1) )
ture generation problem anH minorfree
graph
G

n
nodes
requires
O(n
p
computation steps = 0.5k k/(1 2/3).
Proof : apply Lemma 5 using main result paper Alon,Seymour
Thomas (1990) shown class graphs satisfies k kn-separator

n+O(1) )
theorem
p Thus, solve general instance problem O(n
= 2/3.
= k k/2(1 2/3), required.
2

noted Proposition 4.5 Alon, Seymour Thomas (1990) gives
bound k kn treewidth class graphs, constructive, cannot
combined Theorem 1 requires tree decomposition available.
planar graphs, Corollary 3 provides stronger result.
Corollary 3 general instance graph coalition structure
generation problem

planar graph G n nodes solved O(n n+O(1) ) computation steps,
180

fiCoalition Structure Generation Graphs

=



2/(1

p
2/3).

Proof : apply Lemma 5 using main result workof Lipton Tarjan (1979)
shown class graphs satisfies 2 2n-separator theorem



n+O(1)
=2/3. Thus,
)
p solve general instance problem O(n
= 2/(1 2/3), required.
2
Recall class planar graphs equivalent class K3,3 K5 minorfree
graphs. graphs, Theorem 2 shows graph coalition structure generation
problem NPcomplete, even simple, edge sum, coalition valuation functions (the proof
theorem presented 5.1 below). However, mentioned previous section,
GCSG smaller minorfree instances solved linear time.
5.1 Planar Graphs
prove NP-hardness result planar graphs. Since planar graphs K5 minor free,
hardness result must hold class Kk minorfree graphs k 5.
proof proceeds finding representation general 3-SAT problem GCSG problem
planar graph.
Theorem 2 class edge sum graph coalition structure generation problems planar graphs NPcomplete. Moreover, 3-SAT problem clauses represented
GCSG problem planar graph O(m2 ) nodes.
Proof : Suppose 3-SAT problem clauses C1 , . . . Cm . construct
edge sum graph coalition structure generation problem planar graph O(m2 ) nodes
which, solved, reveals solution 3-SAT problem one exists. use
series diagrams define components construct appropriate
edge sum graph. diagrams denote edge values using symbols given key
Figure 1.
first component given Figure 2. use symbol Subfigure 2b
represent three nodes surround subgraph edge values given Subfigure 2a.
subgraph edge sum problem graph, contribution edge values
make valuation coalition structure 2, equality induced
structure three outer nodes shown one Subfigure 2c, Subfigure 2d
Subfigure 2e. induced coalition structure three nodes one
two structures, contribution less 2. similarly describe two
triangular components Figures 2, 4 5. planar graph edge sum problem
construct created components, connected edges
value 1, others overlap, sense share nodes.
components sharing nodes other, share edges. Moreover,
components share nodes form triangle borders component.
two components share pair nodes, represent symbolically drawing
symbols adjacent along corresponding side triangular
181

fiVoice, Polukarov, & Jennings

1
0
-2

(a) Edge values
Figure 1: Edge
value key.

(b) Symbol

(c) Optimum 1

(d) Optimum 2

(e) Optimum 3

Figure 2: Edge sum problem component.

(a) Edge values (b) Symbol (c) Optimum 1 (d) Optimum 2

(a) Edge values

(b) Symbol

(c) Optimum

Figure 4: Edge sum problem component.

Figure 3: Edge sum problem component.

symbols. So, edges symbols components touch, mean
components share edge within graph.
graph consisting components, constructed way, say
coalition structure locally optimal induced structure every component
optimal component every connecting edge part component
lies inside coalition. every coalition structure, component, contribution
edges component make value coalition structure bounded
local optimum. Thus, coalition structure locally optimal must
optimal. Furthermore, coalition value coalition structure straightforward
calculate - simply sum local optimums component connecting edge. Note,
value obtained always represents upper bound total valuation
coalition structure, thus locally optimal structure exists, optimal coalition
structures must locally optimal. However, guaranteed locally optimal
structure exist.
mind, possible provide intuition regarding components.
component Figure 5 coalition structure locally optimal
unless three nodes form outer triangle either lie coalition
different coalitions. component Figure 3 coalition structure
locally optimal unless exactly one bottom two nodes coalition
top node. component Figure 2 similar Figure 3, except allows
addition possibility locally optimal coalition structure three outer nodes
different coalitions. component Figure 4, coalition structure
locally optimal bottom two node coalition, coalition
contain top node. describe constructs made
described components. first given Figure 6. locally
optimal coalition structure, nodes X always coalition pair
nodes labelled lie coalition pair nodes
182

fiCoalition Structure Generation Graphs







X

(a)

Construc-



X

B

B

(b) Symbol
(a) Construction

tion

(b) Optimum 1




X

(c) Optimum 1

(d) Optimum 2

Figure 5: Edge sum problem
component.
B

(c) Optimum 2
Figure 6: Edge sum problem construct.

labelled B lie coalition other. reduction 3-SAT problems,
representing logical states whether certain pairs agents lie
coalition locally optimal coalition structure. construct allows us enforce
two pairs represent logical state whilst also allowing coalition passes
plane.
second third constructs given Figures 7 8. second construct,
locally optimal coalition structure, pair nodes labelled together
coalition, pair nodes labelled B coalition, similarly
pair nodes labelled C. pair nodes labelled coalition,
pair nodes labelled B coalition, similarly pair
nodes labelled C. Thus, representation 3-SAT problem, locally optimal
solution pairs nodes labelled A, B C always represent logical state.
third construct similar, except locally optimal coalition structure,
state whether pair nodes labelled C coalition
opposite state two pairs nodes. Thus, representation
3-SAT problem, locally optimal solution pairs nodes labelled B
represent logical state, C represent negation state. last
construct given Figure 9. complex constructs, shall
first examine three subgraphs it. first part, AX, consists subgraph three
components pair nodes labelled pair nodes labelled X, second,
covers three components B CZ consists bottom two components.
Note middle triangle diagram edges X, Y, Z, component, merely
183

fiVoice, Polukarov, & Jennings







B

B

B

C

C
(a) Construction

C

(b) Optimum 1

(c) Optimum 2

Figure 7: Edge sum construct.

B


B


B


C

C

(a) Construction

(b) Optimum 1

C
(c) Optimum 2

Figure 8: Edge sum construct.

empty space. Subfigures 9b9h show locally optimal coalition structures
three parts (with outer nodes component shown). Since
construct union three parts, coalition structure locally optimal
subgraphs, locally optimal whole construct. However,
every combination local optimums possible. For, coalition structure induces
Subfigure 9b AX Subfigure 9d three node triangle XYZ must lie
coalition, possible coalition structure induce Subfigure 9f.
locally optimal coalition structure, cannot true node A, B C lies
different coalition node paired with. Suppose think pair nodes
representing false state lie coalition true state
different coalitions. Then, locally optimal coalition structure construct,
least one A, B C must represent true state. straightforward check
exist locally optimal coalition structures construct induce every possible
combination states besides A, B C represent falsehood. Thus,
construct enforces logical within 3-SAT solution representation. construct
edge sum problem represent general 3-SAT problem follows. create copy
construct Figure 9 clause problem. three pairs labelled A, B, C
identified three literals corresponding clause. identify coalition
structure constructs set logical values literals clauses
saying literal associated pair node set true nodes
lie inside single coalition. variable create path copies constructs
184

fiCoalition Structure Generation Graphs

X





B

Z

X





X

C
(a) Construction

(b) AX: Optimum 1

(c) AX: Optimum 2

Z


(d) BY: Optimum 1

B

Z

Z

B



C

(e) BY: Optimum 2

C

C

(f) CZ: Optimum 1 (g) CZ: Optimum 2 (h) CZ: Optimum 3

Figure 9: Edge sum construct.

Figures 7 8, pair nodes labelled B one component shared
labelled following component. path include copy construct
Figure 7 literal representation variable, copy construct
Figure 8 literal representation variables negation. connect
pair nodes represents literal representation variable negation
pair nodes labelled C corresponding construct path, using parallel pair
connecting edges, value 1. ensures locally optimal coalition structure
assign consistent logical values literal representations variable
negative. ensure resulting graph planar, replace two parallel
pairs connecting edges cross two copies construct
Figure 6. For, two copies construct Figure 6 first copy shares
nodes labelled B nodes labelled second copy, then, locally
optimal coalition structure, logical value represented nodes labelled
first construct equal logical value represented nodes labelled B second
construct. Furthermore logical value represented nodes labelled X two
constructs equal logical value nodes labelled two constructs.
allows logical values pass plane.
construction, locally optimal coalition structure exists, original 3-SAT
problem must satisfiable. Furthermore, 3-SAT problem satisfiable,
simply set construct locally optimal coalition structure agrees
logical value variables literals, create coalition structure
entire graph taking union overlapping coalitions. Note, always possible
construction. constructs Figures 7 9 designed induced
optimums, nodes never coalition node B C,
nodes B never coalition node C. Moreover, construct
Figure 6 locally optimal structure, coalition XY always disjoint
nodes B. means combining two locally optimal coalition structures
agree across pairs create coalitions local two pairs nodes
connected edges used connect them. Thus, combining several
connections always possible without contradiction.
185

fiVoice, Polukarov, & Jennings

B

B


!A

B

B

B

B

!B
B

!A
!A

C

!B
!C
!C

C
C

C
B

!A
C

C

Figure 10: Reduction (A B B) (!A!B!C) (!A B C).

So, locally optimal coalition structure exists original 3-SAT problem
satisfiable, given locally optimal coalition structure, identify solution
3-SAT problem. Furthermore, locally optimal coalition structure exists,
coalition structure optimal locally optimal. size graph
O(m2 ) thus result follows. example reduction process shown
Figure 10 3-SAT problem (A B B) (!A!B!C) (!A B C).
2

6. Related Work
section, give overview related work, broadly classified
two main categories: clustering algorithms algorithms coalition structure
generation (CSG). former relevant work deals partitioning
graph structures subgraphs; however, unlike case, values partitions
determined certain, problemspecific valuation function. latter,
hand, considers general valuation functions, allows structure primitive
set elements.
detail, clustering one primery tools machine learning deals
finding structure collection unlabeled data. goal organise objects
groupsclusterswhose members similar dissimilar
objects belonging clusters. certain relevant scenarios, instead actual description objects, relationships known. Thus, like
work, objects typically represented node set signed graph,
edge labels indicate whether two connected nodes similar different. However,
clustering algorithms usually designed solving problems associated particular
objectives (and hence, valuation functions)e.g., correlation modularity mentioned previous sections. contrast, work concerned general class
valuation functions, characterised single assumption independence disconnected members. Thus, particular, Corollary 1 viewed generalisation
result Xin (2011) providing linear time algorithm correlation clustering
graphs bounded treewidth. sense, literature CSG problem
survey below, perhaps relevant research, deals designing universal
186

fiCoalition Structure Generation Graphs

algorithms, valuation function part input. However, hand,
works assume structure primitive set elements.
several algorithms developed CSG. Sandholm, Larson, Andersson,
Shehory TohmeIn (1999), proposed anytime procedure worst case guarantees;
however, reaches optimal solution checking possible coalition structures,
runs time O(nn ). Specifically, given graph node set represents coalition
structures, connected edge belong two consequtive
levels coalition structure level (i 1) obtained one level
merging two coalitions one, algorithm firstly searches two bottom levels,
explores remaining levels one one, starting top moving downwards.
similar algorithm proposed Dang Jennings (2004): searching two
bottom one top level, algorithm goes certain subsets remaining
levels (as determined sizes coalitions present corresponding structures),
instead searching levels one one. hand, algorithms based dynamic
programming (DP) (Yeh, 1986; Rothkopf, Pekec, & Harstad, 1998) work iterating
coalition structures size 1, size 2, size n:
every coalition C, value coalition compared value could possibly
obtained splitting C two coalitions. Visualising process graph
coalition structures before, start bottom node move upwards
series connected nodes (a path) optimal node reached. Importantly,
multiple paths lead optimal node, DP reach
paths. Based observation, improved dynamic programming algorithm
(IDP) developed Rahwan Jennings (2008b). main idea IDP remove
edges coalitions structure graph disregard many splittings coalitions
possible, yet without losing guarantee path leads every node
graph. avoids counting approximately 2/3 operations compared DP
evaluates every edge coalition structure graph, meaning IDP find optimal
solution O(3n ) time. However, DP IDP algorithms anytimethat is,
allow trade computation time solution quality. end, Rahwan, Ramchurn,
Giovannucci Jennings (2009) developed integer partition (IP) algorithm,
anytime. works dividing search space regions, according coalition
structure configurations based sizes coalitions contain, performing
branch-and-bound search. Although procedure worst case complexity O(nn ),
practice, much faster DP based algorithms. Furthermore, IP algorithm
improved upon, using DP preprocessing (Rahwan & Jennings, 2008a). date,
combined algorithm, termed IDP-IP, fastest anytime algorithm, capable
finding optimal solution O(3n ) time.
CSG problem also tackled heuristic methods. particular, Sen
Dutta (2000) gave genetic algorithm starts initial, randomly generated, set
coalition structures, called population, repeatedly evaluates every member
current population, selects members based evaluation, constructs new
members selected ones exchanging and/or modifying contents. Keinnen (2009), based process Simulated Annealinga generic, stochastic local search
technique: iteration, algorithm explores different neighbourhoods certain
coalition structure, every neighbourhood defined according different criterion.
187

fiVoice, Polukarov, & Jennings

hand, Shehory Kraus (1998) proposed decentralised greedy procedure
iteration, best candidate coalitions (those overlap
coalitions currently present coalition structure) added structure,
search done distributive fashioni.e., agents negotiate one
searches coalitions. significantly improved distribution mechanism later
proposed Rahwan Jennings (2007). Another greedy algorithm (Mauro, Basile, Ferilli, & Esposito, 2010) based GRASPa general purpose greedy algorithm that,
iteration, performs quick local search try improve solution (Feo & Resende,
1995). CSG version GRASP, coalition structure constructed iteratively,
every iteration consists two steps: first add best candidate coalition
structure, second explore different neighbourhoods current structure.
two iterations repeated whole set agents covered, whole
process repeated achieve better solutions. However, heuristic techniques
guarantee optimal value reached point, give
means evaluating quality coalition structure selected.
alternative approach CSG problem utilise compact representation schemes
valuation functions proposed (Ohta, Conitzer, Ichimura, Sakurai, Iwasaki, & Yokoo,
2009). Indeed, practice, functions often display significant structure,
several methods developed represent concisely (e.g., set rules
compute function terms skills possessed agents types determining possible contribution coalition). Thus, marginal contribution nets,
MC-nets (Ieong & Shoham, 2005), CSG problem formulated mixed integer
program (MIP) (Ohta et al., 2009), solved reasonably well compared
IP algorithm, make use compact representations. However, general
problem stays NP-hard, also shown compact representations
synergy coalition groups (Conitzer & Sandholm, 2006) skill games (Ohta, Iwasaki,
Yokoo, Maruono, Conitzer, & Sandholm, 2006; Bachrach, Meir, Jung, & Kohli, 2010) (for
latter, authors also able define subclass instances problem
solved time polynomial number agents n number skills k).
agent-type representation, two dynamic programming algorithms proposed solve
CSG problem (Aziz & de Keijzer, 2011; Ueda, Kitaki, Iwasaki, & Yokoo, 2011),
run O(n2t ) time, number different types.
Another interesting direction look coalition structure generation framework distributed constraint optimisation problems (DCOPs) recently become
popular approach modeling cooperative agents (Modi, 2003). Thus, Ueda, Iwasaki,
Yokoo, Silaghi Matsui (2010) consider CSG problem multi-agent system represented one big DCOP, every coalitions value computed optimal solution
DCOP among agents coalition. Instead solving O(2n ) DCOPs,
authors suggest modifying big DCOP solving using existing algorithms, e.g.,
ADOPT (Modi, 2003) DPOP (Petcu & Faltings, 2005).
hand, Rahwan, Michalak, Elkind, Faliszewski, Sroka, Wooldridge Jennings (2011) proposed constrained coalition formation (CCF) framework,
constraints coalition structures formed. particular, CCF problem given set agents, set feasible coalition structures characteristic
function assigning values coalitions appear feasible coalition structures.
188

fiCoalition Structure Generation Graphs

Although general case, notion feasibility defined coalition structures,
many settings interest constraints implied coalition structures reduced
constraints individual coalitionssuch settings termed locally constrained. represent constraints succinctly, authors propose use propositional logic.
define natural subclass locally constrained CCF problems develop
algorithm solve CSG problem based divide-and-conquer techniques.
Finally, couple recent papers considered problem coalition structure generation combinatorial structuresi.e., graphs. Thus, Aziz de Keijzer (2011) showed
polynomial time bounds coalition structure generation contexts spanning tree
games, edge path coalitional games vertex path coalitional games, value
coalition nodes either 1 0, depending whether contains spanning
tree, edge path vertex path, respectively. authors also prove NP-hardness
GCSG problem general graphs edge sum valuation function. paper, present stronger result showing hardness problem planar graphs.
Independently, Bachrach, Kohli, Kolmogorov Zadimoghaddam (2011) showed
coalition structure generation problem intractable planar graphs edge sum
valuation function, also provided algorithms constant factor approximations
planar, minorfree bounded degree graphs. However, aforementioned papers,
like classic literature clustering, problem considered particular context
(i.e., associated specific valuation function). contrast, results presented
apply general class valuation functions, characterised single assumption
independence disconnected members.

7. Conclusions
key organisational form multi-agent systems involves members coalition
coordinating actions achieve common goals. agents organised effectively,
cooperation significantly improve performance individual system
whole, especially cases single agents insufficient skills resources complete given tasks own. reason, generating good coalitional structures
one fundamental problems studied AI.
However, many real-life scenarios, certain subsets agents able cooperate
apply joint actions. Indeed, act collectively, group agents 1) find (most)
beneficial plan action, 2) agree it, 3) coordinate actions among members
group. Now, may achievable arbitrary subset agents
connected related other. Therefore, study coalition formation
taking consideration social (or, communication) structure set participants,
besides natural interesting research direction, may provide key many
positive results terms problem tractability, well quality stability
solutions. Moreover, approach obviously much appealing practical
perspective considering agents interacting vacuum.
end, paper studies problem coalition structure generation graphs
(GCSG) provides foundation analysis computational complexity.
work stands existing literature graph partitioning (or, clustering)
focus specific coalition valuation function, rather looks general
189

fiVoice, Polukarov, & Jennings

class functions characterised single assumption independence disconnected
members (IDM).
results show certain important cases indeed valuable identify
valuation function satisfies IDM property, significantly reduces complexity
GCSG problem one faces. particular,
Algorithm 1 uses simple search procedure

guaranteed bound n2 e+n
computational
steps general graphs n
n
nodes e edges. Hence, whenever graph sparse bound gets lower
3n number steps required solve coalition structure generation problem
unstructured set elementsutilising graph structure beneficial.
graph n nodes known tree decomposition width w, Algorithm 2 requires
O(ww+O(1) n) computational steps, implying problem solved linear time
bounded treewidth graphs! addition, coupling Algorithm 2 existing separator
theorems minorfree planar graphs, provides improved computational bounds
coalition structure generation important graph classes, although, show
Theorem 2, problem remains NPcomplete even planar graphs simple edge sum
valuation functions.
work suggests several directions future research topic. First, although
theoretical bounds give complexity problem minorfree planar graphs
close best possible, tight. Closing gap would complete results.
Second, perhaps main direction study, exploring approximability
GCSG problem interesting graph classes, developing approximation
schemes applicable. line, partial results provided Bachrach, Kohli,
Kolmogorov Zadimoghaddam (2011) give algorithms constant factor approximations planar, minorfree bounded degree graphs endowed edge sum
valuation function. challenging task see results extend
general class IDM functions. Finally, would interesting incorporate ideas
compact representation (Ohta et al., 2009) constrained coalition formation (Rahwan
et al., 2011) graph coalition structure generation.

Appendix
Lemma 1 Given graph G = (N, E) coalition valuation function v() IDM
property, A, B N edges G \ B B \ A,
v(A) v(A B) = v(A B) v(B).

(3)

Proof : B \ = B = B B = A, result holds. Now, let us
show holds kB \ Ak = 1. Suppose otherwise, let B
kA \ Bk minimal B kB \ Ak = 1 (3) violated. cannot
\ B = , otherwise B = B = B, would imply (3) holds. Let
x element \ B. Then, IDM property,
v(A) v(A \ {x}) = v(A B) v((A B) \ {x}),
190

(4)

fiCoalition Structure Generation Graphs

but, choice B, set \ {x} must satisfy (3), since x B,

v(A \ {x}) v(A B) = v((A B) \ {x}) v(B).
(5)
Adding (4) (5) gives us v(A) v(A B) = v(A B) v(B), contradiction.
show result holds general. Suppose otherwise, let
B kB \ Ak minimal B (3) violated. Let x
element B \ let A0 = (B \ {x}). Now, A0 B = B \ {x} A0 B = B.
Furthermore, B \ A0 = {x}, applying results proven far pair A0 , B,
get
v(A0 ) v(A0 B) = v(A0 B) v(B),
meaning
v(A (B \ {x})) v(B \ {x}) = v(A B) v(B).
Furthermore, choice B, since x A,
v(A) v(A B) = v(A (B \ {x})) v(B \ {x}).
two relations prove result holds B, contradiction.
completes proof.
2
Lemma 2 Let G = (N, E) graph tree decomposition (X, ), X =
{X1 , . . . , Xm } n = |N | tree X. Suppose Xi
numbered order shortest distance X1 , X1 may chosen arbitrarily.
Then, C N ,
v(C) =


X

v(C Xi ) v C Xi

i=1

[


Xj .

(6)

j<i

Proof : Towards contradiction, let us suppose result hold G.
Let (X, ) tree decomposition minimal = kXk (6) violated.
= 1, X = {N } equation (6) becomes
v(C) = v(C N ) v(C N ),
trivially true. must > 1. choice numbering, Xm must
leaf node . Let k Xk node Xm connected . Since
Xm \ Xk disjoint Xi 6= m, edges G elements
Xm \ Xk
Xk \ Xm . Furthermore, < 6= k, Xm Xi Xm Xk ,
Xm j<m Xj = Xm Xk
Xk

[

Xj = (Xm Xk )

j<k

[
j<k

191

Xj .

fiVoice, Polukarov, & Jennings

Thus, C N ,
v(C Xm ) v(C Xm

[

Xj ) + v(C Xk ) v(C Xk

j<m

[

Xj )

j<k

= v(C Xm ) v(C Xm Xk ) + v(C Xk ) v(C (Xm Xk )

[

Xj )

j<k

= v(C (Xm Xk )) v(C (Xm Xk )

[

Xj ),

j<k

Lemma 1. Furthermore, < k,
[
[
Xi
Xj = Xi Xm
Xj ,
j<i

j<i

so,

X

v(C Xi ) v C Xi

i=1

=

m1
X

[

Xj



j<i

v(C Yi ) v C Yi

i=1

[


Yj ,

j<i

Yi = Xi 6= k Yk = Xk Xm . However, Yi form tree decomposition
G 1 nodes, (with tree topology Xm leaf removed),
thus sum must equal v(C) choice m. Since C chosen arbitrarily,
leads us contradiction, result must hold general.
2
Lemma 3 graph G = (N, E), P, Q N , P coalition structure
P Q coalition structure Q, P(P Q) = Q(P Q), E = U (P, Q)
coalition structure P Q P 0 P , Q0 Q, E(P ) = P(P 0 )
E(Q) = Q(Q0 ).
Proof : Firstly, P, either (P \ Q) B Q
B 6= . Thus, union sets E covers P . symmetry, union
sets E must also cover Q.
Now, P 0 P ,
E(P 0 ) = {(A P 0 ) : P, (P \ Q)} {(A B) P 0 : P, B Q, B 6= }.
However, P, B Q B 6= , since P(P Q) = Q(P Q) coalition
structure P Q, must (P Q) = B (P Q). B Q, B P 0
equal (B (P Q)) P 0 P 0 . Thus,
E(P 0 ) = {(A P 0 ) : P} = P(P 0 ).
symmetry, Q0 Q, E(Q0 ) = Q(Q0 ).
192

fiCoalition Structure Generation Graphs

remains show E coalition structure. Towards contradiction, suppose
A, B E B 6= 6= B. Then, since E(P ) = P, P
coalition structure, must either P = B P P B P disjoint.
Likewise, either Q = B Q B Q = . Now, P = B P Q = B Q,
= B B P = B Q = , B = , contradictions.
Suppose P = B P B Q = . implies P = B P non-empty,
B 6= , also implies P Q = B P Q = , means, P
element P subset P \ Q. However, element E would
P subset would P itself, meaning = B = P , another contradiction.
symmetry, Q = B Q B P = also leads contradiction,
therefore scenario impossible. Thus, shown E coalition structure,
required.
2

References
Alon, N., Seymour, P., & Thomas, R. (1990). separator theorem graphs
excluded minor applications. Proceedings 22nd ACM Symposium
Theory Computing, pp. 293299.
Aziz, H., & de Keijzer, B. (2011). Complexity coalition structure generation.
10th International Joint Conference Autonomous Agents Multi-Agent Systems
(AAMAS), pp. 191198.
Bachrach, Y., Kohli, P., Kolmogorov, V., & Zadimoghaddam, M. (2011). Optimal coalition
structures graph games. http://arxiv.org/abs/1108.5248.
Bachrach, Y., Meir, R., Jung, K., & Kohli, P. (2010). Coalitional structure generation
skill games. 24th AAAI Conference Artificial Intelligence (AAAI), pp.
703708.
Balas, E., & Padberg, M. W. (1976). Set partitioning: survey. SIAM Rev., 18 (4), 710760.
Bansal, N., Blum, A., & Chawla, S. (2003). Correlation clustering. Machine Learning
Journal, 56 (1-3), 89113.
Bern, M. W., Lawlerand, E. L., & Wong, A. L. (1987). Linear-time computation optimal
subgraphs decomposable graphs. Journal Algorithms, 8 (2), 216235.
Bodlaender, H. L. (1998). partial k-arboretum graphs bounded treewidth. Theoretical Computer Science, 209 (1-2), 145.
Brandes, U., Delling, D., Gaertler, M., Gorke, R., Hoefer, M., Nikoloski, Z., & Wagner,
D. (2008). modularity clustering. IEEE Transactions Knowledge Data
Engineering, 20 (2), 172188.
Conitzer, V., & Sandholm, T. (2006). Complexity constructing solutions core
based synergies among coalitions. Artificial Intelligence, 170 (6), 607619.
Dang, V. D., Dash, R. K., Rogers, A., & Jennings, N. R. (2006). Overlapping coalition
formation efficient data fusion multi-sensor networks. AAAI-06, pp. 635
640.
193

fiVoice, Polukarov, & Jennings

Dang, V. D., & Jennings, N. R. (2004). Generating coalition structures finite bound
optimal guarantees. 3rd International Joint Conference Autonomous
Agents Multi-Agent Systems (AAMAS), pp. 564571.
Demange, G. (2004). group stability heirarchies networks. Journal Political
Economy, 112 (4), 754778.
Deng, X., & Papadimitriou., C. (1994). complexity cooperative solution concepts.
Mathematics Operations Research, 19 (2), 257266.
Feo, T. A., & Resende, M. G. (1995). Greedy randomized adaptive search precedures.
Journal Global Optimization, 6, 109133.
Hopcroft, J., & Tarjan, R. (1973). Efficient algorithms graph manipulation. Communications ACM, 16 (6), 372378.
Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: compact representation
scheme coalitional games. Proceedings 6th ACM Conference Electronic
Commerce (ACM EC), pp. 193202.
Keinanen, H. (2009). Simulated annealing multi-agent coalition formation. 3rd
KES International Symposium Agent Multi-Agent Systems (KES-AMSTA),
pp. 3039.
Klusch, M., & Shehory, O. (1996). polynomial kernel-oriented coalition formation algorithm rational information agents. 2nd International Conference MultiAgent Systems (ICMAS), pp. 157164.
Kuratowski, K. (1930). Sur le probleme des courbes gauches en topologie. Fundamenta
Mathematicae, 15, 271283.
Lipton, R. J., & Tarjan, R. E. (1979). separator theorem planar graphs. Journal
Applied Mathematics, 36 (2), 177189.
Mauro, N. D., Basile, T. M. A., Ferilli, S., & Esposito, F. (2010). Coalition structure
generation grasp. 14th International Conference Artificial Intelligence:
Methodology, Systems, Applications (AIMSA), pp. 111120.
Modi, P. J. (2003). Distributed constraint optimization multiagent systems. Ph.D. thesis,
University Southern California, Los Angeles, CA, USA.
Norman, T. J., Preece, A. D., Chalmers, S., Jennings, N. R., Luck, M., Dang, V. D., Nguyen,
T. D., Deora, J. S. V., Gray, W. A., & Fiddian, N. J. (2004). Agent-based formation
virtual organisations. International Journal Knowledge Based Systems, 17 (2-4),
103111.
Ohta, N., Conitzer, V., Ichimura, R., Sakurai, Y., Iwasaki, A., & Yokoo, M. (2009). Coalition structure generation utilizing compact characteristic funciton representations.
Proceedings 15th International Joint Conference Principles Practice
Constraint Programming, pp. 623638.
Ohta, N., Iwasaki, A., Yokoo, M., Maruono, K., Conitzer, V., & Sandholm, T. (2006). compact representation scheme coalitional games open anonymous environments.
21st National Conference Artificial Intelligence (AAAI), pp. 697702.
194

fiCoalition Structure Generation Graphs

Petcu, A., & Faltings, B. (2005). scalable method multiagent constraint optimization.
19th International Joint Conference Artificial Intelligence (IJCAI), pp. 266
271.
Rahwan, T., & Jennings, N. R. (2007). algorithm distributing coalitional values
calculations among cooperative agents. Artificial Intelligence, 171 (8-9), 535567.
Rahwan, T., & Jennings, N. R. (2008a). Coalition structure generation: Dynamic programming meets anytime optimisation. 23rd AAAI Conference Artificial
Intelligence (AAAI), pp. 156161.
Rahwan, T., & Jennings, N. R. (2008b). improved dynamic programming algorithm
coalition structure generation. Proceedings 7th International Conference
Autonomous Agents Multi-Agent Systems, pp. 14171420.
Rahwan, T., Michalak, T. P., Elkind, E., Faliszewski, P., Sroka, J., Wooldridge, M., &
Jennings, N. R. (2011). Constrained coalition formation. 25th AAAI Conference
Artificial Intelligence (AAAI), pp. 719725.
Rahwan, T., Ramchurn, S. D., Giovannucci, A., & Jennings, N. R. (2009). anytime
algorithm optimal coalition structure generation. Journal Artificial Intelligence
Research (JAIR), 34, 521567.
Robertson, N., & Seymour, P. (1983). Graph minors. i. excluding forest. Journal
Combinatorial Theory, Series B 35 (1), 3961.
Robertson, N., & Seymour, P. (1995). Graph minors. xiii. disjoint paths problem.
Journal Combinatorial Theory, Series B 63 (1), 65110.
Robertson, N., & Seymour, P. (2004). Graph minors. xx. wagners conjecture. Journal
Combinatorial Theory, Series B 92 (2), 325357.
Rothkopf, M. H., Pekec, A., & Harstad, R. M. (1998). Computationally manageable combinatorial auctions. Management Science, 44 (8), 11311147.
Sandholm, T., Larson, K., Andersson, M., Shehory, O., & Tohme, F. (1999). Coalition
structure generation worst case guarantees. Artificial Intelligence, 111 (1-2),
209238.
Sandholm, T., & Lesser, V. R. (1997). Coalitions among computationally bounded agents.
Artificial Intelligence, 94 (1-2), 99137.
Sen, S., & Dutta, P. (2000). Searching optimal coalition structures. 6th International Conference Multi-Agent Systems (ICMAS), pp. 286292.
Shehory, O., & Kraus, S. (1998). Methods task allocation via agent coalition formation.
Artificial Intelligence, 101 (1-2), 165200.
Stanica, P. (2001). Good lower upper bounds binomial coefficients. Journal
Inequalities Pure Applied Mathematics, 2 (3), Art. 30.
Tsvetovat, M., Sycara, K. P., Chen, Y., & Ying, J. (2001). Customer coalitions
electronic market place. AA-01, pp. 263264.
Ueda, S., Iwasaki, A., Yokoo, M., Silaghi, M. C., & Matsui, T. (2010). Coalition structure
generation based distributed constraint optimization. 24th AAAI Conference
Artificial Intelligence (AAAI), pp. 197203.
195

fiVoice, Polukarov, & Jennings

Ueda, S., Kitaki, M., Iwasaki, A., & Yokoo, M. (2011). Concise characteristic function
representations coalitional games based agent types. 22nd International
Joint Conference Artificial Intelligence (IJCAI), pp. 393399.
Wagner, K. (1937). Uber eine eigenschaft der ebenen komplexe. Mathematische Annalen,
114 (1), 570590.
Xin, X. (2011). FPT algorithm correlation clustering problem. Key Engineering
Materials, Advanced Materials Computer Science(474-476), 924927.
Yeh, D. Y. (1986). dynamic programming approach complete set partitioning
problem. BIT Numerical Mathematics, 26 (4), 467474.
Yong, G., Li, Y., Weiming, Z., Jichang, S., & Changying, W. (2003). Methods resource
allocation via agent coalition formation grid computing systems. Proceedings IEEE International Conference Robotics, Intelligent Systems Signal
Processing, Vol. 1, pp. 295300.

196

fiJournal Artificial Intelligence Research 45 (2012) 443-480

Submitted 3/12; published 11/12

New Look BDDs Pseudo-Boolean Constraints
Ignasi Abo
Robert Nieuwenhuis
Albert Oliveras
Enric Rodrguez-Carbonell

iabio@lsi.upc.edu
roberto@lsi.upc.edu
oliveras@lsi.upc.edu
erodri@lsi.upc.edu

Technical University Catalonia (UPC), Barcelona.

Valentin Mayer-Eichberger

mayereichberger@gmail.com

Abstract
Pseudo-Boolean constraints omnipresent practical applications, thus significant effort devoted development good SAT encoding techniques
them. encodings first construct Binary Decision Diagram (BDD)
constraint, encode BDD propositional formula. BDD-based approaches important advantages, dependent size
coefficients, able share BDD representing many constraints.
first focus size resulting BDDs, considered open problem
research community. report previous work proved
Pseudo-Boolean constraints polynomial BDD exists. also give alternative simpler proof assuming NP different Co-NP. interestingly,
also show overcome possible exponential blowup BDDs coefficient decomposition. allows us give first polynomial generalized arc-consistent
ROBDD-based encoding Pseudo-Boolean constraints. Finally, focus practical
issues: show efficiently construct ROBDDs, encode
SAT 2 clauses per node, present experimental results confirm
approach competitive encodings state-of-the-art Pseudo-Boolean solvers.

1. Introduction
paperwe study Pseudo-Boolean constraints (PB constraints short), is, constraints form a1 x1 + + xn # K, ai K integer coefficients,
xi Boolean (0/1) variables, relation operator # belongs {<, >, , , =}.
assume # ai K positive since cases easily
reduced one (see Een & Sorensson, 2006).
constraint ( positive coefficients) Boolean function C : {0, 1}n {0, 1}
monotonic decreasing sense solution C remains solution
flipping inputs 1 0. Therefore constraints expressed set
clauses negative literals. example, clause could simply define (minimal)
subset variables cannot simultaneously true. Note however every
monotonic function PB constraint. example, function expressed two
clauses x1 x2 x3 x4 (single) equivalent PB constraint a1 x1 + + a4 x4 K
(since without loss generality a1 a2 a3 a4 , also x1 x3 needed).
Hence, even among monotonic Boolean functions, PB constraints rather restricted
class (see also Smaus, 2007).
c
2012
AI Access Foundation. rights reserved.

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

PB constraints omnipresent practical SAT applications, typical 0-1
linear integer problems, also ingredient new SAT approaches to, e.g., cumulative scheduling (Schutt, Feydy, Stuckey, & Wallace, 2009), logic synthesis (Aloul, Ramani,
Markov, & Sakallah, 2002) verification (Bryant, Lahiri, & Seshia, 2002),
surprising significant number SAT encodings constraints proposed literature. interested encoding PB constraint C clause
set (possibly auxiliary variables) equisatisfiable, also generalized
arc-consistent (GAC): given partial assignment A, xi false every extension
satisfying C, unit propagating sets xi false.
knowledge, polynomial GAC encoding far given Bailleux,
Boufkhad, Roussel (2009). existing encodings based building (forms
of) Binary Decision Diagrams (BDDs) translating CNF. Although approach Bailleux et al. BDD-based, main motivation revisit BDD-based
encodings following:
Example 1. Let us consider two Pseudo-Boolean constraints: 3x1 + 2x2 + 4x3 5
30001x1 + 19999x2 + 39998x3 50007. clearly equivalent: Boolean function
represent expressed, e.g., clauses x1 x3 x2 x3 . However, encodings
like one Bailleux et al. (2009) heavily depend concrete coefficients
constraint, generate significantly larger SAT encoding second one. Since,
given variable ordering, ROBDDs canonical representation Boolean functions
(Bryant, 1986), i.e., Boolean function unique ROBDD, ROBDD-based encoding
treat constraints equivalently.
Another reason revisiting BDDs practical problems numerous PB constraints exist share variables among other. Representing single
ROBDD potential generating much compact SAT encoding moreover likely better propagation properties.
mentioned, BDD-based approaches already studied literature. good example work Een Sorensson (2006), GAC encoding
using six three-literals clauses per BDD node given. However, comes study
BDD size, page 9 cite work Bailleux, Boufkhad, Roussel (2006)
say proven general PB-constraint generate exponentially sized
BDD. Section 7 explain approach Bailleux et al use ROBDDs,
prove example use show exponentiality method turns
polynomial ROBDDs. Somewhat surprisingly, probably due different names
PB constraints receive (0-1 integer linear constraints, linear threshold functions, weight
constraints, knapsack constraints), work Hosaka, Takenaga, Yajima (1994)
remained unknown research community. paper, proved
PB constraints polynomial-sized ROBDDs exist. self-containedness
article, bring interesting result knowledge research community,
include family PB constraints prove that, regardless variable ordering,
corresponding ROBDD always exponential size.

444

fiA New Look BDDs Pseudo-Boolean Constraints

Main contributions organization paper:
Subsection 3.2: reproduce family PB constraints proposed Hosaka et al.
(1994), polynomial-size ROBDD exist. self-containedness, give
clearer alternative proof original paper.
Subsection 3.3: simple proof that, unless NP=co-NP, PB constraints
admit polynomial-size ROBDD, independently variable order.
Subsection 4.1: proof PB constraints whose coefficients powers two
admit polynomial-size ROBDDs.
Subsections 4.2 4.3: GAC polynomial (size O(n3 log amax )) ROBDD-based
encoding PB constraints.
Section 5: algorithm construct ROBDDs Pseudo-Boolean constraints
polynomial time w.r.t. size final ROBDD.
Section 6: GAC SAT encoding BDDs monotonic functions, general
class Boolean functions PB constraints. encoding uses one binary
one ternary clause per node (the standard if-then-else encoding BDDs used
in, e.g., Een & Sorensson, 2006, requires six ternary clauses per node). Moreover,
translation works BDD variable ordering.
Section 7: related work section, summarizing important ingredients
existing encodings Pseudo-Boolean constraints SAT.
Section 8: experimental evaluation comparing approach encodings
tools.
article extends shorter preliminary paper BDDs Pseudo-Boolean Constraints Revisited (Abo, Nieuwenhuis, Oliveras, & Rodrguez-Carbonell, 2011),
presented SAT 2011 conference. Extensions include: (i) proofs technical
results, (ii) multiple examples illustrating various concepts algorithms presented,
(iii) PB constraint family Hosaka et al. (1994) polynomial ROBDD exists, (iv) algorithm efficiently construct ROBDDs Pseudo-Boolean constraints, (v)
detailed related work section, (vi) extensive experimental results comparing encoding
approaches (vii) brief report experience trying take advantage
sharing potential BDDs.

2. Preliminaries
Let X = {x1 , x2 , . . .} fixed set propositional variables. x X x x
positive negative literals, respectively. negation literal l, written l, denotes x
l x, x l x. clause disjunction literals x1 . . . xp xp+1 . . . xn ,
sometimes written x1 . . . xp xp+1 . . . xn . CNF formula conjunction clauses.
(partial) assignment set literals {x, x} x, i.e.,
contradictory literals appear. literal l true l A, false l A,
undefined otherwise. Sometimes write set pairs x = v, v
445

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

x1

x1
1

0

0

x2
0

x3
0

x2
1

x3

1
0

1

1 0

x3
1

x2
1

0

0

0

x2
0

1

x3
1

0

x1

x3
0

1

1 0

x3

x2
1

0

1

1

0

0

x2
1

0

x3

x2

1
1

0

x3
0

1

0

x1
1

0

1

1

x3

1

0

0

1

1

0

Figure 1: Construction BDD 2x1 + 3x2 + 5x3 6

1 x true 0 x false A. clause C true least one
literals true A. formula F true clauses true A.
case, model F . Systems decide whether formula F model called
SAT-solvers, main inference rule implement unit propagation: given CNF
F assignment A, find clause F literals false except
one, say l, undefined, add l repeat process reaching fixpoint.
Pseudo-Boolean constraints (PB constraints short) constraints form a1 x1 +
+ xn # K, ai K integer coefficients, xi Boolean (0/1)
variables, relation operator # belongs {<, >, , , =}. assume #
ai K positive, since cases easily reduced one 1 :
(i) changing straightforward coefficients negative; (ii) replacing ax
a(1 x) a; (iii) replacing (1 x) x. Negated variables like x handled positive
ones or, alternatively, replaced fresh x0 adding clauses x x0 x x0 .
particular case Pseudo-Boolean constraints one cardinality constraints,
coefficients ai equal 1.
main goal find CNF encodings PB constraints. is, given PBconstraint C, construct equisatisfiable clause set (a CNF) model
restricted variables C model C viceversa. Two extra properties
sought: (i) consistency checking unit propagation simply consistency: whenever
partial assignment cannot extended model C, unit propagation
produces contradiction (a literal l negation l); (ii) generalized arc-consistency
GAC (again unit propagation): given assignment extended
model C, {x} cannot, unit propagation produces x.
concretely, use ROBDDs finding encodings. ROBDDs introduced
means following example.
Example 2. Figure 1 explains (one method for) construction ROBDD PB
constraint 2x1 + 3x2 + 5x3 6 ordering [x1 , x2 , x3 ]. root node selector
variable x1 . false child represents PB constraint assuming x1 = 0 (i.e., 3x2 +5x3 6)
true child represents 2+3x2 +5x3 6, is, 3x2 +5x3 4. two children
next variable ordering (x2 ) selector, process repeated reach
1. =-constraint split -constraint -constraint. consider (generalized
arc-)consistency latter two isolatedly, original =-constraint.

446

fiA New Look BDDs Pseudo-Boolean Constraints

last variable sequence. Then, constraint form 0 K True node
(1 figure) K 0 positive, False node (0) K < 0. construction
(leftmost figure), known Ordered BDD. obtaining Reduced Ordered
BDD (ROBDD short rest paper), two reductions applied fixpoint:
removing nodes identical children (as done leftmost x3 node second BDD
figure), merging isomorphic subtrees, done x3 third BDD. fourth
final BDD fixpoint. given ordering, ROBDDs canonical representation
Boolean functions: Boolean function unique ROBDD. BDDs encoded
CNF introducing auxiliary variable every node. selector variable
node x auxiliary variables false true child f t, respectively,
add if-then-else clauses:
x f
x f




x
x

f
f




follows, size BDD number nodes. say BDD represents PB constraint represent Boolean function. Given assignment
variables BDD, define path induced path starts
root BDD step, moves false (true) child node
selector variable false (true) A.

3. Exponential ROBDDs PB Constraints
section study size ROBDDs PB constraints. start defining
notion interval PB constraint. Then, Section 3.2 consider two families PB
constraints study ROBDD size: first prove example given Bailleux
et al. (2006) polynomial ROBDDs, reproduce example Hosaka et al.
(1994) exponential ROBDDs regardless variable ordering. Finally, relate
ROBDD size PB constraint well-known subset sum problem.
3.1 Intervals
formally defining notion interval PB constraint, let us first give
intuitive explanation.
Example 3. Consider constraint 2x1 + 3x2 + 5x3 6. Since combination
coefficients adds 6, constraint equivalent 2x1 + 3x2 + 5x3 < 6, hence
2x1 + 3x2 + 5x3 5. process cannot repeated since 5 obtained
existing coefficients.
Similarly, could try increase right-hand side constraint. However,
combination coefficients adds 7, implies constraint
equivalent 2x1 + 3x2 + 5x3 7. all, state constraint equivalent
2x1 + 3x2 + 5x3 K K [5, 6]. trivial see set valid Ks
always interval.
Definition 4. Let C constraint form a1 x1 + + xn K. interval C
consists integers a1 x1 + + xn , seen Boolean function,
equivalent C.
447

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

Similarly, given ROBDD representing PB constraint node selector
variable xi ,we refer interval integers constraint
ai xi + xn represented (as Boolean function) ROBDD rooted .
following, unless stated otherwise, ordering used ROBDD
[x1 , x2 , . . . , xn ].
Proposition 5. [, ] interval node selector variable xi then:
1. assignment {xj = vj }nj=i ai vi + + vn = .
2. assignment {xj = vj }nj=i ai vi + + vn = + 1.
3. assignment {xj = vj }i1
j=1 K a1 v1 a2 v2 ai1 vi1 [, ]
4. Take h < . exists assignment {xj = vj }nj=i ai vi + + vn > h
path goes True.
5. Take h > . exists assignment {xj = vj }nj=i ai vi + + vn h
path goes False.
6. interval True node [0, ).
7. interval False node (, 1]. Moreover, interval
negative values.
Proof.

1. Since 1 belong interval , constraints
ai xi + ai+1 xi+1 + + xn 1
ai xi + ai+1 xi+1 + + xn

different. means partial assignment satisfying second one
first one.
2. proof analogous previous one.
3. Take partial assignment {x1 = v1 , . . . , xi1 = vi1 } whose path goes root
. Therefore, definition ROBDD, ROBDD constraint
ai xi + ai+1 xi+1 + + xn K a1 v1 ai1 vi1 .
Therefore, definition interval ,
K a1 v1 a2 v2 ai1 vi1 [, ].
4. Intuitively, property states that, h interval ,
assignment satisfies ROBDD rooted constraint ai xi + +
xn h.
Since h belong interval , ROBDD
C 0 : ai xi + + xn h
. Therefore, exists assignment either
448

fiA New Look BDDs Pseudo-Boolean Constraints

[5, 6]

[5, 6]

x1

x1

0

[5, 7]

0
1

x2
0

[0, )

1

0

0

x3 [0, 4]
0

x2

[5, 7]

1

[5,) x
3

1

(, 1]

1

[0, )

x2

1

[3, 4]

1

0

x3 [0, 4]
1

0

0

1

1

0

(, 1]

Figure 2: Intervals ROBDD 2x1 + 3x2 + 5x3 6
(i) goes False satisfies C 0 ;
(ii) goes True satisfy C 0 .
want prove assignment satisfies (ii). Assume satisfies (i). Since
goes False belongs interval , holds
ai vi + + vn > .
Since > h, assignment satisfy C 0 , contradiction. Therefore,
assignment satisfies (ii).
5. Take assignment second point proposition. Since + 1
belong interval, path assignment goes False. Moreover,
ai vi + + vn = + 1 h.
6. True node ROBDD tautology. Therefore, represents PB
constraint 0 h h [0, ).
7. False node ROBDD contradiction. Therefore, represents PB
constraint 0 h h (, 1]. Moreover, ai xi + + xn < 0 also
contradiction, hence constraint also represented False node. Therefore,
node interval negative values.
prove that, given ROBDD PB constraint, one easily compute
intervals every node bottom-up. first start motivating example.
Example 6. Let us consider constraint 2x1 + 3x2 + 5x3 6. Assume
variables appear every path root leaves (otherwise, add extra nodes
rightmost BDD Figure 2). Assume computed intervals two
children root (rightmost BDD Figure 2). means false child root
BDD 3x2 +5x3 [5, 7] true child BDD 3x2 +5x3 [3, 4]. Assuming
x1 false, false child would also represent constraint 2x1 +3x2 +5x3 [5, 7],
assuming x1 true, true child would represent constraint 2x1 +3x2 +5x3 [5, 6].
Taking intersection two intervals, infer root node represents
2x1 + 3x2 + 5x3 [5, 6].
449

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

formally, interval every node computed follows:
Proposition 7. Let a1 x1 + a2 x2 + + xn K constraint, let B ROBDD
order [x1 , . . . , xn ]. Consider node selector variable xi , false child f (with
selector variable xf interval [f , f ]) true child (with selector variable xt
interval [t , ]), shown Figure 3. interval [, ], with:
= max{f + ai+1 + + af 1 , + ai + ai+1 + + at1 },
= min{f , + ai }.
..

.
xi [, ]
0

xf
[f , f ]

1

xt
[t , ]

Figure 3: interval node computed childrens intervals.
moving proof, want note every path root
leaves ROBDD variables present, definition would much simpler
( = max{f , + ai }). coefficients necessary account variables
removed due ROBDD reduction process.
Proof. Let us assume [, ] interval . One following statements
hold:
1. exists h [, ] belong interval .
2. exists h < belonging interval .
3. exists h > belonging interval .
prove none cases hold.
1. Let us define
C 0 : ai xi + + xn h.
h belong interval, exists assignment {xj = vj }nj=i
either satisfies C 0 path goes False satisfy C 0
path goes True. Assume assignment satisfies C 0 path goes
False (the case similar). two possibilities:
assignment satisfies vi = 0. Since h , holds
h ai+1 vi+1 af 1 vf 1 ai+1 vi+1 af 1 vf 1
ai+1 af 1 f .
450

fiA New Look BDDs Pseudo-Boolean Constraints

hand, since h ,
h ai+1 vi+1 af 1 vf 1 h f .
Therefore, h ai+1 vi+1 af 1 vf 1 belongs interval f . Since
assignment {xf = vf , . . . , xn = vn } goes f False, have:
af vf + + vn > h ai+1 vi+1 af 1 vf 1
ai+1 vi+1 + + af vf + vn > h
Hence, adding ai vi sum one see assignment satisfy
C 0 , contradiction.
case vi = 1 gives similar contradiction.
2. definition , either h < f + ai+1 + + af 1 h < + ai + ai+1 + +
at1 . consider first case, since one similar. Therefore,
h ai+1 af 1 < f . Due point 4 Proposition 5, exists assignment
{xf = vf , . . . xn = vn }
af vf + vn > h ai+1 af 1
path goes f True. Hence, assignment
{xi = 0, xi+1 = 1, . . . , xf 1 = 1, xf = vf , . . . , xn = vn }
satisfy constraint ai xi + + xn h path goes True.
definition interval, h cannot belong interval .
3. case similar previous one.

proposition gives natural way computing intervals ROBDD bottomup fashion. procedure initialized computing intervals terminal nodes
detailed Proposition 5, points 6 7.
Example 8. Let us consider constraint 2x1 + 3x2 + 5x3 6. ROBDD
shown left-hand side Figure 2, together intervals. computation,
first compute intervals True False nodes, [0, ) (, 1]
virtue Proposition 5. Then, compute interval node x3
selector variable previous propositions formula: 3 = max{0, + 5} = 0, 3 =
min{, 1 + 5} = 4. Therefore, interval [0, 4].
next step, compute interval node selector variable x2 : 2 =
max{0 + 5, 0 + 3} = 5, 2 = min{, 4 + 3} = 7. Thus, interval [5, 7]. Finally,
compute roots interval: 1 = max{5, 0 + 2 + 3} = 5, 1 = min{7, 4 + 2} = 6,
is, [5, 6].
451

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

3.2 Families PB Constraints ROBDD Size
start revisiting family PB constraints given Bailleux et al. (2006),
proved that, concrete variable ordering, non-reduced BDDs grow exponentially family. prove ROBDDs polynomial family, even independent variable ordering. family defined
P
considering a, b n positive integers ni=1 bi < a. coefficients
= + bi right-hand side constraint K = n/2. first prove
constraint C : 1 x1 + + n xn K equivalent cardinality constraint
C 0 : x1 + + xn n/2 1. simplicity, assume n even.
Take assignment satisfying C 0 . case, n/2 1 variables xi
assigned true, assignment also satisfies C since:
1 x1 + + n xn

n
X
i=n/2+2

= (n/2 1)a +

n
X
i=n/2+2

bi < K +

n
X

bi < K.

i=1

Consider assignment satisfying C 0 . case, least n/2
true variables assignment satisfy C either:
1 x1 + + n xn

n/2
X
i=1

= (n/2) +

n/2
X
i=1

bi > (n/2) = K.

Since two constraints equivalent ROBDDs canonical, ROBDD representation C C 0 same. ROBDD C 0 known quadratic
size cardinality constraint (see, instance, Bailleux et al., 2006).
following, present family PB constraints admit exponential
ROBDDs. example first given Hosaka et al. (1994), clearer alternative
proof given next. First all, prove lemma that, certain technical conditions,
gives lower bound number nodes ROBDD PB constraint.
Lemma 9. Let a1 x1 + + xn K PB constraint, let integer
1 n. Assume every assignment {x1 = v1 , x2 = v2 , . . . , xi = vi } admits
extension {x1 = v1 , . . . , xn = vn } a1 v1 + + vn = K. Let number
different results obtain adding subset coefficients a1 , a2 , . . . , ai , i.e.,
= |{


X

j=1

aj bj : bj {0, 1}}|. Then, ROBDD size ordering [x1 , x2 , . . . , xn ]

least .
Proof. Let us consider PB constraint satisfies conditions lemma.
prove ROBDD least distinct nodes showing two assignments
form {x1 = v1 , . . . , xi = vi } {x1 = v10 , . . . , xi = vi0 } a1 v1 + + ai vi 6=
a1 v10 + + ai vi0 lead different nodes ROBDD.
Assume true: two assignments {x1 = v1 , . . . , xi = vi }
{x1 = v10 , . . . , xi = vi0 } a1 v1 + + ai vi < a1 v10 + + ai vi0 paths end
node. Take extended assignment = {x1 = v1 , . . . , xn = vn }
a1 v1 + vn = K. Since satisfies PB constraint, path defines ends
452

fiA New Look BDDs Pseudo-Boolean Constraints

true node. However, assignment A0 = {x1 = v10 , . . . , xi = vi0 , xi+1 = vi+1 , . . . , xn = vn }
satisfy constraint, since
a1 v10 + ai vi0 + ai+1 vi+1 + vn > a1 v1 + + vn = K.
However, nodes defined {x1 = v1 , . . . , xi = vi } {x1 = v10 , . . . , xi = vi0 }
same, path defined A0 must also end true node, contradiction.
show family PB constraints admits exponential ROBDDs.
Theorem 10. Let n positive integer, let us define ai,j = 2j1 + 22n+i1
1 i, j 2n; K = (24n 1)n. Then, PB constraint
2n X
2n
X
i=1 j=1

ai,j xi,j K

least 2n nodes variable ordering.
Proof. convenient describe coefficients binary notation:
2n

z

}|

2n

{

0 1
0 1

z

0 0
0 0

}|

0
1
.
..

1
0

{

a1,1 = 0
a1,2 = 0

0
0


a1,2n1 = 0
a1,2n = 0

0
0

0 1
0 1

0 1
1 0

0
0

0
0

a2,1 = 0
a2,2 = 0

0
0

1 0
1 0

0 0
0 0

1
0


a2,2n1 = 0
a2,2n = 0

0
1
.
..

0
0

1 0
1 0
.
..

0 1
1 0

0
0

0
0

a2n,2n = 1 0

0 0

1 0

0

0

K/n = 1 1

1 1

1 1

1

1



First all, one see sum 2K.
Let us take arbitrary bijection

F = (F1 , F2 ) : {1, 2, . . . , 4n2 } {1, 2, . . . , 2n} {1, 2, . . . , 2n},
consider ordering defined it: [xF (1) , xF (2) , . . . , xF (4n2 ) ], xF (k) = xF1 (k),F2 (k)
every k. want prove ROBDD PB constraint ordering
least 2n nodes.
proof consist showing hypotheses Lemma 9 hold. is, first
show variable ordering, find integer assignment
453

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

first variables extended full assignment adds K. Then, prove
least 2n different values add first coefficients, required
Lemma 9.
Let us define bk 1 k 2n position k-th different value tuple
(F1 (1), F1 (2), . . . , F1 (4n2 )). formally,
bk =


1

k = 1,

n

min r : F1 (r) 6 {F1 (b1 ), F1 (b2 ), . . . , F1 (bk1 )}

k > 1.

Analogously, let us define c1 , . . . , c2n
ck =


1

k = 1,
n



min : F2 (s) 6 {F2 (c1 ), F2 (c2 ), . . . , F2 (ck1 )}

k > 1.

Let us denote ir = F1 (br ) js = F2 (cs ) 1 r, 2n. Notice
{i1 , i2 , . . . , i2n } {j1 , j2 , . . . , j2n } permutations {1, 2, . . . , 2n}. Assume bn cn
(the case analogous), take arbitrary assignment {xF (1) = vF (1) , xF (2) =
vF (2) , . . . , xF (cn ) = vF (cn ) }. want extend complete assignment
2

4n
X

aF (k) vF (k) = K.

k=1

Figure 4 represents initial assignment. values top-left square since
assignment undefined xir ,js r > n > n. Extending assignment
sum K amounts completing table way exactly n
ones every column row.
assignment completed following way: first, complete top left
square way, instance, adding zeros every non-defined cell. Then, copy
square bottom-right square and, finally, add complementary square
two squares (i.e., write 0 instead 1 1 instead 0). Figure 5 shows extended
assignment example.
formally, assignment completed follows:

vir ,js =



0




v

irn ,js


vir ,jsn






virn ,jsn






r, n vir ,js undefined,
r > n n,
> n r n,
r, > n,

0 = 1 1 = 0.
Now, let us prove satisfies requirements, i.e., coefficients corresponding
true variables assignment add exactly K. Let us fix r, n. Denote = ir ,
j = js , i0 = ir+n j 0 = js+n .
vi,j = 0, definition vi0 ,j = vi,j 0 = 1 vi0 ,j 0 = 0. Therefore,
ai,j vi,j + ai0 ,j vi0 ,j + ai,j 0 vi,j 0 + ai0 ,j 0 vi0 ,j 0

= ai0 ,j + ai,j 0
0

0

= 22n+i 1 + 2j1 + 22n+i1 + 2j 1
ai,j + ai0 ,j + ai,j 0 + ai0 ,j 0
=
.
2
454

fiA New Look BDDs Pseudo-Boolean Constraints

i1

i2

j1

1

1

j2

0

...

jn

1

...



in+1

in+2

...

i2n

0

1

jn+1

jn+2

...

j2n

Figure 4: arbitrary assignment. 0, 1 nothing position (ir , js ) depending
whether xir ,js false, true unassigned.

455

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

i1

i2

...



in+1

in+2

...

i2n

j1

1

1

0

0

0

0

1

1

j2

0

0

0

0

1

1

1

1

...

0

1

0

0

1

0

1

1

jn

0

0

1

0

1

1

0

1

jn+1

0

0

1

1

1

1

0

0

jn+2

1

1

1

1

0

0

0

0

...

1

0

1

1

0

1

0

0

j2n

1

1

0

1

0

0

1

0

Figure 5: Extended assignment. exactly n ones every column row.

Analogously, vi,j = 1,
ai,j vi,j + ai0 ,j vi0 ,j + ai,j 0 vi,j 0 + ai0 ,j 0 vi0 ,j 0 =

ai,j + ai0 ,j + ai,j 0 + ai0 ,j 0
.
2

Therefore,
2

2

4n
X
k=1

aF (k) vF (k)

4n
1X

= K.
=
2 k=1 F (k)

Lemma 9, number nodes ROBDD least number different
results obtain adding subset coefficients aF (1) , aF (2) , . . . , aF (cn ) . Consider set aF (c1 ) , aF (c2 ) , . . . , aF (cn ) . see different subsets add
different values, hence ROBDD size least 2n .
sum subset {aF (c1 ) , aF (c2 ) , . . . , aF (cn ) }
= aF (c1 ) v1 + aF (c2 ) v2 + + aF (cn ) vn ,

vr {0, 1}.

Let us look 2n last bits binary notation: digits 0 except
positions F2 (c1 ), F2 (c2 ), . . . , F2 (cn ), v1 , v2 , . . . , vn . Therefore, two subsets
add same, 2n last digits sum same. means values
(v1 , . . . , vn ) same, thus subset.
456

fiA New Look BDDs Pseudo-Boolean Constraints

3.3 Relation Subset Sum Problem ROBDD Size
section, study relationship ROBDD size PB constraint
subset sum problem. allows us to, assuming NP co-NP different,
give much simpler proof exist PB constraints admit polynomial
ROBDDs.
Lemma 9 exponential ROBDD example Theorem 10 suggest
relationship size ROBDDs number ways obtain K
adding coefficients PB. seems K obtained lot
different ways, ROBDD large.
section explore another relationship problem adding K
subset coefficients size ROBDDs. sense, give proof
converse last paragraph true: NP co-NP different,
exponentially-sized ROBDDs PB constraints subsets coefficients adding
K. Let us start defining one version well-known subset sum problem.
Definition 11. Given set positive integers = {a1 , . . . , } integer K,
subset sum problem (K, S) consists determining whether exists subset
{a1 , . . . , } sums exactly K.
well-known subset sum problem NP-complete K 2n ,
polynomial algorithms n K also polynomial n. given subset
sum problem (K, S) = {a1 , . . . , }, construct associated PB constraint
a1 x1 + +an xn K. previous section seen one PB constraint family whose
coefficients add K exponential number ways, thus generating exponential
ROBDD. Now, assuming NP co-NP different, see exists
PB constraint family exponential ROBDDs ordering coefficients
cannot add K. First, show ROBDDs act unsatisfiability certificates
subset sum problem.
Theorem 12. Let (K, S) UNSAT subset sum problem. Then, ROBDD
associated PB constraint polynomial size, act polynomial unsatisfiability
certificate (K, S).
Proof. need show how, polynomial time, check whether ROBDD
unsatisfiability certificate (K, S). that, note subset sum problem
UNSAT PB constraints
a1 x1 + + xn K,

a1 x1 + + xn K 1

equivalent, happens ROBDDs same. Therefore,
show, polynomial time, given ROBDD represents constraints.
done, instance, building ROBDD (using Algorithm 1 Section 5)
comparing ROBDDs.
key point that, assume NP different co-NP, exists
family UNSAT subset sum problems polynomial-sized unsatisfiability certificate.
Hence, family consisting associated PB constraints admit polynomial
ROBDDs.
457

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

Hence, PB constraints associated difficult-to-certify UNSAT subset sum problems
produce exponential ROBDDs. However, subset sum NP-complete K 2n .
PB constraints industrial problems usually K nr r, could expect
non-exponential ROBDDs constraints.

4. Avoiding Exponential ROBDDs
section introduce positive results. restrict particular class
PB constraints, coefficients powers two. show below,
constraints admit polynomial ROBDDs. Moreover, PB constraint reduced
class means coefficient decomposition.
Example 13. Let us take PB constraint 9x1 + 8x2 + 3x3 10. Considering binary
representation coefficients, constraint rewritten (23 x3,1 + 20 x0,1 ) +
(23 x3,2 ) + (21 x1,3 + 20 x0,3 ) 10 add binary clauses expressing xi,r = xr
appropriate r.
4.1 Power-of-two PB Constraints Polynomial-size ROBDDs
Let us consider PB constraint form:
C : 20 0,1 x0,1 + 20 0,2 x0,2 + + 20 0,n x0,n
21 1,1 x1,1 + 21 1,2 x1,2 + + 21 1,n x1,n
...


2 m,1 xm,1 + 2 m,2 xm,2 + + 2m m,n xm,n

+
+
+
K,

i,r {0, 1} r. Notice every PB constraint whose coefficients
powers 2 expressed way. Let us consider ROBDD representation
ordering [x0,1 , x0,2 , . . . , x0,n , x1,1 , . . . , xm,n ].
Lemma 14. Let [, ] interval node selector variable xi,r . 2i divides
0 < (n + r 1) 2i .
Proof. Proposition 5.1, expressed sum coefficients
multiples 2i , hence multiple 2i . Proposition 5.7, node
whose interval contains negative values False node, hence 0. Now, using
Proposition 5.3, must assignment variables {x0,1 , . . . , xi,r1 }
20 0,1 x0,1 + + 2i i,r1 xi,r1 belongs interval. Therefore:
20 0,1 x0,1 + + 2i i,r1 xi,r1 20 + 20 + + 2i

= n20 + n21 + + n2i1 + (r 1) 2i = n(2i 1) + 2i (r 1)

< 2i (n + r 1)

Corollary 15. number nodes selector variable xi,r bounded n + r 1.
particular, size ROBDD belongs O(n2 m).
458

fiA New Look BDDs Pseudo-Boolean Constraints

Proof. Let 1 , 2 , . . . , nodes selector variable xi,r . Let [j , j ] interval
j . Note intervals pair-wise disjoint since non-empty intersection would
imply exists constraint represented two different ROBDDs. Hence
assume, without loss generality, 1 < 2 < < . Due Lemma 14, know
j j1 2i . Hence 2i (n + r 1) > t1 + 2i 1 + 2i (t 1) 2i (t 1)
conclude < n + r.
4.2 Consistent Encoding PB Constraints
Let us take arbitrary PB constraint C : a1 x1 + xn K assume
largest coefficient. = log , rewrite C splitting coefficients
powers two shown Example 13:
C :

20 0,1 x0,1
21 1,1 x1,1
2m m,1 xm,1

20 0,2 x0,2 + + 20 0,n x0,n
21 1,2 x1,2 + + 21 1,n x1,n
...
+ 2m m,2 xm,2 + + 2m m,n xm,n

+
+

+
+
+
K,

m,r m1,r 0,r binary representation ar . Notice C C represent
constraint add clauses expressing xi,r = xr appropriate r.
process called coefficient decomposition PB constraint. similar idea given
Bartzis Bultan (2003).
important remark that, using consistent SAT encoding ROBDD C
(e.g. one given Een & Sorensson, 2006, one presented Section 6) adding
clauses expressing xi,r = xr appropriate r, obtain consistent encoding
original constraint C using O(n2 log ) auxiliary variables clauses.
difficult see. Take assignment variables C cannot
extended model C. coefficients corresponding variables
true add K. Using clauses xi,r = xr , unit propagation produce
assignment xi,r cannot extended model C. Since encoding
C consistent, false clause found. Conversely, consider assignment
variables C extended model C, assignment clearly
extended model C clauses expressing xi,r = xr . Hence, unit propagation
clauses encoding C detect false clause.
Example 16. Consider PB constraint C : 2x1 + 3x2 + 5x3 6. obtaining
consistent encoding presented, first rewrite C splitting coefficients
powers two:
C 0 : 1x0,2 + 1x0,3 + 2x1,1 + 2x1,2 + 4x2,3 6.

Next, encode C 0 ROBDD finally encode ROBDD SAT add clauses
enforcing relations xi,j = xj . Or, instead that, replace xi,j xj
ROBDD, encode result SAT. Figure 6 shows decision diagram
replacement.
4.3 Generalized Arc-consistent Encoding PB Constraints
Unfortunately, previous approach result GAC encoding. intuitive
idea seen following example:
459

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

x2
0

0

x3

1
1

x1 1

0

x1
1

0

0

x2

1

0

1

x3
1

0

Figure 6: Decision Diagram 2x1 + 3x2 + 5x3 6 decomposing coefficients
powers two.

Example 17. Let us consider constraint 3x1 + 4x2 6. splitting coefficients
powers two, obtain C 0 : x0,1 + 2x1,1 + 4x2,2 6. set x2,2 true, C 0 implies
either x0,1 x1,1 false, encoding cannot exploit fact
variables receive truth value hence propagated. Adding
clauses stating x0,1 = x1,1 help sense.
order overcome limitation, follow method presented Bessiere, Katsirelos, Narodytska, Walsh (2009) Bailleux et al. (2009). Let C : a1 x1 + +an xn
K arbitrary PB constraint. denote Ci constraint a1 x1 + + ai 1 + +
xn K, i.e., constraint assuming xi true. every 1 n,
encode Ci Section 4.2 and, addition, add binary clause ri xi , ri
root ROBDD Ci . clause helps us preserve GAC: given assignment
{xi } cannot extended model C, literal ri propagated
using (because encoding Ci consistent). Hence added clause allow us
propagate xi .
Example 18. Consider PB constraint C : 2x1 + 3x2 + 5x3 6. Let us define
constraints C1 : 3x2 + 5x3 4, C2 : 2x1 + 5x3 3 C3 : 2x1 + 3x2 1. Now, encode
constraints ROBDDs previous section, coefficient decomposition.
Figure 7 shows resulting ROBDDs. Finally, need encode SAT consistently, add clauses ri xi , assuming variable associated root
ROBDD Ci ri .
encoding GAC: take instance assignment = {x1 = 1}. Constraint C3
satisfied. Hence, consistency, r3 propagated. Therefore, clause r3 x3 propagates
x3 , wanted. propagation assignments similar.
all, suggested encoding GAC uses O(n3 log(aM )) clauses auxiliary
variables, largest coefficient.
460

fiA New Look BDDs Pseudo-Boolean Constraints

r1
x2

r2
x3

0

0

1

x3
0

x2

1

0

0
0

1

1

1

1

x2

1

0

1

0

x3
0

1

r3
x1

1

0

Figure 7: ROBDDs2 C1 , C2 C3 coefficient decomposition.

5. Algorithm Constructing ROBDDs Pseudo-Boolean
Constraints
Let us fix Pseudo-Boolean constraint a1 x1 + + xn K variable ordering
[x1 , x2 , . . . , xn ]. goal section prove one construct ROBDD
constraint using ordering polynomial time respect ROBDD size
n.
algorithm builds standard ROBDDs, used build ROBDDs
coefficient decomposition: need first split coefficients and, secondly, apply
algorithm. Forthcoming Example 21 shows detail overall process. similar
version algorithm described Mayer-Eichberger (2008).
key point algorithm label node ROBDD
interval. following, every {1, 2, . . . , n + 1}, use set Li consisting
pairs ([, ], B), B ROBDD constraint ai xi + + xn K 0
every K 0 [, ] (i.e., [, ] interval B). sets kept tuple
L = (L1 , L2 , . . . , Ln+1 ).
Note definition ROBDDs intervals, ([1 , 1 ], B1 ) ([2 , 2 ], B2 ) belong
Li either [1 , 1 ] = [2 , 2 ] [1 , 1 ] [2 , 2 ] = . Moreover, first case holds
B1 = B2 . Therefore, Li represented binary search tree-like data
structure, insertions searches done logarithmic time. function
search(K, Li ) searches whether exists pair ([, ], B) Li K [, ].
tuple returned exists, otherwise empty interval returned first component
pair. Similarly, also use function insert(([, ], B), Li ) insertions.
overall algorithm detailed Algorithm 1 Algorithm 2:
Theorem 19. Algorithm 1 runs O(nm log m) time (where size ROBDD)
correct following sense:
2. Actually, diagram replacing variables xi,j xj ROBDD. However, denote
ROBDDs simplicity.

461

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

Algorithm 1 Construction ROBDD algorithm
Require: Constraint C : a1 x1 + . . . + xn K 0
Ensure: returns B ROBDD C.
1: insuch 1 n + 1



2:
Li
(, 1], F alse , [ai + ai+1 + + , ), rue
3: end
4: L (L1 , . . . , Ln+1 ).
5: ([, ], B) BDDConstruction(1, a1 x1 + . . . + xn K 0 , L).
6: return B.

Algorithm 2 Procedure BDDConstruction
Require: integer {1, 2, . . . , n + 1}, constraint C : ai xi + . . . + xn K 0 set L
Ensure: returns [, ] interval C B ROBDD
1: ([, ], B) search(K 0 , Li ).
2: [, ] 6=
3:
return ([, ], B)
4: else
5:
([F , F ], BF ) := BDDConstruction(i + 1, ai+1 xi+1 + . . . + xn K 0 , L).
6:
([T , ], BT ) := BDDConstruction(i + 1, ai+1 xi+1 + . . . + xn K 0 ai , L).
7:
[T , ] = [F , F ]
8:
B BT
9:
[, ] [T + ai , ]
10:
else
11:
B ite(xi , BT , BF ) // root xi , true branch BT false branch BF .
12:
[, ] [F , F ] [T + ai , + ai ]
13:
end
14:
insert(([, ], B), Li )
15:
return ([, ], B)
16: end

462

fiA New Look BDDs Pseudo-Boolean Constraints

1. K 0 belongs interval returned BDDConstruction(ai xi + +an xn K 0 , L).
2. tuple ([, ], B) returned BDDConstruction consist BDD B
interval [, ].
3. BDDConstruction returns ([, ], B), BDD B reduced.
Proof. Let us first start three correctness statements:
1. K 0 found Li line 1 Algorithm 2 statement obviously true. Otherwise
let us reason induction i. base case = n + 1, since Ln+1
contains intervals (, 1] [0, ], search call line 1 succeed
hence result holds. < n + 1 assume, induction hypothesis,
K 0 [F , F ] K 0 ai [T , ]. two intervals coincide result obvious,
otherwise also easy see K 0 [F , F ] [T + ai , + ai ].
2. Let us prove every moment tuples L correct, i.e., contain
BDDs correct interval. Since returned value always element
Li , proves statement.
Proposition 5.6 5.7, initial tuples L correct. prove
previously inserted intervals correct, current interval also correct.
follows virtue Proposition 7.
3. Let us prove tuples L contain reduced BDDs. before,
initial BDDs L reduced. Let B BDD computed algorithm,
children BT BF . induction hypothesis, reduced, B reduced
two children equal. algorithm creates node
childrens intervals different. Therefore, BT BF represent
Boolean constraint, different BDDs.
Regarding runtime, since cost search insertion Li logarithmic size,
cost algorithm O(log m) times number calls BDDConstruction. Hence,
remains show O(nm) calls.
Every call (but first one) BDDConstruction done exploring
edge ROBDD. Notice edge explored twice, since edges explored
parent node whenever reach explored node recursive calls
BDDConstruction. hand, every edge ROBDD make 2k 1
calls, k length edge (if nodes joined edge variables xi
xj say length |i j|). Since ROBDD O(m) edges length
O(n), number calls O(nm).
Let us illustrate algorithm example:
Example 20. Take constraint C : 2x1 + 3x2 + 5x3 6, let us apply algorithm
obtain ROBDD ordering [x1 , x2 , x3 ]. Figure 8 represents recursive calls
BDDConstruction returned parameters (the ROBDD interval).
463

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

1. BDDConstruction(1, 2x1 + 3x2 + 5x3 6, L)
x1 [5, 6]
0

1

x2
0

1

x3
0

1

1

0

2. BDDConstruction(2, 3x2 + 5x3 6, L)

7. BDDConstruction(2, 3x2 + 5x3 4, L)

x2 [5, 7]
0

x3 [2, 4]

1
0

x3
0

1

1

3. BDDConstruction(3, 5x3 6, L)

1

1

0

0

4. BDDConstruction(3, 5x3 3, L)
x3
0

1 [5, )
1

5. BDDConstruction(4, 0 3, L)

8. BDDConstruction(3, 5x3 4, L)

[0, 4]

x3
0

1

1

0

9. BDDConstruction(3, 5x3 1, L)

[0, 4]

0

6. BDDConstruction(4, 0 2, L)

1 [0, )

0 (, 1]

Figure 8: Recursive calls BDDConstruction, returned values.

464

x3 [0, 4]
0

1

1

1

0

fiA New Look BDDs Pseudo-Boolean Constraints

calls number 3, 5, 6, 8 9, search function returns true interval
ROBDD returned without computation.
call number 7, two recursive calls return interval (and, therefore,
ROBDD). Hence, ROBDD returned.
call number 1 two recursive calls return two different ROBDDs, adds
node join two ROBDDs another one, returned. happens
calls number 2 4.
overall process coefficient decomposition would work following example:
Example 21. Let us take constraint C : 2x1 + 3x2 + 5x3 6. want build
ROBDD coefficient decomposition using Algorithm 1, proceed follows:
1. Split coefficients obtain C 0 : 1y1 + 1y2 + 2y3 + 2y4 + 4y5 6, x1 = y3 ,
x2 = y1 = y4 x3 = y2 = y5 .
2. Apply algorithm C 0 obtain ROBDD B 0 .
3. Replace y1 x2 , y2 x3 , etc. nodes B 0 .

6. SAT Encodings BDDs Monotonic Functions
section consider BDD representing monotonic function F want
encode SAT. expected, want encoding small possible GAC.
encoding explained valid type BDDs, so, particular, valid
ROBDDs. main differences Minisat+ encoding (Een & Sorensson, 2006)
number clauses generated (6 ternary clauses per node versus one binary one
ternary clauses per node) encoding GAC variable ordering.
usual, encoding introduces auxiliary variable every node. Let node
selector variable x auxiliary variable n. Let f variable false child
variable true child. two clauses per node needed:
f n

x n.

Furthermore, add unit clause variable True node another one
negation variable False node.
Theorem 22. encoding consistent following sense: partial assignment
cannot extended model F r propagated unit propagation,
r root BDD.
Proof. prove theorem induction number variables BDD.
BDD variables, BDD either True node False node
result trivial.
Assume result true BDDs less k variables, let F
function whose BDD k variables. Let r root node, x1 selector variable
465

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

f, respectively false true children (note abuse notation identify
nodes auxiliary variable). denote F1 function F|x1 =1 (i.e., F
setting x1 true) F0 function F|x1 =0 .
Let partial assignment cannot extended model F .
Assume x1 A. Since cannot extended, assignment \ {x1 } cannot
extended model F1 . definition BDD, function F1
BDD. induction hypothesis, propagated, since x1 A, r also
propagated.
Assume x1 6 A. Then, assignment \ {x1 } cannot extended model
F0 . Since F0 f BDD, induction hypothesis f propagated,
hence r also is.
Let partial assignment, assume r propagated. Then, either f
also propagated propagated x1 (note x1
propagated appears one clause already true).
Assume f propagated. Since f BDD F0 , induction
hypothesis assignment \ {x1 , x1 } cannot extended model F0 .
Since function monotonic, neither \ {x1 , x1 } extended model
F . Therefore, cannot extended model F .
Assume propagated x1 A. Since BDD F1 ,
induction hypothesis \ {x1 } cannot extended model F1 , neither
extended model F .

obtaining GAC encoding, add unit clause.
Theorem 23. add unit clause forcing variable root node true,
previous encoding becomes generalized arc-consistent.
Proof. prove induction variables BDD. case zero
variables trivial, let us prove induction case.
before, let r root node, x1 selector variable n auxiliary variable,
f, false true children. denote F0 F1 functions F|x1 =0 F|x1 =1 .
Let partial assignment extended model F . Assume
{xi } cannot extended. want prove xi propagated.
Let us assume x1 A. case, propagated due clause x1 n
unit clause n. Since x1 {xi } cannot extended model F ,
\ {x1 } {xi } neither extended assignment satisfying F1 . induction
hypothesis, since BDD function F1 , xi propagated.
Let us assume x1 6 xi 6= x1 . Since F monotonic, {xi } cannot
extended model F cannot extended model F0 . Notice
f propagated thanks clause f n unit clause n. induction
hypothesis, method GAC F0 , xi propagated.
466

fiA New Look BDDs Pseudo-Boolean Constraints

Finally, assume x1 6 xi = x1 . Since {x1 } cannot extended
model F , cannot extended model F1 . Theorem 22, propagated
and, due x1 n n, also x1 .

finish section example illustrating suggested encoding BDDs
SAT used different PB encoding methods presented paper.
Example 24. Consider constraint C : 2x1 + 3x2 + 5x3 6. encode constraint SAT three methods: usual ROBDD encoding; consistent
approach ROBDDs splitting coefficients, explained Section 4.2;
GAC approach ROBDDs splitting coefficients explained Section 4.3.
1. BDD-1: method builds ROBDD C encodes SAT. Hence
start building ROBDD C, seen last picture Figure 1.
Now, need encode SAT. Let y1 , y2 y3 fresh variables corresponding
nodes ROBDD C respectively x1 , x2 x3 selector variable.
node y1 , add clauses y2 y1 x1 y3 y1 .

y2 , add clauses > y2 x2 y3 y2 , > tautology
symbol.
y3 , add clauses > y3 x3 y3 , contradiction symbol.
Moreover, add unit clauses >, y1 . all, removing
units tautologies, clauses obtained y1 , y2 , x1 y3 , x2 y3 x3 y3 .
2. BDD-2: build ROBDD C coefficient decomposition Example 21.
Figure 6 shows resulting ROBDD. introduce variables y1 , y2 , . . . , y6 every
node ROBDD. precisely, first x2 node (starting top-down) receives
variable y1 , next x2 node gets y5 . first x3 receives y2 one y6 .
Finally leftmost x1 node gets variable y3 one y4 . add
following clauses: y2 y1 , y4 x2 y1 , y3 y2 , y4 x3 y2 , > y3 , y5 x1 y3 ,
y5 y4 , y6 x1 y4 , > y5 , y6 x2 y5 , > y6 , x3 y6 , unit
clauses >, y1 .
removing units clauses tautologies, obtain y1 , y2 , y3 , y4 x2 ,
y4 x3 , y5 x1 , y5 y4 , y6 x1 y4 , y6 x2 y5 x3 y6 .

Notice encoding consistent: assignment = {x2 , x3 },
y4 propagated clause y4 x2 , turn propagates y5 due clause y5 y4
finally y6 propagated clause y6 x2 y5 . contradiction found
clause x3 y6 .

However, encoding GAC: partial assignment = {x1 } propagate y5 . However, x3 also propagated.

3. BDD-3: let C1 , C2 C3 constraints setting respectively x1 , x2 x3 true.
Figure 7 shows ROBDDs constraints. encode ROBDDs
467

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

usual, BDD-2, replacing unit clause r root r xi .
case variables associated roots C1 , C2 C3 y1 , z1 w1
respectively.
removing units tautologies, clauses C1 y1 x1 , y2 y1 , y4 x2 y1 ,
y3 y2 , y4 x3 y2 , y4 x2 y3 x3 y4 .
Clauses C2 z1 x2 x3 z1 .
Finally, clauses C3 w1 x3 , w2 w1 , x1 w1 x2 w2 .
encoding GAC. Take, instance, assignment = {x1 }. case, w1
propagated virtue x1 w1 x3 propagated clause w1 x3 .

7. Related Work
Due ubiquity Pseudo-Boolean constraints success SAT solvers, problem encoding constraints SAT thoroughly studied literature.
following review important contributions, paying special attention
basic idea based, encoding size, propagation properties
encodings fulfill. ease presentation, remaining section always
assume constraint want encode a1 x1 + . . . + xn k, maximum
coefficient amax .
first encoding mention one proposed Warners (1998). nutshell,
encoding uses several adders numbers binary representation. First all, left hand
side constraint split two halves, recursively treated compute
corresponding partial sum. that, two partial sums added final
result compared k . encoding uses O(n log(amax )) clauses variables
neither consistent GAC. surprising, since adders numbers binary
make extensive use xors, good propagation properties.
Bailleux et al. (2006) introduced encoding close using BDD
translating clauses. order understand differences construction BDDs let us introduce detail. First all, coefficients ordered
small large. Then, root labeled variable Dn,k , expressing sum
first n terms k. two children Dn1,k Dn1,kan ,
correspond setting xn false true, respectively. process repeated nodes
corresponding trivial constraints reached, encoded true false.
node Di,b children Di1,b Di1,bai , following four clauses added:
Di1,bai Di,b
Di1,bai xi Di,b

Di1,b Di,b
Di1,b xi Di,b

Example 25. encoding proposed Bailleux et al. (2006) 2x1 + + 2x10 + 5x11 +
6x12 10 illustrated Figure 9. Node D10,5 represents 2x1 + 2x2 + + 2x10 5,
whereas node D10,4 represents 2x1 + 2x2 + 2x10 4. method fails identify
two PB constraints equivalent hence subtrees B C merged,
yielding much larger representation ROBDDs.
468

fiA New Look BDDs Pseudo-Boolean Constraints

D12,10
0

1

D11,10
0

D11,4
1

0

D10,10

D10,5

D10,4



B

C

1

D10,1
false

Figure 9: Tree-like construction Bailleux et al. (2006) 2x1 + +2x10 +5x11 +6x12 10

resulting encoding GAC, example PB constraint family given
kind non-reduced BDDs, concrete variable ordering exponentially large.
However, shown Section 3.2, ROBDDs family polynomial.
Several important new contributions presented paper MiniSAT
team (Een & Sorensson, 2006). paper describes three encodings,
implemented MiniSAT+ tool. first one standard ROBDD construction
Pseudo-Boolean constraints. done two steps: first, suggest simple dynamic
programming algorithm constructing non-reduced BDD, later reduced.
result ROBDD, first step may take exponential time even final ROBDD
polynomial. ROBDD constructed, suggest encode SAT using
6 ternary clauses per node. paper showed that, given concrete variable ordering,
encoding GAC. Regarding ROBDD size, authors cite work Bailleux et al.
(2006) state BDDs exponential worst case. seen before,
citation correct Bailleux et al construct ROBDDs.
second method similar one Warners (1998) sense construction relies network adders. First coefficients decomposed binary
representation. bit i, bucket created variables whose coefficient bit
set one. i-th bit left-hand side constraint computed using series
full adders half adders. Finally, resulting sum lexicographically compared k.
resulting encoding neither consistent GAC uses number adders linear
sum number digits coefficients.
last method suggest use sorting networks. Numbers expressed
unary representation coefficients decomposed using mixed radix representation.
smaller number representation, smaller encoding. setting,
sorting networks used play role adders, better propagation
properties. N smaller sum digits coefficients base 2, size
encoding O(N log2 N ). Whereas encoding GAC arbitrary PseudoBoolean constraints, generalized arc-consistency obtained cardinality constraints.
469

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

Encoding
Warners
Non-reduced BDD
ROBDD
Adders
Sorting Networks
Watch Dog (WD)
Gen. Arc-cons. WD

Reference
(Warners, 1998)
(Bailleux et al., 2006)
(Een & Sorensson, 2006)
(Een & Sorensson, 2006)
(Een & Sorensson, 2006)
(Bailleux et al., 2009)
(Bailleux et al., 2009)

Clauses
O(n log amax )
Exponential
Exponential (6 per node)
P
O( log ai )
P
P
O(( log ai ) log2 ( log ai )
O(n2 log n log amax )
O(n3 log n log amax )

Consist.

YES
YES

YES
YES
YES

GAC

YES
YES



YES

Table 1: Summary comparing different encodings.
first polynomial GAC encoding Pseudo-Boolean constraints, called WatchDog, introduced Bailleux et al. (2009). uses O(n2 log n log amax ) variables
O(n3 log n log amax ) clauses. Again, numbers expressed unary representation
totalizers used play role sorting networks. order make comparison
right hand side trivial, left-hand side k incremented k becomes
power two. Then, coefficients decomposed binary representation
bit added independently, taking account corresponding carry. paper, another encoding consistent uses O(n log n log amax ) variables
O(n2 log n log amax ) clauses also presented.
Finally, worth mentioning work Bartzis Bultan (2003). authors
deal general case variables xi Boolean, bounded
integers 0 xi < 2b . suggest BDD-based approach similar flavor
method Section 4, instead decomposing coefficients do, decompose
variables xi binary representation. BDD ordering starts first bit
x1 , first bit x2 , etc... that, second bit treated similar
P
fashion, on. resulting BDD O(n b ai ) nodes nothing mentioned
propagation properties. case Pseudo-Boolean constraints, i.e. b = 1,
approach amounts standard BDDs.
Table 1 summarizes different encodings PB constraints SAT.

8. Experimental Results
goal section assess practical interest encodings presented
paper. aim evaluate extent BDD-based encodings interesting
practical point view. us, means study whether competitive
existing techniques, whether show good behavior general interesting
specific types problems, whether produce smaller encodings.
purpose, first all, compare encodings SAT encodings
terms encoding time, number clauses number variables. that, also
consider total runtime (that is, encoding time plus solving time) encodings
compare runtime state-of-the-art Pseudo-Boolean solvers. Finally, briefly
report experiments sharing, is, trying encode several Pseudo-Boolean
constraints single ROBDD.
470

fiA New Look BDDs Pseudo-Boolean Constraints

experiments performed 2Ghz Linux Quad-Core AMD time limit
1800 seconds per benchmark. benchmarks used experiments obtained
Pseudo-Boolean Competition 2011 (http://www.cril.univ-artois.fr/PB11/),
category optimization, small integers, linear constraints (DEC-SMALLINT-LIN).
compactness clarity, grouped benchmarks come source
families. However, individual results found http://www.lsi.upc.edu/~iabio/
BDDs/results.ods.
8.1 Bergmann-Hommel Test
order summarize experiments make easier extract conclusions, every experiment accompanied Bergmann-Hommel non-parametric hypothesis test
(Bergmann & Hommel, 1988) results confidence level 0.1.
Bergmann-Hommel test way comparing results n different methods
multiple independent data sets. gives us two interesting pieces information. First
all, sorts methods giving real number 1 n,
lower number better method. Moreover, indicates, pair methods,
whether one method significantly improves upon other. example, Figure 10
output Bergmann-Hommel test. BDD-1 best method significant
difference method BDD-2 (this illustrated thick line connecting
methods). hand, Bergmann-Hommel test indicates BDD-1
significantly better Adder, since thick line connecting BDD-1 Adder.
said BDD-1 WD-1, BDD-1 BDD-3, BDD-1 WD-2, BDD-2
Adder, etc.
give quick overview Bergmann-Hommel test computed.
remaining section skipped reader interested details
test. hand, detailed information, refer reader work
Bergmann Hommel (1988).
Let us assume n methods data sets, let Ci,j result (time,
number variables value) i-th method j-th benchmark.
every data set, assign number every method: best method data set
1, second 2, on. Then, every method, compute average
values different data sets. obtained value denoted Ri called
average rank i-th method. method smaller average rank better
method bigger one.
average ranks make possible rank different methods. However,
also interested detecting whether differences methods significant
not: computed second part test. that, need previous
definitions.
R R
Given i, j N = {1, 2, . . . , n}, denote pi,j p-value3 zi,j = j

n(n1)/(6m)

respect normal distribution N (0, 1). partition N = {1, 2, . . . , n} collection sets
P = {P1 , P2 , . . . , Pr } (i) Pi subsets N , (ii) P1 P2 Pr = N
3. p-value z respect normal distribution N (0, 1) probability p[ |Z| > |z| ],
random variable Z N (0, 1).

471

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

(iii) Pi Pj = every 6= j. Given P partition N , define
L(P ) =

r
X
|Pi |(|Pi | 1)
i=1

2

p(P ) minimum pi,j j belong subset Pk P .
Bergmann-Hommel test ensures (with significance level ) methods
j significantly different partition P p(P ) > L(P )
j belong subset Pk P . Hence, time-consuming test
since number partitions large.
case, data sets families benchmarks. use families
instead benchmarks data sets must independent.
8.2 Encodings SAT
start comparing different methods encoding Pseudo-Boolean constraints
SAT. focused time spent encoding, number auxiliary variables
used number clauses. Moreover, benchmark family, also report
number PB-constraints encoded SAT.
encodings included experimental evaluation are: adder encoding presented Een Sorensson (2006) (Adder), consistent WatchDog encoding
Bailleux et al. (2009) (WD-1), GAC version (WD-2), encoding ROBDDs without coefficient decomposition, using Algorithm 1 encoding Section 6 (BDD1); encoding ROBDDs coefficient decomposition explained Section 4.2
(BDD-2), Algorithm 1 encoding Section 6; GAC approach
Section 4.3 (BDD-3 ), also Algorithm 1 encoding Section 6. Notice
BDD-1 method similar ROBDDs presented Een Sorensson (2006).
However, since Algorithm 1 produces every node once, BDD-1 faster. Also,
encoding Section 6 creates two clauses per BDD node, opposed six clauses
suggested Een Sorensson.
Table 2 shows number problems different methods could encode without
timing out. first column corresponds family problems. second column
shows number problems family. third fourth columns contain average number SAT Pseudo-Boolean constraints problem. experiments,
considered constraint SAT clause 3 variables. Small PB
constraints benefit encodings hence constraints naive
encoding SAT always used. remaining columns correspond number
encoded problems without timing out. Time limit set 1800 seconds per benchmark.
Table 3 shows time spent encode benchmarks different methods.
before, first columns correspond family problems, number problems
family average number SAT Pseudo-Boolean constraints problems. remaining columns correspond average encoding time (in seconds) per
benchmarks method. Timeouts counted 1800 seconds average computation.
Table 4 shows average number auxiliary variables required encoding PB
constraints (SAT constraints counted). meaning first 4 columns
472

fiA New Look BDDs Pseudo-Boolean Constraints

Family
lopes
army
blast
cache
chnl
dbstv30
dbstv40
dbstv50
dlx
elf
fpga
j30
j60
j90
j120
neos
ooo
pig-crd
pig-cl
ppp
robin
13queen
11tsp11
vdw

Pr
200
12
8
9
21
5
5
5
3
5
36
17
18
17
28
4
19
20
20
6
6
100
100
5

TOTAL

669

SAT
502,671
192
6,510
181,100
0
326,200
985,200
2,552,000
20,907
46,446
0
13,685
30,832
50,553
104,147
1,451
95,217
0
161,150
29,846
0
8
2,662
8,978

PB
592,715
451
1,253
4,507
125
2,701
4,801
7501
857
1,399
687
270
309
337
516
3,831
4,487
113
58
1,023
761
93
45
267,840

Adder
188
12
8
9
21
5
5
5
3
5
36
17
18
17
28
4
19
20
20
6
6
100
100
5

WD-1
188
12
8
9
21
5
5
5
3
5
36
17
18
17
28
4
19
20
20
6
6
100
100
5

WD-2
118
12
8
9
21
0
0
0
3
5
36
17
18
8
11
4
19
18
20
6
2
100
100
5

BDD-1
197
12
8
9
21
5
5
5
3
5
36
17
18
17
28
4
19
20
20
6
6
100
100
5

BDD-2
197
12
8
9
21
5
5
5
3
5
36
17
18
17
28
4
19
20
20
6
6
100
100
5

BDD-3
188
12
8
9
21
0
0
0
3
5
36
17
18
11
18
4
19
20
20
6
6
100
100
5

657

657

540

666

666

626

Table 2: Number problems encoded (without timing out) different methods.
6

5

4

3

WD2

2

1

BDD1
BDD2

BDD3
WD1

Adder

Figure 10: Statistical comparison results Table 3, time spent different
methods encoding.

before, others contain average number auxiliary variables (in
thousands) benchmarks time out.
Finally, Table 5 contains average number (in thousands) clauses needed encode
problem. before, considered benchmarks timed out,
clauses due encoding SAT constraints counted.
Figures 10, 11 12 represent Bergmann-Hommel tests tables.
show BDD-1, BDD-2 Adders best methods terms time, variables
clauses. worth mentioning BDD-1 BDD-2 faster use significantly less
clauses Adder. However, Adders uses significantly less auxiliary variables BDD-2.
Notice BDD-1 GAC, BDD-2 consistent Adder consistent,
least theoretically BDD-1 clauses unit propagation power BDD-2 clauses,
BDD-2 clauses better Adder clauses. Hence, BDD-1 best method using
criteria BDD-2 better Adder. Regarding methods, seems clear
473

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

Family
lopes
army
blast
cache
chnl
dbstv30
dbstv40
dbstv50
dlx
elf
fpga
j30
j60
j90
j120
neos
ooo
pig-crd
pig-cl
ppp
robin
13queen
11tsp11
vdw

Pr
200
12
8
9
21
5
5
5
3
5
36
17
18
17
28
4
19
20
20
6
6
100
100
5

SAT
502,671
192
6,510
181,100
0
326,200
985,200
2,552,000
20,907
46,446
0
13,685
30,832
50,553
104,147
1,451
95,217
0
161,150
29,846
0
8
2,662
8,978

PB
592,715
451
1,253
4,507
125
2,701
4,801
7,501
857
1,399
687
270
309
337
516
3,832
4,487
113
58
1,024
761
93
45
267,840

Average

Adder
335.23
0.37
3.89
23.08
0.54
57.77
211.51
547.30
3.73
7.37
1.90
3.64
6.85
14.81
19.25
10.43
13.48
0.97
7.73
6.13
12.03
0.19
0.46
170.33

WD-1
292.14
0.43
2.45
18.74
1.05
97.21
210.25
552.99
3.05
6.53
2.46
4.62
10.69
31.02
47.62
12.65
9.67
3.29
8.78
5.09
67.41
0.45
0.51
109.42

WD-2
996.07
39.98
40.41
81.65
87.08



8.41
21.68
69.90
81.03
466.07
1,277.28
1,305.55
257.97
71.20
517.51
284.15
33.26
1,315.96
100.29
24.42
441.21

BDD-1
165.75
0.19
2.20
16.19
0.13
45.85
105.62
272.02
2.76
5.19
0.30
3.13
8.19
28.20
21.68
3.46
7.76
0.22
7.35
3.17
2.94
0.14
0.30
47.15

BDD-2
163.66
0.19
1.89
15.74
0.13
83.09
165.96
468.51
2.75
5.90
0.30
3.67
8.77
27.76
25.50
5.32
7.88
0.21
7.31
3.23
2.82
0.17
0.33
46.32

BDD-3
316.35
10.26
23.15
47.78
2.68



6.19
13.42
3.75
42.44
252.69
1,155.18
967.10
77.04
26.35
9.52
10.79
9.83
301.11
18.48
6.30
125.91

110.57

99.79

510.40

55.90

57.66

223.41

Table 3: Average time spent encoding different methods.

Family
lopes
army
blast
cache
chnl
dbstv30
dbstv40
dbstv50
dlx
elf
fpga
j30
j60
j90
j120
neos
ooo
pig-crd
pig-cl
ppp
robin
13queen
11tsp11
vdw
Average

Pr
200
12
8
9
21
5
5
5
3
5
36
17
18
17
28
4
19
20
20
6
6
100
100
5

SAT
502,671
192
6,510
181,100
0
326,200
985,200
2,552,000
20,907
46,446
0
13,685
30,832
50,553
104,147
1,451
95,217
0
161,150
29,846
0
8
2,662
8,978

PB
592,715
451
1,253
4,507
125
2,701
4,801
7,501
857
1,399
687
270
309
337
516
3,832
4,487
113
58
1,024
761
93
45
267,840

Adder
1,744.05
4.63
27.77
145.66
8.39
219.82
2,468.45
6,135.13
10.4
20.37
21.15
18.15
37.03
65.4
159.43
73.74
118.15
15.26
7.68
57.13
171.67
2.2
3.37
1,895.39

WD-1
3,566.11
10.96
62.22
339.18
24.55
709.73
6,564.44
16,365.39
21.62
42.78
53.96
50.8
112.35
217.01
540
185.59
273.54
50.75
25.25
141.49
628.45
6.17
8.83
3,356.94

WD-2
5,478.81
245.49
1,394.5
2,393.97
1,007.9



247.79
571.38
1,074.03
1,190.59
4,775.72
6,543.52
5,713.75
3,542.94
2,248.34
2,966.58
1,984.06
623.58
3,634.13
461.54
170.59
12,818.51

BDD-1
2,393.97
6.36
36.74
201.18
6.76
441.86
4,282.16
11,111.06
12.40
24.62
13.27
44.96
157.92
553.8
612.13
79.33
162.25
11.93
4.01
81.57
158.55
5.63
5.71
1,391.65

BDD-2
2,394
6.36
39.67
210.83
6.76
1,695.87
7,225.63
19,723.37
13.89
28.13
13.27
59.82
180.05
553.76
806.08
122.53
168.61
11.93
4.01
82.86
158.55
7.08
6.51
1,391.65

BDD-3
7,734.65
479.83
761.29
1,503.19
184.59



126.81
306.76
242.03
1,153.51
7,285.69
19,793.49
22,246.82
2,003.95
1,315.77
632.33
310.03
382.67
16,565.75
791.43
221.21
5,875.58

591.35

1,266.23

1,876.33

892.62

998.44

3,807.34

Table 4: Average number auxiliary variables (in thousands) used.

474

fiA New Look BDDs Pseudo-Boolean Constraints

Family
lopes
army
blast
cache
chnl
dbstv30
dbstv40
dbstv50
dlx
elf
fpga
j30
j60
j90
j120
neos
ooo
pig-crd
pig-cl
ppp
robin
13queen
11tsp11
vdw
Average

Pr
200
12
8
9
21
5
5
5
3
5
36
17
18
17
28
4
19
20
20
6
6
100
100
5

SAT
502,671
192
6,510
181,100
0
326,200
985,200
2,552,000
20,907
46,446
0
13,685
30,832
50,553
104,147
1,451
95,217
0
161,150
29,846
0
8
2,662
8,978

PB
592,715
451
1,253
4,507
125
2,701
4,801
7,501
857
1,399
687
270
309
337
516
3,832
4,487
113
58
1,024
761
93
45
267,840

Adder
10,643.63
26.09
184.95
980.51
56.82
1,497.41
17,184.62
42,797.38
65.25
129.05
139.45
121.56
253.16
450.49
1,102.86
471.47
793.36
104.68
52.41
392.6
1,185.92
14.73
23.31
10,885.96
3,675.7

WD-1
7,471.89
34.5
102.07
550.68
116.97
3,367.75
16,916.6
44,310.83
35.68
71.28
175.66
164.78
494.83
1,286.17
3,803.28
594.72
442.47
367.03
180.33
271.66
6,916.35
38.91
25.35
6,564.14
2,970.54

WD-2
22,082.47
2,155.7
2,108.48
3,651.91
4,936.16



377.76
881.02
3,615.21
3,889.58
22,843.05
34,136.7
26,205.19
12,410.1
3,378.16
20,711.11
14,641.26
1,804.29
19,694.86
5,068.35
1,335.6
24,274.21
8,297.1

BDD-1
3,049.06
10.87
70.9
272.27
11.23
857.39
5,526.6
14,400.14
23.04
46.3
15.65
89.53
311.15
1,106.22
1,186.55
139.26
219.41
19.86
4.07
100.89
281.42
10.84
7.76
1,662.73
1,174.38

BDD-2
3,049.02
10.87
65.46
275.26
11.23
3,282.22
11,259.29
31,279.2
22.59
46.07
15.65
116.04
351.01
1,095.12
1,570.73
220.45
227.61
19.86
4.07
103.24
281.42
13.73
9.35
1,662.73
1,380.41

BDD-3
9,746.32
924.93
1,264.83
2,418.77
285.67



208.92
507.46
278.37
2,244.021
14,355.2
39,112.08
44,068.97
3,681.26
2,126.42
958.65
314.05
654.48
28,875.61
1,573.89
433.59
7,262.88
5,843.53

Table 5: Average number clauses (in thousands) used.

6

5

4

3

2

WD2
BDD3

1

Adder
BDD1
WD1

BDD2

Figure 11: Statistical comparison results Table 4, number auxiliary variables
used different encodings.

6

5

4

3

WD2

2

1

BDD1
BDD2

BDD3
WD1

Adder

Figure 12: Statistical comparison results Table 5, number clauses used
different methods.

475

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

Family
lopes
army
blast
cache
chnl
dbstv30
dbstv40
dbstv50
dlx
elf
fpga
j30
j60
j90
j120
neos
ooo
pig-crd
pig-cl
ppp
robin
13queen
11tsp11
vdw
TOTAL

Adder
42
9
8
9
3
5
0
0
3
5
25
17
17
17
14
2
15
2
2
4
3
100
100
1

WD-1
54
12
8
9
3
5
5
5
3
5
36
17
17
17
16
2
19
2
1
3
3
100
100
1

WD-2
40
7
8
9
2
0
0
0
3
5
36
17
17
7
9
2
16
2
2
4
2
100
96
1

BDD-1
56
10
8
9
5
5
5
5
3
5
36
17
17
17
16
2
18
2
1
3
3
100
100
1

BDD-2
57
11
8
9
5
5
5
5
3
5
36
17
17
17
16
2
19
2
1
4
3
100
100
1

BDD-3
61
5
8
9
3
0
0
0
3
5
36
17
17
8
11
2
17
1
2
4
6
100
75
1

bsolo
39
6
8
7
21
5
5
5
3
5
36
17
17
17
13
2
14
19
3
4
3
100
72
1

MiniSAT
66
6
8
8
3
5
5
5
3
5
33
17
17
17
12
2
15
2
2
4
3
100
90
1

SAT4J
23
6
8
6
1
5
5
5
3
5
36
17
17
17
16
2
14
2
2
4
4
100
93
1

Wbo
63
6
8
6
3
5
5
5
3
5
36
17
17
17
16
2
15
2
2
3
3
100
100
1

borg
37
10
8
6
21
5
5
5
3
5
36
17
17
17
16
2
14
20
5
5
3
100
100
1

SMT
43
5
8
9
0
5
5
5
3
5
36
17
17
17
16
2
17
0
0
4
4
100
100
1

VBS
77
12
8
9
21
5
5
5
3
5
36
17
17
17
17
2
19
20
5
6
6
100
100
2

403

443

385

444

448

391

422

429

392

440

458

419

514

Table 6: Number problems solved different methods.
encoding n different constraints order obtain GAC, done WD-2
BDD-3, good idea terms variables clauses.
8.3 SAT vs. PB
section compare state-of-the-art solvers Pseudo-Boolean problems
encodings SAT. SAT approach, encoding done,
SAT formula given SAT Solver Lingeling (Biere, 2010) version 276.
considered SAT encodings previous section. Regarding Pseudo-Boolean
solvers, considered MiniSAT+ (Een & Sorensson, 2006) best non-parallel
solvers optimization, small integers, linear constraints category PseudoBoolean Competition: borg (Silverthorn & Miikkulainen, 2010) version pb-dec-11.04.03,
bsolo (Manquinho & Silva, 2006) version 3.2, wbo (Manquinho, Martins, & Lynce, 2010)
version 1.4 SAT4J (Berre & Parrain, 2010) version 2.2.1. also included
SMT Solver Barcelogic (Bofill, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Rubio, 2008)
PB constraints, couples SAT solver theory solver PB constraints.
Table 6 shows number instances solved method. Table 7 shows average
time spent methods. SAT encodings, times include encoding
SAT solving time. before, time limit 1800 seconds per benchmark set,
average computation, timeout counted 1800 seconds. tables include
column VBS (Virtual Best Solver), represents best solver every instance.
gives idea speedup could obtain portfolio approach.
Figure 13 shows result Bergmann-Hommel test: SMT best method,
whereas Adder, BDD-3 WD-2 worst ones. significant difference
methods. main conclusion infer BDD encodings
definitely competitive method. Also, technique outperforms others
benchmark families, hence portfolio strategies would make lot sense
476

fiA New Look BDDs Pseudo-Boolean Constraints

Family
lopes
army
blast
cache
chnl
dbstv30
dbstv40
dbstv50
dlx
elf
fpga
j30
j60
j90
j120
neos
ooo
pig-crd
pig-cl
ppp
robin
13queen
11tsp11
vdw

Adder
1,515
660
6.12
253
1,543
1,049


7.06
13.87
586
16.7
137
24.18
978
1,023
479
1,620
1,624
631
938
47.52
28.36
1,645

WD-1
1,420
139
2.56
123
1,543
128
366
935
4.88
10.14
5.27
7.79
114
36.63
854
936
190
1,620
1,715
1,001
921
1.64
8.29
1,568

WD-2
1,561
1,141
46.78
396
1,716



25.72
44.09
113
116
551
1,303
1,364
1,405
493
1,680
1,693
656
1,353
264
428.6
1,545

BDD-1
1,408
543
2.42
75.49
1,508
91.66
198
629
4.29
7.97
0.92
5.94
113
39.72
839
910
151
1,620
1,718
906
913
4.63
23.86
1,493

BDD-2
1,401
469
1.99
115
1,508
192
324
792
4.34
9
0.92
8.42
116
39.46
851
915
176
1,620
1,718
858
913
4.51
18.32
1,493

BDD-3
1,435
1,298
27.63
375
1,681



19.58
30.03
37.64
77.88
398
1,233
1,262
1,073
488
1,725
1,721
646
719
643
731
1,612

bsolo
1,509
1,028
0.12
653
0.55
59.28
187
200
3.47
28.58
0.27
6.53
110
0.9
967
1,106
645
114
1,658
605
936
54.82
855
1,478

MiniSAT
1,344
913
0.51
395
1,551
32.6
72.25
430
1.29
2.97
242
4.6
115
3.96
1,031
1,276
453
1,626
1,623
919
971
238
369
1,448

SAT4J
1,661
1,127
0.84
670
1,751
99.81
9.74
21.22
1.6
2.31
1.47
14.57
105
1.42
849
1,038
575
1,749
1,705
602
778
18.92
503
1,596

Wbo
1,364
1,084
0.08
606
1,673
1.54
5.69
16.13
0.55
1.42
5.17
0.53
101
0.41
839
901
486
1,685
1,742
901
920
5.9
229
1,441

borg
1,555
438
2.13
636
3.78
9.87
45.33
121
3.15
11.61
3.04
1.93
104
3.32
841
976
512
3.92
1,369
390
963
20.35
27.64
1,450

SMT
1,464
1,066
0.03
266

1.28
4.44
11.36
0.17
0.69
0.1
0.28
101
0.15
814
925
259


601
605
1.92
1.81
1,441

VBS
1,249
86
0.03
63.95
0.47
1.28
4.44
11.36
0.17
0.69
0.07
0.28
101
0.15
756
901
126
1.92
1,367
210
444
1.28
1.51
1,186

783

669

958

667

667

1,003

764

772

849

710

613

696

475

Av.

Table 7: Time spent different methods solving problem (in seconds).

12

11

10

9

8

7

6

5

4

WD2

3

2

SMT
BDD3

Wbo
Adder
SAT4J
MiniSAT
bsolo

borg
BDD1
BDD2
WD1

Figure 13: Statistical comparison results Table 7, runtime different methods.

477

1

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

area, witnessed performance Borg, implements approach. Finally,
also want mention possible exponential explosion BDDs rarely occurs
practice hence, coefficient decomposition seem pay practical situations.
Regarding Best Virtual Solver, SMT contributes 52% problems. 25%
cases best solution given specific PB solver. Among them, Wbo contributes
10% problems bsolo 8%. Finally, encoding methods give best
solution 23% cases: 14% times due Watchdog methods 8%
times due BDD-based methods.
8.4 Sharing
One possible advantages using ROBDDs encode Pseudo-Boolean constraints
ROBDDs allow one encode set constraints, one. would seem
natural think two constraints similar enough, two individual ROBDDs
would similar structure, merging single one would result ROBDD
whose size smaller sum two individual ROBDDs. However, main
difficulty decide constraints encoded together, since bad choice could
result ROBDD whose size larger sum ROBDDs individual
constraints.
performed initial experiments criteria similarity constraints
took account variables appeared constraints. first fixed
integer k chose constraint largest set variables. that, looked
constraint k variables appeared first constraint. next step
look another constraint k variables appeared two
previous constraints on, reaching fixpoint. Finally, selected constraints
encoded together.
tried experiment benchmarks different values k rarely gave
advantage. However, still believe could way encoding different
constraints single ROBDD, different criteria selecting constraints
studied. see possible line future research.

9. Conclusions Future Work
theoretical practical contributions made. Regarding theoretical
part, negatively answered question whether PB constraints admit polynomial BDDs citing work Hosaka et al. (1994) which, best knowledge,
largely unknown research community. Moreover, given simpler proof
assuming NP different co-NP, relates subset sum problem
ROBDDs size PB constraints.
practical level, introduced ROBDD-based polynomial generalized
arc-consistent encoding PB constraints developed BDD-based generalized arcconsistent encoding monotonic functions uses two clauses per BDD node.
also presented algorithm efficiently construct ROBDDs proved
overall method competitive practice state-of-the-art encodings tools.
future work practical level, plan study type Pseudo-Boolean
478

fiA New Look BDDs Pseudo-Boolean Constraints

constraints likely produce smaller ROBDDs encoded together rather
encoded individually.

Acknowledgments
UPC authors partially supported Spanish Min. Educ. Science
LogicTools-2 project (TIN2007-68093-C02-01). Abo also partially supported FPU
grant.

References
Abo, I., Nieuwenhuis, R., Oliveras, A., & Rodrguez-Carbonell, E. (2011). BDDs pseudoboolean constraints: revisited. Proceedings 14th international conference
Theory application satisfiability testing, SAT 11, pp. 6175, Berlin, Heidelberg.
Springer-Verlag.
Aloul, F. A., Ramani, A., Markov, I. L., & Sakallah, K. A. (2002). Generic ILP versus
specialized 0-1 ILP: update. Proceedings 2002 IEEE/ACM international
conference Computer-aided design, ICCAD 02, pp. 450457, New York, NY, USA.
ACM.
Bailleux, O., Boufkhad, Y., & Roussel, O. (2006). Translation Pseudo Boolean Constraints SAT. Journal Satisfiability, Boolean Modeling Computation, JSAT,
2 (1-4), 191200.
Bailleux, O., Boufkhad, Y., & Roussel, O. (2009). New Encodings Pseudo-Boolean Constraints CNF. Kullmann, O. (Ed.), 12th International Conference Theory
Applications Satisfiability Testing, SAT 09, Vol. 5584 Lecture Notes
Computer Science, pp. 181194. Springer.
Bartzis, C., & Bultan, T. (2003). Construction efficient bdds bounded arithmetic
constraints. Proceedings 9th international conference Tools algorithms construction analysis systems, TACAS 03, pp. 394408, Berlin,
Heidelberg. Springer-Verlag.
Bergmann, B., & Hommel, G. (1988). Improvements general multiple test procedures
redundant systems hypotheses. Bauer, P., Hommel, G., & Sonnemann,
E. (Eds.), Multiple Hypothesenprfung - Multiple Hypotheses Testing, pp. 100115.
Springer-Verlag.
Berre, D. L., & Parrain, A. (2010). Sat4j library, release 2.2. Journal Satisfiability,
Boolean Modeling Computation, JSAT, 7 (2-3), 596.
Bessiere, C., Katsirelos, G., Narodytska, N., & Walsh, T. (2009). Circuit complexity
decompositions global constraints. Proceedings 21st international jont
conference Artifical intelligence, IJCAI 09, pp. 412418, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Biere, A. (2010). Lingeling, Plingeling, PicoSAT PrecoSAT SAT Race 2010. Tech.
rep., Institute Formal Models Verification, Johannes Kepler University, Al479

fiAbo, Nieuwenhuis, Oliveras, Rodrguez-Carbonell, & Mayer-Eichberger

tenbergerstr. 69, 4040 Linz, Austria. Technical Report 10/1, August 2010, FMV
Reports Series.
Bofill, M., Nieuwenhuis, R., Oliveras, A., Rodrguez-Carbonell, E., & Rubio, A. (2008).
Barcelogic SMT Solver. Computer-aided Verification (CAV), Vol. 5123 Lecture
Notes Computer Science, pp. 294298.
Bryant, R. E. (1986). Graph-Based Algorithms Boolean Function Manipulation. IEEE
Transactions Computers, 35 (8), 677691.
Bryant, R. E., Lahiri, S. K., & Seshia, S. A. (2002). Deciding CLU Logic Formulas via
Boolean Pseudo-Boolean Encodings. Proceedings International Workshop Constraints Formal Verification, CFV 02. Associated International
Conference Principles Practice Constraint Programming (CP 02).
Een, N., & Sorensson, N. (2006). Translating Pseudo-Boolean Constraints SAT. Journal
Satisfiability, Boolean Modeling Computation, 2, 126.
Hosaka, K., Takenaga, Y., & Yajima, S. (1994). Size Ordered Binary Decision
Diagrams Representing Threshold Functions. Algorithms Computation, 5th
International Symposium, ISAAC 94, pp. 584592.
Manquinho, V. M., Martins, R., & Lynce, I. (2010). Improving Unsatisfiability-Based Algorithms Boolean Optimization. Strichman, O., & Szeider, S. (Eds.), 13th
International Conference Theory Applications Satisfiability Testing, Vol.
6175 SAT 10, pp. 181193. Springer.
Manquinho, V. M., & Silva, J. P. M. (2006). Using Cutting Planes Pseudo-Boolean
Optimization. Journal Satisfiability, Boolean Modeling Computation, JSAT,
2 (1-4), 209219.
Mayer-Eichberger, V. (2008). Towards Solving System Pseudo Boolean Constraints
Binary Decision Diagrams. Masters thesis, New University Lisbon.
Schutt, A., Feydy, T., Stuckey, P. J., & Wallace, M. G. (2009). cumulative decomposition bad sounds. Proceedings 15th international conference
Principles practice constraint programming, CP09, pp. 746761, Berlin,
Heidelberg. Springer-Verlag.
Silverthorn, B., & Miikkulainen, R. (2010). Latent class models algorithm portfolio
methods. Proceedings Twenty-Fourth AAAI Conference Artificial Intelligence.
Smaus, J. (2007). Boolean Functions Encodable Single Linear Pseudo-Boolean
Constraint. Hentenryck, P. V., & Wolsey, L. A. (Eds.), 4th International Conference
Integration AI Techniques Constraint Programming, CPAIOR
07, Vol. 4510 Lecture Notes Computer Science, pp. 288302. Springer.
Warners, J. P. (1998). Linear-Time Transformation Linear Inequalities Conjunctive
Normal Form. Information Processing Letters, 68 (2), 6369.

480

fiJournal Artificial Intelligence Research 45 (2012) 601-640

Submitted 8/12; published 12/12

Irrelevant Independent Natural Extension
Sets Desirable Gambles
Gert de Cooman

gert.decooman@ugent.be

Ghent University, SYSTeMS Research Group
Technologiepark 914, 9052 Zwijnaarde, Belgium

Enrique Miranda

mirandaenrique@uniovi.es

University Oviedo, Dept. Statistics O.R.
C-Calvo Sotelo, s/n, Oviedo 33007, Spain

Abstract
results paper add useful tools theory sets desirable gambles,
growing toolbox reasoning partial probability assessments. investigate
combine number marginal coherent sets desirable gambles joint set using
properties epistemic irrelevance independence. provide formulas smallest
joint, called independent natural extension, study main properties.
independent natural extension maximal coherent sets desirable gambles allows us
define strong product sets desirable gambles. Finally, explore easy way
generalise results also apply conditional versions epistemic irrelevance
independence. set tools easily implemented computer programs
clearly beneficial fields, like AI, clear interest coherent reasoning
uncertainty using general robust uncertainty models require full specification.

1. Introduction
reasoning decision making uncertainty, little doubt probabilities play leading part. Imprecise probability models provide well-founded extension
probabilistic reasoning, allow us deal incomplete probability assessments,
indecision robustness issues.1
Early imprecise probability models (going back to, amongst others, Bernoulli, 1713;
Boole, 1952, 1961; Koopman, 1940) centered lower upper probabilities events
propositions. later stages (see instance Smith, 1961; Williams, 1975b and,
clearest statement, Walley, 1991, Section 2.7), became apparent language
events lower probabilities lacking power expression, much
expressive theory built using random variables lower previsions (or lower expectations), instead.2 However, even though quite successful,
quite well developed, number problems lower prevision approach.
mathematical complexity fairly high, especially conditioning independence
1. get good idea field imprecise probabilities about, evolving, browse
online proceedings biennial ISIPTA conferences, found web site (www.
sipta.org) Society Imprecise Probability: Theories Applications.
2. contrast, precise probability models, expressive power probabilities expectations
same: linear prevision expectation set (bounded) real-valued maps uniquely
determined restriction events (a finitely additive probability), vice versa.
c
2012
AI Access Foundation. rights reserved.

fiDe Cooman & Miranda

enter picture. Also, coherence requirements, specify basic rules proper
inference using (conditional) lower previsions, quite cumbersome, rather harder
chop intuitive elementary building blocks precise-probabilistic counterparts, even though latter turn special instances former. Finally,
case many approaches probability, see on,
theory coherent lower previsions issues conditioning sets probability zero.
attractive solution problems offered Walley (2000), form
sets desirable gambles. Walleys work inspired earlier ideas Smith (1961)
Williams (1975b), previous work along lines also done Seidenfeld,
Schervish, Kadane (1995). approach, primitive notions probabilities
events, expectations random variables. Rather, starting point question
whether gamble, risky transaction, desirable subject, i.e. strictly preferred
zero transaction, status quo. basic belief model probability measure,
lower prevision, set desirable gambles.
Let us briefly summarise believe working sets desirable gambles
basic belief models deserves attention AI community:
Primo, number examples literature shown (Couso & Moral, 2011;
De Cooman & Quaeghebeur, 2012; Moral, 2005), shall see (look
instance Examples 1 2), working making inferences using set desirable
gambles subjects uncertainty model general expressive. also
arguably simpler elegant mathematical point view,
intuitive geometrical interpretation (Quaeghebeur, 2012b).
Secundo, shall see Sections 4 5 approach coherent marginalisation
conditioning especially straightforward, issues conditioning
sets probability zero.
Tertio, argue Section 2.3, similarity accepting
gamble one hand, accepting proposition true other, working
sets desirable gambles leads account probabilistic inference logical
flavour; see work Moral Wilson (1995) early discussion idea.
Quarto, working sets desirable gambles encompasses subsumes special
cases classical (or precise) probabilistic inference inference classical propositional logic; see Sections 2 5.
finally, quinto, made clear discussion throughout, sets desirable
gambles eminently suited dealing partial probability assessments, situations
experts express beliefs, preferences behavioural dispositions using finitely
many assessments need determine unique probability measure. particular,
discuss connection partial preferences Section 2.1.
Let us try present preliminary defense sweeping claims examples. One particular perceived disadvantage working lower previsionsor
previsions probabilities matteris conditioning lower prevision need
lead uniquely coherent results conditioning event lower upper probability
zero; see instance work Walley (1991, Section 6.4). precise probabilities,
difficulty circumvented using full conditional measures (Dubins, 1975).
already mentioned, imprecise-probabilities context, working informative coherent sets desirable gambles rather lower previsions provides
602

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

elegant intuitively appealing way problem, already suggested
Walley (1991, Section 3.8.6 Appendix F), argued much detail
later work (Walley, 2000). connection full conditional measures maximal
coherent sets desirable gambles recently explored Couso Moral (2011):
latter still general expressive.
work De Cooman Quaeghebeur (2012) shown working sets
desirable gambles especially illuminating context modelling exchangeability
assessments: exposes simple geometrical meaning notion exchangeability,
leads simple particularly elegant proof significant generalisation de
Finettis representation theorem exchangeable random variables (de Finetti, 1931).
Exchangeability structural assessment, independence, quite common
context probabilistic graphical models, Bayesian (Pearl, 1985) credal networks (Cozman, 2000). Conditioning independence are, course, closely related.
recent paper (De Cooman, Miranda, & Zaffalon, 2011), investigated notions epistemic independence finite-valued variables using coherent lower previsions, thus adding
literature assessments epistemic irrelevance independence studied
graphical models (De Cooman, Hermans, Antonucci, & Zaffalon, 2010; Destercke & De
Cooman, 2008), alternative often used notion strong independence.
above-mentioned problems conditioning, fact coherence requirements conditional lower previsions are, honest, quite cumbersome work with,
turned quite complicated exercise. reason why, present
paper, intend show looking independence using sets desirable gambles leads
elegant theory avoids complexity pitfalls working coherent lower previsions. this, build strong pioneering work epistemic
irrelevance Moral (2005). focus symmetrised notion epistemic
independence, much seen application continuation
ideas.
goal paper show local models variables, together
independence assessments, combined order produce joint model. joint
model used draw inferences, done instance context Bayesian
credal networks (Antonucci, de Campos, & Zaffalon, 2012; Cozman, 2000; Pearl, 1985).
One core ideas probabilistic graphical models provide representation
joint model less taxing computational point view.
three main novelties approach: first allow imprecision
local modelsalthough precise models particular case; second
model local probability assessments means sets desirable gambles,
above-mentioned advantages possess coherent lower previsions; third
stress epistemic irrelevance independence rather common
assessment strong independence, reasons become clear onalthough
also discuss strong independence.
results paper adding useful tools growing toolbox
reasoning partial probability assessments sets desirable gambles constitute,
something already started work exchangeability (De Cooman & Quaeghebeur,
2012) work epistemic irrelevance credal networks Moral (2005).
regard, also interesting mention algorithms making inferences sets
603

fiDe Cooman & Miranda

desirable gambles recently established (Couso & Moral, 2011; Quaeghebeur,
2012a). set tools easily implemented computer programs
clearly beneficial field like AI, surely interested coherent reasoning
uncertainty general robust uncertainty models require full specification. paper constitutes step direction, also allows us
see clearly main difficulties faced working sets desirable
gambles. remain, however, number important situations dealt with,
future lines research discussed number places paper, well
Conclusion.
Section 2 summarise relevant results existing theory sets desirable
gambles. mentioning useful notational conventions Section 3, recall basic
marginalisation, conditioning extension operations sets desirable gambles Sections 4 5. use combine number marginal sets desirable gambles
joint satisfying epistemic irrelevance (Section 6), epistemic independence (Section 7).
Section 8, study particular case maximal coherent sets desirable gambles,
derive concept strong product. Section 9 deals conditional independence
assessments.

2. Coherent Sets Desirable Gambles Natural Extension
Let us begin explaining basic uncertainty models, coherent sets desirable
gambles, (more details found Augustin, Coolen, De Cooman, & Troffaes,
2012; Couso & Moral, 2011; De Cooman & Quaeghebeur, 2012; Moral, 2005; Walley, 2000).
Consider variable X taking values possibility space X, assume
paper finite.3 model information X means sets desirable gambles.
gamble real-valued function X, denote set gambles X G(X ).
linear space point-wise addition gambles, point-wise multiplication
gambles real numbers. subset G(X ), denote posi(A) set
positive linear combinations gambles A:
posi(A) :=

X
n
k=1


k fk : fk A, k > 0, n > 0 .

call convex cone closed positive linear combinations, meaning
posi(A) = A.
two gambles f g X, write f g (x X )f (x) g(x),
f > g f g f 6= g. gamble f > 0 called positive. gamble g 0 called
non-positive. G(X )6=0 denotes set non-zero gambles, G(X )>0 convex cone
positive gambles, G(X )0 convex cone non-positive gambles.
3. results section remain valid working general, possibly infinite, possibility
spaces, case gambles assumed bounded real-valued functions. make finiteness assumption avoid deal controversial issue conglomerability (Miranda,
Zaffalon, & De Cooman, 2012; Walley, 1991), make discussion independence
later sections significantly easier, practically implementable inference systems AI
finitary case.

604

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

2.1 Coherence Avoiding Non-positivity
Definition 1 (Avoiding non-positivity coherence). say set desirable gambles G(X ) avoids non-positivity f 6 0 gambles f posi(D),
words G(X)0 posi(D) = . called coherent satisfies following
requirements:
D1. 0
/ D;
D2. G(X )>0 D;
D3. = posi(D).
denote D(X) set coherent sets desirable gambles X.
Requirement D3 turns convex cone. Due D2, includes G(X )>0 ; due D1D3,
excludes G(X)0 , therefore avoids non-positivity:
D4. f 0 f
/ D, equivalently G(X )0 = .
set G(X )>0 coherent, smallest subset G(X ). set represents
minimal commitments part subject, sense knows nothing
likelihood different outcomes prefer zero gambles
sure never decrease wealth possibility increasing it. Hence,
usually taken model complete ignorance, called vacuous model.
One interesting feature coherent sets desirable gambles linked
field decision making incomplete preferences (Aumann, 1962; Dubra, Maccheroni,
& Ok, 2004; Shapley & Baucells, 1998), formally equivalent strict
versions partial preference orderings (Buehler, 1976; Giron & Rios, 1980). Given
coherent set desirable gambles D, define strict preference relation
gambles
f g f g gambles f g G(X ).
Indeed, due linearity utility scale, exchanging gamble g gamble f
transaction reward function f g, strictly preferring f g means
exchange strictly preferred status quo (zero). relation satisfies
following conditions:
SP1. f f f G(X )

[irreflexivity]

SP2. f > g f g f, g G(X )

[monotonicity]

SP3. f g g h f h f, g, h G(X )

[transitivity]

SP4. f g f + (1 )h g + (1 )h (0, 1] f, g, h G(X ) [mixture
independence]
Conversely, preference relation satisfying axioms determines coherent set
desirable gambles. Partial preference orderings provide foundation general decision
theory imprecise probabilities imprecise utilities (Fishburn, 1975; Seidenfeld et al.,
1995; Seidenfeld, Schervish, & Kadane, 2010). See also work Moral Wilson
(1995), Walley (1991, 2000) Quaeghebeur (2012b, Section 2.4) information.
605

fiDe Cooman & Miranda

2.2 Natural Extension
consider anyTnon-empty family coherent sets desirable gambles Di , I,
intersection iI Di still coherent. idea behind following result,
brings fore notion coherent inference. subject gives us assessment,
set G(X ) gambles X finds desirable, tells us exactly
assessment extended coherent set desirable gambles, construct
smallest set.
Theorem 1 (De Cooman & Quaeghebeur, 2012). Consider G(X ), define
natural extension by:4
\
E(A) :=
{D D(X) : D} .

following statements equivalent:
(i) avoids non-positivity;

(ii) included coherent set desirable gambles;
(iii) E(A) 6= G(X );
(iv) set desirable gambles E(A) coherent;
(v) E(A) smallest coherent set desirable gambles includes A.
(and hence all) equivalent statements hold,

E(A) = posi G(X )>0 .

shows assessment finite description, represent
natural extension computer storing finite description extreme rays.
Although general assessments need finite description (for instance
considered Eq. (3) need one), interest
vast range practical situations. description many cases partial
probability assessments given finite description, efficient algorithms
verifying coherence computing natural extension set gambles, refer
work Couso Moral (2011) Quaeghebeur (2012a).
2.3 Connection Classical Propositional Logic
definition coherent set desirable gambles, Theorem 1, make clear inference desirable gambles bears formal resemblance deduction classical proposition
logic: D3 production axiom states positive linear combinations desirable
gambles desirable. exact correspondences listed following table:
Classical propositional logic
logical consistency
deductively closed
deductive closure
4. usual, expression, let



= G(X ).

606

Sets desirable gambles
avoiding non-positivity
coherent
natural extension

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

shall see inference sets desirable gambles (precise-)probabilistic
inference, particular Bayess Rule, special case. easy see also
generalises (includes special case) classical propositional logic: proposition p
identified subset Ap Stone space X , accepting proposition p corresponds
judging gamble IAp 1 + desirable > 0.5 IAp so-called
indicator (gamble) Ap , assuming value 1 Ap 0 elsewhere. See work De
Cooman (2005) detailed discussion.
2.4 Helpful Lemmas
order prove number results paper, need following lemmas, one
convenient version separating hyperplane theorem. rely heavily
assumption finite space X , easily extended general case.
Lemma 2. Assume X finite, consider finite subset G(X ). 0
/
posi(G(X )>0 A) ifPand probability mass function p p(x) > 0
x X xX p(x)f (x) > 0 f A.

Proof. clearly suffices prove necessity. Since 0
/ posi(G(X )>0 A), infer
version separating hyperplane theorem (Walley, 1991, Appendix E.1)
linear functional G(X )
(x X)(I{x} ) > 0 (f A)(f ) > 0.

P
(X ) = xX (I{x} ) > 0, let p(x) := (I{x} )/(X
P ) > 0 x X,
p probability mass function X (f )/(X ) = xX p(x)f (x) > 0
f A.
second lemma implies consider coherent set desirable gambles
include gamble opposite, always find coherent superset
includes one two:
Lemma 3. Consider convex cone gambles X max f > 0 f A.
Consider non-zero gamble g X. g
/ 0
/ posi(A {g}).
Proof. Consider non-zero gamble g
/ A, assume ex absurdo 0 posi(A {g}).
follows assumptions f > 0 0 =
f + (g). Hence g A, contradiction.
2.5 Maximal Coherent Sets Desirable Gambles
element D(X) called maximal strictly included element
D(X), words, adding gamble f makes sure longer extend
set {f } set still coherent:
(D D(X))(D = ).
5. equivalent judging gamble IAp 1 desirable, case obtain
coherent set desirable gambles; gamble IAp 1 almost-desirable sense Walley
(1991, Section 3.7.3).

607

fiDe Cooman & Miranda

M(X ) denotes set maximal elements D(X).
following proposition provides useful characterisation maximal elements.
Proposition 4 (De Cooman & Quaeghebeur, 2012). Consider D(X ).
maximal coherent set desirable gambles
(f G(X )6=0 )(f
/ f D).
case classical propositional logic (see, instance, De Cooman, 2005),
coherence inference described completely terms maximal elements.
essence following important result, continues hold infinite X,
constructive proof given case X finite, based argument
suggested Couso Moral (2011).
Theorem 5 (Couso & Moral, 2011; De Cooman & Quaeghebeur, 2012). set
avoids non-positivity maximal M(X ) M.
Moreover
\
E(A) =
m(A),
let

m(A) := {M M(X ) : M} .

(1)

shows (coherent) sets desirable gambles instances so-called strong
belief structures described studied detail De Cooman (2005), strong
belief structures classical propositional logic embedded. guarantees amongst
things AGM-like (De Cooman, 2005; Gardenfors, 1988) account belief
expansion revision possible objects.
2.6 Coherent Lower Previsions
conclude section shedding light connection coherent sets
desirable gambles, coherent lower previsions, probabilities.
Given coherent set desirable gambles D, functional P defined G(X )
P (f ) := sup { : f D}

(2)

coherent lower prevision (Walley, 1991, Thm. 3.8.1), is, corresponds taking
lower envelope expectations associated set finitely additive probabilities.
conjugate upper prevision P defined P (f ) := inf { : f D} = P (f ).
Many different coherent sets desirable gambles induce coherent lower prevision P , typically differ boundaries. sense, say
sets desirable gambles informative coherent lower previsions: although
gamble positive lower prevision always desirable one negative lower
prevision desirable, lower prevision generally provide information
desirability gamble whose lower prevision equal zero. reason
need consider sets desirable gambles want additional information.
see clearly, consider following adaptation example Moral (2005,
Example 1):
608

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

Example 1. Consider X 1 = X 2 = {a, b}, let P coherent lower prevision
G(X 1 X 2 ) given


f (b, a) + f (b, b) f (b, a) + 3f (b, b)
:=
P (f )
,
min
gambles f X 1 X 2 .
2
4
coherent lower prevision induced following coherent sets desirable
gambles means Eq. (2):
:= {f : f > 0 P (f ) > 0}
:= {f : f (b, a) = f (b, b) = 0 f (a, a) + f (a, b) > 0} .
However, two sets encode different preferences, gamble g given g(a, a) = 2,
g(a, b) = 1, g(b, a) = g(b, b) = 0, P (g) = 0, considered desirable
D. coherent lower previsions able distinguish preferences
weak preferences, sets desirable gambles can. shall see Section 5
differences come play considering conditioning.
smallest set desirable gambles induces given coherent lower previsionan
open coneis called associated set strictly desirable gambles, given
:= {f G(X ) : f > 0 P (f ) > 0} .

(3)

instance case set Example 1. Sets strictly desirable gambles
one-to-one relationship coherent lower previsions, suffer
problems conditioning sets (lower) probability zero, sense
conditional models determine caseby means Eqs. (8) (10)
Section 5are always vacuous (Zaffalon & Miranda, 2012; Quaeghebeur, 2012b).
one reasons paper considering general model coherent
sets (not necessarily strictly) desirable gambles. additional discussion
sets desirable gambles informative coherent lower previsions, refer
Walley (2000) Quaeghebeur (2012b).
lower upper prevision coincide gambles, functional
P defined G(X ) P(f ) := P (f ) = P (f ) f G(X ) linear prevision, i.e.,
corresponds expectation operator respect finitely additive probability.
happens particular maximal coherent set desirable gambles M:
P (f ) = sup { : f M} = inf { : f
/ M} = inf { : f M} = P (f );
see second equality holds, observe f also f
< , consequence set { : f M} interval unbounded
below. third equality follows Proposition 4. Thus, boundary behaviour,
precise probability models correspond maximal coherent sets desirable gambles; see
work Couso Moral (2011, Section 5), Miranda Zaffalon (2010, Proposition 6)
Williams (1975a) information. Moreover, coherent lower prevision P
lower envelope credal set M(P ) induces, given
M(P ) := {P linear prevision : (f G(X ))P(f ) P (f )} .
conclude point least basic representational aspects, models
involving coherent sets desirable gambles generalise classical propositional logic
precise probability finitary approach championed de Finetti (1937, 1975).
609

fiDe Cooman & Miranda

3. Basic Notation
highlighted basic facts general approach uncertainty modelling, ready turn independence. order talk this, need
able deal models involving one variable. present section,
introduce notational devices use make discussion elegant possible.
on, consider number variables Xn , n N , taking values
respective finite sets X n . N finite non-empty index set.6
every subset R N , denote XR tuple variables (with one component
:= rR X r . Cartesian
r R) takes values Cartesian
product X R
product set maps xR R rR X r xr := xR (r) X r
r R. Elements X R generically denoted xR zR , corresponding components
xr := xR (r) zr := zR (r), r R.
assume variables Xn logically independent, means
subset R N , XR may assume values X R .
denote G(X R ) set gambles defined X R . frequently resort
simplifying device identifying gamble X R gamble X N , namely
cylindrical extension. give example, K G(X N ), trick allows us consider
KG(X R ) set gambles K depend variable XR . another
example, device allows us identify gambles I{xR } I{xR }X N\R , therefore
also events {xR } {xR } X N \R . generally, event X R ,
identify gambles IA IAX N\R , therefore also events X N \R .
must paySparticular attention case R = . definition, X set
maps r X r = . contains one element x : empty map. means
uncertainty value variable X : assume one
value (the empty map). Moreover IX = I{x } = 1. Also, identify G(X )
set real numbers R. one coherent set desirable gambles X : set
R>0 positive real numbers.
One final notational convention handy used throughout: n
index, identify n {n}. take X {n} , G(X {n} ), D{n} also refer
X n , G(X n ) Dn , respectively. trick, amongst things, allows us consider
two disjoint index sets N1 N2 , consider sets constitute index
themselves, leading new index set {N1 , N2 }. variables XN1 XN2
combined joint variable X{N1 ,N2 } , course identified variable
XN1 N2 : joint variables considered single variables, combined constitute
new joint variables.

4. Marginalisation Cylindrical Extension
Suppose set DN G(X N ) desirable gambles modelling subjects information uncertain variable XN .
6. assumption finiteness spaces X n essential proofs results established
later on, Theorem 13 Proposition 18. also allows us simplify expressions
sets gambles derived assumption epistemic irrelevance independence,
derive instance Lemma 11 Proposition 14.

610

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

interested modelling information variable XO ,
subset N . done using set desirable gambles belong DN
depend variable XO :
margO (DN ) := {g G(X ) : g DN } = DN G(X ).

(4)

Observe DN coherent obtain marg (DN ) = G(X )>0 , identified
set positive real numbers R>0 . Also, O1 O2 N :


margO1 (margO2 (DN )) = g G(X O1 ) : g margO2 (DN )
= {g G(X O1 ) : g G(X O2 ) DN }

= {g G(X O1 ) : g DN } = margO1 (DN ).

(5)

Coherence trivially preserved marginalisation.
Proposition 6. Let DN set desirable gambles X N , consider subset
N .
(i) DN avoids non-positivity, margO (DN ).
(ii) DN coherent, margO (DN ) coherent set desirable gambles X .
look kind inverse operation marginalisation. Suppose
coherent set desirable gambles G(X ) modelling subjects information
uncertain variable XO , want extend coherent set desirable gambles
X N , representing information. looking coherent set desirable
gambles DN G(X N ) margO (DN ) = small possible:
conservative coherent set desirable gambles X N marginalises .
turns set always exists difficult find.
Proposition 7. Let subset N let D(X ). conservative
(smallest) coherent set desirable gambles X N marginalises given
extN (DO ) := posi(G(X N )>0 ).

(6)

called cylindrical extension set desirable gambles X N , clearly
satisfies
margO (extN (DO )) = .
(7)
extension called weak extension Moral (2005, Section 2.1).7
Proof. clear coherence requirements Eq. (4) coherent set
desirable gambles marginalises must include G(X N )>0 , therefore
also posi(G(X N )>0 ) = extN (DO ). therefore suffices prove posi(G(X N )>0 )
coherent, marginalises .
7. main difference result Morals excluding zero gamble
coherent set desirable gambles, Moral including it.

611

fiDe Cooman & Miranda

prove coherence, suffices prove avoids non-positivity, Theorem 1.
obvious coherent set desirable gambles X .
left prove margO (extN (DO )) = . Since g obvious
g extN (DO ) g G(X ), see immediately margO (extN (DO )),
concentrate proving converse inclusion. Consider f margO (extN (DO )),
meaning f G(X ) f extN (DO ). latter means g ,
h G(X N )>0 , non-negative max{, } > 0 f = g + h.
Since need prove f , assume without loss generality > 0.
h = (f g)/ G(X ) therefore also h G(X )>0 , whence indeed f ,
coherence .

5. Conditioning
Suppose set DN G(X N ) desirable gambles modelling subjects information uncertain variable XN .
Consider subset N , assume want update model DN
information XI = xI . leads updated, conditioned, set desirable gambles:


DN |xI := f G(X N ) : f > 0 I{xI } f DN .
(8)

technical reasons, mainly order streamline proofs much possible,
also allow admittedly pathological case = . Since I{x } = 1, amounts
conditioning all.
Eq. (8) introduces conditioning operator | essentially used Walley (2000)
Moral (2005). prefer slightly modified version , introduced De Cooman
Quaeghebeur (2012). Since I{xI } f = I{xI } f (xI , ), characterise updated model
DN |xI set


DN xI := g G(X N \I ) : I{xI } g DN G(X N \I ),
specific sense g G(X N \I ):

g DN xI I{xI } g DN I{xI } g DN |xI ,

(9)

f G(X N ):
f DN |xI (f > 0 f (xI , ) DN xI ).
equation shows, one-to-one correspondence DN |xI
DN xI . prefer second operator find intuitive conditioning
gamble xI X produces gamble depends remaining N \
variables. useful instance combining conditional sets gambles,
Proposition 24 later on.
immediate prove conditioning preserves coherence:
Proposition 8. Let DN coherent set desirable gambles X N , consider
subset N . DN xI coherent set desirable gambles X N \I .
612

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

order marginalisation conditioning reversed, conditions:
Proposition 9. Let DN coherent set desirable gambles X N , consider
disjoint subsets N . xI X :
margO (DN xI ) = margIO (DN )xI .
Proof. Consider h G(X N ) observe following chain equivalences:
h margO (DN xI ) h G(X ) h DN xI
h G(X ) I{xI } h DN
h G(X ) I{xI } h margIO (DN )
h G(X ) h margIO (DN )xI
h margIO (DN )xI .
end section, let us briefly look consequences type updating
coherent lower previsions associated coherent sets desirable gambles.
allow us back claim standard probability theory recovered
special case theory coherent sets desirable gambles, also allows us derive
Bayess Rule.
Let us denote P N lower prevision induced joint model DN , P (|xI )
conditional lower prevision G(X N \I ) induced updated set DN xI .
gamble g X N \I :


P (g|xI ) = sup { : g DN xI } = sup : I{xI } [g ] DN .
(10)

allows us clarify sets desirable gambles indeed informative coherent lower previsions, using example Moral (2005):
Example 2. Consider lower prevision P coherent sets desirable gambles Example 1. Consider event X1 = a, (upper) probability zero . conditioning event, obtain two different
updated sets: one hand,
D(X1 = a) = {g G(X 2 ) : g > 0} = G(X 2 )>0
whereas
(X1 = a) = {g G(X 2 ) : g(a) + g(b) > 0} .
means sets represent different information conditioning event
probability zero X1 = a. Indeed, apply Eq. (10) see first one induces
vacuous lower prevision P (g|X1 = a) = min{g(a), g(b)} gamble g X 2 ,
second one induces uniform linear prevision: P (g|X1 = a) = g(a)+g(b)
.
2
lower previsions P N P (|xI ) satisfy condition called Generalised
Bayes Rule (this follows Williams, 1975b Miranda & Zaffalon, 2010, Thm. 8):
P N (I{xI } [g P (g|xI )]) = 0.
613

(11)

fiDe Cooman & Miranda

refer work Walley (1991, 2000) information rule. leads
Bayess Rule special case joint model DN induces precise prevision PN .
Indeed, let g = I{xN\I } generically denote probability mass p, infer
Eq. (11) linearity PN PN (I{xI } I{xN\I } ) = P (I{xN\I } |xI )PN (I{xI } ),
words p(xN ) = p(xN \I |xI )p(xI ). See Section 2.6 details relationship
coherent lower (and linear) previsions sets desirable gambles.
Remark 1. lower prevision P also one-to-one correspondence so-called
set almost desirable gambles, namely
:= {f : P (f ) 0} .
set corresponds uniform closure coherent set desirable gambles
induces P means Eq. (2). Although sets almost-desirable gambles interesting,
allow us work non-strict preference relations (Walley, 1991, Section 3.7.6),
opted considering general model coherent sets desirable gambles two
(admittedly related) reasons. Like sets strictly desirable gambles, sets almost-desirable
gambles permit elicit boundary behaviour, may important updating, discussed Example 2. Moreover, conditioning set almost desirable
gambles may produce incoherent models sets probability zero involved (Miranda
& Zaffalon, 2010, Proposition 5 Example 2): take instance X 1 = X 2 = {0, 1}
linear prevision P mass function p(0, ) = 0 p(, 1) = 12 , associated
set almost desirable gambles is:
= {f : f (1, 0) + f (1, 1) 0} ,
use Eq. (8) condition set X1 = 0, get G(X 1,2 ),
incoherent set. means sets almost desirable gambles, generally
sets gambles associated non-strict preferences, conditioning operation must
modified order avoid producing incoherences. turns unique way
this; see work Hermans (2012) details.

6. Irrelevant Natural Extension
ready look simplest type irrelevance judgement.
Definition 2. Consider two disjoint subsets N . say XI epistemically
irrelevant XO learning value XI influence change subjects
beliefs XO .
set DN desirable gambles X N capture type epistemic irrelevance? Observing XI = xI turns DN updated set DN xI desirable gambles
X N \I , come following definition:
Definition 3. set DN desirable gambles X N said satisfy epistemic irrelevance
XI XO
margO (DN xI ) = margO (DN ) xI X .
614

(12)

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

before, technical reasons also allow empty. clear
definition variable X , whose constant value certain,
epistemically irrelevant variable XO . Similarly, see variable XI
epistemically irrelevant variable X . seems accordance intuition.
refer Levi (1980) Walley (1982, 1991) related notions terms coherent
lower previsions credal sets.
epistemic irrelevance condition reformulated trivially interesting
slightly different manner.
Proposition 10. Let DN coherent set desirable gambles X N , let
disjoint subsets N . following statements equivalent:
(i) margO (DN xI ) = margO (DN ) xI X ;
(ii) f G(X ) xI X : f DN I{xI } f DN .
Proof. suffices take account f margO (DN ) f DN
f G(X ), f margO (DN xI ) f G(X ) I{xI } f DN .
Irrelevance assessments useful constructing sets desirable gambles
ones. Suppose coherent set desirable gambles X , assessment XI epistemically irrelevant XO , disjoint index sets.
combine structural irrelevance assessment coherent set
desirable gambles X IO , generally, X N , N O? see
done way conservative possible, introduce following sets


Airr
I{xI } g : g xI X
(13)
IO := posi
= {h G(X IO )6=0 : (xI X )h(xI , ) {0}} .

(14)

irr
Clearly, quite important streamlining proofs, Airr
= AI =
G(X )>0 . intuition behind Eq. (13) consider cylindrical extensions
gambles space X IO , take natural extension resulting set.
alternative expression (14) shows equivalent selecting gamble
finite number xI X , derive gamble X IO .
Let us give two important properties sets:

Lemma 11. Consider disjoint subsets N , coherent set desirable
gambles X . Airr
IO coherent set desirable gambles X IO .
Proof.PD1. Assume ex absurdo n > 0, real k > 0 fk Airr
IO
nk=1 k fk = 0. follows assumptions



{1,
.
.
. , n}
Pn
xI X f (xI , ) 6= 0. implies sum k=1 k fk (xI , ) = 0
gambles k fk (xI , ) zero. Since non-zero ones belong , contradicts
coherence .
D2. Consider h G(X IO )>0 . clearly h(xI , ) 0 therefore h(xI , )
{0} xI X . Since h 6= 0, follows indeed h Airr
IO .
D3. Trivial, using posi(posi(D)) = posi(D) set desirable gambles D.
615

fiDe Cooman & Miranda

Lemma 12. Consider disjoint subsets N , coherent set desirable
gambles X . margO (Airr
IO ) = .
Proof. obvious Eq. (14) indeed:
irr
margO (Airr
IO ) = AIO G(X ) = {h G(X )6=0 : (xI X )h {0}}

= {h G(X )6=0 : h {0}} = .
Theorem 13. Consider disjoint subsets N , coherent set desirable gambles X . smallest coherent set desirable gambles X N
marginalises satisfies epistemic irrelevance condition (12) XI XO
irr
given extN (Airr
IO ) = posi(G(X N )>0 AIO ).
Proof. Consider coherent set DN desirable gambles X N marginalises
satisfies irrelevance condition (12). implies margO (DN xI ) =
xI X , g DN xI , therefore I{xI } g DN g , Eq. (9). infer
irr
coherence Airr
IO DN , therefore also posi(G(X N )>0 AIO ) DN .
consequence, suffices prove (i) extN (Airr
IO ) coherent, (ii) marginalises ,
(iii) satisfies epistemic irrelevance condition (12). set
do.
(i). Lemma 11, Airr
IO coherent set desirable gambles X IO , Proposition 7
irr
implies posi(G(X N )>0 Airr
IO ) = extN (AIO ) coherent set desirable gambles
X N .
(ii). Marginalisation leads to:
irr
irr
margO (extN (Airr
IO )) = margO (margIO (extN (AIO ))) = margO (AIO ) = ,

first equality follows Eq. (5), second Eq. (7), third
Lemma 12.
(iii). follows Proposition 9 Eq. (7)
irr
irr
margO (extN (Airr
IO )xI ) = margIO (extN (AIO ))xI = AIO xI ,

shown (ii) margO (extN (Airr
IO )) = , proving equality
irr )) amounts proving Airr x = .
margO (extN (Airr
)x
)
=
marg
(ext
(A

N


IO
IO
IO
irr x , concentrate
obvious definition Airr






IO
IO
irr
converse inclusion. Consider h Airr
IO xI ; I{xI } h AIO , infer
Eq. (14) particular h {0}. since Airr
IO coherent Lemma 11, see
h 6= 0 therefore indeed h .
Theorem 13 mentioned briefly, hint proof, Moral (2005, Section 2.4).
believe result trivial therefore decided include version
proof here. notion epistemic irrelevance called weak epistemic irrelevance
Moral. version epistemic irrelevance requires addition DN
equal irrelevant natural extension , therefore smallest model
satisfies (weak) epistemic irrelevance condition (12). feel comfortable
616

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

reasons so, decided follow lead this. main reason
tied philosophy behind partial assessments (or probability
specifications). assessment, local (e.g. stating gambles set
desirable) structural (e.g. imposing symmetry irrelevance), serves
restrict possible models, stage conservative (smallest possible)
model considered one used, possibly refined additional
assessments. calling model irrelevant smallest weakly irrelevant model
would, believe, conflict approach: larger models obtained later adding, say,
symmetry assessments, would longer deserve called irrelevant (but would
still satisfy relevant conditions).
infer Theorem 13 Eq. (13) extreme rays irrelevant natural
extension form I{xI } g, g extreme ray , representing
finding extension computer computational complexity linear
number extreme rays linear number elements product set X
therefore essentially exponential number |I| irrelevant variables Xi , I.
generally, also case fairly general situation generated
finite number so-called generalised extreme rays, described detail Couso
Moral (2011, Section 4) Quaeghebeur (2012a, Section 3).

7. Independent Natural Extension
turn independence assessments, constitute symmetrisation irrelevance
assessments.
Definition 4. say variables Xn , n N epistemically independent
learning values number influence change beliefs
remaining ones: two disjoint subsets N , XI epistemically irrelevant
XO .
set DN desirable gambles X N capture type epistemic independence?
Definition 5. coherent set DN desirable gambles X N called independent
margO (DN xI ) = margO (DN ) disjoint subsets N , xI X .
definition, allow empty too, lead
substantive requirement, condition margO (DN xI ) = margO (DN ) trivially
satisfied empty.
Independent sets interesting factorisation property, means product
two desirable gambles depend different variables desirable, provided one gambles positive; refer work De Cooman et al. (2011) another paper factorisation considered somewhat unusual form. Factorisation
follows characterisation epistemic irrelevance given Proposition 10
properties coherence.

617

fiDe Cooman & Miranda

Proposition 14 (Factorisation independent sets). Let DN independent coherent set desirable gambles X N . disjoint subsets N
f G(X ):
f DN (g G(X )>0 )(f g DN ).
(15)
Proof. Fix arbitrary disjoint subsets N f G(X ); show
Eq. (15) holds. part trivial. part, assume P
f DN consider
g G(X
)
.



show

f
g


.
Since
g
=
N
xI X I{xI } g(xI ), see
P >0
f g = xI X g(xI )I{xI } f . since f margO (DN ), infer independence
DN Proposition 10 f DN xI therefore I{xI } f DN xI X .
conclude f g positive linear combination elements I{xI } f DN , therefore
belongs DN coherence.
Independence assessments useful constructing joint sets desirable gambles
marginal ones. Suppose coherent sets Dn desirable gambles X n , n N
assessment variables Xn , n N epistemically independent.
combine Dn structural independence assessment coherent set
desirable gambles X N way conservative possible? call independent
product Dn independent DN D(X N ) marginalises Dn n N ,
means looking smallest independent product.
on, going prove smallest independent product always exists.
elegantly, however, need preparatory work involving
particular sets desirable gambles constructed Dn . Consider,
special case Eq. (14), subset N N \ I:


I{xI } g : g xI X
(16)
Airr
I{o} := posi


= h G(X I{o} )6=0 : (xI X )h(xI , ) {0} ,
(17)
use sets construct following set gambles X N :


[

[
irr
nN Dn := posi G(X N )>0
Airr
=
posi

N \{n}{n}
N \{n}{n} ,
nN

(18)

nN

second equality holds set G(X N )>0 included Airr
N \{n}{n} every
n N . set nN Dn gathers subsets G(X N ) derive different Dn
means assumption epistemic irrelevance, considers natural extension
union, minimal coherent superset (we shall show indeed coherent
Proposition 15 below). Observe that, quite trivially, Airr
{n}\{n}{n} = Dn therefore
m{n} Dm = Dn . prove number important properties nN Dn .
Proposition 15 (Coherence). Let Dn coherent sets desirable gambles X n , n
N . nN Dn coherent set desirable gambles X N .

Proof. Let, ease notation, := nN Airr
N \{n}{n} . follows Theorem 1
prove avoids non-positivity. consider f posi(AN ),
assume ex absurdo f 0. n 0 fn Airr
N \{n}{n}
618

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

P
irr
f =
nN n fn maxnN n > 0 [recall \{n}{n} convex cones,
Lemma 11]. Fix arbitrary N . Let



:= fm (xN \{m} , ) : xN \{m} X N \{m} , fm (xN \{m} , ) 6= 0 ,
follows Eq. (17)
finite non-empty subset Dm , coherence
Dm , Theorem 1 Lemma 2 imply mass function pm X
expectation operator Em (xm X )pm (xm ) > 0
(xN \{m} X N \{m} )(fm (xN \{m} , ) 6= 0 Em (fm (xN \{m} , )) > 0).
define gamble gN \{m} X N \{m} letting
gN \{m} (xN \{m} ) := Em (fm (xN \{m} , ))
xN \{m} X N \{m} , gN \{m} > 0.
Since weQcan N , define mass function pN X N letting
pN (xN ) := mN pm (xm ) > 0 xN X N . corresponding expectation operator
EN course product operator marginals
Em . thenPit follows
P
reasoning assumptions EN (f ) = mN EN (fm ) = mN EN (gm ) >
0, whereas f 0 leads us conclude EN (f ) 0, contradiction.
Lemma 16. Consider disjoint subsets I, R N \ (I R). f (xR , )
irr
Airr
I{o} {0} f AIR{o} xR X R .
:= f (xR , ) X I{o} .
Proof. Fix f Airr
IR{o} xR X R consider gamble g
follows assumptions xI X :
g(xI , ) = f (xR , xI , ) {0},
whence indeed g Airr
I{o} {0}.
Proposition 17 (Marginalisation). Consider coherent marginal sets desirable gambles Dn n N . Let R subset N , margR (nN Dn ) = rR Dr .
Proof. Since interpreting gambles X R special gambles X N , clear
irr
Eq. (17) r R, Airr
R\{r}{r} \{r}{r} . Eqs. (6) (18) tell us
extN (rR Dr ) nN Dn . invoke Eq. (7), leads
rR Dr = margR (extN (rR Dr )) margR (nN Dn ),
concentrate converse inequality.
Consider therefore f margR (nN Dn ) = (nN Dn ) G(X R ), assume ex
absurdo f
/ rR Dr .
follows coherence nN Dn f 6= 0 [see Proposition 15]. Since f
nN Dn , N , fs Airr
N \{s}{s} , g G(X N ) g 0
P
f = g + sS fs . Clearly \ R 6= , \ R = would imply that, xN \R
619

fiDe Cooman & Miranda

P
element X N \R , f = f (xN \R , ) = g(xN \R , ) + sSR fs (xN \R , ) rR Dr , since
infer Lemma 16 fs (xN \R , ) Airr
R\{s}{s} {0} R.
follows coherence rR Dr [Proposition 15], f
/ rR Dr Lemma 3
0
/ posi({f } rR Dr ). Let, ease notation,



SR := fs (zN \R , ) : R, zN \R X N \R , fs (zN \R , ) 6= 0 .


SR clearly finite subset rR Dr [to see this, use similar argument above,
involving Lemma 16], infer Lemma 2 mass function pR
X R associated expectation operator ER


(xR X R )pR (xR ) > 0
(s R)(zN \R X N \R )ER (fs (zN \R , )) 0


ER (f ) < 0.
P
P
infer f = f (zN \R , ) = g(zN \R , ) + sSR fs (zN \R , ) + sS\R fs (zN \R , )
zN \R X N \R :
0 > ER (f ) ER (g(zN \R , ))

X

ER (fs (zN \R , ))

sSR

=

X

sS\R

ER (fs (zN \R , )) =

X

X

pR (xR )fs (zN \R , xR ).

sS\R xR X R

gambles fs (, xR ) X N \R [where xR X R \ R] clearly zero.
non-zero ones belong sN \R Ds N \ R xR X R , Lemma 16,
coherence set desirable gambles
sN \R Ds [Proposition 15] guarantees
P
P
positive linear combination h := sS\R xR X R pR (xR )fs (, xR ) also belongs
sN \R Ds . contradicts h 0. Hence indeed f rR Dr .
Proposition 18 (Conditioning). Consider coherent marginal sets desirable gambles
Dn n N , define nN Dn means Eq. (18). nN Dn independent:
disjoint subsets N , xI X ,
margO (nN Dn xI ) = margO (nN Dn ) = oO .
could probably proved indirectly using semi-graphoid properties conditional epistemic irrelevance, proved Moral (2005); appears need reverse weak
union, reverse decomposition, contraction. give direct proof. Proposition 17
also seen special case present result = .
Proof. Fix arbitrary disjoint subsets N , arbitrary xI X . second
equality follows Proposition 17, concentrate proving margO (nN Dn xI )
coincides oO . proof similar Proposition 17.
first show oO nN Dn xI . Consider gamble f oO ,
show I{xI } f nN Dn . assumption, non-negative reals
P
, gambles fo Airr






g

G(X
)


f
=
g
+
>0
oO fo
O\{o}{o}
620

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

max{, maxoO } > 0. Fix let fo := I{xI } fo G(X N ). follows

definition Airr
O\{o}{o} fo (zN \{o} , ) = I{xI } (zI )fo (zO\{o} , ) {0}

irr
zN \{o} X N \{o} . Since fo 6= 0, definition Airr
N \{o}{o} tells us fo \{o}{o} .
:= I{xI } g G(X N ), g > 0. follows Eq. (18)
Similarly, let g P
indeed I{xI } f = g + oO fo nN Dn .
turn converse inclusion, nN Dn xI oO . Consider gamble
f G(X ) I{xI } f belongs nN Dn assume ex absurdo f
/ oO .
Let, sake notational simplicity, C := N \ (I O).
follows coherence nN Dn f 6= 0 [see Proposition 15]. Since I{xI } f
nN Dn , N , fs Airr
N \{s}{s} , g G(X N ) g 0
P
I{xI } f = g + sS fs . Clearly \ OP6= , \ = would imply that, xC
element X C , f = g(xI , xC , ) + sSO fs (xI , xC , ) oO , since Lemma 16 shows
fs (xI , xC , ) Airr
O\{s}{s} O.
follows coherence oO [Proposition 15], f
/ oO Lemma 3
0
/ posi({f } oO ). Let, ease notation,

:= {fs (xI , zC , ) : O, zC X C , fs (xI , zC , ) 6= 0} .

clearly finite subset oO [to see this, use similar argument above,
involving Lemma 16], infer Lemma 2 mass function pO
X associated expectation operator EO


(xO X )pO (xO ) > 0
(s O)(zC X C )EO (fs (xI , zC , )) 0


EO (f ) < 0.
Since f = g(xI , zC , )+
see that:

P

sSO fs (xI , zC , )+

0 > EO (f ) EO (g(xI , zC , ))

P

sS\O

X

fs (xI , zC , ) choice zC X C ,

EO (fs (xI , zC , ))

sSO

=

X

X

EO (fs (xI , zC , )) =

X

pO (xO )fs (xI , zC , xO )).

sS\O xO X

sS\O

Similarly,
zC PX C zI X \ {xI } infer 0 = g(zI , zC , ) +
P
sSO fs (zI , zC , ) +
sS\O fs (zI , zC , ) that:
0 EO (g(zI , zC , ))

X

EO (fs (zI , zC , ))

sSO

=

X

EO (fs (zI , zC , )) =

X

X

pO (xO )fs (zI , zC , xO )).

sS\O xO X

sS\O

Hence
h :=

X

X

pO (xO )fs (, , xO ) 0.

sS\O xO X

621

fiDe Cooman & Miranda

gambles fs (, , xO ) X IC [where xO X \ O] clearly
zero. non-zero ones belong sIC Ds , Lemma 16. coherence
set desirable gambles sIC Ds [Proposition 15] guarantees positive linear
combination h element cC Dc h 0, contradiction. Hence indeed
f oO .
Theorem 19 (Independent natural extension). Consider coherent sets Dn desirable gambles X n , n N . nN Dn smallest coherent set desirable
gambles X N independent product coherent sets desirable gambles Dn ,
n N.
call nN Dn independent natural extension marginals Dn .
Proof. follows Propositions 15, 17 18 nN Dn independent product
DN Dn . prove smallest one, consider independent product DN
Dn . Fix n N . consider xN \{n} X N \{n} , margn (DN xN \{n} ) = Dn ,
assumption. therefore consider g Dn , turn implies g DN xN \{n} ,
therefore I{xN\{n} } g DN , Eq. (9). infer coherence Airr
N \{n}{n} DN ,
therefore also nN Dn DN .
One useful properties independent natural extension, associativity: allows us construct extension modular fashion.
Theorem 20 (Associativity independent natural extension). Let N1 N2
disjoint non-empty index sets, consider Dnk D(X nk ), nk Nk , k = 1, 2. given
DN1 := n1 N1 Dn1 DN2 := n2 N2 Dn2 , holds
DN1 DN2 = nN1 N2 Dn .
Proof. first prove DN1 DN2 nN1 N2 Dn . Fix gamble h Airr
{N1 }{N2 }
xN1 X N1 , h(xN1 , ) DN2 {0} Eq. (17). follows Eq. (18)
gambles hnxN2 Airr
N2 \{n2 }{n2 } {0} n2 N2
1

h(xN1 , )

X

hnxN2 .
1

n2 N2

Define, n2 N2 , gamble gn2 X N letting gn2 (xN \{n2 } , ) := hnxN2 (xN2 \{n2 } , )
1
xN X N . follows Eq. (17) gn2 (xN \{n2 } , ) Dn2 {0}
xN X N , therefore gn2 Airr
N \{n2 }{n2 } {0}. Moreover,
h=

X

I{xN1 } h(xN1 , )

X

I{xN1 }

xN1 X N1



xN1 X N1

X

n2 N2

hnxN2 =
1

X

X

n2 N2 xN1 X N1

I{xN1 } hnxN2 =
1

X

gn2 ,

n2 N2

therefore follows Eq. (18) h nN1 N2 Dn , since clearly h 6= 0
Eq. (17). conclude Airr
{N1 }{N2 } nN1 N2 Dn . Similarly, prove
622

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

inclusion Airr
{N2 }{N1 } nN1 N2 Dn , therefore also DN1 DN2 nN1 N2 Dn ,
Eq. (18).
Next, prove converse inclusion nN1 N2 Dn DN1 DN2 . Consider gamble
h nN1 N2 Dn , Eq. (18) hn Airr
N1 N2 \{n}{n} {0} n N1 N2

X
X
X
h
hn = h1 + h2 , h1 :=
hn1 h2 :=
hn2 .
n1 N1

nN

n2 N2

Fix xN1 X N1 . n2 N2 , hn2 Airr
N1 N2 \{n2 }{n2 } {0} implies
irr
hn2 (xN1 , ) AN2 \{n2 }{n2 } {0} Lemma 16. Hence h2 (xN1 , ) DN2 {0} Eq. (18),
irr
therefore h2 Airr
{N1 }{N2 } {0} Eq. (17). Similarly, h1 A{N2 }{N1 } {0},
therefore h DN1 DN2 Eq. (18), since clearly h 6= 0.
conclude section, establish connection independent natural extension sets desirable gambles eponymous notion coherent lower previsions,
studied detail De Cooman et al. (2011). Given coherent lower previsions P n G(X n ),
n N , independent natural extension coherent lower prevision given


X
E N (f ) := sup
min f (zN )
[hn (zN ) P n (hn (, zN \{n} ))]
(19)
hn G(X N ) zN X N
nN

nN

gambles f X N . point-wise smallest (most conservative) joint lower
prevision satisfies property coherence Walley (1991, ch. 7) marginals
P n given assessment epistemic independence variables Xn , n N .
correspondence coherent lower previsions sets desirable gambles
mentioned Section 2.6; show next correspondence
marginals, also holds associated independent natural extensions.
Theorem 21. Let Dn coherent sets desirable gambles X n n N , let
nN Dn independent natural extension. Consider coherent lower previsions
P n G(X n ) given P n (fn ) := sup { R : fn Dn } fn G(X n ).
independent natural extension E N marginal lower previsions P n , n N satisfies
E N (f ) = sup { R : f nN Dn } f G(X N ).
Proof. Fix gamble f G(X N ). First, consider real number < E N (f ),

Pfollows Eq. (19) > 0 hn G(X N ), n N f
nN gn , defined gambles gn X N gn (zN ) := hn (zN )P n (hn (zN \{n} , ))+
zN X N . follows definition P n
gn (zN \{n} , ) = hn (zN \{n} , ) P n (hn (zN \{n} , )) + Dn zN \{n} X N \{n} .
Since clearly gn 6= 0, Eq. (17) tells us gn Airr
N \{n}{n} , infer
P
Eq. (18) nN gn nN Dn , therefore also f nN Dn . guarantees
E N (f ) sup { R : f nN Dn }.
623

fiDe Cooman & Miranda

prove converse inequality, consider real number f nN Dn .
infer using Eq. (18) gambles hn Airr
N \{n}{n} , n N f
P
nN hn . n N zN \{n} X N \{n} , follows Eq. (17) hn (zN \{n} , )
Dn {0}, therefore P n (hn (zN \{n} , )) 0, whence
X

nN

X
hn (zN ) P n (hn (zN \{n} , ))
hn (zN ) f (zN ) .
nN

infer Eq. (19) E N (f ) find indeed also E N (f )
sup { R : f nN Dn }.
similar way irrelevant natural extension, infer Eqs. (16) (18)
computational complexity finding representing independent natural extension number marginal models Dn linear number extreme rays
Dn , linear number elements sets X N \{n} therefore essentially exponential number |N | independent variables Xn , n N . Similar results hold
general case marginal sets desirable gambles characterised
using finite number generalised extreme rays, described Couso Moral (2011)
Quaeghebeur (2012a).

8. Maximal Coherent Sets Desirable Gambles Strong Products
seen collection Dn , n N marginal coherent sets desirable gambles, always smallest independent product, called independent
natural extension nN Dn . proceeded way way
excluding may other, larger, independent products. Indeed, show
section case. Using notions independent natural extension maximal coherent sets desirable gambles, consistently define specific independent
product typically strictly includes independent natural extension. call
strong product, close spirit strong product used coherent lower
prevision theory (Couso, Moral, & Walley, 2000; Cozman, 2000, 2005; De Cooman et al.,
2011), shall see Theorem 28.
8.1 Independent Products Maximal Coherent Sets Desirable Gambles
begin mentioning number interesting facts maximal coherent sets desirable gambles, independent products. following result already (essentially)
proved Couso Moral (2011): updating coherent set desirable gambles preserves
maximality.
Proposition 22. Let MN M(X N ), consider disjoint subsets N .
margO (MN xI ) M(X ) xI X .
Proof. Suppose xI X margO (MN xI ) maximal.
means f G(X ) neither f f belong MN xI ,
turn implies neither I{xI } f I{xI } f belong MN . contradicts
maximality MN .
624

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

hand, taking independent natural extension necessarily preserve maximality: Mn M(X n ) n N , necessarily hold
nN Mn M(X N ), counterexample Section A.1 shows. Interestingly, example present isolated case: consider two binary variables, independent natural extension two maximal coherent sets desirable gambles never maximal,
see next proposition. open problem whether negative result
extended finite set (not necessarily binary) variables.
intuitive explanation result maximal sets gambles
half-space excluding one two rays determining boundary,
zero gamble desirable; apply notion independent natural
extension end missing three four parts boundary set gambles
product space, preventing product maximal.
Proposition 23. Consider X 1 = X 2 = {0, 1}, let M1 M2 maximal coherent sets desirable gambles X 1 X 2 , respectively. independent natural
extension M1 M2 maximal coherent set desirable gambles.
Proof. Let pk mass function linear prevision Pk determined Mk , k = 1, 2.
deduce Theorem 21 lower prevision determined M1 M2 independent natural extension linear previsions P1 P2 , therefore equal
independent product P{1,2} linear previsions [see Proposition 25 De Cooman
et al., 2011]. linear prevision G(X {1,2} ) mass function defined
p{1,2} (x1 , x2 ) := p1 (x1 )p2 (x2 ) (x1 , x2 ) X {1,2} .
really get proof tracks, make useful observation. maximal
Mk semi-plane origin excludes origin, includes boundary
one side origin, excludes boundary side. means
unique element ak X k elements fk included boundarythose
elements fk Mk Pk (fk ) zeroare positive fk (ak ) > 0. denote single
element X k bk . words, express
Mk = {fk : Pk (fk ) > 0} {fk : Pk (fk ) = 0, fk Mk },
consider fk Mk Pk (fk ) = pk (ak )fk (ak ) + pk (bk )fk (bk ) = 0, fk (ak ) > 0
cannot gk Mk Pk (gk ) = 0 gk (bk ) > 0: otherwise, zero gamble
k)
would convex combination fk gk [it would 0 = fk fgkk (b
(bk ) gk ] would thus
belong Mk , contradiction coherence. Note reasoning assume
implicitly pk (ak ) (0, 1); otherwise, instance pk (ak ) = 0, gamble fk satisfies
Pk (fk ) = 0 fk (bk ) = 0, fk belong Mk fk (ak ) > 0.
ready turn proof. number possibilities.
First, assume pk (ak ) > 0 pk (bk ) > 0 k = 1, 2. Consider gamble h
X {1,2} h(a1 , a2 ) = h(b1 , b2 ) = 0, min h < 0, max h > 0
P{1,2} (h) = p1 (a1 )p2 (b2 )h(a1 , b2 ) + p1 (b1 )p2 (a2 )h(b1 , a2 ) = 0.
course, always gamble, going show belong
M1 M2 .
625

fiDe Cooman & Miranda

Assume ex absurdo does, meaning h1 Airr
{2}{1} h2
irr
A{1}{2} h h1 + h2 . definition, h1 (, x2 ) M1 {0} therefore
P1 (h1 (, x2 )) 0 x2 X 2 . Similarly, P2 (h2 (x1 , )) 0 x1 X 1 . Hence
0 = P{1,2} (h) P{1,2} (h1 ) + P{1,2} (h2 ) 0, taking account
X
X
P{1,2} (h1 ) =
p2 (x2 )P1 (h1 (, x2 )) 0 P{1,2} (h2 ) =
p1 (x1 )P2 (h2 (x1 , )) 0.
x2 X 2

x1 X 1

consequence, P{1,2} (h1 ) = P{1,2} (h2 ) = 0. turn implies P1 (h1 (, x2 )) =
0 x2 X 2 P2 (h2 (x1 , )) = 0 x1 X 1 . Given observations
made start proof, therefore come conclusion h1 (a1 , x2 ) 0
x2 X 2 h2 (x1 , a2 ) 0 x1 X 1 . h(a1 , a2 ) = 0 implies
h1 (a1 , a2 ) = h2 (a1 , a2 ) = 0, turn implies h1 (b1 , a2 ) = h2 (a1 , b2 ) = 0,
0 = P1 (h1 (, a2 )) = p1 (a1 )h1 (a1 , a2 ) + p1 (b1 )h1 (b1 , a2 ) 0 = P2 (h2 (a1 , )) =
p2 (a1 )h1 (a1 , a2 ) + p2 (b1 )h1 (a1 , b2 ). eventually find
h(b1 , a2 ) h1 (b1 , a2 ) + h2 (b1 , a2 ) 0 h(a1 , b2 ) h1 (a1 , b2 ) + h2 (a1 , b2 ) 0,
contradicts min h < 0.
Now, non-zero h h(a1 , a2 ) = h(b1 , b2 ) = 0 = P{1,2} (h) min h < 0
max h > 0 belong M1 M2 , neither h, means M1 M2
maximal.
Next consider cases one marginal linear previsions degenerate.
Assume instance p1 (a1 ) = 0 p2 (a2 ) (0, 1) [the cases one
marginals degenerate similar]. Consider non-zero gamble h2
/ M2
P2 (h2 ) = 0 [always possible]. h2 M2 follows observations made
beginning proof h2 (a2 ) < 0. consider gamble h defined
h(b1 , a2 ) := h2 (a2 ) < 0,

h(b1 , b2 ) := h2 (b2 ) 0,

h(a1 , a2 ) = h(a1 , a2 ) := 1.

follows P{1,2} (h) = P2 (h2 ) = 0. see h
/ M1 M2 , assume
irr
f1 Airr

f




h

f
2
1 + f2 . 0 = P{1,2} (h)
{2}{1}
{1}{2}
P{1,2} (f1 ) + P{1,2} (f2 ) 0 therefore 0 = P{1,2} (f1 ) = p2 (a2 )f1 (b1 , a2 ) + p2 (b2 )f1 (b1 , b2 ).
hand, f1 Airr
{2}{1} also implies P1 (f1 (, x2 )) 0 x2 X 2 ,
therefore f1 (b1 , a2 ) 0 f1 (b1 , b2 ) 0. Hence f1 (b1 , ) = 0, therefore f2 (b1 , )
h(b1 , ) = h2 since h2
/ M2 , follows f2 (b1 , )
/ M2 . must
definition f2 (b1 , ) M2 {0}, mean f2 (b1 , ) = 0, whence h2 0,
contradicting h2 (a2 ) < 0. implies h cannot belong M1 M2 .
Similarly, h belongs M1 M2 , must g1 Airr
{2}{1} g2
irr
A{1}{2} h g1 + g2 . 0 = P{1,2} (h) P{1,2} (g1 ) + P{1,2} (g2 )
0, whence 0 = P{1,2} (g1 ) = p2 (a2 )g1 (b1 , a2 ) + p2 (b2 )g1 (b1 , b2 ). g1 Airr
{2}{1} also
implies P1 (g1 (, x2 )) 0 x2 X 2 , therefore g1 (b1 , a2 ) 0 g1 (b1 , b2 )
0. Hence g1 (b1 , ) = 0, therefore find g1 (a1 , a2 ) 0 g1 (a1 , b2 ) 0 [if,
say, g1 (a1 , a2 ) < 0 g1 (, a2 ) < 0 also g1 (b1 , a2 ) = 0, contradicts
irr
g1 (, a2 ) M1 {0}, consequence g1 Airr
{2}{1} ]. Since, moreover, g2 A{1}{2}
implies 0 P2 (g2 (a1 , )) = p2 (a2 )g2 (a1 , a2 ) + p2 (b2 )g2 (a1 , b2 ) therefore also
626

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

g2 (a1 , a2 ) 0 g2 (a1 , b2 ) 0, follows h(a1 , a2 ) 0 h(a1 , b2 ) 0,
contradicts h(a1 , a2 ) = h(a1 , b2 ) = 1 < 0. Hence, h belong M1 M2
either, M1 M2 maximal.
Finally, turn cases marginals degenerate. Assume instance
p1 (a1 ) = p2 (a2 ) = 0 [the cases marginals degenerate, similar].
Consider gamble h given
h(a1 , a2 ) = h(b1 , b2 ) = 0,

h(b1 , a2 ) = 1,

h(a1 , b2 ) = 1,

P{1,2} (h) = p1 (b1 )p2 (b2 )h(b1 , b2 ) = 0. see h
/ M1 M2 , assume ex absurdo
irr
u1 Airr

u




h
u1 + u2 . u1 Airr
2
{2}{1}
{1}{2}
{2}{1}
implies
u1 (b1 , b2 ) = P1 (u1 (, b2 )) 0 u1 (b1 , a2 ) = P1 (u1 (, a2 )) 0,
similarly u2 Airr
{1}{2} implies u2 (b1 , b2 ) = P2 (u2 (b1 , )) 0 u2 (a1 , b2 ) =
P2 (u2 (a1 , )) 0. also follows P{1,2} (h) = 0, P{1,2} (u1 ) 0 P{1,2} (u2 ) 0
u1 (b1 , b2 ) = P{1,2} (u1 ) = 0 u2 (b1 , b2 ) = P{1,2} (u2 ) = 0, consequence
find u1 (a1 , b2 ) 0 u2 (b1 , a2 ) 0 [if, say, u1 (a1 , b2 ) < 0 u1 (, b2 ) < 0
also u1 (b1 , b2 ) = 0, contradicts u1 (, b2 ) M1 {0}, consequence u1 Airr
{2}{1} ].
consequence, 1 = h(a1 , b2 ) u1 (a1 , b2 ) + u2 (a1 , b2 ) 0, contradiction. Hence
indeed, h belong M1 M2 .
irr
Finally, assume ex absurdo v1 Airr
{2}{1} v2 A{1}{2}
h v1 + v2 . v1 Airr
{2}{1} implies
v1 (b1 , b2 ) = P1 (v1 (, b2 )) 0 v1 (b1 , a2 ) = P1 (v1 (, a2 )) 0,
similarly v2 Airr
{1}{2} implies v2 (b1 , b2 ) = P2 (v2 (b1 , )) 0 v2 (a1 , b2 ) =
P2 (v2 (a1 , )) 0. also follows P{1,2} (h) = 0, P{1,2} (v1 ) 0 P{1,2} (v1 ) 0
v1 (b1 , b2 ) = P{1,2} (v1 ) = 0 v2 (b1 , b2 ) = P{1,2} (v2 ) = 0, consequence find
v1 (a1 , b2 ) 0 v2 (b1 , a2 ) 0 [if, say, v1 (a1 , b2 ) < 0 v1 (, b2 ) < 0 also
v1 (b1 , b2 ) = 0, contradicts v1 (, b2 ) M1 {0}, consequence v1 Airr
{2}{1} ].
consequence, 1 = h(b1 , a2 ) v1 (b1 , a2 ) + v2 (b1 , a2 ) 0, contradiction. shows
h belong M1 M2 either, therefore set maximal.
hand, Example A.2 Appendix shows independent
products maximal coherent sets desirable gambles maximal; hence, independent natural extension maximal coherent sets independent product.
Indeed, establish following result:
Proposition 24. Consider maximal coherent sets desirable gambles M1 M(X 1 )
M2 M(X 2 ).
(i) Let D{1,2} coherent set desirable gambles X {1,2} M1 M2
D{1,2} . D{1,2} independent marginals M1 M2 .
(ii) consequence, maximal set gambles M{1,2} independent product
marginals M{1,2} x2 x2 X 2 M{1,2} x1
x1 X 1 .
627

fiDe Cooman & Miranda

Proof. (i). every x1 X 1 M2 = (M1 M2 )x1 D{1,2} x1 ,
equality follows Proposition 18. Since M2 maximal, implies M2 =
D{1,2} x1 x1 X 1 , similar argument shows M1 = D{1,2} x2
x2 X 2 . hand, follows Proposition 17 M2 = marg2 (M1 M2 )
marg2 (D{1,2} ). Since M2 maximal, implies M2 = marg2 (D{1,2} ), similar
argument shows M1 = marg1 (D{1,2} ). summary, see marg1 (D{1,2} ) =
D{1,2} x2 x2 X 2 , marg2 (D{1,2} ) = D{1,2} x1 x1 X 1 , showing
D{1,2} indeed independent.
(ii). follows definition independent product necessary
M{1,2} x2 M{1,2} x1 x2 x1 , respectively. see
also sufficient condition M{1,2} independent product, note
case M{1,2} x1 M{1,2} x2 M{1,2} , sets M{1,2} x1 M{1,2} x2
maximal, Proposition 22. hand, Proposition 17 implies
marg1 (M{1,2} x1 M{1,2} x2 ) = M{1,2} x1 marg1 (M{1,2} ),
sets equal. Similarly, deduce
marg2 (M{1,2} x1 M{1,2} x2 ) = M{1,2} x2 marg2 (M{1,2} ),
therefore marg1 (M{1,2} ) marg2 (M{1,2} ) M{1,2} . Invoking first part
proposition, find M{1,2} independent product marginals.
first part proposition provides us simple characterisation
independent products two maximal sets: simply coherent supersets
independent natural extension; particular, means maximal superset
independent natural extension independent product, two maximal sets
always maximal products (although differ independent natural
extension). second part implies sets conditional gambles coincide
conditioning events, automatically agree marginal sets gambles,
consequence set independent product.
8.2 Strong Product Properties
consider case coherent marginal sets desirable gambles Dn
n N . define strong product nN Dn set desirable gambles
product space X N given by:8
\
nN Dn :=
{nN Mn : Mn m(Dn ), n N } ,
m(Dn ) given Eq. (1). strong product corresponds set desirable
gambles determined notion independence restrictive epistemic irrelevance independence considered far: strong independence (Couso
et al., 2000; Cozman, 2012), sometimes called type-3 independence (de Campos & Moral,
8. paper focusses independent natural extension, much direct behavioural
justification, forgo discussing complexity computing strong product, which,
face it, appears significantly higher independent natural extension.

628

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

1995). Strong independence means associated joint credal set convex hull
set linear previsions stochastic independent products linear previsions
dominate marginals; or, equivalently, associated lower prevision
lower envelope products linear previsions dominate marginals.
clearer Theorem 28.
maximal coherent sets desirable gambles Mn M(X n ), n N strong product independent natural extension coincide: nN Mn = nN Mn , clearly
m(Mn ) = {Mn }. Taking account Proposition 23, deduce strong product
maximal coherent sets desirable gambles necessarily maximal; Example A.2
Appendix shows independent products may strictly include
strong product.
marginalisation properties strong product follow directly
independent natural extension.
Proposition 25 (Marginalisation). Consider coherent sets desirable gambles Dn
n N . Let R subset N , margR (nN Dn ) = rR Dr .
Proof. Consider f G(X R ) observe following chain equivalences:
f nN Dn (Mn m(Dn ), n N )f nN Mn
(Mn m(Dn ), n N )f rR Mr
(Mr m(Dr ), r R)f rR Mr
f rR Dr ,
second equivalence follows Proposition 17.
Next, show strong product coherent marginal sets desirable gambles
Dn independent product marginals. order so, first establish
following simple yet powerful result:
j
, j J non-empty family independent coherent sets
Proposition 26. Let DN

j
desirable gambles X N . intersection DN := jJ DN
independent
coherent set desirable gambles X N .

Proof. Consider disjoint subsets N , xI X .
j
xI )
h margO (DN xI ) (j J)h margO (DN
j
(j J)h margO (DN
)

h margO (DN ).
Proposition 27. Consider coherent marginal sets desirable gambles Dn n N .
strong product nN Dn independent product marginals.
Proof. Taking account Proposition 26, need show sets Dn
marginals strong product nN Dn . immediate consequence Proposition 25.
629

fiDe Cooman & Miranda

strong product may strictly include independent natural extension,
see example Section A.3. open question whether, like independent natural extension, strong product associative. Although
able prove associativity general, difficult show suffices establish maximal sets desirable gambles, one inclusions, namely
nN1 N2 Mn (nN1 Mn ) (nN2 Mn ) holds strong product always includes independent natural extension. suspect, able prove,
converse inclusion also holds, strong product associative, taking
account definition taking intersection sets gambles determined
associative operator (the independent natural extension).
conclude section, establish connection strong product sets
desirable gambles eponymous notion coherent lower previsions, studied
instance De Cooman et al. (2011) (see also Cozman, 2012 comments corresponding notion terms credal sets, sometimes called strong extension).
Given coherent lower previsions P n G(X n ), n N , strong product coherent
lower prevision defined
N (f ) := inf {nN Pn (f ) : (n N )Pn M(P n )}
gambles f X N ; intuition behind notion, taking account correspondence coherent lower previsions sets desirable gambles discussed
Section 2.6, intersection family sets desirable gambles closely related
taking lower envelope associated family coherent lower previsions.
start linear previsions Pn G(X n ), strong product corresponds
linear product nN Pn , coincides also independent natural extension EN .
begin coherent lower previsions P n G(X n ), strong product N
lower envelope set strong products determined dominating linear previsions.
Theorem 28. Let Dn coherent sets desirable gambles G(X n ) n N ,
let nN Dn strong product. Consider coherent lower previsions P n G(X n )
given P n (f ) := sup { R : f Dn }. strong product N marginal
lower previsions P n , n N satisfies
N (f ) = sup { R : f nN Dn } .
Proof. Assume first Dn maximal coherent set desirable gambles n
N . follows P n linear prevision, denote Pn , n N .
strong product linear previsions Pn , n N coincides linear independent
product nN Pn , also independent natural extension (use Proposition 10
De Cooman et al., 2011). Since proved Theorem 21
coherent lower prevision associated nN Dn = nN Dn , conclude strong
product nN Dn associated strong product linear previsions Pn .
move next general case. Fix gamble f X N . Consider real number
< N (f ). n N , consider maximal coherent set desirable gambles
Mn m(Dn ), associated linear prevision Pn , clearly Pn M(P n ). Hence
nN Pn (f ) N (f ) > , infer arguments necessarily
630

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

f nN Mn . Hence f nN Dn . leads conclusion N (f )
sup { R : f nN Dn }.
Conversely, consider real number f nN Dn . Consider arbitrary
Pn M(P n ), n N , maximal coherent sets desirable gambles Mn
m(Dn ) inducing them: let Dn set strictly desirable gambles induces Pn , given
Eq. (3). set coherent Walley (1991, Thm. 3.8.1). Consider set Dn Dn ,
let us show coherent. Condition D2 holds trivially satisfied Dn .
see D3 holds, taking account Dn Dn coherent sets gambles,
particular cones, suffices show gamble f Dn g Dn ,
sum f + g belongs Dn Dn . Consider thus gambles f, g. f G(X n )>0 , also
belongs Dn consequence f + g Dn ; hand, f Dn \ G(X n )>0 ,
follows Pn (f ) > 0, whence Pn (f + g) = Pn (f ) + Pn (g) Pn (f ) + P n (g) > 0,
therefore f + g Dn . Since Dn Dn coherent, deduce
condition D1 also holds, consequence set Dn Dn indeed coherent.
Now, Theorem 5 implies maximal coherent set desirable gambles
Mn m(Dn Dn ) m(Dn ), Walley (1991, Thm. 3.8.3) deduce Dn
Mn induce Pn means Eq. (2). f nN Mn ,
therefore nN Pn (f ) , using argumentation above. Hence N (f ) , therefore
N (f ) sup { R : f nN Dn }.
Together Theorem 21 fact strong product lower previsions may
strictly dominate independent natural extension (see Example 9.3.4 Walley, 1991),
also shows strong product marginal sets desirable gambles may strictly
include independent natural extension. explicit example given Appendix A.3.

9. Conditional Irrelevance Independence
final step take paper, consists extending results irrelevance
independence simple common form conditional irrelevance independence.
Next variables XN X N , also consider another variable assuming values
finite set Y.
Consider two disjoint subsets N . say XI epistemically irrelevant
XO when, conditional , learning value XI influence change
beliefs XO .
set desirable gambles X N capture type conditional
epistemic irrelevance? Clearly, require that:
margO (DxI , y) = margO (Dy) xI X Y.
before, technical reasons also allow empty. clear
definition variable X , whose constant value certain,
conditionally epistemically irrelevant variable XO . Similarly, see
variable XI conditionally epistemically irrelevant variable X . seems
accordance intuition.
631

fiDe Cooman & Miranda

Also, singleton, uncertainty conditioning
amounts conditioning all: epistemic irrelevance seen special case
conditional epistemic irrelevance.
want argue that, conversely, specific definite way
conditional epistemic irrelevance statements reduced simple epistemic irrelevance
statements. crucial results allow us establish this, following conceptually
simple theorem corollary.
Theorem 29 (Sequential updating). Consider subset R N , coherent set
desirable gambles X N Y.
(Dy)xR = (DxR )y = DxR ,

xR X R Y.

(20)

Proof. Fix xR X R Y. Clearly, three sets Eq. (20) subsets
G(X N \R ). take gamble f X N \R , consider following chains equivalences:
I{y} I{xR } f I{xR } f Dy f (Dy)xR
I{y} I{xR } f I{y} f DxR f (DxR )y
I{y} I{xR } f f DxR , y.
Corollary 30 (Reduction). Consider disjoint subsets N , coherent set desirable gambles X N Y. following statements equivalent:
(i) margO (DxI , y) = margO (Dy) xI X Y;
(ii) margO ((Dy)xI ) = margO (Dy) xI X Y.
tells us model (XN , ) represents epistemic irrelevance XI XO ,
conditional possible value , model Dy XN
represents epistemic irrelevance XI XO .
suppose marginal conditional models Dn X n , n N . notation
Dn concise way representing family conditional models Dn y, Y.
combine Corollary 30 Theorem 19, obtain following:
Corollary 31. smallest conditionally independent product DY marginal models
Dn given nN (Dn ), meaning Y, Dy = nN (Dn y).
also shows calculating conditionally independent natural extension has,
comparison independent natural extension, additional factor computational
complexity simply linear number possible values conditioning
variable .

10. Conclusions
Sets desirable gambles informative coherent lower previsions,
shown Section 2.6, helpful avoiding problems involving zero probabilities.
Moreover, simple axiomatic definition, seen Section 2.1.
632

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

overlooked much development theory imprecise probabilities,
last five six years effort devoted bringing
simplifying unifying notion fore.
Working sets desirable gambles allows us show computational complexity checking whether gamble belongs independent natural extension compares favourably computing strong product, complexity
exponential number variables.
results also show model assessments epistemic independence
easily using sets desirable gambles, derive existing results
lower previsions.
Moreover, results Section 7 indicate constructing global joint models (i.e. coherent sets desirable gambles) local ones something easily efficiently done types credal networks (Cozman, 2000, 2005). interpretation
graphical structure credal networks usually taken following:
node (variable), conditional parents, non-parent non-descendants strongly
independent (Cozman, 2000, 2005). replace assumption strong independence weaker one epistemic irrelevance, work De Cooman et al.
(2010), tends produce conservative independent products.9
consider credal network made n unconnected nodes X1 , . . . , Xn , interpretation simple: variable Xk , remaining variables X1 , . . . , Xk1 ,
Xk+1 , . . . Xn epistemically irrelevant it. expression (18) independent
natural extension nk=1 Dk , reasoning behind Section 7, show nk=1 Dk
smallest (most conservative) coherent joint set desirable gambles expresses
epistemic irrelevancies graph.
Interestingly, make network slightly complicated looking developments Section 9, tell us conditionally independent natural extension
nk=1 Xk conservative (conditional) joint model reflects independence
conditions embedded following graphical structure:

...
X1

X2

Xn1

Xn

variable Xk , remaining variables X1 , . . . , Xk1 , Xk+1 , . . . Xn epistemically
irrelevant Xk , conditional .
Now, tree built recursively using simple networks like one
building blocks: similarly done De Cooman et al. (2010, Section 4),
use recursion leaves root, step conditional
model put together joint one using epistemic irrelevance/independence
assessments marginal extension theorem (Miranda & De Cooman, 2007), allows
us combine hierarchical information. suggests developments paper
9. See also Section 8 details strong independence; surmise computational
complexity dealing strong products worse computing independent natural
extension.

633

fiDe Cooman & Miranda

used good advantage finding efficient algorithms inference credal trees
epistemic irrelevance, using sets desirable gambles uncertainty models.
approach could build ideas proposed De Cooman et al. (2010) Destercke
De Cooman (2008) context credal trees lower previsions local uncertainty
models, make general also directly amenable simple assessment
elicitation local models. would interesting applications dealing
hidden Markov models imprecise transition emission models, are,
course, special credal trees.
expect generalising algorithms towards general credal networks
(polytrees, . . . ) difficult, rely heavily pioneering work
Moral (2005) graphoid properties epistemic irrelevance. sense, would
interesting model assumptions independence variables using sets
desirable gambles, instance intermediate assumptions epistemic irrelevance
independence (that is, epistemic irrelevance pairs sets variables only).
Moreover, algorithms computing irrelevant independent natural extension,
well strong product, need devised.
open problems would generalise work infinite sequences random
variables, would allow us deal unbounded trees, and, already
discussed paper, establish associativity strong product extend
results variables taking values infinite spaces.

Acknowledgments
work supported SBO project 060043 IWT-Vlaanderen, project
MTM2010-17844. would like thank reviewers helpful comments.

Appendix A. Examples
appendix, gathered number examples counterexamples.
A.1 Independent Natural Extension Need Preserve Maximality
Let X = {0, 1} let subset G(X ) given
:= {f G(X ) : f (0) + f (1) > 0 f (0) = f (1) > 0} .
easy see coherent set desirable gambles.
1
moreover maximal: non-zero f
/ M, either f (0) + f (1) < 0,
whence f (0) f (1) > 0 f > 0, f (0) = f (1) < 0

f (0) = f (1) > 0, means f M.
Let N = {1, 2}, X 1 = X 2 = X M1 = M2 = M. independent
natural extension M1 M2 given


irr
M1 M2 := posi G(X {1,2} )>0 Airr
{1}{2} A{2}{1}
n

irr
= h1 + h2 : h1 Airr

{0},
h



{0}
\ {0},
2
{1}{2}
{2}{1}
634

0

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

irr
taking account non-negative gambles belong Airr
{1}{2} A{2}{1}
irr
irr
Airr
{1}{2} {0} A{2}{1} {0} convex cones. Recall h1 A{1}{2} {0}
iff h1 (0, ) {0} h1 (1, ) {0}, similarly h2 Airr
{2}{1} {0} iff
h2 (, 0) {0} h2 (, 1) {0}. means gamble h M1 M2
expressed

h(0, 0) = + ,

h(0, 1) = + ,

h(1, 0) = + ,

h(1, 1) = + ,

, . . . , real numbers satisfying following constraints:
+ > 0 = 0
+ > 0 = 0
+ > 0 = 0
+ > 0 = 0
max{, , , } > 0.
gamble h given h(0, 0) = h(1, 1) = 1 h(0, 1) = h(1, 0) = 1 belong
M1 M2 : since h(0, 0) + h(0, 1) + h(1, 0) + h(1, 1) = 0, = 0,
= 0, = 0 = 0, implies h(0, 0) 0, contradiction.
h belong M1 M2 either, h(0, 0) + h(0, 1) + h(1, 0) + h(1, 1) = 0
similarly implies h(1, 1) 0. Hence, independent natural extension M1
M2 maximal.
A.2 Maximal Independent Product Maximal Sets
Next, construct example independent product maximal sets
maximal.
Consider spaces X 1 X 2 , maximal marginal coherent sets desirable
gambles M1 M2 Section A.1. consider set desirable gambles
defined
:= {h G(X N ) : h(0, 0) + h(0, 1) + h(1, 0) + h(1, 1) > 0}
{h G(X N ) : h(0, 0) + h(0, 1) + h(1, 0) + h(1, 1) = 0
[h(0, 0) > 0 h(0, 0) = 0, h(0, 1) > 0 h(0, 0) = h(0, 1) = 0, h(1, 0) > 0]}.
first show M1 M2 . According discussion Section A.1, gamble
h M1 M2 satisfies h(0, 0) = + , h(0, 1) = + , h(1, 0) = + , h(1, 1) = + ,
particular
min{ + , + , + , + } 0,
whence
h(0, 0) + h(0, 1) + h(1, 0) + h(1, 1) = ( + ) + ( + ) + ( + ) + ( + )
= ( + ) + ( + ) + ( + ) + ( + ) 0.
h(0, 0) + h(0, 1) + h(1, 0) + h(1, 1) = 0, implies + = + = + = + = 0,
therefore, looking characterisation M1 M2 Section A.1,
635

fiDe Cooman & Miranda

= 0, = 0, = 0 = 0 implies particular
h(0, 0) = + 0. see either h(0, 0) > 0, case h ,
h(0, 0) = 0. implies = = = = 0. h(0, 1) = 0,
either h(0, 1) > 0, case h , h(0, 1) = = = 0.
follows conditions imposed , . . . , Section A.1 h(1, 0) = > 0,
means h belongs . So, indeed, M1 M2 .
show maximal coherent set desirable gambles. easy
see coherent. show maximal, consider non-zero gamble h
G(X {1,2} ); three possibilities. h(0, 0) + h(1, 0) + h(0, 1) + h(1, 1) > 0,
h h
/ . h(0, 0) + h(1, 0) + h(0, 1) + h(1, 1) < 0, h h
/ .
h(0, 0) + h(1, 0) + h(0, 1) + h(1, 1) = 0, exactly one h, h belongs .
conclude, note independent product M1 M2 Proposition 24.
A.3 Strong Product May Strictly Include Independent Natural
Extension
following adaptation example Walley (1991, Example 9.3.4).
Consider X = {0, 1} let P coherent lower prevision determined P ({0}) =
2/5 P (1) = 1/2, f G(X ) that:


1
1
2
3
P (f ) = min
f (0) + f (1), f (0) + f (1) .
2
2
5
5
P associate coherent set (strictly) desirable gambles Eq. (3):
:= {f : f > 0 P (f ) > 0} .
let N = {1, 2}, X 1 = X 2 = X D1 = D2 = D. Consider gamble h X {1,2}
determined
51
49
h(0, 0) = h(1, 1) =
, h(0, 1) = h(1, 0) =
.
100
100
see D1 D2 strictly included D1 D2 , show h belongs D1 D2
D1 D2 .
irr
latter claim, consider gambles h1 Airr
{1}{2} h2 A{2}{1} , assume
ex absurdo h h1 +h2 . see (h1 +h2 )(0, 0) = +, (h1 +h2 )(0, 1) = +,
(h1 + h2 )(1, 0) = + (h1 + h2 )(1, 1) = + , real numbers , . . . , must
satisfy following constraints:


1
1 2
3
max{, } > 0 min
+ , + 0
2
2 5
5


1
1 2
3
max{, } > 0 min
+ , + 0
2
2 5
5


1
1 2
3
max{, } > 0 min
+ , + 0
2
2 5
5


1
1 2
3
max{, } > 0 min
+ , + 0.
2
2 5
5
636

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

consequence,
3
2
( + ) + ( + + + + + )
5
5
2
3
2
3
6 1
1
6 1
1
= ( + ) + ( + ) + ( + ) + ( + ) 0,
5
5
5
5
5 2
2
5 2
2
hand
2
3
( + ) + ( + + + + + )
5
5


2
3
39
h(0, 0) + (h(0, 1) + h(1, 0) + h(1, 1)) =
,
5
5
500

contradiction. implies h belong D1 D2 .
former claim, consider arbitrary maximal coherent set desirable gambles
M1 m(D1 ) M2 m(D2 ). follows discussion Section 2.6
M1 induces linear prevision P1 P 1 , M2 induces linear prevision P2 P 2 .
follows discussion Example 9.3.4 Walley (1991)
(P1 P2 )(h)

1
> 0,
100

tells us h belongs set strictly desirable gambles induces P1 P2 ,
Eq. (3). Since smallest coherent set desirable gambles induces
P1 P2 , since M1 M2 another set, Theorem 28, deduce h
M1 M2 . follows indeed h D1 D2 .

References
Antonucci, A., de Campos, C., & Zaffalon, M. (2012). Probabilistic graphical models.
Coolen, F., Augustin, T., De Cooman, G., & Troffaes, M. C. M. (Eds.), introduction
imprecise probabilities, chap. 10. John Wiley & Sons. press.
Augustin, T., Coolen, F. P. A., De Cooman, G., & Troffaes, M. C. M. (Eds.). (2012).
Introduction Imprecise Probabilities. John Wiley & Sons. press.
Aumann, R. J. (1962). Utility theory without completeness axiom. Econometrica, 30,
445462.
Bernoulli, J. (1713). Ars Conjectandi. Thurnisius, Basel.
Boole, G. (1847, reprinted 1961). Laws Thought. Dover Publications, New York.
Boole, G. (2004, reprint work originally published Watts & Co., London, 1952).
Studies Logic Probability. Dover Publications, Mineola, NY.
Buehler, R. J. (1976). Coherent preferences. Annals Statistics, 4, 10511064.
Couso, I., & Moral, S. (2011). Sets desirable gambles: conditioning, representation,
precise probabilities. International Journal Approximate Reasoning, 52 (7), 1034
1055.
637

fiDe Cooman & Miranda

Couso, I., Moral, S., & Walley, P. (2000). survey concepts independence imprecise
probabilities. Risk Decision Policy, 5, 165181.
Cozman, F. G. (2000). Credal networks. Artificial Intelligence, 120, 199233.
Cozman, F. G. (2005). Graphical models imprecise probabilities. International Journal
Approximate Reasoning, 39 (2-3), 167184.
Cozman, F. G. (2012). Sets probability distributions, independence, convexity. Synthese, 186, 177200.
de Campos, L. M., & Moral, S. (1995). Independence concepts convex sets probabilities. Besnard, P., & Hanks, S. (Eds.), Eleventh Conference Uncertainty
Artificial Intelligence, pp. 108115. San Francisco, CA.
De Cooman, G. (2005). Belief models: order-theoretic investigation. Annals Mathematics Artificial Intelligence, 45 (12), 534.
De Cooman, G., Hermans, F., Antonucci, A., & Zaffalon, M. (2010). Epistemic irrelevance
credal nets: case imprecise Markov trees. International Journal Approximate
Reasoning, 51 (9), 10291052.
De Cooman, G., Miranda, E., & Zaffalon, M. (2011). Independent natural extension. Artificial Intelligence, 175 (1213), 19111950.
De Cooman, G., & Quaeghebeur, E. (2012). Exchangeability sets desirable gambles.
International Journal Approximate Reasoning, 53 (3), 363395. Special issue
honour Henry E. Kyburg, Jr.
de Finetti, B. (1931). Sul significato soggettivo della probabilita. Fundamenta Mathematicae, 17, 298329.
de Finetti, B. (1937). La prevision: ses lois logiques, ses sources subjectives. Annales de
lInstitut Henri Poincare, 7, 168. English translation (Kyburg Jr. & Smokler,
1964).
de Finetti, B. (1970). Teoria delle Probabilita. Einaudi, Turin.
de Finetti, B. (19741975). Theory Probability: Critical Introductory Treatment. John
Wiley & Sons, Chichester. English translation (de Finetti, 1970), two volumes.
Destercke, S., & De Cooman, G. (2008). Relating epistemic irrelevance event trees.
Dubois, D., Lubiano, M., Prade, H., Gil, M., Grzegiorzewski, P., & Hryniewicz, O.
(Eds.), Soft Methods Handling Variability Imprecision, pp. 6673. Springer.
Dubins, L. E. (1975). Finitely additive conditional probabilities, conglomerability disintegrations. Annals Probability, 3, 8899.
Dubra, J., Maccheroni, F., & Ok, E. (2004). Expected utility theory without completeness axiom. Journal Economic Theory, 115 (1), 118133.
Fishburn, P. C. (1975). theory subjective expected utility vague preferences.
Theory Decision, 6, 287310.
Gardenfors, P. (1988). Knowledge Flux Modeling Dynamics Epistemic States.
MIT Press, Cambridge, MA.
638

fiIrrelevant Independent Natural Extension Sets Desirable Gambles

Giron, F. J., & Rios, S. (1980). Quasi-Bayesian behaviour: realistic approach
decision making?. Bernardo, J. M., DeGroot, M. H., Lindley, D. V., & Smith, A.
F. M. (Eds.), Bayesian Statistics, pp. 1738. Valencia University Press, Valencia.
Hermans, F. (2012). operational approach graphical uncertainty modelling. Ph.D.
thesis, Faculty Engineering Architecture.
Koopman, B. O. (1940). Axioms Algebra Intuitive Probability. Annals
Mathematics, Second Series, 41 (2), 269292.
Kyburg Jr., H. E., & Smokler, H. E. (Eds.). (1964). Studies Subjective Probability. Wiley,
New York. Second edition (with new material) 1980.
Levi, I. (1980). Enterprise Knowledge. MIT Press, London.
Miranda, E., & De Cooman, G. (2007). Marginal extension theory coherent lower
previsions. International Journal Approximate Reasoning, 46 (1), 188225.
Miranda, E., & Zaffalon, M. (2010). Notes desirability coherent lower previsions.
Annals Mathematics Artificial Intelligence, 60 (34), 251309.
Miranda, E., Zaffalon, M., & De Cooman, G. (2012). Conglomerable natural extension.
International Journal Approximate Reasoning, 53 (8), 12001227.
Moral, S. (2005). Epistemic irrelevance sets desirable gambles. Annals Mathematics
Artificial Intelligence, 45 (12), 197214.
Moral, S., & Wilson, N. (1995). Revision rules convex sets probabilities. Coletti,
G., Dubois, D., & Scozzafava, R. (Eds.), Mathematical Models Handling Partial
Knowledge Artificial Intelligence, pp. 113128. Plenum Press, New York.
Pearl, J. (1985). Bayesian netowrks: model self-activated memory evidential reasoning. Proceedings 7th Conference Cognitive Science Society, pp.
329334, Irvine, CA. University California.
Quaeghebeur, E. (2012a). CONEstrip algorithm. Proceedings SMPS 2012, pp.
4554. Springer.
Quaeghebeur, E. (2012b). Desirability. Coolen, F., Augustin, T., de Cooman, G., &
Troffaes, M. C. M. (Eds.), Introduction Imprecise Probabilities, chap. 2. John
Wiley & Sons. press.
Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (1995). representation partially
ordered preferences. Annals Statistics, 23, 21682217. Reprinted (Seidenfeld,
Schervish, & Kadane, 1999), pp. 69129.
Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (1999). Rethinking Foundations
Statistics. Cambridge University Press, Cambridge.
Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (2010). Coherent choice functions
uncertainty. Synthese, 172 (1), 157176.
Shapley, L. S., & Baucells, M. (1998). theory multiperson utility. Discussion paper,
department economics 779, UCLA.
Smith, C. A. B. (1961). Consistency statistical inference decision. Journal
Royal Statistical Society, Series A, 23, 137.
639

fiDe Cooman & Miranda

Walley, P. (1982). elicitation aggregation beliefs. Tech. rep., University
Warwick, Coventry. Statistics Research Report 23.
Walley, P. (1991). Statistical Reasoning Imprecise Probabilities. Chapman Hall,
London.
Walley, P. (2000). Towards unified theory imprecise probability. International Journal
Approximate Reasoning, 24 (23), 125148.
Williams, P. M. (1975a). Coherence, strict coherence zero probabilities. Proceedings
Fifth International Congress Logic, Methodology Philosophy Science,
Vol. VI, pp. 2933. Dordrecht. Proceedings 1974 conference held Warsaw.
Williams, P. M. (1975b). Notes conditional previsions. Tech. rep., School Mathematical
Physical Science, University Sussex, UK. Revised journal version: (Williams,
2007).
Williams, P. M. (2007). Notes conditional previsions. International Journal Approximate Reasoning, 44 (3), 366383. Revised journal version (Williams, 1975b).
Zaffalon, M., & Miranda, E. (2012). Probability time. Submitted publication.

640

fiJournal Artificial Intelligence Research 45 (2012) 363-441

Submitted 03/12; published 10/12

Transforming Graph Data Statistical Relational Learning
Ryan A. Rossi

rrossi@purdue.edu

Department Computer Science, Purdue University
West Lafayette, 47907 USA

Luke K. McDowell

lmcdowel@usna.edu

Department Computer Science, U.S. Naval Academy
Annapolis, MD 21402, USA

David W. Aha

david.aha@nrl.navy.mil

Navy Center Applied Research Artificial Intelligence
Naval Research Laboratory (Code 5514)
Washington, DC 20375, USA

Jennifer Neville

neville@purdue.edu

Department Computer Science, Purdue University
West Lafayette, 47907 USA

Abstract
Relational data representations become increasingly important topic due
recent proliferation network datasets (e.g., social, biological, information networks)
corresponding increase application Statistical Relational Learning (SRL)
algorithms domains. article, examine categorize techniques
transforming graph-based relational data improve SRL algorithms. particular, appropriate transformations nodes, links, and/or features data dramatically
affect capabilities results SRL algorithms. introduce intuitive taxonomy
data representation transformations relational domains incorporates link transformation node transformation symmetric representation tasks. specifically,
transformation tasks nodes links include (i) predicting existence, (ii)
predicting label type, (iii) estimating weight importance, (iv) systematically constructing relevant features. motivate taxonomy detailed
examples use survey competing approaches tasks. also discuss general conditions transforming links, nodes, features. Finally, highlight
challenges remain addressed.

1. Introduction
article, examine categorize techniques transforming relational data improve Statistical Relational Learning (SRL) algorithms. Below, Section 1.1 first introduces
relational data SRL. summarize primary types representations relational
data, explain focus data represented graphs. Section 1.1 also describes
transforming content (rather type) representation improve SRL
analysis. instance, predicting new links graph increase accuracy relational
node classification. Section 1.2 identifies scope article. Finally, Section 1.3
summarizes organization approach article, includes description
taxonomy relational representation transformation.
c
2012
AI Access Foundation. rights reserved.

fiRossi, McDowell, Aha, & Neville

1.1 Relational Data, SRL, Representation Choices
majority research machine learning assumes independently identically distributed data. independence assumption often violated relational data, encode dependencies among data instances. instance, people often linked business
associations, information one person highly informative prediction
task involving associate person. generally, relational data described
set nodes, connected one types relations (or links).
Relational information seemingly ubiquitous; present domains Internet
world-wide web (Faloutsos, Faloutsos, & Faloutsos, 1999; Broder et al., 2000; Albert, Jeong, & Barabasi, 1999), scientific citation collaboration (McGovern et al., 2003;
Newman, 2001b), epidemiology (Pastor-Satorras & Vespignani, 2001; Moore & Newman,
2000; May & Lloyd, 2001; Kleczkowski & Grenfell, 1999) communication analysis (Rossi
& Neville, 2010), metabolism (Jeong, Tombor, Albert, Oltvai, & Barabasi, 2000; Wagner
& Fell, 2001), ecosystems (Dunne, Williams, & Martinez, 2002; Camacho, Guimera, &
Nunes Amaral, 2002), bioinformatics (Maslov & Sneppen, 2002; Jeong, Mason, Barabasi,
& Oltvai, 2001), fraud terrorist analysis (Neville et al., 2005; Krebs, 2002), many
others. links data may represent citations, friendships, associations, metabolic
functions, communications, co-locations, shared mechanisms, many explicit implicit relationships.
Statistical relational learning (SRL) methods developed address problems reasoning learning domains complex relations probabilistic structure
(Getoor & Taskar, 2007). particular, SRL algorithms leverage relational information
attempt learn models higher predictive accuracy. key characteristic many
relational datasets correlation statistical dependence values
attribute across linked instances (e.g., two friends likely share political views
two randomly selected people). relational autocorrelation provides unique opportunity increase accuracy statistical inferences (Jensen, Neville, & Gallagher,
2004). Similarly, relational information exploited many reasoning tasks
identifying useful patterns optimizing systems (Easley & Kleinberg, 2010).
Representation issuesincluding knowledge, model, data representationhave
heart artificial intelligence community decades (Amarel, 1968; Minsky, 1974;
Russell & Norvig, 2009). important, focus data representation issues, simple examples include choices whether discretize continuous
features add higher-order polynomial features. decisions significant
effect accuracy efficiency AI algorithms. especially critical
performance SRL algorithms because, relational domains, even larger space
potential data representations consider. complex structure relational data
often represented variety ways choice specific data representation
impact applicability particular models/algorithms performance.
Specifically, two categories decisions need considered context
relational data representation.
First, consider type data representation use (cf., hierarchy
De Raedt, 2008, ch. 4). instance, relational data propositionalized
application standard, non-relational learning algorithms. often, order fully
364

fiTransforming Graph Data Statistical Relational Learning

exploit relational information, SRL researchers chosen represent data either
using attributed graph relational database (see e.g., Friedman, Getoor, Koller, &
Pfeffer, 1999), via logic programs (see e.g., Kersting & De Raedt, 2002).1 choice
different strengths. article, focus graph-based representation,
common choice addressing growing interest network data applications analyzing electronic communication online social networks Facebook,
Twitter, Flickr, LinkedIn (Mislove, Marcon, Gummadi, Druschel, & Bhattacharjee,
2007; Ahmed, Berchmans, Neville, & Kompella, 2010). Specifically, assume graphbased data representation G = hV, E, XV , XE nodes V entities (e.g., people,
places, events) links E represent relationships among entities (e.g., friendships, citations). XV set features entities V . Likewise, set features
XE provides information relation links E.
Next, given type representation, must consider specific content data
representation, large space choices. instance, features nodes
links graph constructed using wide range aggregation functions, based
multiple kinds links paths. SRL researchers already recognized importance
data representation choices (e.g., Getoor & Diehl, 2005), many separate studies
examined techniques feature construction (Neville, Jensen, Friedland, & Hay, 2003),
node weighting (Tang, Musolesi, Mascolo, & Latora, 2009), link prediction (Taskar, Wong,
Abbeel, & Koller, 2003), etc. However, article first comprehensively survey
approaches relational representation transformation graph-based data.
Given set (graph-based) relational data, define relational representation transformation change space links, nodes, and/or features used represent
data. Typically, goal transformation improve performance
subsequent SRL application. instance, Figure 1 original graph representation G
transformed new representation G links, nodes, features (such link
weights) added, links removed. SRL algorithm
analysis applied new representation, instance classify nodes
identify anomalous links. particular transformations used produce G
vary depending upon intended application, sometimes substantially improve
accuracy, speed, complexity final application. instance, Gallagher, Tong,
Eliassi-Rad, Faloutsos (2008) found adding links similar nodes could increase node classification accuracy 15% tasks. Similarly, Neville
Jensen (2005) demonstrated adding nodes represent underlying groups enabled
simpler inference increased accuracy.
1.2 Scope Article
article focuses examining categorizing various techniques changing
representation graph-based relational data. shown Figure 1, typically view
changes pre-processing step enables increased accuracy speed
task, object classification. However, output techniques
valuable. instance, administrators social network may interested
1. latter case, applicable SRL algorithms often referred probabilistic inductive logic
programming (ILP) (De Raedt & Kersting, 2008).

365

fiRossi, McDowell, Aha, & Neville

C

C
SRL Analysis /
Application

Representation
Transformation

L
L

L

G


G


Result

Figure 1: Example Transformation Subsequent Analysis: original relational representation G transformed G dotted lines represent predicted links, squares represent predicted nodes, bold links represent link
weighting. Changes may based link structure, link features, node features (here, similar node shadings indicate similar feature values). SRL
analysis applied new representation. example, SRL
analysis produces label (C L) node, example task discussed Section 2.1. article focuses representation transformation
(left side figure), subsequent analysis.

link prediction predicted links presented users potential new
friendship links. Alternatively, techniques may also applied improve
comprehensibility model. example, prediction protein-protein interactions
provides insights protein function (Ben-Hur & Noble, 2005). Thus, techniques
survey may used multiple purposes, relevant publications may used
different contexts. Regardless original context, examine general
applicability benefits technique. techniques applied,
transformed data used (e.g., friendship suggestions), examined greater
understanding, used task (e.g., object classification), used recursively
input another representation change (e.g., object/node prediction followed
link prediction).
attempt survey many methods could used SRL analysis
(e.g., right side Figure 1), although relevant set methods analysis
overlaps set methods facilitate transformations consider. instance, collective classification (Neville & Jensen, 2000; Taskar, Abbeel, & Koller, 2002)
important SRL application define Section 2 use running example
SRL analysis task. output classification could also used create
new attributes nodes (a data representation change). discuss possibility
Section 6.2, focus cases node labeling particularly useful
pre-processing step (e.g., applying certain stacked algorithms), rather surveying wide range possible classification algorithms, whether collective not. Likewise,
survey issues model knowledge representation, whether sta366

fiTransforming Graph Data Statistical Relational Learning

tistical dependencies nodes, links, features modeled Structural
Logistic Regression (Popescul, Popescul, & Ungar, 2003b) Markov Logic Network
(Domingos & Richardson, 2004). consider issues briefly, Section 8.4.
Furthermore, focus transformations change content graph data
representation. particular, examine transformations graph data modify
set links nodes, modify features. consider changing graph data
different type representation, e.g., propositionalizing data changing
logic program. However, transformations discuss, node link
feature aggregation, form propositionalization. addition, Section 6.3.3 describes
number techniques structure learning logic programs, techniques
closely related analogous problem feature construction graph-based representations. Finally, many techniques discuss also applicable
logical representations. instance, link weighting could applied weight known
relations using logic program detect anomalous objects. focus, however,
methods useful transforming graph-based representations.
1.3 Approach Organization Article
many dimensions relational data transformation, complicate task
understanding selecting appropriate techniques. assist process,
introduce simple intuitive taxonomy representation transformation identifies link transformation node transformation symmetric representation tasks.
specifically, transformation tasks nodes links include (i) predicting
existence, (ii) predicting label type, (iii) estimating weight importance,
(iv) constructing relevant features. addition, propose taxonomy constructing link node features consists non-relational features, topology features,
relational node-value features, relational link-value features. relational transformation task, survey applicable techniques, examine necessary conditions,
provide detailed examples comparisons.
article organized follows. next section presents taxonomy relational
representation transformation discusses motivating example. Section 3, review
algorithms link prediction, Section 4 examines task link interpretation
(i.e., constructing link labels, link weights, link features). Sections 5 6 consider
corresponding prediction interpretation tasks nodes instead links. Section
7, summarize algorithms jointly transform nodes links. Section 8 discusses
methods evaluating representation transformations challenges future work,
Section 9 concludes.

2. Overview Motivating Example
section first introduce running example based classification data
Facebook, describe relational algorithms could used perform task.
Next, introduce taxonomy relational representation transformation explain
type transformation could aid Facebook classification task. Finally,
formally define type relational representation transformation.
367

fiRossi, McDowell, Aha, & Neville

2.1 Motivating SRL Analysis Example: Classification Task
example, consider hypothetical data inspired Facebook (www.facebook.com),
one popular online social networks. assume given graph
G = hV, E, XV , XE nodes V users 2 links E represent friendships
Facebook. XV set features users V gender, relationship
status, school, favorite movies, musical preference (though information may missing
users). Likewise, set features XE provides information friendship
links E time formation possibly contents message
sent link formation requested one users.
example SRL analysis task (see Figure 1) predict political affiliation (liberal,
moderate, conservative) every node (person) G. assume affiliation,
call class label node, known people G.3
Moreover, assume users political affiliation likely correlated
characteristics user (to lesser degree) users friends. next section
summarizes correlations used classification.
example, assume links simple, binary friendship connections. However, link types could used represent kinds relationships. instance,
link might indicate two people communicated via wall-post message,
two people chosen join Facebook group. addition, notion friendship Facebook weak thus significant portion persons friends often
casual acquaintances. Thus, representation changes link deletion weighting
may significant impact classification accuracy. notational purposes, add
tilde top graph components symbol indicate undergone
transformation (e.g., modified link set E denoted E).
2.2 Background: Features Methods Classification
predict political affiliation Facebook users, conventional classification approaches
would ignore links classify user using information known user,
gender location. assume information represented
form non-relational features, features computed directly
XV without considering links E. refer classification based
features non-relational classification. Alternatively, relational classification, links
explicitly used construct additional relational features capture information
users friends. instance, relational feature could compute, user,
proportion friends male live particular region. Using relational
information potentially increase classification accuracy, though may sometimes decrease
accuracy well (Chakrabarti, Dom, & Indyk, 1998). Finally, even greater (and usually
reliable) increases occur class labels (e.g., political affiliations)
linked users used instead derive relevant features (Jensen et al., 2004). instance,
2. general, may one type node. instance, nodes citation network may
represent papers authors.
3. Later, discuss representation change node labeling, also constructs estimated label
every node. discussed Section 1.2, representation changes sometimes resemble output
SRL analysis, focus changes particularly useful pre-processing
subsequent SRL analysis.

368

fiTransforming Graph Data Statistical Relational Learning

class-label relational feature could compute, user, proportion friends
liberal views. However, using features challenging since
labels initially unknown, thus typically must estimated iteratively
refined way. process jointly inferring labels interrelated nodes
known collective classification (CC).
CC requires models inference procedures use inferences one user
affect inferences related users. Many algorithms considered CC,
including Gibbs Sampling (Jensen et al., 2004), relaxation labeling (Chakrabarti, Dom, &
Indyk, 1998), belief propagation (Taskar et al., 2002), ICA (Neville & Jensen, 2000; Lu &
Getoor, 2003), weighted neighbor techniques (Macskassy & Provost, 2007). See
work Sen et al. (2008) survey.
concrete example SRL analysis, explain many techniques survey
terms Facebook classification task, special emphasis CC. However,
features transformation techniques apply many SRL tasks data sets
relationship classification, anomalous link detection, entity resolution, group
discovery (Getoor & Diehl, 2005).
2.3 Representation Transformation Tasks Improving SRL
Figure 2 shows proposed taxonomy relational representation transformation.
two main tasks taxonomy link transformation node transformation.
find powerful elegant symmetry two tasks. particular,
link node representation transformation tasks decomposed prediction
interpretation tasks. former task involves predicting existence new nodes
links. latter task interpretation involves three parts: constructing weights,
labels, features nodes links. Together, yields eight distinct transformation tasks
shown leaves taxonomy Figure 2. Underneath eight tasks
figure, list primary graph component modified task (i.e., V , E, XV ,
XE ), followed illustration possible representation change task.
text below, summarize Figure 2, organized around four larger categories link
prediction, link interpretation, node prediction, node interpretation.
First, link prediction adds new links graph. sample graph task
(Figure 2A) shows link predicted similarity two nodes
used predict new link them. Intuitively, Facebook users share values
many non-relational features may also share political affiliation. Thus, adding
links people increase autocorrelation improve accuracy collective classification. many simple link prediction algorithms based similarity,
neighbor properties, shortest path distances, infinite sums paths (i.e. random walks),
strategies. Section 3 provides detail techniques.
Second, several types link interpretation, involves constructing
weights, labels, features existing links. instance, many graphs (including
Facebook data), links (or friendships) equal importance. Thus, Figure 2B
shows result performing link weighting. case, weights based similarity feature values pair linked nodes, assumption
high similarity may indicate stronger relationships. (Link prediction techniques may also
369

fiRossi, McDowell, Aha, & Neville

Relational
Representation
Transformation

input


E

weighted link

labeled link

predicted node

weighted node

labeled node

Node Transformation

Link Interpretation

Node Prediction

Link Weighting

Link Labeling

Link Feature
Construction

X E

X E

X E

V

Node Interpretation

Node Weighting

Node Labeling

Node Feature
Construction

X V

X V

X V

p
p

B.

C
w

w

A.

p

L

Link Transformation

Link Prediction

predicted link

2

+

3

3

p

C.

-

C



D.

E.

F.

L
G.

.1 B

.2

L

-

.3 B

.5

.3

H.

Figure 2: Relational Representation Transformation Taxonomy: Link node
transformation formulated symmetric tasks leading four main transformation tasks: predicting links, interpreting links, predicting nodes, interpreting nodes. task yields modified graph component: E, XE , V ,
XV , respectively. Interpretation divided weighting, labeling,
constructing features. Examples tasks relational representation
transformation shown leaves taxonomy. example
graphs, nodes similar shadings similar feature values.

370

fiTransforming Graph Data Statistical Relational Learning

use similarity measures, identifying probable new links, rather weighting
existing links.) Alternatively, link labeling may used assign kind discrete label
link. instance, Figure 2C shows links might labeled either personal
(p) work (w) related, e.g., based known feature values analysis communication events linked users. hand, links might instead labeled
positive negative influence (i.e., labeled +/). Finally, Figure 2D shows
link feature construction used add general kinds feature values
link. instance, link feature might count number communication events
occurred two people number friends common. Link weighting
labeling could perhaps viewed special cases link feature construction,
separate later sections show useful techniques task
differ. three link interpretation tasks could help example classification
problem. particular, model learned predict political affiliation might choose place
special emphasis links highly weighted labeled personal.
link features might used represent complex dependencies, instance modeling influence users work friendships, friendship links nodes
large number friends common. details techniques
provided Section 4.
Third, node prediction adds additional nodes (and associated links) graph.
instance, Figure 2E shows result relational clustering applied
discover two latent groups graph, user connected one latent
group node. discovered node Facebook might represent types social processes,
influences, tightly knit group friends. clustering techniques used
identify new nodes could designed identify people particularly similar
respect relevant characteristic, political affiliation. new nodes
associated links could used several ways. instance, though present
small example Figure 2E, nodes far away (in terms shortest
path length) original graph may much closer new graph. Thus, links
latent node may allow influence propagate effectively algorithm
CC applied. Alternatively, identification distinct latent groups may even enable
efficient accurate algorithms applied separately group (Neville & Jensen,
2005). Node prediction discussed Section 5.
Finally, several types node interpretation, involves constructing
weights, labels, feature values existing nodes. instance, links,
nodes may influential others thus weight. Figure 2F
demonstrates node weighting, weights might assigned based numbers
friends via PageRank/eigenvector techniques. See Section 6.1 details.
Alternatively, Figure 2G shows example node labeling. graph represents
training graph, node given estimated label conservative (C),
liberal (L), moderate (M). labels might estimated using non-relational
features via textual analysis. classification algorithms learn model based
true labels training graph, approaches instead first compute estimated
labels, learn model new representation (Kou & Cohen, 2007). Section 6.2
discusses simplify inference. Finally, Figure 2H shows result node feature
construction, arbitrary feature values added node. instance, suppose
371

fiRossi, McDowell, Aha, & Neville

find users relatively Facebook friends often moderate
many friends often liberal. case, feature counting number friends
node would useful. directly exploit autocorrelation, different feature might
count proportion users friends conservative, common political
affiliation users friends. feature correlated political affiliation could
used improve performance classification algorithm example problem.
Identifying and/or computing features essential performance SRL
algorithms challenging; Section 6.3 considers process.
Table 2.3, summarize prominent techniques performing
tasks link prediction, link interpretation, node prediction, node interpretation.
Sections 3-6 provide detail category turn.
2.4 Relational Representation Transformation: Definitions Terminology
assume initial relational data represented graph G = hV, E, XV , XE
vi V corresponds node edge eij E corresponds
(directed) link nodes j. XV set features nodes V ,
XkV XV k th feature. Likewise, XE set features links
E, XkE k th feature. features XE could refer link weights,
distances, types, among possibilities. preceding notation lets us identify,
instance, values particular feature XkV nodes. Alternatively, xvi refers
vector containing feature values particular node vi , xeij contains
feature values particular edge eij . Table 2.3 summarizes notation.
Relational representation transformation process transforming original
graph G new graph G = hV , E, XV , XE arbitrary set transformation techniques. process, nodes, links, weights, labels, general features may
added, nodes links may removed. theory, transformation seeks
optimize objective function (for instance, maximize autocorrelation), although
practice objective function may completely specified guaranteed improved transformation. define specifically four primary parts
relational representation transformation:
Definition 2.1 (Link Prediction) Given nodes V , observed links E and/or feature
set X = (XE , XV ), link prediction task defined creation modified link set
E E 6= E. Usually, involves adding new links present E,
links may also deleted.
Definition 2.2 (Link Interpretation) Given nodes V , observed links E and/or
feature set X = (XE , XV ), link interpretation task defined creation new
link feature XkE XkE
/ XE . task may estimate feature value every link.
Alternatively, values XkE may partially estimated, example, original
features missing values additional links also introduced link prediction.
Definition 2.3 (Node Prediction) Given nodes V , links E and/or feature set
X = (XE , XV ), node transformation defined creation modified node set V
V V . addition, many node prediction tasks simultaneously create new links,
372

fiTransforming Graph Data Statistical Relational Learning

Relational Representation Transformation
Links
?

Prediction

?
?

?

Weighting

?

?

?
?

Labeling

?
?

Feature

Nodes

Adamic/Adar (Adamic &
Adar, 2001), Katz (Katz, 1953),
others (Liben-Nowell &
Kleinberg, 2007)
Text Feature Similarity
(Macskassy, 2007)
Classification
via
RMN
(Taskar et al., 2003) SVM
(Hasan, Chaoji, Salem, & Zaki,
2006)
Latent Variable Estimation (Xiang, Neville, & Rogati,
2010)
Linear Combination Features (Gilbert & Karahalios,
2009)
Aggregating Intrinsic Information (Onnela, Saramaki,
Hyvonen, Szabo, Lazer, Kaski,
Kertesz, & Barabasi, 2007)
LDA (Blei et al., 2003), PLSA
(Hofmann, 1999),
Link Classification via Logistic Regression (Leskovec, Huttenlocher, & Kleinberg, 2010),
Bagged Decision Trees (Kahanda & Neville, 2009),
Link
Feature
Similarity
(Rossi & Neville, 2010)
Link Aggregations (Kahanda
& Neville, 2009)

?

Graph Features (Lichtenwalter, Lussier, & Chawla, 2010)

?

?
?

?

Betweenness (Freeman, 1977),
Closeness (Sabidussi, 1966)

?

HITs (Kleinberg, 1999), Prob.
HITs (Cohn & Chang, 2000),
SimRank (Jeh & Widom, 2002)
PageRank (Page, Brin, Motwani, & Winograd, 1999), Topical PageRank (Haveliwala, 2003;
Richardson & Domingos, 2002)
LDA (Blei et al., 2003), PLSA
(Hofmann, 1999),
Node
Classification
via
Stacked Model (Kou & Cohen, 2007) RN (Macskassy &
Provost, 2003)

?

?
?

?
?

Construction
?

Spectral Clustering (Neville
& Jensen,
2005),
MixedMembership
Relational
Clustering (Long et al., 2007)
LDA (Blei, Ng, & Jordan, 2003),
PLSA (Hofmann, 1999),
Hierarchical Clustering via
Edge-betweenness (Newman &
Girvan, 2004)

MLN Structure Learning (Kok
& Domingos, 2009, 2010)
Database
Query
Search
(Popescul et al., 2003b), RPT
(Neville, Jensen, Friedland, et al.,
2003)
FOIL, nFOIL (Landwehr, Kersting, & De Raedt, 2005), kFOIL
(Landwehr, Passerini, De Raedt,
& Frasconi, 2010), Aleph (Srinivasan, 1999),

Table 1: Summary Techniques: summary prominent graph transformation techniques tasks predicting existence nodes links interpreting
weighting, labeling, constructing general features.

373

fiRossi, McDowell, Aha, & Neville

Symbol

Description

G

Initial graph

G

Transformed graph

E

Initial link set

V

Initial node set

E

Initial set link features

V

Initial set node features

X
X

XkE
XkV
xeij
xvi
symbols

(vi )


Initial link feature k (XkE XE ) (for one feature, values links)
Initial node feature k (XkV XV ) (for one feature, values nodes)
Initial feature vector eij (for one link, values link features)
Initial feature vector vi (for one node, values node features)
Description
Adjacency matrix graph
Neighbors vi
Cut-off value

Table 2: Summary Notation used Survey: top half table shows
symbols sometimes written tilde top symbol, indicating
result transformation. conciseness, table demonstrates
notation G G.

e.g., initial node vi V predicted node vj V . Thus, task may also
produce modified link set E.
Definition 2.4 (Node Interpretation) Given nodes V , observed links E and/or
feature set X = (XE , XV ), node interpretation task defined creation new
/ XV . link interpretation, values XkV may
node feature XkV XkV
estimated nodes. node feature XkV could represent node weights,
labels, general features.
Section 2.2 introduced notion non-relational feature, node feature
XkV constructed without making use links (i.e., without using E XE ).
features sometimes referred articles attributes intrinsic features.
important terms also referred multiple different ways. aid reader,
Table 2.4 summarizes key synonyms terms found often
literature.

3. Link Prediction
section focuses predicting existence links Section 4 considers link
interpretation. Given initial graph G = hV, E, XV , XE i, interested creating
modified link set E, usually prediction new links present
374

fiTransforming Graph Data Statistical Relational Learning

Term

Potential synonyms

Nodes

Vertices, points, objects, entities, individuals, users, constants, ...

Links

Edges, relationships, ties, arcs, events, interactions, predicates

Topology

Link/network/graph structure, relational information

Features

Attributes, variables, co-variates, queries, predicates, ...

Graph Measures

Topology-based metrics (such proximity, centrality, betweenness, ...)

Similarity

Distance (the inverse similarity), likeness

Clusters

Classes, communities, groups, roles, topics

Non-relational Features

Intrinsic attributes/features, local attributes/features, ...

Relational Features

Features, link-based features, graph features, aggregates, queries, ...

Structure Learning

Feature generation/construction, hypothesis learning

Parameter Learning

Model selection, function learning

Table 3: Synonyms Literature: summary possible synonyms found
literature important terms related relational data.

E. task motivated several ways. instance, may need
predict missing links present E incomplete data collection
problems. Similarly, may interested predicting hidden links,
assume exists unobservable interactions goal discover
model interactions. example, network representing criminals terrorist
activity, may seek predict link two people (nodes) directly
connected whose actions share common motivation cause. missing
hidden links, predicting links may improve accuracy subsequent learned
model. Alternatively, may seek predict future links evolving network,
new friendships connections formed next year. might also interested
predicting links objects spatially related. Finally, may wish predict
beneficial links, instance, predicting pairs individuals likely successful
working together.
Figure 3 summarizes one general approach often used link prediction
tasks. summary, scores weights computed every pair nodes graph,
shown Figure 3(b). Predicted links weight greater threshold , along
original links, used create new link set E + (shown Figure 3(e)). (At
step, original links low weight could also deleted appropriate.)
final step, weights predicted links often discarded, yielding new graph
uniform link weights shown Figure 3(f).
key challenge approach compute weight score possible
link. information used computation provides natural way categorize
link prediction techniques. Below, Section 3.1 describes techniques use
non-relational features nodes (ignoring initial links), Section 3.2 describes
topology-based techniques use graph structure (i.e., links relations).
375

fiRossi, McDowell, Aha, & Neville

(a) Initial Graph G = hE, V

(b) Weighted Links wij E

(c) Predicted Links (E E)

(d) Pruning Predicted Links
(E > )

(e) E + := E > + E

(f) E + Uniform Link
Weights

Figure 3: Example Demonstrating General Approach Link Prediction:
initial graph (a) used input link predictor, yielding complete
graph (b) weights wij estimated pairs nodes.
next step shows removal initial (observed) links consideration (c),
followed pruning predicted links weight cut-off value
(d). remaining predicted links combined initial links (e).
Often, estimated weights initial predicted links discarded,
leaving uniform weight graph (f).

Finally, Section 3.3 describes hybrid techniques exploit node features
graph structure.
3.1 Non-relational (Feature-Based) Link Prediction
section, consider link predictors exploit graph structure relational features derived using graph structure. given arbitrary pair nodes
376

fiTransforming Graph Data Statistical Relational Learning

vi vj graph node represented feature vector xvi
xvj , respectively. Feature-based link prediction defined using arbitrary similarity
measure S(xvi , xvj ) means estimate likelihood link exist vi
vj . Typically, link created similarity exceeds fixed cut-off value; another
strategy predict links among n% node pairs highest similarity.
traditional approach simply define measure similarity two objects,
possibly based knowledge application and/or problem-domain. many
similarity metrics proposed mutual information, cosine similarity,
many others (Lin, 1998). instance, Macskassy (2007) represents textual content
node feature vector uses cosine similarity create new links nodes
graph. Macskassy showed combination initial links predicted
text-based links increased classification accuracy compared using initial links
text-based links. addition leveraging textual information predict links,
might use arbitrary set features combined proper measure similarity
link prediction. instance, many recommender systems implicitly predict link
two users based similarity ratings items movies books
(Adomavicius & Tuzhilin, 2005; Resnick & Varian, 1997). case, cosine similarity
correlation commonly used similarity metrics.
Alternatively, similarity measure learned predicting link existence. link
prediction problem transformed standard supervised classification problem
binary classifier trained determine similarity two nodes based
feature vectors. One approach work Hasan et al. (2006), used
Support Vector Machines (SVMs) link prediction found non-relational feature
(keyword match count) useful predicting links bibliographic network.
many link prediction approaches (Taskar et al., 2003; Getoor, Friedman, Koller,
& Taskar, 2003) apply traditional machine learning algorithms. However,
use features based graph structure well non-relational features
focus section. Thus, discuss techniques Section 3.3.
Finally, variants topic models used link prediction. types models
traditionally use text documents (non-relational information) infer mixture latent topics document. Inter-document topic similarity used
similarity metric link prediction (Chang & Blei, 2009). However, many topic
models capable performing joint transformation nodes links, defer full
discussion techniques Section 7.
3.2 Topology-Based Link Prediction
Topology-based link prediction uses local relational neighborhood and/or global
graph structure predict existence unobserved links. Table 3.2 summarizes
common metrics used task. Below, discuss many
approaches, starting simplest local metrics moving complex
techniques based global measures and/or supervised learning. systematic study
many approaches applied social network data, see work Liben-Nowell
Kleinberg (2007).
377

fiRossi, McDowell, Aha, & Neville

Local Node Metrics

Description

Common Neighbors

Number common neighbors x y, w(x, y) = |(x)(y)| (Newman,
2001a)

Jaccards Coefficient

Probability x share common neighbors (normalized),
|(x)(y)|
(Jaccard, 1901; Salton & McGill, 1983)
|(x)(y)|

Adamic/Adar

Similar toPcommon neighbors, assigns weight rare neighbors,
1
w(x, y) = z(x)(y) log |(z)|
(Adamic & Adar, 2001)

RA

Essentially equivalent Adamic/Adar |(z)| small,
P
1
w(x, y) = z(x)(y) |(z)|
(Zhou, Lu, & Zhang, 2009)

Preferential Attachment

Probability link x product degree x y,
w(x, y) = |(x)| |(y)| (Barabasi & Albert, 1999)

Cosine Similarity

|(x)(y)|
w(x, y) =

(Salton & McGill, 1983)

Sorensen Index

w(x, y) =

(Green, 1972; Zhou et al., 2009)

Hub Index

Nodes large degree likely assigned higher score,
w(x, y) =

|(x)||(y)|
2|(x)(y)|
|(x)|+|(y)|

|(x)(y)|
min{|(x)|,|(y)|}

w(x, y) =

(Ravasz, Somera, Mongru, Oltvai, & Barabasi, 2002)
|(x)(y)|
max{|(x)|,|(y)|}

(Ravasz et al., 2002)

Hub Depressed Index

Analogous Hub Index, w(x, y) =

Leicht-Holme-Newman

Assigns large weight pairs many common neighbors, normalized
|(x)(y)|
expected number common neighbors, w(x, y) = |(x)||(y)| (Leicht,
Holme, & Newman, 2006)

Global Graph Metrics

Description

Graph Distance

Length shortest path x

Katz

Number paths x y, exponentially damped length thereby
assigning weight shorter paths, w(x, y) = [(I A)1 ]xy (Katz, 1953)

Hitting time

Number steps required random walk starting x reach (Brightwell
& Winkler, 1990)

Commute Time

Expected number steps reach node starting x returning
+
+
back x, defined w(x, y) = L+
xx + Lyy 2Lxy L Laplacian matrix
(Gobel & Jagers, 1974)

Rooted PageRank

Similar Hitting time, step probability random
walk reset starting node x, w(x, y) = [(IP)1 ]xy P = D1
(Page et al., 1999)

SimRank

x similar extent joined similar neighbors,
P

w(x, y) =

P

v(y) sim(u,v)
|(x)||(y)|

u(x)

(Jeh & Widom, 2002)

K-walks

Number walks length k x y, defined w(x, y) = [Ak ]xy

Meta-Approaches

Description

Low-rank Approximation

Compute rank-k matrix Ak best approximates (hopefully reducing
noise), compute similarity Ak using local global metric
(Eckart & Young, 1936; Golub & Reinsch, 1970)

Unseen Bigrams

Compute initial scores using local global metric, augment scores
w(x, y) using values w(z, y) nodes z similar x (Essen &
Steinbiss, 1992; Lee, 1999)

Clustering

Compute initial scores using local global metric, discard links
lowest scores, re-compute scores modified graph (Johnson,
1967; Hartigan & Wong, 1979)

Table 4: Topology Metrics: Summary common metrics link prediction.
Notation: Let (x) neighbors x adjacency matrix G.
378

fiTransforming Graph Data Statistical Relational Learning

3.2.1 Metrics Based Local Neighborhood Nodes
simplest approaches use local neighborhood nodes graph devise
measure topology similarity, use pairwise similarities nodes predict
likely links. shown Table 3.2, numerous metrics, often based
number neighbors two nodes share common, varying strategies
normalization.
Zhou et al. (2009) compares nine local similarity measures six datasets finds
simplest link predictor, common neighbors, performs best overall. also
propose new metric, RA, outperforms initial nine metrics two datasets.
new metric similar Adamic/Adar metric, uses different normalization factor yields better performance networks higher average degree.
also propose method uses additional two-hop information avoid degenerate cases
links assigned similarity score. results highlight importance
selecting appropriate metrics specific problems datasets. another related
investigation, Clauset, Moore, Newman (2008) evaluate hierarchical random graph
predictor local topology metrics common neighbors, Jaccards coefficient
degree product three types networks: metabolic, ecology social network.
find baseline measure based shortest paths performs best metabolic
network, relationships homogeneous, hierarchical metric
performs best links create complex relationships, predator-prey
relationships found ecology network.
Liu Lu (2010) proposed local random-walk algorithm efficient alternative
global random-walk predictors large networks. method evaluated alongside
metrics (i.e., common neighbors, local paths, RA, random-walk variants)
shown perform better networks efficiently global
random-walk models.
3.2.2 Metrics Based Global Graph Structure
sophisticated similarity metrics based global graph properties, often involving
weighted computation based number paths pair nodes.
instance, Katz measure (1953) counts number paths pair nodes,
shorter paths count computation. Rattigan Jensen (2005) demonstrated even fairly simple metric could effective task anomalous link
prediction, identification statistically unlikely links among links
initial graph.
related measure hitting time metric, average number steps
required random walk starting node x reach node y. Gallagher et al. (2008)
use random walks restart estimate similarity every pair nodes.
focus sparsely labeled networks unlabeled nodes may labeled
nodes support learning and/or inference relational classification. prediction
new links improves flow information labeled unlabeled nodes, leading
increase classification accuracy 15%. Note adding teleportation probabilities
random walk approach roughly yields PageRank algorithm said
heart Google search engine (Page et al., 1999).
379

fiRossi, McDowell, Aha, & Neville

SimRank metric (Jeh & Widom, 2002) proposes two nodes x similar
linked neighbors similar. Interestingly, show approach
equivalent metric based time required two backwards, random walks
starting x arrive node. approaches based
random walks, metric could computed via repeated simulations,
efficiently computed via recursive set-point approach.
3.2.3 Meta-approaches Supervised Learning Approaches
metrics modified combined multiple ways. Liben-Nowell Kleinberg (2007) consider several meta-approaches use local global similarity
metric subroutine. instance, metrics discussed defined
terms arbitrary adjacency matrix A. Given formulation, imagine first
computing low-rank approximation Ak matrix using technique singular
value decomposition (SVD), computing local global graph metric using
modified Ak . idea Ak retains key structure original matrix, noise
reduced. Liben-Nowell Kleinberg also propose two meta-approaches
based removing spurious links suggested first round similarity computation (the
clustering approach) based augmenting similarity scores node x based
scores nodes similar x (the unseen bigrams approach). compare performance three meta-approaches vs. multiple local global metrics
task predicting future links social network. Katz measure metaapproaches based clustering low-rank approximation perform best three
five arXiv datasets, simple local measures common neighbors Adamic/Adar
also perform surprisingly well.
Supervised learning methods also used combine augment similarity
metrics discussed. instance, Lichtenwalter et al. (2010) investigate several
supervised methods link prediction sparsely labeled networks, using many metrics Table 3.2. metrics used features simple classifiers C4.5,
J48, naive Bayes. find supervised approach leads 30% improvement
AUC simple unsupervised link prediction metrics. Similarly, Kashima Abe
(2006) propose supervised probabilistic model assumes biological network
evolved time, uses topological features estimate model parameters. evaluate proposed method protein-protein metabolic networks
report increased precision compared simpler metrics Adamic/Adar, Preferential
Attachment, Katz.
3.2.4 Discussion
general, local topology metrics sacrifice amount accuracy computational
gains global graph metrics may perform better costly estimate
infeasible huge networks. appropriate, supervised methods combine multiple
local metrics may offer promising alternative. next subsection discusses additional
work link prediction used supervised methods.
Link prediction using metrics especially sensitive characteristics
domain application. instance, many networks biology, identification
380

fiTransforming Graph Data Statistical Relational Learning

links costly, contain missing incomplete links, removal insignificant links
significant issue social networks. reason, researchers analyzed
proposed many different metrics working domains web analysis (Kleinberg,
1999; Broder et al., 2000), social network analysis (Zheleva, Getoor, Golbeck, & Kuter,
2010; Xiang et al., 2010; Koren, North, & Volinsky, 2007), citation analysis (Borgman &
Furner, 2002), ecology communities (Zhou et al., 2009), biological networks (Jeong et al.,
2000), many others (Barabasi & Crandall, 2003; Newman, 2003).
3.3 Hybrid Link Prediction
subsection, examine approaches perform link prediction using
attributes graph topology. approaches, two key questions. First,
kinds features used? Second, information multiple
features combined single measure probability used prediction?
first consider mix non-relational relational features used.
expected, best features vary based domain specific network. instance,
Taskar et al. (2003) studied link prediction network web pages found simple
local topology metrics (which called transitivity similarity) important
non-relational features based words presents pages. Similarly, Hasan
et al. (2006) found another topology metric (shortest distance) useful
predicting co-authorship links bibliographic network based DBLP.
single metric/feature, hitting time, used link prediction,
must ensure metric works well nodes yields consistent ranking.
However, multiple feature values combined way, may
acceptable use wider range features, especially supervised learner later select
weight important features based training data. Thus, hybrid systems
link prediction tend diverse feature set. instance, Zheleva et al.
(2010) propose new features based combining two different kinds networks (social
affiliation networks). Features based groups topology constructed
combined network used along descriptive non-relational features, yielding
improvement 15-30% compared system without combined-network features.
second example complex features provided Ben-Hur Noble (2005),
design new pairwise kernel predicting links proteins (protein-protein
interactions). pairwise kernel tensor-product two linear kernels original
feature space, especially useful domains two nodes might
common features. approach also applied user preference prediction
recommender systems (Basilico & Hofmann, 2004). Vert Yamanishi (2005) propose
related approach, supervised learning used create mapping original
nodes new euclidean space simple distance metrics used link
prediction.
Given great diversity possible features link prediction, interesting approach
system automatically searches relevant features use. example, Popescul,
Popescul, Ungar (2003a) propose unique link prediction approach systematically
generates searches space relational features learn potential link predictors. use logistic regression link prediction consider search space covering
381

fiRossi, McDowell, Aha, & Neville

equi-joins, equality selections, aggregation operations. approach, model selection algorithm continues add one feature time model long Bayesian
Information Criterion (BIC) score training set improved. find
search algorithm discovers number useful topology-based features, co-citation
bibliographic coupling, well complex features. However, complexity
searching large feature space avoiding overfitting present challenges.
next consider second key question: information multiple
features combined single measure used link prediction? prior
work taken supervised learning approach, non-relational topologybased metrics used features describe possible link. supervised
techniques discussed Section 3.2, model learned training data
used predict unseen links.
supervised approaches apply classifier separately possible link,
using classifier support vector machine, decision tree, logistic regression
(Popescul et al., 2003a; Ben-Hur & Noble, 2005; Hasan et al., 2006). approaches,
flat feature representation link created, prediction made
possible link independent predictions.
contrast, early work Relational Bayesian Networks (RBNs) (Getoor et al., 2003)
Relational Markov Networks (RMNs) (Taskar et al., 2003) involved joint inference
computation link prediction, prediction could influenced nearby link
predictions (and sometimes also newly predicted node labels). Using webpage network
social network, Taskar et al. demonstrated joint inference using belief propagation could improve accuracy compared independent inference approach. However,
approach computationally intensive, noted getting belief propagation
algorithm converge significant problem. possible solution computational
challenge simpler approach presented Bilgic, Namata, Getoor (2007).
method involved repeatedly predicting labels node, predicting links
nodes using available features (including predicted labels), re-predicting labels
new links, forth. link prediction based independent inference
step using logistic regression, simpler approaches discussed above. However,
repeated application step allows possibility link feature values changing
iterations based intermediate predictions, thus allowing link predictions
influence other.
Recently, Backstrom Leskovec (2011) proposed novel approach supervised,
final predictions based random walk rather directly
output learned classifier. Given particular target node v social network,
along nodes known link v, study predict
links v likely arise future (or recommended). define
simple link features based node profile similarity messaging behavior,
use features estimate initial link weights. show learn weights
(or transition probabilities) manner optimizes likelihood subsequent
random walk, starting v, arrive nodes already known link v.
random walk thus guided links already known exist, call
process supervised random walk. argue learning process greatly reduces
need manually specify complex graph-based features, show outperforms
382

fiTransforming Graph Data Statistical Relational Learning

supervised approaches well unsupervised approaches Adamic/Adar
measure.
final approach link prediction use kind unsupervised dimensionality
reduction yields new matrix way reveals possible new links. instance,
Hoff, Raftery, Handcock (2002) propose latent space approach initial link
information projected low-dimensional space. Link existence predicted
based spatial representation nodes new latent space. models
perform kind factorization link adjacency matrix thus often referred
matrix factorization techniques. advantage models spatial representation enables simpler visualization human interpretation. Related approaches
also proposed temporal networks (Sarkar & Moore, 2005), mixed-membership
models (Nowicki & Snijders, 2001; Airoldi, Blei, Fienberg, & Xing, 2008), situations
latent vector representing node usefully constrained binary (Miller,
Griffiths, & Jordan, 2009). Typically, models capability including
attributes covariates affect link prediction directly part latent
space representation. However, Zhu, Yu, Chi, Gong (2007) demonstrated attributes also represented related distinct latent space. recently, Menon
Elkan (2011) showed matrix factorization technique link prediction scale
much larger graphs training stochastic gradient descent instead MCMC.
3.4 Discussion
Link prediction remains challenge, part large number possible
links (i.e., N 2 possible links given N observed nodes), widely varying data
characteristics. Depending domain, best approach may use single nonrelational metric topology metric, may use richer set features evaluated
learned model. Future work may also wish consider using ensemble link
predictors yield even better accuracy.
discussion link prediction focused predicting new links based existing
links properties nodes. context web, however, link prediction
sometimes taken forms. instance, Sarukkai (2000) used web server traces
predict next page user visit, given recent browsing history. particular,
use Markov chains, related random walks discussed Section 3.2,
task also call link prediction. recently, DuBois Smyth (2010)
model relational events (i.e., links) using latent classes event/link arises
latent class properties event (i.e. sender, receiver, type) chosen
distributions nodes conditioned assigned class. work, local
community node influences distribution computed node, way related
computations stochastic block modeling (Airoldi et al., 2008). DuBois & Smyths
task also form link prediction, goal predict presence
absence static link, frequency occurrence possible event/link.
One might also interested deleting pruning away noisy, less informative links.
instance, friendship links Facebook usually extremely noisy since cost
adding friendship links insignificant. techniques used section could
383

fiRossi, McDowell, Aha, & Neville

also used remove existing links wherever link prediction algorithm yields
low score (or weight) observed link original graph.
Indeed, since link prediction algorithms effectively assign score every possible
link, could also used assign weight set initial links G.
link weighting one three subtasks link interpretation shown taxonomy
Figure 2. However, practice weights needed initial links, different
features algorithms often possible and/or effective. next section
discusses link weighting algorithms, well link interpretation general. Also,
Section 7 discuss additional methods link prediction seek jointly
transform nodes links.

4. Link Interpretation
Link interpretation process constructing weights, labels, general features
links. three tasks link interpretation related somewhat overlapping. First,
link weighting task assigning weight link. weights may represent
relevance importance link, typically expressed continuous values.
Thus weights provide explicit order links. Second, link labeling similar,
except usually assigns discrete values link. could represent positive
negative relationship, could used, instance, assign one five topics email
communication flows. Finally, link feature construction process generating set
discrete continuous features links. instance, features might count
frequency particular words appeared messages two nodes connected
link, simply count number messages.
sense, link feature construction subsumes link weighting labeling, since
weights labels viewed simply possible link features discovered. However, many tasks makes sense compute one particular feature summarizes
relevance link (the weight) and/or one particular feature summarizes type
link (the label). weights labels may especially useful later processing, example collective classification. Moreover, techniques used general
feature construction tend toward simpler approaches aggregation discretization, whereas best techniques computing weights labels may involve much
complexity, including global path computations supervised learning. reason,
treat link weighting (Section 4.1) link labeling (Section 4.2) separately general
link feature construction (Section 4.3).
4.1 Link Weighting
Given initial graph G = hV, E, XV , XE i, task assign continuous value (the
weight) existing link G, representing importance influence link.
previously discussed, link weighting could potentially accomplished applying link
prediction technique simply retaining computed scores link weights. instance,
Lassez, Rossi, Jeev (2008) perform link prediction weighting applying singular
value decomposition adjacency matrix, retaining k significant
singular-vectors (similar low-rank approximation techniques discussed Section 3.2).
384

fiTransforming Graph Data Statistical Relational Learning

show querying (e.g., PageRank) resultant weighted graph yield
relevant results compared unweighted graph.
Unlike link prediction, however, link weighting techniques designed
work links already exist graph. techniques dont work
predicting unseen links weight links based known properties/features
existing links, compute additional link features yield
sensible results links already exist.
simplest case, link weighting aggregating intrinsic property links.
example, Onnela et al. (2007) defines link weights based aggregated duration
phone calls individuals mobile communication network. cases, simply
counting number interactions two nodes may appropriate.
Thus, link features like duration, direction, frequency known,
aggregated way generate link weights. actual link weights already known
links, supervised methods used weight prediction, using
known weights training data. instance, Kahanda Neville (2009) predict link
strength within Facebook dataset, stronger relationships identified based
users explicit identification top friends via popular Facebook application.
Gilbert Karahalios (2009) also predict link strength Facebook, form training data survey data collected 35 participants (yielding strength ratings
2000 links). algorithms generate large number (50-70) features
link network, learn predictive model via regression technique bagged decision trees, Kahanda Neville finds performs best among
several alternatives. Gilbert Karahalios generate features based profile similarity
(e.g., two users similar education levels?) based user interactions (e.g.,
frequently topics two users communicate?). find interaction features helpful, especially feature based number days since
last communication event. Kahanda Neville use similar kinds features,
term attribute-based transactional features, also add topological features (such
Adamic/Adar discussed Section 3.2) network-transactional (NTR) features.
NTR features based communications users (e.g., number
email messages exchanged) moderated way larger network context.
moderation often takes form normalization, instance dampen influence
node sent large number messages many different friends. find
NTR features far helpful prediction, many
features also contribute overall predictive accuracy.
training data sample link weights available, approaches based
parameterized probabilistic model still possible. However, since candidate link features
longer evaluated training data, approaches must (manually)
choose features use much carefully. instance, Xiang et al. (2010)
examine link weight prediction two social network datasets (Facebook LinkedIn),
use 5-11 features link. hypothesize relationship strength hidden
cause user interactions, propose link-based latent variable model capture
dependence. inference, use coordinate ascent optimization procedure predict
strength link. Since actual strength link known, prediction
tasks domain cannot directly evaluate accuracy. However, Xiang et al. demonstrate
385

fiRossi, McDowell, Aha, & Neville

using link strengths produced method leads higher autocorrelation
higher collective classification accuracy predicting user attributes gender
relationship status.
number researchers considered importance recency evaluating link
weight, assumption events interactions occurred recently
weight. instance, Roth et al. (2010) propose Interactions Rank metric
weighting link based messages two nodes. formula separately
weights incoming outgoing messages link, imposes exponential decay
importance message based old is. Roth et al. use metric
weight links call implicit social network, node represents
group users. demonstrate metric used accurately predict users
missing email distribution list. However, basic metric simple
compute could applied many tasks.
Interactions Rank metric weights link heavily connects two nodes
frequently and/or recently communicated. Alternatively, Sharan Neville (2008)
considered weight links graph links (such hyperlinks
friendships) may appear disappear time. particular, construct
summarized graph nodes links ever existed past present.
link new graph weighted based kernel function provide
weight links present often recently past. explain
modify standard relational classifiers use weighted links, demonstrate
variety kernels (including exponential linear decay kernels) produce weighted
links yield higher classification accuracy compared non-weighted graph.
recently, Rossi Neville (2012) extended work handle time-varying attribute
values, may serve basis incorporating temporal dynamics additional
tasks.
4.2 Link Labeling
Given initial graph G = hV, E, XV , XE i, task construct discrete label
one links G. labels used describe type relationship
link represents. instance, Facebook example, link labeling algorithm may
create labels representing work personal relationships. labels would enable
subsequent classification models separately account influence different
kinds relationships.
prior work link labeling assumed text (such message) describes link, based unsupervised textual analysis techniques
Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Latent Semantic Analysis (LSA)
(Deerwester, Dumais, Furnas, Landauer, & Harshman, 1990), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999). Traditionally, techniques used
assign one latent topics document collection documents.
topics formed defined implicitly probability distribution likely
word appear, given topic associated document. topics
always semantically meaningful, often manual inspection reveals
prominent topics represent sensible concepts advertising government re386

fiTransforming Graph Data Statistical Relational Learning

lations. However, even semantic associations obvious, inferring
topics set links still aid analysis, since topics identify links
represent similar kinds relationships.
textual analysis techniques developed independent documents mind,
inter-linked nodes, adapted label links several ways. instance,
Rossi Neville (2010) examined messages developers contributing opensource software project. treat message separate document, use LDA
infer single likely latent topic message (i.e., link label). technique
could used graph textual content associated links. Rossi Neville
also go further, consider impact time-varying topics time-varying topic/word
associations, running multiple iterations LDA, one per time epoch. Using model,
study problem predicting effectiveness different developers (nodes)
network. demonstrate accuracy predictions significantly improved
modeling temporal evolution communication topics.
McCallum, Wang, Corrada-Emmanuel (2007) describe alternative way extending LDA-like approaches link labeling. LDA essentially Bayesian network
models probabilistic dependencies documents, associated topics, words associated topics. propose extend model Author-RecipientTopic (ART) model, choice topic document (message) depends
author recipient message. parameters learned
model, inference (e.g., Gibbs sampling) used infer likely latent
topics message. make use topics assign roles people email
communication network, demonstrate outperforms simpler models.
Supervised techniques also used link labeling. instance, Taskar et al.
(2003) study academic webpage network consider predict node labels (such
Student Professor) simultaneously predicting link labels (such adviserof). Given labeled training graph, learn complex Relational Markov Network
(RMN) predict labels existence new links. make link
prediction tractable, candidate new links considered, links
suggested textual reference, inside page, entity graph.
RMN utilizes text-based features, instance based anchor text known links
heading HTML section possible link reference found.
demonstrate RMNs joint inference nodes links improves performance
compared separate inference. However, learning inference RMNs often
significant challenge, practice limits number types feature
considered.
RMN approach learns training data uses joint inference
entire graph. simpler supervised approach create set features link
use features learning inference arbitrary classifier treats
link separately. Leskovec, Huttenlocher, Kleinberg (2010) study particular form
approach two link labels, representing positive negative relationship (such friendship vs. animosity). create link features based (signed)
degree nodes involved link also based transitivity-like properties computed known labels nearby links. demonstrate approach using data
Epinions, Wikipedia, Slashdot, users manually indicated positive
387

fiRossi, McDowell, Aha, & Neville

negative relationships users. Given network almost edges labeled,
label classifier able predict label (positive negative) single unlabeled edge
high accuracy. Interestingly, show classifiers predictive accuracy
particular dataset decreases slightly classifier trained different dataset
vs. trained dataset used predictions. argue theories
balance status social psychology partially explain ability predictive
models generalize across datasets. Unlike techniques discussed
section, work make use text-based features. However, general problem
predicting sign link related sentiment analysis (or opinion mining)
natural language processing (Godbole, Srinivasaiah, & Skiena, 2007; Pang & Lee, 2008).
sentiment analysis algorithms could reformulated predict label (such
positive negative) link given associated text.
link two nodes established based many different kinds
relationships, many types algorithms could potentially used
labeling links, even original algorithm designed purpose. instance,
Markov Logic Networks (MLNs) used extract semantic networks text,
yielding graph nodes represent objects concepts (Kok & Domingos, 2008).
process produces relations teaches written nodes,
could used link labels analysis. Another example Group-Topic
(GT) model proposed McCallum, Wang, Mohanty (2007), which, like previously
mentioned ART model, Bayesian network. model intended graphs two
nodes (such people) become connected participate event,
voting yes political bill. Rather directly labeling links (like
ART), GT model clusters nodes (such people) latent groups based
textual descriptions events/votes. However, GT model also simultaneously infers
set likely topics event, could used label implicit links
nodes. results model could also used add new nodes graph
represent latent groups discovered.
4.3 Link Feature Construction
Link feature construction systematic construction features links, typically
purpose improving accuracy understandability SRL algorithms. Link feature
construction important many prediction tasks, received considerably
less attention node feature construction literature. Fortunately, many
computations developed node feature construction also apply link
features. avoid redundancy, defer analysis feature construction
discussion node feature construction Section 6.3. section briefly discusses
techniques node feature construction applied links, summarizes
major types link features computed.
Section 6.3 later describe feature values relational data often based
aggregating values multiple nodes. instance, feature might compute
average common feature value among neighbors particular node.
aggregation-based features help account varying number neighbors
node may have. links, aggregation less essential, since (usually) link precisely
388

fiTransforming Graph Data Statistical Relational Learning

(a) link-aggregation

(b) link-aggregation

Figure 4: Link Feature Aggregation Example: figure demonstrates unknown link feature value computed aggregating link feature values
surrounding links. aggregation operator Mode.

two endpoint nodes. However, aggregation still useful computing features
collect information larger area graph. instance, Figure 4, link feature
value computed link center subgraph (the target link).
computation considers feature values (positive negative signs) links
adjacent target link. case, aggregation operator Mode,
result new link feature value. example used link features input, node
feature values (e.g., lightly-shaded nodes Figure 4) could also aggregated
form new link feature. way, aggregation operators discussed nodes
Section 6.3 also applied links.
Figure 5 summarizes kinds features constructed link. figure
organized around sources information go computing single link feature
(i.e., inputs), rather details feature computation (such type
aggregation function used). bottom figure shows four types link
features, represented subgraph. case, emphasized link bottom
subgraph target link new feature value computed.
subgraphs shows varying amounts information displays
features, nodes, and/or links used inputs kind link feature.
simplest type non-relational link feature, computed
link solely information already known link. Thus, Figure 5A shows
feature values already known target link, used
construct new feature value. instance, message associated link,
link feature could count number times certain word occurs, number
distinct words. Alternatively, date associated link, feature might
compute number months since link formed. Onnela et al. (2007) computed
kind feature aggregated duration phone calls two people
form new link feature (which also used link weight).
remaining feature types relational, meaning depend way
graph (not single link). First, topology features (Figure 5B)
computed using topology graph. feature might, instance,
compute total number links adjacent target link. Likewise, Kahanda
Neville (2009) computed clustering coefficient pair linked nodes,
measures extent two nodes neighbors common (Newman, 2003),
well topological features Adamic/Adar measure discussed Section 4.1.
389

fiRossi, McDowell, Aha, & Neville

input

V,E,XV,XE

target link

Link Feature
Construction

E
X

link-value

p

V,E,XV,XE

node-value

L
Non-relational
Link Features

Relational Features

V,E V,E,XE V,E,XV
Topology
Features

Link-value
Features

Node-value
Features

C
p

w

C
L

w

L
A.

B.

E

C.

L
D.

X

Figure 5: Link Feature Taxonomy: link feature classes non-relational features,
topology features, relational link-value features, relational node-value features.
subgraphs bottom, information potentially used
class link feature (i.e., nodes V , links E, node features X V , and/or link
features X E ) shown. emphasized link represents feature value
computed (i.e., target link).

used link features help predict link strength, could also used
tasks.
Next, relational link-value features computed using feature values
nearby links. instance, Figure 5C shows link labels personal (p) work
(w) might identified links adjacent target link. new link feature could
formed representing distribution labels, taking common
label, (when link features numeric) averaging. Leskovec, Huttenlocher,
Kleinberg (2010) used link-value features working graphs link
sign feature positive negative (as Figure 4). computed features
based signed-degree two nodes connected target link well
complex measures based paths two nodes (e.g., measure sign
transitivity).
390

fiTransforming Graph Data Statistical Relational Learning

Finally, relational node-value features computed using feature
values nodes close attached target link. instance,
Figure 5D shows node labels conservative (C) liberal (L) might identified
nodes close target link. link-value features, labels could used
create new feature value summarization aggregation. Often, two nodes
directly attached target link used. instance, work Gilbert
Karahalios (2009) Kahanda Neville (2009) construct link features based
similarity two nodes social network profiles. However, feature values distant
nodes could also used, instance compute new link feature based similar
friends two people (nodes) are.

5. Node Prediction
Node transformation includes node prediction (e.g., predicting existence new nodes)
node interpretation (e.g., constructing node weights, labels, features). section
focuses node prediction, Section 6 considers node interpretation.
Given graph existing nodes V , node prediction used two distinct ways.
First, node prediction algorithm could used discover additional nodes
type already present V . instance, given set people
communicate via email, simple algorithm might used create new nodes
represent email recipients implied messages, explicitly represented
original graph. Alternatively, supervised unsupervised machine learning techniques
could used discover, instance, new research papers people information
available web (Craven et al., 2000; Cafarella, Wu, Halevy, Zhang, & Wang, 2008).
techniques valuable, certainly used add new nodes graph.
However, work examined context general knowledge base
construction, rather relational learning.4
focus second type node prediction, involves predicting nodes
different type already present graph. new nodes might
represent locations, communities (Kleinberg, 1999), roles (McCallum, Wang, & CorradaEmmanuel, 2007; Rossi, Gallagher, Neville, & Henderson, 2012), shared characteristics,
social processes (Tang & Liu, 2009; Hoff et al., 2002), functions (Letovsky & Kasif, 2003),
kind relationship. instance, running Facebook example,
newly discovered node may represent common interest hobby multiple people
share. nodes usually referred latent nodes (and nodes connected
node form latent group).5 meaning nodes depend upon
features and/or links included input node prediction algorithm.
instance, including work-based friendships lead different groups
personal friendships considered.
4. recent work Kim Leskovec (2011) exception. technique uses EM infer
existence missing nodes links based known topology graph.
5. Prior work sometimes refers nodes hidden nodes, especially thought
represent concrete characteristics, geographic location, could measured were,
reason, observed data.

391

fiRossi, McDowell, Aha, & Neville

Figure 6: Alternative Representations Newly Predicted Groups: left
figure shows new feature (with value X Y) could added node,
right figure demonstrates creation two new nodes represent
groups.

many advantages type representation change regards accuracy understandability. instance, nodes directly connected
original graph similar way become, links new nodes,
closer graph space. Intuitively, nodes connected high level concept share
latent properties representing latent structure directly impact classification,
network analysis, many tasks. instance, reducing path length
similar nodes enables influence nodes propagate effectively collective
classification (CC) performed nodes. model still learn exploit
new nodes relationships, even semantic meaning new nodes
precisely understood.
popular methods predicting new nodes based clustering,
context means grouping nodes nodes within group similar
nodes groups. Typically, one new node created
group, links added existing node corresponding
group node (see right side Figure 6). techniques may also associate node
multiple groups, link weights representing affinity group.
new groups discovered, whether via clustering via technique,
alternative creating new nodes links simply add new feature(s) node
represent group information. left side Figure 6 demonstrates alternative.
instance, new node feature might represent running hobby, may simply
represent belonging discovered group #17, unknown meaning. Popescul
Ungar (2004) use CiteSeer dataset demonstrate technique derive features
improve predictive accuracy. advantage approach, opposed adding
new nodes, potentially enables simpler, non-relational algorithms make use
new information. potential disadvantage, though, also allow
algorithms CC propagate influence newly connected nodes, discussed
above. However, methods use general strategy generate much larger
392

fiTransforming Graph Data Statistical Relational Learning

numbers latent features used classification (Tang & Liu, 2009; Menon &
Elkan, 2010). Tang & Liu demonstrate that, cases, resultant large number
link-based features may make collective inference unnecessary obtaining good accuracy.
Naturally, whether information discovered clusterings best represented
via new nodes new features depend upon dataset inference task.
section, simplicity discuss algorithm assuming new nodes
created (even algorithm originally described terms creating new features).
discussion link prediction, organize discussion around kinds
information used prediction. Section 5.1 discusses non-relational (attributebased) node prediction, Section 5.2 discusses topology-based node prediction, Section 5.3 discusses hybrid approaches use node feature values topology
graph.
5.1 Non-relational (Attribute-Based) Node Prediction
many clustering algorithms used cluster existing nodes using
non-relational features (attributes), used add new nodes
graph. two primary types hierarchical clustering algorithms (e.g., agglomerative
divisive clustering) partitioning algorithms k-means, k-medoids (Berkhin, 2006;
Zhu, 2006), EM-based algorithms, self-organizing maps (Kohonen, 1990).
discuss algorithms since well studied non-relational data
easily applied relational data clustering based attribute values
desired.
5.2 Topology-Based Node Prediction
techniques described section link existing nodes one new nodes (i.e.,
latent groups), based original link structure graph. cases, finding
grouping depends upon computing kind similarity metric every pair
nodes. Two key questions thus serve identify techniques. First, kind
similarity metric used? Second, metric used predict
groupings? address question turn.
5.2.1 Types Metrics Group Prediction
type topology-based link weighting metric (see Table 3.2) could conceivably used
latent node prediction. metric suitable long produces high values
pairs nodes belong group lower values pairs.
instance, high value Katz metric (see Section 3.2) indicates two nodes
many short paths them, thus may belong group. Metrics
representing distance rather similarity also used negating metric.
instance, Girvan Newman (2002) focus detecting community structure extending
concept node-betweenness links. Intuitively, network contains latent groups
loosely connected intergroup links, shortest paths
different groups must go along links. links connect different groups
assigned high link-betweenness value (which corresponds low similarity value).
393

fiRossi, McDowell, Aha, & Neville

underlying group structure trivially revealed removing links
highest betweenness.
idea using link-betweenness relational clustering extended
number directions. instance, Newman Girvan (2004) introduced random-walk
betweenness, expected number times random walk pair
nodes pass particular link. addition, Radicchi, Castellano, Cecconi, Loreto,
Parisi (2004) proposed using link-based clustering coefficient metric. showed
metric performs comparably original link-betweenness metric Girvan
Newman, much faster local graph measure instead global graph
measure.
Zhou (2003) describes new metric, dissimilarity index, computed
follows. node i, compute vector di value dij represents
distance node node j (Zhou measures distance based average number
steps needed random walk starting node reach node j, distance metric
could used). nodes k similar, similar distance
vectors. Thus, dissimilarity index nodes k defined based Euclidean-like
distance computation vectors di dk . Zhou demonstrates technique
outperforms link-betweenness approach Girvan & Newman random modular
networks.
Relatively simple metrics often lead useful results. instance, Ravasz et al.
(2002) used simple clustering coefficient metric study metabolic networks. study
reveals metabolic networks forty-three organisms organized many small,
highly-connected modules. Furthermore, find E. coli, hidden hierarchical
modularity closely overlaps known metabolic functions.
5.2.2 Using Metrics Group Prediction
simplest techniques identifying new groups perform kind hierarchical
clustering. instance, similarities weights computed every pair
nodes, links removed graph. Next, weighted links placed
nodes one one, ordered weights. intuition varying degrees
clusters formed links added. particular, approach forms hierarchical
tree leaves represent finest granularity clustering every node
separate cluster. move tree larger clusters formed, reach top
nodes joined one large cluster. type hierarchical approach
used work Zhou (2003). Girvan Newman (2002) use similar strategy,
start instead original graph iteratively remove less similar links
graph reveal underlying community structure. challenge approaches,
clustering general, select appropriate number final clusters,
corresponds selecting level clustering tree.
Spectral clustering (Dhillon, 2001; Ng, Jordan, & Weiss, 2001; Kamvar, Klein, & Manning, 2003) also used group identification. Spectral clustering relies upon computing similarity matrix describes data points, transforming matrix
way yields new matrix U clustering rows U using simple clustering
algorithm (such k-means) trivially identify interesting groups data.
394

fiTransforming Graph Data Statistical Relational Learning

matrix transformation several variants, involves computing kind Laplacian
S, computing eigenvectors resultant matrix using eigenvectors represent original data. motivation transformation seen
identifying good graph cuts original graph (those yield good separations
highly-connected nodes groups) identifying nodes closely related
terms random walks; see work von Luxburg (2007) overview. Spectral
clustering originally applied non-relational data, but, hierarchical techniques described above, applied relational data using link-based metrics
computing similarity matrix. instance, Neville Jensen (2005) use node
adjacency matrix spectral clustering technique described Shi Malik (2000)
identify latent groups graphs. show technique enables simpler
inference (since group handled separately), ultimately yields accurate
classification compared approaches ignore group structure. Tang Liu (2011)
also use spectral clustering link graph, order create much larger
number latent features used learn supervised classifier. Unlike
latent groups work Neville Jensen, technique allows node
associated one cluster output spectral clustering, Tang
& Liu claim leads improved classification accuracy. Spectral clustering also used
complex similarity metrics, described next subsection.
Techniques borrowed web search also useful node prediction. instance, given adjacency matrix webpage graph, Hits algorithm (Kleinberg,
1999) computes first eigenvectors AAT A, represent
authoritative nodes (the authorities) well prominent nodes point (the
hubs). Normally, algorithm used find single prominent community authorities hubs (to assist web search), secondary communities
discovered also considering non-principal eigenvectors AAT (Gibson, Kleinberg, & Raghavan, 1998). node prediction algorithm could treat
community latent group add new node links represent group.
techniques may especially useful detecting patterns influence graph
adding explicit links represent influence.
5.3 Hybrid Node Prediction
techniques previous section added new nodes graph, often based
clustering, using topology graph. principle, technique also used
nodes attributes produce meaningful latent groups/nodes. section
considers add attribute information techniques node prediction.
simple approach define kind similarity metric combines nonrelational topology-based similarity single value, provide similarity
metric one previously mentioned clustering algorithms. instance, Neville,
Adler, Jensen (2004) use weighted combination attribute link information
1X
S(i, j) =
sk (i, j) + (1 ) l
k
k

metric, sk (i, j) = 1 iff nodes j value kth attribute,
l = 1 iff link exists j. constant controls relative
395

fiRossi, McDowell, Aha, & Neville

importance attributes vs. links. use metric NCut spectral
clustering technique add new nodes graph, demonstrate additional
nodes increase performance relational classification. similar weighted combination
attribute link-based similarity used Bhattacharya Getoor (2005) entity
resolution.
Attribute-based information also incorporated ad-hoc basis. instance,
Adibi, Chalupsky, Melz, Valente, et al. (2004) describe group finding algorithm
initial seed set clusters formed based handcrafted set logical rules,
clusters refined using probabilistic system based mutual information.
system, logic-based component primarily uses attributes node (person),
probabilistic system primarily uses links describe connections
people. However, components make use attributes links.
principled approach define kind generative model represents
dependence observed attributes links latent group nodes, use
model estimate group membership. instance, Kubica, Moore, Schneider,
Yang (2002) define generative model node belongs one groups,
group members tend link other. particular, use group membership
chart track whether node belongs group, local search possible
states chart (using stochastic hill climbing) try identify membership changes
would better explain known data. step, maximum likelihood used
estimate parameters model. demonstrate usefulness technique
news articles, webpages, synthetic data.
Generative models also used sophisticated inference. example,
Taskar, Segal, Koller (2001) treat group membership latent variable uses
loopy belief propagation implicitly perform clustering nodes. Likewise, Mixed
Membership Relational Clustering (MMRC) (Long et al., 2007) uses EM variants estimate group memberships. particular, uses first round hard clustering (where
object assigned exactly one cluster), following round soft clustering continuous strength values associated membership assignment. Mixed membership stochastic blockmodels (Airoldi et al., 2008) also assign continuous group membership
values node, use topological information (not attributes) group
assignments use variational inference techniques generative model. Finally,
Long, Zhang, Wu, Yu (2006) demonstrate node clustering performed instead using spectral clustering, focuses particularly simultaneously cluster
multiple types nodes (e.g., simultaneously cluster web pages web users two
distinct sets groups).
group prediction algorithms assume links likely connect nodes
belong group. exception work Anthony desJardins (2007),
also use generative model links attributes depend latent group
memberships, types links likely occur nodes
belong group. instance, note groups social network
defined gender, link representing dating likely connect two nodes
different groups.
396

fiTransforming Graph Data Statistical Relational Learning

Figure 7: Lifted Graph Representation: initial graph G clustered transformed lifted graph representation G. lifted graph representation
created clustering nodes, links, both.

5.4 Discussion
techniques described produce single clustering nodes, usually
based assigning every node single group. contrast, multi-clustering emerging
research area aims provide multiple orthogonal clusterings complex data (Strehl
& Ghosh, 2003; Topchy, Law, Jain, & Fred, 2004). instance, individuals Facebook
might clustered multiple ways latent node types might represent friend groups,
work relations, socioeconomic status, locations, family circles. type multi-clustering
performed McCallum, Wang, Corrada-Emmanuel (2007) latent nodes
created based roles topics. addition, Kok Domingos (2007) propose Statistical Predicate Invention (SPI), node transformation approach based Markov Logic
Networks (Richardson & Domingos, 2006). SPI clusters nodes, features links forming basis prediction predicates (or potential nodes). SPI considers multiple
relational clusterings based observation multiple distinct clusterings may
necessary to, instance, group individuals based friendships work relationships. demonstrate MLN inference estimate clusters improves
performance compared two simpler baselines. similar node prediction approach applies
MLNs role labeling (Riedel & Meza-Ruiz, 2008).
Node deletion may also useful cases. instance, node deletion might
beneficial removing outdated spurious nodes graph. Alternatively,
may multiple nodes represent real-world object concept, case
deletion purposes entity resolution important (Pasula, Marthi, Milch,
Russell, & Shpitser, 2003; Bhattacharya & Getoor, 2007; Singla & Domingos, 2006).
Finally, node representation changes used improve accuracy,
also yield graphs processed efficiently desirable
properties. Section 5.2 already discussed Neville Jensen (2005) used addition
latent nodes enable simpler inference. Another possibility creation super-nodes
represent one original nodes. instance, Figure 7 demonstrates
five original nodes can, clustering, collapsed three super-nodes, yielding lifted
graph representation. kind representation change used efficient
397

fiRossi, McDowell, Aha, & Neville

inference Markov Logic Networks (see Section 6.3) network anonymization (see
Section 8.6).

6. Node Interpretation
Node interpretation process constructing weights, labels, general features
nodes. symmetric tasks link interpretation, node weighting seeks
assign continuous value node, representing nodes importance, node
labeling seeks assign discrete value link, representing type, group, class
node. Likewise, node feature construction process systematically generating
general-purpose node features based on, instance, aggregation, dimensionality reduction,
subgraph patterns.
discussed Section 4 links, node feature construction could viewed subsuming node weighting node labeling, since general feature construction could always
used construct feature values treated weights labels nodes.
practice, however, techniques used tend rather different. instance, PageRank
often used node weighting supervised classification often used node labeling,
techniques rarely used general feature construction. Nonetheless, node
interpretation (more link interpretation) substantial overlap techniques actually used weighting labeling vs. used general
feature construction. Below, first discuss node weighting Section 6.1 labeling
Section 6.2. Section 6.3 discusses node feature construction, mentioning briefly
relevant techniques previously discussed weighting labeling.
6.1 Node Weighting
Given initial graph G = hV, E, XV , XE i, task assign continuous value (the
weight) existing node G, representing importance influence node.
Node weighting techniques used information retrieval, search engines, social
network analysis, many domains way discover important nodes
respect defined measure. node prediction classified based
whether use node attributes, graph topology, construct
weighting.
6.1.1 Non-relational (Attribute-Based) Node Weighting
simplest node weighting techniques use node features XV (i.e., attributes).
instance, nodes representing documents might weighted based number
query-relevant words contain, nodes representing companies might ranked
based gross annual sales. Many sophisticated strategies also considered. instance, Latent Semantic Indexing (Deerwester et al., 1990) used
identify important semantic concepts corpus text, nodes ranked
based connection concepts. methods extensively applied
quantify rank importance scientific publications (Egghe & Rousseau, 1990).
However, techniques extensively studied elsewhere also ignore
graph structure (such citations), discuss here.
398

fiTransforming Graph Data Statistical Relational Learning

6.1.2 Topology-Based Node Weighting
Several node weighting algorithms use topology graph developed
support early search engines. Examples kind algorithm include PageRank
(Page et al., 1999), Hits (Kleinberg, 1999), SALSA (Lempel & Moran, 2000).
algorithms rank relative importance web sites, conceptually based
kind eigenvector analysis (Langville & Meyer, 2005), though practice iterative computation may used. instance, PageRank models web Markov Chain
implemented systematically computing principal eigenvector limk Ak e
adjacency matrix e unit vector. Hits, previously described, instead computes principal eigenvectors AAT A. algorithms continue
important webpage ranking, also applied many kinds
graphs (Kosala & Blockeel, 2000).
social network analysis, objective topology-based node weighting typically
identify influential significant individuals social network.
variety centrality measures devised use local global network structure characterize importance individuals (Wasserman & Faust, 1994). Examples
metrics include node degree, clustering coefficient (Watts & Strogatz, 1998), betweenness (Freeman, 1977), closeness (i.e., distance/shortest paths), eigenvector centrality (Bonacich & Lloyd, 2001), many others (Jackson, 2008; Newman, 2010; Sabidussi,
1966). addition, White Smyth (2003) considered compute relative node
rankings, i.e., rankings relative set particularly interesting nodes. show
compute relative rankings metrics based shortest paths well
Markov chain-based techniques (e.g., produce PageRank priors). addition,
similarity metrics described Table 3.2 alternatively formulated
computing weights nodes.
recently, node weighting techniques extended measure relative
importance nodes temporally-varying data. instance, Kossinets, Kleinberg,
Watts (2008) Tang et al. (2009) define notions temporal distance based
analysis frequently information exchanged nodes. information
used define range new graph metrics, global temporal efficiency, local temporal efficiency, temporal clustering coefficient (Tang et al., 2009). recently,
Tang, Musolesi, Mascolo, Latora, Nicosia (2010) define notions temporal betweenness
temporal closeness. argue incorporating temporal information
metrics provides better understanding dynamic processes network
accurately identifies important nodes (people). metrics primarily concern networks time-varying interactions (e.g., communications people),
could also applied types data intermittent interactions
nodes nodes/link join leave network time. metrics also
apply links, could possibly used improve link prediction algorithms.
6.1.3 Hybrid Node Weighting
also hybrid node weighting approaches use attributes graph
topology (Bharat & Henzinger, 1998; Cohn & Hofmann, 2001). instance,
various approaches modify Hits (Chakrabarti, Dom, Raghavan, et al., 1998; Bharat
399

fiRossi, McDowell, Aha, & Neville

& Henzinger, 1998) PageRank (Haveliwala, 2003) construct node weights based
content links. Topic-Sensitive PageRank (Haveliwala, 2003) seeks compute
biased set PageRank vectors using set representative topics. Alternatively, Kolda,
Bader, Kenny (2005) propose TOPHITS, hybrid approach adds anchor text (i.e.,
clickable text hyperlink) adjacency matrix representation used Hits.
use higher-order analogue SVD known Parallel Factors (PARAFAC)
decomposition (Harshman, 1970) identify key topics graph well
important nodes. hybrid approaches proposed SimRank (Jeh
& Widom, 2002), Topical methods (Haveliwala, 2003; Nie, Davison, & Qi, 2006; Kolda
& Bader, 2006), Probabilistic HITs (Cohn & Chang, 2000), many others (Richardson
& Domingos, 2002; Lassez et al., 2008). Section 7 discusses relevant work
context joint node link transformation techniques.
Recently, node weighting approaches applied Adversarial Information Retrieval (AIR) detect moderate influence spam web sites. Typically, techniques produce weights using topology graph information,
necessarily kind attribute information used techniques discussed
above. instance, TrustRank (Gyongyi, Garcia-Molina, & Pedersen, 2004) based
PageRank uses set trusted sites evaluated humans propagate trust
locally reachable sites. hand, SpamRank (Benczur, Csalogany, Sarlos, &
Uher, 2005) measures amount undeserved PageRank analyzing backlinks
site. algorithms try identify link farms link spam alliances (Wu
& Davison, 2005), given seed set known link farm pages. Among AIR methods,
TrustRank widely known suffers biases human-selected set
trustworthy sites may favor certain communities others.
6.2 Node Labeling
Given initial graph G = hV, E, XV , XE i, task assign discrete label
nodes G. first discuss labeling techniques based classification,
consider unsupervised textual analysis techniques.
many cases, node labeling may considered end itself. instance,
running Facebook example, stated goal predict political affiliation
node label already known. cases, however, node labeling
properly understood representation change supports desired task.
instance, definitions anomalous link detection (Rattigan & Jensen, 2005),
estimated node labels would allow us identify links nodes whose labels indicate
rarely, ever, connected. Alternatively, datasets estimating node
labels may enable us subsequently partition data based node type, enabling us
learn accurate models type node.
Even node labeling final goal, Facebook example, intermediate
label estimation may still useful representation change. particular, Kou Cohen
(2007) describe stacked model relational classification relabels training set
estimated node labels using non-relational classifier. use estimated
labels learn new classifier (one uses attributes relational features),
use new classifier perform relational classification test graph. approach
400

fiTransforming Graph Data Statistical Relational Learning

yields high accuracy, comparable much complex algorithms collective
classification (CC). Fast Jensen (2008) analyze result discuss
explained natural bias CC algorithms: training performed given
node labels inference depends part estimated labels (McDowell, Gupta, &
Aha, 2009). Stacked models compensate bias instead training relabeled
(estimated) training set. addition, inference new classifier needs single
pass test graph, yielding much faster inference CC techniques like Gibbs
sampling belief propagation. recently, Maes, Peters, Denoyer, Gallinari (2009)
extend ideas node relabeling order generate larger training set via multiple
simulated iterations classification. show cases approach
outperform stacked models CC algorithms like Gibbs sampling.
Thus, multiple reasons creating new labels nodes graph.
labeling accomplished relational-aware algorithms like described
well earlier algorithms used relational collective classification (Chakrabarti,
Dom, & Indyk, 1998; Neville & Jensen, 2000; Taskar et al., 2001; Lu & Getoor, 2003;
Macskassy & Provost, 2003). Node labeling course also done traditional,
non-relational algorithms SVM, decision trees, kNN, logistic regression, Naive
Bayes, among various others (Lim, Loh, & Shih, 2000; Michie, Spiegelhalter, Taylor, &
Campbell, 1994; Burges, 1998; Cristianini & Shawe-Taylor, 2000; Joachims, 1998).
methods simply use features XV exploit topology link-structure.
techniques assign new labels via supervised learning. Labels also
assigned via unsupervised techniques textual analysis. many networks
real-world contain textual content social networks, email/communication
networks, citation networks, many others. Traditional textual analysis models
LSA (Deerwester et al., 1990), PLSA (Hofmann, 1999) LDA (Blei et al., 2003)
used assign node topic representing abstraction textual information.
recent techniques Link-LDA (Erosheva, Fienberg, & Lafferty, 2004) LinkPLSA (Cohn & Hofmann, 2001) aim incorporate link structure traditional
techniques order accurately discover nodes type.6 particular, work
Cohn Hofmann demonstrate technique produce accurate node
labels techniques use node attributes link topology.
also sophisticated topic models developed specific tasks
social tagging (Lu, Hu, Chen, & ran Park, 2010) temporal data (Huh & Fienberg,
2010; & Parker, 2010).
6.3 Node Feature Construction
Node feature construction systematic construction features nodes, typically
purpose improving accuracy understandability SRL algorithms. Feature
construction common relational representation change, frequently
done performing task classification. instance, performing CC
classify nodes example Facebook political affiliation task, likely compute
6. names Link-LDA Link-PLDA come work Nallapati, Ahmed, Xing, Cohen
(2008), original papers describing techniques.

401

fiRossi, McDowell, Aha, & Neville

new features representing information node (e.g., age bracket?)
known information nodes neighbors (e.g., many liberal?).
Different techniques node feature construction described many previous
investigations, though feature construction necessarily focus many
investigations. section, summarize explain different aspects feature
construction. particular, Section 6.3.1 presents discusses taxonomy features
based kinds inputs, topology information link feature values,
use computing new feature values. Next, Section 6.3.2 describes possible operators, aggregation discretization, applied inputs. Finally,
Section 6.3.3 examines perform automatic feature search selection support
desired computational task.
6.3.1 Relational Feature Inputs
node feature categorized according types information uses
computing feature values. possible information use includes set nodes V
links E, node features XV , link features XE . Figure 8 shows taxonomy
node features based sources information (the inputs) use.
taxonomy consistent distinctions previously made literature (e.g., non-relational relational features), best knowledge
complete taxonomy never previously described. taxonomy consists
four basic types: non-relational features three types relational features (topology features, relational link-value features, relational node-value features).
describe give examples each.
Non-relational Features: node feature considered non-relational feature
value feature particular node computed using non-relational
features (i.e., attributes) node, ignoring link-based information. instance, Figure 8A shows node corresponding nodes feature vector. new
feature value might constructed vector using kind dimensionality reduction, adding together several feature values, thresholding particular
value, etc.
Topology Features: feature considered topology-based feature values
feature computed using nodes V links E, ignoring existing
node link feature values. instance, Figure 8B, new feature value
computed node bottom left figure (the target node), using
topological information shown. particular, new feature value might count
number adjacent nodes, count many shortest paths graph pass
target node.
Relational Link-value Features: feature considered relational link-value
feature feature values links adjacent target node
used computing new feature. Typically, kind aggregation operator
applied values, count, mode, average, proportion, etc. instance,
Figure 8C, values links shown represent communication topics (work
personal), new link-value feature might compute mode values (p).
402

fiTransforming Graph Data Statistical Relational Learning

input

V,E,XV,XE

target node

Node Feature
Construction

XV



link-value

p

V,E,XV,XE

node-value

L

Non-relational
Node Features

Relational Features

V,E V,E,XE V,E,XV
Topology
Features

Link-value
Features

Node-value
Features

C
p

L

p

.5 P

A.

w

B.

V

C.

L
D.

X

Figure 8: Node Features Taxonomy Based Inputs Used: classes node
features non-relational features, topology features, relational link-value features, relational node-value features. classes defined respect
relational information used construction features (i.e., nodes
V , links E, node features XV , link features XE ). double-lined target node
represents new feature value computed. Parts C show
single feature value link node simplicity, general
one feature may exist used.

Usually computation include links directly connected target
node, links hops away could also used.
Relational Node-value Features: feature considered relational node-value
feature feature values nodes linked target node used construction. Links used identifying nodes, although nodes
one hop away target node may also included. instance, Figure 8D
shows feature values adjacent nodes (C L) could, instance,
used compute new node-value feature based mode (L) values.
Alternatively, one feature might count number adjacent C nodes another
might count number adjacent L nodes.
403

fiRossi, McDowell, Aha, & Neville

Feature computation may also applied recursively. instance, ReFeX system (Henderson, Gallagher, Li, Akoglu, Eliassi-Rad, Tong, & Faloutsos, 2011) first computes features every node based degree (a topology-based feature), considers recursive combinations features (such mean out-degree nodes
neighbors). Henderson et al. show recursive features often improve classification accuracy datasets network structure predictive. Alternatively,
topology-based feature betweenness might computed, relational nodevalue feature might compute average betweenness nodes neighbors
target label C. example hybrid feature uses
node-value topology-based information.
Another interesting aspect relational features potential feature value recomputation. particular, many techniques collective classification involve computing
node feature (such number neighbors currently labeled C) feature
depends feature values estimated (e.g., predicted node labels)
thus may change (Jensen et al., 2004; Sen et al., 2008). addition, McDowell, Gupta,
Aha (2010) describe features similar need recomputation,
meta-features use depend upon estimated label probabilities node
neighborhood target node. contrast, kind feature re-computation much
less applicability non-relational data, nodes assumed independent
other. However, occur techniques semi-supervised learning
co-learning.
6.3.2 Relational Feature Operators
previous section described features according different kinds inputs
use feature value computation, whereas section describes different operators
used computation. Table 5 summarizes operators.
cases, operator used many different types relational input. instance,
aggregation operators computed using graph topology, relational node-value
inputs, and/or relational link-value inputs, indicated appropriate checkmarks
Table 5. contrast, path walk-based operators generally use graph topology;
operators, lighter colored checkmarks Table 5 indicate path/walk-based
operators could sensibly used conjunction relational link-value node-values
inputs, rarely ever done. discuss operators
Table 5 detail.
Relational Aggregates: Aggregation refers function returns single value
collection input values set, bag, list. classical statistical
aggregation operators Average, Mode, Exists, Count, Max, Min, Sum (Neville
& Jensen, 2000; Lu & Getoor, 2003). SRL, another frequent operator Proportion,
computes, instance, fraction nodes neighbors meet criteria
label C (McDowell, Gupta, & Aha, 2007). operators may also
combined thresholds, e.g., evaluate whether Count nodes neighbors
labeled C least 3. thresholding turns numerical aggregate Boolean
feature, needed tree-based algorithms (Neville, Jensen, Friedland, et al., 2003).
Perlich Provost (2003) describe set complex relational aggregates depend
404

fiTransforming Graph Data Statistical Relational Learning

Relational aggregates

Mode, Average, Count, Proportion, Degree, ...

Temporal aggregates

Exponential/linear decay, union, ...

X

Set operators

Union, intersection, multiset, ...

X

Clique potentials

Direct link cliques, co-citation cliques, triads, ...

Subgraph patterns

Two star, three-star, triangle (i.e., transitivity), ...

Dimensionality reduction

PCA, SVD, Factor Analysis, Principal Factor Analysis, Independent Component Analysis, ...

Path/walk-based measures

Betweenness, common neighbors, Jaccards coefficient, Adamic/Adar, shortest paths, random-walks,
...

Textual analysis

LSA, LDA, PLSA, Link-LDA, Link-PLSA, ...

X

Relational clustering

Spectral partitioning, Hierarchical clustering, Partitioning relocation methods (k-means, k-medoids),
...

X

X

Relational Node-value

Example Techniques

Relational Link-value

Relational Operators

Topology

Non-relational

Inputs

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

Table 5: Relational Feature Operators: Summary popular types relational feature operators. check used indicate classes inputs (see
Section 6.3.1) operator naturally uses constructing feature values, lighter check indicates operator could sensibly used
input combination rarely ever used.

405

fiRossi, McDowell, Aha, & Neville

distribution attribute values associated node (e.g., via links
relational join). instance, aggregates may use function edit distance
compare nodes distribution reference distribution computed training
data. Perlich Provost demonstrate aggregations cases improve
performance compared simpler alternatives. also aggregate operators use
topology-based information. instance, operator Degree, simply counts
number adjacent links, predictive feature, applied carefully
relational data avoid bias (Jensen, Neville, & Hay, 2003).
Temporal Aggregates: Relational information might also contain temporal information
form timestamps durations links, node, features. general, data
handled defining special temporal-aggregation features computed raw
data (McGovern, Collier, Matthew Gagne, Brown, & Rodger, 2008) defining graph
summarizes temporal information (usually decreasing importance
less recent information) (Sharan & Neville, 2008; Rossi & Neville, 2010). Rossi Neville
discuss example latter approach, explore impact using various
temporal-relational information various kernels summarization. Alternatively, Section 6.1 discusses notions temporal distance used modify path/walk-based
metrics node betweenness closeness.
Set Operators: traditional domain-independent set operators set union,
intersection, difference applied construct features (Kohavi & John, 1997).
instance, two attributes represent presence word
page (node), new feature might represent case page contains
words (i.e., feature intersection). relational data, complex set-based
features possible. instance, feature collective classification might represent
union class labels nodes adjacent target node. Neville, Jensen,
Gallagher (2003) propose complex approach feature value multiset
represents complete distribution adjacent nodes labels (e.g., {3C, 2M, 5L}
indicate labels ten adjacent nodes). Using feature representation, show
independent-value approach assumes labels independently
drawn distribution yields effective relational classification. Recently,
McDowell et al. (2009) showed that, CC, multiset approach usually outperformed
types features proportion count-based aggregates discussed above.
Clique Potentials: probabilistic models Relational Markov Networks (RMNs)
(Taskar et al., 2002) perform inference related nodes without computing aggregates.
Instead, use clique-specific potential functions represent probabilistic dependencies, product term probability computation naturally expands accommodate
varying number neighbors node. one sense, featureless approach,
since need choose relational aggregation function. However, different kinds
dependencies still represented different cliques. instance, Taskar et al.
consider different sets cliques webpage classification: one based hyperlinks,
including information based links appear within page. Likewise, later
work added additional types cliques enable link prediction (Taskar et al., 2003). Thus,
even models remain important feature choices made.
406

fiTransforming Graph Data Statistical Relational Learning

Figure 9: Subgraph Patterns Link Labels. subgraph represents possible
pattern particular feature could look relation target node (the
bottom-left node case).

probabilistic models also use link-based information without computing explicit
features, random walk-based classifier Lin Cohen (2010) weightedneighbor approach Macskassy Provost (2007). Even cases, however, choices
remain types links use. instance, webpage graphs, co-citation
links may predictive class labels direct links (Macskassy & Provost, 2007;
McDowell et al., 2009).
Subgraph Patterns: subgraph pattern feature one based existence
particular pattern graph adjacent target node. feature might count
many times particular pattern exists target node, produce value true
least one pattern exists. simplest pattern called reciprocity; true
target node links node j j links back i. cases, however, patterns
complex involve nodes. Robins, Pattison, Kalish, Lusher (2007)
define many patterns including two-star (a node least two links), three-star (a
node least three links), triangle (also known transitivity, j k
k). patterns defined directed undirected links.
Many patterns possible. instance, Robins, Snijders, Wang, Handcock
(2006) use subgraph patterns probabilistically modeling graphs. argue using
complex patterns alternating k-triangle (based finding k triangles
share common side) help avoid degeneracy might otherwise arise
graph generation. Furthermore, subgraph patterns also extended exploit labels
links and/or nodes. instance, assume links labeled 1 2 (representing different topics) links labeled plus minus sign (representing
positive negative relationships). Figure 9 demonstrates three possible subgraph patterns,
based different link labelings, relative target node shown bottom left
subgraph. subgraph feature could compute, node, number matches
one patterns, feature could used later analysis.
Dimensionality Reduction goal dimensionality reduction find lower kdimensional representation initial n features (Sarwar, Karypis, Konstan, & Riedl,
2000; Fodor, 2002). formally, given initial n-dimensional feature vector x =
{x1 , x2 , ..., xn }, find lower k-dimensional representation x x = {x1 , x2 , ..., xk }
k n significant information original data captured, according criterion. many dimensionality reduction methods Principal
407

fiRossi, McDowell, Aha, & Neville

Component Analysis (PCA), Principal Factor Analysis (PFA), Independent Component
Analysis (ICA).
Dimensionality reduction techniques applied adjacency matrix
graph G create low-dimensionality graph representation; Section 3.3 explained
used link prediction. techniques also useful feature computation.
instance, Bilgic, Mihalkova, Getoor (2010) investigate active learning improve
accuracy collective classification. technique involves non-relational
relational features, demonstrate first applying dimensionality reduction (with
PCA) non-relational features simplifies learning, leading substantial gains accuracy.
Operators: mention briefly operators already discussed extensively elsewhere. Path-based measures (such betweenness distance)
walk-based measures (such PageRank) discussed Sections 6.1.
types measures used features classifier predict links (Lichtenwalter
et al., 2010) well validating relational sampling techniques (Leskovec, Chakrabarti,
Kleinberg, Faloutsos, & Ghahramani, 2010; Moreno & Neville, 2009; Ahmed, Neville, &
Kompella, 2012a, 2012b). measures typically use topology (not features), one could easily imagine computing metrics based, instance, paths
edge particular label type. Textual analysis techniques discussed
Sections 4.2 6.2, relational clustering techniques discussed Section 5.
operators used specifically node/link prediction, weighting, labeling,
also used general feature construction.
Finally, operators based similarity measures. Similarity two
nodes often computed, instance link prediction (Section 3) weighting (Section 4.1). computations easily lead feature value link, since link
obviously refers two endpoint nodes compared. However, computing
node feature value, usually obvious node comparison, similarity measures typically used node feature values. measures can, however, used
node prediction, Section 5 discusses cases newly discovered nodes/groups
used create new node features. particular instance relational similarity
functions, graph kernels structured data (Gartner, 2003) also used. kernels
used either nodes single graph (Kondor & Lafferty, 2002)
compute similarity two graphs (Vishwanathan, Schraudolph, Kondor, & Borgwardt, 2010). instance, former type kernel another technique could also
used link group prediction.
Discussion: Many feature operators discussed naturally used compute feature values links additions nodes. instance, textual analysis applied
links text associated link, node-centered path-based measures
analogous formulations links. One difference nodes naturally may link
many nodes, whereas assume links two endpoints. Thus, relational aggregates Count initially seem useful computing link features. However,
Figure 4 previously demonstrated link-aggregation accomplished broadening computation include multiple links nodes logically connected
endpoint node target link. Naturally, feature inputs operators
408

fiTransforming Graph Data Statistical Relational Learning

better suited computing node features vs. computing link features. next section
examines select appropriate features given task.
6.3.3 Searching, Evaluating, Selecting Relational Features
Given large number possible features could used task (such
example Facebook classification task), features actually used learn
model? cases, selection done manually based prior experience trial
error. many situations, though, automatic feature selection desirable.
non-relational data, widely studied topic machine learning (Guyon &
Elisseeff, 2003; Koller & Sahami, 1996; Yang & Pedersen, 1997; Dash & Liu, 1997; Jain
& Zongker, 1997; Pudil, Novovicova, & Kittler, 1994), selecting relational features
received considerably less attention. Given large number possible features, efficient
strategies searching evaluating possible features needed. section,
first summarize two key problems feature search feature evaluation,
give examples issues resolved actual SRL systems.
Search: first step searching relational features define possible
relational feature space specifying possible raw feature inputs (e.g., node link
feature values) operators consider. possible operators include domainindependent operators (e.g., mode, count) and/or problem-specific operators (e.g., count
number friends divided number groups). Domain-independent operators
obviously general easier apply, problem-specific operators reduce
number possibilities must considered require effort expert
knowledge. However, approaches vulnerable selection biases (Jensen et al., 2003;
Jensen & Neville, 2002). second step pick appropriate search strategy, usually
either exhaustive, random, guided. exhaustive strategy consider features
possible given specified inputs operators, random strategy
consider fraction space. guided strategy use heuristic subsystem identify features considered. three cases, feature
considered subjected evaluation strategy assesses usefulness;
strategies described next.
Evaluation Selection: feature considered must evaluated
way determine retained use final model. instance, candidate
feature may evaluated adding current classification model; improves
accuracy holdout set, immediately (and greedily) added set retained
features (Davis, Burnside, Castro Dutra, Page, & Costa, 2005; Davis, Ong, Struyf, Burnside,
Page, & Costa, 2007). cases, every candidate feature assigned score
best scoring feature retained (Neville, Jensen, Friedland, et al., 2003),
features added model based decreasing score, long new features
continue improve model (Mihalkova & Mooney, 2007). Simpler techniques
require evaluating overall model also used. instances, metrics
correlation mutual information used estimate useful feature
desired task. metrics strategies could used include Akaikes information
criterion (AIC) (Akaike, 1974), Mallows Cp (Mallows, 1973), Bayesian information criterion
(BIC) (Hannan & Quinn, 1979; Schwarz, 1978) many others (Shao, 1996; George &
409

fiRossi, McDowell, Aha, & Neville

Proposed System

Search method

Feature Evaluation

Exhaustive

Chi-square statistic/p-value

RDN-Boosting (Natarajan, Khot, Kersting, Gutmann, & Shavlik, 2012; Khot,
Natarajan, Kersting, & Shavlik, 2011)

Exhaustive

Weighted variance

ReFeX (Henderson et al., 2011)

Exhaustive

Log-binning disagreement

Random

Chi-square statistic/p-value

SAYU (Davis et al., 2005)

Aleph

AUC-PR

nFOIL (Landwehr et al., 2005)

FOIL

Conditional Log-Likelihood

SAYU-VISTA (Davis et al., 2007)

Aleph

AUC-PR

ProbFOIL (De Raedt & Thon, 2010)

FOIL

m-estimate

kFOIL (Landwehr et al., 2010)

FOIL

Kernel target alignment

Greedy hill-climbing

Bayesian model selection

Beam search

WPLL

Template-based

WPLL

Level-wise search

Pseudo-likelihood

Aleph++

m-estimate

RPT (Neville, Jensen, Friedland, et al.,
2003)

Spatiotemporal

RPT

(McGovern

et al., 2008)

PRM struct. learning (Getoor, Friedman, Koller, & Taskar, 2001)

TSDL (Kok & Domingos, 2005)
BUSL (Mihalkova & Mooney, 2007)
PBN

Learn-And-Join

(Khosravi,

Tong Man, Xu, & Bina, 2010)

Discriminative MLN structure
learning (Huynh & Mooney, 2008; Biba,
Ferilli, & Esposito, 2008)

Table 6: Systems Searching Selecting Node Features: summary
systems used automatically search select
appropriate features given task. Note that, depending context,
papers may describe function terms learning best rules
system learning structure (e.g., MLN). MLNbased systems described; these, WPLL weighted pseudo
log-likelihood.

McCulloch, 1993). Frequently, possible feature may particular parameter whose
value must set (such threshold); selecting best value given feature
use evaluation metrics may use simpler estimation technique, e.g., based
maximum likelihood.
Examples: Table 6 summarizes strategies used number SRL systems automatically search features. columns table describe system searches
410

fiTransforming Graph Data Statistical Relational Learning

features features evaluated. instance, Relational Probability Trees
(RPTs) (Neville, Jensen, Friedland, et al., 2003) extension probability estimation
trees relational data use exhaustive search strategy feature selection. particular, RPT learning involves automatically searching space possible features
using aggregation functions Mode, Average, Count, Proportion, Min, Max,
Exists, Degree. aggregations involve node link feature values (e.g.,
Average) topology information (e.g., Degree). features used
classification tasks, predicting class label document. feature
evaluated based using chi-square statistic measure correlation
feature class label; yields feature score associated p-value. Features
p-values level statistical significance discarded, remaining
feature highest score chosen inclusion model. selection process
also extended use randomization tests adjust biases common
relational data (Jensen et al., 2003; Jensen & Neville, 2002). RPTs also extended
temporal domains (Sharan & Neville, 2008; Rossi & Neville, 2012).
RPTs represent conditional probability distributions using single tree. contrast,
Natarajan et al. (2012) propose using gradient boosting (Friedman, 2001)
conditional probability distribution represented weighted sum regression trees
grown stage-wise optimization. features tree selected via depthlimited, exhaustive search, though note domain knowledge could also used
guide search. Natarajan et al. argue resultant set multiple, relatively shallow
trees allows efficient learning complex structures, demonstrate technique
outperform alternatives based single trees Markov Logic Networks discussed
below.
Another system uses exhaustive search ReFeX (Henderson et al., 2011),
uses aggregates Sum Mean operators recursively generate features based
degree node local neighborhood. prune resultant large set, ReFeX uses
logarithmic binning feature values, clusters features based similarity
binned space, retains one feature cluster. logarithmic binning
chosen favors features discriminative high-degree nodes.
recursive approach also modified constructing features dynamic
networks (Rossi, Gallagher, Neville, & Henderson, 2012).
Alternatively, spatiotemporal RPTs (McGovern et al., 2008) use random search strategy. particular, RPTs add temporal spatial-based features set possible
features. resultant feature space large exhaustive search, instead random
sampling used. pre-defined number features considered, best
scored feature added model.
remaining systems discuss use guided search strategy,
heuristic sub-system provides candidate features considered. instance,
several systems (Davis et al., 2005; Landwehr et al., 2005) use ILP system
generate candidate features, evaluate features select ultimate use.
particular, SAYU (Davis et al., 2005) uses ILP system Aleph (Srinivasan, 1999)
generate candidate feature (which consider new view original data).
Aleph creates candidates features based positive examples, training data,
concept predicted. proposed feature evaluated learning
411

fiRossi, McDowell, Aha, & Neville

new model includes feature computing area precision-recall
curve (AUC-PR). feature improves AUC-PR score, permanently added
model feature search continues. SAYU-VISTA (Davis et al., 2007) retains
general approach extends types features considered, particular
adding ability dynamically link together objects different types recursively
build new features constructed features. Davis et al. demonstrate link
connections especially helpful improving performance compared original SAYU
system. Landwehr et al. (2005) describe nFOIL system similar SAYU
developed independently, De Raedt Thon (2010) describe ProbFOIL
upgrades deterministic rule learner like FOIL probabilistic. Landwehr et al. (2010)
describe related kFOIL system integrates FOIL kernel methods. also
consider impact several different feature scoring functions.
number systems considered perform structure learning Probabilistic Relational Models (PRMs) (Getoor et al., 2001) Markov Logic Networks
(MLNs) (Domingos & Richardson, 2004), general case feature selection problems described above. instance, MLN weighted set first-order formulas;
structure learning corresponds learning formulas weight learning corresponds
learning associated weights. first MLN structure learning approaches systematically construct candidate clauses starting empty clause, greedily adding literals
it, testing resulting clauses fit training data using statistical measure (Kok
& Domingos, 2005; Biba et al., 2008). However, top-down approaches inefficient
initial proposal clauses ignores training data, resulting large number
possible features considered possible problems local minima. response,
number bottom-up approaches proposed. particular, Mihalkova
Mooney (2007) use propositional Markov network structure learner construct template
networks guide construction features based training data. recent
work examined enable bottom-up approaches learn longer clauses based
constraining search consider features consistent certain patterns motifs (Kok & Domingos, 2010), clustering input nodes create lifted graph
representation, enabling feature search smaller graph (Kok & Domingos, 2009).
Khosravi et al. (2010) perform MLN structure learning first learning structure
simpler Parametrized Bayes Net (PBN) (Poole, 2003), converting result
MLN. data contains significant number descriptive attributes, show
approach dramatically improves runtime structure learning also improves
predictive accuracy. Schulte (2011) given theoretical justification approach.
Another alternative, proposed Khot et al. (2011), extend previously mentioned
work Natarajan et al. (2012) gradient boosting MLNs. Essentially, problem
learning MLNs transformed series relational regression problems
functional gradients represented clauses trees. several datasets demonstrate faster MLN structure learning accurate better baselines including
algorithms Mihalkova Mooney (2007) Kok Domingos (2010).
techniques MLNs seek learn network structure best explains
training data whole. contrast, situations prediction specific predicate desired (e.g., predict political affiliation Facebook example),
Huynh Mooney (2008) Biba et al. (2008) propose discriminative approaches
412

fiTransforming Graph Data Statistical Relational Learning

MLN structure learning. instance, Huynh Mooney use modified version
Aleph (Srinivasan, 1999) compute large number candidate clauses, use form
L1 -regularization force weights subsequently learned clauses
zero clause helpful predicting predicate. regularization,
conjunction appropriate optimization function, effectively leads selecting
smaller set features useful desired task.
Discussion: focus article graph-based data representations (see Section 1.2).
However, many examples discussed use logical representation instead.
include section techniques used constructing searching
features rules similar settings. instance, RPTs (a graphbased approach) RDN-Boosting (a logical approach) use exhaustive search
probabilistic decision trees, different feature scoring strategies.
Popescul et al. (2003a) examine automatically learn new relational features
links (to support link prediction), techniques could also applied constructing
node features. particular, treat feature relational database query, use
concept refinement graphs (Shapiro, 1982) consider refining initial query
equi-joins, equality selections, statistical aggregates. refinement,
refinements considered; search guided sampling possible refinements proceeding results particular refinement type seems
promising. features chosen combined logistic regression classifier. evaluation specific features, use Bayesian Information Criterion (BIC) (Schwarz,
1978), includes term penalizes feature complexity reduce danger
overfitting.
discussed multiple systems include notions aggregation including RPTs,
SAYU-VISTA, work Popescul et al. (2003a) discussed above. also
aggregate-based learning approaches Crossmine (Yin, Han, Yang, & Yu, 2006),
CLAMF (Frank, Moser, & Ester, 2007), Multi-relational Decision Trees (MRDTL) (Leiva,
Gadia, & Dobbs, 2002), Confidence-based Concept Discovery (C2 D) (Kavurucu, Senkul, &
Toroslu, 2008), many others (Perlich & Provost, 2006; Krogel & Wrobel, 2001; Knobbe,
Siebes, & Marseille, 2002). also possibilities feature evaluation.
instance, GleanerSRL (Goadrich & Shavlik, 2007) uses Aleph (Srinivasan, 1999) search
clauses uses metric precision recall evaluating clauses.

7. Jointly Transforming Nodes Links
previous sections, primarily discussed relational representation transformation
techniques applied independently one another. instance, one technique
might used predict links, another builds transformed representation
applying node labeling technique. section instead examines joint transformation
tasks combine node link transformation way, instance label nodes
weight links simultaneously. techniques may enable subtask influence
helpful ways, avoids bias might introduced requiring
serialization two tasks (such link weighting node labeling) might usefully
performed jointly.
413

fiRossi, McDowell, Aha, & Neville

One recent approach proposed Namata, Kok, Getoor (2011) collectively performs link prediction, node labeling, entity resolution (which seen form
node deletion/merging). present iterative algorithm solves three tasks
simultaneously propagating information among solutions three tasks.
particular, introduce notion inter-relational features, relational features one task depend upon predicted values another. results show
using features improve accuracy, inferring predicted values
three tasks simultaneously significantly improve accuracy compared performing
three tasks sequence, even possible orderings considered.
Techniques model full distribution across links attributes RMNs
(Taskar et al., 2002), PRMs (Friedman et al., 1999), MLNs (Domingos & Richardson,
2004) also used scenario, instance jointly predict node link labels.
section, however, focus particularly recent techniques presume
existence textual content associated nodes links graph
(although basic algorithms would also work kinds features). consider
three types techniques, based kind input text use: stand-alone text
documents (e.g., legal memos links), text documents connected links (e.g.,
webpages hyperlinks), entities connected links associated text (e.g.,
people connected email messages). Table 7 lists prominent models,
grouped according three types. columns table indicate kinds
input models use (middle section) types transformation perform
(right-hand section). text documents corresponds node features table,
text associated links yields link features. discuss three types
techniques detail.
7.1 Using Text Documents Links
First, many techniques used assign topics labels nodes nodes
(such documents) associated text. instance, first row Table 7 indicates
LDA PLSA use nodes node features perform node prediction,
weighting, labeling. Section 6 already mentioned techniques used
label node one discovered topics, typical use. However,
techniques also perform node weighting (using weights associated
topics) and/or node prediction (by converting discovered topics new latent nodes
discussed introduction Section 5). Table 7, use lighter checkmarks
represent kind situations transformation task could performed
particular model primary use/output.
LDA PLSA treat document bag words seek assign one
topics (labels) document based words. contrast, Nubbi (Chang, BoydGraber, & Blei, 2009) designs approach based LDA graph defined based
objects (nodes) referenced set documents, links predicted based
relationships implied text documents. addition, nodes
links associated likely topic(s) based relationships. Thus,
model simultaneously performs link prediction, link labeling, node labeling. similar
414

fiTransforming Graph Data Statistical Relational Learning

Link Labeling

Node Prediction

Node Weighting

Node Labeling

E

XE

XE

V

XV

XV

V

XV

LDA/PLSA

X

X

Nubbi

X

X

X

Joint Transformation Model

E

XE

Link Weighting

Input

Nodes

Link Prediction

Links

X

X

X

X

X

X

X

Link-LDA, Link-PLSA

X

X

X

X

X

X

X

X

Pairwise-Link-LDA

X

X

X

X

X

X

X

X

Link-PLSA-LDA

X

X

X

X

X

X

X

X

Relational Topic Model (RTM)

X

X

X

X

X

X

X

X

Topic-Link LDA

X

X

X

X

X

X

X

X

Group-Topic (GT)

X

X

X

X

X

X

X

X

Author-Recipient-Topic (ART)

X

X

X

X

X

Block-LDA

X

X

X

X

X

X
X

X

X

Table 7: Summary Joint Transformation Models: middle section
table indicates types graph features used inputs model,
right side table indicates types link node transformation
performed model. Lighter checkmarks indicate output
model transformed perform particular transformation task (e.g.,
use node labels create new latent group nodes), task
primary goal specified model.

415

fiRossi, McDowell, Aha, & Neville

result produced semantic network extraction Kok Domingos (2008)
discussed Section 4.2.
7.2 Using Text Document Links
second type joint transformation also uses text documents, adds known links
documents model. instance, Section 6 discussed Link-LDA
Link-PLSA add link modeling LDA PLSA order perform node labeling;
discussed LDA PLSA modified also achieve node prediction
weighting. shown Table 7, Link-LDA Link-PLSA also used link
prediction weighting learning model training graph using
predict unseen links new test graph (Nallapati et al., 2008).
Link-LDA Link-PLSA model links way similar model
presence words document (node). instance, Link LDAs generative model,
generate one word, document chooses topic, chooses word topicspecific multinomial. identical process (using topic-specific multinomial) used
generate, particular document, one target document link to. Thus, Link-LDA
Link-PLSA directly extend original LDA PLSA models add links.
Nallapati et al. (2008) argue Link-LDAs Link-PLSAs extensions links,
pragmatic, adequately capture topical relationship two documents
linked together. Instead, propose two alternatives. first, Pairwise LinkLDA, replaces link model Link-LDA model based mixed membership
stochastic blockmodels (Airoldi et al., 2008), possible link modeled
Bernoulli variable conditioned topic chosen based topic distributions
two endpoints link. second approach, Link-PLSA-LDA, retains
link generation model Link-LDA, changes word generation model
documents (the ones incoming links) words document depend
topics documents link it. downside latter approach
works nodes divided set outgoing links set
incoming links. However, Nallapati et al. argue limitation largely
overcome duplicating nodes incoming outgoing links. Moreover,
approach much faster scalable Pairwise Link-LDA. Nallapati et al.
demonstrate models outperform Link-LDA likelihood ranking task,
Link-PLSA-LDA also outperforms Link-LDA link prediction task. also show
Link-PLSA-LDA Link-LDA comparable terms execution time,
Pairwise Link-LDA much slower.
Changes generative model used approaches encode different assumptions data lead significant performance differences. instance,
Chang Blei (2009) introduce Relational Topic Model (RTM) compare
Pairwise Link-LDA model discussed above. models allow similar flexibility terms
links defined, Chang Blei argue model forces topic
assignments used generate words documents also generate
links, true Pairwise Link-LDA. demonstrate RTM provides
accurate predictions link suggestions Pairwise Link-LDA several
baselines.
416

fiTransforming Graph Data Statistical Relational Learning

Another possible change model add types objects. instance,
Topic-Link LDA (Liu, Niculescu-Mizil, & Gryc, 2009) models documents, links,
likely topics associated document, also explicitly considers
author document clusters authors multiple communities. Creating
new clustering equivalent finding per-document topics author
associated one document. argue approach analogous
unifying separate tasks (1) assigning topics documents (2) analyzing
social network authors. show approach cases outperform
LDA Link-LDA.
7.3 Using Text Associated Links
final type joint transformation techniques form link features based text associated
links, text email messages (McCallum, Wang, & Corrada-Emmanuel,
2007) scientific abstracts relate particular protein-protein interaction (Balasubramanyan & Cohen, 2011). Several techniques discussed previously
context link interpretation. instance, Section 4.2 discussed models
Author-Recipient-Topic (ART) model (McCallum, Wang, & Corrada-Emmanuel, 2007)
Group-Topic (GT) model (McCallum, Wang, & Mohanty, 2007) extend LDA perform
link labeling; strength predicted labels (topics) also used weight
links. addition, GT model directly assigns nodes groups (i.e., node labeling),
labels ART associates link could also used label associated
nodes. RART model (McCallum, Wang, & Corrada-Emmanuel, 2007) extends ART
allowing node multiple roles. recently, Block-LDA (Balasubramanyan
& Cohen, 2011) merges ideas latent variables models stochastic blockmodels. specifically, Block-LDA shares information three components:
link model shares information block structure shared topic
model. Unlike GT ART, however, Block-LDA focuses labeling nodes rather
links. Balasubramanyan Cohen evaluate Block-LDA protein dataset
Enron email corpus demonstrate outperforms Link-LDA several
baselines task protein functional category prediction.
7.4 Discussion
techniques discussed variants latent group models focus
node and/or link label prediction, also used node prediction
new nodes represent newly discovered topics latent groups. models also
extended incorporate notions time (Dietz, Bickel, & Scheffer, 2007; Wang, Blei, &
Heckerman, 2008; Wang & McCallum, 2006), topic hierarchies (Li & McCallum, 2006),
correlations topics (Blei & Lafferty, 2007). addition, links usually assumed
generated based overall topic(s) node link. contrast, Latent
Topic Hypertext Model (LTHM) (Gruber, Rosen-Zvi, & Weiss, 2008) models link
originating specific word document. Somewhat surprisingly, show
approach leads model fewer parameters models like Link-LDA,
demonstrate approach outperforms Link-LDA Link-PLSA
evaluated link prediction task.
417

fiRossi, McDowell, Aha, & Neville

(a) Initial Graph

(b) Joint Transformation

Figure 10: Example Joint Transformation: example, new latent nodes
added represent discovered topics, weighted links added
original node new latent node. addition, weighted links added
latent nodes, representing connection strength topics.
Finally, new links original nodes may also predicted. Note
example adapted results found work Nallapati et al. (2008).

new nodes added graph represent discovered topics, links
invariably added connect existing nodes new nodes. However, models may
also learn information discovered topics related other.
instance, Figure 10 shows two new topics discovered graph
connected existing nodes. addition, topics connected
new links weight link represents frequently document
topic cites document representing different topic. Adding additional links
graph lets original nodes connected closely primary topics
also related topics.

8. Discussion Challenges
section discuss additional issues related relational representation
transformation highlight important challenges future work.
8.1 Guiding Evaluating Representation Transformation
goal representation transformation often improve data representation
way leads better results subsequent task possibly understandable representation. evaluate whether particular transformation technique
accomplished goal? first address question, consider final
goal used directly guide initial transformation.
tasks, representation evaluation straightforward provided ground truth
values known hold-out data set. instance, test technique link
418

fiTransforming Graph Data Statistical Relational Learning

prediction effective, accuracy measured links predicted hold-out set
(Taskar et al., 2003; Liu et al., 2009). particular evaluation metric modified
appropriate domain. instance, Chang Blei (2009) evaluate precision
twenty highest-ranked links suggested document, Nallapati et al. (2008)
consider custom metric called RKL measures rank last true link suggested
model. Likewise, desired task involves classification, classification
algorithm run hold-out data, without representation change,
see change increases classification accuracy.
cases, may difficult directly measure well representation change
performed, classification used surrogate measure: accuracy increases,
change assumed beneficial. instance, classification used evaluate link prediction (Gallagher et al., 2008), link weighting (Xiang et al., 2010), link labeling (Rossi & Neville, 2010; Macskassy, 2007), node prediction (Neville & Jensen,
2005). addition, node labeling naturally classification problem, node weighting
usually evaluated ways, e.g., based query relevance.
techniques used direct evaluation feasible, exists
metric believed related. instance, higher autocorrelation
graph associated presence sensible links, algorithms
collective classification typically perform better level autocorrelation higher.
Thus, Xiang et al. (2010) demonstrate success technique estimating relationship strengths (link weights) based part showing increase autocorrelation
measured several attributes social network. Likewise, increased information gain
attributes could used demonstrate improved representation (Lippi,
Jaeger, Frasconi, & Passerini, 2009), link perplexity could used assess topic labelings (Balasubramanyan & Cohen, 2011). Naturally, appropriate evaluation
techniques vary based upon task, comparison transformation techniques may
yield different results depending upon metric chosen.
Ideally, representation transformation would guided directly final goal
executed, rather evaluated transformation complete.
often case feature selection structure learning algorithms discussed
Section 6.3: task accuracy (or surrogate measure) evaluated particular feature
added, retained accuracy improved. cases, transformation
even directly specified desired end goal. instance, supervised random
walk approach discussed Section 3.3 uses gradient descent method obtain new link
weights links predicted subsequent random walk (their final goal)
accurate. Likewise, Menon Elkan (2010) show add supervision methods
generating latent features (see introduction Section 5) features learned
would relevant final classification task. show, however, adding
supervision always helpful. final example, Shi, Li, Yu (2011) use
quadratic program optimize linear combination link weights final link
weights lead directly accurate classification via label propagation algorithm.
general, ensuring particular transformation improve performance
final SRL task remains challenging. Many transformations cannot directly guided
final goal, either suitable supervised data available, clear
419

fiRossi, McDowell, Aha, & Neville

modify transformation algorithms use information (e.g., latent
topic models Section 7 group detection algorithms Section 5).
8.2 Causal Discovery
Causal discovery refers identifying cause-and-effect relationships (i.e., smoking causes
cancer) either online experimentation (Aral & Walker, 2010) observational
data. challenge distinguish true causal relationships mere statistical correlations. One approach use quasi-experimental designs (QEDs), take advantage
circumstances non-experimental data identify situations provide equivalent
experimental control randomization. Jensen, Fast, Taylor, Maier (2008) propose
system discover knowledge applying QEDs discovered automatically.
recently, Oktay, Taylor, Jensen (2010) apply three different QEDs demonstrate
one gain causal understanding social media system. also another causal
discovery technique linear models proposed Wang Chan (2010). challenge
remains extend techniques apply broader range relational data.
8.3 Subgraph Transformation Graph Generation
majority article focused transformation tasks centered around nodes
links graphs. However, also useful tasks subgraph transformation
seek identify frequent/informative substructures set graphs create features
classify subgraphs (Inokuchi, Washio, & Motoda, 2000; Deshpande, Kuramochi,
Wale, & Karypis, 2005). instance, Kong Yu (2010) consider use semisupervised techniques perform feature selection subgraph classification given
labeled subgraphs. nodes links, subgraphs tasks prediction,
labeling, weighting, feature generation described. Many techniques
described node-centered features also used context, full
discussion subgraph transformation beyond scope article.
Recently, graph generation algorithms attracted significant interest. algorithms use model represent family graphs, present way generate multiple samples family. Two prominent models Kronecker Product Graph Models
(KPGMs) (Leskovec, Chakrabarti, et al., 2010) based preferential attachment
(Price, 1976; Barabasi & Albert, 1999). graph generation methods take advantage
global (with KPGMs) local (with preferential attachment models) graph properties
generate distribution graphs potentially include attributes. Sampling
models useful creating robust algorithms, instance training
classifier family related graphs instead single graph. Newman (2003) surveys
additional network models properties relevant graph generation.
8.4 Model Representation
SRL also notion model representation: kind statistical model
learned represent relationship nodes, links, features?
prominent models SRL Probabilistic Relational Models (PRMs) (Friedman
et al., 1999), Relational Markov Networks (RMNs) (Taskar et al., 2002), Relational Depen420

fiTransforming Graph Data Statistical Relational Learning

dency Networks (RDNs) (Neville & Jensen, 2007), Structural Logistic Regression (Popescul
et al., 2003b), Conditional Random Fields (CRFs) (Lafferty, McCallum, & Pereira, 2001),
Markov Logic Networks (MLNs) (Domingos & Richardson, 2004; Richardson & Domingos, 2006); full discussion models beyond scope article. many cases
techniques relational representation transformation, link prediction, performed regardless kind statistical model subsequently used. However,
choice statistical model strongly interact kinds node link features
useful (or even possible use); Section 6.3 describes connections.
number relevant comparisons already published (Jensen et al., 2004; Neville
& Jensen, 2007; Macskassy & Provost, 2007; Sen et al., 2008; McDowell et al., 2009; Crane
& McDowell, 2011), work needed evaluate interaction choice
statistical model feature selection, evaluate statistical models work best
domains certain characteristics.
8.5 Temporal Spatial Representation Transformation
appropriate, already discussed multiple techniques incorporate
temporal information graph data (see especially Sections 4.2, 6.1, 6.3).
techniques focused solving particular problems node classification, dealing
data invariably requires studying represent time-varying elements.
However, work needed examine general tradeoffs involved different
temporal representations. instance, Hill, Agarwal, Bell, Volinsky (2006) provide
generic framework modeling temporal dynamic network central goal
build approximate representation satisfies pre-specified objectives. focus
summarization (representing historical behavior two nodes concise manner),
simplification (removing noise edges nodes, spurious transactions, stale relationships), efficiency (supporting fast analysis updating), predictive performance
(optimizing representation maximize predictive performance). work provides
number useful building blocks, comparisons needed to, instance, evaluate merits using summarized networks general-purpose algorithms vs. using
specialized algorithms data maintains temporal distinctions.
Temporal data one particular kind data represented relational
sequence. Kersting, De Raedt, Gutmann, Karwath, Landwehr (2008) survey area
relational sequence learning explains multiple tasks related data,
sequence mining alignment. tasks often involve need identify relevant
features structure, identifying frequent patterns useful similarity functions.
Thus, set useful techniques feature construction search domain overlap
discussed Section 6.3.
8.6 Privacy Preserving Representation
sometimes desire make private graph-based data publicly available (e.g.,
support research public policy) way preserves privacy individuals
described data. goal privacy preserving representation transform
data way minimizes information loss maximizing anonymization, e.g.,
prevent individuals anonymized network identified. Naive approaches
421

fiRossi, McDowell, Aha, & Neville

anonymization operate simply replacing individuals name (or attributes)
arbitrary meaningless unique identifiers. However, social networks many
adversarial methods true identity user often discovered
anonymized network. particular, adversarial methods use network
structure and/or remaining attributes discover identities users within network
(Liu & Terzi, 2008; Zhou, Pei, & Luk, 2008; Narayanan & Shmatikov, 2009).
early approach Zheleva Getoor (2007) examines graph may modified
prevent sensitive relationships (a particular kind labeled link) disclosed.
describe approach terms node anonymization edge anonymization.
Node anonymization clusters nodes equivalence classes based node attributes
only, edge anonymization approaches based cleverly removing
sensitive edges. Backstrom, Dwork, Kleinberg (2007) address related family attacks
adversary able learn whether edge exists targeted pairs nodes.
recently, Hay, Miklau, Jensen, Towsley, Weis (2008) study privacy issues
graphs contain attributes. goal prevent structural re-identification
(i.e., identity reconstruction using graph topology information) anonymizing graph via
creating aggregate network model allows samples drawn model.
approach generalizes graph partitioning nodes summarizing graph
partition level. approach differs approaches described
drastically changes representation opposed making incremental
changes. However, method enforces privacy still preserving enough network
properties allow wide variety network analyses performed.
investigations key factors information available graph,
resources attacker, type attacks must defended against.
addition, attacker possibly obtain additional information related graph
sources, challenges even difficult. work needed
provide strong privacy guarantees still enabling partial public release graph-based
information.

9. Conclusion
Given increasing prevalence importance relational data, article surveyed
significant current issues relational representation transformation. presenting new taxonomy important transformation tasks Section 2, next
discussed four primary tasks link prediction, link interpretation, node prediction,
node interpretation. Section 7 considered tasks accomplished
simultaneously via techniques joint transformation. Finally, Section 8 considered
perform representation evaluation key challenges future work.
additional possible representation transformations space
discuss, fit cleanly taxonomy Figure 2. instance, bipartite
graph customers products, may useful eliminate product nodes, replacing
information content new links among customers purchased
product. somewhat related group discovery techniques Section 5.
also considered depth potential transforming nodes edges
422

fiTransforming Graph Data Statistical Relational Learning

vice versa (though representation choices Figure 6 also relevant here),
technique sometimes useful pre-processing step.
taxonomy presented Section 2 highlighted symmetry possible
transformation tasks links nodes. symmetry helped organize
survey, also suggests areas techniques developed one entities
used analogous task other. instance, Liben-Nowell Kleinberg
(2007) reformulated traditional node weighting algorithms weight links. Likewise, topic
discovery techniques based LDA used node labeling link labeling.
Finally, many techniques used create node features also used create link
features, vice versa, although node features studied much thoroughly.
discussed Section 8, remains much work do. instance, link prediction
remains difficult problem, especially general case two arbitrary
nodes might connected together. Even significantly, described
wide range techniques address transformation tasks, end
day practitioner left wide range choices without many guarantees
might work best. instance, node weighting may improve classification accuracy
one dataset decrease another. challenge made difficult
techniques described come wide range areas, including
graph theory, social network analysis, numerical linear algebra (e.g., matrix factorization),
metric learning, information theory, information retrieval, inductive logic programming,
statistical relational learning, probabilistic graphical models. breadth
techniques relevant relational transformation wonderful resource, also means
evaluating representation change techniques relevant particular task
time-consuming, technically challenging, incomplete process. Therefore, much
work needed establish theoretical understanding different representation
changes affect data, different data characteristics interact process,
combination techniques data characteristics affect final results
analysis relational data.

Acknowledgments
thank reviewers many helpful suggestions feedback. majority
work completed Naval Research Laboratory, Ryan Rossi supported
ASEE/ONR NREIP summer internship 2010 NSF Graduate Research
Fellowship (at Purdue University). Luke McDowell supported part NSF award
number 1116439 grant ONR. research also partly supported
NSF contract number IIS-1149789. views conclusions contained
herein authors interpreted necessarily representing
official policies endorsements, either expressed implied, ONR, NSF, U.S.
Government.

References
Adamic, L. A., & Adar, E. (2001). Friends neighbors web. Social Networks,
25 (3), 211230.
423

fiRossi, McDowell, Aha, & Neville

Adibi, J., Chalupsky, H., Melz, E., Valente, A., et al. (2004). KOJAK group finder:
Connecting dots via integrated knowledge-based statistical reasoning.
Proceedings 16th Conference Innovative Applications Artifical Intelligence,
pp. 800807.
Adomavicius, G., & Tuzhilin, A. (2005). Toward next generation recommender
systems: survey state-of-the-art possible extensions. IEEE Transactions
Knowledge Data Engineering, 17 (5), 734749.
Ahmed, N., Neville, J., & Kompella, R. (2012a). Network sampling designs relational
classification. Proceedings 6th International AAAI Conference Weblogs
Social Media.
Ahmed, N., Neville, J., & Kompella, R. (2012b). Space-efficient sampling social activity
streams. BigMine, pp. 18.
Ahmed, N., Berchmans, F., Neville, J., & Kompella, R. (2010). Time-based sampling
social network activity graphs. Proceedings 8th Workshop Mining
Learning Graphs, pp. 19.
Airoldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed membership
stochastic blockmodels. Journal Machine Learning Research, 9, 19812014.
Akaike, H. (1974). new look statistical model identification. IEEE Transactions
Automatic Control, 19 (6), 716723.
Albert, R., Jeong, H., & Barabasi, A. (1999). Internet: Diameter world-wide web.
Nature, 401 (6749), 130131.
Amarel, S. (1968). representations problems reasoning actions. Machine
Intelligence, 3, 131171.
Anthony, A., & desJardins, M. (2007). Data clustering relational push-pull model.
Proceedings Seventh IEEE International Conference Data Mining Workshops, ICDMW 07, pp. 189194.
Aral, S., & Walker, D. (2010). Creating Social Contagion Viral Product Design:
Randomized Trial Peer Influence Networks. Proceedings 31st International Conference Information Systems.
Backstrom, L., & Leskovec, J. (2011). Supervised random walks: predicting recommending links social networks. Proceedings 4th International Conference
Web Search Data Mining, pp. 635644.
Backstrom, L., Dwork, C., & Kleinberg, J. M. (2007). Wherefore art thou r3579x?:
anonymized social networks, hidden patterns, structural steganography. Proceedings 16th International World Wide Web Conference, pp. 181190.
Balasubramanyan, R., & Cohen, W. (2011). Block-LDA: Jointly modeling entity-annotated
text entity-entity links. Proceedings 7th SIAM International Conference
Data Mining.
Barabasi, A., & Albert, R. (1999). Emergence scaling random networks. Science,
286 (5439), 509512.
424

fiTransforming Graph Data Statistical Relational Learning

Barabasi, A., & Crandall, R. (2003). Linked: new science networks. American journal
Physics, 71 (4), 409410.
Basilico, J. B., & Hofmann, T. (2004). Unifying collaborative content-based filtering.
Proceedings 21st International Conference Machine Learning, pp. 6572.
Ben-Hur, A., & Noble, W. (2005). Kernel methods predicting protein-protein interactions. Bioinformatics, 21 (Suppl. 1), 3846.
Benczur, A., Csalogany, K., Sarlos, T., & Uher, M. (2005). Spamrankfully automatic link
spam detection. Adversarial Information Retrieval Web, pp. 2538.
Berkhin, P. (2006). Survey clustering data mining techniques. Grouping Multidimensional
Data: Recent Advances Clustering, 10, 2571.
Bharat, K., & Henzinger, M. (1998). Improved algorithms topic distillation hyperlinked environment. Proceedings 21st International SIGIR Conference
Research Development Information Retrieval, pp. 104111.
Bhattacharya, I., & Getoor, L. (2005). Relational clustering multi-type entity resolution.
Proceedings 4th International workshop Multi-relational Mining, pp. 312.
Bhattacharya, I., & Getoor, L. (2007). Collective entity resolution relational data. Transactions Knowledge Discovery Data, 1 (1), 136.
Biba, M., Ferilli, S., & Esposito, F. (2008). Discriminative structure learning Markov
logic networks. Inductive Logic Programming, 5194, 5976.
Bilgic, M., Mihalkova, L., & Getoor, L. (2010). Active learning networked data.
Proceedings 27th International Conference Machine Learning.
Bilgic, M., Namata, G. M., & Getoor, L. (2007). Combining collective classification link
prediction. Proceedings 7th IEEE International Conference Data Mining
Workshops, pp. 381386.
Blei, D., Ng, A., & Jordan, M. (2003). Latent Dirichlet allocation. Journal Machine
Learning Research, 3, 9931022.
Blei, D., & Lafferty, J. (2007). correlated topic model science. Annals Applied
Statistics, 1 (1), 1735.
Bonacich, P., & Lloyd, P. (2001). Eigenvector-like measures centrality asymmetric
relations. Social Networks, 23 (3), 191201.
Borgman, C., & Furner, J. (2002). Scholarly communication bibliometrics. Annual
Review Information Science Technology, 36, 372.
Brightwell, G., & Winkler, P. (1990). Maximum hitting time random walks graphs.
Random Structures & Algorithms, 1 (3), 263276.
Broder, A., Kumar, R., Maghoul, F., Raghavan, P., Rajagopalan, S., Stata, R., Tomkins,
A., & Wiener, J. (2000). Graph structure web. Computer networks, 33 (1-6),
309320.
Burges, C. (1998). tutorial support vector machines pattern recognition. Data
mining knowledge discovery, 2 (2), 121167.
425

fiRossi, McDowell, Aha, & Neville

Cafarella, M. J., Wu, E., Halevy, A., Zhang, Y., & Wang, D. Z. (2008). Webtables: Exploring
power tables web. Proceedings VLDB, pp. 538549.
Camacho, J., Guimera, R., & Nunes Amaral, L. (2002). Robust patterns food web
structure. Physical Review Letters, 88 (22), 228102: 14.
Chakrabarti, S., Dom, B., & Indyk, P. (1998). Enhanced hypertext categorization using
hyperlinks. Proceedings ACM SIGMOD International Conference Management Data, pp. 307318.
Chakrabarti, S., Dom, B., Raghavan, P., Rajagopalan, S., Gibson, D., & Kleinberg, J.
(1998). Automatic resource compilation analyzing hyperlink structure associated text. Computer Networks ISDN Systems, 30 (1-7), 6574.
Chang, J., & Blei, D. (2009). Relational topic models document networks.
9th International Conference Artificial Intelligence Statistics (AISTATS), pp.
8188.
Chang, J., Boyd-Graber, J., & Blei, D. (2009). Connections lines: augmenting
social networks text. Proceedings 15th ACM SIGKDD International
Conference Knowledge Discovery Data Mining, pp. 169178.
Clauset, A., Moore, C., & Newman, M. (2008). Hierarchical structure prediction
missing links networks. Nature, 453 (7191), 98101.
Cohn, D., & Chang, H. (2000). Learning probabilistically identify authoritative documents. Proceedings 17th International Conference Machine Learning, pp.
167174.
Cohn, D., & Hofmann, T. (2001). missing link-a probabilistic model document content hypertext connectivity. Advances Neural Information Processing Systems,
13, 430436.
Crane, R., & McDowell, L. K. (2011). Evaluating markov logic networks collective
classification. Proceedings 9th MLG Workshop 17th ACM SIGKDD
Conference Knowledge Discovery Data Mining.
Craven, M., DiPasquo, D., Freitag, D., McCallum, A., Mitchell, T., Nigam, K., & Slattery,
S. (2000). Learning construct knowledge bases World Wide Web. Artificial
Intelligence, 118 (1-2), 69113.
Cristianini, N., & Shawe-Taylor, J. (2000). Introduction Support Vector Machines
kernel-based learning methods. Cambridge University Press.
Dash, M., & Liu, H. (1997). Feature selection classification. Intelligent data analysis,
1 (3), 131156.
Davis, J., Burnside, E., Castro Dutra, I., Page, D., & Costa, V. (2005). integrated
approach learning Bayesian networks rules. Proceedings European
Conference Machine Learning, pp. 8495.
Davis, J., Ong, I., Struyf, J., Burnside, E., Page, D., & Costa, V. S. (2007). Change representation statistical relational learning. Proceedings 20th International
Joint Conference Artificial Intelligence, pp. 27192725.
426

fiTransforming Graph Data Statistical Relational Learning

De Raedt, L. (2008). Logical relational learning. Springer.
De Raedt, L., & Kersting, K. (2008). Probabilistic inductive logic programming. SpringerVerlag.
De Raedt, L., & Thon, I. (2010). Probabilistic rule learning. Inductive Logic Programming,
6489, 4758.
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990).
Indexing latent semantic analysis. Journal American Society Information
Science, 41, 391407.
Deshpande, M., Kuramochi, M., Wale, N., & Karypis, G. (2005). Frequent substructurebased approaches classifying chemical compounds. IEEE Transactions Knowledge Data Engineering, 13, 10361050.
Dhillon, I. (2001). Co-clustering documents words using bipartite spectral graph partitioning. Proceedings seventh ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 269274.
Dietz, L., Bickel, S., & Scheffer, T. (2007). Unsupervised prediction citation influences.
Proceedings 24th International Conference Machine Learning, pp. 233240.
Domingos, P., & Richardson, M. (2004). Markov logic: unifying framework statistical
relational learning. Proceedings ICML Workshop Statistical Relational
Learning, pp. 4954.
DuBois, C., & Smyth, P. (2010). Modeling Relational Events via Latent Classes. Proceedings 16th ACM SIGKDD International Conference Knowledge Discovery
Data Mining, pp. 803812.
Dunne, J., Williams, R., & Martinez, N. (2002). Food-web structure network theory:
role connectance size. Proceedings National Academy Sciences
United States America, 99 (20), 12917.
Easley, D., & Kleinberg, J. (2010). Networks, Crowds, Markets: Reasoning
Highly Connected World. Cambridge University Press.
Eckart, C., & Young, G. (1936). approximation one matrix another lower rank.
Psychometrika, 1 (3), 211218.
Egghe, L., & Rousseau, R. (1990). Introduction informetrics. Elsevier Science Publishers.
Erosheva, E., Fienberg, S., & Lafferty, J. (2004). Mixed-membership models scientific
publications. Proceedings National Academy Sciences United States
America, 101 (Suppl 1), 5220.
Essen, U., & Steinbiss, V. (1992). Cooccurrence smoothing stochastic language modeling. Proceedings International Conference Acoustics, Speech, Signal
Processing, pp. 161164.
Faloutsos, M., Faloutsos, P., & Faloutsos, C. (1999). power-law relationships
internet topology. Proceedings ACM SIGCOMM International Conference
Applications, Technologies, Architectures, Protocols Computer Communication, pp. 251262.
427

fiRossi, McDowell, Aha, & Neville

Fast, A., & Jensen, D. (2008). stacked models perform effective collective classification.
Proceedings IEEE International Conference Data Mining, pp. 785790.
Fodor, I. (2002). Survey Dimension Reduction Techniques. US DOE Office Scientific
Technical Information, 18.
Frank, R., Moser, F., & Ester, M. (2007). method multi-relational classification
using single multi-feature aggregation functions. Proceedings Principles
Practice Knowledge Discovery Databases, 1, 430437.
Freeman, L. C. (1977). set measures centrality based betweenness. Sociometry,
40, 3541.
Friedman, J. (2001). Greedy function approximation: gradient boosting machine..
Annals Statistics, 29 (5), 11891232.
Friedman, N., Getoor, L., Koller, D., & Pfeffer, A. (1999). Learning probabilistic relational models. Proceedings 16th International Joint Conference Artificial
Intelligence, pp. 13001309. Springer-Verlag.
Gallagher, B., Tong, H., Eliassi-Rad, T., & Faloutsos, C. (2008). Using ghost edges
classification sparsely labeled networks. Proceedings 14th ACM SIGKDD
International Conference Knowledge Discovery Data Mining, pp. 256264.
Gartner, T. (2003). survey kernels structured data. ACM SIGKDD Explorations
Newsletter, 5 (1), 4958.
George, E., & McCulloch, R. (1993). Variable selection via Gibbs sampling. Journal
American Statistical Association, 88, 881889.
Getoor, L., Friedman, N., Koller, D., & Taskar, B. (2003). Learning probabilistic models
link structure. Journal Machine Learning Research, 3, 679707.
Getoor, L., & Taskar, B. (Eds.). (2007). Introduction Statistical Relational Learning.
MIT Press.
Getoor, L., & Diehl, C. P. (2005). Link mining. SIGKDD Explorations, 7, 312.
Getoor, L., Friedman, N., Koller, D., & Taskar, B. (2001). Learning probabilistic models
relational structure. Proceedings International Conference Machine Learning,
pp. 170177.
Gibson, D., Kleinberg, J., & Raghavan, P. (1998). Inferring web communities link
topology. Proceedings 9th ACM Conference Hypertext Hypermedia,
pp. 225234.
Gilbert, E., & Karahalios, K. (2009). Predicting tie strength social media. Proceedings 27th CHI International Conference Human Factors Computing
Systems, pp. 211220.
Girvan, M., & Newman, E. J. (2002). Community structure social biological networks.
Proceedings National Academy Sciences, 99 (12), 78217826.
Goadrich, M., & Shavlik, J. (2007). Combining clauses various precisions recalls
produce accurate probabilistic estimates. Proceedings 17th International
Conference Inductive Logic Programming, pp. 122131.
428

fiTransforming Graph Data Statistical Relational Learning

Gobel, F., & Jagers, A. (1974). Random walks graphs. Stochastic processes
applications, 2 (4), 311336.
Godbole, N., Srinivasaiah, M., & Skiena, S. (2007). Large-scale sentiment analysis news
blogs. Proceedings International Conference Weblogs Social
Media.
Golub, G., & Reinsch, C. (1970). Singular value decomposition least squares solutions.
Numerische Mathematik, 14 (5), 403420.
Green, J. (1972). Latitudinal variation associations planktonic Rotifera. Journal
zoology, 167 (1), 3139.
Gruber, A., Rosen-Zvi, M., & Weiss, Y. (2008). Latent topic models hypertext.
Proceedings 24th Conference Uncertainty Artificial Intelligence, pp. 230
239.
Guyon, I., & Elisseeff, A. (2003). introduction variable feature selection. Journal
Machine Learning Research, 3, 11571182.
Gyongyi, Z., Garcia-Molina, H., & Pedersen, J. (2004). Combating web spam trustrank.
Proceedings VLDB, pp. 576587.
Hannan, E., & Quinn, B. (1979). determination order autoregression.
Journal Royal Statistical Society. Series B (Methodological), 41, 190195.
Harshman, R. (1970). Foundations PARAFAC procedure: Models conditions
explanatory multi-modal factor analysis. UCLA Working Papers Phonetics,
16 (1), 84.
Hartigan, J., & Wong, M. (1979). k-means clustering algorithm. Journal Royal
Statistical Society. Series C, Applied statistics, 28, 100108.
Hasan, M. A., Chaoji, V., Salem, S., & Zaki, M. (2006). Link prediction using supervised
learning. Proceedings SDM Workshop Link Analysis, Counterterrorism
Security.
Haveliwala, T. (2003). Topic-sensitive pagerank: context-sensitive ranking algorithm
web search. IEEE transactions knowledge data engineering, 15, 784796.
Hay, M., Miklau, G., Jensen, D., Towsley, D., & Weis, P. (2008). Resisting structural reidentification anonymized social networks. Proceedings VLDB, pp. 102114.
He, D., & Parker, D. (2010). Topic Dynamics: alternative model Bursts Streams
Topics. Proceeding 16th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 443452.
Henderson, K., Gallagher, B., Li, L., Akoglu, L., Eliassi-Rad, T., Tong, H., & Faloutsos,
C. (2011). Know: Graph Mining Using Recursive Structural Features.
Proceedings 17th ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 110.
Hill, S., Agarwal, D., Bell, R., & Volinsky, C. (2006). Building effective representation
dynamic networks. Journal Computational Graphical Statistics, 15 (3),
584608.
429

fiRossi, McDowell, Aha, & Neville

Hoff, P., Raftery, A., & Handcock, M. (2002). Latent space approaches social network
analysis. Journal American Statistical Association, 97 (460), 10901098.
Hofmann, T. (1999). Probabilistic latent semantic analysis. Proceedings Uncertainty
Artificial Intelligence, pp. 289296.
Huh, S., & Fienberg, S. (2010). Discriminative Topic Modeling based Manifold Learning.
Proceeding 16th ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 653661.
Huynh, T., & Mooney, R. (2008). Discriminative structure parameter learning
markov logic networks. Proceedings 25th International Conference Machine Learning.
Inokuchi, A., Washio, T., & Motoda, H. (2000). apriori-based algorithm mining
frequent substructures graph data. Principles Data Mining Knowledge
Discovery, pp. 1323.
Jaccard, P. (1901). Etude comparative de la distribution florale dans une portion des Alpes
et du Jura. Impr. Corbaz.
Jackson, M. (2008). Social economic networks. Princeton Univ Press.
Jain, A., & Zongker, D. (1997). Feature selection: Evaluation, application, small sample performance. IEEE Transactions Pattern Analysis Machine Intelligence,
19 (2), 153158.
Jeh, G., & Widom, J. (2002). SimRank: measure structural-context similarity.
Proceedings eighth ACM SIGKDD International Conference Knowledge Discovery Data Mining, pp. 538543.
Jensen, D., & Neville, J. (2002). Linkage autocorrelation cause feature selection bias
relational learning. Proceedings 19th International Conference Machine
Learning, pp. 259266.
Jensen, D., Neville, J., & Gallagher, B. (2004). collective inference improves relational
classification. Proceedings 10th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 593598.
Jensen, D., Neville, J., & Hay, M. (2003). Avoiding bias aggregating relational data
degree disparity. Proceedings 20th International Conference Machine
Learning, pp. 274281.
Jensen, D., Fast, A., Taylor, B., & Maier, M. (2008). Automatic identification quasiexperimental designs discovering causal knowledge. Proceeding 14th
ACM SIGKDD International Conference Knowledge Discovery Data Mining,
pp. 372380.
Jeong, H., Mason, S., Barabasi, A., & Oltvai, Z. (2001). Lethality centrality protein
networks. Nature, 411 (6833), 4142.
Jeong, H., Tombor, B., Albert, R., Oltvai, Z., & Barabasi, A. (2000). large-scale
organization metabolic networks. Nature, 407 (6804), 651654.
430

fiTransforming Graph Data Statistical Relational Learning

Joachims, T. (1998). Text categorization support vector machines: Learning many
relevant features. Proceedings European Conference Machine Learning,
pp. 137142.
Johnson, S. (1967). Hierarchical clustering schemes. Psychometrika, 32 (3), 241254.
Kahanda, I., & Neville, J. (2009). Using transactional information predict link strength
online social networks. Proceedings 4th International Conference Weblogs
Social Media, pp. 106113.
Kamvar, S., Klein, D., & Manning, C. (2003). Spectral learning. Proceedings 18th
International Joint Conference Artificial Intelligence, pp. 561566.
Kashima, H., & Abe, N. (2006). parameterized probabilistic model network evolution
supervised link prediction. Proceedings IEEE International Conference
Data Mining, pp. 340349.
Katz, L. (1953). new status index derived sociometric analysis. Psychometrika,
18 (1), 3943.
Kavurucu, Y., Senkul, P., & Toroslu, I. (2008). Aggregation confidence-based concept discovery multi-relational data mining. Proceedings IADIS European Conference
Data Mining, pp. 4352.
Kersting, K., & De Raedt, L. (2002). Basic principles learning Bayesian logic programs.
Tech. rep. 174, Institute Computer Science, University Freiburg.
Kersting, K., De Raedt, L., Gutmann, B., Karwath, A., & Landwehr, N. (2008). Relational
sequence learning. Probabilistic inductive logic programming, 4911, 2855.
Khosravi, H., Tong Man, O., Xu, X., & Bina, B. (2010). Structure learning markov logic
networks many descriptive attributes. Proceedings 24th Conference
Artificial Intelligence, pp. 487493.
Khot, T., Natarajan, S., Kersting, K., & Shavlik, J. (2011). Learning markov logic networks via functional gradient boosting. Data Mining (ICDM), 2011 IEEE 11th
International Conference on, pp. 320329. IEEE.
Kim, M., & Leskovec, J. (2011). network completion problem: Inferring missing nodes
edges networks. Proceedings SIAM International Conference Data
Mining.
Kleczkowski, A., & Grenfell, B. (1999). Mean-field-type equations spread epidemics:
small worldmodel. Physica A: Statistical Mechanics Applications, 274 (12), 355360.
Kleinberg, J. (1999). Authoritative sources hyperlinked environment. Journal
ACM, 46 (5), 604632.
Knobbe, A., Siebes, A., & Marseille, B. (2002). Involving aggregate functions multirelational search. Principles Data Mining Knowledge Discovery, pp. 145
168.
Kohavi, R., & John, G. (1997). Wrappers feature subset selection. Artificial intelligence,
97 (1-2), 273324.
431

fiRossi, McDowell, Aha, & Neville

Kohonen, T. (1990). self-organizing map. Proceedings IEEE, 78 (9), 14641480.
Kok, S., & Domingos, P. (2005). Learning structure Markov logic networks.
Proceedings 22nd International Conference Machine Learning, pp. 441448.
Kok, S., & Domingos, P. (2007). Statistical predicate invention. Proceedings 24th
International Conference Machine Learning, pp. 433440.
Kok, S., & Domingos, P. (2008). Extracting semantic networks text via relational clustering. Proceedings European Conference Machine Learning Principles
Practice Knowledge Discovery Databases, pp. 624639.
Kok, S., & Domingos, P. (2009). Learning markov logic network structure via hypergraph
lifting. Proceedings 26th International Conference Machine Learning, pp.
505512.
Kok, S., & Domingos, P. (2010). Learning markov logic networks using structural motifs.
Proceedings 27th International Conference Machine Learning.
Kolda, T. G., Bader, B. W., & Kenny, J. P. (2005). Higher-order web link analysis using
multilinear algebra. Proceedings IEEE International Conference Data
Mining, pp. 242249.
Kolda, T., & Bader, B. (2006). TopHITS model higher-order web link analysis.
Proceedings SIAM Data Mining Conference Workshop Link Analysis,
Counterterrorism Security, pp. 2629.
Koller, D., & Sahami, M. (1996). Toward optimal feature selection. Proceedings
13th International Conference Machine Learning, pp. 284292.
Kondor, R., & Lafferty, J. (2002). Diffusion kernels graphs discrete structures.
Proceedings 19th International Conference Machine Learning, pp. 315
322.
Kong, X., & Yu, P. (2010). Semi-supervised feature selection graph classification. Proceeding 16th ACM SIGKDD International Conference Knowledge Discovery
Data Mining, pp. 793802.
Koren, Y., North, S., & Volinsky, C. (2007). Measuring extracting proximity graphs
networks. Transactions Knowledge Discovery Data (TKDD), 1 (3), 12:1
12:30.
Kosala, R., & Blockeel, H. (2000). Web Mining Research: Survey. ACM SIGKDD
Explorations Newsletter, 2 (1), 115.
Kossinets, G., Kleinberg, J., & Watts, D. (2008). structure information pathways
social communication network. Proceeding 14th ACM SIGKDD International
Conference Knowledge Discovery Data Mining, pp. 435443.
Kou, Z., & Cohen, W. (2007). Stacked graphical models efficient inference markov
random fields. Proceedings 7th SIAM International Conference Data
Mining, pp. 533538.
Krebs, V. (2002). Mapping networks terrorist cells. Connections, 24 (3), 4352.
432

fiTransforming Graph Data Statistical Relational Learning

Krogel, M., & Wrobel, S. (2001). Transformation-based learning using multirelational aggregation. Inductive logic programming, 2157, 142155.
Kubica, J., Moore, A., Schneider, J., & Yang, Y. (2002). Stochastic link group detection.
Proceedings 18th AAAI Conference Artificial Intelligence, pp. 798806.
Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilistic models segmenting labeling sequence data. Proceedings 18th
International Conference Machine Learning, pp. 282289.
Landwehr, N., Kersting, K., & De Raedt, L. (2005). nFOIL: Integrating nave bayes
FOIL. Proceedings 20th AAAI Conference Artificial Intelligence, pp.
275282.
Landwehr, N., Passerini, A., De Raedt, L., & Frasconi, P. (2010). Fast learning relational
kernels. Machine learning, 78 (3), 305342.
Langville, A., & Meyer, C. (2005). Survey Eigenvector Methods Web Information
Retrieval. SIAM Review, 47 (1), 135161.
Lassez, J.-L., Rossi, R., & Jeev, K. (2008). Ranking links web: Search surf
engines. IEA/AIE, pp. 199208.
Lee, L. (1999). Measures distributional similarity. Proceedings 37th annual meeting Association Computational Linguistics Computational Linguistics,
pp. 2532.
Leicht, E., Holme, P., & Newman, M. (2006). Vertex similarity networks. Physical Review
E, 73 (2), 026120.
Leiva, H. A., Gadia, S., & Dobbs, D. (2002). Mrdtl: multi-relational decision tree learning
algorithm. Proceedings 13th International Conference Inductive Logic
Programming, pp. 3856.
Lempel, R., & Moran, S. (2000). stochastic approach link-structure analysis
(SALSA) TKC effect. Computer Networks, 33 (1-6), 387401.
Leskovec, J., Chakrabarti, D., Kleinberg, J., Faloutsos, C., & Ghahramani, Z. (2010). Kronecker graphs: approach modeling networks. Journal Machine Learning
Research, 11, 9851042.
Leskovec, J., Huttenlocher, D., & Kleinberg, J. (2010). Predicting positive negative
links online social networks. Proceedings 19th International World Wide
Web Conference, pp. 641650.
Letovsky, S., & Kasif, S. (2003). Predicting protein function protein/protein interaction
data: probabilistic approach. Bioinformatics, 19 (Suppl 1), i197.
Li, W., & McCallum, A. (2006). Pachinko allocation: DAG-structured mixture models
topic correlations. Proceedings 23rd International Conference Machine
Learning, pp. 577584.
Liben-Nowell, D., & Kleinberg, J. (2007). link-prediction problem social networks.
Journal American Society Information Science Technology, 58 (7), 1019
1031.
433

fiRossi, McDowell, Aha, & Neville

Lichtenwalter, R., Lussier, J., & Chawla, N. (2010). New Perspectives Methods Link
Prediction. Proceeding 16th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 243252.
Lim, T., Loh, W., & Shih, Y. (2000). comparison prediction accuracy, complexity,
training time thirty-three old new classification algorithms. Machine Learning,
40 (3), 203228.
Lin, D. (1998). information-theoretic definition similarity. Proceedings 15th
International Conference Machine Learning, pp. 296304.
Lin, F., & Cohen, W. W. (2010). Semi-supervised classification network data using
labels. Proceedings International Conference Advances Social
Network Analysis Mining.
Lippi, M., Jaeger, M., Frasconi, P., & Passerini, A. (2009). Relational information gain.
Machine Learning, 83 (2), 121.
Liu, K., & Terzi, E. (2008). Towards identity anonymization graphs. Proceedings
ACM SIGMOD International Conference Management Data, pp. 93106.
Liu, W., & Lu, L. (2010). Link prediction based local random walk. Europhysics Letters,
89, 58007.
Liu, Y., Niculescu-Mizil, A., & Gryc, W. (2009). Topic-link LDA: joint models topic
author community. Proceedings 26th International Conference Machine
Learning, pp. 665672.
Long, B., Zhang, Z., & Yu, P. (2007). probabilistic framework relational clustering.
Proceedings 13th ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 470479.
Long, B., Zhang, Z., Wu, X., & Yu, P. S. (2006). Spectral clustering multi-type relational
data. Proceedings 23rd International Conference Machine Learning, pp.
585592.
Lu, C., Hu, X., Chen, X., & ran Park, J. (2010). Topic-Perspective Model Social
Tagging Systems. Proceeding 16th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 683692.
Lu, Q., & Getoor, L. (2003). Link-based classification. Proceedings 20th International Conference Machine Learning, pp. 496503.
Macskassy, S., & Provost, F. (2003). simple relational classifier. Proceedings
SIGKDD 2nd Workshop Multi-Relational Data Mining, pp. 6476.
Macskassy, S., & Provost, F. (2007). Classification networked data: toolkit
univariate case study. Journal Machine Learning Research, 8, 935983.
Macskassy, S. A. (2007). Improving learning networked data combining explicit
mined links. Proceedings 22nd AAAI Conference Artificial Intelligence,
pp. 590595.
Maes, F., Peters, S., Denoyer, L., & Gallinari, P. (2009). Simulated iterative classification new learning procedure graph labeling. Machine Learning Knowledge
Discovery Databases, 5782, 4762.
434

fiTransforming Graph Data Statistical Relational Learning

Mallows, C. (1973). comments Cp . Technometrics, 42 (1), 8794.
Maslov, S., & Sneppen, K. (2002). Specificity stability topology protein networks.
Science, 296 (5569), 910913.
May, R., & Lloyd, A. (2001). Infection dynamics scale-free networks. Physical Review
E, 64 (6), 66112.
McCallum, A., Wang, X., & Corrada-Emmanuel, A. (2007). Topic role discovery
social networks experiments enron academic email. Journal Artificial
Intelligence Research, pp. 249272.
McCallum, A., Wang, X., & Mohanty, N. (2007). Joint group topic discovery
relations text. Statistical Network Analysis: Models, Issues New Directions,
Lecture Notes Computer Science 4503, pp. 2844.
McDowell, L., Gupta, K., & Aha, D. (2009). Cautious collective classification. Journal
Machine Learning Research, 10, 27772836.
McDowell, L., Gupta, K., & Aha, D. (2007). Cautious inference collective classification.
Proceedings 22nd AAAI Conference Artificial Intelligence.
McDowell, L., Gupta, K., & Aha, D. (2010). Meta-Prediction Collective Classification.
Proceedings 23rd International FLAIRS Conference.
McGovern, A., Friedland, L., Hay, M., Gallagher, B., Fast, A., Neville, J., & Jensen, D.
(2003). Exploiting relational structure understand publication patterns highenergy physics. SIGKDD Explorations, 5 (2), 165172.
McGovern, A., Collier, N., Matthew Gagne, I., Brown, D., & Rodger, A. (2008). Spatiotemporal Relational Probability Trees: Introduction. Eighth IEEE International
Conference Data Mining, ICDM., pp. 935940.
Menon, A., & Elkan, C. (2011). Link prediction via matrix factorization. Proceedings
European Conference Machine Learning Principles Practice Knowledge Discovery Databases, pp. 437452.
Menon, A., & Elkan, C. (2010). Predicting labels dyadic data. Data Mining
Knowledge Discovery, 21 (2), 327343.
Michie, D., Spiegelhalter, D., Taylor, C., & Campbell, J. (1994). Machine Learning, Neural
Statistical Classification. Ellis Horwood Limited.
Mihalkova, L., & Mooney, R. (2007). Bottom-up learning Markov logic network structure.
Proceedings 24th International Conference Machine Learning, pp. 625
632.
Miller, K., Griffiths, T., & Jordan, M. (2009). Nonparametric latent feature models link
prediction. Advances Neural Information Processing Systems (NIPS), 10, 1276
1284.
Minsky, M. (1974). framework representing knowledge. Tech. rep., Massachusetts
Institute Technology, Cambridge, MA, USA.
Mislove, A., Marcon, M., Gummadi, K., Druschel, P., & Bhattacharjee, B. (2007). Measurement analysis online social networks. Proceedings 7th ACM SIGCOMM
Conference Internet measurement, pp. 2942.
435

fiRossi, McDowell, Aha, & Neville

Moore, C., & Newman, M. (2000). Epidemics percolation small-world networks.
Physical Review E, 61 (5), 56785682.
Moreno, S., & Neville, J. (2009). investigation distributional characteristics
generative graph models. Proceedings 1st Workshop Information
Networks.
Nallapati, R., Ahmed, A., Xing, E., & Cohen, W. (2008). Joint latent topic models text
citations. Proceeding 14th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 542550.
Namata, G., Kok, S., & Getoor, L. (2011). Collective graph identification. Proceedings
17th ACM SIGKDD International Conference Knowledge Discovery Data
Mining, pp. 8795. ACM.
Narayanan, A., & Shmatikov, V. (2009). De-anonymizing social networks. Proceedings
30th IEEE Symposium Security Privacy, pp. 173187.
Natarajan, S., Khot, T., Kersting, K., Gutmann, B., & Shavlik, J. (2012). Gradient-based
boosting statistical relational learning: relational dependency network case.
Machine Learning, 86, 2556.
Neville, J., Adler, M., & Jensen, D. (2004). Spectral clustering links attributes.
Tech. rep. 04-42, Dept Computer Science, University Massachusetts Amherst.
Neville, J., & Jensen, D. (2000). Iterative classification relational data. Proceedings
Workshop SRL, 17th AAAI Conference Artificial Intelligence, pp. 4249.
Neville, J., & Jensen, D. (2005). Leveraging relational autocorrelation latent group
models. Proceedings 5th IEEE International Conference Data Mining,
pp. 322329.
Neville, J., & Jensen, D. (2007). Relational dependency networks. Journal Machine
Learning Research, 8, 653692.
Neville, J., Jensen, D., Friedland, L., & Hay, M. (2003). Learning relational probability
trees. Proceedings 9th ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 625630.
Neville, J., Jensen, D., & Gallagher, B. (2003). Simple estimators relational Bayesian
classifers. Proceedings 3rd IEEE International Conference Data Mining,
pp. 609612.
Neville, J., Simsek, O., Jensen, D., Komoroske, J., Palmer, K., & Goldberg, H. (2005). Using
relational knowledge discovery prevent securities fraud. Proceedings 11th
ACM SIGKDD International Conference Knowledge Discovery Data Mining,
pp. 449458.
Newman, M. (2010). Networks: Introduction. Oxford Univ Press.
Newman, M. E. J. (2003). structure function complex networks. SIAM Review,
45, 167256.
Newman, M. (2001a). Clustering preferential attachment growing networks. Physical
Review E, 64 (2), 025102.
436

fiTransforming Graph Data Statistical Relational Learning

Newman, M. (2001b). structure scientific collaboration networks. Proceedings
National Academy Sciences, 98 (2), 404409.
Newman, M., & Girvan, M. (2004). Finding evaluating community structure networks. Physical review E, 69 (2), 26113.
Ng, A., Jordan, M., & Weiss, Y. (2001). spectral clustering: Analysis algorithm.
Advances Neural Information Processing Systems, pp. 849856.
Nie, L., Davison, B., & Qi, X. (2006). Topical link analysis web search. Proceedings
29th International ACM SIGIR Conference Research Development
Information Retrieval, p. 98.
Nowicki, K., & Snijders, T. (2001). Estimation prediction stochastic blockstructures.
Journal American Statistical Association, 96, 10771087.
Oktay, H., Taylor, B., & Jensen, D. (2010). Causal Discovery Social Media Using QuasiExperimental Designs. Proceedings ACM SIGKDD 1st Workshop Social
Media Analytics (SOMA-KDD).
Onnela, J.-P., Saramaki, J., Hyvonen, J., Szabo, G., Lazer, D., Kaski, K., Kertesz, J., &
Barabasi, A.-L. (2007). Structure tie strengths mobile communication networks.
Proceedings National Academy Sciences, 104, 73327336.
Page, L., Brin, S., Motwani, R., & Winograd, T. (1999). pagerank citation ranking:
Bringing order web. Tech. rep., Technical Report, Stanford Digital Library
Technologies Project.
Pang, B., & Lee, L. (2008). Opinion mining sentiment analysis. Foundations Trends
Information Retrieval, 2 (1-2), 1135.
Pastor-Satorras, R., & Vespignani, A. (2001). Epidemic spreading scale-free networks.
Physical Review Letters, 86 (14), 32003203.
Pasula, H., Marthi, B., Milch, B., Russell, S., & Shpitser, I. (2003). Identity uncertainty
citation matching. NIPS. MIT Press.
Perlich, C., & Provost, F. (2003). Aggregation-based feature invention relational concept classes. Proceedings 9th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 167176.
Perlich, C., & Provost, F. (2006). Acora: Distribution-based aggregation relational
learning identifier attributes. Machine Learning, 62, 65105.
Poole, D. (2003). First-order probabilistic inference. International Joint Conference
Artificial Intelligence, pp. 985991.
Popescul, A., Popescul, R., & Ungar, L. H. (2003a). Statistical relational learning
link prediction. Proceedings Workshop Learning Statistical Models
Relational Data IJCAI.
Popescul, A., Popescul, R., & Ungar, L. H. (2003b). Structural logistic regression link
analysis. Proceedings Second International Workshop Multi-Relational
Data Mining, pp. 92106.
437

fiRossi, McDowell, Aha, & Neville

Popescul, A., & Ungar, L. H. (2004). Cluster-based concept invention statistical relational learning. Proceedings 10th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 665670.
Price, D. (1976). general theory bibliometric cumulative advantage processes.
Journal American Society Information Science, 27 (5), 292306.
Pudil, P., Novovicova, J., & Kittler, J. (1994). Floating search methods feature selection.
Pattern recognition letters, 15 (11), 11191125.
Radicchi, F., Castellano, C., Cecconi, F., Loreto, V., & Parisi, D. (2004). Defining
identifying communities networks. Proceedings National Academy Sciences,
101 (9), 26582663.
Rattigan, M. J., & Jensen, D. (2005). case anomalous link discovery. SIGKDD
Explorations Newsletter, 7 (2), 4147.
Ravasz, E., Somera, A., Mongru, D., Oltvai, Z., & Barabasi, A. (2002). Hierarchical organization modularity metabolic networks. Science, 297 (5586), 15511555.
Resnick, P., & Varian, H. (1997). Recommender systems. Communications ACM,
40 (3), 5658.
Richardson, M., & Domingos, P. (2002). intelligent surfer: Probabilistic combination
link content information pagerank. Advances Neural Information
Processing Systems, pp. 14411448.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine Learning, 62 (1),
107136.
Riedel, S., & Meza-Ruiz, I. (2008). Collective semantic role labelling markov logic.
Proceedings Twelfth Conference Computational Natural Language Learning,
pp. 193197.
Robins, G., Pattison, P., Kalish, Y., & Lusher, D. (2007). introduction exponential
random graph (p*) models social networks. Social Networks, 29, 173191.
Robins, G., Snijders, T., Wang, P., & Handcock, M. (2006). Recent developments exponential random graph (p*) models social networks. Social Networks, 29, 192215.
Rossi, R., Gallagher, B., Neville, J., & Henderson, K. (2012). Dynamic behavioral mixedmembership model large evolving networks. Arxiv preprint arXiv:1205.2056,
pp. 117.
Rossi, R., & Neville, J. (2010). Modeling evolution discussion topics communication improve relational classification. Proceedings ACM SIGKDD 1st
Workshop Social Media Analytics (SOMA-KDD), pp. 110.
Rossi, R., Gallagher, B., Neville, J., & Henderson, K. (2012). Role-Dynamics: Fast Mining
Large Dynamic Networks. LSNA-WWW, pp. 19.
Rossi, R., & Neville, J. (2012). Time-evolving relational classification ensemble methods.
Proceedings 16th Pacific-Asia Conference Knowledge Discovery Data
Mining, pp. 112.
438

fiTransforming Graph Data Statistical Relational Learning

Roth, M., Ben-David, A., Deutscher, D., Flysher, G., Horn, I., Leichtberg, A., Leiser, N.,
Merom, R., & Mattias, Y. (2010). Suggesting Friends Using Implicit Social Graph.
Proceeding 16th ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 233242.
Russell, S. J., & Norvig, P. (2009). Artificial Intelligence: Modern Approach (3rd International Edition edition). Prentice Hall.
Sabidussi, G. (1966). centrality index graph. Psychometrika, 31 (4), 581603.
Salton, G., & McGill, M. (1983). Introduction modern information retrieval, Vol. 1.
McGraw-Hill New York.
Sarkar, P., & Moore, A. (2005). Dynamic Social Network Analysis using Latent Space
Models. SIGKDD Explorations Newsletter, 7 (2), 3140.
Sarukkai, R. R. (2000). Link prediction path analysis using markov chains. Proceedings 9th International World Wide Web Conference Computer Networks:
International Journal Computer Telecommunications Networking, pp. 377386.
Sarwar, B., Karypis, G., Konstan, J., & Riedl, J. (2000). Application dimensionality
reduction recommender systema case study. ACM WebKDD 2000 Web Mining
E-Commerce Workshop.
Schulte, O. (2011). tractable pseudo-likelihood function bayes nets applied relational
data. SDM, pp. 462473.
Schwarz, G. (1978). Estimating dimension model. annals statistics, 6 (2),
461464.
Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., & Eliassi-Rad, T. (2008). Collective classification network data. AI Magazine, 29 (3), 93.
Shao, J. (1996). Bootstrap model selection. Journal American Statistical Association,
91 (434), 655665.
Shapiro, E. (1982). Algorithmic Program Debugging. ACM Distinguished Dissertation..
Sharan, U., & Neville, J. (2008). Temporal-relational classifiers prediction evolving
domains. Proceedings 8th IEEE International Conference Data Mining,
pp. 540549.
Shi, J., & Malik, J. (2000). Normalized cuts image segmentation. IEEE Transactions
Pattern Analysis Machine Intelligence, 22 (8), 888905.
Shi, X., Li, Y., & Yu, P. (2011). Collective prediction latent graphs. Proceedings
20th ACM Conference Information Knowledge Management, pp. 1127
1136.
Singla, P., & Domingos, P. (2006). Entity resolution markov logic. Proceedings
6th IEEE International Conference Data Mining, pp. 572582.
Srinivasan, A. (1999). aleph manual. Computing Laboratory, Oxford University, 1.
Strehl, A., & Ghosh, J. (2003). Cluster ensemblesa knowledge reuse framework combining multiple partitions. Journal Machine Learning Research, 3, 583617.
439

fiRossi, McDowell, Aha, & Neville

Tang, J., Musolesi, M., Mascolo, C., & Latora, V. (2009). Temporal distance metrics
social network analysis. Proceedings 2nd ACM workshop Online social
networks, pp. 3136.
Tang, J., Musolesi, M., Mascolo, C., Latora, V., & Nicosia, V. (2010). Analysing information
flows key mediators temporal centrality metrics. Proceedings
3rd Workshop Social Network Systems, pp. 16.
Tang, L., & Liu, H. (2009). Relational learning via latent social dimensions. Proceedings
15th ACM SIGKDD International Conference Knowledge Discovery
Data Mining, pp. 817826.
Tang, L., & Liu, H. (2011). Leveraging social media networks classification. Journal
Data Mining Knowledge Discovery, 23, 447478.
Taskar, B., Abbeel, P., & Koller, D. (2002). Discriminative probabilistic models relational
data. Eighteenth Conference Uncertainty Artificial Intelligence, pp. 485492.
Taskar, B., Segal, E., & Koller, D. (2001). Probabilistic classification clustering
relational data. Proceedings 17th International Joint Conference Artificial
Intelligence, pp. 870878.
Taskar, B., Wong, M., Abbeel, P., & Koller, D. (2003). Link prediction relational data.
Advances Neural Information Processing Systems.
Topchy, A., Law, M., Jain, A., & Fred, A. (2004). Analysis consensus partition cluster
ensemble. Proceedings 4th IEEE International Conference Data Mining,
pp. 225232.
Vert, J., & Yamanishi, Y. (2005). Supervised graph inference. Advances Neural Information Processing Systems, 17, 14331440.
Vishwanathan, S., Schraudolph, N., Kondor, R., & Borgwardt, K. (2010). Graph kernels.
Journal Machine Learning Research, 11, 12011242.
von Luxburg, U. (2007). tutorial spectral clustering. Statistics Computing, 17 (4),
395416.
Wagner, A., & Fell, D. (2001). small world inside large metabolic networks. Proceedings
Royal Society London. Series B: Biological Sciences, 268 (1478), 18031810.
Wang, C., Blei, D., & Heckerman, D. (2008). Continuous time dynamic topic models.
Proceedings Uncertainty Artificial Intelligence.
Wang, X., & McCallum, A. (2006). Topics time: non-Markov continuous-time model
topical trends. Proceedings 12th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 424433.
Wang, Z., & Chan, L. (2010). efficient causal discovery algorithm linear models.
Proceeding 16th ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 11091118.
Wasserman, S., & Faust, K. (1994). Social network analysis: Methods applications.
Cambridge University Press.
440

fiTransforming Graph Data Statistical Relational Learning

Watts, D., & Strogatz, S. (1998). Collective dynamics small-worldnetworks. Nature,
393 (6684), 440442.
White, S., & Smyth, P. (2003). Algorithms estimating relative importance networks.
Proceedings ninth ACM SIGKDD International Conference Knowledge
Discovery Data mining, pp. 266275.
Wu, B., & Davison, B. (2005). Identifying link farm spam pages. Special interest tracks
posters 14th International Conference World Wide Web, pp. 820829.
Xiang, R., Neville, J., & Rogati, M. (2010). Modeling relationship strength online social
networks. Proceedings 19th International World Wide Web Conference, pp.
981990.
Yang, Y., & Pedersen, J. (1997). comparative study feature selection text categorization. Proceedings 14th International Conference Machine Learning,
pp. 412420.
Yin, X., Han, J., Yang, J., & Yu, P. (2006). Crossmine: Efficient classification across multiple
database relations. Transactions Knowledge Data Engineering, 18 (6), 770783.
Zheleva, E., Getoor, L., Golbeck, J., & Kuter, U. (2010). Using friendship ties family
circles link prediction. Advances Social Network Mining Analysis, pp.
97113.
Zheleva, E., & Getoor, L. (2007). Preserving privacy sensitive relationships graph
data. PinKDD, pp. 153171.
Zhou, B., Pei, J., & Luk, W. (2008). brief survey anonymization techniques privacy
preserving publishing social network data. SIGKDD Explorations, 10 (2), 1222.
Zhou, H. (2003). Distance, dissimilarity index, network community structure. Physical
review e, 67 (6), 61901.
Zhou, T., Lu, L., & Zhang, Y. (2009). Predicting missing links via local information.
European Physical Journal B-Condensed Matter Complex Systems, 71 (4), 623
630.
Zhu, S., Yu, K., Chi, Y., & Gong, Y. (2007). Combining content link classification
using matrix factorization. Proceedings 30th Annual International ACM
SIGIR Conference Research Development Information Retrieval, pp. 487
494. ACM.
Zhu, X. (2006). Semi-supervised learning literature survey. Computer Science Tech Reports,
1530, 160.

441

fiJournal Artificial Intelligence Research 45 (2012) 257-286

Submitted 6/12; published 10/12

Generating Approximate Solutions Traveling
Tournament Problem using Linear Distance Relaxation
Richard Hoshino
Ken-ichi Kawarabayashi

richard.hoshino@gmail.com
k keniti@nii.ac.jp

National Institute Informatics
JST ERATO Kawarabayashi Project
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan

Abstract
domestic professional sports leagues, home stadiums located cities
connected common train line running one direction. instances,
incorporate geographical information determine optimal nearly-optimal solutions
n-team Traveling Tournament Problem (TTP), NP-hard sports scheduling problem whose solution double round-robin tournament schedule minimizes sum
total distances traveled n teams.
introduce Linear Distance Traveling Tournament Problem (LD-TTP), solve
n = 4 n = 6, generating complete set possible solutions elementary
combinatorial techniques. larger n, propose novel expander construction
generates approximate solution LD-TTP. n 4 (mod 6), show
expander construction produces feasible double round-robin tournament schedule whose
total distance guaranteed worse 43 times optimal solution, regardless
n teams located. 34 -approximation LD-TTP stronger
currently best-known ratio 35 + general TTP.
conclude paper applying linear distance relaxation general (nonlinear) n-team TTP instances, develop fast approximate solutions simply
assuming n teams lie straight line solving modified problem.
show technique surprisingly generates distance-optimal tournament
benchmark sets 6 teams, well close-to-optimal schedules larger n, even
teams located around circle positioned three-dimensional space.

1. Introduction
paper, introduce simple yet powerful technique develop approximate solutions
Traveling Tournament Problem
(TTP), assuming n teams located
n
straight line, thereby reducing 2 pairwise distance parameters n 1 variables,
solving relaxed problem.
Traveling Tournament Problem (TTP) inspired real-life problem
optimizing regular-season schedule Major League Baseball. goal TTP
determine optimal double round-robin tournament schedule n-team sports
league minimizes sum total distances traveled n teams. Since problem
first proposed (Easton, Nemhauser, & Trick, 2001), TTP attracted significant
amount research (Kendall, Knust, Ribeiro, & Urrutia, 2010), numerous heuristics
developed solving hard TTP instances, local search techniques well integer
constraint programming.
c
2012
AI Access Foundation. rights reserved.

fiHoshino & Kawarabayashi

many ways, TTP variant well-known Traveling Salesman Problem
(TSP), asking distance-optimal schedule linking venues close one another.
computational complexity TSP NP-hard; recently, shown solving
TTP strongly NP-hard (Thielen & Westphal, 2010).
Linear Distance Traveling Tournament Problem (LD-TTP), assume n
teams located straight line. straight line relaxation natural heuristic
n teams located cities connected common train line running one
direction, modelling actual context domestic sports leagues countries Chile,
Sweden, Italy, Japan. example, Figure 1 illustrates locations six home
stadiums Nippon Pro Baseballs Central League, situated close proximity major
stations Japans primary bullet-train line.

Figure 1: six Central League teams Japanese Pro Baseball.

Section 2, formally define TTP. Section 3, solve LD-TTP n = 4
list 18 non-isomorphic tournament schedules achieving optimal distance.
Section 4, solve LD-TTP n = 6 show 295 non-isomorphic
tournament schedules attain one seven possible values optimal
distance. Section 5, provide expander construction produce feasible double
round-robin tournament schedule tournament n = 6m 2 teams, prove
34 -approximation distance-optimal schedule, 1. Section 6,
apply theories known (non-linear) 6-team benchmark sets (Trick, 2012),
show cases, optimal solution appears list 295. also apply
expander construction various benchmark sets 10 16 teams, showing
optimality gap actually far lower theoretical maximum 33.3%. Section 7,
in-depth analysis optimality gap, conclude paper Section 8
open problems directions future research.

2. Traveling Tournament Problem
Let {t1 , t2 , . . . , tn } n teams sports league, n even. Let n n
distance matrix, entry Di,j distance home stadiums teams ti
tj . definition, Di,j = Dj,i 1 i, j n, diagonal entries Di,i zero.
assume distances form metric, i.e., Di,j Di,k + Dk,j i, j, k.
258

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

TTP requires tournament lasting 2(n 1) days, every team exactly one
game scheduled day byes days (this explains n must even.)
objective minimize total distance traveled n teams, subject following
conditions:
(a) each-venue: pair teams plays twice, others home venue.
(b) at-most-three: team may home stand road trip lasting three
games.
(c) no-repeat: team cannot play opponent two consecutive games.
calculating total distance, assume every team begins tournament
home returns home playing last away game. Furthermore, whenever team
road trip consisting multiple away games, team doesnt return home
city rather proceeds directly next away venue.
illustrate specific example, Table 1 lists distance-optimal schedule (Easton
et al., 2001) bechmark set known NL6 (six teams Major League Baseballs
National League). schedule, subsequent schedules presented paper,
home games marked bold.
Team
Florida (FLO)
Atlanta (ATL)
Pittsburgh (PIT)
Philadelphia (PHI)
New York (NYK)
Montreal (MON)

1
2
3
4
5
ATL PHI NYK PIT NYK
FLO NYK PIT
PHI MON
NYK MON ATL FLO PHI
MON FLO MON ATL PIT
PIT ATL FLO MON FLO
PHI PIT
PHI NYK ATL

6
MON
PIT
ATL
NYK
PHI
FLO

7
8
PIT PHI
PHI MON
FLO NYK
ATL FLO
MON PIT
NYK ATL

9
MON
NYK
PHI
PIT
ATL
FLO

10
ATL
FLO
MON
NYK
PHI
PIT

Table 1: optimal TTP solution NL6.
example, total distance traveled Florida DFLO,ATL +DATL,PHI +DPHI,FLO +
DFLO,NYK + DNYK,MON + DMON,PIT + DPIT,FLO . Based NL6 distance matrix (Trick,
2012), tournament schedule Table 1 requires 23916 miles total team travel,
shown minimum distance possible.

3. 4-Team LD-TTP

Figure 2: general instance LD-TTP n = 4.
Linear Distance TTP, assume n home stadiums lie straight line,
t1 one end tn other. Thus, Di,j = Di,k + Dk,j triplets (i, j, k)
259

fiHoshino & Kawarabayashi

1 < k < j n. Since Triangle
Inequality replaced Triangle Equality,

longer need consider n2 entries distance matrix D; tournaments total
travel distance function n 1 variables, namely set {Di,i+1 : 1 n 1}.
notational convenience, denote di := Di,i+1 1 n 1.
Team
t1
t2
t3
t4

1
t4
t3
t2
t1

2
t3
t4
t1
t2

3
t2
t1
t4
t3

4
t4
t3
t2
t1

5
t3
t4
t1
t2

6
t2
t1
t4
t3

Table 2: optimal LD-TTP solution n = 4.

Table 2 gives feasible solution 4-team LD-TTP. claim solution
optimal, possible 3-tuples (d1 , d2 , d3 ). see so, define ILBti
independent lower bound team ti , minimum possible distance traveled
ti order complete games, independent
teams schedules.
P
trivial lower bound total travel distance LB ni=1 ILBti .
Recall calculating ti travel distance, assume ti begins tournament home returns home playing last away game. Since ti must play road
game three teams, ILBti = 2(d1 + d2 + d3 ) 1 4.
implies LB 8(d1 + d2 + d3 ). Since Table 2 tournament schedule whose total
distance trivial lower bound, completes proof.
remark Table 2 unique solution - example, generate another
optimal schedule simply reading Table 2 right left. Assuming first match
t1 t2 occurs home city t2 , straightforward computer search finds 18
different schedules total distance 8(d1 + d2 + d3 ), provided Table 3 below.
(For readability, replaced occurrence ti single index i.) Thus,
symmetry, 36 optimal schedules 4-team LD-TTP. interested reader,
Appendix provides actual Maplesoft code generated optimal schedules.
234234
143143
412412
321321

234243
143134
412421
321312

234342
143431
412124
321213

243234
134143
421412
312321

243243
134134
421421
312312

243432
134341
421214
312123

342342
431431
124124
213213

342432
431341
124214
213123

432342
341431
214124
123213

432432
341341
214214
123123

342342
431431
124124
213213

342432
431341
124214
213123

342342
431431
124124
213213

342432
431341
124214
213123

432342
341431
214124
123213

432432
341341
214214
123123

432342
341431
214124
123213

432432
341341
214214
123123

Table 3: eighteen non-isomorphic optimal LD-TTP solutions n = 4.
completes analysis 4-team Linear Distance TTP.
260

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

4. 6-Team LD-TTP
Unlike previous section, analysis 6-team LD-TTP requires work.

Figure 3: general instance LD-TTP n = 6.
6-team instance LD-TTP represented 5-tuple (d1 , d2 , d3 , d4 , d5 ).
define = 14d1 + 16d2 + 20d3 + 16d4 + 14d5 . claim following:
Theorem 1. Let 6-team instance LD-TTP. optimal solution
schedule total distance + 2 min{d2 + d4 , d1 + d4 , d3 + d4 , 3d4 , d2 + d5 , d2 + d3 , 3d2 }.
prove Theorem 1 elementary combinatorial arguments, thus demonstrating utility linear distance relaxation presenting new techniques attack
general TTP ways differ integer/constraint programming. proof
follow several lemmas, prove one one.
Lemma 1. feasible schedule must total distance least S.
Proof. 1 k 5, define ck total number times team crosses
bridge length dk , connecting home stadiums teams
Pt5k tk+1 . Let Z
total travel distance schedule. Since linear, Z = k=1 ck dk . Since team
crosses every bridge even number times, ck always even.
Let Lk home venues {t1 , t2 , . . . , tk } Rk home venues {tk+1 , . . . , t6 }.
each-venue condition, every team Lk plays road game every team Rk .
at-most-three condition, every team Lk must make least 2d 6k
3 e trips across
bridge, half trips direction. Similarly, every team Rk must make
k
least 2d k3 e trips across bridge, implying ck 2kd 6k
3 e + 2(6 k)d 3 e.
Thus, c1 14, c2 16, cP
4 16, c5 14. show c3 20,
complete proof Z = ck dk 14d1 + 16d2 + 20d3 + 16d4 + 14d5 = S.
Since n = 6 teams, 2(n 1) = 10 days games. 1 9,
let Xi,i+1 total number times d3 -length bridge crossed teams move
games ith day games (i + 1)th day. Let Xstart,1 X10,end
respectively number times teams cross bridge playPtheir first game,
return home played last game. c3 = Xstart,1 + 9i=1 Xi,i+1 +X10,end .
1 9, let f (i) denote number games played L3 day i. Thus,
day i, exactly 2f (i) teams left bridge 6 2f (i) teams
right. f (i) {0, 1, 2, 3} i. Since |L3 | |R3 | odd, Xstart,1 1
X10,end 1.
f (i) < f (i + 1), Xi,i+1 2, least two teams played R3 day
must cross play next game L3 . Similarly, f (i) > f (i + 1), Xi,i+1 2.
f (i) = f (i + 1) = 1, day i, two teams p q play L3 four
teams play R3 . Xi,i+1 = 0 team crosses bridge day i, forcing p
261

fiHoshino & Kawarabayashi

q play day + 1, thus violating no-repeat condition. Thus,
least one p q must cross bridge, exchanging positions least one team
must cross play L3 . Thus, Xi,i+1 2. Similarly, f (i) = f (i + 1) = 2,
Xi,i+1 2.
f (i) = f (i + 1) = 0, teams play R3 days + 1. Xstart,1 = 3
= 1 X10,end = 3 = 9. 2 8, {t1 , t2 , t3 } must play home
game either day 1 day + 2, order satisfy at-most-three condition. Thus,
one two days, least two teams {t1 , t2 , t3 } play home, implying least
four teams L3 . Therefore, must Xi1,i 4 Xi+1,i+2 4.
derive results f (i) = f (i + 1) = 3. Xstart,1 = 3 = 1,
X10,end = 3 = 9, either Xi1,i 4 Xi+1,i+2 4 2 8.
double round-robin schedule, sequence
{f (1), . . . , f (10)} pair
P9
consecutive 0s consecutive 3s, c3 = Xstart,1 + i=1 Xi,i+1 +X10,end 1+92+1 = 20.
case, still c3 P
20 results previous two
paragraphs. therefore proven Z =
ck dk S.
Lemma 2. Consider feasible schedule total distance Z =
teams t1 t2 must play Days 1 10.

P

ck dk . c2 = 16,


Proof. Lemma 1, 1 9 define Xi,i+1
total number
times d2 -length bridge crossed teams move games ith



day games (i + 1)th day. Similarly define Xstart,1
X10,end
P
9



c2 = Xstart,1 + i=1 Xi,i+1 + X10,end .
P

prove 9i=1 Xi,i+1
16. this, 1 10, let g(i) denote
number games played L2 (i.e., home stadiums t1 t2 ) day i.
day i, exactly 2g(i) teams left d2 -length bridge 6 2g(i) teams
right. Clearly, 0 g(i) 2 1 10.

|g(i + 1) g(i)| = 1, Xi,i+1
2, least two teams played day
one side bridge must cross play next game side.

|g(i + 1) g(i)| = 2, Xi,i+1
= 4.
g(i) = g(i + 1) = 1, day i, two teams p q play L2 four

teams play R2 . Xi,i+1
= 0 team crosses bridge day i, forcing p
q play day + 1, thus violating no-repeat condition. Thus,
least one p q must cross bridge, exchanging positions least one team

must cross play L2 . Thus, Xi,i+1
2. Similarly, g(i) = g(i + 1) = 2, two
teams p q play R2 four teams play L2 , apply

argument show Xi,i+1
2. remaining case consider g(i) = g(i + 1) = 0,

case Xi,i+1 could equal 0.
Suppose days g(i) = 0, b days g(i) = 1, c days g(i) = 2.
+ b + c = 10. Since t1 t2 play five home games, implies b + 2c = 10.
this, see = c six possible triplets (a, b, c), namely
(0, 10, 0), (1, 8, 1), (2, 6, 2), (3, 4, 3), (4, 2, 4), (5, 0, 5).
= 0 = 1, P
exist index g(i) = g(i + 1) = 0, implying


Xi,i+1
2 1 9. Hence, 9i=1 Xi,i+1
9 2 = 18 cases. = 2,
P

one index g(i) = g(i + 1) = 0, implying 9i=1 Xi,i+1
16.

262

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

P

Suppose 9i=1 Xi,i+1
< 16. must 3 5, two indices satisfying g(i) = g(i + 1) = 0. example,P
one 10-tuple (g(1), g(2), . . . , g(9), g(10)) =

(1, 0, 0, 0, 1, 1, 1, 2, 2, 2), 9i=1 Xi,i+1
= 14. simple case analysis
(a, b, c) {(3, 4, 3), (4, 2, 4), (5, 0, 5)} shows bad 10-tuples violate atmost-three condition; example, 10-tuple above, either t1 t2 must play four
consecutive road games start tournament, contradiction.
P9

Therefore, proven
i=1 Xi,i+1 16 cases. implies


= X10,end
= 0. Hence, Days 1 10, t1 t2 stay L2
c2 = 16, Xstart,1
four teams stay R2 . Since t1 t2 teams L2 , clearly forces
two teams play other, begin end tournament.
Lemma 3. Let S1 set tournament schedules distance + 2(d2 + d4 ), S2
distance + 2(d1 + d4 ), S3 distance + 2(d3 + d4 ), S4 distance + 6d4 , S5
distance + 2(d2 + d5 ), S6 distance + 2(d2 + d3 ), S7 distance + 6d2 .
set {S1 , S2 , . . . , S7 } non-empty.
Proof. seven sets, suffices find one feasible schedule
desired total distance. {S1 , S2 , S3 , S4 }, least one set appeared
previously literature, solution 6-team benchmark set
context. (As see following section, label six teams NL6
benchmark set Table 1 element S4 .)
t1
t2
t3
t4
t5
t6

1
t2
t1
t4
t3
t6
t5

2
t3
t6
t1
t5
t4
t2

3
t4
t5
t6
t1
t2
t3

4
t6
t4
t5
t2
t3
t1

5
t3
t6
t1
t5
t4
t2

6
t5
t3
t2
t6
t1
t4

7
t6
t4
t5
t2
t3
t1

8
t4
t5
t6
t1
t2
t3

9
t5
t3
t2
t6
t1
t4

10
t2
t1
t4
t3
t6
t5

d1
4
2
2
2
2
2

d2
4
4
4
2
2
2

d3
4
2
4
4
2
2

d4
2
2
2
4
4
4

d5
2
2
2
2
2
4

Table 4: Optimal CIRC6 solution, distance + 2(d2 + d4 ) = 14d1 + 18d2 + 20d3 + 18d4 + 14d5 .

solution CIRC6 (Trick, 2012), Di,j = min{j i, 6 (j i)} 1 <
j 6, element S1 . Table 4 provides schedule. 1 k 5, list
number times dk bridge crossed six teams.
conclude proof noting |Si+3 | = |Si | 2 4, label
teams backward t6 t1 create feasible schedule distance dk replaced
d6k . Therefore, shown Si non-empty.
ready prove Theorem 1, optimal solution 6-team instance
schedule appears S1 S2 . . . S7 . note seven optimal
distances minimum, depending 5-tuple (d1 , d2 , d3 , d4 , d5 ).
P
Proof. Suppose optimal solution total distance Z =
ck dk . Lemma 1,
c1 , c5 14, c2 , c4 16, c3 20. Recall coefficient ck even.
263

fiHoshino & Kawarabayashi

Lemma 3, S1 non-empty, schedule cannot optimal Z > +2(d2 +d4 ).
Thus, c2 , c4 18, must (c1 , c2 , c3 , c4 , c5 ) = (14, 18, 20, 18, 14) Z =
+ 2(d2 + d4 ), forcing schedule set S1 .
Suppose c2 c4 , suffices check possibility c2 = 16. Lemma 2,
t1 t2 must play Days 1 10. three cases:
Case 1: c2 = 16, c1 = 14.
Case 2: c2 = 16, c1 16, c4 = 16.
Case 3: c2 = 16, c1 16, c4 18.
Case 1, every team must travel minimum number times across d1 -
d2 -bridges: team t1 take two road trips, team t2 take two road trips
play {t3 , t4 , t5 , t6 }, {t3 , t4 , t5 , t6 } must play road games t1 t2
consecutive days.
symmetry, may assume first match t1 t2 occurs home
city t2 (i.e., road game t1 ). Lemma 2, schedule team t1 must one
following four cases, permutation {p, q, r, s} {3, 4, 5, 6}.
Case
#A1
#A2
#A3
#A4

Team
t1
t1
t1
t1

1
t2
t2
t2
t2

2
t?
t?
t?
t?

3
tp
t?
tp
t?

4
tq
tp
tq
tp

5
t?
tq
tr
tq

6
t?
t?
t?
tr

7
t?
t?
t?
t?

8
tr
tr
t?
t?

9
ts
ts
ts
ts

10
t2
t2
t2
t2

four cases, t1 plays home game ts day 9. words, ts plays
road t1 day 9, forcing ts road game t2 take place either
day day after. latter possible, t2 already game scheduled
t1 day 10; thus, ts must play road t2 day 8.
Hence, t2 plays home game ts day 8 road game t1 day 10.
suppose t2 home game day 9. t2 opponent day must tr ,
must either Case #A1 #A2 above. (This way ensure tr
plays road games t1 t2 consecutive days.)
Team
t1
t2

1
t1
t1

2

3

4

5

6

7

8
tr
ts

9
ts
tr

10
t2
t1

six teams tournament, days 8 9, set four teams
assigned game. table, clear teams tp tq must
play day 8 day 9, violation no-repeat condition.
contradiction, therefore t2 must play road game day 9, team
{t3 , t4 , t5 , t6 }.
mentioned earlier, t2 take two road trips play four teams {t3 , t4 , t5 , t6 },
forces one following two scenarios:
264

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

Case
#B1
#B2

Team
t2
t2

1
t1
t1

2
tp
tp

3
tq
t?

4
t?
t?

5
t?
t?

6
t?
tq

7
tr
tr

8
ts
ts

9
t?
t?

10
t1
t1

4 2 = 8 pairs matching cases t1 cases t2 , check
whether exists feasible schedule team {t3 , t4 , t5 , t6 } plays road
games t1 t2 consecutive days. quick check shows possibility
pairing Case #A1 Case #B1, leading following schedule first
two teams:
Team
t1
t2

1
t2
t1

2
t?
tp

3
tp
tq

4
tq
t?

5
t?
t?

6
t?
t?

7
t?
tr

8
tr
ts

9
ts
t?

10
t2
t1

structural characterization reduces search space considerably,
(see Appendix B) show either (i) c4 22, (ii) c3 22 c4 18. Lemma
3, latter implies Z = + 2(d3 + d4 ) former implies Z = + 6d4 . Therefore,
optimal schedule must S3 S4 .
Case 2, demonstrate structural characterization exists c2 = c4 = 16.
this, use Lemma 2 (for c2 = 16) symmetric analogue (for c4 = 16) show
order violate at-most-three no-repeat conditions, t3 t4 must play
Days 1 10, well Day 2 9.
violates each-venue condition. Hence, may eliminate case.
Case 3, c1 16 c4 18, Z least + 2(d1 + d4 ). Lemma 3,
must Z = + 2(d1 + d4 ) optimal schedule must S2 .
shown c2 = 16, schedule appears S2 S3 S4 .
symmetry, c4 = 16, schedule appears S5 S6 S7 . Finally, c2 , c4 18,
schedule appears S1 . concludes proof.
Theorem 1, seven possible optimal distances. optimal distance, enumerate set tournament schedules distance, thus producing
complete set possible LD-TTP solutions, instances, case n = 6.
Theorem 2. Consider set feasible tournaments first game
t1 t2 occurs home city t2 . 295 schedules whose total distance
appears S1 S2 . . . S7 , grouped follows:
Total Distance
# Schedules

S1
223

S2
4

S3
8

S4
24

S5
4

S6
8

S7
24

derive Theorem 2 computer search. {S1 , S2 , S3 , S4 }, develop
structural characterization theorem, similar Case 1 above, shows feasible
schedule set must certain form. characterization reduces search
space, brute-force search (using Maplesoft) enumerates possible schedules.
took several days enumerate 223 schedules S1 , Maplesoft took less
100 seconds enumerate set schedules S3 S4 . noted earlier,
265

fiHoshino & Kawarabayashi

set schedules Si (for 2 4), immediately set schedules Si+3
symmetry. full details case, refer reader Appendix B.
Let us briefly explain |S1 | odd. schedule S, let (S) denote schedule
produced playing games backwards (i.e., ti hosts tj day iff ti hosts tj
day (11 d) (S).) let (S) denote schedule produced labelling six
teams reverse order (i.e., ti hosts tj day iff t7i hosts t7j day (S).)
schedule S, clearly 6= (S) 6= (S).
schedule S1 , exactly one (S ) ((S )) belongs S1 , since weve
stipulated first game t1 t2 occurs home city t2 . Since
mapping functions () involutions, schedules S1 grouped
pairs. However, 13 exceptional cases, schedule S1 pair, since
= ((S )). One example given Table 5.
t1
t2
t3
t4
t5
t6

1
t3
t4
t1
t2
t6
t5

2
t6
t5
t4
t3
t2
t1

3
t5
t4
t6
t2
t1
t3

4
t4
t3
t2
t1
t6
t5

5
t3
t6
t1
t5
t4
t2

6
t5
t3
t2
t6
t1
t4

7
t2
t1
t6
t5
t4
t3

8
t4
t6
t5
t1
t3
t2

9
t6
t5
t4
t3
t2
t1

10
t2
t1
t5
t6
t3
t4

Table 5: schedule S1 property = ((S )).
schedule, pair (i, j), ti hosts tj day iff t7i hosts t7j day
11 d. thirteen exceptions justify odd parity |S1 |. 2 7,
schedule = ((S )), explains |Si | even cases.

5. Approximation Algorithm
solved LD-TTP n = 4 n = 6, cases, determined
complete set schedules attaining optimal distances. natural follow-up question
whether techniques scale larger values n. give partial answer question,
show n 4 (mod 6), develop solution n-team LD-TTP whose
total distance 33% higher optimal solution, although practice
optimality gap actually much lower.
construction 34 -approximation, note ratio stronger
currently best-known ( 53 + )-approximation general TTP (Yamaguchi, Imahori,
Miyashiro, & Matsui, 2011). schedule based expander construction,
completely different previous approaches generate approximate TTP solutions.
describe construction, apply benchmark instances 10 teams
16 teams.
Let positive integer. first create single round-robin tournament U 2m
teams, expand double round-robin tournament n = 6m 2 teams.
use variation Modified Circle Method (Fujiwara, Imahori, Matsui, & Miyashiro,
2007) build U , single round-robin schedule. Let {u1 , u2 , . . . , u2m1 , x} 2m
teams. team plays 2m 1 games, according three-part construction:
266

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

(a) 1 k m, team k plays teams following order: {2m k +
1, 2m k + 2, . . . , 2m 1, 1, 2, . . . , k 1, x, k + 1, k + 2, . . . , 2m k}.
(b) + 1 k 2m 1, team k plays teams following order:
{2m k + 1, 2m k + 2, . . . , k 1, x, k + 1, k + 2, . . . , 2m 1, 1, 2, . . . , 2m k}.
(c) Team x plays teams following order: {1, + 1, 2, + 2, . . . ,
1, 2m 1, m}.

u1
u2
u3
u4
u5
u6
u7
x

1
x

u7
u6
u5
u4
u3
u2
u1

2
u2
u1
u7
u6
x

u4
u3
u5

3
u3
x

u1
u7
u6
u5
u4
u2

4
u4
u3
u2
u1
u7
x

u5
u6

5
u5
u4
x

u2
u1
u7
u6
u3

6
u6
u5
u4
u3
u2
u1
x

u7

7
u7
u6
u5
x

u3
u2
u1
u4

Table 6: single round-robin construction 2m = 8 teams.
games involving team x, designate one home team one road team
follows: 1 k m, uk plays road games meets team x, finishing
remaining games home. + 1 k 2m 1, opposite scenario,
uk plays home games meets team x, finishing remaining games
road. example, Table 6 provides single round-robin schedule case
= 4.
construction ensures 1 i, j 2m1, match ui uj
exactly one home team one road team. verify this, note ui home team
uj road team iff occurs j set {1, 2m 1, 2, 2m 2, . . . , 1, + 1, m}.
expand single round-robin tournament U 2m teams double
round-robin tournament n = 6m 2 teams. accomplish this, keep x
transform uk three teams, {t3k2 , t3k1 , t3k }, set teams precisely
{t1 , t2 , t3 , . . . , t6m5 , t6m4 , t6m3 , x}.

t3i2
t3i1
t3i
t3j2
t3j1
t3j

6r 5
t3j1
t3j
t3j2
t3i
t3i2
t3i1

6r 4
t3j
t3j2
t3j1
t3i1
t3i
t3i2

6r 3
t3j2
t3j1
t3j
t3i2
t3i1
t3i

6r 2
t3j1
t3j
t3j2
t3i
t3i2
t3i1

6r 1
t3j
t3j2
t3j1
t3i1
t3i
t3i2

6r
t3j2
t3j1
t3j
t3i2
t3i1
t3i

Table 7: Expanding one time slot U six time slots .
267

fiHoshino & Kawarabayashi

Suppose ui home team game uj , played time slot r.
expand time slot U six time slots , namely slots 6r 5 6r.
describe match assignments Table 7.
proceeding further, let us explain idea behind construction. Recall
each-venue condition, team must visit every opponents home stadium
exactly once, at-most-three condition, road trips three games.
build tournament maximizes number three-game road trips, ensure
majority road trips involve three venues closely situated one another,
minimize total travel. Table 7 above, {t3j2 , t3j1 , t3j } located region,
teams {t3i2 , t3i1 , t3i } play three road games
teams highly-efficient manner.
explain expand time slots games involving team x.
1 k m, consider game uk x. expand time slot U six
time slots , described Table 8.
t3k2
t3k1
t3k
x

6r 5
x
t3k
t3k1
t3k2

6r 4
t3k
x
t3k2
t3k1

6r 3
t3k1
t3k2
x
t3k

6r 2
x
t3k
t3k1
t3k2

6r 1
t3k
x
t3k2
t3k1

6r
t3k1
t3k2
x
t3k

Table 8: six time slot expansion 1 k m.
+ 1 k 2m 1, consider game uk x. expand
time slot U six time slots , described Table 9.
t3k2
t3k1
t3k
x

6r 5
x
t3k
t3k1
t3k2

6r 4
t3k
x
t3k2
t3k1

6r 3
t3k1
t3k2
x
t3k

6r 2
x
t3k
t3k1
t3k2

6r 1
t3k
x
t3k2
t3k1

6r
t3k1
t3k2
x
t3k

Table 9: six time slot expansion + 1 k 2m 1.
construction builds double round-robin tournament n = 6m 2 teams
2n 2 = 12m 6 time slots. give example, Table 10 provides case
= 2.
straightforward verify tournament schedule n = 6m 2 teams
feasible 1, i.e., satisfies each-venue, at-most-three, no-repeat conditions.
show expander construction gives 34 -approximation LD-TTP,
regardless values distance parameters d1 , d2 , . . . , dn1 .
Let n-team instance LD-TTP, n = 6m 2 1. Let
total distance optimal solution . Using expander construction, generate
feasible tournament total distance less 43 S. gives 34 -approximation
LD-TTP.
268

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

t1
t2
t3
t4
t5
t6
t7
t8
t9
x

1
x
t3
t2
t9
t7
t8
t5
t6
t4
t1

2
t3
x
t1
t8
t9
t7
t6
t4
t5
t2

3
t2
t1
x
t7
t8
t9
t4
t5
t6
t3

4
x
t3
t2
t9
t7
t8
t5
t6
t4
t1

5
t3
x
t1
t8
t9
t7
t6
t4
t5
t2

6
t2
t1
x
t7
t8
t9
t4
t5
t6
t3

7
t5
t6
t4
t3
t1
t2
x
t9
t8
t7

8
t6
t4
t5
t2
t3
t1
t9
x
t7
t8

9
t4
t5
t6
t1
t2
t3
t8
t7
x
t9

10 11
t5 t6
t6 t4
t4 t5
t3 t2
t1 t3
t2 t1
x t9
t9 x
t8 t7
t7 t8

12 13 14 15 16 17 18
t4 t8 t9 t7 t8 t9 t7
t5 t9 t7 t8 t9 t7 t8
t6 t7 t8 t9 t7 t8 t9
t1 x t6 t5 x t6 t5
t2 t6 x t4 t6 x t4
t3 t5 t4 x t5 t4 x
t8 t3 t2 t1 t3 t2 t1
t7 t1 t3 t2 t1 t3 t2
x t2 t1 t3 t2 t1 t3
t9 t4 t5 t6 t4 t5 t6

Table 10: case = 2, producing 10-team tournament.
Let y1 , y2 , . . . , yn n = 6m 2 teams , order, dk
distance yk yk+1 1 k n 1. map set {t1 , t2 , . . . , tn1 , x}
{y1 , y2 , . . . , yn } follows: ti = yi 1 3m 3, x = y3m2 , ti = yi+1
3m 2 6m 3. Figure 4 below, illustrate mapping case = 2,
n = 6m 2 teams divided three triplets singleton x:

Figure 4: labeling n = 6m 2 teams, = 2.
apply labeling expander construction create feasible n-team
tournament , n = 6m 2 1. following theorem tells us total
distance tournament, function n 1 distance parameters d1 , d2 , . . . , dn1 .
Theorem 3. Let n-team double round-robin tournament created expander
construction, n = 6m 2. 1 k 6m 3, let fk total
number
Pn1
times dk -length bridge crossed, total distance
k=1 fk dk .
value fk given Table 11. addition, f1 = (8n 8)/3, f2 = 4n 4,
f3m2 = fn/21 = (n2 + 6n 16)/3, f3m1 = fn/2 = (n2 + 9n 22)/3, f3m = fn/2+1 =
(n2 + 9n 34)/3, f6m3 = fn1 = (8n 2)/3.
Case
(a)
(b)
(c)
(d)
(e)
(f)

k
k = 4, 7, 10, . . . , 3m 5
k = 5, 8, 11, . . . , 3m 4
k = 3, 6, 9, 12, . . . , 3m 3
k = 3m + 1, 3m + 4, . . . , 6m 8, 6m 5
k = 3m + 2, 3m + 5, . . . , 6m 7, 6m 4
k = 3m + 3, 3m + 6, . . . , 6m 6

fk
4k(n k)/3 + (6n + 8k 20)/3
4k(n k)/3 + (4n + 12k 20)/3
4k(n k)/3 + (4n + 6k 16)/3
4k(n k)/3 + (8n 4k 22)/3
4k(n k)/3 + (14n 10k 16)/3
4k(n k)/3 + (4n 2k 4)

Table 11: formulas fk function n k.

269

fiHoshino & Kawarabayashi

Proof. six cases, carefully enumerate number times team
crosses bridge, considering activity team tournament schedule .
(a) k teams left dk -length bridge, one team crosses bridge 2(nk)/3
times, (k + 5)/3 teams cross bridge 2(n k + 3)/3 times (2k 8)/3 teams
cross bridge 2(n k + 6)/3 times. n k 1 teams right
bridge (not including team x), (2n 3k 5)/3 teams cross bridge
2(k + 2)/3 times remaining (n + 2)/3 teams cross bridge 2(k + 5)/3 times.
Finally, team x crosses bridge (4k + 2)/3 times. there, sum cases
determine fk = 4k(n k)/3 + (6n + 8k 20)/3.
(b) k teams left dk -length bridge, one team crosses bridge 2(n
k + 1)/3 times, (k + 4)/3 teams cross bridge 2(n k + 4)/3 times (2k 7)/3
teams cross bridge 2(n k + 7)/3 times. n k 1 teams right
bridge (not including team x), (2n 3k 2)/3 teams cross bridge
2(k + 1)/3 times, (n 4)/3 teams cross bridge 2(k + 4)/3 times, one team
crosses 2(k + 7)/3 times. Finally, team x crosses bridge (4k 2)/3 times.
there, sum cases determine fk = 4k(n k)/3 + (4n + 12k 20)/3.
(c) k teams left dk -length bridge, (k + 6)/3 teams cross
bridge 2(n k + 2)/3 times, remaining (2k 6)/3 teams cross bridge
2(n k + 5)/3 times. n k 1 teams right bridge (not
including team x), (n k 1)/3 teams cross bridge 2k/3 times
remaining 2(n k 1)/3 teams cross bridge (2k + 6)/3 times. Finally, team x
crosses bridge 4k/3 times. there, sum cases determine
fk = 4k(n k)/3 + (4n + 6k 16)/3.
(d) k 1 teams left dk -length bridge (not including team x), (k + 5)/3
teams cross bridge 2(n k)/3 times, remaining (2k 8)/3 teams cross
bridge 2(n k + 3)/3 times. n k teams right bridge,
(nk +3)/3 cross bridge 2(k +2)/3 times remaining (2n2k 3)/3 teams
cross bridge 2(k + 5)/3 times. Finally, team x crosses bridge 2(n k)/3 times.
there, sum cases determine fk = 4k(nk)/3+(8n4k22)/3.
(e) k 1 teams left dk -length bridge (not including team x), (3k
n + 4)/3 teams cross bridge 2(n k + 1)/3 times, remaining (n 7)/3
teams cross bridge 2(n k + 4)/3 times. n k teams right
bridge, (n k + 4)/3 cross bridge 2(k + 4)/3 times remaining
(2n 2k 4)/3 teams cross bridge 2(k + 7)/3 times. Finally, team x crosses
bridge 2(n k + 4)/3 times. there, sum cases determine
fk = 4k(n k)/3 + (14n 10k 16)/3.
(f) k 1 teams left dk -length bridge (not including team x), (3k
n + 1)/3 teams cross bridge 2(n k + 2)/3 times, remaining (n 4)/3
teams cross bridge 2(n k + 5)/3 times. n k teams right
bridge, (n k + 2)/3 cross bridge 2(k + 3)/3 times remaining
(2n 2k 2)/3 teams cross bridge 2(k + 6)/3 times. Finally, team x crosses
270

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

bridge 2(n k + 2)/3 times. there, sum cases determine
fk = 4k(n k)/3 + (4n 2k 4).
Finally, clear exceptional cases. k = 1, team t1 crosses bridge 2(n 1)/3
times, remaining n 1 teams cross twice. Thus, f1 = 2(n 1)/3 + 2(n 1) =
(8n 8)/3. k = 2, team t1 crosses bridge 2(n 1)/3 times, team t2 crosses 2(n + 2)/3
times, (2n 5)/3 teams cross twice, (n 1)/3 teams cross four times. Thus, f2 =
2(n 1)/3 + 2(n + 2)/3 + (4n 10)/3 + (4n 4)/3 = 4n 4. k = n 1, team tn
crosses bridge 2(n + 2)/3 times, remaining n 1 teams cross twice. Thus,
fn1 = 2(n + 2)/3 + 2(n 1) = (8n 2)/3.
k = n2 1, formula fk case (d), except one team
makes additional trip across bridge. k = n2 1, formula fk
case (e), except one team makes one fewer trip across bridge. Finally,
k = n2 + 1, formula fk case (f), except two teams
make one additional trip across bridge. straightforward calculation results
verifying f3m2 = fn/21 = (n2 + 6n 16)/3, f3m1 = fn/2 = (n2 + 9n 22)/3,
f3m = fn/2+1 = (n2 + 9n 34)/3. completes proof.
example, case = 2 (see Table 10), n = 10, total travel
distance 24d1 + 36d2 + 42d3 + 48d4 + 56d5 + 52d6 + 38d7 + 36d8 + 26d9 .
Pn1
Let = k=1
lk dk trivial lower bound , found adding independent
lower bounds team ti . described proof Lemma 1, lk =
k
2kd nk
3 e + 2(n k)d 3 e k teams left dk bridge must make
least 2d nk
3 e trips across bridge, n k teams right bridge must
make least 2d k3 e trips across.
3, straightforward verify flkk < 34 1 k n 1, thus
establishing 34 -approximation LD-TTP. ratio 43 best possible due
l3 = 4n 8, implying fl33 34 n .
case k = 3, f3 = 16n34
3
worst-case scenario achieved dk = 0 k 6= 3, i.e., teams {t1 , t2 , t3 }
located one vertex, remaining n 3 teams located another vertex.
natural question whether exist similar constructions n 0 n 2
(mod 6). cases, addition n 4 case analyzed, ask whether
4
3 -approximation best possible. one many open questions arising
work.

6. Application Benchmark Sets
apply theories various benchmark TTP sets. start case n = 6,
apply Theorems 1 2 known 6-team TTP benchmarks. addition NL6,
examine six-team set Super Rugby League (SUPER6), six galaxy stars whose
coordinates appear three-dimensional space (GALAXY6), earlier six-team circular
distance instance (CIRC6), trivial constant distance instance (CON6)
pair teams distance one unit.
benchmark sets, first order six teams approximate
straight line, either formal line best fit informal check inspection.
271

fiHoshino & Kawarabayashi

ordered six teams, determine
five-tuple (d1 , d2 , d3 , d4 , d5 ) dis
tance matrix ignore 62 5 = 10 entries. Modifying benchmark set
assuming six teams lie straight line, solve LD-TTP via Theorem 1. Using
Theorem 2, take set tournament schedules achieving
optimal distance
6
apply actual distance matrix benchmark set (with 2 entries)
optimal schedules output tournament minimum total distance.
simple process, taking approximately 0.3 seconds computation time per
benchmark set, generates feasible solution 6-team TTP. surprise,
algorithm outputs distance-optimal schedule five benchmark sets.
unexpected result, given non-linearity data sets: example, CIRC6
teams arranged circle, GALAXY6 uses three-dimensional distances.
illustrate theory, let us begin NL6, ordering six teams south north:

Figure 5: Location six NL6 teams.
Thus, Florida t1 , Atlanta t2 , Pittsburgh t3 , Philadelphia t4 , New York t5 ,
Montreal t6 . NL6 distance matrix (Trick, 2012), (d1 , d2 , d3 , d4 , d5 ) =
(605, 521, 257, 80, 337).
Since 2 min{d2 + d4 , d1 + d4 , d3 + d4 , 3d4 , d2 + d5 , d2 + d3 , 3d2 } = 6d4 = 480, Theorem
1 tells us optimal LD-TTP solution total distance + 6d4 = 14d1 + 16d2 +
20d3 + 22d4 + 14d5 = 28424. Theorem 2, 24 schedules set S4 , total
distance + 6d4 . Two 24 schedules presented Table 12.
t1
t2
t3
t4
t5
t6

1
t2
t1
t5
t6
t3
t4

2
t4
t5
t6
t1
t2
t3

3
t5
t3
t2
t6
t1
t4

4
t3
t4
t1
t2
t6
t5

5
t5
t6
t4
t3
t1
t2

6
t6
t3
t2
t5
t4
t1

7
t3
t4
t1
t2
t6
t5

8
t4
t6
t5
t1
t3
t2

9
t6
t5
t4
t3
t2
t1

10
t2
t1
t6
t5
t4
t3

t1
t2
t3
t4
t5
t6

1
t2
t1
t6
t5
t4
t3

2
t5
t6
t4
t3
t1
t2

3
t6
t3
t2
t5
t4
t1

4
t3
t5
t1
t6
t2
t4

5
t6
t4
t5
t2
t3
t1

6
t4
t3
t2
t1
t6
t5

7
t3
t5
t1
t6
t2
t4

8
t5
t4
t6
t2
t1
t3

9
t4
t6
t5
t1
t3
t2

10
t2
t1
t4
t3
t6
t5

Table 12: Two LD-TTP solutions total distance + 6d4 .
Removing straight line assumption, apply actual NL6 distance matrix
determine total distance traveled 24 schedules set S4 ,
naturally produce different sums. left schedule Table 12 best among
24 schedules, total distance 23916, right schedule worst, total
272

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

distance 24530. note left schedule, achieving optimal distance 23916
miles, identical Table 1.
repeat analysis four benchmark sets. each, mark
sets {S1 , S2 , . . . , S7 } produced optimal schedule.
Benchmark
Data Set
NL6
SUPER6
GALAXY6
CIRC6
CON6

Optimal
Solution
23916
130365
1365
64
43

LD-TTP
Solution
23916
130365
1365
64
43

Optimal
Schedule
S4
S3
S1
S1
S1

Table 13: Comparing LD-TTP TTP benchmark data sets.
sophisticated branch-and-price heuristic (Irnich, 2010) solved NL6 one
minute, yet required three hours solve CIRC6. latter problem considerably
difficult due inherent symmetry data set, required branching.
However, LD-TTP approach, problems solved optimality
amount time approximately 0.3 seconds.
Based results Table 13, ask whether exists 6-team instance
optimal TTP solution different optimal LD-TTP solution. question
answered following section.
conclude section, apply 34 -approximation produced expander
construction various (non-linear) benchmark sets n 4 (mod 6). apply
construction 10-team 16-team instances earlier examples (Trick, 2012).
Instance
CONS10
CIRC10
NL10
SUPER10
GALAXY10
CONS16
CIRC16
NL16
GALAXY16

Optimal
124
242
59436
316329
4535
327
[846, 916]
[249477, 261687]
[13619, 14900]

Solution
128
276
63850
361924
4862
334
994
286439
15429

Percentage Gap
3.2%
14.0%
7.4%
14.4%
7.2%
2.1%
[8.5%, 17.5%]
[9.5%, 14.8%]
[3.6%, 13.3%]

Table 14: Comparing construction optimal solution nine benchmark sets.
GALAXY, NL, SUPER instances, first need arrange n teams
approximate straight line. this, apply simple algorithm first randomly
assigns n teams {t1 , t2 , . . . , tn1 , x}, calculates sum total distances
adjacent pair teams. generate local line-of-best-fit recursively selecting
two teams ti tj switching positions reduces sum n1 distances.
algorithm terminates permutation n teams {t1 , t2 , . . . , tn1 , x}
273

fiHoshino & Kawarabayashi

locally optimal (but perhaps globally), apply expander construction
calculate total travel distance n-team tournament.
Instead time-consuming process enumerates n! permutations teams,
simple algorithm generates fast solution benchmark instances less
2 seconds total computation time. Despite simplicity approach, see
Table 14 optimality gap extremely small constant instances (CONS),
quite reasonable (non-linear) instances.

7. Optimality Gap
Table 13, five 6-team benchmark instances produced identical solutions
TTP LD-TTP. natural question whether always case. show
TTP LD-TTP solutions must identical n = 4, necessarily n = 6.
instance n teams, define X total distance optimal TTP
solution, X total distance optimal LD-TTP solution. Define OGn
X X
maximum optimality gap, largest value X taken instances .
Theorem 4. instance n = 4 teams, optimal TTP solution optimal
LD-TTP solution. words, OG4 = 0%.
Proof. Table 3, showed 18 non-isomorphic schedules total distance
8(d1 + d2 + d3 ), i.e., 18 different solutions LD-TTP. 18 schedules,
remove linear distance assumption determine total travel distance
function six distance parameters (i.e., variables {Di,j : 1 < j 4}).
example, schedule Table 2 total distance 4D1,2 + 2D1,3 + 2D1,4 + 3D2,3 + D2,4 +
5D3,4 represent 6-tuple (4, 2, 2, 3, 1, 5). Considering 4! permutations
{t1 , t2 , t3 , t4 }, 18 24 tournament schedules, producing 36 unique 6-tuples,
including (4, 2, 2, 3, 1, 5). Denote L set thirty-six 6-tuples.
brute-force enumeration finds 1920 feasible 4-team tournaments.
1920 tournaments, determine 6-tuple representing total travel distance, find
246 unique 6-tuples, denote set A. definition, L A.
prove OG4 = 0, must verify set {D1,2 , D1,3 , D1,4 , D2,3 , D2,4 , D3,4 }
satisfying Triangle Inequality, optimal solutions TTP LD-TTP
same, i.e., optimal solution among schedules (whose six-tuples given A)
appears subset linear-distance schedules (whose six-tuples given L).
establish this, first use Triangle Inequality verify 204 24636 = 210
elements A\L, corresponding schedule dominated least one elements
L.
example, six-tuple (3, 4, 3, 4, 1, 4) one 210 elements A\L. Comparing
six-tuple (4, 2, 2, 3, 1, 5) L, see corresponding schedule A\L
total distance 2D1,3 +D1,4 +D2,3 D1,2 D3,4 = (D1,3 +D1,4 D3,4 )+(D1,3 +D2,3 D1,2 ) 0
corresponding schedule L, given Table 2.
computer search shows 204 210 elements A\L handled
applying Triangle Inequality way, showing dominated least one element L. six exceptions, namely 6-tuples set {(2, 3, 3, 3, 3, 4),
(3, 2, 3, 3, 4, 3), (3, 3, 2, 4, 3, 3), (3, 3, 4, 2, 3, 3), (3, 4, 3, 3, 2, 3), (4, 3, 3, 3, 3, 2)}.
274

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

cases, analysis slightly harder. Consider six-tuple (2, 3, 3, 3, 3, 4); rest
handled way, symmetry.
twelve 6-tuples L 17 total trips, D1,2 coefficient
strictly less D4,5 coefficient. (An example one 6-tuple (4, 2, 2, 3, 1, 5).)
Taking average, derive 6-tuple (7/3, 17/6, 17/6, 17/6, 17/6, 10/3), implying
existence least one LD-TTP schedule whose total distance X (14D1,2 +
17D1,3 + 17D1,4 + 17D2,3 + 17D2,4 + 20D3,4 )/6. Let distance represented
6-tuple (2, 3, 3, 3, 3, 4). Triangle Inequality,
6(Y X) = 2D1,2 + D1,3 + D1,4 + D2,3 + D2,4 + 4D3,4
= (D1,3 + D3,4 + D4,2 D2,1 ) + (D1,4 + D4,3 + D3,2 D2,1 ) + 2D3,4
0 + 0 + 2 0 = 0.
words, shown every element A\L dominated least
one element L. Therefore, choice {D1,2 , D1,3 , D1,4 , D2,3 , D2,4 , D3,4 } satisfying
Triangle Inequality, optimal solutions TTP LD-TTP same, i.e.,
OG4 = 0%.
earlier paper (Hoshino & Kawarabayashi, 2012), authors conjectured
OG6 > 0%, although unable find 6-team instance positive optimality
1
2.3%.
gap. present simple instance show OG6 43
Let
6-team instance D1,2 = D5,6 = 2 Di,j = 1. Clearly
62 = 15 distances satisfy Triangle Inequality. show X = 43
1
. Consider Table 15, solution TTP
X = 44, thus proving OG6 43
(but LD-TTP) 43 trips.
t1
t2
t3
t4
t5
t6

1
t5
t6
t4
t3
t1
t2

2
t2
t1
t6
t5
t4
t3

3
t3
t5
t1
t6
t2
t4

4
t5
t6
t4
t3
t1
t2

5
t3
t5
t1
t6
t2
t4

6
t4
t3
t2
t1
t6
t5

7
t6
t4
t5
t2
t3
t1

8
t4
t3
t2
t1
t6
t5

9
t2
t1
t6
t5
t4
t3

10
t6
t4
t5
t2
t3
t1

# Trips
7
7
8
7
7
7

Table 15: optimal 43-trip TTP solution beats optimal LD-TTP solution.
inspection, see team travels along bridge connecting stadiums
t1 t2 , along bridge connecting stadiums t5 t6 . Thus, total travel
distance must 43 1 = 43, since 2-unit distances D1,2 D5,6 appear
total sum. Since every 6-team tournament must least 43 total trips (see Table 13),
proves X = 43.
295 potentially-optimal LD-TTP schedules Theorem 2, consider
6! = 720 permutations (t1 , t2 , t3 , t4 , t5 , t6 ) see tournament total
distance 43. computer search shows 36 295 schedules total distance
44, none distance 43. proves X = 44 optimal LD-TTP travel
distance instance .
275

fiHoshino & Kawarabayashi

1
2.3%. ask whether
Therefore, maximum optimality gap OG6 least 43
gap made larger, propose following question.

Problem 1. Determine value OGn n 6.
Suppose OG6 = 5%. one 295 LD-TTP solutions Theorem 2
5% higher optimal TTP solution, found fraction computational
cost. course, necessary case n = 6 use integer constraint
programming output TTP solution reasonable amount time. However,
larger values n, linear distance relaxation technique would allow us quickly generate
close-to-optimal solutions exact optimal total distance unknown difficult
computationally. hopeful approach help us develop better upper
bounds large unsolved benchmark instances.

8. Conclusion
many professional sports leagues, teams divided two conferences, teams
intra-league games within conference well inter-league games
teams conference. TTP models intra-league tournament play. NPcomplete Bipartite Traveling Tournament Problem (Hoshino & Kawarabayashi, 2011) models inter-league play, would interesting see whether linear distance relaxation
also applied bipartite instances help formulate new ideas inter-league tournament scheduling.
conclude paper proposing two new benchmark instances Traveling
Tournament Problem, well open problem conjecture Linear Distance
TTP. first begin benchmark instances.
n 4, define LINEn instance n teams located
straight line, distance one unit separating pair adjacent teams, i.e., dk = 1
1 k n 1. define INCRn increasing-distance scenario
n teams arranged dk = k 1 k n 1. Figure 6 illustrates location
team INCR6.

Figure 6: instance INCR6.
definition, TTP solution matches LD-TTP solution two
instances. Theorem 1, optimal solutions LINE6 INCR6 total distance
84 250, respectively. naturally motivates following problem:
Problem 2. Solve TTP instances LINEn INCRn, n 8.
conclude one problem, inspired Theorem 2 listed seven
possible optimal distances 6-team LD-TTP:
276

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

Problem 3. Let P Dn denote number possible distances solution
n-team LD-TTP. example, P D4 = 1 P D6 = 7. Prove disprove P Dn
exponential n.

Acknowledgments
research partially supported Japan Society Promotion Science
(Grant-in-Aid Scientific Research), C & C Foundation, Kayamori Foundation,
Inoue Research Award Young Scientists. authors thank Brett Stevens
Carleton University suggesting idea Linear Distance TTP 2010
Winter Meeting Canadian Mathematical Society.

Appendix
used Maplesoft (www.maplesoft.com) generate set optimal LD-TTP schedules
n = 4 n = 6. appendix, explain process generated
36 optimal schedules case n = 4.
simplify notation, used numbers 1 4 represent team numbers
opponents road games, numbers 11 14 represent team numbers
opponents home games. Thus, notation, schedule left (from Table 2)
identical 4 6 matrix right.
Team
t1
t2
t3
t4

1
t4
t3
t2
t1

2
t3
t4
t1
t2

3
t2
t1
t4
t3

4
t4
t3
t2
t1

5
t3
t4
t1
t2

6
t2
t1
t4
t3

14 13 2 4 3 12
13 14 11 3 4 1
2 1 14 12 11 4
1 2 3 11 12 13

produce set 36 schedules, following code used:
restart: with(combinat):
A1 :=
A3 :=
B1 :=
B3 :=
C1 :=
C3 :=
Z[{1,
Z[{1,

<,>(12, 1, 14, 3): A2
<,>(2, 11, 14, 3): A4
<,>(13, 14, 1, 2): B2
<,>(3, 14, 11, 2): B4
<,>(14, 13, 2, 1): C2
<,>(4, 13, 2, 11): C4
2}] := d1: Z[{2, 3}] :=
3}] := d1+d2: Z[{1, 4}]

:= <,>(12, 1, 4, 13):
:= <,>(2, 11, 4, 13):
:= <,>(13, 4, 1, 12):
:= <,>(3, 4, 11, 12):
:= <,>(14, 3, 12, 1):
:= <,>(4, 3, 12, 11):
d2: Z[{3, 4}] := d3:
:= d1+d2+d3: Z[{2, 4}] := d2+d3:

dist := proc (myinput, k)
local i, myseq, x; x := 0; myseq :=
7
7<= myseq[i] 7<= myseq[i+1]
elif 7<= myseq[i] myseq[i+1]<7
elif myseq[i]<7 7<= myseq[i+1]

[7, op(myinput), 7];
x:=x
x:=x+Z[{myseq[i+1],k}]
x:=x+Z[{myseq[i],k}]
277

fiHoshino & Kawarabayashi

elif myseq[i]<7 myseq[i+1]<7 x:=x+Z[{myseq[i+1],myseq[i]}]
else RETURN(ERROR)
fi:
od: x: end:
checker := proc (my720)
local k, flag, x, y, goodlist, temp; goodlist := NULL;
k 720
temp := Matrix([seq(my720[k][t], = 1 .. 6)]); flag := 0;
x 4 5
abs(temp[x][y]-temp[x][y+1])=10 flag:=1 fi:
od: od:
x 4
dist([seq(temp[x,k],k = 1..6)],x)<>2*(d1+d2+d3) flag := 1 fi:
od:
flag = 0 goodlist := goodlist, temp: fi:
od: goodlist: end:
my720 := permute([A1, A4,
my720 := permute([A1, A4,
my720 := permute([A1, A4,
my720 := permute([A1, A4,
my720 := permute([A2, A3,
my720 := permute([A2, A3,
my720 := permute([A2, A3,
my720 := permute([A2, A3,
finallist := [set1, set2,

B1, B4, C1,
B1, B4, C2,
B2, B3, C1,
B2, B3, C2,
B1, B4, C1,
B1, B4, C2,
B2, B3, C1,
B2, B3, C2,
set3, set4,

C4]):
C3]):
C4]):
C3]):
C4]):
C3]):
C4]):
C3]):
set5,

set1 := checker(my720):
set2 := checker(my720):
set3 := checker(my720):
set4 := checker(my720):
set5 := checker(my720):
set6 := checker(my720):
set7 := checker(my720):
set8 := checker(my720):
set6, set7, set8];

Appendix B
provide Maplesoft code generated 295 non-isomorphic schedules Theorem 2. Due symmetry, need consider cases S1 , S2 , S3 , S4 .
authors would happy provide full set 295 schedules (available simple .txt
file upon request), and/or answer questions explain code generates
complete set optimal schedules n = 6 case LD-TTP.
restart: with(combinat):
Z := Matrix(6, 6, 0):
Z[1, 2] := a: Z[1, 3] := a+b: Z[1, 4] := a+b+c: Z[1, 5] := a+b+c+d:
Z[1, 6] := a+b+c+d+e: Z[2, 3] := b: Z[2, 4] := b+c: Z[2, 5] := b+c+d:
Z[2, 6] := b+c+d+e: Z[3, 4] := c: Z[3, 5] := c+d: Z[3, 6] := c+d+e:
Z[4, 5] := d: Z[4, 6] := d+e: Z[5, 6] := e:
6 j i+1 6 Z[j, i] := Z[i, j] od: od:
all252 := choose(10, 5): combos := []:
278

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

252
test := all252[i]: flag := 0:
j 2 test[j+3]-test[j] <= 3 flag := 1: fi: od:
j 4 test[j+1]-test[j] >= 5 flag := 1: fi: od:
or(test[1] >= 5, test[5] <= 6) flag := 1 fi:
flag = 0 combos := [op(combos), test]: fi:
od:
totaldist := proc (myinput, k)
local i, myseq, y; := 0; myseq := [7, op(myinput), 7];
11
7<= myseq[i] 7<= myseq[i+1] y:=y
elif 7<= myseq[i] myseq[i+1]<7 y:=y+Z[myseq[i+1],k]
elif myseq[i]<7 7<= myseq[i+1] y:=y+Z[myseq[i],k]
elif myseq[i]<7 myseq[i+1]<7 y:=y+Z[myseq[i+1],myseq[i]]
else RETURN(ERROR)
fi:
od: y: end:
getseq := proc (myfive, k)
local myperm, myseq, mylist, i, j;
mylist := []; myperm := permute(minus({1, 2, 3, 4, 5, 6}, {k}));
120 myseq := [seq(7, = 1 .. 10)];
j 5 myseq[myfive[j]] := myperm[i][j]: od:
mylist := [op(mylist), myseq]
od:
mylist: end:
checkdup := proc (tryj, j, tryk, k)
local i, val1, val2, x; x := 0;
i:=0: x=0 i<10
i:=i+1; tryj[i]=tryk[i] tryj[i]<7 x:=1: fi: od:
i:=0: x=0 i<10
i:=i+1; tryj[i]=k tryk[i]<7 x:=1: fi: fi: od:
i:=0: x=0 i<10
i:=i+1; tryk[i]=j tryj[i]<7 x:=1: fi: fi: od:
i:=0: x=0 i<10
i:=i+1; tryk[i]=j val1:=i fi: tryj[i]=k val2:=i: fi: od:
x = 0 abs(val1-val2) <= 1 x := 1: fi: fi:
x: end:
fivetuple := proc (myset)
[coeff(myset,a),coeff(myset,b),coeff(myset,c),coeff(myset,d),coeff(myset,e)]:
end:

279

fiHoshino & Kawarabayashi

p 5 q 5 r 5
5 5 k 6
S[k, [2*p, 2*q, 2*r, 2*s, 2*t]] := NULL:
od: od: od: od: od: od:
kk 6 allvals[kk] := {}: od:
r 194 x := getseq(combos[r], 1):
120 in(2, {seq(x[s][k], k = 6 .. 10)})
:= fivetuple(totaldist(x[s], 1));
allvals[1] := {y, op(allvals[1])}; S[1, y] := S[1, y], x[s] fi: od: od:
r 194 x := getseq(combos[r], 2):
120 := fivetuple(totaldist(x[s], 2));
allvals[2] := {y, op(allvals[2])}; S[2, y] := S[2, y], x[s] od: od:
r 194 x := getseq(combos[r], 3);
120 := fivetuple(totaldist(x[s], 3));
allvals[3] := {y, op(allvals[3])}; S[3, y] := S[3, y], x[s] od: od:
r 194 x := getseq(combos[r], 4);
120 := fivetuple(totaldist(x[s], 4));
allvals[4] := {y, op(allvals[4])}; S[4, y] := S[4, y], x[s] od: od:
r 194 x := getseq(combos[r], 5);
120 := fivetuple(totaldist(x[s], 5));
allvals[5] := {y, op(allvals[5])}; S[5, y] := S[5, y], x[s] od: od:
r 194 x := getseq(combos[r], 6);
120 := fivetuple(totaldist(x[s], 6));
allvals[6] := {y, op(allvals[6])}; S[6, y] := S[6, y], x[s] od: od:
pp 10 qq 10 rr 10 ss 10
triplet1[[2*pp, 2*qq, 2*rr, 2*ss, 6]] := []: od: od: od: od:
pp 10 qq 10 rr 10 ss 10
triplet2[[6, 2*ss, 2*rr, 2*qq, 2*pp]] := []: od: od: od: od:
pp nops(allvals[1])
qq nops(allvals[2])
rr nops(allvals[3])
val := allvals[1][pp]+allvals[2][qq]+allvals[3][rr];
triplet1[val] := [op(triplet1[val]), [pp, qq, rr]]
od: od: od:
pp nops(allvals[4])
280

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

qq nops(allvals[5])
rr nops(allvals[6])
val := allvals[4][pp]+allvals[5][qq]+allvals[6][rr];
triplet2[val] := [op(triplet2[val]), [pp, qq, rr]]
od: od: od:
getnext := proc (inputset, x, setx)
local i, k1, k2, k3, candx, mylist;
mylist := NULL;
k1 := op(minus({1, 2, 3, 4, 5, 6, 7}, {op(inputset[1])}));
k2 := op(minus({1, 2, 3, 4, 5, 6, 7}, {op(inputset[2])}));
k3 := op(minus({1, 2, 3, 4, 5, 6, 7}, {op(inputset[3])}));
nops(setx) candx := setx[i];
checkdup(candx, x, inputset[1], k1) = 0
checkdup(candx, x, inputset[2], k2) = 0
checkdup(candx, x, inputset[3], k3) = 0
mylist := mylist, candx fi: fi: fi: od:
[mylist]: end:
getpos := proc (aval, bval, cval, dval)
local pos3, pos4, pos5, pos6;
aval = 3 pos3 := 2 elif aval = 4 pos4 := 2 elif aval =
pos5 := 2 elif aval = 6 pos6 := 2 else RETURN(ERROR) end if;
bval = 3 pos3 := 3 elif bval = 4 pos4 := 3 elif bval =
pos5 := 3 elif bval = 6 pos6 := 3 else RETURN(ERROR) end if;
cval = 3 pos3 := 7 elif cval = 4 pos4 := 7 elif cval =
pos5 := 7 elif cval = 6 pos6 := 7 else RETURN(ERROR) end if;
dval = 3 pos3 := 8 elif dval = 4 pos4 := 8 elif dval =
pos5 := 8 elif dval = 6 pos6 := 8 else RETURN(ERROR) end if;
[pos3, pos4, pos5, pos6]: end:

5
5
5
5

firsttwo := proc (aval, bval, cval, dval,new1,new2)
local pairs12, p, q, i, t1, t2, flag; pairs12 := {};
p nops(new1) q nops(new2)
checkdup(new1[p], 1, new2[q], 2) = 0
new1[p][4] <> bval new1[p][6] <> cval new1[p][9] <> dval
new2[q][2] <> aval new2[q][5] <> bval new2[q][7] <> cval
flag := 0;
t1:=[2,aval,bval,new1[p][4],new1[p][5],new1[p][6],cval,dval,new1[p][9],2];
t2:=[1,new2[q][2],aval,bval,new2[q][5],new2[q][6],new2[q][7],cval,dval,1];
9 t1[i]=t2[i+1] t1[i+1]=t2[i] flag:=1 fi: od:
flag = 0 pairs12 := {op(pairs12), [new1[p], new2[q]]} fi: fi: fi:
od: od:
pairs12: end:

281

fiHoshino & Kawarabayashi

firstthree := proc (pairs12, sixpos, new6)
local last6, mytry, trips126, p, q, k; last6 := {}; trips126 := {};
k nops(new6) mytry := new6[k];
mytry[sixpos] = 1 mytry[sixpos+1] = 2
last6 := {op(last6), mytry} fi:
od:
p nops(pairs12) q nops(last6)
checkdup(pairs12[p][1], 1, last6[q], 6) = 0
checkdup(pairs12[p][2], 2, last6[q], 6) = 0
trips126 := {op(trips126), [op(pairs12[p]), last6[q]]}: fi:
od: od:
trips126: end:
steps126 := proc (new1, new2, new6)
local i, j, k, finalsol; finalsol := NULL;
nops(new1) j nops(new2)
checkdup(new1[i], 1, new2[j], 2) = 0
k nops(new6)
checkdup(new1[i], 1, new6[k], 6) = 0
checkdup(new2[j], 2, new6[k], 6) = 0
finalsol := finalsol, [new1[i], new2[j], new6[k]]:
fi: od: fi: od: od:
[finalsol]: end:
steps345 := proc (iset, new3, new4, new5)
local mylist, tryd, trye, tryf, candd, cande, candf, a, b, c;
mylist := NULL; tryd := getnext(iset, 3, new3);
tryd <> [] trye := getnext(iset, 4, new4);
trye <> [] tryf := getnext(iset, 5, new5);
tryf <> [] nops(tryd) candd := tryd[a];
b nops(trye) cande := trye[b];
checkdup(candd, 3, cande, 4) = 0
c nops(tryf) candf := tryf[c];
checkdup(candf, 5, candd, 3) = 0
checkdup(candf, 5, cande, 4) = 0
mylist := mylist, [iset[1], iset[2], candd, cande, candf, iset[3]]:
fi: fi: od: fi: od: od: fi: fi: fi:
[mylist]: end:
allsix := proc (my126,aval,bval,cval,dval,new3,new4,new5)
local last3, last4, last5, k, mytry, tempval, pos3, pos4, pos5,
finalresult, p, q, r, s, trips345, my345, valnext, finalans;
last3:={}; last4:={}; last5:={}; finalresult:={}; trips345:={};
tempval := getpos(aval, bval, cval, dval);
pos3 := tempval[1]; pos4 := tempval[2]; pos5 := tempval[3];
282

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

k nops(new3) mytry := new3[k];
mytry[pos3] = 1 mytry[pos3+1] = 2
checkdup(my126[1], 1, mytry, 3) = 0
checkdup(my126[2], 2, mytry, 3) = 0
checkdup(my126[3], 6, mytry, 3) = 0
last3 := {op(last3), mytry} fi: fi:
od:
k nops(new4) mytry := new4[k];
mytry[pos4] = 1 mytry[pos4+1] = 2
checkdup(my126[1], 1, mytry, 4) = 0
checkdup(my126[2], 2, mytry, 4) = 0
checkdup(my126[3], 6, mytry, 4) = 0
last4 := {op(last4), mytry} fi: fi:
od:
k nops(new5) mytry := new5[k];
mytry[pos5] = 1 mytry[pos5+1] = 2
checkdup(my126[1], 1, mytry, 5) = 0
checkdup(my126[2], 2, mytry, 5) = 0
checkdup(my126[3], 6, mytry, 5) = 0
last5 := {op(last5), mytry} fi: fi:
od:
p nops(last3) q nops(last4)
checkdup(last3[p], 3, last4[q], 4) = 0
r nops(last5)
checkdup(last3[p], 3, last5[r], 5) = 0
checkdup(last4[q], 4, last5[r], 5) = 0
trips345 := {op(trips345), [last3[p], last4[q], last5[r]]}:
fi: od: fi: od: od:
nops(trips345) my345 := trips345[s];
finalresult := {op(finalresult),
[my126[1], my126[2], my345[1], my345[2], my345[3], my126[3]]}:
od:
finalresult: end:
checkallsolutions := proc(sixtuples)
local k,rr,mysolutions,cand1,cand2,cand3,cand4,cand5,cand6,
new1,new2,new3,new4,new5,new6,mytry,firsthalf,val:
mysolutions := []:
rr nops(sixtuples)
cand1 := [S[1, allvals[1][sixtuples[rr][1]]]];
cand2 := [S[2, allvals[2][sixtuples[rr][2]]]];
cand3 := [S[3, allvals[3][sixtuples[rr][3]]]];
cand4 := [S[4, allvals[4][sixtuples[rr][4]]]];
cand5 := [S[5, allvals[5][sixtuples[rr][5]]]];
cand6 := [S[6, allvals[6][sixtuples[rr][6]]]];
283

fiHoshino & Kawarabayashi

new1 := {}; new2 := {}; new3 := {}; new4 := {}; new5 := {}; new6 := {};
k nops(cand1) mytry := cand1[k]; new1 := {mytry, op(new1)}: od:
k nops(cand2) mytry := cand2[k]; new2 := {mytry, op(new2)}: od:
k nops(cand3) mytry := cand3[k]; new3 := {mytry, op(new3)}: od:
k nops(cand4) mytry := cand4[k]; new4 := {mytry, op(new4)}: od:
k nops(cand5) mytry := cand5[k]; new5 := {mytry, op(new5)}: od:
k nops(cand6) mytry := cand6[k]; new6 := {mytry, op(new6)}: od:
firsthalf := steps126(new1, new2, new6);
k nops(firsthalf) val:=steps345(firsthalf[k],new3,new4,new5);
val <> [] mysolutions := [op(mysolutions), op(val)]: fi:
od: od:
mysolutions: end:
generatesolutions := proc(sixtuples)
local cand1,cand2,cand3,cand4,cand5,cand6,new1,new2,new3,new4,new5,new6,
k,rr,x,y,mytry,flag,aval,bval,cval,dval,pairs12,sixpos,trips126,my24,sols:
sols := {}: my24 := permute([3,4,5,6]):
rr nops(sixtuples)
cand1 := [S[1, allvals[1][sixtuples[rr][1]]]];
cand2 := [S[2, allvals[2][sixtuples[rr][2]]]];
cand3 := [S[3, allvals[3][sixtuples[rr][3]]]];
cand4 := [S[4, allvals[4][sixtuples[rr][4]]]];
cand5 := [S[5, allvals[5][sixtuples[rr][5]]]];
cand6 := [S[6, allvals[6][sixtuples[rr][6]]]];
new1:={}; new2:={}; new3:={}; new4:={}; new5:={}; new6:={};
k nops(cand1) mytry := cand1[k];
{mytry[1], mytry[2], mytry[3], mytry[7], mytry[8]} = {7}
mytry[10] = 2 new1 := {mytry, op(new1)}: fi: od:
k nops(cand2) mytry := cand2[k];
{mytry[3], mytry[4], mytry[8], mytry[9], mytry[10]} = {7}
mytry[1] = 1 new2 := {mytry, op(new2)}: fi: od:
k nops(cand3) mytry := cand3[k]; flag := 0;
and(mytry[1] > 2, mytry[10] > 2)
mytry[2] = 1 mytry[3] = 2 flag := 1 fi:
mytry[3] = 1 mytry[4] = 2 flag := 1 fi:
mytry[7] = 1 mytry[8] = 2 flag := 1 fi:
mytry[8] = 1 mytry[9] = 2 flag := 1 fi:
flag = 1 new3 := {mytry, op(new3)}: fi: fi: od:
k nops(cand4) mytry := cand4[k]; flag := 0;
and(mytry[1] > 2, mytry[10] > 2)
mytry[2] = 1 mytry[3] = 2 flag := 1 fi:
mytry[3] = 1 mytry[4] = 2 flag := 1 fi:
mytry[7] = 1 mytry[8] = 2 flag := 1 fi:
mytry[8] = 1 mytry[9] = 2 flag := 1 fi:
flag = 1 new4 := {mytry, op(new4)}: fi: fi: od:
284

fiGenerating Approximate Solutions TTP using Linear Distance Relaxation

k nops(cand5) mytry := cand5[k]; flag := 0;
and(mytry[1] > 2, mytry[10] > 2)
mytry[2] = 1 mytry[3] = 2 flag := 1 fi:
mytry[3] = 1 mytry[4] = 2 flag := 1 fi:
mytry[7] = 1 mytry[8] = 2 flag := 1 fi:
mytry[8] = 1 mytry[9] = 2 flag := 1 fi:
flag = 1 new5 := {mytry, op(new5)}: fi: fi: od:
k nops(cand6) mytry := cand6[k]; flag := 0;
and(mytry[1] > 2, mytry[10] > 2)
mytry[2] = 1 mytry[3] = 2 flag := 1 fi:
mytry[3] = 1 mytry[4] = 2 flag := 1 fi:
mytry[7] = 1 mytry[8] = 2 flag := 1 fi:
mytry[8] = 1 mytry[9] = 2 flag := 1 fi:
flag = 1 new6 := {mytry, op(new6)}: fi: fi: od:
x 24
aval := my24[x][1]; bval := my24[x][2];
cval := my24[x][3]; dval := my24[x][4];
pairs12 := firsttwo(aval, bval, cval, dval,new1,new2);
sixpos := getpos(aval, bval, cval, dval)[4];
trips126 := firstthree(pairs12, sixpos, new6);
nops(trips126)
sols := {op(sols),op(allsix(trips126[y],
aval,bval,cval,dval,new3,new4,new5))}:
od:
od:
od:
sols: end:

S4cases := []:
pp 8 qq 8
xx := triplet1[[8, 10, 2*pp, 2*qq, 6]];
yy := triplet2[[6, 6, 20-2*pp, 22-2*qq, 8]];
u xx v yy S4cases := [op(S4cases), [op(u), op(v)]]:
od: od: od: od:
SolutionsForS4 := generatesolutions(S4cases):
S3cases := []:
pp 2 8 qq 8
xx := triplet1[[8, 10, 2*pp, 2*qq, 6]];
yy := triplet2[[6, 6, 22-2*pp, 18-2*qq, 8]];
u xx v yy S3cases := [op(S3cases), [op(u), op(v)]]:
od: od: od: od:
SolutionsForS3 := generatesolutions(S3cases):

285

fiHoshino & Kawarabayashi

S2cases := []:
pp 9 qq 9
xx := triplet1[[10, 10, 2*pp, 2*qq, 6]];
yy := triplet2[[6, 6, 20-2*pp, 18-2*qq, 8]];
u xx v yy S2cases := [op(S2cases), [op(u), op(v)]]:
od: od: od: od:
SolutionsForS2 := checkallsolutions(S2cases):
S1cases := []:
pp 8 qq 8 rr 8
xx := triplet1[[8, 2*pp, 2*qq, 2*rr, 6]]:
yy := triplet2[[6, 18-2*pp, 20-2*qq, 18-2*rr, 8]]:
u xx v yy S1cases := [op(S1cases), [op(u), op(v)]]:
od: od: od: od: od:
SolutionsForS1 := checkallsolutions(S1cases):

References
Easton, K., Nemhauser, G., & Trick, M. (2001). traveling tournament problem: description benchmarks. Proceedings 7th International Conference Principles
Practice Constraint Programming, 580584.
Fujiwara, N., Imahori, S., Matsui, T., & Miyashiro, R. (2007). Constructive algorithms
constant distance traveling tournament problem. Lecture Notes Computer
Science, 3867, 135146.
Hoshino, R., & Kawarabayashi, K. (2011). Scheduling bipartite tournaments minimize
total travel distance. Journal Artificial Intelligence Research, 42, 91124.
Hoshino, R., & Kawarabayashi, K. (2012). linear distance traveling tournament problem. Proceedings 26th AAAI Conference Artificial Intelligence, 17701778.
Irnich, S. (2010). new branch-and-price algorithm traveling tournament problem.
European Journal Operational Research, 204, 218228.
Kendall, G., Knust, S., Ribeiro, C., & Urrutia, S. (2010). Scheduling sports: annotated
bibliography. Computers Operations Research, 37, 119.
Thielen, C., & Westphal, S. (2010). Complexity traveling tournament problem.
Theoretical Computer Science, 412, 345351.
Trick, M. (2012). Challenge traveling tournament problems.. [Online; accessed 9-June-2012].
Yamaguchi, D., Imahori, S., Miyashiro, R., & Matsui, T. (2011). improved approximation algorithm traveling tournament problem. Annals Operations Research,
61(4), 10771091.

286

fiJournal Artificial Intelligence Research 45 (2012) 515-564

Submitted 8/12; published 12/12

Safe Exploration State Action Spaces
Reinforcement Learning
Javier Garca
Fernando Fernandez

fjgpolo@inf.uc3m.es
ffernand@inf.uc3m.es

Universidad Carlos III de Madrid,
Avenida de la Universidad 30,
28911 Leganes, Madrid, Spain

Abstract
paper, consider important problem safe exploration reinforcement
learning. reinforcement learning well-suited domains complex transition
dynamics high-dimensional state-action spaces, additional challenge posed
need safe efficient exploration. Traditional exploration techniques
particularly useful solving dangerous tasks, trial error process may lead
selection actions whose execution states may result damage
learning system (or system). Consequently, agent begins interaction
dangerous high-dimensional state-action space, important question arises;
namely, avoid (or least minimize) damage caused exploration
state-action space. introduce PI-SRL algorithm safely improves suboptimal
albeit robust behaviors continuous state action control tasks efficiently
learns experience gained environment. evaluate proposed method
four complex tasks: automatic car parking, pole-balancing, helicopter hovering,
business management.

1. Introduction
Reinforcement learning (RL) (Sutton & Barto, 1998) type machine learning whose
main goal finding policy moves agent optimally environment, generally formulated Markov Decision Process (MDP). Many RL methods used
important complex tasks (e.g., robot control see Smart & Kaelbling, 2002; Hester,
Quinlan, & Stone, 2011, stochastic games see Mannor, 2004; Konen & Bartz-Beielstein,
2009 control optimization complex dynamical systems see Salkham, Cunningham,
Garg, & Cahill, 2008). RL tasks focused maximizing long-term cumulative reward, RL researchers paying increasing attention long-term
reward maximization, also safety approaches Sequential Decision Problems
(SDPs) (Mihatsch & Neuneier, 2002; Hans, Schneegass, Schafer, & Udluft, 2008; Martn H.
& Lope, 2009; Koppejan & Whiteson, 2011). Well-written reviews matters also
found (Geibel & Wysotzki, 2005; Defourny, Ernst, & Wehenkel, 2008). Nevertheless,
important ensure reasonable system performance consider safety
agent (e.g., avoiding collisions, crashes, etc.) application RL dangerous
tasks, exploration techniques RL offer guarantees issues. Thus,
using RL techniques dangerous control tasks, important question arises; namely,
ensure exploration state-action space cause damage injury
c
2012
AI Access Foundation. rights reserved.

fiGarca & Fernandez

while, time, learning (near-)optimal policies? matter, words,
one ensuring agent able explore dangerous environment safely
efficiently. many domains exploration/exploitation process may lead
catastrophic states actions learning agent (Geibel & Wysotzki, 2005).
helicopter hovering control task one case involving high risk, since policies
crash helicopter, incurring catastrophic negative reward. Exploration/exploitation
strategies greedy may even result constant helicopter crashes (especially
high probability random action selection). Another example found
portfolio theory analysts expected find portfolio maximizes profit
avoiding risks considerable losses (Luenberger, 1998). Since maximization expected
returns necessarily prevent rare occurrences large negative outcomes, different
criteria safe exploration needed. exploration process new policies
evaluated must conducted extreme care. Indeed, environments, method
required explores state-action space, safe manner.
paper, propose Policy Improvement Safe Reinforcement Learning
(PI-SRL) algorithm safe exploration dangerous continuous control tasks.
method requires predefined (and safe) baseline policy assumed suboptimal
(otherwise, learning would pointless). Predefined baseline policies used
different ways approaches. work Koppejan Whiteson (2011), singlelayers perceptrons evolved, albeit starting prototype network whose weights correspond baseline policy provided helicopter control task competition software (Abbeel,
Coates, Hunter, & Ng, 2008). approach viewed simple form population seeding proven advantageous numerous evolutionary methods
(e.g. see Hernandez-Daz, Coello, Perez, Caballero, Luque, & Santana-Quintero, 2008; Poli
& Cagnoni, 1997). work Martn de Lope (2009), weights neural networks also evolved inserting several baseline policies (including provided
helicopter control task competition software) initial population. minimize
possibility evaluating unsafe policies, approach prevents crossover mutation
operators permitting anything tiny changes initial baseline policies.
paper, present PI-SRL algorithm, novel approach improving baseline
policies dangerous domains using RL. PI-SRL algorithm composed two different steps. first, baseline behavior (robust albeit suboptimal) approximated
using behavioral cloning techniques (Anderson, Draper, & Peterson, 2000; Abbott, 2008).
order achieve goal, case-based reasoning (CBR) techniques (Aamodt & Plaza,
1994; Bartsch-Sprl, Lenz, & Hbner, 1999) used successfully applied
imitation tasks past (Floyd & Esfandiari, 2010; Floyd, Esfandiari, & Lam, 2008).
second step, PI-SRL algorithm attempts safely explore state-action space
order build accurate policy previously-learned behavior. Thus, set
cases (i.e., state-action pairs) obtained previous phase improved
safe exploration state-action space. perform exploration, small amounts
Gaussian noise randomly added greedy actions baseline policy approach.
exploration strategy used successfully previous works (Argall, Chernova,
Veloso, & Browning, 2009; Van Hasselt & Wiering, 2007).
novelty present study use two new, main components: (i) risk
function determine degree risk particular state (ii) baseline behavior
516

fiSafe Exploration State Action Spaces Reinforcement Learning

capable producing safe actions supposedly risky states (i.e., states lead
damage injury). addition, present new definition risk based
agent unknown known space. described Section 5 greater detail,
new definition completely different traditional definitions risk found literature (Geibel, 2001; Mihatsch & Neuneier, 2002; Geibel & Wysotzki, 2005). paper also
reports experimental results obtained application new approach four
different domains: (i) automatic car parking (Lee & Lee, 2008), (ii) pole-balancing (Sutton
& Barto, 1998), (iii) 2009 RL Competition helicopter hovering (Ng, Kim, Jordan, & Sastry,
2003) (iv) business management (Borrajo, Bueno, de Pablo, Santos, Fernandez, Garca,
& Sagredo, 2010). domain, propose learning near-optimal policy which,
learning phase, minimize car crashes, pole disequilibrium, helicopter crashes
company bankruptcies, respectively. important note comparison
approach agent optimal exploration policy possible since,
proposed domains (each high-dimensional continuous state action space,
well complex stochastic dynamics), know optimal exploration policy
is.
Regarding organization remainder paper, Section 2 introduces key
definitions, Section 3 describes detail learning approach proposed. Section 4,
evaluation performed four mentioned domains presented. Section 5
discusses related work Section 6 summarizes main conclusions study.
sections,
term return used refer expected cumulative future discounted
P
reward R = t=0 rt , term reward used refer single real value used
evaluate selection action particular state denoted r.

2. Definitions
illustrate concept safety used approach, navigation problem presented
Figure 1. navigation problem presented Figure 1, control policy must
learned get particular start state goal state, given set demonstration
trajectories. environment, assume task difficult due stochastic
complex dynamic environment (e.g., extremely irregular surface case
robot navigation domain wind effects case helicopter hover task).
stochasticity makes impossible complete task using exactly trajectory
every time. Additionally, problem supposes set demonstrations baseline
controller performing task (the continuous black lines) also given. set
demonstrations composed different trajectories covering well-defined region
state space (the region within rectangle).
approach based addition small amounts Gaussian noise perturbations baseline trajectories order find new better ways completing
task. noise affect baseline trajectories different ways, depending
amount noise added which, turn, depends amount risk taken. risk
desired, noise added baseline trajectories 0 and, consequently, new
improved behavior discovered (nevertheless, robot never fall cliff
helicopter never crash). If, however, intermediate level risk desired,
small amounts noise added baseline trajectories new trajectories (the
517

fiGarca & Fernandez

Figure 1: Exploration strategy based addition small amounts noise baseline
policy behavior. Continuous lines represent baseline behavior, newly
explored behaviors indicated dotted dashed lines.
dotted blue lines) complete task discovered. cases, exploration new
trajectories leads robot unknown regions state space (the dashed red lines).
robot assumed able detect situations risk function use
baseline behavior return safe, known states. If, instead, high risk desired,
large amounts noise added baseline trajectories, leading discovery
new trajectories (but also higher probability robot gets damaged).
iteration process leads robot progressively safely explore state
action spaces order find new improved ways complete task. degree
safety exploration, however, depend risk taken.
2.1 Error Non-Error States
paper, follow far notation presented Geibel et al. (2005)
definition concept risk. study, Geibel et al. associate risk error
states non-error states, former understood state considered
undesirable dangerous enter.
Definition 1 Error non-error states. Let set states set
error states. state undesirable terminal state control
agent ends reached damage injury agent, learning system
external entities. set considered set non-error terminal states
= control agent ends normally without damage injury.
terms RL, agent enters error state, current episode ends damage
learning system (or systems); whereas enters non-error state, episode
ends normally without damage. Thus, Geibel et al. define risk respect
policy , (s), probability state sequence (si )i0 s0 = s, generated
execution policy , terminates error state s0 . definition, (s) = 1
. , (s) = 0 = . states
/ , risk
taken depends actions selected policy . definitions,
518

fiSafe Exploration State Action Spaces Reinforcement Learning

theoretical framework introduce definition risk associated
known unknown states.
2.2 Known Unknown States Continuous Action State Spaces
assume continuous, n-dimensional state space <n state = (s1 , s2 , . . . ,
sn ) vector real numbers dimension individual domain Dis <.
Similarly, assume continuous m-dimensional action space <m
action = (a1 , a2 , . . . , ) vector real numbers dimension
individual domain Dia <. Additionally, agent considered endowed
memory, case-base B, size . memory element represents state-action pair,
case, agent experienced before.
Definition 2 (Case-base). case-base set cases B = {c1 . . . , c }. Every case
ci consists state-action pair (si , ai ) agent experienced past
associated value V (si ). Thus, ci =< si , ai , V (si ) >, first element represents
cases problem part corresponds state si , following element ai depicts case
solution (i.e., action expected agent state si ) final element
V (si ) value function associated state si . state si composed n
continuous state variables action ai composed continuous action variables.
agent receives new state sq , first retrieves nearest neighbor sq
B according given similarity metric performs associated action.
paper, use Euclidean distance similarity metric (Equation 1).
v
uX
u n
d(sq , si ) = (sq,j si,j )2

(1)

j=0

Euclidean distance metric useful value function expected continuous smooth throughout state space (Santamara, Sutton, & Ram, 1998). However,
since value function unknown priori Euclidean distance metric particularly suitable many problems, many researchers begun ask distance
metric learn adapt order achieve better results (Taylor, Kulis, & Sha,
2011). use distance metric learning techniques would certainly desirable
order induce powerful distance metric specific domain, consideration
lies outside scope present study. paper, therefore, focused
domains Euclidean distance proven successful (i.e., successfully applied car parking (Cichosz, 1995), pole-balancing (Martin H & de Lope, 2009),
helicopter hovering control (Martin H & de Lope, 2009) SIMBA (Borrajo et al., 2010).
Traditionally, case-based approaches use density threshold order determine
new case added memory. distance nearest neighbor
sq greater , new case added. sense, parameter defines size
classification region case B (Figure 2). new case sq within
classification region case ci , considered known state. Hence, cases

associated value function V B
B describe case-based policy agent B
.
519

fiGarca & Fernandez

Figure 2: Known Unknown states.
Definition 3 (Known/Unknown states). Given case-base B = {c1 . . . , c } composed
cases ci = (si , ai , V (si )) density threshold , state sq considered known
min1i d(sq , si ) unknown cases. Formally, set known
states, set unknown states = = S.
Definition 3, states identified known unknown. agent
receives new state , performs action ai case ci d(s, si ) =
min1j d(s, sj ). However, agent receives state where, definition,
distance state B larger , case retrieved. Consequently, action
performed state unknown agent.
Definition 4 (Case-Based risk function). Given case base B = {c1 . . . , c } composed
cases ci = (si , ai , V (si )), risk state defined Equation 2.

B

%


(s) =

0
1

min1j d(s, sj ) <
otherwise

(2)



Thus, %B (s) = 1 holds (i.e., unknown), state
associated case and, hence, action performed given situation

unknown. , %B (s) = 0.
derived caseDefinition 5 (Safe case-based policy). case-based policy B
base B = {c1 . . . . , c } safe when, initial known state s0 respect B,
always produces known non-error states respect B.
execution B






B
s0 | %B (s0 ) = 0, (si )i>0
%B (si ) = 0

(3)

Additionally, assumed probability state sequence (si )i0
, terminates error state
known state s0 , generated executing policy B

B (s0 ) = 0 (i.e., = ).
520

fiSafe Exploration State Action Spaces Reinforcement Learning

Definition 6 (Safe case-based coverage). coverage single state respect
safe case-base B = {c1 . . . . , c } defined state si min1i d(s, si ) .
Therefore, assume safe case-based provide actions entire state
space, rather known states .
Figure 3 graphically represents relationship known/unknown error/non learnt,
error states. green area image denotes safe case-based policy B
area state space corresponding initial known space. agent following
always green area resulting episodes end without
policy B
damages. Consequently, subset non-error states also form part known space.
Formally, let subsets non-error states belonging known unknown
spaces, respectively, = . . yellow area Figure,
contrast, represents unknown space . space found error states,
well subset remaining non-error states. Formally, .
Understood way, PI-SRL algorithm summed follows:
.
first step, learn known space (green area) safe case-based policy B

second step, adjust known space (green area) unknown space (yellow
area) order explore new improved behaviors avoiding error states (red
area). process adjusting known space space used safe
better policies, algorithm forget ineffectual known states, shown
Section 4.

Figure 3: Known/unknown error/non-error states given Case Base B.

2.3 Advantages Using Prior Knowledge Predetermined
Exploration Policies
present subsection, advantages using teacher knowledge RL, namely (i)
provide initial knowledge task learned (ii) support exploration
process, highlighted. Furthermore, explain believe knowledge
521

fiGarca & Fernandez

indispensable RL tackling highly complex realistic problems large, continuous
state action spaces particular action may result undesirable
consequence.
2.3.1 Providing Initial Knowledge Task
RL algorithms begin learning without previous knowledge task
learnt. cases, exploration strategies greedy used. application
strategy results random exploration state action spaces gather
knowledge task. enough information discovered environment algorithms behavior improve. random exploration policies, however,
waste significant amount time exploring irrelevant regions state action
spaces optimal policy never encountered. problem compounded
domains extremely large continuous state action spaces random
exploration never likely visit regions spaces necessary learn (near-)optimal
policies. Additionally, many real RL tasks real robots, random exploration
gather information environment cannot even applied. real robots,
considered sufficient information much information real robot
gather environment. Finally, impossible avoid undesirable situations
high-risk environments without certain amount prior knowledge task,
use random exploration would require undesirable state visited
labeled undesirable. However, visits undesirable states may result damage
injury agent, learning system external entities. Consequently, visits
states avoided earliest steps learning process.
Mitigating difficulties described above, finite sets teacher-provided examples
demonstrations used incorporate prior knowledge learning algorithm.
teacher knowledge used two general ways, either (i) bootstrap learning algorithm (i.e., sort initialization procedure) (ii) derive policy
examples. first case, learning algorithm provided examples demonstrations bootstrap value function approximation lead agent
relevant regions space. second way teacher knowledge
used refers Learning Demonstration (LfD) approaches policy derived finite set demonstrations provided teacher. principal drawback
approach, however, performance derived policy heavily limited
teacher ability. one way circumvent difficulty improve performance
exploring beyond provided teacher demonstrations, raises
question agent act encounters state demonstration
exists (an unknown state).
2.3.2 Supporting Exploration Process
furnishing agent initial knowledge helps mitigate problems associated
random exploration, alone sufficient prevent undesirable situations
arise subsequent explorations undertaken improve learner ability. additional mechanism necessary guide subsequent exploration process way
agent may kept far away catastrophic states. paper, teacher,
522

fiSafe Exploration State Action Spaces Reinforcement Learning

rather policy derived current value function approximation used
selection actions unknown states. One way prevent agent encountering
unknown states exploration process would requesting beginning
teacher demonstration every state state space. However, strategy
possible due (i) computational infeasibility given extremely large number states
state space (ii) fact teacher forced give action
every state, given many states ineffectual learning optimal policy.
Consequently, PI-SRL requests teacher action action actually required
(i.e., agent unknown state).
paper supposes teacher available task learned,
teacher taken baseline behavior. Although studies examined use
robotic teachers, hand-written control policies simulated planners, great majority
date made use human teachers. paper uses suboptimal automatic controllers
teachers, taken teachers policy.
Definition 7 (Baseline behavior). Policy considered baseline behavior
three assumptions made: (i) able provide safe demonstrations
task learnt prior knowledge extracted; (ii) able support
subsequent exploration process, advising suboptimal actions unknown states reduce
probability entering error states return system known situation;
(iii) performance far optimal.
optimal baseline behaviors certainly ideal behave safely, non-optimal behaviors often easy (or easier) implement generate optimal ones. PI-SRL
algorithm uses baseline behavior two different ways. First, uses safe demonstrations provide prior knowledge task. step, algorithm builds

initial known space agent derived safe case-based policy B

purpose mimicking B . second step, PI-SRL uses support
subsequent exploration process conducted improve abilities previously-learnt
. exploration process continues, action requested required,
B

is, agent unknown state (Figure 4). step, acts backup
policy case unknown state intention guiding learning away
catastrophic errors or, least, reducing frequency. important note
baseline behavior cannot demonstrate correct action every possible state. However,
baseline behavior might able indicate best action cases,
action supplies should, least, safer obtained random
exploration.
2.4 Risk Parameter
order maximize exploration safety, seems advisable movement
state space arbitrary, rather known space expanded gradually
starting known state. exploration carried perturbation
. Perturbation trajectories
state-action trajectories generated policy B
accomplished addition Gaussian random noise actions B order
obtain new ways completing task. Thus, Gaussian exploration takes place
523

fiGarca & Fernandez

Figure 4: exploration process PI-SRL requests actions baseline behavior, ,
really required.
around current approximation action ai current known state sc ,
ci = (si , ai , V (si )) d(sc , si ) = min1j d(s, sj ). action performed sampled
Gaussian distribution mean action output given instance selected
B. ai denotes algorithm action output, probability selecting action a0i ,
(s, a0i ) computed using Equation 4.
(s, a0i ) =

0
2
2
1 e(ai ai ) /2
2 2

2 > 0.

(4)

shape Gaussian distribution depends parameter (standard deviation).
study, used width parameter. large values imply wide bellshaped distribution, increasing probability selecting actions a0i different
current action ai , small value implies narrow bell-shaped distribution, increasing
probability selecting actions a0i similar current action ai . 2 = 0,
assume (s, ai ) = 1. Hence, value directly related amount perturbation
. Higher values imply
added state-action trajectories generated policy B
greater perturbations (more Gaussian noise) greater probability visiting unknown
states.
Definition 8 (Risk Parameter). parameter considered risk parameter. Large
values increase probability visiting distant unknown states and, hence, increase
probability reaching error states.
exploratory actions drive agent edge known space force
go slightly beyond, unknown space, search better, safer behaviors.
period time, execution exploratory actions increases known space
. risk
improves abilities previously-learned safe case-based policy B
parameter , well , design parameters must selected user.
Section 3.3, guidelines selection offered.
important note approach proposed study based two logical
assumptions RL derived following generalization principles (Kaelbling, Littman,
& Moore, 1996; Sutton & Barto, 1998):
524

fiSafe Exploration State Action Spaces Reinforcement Learning

(i) Nearby states similar optimal actions. continuous state spaces,
impossible agent visit every state store value (or optimal action)
table. generalization techniques needed. large, smooth state spaces,
similar states expected similar values similar optimal actions. Therefore,
possible use experience gathered environment limited subset
state space produce reliable approximation much larger subset (Boyan, Moore,
& Sutton, 1995; Hu, Kostiadis, Hunter, & Kalyviotis, 2001; Fernandez & Borrajo, 2008).
One must also note that, proposed domains, optimal action also considered
safe action sense never produces error states (i.e., action considered
optimal leads agent catastrophic situation).
(ii) Similar actions similar states tend produce similar effects. Considering deterministic domain, action performed state st always produces
state st+1 . stochastic domain, understood intuitively execution
action state st produce similar effects (i.e., produces states {s1t+1 , s2t+1 , s3t+1 , . . .}
i, j 6= j dist(sit+1 , sjt+1 ) 0). Additionally, execution action a0t
state s0t st produces states {s0 1t+1 , s0 2t+1 , s0 3t+1 , . . .} i, j dist(s0 it+1 , sjt+1 ) 0.
explained earlier, present study uses Euclidean distance similarity metric,
proven successful proposed domains. result assumption,
approximation techniques used, actions generate similar effects
grouped together one action (Jiang, 2004). continuous action spaces, need
generalization techniques even greater (Kaelbling et al., 1996). paper,
assumption also allows us assume low values increase probability visiting
known states and, hence, exploring less taking less risks, greater values
increase probability reaching error states.

3. PI-SRL Algorithm
PI-SRL algorithm composed two main steps described detail below.
3.1 First Step: Modeling Baseline Behaviors CBR
first step PI-SRL approach behavioral cloning, using CBR allow software
agent behave similar manner teacher policy (baseline behavior) (Floyd et al.,
2008). Whereas LfD approaches named differently according learned (Argall et al., 2009), prevent terminological inconsistencies here, consider behavioral
cloning (also known imitation learning) area LfD whose goal reproduction/mimicking underlying teacher policy (Peters, Tedrake, Roy, & Morimoto,
2010; Abbott, 2008).
using CBR behavioral cloning, case built using agents state
received environment, well corresponding action command performed
teacher. PI-SRL, objective first step properly imitate behavior
using cases stored case-base. point, important question arises; namely,
case-base B learnt using sample trajectories provided that,
end learning process, resulting policy derived B mimics behavior
? Baseline behavior function maps states actions : or,
525

fiGarca & Fernandez

words, function that, given state si S, provides corresponding action ai A.
paper, want build policy B derived case-base composed cases (sj , aj )
that, new state sq , case minimum Euclidean distance dist(sq , sj )
retrieved corresponding action aj returned. Intuitively, assumed
B built simply storing cases (si , ai ) gathered one interaction
environment limited number episodes K. end K episodes,
one expects resulting B able properly mimic behavior . However,
informal experimentation helicopter hovering domain shows case
(Section 4.3). helicopter hovering, K = 100 episodes prohibitive number
600,000 cases stored, policy derived case-base B unable correctly
imitate baseline behavior and, instead, continuously crashes helicopter. Indeed,
order B mimic large continuous stochastic domains, approach
requires larger number episodes and, consequently, prohibitive number cases.
fact, perfectly mimic domains, infinite number cases would required.
Figure 5 attempts explain believe learning process work.
it, region space represented simply storing cases derived form
c = (s, a) shown. stored case (red circles) covers area space represents
centroid Voronoi region.

Figure 5: Effects storing training cases.
previously-learned policy B used new state sq presented, action aj performed, corresponding case cj = (sj , aj ) Euclidean distance
dist(sq , sj ) less stored cases. However, use policy
provide action situation sq , action ai provided different aj .
point, policy B said classify state sq obtained class aj ,
policy said classify state sq desired class ai (insofar
policy mimicked), |ai aj | > 0. Furthermore, |ai aj | understood
classification error. case-base stored possible pairs (si , ai )
able generate domain, actions aj ai would always identical,
dist(sq , sj ) = 0 |ai aj | = 0. However, stochastic large, continuous domain,
impossible store cases. sum classification errors episode
526

fiSafe Exploration State Action Spaces Reinforcement Learning

leads visiting unexplored regions case space (i.e., regions new
state sq received environment Euclidean distance dist(sq , sj ) >>
respect closest case cj = (sj , aj ) B). unexplored regions visited,
difference obtained class derived B desired class derived
large (i.e., |ai aj | >> 0) probability error states might visited
greatly increases.
may concluded, therefore, simply storing pairs c = (s, a) generated
sufficient properly mimic behavior. reason, algorithm Figure 6
proposed.
CBR Approach Behavioral Cloning
00
01
02
03

Given
Given
Given
1. Set

04
05
06
07

2. Repeat
Set k = 0
k < maxEpisodeLength
Compute case < sc , ac , 0 > closest current state sk

08
09
10
11
12
13
14
15
16
17
18
19
20






baseline behavior
density threshold
maximum number cases
case-base B =



%B (sk ) = 0 // equation 2
Set ak = ac
else
Set ak using baseline behavior
Create new case cnew = (sk , ak , 0)
B := B cnew
Execute ak , receive sk+1
Set k = k + 1
end
kBk >
Remove kBk least-frequently-used cases B
stop criterion becomes true

3. Return B performing safe case-based policy B

Figure 6: CBR algorithm behavioral cloning.


first step algorithm, state-value function V B (si ) initialized 0 (see

line 07). value V B (si ) case computed second step algorithm
Section 3.2. Additionally, step uses case-based risk function (Equation 2)
determine whether new state sk considered risky (line 08). new state
risky (i.e., known state sk ), 1-nearest neighbor strategy followed (line
09). Otherwise, algorithm performs action ak using baseline behavior
new case cnew = (sk , ak , 0) built added case-base B (line 13). Starting
empty case-base, learning algorithm continuously increases competence storing
new experiences. However, number reasons inflow new cases
limited. Large case-bases increase time required find closest cases new
example. may partially solved using techniques reduce retrieval time
(e.g., k-d trees used work), nevertheless reduce storage
527

fiGarca & Fernandez

requirements. Several approaches removal ineffectual cases training exist,
including Ahas IBx algorithms (Aha, 1992) nearest prototype approach (Fernandez
& Isasi, 2008). number cases stored B exceeds critical value kBk >
realization retrieval within certain amount time cannot guaranteed,
removal cases inevitable. efficient approach problem
removal least-frequently-used elements B (line 18).
result step constrained case-base B describing safe case-based policy

B mimics baseline behavior , though perhaps deviation (line 20).
Formally, let U (T ) estimate utility baseline behavior computed
) U ( ).
averaging sum rewards accumulated NT trials. Then, U (B

3.2 Second Step: Improving Learned Baseline Behavior
learned previous
step PI-SRL algorithm, safe case-based policy B
step improved safe exploration state-action space. First, case ci

B, state-value function V B (si ) computed following Monte Carlo (MC) approach
(Figure 7).
MC Algorithm Adapted CBR
00
01
02
03

Given case-base B
1. Initialize, ci B
V (s) arbitrary
Returns(s) empty list

04
05
06
07
08
09

2. k < maxN umberEpisodes

Generate episode using B
appearing episode < s, a, V (s) > B
R return following first occurrence
Append R Returns(s)
V(s) average(Returns(s))

10
11

Set k = k + 1
3. Return B

Figure 7: Monte Carlo algorithm computation state-value function case.
algorithm similar spirit first-visit MC method V (Sutton & Barto,
1998), adapted paper work policy given case-base. algorithm
shown Figure 7, returns state si B accumulated averaged, following
derived case base B (see line 09). important note
policy B
algorithm term return following first occurrence refers expected return
(i.e., expected cumulative future discounted reward starting state), whereas
Returns refers list composed return different episodes. One
principal reasons using MC method allows us quickly easily estimate

state values V B (si ) case ci B. addition, MC methods shown
successful wide variety domains (Sutton & Barto, 1998). state-value

function V B (si ) computed case ci B, small amounts Gaussian noise
order obtain new improved ways
randomly added actions policy B
528

fiSafe Exploration State Action Spaces Reinforcement Learning

complete task. algorithm used improve baseline behavior learned
previous step depicted Figure 8. algorithm composed four steps performed
episode.
- (a) Initialization step. algorithm initializes list used store cases occurring
episode sets cumulative reward counter episode 0.
- (b) Case Generation. algorithm builds case step episode.
new state sk , closest case < s, a, V (s) > B computed using Euclidean
distance metric Equation 1 (see line 09 algorithm Figure 8). order determine
perceived degree risk new state sk , case-based risk function used (line

10). %B (sk ) = 0, sk (known state). case, action ak performed
computed using Equation 4 new case cnew =< s, ak , V (s) > built added
list cases occurred episode (line 13). important note
new case < s, ak , V (s) > built replacing action corresponding closest case
< s, a, V (s) > B, new action ak resulting application random
Gaussian noise Equation 4. Thus, algorithm produces smooth changes

cases B ak a. If, however, %B (sk ) = 1, state sk (i.e., unknown
state [line 14]). unknown states, action ak performed suggested baseline
behavior defines safe behavior (line 15). new case < sk , ak , 0 > built
added list cases episode actions performed using
agent known state. Finally, reward obtained episode accumulated,
r(sk , ak ) immediate reward obtained action ak performed state sk
(line 18).
- (c) Computing state-value function unknown states. step,
state-value function states considered unknown previous step
computed. previous step (line 17), state-value function states set
0. algorithm proceeds manner similar first-visit MC algorithm Figure 7.
case, return unknown state si computed, averaged since
one episode considered (line 24 25). return si computed, taking
account first visit state si episode (each occurrence state episode
called visit si ), although state si could appear multiple times rest
episode.
- (d) Updating cases B using experience gathered. Updates B
made cases gathered episodes cumulative reward similar
best episode found point using threshold (line 27). way, good sequences
provided updates since shown sequences experiences
cause adaptive agent converge stable useful policy, whereas bad sequences may
cause agent converge unstable bad policy (Wyatt, 1997). also prevents
degradation initial performance B computed first step algorithm
use bad episodes, episodes errors, updates. step, two types
updates appear, namely, replacements additions new cases. Again, algorithm
iterates case ci = (si , ai , V (si )) listCasesEpisode (line 29). si known state
(line 30), compute case < si , a, V (si ) > B corresponding state si (line 31).
One note case ci = (si , ai , V (si )) listCasesEpisode built line 13
algorithm, replacing action corresponding case < si , a, V (si ) > B
new action ai resulting application random Gaussian noise action
529

fiGarca & Fernandez

Policy Improvement Algorithm
00
01
02
03
04
05
06
07
08
09

Given case-base B, maximum number cases
Given baseline behavior
Given update threshold
1. Set maxT otalRwEpisode = 0, maximum cumulative reward reached episode
2. Repeat
(a) Initialization step:
set k = 0, listCasesEpisode , totalRwEpisode = 0
(b) Case generation:
k < maxEpisodeLength
Compute case < s, a, V (s) > B closest current state sk


10
11
12
13

%B (sk ) = 0 // known state
Chose action ak using equation 4
Perform action ak
Create new instance cnew := (s, ak , V (s))

14
15
16
17
18
19

else // unknown state
Chose action ak using
Perform action ak
Create new instance cnew := (sk , ak , 0)
totalRwEpisode := totalRwEpisode + r(sk , ak )
listCasesEpisode := listCasesEpisode cnew

20
21
22
23
24
25
26
27
28
29

Set k = k + 1
(c) Computing state-value function unknown states:
instance ci listCasesEpisode


%B (si ) = 1 // unknown state
P
return(si ) := kj=n jn r(sj , aj ) // n first ocurrence si episode
V (si ) := return(si )
(d) Updating cases B using experience gathered :
totalRwEpisode > (maxT otalRwEpisode )
maxT otalRwEpisode := max (maxT otalRwEpisode, totalRwEpisode)
case ci =< si , ai , V (si ) > listCasesEpisode


30
31
32

%B (si ) = 0 // known state
Compute case < si , a, V (si ) > B corresponding state si
Compute = r(si , ai ) + V (si+1 ) V (si )

33
34
35
36
37

> 0
Replace case < si , a, V (si ) > B case < si , ai , V (si ) > listCasesEpisode
V (si ) = V (si ) +
else // unknown state
B := B ci

38
39
40
41

kBk >
Remove kBk least-frequently-used cases B
stop criterion becomes true
3. Return B

Figure 8: Description step two PI-SRL algorithm.

Equation 4. Then, temporal distance (TD) error computed (line 32). > 0,
performing action ai results positive change value state. action,
530

fiSafe Exploration State Action Spaces Reinforcement Learning

turn, could potentially lead higher return and, thus, better policy. Van Hasselt
Wiering (2007) also update value function using actions potentially lead
higher return. TD error positive, ai considered good selection
reinforced. algorithm, reinforcement carried updating output
case < si , a, V (si ) > B ai (line 34). Therefore, update case-base occurs
TD error positive. similar linear reward-inaction update learning
automata (Narendra & Thathachar, 1974, 1989) sign TD error used
measure success. PI-SRL updates case-base actual improvements
observed, thus avoiding slow learning plateaus value space
TD errors small. shown empirically procedure result
better policies step size depends size TD error (Van Hasselt &
Wiering, 2007). important note replacements produce smooth changes
case-base B since action replaced ai results higher V (si ) ai a.
form updating understood risk-seeking approach, overweighting
transitions successor states promise above-average return (Mihatsch & Neuneier,
2002). Additionally, prevents degradation B, ensuring replacements made
action potentially lead higher V (si ).
If, instead, si known state, case ci added B (line 37). Finally,
algorithm removes cases B necessary (line 39). Complex scoring metrics calculate
cases removed given moment proposed several authors.
Forbes Andres (2002) suggest removal cases contribute least overall
approximation, Driessens Ramon (2003) pursue error-oriented view
propose deletion cases contribute prediction error examples.
principal drawback sophisticated measures complexity.
determination case(s) removed involves computation score value
ci B, turn requires least one retrieval regression, respectively,
cj B (j 6= i). entire repeated sweeps case-base entail enormous
computational load. Gabel Riedmiller (2005) compute different score metric
ci B, requiring computation set k-nearest neighbors around ci .
approaches well-suited systems learning adjusted time requirements
high-dimensional state space, requiring use larger case-bases
proposed here. Rather, paper, propose removal least-frequently-used
cases. idea seems intuitive insofar least-frequently-used cases usually contain
worse estimates corresponding states value; although strategy might lead
function approximator forgets valuable experience made past
(e.g., corner cases). Despite this, PI-SRL performs successfully domains proposed
using strategy, demonstrated Section 4. Thus, ability forget ineffectual
known states described Section 2 result algorithm removing kBk cases
least-frequently-used cases B.
3.3 Parameter Setting Design
One main difficulties applying PI-SRL algorithm given problem
decide appropriate set parameter values threshold , risk parameter ,
update threshold maximum number cases . incorrect value
531

fiGarca & Fernandez

parameter lead mislabeling state known really unknown, potentially
leading damage injury agent. case risk parameter , high values
continuously result damage injury; low values safe, allow
exploration state-action space sufficient reaching near-optimal policy. Unlike
, parameter related risk, instead directly related
performance algorithm. Parameter used determine good episode
must respect best episode obtained, since best episodes used
update case-base B. value large, bad episodes may used update B
(influencing convergence performance algorithm). If, instead, low,
number updates B may insufficient improving baseline behavior. Finally,
high value allows large case-bases, increasing computational effort
retrieval degrading efficiency system. contrast, low value might
excessively restrict size case-base thus negatively affect final performance
algorithm. subsection, solid perspective given automatic definition
parameters. parameter setting proposed taken suitable set
heuristics tested successfully wide variety domains (Section 4).
Parameter : parameter domain-dependent related average size
actions. paper, value parameter established
computing mean distance states execution baseline
behavior . Expressed another way, execution policy provides
state-action sequence form s1 a1 s2 a2 . . . sn . Thus, value
computed using Equation 5.

=

dist(s1 , s2 ) + . . . + dist(sn1 , sn )
n1

(5)

Parameter : Several authors agree impossible completely avoid
accidents (Moldovan & Abbeel, 2012; Geibel & Wysotzki, 2005). important
note PI-SRL completely safe first step algorithm executed.
However, proceeding way, performance algorithm heavily
limited abilities baseline behavior. running subsequent
exploratory process inevitable learner performance improved beyond
baseline behavior. Since agent operates state incomplete knowledge
domain dynamic, inevitable exploratory process
unknown regions state space visited agent may reach error
state. However, possible adjust risk parameter determine level
risk assumed exploratory process. paper, start low
values (low risk) gradually increase. Specifically, propose beginning
= 9 107 increasing value iteratively either accurate policy
obtained amount damage injury high.
Parameter : value parameter set relative best episode obtained.
paper, value set 5% cumulative reward best episode
obtained.
532

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 9: Trajectories generated baseline policy deterministic, slightly
stochastic highly stochastic domain.
Parameter : Previously, estimated maximum number cases stored
case-base estimated maximum number cases required properly mimic baseline behavior . follows description value
computed. Figure 9 presents trajectories (sequences states) followed baseline policy three different domains: deterministic, slightly stochastic highly
stochastic. domain, different sequences states produced
represented {s00 , s01 , s02 , . . . , s0n }, {s00 , s11 , s12 , . . . , s1n },. . ., {s00 , sm1 , sm2 , . . . , smn },
sji i-th state, s00 initial state sjn final state resulting
trajectory episode j. deterministic domain, different executions
always result trajectory. case, set maximum number
cases = n cases computed episode stored.
slightly stochastic domain, trajectories produced different episodes
different, slightly so. Here, suppose case-base beginning
empty. Additionally, assume states {s00 , s01 , s02 , . . . , s0n } corresponding first trajectory produced domain stored case-base.
Furthermore, domain execute different episodes, obtaining different
trajectories. Following execution episodes, compute maximum distance i-th state first trajectory (previously added case-base)
i-th state produced trajectory j max1jm d(s0i , sji ).
slightly stochastic domain, maximum distance exceed threshold
case max1jm d(s0i , sji ) < . point, assume i-th state
trajectory j least one neighbor distance less (corresponding
state s0i ). Thus, i-th state j added case-base.
contrast, highly stochastic domain, maximum distance greatly exceeds
threshold cases max1jm d(s0i , sji ) >> . domain,
estimate total number cases added case-base following
533

fiGarca & Fernandez

way. i-th state sequence
j first trajectory,k estimate number
max1jm d(s0i ,sji )
cases added case-base
or, words,

compute number intervals range [0, max1jm d(s0i , sji )] width
(the threshold used decide whether new case added casebase). Consequently, estimated number cases addedjto case-base, taking

k
Pn
max1jm d(s0i ,sji )
account states sequence, computed i=0
. Finally,

estimated maximum number cases computed shown Equation 6.

=n+


n
X
max1jm d(s0i , sji )


i=0

!
(6)

important remember deterministic domain, summation equation 6 equal 0 that, therefore, = n. increase value element
related increase stochasticity environment, insofar greater
stochasticity environment increases number cases required. Finally,
number cases large nearly infinite, threshold increased
make restrictive addition new cases case-base. However,
increase may also adversely affect final performance algorithm.

4. Experimental Results
section presents experimental results collected use PI-SRL policy
learning four different domains presented order increasing complexity (i.e., increasing number variables describing states actions): car parking problem (Lee &
Lee, 2008), pole-balancing (Sutton & Barto, 1998), helicopter hovering (Ng et al., 2003)
business simulator SIMBA (Borrajo et al., 2010). domains,
proposed learning near-optimal policy minimizes car accidents,
pole disequilibrium, helicopter crashes company bankruptcies, respectively,
learning phase. four domains stochastic experimentation.
helicopter hovering business simulator SIMBA are, themselves, stochastic and,
additionally, generalized domains, made car parking pole-balancing domains stochastic intentional addition random Gaussian noise actions
reward function. results PI-SRL four domains compared yielded
two additional techniques, namely, evolutionary RL approach selected winner
helicopter domain 2009 RL Competition (Martn H. & Lope, 2009) Geibel
Wysotzkis risk-sensitive RL approach (Geibel & Wysotzki, 2005). evolutionary approach, several neural networks cloning error-free teacher policies added initial
population (guaranteeing rapid convergence algorithm near-optimal policy and,
indirectly, minimizing agent damage injury). Indeed, winner helicopter
domain agent highest cumulative reward, winner must also indirectly
minimize helicopter crashes insofar incur large catastrophic negative rewards.
hand, risk-sensitive approach defines risk probability (s) reaching
terminal error state (e.g., helicopter crash ending agent control), starting initial
534

fiSafe Exploration State Action Spaces Reinforcement Learning

state s. case, new value function weighted sum risk probability,
, value function, V , used (Equation 7).
V (s) = V (s) (s)

(7)

parameter 0 determines influence V (s)-values compared (s)values. = 0, V corresponds computation minimum risk policies. large
values, original value function multiplied dominates weighted criterion.
Geibel Wysotzki (2005) consider finite (discretized) action sets study,
algorithm adapted continuous action sets. use CBR value
risk function approximation Gaussian exploration around current action.
experiments, domain, three different values used, modifying influence
V -values compared -values. cases, goal improve control
policy while, time, minimizing number episodes agent damage
injury. domain, establish different risk levels modifying risk parameter
values according procedure described subsection 3.3. important note
one baseline behavior used initialize evolutionary RL approach exactly
used subsequently first second step PI-SRL. Furthermore, case-base
risk-sensitive approach begin scratch since initialized safe
. makes comparison performances fair possible,
case-based policy B
taking account different techniques make use baseline behaviors.
4.1 Car Parking Problem
car parking problem represented Figure 10 originates RL literature (Cichosz, 1996). car, represented rectangle Figure 10, initially located
inside bounded area, represented dark solid lines, referred driving area.
goal learning agent navigate car initial position garage,
car entirely inside, minimum number steps. car cannot move
outside driving area. Figure 10 (b) shows two possible paths car take
starting point garage obstacle order correctly perform
task. consider optimal policy domain reaches goal
state shortest time which, time, free failures.
state space domain described three continuous variables, namely,
coordinates center car xt yt angle cars axis
X coordinate system. car modeled essentially two control
inputs, speed v steering angle , let us suppose car controlled
steering angle (i.e., moves constant speed). Thus, action space described one
continuous variable [1, 1] corresponding turn radius, used equations
below. agent receives positive reward value r = (1 (dist(Pt , Pg ))) 10,
Pt = (xt , yt ) center car, Pg = (xg , yg ) center garage (i.e., goal
position) normalizing function scaling Euclidean distance dist(Pt , Pg )
Pt Pg range [0, 1] car inside garage (i.e., reward value greater
car parked correctly center garage). agent receives reward
-1 whenever hits wall obstacle. steps receive reward -0.1. Thus,
difficulty problem lies reinforcement delay, also fact
535

fiGarca & Fernandez

Figure 10: Car Parking Problem: (a) Model car parking problem. (b) Examples
trajectories generated agent park car garage.
punishments much frequent positive rewards (i.e., much easier hit
wall park car correctly). motion car described following
equations (Lee & Lee, 2008)
t+1 = + v /(l/2) tan( ),

(8)

xt+1 = xt + v cos(t+1 ),

(9)

yt+1 = yt + v sin(t+1 ),

(10)

v linear velocity car (assumed constant value),
maximum steering angle (i.e., car change position maximum angle
directions) simulation time step. Gaussian noise added
actions rewards standard deviation 0.1, since noisy interactions inevitable
real-world applications. Adding noise actuators environment,
transform deterministic domain stochastic domain. important note
noise added transform domain stochastic domain independent
Gaussian noise standard deviation (risk parameter) used explore state
action space second step PI-SRL algorithm. case, Gaussian noise
standard deviation used exploration added noise previously added
actuators. paper, l = 4 (m), v = 1.0 (m/s), = 0.78 (rad) = 0.5 (s)
(the driving area obstacle dimensions detailed Figure 10 [a]). initial position
car fixed xs = 4.0, ys = 4.0 = 0.26 (rad), goal position
xg = 22.5 yg = 13.5. domain, designed baseline behavior
average cumulative reward per trial 4.75.
order perform PI-SRL algorithm, modeling baseline behavior step exe learned demonstrations
cuted. result step safe case-based policy B
provided baseline behavior (see subsection 3.1). computed following
procedure described subsection 3.3 resulting values 0.01 207, respectively.
536

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 11: Car Parking Task Modeling Baseline Behavior Step: (a) Number steps per
trial executed Case Base B baseline behavior . (b) Cumulative
reward per trial baseline behavior , learned Safe Case Based Policy
IBL approach.
B
Figure 11 (a) graphically represents execution modeling baseline behavior step.
it, two different learning processes presented and, one, number steps per
trial executed baseline behavior (continuous red lines) cases B (dashed
green lines) shown. beginning learning process empty case-base B,
steps performed using baseline behavior . learning process continues,
learned. around trials
new cases added B safe case-based policy B
40-50, practically steps performed using cases B rarely used,
means safe case-based policy learned. two learning processes shown
Figure 11 (a), modeling baseline behavior step performed without collisions
wall obstacle. words, baseline behavior cloned safely without
errors. Figure 11 (b) shows cumulative reward three different execution processes:
first (continuous red lines) corresponding performance baseline behavior
, second (dashed green lines) corresponding previously-learned safe case-based
(derived B ) third (dashed blue lines) corresponding instancepolicy B
based learning (IBL) approach consisting storing cases memory. IBL approach,
new items classified examining cases stored memory determining
similar case(s) given particular similarity metric (Euclidean distance used paper). classification nearest neighbor (or nearest neighbors) taken
classification new item using 1-nearest neighbor strategy (Aha & Kibler, 1991).
approach, two different executions carried out. IBL approach, training
process performed saving training cases produced baseline behavior
50 trials (so consider approach IB1 algorithm sense saves every
case training phase, see Aha & Kibler, 1991). Figure 11 (b) shows safe
case-based policy B almost perfectly mimics behavior baseline behavior .
domain, performance IB1 approach also similar.
Figure 12 (a) shows results different risk configurations obtained improving
learned baseline behavior step. risk configuration, two different learning pro537

fiGarca & Fernandez

Figure 12: Improving learned baseline behavior step car parking problem: (a) Cumulative reward per episode different risk configurations () obtained
PI-SRL. (b) Cumulative reward per episode evolutionary RL risksensitive RL approaches. cases, episode ending failure marked.
cesses performed. trials ending failure (car hits wall obstacle) marked
(blue triangles). learning processes Figure 12 (a) demonstrate number
failures increases increase parameter . low level risk ( = 9 104 ),
although failures produced, performance nevertheless weak (around baseline behavior ) constant throughout whole learning process. Additional
experiments demonstrated increasing value = 9102 increases
number failures without improving performance. Figure 12 (b) shows results
evolutionary risk-sensitive RL approaches different values. Regarding former,
number failures higher obtained PI-SRL approach, final
performance similar. case latter, performance higher = 1.0 (value
maximization), yet agent consistently crashes car wall.
Figure 13 shows mean number failures (i.e., car collisions) cumulative reward
approach 500 trials red circles corresponding PI-SRL algorithm,
black triangles risk-sensitive approach blue square evolutionary
RL approach. Additionally, Figure 13 shows two asymptotes. horizontal asymptote
established according cumulative reward obtained highest value.
horizontal asymptote indicates higher values increase number failures without
improving cumulative reward (which may, fact, get worse). vertical asymptote
F ailures = 0 indicates reducing risk parameter reduce number
failures. Figure 13 also shows performance two additional risk levels,
high level risk ( = 9 101 ) low level risk ( = 0), respect
Figure 12. using low level risk = 0, additional random Gaussian
noise added actions algorithm free failures, although performance
learned first step
improve respect safe case-based policy B
algorithm. PI-SRL medium level risk ( = 9 104 ) also free
failures, yet performance also slightly improved. PI-SRL algorithm high level
risk ( = 9 102 ) obtains highest cumulative reward, 3053.37, mean
538

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 13: Mean number failures (car collisions) cumulative reward 500 trials
approach car parking task. means computed 10
different executions.

78.8 failures. However, using high level risk ( = 9 101 ), number
failures greatly increases and, consequently, cumulative reward decreases. shown
Figure 12, PI-SRL high risk ( = 9 102 ) evolutionary RL approach obtain
similar performance, PI-SRL demonstrates faster convergence (thus, Figure 13,
cumulative reward obtained PI-SRL higher). Pareto comparison criterion
used compare solutions Figure 13. Using principle, one solution strictly
dominates (or preferred to) solution parameter strictly worse
corresponding parameter least one parameter strictly better.
written y, indicating strictly dominates y. accordance Pareto
principle, assume points Figure 13 corresponding PI-SRL solutions,
save PI-SRL high level risk, Pareto frontier, since points
strictly dominated solution (i.e., solution has, time,
higher cumulative reward lower number failures PI-SRL). domain,
solution PI-SRL medium level risk strictly dominates (or preferred to)
risk-sensitive solutions (PI-SRL = 9 103 risk-sensitive) solution PI-SRL
high level risk strictly dominates solution evolutionary RL solution
(PI-SRL = 9 102 evolutionary RL).
Nevertheless, important note ultimate decision approach
Figure 13 best depends criteria researcher. If, instance, minimization number failures deemed important optimization criterion
(independently improvement obtained respect baseline behavior ),
best approach PI-SRL low level risk ( = 9 104 ). Similarly,
maximization cumulative reward instead judged important optimization criterion (independently number failures generated), best approach
PI-SRL high level risk ( = 9 102 ).
Figure 14 shows evolution cases case-base B (known space) different
trials high-risk learning process. graph presents set known states (green
539

fiGarca & Fernandez

Figure 14: Car parking problem: Evolution known space different trials = 0
(a), = 50 (b), = 100 (c) = 200 (d) high-risk learning process
( = 9 102 ). graph corresponds situation state space
accordance case-base B trial .
area), error states (red area), unknown states (yellow area) non-error states
(orange circles). PI-SRL adapts known space order find safer better policies
complete task. Figure 14 (a) shows initial situation B (corresponding
). robust sense never results
previously-learned safe case-based policy B
collisions, suboptimal (it selects longest parking path driving around upper
side obstacle). learning process progresses (Figure 14 (b)), PI-SRL finds
shorter path park car garage along upper side obstacle (increasing
performance), comes closer obstacle (increasing probability
collisions). Figure 14 (c), PI-SRL finds new even shorter path, time along
lower side obstacle. However, still cases case-base B corresponding
older path along upper side obstacle (so Figure 14 (c) indicates two paths
park car). Finally, Figure 14 (d), cases corresponding suboptimal path
along upper side obstacle removed B replaced new cases
corresponding safe improved path along lower side obstacle.
words, PI-SRL adapts known space exploration unknown space
order find new improved behaviors. process adjusting known space
540

fiSafe Exploration State Action Spaces Reinforcement Learning

safe better policies, algorithm forgets previously-learned, yet ineffective
known states.
following experiment, becomes apparent domain noisy enough, even
taking risk (i.e., noise added actuator exploration),
agent could nevertheless perform poorly constantly produce collisions. experiment
also serves explain domain noise never sufficient efficient exploration
space without action selection noise. experiment, intentionally added
noise actuators performed second step PI-SRL again, however
time taking risk (i.e., = 0). test, added random Gaussian noise
standard deviation 0.3, rather standard deviation 0.1 used previously,
actuators. Figure 15 shows two executions second step (improving learned
baseline policy) PI-SRL algorithm x-axis indicating number trials,
y-axis cumulative reward per episode failures (i.e., collisions) marked blue
triangles. experiments Figure 12 (b), case-based policy B low level
risk ( = 9 104 ) never produces failures. contrast, experiments shown
Figure 15, case-based policy B continually collides wall although
risk parameter set 0 ( = 0). Furthermore, increase performance also
detected.

Figure 15: Improving learned baseline behavior step car parking task: Two learning processes risk configuration = 0 increase noise
actuators.

increase noise actuators second step algorithm respect
first step (the case-based policy B learned first step using Gaussian random
noise actuator standard deviation 0.1, second step performed
using Gaussian random noise actuator standard deviation 0.3) takes
agent beyond known space case-base B learnt first step PI-SRL
allows find new trajectories parking car garage. new situation,
exploration process guided follows. known state reached, agent performs
action retrieved B without addition Gaussian noise, since risk parameter
= 0 (see line 11 Figure 8 algorithm). unknown state reached, agent performs
541

fiGarca & Fernandez

action advised baseline behavior (see line 15). Using exploration
process, new better trajectory found parking car garage,
resulting cases episode corresponding unknown states added case-base
(see line 37), slightly improving performance Figure 15. important note
replacements cases (see line 34) change actions B, since
replaced action previously retrieved B plus certain amount Gaussian
noise standard deviation (see line 11). Nevertheless, given risk parameter
set 0, actions retrieved case-base replaced. exploration
process, however, = 0 (i.e., taking risk) lead optimal behavior since:
actions performed unknown situations added case-base B performed using baseline behavior supposed perform suboptimal actions
(see definition baseline behavior).
actions cases B replaced improved actions. Gaussian
noise standard deviation used explore different better actions
provided B ; however, case, risk parameter set = 0
new better actions discovered.
Additional experiments demonstrate PI-SRL behaves much worse higher
value noise used actuators (with collisions episodes). assume taking
risk (i.e., = 0) implies always performing actions discovering
newer better actions provided learned case-base B baseline
behavior . PI-SRL, replacements case-base executed towards
promising action which, case, guarantees higher return.
exploration necessary order obtain (near-)optimal behavior, since without
exploration, new better actions discovered PI-SRL performance limited
case-based policy learned first step B baseline behavior
which, one must remember, intended perform suboptimal policies.
4.2 Pole-Balancing
name suggests, objective pole-balancing problem balance pole
vertically top moving cart (Sutton & Barto, 1998). state description consists
four-dimensional vector containing angle , radial speed 0 , cart position
x speed x0 . action consists real-valued force used push cart.
study, reward computed encourage actions keep pole upright
possible cart cart centered possible track. Thus, reward
step computed rt = 1 ((t ) + (xt ))/2, normalizing functions
scaling angle position xt range [0, 1]. episode composed 10,000
steps, although may nevertheless end prematurely pole becomes unbalanced (i.e.,
inclination twelve degrees either direction) cart falls
track (i.e., 2.4m center track),
considered failures. car parking problem, Gaussian noise added actions
rewards, time standard deviation 104 . pole-balancing domain
becomes stochastic addition noise actuators reward function.
542

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 16: Modeling baseline behavior step pole-balancing task: (a) Number steps per
trial executed case-base B baseline behavior . (b) Cumulative reward
IBL approach.
per trial , learned safe case-based policy B
hand-made baseline behavior demonstrates execution safe, yet suboptimal
policy, average cumulative reward per episode/trial 9292.
learnt
modeling baseline behavior step PI-SRL, safe case-based policy B
demonstrations provided baseline behavior . computed following
procedure described subsection 3.3, values 0.02 12572, respectively.
Figure 16 (a) shows two different learning processes modeling baseline behavior step.
learning process, Figure 16 (a) shows number steps per trial executed
baseline behavior (continuous red lines) case-base B (dashed green lines).
beginning learning process, case-base B empty steps performed
using baseline behavior . learning process progresses, however, B filled
learnt. end learning process (after around
safe case-based policy B
45-50 trials), almost steps performed using cases B rarely used.
important note modeling baseline behavior step performed without
failures (i.e., pole disequilibrium cart track) case. previous
task, Figure 16 (b) represents three independent execution processes using previously (derived B indicated dashed green lines),
learned safe case-based policy B
baseline behavior (indicated continuous red lines) approach based
IBL (indicated dashed blue lines) (Aha & Kibler, 1991). average cumulative
9230 (Figure 16 [b]). almost perfectly clones ,
reward per episode B

B
IB1 approach which, cases, results pole disequilibrium cart falling
track averages cumulative reward per episode 8055.
Figure 17 (a) shows results PI-SRL different risk configurations.
configuration, learning curves shown two different learning processes performed.
Additionally, episode ending failure marked (blue triangles). increase
risk increases probability failure, policy obtained nevertheless better terms
cumulative reward. Nevertheless, much greater risk values ( = 9 105 ) produce
failures without accompanying increase cumulative reward. Figure 17 (b) shows
results evolutionary risk-sensitive RL approaches, former
543

fiGarca & Fernandez

Figure 17: Improving learned baseline behavior step pole-balancing task: (a) Cumulative reward per episode different risk configurations () obtained PISRL. (b) Cumulative reward per episode obtained evolutionary risksensitive RL approaches. cases, episode ending failure marked.

clearly algorithm greatest number failures. risk-sensitive approach,
= 2.0 (value maximization), agent selects actions result higher value,
also higher risk. contrary, = 0 (risk minimization), agent
learns risk function (at around episode 6000), selects actions lower risk (and
lower number failures), also considerably weak performance. value = 0.1
produces intermediate policy. Consequently, concluded PI-SRL high
level risk obtains better policies less failures evolutionary risk-sensitive
RL approaches. Figure 18 reinforces previous conclusions.

Figure 18: Mean number failures (pole disequilibrium cart track) cumulative reward 500 trials approach pole-balancing task.
means computed 10 different executions.
544

fiSafe Exploration State Action Spaces Reinforcement Learning

it, mean number failures cumulative reward 12,000 trials shown,
red circles corresponding PI-SRL, black triangles corresponding risksensitive approach blue square corresponding evolutionary RL approach.
figure also shows performance two additional risk levels, high level risk
( = 9 104 ) low level risk ( = 0), respect Figure 17. cumulative
reward number failures increase high level risk ( = 9 105 ).
risk level represents inflection point higher levels risk produce failures
without accompanying improvement cumulative reward. fact, high level
risk ( = 9 104 ) results reduction cumulative reward compared
high level risk ( = 9 105 ). Again, Pareto comparison criterion may used
compare solutions Figure 18. domain, solution PI-SRL
low level risk strictly dominates risk-sensitive solutions = 0.0 = 0.1,
PI-SRL = 9 107 risk-sensitive = 0.0 = 0.1. Additionally,
solution PI-SRL high level risk strictly dominates evolutionary RL solution,
PI-SRL = 9 105 evolutionary RL.
Lastly, Figure 19 shows evolution known space derived case-base
B different trials high-risk learning process. graph, error states (red
area), set unknown states (yellow area), set known states (green area)
set non-error states (orange circles) represented. known space
graph computed taking cases B trials = 0, 3000, 6000 8000.
graph, non-error states computed 10 different executions B
trial (the orange circles representing terminal states executions).
first graph (Figure 19 [a]) presents initial known space resulting modeling
baseline behavior step. evolution Figure 19 demonstrates two different points. First,
PI-SRL progressively adapts known space order encounter better behavior
known space tends compressed toward center coordinates.
due fact reward greater angle pole cart
position x 0 (i.e., pole upright possible cart cart centered
track). Second, risk failure pole-balancing domain greater
early trials learning process. beginning learning process (Figure 19 [a]),
= 0), regions known space close error space. situation,
slight modifications actions consistently produce visits states (i.e., pole
disequilibrium cart falling track). learning process advances (Figure 19
[b], [c] [d]), known space compressed toward origin coordinates away
error space. Consequently, probability visiting error states decreases.
example, returning Figure 17 (a), high-risk learning processes, 52% failures
(126) occur first 4000 trials, remaining 48% (117) occur last 8000
trials.
4.3 Helicopter Hovering
suggested name, objective domain make helicopter hover close
possible defined position duration established episode. task challenging two main reasons. Firstly, state action spaces high-dimensional
continuous (more specifically, state space 12-dimensional action space
545

fiGarca & Fernandez

Figure 19: Pole-balancing task: Evolution known space different trials = 0 (a),
= 3000 (b), = 6000 (c) = 8000 (d) high-risk learning process
( = 9 105 ). graph corresponds situation state space
according case-base B trial .
4-dimensional). Secondly, generalized domain whose behavior modified wind
factor. helicopter episode composed 6000 steps, although may end prematurely
helicopter crashes. first step PI-SRL performed order imitate baseline
behavior . computed following procedure described subection 3.3
values 0.3 49735, respectively. step performed, resulting
able properly imitate baseline behavior .
safe case-based policy B

Figure 20 (a) shows two learning processes modeling baseline behavior step.
Similar previous tasks, learning processes progress, number steps executed
baseline behavior reduced number steps using case-base B
increases. end learning process, case-base B stores safe case-based
. Figure 20 (b) compares performance (in terms cumulative reward per
policy B
IB1 approach. Regarding
episode) , learned case-based policy B
first two, average cumulative reward per episode -78035.93, obtained
-85130.11. Although perfectly mimic baseline behavior ,
B

B
546

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 20: Modeling baseline behavior step helicopter hovering task: (a) Number
steps per trial executed case-base B baseline behavior . (b) Cumula IBL
tive reward per trial , learned safe case-based policy B
approach.
nevertheless performs safe policy without crashing helicopter. regard
training process IB1 approach, every case produced 15 episodes baseline
behavior stored. Figure 20 (b) demonstrates IB1 approach consistently results
helicopter crashes, performance extremely far learned safe case . Improvement policy begins state-action space safely
based policy B
B
explored execution step two PI-SRL.
Figure 21 (a) shows results different risk levels. PI-SRL low medium
levels risk levels produce helicopter crashes PI-SRL, performance nevertheless
quite weak.

Figure 21: Improving learned baseline behavior step helicopter hovering task: (a)
Cumulative reward per episode different risk configurations obtained PISRL. (b) Cumulative reward per episode obtained evolutionary risksensitive RL approaches. cases, episode ending failure marked.
547

fiGarca & Fernandez

Conversely, high level risk established produces near-optimal policy
low number collisions. Extensive experimentation demonstrates increasing risk
parameter = 9 103 also increases number crashes without accompanying
improvement cumulative reward. Figure 21 (b) shows results evolutionary
RL approach which, remembered, selected winner RL Competition
2009 domain (Martn H. & Lope, 2009), well risk-sensitive RL algorithm
different values. comparison results evolutionary RL approach
PI-SRL shows similar cumulative reward, also significantly higher number
crashes former latter. evolutionary approach, crashes
occur early steps learning process; PI-SRL, accidents occur
advanced steps learning process. case risk-sensitive RL algorithm,
= 0 = 0.01 risk function learned around episode 3000. point,
agent selects lower-risk actions number crashes considerably reduced.
= 0.4 agent selects actions resulting higher values without taking risk
account, performance improves, expense increased number accidents.
Nevertheless whatever value, number crashes higher performance
worse PI-SRL.

Figure 22: Mean number failures (helicopter crashes) cumulative reward 5000
episodes approach helicopter hovering task. means
computed 10 different executions.
information Figure 22, indicating mean number failures cumulative
reward 5000 episodes approach, complements conclusions made above.
data computed 10 independent executions approach.
previous domains, PI-SRL indicated red circles, risk-sensitive approach
black triangles evolutionary RL approach blue square. Figure 22 also shows
performance two additional risk levels, high level risk ( = 9 102 )
low level risk ( = 0), respect Figure 21. Figure 22 demonstrates
evolutionary RL approach obtains highest cumulative reward (7.13 107 ),
followed closely PI-SRL (7.57 107 ). approaches far results.
Regarding number failures (i.e., helicopter crashes), PI-SRL low level
risk ( = 0), low level risk ( = 9 105 ) medium level risk ( = 9 104 )
548

fiSafe Exploration State Action Spaces Reinforcement Learning

produces collisions, PI-SRL algorithm medium risk preferable inasmuch
cumulative reward higher (18.01 107 ). Using Pareto comparison criterion,
PI-SRL solution high level risk strictly dominates solutions risksensitive approach (PI-SRL = 9 103 risk-sensitive). Moreover, PI-SRL strictly
dominated solution.

Figure 23: Evolution known space different episodes helicopter hovering
task. (a) Example representation single known state radar chart. (b),
(c), (d) Known states episodes = 0, = 500 = 4000, respectively,
high-risk learning process ( = 9 103 ). graph corresponds
situation known space according case-base B episode .
pole-balancing domain, Figure 23 shows evolution known space
according case-base B different episodes high-risk learning process.
case, radar charts used due high number features describing states.
radar chart graphical method displaying multivariate data two-dimensionally.
Figure, axis represents one features state and, preserve simplicity
representation, charts generated normalizing absolute values features
0 1. Figure 23 (a) example representation single known state.
549

fiGarca & Fernandez

value axis corresponds value individual feature state
line drawn connecting feature values axis. line Figure 23 (a)
represents single state, Figures 23 (b), (c) (d) show known space according
case-base B episodes 0, 500 4000, respectively. three charts represent
single state, rather states B corresponding episode. Thus,
graph, set known states marked (green area). state considered error state
single feature value state greater 1. limits (marked red line
graphs) computed taking account helicopter crashes (i)
velocity along main axes exceeds 5 m/s, (ii) position helicopter
20 m, (iii) angular rate around main axes exceeds 2 2 rad/s
(iv) orientation 30 degrees target orientation. previous
tasks, Figure 23 indicates two different matters. First, learning proceeds, known
space derived B adjusted space used better safer policies.
helicopter domain, agent tries hover helicopter close possible target
position (i.e., origin coordinates), since immediate rewards greater closer
helicopter hovers origin. Thus, known space starts expand (Figure 23
[b]) and, progressively, concentrated origin coordinates (Figure 23 [c] [d]).
regard second matter, probability crashing low since,
beginning, known space already appears concentrated origin far
error space (Figure 23 [b]). words, beginning, features
known space (i.e., forward, sideways downward velocities; x, y, z coordinates; x,
z angular-rates; x, z quaternation) far error space limits,
decreasing probability visiting error state.
previous experiments, second step PI-SRL performed using
initial case-base B free failures built first step algorithm.
following experiments show performance second step PI-SRL different
initial policies used. Figure 24 (a) shows performance policies used initial
policies. continuous black line indicates performance initial safe case-based
policy B , average cumulative reward per episode -85,130.11, used previous
experiments prior execution step two algorithm. remaining lines
Figure correspond performance three different initializations case-base B
used new experiments, prior execution step two algorithm. Using
poor initial policy (dashed green lines) helicopter crashed nearly
episodes, average cumulative reward per episode calculated -108,548.03.
Using different poor (albeit less poor) initial policy (continuous red lines)
helicopter crashed occasionally, average cumulative reward per episode -91,723.89.
Finally, near-optimal policy (dashed blue lines) whereby helicopter hovering free
failures yields average cumulative reward per episode -13,940.1.
Figure 24 (b) shows performance second step (improving baseline behavior
step) PI-SRL, starting case-base B corresponding poor, poor
near-optimal policies presented Figure 24 (a). Figure 24 (b), dashed blue lines
correspond use case-base B containing near-optimal policy, continuous
red lines correspond use case-base B containing poor policy dashed
green lines correspond use case-base B containing poor policy.
experiments Figure conducted using high level risk domain
550

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 24: (a) performance different initial policies helicopter hovering task.
(b) performance different executions second step PI-SRL,
starting case-base B containing policy three different types:
poor, poor near-optimal.

( = 9 103 ). graph indicates use near-optimal policy initial
policy high level risk level, case-base worsen performance which,
fact, appears improve slightly. second step PI-SRL prevents degradation
initial performance B, since updates cases case-base made using bad
episodes. words, updates B made cases gathered episodes
cumulative reward similar best episode found particular point
using threshold (whose value set 5% cumulative reward best episode).
example, cumulative reward best episode -13,940.1, episodes
cumulative reward higher -14,637 used update case-base (discarding
bad episodes episodes failures). way, good sequences experiences
provided updates, since proven good sequences experiences
cause adaptive agent converge stable useful policy, bad sequences
may cause agent converge unstable poor policy (Wyatt, 1997). solid red
lines Figure 24 (b) show using poor policy failures initial policy produces
higher number failures using initial policy free failures. However
despite poor initialization, PI-SRL nevertheless able learn near-optimal policy
well policy free failures used initialize B (see lines corresponding high
level risk, = 9 103 , Figure 21 (a)). Finally, dashed green lines Figure 24
(b) show use poor initial policy many failures results decreased
performance higher number failures produced, even though nevertheless able
learn better behavior. case, algorithm falls local minimum, probably
biased poor initialization. cases poor policies, number
failures higher beginning learning process decreases learning
process proceeds. poor poor initial policies close
error space, stark contrast initial policy shown Figure 23 which,
beginning, already appears concentrated origin, far error space.
551

fiGarca & Fernandez

learning process proceeds, different policies compressed away error
space number failures decreases.
4.4 SIMBA
Business simulators powerful tools improving management decision-making processes. example tool SIMulator Business Administration (SIMBA)
(Borrajo et al., 2010). SIMBA competitive simulator, since agents compete
agents management different virtual companies. simulator,
result twenty years experience university students business
executives, emulates business realities using variables, relationships events
present business world. objective provide users integrated vision
company, using basic techniques business management, simplifying complexity
emphasizing content principles greatest educational value (Borrajo et al.,
2010). experiments performed here, learning agent competes five handcoded agents (Borrajo et al., 2010). Decision-making SIMBA episodic task
decisions made sequentially. make business decision, state must studied
10 continuous decision variables (e.g., selling price, advertising expenses, etc.) must
set, followed study state composed 12 continuous variables (e.g., material
costs, financial expenses, economic productivity, etc.) (Borrajo et al., 2010). episode
composed 52 steps, although may prematurely company goes bankrupt (i.e.,
losses higher 10% net assets).

Figure 25: Modeling baseline behavior step SIMBA Task: (a) Number steps per trial
executed case-base B baseline behavior . (b) Cumulative reward per
IBL approach.
trial , learned safe case-based policy B
Figure 25 (a) shows evolution number steps executed baseline behavior case-base B two learning processes performing modeling baseline
behavior step. computed following procedure described subsection 3.3
values 1 102 513, respectively. episodes (approximately 25),
learned. Figure 25 (b) shows performance previouslysafe case-based policy B

learned B , IB1 approach. study, mean profits per episode
552

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 26: Improving learned baseline behavior step SIMBA task: (a) mean
profits per episode different risk configurations obtained PI-SRL agent
five hand-coded agents. (b) mean profits per episode obtained
evolutionary risk-sensitive RL agent five hand-coded agents.
cases, episode ending failure (bankruptcy) noted.
4.02 million Euros. IB1
5.24 million Euros, obtained B
approach, cases generated using baseline behavior 25 episodes stored.
experiments demonstrate SIMBA, contrast previous domains, storing cases sufficient obtaining safe policy performance similar using
modeling baseline behavior step (with mean profits per episode 3.98 million Euros).
learned, execute improving learned baseline
safe case-based policy B
behavior step.
Similar findings earlier tasks, Figure 26 (a) indicates low medium
levels risk produce bankruptcies, performance nevertheless weak. highest
level risk produces near-optimal policy low number number failures.
contrast, Figure 26 (b) presents results evolutionary risk-sensitive RL approaches, former clearly yields highest number failures.
risk-sensitive case, number bankruptcies cases insufficient learning risk function . comparative results Figure 26 show PI-SRL
= 9 101 obtains better policies less failures evolutionary risk-sensitive
RL approaches.
Figure 27 shows graphical representation different solutions domain.
shows mean number failures cumulative reward different approaches
100 episodes, data computed 10 independent executions approach.
Figure, red circles correspond PI-SRL algorithm, black triangles correspond
risk-sensitive approach blue square corresponds evolutionary RL approach.
Figure 27 also shows performance two additional risk levels, high ( = 9 102 )
low ( = 0), respect Figure 26. experiments Figure 27 demonstrate
PI-SRL high level risk ( = 9 101 ) obtains highest cumulative reward,
6693.58. Additionally, PI-SRL low level risk ( = 0), low level risk
( = 9 101 ) medium level risk ( = 9 100 ) approaches lowest

553

fiGarca & Fernandez

Figure 27: Mean number failures (company bankruptcies) cumulative reward
100 episodes approach SIMBA task. means
computed 10 different executions.
mean number failures, 0.0. However, PI-SRL medium level risk preferred
inasmuch performance superior terms cumulative reward. PI-SRL
high level risk ( = 9 102 ) increases number failures obtains lower cumulative
reward compared PI-SRL high level risk. Using Pareto comparison
criterion, PI-SRL high level risk strictly dominates solutions (PI-SRL
= 9101 risk-sensitive PI-SRL = 9101 evolutionary RL), approach
strictly dominated solution.
Due difficulty representing high-dimensional state action space
SIMBA domain, graphs provided evolution known space.

5. Related Work
Reinforcement learning (RL) case-based reasoning (CBR) techniques combined literature different ways. work Bianchi et al. (2009), new approach
presented permitting use cases heuristics speed RL algorithms. Additionally, Sharma et al. (2007) use combination CBR RL (called CARL) achieve
transfer playing Game AI across variety scenarios MadRTS TM,
commercial Real Time Strategy game. CBR also used state value function
approximation RL (Gabel & Riedmiller, 2005). However, present study is,
knowledge, first time CBR RL used conjunction safe exploration dangerous domains. field safe reinforcement learning, three principal
trends observed: (i) approaches based return variance, (ii) risk-sensitive
approaches based definition error states (iii) approaches using teachers.
5.1 Approaches Based Return Variance
literature, long known optimal policy optimal expected
return MDP quite sensitive parameter variations (even optimal policy may
554

fiSafe Exploration State Action Spaces Reinforcement Learning

perform badly cases due stochastic nature problem). mitigate
problem, agent try maximize return associated worst-case scenario,
even though case may highly
unlikely. Thus, trend, risk refers worst
P
r variance. example approach
outcomes return R =


t=0
worst-case control worst possible outcome R optimized (Coraluppi
& Marcus, 1999; Heger, 1994). worst case control strategies, optimality criterion
exclusively focused risk-avoiding policies. policy considered optimal
worst-case return superior. approach, however, restrictive inasmuch takes
rare scenarios fully account.
value return introduced Heger (1994) seen extension
worst case control MDPs. concept establishes returns R <
policy occur probability lower neglected. algorithm less
pessimistic pure worst case control, given extremely rare scenarios effect
policy. work Heger et al., idea weighting return risk, namely
expected value-variance criterion, also introduced.
risk-sensitive control based use exponential utility functions, return R
transformed reflect subjective measure utility. Instead maximizing expected
value R, objective maximize U = 1 logE(eR ), parameter
R usual return. shown that, depending parameter , policies
high variance V (R) penalized ( < 0) enforced ( > 0). Instead, Neuneier
Mihatsch (2002) consider worst-case-outcomes policy, (i.e., risk related
variability return). study, authors demonstrate learning algorithm
interpolates risk-neutral worst-case criterion limiting
behavior exponential utility functions. noted approaches based
variability return worst possible outcomes suited problems
policy small variance produce large risk (Geibel & Wysotzki, 2005).
view risk present study, however, concerned variance
return worst possible outcome, instead fact processes generally
possess unsafe states avoided. Consequently, address different class
problems dealt approaches focusing variability return.
5.2 Risk-sensitive Approaches based Error States.
second trend approaches, concept risk based definition error
states fatal transitions. Thus, Geibel et al. (2005) , instance, establish risk
function probability entering error state. Instead, Hans et al (2008) consider
transition fatal corresponding reward less given threshold .
first case demonstrated Section 4, learned TD methods require
error states (i.e., car collisions, pole-balancing disequilibrium, helicopter crashes
company bankruptcies) visited repeatedly order approximate risk function
and, subsequently, avoid dangerous situations. second case, concept risk
joined reward. Moreover, mentioned studies either (i) assume
system dynamics known, (ii) tolerate undesirable states exploration
or, contrast paper, (iii) deal problems high-dimensional
continuous state-action spaces. Regarding latter, Geibel et al. write
555

fiGarca & Fernandez

approach also extended continuous action sets (e.g., using actor-critic
method), give details may done entirely continuous
problems. Section 4, present approach solves problem.
5.3 Approaches Using Teachers
last trend approaches based use teachers three different ways:
(i) bootstrap learning algorithm (i.e., initialization procedure), (ii) derive
policy finite demonstration set (iii) guide exploration process.
5.3.1 Bootstrapping Learning Algorithm
work Driessens Szeroski (2004), bootstraping procedure used relational RL finite set demonstrations recorded human expert
later presented regression algorithm. allows regression algorithm build
partial Q-function later used guide exploration state space
using Boltzmann exploration strategy. Smart Kaelbling (2000) also use examples,
training runs bootstrap Q-learning approach HEDGER algorithm.
initial knowledge bootstrapped Q-learning approach allows agent learn
effectively helps reduce time spent random actions. Teacher behaviors
also used form population seeding neuroevolution approaches (Yao, 1999; Siebel &
Sommer, 2007). Evolutionary methods used optimize weights neural networks,
starting prototype network whose weights correspond teacher (or baseline
policy). Using technique, RL Competition helicopter hovering task winners Martin et
al. (2009) developed evolutionary RL algorithm several teachers provided
initial population. algorithm restricts crossover mutation operators, allowing
slight changes policies given teachers. Consequently, rapid convergence algorithm near-optimal policy ensured, indirect minimization
damage agent. However, teachers included initial population resulting
ad-hoc training regimen conducted competition. Consequently, proposed
approach seems somewhat ad-hoc easily generalizable arbitrary RL problems.
work Koppejan et al. (2009, 2011), neural networks also evolved, beginning
one whose weights corresponds teacher behavior. approach
proven advantageous numerous applications evolutionary methods (Hernandez-Daz
et al., 2008; Koppejan & Whiteson, 2009), Koppejans algorithm nevertheless also seems
somewhat ad-hoc designed specialized set environments.
5.3.2 Deriving Policy Finite Set Demonstrations
approaches falling category framed according field Learning
Demonstration (LfD) (Argall et al., 2009). Highlighting study Abbeel et al. (2010)
based apprenticeship learning, approach composed three distinct steps.
first, teacher demonstrates task learned state-action trajectories
teachers demonstration recorded. second step, state-action trajectories seen
point used learn dynamics model system. model, (near)optimal policy found using reinforcement learning (RL) algorithm. Finally,
policy obtained tested running real system. work Tang et
556

fiSafe Exploration State Action Spaces Reinforcement Learning

al. (2010), algorithm based apprenticeship learning also presented automaticallygenerating trajectories difficult control tasks. proposal based learning
parameterized versions desired maneuvers multiple expert demonstrations. Despite
approachs potential strengths general interest, inherently linked
information provided demonstration dataset. result, learner performance
heavily limited quality teachers demonstrations.
5.3.3 Guiding Exploration Process
Driessens Szeroski (2004), context relational RL, also use given teachers
policy, rather policy derived current Q-function hypothesis (which
informative early learning stages), selection actions. approach,
episodes performed teacher interleaved normal exploration episodes.
mixture teacher normal exploration make easier regression algorithm
distinguish beneficial poor actions. context LfD,
approaches include teacher advice (Argall et al., 2009). advice used improve
learner performance, offering information beyond provided demonstration
dataset. approach, following initial task demonstration teacher, agent
directly requests additional demonstration teacher different states
previously demonstrated states single action cannot selected
certainty (Chernova & Veloso, 2007, 2008).
works mentioned trend, explicit definition risk ever given.

6. Conclusions
work, PI-SRL, algorithm policy improvement safe reinforcement
learning high-risk tasks, described. main contributions algorithm
definitions novel case-based risk function baseline behavior safe exploration
state-action space. use case-based risk function presented possible
inasmuch policy stored case-base. represents clear advantage
approaches, e.g., evolutionary RL (Martn H. & Lope, 2009; Koppejan & Whiteson,
2011) extraction knowledge known space agent impossible
using weights neural-networks. Additionally, completely different notion
risk others found literature presented. According notion, risk
independent variance return reward function, require
identification error states learning risk functions. Rather, concept
risk described paper based distance known unknown
space and, therefore, domain-independent parameter (in sense, proposal allows
application parameter-setting method described subsection 3.3).
Koppejan et al. (2011) also use function identify dangerous states, contrast
approach, definition function requires strong previous knowledge domain.
Furthermore, approaches risk found literature tackle problems
entirely continuous (Geibel & Wysotzki, 2005) report results
one continuous domain (Koppejan & Whiteson, 2011). Consequently, difficult know
certain approaches literature generalize easily arbitrary domains.
557

fiGarca & Fernandez

paper presents PI-SRL algorithm great detail demonstrates effectiveness four entirely different continuous domains: car parking problem, pole-balancing,
helicopter hovering business management (SIMBA). experiments presented
paper demonstrate different characteristics learning capabilities PI-SRL
algorithm.
(i) PI-SRL obtains higher quality solutions. experiments Section 4 demonstrate
that, save helicopter hovering task, PI-SRL obtains cases best cumulative
reward per episode least number failures. Additionally, using Pareto comparison criterion said that, save high risk configuration car parking
problem, approach strictly dominated approach.
(ii) PI-SRL adjusts initial known space safe better policies. initial known
space resulting first step PI-SRL, modeling baseline behavior, adjusted
improved second step algorithm, improving learned baseline behavior.
Additionally, experiments demonstrate adjustment process compress
known space away error space (e.g., pole-balancing domain, subsection 4.2,
helicopter hovering domain, subsection 4.3) or, occasions, require known
space move closer error space (e.g., car parking problem, subsection 4.1)
event better policies found there.
(iii) PI-SRL works well domains differently structured state-action spaces
value function vary sharply. Although car parking problem, polebalancing domain, helicopter hovering task business simulator represent
differently structured problems, experiments study nevertheless demonstrate
PI-SRL performs well each. Furthermore, even domains car parking
problem value function varies sharply due presence obstacle,
experimental results demonstrate PI-SRL nevertheless successfully handle
difficulty. However, impossible avoid failures known space edge
edge error states algorithm would often explore error states.
(iv) number failures depends distance known space
error space. experiments pole-balancing helicopter hovering domains demonstrate number failures depends close known space error
space. Due structure domains, improving learned baseline behavior
step algorithm tends concentrate known space origin coordinates
away error space. greater distance known space error
space, lower number failures. Additionally, helicopter hovering, known
space is, beginning, far error space (consequently, number failures also low beginning). Therefore, initial distribution known space
learned baseline policy later influences number failures obtained
second step PI-SRL.
(v) PI-SRL completely safe first step algorithm executed. However,
proceeding way, algorithm performance would heavily limited
capabilities baseline behavior. learner performance improved beyond
performance baseline behavior, subsequent exploratory process
second step PI-SRL must carried out. Since complete knowledge domain
dynamic possessed, however, also inevitable that, exploratory
558

fiSafe Exploration State Action Spaces Reinforcement Learning

process, unknown regions state space visited agent may reach error
states.
(vi) risk parameter allows user configure level risk assumed.
algorithm, user gradually increase value risk parameter order
obtain better policies, also assuming greater likelihood damage learning
system.
(vii) PI-SRL performs successfully even poor initial policy failures used.
experiments Figure 24 helicopter hovering domain demonstrate PI-SRL
able learn near-optimal policy despite poor initialization, policy
free failures used initialize case-base B. However, Figure also shows
poor initial policy many failures used, PI-SRL decreases performance
produces higher number failures, although better behavior still learnt.
case, algorithm falls local minimum, likely biased poor initialization.
follows, applicability method discussed, allowing reader
clearly understand scenarios proposed PI-SRL approach may applicable.
applicability restricted domains following characteristics.
(i) mandatory scenario satisfy two assumptions described Section 2.
According first assumption, nearby states domain must necessarily similar actions. According other, similar actions similar states produce similar
effects. fact similar actions lead similar states assumes degree smoothness dynamic behavior system which, certain environments, may hold.
However, clearly explain Section 2, consider assumptions logical
assumptions derived generalization principles RL literature (Kaelbling et al.,
1996; Jiang, 2004).
(ii) applicability method limited size case-base B required
mimic baseline behavior. possible apply proposed approach tasks when,
first step PI-SRL algorithm, modeling baseline behavior, prohibitively large
number cases required properly mimic complex baseline behaviors. case,
threshold increased restrict addition new cases casebase. However, increase may adversely affect final performance algorithm.
Nevertheless, experiments performed Section 4 demonstrate relatively simple
baseline behaviors mimicked almost perfectly using manageable number cases.
(iii) PI-SRL algorithm requires presence baseline behavior. proposed
method requires presence baseline behavior safely demonstrates task
learned. baseline behavior conducted human teacher hand-coded
agent. important note, nevertheless, presence baseline behavior
guaranteed domains.
Finally, logical continuation present study would take account automatic
graduation risk parameter along learning process. example, would
particularly interesting exploit fact known space far away error
space order increase risk parameter or, contrary, reduce
close. future work aims deploy algorithm real environments, inasmuch
uncertainty real environments presents biggest challenge autonomous
robots. Autonomous robotic controllers must deal large number factors
robotic mechanical system electrical characteristics, well environmental
559

fiGarca & Fernandez

complexity. However, use PI-SRL algorithm (or risk-sensitive approaches)
learning processes real environments could reduce amount damage incurred
and, consequently, allow lifespan robots extended. might worthwhile
add mechanism algorithm detect known state lead directly
error state. problems currently investigated.

Acknowledgments
study partially supported Spanish MICIIN projects TIN2008-06701-C0303, TRA2009-0080 CCG10-UC3M/TIC-5597. offer gratitude special thanks
Raquel Fuentetaja Pizan, Assistant Professor Universidad Carlos III de Madrid
Planning & Learning Group (PLG), generous invaluable comments
revision paper. would also like thank Jose Antonio Martn, Assistant
Professor Universidad Complutense de Madrid, invaluable comments regarding
evolutionary RL algorithm.

References
Aamodt, A., & Plaza, E. (1994). Case-Based Reasoning; Foundational Issues, Methodological Variations, System Approaches. AI Communications, 7 (1), 3959.
Abbeel, P., Coates, A., Hunter, T., & Ng, A. Y. (2008). Autonomous Autorotation
RC Helicopter. ISER, pp. 385394.
Abbeel, P., Coates, A., & Ng, A. Y. (2010). Autonomous helicopter aerobatics
apprenticeship learning. I. J. Robotic Res., 29 (13), 16081639.
Abbott, R. G. (2008). Robocup 2007: Robot soccer world cup xi.. chap. Behavioral Cloning
Simulator Validation, pp. 329336. Springer-Verlag, Berlin, Heidelberg.
Aha, D. W. (1992). Tolerating Noisy, Irrelevant Novel Attributes Instance-Based
Learning Algorithms. International Journal Man-Machine Studies, 36 (2), 267287.
Aha, D. W., & Kibler, D. (1991). Instance-based learning algorithms. Machine Learning,
pp. 3766.
Anderson, C. W., Draper, B. A., & Peterson, D. A. (2000). Behavioral cloning student
pilots modular neural networks. Proceedings Seventeenth International
Conference Machine Learning, pp. 2532. Morgan Kaufmann.
Argall, B., Chernova, S., Veloso, M., & Browning, B. (2009). Survey Robot Learning
Demonstration. Robotics Autonomous Systems, 57 (5), 469483.
Bartsch-Sprl, B., Lenz, M., & Hbner, A. (1999). Case-based reasoning: Survey future
directions.. Puppe, F. (Ed.), XPS, Vol. 1570 Lecture Notes Computer Science,
pp. 6789. Springer.
Bianchi, R., Ros, R., & de Mantaras, R. L. (2009). Improving reinforcement learning
using case-based heuristics.. Vol. 5650, pp. 7589. Lecture Notes Artificial Intelligence, Springer, Lecture Notes Artificial Intelligence, Springer.
560

fiSafe Exploration State Action Spaces Reinforcement Learning

Borrajo, F., Bueno, Y., de Pablo, I., Santos, B. n., Fernandez, F., Garca, J., & Sagredo, I.
(2010). SIMBA: Simulator Business Education Research. Decission Support
Systems, 48 (3), 498506.
Boyan, J., Moore, A., & Sutton, R. (1995). Proceedings workshop value function
approximation, machine learning conference 1995... Technical Report CMU-CS-95206.
Chernova, S., & Veloso, M. (2007). Confidence-based policy learning demonstration
using gaussian mixture models. Joint Conference Autonomous Agents
Multi-Agent Systems.
Chernova, S., & Veloso, M. (2008). Multi-thresholded approach demonstration selection
interactive robot learning. Proceedings 3rd ACM/IEEE international
conference Human robot interaction, HRI 08, pp. 225232, New York, NY, USA.
ACM.
Cichosz, P. (1995). Truncating temporal differences: efficient implementation
td(lambda) reinforcement learning. Journal Artificial Intelligence Research
(JAIR), 2, 287318.
Cichosz, P. (1996). Truncated temporal differences function approximation: Successful examples using cmac. Proceedings Thirteenth European Symposium
Cybernetics Systems Research (EMCSR-96).
Coraluppi, S. P., & Marcus, S. I. (1999). Risk-Sensitive Minimax Control DiscreteTime, Finite-State Markov Decision Processes. AUTOMATICA, 35, 301309.
Defourny, B., Ernst, D., & Wehenkel, L. (2008). Risk-aware decision making dynamic
programming. NIPS 2008 Workshop Model Uncertainty Risk RL.
Driessens, K., & Ramon, J. (2003). Relational instance based regression relational rl.
International Conference Machine Learning (ICML), pp. 123130.
Driessens, K., & Dzeroski, S. (2004). Integrating guidance relational reinforcement
learning. Machine Learning, 57 (3), 271304.
Fernandez, F., & Isasi, P. (2008). Local feature weighting nearest prototype classification.
Neural Networks, IEEE Transactions on, 19 (1), 4053.
Fernandez, F., & Borrajo, D. (2008). Two steps reinforcement learning. International
Journal Intelligent Systems, 23 (2), 213245.
Floyd, M. W., & Esfandiari, B. (2010). Toward domain-independent case-based reasoning
approach imitation: Three case studies gaming. Workshop Case-Based
Reasoning Computer Games 18th International Conference Case-Based
Reasoning (ICCBR), pp. 5564.
Floyd, M. W., Esfandiari, B., & Lam, K. (2008). Case-Based Reasoning Approach
Imitating Robocup Players. Proceedings 21st International Florida Artificial
Intelligence Research Society Conference, pp. 251256.
Forbes, J., & Andre, D. (2002). Representations learning control policies.
University New South, pp. 714.
561

fiGarca & Fernandez

Gabel, T., & Riedmiller, M. (2005). Cbr state value function approximation reinforcement learning. Proceedings 6th International Conference Case-Based
Reasoning (ICCBR 2005, pp. 206221. Springer.
Geibel, P. (2001). Reinforcement Learning Bounded Risk. Proceedings 18th
International Conference Machine Learning, pp. 162169. Morgan Kaufmann.
Geibel, P., & Wysotzki, F. (2005). Risk-sensitive Reinforcement Learning Applied Control
Constraints. Journal Artificial Intelligence Research (JAIR), 24, 81108.
Hans, A., Schneegass, D., Schafer, A. M., & Udluft, S. (2008). Safe Exploration Reinforcement Learning. European Symposium Artificial Neural Network, pp.
143148.
Heger, M. (1994). Consideration Risk Reinforcement Learning. 11th International
Conference Machine Learning, pp. 105111.
Hernandez-Daz, A. G., Coello, C. A. C., Perez, F., Caballero, R., Luque, J. M., & SantanaQuintero, L. V. (2008). Seeding initial population multi-objective evolutionary algorithm using gradient-based information. IEEE Congress Evolutionary
Computation, pp. 16171624. IEEE.
Hester, T., Quinlan, M., & Stone, P. (2011). real-time model-based reinforcement learning
architecture robot control. Tech. rep. arXiv e-Prints 1105.1749, arXiv.
Hu, H., Kostiadis, K., Hunter, M., & Kalyviotis, N. (2001). Essex wizards 2001 team
description. Birk, A., Coradeschi, S., & Tadokoro, S. (Eds.), RoboCup, Vol. 2377
Lecture Notes Computer Science, pp. 511514. Springer.
Jiang, A. X. (2004). Multiagent reinforcement learning stochastic games continuous
action spaces..
Kaelbling, L., Littman, M., & Moore, A. (1996). Reinforcement learning: survey. Journal
Artificial Intelligence Research (JAIR), 4, 237285.
Konen, W., & Bartz-Beielstein, T. (2009). Reinforcement learning games: failures
successes. Proceedings 11th Annual Conference Companion Genetic
Evolutionary Computation Conference: Late Breaking Papers, GECCO 09, pp. 2641
2648, New York, NY, USA. ACM.
Koppejan, R., & Whiteson, S. (2009). Neuroevolutionary reinforcement learning generalized helicopter control. GECCO 2009: Proceedings Genetic Evolutionary
Computation Conference, pp. 145152.
Koppejan, R., & Whiteson, S. (2011). Neuroevolutionary reinforcement learning generalized control simulated helicopters. Evolutionary Intelligence, 4, 219241.
Lee, J.-Y., & Lee, J.-J. (2008). Multiple Designs Fuzzy Controllers Car Parking Using
Evolutionary Algorithm, pp. 16. No. May.
Luenberger, D. G. (1998). Investment science. Oxford University Press.
Mannor, S. (2004). Reinforcement learning average reward zero-sum games. ShaweTaylor, J., & Singer, Y. (Eds.), COLT, Vol. 3120 Lecture Notes Computer Science,
pp. 4963. Springer.
562

fiSafe Exploration State Action Spaces Reinforcement Learning

Martin H, J., & de Lope, J. (2009). Exa: effective algorithm continuous actions
reinforcement learning problems. Industrial Electronics, 2009. IECON 09. 35th
Annual Conference IEEE, pp. 2063 2068.
Martn H., J. A., & Lope, J. (2009). Learning Autonomous Helicopter Flight Evolutionary Reinforcement Learning. 12th International Conference Computer
Aided Systems Theory (EUROCAST), pp. 7582.
Mihatsch, O., & Neuneier, R. (2002). Risk-Sensitive reinforcement learning. Machine Learning, 49 (2-3), 267290.
Moldovan, T. M., & Abbeel, P. (2012). Safe exploration markov decision processes.
CoRR, abs/1205.4810.
Narendra, K. S., & Thathachar, M. A. L. (1974). Learning automata - survey. Ieee
Transactions Systems Man Cybernetics, SMC-4 (4), 323334.
Narendra, K. S., & Thathachar, M. A. L. (1989). Learning automata: introduction.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Ng, A. Y., Kim, H. J., Jordan, M. I., & Sastry, S. (2003). Autonomous Helicopter Flight
via Reinforcement Learning. Thrun, S., Saul, L. K., & Scholkopf, B. (Eds.), NIPS.
MIT Press.
Peters, J., Tedrake, R., Roy, N., & Morimoto, J. (2010). Robot learning. Sammut, C.,
& Webb, G. I. (Eds.), Encyclopedia Machine Learning, pp. 865869. Springer.
Poli, R., & Cagnoni, S. (1997). Genetic programming user-driven selection: Experiments evolution algorithms image enhancement. Genetic Programming
1997: Proceedings Second Annual Conference, pp. 269277. Morgan Kaufmann.
Salkham, A., Cunningham, R., Garg, A., & Cahill, V. (2008). collaborative reinforcement learning approach urban traffic control optimization. Web Intelligence
Intelligent Agent Technology, 2008. WI-IAT 08. IEEE/WIC/ACM International
Conference on, Vol. 2, pp. 560566.
Santamara, J. C., Sutton, R. S., & Ram, A. (1998). Experiments reinforcement
learning problems continuous state action spaces. Adaptive Behavior, 6,
163218.
Sharma, M., Holmes, M., Santamaria, J., Irani, A., Isbell, C., & Ram, A. (2007). Transfer
learning real-time strategy games using hybrid cbr/rl. Proceedings
Twentieth International Joint Conference Artificial Intelligence.
Siebel, N. T., & Sommer, G. (2007). Evolutionary reinforcement learning artificial neural
networks. International Journal Hybrid Intelligent Systems, 4, 171183.
Smart, W. D., & Kaelbling, L. P. (2000). Practical reinforcement learning continuous
spaces. Artificial Intelligence, pp. 903910. Morgan Kaufmann.
Smart, W. D., & Kaelbling, L. P. (2002). Effective reinforcement learning mobile robots.
ICRA, pp. 34043410. IEEE.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT
Press.
563

fiGarca & Fernandez

Tang, J., Singh, A., Goehausen, N., & Abbeel, P. (2010). Parameterized maneuver learning autonomous helicopter flight. International Conference Robotics
Automation (ICRA).
Taylor, M. E., Kulis, B., & Sha, F. (2011). Metric learning reinforcement learning agents.
Proceedings International Conference Autonomous Agents Multiagent
Systems (AAMAS).
Van Hasselt, H., & Wiering, M. A. (2007). Reinforcement Learning Continuous Action
Spaces. Approximate Dynamic Programming Reinforcement Learning, 2007.
ADPRL 2007. IEEE International Symposium on, pp. 272279.
Wyatt, J. (1997). Exploration Inference Learning Reinforcement. University
Edinburgh.
Yao, X. (1999). Evolving artificial neural networks. PIEEE: Proceedings IEEE, 87,
14231447.

564

fiJournal Artificial Intelligence Research 45 (2012) 1-45

Submitted 11/11; published 9/12

Interactions Knowledge Time
First-Order Logic Multi-Agent Systems:
Completeness Results
F. Belardinelli
A. Lomuscio

f.belardinelli@imperial.ac.uk
a.lomuscio@imperial.ac.uk

Department Computing
Imperial College London, UK

Abstract
investigate class first-order temporal-epistemic logics reasoning multiagent systems. encode typical properties systems including perfect recall, synchronicity, learning, unique initial state terms variants quantified interpreted systems, first-order extension interpreted systems. identify several monodic
fragments first-order temporal-epistemic logic show completeness respect
corresponding classes quantified interpreted systems.

1. Introduction
reactive systems (Pnueli, 1977) traditionally specified using plain temporal logic,
well-established tradition Artificial Intelligence (AI) and, particular, MultiAgent Systems (MAS) research adopt expressive languages. Much tradition
inspired earlier, seminal work AI McCarthy (1979, 1990) others aimed
adopting intentional stance (Dennett, 1987) reasoning intelligent systems.
Specifically, logics knowledge (Fagin, Halpern, Moses, & Vardi, 1995), beliefs, desires,
intentions, obligations, etc., put forward represent informational
motivational attitudes agents system. Theoretical explorations focused
soundness completeness number axiomatisations well decidability
computational complexity corresponding logics.
great majority work lines focuses propositional languages. Yet, specifications supporting quantification increasingly required applications. example,
often necessary refer different individuals different instances time.
Quantified modal languages (Garson, 2001) long attracted considerable attention.
Early work included analysing philosophical logical implications different setups quantification domains, particularly combination temporal concepts.
recently, considerable attention given identifying suitable fragments
preserve completeness decidability, studying resulting computational complexity satisfiability problem. article follows direction.
detail, investigate meta-theoretical properties monodic fragments
quantified temporal-epistemic logic interactions quantifiers, time,
knowledge agents present. deep-rooted interest (Fagin et al., 1995;
Meyden, 1994) understanding implications interaction axioms context,
often express interesting properties MAS, including perfect recall, synchronicity,
c
2012
AI Access Foundation. rights reserved.

fiBelardinelli & Lomuscio

learning. features well-understood propositional level (Fagin,
Halpern, & Vardi, 1992; Halpern, van der Meyden, & Vardi, 2004) commonly used
several application areas. technical question paper aims resolve whether
similar range results provided presence (limited forms of) quantification.
shall demonstrate, answer question largely positive.
1.1 State Art
analysis application temporal-epistemic logic first-order setting
established tradition AI. One early contributions work Moore (1990),
presents theory action takes consideration epistemic preconditions
actions effects knowledge. recently, number first-order temporalepistemic logics reasoning MAS introduced Wooldridge et al. (2002,
2006, 1999), often context MABLE programming language agents.
authors introduced first-order branching time temporal logic MAS (Wooldridge
& Fisher, 1992), developed series papers (Wooldridge et al., 2002, 2006).
First-order multi-modal logics also constitute conceptual base number
agent theories, BDI logics (Rao & Georgeff, 1991), KQML framework (Cohen
& Levesque, 1995), LORA framework (Wooldridge, 2000b). include
operators mental attitudes (e.g., knowledge, belief, intention, desire, etc.), well
temporal dynamic operators form quantification. However,
current literature far fallen short systematic analysis formal properties
frameworks. frameworks rich unlikely
finitely axiomatisable, let alone decidable. Still, earlier contributions inspiration
present investigation, among explicitly addressed
subject first-order temporal-epistemic languages MAS setting.
purely theoretical level, first-order temporal epistemic logics also received increasing attention range contributions axiomatisability (Degtyarev
et al., 2003; Sturm et al., 2000; Wolter & Zakharyaschev, 2002), decidability (Degtyarev
et al., 2002; Hodkinson et al., 2000; Wolter & Zakharyaschev, 2001), complexity (Hodkinson, 2006; Hodkinson et al., 2003). Wolter Zakharyaschev (2001) introduced
monodic fragment quantified modal logic, modal operators restricted
formulas one free variable, proved decidability various fragments. Similar results obtained monodic fragments first-order temporal
logic (Hodkinson, 2002; Hodkinson et al., 2000), computational complexity
formalisms analysed (Hodkinson, 2006; Hodkinson et al., 2003). Further, Wolter
Zakharyaschev (2002) provided complete axiomatisation monodic first-order
validities natural numbers. monodic fragment first-order epistemic logic
also explored (Sturm et al., 2000; Sturm, Wolter, & Zakharyaschev, 2002),
axiomatisation including common knowledge provided. lines research
constitute theoretical background research set.
contributions discussed previously used plain Kripke models underlying semantics. However, argued though applications computationallygrounded semantics (Wooldridge, 2000a) preferable, enables systems modelled directly. introduced quantified interpreted systems (QIS) fill gap (Belar2

fiInteractions Knowledge Time First-Order Logic MAS

dinelli & Lomuscio, 2009). enabled us provide complete axiomatisation
monodic fragment quantified temporal-epistemic logic linear time (Belardinelli & Lomuscio, 2011). However, interaction temporal epistemic modalities
studied. Preliminary investigations interactions temporal epistemic
operators first-order setting already appeared (Belardinelli & Lomuscio, 2010).
paper extend previous results also consider epistemic languages containing
common knowledge operator.
1.2 Present Contribution
paper extends current state art first-order temporal-epistemic logic
introducing family provably complete calculi variety quantified interpreted systems characterising range properties including perfect recall, learning, synchronicity,
unique initial state. prove completeness presented first-order
temporal-epistemic logics via quasimodel construction, previously used
(Hodkinson, Wolter, & Zakharyaschev, 2002; Hodkinson et al., 2000) prove decidability
monodic fragments first-order temporal logic (FoTL). Quasimodels also
applied first-order temporal well epistemic logic (Sturm et al., 2000; Wolter & Zakharyaschev, 2002). Wolter et al. (2002) present complete axiomatisation monodic
fragment FoTL natural numbers; similar result variety first-order epistemic logics common knowledge also appeared (Sturm et al., 2000). However,
interaction temporal epistemic modalities first order setting
taken account yet, interpreted systems semantics. Nonetheless,
features essential applications multi-agent systems subject
analysis here.
1.2.1 Structure Paper.
Section 2 first introduce first-order temporal-epistemic languages Lm LCm
common knowledge set Ag = {1, . . . , m} agents. present relevant
classes QIS well monodic fragments Lm LCm . Sections 3 introduce
axiomatisations classes QIS, details completeness proofs
presented Sections 4 5. Finally, Section 6 elaborate results obtained
discuss possible extensions future work.

2. First-Order Temporal-Epistemic Logics
Interpreted systems standard semantics interpreting temporal-epistemic logics
multi-agent setting (Fagin et al., 1995; Parikh & Ramanujam, 1985). extend interpreted
systems first-order case enriching structures domain individuals.
first investigated static quantified interpreted systems, account
evolution system given (Belardinelli & Lomuscio, 2008, 2009). Then, fully-fledged
QIS language also temporal modalities introduced (Belardinelli & Lomuscio,
2010, 2011). follow definition QIS provided references.
3

fiBelardinelli & Lomuscio

2.1 First-Order Temporal-Epistemic Languages
Given set Ag = {1, . . . , m} agents, first-order temporal-epistemic language Lm
contains individual variables x1 , x2 , . . ., individual constants c1 , c2 , . . ., n-ary predicate constants P1n , P2n , . . ., n N, propositional connectives , quantifier ,
linear time operators U, epistemic operator Ki agent Ag.
language LCm also contains common knowledge operator C (Fagin et al., 1992).
simplicity consider one group agents common knowledge modality,
is, whole Ag; C really tantamount CAg . extension proper non-empty
subsets Ag problematic.
languages Lm LCm contain symbol functions; terms t1 , t2 , . . .
languages either individual variables constants.
Definition 1. Formulas Lm defined Backus-Naur form follows:
::= P k (t1 , . . . , tk ) | | 0 | x | | U 0 | Ki
language LCm extends Lm following clause:
formula LCm , also C formula LCm .
formulas U0 read next step eventually 0
respectively. formula Ki represents agent knows , C stands
common knowledge set Ag agents.
define symbols , , , , G (always future), F (some time future)
standard. Further, introduce abbreviations. V
operator Ki dual Ki ,
is, Ki defined Ki , E shorthand iAg Ki . k N, E k
defined follows: E 0 = E k+1 = EE k . formulas Ki E read
agent considers possible every agent knows respectively.
Free bound variables defined standard. [~y ] mean ~y = y1 , . . . , yn
free variables . Additionally, [~y /~t] formula obtained substituting simultaneously some, possibly all, free occurrences ~y ~t = t1 , . . . , tn
renaming bound variables. sentence formula free variables.
2.2 Quantified Interpreted Systems
introduce quantified interpreted systems assume set Li local states li , li0 , . . .
agent Ag multi-agent system. consider set Le local states
environment e well. set Le L1 . . .Lm contains global states
multi-agent system. represent temporal evolution MAS consider
flow time N natural numbers; run function r : N S. Intuitively, run
represents one possible evolution MAS assuming N flow time. Given
above, define quantified interpreted systems languages Lm LCm follows:
Definition 2 (QIS). quantified interpreted system triple P = hR, D, Ii where:
R non-empty set runs;
non-empty set individuals;
4

fiInteractions Knowledge Time First-Order Logic MAS

r R, n N, first-order interpretation, is, function
every constant c, I(c, r(n)) D,
every predicate constant P k , I(P k , r(n)) k-ary relation D.
Further, every r, r0 R, n, n0 N, I(c, r(n)) = I(c, r0 (n0 )).
Notice assume unique domain interpretation, well fixed interpretation
individual constants; simply write I(c). Following standard notation (Fagin et al.,
1995), r R n N, pair (r, n) point P. r(n) = hle , l1 , . . . , lm
global state point (r, n) (n) = le ri (n) = li environments agent
local state (r, n) respectively. Further, Ag epistemic equivalence relation
defined (r, n) (r0 , n0 ) iff ri (n) = ri0 (n0 ). Clearly, equivalence
relation. Two points (r, n) (r0 , n0 ) said epistemically
reachable, simply

reachable, (r, n) (r0 , n0 ) transitive closure iAg .
paper consider following classes QIS.
Definition 3. quantified interpreted system P satisfies
synchronicity

iff

every Ag, points (r, n), (r0 , n0 ),
(r, n) (r0 , n0 ) implies n = n0

perfect recall agent

iff

points (r, n), (r0 , n0 ), (r, n) (r0 , n0 ) n > 0
either (r, n 1) (r0 , n0 )
k < n0 (r, n 1) (r0 , k)
k 0 , k < k 0 n0 implies (r, n) (r0 , k 0 )

learning agent

iff

points (r, n), (r0 , n0 ), (r, n) (r0 , n0 )
either (r, n + 1) (r0 , n0 )
k > n0 (r, n + 1) (r0 , k)
k 0 , k > k 0 n0 implies (r, n) (r0 , k 0 )

unique initial state

iff

r, r0 R, r(0) = r0 (0)

conditions extensively discussed literature (Halpern et al., 2004)
together equivalent formulations. Intuitively, QIS synchronous time part
local state agent. QIS satisfies perfect recall agent local state records
everything happened (from agents point view) far run.
learning dual perfect recall: agent acquire new knowledge run.
Finally, QIS unique initial state runs start global state.
QIS P satisfies perfect recall (resp. learning) P satisfies perfect recall (resp.
learning) agents. denote class QIS agents QIS ; superscripts
pr, nl, sync, uis denote subclasses QIS satisfying perfect recall, learning,
synchronicity, unique initial state respectively. instance, QIS sync,uis


class synchronous QIS agents unique initial state.
assign interpretation formulas Lm LCm means quantified
interpreted systems. Let assignment variables individuals D, valuation
5

fiBelardinelli & Lomuscio

(t) term defined (y) = y, (t) = I(c) = c. variant ax
assignment assigns x agrees variables.
Definition 4. satisfaction formula Lm point (r, n) P assignment
, denoted (P , r, n) |= , defined inductively follows:
(P , r, n) |= P k (t1 , . . . , tk )
(P , r, n) |=
(P , r, n) |= 0
(P , r, n) |= x
(P , r, n) |=
(P , r, n) |= U 0

iff
iff
iff
iff
iff
iff

(P , r, n) |= Ki

iff

hI (t1 ), . . . , (tk )i I(P k , r(n))
(P , r, n) 6|=
(P , r, n) 6|= (P , r, n) |= 0
x
D, (P , r, n) |=
(P , r, n + 1) |=
n0 n (P , r, n0 ) |= 0
(P , r, n00 ) |= n n00 < n0
r0 , n0 , (r, n) (r0 , n0 ) implies (P , r0 , n0 ) |=

LCm consider also case common knowledge operator:
(P , r, n) |= C

iff

k N, (P , r, n) |= E k

truth conditions , , , , G F defined above.
definition follows (P , r, n) |= C iff (r0 , n0 ) reachable (r, n),
(P , r0 , n0 ) |= .
formula true point (r, n) satisfied (r, n) every assignment ;
true QIS P true every point P; valid class C QIS true
every QIS C. Further, formula satisfiable QIS P satisfied
point P, assignment ; satisfiable class C QIS satisfiable
QIS C.
considering combinations pr, nl, sync uis obtain 16 subclasses QIS
N. independent, axiomatisable. Indeed,
axiomatisable even propositional level (Halpern & Moses, 1992; Halpern &
Vardi, 1989). first column Table 1 group together classes QIS share
set validities languages Lm LCm . proofs equivalences
similar propositional case (Halpern et al., 2004) reported
here. Further, define languages PLm PLCm propositional fragments
Lm LCm respectively (formally, PLm PLCm obtained restricting atomic
formulas 0-ary predicate constants p1 , p2 , . . .). Table 1 summarises results Halpern
et al. (2004) concerning axiomatisability propositional validities PLm PLCm .
Observe that, regards language PLm , = 1 sets validities various
classes QIS axiomatisable, 2 axiomatisation given
QIS nl,uis
QIS nl,pr,uis
(Halpern & Vardi, 1986, 1989). language PLCm ,


restrict case 2, = 1 PLCm expressive power
PLm . 2 class validities PLCm recursive axiomatisation QIS ,
uis
sync,uis
QIS nl,sync,uis
, QIS nl,pr,sync,uis
.
QIS sync


, QIS , QIS
next section show axiomatisability results propositional level
lifted monodic fragment languages Lm LCm .
6

fiInteractions Knowledge Time First-Order Logic MAS

QIS
sync,uis
QIS , QIS sync
,
QIS uis

, QIS
pr,uis
QIS pr
, QIS
pr,sync
QIS
, QIS pr,sync,uis

nl
QIS
QIS nl,sync

QIS nl,pr

QIS nl,pr,sync

QIS nl,uis

QIS nl,pr,uis

QIS nl,sync,uis
, QIS nl,pr,sync,uis



PL1
X
X
X
X
X
X
X
X
X
X

PLm , 2
X
X
X
X
X
X
X
7
7
X

PLCm , 2
X
7
7
7
7
7
7
7
7
X

Table 1: Equivalences classes QIS axiomatisability results propositional fragments PLm PLCm . sign X indicates set validities
specific class axiomatisable; 7 indicates not.

2.3 Monodic Fragment
rest paper show sufficient condition lifting results
Table 1 first-order case restrict languages Lm LCm monodic
fragments.
Definition 5. monodic fragment L1m set formulas Lm subformula form Ki , , 1 U2 contains one free variable. Similarly,
1 set formulas LC
monodic fragment LCm
subformula
form Ki , C, , 1 U2 contains one free variable.
monodic fragments number first-order modal logics thoroughly
investigated literature (Hodkinson et al., 2000, 2003; Wolter & Zakharyaschev, 2001,
2002). case Lm LCm fragments quite expressive contain
formulas like following:
C(zAvailable(y, z)UxRequest(x, y))

(1)

Ki xyz(Request(x, y) Available(y, z))
Ki xyz(Request(x, y) Available(y, z))

(2)

Formula (1) intuitively states common knowledge every resource
eventually requested somebody, time resource remains available
everybody. Notice free variable within scope modal operators U
C. Formula (2) represents agent knows next step every resource
available whenever requested, next step agent knows indeed
case. However, note formula
xKi (Process(x) yF Access(x, y))
7

fiBelardinelli & Lomuscio

intuitively means agent knows every process eventually try access
every resource, L1m x occur free within scope modal operator
F . Still, monodic fragments Lm LCm quite expressive contain
de dicto formulas, i.e., formulas free variable appears scope modal
operator, (2). So, limitation really de formulas.
stress fact formulas propositional equivalent
case interepreted quantified interpreted systems domain
quantification infinite, cardinality cannot bounded advance.
Finally, observe Barcan formulas x x Ki x xKi
true quantified interpreted systems, QIS includes unique domain
quantification. implies universal quantifier commutes temporal
modality epistemic modality Ki . Thus, case formulas
, 0 Lm , 0 validity, L1m 0
/ L1m . instance,
consider = xP (x, y) 0 = x P (x, y). see remark
interfere results.

3. Axiomatisations
section present sound complete axiomatisations sets monodic validities classes quantified interpreted systems Section 2. First, introduce
basic system QKTm extends first-order case multi-modal epistemic logic
S5m combined linear temporal logic LTL.
Definition 6. system QKTm contains following schemes axioms rules,
, formulas L1m = inference relation.
First-order logic

Temporal logic

Epistemic logic

Taut
MP
Ex
Gen
K
T1
T2
Nec
T3
K

4
5
Nec

classical propositional tautologies
, =
x [x/t]
[x/t] = x, x free
( ) ( )

U ( (U))
=
= (U)
Ki ( ) (Ki Ki )
Ki
Ki Ki Ki
Ki Ki Ki
= Ki

operator Ki S5 modality, next U operators axiomatised linear-time modalities (Fagin et al., 1995). add classical postulates
Ex Gen quantification, sound consider unique domain
individuals quantified interpreted systems.
8

fiInteractions Knowledge Time First-Order Logic MAS

Definition 7. system QKT Cm extends QKTm following schemes axioms
1 = inference
common knowledge, , formulas LCm
relation.
C1
C2

C ( EC)
( E) = C

consider standard definitions proof theorem; `S means formula
theorem formal system S. remark Barcan formula (BF ) 2x
x2 provable unary modal operator 2 axioms K Ex, rules
P Gen. notions soundness completeness system respect
class C QIS defined standard: sound w.r.t. C , ` implies C |= .
Similarly, complete w.r.t. C , C |= implies ` .
paper focus schemes axioms Table 2 specify key interactions
time knowledge (Halpern et al., 2004). use 1, . . . , 5 superscripts denote
KT1
KT2
KT3
KT4
KT5

Ki (Ki Ki ) Ki ((Ki )U((Ki )U))
Ki Ki
(Ki )UKi Ki ((Ki )UKi )
Ki Ki
Ki Kj
Table 2: axioms KT1-KT5.

systems obtained adding QKTm QKTCm combination KT1-KT5.
instance, system QKTC2,3
extends QKTCm axioms KT2 KT3.
straightforward check axioms QKTm QKTCm valid every
QIS inference rules preserve validity. However, axioms KT1-KT5 valid
specific classes QIS stated following Remark.
Remark 1. QIS P satisfies axioms KT1-KT5 first column P satisfies
corresponding semantical condition second column.
Axiom
KT1
KT2
KT3
KT4
KT5

Condition QIS
perfect recall
perfect recall, synchronicity
learning
learning, synchronicity
agents share knowledge, i.e.,
i, j Ag, (r, n) (r0 , n0 ) iff (r, n) j (r0 , n0 ).

results shown similar way propositional case (Halpern et al.,
2004); proofs omitted.
using Remark 1 prove soundness results first-order temporalepistemic systems.
Theorem 1 (Soundness). systems reported first second column following table sound w.r.t. corresponding classes QIS third column.
9

fiBelardinelli & Lomuscio

Systems
QKTm
QKT Cm
1
1
QKTm
QKT Cm
2
2
QKTm
QKT Cm
3
3
QKTm
QKT Cm
4
4
QKTm
QKT Cm
2,3
2,3
QKTm
QKT Cm
1,4
1,4
QKTm
QKT Cm
1,4,5
1,4,5
QKTm
QKT Cm

QIS
sync,uis
QIS , QIS sync
,
QIS uis

, QIS
pr,uis
QIS pr
, QIS
pr,sync
QIS
, QIS pr,sync,uis

nl
QIS , QIS nl,uis

QIS nl,sync

nl,pr,uis
QIS nl,pr
, QIS
nl,pr,sync
QIS
QIS nl,sync,uis
, QIS nl,pr,sync,uis



Proof. results follow Remark 1 line reasoning similar used
propositional case (Fagin et al., 1995; Halpern et al., 2004). Notice quantified
interpreted systems P satisfies learning, synchronicity, unique initial state,
P satisfies also perfect recall, is, P QIS nl,sync,uis
implies P QIS nl,pr,sync,uis
.


Further, agents share knowledge, therefore KT5 holds P.
anticipated above, calculi complete w.r.t. corresponding classes
quantified interpreted systems Theorem 1. next theorem summarise
completeness results proved rest paper.
Theorem 2 (Completeness). systems reported first second column
following table complete w.r.t. corresponding classes QIS third column.
Systems
QKTm
QKT Cm
1
QKTm
2
QKTm
3
QKTm
4
QKTm
2,3
QKTm
1,4
QKTm
QKT12,3
1,4,5
1,4,5
QKTm
QKT Cm

QIS
sync,uis
uis
QIS , QIS sync
, QIS , QIS
pr
pr,uis
QIS , QIS
QIS pr,sync
, QIS pr,sync,uis


nl
QIS
QIS nl,sync

QIS nl,pr

QIS nl,pr,sync

QIS nl,uis
, QIS nl,pr,uis
1
1
nl,pr,sync,uis
QIS nl,sync,uis
,
QIS



observe that, regards language L1m , sets monodic validities axiomatisable classes introduced QIS nl,uis
QIS nl,pr,uis
. However, L11


QIS nl,uis
QIS nl,pr,uis
equivalent QIS nl,pr
. Thus, sets monodic
1
1
1
nl,pr,uis
nl,uis
validities QIS 1
QIS 1
axiomatised QKT2,3
1 .
1
regards language LCm , set monodic validities QIS , QIS sync
,
sync,uis
nl,sync,uis
nl,pr,sync,uis
QIS uis
,
QIS

axiomatisable,

well



QIS

QIS
.




classes recursively axiomatisable, case already propositional level (Halpern & Moses, 1992; Halpern & Vardi, 1986, 1989).
proving completeness results reported introduce Kripke models
generalisation quantified interpreted systems.
10

fiInteractions Knowledge Time First-Order Logic MAS

3.1 Kripke Models
prove completeness results Theorem 2, first introduce appropriate class
Kripke models generalisation QIS prove completeness models.
apply correspondence result Kripke models QIS obtain desired
results.
Definition 8 (Kripke model). Kripke model tuple = hW, RW , {i }iAg , D, Ii

W non-empty set states;
RW non-empty set functions r : N W ;
every agent Ag, equivalence relation W ;
non-empty set individuals;
every w W , first-order interpretation, is, function
every constant c, I(c, w) D,
every predicate constant P k , I(P k , w) k-ary relation D.
Further, every w, w0 W , I(c, w) = I(c, w0 ).
Notice Def. 8 differs notions Kripke model includes
set RW functions guarantee correspondence Kripke models
QIS one-to-one. also assume unique domain interpretation, well fixed
interpretation individual constants, also case simply write I(c). Kripke
models generalisation QIS specify inner structure states
W . Also Kripke models introduce points pairs (r, n) r RW n N.
point derives properties corresponding state; instance, (r, n) (r0 , n0 )
r(n) r0 (n0 ).
consider Kripke models satisfying synchronicity, perfect recall, learning,
unique initial state. definition subclasses analogous Def. 3.
Definition 9. Kripke model satisfies
synchronicity

iff

every Ag, points (r, n), (r0 , n0 ),
(r, n) (r0 , n0 ) implies n = n0

perfect recall agent

iff

points (r, n), (r0 , n0 ), (r, n) (r0 , n0 ) n > 0
either (r, n 1) (r0 , n0 )
k < n0 (r, n 1) (r0 , k)
k 0 , k < k 0 n0 implies (r, n) (r0 , k 0 ).

learning agent

iff

points (r, n), (r0 , n0 ), (r, n) (r0 , n0 )
either (r, n + 1) (r0 , n0 )
k > n0 (r, n + 1) (r0 , k)
k 0 , k > k 0 n0 implies (r, n) (r0 , k 0 ).

unique initial state

iff

r, r0 RW , r(0) = r0 (0).
11

fiBelardinelli & Lomuscio

let Km class Kripke models agents. Hereafter adopt
sync,uis
naming conventions QIS; instance, Km
class synchronous Kripke
models agents unique initial state. Further, inductive clauses
satisfaction relation |= respect assignment straightforwardly defined
QIS, well notions truth validity.
Definition 10. satisfaction formula Lm (resp. LCm ) point (r, n)
assignment , (M , r, n) |= , inductively defined follows:
(M , r, n) |= P k (t1 , . . . , tk )
(M , r, n) |=
(M , r, n) |= 0
(M , r, n) |= x
(M , r, n) |=
(M , r, n) |= U 0

iff
iff
iff
iff
iff
iff

(M , r, n) |= Ki
(M , r, n) |= C

iff
iff

hI (t1 ), . . . , (tk )i I(P k , r(n))
(M , r, n) 6|=
(M , r, n) 6|= (M , r, n) |= 0
x
D, (Ma , r, n) |=
(M , r, n + 1) |=
n0 n (M , r, n0 ) |= 0
n n00 < n0 implies (M , r, n00 ) |=
r0 , n0 , (r, n) (r0 , n0 ) implies (M , r0 , n0 ) |=
k N, (M , r, n) |= E k

formula true point (r, n) satisfied (r, n) every assignment ;
true Kripke model true every point M; valid class C
Kripke models true every Kripke model C. Further, formula satisfiable
Kripke model satisfied point M, assignment ;
satisfiable class C Kripke models satisfiable Kripke model C.
relate Kripke models quantified interpreted systems means map g :
Km QIS (Lomuscio & Ryan, 1998). Let = hW, RW , {i }iAg , D, Ii Kripke
model. every agent Ag, (r, n) M, let equivalence class [(r, n)]i = {(r0 , n0 ) |
(r, n) (r0 , n0 )} local state agent i; (r, n) local state
environment. define g(M) tuple hR0 , D, 0 R0 contains runs rr
r RW rr (n) = h(r, n), [(r, n)]1 , . . . , [(r, n)]m i. Further, M,
every constant c, 0 (c, rr (n)) = I(c, r(n)), 0 (P k , rr (n)) = I(P k , r(n)).
structure g(M) QIS satisfies following result:
Lemma 1. every Lm (resp. LCm ),
(M , r, n) |= iff (g(M) , rr , n) |=
Proof. proof induction structure . atomic formula P k (t1 , . . . , tk ), (M , r, n) |= iff hI (t1 ), . . . , (tk )i I(P k , r(n)), iff
hI 0 (t1 ), . . . , 0 (tk )i 0 (P k , rr (n)), iff (g(M) , rr , n) |= . inductive cases
propositional connectives quantifiers straightforward, well temporal operators U. = Ki , (M , r, n) |= iff (r, n) (r0 , n0 )
0
implies (M , r0 , n0 ) |= , (r, n) (r0 , n0 ) iff rri (n) = rri (n0 ). Thus, (M , r, n) |= iff
0
(rr , n) 0i (rr , n0 ) implies (M , r0 , n0 ) |= . Again, induction hypothesis (M , r, n) |=
0
0
iff (rr , n) 0i (rr , n0 ) implies (g(M) , rr , n0 ) |= , i.e., iff (g(M) , rr , n) |= . case
= C treated similarly considering epistemic reachability relation.

12

fiInteractions Knowledge Time First-Order Logic MAS

Notice satisfies synchronicity, perfect recall, learning, unique
initial state, also g(M) satisfies property. follows fact
0
(r, n) (r0 , n0 ) iff (rri , n) 0i (rr , n0 ). Thus, g defines map 16 subclasses
Km outlined Def. 9 corresponding subclass QIS obtain following
corollary Lemma 1.
Corollary 1. Let X subset {pr, nl, sync, uis}. every monodic formula L1m
1 ), satisfiable KX , satisfiable QIS X .
(resp. LCm


reasoning monodic fragments Lm LCm dealing
learning perfect recall, introduce following class monodic friendly Kripke
models. structures motivated fact KT1 KT3 weak
enforce either perfect recall learning Kripke models axioms
restricted monodic formulas. However, suffice monodic friendly structures.
following, also prove satisfiability Kripke models equivalent satisfability
monodic friendly structures restrict languages monodic formulas.
Definition 11 (mf-model). monodic friendly
Mmf = hW, RW , {i,a }iAg,aD , D, Ii

Kripke

model





tuple

W , RW , defined Kripke models;
Ag, D, i,a equivalence relation W .
define synchronicity, perfect recall, learning, unique initial state
also mf-models parametrising Def. 9 relation i,a . instance, mf-model
satisfies perfect recall agent points (r, n), (r0 , n0 ), D, whenever
(r, n) i,a (r0 , n0 ) n > 0 either (r, n 1) i,a (r0 , n0 ) k < n0
(r, n 1) i,a (r0 , k) k 0 , k < k 0 n0 implies (r, n) i,a (r0 , k 0 ). regards
subclasses class MF mf-models agents, adopt naming
conventions QIS Kripke models. Notice Kripke models seen
mf-models Ag, a, b D, i,a equal i,b .
1 ) mf-model
Finally, satisfaction relation |= L1m (resp. LCm
mf
defined way Kripke models, except epistemic operators:
(Mmf , r, n) |= Ki [y]

iff

r0 , n0 , (r, n) i,(y) (r0 , n0 ) implies (Mmf , r0 , n0 ) |= [y]

appears free . Notice sentence, (Mmf , r, n) |= Ki
iff (r, n) i,a (r0 , n0 ) implies (Mmf , r0 , n0 ) |= D. case common
knowledge operator C straightforward definition E k . particular, two points (r, n)
0 0
(r0 , n0 ) epistemically reachable
D, simply reachable, (r, n) (r , n ),
transitive closure iAg i,a .
remark converse Barcan formula, CBF , Ki x xKi holds
mf-models; Barcan formula, BF , xKi Ki x not. check
consider mf-model = hW, RW , {i,a }iAg,aD , D, Ii Fig.1(a)
- W = {w, w0 , w00 }
- RW = {r, r0 , r00 } r(0) = w, r0 (0) = w0 , r00 (0) = w00
13

fiBelardinelli & Lomuscio

..
.

..
.

..
.

..
.

i,b

i,a

..
.

..
.
i,d

i,c

w0

w

w00

v0

v

P (a)

P (a), P (b)

P (b)

Q(c)

Q(c)

v 00

(b) mf-model M0 .

(a) mf-model M.

Figure 1: Arrows represent system runs; epistemically related states grouped
together.

- = {a, b}
- I(P 1 , r(0)) = {a, b}, I(P 1 , r0 (0)) = {a} I(P 1 , r00 (0)) = {b}
- i,a i,b equivalence relations (r, 0) i,a (r0 , 0) (r, 0) i,b (r00 , 0).
see (M, r, 0) |= xKi P (x), (M, r, 0) 6|= Ki xP (x) (r, 0) i,a (r0 , 0)
(M , r0 , 0) 6|= P (x) (x) = b.
Furthermore, K axiom Ki ( 0 ) (Ki Ki 0 ) valid mf-models
either. fact, consider mf-model M0 = hW 0 , R0W 0 , {0i,a }iAg,aD0 , D0 , 0 Fig.1(b)

- W 0 = {v, v 0 , v 00 }
- R0W 0 = {q, q 0 , q 00 } q(0) = v, q 0 (0) = v 0 , q 00 (0) = v 00
- D0 = {c, d}
- 0 (Q1 , q(0)) = {c}, 0 (Q1 , q 0 (0)) = {c} 0 (Q1 , q 00 (0)) =
- i,c i,d equivalence relations (q, 0) i,c (q 0 , 0) (q, 0) i,d (q 00 , 0).
Finally, let (x) = c. check (M , q, 0) |= (Q(x) xQ(x)) Q(x)
(M , q 0 , 0) |= (Q(x) xQ(x)) Q(x). Thus, (M , q, 0) |= Ki (Q(x) xQ(x)) Ki Q(x).
(M, q 00 , 0) 6|= xQ(x), (M , q, 0) 6|= Ki xQ(x).
prove following lemma, used completeness proof
systems satisfying perfect recall learning. lemma states that, deal
satisfability monodic formulas, mf-models suffice.
Lemma 2. Let MF K,BF
class mf-models validating formulas K BF .

1 ),
every monodic formula L1m (resp. LCm
Km |= iff MF K,BF
|=

14

fiInteractions Knowledge Time First-Order Logic MAS

Proof. implication right left follows fact class Km
Kripke models isomorphic subclass monodic friendly Kripke models
Ag, a, b D, i,a equal i,b . words, given Kripke model =
hW, RW , {i }iAg , D, Ii define mf-model M0 = hW, RW , {i,a }iAg,aD , D, Ii,
every D, i,a equal . straightforward see M0 validates
K BF (in particular, counterexamples Fig. 1 ruled out). Further,
6|= M0 6|= . Thus, MF K,BF
|= , Km |= .

implication left right, assume Mmf = hW, RW ,
{i,a }iAg,aD , D, Ii mf-model validating K BF (Mmf , r, n) 6|=
point (r, n) assignment . build Kripke model M0 =
hW 0 , R0W 0 , {0i }iAg , D0 , 0 Mmf (M0 , r, n) 6|= follows. start
assuming W 0 = W , SR0 = R D0 = D. Further, Ag, define 0i
transitive closure aD i,a . Finally, set 0 = I. check Kripke
model M0 well defined validate .
First all, point following issue associated construction above:
case point (q, k) monodic formula [x], happens
(Mmf , q, k) |= Ki [x], (q, k) i,(x) (q 0 , k 0 ) (q, k) i,(y) (q 00 , k 00 ) (x) 6= (y).
Further, suppose (Mmf , q 00 , k 00 ) 6|= [x], obviously (Mmf , q 0 , k 0 ) |=
[x]. definition 0i (M0 , q, k) 6|= Ki [x]; two models
satisfy formulas. solve problem modifying interpretation
according structure monodic formula [x], keeping truth value
[x] point (q, k). consider relevant cases according structure [x];
induction hypothesis consists fact able find interpretation
subformulas [x].
[x] = P (x) simply assume (x) I(P, q 00 (k 00 )), (M0 , q 00 , k 00 ) |=
[x] (M0 , q, k) |= Ki [x]. Note change truth value
epistemic formula (q, k) assumed (q, k) 6i,(x) (q 00 , k 00 ) (otherwise [x] would
satisfied (q 00 , k 00 )). cases propositional connectives modal operators
similarly dealt applying induction hypothesis. [x] = y[x, y]


b
, q 00 , k 00 ) 6|= [x, y].
(Mmf , q 00 , k 00 ) 6|= [x], therefore exists b (Mmf
consider 4 different cases depending whether (q, k) satisfies
4 formulas:

Ki x [x, y]

(3)

Ki x [x, y]

(4)

Ki x [x, y]

(5)

Ki x [x, y]

(6)

using axioms inference rules QKTm Formula (3) show
follows (where used entailment):
(Mmf , q, k) |= Ki y[x, y] Ki x[x, y]
(Mmf , q, k) |= xKi y[x, y] yKi x[x, y] Ex
(Mmf , q, k) |= Ki xy[x, y] Ki yx[x, y] zKi Ki z
(Mmf , q, k) |= Ki (xy[x, y] yx[x, y]) Ki ( ) Ki Ki
15

fiBelardinelli & Lomuscio

(Mmf , q, k) |= Ki (xu[x, u] yv[v, y])
(Mmf , q, k) |= Ki xyuv([x, u] [v, y])
(Mmf , q, k) |= Ki xy([x, y] [x, y])

change variables
prefixing
Ex

last formula contradiction; (3) cannot hold (q, k). Similarly, Formula
(4) cannot hold (q, k) either because:
(Mmf , q, k) |= Ki y[x, y] Ki x[x, y]
(Mmf , q, k) |= xKi y[x, y] Ki x[x, y]
(Mmf , q, k) |= Ki xy[x, y] Ki x[x, y]
(Mmf , q, k) |= Ki xy[x, y] Ki yx[x, y]
(Mmf , q, k) |= Ki (xy[x, y] yx[x, y])
(Mmf , q, k) |= Ki xy([x, y] [x, y])

Ex
zKi Ki z
z Ki Ki z
Ki Ki Ki ( )
similarly

Note derivations make use formulas K BF (for instance,
prove theorems Ki ( ) Ki Ki zKi Ki z). Finally, satify
Formulas (5) (6) (q, k), guarantee existence individual x
avoiding clash (x). So, introduce new individual a0 domain D0
a0 (x) satisfy formulas points. Thus, a0 seen copy
(x). Finally, induction hypothesis modify interpretation 0
(M0 , q 00 , k 00 ) |= [x, y].
case common knowledge operator derives one Ki . result,
obtain Kripke model M0 (M0 , r, n) 6|= .
Moreover, procedure described above, Mmf satisfies perfect recall, learning, synchronicity, unique initial state, also M0 satisfies property.
Thus, Lemma 2 prove following result.
Corollary 2. Let X subset {pr, nl, sync, uis}. every monodic formula L1m
1 ), satisfiable MF X also validating formulas K BF ,
(resp. LCm

X.
satisfiable Km
Proof. easy see Mmf satisfies either synchronicity unique
initial state, M0 well way defined. Further, suppose Mmf
satisfies perfect recall, (r, n) 0i (r0 , n0 ) n > 0. means sequence
a1 , . . . , ak individuals sequence (q1 , m1 ), . . . , (qk , mk ) points (i)
(r, n) = (q1 , m1 ) (r0 , n0 ) = (qk , mk ); (ii) (qj , mj ) i,aj (qj+1 , mj+1 ) j < k.
show result k = 3, case k > 3 follows straightforward generalisation.
(q1 , m1 1) i,a1 (q2 , m2 ), definition 0i (q1 , m1 1) 0i
(q3 , m3 ) well. Hence, Mmf satisfies perfect recall. Otherwise, suppose perfect
recall l2 < m2 (q1 , m1 1) i,a1 (q2 , l2 ), l20 , l2 < l20 m2
implies (q1 , m1 ) i,a1 (q2 , l20 ). consider l20 (h) = m2 h, 0 h < m2
l2 . perfect recall, either (i) exists p3 (h) (q2 , l20 (h)) i,a2 (q3 , p3 (h)),
p03 (h), p3 (h) < p03 (h) p3 (h 1) implies (q2 , l2 (h)) i,a2 (q3 , p03 (h)), (ii)
(q2 , l20 (h) 1) i,a2 (q3 , p3 (h 1)), p3 (1) = m3 . Notice cases,
definition 0i , (q1 , m1 ) 0i (q3 , p03 (h)) 0 h < m2 l2 , is,
(q1 , m1 ) 0i (q3 , p03 ) p3 [l2 + 1] < p03 m3 . Further, l2 , either (i) exists l3
16

fiInteractions Knowledge Time First-Order Logic MAS

(q2 , l2 1) i,a2 (q3 , l3 ), l30 , l3 < l30 p3 [l2 + 1] implies (q2 , l2 ) i,a2 (q3 , l30 ),
(ii) (q2 , l2 1) i,a2 (q3 , p3 [l2 + 1]). first case, l30 strictly less
m3 , l30 (q1 , m1 1) 0i (q3 , l30 ) l300 , l30 < l300 m3 implies
(q1 , m1 ) 0i (q3 , l300 ). Otherwise, (q1 , m1 1) 0i (q3 , m3 ). Hence, Mmf satisfies
perfect recall.
proof learning similar.
Finally, combining Corollaries 1 2 immediately obtain following result.
Corollary 3. Let X subset {pr, nl, sync, uis}. every monodic formula L1m
1 ), satisfiable MF X also validating formulas K BF ,
(resp. LCm

satisfiable QIS X
m.
next section show indeed possible build mf-model.

4. Completeness Proof
section outline main steps completeness proof, based
quasimodel construction (Gabbay, Kurucz, Wolter, & Zakharyaschev, 2003; Hodkinson
et al., 2000). Differently contributions, explicitly take account
interaction temporal epistemic modalities. Intuitively, quasimodel
monodic formula relational structure whose points sets sets subformulas
. set sets subformulas describes possible state affairs, contains
sets subformulas defining individuals state. formally, given formula
LCn1 define
subC = sub {EC | C sub} {Ki C | C sub, Ag}
sub set subformulas . L1n , subC simply sub. Further,
define
subC = subC { | subC } { | subC } { | subC }
Observe subC closed negation modulo equivalences 1,
is, subC , form subC ; otherwise,
subC . Finally, let subn subset subC containing formulas
n free variables. So, sub0 set sentences subn . x variable occurring
, define subx = {[y/x] | [y] sub1 }. Clearly, x free variable
formulas subx . con denote set constants occurring . Table 3
report set suby equal Formula (1) thus abbreviated:
C(zAv(y, z)UxReq(x, y))
Further, k N define closures clk clk,i mutual recursion.

Definition 12. Let cl0 = subx k 0, clk+1 = iAg clk,i . k 0, Ag,
clk,i = clk {Ki (1 . . . n ), Ki (1 . . . n ) | 1 , . . . , n clk }.
17

fiBelardinelli & Lomuscio

suby

{, C(zAv(y, z)UxReq(x, y)), EC(zAv(y, z)UxReq(x, y)),
{Ki C(zAv(y, z)UxReq(x, y))}iAg , zAv(y, z)UxReq(x, y), zAv(y, z), xReq(x, y),
, C(zAv(y, z)UxReq(x, y)), EC(zAv(y, z)UxReq(x, y)),
{Ki C(zAv(y, z)UxReq(x, y))}iAg , zAv(y, z)UxReq(x, y), zAv(y, z),
xReq(x, y),
, C(zAv(y, z)UxReq(x, y)), EC(zAv(y, z)UxReq(x, y)),
{ Ki C(zAv(y, z)UxReq(x, y))}iAg , zAv(y, z)UxReq(x, y), zAv(y, z),
xReq(x, y),
, C(zAv(y, z)UxReq(x, y)), EC(zAv(y, z)UxReq(x, y)),
{ Ki C(zAv(y, z)UxReq(x, y))}iAg , zAv(y, z)UxReq(x, y), zAv(y, z),
xReq(x, y)}

Table 3: set suby equal Formula (1).


{, C(zAv(y, z)UxReq(x, y)), EC(zAv(y, z)UxReq(x, y)),
{Ki C(zAv(y, z)UxReq(x, y))}iAg , zAv(y, z)UxReq(x, y), zAv(y, z), xReq(x, y),
, C(zAv(y, z)UxReq(x, y)), EC(zAv(y, z)UxReq(x, y)),
{ Ki C(zAv(y, z)UxReq(x, y))}iAg , zAv(y, z)UxReq(x, y), zAv(y, z),
xReq(x, y)}
Table 4: type cl0 , equal Formula (1).

define ad() greatest number alternations distinct Ki along branch
parse tree (Halpern et al., 2004). Further, index finite sequence = i1 , . . . , ik
agents 6= in+1 , 1 n < k; length denoted ||. Also, ]i
absorptive concatenation indexes ]i = ik = i. Finally, K
shorthand Ki1 . . . Kik . let index || ad(). empty
sequence cl = clad() . = 0 ]i, cl = clk,i k = ad() ||.
introduce types quasimodels, intuitively seen individuals described
maximal consistent sets formulas.
Definition 13 (Type). -type maximal consistent subset cl , i.e.,
every monodic formulas 0 cl ,
(i) iff
/ t;
(ii) 0 iff , 0 t.
Two -types t, t0 said agree sub0 sub0 = t0 sub0 , i.e., share
sentences. Given -type constant c con, ht, ci indexed type
, abbreviated tc . Table 4 report type cl0 , equal Formula (1).
introduce state candidates, intuitively represent states quasimodel.
Definition 14 (State candidate). -state candidate pair C = hT, con

(i) set -types agree sub0 ;
(ii) con set containing c con indexed type tc .
18

fiInteractions Knowledge Time First-Order Logic MAS

also introduce notion point, describes state candidate perspective particular type.
Definition 15 (Point). -point pair P = hC, ti
(i) C = hT, con -state candidate ;
(ii) -type.
Note that, slight abuse notation, call points pairs (r, n) QIS
pairs P = hC, ti. consistent previous work (Fagin et al., 1995; Halpern
et al., 2004); context disambiguate. Also, write C C = hT, con
. Similarly P. Given -state candidate C = hT, con point P = hC, ti
define formulas C P follows:
C :=

^

xt[x] x

tT

_
tT

t[x]

^

t[x/c]

tc con

P := C
distinguish type conjuction formulas contains.
-state candidate C S-consistent formula C consistent w.r.t. system S,
i.e., 0S C . Similarly, -point P S-consistent formula P consistent w.r.t. S.
refer plain consistency whenever system reference understood. Consistent
state candidates represent states quasimodels. define relations
suitability constitute relational part quasimodels.
Definition 16.
1 -type t1 2 -type t2 -suitable, t1 t2 , iff 1 = 2
t1 t2 consistent. i-suitable, t1 t2 , iff 1 ]i = 2 ]i t1 Ki t2
consistent.
1 -state candidate C1 2 -state candidate C2 -suitable, C1 C2 , iff
1 = 2 C1 C2 consistent. i-suitable, C1 C2 , iff 1 ]i = 2 ]i
C1 Ki C2 consistent.
1 -point P1 2 -point P2 -suitable, P1 P2 , iff 1 = 2 P1
P2 consistent. i-suitable, P1 P2 , iff 1 ]i = 2 ]i P1 Ki P2
consistent.
1 -point P1 = hC1 , t1 2 -point P2 = hC2 , t2 -suitable constant
c con, P1 c P2 , iff P1 P2 , tc1 T1con tc2 T2con . i-suitable
c, P1 ci P2 , iff P1 P2 , tc1 T1con tc2 T2con .
using axioms , 4 5 shown relation reflexive,
transitive symmetric, is, equivalence relation. Also, relation serial.
following lemma list properties relations useful
follows.
Lemma 3.

(i) Let subx , t1 t2 t1 iff t2 .
19

fiBelardinelli & Lomuscio

(ii) Let Ki subx let -type, Ki iff -types t0 , t0 implies
t0 . Moreover, let |]i| ad(), Ki iff ]i-types t0 , t0 implies
t0 .
Proof.
(i) proof similar one Lemma 9(i) work Wolter et al. (2002).
t1
/ t2 t2 since t1 t2 consistent, also
consistent, contradiction. right left, t2
/ t1
t1 . Since t1 t2 consistent, also consistent,
contradiction.
(ii) left right, Ki
/ t0 t0 since Ki t0 consistent,
also Ki Ki consistent, contradiction. right left,
Ki
/ extend set {} { | Ki t} -type t0 . particular,
t0 t0 . Moreover, |]i| ad() similarly prove Ki
iff ]i-types t0 , t0 implies t0 .
present frame underlying quasimodel .
Definition 17 (Frame). frame F tuple hR, D, {i,a }iAg,aD , fi
(i) R non-empty set indexes r, r0 , . . .;
(ii) non-empty set individuals;
(iii) every Ag, D, i,a equivalence relation set points (r, n)
r R n N;
(iv) f partial function associating point (r, n) consistent state candidate
f(r, n) = Cr,n
(a) domain f empty;
(b) f defined (r, n), defined (r, n + 1);
(c) f defined (r, n) (r, n) i,a (r0 , n0 ), f defined (r0 , n0 ).
function f partial take consideration case synchronous systems. Also,
straightforward introduce frames satisfying perfect recall, learning, synchronicity,
unique initial state, following definitions given mf-models. Next,
provide definition objects, correspond runs Gabbay et al. (2003).
choose terminology avoid confusion runs QIS.
Definition 18 (Object). Given individual D, object frame F map
con
associating type (r, n) Tr,n every (r, n) Dom(f) f(r, n) = Cr,n = hTr,n , Tr,n

1. (r, n) (r, n + 1)
2. (r, n) i,a (r0 , n0 ) (r, n) (r0 , n0 )
20

fiInteractions Knowledge Time First-Order Logic MAS

3. U (r, n) iff n0 n (r, n0 ) n00 , n n00 < n0
implies (r, n00 )
4. (r, n) -types, (r0 , n0 ), (r, n) i,a (r0 , n0 ) (r0 , n0 ) =
5. C (r, n) exists point (r0 , n0 ) reachable (r, n)
(r0 , n0 )
object+ satisfies (1), (2), (3), (5) (40 ) instead (4).
(40 ) (r, n) -type, ]i-type (r, n) t, (r0 , n0 ) i,a (r, n),
(r0 , n0 ) = t.
Intuitively, object identifies individual, represented types, across
different state candidates. elements give definition quasimodel.
Definition 19 (Quasimodel). quasimodel tuple Q = hR, O, {i, }iAg,O , fi
hR, O, {i, }iAg,O , fi frame,
1. Tr,n Tr,n Cr,n
2. Cr,n Cr,n+1
3. (r, n) i, (r0 , n0 ) (r, n) (r0 , n0 )
4. every Tr,n exists object (r, n) =
con object O.
5. every c con, function c c (r, n) = tc Tr,n

quasimodel+ defined quasimodel clauses (4) (5) refer objects+
rather objects. define quasimodels (resp. quasimodel+ ) satisfying perfect recall,
learning, synchronicity, unique initial state, assuming corresponding
condition underlying frame. difference objects (resp. quasimodel)
objects+ (resp. quasimodel+ ) purely technical. particular, latter needed
systems satisfying perfect recall learning become apparent Section 5.
following lemma list properties quasimodels useful
follows.
Lemma 4. every quasimodel Q, every object O,
(i) Ki (r, n) iff (r0 , n0 ), (r0 , n0 ) i, (r, n) implies (r0 , n0 ).
(ii) C (r, n) iff points (r0 , n0 ) reachable (r, n) (r0 , n0 ).
Proof.
(i) implication left right follows fact (r0 , n0 ) i, (r, n) implies
(r, n) (r0 , n0 ). implication right left, Ki
/ (r, n)
Lemma 3(ii) -type (r, n) t. Definition 18
(r0 , n0 ), (r, n) i, (r0 , n0 ) (r0 , n0 ) = t.
21

fiBelardinelli & Lomuscio

(ii) implication left right proved induction length path
(r, n) (r0 , n0 ). base case inductive step follow axiom C1.
implication right left follows Definition 18.
state main result section, is, satisfability quasimodels implies
satisfability mf-models. follows quasimodel Q validates formula belongs
every type every state-candidate Q.
Theorem 3. quasimodel (resp. quasimodel+ ) Q monodic formula ,
satisfiable mf-model Mmf . Moreover, Q validates formulas K BF ,
Mmf . Finally, Q satisfies perfect recall, learning, synchronicity,
unique initial state, Mmf .
Proof. proof inspired Lemmas 11.72 12.9 work Gabbay
et al. (2003), consider monodic friendly Kripke models rather standard
Kripke models. First, every monodic formula form Ki , C, 1 U2
introduce k-ary predicate constant Pk k equal 0 1, depending whether
0 1 free variables . formula Pk (x) called surrogate . Given
monodic formula denote formula obtained substituting modal
subformulas within scope another modal operator surrogates.
Since every state candidate C quasimodel Q consistent system
first-order temporal-epistemic logic considered Section 3 based classical first-order
logic, formula C consistent respect first-order (non-modal) logic. Godels
completeness theorem first-order structure = hI, Di, non-empty set
individuals first-order interpretation D, satisfies C , i.e., |= C
assignment variables elements D. intend build mf-model
joining first-order structures. However, possible structures
different domains different cardinalities. solve problem, consider cardinal
number 0 greater cardinality set objects Q define
= {h, | O, < }
Then, (r, n) Q, -type Tr,n
|{h, | (r, n) = t}| =
method described Claim 11.24 Gabbay et al. (2003), expand
first-order structure obtain structure Ir,n = hIr,n , Di domain Ir,n
satisfies Cr,n

|{a | (x) = Ir,n
|= t[x]}| =

So, assume without loss generality first-order structures Ir,n share
domain D, every Tr,n , h, D,

(r, n) = iff Ir,n
|= t[x]

(x) = h, i. Equivalently, Tr,n , (x) = h, D,

(r, n) = { cl | Ir,n
|= [x]}

22

(7)

fiInteractions Knowledge Time First-Order Logic MAS

Moreover, Ir,n (c) = hc , 0i every c con.
define mf-model Mmf tuple hW, R, {i,a }iAg,aD , D, Ii W
set points (r, n) r R Q n N; R set runs N W
r(n) = (r, n); defined above; Ag h, D, i,h,i defined i, ;
interpretation obtained joining various first-order interpretations Ir,n ,
i.e., I(P, r(n)) = Ir,n (P ) every predicate constant P . prove following
result Mmf .
Lemma 5. mf-model Mmf obtained quasimodel Q described above,
every subx ,

Ir,n
|= iff (Mmf , r, n) |=

Moreover, Q quasimodel+ , f(r, n) -state candidate ad(K ) ad()

Ir,n
|= iff (Mmf , r, n) |=

Proof. proof similar Lemma 12.10 work Gabbay et al. (2003).
begin first part. base case induction follows definition interpretation mf-model. step propositional connectives quantifiers follows
induction hypothesis equations 1 2 = 1 2 , 1 = 1 , x1 = x1 .
let = [x] assume (x) = h, i, have:

Ir,n
|= [x]

iff

[x] (r, n)

(8)

iff

[x] (r, n + 1)

(9)

iff


Ir,n+1
|= [x]

(Mmf , r, n + 1) |= [x]
(Mmf , r, n) |= [x]

iff
iff

(10)
(11)

Steps (8) (10) follow Equation (7). Step (9) motivated Lemma 3(i),
step (11) follows induction hypothesis.
Let = (U0 )[x] (x) = h, i, have:

Ir,n
|= (U0 )[x] iff

iff

(U0 )[x] (r, n)
0

(12)
0

0

n n [x] (r, n )
[x] (r, n00 ) n n00 < n0

iff

0

n n



Ir,n
00


Ir,n
0

|=

(13)

0 [x]

00

|= [x] n n < n0
0

iff

n n

iff

(Mmf , r, n00 ) |= [x]
(Mmf , r, n) |= U0 [x]

(Mmf , r, n0 )

(14)
0

|= [x]
00

n n < n0

(15)

Steps (12) (14) follow Equation (7). Step (13) motivated Def. 18, step
(15) follows induction hypothesis.
23

fiBelardinelli & Lomuscio

Let = Ki [x] (x) = h, i, have:

Ir,n
|= Ki [x]

iff
iff

Ki [x] (r, n)
0

0

0

0

(16)
0

0

(r , n ) i, (r, n), [x] (r , n )

(17)

(r , n ) i,h,i (r, n), Ir0 ,n0 |= [x]
(r0 , n0 ) i,h,i (r, n), (Mmf , r0 , n0 )

iff



iff



iff

(Mmf , r, n)

(18)
|= [x]

(19)

|= Ki [x]

Steps (16) (18) follow Equation (7). Step (17) motivated Lemma 4(i), step
(19) follows induction hypothesis.
Let = C[x] (x) = h, i, have:

Ir,n
|= C[x]

iff
iff
iff

C[x] (r, n)
0

0

0

0

0

0

(20)
0

0

(r , n ) reachable (r, n), [x] (r , n )
(r , n ) reachable

(r, n), Ir0 ,n0

(21)

|= [x]

(22)

0

(23)

0

iff

(r , n ) reachable (r, n), (M, r , n ) |= [x]

iff

(M, r, n) |= C[x]

Steps (20) (22) follow Equation (7). Step (21) motivated Lemma 4(ii),
step (23) follows induction hypothesis.
prove second part lemma. cases identical first part,
except = Ki . Suppose f(r, n) -state candidate ad(K ) ad().
implication left right, (r, n) i, (r0 , n0 ) (r0 , n0 ) 0 -type
]i = 0 ]i. Thus, ad(K0 ) ad(K]i ) ad(K Ki ) ad(). So, apply
induction hypothesis. implication right left, ad(K Ki ) ad()
|]i| ad() Lemma 3(ii) ]i-type (r, n)
t. Def. 18 (r0 , n0 ) (r, n) i, (r0 , n0 ) (r0 , n0 ) = t. Since
ad(K]i ) = ad(K Ki ) ad() apply induction hypothesis.
complete proof Theorem 3, definition quasimodel
Tr,n Tr,n Cr,n . Therefore, satisfied mf-model Mmf point (r, n).
also remark Q validates formulas K BF , Mmf . case
as, K BF belong every type every state-candidate Q, Lemma 5
Mmf validates K BF well.
Finally, Q satisfies perfect recall, learning, synchronicity, unique
initial state, mf-model obtained Q satisfies corresponding constraints
construction. show fact perfect recall: (r, n) i,h,i (r0 , n0 ) n > 0,
particular (r, n) i, (r0 , n0 ). Since Q satisfies perfect recall, either (r, n 1) i, (r0 , n0 ),
k < n0 (r, n 1) i, (r0 , k) k 0 , k < k 0 n0 implies
(r, n) i, (r0 , k 0 ). definition i,h,i obtain either (r, n 1) i,h,i (r0 , n0 ),
k < n0 (r, n 1) i,h,i (r0 , k) k 0 , k < k 0 n0 implies
(r, n) i,h,i (r0 , k 0 ), is, Mmf satisfies perfect recall well.
next show existence quasimodels monodic .
24

fiInteractions Knowledge Time First-Order Logic MAS

5. Dealing System
section consider completeness proof system Theorem 2. particular, show monodic formula consistent respect system S,
build quasimodel (or quasimodel+ specific cases) based frame
S. following sections symbol ` represents provability appropriate system
S. start lemmas useful construction quasimodel
system.
Lemma 6. (i) consistent monodic formula consistent -state candidate C = hT, con .
(ii) Let P = hC, ti consistent -point C = hT, con i, let c con.
Then,
(a) C C0 exists -point P0 = hC0 , t0 P P0 .
(b) tc con C C0 exists -point P0 = hC0 , t0 P c P0 .
(c) 1 U2 sequence -points Pj = hCj , tj j k
realises 1 U2 , i.e., P = P0 . . . Pk , 2 tk 1 tj j < k.
(d) 1 U2 tc sequence -points Pj = hCj , tj j k
c-realises 1 U2 , i.e., sequence realises 1 U2 P0 c . . . c Pk .
(e) Ki -point P0 = hC0 , t0 P P0 t0 .
(f ) Ki tc -point P0 = hC0 , t0 P ci P0 t0 .
(g) C sequence -points Pj = hCj , tj j k
P = P0 i0 . . . ik1 Pk tk .
(h) C tc sequence -points Pj = hCj , tj j k
P = P0 ci0 . . . cik1 Pk tk .
Proof. proof similar one Claims 11.75, 11.76 12.13 work
Gabbay et al. (2003), consider -state candidates -points. Let
disjunction formulas P -points P . Consider formula ,
obtained substituting subformulas form Ki , C, 1 U2
within scope another modal operator surrogates. check
true (non-modal) first-order structures. Since QKTm QKTCm
extend first-order logic, semantical completeness first-order logic
`

(24)

W
(i) Notice that, previous remark, ` also = {P|P -point } P .
Moreover, consistent (24) also consistent. Therefore,
disjunct P P consistent. So, P = hC, ti.
(a) (24) Nec ` . So, P consistent must
-point P0 P P0 also consistent.
(b) proof similar (a).
25

fiBelardinelli & Lomuscio

(c) proof contradiction. Let U set -points P0
W exist points Pj = hCj , tj j < k P = P0 . . . Pk = P0 . Let = {P0 |P0 U } P0 .
show
` 2
(25)
otherwise, would sequence realising 1 U2 . Moreover, definition
U,
`
(26)
(25) obtain
` G G2
together (25) (26) derive
` (2 G2 )

(27)

consider P1 U P P1 . (27)
`

P1 (2 G2 )

`

P1 G2

`

(P P1 ) G2

(28)

hand, since 1 U2
` (P P1 ) F 2

(29)

(28) (29) contradict fact P P1 .
(d) proof similar (c).
(e) First remark P Ki ( ) consistent. Thus, exists -point
P0 = hC0 , t0 P Ki (P0 ) consistent. Hence, P P0 t0 .
(f) proof similar (e).
(g) proof contradiction. Let V minimal set -points DWsuch (i)
P V ; (ii) V D0 Ag, D0 V . Let = {D|DV } .
show
`
(30)
(30) hold, would sequence specified lemma. Moreover,
definition V ,
` Ki
(31)
Ag. (30) (31) obtain
` ( E)
axiom C2,
` C
definition P,
` P C
contradicts (32).
26

(32)

fiInteractions Knowledge Time First-Order Logic MAS

(h) proof similar (g).
following result always possible extend -suitability relation
types -suitability points.
Lemma 7. Suppose t0 -types t0 , -points
P = hC, ti P0 = hC0 , t0 P P0 . particular, c con,
-points P = hC, ti P0 = hC0 , t0 P c P0 .
W
Proof. Lemma 6 ` ` = {P|P -point } P .
Since t0 , ( t0 ) consistent. Thus, must -points P
P0 P (P0 t0 ) consistent. Then, case P = hC, ti
P0 = hC0 , t0 -state candidates C C0 . result, P P0 . second part
lemma proved similarly first observing t0 t[x/c] t[x/c]
consistent. Hence, also t[x/c] ( t0 [x/c]) consistent. Thus, must
-points P P0 P t[x/c] (P0 t0 [x/c]) consistent. So, tc con
t0c 0con , is, P c P0 .
According Lemma 7 always extend possibly infinite sequence -types t0
t1 . . . possibly infinite sequence -points P0 P1 . . . Pk = hCk , tk i.
Definition 20. Let -sequence possibly infinite sequence C0 C1 . . . -state
candidates. -sequence acceptable k 0,
(i) 1 U2 tk , tk Ck , 1 U2 realised sequence -points Pj =
hCj , tj k j n;
(ii) 1 U2 tck , tck Ck , 1 U2 c-realised sequence -points Pj =
hCj , tj k j n.
following lemma entails completeness result.
Lemma 8. Every finite -sequence -state candidates extended infinite
acceptable -sequence.
Proof. Assume C0 . . . Cn finite -sequence 1 U2 tk Ck
k n. Either 1 U2 realised C0 . . . Cn , Lemma 6(ii)(c) extend
-sequence 0 realises 1 U2 . procedure repeated formulas
form 1 U2 appearing point -sequence. Thus, obtain (possibly
infinite) -sequence C0 C1 . . . property (i) Definition 20 satisfied.
also satisfy property (ii) reason similarly using Lemma 6(ii)(d) instead.
let X new object, sequence X, . . . , X, Cn , Cn+1 , . . . acceptable n
starts n copies X Cn , Cn+1 , . . . acceptable -sequence.
consider completeness proof single class QIS.
uis
sync,uis
5.1 Classes QIS , QIS sync
, QIS QIS

start completeness proof systems QKTm QKTCm ,
interaction temporal epistemic operators.
27

fiBelardinelli & Lomuscio

monodic formula consistent, Lemma 6(i) consistent -state
candidate C = hT, con type . Also, Lemma 8
extend C infinite acceptable -sequence. So, set infinite acceptable sequences non-empty. Let R set -sequences acceptable n,
n N. r R, k N, define partial function f R N f(r, k) = Ck r
-sequence X, . . . , X, Cn , Cn+1 , . . . acceptable n k n, undefined otherwise.
Finally, let set functions associating every (r, n) Dom(f) type
(r, n) Tr,n
(A) (r, n) (r, n + 1);
(B) U (r, n) iff n0 n (r, n0 ) (r, n00 )
n n00 < n0 ;
(C) (r, n) -types, (r0 , n), (r0 , n) = t;
(D) C (r, n) exists point (r0 , n) sequence -points Pj =
hCj , tj j k, hf(r, n), (r, n)i = P0 i0 . . . ik1 Pk , tk , f(r0 , n) =
Ck (r0 , n) = tk .
show non-empty. Condition (A) guaranteed Lemma 6(ii)(a),
condition (B) fact r acceptable -sequence. regards (C) remark
(r, n) find consistent -point P = hC, ti reasoning similarly
Lemma 6(i), Lemma 8, C extended -sequence r0 acceptable
n. Finally, set (r0 , n) = t. (D) observe C (r, n)
Lemma 6(ii)(g) exists sequence -points Pj = hCj , tj j k,
hf(r, n), (r, n)i = P0 i0 . . . ik1 Pk tk . Now, Ck extended sequence r0 acceptable n (r0 , n) = tk . Finally, Ag, O, define
(r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ) n = n0 .
Lemma 9. tuple hR, O, {i, }iAg,O , fi synchronous frame.
Proof. previously shown R non-empty. Also, i,
equivalence relation definition, f satisfies conditions Definition 17. Further,
frame synchronous definition i, .
prove main result.
Lemma 10. tuple hR, O, {i, }iAg,O , fi synchronous quasimodel
validates formulas K BF .
Proof. previous lemma, remains prove functions objects.
Conditions (1), (3), (4) (5) objects safisfied remarks (A)-(D) above. Condition (2) satisfied definition i, . Furthermore, conditions (1), (2) (3)
quasimodels satisfied definitions R, f i, . regards (4), extend
function (r, n) = Dom(f) using Lemma 6(ii)(a), (c), (e) (g). (5)
function c c (r, n) = tc object Lemma 6(ii)(b), (d), (f) (h).
Finally, Q validates formulas K BF , C, C Q, consistent
QKTm (resp. QKTCm ).
28

fiInteractions Knowledge Time First-Order Logic MAS

completeness QKTm QKTCm respect classes QIS QIS sync

quantified interpreted systems directly follows Lemma 10 together Theorem 3.
Thus, obtain following item Theorem 2.
Theorem 4 (Completeness). system QKTm (resp. QKTCm ) complete w.r.t.
classes QIS QIS sync
QIS.

sync,uis
prove completeness QIS uis
use next result,
QIS
extension propositional case (Halpern et al., 2004).
1 ) satisfiable
Remark 2. Suppose X subset {pr, sync}. L1m (resp. LCm
X
X,uis
QIS also satisfiable QIS .

Thus, system QKTm (resp. QKTCm ) also complete w.r.t. classes QIS uis

sync,uis
QIS
QIS.
pr,uis
5.2 Classes QIS pr
QIS

begin investigate systems interactions time knowledge
pr,uis
present. completeness proof QKT1m respect QIS pr
relies
QIS
following lemma.
Lemma 11. -points P1 = hC1 , t1 i, P2 = hC2 , t2 ]i-type t02 , P1 P2
t2 t02 ]i-point P02 = hC02 , t02 -sequence S1 . . . Sn = P02
]i-points Sk = hDk , sk i, s1 t1 sk t2 1 < k n. Further,
k n.
P1 c P2 sck TDcon
k
Proof. extend proof Halpern et al. (2004, Lemma 5.5) deal state
candidates monodic friendly Kripke frames. cited result prove
t1 t2 t2 t02 sequence ]i-types s1 . . . sn = t02
s1 t1 sk t2 1 < k n. Lemma 7 extend sequence
]i-types sequence ]i-points S1 . . . Sn Sk = hDk , sk
lemmas statement satisfied. particular, P1 c P2 also Lemma 7 assume
without loss generality sck TDcon
k n.
k
consistent L1m define quasimodel+ establish completeness
QKT1m respect QIS pr
. Let R set acceptable -sequences, define
f f(r, k) = Ck r -sequence C0 , C1 , . . . . Finally, let set
functions associating every (r, n) Dom(f) type (r, n) Tr,n conditions
(A) (B) satisfied
(C) (r, n) -type, ]i-type (r, n) t, (r0 , n0 ), (r0 , n0 ) = t.
(E) (r, n) (r0 , n0 ) n > 0 either (a) (r, n 1) (r0 , n0 ) (b)
k < n0 (r, n 1) (r0 , k) k 0 , k < k 0 n0 implies (r, n)
(r0 , k 0 ).
Finally, Ag, O, define (r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ).
following lemma shows set non-empty. particular, conditions (C)
(E) satisfied functions O.
29

fiBelardinelli & Lomuscio

Lemma 12. set functions satisfies conditions (A), (B), (C) (E)
non-empty.
Proof. Conditions (A) (B) follow respectively Lemma 6(ii)(a) fact
r acceptable -sequence. regards (C) (E), proof proceeds induction
n. result n = 0 immediate, take r0 acceptable -sequence
starting C C. Further, define (r0 , 0) = t. Thus, (r0 , 0) (r, 0)
(C) (E) satisfied.
suppose n > 0 result holds n 1. Since f(r, n 1) f(r, n)
(r, n) t, follows Lemma 11 ]i-point P = hC, ti -sequence
]i-points P0 S0 . . . Sk = P Sk0 = hDk0 , sk0 sk0 (r, n) k 0 k.
induction hypothesis, exists every ]i-type (r, n1) point
(r0 , n0 ) (r0 , n0 ) = s. case (a), take = t; (r, n 1) (r0 , n0 )
(r0 , n0 ) = t. Thus, also case (r, n) (r0 , n0 ). case (b), take = t0 .
Hence, (r, n 1) (r0 , n0 ) (r0 , n0 ) = t0 . suppose r0 derived
acceptable -sequence v0 , v1 , . . .. Let r00 run derived acceptable sequence
initial segment v0 , . . . , vn0 , D0 , . . . , Dk . Again, run exists Lemma 8.
define (r00 , n0 + k + 1) = sk = t. Thus, (r, n) (r00 , n0 + k + 1) (C)
(E) satisfied.
prove following lemma.
Lemma 13. tuple hR, O, {i, }iAg,O , fi frame satisfies perfect recall.
Proof. Lemmas 6(i), 8 12 sets R non-empty. Also, f satisfies
conditions Definition 17. Finally, i, equivalence relation definition,
satisfies perfect recall definition functions O.
Finally, prove main result section.
Lemma 14. tuple hR, O, {i, }iAg,O , fi quasimodel+ perfect recall
validates formulas K BF .
Proof. previous lemma hR, O, {i, }iAg,O , fi frame satisfying perfect
recall; left prove functions objects+ . Conditions (1)-(4)
objects+ safisfied remarks (A)-(E) definition i, . Furthermore, conditions
(1), (2) (3) quasimodels+ satisfied definitions R, f i, .
regards (4), follows Lemma 11. Finally, condition (5) quasimodels+ holds
Lemma 6(ii)(b), (d), (f) (h) Lemma 11. Finally, Q validates formulas K
BF , C, C Q, consistent QKTm .
completes proof QIS pr
. Thus, obtain following item Theorem 2.
Theorem 5 (Completeness). system QKT1m complete w.r.t. class QIS pr
QIS.
completeness QKT1m respect QIS pr,uis
follows Remark 2.

5.3 Classes QIS pr,sync
QIS pr,sync,uis


completeness QKT2m respect QIS pr,sync
proved similarly previous

case using following lemma instead Lemma 11.
30

fiInteractions Knowledge Time First-Order Logic MAS

Lemma 15. -state candidates C1 , C2 ]i-state candidate C02 , ]i-state
candidate C01
C1 C2 C2 C02 C1 C01 C01 C02 .
c con, P1 = hC1 , t1 i, P2 = hC2 , t2 P02 = hC02 , t02 i, P1 c P2
P2 ci P02 P01 = hC01 , t01 i, P1 ci P01 P01 c P02 .
Proof. C1 C2 C2 C02 exist t1 C1 , t2 C2 t02 C02
t1 t2 t2 t02 . Moreover, without loss generality assume
0con . Following proof Halpern et
c con, tc1 T1con , tc2 T2con t0c
2 T2
0
al. (2004, Lemma 5.8) find ]i-type t1 t1 t01 t01 t02 . Define T10
0
0
0con
set t01 T10con set t0c
1 . show C1 = hT1 , T1
0
0
0
consistent ]i-state candidate C1 C1 , C1 C2 , c con, P1 ci P01
P01 c P02 .
consistent L1m define quasimodel+ establish complete. Let R set -sequences acceptness QKT2m respect QIS pr,sync

able n, n N, define f f(r, k) = Ck r -sequence
X, . . . , X, Cn , Cn+1 , . . . acceptable n k n, undefined otherwise. Finally, let
set functions associating every (r, n) Dom(f) type (r, n) Tr,n
conditions (A) (B) Section 5.1 satisfied
(C) (r, n) -type, ]i-type (r, n) t, (r0 , n), (r0 , n) = t.
(F) (r, n) (r0 , n) n > 0 (r, n 1) (r0 , n 1).
Finally, Ag, O, define (r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ) n = n0 .
following remark shows set non-empty. particular, conditions (C)
(F) satisfied functions O.
Lemma 16. set functions satisfies condition (A), (B), (C) (F)
non-empty.
Proof. Conditions (A) (B) follow Lemma 6(ii)(a) fact r
acceptable -sequence. regards (C) (F), assume (r, n) f(r, n) -type,
]i-type (r, n) t. f(r, n) different (r, n) consider set
U = { | Ki s}. check U consistent extended ]i-type s0
s0 . define 0 collection s0 . Further, sc con ,
set s0c 0con . Let C0 = hT 0 , 0con i. Clearly, C C0 hC, si ci hC0 , s0 i. Lemma 15
construct -sequence C0 . . . Cn Cn = C0 f(r, k) Ck
k n. Lemma 8 extend -sequence infinite acceptable -sequence
r0 . particular, function extended k n, (r, k) (r0 , k)
(r0 , n) = t. Thus, (C) (F) satisfied.
show following lemma.
Lemma 17. tuple hR, O, {i, }iAg,O , fi frame satisfies perfect recall
synchronicity.
31

fiBelardinelli & Lomuscio

Proof. Lemmas 6(i), 8 16 sets R non-empty. Also, f satisfies
conditions Definition 17. Finally, i, equivalence relation definition,
satisfies perfect recall synchronicity definition functions O.
prove main result.
Lemma 18. tuple hR, O, {i, }iAg,O , fi quasimodel+ perfect recall
synchronicity, validates formulas K BF .
Proof. previous lemma hR, O, {i, }iAg,O , fi frame satisfying perfect
recall synchronicity; left prove functions objects+ .
Conditions (1)-(4) objects+ safisfied remarks (A)-(F) definition
i, . Furthermore, conditions (1), (2) (3) quasimodels+ satisfied
definitions R, f i, . regards condition (4), make use Lemma 15
show holds. Additionally, (5) holds Lemma 6(ii)(b), (d), (f) (h) Lemma
15. Finally, Q validates formulas K BF , C, C Q,
consistent QKTm .
completes proof QKT2m . Thus, obtain following item Theorem 2.
Theorem 6 (Completeness). system QKT2m complete w.r.t. class QIS pr,sync


QIS.
follows Remark 2.
completeness QKT2m respect QIS pr,sync,uis

5.4 Class QIS nl

First, give following definitions, used completeness proof.
Definition 21. -type, t,i conjunction -types t0 t0 .
Similarly, P -point, P,i set -points P0 P P0 .
Definition 22. Two sequences types 0 -concordant n N
(or n may ) non-empty consecutive intervals 1 , . . . , n 01 , . . . , 0n 0
j s0 0j s0 j n.
Two sequences 0 state candidates -concordant C, either
C C 0 , two sequences 0 types 0 respectively
-concordant.
prove completeness QKT3m respect QIS nl
need following lemma,
dual Lemma 11.
Lemma 19. -points P1 = hC1 , t1 i, P2 = hC2 , t2 ]i-type t01 , P1 P2
t1 t01 exists ]i-point P01 = hC01 , t01 -sequence P01 = S1 . . . Sn
]i-points Sk = hDk , sk i, sk t1 k < n, t2 sn . Further, P1 c P2
sck TDcon
k n.
k
Proof. adapting result Halpern et al. (2004, Lemma 5.11) types
prove t1 t2 t1 t01 sequence ]i-types t01 = s0 . . . sn
sk t1 k < n sn t2 . Lemma 7 extend sequence
]i-types sequence ]i-points S1 . . . Sn Sk = hDk , sk i. So,
32

fiInteractions Knowledge Time First-Order Logic MAS

statement lemma satisfied. particular, P1 c P2 Lemma 7
assume without loss generality sck TDcon
k n.
k
pointed Halpern et al. (2004), Lemma 19 sufficient construct
quasimodel+ satisfies learning. fact, given -sequence = C0 , C1 , . . . state candidates ]i-type t00 t0 t00 t0 C0 , Lemma 19 find
-sequence 0 = C00 , C01 , . . . t00 C00 learning satisfied. However,
follow acceptability 0 also acceptable. So, propositional
case, work trees state candidates. Hereafter extend definitions
given Halpern et al. (2004) able deal points monodic friendly Kripke
models.
Definition 23. Let k ad(). k-tree state candidates set -state
candidates || k contains unique -state candidate, i.e., root,
every -point C ,
t0 ]i-type t0 |]i| k ]i-state candidate
C0 t0 C0 ;
= 0 ]i 0 -state candidate C0 0 -type t0 C0
t0 .
Similarly, define k-tree points set -points || k
contains unique -point, every -point P = hC, ti ,
t0 ]i-type t0 |]i| k, ]i-point P0 =
hC0 , t0 ;
= 0 ]i 0 -point P0 = hC0 , t0 t0 .
Intuitively, k-tree view epistemic state quasimodel particular
type t, k steps t. extend -suitability relation k-trees.
Definition 24. Let 0 k-trees state candidates . say f 0
whenever f function associating -state candidate C -type C
finite -sequences -state candidates 0 -types that:
1. f (C) = C0 . . . Ck (a) C = C0 (b) Cj j < k Ck 0 .
Similarly, f (t) = t0 . . . tk (a) = t0 (b) tj Cj j < k
k Ck .
2. Let C t0 C0 C, C0 . t0 f (t) f (t0 ) concordant;
3. least one C sequence f (C) length least 2.
Further, let 0 k-trees points . say f 0 whenever f
function associating -point P finite -sequence -points 0
that:
1. f (P) = P0 . . . Pk (a) P = P0 (b) Pj j < k Pk 0 ;
33

fiBelardinelli & Lomuscio

2. Let P = hC, ti P0 = hC0 , t0 . t0 f (t) f (t0 ) -concordant;
3. least one P sequence f (P) length least 2.
Finally, constant c con, say cf 0 whenever f 0 f (P) =
P0 c . . . c Pk .
Notice given k-tree state candidates root C C, obtain
k-tree points P0 = hC0 , t0 iff C0 . Also, , 0 k-tree state
candidates f 0 , also f 0 0 k-trees points
based 0 respectively.
show obtain acceptable sequences state candidates sequences
trees. Given two sequences -state candidates = C0 , . . . , Ck = C00 , . . .,
finite, fusion defined C0 , . . . , Ck1 , C00 , . . . Ck = C00 . Further,
given infinite sequence = 0 f0 1 f1 . . . k-trees, say sequence
-state candidates compatible exists h N -state candidates
Ch , Ch+1 , . . ., Cj j j h, = fh (Ch ) fh+1 (Ch+1 ) . . .. sequence
acceptable every -sequence compatible infinite acceptable.
basic idea completeness proof define quasimodel+ starting
acceptable sequence . Next introduce definitions lemmas essential
completeness proof.
Given k-tree -point P inductively define formula tree,P
describes k-tree viewpoint P.
Definition 25. P -point, tree,P ::= P . P 0 ]i-point 0 6= 0 ]i

^
tree,P = P
Ki tree,P0
{0 point P0 |t0 t}

0 k-trees, P P0 0 , write (, P) + (0 , P0 )
sequence k-trees 0 , . . . , l functions f0 , . . . , fl1 (a) = 0 f0 . . . fl1
l = 0 ; (b) fj (P) = P j l 2 fl1 (P) = (P, P0 ). Similarly, (, P) c+ (0 , P0 )
(, P) + (0 , P0 ) (a) = 0 cf0 . . . cfl1 l = 0 .
prove following lemma, extends result Halpern et al. (2004, Lemma 5.12)
points.
Lemma 20. Suppose k-tree points P = hC, ti -point || = k,
(a) t0 -type tree,P (t0 ) consistent, k-tree 0
-point P0 = hC0 , t0 0 (, P) + (0 , P0 ) tree0 ,P0 consistent.
Further, tc con (, P) c+ (0 , P0 ).
W
(b) ` tree,P {(0 ,P0 )|(,P)+ (0 ,P0 )} tree0 ,P0
(c) tree,P U 0 consistent, sequence 0 , . . . , l k-trees
points P0 , . . . , Pl (i) Pj j j l; (ii) (0 , P0 ) = (, P); (iii)
(j , Pj ) + (j+1 , Pj+1 ) j < l; (iv) treej ,Pj consistent j < l; (v)
treel ,Pl 0 consistent. Further, tc con (iii) (j , Pj ) c+ (j+1 , Pj+1 )
j < l.
34

fiInteractions Knowledge Time First-Order Logic MAS

Proof. proceed induction k. case k = 0 immediate using standard
arguments tree,P P .
Assume k > 0 = 0 ]i 6= 0 . first prove part (a) = Ki 0 ,
part (b), general case (a), finally (c).
regards part (a) = Ki 0 , note tree,P (t0 Ki 0 ) implies
tree,P Ki P,i UKi ( 0 t0 ,i )
definition k-tree 0 -point P . Let
(k 1)-tree consisting -points | | k 1. axiom KT3 also
tree ,P Ki P,i UKi ( 0 t0 ,i ) consistent, part (c) sequence 0 , . . . , l
(k 1)-trees points P0 , . . . , Pl (i) Pj j j l; (ii) (0 , P0 ) =
( , P ); (iii) (j , Pj ) + (j+1 , Pj+1 ) j < l; (iv) treej ,Pj Ki P,i consistent
j < l; (v) treel ,Pl Ki ( 0 t0 ,i ) consistent.
Again, definition relation + sequence (k 1)-trees 0 , . . . ,
functions f0 , . . . , fm1 (a) = 0 = 0 f0 . . . fm1 = l . Moreover,
(k 1)-points u0 , . . . , um u0 = P , um = Pl , j < m, uj = Pj 0
j 0 j, uj = uj+1 fj (uj ) = uj , uj 6= uj+1 fj (uj ) = (uj , uj+1 ).
show define k-tree 0j extending j j < m. (iv)
uj Ki P,i consistent j < m, uj P. P 0j . Similarly,
um Ki t0 ,i consistent; exists P0 = hC0 , t0 P0 0m . Further,
saturate 0j conditions k-trees satisfied particular 00 = .
show construct fj0 j < m. point S0 = hD0 , s0 0j \ j must
exist point = hD, si j agent j 0 Ag j 0 s0 . Lemma 19
follows exists sequence S0 starting S0 j 0 -concordant fj (S).
Moreover, take Pj = (P) j < 1, Pm1 = (P, P0 ). define fj0
agrees fj j , S0 0j \ j fj0 (S0 ) = S0 .
Notice 00 = construction. > 0 follows immediately definition
(, P) + (m , P0 ) treem ,P0 Ki 0 consistent. = 0 easily
check P0 t0 . Since also t, follows t0 .
define f f (u) = u every u 6= P f (P) = (P, P0 ). (, P) f (, P0 ).
Since also P P0 (, P) + (, P0 ).
second part (a) follows similar line reasoning.
prove part (b), contradiction assume
_
0 tree,P
tree0 ,P0
{(0 ,P0 )|(,P)+ (0 ,P0 )}

V
tree,P {(0 ,P0 )|(,P)+ (0 ,P0 )} tree0 ,P0 consistent. temporal reasoning
must point u
^
tree,P (u
tree0 ,P0 )
(33)
{(0 ,P0 )|(,P)+ (0 ,P0 )}

W
consistent. Note tree0 ,P0 equivalent P0 {0 point P 0 |t t0 } Ki tree0 ,P .
Thus, consistency (33) implies tree 0 (, P) + (0 , u)
35

fiBelardinelli & Lomuscio

exists 0 -point P0 = hC0 , t0 t0 tu
^
tree,P (u Ki (

tree0 ,P0 ))

(34)

{0 |(,P)+ (0 ,P0 )}



+


consistent. part (a)
Vthere exists k tree P (, P) ( , P )
tree ,P u Ki ( {0 |(,P)+ (0 ,P 0 )} tree0 ,P0 ) consistent. means

P = u. Thus contradiction, since tree ,u Ki tree ,P inconsistent.
general case (a) follows (b). Part (c) also follows (b).
following lemma correspondent Lemma 8 k-trees.

Lemma 21. L1m consistent QKT3m , exists acceptable sequence
ad()-trees state candidates belongs root first tree.
Proof. Lemma 8 key part proof consists showing that, given finite
sequence 0 f0 . . . fl1 l d-trees points -point P = hC, ti l
U 0 (resp. t), Lemmas 19 20 extend sequence trees
satisfy acceptability. Specifically, suppose U 0 t. Let include P 0 -points
P0 = hC0 , t0 l |0 | k = ||. Note k-tree. Further, Lemma 20
find sequence 0 , . . . , n k-trees points P0 , . . . , Pn (i) Pj j
j n; (ii) (0 , P0 ) = (, P); (iii) (j , Pj ) + (j+1 , Pj+1 ) j < l; (iv) treej ,Pj
consistent j < l; (v) treen ,Pl 0 consistent. using Lemma 19
extend sequence ad()-trees starting l satisfies U 0 proof
Lemma 20(a). argument similar. Since consistent, must
tree root C C; extend
complete proof.
consistent L1m define quasimodel+ establish completeness
QKT3m respect QIS nl
. Let R consist acceptable -sequences compatible
ad()-tree , function f given f(r, k) = Ck r acceptable
-sequence C0 , C1 , . . .. Further, let set functions associating every (r, n)
Dom(f) type (r, n) Tr,n conditions (A), (B) (C) given previously
satisfied following holds:
(G) (r, n) (r0 , n0 ) either (r, n + 1) (r0 , n0 ) exists k > n0
(r, n + 1) (r0 , k) k 0 , k > k 0 n0 implies (r, n) (r0 , k 0 ).
Finally, Ag, O, (r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ).
previous cases following.
Lemma 22. set functions satisfies conditions (A), (B), (C) (G)
non-empty.
Proof. Conditions (A) (B) guaranteed Lemma 6(ii) fact r
acceptable -sequence respectively. regards (C) (G), assume (r, n)
-type, ]i-type (r, n) t. using proofs Lemmas 20 19
find acceptable -sequence r0 compatible d-tree f(r0 , 0)
(G) satisfied.
show following lemma.
36

fiInteractions Knowledge Time First-Order Logic MAS

Lemma 23. tuple hR, O, {i, }iAg,O , fi frame satisfies learning.
Proof. Lemmas 6(i), 21 22 sets R non-empty. Also, f satisfies
conditions Definition 17. Further, i, equivalence relation definition.
Finally, learning condition satisfied definition functions O.
Lemma 24. tuple hR, O, {i, }iAg,O , fi quasimodel+ satisfies
learning validates formulas K BF .
Proof. previous lemma hR, O, {i, }iAg,O , fi frame satisfying learning;
left prove functions objects+ . Conditions (1)-(4) objects+
safisfied remarks (A)-(G) definition i, . Furthermore, conditions (1), (2)
(3) quasimodels+ satisfied definitions R, f i, . regards (4)
use Lemma 19 show holds. Finally, (5) holds Lemma 6(ii)(b), (d), (f)
(h) Lemma 19. Finally, Q validates K BF , C, C Q,
consistent QKTm .
completes proof QKT3m . Thus, obtain following item Theorem 2.
Theorem 7 (Completeness). system QKT3m complete w.r.t. class QIS nl
QIS.
5.5 Class QIS nl,sync

show QKT4m complete axiomatisation QIS nl,sync
, analogously Lemma 15,

need following.
Lemma 25. -state candidate C1 , C2 ]i-state candidate C01 exists ]i-state
candidate C02
C1 C2 C1 C01 C01 C02 C2 C02 .
c con, P1 = hC1 , t1 i, P2 = hC2 , t2 P01 = hC01 , t01 i, P1 c P2
P1 ci P01 P01 c P02 P2 ci P02 .
Proof. proof similar Lemma 15. C1 C2 C1 C01 exist
t1 C1 , t2 C2 t01 C01 t1 t2 t1 t01 . Moreover, without loss
0con .
generality, assume c con, tc1 T1con , tc2 T2con t0c
1 T1
adapting proof Halpern et al. (2004, Lemma 5.18) find ]i-type t02
t2 t02 t01 t02 . define T20 set t02 T10con set t0c
2.
Clearly, C02 = hT20 , T20con consistent ]i-state candidate C2 C02 , C01 C02 ,
c con, P2 ci P02 P01 c P02 .
systems including axiom KT4m define synchronous version relation
k-trees.
Definition 26. 0 k-trees state candidates sync
0 iff
f
0
f C , f (C) exactly length 2. Similarly, 0
k-trees points sync
0 iff f 0 P , f (P) exactly
f
length 2.
37

fiBelardinelli & Lomuscio

c con, relation cf sync defined similarly. define sync-acceptable
sequence trees acceptable sequence relation substituted
relation sync , is, sequence acceptable every sync -sequence compatible
infinite acceptable. Similarly, given relations + c+ c con,
definitions sync,+ c sync,+ straightforward. state following result,
simplified version Lemma 20. proof analogous Lemma 20,
Lemma 25 used instead Lemma 19.
Lemma 26. Let k-tree points P -point || = k,
(a) t0 -type tree,P (t0 ) consistent, exists k-tree 0
-point P0 = hC0 , t0 0 (, P) sync,+ (0 , P0 ) tree0 ,P0
consistent. Further, tc con (, P) c sync,+ (0 , P0 ).
W
(b) ` tree,P {(0 ,P0 )|(,P)sync,+ (0 ,P0 )} tree0 ,P0
(c) tree,P U 0 consistent, exists sequence 0 , . . . , l k-trees
points P0 , . . . , Pl (i) Pj j j l; (ii) (0 , P0 ) = (, P); (iii)
(j , Pj ) sync,+ (j+1 , Pj+1 ) j < l; (iv) treej ,Pj consistent j < l;
(v) treel ,Pl 0 consistent. Further, tc con (iii) (j , Pj ) c sync,+
(j+1 , Pj+1 ) j < l.
Further, make use Lemma 26 adapt Lemma 21 obtain following result.
Lemma 27. L1m consistent QKT4m , exists sync-acceptable
sequence ad()-trees state candidates belongs root first tree.
consistent L1m define quasimodel+ establish completeness QKT4m respect QIS nl,sync
. Let X new object, sequence

X, . . . , X, Cn , Cn+1 , . . . sync-acceptable n starts n copies X Cn , Cn+1 , . . .
sync-acceptable -sequence compatible ad()-tree . Let R consist sequences sync-acceptable n, n N. function f defined f(r, k) = Ck
r -sequence X, . . . , X, Cn , Cn+1 , . . . sync-acceptable n k n; f(r, k) undefined otherwise. Further, Let set functions associating every (r, n) Dom(f)
type (r, n) Tr,n conditions (A), (B) (C) satisfied following
holds:
(H) (r, n) (r0 , n0 ) (r, n + 1) (r0 , n0 + 1).
Finally, Ag, O, (r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ) n = n0 .
Similarly Lemma 22, show following.
Lemma 28. set functions satisfies conditions (A), (B), (C) (H)
non-empty.
Moreover, following result follows Lemmas 6(i), 27 28.
Lemma 29. tuple hR, O, {i, }iAg,O , fi frame satisfies learning
synchronicity.
38

fiInteractions Knowledge Time First-Order Logic MAS

Finally, adapting proof Lemma 24 state following result.
Lemma 30. tuple hR, O, {i, }iAg,O , fi quasimodel+ learning
synchronicity, validates formulas K BF .
completes proof QKT4m . Thus, obtain following item Theorem 2.
Theorem 8 (Completeness). system QKT4m complete w.r.t. class QIS nl,sync


QIS.
5.6 Classes QIS nl,pr
QIS nl,pr,uis

1
obtain completeness proof QIS nl,pr
combine results shown QIS pr


nl
QIS .
2,3
L1m consistent QKTm
Lemma 21 exists acceptable
sequence ad()-trees belongs root first tree. Let R
set acceptable -sequences suffix compatible ,
function f defined Section 5.2. Further, set functions associating
every (r, n) Dom(f) type (r, n) Tr,n satisfies conditions (A), (B), (C),
(E) (G). Finally, Ag, O, (r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ).
Lemma 31. set functions satisfies conditions (A), (B), (C), (E) (G)
non-empty.
Proof. show conditions (C) satisfied similarly cases
nl
QIS pr
QIS . (C), suppose (r, n) -type f(r, n) ]i-type.
Also, sequence ad()-trees 0 f0 1 f1 . . . state candidates. run r
derived definition -sequence C0 , C1 , . . . suffix CN , CN +1 , . . .
compatible , f(r, n) = Cn . consider two cases.
n N , exists k N Cn k . Lemma 11 exists
-sequence S0 . . . Sh ]i-state candidates Sh S0 , . . . , Sh
-concordant C0 , . . . , Cn . Further, assume Sh k let Sh , Sh+1 , . . .
sequence compatible . consider -sequence S0 S1 . . ..
construction run r0 derived sequence R assume
(r0 , h) = t.
n < N , Lemma 11 exists -sequence S0 . . . Sh ]i-state
candidates Sh S0 , . . . , Sh -concordant C0 , . . . , Cn . Lemma 19
extend sequence -sequence S0 . . . Sk -concordant
C0 , . . . , CN . Since CN N, assume Sk
well. Let Sh , Sh+1 , . . . sequence compatible , consider -sequence
S0 S1 . . .. previous case, run r0 derived sequence R
construction assume (r0 , h) = t.
Lemmas 6(i), 21 31 obtain next result.
Lemma 32. tuple hR, O, {i, }iAg,O , fi frame satisfies perfect recall
learning.
Finally, state following lemma, whose proof follows lines corresponding
nl
proofs QIS pr
QIS Lemma 31.
39

fiBelardinelli & Lomuscio

Lemma 33. tuple hR, O, {i, }iAg,O , fi quasimodel+ satisfies perfect
recall learning, validates formulas K BF .
establishes completeness QKT2,3 . Thus, obtain following item
Theorem 2.
nl,pr
Theorem 9 (Completeness). system QKT2,3

complete w.r.t. class QIS
QIS.

completeness QKT2,3
respect QIS nl,pr,uis
follows following
1
1
remark, whose proof analogous propositional case.
Remark 3. formula L11 satisfiable QIS nl,pr
(resp. QIS nl,pr,sync
) iff
1
1
nl,pr,uis
nl,pr,sync,uis
satisfiable QIS 1
(resp. QIS 1
).
5.7 Class QIS nl,pr,sync

1,4
prove completeness QKTm
respect QIS nl,pr,sync
combine results

nl,pr
obtained QIS previous section QIS nl,sync
QIS pr,sync
.


1,4
1
Specifically, Lm consistent QKTm Lemma 27 construct syncacceptable sequence ad()-trees belongs root first tree. Let R
set sync-acceptable -sequences suffixes compatible ;
function f defined Section 5.2. Further, set functions associating
every (r, n) Dom(f) type (r, n) Tr,n satisfies conditions (A), (B), (C),
(F) (H). Finally, Ag, O, (r, n) i, (r0 , n0 ) iff (r, n) (r0 , n0 ) n = n0 .
adapting proof Lemma 31 means Lemmas 15 25 show
following result.

Lemma 34. set functions satisfies conditions (A), (B), (C), (F) (H)
non-empty.
Lemmas 6(i), 27 34 obtain following result.
Lemma 35. tuple hR, O, {i, }iAg,O , fi frame satisfies perfect recall,
learning synchronicity.
Finally, state following lemma whose proof follows lines corresponding
proofs QIS pr,sync
, QIS nl,sync
Lemma 34.


Lemma 36. tuple hR, O, {i, }iAg,O , fi quasimodel+ satisfies perfect
recall, learning synchronicity, validates formulas K BF .
completes proof QKT1,4
. Thus, obtain following item Theorem 2.
1,4
Theorem 10 (Completeness). system QKTm
complete w.r.t. class QIS nl,pr,sync

QIS.

40

fiInteractions Knowledge Time First-Order Logic MAS

5.8 Classes QIS nl,sync,uis
QIS nl,pr,sync,uis


1,4,5
show system QKTm
complete respect classes QIS nl,sync,uis

nl,pr,sync,uis
QIS
. completeness result follows next remark.

Remark 4. formula Lm valid QIS nl,sync,uis
iff valid QIS nl,pr,sync,uis
.


proof straightforward extension first-order result Halpern et al. (2004,
Proposition 5.22). Given remark axiom KT5 sufficient prove
completeness QKT1,4
respect QIS nl,pr,sync,uis
. result previous
1
1
1,4
nl,pr,sync
section, QKT1 indeed complete respect QIS 1
. desired result follows
Remark 3. Thus, obtain following item Theorem 2.
1,4
Theorem 11 (Completeness). system QKTm
complete w.r.t. classes QIS nl,sync,uis

QIS nl,pr,sync,uis

QIS.


6. Conclusions Work
paper investigated interaction axioms context monodic first-order
temporal-epistemic logic. Specifically, explored classes quantified interpreted systems
satisfying conditions synchronicity, learning, perfect recall, unique
initial state. contribution article concerns provably complete axiomatisation
classes.
results presented extend previous contributions first-order epistemic temporal logic interactions (e.g., see Belardinelli & Lomuscio, 2011, Sturm et al., 2000,
Wolter & Zakharyaschev, 2002), direction previously explored
propositional level (Halpern et al., 2004). findings show characterisation
axioms considered propositional level extended first-order monodic
setting.
temporal-epistemic logic first-order context far mostly attracted theoretical contributions, evidence literature increasingly embraced
applications. instance, active interest verifying artifact-centric systems
first-order modal specifications (Belardinelli, Lomuscio, & Patrizi, 2011a, 2011b;
Deutsch, Hull, Patrizi, & Vianu, 2009; Deutsch, Sui, & Vianu, 2007; Calvanese, Giacomo,
Lenzerini, & Rosati, 2012; Hariri, Calvanese, Giacomo, Masellis, & Felli, 2011).
Given this, remains importance investigate questions pertaining computational aspects formalisms introduced, including decidability computational complexity satisfiability model checking problems. Work far (including
Belardinelli & Lomuscio, 2011; Hodkinson al., 2000; Wolter & Zakharyaschev 2001)
focused fragments interaction present, know literature (Halpern et al., 2004) interactions make problems harder. leave
work, particularly connection addition epistemic modalities
(e.g., explicit algorithmic knowledge, see Halpern & Pucella, 2005), branching-time
modalities. Epistemic variants branching-time CTL well understood propositional level (Meyden & Wong, 2003) first-order extensions yet
explored.
41

fiBelardinelli & Lomuscio

Acknowledgments
research presented supported European Commission Marie
Curie Fellowship FoMMAS (grant n. 235329) STREP Project ACSI (grant
n. 257593), UK Engineering Physical Sciences Research Council Leadership
Fellowship Trusted Autonomous Systems (grant n. EP/I00520X/1).
would like thank anonymous reviewers Mr. Andrew V. Jones valuable
comments paper.

References
Belardinelli, F., & Lomuscio, A. (2009). Quantified epistemic logics reasoning
knowledge multi-agent systems. Artificial Intelligence, 173 (9-10), 9821013.
Belardinelli, F., & Lomuscio, A. (2011). First-order linear-time epistemic logic group
knowledge: axiomatisation monodic fragment. Fundamenta Informaticae,
106 (2-4), 17590.
Belardinelli, F., & Lomuscio, A. (2008). complete quantified epistemic logic reasoning
message passing systems. Computational Logic Multi-Agent Systems, 8th
International Workshop, CLIMA VIII. Revised Selected Invited Papers, Vol. 5056
Lecture Notes Computer Science, pp. 248267. Springer.
Belardinelli, F., & Lomuscio, A. (2010). Interactions time knowledge firstorder logic multi-agent systems. Principles Knowledge Representation
Reasoning: Proceedings 12th International Conference, KR 2010. AAAI Press.
Belardinelli, F., Lomuscio, A., & Patrizi, F. (2011a). computationally-grounded semantics artifact-centric systems abstraction results. Proceedings 22nd
International Joint Conference Artificial Intelligence, IJCAI 2011, pp. 738743.
AAAI Press.
Belardinelli, F., Lomuscio, A., & Patrizi, F. (2011b). Verification deployed artifact systems via data abstraction. Service-Oriented Computing: Proceedings 9th
International Conference, ICSOC 2011, Vol. 7084 Lecture Notes Computer Science, pp. 142156. Springer.
Calvanese, D., Giacomo, G. D., Lenzerini, M., & Rosati, R. (2012). View-based query
answering description logics: Semantics complexity. Journal Computer
System Sciences, 78 (1), 2646.
Cohen, P., & Levesque, H. (1995). Communicative actions artificial agents. Proceedings 1st International Conference Multi-Agent Systems, ICMAS 1995, pp.
6572. AAAI Press.
Degtyarev, A., Fisher, M., & Konev, B. (2003). Monodic temporal resolution. Automated
Deduction: Proceedings 19th International Conference Automated Deduction,
CADE-19, Vol. 2741 Lecture Notes Computer Science, pp. 397411. Springer.
Degtyarev, A., Fisher, M., & Lisitsa, A. (2002). Equality monodic first-order temporal
logic. Studia Logica, 72 (2), 147156.
Dennett, D. (1987). Intentional Stance. MIT Press.
42

fiInteractions Knowledge Time First-Order Logic MAS

Deutsch, A., Hull, R., Patrizi, F., & Vianu, V. (2009). Automatic verification datacentric business processes. Database Theory: Proceedings 12th International
Conference, ICDT 2009, Vol. 361 ACM International Conference Proceeding Series,
pp. 252267. ACM Press.
Deutsch, A., Sui, L., & Vianu, V. (2007). Specification verification data-driven web
applications. Journal Computer System Sciences, 73 (3), 442474.
Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning Knowledge.
MIT Press.
Fagin, R., Halpern, J. Y., & Vardi, M. Y. (1992). machines know?
properties knowledge distributed systems. Journal ACM, 39 (2), 328376.
Gabbay, D., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2003). Many-Dimensional Modal
Logics: Theory Applications, Vol. 148 Studies Logic. Elsevier.
Garson, J. (2001). Quantification modal logic. Gabbay, D., & Guenthner, F. (Eds.),
Handbook Philosophical Logic, Vol. 3, pp. 267323. Reidel.
Halpern, J., & Moses, Y. (1992). guide completeness complexity modal logics
knowledge belief. Artificial Intelligence, 54, 319379.
Halpern, J., van der Meyden, R., & Vardi, M. (2004). Complete axiomatizations reasoning knowledge time. SIAM Journal Computing, 33 (3), 674703.
Halpern, J., & Vardi, M. (1986). complexity reasoning knowledge time.
ACM Symposium Theory Computing, STOC 1986, pp. 304315. ACM Press.
Halpern, J., & Vardi, M. (1989). complexity reasoning knowledge time
1: lower bounds. Journal Computer System Sciences, 38 (1), 195237.
Halpern, J., & Pucella, R. (2005). Probabilistic algorithmic knowledge. Logical Methods
Computer Science, 1 (3).
Hariri, B. B., Calvanese, D., Giacomo, G. D., Masellis, R. D., & Felli, P. (2011). Foundations
relational artifacts verification. Business Process Management: Proceedings
9th International Conference, BPM 2011, Vol. 6896 Lecture Notes Computer
Science, pp. 379395. Springer.
Hodkinson, I. (2002). Monodic packed fragment equality decidable. Studia Logica,
72, 185197.
Hodkinson, I. (2006). Complexity monodic guarded fragments linear real time.
Annals Pure Applied Logic, 138, 94125.
Hodkinson, I., Kontchakov, R., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2003).
computational complexity decidable fragments first-order linear temporal logics.
Proceedings 10th International Symposium Temporal Representation
Reasoning / 4th International Conference Temporal Logic, TIME-ICTL 2003, pp.
9198. IEEE Computer Society Press.
Hodkinson, I., Wolter, F., & Zakharyaschev, M. (2000). Decidable fragment first-order
temporal logics. Annals Pure Applied Logic, 106 (1-3), 85134.
43

fiBelardinelli & Lomuscio

Hodkinson, I., Wolter, F., & Zakharyaschev, M. (2002). Decidable undecidable fragments first-order branching temporal logics. Proceedings 17th IEEE Symposium Logic Computer Science, LICS 2002, pp. 393402. IEEE Computer
Society Press.
Lomuscio, A., & Ryan, M. (1998). relation interpreted systems Kripke
models. Agent Multi-Agent Systems: Proceedings AI97 Workshop
theoretical practical foundations intelligent agents agent-oriented systems,
Vol. 1441 Lecture Notes Artificial Intelligence, pp. 4659. Springer.
McCarthy, J. (1979). Ascribing mental qualities machines. Ringle, M. (Ed.), Philosophical Perspectives Artificial Intelligence, pp. 161195. Harvester Press.
McCarthy, J. (1990). Artificial intelligence, logic formalizing common sense. Thomason, R. (Ed.), Philosophical Logic Artificial Intelligence, pp. 161190. Kluwer
Academic.
Meyden, R. (1994). Axioms knowledge time distributed systems perfect
recall. Proceedings 9th Annual IEEE Symposium Logic Computer
Science, LICS 1994, pp. 448457. IEEE Computer Society Press.
Meyden, R. v., & Wong, K. (2003). Complete axiomatizations reasoning knowledge
branching time. Studia Logica, 75 (1), 93123.
Moore, R. C. (1990). formal theory knowledge action. Allen, J., Hendler, J., &
Tate, A. (Eds.), Readings Planning, pp. 480519. Kaufmann.
Parikh, R., & Ramanujam, R. (1985). Distributed processes logic knowledge.
Logics Programs, Conference Proceedings, Vol. 193 Lecture Notes Computer
Science, pp. 256268. Springer.
Pnueli, A. (1977). temporal logic programs. Proceedings 18th International
Symposium Foundations Computer Science, FOCS 1977, pp. 4657.
Rao, A., & Georgeff, M. (1991). Deliberation role formation intentions.
Proceedings 7th Conference Uncertainty Artificial Intelligence, pp.
300307. Kaufmann.
Sturm, H., Wolter, F., & Zakharyaschev, M. (2000). Monodic epistemic predicate logic.
Logics Artificial Intelligence, European Workshop, JELIA 2000, Vol. 1919
Lecture Notes Computer Science, pp. 329344. Springer.
Sturm, H., Wolter, F., & Zakharyaschev, M. (2002). Common knowledge quantification.
Economic Theory, 19, 157186.
Wolter, F., & Zakharyaschev, M. (2001). Decidable fragments first-order modal logics.
Journal Symbolic Logic, 66 (3), 14151438.
Wolter, F., & Zakharyaschev, M. (2002). Axiomatizing monodic fragment first-order
temporal logic. Annals Pure Applies Logic, 118 (1-2), 133145.
Wooldridge, M. (2000a). Computationally grounded theories agency. Proceedings
International Conference Multi-Agent Systems, ICMAS 2000, pp. 1322. IEEE
Computer Society Press.
44

fiInteractions Knowledge Time First-Order Logic MAS

Wooldridge, M. (2000b). Reasoning Rational Agents. MIT Press.
Wooldridge, M., & Fisher, M. (1992). first-order branching time logic multi-agent
systems. Proceedings 10th European Conference Artificial Intelligence,
ECAI 1992, pp. 234238. John Wiley Sons.
Wooldridge, M., Fisher, M., Huget, M., & Parsons, S. (2002). Model checking multi-agent
systems MABLE. Proceedings 1st International Conference Autonomous Agents Multiagent Systems, AAMAS 2002, pp. 952959. ACM Press.
Wooldridge, M., Huget, M., Fisher, M., & Parsons, S. (2006). Model checking multiagent
systems: MABLE language applications. International Journal Artificial
Intelligence Tools, 15 (2), 195226.
Wooldridge, M. (1999). Verifying agents implement communication language.
Proceedings 16th National Conference Artificial Intelligence 11th Conference Innovative Applications Artificial Intelligence, pp. 5257. AAAI Press.

45

fiJournal Artificial Intelligence Research 45 (2012) 761-780

Submitted 08/12; published 12/12

Evaluating Indirect Strategies ChineseSpanish
Statistical Machine Translation
Marta R. Costa-jussa

vismrc@i2r.a-star.edu.sg

Institute Infocomm Research,
Singapore 138632

Carlos A. Henrquez Q.

carlos.henriquez@upc.edu

Universitat Politecnica de Catalunya,
08034 Barcelona

Rafael E. Banchs

rembanchs@i2r.a-star.edu.sg

Institute Infocomm Research,
Singapore 138632

Abstract
Although, Chinese Spanish two spoken languages world,
much research done machine translation language pair.
paper focuses investigating state-of-the-art Chinese-to-Spanish statistical machine
translation (Smt), nowadays one popular approaches machine
translation. purpose, report details available parallel corpus
Basic Traveller Expressions Corpus (Btec), Holy Bible United Nations (Un).
Additionally, conduct experimental work largest three corpora
explore alternative Smt strategies means using pivot language. Three alternatives
considered pivoting: cascading, pseudo-corpus triangulation. pivot language,
use either English, Arabic French. Results show that, phrase-based Smt system,
English best pivot language Chinese Spanish. propose system
output combination using pivot strategies capable outperforming direct
translation strategy. main objective work motivating involving
research community work important pair languages given demographic
impact.

1. Introduction
Chinese Spanish distant languages many aspects. However, come close
together ranking spoken languages world (Ethnologue, 2012).
Web 2.0 era, content produced users, number native
speakers excellent indicator actual relevance machine translation two
languages. course, factors literacy, amount text published strength
commercial relationships also taken account, factors actually
support idea strategic importance developing machine translation
technologies Chinese Spanish. huge increase volume online contents
Chinese last years, well steady increase commercial relationships
Spanish speaking Latin American countries China two basic examples
supporting fact. Needless say, languages involve many economical interests
c
2012
AI Access Foundation. rights reserved.

fiCosta-jussa, Henrquez & Banchs

(Zapatero, 2010). Nevertheless, two languages seem become far apart
looking bilingual resources.
recently interested gathering collecting ChineseSpanish bilingual
resources research machine translation application purposes. amount bilingual resources currently available specific language pair surprisingly low.
Similarly, related amount work found, within computational linguistic
community, reduced small set references (Banchs, Crego, Lambert, &
Marino, 2006; Banchs & Li, 2008; Bertoldi, Cattoni, Federico, & Barbaiani, 2008; Wang,
Wu, Hu, Liu, Li, Ren, & Niu, 2008). Apart Btec1 corpus available International Workshop Spoken Language Translation (Iwslt) competition (Bertoldi et al.,
2008) Holy Bible datasets (Banchs & Li, 2008), aware Chinese
Spanish parallel corpus suitable training phrase-based (Koehn, Och, & Marcu, 2003)2
statistical machine translation systems two languages, six-language
parallel corpus (including Chinese Spanish) United Nations released
research purposes (Rafalovitch & Dale, 2009).
Using recently released United Nations parallel corpus starting point, work
focuses problem developing Chinese-to-Spanish phrase-based machine translation
technologies limited set bilingual resources. explore evaluate different
alternatives problem hand means pivot-language strategies
languages available United Nations parallel corpus, Arabic, English
French 3 . Existing strategies system cascading, pseudo-corpus generation triangulation implemented compared baseline system built direct
translation approach. follows, briefly describe pivot approaches:
cascaded approach generates Chinese-to-Spanish translations concatenating
system translates Chinese pivot language system translates
pivot language Spanish.
pseudo-corpus approach builds synthetic ChineseSpanish corpus either
translating Spanish pivot side Chinesepivot corpus translating
Chinese pivot side PivotSpanish corpus.
triangulation approach implements Chinese-to-Spanish translation system
combining translation table probabilities Chinesepivot system Pivot
Spanish system.
Additionally, implement evaluate system combination three pivot strategies based minimum Bayes risk (Mbr) (Kumar & Byrne, 2004) technique.
combination strategy capable outperforming direct system.
Besides experimenting different pivot languages compare mentioned approaches, also wanted determine pivot alone gives best results why.
1. Basic Traveller Expressions Corpus.
2. Note phrase-based commonly used refer statistical machine translation systems,
term phrase refers segments one one word usual meaning
multi-word syntactical consitutent, linguistics.
3. Although Russian available Un corpus, discard use proper
preprocessing tools it.

762

fiEvaluating Indirect Strategies ChineseSpanish SMT

Hence, present short comparison amount reordering vocabulary sizes
pivot languages, following study presented Birch et al. (2008) identified
two properties key elements predicting machine translation quality. results
comparisons, together translation quality obtained different approaches, show English best pivot language Chinese-to-Spanish translation
purposes experimental framework.
paper structured follows. Section 2 motivates work intended
bring light investigation Chinese-to-Spanish translation task. Section 3
presents related work Chinese-to-Spanish translation task. Section 4 reports
details main parallel corpora available translation task. Next, section
5 describes main strategies performing Chinese-to-Spanish translation
tested work: direct, cascade, pseudo-corpus triangulation. Section 6 presents
evaluation framework includes corpus statistics, system evaluation
details. Then, section 7 reports experiments (including system combination)
results. Finally, section 8 concludes work proposes new research directions
area.

2. Motivation
Although current web translation systems allow performing translations
Chinese Spanish, quality current Chinese-to-Spanish translations still well
quality achieved language pairs, English Spanish. far
know, much research translation task. main reason may
lack parallel corpora. study intends make progress involve researchers
area ChineseSpanish statistical machine translation by:
1. Listing available parallel corpora ChineseSpanish.
2. Comparing different methodologies performing statistical machine translation: cascaded (Wang et al., 2008), pseudo-corpus generation (Banchs et al., 2006; de Gispert
& Marino, 2006) triangulation (Wu & Wang, 2007).
3. Evaluating best language (among Arabic, English French) generating cascade, pseudo-corpus triangulation Mt ChineseSpanish.
4. Performing output system combination explore new ways improving Chineseto-Spanish translation.

3. Related Work
One first works dealing ChineseSpanish statistical machine translation
presented Banchs et al. (2006). Authors experimented two independent corpora
ChineseEnglish EnglishSpanish translate Chinese Spanish. built
translation systems using so-called Ngram-based approach, differs
phrase-based system mainly translation reordering model (Marino, Banchs,
Crego, de Gispert, Lambert, Fonollosa, & Costa-jussa, 2006).
763

fiCosta-jussa, Henrquez & Banchs

research event recently performed language pair 2008 Iwslt
evaluation campaign (Paul, 2008). evaluation organized two Chinese-to-Spanish tracks.
One focused direct translation one pivot translation
English. best translation results accordingly manual evaluation obtained
far pivot task.
best systems tracks developed Wang et al. (2008). Regarding
direct system, used standard phrase-based Smt system. makes different
participating systems provide Chinese segmentation
Ldc (Linguistic Data Consortium) bilingual dictionary. Regarding pivot task,
compared two different approaches. first one, referred triangulation, consisted
training two translation models ChineseEnglish corpus EnglishSpanish corpus,
building new translation model ChineseSpanish translation combining
two previous models proposed Wu & Wang (2007); second one obtained better
results based cascaded approach. idea translate Chinese
English English Spanish, means performing two translations.
participants also proposed cascaded methodology. approximation
done n-best translations (Khalilov, Costa-Jussa, Henrquez, Fonollosa, Hernandez,
Marino, Banchs, Chen, Zhang, Aw, & Li, 2008).
Another proposal generate pseudo-corpus means translate either
English Chinese Spanish, creating parallel ChineseSpanish corpus.
pseudo-corpus used train ChineseSpanish translation (Bertoldi et al., 2008).
mentioned aboved, comparison performed Wang et al. (2008) showed
cascaded approach performed better phrase-table combination Chinese
Spanish pivot task.
Finally, previous work (Costa-jussa, Henrquez, & Banchs, 2011b) compared two
standard pivot approaches (pseudo-corpus cascaded) using English direct system. Experiments work showed quality direct system
pivot systems differ much. Additionally, cascaded system presented slightly
better results pseudo-corpus system. previous work (Costa-jussa,
Henrquez, & Banchs, 2011a), compared two pivot approaches (pseudo-corpus
cascaded) using Arabic, French English pivot languages direct system.
concluded English best pivot language.
present work, extending two previous studies by: (1) using pivot
strategies (including triangulation strategy); (2) introducing measure pre-evaluate
quality pivot approaches; (3) extending pivot combination experiments; (4)
providing evaluation.
Note working United Nations (Un) corpus rather
Btec corpus (the one used Iwslt). former freely available larger
latter.

4. ChineseSpanish Parallel Corpora
limited resources language pair ChineseSpanish comparison
number native speakers languages. practice, also common translate
Chinese Spanish English even manual translations conducted.
764

fiEvaluating Indirect Strategies ChineseSpanish SMT

parallel corpus sentence level, Basic Travel Expressions Corpus
(Btec) (Paul, Yamamoto, Sumita, & Nakamura, 2009), collection sentences
bilingual travel experts consider useful people going coming another
country. corpus contains around 160,000 parallel sentences around 20,000
sentences 180,000 words actively used Mt purposes Iwslt evaluation
campaign. full corpus freely available, 20,000 version available
participation purposes 2008 Iwslt evaluation campaign.
Another parallel corpus Holy Bible, proved good resource
CLIR (Cross-language information retrieval) (Chew, Verzi, Bauer, & McClain, 2006).
corpus contains around 28,000 parallel sentences around 800,000 tokens per language.
main advantages using corpus worlds translated book;
covers variety literary styles including narrative, poetry, correspondence; great care
taken translations; and, perhaps surprisingly, vocabulary appears
high rate coverage (as much 85%) modern-day language.
Finally, United Nations multilanguage corpus (Rafalovitch & Dale, 2009),
freely available online research purposes. Among others, contains parallel
texts sentence level following languages: Chinese, English, Spanish, French
Arabic. consists 2100 United Nations General Assembly resolutions translation
six official languages United Nations, average around 3 million tokens per
language. material using work. Table 1 shows statistics
three different corpora corresponding languages.
Corpus
Btec
Holy
Bible

Un

Lang.
Chinese
English
Spanish
Chinese
English
Spanish
Chinese
English
Spanish
Arabic
French

Sent.
20
20
20
30
30
30
60
60
60
60
60

Words
164
182
147
814
908
836
1,750
2,080
2,380
2,720
2,380

Vocab.
8
8
17
13
12
27
18
15
20
17
18

Avg. sent. length
6
7
9
26
29
27
28
34
39
44
39

Table 1: Available corpora ChineseSpanish (all figures given thousands, except
average sentence length)

Additionally, surf web find several publications available
Chinese Spanish e.g. Global Asia Magazine (2012), additional material consists mainly comparable corpora rather parallel corpora. comparable material
cannot directly used statistical machine translation system. However,
many nice algorithms extract parallel corpora comparable corpora (Moore,
2002; Senrich, 2010; Abdul-Rauf, Fishel, Lambert, Noubours, & Sennrich, 2012).
765

fiCosta-jussa, Henrquez & Banchs

5. Direct Pivot Statistical Machine Translation Approaches
several strategies follow translating pair languages
statistical machine translation (Smt). section present details ones
using work.
general, statistical machine translation system relies translation source
language sentence target language sentence t. Among possible target language
sentences choose one highest probability, show equation (2):

= arg max [P (t|s)]

(1)



= arg max [P (t) P (s|t)]


(2)

probability decomposition based Bayes theorem known source-channel
approach statistical machine translation (Brown, Cocke, Della Pietra, Della Pietra, Jelinek, Lafferty, Mercer, & Roossin, 1990). allows model independently target
language model P (t) source translation model P (s|t). one hand, translation model weights likely words foreign language translation words
source language; language model, hand, measures fluency hypothesis
t. search process represented arg max operation.
Later on, variation proposed Och & Ney (2002) named log-linear model.
allows using two models features weight independently
seen equation (3):
= arg max


"
X

#

hm (s, t)

(3)

m=1

equation interpreted maximum-entropy framework. see eq.
(2) special case eq. (3). fact, logarithm (2) would similar
(3). Then, identify h1 (s, t) log(p(t)) h2 (s, t) log(p(s|t)),
taking = 2 (two models) 1 = 2 = 1. general case, obtained
maximizing objective function held-out set (development set).
Among additional features used log-linear model lexical
models, word bonus, reordering model. lexical models particularly useful
cases translation model may sparse. example, phrases may
appeared times translation model probability may well estimated. Then,
lexical models provide probability among words (Koehn et al., 2003). word bonus
used compensate language model benefits shorter outputs. reordering
model used provide reordering phrases. not, reordering would
treated internally phrase. Finally, mentioned name log-linear
clearly misnomer many features logarithms all.
regards reordering model, standard way implementing distancebased model gives linear cost depending reordering distance. instance,
consecutive target words t1 , t2 come translating source words s1 s5 ,
sub-scripts indicate word position corresponding sentences, movement
766

fiEvaluating Indirect Strategies ChineseSpanish SMT

Figure 1: Word alignment two sentences
= 5 1 = 4 words taken place cost double movement
= 2 words. visual representation phrases seen Figure 1
Besides traditional distance-based reordering mentioned before, state-of-the-art systems implement additional lexicalized reordering model (Tillman, 2004). lexicalized
reordering model classifies phrases movement make relative previous used
phrase, i.e., phrase model learns likely followed previous phrase
(monotone), swapped (swap) connected (discontinuous). instance,
considering sub-scripts word positions corresponding sentences, Figure
1 bilingual phrases (s1 t1 ) (s5 t2 ) connected, (s7 t6 ) followed
(s6 t5 ) (s2 t4 ) swapped (s3 s4 t3 ).
5.1 Direct System
direct system uses phrase-based translation approach (Koehn et al., 2003).
basic idea segment given source sentence segments one words,
source segment translated using bilingual phrase obtained training
corpus finally compose target sentence phrase translations. bilingual
phrase pair source words n target words extracted parallel sentence
belongs bilingual corpus previously aligned words. extraction, consider
words consecutive source target sides consistent
word alignment. consider phrase consistent word alignment
word inside phrase aligned one word outside phrase.
Regarding segmentation sentence K phrases, assume possible
segmentations (which considered hidden variable ) probability
(t):

P (s|t) =

X

P (s, |t)

(4)

P (M |t)P (s|t)

(5)



=

X


= (t)

X

P (s|t)

(6)



Then, consider monotone translations phrase sk produced tk (Zens,
Och, & Ney, 2002).

P (s|t) =

K

k=1

767

p(sk , tk )

(7)

fiCosta-jussa, Henrquez & Banchs

Finally, phrase translation probabilities estimated relative frequencies
bilingual phrases corpus.

p (s|t) =

N (s, t)
N (t)

(8)

N (s, t) counts number times phrase translated N (t)
number times phrase target language appears training corpus.
5.2 Pivot-Based Systems
cascaded approach handles sourcepivot pivottarget system independently. built tuned improve local translation quality
composed translate source language target language two steps: first,
translation output source pivot computed used obtain
target translation output.
pseudo-corpus approach translates pivot section sourcepivot parallel corpus target language using pivottarget system built previously. Then,
sourcetarget Smt system built using source side translated pivot side
sourcepivot corpus. pseudo-corpus system tuned using original sourcetarget
development corpus, since available.
triangulation approach combines sourcepivot (P (s|p) P (p|s)) pivot
target (P (p|t) P (t|p)) relative frequencies following strategy proposed Cohn &
Lapata (2007) order build sourcetarget translation model. translation probabilities computed assuming independence source target phrases
given pivot phrase.

P (s|t) =

X

P (s|p)P (p|t)

(9)

P (t|p)P (p|s)

(10)

p

P (t|s) =

X
p

s, t, p represent phrases source, target pivot language respectively.
lexical weights computed similar manner, following strategy proposed
Cohn & Lapata (2007). approach handle lexicalized reordering
pivot strategies therefore represents limitation potential. Instead,
simple distance-based reordering applied decoding. model gives cost linear
reordering distance. instance, skipping two words costs twice much
skipping one word.
corresponding translation model obtained, sourcetarget system
tuned using original sourcetarget development corpus mentioned previous
approach.
768

fiEvaluating Indirect Strategies ChineseSpanish SMT

6. Evaluation Framework
following section introduces details evaluation framework. report
statistics Un corpus, description built systems evaluation
details.
6.1 Corpus Statistics
far know, discussed section 4, three parallel corpora available
ChineseSpanish language pair: Btec, Holy Bible Un.4 former used
2008 Iwslt complete experiments pivot strategies reported works
Bertoldi et al. (2008). Holy Bible used similar purposes Henrquez,
Banchs & Marino (2010).
study decide use Un corpus taking advantage fact
largest corpus (among three) contains sentences six languages,
therefore experiment different pivot languages.
experimenting different pivot languages, order make systems
comparable possible, first sentence selection corpus systems
built exactly training, tuning testing sets. selection process
follows:
1. corpora tokenized, using standard tokenizer available Moses (Koehn,
Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer,
Bojar, Constantin, & Herbst, 2007) Spanish, English French; ictclass (Zhang,
Yu, Xiong, & Liu, 2003) Chinese; Mada+Tokan (Habash & Rambow, 2005)
Arabic.
2. Spanish, English French corpora lowercased.
3. sentence 100 words language, deleted corpora.
4. sentence pair word ratio larger three Chinesepivot pivot
Spanish parallel corpora, deleted corpora.
5. extract tuning test sets identified sentences ocurring
corpora languages. tuning testing sets drawn sentences
assure appear training corpus. Additionally, sentences,
want select differ sentences training set
lowest out-of-vocabulary rate. order this, perplexity
English language model computed sentence-by-sentence basis using
leave-one-out strategy; then, selected two thousand sentences
highest perplexity lowest ratio out-of-vocabulary words constructing
tuning testing sets. highest perplexity criterion used order
avoid tuning test sentences similar ones training set.
lowest out-of-vocabulary words criterion used minimize number outof-vocabulary words tuning test translation. two criteria used
4. review process paper, aware new corpus KDE (K Desktop Environment), available recent OPUS project 5

769

fiCosta-jussa, Henrquez & Banchs

sequentially, first selected sentences highest perplexity, among
them, selected lowest ratio out-of-vocabulary words.
Table 2 shows main statistics corpora used divided experimentation.
Dataset

Train

Dev.

Test

Lang.
Chinese
Spanish
English
Arabic
French
Chinese
Spanish
English
Arabic
French
Chinese
Spanish
English
Arabic
French

Sent.
58
58
58
58
58
1
1
1
1
1
1
1
1
1
1

Words
1,700
2,300
2,000
2,600
2,300
33.0
43.4
37.4
48.8
44.1
33.7
44.2
38.1
49.3
44.9

Vocab.
17
20
14
17
18
3
5
4.2
4.6
5
3.8
5
4.2
4.6
5

Table 2: Un Corpus Statistics used research (all figures given thousands)

6.2 System Implementation Evaluation Details
systems build using revision 4075 Moses (Koehn et al., 2007). systems,
used default Moses parameters includes grow-diagonal-final-and word
alignment symmetrization, lexicalized reordering (where possible), relative frequencies,
lexical weights phrase bonus translation model (with phrases length 10),
5-gram language model using Kneser-Ney smoothing word penalty model. Therefore,
14 different features combined equation (3). language model built using
Srilm (Stolcke, 2002) version 1.5.12. optimization done using Mert (Och, 2003).
word aligning used Giza++ (Och & Ney, 2000) version 1.0.5.
order evaluate translation quality, used Bleu (Papineni, Roukos, Ward,
& Zhu, 2001), Ter (Snover, Dorr, Schwartz, Micciulla, & Makhoul, 2006) Meteor
(Banerjee & Lavie, 2005) automatic evaluation metrics.
Additionally, significance tests performed study system better
other. tests followed pair bootstrap resampling method presented Koehn
(2004): Given two translation outputs coming two different systems, created two
new virtual test sets drawing sentences replacement translation outputs.
obtained them, computed Bleus observed system performs
better. procedure repeated 1, 000 times. end, one systems outperformed 99% time, concluded indeed better Bleu score
99% statistical significance.
770

fiEvaluating Indirect Strategies ChineseSpanish SMT

7. ChineseSpanish Machine Translation Strategies
Given different languages available Un corpora, tested three different language
pivots. Additionally, compared cascaded, pseudo-corpus triangulation pivot
strategies. Finally, tried combine system outputs improve translation.
7.1 Experimenting Different Pivot Languages
built compared several translation approaches order study impact
different pivot languages translating Chinese Spanish. Moreover,
evaluated quality pivot approaches differs direct system. built
pivot systems using five languages available Un parallel corpus: English,
Spanish, Chinese, Arabic French, built direct system ChineseSpanish
parallel corpus.
particular, experimented following Chinese-to-Spanish systems: direct
Chinese-to-Spanish system quality upper bound; three cascaded, three pseudo-corpus
three triangulation approaches, using English, Arabic French pivots. order
build pivot systems, need corresponding Chinesepivot pivotSpanish
systems.
Table 3 shows Bleu, Ter Meteor scores achieved intermediate
systems trained Un Corpus later used built different pivot approaches. Meteor score Chinese-to-Arabic system shown
postprocessing tools required language.

ChineseEnglish
ChineseArabic
ChineseFrench
EnglishSpanish
ArabicSpanish
FrenchSpanish

Bleu
35.67
46.11
28.31
51.22
41.79
46.42

Ter
51.07
56.12
63.21
32.02
44.37
40.25

Meteor
36.77

47.35
70.12
60.22
64.76

Table 3: Pivot Systems.
Table 4 shows results Chinese-to-Spanish configurations Un corpus.
see best pivot system used pseudo-corpus approach English
pivot language.
Chinese-to-Spanish, fact pseudo-corpus English outperforms
cascaded English according Bleu score statistically significant,
99% confidence (Koehn, 2004). results, however, coherent previous works
using language pair (Bertoldi et al., 2008; Henrquez Q. et al., 2010) also
reported pseudo-corpus strategy better cascaded strategy. cascaded
pseudo-corpus approaches English statistically significantly better
triangulation approach, 99% confidence. best knowledge, reasons
one pivot approach better reported literature. Moreover,
given difference among approaches pseudo-corpus cascaded approaches
771

fiCosta-jussa, Henrquez & Banchs

Languages

System

Bleu

Ter

Meteor

Pivot vocab.

ChineseSpanish
ChineseEnglishSpanish
ChineseFrenchSpanish
ChineseArabicSpanish
ChineseEnglishSpanish
ChineseFrenchSpanish
ChineseArabicSpanish
ChineseEnglishSpanish
ChineseFrenchSpanish
ChineseArabicSpanish

direct
cascaded
cascaded
cascaded
pseudo
pseudo
pseudo
triangulation
triangulation
triangulation

33.06
32.90
30.37
28.88
32.97
32.61
32.23
32.05
30.41
30.61

57.32
56.67
60.33
60.37
57.39
57.43
57.47
57.91
59.70
59.53

53.96
54.06
50.96
50.15
53.99
53.55
53.27
53.37
51.51
51.43

14k
18k
17k
14k
18k
17k
14k
18k
17k

Table 4: Chinese-to-Spanish cascaded, pseudo-corpus triangulation approaches.
significant, better perform experiments particular task language
pair.
three approaches, according scores table 4 English best pivot
language, statistical signicance 99%, coherent pivotSpanish
results table 3.
follows, use procedure predict suitable pivot language justify
language may better pivot another. example, pivot vocabulary
sizes play important role. Birch et al. (2008) concluded study target
vocabulary size negative impact translation quality measured Bleu
score seen Arabic French larger vocabulary size
English.
Apart vocabulary size, research mentioned also measured success
machine translation terms word reordering, i.e., differences word order occur
parallel corpus, mainly driven syntactic differences languages.
order measure reordering translation assumed reordering occurs
two adjacent blocks source side. simplification allowed detect
extract reordering deterministic way.
block defined Birch et. al. (2008) segment consecutive source words
(source span) aligned set target words. target words also form block
. definition block set, formally defined reordering r two blocks
B adjacent source, relative order blocks source
reversed target reordering consistent. reordering blocks
Bs consistent block Cs , consisting union blocks Bs , also
consistent. block said consistent span defined corresponding
target block contain words aligned source words outside .
definition consistent block equivalent definition phrase phrase-based
machine translation paradigm. Finally, set reorderings r sentence defined
R unique given pair sentences. Summarizing, concept reordering
equivalent swap movement described lexicalized reordering end
section 5.
772

fiEvaluating Indirect Strategies ChineseSpanish SMT

Languages
ChineseEnglishSpanish
ChineseFrenchSpanish
ChineseArabicSpanish

SourcePivot
0.3955
0.6200
0.6921

PivotTarget
0.2124
0.0170
0.0908

Average
0.3039
0.3185
0.3914

Table 5: Chinese-to-Spanish RQuantity metrics depending pivot used.

concepts defined, developed metric called RQuantity, defined
sentence level metric averaged corpus:
rR |rAs |

P

RQuantity =



+ |rBs |

(11)

R set reorderings sentence, source sentence length, B
two blocks involved reordering, |rAs | size span block
source side |rBs | size span block B source side (Birch et al., 2008).
objective RQuantity measure amount reordering need
translating source language target language. minimum RQuantity
given sentence 0 translation involve word movement maximum
P
( Ii=2 i)/I words translation inverted compared order
source sentence.
computed RQuantity different language pairs involved pivot
approaches. seen table 5 English appears best pivot
lowest average RQuantity three, i.e. pivot needs
least amount reordering average achieve final translation. French Arabic
required less movements translate Spanish English, lot reordering
needed obtain first step Chinese, hence penalizing average. result
coherent conclusion obtained Birch et al. (2008), says amount
reordering also negative impact Bleu score.
results support intuitive idea English works good intermediate
step Chinese Spanish. French Arabic vocabulary
closer size Spanish reorderings also complex English
first step, making source-to-pivot translation harder candidates.
gradual increase difficulty (measured target vocabulary size reordering) presented
English seems benefit global result.
Nevertheless, also possible Un texts authored English, then,
translated languages. would also favour English best pivot
language.
order observe benefits pivot language direct translation,
table 6 presents three examples Bleu scores pivot approach better
direct approach. Notice phrases disappeared
direct translation correctly appear pseudo-corpus approach.
773

fiCosta-jussa, Henrquez & Banchs

DIRECT
PSEUDO
REF
EN REF
DIRECT
PSEUDO
REF
EN REF
DIRECT
PSEUDO
REF
EN REF

cuestiones como que consideren seriamente la posibilidad de ratificar la tortura otros tratos
penas crueles , inhumanos degradantes
como cuestiones que consideren seriamente la posibilidad de ratificar la convencion contra
la tortura otros tratos penas crueles , inhumanos degradantes
considere seriamente la posibilidad de ratificar , con caracter prioritario , la convencion
contra la tortura otros tratos penas crueles , inhumanos degradantes
seriously consider ratifying , matter priority , convention torture
cruel , inhuman degrading treatment punishment
habiendo examinado el segundo informe de la comision la recomendacion que figura en el
habiendo examinado el segundo informe de la comision de verificacion de poderes las
recomendaciones que figuran en el
habiendo examinado el segundo informe de la comision de verificacion de poderes la
recomendacion que figura en el
considered second report credentials committee recommendation
contained therein
pide al secretario general que prepare un informe sobre la aplicacion de esta resolucion la
asamblea general , quincuagesimo sexto perodo de sesiones
pide al secretario general que prepare un informe sobre la aplicacion de la presente resolucion
para su examen por la asamblea general en su quincuagesimo sexto perodo de sesiones
pide al secretario general que prepare un informe sobre la aplicacion de la presente resolucion ,
que sera examinado por la asamblea general en su quincuagesimo sexto perodo de sesiones
requests secretary-general prepare report implementation present resolution
consideration general assembly fifty-sixth session .

Table 6: Chinese-to-Spanish examples pseudo-corpus system (through English) better direct system. En ref English reference
sentence

7.2 Pivot Combination
Using 1-best translation output different pivot strategies, built n-best
list computed final translation using minimum Bayes risk (Mbr) (Kumar & Byrne,
2004).
translating sentence s, obtain translation t0 evaluated
reference measure systems performance. Mbr focuses finding best
performance possible translations. so, uses loss function LF(t, t0 )
measures loss obtaining hypothesis t0 instead real translation t. Bayes
Risk defined expected value loss function possible hypotheses t0
translations t.
E(LF) =

X

LF(t, t0 )p(t0 |s)

(12)

t,t0

p(t0 |s) translation probability hypothesis t0 given source sentence
obtained decoder, approximation real probability distribution.
objective finding best performance possible translation therefore
minimize Bayes Risk. Given loss function distribution, decision rule
minimizes Bayes Risk (Bickel & Doksum, 1977; Goel & Byrne, 2000) given by:
= arg min
0


X

LF(t, t0 )p(t0 |s)



774

(13)

fiEvaluating Indirect Strategies ChineseSpanish SMT

Mbr used literature decoding (Ehling, Zens, & Ney, 2007)
postprocess n-best list. instance, last approach used Khalilov et al.
(2008) together cascaded approach order obtain best ChineseSpanish
translation. current version Moses toolkit includes implementations.
Mbr algorithm implemented Moses postprocess uses 1Bleu(t, t0 ) loss
function. experiment, consider hypothesis equally likely therefore
p(t0 |s) positive constant therefore could discarded. end, Mbr chooses
hypotheses fullfills:


= arg min
0



X

1 Bleu(t, t0 )

(14)

t6=t0

Different n-best lists built compare different Mbr outputs: cascaded Mbr
using three pivot languages (hence n = 3, one hypothesis per pivot), pseudo-corpus
Mbr using three pivot languages (n = 3), triangulation Mbr using three
languages (n = 3), combination cascaded, pseudo-corpus triangulation outputs
using two languages (n = 6, one hypothesis per pivot strategy) another using
(n = 9). important mention n-best lists must least 3 hypothesis
per sentence. two hypothesis would work expected Loss
Function would always choose longest one, explained definition
Bleu:
N
X

!

pn (t, t0 )
Bleu(t, t0 ) = exp
log
(t, t0 )
N
n=1

(15)

pn (t, t0 ) precision n-grams hypothesis t0 reference t; (t, t0 )
brevity penalty disfavouring translation t0 shorter reference t.
pn (t, t0 ) = pn (t0 , t)
t, t0 : length(t) > length(t0 )



(16)

1 Bleu(t, ) 1 Bleu(t , t)

(17)

0

0

Tables 7 8 show results different ChineseSpanish output systems (from
table 4) combined Mbr technique. tables, observed
combinations pivot strategies obtained better results metrics direct
approach. case Ar + Fr, combination statistically significantly
better direct system terms Bleu score (with 99% confidence).
Mbr cascaded triangulation approach (1st row, 2nd 4th column, respectively, table 8) outperform direct system.
Finally, D+A (which combine languages pivot system outputs
table 4 including direct approach) best Chinese-to-Spanish systems.
experimented reverse translation direction (from Spanish Chinese) would unable assess subjective evaluations resulting translation
outputs. However, reversed direction, intuition reordering difficulties
moved pivottarget step cascade system.
775

fiCosta-jussa, Henrquez & Banchs

En+Fr
En+Ar
Ar+Fr


33.58*/56.34/54.48
33.53*/56.15/54.63
33.14/56.83/53.95

Table 7: Chinese-to-Spanish percent Bleu/Ter/Meteor scores system combinations
two languages pivot approaches using Mbr. (*) statistically significant
better Bleu direct system.


D+A

Cascaded
32.66/57.27/53.34
33.60*/56.72/54.38

Pseudo
33.30*/57.04/53.91
33.77*/56.52/54.47

Triangulation
31.84/58.12/53.05
32.90/57.01/54.03


33.97*/56.00/54.87
34.09*/55.88/55.02

Table 8: Chinese-to-Spanish percent Bleu/Ter/Meteor scores system combinations
En + Fr + Ar languages (A), direct system (D) pivot approaches using
Mbr. (*) statistically significant better Bleu direct system.

Regarding fact English best pivot language task consideration, argue English might constitute better intermediate step
Spanish Chinese, rather French Arabic, based assumption Spanish
closer French (both romance languages derived Latin) Arabic (the
Iberian peninsula occupied Arabic culture 500 years, Spanish
strong influence Arabic) Chinese French Arabic. sense, English
seems represent optimal intermediate point Chinese Spanish,
translation complexity divided two phases. reordering burden resolved
Chinese-to-English phase morphology generation burden resolved
English-to-Spanish phase. Thinking translation-space non-conservative field,
say English middle way Chinese Spanish,
passing French Arabic implies larger path kind detour proximities Spanish. conjecture, course, nicely explains
observing. Definitively, research needed better understand happening.

8. Conclusions
work provided brief survey state-of-the-art ChineseSpanish Smt. First
all, language pair great interest economically culturally take
account high number Chinese Spanish speakers. Besides, statistical machine
translation popular approach field Mt given shown great
quality international evaluation campaigns Nist (2009) Wmt (2012).
main points covered study were:
mainly three ChineseSpanish parallel corpora (Btec, Holy Bible Un)
freely available research purposes.
776

fiEvaluating Indirect Strategies ChineseSpanish SMT

English best pivot language conducting Chinese-to-Spanish translations
compared languages French Arabic. system built using English
pivot significantly better ones built French Arabic, 99%
confidence comparisons.
preference pivot language appears correlated proposed
translation-quality prediction metrics differences vocabulary sizes
amount reordering. According conclusion, best pivot language
English lowest increase vocabulary size lowest increase
reordering complexity.
significant difference found among best cascaded pseudo-corpus pivot
approaches, pseudo-corpus strategy best pivot strategy Chineseto-Spanish. Additionally, pseudo-corpus cascaded approaches significantly
better triangulation approach.
output combination using Mbr able improve direct system 1 Bleu
point best case. improvement significantly better 99% confidence
coherent improvements evaluation metrics studied.
future research plan work problem automatically extracting parallel
corpus comparable corpora collected web. Additionally, intend develop
freely available ChineseSpanish translation system would allow collecting user
feedback. Then, work special techniques incorporate knowledge
Smt system.

Acknowledgments
authors would like specially thank reviewers comments helped
lot improve work. Additionally, authors would like thank Universitat
Politecnica de Catalunya Institute Infocomm Research support
permission publish research.
work partially funded Seventh Framework Program European
Commission International Outgoing Fellowship Marie Curie Action (IMTraP2011-29951); Spanish Ministry Economy Competitiveness FPI
Scholarship BES-2008-003851 Ph.D. students AVIVAVOZ project (TEC200613694-C03-01); BUCEADOR project (TEC2009-14094-C04-01).

References
Abdul-Rauf, S., Fishel, M., Lambert, P., Noubours, S., & Sennrich, R. (2012). Extrinsic
evaluation sentence alignment systems. Proceedings LREC workshop Creating Cross-language Resources Disconnected Languages Styles, CREDISLAS,
Istambul.
Banchs, R. E., Crego, J. M., Lambert, P., & Marino, J. B. (2006). Feasibility Study
Chinese-Spanish Statistical Machine Translation. Proc. 5th Int. Symposium
777

fiCosta-jussa, Henrquez & Banchs

Chinese Spoken Language Processing (ISCSLP)CONLL, pp. 681692, Kent Ridge,
Singapore.
Banchs, R. E., & Li, H. (2008). Exploring Spanish Morphology effects Chinese-Spanish
SMT. MATMT 2008: Mixing Approaches Machine Translation, pp. 4953,
Donostia-San Sebastian, Spain.
Banerjee, S., & Lavie, A. (2005). METEOR: Automatic Metric MT Evaluation
Improved Correlation Human Judgments. Proceedings ACL Workshop
Intrinsic Extrinsic Evaluation Measures MT and/or Summarization.
Bertoldi, N., Cattoni, R., Federico, M., & Barbaiani, M. (2008). FBK @ IWSLT-2008.
Proc. International Workshop Spoken Language Translation, pp. 3438,
Hawaii, USA.
Bickel, P. J., & Doksum, K. A. (1977). Mathematical: Basic Ideas Selected topics.
HoldenDay Inc., Oakland, CA, USA.
Birch, A., Osborne, M., & Koehn, P. (2008). Predicting Success Machine Translation.
Proceedings 2008 Conference Empirical Methods Natural Language
Processing, pp. 745754, Honolulu, Hawaii. Association Computational Linguistics.
Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Lafferty, J. D.,
Mercer, R. L., & Roossin, P. S. (1990). Statistical Approach Machine Translation.
Computational Linguistics, 16 (2), 7985.
Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., & Specia, L. (2012). Findings
2012 workshop statistical machine translation. Proceedings Seventh
Workshop Statistical Machine Translation, pp. 1051, Montreal, Canada.
Chew, P. A., Verzi, S. J., Bauer, T. L., & McClain, J. T. (2006). Evaluation Bible
Resource Cross-language Information Retrieval. Proceedings Workshop
Multilingual Language Resources Interoperability, pp. 6874.
Cohn, T., & Lapata, M. (2007). Machine Translation Triangulation: Making Effective
Use Multi-Parallel Corpora. Proc. ACL.
Costa-jussa, M., Henrquez, C., & Banchs, R. E. (2011a). Enhancing Scarce-resource Language Translation Pivot Combinations. 5th International Joint Conference
Natural Language Processing, IJCNLP, Chiang Mai, Thailand.
Costa-jussa, M., Henrquez, C., & Banchs, R. E. (2011b). Evaluacion de estrategias para
la traduccion automatica estadstica de chino castellano con el ingles como lengua
pivote. XXVII edicion del Congreso Anual de la Sociedad Espanola para el Procesamiento del Lenguaje Natural, SEPLN, Huelva.
de Gispert, A., & Marino, J. (2006). Catalan-English Statistical Machine Translation without Parallel Corpus: Bridging Spanish. Proc. LREC 5th Workshop
Strategies developing Machine Translation Minority Languages (SALTMIL06),
pp. 6568, Genova.
EastAsiaFoundation (2012). Global asia magazine.. [Online; accessed 12-December-2012].
778

fiEvaluating Indirect Strategies ChineseSpanish SMT

Ehling, N., Zens, R., & Ney, H. (2007). Minimum Bayes Risk Decoding BLEU. Proceedings 45th Annual Meeting Association Computational Linguistics Companion Volume Proceedings Demo Poster Sessions, pp. 101104,
Prague, Czech Republic. Association Computational Linguistics.
Ethnologue (2012). Ranking spoken languages.. [Online; accessed 12-December2012].
Goel, V., & Byrne, W. (2000). Minimum Bayes-risk Automatic Speech Recognition. Computer Speech Language, 14 (2), 115135.
Habash, N., & Rambow, O. (2005). Arabic Tokenization, Part-of-Speech Tagging Morphological Disambiguation One Fell Swoop. Proc. 43rd Annual Meeting
Association Computational Linguistics, pp. 573580, Ann Arbor, MI. Association Computational Linguistics.
Henrquez Q., C. A., Banchs, R. E., & Marino, J. B. (2010). Learning Reordering Models
Statistical Machine Translation Pivot Language. Internal Report TALP-UPC.
Khalilov, M., Costa-Jussa, M. R., Henrquez, C. A., Fonollosa, J. A. R., Hernandez, A.,
Marino, J. B., Banchs, R. E., Chen, B., Zhang, M., Aw, A., & Li, H. (2008).
TALP & I2R SMT Systems IWSLT 2008. Proc. International Workshop
Spoken Language Translation, pp. 116123, Hawaii, USA.
Koehn, P. (2004). Statistical Significance Tests Machine Translation Evaluation.
Proceedings EMNLP, Vol. 4, pp. 388395.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.
(2007). Moses: Open source toolkit statistical machine translation. Proc.
ACL, pp. 177180, Prague, Czech Republic.
Koehn, P., Och, F., & Marcu, D. (2003). Statistical Phrase-Based Translation. Proc.
41th Annual Meeting Association Computational Linguistics.
Kumar, S., & Byrne, W. (2004). Minimum Bayes-Risk Decoding Statistical Machine
Translation. Proceedings Human Language Technology North American
Association Computational Linguistics Conference (HLT/NAACL04), pp. 169
176, Boston, USA.
Marino, J., Banchs, R. E., Crego, J. M., de Gispert, A., Lambert, P., Fonollosa, J. R.,
& Costa-jussa, M. R. (2006). N-gram Based Machine Translation. Computational
Linguistics, 32 (4), 527549.
Moore, R. (2002). Fast Accurate Sentence Alignment Bilingual Corpora. Proc.
AMTA, pp. 135144, London.
Nist (2009). NIST machine translation evaluation campaign..
December-2012].

[Online; accessed 12-

Och, F. J., & Ney, H. (2000). Improved Statistical Alignment Models. Proc.
38th Annual Meeting Association Computational Linguistics, pp. 440447,
Hongkong, China.
779

fiCosta-jussa, Henrquez & Banchs

Och, F. (2003). Minimum Error Rate Training Statistical Machine Translation. Proc.
41th Annual Meeting Association Computational Linguistics, pp.
160167.
Och, F., & Ney, H. (2002). Dicriminative training maximum entropy models statistical machine translation. Proc. 40th Annual Meeting Association
Computational Linguistics, pp. 295302, Philadelphia, PA.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2001). BLEU: Method Automatic
Evaluation Machine Translation. IBM Research Report, RC22176.
Paul, M. (2008). Overview iwslt 2008 evaluation campaign. Proc. International Workshop Spoken Language Translation, pp. 117, Hawaii, USA.
Paul, M., Yamamoto, H., Sumita, E., & Nakamura, S. (2009). Importance Pivot
Language Selection Statistical Machine Translation. HLT-NAACL (Short Papers), pp. 221224.
Rafalovitch, A., & Dale, R. (2009). United Nations General Assembly Resolutions: SixLanguage Parallel Corpus. Proc. MT Summit XII, pp. 292299, Ottawa.
Senrich, R. (2010). MT-based Sentence Alignment OCR-generated Parallel Texts.
Proc. AMTA, Denver.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L., & Makhoul, J. (2006). Study Translation Edit Rate Targeted Human Annotation. Proceedings Association
Machine Translation Americas.
Stolcke, A. (2002). SRILM: extensible language modeling toolkit.. Proc. Int.
Conf. Spoken Language Processing, pp. 901904, Denver, CO.
Tillman, C. (2004). Block Orientation Model Statistical Machine Translation.
HLT-NAACL.
Wang, H., Wu, H., Hu, X., Liu, Z., Li, J., Ren, D., & Niu, Z. (2008). TCH Machine
Translation System IWSLT 2008. Proc. International Workshop
Spoken Language Translation, pp. 124131, Hawaii, USA.
Wu, H., & Wang, H. (2007). Pivot Language Approach Phrase-Based Statistical Machine
Translation. Proc. ACL, pp. 856863, Prague.
Zapatero, J. R. (2010). China top priority spanish economy; companies
well aware that.. [Online; accessed 12-December-2012].
Zens, R., Och, F., & Ney, H. (2002). Phrase-based statistical machine translation. Verlag,
S. (Ed.), Proc. German Conference Artificial Intelligence (KI).
Zhang, H., Yu, H., Xiong, D., & Liu, Q. (2003). HHMM-based chinese lexical analyzer
ICTCLAS. Proc. 2nd SIGHAN Workshop Chinese language processing,
pp. 184187, Sapporo, Japan.

780

fiJournal Artificial Intelligence Research 45 (2012) 641-684

Submitted 11/12; published 12/12

Learning Predict Textual Data
Kira Radinsky
Sagie Davidovich
Shaul Markovitch

kirar@cs.technion.ac.il
mesagie@gmail.com
shaulm@cs.technion.ac.il

Computer Science Department
TechnionIsrael Institute Technology
Haifa 32000, Israel

Abstract
Given current news event, tackle problem generating plausible predictions
future events might cause. present new methodology modeling predicting
future news events using machine learning data mining techniques. Pundit
algorithm generalizes examples causality pairs infer causality predictor. obtain
precisely labeled causality examples, mine 150 years news articles apply semantic
natural language modeling techniques headlines containing certain predefined causality
patterns. generalization, model uses vast number world knowledge ontologies.
Empirical evaluation real news articles shows Pundit algorithm performs
well non-expert humans.

1. Introduction
Causality studied since antiquity, e.g., Aristotle, modern perceptions
causality influenced, perhaps, work David Hume (17111776),
referred causation strongest important associative relation,
lies heart perception reasoning world, constantly
supposed connection present fact inferred
it.
Causation also important designing computerized agents. agent, situated complex environment, plans actions, reasons future changes
environment. changes result actions, many others
result various chains events necessarily related agent. process
observing event, reasoning future events might caused it, called
causal reasoning.
past, computerized agents could operate complex environments due
limited perceptive capabilities. proliferation World Wide Web, however,
changed that. intelligent agent act virtual world Web, perceiving
current state world extensive sources textual information, including Web
pages, tweets, news reports, online encyclopedias, performing various tasks
searching, organizing, generating information. act intelligently complex
virtual environment, agent must able perceive current state reason
future states causal reasoning. reasoning ability extremely helpful
conducting complex tasks identifying political unrest, detecting tracking
c
2012
AI Access Foundation. rights reserved.

fiRadinsky, Davidovich & Markovitch

social trends, generally supporting decision making politicians, businesspeople,
individual users.
many works devoted extracting information text (e.g., Banko,
Cafarella, Soderl, Broadhead, & Etzioni, 2007; Carlson, Betteridge, Kisiel, Settles, Hruschka, & Mitchell, 2010), little done area causality extraction,
works Khoo, Chan, Niu (2000) Girju Moldovan (2002) notable
exceptions. Furthermore, algorithms developed causality extraction try detect
causality cannot used predict it, is, generate new events given event
might cause.
goal paper provide algorithms perform causal reasoning, particular causality prediction, textually represented environments. developed
causality learning prediction algorithm, Pundit, that, given event represented
natural language, predicts future events cause. algorithm trained examples
causality relations. uses large ontologies generalize causality pairs
generate prediction model. model represented abstraction tree, that, given
input cause event, finds appropriate generalization, uses learned rules
output predicted effect events.
implemented algorithm applied large collection news reports
last 150 years. extract training examples news corpus,
use correlation, means causality often misidentified. Instead, use textual
causality patterns (such X X causes Y), applied news headlines,
identify pairs structured events supposedly related causality. result
semantically-structured causality graph 300 million fact nodes connected
one billion edges. evaluate method, tested news archive 2010,
used training. results judged human evaluators.
give intuition type predictions algorithm generates, present
two examples actual predictions made system. First, given event Magnitude 6.5 earthquake rocks Solomon Islands, algorithm predicted tsunamiwarning issued Pacific Ocean. learned past examples
trained, one
h7.6 earthquake strikes island near India, tsunami warning issued Indian Oceani.
Pundit able infer earthquake occurring near island would result
tsunami warning issued ocean. Second, given event Cocaine found
Kennedy Space Center, algorithm predicted people arrested.
partially based past example hpolice found cocaine lab 2 people arrested i.

contributions work threefold: First, present novel scalable algorithms generalizing causality pairs causality rules. Second, provide new method
using casualty rules predict new events. Finally, implement algorithms
large scale system perform empirical study realistic problems judged human
raters. make extracted causality information publicly available research
field 1 .
1. http://www.technion.ac.il/~kirar/Datasets.html

642

fiLearning Predict Textual Data

2. Learning Predicting Causality
section, describe Pundit algorithm learning predicting causality.
start overview learning prediction process. training, learning
algorithm receives causality event pairs, extracted historical news archives (Section
3). algorithm generalizes given examples using world knowledge
produces abstraction tree (AT)(Section 2.4). node AT, prediction
rule generated examples node (Section 2.5). Then, prediction
phase, algorithm matches given new event nodes AT, associated
rule applied produce possible effect events (Section 2.6). events
filtered (Section 2.7) effect event output. output event also given
natural language, sentence-like form. process illustrated Figure 1.

Implausible
event lter

Figure 1: Structure Pundit prediction algorithm

2.1 Event Representation
basic element causal reasoning event. Topic Tracking Detection (TDT)
community (Allan, 2002) defined event particular thing happens specific time place, along necessary preconditions unavoidable consequences.
643

fiRadinsky, Davidovich & Markovitch

philosophical theories consider events exemplifications properties objects
times (Kim, 1993). example, Caesars death 44 BC Caesars exemplification
property dying time 44 BC. theories impose structure events,
change one elements yields different event. example, Shakespears death
different event Caesars death, objects exemplifying property different.
section, discuss way represent events following Kims (1993) exemplification theory allow us easily compare them, generalize them, reason
them.
three common approaches textual event representation: first approach
describes event sentence level running text individual terms (Blanco, Castell,
& Moldovan, 2008; Sil, Huang, & Yates, 2010). Event similarity treated distance
metric two events bag words. approaches useful,
often fail perform fine-grained reasoning. Consider, example, three events: US Army
bombs warehouse Iraq, Iraq attacks US base, Terrorist base attacked
US Marines Kabul. Representing events individual terms alone might yield
first two similar first last words
common. However, approaches disregard fact actors first last event
military groups Kabul Iraq event locations. facts
taken account, first last events clearly similar first
second.
second approach describes events syntax-driven manner, event text
transformed syntax-based components, noun phrases (Garcia, 1997; Khoo et al.,
2000; Girju & Moldovan, 2002; Chan & Lam, 2005). example, representation
erroneously finds second third events similar due syntactic
similarity them. Using first two approaches, hard make practical
generalizations events compare way takes account
semantic elements compose them.
third approach semantic (similar representation Cyc; Lenat & Guha,
1990), maps atomic elements event semantic concepts. approach
provides grounds canonic representation events comparable generalizable. work, follow third approach represent events semantically.
Given set entities represent physical objects abstract concepts
real world (e.g., people, instances, types), set actions P , define event
ordered set e = hP, O1 , . . . , O4 , ti, where:
1. P temporal action state events objects exhibit.
2. O1 actor performed action.
3. O2 object action performed.
4. O3 instrument action performed.
5. O4 location event.
6. time-stamp.
644

fiLearning Predict Textual Data

example, event U.S Army destroyed warehouse Iraq explosives,
occurred October 2004, modeled as: Destroy (Action); U.S Army (Actor);
warehouse (Object); explosives (Instrument); Iraq (Location); October 2004 (Time).
approach inspired Kims (1993) property-exemplification events theory.
2.2 Learning Problem Definition
treat causality inference supervised learning problem. Let Ev universe
possible events. Let f : Ev Ev {0, 1} function
(
1 e1 causes e2 ,
f (e1 , e2 ) =
0 otherwise.
denote f + = {(e1 , e2 )|f (e1 , e2 ) = 1}. assume given set possible positive
examples E f + .
goal merely test whether pair events plausible cause-effect pair
f , generate given event e events cause. purpose define
g : Ev 2Ev g(e) = {e0 |f (e, e0 ) = 1}; is, given event, output set events
cause. wish build predictor g using examples E.
Learning f E could solved standard techniques concept learning
positive examples. requirement learn g, however, presents challenging task
structured prediction positive examples.
2.3 Generalizing Objects Actions
goal develop learning algorithm automatically produces causality function
based examples causality pairs. inferred causality function able
predict outcome given event, even never observed before. example, given
training examples hearthquake Turkey, destructioni hearthquake Australia,
destructioni, current new event earthquake Japan, reasonable prediction
would destruction. able handle predictions, must endow learning
algorithm generalization capacity. example, scenario, algorithm
must able generalize Australia Turkey countries, infer earthquakes
countries might cause destruction. type inference knowledge Japan
also country enables algorithm predict effects new events using patterns
past.
generalize set examples, consisting pair events, perform
generalization components events. two types components
objects actions.
generalize objects, assume availability semantic network Go = (V, E),
nodes V objects universe, labels edges
relations isA, partOf CapitalOf. work, consider one largest
semantic networks available, LinkedData ontology (Bizer, Heath, & Berners-Lee, 2009),
describe detail Section 3.
define two objects similar relate third object way.
relation label sequence labels semantic network. example,
645

fiRadinsky, Davidovich & Markovitch

Paris London considered similar nodes connected path
Capitalof Incontinent

node Europe. formally define idea.
Definition 1. Let a, b V . sequence labels L = l1 , . . . , lk generalization path
a, b, denoted GenPath(a,b), exist two paths G, (a, v1 , l1 ), . . . (vk , vk+1 , lk )
(b, w1 , l1 ), . . . (wk , wk+1 , lk ), s.t. vk+1 = wk+1 .
Overgeneralization events avoided e.g., given two similar events, one
occurring Paris one London, wish produce generalization city Europe
Capitalof Incontinent

( Europe) rather abstract generalization city
Capitalof Incontinent IsA

continent ( Continent). wish generalization
specific possible. call minimal generalization objects.

Definition 2. minimal generalization path, denoted GenP ath(a, b), defined
set containing shortest generalization paths. denote distGen (a, b) length
GenP ath(a, b).
Path-based semantic distances one shown successful
many NLP applications. example, semantic relatedness two words measured
means function measured distance words taxonomy (Rada,
Mili, Bicknell, & Blettner, 1989; Strube & Ponzetto, 2006). build metric
expand handle events structured contain several objects different
ontologies.
efficiently produce GenP ath, designed algorithm (described Figure 2),
based dynamic programming, computes GenP ath object pairs G.
simplicity, describe algorithm computes single path two nodes
b, rather set shortest paths. step 1 queue holds nodes
generalization initialized. step 2, algorithm identifies nodes (a, b)
common node (c) connecting via type edge (l). c
thought generalization b. gen structure maps pair nodes
generalization (M gen.Gen) generalization path (M Gen.P red). step 3,
dynamic programming manner, algorithm iterates nodes (a, b) gen
found minimal generalization previous iterations, finds two nodes one
(x) connecting one (y) connecting b via type edge l (step 3.4). Thus,
minimal generalization x minimal generalization b, path
GenP ath a, b addition edge type l. update performed
steps 3.4.13.4.4. Eventually, nodes minimal generalization
expanded (i.e., algorithm cannot find two nodes connect via edge
type), stops returns gen (step 4). prediction, several gen exists,
consider prediction corresponding GenP ath.
define distance actions using ontology Gp , similarly way
defined distance objects. Specifically, use VerbNet (Kipper, 2006) ontology,
one largest English verb lexicons. mapping many online
resources, Wordnet (Miller, 1995). ontology hierarchical based
classification verbs Levin classes (Dang, Palmer, & Rosenzweig, 1998).
resource widely used many natural language processing applications (Shi &
646

fiLearning Predict Textual Data

Procedure Minimal Generalization Path(G)
(1) Q new Queue
(2) Foreach {(a, c, l), (b, c, l) E(G)|
a, b, c V (AT ), l Lables}
(2.1) gen(a, b).Gen = c
(2.2) gen(a, b).P red = l
(2.3) gen(a, b).Expanded = f alse
(2.4) Q.enqueue((a, b))
(3) Q 6= :
(3.1) (a, b) Q.dequeue()
(3.2) gen(a, b).Expanded 6= true:
gen(a, b).Expanded = true
(3.4) Foreach {(x, a, l), (y, b, l) E(AT )|
x, V (AT ), l Lables}
(3.4.1) gen(x, y).Gen = gen(a, b).Gen
(3.4.2) gen(x, y).P red = gen(a, b).P red||l
(3.4.3) gen(x, y).Expanded = f alse
(3.4.4) Q.enqueue((x, y))
(4)Return gen
Figure 2: Procedure calculating minimal generalization path object pairs
Mihalcea, 2005; Giuglea & Moschitti, 2006). Using ontology describe connections
verbs. Figure 10 shows node ontology generalizes actions hit
kick.
2.4 Generalizing Events
order provide strong support generalization, wish find similar events
generalized single abstract event. example, wish infer
hearthquake Turkey, destructioni hearthquake Australia, destructioni examples
group events. Therefore, wish cluster events way
events similar causes effects clustered together. clustering methods,
distance measure objects defined. Let ei = hP , O1i , . . . , O4i , ti
ej = hP j , O1j , . . . , O4j , tj two events. previous subsection defined distance
function objects (and actions). Here, define similarity two events
ei ej function distances objects actions:
Gp
j
j
Go



SIM (ei , ej ) = f distGen
(P , P j ), distG
Gen (O1 , O1 ), . . . , distGen (O4 , O4 ) ,

(1)

where, distG
Gen distance function distGen graph G, f aggregation
function. work, mainly use average aggregation function, also
analyze several alternatives.
647

fiRadinsky, Davidovich & Markovitch

Likewise, similarity two pairs cause-effect events hci , ei hcj , ej
defined as:

SIM (hci , ei i, hcj , ej i) = f SIM (ci , cj ), SIM (ei , ej ) .

(2)

Using similarity measure suggested above, clustering process thought
grouping training examples way low variance
effects high similarity cause. similar information gain methods
examples clustered class. use HAC hierarchical clustering algorithm
(Eisen, Spellman, Brown, & Botstein, 1998) clustering method. algorithm starts
joining closest event pairs together cluster. keeps repeating process
joining closest two clusters together elements linked hierarchical
graph events call abstraction tree (AT). Distance clusters measured
distance representative events. allow this, assign node
representative cause event, event closest centroid nodes cause
events. prediction phase, input cause event matched one
created clusters, i.e., closest representative cause event cluster.
2.5 Causality Prediction Rule Generation
last phase learning creation rules allow us, given cause event,
generate prediction it. input cause event matched node
centroid, naive approach would return effect event matched centroid. This,
however, would provide us desired result. Assume event ei =Earthquake
hits Haiti occurred today, matched node represented centroid:
Earthquake hits Turkey, whose effect Red Cross help sent Ankara. Obviously,
predicting Red Cross help sent Ankara earthquake Haiti
reasonable. would like able abstract relation past cause
past effect learn predicate clause connects them, example Earthquake hits
[Country Name] yielding Red Cross help sent [Capital Country]. prediction,
clause applied input event ei , producing predicted effect.
example, logical predicate clause would CapitalOf, CapitalOf(Turkey)= Ankara.
applied current event ei , CapitalOf(Haiti) = Port-au-Prince, output
Red Cross help sent Port-au-Prince. Notice clauses
applied certain types objects case, countries. clauses length,
e.g., pair hsuspect arrested Brooklyn, Bloomberg declares emergencyi produces
clause Mayor(BoroughOf(x)), Brooklyn borough New York, whose mayor
Bloomberg.
show learn clauses node graph. Recall
semantic network graph GO edge-labeled graph, edge triplet
hv1 , v2 , li, l predicate (e.g., CapitalOf). rule-learning procedure divided
two main steps. First, find undirected path pi length k GO
object cause event object effect event. Note
necessarily look paths two objects role. example,
found path location cause event (Brooklyn) actor
effect event (Bloomberg). Second, construct clause using labels path pi
648

fiLearning Predict Textual Data

predicates. call predicate projection size k, pred = l1 , . . . , lk
event ei event ej . prediction, projections applied new event
e = hP , O1 , . . . , O4 , ti finding undirected path GO Oi sequence
labels pred. k unknown, algorithm, training example hct , et node
AT, finds possible predicate paths increasing sizes k objects
ct objects et GO graph. path weighted number times
occurred node, support rule. full predicate generation procedure
described Figure 3. function LearnP redicateClause calls inner function
F indP redicateP ath different k sizes different objects given cause
effect events. F indP redicateP ath recursive function tries find path
two objects graph length k. returns labels path found.
rule generated template generating prediction future event given cause
event. example rule seen Figure 4. Rules return NULL
displayed figure. example, generate object O1 future event,
l1 l2
try apply path


object O4 cause, thus generating possible objects
l1 l2
object O1 prediction (see Section 2.6). Similarly, path


applied
O2 , generating possible objects. object O2 prediction, simple path
one label generated. Therefore, prediction, possible objects O2
ones connect O4cause label l8 (if any). object O3 prediction,
use O3cause . O4 special rule generated (F indP redicateP ath returned NULL
objects), final prediction O4ef f ect .
2.6 Prediction
Given trained model g, applied new event e = hP , O1 , . . . , O4 , ti order
produce effects. process divided two main steps propagating event
retrieve set matched nodes, applying rules matched node
produce possible effects.
Given new event, Pundit traverses starting root. node
search frontier, algorithm computes similarity (SIM (ei , ej )) input event
centroid children node, expands children better
similarity parent. idea stated intuitively attempt find
nodes least general still similar new event. full algorithm
illustrated Figure 5. illustration process seen Figure 6. Here, event
bombing Baghdad received input. system searches least general
cause event observed past (for simplicity show short notation
cause events AT). case, generalized cluster: bombing city.
candidates selected military communication cluster bombing cluster
(as node bombing worship area lower score bombing).
node retrieved previous stage, predicate projection, pred, applied
new event e = hP , O1 , . . . , O4 , ti. Informally, say pred applied
finding undirected path GO Oi labels pred. rule generates
possible effect event retrieved node. projection results reached
objects vertex. formal explanation pred applied V0 :
V0 , V1 . . . Vk : (V0 , V1 , l1 ), . . . (Vk1 , Vk , lk ) Edges(GO ). projection results
649

fiRadinsky, Davidovich & Markovitch

Procedure FindPredicatePath(curEntity, goalEntity, depth)
curEntity = goalEntity Return
Else
depth = 0 Return N U
Else
Foreach relation outEdges(curEntity)
solution FindPredicatePath(relation(curEntity), goalEntity, depth 1)
solution 6= N U
Foreach existingSolution
Solutions :
Return Solutions (existingSolution||relation||solution)
Return Solutions
Procedure LearnPredicateClause(hP c , O1c , . . . , O4c , tc i, hP e , O1e , . . . , O4e , te i, depth)
Foreach Oic Oc , Oje Oe , k {1 . . . depth}
rule(j)
Foreach Oic Oc , Oje Oe , k {1 . . . depth}

rule(j) rule(j) {hOic , FindPredicatePath(Oic , Oje , k)i}
Return rule
Figure 3: Procedure generating rule two events inferring paths two
events causality graph.

objects Vk . projection results nodes weighted similarity
target cause node Gen ranked support rule (for tie
breaking). several Gen exists, highest similarity considered. See Figure 7
complete formal description algorithm. example (Figure 6), candidate
bombing [city] following rules:
1. P ef f ect = happen, O1ef f ect = riot , O4ef f ect = O4cause
2. P ef f ect = happen, O1ef f ect = riot , O4ef f ect = O4cause

mainstreetof



3. P ef f ect = kill, O2ef f ect = people
4. P ef f ect = condemn, O1ef f ect = O4

mayorof boroughof





, O2ef f ect = attack

clarity, objects rule applied (the rule object NULL),
use effect concept matched training example.
event Baghdad Bombing (O1 = Bombing, O4 = Baghdad), applying rules
yields following:
1. Baghdad Riots (P ef f ect = happen, O1ef f ect = riot , O4ef f ect = Baghdad).
2. Caliphs Street Riots (P ef f ect = happen, O1ef f ect = riot , O4ef f ect = Caliphs Street
mainstreetof



O4cause ).
650

fiLearning Predict Textual Data

Rule(cause, ef f ect) =

O1

O4cause

l1

O2

{O4cause

l8

O3

{O3cause ;}

l

2
! !,
O2cause

l1

l

3
! !

!}

O4

Figure 4: example generated rule

3. People killed (P ef f ect = kill, O2ef f ect = people).
4. rule cannot applied given event, outgoing edge type
borough-of node Baghdad.

2.7 Pruning Implausible Effects
cases, system generated implausible predictions. example, event
hlightning kills 5 peoplei, system predicted hlightning arrestedi.
prediction based generalized training examples people killed
people got arrested, as: hMan kills homeless man, man arrested i. could
determine logical event is, could avoid false predictions. section
discuss filter out.
goal filtering component different predictor.
predictors goal generate predictions future events, components goal
monitor predictions. predictor learns causality relation events,
component learns plausibility.
right way perform filtering utilize common sense knowledge
action. knowledge would state type actor object perform
action, possible instruments action preformed possible
locations. knowledge would existed, would identified action
arrest object human. However, common sense knowledge currently
available. Therefore, resort common practice using statistical
methods.
651

fiRadinsky, Davidovich & Markovitch

Procedure Propagation(e = hP , O1 , . . . , O4 , ti)
(1) Candidates {}
(2) Q new Queue
(3) Q.enqueue(G.root)
(4) Q 6= :
(4.1) cur Q.dequeue()
(4.2) Foreach edge cur.OutEdges:
SIM (e, edge.Source) > SIM
(e, edge.Destination):
Candidates Candidates
{(edge.Source, SIM (e, edge.Source))}
Else :
Q.enqueue(edge.Destination)
(5) Return Candidates

Figure 5: Procedure locating candidates prediction. algorithm saves set
possible matched results (Candidates), queue holding search frontier
(Q). step 4, algorithm traverses graph. step 4.2, edge,
algorithm tests whether similarity new event e parent node
(edge.Source) higher child node (edge.Destination). test
succeeds, parent node, similarity score, added possible
results. edges exposed, algorithm returns possible results
step 5.

Baghdad
bombing

military
0.2

military
communication

bombing
0.7
bombing
0.8
city

0.3

bombing
0.65 worship area

0.2

0.75
bombing Kabul

Figure 6: event bombing Baghdad received input. system searches
least general cause event observed past. case
generalized cluster: bombing city. rule node applied
Baghdad bombing generate prediction.

652

fiLearning Predict Textual Data

Procedure FindPredicatePathObjects(entity, path = hl1 . . . lk i)
(1) Candidates {}
(2) Q new Queue
(3) Q.enqueue(entity)
(4) labelIndexInP ath = 1
(5) path.Count == 0: Return {entity}
(6) Q 6= :
cur Q.dequeue()
Foreach edge {edge cur.OutEdges|edge.label = path[labelIndexInP ath]}:
labelIndexInP ath = k :
Candidates Candidates {edge.Destination}
Else :
labelIndexInP ath > k: Return Candidates
Q.enqueue(edge.Destination)
labelIndexInP ath labelIndexInP ath + 1
(7) Return Candidates
Procedure ApplyPredicateClause(hP, O1 , . . . , O4 , ti, rule)
Foreach = 1...4
Oiprediction
Foreach path = {Oj , {l1 . . . lk }} rule(i)

Oiprediction FindPredicatePathObjects(Oj , hl1 . . . lk i)
Return hO1prediction . . . O4prediction
Figure 7: Procedure applying rule new given event. main procedure ApplyPredicateClause. procedure generates objects predicted event O1 . . . O4 given
rule. rule list lists tuples. tuple concept path.
tuple function FindPredicatePathObjects applied. procedure finds
objects path whose labels connect given concept. objects
stored Candidates (step 1). algorithm holds queue Q frontier (step
2). queue first holds given entity (step 3). procedure holds counter indicating whether followed entire given path (step 4). algorithm checks
whether edge label path[labelIndexInPath] going object
head frontier. algorithm reaches end given path
(labelIndexInP ath = k), returns candidates.

information extraction literature, identifying relationship facts
plausibility widely studied. methods usually estimate prior probability relation examining frequency pattern large corpus,
Web (Banko et al., 2007). example, relation hPeople,arrest,Peoplei
methods return phrase mentioned 188 times Web, relation
hPeople,arrest,[Natural Disaster]i mentioned 0 times. Similarly, estimate prior
probability event occur prior appearance New York Times,
653

fiRadinsky, Davidovich & Markovitch

Procedure Pruning Implausible Effects(ev = hPi , O1 , . . . , O4 , ti, generalizationBound)
(1) Foreach j 1 . . . 4 :
generalizationPath = {}
0 . . . generalizationBound
Gen(Oi ) F indP redicateP athObjects(OjS, generalizationP ath)
generalizationPath generalizationP ath {IsA}
(2) Return Averagei,j,i6=j (M axo1 Gen(Oi ),o2 Gen(Oj ) P CI(o1 , o2 , i, j))
Figure 8: procedure calculating PMCI event. procedure, step 1,
first generates generalizations type IsA object (with path whose
length generalizationBound). purpose uses function
FindPredicatePathObjects (defined Figure 7). generalization procedure
repeated objects comprising event ev, result stored
Gen. final result algorithm calculated step 2. two objects
(o1 , o2 ) generalization (Gen), also contains original objects,
find maximum PMCI. compute final result averaging
maximum PMCI.

primary source news headlines. filter events that, priori, low
probability occur.
present algorithm Figure 8. calculated many times semantic concepts representing event, immediate generalizations, actually occurred together
past semantic roles. example, check many times lightning
natural phenomena arrested. Formally, define point-wise mutual concept
information (PMCI) two entities verbs oi , oj (e.g., lightning arrest) roles
ri , rj (e.g., actor action) defined

P CI(oi , oj , ri , rj ) = log

p(oi @ri , oj @rj )
.
p(oi @ri )p(oj @rj )

(3)

Given event, calculate average PMCI components. algorithm filters
predicted events low average PMCI. assume cause effect
examples training ground truth, yield high PMCI. Therefore,
evaluate threshold filtering training data. is, collected
effects observed training data estimated average PMCI entire
NYT dataset.
reader note applying rules might create problem. past
earthquake occurred Tokyo, pruning procedure might return low plausibility.
handle type errors, calculate PMCI upper level categories entities
(e.g., natural disasters) rather specific entities (e.g., earthquakes). therefore restrict
two upper level categories.
654

fiLearning Predict Textual Data

3. Implementation Details
previous section, presented high-level algorithm requires training examples
, knowledge entities GO , event action classes P . One main challenges
work build scalable system meet requirements.
present system mines news sources extract events, constructs canonical
semantic model, builds causality graph top events. system crawled,
4 months, several dynamic information sources (see Section 3.1 details).
largest information source NYT archive, optical character recognition
(OCR) performed. overall gathered data spans 150 consecutive years
(1851 2009).
generalization objects, system automatically reads Web content extracts world knowledge. knowledge mined structured semi-structured
publicly available information repositories. generation causality graph distributed 20 machines, using MapReduce framework. process efficiently unites
different sources, extracts events, disambiguates entities. resulting causality graph
composed 300 million entity nodes, one billion static edges (connecting different objects encountered events), 7 million causality edges (connecting
events found Pundit cause other). rule generated
using average 3 instances standard deviation 2.
top causality graph, search indexing infrastructure built enable
search millions documents. highly scalable index allows fast walk
graph events, enabling efficient inference capabilities prediction phase
algorithm.
3.1 World Knowledge Mining
entity graph Go composed concepts Wikipedia, ConceptNet (Liu & Singh,
2004), WordNet (Miller, 1995), Yago (Suchanek, Kasneci, & Weikum, 2007), OpenCyc;
billion labeled edges graph Go predicates ontologies.
section describe process knowledge graph created search
system built upon it.
system creates entity graph collecting content, processing feeds,
processing formatted data sets (e.g., Wikipedia). crawler archives documents raw format, transforms RDF (Resource Description Framework)
format (Lassila, Swick, Wide, & Consortium, 1998). concepts interlinked humans part Linked Data project (Bizer et al., 2009). goal Bizer et al.s
(2009) Linked Data project extend Web interlinking multiple datasets RDF
setting RDF links data items different data sources. Datasets include
DBPedia (a structured representation Wikipedia), WordNet, Yago, Freebase, more.
September 2010 grown 25 billion RDF triples, interlinked around 395
million RDF links.
use SPARQL queries way searching knowledge graph. Experiments
performance queries Berlin benchmark (Bizer & Schultz, 2009) provided
evidence superiority Virtuoso open source triple structures task.
655

fiRadinsky, Davidovich & Markovitch

3.2 Causality Event Mining Extraction
supervised learning algorithm requires many learning examples able generalize
well. amount temporal data extremely large, spanning millions articles,
goal obtaining human annotated examples becomes impossible. therefore provide
automatic procedure extract labeled examples learning causality dynamic
content. work, used NYT archives years 1851 2009, WikiNews,
BBC 14 million articles total (see data statistics Table 1). Extracting
causal relations events text hard task. state-of-the-art precision
task around 37% (Do, Chan, & Roth, 2011). hypothesis
information regarding event found headlines. structured
therefore easier analyze. Many times headline contain cause
effect event. assume headlines describing events
developed extraction algorithm identify headlines extract events
them. News headlines quite structured, therefore accuracy stage
(performed representative subset data) 78% (see Section 4.2.1). system
mines unstructured natural language text found headlines, searches causal
grammatical patterns. construct patterns using causality connectors (Wolff, Song,
& Driscoll, 2002; Levin & Hovav, 1994). work used following connectors:
1. Causal Connectives: words because, as, connectors.
2. Causal prepositions: words due of.
3. Periphrastic causative verbs: words cause lead to.
constructed set rules extracting causality pair. rule structured as:
hPattern, Constraint, Priorityi, Pattern regular expression containing causality
connector, Constraint syntactic constraint sentence pattern
applied, Priority priority rule several rules matched.
following constraints composed:
1. Causal Connectives: pattern [sentence1] [sentence2] used following constraints: [sentence1] cannot start when, how, where, [sentence2]
cannot start all, hours, minutes, years, long, decades. pattern
[sentence1], [sentence2] add constraint [sentence1] cannot start
number. pattern match sentence Afghan vote, complaints
fraud surface match sentence 10 years Lansing, state
lawmaker Tom George returns. pattern [sentence1] [sentence2] used
constraint [sentence2] verb. Using constraint, pattern
match sentence Nokia cut jobs tries catch rivals,
sentence civil rights photographer unmasked informer.
2. Causal prepositions: pattern [sentence1][because of, due to] [sentence2]
required constraints [sentence1] start when, how, where.
3. Periphrastic causative verbs: pattern [sentence1] [leads to, Leads to, lead
to, Lead to, led to, Led to] [sentence2] used, [sentence1] cannot con656

fiLearning Predict Textual Data

tain when, how, where, prefix cannot study studies. Additionally, consider periphrastic causative verbs, allow additional verbs
[sentence1] [sentence2].
result rule application pair sentences one tagged cause, one
tagged effect.
Given natural-language sentence (extracted article headline), representing
event (either learning prediction), following procedure transforms
structured event:
1. Root forms inflected words extracted using morphological analyzer derived
WordNet (Miller, 1995) stemmer. example, article headline
10/02/2010: U.S. attacks kill 17 militants Pakistan, words attacks, killed
militants transformed attack, kill, militant respectively.
2. Part-Of-Speech tagging (Marneffe, MacCartney, & Manning, 2006) performed,
verb identified. class verb identified using VerbNet vocabulary
(Kipper, 2006), e.g., kill belongs P =murder class.
3. syntactic template matching verb applied extract semantic relations
thus roles words (see example Figure 10). templates based
VerbNet, supplies verb class set syntactic templates.
templates match syntax thematic roles entities sentence.
match templates even continuous sentence tree. allows
match sentence even auxiliary verb subject
main transitive verb. example, template NP1 V NP2,
transforms NP1 Agent, NP2 Patient. Therefore, match U.S. attacks
Actor, militant Patient . template matched,
sentence transformed typed-dependency graph grammatical relations
(Marneffe et al., 2006). example, U.S. attacks identified subject
sentence (candidate Actor), militants object (candidate Patient),
Pakistan preposition (using Locations lexicons). Using analysis,
identify Location Pakistan.
4. word Oi mapped Wikipedia-based concept. word matches
one concept, perform disambiguation computing cosine similarity
body news article body Wikipedia article associated
concept. example, U.S matched several concepts, United
States, University Salford, Us (Brother Ali album). similar
content Wikipedia concept United States. word Oi found
Wikipedia, treated constant, i.e., generalization applied it,
used similarity calculation. is, distGen (const1 , const2 ) = 0
const1 = const2 , distGen (const1 , const2 ) = k otherwise. experiments,
set k = 4, length longest distance found two concepts
GO .
5. time event time publication article news, e.g.,
=10/02/2010.
657

fiRadinsky, Davidovich & Markovitch

Data Source
NYT
BBC
WikiNews

Number Titles Extracted
14,279,387
120,445
25,808

Table 1: Data Summary.
Time

5
QuanGer

kill
AcGon

Troops
APribute

Afghan
1/2/1987
11:15AM +(3h)

Time-
frame

Event2

5 Afghan troops killed

Army
bombard

1/2/1987
11:00AM +(2h)

US Army

Time-
frame

Weapons
warehouse

AcGon

US

Event1

LocaGon

Kabul

Instr
ume
nt

US Army bombards weapons
warehouse Kabul missiles

Missiles

Figure 9: pair events causality graph. first represents cause event
second represents effect event. extracted headline published 1/2/1987: 5 Afghan troops killed US army bombards warehouse
Kabul.

example, final result event e = hMurder-Class, United States America,
Militant, NULL, Pakistan, 10/02/2010i . final result stage causality graph
composed causality event pairs. events structured described Section 2.1.
illustrate pair Figure 9.
certain cases, additional heuristics needed order deal brevity
news language. used following heuristics:
1. Missing Context McDonalds recalls glasses due cadmium traces, extracted event cadmium traces needs additional context Cadmium traces [in McDonalds glasses]. object missing, first sentence ([sentence1]) subject
used.
658

fiLearning Predict Textual Data

Class Hit-18.1
Roles Restrictions:
Agent[int control] Patient[concrete] Instrument[concrete]
Members: bang, bash, hit, kick, . . .
Frames:
Example
Syntax
Semantics
cause(Agent, E)
manner(during(E),
directedmotion, Agent)
!contact(during(E),
Paula hit ball Agent V Patient
Agent, Patient)
manner(end(E),forceful,
Agent)
contact(end(E), Agent,
Patient)

Figure 10: VerbNet Template.
2. Missing entities verbs text 22 dead structured event 22
[people] [are] dead. number appears subject, word people added
used subject, added verb.
3. Anaphora resolution text boy hangs sees reports Husseins
execution modeled [boy1 ] sees reports Husseins execution causes [boy1 ]
hangs [boy1 ] (Lappin & Leass, 1994).
4. Negation text Matsui still playing despite struggles modeled as:
[Matsui] struggles causes event Matsui [not] playing. Modeling preventive
connectors (e.g., despite) requires negation modeled event.

4. Experimental Evaluation
section, describe set experiments performed evaluate ability
algorithms predict causality. first evaluate predictive precision algorithm,
continue analyzing part algorithm separately, conclude qualitative evaluation.
4.1 Prediction Evaluation
prediction algorithm trained using news articles period 1851 2009.
world knowledge used algorithm based Web resource snapshots (Section 3)
dated 2009. evaluation performed separate data Wikinews articles
year 2010. refer data test data.
task tackled algorithm addressed before, could find
baseline algorithm compare against. therefore decided compare algorithms
performance human predictors. algorithm human competitors
assigned basic task predicting event given event might cause. evaluate
prediction using two metrics. first metric accuracy: whether predicted
659

fiRadinsky, Davidovich & Markovitch

event actually occurred real world. two possible problems metric.
First, predicted event, though plausible, still might actually occurred real
world. Second, predicted event might happened real world
caused given event, example, trivial predictions always true (the
sun rise). therefore use additional metric, event quality, likelihood
predicted event caused given event.
experiments conducted follows:
1. Event identification algorithm assumes input predictor h
event. find news headlines represent event, randomly sample n = 1500
headlines test data. headline, human evaluator requested
decide whether headline event cause events. denote
set headlines labeled events E. randomly sample k = 50 headlines
E. denote group C.
2. Algorithm event prediction headline ci C, Pundit performs event extraction, produces event P undit(ci ) highest score caused
event represented ci . Although system provides ranked list results,
simplify human evaluation theses results, consider highest score
prediction. tie top score, pick one random. results
stage pairs: {(ci , P undit(ci ))|ci C}.
3. Human event prediction event ci C, human evaluator asked predict
event might cause. evaluator instructed read given headline
predict likely outcome, using online resource time limit.
evaluators presented empty structured forms 5 fields
output event need provide. human result denoted human(ci ).
results stage pairs: {(ci , human(ci ))|ci C}.
4. Human evaluation results
(a) Quality: present = 10 people triplet (ci , human(ci ), P undit(ci )).
human evaluators asked grade (ci , human(ci )) (ci , P undit(ci ))
scale 0-4 (0 highly implausible prediction 4 highly plausible
prediction). allowed use resource limited time.
human evaluators different performed predictions.
(b) Accuracy: predicted event, checked news (and Web resources), year time cause event, see whether
predicted events reported.
Human evaluation conducted using Amazon Mechanical Turk, emerging utility
performing user study evaluations, shown precise certain tasks
(Kittur, Chi, & Suh, 2008). evaluation, tasks created routing question
random users obtaining answers. filtered raters using CAPTCHA.
restricted US-based users, events used system extracted
NYT. perform manual filtering results. average times
human tasks reported table 2. observed time-consuming
660

fiLearning Predict Textual Data

Human Event
Identification
1 min 26 sec

Human Event
Prediction
4 min 10 sec

Human Evaluation
(Quality)
1 min 44 sec

Human Evaluation
(Accuracy)
6 min 24 sec

Table 2: Response times human evaluators different evaluation tasks.

Pundit
Humans

[0-1)

[1-2)

[2-3)

[3-4]

Average
Quality

0
0

2
3

19
24

29
23

3.08
2.86

Table 3: Quality results. histogram rankings users humans
algorithm.

task humans verify event indeed happened past. timeconsuming task Human Event Prediction. surprising, cases required
use external resources, whereas quality evaluation measured whether
events make sense. Additionally, manually investigated human evaluations
category, find correlation response time quality human
prediction. used Mechanical Turk, know external resources
evaluators used. measured inter-rater reliability using Fleiss kappa statistical test,
measures consistency ratings. raters test, obtained
= 0.3, indicates fair agreement (Landis & Koch, 1977; Viera & Garrett, 2005).
result quite significant, following reasons:
1. Conservativeness measure.
2. Subjectivity predictions asking people whether prediction makes sense often
leads high variance responses.
3. Small dataset tests performed 10 people asking categorize 5
different scales plausibility 50 examples.
4. Lack formal guidelines evaluating plausibility prediction instructions given human evaluators regarding considered plausible not.
Additionally, comparison, similar tasks natural language processing, sentence
formality identification (Lahiri, Mitra, & Lu, 2011), usually reach kappa values 0.1 0.2.
quality evaluation yielded Pundits average predictive precision 3.08/4 (3
plausible prediction), compared 2.86/4 humans. event, average
results rankers, producing average score algorithms performance
event, averaged score human predictors (see Table 3). performed
paired t-test k paired scores. advantage algorithm human
evaluators found statistically significant, p 0.05.
661

fiRadinsky, Davidovich & Markovitch

Algorithm
Pundit
Humans

Average Accuracy
63%
42%

Table 4: Prediction accuracy human algorithm.
accuracy results reported Table 4. performed Fishers exact test (as
results binary) k paired scores. results found statistically
significant, p 0.05.
4.2 Component Analysis
section, report results empirical analysis different parts
algorithm.
4.2.1 Evaluation Extraction Process
Section 3.1, described process extracting causality pairs news.
pairs used training set learning algorithm. process consists two
main parts: causality identification event extraction. perform set experiments
provide insights extracted training data quality.
Causality Extraction Experiment first step building training set consists
using causality patterns extract pairs sentences causality relation holds.
assess quality process, randomly sampled 500 pairs training
set presented human evaluators. pair evaluated 5 humans.
filtered raters using CAPTCHA filtered outliers. evaluators shown
two sentences system believed causally related asked evaluate
plausibility relation scale 0-4.
results show averaged precision extracted causality events 3.11
4 (78%), 3 means plausible causality relation, 4 means highly plausible
causality relation. example, causality pair: pulling car 2 New Jersey
police officers shot, got high causality precision score, plausible causeeffect relation, system extracted headline 2 New Jersey Police Officers
Shot Pulling Car.
comparison, temporal rule extraction systems (Chambers, Wang, & Jurafsky,
2007) reach precision 60%. better performance system explained
use specially crafted templates (we attempt solve general problem
temporal information extraction).
causality pairs extracted judged high quality. main reason
errors events, although reported news matching templates
described, common-sense causality knowledge. example, Aborted landing
Beirut Hijackers fly airliner Cyprus, rated unlikely causally related,
although event took place April 09, 1988.

662

fiLearning Predict Textual Data

Quality Precision

Action
93%

Actor
74%

Object
76%

Instrument
79%

Location
79%

Time
100%

Table 5: Extraction precision 5 event components using causality patterns.

Actor
Matching
84%

Object
Matching
83%

Instrument
Matching
79%

Location
Matching
89%

Action
Matching
97%

Table 6: Entity-to-ontology matching precision.

Event Extraction Experiment pair sentences determined casualty relation, algorithm extracts structured event sentences.
event includes following roles: action, actor, object, instrument, time.
assess quality process, used 500 pairs previous experiment presented 1000 associated sentences 5 human evaluators.
evaluators shown sentence together extracted roles: action, actor, object,
instrument, time, asked mark role assignment right
wrong.
Table 5 shows precision extracted event components ranges 74
100%. comparison, works (Chambers & Jurafsky, 2011) extracting entities
different types relations reach 42 53% precision. higher precision results
mainly due use domain-specific templates.
performed additional experiments evaluate matching every entity
experiment world-knowledge ontology. matching based semantic
similarity. ranker asked indicate whether extracted entity mapped
correctly Wikipedia URI. results summarized Table 6.
4.2.2 Evaluation Event Similarity Algorithm
learning prediction algorithms strongly rely event similarity function
dist described Section 2.4. evaluate quality function, randomly sampled
30 events training data found similar event entire
past data (according similarity function). human evaluator asked
evaluate similarity events scale 15. repeated experiment,
replacing average aggregator function f minimum maximum functions.
results presented Table 7. general precision average function
high (3.9). Additionally, average function performed substantially (confirmed
t-test) better minimum maximum. result indicates distance functions aggregate several objects structured event (rather
selecting minimum maximum one events) yield highest performance.
663

fiRadinsky, Davidovich & Markovitch

Minimum
1.9

Maximum
3.5

Average
3.9

Table 7: Comparison different aggregations event-similarity f .
4.2.3 Importance Abstraction
Given cause event whose effect wish predict, use algorithm described
Section 2.4 identify similar generalized events. evaluate importance stage,
compose alternative matching algorithm, similar nearest-neighbor approach
(as applied work Gerber, Gordon, & Sagae, 2010), matches cause event
cause events training data. Instead building abstraction tree, algorithm
simply finds closest cause past based text similarity. rank matched
results using TF-IDF measure.
applied original algorithm baseline algorithm 50 events
used prediction. event, asked human evaluator compare prediction
original baseline algorithm. results showed 83% cases
predictions generalization rated plausible nearestneighbor approach without generalization.
4.2.4 Analysis Rule Generation Application
order generate appropriate prediction respect given cause event,
learned rule applied, described Section 2.5. observe 31% predictions, non-trivial rule generated applied (that is, non-NULL rule
simply output effect observed matched past cause-effect pair example).
those, application predicted correctly 90% cases generated
plausible object effect. results indicate generalization rule-generation
techniques essential performance algorithm.
4.2.5 Analysis Pruning Implausible Causation
eliminate situations generated prediction implausible, devised algorithm (Section 2.7) prevents implausible predictions. randomly selected 200
predictions algorithm predictions based human-labeled events extracted
Wikinews articles (see Section 4.1). human rater requested label predictions considered implausible. applied filtering rules 200
predictions well. algorithm found 15% predictions implausible 70%
precision 90% recall respect human label. qualitative example filtered
prediction Explosion surrender cause event Explosion Afghanistan kills
two.
4.3 Qualitative Analysis
better understanding algorithms strengths weaknesses present
examples results. Given event Louisiana flood, algorithm predicted
[number] people flee. prediction process illustrated Figure 11.
664

fiLearning Predict Textual Data

1. Raw data:
prediction based following raw news articles:
(a) 150000 flee hurricane nears North Carolina coast.
(b) million flee huge storm hits Texas coast.
(c) Thousands flee storm whips coast Florida.
(d) Thousands Dallas Flee Flood Severe Storms Move Southwest.
2. Causality pair extraction:
template used process headlines following structured events:
(a) Cause Event: near (Action); hurricane (Actor); Coast(Object); North Carolina
(Object Attribute) ; (Instrument); Carolina (Location); 31 Aug 1993 (Time).
Effect Event: flee (Action); People (Actor); 150000(Actor Attributes); Carolina
(Location); 31 Aug 1993 (Time).
(b) Cause Event: hit (Action); Storm (Actor); Huge (Actor Attributes); Coast(Object);
Texas (Object Attribute); Texas (Location); 13 Sep 2008 (Time).
Effect Event: flee (Action); People (Actor); million(Actor Attributes); Texas
(Location); 13 Sep 2008 (Time).
(c) Cause Event: whip (Action); Storm (Actor); Coast(Object); Florida (Object
Attribute); Florida (Location); March 19, 1936 (Time).
Effect Event: flee (Action); People (Actor); thousands(Actor Attributes); Florida
(Location); March 19, 1936 (Time).
(d) Cause Event: move (Action); Storm (Actor); Severe (Actor Attributes); Dallas
(Location); May 27, 1957 (Time).
Effect Event: flee (Action); People (Actor); thousands(Actor Attributes);
Flood(Object); Dallas (Location); May 27, 1957 (Time).
3. Learning abstraction tree:
four events clustered together AT. clustered
node causes found similar: actors weather
hazards location state United States. effects found
similar actions actors similar across events, actor
attributes numbers. generalization, following world knowledge
used:
(a) Storm, hurricane flood weather hazards (extracted in-category
relation Wikipedia).
(b) Carolina, Texas, California located United States (extracted
located-in relation Yago).
4. Prediction:
665

fiRadinsky, Davidovich & Markovitch

prediction, event Louisiana flood (which occur training
examples) found similar node, node rule output
[number] people flee.

150000 flee hurricane
nears north Carolina coast.
million flee
huge storm hits
Texas coast.

Cause Event: near (Action);
hurricane (Actor);
Coast(Object);
North Carolina (Object Attribute) ;
(Instrument); Carolina (Location);
31 Aug 1993 (Time).
Effect Event: flee (Action);
People (Actor);
150000(Actor Attributes);
Carolina (Location);
31 Aug 1993 (Time).

Storm, Hurricane Flood
`Weather hazards''
Carolina, Texas, California, Nebraska
``United States''








Louisiana(Location);
Flood (Actor)

people (Actor);
flee (Action)

Implausible
event lter

Figure 11: Examples prediction
another example, given event 6.1 magnitude aftershock earthquake hits Haiti,
highest matching predictions were: [number] people dead, [number] people
missing, [number] magnitude aftershock earthquake strike island near Haiti
earthquake turn United States Virgin Islands. first three predictions seem
reasonable. example, third prediction came rule natural disasters
hitting countries next shore tend affect nearby countries. case predicted
earthquake affect United States Virgin Islands, geographically
close Haiti. fourth prediction, however, realistic earthquake cannot
change course. created match past example tornado hitting
country coast. implausible causation filters prediction, low
PMCI, output system [number] people dead. example
also interesting, issues prediction using spatial locality (the United States Virgin
Islands [near] Haiti).
Additional examples 50 test predictions seen Table
8.
666

fiLearning Predict Textual Data

Cause event

Human-predicted effect event

Al-Qaida demands hostage
exchange
Afghanistans parliament rejects Karzais cabinet nominations
Remains 1912 expedition
plane found Antarctica

Al-Qaida
exchanges
hostage
Parliament
accepts
Karzais cabinet nominations
Europe museums vie remains

North Korea seeks diplomatic
relations US
Volcano erupts Democratic
Republic Congo
Icelands President vetoes repayment Ice save losses
Death toll Brazil mudslides rises sixty
7.0 magnitude earthquake
strikes Haitian coast
2 Palestinians reportedly shot
dead Israeli troops
Professor Tehran University killed bombing

UN officials offer mediation
services
Scientists Republic
Congo investigate lava beds
Banks Reykjavik report
record withdrawals
Rescuers Brazil abandon rescue efforts
Tsunami Haiti affects
coast
Israeli citizens protest
Palestinian leaders
Tehran students remember
slain professor memorial
service
Mafia kills people guns
town
Islamist group would adopt
another name UK
German officials suspend tariffs
Someone fired

Alleged drug kingpin arrested
Mexico
UK bans Islamist group
China overtakes Germany
worlds biggest exporter
Cocaine found Kennedy
Space Center

Algorithm-predicted effect event
country refuse
demand
Many critics rejection
Enduring
mystery
solved
Antarctica
North Korea rift
grow
Thousands people
flee Congo
Official administration reaction issued
Testimonies
heard
Tsunami warning
issued
Israeli troops
face scrutiny
Professor
funeral
held
Kingpin sent
prison
Group grow
Wheat price fall
People arrested

Table 8: Human algorithm predictions events. Predictions bold labeled
evaluators correct predictions.

4.4 Discussion
experiments report precision algorithms. experiments
measuring recall system necessary. However, experiments validation step required human intervention. example, validating prediction occurred
future news. order perform full recall experiment one apply algorithm
news headlines reported certain day measure appearance
667

fiRadinsky, Davidovich & Markovitch

corresponding predictions future news. Unfortunately, performing human validation
large prediction space hard. leave task performing experiments
provide rough estimate recall future work.
common practice compare system performance previous systems tackling
problem. However, ambitious task tackled work immediate
baselines compare with. is, comparable system neither scale
ability take arbitrary cause event natural language output effect event
natural language. Instead, compared agents know capable performing
task humans.
Although results indicate superiority system human agents,
claim system predictions perform better humans. rather provide
evidence system provides similar predictions humans, sometimes
even outperforms human ability predict, supported superiority
system accuracy evaluation.
fully support claim superiority system humans, wider experiments
performed. Experiments larger order magnitude provide results
higher agreement raters shed light different types events
systems performance better. Additionally, experiments comparing system
performance experts fields individual prediction valuable
well. point, assume performance experts would higher
algorithm. main reason causality knowledge used train
algorithms. knowledge extracted headlines tend simple causality
contents, easily understandable general population. type knowledge
limits complexity predictions made Pundit. Pundit predictions
therefore tend closer common knowledge average human. order
predict complex events would need rely better training examples news
headlines alone.
evaluation presented section provides evidence quality predictions system provide. results impressive sense
comparable humans, thus providing evidence ability machine
perform one desirable goals general AI.

5. Related Work
aware work attempts perform task face: receive arbitrary
news events natural language representation predict events cause. Several
works, however, deal related tasks. general, work focus better information extraction causality extraction techniques, rather information
leveraged prediction. present novel methods combining world knowledge
event extraction methods represent coherent events, present novel methods
rule extraction generalization using knowledge.
5.1 Prediction Web Behavior, Books Social Media
Several works focused using search-engine queries prediction traditional
media (Radinsky, Davidovich, & Markovitch, 2008) blogs (Adar, Weld, Bershad, &
668

fiLearning Predict Textual Data

Gribble, 2007). Ginsberg et al. (2009) used queries predicting H1N1 influenza outbreaks. context causality recognition, Gordon, Bejan, Sagae (2011) present
methodology mining blogs extract common-sense causality. evaluation done
human-labeled dataset test consists fact two possible effects. Applying point-mutual information personal blog stories, authors select best prediction
candidate. work differs authors focus personal commonsense mining consider whether predictions actually occurred. works
focused predicting Web content change itself. example, Kleinberg (2002, 2006) developed general techniques summarizing temporal dynamics textual content
identifying bursts terms within content. Similarly, Amodeo, Blanco, Brefeld (2011)
built time series model publication dates documents relevant query order
predict future bursts. Social media used predict riots (Kalev, 2011) movie box
office sales (Asur & Huberman, 2010; Joshi, Das, Gimpel, & Smith, 2010; Mishne, 2006).
works (Jatowt & Yeung, 2011; Yeung & Jatowt, 2011; Michel, Shen, Aiden, Veres,
Gray, Google Books Team, Pickett, Hoiberg, Clancy, Norvig, Orwant, Pinker, Nowak, &
Aiden, 2011) explored use text mining techniques news books explain
culture develops, peoples expectations memories are.
work differs several ways: First, present general-purpose
prediction algorithm rather domain-specific one. Second, unlike works,
combines variety heterogenous online sources, including world knowledge mined
Web. Finally, focus generation future event predictions represented
entirely natural language, provide techniques enrich generalize historical
events purpose future event prediction.
5.2 Textual Entailment
related topic work textual entailment (TE) (Glickman, Dagan, & Koppel,
2005). text said entail textual hypothesis h people reading agree
meaning implies truth h. TE divided three main categories:
recognition, generation, extraction. section, provide short summary
first two categories. detailed overview refer reader survey
Androutsopoulos Malakasiotis (2010). discuss specific task causality
extraction text Section 5.3.4.
5.2.1 Textual Entailment recognition
task, pairs texts given input, output whether TE relations hold
pair. approaches map text logical expressions (with semantic enrichment, using WordNet, example) perform logical entailment checks, usually using
theorem provers (Raina, Ng, & Manning, 2005; Bos & Markert, 2005; Tatu & Moldovan,
2005). approaches map two texts vector space model, word
mapped strongly co-occurring words corpus (Mitchell & Lapata, 2008),
similarity measures vectors applied. measure syntactic similarity applying graph similarity measure syntactic dependency graphs two
texts (Zanzotto, Pennacchiotti, & Moschitti, 2009). Similarly, methods measure
semantic distance similarity words text (Haghighi, 2005), usually exploiting
669

fiRadinsky, Davidovich & Markovitch

resources WordNet well. last set approaches represents two texts
single feature vector trains machine learning algorithm, later, given two
new texts represented via vector, determine whether entail (Bos &
Markert, 2005; Burchardt, Pennacchiotti, Thater, & Pinkal, 2009; Hickl, 2008). example, Glickman et al. (2005) show naive Bayes classifier trained lexical features, i.e.,
number times words appeared words h. features usually include
polarity (Haghighi, 2005), whether theorem prover managed prove entailment (Bos
& Markert, 2005), tagging named entities categories people, organizations,
locations.
5.2.2 Textual Entailment Generation
discuss TE generation, where, given expression, system output
set expressions entailed input. task closely related
one presented work: TE generation, text received entailed text
generated output. Androutsopoulos Malakasiotis (2010) mention benchmarks
exist evaluate task, common costly approach evaluate using
human judges. also encountered difficulty task, performed human
evaluation.
TE generation methods divided two types: use machine translation techniques use template-based techniques. use machine
translation techniques try calculate set transformations highest probability, using training corpus. Quirk, Brockett, Dolan (2004) cluster news articles
referring event, select pairs similar sentences, apply aforementioned
techniques. methods use template-based approaches large corpora,
Web. methods (Idan, Tanev, & Dagan, 2004) start initial seed sentences
(composed entities), use search engine find entities entailment relations hold. relations used templates. find additional entities
relations hold, relations searched again. TE generation system, given text, matches template outputs texts matched
template. Others (Ravichandran, Ittycheriah, & Roukos, 2003) also add additional
filtering techniques templates.
work closely related template-based approach. crafted
new set templates extract causality pairs news.
5.3 Information Extraction
Information Extraction study automatic extraction information unstructured sources. categorizes types information extracted three types: entities,
relationships entities, higher-order structures tables lists.
closely related tasks entity extraction relation extraction;
rest refer reader survey Sarawagi (2008). former task, similar
process extracting concepts, deals extracting noun phrases text.
latter task, given document relation input, problem extract entity
pairs document relation holds. Whereas works deal
one element problem extraction information needed understand given
670

fiLearning Predict Textual Data

causality, deal actual causality prediction. claim create
precise information extraction methods, rather try leverage knowledge
perform important AI task future event prediction.
5.3.1 Entity Extraction
entity extraction, two categories methods exist rule-based statistical methods.
Rule-based methods (Riloff, 1993; Riloff & Jones, 1999; Jayram, Krishnamurthy, Raghavan,
Vaithyanathan, & Zhu, 2006; Shen, Doan, Naughton, & Ramakrishnan, 2007; Ciravegna,
2001; Maynard, Tablan, Ursu, Cunningham, & Wilks, 2001; Hobbs, Bear, Israel, & Tyson,
1993) define contextual patterns consisting regular expression features entities
text (e.g., entity word, part-of-speech tagging). rules either manually
coded domain expert learned using bottom-up (Ciravegna, 2001; Califf & Mooney,
1999) top-down learners (Soderland, 1999). Others follow statistical methods define
numerous features sentence treat problem classification problem,
applying well-known machine learning algorithms (e.g., Hidden Markov Models; Agichtein
& Ganti, 2004; Borkar, Deshmukh, & Sarawagi, 2001). system deal
many challenges field, propose large scale domain-specific approach driven
specific extraction templates.
5.3.2 Relation Extraction
Relation extraction developed widely last years large text corpora
(Schubert, 2002) and, particular, different Web resources, general Web
content (Banko et al., 2007; Carlson et al., 2010; Hoffmann, Zhang, & Weld, 2010), blogs
(Jayram et al., 2006), Wikipedia (Suchanek et al., 2007), news articles (e.g., topic
detection tracking task (Section 5.3.3)). Given two entities, first task domain classify relationship. Many feature-based methods (Jiang & Zhai, 2007;
Kambhatla, 2004; Suchanek, 2006) rule-based methods (Aitken, 2002; Mcdonald, Chen,
Su, & Marshall, 2004; Jayram et al., 2006; Shen et al., 2007) developed
task. methods use different features extracted text, words,
grammar features, parse tree dependency graphs, features extra ion
external relation repositories (e.g., Wikipedia Infobox) add additional features (Nguyen
& Moschitti, 2011; Hoffmann, Zhang, Ling, Zettlemoyer, & Weld, 2011). Labeled training
examples, feature extracted, fed machine learning classifier, sometimes using transformations kernels (Zhao & Grishman, 2005; Zhang,
Zhang, Su, & Zhou, 2006; Zelenko, Aone, & Richardella, 2003; Wang, 2008; Culotta &
Sorensen, 2004; Bunescu & Mooney, 2005; Nguyen, Moschitti, & Riccardi, 2009), which,
given new unseen entities, able classify categories.
Given relation, second common task domain find entities satisfy
relation. information extraction tasks, task relevant ours,
try find structured events causality relation holds. works
domain focus large collections, Web, labeling entities relations
infeasible (Agichtein & Gravano, 2000; Banko et al., 2007; Bunescu & Mooney, 2007;
Rosenfeld & Feldman, 2007; Shinyama & Sekine, 2006; Turney, 2006). Usually seed entity
databases used, along manual extraction templates, expanded
671

fiRadinsky, Davidovich & Markovitch

filtered iteratively. Sarawagi states spite extensive research topic,
relationship extraction means solved problem. accuracy values still range
neighborhood 50%70% even closed benchmark datasets . . . open domains
like Web, state-of-the-art systems still involve lot special case handling
cannot easily described principled, portable approaches. (Sarawagi, 2008, p. 331).
Similarly, task size corpus allow us assume labeled sets.
Instead, like common approaches presented here, also start predefined set
patterns.
5.3.3 Temporal Information Extraction
temporal information extraction task deals extraction ordering events
many events time. Temporal information extraction categorized three
main subtasks predicting temporal order events time expressions described
text, predicting relation events, identifying document
written. task found important many natural language processing
applications, question answering, information extraction, machine translation
text summarization, require mere surface understanding.
approaches (Ling & Weld, 2010; Mani, Schiffman, & Zhang, 2003; Lapata & Lascarides, 2006; Chambers et al., 2007; Tatu & Srikanth, 2008; Yoshikawa, Riedel, Asahara,
& Matsumoto, 2009) learn classifiers predict temporal order pair events
predefined features pair.
related works deal topic detection tracking (Cieri, Graff, Liberman,
Martey, & Strassel, 2000). area includes several tasks (Allan, 2002). them,
multiple, heterogenous new sources used, including audio. story segmentation task
aims segment data constituent stories. topic tracking task e.g., work
Shahaf Guestrin (2010) aims find stories discussing certain topic. subtask
link detection task which, given pair stories, aims classify whether
topic. topic detection task e.g. works Ahmed, Ho,
Eisenstein, Xing, Smola, Teo (2011) Yang, Pierce, Carbonell (1998) aims
detect clusters topic-cohesive stories stream topics. first-story detection task
aims identify first story topic (Jian Zhang & Yang, 2004). paper,
focused short text headlines extraction events them. work differs
temporal information extraction, generate predictions future
events, whereas temporal information extraction tasks focus identifying clustering
text corpus topics.
5.3.4 Causality Pattern Extraction Recognition
first stage learning process extract causality pairs text. Causality
extraction discussed literature past, divided
following subgroups:
1. Use handcrafted domain-specific patterns. studies deal causality extraction using specific domain knowledge. Kaplan Berry-Rogghe (1991) used scientific
texts create manually designed set propositions later applied
672

fiLearning Predict Textual Data

new texts extract causality. methods require handcrafted domain knowledge,
problematic obtain real-world tasks, especially large amounts.
2. Use handcrafted linguistic patterns. works use general approach
applying linguistic patterns. example, Garcia (1997) manually identified 23
causative verb groups (e.g., result in, lead to, etc.). sentence contained one
verbs, classified containing causation relation. precision 85%
reported. Khoo et al. (2000) used manually extracted graphical patterns based
syntactic parse trees, reporting accuracy 68% English medical
database. Similarly, Girju Moldovan (2002) defined lexicon-syntactic patterns
(pairs noun phrases causative verb between) additional semantic
constraints.
3. Semi-supervised pattern learning approaches. set approaches uses supervised
machine learning techniques identify causality text. example, Blanco et al.
(2008) Sil et al. (2010) use syntactic patterns features later fed
classifiers, whose output whether text implies causality cause effect
themselves.
4. Supervised pattern learning approaches. many works design
inference rules discover extraction patterns given relation using training examples (Riloff, 1996; Riloff & Jones, 1999; Agichtein & Gravano, 2000; Lin & Pantel,
2001). Specifically, Chan Lam (2005) dealt problem creating syntactic
patterns cause-effect extraction.
domain causality pattern extraction, work resembles handcrafted
linguistic patterns pattern approaches. evaluated performance specific
domain. Since goal obtain precise set examples feed
learning, chose follow approach well.
5.4 Learning Causality
drawn algorithmic motivation work machine learning
community. section, give partial review main areas machine learning
relevant work.
5.4.1 Bayesian Causal Inference
functional causal model (Pearl, 2000) assumes set observables X1 . . . Xn ,
vertices directed acyclic graph G. semantics graph parents
node directed causes. shown satisfy Reichenbachs common cause principle,
states node Z children X, , X statistically dependent,
Z causally influencing both. model, similar Bayesian network,
satisfies several conditions: (1) Local Causal Markov condition: node statistically
independent non-descendants, given parents; (2) Global Causal
Q Markov condition: dseparation criterion; (3) Factorization criterion: P (X1 , . . . , Xn ) = P (Xi |P arents(Xi )).
theoretical literature inference learning causality models extensive.
models resemble work use structural models. literature inference
673

fiRadinsky, Davidovich & Markovitch

learning causality models extensive, knowledge solutions
scale scope tasks discussed paper. contrast Bayesian approach,
causality graph work contains less detailed information. work combines
several linguistic resources learned data several heuristics build
causality graph.
5.4.2 Structured Learning
important problem machine learning field structured learning, input
output classifier complex structure, relational domain,
object related another, either time features. task resembles structured
learning also use structured input (structured events given input) produce
structured event output.
Many generative models developed, including hidden Markov models, Markov
logic networks, conditional random fields, among others. approaches use transformations, kernels, unite objects, ignoring structure, feed
standard structured classifier, e.g., kernelized conditional random fields (Lafferty,
Zhu, & Liu, 2004), maximum margin Markov networks (Taskar, Guestrin, & Koller, 2003),
others (Bakir, Hofmann, Scholkopf, Smola, Taskar, & Vishwanathan, 2007).
dealing complex output, annotated parse trees natural language problems,
approaches define distance metric label space objects,
apply standard machine learning algorithms, e.g., structured support vector machines
(Joachims, 2006).
5.4.3 Learning Positive Examples (One Class Classification)
system fed examples sort causes b, examples sort
cause b, must deal problem learning positive examples only.
challenge multi-class learning mechanisms, require negative
positive examples. theoretical studies possibility learning
positive unlabeled data provided work Denis (1998) (probably approximately
correct (PAC) learning) Muggleton (1996) (Bayesian learning).
works (Tax, 2001; Manevitz & Yousef, 2000; Manevitz, Yousef, Cristianini, ShaweTaylor, & Williamson, 2001) domain develop algorithms use one-class SVM
(Vapnik, 1995) learn support using positive distribution. construct
decision boundaries around positive examples differentiate possible
negative data. Tax Duin (1991) use hyper-sphere defined radius around
positive class points (support vector data description method). also use
kernel tricks finding sphere (Tax, 2001). Scholkopf et al. (1999, 2000) develop
methods try separate surface region positive labeled data region
unlabeled data.

6. Conclusions
Much research carried information extraction ontology building.
work, discuss leverage knowledge large-scale AI problem event
674

fiLearning Predict Textual Data

prediction. present system trained predict future events, using cause event
input. event represented tuple one predicate 4 general semantic roles.
event pairs used training extracted automatically news headlines using
simple syntactic patterns. Generalization unseen events achieved by:
1. Creating abstraction tree (AT) contains entities observed events together
subsuming categories extracted available online ontologies.
2. Finding predicate paths connecting entities cause events entities effect
events, paths extracted available ontologies.
discuss many challenges building system: obtaining large enough dataset,
representing knowledge, developing inference algorithms required
task. perform large-scale mining apply natural language techniques transform
raw data 150 years history archives structured representation events,
using mined Web-based object hierarchy action classes. shows scalability
proposed method, crucial method requires large amounts
data work well. However, engineering design analysis performed
scale entire knowledge web provide real-time alerts. also show
numerous resources built different people different purposes (e.g., different
ontologies) fact merged via concept-graph build system work well
practice.
perform large-scale learning large data corpus present novel inference
techniques. consider rule extraction generalization. propose novel methods
rule generalization using existing ontologies, believe useful many
related tasks. Tasks entailment topic tracking benefit
concepts understanding sequences generalizations.
work scratch surface real-time fully functional
prediction system. Due complexity problem, size system
many components, errors unavoidable. example, errors due noise event
extraction, noise similarity calculation events, etc. Although perform
experiments analyzing different components system errors addition
overall system performance, believe additional training examples better
sources knowledge deeper ontologies bring many improvements algorithms.
future work, suggest following directions extensions:
1. Better event extraction event matching Event extraction techniques, e.g.,
proposed et al. (2011) provide higher analysis data entire
text rather titles. Event similarity enriched many ways, e.g.,
work compared three aggregation functions f , however, coherent
way learning weights Oi past data applied.
2. Analysis knowledge sources believe in-depth analysis different
types knowledge obtained Web individual contributions
studied. work, explore sensitivity system initial
noise conceptual networks, believe proper analysis
better networks provide higher prediction accuracy, already carried
LinkedData community.
675

fiRadinsky, Davidovich & Markovitch

3. Large scale experiments Performance larger experiments humans larger
periods times, even comparison experts provide insights
performance reliability system. Automation experiments without
human involvement measure accuracy predictions make possible provide
richer metrics performance, recall.
4. Time effect work, events treated similarly, even events 100
years ago. future directions, wish investigate give decaying weight
information events system, causality learned event
took place 1851 might less relevant prediction 2010. However, much
common-sense knowledge still used even learned events happened
long time ago. example, headlines Order Restored Riots (1941)
Games Suspended Riot (1962) still relevant today.
experimental evaluation showed predictions Pundit algorithm
least good non-expert humans. believe work one first
harness vast amount information available Web perform event prediction
general purpose, knowledge based, human-like.

References
Adar, E., Weld, D. S., Bershad, B. N., & Gribble, S. D. (2007). search: visualizing
predicting user behavior. Proceedings International Conference
World Wide Web (WWW).
Agichtein, E., & Ganti, V. (2004). Mining reference tables automatic text segmentation.
Proceedings Tenth ACM SIGKDD International Conference Knowledge
Discovery Data Mining (KDD).
Agichtein, E., & Gravano, L. (2000). Snowball: extracting relations large plain-text
collections. Proceedings Joint Conference Digital Libraries (JCDL), pp. 85
94.
Ahmed, A., Ho, Q., Eisenstein, J., Xing, E. P., Smola, A. J., & Teo, C. H. (2011). Unified
analysis streaming news. Proceedings International Conference
World Wide Web (WWW).
Aitken, J. (2002). Learning information extraction rules: inductive logic programming
approach. Proceedings 15th European Conference Artificial Intelligence
(ECAI), pp. 355359.
Allan, J. (Ed.). (2002). Topic Detection Tracking: Event-based Information Organization, Vol. 12. Kluwer Academic Publishers, Norwell, MA, USA.
Amodeo, G., Blanco, R., & Brefeld, U. (2011). Hybrid models future event prediction.
Proceedings ACM Conference Information Knowledge Management
(CIKM).
Androutsopoulos, I., & Malakasiotis, P. (2010). survey paraphrasing textual
entailment methods. Journal Artificial Intelligence Research (JAIR), 38, 135187.
Asur, S., & Huberman, B. A. (2010). Predicting future social media. ArxiV.
676

fiLearning Predict Textual Data

Bakir, G. H., Hofmann, T., Scholkopf, B., Smola, A. J., Taskar, B., & Vishwanathan, S.
V. N. (2007). Predicting Structured Data. MIT Press.
Banko, M., Cafarella, M. J., Soderl, S., Broadhead, M., & Etzioni, O. (2007). Open information extraction web. Proceedings International Joint Conferences
Artificial Intelligence (IJCAI).
Bizer, C., Heath, T., & Berners-Lee, T. (2009). Linked data story far. International
Journal Semantic Web Information Systems (IJSWIS).
Bizer, C., & Schultz, A. (2009). berlin sparql benchmark. International Journal
Semantic Web Information Systems (IJSWIS).
Blanco, E., Castell, N., & Moldovan, D. (2008). Causal Relation Extraction. Proceedings
International Conference Language Resources Evaluation (LREC).
Borkar, V., Deshmukh, K., & Sarawagi, S. (2001). Automatic text segmentation extracting structured records. Proceedings ACM SIGMOD International Conference
Management Data (KDD).
Bos, J., & Markert, K. (2005). Recognising textual entailment logical inference.
Proceedings Human Language Technology Conference Conference Empirical
Methods Natural Language Processing (HLT EMNLP).
Bunescu, R., & Mooney, R. (2007). Learning extract relations web using
minimal supervision. Proceedings 45th Annual Meeting Association
Computational Linguistics (ACL), pp. 576583.
Bunescu, R. C., & Mooney, R. J. (2005). shortest path dependency kernel relation
extraction. Proceedings Conference Human Language Technology
Empirical Methods Natural Language Processing (HLT EMNLP), pp. 724731.
Burchardt, A., Pennacchiotti, M., Thater, S., & Pinkal, M. (2009). Assessing impact
frame semantics textual entailment. Natural Language Engineering, 15, 527550.
Califf, M. E., & Mooney, R. J. (1999). Relational learning pattern-match rules information extraction. Proceedings Sixteenth National Conference Artificial
Intelligence (AAAI), pp. 328334.
Carlson, A., Betteridge, J., Kisiel, B., Settles, B., Hruschka, E., & Mitchell, T. (2010).
Toward architecture never-ending language learning. Proceedings
Association Advancement Artificial Intelligence (AAAI).
Chambers, N., & Jurafsky, D. (2011). Template-Based Information Extraction without
Templates. Proceedings Annual Meeting Association Computational
Linguistics (ACL).
Chambers, N., Wang, S., & Jurafsky, D. (2007). Classifying temporal relations
events. Proceedings Annual Meeting Association Computational
Linguistics (ACL) (Poster).
Chan, K., & Lam, W. (2005). Extracting causation knowledge natural language texts.
International Journal Information Security (IJIS), 20, 327358.
677

fiRadinsky, Davidovich & Markovitch

Cieri, C., Graff, D., Liberman, M., Martey, N., & Strassel, S. (2000). Large, multilingual,
broadcast news corpora cooperative research topic detection tracking:
tdt-2 tdt-3 corpus efforts. Proceedings International Conference
Language Resources Evaluation (LREC).
Ciravegna, F. (2001). Adaptive information extraction text rule induction
generalisation. Proceedings 17th International Joint Conference Artificial
Intelligence (IJCAI).
Culotta, A., & Sorensen, J. (2004). Dependency tree kernels relation extraction.
Proceedings 42nd Meeting Association Computational Linguistics
(ACL), pp. 423429.
Dang, H. T., Palmer, M., & Rosenzweig, J. (1998). Investigating regular sense extensions
based intersective levin classes. Proceedings International Conference
Computational Linguistics (COLING).
Denis, F. (1998). PAC learning positive statistical queries. Proceedings
International Conference Algorithmic Learning Theory (ALT), pp. 112126.
Do, Q., Chan, Y., & Roth, D. (2011). Minimally supervised event causality identification.
Proceedings Conference Empirical Methods Natural Language Processing
(EMNLP).
Eisen, M. B., Spellman, P. T., Brown, P. O., & Botstein, D. (1998). Cluster analysis
display genome-wide expression patterns. PNAS, 95, 1486314868.
Garcia, D. (1997). Coatis, NLP system locate expressions actions connected
causality links. Proceedings Knowledge Engineering Knowledge Management
Masses (EKAW).
Gerber, M., Gordon, A. S., & Sagae, K. (2010). Open-domain commonsense reasoning using
discourse relations corpus weblog stories. Proceedings Formalisms
Methodology Learning Reading, NAACL-2010 Workshop.
Ginsberg, J., Mohebbi, M. H., Patel, R. S., Brammer, L., Smolinski, M. S., & Brilliant, L.
(2009). Detecting influenza epidemics using search engine query data. Nature, 457,
10121014.
Girju, R., & Moldovan, D. (2002). Text mining causal relations. Proceedings
Annual International Conference Florida Artificial Intelligence Research
Society (FLAIRS), pp. 360364.
Giuglea, A.-M., & Moschitti, A. (2006). Shallow semantic parsing based framenet, verbnet propbank. Proceedings 17th European Conference Artificial
Intelligence (ECAI 2006).
Glickman, O., Dagan, I., & Koppel, M. (2005). probabilistic classification approach
lexical textual entailment. Proceedings Association Advancement
Artificial Intelligence (AAAI).
Gordon, A. S., Bejan, C. A., & Sagae, K. (2011). Commonsense causal reasoning using
millions personal stories. Proceedings Association Advancement
Artificial Intelligence (AAAI).
678

fiLearning Predict Textual Data

Haghighi, A. D. (2005). Robust textual inference via graph matching. Proceedings
Human Language Technology Conference Conference Empirical Methods Natural
Language Processing (HLT EMNLP).
Hickl, A. (2008). Using discourse commitments recognize textual entailment. Proceedings International Conference Computational Linguistics (COLING).
Hobbs, J. R., Bear, J., Israel, D., & Tyson, M. (1993). Fastus: finite-state processor
information extraction real-world text. Proceedings 13th International
Joint Conference Artificial Intelligence (IJCAI), pp. 11721178.
Hoffmann, R., Zhang, C., Ling, X., Zettlemoyer, L., & Weld, D. S. (2011). Knowledge-based
weak supervision information extraction overlapping relations. Proceedings
49th Annual Meeting Association Computational Linguistics: Human
Language Technologies (HLT).
Hoffmann, R., Zhang, C., & Weld, D. S. (2010). Learning 5000 relational extractors. Proceedings 48th Annual Meeting Association Computational Linguistics
(ACL).
Idan, I. S., Tanev, H., & Dagan, I. (2004). Scaling web-based acquisition entailment
relations. Proceedings Conference Empirical Methods Natural Language
Processing (EMNLP), pp. 4148.
Jatowt, A., & Yeung, C. (2011). Extracting collective expectations future
large text collections. Proceedings ACM Conference Information
Knowledge Management (CIKM).
Jayram, T. S., Krishnamurthy, R., Raghavan, S., Vaithyanathan, S., & Zhu, H. (2006).
Avatar information extraction system. IEEE Data Engineering Bulletin, 29, 4048.
Jian Zhang, Z. G., & Yang, Y. (2004). probabilistic model online document clustering
application novelty detection. Proceedings Annual Conference
Neural Information Processing Systems (NIPS).
Jiang, J., & Zhai, C. (2007). systematic exploration feature space relation
extraction. Proceedings Human Language Technologies Conference
North American Chapter Association Computational Linguistics (HLT
NAACL), pp. 113120.
Joachims, T. (2006). Structured output prediction support vector machines. Yeung,
D.-Y., Kwok, J., Fred, A., Roli, F., & de Ridder, D. (Eds.), Structural, Syntactic,
Statistical Pattern Recognition, Vol. 4109 Lecture Notes Computer Science, pp.
17. Springer Berlin / Heidelberg.
Joshi, M., Das, D., Gimpel, K., & Smith, N. A. (2010). Movie reviews revenues:
experiment text regression. Proceedings North American Chapter
Association Computational Linguistics - Human Language Technologies (NAACL
HLT).
Kalev (2011). Culturomics 2.0: Forecasting large-scale human behavior using global news
media tone time space. First Monday, 15 (9).
679

fiRadinsky, Davidovich & Markovitch

Kambhatla, N. (2004). Combining lexical, syntactic semantic features maximum
entropy models information extraction. Proceedings Annual Meeting
Association Computational Linguistics (ACL), pp. 178181.
Kaplan, R., & Berry-Rogghe, G. (1991). Knowledge-based acquisition causal relationships
text. Knowledge Acquisition, 3, 317337.
Khoo, C., Chan, S., & Niu, Y. (2000). Extracting causal knowledge medical database
using graphical patterns. Proceedings Annual Meeting Association
Computational Linguistics (ACL), pp. 336343.
Kim, J. (1993). Supervenience mind. Selected Philosophical Essays.
Kipper, K. (2006). Extending verbnet novel verb classes. Proceedings International Conference Language Resources Evaluation (LREC).
Kittur, A., Chi, H., & Suh, B. (2008). Crowdsourcing user studies mechanical turk.
Proceedings ACM CHI Conference Human Factors Computing Systems
premier International Conference human-computer interaction (CHI).
Kleinberg, J. (2006). Temporal dynamics on-line information systems. Data Stream
Management: Processing High-Speed Data Streams. Springer.
Kleinberg, J. (2002). Bursty hierarchical structure streams. Proceedings
Annual ACM SIGKDD Conference (KDD).
Lafferty, J., Zhu, X., & Liu, Y. (2004). Kernel conditional random fields: Representation
clique selection. 21st International Conference Machine Learning (ICML).
Lahiri, S., Mitra, P., & Lu, X. (2011). Informality judgment sentence level experiments formality score. Proceedings 12th International Conference
Computational Linguistics Intelligent Text Processing (CICLing).
Landis, & Koch (1977). measurement observer agreement categorical data.
Biometrics, 33 (1), 74159.
Lapata, M., & Lascarides, A. (2006). Learning sentence-internal temporal relations. Journal
Artificial Intelligence Research (JAIR), 27, 85117.
Lappin, S., & Leass, H. (1994). algorithm pronominal anaphora resolution. Computational Linguistics, 20, 535561.
Lassila, O., Swick, R. R., Wide, W., & Consortium, W. (1998). Resource description framework (rdf) model syntax specification..
Lenat, D. B., & Guha, R. V. (1990). Building Large Knowledge-Based Systems: Representation Inference Cyc Project. Addison-Wesley.
Levin, B., & Hovav, M. R. (1994). preliminary analysis causative verbs english.
Lingua, 92, 3577.
Lin, D., & Pantel, P. (2001). Dirt-discovery inference rules text. Proceedings
Annual ACM SIGKDD Conference (KDD).
Ling, X., & Weld, D. (2010). Temporal information extraction. Proceedings
Association Advancement Artificial Intelligence (AAAI).
680

fiLearning Predict Textual Data

Liu, H., & Singh, P. (2004). Conceptnet: practical commonsense reasoning toolkit. BT
Technology Journal, 22, 211226.
Manevitz, L. M., & Yousef, M. (2000). Document classification neural networks using
positive examples. Proceedings 23rd Annual International ACM SIGIR
Conference Research Development Information Retrieval (SIGIR), pp. 304
306.
Manevitz, L. M., Yousef, M., Cristianini, N., Shawe-Taylor, J., & Williamson, B. (2001).
One-class svms document classification. Journal Machine Learning Research,
2, 139154.
Mani, I., Schiffman, B., & Zhang, J. (2003). Inferring temporal ordering events news.
Proceedings North American Chapter Association Computational
Linguistics - Human Language Technologies (NAACL HLT).
Marneffe, M., MacCartney, B., & Manning, C. (2006). Generating typed dependency parses
phrase structure parses. Proceedings International Conference Language Resources Evaluation (LREC).
Maynard, D., Tablan, V., Ursu, C., Cunningham, H., & Wilks, Y. (2001). Named entity
recognition diverse text types. Recent Advances Natural Language Processing Conference (RANLP), pp. 11721178.
Mcdonald, D. M., Chen, H., Su, H., & Marshall, B. B. (2004). Extracting gene pathway
relations using hybrid grammar: arizona relation parser. Bioinformatics, 20,
33703378.
Michel, J., Shen, Y., Aiden, A., Veres, A., Gray, M., Google Books Team, Pickett, J.,
Hoiberg, D., Clancy, D., Norvig, P., Orwant, J., Pinker, S., Nowak, M., & Aiden, E.
(2011). Quantitative analysis culture using millions digitized books. Science,
331, 176182.
Miller, G. (1995). Wordnet: lexical database english. Journal Communications
ACM (CACM), 38, 3941.
Mishne, G. (2006). Predicting movie sales blogger sentiment. Proceedings
Association Advancement Artificial Intelligence (AAAI) Spring Symposium.
Mitchell, J., & Lapata, M. (2008). Vector-based models semantic composition. Proceedings Annual Meeting Association Computational Linguistics (ACL).
Muggleton, S. (1996). Learning positive data. Proceedings Inductive Logic
Programming Workshop, pp. 358376.
Nguyen, T.-V. T., & Moschitti, A. (2011). Joint distant direct supervision relation
extraction. Proceedings 5th International Joint Conference Natural
Language Processing (IJCNLP).
Nguyen, T.-V. T., Moschitti, A., & Riccardi, G. (2009). Convolution kernels constituent,
dependency sequential structures relation extraction. Proceedings
2009 Conference Empirical Methods Natural Language Processing (EMNLP).
Pearl, J. (2000). Causality: Models, Reasoning, Inference. Cambridge University Press.
681

fiRadinsky, Davidovich & Markovitch

Quirk, C., Brockett, C., & Dolan, W. (2004). Monolingual machine translation paraphrase generation. Proceedings Conference Empirical Methods Natural
Language Processing (EMNLP), pp. 142149.
Rada, R., Mili, H., Bicknell, E., & Blettner, M. (1989). Development application
metric semantic nets. IEEE Transactions Systems, Man Cybernetics, 19 (1),
1730.
Radinsky, K., Davidovich, S., & Markovitch, S. (2008). Predicting news tomorrow
using patterns web search queries. Proceedings IEEE/WIC International
Conference Web Intelligence (WI).
Raina, R., Ng, A. Y., & Manning, C. D. (2005). Robust textual inference via learning
abductive reasoning. Proceedings Association Advancement
Artificial Intelligence (AAAI).
Ravichandran, D., Ittycheriah, A., & Roukos, S. (2003). Automatic derivation surface text
patterns maximum entropy based question answering system. Proceedings
North American Chapter Association Computational Linguistics: Short
Papers (NAACL Short), pp. 8587.
Riloff, E. (1993). Automatically constructing dictionary information extraction
tasks. Proceedings Association Advancement Artificial Intelligence
(AAAI), pp. 811816.
Riloff, E. (1996). Automatically Generating Extraction Patterns Untagged Text.
Proceedings Association Advancement Artificial Intelligence (AAAI).
Riloff, E., & Jones, R. (1999). Learning dictionaries information extraction multi-level
bootstrapping. Proceedings Association Advancement Artificial
Intelligence (AAAI).
Rosenfeld, B., & Feldman, R. (2007). Using corpus statistics entities improve semisupervised relation extraction web. Proceedings 45th Annual Meeting
Association Computational Linguistics (ACL), pp. 600607.
Sarawagi, S. (2008). Information extraction. Foundations Trends Databases, 1 (3),
261377.
Scholkopf, B., Williamson, R., Smola, A., Shawe-Taylor, J., & Platt, J. (2000). Support
vector method novelty detection. Proceedings Annual Conference
Neural Information Processing Systems (NIPS), pp. 582588.
Scholkopf, B., Williamson, R. C., Smola, A., & Shawe-Taylor, J. (1999). Sv estimation
distributions support. Proceedings Annual Conference Neural Information
Processing Systems (NIPS).
Schubert, L. (2002). derive general world knowledge texts?. Proceedings
Second Conference Human Language Technology (HLT).
Shahaf, D., & Guestrin, C. (2010). Connecting dots news articles. Proceedings
Annual ACM SIGKDD Conference (KDD).
682

fiLearning Predict Textual Data

Shen, W., Doan, A., Naughton, J. F., & Ramakrishnan, R. (2007). Declarative information
extraction using datalog embedded extraction predicates. Proceedings
Conference Large Data Bases (VLDB), pp. 10331044.
Shi, L., & Mihalcea, R. (2005). Putting pieces together: Combining framenet, verbnet
wordnet robust semantic parsing. Proceedings Sixth International
Conference Intelligent Text Processing Computational Linguistics (CICLing),
pp. 100111.
Shinyama, Y., & Sekine, S. (2006). Preemptive information extraction using unrestricted
relation discovery. Proceedings North American Chapter Association
Computational Linguistics - Human Language Technologies (NAACL HLT).
Sil, A., Huang, F., & Yates, A. (2010). Extracting action event semantics web
text. Proceedings Association Advancement Artificial Intelligence
(AAAI) Fall Symposium Commonsense Knowledge.
Soderland, S. (1999). Learning information extraction rules semi-structured free
text. Machine Learning, 34.
Strube, M., & Ponzetto, S. P. (2006). Wikirelate! computing semantic relatedness using
wikipedia. Proceedings Association Advancement Artificial Intelligence (AAAI).
Suchanek, F. M. (2006). Combining linguistic statistical analysis extract relations
web documents. Proceedings 12th ACM SIGKDD International Conference Knowledge Discovery Data Mining (KDD), pp. 712717.
Suchanek, F. M., Kasneci, G., & Weikum, G. (2007). Yago: core semantic knowledge.
Proceedings International Conference World Wide Web (WWW).
Taskar, B., Guestrin, C., & Koller, D. (2003). Max-margin markov networks. Proceedings
Annual Conference Neural Information Processing Systems (NIPS).
Tatu, M., & Moldovan, D. (2005). semantic approach recognizing textual entailment.
Proceedings Human Language Technology Conference Conference Empirical
Methods Natural Language Processing (HLT EMNLP).
Tatu, M., & Srikanth, M. (2008). Experiments reasoning temporal relations
events. Proceedings International Conference Computational Linguistics
(COLING).
Tax, D. (2001). One class classification. PhD thesis, Delft University Technology.
Tax, D. M. J., & Duin, R. P. W. (1991). Support vector domain description. Pattern
Recognition Letters, 20, 11911199.
Turney, P. D. (2006). Expressing implicit semantic relations without supervision. Proceedings 44th Annual Meeting Association Computational Linguistics
(ACL).
Vapnik, V. (1995). Nature Statistical Learning Theory. Springer-Verlag, NY, USA.
Viera, A. J., & Garrett, J. M. (2005). Understanding interobserver agreement: kappa
statistic. Family Medicine, 37 (5), 360363.
683

fiRadinsky, Davidovich & Markovitch

Wang, M. (2008). re-examination dependency path kernels relation extraction.
Proceedings Third International Joint Conference Natural Language Processing (ACL IJCNLP).
Wolff, P., Song, G., & Driscoll, D. (2002). Models causation causal verbs.
Proceedings Annual Meeting Association Computational Linguistics
(ACL).
Yang, Y., Pierce, T., & Carbonell, J. (1998). study retrospective online event
detection. Proceedings ACM SIGIR Special Interest Group Information Retrieval (SIGIR).
Yeung, C., & Jatowt, A. (2011). Studying past remembered: Towards computational history large scale text mining. Proceedings ACM Conference
Information Knowledge Management (CIKM).
Yoshikawa, K., Riedel, S., Asahara, M., & Matsumoto, Y. (2009). Jointly identifying temporal relations markov logic. Proceedings Third International Joint
Conference Natural Language Processing (ACL IJCNLP).
Zanzotto, F. M., Pennacchiotti, M., & Moschitti, A. (2009). machine learning approach
textual entailment recognition. Natural Language Engineering, 15, 551582.
Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods relation extraction.
Journal Machine Learning Research, 3, 10831106.
Zhang, M., Zhang, J., Su, J., & Zhou, G. (2006). composite kernel extract relations
entities flat structured features. Proceedings 21st
International Conference Computational Linguistics 44th Annual Meeting
Association Computational Linguistics, pp. 825832.
Zhao, S., & Grishman, R. (2005). Extracting relations integrated information using
kernel methods. Proceedings 43rd Annual Meeting Association
Computational Linguistics (ACL), pp. 419426.

684

fiJournal Artificial Intelligence Research 45 (2012) 731-759

Submitted 06/12; published 12/12

Tractable Set Constraints
Manuel Bodirsky

bodirsky@lix.polytechnique.fr

Ecole Polytechnique, LIX
(UMR 7161 du CNRS)
91128 Palaiseau, France

Martin Hils

hils@math.univ-paris-diderot.fr

Institut de Mathematiques de Jussieu
(UMR 7586 du CNRS)
Universite Paris Diderot Paris 7
UFR de Mathematiques
75205 Paris Cedex 13, France

Abstract
Many fundamental problems artificial intelligence, knowledge representation,
verification involve reasoning sets relations sets modeled
set constraint satisfaction problems (set CSPs). problems frequently intractable,
several important set CSPs known polynomial-time tractable.
introduce large class set CSPs solved quadratic time. class,
call EI, contains previously known tractable set CSPs, also new
ones crucial importance example description logics. class EI set
constraints elegant universal-algebraic characterization, use show
every set constraint language properly contains EI set constraints already
finite sublanguage NP-hard constraint satisfaction problem.

1. Introduction
Constraint satisfaction problems computational problems where, informally, input
consists finite set variables finite set constraints imposed variables;
task decide whether assignment values variables
constraints simultaneously satisfied. Set constraint satisfaction problems special
constraint satisfaction problems values sets, constraints might,
instance, force one set includes another set x, one set x disjoint another
set y. constraints might also ternary (or, generally, finite arity),
constraint intersection two sets x contained z, symbols
px X yq z.
systematically study computational complexity constraint satisfaction problems, turned fruitful approach consider constraint satisfaction problems
CSPpq set allowed constraints formed fixed finite set relations
R Dk (possibly infinite) common domain D. example, equals Q,
rational numbers, tu usual order rationals, CSPpq
problem deciding whether given set (binary) constraints form x
common solution rational numbers. way parametrizing conc
2012
AI Access Foundation. rights reserved.

fiBodirsky & Hils

straint satisfaction problem constraint language led many strong algorithmic
results (e.g., Bulatov & Dalmau, 2006; Idziak, Markovic, McKenzie, Valeriote, & Willard,
2010; Barto & Kozik, 2009; Bodirsky & Kutz, 2007; Bodirsky & Kara, 2009), many
powerful hardness conditions large classes constraint satisfaction problems (Schaefer,
1978; Bulatov, Krokhin, & Jeavons, 2005; Bulatov, 2003, 2006; Bodirsky & Kara, 2009).
set constraint language set relations R pP pNqqk common domain
P pNq set subsets natural numbers; moreover, require
relation R defined Boolean combination equations signature [, \, c,
0, 1, function symbols intersection, union, complementation, empty
full set, respectively. Details formal definition set constraint languages
found Section 3. Section 4, give many examples set constraint languages.
choice N notational convenience; could selected infinite set
purposes, e.g., Rn instead N, natural choice spatial reasoning. One may even
replace P pNq infinite Boolean algebra (see Theorem 28).
following, set constraint satisfaction problem (set CSP) problem form
CSPpq finite set constraint language . shown Marriott Odersky
set CSPs contained NP.
Drakengren Jonsson (1998) initiated search set CSPs solved
polynomial time. showed CSPpt, ||, uq solved polynomial time,

x holds iff x subset equal y;
x || holds iff x disjoint sets;
x holds iff x distinct sets.
also showed CSPpq solved polynomial time relations
defined formulas form
x1

y1 _ _ xk yk _ x0 y0

x1

y1 _ _ xk yk _ x0 || y0

form

x0 , . . . , xk , y0 , . . . , yk necessarily distinct variables (Drakengren & Jonsson,
1998, Thm. 20). call set relations defined way Drakengren Jonssons set constraint language. easy see algorithm
present runs time quadratic size input. hand, also show
contains relation defined x1 y1 _ x2 y2 relation defined
x1 x0 _ x2 x0 problem CSPpq NP-hard (Drakengren & Jonsson, 1998,
Thm. 22).
1.1 Contributions Outline.
present significant extension Drakengren Jonssons (1998) set constraint language (Section 4) whose CSP still solved quadratic time input size (Section 5); call set constraint language EI. Unlike Drakengren Jonssons set
732

fiTractable Set Constraints

constraint language, language also contains ternary relation defined px X q z,
relation particular interest description logics discuss
below. Moreover, show extension EI contains finite sublanguage
NP-hard set CSP (Section 6), using concepts model theory universal
algebra. sense, present maximal tractable class set constraint satisfaction
problems.
algorithm based concept independence constraint languages
discovered several times independently 90s (Lassez & McAloon, 1989; Jonsson &
Backstrom, 1998; Marriott & Odersky, 1996, see also Koubarakis, 2001; Broxvall, Jonsson,
& Renz, 2002; Cohen, Jeavons, Jonsson, & Koubarakis, 2000); however, apply
concept twice novel, nested way, leads two level resolution procedure
implemented run quadratic time. technique use prove correctness
algorithm also important contribution paper, believe similar
approach applied many contexts; technique inspired already
mentioned connection universal algebra.
1.2 Application Areas Related Literature
mention three different contexts set constraints appeared literature.
1.2.1 Set Constraints Programming Languages.
Set constraints find applications program analysis; here, set constraint form
X , X set expressions. Examples set expressions 0 (denoting
empty set), set-valued variables, union intersection sets, also expressions
form f pZ1 , Z2 q f function symbol Z1 , Z2 set expressions.
Unfortunately, worst-case complexity reasoning tasks considered
setting high, often EXPTIME-hard (for survey this, see Aiken, 1994).
recently, shown quantifier-free combination set constraints (without
function symbols) cardinality constraints (quantifier-free Pressburger arithmetic)
satisfiability problem NP (Kuncak & Rinard, 2007). logic (called QFBAPA)
interesting program verification (Kuncak, Nguyen, & Rinard, 2006).
1.2.2 Tractable Description Logics.
Description logics family knowledge representation formalisms used
formalize reason concept definitions. computational complexity
computational tasks studied various formalisms usually quite high.
However, last years series description logics, example EL, EL , Horn-FL0 ,
various extensions fragments (Kusters & Molitor, 2002; Baader, 2003; Baader,
Brandt, & Lutz, 2005; Krotzsch, Rudolph, & Hitzler, 2006), discovered
crucial tasks e.g. entailment, concept satisfiability knowledge base satisfiability
decided polynomial time.
Two basic assertions made EL
Horn-FL0 C1 ||C2 (there
C1 also C2 ) C1 X C2 C3 (every C1 C2 also C3 ), concept
names C1 , C2 , C3 . EI set constraints, latter treated
733

fiBodirsky & Hils

framework Drakengren Jonsson. None description logics tractable
knowledge base satisfiability problem contains EI set constraints.
1.2.3 Spatial Reasoning.
Several spatial reasoning formalisms (like RCC-5 RCC-8) closely related set constraint satisfaction problems. formalisms allow reason relations
regions; fundamental formalism RCC-5 (see, e.g., Jonsson & Drakengren, 1997),
one think region non-empty set, possible (binary) relationships containment, disjointness, equality, overlap, disjunctive combinations thereof. Thus,
exclusion empty set prominent difference set constraint languages studied Drakengren Jonsson (1998) contained class set constraint
languages considered RCC-5 fragments. see Section 3
CSP RCC-5 CSPs reducts set CSPs (Proposition 2).

2. Constraint Satisfaction Problems
use existing terminology logic model theory, convenient describe
constraint languages structures (see, e.g., Hodges, 1993). structure tuple
pD; f1, f2, . . . , R1, R2, . . . q set (the domain ), fi function
Dki (where ki called arity fi ), Ri relation D, i.e., subset
Dli (where li called arity Ri ). function fi assume
function symbol denote fi , relation Ri relation symbol
denote Ri . Constant symbols treated 0-ary function symbols.
set relation function symbols structure called signature ,
also say -structure. signature contains relation symbols
function symbols, also say relational structure. context
constraint satisfaction, relational structures also called constraint languages,
constraint language 1 called sublanguage (or reduct) constraint language
relations 1 subset relations (and called expansion 1 ).
Let relational structure domain finite signature . constraint
satisfaction problem following computational problem, also denoted CSPpq:
given finite set variables V conjunction finitely many atomic formulas
form Rpx1 , . . . , xk q, x1 , . . . , xk P V R P , satisfiable ; is,
exist assignment : V every constraint Rpx1 , . . . , xk q input
pspx1 q, . . . , spxk qq P R ?
mapping also called solution instance CSPpq, conjuncts
called constraints. Note introduce constraint satisfaction problems
CSPpq finite constraint languages, i.e., relational structures finite relational
signature.

Example 1. problem CSPppQ; qq problem deciding whether given set
constraints form x solution simultaneously satisfies constraints.
734

fiTractable Set Constraints

3. Set Constraint Languages
section, give formal definitions set constraint languages. Let structure domain P pNq, set subsets natural numbers, signature
t[, \, c, 0, 1u,



[ binary function symbol denotes intersection, i.e., [S X;
\ binary function symbol union, i.e., \S Y;

c unary function symbol complementation, i.e., cS function maps
N NzS;
0 1 constants (treated 0-ary function symbols) denoting empty set
full set N, respectively.

H

Sometimes, simply write [ function [S \ function \S , i.e.,
distinguish function symbol respective function. use symbols
[, \ symbols X, prevent confusion meta-mathematical usages X
text.
set constraint language relational structure whose relations quantifier-free
definition S. always allow equality first-order formulas, equality symbol
always interpreted true equality relation domain structure.
write x abbreviation x [ x.
Example 2. ternary relation px, y, z q
definition z [ px [ q x [ S.

P P pN q3 | x [ z

(

quantifier-free

Theorem 1 (See Marriott & Odersky, 1996, Proposition 5.8). Let set constraint
language finite signature. CSPpq NP.
easy see NP-hard set CSPs, shown next example.
Example 3. Consider set constraint language contains eight relations

tpx, y, zq | x 1 _ 1 _ z 1u
tpx, y, zq | x 1 _ 1 _ z 1u
tpx, y, zq | x 1 _ 1 _ z 1u
tpx, y, zq | x 1 _ 1 _ z 1u
tpx, y, zq | x 1 _ 1 _ z 1u
tpx, y, zq | x 1 _ 1 _ z 1u
tpx, y, zq | x 1 _ 1 _ z 1u
tpx, y, zq | x 1 _ 1 _ z 1u .
set CSP relations well-known 3-SAT problem, NPcomplete (Garey & Johnson, 1978).
well-known structure pP pNq; \, [, c, 0, 1q Boolean algebra,
735

fiBodirsky & Hils

0 playing role false, 1 playing role true;
c playing role


;

[ \ playing role ^ _, respectively.

refer work Koppelberg (1989) background Boolean algebras.
confuse logical connectives connectives Boolean algebras, always
use symbols [, \, c instead usual function symbols ^, _, Boolean
algebras. facilitate notation, also write x instead cpxq, x instead
p x q.
assume terms functional signature
t[, \, c, 0, 1u written


(inner) conjunctive normal form (CNF), i.e., ni1 nj 1 lij lij either
form x form x variable x. Note every term t[, \, c, 0, 1u rewritten equivalent term form, using usual laws Boolean algebras (Boole,
1847). allow special
case n 0 (in case becomes 1), special case
ni
ni 0 (in case j 1 lij becomes 0). refer ci : tlij | 1 j ni u
(inner) clause t, lij (inner) literal ci . say set inner clauses
satisfiable exists assignment V P pNq inner clauses,

union evaluation literals equals N (this case formula
n
i1 ci 1 satisfying assignment).
Example 4. Inequality x P pNq equivalently written py \ xq[px \ q 1;
formula, two inner clauses, positive negative inner literal.
assume quantifier-free formulas signature
t[, \, c, 0, 1u written

mi
(outer) conjunctive normal form (CNF), i.e.,
j 1 Lij Lij either
i1
form 1 (a positive (outer) literal ) form 1 (a negative (outer) literal ).
Again, well-known easy see every quantifier-free formula find
formula form equivalent every Boolean algebra. refer
Ci : tLij | 1 j mi u (outer) clause , Lij (outer) literal Ci .
Whenever convenient, identify set clauses.
Example 5. Consider formula px ^ z q _ px ^ z q. rewritten
px _ z q^px _ z q. subsequently replace inequality literals x
py \ xq [ px \ q 1 (see Example 4), arrive formula discussed
normal form: two outer clauses, one two positive outer literals,
two negative outer literals.
mentioned introduction, CSPs reducts RCC-5 (and therefore also
many reducts RCC-8) formulated set CSPs. network satisfaction problem RCC-5 seen CSP following structure RCC-5 domain
P pNqztHu binary relations given follows: relation R RCC-5
exists quantifier-free t\, [, c, 0, 1u-formula px1 , x2 q ps1 , s2 q P R
s1 s2 non-empty, ps1 , s2 q true S. clear
finitely many inequivalent quantifier-free formulas language t\, [, c, 0, 1u,
hence RCC-5 finitely many relations.
736

fiTractable Set Constraints

Proposition 2. Let reduct RCC-5. exists set constraint language
CSPpq CSPpq problem.
Proof. Let 1 , . . . , n formulas define relations (which domain
P pNqztHu). Let structure domain P pNq relations R1 , . . . , Rn defined
1 , . . . , n S. is, difference additional element
tHu, appears none relations . prove every finite
conjunction atomic formulas form Ri px1 , . . . , xk q satisfiable
conjunction satisfiable . satisfiable clearly also satisfiable
since induced substructure . Conversely, satisfiable ,
solution must property spxi q H, xi appear
constraint (since constraints force arguments non-empty). Hence, setting
spxi q non-empty subset N still solution, implies claim.
Proposition 2 shows class set CSPs contains class CSPs reducts
RCC-5. inclusion clearly strict: set CSPs cannot formulated
CSPs reducts RCC-5, since relations set constraint languages arbitrary
arity, reducts RCC-5 contain binary relations.

4. Horn-Horn Set Constraints
section, study Horn-Horn set constraints, class set constraints admits
intuitive syntactic description thus easy define. Universal algebraic
considerations class Section 4.2 lead us another class set constraints, called
EI introduced Section 4.3. class strictly contains class Horn-Horn set
constraints. universal algebraic description key maximality result
prove Section 6. tractability, set constraint languages EI allow
(linear-time) reduction satisfiability Horn-Horn clauses (see Proposition 22). note
classes set constraints (Horn-Horn EI) studied before.
4.1 Horn-Horn Relations
Definition 3. quantifier-free formula called Horn-Horn
1. every outer clause outer Horn, i.e., contains one positive outer literal,
2. every inner clause positive outer literals inner Horn, i.e., contains one
positive inner literal.
relation R P pNqk called
outer Horn defined conjunction outer Horn clauses;
inner Horn defined formula form pc1 [ [ ck q
ci inner Horn;

1

Horn-Horn defined Horn-Horn formula S.
Example 6. Inequality

Horn-Horn: recall may defined py \ xq[px \ yq 1.
737

fiBodirsky & Hils

Example 7. Using previous example, relation
easily seen Horn-Horn, too.
Example 8. ternary relation tpx, y, z q | x [
Horn-Horn definition x \ \ z 1.

tpx, y, u, vq | x _ u vu

zu, encountered above,

Proposition 4. Drakengren Jonssons set constraint language contains Horn-Horn
relations.
Proof. disjointness relation || definition x \ 1, inner Horn.
inequality relation Horn-Horn since inner clauses definition py \ xq[px \ q
1 one positive inner literal. inclusion relation x definition
\ x 1, inner Horn.
Horn-Horn preserved adding additional outer disequality literals outer
clauses, relations considered Drakengren Jonssons language Horn-Horn.
4.2 Universal Algebraic Preliminaries
see section, class Horn-Horn formulas preserved several
important functions defined set subsets natural numbers.
Definition 5.
Let : pP pNqq2 P pNq function maps pair sets pS1 , S2 q
set t2a | P S1 u t2a 1 | P S2 u;

denote Fin set finite non-empty subsets N, let F : P pNq P pFin q,
F pS q : tS0

| S0 finite non-emptyu ;

let G : N Fin bijection (since sets countable, bijection exists);
let e : P pNq P pNq defined
epS q tG1 pT q |

P F pS qu ;

let ei function defined eipx, q epipx, qq.
Intuitively, functions e ei designed forget unions (this
formalized Definition 34), preserving basic operations (see Lemma
11 Proposition 8 case e).
Definition 6. Let f : pP pNqqk P pNq function, R P pNql relation.
say f preserves R following holds: a1 , . . . , ak P pP pNqql
pf pa11, . . . , ak1 q, . . . , f pa1l , . . . , akl qq P R ai P R k. f preserve R,
also say f violates R. say f strongly preserves R a1 , . . . , ak P pP pNqql
pf pa11 , . . . , ak1 q, . . . , f pa1l , . . . , akl qq P R ai P R k.
first-order formula defines relation R S, f preserves (strongly preserves)
R, also say f preserves (strongly preserves) . Finally, g : pP pNqql P pNq
function, say f preserves (strongly preserves) g preserves (strongly
preserves)
(
graph g, i.e., relation px1 , . . . , xl , g px1 , . . . , xl qq | x1 , . . . , xl N .
738

fiTractable Set Constraints

Note injective function f preserves function g, also strongly preserves
g.
Example 9. Consider function f : pP pNqq2 P pNq, px, q x \ y. f preserves
, since x \ x1 \ y1 whenever x x1 y1 y. hand, f strongly
preserve , shown f pN, Hq f pH, Nq.
Fact 7. mapping isomorphism S2 S.
Proof. mapping
inverted mapping sends N ta | 2a P
u, ta | 2a 1 P u . straightforward verify strongly preserves 0, 1, c, \, [.
Clearly, ipx, q H x

H.

Similarly, since natural numbers partitioned even odd numbers,
ipx, q N x N.
Let S1 S2 subsets N. verify preserves c show
ipcpS1 , S2 qq, definition equal ipS1 , S2 q, equals cpipS1 , S2 qq. Suppose
2a1 . Then:
P pS 1 , 2 q 1

P S1

a1 R S1
2a1 R ipS1, S2q
P ipS1, S2q

argument 2a1
even strongly preserves c.

1 analogous. Thus, preserves c. Since injective,

Let pS1 , S2 q pT1 , T2 q pP pNq2 . show ippS1 , S2 q\pT1 , T2 qq,
definition equal ipS1 \ T1 , S2 \ T2 q, equals ipS1 , S2 q \ ipT1 , T2 q.
2a1 before:
P ipS1 \ T1 , S2 \ T2 q a1

P pS1 \ T1q

2a1 P ipS1, S2q \ ipT1, T2q

argument 2a1 1 analogous. Thus, preserves
injective, even strongly preserves \.
verification

\.

Since

[ similar \.

Proposition 8. function e following properties.
e injective,

[,
x, y, z P P pNq x \
epxq \ epy q epz q.

e strongly preserves 1, 0,


z, x

739



y,



x,

fiBodirsky & Hils

Proof. verify properties one one. Since G bijective, epxq epy q
x finite subsets. case x y, hence e
injective. Thus, prove e strongly preserves 1, 0, [, suffices check e
preserves 1, 0, [.
Since G bijective, GpNq equals set finite subsets N,
hence epNq N, shows e preserves 1. also compute epHq G1 pF pHqq
G1 pHq H.
Next, verify x, P P pNq epxq [ epy q epx [ q. Let P N
arbitrary. P epxq [ epy q Gpaq P F pxq X F py q. definition F
since Gpaq finite subset N, case Gpaq P F px [ q.
case P epx [ q, concludes proof e preserves [.
verify x \ z, x y, x, epxq \ epy q epz q. First
observe u, v N u v epuq epv q since e preserves [.
implies epxq \ epy q epz q. Since x x, a, b P x,
R y, b P y, b R x. ta, bu P F pz q, ta, bu R F pxq F py q. Hence,
G1 pta, buq P epz q, G1 pta, buq R epxq \ epy q. shows epz q epxq \ epy q.
Note particular e preserves , , ||. Moreover, epcpxqq cpepxqq:
follows preservation ||, since x||cpxq, therefore epxq||epcpxqq, equivalent
inclusion above. e strongly preserve [, 0, 1, therefore also ei
strongly preserves [, 0, 1.
following direct consequence fact isomorphisms k
preserve Horn formulas ; since simple proof instructive follows,
give special case relevant here.
Proposition 9. Outer Horn relations preserved i.
Proof. Let conjunction outer Horn clauses variables V . Let tt0 1, t1
1, . . . , tk 1u outer clause . Let u, v : V P pNq two assignments satisfy
clause. Let w : V P pNq given x ipupxq, v pxqq. Suppose w satisfies
tj 1 1 j k. Since injective must tj 1 u v
1 j k, therefore neither assignment satisfies negative literals. Hence, u v
must satisfy t0 1. Since isomorphism S2 S, preserves particular
t0 1, hence w also satisfies t0 1.
Proposition 10. Inner Horn relations strongly preserved e.






Proof. Observe x \ p j j q 1 equivalent x [ p j yj q j yj , strongly
preserved e since e strongly preserves [. clearly implies statement.
Note Proposition 9 Proposition 10 imply ei strongly preserves inner Horn
relations. later also need following.

N, k 1. following equivalent.
epx1 q \ \ epxk q \ epy1 q \ \ epyl q 1.

exists k xi \ p j j q 1.

Lemma 11. Let x1 , . . . , xk , y1 , . . . , yl
1.
2.

740

fiTractable Set Constraints

3. exists k epxi q \ p


j

epyj qq 1.

0, j yjl 1 jl epyj q 1.
Proof. implication
p1q p2q, suppose every k ai P N
ai R Xi : xi \ p j j q. Let c G1 ta1 , a2 , . . . , ak u . k,


c R epxi q \ j l epyj q. see this, first observe ai P j l yj [ xi . Therefore,
ta1, . . . , ak u P jl F pyj q [ F pxiq k. conclude c R epx1q \ \ epxk q \
epy1 q \ \ epyl q.
implication p2q p3q follows directly Proposition 10. implication p3q
p1q trivial. second statement direct consequence Proposition 10.
k

Proposition 12. Every Horn-Horn relation preserved e i, particular
ei.
Proof. Suppose R Horn-Horn definition variables V . Since R
particular outer Horn, preserved Proposition 9.
verify R preserved e. Let u : V P pNq assignment
satisfies . is, u satisfies least one literal outer clause . suffices
show assignment v : V P pNq defined x epupxqq satisfies outer
literal. Suppose first outer literal positive; Horn-Horn,
form x \ y1 \ \ yl 1 form y1 \ \ yl 1, preserved e
Lemma 11.
Now, suppose outer literal negative, is, form x1 \ \ xk \ y1 \
\ yl 1 k 0. treat case k 1, case similar.
Suppose contradiction v px1 q \
\ vpxk q \ vpy1q \ \ vpyl q 1. Lemma 11,
exists k upxi q \ p j upy j qq 1. particular
upx1 q \ \ upxk q \ upy1 q \ \ upyl q 1, contradiction assumption u
satisfies .
4.3 EI Set Constraints
section introduce class EI set constraints, show strictly contains
Horn-Horn relations, give several examples non-examples. present
algorithmic reduction CSPs EI set constraints satisfiability finite sets
Horn-Horn clauses.
Definition 13. set relations quantifier-free definition
preserved operation ei denoted EI.
Remark. Note definition operation ei (Definition 5) involved bijection G
N Fin ; see later (Proposition 36 Proposition 37) class
EI independent precise choice G.
Recall Proposition 12 EI contains Horn-Horn relations. present
examples relations EI, examples relations EI
Horn-Horn.
741

fiBodirsky & Hils

Example 10. give example relation clearly EI. relation
R tpx, q | x \ 1u violated ei: consider S1 t2a | P Nu S2 t2a
1 | P Nu. pS1 , S2 q P R, since isomorphism S2
also pipS1 , S1 q, ipS2 , S2 qq P R. Since neither ipS1 , S1 q ipS2 , S2 q ipS2 , S2 q
ipS1 , S1 q, get epipS1 , S1 qq \ epipS2 , S2 qq ep1q 1 Proposition 8. Therefore,
peipS1, S1q, eipS2, S2qq R R wanted show.
Example 11. relation R tpx, y, z q | px q _ py z qu also preserved
ei: note p0, 1, 1q, p0, 0, 1q P R, eip0, 0q, eip1, 0q, eip1, 1q pairwise distinct
since ei injective.
Example 12. formula

p x [ xq
^ px [ yq
^ pv 1 _ u 1 _ x \ 1q
clearly Horn-Horn. However, relation defined formula EI:
px1, y1, u1, u2q und px2, y2, u2, v2q relation, neither ipx1, x2q ipy1, y2q
ipy1 , y2 q ipx1 , x2 q. Proposition 8, peipx1 , x2 q, eipy1 , y1 q, eipu1 , u2 q, eipv1 , v2 qq satisfies
formula. equivalent Horn-Horn formula, since formula preserved
i.
Example 13. formula ppx \ 1q _ pu \ v 1qq ^ px \ 1q ^ px \ 1q
Horn-Horn. However, preserved e i: reason one clauses
negative literal x \ 1, conjuncts tx \ 1u tx \ 1u. Therefore,
every tuple P R tuple eptq satisfies x \ 1 R well. Proposition 9,
R preserved i. case, authors suspect equivalent Horn-Horn
formula. generally, open whether exist formulas preserved e
i, equivalent Horn-Horn formula.
Corollary 14. class Horn-Horn relations proper subclass EI.
Proof. Proposition 12 shows EI contains Horn-Horn relations. Example 12 shows
inclusion strict.
prepare results viewed partial converse Proposition 12.
Definition 15. quantifier-free formula (in syntactic form described end
Section 3) called reduced every formula obtained removing outer literal
equivalent S.
note slightly different notion reduced formula introduced
Bodirsky, Chen, Pinsker (2010). variant using better suited
purposes.
Lemma 16. structure S, every quantifier-free formula equivalent reduced
formula.
742

fiTractable Set Constraints

Proof. clear every quantifier-free formula written formula CNF
form discussed Theorem 1. remove successively outer
literals long results equivalent formula.
first prove partial converse Proposition 9.
Proposition 17. Let reduced formula preserved i. outer clause
Horn.
Proof. Let V set variables . Assume contradiction contains outer
clause two positive literals, t1 1 t2 1. remove literal t1 1
clause C, resulting formula inequivalent , hence assignment
s1 : V P pNq satisfies none literals C except t1 1. Similarly,
assignment s2 : V P pNq satisfies none literals C except t2 1.
injectivity i, since strongly preserves c, [, \, 1, assignment : V P pNq
defined x ips1 pxq, s2 pxqq satisfy two literals t1 1 t2 1. Since
strongly preserves c, \, [, none literals C satisfied mappings
well, contradiction assumption preserved i.
Definition 18. Let V set variables, : V P pNq mapping.
function V P pNq form x epspxqq called core assignment.
Lemma 19. every quantifier-free formula exists formula inner
clauses inner Horn, satisfying core assignments.
preserved ei, set satisfying core assignments closed
ei.
Proof. Suppose outer clause C positive outer literal 1
contains inner clause c : x1 \ \ xk \ 1 \ \ l Horn, i.e., k 2.
replace outer literal 1 k literals t1 1, . . . , tk 1 ti obtained
replacing c xi \ 1 \ \ l .
claim resulting formula 1 set satisfying core assignments.
Observe xi \ 1 \ \ l c, hence ti 1 implies 1. arbitrary satisfying
assignment 1 satisfies either one positive outer literals ti 1, case
observation shows also satisfies , satisfies one outer literals C,
case also satisfies literal . Hence, 1 implies . Conversely, let
satisfying core assignment . satisfies literal C 1, also
satisfies literal 1 , satisfies 1 . Otherwise, must satisfy 1, hence
spx1 q\ \ spxk q\ spy1 q\ \ spyl q 1. Since core assignment, Lemma 11 implies
exists k spxi q \ spy1 q \ \ spyl q 1. satisfies 1 .
Suppose outer clause C negative outer literal 1
contains inner clause c : x1 \ \ xk \ 1 \ \ l Horn, i.e., k 2.
replace clause C k clauses C1 , . . . , Ck Ck obtained C
replacing c xi \ 1 \ \ l .
claim resulting formula 1 set satisfying core assignments.
Observe x1 \ \ xk \ 1 \ \ l 1 implies xi \ 1 \ \ l 1, every
k. observation shows arbitrary assignment also assignment 1 .
743

fiBodirsky & Hils

Conversely, let satisfying core assignment 1 . satisfies one literals
C 1, satisfies . Otherwise, must satisfy xi \ 1 \ \ l 1
k, Lemma 11 also satisfies x1 \ \ xk \ 1 \ \ l 1.
perform replacements obtain formula 1 inner clauses
Horn; formula satisfies requirements first statement lemma.
prove second statement, let u, v : V P pNq two satisfying core assignments
1 . Since 1 satisfying core assignments, u v also satisfy .
mapping w : V P pNq given x eipupxq, v pxqq core assignment,
ei preserves , mapping w satisfies . Since 1 core assignments,
w also satisfying assignment 1 , proves statement.
single technical condition guarantees, extra condition
(see Proposition 21) formulas satisfying certain universl algebraic property HornHorn. allow us perform reduction CSP associated (finite) set
constraint languages EI satisfiability Horn-Horn clauses.
Definition 20. quantifier-free formula (in syntactic form described end
Section 3) called strongly reduced every formula obtained removing outer
literal set satisfying core assignments S.
Proposition 21. Let strongly reduced formula whose inner clauses Horn.
set satisfying core assignments closed ei, Horn-Horn.
Proof. Let V set variables . suffices show clauses outer
Horn. Assume contradiction contains outer clause two positive literals,
t1 1 t2 1. remove literal t1 1 clause C, resulting formula
strictly less satisfying core assignments; shows existence core assignment
s1 : V P pNq satisfies none literals C except t1 1. Similarly, exists
core assignment s2 : V P pNq satisfies none literals C except t2 1.
assumption, inner clauses t1 t2 Horn. claim assignment
: V P pNq defined x eips1 pxq, s2 pxqq satisfy clause C. Since ei
strongly preserves inner Horn clauses, satisfy t1 1 _ t2 1.
reasons satisfy literals C; contradicts assumption
satisfying core assignments preserved ei.
Satisfiability Horn-Horn clauses computational problem decide whether,
given finite set Horn-Horn clauses, satisfying assignment S.
Proposition 22. Let finite set constraint language EI. CSPpq
reduced linear time satisfiability Horn-Horn clauses.
Proof. Let instance CSPpq, let V set variables appear
. constraint Rpx1 , . . . , xk q , let R definition R S.
Lemma 19, exists formula R satisfying core assignments R
inner clauses Horn; moreover, since R preserved ei, lemma
asserts set satisfying core assignments R preserved ei.
assume without loss generality R strongly reduced; seen similarly
Lemma 16. Proposition 21, formula R Horn-Horn.
744

fiTractable Set Constraints

Let set Horn-Horn clauses formulas R px1 , . . . , xk q obtained constraints Rpx1 , . . . , xk q described manner. claim satisfiable instance
CSPpq satisfiable. follows fact constraint
Rpx1 , . . . , xk q , formulas R R satisfying core assignments,
R R preserved ei (for R follows Proposition 12),
particular function x eipx, xq.
Note Proposition 22 reduce satisfiability EI satisfiability proper
subclass Horn-Horn set constraints: general Horn-Horn set constraints allow
inner clauses negative outer literals Horn, reduction produces HornHorn clauses inner clauses Horn.

5. Algorithm Horn-Horn Set Constraints
present algorithm takes input set Horn-Horn clauses decides
satisfiability pP pNq; \, [, c, 0, 1q time quadratic length input.
Proposition 22, section therefore conclude proof CSPpq tractable
relations EI.
mentioned introduction, algorithm based two procedures,
resolution-like. inner procedure essentially well-known positive unit resolution
procedure Horn-SAT, outer procedure basically algorithm
used literature independence constraint satisfaction (see, e.g., Jonsson &
Backstrom, 1998; Koubarakis, 2001; Broxvall et al., 2002; Cohen et al., 2000). contribution section way nest two algorithms obtain polynomial-time
decision procedure satisfiability Horn-Horn clauses.
start discussing first procedure algorithm, call inner
resolution algorithm. case Boolean positive unit resolution (Dowling & Gallier,
1984) one implement procedure Inner-Res runs linear time
input size.
Lemma 23. Let finite set inner Horn clauses. following equivalent.
1.



1 satisfiable S.

2. Inner-Respq Figure 1 accepts.
3.



1 solution whose image contained


tH, Nu.

Proof. obvious 1 unsatisfiable Inner-Respq rejects; fact,

inner clauses c derived Inner-Res , formula c 1 logically implied

1. Conversely, algorithm accepts set eliminated variables
N remaining variables H, satisfies clauses: removed clauses
positive literal satisfied, remaining clauses least one negative literal
final stage algorithm, clauses negative literals final stage
algorithm satisfied.




proof previous lemma shows 1 satisfiable
1 satisfiable two-element Boolean algebra. see following,
745

fiBodirsky & Hils

Inner-Res()
// Input: finite set inner Horn clauses
// Accepts iff 1 satisfiable
entire algorithm:
contains empty clause, reject.
Repeat := true
Repeat = true
Repeat := false
contains positive unit clause txu
Repeat := true
Remove clauses literal x occurs.
Remove literal x clauses.
End
Loop
Accept



Figure 1: Inner Resolution Algorithm.
holds generally (and inner Horn clauses). following
well-known, shown proof given Koppelberg (1989)
weaker Proposition 2.19 there. give proof convenience reader.
Fact 24. Let t1 , t2 terms

t[, \, c, 0, 1u. following equivalent:

1 ^ t2 1 satisfiable two-element Boolean algebra;
t1 1 ^ t2 1 satisfiable Boolean algebras;
t1 1 ^ t2 1 satisfiable Boolean algebra;
t1 1 ^ t2 1 satisfiable finite Boolean algebra.

1. t1
2.
3.
4.

Proof. Obviously, (1) implies (2), (2) implies (3).
(3) implies (4), assume t1 1 ^ t2 1 satisfying assignment
Boolean algebra C. Let x1 , . . . , xn variables occur t1 t2 , let xi ci
satisfying assignment. t1 1 ^ t2 1 satisfiable Boolean sub-algebra
n
C1 C generated tc1 , . . . , cn u, C1 finite (it 2p2 q elements).
(4) implies (1), first note finite Boolean algebra isomorphic Boolean
algebra pP pX q; [, \, c, 0, 1q subsets finite set X. x P X, consider map hx :
P pX q t0, 1u, hpY q : 1 x P , hpY q 0 otherwise. hx homomorphism
Boolean algebras. particular, shows every non-zero element finite
Boolean algebra C, homomorphism h C two-element Boolean algebra
hpaq 0. suppose (4), assume t1 1 ^ t2 1 satisfying
assignment finite Boolean algebra C. Let c element denoted t2 C
assignment, c 1. let h homomorphism C t0, 1u
hpcq 0, i.e. hpcq 1. construction, image satisfying assignment h
satisfying assignment t1 1 ^ t2 1 t0, 1u.
746

fiTractable Set Constraints

statement t1 1 instead t1 1 ^ t2 1 given Proposition
2.19 (Koppelberg, 1989). Fact 24 following consequence crucial way
use inner resolution procedure algorithm.
Lemma 25. Let finite set inner Horn clauses. following equivalent:
1. Inner-RespYtx1 , . . . , xk , y0 , . . . , yl uq rejects.


1 S.


Proof.
1 implies x1 \ \ xk \ 1 \ \ l 1

1 ^ x1 \
\ xk \ y1 \ \ yl 1 unsatisfiable S. Fact 24, case
1 ^ x1 \ \ xk \ 1 \ \
1 unsatisfiable 2-element
l
Boolean algebra, case 1 ^ x1 \ \ xk \ 1 \ \ l 0
2.

1 implies x1 \ \ xk \ 1 \ \ l

unsatisfiable two-element Boolean algebra. seen Lemma 23,
turn holds Inner-RespYtx1 1, . . . , xk 1, y1 1, . . . , yl 1uq rejects.
Outer-Res()
// Input: finite set Horn-Horn clauses
// Accepts iff satisfiable pP pNq; [, \, c, 0, 1q
entire algorithm:
contains empty clause, reject.
Repeat := true
Repeat = true
Repeat := false
Let set inner Horn clauses terms
positive unit clauses tt 1u .
Inner-Res rejects , reject.
negative literal 1 clauses
inner clause tx1 , . . . , xk , 1 , . . . , l u
Call Inner-Res
tx1 1, . . . , xk 1, y0 1, . . . , yl 1u
Inner-Res rejects remove clause
End
clauses removed,
Remove outer literal 1 clause
Repeat := true
End
Loop
Accept

Figure 2: Outer Resolution Algorithm.
Theorem 26. algorithm Outer-Res Figure 2 decides satisfiability sets HornHorn clauses quadratic time.
Proof. first argue algorithm rejects , indeed solution. First
note whole argument, set clauses satisfying tuples
747

fiBodirsky & Hils

(i.e., corresponding formulas equivalent): Observe negative literals get
removed clauses, negative literal 1 gets removed clause
Inner-Res rejects tx1 1, . . . , xk 1, y0 1, . . . , yl 1u inner clause
tx1, . . . , xk , y1, . . . , yl u t. Lemma 25, Inner-Res rejects Ytx1 1, . . . , xk 1, y0
1, . . . , yl 1u implies x1 \ \ xk \ 1 \ \ l 1. Hence, positive
unit clauses imply 1 therefore literal 1 removed clause
without changing set satisfying tuples. algorithm rejects either Inner-Res
rejects derives empty clause. cases clear satisfiable.
Thus, suffices construct solution algorithm accepts. Let set
inner clauses terms positive unit clauses final stage, algorithm
accepts. remaining negative outer literal tt 1u remaining inner clause
tx1 , . . . , xk , 1 , . . . , l u exists assignment V P pNq satisfies
Ytx1 \ \ xk \ 1 \ \ l 1u: otherwise, Lemma 25, inner resolution algorithm
would rejected tx1 1, . . . , xk 1, y0 1, . . . , yl 1u, would removed
inner clause t. Let D1 , . . . , Ds enumeration remaining inner clauses
appear remaining negative outer literals.
Write s-ary operation defined px1 , . . . , xs q ipx1 , ipx2 , . . . , ipxs1 , xs q qq
(where Fact 7). claim : V P pNq given
x pD1 pxq, . . . , Ds pxqq

satisfies clauses . Let C clause . assumption, final stage
algorithm, clause C still non-empty. Also note since formulas input
Horn-Horn, contain one positive literal. holds particular C,
therefore distinguish following cases:
final state algorithm, C still contains negative literal 1. Since 1
removed, remaining inner clause tx1 , . . . , xk , 1 , . . . , l u
t. Observe spx1 q \ \ spxk q \ spy1 q \ \ spyl q 1 Dj px1 q \
\ Dj pxk q \ Dj py1q \ \ Dj pyl q 1 1 j s. Hence, since
px1 q \ \ pxk q \ py1 q \ \ pyl q 1, satisfies 1. shows
satisfies C.
negative literals removed C algorithm. positive
literal t0 1 C inner clauses t0 Horn. part
, therefore t0 1 satisfied s. Indeed, assumption assignments Dj
satisfy , preserved i.
conclude solution . inner resolution algorithm linear time
complexity; outer resolution algorithm performs linear number calls
inner resolution algorithm, straightforward implement necessary data
structures outer resolution obtain running time quadratic input
size.
Combining Proposition 22 Theorem 26, obtain following.
Theorem 27. Let finite set constraint language EI. CSPpq
solved quadratic time.
748

fiTractable Set Constraints

6. Maximal Tractability
section show class EI maximal tractable set constraint language.
specifically, let set constraint language strictly contains EI relations.
show contains finite set relations 1 already problem
CSPp1 q NP-hard (Theorem 40).
6.1 Universal-Algebraic Approach
proof use so-called universal-algebraic approach complexity constraint satisfaction problems, requires re-formulate set CSPs constraint
satisfaction problems -categorical structures. detailed introduction
universal-algebraic approach -categorical structures (see Bodirsky, 2012). structure
countable domain called -categorical countable structures satisfy
first-order sentences isomorphic (see, e.g., Hodges, 1993).
theorem Ryll-Nardzewski, countable signatures, equivalent requiring
every relation preserved automorphisms1 first-order definable
(see, e.g., Hodges, 1993). useful consequence -categorical structure , whenever two tuples c pc1 , . . . , cn q pd1 , . . . , dn q satisfy
first-order formulas, automorphism maps c d.
example -categorical structure pQ; q (by Cantors theorem), nonexample given pZ; q. Note pQ; q pZ; q CSP; indeed, two
infinite linear orders share CSP, since even finite substructures.
characterisation infinite structures -categorical structure
CSP given Bodirsky, Hils, Martin (2011). Empirically,
observed constraint satisfaction problems studied temporal spatial
reasoning typically called qualitative formulated
-categorical template.
Set constraint languages general -categorical (this follows easily
mentioned theorem Ryll-Nardzewski). However, every set CSP formulated
CSP -categorical structure. see this, first recall basic facts
Boolean algebras. countable atomless2 Boolean algebras isomorphic (Koppelberg,
1989, Corollary 5.16; see also Hodges, 1993, Example 4 page 100). Let denote
countable atomless Boolean algebra, let denote domain A. Again, use [
\ denote join meet A, respectively. Since axioms Boolean algebras
property atoms written first-order sentences, follows
-categorical. structure B quantifier elimination every first-order formula
B equivalent quantifier-free formula. well-known quantifier elimination
(see Hodges, 1993, Exercise 17 page 391). also make use following.
Theorem 28 (Marriott & Odersky, 1996, Corollary 5.7). quantifier-free formula satisfiable infinite Boolean algebra satisfiable infinite Boolean
algebras.
1. isomorphism structure called automorphism .
2. atom Boolean algebra element x 0 x X
0. Boolean algebra contains atoms, called atomless.

749

x

fiBodirsky & Hils

particular, B infinite Boolean algebra 1 , . . . , n quantifier-free
formulas signature t[, \, c, 0, 1u, relational structure signature tR1 , . . . , Rn u Ri n defined B, CSPpq
depend choice B.
fundamental concept complexity theory constraint satisfaction problems
notion primitive positive definitions. first-order formula called primitive positive
(pp) form
Dx1, . . . , xn p1 ^ ^ mq
formula form Rpy1 , . . . , yl q form y1 y2 ,
R relation symbol y1 , y2 , . . . , yl either free variables tx1 , . . . , xn u.
say k-ary relation R Dk primitive positive definable (pp definable)
-structure domain iff exists primitive positive formula px1 , . . . , xk q
k free variables x1 , . . . , xk tuple pb1 , . . . , bk q R pb1 , . . . , bk q
true .
Example 14. relation tpx, q P P pNq2 | x u pp definable pP pNq; S, q
tpx, y, z q | x [ z u. pp definition px, x, q ^ x (the definition even
quantifier-free).
Example 15. relation tpx1 , x2 , x3 , q P P pNq4 | x1 [ x2 [ x3 u pp definable
pP pNq; q tpx, y, z q | x [ z u. pp definition Du pS px1 , x2 , uq ^
pu, x3 , qq.
every relation structure preserved operation f , f called
polymorphism . Note polymorphisms also preserve relations pp
definition . following shown finite domain constraint satisfaction
Bulatov et al. (2005); easy proof also works infinite domain constraint satisfaction.
Lemma 29. Let R relation primitive positive definition structure .
CSPpq CSP expansion relation R polynomial-time equivalent.
following theorem one reasons useful work -categorical
templates (when possible).
Theorem 30 (Bodirsky & Nesetril, 2006). Let -categorical structure. R
primitive positive definable R preserved polymorphisms .
previous next result together used translate questions
primitive positive definability
purely operational questions. Let set, let Opnq
pnq set operations finite arity.
Dn D, let 8
n1
p
n
q
operation P called projection fixed P t1, . . . , nu n-tuples
px1, . . . , xnq P Dn identity px1, . . . , xnq xi. composition k-ary
operation f k operations g1 , . . . , gk arity n n-ary operation defined

pf pg1, . . . , gk qqpx1, . . . , xnq

f g1px1, . . . , xnq, . . . , gk px1, . . . , xnq .
750

fiTractable Set Constraints

Definition 31. say F locally generates f : Dn every finite subset
operation g : Dn obtained operations F
projection maps composition f paq g paq P .
Theorem 32 (see Szendrei, 1986, Corollary 1.9; also Bodirsky, 2012, Proposition 5.2.1). Let
F set operations domain D. operation f : Dk preserves
finitary relations preserved operations F F locally generates
f.
set automorphisms structure denoted Autpq. following,
always consider sets operations F contain AutpAq, therefore make following
convention. F O, say F generates f P F AutpAq locally generates f .
6.2 EI Set Constraints Atomless Boolean Algebra
previous subsection seen set CSPs formulated CSPs
-categorical structures. section, describe -categorical templates
correspond set CSPs EI set constraints. order so, define analogs
operations e i, defined instead P pNq.
Proposition 33. isomorphism A2 A.
Proof. straightforward verify A2 countable atomless Boolean algebra.

Motivated properties e described Lemma 11, make following definition.
Definition 34. Let B B1 two arbitrary Boolean algebras domains B B 1 ,
respectively, let g : B B 1 function strongly preserves [, 0, 1. say
g forgets unions k 1, l 0, x1 , . . . , xk , y1 , . . . , yl P B
epx1 q \ \ epxk q \ epy1 q \ \ epyl q 1
exists k xi \ y1 \ \ yl

1.

Proposition 35. exists injection e : strongly preserves
A, forgets unions.

[, 0, 1

Proof. construction e standard application Konigs tree lemma categorical structures (see, e.g., Bodirsky & Dalmau, 2012, Lemma 2); suffices show
injection f every finite induced substructure B f
strongly preserves [, 0, 1, forgets unions.
let B finite substructure A, let B domain B. Let C
pP pB q; [, \, c, 0, 1q Boolean algebra subsets B. claim g : B P pB q
given g p1q 1 g pxq tz | z 0 ^ z B xu x 1
preserves 0 1: definition;
751

fiBodirsky & Hils

preserves

[: x, P B (including case x 1 1)
g pxq [C g py q tz | z 0 ^ z B x ^ z B u
(
z | z 0 ^ z B px [B q
g p x [B q ;

injective: x,
x y;
strongly preserves

P B gpxq gpyq, x B B x, hence
[: follows previous two items;

forgets unions: shown analogously proof Lemma 11.






Indeed, one xi \ y1 \ \ yl 1 iff xi B j yj iff xi [ j yj j yj iff


g pxi q[ j g pyj q j g pyj q iff g pxi q\ g py1 q\ \ g pyl q 1. Thus, xi \ y1 \ \ yl 1
1 k implies g px1 q \ \ g pxk q \ g py1 q \ \ g pyl q 1.
prove converse, use finite Boolean algebra B may identified
pP pAq; [, \, c, 0, 1q finite set A. Xi : xi \ y1 \ \ yl 1 1, . . . , k,
may choose ai P AzXi , i.e. ai P j yj [ xi , 1, . . . , k. Let C : ta1 , . . . , ak u
A, C P B. construction, k one tC u R g pxi q \ g py1 q \ \ g pyl q.
particular, follows g px1 q \ \ g pxk q \ g py1 q \ \ g pyl q 1.
Clearly, embedding h C A. f : hpg q homomorphism
B forgets unions.
Proposition 36. Let quantifier-free formula signature t[, \, c, 0, 1u.
e preserves e preserves A. Moreover, every operation
strongly preserves [, 0, 1 forgets unions generates e, generated
e.
Proof. Let tuple elements A. Clearly, exists tuple b elements
P pNq b satisfy set quantifier-free formulas; follows
fact every finite Boolean algebra Boolean algebra subsets finite set.
observe whether tuple epbq satisfies quantifier-free formula
depends , Lemma 11. Since e strongly preserves [, 0, 1, forgets unions,
true quantifier-free formulas hold epaq. Hence, e preserves
e preserves S.
prove second part statement, use Theorem 32. Suppose c
tuples (of length) elements satisfy quantifier-free
formulas. Since quantifier-elimination, follows c satisfy firstorder formulas A. consequence theorem Ryll-Nardzewski mentioned

beginning Section 6.1, exists automorphism maps c d.
observations Theorem 32, implies operations strongly preserve
[, 0, 1, forget unions generate other.
r operation px, q epipx, qq.
Let ei
752

fiTractable Set Constraints

Proposition 37. Let quantifier-free formula signature t[, \, c, 0, 1u.
r preserves A. Moreover, every binary operation
ei preserves ei
r generated
g strongly preserves [, 0, 1, forgets unions generates ei,
r
ei.
Proof. arguments similar ones given proof Proposition 36. a1
a2 n-tuples elements A, n-tuples b1 , b2 elements P pNq
pa1 , a2 q pb1 , b2 q satisfy set quantifier-free formulas. Whether
eipb1 , b2 q satisfies quantifier-free formula depends , ei strongly preserves [,
r pa1 , a2 q, ei
r preserves
0, 1, forgets unions. holds ei
ei preserves S.
proof second part statement identical one Proposition 36.

6.3 Central Argument
give central argument maximal tractability EI, stated universalalgebraic language. say operation Ak depends argument
P t1, . . . , k u pk 1q-ary operation f 1 x1 , . . . , xk P
f px1 , . . . , xk q f 1 px1 , . . . , xi1 , xi

1 , . . . , xk

q.

equivalently characterize k-ary operations depend i-th argument
requiring x1 , . . . , xk P x1i P
f px1 , . . . , xk q f px1 , . . . , xi1 , x1i , xi

1 , . . . , xk

q.

following general fact injective maps.
Lemma 38. Let f : Ak function depends arguments,
locally generated set injective operations F. f injective.
Proof. first prove every term px1 , . . . , xn q formed operations F
variables x1 , . . . , xn every variable appears least defines injective
map. prove induction term structure. case n 1
x1 nothing show. Otherwise, form f pT1 , . . . , Tk q k-ary f P F
Tj Tj pxi1 , . . . , ximpj q q j k term operations F variables
xi1 , . . . , ximpj q appears least Tj . suppose a1 , . . . , P
b1 , . . . , bn P pa1 , . . . , q pb1 , . . . , bn q. want show ai bi
n. Since f injective must Tj pai1 , . . . , aimpj q q Tj pbi1 , . . . , bimpj q q
j k. Since every variable x1 , . . . , xn appears least once, variable
xi must appear Tj , j k. Since Tj defines injective operation inductive
assumptions, must ai bi . follows defines injective map.
suppose f operation locally generated F depends
arguments. Thus, ci1 , . . . , cin di f pci1 , . . . , cin q
f pci1 , . . . , cii1 , di , cii 1 , . . . , cin q. Let a1 , . . . , , b1 , . . . , bn P f pa1 , . . . , q
f pb1 , . . . , bn q. show a1 b1 , . . . , bn . Since f locally generated
753

fiBodirsky & Hils

F, exists term px1 , . . . , xn q composed variables x1 , . . . , xn operations F pe1 , . . . , en q f pe1 , . . . , en q elements e1 , . . . , en
set ta1 , . . . , , b1 , . . . , bn , c11 , . . . , cnn , d1 , . . . , dn u. i, variable xi must appear
px1 , . . . , xn q pci1 , . . . , cin q pci1 , . . . , cii1 , di , cii 1 , . . . , cin q. Hence, argument
beginning proof shows px1 , . . . , xn q defines injective map,
therefore a1 b1 , . . . , bn . shown f injective.
r u. either tf u generates ei,
r f
Theorem 39. Let f operation generated tei
generated teu.

r u.
Proof. show statement theorem, let f k-ary operation generated tei
sake notation, let x1 , . . . , xl arguments f depends, l k.
Let f 1 : Al operation given f 1 px1 , . . . , xl q f px1 , . . . , xl1 , xl , xl , . . . , xl q.
Observe f 1 depends arguments, locally generated injective operations;
Lemma 38, f 1 injective. Since f 1 generated operations preserve 0, 1,
[, also f 1 preserves them. f 1 injective, even strongly preserves 0, 1, [.
Consider first case l 1, i.e., f 1 unary. finite subsets A,
operation f 1 equals automorphism A, f generated AutpAq
nothing show. assume otherwise; is, assume finite set
P AutpAq f 1 pxq apxq x P S. claim f 1 forgets
unions. see this, let u1 , . . . , um , v1 , . . . , vn f 1 pu1 q \ \ f 1 pum q \
u, term pxq composed
f 1 pv1 q \ \ f 1 pvn q 1. Since f 1 generated tei
r
ei, automorphisms A, single variable x f 1 pxq pxq
x P tu1 , . . . , um , v1 , . . . , vn u. choice S, term cannot composed
automorphisms alone, hence must P AutpAq operational terms T1 , T2
r f 1 pxq apei
r pT1 pxq, T2 pxqqq
composed automorphisms ei
r
x P S. ei forgets unions, exists k T1 pui q\ T1 pv1 q\ \ Tl pvn q 1.
Since T1 strongly preserves [, means ui \ v 1 \ \ v n 1 (see proof
Proposition 10), wanted show. Proposition 36 follows f 1
generated e. f generated e well.
Next, consider case l 1. Let g binary operation defined g px, q
1
f px, y, . . . , q. functions depends arguments, cannot generated
automorphisms alone. Hence, term form
r pT1 px, q, T2 px, qqq
px, q apei


P AutpAq,
r automorphisms A,
T1 T2 operational terms composed ei,
two variables x y,

g px, q px, q px, q P tu1 , . . . , um , v1 , . . . , vn u.

claim g forgets unions. Assume g pu1 q\ \ g pum q\ g pv1 q\ \ g pvn q 1
elements u1 pu11 , u21 q, . . . , um pu1m , u2m q, v1 pv11 , v12 q, . . . , vn pvn1 , vn2 q A2 .
r forgets unions, exists k T1 pui q\ T1 pv1 q\ \ T1 pvn q 1
Since ei
754

fiTractable Set Constraints

T2 pui q\T2 pv1 q\ \T2 pvn q 1. Suppose first T1 depends arguments. T1
defines injective operation strongly preserves [. follows ui \ v 1 \ \ v n 1
A2 since equations inner Horn. argue similarly T2 depends
arguments, cases established g forgets unions. Suppose
T1 T2 depend arguments. Consider first case
T1 depends first argument. function x T1 px, xq injective
strongly preserves [, T1 pui q \ T1 pv1 q \ \ T1 pvn q 1 derive
u1i \ v11 \ \ vn1 1 holds A. case, T2 must depend second argument,
since depends arguments. therefore also u2i \ v12 \ \ vn2 1 holds
A. situation T1 depends second argument T2 depends
r
first argument analogous. g forgets unions. Proposition 37, g generates ei.
r
Consequently, also f generates ei.
Theorem 40. Let set constraint language. Suppose contains relations
EI, also contains relation EI. finite sublanguage 1
CSPp1 q NP-hard.
Proof. R1 , R2 , . . . relations , let 1 , 2 , . . . quantifier-free formulas
define R1 , R2 , . . . pP pNq; \, [, c, 0, 1q. Let R1A , R2A , . . . relations defined
1 , 2 , . . . A, let relational structure domain exactly
r contains
relations. Proposition 37, contains relation preserved ei,
r Consider set F polymorphisms .
relations preserved ei.
Theorem 32, operations F locally generated eri.
set F contain eri, since would contradict Theorem 32 fact
r Since F locally closed, follows
contains relation preserved ei.
Theorem 39 operations f P F generated e. relation
tpx, y, zq | x z _ x zu preserved operations F (we already seen
relation Example 5), hence pp definable Theorem 30. relation
NP-complete CSP (Bodirsky & Kara, 2008). Let 1 reduct contains
exactly relations appear pp definition tpx, y, z q | x z _ x z u
. Clearly, finitely many relations; denote corresponding relation
symbols 1 . Lemma 29, CSPp1 q NP-hard.
establishes also hardness CSPpq: let 1 1 -reduct . claim
CSPp1 q CSPp1 q computational problem. show
conjunction atomic 1 -formulas satisfiable 1 true 1 .
Replacing atomic 1 -formula quantifier-free definition, follows
Theorem 28.

7. Concluding Remarks
introduced powerful set constraint language EI set constraints,
particular contains Horn-Horn set constraints previously studied tractable set
constraint languages. Constraint satisfaction problems EI solved polynomial
even quadratic time. tractability result complemented complexity result
shows tractability EI set constraints best-possible within large class
set constraint languages.
755

fiBodirsky & Hils

would also like remark algorithm test whether given finite set
constraint language (where relations language given quantifier-free formulas
signature t\, [, c, 0, 1u) contained EI. means so-called metaproblem EI set constraints decided effectively.
Proposition 41. algorithm test whether given quantifier-free formula
signature t\, [, c, 0, 1u defines relation EI.
Proof. clear effectively transformed normal form described
Section 3, assume conjunction outer clauses,
atomic formula form 1 inner conjunctive normal form.
Let n number variables . test two n-tuples u1 , u2
elements P pNq satisfy , n-tuple eipu1 , u2 q satisfies well. Note
whether tuple satisfies depends Boolean algebra generated
entries tuple S. Boolean algebra generated n elements size
n
22 ; therefore, finitely many cases check. pair Boolean
algebras generating tuples u1 , u2 , check whether eipu1 , u2 q satisfies follows.
Lemma 11, eipu1 , u2 q satisfies atomic formula 1 every inner clause
x1 \
\ xk \ epy1q \ \ epy1q exists k ipu1, u
2 q satisfies
xi \ j j 1. turn true u1 u2 satisfy xi \ j j 1.
truth value non-atomic formulas tuple eipu1 , u2 q computed
truth value atomic formulas usual way.

Finally would also like remark one analogously obtain tractability
class constraints inner clauses positive outer literals dual Horn
(i.e., one negative literal). statements proofs respective result
obtained dualizing following formal sense: dual relation R
definable Boolean algebra relation tcptq | P Ru. dual k-ary operation
f domain operation px1 , . . . , xk q cpf pcpx1 q, . . . , cpxk qqq. proofs
translate literally proofs dualized versions statements.

Acknowledgments
extended abstract article appeared proceedings IJCAI11 (Bodirsky, Hils,
& Krimkevitch, 2011)3 . want thank Francois Bossiere pointed mistakes
conference version paper. One mistake concerned reduction CSP
languages EI satisfiability Horn-Horn clauses; concerned problem
previous proof Theorem 26.
Manuel Bodirsky received funding ERC European Communitys
Seventh Framework Programme (FP7/2007-2013 Grant Agreement no. 257039).
3. third author conference version left author team preparation journal version.

756

fiTractable Set Constraints

References
Aiken, A. (1994). Set constraints: Results, applications, future directions. Proceedings
Second Workshop Principles Practice Constraint Programming,
pp. 326335.
Baader, F. (2003). Least common subsumers specific concepts description logic
existential restrictions terminological cycles. Proceedings International
Joint Conferences Artificial Intelligence (IJCAI), pp. 319324.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. International Joint
Conferences Artificial Intelligence (IJCAI), pp. 364369.
Barto, L., & Kozik, M. (2009). Constraint satisfaction problems bounded width.
Proceedings Annual Symposium Foundations Computer Science (FOCS),
pp. 595603.
Bodirsky, M. (2012). Complexity classification infinite-domain constraint satisfaction.
Memoire dhabilitation diriger des recherches, Universite Diderot Paris 7. Available
arXiv:1201.0856.
Bodirsky, M., Chen, H., & Pinsker, M. (2010). reducts equality primitive
positive interdefinability. Journal Symbolic Logic, 75 (4), 12491292.
Bodirsky, M., & Dalmau, V. (2012). Datalog constraint satisfaction infinite templates. appear Journal Computer System Sciences. preliminary
version appeared proceedings Symposium Theoretical Aspects
Computer Science (STACS05).
Bodirsky, M., Hils, M., & Krimkevitch, A. (2011). Tractable set constraints. Proceedings
International Joint Conferences Artificial Intelligence (IJCAI), pp. 510515.
Bodirsky, M., Hils, M., & Martin, B. (2011). scope universal-algebraic approach constraint satisfaction. appear Logical Methods Computer Science (LMCS), 9099. Available arXiv:0909.5097v3. extended abstract
announced results appeared proceedings Logic Computer
Science (LICS10).
Bodirsky, M., & Kara, J. (2008). complexity equality constraint languages. Theory
Computing Systems, 3 (2), 136158. conference version appeared proceedings
Computer Science Russia (CSR06).
Bodirsky, M., & Kara, J. (2009). complexity temporal constraint satisfaction problems. Journal ACM, 57 (2), 141. extended abstract appeared
Proceedings Symposium Theory Computing (STOC08).
Bodirsky, M., & Kutz, M. (2007). Determining consistency partial tree descriptions.
Artificial Intelligence, 171, 185196.
Bodirsky, M., & Nesetril, J. (2006). Constraint satisfaction countable homogeneous
templates. Journal Logic Computation, 16 (3), 359373.
Boole, G. (1847). Investigation Laws Thought. Walton, London. Reprinted
Philisophical Library, New York, 1954.
757

fiBodirsky & Hils

Broxvall, M., Jonsson, P., & Renz, J. (2002). Disjunctions, independence, refinements.
Artificial Intelligence, 140 (1/2), 153173.
Bulatov, A. A. (2003). Tractable conservative constraint satisfaction problems. Proceedings Symposium Logic Computer Science (LICS), pp. 321330, Ottawa,
Canada.
Bulatov, A. A. (2006). dichotomy theorem constraint satisfaction problems
3-element set. Journal ACM, 53 (1), 66120.
Bulatov, A. A., & Dalmau, V. (2006). simple algorithm Maltsev constraints. SIAM
Journal Computing, 36 (1), 1627.
Bulatov, A. A., Krokhin, A. A., & Jeavons, P. G. (2005). Classifying complexity
constraints using finite algebras. SIAM Journal Computing, 34, 720742.
Cohen, D., Jeavons, P., Jonsson, P., & Koubarakis, M. (2000). Building tractable disjunctive
constraints. Journal ACM, 47 (5), 826853.
Dowling, W. F., & Gallier, J. H. (1984). Linear-time algorithms testing satisfiability
propositional Horn formulae. Journal Logic Programming, 1 (3), 267284.
Drakengren, T., & Jonsson, P. (1998). Reasoning set constraints applied tractable
inference intuitionistic logic. Journal Logic Computation, 8 (6), 855875.
Garey, M., & Johnson, D. (1978). guide NP-completeness. CSLI Press, Stanford.
Hodges, W. (1993). Model theory. Cambridge University Press.
Idziak, P. M., Markovic, P., McKenzie, R., Valeriote, M., & Willard, R. (2010). Tractability
learnability arising algebras subpowers. SIAM Journal Computing, 39 (7), 30233037.
Jonsson, P., & Backstrom, C. (1998). unifying approach temporal constraint reasoning.
Artificial Intelligence, 102 (1), 143155.
Jonsson, P., & Drakengren, T. (1997). complete classification tractability RCC-5.
Journal Artificial Intelligence Research, 6, 211221.
Koppelberg, S. (1989). Projective boolean algebras. Handbook Boolean Algebras,
Vol. 3, pp. 741773. North Holland, Amsterdam-New York-Oxford- Tokyo.
Koubarakis, M. (2001). Tractable disjunctions linear constraints: Basic results applications temporal reasoning. Theoretical Computer Science, 266, 311339.
Krotzsch, M., Rudolph, S., & Hitzler, P. (2006). complexity Horn description
logics. OWL: Experiences Directions Workshop.
Kuncak, V., Nguyen, H. H., & Rinard, M. C. (2006). Deciding boolean algebra presburger arithmetic. Journal Automatic Reasoning, 36 (3), 213239.
Kuncak, V., & Rinard, M. C. (2007). Towards efficient satisfiability checking boolean
algebra presburger arithmetic. Proceedings International Conference
automated deduction (CADE), pp. 215230.
Kusters, R., & Molitor, R. (2002). Approximating specific concepts description
logics existential restrictions. AI Communications, 15 (1), 4759.
758

fiTractable Set Constraints

Lassez, J.-L., & McAloon, K. (1989). Independence negative constraints. International
Joint Conference Theory Practice Software Development (TAPSOFT), Volume 1, pp. 1927.
Marriott, K., & Odersky, M. (1996). Negative Boolean constraints. Theoretical Computer
Science, 160 (1&2), 365380.
Schaefer, T. J. (1978). complexity satisfiability problems. Proceedings
Symposium Theory Computing (STOC), pp. 216226.
Szendrei, A. (1986). Clones universal algebra. Seminaire de Mathematiques Superieures.
Les Presses de lUniversite de Montreal.

759

fiJournal Artificial Intelligence Research 45 (2012) 305-362

Submitted 4/12; published 10/12

Tutorial Dual Decomposition Lagrangian Relaxation
Inference Natural Language Processing
Alexander M. Rush

SRUSH @ CSAIL . MIT. EDU

Computer Science Artificial Intelligence Laboratory
Massachusetts Institute Technology
Cambridge, 02139, USA

Michael Collins

MCOLLINS @ CS . COLUMBIA . EDU

Department Computer Science
Columbia University
New York, NY 10027, USA

Abstract
Dual decomposition, generally Lagrangian relaxation, classical method combinatorial optimization; recently applied several inference problems natural language processing (NLP). tutorial gives overview technique. describe example algorithms, describe formal guarantees method, describe practical issues implementing
algorithms. examples predominantly drawn NLP literature, material
general relevance inference problems machine learning. central theme
tutorial Lagrangian relaxation naturally applied conjunction broad class combinatorial algorithms, allowing inference models go significantly beyond previous work
Lagrangian relaxation inference graphical models.

1. Introduction
many problems statistical natural language processing, task map input x (e.g.,
string) structured output (e.g., parse tree). mapping often defined
= argmax h(y)

(1)

yY

finite set possible structures input x, h : R function assigns
score h(y) Y. example, part-of-speech tagging, x would sentence,
would set possible tag sequences x; parsing, x would sentence would
set parse trees x; machine translation, x would source-language sentence
would set possible translations x. problem finding referred
decoding problem. size typically grows exponentially respect size
input x, making exhaustive search intractable.
paper gives overview decoding algorithms NLP based dual decomposition,
generally, Lagrangian relaxation. Dual decomposition leverages observation
many decoding problems decomposed two sub-problems, together linear
constraints enforce notion agreement solutions different problems.
sub-problems chosen solved efficiently using exact combinatorial
c
2012
AI Access Foundation. rights reserved.

fiRUSH & C OLLINS

algorithms. agreement constraints incorporated using Lagrange multipliers, iterative
algorithmfor example, subgradient algorithmis used minimize resulting dual. Dual
decomposition algorithms following properties:
typically simple efficient. example, subgradient algorithms involve two
steps iteration: first, sub-problems solved using combinatorial algorithm; second, simple additive updates made Lagrange multipliers.
well-understood formal properties, particular connections linear programming (LP) relaxations.
cases underlying LP relaxation tight, produce exact solution
original decoding problem, certificate optimality.1 cases underlying LP
tight, heuristic methods used derive good solution; alternatively, constraints
added incrementally relaxation tight, point exact solution
recovered.
Dual decomposition, two combinatorial algorithms used, special case
Lagrangian relaxation (LR). useful also consider LR methods make use single
combinatorial algorithm, together set linear constraints incorporated using
Lagrange multipliers. use single combinatorial algorithm qualitatively different
dual decomposition approaches, although techniques closely related.
Lagrangian relaxation long history combinatorial optimization literature, going back
seminal work Held Karp (1971), derive relaxation algorithm traveling
salesman problem. Initial work Lagrangian relaxation/dual decomposition decoding statistical models focused MAP problem Markov random fields (Komodakis, Paragios, &
Tziritas, 2007, 2011). recently, decoding algorithms derived several models
statistical NLP, including models combine weighted context-free grammar (WCFG)
finite-state tagger (Rush, Sontag, Collins, & Jaakkola, 2010); models combine lexicalized
WCFG discriminative dependency parsing model (Rush et al., 2010); head-automata models
non-projective dependency parsing (Koo, Rush, Collins, Jaakkola, & Sontag, 2010); alignment
models statistical machine translation (DeNero & Macherey, 2011); models event extraction
(Riedel & McCallum, 2011); models combined CCG parsing supertagging (Auli & Lopez,
2011); phrase-based models statistical machine translation (Chang & Collins, 2011); syntaxbased models statistical machine translation (Rush & Collins, 2011); models semantic parsing (Das, Martins, & Smith, 2012); models parsing tagging make use document-level
constraints (Rush, Reichart, Collins, & Globerson, 2012); models coordination problem
natural language parsing (Hanamoto, Matsuzaki, & Tsujii, 2012); models based intersection weighted automata (Paul & Eisner, 2012). give overview several
algorithms paper.
focus examples natural language processing, material tutorial
general relevance inference problems machine learning. clear relevance
problem inference graphical models, described example Komodakis et al.
(2007, 2011); however one central theme tutorial Lagrangian relaxation naturally
1. certificate optimality information allows proof optimality solution constructed polynomial time.

306

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

applied conjunction much broader class combinatorial algorithms max-product
belief propagation, allowing inference models go significantly beyond graphical models.
remainder paper structured follows. Section 2 describes related work. Section 3
gives formal introduction Lagrangian relaxation. Section 4 describes dual decomposition
algorithm (from Rush et al., 2010) decoding model combines weighted context-free
grammar finite-state tagger. algorithm used running example throughout
paper. Section 5 describes formal properties dual decomposition algorithms. Section 6 gives
examples algorithms, section 7 describes practical issues. Section 8 gives overview
work alternative optimization methods subgradient methods described tutorial.
Finally, section 9 describes relationship LP relaxations, describes tightening methods.

2. Related Work
tutorial draws ideas fields combinatorial optimization, machine learning,
natural language processing. section, give summary work fields
relevant methods describe.
2.1 Combinatorial Optimization
Lagrangian relaxation (LR) widely used method combinatorial optimization, going back
seminal work Held Karp (1971) traveling salesman problem. See work
Lemarechal (2001) Fisher (1981) surveys LR methods, textbook Korte
Vygen (2008) background combinatorial optimization. Decomposing linear integer
linear programs also fundamental technique optimization community (Dantzig & Wolfe,
1960; Everett III, 1963). direct relationship LR algorithms linear
programming relaxations combinatorial optimization problems; again, see textbook Korte
Vygen.
2.2 Belief Propagation, Linear Programming Relaxations Inference MRFs
large amount research MAP inference problem Markov random fields
(MRFs). tree-structured MRFs, max-product belief propagation (max-product BP) (Pearl, 1988)
gives exact solutions. (Max-product BP form dynamic programming, closely related Viterbi algorithm.) general MRFs underlying graph may contain cycles,
MAP problem NP-hard: led researchers consider number approximate inference algorithms. Early work considered loopy variants max-product BP (see example
Felzenszwalb & Huttenlocher, 2006, application loopy max-product BP problems
computer vision); however, methods heuristic, lacking formal guarantees.
recent work considered methods based linear programming (LP) relaxations
MAP problem. See work Yanover, Meltzer, Weiss (2006), section 1.6 work
Sontag, Globerson, Jaakkola (2010), description. Methods based LP relaxations
benefit stronger guarantees loopy belief propagation. Inference cast optimization
problem, example problem minimizing dual. Since dual problem convex, convergence results convex optimization linear programming leveraged directly. One
particularly appealing feature methods certificates optimality given
exact solution MAP problem found.
307

fiRUSH & C OLLINS

Komodakis et al. (2007, 2011) describe dual decomposition method provably optimizes
dual LP relaxation MAP problem, using subgradient method. work
crucial reference tutorial. (Note addition, Johnson, Malioutov, & Willsky, 2007, also
describes LR methods inference MRFs.)
tutorial focus subgradient algorithms optimization dual objective. See
section 8 discussion alternative optimization approaches developed within
machine learning community.
2.3 Combinatorial Algorithms Belief Propagation
central idea algorithms describe use combinatorial algorithms maxproduct BP. idea closely related earlier work use combinatorial algorithms within
belief propagation, either MAP inference problem (Duchi, Tarlow, Elidan, & Koller, 2007),
computing marginals (Smith & Eisner, 2008). methods generalize loopy BP way
allows use combinatorial algorithms. Again, argue methods based Lagrangian
relaxation preferable variants loopy BP, stronger formal guarantees.
2.4 Linear Programs Decoding Natural Language Processing
Dual decomposition Lagrangian relaxation closely related integer linear programming
(ILP) approaches, linear programming relaxations ILP problems. Several authors
used integer linear programming directly solving challenging problems NLP. Germann, Jahr,
Knight, Marcu, Yamada (2001) use ILP test search error greedy phrase-based translation system short sentences. Roth Yih (2005) formulate constrained sequence labeling
problem ILP decode using general-purpose solver. Lacoste-Julien, Taskar, Klein,
Jordan (2006) describe quadratic assignment problem bilingual word alignment decode using ILP solver. work Riedel Clarke (2006) Martins, Smith, Xing
(2009) formulates higher-order non-projective dependency parsing ILP. Riedel Clarke decode using ILP method constraints added incrementally. Martins et al. solve LP
relaxation project valid dependency parse. Like many works, method presented
tutorial begins ILP formulation decoding problem; however, instead employing general-purpose solver aim speed decoding using combinatorial algorithms
exploit underlying structure problem.

3. Lagrangian Relaxation Dual Decomposition
section first gives formal description Lagrangian relaxation, gives description
dual decomposition, important special case Lagrangian relaxation. descriptions
give deliberately concise. material section essential remainder
paper, may safely skipped reader, returned second reading. However
descriptions may useful would like immediately see formal treatment
Lagrangian relaxation dual decomposition. algorithms paper special cases
framework described section.
308

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

3.1 Lagrangian Relaxation
assume finite set Y, subset Rd . score associated
vector
h(y) =

also vector Rd . decoding problem find

= argmax h(y) = argmax
yY

(2)

yY

definitions, structure represented d-dimensional vector, score
structure linear function, namely . practice, structured prediction problems
often binary vector (i.e., {0, 1}d ) representing set parts2 present structure y.
vector assigns score part, definition h(y) = implies score
sum scores parts contains.
assume problem Eq. 2 computationally challenging. cases,
might NP-hard problem. cases, might solvable polynomial time,
algorithm still slow practical.
first key step Lagrangian relaxation choose finite set 0 Rd
following properties:
0 . Hence 0 contains vectors found Y, addition contains vectors
Y.
value Rd , easily find
argmax
yY 0

(Note replaced Eq. 2 larger set 0 .) easily mean
problem significantly easier solve problem Eq. 2. example, problem
Eq. 2 might NP-hard, new problem solvable polynomial time;
problems might solvable polynomial time, new problem significantly
lower complexity.
Finally, assume

= {y : 0 Ay = b}

(3)

Rpd b Rp . condition Ay = b specifies p linear constraints y.
assume number constraints, p, polynomial size input.
implication linear constraints Ay = b need added set 0 ,
constraints considerably complicate decoding problem. Instead incorporating hard
constraints, deal constraints using Lagrangian relaxation.
2. example, context-free parsing part might correspond tuple hA B C, i, k, ji B C
context-free rule, i, k, j integers specifying non-terminal spans words . . . j input sentence,
non-terminal B spans words . . . k, non-terminal C spans words (k + 1) . . . j. finite-state tagging
bigram tagging model part might tuple hA, B, ii A, B tags, integer specifying tag
B seen position sentence, tag seen position (i 1). See work Rush et al. (2010)
detailed treatment examples.

309

fiRUSH & C OLLINS

introduce vector Lagrange multipliers, u Rp . Lagrangian
L(u, y) = + u (Ay b)
function combines original objective function , second term incorporates
linear constraints Lagrange multipliers. dual objective
L(u) = max0 L(u, y)
yY

dual problem find
minp L(u)

uR

common approachwhich used algorithms paperis use subgradient
algorithm minimize dual. set initial Lagrange multiplier values u(0) = 0.
k = 1, 2, . . . perform following steps:
(k) = argmax L(u(k1) , y)

(4)

u(k) = u(k1) k (Ay (k) b)

(5)

yY 0

followed
k > 0 step size kth iteration. Thus iteration first find structure
(k) , update Lagrange multipliers, updates depend (k) .
crucial point (k) found efficiently,




argmax L(u(k1) , y) = argmax + u(k1) (Ay b) = argmax 0
yY 0

yY 0

yY 0

0 = + A> u(k1) . Hence Lagrange multiplier terms easily incorporated
objective function.
state following theorem:
Theorem 1 following properties hold Lagrangian relaxation:
a). u Rp , L(u) maxyY h(y).
b). suitable choice step sizes k (see section 5), limk L(u(k) ) = minu L(u).
c). Define yu = argmaxyY 0 L(u, y). u Ayu = b, yu = argmaxyY (i.e.,
yu optimal).
particular, subgradient algorithm described above, k Ay (k) = b,
(k) = argmaxyY .
d). minu L(u) = maxQ , set Q defined below.

310

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

Thus part (a) theorem states dual value provides upper bound score
optimal solution, part (b) states subgradient method successfully minimizes upper
bound. Part (c) states ever reach solution (k) satisfies linear constraints,
solved original optimization problem.
Part (d) theorem gives direct connection Lagrangian relaxation method
LP relaxation problem Eq. 2. define set Q. First, define set
distributions set 0 :
0

= { : R|Y | ,

X
yY 0

= 1, 0 1}

convex hull 0 defined
Conv(Y 0 ) = { Rd : s.t. =

X

y}

yY 0

Finally, define set Q follows:
Q = {y : Conv(Y 0 ) Ay = b}
Note similarity Eq. 3: simply replaced 0 Eq. 3 convex hull 0 . 0
subset Conv(Y 0 ), hence subset Q. consequence Minkowski-Weyl theorem
(Korte & Vygen, 2008, Thm. 3.31) Conv(Y 0 ) polytope (a bounded set specified
intersection finite number half spaces), Q therefore also polytope. problem
max
Q

therefore linear program, relaxation original problem, maxyY .
Part (d) theorem 1 direct consequence duality linear programming.
following implications:
minimizing dual L(u), recover optimal value maxQ LP
relaxation.
maxQ = maxyY say LP relaxation tight. case
subgradient algorithm guaranteed3 find solution original decoding problem,
= argmax = argmax
Q

yY

cases LP relaxation tight, methods (e.g., see Nedic & Ozdaglar,
2009) allow us recover approximate solution linear program, = argmaxQ
. Alternatively, methods used tighten relaxation exact solution obtained.
3. assumption unique solution problem maxyY ; solution unique
subtleties may arise.

311

fiRUSH & C OLLINS

3.2 Dual Decomposition
give formal description dual decomposition. see, dual decomposition
special case Lagrangian relaxation;4 however, important enough purposes
tutorial warrant description. Again, section deliberately concise, may safely
skipped first reading.
assume finite set Rd . vector associated
score
f (y) = (1)
0

(1) vector Rd . addition, assume second finite set Z Rd , vector
z Z associated score
g(z) = z (2)
decoding problem find
argmax (1) + z (2)
yY,zZ


Ay + Cz = b
0

Rpd , C Rpd , b Rp .
Thus decoding problem find optimal pair structures, linear constraints
specified Ay + Cz = b. practice, linear constraints often specify agreement constraints
z: is, specify two vectors sense coherent.
convenience, make connection Lagrangian relaxation clear, define
following sets:
W = {(y, z) : Y, z Z, Ay + Cz = b}

W 0 = {(y, z) : Y, z Z}
follows decoding problem find


argmax (1) + z (2)



(6)

(y,z)W

Next, make following assumptions:
4. Strictly speaking, Lagrangian relaxation also viewed special case dual decomposition: formulation section set Z = Y, (2) = 0, Ci,j = 0 i, j, thus recovering Lagrangian relaxation
problem previous section. sense Lagrangian relaxation dual decomposition equivalent (we
transform Lagrangian relaxation problem dual decomposition problem, vice versa). However,
view dual decomposition naturally viewed special case Lagrangian relaxation, particular
methods described tutorial go back work Held Karp (1971) (see section 6.3), makes
use single combinatorial algorithm. addition, Lagrangian relaxation appears standard term
combinatorial optimization literature: example textbook Korte Vygen (2008) description
Lagrangian relaxation mention dual decomposition; several tutorials Lagrangian relaxation
combinatorial optimization literature (e.g., see Lemarechal, 2001; Fisher, 1981), found
difficult find direct treatments dual decompositon. Note however recent work machine learning
computer vision communities often used term dual decomposition (e.g., Sontag et al., 2010; Komodakis et al.,
2007, 2011).

312

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

value (1) Rd , easily find argmaxyY (1) . Furthermore,
0
value (2) Rd , easily find argmaxzZ z (2) . follows (1) Rd ,
0
(2) Rd , easily find
(y , z ) = argmax (1) + z (2)

(7)

(y,z)W 0

setting
= argmax (1) ,
yY

z = argmax z (2)
zZ

Note Eq. 7 closely related problem Eq. 6, W replaced W 0 (i.e.,
linear constraints Ay + Cz = b dropped). easily mean
optimization problems significantly easier solve original problem Eq. 6.
clear problem special case Lagrangian relaxation setting,
described previous section. goal involves optimization linear objective,
finite set W, given Eq. 6; efficiently find optimal value set W 0 W
subset W 0 , W 0 dropped linear constraints Ay + Cz = b.
dual decomposition algorithm derived similar way before. introduce
vector Lagrange multipliers, u Rp . Lagrangian
L(u, y, z) = (1) + z (2) + u (Ay + Cz b)
dual objective
L(u) = max 0 L(u, y, z)
(y,z)W

subgradient algorithm used find minuRp L(u). initialize Lagrange multipliers u(0) = 0. k = 1, 2, . . . perform following steps:
(y (k) , z (k) ) = argmax L(u(k1) , y, z)
(y,z)W 0

followed
u(k) = u(k1) k (Ay (k) + Cz (k) b)
k > 0 stepsize.
Note solutions (k) , z (k) iteration found easily, easily verified

!
(k1)

argmax L(u

, y, z) =

(y,z)W 0

argmax
yY

0(1)

, argmax z

0(2)

,

zZ

0(1) = (1) + A> u(k1) 0(2) = (2) + C > u(k1) . Thus dual decomposes two
easily solved maximization problems.
formal properties dual decomposition similar stated theorem 1.
particular, shown
minp L(u) = max (1) + (2)

uR

(,)Q

313

fiRUSH & C OLLINS


NP

VP

N

V

United

flies

NP




N



large

jet

Figure 1: example parse tree.

set Q defined
Q = {(, ) : (, ) Conv(W 0 ) + C = d}
problem
max (1) + (2)

(,)Q

linear programming problem, L(u) dual linear program.
descriptions Lagrangian relaxation dual decomposition given
sufficient level generality include broad class algorithms, including introduced paper. remainder paper describes specific algorithms developed within
framework, describes experimental results practical issues arise, elaborates
theory underlying algorithms.
Note section described dual-decomposition approach two components. generalization two components relatively straightforward; example
see work Komodakis et al. (2007, 2011), see also work Martins, Smith, Figueiredo,
Aguiar (2011).

4. Example: Integration Parser Finite-State Tagger
next describe dual decomposition algorithm decoding model combines
weighted context-free grammar finite-state tagger. classical approach problem
use dynamic programming algorithm, based construction Bar-Hillel, Perles,
Shamir (1964) intersection context-free language finite-state language. dual
decomposition algorithm advantages exhaustive dynamic programming, terms
efficiency simplicity. use dual decomposition algorithm running example
throughout tutorial.
first give formal definition problem, describe motivation problem, describe classical dynamic programming approach. describe dual decomposition
algorithm.
314

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

4.1 Definition Problem
Consider problem mapping input sentence x parse tree y. Define set
parse trees x. parsing problem find
= argmax h(y)

(8)

yY

h(y) score parse tree Y.
consider case h(y) sum two model scores: first, score
weighted context-free grammar; second, score part-of-speech (POS) sequence
finite-state part-of-speech tagging model. formally, define h(y)
h(y) = f (y) + g(l(y))

(9)

functions f , g, l defined follows:
1. f (y) score weighted context-free grammar (WCFG). WCFG consists
context-free grammar set rules G, scoring function : G R assigns
real-valued score rule G. score entire parse tree sum scores
rules contains. example, consider parse tree shown Figure 1; tree,
f (y) = (S NP VP) + (NP N) + (N United)
+(VP V NP) + . . .

remain agnostic scores individual context-free rules defined. one
example, probabilistic context-free grammar, would define ( ) = log p(
|). second example, conditional random field (CRF) (Lafferty, McCallum, &
Pereira, 2001) would define ( ) = w ( ) w Rq parameter
vector, ( ) Rq feature vector representing rule .
2. l(y) function maps parse tree sequence part-of-speech tags y.
parse tree Figure 1, l(y) would sequence N V N.
3. g(z) score part-of-speech tag sequence z mth-order finite-state tagging
model. model, zi = 1 . . . n ith tag z,
g(z) =

n
X

(i, zim , zim+1 , . . . , zi )

i=1

(i, zim , zim+1 , . . . , zi ) score sub-sequence tags zim , zim+1 , . . . , zi
ending position sentence.5
remain agnostic terms defined. one example, g(z) might
log-probability z hidden Markov model, case
(i, zim . . . zi ) = log p(zi |zim . . . zi1 ) + log p(xi |zi )
5. define zi 0 special start POS symbol.

315

fiRUSH & C OLLINS

xi ith word input sentence. another example, CRF would

(i, zim . . . zi ) = w (x, i, zim . . . zi )
w Rq parameter vector, (x, i, zim . . . zi ) feature-vector representation
sub-sequence tags zim . . . zi ending position sentence x.
motivation problem follows. scoring function h(y) = f (y) + g(l(y))
combines information parsing model tagging model. two models capture
fundamentally different types information: particular, part-of-speech tagger captures information adjacent POS tags missing f (y). information may improve
parsing tagging performance, comparison using f (y) alone.6
definition h(y), conventional approach finding Eq. 8 construct
new context-free grammar introduces sensitivity surface bigrams (Bar-Hillel et al., 1964).
Roughly speaking, approach (assuming first-order tagging model) rules
VP V NP
replaced rules
VPN,N VN,V NPV,N

(10)

non-terminal (e.g., NP) replaced non-terminal tracks preceding last
POS tag relative non-terminal. example, NPV,N represents NP dominates sub-tree
whose preceding POS tag V, whose last POS tag N. weights new rules
context-free weights f (y). Furthermore, rules
V flies
replaced rules
VN,V flies
weights rules context-free weights f (y) plus bigram tag weights
g(z), example bigram N V. dynamic programming parsing algorithmfor example
CKY algorithmcan used find highest scoring structure new grammar.
approach guaranteed give exact solution problem Eq. 8; however
often inefficient. greatly increased size grammar introducing refined
non-terminals, leads significantly slower parsing performance. one example, consider
case underlying grammar CFG Chomsky-normal form, G non-terminals,
use 2nd order (trigram) tagging model, possible part-of-speech tags. Define
n length input sentence. Parsing grammar alone would take O(G3 n3 )
time, example using CKY algorithm. contrast, construction Bar-Hillel et al. (1964)
6. assumed sensible, theoretical and/or empirical sense, take sum scores f (y)
g(l(y)). might case, example, f (y) g(z) defined structured prediction models (e.g.,
conditional random fields), parameters estimated jointly using discriminative methods. f (y) g(z)
log probabilities PCFG HMM respectively, strict probabilistic sense make
sense combine scores way: however practice may work well; example, type log-linear
combination probabilistic models widely used approaches statistical machine translation.

316

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

results algorithm run time O(G3 6 n3 ).7 addition tagging model leads
multiplicative factor 6 runtime parser, significant decrease
efficiency (it uncommon take values say 5 50, giving values 6 larger
15, 000 15 million). contrast, dual decomposition algorithm describe next
takes O(k(G3 n3 + 3 n)) time problem, k number iterations required
convergence; experiments, k often small number. significant improvement
runtime Bar-Hillel et al. method.
4.2 Dual Decomposition Algorithm
introduce alternative formulation problem Eq. 8, lead directly
dual decomposition algorithm. Define set POS tags. Assume input sentence
n words. parse tree y, position {1 . . . n}, tag , define
y(i, t) = 1 parse tree tag position i, y(i, t) = 0 otherwise. Similarly, tag
sequence z, define z(i, t) = 1 tag sequence tag position i, 0 otherwise.
example, following parse tree tag sequence y(4, A) = 1 z(4, A) = 1:


NP
N

VP
V

United flies

NP




N

N

large jet

V

United1 flies2





some3 large4

N
jet5

addition, define Z set possible POS tag sequences input sentence.
introduce following optimization problem:
Optimization Problem 1 Find
argmax f (y) + g(z)

(11)

yY,zZ

{1 . . . n}, , y(i, t) = z(i, t).
Thus find best pair structures z share POS sequence.
define (y , z ) pair structures achieve argmax problem. crucial
7. precise, assume finite-state automaton Q states context-free chart rule productions
hA B C, i, k, ji A, B, C G 1 < k < j n well productions hA wi , ii G
{1 . . . n}. (Here use wi refer ith word sentence, set G refer set nonterminals grammar. follows G = |G|.) Applying Bar-Hillel intersection gives new rule productions
hAs1 ,s3 Bs1 ,s2 Cs2 ,s3 , i, k, ji s1 , s2 , s3 {1 . . . Q} well hAs,t wi , ii s, {1 . . . Q}
(s, t) valid state transition FSA. intersection, count free variables see
O(G3 n3 Q3 ) rule productions, implies CKY algorithm find best parse O(G3 n3 Q3 ) time.
case tagging, 2nd-order tagging model represented FSA |T |2 states, state
represents previous two tags. intersection, yields O(G3 n3 |T |6 ) time algorithm.

317

fiRUSH & C OLLINS

claim, easily verified, also argmax problem Eq. 8. sense,
solving new problem immediately leads solution original problem.
make following two assumptions. Whether assumptions satisfied
depend definitions f (y) (for assumption 1) definitions Z g(z)
(for assumption 2). assumptions hold f (y) WCFG g(z) finite-state tagger,
generally may hold parsing tagging models.
Assumption 1 Assume introduce variables u(i, t) R {1 . . . n}, .
assume value variables, find




argmax f (y) +
yY

X

u(i, t)y(i, t)

i,t

efficiently.
example. Consider WCFG grammar Chomsky normal form. scoring
function defined
f (y) =

X
XY Z

c(y, X Z)(X Z) +

X
i,t

y(i, t)(t wi )

write c(y, X Z) denote number times rule X Z seen
parse tree y, y(i, t) = 1 word POS t, 0 otherwise (note y(i, t) = 1 implies
rule wi used parse tree). highest scoring parse tree f (y)
found efficiently, example using CKY parsing algorithm.




argmax f (y) +
yY

X

u(i, t)y(i, t) =

i,t





argmax
yY

X

XY Z

c(y, X Z)(X Z) +

X
i,t

y(i, t)((t wi ) + u(i, t))

argmax found easily using CKY algorithm, scores (t wi )
simply replaced new scores defined 0 (t wi ) = (t wi ) + u(i, t).
Assumption 2 Assume introduce variables u(i, t) R {1 . . . n}, .
assume value variables, find




argmax g(z)
zZ

X

u(i, t)z(i, t)

i,t

efficiently.
example. Consider 1st-order tagging model,
g(z) =

n
X

(i, zi1 , zi )

i=1

318

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

Initialization: Set u(0) (i, t) = 0 {1 . . . n},
k = 1 K


P



P

(k) argmaxyY f (y) +
z (k) argmaxzZ g(z)



(k1) (i, t)y(i, t) [Parsing]
i,t u



(k1) (i, t)z(i, t) [Tagging]
i,t u

(k) (i, t) = z (k) (i, t) i, Return (y (k) , z (k) )
Else u(k+1) (i, t) u(k) (i, t) k (y (k) (i, t) z (k) (i, t))

Figure 2: dual decomposition algorithm integrated parsing tagging. k k = 1 . . . K
step size kth iteration.






argmax g(z)
zZ

X

u(i, t)z(i, t)

i,t



n
X
X
u(i, t)z(i, t)
= argmax (i, zi1 , zi )
zZ

= argmax
zZ

i,t

i=1
n
X
0

(i, zi1 , zi )

i=1


0 (i, zi1 , zi ) = (i, zi1 , zi ) u(i, zi )
argmax found efficiently using Viterbi algorithm, new 0 terms
incorporate u(i, t) values.
Given assumptions, dual decomposition algorithm shown Figure 2. algorithm
manipulates vector variables u = {u(i, t) : {1 . . . n}, }. soon see
variable u(i, t) Lagrange multiplier enforcing constraint y(i, t) = z(i, t) optimization
problem. iteration algorithm finds hypotheses (k) z (k) ; assumptions 1 2
step efficient. two structures POS sequence (i.e., (k) (i, t) = z (k) (i, t)
(i, t)) algorithm returns solution. Otherwise, simple updates made
u(i, t) variables, based (k) (i, t) z (k) (i, t) values.
moment well give example run algorithm. First, though, give important
theorem:
Theorem 2 iteration algorithm Figure 2 (k) (i, t) = z (k) (i, t)
(i, t), (y (k) , z (k) ) solution optimization problem 1.
319

fiRUSH & C OLLINS

theorem direct consequence theorem 5 paper.
Thus reach agreement (k) z (k) , guaranteed optimal
solution original problem. Later tutorial give empirical results various NLP
problems showing often, quickly, reach agreement. also describe
theory underlying convergence; theory underlying cases algorithm doesnt converge;
methods used tighten algorithm goal achieving convergence.
Next, consider efficiency algorithm. concrete, consider case
f (y) defined weighted CFG, g(z) defined finite-state tagger.
iteration algorithm requires decoding two models. number
iterations k relatively small, algorithm much efficient using construction
Bar-Hillel et al. (1964). discussed before, assuming context-free grammar Chomsky
normal form, trigram tagger tags, CKY parsing algorithm takes O(G3 n3 ) time,
Viterbi algorithm tagging takes O(T 3 n) time. Thus total running time dual
decomposition algorithm O(k(G3 n3 + 3 n)) k number iterations required
convergence. contrast, construction Bar-Hillel et al. results algorithm running
time O(G3 6 n3 ). dual decomposition algorithm results additive cost incorporating
tagger (a 3 n term added run time), whereas construction Bar-Hillel et al. results
much expensive multiplicative cost (a 6 term multiplied run time). (Smith &
Eisner, 2008, makes similar observation additive versus multiplicative costs context
belief propagation algorithms dependency parsing.)
4.3 Relationship Approach Section 3
easily verified approach described instance dual decomposition
framework described section 3.2. set set parses input sentence; set
Z set POS sequences input sentence. parse tree Rd represented
vector f (y) = y(1) (1) Rd : number ways representing parse
trees vectors, see work Rush et al. (2010) one example. Similarly, tag sequence
0
0
z Rd represented vector g(z) = z (2) (2) Rd . constraints
y(i, t) = z(i, t)
(i, t) encoded linear constraints
Ay + Cz = b
suitable choices A, C, b, assuming vectors z include components y(i, t)
z(i, t) respectively.
4.4 Example Run Algorithm
give example run algorithm. simplicity, assume step size k
equal 1 iterations k. take input sentence United flies large jet. Initially,
algorithm sets u(i, t) = 0 (i, t). example, decoding initial weights
leads two hypotheses
320

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING


NP


N

VP




V

N

United flies large jet

V





United1 flies2 some3 large4

N
jet5

two structures different POS tags three positions, highlighted red; thus two
structures agree. update u(i, t) variables based differences, giving new
values follows:
u(1, A) = u(2, N ) = u(5, V ) = 1
u(1, N ) = u(2, V ) = u(5, N ) = 1
u(i, t) values shown still value 0. decode new u(i, t) values,
giving structures

VP

NP
N

V

United flies

NP




N



large jet

N





United1 flies2 some3 large4

N
jet5

Again, differences structures shown red. update u(i, t) values
obtain new values follows:
u(5, N ) = 1
u(5, V ) = 1
u(i, t) values 0. (Note updates reset u(1, A), u(1, N ), u(2, N )
u(2, V ) back zero.)
decode again, new u(i, t) values; time, two structures
321

fiRUSH & C OLLINS

100

% examples converged

80

60

40

20

0

50
<=

20
<=

10
<=

4
<=

3
<=

2
<=

1
<=

number iterations

Figure 3: Convergence results work Rush et al. (2010) integration probabilistic
parser POS tagger, using dual decomposition. show percentage examples
exact solution returned algorithm, versus number iterations
algorithm.


NP
N

VP
V

United flies

NP




N

large jet

N

V





United1 flies2 some3 large4

N
jet5

two structures identical sequences POS tags, algorithm terminates,
guarantee solutions optimal.
Rush et al. (2010) describe experiments using algorithm integrate probabilistic parser
Collins (1997) POS tagger Toutanova, Klein, Manning, Singer (2003). (In
experiments stepsize k held constant, instead set using strategy described
section 7.2 paper.) Figure 3 shows percentage cases exact solutions returned
(we agreement (k) z (k) ) versus number iterations algorithm.
algorithm produces exact solutions 99% examples. 94% examples
algorithm returns exact solution 10 iterations fewer. models least,
dual decomposition algorithm guaranteed give exact solution, case
successful achieving goal.
322

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

5. Formal Properties
give formal properties algorithm described previous section. first
describe three important theorems regarding algorithm, describe connections
algorithm subgradient optimization methods.
5.1 Three Theorems
Recall problem attempting solve (optimization problem 1)
argmax f (y) + g(z)
yY,zZ

= 1 . . . n, ,

y(i, t) = z(i, t)

first step introduce Lagrangian problem. introduce Lagrange
multiplier u(i, t) equality constraint y(i, t) = z(i, t): write u = {u(i, t) :
{1 . . . n}, } denote vector Lagrange mulipliers. Lagrange multiplier take
positive negative value. Lagrangian
L(u, y, z) = f (y) + g(z) +

X
i,t

u(i, t) (y(i, t) z(i, t))

(12)

Note grouping terms depend z, rewrite Lagrangian




L(u, y, z) = f (y) +

X
i,t





u(i, t)y(i, t) + g(z)

X

u(i, t)z(i, t)

i,t

defined Lagrangian, dual objective
L(u) =

max L(u, y, z)

yY,zZ



= max f (y) +
yY


X
i,t



u(i, t)y(i, t) + max g(z)
zZ


X

u(i, t)z(i, t)

i,t

assumptions 1 2 described above, dual value L(u) value u calculated
efficiently: simply compute two maxs, sum them. Thus dual decomposes
convenient way two efficiently solvable sub-problems.
Finally, dual problem minimize dual objective, is, find
min L(u)
u

see shortly algorithm Figure 2 subgradient algorithm minimizing dual
objective.
Define (y , z ) optimal solution optimization problem 1. first theorem
follows:
Theorem 3 value u,
L(u) f (y ) + g(z )
323

fiRUSH & C OLLINS

Hence L(u) provides upper bound score optimal solution. proof simple:
Proof:
L(u) =

=

max L(u, y, z)

yY,zZ

(13)

max

L(u, y, z)

(14)

max

f (y) + g(z)

(15)

yY,zZ:y=z

yY,zZ:y=z



= f (y ) + g(z )

(16)

use shorthand = z state y(i, t) = z(i, t) (i, t). Eq. 14 follows
adding constraints = z, optimizing smaller set (y, z) pairs, hence
max cannot increase. Eq. 15 follows = z,
X
i,t

u(i, t) (y(i, t) z(i, t)) = 0

hence L(u, y, z) = f (y) + g(z). Finally, Eq. 16 follows definition z .
property L(u) f (y ) + g(z ) value u often referred weak duality.
value inf u L(u) f (y ) g(z ) often referred duality gap optimal duality
gap (see example Boyd & Vandenberghe, 2004).
Note obtaining upper bound f (y ) + g(z ) (providing relatively tight)
useful goal itself. First, upper bounds form used admissible heuristics
search methods A* branch-and-bound algorithms. Second, method
generates potential solution (y, z), immediately obtain upper bound far solution
optimal,
(f (y ) + g(z )) (f (y) + g(z)) L(u) (f (y) + g(z))
Hence L(u) (f (y) + g(z)) small, (f (y ) + g(z )) (f (y) + g(z)) must small. See
section 7 discussion.
second theorem states algorithm Figure 2 successfully converges minu L(u).
Hence algorithm successfully converges tightest possible upper bound given dual.
theorem follows:
Theorem 4 Consider algorithm Figure 2. sequence 1 , 2 , 3 , . . . k > 0
k 1,
lim k = 0

k


X

k=1

k = ,


lim L(uk ) = min L(u)
u

k

Proof: See work Shor (1985). See also Appendix A.3.
algorithm actually subgradient method minimizing L(u): return point
section 5.2. though, important point algorithm successfully minimizes L(u).
final theorem states ever reach agreement algorithm Figure 2,
guaranteed optimal solution. first need following definitions:
324

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

Definition 1 value u, define




(u) = argmax f (y) +
yY



X

u(i, t)y(i, t)

i,t





z (u) = argmax g(z)
zZ

X

u(i, t)z(i, t)

i,t

theorem then:
Theorem 5 u

(u) (i, t) = z (u) (i, t)

i, t,
f (y (u) ) + g(z (u) ) = f (y ) + g(z )
i.e., (y (u) , z (u) ) optimal.
Proof: have, definitions (u) z (u) ,
L(u) = f (y (u) ) + g(z (u) ) +

X
i,t

= f (y

(u)

) + g(z

(u)

u(i, t)(y (u) (i, t) z (u) (i, t))

)

second equality follows (u) (i, t) = z (u) (i, t) (i, t). L(u) f (y ) +
g(z ) values u, hence
f (y (u) ) + g(z (u) ) f (y ) + g(z )
z optimal, also
f (y (u) ) + g(z (u) ) f (y ) + g(z )
hence must
f (y (u) ) + g(z (u) ) = f (y ) + g(z )
Theorems 4 5 refer quite different notions convergence dual decomposition
algorithm. remainder tutorial, avoid confusion, explicitly use following
terms:
d-convergence (short dual convergence) used refer convergence dual
decomposition algorithm minimum dual value: is, property limk L(u(k) ) =
minu L(u). theorem 4, assuming appropriate step sizes algorithm, always
d-convergence.
e-convergence (short exact convergence) refers convergence dual decomposition algorithm point y(i, t) = z(i, t) (i, t). theorem 5, dual
decomposition algorithm e-converges, guaranteed provided optimal solution. However, algorithm guaranteed e-converge.
325

fiRUSH & C OLLINS

5.2 Subgradients
proof d-convergence, defined theorem 4, relies fact algorithm Figure 2
subgradient algorithm minimizing dual objective L(u). Subgradient algorithms
generalization gradient-descent methods; used minimize convex functions
non-differentiable. section describes algorithm Figure 2 derived subgradient
algorithm.
Recall L(u) defined follows:
L(u) =

max L(u, y, z)

yY,zZ



= max f (y) +
yY




X
i,t

u(i, t)y(i, t) + max g(z)
zZ


X

u(i, t)z(i, t)

i,t

goal find minu L(u).
First, note L(u) following properties:
L(u) convex function. is, u(1) Rd , u(2) Rd , [0, 1],
L(u(1) + (1 )u(2) ) L(u(1) ) + (1 )L(u(2) )
(The proof simple: see Appendix A.1.)
L(u) differentiable. fact, easily shown piecewise linear function.
fact L(u) differentiable means cannot use gradient descent method
minimize it. However, nevertheless convex function, instead use subgradient
algorithm. definition subgradient follows:
Definition 2 (Subgradient) subgradient convex function L : Rd R u vector (u)
v Rd ,
L(v) L(u) + (u) (v u)
subgradient (u) tangent point u gives lower bound L(u): sense
similar8 gradient convex differentiable function.9 key idea subgradient
methods use subgradients way would use gradients gradient descent
methods. is, use updates form
u0 = u (u)
u current point search, (u) subgradient point, > 0 step size,
u0 new point search. suitable conditions stepsizes (e.g., see theorem 4),
updates successfully converge minimum L(u).
8. precisely, function L(u) convex differentiable, gradient point u subgradient
u.
9. noted, however, given point u, may one subgradient: occur,
example, piecewise linear function points gradient defined.

326

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

S(flies)
NP(United)

VP(flies)

N

V

United

flies

NP(jet)




N

large jet

*0

United1 flies2

some3 large4

jet5

Figure 4: lexicalized parse tree, dependency structure.
calculate subgradient L(u)? turns convenient
form. (see definition 1), define (u) z (u) argmaxs two maximization
problems L(u). define vector (u)
(u) (i, t) = (u) (i, t) z (u) (i, t)
(i, t), shown (u) subgradient L(u) u. updates
algorithm Figure 2 take form
u0 (i, t) = u(i, t) (y (u) (i, t) z (u) (i, t))
hence correspond directly subgradient updates.
See Appendix A.2 proof subgradients take form, Appendix A.3 proof
convergence subgradient optimization method.

6. Examples
section describe examples dual decomposition algorithms. first example,
also work Rush et al. (2010), dual decomposition algorithm combines two
parsing models. second example, work Komodakis et al. (2007, 2011), dual
decomposition algorithm inference Markov random fields. Finally, describe algorithm
Held Karp (1971) traveling salesman problem, algorithm Chang Collins
(2011) decoding phrase-based translation models.
6.1 Combined Constituency Dependency Parsing
Rush et al. (2010) describe algorithm finding highest scoring lexicalized context-free
parse tree input sentence, combination two models: lexicalized probabilistic
context-free grammar, discriminative dependency parsing model.
Figure 4 shows example lexicalized context-free tree. take set
lexicalized trees input sentence, f (y) score tree lexicalized
parsing modelspecifically, f (y) log-probability model Collins (1997).
model, lexicalized rule receives score log probability, log
probability sum log probabilities rules contains.
327

fiRUSH & C OLLINS

100

% examples converged

80

60

40

20

0

50
<=

20
<=

10
<=

4
<=

3
<=

2
<=

1
<=

number iterations

Figure 5: Convergence results work Rush et al. (2010) integration lexicalized
probabilistic context-free grammar, discriminative dependency parsing model.
show percentage examples exact solution returned algorithm,
versus number iterations algorithm.

second model dependency parsing model. example dependency parse also shown
Figure 4. set possible dependency parses sentence Z; parse z receives
score g(z) dependency parsing model. use discriminative dependency parsing
model Koo, Carreras, Collins (2008) (see also McDonald, 2006).
lexicalized parse tree y, mapping underlying dependency structure l(y).
decoding problem consider find
argmax f (y) + g(l(y))

(17)

yY

motivation problem allow us inject information dependency
parsing model g(z) lexicalized parsing model Collins (1997); Rush et al. (2010) show
gives significant improvements parsing accuracy.
problem solved exactly using dynamic programming approach,
dynamic program created intersection two models (there clear analogy
Bar-Hillel et al. (1964) method construction dynamic program intersection
PCFG HMM). However dynamic program relatively inefficient.
develop dual decomposition algorithm similar way before. dependency
(i, j) {0 . . . n} head word (we use 0 denote root symbol) j {1 . . . n},
j 6= i, modifier, define y(i, j) = 1 contains dependency (i, j), y(i, j) = 0
otherwise. define similar variables z(i, j) dependency structures. reformulate
problem Eq. 17 as:
Optimization Problem 2 Find
argmax f (y) + g(z)
yY,zZ

(i, j), y(i, j) = z(i, j).
328

(18)

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

Lagrangian introduced problem, similar Eq. 12,
subgradient algorithm used minimize resulting dual. introduce Lagrange multipliers
u(i, j) dependencies (i, j), whose initial values u(0) (i, j) = 0 i, j. iteration
algorithm find




(k) = argmax f (y) +
yY

X

u(k1) (i, j)y(i, j)

i,j

using dynamic programming algorithm lexicalized context-free parsing (a trivial modification
original algorithm finding argmaxy f (y)). addition find




z (k) = argmax g(z)
zZ

X

u(k1) (i, j)z(i, j)

i,j

using dynamic programming algorithm dependency parsing (again, requires trivial modification existing algorithm). (k) (i, j) = z (k) (i, j) (i, j) algorithm
e-converged, guaranteed solution optimization problem 2. Otherwise,
perform subgradient updates
u(k) (i, j) = u(k1) (i, j) k (y (k) (i, j) z (k) (i, j))
(i, j), go next iteration.
Rush et al. (2010) describe experiments algorithm. method e-converges
99% examples, 90% examples e-converging 10 iterations less. Figure 5 shows
histogram number examples e-converged, versus number iterations
algorithm. method gives significant gains parsing accuracy model Collins (1997),
significant gains baseline method simply forces lexicalized CFG parser
dependency structure first-best output dependency parser.10
6.2 MAP Problem Pairwise Markov Random Fields
Markov random fields (MRFs), generally graphical models, widely used machine
learning statistics. MAP problem MRFs problem finding likely setting
random variables MRFis inference problem central importance. section
describe dual decomposition algorithm work Komodakis et al. (2007, 2011)
finding MAP solution pairwise, binary, MRFs. Pairwise MRFs limited case
potential functions consider pairs random variables, opposed larger subsets; however,
generalization method non-pairwise MRFs straightforward.
commonly used approach MAP problem MRFs use loopy max-product belief propagation. dual decomposition algorithm advantages terms stronger formal
guarantees, described section 5.
10. Note Klein Manning (2002) describe method combination dependency parser constituent
based parser, score entire structure sum scores two models. approach
A* algorithm developed, admissible estimates within A* method computed efficiently using
separate inference two models. interesting connections A* approach dual
decomposition algorithm described section.

329

fiRUSH & C OLLINS

MAP problem follows. Assume vector variables y1 , y2 , . . . , yn ,
yi take two possible values, 0 1 (the generalization two possible values
variable straightforward). 2n possible settings n variables. MRF
assumes underlying undirected graph (V, E), V = {1 . . . n} set vertices
graph, E set edges. MAP problem find
argmax h(y)

(19)

y{0,1}n


X

h(y) =

i,j (yi , yj )

{i,j}E

i,j (yi , yj ) local potential associated edge {i, j} E, returns real
value (positive negative) four possible settings (yi , yj ).
underlying graph E tree, problem Eq. 19 easily solved using max-product
belief propagation, form dynamic programming. contrast, general graphs E, may
contain loops, problem NP-hard. key insight behind dual decomposition algorithm
decompose graph E trees T1 , T2 , . . . , Tm . Inference tree
performed efficiently; use Lagrange multipliers enforce agreement inference
results tree. subgradient algorithm used, iteration first perform
inference trees T1 , T2 , . . . , Tm , update Lagrange multipliers cases
disagreements.
simplicity, describe case = 2. Assume two trees
T1 E, T2 E, T1 T2 = E.11 Thus trees contains subset edges
(1)
E, together trees contain edges E. Assume define potential functions i,j
(2)

(i, j) T1 i,j (i, j) T2
X

i,j (yi , yj ) =

{i,j}E

(1)

X
{i,j}T1

(2)

X

i,j (yi , yj ) +

i,j (yi , yj )

{i,j}T2

easy do: example, define

i,j
(yi , yj ) =

i,j (yi , yj )
#(i, j)

= 1, 2 #(i, j) 2 edge {i, j} appears trees, 1 otherwise.
define new problem equivalent problem Eq. 19:
Optimization Problem 3 Find
argmax

X

y{0,1}n ,z{0,1}n

{i,j}T1

(1)

i,j (yi , yj ) +

X

(2)

i,j (zi , zj )

{i,j}T2

yi = zi = 1 . . . n.
11. may always possible decompose graph E 2 trees way. Komodakis et al. (2007, 2011)
describe algorithm general case 2 trees.

330

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

Note similarity previous optimization problems. goal find pair structures,
{0, 1}n z {0, 1}n . objective function written
f (y) + g(z)

(1)

X

f (y) =

i,j (yi , yj )

{i,j}T1


(2)

X

g(z) =

i,j (zi , zj )

{i,j}T2

set constraints, yi = zi = 1 . . . n, enforce agreement z.
proceed beforewe define Lagrangian Lagrange multiplier ui
constraint:
(1)

X

L(u, y, z) =

{i,j}T1

(2)

X

i,j (yi , yj ) +

i,j (zi , zj ) +

n
X
i=1

{i,j}T2

ui (yi zi )

minimize dual
L(u) = max L(u, y, z)
y,z

(0)

using subgradient algorithm. algorithm initialized ui
iteration algorithm find

X

(k) = argmax
y{0,1}n

= 0 = 1 . . . n.


X
(k1)
(1)
u
yi
(yi , yj ) +


i,j



{i,j}T1




z (k) = argmax
z{0,1}n

X

(2)
i,j (zi , zj )


X (k1)
u
zi





{i,j}T2

steps achieved efficiently, T1 T2 trees, hence max-product belief propP (k1)
P (k1)
agation produces exact answer. (The Lagrangian terms ui
yi ui
zi easily
(k)
(k)
incorporated.) yi = zi algorithm e-converged, guaranteed
solution optimization problem 3. Otherwise, perform subgradient updates form
(k)

ui

(k1)

= ui

(k)

k (yi

(k)

zi )

= {1 . . . n}, go next iteration. Intuitively, updates bias two inference
problems towards agreement other.
Komodakis et al. (2007, 2011) show good experimental results method. algorithm
parallels max-product belief propagation, ui values interpreted
messages passed sub-problems.
331

fiRUSH & C OLLINS

1

5

1

3

5

7
4

3

6

7
4

2

6

2

Figure 6: illustration approach Held Karp (1971). left tour
vertices 1 . . . 7. right 1-tree vertices 1 . . . 7. 1-tree consists tree
vertices 2 . . . 7, together 2 additional edges include vertex 1. tour
left also 1-tree (in fact every tour also 1-tree).

6.3 Held Karp Algorithm TSPs
next example approach Held Karp (1971) traveling salesman problems (TSPs),
notable original paper Lagrangian relaxation. algorithm
instance dual decomposition. Instead leveraging two combinatorial algorithms,
combination agreement constraints, makes use single combinatorial algorithm, together
set linear constraints incorporated using Lagrange multipliers. use
two combinatorial algorithms, seen dual decomposition, useful technique,
broadening scope algorithms make use single combinatorial algorithm
useful. NLP decoding algorithms leverage single combinatorial algorithm, see algorithm Chang Collins (2011) decoding phrase-based translation models (we describe
algorithm next section), algorithm Rush Collins (2011) decoding
syntax-based translation models.
TSP defined follows. undirected graph (V, E) vertices V = {1, 2, . . . , n},
edges E. edge e E score e R. subset edges E represented
vector = {ye : e E}, ye = 1 edge subset, ye = 0 otherwise. Thus
vector {0, 1}|E| . tour graph subset edges corresponds path
graph begins ends vertex, includes every vertex exactly
once. See Figure 6 example tour. use {0, 1}|E| denote set possible
tours. traveling salesman problem find
argmax

X

yY

eE

ye e

problem well-known NP-hard.12
key idea work Held Karp (1971) 1-tree, which, like tour, subset
E. Held Karp define 1-tree follows:
1-tree consists tree vertex set {2, 3, . . . , n}, together two distinct
edges vertex 1... Thus, 1-tree single cycle, cycle contains vertex 1,
vertex 1 always degree two.
12. many presentations traveling salesman problem goal find minimum cost tour: consistency
rest tutorial presentation considers maximization problem, equivalent.

332

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

Figure 6 shows example 1-tree. define 0 set possible 1-trees. follows
subset 0 , every tour also 1-tree.
Crucially, possible find
X
argmax
ye e
yY 0

eE

using efficient algorithm. first step, find maximum scoring spanning tree
vertices {2, 3, . . . , n}, using maximum spanning tree algorithm. second step, add
two highest scoring edges include vertex 1. simple show resulting 1-tree
optimal. Thus search set NP-hard, search larger set 0 performed
easily. Lagrangian relaxation algorithm explicitly leverage observation.
Next, note
= {y : 0 , {1, 2, . . . , n},

e:ie ye

P

= 2}

constraint form
X

ye = 2

(20)

e:ie

corresponds property ith vertex exactly two incident edges. Thus
add constraint vertex exactly two incident edges, go set 1-trees
set tours. Constraints form Eq. 20 linear ye variables, therefore
easily incorporated Lagrangian.
Held Karp introduce following optimization problem:
Optimization Problem 4 Find
X

argmax
yY 0

{1, 2, . . . , n},

e:ie ye

P

ye e

eE

= 2.

clear equivalent finding highest scoring tour graph.
before, deal equality constraints using Lagrange multipliers. Define Lagrange multipliers vector u = {ui : {1 . . . n}}. Lagrangian
L(u, y) =

X

ye e +

n
X

!

ui

e:ie

i=1

eE

X

ye 2

dual objective
L(u) = max0 L(u, y)
yY

(0)

subgradient algorithm takes following form. Initially set ui
iteration find
(k) = argmax
yY 0

X

ye e +

eE

n
X
(k1)

X

i=1

e:ie

ui

constraints satisfied, i.e.,
X

ye(k) = 2

e:ie

333

= 0 = 1 . . . n.

!!

ye 2

(21)

fiRUSH & C OLLINS

algorithm terminates, guarantee structure (k) solution optimization
problem 4. Otherwise, subgradient step used modify Lagrange multipliers.
shown subgradient L(u) u vector g (u) defined
g (u) (i) =

X
e:ie

ye(u) 2

(u) = argmaxyY 0 L(u, y). Thus subgradient step {1 . . . n},
!
(k)
ui

=

(k1)
ui

k

X
e:ie

ye(k) 2

(22)

Note problem Eq. 21 easily solved. equivalent finding
X

(k) = argmax
yY 0

ye e0

eE

modified edge weights e0 : edge e = {i, j}, define
(k1)

e0 = e + ui

(k1)

+ uj

Hence new edge weights incorporate Lagrange multipliers two vertices edge.
subgradient step Eq. 22 clear intuition. vertices greater 2 incident
edges (k) , value Lagrange multiplier ui decreased, effect
penalising edges including vertex i. Conversely, vertices fewer 2 incident edges,
ui increase, edges including vertex preferred. algorithm manipulates
ui values effort enforce constraints vertex exactly two incident edges.
note qualitative difference example previous algorithms.
previous algorithms employed two sets structures Z, two optimization problems,
equality constraints enforcing agreement two structures. TSP relaxation instead involves single set Y. two approaches closely related, however, similar theorems
apply TSP method (the proofs trivial modifications previous proofs).
L(u)

X

ye e

eE

u, optimal tour. appropriate step sizes subgradient algorithm,

lim L(u(k) ) = min L(u)
u

k

(k)

Finally, ever find structure
satisfies linear constraints, algorithm
e-converged, guaranteed solution traveling salesman problem.
6.4 Phrase-Based Translation
next consider Lagrangian relaxation algorithm, described work Chang Collins
(2011), decoding phrase-based translation models (Koehn, Och, & Marcu, 2003). input
phrase-based translation model source-language sentence n words, x = x1 . . . xn .
output sentence target language. examples section use German
source language, English target language. use German sentence

334

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

wir mussen auch diese kritik ernst nehmen
running example.
key component phrase-based translation model phrase-based lexicon, pairs
sequences words source language sequences words target language.
example, lexical entries relevent German sentence shown include
(wir mussen, must)
(wir mussen auch, must also)
(ernst, seriously)
on. phrase entry associated score, take value reals.
introduce following notation. phrase tuple (s, t, e), signifying subsequence xs . . . xt source language sentence translated target-language string e,
using entry phrase-based lexicon. example, phrase (1, 2, must) would specify sub-string x1 . . . x2 translated must. phrase p = (s, t, e) receives
score (p) R model. given phrase p, use s(p), t(p) e(p) refer
three components. use P refer set possible phrases input sentence x.
derivation finite sequence phrases, p1 , p2 , . . . pL . length L
positive integer value. derivation use e(y) refer underlying translation defined
y, derived concatenating strings e(p1 ), e(p2 ), . . . e(pL ). example,
= (1, 3, must also), (7, 7, take), (4, 5, criticism), (6, 6, seriously)

(23)


e(y) = must also take criticism seriously
score derivation defined
h(y) = g(e(y)) +

X

(p)

py

g(e(y)) score (log-probability) e(y) n-gram language model.
set valid derivations defined follows. derivation y, define y(i)
= 1 . . . n number times source word translated derivation.
formally,
X
y(i) =
[[s(p) t(p)]]
py

[[]] 1 statement true, 0 otherwise. set valid derivations
= {y P : = 1 . . . n, y(i) = 1}
P set finite length sequences phrases. Thus derivation valid,
source-language word must translated exactly once. definition, derivation Eq. 23
valid. decoding problem find
= argmax h(y)
yY

335

(24)

fiRUSH & C OLLINS

problem known NP-hard. useful intuition follows. dynamic programming
approach problem would need keep track bit-string length n specifying
n source language words havent translated point dynamic program.
2n bit-strings, resulting dynamic program exponential number
states.
describe Lagrangian relaxation algorithm. before, key idea define
set 0 subset 0 ,
argmax h(y)

(25)

yY 0

found efficiently. defining
0 = {y P :

Pn

i=1 y(i)

= n}

Thus derivations 0 satisfy weaker constraint total number source words translated
exactly n: dropped y(i) = 1 constraints. one example, following derivation
member 0 , member Y:
= (1, 3, must also), (1, 2, must), (3, 3, also), (6, 6, seriously)

(26)

case y(1) = y(2) = y(3) = 2, y(4) = y(5) = y(7) = 0, y(6) = 1. Hence
words translated once, words translated 0 times.
definition 0 , problem Eq. 25 solved efficiently, using dynamic
programming. contrast dynamic program Eq. 24, keeps track bit-string
length n, new dynamic program merely needs keep track many source language words
translated point search.
proceed follows. Note
= {y : 0 , = 1 . . . n, y(i) = 1}
introduce Lagrange multiplier u(i) constraint y(i) = 1. Lagrangian
L(u, y) = h(y) +

n
X
i=1

u(i) (y(i) 1)

subgradient algorithm follows. Initially set u(0) (i) = 0 i. iteration
find
(k) = argmax L(u(k1) , y)
(27)
yY 0

perform subgradient step
u(k) (i) = u(k1) (i) k (y (k) (i) 1)

(28)

point (k) (i) = 1 = 1 . . . n, guaranteed optimal
solution original decoding problem.
336

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

problem Eq. 27 solved efficiently,
argmax L(u(k1) , y)
yY 0



= argmax g(e(y)) +
yY 0

X

(p) +

py

n
X
i=1





u(k1) (i) (y(i) 1)



= argmax g(e(y)) +
yY 0

X

0 (p)

py


0 (p) = (p) +

t(p)
X

u(k1) (i)

i=s(p)

0 (p),

Thus new phrase scores,
take account Lagrange multiplier values
positions s(p) . . . t(p). subgradient step Eq. 28 clear intuition. source
language word translated once, associated Lagrange multiplier u(i)
decrease, causing phrases including word penalised next iteration. Conversely,
word translated 0 times Lagrange multiplier increase, causing phrases including
word preferred next iteration. subgradient method manipulates u(i) values
attempt force source-language word translated exactly once.
description given sketch: Chang Collins (2011) describe details
method, including slightly involved dynamic program gives tighter relaxation
method described here, tightening method incrementally adds constraints
method initially e-converge. method successful recovering exact solutions
phrase-based translation model, far efficient alternative approaches based
general-purpose integer linear programming solvers.

7. Practical Issues
section reviews various practical issues arise dual decomposition algorithms.
describe diagnostics used track progress algorithm minimizing dual,
providing primal solution; describe methods choosing step sizes, k , algorithm;
describe heuristics used cases algorithm provide exact
solution. continue use algorithm section 4 running example, although
observations easily generalized Lagrangian relaxation algorithms.
first thing note iteration algorithm produces number useful terms,
particular:
solutions (k) z (k) .
current dual value L(u(k) ) (which equal L(u(k) , (k) , z (k) ).
addition, cases function l : Z maps structure
structure l(y) Z, also
primal solution (k) , l(y (k) ).
337

fiRUSH & C OLLINS

-13
-14

Value

-15
-16
-17
-18

Current Primal
Current Dual

-19
0

10

20

30
Round

40

50

60

Figure 7: Graph showing dual value L(u(k) ) primal value f (y (k) ) + g(l(y (k) )), versus
iteration number k, subgradient algorithm translation example work
Rush Collins (2011).

primal value f (y (k) ) + g(l(y (k) )).
primal solution mean pair (y, z) satisfies constraints optimization problem.
example, optimization problem 1 (the combined HMM PCFG problem section 4)
primal solution properties Y, z Z, y(i, t) = z(i, t) (i, t).
one example, algorithm Figure 2, iteration produce parse tree (k) .
simple recover POS sequence l(y (k) ) parse tree, calculate score
f (y (k) ) + g(l(y (k) )) combined model. Thus even (k) z (k) disagree, still use
(k) , l(y (k) ) potential primal solution. ability recover primal solution value
(k) always holdbut cases hold, useful. allow us,
example, recover approximate solution cases algorithm hasnt e-converged
exact solution.
describe various items described used practical applications
algorithm.
7.1 Example Run Algorithm
Figure 7 shows run subgradient algorithm decoding approach machine translation
described work Rush Collins (2011). behavior typical cases
algorithm e-converges exact solution. show dual value L(u(k) ) iteration,
value f (y (k) ) + g(l(y (k) )). important points follows:
L(u) provides upper bound f (y ) + g(z ) value u,
L(u(k) ) f (y (k) ) + g(l(y (k) ))
every iteration.
example e-convergence exact solution, point
L(u(k) ) = f (y (k) ) + g(z (k) ))
338

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

-13

4

Gap

3.5

-14

3
Value

Value

-15
-16

2.5
2
1.5

-17

1
-18

Best Primal
Best Dual

-19
0

10

20

30
Round

40

50

0.5
0
60

0

10

20

30
Round

40

50

60

Figure 8: graph left shows best dual value Lk best primal value pk , versus
iteration number k, subgradient algorithm translation example work
Rush Collins (2011). graph right shows Lk pk plotted k.
(y (k) , z (k) ) guaranteed optimal (and addition, z (k) = l(y (k) )).
dual values L(u(k) ) monotonically decreasingthat is, iterations

L(u(k+1) ) > L(u(k) )
even though goal minimize L(u). typical: subgradient algorithms
general guaranteed give monotonically decreasing dual values. However, see
iterations dual decreasesthis typical.
Similarly, primal value f (y (k) ) + g(z (k) ) fluctuates (goes down) course
algorithm.
following quantities useful tracking progress algorithm kth iteration:
L(u(k) ) L(u(k1) ) change dual value one iteration next.
soon see useful choosing step size algorithm (if value
positive, may indication step size decrease).
0

Lk = mink0 k L(u(k ) ) best dual value found far. gives us tightest upper bound
f (y ) + g(z ) k iterations algorithm.
0

0

pk = maxk0 k f (y (k ) ) + g(l(y (k ) )) best primal value found far.
Lk pk gap best dual best primal solution found far algorithm.
Lk f (y ) + g(z ) pk ,
Lk pk f (y ) + g(z ) pk

hence value Lk pk gives us upper bound difference f (y ) + g(z )
pk . Lk pk small, guarantee primal solution close
optimal.
Figure 8 shows plot Lk pk versus number iterations k previous example, addition shows plot gap Lk pk . graphs are, surprisingly, much
smoother graph Figure 7. particular guaranteed values Lk pk
monotonically decreasing increasing respectively.
339

fiRUSH & C OLLINS

-13

0.01
0.005
0.0005

-13.5

Value

-14
-14.5
-15
-15.5
-16
0

5

10

15

20
Round

25

30

35

40

Figure 9: Graph showing dual value L(u(k) ) versus number iterations k, different
fixed step sizes.

7.2 Choice Step Sizes k
Figure 9 shows convergence algorithm various choices step size, chosen
keep stepsize constant iteration. immediately see potential dilemma arising.
small step size ( = 0.0005), convergence smooththe dual value monotonically
decreasingbut convergence slow. large step size ( = 0.01), convergence much
faster initial phases algorithm, dual fluctuates quite erratically. practice
often difficult choose constant step size gives good convergence properties
early late iterations algorithm.
Instead, found often find improved convergence properties choice
step size k
decreases increasing k. One possibility use definition k = c/k
k = c/ k c > 0 constant. However definitions decrease step size
rapidlyin particular, decrease step size iterations, even cases progress
made decreasing dual value. many cases found effective
definition
c
k =
t+1
c > 0 constant, < k number iterations prior k dual value
0
0
increases rather decreases (i.e., number cases k 0 k L(u(k ) ) > L(u(k 1) )).
definition step size decreases dual value moves wrong direction.
7.3 Recovering Approximate Solutions
Figure 10 shows run algorithm fail get e-convergence exact solution.
section 9.4 describe one possible strategy, namely tightening relaxation,
used produce exact solution cases. Another obvious strategy, approximate,
simply choose best primal solution generated k iterations algorithm,
0
0
fixed k: i.e., choose (k ) , l(y (k ) )
0

0

k 0 = argmax f (y (k ) ) + g(l(y (k ) ))
k0 k

340

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

0
Best Primal
-5

Value

-10
-15
-20
-25

Current Primal
Current Dual

-30
0

10

20

30

40
Round

50

60

70

100

100

90

90
Percentage

Percentage

Figure 10: Graph showing dual value L(u(k) ) primal value f (y (k) ) + g(l(y (k) )), versus
iteration number k, subgradient algorithm translation example
work Rush Collins (2011), method e-converge exact
solution.

80
70
60

0

200
400
600
800
Maximum Number Dual Decomposition Iterations

70
60

% validation UAS
% certificates
% match K=5000

50

80

f score
% certificates
% match K=50

50
1000

0

10
20
30
40
Maximum Number Dual Decomposition Iterations

50

Figure 11: Figures showing effects early stopping non-projective parsing algorithm
Koo et al. (2010) (left graph) combined constituency dependency parsing (right
graph). case plot three quantities versus number iterations, k: 1)
accuracy (UAS f-score); 2) percentage cases algorithm e-converges
giving exact solution, certificate optimality; 3) percentage cases
best primal solution kth iteration running algorithm
e-convergence.

described before, use Lk pk upper bound difference approximate solution optimal solution.
7.4 Early Stopping
interesting also consider strategy returning best primal solution early algorithm cases algorithm eventually e-converge exact solution. practice,
strategy sometimes produce high quality solution, albeit without certificate optimality, faster running algorithm e-convergence. Figure 11 shows graphs two problems:
non-projective dependency parsing (Koo et al., 2010), combined constituency dependency
341

fiRUSH & C OLLINS

parsing (Rush et al., 2010). case show three quantities vary number
iterations algorithm. first quantity percentage cases algorithm econverges, giving exact solution, certificate optimality. combined constituency
dependency parsing takes roughly 50 iterations (over 95%) cases e-converge;
second algorithm takes closer 1000 iterations.
addition, show graphs indicating quality best primal solution generated
iteration k algorithm, versus number iterations, k. early stopping strategy would
pick fixed value k, simply return best primal solution generated first k
iterations algorithm. first plot accuracy (f-score, dependency accuracy respectively)
two models early stopping: see accuracy quickly asymptotes
optimal value, suggesting returning primal solution e-convergence often yield high
quality solutions. also plot percentage cases primal solution returned fact
identical primal solution returned algorithm run e-convergence. see
curve asymptotes quickly, showing many cases early stopping strategy
fact produce optimal solution, albeit without certificate optimality.

8. Alternatives Subgradient Optimization
tutorial focused subgradient methods optimization dual objective. Several
alternative optimization algorithms proposed machine learning literature;
section give overview approaches.
Wainwright, Jaakkola, Willsky (2005) describe early important algorithm Markov
random fields (MRFs) based LP relaxations, tree-reweighted message passing (TRW). Following work Kolmogorov (2006), use TRW-E refer edge-based variant TRW,
TRW-T refer tree-based algorithm. Kolmogorov (2006) derives variant, TRW-S
(the refers sequential nature algorithm). three algorithmsTRW-E, TRW-T,
TRW-Sare motivated LP relaxation MRFs, none guarantee
converging optimal value LP. TRW-S strongest guarantee three algorithms, namely monotonically improves dual value, may converge optimal
dual value.
Yanover et al. (2006) describe experiments comparing TRW-based algorithms generic LP
solvers MRF problems (specifically, LP solver use CPLEX13 ). TRW-based algorithms considerably efficient CPLEX, due fact TRW-based methods
leverage underlying structure MRF problem. various Lagrangian relaxation algorithms described current paper viewed specialized algorithms solving LP
relaxations, explicitly leverage combinatorial structure within underlying problem.
Komodakis et al. (2007, 2011) give experiments comparing subgradient method TRWS TRW-T algorithms. experiments TRW-S generally performs better TRW-T.
several cases TRW-S finds optimal dual solution faster subgradient method;
cases TRW-S appears get stuck (as expected given lack convergence guarantee),
subgradient method finds global optimum. Overall, subgradient method competitive
TRW-S: may initially make slower progress dual objective, benefit guaranteed
convergence global optimum LP relaxation.
13. http://www.ilog.com/products/cplex/

342

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

Another important class algorithms optimizing dual LP block coordinate
descent algorithms: example MPLP algorithm Globerson Jaakkola (2007). See
work Sontag et al. (2010) discussion methods. Like TRW-S, MPLP algorithm
guaranteed monotonically improve dual value, guaranteed converge
global optimum MRF LP. several experimental settings, MPLP algorithm produces
better dual values early iterations subgradient methods, get stuck non-optimal
solution (Jojic, Gould, & Koller, 2010; Martins, Figueiredo, Aguiar, Smith, & Xing, 2011; Meshi
& Globerson, 2011). Another complication MPLP requires computing max-marginals
sub-problems iteration instead MAP assignments. Max-marginals may slower
compute practice, combinatorial problems computation may asymptotically
slower. (For example, directed spanning tree models Koo et al., 2010, MAP problem
solved O(n2 ) time n length input sentence, aware
algorithm solves max-marginal problem better O(n4 ) time.)
work, Jojic et al. (2010) describe accelerated method MRF inference, using
method Nesterov (2005) smooth objective underlying decomposition. method
relatively fast rate convergence (O(1/) time reach solution -close optimal).
Experiments work Jojic et al. (2010) show decrease number iterations required
compared subgradient; however work Martins et al. (2011) accelerated method
requires iterations subgradient algorithm. sets experiments, MPLP makes
initial progress either method. Accelerated subgradient also requires computing subproblem marginals, similar disadvantages MPLPs requirement max-marginals.
Recently, Martins et al. (2011) proposed augmented Lagrangian method inference using
alternating direction method multipliers (ADMM). See tutorial Boyd, Parikh, Chu,
Peleato, Eckstein (2011) ADMM. augmented Lagrangian method extends
objective quadratic penalty term representing amount constraint violation. ADMM
method optimizing augmented problem able maintain similar decomposibility properties dual decomposition. Like subgradient method, ADMM guaranteed find
optimum LP relaxation. Martins et al. (2011) show empirically ADMM requires
comparable number iterations MPLP find good primal solution, still guaranteed optimize LP. challenge ADMM extra quadratic term may complicate
sub-problem decoding, example clear directly decode parsing problems presented work quadratic term objective. Several alternative approaches
proposed: Martins et al. (2011) binarize combinatorial sub-problems binary-valued factor
graphs; Meshi Globerson (2011) avoid problem instead applying ADMM dual
LP; Martins (2012) Das et al. (2012) use iterative active set method utilizes MAP
solutions original sub-problems solve quadratic version. Martins (2012) also describes
recent results ADMM give O(1/) bound relaxed primal convergence.

9. Relationship Linear Programming Relaxations
section describes close relationship dual decomposition algorithm linear
programming relaxations. connection useful understanding behavior
algorithm, particular understanding cases algorithm e-converge
exact solution. addition, suggest strategies tightening algorithm exact
solution found.
343

fiRUSH & C OLLINS

9.1 Linear Programming Relaxation
continue use algorithm section 4 example; generalization problems
straightforward. First, define set
= { : R|Y| ,

X


= 1, 0 1}

Thus simplex, corresponding set probability distributions finite set Y.
Similarly, define
X
z = { : R|Z| ,
z = 1, z 0 z 1}
z

set distributions set Z.
define new optimization problem, follows:
Optimization Problem 5 Find
max

,z

X

f (y) +



X

z g(z)

(29)

z

i, t,
X

y(i, t) =



X

z z(i, t)

(30)

z

optimization problem linear program: objective Eq. 29 linear variables
; constraints Eq. 30, together constraints definitions z ,
also linear variables.
optimization problem similar original problem, optimization problem 1.
see this, define 0y follows:
0y = { : R|Y| ,

X


= 1, {0, 1}}

Thus 0y subset , constraints 0 1 replaced {0, 1}.
Define 0z similarly. Consider following optimization problem, replace z
Eq. 29 0y 0z respectively:
Optimization Problem 6 Find
max
0

X

,0z

f (y) +

X

z g(z)

(31)

z

i, t,
X

y(i, t) =



X
z

344

z z(i, t)

(32)

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

new problem equivalent original problem, optimization problem 1: choosing
vectors 0y 0z equivalent choosing single parse Y, single POS
sequence z. sense, optimization problem 5 relaxation original problem,
constraints form {0, 1} z {0, 1} replaced constraints form
0 1 0 z 1.
Note optimization problem 6 integer linear program, objective
linear variables, constraints variables combine linear constraints
integer constraints (that z must either 0 1). also worth noting
actually convex hull finite set 0y . points 0y form vertices polytope
.
useful theorem, central relationship linear programming combinatorial optimization problems, following:
Theorem 6 finite set Y, function f : R,
max f (y) = max
yY



X

f (y)

yY

defined above.
proof simple, given Appendix A.4.
9.2 Dual New Optimization Problem
describe dual problem linear program Eqs. 29 30.
function (u) vector dual variables u = {u(i, t) : {1 . . . n}, }. crucial result
two dual functions (u) L(u) identical.
new Lagrangian
!

(u, , ) =

X

f (y) +



=

X

X

z g(z) +

z

u(i, t)

X


i,t

y(i, t)

X

z z(i, t)

z



X
X
X

y(i, t)
f (y) +
u(i, t)




i,t



X
X
X
+ z g(z)
u(i, t)
z z(i, t)
z

z

i,t

new dual objective
(u) =

max

,z

(u, , )

Note simply maximized primal ( ) variables, ignoring
constraints Eq. 30. dual problem find
min (u)
u

Two theorems regarding dual problem follows:
345

fiRUSH & C OLLINS

Theorem 7 Define ( , ) solution optimization problem Eqs. 29 30.
min (u) =

X

u

f (y) +



X

z g(z)

z

Proof. follows immediately results linear programming duality see textbook
Korte Vygen (2008) details.
Note equality above, contrast previous result,
min L(u) f (y ) + g(z )
u

dual function gave upper bound best primal solution.
second theorem follows:
Theorem 8 value u,
(u) = L(u)

Thus two dual functions identical. Given subgradient algorithm described minimizes L(u), therefore also minimizes dual linear program Eqs. 29 30.
Proof.
(u) =



X
X
X
y(i, t) +
u(i, t)
max f (y) +







i,t



X
X
X
z z(i, t)
u(i, t)
max z g(z)

z

z

z

i,t





= max f (y) +
yY

X

u(i, t)y(i, t) +

i,t





max g(z)
zZ

X

u(i, t)z(i, t)

i,t

= L(u)
used theorem 6 give




X
X
X
X
u(i, t)y(i, t)
max f (y) +
u(i, t)
y(i, t) = max f (y) +





i,t

yY



i,t

used similar result replace max z max Z.
346

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING



y1

y2

y3

x

x

x



Z



b

b

c

c







z1

z2

z3



b

b



c

c













Figure 12: simple example three possible parse trees three possible POS sequences.



Z

y1

y2

y3

x

x

x





b

b

c



c

y1

y2

y3

x

x

x





b

b

c

c













z1

z2

z3

z1

z2

z3



b

b



c

c













Z



b

b



c

c













Figure 13: Illustration two solutions satisfy constraints Eq. 30. left, solution
= [0, 0, 1], = [0, 0, 1] puts weight 1 y3 z3 . right, fractional
solution = [0.5, 0.5, 0] = [0.5, 0.5, 0] puts 0.5 weight y1 /y2 z1 /z2 .

347

fiRUSH & C OLLINS

9.3 Example
give example illustrates ideas. example also illustrate
happens algorithm fails e-converge.
assume three possible parse trees, = {y1 , y2 , y3 }, three possible tag
sequences, Z = {z1 , z2 , z3 }, shown Figure 12. write distributions sets
vectors = [0, 0, 1] = [0.5, 0.5, 0].
consider pairs vectors (, ) satisfy constraints Eq. 30. Figure 13 illustrates
two possible solutions. One pair, denote (1 , 1 ), 1 = [0, 0, 1], 1 =
[0, 0, 1]. easily verified definition
X

y1 y(1, c) =

yY

X

z1 z(1, c) =

zY

X

y1 y(2, c) =

yY

X

z1 z(2, c) = 1

zY

expected values equal 0: hence (1 , 1 ) satisfies constraints. potential solution integral, puts weight 1 single parse tree/POS-tag sequence,
structures weight 0.
second pair satisfies constraints 2 = [0.5, 0.5, 0], 2 = [0.5, 0.5, 0].
definitions,
X

y2 y(1, a) =

X

z2 z(1, a) =

y2 y(1, b) =

X

z2 z(1, b) = 0.5

zY

yY

zY

yY

X


X
yY

y2 y(2, a) =

X

z2 z(2, a) =

X
yY

zY

y2 y(2, b) =

X

z2 z(2, b) = 0.5

zY

expected values equal 0. pair (2 , 2 ) fractional solution,
puts fractional (0.5) weight structures.
Next, consider different definitions functions f (y) g(z). Consider first definitions
f = [0, 0, 1] g = [0, 0, 1] (we write f = [0, 0, 1] shorthand f (y1 ) = 0, f (y2 ) = 0,
f (y3 ) = 1). solution problem Eqs. 29 30 pair (1 , 1 ).
Alternatively, consider definitions f = [1, 1, 2] g = [1, 1, 2]. case following
situation arises:
pair (1 , 1 ) achieves score 0 objective Eq. 29, whereas pair (2 , 2 )
achieves score 2. Thus solution problem Eqs. 29 30 (2 , 2 ),
fractional solution.
theorem 7, minu (u) equal value optimal primal solution, i.e., minu (u) =
2. Hence minu L(u) = 2.
contrast, solution original optimization problem 1 (y , z ) = (y3 , z3 ): fact,
(y3 , z3 ) pair structures satisfies constraints y(i, t) = z(i, t) (i, t).
Thus f (y ) + g(z ) = 0.
min L(u) = 2 > f (y ) + g(z ) = 0
u

Thus clear gap minimum dual value, score optimal
primal solution.
348

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

5

Dual

4
3
2
1
0
0

1

2

3

4

5

6

7

8

9

10

k
1
2
3
4
5
6
7
8
9

(k)
y3
y2
y1
y1
y2
y1
y2
y1
y2

z (k)
z2
z1
z1
z1
z2
z1
z2
z1
z2

Round

Figure 14: Figures showing progress subgradient algorithm f = [1, 1, 2] g =
[1, 1, 2]. graph shows dual value L(u(k) ) versus number iterations k.
table shows hypotheses (k) z (k) versus number iterations k. later iterations
method alternates hypotheses (y1 , z1 ) (y2 , z2 ).

Figure 14 shows trace subgradient method problem. nine iterations
method reached L(u(9) ) = 2.06, close optimal dual value. case, however,
algorithm reach agreement structures (k) z (k) . Instead, reaches
point alternates solutions (y (1) , z (1) ), (y (2) , z (2) ). Thus dual d-converges
minimum value, primal solutions generated alternate structures y1 , y2 , z1 , z2
greater 0 weight fractional solution (2 , 2 ). behavior typical cases
duality gap, i.e., minu L(u) strictly greater f (y ) + g(z ).
9.4 Fixing E-Convergence: Tightening Approaches
describe tightening approach used fix issue non-convergence given
previous example.
Consider problem integrated CFG parsing HMM tagging. Assume input
sentence length n. first approach follows. introduce new variables y(i, t1 , t2 )
= 1 . . . (n 1), t1 , t2 , y(i, t1 , t2 ) = 1 y(i, t1 ) = 1 y(i + 1, t2 ) = 1, 0
otherwise. Thus new variables track tag bigrams. Similarly, introduce variables z(i, t1 , t2 )
tag sequences z Z. define set constraints
y(i, t) = z(i, t)
{1 . . . n}, (the constraints before), addition
y(i, t1 , t2 ) = z(i, t1 , t2 )
{1 . . . n 1}, t1 , t2 .
proceed before, using Lagrange multipliers u(i, t) enforce first set constraints, Lagrange multipliers v(i, t1 , t2 ) enforce second set constraints. dual
349

fiRUSH & C OLLINS

decomposition algorithm require us find
(k) = argmax f (y) +

X

yY

i,t

z (k) = argmax g(z)

X

u(k1) (i, t)y(i, t) +

X

v (k1) (i, t1 , t2 )y(i, t1 , t2 )

(33)

v (k1) (i, t1 , t2 )z(i, t1 , t2 )

(34)

i,t1 ,t2


zZ

i,t

u(k1) (i, t)z(i, t)

X
i,t1 ,t2

iteration, followed updates form
u(k) (i, t) u(k1) (i, t) (y (k) (i, t) z (k) (i, t))

v (k) (i, t1 , t2 ) v (k1) (i, t1 , t2 ) (y (k) (i, t1 , t2 ) z (k) (i, t1 , t2 ))
shown g(z) defined bigram HMM model, method
guaranteed e-converge exact solution. fact, underlying LP relaxation tight,
integral solutions possible.
problem approach finding argmax Eq. 33 expensive, due
v(i, t1 , t2 )y(i, t1 , t2 ) terms: fact, requires exact dynamic programming algorithm
intersection bigram HMM PCFG. Thus end algorithm least
expensive integration bigram HMM PCFG using construction Bar-Hillel et al.
(1964).14
second approach, may efficient, follows. Rather introducing
constraints form Eq. 33, might introduce selected constraints. example,
previous non-convergent example might add single constraint
y(1, a, b) = z(1, a, b)
single Lagrange multiplier v(1, a, b) new constraint, dual decomposition
algorithm requires following steps iteration:
(k) = argmax f (y) +

X

yY

i,t

z (k) = argmax g(z)

X

u(k1) (i, t)y(i, t) + v (k1) (1, a, b)y(1, a, b)

(35)

u(k1) (i, t)z(i, t) v (k1) (1, a, b)z(1, a, b)

(36)


zZ

i,t

updates
u(k) (i, t) u(k1) (i, t) (y (k) (i, t) z (k) (i, t))

v (k) (1, a, b) v (k1) (1, a, b) (y (k) (1, a, b) z (k) (1, a, b))
Figure 15 shows run subgradient algorithm single constraint added.
fractional solution (2 , 2 ) eliminated, method e-converges correct solution.
Two natural questions arise:
14. g(z) defined bigram HMM, clearly nothing gained efficiency Bar-Hillel et al.
(1964) method. g(z) complex, example consisting trigram model, dual decomposition method
may still preferable.

350

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

5

Dual

4
3
2
1
0
7

8

9

10

11

12

13

14

15

16

k
7
8
9
10
11
12
13
14
15
16

(k)
y3
y2
y1
y3
y2
y1
y3
y2
y1
y3

z (k)
z2
z3
z2
z1
z3
z2
z1
z3
z2
z3

Round

Figure 15: Figures showing progress subgradient algorithm f (y) = [1, 1, 2] g(z) =
[1, 1, 2], additional constraint y(1, a, b) = z(1, a, b) incorporated Lagrangian. graph shows dual value L(u(k) ) versus number iterations k.
table shows hypotheses (k) z (k) versus number iterations k.

constraints added? One strategy first run subgradient method
basic constraints, shown Figure 14. heuristic used determine
dual longer decreasing significant rate. point, determined
algorithm oscillating solutions (y1 , z1 ) (y2 , z2 ), additional
constraint y(1, a, b) = z(1, a, b) would rule solutions; hence constraint added.
efficient adding constraints Eq. 33? toy example
simple illustrate benefit adding selected constraints. understand benefit,
consider case sentence length n reasonably large. case, might
add bigram constraints positions sentence: practice CKY decoding
algorithm need introduce Bar-Hillel et al. (1964) machinery selected
points, much efficient introducing constraints.
examples methods tighten dual decomposition/Lagrangian relaxation techniques using additional constraints, see work Sontag, Meltzer, Globerson, Jaakkola, Weiss (2008),
Rush Collins (2011), Chang Collins (2011), Das et al. (2012). related previous work non-projective dependency parsing (Riedel & Clarke, 2006) incrementally adds
constraints integer linear program solver.
9.5 Compact Linear Programs
LP relaxations described large set variables: is, one variable
member Z. cases interest, sets Z exponential size.
section describe derive equivalent linear programs far fewer variables.
problem practical interest: many problems, found beneficial implement
underlying LP relaxation within generic LP solver, way debugging dual decomposition
351

fiRUSH & C OLLINS

algorithms. practical compact LPs describe section, clearly
impractical exponential-size linear programs described previous section.
First, consider abstract description Lagrangian relaxation given section 3. LP
relaxation
argmax
Q


Q = {y : Conv(Y 0 ) Ay = b}

Rpd b Rp . Recall Conv(Y 0 ) convex hull set 0 . Next, assume
Conv(Y 0 ) defined polynomial number linear constraints: is,
Conv(Y 0 ) = {y Rd : Cy = e, 0}

(37)

C Rqd e Rq , number constraints, q, polynomial. case
explicit characterization set Q
Q = {y Rd : Cy = e, 0 Ay = b}
d, p, q polynomial size, resulting linear program polynomial size.
sense compact.
remaining question whether characterization form Eq. 37 exists, so,
defined. Recall made assumption value ,
argmax

(38)

yY 0

found using combinatorial algorithm. many combinatorial algorithms, LP
formulations polynomial size: formulations lead directly definitions C
e.15 example, Martin, Rardin, Campbell (1990) give construction dynamic programming algorithms, includes parsing algorithms weighted context-free grammars,
Viterbi algorithm, dynamic programs used NLP. Martins et al. (2009) make use
construction directed spanning trees (see also Magnanti & Wolsey, 1994), apply nonprojective dependency parsing. Korte Vygen (2008) describe many constructions.
short, given combinatorial algorithm solves problem Eq. 38, often straightforward
find recipe constructing pair (C, e) completely characterizes Conv(Y 0 ).
straightforward extend idea LP dual decomposition. Consider
running example, (optimization problem 1),
argmax f (y) + g(z)
yY,zZ

= 1 . . . n, ,

y(i, t) = z(i, t)

Rush et al. (2010) give full description compact LP problem: give sketch here.
15. one subtlety here: cases additional auxilliary variables may need introduced. See example spanning tree construction Magnanti Wolsey (1994). However number auxilliary variables
generally polynomial number, hence benign.

352

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

Define vector {0, 1}d specifies context-free rules contains.
follows subset {0, 1}d .
f (y) =
Rd vector specifying weight rule. Similarly, define z vector
0
{0, 1}d specifies trigrams z contains (assuming g(z) trigram tagging model).
0
follows Z subset {0, 1}d . write
g(z) = z 0
0

0 Rd . compact LP
argmax
+ 0
Conv(Y),Conv(Z)
= 1 . . . n, ,
0

(i, t) = (i, t)

Again, existence combinatorial algorithms problems argmaxyY argmaxzZ z
implies explicit representations
Conv(Y) = { Rd : = b, 0}



0

Conv(Z) = { Rd : C = e, 0}
A, b, C e polynomial size. Rush et al. (2010) describe construction detail
case weighted CFG combined finite-state tagger.
9.6 Summary
summarize, key points section follows:
introduced linear programming problem relaxation original problem.
function L(u) shown dual linear programming relaxation.
cases optimal solution underlying LP fractional, subgradient method
still d-converge minu L(u). However primal solutions (y (k) , z (k) ) alternate
different solutions satisfy y(i, t) = z(i, t) constraints.
practice, tightening methods used improve convergence. methods selectively introduce constraints effort improve convergence method, cost
increased complexity finding (k) and/or z (k) . precise constraints added
chosen identifying constraints frequently violated subgradient method.
Finally, described methods construct compact linear program equivalent
original LP relaxation. linear program often small enough solved generic
LP solver; useful debugging dual decomposition Lagrangian relaxation algorithms.
353

fiRUSH & C OLLINS

10. Conclusions
broad class inference problems statistical NLP areas machine learning
amenable Lagrangian relaxation (LR) methods. LR methods make use combinatorial algorithms combination linear constraints introduced using Lagrange multipliers: iterative methods used minimize resulting dual objective. LR algorithms simple
efficient, typically involving repeated applications underlying combinatorial algorithm,
conjunction simple additive updates Lagrange multipliers. well-understood
formal properties: dual objective upper bound score optimal primal solution;
close connections linear programming relaxations; crucially, potential
producing exact solution original inference problem, certificate optimality. Experiments several NLP problems shown effectiveness LR algorithms inference:
LR methods often considerably efficient existing exact methods, stronger
formal guarantees approximate search methods often used practice.

Acknowledgments
thank anonymous reviewers helpful comments. Tommi Jaakkola David Sontag introduced us dual decomposition Lagrangian relaxation inference probabilistic models;
work would happened without them. benefited many discussions
Yin-Wen Chang, Terry Koo, Roi Reichart, Tommi David collaborators
work dual decomposition/Lagrangian relaxation NLP. also thank Shay Cohen, Yoav
Goldberg, Mark Johnson, Andre Martins, Ryan McDonald, Slav Petrov feedback earlier
drafts paper. Columbia University gratefully acknowledges support Defense Advanced Research Projects Agency (DARPA) Machine Reading Program Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181. Alexander Rush supported
National Science Foundation Graduate Research Fellowship.

Appendix A. Proofs
section derive various results combined parsing tagging problem. Recall
case Lagrangian defined
L(u, y, z) = f (y) + g(z) +

X
i{1...n},tT

u(i, t)(y(i, t) z(i, t))

dual objective L(u) = maxyY,zZ L(u, y, z). n number words
sentence, finite set part-of-speech tags.
first prove L(u) convex function; derive expression subgradients
L(u); give convergence theorem algorithm Figure 2, subgradient
algorithm minimization L(u).
Finally, give proof theorem 6.
A.1 Proof Convexity L(u)
theorem follows:
354

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

Theorem 9 L(u) convex. is, u(1) Rd , u(2) Rd , [0, 1],
L(u(1) + (1 )u(2) ) L(u(1) ) + (1 )L(u(2) )
Proof: Define
(y , z ) = arg max L(u , y, z)
yY,zZ

u = u(1) + (1 )u(2) . follows
L(u ) = L(u , , z )
addition, note
L(u(1) , , z ) max L(u(1) , y, z) = L(u(1) )
yY,zZ

similarly
L(u(2) , , z ) L(u(2) )
follows
L(u(1) , , z ) + (1 )L(u(2) , , z ) L(u(1) ) + (1 )L(u(2) )
Finally, easy show
L(u(1) , , z ) + (1 )L(u(2) , , z ) = L(u , , z ) = L(u )
hence
L(u ) L(u(1) ) + (1 )L(u(2) )
desired result.
A.2 Subgradients L(u)
value u Rd , define
(y (u) , z (u) ) = argmax L(u, y, z)
yY,zZ

equivalently,




(u) = argmax f (y) +

X

yY



u(i, t)y(i, t)

i,t





z (u) = argmax g(z)

X

zZ

u(i, t)z(i, t)

i,t

define (u) vector components
(u) (i, t) = (u) (i, t) z (u) (i, t)
355

fiRUSH & C OLLINS

{1 . . . n}, , (u) subgradient L(u) u.
result special case following theorem:16
Theorem 10 Define function L : Rd R
L(u) = max (ai u + bi )
i{1...m}

ai Rd bi R {1 . . . m}. value u,
j = argmax (ai u + bi )
i{1...m}

aj subgradient L(u) u.
Proof: aj subgradient point u, need show v Rd ,
L(v) L(u) + aj (v u)
Equivalently, need show v Rd ,
max (ai v + bi ) max (ai u + bi ) + aj (v u)

i{1...m}

i{1...m}

(39)

show this, first note
aj u + bj = max (ai u + bi )
i{1...m}

hence
max (ai u + bi ) + aj (v u) = bj + aj v max (ai v + bi )

i{1...m}

i{1...m}

thus proving theorem.
A.3 Convergence Proof Subgradient Method
Consider convex function L : Rd R, minimizer u (i.e., u = argminuRd L(u)).
subgradient method iterative method initializes u value u(0) Rd ,
sets
u(k+1) = u(k) k gk
k = 0, 1, 2, . . ., k > 0 stepsize kth iteration, gk subgradient u(k) :
is, v Rd ,
L(v) L(u(k) ) + gk (v u(k) )
following theorem useful proving convergence method (the
theorem proof taken Boyd & Mutapcic, 2007):
16. specific, definition L(u) written form maxi{1...m} (ai u + bi ) follows. Define
integer |Y| |Z|. Define (y (i) , z (i) ) {1 . . . m} list possible pairs (y, z)
z Z. Define bi = f (y (i) ) + g(z (i) ), ai vector components ai (l, t) = (i) (l, t) z (i) (l, t)
l {1 . . . n}, . verifed L(u) = maxi{1...m} (ai u + bi ).

356

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

Theorem 11 Assume k, ||gk ||2 G2 G constant. k 0,
||u(0) u ||2 + G2
min L(u ) L(u ) +
P
i{0...k}
2 ki=0


(i)

Pk

2
i=0

Proof: First, given updates u(k+1) = u(k) k gk , k 0,
||u(k+1) u ||2 = ||u(k) k gk u ||2

= ||u(k) u ||2 2k gk (u(k) u ) + k2 ||gk ||2

subgradient property,
L(u ) L(u(k) ) + gk (u u(k) )
hence
gk (u(k) u ) L(u ) L(u(k) )

Using inequality, together ||gk ||2 G2 , gives





||u(k+1) u ||2 ||u(k) u ||2 + 2k L(u ) L(u(k) ) + k2 G2

Taking sum sides = 0 . . . k gives
k
X
i=0

k
X

||u(i+1) u ||2

i=0

||u(i) u ||2 + 2

k
X
i=0



k
X



L(u ) L(u(i) ) +

i2 G2

i=0

hence
||u(k+1) u ||2 ||u(0) u ||2 + 2
Finally, using

||u(k+1)
k
X
i=0




u ||2

0



(i)



L(u ) L(u )

k
X
i=0

k
X





L(u ) L(u(i) ) +

k
X
i=0

!



i=0

i2 G2

!


(i)

L(u ) min L(u )
i{0...k}

gives
(0)

0 ||u

2

u || + 2

k
X

!



i=0

!


(i)

L(u ) min L(u ) +
i{0...k}

k
X

i2 G2

i=0

Rearranging terms gives result theorem.
theorem number consequences. one example, constant step-size, k = h
h > 0,
!
P
||u(0) u ||2 + G2 ki=1 i2
Gh
lim
=
Pk
k
2
2 i=1
hence limit value
min L(u(i) )
i{1...k}

within Gh/2 optimal solution. slightly involved argument shows
P
assumptions k > 0, limk k = 0,
k=0 k = ,
lim

k

||u(0) u ||2 + G2
P
2 ki=1

See Boyd Mutapcic full derivation.
357

Pk

2
i=1

!

=0

fiRUSH & C OLLINS

A.4 Proof Theorem 6
Recall goal prove
X

max f (y) = max
yY



f (y)

yY

show proving: (1) maxyY f (y) maxy yY f (y), (2) maxyY f (y)
P
maxy yY f (y).
First, consider case (1). Define member
P

f (y ) = max f (y)
yY

Next, define = 1, = 0 6= .
X

f (y) = f (y )

yY

Hence found setting variables
X

f (y) = max f (y)
yY

yY

follows
max



X
yY

f (y) max f (y)
yY

Next, consider case (2). Define setting variables
X

f (y) = max

X





f (y)

yY

Next, define member
f (y ) = max
f (y)

y:y >0

easily verified
f (y )

X

f (y)



Hence found
f (y ) max



X

f (y)



follows
max f (y) max
yY



358

X


f (y)

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

References
Auli, M., & Lopez, A. (2011). comparison loopy belief propagation dual decomposition
integrated ccg supertagging parsing. Proceedings 49th Annual Meeting
Association Computational Linguistics: Human Language Technologies, pp. 470480,
Portland, Oregon, USA. Association Computational Linguistics.
Bar-Hillel, Y., Perles, M., & Shamir, E. (1964). formal properties simple phrase structure
grammars. Language Information: Selected Essays Theory Application,
pp. 116150.
Boyd, S., Parikh, N., Chu, E., Peleato, B., & Eckstein, J. (2011). Distributed optimization
statistical learning via alternating direction method multipliers. Publishers.
Boyd, S., & Vandenberghe, L. (2004). Convex optimization. Cambridge Univ Pr.
Boyd, S., & Mutapcic, A. (2007). Subgradient Methods. Course Notes EE364b, Stanford
University, Winter 2006-07.
Chang, Y., & Collins, M. (2011). Exact Decoding Phrase-based Translation Models
Lagrangian Relaxation. appear proc. EMNLP.
Collins, M. (1997). Three Generative, Lexicalised Models Statistical Parsing. Proc. ACL, pp.
1623.
Dantzig, G., & Wolfe, P. (1960). Decomposition principle linear programs. Operations
research, Vol. 8, pp. 101111.
Das, D., Martins, A., & Smith, N. (2012). exact dual decomposition algorithm shallow
semantic parsing constraints. Proceedings of* SEM.[ii, 10, 50].
DeNero, J., & Macherey, K. (2011). Model-Based Aligner Combination Using Dual Decomposition. Proc. ACL.
Duchi, J., Tarlow, D., Elidan, G., & Koller, D. (2007). Using combinatorial optimization within maxproduct belief propagation. Advances Neural Information Processing Systems (NIPS).
Everett III, H. (1963). Generalized lagrange multiplier method solving problems optimum
allocation resources. Operations Research, 399417.
Felzenszwalb, P., & Huttenlocher, D. (2006). Efficient belief propagation early vision. International journal computer vision, 70(1), 4154.
Fisher, M. L. (1981). lagrangian relaxation method solving integer programming problems.
Management Science, 27(1), pp. 118.
Germann, U., Jahr, M., Knight, K., Marcu, D., & Yamada, K. (2001). Fast decoding optimal
decoding machine translation. Proceedings 39th Annual Meeting Association
Computational Linguistics, pp. 228235. Association Computational Linguistics.
Globerson, A., & Jaakkola, T. (2007). Fixing max-product: Convergent message passing algorithms
map lp-relaxations. NIPS.
Hanamoto, A., Matsuzaki, T., & Tsujii, J. (2012). Coordination structure analysis using dual decomposition. Proceedings 13th Conference European Chapter Association
Computational Linguistics, pp. 430438, Avignon, France. Association Computational
Linguistics.
359

fiRUSH & C OLLINS

Held, M., & Karp, R. M. (1971). traveling-salesman problem minimum spanning trees:
Part ii. Mathematical Programming, 1, 625. 10.1007/BF01584070.
Johnson, J., Malioutov, D., & Willsky, A. (2007). Lagrangian relaxation map estimation
graphical models. 45th Annual Allerton Conference Communication, Control Computing.
Jojic, V., Gould, S., & Koller, D. (2010). Fast smooth: Accelerated dual decomposition
MAP inference. Proceedings International Conference Machine Learning (ICML).
Klein, D., & Manning, C. (2002). Fast exact inference factored model natural language
parsing. Advances neural information processing systems, 15(2002).
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. Proceedings
2003 Conference North American Chapter Association Computational
Linguistic Human Language Technology, NAACL 03, pp. 4854.
Kolmogorov, V. (2006). Convergent tree-reweighted message passing energy minimization.
Pattern Analysis Machine Intelligence, IEEE Transactions on, 28(10), 15681583.
Komodakis, N., Paragios, N., & Tziritas, G. (2007). MRF Optimization via Dual Decomposition:
Message-Passing Revisited. Proc. ICCV.
Komodakis, N., Paragios, N., & Tziritas, G. (2011). Mrf energy minimization beyond via dual
decomposition. Pattern Analysis Machine Intelligence, IEEE Transactions on, pp. 11.
Koo, T., Carreras, X., & Collins, M. (2008). Simple semi-supervised dependency parsing. Proc.
ACL/HLT.
Koo, T., Rush, A. M., Collins, M., Jaakkola, T., & Sontag, D. (2010). Dual decomposition
parsing non-projective head automata. EMNLP.
Korte, B., & Vygen, J. (2008). Combinatorial Optimization: Theory Algorithms. Springer
Verlag.
Lacoste-Julien, S., Taskar, B., Klein, D., & Jordan, M. (2006). Word alignment via quadratic assignment. Proceedings main conference Human Language Technology Conference
North American Chapter Association Computational Linguistics, pp. 112119.
Association Computational Linguistics.
Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional Random Fields: Probabilistic Models
Segmenting Labeling Sequence Data. Proc. ICML, pp. 282289.
Lemarechal, C. (2001). Lagrangian Relaxation. Computational Combinatorial Optimization, Optimal Provably Near-Optimal Solutions [based Spring School], pp. 112156, London,
UK. Springer-Verlag.
Magnanti, T. L., & Wolsey, L. A. (1994). Optimal trees. Tech. rep. 290-94, Massachusetts Institute
Technology, Operations Research Center.
Martin, R., Rardin, R., & Campbell, B. (1990). Polyhedral characterization discrete dynamic
programming. Operations research, 38(1), 127138.
Martins, A. (2012). Geometry Constrained Structured Prediction: Applications Inference
Learning Natural Language Syntax. Ph.D. thesis.
360

fiA UTORIAL UAL ECOMPOSITION NATURAL L ANGUAGE P ROCESSING

Martins, A., Figueiredo, M., Aguiar, P., Smith, N., & Xing, E. (2011). augmented lagrangian
approach constrained map inference. International Conference Machine Learning.
Martins, A., Smith, N., & Xing, E. (2009). Concise Integer Linear Programming Formulations
Dependency Parsing. Proc. ACL, pp. 342350.
Martins, A., Smith, N., Figueiredo, M., & Aguiar, P. (2011). Dual decomposition many overlapping components. Proceedings 2011 Conference Empirical Methods Natural
Language Processing, pp. 238249, Edinburgh, Scotland, UK. Association Computational
Linguistics.
McDonald, R. (2006). Discriminative Training Spanning Tree Algorithms Dependency
Parsing. Ph.D. thesis, University Pennsylvania, Philadelphia, PA, USA.
Meshi, O., & Globerson, A. (2011). alternating direction method dual map lp relaxation.
Machine Learning Knowledge Discovery Databases, 470483.
Nedic, A., & Ozdaglar, A. (2009). Approximate primal solutions rate analysis dual subgradient methods. SIAM Journal Optimization, 19(4), 17571780.
Nesterov, Y. (2005). Smooth minimization non-smooth functions. Mathematical Programming,
103(1), 127152.
Paul, M. J., & Eisner, J. (2012). Implicitly intersecting weighted automata using dual decomposition. Proc. NAACL.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference
(2nd edition). Morgan Kaufmann Publishers.
Riedel, S., & Clarke, J. (2006). Incremental integer linear programming non-projective dependency parsing. Proceedings 2006 Conference Empirical Methods Natural
Language Processing, pp. 129137. Association Computational Linguistics.
Riedel, S., & McCallum, A. (2011). Fast robust joint models biomedical event extraction.
Proceedings 2011 Conference Empirical Methods Natural Language Processing,
pp. 112, Edinburgh, Scotland, UK. Association Computational Linguistics.
Roth, D., & Yih, W. (2005). Integer linear programming inference conditional random fields.
Proceedings 22nd international conference Machine learning, pp. 736743. ACM.
Rush, A., Reichart, R., Collins, M., & Globerson, A. (2012). Improved parsing pos tagging
using inter-sentence consistency constraints. Proceedings 2012 Joint Conference
Empirical Methods Natural Language Processing Computational Natural Language
Learning, pp. 14341444, Jeju Island, Korea. Association Computational Linguistics.
Rush, A., & Collins, M. (2011). Exact Decoding Syntactic Translation Models Lagrangian Relaxation. Proc. ACL.
Rush, A., Sontag, D., Collins, M., & Jaakkola, T. (2010). Dual Decomposition Linear
Programming Relaxations Natural Language Processing. Proc. EMNLP.
Shor, N. Z. (1985). Minimization Methods Non-differentiable Functions. Springer Series
Computational Mathematics. Springer.
Smith, D., & Eisner, J. (2008). Dependency parsing belief propagation. Proc. EMNLP, pp.
145156.
361

fiRUSH & C OLLINS

Sontag, D., Globerson, A., & Jaakkola, T. (2010). Introduction dual decomposition inference.
Sra, S., Nowozin, S., & Wright, S. J. (Eds.), Optimization Machine Learning. MIT
Press.
Sontag, D., Meltzer, T., Globerson, A., Jaakkola, T., & Weiss, Y. (2008). Tightening LP relaxations
MAP using message passing. Proc. UAI.
Toutanova, K., Klein, D., Manning, C. D., & Singer, Y. (2003). Feature-rich part-of-speech tagging
cyclic dependency network. HLT-NAACL.
Wainwright, M., Jaakkola, T., & Willsky, A. (2005). MAP estimation via agreement trees:
message-passing linear programming.. IEEE Transactions Information Theory,
Vol. 51, pp. 36973717.
Yanover, C., Meltzer, T., & Weiss, Y. (2006). Linear Programming Relaxations Belief
PropagationAn Empirical Study. Journal Machine Learning Research, Vol. 7,
p. 1907. MIT Press.

362

fiJournal Artificial Intelligence Research 45 (2012) 79-124

Submitted 03/12; published 09/12

Approximative Inference Method Solving
Satisfiability Problems
Hanne Vlaeminck
Joost Vennekens
Marc Denecker
Maurice Bruynooghe

hanne.vlaeminck@cs.kuleuven.be
joost.vennekens@cs.kuleuven.be
marc.denecker@cs.kuleuven.be
maurice.bruynooghe@cs.kuleuven.be

Department Computer Science,
Celestijnenlaan 200a
3001 Heverlee, Belgium

Abstract
paper considers fragment second-order logic. Many interesting problems, conformant planning, naturally expressed finite domain satisfiability problems logic. satisfiability problems computationally hard (P
2 )
many problems often solved approximately. paper, develop general
approximative method, i.e., sound incomplete method, solving satisfiability
problems. use syntactic representation constraint propagation method firstorder logic transform satisfiability problem SO(ID) satisfiability
problem (second-order logic, extended inductive definitions). finite domain satisfiability problem latter language NP handled several existing
solvers. Inductive definitions powerful knowledge representation tool, motivates us also approximate SO(ID) problems. order this, first show
perform propagation inductive definitions. Next, use approximate
SO(ID) satisfiability problems. provides general theoretical framework
number approximative methods literature. Moreover, also show
use framework solving practical useful problems, conformant planning,
effective way.

1. Introduction
Finite model generation logical paradigm solving constraint problems. successful
instance field SAT, efficient solvers low level CNF language developed. instances, expressive languages, Answer Set Programming
(ASP) (Baral, 2003) model expansion (MX) (extensions of) first order logic. ASP,
e.g., finite Herbrand models answer set program computed (Baral, 2003). Model
expansion (MX) (Mitchell & Ternovska, 2005) generalizes Herbrand model generation
aims computing one models theory expand finite interpretation I0
(possibly empty) subset symbols . MX first order logic (MX(FO)) formally
equivalent finite domain satisfiability checking problem existential second-order
logic (SAT (SO))1 known Fagins celebrated theorem capture NP (Fagin,
1974). is, problems NP exactly precise sense equivalent
satisfiability problem, hence MX(FO) problem. range solvers exists
1. specifically, search problem witness problem.
c
2012
AI Access Foundation. rights reserved.

fiVlaeminck, Vennekens, Denecker & Bruynooghe

finite model generation, e.g., overview state-of-the-art ASP MX(FO()) solvers
(here, FO() refers family extensions FO) found reports
ASP competition (e.g., Denecker, Vennekens, Bond, Gebser, & Truszczynski, 2009).
Example 1.1. bounded planning problem modeled Finite Model Generation
problem. problem deliberately kept simple serve running example
paper: glass may clean not, cleaned action wiping.
represent dynamic domain following FO theory Tact :
: (Clean(t + 1) Clean(t) W ipe(t)).
Clean(0) InitiallyClean.

(1)
(2)

bounded planning problem considering turn dirty glass clean
one n steps, n N given constant. indeed formulated
Model Expansion problem: find model satisfies Tact InitiallyClean false,
Clean(n) true. formulate problem equivalently finite domain
satisfiability problem, namely satisfiability problem range [0 . . . n] time
points following formula:
W ipe, Clean, InitiallyClean : (act InitiallyClean Clean(n)),

(3)

act denote conjunction sentences Tact . n > 0, formula
indeed satisfiable suitable interpretation constants 0 n binary function
+, and, moreover, witness W satisfiability provides plan. instance, wiping
time point 0 job, verified witness W W ipeW = {0}
CleanW = {1, . . . , n}.
large number search problems indeed seen Finite Model Generation
problems, also number problems higher complexity NP,
consequently cannot formulated MX(FO) problem. Indeed, paper
interested NP, next level P2 polynomial hierarchy. Perhaps
prototypical problem finite domain satisfiability SO: satisfaction finite
interpretations P2 every sentence P2 -hard sentences
(Immerman, 1998). interesting P2 problem conformant planning,
discuss detail Section 7, already introduce next example.
Example 1.2. Extending Example 1.1, suppose know whether object
initially clean dirty, still want plan guaranteed make clean, matter
initial situation was. longer standard planning problem, called
conformant planning problem. formulate following satisfiability
problem:
W ipe InitiallyClean, Clean : (act Clean(n)).
(4)
words, need assignment action W ipe goal Clean(n) satisfied
every initial situation InitiallyClean fluent Clean satisfy action theory.
Solving problems like would require us make choice existentially quantified predicates check implication satisfied every interpretation
80

fiAn Approximative Inference Method

universally quantified predicates. done principle, practice
often expensive. paper, explore alternative based propagation
method first order logic developed Wittocx, Denecker, Bruynooghe (2010).
method computes polynomial time, given theory partial interpretation I,
approximation certainly true (= true models expand
I) certainly false (= false models). Now, interesting property
propagation method syntactically represented monotone inductive
definition (Denecker & Ternovska, 2008) defines (in approximative way) underestimates predicates complements. monotone inductive
definition essentially set propagation rules, similar definite logic program,
interpreted least fixpoint immediate consequence operator.
given theory obtain approximating definition linear transformation
original FO formula.
Returning example, need find interpretation action predicates, every interpretation predicates, implication act
Clean(n) satisfied, i.e., without knowing anything predicates, already certain implication satisfied. basic idea behind
method approximate problem form P Q , using approximate definition Wittocx et al. check whether interpretation existentially
quantified predicates P property making true, regardless predicates Q.
Essentially, reduces problem SO(ID) problem (where SO(ID)
refer extended inductive definitions).
Section 5, extend method SO(ID) problems. argued Denecker
Ternovska, inductive definitions useful tool knowledge representation.
example, many dynamic domains formulated naturally modular way using
inductive definitions, quite tedious FO. already mentioned conformant planning typical satisfiability problem. Typically, conformant
planning problems require modeling dynamic domain. come back
syntax semantics inductive definitions next section, dynamic domain
Example 1.1 can, alternative action theory Tact , formulated following
inductive definition act :


Clean(t + 1) Clean(t).

Clean(t + 1) W ipe(t).
(5)


Clean(0)
InitiallyClean.
conformant planning problem formulated alternatively satisfiability problem formula W ipe InitiallyClean, Clean : (act Clean(n)). However,
longer satisfiability problem, SO(ID) satisfiability problem.
motivates us see extend approximation method SO(ID)
satisfiability problems. purpose, first show symbolically represent propagation inductive definitions. Next, show use together
representation propagation FO approximate finite domain SO(ID) satisfiability
problems.
approximation method number benefits. First all, general method,
applied automatically approximately solve problem. Second,
81

fiVlaeminck, Vennekens, Denecker & Bruynooghe

required computation carried off-the-shelf MX(FO(ID)) solver, allowing
method benefit effortlessly improvements solver technology,
IDP system Marien, Wittocx, Denecker (2006). Finally, show Section 7,
method elegantly generalizes number approximate reasoning methods
literature (e.g., Baral, Gelfond, & Kosheleva, 1998; Son, Tu, Gelfond, & Morales, 2005;
Denecker, Cortes Calabuig, Bruynooghe, & Arieli, 2010; Doherty, Magnusson, & Szalas,
2006; Son et al., 2005).
Parts work already presented JELIA 2011 conference (Vlaeminck, Wittocx, Vennekens, Denecker, & Bruynooghe, 2010).

2. Preliminaries
assume familiarity classical first-order logic (FO) second-order logic (SO).
section introduce notations conventions used throughout paper.
2.1 First-order Logic
vocabulary finite set predicate symbols P function symbols F ,
associated arity. Constants function symbols arity 0. often denote symbol
arity n S/n. interpretation consists domain assignment
relation P Dn predicate symbol P/n assignment function
F : Dn function symbol F/n . assume P contains equality
predicate = interpreted identity relation. pre-interpretation consists
domain interpretation function symbols. -interpretation 0 ,
denote I|0 restriction symbols 0 . 1 2 two disjoint
vocabularies, 1 -interpretation domain J 2 -interpretation
domain, + J denotes unique (1 2 )-interpretation domain
(I + J)|1 = (I + J)|2 = J.
Terms formulas vocabulary defined usual. expression form
P n-ary predicate Dn called domain atom. domain literal
P (d)
negation P (d)
thereof. usual, denote formula
domain atom P (d)
[x] indicate set free variables subset x. formula without
free variables called sentence. satisfaction relation |= defined usual.

interpretation I, formula n free variables tuple n domain elements d,


use |= [d] shorthand I, [x : d] |= [x], variable assignment,
variable assignment except maps variables
[x : d]
define truth evaluation function ([d])
follows:
x domain elements d.





([d]) = iff |= [d] ([d]) = f otherwise. say formula negation
normal form contains implications equivalences, negations occur directly
front atoms. define inverse truth values follows: (f )1 = (t)1 = f .
also define following strict order truth values: f <t t. truth order
point-wise extends interpretations: J two -interpretations, say
J every predicate symbol P tuple domain elements holds
P J (d).

P (d)
Similar real number r approximated interval [l, u] l r
u, paper approximate -interpretations K pair (I, J) -interpretations,
82

fiAn Approximative Inference Method

K J. denote [I, J] interval interpretations K.
interval empty 6t J. follows easily well-known monotonicity
results, evaluate positive occurrences (i.e., scope even number
negations) atoms formula I, negative occurrences (i.e., scope
odd number negations) J, underestimating truth interval
[I, J]. Conversely, evaluate positive occurrences J negative occurrences I,
overestimating truth [I, J]. state property formally,
introduce following notation.
Definition 2.1 (Pos-neg evaluation relation +IJ ). Let -formula let J
-interpretations. define pos-neg evaluation J, denoted +IJ ,
induction size :
atom = P (t), +IJ = ;
= , +IJ = ( +JI )1 ;
= 1 2 , +IJ = iff i+IJ = = 1, 2;
= x , +IJ = iff +I[x/d]J[x/d] = t.
indeed that, K [I, J], +IJ K +JI . Also,
K = +KK .
intimate connection approximation interpretation
pair interpretations Belnaps four-valued logic (1977). denote truth values
true, false, unknown inconsistent four-valued logic respectively t, f , u i.
truth values, truth order precision order p defined shown Figure
1.
four-valued relation arity n domain function Dn {t, f , u, i}.
four-valued interpretation vocabulary consists pre-interpretation P ,
four-valued relation arity n predicate symbol P/n . Again,
precision order pointwise extends interpretations: J two -interpretations,
say p J every predicate symbol P tuple domain elements
p P J (d).
Similarly, also truth order extended interpretations.
holds P (d)
@t^
<t

<p

<t

u^

@i

<p

f_

<t

<t

@i^

?t
<p

<p

u

f

Figure 1: truth precision order
83

fiVlaeminck, Vennekens, Denecker & Bruynooghe

natural isomorphism Belnaps four truth values pairs two
standard truth values:
(t, t) = t;
(f , t) = u;
(t, f ) = i;
(f , f ) = f .
Intuitively, mapping interprets first argument underestimate real
truth value, second argument overestimate: underestimate f
overestimate t, real truth value indeed unknown; whereas, underestimate
overestimate f , cannot exist real truth value, since 6 f ,
end inconsistency. isomorphism extends obvious way isomorphism
pairs (I, J) two-valued interpretations four-valued interpretations
share pre-interpretations: (I, J) isomorphic iff, predicate
= (P (d),
P J (d)).
also denote isomorphism .
P/n tuple Dn , P (d)
tight link pos-neg evaluation function +IJ Belnap
evaluation :
= (+IJ , +JI ), (I, J) = I.
three-valued structure (i.e., never assigns i) corresponds standard
Kleene evaluation (1952). rest paper, often omit isomorphism ,
and, e.g., simply denote four-valued truth value formula pair interpretations
(I, J) (I,J) . important property, already stated different notation,
(I,J) p K K [I, J].
natural well-known alternative way using interval [I, J]
J assign truth value formula : supervaluation (van Fraassen, 1966).
Definition 2.2 (Supervaluation sv(I,J) (.)). supervaluation sv(I,J) () sentence
pair interpretations (I, J) (or equivalently, three-valued interpretation (I, J))
defined
sv(I,J) () = glbp ({K |K [I, J]}).
easy see always sv(I,J) () p (I,J) . inequality may strict.
instance, take = Q Q interpretations J Q(I,J) = u,
sv(I,J) () = t, (I,J) = u. supervaluation following interesting property.
Let interpretation free vocabulary formula = Q , let (J1 , J2 )
=u
least precise pair interpretations Q domain (i.e., Q(J1 ,J2 ) (d)
n

Q/n Q ). sv(I+J1 ,I+J2 ) () = = t.
Key approach simulate four-valued truth evaluation pairs
interpretations encoding certainly true certainly false, using single
two-valued structure tf new vocabulary tf . show next section,
gives us convenient vocabulary syntactically represent construction
approximation. new vocabulary tf contains function symbols F and,
predicate P P , two symbols P ct P cf . interpretations P ct P cf
certainly true
tf contain, respectively, tuples P (d)
certainly false. Formally, vocabulary four-valued -interpretation
84

fiAn Approximative Inference Method

= (I, J), tf -interpretation tf pre-interpretation I, defined
by:
(P ct )I

tf

(d)
p t} = P ,
= {d|P

(P cf )I

tf

(d)
p f } = Dn \ P J .
= {d|P
tf

tf

interpretation three-valued iff (P ct )I (P cf )I disjoint P .
tf
tf
two-valued iff (P ct )I (P cf )I others complement Dn . Also, p J ,
tf
tf
tf
tf
then, P , (P ct )I (P ct )J (P cf )I (P cf )J .
Definition 2.3 (ct cf ). given -formula [x], let ct [x] tf -formula
obtained first reducing negation normal form replacing occurrences
positive literals P (t) P ct (t) negative literals P (t) P cf (t), let cf [x]
formula ([x])ct .
interesting property formulas ct cf contain negations.
Also, following proposition well-known.
Proposition 2.1 (Feferman, 1984). every -formula interpretation I, holds
p [d]
+IJ = (ct [d])
tf = t. Also, [d]
p f
[d]
tf
+JI
cf



[d]
= f ( [d])
= t.
2.2 FO(ID)
subsection recall FO(ID) (Denecker & Ternovska, 2008), extension FO
construct respresent common forms inductive definitions,
monotone induction, induction well-founded order iterated induction.
illustrated Denecker Ternovska, FO(ID) used represent different sorts
common knowledge, temporal dynamic domains, closed world assumption, defaults, causality, etc. paper, use definitions symbolically represent
propagation, FO formulas, already mentioned introduction, also
propagation inductive definitions themselves.
definitional rule vocabulary expression form x P (t)
P (t) atomic formula FO formula. symbol new connective, called
definitional implication, distinguished FO material implication symbol
(or converse ). definition finite set definitional rules. predicate
symbol P head rule called defined predicate; predicate
function symbols called open symbols parameters definition;
set defined predicates denoted Def (), remaining symbols Open() (note
Open() therefore also includes F ).
Given interpretation open predicates, definition unique model,
constructed firing rules appropriate order. defining
formally, first consider example.
Example 2.1. Reachability graph expressible FO. is, FO
formula vocabulary consisting two predicates Edge/2 Reach/2
model , (d1 , d2 ) ReachM iff non-empty path d1 d2
85

fiVlaeminck, Vennekens, Denecker & Bruynooghe

graph represented EdgeM . represent reachability inductive definition
however. following definition defines predicate Reach terms open predicate
Edge.
(
)
xy Reach(x, y) Edge(x, y).
xy Reach(x, y) z(Reach(x, z) Reach(z, y)).
). definition given Open()-interpretation O,
Definition 2.4 (Operator
two-valued Def ()-interpretations
define immediate consequence operator

(I) = J iff defined predicate P/n tuple Dn , holds
= iff exists rule x P (t) [x], t(O+I) = (O+I) [d]
= t.
P J (d)

model positive definition (i.e., defined predicates occur negatively
body rules) defined least fixpoint immediate consequence operator.
use odI|Open() () denote model definition extending restriction
open predicates function symbols . open predicates,
omit subscript simply use od(). postpone going detail
construct model general (non-monotonic) inductive definition Section
5. next two sections, use positive definitions.
FO(ID) formulas inductively defined rules standard FO formulas,
augmented one extra case:
definition FO(ID) formula (over )).
Note rule bodies contain definitions, rules occur inside definitions
FO(ID) formulas whereas definitions used FO(ID) formulas
anywhere atoms used.
define satisfaction relation |= FO(ID) using standard inductive
rules FO, augmented one extra rule:
|= = odI|Open() (),
on, assume without loss generality definition , holds
every defined predicate P Def () defined exactly one rule, denoted x(P (x)
P [x]). Indeed, definition brought form process similar
predicate completion. transformation consists first transforming rules form
x(P (t) ) equivalent rules y(P (y) x(y = )). Next, one merges rules
form x(P (x) [x]) x(P (x) 1 [x] . . . n [x]).

3. Propagation FO
section give general, symbolic representation propagation first-order logic.
this, base work Wittocx et al. (2010). come back
precise relation material presented section work end
section.
Suppose FO theory vocabulary , pre-interpretation ,
finite three-valued interpretation represents (incomplete) knowledge
86

fiAn Approximative Inference Method

(Clean(t + 1) (Clean(t) W ipe(t))).


A1
Act
1

Clean(t + 1) (Clean(t) W ipe(t))
= 3

Clean(t + 1)
f = 3

A2 (t)
Act
2 (3)

Clean(t) W ipe(t)
= 3

Clean(t)
= 3

A3 (t)
Act
3 (3)

Clean(t + 1)
Cleancf (4)

W ipe(t)
= 3

Clean(t)
Cleancf (3)

W ipe(t)
W ipecf (3)

Figure 2: Propagation FO.
predicates . would like know implications knowledge, assuming
theory satisfied context I. find out, look set
models complete three-valued interpretation, i.e., = {M | |=
p }. Given partial information I, everything true
must certainly true according , everything false must
certainly false according . words, information allows us
derive captured greatest lower bound G = glbp M.
general, computing greatest lower bound may expensive (the data complexity P2 ) practical use. However, may still achieve useful results
computing approximation p p G. compute
approximation propagating three-valued interpretation parse tree
. illustrate following example.
Example 3.1. Consider sentence : Clean(t + 1) Clean(t) W ipe(t). Rewriting
negation normal form, becomes:
Clean(t + 1) (Clean(t) W ipe(t)).
Now, assume satisfied, know Clean false timepoint 4.
knowledge satisfied, immediately follows that, timepoints t,
disjunctive formula Clean(t + 1) (Clean(t) W ipe(t)) satisfied. Using fact
Clean false timepoint 4, deduce conjunction (Clean(t)
W ipe(t)) true timepoint 3. Therefore, models Clean false
timepoint 4, W ipe Clean false timepoint 3. reasoning process
illustrated left part Figure 2.
construct symbolic representation propagation process. First,
introduce additional vocabulary Aux refer different nodes parse
tree process operates. use additional vocabulary transform
FO formula equivalence normal form formula. similar Tseitin
transformation (1968) propositional logic.
Definition 3.1 (EN F ()). FO formula negation normal form, introduce
new predicate symbol arity n non-literal subformula [x] n
87

fiVlaeminck, Vennekens, Denecker & Bruynooghe

free variables. denote set new predicates Aux(). new
predicate symbols defined formula Eq(A ) follows. make notation simpler
assume 1 , . . . , n non-literal subformula. definitions analogous
whenever literal, instead Ai literal used body
definition.
[x] subformula form 1 [x1 ] 2 [x2 ] . . . n [xn ], Eq(A )
x (A (x) A1 (x1 ) A2 (x2 ) . . . (xn )).
[x] subformula form 1 [x1 ] 2 [x2 ] . . . n [xn ], Eq(A )
x (A (x) A1 (x1 ) A2 (x2 ) . . . (xn )).
[x] subformula form 1 [x, y], Eq(A ) equals x (A (x)
A1 (x, y)).
[x] subformula form 1 [x, y], Eq(A ) equals x (A (x)
A1 (x, y)).
define equivalence normal form set Eq(A ), denote
EN F ().
Example 3.2. According definition, EN F () theory Example 3.1 is:
A1 A2 (t).
A2 (t) Clean(t + 1) A3 (t).
A3 (t) Clean(t) W ipe(t).
illustrated right side Figure 2.
Using auxiliary vocabulary, write propagations shown
Figure 2 following implications.
A1
A2 (3) Clean(4)
A3 (3)
A3 (3)






A2 (3).
A3 (3).
Clean(3).
W ipe(3).

Note rules top-down rules, is, implications propagate information
subformula parse tree, component subformula (possibly
using also information components subformula, implication
A2 (3)Clean(4) A3 (3)). general, also bottom-up propagations course possible.
instance, Clean(4) could derive A2 (3). every predicate , derive
Eq(A ) set implications 1 2 , propagation corresponds
deriving consequent 2 antecedent 1 (so, different implications
logically equivalent). defined Table 1. last column table indicates
whether rule top-down (TD) bottom-up (BU).
Definition 3.2 (IN F ()). Given equivalence EN F () certain formula ,
denote Imp() set implications obtained Table 1.
define implication normal form , denoted F (), follows: F () = EN F () Imp().
88

fiAn Approximative Inference Method

= Eq(A )

Imp()

x (L L1 . . . Ln ).

x
x
x
x

(L1 . . . Ln L).
(Li L).
(L Li ).
(L L1 . . . Li1 Li+1 . . . Ln Li ).

1in
1in
1in

BU
BU
TD
TD

x (L L1 . . . Ln ).

x
x
x
x

(L1 . . . Ln L).
(Li L).
1in
(L Li ).
1in
(L L1 . . . Li1 Li+1 . . . Ln Li ). 1 n

BU
BU
TD
TD

x (L[x] L0 [x, y]).

x ((y L0 [x, y]) L[x]).
x(y L0 [x, y]) L[x]).
xy (L[x] L0 [x, y]).
xy ((L[x] z (y 6= z L0 [x, y][y/z])) L0 [x, y]).

BU
BU
TD
TD

x (L[x] L0 [x, y]).

x ((y L0 [x, y]) L[x]).
x(y L0 [x, y]) L[x]).
xy (L[x] L0 [x, y]).
xy ((L[x] z (y 6= z L0 [x, y][y/z])) L0 [x, y]).

BU
BU
TD
TD

Table 1: ENF INF

work Wittocx et al. (2010) proven models models F ()
true correspond, sense restriction model F ()
also model , vice versa, every model extended model
F () . implications form core approximation method.
approximation could made complete adding implications
F (), definition tries strike balance completeness ease
automatically deriving implications.
Example 3.3. three formulas EN F () Example 3.2, following
table shows corresponding set implications Imp(). complete theory F ()
consists union three sets.
A1 A2 (t).
(t A2 (t)) A1 .
(t A2 (t)) A1 .
(A1 A2 (t)).
((A1 t0 (t 6= t0
A2 (t0 ))) A2 (t)).

(A2 (t) Clean(t + 1) A3 (t)).
(Clean(t + 1) A3 (t) A2 (t)).
(Clean(t + 1) A2 (t)).
(A3 (t) A2 (t)).
(A2 (t) Clean(t + 1)).
(A2 (t) A3 (t)).
(A2 (t) A3 (t) Clean(t + 1)).
(A2 (t) Clean(t + 1) A3 (t)).

(A3 (t) (Clean(t) W ipe(t))).
(Clean(t) W ipe(t) A3 (t)).
(Clean(t) A3 (t)).
(W ipe(t) A3 (t)).
(A3 (t) Clean(t)).
(A3 (t) W ipe(t)).
(A3 (t) W ipe(t) Clean(t)).
(A3 (t) Clean(t) W ipe(t)).

reader verify four implications representing propagation Example 3.1
indeed belong F ().
propagation process Example 3.1 described least fixpointcomputation, apply implications (i.e., infer head body
already inferred), longer infer new information. represent
fixpoint computation inductive definition syntax FO(ID). However,
two complications.
89

fiVlaeminck, Vennekens, Denecker & Bruynooghe

First, paper, always need implications F (). Indeed,
typically subset symbols already know
know. conformant planning example, instance, case
existentially quantified predicate W ipe/2, simply use model expansion
system guess complete interpretation predicate. job propagation
process figure consequences particular guess. this, implications predicate (i.e., W ipe/2) head obviously needed.
Second, fixpoint computation needs infer atoms true also
false. However, syntax FO(ID) allow negative literals
heads rules. Therefore, definition contain rules predicates P
original vocabulary head, instead use predicates P ct P cf
tf -vocabulary. Since need rules fully known predicates head,
introduce P ct P cf predicates P \ .
ct
ct
given formula , therefore define ct
formula (see Definition 2.3) P
cf
replaced P P P every predicate P .
Definition 3.3 (Approx ()). formula , define Approx ()
inductive definition contains, every sentence x ( L[x]) F () L
ct
literal predicate , definitional rule x(L[x]ct
). also define
TD
ApproxBU
() (and Approx ()) way Approx (), containing
definitional rules coming bottom-up (respectively, top-down) rules F ().
often assume without loss generality = . Whenever case
drop use Approx() rather Approx (), denote approximative
definition.
Example 3.4. Using implications F () Example 3.3, obtain definition
shown Figure 3 Approx(). take = {W ipe}, get definition
Approx () Figure 3, apart last seven definitional rules replaced
following five definitional rules.


..




.






ct
cf



(t)

Clean
(t)

W
ipe(t).


3


cf

ct
A3 (t)
Clean (t).


W ipe(t).
Acf

3 (t)




cf
ct




Clean
(t)


(t).
3






cf
ct
Clean (t) A3 (t) W ipe(t).
contrast Approx() definition longer approximates predicate W ipe.
definition Approx () used find certainly holds holds given two
valued interpretation predicates .
Example 3.5. larger example, look Example 1.1. Let us take
= act = {W ipe}. definition Approx () found Appendix
A.
approximative definition useful properties, formulate
next two theorems. first property that, using approximative definition
90

fiAn Approximative Inference Method

ct
A1



cf



1


ct


A2 (t)




Acf

2 (t)







Acf

2 (t)


ct



2 (t)

ct


A2 (t)



Cleancf (t + 1)


cf
A3 (t)
Cleanct (t + 1)




Act

3 (t)







Act

3 (t)


cf



3 (t)


cf



3 (t)


cf


Clean
(t)


cf


W
ipe
(t)




Cleanct (t)



W ipect (t)


Act

2 (t).




Acf
(t).
2


ct


A1 .


cf
0
0
ct 0

A1 (t 6= A2 (t )).







cf
cf
Clean (t + 1) A3 (t).



ct

Clean (t + 1).



ct

A3 (t).



cf


A2 (t).



cf
A2 (t).
cf

Act
2 (t) A3 (t).


ct


A2 (t) Cleancf (t + 1).






cf
cf

Clean (t) W ipe (t).




Cleanct (t).





W ipect (t).




Act
(t).
3




Act
(t).
3


cf

cf

A3 (t) W ipe (t).



cf
cf
A3 (t) Clean (t).

A1

A2 (t)


A3 (t)

Clean(t + 1)



Clean(t)

W ipe(t)

Figure 3: Example approximative definition
together encoding three-valued interpretation original vocabulary,
give exact characterization approximative definition computes. Indeed,
setting, ApproxBU () actually encodes three-valued Kleene evaluation
I. Moreover, adding top-down rules change this, since compute
actually anything, long information original vocabulary provided
input. formally state property, need define encode
four-valued interpretation definition. on, assume vocabulary
-pre-interpretation I, contains constant symbol Cd every domain element
domain I, pre-interpretation holds (Cd )I = d.
allows us identify Cd therefore, abusing notation, use denote Cd
follows.
Definition 3.4 (I ). Given four-valued -interpretation I, definition associated
denoted defined


=

| P (d)
p t}
{P ct (d)
cf



{P (d) | P (d) p f }

Theorem 3.1. Given -formula four-valued -interpretation I, following
holds:
a) case three-valued holds Approx() logically equivalent
ApproxBU () , is, od(Approx() ) = od(ApproxBU () )
91

fiVlaeminck, Vennekens, Denecker & Bruynooghe

b) Let od(Approx() ), v1 truth value Act
v2 truth

value Acf
, (v1 , v2 ) corresponds four-valued truth value , i.e.,
= (v1 , v2 ).

Proof. See Appendix B.
summary, theorem says that, first all, approximation always
computes four-valued Belnap evaluation four-valued structure I. Moreover,
computation done bottom-up rules approximation alone. threevalued, top-down rules actually effect all. four-valued,
may still serve purpose, however: bottom-up rules derived
subformula inconsistent, propagate information derive smaller
formulas also inconsistent. see this, consider following formula P Q, take
four-valued interpretation P = Q = t. one verify
cf
bottom-up rules Approx() infer Act
P Q AP Q true.
However, top-down rules also infer Qcf true.
theorem information add approximative definition
form definition , i.e., assert truth, resp. falsity domain atoms.
following definition allows us assert truth falsity grounded

subformula [d].
Definition 3.5 ( ). Given -formula , -pre-interpretation I, set
[x] subformula arity n Dn
formulas ()[d],
domain I, define follows:
cf



= {Act
(d) | [d] } {A (d) | [d] }.

assert way truth (or falsity) set grounded subformulas ,
obtain approximation everything holds (respectively, hold)
models . However, opposed theorem above, next theorem
give exact characterization approximation get.
Theorem 3.2. Given -formula , set defined subformula 0 [x0 ]
cf 0
0 0
0
. Let od(Approx() ). |= Act
0 (d ) (resp. A0 (d )), |= [d ]
(resp. |= 0 [d0 ]).
Note interesting special case theorem take equal {}
thus add Act
Approx(). definition gives approximation everything
certainly true resp. certainly false models .
Returning exact relationship work Wittocx et al. (2010)
content section, see Wittocx et al. interested special
case, i.e., approximating models theory. reason transformation
formula EN F () already includes formula t, cause rule
Act
always included approximating definition. soundness results
also formulated proven setting. However, difficult see
proofs trivially adapted proof Theorem 3.2 general setting
used section.
92

fiAn Approximative Inference Method

4. Approximating SO-Satisfiability Problems
use approximate definition previous section approximate following problem. Take formula F = P Q : . ease presentation,
assume second-order formulas paper contain free predicate symbols,
results generalize setting free predicate symbols. also assume
Q contains predicate symbols. follows, denote vocabulary
. question want answer whether formula F satisfied given
finite-domain pre-interpretation constant function symbols formula.
satisfiability problem boils deciding whether find witness
satisfiability formula, following sense.
Definition 4.1 (Witness). call J witness satisfiability formula P Q :
given finite pre-interpretation I, J interpretation \ Q extending (i.e., J
interpretation whole vocabulary without universally quantified predicates)
Q : satisfied J. Equivalently, J witness three-valued holds
interpretation J expands J assigning u domain atom Q(d),
svJ () = t.
goal section approximate satisfiability problems
satisfiability problem following sense.
Definition 4.2 (Sound approximation). Consider satisfiability problem
formula P Q : , FO formula alphabet . SO(ID) formula
form P R : 0 , 0 FO(ID) formula alphabet \ Q R, sound
approximation satisfiability problem if, whenever J witness satisfiability
P R : 0 , J|\Q witness satisfiability P Q : .
words, sound approximation G satisfiability problem
formula F stronger SO(ID) formula, i.e., one fewer witnesses P .
4.1 Naive Method
use results Theorem 3.1 construct sound approximation given
formula.
Definition 4.3 (APP(F )). Given formula F = P Q : . Take alphabet
function symbols predicates P . define APP(F ) formula
tf
P R : Approx () Act
vocabulary R, R = (Q Aux()) .
intuition -interpretation I, Approx () give result
four-valued evaluation -interpretation expands assigning unknown universally quantified predicates Q. entire FO formula evaluates
true four-valued interpretation, know satisfied interpretation expands (in words, every interpretation Q predicates), thus
witness satisfiability entire formula F . auxiliary predicates
Aux() introduced transformation ENF needed way
propagation works, value completely determined P .
93

fiVlaeminck, Vennekens, Denecker & Bruynooghe

Proposition 4.1. formula F form P Q : , holds APP(F )
sound approximation F .
Proof. follows immediately Theorem 3.1, take three-valued inter = u Q Q Dn ,
pretation, interpretation (Q(d))
= (P (d))
P P Dn , domain I.
(P (d))
example, take F formula P Q : = P Q, APP(F )
becomes:
ct

ct


P

Q






Acf P Qcf

cf
ct
cf
ct

P, Q , Q , , :
Act
.
ct Act P
Q







cf

Q Acf

start interpretation open predicate P definition Approx{P } ().
Let us take interpretation makes P true. unique model definition
ct
cf
extends interpretation assigns true Act
false Q , Q
ct
Acf
. Therefore, satisfies Approx{P } () . Hence, witness
satisfiability APP(F ), and, indeed, also witness satisfiability original
formula P Q : P Q.
approximation method sound, many applications still incomplete.
Indeed, let us look following formula: F = Q : Q Q. APP(F ) becomes:



Qct Qcf
Act






cf
cf Qct




Q






Qct Act Qct

cf

:
,

Qct , Qcf , Act
Act

.
cf Acf



Q






cf


Qcf Act


Q




Qct Acf



definition entail Act
, APP(F ) unsatisfiable, even though original formula F clearly always satisfied. problem that, showed
previous section, definition encodes three-valued Kleene evaluation,
strong enough find formula F satisfied. this, need stronger
supervaluation.
Recall preliminaries saw supervaluation Kleene evaluation
general equal. However, formulas equal. literature, several
classes formulas agree proposed, e.g., context locally
closed databases (Denecker et al., 2010), context reasoning incomplete
first-order knowledge (Liu & Levesque, 1998). latter introduces normal form N F
first-order formulas, supervaluation coincides Kleene evaluation,
proves certain classes formulas normal form N F. One class
CNF formulas every two literals conflict-free: pair literals
conflict-free either polarity, use different predicates,
use different constants argument position. immediately follows
94

fiAn Approximative Inference Method

approximation complete formulas first-order formula satisfies
condition.
Proposition 4.2. formula F form P Q : , N F
normal form (according Liu & Levesque, 1998) satisfiable respect given finite
pre-interpretation SO-formula APP(F ) satisfiable w.r.t. I.
Proof. follows immediately results Liu Levesque Theorem 3.1.
4.2 Complete Method
Unfortunately, many applications give rise formulas first-order part falls
outside class N F, means completeness method guaranteed.
Particularly troublesome practice formulas common form P Q : 1 2 .
formulas, naive approximation method previous section tries find
interpretations P implication = (1 2 ) holds Q. However,
look details approximative definitions, find Act
defined rule
cf
ct
body 1 2 . words, approximation derive holds
Q either case 1 false Q 2 true Q. However,
rarely case. practical applications, witnesses interest
typically satisfy implication 1 2 always falsify 1 always
satisfy 2 , rather interpretation Q satisfies 1 also satisfies 2.
instance, conformant planning example, always interpretations
fluents satisfy action theory act , arbitrarily assign
fluent value wrong initial value actions performed. Even
set actions completely correct conformant plan, therefore cannot make goal
certainly true, still unsatisfied wrong interpretations
fluents. course, bother good method finding conformant plans.
thing matter goal satisfied interpretations
fluents satisfy action theory.
Luckily, approximation method also used discover kind witnesses.
thing required add approximative definition = Approx ()
rule Act
1 t. do, seed approximation assumption 1
holds. Starting assumption, top-down rules derive properties
predicates Q shared interpretations Q actually satisfy 1 .
bottom-up rules propagate information upwards discover whether
properties suffice ensure 2 also holds. do, know 2 indeed
must hold every interpretation Q satisfies 1 therefore found
witness formula.
want find witnesses kind degenerate witnesses either make
1 false Q 2 true Q, could simply combine new method old
ct
ct
one check either whether Act
2 holds according {A1 t} whether holds
according itself. However, turns necessary: achieve
ct
effect checking whether {Act
1 t} implies . because, first,
ct
definition {Act
1 t} able derive whenever can:
ct
ct
derive A2 {A1 t} obviously still able so; would
95

fiVlaeminck, Vennekens, Denecker & Bruynooghe

ct
able derive Acf
1 , {A1 t} also able so, simply
approximation flow information ct cf variants
formula, additional assumption Act
1 holds change original derivation
ct
ct
ct
Acf
1 . Second, {A1 t} derive A2 , also derives , simply
cf
ct
contains rule Act
A1 A2 . Therefore, find kinds witnesses
ct
checking whether Act
implied single definition {A1 t}.

Definition 4.4 (APP (F )). formula F = P Q : ,
definition
form 1 2 , define APP (F ) P R : Act
,
ct
Approx (1 2 ) {A1 t}.
Note obtain Definition 4.3 special case taking trivial formula
1 . approximation method still sound, following proposition states.
Proposition 4.3. Given formula F form P Q : , = 1 2 ,
SO(ID) formula APP (F ) sound approximation F .
Proof. See Appendix C.
Since approximative definition APP (F ) contains rules Approx(F ),
hard see new approximation method least complete one
using APP(F ) (Definition 4.3). Moreover, seen following example,
also strictly complete.
Example 4.1. Let us consider following formula F = P Q : (Q P ) Q.
P = clearly witness satisfiability problem. denote (Q P ) Q
1 (Q P ) 2 , APP(F ) following formula.

Act

1


cf






ct1
A2
P R :

Acf
2



ct

Q


cf
Q








ct
Acf
2 Q
ct
A2 Qcf
P Qct
P Qcf
Act
2 P
Acf
2











Act
1 .










Now, even P = t, definition body formula entail Act
1 = t.

Therefore, APP(F ) satisfiable. hand, APP (F ) formula
above, apart definition contains one rule, is, rule Act
2 t.
easy verify APP (F ) satisfiable, indeed P = witness.
Obviously, new method still complete formulas 1 2 , 1 2
satisfies normal form N F. However, method also works many formulas outside
class. Unfortunately, difficult characterize precisely much complete
new method is. instance, one source loss completeness comes fact
current translation ENF cannot recognize multiple occurrences subformula,
introduce different Tseitin predicate occurrence. Even though cannot
96

fiAn Approximative Inference Method

guarantee completeness method general, always found solutions
conformant planning benchmarks considered Section 6.
final remark method approximative definition Approx (1
2 ) contains number rules superfluous context. Indeed, method,
definition takes input interpretation P together assumption
1 certainly true. uses bottom-up top-down rules derived 1
compute effect inputs predicates Q. Finally, rules derived 2
compute whether derived information Q suffices make 2 certainly true.
However, know Theorem 3.1, bottom-up rules 2 needed
this. Therefore, top-down rules 2 actually contribute nothing could

TD
well removed. Adapting Definition 4.4 use
BU = \ Approx (2 ) instead

leads following definition.
Definition 4.5 (APP
BU (F )). formula F = P Q : ,
ct

form 1 2 , define APP
BU (F ) P R : BU ,
TD
ct

BU = Approx (1 2 ) \ Approx (2 ) {A1 t}.

follows directly Theorem 3.1 Proposition 4.3 sound approximation. removed top-down rules approximation 2 , remaining
rules serve, already know, compute Kleene evaluation 2 .
computing Kleene evaluation subformula 2 , use
pt
Tseitin predicates Act
. alternative avoid Tseitin predicates
defining Act
2 directly single rule:
ct
Act
2 (2 )

variant summarized following definition.
Definition 4.6 (APP
BU,U nf (F )). formula F = P Q : ,
ct

1 2 , define APP
BU,U nf (F ) P R : BU,U nf ,
ct
ct
ct

BU,U nf = Approx () \ Approx (2 ) {A2 (2 ) } {A1 t}.

approximation actually equivalent Def. 4.5. follows
fact bottom-up rules 2 positive non-recursive, allows us
eliminate Tseitin predicates introduced parse tree 2 applying unfolding
procedure Tamaki Sato (1984). iteratively applying equivalence preserving
procedure, reduce rules generated approximate 2
ct
single rule Act
2 (2 ) .

5. Approximating SO(ID)-Satisfiability Problems
Inductive definitions important knowledge representation. argued Denecker
Ternovska (2008), inductive definitions used represent mathematical
concepts, also sort common sense knowledge often represented logic
programs, dynamic domains, closed world assumption, defaults, causality, etc.
97

fiVlaeminck, Vennekens, Denecker & Bruynooghe

Therefore, inductive definitions make task representing problem logic considerably easier. example use inductively defined Situation Calculus
reasoning actions (Denecker & Ternovska, 2007). Recall introduction
showed represent Tact Example 1.1 inductive definition act :


Clean(t + 1) Clean(t).

Clean(t + 1) W ipe(t).
.


Clean(0)
InitiallyClean.
associated conformant planning problem expressed SO(ID) satisfiability problem:
W ipe Clean, InitiallyClean : act Clean(n).
show detail Section 7, general conformant planning problem
seen satisfiability problem form
F : (act init ) (prec goal ),
AI
predicates represent actions, initial fluents F fluents.
definition act defines fluents change terms action, init first
order formula initial situation, prec describes preconditions actions
goal goal. motivates extension approximation method formulas
including definitions. However, analyze general case definitions may
appear arbitrary locations formula, instead restrict attention formulas
form
P Q : ( 1 ) 2 ,
definition Def () Q 1 2 FO formulas. Even
though restrictions strictly necessary, allow us keep technical
details relatively simple (in particular, avoid need approximation rules infer
definition whole certainly true/false), still covering way
definitions typically used: assumption predicates indeed
definition formula 1 say be, 2 states properties
satisfy.
extend approximative method ( 1 ) 2 satisfiability problems,
need syntactic representation (i.e., approximative definition) describes sound
inferences made definition three-valued context. section
propose two ways obtain approximative definition, accordingly, two ways
approximate ( 1 ) 2 satisfiability problems. continue, first
need recall preliminaries.
5.1 Preliminaries Well-founded Semantics Inductive Definitions
Earlier, defined model positive inductive definition given two-valued interpretation open predicates. on, inductive definitions longer
positive definitions, model definition longer always computed
least fixpoint immediate consequence operator introduced Section 2. Moreover,
98

fiAn Approximative Inference Method

follows want use inductive definitions together four-valued information
open predicates (for example, information obtained propagation
first order theory). Therefore, recall (see, e.g., Denecker & Ternovska, 2008)
define well-founded model non-monotone inductive definition (that is, negation
body rules allowed), given four-valued information open predicates, denote W F MO (). order this, first need define
additional concepts. Recall P denotes body unique rule predicate P
head.
Definition 5.1 (Operator ). definition given (potentially 4-valued)
Open()-interpretation O, define operator 4-valued Def ()-interpretations
domain Open() (I) = J iff defined predicate P/n
n-tuple Dn , holds
= O+I [d]

P J (d)
P
Recall preliminaries defined isomorphism maps pair
interpretations (I, J) corresponding four-valued interpretation I.
Definition 5.2 (W F MO ()). define well-founded model 4-valued
O,
interpretation (I, J) (I, J) maximal oscillation pair operator ST


ST operator J(lf p(K(T (K, J)1 )). I.e., (I, J) least precise pair
2-valued interpretations


= ST
(J) J = ST
(I).

explanation order. First look operator K(TO (K, J)1 ). operator takes Def ()-structure K, turns 4-valued one combining J,
applies operator , projects result first argument. see
K(TO (K, J)1 ) = L iff defined predicate P/n n-tuple Dn , holds
= iff (P [d])
+(O1 +L)(O2 +J) =
P L (d)
words, positive occurrences atoms evaluated O1 + L, negative occurrences
O2 + J. J, operator K(TO (K, J)1 ) monotone, therefore
maps J least fixpoint. proven
least fixpoint. operator ST
operator antimonotone, therefore maximal oscillation pair. definition
nonof well-founded model maximal oscillation pair operator ST
constructive definition. maximal oscillation pair constructed iterating
following operator, starting least precise interpretation extends O,
reaches least fixpoint.

Definition 5.3 (Stable operator ST
). define operator ST pairs interpretations as:


ST
(I, J) = (ST (J), ST (I)).

stable operator monotone w.r.t. precision order p pairs interpretations
fixpoints therefore form complete lattice. fixpoints operator called
99

fiVlaeminck, Vennekens, Denecker & Bruynooghe

four-valued stable fixpoints , least precise fixpoints precisely
well-founded model given O.
define semantics inductive definitions general case.
reader easily verify indeed generalizes definition odO () positive
definitions gave Section 2.2.
Definition 5.4 (Satisfaction relation definitions). |= iff (I|Def () , I|Def () )
well-founded model I|Open() .
Note definition three-valued well-founded model every possible
interpretation open predicates Open(), definition model (i.e.
exists interpretation |= ). call definition total
two-valued well-founded model every possible two-valued interpretation open
predicates.
definitions generalize rather obvious way standard well-founded
semantics propositional logic programs strongly linked stable semantics (Gelfond & Lifschitz, 1988). case propositional logic program ,
Open() = {}, operator K(T (K, J)1 ) nothing else immediate consequence operator TJ Gelfond Lifschitz reduct J , operator maps
J lf p(K(T (K, J)1 ) stable operator . shown Van Gelder (1993),
maximal oscillation pair indeed well-founded model .
5.2 Approximating SO(ID)-Satisfiability Problems
Assume formula , FO(ID) formula instead FO.
concepts witness (Definition 4.1) sound approximation (Definition 4.2) straightforwardly generalised FO(ID) formula. allows us develop two
approaches. first one, Section 5.2.1, replaces definition completion
applies method Section 4. However, completion weaker
definition. Therefore, Section 5.2.2, develop another approach constructs
approximation conjunction definition FO formula.
5.2.1 Using Completion Definition
first approach based use completion (Clark, 1978). completion
definition conjunction equivalences x P (x) P (x) predicates
P Def (), P (x) body unique rule P head. useful
property definition implies completion compl(). Moreover, nonrecursive, two actually equivalent. Replacing definition completion
( 1 ) 2 obtain formula (comp() 1 ) 2 . every model
model comp(), every model (comp() 1 ) 2 model ( 1 ) 2
every witness (compl() 1 ) 2 satisfiability problem witness
( 1 ) 2 satisfiability problem. Hence use results Section 4
formulate following proposition.
Proposition 5.1. formula APP
BU ((compl() 1 ) 2 ) sound approximation
P Q( 1 ) 2 .
100

fiAn Approximative Inference Method

disadvantage using completion matter complete approximation method defined Definition 4.4 is, never able infer something
follows compl(). instance, inductive definition {P P }
entails P , completion P P not.
Denecker Ternovska (2008) proven that, addition non-recursive definitions, class recursive definitions equivalent completion. particular,
case definitions strict well-founded order 2 . therefore replace
definitions completion without losing precision. theory Tact Example
1.1 actually completion definition act . Since act recursive definition
strict well-founded order (we make use time argument predicates
construct well-founded order), act Tact equivalent.
Gaspipe conformant planning problem (Son et al., 2005), hand, uses
dynamic domain completion suffice. Summarized, objective
conformant planning problem start flame burner connected
gas tank pipe line. pipe line consists sections connected
valves. valve opened gas one side other, gas
spread far possible. formalized inductive definition
reachability relation pipe line:
(
)
x, Gas(x, t) Gas(y, t) v Connected(x, y, v) Open(v, t).
x, Gas(x, t) ank(x).
reachability definitions equivalent completion. Therefore, approximative method presented subsection work. problem completion
case correctly minimize defined predicates presence
recursion, would allow models loop pipe line filled gas even
connected tank. missing, therefore, unfounded set reasoning allows well-founded semantics correctly minimize defined predicates.
5.2.2 Using Certainly True/Possibly True Approximation
approximative definition Approx () used Section 4 nice property
defines, subformula (including itself), whether certainly true certainly
false. property allowed us find witnesses simply asserting Act

hold according definition. want apply method formulas
contain definition , construct approximative definition defines
whether subformulas (including itself) certainly true certainly
false. Section 5.2.1, naive method managed simply replacing
completion. want improve method constructing approximation
also takes account unfounded set reasoning performed well-founded
semantics.
also take aspect well-founded semantics account, however,
becomes difficult define definition whole certainly true certainly
false. Luckily, needed stick assumption definitions appear
antecedent implication . Indeed, approximate implications
2. order < well-founded infinite descending chains . . . < xn < xn1 < . . . < x1

101

fiVlaeminck, Vennekens, Denecker & Bruynooghe

assuming antecedent certainly true (Definition 4.4), really need
approximation consequences definition. end, transform
original definition approximative definition 0 well-founded
model 0 , given approximation open predicates , approximates
well-founded models given interpretation open predicates
approximated O. words, construct approximative definition 0 whose
two-valued well-founded model encodes potentially four-valued well-founded model
original definition , given potentially four-valued interpretation predicates
. therefore represent four-valued interpretation orginal vocabulary
two-valued interpretation larger vocabulary 0 . However, instead introducing,
predicate P , predicate P ct (P certainly true) P cf (P certainly false),
before, introduce predicates P ct P pt (P possibly true, i.e., P
certainly false). Let ct/pt denote vocabulary F {P ct | P } {P pt | P }.
four-valued -interpretation I, define corresponding ct/pt -interpretation ct/pt
ct/pt
interpretation pre-interpretation (P ct )I
= {d |
ct/pt

pt





P (d) p t} (P )
= {d | (P (d) p t)}.
Also, -formula , define formula ct/pt formula obtain
replacing positive occurrences predicate P P ct , negative occurrences
P pt , finally reducing negational normal form. easy see ct/pt also
obtained ct replacing, every predicate P , occurrences P cf P pt .
Unlike ct , ct/pt always positive formula contain negations. particular,
P ct occurs positively P pt occurs negatively. subvocabulary ,
ct/pt

denotes ct/pt P ct P pt replaced P every predicate
P . Again, follows use denote predicates need
approximated two-valued information them.
ct/pt

ct/pt

Definition 5.5 (App ()). definition , define App
{Rct Rpt } Rct consists rules

() definition

x(P ct (x) ct/pt
)

Rpt consists rules
x(P pt (x) ()ct/pt
)

every definitional rule x (P (x) ) .
assume rest paragraph without loss generality empty,
ct/pt
drop notation App .
Example 5.1. Consider following inductive definition.


B B

Assume = {}.

Appct/pt

ct
B


ct

=
pt
B


pt






102

B ct Apt
Dpt
B pt Act
Dct









.

fiAn Approximative Inference Method

see three-valued interpretation {D = u}, translates {Dct = f , Dpt =
t}, approximative definition correctly infer B ct false B pt true.
take {D = f } interpretation open predicate D, see approximative
definition correctly infers B ct B pt false. example unfounded
set reasoning: known true, approximation detects B could
derived B therefore must false. kind reasoning could done
previous, completion-based approximation method, since sound w.r.t.
semantics definition itself, w.r.t. weaker completion.
example also demonstrates use vocabulary ct/pt instead
ct/cf , since latter would yielded definition:
ct

B
B ct Acf



ct


Dcf
.
B cf B cf Act



cf


Dct
{D = f }, definition would fail infer B cf . Intuitively, reason
unfounded set reasoning well-founded semantics tries minimize extension
defined predicates making many atoms false possible. Using ct/cf
vocabulary, well-founded semantics approximating definition therefore attempts
possible, actually corresponds maximizing
falsify many atoms P cf (d)
possible extension original predicates P , instead minimizing well-founded
semantics original definition does.
two-valued interpretation double vocabulary ct/pt corresponds fourvalued interpretation original vocabulary . want establish link
ct/pt
well-founded model original definition App . complict/pt
cating factor that, example shows, definition App
longer
monotone therefore longer guaranteed two-valued well-founded model.
three-valued interpretation ct/pt longer corresponds even four-valued
interpretation original vocabulary , prove correspondence
ct/pt
well-founded model App
two-valued.
Theorem 5.2. Let four-valued interpretation open predicates definition
. Appct/pt () two-valued well-founded model given Oct/pt
unique four-valued stable fixpoint given O. Moreover, Appct/pt () two-valued wellfounded model I, unique four-valued stable fixpoint unique interpretation
ct/pt = I.
Proof. See Appendix D.
theorem requires unique four-valued stable model four-valued
input interpretation O. stronger requirement common condition
totality, requires definition two-valued well-founded model given
two-valued input interpretation O. following example shows, stronger condition
indeed necessary.
103

fiVlaeminck, Vennekens, Denecker & Bruynooghe

Example 5.2. Consider following definition:




B.

B C.




C O.
definition total, because, two-valued interpretation open predicate
O, ({A}, {A}) two-valued well-founded model. However, three-valued
interpretation ({}, {O}) (i.e., unknown) open predicate O, three-valued
well-founded model ({}, {O, A, B, C}) unique three-valued stable fixpoint, since
({A}, {O, A, C}) also fixpoint. indeed, find
ct






B ct



C ct
ct/pt
App
() =

Apt



B pt



pt
C

B pt .






Apt C ct .


ct
pt
.

B ct .



ct
pt
C .


pt
ct
.

two-valued well-founded model given {Opt }. easiest way see
fill fact know Opt Oct f propagate information:
ct

B pt .






ct
pt
ct



B C .






C ct f t.



Apt B ct .








pt
ct
pt


B



C
.




pt

C f .

ct

B pt .






ct
pt



B f .









ct

B pt .


















ct

B pt .




















Apt B ct .








pt
ct


B



t.




pt

C


Apt f .







pt
ct


B


.




pt

C



Apt








pt
ct


B


.




pt

C

So, left loop negation, means Act B pt remain unknown
three-valued well-founded model ({Apt , C pt }, {Act , Apt , C pt , B pt }) definition.
computing three-valued well-founded model given O, approximative definition Appct/pt () produce precise results approximation compl();
particular detect atoms unfounded set must false, illustrated
Example 5.2 above. use Appct/pt () approximation () 2 -problem,
still need show combined approximation Approx ()
produce sound approximation . this, need combine one definition
ct/cf-predicates another definition ct/pt-predicates. achieve first merging
two definitions adding rules copy information one vocabulary
other.
104

fiAn Approximative Inference Method

Definition 5.6 (D ). Given vocabulary , subvocabulary , inductive
definition first-order formula . define following inductive definition.
ct/pt

App

{Ocf

()

Approx () {Act
t}

{Opt Ocf | every predicate Open() \ }

{P cf P pt | every predicate P Def () \ }

ct
f , f | every predicate Open() \ occur }

definition indeed contains rules approximation rules
approximation , is, Appct/pt () Approx() {Act
t}, respectively,
also number extra rules make connection two approximations.
approximate defined predicate Q approximation uses pair predicates
Qct Qpt approximation uses Qct Qcf . Hence, number extra
rules needed transfer information predicates Qpt Qcf . rules
{Opt Ocf } transfer information approximation derived
truth open predicate (by means Ocf ) corresponding predicate Opt
approximation definition. rules {P cf P pt } turn propagate information
derived truth defined predicate approximation definition
corresponding predicate P cf approximation . Finally, rules {Ocf f , Oct
f } make sure Ocf Oct defined atoms (instead open ones)
default value u. following proposition relates well founded model
models .
Proposition 5.3. Given vocabulary , subvocabulary FO(ID) formula .
(resp. P cf (d)),
holds
Then, every -interpretation I, W F MI (D ) |= P ct (d)
(resp. |= P (d)).

every model extending |= P (d)
Proof. See Appendix E.
proposition analogue inductive definitions result Theorem 3.2
states FO formulas. One difference two results proposition always assumes definition holds, Theorem 3.2 makes
assumption FO formula approximated. discussed beginning
section, restriction problem, way approximate
implications allow definitions appear antecedent. second
difference Theorem 3.2 applies arbitrary subformulas ,
is, however, easy corrolary proposition
proposition considers atoms P (d).
result fact holds formula contains predicates defined
(or cf (d)),
every model
, i.e., whenever W F MI (D ) |= ct (d)
(resp. |= (d)).

extending I, holds |= (d)
introduced approximation Appct/pt () aim complete
completion-based approximation. long single definition considered
105

fiVlaeminck, Vennekens, Denecker & Bruynooghe

isolation, succeeded goal. However, also incorporated
additional formula , longer case. instance, consider following FO(ID)
theory:


Q P Q.
Here, approximation cannot derive P certainly true, simply
ct/pt-approximation Appct/pt () contain rules head-to-body propagation, i.e.,
rules infer something body definitional rule, given information
head. contrast, approximation completion contain rules
therefore problems reaching conclusion. motivates us use
use D(compl()) instead. definition implies completion,
sound.
obtain sound approximation SO(ID) formula P Q = (
) 2 , need plug approximation D(compl())
suitable SO(ID) formula, similar one defined Definition 4.4
formula P Q 1 2 . small complication, however, that, discussed previously,
cf
approximation definition define predicates Act
tell us
definition whole certainly true certainly false. Therefore, longer
use normal approximation entire implication certainly true.
present approximation SO(ID), let us first introduce reformulation
original approximation SO, avoids use Act
.
Proposition 5.4. formula F = P Q : , form 1 2 ,
approximation defined Definition 4.4, i.e., formula
ct
P R : (Approx (1 2 ) {Act
1 t})

equivalent
cf
ct
P R : (Approx (1 ) Appox (2 ) {Act
1 t}) (A1 A2 )

Proof. obvious fact difference Approx (1 2 )
Approx (1 ) Appox (2 ) precisely set rules ensure Act
equivalent
ct
Acf
1 A2 .

approximation SO(ID) essentially consists replacing Approx (1 )
compl()
cf
{Act
Acf
form.
1 t} A1 respectively
Proposition 5.5. Given SO(ID) formula F = P Q ( ) 2 . define
APP wf (F ) following SO(ID) formula.
ct
P R : (Dcompl() Approx (2 )) (Acf
A2 ).

APP wf (F ) sound approximation F .
compl()

cases, approximating
instead gain us anything. instance, consider P Q( ) 2 problem, contains open
predicates (as case conformant planning problems consider next
106

fiAn Approximative Inference Method

sections). case, never need head-to-body propagation, therefore
compl()
complete
, therefore better using former.
case approximation method , well rules
definition APP wf necessary. Indeed, bottom-up rules Approx (2 )
needed, unfolded single rule. Therefore, define two
variants Definition 5.5.
Definition 5.7 (APP wf
BU (F )). Given SO(ID) formula F = P Q ( ) 2 .
wf
define APP BU (F ) following SO(ID) formula,
cf
ct
P R : (Dcompl() ApproxBU
(2 )) (A A2 ),

define APP wf
BU,U nf (F ) following SO(ID) formula,
ct
cf
ct
P R : (Dcompl() {Act
2 (2 ) }) (A A2 ).

6. Experimental Evaluation
paper seen number methods approximate SO(ID)
satisfiability problems. subsection, explore, number experiments,
use methods solve practically useful problems fast possible.
performed experiments number conformant planning benchmarks
paper Son et al. (2005). show Section 7, benchmarks
form F = , stratified definition, therefore equivalent
completion. Therefore, F equivalent formula compl() ,
denote F cp . experiments run dual core 2.4 Ghz CPU, 2.8 Gb RAM
Linux machine, using IDP model expansion system FO(ID) (Marien et al., 2006).
time-out twenty minutes used.
first question want answer whether definitions, completion
based approximation faster ct/pt approximation. hard see that, even
though Approx(compl()) linear size parse tree compl(), definition
may contain rules Appct/pt (), moreover, rules may contain lot
recursion. pose challenge current solvers, suggests likely
efficient use ct/pt approximation definitions. first column Table 2
shows times using completion definition , is, APP (F cp ),
second column, ct/pt-approximation used, is, APP wf (F ). expected,
ct/pt-approximation consistently faster.
Table 2 also compares solving times full completion-based approximative definition
cp
(in first column) approximation APP
BU (F ) (Def. 4.5), topdown propagation rules removed (third column). see BT
BTC benchmarks get order magnitude improvement. fourth column
cp
Table 2 shows timings unfolded approximation 2 , APP
BU,U nf (F ),
intermediate Tseitin predicates removed (Def. 4.6). see unfolding
consistently provides speed-up.
results suggest combining techniques, is, using ct/pt
approximation unfolding bottom-up approximation 2 together,
107

fiVlaeminck, Vennekens, Denecker & Bruynooghe

APP (F cp )

APP wf (F )

cp
APP
BU (F )

cp
APP
BU,U nf (F )

APP wf
BU,U nf (F )

BT(2,2)
BT(4,2)
BT(6,2)
BT(8,4)
BT(10,4)

0,151
3,404
38,93
-

0,109
3,493
14,76
-

0,115
0,312
0,876
32,91
-

0,065
0,153
0,409
1,774
-

0,031
0,064
0,113
0,462
1,643

BTC(2,2)
BTC(4,2)
BTC(6,2)
BTC(8,4)

0,210
-

0,131
-

0,171
40,081
-

0,116
8,408
-

0,037
0,109
0,335
41,894

0,390
1,101
6,597
31,275
-

0,026
0,036
0,067
0,120
0,231

0,507
1,250
8,995
42,583
-

0,473
1,266
6,997
28,387
-

0,049
0,052
0,128
0,396
1,530

7,023
-

0,374
-

5,217
-

2,792
-

0,100
0,358
6,650
193,290
2485

Problem

Domino(100)
Domino(200)
Domino(500)
Domino(1000)
Domino(2000)
Ring(2)
Ring(4)
Ring(6)
Ring(8)
Ring(10)

Table 2: first column gives name benchmark ones different
execution times. second column gives execution time approximation
completion third cp/pt approximation. fourth fifth column use
variants completion approximation. fourth column, top-down rules
2 removed addition, fifth column, remaining bottom-up rules
unfolded. last column combines cp/pt approximation changes. -
means execution interrupted 20 minutes.
give us fastest way approximating ( 1 ) 2 satisfiability problems. Indeed,
formula APP wf
BU,U nf (F ) (Definition 5.7) does, results method
shown last column Table 2. expected, far fastest method.

7. Applications Related Work
literature, many examples found approaches perform kind
approximate reasoning models logical theory. Often, approaches,
specific problem hand, seem boil instantiation general
methods presented here. section give examples.
7.1 Conformant Planning
general, conformant planning problem planning problem non-deterministic
domain initial state may fully known. goal come
plan (i.e., sequence actions) nevertheless guaranteed work. hard
problem: decision problem deciding whether conformant plan fixed length
k exists P2 -complete3 (Baral, Kreinovich, & Trejo, 2000; Turner, 2002). Therefore, one
3. planning domains executability actions given state cannot determined polynomially, even P
3 (Turner, 2002)

108

fiAn Approximative Inference Method

typically attempts solve approximately. section, show apply
approximative methods solve conformant planning problems.
Example 7.1. Let us consider Clogged Bombs Toilet domain (McDermott, 1987;
Son et al., 2005). number packages toilet. packages may
contain bomb disarmed dunking package toilet. Dunking
package toilet also clogs toilet cannot throw package clogged toilet.
Flushing toilet unclogs it. effects actions fluents modeled
following definition act , preconditions conjunction prec sentences
Tprec .

act



Clogged(0) Init Clogged.





Clogged(t + 1) p : Dunk(p, t) (Clogged(t) F lush(t)).

=


Armed(p, 0) Init Armed(p).






Armed(p, + 1) Armed(p, t) Dunk(p, t).

Tprec

p : Dunk(p, t) Clogged(t).
( p : Dunk(p, t) F lush(t)).
=
p p2 : Dunk(p, t) Dunk(p2 , t) p = p2 .
p t2 : Dunk(p, t) Dunk(p, t2 ) = t2 .

consider following regular planning problem: given completely specified initial
situation (specified formula init ), find plan packages disarmed.
formulate problem following formula:
A, F , : act prec init (t p Armed(p, t)),
A, denote action predicates {Dunk/2, F lush/1}, F denote
fluent predicates {Armed/2, Clogged/1} denote predicates used
describe initial situation {Init Clogged/0, Init Armed/1}. imagine initial
situation specified, want find plan works possible initial
situations, words conformant plan. formulate problem finding
plan follows.
F , : act (prec p Armed(p, t)).
formalized general follows.
Definition 7.1 (Conformant planning). Let vocabulary, consisting set
predicates A, denoting actions, I, denoting initial fluents, F denoting fluents. Let
Tact FO(ID) theory Tinit , Tprec Tgoal FO theories, , Tact
specifies values fluents given interpretation actions initial fluents,
Tinit theory specifying initial situation, Tprec contains preconditions actions,
Tgoal specifies goal planning problem. act denote conjunction
sentences possibly definitions Tact similarly theories.
problem conformant planning decide satisfiability following formula:
F : (act init ) (prec goal ).
109

(6)

fiVlaeminck, Vennekens, Denecker & Bruynooghe

AR :


Cloggedct (0)



ct (t + 1)

Clogged




Armedct (p, 0)




Armedct (p, + 1)




Cloggedpt (0)




Cloggedpt (t + 1)




Armedpt (p, 0)




Armedpt (p, + 1)




Init Cloggedct




Init
Armedct (p)

Init Cloggedpt


Init Cloggedcf




Init
Armedpt (p)




Armedcf
Init



cf (t)

Clogged



cf (p, t)

Armed




Act

2

































Init Cloggedct .
p : Dunk(p, t) (Cloggedct (t) F lush(t)).
Init Armedct (p).
Armed(p, t) Dunk(p, t).
Init Cloggedpt .
p : Dunk(p, t) (Cloggedpt (t) F lush(t)).
Init Armedpt (p).
Armed(p, t) Dunk(p, t).
f.
f.
Init Cloggedcf .
f.
Init Armedcf (p).
f.
Cloggedpt (t).
Armedpt (p, t).
pt : Dunk(p, t) Cloggedcf (t)
(pt : Dunk(p, t) F lush(t))
p1 p2 : Dunk(p, t) Dunk(p2 , t) p1 = p2
pt1 t2 : Dunk(p, t1 ) Dunk(p, t2 ) t1 = t2
tp : Armedcf (p, t).














































































Act
2 .

Figure 4: complete approximation Clogged Bombs Toilet example.
words, must plan (A), matter nondeterministic
F ), long specification effects actions (act )
aspects turn (I,
(partial) specification initial situation (init ) obeyed, plan
executable (prec ) achieve goal (goal ).
Formula 6 exactly form assumed above, thus use one
methods approximate conformant planning problems.
Example 7.1. (continued) Continuing Clogged Bombs Toilet example,
using ct/pt-approximation definition, unfolding constraint (prec
p Armed(p, t))ct , get approximating formula APP wf
BU,U nf (Definition 5.7),
shown Figure 4, R ct- cf-predicates introduced approximation
method.
result applying general approximation method conformant planning
problem, specified Tact , Tprec , Tgoal Tinit above, similar approximation AL action theory logic program work Son et al. (2005).
However, small differences details make difficult formally
compare two. Nevertheless, experiments discussed section, method
always finds correct solution (unless times out), method Son et al.
Moreover, two approaches also found solutions comparable execution times.
detail, Table 3 presents following results. implemented conformant
planner iteratively calling IDP model generator FO(ID) (Marien et al., 2006)
approximation, giving increasing number timesteps either plan
110

fiAn Approximative Inference Method

Problem

IDP

Smodels

Cmodels

BT(2,2)
BT(4,2)
BT(6,2)
BT(8,4)
BT(10,4)

0.438
0.513
1.050
1.55
2.80

0.199
0.219
0.587
30.9
-

0.145
0.212
0.425
2.39
5.80

BTC(2,2)
BTC(4,2)
BTC(6,2)
BTC(8,4)

0.273
0.844
1.60
43.7

0.136
0.412
3.88
-

0.139
0.389
1.23
102

Cleaner(2,2)
Cleaner(2,5)
Cleaner(2,10)
Cleaner(4,2)
Cleaner(4,5)
Cleaner(4,10)
Cleaner(6,2)
Cleaner(6,5)

0.644
1.57
1.55
2460
8.30
-

0.226
72.5
13.8
-

0.376
1.36
1.13
6.16
-

Domino(100)
Domino(200)
Domino(500)
Domino(1000)
Domino(2000)

0.176
0.181
0.212
0.236
0.339

0.096
0.114
0.324
0.618
1.22

0.090
0.151
0.354
0.660
1.32

0.655
1.56
7.35
157
1537

0.285
2.092
19.1
-

0.296
0.937
3.542
19.860
232

Ring(2)
Ring(4)
Ring(6)
Ring(8)
Ring(10)

Table 3: Comparison IDP vs Cmodels vs Smodels

found maximum number timesteps reached. compared planner
CPASP conformant planner (Son et al., 2005), using experimental setup
Section 6. CPASP takes action theory action language AL, encodes
approximation transition diagram corresponding action theory, means
answer set program. answer set solver used find conformant plans.
Son et al., used ASP solver behind CPASP CModels (E. Giunchiglia &
Maratea, 2011) SModels (Niemela, Simons, & Syrjanen, 2000). Table 3 shows,
combination approximation IDP system comparable to, overall slightly
worse, combination CModels Son et al.s approximation. compared
approximation given SModels, method tends bit better.
results line results ASP competition (Denecker et al., 2009) concerning
performance SModels, CModels IDP general, suggesting that, conformant
planning, approximation Son et al. comparable quality.
Another approximative method solving conformant planning problems found
work Palacios Geffner (2009). paper, authors consider conformant
planning problems, specified language Strips extended conditional effects
negation. define transformation K0 transforms conformant planning
problem classical planning problem sound incomplete way. fluent
literal L conformant planning specification, two new literals KL KL created,
111

fiVlaeminck, Vennekens, Denecker & Bruynooghe

denoting L known true, resp. known true, initial situation,
action preconditions effects translated initial situation, preconditions
effects reference new knowledge literals. hard verify
approximation method generalizes transformation: take encoding
conformant planning problem P , approximation obtained method
interpreted classical planning problem ct/cf vocabulary. planning problem
exactly planning problem specified K0 (P ) (i.e., action preconditions
effects correspond), apart initial situation. K0 transformation
propagation knowledge initial situation: given initial situation (specified
set clauses), K0 (I) consists literals KL L unit clause
I. means that, e.g., initial situation = {P Q, P }, K0 (I) include
literal KQ, method able infer Qct holds (which means
approximation method complete K0 transformation).
general method, allow solving conformant planning problems, also allows approximating number related problems temporal domains.
Consider, example, following problem: Given certain action happens
timepoint t, certainly lead property true ? formalized
following satisfiability problem, method applies again.
AIF : ((act init prec A(t)) ).
formula true possible plans A(t) happens, property holds.
variant problem so-called projection problem: Given exactly know
actions happened (we thus assume preconditions satisfied),
property hold ? order formulate problem satisfiability problem,
need express actions happened. done, example,
using inductive definition . projection problem expressed
AIF : ((act init ) ) satisfiability problem. Another variant following
problem: property 1 holds certain plan, property 2 also hold?,
expressed AIF ((act init prec 1 ) 2 ) satisfiability problem.
7.2 Querying Reasoning Open Databases
Approximate methods similar used context databases without
complete information, particular databases without CWA, open databases
(Baral et al., 1998; Liu & Levesque, 1998) databases make forms local closed
world assumptions (Denecker et al., 2010; Doherty et al., 2006). papers
goal compute certain possible answers queries. task
high complexity (from CoNP locally closed database without integrity constraints
possibly P2 databases first-order constraints - assuming given finite domain),
approximate methods presented translate FO query approximate FO
FO(FP)4 query solved directly database tables using standard
(polynomial) query methods.
method presented paper provide similar functionality. Let DB
set ground literals, representing incomplete database. Let background theory:
4. FO(FP) extension FO least greatest fixpoint constructs.

112

fiAn Approximative Inference Method

may contain integrity constraints, view definitions (datalog view programs special
case FO(ID) definitions), local closed world statements expressed FO, etc. given
holds Herbrand models
FO query Q [x], goal find tuples Q [d]
DB . problem deciding whether given tuple answer corresponds
satisfiability problem formula

R(DB Q [d]),

(7)

directly use approximation method problem. allows us
answer yes/no queries well decide whether given tuple certain answer
query, approximation method directly provide method compute (an
approximation of) tuples.
However, let us look following satisfiability problem.
R0 : DDB ApproxBU (Q [x]),
looks much like approximation ( 1 ) 2 satisfiability problems (as
formulated Proposition 5.5). definition DDB approximating
database DB background knowledge (note possibly contains definitions),
bottom evaluation query, constraint Act
Q dropped.
definition DDB ApproxBU (Q [x]) consists rules describing propagations allowed database theory , rules defining predicate symbol Act
Q ,
AQ Tseitin predicate representing query Q [x]. unique Herbrand model
definition, interpretation Act
Q contains tuples propagation
derive certainly satisfy query sound approximation full set
answers!
work Denecker et al. (2010), locally closed database LCDB assumed.
locally closed database consists standard database DB, i.e. set atoms, together
set local closed world assumptions LCWA(P (x), [x]). LCWA
statements expresses databases knowledge P complete tuples x
therefore true DB false
satisfy formula [x]. atom P (d)
holds domain
DB LCWA(P (x), [x]) [d]
discourse; otherwise unknown. authors present approximate reasoning
method query answering locally closed databases show approximate
query answering formulated fixpoint query. Basically, boils
following. One constructs following definition




...






P ct (x) P (x)

,
LCWA =
P cf (x) P ct (x) ct


P [x]






...
every relation P every local closed world assumption LCWA(P (x), [x]). Although
authors phrase form, method finding approximation
certain answers query Q [x] actually boils solving following satisfiability
problem:
R0 : DB CW A(DB) LCWA ApproxBU (Q [x]),
113

fiVlaeminck, Vennekens, Denecker & Bruynooghe

R0 denotes predicates auxiliary predicates occurring body existential formula. CW A(DB), denote formula expressing closed world assumption
database DB. presence closed world assumption might seem strange
first sight, since whole idea behind locally closed world databases assume CWA
per default. However, order correctly apply local closed world assumptions,
need exact specification database not, precisely
expressed DB CW A(DB). Indeed, given DB CW A(DB), LCWA
seen approximative definition certainly true false context
locally closed world assumptions. predicate Act
Q contain approximate
answer query Q [x], i.e., lower bound tuples query Q [x]
certainly true. Similarly, predicate Acf
Q contain lower bound tuples
query false.
limitation approach Denecker et al. extend method
one type integrity constraints, namely functional dependencies. way
functional dependencies handled extending LCWA extra propagation rules
taking functional dependencies account. contrast, general method
used easily extend arbitrary integrity constraints. works follows.
Let Tint set first-order integrity constraints. approximate problem
finding certain queries following satisfiability problem.
BU
ct t} DB CW A(DB) LCWA Approx
(Q [t]).
R0 : Approx(Tint ) {ATint

Again, predicate Act
Q contain approximate answer query Q [x].
Doherty et al. (2006), propose yet another approach asking queries incomplete
database. authors use term approximate database denote database, consisting
two layers: extentional intensional layer. layers external
representation towards user, internal representation.
extentional database consists positive negative literals, internally
stored classical database, using Feferman transformation. example, extentional database (EDB), entered user,
Color(Car1, Black), Color(Car1, Red), Color(Car2, Red),
internally stored
Colorct (Car1, Black), Colorcf (Car1, Red), Colorct (Car2, Red).
intentional database consists rules infer additional information facts
EDB. user write rules form ()P1 (x1 ). . .()Pn (xn ) ()P (x)),
internally stored (()P1 (x1 ))ct . . . (()Pn (xn ))ct (()P (x)))ct .
example IDB rule following rule
Color(x, y1 ) y1 6= y2 Color(x, y2 ),
internally stored
Colorct (x, y1 ) y1 6= y2 Colorcf (x, y2 ).
114

fiAn Approximative Inference Method

evaluate query, naive algorithm based exhaustively applying rules EDB
used.
rules IDB resemble F formulas sense describe valid
inferences made based incomplete information. internal representation
IDB indeed similar representation F formulas definitional rules.
However, key difference approach Doherty et al., user wants add
property database (e.g., car one color), write
inferences valid according property, approach inference
rules automatically generated property itself. Manually writing valid
inferences sanctioned property easy task. example, take property
car inspected suspect black paper Doherty
et al.. expressed FO formula = c(Suspect(c) Color(c, Black)
Investigate(c)). While, method, Approx() constructs approximation valid
inferences made formula, user write following
rules Doherty et al.s approach:
Suspect(c) Color(c, Black) Investigate(c)
Suspect(c) Investigate(c) Color(c, Black)
Suspect(c) Investigate(c)
...
method therefore generalizes work Doherty et al. deriving rules automatically general first-order theory.
Liu Levesque (1998) propose another type reasoning open databases.
consider simple form first order knowledge bases, called proper knowledge bases.
interesting feature knowledge bases easy obtain complete
characterization certainly true, resp. certainly false. terminology,
|= P ct (d)

means one construct definition , KB |= P (d)
cf

KB |= P (d) |= P . holds every two valued extension
three valued interpretation encoded model KB. Levesque et al. use
evaluation procedure based three-valued Kleene-evaluation check whether
query holds knowledge base. mentioned earlier, also define normal form
N F queries, prove Kleene-evaluation complete. work
extends work, sense take general first order knowledge base
approximately solve queries, shown above. course, since general
longer guarantee complete characterization certainly true/false,
longer guarantee completeness, even query normal form N F. Another
difference work Liu Levesque work here, assume
fixed countable infinite domain, assume fixed finite domain. indeed
theoretical difference, practice make difference, since evaluation
method considers finite set domain elements determined up-front.

8. Conclusions Future Work
Even problem computationally hard general, specific instances might still
solved efficiently. approximate methods important: cannot solve
115

fiVlaeminck, Vennekens, Denecker & Bruynooghe

every instance, instances solve, solve quickly. computational
logic, hard problems arise quite readily. therefore surprising literature
contains numerous examples algorithms perform approximate reasoning tasks
various logical formalisms various specific contexts. Since many algorithms share
common ideas, natural question whether seen instances
general method general language.
paper presents method. start propagation method FO()
developed Wittocx, Marien, Denecker (2008) symbolic expression (Wittocx,
2010) generalize method approximating P2 -complete SO(ID) satisfiability problem solving NP problem. Importantly, syntactic method
transforms SO(ID) formula SO(ID) formula. affords us freedom
use off-the-shelf solver language perform approximative reasoning.
Moreover, also makes significantly easier update method adding (or removing)
specific propagations.
Since method approximation, necessarily incomplete. Nevertheless,
experiments shown that, practice, often manage find solution.
interesting topic future work determine classes problems, method
shown complete.
summary, contributions paper (1) extended logical
representation describing propagation process general method approximating
SAT (SO) problems; (2) shown approximate inductive definitions, use
approximate class useful SAT (SO(ID))-problems; (3) examined
existing approximation methods fit general framework.

Acknowledgments
work supported Research Foundation - Flanders FWO-Vlaanderen, projects
G.0489.10 G.035712, Research Fund KULeuven, project GOA/08/008. Hanne
Vlaeminck supported IWT-Vlaanderen.

Appendix A. Example Approximation
Figure 5 shows full approximation act Example 1.1.

Appendix B. Proof Theorem 3.1
Proof. First, remark Feferman (1984) showed four-valued evaluation
formula interpretation simulated computing standard two-valued
evaluation ct cf tf . easy verify bottom-up rules Approx()
inductively encode evaluation. split proof two parts. First assume
three-valued. show case bottom-up rules used, i.e., leaving
top-down rules change model definition. proves first
part theorem, together remark also proves second part
theorem case three-valued. Then, left prove,
second part theorem also holds four-valued I.
116

fiAn Approximative Inference Method

























































































































































































Act
act
Acf
act
Acf
act
Act
0
Act
8
Acf
0
Acf
8









Act
0
Acf
0
Act
1 (t)
Acf
1 (t)
Acf
1 (t)
Acf
1 (t)
Act
1 (t)
Act
2 (t)
Acf
2 (t)
Act
5 (t)
Acf
5 (t)
Act
2 (t)
Act
2 (t)
Acf
2 (t)
Cleanct (t + 1)
Cleancf (t + 1)
Acf
4 (t)
Act
4 (t)
Act
4 (t)
Act
4 (t)
Acf
4 (t)
Cleancf (t)
Cleanct (t)
Act
5 (t)
Act
5 (t)
Acf
5 (t)
Acf
6 (t)
Act
6 (t)
Cleancf ((t + 1))
Cleanct ((t + 1))
Acf
6 (t)
Acf
6 (t)
Act
6 (t)
Cleancf (t)
Cleanct (t)





































Acf
8
Acf
8
Act
8
Act
9
Acf
9
Act
11
Acf
11
Act
9
(t : Act
Act
1 (t)).
9
(t : Acf
Acf
1 (t)).
9
Act
Cleanct (0)
0.
ct
cf
(Acf
0 (t1 : (t1 = A1 (t1 )))). Clean (0)
cf
Acf
(t).
InitiallyClean
2
cf
A5 (t).
InitiallyCleanct
ct
ct
(A2 (t) A5 (t)).
Act
11
Act
(t).
Act
1
11
ct
(Acf
Acf
1 (t) A5 (t)).
11
ct
A1 (t).
Acf
12
ct
(Acf
Act
12
1 (t) A2 (t)).
Cleancf ((t + 1)).
Cleancf (0)
Act
(t).
Cleanct (0)
4
cf
ct
(Clean (t + 1)) A4 (t)).
Act
12
cf
A2 (t).
Acf
12
cf
(Act
InitiallyCleancf
2 (t) A4 (t)).
cf
A2 (t).
InitiallyCleanct
ct
(Act
2 (t) Clean (t + 1)).
Cleanct (t).
W ipe(t).
(Cleancf (t) W ipe(t)).
Acf
4 (t).
(Act
4 (t) W ipe(t)).
Act
6 (t).
Cleanct (t + 1).
cf
(Acf
6 (t) Clean (t + 1)).
cf
A5 (t).
cf
(Act
5 (t) Clean (t + 1)).
cf
A5 (t).
cf
(Act
5 (t) A6 (t)).
ct
Clean (t).
W ipe(t).
(Cleancf (t) W ipe(t)).
Act
6 (t).
(Acf
6 (t) W ipe(t)).
ct
Act
0 A8 .
cf
A0 .
Acf
8 .
Act
act .
Act
act .
ct
Acf
act A8 .
cf
Aact Act
0.



























Acf
9 .
Acf
11 .
ct
(Act
9 A11 ).
ct
A8 .
ct
(Acf
8 A11 ).
ct
A8 .
ct
(Acf
8 A9 ).
cf
Clean (0).
InitiallyCleanct .
(Cleanct (0) InitiallyCleancf ).
Acf
9 .
cf
(Act
9 InitiallyClean ).
Acf
.
9
ct
(Act
9 Clean (0).
ct
A12 .
Cleanct (0).
cf
(Acf
12 Clean (0)).
cf
A11 .
cf
(Act
11 Clean (0)).
Acf
.
11
cf
(Act
11 A12 ).
InitiallyCleancf .
InitiallyCleanct .
Act
12 .
Acf
12 .

























































































































































































Figure 5: Approx{W ipe} (act ), act taken Example 1.1.
let us assume three-valued. prove od(ApproxBU () ) =
od(Approx() ) contradiction. Assume predicate Act
(the proof goes
cf
ct
analogously ) od(Approx() ) |= od(ApproxBU ()
) 6|= Act
. preliminaries recalled model positive inductive definition
. model
least-fixpoint immediate consequence operator
117

fiVlaeminck, Vennekens, Denecker & Bruynooghe

definition thus limit sequence applications immediate consequence
operator. One prove (see, e.g., Denecker & Ternovska, 2004)
apply immediate consequence operator complete definition every step. I.e.,
applying immediate consequence operator subset definition,
longer exists immediate consequence operator give something new, gives
model. Suppose take sequence first apply bottom-up
rules, bottom-up rules applicable try apply top-down rules.
Suppose Act
first atom infer top rules sequence. Obviously
Act
cannot
top-level atom Act

, since top-down rules this.
case study type (sub)formula occurs in, e.g., assume subformula
formula = 0 . fact Act
true, follows body
cf
cf
ct
ct
top rule A0 true, thus Act
A0 true.
ct
Since Act
first atom inferred top-down rule, since
ct
ct
ct
true, also A0 must true. since became true last step
sequence, Act
0 must true already. means applying
cf
bottom-up rules Act
0 A0 true, contradiction fact
three-valued bottom-up rules encode four-valued evaluation.
proof analogous types subformulas.
case four-valued, three-valued longer case
bottom-up rules contribute model (i.e., od(ApproxBU () ) 6=
od(Approx() )). see this, consider following formula P Q, take
four-valued interpretation P = Q = t. one verify
cf
bottom-up rules Approx() infer Act
P Q AP Q true.
However, top-down rules also infer Qcf true. happens
inconsistency inferred certain subformula, propagates back
parse-tree. However, similar above, case study structure
cf
prove (for top formula ) od(ApproxBU () ) |= Act

cf
BU () clearly direct
od(Approx() ) |= Act
. since since Approx
encoding four-valued evaluation, concludes proof.

Appendix C. Proof Proposition 4.3
Proof. Take witness satisfiability APP (F ). First let us remark
Open(Approx (1 2 ) {Act
1 t}) = . fact witness
satisfiability APP (F ) know model definition extending concf
tains Act
construction Approx (1 2 ) must also contain either A1
Act
2 .
Assume first Acf
1 true . application Theorem 3.2 (where take
= {1 } 0 = 1 ) gives: extends |= 1 , 6|= 1 , assumption
|= 1 results contradiction hence 6|= 1 , case 1 2 holds
every extending I, thus witness satisfiability F .
Next, assume Act
2 true . applying Theorem 3.2 (where time
= {1 } 0 = 2 ) gives: extends |= 1 |= 2 , hence also
118

fiAn Approximative Inference Method

case 1 2 hold every extending I, means witness
satisfiability F .

Appendix D. Proof Theorem 5.2
key ingredient proof Theorem 5.2 following property Appct/pt ().
Oct/pt
immediate consequence operator TApp
ct/pt () two-valued interpretations simulates
(O ,O )

immediate consequence operator 1 2 four-valued interpretations original
definition. made precise following lemma.
Definition D.1. pair -interpretations (I, J), use t(I, J) denote ct/pt interpretation (I, J)ct/pt .
Lemma D.1. (O1 , O2 ) (I, J),
(O1 ,O2 )



(O1 ,O2 )

Proof. Let (I 0 , J 0 )

t(O ,O )

(I, J) = t1 (TApp1ct/pt2 () (t(I, J))).
t(O ,O )

(I, J) let F = TApp1ct/pt2 () (t(I, J)). first show

F |ct = 0 . Since F |ct depends rules Appct/pt () predicate ct ,
discard rules head pt . result, left single copy
positive occurrences atoms replaced ct variant negative
ones pt variant. implies evaluation bodies remaining
rules according t(O1 I, O2 J) identical evaluation bodies
original rules (O1 I, O2 J) construction 0 , thus proving equality.
proof remaining equality F |pt = J 0 analoguous.
Proof Theorem 5.2. First, recall that, given partial knowledge (O1 , O2 ), threevalued well-founded models , resp. Appct/pt () least fixpoints operators
(O ,O )
t(O ,O2 )
ST 1 2 resp. ST App1ct/pt
(note since t(O1 , O2 ) two-valued, abuse notation
()
rest proof denote two-valued pair (t(O1 , O2 ), t(O1 , O2 ))
t(O1 , O2 )).
Now, latter operator rather peculiar, sense actually juggling four
different interpretations original alphabet . detail, element
domain looks like this:


Ict
Jct
(I, J) = , .
Ipt
Jpt
Ict Jct interpret alphabet ct , Ipt Jpt interpret pt . apply
t(O ,O2 )
operator ST App1ct/pt
, obtain new pair:
()

0
0
Ict
Jct
(I 0 , J 0 ) = , .
0
0
Ipt
Jpt


119

fiVlaeminck, Vennekens, Denecker & Bruynooghe

0
0
general definition ST
construction, obvious Ict Ipt depends
Jct Jpt . However, particular case, operator exhibits even structure.
(O ,O2 )
operator STApp1ct/pt
(J) uses argument J fixed interpretation negative
()
occurrences, remains constant throughout least fixpoint computation
positive occurrences. Now, Appct/pt () contains two copies interact
negative occurrences (that is, occurrences pt predicate body rule
ct predicate head always negative ones, vice versa). means
long keep interpretation negative occurrence fixed constant value J,
0 , discard
two copies interact all. Consequently, construct Ict
rules pt predicate head. means left rules whose head
ct predicate whose body contains positive occurrences ct predicates
0 depends J . Moreover,
negative occurrences pt predicates. Therefore, Ict
pt
0 map symbols back original alphabet (let
value Ict
0 ) = ST (O1 ,O2 ) (orig(J )). Similarly,
orig( ct ) = orig( pt ) = ), orig(Ict
pt

also obtain that:
(O1 ,O2 )

(orig(Jct )),

(O1 ,O2 )

(orig(Ipt )),

(O1 ,O2 )

(orig(Ict )).

0
orig(Ipt
) = ST

0
orig(Jct
) = ST

0
orig(Jpt
) = ST

words,
(O ,O2 )

0
0
(orig(Ict
), orig(Jpt
)) = ST 1
0
0
(orig(Ipt
), orig(Jct
)) = ST

(orig(Ict ), orig(Jpt )),

(O1 ,O2 )
(orig(Ipt ), orig(Jct )).


Now, consider construction well-founded model Appct/pt (),
sequence form:

1

2



0
0
1
2

Ict
Jct
Ict
Jct
Ict
Jct
Ict
Jct
, 7 , 7 , 7 7 , .
0
0
1
1
2
2


Ipt
Jpt
Ipt
Jpt
Ipt
Jpt
Ipt
Jpt
Let (I , J )i0 well-founded model construction original definition , i.e.,
0 = f
predicates P/n Def () domain tuples Dn (P (d))
J 0 = t, (I n+1 , J n+1 ) = ST (O1 ,O2 ) (I n , J n ) n 0. easy see
(P (d))

0 J 0 ) = (I , J ). provides base case, equation provides
t1 (Ict
0 0
pt
J ). words,
inductive step prove that, i, (I , J ) = t1 (Ict
pt
well-founded model construction original definition tracked elements
well-founded model construction Appct/pt ():
0

1

2



Ict

Ict

Ict

Ict

, 7 , 7 , 7 7 , .
0
1
2


Jpt

Jpt

Jpt

Jpt
diagonal? t1 (J0ct I0ct ) = (>, ),
precise element >p lattice pairs interpretations. Therefore, find
120

fiAn Approximative Inference Method

(O ,O )

diagonal actually tracks construction greatest fixpoint ST 1 2 .
Combining two results, see (L1 , L2 ) (G1 , G2 ) least greatest
fixpoint, respectively, well-founded model Appct/pt () looks like this:


L1
G1
, .
G2
L2
Note unique four-valued stable fixpoint least greatest stable fixpoint
hence also well-founded fixpoint. immediately concludes proof.

Appendix E. Proof Proposition 5.3
Lemma E.1. Given -definition , FO-formula . Let subset 0 (a
renamed copy ). Consider definition
= Appct/pt () {P ct P 0ct }P {Opt O0pt }OOpen(0 ) .
Assume three-valued interpretation approximates models .
holds W F MI ct/pt (D) also approximates , i.e.,
W F ct/pt (D), |= ( ) P (d),

- P ct (d)

6 W F ct/pt (D), |= ( ) P (d).

- P pt (d)

Proof. prove induction well-founded model construction alternative
one described paper, found work Denecker
Vennekens (2007). Assume induction sequence (Ii )0in , four-valued Def (D)interpretations I0 interpretation everything completely unknown,
= W F MI ct/pt (D). prove every Ii sound approximation .
trivially case n = 0. assume Ik sound approximation
. prove also case Ik+1 . need prove two things
cannot true Ik+1 P (d)
true models ,
this: first, atom P ct (d)
pt
cannot false Ik+1 false models
second, atom P (d)
. prove cases contradiction.
start first case. assume k-th well-founded induction step,
incorrectly deduced, i.e.,
certain predicate P domain tuple D, P ct (d)

inferred k-th step,
exists model |= , s.t. 6|= P (d). Since P ct (d)
ct
is, (P )ct [d]
already made
means body rule defining P (d),
true previous step. induction hypothesis tells us every model
semantics inductive definitions says
holds |= P [d],
also |= P , contradiction assumption.
Next, consider second case. time, assume k-th well-founded
inferred false, exists model , s.t.
induction step, P pt (d)

|= P (d). Now, using alternative version well-founded semantics,
two ways domain atom become false. Indeed, domain atom become false
121

fiVlaeminck, Vennekens, Denecker & Bruynooghe

body defining rule already false, part unfounded
set.
made false body defining rule (P )pt already false,
P pt (d)
argument completely analogous one - using induction hypothesis -

gives contradiction assumption. now, left prove, P pt (d)
pt

cannot incorrectly made false application unfounded set rule. P (d)
made false application unfounded set rule, means
set U atoms, unknown Ik , made false bodies
rules defining atoms, Kleene evaluation bodies returns false. possible
verify always find U contains pt -atoms.
let us take model . obviously also model

. Consider corresponding set U 0 , consisting domain atoms P (d),
pt
pt
pt


P (d) U S. every atom Q [d] body (P ) set U S, induction
(Qpt [d])
Ik . Similarly, every atom Qct
hypothesis actually tells us (Q[d])
pt
ct

k (Q[d])
. Now, since Qpt atoms occur
body (P ) , says (Q [d])
positively Qct negatively (P )pt , follows interprets literals
U 0 false way Ik . Thus, U 0 also unfounded set (indeed,
turning atoms U 0 false make bodies defining rules false),
contradiction assumption, concludes
thus |= P (d),
proof lemma.

Proof Proposition 5.3. proof proposition easy proof induction
construction well-founded model , using lemma above,
soundness Approx().

References
Baral, C. (2003). Knowledge Representation, Reasoning, Declarative Problem Solving.
Cambridge University Press, New York, NY, USA.
Baral, C., Gelfond, M., & Kosheleva, O. (1998). Expanding queries incomplete databases
interpolating general logic programs. J. Log. Program., 35 (3), 195230.
Baral, C., Kreinovich, V., & Trejo, R. (2000). Computational complexity planning
approximate planning presence incompleteness. Artif. Intell., 122 (1-2), 241
267.
Belnap, N. D. (1977). useful four-valued logic. Dunn, J. M., & Epstein, G. (Eds.),
Modern Uses Multiple-Valued Logic, pp. 837. Reidel, Dordrecht. Invited papers
Fifth International Symposium Multiple-Valued Logic, held Indiana
University, Bloomington, Indiana, May 13-16, 1975.
Clark, K. L. (1978). Negation failure. Logic Data Bases, pp. 293322. Plenum
Press.
Denecker, M., Cortes Calabuig, A., Bruynooghe, M., & Arieli, O. (2010). Towards logical
reconstruction theory locally closed databases. ACM Transactions Database
Systems, 35 (3), 22:122:60.
122

fiAn Approximative Inference Method

Denecker, M., & Ternovska, E. (2004). logic non-monotone inductive definitions
modularity properties. Lifschitz, V., & Niemela, I. (Eds.), LPNMR, Vol. 2923
LNCS, pp. 4760. Springer.
Denecker, M., & Ternovska, E. (2007). Inductive situation calculus. Artificial Intelligence,
171 (5-6), 332360.
Denecker, M., & Ternovska, E. (2008). logic nonmonotone inductive definitions. ACM
Transactions Computational Logic (TOCL), 9 (2), Article 14.
Denecker, M., & Vennekens, J. (2007). Well-founded semantics algebraic theory
non-monotone inductive definitions. Baral, C., Brewka, G., & Schlipf, J. S. (Eds.),
LPNMR, Vol. 4483 LNCS, pp. 8496. Springer.
Denecker, M., Vennekens, J., Bond, S., Gebser, M., & Truszczynski, M. (2009). second
Answer Set Programming competition. Erdem, E., Lin, F., & Schaub, T. (Eds.),
LPNMR, Vol. 5753 LNCS, pp. 637654. Springer.
Doherty, P., Magnusson, M., & Szalas, A. (2006). Approximate databases: support tool
approximate reasoning. Journal Applied Non-Classical Logics, 16 (1-2), 87118.
E. Giunchiglia, Y. L., & Maratea, M. (2011). Cmodels homepage. http://www.cs.utexas.
edu/users/tag/cmodels.html.
Fagin, R. (1974). Generalized first-order spectra polynomial-time recognizable sets.
Complexity Computation, 7, 4374.
Feferman, S. (1984). Toward useful type-free theories. Journal Symbolic Logic, 49 (1),
75111.
Gelfond, M., & Lifschitz, V. (1988). stable model semantics logic programming.
Kowalski, R. A., & Bowen, K. A. (Eds.), ICLP/SLP, pp. 10701080. MIT Press.
Immerman, N. (1998). Descriptive Complexity. Springer Verlag.
Kleene, S. C. (1952). Introduction Metamathematics. Van Nostrand.
Liu, Y., & Levesque, H. J. (1998). completeness result reasoning incomplete
first-order knowledge bases. KR, pp. 1423.
Marien, M., Wittocx, J., & Denecker, M. (2006). IDP framework declarative problem
solving. Search Logic: Answer Set Programming SAT, pp. 1934.
McDermott, D. (1987). critique pure reason. Computational Intelligence, 3, 151160.
Mitchell, D. G., & Ternovska, E. (2005). framework representing solving NP
search problems. Veloso, M. M., & Kambhampati, S. (Eds.), AAAI, pp. 430435.
AAAI Press / MIT Press.
Niemela, I., Simons, P., & Syrjanen, T. (2000). Smodels: system answer set programming. Proceedings 8th International Workshop Non-Monotonic Reasoning,
Breckenridge, Colorado, USA. CoRR, cs.AI/0003033.
Palacios, H., & Geffner, H. (2009). Compiling uncertainty away conformant planning
problems bounded width. Journal Artificial Intelligence Research (JAIR), 35,
623675.
123

fiVlaeminck, Vennekens, Denecker & Bruynooghe

Son, T. C., Tu, P. H., Gelfond, M., & Morales, A. R. (2005). approximation action
theories application conformant planning. Baral, C., Greco, G., Leone,
N., & Terracina, G. (Eds.), LPNMR, Vol. 3662 LNCS, pp. 172184. Springer.
Tamaki, H., & Sato, T. (1984). Unfold/fold transformations logic programs. ICLP,
pp. 127138.
Tseitin, G. S. (1968). complexity derivation propositional calculus. Slisenko,
A. O. (Ed.), Studies Constructive Mathematics Mathematical Logic II, pp. 115
125. Consultants Bureau, N.Y.
Turner, H. (2002). Polynomial-length planning spans polynomial hierarchy. JELIA,
pp. 111124.
van Fraassen, B. (1966). Singular terms, truth-value gaps free logic. Journal Philosophy, 63 (17), 481495.
Van Gelder, A. (1993). alternating fixpoint logic programs negation. Journal
Computer System Sciences, 47 (1), 185221.
Vlaeminck, H., Wittocx, J., Vennekens, J., Denecker, M., & Bruynooghe, M. (2010).
approximate method solving problems. Fisher, M., van der Hoek, W.,
Konev, B., & Lisitsa, A. (Eds.), JELIA, Lecture Notes Computer Science, pp.
326338. Springer.
Wittocx, J. (2010). Finite Domain Symbolic Inference Methods Extensions FirstOrder Logic. Ph.D. thesis, Department Computer Science, K.U.Leuven, Leuven,
Belgium.
Wittocx, J., Denecker, M., & Bruynooghe, M. (2010). Constraint propagation extended
first-order logic. CoRR, abs/1008.2121.
Wittocx, J., Marien, M., & Denecker, M. (2008). Approximate reasoning first-order logic
theories. Brewka, G., & Lang, J. (Eds.), KR, pp. 103112. AAAI Press.

124

fiJournal Artificial Intelligence Research 45 (2012) 197255

Submitted 12/11; published 10/12

Reasoning Ontologies Hidden Content:
Import-by-Query Approach
Bernardo Cuenca Grau
Boris Motik

bernardo.cuenca.grau@cs.ox.ac.uk
boris.motik@cs.ox.ac.uk

Department Computer Science, Oxford University
Wolfson Building, Parks Road, Oxford OX1 3QD UK

Abstract
currently growing interest techniques hiding parts signature
ontology Kh reused another ontology Kv . Towards goal,
paper propose import-by-query framework, makes content Kh
accessible limited query interface. Kv reuses symbols Kh certain
restricted way, one reason Kv Kh accessing Kv query interface.
map landscape import-by-query problem. particular, outline
limitations framework prove certain restrictions expressivity Kh
way Kv reuses symbols Kh strictly necessary enable reasoning
setting. also identify cases reasoning possible present suitable
import-by-query reasoning algorithms.

1. Introduction
Ontologiesformal conceptualizations domain interesthave become increasingly
important computer science. play central role many applications,
Semantic Web biomedical information systems. widely used ontology languages Web Ontology Language (OWL) (Horrocks, Patel-Schneider, & van Harmelen, 2003) revision OWL 2 (Cuenca Grau, Horrocks, Motik, Parsia, Patel-Schneider,
& Sattler, 2008), standardized World Wide Web Consortium (W3C).
formal underpinning OWL family languages provided description logics (DLs) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2007)knowledge
representation formalisms well-understood computational properties.
Constructing ontologies labor-intensive task, reusing (parts of) well-established
ontologies seen key reducing ontology development cost. Consequently, problem
ontology reuse recently received significant attention (Stuckenschmidt, Parent, &
Spaccapietra, 2009; Lutz & Wolter, 2010; Lutz, Walther, & Wolter, 2007; Cuenca Grau,
Horrocks, Kazakov, & Sattler, 2008, 2007; Doran, Tamma, & Iannone, 2007; Jimenez-Ruiz,
Cuenca Grau, Sattler, Schneider, & Berlanga Llavori, 2008).
discuss problems ontology reuse means example health-care
domain. particular, ontologies currently used several countries describe
electronic patient records (EPR). representation patients data typically involves
ontological descriptions human anatomy, medical conditions, drugs treatments,
on. latter domains already described well-established reference ontologies, SNOMED-CT, GALEN, Foundational Model Anatomy (FMA).
order save resources, increase interoperability applications, rely experts
c
2012
AI Access Foundation. rights reserved.

fiCuenca Grau & Motik

knowledge, reference ontologies reused whenever possible.
example, assume reference ontology Kh describes concepts ventricular septum defect; then, one might reuse terms Kh order define ontology
Kv concepts patients ventricular septum defect, might
embedded EPR application.
enable ontology reuse, OWL provides importing mechanism: ontology Kv
import another ontology Kh , result logically equivalent Kv Kh . OWL
reasoners deal imports loading ontologies merging contents, thus
requiring physical access axioms Kh . vendor Kh , however, may reluctant
distribute (parts of) contents Kh , might allow competitors plagiarize
Kh . Moreover, Kh may contain information sensitive privacy point view.
Finally, one may want impose varying cost reuse dierent parts Kh .
Rather publishing entire ontology, vendor Kh might want freely
distribute symbols describe organs medical conditions, without distributing
axioms describing symbols. Furthermore, vendor might want completely
hide sensitive information Kh , information treatments. should,
however, possible reuse published part Kh without aecting ontologys
consequences; is, part Kh used construct ontology Kv , query
q mentioning symbols Kv answered Kv respective part
Kh way would done Kv Kh . stipulate Kh
publicly available, call ontology Kh hidden and, analogy, call Kv visible.
Motivated scenarios, several approaches hiding subset signature
Kh developed. example, one possible approach publish -interpolant
Kh ontology contains symbols coincides Kh logical
consequences formed using symbols (Konev, Walther, & Wolter, 2009; Wang,
Wang, Topor, Pan, & Antoniou, 2009; Wang, Wang, Topor, & Pan, 2008; Wang, Wang,
Topor, & Zhang, 2010; Wang et al., 2008; Lutz & Wolter, 2011; Nikitina, 2011). Publishing
interpolant ensures sensitive information Kh (i.e., information
symbols Kh mentioned interpolant) exposed way; furthermore,
interpolants preserve consequences symbols additional advantage
developers Kv reason union Kv interpolant using o-theshelf reasoners. interpolation approach may, however, exhibit several drawbacks. First,
interpolant may exist Kh expressed relatively weak ontology language
satisfies certain syntactic conditions (Konev et al., 2009). Second, although interpolants
preserve logical consequences formed using symbols , robust
replacement (Sattler, Schneider, & Zakharyaschev, 2009)that is, union Kv
-interpolant Kh guaranteed yield consequences Kh Kv query
q involving symbols Kv . Finally, -interpolant Kh exponentially
larger Kh , may reveal information strictly needed. refer
reader Section 7 detailed discussion related work.
paper, propose novel approach ontology reuse addresses problems
outlined making Kh accessible via limited query interface called oracle.
oracle advertises public subset signature Kh (e.g., symbols describing organs
medical conditions), answer queries Kh expressed particular
query language use symbols . certain assumptions, so198

fiReasoning Ontologies Hidden Content

called import-by-query algorithm reason Kv Kh (e.g., determine satisfiability
Kv Kh ) posing queries oracle Kh , without accessing axioms
Kh . Furthermore, reasoning performed without making axioms Kv
available Kh , beneficial Kv might also contain sensitive information
privacy point view. Finally, framework applicable even cases
relevant interpolant Kh exist.
order achieve benefits, however, Kv must reuse symbols
syntactically restricted way, formal properties import-by-query algorithms
specific restrictions necessary import-by-query algorithm exist depend
oracle query language ontology languages used express Kv Kh .
paper, explore properties import-by-query reasoning languages ranging
lightweight description logic EL (Baader, Brandt, & Lutz, 2005) expressive logic
ALCHOIQ (Horrocks & Sattler, 2005), combined following types oracles.
Queries concept satisfiability oracles concepts constructed using symbols
expressed particular DL; query, oracle decides satisfiability
query concept w.r.t. Kh .
Queries ABox satisfiability oracles ABoxes constructed using symbols
; query, oracle decides satisfiability query ABox w.r.t. Kh .
Queries ABox entailment oracles consist ABox assertion, constructed using symbols ; query, oracle determines whether
assertion entailed Kh query ABox.
Concept satisfiability, ABox satisfiability, ABox entailment implemented
state-of-the-art DL reasoners, mentioned query languages seem like
natural foundation practical implementations framework.
main contributions paper follows:
1. present import-by-query framework, formalize notions oracle
import-by-query algorithm, establish connections import-by-query
algorithms based dierent types oracles.
2. explore limitations framework wide range description logics
formulate precise conditions import-by-query algorithms fail exist.
3. identify sucient conditions visible ontology Kv import-byquery algorithm obtained.
4. present general hypertableau-based (Motik, Shearer, & Horrocks, 2009) importby-query algorithm relies ABox satisfiability oracles applicable
Kv Kh given expressive description logic ALCHIQ (Horrocks & Sattler,
1999), provided Kv satisfies sucient conditions.
5. general algorithm, however, unlikely suitable practice due high
degree nondeterminism. Therefore, present practical (goal-oriented) variant
applicable whenever Kh expressed Horn DL. algorithm
199

fiCuenca Grau & Motik

readily applied ontologies expressed lightweight description logic EL,
guaranteed computationally optimal. Therefore, also present
practical computationally optimal algorithm used Kv Kh
expressed EL.
6. establish lower bounds size number queries importby-query algorithm may need ask oracle order solve reasoning task.
results provide flexible useful ways ontology designers ensure selective
access ontologies, well family reasoning algorithms provide starting
point implementation optimization. Furthermore, believe techniques
also adapted settings, distributed ontology reasoning, collaborative
ontology development scenarios ontology developers restricted access
parts ontology developed others.

2. Preliminaries
section, recapitulate description logic notation used paper, present
overview various hypertableau reasoning algorithms description logics (Motik et al.,
2009), recapitulate various notions modular ontology reuse (Lutz, Walther, &
Wolter, 2007; Cuenca Grau, Horrocks, Kazakov, & Sattler, 2008; Konev, Lutz, Walther, &
Wolter, 2008).
2.1 Description Logics
syntax description logic ALCHOIQ defined w.r.t. pairwise-disjoint countably
infinite sets atomic concepts NC , atomic roles NR , named individuals NI . Set NC
contains distinguished infinite subset NC nominal concepts (or simply nominals).
role either atomic role inverse role R R atomic role.
set concepts smallest set containing , A, C, C1 C2 , R.C (existential
restriction), n R.C (cardinality restriction), atomic concept, C, C1 ,
C2 concepts, R role, n nonnegative integer. Furthermore, , C1 C2 , R.C,
n R.C abbreviations , (C1 C2 ), (R.C), ( n+1 R.C),
respectively. also often treat concepts form R.C abbreviations 1 R.C.
concept inclusion axiom form C1 C2 C1 C2 concepts, concept
equivalence C1 C2 abbreviation C1 C2 C2 C1 , concept definition
concept equivalence form C atomic concept. role inclusion
axiom form R1 R2 R1 R2 roles. TBox axiom either concept
inclusion axiom role inclusion axiom. TBox finite set TBox axioms.
assertion form C(a), R(a, b), R(a, b), b, b, C concept, R role,
b individuals. ABox finite set assertions. ABox normalized
contains assertions form A(a), A(a), R(a, b), R(a, b), b,
atomic concept R atomic role. axiom either TBox axiom
assertion. knowledge base K = consists TBox ABox A.

200

fiReasoning Ontologies Hidden Content

Table 1: Model-Theoretic Semantics ALCHOIQ
Interpretation Roles
(R )I = {y, x | x, RI }
Interpretation Concepts
=
(C)I = \ C
(C1 C2 )I = C1I C2I
(R.C)I = {x | : x, RI C }
( n R.C)I = {x | {y | x, RI C } n}
Satisfaction Axioms Interpretation








|= C
|= R1 R2
|= C(a)
|= R(a, b)
|= R(a, b)
|= b
|= b









C DI
R1I R2I
aI C
aI , bI RI
aI , bI
/ RI


=b
aI = bI

signature set atomic concepts atomic roles. concept, role,
axiom, set axioms, signature , written sig(), set atomic concepts
atomic roles occurring .1
cardinality set written S. interpretation = (I , ) consists
nonempty domain set function assigns object aI individual
a, set AI atomic concept implies AI = 1,
relation RI atomic role R. Table 1 defines extension roles
concepts, well satisfaction axioms I. interpretation model
K, written |= K, satisfies axioms K; exists, K satisfiable.
concept C satisfiable w.r.t. K model K exists C = .
Sometimes, nominal concepts defined form {a} individual,
concept interpreted ({a})I = {aI }; is, nominal concept contains precisely
given individual. drawback definition blurs distinction
concepts individuals syntactic level. distinction important
import-by-query framework since framework supports sharing concepts,
individuals. paper thus use given alternative definition, nominals
special atomic concepts singleton interpretation. well known
two definitions equally expressive (Baader et al., 2007).
results use general notion description logic. Formally, define
description logic DL pair consisting set concepts set knowledge bases.
call elements former set DL-concepts elements latter set DLknowledge bases. concept DL-knowledge base must DL-concept. DL-TBox
(resp. DL-ABox ) DL-knowledge base containing assertions (resp. TBox axioms).
1. Note treating nominals special atomic concepts (and individuals); hence, sig()
includes nominals, individuals occurring .

201

fiCuenca Grau & Motik

DL-TBox axiom (resp. DL-assertion) TBox axiom (resp. assertion) occurs
DL-knowledge base. description logic DL1 fragment DL2 (or, conversely,
DL2 extends DL1 ) DL1 -concept DL2 -concept DL1 -knowledge base
DL2 -knowledge base. Since unqualified notions concept knowledge base
defined ALCHOIQ, definitions imply description logic considered
paper fragment ALCHOIQ.
Let DL1 DL2 description logics. say DL1 allows DL2 -definitions
if, DL1 -knowledge base K, atomic concept A, DL2 -concept C,
K {A C} DL1 -knowledge base. Furthermore, DL1 finite model
property satisfiable DL1 -knowledge base model finite domain.
description logic ALC obtained ALCHOIQ disallowing nominal concepts
(O), inverse roles (I), role inclusion axioms (H), cardinality restrictions (Q).
description logics ALC ALCHOIQ named appending combinations
letters O, H, I, Q ALC.
DL EL (Baader et al., 2005) (resp. FL0 , see Baader et al., 2007) obtained
ALC allowing concepts form , , A, C1 C2 , R.C (resp. R.C)
R atomic, allowing assertions form C(a) R(a, b), C
EL (resp. FL0 ) concept R atomic role. recent years, significant eort
devoted development DL languages good computational properties,
EL, DL-Lite (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007), Horn-SHIQ
(Hustadt, Motik, & Sattler, 2005). ALCHIQ knowledge base Horn expressed
Horn-SHIQ fragment ALCHIQ.
ABox A, G(A) denote graph whose nodes precisely individuals
occurring A, contains undirected edge individuals b
= b b occur together assertion A. Individuals b
connected b connected G(A); furthermore, connected pairs
individuals occurring connected. ABox connected component
G(A ) connected component G(A).
2.2 Hypertableau Reasoning Algorithm
hypertableau calculus Motik et al. (2009) decides satisfiability ALCHOIQ
knowledge base K. show Section 4.1, presence nominals precludes
existence import-by-query algorithm; hence, section present overview
simplified version algorithm applicable K ALCHIQ knowledge base.
algorithm first preprocesses K set rules Rimplications interpreted first-order semanticsand normalized ABox K equisatisfiable
R A. Preprocessing consists three steps. First, transitivity axioms eliminated
K encoding using concept inclusions. Second, axioms normalized
complex concepts replaced atomic ones way similar structural transformation first-order logic. Third, normalized axioms translated rules
using correspondence description first-order logic. omit details
preprocessing sake brevity; Motik et al. (2009) present relevant
details. Preprocessing produces so-called HT-rulessyntactically restricted rules

202

fiReasoning Ontologies Hidden Content

hypertableau calculus guaranteed terminate; precise syntactic form HT-rules
described Section 2.2.1.
preprocessing, satisfiability RA decided using hypertableau calculus,
described Section 2.2.2.
2.2.1 HT-Rules
Let NV set variables disjoint set individuals NI . atom expression
form C(s) (a concept atom), R(s, t) (a role atom), (an equality atom),
s, NV NI , C concept, R role. rule expression form
(1)

U1 . . . Um V1 . . . Vn

Ui Vj atoms, 0, n 0. Conjunction U1 . . . Um called body,
disjunction V1 . . . Vn called head rule. empty body empty
head written , respectively. Rules interpreted universally quantified
FOL implications usual way. rule Horn contains one head atom.
HT-rule rule form




Ai (x) Rij (x, yi ) Sij (yi , x)


Bij (yi )

(2)

Ci (x) Rij (x, yi ) Sij (yi , x) Dij (yi ) yi yj
, atomic roles; , B , atomic concepts;
Rij , Sij , Rij

ij
ij
ij
Ci either atomic concepts concepts form n R.A n R.A. addition,
variable yi occurring HT-rule required occur body atom form
Rij (x, yi ) Sij (yi , x). Intuitively, body head HT-rules seen
star-shaped: center variable x represents center star, branch variables yi
connected center role atoms. shape ensures satisfiable
HT-rules always tree-like modela property used explain
good computational properties many DLs.
Motik et al. (2009) shown, preprocessing K produces equisatisfiable
set HT-rules normalized ABox; furthermore, K Horn, resulting set
contains Horn HT-rules. Furthermore, certain description logic constructors
used K, R satisfies certain syntactic restrictions discussed next.

K use cardinality restrictions, HT-rule R contains atom
form yi yj head.
K use inverse roles, HT-rule R contains atom form
(y , x) head atom form (y , x) body.
Sij

ij
K use role hierarchies, HT-rule R contains role atom
head.
example, consider following knowledge base K corresponding set
HT-rules R obtained K.
R.B
R.C



A(x) R.B(x)



A(x) R.C(x)
203

(3)
(4)

fiCuenca Grau & Motik



1 R.

R(x, y1 ) R(x, y2 ) y1 y2



BC

B(x) C(x) D(x)



R.D E

R(x, y) D(y) E(x)

(5)
(6)
(7)

Note R set Horn HT-rules. Note also K uses cardinality restriction
1 R., R contains rule equality atom head. Furthermore, K
use role hierarchies, rule R contains role atom head. Finally, K
use inverse roles, role atom occurring body rule R contains center
variable x first position branch variable yi second position.
applied EL knowledge base, transformation Motik et al. (2009)
produces EL-rulesHT-rules form (8) C either atomic concept
concept form R.A atomic concept.


mi
k




Ri (x, yi )
Ai (x)
Bij (yi ) C(x)
(8)
i=1

i=1

j=1

Note rules previous example except third one (which uses equality
head) EL-rules.
2.2.2 Hypertableau Calculus HT-Rules

Given arbitrary set HT-rules R normalized ABox A, satisfiability R
decided using calculus described Definition 1.
Definition 1. Individuals. set named individuals NI , set individuals
NX inductively defined smallest set NI NX and, x NX ,
x.i NX integer i. individuals NX \ NI unnamed. individual x.i
successor x, x predecessor x.i; descendant ancestor transitive
closures successor predecessor, respectively.
Pairwise Anywhere Blocking. label LA (s) individual label
LA (s, t) individual pair s, ABox defined follows:
LA (s) = {A | A(s) atomic}
LA (s, t) = {R | R(s, t) A}
Let strict ordering NX containing ancestor relation. induction ,
assign individual blocking status follows.
Individual directly blocked individual following conditions hold,
predecessors t, respectively:
unnamed, blocked, s;2
LA (s) = LA (t) LA (s ) = LA (t );

LA (s, ) = LA (t, ) LA (s , s) = LA (t , t).
2. blocking used ALCHOIQ knowledge bases, individuals also required
unnamed; however, restriction needed ALCHIQ knowledge bases.

204

fiReasoning Ontologies Hidden Content

Hyp-rule

-rule

-rule
-rule

Table 2: Hypertableau Derivation Rules
Derivation Rules HT-rules
1. R form (1)
2. mapping variables individuals exists
2.1 (x) indirectly blocked variable x ,
2.2 (Ui ) 1 m,
2.3 (Vj ) 1 j n,
A1 = {} n = 0;
Aj := {(Vj )} 1 j n otherwise.
1. n R.C(s) blocked
2. individuals u1 , . . . , un exist
{ar(R, s, ui ), C(ui ) | 1 n} {ui uj | 1 < j n} A,
A1 := {ar(R, s, ti ), C(ti ) | 1 n} {ti tj | 1 < j n}
t1 , . . . , tn fresh successors s.
1. = neither indirectly blocked
A1 := mergeA (s t) named descendant t,
A1 := mergeA (t s) otherwise.
1. {A(s), A(s)} {R(s, t), R(s, t)}
neither indirectly blocked
2.
A1 := {}.
-rule EL-rules

R.A(s) {R(s, aA ), A(aA )}
-rule
A1 := {R(s, aA ), A(aA )}

Individual indirectly blocked predecessor blocked.
Individual blocked either directly indirectly blocked.
Pruning Merging. ABox pruneA (s) obtained removing assertions containing descendant s. ABox mergeA (s t) obtained pruneA (s)
replacing assertions.
Clash. ABox contains clash A; otherwise, clash-free.
Derivation Rules. derivation rules consist Hyp-, -, -, -rule
Table 2, which, given R clash-free ABox A, derive ABoxes A1 , . . . , .
Hyp-rule, (U ) obtained U replacing variable x (x). role R
individuals t, function ar(R, s, t) returns assertion R(s, t) R atomic, assertion
S(t, s) R inverse role R = .
Derivation. derivation R pair (T, ) finitely branching
tree labels nodes ABoxes ( i) () = root,
( ii) node t, derivation rule applicable R (t), children
t1 , . . . , tn (t1 ), . . . , (tn ) result applying one derivation rule R
(t). algorithm returns derivation R leaf node labeled
clash-free ABox, f otherwise.
205

fiCuenca Grau & Motik

Hyp-rule similar one hypertableau calculus first-order logic: given
HT-rule form (1) ABox A, Hyp-rule tries unify atoms U1 , . . . , Um
subset assertions A; unifier found, rule nondeterministically
derives (Vj ) 1 j n. example, given rule A(x) R.C(x) D(x)
assertion A(a), Hyp-rule derives either R.C(a) D(a). -rule deals
existential quantifiers; example, given R.C(a), rule introduces fresh individual
derives R(a, t) C(t). -rule deals equality; example, given b,
rule replaces individual assertions individual b. Finally, -rule
detects obvious contradictions A(a) A(a), R(a, b) R(a, b), a.
Since ALCHIQ allows cyclic concept inclusions form C R.C, termination
hypertableau calculus requires blocking mechanism prevent -rule
generating infinite sequences successors. individual directly blocked
another individual t, -rule longer applicable s, prevents introduction
fresh successors s. Furthermore, descendants indirectly blocked,
prevents application rules Table 2 descendants s.
derivation R exists leaf node labeled clash-free ABox
, model R constructed via well-known technique called
unraveling. Models R obtained way called canonical forest models,
Motik et al. (2009) discuss depth properties models.
Let R set HT-rules (3)(7) given Section 2.2.1, let = {A(a), E(a)};
next show demonstrate using hypertableau algorithm R unsatisfiable. applying Hyp-rule A(a), derive R.B(a) R.C(a). Next,
applying -rule R.B(a) derive R(a, t1 ) B(t1 ); applying -rule
R.C(a) derive R(a, t2 ) C(t2 ). Individuals t1 t2 fresh successors
actually form s.1 s.2; however, clarity write simply t1
t2 . applying Hyp-rule R(a, t1 ) R(a, t2 ), derive t1 t2 . Furthermore,
apply -rule t1 t2 , must replace t1 t2 assertions; thus, replace
R(a, t1 ) B(t1 ) R(a, t2 ) B(t2 ), respectively. Next, applying Hyp-rule
B(t2 ) C(t2 ) derive D(t2 ). Next, applying Hyp-rule R(a, t2 ) D(t2 )
derive E(a). Finally, applying -rule E(a) E(a) derive .
thus constructed derivation R whose (only) leaf contains clash,
R unsatisfiable.
2.2.3 Hypertableau Algorithm EL-rules

Since EL knowledge base ALCHIQ knowledge base well, hypertableau
algorithm straightforwardly applied EL KBs. Motik Horrocks (2008) showed,
however, worst-case optimal algorithm obtained modifying -rule.
modified algorithm works set R EL-rules.
following algorithm checks satisfiability R A, R set EL-rules
normalized ABox.
Definition 2. named individual NI atomic concept NC , let aA
fresh individual uniquely associated A. hypertableau algorithm
EL one described Definition 1, derivation rules include
Hyp-, -, -rule Table 2.
206

fiReasoning Ontologies Hidden Content

2.3 Modularity
Let Kv knowledge base reuses knowledge base Kh , let subset
sig(Kh ) reused Kv is, = sig(Kh ) sig(Kv ). often beneficial
Kv reuses Kh modular way; intuitively, case knowledge base Kv
aect meaning symbols (Lutz, Walther, & Wolter, 2007; Cuenca Grau,
Horrocks, Kazakov, & Sattler, 2008; Konev, Lutz, Walther, & Wolter, 2008). Two dierent
notions modularity considered literature, providing dierent formal
account means Kv aect meaning symbols .
knowledge base Kv deductively modular w.r.t. signature if, concepts C
expressed description logic Kv sig(C) sig(D) ,
Kv |= C implies |= C D. is, axioms Kv must give
rise nontrivial logical consequences involve symbols .
knowledge base Kv semantically modular w.r.t. signature if, interpretation = (I , ) symbols , exists interpretation J = (J , J )
= J , X = X J X , J |= Kv . is, axioms Kv
allowed impose constraints interpretation symbols .
Semantic modularity stronger deductive one: Kv semantically modular
w.r.t. , also deductively modular w.r.t. ; converse hold necessarily.
Deciding whether knowledge base Kv deductively semantically modular w.r.t.
signature hard computational problem DLs, often undecidable
(Lutz et al., 2007; Konev et al., 2008). Cuenca Grau, Horrocks, Kazakov, Sattler
(2008) defined several practically useful sucient syntactic conditions guarantee
semantic modularity.

3. Import-by-Query Framework
section introduce framework. first present motivating example,
proceed formalization import-by-query problem.
Consider medical research company (MRC) developed knowledge base
human anatomy. knowledge base contains concepts describing organs Heart
TV (tricuspid valve); medical conditions CHD (congenital heart defect), VSD
(ventricular septum defect), (aortic stenosis); treatments Surgery.
roles part, con, treatment relate organs parts, medical conditions,
treatments, respectively, used define concepts VSD Heart (a
heart ventricular septum defect) Sur Heart (a heart requires surgical
treatment). focus reusing schema knowledge, assume knowledge
base consists TBox Th , shown Table 3. Assume MRC wants
freely distribute information organs conditions, hide information
treatments. Thus, MRC identifies set public symbols Th ; write symbols
bold, remaining private symbols sans serif. MRC want distribute
axioms Th , might allow competitors copy parts Th ; therefore, say
knowledge base Th hidden.
Consider also health-care provider (HCP) reuses Th describe types patients
VSD Patient (patients ventricular septum defect), HS Patient (patients
requiring heart surgery), Patient (patients aortic stenosis), EA Patient (patients
207

fiCuenca Grau & Motik

Table 3: Example Knowledge Bases
Hidden Knowledge Base Th

1
Heart Organ part.TV
2
VSD CHD
3
CHD
4 VSD Heart Heart con.VSD
5 VSD Heart treatment.Surgery
6 Sur Heart Heart treatment.Surgery
Visible Knowledge Base Kv

1 VSD Patient Patient hasOrg.VSD Heart
2 HS Patient Patient hasOrg.Sur Heart
3 Patient Patient hasOrg.(Heart con.AS)
4
Ab TV TV
5
Dis TV Ab TV
6
EA Heart VSD Heart part.Dis TV
7 EA Patient Patient hasOrg.EA Heart
8 Ab TV Heart Heart part.Ab TV
9 TVD Patient Patient hasOrg.Ab TV Heart
Ebsteins anomaly), TVD Patient (patients tricuspid valve defect). Since
TBox Th describe Ebsteins anomaly, HCP defines EA Heart heart
ventricular septum defect displaced tricuspid valve Dis TV ; furthermore,
defines displaced tricuspid valve abnormal, Ab TV Heart heart abnormal tricuspid valve. general, HCPs knowledge base could contain ABox assertions,
denote knowledge base Kv call visible. axioms Kv shown
Table 3, private symbols Kv written italic. HCP use combined
knowledge base Kv Th deduce VSD Patient HS Patient (patients ventricular septum defect require heart surgery) EA Patient TVD Patient (patients
Ebsteins anomaly kind patients tricuspid valve defect).
support scenarios, propose import-by-query framework. Instead publishing (a subset of) axioms Th , MRC publish oracle Th service
advertises set public symbols Th query language L, answer
L-queries Th provided queries use symbols . so-called import-byquery algorithm reason Kv Th (e.g., determine satisfiability Kv Th )
without physical access contents Th , asking queries oracle.
existence algorithm, however, depends oracles query language,
DLs used express Kv Th , way symbols reused Kv .
One popular query languages description logics concept satisfiability,
available DL reasoners known us. thus natural consider concept
satisfiability oracles, advertise signature check satisfiability w.r.t. Th
(not necessarily atomic) concepts formed using symbols . Later show
import-by-query algorithms based concept satisfiability oracles exist rather strong
208

fiReasoning Ontologies Hidden Content

restrictions imposed way Kv reuses symbols ; roughly speaking,
possible mix roles concepts private Kv existential universal
restrictions. example, means axioms 6 8 Table 3 would
allowed Kv . overcome limitations concept satisfiability oracles, consider two
additional types (closely related) oracles powerful oracles based
concept satisfiability. ABox satisfiability oracle given ABox sig(A) ,
checks satisfiability Th . ABox entailment oracle given ABox
assertion sig(A) sig() , checks whether Th |= . ABox
satisfiability entailment implemented state-of-the-art DL reasoners,
oracles based inferences seem natural.
practice, natural express oracle queries DL Th ; however,
sake generality allow queries expressed arbitrary description logic L.
Intuitively, allows Kv learn structure models Th ,
allows us obtain general results nonexistence import-by-query algorithms.
Definition 3 formally introduces dierent types oracles.
Definition 3. Let Th TBox, let signature, let L description logic.
concept satisfiability oracle Th , , L Boolean function cTh ,,L that,
L-concept C sig(C) , returns C satisfiable w.r.t. Th .
ABox satisfiability oracle Th , , L Boolean function aTh ,,L that,
connected L-ABox sig(A) , returns Th satisfiable.
ABox entailment oracle Th , , L Boolean function eTh ,,L that,
connected L-ABox sig(A) L-assertion mentions
individuals sig() , returns Th |= .
use generic term oracle either concept satisfiability, ABox satisfiability,
ABox entailment oracle. Furthermore, L description logic Th ,
abbreviate Th ,,L Th , . Finally, often refer oracle arguments (i.e.,
concepts C, ABoxes A, pairs A, case concept satisfiability, ABox
satisfiability, ABox entailment oracles, respectively) oracle queries.
next formally define import-by-query algorithms using well-known notion
oracle Turing machine. precise definition latter given Papadimitriou (1993);
next present informal overview main ideas. oracle Turing machine
separate query tape, write arbitrary strings given alphabet.
point time, enter special state q? , upon black-box oracle checks
whether string currently written query tape belongs language associated
; case, enters special state qyes , otherwise enters special
state qno . allows oracles answers aect computation . combination
usually written . definition assumes computation
depends input oracles answers; is, 1 2 two distinct
oracles, computations 1 indistinguishable computations 2
1 2 return answers queries encountered computations. rest
paper, make assumptions type : reasonable Turing
machine model used. merely assume equipped suitable notion
run captures computation input. run (but
need to) accept reject input.
209

fiCuenca Grau & Motik

Definition 4. class inputs C class triples form C , KvC , ThC C
signature, KvC knowledge base, ThC TBox sig(KvC ) sig(ThC ) C .
triple C called input.
import-by-query algorithm description logic L class inputs C based
oracles type x {a, e, c} oracle Turing machine ibqx combined
oracle type x. input , Kv , Th C following properties must satisfied,
ibqx [Th , , L] combination ibqx oracle xTh ,,L :
1. whenever ibqx [Th , , L] enters state q? run, string query tape encodes
query accepted xTh ,,L ;
2. ibqx [Th , , L] accepting run Kv Kv Th satisfiable;
3. run ibqx [Th , , L] Kv finite.

Intuitively, transition relation ibqx takes account possible answers
oracle type x, ibqx executable actual oracle unknown. Thus,
ibqx seen computer program particular subroutine missing. Given
input , Kv , Th C, parameterize ibqx xTh ,,L obtain ibqx [Th , , L],
latter Turing machine freely applied Kv .
rest paper, whenever oracle type explicitly given, discussion
applies oracle types. consider various classes inputs,
defined using following formulation:
C largest class triples C , KvC , ThC sig(KvC ) sig(ThC ) C
C , KvC , ThC satisfy condition.

Usually, however, abbreviate formulations follows:

C[C , KvC , ThC ] class inputs C , KvC , ThC satisfy condition.

Definition 4 straightforwardly implies following property, essentially reformulates idea runs Turing machine determined oracles
answers, oracles themselves.
Proposition 1. Let ibq import-by-query algorithm description logic L
class inputs C, let , Kv , Th1 arbitrary input C, let Q1 , . . . , Qn
oracle queries encountered possible runs ibq[Th1 , , L] Kv . Then, Th2
, Kv , Th2 C 1 ,,L (Qi ) = 2 ,,L (Qi ) 1 n, run
h
h
ibq[Th1 , , L] Kv run ibq[Th2 , , L] Kv vice versa.

Section 4 identify DLs defining oracle query language classes
inputs import-by-query algorithm based oracles particular type exists.
following proposition shows suces prove nonexistence results
expressive DL smallest class inputs; then, analogous results hold
weaker DL larger class inputs.
Proposition 2. Let L1 description logic let L2 fragment L1 ; let C1 C2
classes inputs triple C1 also belongs C2 ; let x {a, c, e}
oracle type. import-by-query algorithm L1 C1 based oracles type
x, also import-by-query algorithm L2 C2 based oracles type x.
210

fiReasoning Ontologies Hidden Content

Proof. prove contrapositive claim. Let ibqx import-by-query algorithm
L2 C2 . Since triple C1 also contained C2 , ibqx clearly import-byquery algorithm L2 C1 . Let , Kv , Th C1 arbitrary input, let Q
arbitrary L2 -query encountered run ibqx [Th , , L] Kv . Since L2 fragment
L1 , Q L1 -query well. Thus, ibqx import-by-query algorithm L1 C1 .
following theorem shows oracles certain types simulate oracles
types. important 1 simulate 2 show import
query algorithm exists particular class inputs applicable 1 , also
algorithm exists applicable 2 .
Theorem 1. Let smallest partial order class oracles satisfies
following conditions TBox Th , signature , description logic L:
1. cTh ,,L aTh ,,L eTh ,,L ;
2. L-ABox L-assertion {} L-ABox,
eTh ,,L aTh ,,L holds well.
Let L description logic, let C class inputs, let x1 , x2 {a, c, e} oracle
types xTh1 ,,L xTh2 ,,L , Kv , Th C. Then, import-by-query algorithm ibqx1 L C transformed import-by-query algorithm ibqx2
L C that, input , Kv , Th C, ibqx1 [Th , , L] run Kv n
oracle queries ibqx2 [Th , , L] run Kv n oracle queries.
Proof. Let ibqx1 arbitrary import-by-query algorithm L C, consider
arbitrary input , Kv , Th C. Conditions 1 2 ensure xTh1 ,,L reducible
xTh2 ,,L sense computable total function f exists domain
xTh1 ,,L domain xTh2 ,,L query Q accepted xTh1 ,,L ,
xTh1 ,,L (Q) = xTh2 ,,L (f (Q)). particular, ABox satisfiability oracle reducible
ABox entailment oracle via f (A) = (A, ) ABox A. Furthermore, Condition
2 holds, ABox entailment oracle reducible ABox satisfiability oracle via
f (A, ) = {}. Finally, concept satisfiability oracle reducible ABox satisfiability oracle via f (C) = {C(a)} fresh individual.
Algorithm ibqx2 simply simulate ibqx1 input , Kv , Th C; furthermore, whenever ibqx1 [Th , , L] poses query Q xTh1 ,,L , ibqx2 [Th , , L] computes
f (Q) poses query f (Q) xTh2 ,,L . Since ibqx1 import-by-query algorithm
L C, ibqx2 . Furthermore, input, one-to-one correspondence runs algorithms corresponding runs posing exactly number
oracle queries.
next show that, shared signature contains atomic concepts,
close correspondence ABox concept satisfiability oracles.
Theorem 2. Let L description logic let C[C , KvC , ThC ] class inputs C
contains atomic concepts. Then, import-by-query algorithm ibqa L C
transformed import-by-query algorithm ibqc L C following
statements hold input , Kv , Th C.
211

fiCuenca Grau & Motik

run ibqa [Th , , L] Kv n oracle queries maximum number
individuals query ABox, run ibqc [Th , , L] Kv n oracle
queries exists.
run ibqc [Th , , L] Kv n oracle queries, run ibqa [Th , , L]
Kv n oracle queries exists.
Proof. Let ibqa import-by-query algorithm L C. define ibqc that,
input , Kv , Th C, algorithm ibqc [Th , , L] simulates steps algorithm
ibqa [Th , , L]; furthermore, ibqa [Th , , L] queries aTh ,,L ABox A, algorithm
ibqc [Th , , L] proceeds follows.
1. algorithm transforms ABox iterating assertions
form b and, assertion, replacing one individual (say a)
one (say b) assertions.
2. contains individual cTh ,,L (B1 . . . Bn ) = f
B1 , . . . , Bn concepts Bi (a) , ibqc [Th , , L] proceeds
way ibqa [Th , , L] aTh ,,L (A) = f; otherwise, ibqc [Th , , L] proceeds
way ibqa [Th , , L] aTh ,,L (A) = t.
obvious correspondence runs ibqa [Th , , L] ibqc [Th , , L]
Kv ; furthermore, whenever ibqa [Th , , L] issues query aTh ,,L , ibqc [Th , , L] issues
queries cTh ,,L order determine proceed. Finally, note
second statement theorem directly follows Theorem 1.
finally show without loss generality assume Kv contain concept
con.AS axiom 3 Table 3.
Definition 5. Let signature. concept C -modal sig(C) C
form R.D, R.D, n R.D, n R.D.
Intuitively, -modal concepts always treated atomic point view
Kv , rely oracle compute relevant consequences concepts.
Theorem 3. Let L, DL1 , DL2 description logics DL1 -concept
also L-concept DL2 allows DL1 -definitions; let x {a, c, e}; let C[C , KvC , ThC ]
class inputs KvC DL1 -knowledge base ThC DL2 -TBox; let
D[D , KvD , ThD ] class inputs consisting triples , Kv , Th C[C , KvC , ThC ]
Kv contains -modal concepts. Then, import-by-query algorithm ibqx2 L
transformed import-by-query algorithm ibqx1 L C.
Proof. signature, C concept, concept, axiom, knowledge base,
say C -outermost C -modal C occur proper
subconcept another -modal concept.
Let , Kv , Th C arbitrary input C, let set -outermost concepts
Kv , let XC fresh atomic concept uniquely associated C S. define
, Th , Kv follows: = {XC | C S}; Kv obtained Kv replacing
C XC ; Th = Th {XC C | C S}. Clearly, Kv Th equisatisfiable
212

fiReasoning Ontologies Hidden Content

Kv Th , , Kv , Th D. Let ibqx2 arbitrary import-by-query algorithm L
D. define ibqx1 algorithm , Kv , Th C simulates steps
ibqx2 input , Kv , Th D, following modifications:
ibqx1 [Th , , L] treats concepts atomic;
whenever ibqx2 [Th , , L] queries xT , ,L query Q , ibqx1 [Th , , L] queries
h
xTh ,,L query Q obtained Q replacing occurrence XC C.
obvious correspondence runs ibqx2 [Th , , L] ibqx1 [Th , , L]
Kv , ibqx1 import-by-query algorithm L C.

4. Limitations Import-by-Query Framework
section, explore limitations import-by-query framework show
import-by-query algorithms exist certain conditions. negative results
apply classes input Kv Th expressed description logic DL
lightweight possible, oracle based ABox satisfiability, oracle accepts
queries expressed description logic L expressive possible. Theorem 1
Proposition 2, results also apply oracle types, queries expressed
fragment L, classes input Kv Th expressed description logic
extends DL.
particular, Section 4.1 establish following general limitations importby-query framework.
presence nominals Th may preclude existence import-by-query
algorithm even = (cf. Theorem 4).
Deductive modularity TBox Kv w.r.t. necessary condition
existence import-by-query algorithm (cf. Theorem 5).
Deductive modularity, however, sucient, even Kv Th EL
allowed contain atomic concepts (cf. Theorem 6).
response negative results, import-by-query algorithms proposed paper
subjected following restrictions:
R1. Th allowed contain nominals.
R2. TBox Kv required semantically modular w.r.t. .
show Section 5.1 two restrictions sucient guarantee existence
import-by-query algorithm Kv ALCHIQ Th ALCHIQ, provided
contains atomic concepts.
Section 4.2, however, show restrictions input necessary
allowed contain atomic roles. Roughly speaking, restrictions R1 R2 insucient
since axioms Kv arbitrarily propagate information symbols private
Kv via role hidden part canonical model Kv Th (that is, part
canonical model cannot constructed using axioms Kv ); propagation
213

fiCuenca Grau & Motik

occur via existential (cf. Theorem 7) universal quantification (cf. Theorem 8).
overcome negative results, define Section 5.1 HT-safety condition that,
one hand, ensures semantic modularity and, hand, prevents arbitrary
transfer information symbols private Kv hidden parts canonical
model via role . condition, however, still insucient enable import-byquery reasoning Th contains universal quantifiers, inverse roles, functional roles,
Kv entails cyclic axioms form R.A R (cf. Theorem 9).
overcome negative result, Section 5.1 introduce acyclicity condition
together HT-safety guarantees existence import-by-query algorithm based
ABox satisfiability oracles Kv Th expressed ALCHIQ.
Finally, Section 4.3 show import-by-query algorithm based concept
satisfiability oracles exists class inputs C[C , KvC , ThC ] KvC EL
satisfies HT-safety condition, ThC EL (cf. Theorem 10). Section 5.2.2,
however, present algorithm based ABox entailment oracles applies
class inputs C. Thus, practically relevant cases exist import-by-query reasoning
impossible concept satisfiability oracles, becomes feasible ABox oracles.
4.1 General Limitations
first show presence nominals hidden knowledge base precludes
existence import-by-query algorithm visible knowledge base satisfiable
infinite models. Expressive DLs used practice often finite model property,
negative result holds even shared signature empty; thus, rest
paper consider DLs nominals, leave investigation
conditions enable import-by-query reasoning DLs future work.
Theorem 4. description logic DL without finite model property, importby-query algorithm based ABox satisfiability oracles exists L = ALCHOIQ
class inputs C[C , KvC , ThC ] C = , KvC DL-knowledge base, ThC
ALCHOIQ-TBox.
Proof. Let C arbitrary class inputs let ibqa arbitrary import-by-query
algorithm C ibqa satisfy theorems assumptions. Furthermore, let
, Kv , Th1 C arbitrary input KvC satisfiable infinite models, = ,
Th1 = . Since runs ibqa [Th1 , , L] Kv finite, number individuals
occurring query ABox run bounded integer n. Let Th2
follows, O1 , . . . , fresh nominal concepts:
Th2 = { O1 . . . }

(9)

Clearly, Kv Th1 satisfiable, Kv Th2 not. Consider arbitrary query ABox
occurring run ibqa [Th1 , , L]. Since = , consists assertions form
b b; furthermore, contains n individuals, 1 , (A) = implies
h

2 , (A) = t, converse holds monotonicity first-order logic. then,
h

Proposition 1, runs ibqa [Th1 , , L] Kv coincide runs ibqa [Th2 , , L]
Kv , contradicts fact Kv Th1 satisfiable Kv Th2 not.
214

fiReasoning Ontologies Hidden Content

next present strong result: deductive modularity necessary requirement
existence import-by-query algorithm; is, import-by-query algorithm
exists class inputs contains triple , Kv , Th TBox Kv
deductively modular w.r.t. . Intuitively, without deductive modularity, Kv
arbitrarily influence consequences Th , oracle cannot take account
since access axioms Kv . sake generality,
impose conditions .
Theorem 5. Let DL1 arbitrary fragment ALCHIQ; let DL2 arbitrary
description logic extends EL allows DL1 -definitions; let arbitrary
signature; let Kv arbitrary satisfiable DL1 -knowledge base whose TBox
deductively modular w.r.t. . Then, import-by-query algorithm based ABox satisfiability oracles exists L = ALCHIQ class inputs C[C , KvC , ThC ] C = ,
KvC = Kv , ThC DL2 -TBox.
Proof. Let C class inputs satisfying theorems conditions, let , Kv , Th1 C
input Th1 = . Since Kv deductively modular w.r.t. , possibly complex DL1 concepts C1 C2 exist sig(C1 ) , sig(C2 ) , Tv |= C1 C2 ,
|= C1 C2 . Let ibqa import-by-query algorithm L = ALCHIQ C. Finally,
let Th2 follows, A, B1 , B2 , R occur .
Th2 = { B1 C1 , B2 C2 , R.(A B1 ), B2 }

(10)

Clearly, Kv Th1 satisfiable, Kv Th2 not. Consider arbitrary L-ABox
sig(A) . Th1 unsatisfiable, Th2 . Conversely, assume


Th1 satisfiable model = (I , ). Since |= C1 C2 , interpretation





= (I , ) domain element x exist x C1I x C2I .

loss generality assume = . Let following interpretation:

aI
AI
B1I
B2I
RI
XI





=

= aI individual occurring
= {x}


= C1I C1I


= C2I C2I
= {o, x | }


= X X atomic concept role X




ALCHIQ-concept E sig(E) , since disjoint,


straightforward induction structure E one show E = E



E = E . Furthermore, atomic role . Thus |= A,
straightforward check |= Th2 . Consequently, 1 ,,L (A) = 2 ,,L (A) Lh

h

ABox sig(A) . Hence, Proposition 1, runs ibqa [Th1 , , L] Kv coincide
runs ibqa [Th2 , , L] Kv , contradicts fact Kv Th1 satisfiable
Kv Th2 not.

Theorem 5 shows deductive modularity necessary requirement
import-by-query algorithm exist, following theorem shows sucient
215

fiCuenca Grau & Motik

requirement, even contains atomic concepts, Kv EL-knowledge base, Th
EL-TBox.
Theorem 6. import-by-query algorithm based ABox satisfiability oracles exists
L = ALCHIQ class inputs C[C , KvC , ThC ] C contains atomic concepts,
KvC ThC EL, TBox KvC deductively modular w.r.t. C .
Proof. Let ibqa import-by-query algorithm satisfying theorems assumptions, let
= {A, B, C}, let Kv , Th1 , Th2 following EL knowledge bases:
Kv = { A(a), B R.C }

(11)

= Th1 { S.B }

(13)

Th1
Th2

={C}

(12)

TBox Kv clearly deductively modular w.r.t. , , Kv , Thi C {1, 2};
furthermore, Kv Th1 satisfiable, whereas Kv Th2 not. Consider arbitrary
query ABox sig(A) ; since contains assertions form X(a),
X(a), b, b sig(X) , 1 ,,L (A) = 2 ,,L (A). then,
h

h

Proposition 1, runs ibqa [Th1 , , L] Kv coincide runs ibqa [Th2 , , L]
Kv , contradicts fact Kv Th1 satisfiable Kv Th2 not.

deductive modularity sucient, semantic modularity sucient
cases: Section 5.1 present import-by-query algorithm applied
case contains atomic concepts, Kv Th ALCHIQ, TBox
Kv semantically modular w.r.t. .
4.2 Limitations Importing Atomic Roles
section, establish limitations import-by-query framework cases
allowed contain atomic roles. particular, show semantic modularity
sucient guarantee existence import-by-query algorithm.
Theorems 7 8 demonstrate problems arise due certain fundamental limitations oracle query languages. understand intuition behind results,
assume shared signature contains one atomic role R. Even relatively simple
DL EL, knowledge base Th imply existence arbitrarily long R-chains using axiom
C R.C. oracle languages consider, however, examine
bounded prefixes chains. example, assume use ABox satisfiability oracle query language based ALCHIQ. concept query ABox corresponds
first-order formula, well known satisfiability formula
first-order interpretation depends formulas quantifier depth. Since number
oracle calls run import-by-query algorithm must bounded, import-by-query
algorithm examine bounded prefix model Th . leads us
fundamental problem: Th changed interesting consequences
detected examining longer R-chains, consequences go undetected
algorithm render algorithm incorrect. Theorem 7 exploits fact

216

fiReasoning Ontologies Hidden Content

interesting consequences Th detected Kv using axioms existentially quantified concepts (i.e., proof uses axiom R.B2 B2 ), whereas Theorem (8) analogously
uses axioms universally quantified concepts (i.e., B R.B).
alternative intuitive explanation results Theorems 7 8 think
culprit axioms R.B2 B2 B R.B Kv propagating information
Kv Th . order miss interesting consequences Th , import-by-query
algorithm must examine suciently large portion hidden part canonical model
Kv Th order correctly evaluate culprit axioms. This, however, impossible
bound portion size determined algorithms inputs.
Theorem 7. import-by-query algorithm based ABox satisfiability oracles exists
L = ALCHIQ class inputs C[C , KvC , ThC ] C arbitrary, KvC ThC
expressed EL, TBox KvC semantically modular w.r.t. C .
Proof. Let ibqa import-by-query algorithm satisfying theorems assumptions, let
= {A1 , A2 , R}, let Kv following EL knowledge base:
Kv = { B1 (a), B1 S.A1 , A2 B2 , R.B2 B2 , S.B2 }

(14)

TBox Kv semantically modular w.r.t. : interpretation symbols
, interpretation J X J = X X , B1J = , B2J = J ,
J = model TBox Kv . Let Th1 following EL TBox:
Th1 = { A1 C, C R.C }

(15)

Since run ibqa [Th1 , , L] Kv finite, integer n exists query
ABox occurring run contains concepts quantifier depth n. Let Th2
following EL TBox:
Th2 = {A1 R
. . R .A2 }
.
n + 1 times

(16)

Clearly, Kv Th1 satisfiable, whereas Kv Th2 not. Consider arbitrary query ABox
occurring run ibqa [Th1 , , L]. next show 1 ,,L (A) = 2 ,,L (A).
h

h

Assume Th1 satisfiable. Since expressed ALCHIQ Th1 EL,
canonical forest model = (I , ) Th1 exists (e.g., model obtained
applying hypertableau algorithm Th1 A). Due (15), x AI1 , infinite
x RI 0 i.
sequence {0x , 1x , 2x , . . .} exists 0x = x ix , i+1
J
J
Let J = ( , ) interpretation defined follows:
J =

x
AJ2 = AI2 {n+1
| x AI1 }

X J = X X = A2

Clearly, J |= Th2 . Furthermore, since |= A, contains concepts quantifier depth
n, J coincide depth n, J |= A. Thus, Th2 satisfiable.
Assume Th2 satisfiable. canonical forest model = (I , ) Th2
x } exists
exists. Due (16), x AI1 , finite sequence {0x , 1x , 2x , . . . , n+1
217

fiCuenca Grau & Motik

x RI 0 < n. Let J = (J , J ) interpresuch 0x = x ix , i+1
tation defined follows:

J =
x
C J = {0x , . . . , n+1
| x AI1 }

x , x | x AI }
RJ = RI {n+1
n+1
1
X J = X X {R, C}

Clearly, J |= Th1 . Furthermore, since |= A, C sig(A), contains concepts
quantifier depth n, J coincide depth n, J |= A. Thus,
Th1 satisfiable.
Proposition 1, runs ibqa [Th1 , , L] Kv coincide runs ibqa [Th2 , , L]
Kv , contradicts fact Kv Th1 satisfiable Kv Th2 not.
Theorem 8. import-by-query algorithm based ABox satisfiability oracles exists
L = ALCHIQ class inputs C[C , KvC , ThC ] C arbitrary, KvC expressed
FL0 , ThC expressed EL, TBox KvC semantically modular w.r.t. C .
Proof. Let ibqa import-by-query algorithm satisfying theorems assumptions, let
= {A1 , A2 , R}, let Kv following FL0 knowledge base.
Kv = { A1 (a), B(a), B R.B, A2 B }

(17)

TBox Kv semantically modular w.r.t. : interpretation ,
interpretation J X J = X X B J = model TBox
Kv . Let Th1 EL TBox (15) given proof Theorem 7. Since run
ibqa [Th1 , , L] Kv finite, integer n exists query ABox occurring
run contains concepts quantifier depth n. Let Th2 EL TBox (16)
Theorem 7. Clearly, Kv Th1 satisfiable, whereas Kv Th2 not. Using arguments
analogous proof Theorem 7, one show 1 ,,L (A) = 2 ,,L (A)
h

h

query ABox occurring run ibqa [Th1 , , L]. Proposition 1, runs
ibqa [Th1 , , L] Kv coincide runs ibqa [Th2 , , L] Kv , contradicts
fact Kv Th1 satisfiable Kv Th2 not.

possible way overcome negative results prevent axioms Kv
propagating information via roles hidden part canonical model
Kv Th . Section 5.1, achieve requiring Kv HT-safe. Roughly speaking,
Kv semantically modular w.r.t. , but, addition, translated set
HT-rules Rv variables x role atom form R(x, y) R
guarded suitable concepts. example, although knowledge base Kv (17)
semantically modular w.r.t. = {A1 , A2 , R}, axiom B R.B Kv violates HTsafety condition since body corresponding HT-rule B(x) R(x, y) B(y)
contain guard concept atom variable y. order streamline presentation
ensure notions needed enable import-by-query reasoning defined one
place, formalize HT-safety Definition 6 Section 5.1. Unfortunately, Theorem 9
shows, HT-safety alone ensure existence import-by-query algorithm.
Theorem 9. import-by-query algorithm based ABox satisfiability oracles exists
L = ALCHIQ class inputs C[C , KvC , ThC ] C arbitrary, KvC expressed
EL, ThC expressed Horn-ALCIF, TBox KvC HT-safe w.r.t. C .
218

fiReasoning Ontologies Hidden Content

Proof. Let ibqa import-by-query algorithm satisfying theorems assumptions, let
= {B, R}, let Kv following EL knowledge base:
Kv = { A(a), B(a), R.A }

(18)

TBox Kv semantically modular w.r.t. : interpretation ,
interpretation J X J = X X AJ = model TBox
Kv . According Definition 6, TBox Kv HT-safe well. Let Th1
following Horn-ALCIF TBox:
Th1 = { B C , B R.C, C R.C, 1 R }

(19)

Since run ibqa [Th1 , , L] Kv finite, integers n exist query
ABox occurring run contains n individuals concepts quantifier depth
m. Let k = n + let D0 , . . . , Dk distinct fresh atomic concepts. Let Th2
following Horn-ALC TBox:
Th2 = Th1 { Di Dj , | 0 < j k } { Dj1 R.Dj | 1 k }
{ B D0 , Dk }

(20)

Clearly, Kv Th1 satisfiable, whereas Kv Th2 not. Consider arbitrary query
ABox occurring run ibqa [Th1 , , L]. next show 1 ,,L (A) = 2 ,,L (A).
h

h

clearly holds Th1 unsatisfiable, assume Th1 satisfiable. Then,
exists canonical forest model = (I , ) Th1 A. Consider arbitrary domain
element x B . say domain element reachable x steps
sequence domain elements 0 = x, 1 , 2 , . . . , = exist , i+1 RI
1 < . x y, axioms Th1 ensure following properties:
1. sequence unique consists unique domain elements.
RI inverse-functional relation so, 0 < , domain element
element , i+1 RI , = j 0 < < j ; furthermore,
0 B 0 C , C 0 < , ensures 0 = 0 < .
2. x B distinct x exists reachable form x .
xi B 0 < RI inverse-functional.
3. < k. contains n individuals concepts
quantifier depth m.
Let J = (J , J ) interpretation defined follows:

J =
X J = X X sig(Th1 )
DkJ =
DiJ =
{y | reachable x steps } 0 < k
xB

Interpretations J coincide symbols Th1 , J |= Th1 . Furthermore,
DiI < k, properties 13 DjI j = i, J |= Th2 . then,
Proposition 1, runs ibqa [Th1 , , L] Kv coincide runs ibqa [Th2 , , L]
Kv , contradicts fact Kv Th1 satisfiable Kv Th2 not.
219

fiCuenca Grau & Motik

proof Theorem 9 uses Kv implies Rn .A arbitrary n,
R . Furthermore, axioms Th containing universally quantified concepts propagate information along R-chain unknown level m. import-by-query algorithm
cannot determine depth must examine model Kv , precludes
termination requirement Definition 4. Section 5.1, present sucient acyclicity
restriction Kv bounds n ensures existence import-by-query algorithm.
4.3 ABox vs. Concept Satisfiability Oracles
section show that, Kv EL-knowledge base Th EL-TBox,
import-by-query algorithm based concept satisfiability oracles exists, even Kv HTsafe w.r.t. . interesting Section 5.2.2 present algorithm based
ABox entailment oracles handle case. Thus, ABox oracles strictly
expressive concept satisfiability oracles.
Theorem 10. import-by-query algorithm based concept satisfiability oracles exists
L = ALCHIQ class inputs C[C , KvC , ThC ] C arbitrary, KvC ThC
expressed EL, TBox KvC HT-safe w.r.t. C .
Proof. Let ibqc import-by-query algorithm satisfying theorems assumptions, let
= {R}, let Kv following EL knowledge base:
Kv = { A(a), R.A }

(21)

Definition 6, TBox Kv HT-safe. Let Th1 = . run ibqc [Th1 , , L] Kv
finite, integer n exists query concept occurring run contains
concepts quantifier depth n. Let Th2 following EL TBox:
Th2 = { R.
. . . R . }
n + 1 times

(22)

Clearly, Kv Th1 satisfiable, whereas Kv Th2 not. Furthermore, straightforward
see that, ALCHIQ concept C quantifier depth n sig(C) ,
Th1 |= C Th2 |= C , cT 1 , (C) = cT 2 , (C). then,
h

h

Proposition 1, runs ibqc [Th1 , , L] Kv coincide runs ibqc [Th2 , , L]
Kv , contradicts fact Kv Th1 satisfiable Kv Th2 not.
Note knowledge base Kv used proof Theorem 10 analogous
one proof Theorem 9that is, entails cyclic axiom form R.A
R . negative result Theorem 9, however, apply
case Th expressed EL. algorithm presented Section 5.2.2 handle
knowledge bases via ABox entailment oracle. Intuitively, ABoxes
encode cyclic structures, whereas concepts cannot.

5. Import-by-Query Algorithms
section, identify several cases import-by-query algorithms exist.
simplicity, throughout section assume Kv contain -modal concepts;
Theorem 3 without loss generality.
220

fiReasoning Ontologies Hidden Content

overcome negative results Section 4, Sections 5.1.1 5.1.2 introduce
HT-safety acyclicity conditions, respectively, Kv must satisfy order prevent undesirable interactions axioms Kv Th . Furthermore, rest
paper assume Kv preprocessed described Motik et al. (2009)
corresponding set HT-rules Rv ABox Av ; convenient HT-rules
contain nested quantifiers. thus formulate HT-safety acyclicity terms
Rv Av , define Kv HT-safe (resp. acyclic) corresponding Rv
Av HT-safe (resp. acyclic). algorithms take inputs Rv Av , specify
allowed inputs using classes C[C , RCv ACv , ThC ] triples C , RCv ACv , ThC Rv
set HT-clauses, Av normalized ABox, sig(RCv ACv ) sig(ThC ) C .
Section 5.1 present general import-by-query algorithm based ABox satisfiability oracles applicable case Kv imports atomic concepts
roles, Kv Th expressed ALCHIQ. order algorithm applicable,
however, Kv must HT-safe acyclic. contains atomic concepts,
acyclicity vacuously satisfied Kv HT-safety becomes equivalent semantic
modularity; thus, atomic concepts shared, algorithms applicable whenever
Kv semantically modular w.r.t. .
algorithm Section 5.1, however, unlikely suitable practice due
high degree nondeterminism. Therefore, Section 5.2.1 present import-by-query
algorithm based ABox entailment oracles that, believe, suited implementation
optimization. algorithm requires Th Horn knowledge base, allows
algorithm goal-oriented.
practical algorithm Section 5.2.1 readily applied EL knowledge bases,
guaranteed optimal. Therefore, Section 5.2.2 present EL-specific
import-by-query algorithm case Kv Th expressed EL. addition
optimal EL knowledge bases, EL-specific algorithm require Kv
acyclic somewhat relaxes HT-safety requirement.
5.1 Import-by-Query ALCHIQ

next present import-by-query algorithm based ABox satisfiability oracles
applicable set HT-rules Rv TBox Th ALCHIQ. assumptions made
type symbols : Rv reuse atomic concepts roles Th .
5.1.1 HT-Safety
define HT-safety condition allows us overcome negative results
Theorems 7 8, also guarantees semantic modularity required overcome
negative results Theorems 5 6. contains atomic concepts, HT-safety
reduces semantic modularity Rv w.r.t. .
notion HT-safety Rv consists following building blocks. first identity
safe conceptsthat is, concepts private Rv propagated
models Th . Next, transform Rv reduct replacing Rv safe concepts
, require reduct semantically modular w.r.t. . latter property
ensures interpretation symbols extended interpretation
symbols Rv interpreting safe concepts empty set. Finally, motivated
221

fiCuenca Grau & Motik

Section 4.2, impose syntactic restriction HT-rule Rv : body atom
R(x, y) R , require variables x guarded safe concept.

Definition 6. Let Rv set HT-rules let signature. set safe concepts
Rv smallest set safe(Rv , ) that, HT-rule Rv whose body
contains atom form R(x, yi ) R(yi , x) R atom form A(x)
A(yi ) , safe(Rv , ).
reduct Rv w.r.t. set rules obtained Rv removing rule
containing concept safe(Rv , ) body, removing head
remaining rules atom containing concept safe(Rv , ).
set Rv HT-safe w.r.t.
1. reduct Rv w.r.t. semantically modular w.r.t. ,

2. rule Rv body atom form R(x, yi ) R(yi , x)
R , body contains atoms A(x) B(yi ) A, B safe(Rv , ).

HT-safety invalidates proofs Theorems 7 8: knowledge bases Kv used
proofs two theorems HT-safe w.r.t. respective signatures .
particular, consider Kv used proof Theorem 7. set HT-rules Rv obtained
TBox Kv shown below.
B1 S.A1
A2 B2

R.B2 B2
S.B2



B1 (x) S.A1 (x)



A2 (x) B2 (x)



R(x, y) B2 (y) B2 (x)



S(x, y) B2 (y)

(23)
(24)
(25)
(26)

safe(Rv , ) = {B2 }. straightforward see reduct Rv w.r.t. , shown
below, semantically modular w.r.t. = {A1 , A2 , R}.
B1 (x) S.A1 (x)

(27)

A2 (x)

(28)

Consider Kv used proof Theorem 8. set HT-rules Rv obtained
TBox Kv shown below.
B R.B

A2 B



B(x) R(x, y) B(y)



A2 (x) B(x)

(29)
(30)

safe(Rv , ) = {B}, reduct Rv w.r.t. empty thus semantically modular
w.r.t. = {A1 , A2 , R}; however, first HT-rule satisfy Condition 2 Definition 6 since rule body contain atom form A(y) safe(Rv , ).
Note that, contains atomic concepts, safe(Rv , ) = . reduct Rv
w.r.t. equal Rv , Condition 1 Definition 6 holds Rv
semantically modular w.r.t. ; furthermore, Condition 2 vacuously holds Rv . Thus,
HT-safety reduces semantic modularity w.r.t. atomic concepts shared.
following proposition shows that, given interpretation symbols , obtain
model Rv interpreting safe concepts empty set.
222

fiReasoning Ontologies Hidden Content

Proposition 3. Let Rv set HT-rules HT-safe w.r.t. . Then,
interpretation symbols , model J Rv exists J = , X J = X
symbol X , X J = atomic concept X safe(Rv , ).
Proof. Let interpretation symbols , let Rv reduct Rv w.r.t

. Since Rv semantically modular w.r.t. , model Rv exists =

X = X symbol X . Let J interpretation obtained
setting X J = X safe(Rv , ). Consider arbitrary HT-rule Rv .
safe(Rv , ) occurs body , AJ = clearly implies J |= . Otherwise,
let Rv rule obtained removing head atoms contain safe concept;
|= clearly implies J |= . Consequently, J |= Rv .
Finally, note HT-safety syntactic condition; fact, checking HT-safety
undecidable general requires checking semantic modularity set HT-rules
w.r.t. signature. mentioned Section 2.3, however, several practically useful syntactic
conditions known guarantee semantic modularity (Cuenca Grau, Horrocks, Kazakov, & Sattler, 2008), condition used obtain purely syntactic
HT-safety notion.
5.1.2 Acyclicity
negative result Theorem 9 relies Kv containing cyclic axiom R.A
R . next present sucient condition detect cycles
polynomial time.
test involves set function-free first-order formulae equality D(Rv , Av )
whose consequences summarize models Rv Th Av ; precisely, projection
canonical model Rv Th Av symbols sig(Rv ) homomorphically
embedded set ground facts entailed D(Rv , Av ). Intuitively, since axioms
Th available, facts entailed D(Rv , Av ) reflect possible consequences
Th information derived using Rv Av . Theory D(Rv , Av ) also keeps
track paths visible part canonical models Rv Th Av using
two special binary predicates: Succ keeps track successorship relation domain
elements, -Desc keeps track descendant relation via roles contained .
acyclicity condition checks whether -Desc relation entailed D(Rv , Av )
cyclic; case, establish bound length paths roles .
Definition 7. Let Rv set HT-rules, let Av ABox, let signature.
atomic concept sig(Rv ) sig(Av ), let vA vA individuals uniquely associated
A, respectively; furthermore, let Succ -Desc binary predicates
occurring Rv Av . Function tt() maps atom occurring Rv Av
conjunction atoms follows, z arbitrary term:
tt(A(z)) = ;
tt( n R.C(z)) = ar(R, z, vC ) tt(C(vC )) Succ(z, vC );
tt() = atom form covered two cases.
223

fiCuenca Grau & Motik

Furthermore, D(Rv , Av ) set function-free formulas first-order logic
equality defined follows, variables implicitly universally quantified.
assertion Av , set D(Rv , Av ) contains tt().
individual c occurring Av atomic concept , set D(Rv , Av )
contains A(c).
HT-rule Rv form (1) 1 j n, set D(Rv , Av ) contains
following formula:
tt(U1 ) . . . tt(Um ) tt(Vj )

(31)

atomic concept , set D(Rv , Av ) contains following formula:
Succ(z1 , z2 ) A(z2 )

(32)

atomic roles R, R , set D(Rv , Av ) contains following formulae:
R (z1 , z2 ) R(z1 , z2 )


R (z1 , z2 ) R(z2 , z1 )

(33)
(34)



(35)



(36)



(37)

R(z, z1 ) R (z, z2 ) z1 z2
R(z1 , z) R (z2 , z) z1 z2
R(z1 , z) R (z, z2 ) z1 z2

atomic role R , set D(Rv , Av ) contains following formulae:
Succ(z1 , z2 ) R(z1 , z2 ) -Desc(z1 , z2 )

-Desc(z1 , z2 ) -Desc(z2 , z3 ) -Desc(z1 , z3 )

(38)
(39)

Set D(Rv , Av ) contains harmful cycle D(Rv , Av ) |= -Desc(vC , vC ) vC . Furthermore, Rv Av acyclic w.r.t. D(Rv , Av ) contain harmful cycle.
set formulae D(Rv , Av ) straightforwardly transformed equivalent datalog program equality using well-known equivalences first-order logic;
therefore, often refer D(Rv , Av ) datalog program.
Acyclicity allows us express axioms 6 8 Table 3. Intuitively, acyclicity
ensures visible parts canonical forest models Rv Th Av contain
infinite chains roles ; use property algorithm define suitable
blocking condition. explain intuition means example. Let = {C, R, U }
C concept R U roles, Av = {A(a)}, Rv contains following HTrules; corresponding formulae D(Rv , Av ) shown symbol. Note
Rv HT-safe w.r.t. .
A(x) R.B(x)
A(x) S.C(x)

A(x) R(x, vB ) B(vB ) Succ(x, vB ) (40)
A(x) S(x, vC ) C(vC ) Succ(x, vC ) (41)

224

fiReasoning Ontologies Hidden Content

B, C
U


U

B, C,E
b

U


R


b

A, C
e

R

R





c

C,



C,
c





(a) Model






(b) Extended Model

B, C
B, C,

vB

vB vC
vD

R R
U U

A, C

B, C, E

vB


R R
U U



vA

R R
U U


C,


A, C

vC
vD

(c) Acyc. Check


vA

A, C


vA


C,


(d) Acyc. Check (I)

vC
vD

(e) Acyc. Check (II)

Figure 1: Canonical Models Acyclicity
A(x) S.D(x)

S(x, y1 ) S(x, y2 ) y1 y2



A(x) S(x, vD ) D(vD ) Succ(x, vD ) (42)
S(x, y1 ) S(x, y2 ) y1 y2

(43)

C(x) D(x) S.A(x) C(x) D(x) S(x, vA ) A(vA ) Succ(x, vA ) (44)

Consider also following hidden TBox expressed ALCHIQ:
Th = { 1 R., R U , U. C }

(45)

Figure 1(a) shows canonical model Rv Th Av . Furthermore, Figure 1(c) shows
ground atoms entailed D(Rv , Av ) represented graph G solid arrows show
roles R, U , S, dashed arrows show special predicate -Desc; clarity,
atoms involving special predicate Succ included following
figures. Note D(Rv , Av ) entails R(vA , vB ) R(a, vB ); atoms, together
rules (34) (35), entail vA va ; consequently, vA va represented Figure 1(c)
node. Structure G summarizes sense homomorphically
embedded G. repetitive structure represented G cycle nodes vA
vC via S; however, since shared symbol (i.e., ), give rise
harmful cycle. Consequently, Rv Av acyclic w.r.t. , guarantees
visible part model Rv Th Av contain R-chains unbounded length,
regardless contents Th . Accordingly, canonical model Rv Th Av shown
Figure 1(a) contains R-chains.
225

fiCuenca Grau & Motik

Note, however, G overestimates canonical model I; example, G contains
individual vA instance C, reflected I. let us
assume Rv Rv extended following HT-rule:
A(x) C(x) R.C(x)



A(x) C(x) R(x, vC ) C(vC ) Succ(x, vC )

(46)

canonical model Rv Th Av clearly Rv Th Av ; however,
D(Rv , Av ) contains harmful cycle, shown Figure 1(d). Intuitively, D(Rv , Av ) provides
us conservative overestimate canonical models, cases detect
cycles really exist canonical models. necessary consequence
fact acyclicity checked polynomial time.
Definition 7, however, provides us sucient check. example, let Rv Rv
extended following HT-rules:
A(x) R.E(x)



B(x) C(x) E(x) R.A(x)



A(x) R(x, ) E(vE ) Succ(x, )

B(x) C(x) E(x)
R(x, vA ) A(vA ) Succ(x, vA )

(47)
(48)

canonical model Rv Th Av ground atoms entailed D(Rv , Av ) shown
Figures 1(b) 1(e), respectively. HT-rules Rv \ Rv enforce existence
infinite R-chain, reflected harmful cycle (e.g., self-loop vA ).
Acyclicity indeed checked polynomial time, shown next.
Proposition 4. Acyclicity Rv Av w.r.t. checked polynomial time.

Proof. Let D(Rv , Av ) specified Definition 7. number fresh individuals
form vA vA clearly linear size Rv , Av , , size D(Rv , Av )
polynomial size Rv , Av , .
compute set positive ground atoms follow D(Rv , Av )
polynomial time using forward chaining. predicates D(Rv , Av ) bounded arity,
number atoms polynomial size D(Rv , Av ). straightforwardly implies claim proposition show that, given set facts rule
D(Rv , Av ), compute set entailed facts polynomial time. Rules
form (31) contain bounded number variables, set entailed facts
computed polynomial time simply considering possible mappings variables
individuals. Assume form (31). number variables linear
size Rv , exponentially many mappings variables individuals.
can, however, determine values x yi make body true follows.
variable yi , let Pi binary relation initially contains pairs individuals
occurring D(Rv , Av ); relation eventually contain pairs values x yi
make body true. remove Pi pairs satisfy
body atoms contain variables x yi . Next, Pi Pj , remove
pairs c, c Pi c exists c, c Pj . consider
consequent atom ; contains variables x yi , infer ground atoms
obtained replacing x c yi c c, c Pi ; contains variables
yi yj , infer ground atoms c c individual c exists c, c Pi
c, c Pj . clearly done polynomially many steps number
individuals D(Rv , Av ) maximal number variables rule Rv .
226

fiReasoning Ontologies Hidden Content

Table 4: Additional Derivation Rules
individual atomic concept exist
1. indirectly blocked
2. {A(s), A(s)} =
A1 := {A(s)} A2 := {A(s)}.

individuals atomic roles R, R exist
1. neither indirectly blocked A,
2. R (s, t) A,
3. {R(s, t), R(s, t)} =
A1 := {R(s, t)} A2 := {R(s, t)}.

individuals atomic roles R, R exist
1. neither indirectly blocked A,
2. R (s, t) A,
3. {R(t, s), R(t, s)} = ,
A1 := {R(t, s)} A2 := {R(t, s)}.

individuals s, s1 , s2 exist
1. none s, s1 , s2 indirectly blocked A,
2. {s1 s2 , s1 s2 } = ,
3. atomic roles R, R exist
3.1 {R(s, s1 ), R (s, s2 )}
3.2 {R(s1 , s), R (s2 , s)}
3.2 {R(s1 , s), R (s, s2 )}
A1 := {s1 s2 } A2 := {s1 s2 }.
1.
2. connected component A| exists aTh , (A ) = f
A1 := {}.


A-cut

R-cut

R -cut

-cut

-rule

5.1.3 Import-by-Query Algorithm
next present import-by-query algorithm applicable Rv Av HT-safe
acyclic w.r.t. . algorithm modifies standard hypertableau algorithm follows.
First, several cut rules nondeterministically guess relevant assertions involving
symbols . Second, -rule checks whether guesses indeed consistent
Th . Third, relaxed blocking condition ensures termination.
Definition 8. Let C[C , RCv ACv , ThC ] class inputs RCv ACv acyclic w.r.t.
C , RCv HT-safe w.r.t. C , ThC ALCHIQ TBox. ALCHIQ -algorithm
takes triple , Rv Av , Th C obtained modifying Definition 1 follows.
Blocking. unnamed individual blocking-relevant if, predecessor
s,
LA (s, ) = LA (s , s) = .
Then, individual ABox assigned blocking status way
Definition 1, dierence directly blocked if, addition conditions
Definition 1, blocking-relevant.

227

fiCuenca Grau & Motik

B

Anm

b

R

A, C

B



e

R

b

R

C

Ad





c

C,



C



A, C

c



f



Ac

C,

C

(a) Clash-free ABox

f

(b) ABox A|
Anm
fin

Acfin
C, E

b

R

C, E

C

e

R

Adfin

c



e



C, E






C, E

R

C, E



C, E

C, E




C, E

C, E

(c) Saturation via Rh
B
b

R

A, C, E

B




C, E

C, E

A, C, E R

C, D, E


c


C, E





C, E


C, E



e

C, D, E


f


C, E

(d) Extended ABox Afin

Figure 2: Completeness ALCHIQ -algorithm
Derivation Rules. derivation rules given Tables 2 4, A|
ABox obtained removing assertion containing indirectly blocked individual
assertion sig() .
Section 5.1.4 show cut rules Table 4 needed know
Th expressed description logic ALC ALCHIQ. algorithm
indeed import-by-query algorithm, show next.

Theorem 11. ALCHIQ -algorithm import-by-query algorithm based ABox
satisfiability oracles class inputs C[C , RCv ACv , ThC ] Definition 8. algorithm implemented runs N2ExpTime N , total number
oracle queries size query exponential N ,
N = |Rv Av | + || input Rv , Av , .
proof Theorem 11 lengthy quite technical, defer appendix
next discuss intuitions. derivation rules Table 2 clearly sound.
228

fiReasoning Ontologies Hidden Content

Furthermore, due acyclicity, chains assertions involving roles bounded
length, enables blocking ensures termination. next sketch completeness
argument. particular, completeness need show existence clash-free
ABox derivation rule applicable implies satisfiability input.
Let clash-free ABox labeling leaf derivation , Rv Av , Th , let Rh
set HT-rules corresponding Th . model Rv Th extended
model Rv Av Th , suces show satisfiability Rv Th .
end, extend clash-free ABox Afin derivation rule standard
hypertableau algorithm applicable Rv Rh Afin ; thus, Rv Afin Th satisfiable,
since Afin Rv Th monotonicity. construction Afin proceeds
follows:
1. split projection A| . particular, define Anm ABox containing assertions A| involving individuals reachable named individual;
furthermore, nonblocked blocking-relevant individual t, define
ABox containing assertions A| involving individuals reachable t.
2. apply standard hypertableau algorithm Rh connected

components Anm , Rh ; let Anm
fin Afin clash-free ABoxes

labeling leaves respective derivations. -rule applicable
ABoxes exist.

3. define Afin union A, Anm
fin , Afin , assertions C(s)



blocked blocker , C(s ) Afin , sig(C) sig(Rh ).

Let us call individuals old, individuals introduced second step
new ; observe following. (1) Due cut rules, second step cannot
derive fresh assertions involving old individuals symbols without leading
contradiction. (2) connected components Anm disjoint,
HT-rules Rh applied Afin subsets correspond connected
component Anm . (3) Due (1), HT-rule Rv become applicable
assertions involving old individuals. (4) Due HT-safety, HT-rule Rv
become applicable assertion Afin involves new individual. (5) Due (1)

third step construction above, individual blocked A, Anm
fin , Afin ,
blocked Afin well. Then, (1)(5) imply derivation rule standard
hypertableau algorithm applicable Rv Rh Afin , proves completeness.
explain intuition example = {C, R}, Av = {A(a)}, Rv consists
HT-rules (40)(44), Th defined follows:
Th = { R. C, C T.C, C E }

(49)

shown Section 5.1.2, Rv Av acyclic w.r.t. , ALCHIQ -algorithm
applicable. algorithm produces derivation leaf labeled
ABox shown Figure 2(a); readability, show neither negative assertions
assertions involving complex concepts. Individual f directly blocked c A,
assertions C(a) C(d) introduced A-cut rule. construct Afin ,
assertions containing symbol removed, resulting ABox A| shown
229

fiCuenca Grau & Motik

R, U
A, C

B, C



vB

R, U


R
U

R
U

C,

R
U

vC
vD

R
U




E, C



R, U

vA

A, C

R, U

Figure 3: Acyclicity Check Th ALCHI
Figure 2(b). ABox split connected components Anm , Ac , Ad ; note
c nonblocked blocking-relevant individuals. Next, Anm , Ac , Ad
completed w.r.t. Rh using standard hypertableau algorithm; Figure 2(c) shows
c

resulting ABoxes Anm
fin , Afin , Afin . Note C(a) C(d) consistent
axiom R. C Th . Finally, Afin obtained taking union A, Anm
fin ,
Acfin , Adfin , adding E(f ); latter f blocked c E(c) Acfin .
result shown Figure 2(d); clearly, derivation rule standard hypertableau
algorithm applicable Afin .
5.1.4 Hidden Ontology DLs ALC ALCHIQ

main limitation acyclicity condition Definition 7 stems fact
must anticipate possible consequences Th . acyclicity conditions
derivation rules Table 4 simplified hidden ontology known
expressed description logic ALC ALCHIQ.
Th known use cardinality restrictions, omit rules (35)(37)
definition D(Rv , Av ), -cut rule Table 4 required.
Th known use inverse roles, omit rules (34), (36), (37)
definition D(Rv , Av ), R -cut rule required, Conditions 3.2
3.3 removed -cut rule.
Th known use role hierarchies, omit rules (33) (34)
definition D(Rv , Av ), R-cut Table 4 required, R -cut rule
need applied R R same.
simplifications allow approach applied wider range visible ontologies. example, consider set Rv consisting HT-rules (40)(44) (47)(48),
obtained harmful cycle w.r.t. = {C, R, U }, shown Figure 1(e).
Th known expressed ALCHI (and use cardinality restrictions),
230

fiReasoning Ontologies Hidden Content

omit formulas form (35)(37) definition D(Rv , Av ); ground atoms
entailed D(Rv , Av ) shown Figure 3. change makes Rv Av acyclic
w.r.t. : sure that, arbitrary hidden TBox expressed ALCHI,
infinite R-chains need considered reasoning Rv .
5.2 Practical Import-by-Query Algorithms
algorithm presented Section 5.1.3 suited practical implementation
derivation rules Table 4 incur huge amount nondeterminism. section,
present practical import-by-query algorithms nondeterministic rules replaced
demand oracle calls, makes algorithms goal-oriented.
algorithms make assumptions kinds symbols contained : atomic
concepts roles shared.
5.2.1 Importing Horn Ontologies
section, present practical algorithm applies Th expressed
Horn-ALCHIQ fragment ALCHIQ. well known Th transformed
set Horn HT-rules. allows us eliminate nondeterministic cut rules,
use ABox entailment oracle instead ABox satisfiability oracle, define oracle
query rules deterministically complete query ABox missing assertions
entailed Th A. algorithm issues oracle queries demand, goal oriented
thus amenable implementation.
Definition 9. Let C[C , RCv ACv , ThC ] class inputs RCv ACv acyclic w.r.t.
C , RCv HT-safe w.r.t. C , ThC Horn-ALCHIQ TBox. Horn-ALCHIQ e algorithm takes triple , Rv Av , Th C obtained Definition 8 replacing
derivation rules Table 4 Table 5.
algorithm indeed import-by-query algorithm worst-case complexity algorithm non-Horn case.
Theorem 12. Horn-ALCHIQ e -algorithm import-by-query algorithm based
ABox entailment oracles class inputs C[C , RCv ACv , ThC ] Definition 9.
algorithm implemented runs N2ExpTime N , total number
oracle queries size query also exponential N ,
N = |Rv Av | + || input Rv , Av , .

proof Theorem 12 obtained modification one Theorem 11
given appendix.
5.2.2 Import-by-Query EL

section, present import-by-query algorithm based ABox entailment oracles
handle case Kv Th expressed EL. setting,
Theorems 5 7 provide clues features hinder existence import-by-query
algorithm. particular, longer necessary Kv acyclic.
algorithm based hypertableau framework, Kv first converted
set Rv EL-rules normalized ABox Av . Since EL allow inverse roles
231

fiCuenca Grau & Motik

Table 5: Additional Derivation Rules Horn KBs
connected component A| , individual ,
atomic concept {} exist
1. indirectly blocked A,
2. A(s) A,
3. eTh , (A , A(s)) =
A1 := {A(s)}

connected component A| , individuals ,
atomic roles R, R exist
1. neither indirectly blocked A,
2. R (s, t) R (t, s) ,
3. R(s, t) A,
4. eTh , (A , R(s, t)) =
A1 := {R(s, t)}

connected component A| individuals s, s1 , s2
exist
1. none s, s1 , s2 indirectly blocked A,
2. s1 s2 A,
3. atomic roles R, R exist
3.1 {R(s, s1 ), R (s, s2 )}
3.2 {R(s1 , s), R (s2 , s)}
3.3 {R(s1 , s), R (s, s2 )} A,
4. eTh , (A , s1 s2 ) =
A1 := {s1 s2 }


e -concept

e -role

e -

universal quantification, danger information propagating successor
predecessor; therefore, relax HT-safety condition shown Definition 10.
Definition 10. Let Rv set EL-rules, let signature Then, Rv EL-safe
w.r.t.
satisfies Condition 1 Definition 6,
rule Rv body atom form R(x, yi ) R ,
body contains atom B(yi ) B safe(Rv , ).
algorithm takes set Rv EL-safe rules normalized ABox Av . applies
standard EL hypertableau derivation rules; furthermore, like Horn-ALCHIQ
e -algorithm Section 5.2.1, uses oracle complete ABoxes encountered
derivation relevant concept assertions.
Definition 11. Let C[C , RCv ACv , ThC ] class inputs RCv set EL-rules
EL-safe w.r.t. C , ACv normalized ABox, ThC EL TBox. EL
e -algorithm takes triple , Rv Av , Th C obtained extending algorithm
Definition 2 e -concept derivation rule shown Table 5.

232

fiReasoning Ontologies Hidden Content

algorithm indeed import-by-query algorithm, implemented run
polynomial time, shown following theorem. contrast algorithms
presented thus far, EL e -algorithm optimal amenable implementation.
Theorem 13. EL e -algorithm import-by-query algorithm based ABox entailment oracles class inputs C[C , RCv ACv , ThC ] Definition 11. algorithm
implemented runs PTime N polynomial number N
calls eTh , , N = |Rv Av | + || input Rv , Av , .
proof Theorem 13 rather technical lengthy, given appendix.
intuition behind proof, however, case ALCHIQ algorithm, dierences due fact ABoxes produced EL
e -algorithm specific shape.

6. Lower Bound Complexity Import-by-Query Reasoning
section show import-by-query algorithm handles input
ALCHIQ -algorithm make polynomial number (in ||) queries
polynomial size (in ||). result applies already contains atomic
concepts, requirement ALCHIQ -algorithm applicable
TBox Kv semantically modular w.r.t. .

Theorem 14. Let C[C , KvC , ThC ] class inputs C contains atomic concepts, KvC ALCHIQ knowledge base semantically modular C , ThC
ALCHIQ TBox. Then, import-by-query algorithm ibqa based ABox satisfiability
oracles L = ALCHIQ C exists that, input , Kv , Th C, total
number oracle queries possible runs ibqa [Th , , L] Kv , well size
query, polynomial ||.
Proof. Assume ibqa algorithm satisfies theorems assumptions; then,
integers c1 c2 exist that, input , Kv , Th C, total number oracle
queries possible runs ibqa [Th , , L] Kv smaller equal ||c1 ,
maximal size query ABox smaller equal ||c2 .
next construct particular input C show ibqa violates
assumption. Let k arbitrary integer k c1 +c2 < 2k ; k exists since c1
c2 fixed. Let = {A1 , . . . , Ak } arbitrary atomic concepts, let Z, B, C1 , . . ., Ck ,
C 1 , . . ., C k atomic concepts occurring . Then, define Kv = Tv Av
Av = {Z(a)} Tv contains following axioms:
(50)

B R.B

Z B C1 . . . Ck

(51)

(C1 R.C 1 ) (C 1 R.C1 )

(53)

Cj C j

1jk

Cj1 R.C j1 (Cj R.C j ) (C j R.Cj ) 1 < j k

C j1 (Cj1 R.Cj1 ) (Cj R.Cj ) (C j R.C j ) 1 < j k
Ci Ai

1ik
233

(52)
(54)
(55)
(56)

fiCuenca Grau & Motik

C Ai

1ik

(57)

TBox Tv uses well-known integer counting technique (Tobies, 2000). Consider
arbitrary model Kv . Domain elements assigned integers 0
2k 1 means 2k atomic concepts C1 , . . ., Ck , C 1 , . . ., C k . Axiom (51) implies
aI (C k . . . C 1 )I , initializes counter 0. Axiom (50) ensures aI
origin infinite R-chain. Axioms (52) ensure domain element chain
labeled Cj C j . Axioms (53), (54), (55) increment counter R.
Finally, axioms together axioms (56) (57) ensure possible number
0 2k 1 assigned domain element R-chain. Clearly, Tv
semantically modular w.r.t. since extend interpretation symbols
model Tv interpreting symbols empty set.
Let Th1 = , let A1 , . . . , query ABoxes occurring possible runs

ibq [Th1 , , L] Kv , let n maximal size Ai 1 m. assumptions,
k c1 n k c2 , implies n = k c1 +c2 < 2k due way chose
k. 1 m, let Ai following ABox equivalent Ai :
Ai unsatisfiable, Ai = {}.

Ai satisfiable, let Ai ABox contains individual exactly one
concept assertion form D(s) disjunctive normal form; is,
expressed disjunction concepts form ()A1 . . . ()Ak . Ai
obtained Ai applying de Morgans laws.

Let D1 , . . . , disjunctive concepts occur satisfiable ABox Ai . Ai
contains n concepts, 1 n. Furthermore, let U subset
{D1 , . . . , } containing precisely Di exactly one disjunct. Finally, let
concept form ()A1 . . . ()Ak occur U ; exists
n < 2k . let Th2 following TBox:
Th2 = {S }

(58)

next show that, 1 j , concept Dj satisfiable w.r.t. Th2 .
claim trivial Dj contain S; otherwise, Dj contains disjunct = S,
interpretation satisfying Th2 Dj obtained interpreting nonempty set.
next show 1 ,,L (Ai ) = 2 ,,L (Ai ) 1 m; since Ai Ai
h

h

equivalent, 1 ,,L (Ai ) = 2 ,,L (Ai ) well. statement clearly holds Ai
h

h

unsatisfiable, assume Ai satisfiable. Since Ai consists assertions form
D(s) satisfiable w.r.t. Th2 , interpretation satisfying Ai Th2 obtained
disjoint union interpretations satisfying D.
Proposition 1, runs ibqa [Th1 , , L] Kv coincide runs

ibq [Th2 , , L] Kv ; however, straightforward see Kv Th1 satisfiable, whereas
Kv Th2 unsatisfiable, contradiction.

7. Related Work
currently growing interest techniques hiding parts ontology Th . One
possible approach hide subset signature Th first extracting Th 234

fiReasoning Ontologies Hidden Content

module subset Th preserves -consequences (i.e., logical consequences
formed using symbols )and publishing ontology Th \ . order
ensure sensitive information disclosed, module
depleting (Kontchakov, Pulina, Sattler, Schneider, Selmer, Wolter, & Zakharyaschev,
2009)that is, ontology Th \ indistinguishable empty ontology
w.r.t. -consequences. approach ensures -consequences disclosed
external applications oers additional advantage one reason union
Kv Th \ using o-the-shelf DL reasoners. Finally, although determining whether
subset ontology depleting module signature undecidable problem
many DLs (and hence extraction minimal depleting modules often computationally
infeasible), several practical techniques extracting (not necessarily minimal) depleting
modules known (Cuenca Grau, Horrocks, Kazakov, & Sattler, 2008).
important disadvantage approach module may also contain
relevant information sensitive (e.g., may entail consequences symbols
) hence union Kv (which may use symbols ) Th \ may
contain enough information answer relevant queries. Furthermore, adopting
approach, vendor Th would distribute subset axioms Th , may allow
competitors plagiarize parts Th . Finally, published axioms might mention symbols
(even entail -consequence) external applications would
aware presence symbols ontology.
drawbacks overcome publishing -interpolant Th
ontology contains symbols coincides Th logical consequences formed using symbols (Konev et al., 2009; Wang et al., 2009, 2008;
Lutz & Wolter, 2011; Nikitina, 2011). contrast module extraction approach, publishing interpolant ensures sensitive information Th (i.e., information
symbols Th mentioned interpolant) exposed way
external applications; furthermore, interpolants preserve consequences symbols
. Similarly module extraction approach, using interpolation additional
advantage developers Kv reason union Kv interpolant
using o-the-shelf DL reasoners.
interpolation approach may, however, several drawbacks. First, interpolant
may exist Th expressed relatively weak DL satisfies certain syntactic
conditions (Konev et al., 2009). contrast, import-by-query often possible even
interpolant Th signature interest exist.
Second, although interpolants preserve logical consequences formed using symbols
, robust replacement (Sattler et al., 2009)that is, union Kv
-interpolant Th guaranteed yield consequences Th Kv
query q involving symbols . example, given = {R} Th = {A R.B},
empty ontology -interpolant (it preserves consequences form C
C arbitrary boolean concepts signature {A, B}); however, Kv = {B }
Kv Th entails consequence , whereas union Kv
(empty) interpolant not. Thus, interpolant published, cannot
imported Kv guarantee relevant consequences preserved, unless
suitable restrictions imposed Kv .

235

fiCuenca Grau & Motik

Finally, -interpolant Th exponentially larger Th , may reveal
information strictly needed. Although import-by-query algorithms
also formulate worst-case exponentially many queries oracle, algorithms
may limit flow irrelevant information Th Kv , especially Th expressed
Horn DL, case import-by-query algorithms issue queries demand.
example, = {R, C}, = , Kv = {A R.B, B C} Th = {R.R.C C},
-interpolant equal Th thus publishing interpolant reveals entire contents Th .
contrast, import-by-query algorithm EL would reveal positive information
Th , would disclose fact ABox form {R(a, b), C(b)}
satisfiable w.r.t. Th .
idea accessing ontology oracle similar spirit proposal
Calvanese, De Giacomo, Lembo, Lenzerini, Rosati (2004) query answering peerto-peer setting. authors consider problem answering conjunctive query q
KBs Kv Kh mappings reformulating q queries evaluated
Kv Kh isolation. query reformulation algorithm accesses Kv , q
answered using oracle Kh . setting, however, focus reuse
data, rather schema. Since satisfiable Kh cannot aect subsumption concepts
Kv , results Calvanese et al. (2004) applicable schema reasoning.

8. Conclusion
paper, proposed studied import-by-query framework. results
provide flexible way ontology designers ensure selective access ontologies.
framework thus provides key theoretical insights issues surrounding ontology
privacy. Furthermore, believe algorithms practicable applied Horn
ontologies; thus, results provide starting point development practical importby-query systems.
problem import-by-query novel, see many open questions. example,
problem relevant theory practice allow hidden ontology
selectively export data schema statements.

Acknowledgments
extended version paper Import-by-Query: Ontology Reasoning
Access Limitations Bernardo Cuenca Grau, Boris Motik, Yevgeny Kazakov published IJCAI 2009 paper Pushing Limits Reasoning Ontologies
Hidden Content Bernardo Cuenca Grau Boris Motik published KR 2010.
research supported Royal Society EPSRC projects ExODA
(EP/H051511/1) HermiT (EP/F065841/1).

236

fiReasoning Ontologies Hidden Content

Appendix A. Proof Theorem 11
use following definitions intermediate results prove theorem.
Definition 12. ABox HT-ABox assertions satisfy following
conditions, B atomic negated atomic concept, role, R atomic role,
b named individuals, individual, j integers.
1. concept assertion form B(s) n S.B(s).

2. role assertion form R(a, b), R(s, s.i), R(s.i, s).

3. individual s.i occurs assertion A, contains role assertion
form R(s, s.i) R(s.i, s).
4. equality form s.i s.j, s.i.j s, s, b.i.

Furthermore, extended HT-ABox additionally allowed contain assertions
form R(s, s) s.i s.
Lemma 1. Let R set HT-rules let ABox. Then, ABox labeling
node derivation R HT-ABox.

Proof. proof straightforward modification proof Lemma 4 Motik et al.
(2009), due following observations: since HT-rules allow atoms
form R(x, x) head, one cannot derive atoms form R(s, s); this, turn,
guarantees one cannot derive equalities form s.i s.
Lemma 2. (Motik et al., 2009, Lemma 6) Let R set HT-rules let clashfree extended HT-ABox containing indirectly blocked individuals. derivation rule
applicable R A, R satisfiable.

Definition 13. weakened pairwise anywhere blocking, abbreviated w-blocking,
Definition 1, dierence following condition used instead
LA (s ) = LA (t ):
HT-rule R containing body atom form R(x, y) R(y, x)
R atomic role R LA (s, ) LA (s , s), atomic
concept occurring , LA (s ) LA (t ).

Lemma 3. Lemma 2 holds even derivation R uses w-blocking.

Proof (Sketch). Let ABox labeling leaf derivation R A; let
individual blocked w-blocking; let parents
t. proof Motik et al. (2009, Lemma 6) hold, must show HT-rule
applicable interpretation obtained unraveling . Let R arbitrary
HT-rule. contain body role atom role R LA (s, ) LA (s , s),
Hyp-rule cannot applied mapping (x) = s. Furthermore,
contain atomic concept A, fact LA (s ) LA (t ) vice versa
cannot aect applicability . Thus, straightforward modification proof
Motik et al. (2009, Lemma 6), construct model R unraveling .
straightforward see derivation rules Table 4 invalidate Lemma
1that is, given HT-ABox, always produce HT-ABox.
237

fiCuenca Grau & Motik

A.1 Termination
first show logical consequences datalog program D(Rv , Av ) Definition 7 overestimate ABoxes produced hypertableau algorithm; is,
show ABox (t) labeling derivation node homomorphically embedded
set ground facts entailed D(Rv , Av ).
= s.i either R(s, ) (t) R(s , s) (t) R , say
-successor s.
Lemma 4. Let Rv set HT-rules, let Av ABox, let signature, let
D(Rv , Av ) given Definition 7, let Th ALCHIQ TBox, let (T, )
derivation , Rv Av , Th . Then, derivation node , mapping
individuals (t) individuals D(Rv , Av ) exists satisfying following
properties individuals occurring (t):
1. A(s) (t) atomic concept implies D(Rv , Av ) |= A((s)).
2. R(s, ) (t) implies D(Rv , Av ) |= R((s), (s )).
3. successor (t), D(Rv , Av ) |= Succ((s), (s )).
4. -successor (t), D(Rv , Av ) |= -Desc((s), (s )).
5. n R.C(s) (t) R possibly inverse role, following conditions hold:
(a) C atomic concept, D(Rv , Av ) |= C(vC );
(b) D(Rv , Av ) |= ar(R, (s), vC );

(c) D(Rv , Av ) |= Succ((s), vC );

(d) R , D(Rv , Av ) |= -Desc((s), vC ).
6. (t), D(Rv , Av ) |= (s) (s ).
7. unnamed individual (t), atomic concept sig(Rv ) sig(Av ) exists
(s) = vA (s) = vA .
Proof. prove lemma induction structure derivation.
root node derivation, let map individual Av itself. ABox () = Av
trivially satisfies Properties 3, 4, 7 since Av contains named individuals. Properties
5 6 also hold trivially () normalized ABox hence contain
assertions form n R.C(s) form . Finally, Properties 1 2 hold
() D(Rv , Av ).
induction step, assume that, derivation node , ABox (t) satisfies
claim mapping . child node , consider possible
ways (t ) derived (t).
-rule: properties hold trivially (t ) .

238

fiReasoning Ontologies Hidden Content

A-cut: properties hold trivially (t ) except Property 1 case
(t ) = (t) {A(s)} . named individual (t), occurs Av Property 1 holds D(Rv , Av ) contains assertion A(s)
occurring Av . unnamed, successor individual (t); induction hypothesis (Property 3)
D(Rv , Av ) |= Succ((s ), (s)); however, D(Rv , Av ) contains formula (32)
, D(Rv , Av ) |= A((s)), required.
R-cut: properties hold trivially (t ) except Property 2 case
(t ) = (t) {R(s, )} R . Condition 2 R-cut R (s, ) (t)
atomic role R , D(Rv , Av ) |= R ((s), (s )) induction
assumption. Since R, R D(Rv , Av ) contains formulae (33) roles ,
D(Rv , Av ) |= R((s), (s )), (t ) satisfies Property 2 .
R -cut: properties hold trivially (t ) except Property 2 case
(t ) = (t) {R(s , s)} R . Condition 2 R -cut R (s, ) (t)
atomic role R , D(Rv , Av ) |= R ((s), (s )) induction
assumption. Since R, R D(Rv , Av ) contains formulae (34) roles ,
D(Rv , Av ) |= R((s ), (s)), (t ) satisfies Property 2 .
-rule: properties hold trivially (t ) .
-rule: Assume (t ) defined follows, n R.C(s) (t), si fresh
successors s, C possibly negated atomic concept:
(t ) = (t) { ar(R, s, si ), C(si ) | 1 n } { si sj | 1 < j n }
Let = {si vC | 1 n}. Properties 5 6 hold trivially (t ) ,
obvious Property 7 holds well. Hence, focus showing Properties 14. Property 1, assume C atomic concept; since Property
5(a) holds (t) induction assumption, D(Rv , Av ) |= C(vC ),
required. Property 2, since Property 5(b) holds (t) induction assumption, D(Rv , Av ) |= ar(R, (s), vC ), required. Property
3, since Property 5(c) holds (t) induction assumption,
D(Rv , Av ) |= Succ((s), vC ), Property 3 holds (t ) . Property 4,
assume R ; Property 5(d) holds (t) induction assumption,
D(Rv , Av ) |= -Desc((s), vC ), Property 4 holds (t ) .
Hyp-rule: Assume (t ) = (t) {} head atom HT-rule
form (2). Properties 3, 4, 7 hold trivially (t ) , focus
remaining properties. Condition 2 Hyp-rule, (t) contains individuals
s, s1 , . . . , sn statements left column following table
holds. then, induction assumption, statements right column
hold well.
Ai (s) (t)
Rij (s, si ) (t)
Sij (si , s) (t)
Bij (si ) (t)

D(Rv , Av ) |= Ai ((s))
D(Rv , Av ) |= Rij ((s), (si ))
D(Rv , Av ) |= Sij ((si ), (s))
D(Rv , Av ) |= Bij ((si ))





239

fiCuenca Grau & Motik

HT-rule , datalog program contains rule (31). Thus, statements
following table hold well:
D(Rv , Av ) |= tt(Ci ((s)))
((s), (s ))
D(Rv , Av ) |= Rij

((s ), (s))
D(Rv , Av ) |= Sij

D(Rv , Av ) |= Dij ((si ))
D(Rv , Av ) |= (si ) (sj )
Consequently, Properties 2 6 clearly hold; Property 1 also holds since
atomic concept atom tt() = . show Property 5, assume Ci ((s))
form n R.C((s)),
tt(Ci ((s))) = ar(R, (s), vC ) tt(C(vC )) Succ((s), vC ).
Then, following holds:
D(Rv , Av ) |= ar(R, (s), vC )
D(Rv , Av ) |= tt(C(vC ))
D(Rv , Av ) |= Succ((s), vC )
Thus, Properties (5a), (5b), (5c) hold. Finally, R , Property (5d) holds
datalog program entails assertion Succ((s), vC ), contains formulae
(38) (34) roles .
-cut rule: Assume (t ) = (t) {} assertion form s1 s2
s1 s2 . Then, (t) trivially satisfies Properties 15 7 . Property 6 also
holds trivially form s1 s2 , assume form s1 s2 .
preconditions -cut rule, individual (t) atomic roles R, R
exist
{ R(s, s1 ), R (s, s2 ) } (t)
{ R(s1 , s), R (s2 , s) } (t)
{ R(s1 , s), R (s, s2 ) } (t).
induction hypothesis (Property 2),

D(Rv , Av ) |= { R((s), (s1 )), R ((s), (s2 )) }
D(Rv , Av ) |= { R((s1 ), (s)), R ((s2 ), (s)) }
D(Rv , Av ) |= { R((s1 ), (s)), R ((s), (s2 )) }.
then, since datalog program contains formulas (35)(37) roles ,
D(Rv , Av ) |= (s1 ) (s2 ), required.
-rule: Assume (t ) = merge(t) (s ). Then, Conditions 1 2
-rule, (t) = . Furthermore, induction assumption,
D(Rv , Av ) |= (s) (s ). Since merging merely replaces , semantics
equality (t ) satisfies required properties.
next use Lemma 4 prove length chains role assertions involving
role bounded.
240

fiReasoning Ontologies Hidden Content

Lemma 5. Let Rv , Av , , D(Rv , Av ), (T, ) Lemma 4 additional
restriction Rv Av acyclic w.r.t. . Let N number individuals
form vC occurring D(Rv , Av ), let arbitrary derivation node (T, ), let
s1 , . . . , unnamed individuals occurring (t) si+1 -successor si
1 < . Then, N .
Proof. Assume that, integer > N , unnamed individuals s1 , . . . , satisfying
conditions lemma exist, let mapping satisfying Lemma 4. Property 7
Lemma 4, 1 (si ) = vCi Ci (because si unnamed).
Furthermore, Property 4 Lemma 4, also D(Rv , Av ) |= -Desc((si ), (si+1 ))
1 < . then, since > N predicate -Desc(x, y) axiomatized
transitive formula (39) D(Rv , Av ), clearly obtain harmful cycle,
contradiction.
ready prove main claim.
Lemma 6 (Termination). Let Rv , Av , , D(Rv , Av ), N , (T, ) Lemma 5.
Then, (T, ) finite.
Proof. Let depth individual number ancestors, let c r
numbers atomic concepts roles, respectively, occurring Rv Av ; finally, let
= (22cr + 1)(N + 1) + 1. Consider arbitrary derivation node . Let
individual (t) depth i(N + 1) + 1. simple induction i, one show
least ancestors blocking-relevant. induction base straightforward
= 0; furthermore, induction step holds because, Lemma 5 fact
(t) HT-ABox, depth nearest blocking-relevant ancestor
N + 1 less depth s. Thus, individual depth least 22cr + 1
blocking-relevant ancestors; since 22cr possible concept role labelings
individual predecessor, one blocking ancestors blocked due
definition blocking; hence, either directly indirectly blocked (t). rest
proof claim analogous proof Lemma 7 Motik et al. (2009).
A.2 Soundness
Lemma 7 (Soundness). Let Rv set HT-rules, let Th ALCHIQ TBox, let
ABox Rv Th satisfiable, let A1 , . . . , ABoxes obtained
applying derivation rule Table 2 4 Rv A. Then, Rv Th Ai satisfiable
1 n.
Proof. Let model Rv Th A, let us consider possible derivation rules
derive A1 , . . . , . cases Hyp-, -, -, -rule
proof Motik et al. (2009, Lemma 5). Furthermore, law excluded middle
first-order logic, claim true A, R-cut, R -cut -cut rules. Assume
-rule derives is, Th unsatisfiable connected component
A| . then, since A| A, monotonicity first-order logic Rv Th
unsatisfiable well, contradiction.

241

fiCuenca Grau & Motik

A.3 Completeness
Definition 14 Proposition 5 show part model implied Th
always extended model Rv . say assertion atomic form
A(a) atomic concept, R(a, b) R atomic role.
Definition 14. Let signature, let Rv set HT-rules, let nonempty
clash-free ABox containing exactly one individual sig(A) . ABox
Rv -extension w.r.t. following conditions hold:
1. contains exactly one individual, | = A, sig(A ) sig(Rv );
2. derivation rule Table 2 applicable Rv ;
3. contain assertion form A(s) safe(Rv , ).
Proposition 5. , Rv , Definition 14 Rv additionally HTsafe, least one Rv -extension w.r.t. exists.
Proof. Let individual occurring A, let = (I , ) interpretation
symbols defined follows:


{s} A(s)
{s, s} R(s, s)
= {s}
AI =
RI =

otherwise

otherwise
Since Rv HT-safe w.r.t. sig(A) , Proposition 3 model J Rv exists
J = , X J = X symbol X , X J = X safe(Rv , ).
define ABox follows:
= {s s}
{A(s) | AJ sig(Rv )}
{A(s) | AJ sig(Rv )}
{R(s, s) | s, RJ R sig(Rv )}
{ 1 R.A(s) | ( 1 R.A)J {R, A} sig(Rv )}
{ 1 R.A(s) | ( 1 R.A)J {R, A} sig(Rv )}
show Rv -extension w.r.t. . Since J coincides
interpretation atomic concepts roles , satisfies Properties 1 3
Definition 14. next show hypertableau derivation rule applicable
Rv . - -rule clearly applicable . Furthermore, construction
ensures 1 R.C(s) {R(s, s), C(s)} , -rule
applicable either. Finally, assume Hyp-rule applicable HT-rule
Rv mapping . Since contains individual s, mapping
maps variables s. Since J |= Rv , rule contains head atom Vj
J |= (Vj ). Note Vj form n R.C, n = 1 since J contains
one element. Thus, (Vj ) form A(s), R(s, s), 1 R.C(s), s,
sig(Rv ), R sig(Rv ), sig(C) sig(Rv ). then, construction
(Vj ) , contradicts assumption Hyp-rule applicable Rv
.
242

fiReasoning Ontologies Hidden Content

ready prove main claim section.
Lemma 8 (Completeness). Let , Rv Av , Th input ALCHIQ -algorithm.
derivation , Rv Av , Th contains leaf node labeled clash-free ABox,
Rv Av Th satisfiable.
Proof. Let ABox obtained clash-free ABox labeling leaf derivation
, Rv Av , Th removing assertions involving indirectly blocked individual. Since
Rv Av acyclic w.r.t. , ABox finite Lemma 6. Furthermore, clearly HTABox derivation rule applicable Rv , A, aTh , . Finally, straightforward
see mapping h individuals Av individuals exists
h(a) = individual occurring A, C(a) Av implies C(h(a)) A,
R(a, b) Av implies R(h(a), h(b)) A. Hence, model Rv Th extended
model Rv Av Th interpreting individual occurring Av
way h(a). Thus, prove lemma showing Rv Th satisfiable.
Let Rh result transforming Th set HT-rules described Motik
et al. (2009); then, Rv Av Th equisatisfiable Rv Av Rh , model
latter model former well. Therefore, rest proof extend
clash-free extended HT-ABox Afin derivation rule Table 2 applicable
Rv Rh Afin . Lemma 3, Rv Afin Rh satisfiable, which, together
Afin , implies satisfiability Rv Rh . proceeding construction
Afin , next introduce several useful definitions notational conventions.
Let v = sig(Rv ) sig(Av ) let h = sig(Rh ).
proof, term blocking refers version blocking given Definition
8; term w-blocking refers version blocking Definition 13; term sblocking refers standard blocking given Definition 1 additional
requirement individuals s, , t, unnamed.
blocked individual s, pick arbitrary fixed individual blocks
s, call blocker s.
modified hypertableau algorithm standard hypertableau algorithm Definition 1 dierence uses s-blocking
applied ABoxes contain unnamed individuals; individuals
treated algorithm named. modified hypertableau algorithm clearly sound, complete, terminating.
projection ABox set individuals ABox consisting exactly
assertions contain individuals S.
proceed construction Afin . end, split A| ABoxes
Anm follows; use ABoxes later construct Afin .
ABox Anm projection A| set containing named individuals
unnamed individuals connected named individual A| .
243

fiCuenca Grau & Motik

nonblocked blocking-relevant individual A, ABox projection
A| set containing (unnamed) individuals connected A| .
Let Anm
der result taking clash-free ABox labeling leaf derivation
Rh Anm modified hypertableau algorithm removing assertions
containing indirectly blocked individual; furthermore, nonblocked blockingrelevant individual A, let Atder obtained analogous way. ABoxes Anm
der
Atder exist aTh , (A ) = connected component Anm , aTh , (At ) =
t, modified hypertableau algorithm sound, complete, terminating.
Since supply unnamed individuals unlimited, assume without loss generality
-rule always introduces individuals globally freshthat is,
occur ABox.


next extend Anm
der Ader assertions necessary satisfy Rv . Let
nm


nm


(resp. ) let Ader Ader (resp. corresponding Ader ). say
individual u fresh Ader u occurs Ader . fresh
individual u Ader , define Ader [u] Rv -extension projection Ader | {u};
without loss generality, assume Ader [u1 ] = Ader [u2 ] u1 u2
projections Ader | {u1 } {u2 } isomorphic (i.e., identical renaming
individuals). Finally, let Afin union Ader Ader [u] u fresh

Ader ; thus, obtain ABoxes Anm
fin Afin . Condition 1 Definition 14, atomic

assertions Ader |sig(Rh ) coincide atomic assertions |sig(Rh ) . Furthermore,
since individuals involved s-blocking required unnamed isomorphic
individuals extended way, construction aect s-blockingthat
is, u s-blocked u s-blocked Ader .

define Afin ABox obtained


1. taking union A, Anm
fin , Afin nonblocked blocking-relevant individual
A,


2. adding A(s) blocked individual blocker A(s ) Asfin
h .3

Lemma 1, Anm
fin Afin HT-ABoxes, Afin clearly extended HT-ABox.
next show hypertableau derivation rule applicable Rv Rh Afin .
end, first show Afin satisfies following property (*): Afin
atomic assertion assertion form b sig() v individuals
mentioned occur A, A. particular, note extension Anm
der

nm

Ader Afin Afin , respectively, introduce atomic assertion involves
individual sig() (v \ ) = ; hence, possibility

Afin , A, sig() v Anm
der Ader t. consider next
former case; latter one analogous. prove (*) induction application

derivation rules construction Anm
der . end, show ABox
derivation Anm Rh satisfies following properties:

3. Note that, since blocked, blocking-relevant.

244

fiReasoning Ontologies Hidden Content

1. atomic assertion assertion form b sig() v
individuals mentioned occur A, A.
2. R(a, b) b occur R h \ , exists
S(a, b) S(b, a) A.
3. b occurs A, R individual c occurring
exist R(a, c) R(c, a) A.

base case trivial. next consider ways assertion derived.
application -rule -rule clearly preserves (1)(3). application
-rule, modified hypertableau algorithm treats individuals named;
furthermore, b b occur A, (1) b A, = b since
-rule applicable A; then, straightforward see (1)(3) remain
preserved. Finally, following types assertions relevant application
Hyp-rule HT-rule Rh :
A(a) . Since A-cut rule applicable A,
A(a) A(a) A, (1) holds.

R(a, b) b A. body contains atom matched
assertion R (a, b) R (b, a) R v satisfies induction
assumption; thus, exists S(a, b) S(b, a) A, (2) holds.
Furthermore, R , assertion satisfies preconditions R-cut
R -cut rule; since rules applicable A, R(a, b)
R(a, b) A, (1) holds.
b A. body contains atom matched assertion
R (a, c) R (c, a) R v satisfies induction assumption;
thus, exists S(a, c) S(c, a) A, (3) holds. Furthermore,
b A, body also contains atom matched assertion
R (a, c) R (c, a) satisfies induction assumption; thus,
exists (a, c) (c, a) A. precondition -cut rule
satisfied and, since rule applicable A, b b A,
(1) holds.
completes proof (1)(3). Property (*) straightforward consequence (1):
derivation assertion sig() v individuals mentioned occur
either makes dierence leads contradiction. straightforward consequence
(*) (59) (60) hold individuals u v occur A:
LAfin (u) v = LA (u)

LAfin (u, v) v = LA (u, v)

(59)
(60)

show derivation rule hypertableau algorithm w-blocking
applicable Rv Rh Afin . considering possible derivation rules.
(-rule) Assume -rule applicable assertion n R.C(s) Afin ,
w-blocked Afin . show blocked A, s-blocked

Anm
fin , s-blocked Afin . following cases.
245

fiCuenca Grau & Motik

n R.C(s) A. Assume blocked blocker t, let
predecessors t, respectively. definition blocking, (61)(65) hold:
LA (s) = LA (t)


(61)



LA (s ) = LA (t )


(62)


LA (s, ) = LA (t, )




LA (s , s) = LA (t , t)





LA (s, ) LA (s , s) v \

(63)
(64)
(65)

(59) (60), following properties hold well:
LAfin (s) v = LAfin (t) v




LAfin (s ) v = LAfin (t ) v

(66)
(67)

Furthermore, second item construction Afin ensures LAfin (s)
LAfin (t) coincide concept C h , ensures following property:
LAfin (s) = LAfin (t)

(68)

(65), A| contains assertion involving individuals , individuals . construction Afin , following properties hold:
LAfin (s, ) = LAfin (t, )




LAfin (s , s) = LAfin (t , t)

(69)
(70)

Consider rule Rv Rh . Rh , role body occurs
LAfin (s, ) LAfin (s , s), satisfies condition weakened pairwise anywhere
blocking. Rv , satisfies condition weakened pairwise anywhere
blocking due (67). Together (68)(70), implies w-blocked t,
contradiction. Consequently, blocked A.
n R.C(s) Anm
fin n R.C(s) A. occurs successor
individual occurs A, s-blocked Anm
fin since modified hypertableau algorithm treats individuals occurring named individuals
cannot s-blocked. Otherwise, construction Afin , LAfin (u) = LAnm
(u)
fin
nm A;
LAfin (u, v) = LAnm
(u,
v)


individuals
u

v
occurring


fin
fin
again, s-blocked Anm
fin .
n R.C(s) Atfin n R.C(s) A. case completely analogous
previous one.
Let ABox property holds; note n R.C(s) .
-rule applicable , contains individuals u1 , . . . , un
{ar(R, s, ui ), C(ui ) | 1 n} {ui uj | 1 < j n} .
construction Afin Afin , contradicts assumption
-rule applicable Afin .
246

fiReasoning Ontologies Hidden Content

(-rule, first variant) Property (59) holds individual occurring A, (71)

(72) hold individual occurring Anm
fin Afin , respectively.
LAfin (s) h = LAnm
(s) h
fin
LAfin (s) h = LAt (s) h
fin

(71)
(72)

Thus, {A(s), A(s)} Afin implies {A(s), A(s)} , A, Anm
fin ,


Afin . Since first variant -rule applicable , applicable
Afin either.
(-rule, second variant) Property (60) holds pair individuals occurring

A. Furthermore, Anm
fin Afin contain negative assertions already
present A. Since second variant -rule applicable A, Anm
fin ,

Afin , applicable Afin either.
(-rule, third variant) Suppose -rule applicable assertion form

Afin . construction Afin , A, Anm
fin , Afin

t. then, since -rule applicable , applicable Afin either.
(-rule) Assume -rule applicable Afin . Then, assertion
Afin exists = . construction Afin , , = A,



= Anm
fin , = Afin t. then, since -rule applicable ,
applicable Afin either.
(Hyp-rule) Assume Hyp-rule applicable Afin HT-rule Rv Rh
form (2). Thus, mapping variables individuals Afin exists
(Ui ) Afin 1 m, (Vj ) Afin 1 j n. Let = (x)
ui = (yi ). following possibilities:

Rh . Let ABox chosen among Anm
fin Afin containing individual s.
Consider ui . contains atom form Rij (x, yi ) Rij (yi , x)
Rij h , Afin contains assertion form Rij (s, ui ) Rij (ui , s).
definition blocking, pair individuals u v belong dierent Anm
, ABox contain assertion form (u, v) h ;

then, construction Afin , u v belong dierent Anm
fin Afin ,

ABox Afin contain assertion either. Thus, ui occur ,
Hyp-rule applicable , contradiction.

Rv . first show following property (**): ui occur
A, = uj uj . consider first case occur A.
Consider arbitrary uj . Since HT-rule, body contains atom
form Rjk (x, yj ) Rjk (yj , x), Afin contains assertion form Rjk (s, uj )
Rjk (uj , s). following two possibilities Rjk .
Rjk v \. construction Afin , assertion Rjk (s, uj ) Rjk (uj , s)
occurring must introduced via Rv -extension, uj = s.

Rjk . Since HT-safe w.r.t. , contains atom form A(x)
safe(Rv , ) body. Condition 3 Definition 14, A(s) Afin ,
contradiction.

247

fiCuenca Grau & Motik

case ui occur symmetric; dierence
case Rjk consider body atom B(yj ) B safe(Rv , ).

Let = occurs A, let ABox contains otherwise.
straightforward consequence (**) (Ui ) 1 m; furthermore, Afin (Vj ) Afin imply (Vj ) 1 j n. then,
Hyp-rule applicable , contradiction.
Thus, derivation rule hypertableau algorithm w-blocking applicable
Rv Rh Afin , Rv Rh Afin satisfiable Lemma 3. explained earlier,
proves claim lemma.
Lemmas 6, 7, 8 immediately imply Theorem 11.

Appendix B. Proof Theorem 12
termination argument Horn-ALCHIQ e -algorithm analogous nonHorn case: derivation , Rv Av , Th , node derivation,
find embedding Lemma 4; proof straightforward variant
proof given non-Horn case. Termination follows exactly non-Horn
case. Soundness consequence soundness standard hypertableau algorithm
together following lemma.
Lemma 9. Let Rv set HT-rules, let Th Horn-ALCHIQ TBox, let
ABox Rv Th satisfiable. Furthermore, let A1 ABox obtained
applying derivation rule Table 5 Rv A. Then, Rv Th A1 satisfiable.
Proof. Let model Rv Th A, let us assume derivation rule
Table 5 derives A1 = {}. preconditions e -concept, e -role, e -
rules, eTh , (A , ) = connected component A| , Th |= . Since
A| A, |= Th , |= , implies claim.
show completeness algorithm. set HT-rules R Horn,
derivation hypertableau algorithm contains exactly one leaf node,
identify derivation sequence ABoxes A0 , . . . , . following proposition
straightforward consequence fact R Horn set HT-rules.
Proposition 6. Let R set Horn HT-rules, let ABox, let A0 , . . . ,
derivation R A. Then, assertion mentions individuals
Ai 1 n, R |= .
Lemma 10 (Completeness). Let , Rv Av , Th input Horn-ALCHIQ e algorithm. derivation , Rv Av , Th contains leaf node labeled clash-free
ABox, Rv Av Th satisfiable.
Proof. proof analogous proof Lemma 8: given ABox labeling
derivation leaf, construct ABox Afin derivation rule hypertableau
algorithm w-blocking applicable Rv Rh Afin . construction
248

fiReasoning Ontologies Hidden Content

bulk proof exactly Lemma 8, next prove properties
aected dierence derivation rules.
preconditions derivation rules Table 5 clearly ensure that, whenever
derivation rule applied HT-ABox, result also HT-ABox; consequently, Afin
extended HT-ABox.
next show property (*) holds despite change derivation rules:
Afin atomic assertion assertion form b sig() v
individuals mentioned occur A, A. particular, note
construction Afin introduce atomic assertion involves individual
sig() (v \ ) = . Assume sig() individuals
occur A. Proposition 6 Rh |= . Furthermore, say
Lemma 8 one show preconditions e -concept, e -role, e - rule
satisfied A; since relevant rule applicable A, A, proves
claim.
rest proof exactly Lemma 8.
Theorem 12 follows immediately Lemmas 9 10.

Appendix C. Proof Theorem 13
set EL-rules R ABox A, derivation EL hypertableau algorithm contains exactly one leaf node, identify derivation sequence ABoxes
A0 , A1 , . . . , . Since Aj1 Aj 1 n, ABox labeling derivation leaf
uniquely defined R A. following lemma captures relevant properties
standard EL hypertableau algorithm, proved slight variation
proofs Motik Horrocks (2008) Baader et al. (2005).
Lemma 11. Let R set EL-rules, let ABox containing named individuals, let Af ABox labeling leaf derivation R A. following
properties hold pair atomic concepts A, B sig(R) individual A:
1. A(s) Af R |= A(s).
2. B(aA ) Af R |= B.
3. R R, Af Af , Af ABox labeling
leaf derivation R .
like EL hypertableau algorithm, derivation EL e -algorithm
contains exactly one leaf node, ABox labeling derivation leaf uniquely defined
, Rv Av , Th . next show several useful properties algorithm.
Lemma 12. Let , Rv Av , Th input EL e -algorithm let Ae
ABox labeling leaf derivation , Rv Av , Th . following holds.
1. Let Rh set EL-rules corresponding Th described Motik et al. (2009),
let AEL ABox labeling leaf derivation standard EL hypertableau
algorithm Rv Rh Av ; then, Ae AEL .
249

fiCuenca Grau & Motik

2. Ae B(aA ) Ae , B safe(Rv , ).
Proof. (Claim 1) Let A0 , . . . , derivation EL e -algorithm Rv , Av ,
eTh , A0 = Av = Ae . prove claim inductively showing
Aj AEL 0 j n. induction base, clearly A0 AEL . Assume
Aj1 AEL let Aj obtained Aj1 applying derivation rule
EL e -algorithm. Hyp-rule applied Aj1 Rv , Rv Rh ,
Aj1 AEL , fact Hyp-rule applicable AEL imply Aj AEL .
argument analogous -rule. Finally, assume e -concept rule derives
A(s) {} Aj1 . preconditions e -concept rule,
eTh , (A , A(s)) = connected component Aj1 | . Property 1 Lemma
11 A(s) , ABox labeling leaf derivation standard EL
hypertableau algorithm Rh . AEL , Rh Rh Rv , Property 3
Lemma 11 imply AEL ; consequently, A(s) AEL Aj AEL .
(Claim 2) Consider arbitrary individual form aA arbitrary
assertion B(aA ) Ae . Claim 1, B(aA ) AEL , Property 2 Lemma 11
Av Rv Rh |= B. Since Rv Rh EL-rules, Av aect subsumption
inferences, Rv Rh |= B. Since Ae , interpretation exists AI =
|= Rh . Assume B safe(Rv , ). Proposition 3 fact Rv
EL-safe, model J Rv exists X J = X X sig(Rh ), B J = .
Thus, J |= Rv Rh , contradicts fact Rv Rh |= B.
Soundness EL e -algorithm follows immediately Property 1 Lemma 12
fact standard EL hypertableau algorithm sound. next prove
algorithm complete.
Lemma 13 (Completeness). Let , Rv Av , Th input EL e -algorithm let
Th EL TBox, let Ae ABox labeling leaf derivation , Rv Av , Th .
Ae , Rv Av Th satisfiable.
Proof. Let Rh result transforming Th set EL-rules described Motik
et al. (2009); then, Rv Av Th equisatisfiable Rv Av Rh , model
latter model former well. Therefore, rest proof extend
clash-free ABox Afin derivation rule Table 2 applicable Rv Rh
Afin . Lemma 3, Rv Afin Rh satisfiable, which, together Afin ,
implies satisfiability Rv Rh . Let v = sig(Rv ) sig(Av ) h = sig(Rh ).
next present construction Afin . first step extend Ae
satisfies Rh , achieve applying EL hypertableau algorithm Rh Ae .
assume individuals Ae form aA reused whenever v .
call individuals Ae old freshly introduced individuals new, call
individual -relevant form aA .
next show ABox Aj derivation Rh Ae satisfies following
properties (*):
1. Aj implies Ae whenever form
(a) C(s) sig(C) v old individual,
250

fiReasoning Ontologies Hidden Content

(b) R(s, t) R v old individuals.
2. C(s) Aj , following properties hold:
(a) sig(C) v sig(C) h ,

(b) new individual, sig(C) h .
3. R(s, t) Aj , following properties hold:
(a) new individual, R h ,

(b) new old, -relevant R h .
proof (*) induction application derivation rules.
induction base, A0 = Ae . Statements (1) (2a) hold trivially, (2b) (3)
vacuously true since Ae contains old individuals. Assume (1)(3) hold
Aj1 consider application derivation rule derives Aj .
Assume -rule applied R.A(s) Aj1 , deriving R(s, aA ) A(aA ).
{R, A} v old, R.A(s) Ae induction assumption; since rule applicable Ae , {R(s, aA ), A(aA )} Ae , (1) holds. Furthermore,
v \ , old (2b), R v (2a); R.A(s) Ae , -rule
cannot applicable R.A(s) Aj1 . Consequently, {R, A} h , A(aA )
clearly satisfies (2), R(s, aA ) clearly satisfies (3a). Finally, aA old ,
R(s, aA ) clearly satisfies (3b).
Assume Hyp-rule applied EL-rule Rh form (8), deriving
C(s). Then, individuals t1 , . . . , tm Aj1 exist Ai (s) Aj1 1 k
{Ri (s, ti ), Bi,1 (ti ), . . . , Bi,mi (ti )} Aj1 1 m. ABox Aj trivially satisfies
(1b) (3), satisfies (2) Rh , sig(C) h . show (1a), assume
old individual sig(C) v ; since Rh , sig(C) . Property 1
Lemma 11, Rh Ae |= C(s). Since sig(C) , Rh Ae | |= C(s). Since
e -concept rule applicable Ae , C(s) Ae , Aj satisfies (1a).
completes proof (*). Let Ader ABox labeling leaf derivation
EL hypertableau algorithm Rh Ae . Ader clash-free since
/ Ae
e -rule applicable Ae ; furthermore, Ader satisfies (*).
extend Ader EL-rules Rv satisfied matched
new individuals. end, new individual u Ader , let Ader [u] Rv extension w.r.t. projection Ader {u}; Ader [u] exists Proposition 5
fact Rv EL-safe. Let Afin union Ader Ader [u]. Since
Av Ae Ae Afin , Av Afin . Furthermore, since Ader Ader [u]
u new Ader , Afin . Finally, (*), Property 2 Lemma
12, fact Ader [u] contains one individual safe concepts, Afin
satisfies following properties (**):
1. B(s) Afin -relevant new, B safe(Rv , ).
2. R(s, t) Afin , following properties hold:
(a) new individual = t, R h ,
251

fiCuenca Grau & Motik

(b) new old, -relevant R h .
complete proof lemma, show derivation rule hypertableau
algorithm applicable Afin Rv Rh .
(-rule) Consider arbitrary R.C(s) Afin . R.C(s) Ader , since -rule
applicable Ader , {R(s, t), C(t)} Ader Afin . R.C(s) Ader [u]
individual u new Ader , Definition 14 -rule applicable Ader [u],
{R(s, t), C(t)} Ader [u] Afin . Either way, -rule applicable R.C(s) Afin .
(Hyp-rule) Assume Hyp-rule applicable EL-rule Rv Rh
form (8), deriving C(s). Then, individuals t1 , . . . , tm Afin exist Ai (s) Afin
1 k {Ri (s, ti ), Bi,1 (ti ), . . . , Bi,mi (ti )} Afin 1 m.
following possibilities:
Rh . new individual u assertion Ader [u] \ Ader , Definition 14 either sig() v \ form R.C. Thus, Ai (s) Ader
1 k {Ri (s, ti ), Bi,1 (ti ), . . . , Bi,mi (ti )} Ader 1 m,
Hyp-rule applicable Ader , contradiction.
Rv . first show following property (***): ti new, = tj
tj . following cases.
Assume new consider arbitrary 1 m. Clearly, Ri v ;
furthermore, Ri , since EL-safe, body contains atom
matched Bij (ti ) Afin Bij safe(Rv , ). Assume
ti = s. ti new, Ri Statement (2a) (**); furthermore, ti
old, Ri ti -relevant Statement (2b) (**); consequently,
Ri ti either new -relevant. then, Statement (1) (**)
Property 3 Definition 14, Bij (ti ) Afin , contradiction. Hence,
conclude = ti .
Assume ti new 1 m. ti = s, Statement (2a) (**)
Ri . Since EL-safe, body contains atom
matched Bij (ti ) Afin Bij safe(Rv , ). Statement (1)
(**) implies Bij (ti ) Afin , contradiction. Hence, conclude
= ti ; previous case = tj 1 j m.
Let = Ae old, = Ader [s] otherwise. straightforward consequence
(***) Ai (s) 1 k {Ri (s, ti ), Bi,1 (ti ), . . . , Bi,mi (ti )}
1 m. Hyp-rule applicable , C(s) . Since
Afin , C(s) Afin , contradicts assumption Hyp-rule
applicable Afin .
completes proof lemma.
Finally, prove EL e -algorithm optimal import-by-query algorithm.

252

fiReasoning Ontologies Hidden Content

Theorem 13. EL e -algorithm import-by-query algorithm based ABox entailment oracles class inputs C[C , RCv ACv , ThC ] Definition 11. algorithm
implemented runs PTime N polynomial number (in N )
calls eTh , , N = |Rv Av | + || input Rv , Av , .

Proof. EL e -algorithm import-by-query algorithm straightforward
consequence Lemmas 12 13. estimate algorithms running time, note
application derivation rule adds assertion form C(a) R(a, b)
C v {}, b individuals occurring either Av form aA
sig(Rv ). Clearly, maximal number individuals occurring ABox
derivation polynomial size Av , Rv , , maximal number
assertions. Furthermore, derivation rule removes assertions ABox,
number assertions ABox monotonically increases course derivation.
Consequently, number rule applications polynomial size Av , Rv , .
way standard EL hypertableau algorithm (Motik & Horrocks, 2008),
one show derivation rule applied polynomial time, implies
claim theorem.

References
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL Envelope. Kaelbling, L. P., &
Saotti, A. (Eds.), Proc. 19th Int. Joint Conference Artificial Intelligence
(IJCAI 2005), pp. 364369, Edinburgh, UK. Morgan Kaufmann Publishers.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2007). Description Logic Handbook: Theory, Implementation Applications
(2nd edition). Cambridge University Press.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
Reasoning Ecient Query Answering Description Logics: DL-Lite Family.
Journal Automated Reasoning, 9 (3), 385429.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2004).
Ask Peer: Ontolgoy-based Query Reformulation. Dubois, D., Welty, C. A., &
Williams, M.-A. (Eds.), Proc. 9th Int. Conf. Principles Knowledge
Representation Reasoning (KR 2004), pp. 469478. AAAI Press.
Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2007). Logical Framework
Modularity Ontologies. Veloso, M. M. (Ed.), Proc. 20th Int. Joint Conf.
Artificial Intelligence (IJCAI 2007), pp. 298303, Hyderabad, India.
Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2008). Modular Reuse Ontologies: Theory Practice. Journal Artificial Intelligence Research, 31, 273318.
Cuenca Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P., & Sattler, U.
(2008). OWL 2: next step OWL. Journal Web Semantics: Science, Services
Agents World Wide Web, 6 (4), 309322.
Doran, P., Tamma, V. A. M., & Iannone, L. (2007). Ontology module extraction ontology
reuse: ontology engineering perspective. Silva, M. J., Laender, A. H. F., BaezaYates, R. A., McGuinness, D. L., Olstad, B., Olsen, . H., & Falcao, A. O. (Eds.), Proc.
253

fiCuenca Grau & Motik

16th ACM Conference Information Knowledge Management (CIKM
2007), pp. 6170, Lisbon, Portugal. ACM.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDF
OWL: making Web Ontology Language. Journal Web Semantics, 1 (1),
726.
Horrocks, I., & Sattler, U. (1999). Description Logic Transitive Inverse Roles
Role Hierarchies. Journal Logic Computation, 9 (3), 385410.
Horrocks, I., & Sattler, U. (2005). Tableaux Decision Procedure SHOIQ. Proc.
19th Int. Joint Conf. Artificial Intelligence (IJCAI 2005), pp. 448453,
Edinburgh, UK. Morgan Kaufmann Publishers.
Hustadt, U., Motik, B., & Sattler, U. (2005). Data Complexity Reasoning Expressive Description Logics. Proc. 19th Int. Joint Conf. Artificial Intelligence
(IJCAI 2005), pp. 466471, Edinburgh, UK. Morgan Kaufmann Publishers.
Jimenez-Ruiz, E., Cuenca Grau, B., Sattler, U., Schneider, T., & Berlanga Llavori, R.
(2008). Safe Economic Re-Use Ontologies: Logic-Based Methodology
Tool Support. Bechhofer, S., Hauswirth, M., Homann, J., & Koubarakis, M.
(Eds.), Proc. 5th European Semantic Web Conference (ESWC 2008), Vol. 5021
LNCS, pp. 185199, Tenerife, Spain. Springer.
Konev, B., Lutz, C., Walther, D., & Wolter, F. (2008). Semantic Modularity Module
Extraction Description Logics. Ghallab, M., Spyropoulos, C. D., Fakotakis, N.,
& Avouris, N. M. (Eds.), Proc. 18th European Conf. Artificial Intelligence
(ECAI 2008), Vol. 178 FAIA, pp. 5559, Patras, Greece. IOS Press.
Konev, B., Walther, D., & Wolter, F. (2009). Forgetting Uniform Interpolation
Large-Scale Description Logic Terminologies. Boutilier, C. (Ed.), Proc. 21st
Int. Joint Conf. Artificial Intelligence (IJCAI 2009), pp. 830835, Pasadena, CA,
USA.
Kontchakov, R., Pulina, L., Sattler, U., Schneider, T., Selmer, P., Wolter, F., & Zakharyaschev, M. (2009). Minimal Module Extraction DL-Lite Ontologies Using
QBF Solvers. Boutilier, C. (Ed.), Proc. 21st Int. Joint Conf. Artificial
Intelligence (IJCAI 2009), pp. 836841, Pasadena, CA, USA.
Lutz, C., Walther, D., & Wolter, F. (2007). Conservative Extensions Expressive Description Logics. Veloso, M. M. (Ed.), Proc. 20th Int. Joint Conf. Artificial
Intelligence (IJCAI 2007), pp. 453458, Hyderabad, India.
Lutz, C., & Wolter, F. (2010). Deciding inseparability conservative extensions
description logic EL. Journal Symbolic Computation, 45 (2), 194228.

Lutz, C., & Wolter, F. (2011). Foundations Uniform Interpolation Forgetting
Expressive Description Logics. Walsh, T. (Ed.), Proc. 22nd Int. Joint Conf.
Artificial Intelligence (IJCAI 2011), pp. 989995, Barcelona, Spain.
Motik, B., & Horrocks, I. (2008). Individual Reuse Description Logic Reasoning.
Armando, A., Baumgartner, P., & Dowek, G. (Eds.), Proc. 4th Int. Joint Conf.
Automated Reasoning (IJCAR 2008), Vol. 5195 LNAI, pp. 242258, Sydney,
NSW, Australia. Springer.
254

fiReasoning Ontologies Hidden Content

Motik, B., Shearer, R., & Horrocks, I. (2009). Hypertableau Reasoning Description
Logics. Journal Artificial Intelligence Research, 36, 165228.
Nikitina, N. (2011). Forgetting General EL Terminologies. Rosati, R., Rudolph, S.,
& Zakharyaschev, M. (Eds.), Proc. 24th Int. Workshop Description Logics
(DL 2011), Vol. 745 CEUR Workshop Proceedings, Barcelona, Spain.
Papadimitriou, C. H. (1993). Computational Complexity. Addison Wesley.
Sattler, U., Schneider, T., & Zakharyaschev, M. (2009). Kind Module
Extract?. Cuenca Grau, B., Horrocks, I., Motik, B., & Sattler, U. (Eds.), Proc.
22nd Int. Workshop Description Logics (DL 2009), Vol. 477 CEUR Workshop
Proceedings, Oxford, UK.
Stuckenschmidt, H., Parent, C., & Spaccapietra, S. (Eds.). (2009). Modular Ontologies:
Concepts, Theories Techniques Knowledge Modularization, Vol. 5445 LNCS.
Springer.
Tobies, S. (2000). Complexity Reasoning Cardinality Restrictions Nominals
Expressive Description Logics. Journal Artificial Intelligence Research, 12, 199
217.
Wang, K., Wang, Z., Topor, R. W., Pan, J. Z., & Antoniou, G. (2009). Concept Role
Forgetting ALC Ontologies. Bernstein, A., Karger, D. R., Heath, T., Feigenbaum,
L., Maynard, D., Motta, E., & Thirunarayan, K. (Eds.), Proc. 8th Int. Semantic
Web Conference (ISWC 2009), Vol. 5823 LNCS, pp. 666681, Chantilly, VA, USA.
Springer.
Wang, Z., Wang, K., Topor, R. W., & Pan, J. Z. (2008). Forgetting Concepts DL-Lite.
Bechhofer, S., Hauswirth, M., Homann, J., & Koubarakis, M. (Eds.), Proc.
5th European Semantic Web Conference (ESWC 2008), Vol. 5021 LNCS, pp.
245257. Springer.
Wang, Z., Wang, K., Topor, R. W., & Zhang, X. (2010). Tableau-based Forgetting
ALC Ontologies. Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proc.
19th European Conference Artificial Intelligence, Vol. 215 Frontiers Artificial
Intelligence Applications, pp. 4752, Lisbon, Portugal. IOS Press.

255

fiJournal Artificial Intelligence Research 45 (2012) 481514

Submitted 06/12; published 11/12

Complexity Judgment Aggregation
Ulle Endriss
Umberto Grandi
Daniele Porello

ulle.endriss@uva.nl
umberto.uni@gmail.com
danieleporello@gmail.com

Institute Logic, Language Computation
University Amsterdam
Postbus 94242, 1090 GE Amsterdam
Netherlands

Abstract
analyse computational complexity three problems judgment aggregation:
(1) computing collective judgment profile individual judgments (the winner
determination problem); (2) deciding whether given agent influence outcome
judgment aggregation procedure favour reporting insincere judgments (the
strategic manipulation problem); (3) deciding whether given judgment aggregation
scenario guaranteed result logically consistent outcome, independently
judgments supplied individuals (the problem safety agenda).
provide results specific aggregation procedures (the quota rules, premisebased procedure, distance-based procedure) classes aggregation procedures
characterised terms fundamental axioms.

1. Introduction
Judgment aggregation (JA) branch social choice theory studies properties
procedures amalgamating several agents individual judgments regarding truth
falsity set inter-related propositions collective judgment reflecting views
group agents whole (List & Pettit, 2002; List & Puppe, 2009). classic
example due Kornhauser Sager (1993): Suppose three judges decide
legal case involving possible breach contract. Two relevant propositions
really binding contract rather informal promise (proposition p)
defendant broke promise (proposition q). defendant pronounced
guilty conjunction two propositions found true (p q). judges
take following views matter:
Judge 1
Judge 2
Judge 3
Majority

p
Yes
Yes

Yes

q
Yes

Yes
Yes

pq
Yes




Note position individual judge logically consistent. However,
aggregate information using majority rule (i.e., accept proposition
strict majority judges do), arrive collective judgment set
inconsistent. paradoxical outcome, variations known doctrinal
c
2012
AI Access Foundation. rights reserved.

fiEndriss, Grandi, & Porello

paradox (Kornhauser & Sager, 1993) discursive dilemma (Pettit, 2001), inspired
important fast growing literature JA, starting seminal contribution
List Pettit (2002), showed fact aggregation procedure satisfying certain
axioms encoding natural desiderata avoid kind paradox.
literature JA largely developed outlets associated Philosophy, Economic Theory, Political Science, Logic, recently JA also come recognised
relevant AI, particularly design analysis multiagent systems.
reasons clear: multiagent system, different autonomous software agents may
different opinions issues (maybe due difference access relevant
information, due different reasoning capabilities), joint course action
needs extracted diverse views. Indeed, AI, related problem belief
merging studied time (see, e.g., Konieczny & Pino Perez, 2002; MaynardZhang & Lehmann, 2003; Chopra, Ghose, & Meyer, 2006; Everaere, Konieczny, & Marquis,
2007), interesting parallels literature JA (Pigozzi, 2006).
JA also found relevant analysis abstract argumentation frameworks
widely studied AI (Caminada & Pigozzi, 2011; Rahwan & Tohme, 2010).
Given relevance JA AI, important understand computational aspects.
However, date, received relatively little attention literature.
course explained origins field Law, Economics, Philosophy.
domains social choice, particularly voting fair division, hand,
recent focus computational aspects successful given rise
field computational social choice (Chevaleyre, Endriss, Lang, & Maudet, 2007; Brandt,
Conitzer, & Endriss, 2012).
help bridge gap, paper shall analyse computational complexity
three important problems arise JA:
Winner determination. winner determination problem problem computing result applying given aggregation procedure given profile individual judgment sets. immediate practical relevance applications JA.
obtain positive negative results: two types aggregation procedures,
namely quota rules premise-based procedure, winner determination
problem easily seen polynomial, certain distance-based procedure
obtain interesting intractability result, establishing completeness parallel
access NP mirrors corresponding results voting theory Dodgson rule
(Hemaspaandra, Hemaspaandra, & Rothe, 1997), Young rule (Rothe, Spakowski,
& Vogel, 2003) Kemeny rule (Hemaspaandra, Spakowski, & Vogel, 2005).
Strategic manipulation. agent may try influence result aggregation
favour reporting set judgments different truthfully held
beliefs. manipulation problem asks, given aggregation procedure, given
profile judgment sets, given agent, whether agent opportunity
manipulate successfully situation. one natural way defining preferences
top JA framework (namely terms Hamming distance)
aggregation procedures independent monotonic, well-known
agents never incentive manipulate (Dietrich & List, 2007c).
cases, interesting explore hard solve manipulation problem,
482

fiComplexity Judgment Aggregation

high complexity might signal certain level immunity manipulation.
context voting, kind question lead series interesting important
results (Bartholdi, Tovey, & Trick, 1989; Faliszewski & Procaccia, 2010), even
careful over-interpret theoretical intractability results necessarily
providing protection practice (Walsh, 2011). one widely used procedure (with
easy winner determination problem), namely premise-based procedure,
able show NP-completeness manipulation problem.
Safety agenda. paradox presented shows aggregation procedures possible obtain collective judgment set logically
inconsistent, even though judgment sets supplied individuals
consistent. important parameter determining possibility paradox
agenda, set propositions pass judgment. given aggregation procedure, problem safety agenda asks whether given agenda
safe sense profile individual judgment sets consistent
ever result collective judgment set inconsistent. various classes
aggregation procedures, defined terms classical axioms, prove safety theorems
fully characterise agendas safe sense relate results
known possibility theorems JA literature (List & Puppe, 2009).
study complexity deciding whether given agenda meets safety conditions
identified find typically highly intractable problem located
second level polynomial hierarchy.
results build extend earlier work complexity judgment aggregation (Endriss, Grandi, & Porello, 2010a, 2010b).
remainder paper organised follows. Section 2 introduce formal
framework JA, including several concrete aggregation procedures important
axioms used define desiderata procedures. Section 2 also includes proofs
simple representation results characterise aggregation procedures satisfy
certain combinations axioms. Section 3 devoted study complexity
winner determination problem Section 4 manipulation
problem. Section 5 introduce problem safety agenda, prove
several agenda characterisation theorems establishing necessary sufficient conditions
safety, finally study complexity deciding whether conditions satisfied.
Section 6 reviews related work computational aspects JA Section 7 concludes
discussion possible avenues future work.
Throughout paper, shall assume familiarity basics complexity theory notion NP-completeness. Helpful introductions include textbooks
Papadimitriou (1994) Arora Barak (2009).

2. Formal Framework Judgment Aggregation
section provide succinct exposition formal framework JA (List &
Puppe, 2009), originally laid List Pettit (2002) since
refined number authors, notably Dietrich (2007). also define three
concrete (families of) aggregation procedures discuss important axiomatic
483

fiEndriss, Grandi, & Porello

properties literature. Finally, prove number representation results,
status folk theorems JA literature often play crucial role
proofs complex results, rarely stated explicitly.
2.1 Notation Terminology
Let L set propositional formulas built finite set propositional variables
using usual connectives , , , , , constants (true) (false).
every formula , define complement , i.e., =
negated, = = formula . say set closed
complementation case whenever .
Definition 1. agenda finite nonempty set L contain
doubly-negated formulas closed complementation.
is, slight departure common definition literature (List & Puppe,
2009), allow tautologies contradictions agenda. reason want
study computational complexity JA, recognising tautology contradiction
computationally intractable problem. write + set non-negated
formulas , i.e., = + { | + }.
Definition 2. judgment set J agenda subset J .
call judgment set J complete J J ; call complementfree 1 case complement J;
call consistent exists assignment makes formulas J true. Let J ()
denote set complete consistent subsets .
shall occasionally interpret judgment set J (characteristic) function J :
{0, 1} J() = 1 J J() = 0 6 J. Hamming distance H(J, J )
two (complete complement-free) judgment sets J J number
positive formulas differ:
X
H(J, J ) =
|J() J ()|
+

Given set N = {1, . . . , n} n > 1 individuals (or agents), write J = (J1 , . . . , Jn )
J ()n generic profile judgment sets, one individual.2 ease exposition,
shall assume n odd (i.e., n > 3). write NJ = {i N | Ji } set
individuals accepting formula profile J .
Definition 3. (resolute) judgment aggregation procedure agenda
set individuals N n = |N | function F : J ()n 2 .
1. property called weak consistency Dietrich (2007), consistency List Pettit (2002).
choice terminology intended emphasise fact purely syntactic notion,
involving model-theoretic concept, distinction believe worth stressing.
2. previous work used general notation J ()N (i.e., set functions N
J ()) set admissible profiles (Endriss et al., 2010a). useful N might infinite
necessarily want associate set individuals set natural numbers,
require level generality here.

484

fiComplexity Judgment Aggregation

is, aggregation procedure maps profile individual (complete consistent)
judgment sets single collective judgment set (an element powerset ).
shall occasionally refer assumption individual judgment sets complete
consistent individual rationality. Note collective judgment set need
complete consistent (that is, collective rationality need hold). kind
procedure defined called resolute, return single judgment set
profile. Later, shall also discuss irresolute JA procedures, may return
nonempty set judgment sets (that tied winning). Finally, note that, since F
defined set profiles consistent complete judgment sets, implicitly
making universal-domain assumption, sometimes stated separate property
(List & Pettit, 2002).
2.2 Axiomatic Properties
Definition 3 put constraints collective judgment set, outcome
aggregation process. role following definition:
Definition 4. judgment aggregation procedure F , defined agenda , said be:
(i) complete F (J ) complete every J J ()n ;
(ii) complement-free F (J ) complement-free every J J ()n ;
(iii) consistent F (J ) consistent every J J ()n .
present several axioms provide normative framework state
desirable properties acceptable aggregation procedure be. Note
every procedure satisfy every axiom. Rather, axioms model desiderata
procedures satisfy others not. first axiom basic requirement, restricting
possible aggregators F terms fundamental properties outcomes produce.
Weak Rationality (WR): F complete complement-free.3
condition differs called collective rationality literature
JA (List & Puppe, 2009), require collective judgment set consistent.
first reason include consistency basic notion rationality
requirements (WR) purely syntactic notions easily checked automatically,
case consistency. second reason consistency intrinsic
aggregation function, rather depends properties agenda. point
made precise Section 5, study consistency class
aggregators depending agenda.
following important axioms discussed literature JA (List &
Pettit, 2002; Dietrich, 2007; List & Puppe, 2009; Nehring & Puppe, 2010):
Unanimity (U): Ji agents N , F (J ).
3. previous work (Endriss et al., 2010a), used definition weak rationality addition
completeness complement-freeness also included (very weak) technical requirement
contradictory formula universally accepted profiles. consequence,
results later stated slightly differently.

485

fiEndriss, Grandi, & Porello

Anonymity (A): profile J J ()n permutation : N N ,
F (J1 , . . . , Jn ) = F (J(1) , . . . , J(n) ).
Neutrality (N): two formulas , agenda profile J J ()n ,
agents N Ji Ji , F (J ) F (J ).
Independence (I): formula agenda two profiles J , J
J ()n , Ji Ji agents N , F (J ) F (J ).
Systematicity (S): two formulas , agenda two profiles J ,
J J ()n , Ji Ji agents N , F (J ) F (J ).
Unanimity expresses idea individuals accept given judgment,
collective.4 Anonymity states aggregation symmetric respect
individuals, i.e., individuals treated same. Neutrality symmetry
requirement propositions: subgroup accepts two propositions, either
neither collectively accepted. Independence says proposition
accepted subgroup two otherwise distinct profiles, proposition
accepted either neither profile. Systematicity satisfied
neutrality independence are. axioms intuitively
appealing, stronger may seem first, several impossibility theorems,
establishing inconsistencies certain combinations axioms desiderata,
proved literature. original impossibility theorem List Pettit
(2002), instance, shows (under certain assumptions regarding agenda)
complete consistent aggregation procedure satisfying (A) (S).
important property monotonicity. introduce two different axioms
monotonicity. first one commonly used literature (Dietrich & List, 2007a;
List & Puppe, 2009). implicitly relies independence axiom. second, introduced
previous work (Endriss et al., 2010a), designed applied neutral procedures.
systematic procedures two formulations equivalent.
I-Monotonicity (MI ): formula agenda two profiles J , J
J ()n , Ji Ji agents N , N
6 Js Js , F (J ) F (J ).
N-Monotonicity (MN ): two formulas , agenda profile J
J ()n , Ji Ji agents N 6 Js Js
N , F (J ) F (J ).
is, (MI ) expresses collectively accepted (in J ) receives additional
support (in J , agent s), continue collectively accepted. Axiom
(MN ) says collectively accepted accepted strict superset
individuals accepting , also collectively accepted.
Axioms used define different classes aggregation procedures: Given
agenda list desirable properties AX provided form axioms, define
F [AX] set procedures F : J ()n 2 satisfy axioms AX.
4. notion unanimity stronger another common formulation requiring J = (J, . . . , J)
imply F (J ) = J (List & Puppe, 2009), two equivalent assumption (I).

486

fiComplexity Judgment Aggregation

2.3 Judgment Aggregation Procedures
Next, define three concrete types aggregation procedures.
2.3.1 Uniform Quota Rules Majority Rule
aggregation procedure F n = |N | individuals quota rule every formula
exists quota q {0, . . . , n+1} F (J ) |NJ | > q .
class quota rules studied depth Dietrich List (2007a). paper,
interested particular class quota rules:
Definition 5. Given {0, . . . , n+1} agenda , uniform quota rule
quota aggregation procedure Fm Fm (J ) |NJ | > m.
aggregation procedure satisfies (A), (I), (MI ), (N) uniform quota
rule; fact follows immediately result Dietrich List (2007a), use
slightly narrow definition quota rule. Provided 6= n + 1, uniform quota rule
Fm also satisfies (U).
quota rule special interest majority rule. majority rule uniform
quota rule = n+1
2 ; accepts formula whenever individuals accepting
rejecting (recall assume n odd). Clearly, majority
rule uniform quota rule satisfies (WR).
2.3.2 Premise-Based Procedure
seen introduction, majority rule may fail produce consistent
outcome. Two basic aggregation procedures set way avoid
problem discussed JA literature beginning, namely
premise-based conclusion-based procedure (Kornhauser & Sager, 1993; Dietrich
& Mongin, 2010). basic idea divide agenda premises conclusions.
premise-based procedure, apply majority rule premises
infer conclusions accept given collective judgments regarding premises;5
conclusion-based procedure directly ask agents judgments
conclusions leave premises unspecified collective judgment set. is,
conclusion-based procedure result complete outcomes (indeed, strictly speaking,
conform Definition 3), shall consider here. premise-based
procedure, hand, set way guarantees consistent complete
outcomes, provides usable procedure practical interest.
many JA problems, may natural divide agenda premises
conclusions. Let = p c agenda divided set premises p set
conclusions c , closed complementation.

5. commonly understood premise-based procedure. Dietrich Mongin (2010),
call rule premise-based majority voting, also investigated general class premise-based
procedures procedure used decide upon premises need majority rule.

487

fiEndriss, Grandi, & Porello

Definition 6. premise-based procedure PBP p c function mapping
profile J = (J1 , . . . , Jn ) J ()n following judgment set:
PBP(J ) = { c | |= },
= { p | |NJ | >

n+1
}
2

is, set premises accepted (strict) majority; PBP return
set together conclusions logically follow ( |= ).
want ensure PBP always returns judgment sets consistent
complete, impose certain restrictions:
want guarantee consistency, impose restrictions
premises. well-known majority rule guaranteed consistent
agenda satisfies so-called median property, i.e., every inconsistent subset inconsistent subset size 6 2 (Nehring & Puppe, 2007;
List & Puppe, 2009).6 result immediately transfers PBP: consistent
set premises satisfies median property.
want guarantee completeness, impose restrictions
conclusions: assignment truth values premises, truth value
conclusion fully determined.
shall see Section 5 deciding whether set formulas satisfies median
property highly intractable. is, general form, deciding whether
PBP consistent aggregation procedure given agenda complex problem.
meaningful analysis, therefore make two additional assumptions. First, assume
agenda closed propositional variables: p propositional variable p
occurring within formulas . Second, equate set premises
set literals. Clearly, above-mentioned conditions consistency completeness
satisfied assumptions.
So, summarise, instance PBP shall work paper defined
follows: assumption agenda closed propositional variables,
PBP accepts literal individuals accept accept ;
PBP accepts compound formula entailed accepted literals.
consistent complete input profiles, assuming n odd, leads resolute
JA procedure consistent complete. downside, PBP violates
standard axioms typically considered, (N) (I). even violates (U):
Agent 1
Agent 2
Agent 3
PBP

p
Yes




q

Yes



r


Yes


pqr
Yes
Yes
Yes


example, three agents unanimously accept p q r, aggregate using
PBP, end rejecting p q r, three premises rejected.
6. shall discuss result detail Section 5.

488

fiComplexity Judgment Aggregation

2.3.3 Distance-Based Procedure
basic idea distance-based approach aggregation select outcome that,
sense, minimises distance input profile. idea used extensively
preference aggregation (Kemeny, 1959) belief merging (Konieczny & Pino Perez,
2002). first example JA procedure based notion distance introduced
Pigozzi (2006), albeit restrictive assumption agenda closed
propositional variables compound formula either unanimously accepted
unanimously rejected agents. importantly, Pigozzis approach syntactic
information contained agenda discarded moving aggregation
level formulas level models. syntactic variant procedure later
defined Miller Osherson (2009), authors call Prototype-Hamming
rule. distance-based procedure shall define analyse here. irresolute
procedure, returning (nonempty) set collective judgment sets.
Definition 7. Given agenda , distance-based procedure DBP function
mapping profile J = (J1 , . . . , Jn ) J ()n following set judgment sets:
DBP(J ) = argmin

X

H(J, Ji )

JJ ()

collective judgment set DBP minimises amount disagreement
individual judgment sets (i.e., minimises sum Hamming distances
individual judgment sets). Note cases majority rule leads consistent
outcome, outcome DBP coincides majority rule (making
resolute procedure profiles). combine DBP tie-breaking rule
obtain resolute procedure.
DBP complete consistent design: judgment sets J () considered candidates searching solution. However, violates standard
axiomatic properties adapted case irresolute JA procedures (Lang,
Pigozzi, Slavkovik, & van der Torre, 2011). particular, DBP independent; indeed, based idea correlations propositions exploited
rather neglected.
2.4 Representation Results
prove number representation results characterise aggregation procedures satisfy certain combinations axioms. results section
known results, butdespite usefulthey rarely stated explicitly
literature.
Observe aggregation procedure F satisfies (I) exists family
sets winning coalitions W 2N , one formula , F (J )
NJ W . Imposing additional axioms, top (I), forces additional structure onto
family winning coalitions:
F satisfies (I) (U) grand coalition belongs every set winning
coalitions: N W .
489

fiEndriss, Grandi, & Porello

F satisfies (I) (N), i.e., satisfies (S), exists single set
winning coalitions W 2N F (J ) NJ W.
F satisfies (I) (A) collective acceptance formula depends
number individuals accepting it: C W |C| = |C | imply C W .
One consequence latter two insights that, F satisfies (A) (S),

|NJ | = |NJ | implies F (J ) F (J ). well-known fact; List Pettit
(2002), instance, use proof impossibility theorem (for special case
J = J ). Note (somewhat surprising) consequence fact that, case n
even, exists aggregation procedure satisfies (A), (S), well (WR).
see this, suffices consider (single) profile J exactly n2 agents accept n2
J |, i.e., either must F (J ), contraagents accept . |NJ | = |N
dicting complement-freeness, neither must F (J ), time contradicting
completeness. emphasise basic impossibility result involve notion
logical consistency.
hand, n odd (which shall continue assume),
axioms characterise relevant class aggregation procedures:
Proposition 1. F F [WR, A, S] exists function h : {0, . . . , n}
{0, 1} satisfying h(i) = 1 h(n i) N F (J ) h(|NJ |) = 1.


Proof. already seen F satisfies (S) (A), |NJ | = |NJ | implies
F () F (J ). latter equivalent existence function h :
{0, . . . , n} {0, 1} F (J ) h(|NJ |) = 1. additional requirement h(i) =
1 h(n i) consequence (WR). direction immediate: acceptance
formula F depends number agents accepting it, F must
anonymous, neutral independent; condition h(i) = 1 h(n i) furthermore ensures
completeness complement-freeness.
Dropping either neutrality independence, obtain following representation results:
Proposition 2. F F [WR, A, I] exists function h : {0, . . . , n}
{0, 1} every formula satisfying h (i) = 1 h (n i) N
F (J ) h (|NJ |) = 1.
Proof. clear characterisation procedures satisfying (I) (A) terms
winning coalitions given above, procedure always decide whether
collectively accepted looking cardinality coalition accepting
. rest proof proceeds Proposition 1.
Proposition 3. F F [WR, A, N] exists function hJ : {0, . . . , n}
{0, 1} every profile J J ()n satisfying hJ (i) = 1 hJ (n i) N
F (J ) hJ (|NJ |) = 1.
Proof. drop (I), winning coalitions anymore associated formulas,
depend profile J in. (N) merely ensures winning coalitions
also depend formula question. (WR) forces symmetry requirement
hJ (i) = 1 hJ (n i). opposite direction immediate.
490

fiComplexity Judgment Aggregation

three representation results above, add (U) list axioms,
corresponds requiring h(n) = 1 characteristic functions h.
Finally, recall seen Section 2.3.1, F F [A, S, MI ]
F uniform quota rule F F [WR, A, S, MI ] F majority
rule. is, representation results stated concern natural weakenings
combination axioms characterising majority rule. particular, chose never
drop anonymity axiom, find appealing uncontroversial
JA. also consider unanimity weak rationality fundamental (although make
exceptions class quota rules). independence neutrality axioms,
hand, much debatable, considered various options
either including including (although always keep least one them,
maintain minimal amount structure). is, classes aggregation procedures
covered representation results natural focus on.

3. Winner Determination
section define problem winner determination given JA procedure
decision problem, study computational complexity problem
procedures presented Section 2.3.
3.1 Problem Definition
problem winner determination voting theory computing election
winner given profile preferences supplied voters. corresponding decision
problem asks, given preference profile candidate, whether given candidate
winner election. JA, want compute F (J ) given profile J .
resolute aggregation procedure F , formulate corresponding decision problem
asking, given formula, whether belongs F (J ):
WinDet(F )
Instance: Agenda , profile J J ()n , formula .
Question: element F (J )?
solving WinDet formula agenda, compute collective
judgment set input profile. Note asking instead whether given judgment set
J equal F (J ) lead appropriate formulation winner determination
problem, actually compute winner would solve decision
problem exponential number times (once possible J ).
case irresolute JA procedures F adapt winner determination
problem following way:
WinDet (F )
Instance: Agenda , profile J J ()n , subset L .
Question: J L J J F (J )?
see appropriate formulation decision problem corresponding
task computing winning set, note compute winner using polynomial
491

fiEndriss, Grandi, & Porello

number queries WinDet follows. First, ask whether exists winning set
including arbitrarily chosen first formula agenda 1 , i.e., L = {1 }. case
answer positive, consider second formula 2 query WinDet L = {1 , 2 }.
Use subset L = {1 , 2 } case negative answer. Continue process
formulas agenda covered.7
3.2 Winner Determination Quota Rules Premise-Based Procedure
immediately clear winner determination polynomial problem quota
rule, including majority rule.
Fact 4. WinDet(Fm ) P uniform quota rule Fm .
Winner determination also tractable premise-based procedure:
Proposition 5. WinDet(PBP) P.
Proof. Counting number agents accepting premises checking
premise whether positive negative instance majority easy. determines collective judgment set far premises concerned. Deciding whether
given conclusion accepted collective amounts model checking
problem (is conclusion true model induced accepted premises/literals?),
also done polynomial time.
3.3 Winner Determination Distance-Based Procedure
want analyse complexity winner determination problem
distance-based procedure. DBP irresolute, study decision problem
WinDet . shall see, WinDet (DBP) p2 -complete, thus showing rule
hard compute. class p2 (also known p2 (O(log n)), PNP[log] PNP
|| )
class problems solved polynomial time asking logarithmic number
queries NP oracle or, equivalently, solved polynomial time asking
polynomial number queries parallel (Wagner, 1987; Hemachandra, 1989).
obtain result, first devise NP oracle used proof
p2 -membership. shall use following problem:
WinDetK (DBP)
Instance: Agenda , profile J J ()n , subset L , P
K N.
Question: J J () L J H(J , Ji ) 6 K?
is, ask whether exists judgment set J Hamming distance
profile K accepts formulas L. words, rather aiming
computing winning judgment set, problem merely allows us compute judgment set
7. line recent work Hemaspaandra, Hemaspaandra, Menton (2012), therefore argue
formulation winner determination problem correct decision problem associated
search problem actually computing winning judgment set.

492

fiComplexity Judgment Aggregation

certain minimal quality (where quality measured terms Hamming distance).
show problem lies NP.8
Lemma 6. WinDetK (DBP) NP.
Proof. show WinDetK (DBP) modelled integer program (without
objective function). proves membership NP (Papadimitriou, 1981). Suppose
want answer instance WinDetK (DBP). number subformulas propositions
occurring agenda linear size (not cardinality) . introduce binary
decision variable subformulas: xi {0, 1} ith subformula.
first write constraints ensure chosen outcome correspond
consistent judgment set (i.e., J J ()). Note rewrite formula
terms negation, conjunction, bi-implication without resulting superpolynomial
(or even superlinear) increase size. need show encode constraints
connectives. following table indicates write constraints:
2 = 1
x2 = 1 x1
3 = 1 2 x3 6 x1 x3 6 x2 x1 + x2 6 x3 + 1
3 = 1 2 x1 + x2 6 x3 + 1 x1 + x3 6 x2 + 1
x2 + x3 6 x1 + 1 1 6 x1 + x2 + x3
continue, consider following way rewriting sum distances featuring
definition WinDetK (DBP):
X



H(J , Ji ) =



=
=

n X
X

|J () Ji ()|

i=1 +
n
XX

1

2

|J () Ji ()|

i=1

n
X
1 X
Ji ()|

|n J ()
2


i=1

need bound sum above. suppose variables xi indices
{1, . . . , m} = || correspond propositions elements
. Let ai = |NJi | number individuals accept ith proposition
(under J ). compute winner DBP, need find consistent judgment
set J (characterised variables x1 , . . . , xm ) minimises sum |n x1 a1 | + +
|n xm |. introducing additional set integer variables yi > 0
= 1, . . . , m. ensure yi = |n xi ai | adding following constraints:9
(i 6 m)
(i 6 m)

n x 6 yi
n x 6 yi

8. proof establishes membership NP, also suggests implement solver
difficult problem. pointed one anonymous reviewer, also possible prove NP-membership
directly, using certificate consists J satisfying assignment J .
9. precise, constraints ensure |n xi ai | 6 yi . However, next constraint force
yi minimal.

493

fiEndriss, Grandi, & Porello

P
sum 21
i=1 yi corresponds Hamming distance winning set
profile. ensure exceed K, add following constraint:

1 X
yi 6 K

2
i=1

Finally, need ensure formulas set L get accepted.
adding one last set constraints:
(for L)

xi = 1

Now, construction, integer program presented feasible
instance WinDetK (DBP) started answered positive.
completes proof.
obtain upper bound winner determination problem DBP,
use standard construction. first involves identifying best value K,
deciding WinDetK (DBP) value K. latter done logarithmic
number queries problem complexity analysed Lemma 6.
Together, yields desired upper bound:
Lemma 7. WinDet (DBP) p2 .
Proof. problem WinDet (DBP) asks whether exists winning judgment set
accepts formulas given subset L . Since Hamming distance
judgment set profile bounded polynomial figure, solve
problem performing binary search K using logarithmic number queries
WinDetK (DBP).
P
precisely, since H(J , Ji )) 6 K = ||
2 |N |, figure polynomial
size problem description, ask first query WinDetK (DBP)

K = K2 empty subset designated formulas. case positive answer,

continue search new K = K4 , otherwise move higher half
interval querying WinDetK (DBP) K = 43 K . process ends logarithmic
number steps, providing exact Hamming distance K w winning candidate
profile J consideration. sufficient run problem WinDetK (DBP)
K = K w subset L original instance WinDet (DBP) wanted
solve. case answer positive, since cannot winning judgment set
Hamming distance strictly less K w , one winning judgment sets contains
formulas L. hand, case negative answer judgment sets containing
L Hamming distance bigger K w , thus cannot belong winning set.
Next, show upper bound established Lemma 7 tight. exploit
similarity DBP Kemeny rule preference aggregation build known
p2 -hardness result Hemaspaandra et al. (2005).
Lemma 8. WinDet (DBP) p2 -hard.
494

fiComplexity Judgment Aggregation

Proof. build reduction problem Kemeny Winner, defined work
Hemaspaandra et al. (2005).10 instance problem consists set candidates C,
profile linear preference orders P = (P1 , . . . , Pn ) C, designated candidate c
C. Define Kemeny score c following expression:
P
KemenyScore(c, P ) = min{ ni=1 dist(Pi , Q) | Q linear order top(Q) = c}
Here, dist(Pi , Q) Hamming distance two linear orders (defined number
ordered pairs candidates disagree) top(Q) preferred
candidate preference order Q. Kemeny Winner asks whether Kemeny score
c less equal Kemeny score candidates C.
build instance WinDet (DBP) decide problem. Define agenda
C following way. First, add propositional variables pab ordered pairs
distinct candidates a, b C; variables encode linear order C binary
relation (where pab stands b). describe properties linear order
means formulas form pab pbc pac pab pba . include
formulas, a, b, c C, C . fact, include m2 + 1 syntactic variants (where
= |C|) them.11 figure m2 + 1 chosen higher maximal
Hamming distance two linear orders (which m2 ).
Given preference profile P , build judgment profile J P encoding
order Pi C judgment set JiP C . example, agent 1s preference order
b c, J1P include set {pab , pba , pbc , pcb , pac , pca }. addition,
JiP include syntactic copies formulas encoding linear orders.
Observe dist(Pi , Pj ) = H(JiP , JjP ) construction. therefore sufficient
ask query WinDet (DBP) using C agenda, J P profile, L =
{pcd | C, c 6= d} set propositions accept sure, obtain answer
initial Kemeny Winner instance designated candidate c. winning ranking
features c top candidates (i.e., formulas pcd accepted candidates d),
Kemeny score lower equal candidates, providing
positive answer original problem. key insight notice judgment
sets encoding relations linear orders considered minimisation
process, since every disagreement one formulas encoding linear orders cause
much greater loss Hamming distance gained modifying
variables encoding individual candidate rankings.
Putting Lemma 7 8 together yields complete characterisation complexity
winner determination distance-based aggregation:
Theorem 9. WinDet (DBP) p2 -complete.
Theorem 9 shows DBP highly intractable. However, adapting efficient heuristics developed Kemeny rule (which, seen proof Lemma 8, closely related
DBP) may possible obtain implementation DBP achieves
acceptable performance practice (Conitzer, Davenport, & Kalagnanam, 2006).
10. Hemaspaandra et al. (2005) work preferences weak orders, point results
remain valid linear orders used instead. simplify presentation, work linear orders.
11. instance, formula might use syntactic variants , , , forth.

495

fiEndriss, Grandi, & Porello

4. Strategic Manipulation
context voting, agent said able manipulate voting rule
exists situation voting manner truthfully reflect preferences
result outcome prefers outcome would realised
vote truthfully (Gaertner, 2006). would constitute appropriate definition
manipulation context JA immediately clear, JA
notion preference. However, fixing suitable notion closeness judgment sets,
possible build preference ordering starting individuals initial judgment
set. approach followed Dietrich List (2007c) JA Everaere
et al. (2007) related setting belief merging. builds assumption
agents individual judgment set also preferred outcome amongst two
outcomes prefer one closer preferred outcome.
measure closeness using Hamming distance call aggregation procedure
F manipulable permits situation agent change outcome get closer
truthful judgment reporting untruthfully.
main interest computational complexity deciding whether given
agent successfully manipulate given profile. context, result showing
manipulation computationally intractable would count positive result. Specifically, study problem premise-based procedure.
family quota rules, (as shall see) impossible manipulate quota
rule aforementioned sense. also study manipulation problem
distance-based procedure, (as seen) even much basic winner
determination problem already intractable procedure.
4.1 Problem Definition
first need define preference ordering judgment sets agent N .
principle, number ways this, one reasonable approach
assume agent judgment set Ji also preferred outcome
preferences outcomes depend close Ji (Dietrich & List, 2007c).
shall measure closeness using Hamming distance, distances could also
used end (Duddy & Piggins, 2012). say agent prefers J J
H(Ji , J) < H(Ji , J ).
employ standard game-theoretical notation denote (J , Ji ) profile
like J , except judgment set agent replaced Ji .
Definition 8. F manipulable profile J J ()n agent N , exists
alternative judgment set Ji J () H(Ji , F (J , Ji )) < H(Ji , F (J )).
is, reporting Ji rather truthful judgment set Ji , agent achieve
outcome F (J , Ji ) outcome closer (in terms Hamming distance)
truthful (and preferred) set Ji outcome F (J ) would get realised
truthfully report Ji . procedure manipulable profile
agent called strategy-proof.
Dietrich List (2007c) shown JA procedure strategy-proof
satisfies (I) (MI ). Indeed, follows immediately definitions: independence
496

fiComplexity Judgment Aggregation

means would-be manipulator consider one proposition time; monotonicity
means always best interest drive support formulas
judgment set reduce support judgment set, i.e.,
best interest report judgment set truthfully.12
aggregation procedures strategy-proofness cannot guaranteed, want
study algorithmic problem computing manipulating judgment set. end,
formulate manipulation decision problem aggregation procedure F :
Manip(F )
Instance:
Question:

Agenda , profile J J ()n , agent N .
Ji J () H(Ji , F (J , Ji )) < H(Ji , F (J ))?

Note asking whether agent manipulate successfully, rather how.
is, problem immediately correspond practical (and potentially
harder) problem computing actual strategy manipulator. However, since
interest obtaining intractability results (to provide protection manipulation), safely concentrate formulation, provides lower bound
corresponding search problem.
seen, uniform quota rules (including majority rule) independent monotonic, means also strategy-proof (so algorithmic
problem deciding Manip arise procedures). course, comes
price always producing outcomes consistent.
4.2 Strategic Manipulation Premise-Based Procedure
prove manipulating premise-based procedure intractable, thus showing existence kind jump computational complexity winner
determination manipulation desirable context.
Theorem 10. Manip(PBP) NP-complete.
Proof. first establish NP-membership. untruthful judgment set Ji yielding preferred outcome serve certificate. Checking validity certificate means
checking (a) Ji actually complete consistent judgment set (b)
outcome produced Ji better outcome produced truthful set Ji .
(a), checking completeness easy. Consistency also decided polynomial time:
every propositional variable p agenda, Ji must include either p p; admits
single possible model; remains done checking compound
formulas Ji satisfied model.13 (b), need compute outcomes
Ji Ji (by Proposition 5, polynomial), compute Hamming distances
Ji , compare two distances.
Next, prove NP-hardness reducing Sat Manip(PBP). Suppose given
propositional formula want check whether satisfiable. build
12. Note contradict Gibbard-Satterthwaite Theorem voting theory (Gaertner,
2006). theorem involves universal-domain assumption, manner using
Hamming distance induce preferences judgment sets amounts domain restriction.
13. is, point crucially rely assumption PBP defined agendas
closed propositional variables.

497

fiEndriss, Grandi, & Porello

judgment profile three agents third agent manipulate aggregation
satisfiable. Let p1 , . . . , pm propositional variables occurring ,
let q1 , q2 two additional propositional variables. Define agenda contains
atoms p1 , . . . , pm , q1 , q2 negation, well + 2 syntactic variants
formula q1 ( q2 ), well complements formulas. instance,
= q1 ( q2 ), might use syntactic variants , , , forth.
consider profile J (with rightmost column weight + 2):

J1
J2
J3
F (J )

p1
1
0
1
1

p2
1
0
1
1







pm
1
0
1
1

q1
0
0
1
0

q2
0
1
0
0

q1 ( q2 )
?
?
1
0

judgments agents 1 2 regarding q1 ( q2 ) irrelevant argument,
indicated ? table (but note determined polynomial
time; particular, J1 (q1 ( q2 )) = 0 ).
agent 3 reports judgment set truthfully (as shown table), Hamming
distance J3 collective judgment set 1 + (m + 2) = + 3. Note
agent 3 decisive propositional variables (i.e., premises) except q1 (which
certainly get rejected). Now:
satisfiable, agent 3 report judgments regarding p1 , . . . , pm correspond satisfying assignment . furthermore accepts q2 , + 2
copies q1 ( q2 ) get accepted collective judgment set. Thus,
Hamming distance J3 new outcome + 2, i.e., agent 3
manipulated successfully.
satisfiable, way get m+2 copies q1 (q2 )
accepted (and q1 get rejected case). Thus, agent 3 means
improving Hamming distance + 3 guarantee
reporting truthfully.
Hence, satisfiable agent 3 manipulate successfully, reduction
Sat Manip(PBP) complete.
Thus, manipulating PBP significantly harder using it, least terms worstcase complexity (and assumption P 6= NP).

5. Safety Agenda
section, introduce concept safety agenda: agenda safe
given aggregation procedure F , collective judgment set returned F
consistent (consistent) input profile. course, question relevant
aggregation procedures always consistent begin with,
consider PBP DBP section. fact, main interest
safety agenda entire classes aggregation procedures, characterised set
498

fiComplexity Judgment Aggregation

axioms AX: safe class F [AX] aggregation procedures safe every
procedure F F [AX].
defining problem relating so-called agenda characterisation results (or
possibility theorems, shall call them) studied JA literature, characterise safe
agendas number natural combinations axioms establish computational
complexity checking safety agenda cases.
5.1 Problem Definition
performing aggregation judgments, would like avoid paradoxical outcomes,
i.e., would like ensure collective judgment set consistent. Whether
indeed case depends several factors: aggregation procedure,
agenda, individual judgment sets. cannot control choices individuals
make. might even know aggregation procedure exactly going
use; might know properties, i.e., might know
belongs certain class procedures. nevertheless guarantee collective
judgment set consistent? formalise question follows:
Definition 9. agenda safe respect class aggregation procedures F,
every procedure F consistent applied profiles judgment sets .
example paradox presented introduction demonstrates unsafety
agenda {p, p, q, q, p q, (p q)} respect majority rule. agenda {p, p},
hand, immediately seen safe respect full class weakly
rational aggregation procedures.
question whether agenda safe closely related rich literature socalled agenda characterisation results (see, e.g., Nehring & Puppe, 2007; Dokow & Holzman,
2010; Dietrich & List, 2007b; List & Puppe, 2009). authors asked following
kind question: given agenda given list axiomatic requirements (always
including requirement consistency), possible find aggregation procedure
meets requirements agenda? may rephrase question follows:
given agenda list axioms AX (now excluding consistency), possible
find procedure F [AX] consistent? distinguish results kind
safety theorems (which also agenda characterisations kind), shall refer
possibility theorems. summarise: possibility theorem shows
consistent procedure F [AX], safety theorem shows procedures
F [AX] consistent.
Note case class aggregation procedures consists single aggregation
procedure (e.g., F [WR, A, S, MI ] consists majority rule), possibility safety
results coincide.
Possibility theorems important point view mechanism designer:
given certain axioms would like see satisfied, still possible design
aggregation procedure meeting know certain characteristics kind
agenda procedure used? is, question likely
ask off-line situation once. Safety theorems, hand,
likely play role on-line situation arguably particular interest
499

fiEndriss, Grandi, & Porello

applications. reason actual users likely want assurance
aggregation consistent (provided certain axioms satisfied agenda
certain properties) rather learn exists consistent form aggregation
(satisfying certain axioms). instance, suppose want give certain guarantees
quality operations multiagent system, without full knowledge precise
specification every individual agent without full knowledge interaction
protocols going employ. might nevertheless sufficient information
safety theorem apply, case check, given agenda, whether consistency
guaranteed. is, deciding whether safety holds question might
answer again, many different agendas. computational
complexity problem relevant question.
5.2 Agenda Properties
shall see, agenda satisfies certain structural properties, might
sufficient condition ensure safety respect certain aggregation rules. turns
types agenda properties help similar feature
known possibility theorems. Specifically, shall make use so-called median property,
introduced Nehring Puppe (2007).14
Definition 10. say agenda satisfies median property (MP), every
inconsistent subset inconsistent subset size 2.
words, satisfies MP minimally inconsistent subset (mi-subset)
2 elements. Note case known include tautologies
(and thus contradictions), definition simplifies requiring mi-subset must
exactly size 2. generalise median property follows:
Definition 11. Let k > 2. agenda satisfies k-median property (kMP), every
inconsistent subset inconsistent subset size k.
is, MP 2MP property. Agendas satisfying MP already
quite simple, restriction made tighter requiring inconsistent subsets
particular form. sequel, call inconsistent set nontrivially inconsistent
contain single formula contradiction.
Definition 12. agenda satisfies simplified median property (SMP), every
nontrivially inconsistent subset subset form {, } logically
equivalent .
simplification yields:
Definition 13. agenda satisfies syntactic simplified median property
(SSMP), every nontrivially inconsistent subset subset form {, }.
14. name median property derives work Nehring Puppe (2007), analyse social
choice functions class vector spaces called median spaces.

500

fiComplexity Judgment Aggregation

Agendas satisfying SSMP composed uncorrelated formulas, i.e., essentially equivalent agendas composed atoms alone. SMP less restrictive, allowing
logically equivalent syntactically different formulas.
Observe every agenda satisfies SMP also satisfies MP. converse
true: = {p, p, p q, (p q)} satisfies MP, SMP. Similarly, every
agenda satisfies SSMP also satisfies SMP. Again, converse true:
= {p, p, p p, (p p)} satisfies SMP, SSMP.
5.3 Safety Theorems: Linking Agenda Properties Axioms
prove several characterisation results safe aggregation judgments, concentrating classes procedures defined weakening axiomatisation majority
rule. begin safety theorem majority rule itself. fact, result
familiar literature (Nehring & Puppe, 2007), although presented
different form. Despite fact known result, still provide proof,
arguably simpler translating result Nehring Puppe setting.
Theorem 11. agenda safe majority rule satisfies MP.
Proof. Let F majority rule.
() First, suppose satisfies MP. need show F (J ) consistent
J J ()n . sake contradiction, suppose not, let mi-subset
F (J ). F (J ) , 2 elements. Clearly, cannot case
F (J ) includes contradiction , would mean majority agents
would accepted . Hence, must set exactly two formulas, say, .
means must accepted n+1
2 agents must
accepted n+1


agents.
Hence,


pigeon hole principle, least one agent
2
must accepted them, thereby contradicting individual rationality.
() direction, suppose satisfy MP, i.e., mi-subset
size k > 3. need show exists profile J F (J ) inconsistent.
Let two distinct formulas . consider profile J following
properties (recall assume n > 3): (1) first n1
2 agents accept formulas
except ; (2) last n1
agents
accept

formulas

except ; (3)
2
n+1
middlemost agent 2 accepts formula . is, individual
agent accepts formulas , i.e., really build individually rational
profile properties (note consistent subset always extended
complete consistent judgment set ). However, profile
formulas majority get F (J ), i.e., F (J ) inconsistent.
reason case able rely known result aforementioned fact
classes aggregation procedures consisting single procedure, safety
possibility results coincide. Unfortunately, larger classes procedures, approach
exploiting known possibility results cannot used.
first establish two sufficient conditions safety agenda, two different
(fairly large) classes aggregation procedures:
Lemma 12. agenda satisfies SSMP, safe F [WR, U].
501

fiEndriss, Grandi, & Porello

Proof. Consider aggregation procedure satisfies (WR) (U). Let agenda
satisfies SSMP. Hence, way obtain inconsistent outcome would
either accept inconsistent formula accept formula syntactic complement
. latter possibility excluded (WR). So, sake excluding also
former possibility, suppose inconsistent formula collectively accepted.
individual rationality, get accepted agents. Hence, (U),
collectively accepted, thus collectively rejected (WR).
Lemma 13. agenda satisfies SMP, safe F [WR, U, N].
Proof. Let F aggregation procedure satisfies (WR), (U) (N), let
agenda satisfies SMP. sake contradiction, suppose exists profile
J J ()n F (J ) inconsistent. distinguish two cases:
(1) exists set {, } F (J ) logically equivalent . given
individual judgment sets assumed complete consistent,
logically equivalent means every agent accepts also accept ,
J . Together (N) entails F (J )
vice versa, i.e., NJ = N
F (J ). already know F (J ); thus, also get F (J ). also
F (J ), obtained contradiction (WR).
(2) exists inconsistent formula F (J ). argument used
proof Lemma 12, contradicts assumption F satisfying (U) (WR).
is, obtain contradiction possible cases.
Next, prove two results concerning necessary conditions safety agenda
(now aim relatively narrowly defined classes aggregation procedures):
Lemma 14. agenda safe F [WR, A, U, S], satisfies SMP.
Proof. Let agenda violates SMP. need provide example
aggregation procedure F satisfies (WR), (A), (U) (S) produce
inconsistent outcome least one input profile. distinguish two cases:
(1) Suppose violates SMP virtue mi-subset size greater 2.
case also violates MP. Theorem 11 shows safe
majority rule. majority rule satisfies (WR), (A), (U) (S), done.
(2) possibility mi-subset consisting two formulas
logical complements, i.e., exists set form {, }
|= 6|= .15 Consider following weakly rational, anonymous,
unanimous systematic aggregation procedure Fh 3 individuals, defined using
notation Proposition 1: h(0) = h(2) = 0 h(1) = h(3) = 1. is, Fh
accepts proposition accepted odd number individuals.16 Consider
following profile, restricted complements: J1 = {, },
15. example, might p q might p.
16. parity rule also used Dokow Holzman (2010) provide witness one
possibility results.

502

fiComplexity Judgment Aggregation

J2 = {, }, J3 = {, }. Note sets consistent. However,
profile (opportunely extended profile whole agenda) generate
inconsistent outcome, since accepted exactly one individual.
Hence, cases fails safe least one procedure F [WR, A, U, S].
Lemma 15. agenda safe F [WR, A, U, I], satisfies SSMP.
Proof. Let agenda violates SSMP. also violates SMP,
Lemma 14 applies done.
Otherwise, must two formulas |= 6= ,
i.e., logical syntactic complements. Let F procedure accepts
(and rejects ) least one agent accepts , accepts (and rejects )
least one agent accepts , behaves like majority rule respect
propositions. F satisfies (WR), (A), (U) (I), safe F , case one
agent accepts another , collective judgment set include .
ready state prove safety theorems:
Theorem 16. agenda safe F [WR, A, U, S] satisfies SMP.
Proof. One direction given Lemma 14. follows Lemma 13 together
observation F [WR, U, N] F [WR, A, U, S].
characterisation safe agendas remains intact widen class aggregation
procedures consideration systematic neutral procedures:
Theorem 17. agenda safe F [WR, A, U, N] satisfies SMP.
Proof. One direction follows Lemma 14 together fact F [WR, A, U, S]
F [WR, A, U, N]; Lemma 13 F [WR, U, N] F [WR, A, U, N].
Indeed, Theorems 16 17 state safety results particularly natural classes
aggregation procedures, argument easily see class F
F [WR, A, U, S] F F [WR, U, N] case safe F
satisfies SMP.
drop neutrality F [WR, A, U, S] rather independence, obtain
even restrictive characterisation safe agendas:
Theorem 18. agenda safe F [WR, A, U, I] satisfies SSMP.
Proof. One direction given Lemma 15; follows Lemma 12 together
F [WR, U] F [WR, A, U, I].
Again, generalise result say that, class F F [WR, A, U, I]
F F [WR, U], case safe F satisfies SSMP.
Finally, uniform quota rules characterisation result kind seek available
literature (albeit different name), least rules certain bounds
imposed quota (Dietrich & List, 2007a). state interesting result follows
(recall n number individuals):
503

fiEndriss, Grandi, & Porello

Theorem 19. Let k > 2. agenda safe class uniform quota rules Fm
satisfying constraint > n nk satisfies kMP.
Theorem 19 reformulation Corollary 2(a) work Dietrich List (2007a)
shall prove here.
Let us conclude presentation safety theorems remark role
axiom (U) results above. Recall made assumption
agenda including contradictory formulas (or complements, i.e., tautologies).
make assumption (which common JA literature certainly
unreasonable), remove mentionings (U) safety results above.
Indeed, ever used (U) proofs avoid situations contradiction gets
unanimously rejected yet collectively accepted. wish make assumption
regarding absence contradictory formulas agenda, still remove
mentionings (U) safety results above, provided replace mentionings
SMP property satisfying SMP including contradictory
formulas (and accordingly results involving SSMP).
5.4 Membership Results Agenda Properties
identified conditions guarantee safety given
agenda, want know difficult decide whether conditions satisfied.
shall see, problem p2 -complete classes aggregation procedures
considered. p2 (also known coNPNP coNP NP oracle) complexity class located second level polynomial hierarchy (Meyer & Stockmeyer, 1972;
Stockmeyer, 1976; Arora & Barak, 2009). class decision problems
certificate negative answer verified polynomial time machine
access oracle answering queries Sat (or NP-complete problem).
prove problem p2 -complete, prove membership p2 p2 -hardness.
begin proving membership p2 . so, need provide algorithm
that, provided certificate intended establish negative answer, verify
correctness certificate polynomial time, assume algorithm
access Sat oracle. sequel, shall write MP median property
problem deciding whether given agenda satisfies median property,
similarly SMP, SSMP kMP.
Lemma 20. MP, SMP, SSMP, kMP p2 .
Proof. shall present proof kMP, intuitively difficult four
problems. proofs three problems similar.
need give algorithm decides correctness certificate violation
kMP polynomial time, assuming access Sat oracle. given agenda
(with = ||), certificate set (a) needs inconsistent
(b) must inconsistent subsets size 6 k. Inconsistency checked
P

single query Sat oracle. = ||, ki=1 mi nonempty
subsets size 6 k, polynomial (and thus also m).17 Hence, second
condition checked polynomial number queries oracle.
17. figure polynomial k, affect argument, k constant.

504

fiComplexity Judgment Aggregation

5.5 Hardness Results Agenda Properties
Next, want show MP, SMP, SSMP kMP p2 -hard. done
giving polynomial-time reduction problem already known p2 -hard
problem investigation. purpose, make use quantified boolean
formulas (QBFs). QSat, satisfiability problem18 general QBFs, PSPACEcomplete, imposing suitable syntactic restrictions generate complete problems
level polynomial hierarchy. Consider QBF following form:
x1 xr y1 ys .(x1 , . . . , xr , y1 , . . . , ys )
arbitrary propositional formula {x1 , . . . , xr } {y1 , . . . , ys } set
propositional variables occurring (that is, could QBF
existential quantifiers occur inside scope universal quantifiers). problem
checking whether formula form satisfiable (i.e., true), shall denote
Sat, known p2 -complete (Arora & Barak, 2009). Below, shall abbreviate
formulas type writing xy.(x, y).
basic intuition MP related problems p2 -hard share
basic structure Sat, asking question form subsets
inconsistent, exist subset certain property? Indeed, embedding,
say, MP Sat relatively straightforward. However, require opposite:
need show even though Sat may appear general MP
agenda problems, actually reduced problems.
first prove technical lemma. Let Sat2 problem checking whether
QBF following form true, given already know (i) tautology,
(ii) contradiction, (iii) logically equivalent literal:
xy.(x, y) xy.(x, y)
Lemma 21. Sat2 p2 -hard.
Proof. reduction Sat: Given QBF form xy.(x, y), show
checking satisfiability equivalent running Sat2 ( a) b
universally b existentially quantified, two new propositional variables b
occurring , i.e., checking satisfiability formula
xayb.[((x, y) a) b] xayb.[((x, y) a) b].
First, note ( a) b cannot tautology, contradiction, equivalent literal;
side constraints specified definition Sat2 satisfied. Also note
first conjunct true exactly original formula xy.(x, y) true.
b always set true, original formula true whenever
set false (a falls scope universal quantifier). Therefore, positive answer
Sat2 instance entails positive answer original Sat instance.
direction immediate, second conjuncts always satisfiable
(by making b false).
18. shall speak satisfiability problems QBFs, even though strictly speaking QBFs
distinction satisfiability, truth validity, every QBF closed formula.

505

fiEndriss, Grandi, & Porello

able prove p2 -hardness SSMP:
Lemma 22. SSMP p2 -hard.
Proof. shall give polynomial-time reduction Sat2 SSMP; claim
follows Lemma 21. Take instance Sat2 , i.e., question whether
xy.(x, y) xy.(x, y) true 6|= , 6|= , 6|=
literals . Suppose x = hx1 , . . . , xr i, define agenda follows:19
= {x1 , x1 , x2 , x2 , . . . , xr , xr , ( ), ( )}
prove violates SSMP answer Sat2 -question
NO. see this, consider following facts. First, suppose violates SSMP.
circumstances case? neither tautology contradiction,
inconsistent subset must nontrivially inconsistent. Furthermore, construction
(consisting largely literals), inconsistent subset including pair
syntactic complements must include either ( ) ( ), well (complementfree) subset {x1 , x1 , . . . , xr , xr }. is, way violating SSMP
find subset literals {x1 , x1 , . . . , xr , xr } make true forces either ( )
( ) false. precisely situation instance Sat2
requires negative answer.
direction, suppose answer Sat2 -question NO. means
able find assignment variables x makes either
unsatisfiable. W.l.o.g., suppose latter situation. Construct subset ,
containing ( ), includes literal xi set true assignment ,
xi otherwise. inconsistent subset , since neither tautology
contradiction, falsifies SSMP.
Proving hardness SMP works similarly:
Lemma 23. SMP p2 -hard.
Proof. construction used proof Lemma 22. additional
insight required observation special kind agenda constructed
proof, SMP SSMP coincide: excluding formulas equivalent
literals, ensure agenda constructed previous proof contain
pairs logically equivalent formulas.
MP give proof using reduction SSMP:
Lemma 24. MP p2 -hard.
Proof. show reduce problem deciding SSMP instance MP.
Let agenda want test SSMP let + = {1 , . . . , }
set non-negated formulas . build set + following way: copy
formulas + times, every time renaming variables occurring , obtaining
19. Using ( ) rather ensures agenda include doubly-negated formulas.

506

fiComplexity Judgment Aggregation

formulas ji 1 6 i, j 6 m. every substitute ii ii pi , pi new variable
occurring ji . Finally, add p1 , . . . , pm + . obtain following set:
+ = {p1 , 11 p1 , . . . , 1m ,
p2 , 21 , 22 p2 , . . . , 2m ,
..
.


p ,
1 , . . . , p }

Define = + { | + }. show satisfies SSMP
satisfies MP. One direction immediate. Suppose satisfy SSMP.
must mi-subset size k > 2.20 Let = {i1 , . . . , ik }. exists
subset , namely = {pi1 , ii11 pi1 , ii12 , . . . , ii1k }, mi-set size k + 1 > 3,
thereby falsifying MP.
opposite direction, suppose satisfy MP. is,
mi-subset size > 3. construction , know subset must
contain formulas superscript complements (all formulas
different variables). subset contain pi pi , find copy
, violates SSMP, case done. Clearly, cannot include
pi pi , would contradict || > 3. left cases
includes either pi pi i. Then, minimality, also ii pi negation
must included. reason cases: (1) pi ii pi ,
dropping disjunction still get inconsistent subset, assumption
minimality; (2) pi (ii pi ) cannot reason; (3) finally,
pi together negation ii pi already inconsistent. Therefore, conclude
must form {pi , ii pi }i , set (one more) formulas
superscript i. easy see set obtain remove
superscript {ii } mi-subset falsifies SSMP. particular,
ii 6 , ii 6 construction, i.e., mi-subset obtain
consist two formulas logical complements.
Finally, establish hardness kMP:
Lemma 25. kMP p2 -hard every k > 2.
Proof. k = 2, claim established Lemma 24. observe
use exactly construction proof Lemma 24 reduce instance
kMP k > 2 instance corresponding (k+1)MP. Hence, simple
inductive argument, kMP must p2 -hard finite k > 2.
5.6 Complexity Safety Agenda
shown deciding whether given agenda satisfies MP, SMP,
SSMP, kMP p2 p2 -hard. Furthermore, Section 5.3 linked
properties safety various combinations axioms. immediate
corollary results, obtain theorem concerning complexity deciding
safety agenda:
20. fact cannot contain two formulas logical complements relevant proof.

507

fiEndriss, Grandi, & Porello

Theorem 26. Deciding problem safety agenda p2 -complete
following classes aggregation procedures:
(i)
(ii)
(iii)
(iv)
(v)

F [WR, A, S, MI ], consisting majority rule;
F [WR, A, U, S], systematic procedures;
F [WR, A, U, N], neutral procedures;
F [WR, A, U, I], independent procedures;
class uniform quota rules Fm > n nk k > 2.

Proof. Concerning p2 -hardness, (i) direct consequence Theorem 11 Lemma 24.
way, (ii) derived Theorem 16 Lemma 23, (iii) Theorem 17
Lemma 23, (iv) Theorem 18 Lemma 22. Finally, (v) follows Theorem 19
together Lemma 25. Membership p2 follows Lemma 20 five cases.
is, case safety agenda guaranteed
structurally simple agendas, deciding whether given agenda meets structural
constraints highly intractable. negative result sense concerns
problem would like able solve efficiently. stress
render problem hopeless. Work QBF solvers seen lot progress recent years
(see, e.g., Narizzano, Pulina, & Tacchella, 2006), tools could deployed check
whether agenda satisfies given type median property.21 event, understanding
naturally arising question JA relates difficult well-studied algorithmic
problem Sat interesting worthwhile right.

6. Related Work: Computational Perspectives Judgment Aggregation
Starting work List Pettit (2002), research JA focussed either
philosophical implications fact aggregation may result inconsistent
outcome derivation impossibility characterisation results. extensive
literature field recently reviewed List Puppe (2009). work
also explored links JA preference aggregation (Dietrich & List, 2007b;
Grossi, 2009; Porello, 2010; Grandi & Endriss, 2011) several recent contributions
furthermore focussed definition analysis specific aggregation procedures (Dietrich & List, 2007a; Dietrich & Mongin, 2010; Miller & Osherson, 2009; Lang et al., 2011).
shall instead concentrate contributions JA either computational
slant otherwise relevant AI.
Besides previous work subject present paper (Endriss et al., 2010a,
2010b), small number contributions computational social choice
taking computational perspective JA (Nehama, 2010; Slavkovik & Jamroga, 2011;
Baumeister, Erdelyi, & Rothe, 2011; Baumeister, Erdelyi, Erdelyi, & Rothe, 2012):
first example work Nehama (2010), proposes framework approximate JA
goal finding aggregation procedure never return inconsistent
21. pointed one anonymous reviewer, Answer Set Programming may also useful framework
reason safety problems. DLV System, instance, provides flexible tool
deciding arbitrary problems located second level polynomial hierarchy (Leone, Pfeifer,
Faber, Eiter, Gottlob, Perri, & Scarcello, 2006).

508

fiComplexity Judgment Aggregation

judgment set replaced goal finding procedure returning
inconsistent set highly unlikely. (negative) result obtained framework
however extend range available procedures significant way. Second,
Slavkovik Jamroga (2011) extend standard JA framework weights (to model
differences influence individuals) provide upper bound complexity
winner determination problem family distance-based aggregation procedures.
Third, Baumeister et al. (2011) provide first study computational complexity
bribery problem JA, asking whether possible obtain desired outcome
k individual agents bribed change judgment set. Finally, Baumeister
et al. (2012) discuss complexity various forms controlling judgment aggregation
processes, e.g., influencing outcome adding removing judges.
clearest example work explores integration ideas JA ideas
coming field traditionally studied AI recent work connections
JA abstract argumentation frameworks (Rahwan & Tohme, 2010; Caminada & Pigozzi,
2011): problem commonly studied abstract argumentation decide
ones set arguments mutually attack accept, reject,
remain undecided. Rahwan Tohme (2010) study variant
problem group agents decide status award argument,
problem naturally lends viewed lens JA. related work,
Caminada Pigozzi (2011) proposed approach JA involves translation
abstract argumentation framework, makes tools techniques abstract
argumentation available aggregation judgments.
field research within AI closely related JA belief merging (see, e.g.,
Konieczny & Pino Perez, 2002; Maynard-Zhang & Lehmann, 2003; Chopra et al., 2006;
Everaere et al., 2007). work Konieczny Pino Perez (2002), particular,
inspired distance-based procedure JA used paper. JA belief
merging modelled Konieczny Pino Perez share interesting features, ultimately
study different problems. JA individuals assumed submit consistent judgment sets, belief merging constraint enforced outcome. reflects
view consistency belief merging (modelled terms integrity constraint)
feasibility requirement, JA amounts rationality assumption.

7. Conclusions Future Work
studied computational complexity three problems JA: computing
winning judgment set given aggregation procedure, deciding whether manipulation
would beneficial given agent given aggregation procedure given
profile, deciding safety agenda given class aggregation procedures.
also proven several safety theorems link safety simple structural properties
agenda provide interesting counterpart known possibility theorems.
results show that, winner determination problem easy quota rules
premise-based procedure, intractable otherwise attractive distance-based
procedure. Regarding strategic manipulation, seen manipulation NP-hard
premise-based procedure, positive result. also seen
quota rules question manipulation complexity arise, least
509

fiEndriss, Grandi, & Porello

model preferences used here. distance-based procedure, investigated
complexity manipulation problem, already winner determination
problem found intractable. work safety agenda,
derived characterisation results wide range procedures, defined terms commonly
used axioms. seen safety guaranteed relatively simple agendas
also seen deciding whether simplicity conditions met highly
intractable.
work computational aspects JA far limited small
number interesting scattered contributions, strongly believe JA
taken important research topic AI computational social choice. One
important direction pursue concerns practical algorithms problems studied
paper (as well related problems naturally arising JA). already mentioned
existing work algorithms winner determination problem Kemeny
rule preference aggregation (Conitzer et al., 2006) may provide starting point
working implementation distance-based procedure work QBF solvers
automated reasoning (Narizzano et al., 2006) work Answer Set Programming (Leone
et al., 2006) could prove helpful tackling challenges identified complexity
results regarding safety agenda.
Alongside development practical algorithms, improving understanding
algorithmic aspects JA studying framework parameterised complexity would
also great interest. context voting, approach lead number
insightful results (Betzler, 2010). Indeed, JA, initial steps direction already
taken Baumeister et al. (2011).
Studying winner determination problem, complexity-theoretic practical terms, distance-based procedures proposed Miller Osherson (2009)
Lang et al. (2011) also constitutes worthwhile direction future work.
Recall analysed manipulation one particular way defining preferences,
namely terms Hamming distance agents true set judgments. Thus,
would interesting investigate extent changing definition manipulation
(by altering notion induced preference) affects complexity result. Indeed,
notions induced preference (and thus manipulation) conceivable. instance,
would-be manipulator might interested status specific propositions (e.g.,
conclusions) might use different notion distance, e.g., one recently
proposed Duddy Piggins (2012).
justified decision study complexity manipulation
problem distance-based procedure fact already much basic
winner determination problem p2 -complete. important question believe
requires discussion research community whether indeed valid argument.
context voting, initial idea Bartholdi et al. (1989) that, say,
NP-hardness result manipulation problem particular voting rule might suggest
rule immune manipulation practice. Recent work strongly
suggests case (Faliszewski & Procaccia, 2010), kind
NP-hard problems encountered context algorithms perform well practice
relatively easy design (Walsh, 2011). question arises whether
still true hardness results respect higher complexity classes. instance,
510

fiComplexity Judgment Aggregation

conceivable possible design algorithms efficiently solve
typical instances winner determination problem distance-based procedure,
might turn much difficult design similarly successful algorithm
corresponding manipulation problem. is, question arises whether hardnessof-manipulation studies need restricted problems winner determination
polynomial, whether jump complexity desirable principle might
provide level protection practice.
Regarding safety agenda, given results natural combinations axioms correspond weakening majority rule, similar study
could also conducted combinations axioms. Indeed, would interesting
explore robust p2 -completeness results are. is, open question suggests
whether exists interesting relevant class aggregation procedures
safety problem falls different complexity class.
Generally speaking, believe much work exploring obvious potential
JA AI multiagent systems needed. lead practical advances
definition interesting new theoretical problems. steps direction
recently taken Slavkovik (2012), concerning modelling collective decision
making multiagent systems, Caminada Pigozzi (2011) Rahwan
Tohme (2010), concerning applications JA abstract argumentation.

Acknowledgments
paper builds earlier work complexity judgment aggregation presented
AAMAS-2010 (Endriss et al., 2010a) COMSOC-2010 (Endriss et al., 2010b).
would like thank reviewers members audiences meetings, three
reviewers Journal Artificial Intelligence Research, attendees workshop seminar talks given topic Amsterdam, Barcelona, Chongqing,
Luxembourg, Moscow, New Delhi, Padova, Paris, Pisa, Tilburg many helpful
suggestions received.

References
Arora, S., & Barak, B. (2009). Computational Complexity: Modern Approach. Cambridge
University Press.
Bartholdi, J. J., Tovey, C. A., & Trick, M. A. (1989). computational difficulty
manipulating election. Social Choice Welfare, 6 (3), 227241.
Baumeister, D., Erdelyi, G., Erdelyi, O. J., & Rothe, J. (2012). Control judgment aggregation. Proceedings 6th Starting AI Researchers Symposium (STAIRS-2012).
IOS Press.
Baumeister, D., Erdelyi, G., & Rothe, J. (2011). hard bribe judges?
study complexity bribery judgment aggregation. Proceedings
2nd International Conference Algorithmic Decision Theory (ADT-2011). SpringerVerlag.
511

fiEndriss, Grandi, & Porello

Betzler, N. (2010). Multivariate Complexity Analysis Voting Problems. Ph.D. thesis,
University Jena.
Brandt, F., Conitzer, V., & Endriss, U. (2012). Computational social choice. Weiss, G.
(Ed.), Multiagent Systems. MIT Press. press.
Caminada, M., & Pigozzi, G. (2011). judgment aggregation abstract argumentation.
Autonomous Agents Multi-Agent Systems, 22 (1), 64102.
Chevaleyre, Y., Endriss, U., Lang, J., & Maudet, N. (2007). short introduction computational social choice. Proceedings 33rd Conference Current Trends
Theory Practice Computer Science (SOFSEM-2007). Springer-Verlag.
Chopra, S., Ghose, A., & Meyer, T. (2006). Social choice theory, belief merging,
strategy-proofness. Information Fusion, 7 (1), 6179.
Conitzer, V., Davenport, A. J., & Kalagnanam, J. (2006). Improved bounds computing Kemeny rankings. Proceedings 21st National Conference Artificial
Intelligence (AAAI-2006).
Dietrich, F. (2007). generalised model judgment aggregation. Social Choice
Welfare, 28 (4), 529565.
Dietrich, F., & List, C. (2007a). Judgment aggregation quota rules: Majority voting
generalized. Journal Theoretical Politics, 19 (4), 391424.
Dietrich, F., & List, C. (2007b). Arrows theorem judgment aggregation. Social Choice
Welfare, 29 (1), 1933.
Dietrich, F., & List, C. (2007c). Strategy-proof judgment aggregation. Economics
Philosophy, 23 (3), 269300.
Dietrich, F., & Mongin, P. (2010). premiss-based approach judgment aggregation.
Journal Economic Theory, 145, 562582.
Dokow, E., & Holzman, R. (2010). Aggregation binary evaluations. Journal Economic
Theory, 145 (2), 495511.
Duddy, C., & Piggins, A. (2012). measure distance judgment sets. Social
Choice Welfare, 39 (4), 855867.
Endriss, U., Grandi, U., & Porello, D. (2010a). Complexity judgment aggregation: Safety
agenda. Proceedings 9th International Conference Autonomous
Agents Multiagent Systems (AAMAS-2010).
Endriss, U., Grandi, U., & Porello, D. (2010b). Complexity winner determination
strategic manipulation judgment aggregation. Proceedings 3rd International Workshop Computational Social Choice (COMSOC-2010).
Everaere, P., Konieczny, S., & Marquis, P. (2007). strategy-proofness landscape
merging. Journal Artificial Intelligence Research (JAIR), 28 (1), 49105.
Faliszewski, P., & Procaccia, A. D. (2010). AIs war manipulation: winning?. AI
Magazine, 31 (4), 5364.
Gaertner, W. (2006). Primer Social Choice Theory. LSE Perspectives Economic
Analysis. Oxford University Press.
512

fiComplexity Judgment Aggregation

Grandi, U., & Endriss, U. (2011). Binary aggregation integrity constraints. Proceedings 22nd International Joint Conference Artificial Intelligence (IJCAI2011).
Grossi, D. (2009). Unifying preference judgment aggregation.. Proceedings 8th
International Conference Autonomous Agents Multiagent Systems (AAMAS2009).
Hemachandra, L. A. (1989). strong exponential hierarchy collapses. Journal Computer System Sciences, 39 (3), 299322.
Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (1997). Exact analysis Dodgson
elections: Lewis Carrolls 1876 system complete parallel access NP. Journal
ACM, 44 (6), 806825.
Hemaspaandra, E., Hemaspaandra, L. A., & Menton, C. (2012). Search versus decision
election manipulation problems. Tech. rep. URCS-TR-2012-971, University
Rochester, Computer Science Department.
Hemaspaandra, E., Spakowski, H., & Vogel, J. (2005). complexity Kemeny elections.
Theoretical Computer Science, 349 (3), 382391.
Kemeny, J. (1959). Mathematics without numbers. Daedalus, 88 (4), 577591.
Konieczny, S., & Pino Perez, R. (2002). Merging information constraints: logical
framework. Journal Logic Computation, 12 (5), 773808.
Kornhauser, L. A., & Sager, L. G. (1993). one many: Adjudication collegial
courts. California Law Review, 81 (1), 159.
Lang, J., Pigozzi, G., Slavkovik, M., & van der Torre, L. (2011). Judgment aggregation
rules based minimization. Proceedings 13th Conference Theoretical
Aspects Rationality Knowledge (TARK-XIII).
Leone, N., Pfeifer, G., Faber, W., Eiter, T., Gottlob, G., Perri, S., & Scarcello, F. (2006).
DLV system knowledge representation reasoning. ACM Transactions
Computational Logic, 7 (3), 499562.
List, C., & Pettit, P. (2002). Aggregating sets judgments: impossibility result. Economics Philosophy, 18 (1), 89110.
List, C., & Puppe, C. (2009). Judgment aggregation: survey. Handbook Rational
Social Choice. Oxford University Press.
Maynard-Zhang, P., & Lehmann, D. J. (2003). Representing aggregating conflicting
beliefs. Journal Artificial Intelligence Research (JAIR), 19, 155203.
Meyer, A. R., & Stockmeyer, L. J. (1972). equivalence problem regular expressions
squaring requires exponential space. Proceedings 13th Annual Symposium Switching Automata Theory (SWAT/FOCS-1972). IEEE Computer
Society.
Miller, M., & Osherson, D. (2009). Methods distance-based judgment aggregation. Social
Choice Welfare, 32 (4), 575601.
513

fiEndriss, Grandi, & Porello

Narizzano, M., Pulina, L., & Tacchella, A. (2006). QBFEVAL web portal. Proceedings 10th European Conference Logics Artificial Intelligence (JELIA2006). Springer-Verlag.
Nehama, I. (2010). Approximate judgment aggregation. Proceedings 3rd International Workshop Computational Social Choice (COMSOC-2010).
Nehring, K., & Puppe, C. (2007). structure strategy-proof social choice. Part I: General characterization possibility results median spaces. Journal Economic
Theory, 135 (1), 269305.
Nehring, K., & Puppe, C. (2010). Abstract Arrowian aggregation. Journal Economic
Theory, 145 (2), 467494.
Papadimitriou, C. H. (1981). complexity integer programming. Journal
ACM, 28 (4), 765768.
Papadimitriou, C. H. (1994). Computational Complexity. Addison Wesley.
Pettit, P. (2001). Deliberative democracy discursive dilemma. Philosophical Issues,
11 (1), 268299.
Pigozzi, G. (2006). Belief merging discursive dilemma. Synthese, 152 (2), 285298.
Porello, D. (2010). Ranking judgments Arrows setting. Synthese, 173 (2), 199210.
Rahwan, I., & Tohme, F. (2010). Collective argument evaluation judgement aggregation. Proceedings 9th International Conference Autonomous Agents
Multiagent Systems (AAMAS-2010).
Rothe, J., Spakowski, H., & Vogel, J. (2003). Exact complexity winner problem
Young elections. Theoretical Computer Science, 63 (4), 375386.
Slavkovik, M. (2012). Judgment Aggregation Multiagent Systems. Ph.D. thesis, University Luxembourg.
Slavkovik, M., & Jamroga, W. (2011). Distance-based judgment aggregation three-valued
judgments weights. Proceedings IJCAI-2011 Workshop Social Choice
Artificial Intelligence.
Stockmeyer, L. J. (1976). polynomial-time hierarchy. Theoretical Computer Science,
3 (1), 122.
Wagner, K. W. (1987). complicated questions maxima minima,
closures NP. Theoretical Computer Science, 51 (12), 5380.
Walsh, T. (2011). hard manipulation problems?. Journal Artificial Intelligence Research (JAIR), 42, 129.

514

fiJournal Artificial Intelligence Research 45 (2012) 287-304

Submitted 05/12; published 10/12

Research Note
Removing Redundant Messages N-ary BnB-ADOPT
Patricia Gutierrez
Pedro Meseguer

PATRICIA @ IIIA . CSIC . ES
PEDRO @ IIIA . CSIC . ES

IIIA - CSIC, Universitat Autonoma de Barcelona
08193 Bellaterra, Spain

Abstract
note considers modify BnB-ADOPT, well-known algorithm optimally solving
distributed constraint optimization problems, double aim: (i) avoid sending
redundant messages (ii) handle cost functions arity. messages exchanged
BnB-ADOPT turned redundant. Removing redundant messages increases
substantially communication efficiency: number exchanged messages cases
least three times fewer (keeping measures almost unchanged), termination
optimality maintained. hand, handling n-ary cost functions addressed
original work, presence thresholds makes practical usage complex. issues
removing redundant messages efficiently handling n-ary cost functions
combined, producing new version BnB-ADOPT+ . Experimentally, show benefits
version original one.

1. Introduction
Distributed Constraint Optimization Problems (DCOPs) used model many actual world
multiagent coordination problems, meeting scheduling (Maheswaran, Tambe, Bowring,
Pearce, & Varakantham, 2004), sensor network (Jain, Taylor, Tambe, & Yokoo, 2009), traffic control (Junges & Bazzan, 2008), coalition structure generation (Ueda, Iwasaki, & Yokoo, 2010).
DCOPs include finite number agents, usual assumption agent holds one variable finite discrete domain. Variables related cost functions define cost
every combination value assignments. costs represent preferences penalty relations.
cost particular assignment sum cost functions evaluated assignment.
goal find complete assignment minimum cost message passing.
Considering distributed search DCOP solving, first proposed complete asynchronous
algorithm ADOPT (Modi, Shen, Tambe, & Yokoo, 2005). Later on, closely related BnBADOPT (Yeoh, Felner, & Koenig, 2010) presented. BnB-ADOPT changes nature
search ADOPT best-first search depth-first branch-and-bound strategy, obtaining better
performance. algorithms complete, compute optimum cost guaranteed
terminate. ADOPT BnB-ADOPT similar communication strategy, using similar set
messages (with small differences processes). also share data structures
semantics store update internal tables.
last years, several complete DCOP algorithms proposed. Following classification appears work Yeoh et al. (2010), search algorithm class mention (in
addition previously cited ADOPT BnB-ADOPT), Synchronous Branch Bound (SBB)
(Hirayama & Yokoo, 1997), Commitment Branch Bound (NCBB) (Chechetka & Sycara,
c
2012
AI Access Foundation. rights reserved.

fiG UTIERREZ & ESEGUER

2006) Asynchronous Forward Bounding (AFB) (Gershman, Meisels, & Zivan, 2009). distributed inference algorithms mention Dynamic Programming Optimization (DPOP) (Petcu &
Faltings, 2005). interest ADOPT BnB-ADOPT comes fact require
polynomial memory, asynchronous communication limited neighbors. Distributed
search algorithms use polynomial memory, DPOP requires exponential memory worst
case. Asynchronous algorithms shown suitable distributed context,
agents active time approach globally robust failures. SBB NCBB
examples synchronous algorithms. Limiting communication neighbors (no agent communicate agent constrained with) common requirement applications
(for instance sensor networks). algorithms, AFB, restriction allow agents broadcast messages.
ADOPT lesser extent BnB-ADOPT exchange large number messages. Often,
major drawback practical applicability, despite good theoretical properties
(soundness, optimality, termination). Aiming decreasing number exchanged messages
without compromising optimality termination, paper contains results detecting redundant messages BnB-ADOPT. Using results generate new version avoids sending
redundant messages. Experimentally, seen removing redundant
messages significantly decreases communication costs (the new algorithm exchanges least three
times fewer messages) several widely used DCOP benchmarks. proposed modifications
also applied ADOPT (Gutierrez & Meseguer, 2010a), although detail results
since according experiments BnB-ADOPT usually achieves better performance.
DCOPs, somehow natural use binary cost functions, agent one-toone relation neighbors. Since agent holds single variable, naturally
brings binary functions. However, cases agent may cost function higher
arity subset agents. original ADOPT (Modi et al., 2005) proposes way deal
n-ary constraints, BnB-ADOPT takes exact strategy deal constraints arity
higher two. However, strategy may cause inefficiency applied BnB-ADOPT.
provide simple way correct inefficiency, easily integrated BnB-ADOPT.
combined improvements new version algorithm called BnB-ADOPT+ .
Experimental results show benefits proposed approach respect original algorithm. comparison state-of-the-art DCOP algorithms also included.
paper structured follows. First, provide basic definitions short description BnB-ADOPT algorithm Section 2. introduce detection redundant messages
Section 3. discuss generalize algorithm deal efficiently cost functions
arity Section 4. Using results, propose new version BnB-ADOPT, called
BnB-ADOPT+ . Section 5 report experimental results benefits BnB-ADOPT+
respect previous version. addition, present results comparing BnB-ADOPT+
DCOP algorithms. Finally, conclude Section 6.

2. Background
Section provide basic DCOP definitions short description BnB-ADOPT
algorithm (Yeoh et al., 2010).
288

fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPT

2.1 DCOP
Distributed Constraint Optimization Problem (DCOP) (Modi et al., 2005), formally defined
(X , D, F, A, ) where:
X = {x1 , ..., xn } set n variables.
= {D1 , ..., Dn } collection finite domains variable xi takes values Di .
F set binary cost functions; fij C involving set variables var(f ) = {xi , xj }
mapping Di Dj 7 N {0, } associates non-negative cost combination
values variables xi xj . scope f var(f ), arity |var(f )|.
= {1, . . . , p} set p agents.
: X A, maps variable one agent.
cost function (also called soft constraint elsewhere) f evaluated particular value tuple
gives cost one pay taking values variables var(f ). Completely
permitted tuples f 0 cost, completely forbidden tuples cost. Intermediate
costs associated tuples neither completely permitted completely forbidden.
cost complete assignment sum evaluation cost functions assignment.
solution complete assignment cost user threshold. solution optimal
minimum cost. make usual simplifying assumption agent owns exactly one
variable, agents variables used interchangeably (we connect agents variables
subindexes, agent owns variable xi ). also assume cost function f among several variables
known every agent owns variable var(f ) (Yokoo, Durfee, Ishida, & Kuwabara, 1998).
Agents communicate messages never lost and, pair agents, messages
delivered order sent.
constraint graph represents DCOP instance, nodes graph correspond variables edges connect pairs variables appearing cost function. depth-first search
(DFS) pseudo-tree arrangement nodes edges constraint graph satisfies
(i) subset edges, called tree edges, form rooted tree (ii) two variables
cost function appear branch tree. edges called backedges. Tree
edges connect parent-child nodes, backedges connect node pseudo-parents
pseudo-children. DFS pseudo-trees constructed using distributed algorithms (Petcu, 2007).
2.2 BnB-ADOPT
BnB-ADOPT (Yeoh et al., 2010) distributed algorithm optimally solves DCOPs using
depth-first branch-and-bound search strategy. closely related ADOPT (Modi et al., 2005),
maintaining data structures communication framework. BnB-ADOPT starts constructing DFS pseudo-tree arrangement agents. this, agent knows parent,
pseudo-parents, children pseudo-children.
2.2.1 DATA TRUCTURES
execution agent maintains: current value di ; current context Xi ,
knowledge current value assignment ancestors; timestamp current value
289

fiG UTIERREZ & ESEGUER

di value assignment current context (so value recency compared); every
value Di context Xi , lower upper bound LBi (d) UBi (d); two bounds LBi
UBi calculated following way:
(d) =

P

(xj ,dj )Xi

fij (d, dj )

P
LBi (d) = (d) + P xc children lbi,c (d)
UBi (d) = (d) + xc children ubi,c (d)
LBi = mindDi {LBi (d)}
UBi = mindDi {UBi (d)}
(d) sum costs cost functions ancestors given assigns
value ancestors assign respective values Xi . Tables lbi,c (d) ubi,c (d) store upper
lower bounds children c, values Di current context Xi . LBi UBi lower
upper bounds optimal solution context Xi . Due memory limitations, agent
store lower upper bounds one context. Agents may reinitialize bounds time
context change.
goal every agent explore search space ultimately chooses value
minimizes LBi . agent BnB-ADOPT changes value assignment able
determine optimal solution value provably better best solution found
far current context. words, LBi (di ) UBi current value di .
prune values search, agents use threshold value H, initially . Hroot remains
entire solving process. agents, threshold values calculated sent
parent children following way. threshold th sent agent value child c
calculated as:
th = min(T Hi , U Bi ) (d)

P

chchildren,ch6=c lbi,ch (d)

Thresholds represent estimated upper bound current context. Therefore, agents
prune values using thresholds sent parents. is, agent changes value di (prunes
it) LBi (di ) min{THi , UBi }. LBi = U Bi , agent reached cost optimal
solution context Xi .
2.2.2 C OMMUNICATION
communication needed BnB-ADOPT calculate global costs individual agents
assignments coordinate search towards optimal solution. BnB-ADOPT agents use three
types messages: VALUE, COST TERMINATE, defined follows:
VALUE(i; j; val; th): agent informs child pseudo-child j takes value val
threshold th;
COST(k; j; context; lb; ub): agent k informs parent j context bounds lb, ub;
TERMINATE(i; j): agent informs child j terminates.
290

fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPT

mentioned above, BnB-ADOPT value assignment timestamped (both VALUE
COST messages). permits VALUE COST messages update context receiver
agent, values recent.
Upon reception VALUE message, value val copied receiver context timestamp
recent, threshold th updated sender parent receiver.
Upon reception COST message child c, values context COST message
recent receiver context copied receiver agent. receiver context
compatible COST message context, agent updates lower upper bounds
lbi,c (d) ubi,c (d) lower upper bounds COST message, respectively. Otherwise, COST message discarded. Contexts compatible iff agree common agentvalue pairs. Every time context change, agents check bounds must reinitialized.
2.2.3 E XECUTION
BnB-ADOPT agent executes following loop. First, reads processes incoming messages. completely processing message queue, changes value lower bound
current value surpasses min{THi , UBi }, case value proven suboptimal current context. Finally, agent sends following messages: VALUE per child,
VALUE per pseudo-child COST parent. process repeats root agent r
reaches termination condition LBr = UBr , means found minimum cost.
sends TERMINATE message children terminates. Upon reception
TERMINATE message, agent sends TERMINATE messages children; agent terminates
LBi = U Bi .
rest paper, assume reader familiarity BnB-ADOPT (for
detailed description, see original source, Yeoh et al., 2010).

3. Redundant Messages
Section present results redundant messages considering BnB-ADOPT working
DCOP instances binary cost functions. following i, j k agents, executing
BnB-ADOPT. Agent i, holding variable xi , takes value v assignment xi v made
informs children, pseudo-children parent new value assignment. state
defined by: (i) value, (ii) context, contexti , set value assignments agents located
branch (timestamps considered part context), (iii) possible
value children c, lower upper bounds lbi,c (d)/ubi,c (d).
message msg sent j redundant if, future time t, messages received
j msg would cause effect, msg could avoided. message
msg sent j containing assignment xi v timestamp updates contextj [i] (that
is, part contextj containing value xi ) timestamp t0 > t0 .
Lemma 1 BnB-ADOPT, agent sends two consecutive VALUE messages agent j timestamps t1 t2 , message timestamp agent assignment t1 < < t2 .
Proof. VALUE message timestamp t1 t2 i, since VALUE
messages consecutive sent agent i. COST messages build contexts
information VALUE messages. Since VALUE contains timestamp t1 t2 i,
COST contain i.
2
291

fiG UTIERREZ & ESEGUER

Theorem 1 BnB-ADOPT, agent sends agent j two consecutive VALUE messages
val, second message redundant.
Proof. Let V1 V2 two consecutive VALUE messages sent agent agent j
value val timestamps t1 t2 , t1 t2 . t1 = t2 , V2 always discarded (and
therefore V2 always redundant), concentrate second option, t1 < t2 .
V1 V2 agent j may receive messages coming agents. V1 reaches j,
following cases possible:
1. V1 update contextj [i] (V1 discarded). V2 arrives j may happen:
(a) V2 update contextj [i] (V2 discarded). Future messages processed
V2 received, V2 redundant.
(b) V2 updates contextj [i] timestamp t. two options: (i) t2 > >
t1 (ii) t2 > = t1 . Option (i) impossible according Lemma 1. Option
(ii) possible, since = t1 value contained V2 already contextj [i].
future messages, every message accepted timestamp t2 contextj [i] would
also accepted timestamp t1 contextj [i]. Since messages
timestamp t1 t2 i, conclude V2 redundant.
2. V1 updates contextj [i] val, timestamp t1 . V2 arrives j may happen:
(a) V2 update contextj [i]; case (1.a).
(b) V2 updates contextj [i]: since V1 updated contextj Lemma 1, timestamp
contextj [i] must t1 . V2 change contextj [i] timestamp rewritten
t2 . Since messages timestamp t1 t2 (Lemma 1),
future message could update contextj t2 would also update t1 . V2
redundant.
considered threshold contained VALUE messages BnB-ADOPT
complete terminates without use thresholds (they included increase efficiency).
see this, enough realize BnB-ADOPT without using thresholds equivalent BnBADOPT thresholds always equal (equal initial value threshold
agent). results Section 5 work Yeoh et al. (2010) remain valid BnB-ADOPT
works thresholds.1 Therefore, BnB-ADOPT without thresholds terminates cost
optimal solution.
2
Lemma 2 BnB-ADOPT, agent k sends two consecutive COST messages C1 C2
context, k detected context change C1 C2 , message
timestamp agent ones C1 C2 i, context incompatible
C1 C2 .
Proof. time agent constrained agent k changes value sends VALUE message
children pseudo-children. Since context change C1 C2 , message
timestamp ones C1 C2 contain context incompatible C1
C2 ; otherwise agent k would necessarily detected context change.
2
1. see this, enough realize thresholds used proof Section 5 proof Lemma 8.
However, proof Lemma remains valid replacing threshold .

292

fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPT

Theorem 2 BnB-ADOPT, agent k sends agent j two consecutive COST messages
content (context, lower/upper bound) k detected context change, second
message redundant.
Proof. Let C1 C2 two consecutive COST messages sent k j content,
contextk changed sending them. message may arrive j C1
C2 (coming agents). Upon reception, recent values C1 (and later C2 )
copied contextj (by PriorityMerge, see Yeoh et al., 2010). j detects context change, tables
lbj,c (d) ubj,c (d) could reinitialized. Otherwise, contextj compatible COST
context tables lbj,c (d) ubj,c (d) updated information contained COST
message.
Copying C2 recent values contextj essential. Let us assume values
copied. Since context change C1 C2 , message timestamps
C1 C2 necessarily include context compatible C2 , according
Lemma 2. Therefore C2 arrives, C2 contains values recent timestamp
incompatible contextj , C2 updates timestamps.
Now, let us consider possible lbj,c (d), ubj,c (d) reinitializations. Since C2 cause
context change j, updates timestamps (Lemma 2), cause lower/upper
bound reinitialization j. that, proof concentrates update bounds.
C1 arrives, may happen:
1. C1 compatible contextj , bounds discarded. C2 arrives may happen:
(a) C2 compatible contextj , bounds discarded. Since C2 = C1 (except
timestamps), actions done detecting C2 compatible
already done detecting C1 compatible. Thus, C2 redundant.
(b) C2 compatible contextj , bounds included j. Since C1
compatible, must least one agent j changed value, received
j C1 C2 . one several VALUE messages its/their way
towards k k descendants. Upon reception, one several COST messages
generated. last sent k j updated bounds, C2
could avoided updated COST arrive j. Consequently,
C2 redundant.
2. C1 compatible contextj , bounds included. C2 arrives j may
happen:
(a) C2 compatible contextj , bounds discarded. Bounds provided
C2 useless, based outdated information. future updated
COST reach j (same reasons previous case 1.b). C2 redundant.
(b) C2 compatible contextj , bounds included. However, causes
change j bounds, unless bounds reinitialized. case least
one agent j changed value, information reached j
C1 C2 . situation case (1.b). Hence, C2 redundant.
2
Temporarily, define BnB-ADOPT+ version BnB-ADOPT following changes:
(i) second two consecutive VALUE messages i, j, val th sent,
293

fiG UTIERREZ & ESEGUER

(ii) second two consecutive COST messages k, j, context, lb ub
k detects context change sent. changes affect algorithms optimality,
proved next.
Theorem 3 BnB-ADOPT+ terminates cost optimal solution.
Proof. Theorem 1 agent sends two consecutive VALUE messages val
agent j, second redundant. However, differ th, also send second VALUE
message efficiency purposes. Observe sending redundant messages cause
incorrect behavior. Theorem 2, COST messages sent BnB-ADOPT+ redundant
eliminated. BnB-ADOPT terminates cost optimal solution (Yeoh
et al., 2010), BnB-ADOPT+ also terminates cost optimal solution.
2
Experimentally, version caused minor benefits. realized ignored threshold management. Observe thresholds reinitialized context
change (caused VALUE COST messages); causes special difficulty original
BnB-ADOPT algorithm time agent processes message queue, sends VALUE
messages children containing thresholds. Now, VALUE messages
sent, children run algorithm threshold periods. avoid this, children way ask threshold parents reinitialization. done
using COST messages, sent children parents. Thus, define BnB-ADOPT+
BnB-ADOPT version following changes:
1. Agent remembers last message sent neighbors.
2. COST message j includes boolean hReq, set true j threshold
reinitialized.
3. j send COST message equal (ignoring timestamps) last message
sent, new COST message sent j detected context change
them.
4. send VALUE message j equal (ignoring timestamps) last message
sent, new VALUE message sent last COST message received
j hReq = true.
immediate see changes alter optimality termination properties
BnB-ADOPT+ : original BnB-ADOPT, includes redundant messages, terminates cost
optimal solution (Yeoh et al., 2010); sending redundant messages algorithm
remains optimal terminates. proposed changes avoid redundant messages also
applied ADOPT algorithm (Gutierrez & Meseguer, 2010a).

4. Dealing N-ary Cost Functions
defined DCOPs using binary cost functions, although DCOP definition easily extended include cost functions arity. Similarly, ADOPT BnB-ADOPT extended
deal cost functions arity. proposed BnB-ADOPT extension exactly
294

fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPT

ADOPT extension handle n-ary cost functions (Yeoh et al., 2010). extension, described
work Modi et al. (2005), follows: 2
...a ternary constraint fijk ... defined three variables xi , xj , xk ... Suppose xi
xj ancestors xk ... ternary constraint, xi xj send VALUE
messages xk . xk evaluates ternary constraint sends COST messages
back tree normal ... Thus, deal n-ary constraint assigning
responsibility evaluation lowest agent involved constraint.
difference evaluation n-ary constraint binary one lowest
agent must wait receive ancestors VALUE messages evaluating ...
words (replacing constraint cost function), proposed extension agents
must send VALUE messages lowest agent DFS pseudo-tree involved cost
function. case binary cost function, lower agent (of two involved cost
function) always receives VALUE messages. case n-ary cost functions (involving
two agents), intermediate agents receive VALUE messages rest agents
involved function. lowest agent xk must receive VALUE messages evaluating
cost function. this, called evaluator agent. Upon reception
messages, xk evaluates function sends COST message parent, receives
processes message COST message. applying technique BnB-ADOPT
issues appear, explained following.
4.1 TERMINATE Messages
version binary BnB-ADOPT, time agent changes value sends VALUE messages
children pseudo-children. non-root agent terminates reaches termination
condition LB = U B receiving TERMINATE message parent (for root agent
TERMINATE message needed). side-effect, last value taken agents
optimal value. feature appreciated distributed environments, optimal values
distributed among agents without requiring central agent charge whole solution.
n-ary BnB-ADOPT, although root agent computes minimum cost, direct implementation may terminate optimal value assigned every agent. Let us consider ternary
cost function among agents i, j k (as Figure 1); root DFS pseudo-tree,
k evaluates cost function. Agent may explore last value, jump back best value
reaches termination condition LB = U B, sends VALUE message k TERMINATE
message child j. Upon reception VALUE message, k send COST message j.
COST message contains last assignment (optimal value) made i. However, j already
processed TERMINATE message i, ended without processing COST message, means j may end outdated context: 3 causes j end assigned
value may optimal one, since truly minimize cost global
solution. 4
2. must also assured agents involved n-ary cost function lie branch pseudo-tree.
guaranteed since agents sharing n-ary cost functions form clique constraint graph. performing
depth first traversal construct DFS pseudo-tree, agents clique necessarily lie branch.
3. would happen agent would sent VALUE messages every child j informing value changes,
original BnB-ADOPT strategy deal n-ary cost functions.
4. Technically speaking, j might terminate incorrect context.

295

fiG UTIERREZ & ESEGUER

VALUE

xi

TH=50

xj

TH=

xk
nary constraint

TH=min(, UB)
(v)-child xc lb(v,child)

xc

xi, xj, xk

Figure 1: Original BnB-ADOPT dealing n-ary constrains, use VALUE messages
simple way correct include TERMINATE messages last assignment made
sender agent. way, receiver update context terminate value
truly minimizes lower bound.
4.2 VALUE Messages
ADOPT strategy n-ary cost functions applied literally BnB-ADOPT, scenarios inefficiently solved. instance, consider Figure 1, variables agents i, j, k
share ternary cost function. Agent root DFS pseudo-tree k lowest agent
(therefore evaluator) ternary cost function. Suppose xj constrained variables
problem, represented gray subtree. Since VALUE messages sent j
k inform value changes, VALUE messages sent j, agent j
threshold provided parent. Thresholds provided parents lowest upper bounds
among visited contexts (the cost best solution found far), thresholds computed
agents upper bounds cost current context. Thresholds introduced BnB-ADOPT speed problem resolution increase pruning opportunities,
tightest threshold agent j j subtree clearly detrimental algorithm
performance.
simple way avoid issue send VALUE messages descendants (children
pseudo-children). However, needed. avoid unnecessary messages
sending VALUE messages lowest agent charge evaluating cost function generate
COST messages updated LB U B children propagate H value
DFS pseudo-tree. agent involved cost function i, neither
evaluator cost function child need receive VALUE messages i.
proposed extension BnB-ADOPT deal n-ary cost functions. Observe
296

fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPT

binary case, proposal collapses existing operation algorithms. rest
paper assume extension included version BnB-ADOPT, deal cost
functions arity.
4.3 Correctness Completeness
define n-ary BnB-ADOPT work Yeoh et al. (2010) (i.e., agent sends VALUE
messages evaluator agent cost functions involved in) plus agent sends
VALUE messages children pseudo-tree. easy show n-ary BnBADOPT version optimal terminates. First, prove agents send VALUE messages
children pseudo-children, extended n-ary BnB-ADOPT terminates cost
optimal solution. Second, show VALUE messages send pseudo-children redundant
(except pseudo-child evaluator agent cost function involved sender). Third,
demonstrate redundant messages binary case remain redundant n-ary case.
Combining results obtain desired output: proof n-ary BnB-ADOPT+ terminates
cost optimal solution.
Theorem 4 N-ary BnB-ADOPT terminates cost optimal solution.
Proof. Imagine extended version n-ary BnB-ADOPT agents send VALUE messages
descendants (children pseudo-children) pseudo-tree. extended version
n-ary BnB-ADOPT working binary case: agent sends VALUE messages
descendants (children pseudo-children) sends COST message parent.
case, easy check results Section 5 paper Yeoh et al. (2010) apply
(it long conceptually easy; observe result Section 5 paper Yeoh et al.
uses fact cost functions binary). particular, Yeoh et al. proved binary BnBADOPT terminates cost optimal solution. Therefore, extended version n-ary
BnB-ADOPT terminates cost optimal solution.
Now, consider VALUE messages sent agent pseudo-children
evaluators cost function involving agent i. show messages redundant.
receiving VALUE message i:
1. Agent j updates context.
2. message comes parent, agent j rewrites threshold message
threshold.
know parent j, consider point (1) only. Agent j evaluating
agent cost function involving i; thus, another agent k branch
j charge evaluation. agent k receive sure VALUE
messages coming ancestors, send COST messages tree.
COST messages reach j, update context exactly way receiving j
VALUE message i. Original VALUE messages redundant effect
obtained COST messages arriving k. Therefore, remove VALUE
messages extended version n-ary BnB-ADOPT, algorithm terminate
cost optimal solution. definition, algorithm n-ary BnB-ADOPT.
2
Next prove redundant messages binary case remain redundant n-ary case.
297

fiG UTIERREZ & ESEGUER

Lemma 3 VALUE COST messages found redundant binary BnB-ADOPT remain redundant
n-ary BnB-ADOPT.
Proof. prove Lemma, enough realize Theorems 1 2 remain valid n-ary
BnB-ADOPT. Observe proofs Lemma 1, Theorems 1 2 required use
binary cost functions. proof Lemma 2 easily generalized n-ary case, simply
replacing children pseudo-children children evaluator pseudochildren.
2
define n-ary BnB-ADOPT+ n-ary BnB-ADOPT removing redundant VALUE COST
messages, Section 3.
Corollary 1 N-ary BnB-ADOPT+ terminates cost optimal solution.
Proof. Combining Theorem 4 Lemma 3 prove n-ary BnB-ADOPT sending
redundant VALUE COST messages terminates cost optimal solution.
Section 3, child may ask parent resend threshold VALUE message
threshold reinitialized. exceptions cause extra difficulty here.
justification show sending messages question optimality termination
binary case, remains fully valid n-ary case.
2
following make difference binary n-ary cases; instead
use single algorithm, n-ary BnB-ADOPT+ , simply name BnB-ADOPT+ .

5. Experimental Results
experimentally evaluated performance original BnB-ADOPT (binary n-ary versions)
proposal n-ary BnB-ADOPT (Section 4) BnB-ADOPT+ (that includes
changes proposed Sections 3 4), using discrete event simulator. Performance evaluated terms communication cost (total number messages exchanged) computation effort
(NCCCs, non-concurrent constraint checks, see Meisels, Kaplansky, Razgon, & Zivan, 2002).
also consider number cycles number iterations simulator must perform
solution found. cycle, agents read incoming messages message queue,
process them, send outgoing messages required. DFS pseudo-tree generated
distributed form, following most-connected heuristic.
First, evaluate impact removing redundant messages binary case, comparing
original BnB-ADOPT BnB-ADOPT+ . Second, evaluate performance non-binary instances. compare original BnB-ADOPT proposal n-ary BnB-ADOPT (changes
Section 4) BnB-ADOPT+ (n-ary BnB-ADOPT saving redundant messages). Lastly,
compare BnB-ADOPT+ two well-known algorithms DCOP solving: Synchronous
Branch Bound (SBB) (Hirayama & Yokoo, 1997) Asynchronous Forward Bounding (AFB)
(Gershman et al., 2009). SBB completely synchronous algorithm whereas AFB performs synchronous value assignments asynchronously computes bounds used pruning. SBB
AFB maintain total order variables perform assignments BnB-ADOPT+ uses partial
ordering following DFS pseudo-tree structure. present last comparison provide
overall picture BnB-ADOPT+ asynchronous nature affects number messages
exchanged computation.
Experiments performed three different benchmarks: random DCOPs (binary ternary
cases), meeting scheduling sensor networks (both binary, obtained public DCOP
298

fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPT

repository, Yin, 2008). Random DCOPs characterized hn, d, p1 i, n number
variables, domain size p1 network connectivity. Random generation assures
connected problems, agents problem belong constraint graph
DFS pseudo-tree. Binary instances contain p1 n(n 1)/2 binary cost functions, ternary
instances contain p1 n(n 1)(n 2)/6 ternary cost functions. Costs selected randomly
set {0,..., 100}. meeting scheduling, variables represent meetings, domains represent time
slots assigned meetings, cost functions meetings share participants
(Maheswaran et al., 2004). sensor networks, variables represent areas need observed,
domains represent time slots, cost functions adjacent areas (Maheswaran
et al., 2004).
Results first experiment comparing BnB-ADOPT BnB-ADOPT+ appear Table 1
Table 2. Table 1 shows results binary random problems averaged 50 instances. Table
1(a) shows results varying network connectivity hn = 10, = 10, p1 = 0.2...0.8i. Table 1(b)
shows results varying domain size hn = 10, = 6...12, p1 = 0.5i. Table 1(c) shows results
varying number variables hn = 6...12, = 10, p1 = 0.5i. Table 2 (a) shows meeting
scheduling instances 4 cases different hierarchical scenarios: case (8 variables), B (10
variables), C (12 variables) (12 variables). Table 2 (b) shows sensor network instances 4
cases different topologies: cases (16 variables), B (16 variables), C (10 variables) (16
variables). two last benchmarks, results averaged 30 instances.
Experiments binary random DCOPs show algorithm BnB-ADOPT+ obtains important communication savings respect original BnB-ADOPT. number messages
reduced 3 6 times connectivity domain size increases, also showing consistent reduction, factor 4 5, increasing number variables. meeting
scheduling instances, messages reduced factor 3 9, sensor networks,
factor 5 8. standard deviation messages also decreases problems
considered.
Regarding NCCCs, mean also moderately reduced instances (around 10%).
binary random benchmark, standard deviation also slightly reduced. meeting scheduling
sensor networks, standard deviation increases. However, looking every problem
separately, number NCCCs BnB-ADOPT+ always smaller every instance. number cycles remain practically unchanged. results clearly indicate that, binary case,
removing redundant messages beneficial enhance communication, achieving also
moderated gains computation.
addition, took particular random instance hn = 10, = 10, p1 = 0.5i, solved
repeatedly using original BnB-ADOPT BnB-ADOPT+ , varying order agents
activated simulator (using DFS pseudo-tree executions). Results quite
similar across executions. Regarding saved messages, BnB-ADOPT always required 4.3
4.4 times messages BnB-ADOPT+ (considering individual executions). results show activation order agents simulator impact message reduction
achieved BnB-ADOPT+ .
Results second experiment appear Table 3, contains results ternary random
instances hn = 8, = 5, p1 = 0.4...0.8i averaged 50 instances. First row contains results
original BnB-ADOPT (including modification Section 4.1). Second row contains results
n-ary BnB-ADOPT proposal (Section 4), thresholds propagated children.
299

fiG UTIERREZ & ESEGUER

(a) < n = 10, = 10, p1 >
p1
0.2
0.3
0.4
0.5
0.6
0.7
0.8

#Messages
1068 (274)
416 (74)
39,158 (36,578)
11,774 (10,105)
270,379 (432,782)
69,277 (92,291)
2,273,768 (2,149,369)
493,137 (422,360)
11,439,563 (10,231,971)
2,205,848 (1,802,655)
60,221,283 (34,121,853)
8,930,713 (5,092,602)
161,327,710 (94,398,879)
22,972,676 (13,464,530)

#NCCC
904 (23)
881 (23)
68,882 (62,180)
62,031 (53,085)
504,373 (796,625)
475,534 (776,820)
4,311,524 (3,923,577)
4,112,299 (3,760,583)
23,759,356 (22,468,476)
22,783,209 (21,040,893)
134,868,051 (90,469,274)
129,143,706 (89,328,458)
360,857,244 (212,464,295)
353,180,585 (209,726,371)

#Cycles
62 (15)
62 (15)
1,751 (1,625)
1,753 (1,629)
10,313 (16,478)
10,317 (16,483)
73,715 (69,676)
73,792 (69,808)
331,947 (299,259)
332,841 (300,784)
1,526,394 (862,540)
1,527,960 (865,974)
3,752,164 (2,210,488)
3,755,118 (2,213,631)

(b) < n = 10, d, p1 = 0.5 >

6
8
10
12

#Messages
618,005 (573,704)
119,841 (104,980)
1,362,586 (951,900)
288,422 (201,488)
2,711,719 (2,929,759)
597,325 (633,879)
4,871,563 (9,725,100)
1,015,541 (1,706,302)

#NCCC
701,352 (642,821)
657,276 (593,830)
2,090,231 (1,470,631)
1,986,430 (1,398,420)
5,092,387 (5,376,943)
4,842,265 (5,133,731)
10,969,641 (20,549,608)
10,342,414 (18,679,208)

#Cycles
20,305 (18,869)
20,342 (18,924)
44,507 (31,209)
44,562 (31,238)
88,224 (96,033)
88,329 (96,195)
157,856 (314,908)
157,994 (315,137)

(c) < n, = 10, p1 = 0.5 >
n
6
8
10
12

#Messages
4,388 (3,272)
1,514 (1,020)
72,783 (54,772)
20,326 (12,971)
2,603,727 (3,358,285)
547,079 (656,709)
111,436,193 (133,362,317)
20,169,771 (23,877,564)

#NCCC
13,077 (13,214)
12,221 (12,439)
173,038 (126,743)
159,698 (113,801)
5,289,823 (6,844,174)
5,005,774 (6,576,047)
187,178,211(237,619,542)
179,110,208 (228,862,664)

#Cycles
350 (259)
350 (259)
3,576 (2,679)
3,581 (2,689)
84,706 (112,469)
84,816 (112,774)
2,633,456 (3,148,339)
2,636,675 (3,152,543)

Table 1: Results (mean standard deviation parenthesis) random binary benchmarks
varying network connectivity, domain size number variables: BnB-ADOPT
(first row), BnB-ADOPT+ (second row).

Third row contains BnB-ADOPT+ results, enhances last version removing redundant
messages.
Experiments ternary random DCOPs show assuring propagation threshold values children produces clear performance benefits (Table 3, second row). Agents send
extra messages children containing threshold values, sent original version,
extra messages contribute better pruning. global effect, less communication
required overall search, significant reductions obtained metrics (messages,
NCCCs cycles). Maintaining positive effect, remove redundant messages using BnBADOPT+ (Table 3, third row). Removing redundant messages causes savings one order
magnitude number messages exchanged. result positive execution
time often dominated communication time. Observe number cycles little
300

fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPT

variation second third row. Also, savings NCCCs, although
significant. results conclude that, n-ary case, proposal nary BnB-ADOPT yields clear benefits communication computation, removing
redundant messages substantially reduces communication.
Finally, present third experiment comparing BnB-ADOPT SBB AFB Figure
2. experiments performed binary random (top), meeting scheduling (middle)
sensor network (bottom) instances. SBB variables statically ordered using width heuristic
described Hirayama Yokoo (1997), AFB variables ordered following heuristic, BnB-ADOPT+ variables partially ordered using connected heuristic
constructing pseudo-tree. restrict solving time one hour; case SBB AFB

(a) Meeting Scheduling


B
C


#Messages
178,899 (3,638)
18,117 (627)
65,556 (912)
15,373 (426)
62,707 (741)
11,343 (347)
41,282 (862)
13,354 (455)

#NCCC
446,670 (2,786)
413,507 (13,446)
125,331 (1,963)
120,900 (1,969)
80,369 (54)
74,518 (407)
60,424 (460)
49,878 (1,692)

(b) Sensor Network
#Cycles
8,202 (302)
8,203 (302)
2,663 (43)
2,665 (43)
2,353 (34)
2,355 (35)
1,545 (48)
1,547 (48)


B
C


#Messages
9,369 (99)
1,103 (73)
12,917 (116)
1,569 (77)
6,429 (59)
1,177 (51)
15,560 (145)
2,155 (81)

#NCCC
7,241 (52)
450 (232)
11,054 (135)
592 (879)
8,786 (52)
1,495 (2,490)
12,641 (57)
2,137 (3,552)

#Cycles
313 (3)
307 (4)
414 (4)
409(4)
340 (5)
340 (6)
477 (2)
477 (2)

Table 2: Results (mean standard deviation parenthesis) meeting scheduling
sensor network instances: BnB-ADOPT (first row), BnB-ADOPT+ (second row).

p1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

#Messages
257,238 (294,137)
147,379 (139,646)
28,614 (23,822)
648,217 (413,580)
401,306 (239,271)
63,778 (33,025)
1,642,247 (975,131)
1,210,143 (523,825)
164,901 (68,876)
2,321,729 (1,106,146)
1,771,256 (678,893)
225,836 (69,630)
3,666,514 (1,316,782)
2,973,239 (1,406,400)
311,524 (93,062)
4,013,891 (897,046)
3,537,027 (1,119,242)
348,199 (73,620)
4,892,733 (788,897)
4,616,032 (1,135,830)
399,662 (70,572)

#NCCC
1,522,580 (1,863,696)
829,594 (852,754)
764,516 (783,253)
4,029,045 (2,807,389)
2,414,946 (1,641,836)
2,237,786 (1,520,761)
12,585,339 (8,483,693)
9,194,309 (4,938,582)
8,744,465 (4,813,577)
19,424,669 (10,248,817)
14,775,187 (6,678,667)
14,337,952 (6,372,442)
35,743,718 (15,729,105)
29,252,469 (16,146,978)
27,614,749 (14,316,979)
41,469,157 (11,804,005)
36,966,889 (13,604,359)
35,314,718 (12,331,316)
55,151,742 (11,209,964)
52,221,369 (14,948,424)
49,189,230 (13,197,765)

#Cycles
10,880 (12,295)
5,793 (5,357)
5,797 (5,360)
24,026 (15,085)
13,938 (8,271)
13,943 (8,270)
55,156 (32,553)
39,373 (17,188)
39,381 (17,197)
74,279 (36,150)
55,101 (21,280)
55,103 (21,280)
115,523 (43,001)
92,020 (45,010)
92,020 (45,010)
124,408 (29,353)
108,858 (36,028)
108,858 (36,028)
150,077 (25,114)
140,472 (35,672)
140,472 (35,672)

Table 3: Results (mean standard deviation parenthesis) random ternary DCOPs:
original BnB-ADOPT (first row), proposal n-ary BnB-ADOPT (second row),
BnB-ADOPT+ (third row).

301

fiG UTIERREZ & ESEGUER

8

10

10

10

7

10

8

10
6

NCCC

messages

10

5

10

4

10

BnB-ADOPT+
SBB
AFB

3

10

BnB-ADOPT+
SBB
AFB

4

10

2

10
0.2

6

10

2

0.3

0.4

0.5

0.6
p1

0.7

10

0.8

9

0.2

0.3

0.4

0.5

0.6
p1

0.7

0.8

12

10

10

8

10

10

7

10

NCCC

messages

10

6

10

5

10

BnB-ADOPT+
SBB (timeouts)
AFB (timeouts)



BnB-ADOPT+
SBB (timeouts)
AFB (timeouts)

6

10

4

10

8

10

4

B
C
meeting scheduling problems

10





B
C
meeting scheduling problems



9

10

10

10

8

10

8

10

7

NCCC

messages

10

6

10

6

10

5

10

4

10

BnB-ADOPT+
SBB (timeouts)
AFB (timeouts)

BnB-ADOPT+
SBB (timeouts)
AFB (timeouts)

4

10

3

10



B
C
sensor network problems





B
C
sensor network problems



Figure 2: Comparison algorithms BnB-ADOPT+ , SBB, AFB. Top: binary random instances.
Middle: meeting scheduling. Bottom: sensor networks.

problem instance could solved amount time, present amount messages
exchanged NCCCs performed timeout.
results observe random instances BnB-ADOPT+ significantly
efficient low connected problems, however tightly connected problems requires
messages computational effort SBB AFB. explain behavior given BnBADOPT+ asynchronous algorithm, designed benefit pseudo-tree structure,
non-connected agents lying different branches pseudo-tree explore search space
302

fiR EMOVING R EDUNDANT ESSAGES N- ARY B N B-ADOPT

parallel. connectivity increases, width pseudo-tree decreases (in fully connected
problem pseudo-tree single branch agents totally ordered). makes
BnB-ADOPT+ asynchronous potential decrease. time, higher number reinitializations (bounds context) performed, since agents links ancestors, reduces
pruning effectiveness.
meeting scheduling sensor network instances, see BnB-ADOPT+ several orders magnitude efficient SBB AFB. structured nature problems
(with different topology every case A, B, C (Maheswaran et al., 2004) suitable build
balanced pseudo-trees) allows BnB-ADOPT+ benefit asynchronous search. addition,
observed instances variability costs smaller random problems:
costs quite similar, others clearly larger. cases, upper bound close
optimum cost reached early execution. However satisfy pruning condition,
lower bounds contributions almost cost functions needed. observed SBB
lesser extent AFB go deep search tree obtain contributions (pruning
usually done last agents ordering) finally subject thrashing.
hand, BnB-ADOPT+ computes local bounds every agent since agents assigned every
moment execution, specialized upper bounds propagated every node pseudotree. allows BnB-ADOPT+ perform pruning and, consequence, reduces search
space faster AFB SBB. confirmed fact empirically testing instances
small variability costs (even synthetic instances tuples cost).
summary, see proposed BnB-ADOPT+ clearly efficient original
BnB-ADOPT. binary instances, BnB-ADOPT+ processes third (or less) total number
messages required BnB-ADOPT (in instances, messages reduced factor 8
9), still reaches optimal solution almost number cycles. ternary instances,
savings reach one order magnitude communication almost cases. Regarding
comparison SBB AFB, new BnB-ADOPT+ outperforms (in number
messages NCCCs) low connected random instances, contrary occurs highly
connected ones. Regarding meeting scheduling sensor network instances, BnB-ADOPT+ outperforms SBB AFB large margin. results indicate value BnB-ADOPT+
optimal DCOP solving.

6. Conclusion
presented two contributions increase performance BnB-ADOPT, reference algorithm optimally solve distributed constrained optimization problems. First, presented theoretical results detect redundant messages. Second, described difficulties dealing
n-ary cost functions (the original algorithm presented detail binary case). Combining two contributions, generated new version BnB-ADOPT. new version, called
BnB-ADOPT+ , obtains substantial savings respect original algorithm, tested
commonly used benchmarks DCOP community.
303

fiG UTIERREZ & ESEGUER

Acknowledgments
paper extension previous publication (Gutierrez & Meseguer, 2010b), generalization n-ary cost functions, deeper explanations, detailed proofs, experiments.
Authors sincerely thank reviewers useful criticisms.

References
Chechetka, A., & Sycara, K. (2006). No-commitment branch bound search distributed
constraint optimization. Proc. AAMAS-06, 14271429.
Gershman, A., Meisels, A., & Zivan, R. (2009). Asynchronous forward bounding distributed
cops. Journal Artificial Intelligence Research, 34, 6188.
Gutierrez, P., & Meseguer, P. (2010a). Saving messages adopt-based algorithms. Proc. 12th DCR
workshop AAMAS-10, 5364.
Gutierrez, P., & Meseguer, P. (2010b). Saving messages BnB-ADOPT. Proc. AAAI-10, 1259
1260.
Hirayama, K., & Yokoo, M. (1997). Distributed partial constraint satisfaction problem. Proc. CP97, 222236.
Jain, M., Taylor, M., Tambe, M., & Yokoo, M. (2009). DCOPs meet realworld: Exploring
unknown reward matrices applications mobile sensor networks. Proc. IJCAI-09, 181
186.
Junges, R., & Bazzan, A. L. C. (2008). Evaluating performance DCOP algorithms real
world dynamic problem. Proc. AAMAS-08, 599606.
Maheswaran, R., Tambe, M., Bowring, E., Pearce, J., & Varakantham, P. (2004). Taking DCOP
real world: Efficient complete solutions distributed event scheduling. Proc. AAMAS04, 310317.
Meisels, A., Kaplansky, E., Razgon, I., & Zivan, R. (2002). Comparing performance distributed
constraints processing algorithms. Proc. 3rd DCR workshop AAMAS-02, 8693.
Modi, P. J., Shen, W.-M., Tambe, M., & Yokoo, M. (2005). ADOPT: Asynchronous distributed
constraint optimization quality guarantees. Artificial Intelligence, 161, 149180.
Petcu, A. (2007). class algorithms Distributed Constraint Optimization. Ph.D. thesis.
Petcu, A., & Faltings, B. (2005). scalable method multiagent constraint optimization. Proc.
IJCAI-05, 266271.
Ueda, S., Iwasaki, A., & Yokoo, M. (2010). Coalition structure generation based distributed
constraint optimization. Proc. AAAI-10, 197203.
Yeoh, W., Felner, A., & Koenig, S. (2010). BnB-ADOPT: asynchronous branch-and-bound
DCOP algorithm. Journal Artificial Intelligence Research, 38, 85133.
Yin, Z. (2008). USC DCOP repository..
Yokoo, M., Durfee, E., Ishida, T., & Kuwabara, K. (1998). distributed constraint satisfaction
problem: Formalization algorithms. IEEE Trans. Know. Data Engin., 10, 673685.

304

fiJournal Artificial Intelligence Research 45 (2012) 125-163

Submitted 05/12; published 09/12

Towards Unsupervised Learning Temporal
Relations Events
Seyed Abolghasem Mirroshandel
Gholamreza Ghassem-Sani

mirroshandel@ce.sharif.edu
sani@sharif.edu

Computer Engineering Department
Sharif University Technology
Azadi Avenue, Tehran 11155-9517, Iran

Abstract
Automatic extraction temporal relations event pairs important task
several natural language processing applications Question Answering, Information
Extraction, Summarization. Since existing methods supervised require
large corpora, many languages exist, concentrated efforts
reduce need annotated data much possible. paper presents two different
algorithms towards goal. first algorithm weakly supervised machine learning
approach classification temporal relations events. first stage,
algorithm learns general classifier annotated corpus. Then, inspired
hypothesis one type temporal relation per discourse, extracts useful information
cluster topically related documents. show combining global
information cluster local decisions general classifier, bootstrapping
cross-document classifier built extract temporal relations events.
experiments show without additional annotated data, accuracy proposed
algorithm higher several previous successful systems. second proposed
method temporal relation extraction based expectation maximization (EM)
algorithm. Within EM, used different techniques greedy best-first search
integer linear programming temporal inconsistency removal. think
experimental results EM based algorithm, first step toward fully unsupervised
temporal relation extraction method, encouraging.

1. Introduction
Much progress made natural language processing (NLP) recent years. Combining statistical symbolic methods played significant role advances.
result, tasks part-of-speech tagging (Sgaard, 2011), parsing (Petrov & Klein, 2007),
named entity recognition (Mikheev, Grover, & Moens, 1998) addressed
satisfactory results. However, tasks temporal information processing, need deeper analysis meaning, achieved results yet
satisfactory.
Temporal information encoded textual description events. Lately, increasing attention practical NLP applications question answering, summarization, information extraction resulted growing demand temporal information
processing (Tatu & Srikanth, 2008). question answering, one may expect system
answer questions event occurred, chronological order bec
2012
AI Access Foundation. rights reserved.

fiMirroshandel & Ghassem-Sani

tween desired events. text summarization, especially multi-document type,
knowing order events useful source correctly merging related information.
Construction TimeBank corpus 2003 (Pustejovsky et al., 2003), provided
opportunity applying different machine learning methods task temporal relation
extraction. However, realized even six-class classification temporal
relations complicated task, even human annotators (Mani, Verhagen, Wellner,
Lee, & Pustejovsky, 2006).
paper presents two different approaches need annotated data
temporal relation learning reduced. first approach weakly supervised machine
learning algorithm classification temporal relations events. first stage,
algorithm learns general classifier annotated corpus. Then, inspired
hypothesis one type temporal relation per discourse, extracts useful information
cluster topically related documents retraining model. combining
global information cluster local decisions general classifier, propose
novel bootstrapping cross-document classifier extract temporal relations events.
experiments show without additional annotated data, accuracy
proposed algorithm least 7% higher state-of-the-art statistical
methods (Chambers, Wang, & Jurafsky, 2007).
second introduced approach novel usage expectation maximization (EM) algorithm temporal relation learning. algorithm also employs Allens interval algebra
(Allen, 1984) correction predicted relations. applying interval algebra, utilize
two different approaches: 1) heuristic search method 2) integer linear programming
(ILP). think experimental results EM based algorithm, first step
toward fully unsupervised temporal relation extraction method, encouraging.
remainder paper organized follows: section 2 previous approaches temporal relation learning. Section 3 explains first proposed method,
evaluated section 4. second algorithm explained section 5, evaluated section 6. Finally, section 7 includes conclusions possible future
work.

2. Temporal Relation Learning
Assuming access texts events time expressions
appropriately tagged, two different tasks pertaining temporal relation learning distinguished: 1) detecting whether exist relation given pair events/time
expressions; 2) identifying relation type positive cases first task. first
task hard evaluate, annotators may ignore many plausible existing
relations tagging corpora (Mani et al., 2006). Accordingly, paper like
existing research, addressed second task, specifically
defined follows: given ordered pair components (x1 , x2 ), x1 x2
annotated events and/or time expressions, temporal relation classifier identifies type
relation ri temporally links x1 x2 . shown Figure 1, temporal relation one fourteen types proposed TimeML (Pustejovsky et al., 2003).
example, Powerful political pressures (event1 ) may convince (event2 ) Conservative
government keep (event3 ) so-called golden share, limits individual holding
126

fiUnsupervised Temporal Relation Learning Events


Figure 1: Different temporal relations TimeML.

15%, restriction (event4 ) expires (event5 ) Dec. 31, 1990 (time1 ). (taken
document wsj 0745 TimeBank, see Pustejovsky et al., 2003). task automatically tag relations pairs (event1 , event2 ), (event3 , event5 ), (event5 , event4 ),
(event5 , time1 ) BEFORE, ENDED BY, ENDS, INCLUDED, respectively
(see Figure 2). Since automatic extraction event-event relations difficult
task, paper, focused particular task, left detection
type temporal relations event-time time-time future work.
many ongoing research focusing temporal relation learning. Additionally,
two important shared tasks temporal information extraction: TempEval
2007 (Verhagen et al., 2007) TempEval 2010 (Verhagen, Sauri, Caselli, & Pustejovsky,
2010). TempEval 2007, three different tasks regarding temporal relations
classification A) events times within sentence; B) creation time
document events; C) main (verb) events adjacent sentences.
TempEval 2010, six different tasks including A, B) Determining time
expressions events input texts specified features; temporal relation classification C) events times within sentence; D) creation time document
events; E) main events consecutive sentences; F) two events one event
syntactically dominates event.
Due focusing temporal relations event pairs, task C TempEval 2007
plus tasks E F TempEval 2010 similar task tackle paper;
however, tasks considered special cases ours. instance, task E
TempEval 2010, event pairs consecutive sentences considered; whereas,
task event pairs either sentence two
sentences input text.
research temporal relation learning divided different categories.
paper, divide efforts three groups: 1) Statistical; 2) Rule-based, 3)
127

fiMirroshandel & Ghassem-Sani

Hybrid, explained following sections.


Figure 2: Temporal relations sentence Powerful political pressures (event1 ) may
convince (event2 ) Conservative government keep (event3 ) so-called
golden share, limits individual holding 15%, restriction
(event4 ) expires (event5 ) Dec. 31, 1990 (time1 ). Bold arrows show relations
event pairs.

2.1 Statistical Methods
statistical methods, classification (or clustering) algorithm employed number tagged and/or extracted features input corpus. Maximum Entropy (Mani,
Wellner, Verhagen, & Pustejovsky, 2007; Derczynski & Gaizauskas, 2010), Support Vector
Machines (Chambers et al., 2007; Bethard & Martin, 2007; Hepple, Setzer, & Gaizauskas,
2007; Cheng, Asahara, & Matsumoto, 2007; Mirroshandel, Ghassem-Sani, & Khayyamian,
2009a, 2009b, 2011), Conditional Random Fields (Llorens, Saquete, & Navarro, 2010; Kolya,
Ekbal, & Bandyopadhyay, 2010), Markov Logic Networks (UzZaman & Allen, 2010;
Ha, Baikadi, Licata, & Lester, 2010) statistical techniques
applied problem.
MaxEnt one first approaches temporal relation learning, uses
maximum entropy classification algorithm (Mani et al., 2007). method, classifier
assigns one six different temporal relation types event-event event-time pair.
classifier relies number features including modality, polarity, tense, aspect,
event class, hand-tagged corpus. addition features,
also relies pairwise agreement two additional features: tense aspect. later
propose new technique improve MaxEnt. results comparing proposed method
MaxEnt given section 4.
128

fiUnsupervised Temporal Relation Learning Events

USFD2 (Derczynski & Gaizauskas, 2010) another method employs maximum
entropy solving tasks C F TempEval 2010. method uses features
MaxEnt plus features related so-called signals text. USFD2 achieved
second highest score task C TempEval 2010. However, results task F
satisfactory enough.
state-of-the-art statistical methods analogous MaxEnt (Chambers et al.,
2007). works two consecutive stages employs event-event features addition
used MaxEnt. work, Support Vector Machines (SVM) used
classification. Similar results reported using Naive Bayes classier instead SVM.
Section 4 also includes results comparing work proposed algorithm.
SVMs also used classification algorithm several research. CUTMP (Bethard & Martin, 2007) applied SVM solving three tasks TempEval 2007.
also used gold-standard TimeBank features event time expressions plus parts
derived parse trees input text. CU-TMP first solves task B uses
results tackle tasks C.
USFD (Hepple et al., 2007) NAIST-Japan (Cheng et al., 2007) two
participants TempEval 2007 used SVM classification. NAIST-Japan,
task defined sequence labeling model. task approached using HMMSVM, relying features dependency-parsed trees standard attributes target
events/time expressions. result system slightly average tasks
B, less average task C. shown extracted features dependency
parsed trees effective task C, contrast tasks B. USFD,
temporal relation learning treated simple classification task (Hepple et al., 2007).
used different classification algorithm WEKA machine learning workbench (Hall
et al., 2009). task C, SVMs gained best result among participants
shared task.
another work, corpus parallel temporal causal relations employed,
SVMs used extract types relations (Bethard & Martin, 2008). Since existing
corpora provide parallel temporal causal annotations, 1000 conjoined event pairs
annotated (Bethard & Martin, 2008; Bethard, 2007). shown causal relation
information could helpful temporal relation extraction, too. also shown
temporal relation information mutually positive effects causal relation extraction
(Bethard & Martin, 2008).
Bethard colleagues (2007a, 2007b) applied SVM classify event pairs
first event verb second one head clausal argument
verb. used combination number event based features (e.g., tense
aspect) syntactic features (e.g., specific path parse tree).
reported results shown high accuracy specific event pairs.
also algorithms utilize grammatical information SVM using
convolution tree kernels (Mirroshandel et al., 2009a, 2009b, 2011). shown
grammatical aspects input text rich sources information temporal relation
classification. Argument Ancestor Path Distance (AAPD) convolution tree kernel
successful tree kernel used SVM classification. kernel similar
CollinsDuffy tree kernel (Collins & Duffy, 2001). CollinsDuffy kernel effectively
counts number common subtrees two comparing parse trees. kernel,
129

fiMirroshandel & Ghassem-Sani

subtrees importance, whereas AAPD, different weighting functions
used compute kernel value. Furthermore, AAPD, significance subtrees
measured using distance so-called argument ancestor path (AAP). AAP
ancestor nodes argument (event). example node (NN) distance
node AAP shown Figure 3. AAPD, closer node
path, less decayed weighting function. words, nodes
located nearer path important farther away.
improve accuracy AAPD, combined kernels,
either linear polynomial (Zhang, Zhang, Su, & Zhou, 2006). However, polynomial
composite kernels shown superior results (Mirroshandel et al., 2009b). Section 4
includes results comparing AAPD AAPD Polynomial kernels.


Figure 3: syntactic parse tree two argument ancestor paths events move
resigned plus distance node NN AAP resigned.

Markov Logic Networks (MLN) another classification algorithm,
used two participants TempEval 2010: TRIPS & TRIOS (UzZaman & Allen, 2010)
NCSU (Ha et al., 2010). TRIPS TRIOS use number features produced
deep semantic parser, plus features extracted target pairs (i.e., event/time
expression). contrast participants, TRIPS TRIOS operate raw texts.
words, systems use tagged events/time expressions.
outperformed teams two tasks (C E). TRIOS also gained second best results four remaining tasks. NCSU another participant TempEval 2010 uses
MLN classification. relies basic annotated features, syntactic features extracted
generated parse trees, lexical semantic features two external resources (Ver130

fiUnsupervised Temporal Relation Learning Events

bOcean WordNet) (Ha et al., 2010). NCSU applied tasks C, D, E, F
two different settings: NCSU-indi NCSU-joint. NCSU-indi, independent MLN
trained task. hand, set global formulae also added
NCSU-joint ensure consistency among classification decisions four local MLNs
(one task). NCSU-indi achieved best result task F second best result
task C.
One successful participants TempEval 2010 TIPSem based
Conditional Random Field (CRF) models classification purpose (Llorens et al., 2010).
TIPSem employs different morphological, syntactic, semantic features building CRF
models. Spanish, achieved best results tasks. English, TIPSem achieved
best results Tasks B D; one best systems tasks.
JU CSE TEMP another participant TempEval 2010 utilized CRF models
temporal relation learning tasks (Kolya et al., 2010). system needs goldstandard features TimeBank time expressions and/or events. comparison
TIPSem, JU CSE TEMP achieved weaker results, shows importance feature
engineering temporal relation learning.
another approach applies different machine learning techniques detect
intra-sentential events, builds corpus sentences two events
least one event triggered key time word (e.g., after, before, etc.). classifier
based number syntactic clausal ordering features (Lapata & Lascarides, 2006;
Bramsen, Deshpande, Lee, & Barzilay, 2006).
exist comprehensive study statistical methods, compares three
different interval based algebras terms classification accuracy, performance, expressiveness power (Denis & Muller, 2010). also algorithms exclusively
work temporal relation classification events time expressions. One
algorithms employs cascaded finite-state grammars (for temporal expression analysis, shallow syntactic parsing, feature generation) together machine learning component
capable effectively using large amounts unannotated data (Boguraev & Ando, 2005).
group statistical methods rely information argument fillers (called
anchors) every event expression valuable clue recognizing temporal relations.
methods, looking set event expressions whose argument fillers similar
distribution, analogous event expressions recognized. Algorithms DIRT (Lin &
Pantel, 2001), TE/ASE (Szpektor, Tanev, Dagan, & Coppola, 2004), Pekars
system (2006) examples type statistical method.
DIRT unsupervised method based extended version so-called distributional hypothesis (Lin & Pantel, 2001). According hypothesis, words occur
contexts usually similar. Here, instead words, algorithm applies
distributional hypothesis certain paths dependency trees parsed corpus.
TE/ASE, too, unsupervised algorithm, two major phases. first
phase (called Anchor Set Extraction), algorithm extracts similar anchors. Then,
second phase (called Template Extraction), system extracts templates resulting
anchor sets. final part algorithm, post-processing transformations
applied extracted templates remove inappropriate templates (Szpektor et al.,
2004).
131

fiMirroshandel & Ghassem-Sani

Pekars approach (2006), co-occurrence two verbs inside locally coherent text
used extract useful information. method three major steps. First, based
local discourse, identifies several pairs clauses related. Next, based
related clauses, tries create number templates verb pairs using
information syntactic behavior. last step, algorithm scores employs
templates relation extraction.
2.2 Rule-Based Methods
common idea behind rule-based methods find general patterns classifying
temporal relations. works, rules (patterns) manually defined.
Perhaps simplest rule-based method one developed using knowledge
resource called VerbOcean (Chklovski & Pantel, 2005). VerbOcean small number
manually designed generic rules. style rules form * <Verb-X> * <VerbY> *. example, rules Verb-X Verb-Y, Verb-X
eventually Verb-Y, Verb-X later Verb-Y happens-before relation
type; also rules Verb-X even Verb-Y Verb-Y least Verb-X
so-called strength relation type. manually creating rules, number
semantic relations (e.g., strength, antonymy, happens-before, etc.) events
detected. Several heuristics also employed filter inappropriate relations (Chklovski
& Pantel, 2005).
another rule-based method temporal relation learning focused biomedical
texts (Mulkar-Mehta, Hobbs, Liu, & Zhou, 2009). shown existing methods
temporal relation learning effective texts. work, specific
axioms (rules) used predict temporal causal relations. pattern extraction
algorithm employed create system rules semi-automatic manner.
XRCE-T, participant TempEval 2007, rule-based system relies syntactic
semantic features (e.g., deep syntactic analysis determination thematic roles)
(Hagege & Tannier, 2007). XRCE-T fact used post-processing module
general purpose linguistic analyzer.
another study, rules temporal transitivity used increase training set.
test accuracy enlarged corpus showed improvements (Mani et al., 2007).
Reasoning pre-determined rules another approach rules usage. work
Tatu Srikanth (2008), rich set axioms (rules) created used first
order logic based theorem prover find proof temporal relation refutation.
set discourse rules used algorithm Muller Tannier (2004)
establish possible relations every two consecutive events input text.
rules based tenses event verbs. classical path-consistency
algorithm (Allen, 1984) applied extracted relations first step.
2.3 Hybrid Methods
shown one increase accuracy temporal relation classifiers
merging discussed methods. example work Chambers Jurafsky
(2008), local decisions generated statistical method combined two types
implicit global rule-based properties. properties included transitivity rule (e.g.,
132

fiUnsupervised Temporal Relation Learning Events

B B C implies C), time expressions normalization (e.g., last
month yesterday). constraints used create densely-connected
network events, global state consistency enforced incorporating
constraints integer linear programming framework (Chambers & Jurafsky, 2008).
Integer linear programming local classifiers shown appropriate
cases number possible relations events restricted (Denis & Muller,
2011). suggested translation constraints temporal intervals endpoints used handle significantly smaller set constraints. translation,
temporal relations preserved. method shown rather high accuracy.
also proposed graph decomposition technique improve accuracy.
another algorithm, spiritually similar (Chambers & Jurafsky,
2008), instead applying global constraints using integer linear programming, so-called
Markov logic (ML) used (Yoshikawa, Riedel, Asahara, & Matsumoto, 2009). Global
constraints easily captured adding weighted first order logic formulas.
shown problem solved ML easily accurately
ILP.
WVALI another hybrid system, enhanced classification process using rules particular knowledge base (Puscasu, 2007). system, different
heuristics temporal reasoning mechanism combined statistical data extracted training corpus. WVALI achieved best results tasks TempEval
2007.
LCC-TE another hybrid system TempEval 2007. combined different machine
learning models human rules temporal relation learning (Min, Srikanth, & Fowler,
2007). LCC-TE uses gold-standard features available TimeBank, well number
derived extended features grammatical semantic features. evaluations
LCC-TE shown acceptable results three tasks.

3. Bootstrapped Cross-Document Classification (BCDC)
section, new method extracting temporal relations events introduced.
call method Bootstrapped Cross-Document Classification (BCDC). results
experiments BCDC show significant improvement previous work terms
accuracy (see Tables 6 7). used SVM three different kernels
learning process. two novelties bootstrapping (self-training) method: 1)
information retrieval based approach extracts useful information exclusively
related documents. 2) builds specific model test document. describing
BCDC, motivation briefly explained next section.
3.1 Motivation
regular corpus heterogeneous documents, verbs, often act event triggers,
may different senses different documents. example, event firing may
sense shooting gun document army, whereas may also sense
ending someones job different document company. However, cluster
topically-related documents, distribution much less divergent. motivated
us apply so-called one sense per discourse hypothesis (Yarowsky, 1995)
133

fiMirroshandel & Ghassem-Sani

problem temporal relation classification, extend scope discourse single
document cluster topically related documents. Also inspired another work
proposed assumptions one event trigger sense one event argument role per discourse
(Ji & Grishman, 2008), based work analogous assumption, called
one type temporal relation per discourse. words, assume similar event
pairs different places topically related documents likely
temporal relations. Although, later explained, explicitly employed
assumption proposed algorithm, tried verify assumption considering
temporal relations Opinion corpus (Mani et al., 2006). corpus, documents
located four different directories specific topic. verification,
considered documents within directory related. words, two
documents considered related documents directory (i.e.,
topic). verify assumption, selected event pairs
appeared once. Opinion corpus, total number 2666 temporally
related event pairs (i.e., TLinks), which, 994 pairs appeared once1 .
Table 1 shows results verification. Supporting samples event pairs
appeared two related documents exact temporal relation.
Even event pairs different relations unrelated documents, also
regarded supporting samples. contrary, event pairs different relations
related documents, considered contradictory samples. shown
Table 1, 95% samples supported assumption (i.e., one temporal
relation per discourse).
Supporting Samples
Contradictory Samples
Total

Count
942(95%)
52(5%)
994

Table 1: distribution assumption one type temporal relation per discourse
Opinion corpus.
example, following sentences, taken different documents topic (i.e., Kenya Tanzania Embassy bombings), event pair (blast
kill) IBEFORE temporal relation sentences:
Reports reaching said massive blast damaged U.S. embassy Nairobi ,
killing 40 people wounding least 1,000 people.
100 people killed 1,000 others wounded
blasts next U.S. embassies Kenya Tanzania Friday.
Dar es Salaam , laid wreath next crater left embassy blast
killed 10 people.
1. counting number event pairs, applied lemmatizer event words.

134

fiUnsupervised Temporal Relation Learning Events

3.2 Feature Engineering
BCDC, two types features used: basic extra event-event features. Basic
features simple features related individual events extra event-event features
extracted two related events. next two sections, features
explained detail.
3.2.1 Basic Features
simple features extracted events. event, five temporal
attributes, tagged standard corpora: 1) tense; 2) grammatical aspect; 3)
modality; 4) polarity, 5) event class. Tense aspect define temporal location
event structure; thus, necessary method temporal relation extraction.
Modality polarity specify non-occurring hypothetical situations. event class
shows type event. range values attributes based work
Pustejovsky et al. (2003), shown Table 2. attributes either annotated
input corpus automatically extracted existing tools.
Attribute
Tense
Aspect
Modality
Polarity
Event Class

Range values
none, present, past, future
none, prog, perfect, prog perfect
none, to, should, would, could, can, might
positive, negative
report, aspectual, state, state, action, perception, occurrence

Table 2: range values five temporal attributes.
addition five mentioned attributes, BCDC also employs string words
constitute event, part speech tags well number contextual features including pairwise agreement tenses aspects. Part speech tags events
either annotated corpora determined existing POS taggers.
example, sentence succeeds James A. Taylor, ..., succeeds event
following features:
[tense: present], [aspect: none], [modality: none], [polarity: positive], [event class: aspectual], [word: succeeds], [pos: verb]
3.2.2 Extra Event-Event Features
Extra event-event features based two related events automatically extracted
input text. case, three types features, defined follows:
Event-Event parse tree: events sentence, algorithm
use parse tree sentence learn useful syntactic properties domination. parse tree, event dominates event B, ancestor B. properties
explicit features, rather implicit properties extracted learned
SVM using appropriate tree kernels proposed work Mirroshan135

fiMirroshandel & Ghassem-Sani

del et al. (2009b). Parse trees extracted statistical parser need
Treebank.
Prepositional phrase: preposition head often indicator temporal class.
Thus, use new feature indicates event part prepositional phrase.
information also extracted parse trees. example, sentence
saw earthquake, relation events saw earthquake
easily determined word prepositional phrase earthquake.
Event-Event distance: based idea strength relationship
two events inversely related textual distance events. means
relationship becomes weaker distance increases (and vice versa). Accordingly, intra- inter-sentential events treated differently. train two separate
models: one intra-sentential events one inter-sentential ones.
3.3 Proposed Algorithm
BCDC applies novel usage bootstrapping classification temporal relations
events. works two main stages. first stage, using standard corpus,
general model learned. stage two, general model retrained test
document based related information. Figure 4 shows flowchart proposed
algorithm, described detail following sections.
3.3.1 Stage One
stage one, BCDC employs discussed features extracted standard corpus
train general model classification using SVM. end stage,
model temporal relation classification. However, models, also
proposed researchers, problem general.
words, model specific information particular domain
consideration. better deal problem, BCDC extra bootstrapping
phase training.
3.3.2 Stage Two
stage two, retrain general model produced stage one, test document
related information. order achieve goal, bootstrapping phase
BCDC proceeds according following steps:
Step 1: first all, unprocessed test document randomly selected.
Step 2: BCDC finds top N documents topically related selected test document large unannotated corpus. choice related documents
made INDRI retrieval system (Strohman, Metzler, Turtle, & Croft, 2005). Note
mentioned large unannotated corpus different training test corpora.

136

fiUnsupervised Temporal Relation Learning Events


Figure 4: flowchart BCDC.

137

fiMirroshandel & Ghassem-Sani

Step 3: step, extract events required features related documents
found INDRI. events specified features section 3.2 automatically annotated EVITA (Saur, Knippen, Verhagen, & Pustejovsky, 2005). Although
events and/or features extracted EVITA may incorrect, experimental results
show still helpful. extra event-event features required
features extracted POS tagger statistical parser.
Step 4: using existing model, temporal relations intrasentential event pairs related documents predicted. Besides, normalized measure
confidence computed relation. used SVM classification
purpose. Therefore, designed confidence measure using SVM, explained
below.
SVM binary classification, positive negative instances linearly partitioned
hyper-plane (with maximum marginal distance instances) original higher
dimensional feature space. order classify new instance X, distance hyperplane computed X assigned class corresponds sign computed distance. distance instance X hyper-plane H, either
positive negative value, supported support vectors X1 . . . Xl computed
equation 1 (Han & Kambert, 2006):
d(X, H) =

l
X

yi Xi X + b0

(1)

i=1

yi class label support vector Xk ; k b0 numeric parameters
automatically determined.
used one-versus-one case multi-class classification classes,
set (m 1) / 2 hyper-planes (i.e., one hyper-plane every class pair) denoted
H defined. hyper-plane separates class j referred Hi,j . Hi used
denote subset 1 hyper-planes H separates class others.
order classify new instance X, distance hyper-plane Hi,j computed.
X assigned class j. end process, every instance X, class
accumulated certain number votes, represented Vi (X), number
times classifier assigned instance X class i. final class X, denoted
C(X), one highest number votes.
process described above, easy compute confidence values based
distance measures equation 1 (i.e., closer case support vectors, less
confident). precisely, multi-class classification, define confidence
instance X sum distances class-separating hyper-planes:
fi
fi
fi X
fi
fi
fi
fi
(X) = fi
d(X, H)fifi
fiHHC(x)
fi

(2)

Based equation 2, larger value (X) shows X confident, vice
versa.

138

fiUnsupervised Temporal Relation Learning Events

Step 5: step, BCDC chooses K confident temporal relations
detected step 4.
Step 6: retrain SVM injecting temporal relations selected step
5. noted test document, original model retrained
confident relations documents related test document
test documents.
model trained original training data plus confident predicted
relations relevant documents current test document predicted relations test documents.
Steps 4-6 repeated one following two termination conditions satisfied: 1) unselected temporal relation, 2) predefined number
iterations reached.
Step 7: retraining phase general model selected test document
finished, temporal relations test document classified based new
specifically retrained model.
Then, still unprocessed test documents, BCDC start step 1
again; otherwise algorithm terminate.
fundamental idea second stage BCDC obtain document-
cluster-wide statistics temporal relations different types events,
using information improve temporal relation identification.
explained above, specific model learned test document, using
number unannotated text documents topically related test document
(i.e., bootstrapping phase). However, test documents topically related, corresponding retrained models similar. sake efficiency,
run bootstrapping phase one test documents, use
retrained model rest. words, run steps 2 6 BCDC one
member set similar test documents, members, solely apply step 7.
explained section 3.1, explicitly use assumption one
type temporal relation per discourse part BCDC. However, bootstrapping,
somehow implicitly benefit assumption seeking topically related documents, likely include similar event pairs identical temporal relations.

4. Experimental Results BCDC
section, specification employed corpora briefly explained. Then,
accuracy BCDC analyzed.
4.1 Characteristic Corpora
used two standard corpora (i.e., TimeBank (v 1.2) Opinion, see Mani et al.,
2006) experiments. TimeBank 183 newswire documents 64, 077 tokens,
139

fiMirroshandel & Ghassem-Sani

Opinion 73 documents 38, 709 tokens. two datasets annotated
based TimeML standard (Pustejovsky et al., 2003). mentioned before,
fourteen temporal relation types (SIMULTANEOUS, IDENTITY, BEFORE, AFTER,
IBEFORE, IAFTER, INCLUDES, INCLUDED, DURING, INV, BEGINS,
BEGUN BY, ENDS, ENDED BY) TLink class TimeML. sake reducing
data sparseness problem, many others (Mani et al., 2006; Tatu & Srikanth, 2008;
Mani et al., 2007; Chambers et al., 2007), used normalized version
relation types including six following relations:
SIMULTANEOUS


Original Relation
X
X IAFTER
X ENDED
X BEGUN
X INCLUDED
X
X IDENTITY
X INV

ENDS
IBEFORE

BEGINS
INCLUDES

Converted Relation
X
IBEFORE X
ENDS X
BEGINS X
INCLUDES X
INCLUDES X
X SIMULTANEOUS
X INCLUDES

Table 3: normalization process temporal relation types.
normalizing, inverse relations merged. conversions shown
Table 3. first six conversions, relations easily converted swapping
arguments. Relations IDENTITY SIMULTAENOUS collapsed, since IDENTITY
subtype SIMULTANEOUS (i.e., two events IDENTITY SIMULTANEOUS coreferential). Similarly, relations INV INCLUDES also
collapsed INV subtype INCLUDES (i.e., identical Allens
CONTAINS) based Allens interval algebra (Allen, 1984). clear
using conversions, information lost.
Relation Type
IBEFORE
BEGINS
ENDS
SIMULTANEOUS
INCLUDES

TOTAL

TimeBank
63
77
114
1304
588
1335 (38.35%)
3481

OTC
131
160
208
1528
950
3170 (51.57%)
6147

Table 4: normalized TLink class distribution TimeBank OTC.
140

fiUnsupervised Temporal Relation Learning Events

experiments, like previous work (Mani et al., 2006; Chambers et al., 2007;
Chambers & Jurafsky, 2008), TimeBank Opinion corpora merged single
corpus called Opinion TimeBank Corpus (OTC). Table 4 shows normalized TLink class
distribution (only Event-Event relations) TimeBank OTC. shown,
relation frequent relation; thus forms majority class,
used baseline experiments.
comparison methods, also used English part TempEval2 corpus. part based TimeBank (Verhagen et al., 2010; Pustejovsky et al., 2003;
Boguraev, Pustejovsky, Ando, & Verhagen, 2007). However, TimeBank annotations
reviewed based guidelines TempEval 2010 temporal relations
modified according specific types shared task.
Relation Type


OVERLAP
BEFORE-OR-OVERLAP
OVERLAP-OR-AFTER
VAGUE
TOTAL

Task E
Training
403
279
652 (41.19%) 124
61
51
137
1583

Test
58
38
(48.63%)
9
6
20
255

Task F
Training
Test
600 (35.46%)
92
299
45
518
100 (33%)
111
55
89
11
75
0
1692
303

Table 5: distribution temporal relation types TempEval-2 Corpus Task E
F.

two parts corpus: 1) training part including 163 documents
53, 450 tokens; 2) test part 21 documents 4, 848 tokens. six different temporal relation types: BEFORE, AFTER, OVERLAP, BEFORE-OR-OVERLAP,
OVERLAP-OR-AFTER, VAGUE. Among six different tasks TempEval 2010,
focused tasks E F, similar problem tackled
paper. Tasks E F tasks consider exclusively relations
two events. distribution temporal relation types tasks training
test parts corpus shown Table 5. majority classes underlined
table.
discussed section 3.3.2, text, retrieve number topically
related texts using public domain software called INDRI. experiments, related
texts retrieved English part TDT5 multilingual news text corpus2 .
total, TDT5 consists 407, 505 text documents English (278, 109 documents), Mandarin
Chinese (56, 486 documents), modern standard Arabic (72, 910 documents). also
250 different topics. Unlike previous TDT corpora, TDT5 contain broadcast
news data; sources newswires.
2. TDT 2004: Annotation Manual, Available http://www.ldc.upenn.edu/Projects/TDT2004.

141

fiMirroshandel & Ghassem-Sani

4.2 Experiments
used LIBSVM java source SVM classification (Chang & Lin, 2011).
EVITA system (Saur et al., 2005) used event extraction. EVITA works
based linguistic statistical information. addition event extraction, event
attributes (which described Table 2) also extracted EVITA. also
used Stanford NLP package3 tokenization, sentence segmentation, part speech
tagging, parsing. INDRI retrieval system (Strohman et al., 2005) employed obtain related documents. INDRI language model based search engine
provides state-of-the-art text search engine. English part TDT5 indexed
INDRI, using search engine, texts highly related specified
documents retrieved.
mentioned earlier, applied algorithm TimeBank, OTC, TempEval 2010 Corpora. randomly selected 20 documents (almost 10 percent total
documents) TimeBank development set. Based several experiments development set different number extracted related documents step 2 BCDC
(i.e., N), number confident relations chosen step 5 (i.e., K), set N
25 K 40.
TimeBank OTC, results evaluated first excluding 20 documents
development set measuring accuracy using five-fold cross validation
method. However, corpus TempEval-2, need cross validation,
training test sets predetermined, reported accuracy
BCDC test set.
Table 6 shows results three different settings proposed algorithm
several others TimeBank OTC. table, baseline majority class
event-event relations (i.e., relation) evaluated corpora. Manis method
regarded successful statistical approach temporal relation identification,
exclusively uses gold standard features events (Mani et al., 2007). Methods proposed
Chambers Mani similar except Chambers also used number extra
features two step algorithm. method currently regarded state-of-the-art
statistical approaches TimeBank OTC. achieve higher accuracy, also
used extra resources WordNet (Chambers et al., 2007).
Argument ancestor path distance (AAPD) accurate convolution tree kernel
uses parse trees event-event sentences temporal relation classification (Mirroshandel et al., 2009b). AAPD polynomial composite kernel combines simple event
kernel AAPD (Mirroshandel et al., 2009b). mentioned simple event kernel linear kernel exclusively uses features Manis method (Mirroshandel
et al., 2009b). AAPD AAPD polynomial kernels designed applied
event pairs within sentence. Accordingly, relations TimeBank
OTC split two parts: 1) relations intra-sentential event pairs,
2) relations inter-sentential event pairs. kernels applied
first part second part, used simple event kernel (i.e., Manis kernel).
Table 6, results reported AAPD AAPD polynomial kernel fact
outcome merging partial results two parts.
3. Available http://nlp.stanford.edu/software/index.shtml

142

fiUnsupervised Temporal Relation Learning Events

Method
Baseline
Chambers
Mani (Event Kernel + Basic Features)
Classic Bootstrapping + Event Kernel + Basic Features
BCDC + Event Kernel + Basic Features
AAPD Kernel + Extra Event-Event Features
Classic Bootstrapping + AAPD Kernel + Extra Event-Event
Features
BCDC + AAPD Kernel + Extra Event-Event Features
AAPD Polynomial Kernel + Basic Features + Extra EventEvent Features
Classic Bootstrapping + AAPD Polynomial Kernel + Basic
Features + Extra Event-Event Features
BCDC + AAPD Polynomial Kernel + Basic Features +
Extra Event-Event Features

TimeBank
Corpus
38.35
59.43
50.97
53.21
59.71
54
57.98

OTC
Corpus
51.57
65.48
62.5
63.12
65.19
63.44
64.53

62.56
57.02

66.29
65.95

59.83

66.55

66.18

68.07

Table 6: accuracy proposed methods event-event temporal relation classification
TimeBank OTC.

BCDC + Event Kernel + Basic Features bootstrapped algorithm,
uses basic features, mentioned section 3.2.1, applying simple event kernel (Mirroshandel et al., 2011). BCDC + AAPD Kernel + Extra Event-Event Features,
utilized extra event-event features AAPD kernel. Third setting (BCDC + AAPD Polynomial Kernel + Basic Features +Extra Event-Event Features) uses AAPD Polynomial
kernel combine basic extra event-event features.
better comparison, also applied classic bootstrapping method
SVM kernels features BCDC. reporting results, initial model
trained standard corpus (i.e., like stage one BCDC). Then, iterative manner,
confident samples documents (rather related documents) used
retrain model. Note case, need process retrieving related
documents, one model learned test documents. order find best
value K (i.e., number confident samples) classic bootstrapping method,
performed several different experiments mentioned development set. Incidentally,
experiments showed here, too, K set 40.
Table 6 indicates, BCDC + AAPD Kernel + Extra Event-Event Features
BCDC + AAPD Polynomial Kernel + Basic Features + Extra Event-Event Features
show significant improvement state-of-the-art method (i.e., Chambers method).
Comparison BCDC classical bootstrapping shows effectiveness proposed idea extracting retraining samples related documents.
improvement TimeBank considerable OTC. seems
different distributions temporal relations two corpora caused difference
143

fiMirroshandel & Ghassem-Sani

improvements. shown Table 4, OTC, majority class (i.e.,
relation) larger part whole corpus. causes learning algorithm
become biased towards relation, thus correct prediction
relations becomes harder. contrary, TimeBank, distribution less biased
thus BCDC shown improvement corpus.
Method 1

Method 2

BCDC + Event Kernel + Basic Features
BCDC + AAPD Kernel + Extra Event-Event Features
BCDC + AAPD Polynomial
Kernel + Basic Features +
Extra Event-Event Features
BCDC + Event Kernel + Basic Features

Mani (Event Kernel + Basic
Features)
AAPD Kernel + Extra EventEvent Features
AAPD Polynomial Kernel
+ Basic Features + Extra
Event-Event Features
Classic Bootstrapping +
Event Kernel + Basic Features
Classic Bootstrapping +
AAPD Kernel + Extra
Event-Event Features
Classic Bootstrapping +
AAPD Polynomial Kernel
+ Basic Features + Extra
Event-Event Features
Chambers

BCDC + AAPD Kernel + Extra Event-Event Features
BCDC + AAPD Polynomial
Kernel + Basic Features +
Extra Event-Event Features
BCDC + Event Kernel + Basic Features
BCDC + AAPD Kernel + Extra Event-Event Features
BCDC + AAPD Polynomial
Kernel + Basic Features +
Extra Event-Event Features

P-Value
TimeBank
0.0134

P-Value
OTC

0.0296

0.0383

0.0212

0.0422

0.0311

0.0514

0.0299

0.0441

0.0402

0.0491

0.0765

0.0878

Chambers

0.0503

0.0487

Chambers

0.0431

0.0397

0.0476

Table 7: statistical significance test results proposed methods.
testing statistical significance, applied type stratified shuffling,
kind compute-intensive randomized test. null hypothesis (i.e., two models
produced observed results same) tested randomly shuffling generated
output event pair two models re-computing evaluation
metrics (i.e., accuracy case). difference particular metric shuffling
equal greater original observed difference metric, counter (nc)
metric incremented. Ideally, perform 2n possible shuffles,
n shows number test cases (i.e., event pairs). But, case, impractical
n rather large number. Therefore, many others, tried 10, 000
144

fiUnsupervised Temporal Relation Learning Events

iterations (nt). finishing iterations, p-value (likelihood incorrectly rejecting
null hypothesis) simply calculated (nc + 1)/(nt + 1) (Yeh, 2000). Table 7 shows
result significance test proposed methods. test, proposed method
compared relevant method.
shown Table 7, majority methods passed test (the p-value less
0.05). four exceptions p-value slightly greater 0.05.
Table 8 shows accuracy BCDC English part corpus used TempEval
2010 tasks E F. JU-CSE, NCSU-indi, NCSU-joint, TIPSem, TIPSem-B, TRIOS,
TRIPS participants TempEval 2010 shared task (Verhagen et al., 2010).
methods Table 6. AAPD kernel applied event pairs
within sentence. Therefore, unable apply task E. inter-sentential
event pairs, AAPD Polynomial kernel almost similar simple event kernel,
cannot use syntactic parse trees, appropriate sources information.
Method
Baseline
JU-CSE
NCSU-indi
NCSU-joint
TIPSem
TIPSem-B
TRIOS
TRIPS
Event Kernel + Basic Features
BCDC + Event Kernel + Basic Features
AAPD Kernel + Extra Event-Event Features
BCDC + AAPD Kernel + Extra Event-Event Features
AAPD Polynomial Kernel + Basic Features + Extra
Event-Event Features
BCDC + AAPD Polynomial Kernel + Basic Features
+ Extra Event-Event Features

Task E
49
56
48
51
55
55
56
58
38.02
44.35


38.54

Task F
33
56
66
25
59
60
60
59
33.23
38.44
40.71
47.20
43.51

45.62

50.41

Table 8: accuracy proposed methods tasks E F TempEval 2010 shared
task.
seen Table 8, although BCDC shown improvement
accuracy temporal relation identification (i.e., comparison base methods),
generally weaker almost participants TempEval 2010. think
weakness due restricted feature set used BCDC. words,
majority participants TempEval 2010 used richer feature sets different levels
(e.g., lexical, syntactic, semantic), used simple event features (plus
syntactic features task F). think verified yet
richer set features, BCDC produce successful results TempEvals tasks, too.
Besides, think replacing base method successful method TipSem
145

fiMirroshandel & Ghassem-Sani

TRIPS, make BCDC competitive participants TempEval 2010. However,
show this, first need find appropriate confidence measure step 4 BCDC4 ,
requires investigation one directions future research.


Figure 5: effects utilizing related documents vs. randomly selected documents
accuracy BCDC TimeBank OTC.

4.3 Analysis
seen Tables 6 8, BCDC shown substantial improvement
several different methods terms accuracy without using extra annotated data.
Bootstrapping using number related documents following positive effects:
1) knowing relation events, better predict relation types
analogous events, may appear related documents.
2) related documents, number sentences similar patterns increase,
tree kernels extract confident information parse trees. Thus
4. noted proposed confidence measure useful SVM classification technique.

146

fiUnsupervised Temporal Relation Learning Events

way, SVM informative.
3) used corpora rather small examples relation. data
sparseness problem affect performance temporal relation identification method.
BCDC, retrieving related documents extraction new temporal relations
documents increase number relations improve performance alleviating
data sparseness problem.
One remaining question impact choosing related documents?.
words, randomly choose number unrelated documents bootstrapping
phase BCDC. show effectiveness idea using related documents,
repeated experiments N = 25 (i.e., original BCDC) randomly selected
documents. results experiments shown Figure 5. shown,
although randomly selected documents slightly improved base methods, however,
improvement comparable using related documents.

5. Using EM Temporal Relation Learning (EMTRL)
Since supervised even semi-supervised methods need annotated corpora, many
languages and/or domains exist, here, propose unsupervised algorithm
temporal relation learning problem. Due encouraging results expectation
maximization (EM) algorithm unsupervised tasks natural language processing
unsupervised grammar induction (Klein, 2005), unsupervised anaphora resolution
(Cherry & Bergsma, 2005; Charniak & Elsner, 2009), unsupervised coreference resolution (Ng, 2008), decided evaluate EM unsupervised temporal relation extraction.
Currently, reported work temporal relation extraction based EM. fact,
yet attempt towards unsupervised approach temporal relation
extraction. Here, explain EM successfully applied task temporal
relation extraction show performance EM encouraging task.
that, first introduce definitions notations later used subsequent
sections.
5.1 EM Algorithm
EM general algorithm maximum likelihood estimation (MLE) (Dempster, Laird, &
Rubin, 1977). algorithm used deal incomplete information.
mentioned before, temporal relation learning, task determine type
temporal relation r two events e1 e2 . algorithm, context means
sentence (or sentences) containing pair events.
5.2 Proposed Model
Let us call new proposed algorithm EMTRL, stands EM based temporal
relation learning. EMTRL operates corpus level, inducing valid temporal clustering
event pairs given corpus. specifically, EMTRL induces probability
distribution maximize P (corpus) (the probability corpus). easily incorporate
147

fiMirroshandel & Ghassem-Sani

linguistic constraints, corpus represented event pairs (ei ej ). assume event pairs
independent:


P (corpus) =

P (ei ej )

(3)

ei ej corpus

rewrite P (ei ej ) uses hidden variable Ci
pair ei ej ) influences observed variables (ei ej ):
X

P (ei ej ) =
Ci

j

j

(temporal class event

P (ei ej , Ci j )

(4)

possible temporal classes

probability P (ei ej , Ci j ) rewritten as:
P (ei ej , Ci j ) = P (ei ej | Ci j )P (T Ci j )

(5)

inducing temporal relations, EMTRL runs EM model. use uniform
distribution P (T Ci j ). clear could choose informative prior
distribution P (T Ci j ), would benefits like better handle skewness distribution. applications EM, settings prior
distribution. However, problem temporal relation learning, cannot
prior distribution except uniform distribution; here, temporal relation
types would seem equal learner.
expand equation 5, pair ei ej represented features,
potentially used determining temporal relation type events ei ej .
Therefore, P (ei ej | Ci j ) rewritten using equation 6:
P (ei ej | Ci j ) = P (ei e1j , ei e2j , ... ei ekj | Ci j )

(6)

ei elj value lth feature ei ej . features, similar
mentioned work Chambers Jurafsky (2008), listed Table 9.
reduce data sparseness problem improve probability estimation, conditional independence assumed features value generation. assume
tense aspect dependent (i.e., tensei aspecti ), tense aspect define
temporal location event structure, thus considering features together
rich source information temporal relation extraction system. conditional
independence assumption, value P (ei ej | Ci j ) rewritten as:
P (ei ej | Ci j ) =



P (ei elj | Ci j )

(7)

f eatures l

probabilities (i.e. P (ei elj | Ci j ) ) regarded parameters proposed
model. using them, likelihood different temporal classes determined.
Based features Table 9 different temporal classes, P (ei elj | Ci j )
defined. Four examples probabilities shown below:
P (class(ei ) = OCCU RREN CE class(ej ) = P ERCEP ION | Ci j =
BEF ORE)

148

fiUnsupervised Temporal Relation Learning Events

Feature
W ord1 & W ord2
Lemma1 & Lemma2
Synset1 & Synset2
P OS1 & P OS2
Event Government V erb1 &
V erb2
Event Government V erb1 &
V erb2 POS
Auxiliary
Class1 & Class2
ense1 & ense2
Aspect1 & Aspect2
odality1 & odality2
P olarity1 & P olarity2
Tense Match
Aspect Match
Class Match
Tense Pair
Aspect Pair
Class Pair
POS pair
P reposition1
P reposition2
Text order
Dominates
Entity Match

Description
text first second events
lemmatized first second events heads
WordNet synset first second events heads
POS first second events
verbs govern first second events
verbs POS govern first second events
auxiliary adverbs verbs modifies governing verbs
Class first second events
tense first second events
aspect first second events
modality first second events
polarity first second events
two events tense
two events aspect
two events class
Pair two events tense
Pair two events aspect
Pair two events class
Pair two events POS
first event prepositional phrase
second event prepositional phrase
first event occurs first document
first event syntactically dominates second event

entity argument shared two
events

Table 9: features events used EMTRL temporal relation learning.

149

fiMirroshandel & Ghassem-Sani

P (ei dominates ej | Ci

j

= AF ER )

P (T ense(ei ) = P AST ense(ej ) = P AST Aspect(ei )
N E Aspect(ej ) = P ROGRESSIV E | Ci j = OV ERLAP )
P (P OS ei = V P OS ej = N | Ci

j

=

= AF ER )

5.3 Induction Algorithm
induce temporal clustering corpus, EM applied proposed model.
EMTRL, corpus (i.e., event pairs) temporal clustering C respectively
observed unobserved (the hidden) random variables. EM algorithm includes two
main steps expectation (E) maximization (M), task defined
following way iteratively estimate parameters model (i.e., P (ei elj | Ci j )):
E-step: Fix current parameters model, assign probability, P (T Ci j | ei ej ),
possible temporal class event pairs (ei ej ) corpus. probability
computed following equation:
P (ei ej , Ci j )
P (ei ej )
rewrite equation 8 using equations 4, 5, 7:
P (T Ci

P (T Ci

j

j

| ei ej ) =

l
f eatures l P (ei ej | Ci j )
Q
0
possible temporal classes P (T Ci j ) f eatures l P (ei

P (T Ci j )

| ei ej ) = P

(8)

Q

elj | Ci0 j )
(9)
Using equation 9, event pair (ei ej ), temporal relation type (temporal class)
highest probability selected. relations later used M-step
update parameters model.
Ci0

j

M-step: fixing determined temporal relations E-step, parameters
model, P (ei elj | Ci j ), updated step. achieving goal, different optimization algorithms conjugate gradient used. However, algorithms
slow costly. addition, difficult smooth methods desired manner.
Therefore, used relative frequency method re-estimation parameters,
using equation 10:
P (ei elj | Ci j ) =

N (ei elj , Ci j )
N (T Ci j )

(10)

N () counts number times given items joint items appeared
corpus. example, updating probability P (ei dominates ej | Ci j = AF ER)
done dividing N (ei dominates ej , Ci j = AF ER) (i.e., number times
ei dominates ej temporal relation ei ej AFTER) N (T Ci j =
AF ER) (i.e., number times relation event pairs corpus AFTER).
150

fiUnsupervised Temporal Relation Learning Events

Steps E repeated one following termination conditions
satisfied: 1) predefined number iterations reached, 2)
changes P (ei elj | Ci j ). practice, EMTRL usually stopped 30 predefined
iterations, final behavior apparent 15 22 iterations.
finishing training phase, temporal relation (T Ci j ) requested event
pairs ei ej determined using following equation:
Ci

j

= arg maxT C 0

possible temporal classes P (T C

0

| ei ej )

(11)

Now, EM algorithm begin either E-Step M-step. start
induction algorithm M-step. clear parameters model available
first iteration EM. Instead, initial distribution temporal clustering
used. important question: one initialize distribution?
Initialization important task EM, EM guarantees find local
maximum likelihood. quality local maxima highly dependent
initial starting point. tested three different ways initialization:
1) Random Initialization: uniform distribution temporal clustering
used; therefore, temporal clustering first step equal probability.
2) 10% Supervised Initialization: used small part labeled corpus (10%
relation type) task. Relations selected randomly.
3) Rule-based Initialization: used specific rules initial estimation temporal relation types used initial estimation computing parameters model.
rules combination so-called GTag rules (Mani et al., 2006), VerbOcean (Chklovski & Pantel, 2005), rules derived certain signal words (e.g., on,
during, when, if) text. GTag contains 187 syntactic lexical rules
inferring labeling temporal relations event, document time, time expressions. rules, 169 event pairs, utilized EMTRL.
169 rules either event pairs sentence two main
events two consecutive sentences. example GTag rule shown below; rules
accessible Blinker part TARSQI toolkit5 .
conjBetweenEvents = ES &&
isT heSameSentence = RU E &&
event1 .class = (OCCU RREN CE|P ERCEP ION |ASP ECT U AL|I ACT ION ) &&
event2 .class = ST E &&
event1 .tense = P AST &&
event2 .tense = P AST &&
event1 .aspect = N E &&
event2 .aspect = P ERF ECT &&
event1 .pos = V ERB &&
event2 .pos = V ERB
5. Available http://www.timeml.org/site/tarsqi/index.html

151

fiMirroshandel & Ghassem-Sani


relation(event1 , event2 ) = AF ER

VerbOcean contains lexical rules two verbs, mined using
lexical syntactic patterns. relation verb pairs one different semantic relations strength, enablement, antonymy, similarity, happens-before.
extracted 4, 205 happens-before rules VerbOcean. Two examples rules
shown below:
announce [happens-before] postpone :: 12.844086
review [happens-before] recommend :: 9.049530

rule contains two verbs, relation, strength value relationship.
example, second rule shows relation happens-before review recommend strength 9.049530. also designed 23 rules based signal
words before, on, when. rules GTag format. example
group rules given below:
isT heSameSentence = rue &&
signal = bef ore &&
signalBetweenT woEvents = rue

relation(event1 , event2 ) = bef ore

Like many statistical NLP tasks, smoothing vital alleviate problem
data sparseness. particular, first iterations, much smoothing required
later iterations. experiments, used simply add-1 smoothing technique
computing equation 10.

6. Experimental Results EMTRL
Like experiments BCDC, TimeBank OTC also used experiments
EMTRL. However, order simplify task, used different normalized version
corpora, contained three following temporal relations:




OVERLAP

main reason simplification EMTRL reducing level supervision task temporal relation learning makes even difficult task,
already considered hard one (Mani et al., 2006). normalize corpora
reduce number relation types three, adopted normalization approach like previous work (Bethard et al., 2007b), IBEFORE relations
merged relations. Similarly, IAFTER relations
also merged relations. remaining ten relation types
collapsed OVERLAP relations. Table 10 shows converted TLink class distribution
TimeBank OTC.
152

fiUnsupervised Temporal Relation Learning Events

Relation Type


OVERLAP
Total

TimeBank Corpus
706
692
2083 (59.83 %)
3481

OTC Corpus
2369
1073
2792 (44.79 %)
6234

Table 10: converted TLink class distribution TimeBank OTC.
Beside TimeBank OTC, performance EMTRL also evaluated
tasks E F TempEval-2 corpus. tasks relations distribution
shown Table 5.
6.1 Results Discussions
experiments, baselines majority class event pair relations
employed corpora (i.e., OV ERLAP corpora). Note Manis method
fact supervised, exclusively uses gold-standard features (Mani et al., 2007).
Chambers method similar Manis, except also uses external resources
WordNet (Chambers et al., 2007). Here, result implementation Mani
Chambers methods different reported results, because, explained
before, considered three temporal relation types reported experiments,
six relation types.
Table 11, addition results employing EMTRL three different initializations, also reported results initializations stand-alone classifiers.
Random Initialization EMTRL + Random Initialization, question may arise
methods determine label different classes. fact, methods
distinguish three different classes (Class1 , Class2 , Class3 ). Among different possible ways unlabeled classes mapped BEF ORE, AF ER,
OV ERLAP , choose mapping similarity predicted annotated temporal relations maximized.
Method Type
Baseline
Mani
Chambers
Random Initialization
EMTRL + Random Initialization
10% Supervised Initialization
EMTRL + 10% Supervised Initialization
Rule-based Initialization
EMTRL + Rule-based Initialization

TimeBank Corpus
59.83
61.55
66.79
35.99
40.92
39.33
48.31
38.92
47.86

OTC Corpus
44.79
60.58
62.94
37.29
43.02
41.14
49.34
41.03
48.78

Table 11: accuracy results different methods TimeBank OTC.
153

fiMirroshandel & Ghassem-Sani

Considering unsupervised nature EMTRL, results Table 11 encouraging. shown table, TimeBanks baseline well OTC.
TimeBank highly biased towards OVERLAP. Accordingly, difficult
learning methods pass baseline TimeBank. performance Manis
method, fully supervised approach, slightly baseline.
case, EMTRLs accuracy considerably baseline. However, case OTC,
performance passed baseline.
shown Table 11, EMTRLs accuracy three different initializations,
respectively superior stand-alone counterparts. statistical
significance results table shows superiority EMTRL
baseline (i.e., case OTC) stand-alone initializations (i.e., corpora)
verified stratified shuffling test significance level = 0.05.
Table 11 shows best accuracy belongs Chambers method. However,
noted method currently best-reported results TimeBank
OTC among supervised temporal relation extraction methods.
Table 11 also shows EMTRL + Randomized Initialization efficient
either corpora. may due fact randomized initialization hard
problem causes divergence probability distribution. hand, two
initializations shown satisfactory results tackling problem. implies
initialization critical factor EMTRL, even little source supervision
crucial achieving satisfactory results.
Method Type
Baseline
NCSU-indi
TRIPS
Random Initialization
EMTRL + Random Initialization
10% Supervised Initialization
EMTRL + 10% Supervised Initialization
Rule-based Initialization
EMTRL+ Rule-based Initialization

Task E
49
48
58
24.75
27.92
26.35
32.20
27.17
32.76

Task F
33
66
59
19.11
22.33
23.77
26.29
23.64
27.03

Table 12: accuracy results different methods tasks E F TempEval-2
Corpus.

Table 12 shows results applying EMTRL corpus TempEval 2010.
comparison accuracy kernels Table 8, EMTRL could achieve encouraging
results. case, EMTRLs accuracy three different initializations, also
respectively superior stand-alone counterparts. TRIPS NCSU-indi
successful supervised systems tasks E F TempEval 2010, respectively
(Verhagen et al., 2010).
154

fiUnsupervised Temporal Relation Learning Events

6.2 Inconsistency Removal
Since pair-wise relation learning system, relation pair events
predicted without considering impact relations event pairs, system may
encounter inconsistencies among predicted relations. may happen selecting
temporal relations equation 9 E-step. also happen finding final class labels
equation 11. Figure 6 shows example inconsistent relation events A,
B, C:



Event
B

fter B

B fter C

Event

Event C
B efore C

Figure 6: contradiction temporal relations three events A, B, C.

several ways eliminating inconsistencies (Mani et al., 2007; Tatu &
Srikanth, 2008; Chambers & Jurafsky, 2008). work, used two different
approaches: greedy best-first search strategy Integer Linear Programming (ILP)
based method. details approaches given next.

6.2.1 Greedy Best-First Search Strategy
order detect possible inconsistencies predicted relations, first build graph
text, node corresponds event, edge represents temporal
relation corresponding events. Then, existing contradiction among connected
nodes graph discovered applying set rules (i.e., 640 rules) based
Allens interval algebra (Allen, 1984). example, consider following three rules:
bef ore(x, y) && bef ore(y, z)
bef ore(x, z)
af ter(x, y) && bef ore(z, y)
af ter(x, z)
af ter(x, y) && includes(y, z)
af ter(x, z)
inconsistent relations graph stored sorted list named SL, based
computed confidence score (i.e., P (T Ci j | ei ej ) equation 9). Thus, SL, first
last elements least confident relations, respectively.
algorithm starts first relation SL, pops relation adds
another list named F L. adding new relation F L, algorithm verifies
consistency among relations F L. new relation introduces inconsistency,
155

fiMirroshandel & Ghassem-Sani

replaced next confident relation corresponding events. replacement
may repeated new relation consistent relations existing
F L. contradictions F L, algorithm move next
element SL F L. operations iterated remain relations
SL. resultant consistent relations F L used subsequent M-step
final result EM.
6.2.2 Integer Linear Programming (ILP)
second approach, cast task finding probable temporal relations
optimization problem. contrast previous method, approach, based
integer linear programming (ILP) framework, finds optimal solution based
parameters model, P (ei elj | Ci j ). method similar Chambers
Jurafsky (2008).
ILP framework, event pair (ei , ej ), relation type ei
ej denoted Ri jM . objective function framework defined follows:
!

max

X X
j >

X

(Pi

jM

Ri

jM

+ Pj

iM

Rj

iM )

(12)



(Pi jM (i.e., P (M | ei ej ) equation 8) probability temporal relation
type ei ej . objective function maximizes sum probabilities
temporal relations event pairs input text. also three following
constraints (i.e., 13, 14, 15) objective function:
j M, > j : Ri

jM ,

Constraint 13 implies Ri

jM

X

(T Ri

j, > j :

Rj

iM

{0, 1}

(13)

variable either zero one.
jM

+ Rj

iM )

=1

(14)



Constraint 14 ensures pair events (ei ej ), one Ri jM
variable set one, rest set zero. words, impossible pair
events two (or more) relations.
Ri

jM 1

+ Rj

kM 2

Ri

kM 3

1

(15)

Constraint 15 guarantees transitivity conditions among event pairs, wherever relations Ri jM 1 Rj kM 2 entail relation Ri kM 3 . obvious transitivity
constraint effective event pairs connected one another. disconnected graph, constraint little effect. example, Figure 6, considering
constraint relations RA BAf ter RB CAf ter , RA CAf ter possible
relation events C.
generating set constraints document, use ILP solver
(SCIP6 ) solve problem. One important issue ILP technique
6. ILP solver fastest existing noncommercial mixed integer programming solver. Available
http://scip.zib.de/

156

fiUnsupervised Temporal Relation Learning Events

effective dense temporal graphs sparse ones.
removing contradictions temporal relations, generated (consistent) relations
easily used updating probabilities model M-step. results
Tables 11 12 without applying greedy best-first search ILP. accuracy
results greedy ILP algorithms TimeBank OTC shown Table
13. Table 14 shows accuracy results tasks E F corpus TempEval
2010. One question may arise enforce transitivity constraints EM,
labels Class1 , Class2 , Class3 , rather BEFORE, AFTER,
OVERLAP. problem happens case EMTRL + Random Initialization,
used prior assignment Class1 = AF ER, Class2 = BEF ORE,
Class3 = OV ERLAP . two initializations (i.e., 10% Supervised
Rule-base), problem occur, algorithm starts actual class
labels BEFORE, AFTER, OVERLAP.
Method Type

EMTRL + Random Initialization
EMTRL + 10% Supervised
Initialization
EMTRL + Rule-based Initialization

TimeBank
Base
Greedy
Method
40.92
41.09

41.03

Base
Method
43.02

48.31

49.54

50.34

47.86

50.88

52.12

ILP

OTC
Greedy

ILP

42.94

43.00

49.34

50.52

51.44

48.78

49.98

51.17

Table 13: accuracy results applying greedy best-first search strategy ILP
TimeBank OTC.

Method Type

EMTRL + Random Initialization
EMTRL + 10% Supervised
Initialization
EMTRL + Rule-based Initialization

Task E
Greedy

ILP

28.03

28.04

Base
Method
22.33

32.20

33.54

33.92

32.76

33.49

34.12

Base
Method
27.92

Task F
Greedy

ILP

22.32

22.30

26.29

27.94

27.92

27.03

28.36

28.55

Table 14: accuracy results applying greedy best-first strategy ILP tasks
E F TempEval 2010.

157

fiMirroshandel & Ghassem-Sani

Tables 13 14 show impact utilizing greedy best first search ILP
approaches EMTRL base method. using strategies,
inconsistencies may exist among predicted temporal relations, removed (in step
E EMTRL) make predicted relations reliable. result, step M,
parameters model updated accurately thus accuracy
whole algorithm iteratively increase.
significance results depicted Tables 13 14 verified
stratified shuffling significance level = 0.05. expected, results
approaches EMTRL + Random Initialization statistically significant.
hand, majority tests EMTRL + 10% Supervised Initialization EMTRL +
Rule-based Initialization, compared output greedy ILP algorithms
base method, statistical significance results verified.

7. Conclusion Future Work
paper, addressed problem temporal relation learning events,
topic interest since early days statistical natural language processing.
concentrated efforts reduce need annotated corpora much
possible. Accordingly, paper, two new algorithms, weakly supervised
unsupervised, presented.
first algorithm two-stage weakly supervised approach classification
temporal relations. first stage algorithm, SVM based classifier trained
learn temporal relations corpus. Then, second stage algorithm,
cross-document bootstrapping technique employed iteratively improve model
produced first stage. idea bootstrapping, inspired
hypothesis called one type temporal relation events per discourse,
test document, global evidences cluster topically related documents
refined local decisions made initial model. results experiment new
technique showed significant improvement terms accuracy related work including
state-of-the-art statistical methods.
second proposed algorithm novel model used EM algorithm
interval algebra reasoning temporal relation learning. compared work
successful fully supervised methods. experiments showed encouraging results,
considering low level supervision provided algorithm.
Currently, working finding ways improvement algorithms,
time trying reduce supervision level. BCDC, extracting semantic
features related documents, may able improve performance. Inconsistency
removal (i.e. ILP greedy best first search) algorithms also employed BCDC.
Besides, employing hypothesis one type temporal relation events per
discourse explicit constraint possible direction research.
EMTRL, one use sources information like narrative information, relations
events document times, relations events time expressions
build denser temporal graph. increases effectiveness greedy best first
search integer linear programming algorithms. also think, verified yet,
using richer feature set may improve accuracy EMTRL.
158

fiUnsupervised Temporal Relation Learning Events

Acknowledgments
authors wish thank associate editor anonymous reviewers
valuable comments.

References
Allen, J. (1984). Towards general theory action time. Artificial intelligence, 23 (2),
123154.
Bethard, S., & Martin, J. (2007). Cu-tmp: Temporal relation classification using syntactic
semantic features. Proceedings 4th International Workshop Semantic
Evaluations, pp. 129132. Association Computational Linguistics.
Bethard, S., & Martin, J. (2008). Learning semantic links corpus parallel temporal
causal relations. Proceedings 46th Annual Meeting Association
Computational Linguistics Human Language Technologies: Short Papers, pp.
177180. Association Computational Linguistics.
Bethard, S., Martin, J., & Klingenstein, S. (2007a). Finding temporal structure text:
Machine learning syntactic temporal relations. International Journal Semantic
Computing, 1 (4).
Bethard, S., Martin, J., & Klingenstein, S. (2007b). Timelines text: Identification
syntactic temporal relations. Semantic Computing, 2007. ICSC 2007. International
Conference on, pp. 1118. IEEE.
Bethard, S. (2007). Finding event, temporal causal structure text: machine learning
approach. Ph.D. thesis, University Colorado Boulder.
Boguraev, B., & Ando, R. (2005). Timeml-compliant text analysis temporal reasoning.
Proceedings IJCAI, Vol. 5, pp. 9971003.
Boguraev, B., Pustejovsky, J., Ando, R., & Verhagen, M. (2007). Timebank evolution
community resource timeml parsing. Language Resources Evaluation, 41 (1),
91115.
Bramsen, P., Deshpande, P., Lee, Y., & Barzilay, R. (2006). Inducing temporal graphs.
Proceedings 2006 Conference Empirical Methods Natural Language
Processing, pp. 189198. Association Computational Linguistics.
Chambers, N., & Jurafsky, D. (2008). Jointly combining implicit constraints improves
temporal ordering. Proceedings Conference Empirical Methods Natural
Language Processing, pp. 698706. Association Computational Linguistics.
Chambers, N., Wang, S., & Jurafsky, D. (2007). Classifying temporal relations
events. Proceedings 45th Annual Meeting ACL Interactive Poster
Demonstration Sessions, pp. 173176. Association Computational Linguistics.
Chang, C., & Lin, C. (2011). Libsvm: library support vector machines. ACM Transactions Intelligent Systems Technology (TIST), 2 (3).
159

fiMirroshandel & Ghassem-Sani

Charniak, E., & Elsner, M. (2009). Em works pronoun anaphora resolution. Proceedings 12th Conference European Chapter Association Computational Linguistics, pp. 148156. Association Computational Linguistics.
Cheng, Y., Asahara, M., & Matsumoto, Y. (2007). Naist. japan: Temporal relation identification using dependency parsed tree. Proceedings 4th International Workshop
Semantic Evaluations, pp. 245248. Association Computational Linguistics.
Cherry, C., & Bergsma, S. (2005). expectation maximization approach pronoun resolution. Proceedings Ninth Conference Computational Natural Language
Learning, pp. 8895. Association Computational Linguistics.
Chklovski, T., & Pantel, P. (2005). Global path-based refinement noisy graphs applied
verb semantics. Natural Language ProcessingIJCNLP 2005, pp. 792803. Springer.
Collins, M., & Duffy, N. (2001). Convolution kernels natural language. Proceedings
NIPS, Vol. 14, pp. 625632.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data via
em algorithm. Journal Royal Statistical Society. Series B (Methodological),
39, 138.
Denis, P., & Muller, P. (2010). Comparison different algebras inducing temporal
structure texts. Proceedings 23rd International Conference Computational Linguistics, pp. 250258. Association Computational Linguistics.
Denis, P., & Muller, P. (2011). Predicting globally-coherent temporal structures texts
via endpoint inference graph decomposition. Twenty-Second International
Joint Conference Artificial Intelligence.
Derczynski, L., & Gaizauskas, R. (2010). Usfd2: Annotating temporal expresions tlinks
tempeval-2. Proceedings 5th International Workshop Semantic Evaluation, pp. 337340. Association Computational Linguistics.
Ha, E., Baikadi, A., Licata, C., & Lester, J. (2010). Ncsu: Modeling temporal relations
markov logic lexical ontology. Proceedings 5th International Workshop
Semantic Evaluation, pp. 341344. Association Computational Linguistics.
Hagege, C., & Tannier, X. (2007). Xrce-t: Xip temporal module tempeval campaign.
Proceedings fourth international workshop semantic evaluations (SemEval2007), pp. 492495.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. (2009).
weka data mining software: update. ACM SIGKDD Explorations Newsletter, 11 (1),
1018.
Han, J., & Kambert, M. (2006). Data Mining: Concepts Techniques (second edition).
San Francisco: Morgan Kaufmann.
Hepple, M., Setzer, A., & Gaizauskas, R. (2007). Usfd: preliminary exploration features
classifiers tempeval-2007 tasks. Proceedings SemEval, pp. 438441.
Ji, H., & Grishman, R. (2008). Refining event extraction cross-document inference.
Proceedings Joint Conference 46th Annual Meeting ACL, pp.
254262. Association Computational Linguistics.
160

fiUnsupervised Temporal Relation Learning Events

Klein, D. (2005). Unsupervised Learning Natural Language Structure. Ph.D. thesis,
Department Computer Science, Stanford University.
Kolya, A., Ekbal, A., & Bandyopadhyay, S. (2010). Ju cse temp: first step towards
evaluating events, time expressions temporal relations. Proceedings
5th International Workshop Semantic Evaluation, pp. 345350. Association
Computational Linguistics.
Lapata, M., & Lascarides, A. (2006). Learning sentence-internal temporal relations. Journal
Artificial Intelligence Research, 27 (1), 85117.
Lin, D., & Pantel, P. (2001). Dirt: discovery inference rules text. Proceedings
seventh ACM SIGKDD international conference Knowledge discovery
data mining, pp. 323328. ACM.
Llorens, H., Saquete, E., & Navarro, B. (2010). Tipsem (english spanish): Evaluating crfs
semantic roles tempeval-2. Proceedings 5th International Workshop
Semantic Evaluation, pp. 284291. Association Computational Linguistics.
Mani, I., Verhagen, M., Wellner, B., Lee, C., & Pustejovsky, J. (2006). Machine learning
temporal relations. Proceedings 21st International Conference Computational Linguistics 44th annual meeting Association Computational
Linguistics, pp. 753760. Association Computational Linguistics.
Mani, I., Wellner, B., Verhagen, M., & Pustejovsky, J. (2007). Three approaches learning
tlinks timeml. Tech. rep., Technical Report CS-07-268, Brandeis University.
Mikheev, A., Grover, C., & Moens, M. (1998). Description ltg system used muc-7.
Proceedings 7th Message Understanding Conference (MUC-7). Fairfax, VA.
Min, C., Srikanth, M., & Fowler, A. (2007). Lcc-te: hybrid approach temporal relation
identification news text. Proceedings 4th International Workshop
Semantic Evaluations, pp. 219222. Association Computational Linguistics.
Mirroshandel, S., Ghassem-Sani, G., & Khayyamian, M. (2009a). Event-time temporal
relation classification using syntactic tree kernels. Proceeding 4th Language
Technology Conference, pp. 300304.
Mirroshandel, S., Ghassem-Sani, G., & Khayyamian, M. (2009b). Using tree kernels
classifying temporal relations events. Proceedings 23th Pacific Asia
Conference Language, Information Computation, pp. 355364.
Mirroshandel, S., Ghassem-Sani, G., & Khayyamian, M. (2011). Using syntactic-based kernels classifying temporal relations. Journal Computer Science Technology,
26 (1), 6880.
Mulkar-Mehta, R., Hobbs, J., Liu, C., & Zhou, X. (2009). Discovering causal temporal
relations biomedical texts recognizing causal temporal relations. Proceedings
AAAI Spring Symposium, Stanford CA.
Muller, P., & Tannier, X. (2004). Annotating measuring temporal relations texts.
Proceedings 20th international conference Computational Linguistics, pp.
5056. Association Computational Linguistics.
161

fiMirroshandel & Ghassem-Sani

Ng, V. (2008). Unsupervised models coreference resolution. Proceedings Conference Empirical Methods Natural Language Processing, pp. 640649. Association
Computational Linguistics.
Pekar, V. (2006). Acquisition verb entailment text. Proceedings main conference Human Language Technology Conference North American Chapter
Association Computational Linguistics, pp. 4956. Association Computational Linguistics.
Petrov, S., & Klein, D. (2007). Improved inference unlexicalized parsing. Proceedings
NAACL HLT 2007, pp. 404411.
Puscasu, G. (2007). Wvali: Temporal relation identification syntactico-semantic analysis.
Proceedings 4th International Workshop SemEval, pp. 484487.
Pustejovsky, J., Hanks, P., Sauri, R., See, A., Gaizauskas, R., Setzer, A., Radev, D., Sundheim, B., Day, D., Ferro, L., & Lazo, M. (2003). timebank corpus. Corpus
Linguistics, Vol. 2003, p. 40.
Saur, R., Knippen, R., Verhagen, M., & Pustejovsky, J. (2005). Evita: robust event recognizer qa systems. Proceedings conference Human Language Technology
Empirical Methods Natural Language Processing, pp. 700707. Association
Computational Linguistics.
Sgaard, A. (2011). Semisupervised condensed nearest neighbor part-of-speech tagging. Proceedings 49th Annual Meeting Association Computational
Linguistics: Human Language Technologies: short papers, Vol. 2, pp. 4852.
Strohman, T., Metzler, D., Turtle, H., & Croft, W. (2005). Indri: language model-based
search engine complex queries. Proceedings International Conference
Intelligent Analysis.
Szpektor, I., Tanev, H., Dagan, I., & Coppola, B. (2004). Scaling web-based acquisition
entailment relations. Proceedings EMNLP, Vol. 4, pp. 4148.
Tatu, M., & Srikanth, M. (2008). Experiments reasoning temporal relations events. Proceedings 22nd International Conference Computational
Linguistics-Volume 1, pp. 857864. Association Computational Linguistics.
UzZaman, N., & Allen, J. (2010). Trips trios system tempeval-2: Extracting temporal
information text. Proceedings 5th International Workshop Semantic
Evaluation, pp. 276283. Association Computational Linguistics.
Verhagen, M., Gaizauskas, R., Schilder, F., Hepple, M., Katz, G., & Pustejovsky, J. (2007).
Semeval-2007 task 15: Tempeval temporal relation identification. Proceedings
4th International Workshop Semantic Evaluations, pp. 7580. Association
Computational Linguistics.
Verhagen, M., Sauri, R., Caselli, T., & Pustejovsky, J. (2010). Semeval-2010 task 13:
Tempeval-2. Proceedings 5th International Workshop Semantic Evaluation, pp. 5762. Association Computational Linguistics.
Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. Proceedings 33rd annual meeting Association Computational
Linguistics, pp. 189196. Association Computational Linguistics.
162

fiUnsupervised Temporal Relation Learning Events

Yeh, A. (2000). accurate tests statistical significance result differences.
Proceedings 18th conference Computational linguistics-Volume 2, pp. 947
953. Association Computational Linguistics.
Yoshikawa, K., Riedel, S., Asahara, M., & Matsumoto, Y. (2009). Jointly identifying temporal relations markov logic. Proceedings Joint Conference 47th
Annual Meeting ACL 4th International Joint Conference Natural
Language Processing AFNLP: Volume 1-Volume 1, pp. 405413. Association
Computational Linguistics.
Zhang, M., Zhang, J., Su, J., & Zhou, G. (2006). composite kernel extract relations
entities flat structured features. Proceedings 21st
International Conference Computational Linguistics 44th annual meeting
Association Computational Linguistics, pp. 825832. Association Computational Linguistics.

163

fiJournal Artificial Intelligence Research 45 (2012) 4778

Submitted 03/12; published 09/12

Tractability CSP Classes Defined Forbidden Patterns
David A. Cohen

dave@cs.rhul.ac.uk

Department Computer Science
Royal Holloway, University London
Egham, Surrey, UK

Martin C. Cooper

cooper@irit.fr

IRIT
University Toulouse III, 31062 Toulouse, France

Paid Creed

p.creed@qmul.ac.uk

School Mathematical Sciences
Queen Mary, University London
Mile End, London, UK

Daniel Marx

dmarx@cs.bme.hu

Computer Automation Research Institute
Hungarian Academy Sciences (MTA SZTAKI)
Budapest, Hungary

Andras Z. Salamon

andras.salamon@ed.ac.uk

Laboratory Foundations Computer Science
School Informatics, University Edinburgh, UK

Abstract
constraint satisfaction problem (CSP) general problem central computer
science artificial intelligence. Although CSP NP-hard general, considerable
effort spent identifying tractable subclasses. main two approaches consider
structural properties (restrictions hypergraph constraint scopes) relational
properties (restrictions language constraint relations). Recently, authors
considered hybrid properties restrict constraint hypergraph relations
simultaneously.
key contribution novel concept CSP pattern classes problems
defined forbidden patterns (which viewed forbidding generic sub-problems).
describe theoretical framework used reason classes problems
defined forbidden patterns. show framework generalises certain known
hybrid tractable classes.
Although close obtaining complete characterisation concerning
tractability general forbidden patterns, prove dichotomy special case: classes
problems arise forbid binary negative patterns (generic subproblems disallowed tuples specified). case show (finite
sets of) forbidden patterns define either polynomial-time solvable NP-complete classes
instances.
c
2012
AI Access Foundation. rights reserved.

fiCohen, Cooper, Creed, Marx & Salamon

1. Introduction
constraint satisfaction paradigm consider computational problems
assign values (from domain) variables, constraints. constraint
limits (simultaneous) values list variables (its scope) assigned.
typical situation pair variables might represent starting times two jobs
machine shop scheduling problem. reasonable constraint would require minimum time
gap values assigned two variables.
Constraint satisfaction proved useful modelling tool variety contexts,
scheduling, timetabling, planning, bio-informatics computer vision.
led development number successful constraint solvers. Unfortunately, solving
general constraint satisfaction problem (CSP) instances NP-hard
significant research effort finding tractable fragments CSP.
principle stratify CSP two quite distinct natural ways. structure constraint scopes instance CSP thought hypergraph
variables vertices, generally relational structure. find
tractable classes restricting relational structure, allowing arbitrary constraints
resulting scopes (Dechter & Pearl, 1987). Sub-problems general constraint
problem obtained restrictions called structural. Alternatively, set allowed assignments variables scope seen relation. choose
allow specified kinds constraint relations, allow interact arbitrary structure (Jeavons, Cohen, & Gyssens, 1997). restrictions called relational
language-based.
Structural subclasses defined specifying set hypergraphs (or relational structures) allowed structures CSP instances. shown tractable
structural classes characterised limiting appropriate (structural) width measures
(Dechter & Pearl, 1989; Freuder, 1990; Gyssens, Jeavons, & Cohen, 1994; Gottlob, Leone,
& Scarcello, 2002; Marx, 2010a, 2010b). example, tractable structural class binary
CSPs obtained whenever restrict constraint structure (which graph
case) bounded tree width (Dechter & Pearl, 1989; Freuder, 1990). fact,
shown that, subject certain complexity-theoretic assumptions, structures
give rise tractable CSPs bounded (hyper-)tree width (Dalmau,
Kolaitis, & Vardi, 2002; Grohe, 2006, 2007; Marx, 2010a, 2010b).
Relational subclasses defined specifying set constraint relations. complexity subclass arising restriction precisely determined
called polymorphisms set relations (Bulatov, Jeavons, & Krokhin, 2005; Cohen
& Jeavons, 2006). polymorphisms specify that, whenever set tuples
constraint relation, cannot case particular tuple (the result applying
polymorphism) constraint relation. thus relationship allowed tuples disallowed tuples inside constraint relations key importance
relational tractability given class instances. Whilst general dichotomy
yet proven relational case, many dichotomies sub-problems
obtained, instance Bulatov (2003), Bulatov et al. (2005) Bulatov (2006).
48

fiTractability CSP Classes Defined Forbidden Patterns

Using structural relational restrictions limits possible subclasses
defined. allowing restrictions structure relations able
identify new tractable classes. call restrictions hybrid reasons tractability.
Several hybrid results published binary CSPs (Jegou, 1993; Weigel & Bliek,
1998; Cohen, 2003; Salamon & Jeavons, 2008; Cooper, Jeavons, & Salamon, 2010; Cooper &
Zivny, 2011b). Instead looking set constraint scopes constraint language,
results captured tractability based properties (coloured) microstructure
CSP instances. microstructure binary CSP instance graph hV, Ei V
set possible assignments values variables E set pairs mutually
consistent variable-value assignments (Jegou, 1993). coloured microstructure,
vertices representing assignment variable vi labelled colour representing
variable vi . maintains distinction assignments different variables.
coloured microstructure CSP instance captures structure
relations CSP instance natural place look tractable classes
neither purely structural purely relational. results (coloured) microstructure
properties, three particular note. First observed class instances
perfect microstructure tractable (Salamon & Jeavons, 2008). proper
generalisation well known hybrid tractable CSP class whose instances allow arbitrary
unary constraints every pair variables constrained equal (Regin,
1994; van Hoeve, 2001), hybrid class whose microstructure triangulated (Jegou,
1993; Weigel & Bliek, 1998; Cohen, 2003). perfect microstructure property excludes
infinite set induced subgraphs microstructure.
Secondly, Joint Winner Property (JWP) (Cooper & Zivny, 2011b) applied CSPs
provides different hybrid class also strictly generalises class CSP instances
disequality constraint (6=) every pair variables arbitrary set
unary constraints, forbidding single pattern (a subgraph) coloured
microstructure. JWP generalized hierarchies soft non-binary constraints (Cooper & Zivny, 2011a), including, example, soft hierarchical global cardinality
constraints, reduction minimum convex cost flow problem.
Thirdly, called broken-triangle property properly extends structural notion
acyclicity interesting hybrid class (Cooper et al., 2010). broken triangle
property specified excluding particular pattern coloured microstructure.
notion forbidden pattern study paper. therefore work directly
CSP instance (or equivalently coloured microstructure) rather microstructure
abstraction simple graph. allows us introduce language expressing
hybrid classes terms forbidden patterns, providing framework search
novel hybrid tractable classes. case binary negative patterns able
characterise tractable (finite sets of) forbidden patterns. also state necessary
condition tractability (finite set of) general patterns.
1.1 Contributions
paper generalise definition CSP instance CSP pattern
three types tuple constraint relations, tuples explicitly al49

fiCohen, Cooper, Creed, Marx & Salamon

lowed/disallowed tuples labelled unknown1 . defining natural notion
containment patterns CSP, able describe problems defined forbidden
patterns: class CSP instances defined forbidding particular pattern exactly
instances contain . use framework capture tractability
identifying local patterns allowed disallowed tuples (within small groups connected
constraints) whose absence enough guarantee tractability.
Using concept forbidden patterns, lay foundations theory
used reason classes CSPs defined hybrid properties. Since first
work kind, primarily focus simplest case: binary patterns tuples
either disallowed unknown (called negative patterns). give large class binary
negative patterns give rise intractable classes problems and, using this, show
negative pattern defines tractable class problems must certain
structure. able prove structure also enough guarantee tractability
thus providing dichotomy tractability defined forbidding binary negative patterns.
Importantly, intractability results also allow us give necessary condition
form general tractable patterns.
remainder paper structured follows. Section 2 define constraint
satisfaction problems, give definitions used paper. Then, Section 3,
define notion CSP pattern describe classes problems defined forbidden
patterns. give examples tractable classes defined forbidden patterns three
variables. Section 4 show one must take size patterns account
notion maximal classes defined forbidding patterns. general, yet able
make conjecture concerning dichotomy hybrid tractability defined general
forbidden patterns. However, Section 5 able give necessary condition
class tractable Section 6 prove dichotomy negative patterns. Finally,
Section 7 summarise results discuss directions future research.

2. Preliminaries
Definition 2.1. CSP instance triple hV, D, Ci where:
V finite set variables (with n = |V |).
finite set called domain (with = |D|).
C set constraints. constraint c C pair c = h, where:
list distinct variables called scope c.
relation arity || called relation c. set tuples
allowed c.
solution CSP instance P = hV, D, Ci mapping : V where,
h, C s() (where s() represents tuple resulting application
component-wise list variables ).
1. viewed natural generalisation CSP three-valued logic.

50

fiTractability CSP Classes Defined Forbidden Patterns

simplicity presentation, assume variables domains.
Unary constraints used impose different domains different variables.
arity CSP largest arity constraint scopes. long-term
aim identify tractable subclasses CSP problem detected
polynomial time. paper describe general theory forbidden patterns
arbitrary arity consider implications new theory tractable classes
arity two (binary) problems specified finite sets forbidden patterns. cases
certain class membership decided polynomial time.
CSP decision problem, asks whether particular CSP instance solution,
already NP-complete binary CSPs. example, straightforward reduction
graph colouring problem set colours used domain
CSP instance, vertices graph map CSP variables vi , edges {i, j} map
disequality constraints vi 6= vj .
sometimes convenient paper use equivalent functional formulation
constraint. alternative formulation scope constraint h, abstracted set variables possible assignment seen function f : D.
constraint relation alternative view function set possible
assignments, , set {T, F } where, convention, tuples occur
constraint relation map . follows assignment set
variables allowed h, restriction mapped .
Definition 2.2. function f : X X, notation f |S means
function domain satisfying f |S (x) = f (x) x S.
Given set V variables domain D, constraint functional representation
pair h, V : {T, F }. CSP instance functional representation triple hV, D, Ci C set constraints functional representation.
solution (to CSP instance hV, D, Ci functional representation) mapping
: V where, h, C (s| ) = .
functional formulation clearly equivalent relational formulation
use whichever seems appropriate throughout paper. choice always
clear context.
following notions standard study CSP. binary CSP instance
one maximum arity constraint scope two. subproblem
variables U V instance hU, D, CU CU set constraints h, C
U . instance arc-consistent v1 , v2 V , solution
subproblem {v1 } extended solution subproblem {v1 , v2 }.
constraint graph binary CSP instance = hV, D, Ci graph vertices
V edges set scopes binary constraints C. Since often convenient
consider (possibly irrelevant) constraint exists every pair variables,
introduce refined notion true constraint graph.
Definition 2.3. binary constraint v1 v2 improper allows every pair
values allowed unary constraints v1 v2 , proper otherwise.
true constraint graph binary CSP instance constraint graph
instance removing improper binary constraints.
51

fiCohen, Cooper, Creed, Marx & Salamon

may also sometimes need disregard unary constraints following.
Definition 2.4. binary reduction CSP instance obtained removing
constraint set constraints whose scope arity two.

3. Forbidden Patterns CSP
paper explain define classes CSP instances forbidding
occurrence certain patterns. CSP pattern generalisation CSP instance.
CSP pattern define relations relative three-valued logic {T, F, U }, meaning
pattern seen representing set CSP instances undefined value U replaced either F . Forbidding CSP pattern equivalent
simultaneously forbidding instances sub-problems.
Definition 3.1. define three-valued logic {T, F, U }, U stands unknown
undefined. set {T, F, U } partially ordered U < U < F F
incomparable. Let finite set. k-ary three-valued relation function
: Dk {T, F, U }. Given k-ary three-valued relations 0 , say realises 0
x Dk (x) 0 (x).
extend definition CSP constraint pattern include additional structure set variable names set domain values, set relations
set question. Adding structure makes patterns specific. therefore capture larger, hence interesting, tractable classes. example, domain
totally ordered define tractable max-closed class (Jeavons & Cooper, 1995);
independent total order domain variable capture
renamable Horn class (Green & Cohen, 2003); placing order variables
pattern allow us define class tree-structured CSP instances.
Definition 3.2. CSP pattern quadruple = hV, D, C, Si, where:
V set variables, associated relational structure universe V .
domain, associated relational structure universe D.
C set constraint patterns. constraint pattern c C pair c = h, i,
V , scope c, list distinct variables : {T, F, U }
three-valued relation (in functional representation) c. constraint pattern
non-trivial three-valued relation maps least one tuple {T, F }.
structure, set consisting relational structures associated
variable set domain.
arity CSP pattern maximum arity constraint pattern h, .
basic type pattern one employs structure, empty.
also frequently require patterns use disequality relation 6=, applied every pair
52

fiTractability CSP Classes Defined Forbidden Patterns

specified subset variables, allow several subsets variables
structure.
paper relations occurring structure arity two, interpretation limited selected binary relations representing disequality partial
order. structure variable set domain clear context,
explicitly mention it. Different kinds structure imposed CSP patterns; indeed
structures specified general relations would interesting area future study.
weakest structure consider allows us say two variables
distinct. Thus structure CSP pattern simply set disequalities
subsets variables. paper denote disequalities NEQ(v1 , . . . , vr ) meaning
variables v1 , v2 , . . . , vr pairwise distinct. pattern structure
called flat. Indeed, paper mostly concerned flat patterns. two
variables occur together scope constraint pattern, also assume
implicitly includes disequality NEQ(v1 , v2 ).
Thus CSP patterns defined using relational structures three sorts: variables,
domain values, variable-value assignments. constraint patterns CSP
pattern three-valued relations sort variable-value assignments. CSP
pattern flat structure specifies relations sort variables. partial order
variables also relation sort variables, partial orders domain
values relations sort domain values.
simplicity presentation, assume throughout paper two constraint
patterns C scope (and that, case CSP instances, two
constraints scope). represent binary CSP patterns simple diagrams. oval represents domain variable, dot domain value. tuples
constraint patterns value F shown dashed lines, value solid
lines value U depicted all.
Definition 3.3. constraint pattern h, called negative never takes
value . CSP pattern negative every constraint pattern negative.
3.1 Patterns, CSPs Occurrence
CSP instance implicitly assumed variables domain values
distinct. equivalent existence implicit disequalities NEQ variable
names domain values. CSP instance CSP pattern (with structure
variables domain values distinct) three-valued relations
constraint patterns never take value U . is, decide possible tuple
whether relation not. Furthermore, CSP instance, pair
variables assume constraint exists scope; explicit constraint
given scope, assume relation complete, i.e. contains tuples.
contrasted CSP patterns absence explicit constraint
pair variables implies truth value tuple undefined.
order define classes CSP instances forbidding patterns, require formal
definition occurrence (containment) pattern within instance. define
general notion containment one CSP pattern within another pattern. Informally,
names variables domain elements CSP pattern inconsequential
53

fiCohen, Cooper, Creed, Marx & Salamon

containment allows renaming variables domain values variable.
Thus, order define containment patterns, firstly require formal definition
renaming. arbitrary renaming, unless explicitly prohibited disequality
structure, two distinct variables may map variable two distinct domain
values may map domain value. Furthermore, pattern occurs another,
may use subset variables second pattern; hence notion require
known renaming-extension.
domain labelling set variables assignment domain values
variables. Variable domain renaming induces mapping domain labellings
scopes constraints: simply assign renamed domain values renamed variables. natural way extend mapping domain labellings mapping
constraint pattern: truth-value mapped domain labelling
truth-value original domain labelling. However, may occur two domain
labellings scope map domain labelling, instead resulting value
taken greatest original truth-values. (In order process
well-defined, two domain labellings constraint mapped domain labelling, original truth-values must comparable.) leads following
formal definition renaming-extension first step towards definition
containment.
Definition 3.4. Let = hV, D, C, Si 0 = hV 0 , D0 , C 0 , 0 CSP patterns.
say 0 renaming-extension exist variable-renaming function : V V 0 domain-renaming function : V D0 s,
assignment-renaming function F : V V 0 D0 induced (s, t) defined
F (hv, ai) = hs(v), t(v, a)i satisfy:
constraint pattern h, C, two domain labellings `, `0
F (`) = F (`0 ), (`) (`0 ) comparable, F (`) denotes
assignment f : s() D0 v , f (s(v)) = t(v, `(v)).
C 0 = {hs(), 0 | h, C}, where, assignment f : s() D0 , 0 (f ) = U
F (`) 6= f every ` , 0 (f ) = max {(`) | F (`) = f } otherwise.
structure, s, F preserve structure. mapping induces
homomorphism relational structures variable-sets, mapping
induces homomorphism relational structures domains. (In
particular, NEQ(v1 , v2 ) S, s(v1 ) 6= s(v2 ) NEQ(s(v1 ), s(v2 )) 0 .)
use patterns define sets CSP instances forbidding occurrence (containment) patterns CSP instances. way able characterise
tractable subclasses CSP. Informally, pattern said occur CSP instance
P find sub-problem Q P (formed taking subsets variables domains)
realises . Q realises if, renaming variables domain values ,
constraint pattern realised corresponding constraint Q. Definition 3.4,
renaming-extension, extra variables, domain values disequalities introduced. Thus need combine notions renaming-extension realisation
formally define mean pattern occurring another pattern (and, particular,
CSP instance).
54

fiTractability CSP Classes Defined Forbidden Patterns

Definition 3.5. say CSP pattern occurs CSP pattern P = hV, D, C, Si
(or P contains ), denoted P , renaming-extension hV, D, C 0 , Si
where, every constraint pattern h, 0 C 0 constraint pattern h, C and,
furthermore, realises 0 .
Pattern 1
d0
b
b

c



b




c





x


z



x

(i)

c




x

(ii)

(iii)

Pattern 2
b





c


x

Example 3.6. example describes three simple containments. Consider three CSP
patterns, Pattern 1(i)(iii). patterns occur in, contained in, Pattern 2
mappings F1 , F2 , F3 , respectively, describe.
F1 simply bijection. Although patterns different, valid containment
Pattern 1(i) Pattern 2 three-valued relation Pattern 2 realisation
three-valued relation Pattern 1(i): replacing (b, d) 7 U (b, d) 7 F .
F2 maps (x, a), (x, b), (y, c) themselves, maps (y, d) (y, d0 )
(y, d). merging domain elements possible values three-valued
constraint relation Pattern 1(ii) comparable tuples involving assignments (y, d)
(y, d0 ) and, furthermore, restriction three-valued relation Pattern 1(ii)
either two assignments realised three-valued constraint relation
Pattern 2: (b, d) 7 F (a, d) 7 . example, replacing (a, d0 ) 7 U
(a, d) 7 . similar manner, Pattern 1(i) also contained Pattern 2 simple
mapping F10 maps (x, b), (x, a) (x, b) (y, c), (y, d) (y, c).
Finally, F3 maps (y, c) (y, d) themselves, maps (x, a) (z, b) Pattern 1(iii) (x, a) (x, b), respectively, Pattern 2. merging variables pos55

fiCohen, Cooper, Creed, Marx & Salamon

sible three-valued relations agree NEQ(x, z) structure
Pattern 1(iii).

Pattern 3
z
b

b





z
c




x

c




x

NEQ(x, z)
(i)

(ii)

Throughout paper, use notation NEQ(v1 , . . . , vr ) denote fact
variables v1 , . . . , vr CSP pattern distinct. worth discussing structure
implies far Definition 3.4 concerned. Structure source pattern must
preserved target pattern. Thus Pattern 1(iii) occurs Pattern 3(i), Pattern 3(i)
contained Pattern 1(iii) since structure NEQ(x, z) preserved target
pattern. structure NEQ(v1 , v2 ) considered preserved renaming-extension
0 even explicitly given 0 implicit, example, due existence
non-trivial constraint pattern h, 0 v1 , v2 . example, consider
two CSP patterns, Pattern 3(i)(ii). Pattern 3(i) mapped Pattern 3(ii)
simple bijection three-valued relation Pattern 3(ii) realisation
three-valued relation Pattern 3(i). structure NEQ(x, z) considered preserved
mapping due existence non-trivial constraint pattern variables
x z Pattern 3(ii). Hence, Pattern 3(i) occurs Pattern 3(ii).
continuing need define mean say class CSP
instances definable forbidden patterns.
Definition 3.7. Let C class CSP instances maximum arity k. say
C definable forbidden patterns set patterns X set
CSP instances maximum arity k none patterns X occur precisely
instances C.
Notation: Let X set CSP patterns maximum arity k. use CSP(X )
denote set CSP instances element X occurs. X singleton
{} use CSP() denote CSP({}).
paper, consider classes CSP(X ) sets X CSP patterns
binary sense constraint patterns scope size exactly two.
X patterns X binary, CSP(X ) closed arc consistency
(in sense arc consistency closure instance CSP(X ) belongs
56

fiTractability CSP Classes Defined Forbidden Patterns

CSP(X )) operation updates unary constraints. Indeed, changing
unary constraints cannot introduce patterns X instance CSP(X ).
3.2 Tractable Patterns
paper define, forbidding certain patterns, tractable subclasses CSP.
Furthermore, give examples truly hybrid classes (i.e. classes definable
purely relational purely structural properties).
Definition 3.8. finite set patterns X intractable CSP(X ) NP-hard.
tractable polynomial-time algorithm solve CSP(X ). single pattern
tractable (intractable) {} tractable (intractable). (We assume throughout paper
P6= NP, therefore sets tractable intractable patterns disjoint.)
worth observing classes CSP instances defined forbidding patterns
fixed domain. Recall, however, CSP instance finite domain.
structure present CSP instance assumed given part instance. particular,
variables CSP instance assumed distinct. finite sets patterns X ,
number possible renaming-extensions particular instance P polynomial
size P . Hence determine whether instance lies CSP(X ) exhaustive
search polynomial time.
need following simple lemmas proofs intractability results later
sections paper.
Lemma 3.9. 1 2 2 3 , 1 3 .
Proof. 0 , constraint pattern h, maps constraint pattern h 0 , 0
0 realises . transitivity follows following facts:
realisation operation transitive.
1 2 2 3 , Definition 3.4, structure 1 preserved 2
hence 3 .
Lemma 3.10. Let X sets CSP patterns suppose every pattern
, pattern X . CSP(X ) CSP(T ).
Proof. Let P CSP(X ), 6 P X . cannot P
, since would imply exists X P hence
P Lemma 3.9. Hence, P CSP(T ).
Corollary 3.11. Let X sets CSP patterns suppose every pattern
, pattern X .
CSP(T ) intractable CSP(X ) intractable conversely,
CSP(X ) tractable whenever CSP(T ) tractable.
Finally, give examples tractable patterns. first example negative
pattern since truth-values relations F U .
57

fiCohen, Cooper, Creed, Marx & Salamon

Pattern 4 simple negative pattern.
v


x
c
c0

w
b
NEQ(v, w, x)

Example 3.12. Consider Pattern 4. defines class CSPs trivially tractable.
Forbidding Pattern 4 ensures paths two variables true
constraint graph. Thus, problem forbidding Pattern 4 decomposed set
independent sub-problems, two variables.

Example 3.13. Cooper Zivny (2011b) showed forbidding pattern Negtrans
shown Pattern 5 describes tractable class CSP instances. seen generalisation well-known tractable class problems, AllDifferent+unary (Costa,
1994; Regin, 1994; van Hoeve, 2001): instance class consists set variables
V , set arbitrary unary constraints V , constraint v 6= w defined pair
distinct variables v, w V . Forbidding Negtrans equivalent saying disallowed
tuples form transitive relation, i.e. (hv, ai , hx, bi) (hx, bi , hw, ci) disallowed
(hv, ai , hw, ci) must also disallowed. Thus Negtrans occur binary CSP
instance class AllDifferent+unary transitivity equality (equality
exactly disallowed).

Pattern 5 Negative transitive pattern (Negtrans)
v
x

w

NEQ(v, w, x)
Cooper Zivny (2011b) also recently showed tractable class defined
forbidding Pattern 5 (Negtrans) extended soft constraint problems.
58

fiTractability CSP Classes Defined Forbidden Patterns

3.3 Tractable Patterns Structure
paper primarily studies patterns weak structure conditions
imposed variables distinct. However, worth pointing adding
structure pattern allows us capture larger classes instances. Example 3.14
show forbidden pattern capture class CSPs tree width 1
adding variable-ordering Pattern 4. case pattern containment must preserve
total order. ordered pattern , consider unordered CSP P
CSP() exists ordering variable set P forbidden.
order define tractable class, must possible find ordering polynomial
time. case patterns Examples 3.14 3.15.
Pattern 6 Tree structure pattern (Tree)

v1
v3
v2
v1 < v2 < v3
Example 3.14. Consider pattern Tree, given Pattern 6. show class
CSP(Tree) exactly set CSPs whose true constraint graph forest (i.e. tree
width 1). First, suppose P CSP(Tree). Then, exists ordering = (v1 , . . . , vn )
variable shares proper constraint one variable preceding
ordering. hand, suppose P CSP whose true constraint graph
tree. ordering vertices according pre-order traversal, obtain ordering
variable shares proper constraint one variable preceding
ordering (its parent); thus, P CSP(Tree).

Example 3.15. Forbidding pattern BTP shown Pattern 7 known brokentriangle property (Cooper et al., 2010). order capture class forbidden
pattern impose total order pattern variables. Cooper et al. (2010)
proved class CSP instances CSP(BTP) solved polynomial time and,
indeed, CSP instances CSP(BTP) unknown total ordering variables
recognised solved polynomial time.

easy see Tree (shown Pattern 6) occurs BTP (with truthvalues U changed ). follows Lemma 3.10 CSP(Tree) CSP(BTP).
Hence class CSP(BTP) includes CSP instances whose true constraint graph tree.
However, CSP(BTP) also includes certain CSP instances whose true constraint graph
59

fiCohen, Cooper, Creed, Marx & Salamon

Pattern 7 Broken triangle pattern (BTP)

b

v1


v3
v2
v1 < v2 < v3

tree width r value r: consider, example, CSP instance r + 1 variables
identical constraint every pair variables simply disallows single
tuple h0, 0i.
tractable forbidden pattern order imposed variables,
obtain another tractable class considering problems forbidding pattern without
ordering condition. class obtained generally smaller, easier establish
containment flat pattern. example, consider Pattern 4 flat version
Pattern 6. seen forbidding Pattern 4 gives rise class CSP instances
paths length greater two true constraint graph.
hand, forbidding Pattern 6 gives much larger class CSP instances
true constraint graph tree width 1.
case broken-triangle property, also obtain strictly smaller tractable
class forbidding Pattern 7 triples variables v1 , v2 , v3 irrespective order.
easily exhibit CSP instance shows inclusion strict: example,
3-variable CSP instance Boolean domains consisting two constraints v1 = v2 ,
v1 = v3 variable ordering v1 < v2 < v3 . unordered version BTP
recently used obtain dichotomy patterns consisting 2 constraints (Cooper &
Escamocher, 2012).

4. Maximal Tractable Classes Defined Forbidden Patterns
relational tractability define maximal tractable sub-problem CSP problem
given set possible relations. class relations maximal possible
add even one relation without sacrificing tractability.
case structural tractability picture less clear, since measure
complexity infinite set hypergraphs (or, generally, relational structures).
obtain tractability bound width measure structures. Whatever
width measure chosen containment class width bounded k inside
class width bounded k +1 maximal class possible (although
k unique maximal class structurally tractable instances). section,
show case forbidden patterns situation similar.
60

fiTractability CSP Classes Defined Forbidden Patterns

Definition 4.1. Let = hV, D, C, Si = hV 0 , D0 , C 0 , 0 two flat CSP patterns.
0 DD
0 . Now, extend constraint pattern
form disjoint unions V V
0 setting value tuple including elements D0
C domain DD
0.
U , extend similarly constraint patterns C 0 : way define C C
0 forming disjoint union 0 adding
Also define structure
0
disequalities NEQ(v, v ) v V v 0 V 0 . set disjoint union
= hV V
0 , DD
0 , C C
0 ,
0 i.

Lemma 4.2. Let flat non-empty (i.e. containing least one variable) binary
CSP patterns.
).
CSP() CSP( ) ( CSP(
) tractable whenever CSP() CSP( ) tractable.
Moreover, CSP(
Proof. begin showing strict inclusion
).
CSP() CSP( ) ( CSP(
inclusion holds follows directly Lemma 3.10. Among patterns
occurs, let pattern smallest number variables. define similarly.
see inclusion strict, observe occur CSP pattern whose
domain disjoint union , whose variable set size equal
larger variable sets . CSP instance containing pattern
neither CSP() CSP( ). However, construct CSP instance containing
), structure
imposing disequalities
pattern contained CSP(

variables means contained pattern:
simply enough variables.
). P CSP() CSP( ) P solved polynomial
Suppose P CSP(
time, tractability CSP() CSP( ).
may suppose P . Choose particular occurrence P let
denote set variables used containment. Consider assignment :
D. Let Pt denote problem obtained making assignment enforcing
arc-consistency resulting problem. corresponds adding new unary
constraints P .
must occur P . see this, observe
show occurs Pt
containment Pm naturally induces containment P extends
P , considering occurrence . Thus, conclude
containment
Pt CSP( ), solved polynomial time.
construction, solution Pt extends solution P adding assignment
variables . Moreover, every solution P corresponds solution Pt
: D. Since size fixed, iterate solutions polynomial
time. P solution, find solution Pt . find
Pt solution, know P solution. Thus, since solve
Pt polynomial time, also solve P polynomial time.
Corollary 4.3. tractable class defined forbidding flat pattern maximal.
61

fiCohen, Cooper, Creed, Marx & Salamon

defined disjoint
Proof. Let tractable flat pattern. Consider pattern

union two copies . Lemma 4.2 CSP()
tractable also
,
CSP() ( CSP()
hence CSP() maximal tractable class.
follows cannot characterise tractable forbidden patterns exhibiting
maximal tractable classes defined forbidding pattern (or finite set patterns,
since Lemma 4.2 finite set replaced single pattern). Indeed,
consequence Lemma 4.2 construct infinite chain patterns,
forbidding one gives rise slightly larger tractable class. Naturally, place
upper bound size patterns finitely many patterns
consider, maximal tractable classes defined forbidden patterns bounded size
necessarily exist.

5. Binary Flat Negative Patterns
moment, able make conjecture concerning complete characterisation complexity general forbidden patterns, although conjecture
dichotomy exists. Nonetheless, restricting attention special case, forbidden
binary flat negative patterns, able obtain dichotomy. Recall pattern
flat structure imposed variables distinct,
negative constraint patterns h, i, never takes value .
begin defining three particular patterns one infinite class patterns.
use patterns characterise large class intractable patterns. prove
finite set flat negative patterns class simple structure: one
patterns must contained one particular set patterns, call pivots.
means tractable set patterns must include pattern occurs
pivot pattern. Furthermore, demonstrate forbidding pivot pattern gives rise
tractable class. leads simple characterisation tractability finite
sets binary flat negative patterns.
Pattern 8 Cycle(6)
c0
c
v1

v2

v3

v6

v5

NEQ(v1 , . . . , v6 )

62

v4

fiTractability CSP Classes Defined Forbidden Patterns

Pattern 9 Valency
x1

x01

x2

x02

x3

x03
NEQ(x1 , x2 , x3 , x01 ) NEQ(x01 , x02 , x03 )

Pattern 10 Path

v1

v2

v3

w1

w2

w3

NEQ(v1 , v2 , v3 , w1 ) NEQ(w1 , w2 , w3 )
Definition 5.1 below, define concept neg-connected binary pattern.
correspond binary patterns true constraint graph every realisation
binary CSP instance connected graph. first generalise notion true
constraint graph CSP patterns. call resulting graph negative structure graph.
Definition 5.1. Let binary pattern. vertices negative structure
graph G variables . pair vertices edge G form
scope whose constraint pattern assigns least one tuple value F . say
pattern neg-connected negative structure graph connected. case
negative patterns, use simpler term connected instead neg-connected.
Pattern 9 (Valency), Pattern 10 (Path) Pattern 11 (Valency+Path)
connected. Note pattern connected may occur connected pattern
(and vice versa). Pattern 8 shows Cycle(6) connected. one example
generic pattern Cycle(k) k 2. structure Cycle(k)
variables distinct, except special case k = 2 structure also
includes NEQ(c, c0 ). additional requirement means Cycle(2) composed
single binary constraint pattern containing two distinct disallowed tuples. following
theorem uses patterns show patterns intractable.

63

fiCohen, Cooper, Creed, Marx & Salamon

Pattern 11 Valency+Path
v1

v2
w1
v3

w2

w3

x

NEQ(v1 , v2 , v3 ), NEQ(w1 , w2 , w3 ), NEQ(x, w2 )
Theorem 5.2. Let X finite set neg-connected binary patterns. If, X ,
least one Cycle(k) (for k 2), Valency, Path, Valency+Path occurs
, X intractable.
Proof. Let X finite set neg-connected negative binary patterns let `
number variables largest element X .
Assuming least one four patterns occurs X , construct
class CSPs element X occurs polynomial-time
reduction well-known NP-complete problem 3SAT (Garey & Johnson, 1979).
construction involve three gadgets, examples shown Figure 1.
gadgets serve particular purpose:
1. cycle gadget, shown Figure 1(a) special case 4 variables, enforces
cycle Boolean variables (v1 , v2 , . . . , vr ) take value.
2. clause gadget Figure 1(b) equivalent clause v1 v2 v3 , since vC
value domain one three vi variables set true.
obtain 3-clauses three variables inverting domains vi
variables.
3. line gadget Figure 1(c), imposes constraint v1 v2 . also used
impose logically equivalent constraint v2 v1 .
cycle gadget connected clause gadget via line gadgets. three types
gadgets specified ensure one negative edge adjacent
vertex coloured microstructure, except cycle gadget connected line
gadget.
Now, suppose instance 3SAT n propositional variables
X1 , . . . , Xn clauses C1 , . . . , Cm .
begin construction CSP instance P solve 3SAT instance using
n copies cycle gadget (Figure 1(a)), m(` + 1) variables. = 1, . . . , n,
m(`+1)
variables along ith copy cycle denoted (vi1 , vi2 , . . . , vi
).
64

fiTractability CSP Classes Defined Forbidden Patterns

v1

F

v2

vC
v2

v1

v3


F
v3

F

v4
(a)

(b)


F
v1

v2
(c)

Figure 1: (a) Making copies variable (v1 = v2 = v3 = v4 ). (b) Imposing
ternary constraint vC = v1 v2 v3 . (c) line constraints length 4
imposes v1 v2 .

solution CSP instance P constraints,
variables vij , j = 1, . . . , m(` + 1) must value, di . therefore consider
vij copy Xi .
Consider clause Cw . eight cases consider similar
show details one case. Suppose Cw Xi Xj Xk .
build clause gadget (Figure 1(b)) three Boolean variables ciw , cjw ckw
invert domain ckw since occurs negatively Cw . solution
constructed CSP must satisfy s(ciw ) s(cjw ) s(ckw ) = .
complete insertion Cw CSP instance adding line gadgets
length ` + 1 (Figure 1(c)). connect cycle gadgets corresponding Xi , Xj Xk
w(`+1)
clause gadget clause Cw since Xi , Xj Xk occur Cw . connect vi
w(`+1)
ciw since Xi positive Cw , s(ciw ) = possible s(vi
) = ,
65

fiCohen, Cooper, Creed, Marx & Salamon

w(`+1)

solution s. Similarly, connect vj
cjw . Finally, since Xk occurs negatively
Cw , impose line constraints direction. ensures s(ckw ) = F
w(`+1)
possible s(vk
) = F . Imposing constraints ensures solution
possible least one cycles corresponding variables Xi , Xj , Xk
assigned value would make corresponding literal Cw true.
continue construction clause 3SAT instance. Since ` constant,
clearly polynomial reduction 3SAT.
show CSP instance P constructed manner described cannot contain pattern X . showing neg-connected
pattern containing Cycle(k) (for 2 k `), Valency, Path, Valency+Path
occur instance. sufficient show CSP instance P contain
patterns X .
CSP instance P constraint contains one disallowed tuple. Thus,
X Cycle(2) cannot occur P . Furthermore, P built
cycles length m(` + 1) paths length ` + 1, cannot contain cycles less
` + 1 vertices. Thus, since ` maximum number vertices element X ,
follows X Cycle(k) , k 3, occur P .
define valency variable x number distinct variables share
constraint pattern x. Suppose Valency , X neg-connected.
possible require variable valency four , pair
variables valency three connected path length ` negative structure
graph . Certainly P variables valency four. Moreover, fact P
built using paths length ` + 1 means two valency three variables joined
path length `. Thus, X occur P Valency .
Next, consider case Path , X neg-connected. must
two distinct (but possibly overlapping) three-variable lines (with disallowed tuples
constraint patterns match domain values) separated ` variables.
place disallowed tuples meet P connect line gadget
cycle gadget. connection sites always distance greater `,
conclude 6 P whenever Path .
Finally, consider case Valency+Path , X neg-connected.
Here, must variable valency least 3 path constraint patterns
three variables intersecting disallowed tuples, must connected path
less ` variables negative structure graph . observed above,
places P disallowed tuples meeting line gadget meets
cycle gadget, path least ` variables one points
every variable valency 3. Thus, 6 P whenever Valency+Path .
remains consider sets negative binary patterns could tractable.
this, need define pivot patterns, Pivot(r), contain every tractable negative
binary pattern.
Definition 5.3. Let V = {p} {v1 , . . . , vr } {w1 , . . . , wr } {x1 , . . . , xr }, = {a, b}
= {NEQ(p, v1 , . . . , vr , w1 , . . . , wr , x1 , . . . , xr )}. define pattern Pivot(r) =
66

fiTractability CSP Classes Defined Forbidden Patterns

Pattern 12 Pivot(3)
v3

v2

v1
w1

w2

w3

p

x3

x2

x1

b

NEQ(p, v1 , v2 , v3 , w1 , w2 , w3 , x1 , x2 , x3 )

hV, D, Cp Cv Cw Cx , Si,
Cp = {h(p, v1 ), ab , h(p, w1 ), ab , h(p, x1 ), bb i}
Cv = {h(vi , vi+1 ), ab | = 1, . . . , r 1}
Cw = {h(wi , wi+1 ), ab | = 1, . . . , r 1}
Cx = {h(xi , xi+1 ), ab | = 1, . . . , r 1}
ab (a, b) = F , ab (s, t) = U (for (s, t) 6= (a, b)), bb (b, b) = F , bb (s, t) = U
(for (s, t) 6= (b, b)). pattern Pivot(r) structure variables
distinct. See Pattern 12 example, Pivot(3).
say pattern variables v1 , . . . , vr distinct-variable pattern
structure includes NEQ(v1 , . . . , vr ). following proposition characterises sets
connected binary flat negative distinct-variable patterns Theorem 5.2 prove
intractable.
Proposition 5.4. connected binary flat negative distinct-variable pattern either
contains Cycle(k) (for k 3), Valency, Path, Valency+Path,
occurs Pivot(r) integer r ||.
Proof. Suppose contain patterns Valency, Cycle(k) (for k 3),
Path, Valency+Path. Recall valency variable x number distinct
variables share constraint pattern x. Since contain Valency
contain one variable valency three variables must valency
two. Moreover, since Cycle(k) 6 k 3, negative structure graph
contain cycles. Thus, since connected, negative structure graph
consists three disjoint paths joined single vertex. two disallowed tuples
67

fiCohen, Cooper, Creed, Marx & Salamon

distinct scopes intersect, call union scopes footprint
intersection. fact negative structure graph acyclic
contain Path means pairs intersecting disallowed tuples must
footprint. Moreover, fact contain Valency+Path means
intersections must occur variable valency 3, exists. fact
flat negative means renaming-extension pair disallowed tuples ha, bi,
hc, di scope hu, vi merged domain-renaming function t,
i.e. t(hu, ai) = t(hu, ci) t(hv, bi) = t(hv, di). follows occurs Pivot(r),
r ||.
Corollary 5.5. Let X finite set connected binary flat negative distinct-variable
patterns. CSP(X ) tractable X occurs Pivot(r),
integer r ||.
prove result patterns necessarily distinct-variable.
Corollary 5.6. Let X finite set connected binary flat negative patterns.
CSP(X ) tractable X occurs Pivot(r), integer
r ||.
Proof. connected binary flat negative pattern, let dv() denote set connected
binary flat negative distinct-variable patterns occurs,
domain || variables. use dv(X ) denote union sets dv()
X .
Lemma 3.10, CSP() CSP(dv()). every CSP instance P P ,
P dv(). follows CSP(dv()) CSP(), hence
CSP() = CSP(dv()). Since CSP(X ) intersection CSP() X
CSP(dv(X )) intersection CSP(dv()) X , CSP(X ) =
CSP(dv(X )).
Corollary 5.5, CSP(dv(X )) tractable pattern dv(),
X , occurs Pivot(r) r | |. But, definition dv(), occurs
| | ||. Therefore, CSP(X ) tractable occurs Pivot(r) integer
r ||.
arbitrary (not necessarily flat negative) binary CSP pattern , denote
neg() flat negative pattern obtained replacing truth-values
U constraint patterns ignoring structure beyond disequalities
variables. Recall structure flat pattern contains disequality relations
variables, neg() flat pattern definition. set patterns X ,
neg(X ) naturally defined set neg(X ) = {neg() : X }. Clearly CSP(neg(X ))
CSP(X ). following result follows immediately Corollary 5.6. provides
necessary condition tractability general patterns.
Corollary 5.7. Let X finite set binary patterns X , neg()
connected. CSP(X ) tractable X neg() occurs
Pivot(r), integer r ||.
68

fiTractability CSP Classes Defined Forbidden Patterns

6. Pivot Theorem
Theorem 6.1. Pivot(r) tractable r 1.
theorem together Corollary 5.6 immediately provides dichotomy finite
sets connected binary flat negative patterns. section devoted proof
theorem (which call pivot theorem). conclude section giving
dichotomy finite sets flat negative patterns necessarily connected.
need definitions graph theory.
Definition 6.2. subdivision graph G graph obtained replacing edges
G simple paths.
minor graph G graph obtained G deleting edges, contracting
edges removing isolated vertices. graph H topological minor graph G
subdivision H subgraph G.
need use following well-known theorem Robertson Seymour (1986).
Theorem 6.3. every planar graph H integer k > 0 graph
contain H minor, tree width k.
particular, graph large tree width, contains large grid minor.
section consider hexagonal grid minors instead (see Figure 2). reason
well-known fact graph maximum degree three minor another graph,
topological minor latter notion convenient proofs.
illustrated Figure 2, h hexagonal grid graph composed hexagons honeycomb
pattern: width h number hexagons horizontal vertical directions.
Definition 6.4. Let g : r N every graph tree width least g(r) contains
3(r + 4) hexagonal grid topological minor.
Let us observe following simple property first:
Lemma 6.5. three degree three vertices hexagonal grid width 3r begin disjoint
paths length r.
Proof. vertex different row simply choose path
along row (in direction away nearest boundary grid). See vertices
a, b c Figure 2 visualise typical situation.
Otherwise may possible, rotating grid 120 240 degrees get
three vertices lie different rows. cannot separate vertices rotating
corners equilateral triangle, x, y, z p, q, r diagram.
triangle interior row, Row 4 x, y, z diagram,
extend two vertices along row drop third interior row
along row. Thus, example, path beginning would drop one row
continue along Row 4.
remaining case three vertices form equilateral triangle occupying
two adjacent rows, like p, q, r diagram. case orientation
two vertices row lie along edge
69

fiCohen, Cooper, Creed, Marx & Salamon

grid. diagram rotate either 120 240 degrees achieve p, q, r.
extend two three vertices along row third shift away
centre triangle order find empty row along path
extended.
Row 6
b



x

Row 5
Row 4

z

Row 3

c
Row 2
Row 1
Row 0

r
p

q

Figure 2: hexagonal grid width 6 rows picked bold numbered.
following combinatorial result crucial algorithm, interesting
right:
Lemma 6.6. Let G 3-connected graph tree width least g(r) let a, b, c
distinct vertices G. G contains 3 pairwise vertex disjoint paths starting a, b, c,
respectively, length r.
Proof. Let H 3(r + 4) hexagonal grid. definition g(r), graph G contains
H topological minor. Note H contains vertices degree 2 boundary,
cause complications proof. avoid complication, observe
H subdivision graph H whose every vertex degree 3 focus
graph H instead.

Let HG
denote subdivision H appearing G let denote vertices

degree three HG
. Mengers theorem (Dirac, 1966) vertex-disjoint paths Pa ,
Pb , Pc G a, b, c distinct vertices sa , sb , sc S, respectively. Choose paths

way total number edges used HG
minimized.

Let x, two vertices correspond adjacent vertices H . means

HG
contains path Q endpoints x whose internal vertices disjoint
S. Suppose that, say, Pa contains internal vertex Q. claim either (1)
x, {sa , sb , sc } (2) sa {x, y} Pb , Pc disjoint Q. Suppose (1)
hold, say, x 6 {sa , sb , sc }. Consider internal vertex q Q closest x used
70

fiTractability CSP Classes Defined Forbidden Patterns

one paths. reroute path q x without using edges

outside HG
. would create new set paths smaller number edges outside


HG , unless rerouted path use edges outside HG
q.
possible path goes q Q. means one path
intersecting Q: path intersects Q q x definition q
path uses vertices q y. Thus case (2) holds.
Lemma 6.5 three independent paths length r + 4 vertices sa , sb
sc (non subdivided) hexagonal grid H, correspond paths Xa , Xb , Xc

HG
. use paths create three independent paths length least r a, b
c G. definition, path Xa go sb sc . Therefore, claim
previous paragraph, Xa disjoint Pb Pc : Xa uses path Q x
y, sb , sc 6 {x, y} means neither (1) (2) happen Pb Pc intersects
Q. Xa disjoint Pa well, create new path Ta simply concatenating
Pa Xa . Otherwise, way Xa intersect Pa first subdivided edge
H Xa goes (this place case (2) claim happen).
case, create new path Ta following Pa meets subdivided edge
following Xa . edge H corresponds 4 edges H, path Ta could
meet 4 edges H fewer Xa does. path Ta will, either case, meet
least r subdivided edges H length least r. build Tb Tc
analogous fashion.
require following technical lemma proofs.
Lemma 6.7. Let P binary CSP instance. Suppose assignment x,
d0 extend solution P solution assigning x
d0 y. path proper binary constraints x y. Furthermore,
path first constraint along path disallows tuple hd, d1
last constraint disallows tuple hd2 , d0 i.
Proof. Let Sx solution P including assignment x similarly let Sy
solution P including assignment d0 y.
Define graph G variables P . edge x variable z
assignment x incompatible domain value z. Similarly,
edge variable z assignment d0 incompatible
domain value z. Finally edge two variables
x constraint proper.
Let Cx component G containing x. Define assignment variables
P setting S(z) = Sx (z) z Cx S(z) = Sy (z) otherwise. solution
P since possible unsatisfied constraint would variable Cx
variable Cx , choice Cx cannot happen.
hypothesis, know S(y) 6= d0 required path proper
binary constraints.
Note Lemma 6.7 binary constraint x forbids
assigning x d0 y, setting d1 = d0 d2 = yields required path
proper binary constraints, length one.
first show CSP(Pivot(r)) tractable special case restricted structure.
71

fiCohen, Cooper, Creed, Marx & Salamon

Lemma 6.8. subclass CSP(Pivot(r)) consisting instances:
arc-consistent binary reduction;
unary constraints variables degree two constraint graph;
true constraint graph subdivided three-connected graph,

time complexity n3 dg(r)+1 .

Proof. Let P instance satisfying conditions lemma. time nd2 ,
join binary constraints along subdivided edge eliminating intermediate variables
go, obtain instance P 0 , whose constraint graph three-connected,
may improper binary constraints arbitrary unary constraints. Let G denote
true constraint graph P G0 three-connected graph obtained G
contracting subdivided edges. true constraint graph P 0 subgraph G0
vertex-set.

solve P 0 CSP(Negtrans) time n2 d3 (n + d) (Cooper & Zivny, 2011b).

clearly n3 dg(r)+1 since g(r) 3 r. Furthermore, G0 tree width

g(r) P 0 solved time ndg(r)+1 (Dechter & Pearl, 1989). either
case solution extended solution original instance P time O(nd).
remaining case consider Negtrans occurs P 0 tree
width G0 also least g(r). complete proof deriving contradiction
P 6 CSP(Pivot(r)), order show case cannot occur.
Suppose Negtrans occurs P 0 variables a, b, c values da , db , dc :
da

dc



c
db
b

Lemma 6.6, G0 contains 3 vertex-disjoint paths Ta , Tb , Tc starting a, b, c, respectively, length least r. Recall true constraint graph G
original instance P subdivided three-connected graph P 0 obtained
P joining binary constraints along subdivided edges. Let Ta , Tb , Tc denote paths
G corresponding Ta , Tb , Tc G0 . Recall also Negtrans occurs P 0 variables
a, b, c values da , db , dc .
let c first vertices along subdivided edges b c
G. embedding Negtrans P 0 shows hdb , da disallowed join
arc-consistent path b a. Since path is, construction, subdivision
edge P 0 know unary constraints occur internal vertices. also know,
arc consistency binary constraints, assignments = da b = db
extend consistent assignment path b. So, Lemma 6.7 know
value da domain hdb , da disallowed P
72

fiTractability CSP Classes Defined Forbidden Patterns

constraint b a. Similarly value dc hdb , dc disallowed P
constraint c. appending path b path Ta
path b c Tc , together Tb , obtain three independent paths length
least r proper constraints P , beginning variable b, two beginning constraints
disallowing tuple value db b. shown Pivot(r) indeed occur
P done.
CSP(Pivot(r)) places upper bound length chain dependencies
may followed discard partial solution cannot extended solution.
Informally speaking, forbidding Pivot(r) pattern bounds amount local search
may done extending partial solution larger partial solution.
amount effort may required increases length chains inference,
worst-case behaviour quantified precisely following result. first
require definitions.
Definition 6.9. Let G graph U subset vertices G. induced graph
G[U ] G U graph vertex set U whose edges edges G
connect two vertices U .
graphs G = hV, Ei G0 = hV 0 , E 0 define G G0 = hV V 0 , E E 0 i.
graph G say hU1 , U2 separation G = G[U1 ] G[U2 ] neither
U1 , U2 subset other. separator separation hU1 , U2 U1 U2
order |U1 U2 |. minimal separator one minimal order.
torso U1 separation hU1 , U2 obtained induced graph G[U1 ]
adding every edge vertices separator hU1 , U2 i.

Theorem 6.10. class Pivot(r)-free instances solvable time n3 dg(r)+3 .
Proof. prove result induction number variables.
base case straightforward. instance fewer g(r)+3 variables,
clearly solve exhaustive search time n3 dg(r)+3 . inductive case

assume solve smaller instances n < k variables time n3 dg(r)+3 .
Let P Pivot(r)-free instance n = k variables. First make P arc-consistent
time O(n2 d2 ) (Bessiere, Regin, Yap, & Zhang, 2005). Since P arc-consistent, unary
constraints longer effect. Remove unary improper binary constraints
P time O(n2 d2 ). Let G true constraint graph resulting instance, P 0 .
G separation order two either three-connected three
vertices. three-connected case solve P 0 , hence P , time n3 dg(r)+1
Lemma 6.8. P three variables trivial solve time O(d3 ).
So, assume G separation order two. definition torso
size-2 separator torso size-2 separator G. Hence find size-2 separation
hU1 , U2 torso U1 separation order two. assume
torso U1 either three-connected three vertices.
consider separator = U1 U2 hU1 , U2 i. empty P 0 composed two smaller independent
Pivot(r)-free instances solved time

3
g(r)+3
3
g(r)+3
n1
+ n2
n1 + n2 = n 1 n1 , n2 < n. follows

solve P time n3 dg(r)+3 , done.
73

fiCohen, Cooper, Creed, Marx & Salamon

= {m} consider structure G[U1 ]. three-connected,
vertex degree least three. case add unary constraint
and, Lemma 6.8, solve instance time n3 dg(r)+1 . Hence find, time

dn3 dg(r)+1 , values variable extend variables U1 . Adding restriction unary constraint variable leaves induced instance U2 Pivot(r)-free

see, induction, solve P 0 time dn3 dg(r)+1 + (n 1)3 dg(r)+3 =

n3 dg(r)+3 , done.
Finally must consider = {x, y}. Since minimal know G[U2 ]
connected path x U2 . Denote Q CSP instance
induced G[U1 ], together path U2 x y. constraint graph
Q subdivision torso U1 either three-connected three
vertices. latter case Q tree width two so, addition unary
constraints x y, solved time O(n2 d3 ) (Dechter & Pearl, 1989). torso
U1 three-connected degrees x Q least three.
addition
unary constraints x can, Lemma 6.8, solve case time
n3 dg(r)+1 . Hence solve Q possible unary constraints x

allow one value x y, time d2 n3 dg(r)+1 .
value variable x know whether extends solution
variables Q. Similarly, value variable know whether extends
solution variables Q. express two restrictions unary constraints,
u(x) u(y) x y. Lastly find binary constraint c(x, y) x
specifies precisely pairs values, allowed u(x) u(y), extend variables
U1 . obtain solving subdivided three-connected instance seeing
pairs disallowed subdivided edge U2 pairs set constraint
relation c(x, y) F .
u(x) allows values x P solution stop.
consider instance R, induced P 0 U2 together constraints
u(x), u(y) c(x, y). construction, P solution R solution.
Pivot(t) occurring R must use pair values disallowed c(x, y) since cannot
occur P 0 . Suppose hd, d0 disallowed c(x, y). follows assignment
x, d0 extend solution Q assigning x d0
extend solution problem induced P 0 U1 . Lemma 6.7 path
proper constraints x G[U1 ]. Furthermore, first constraint along
path disallows tuple hd, d1 last constraint disallows tuple hd2 , d0 i.
follows cannot embed Pivot(r) instance R induced U2 together
constraint c(x, y) (otherwise would able embed
instance P ).
Since R CSP(Pivot(r)), solve time n3 dg(r)+3 inductive hypoth

esis. Thus, final case, complexity d2 n3 dg(r)+1 + n3 dg(r)+3 = n3 dg(r)+3
done.
Theorem 6.1 important gives us tractable class CSPs defined forbidding
negative pattern which, unlike CSP(Tree), contains problems unbounded tree width,
cannot captured structural tractability. true even Pivot(1).
example class CSP instances CSP(Pivot(1)) unbounded tree width,
consider n-variable CSP instance Pn domain {1, . . . , n} whose constraint graph
74

fiTractability CSP Classes Defined Forbidden Patterns

complete graph and, pair distinct values i, j {1, . . . , n}, constraint
variables vi , vj disallows single pair assignments (hvi , ji , hvj , ii). Since assignment hvi , ji occurs single disallowed tuple, Pivot(1) occur Pn , hence
Pn CSP(Pivot(1)). produce example class instances CSP(Pivot(1))
unbounded tree width CSP(Negtrans), modify Pn
introducing Boolean variable vij pair < j replacing constraint
variables vi , vj constraints vi , vij vj , vij : former disallowing single pair
assignments (hvi , ji , hvij , 0i) latter pair assignments (hvj , ii , hvij , 0i).
pattern Negtrans occurs triple assignments (hvi , ji , hvij , 0i , hvj , ii).
dichotomy finite sets connected binary flat negative patterns follows
directly Theorem 6.1 Corollary 5.6.
Theorem 6.11. Let X finite set connected binary flat negative patterns. X
tractable X contained Pivot(r), integer
r ||.
Informally speaking, dichotomy states bounding length problematic
Pivot(r)-style inference chains leads tractability, moreover class
instances defined finite set forbidden flat patterns tractable, must avoid
problematic inference chains form.
dichotomy easily extends patterns necessarily connected.
negative pattern connected, decomposed connected patterns corresponding connected components negative structure graph . call
patterns connected components .
Corollary 6.12. Let X finite set binary flat negative patterns. X tractable
X , connected components contained
Pivot(r), integer r ||.
Proof. Let X finite set binary flat negative patterns. Let CC() represent set
connected components pattern , CC(X ) union sets CC() ( X ).
Suppose X tractable. Consider arbitrary subset X 0 CC(X )
set X 0 contains exactly one connected component pattern X . Lemma 3.10
CSP(X 0 ) CSP(X ), hence X 0 also tractable. Therefore, Corollary 5.6,
pattern 0 X 0 occurs Pivot(r), integer r |0 |. way
true possible choices X 0 X connected
components occur Pivot(r), integer r ||.
hand, suppose X , connected components
X occurs Pivot(r), r ||. Let k number connected components
. occurs disjoint union k copies Pivot(r). tractable
Theorem 6.1 k 1 applications Lemma 4.2. follows , hence X ,
tractable.

7. Conclusion
paper described framework identifying classes CSPs terms forbidden
patterns, used tool identifying tractable classes CSP. gave several
examples small patterns used define tractable classes CSPs.
75

fiCohen, Cooper, Creed, Marx & Salamon

search general result, restricted special case binary
patterns binary CSPs. Theorem 5.2 showed CSP(X ) NP-hard every
pattern set X contains least one four patterns (Patterns 8, 9, 10, 11). Moreover,
showed binary flat negative pattern contain patterns
must contained within (possibly several copies of) special type pattern called
pivot. Hence, contained (several copies of) pivot necessary condition
pattern tractable. showed forbidding pivot pattern defines
tractable class.
Beyond dichotomy binary flat negative patterns, interesting see
new tractable classes defined general binary patterns non-binary
patterns. particular, important area future research determining maximal
tractable classes problems defined patterns fixed size (given number
variables number variable-value assignments). avenue future research
characterisation complexity patterns involving structure uses
disequalities groups variables, total ordering variables.

Acknowledgments
authors acknowledge support ANR Project ANR-10-BLAN-0210, EPSRC grants
EP/F011776/1 EP/I011935/1, ERC Starting Grant PARAMTIGHT (No. 280152),
EPSRC platform grant EP/F028288/1.

References
Bessiere, C., Regin, J.-C., Yap, R. H. C., & Zhang, Y. (2005). optimal coarse-grained
arc consistency algorithm. Artificial Intelligence, 165 (2), pp. 165185. doi:10.1016/
j.artint.2005.02.004.
Bulatov, A., Jeavons, P., & Krokhin, A. (2005). Classifying complexity constraints
using finite algebras. SIAM Journal Computing, 34 (3), pp. 720742. doi:10.
1137/S0097539700376676.
Bulatov, A. A. (2003). Tractable conservative constraint satisfaction problems. LICS 03:
Proceedings 18th IEEE Symposium Logic Computer Science, pp. 321330.
doi:10.1109/LICS.2003.1210072.
Bulatov, A. A. (2006). dichotomy theorem constraint satisfaction problems 3element set. Journal ACM, 53 (1), pp. 66120. doi:10.1145/1120582.1120584.
Cohen, D., & Jeavons, P. (2006). complexity constraint languages. Rossi et al.
(Rossi et al., 2006), chap. 8, pp. 245280.
Cohen, D. A. (2003). new class binary CSPs arc-consistency decision
procedure. CP 03: Proceedings 9th International Conference Principles
Practice Constraint Programming, No. 2833 Lecture Notes Computer
Science, pp. 807811. Springer-Verlag. doi:10.1007/978-3-540-45193-8_57.
Cooper, M. C., & Escamocher, G. (2012). Dichotomy 2-Constraint Forbidden CSP
Patterns. AAAI 12: Proceedings Twenty-Sixth AAAI Conference Ar76

fiTractability CSP Classes Defined Forbidden Patterns

tificial Intelligence. Available from: https://www.aaai.org/ocs/index.php/AAAI/
AAAI12/paper/view/4960/5225.
Cooper, M. C., Jeavons, P. G., & Salamon, A. Z. (2010). Generalizing constraint satisfaction
trees: Hybrid tractability variable elimination. Artificial Intelligence, 174 (9
10), pp. 570584. doi:10.1016/j.artint.2010.03.002.
Cooper, M. C., & Zivny, S. (2011a). Hierarchically nested convex VCSP. CP 11: Proceedings 17th International Conference Principles Practice Constraint
Programming, pp. 187194. Springer-Verlag. doi:10.1007/978-3-642-23786-7_16.
Cooper, M. C., & Zivny, S. (2011b). Hybrid tractability valued constraint problems.
Artificial Intelligence, 175 (910), pp. 15551569. doi:10.1016/j.artint.2011.02.
003.
Costa, M.-C. (1994). Persistency maximum cardinality bipartite matchings. Operations
Research Letters, 15 (3), pp. 143149. doi:10.1016/0167-6377(94)90049-3.
Dalmau, V., Kolaitis, P. G., & Vardi, M. Y. (2002). Constraint satisfaction, bounded
treewidth, finite-variable logics. CP 02: Proceedings 8th International Conference Principles Practice Constraint Programming, No. 2470
Lecture Notes Computer Science, pp. 310326. Springer-Verlag. doi:10.1007/
3-540-46135-3_21.
Dechter, R., & Pearl, J. (1987). Network-based heuristics constraint-satisfaction problems. Artificial Intelligence, 34 (1), pp. 138. doi:10.1016/0004-3702(87)90002-6.
Dechter, R., & Pearl, J. (1989). Tree clustering constraint networks. Artificial Intelligence, 38 (3), pp. 353366. doi:10.1016/0004-3702(89)90037-4.
Dirac, G. A. (1966). Short proof Mengers graph theorem. Mathematika, 13 (1), pp.
4244. doi:10.1112/S0025579300004162.
Freuder, E. C. (1990). Complexity K-Tree Structured Constraint Satisfaction Problems.
AAAI 90: Proceedings Eighth National Conference Artificial Intelligence,
pp. 49. Available from: http://www.aaai.org/Library/AAAI/1990/aaai90-001.
php.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide Theory
NP-Completeness. W. H. Freeman, San Francisco, CA.
Gottlob, G., Leone, N., & Scarcello, F. (2002). Hypertree decompositions tractable
queries. Journal Computer System Sciences, 64 (3), pp. 579627. doi:10.
1006/jcss.2001.1809.
Green, M. J., & Cohen, D. A. (2003). Tractability approximating constraint languages.
CP 03: Proceedings 9th International Conference Principles Practice
Constraint Programming, Vol. 2833 Lecture Notes Computer Science, pp. 392
406. Springer-Verlag. doi:10.1007/978-3-540-45193-8_27.
Grohe, M. (2006). structure tractable constraint satisfaction problems. MFCS 06:
Proceedings 31st Symposium Mathematical Foundations Computer Science, Vol. 4162 Lecture Notes Computer Science, pp. 5872. Springer-Verlag.
doi:10.1007/11821069_5.
77

fiCohen, Cooper, Creed, Marx & Salamon

Grohe, M. (2007). complexity homomorphism constraint satisfaction problems
seen side. Journal ACM, 54 (1), pp. 124. doi:10.1145/
1206035.1206036.
Gyssens, M., Jeavons, P. G., & Cohen, D. A. (1994). Decomposing constraint satisfaction
problems using database techniques. Artificial Intelligence, 66 (1), pp. 5789. doi:
10.1016/0004-3702(94)90003-5.
Jeavons, P., Cohen, D., & Gyssens, M. (1997). Closure properties constraints. Journal
ACM, 44 (4), pp. 527548. doi:10.1145/263867.263489.
Jeavons, P. G., & Cooper, M. C. (1995). Tractable constraints ordered domains. Artificial
Intelligence, 79 (2), pp. 327339. doi:10.1016/0004-3702(95)00107-7.
Jegou, P. (1993). Decomposition domains based micro-structure finite
constraint-satisfaction problems. AAAI 93: Proceedings Eleventh National Conference Artificial Intelligence, pp. 731736. Available from: http:
//www.aaai.org/Library/AAAI/1993/aaai93-109.php.
Marx, D. (2010a). beat treewidth?. Theory Computing, 6 (1), pp. 85112.
doi:10.4086/toc.2010.v006a005.
Marx, D. (2010b). Tractable hypergraph properties constraint satisfaction conjunctive queries. STOC 10: Proceedings 42nd ACM symposium Theory
computing, pp. 735744. ACM. doi:10.1145/1806689.1806790.
Regin, J.-C. (1994). filtering algorithm constraints difference CSPs. AAAI 94:
Proceedings Twelfth National Conference Artificial Intelligence, Vol. 1, pp.
362367. Available from: http://www.aaai.org/Library/AAAI/1994/aaai94-055.
php.
Robertson, N., & Seymour, P. D. (1986). Graph minors. V. Excluding planar graph. Journal Combinatorial Theory, Series B, 41, pp. 92114. doi:10.1016/0095-8956(86)
90030-4.
Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006). Handbook Constraint Programming.
Foundations Artificial Intelligence. Elsevier.
Salamon, A. Z., & Jeavons, P. G. (2008). Perfect constraints tractable. CP 08:
Proceedings 14th International Conference Principles Practice Constraint Programming, Vol. 5202 Lecture Notes Computer Science, pp. 524528.
Springer-Verlag. doi:10.1007/978-3-540-85958-1_35.
van Hoeve, W. J. (2001). alldifferent Constraint: Survey. Proceedings 6th
Annual Workshop ERCIM Working Group Constraints. Available from:
http://arxiv.org/abs/cs/0105015v1.
Weigel, R., & Bliek, C. (1998). reformulation constraint satisfaction problems.
ECAI 98: Proceedings 13th European Conference Artificial Intelligence, pp.
254258.

78

fi
