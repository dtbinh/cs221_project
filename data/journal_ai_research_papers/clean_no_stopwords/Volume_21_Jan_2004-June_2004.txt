Journal Artificial Intelligence Research 21 (2004) 471-497Submitted 11/03; published 04/04Phase Transitions BackbonesAsymmetric Traveling Salesman ProblemWeixiong ZhangZHANG @ CSE . WUSTL . EDUDepartment Computer Science EngineeringWashington University St. LouisSt. Louis, MO 63130, U.S.A.Abstractrecent years, much interest phase transitions combinatorial problems.Phase transitions successfully used analyze combinatorial optimization problems,characterize typical-case features locate hardest problem instances. paper,study phase transitions asymmetric Traveling Salesman Problem (ATSP), NP-hard combinatorial optimization problem many real-world applications. Using random instances1,500 cities intercity distances uniformly distributed, empirically showmany properties problem, including optimal tour cost backbone size, experiencesharp transitions precision intercity distances increases across critical value. experimental results costs ATSP tours assignment problem agree theoreticalresult asymptotic cost assignment problem number cities goes infinity. addition, show average computational cost well-known branch-and-boundsubtour elimination algorithm problem also exhibits thrashing behavior, transitioningeasy difficult distance precision increases. results answer positively open question regarding existence phase transitions ATSP, provide guidance difficultATSP problem instances generated.1. Introduction OverviewPhase transitions combinatorial problems thrashing behavior similar phase transitionscombinatorial algorithms drawn much attention recent years (Gomes, Hogg, Walsh, &Zhang, 2001; Hogg, Huberman, & Williams, 1996; Martin, Monasson, & Zecchina, 2001).extensively studied so-called spin glass theory (Mezard, Parsi, & Virasoro, 1987)physics, phase transition refers phenomenon system properties change rapidlydramatically control parameter undergoes slight change around critical value.transitions appear large systems. larger system usually exhibits sharper abruptphase transitions, leading phenomenon crossover trajectories phase transitionssystems type different sizes.daily-life example phase transitions water changing ice (solid phase) water(liquid phase) steam (gas phase) temperature increases. shown manycombinatorial decision problems phase transitions, including Boolean satisfiability (Cheeseman, Kanefsky, & Taylor, 1991; Mitchell, Selman, & Levesque, 1992; Hogg, 1995; Selman &Kirkpatrick, 1996; Monasson, Zecchina, Kirkpatrick, Selman, & Troyansky, 1999), graph coloring (Cheeseman et al., 1991), symmetric Traveling Salesman Problem (deciding existence complete tour vising given set cities cost less specified value) (Cheeseman et al., 1991; Gent & Walsh, 1996a). Phase transitions used characterize typical-casec 2004 AI Access Foundation. rights reserved.fiZ HANGproperties difficult combinatorial problems (Gomes et al., 2001; Martin et al., 2001). hardestproblem instances decision problems appear often points phase transitions.Therefore, phase transitions used help generate hardest problem instances testing comparing algorithms decision problems (Achlioptas, Gomes, Kautz, & Selman, 2000;Cheeseman et al., 1991; Mitchell et al., 1992).Another important useful concept characterizing combinatorial problemsbackbone (Kirkpatrick & Toulouse, 1985; Monasson et al., 1999). backbone variable refersvariable fixed value among optimal solutions problem; backbonevariables collectively referred backbone problem. problem backbonevariable, algorithm find solution problem backbone variable setcorrect value. Therefore, fraction backbone variables, percentage variablesbackbone, reflects constrainedness problem directly affects algorithm searchingsolution. larger backbone, tightly constrained problem becomes.result likely algorithm set backbone variable wrong value, mayconsequently require large amount computation recover mistake.However, research phase transitions (particularly) backbones optimizationproblems limited, sharp contrast numerous studies phase transitionsbackbones decision problems, represented Boolean satisfiability (e.g., Cheeseman et al.,1991; Mitchell et al., 1992; Hogg, 1995; Selman & Kirkpatrick, 1996; Monasson et al., 1999).early work symmetric TSP introduced concept backbones (Kirkpatrick & Toulouse,1985). However, left question whether exists phase transition TSP,optimization version problem specific, open since 1985. One best (rigorous)phase-transition results obtained number partitioning (Borgs, Chayes, & Pittel, 2001),optimization problem. However, phase transition analyzed Borgs, Chayes, & Pittel (2001),also experimentally Gent & Walsh (1996b, 1998), existence perfect partitiongiven set integers, essence decision problem. addition, Gent & Walsh (1996b,1998) also studied phase transitions size optimal 2-way partition. relationshipphase transitions satisfiability, decision problem, maximum satisfiability,optimization problem, studied Zhang (2001). experimentally shown backbonemaximum Boolean satisfiability also exhibits phase transitions, emerging nonexistencealmost full size abruptly dramatically. addition, relationship backbonesaverage-case algorithmic complexity also considered (Slaney & Walsh, 2001).paper, investigate phase transitions asymmetric Traveling Salesman Problem. Traveling Salesman Problem (TSP) (Gutin & Punnen, 2002; Lawler, Lenstra, Kan, &Shmoys, 1985) archetypical combinatorial optimization problem one first NP-hardproblems studied (Karp, 1972). Many concepts, backbone (Kirkpatrick & Toulouse, 1985),general algorithms, linear programming (Dantzig, Fulkerson, & Johnson, 1959), branchand-bound (Little, Murty, Sweeney, & Karel, 1963), local search (Lin & Kernighan, 1973)simulated annealing (Kirkpatrick, Gelatt, & Vecchi, 1983) first introduced studied usingTSP. problem also often touchstone combinatorial algorithms. Furthermore,fact many real-world problems, scheduling routing, cast TSPsmade problem practical importance. paper, consider asymmetric TSP (ATSP),distance one city another may necessarily distancereverse direction. ATSP general ATSPs difficult solvesymmetric counterparts (Johnson, Gutin, McGeoch, Yeo, Zhang, & Zverovitch, 2002).472fiP HASE RANSITIONSBACKBONESATSPUsing general form problem, i.e., ATSP, provide positive answer longstanding open question posted Kirkpatrick & Toulouse (1985) regarding existence phasetransitions problem, disapprove claim made Kirkpatrick & Selman (1994)Traveling Salesman Problem clear phase transition.Specifically, using uniformly random problem instances 1,500 cities, empiricallyreveal average optimal tour length, accuracy effective lower-bound function problem (the assignment problem, see Martello & Toth (1987)), backboneATSP undergo sharp phase transitions. control parameter precision intercity distances typically represented maximum number digits distances. Noteresults algorithm independent properties problem. Furthermore, showaverage computational cost well-known branch-and-bound subtour elimination algorithm (Balas & Toth, 1985; Bellmore & Malone, 1971; Smith, Srinivasan, & Thompson, 1977)ATSP exhibits phase-transition thrashing behavior computational cost growsabruptly dramatically easy difficult distance precision increases. results leadpractical guidance generate large, difficult random problem instances purposealgorithm comparison.worthwhile mention besides results Kirkpatrick & Toulouse (1985)three additional pieces early empirical work related phase transitions TravelingSalesman Problem. research Zhang & Korf (1996) investigated effects two different distance distributions average complexity subtour elimination algorithmasymmetric TSP. main result average complexity algorithm controllednumber distinct distances random asymmetric TSP. extend resultSection 6. However, need caution results algorithm specific, maynecessarily reflect intrinsic features underlying problem. research phase transitions Cheeseman, Kanefsky, & Taylor (1991) studied decision version symmetricTSP (Cheeseman, 1991). thorough investigation issue also carried (Gent& Walsh, 1996a). Specifically, Gent & Walsh (1996a) analyzed probability tour whoselength less specific value exists given random symmetric euclidean TSP, showingprobability one-to-zero phase transition length desired tour increases. Notephase-transition results Cheeseman, Kanefsky, & Taylor (1991, 1996a) addressopen question Kirkpatrick & Toulouse (1985) optimization versionproblem. experimental results Gent & Walsh (1996a) also showed computationalcost branch-and-bound algorithm, unfortunately specified paper, exhibitseasy-hard-easy pattern.paper organized follows. Section 2, describe ATSP related problemcalled assignment problem (AP). investigate parameter controls phase transitionsSection 3, study various phase transitions ATSP Section 4. Section 5 analyzeasymptotic ATSP tour cost, AP cost precision AP heuristic function solvingATSP number cities grows large number. Section 6, describe well-knownsubtour elimination algorithm ATSP, analyze thrashing behavior algorithm.discuss related work Section 7 finally conclude Section 8.473fiZ HANG2. Asymmetric Traveling Salesman Problem Assignment ProblemGiven cities distance cost pair cities, Traveling Salesman Problem(TSP) find minimum-cost tour visits city returns starting city (Gutin& Punnen, 2002; Lawler et al., 1985). distance city citydistance, problem symmetric TSP (STSP). distance city citynecessarily equal reverse distance, problem asymmetric TSP (ATSP).ATSP difficult STSP, respect optimization approximation (Johnsonet al., 2002). TSPs important NP-hard problems (Garey & Johnson, 1979; Karp, 1972)many practical applications. Many difficult combinatorial optimization problems,vehicle routing, workshop scheduling computer wiring, formulated solvedTSPs (Gutin & Punnen, 2002; Lawler et al., 1985).ATSP formulated integer linear programming problem. Let ff setcities, fi set pairs cities, distance cost matrix specifyingdistance pair cities. following integer linear programming formulation ATSPwell known:fi !"$#&%(')+*,.-/021(1)subject) )+32? 73 ?89&:;ff&<) - 6571+3248/ >:;ff <) -=6571.324- A@CB BD 571E8FHGIff"1J$MK LN<- PORQ 18S T1U:Vff(2)(3)(4)(5)variables - take values zero one, - W5 arc + T1UN optimaltour,ff . Constraints (2) (3) restrict in-degree out-degree cityone, respectively. Constraints (4) impose subtour elimination constraints completetours allowed.ATSP closely related assignment problem (AP) (Martello & Toth, 1987),assign city another city, distancecost assignment,total cost assignments minimized. AP relaxation ATSPassignments need form complete tour. words, removing subtour eliminationconstraints (4) representation, integer linear programming formulationAP. Therefore, AP cost lower bound ATSP tour cost. AP solution happenscomplete tour, also solution ATSP. ATSP NP-hard, APsolved polynomial time, X&+NY precise (Martello & Toth, 1987).3. Control ParameterConsider two cases ATSP, one intercity distancesevery intercity distance distinct. first case, every complete tour going474fiP HASE RANSITIONSBACKBONESATSPcities optimal tour solution ATSP. backbone variable since removingone edge optimal solution prevent finding another optimal solution. ATSPcase easy; finding optimal solution require search all. addition, costoptimal solution also constant, times intercity distance. secondcase distances distinct, every complete tour covering cities high probabilitydistinct cost. Therefore, arc optimal solution almost surely backbone variableremoving may destroy optimal solution. addition, expected difficult findverify optimal solution among large number suboptimal solutions case.Therefore, significant differences two extreme cases. Oneimportant differences number distinct distances distance matrix .precisely, many important characteristics random ATSP, including size backbonecomplexity, determined fraction distinct distances among distances. denotefraction distinct distances distance matrix Z . particularly interested determining Z affects characteristics ATSP gradually increases zero,distances same, one, distances distinct.practice, however, directly control number fraction distinct distancesmatrix . Besides actual structures layouts cities, precision distancesaffects number distinct distances. precision number usually representedmaximal number digits allowed number. even use digitalcomputer solve ATSP, typically 32 bits (or 4 bytes) integers 64 bits (or 8bytes) double precision real numbers. result, number digits distances naturallygood choice control parameter.effect given number digits fraction distinct distances distance matrixrelative problem size . Consider matrix distances uniformly chosenintegers [ Q 1571]\N1.^.^.^71T_ 5` , range _ determined number digits . fixednumber digits , fraction distinct distances larger matrix obviously smallersmaller . Therefore, control parameter fraction Z distinct distancesmust function number digits number cities , denote ZS+b1]a. .find control parameter, consider number distinct distances given integerrange _ . problem finding number distinct distances equivalent followingbin-ball problem. given c balls _ bins, asked place balls bins.ball independently put one bins equal probability. interestedfraction bins empty placements. Here, asymmetric TSP cFd balls correspond total number nondiagonal distances matrix , _ binsrepresent possible integers choose from. Since ball (distance) thrown independentlyuniformly one _ bins (integers), probability one bin emptythrowing c balls 5 e5 5gf_hi . expected number occupied bins (distinct distances)simply _Mj5 e5 5gf_h ilk . Thus, expected fraction distinct distances matrix_ j 5 e5 5gf_h k(6)cNote c5 _uswv , since case expectationmonnumber distinct distances approaches c . hand, _ fixed, Zp+b1]aUqxs Qc goes infinity, since finite number _ bins occupied infinitemonpZ +b1]a.Uqrmonfixed, Zp+b1]aUqtsnumber balls case.475fiZ HANGFollowing convention practice, use decimal values distances. Thus _yz5 Q|{ ,number digits distances. turns plot ZS+b1]a. a.f}~2UU,+ ,relatively scale different problem sizes . shown Figure 1(a).means scaling function effective number digits b+h}~2U + . Functiona.f}~2 U + thus effective number digits controls fraction distinct distancesmatrix , denote x+b1]a . also means effective numberdigits two different problem sizes, say , range _different. two problems, _ needs 7 , respectively, giving 9 r .need point integer range _ also represented numberbases, binary. base use affect results quantitatively, introducesconstant factor results. fact, since alW}~2 U _h , _ range integerschosen, t+b1]a.hya.f}~2 U +u}~2,p_h , independent base valuesintercity distances.interesting note that, controlled effective number digits a.f}~2U,+ , fractiondistinct entities Z property similar phase transition, also shown Figure 1(a). largerproblem, sharper transition, exists crossover point among transitionsproblems different sizes. may examine phase-transition phenomenon closelyusing finite-size scaling. Finite-size scaling (Barber, 1983; Wilson, 1979) methodsuccessfully applied phase transitions similar systems different sizes. Based finitesize scaling, around critical parameter (temperature) . , problems different sizes tendindistinguishable except change scale given power law characteristic length,, problem size exponenttypically form ]rescaling factor. Therefore, finite-size scaling characterizes phase transition precisely aroundcritical point control parameter problem scales infinity. However, analysisrevealed scaling factor large exponent 9 (Zhang, 2003), indicating phasetransitions Figure 1(a) exactly follow power law finite-size scaling.find correct rescaled control parameter, reconsider (6). sv distance range_ grows problem size , i.e., _sv ;sv , rewrite (6)2A }|%(#*monZS+b1]a.Uq%|#}|Ac_c_5 j e5 5gf_h kj 5k 1(7)second equation follows }|%|# h e5 5gf_h. Since underlying controlparameter number digits, alW}~2U2_h , take -W}~2U2_hfc . AsymptoticallyRsv , c , leads -}|~2 U _h \}~2 U ++ \,N}~2 U + . Using - ,rewrite (7)2A }|%(#*monZS+b1]a.UqF5 Q7 j 5U2k(8)rescaled control parameter Ms v expected number distinct distances+ \,N}~2 U + . Therefore, critical point 2 rescaling factor }~2 U + . rescaledphase transition shown Figure 1(b), plots ZS+b1g+ \,N}~2JU,+ .Note number digits used intercity distances nothing measurementprecision distances. larger number digits, higher precision becomes.476fiP HASE RANSITIONSBACKBONESATSPaverage fraction distinct numbers(a) fraction distinct numbers10.80.60.40.20average fraction distinct numbersn = 100n = 500n = 1,000n = 1,5000.511.522.533.5effective number digits(b) rescaled fraction distinct numbers1.0n = 100n = 500n = 1,0000.8n = 1,5000.60.40.2054 3 2 1012rescaled effective number digits3Figure 1: (a) Average fraction distinct distances matrix , Zp+b1]a , controled effectivenumber digits, aS}~2U + , ;65 Q2Q 1] Q2Q 15 Q2Q2Q 5 Q2Q . (b) Average Zp+b1]afinite-size scaling, scaling factor + .]N}|~2 U + , t\ .agrees common practice using effective digits gain precision. Therefore,control parameter turn determined precision intercity distances.Finally, important note even though discussion section focused asymmetric cost matrices ATSP, arguments apply symmetric distance matrices477fiZ HANGsymmetric TSP well. is, c revised +d f7\ , asymptotically _ goesinfinity, }|~29U,cP\}~2U,+ , + \,N}|~29U,+ also rescaled control parameternumber distinct distances symmetric cost matrices.4. Phase Transitionscontrol parameter, effective number digits t+b1]a. intercity distances, identified, position investigate possible phase transitions ATSP relatedassignment problem.set forth investigate phase transitions, generated studied uniformly randomproblem instances 100-, 200-, 300- upto 1,000-cities 1,500-cities. Althoughresults 100-, 200-, 300-, 1,000-city well 1,500-city problems, make resultfigures readable, use data 100-, 500-, 1,000- 1,500-city problems reportresults. problem instances considered, intercity distances independently uniformlychosen [ Q 1571]\N1.^.^.^71T_ 5` given range _ , controlled number digits. varied 1.0 6.0 instances 1,000-cities 1.0 6.5 instances1,500-cities. digits incremented 0.1, i.e., used a57 Q 157|57157\N1.^.^.^ .4.1 Phase Transitions ATSPparticularly interested possible phase transitions ATSP cost, phase transitionsbackbones phase transitions numbers ATSP tours. results backbone shedlight intrinsic tightness constraints among cities precision distancemeasurement changes.4.1.1 ATSP C OSTphase transition ATSP tour cost, fi ! , control parameter ,effective number digits intercity distances. Figure 2(a) shows results 100-, 500-, 1,000and 1,500-city ATSP instances, averaged 10,000 instances data point. reportedtour costs obtained dividing integer ATSP tour costs I_ 5g ,number cities _ range intercity costs. Equivalently, intercity distance virtuallynconverted real value Q 15q .shown, ATSP tour cost increases abruptly dramatically effective number digits increases, exhibiting phase transitions. transitions become sharper problem becomeslarger, exist crossover points among curves different problem sizes. finite-sizescaling, determine critical value control parameter phase transitions occur. Following discussion Section 3, scaling factor form + ]N}~2 U + .numerical result indicated 57 Q \A Q Q2Q . thus use y5 show resultFigure 2(b). worthwhile mention AP cost follows almost phase-transitionpattern ATSP tour cost Figure 2.4.1.2 BACKBONEN UMBERPTIMAL OLUTIONSturn backbone ATSP, percentage directed arcs appearoptimal solutions. backbone also exhibits phase transitions effective number digitsdistances increases. result included Figure 3(a), data point averaged478fiP HASE RANSITIONSBACKBONESATSP(a) ATSP tour costaverage ATSP tour cost1.61.41.210.80.6100city500city1,000city1,500city0.40.200.511.52effective number digits(b) rescaled ATSP tour cost2.5normalized average ATSP tour cost10.80.60.4100city500city1,000city1,500city0.20210123rescaled effective number digitsFigure 2: (a) Average optimal ATSP tour cost. (b) Scaled normalized average optimal tour cost,rescaling factor + ,]N}~2U,+ >5 .10,000 problem instances. rescaled result shown Figure 3(b), critical point=5 . Interestingly, phase-transition pattern backbone follows similar trendfraction distinct entities distance matrix, shown Figure 1. addition, phasetransition patterns tour costs backbones similar, discussed Section 4.3.479fiZ HANG(a) fraction backboneaverage fraction backbone10.80.60.4100city500city1,000city1,500city0.200.511.522.5effective number digits(b) rescaled fraction backboneaverage fraction backbone10.80.60.4100city500city1,000city1,500city0.20210123rescaled effective number digitsFigure 3: (a) Average fraction backbone variables. (b) Rescaled average backbone fraction,rescaling factor + ]N}~2 U + t5 .fraction backbone variables related number optimal solutions problem.thus examined total number optimal solutions ATSP. done small ATSPs,10 cities 150 cities, finding optimal solutions larger problems computationallyexpensive. results averaged 100 trials data point. shown Figure 4,vertical axes logarithmic scale, number optimal tours also undergoes phase480fiP HASE RANSITIONS6BACKBONESATSP(a) number optimal solutions10average number optimal tours20city30city50city100city150city41021001060.511.522.53effective number bits(b) rescaled number optimal toursaverage number optimal tours1020city30city50city100city150city4102100101012rescaled effective number digitsFigure 4: (a) Average number optimal ATSP tours. (b) Rescaled average number optimalATSP tours, rescaling factor + ,]N}~2 U + >6572 Q Q2Q, .transition, exponential constant, number digits increases. Notenumber digits small, costly find optimal solutions, even relatively smallproblems.fraction backbone variables captures essence tightness constraints amongcities. intercity distances become distinct, number tours distinct lengths481fiZ HANGincreases. Consequently, number optimal solutions decreases fraction backbonevariables grows inversely. arcs part backbone, optimal solutions becomerestricted. result, number optimal solutions decreases. fraction backbonevariables increases approaches one, number optimal solutions decreases becomesone well, typically makes problem finding optimal solution difficult.4.1.3 E XISTENCE H AMILTONIAN C IRCUITZ ERO - COST E DGESprecision intercity distances low, likely ATSP tour, complete tourminimal cost among complete tours, cost zero, meaning exists Hamiltoniancircuit consisting zero-cost arcs. decision problem determine Hamiltonian circuitexists given ATSP. examined decision problem using set 10,000 probleminstances used Figures 2 3. result shown Figure 5. Notice although followsrescaling formula + 7]N}~2 U + , critical point transition, Q 2 ,different critical point gu5 phase transitions backbones ATSP tourcost, shown Figures 2 3.4.2 Quality AP Lower-bound Functionexistence Hamiltonian circuits zero-cost arcs also indicates number digitsintercity distances small, example, less 1.9 (or _ 7Q ) ;571] Q2Q ,AP ATSP costs zero, meaning two costs well. useful knowlikely AP cost equal ATSP tour cost; answers issue constitutes firststep elucidation accuracy AP lower-bound cost function.Given random distance matrix , likely AP cost ATSPtour cost effective number digits increases? answer question examiningprobability AP cost fi ! equal corresponding ATSP cost fiP !increases. Figure 6(a) shows results 100-, 500-, 1,000- 1,500-city ATSP instances,averaged set 10,000 instances Figure 2 data point. shownfigure, probability fi !Pfib l also experiences abrupt dramatic phasetransitions. Figure 6(b) shows phase transitions finite-size scaling, critical point57|5 Q Q2Q .results Figure 6 also imply quality AP lower-bound function degradesdistance precision increases. degradation also follow phase-transition process.verified Figure 7, using data set problem instances. Notecritical point phase transition accuracy AP Q , differentcritical point >57|5 phase transition probability fi !"$fi ! .4.3 Many Phase Transitions?far, seen many phase transitions different features ATSP related assignment problem. Qualitatively, phase transitions follow transition pattern, meaningcaptured finite-size rescaling formula + U]N}|~2 U + ,critical point depending particular feature interest.interesting note critical points phase transitions ATSP tour costsfractions backbone variables x5 . close examination also indicates twophase transitions follow almost phase transition, shown Figure 8, rescaled482fiP HASE RANSITIONSBACKBONESATSP(a) probability Hamiltonian circuitprobability Hamiltonian circuit1100city500city1,000city1,500city0.80.60.40.2probability Hamiltonian circuit00.511.5effective number digits(b) rescaled prob. Hamiltonian circuit1100city500city1,000city0.81,500city0.60.40.201 0.500.511.5rescaled effective number digitsFigure 5: (a) Probability existence Hamiltonian circuits zero cost arcs. (b) Rescaledprobability zero-cost Hamiltonian circuits, rescaling factor + TN}~2 U +=x Q 2 .curves ATSP tour cost fraction backbone variables drawn 1,500-cityATSP, averaged 10,000 problem instances.483fiZ HANGnormalizedaverage prob. AP(D)=ATSP(D)average probability AP(D) = ATSP(D)(a) probability AP(D) = ATSP(D)1100city500city1,000city1,500city0.80.60.40.200.511.522.5effective number digits(b) rescaled probability AP(D)=ATSP(D)1100city500city1,000city1,500city0.80.60.40.20321012rescaled effective number digits3Figure 6: (a) Average probability fi !6fio l . (b) Average probability finitesize scaling, rescaling factor + ]N}~2 U + >657|5 Q Q2Q .Except close similarity phase transitions ATSP tour cost fractionbackbone variables, phase transitions different critical points, indicatingundergo type phase transitions different ranges.484finormalized ave. relative error AP(D)average relative error AP(D) ATSP(D)P HASE RANSITIONSBACKBONESATSP(a) precision AP function1.4100city500city1,000city1,500city1.210.80.60.40.200.511.522.5effective number digits(b) rescaled accuracy AP function10.80.60.4100city500city1,000city1,500city0.20210123rescaled effective number digitsFigure 7: (a) Average accuracy AP lower-bound function, measured error AP costrelative ATSP cost. (b) normalized rescaled average accuracy, rescaling factor+ =]N}~2=U,+ > Q .5. Asymptotic ATSP Tour Length AP Precisionby-product phase-transition results, provide numerical values ATSPcost, AP cost accuracy, asymptotically number cities grows. attempt485finormalized cost fraction backboneZ HANG10.80.60.40.20tour costbackbone210123rescaled effective number digitsFigure 8: Simultanous examination phase transitions backbone ATSP tour cost1,500-city problems, rescaled + 5gN}~2 U + .extend previous theoretical results AP cost, shown asymptotically approachf (Aldous, 2001; Mezard & Parsi, 1987), observations relative error APlower bounds decreases problem size increases (Balas & Toth, 1985; Smith et al., 1977).every real number represented digital computer. Thus, infeasible directlyexamine theoretical result reals using digital computer. purpose, hand,phase-transition results indicate precision intercity distances high enough,quantities ATSP examined, including ATSP cost, AP costprecision lower-bound cost function, well backbone, relatively stable, sensechange significantly even precision intercity distances increases further.Therefore, sufficient use high distance precision experimentally analyze asymptoticproperties ATSP cost related quantities.need cautious selecting number digits intercity distances. discussedSection 3, number digits intercity distances gives rise different effective numbers digits problems different sizes. Furthermore, phase transition results Section 4indicate effective numbers digits must scaled properly order effectproblems different sizes investigate asymptotic feature.Therefore, experiments, fixed scaled effective number digits intercity distances, + ]N}~2 U + , constant. Based phase-transition results, especiallycontrol parameter Figure 1, chose take + \,N}~2 U + constant 2.1, two reasons.First, + \,N}~2 U +"\N|5 sufficiently large almost distances distinct, regardlessproblem size, quantities examine change substantially finite-sizescaling. Secondly, + \,N}~2NU,+bM\N|5 relatively small experiment problemslarge sizes. save memory much possible, intercity distances integers 4 bytesimplementation subtour elimination algorithm. Thus number digits must less486fiP HASE RANSITIONSn2004006008001,0001,2001,4001,6002,0002,2002,4002,6002,8003,000digits6.70217.30417.65637.90628.10008.25848.39238.50828.70218.78488.86048.92998.99439.0542AP cost1.63533 0.002541.63942 0.001801.64072 0.001461.64227 0.001251.64297 0.001141.64284 0.001041.64313 0.000961.64319 0.000901.64382 0.000821.64372 0.000771.64360 0.000741.64429 0.000711.64382 0.000681.64421 0.00065BACKBONESATSPATSP costrelative AP error (%)1.64302 0.002540.46817 0.009701.64311 0.001800.22485 0.004681.64314 0.001450.14765 0.003171.64407 0.001250.10904 0.002371.64441 0.001140.08754 0.001911.64402 0.001050.07187 0.001581.64413 0.000960.06148 0.001391.64405 0.000900.05276 0.001171.64451 0.000820.04231 0.000951.64434 0.000770.03813 0.000851.64417 0.000730.03477 0.000791.64481 0.000710.03234 0.000741.64430 0.000680.02966 0.000681.64463 0.000650.02548 0.00061Table 1: Numerical results AP cost, ATSP cost AP error relative ATSP cost,percent. cost matrices uniformly random. data point averaged 10,000problem instances. table, number cities, digits number digitsintercity distances, numerical error bounds represent 95 percent confidenceintervals.9.4 without causing overflow worst case. Using + \,N}~2U2+\N|5 , goroughly 3,000-city ATSPs.Table 1 shows experimental results, 3,000 cities, average AP cost,ATSP tour cost, accuracy AP cost function error AP cost relative ATSPcost. results averaged 10,000 instances problem size. Based results,AP cost approaches 1.6442 ATSP cost 1.6446. Note experimental AP cost1.6442 close theoretical asymptotic AP cost dgf 57 2 (Aldous, 2001;Mezard & Parsi, 1987). addition, accuracy AP function indeed improves problemsize increases, reduced 0.02548% 3,000-city problem instances. result supportsprevious observations (Balas & Toth, 1985; Smith et al., 1977).6. Thrashing Behavior Subtour Eliminationphase-transition results discussed previous section indicate ATSP becomesconstrained difficult distance precision becomes higher. section, studywell-known algorithm ATSP, branch-and-bound subtour elimination (Balas & Toth,1985; Bellmore & Malone, 1971; Smith et al., 1977), behaves. separate issuephase transition phenomena studied consider section behavior particular algorithm, may necessarily feature underlying problem.Nevertheless, still issue interest algorithm oldest still487fiZ HANGamong best known methods ATSP, hope better understanding efficientalgorithm ATSP shed light typical case computational complexity problem.6.1 Branch-and-bound Subtour Eliminationbranch-and-bound (BnB) subtour algorithm elimination (Balas & Toth, 1985; Bellmore & Malone, 1971; Smith et al., 1977) solves ATSP state-space search (Pearl, 1984; Zhang, 1999)uses assignment problem (AP) lower-bound cost function. BnB search takesoriginal ATSP root state space repeats following two steps. First, solvesAP current problem. AP solution complete tour, decomposes subproblems subtour elimination breaks subtour excluding arcs selected subtour.subproblem constrained parent problem, AP cost subproblem mustmuch parent. means AP cost function monotonically nondecreasing. solving AP requires X&+NY computation general, AP child nodeincrementally solved X +Nd time based solution AP parent.many heuristics selecting subtour eliminate (Balas & Toth, 1985), useCarpaneto-Toth scheme (Carpaneto & Toth, 1980), CT scheme short, algorithm.One important feature CT scheme generates duplicate subproblemoverall search space tree. One example scheme shown Figure 9. AP solutionoriginal ATSP contains two subtours root tree figure. subtour\ \ chosen eliminated, since shorter subtour. two waysbreak selected subtour, i.e., excluding directed arc \N1], N1]\, . Assume first exclude\N1], N1]\, , generating two subproblems, nodes fi Figure 9. generatingsecond subproblem , deliberately include \N1], solution. including arcexcluded previous subproblem fi , force exclude current subproblemsolutions original problem appear fi , therefore form partition solutionspace using fi . general, let excluded arc set, included arc setproblem decomposed. Assume arcs selected subtour, [ 1 1.^.^.^1 . ` ,. CT scheme decomposes problem child subproblems, -thone excluded arc set . included arc set 9 ,NF;[ `,1(9)V;[ 1.^.^.^g1 `,1 &571]\N1.^.^.^1JSince excluded arc -th subproblem, :!9 , included arc H5 -stsubproblem, ;:F. , complete tour obtained -th subproblem contain arc, tour obtained 5 -st subproblem must arc . Thus tour -thsubproblem cannot generated &5 -st subproblem, vice versa. summary,state space ATSP BnB using CT subtour elimination scheme representedtree without duplicate nodes.next step, algorithm selects current problem new subproblem setactive subproblems, ones generated yet expanded. processcontinues unexpanded problem, unexpanded problems costs greaterequal cost best complete tour found far.Thanks linear-space requirement, use depth-first branch-and-bound (DFBnB)algorithm. DFBnB explores active subproblems depth-first order. uses upper bound488fiP HASE RANSITIONS1456242635ATSP3E={(3,2)}I={(2,3)}E={(2,3)}I={}1BACKBONESB23 1645E={(3,2),(3,6)}I={(2,3),(6,2)}E={(3,2),(6,2)}I={(2,3)}C236623415415Figure 9: DFBnB subtour elimination ATSP.optimal cost, whose initial value infinity cost approximate solution,one obtained Karps patching algorithm (Karp, 1979; Karp & Steele, 1985), repeatedlypatches two smallest subtours big one complete tour forms. Starting root node,DFBnB selects recently generated node - examine next. AP solution - completetour, - leaf node search tree. cost leaf node less current upperbound , revised cost - . - AP solution complete tour cost greaterequal , - pruned, node costs monotonic descendant -cost smaller - cost. Otherwise, - expanded, generating child nodes. findoptimal goal node quickly, children - searched increasing ordercosts. words use node ordering reduce number nodes explored. speedprocess reaching better, possibly optimal, solution, also apply Karps patching algorithmbest child node current node.algorithm principle algorithm Carpaneto, DellAmico & Toth (1995),probably best known complete algorithm ATSP. main differencetwo that, due consideration space requirement, use depth-first search Carpaneto,DellAmico & Toth (1995) used best-first search.6.2 Thrashing Behavioraverage computational complexity BnB subtour elimination algorithm determinedtwo factors, problem size, number cities, number digits used intercitydistances. Figure 10 illustrates average complexity, measured number calls APfunction, logarithmic scale. result averaged 10,000 problem instancesdata point used phase transitions studied Section 4. Note number APcalls increases exponentially small problems large ones generated usingnumber digits distances.489fiZ HANG4average number AP calls103102101100city500city1,000city1,500city1001012345number digits6Figure 10: Average computational complexity BnB subtour elimination algorithm.characterize thrashing behavior algorithm, normalize result Figure 10way given problem size, minimal maximal AP calls among probleminstances size mapped zero one, respectively, AP callsproportionally adjusted ratio 0 1. allows us compare resultsdifferent problem sizes one figure. also normalize number digits distancesproblem size. curves Figure 11(a) follow pattern similar phase transitionsSection 4. complexity subtour elimination algorithm increases effective numberdigits, exhibits thrashing behavior similar phase transitions. Indeed, use finitesize scaling capture behavior problem size grows, illustrated Figure 11(b).results Figure 11 phase-transition results Section 4 indicate complexitysubtour elimination algorithm goes hand-in-hand accuracy AP functionconstrainedness problem, determined portion distinct entities distancematrix, turn controlled precision distances.Similar results reported Zhang & Korf (1996), effects two differentdistance distributions average complexity subtour elimination algorithm analyzed conclude determinant average complexity number distinct distancesproblem. results section extend Zhang & Korf (1996) different sizesproblems applying finite-size scaling capture thrashing behavior problem sizeincreases.need contrast experimental result section theoretical resultNP-completeness TSP intercity distances 0 1. known degeneratedTSP distances 0 1 still NP-complete (Papadimitriou & Yannakakis, 1993).hand, experimental results showed intercity distances small, relativeproblem size, ATSP easy average. Based experimental result, large portionproblem instances small intercity distances solved assignment problem Karps490fiP HASE RANSITIONSBACKBONESATSPnormalized average number AP callsnormalized average number AP calls(a) normalized number AP calls10.80.60.4100city500city1,000city1,500city0.200.511.522.5effective number digits(b) rescaled normalized # AP calls10.80.60.4100city500city1,000city1,500city0.20432101rescaled effective number digits2Figure 11: (a) Normalized average number AP calls DFBnB subtour elimination. (b) Scaledaverage number AP calls, + 2]N}|~2 U + , 57 Q Q \2 .patching algorithm branch-and-bound search required. discrepancy indicatesworst case problem rare likely pathological.491fiZ HANG7. Related Work DiscussionsTwo lines previous work directly influenced inspired research. first linerelated work expected complexity tree search, shed light BnB subtourelimination algorithm described Section 6.1 solves ATSP tree search. analysiscarried abstract random tree model called incremental tree (Karp & Pearl, 1983;McDiarmid, 1990; McDiarmid & Provan, 1991; Zhang & Korf, 1995; Zhang, 1999). internalnodes variable number children edges assigned finite nonnegativerandom values. cost node sum edge costs along path rootnode. optimal goal node node minimum cost fixed depth . overall goalfind optimal goal node.exist phase transitions cost optimal goal node complexityproblem finding optimal goal . control parameter expected number childnodes common parent node cost parent. cost optimal goalnode almost surely undergoes phase transition linear function depth constantexpected same-cost children node increases beyond one. Meanwhile, best-first searchdepth-first branch-and-bound also exhibit phase-transition behavior, i.e., expected complexitychanges dramatically exponential polynomial expected same-cost childrennode reduced one. Note following result Dechter & Pearl (1985), best-firstsearch optimal searching random tree among algorithms using cost function,terms number node expansions, tie breaking. Thus, results also giveexpected complexity problem searching incremental tree.second line related research characterizing assignment problem (AP)lower-bound cost function relationship ATSP, research interestlong time (Aldous, 2001; Coppersmith & Sorkin, 1999; Frieze, Karp, & Reed, 1992; Frieze &Sorkin, 2001; Karp, 1987; Karp & Steele, 1985; Mezard & Parsi, 1987; Walkup, 1979). firstsurprising result (Walkup, 1979) expected AP cost approaches constant numbercities goes infinity entries distance matrix independent uniform realsnQ 15q . constant subject long history pursuit. shown rigorously,based rigorous replica method statistical physics (Mezard et al., 1987), optimal costrandom assignment approaches asymptotically df (Aldous, 2001), approximately1.64493. results Section 5 show AP ATSP costs approach 1.644211.64463, respectively, agree theoretical results AP cost.importantly, relationship AP cost ATSP cost remarkablydifferent characteristics different distance distributions. one extreme, AP costATSP cost high probability, extreme, differATSP cost, high probability, function problem size . Let fi ! AP costfiP ! ATSP cost distance matrix . expected number zerosv , fi !Vfio l probabilityrow approaches infinity ustending one (Frieze et al., 1992). However, entities uniform integersnQ 1571.^.^.^1+ rq , fi !!fib l probability going zero, growsninfinity (Frieze et al., 1992). Indeed, entities uniform Q 15q ,fio l fi ! f , positive constant (Frieze & Sorkin, 2001).previous results indicate quality AP function varies significantly, dependingunderlying distance distribution. Precisely, difference AP cost ATSP492fiP HASE RANSITIONSBACKBONESATSPcost two phases, controlled number zero distances distance matrix . onephase, difference zero high probability, phase, expectationdifference function problem size . experimental results Section 4 addsanalysis existence phase transition two phases.two-phase result accuracy AP cost function also principle consistentphase-transition result incremental random trees. root search tree cost equalAP cost fio l problem optimal goal node ATSP tour cost fiP ! .subtract AP cost root every node ATSP search tree, root nodecost zero optimal goal node cost equal fib l fi ! . largenumber zero distances , large number same-cost children, AP costchild node search tree likely AP cost parent, since APtend use zero distances. Therefore, expected nodes search treeone child node cost parents.addition phase transitions combinatorial problems mentioned Section 1,related previous results. Results scaling search cost constrainedness symmetric TSP considered Gent, MacIntyre, Prosser, & T. Walsh (1997). Phase transitionsHamiltonian circuit studied Frank, Gent, & Walsh (1998). also shownhard generate difficult Hamiltonian cycle problem instances (Vandegriend & Culberson, 1998).addition, concept backbones studied many problems different names.examples, unary prime implicate refers variable must set fixed valueinstance Boolean satisfiability (Parkes, 1997); frozen development describes pair nodesmust share colors graph coloring problem (Culberson & Gent, 2001).8. Conclusionsmain contributions research twofold. First, answered positively long-standingquestion Traveling Salesman Problem (TSP) phase transitions (Kirkpatrick & Toulouse,1985) disapproved belief problem phase transition (Kirkpatrick& Selman, 1994). studied issue general, optimization version problem,asymmetric TSP (ATSP). empirically showed, using random problem instances distances uniform distribution, many important properties, including ATSP tour costfraction backbone variables, two characteristically different values, transitions rather abrupt dramatic, displaying phase-transition phenomenon.control parameter phase transitions effective number digits representing intercitydistances precision distance measure.Second, results provide practical guidance generate difficult random ATSPproblem instances random instances used comparing asymptotic performance ATSP algorithms. current common practice comparing algorithms usingrandom ensemble generate problem instances different sizes fixed distance precision.phase transition results indicate correct way use instances different sizessimilar features fraction backbone variables. also important point locations hardest, albeit random, problem instances typically dependdistance distribution used. case uniform distribution, requires increasing precisionintercity distances problem size grows.493fiZ HANGimportant note exact locations various phase transitions presented remainmathematically determined, using methods probably statistical physics (Martin et al.,2001; Mezard et al., 1987).like conclude pointing phase transition results paper general.argument control parameter Section 3 general applicable symmetricTSP (STSP). unpublished data also showed phase transitions STSP. resultsATSP uniformly distributed distances hold types intercity distances.part supported previous investigation intercity distances chosen lognormal distribution (Zhang & Korf, 1996). Finally, believe phase transitions persiststructured TSPs long intercity distances independently drawn common distribution.TSPs include proposed studied Cirasella et al., (2001) Johnson et al., (2002),examples, problem instances constraints triangle inequalities, instances convertedparticular applications disk drive optimization, jobshop scheduling, coin collecting optimization, etc.Acknowledgmentsresearch supported part NSF grants IIS-0196057 ITR/EIA-0113618, partDARPA Cooperative Agreements F30602-00-2-0531 F33615-01-C-1897. Thanks SharleeClimer joint work algorithm finding backbone using limit crossing (Climer &Zhang, 2002) critical reading draft. Thanks also Scott Kirkpatrick David Johnson comments draft. Special thanks Allon Percus Sergey Knysh constructivecomments suggestions, especially finite-size scaling, significantly improvedpaper. Thanks also go anonymous reviewers excellent comments. early resultspresented NSF/IPAM Workshop Phase Transitions Algorithmic Complexity, June3-5, 2002, 18-th International Joint Conference Artificial Intelligence (IJCAI-03),Acapulco, Mexico, Aug. 9-15, 2003 (Zhang, 2003).ReferencesAchlioptas, D., Gomes, C., Kautz, H., & Selman, B. (2000). Generating satisfiable instances.Proceedings 17th National Conference Artificial Intelligence (AAAI-02).Aldous, D. J. (2001). =\, limit random assignment problem. Random StructuresAlgorithms, 18, 381418.Balas, E., & Toth, P. (1985). Branch bound methods. Traveling Salesman Problem, pp.361401. John Wiley & Sons, Essex, England.Barber, M. N. (1983). Finite-size scaling. Phase Transitions Critical Phenomena, Vol. 8, pp.145266. Academic Press.Bellmore, M., & Malone, J. C. (1971). Pathology traveling-salesman subtour-elimination algorithms. Operations Research, 19, 278307.Borgs, C., Chayes, J. T., & Pittel, B. (2001). Phase transition finite-size scaling integerpartitioning problem. Random Structures Algorithms, 19, 247288.494fiP HASE RANSITIONSBACKBONESATSPCarpaneto, G., DellAmico, M., & Toth, P. (1995). Exact solution large-scale, asymmetric Traveling Salesman Problems. ACM Trans. Mathematical Software, 21, 394409.Carpaneto, G., & Toth, P. (1980). new branching bounding criteria asymmetrictraveling salesman problem. Management Science, 26, 736743.Cheeseman, P. (1991). Personal communications..Cheeseman, P., Kanefsky, B., & Taylor, W. M. (1991). really hard problems are. Proc.12th International Joint Conference Artificial Intelligence (IJCAI-91), pp. 331337.Cirasella, J., Johnson, D., McGeoch, L. A., & Zhang, W. (2001). asymmetric traveling salesmanproblem: Algorithms, instance generators, tests. Proc. 3rd Workshop AlgorithmEngineering Experiments (ALENEX-2001).Climer, S., & Zhang, W. (2002). Searching backbones fat: limit-crossing approachapplications. Proc. 18th National Conference Artificial Intelligence (AAAI-02), pp.707712.Coppersmith, D., & Sorkin, G. B. (1999). Constructive bounds exact expectationsrandom assignment problem. Random Structures Algorithms, 15, 113144.Culberson, J., & Gent, I. (2001). Frozen development graph coloring. Theoretical ComputerScience, 265, 227264.Dantzig, G. B., Fulkerson, D. R., & Johnson, S. M. (1959). linear programming, combinatorialapproach traveling salesman problem. Operations Research, 7, 5866.Dechter, R., & Pearl, J. (1985). Generalized best-first search strategies optimality .Journal ACM, 32, 505536.Frank, J., Gent, I., & Walsh, T. (1998). Asymptotic finite size parameters phase transitions:Hamiltonian circuit case study. Information Processing Letters, 65(5), 241245.Frieze, A., Karp, R. M., & Reed, B. (1992). assignment bound asymptotically tightasymmetric traveling-salesman problem?. Proc. Integer Programming Combinatorial Optimization, pp. 453461.Frieze, A., & Sorkin, G. B. (2001). probabilistic relationship assignmentasymmetric traveling salesman problems. Proc. SODA-01, pp. 652660.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryNP-Completeness. Freeman, New York, NY.Gent, I., MacIntyre, E., Prosser, P., & Walsh, T. (1997). scaling search cost. Proc.14th National Conference Artificial Intelligence (AAAI-96), pp. 315320.Gent, I., & Walsh, T. (1996a). TSP phase transition. Artificial Intelligence, 88, 349358.Gent, I., & Walsh, T. (1996b). Phase transitions annealed theories: Number partitioningcase study. Proceedings 12th ECAI.Gent, I., & Walsh, T. (1998). Analysis heuristics number partitioning. Computational Intelligence, 14(3), 430451.Gomes, C. P., Hogg, T., Walsh, T., & Zhang, W. (2001). IJCAI-2001 tutorial: Phase transitionsstructure combinatorial problems. http://www.cs.wustl.edu/ zhang/links/ijcai-phasetransitions.html.495fiZ HANGGutin, G., & Punnen, A. P. (Eds.). (2002). Traveling Salesman Problem Variations.Kluwer Academic Publishers.Hogg, T. (1995). Exploiting problem structure search heuristic. Tech. rep., Xerox PARC.Hogg, T., Huberman, B. A., & Williams, C. (1996). Phase transitions search problem.Artificial Intelligence, 81, 115.Johnson, D. S., Gutin, G., McGeoch, L. A., Yeo, A., Zhang, W., & Zverovitch, A. (2002).Traveling Salesman Problem Variations, chap. Experimental Analysis HeuristicsATSP, pp. 445487. Kluwer Academic Publishers, Dordrecht.Karp, R. M. (1972). Reducibility among combinatorial problems. Miller, R. E., & Thatcher, J. W.(Eds.), Comlexity Computer Computations, pp. 85103. Plenum Press.Karp, R. M. (1979). patching algorithm nonsymmetric traveling-salesman problem. SIAMJournal Computing, 8, 561573.Karp, R. M. (1987). upper bound expected cost optimal assignment. Johnson,D. (Ed.), Discrete Algorithms Complexity: Proc. Japan-US Joint Seminar, pp. 14,New York. Academic Press.Karp, R. M., & Pearl, J. (1983). Searching optimal path tree random costs. ArtificialIntelligence, 21, 99117.Karp, R. M., & Steele, J. M. (1985). Probabilistic analysis heuristics. Traveling SalesmanProblem, pp. 181205. John Wiley & Sons, Essex, England.Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. (1983). Optimization simulated annealing. Science,220, 671680.Kirkpatrick, S., & Selman, B. (1994). Critical behavior satisfiability random booleanexpressions. Science, 264(5163), 12971301.Kirkpatrick, S., & Toulouse, G. (1985). Configuration space analysis traveling salesman problems. J. Phys. (France), 46, 12771292.Lawler, E. L., Lenstra, J. K., Kan, A. H. G. R., & Shmoys, D. B. (Eds.). (1985). TravelingSalesman Problem. John Wiley & Sons, Essex, England.Lin, S., & Kernighan, B. W. (1973). effective heuristic algorithm Traveling SalesmanProblem. Operations Research, 21, 498516.Little, J. D. C., Murty, K. G., Sweeney, D. W., & Karel, C. (1963). algorithm travelingsalesman problem. Operations Research, 11, 972989.Martello, S., & Toth, P. (1987). Linear assignment problems. Annals Discrete Mathematics, 31,259282.Martin, O. C., Monasson, R., & Zecchina, R. (2001). Statistical mechanics methods phasetransitions optimization problems. Theoretical Computer Science, 265, 367.McDiarmid, C. J. H. (1990). Probabilistic analysis tree search. Gummett, G. R., & Welsh, D.J. A. (Eds.), Disorder Physical Systems, pp. 249260. Oxford Science.McDiarmid, C. J. H., & Provan, G. M. A. (1991). expected-cost analysis backtrackingnon-backtracking algorithms. Proc. 12th International Joint Conference ArtificialIntelligence (IJCAI-91), pp. 172177, Sydney, Australia.496fiP HASE RANSITIONSBACKBONESATSPMezard, M., & Parsi, G. (1987). solution random link matching problem. J. Physique,48, 14511459.Mezard, M., Parsi, G., & Virasoro, M. A. (Eds.). (1987). Spin Glass Theory Beyond. WorldScientific, Singapore.Mitchell, D., Selman, B., & Levesque, H. (1992). Hard easy distributions SAT problems.Proc. 10th National Conference Artificial Intelligence (AAAI-92), pp. 459465.Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determiningcomputational complexity characteristic phase transitions. Nature, 400, 133137.Papadimitriou, C. H., & Yannakakis, M. (1993). travelling salesman problem distancesone two. Math. Oper. Res., 18, 111.Parkes, A. (1997). Clustering phase transitions. Proc. 14th National ConferenceArtificial Intelligence (AAAI-97).Pearl, J. (1984). Heuristics: Intelligent Search Strategies Computer Problem Solving. AddisonWesley, Reading, MA.Selman, B., & Kirkpatrick, S. (1996). Critical behavior computational cost satisfiabilitytesting. Artificial Intelligence, 81, 273295.Slaney, J., & Walsh, T. (2001). Backbones optimization approximation. Proc. 17thInternational Joint Conference Artificial Intelligence (IJCAI-01), pp. 254259.Smith, T. H. C., Srinivasan, V., & Thompson, G. L. (1977). Computational performance threesubtour elimination algorithms solving asymmetric traveling salesman problems. AnnalsDiscrete Mathematics, 1, 495506.Vandegriend, B., & Culberson, J. (1998). gn,m phase transition hard hamiltoniancycle problem. J. Artificial Intelligence Research, 9, 219245.Walkup, D. W. (1979). expected value random assignment problem. SIAM JournalComputing, 8, 440442.Wilson, K. G. (1979). Problems physics many scales length. Scientific American, 241,158179.Zhang, W. (1999). State-space Search: Algorithms, Complexity, Extensions, Applications.Springer.Zhang, W. (2001). Phase transitions backbones 3-SAT maximum 3-SAT. Proc. Intern.Conf. Principles Practice Constraint Programming (CP-01), pp. 153167.Zhang, W. (2003). Phase transitions asymmetric traveling salesman. Proc. 18th International Joint Conference Artificial Intelligence (IJCAI-03), pp. 12021207.Zhang, W., & Korf, R. E. (1995). Performance linear-space search algorithms. Artificial Intelligence, 79, 241292.Zhang, W., & Korf, R. E. (1996). study complexity transitions asymmetric TravelingSalesman Problem. Artificial Intelligence, 81, 223239.497fiJournal Artificial Intelligence Research 21 (2004) 579-594Submitted 08/03; published 05/04Learn Beat Best StockAllan Borodinbor@cs.toronto.eduDepartment Computer ScienceUniversity TorontoToronto, ON, M5S 3G4 CanadaRan El-Yanivrani@cs.technion.ac.ilDepartment Computer ScienceTechnion - Israel Institute TechnologyHaifa 32000, IsraelVincent Goganvincent@cs.toronto.eduDepartment Computer ScienceUniversity TorontoToronto, ON, M5S 3G4 CanadaAbstractnovel algorithm actively trading stocks presented. traditional expertadvice universal algorithms (as well standard technical trading heuristics) attemptpredict winners trends, approach relies predictable statistical relationspairs stocks market. empirical results historical markets provide strongevidence type technical trading beat market moreover, beatbest stock market. utilize new idea smoothing criticalparameters context expert learning.1. Introductionportfolio selection (PS) problem challenging problem machine learning, onlinealgorithms and, course, computational finance. well known (e.g. see Lugosi, 2001)sequence prediction log loss measure viewed special case portfolio selection, perhaps surprisingly, certain worst case minimax criterion,portfolio selection essentially harder (than prediction) shown (Cover & Ordentlich, 1996) (see also Lugosi, 2001, Thm. 20 & 21). seems qualitativedifference practical utility universal sequence prediction universalportfolio selection. Simply stated, universal sequence prediction algorithms variousprobabilistic worst-case models appear work well practice whereas knownuniversal portfolio selection algorithms seem provide substantial benefitnaive investment strategy (see Section 5).major pragmatic question whether computer program consistentlyoutperform market. closer inspection interesting ideas developed information theory online learning suggests promising approach exploit naturalvolatility market particular benefit simple rather persistent statistical relations stocks rather try predict stock prices winners.c2004AI Access Foundation. rights reserved.fiBorodin, El-Yaniv, & Goganpresent non-universal portfolio selection algorithm1 , try predictwinners. motivation behind algorithm rationale behind constant rebalancingalgorithms worst case study universal trading introduced Cover (1991).proposed algorithm substantially beat market historical markets,also beats best stock. presenting algorithm simplymaking money? are, course caveats obstacles utilizing algorithm.large investors possibility goose laying silver (if golden) eggsperhaps impossible.2. Portfolio Selection ProblemAssume market stocks. Let vt = (vt (1), . . . , vt (m)) daily closing prices2stocks tth day, vt (j) price jth stock. convenientwork relative prices xt (j) = vt (j)/vt1 (j) investment $d jthstock tth day yields dxt (j) dollars. let xt = (xt (1), . . . , xt (m)) denotemarket vector relative prices corresponding tth day. portfolio b allocationwealth stocks, specified proportions b = (b(1),P . . . , b(m)) current dollarwealth invested stocks, b(j) P0 j b(j) = 1. daily returnportfolio b w.r.t. market vector x b x = j b(j)x(j) (compound) totalreturn, retX (b1 , . . .Q, bn ), sequence portfolios b1 , . . . , bn w.r.t. market sequenceX = x1 , . . . , xn nt=1 bt xt . portfolio selection algorithm deterministicrandomized rule specifying sequence portfolios let retX (A) denote totalreturn market sequence X.simplest strategy buy-and-hold stocks using portfolio b. denotestrategy BAHb let U-BAH denote uniform buy-and-hold b = (1/m, . . . , 1/m).say portfolio selection algorithm beats market outpeforms U-BAHgiven market sequence although practice market representednon-uniform BAH.3 Buy-and-hold strategies rely tendency successful marketsgrow. Much modern portfolio theory focuses choose good b buyand-hold strategy. seminal ideas Markowitz (1959) yield algorithmic procedurechoosing weights portfolio b minimize variance feasibleexpected return. variance minimization possible placing appropriate (larger)weights subsets sufficiently anti-correlated stocks, idea shall also utilize.denote optimal hindsight buy-and-hold strategy (i.e. invest beststock) BAH .alternative approach static buy-and-hold dynamically change portfoliotrading period. approach often called active trading. One exampleactive trading constant rebalancing; namely, fix portfolio b (re)invest dollarsday according b. denote constant rebalancing strategy CBALb letCBAL denote optimal (in hindsight) CBAL. constant rebalancing strategy often1. PS algorithm modified universal investing fixed fraction initial wealthuniversal algorithm.2. nothing special daily closing prices problem defined respect(sub)sequence (intra-day) sequence price offers appear stock market.3. example Dow Jones Industrial Average (DJIA) calculated non uniform average 30DJIA stocks; see e.g. http://www.dowjones.com/580fiCan Learn Beat Best Stocktake advantage market fluctuations achieve return significantly greateralways least good best stock BAH real marketsequences constant rebalancing strategy take advantage market fluctuationssignificantly outperform best stock (see e.g. Table 1). now, consider CoverGlusss (1986) classic (but contrived) examplecash one stock1 consisting1 ofa1market, 12 , . . .. consider CBALbmarket sequence price relatives 1/2, 2 , 1/2b = ( 21 , 12 ). odd day daily return CBALb 21 1+ 12 12 = 34 evenday, 3/2. total return n days therefore (9/8)n/2 , illustrating constantrebalancing strategy yield exponential returns no-growth market.assumption daily market vectors observations identically independentlydistributed (i.i.d) random variables, shown (Cover & Thomas, 1991) CBALperforms least good (in sense expected total return) best online portfolioselection algorithm. However, many studies (see e.g. Lo & MacKinlay, 1999) arguestock price sequences long term memory i.i.d.non-traditional objective (in computational finance) develop online tradingstrategies sense always guaranteed perform well.4 Within lineresearch pioneered Cover (Cover & Gluss, 1986; Cover, 1991; Cover & Ordentlich, 1996)one attempts design portfolio selection algorithms provably well (in termstotal return) respect online offline benchmark algorithms. Twonatural online benchmark algorithms uniform buy hold U-BAH, uniform11,...,). naturalconstant rebalancing strategy U-CBAL, CBALb b = (offline benchmark BAH challenging offline benchmark CBAL .portfolio selection algorithm called universal every market sequence Xn days, guarantees subexponential ratio (in n) return retX (A)retX (CBAL ). particular, Cover Ordentlichs Universal Portfolios algorithm (Cover,1991; Cover & Ordentlich, 1996), denoted UNIVERSAL, proven universal;specifically every market sequence X stocks n days, guaranteessubexponential (indeed polynomial) ratiom1(1)retX (CBAL )/retX (UNIVERSAL) = n 2 .BAH . CBALtheoretical perspective surprising performance ratio boundedpolynomial n (for fixed m) whereas CBAL capable exponential returns.practical perspective, bound useful empirical returns observedCBAL portfolios often exponential number trading days. However,motivation underlies potential CBAL algorithms useful! follow motivation develop new algorithm call ANTICOR. attempting systematicallyfollow constant rebalancing philosophy, ANTICOR capable extraordinary performance absence transaction costs, even small transaction costs.4. trading strategy online computes portfolio (t+1)st day using market informationfirst days. contrast offline algorithms U-BAH , CBAL optimalstrategy picking best stock individual day. offline algorithms compute sequenceportfolios function entire market sequence.581fiBorodin, El-Yaniv, & Gogan3. Trying Learn Winnersdirect approach expert learning portfolio selection (reward based)weighted average prediction scheme, adaptively computes weighted averageexperts gradually increasing (by multiplicative additive update rule) relativeweights successful experts. section briefly discuss relatedportfolio selection results along lines.example, context PS problem consider exponentiated gradientEG() algorithm proposed (Helmbold et al., 1998). EG() algorithm computesnext portfoliobt (j) exp {xt (j)/(bt xt )}bt+1 (j) = Pm,j=1 bt (j) exp {xt (j)/(bt xt )}learning rate parameter. EG designed greedily choose best portfolioyesterdays market xt time paying penalty movingpfar yesterdays portfolio. universal bound EG, Helmbold et al. set = 2xmin 2(log m)/nxmin lower bound price relative.5 easy see n increases,decreases 0 think small order achieve universality.= 0, algorithm EG() degenerates uniform CBAL (assuming starteduniform portfolio) universal algorithm. also caseday price relatives stocks identical, EG (as well PS algorithms)converge uniform CBAL. Combining small learning rate reasonablybalanced market expect performance EG similar uniformCBAL confirmed experiments (see Table 1).6Covers universal algorithms adaptively learn days portfolio increasingweights successful CBALs. update rule universal algorithmsRb rett (CBALb )d(b)bt+1 = R,rett (CBALb )d(b)() prior distribution portfolios. Thus, weight possible portfolioproportional total return rett (b) thus far times prior. particular universal algorithm consider experiments uses Dirichlet prior (with parameters( 21 , . . . , 12 )) (Cover & Ordentlich, 1996).7 Somewhat surprisingly, noted (Cover & Ordentlich, 1996) algorithm equivalent static weighted average (given (b))CBALs (see also Borodin & El-Yaniv, 1998, p. 291). equivalence helps demystifyuniversality result also shows algorithm never outperform CBAL .5. Helmbold et al. show eliminate need know xmin n. EG made universal,performance ratio sub-exponential (and polynomial) n.6. Following Helmbold et al. fix = 0.01 experiments. Additional experiments, wide rangefixed settings, confirm datasets choice = 0.01 optimal near optimalchoice. course, possible adaptively set throughout trading period, beyondscope paper.7. papers (Cover, 1991; Cover & Ordentlich, 1996; Blum & Kalai, 1998) consider simpler versionalgorithm (Dirichlet) prior uniform. algorithm also universal achievesratio (nm1 ). Experimentally (on datasets) negligible difference twovariants report results asymptotically optimal algorithm.582fiCan Learn Beat Best Stockdifferent type winner learning algorithm obtained sequenceprediction strategy, noted (Borodin, El-Yaniv, & Gogan, 2000). stock j,(soft) sequence prediction algorithm provides probability p(j) next symbolj {1, . . . , m}. view prediction stock j best relativeprice next day set bt+1 (j) = pj . paper (Borodin et al., 2000) considerspredictions made using prediction component well-known Lempel-Ziv (LZ) losslesscompression algorithm (Ziv & Lempel, 1978). prediction component nicely described(Langdon, 1983) (Feder, 1991). prediction algorithm, LZ provably powerfulvarious senses. First shown asymptotically optimal respectstationary ergodic finite order Markov source (Rissanen, 1983; Ziv & Lempel, 1978).Moreover, Feder shows LZ also universal worst case sense respect(offline) benchmark class finite state prediction machines. summarize, commonapproach devising PS algorithms attempt learn winners using simplesophisticated winner learning schemes.4. Anticor Algorithmpropose different approach, motivated CBAL-inspired philosophy.interpret success uniform CBAL Cover Gluss example Section 2?Clearly, uniform CBAL taking advantage price fluctuation constantly transferring wealth high performing stock relatively low performing stock. Evenless contrived market, CBAL capable large returns. market model favoringuse CBAL one stock growth rates stable long term occasional larger return rates followed smaller rates (and vice versa). marketphenomenon sometimes called reversal mean.many ways one interpret implement algorithms basedphilosophy reversal mean. particular, CBAL viewed staticimplementation philosophy. describe motivation basic ingredientsANTICOR algorithm adaptively (based recent empirical statistics) ratheraggressively8 implements reversal mean.given trading day, consider recent past w trading days, winteger parameter. growth rate stock window time measuredproduct relative prices window.9 Motivated assumptionportfolio stocks performing similarly terms long term growth rates,ANTICORs first condition transferring money stock stock j growthrate stock exceeds stock j recent window time.10 addition,ANTICOR algorithm requires indication stock j start emulate pastgrowth stock near future. end, ANTICOR requires positive correlationstock second last window stock j last window.relative extent transfer money stock stock j depend8. ANTICOR algorithm aggressive (say, compared CBAL) sense transferassets given stock. Various heuristics used moderate behavior.9. Since would rather deal arithmetic instead geometric means use logarithmsrelative prices.10. Note umderlying model assumption reversal mean. One modifyalgorithm account different means.583fiBorodin, El-Yaniv, & Goganstrength correlation well strength self anti-correlationsstocks j (again two consecutive windows). ANTICOR named usecorrelations anticorrelations consecutive windows indicate potentialanticorrelations growth rates stocks j near future (with hopefullygrowth rate stock j becoming greater stock i).Formally, defineLX1 = log(xt2w+1 ), . . . , log(xtw )T LX2 = log(xtw+1 ), . . . , log(xt )T ,(2)log(xk ) denotes (log(xk (1)), . . . , log(xk (m))). Thus, LX1 LX2 two vectorsequences (equivalently, two w matrices) constructed taking logarithmmarket subsequences corresponding time windows [t 2w + 1, w] [t w + 1, t],respectively. denote jth column LXk LXk (j). Let k = (k (1), . . . , k (m)),vectors averages columns LXk . Similarly, let k , vector standarddeviations columns LXk . cross-correlation matrix (and normalization)column vectors LX1 LX2 defined as111(LX1 (i) 1 (i))T (LX2 (j) 2 (j));w1(Mcov (i,j)1 (i)2 (j) 1 (i), 2 (j) 6= 0;Mcor (i, j) =0otherwise.Mcov (i, j) =(3)Mcor (i, j) [1, 1] measures correlation log-relative prices stockfirst window stock j second window. note 1 (i) (respectively,2 (j)) zero window growth rate stock second lastwindow (respectively, stock j last window) constant window.sufficiently large windows time constant growth stock unlikely. However,unlikely case choose move money stock i.12pair stocks j compute claimij , extent want shiftinvestment stock stock j. Namely, claim iff 2 (i) > 2 (j)Mcor (i, j) > 0 case claimij = Mcor (i, j) + A(i) + A(j) A(h) = |Mcor (h, h)|Mcor (h, h) < 0, else 0. Following interpretation success CBAL, Mcor (i, j) > 0used predict stocks j correlated consecutive windows (i.e.current window next window based evidence last two windows)Mcor (h, h) < 0 predicts stock h Pnegatively auto-correlated consecutivewindows. Finally,b(i)=b(i)+j6=i [transferji transferij ] transferij =P t+1bt (i) claimij / j claimij . pseudocode summarizing ANTICOR algorithm appearsFigure 1. pseudocode describes routine ANTICOR(w, t, Xt , bt ) receiveswindow size w, current trading day t, historical market sequence Xt (givingmarket vectors corresponding days 1, . . . , t) current portfolio bt definedbt = bt1xt (bt (1)xt (1), . . . , bt (m)xt (m)). routine first called empty historicalmarket sequence bt uniform portfolio (over stocks). routine11. Recall correlation coefficient normalized covariance covariance dividedproduct standard deviations; is, Cor(X, ) = Cov(X, )/(std(X) std(Y ))Cov(X, ) = E[(X mean(X))(Y mean(Y ))].12. course, approaches used accommodate constant nearly constant growth rate.584fiCan Learn Beat Best Stockreturns new portfolio, rebalance start (t + 1)st tradingday.Algoritm ANTICOR(w, t, Xt , bt )Input:1. w: Window size2. t: Index last trading day3. Xt = x1 , . . . , xt : Historical market sequence4. bt : current portfolio (by end trading day t)Output: bt+1 : Next days portfolio1. Return current portfolio bt < 2w.2. Compute LX1 LX2 defined Equation (2), 1 2 , (vector) averagesLX1 LX2 , respectively.3. Compute Mcor (i, j) defined Equation (3).4. Calculate claims: 1 i, j m, initialize claimij = 05. 2 (i) 2 (j) Mcor (i, j) > 0(a) claimij = claimij + Mcor (i, j);(b) Mcor (i, i) < 0 claimij = claimij Mcor (i, i);(c) Mcor (j, j) < 0 claimij = claimij Mcor (j, j);6. Calculate new portfolio: Initialize bt+1 = bt . 1 i, jP(a) Let transferij = bti claimij / j claimij ;(b) bt+1= bt+1transferij ;(c) bt+1= bt+1+ transferji ;Figure 1: AlgorithmANTICORANTICORw algorithm one critical parameter, window size w. Figure 2depict total return ANTICORw two historical datasets functionwindow size w = 2, . . . , 30 (detailed descriptions datasets appear Section 5).might expect, performance ANTICORw depends significantly window size.However, w, ANTICORw beats uniform market and, moreover, beats beststock using window sizes. course, online trading cannot choose w hindsight.Viewing ANTICORw algorithms experts, try learn best expert.windows, like individual stocks, induce rather volatile set experts standard expertcombination algorithms (Cesa-Bianchi et al., 1997) tend fail.13Alternatively, adaptively learn invest weighted average ANTICORwalgorithms w less maximum W . simplest case uniform investment windows; is, uniform buy-and-hold investment algorithmsANTICORw , w [2, W ], denoted BAHW (ANTICOR). Figure 3 graphs total returnBAHW (ANTICOR) function W values 2 W 50 four datasetsconsider here. Considering graphs, choice W = 30 arbitrary clearly13. assertion based empirical studies conducted various expert advice algorithms.585fiBorodin, El-Yaniv, & GoganNYSE: Anticorw vs. window sizeTSX: Anticorw vs. window size120BAH(Anticor(Window))Anticor(Window)Best StockMarket Return81080Anticorw5Total ReturnTotal Return (logscale)10010BAH(Anticorw)AnticorwBest StockMarketBest StockAnticorwBest Stock6040210201100102510152025030510Window Size (w)(a)30BAH(Anticorw)AnticorwBest StockMarket Return2.5Best StockMarket ReturnAnticorw8Total ReturnTotal Return25DJIA: Anticorw vs. window size3BAH(Anticor )wAnticorw1020(b)SP500: Anticorw vs. window size1215Window Size (w)Anticorw621.5421Best StockBest Stock151015202530Window Size (w)51015202530Window Size (w)(c)Figure 2:0.5(d)ANTICORwtotal return (per $1 investment) vs. window size 2 w 30(a) NYSE; (b) TSX; (c) SP500; (d) DJIA. dashed (red) lines representfinal return best stock dash-dotted (blue) lines, final return(uniform) market. dotted (green) horizontal lines represent uniforminvestment number ANTICORw applications later described.optimal. course, could try optimize parameter W particular datasettraining algorithm historical data beginning trade. However, claimalmost choice W yield returns beat best stock (the exceptionW = 2 DJIA dataset).Since consider various algorithms stocks (whose prices determinedcumulative returns algorithms), back original portfolio selectionproblem ANTICOR algorithm performs well stocks may also perform wellalgorithms. thus consider active investment various ANTICORw algorithms usingANTICOR. consider windows w W . course, continue compoundalgorithm number times. compound twice use buy-and-holdinvestment. resulting algorithm denoted BAHW (ANTICOR(ANTICOR)). One impactcompounding, depicted Figure 4, smooth anti-correlations exhibitedstocks. evident compounding twice returns become almost completely586fiCan Learn Beat Best StockNYSE: Total Return vs. Max Window307625BAH (Anticor)WBAHW(Anticor)510Total ReturnTotal Return (logscale)10TSX: Total Return vs Max WindowBAHW(Anticor)10Best StockMarket410310Best Stock20BAHW(Anticor)15102Best StockMarketBest Stock1015100102102030402501020(a)1.6W6Total ReturnTotal Return1.45Best Stock4Figure 3:BAHW(Anticor)Best Stock1.2BAHW(Anticor)1312BAHW(Anticor)2030Best StockMarket0.8Best StockMarket1050DJIA: Total Return vs Max WindowBAH (Anticor)240(b)SP500: Total Return vs Max Window730Maximal Window Size (W)Maximal Window size (W)40502102030Maximal Window Size (W)Maximal Window Size (W)(c)(d)4050BAHW (ANTICOR)stotal return (per $1 investment) function maximalwindow W : NYSE (a); TSX (b); SP500 (c); DJIA (d).correlated thus diminishing possibility additional compounding substantiallyhelp.14 idea smoothing critical parameters may applicable learningapplications. challenge understand conditions applicationsprocess compounding algorithms smoothing effect.5. Empirical Comparison Algorithmspresent experimental study ANTICOR algorithm three onlinelearning algorithms described Section 3. focus BAH30 (ANTICOR), abbreviatedANTI1 BAH30 (ANTICOR(ANTICOR)), abbreviated ANTI2 . Four historical datasetsused. first NYSE dataset, one used (Cover, 1991; Cover & Ordentlich, 1996;Helmbold et al., 1998) (Blum & Kalai, 1998). dataset contains 5651 daily prices36 stocks New York Stock Exchange (NYSE) twenty two year period July3rd , 1962 Dec 31st , 1984. second TSX dataset consists 88 stocks TorontoStock Exchange (TSX), five year period Jan 4th , 1994 Dec 31st , 1998. third14. smoothing effect also allows use simple prediction algorithms expert advicealgorithms (Cesa-Bianchi et al., 1997), better predict good window size.explored direction.587fiBorodin, El-Yaniv, & GoganDJIA: Dec 14, 2002 Jan 14, 2003Anticor1Stocks1.12Anticor2.22.61Total Return2.820.92.41.80.82.21.60.721.40.61.81.20.51.610.45 10 15 20 25Days5 10 15 20 25Days5 10 15 20 25DaysFigure 4: Cumulative returns last month DJIA dataset: stocks (left panel);ANTICORw algorithms trading stocks (denoted ANTICOR1 , middle panel);ANTICORw algorithms trading ANTICOR algorithms (right panel).dataset consists 25 stocks SP500 (as Apr. 2003) largest marketcapitalization. set spans 1276 trading days period Jan 2nd , 1998 Jan 31st ,2003. fourth dataset consists thirty stocks composing Dow Jones IndustrialAverage (DJIA) two year period (507 days) Jan 14th , 2001 Jan 14th , 2003.15AlgorithmMarket (U-BAH)Best StockCBALU-CBALANTI1ANTI2LZEGUNIVERSALNYSE14.4954.14250.5927.0717,059,811.56238,820,058.1079.7827.0826.99TSX1.616.276.771.5926.7739.071.321.591.59SP5001.343.774.061.645.565.881.671.641.62DJIA0.761.181.230.811.592.280.890.810.80NYSE10.110.322.860.22246.221383.785.410.220.22TSX11.6737.6458.611.187.127.274.801.191.19SP50010.871.651.911.096.619.691.201.091.07DJIA11.432.772.971.533.674.601.831.531.53Table 1: Monetary returns dollars (per $1 investment) various algorithms fourdifferent datasets reversed versions. winner runner-upmarket appear boldface. figures truncated two decimals.four datasets quite different nature (the market returns datasetsappear first row Table 1). every stock NYSE increased value, 3288 stocks TSX lost money, 7 25 stocks SP500 lost money15. four datasets, including sources individual stock compositions downloadedhttp://www.cs.technion.ac.il/rani/portfolios.588fiCan Learn Beat Best Stock25 30 stocks negative market DJIA lost money. exceptionTSX, data sets include highly liquid stocks large market capitalizations.order maximize utility datasets yet present rather different markets,also ran market reverse. simply done reversing order invertingrelative prices. reverse datasets denoted -1 superscript.reverse markets particularly challenging. example, NYSE1 stocksgoing down. Note forward reverse markets (i.e. U-BAH) TSXincreasing TSX1 also challenging market since many stocks (56 88)declining.Table 1 reports total returns various algorithms eight datasets.see prediction algorithms LZ quite well aggressive ANTI1ANTI2 excellent sometimes fantastic returns. Note active strategiesbeat best stock even CBAL markets exception TSX1case still significantly outperform market. reader may well distrustfulappears unbelievable returns ANTI1 ANTI2 especially appliedNYSE dataset. However, recall NYSE dataset consists n = 5651 tradingdays n = total NYSE return approximately 1.0029511ANTI1 (respectively, 1.0074539 ANTI2 ); is, average daily increase less.3% (respectively, .75%). observe learning algorithms UNIVERSAL EGsubstantial advantage U-CBAL. previous expositions algorithmshighlighted particular combinations stocks returns significantly outperformedbest stock. said U-CBAL.DJIA: Cumulative Total Returns2.2Cumulative Total Return2Anti1Anti2Best StockMarketAnti21Anti1.81.61.41.2Best Stock10.8MarketJan01Jan02Jan03DateFigure 5: DJIA: Cumulative returns(the market).ANTI1 , ANTI2 ,best stock uniformBAHtotal returns ANTI1 ANTI2 presented Table 1 impressive fartelling complete story. Consider graphs figure 6. ANTI1 ANTI2perform well respect uniform market best stock throughoutinvestment period, periods cumulative return strategies589fiBorodin, El-Yaniv, & Gogandecrease. (not surprising) behavior indicates certain degree riskusing investment algorithms.finance standard risk measure standard deviation return. Table 2provide annualized returns risks well risk-adjusted returns marketsalgorithms considered here.16 example, annualized return best stockDJIA set 8.6%, annualized risk (standard deviation) 42% annualizedrisk-adjusted return (Sharpe ratio) 11%.AlgorithmMarket(U-BAH)Best StockCBALU-CBALANTI1ANTI2LZEGUNIVERSALNYSE12 14%58%19 24%63%27 30%78%15 13%88%110 28%367%136 35%370%21 23%76%15 13%88%15 13%87%TSX10 12%46%44 55%73%46 40%106%9 13%44%93 45%196%108 60%172%5 25%6%9 13%44%9 13%44%SP5005 24%8%30 51%50%31 42%65%10 22%28%40 37%95%41 44%86%10 25%25%10 22%28%10 22%27%DJIA12 24%-67%8 42%11%11 26%27%9 25%-54%26 35%62%50 39%119%5 28%-33%9 25%-54%9 25%-55%NYSE19 15%-86%4 21%-41%4 40%1%6 13%-77%27 27%86%38 33%101%7 21%176 13%-77%6 13%-77%TSX110 22%29%106 104%98%125 78%156%3 13%-3%48 41%107%48 46%96%36 27%117%3 13%-2%3 13%-2%SP50012 22%-28%10 32%20%13 27%35%1 21%-9%45 32%126%56 36%143%3 26%-0.8%1 22%-9%1 22%-11%DJIA119 25%61%65 114%54%71 76%88%23 25%77%90 31%277%113 35%304%35 27%112%23 25%77%23 25%76%Table 2: Annualized returns respective annualized volatilities well annualized riskadjusted returns (Sharpe Ratio) various algorithms three datasetsreversed versions. winner runner-up Sharpe Ratio marketappear boldface. figures truncated two decimals.6. Commissions, Trading Friction Caveatshandling portfolio stocks algorithm may perform transactionsper day. major concern therefore commissions incur. Within proportional commission model (see e.g. Blum & Kalai, 1998; Borodin & El-Yaniv, 1998, Section14.5.4) exists fraction (0, 1) investor pays rate /2buy sell. Therefore, returnsequence b1 , . . . , bn portfoliosQPrespect market sequence x1 , . . . , xn bt xt (1 j 2 |bt (j) bt (j)|) ,16. annualized return estimated using geometric mean ofthe individual daily returns riskstandard deviation daily returns multiplied 252 252 assumed standardnumber trading days per year. calculations standard. (annualized) Sharpe ratio(Sharpe, 1975) ratio annualized return minus risk-free return (taken 4%) divided(annualized) standard deviation.590fiCan Learn Beat Best Stockbt = bt1xt (bt (1)xt (1), . . . , bt (m)xt (m)).17 investment algorithm simplest formtolerate small proportional commission rates still beat best stock.graphs Figure 6 depict total returns BAH30 (ANTICOR) proportional commission factor = 0.1%, 0.2%, . . . , 1%. strategy withstand small commission factors.example, = 0.1% algorithm still beat best stock four marketsconsider (and beats market < 0.4%). Moreover still clearly beats marketwhenever < 0.4%.NYSE10305Best Stock10TSX25Anti1ReturnReturn (logscale)10Market2015105010060.20.40.60.8Commission Rate ()001SP50020.20.40.60.8Commission Rate ()1DJIA5ReturnReturn4321.51100.1 0.20.40.60.8Commission Rate ()Figure 6: Total returns0.1%, 0.2%, . . . , 1%.0.501BAH30 (ANTICOR)0.20.40.60.8Commission Rate ()1proportional commissions=However, current online brokers charge small proportional commissions, perhaps addition small flat commission rate trades.18 means largeinvestor scale investment suffer small proportional transaction rate.additional caveat assumption trades could implemented usingclosing price. principle nothing special closing price (i.e.algorithms trade time trading day) practical consideration relateddataset gathering availability dictated use prices.19 algorithms17. note Blum Kalai (1998) showed performance guarantee UNIVERSAL still holds(and gracefully degrades) case proportional commissions.18. example, USA site, E*TRADE (https://us.etrade.com) offers flat fee $10 trade5000 shares $.01/share thereafter.19. Specifically, historical closing prices public domain allow experimental reproducibility.Historical intraday trading quotes also gathered data usually protectedcostly obtain.591fiBorodin, El-Yaniv, & Goganassume portfolio adjustments implemented using quoted prices receiveinputs. means transactions implemented simultaneously usingquoted prices. current online brokers computerized system issue transactionorders almost instantly guarantee implemented instantly.trading friction necessarily generate discrepancies input pricesimplementation prices.related problem one must face actually trading differencebid ask prices. bid-ask spreads (and availability stocks buyingselling) functions stock liquidity typically small large market capitalizationstocks. consider large market cap stocks.report abnormal returns using historical markets suspected datasnooping. particular, historical data sets conditioned factstocks traded every day bankrupcies stocks becamevirtually worthless data sets. Furthermore, dataset excessivelymined testing many strategies substantial chance one strategiessuccessful simple over-fitting. Another data snooping hazard stock selection.ANTICOR algorithms fully developed using NYSE TSX datasets.DJIA SP500 sets obtained (from public domain sources) algorithmsfixed. Finally, algorithm one parameter (the maximal window size W ).experiments clearly indicate algorithms performance robust respect W(see, example, Figure 4).7. Concluding RemarksTraditional work financial economics tend focus understanding stock pricedetermination. main question is: predict stock market? Judgingextensive inconclusive work done financial forecasting, perhapsbeneficial question ask. Rather, computer program consistently outperformmarket? Besides practicality, clear successful portfolio selection algorithmmathematical model provide new intuition stock price formation.example, case, algorithms suggest stock price fluctuationssufficiently periodic anti-correlated.number well-respected works report statistically robust abnormal returnssimple technical analysis heuristics, slightly beat market. example,landmark study Brock, Lakonishok, LeBaron (1992) apply 26 simple tradingheuristics DJIA index 1897 1986 provide strong support technicalanalysis heuristics. consistently beating market considered significant (ifimpossible) challenge, approach portfolio selection indicates beating beststock achievable goal. mainly focused idealized frictionlesssetting, believe even frictionless setting (which seems like reasonablestarting point) results previously claimed literature.results presented raise various interesting questions. Since simple statisticalrelations correlation give rise outstanding returns plausible variousother, perhaps sophisticated machine learning techniques, give rise better592fiCan Learn Beat Best Stockportfolio selection algorithms capable larger returns tolerating larger commissionsfees.theoretical side, missing point time analytical modelbetter explains active trading strategies successful. regard,investigating various statistical adversary models along lines suggested Raghavan(1992) Chou et al. (1995). Namely, would like show algorithm performswell (relative benchmark) market sequence satisfies certain constraintsempirical statistics.One final caveat needs mentioned. Namely, entire theory portfolio selectionalgorithms assumes one portfolio selection algorithm impact market!like goose laying golden eggs, widespread use soon lead endgoose. case, market quickly react method consistentlysubstantially beat market.Acknowledgmentsthank Michael Loftus helpful comments. also thank Izzy Nelken SuperComputing Inc. help validating DJIA dataset.ReferencesBlum, A., & Kalai, A. (1998). Universal portfolios without transaction costs.Machine Learning, 30 (1), 2330.Borodin, A., & El-Yaniv, R. (1998). Online Computation Competitive Analysis. Cambridge University Press.Borodin, A., El-Yaniv, R., & Gogan, V. (2000). competitive theory practiceportfolio selection. Proc. 4th Latin American Symposium TheoreticalInformatics (LATIN00), pp. 173196.Brock, L., Lakonishok, J., & LeBaron, B. (1992). Simple technical trading rulesstochastic properties stock returns. Journal Finance, 47, 17311764.Cesa-Bianchi, N., Freund, Y., Haussler, D., Helmbold, D., Schapire, R., & Warmuth, M.(1997). use expert advice. Journal ACM, 44 (3), 427485.Chou, A., Cooperstock, J., El-Yaniv, R., Klugerman, M., & Leighton, T. (1995).statistical adversary allows optimal money-making trading strategies. Proceedings6th Annual ACM-SIAM Symposium Discrete Algorithms.Cover, T. (1991). Universal portfolios. Mathematical Finance, 1, 129.Cover, T., & Gluss, D. (1986). Empirical bayes stock market portfolios. Advances AppliedMathematics, 7, 170181.Cover, T., & Ordentlich, E. (1996). Universal portfolios side information. IEEETransactions Information Theory, 42 (2), 348363.Cover, T., & Thomas, J. (1991). Elements Information Theory. John Wiley & Sons, Inc.Feder, M. (1991). Gambling using finite state machine. IEEE Transactions InformationTheory, 37, 14591465.593fiBorodin, El-Yaniv, & GoganHelmbold, D., Schapire, R., Singer, Y., & Warmuth, M. (1998). Portfolio selection usingmultiplicative updates. Mathematical Finance, 8 (4), 325347.Langdon, G. (1983). note Lempel-Ziv model compressing individual sequences.IEEE Transactions Information Theory, 29, 284287.Lo, A., & MacKinlay, C. (1999). Non-Random Walk Wall Street. PrincetonUniversity Press.Lugosi, G. (2001).Lectures predictionURL:http://www.econ.upf.es/lugosi/ihp.ps.individualsequences.Markowitz, H. (1959). Portfolio Selection: Efficient Diversification Investments. JohnWiley Sons.Raghavan, P. (1992). statistical adversary on-line algorithms. dimacs SeriesDiscrete Mathematics Theoretical Computer Science, 7, 7983.Rissanen, J. (1983). universal data compression system. IEEE Transactions Information Theory, 29, 656664.Sharpe, W. (1975). Adjusting risk portfolio performance measurement. JournalPortfolio Management, 2934. Winter.Ziv, J., & Lempel, A. (1978). Compression individual sequences via variable rate coding.IEEE Transactions Information Theory, 24, 530536.594fiJournal Artificial Intelligence Research 21 (2004) 101-133Submitted 3/03; published 2/04Complexity Results Approximation Strategies MAPExplanationsJames D. ParkAdnan Darwichejd@cs.ucla.edudarwiche@cs.ucla.eduComputer Science DepartmentUniversity CaliforniaLos Angeles, CA 90095AbstractMAP problem finding probable instantiation set variables givenevidence. MAP always perceived significantly harder relatedproblems computing probability variable instantiation (Pr), problemcomputing probable explanation (MPE). paper investigates complexityMAP Bayesian networks. Specifically, show MAP complete NPPPprovide negative complexity results algorithms based variable elimination.also show MAP remains hard even MPE Pr become easy. example,show MAP NP-complete networks restricted polytrees, eveneffectively approximated.Given difficulty computing MAP exactly, difficulty approximatingMAP providing useful guarantees resulting approximation, investigatebest effort approximations. introduce generic MAP approximation framework.provide two instantiations framework; one networks amenable exactinference (Pr), one networks even exact inference hard.allows MAP approximation networks complex even exactly solveeasier problems, Pr MPE. Experimental results indicate using approximationalgorithms provides much better solutions standard techniques, provide accurateMAP estimates many cases.1. Introductiontask computing Maximum Posteriori Hypothesis (MAP) findlikely configuration set variables given partial evidence complementset. focus paper complexity computing MAP Bayesian networks,class best effort methods approximating MAP.One specialization MAP received lot attention ProbableExplanation (MPE). MPE problem finding likely configuration setvariables given complete evidence complement set. primary reasonattention MPE seems much simpler problem MAP generalization.Unfortunately, MPE always suitable task providing explanations. Considerexample problem system diagnosis, component associatedvariable representing health. Given evidence system behavior, oneusually interested computing probable configuration health variables.MAP problem since available evidence usually specify valuec2004AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiPark & Darwichenonhealth variable. common approximate problem using MPE, caseone finding likely configuration every unknown variable, including healthvariables variables particular interest, inputs outputssystem components. However, projection MPE solution health variablesusually likely configuration. Neither configuration obtained choosinglikely state health variable separately.MAP turns difficult problem, even compared MPE problem,Pr problem computing probability evidence. Specifically, provideSection 2 complexity results indicate neither exact approximatesolutions guaranteed MAP, even restricted circumstances. Yet, MAPremains important problem would like generate solutions. Therefore,propose Section 3 general framework based local search besteffort approximationMAP. also provide two specific instances proposed framework, one applicablenetworks amenable exact computation Pr given Section 3,applicable networks even amenable Pr given Section 4.report experimental results method using realworld randomly generatedBayesian networks, illustrate effectiveness proposed framework widerange networks. close paper concluding remarks Section 5.2. MAP Complexitybegin section reviewing complexity theory classes terminology pertain complexity MAP. examine complexity MAP general case,followed examining complexity number MAP variables constrained.consider complexity MAP algorithms based variable elimination.conclude complexity section examining complexity MAP polytrees.2.1 Complexity Reviewassume reader familiar basic notions complexity theory likehardness completeness languages, well complexity class NP.addition NP, also interested class PP derivative it.Informally, PP class contains languages exists nondeterministic Turing machine majority nondeterministic computations acceptstring language. PP thought decision versionfunctional class #P. such, PP powerful language. fact NP PP,inequality strict unless polynomial hierarchy collapses second level. 1Another idea need concept oracle. Sometimes useful askquestions could done operation free. complexity theorymodeled Turing machine oracle. oracle Turing machine Turing machineadditional capability able obtain answers certain queries singletime step. example, may want designate class languages could1. direct result Todas theorem (Toda, 1991). Todas theorem PPP contains entirepolynomial hierarchy (PH), NP = PP, PH PPP = PNP .102fiComplexity Results Approximation Strategies MAP Explanationsrecognized nondeterministic polynomial time PP query could answered free.class languages would NP PP oracle, denoted NP PP .Consider Boolean expression variables X 1 , . . . , Xn . following threeclassical problems complete complexity classes:SAT: truth assignment (world) satisfies ? problem NPcomplete.MAJSAT: majority worlds satisfy ? problem PPcomplete.E-MAJSAT: instantiation variables X 1 ,. . . ,Xk , 1 k n,majority worlds satisfy ? problem NP PP complete.Intuitively, solve NPcomplete problem search solution amongexponential number candidates, easy decide whether given candidateconstitutes solution. example, SAT, searching world satisfiessentence (testing whether world satisfies sentence done time linearsentence size). solve PPcomplete problem, add weights solutions,easy decide whether particular candidate constitutes solutionalso easy compute weight solution. example, MAJSAT, solutionworld satisfies sentence weight solution 1. Finally, solveNPPP complete problem, search solution among exponential numbercandidates, also need solve PPcomplete problem order decide whetherparticular candidate constitutes solution. example, E-MAJSAT, searchinginstantiation x1 , . . . , xk , test whether instantiation satisfies conditionwant, must solve MAJSAT problem.2.2 Decision Problemsdealing decision versions Bayesian network problems paper,define formally section.Bayesian network pair (G, ), G directed acyclic graph (DAG)variables X, defines conditional probability table (CPT) X|U variableX parents U DAG G. is, value x variable Xinstantiation u parents U, CPT X|U assigns number [0, 1], denoted x|u ,represent probability x given u. 2 probability distribution Pr inducedBayesian network (G, ) given follows. complete instantiation x networkvariables X, probability x givenPr(x)def=x|u ,xuxxu instantiation family (a variable parents) representscompatibility relation among instantiations. is, probability assignedcomplete variable instantiation x product parameters consistentinstantiation.following decision problems assume given Bayesian network (G, )rational parameters induces probability distribution Pr. Moreover,evidence e, mean instantiation variables E.2. Hence, mustPxx|u = 1.103fiPark & DarwicheD-MPE: Given rational number p, evidence e, set network variables X,instantiation x Pr(x, e) > p?D-PR: Given rational number p evidence e, Pr(e) > p?D-MAP: Given rational number p, evidence e, set variables Q,instantiation q Pr(q, e) > p? Variables Q called MAP variablescase.decision problems useful examining complexity finding exact solution, really interested functional problem actually computingsolution. cant solve problem exactly, would also like know closeget efficiently. consider approximation algorithms. definenotion approximation factor use discussing complexity approximation algorithms. Specifically, say approximate solution 0 withinapproximation factor > 1 true solution case 0 . Moreover,say algorithm provides f (n)factor approximation case problemssize n, approximate solutions returned algorithm within approximationfactor f (n).2.3 MAP Complexity General CaseComputing MPE, Pr, MAP NPHard, still appears significantdifferences complexity. MPE basically combinatorial optimization problem.Computing probability complete instantiation trivial, real difficultydetermining instantiation choose. D-MPE NP-complete (Shimony, 1994). Prcompletely different type problem, characterized counting instead optimization,need add probability network instantiations. D-PR PP-complete(Litmman, Majercik, & Pitassi, 2001)notice complexity decisionversion, functional version #P-complete (Roth, 1996). MAP combinescounting optimization paradigms. order compute probabilityparticular instantiation, Pr query needed. Optimization also required, orderable decide many possible instantiations. reflected complexityMAP.Theorem 1 D-MAP NPPP -complete.3Proof: Membership NPPP immediate. Given instantiation q MAP variables,verify solution querying PP oracle Pr(q, e) > k.show hardness, reduce E-MAJSAT (Littman, Goldsmith, & Mundhenk, 1998)D-MAP first creating Bayesian network models Boolean formula .variable Xi formula , create analogous variable network values{T, F } uniform prior probability. Then, logical operator, create variablevalues {T, F } whose parents variables corresponding operands, whoseCPT encodes truth table operator (see Figure 1 simple example). Let Vnetwork variable corresponding top level operand.3. result stated without proof Littman (1999).104fiComplexity Results Approximation Strategies MAP ExplanationsX1X2X3Figure 1: Bayesian network produced using reduction Theorem 1 Booleanformula (x1 x2 ) x3 .complete instantiation x variables X appearing Boolean expression , evidence V = , have:Pr(x, V = ) =(12n0x satisfiesotherwiseparticular instantiation q MAP variables X 1 , ..., Xk , evidence V = , have:Pr(q, V = ) =XPr(q, xk+1 , ..xn )xk+1 ,...,xn=#q2n#q number complete variable instantiations compatible q satisfies. Since 2nk possible instantiations Xk+1 , ..., Xn , fraction fq satisfied#q /2nk ,fqPr(q, V = ) = k2Thus, instantiation q MAP variables compatible halfcomplete, satisfying instantiations Pr(q, V = ) > 1/2k+1 . MAP queryvariables X1 , ..., Xk evidence V = threshold 1/2k+1 trueE-MAJSAT query also true. 2fact, theorem strengthened.Theorem 2 D-MAP remains complete NP PP even (1) network depth 2,(2) evidence, (3) variables Boolean, (4) network parameters lieinterval [ 21 , 21 + ] fixed > 0.proof appears Appendix A. Unlike computing probabilities, becomes easynumber evidence nodes bounded constant parametersbounded away 0 (it falls RP described Dagum & Luby, 1997), MAP retainsNPPP complexity even restrictive circumstances. 44. altogether surprising since evaluating score possible solution, MAP variablesact evidence variables.105fiPark & DarwicheNPPP powerful class, even compared NP PP. NP PP contains importantAI problems, probabilistic planning problems (Littman et al., 1998). threeclasses related NP PP NPPP , equalities considered unlikely.fact, NPPP contains entire polynomial hierarchy (Toda, 1991). Additionally,MAP generalized Pr, MAP inherits wild nonapproximability Pr. Bayesiannetwork simulates SAT reduction get:Corollary 3 Approximating MAP within approximation factor f (n) NPhard.Proof: Using evidence V = , exact MAP solution number satisfyinginstantiations divided 2n , 0 unsatisfiable, positive satisfiable.formula unsatisfiable, approximate solution must 0 0/ = 00 0 = 0, = f (n). formula satisfiable, approximate solutionmust positive since M/ > 0. Thus test satisfiability testing approximateMAP solution zero not.22.4 Complexity Parameterized Number Maximization Variablesexamine complexity restrict number maximization variables.Let n number nonevidence variables, k number maximizationvariables. extreme case k = 0, simply D-PR, PPcomplete.extreme, k = n, becomes D-MPE, NPcomplete. constrainingnumber maximization variables dramatic impact complexity.examine issue detail. Let D-MAP subset D-MAP problemsk = O(m), let D-MAPm subset D-MAP problems n k = O(m).consider complexity parameterized classes problems. primaryresults following:Theorem 4 D-MAPlog n PPP , D-MAPlog n NP. However, > 0,D-MAPn D-MAPn remain NPPP complete.Proof: First, k = O(log n), number possible instantiations maximizationvariables bounded polynomial. Thus, given PP oracle, possible decideproblem polynomial time asking instantiation q maximizationvariables whether Pr(q, e) exceeds threshold. Similarly, n k = O(log n),instantiation q maximization variables, test see Pr(q, e) exceedsthreshold summing polynomial number compatible instantiations.k = O(n ) provide simple reduction solve D-MAP problemcreating polynomially larger one satisfying constraint number maximizationvariables. unconstrained problem, simply create new problem addingpolynomial number irrelevant variables, parents children. Similarly,provide reduction general D-MAP problem one constrained n k =O(n ), adding polynomial number maximization variables parents,children, deterministic priors. 2106fiComplexity Results Approximation Strategies MAP Explanations2.5 Results Elimination AlgorithmsSolution general MAP problem seems reach, easier networks? Stateoftheart exact inference algorithms (variable elimination (Dechter, 1996),join trees (Lauritzen & Spiegelhalter, 1988; Shenoy & Shafer, 1986; Jensen, Lauritzen, &Olesen, 1990), recursive conditioning (Darwiche, 2001)) compute P r MPE spacetime complexity exponential width given elimination order.allows many networks solved using reasonable resources even though generalproblems difficult. Similarly, stateoftheart MAP algorithms also solve MAPtime space complexity exponential width used elimination orderbut, MAP, orders used. section, investigate complexityvariable elimination MAP.analyzing complexity variable elimination MAP, review variableelimination. First, need concept potential. potential simply functionsubset variables, maps instantiation variables real number.size potential parameterized number instantiations variables,exponential number variables. Notice CPTs potentials.order use variable elimination Pr, MPE MAP, need three simple operations:multiplication, summingout, maximization. Multiplication two potentials 12 variables XY YZ respectively (where set variablescommon), defined (1 2 )(xyz) = 1 (xy)2 (yz). Notice X Znonempty, size 1 2 greater size either 1 2 . SumOut(,Y)variables XY definedSumOut(, Y)(x) =X(xy),ranges instantiations Y. Maximization similar summingmaximizes unwanted variables:Maximize(, Y)(x) = max (xy).order handle evidence, need concept evidence indicator. evidenceindicator E associated evidence E = e potential variable E E (e) = 1,zero values.Given variable ordering , variable elimination used compute probabilityevidence e follows:1. Initialize P contain evidence indicators e conditional probability tables.2. variable X, according order ,(a) Remove P potentials mentioning X.(b) Let MX product potentials.(c) add SumOut(MX , X) P .3. Return product potentials P .107fiPark & Darwicheiteration, variable X eliminated leads removing mention XP . step 3, variables removed, potentials remaining constantsresulting product single number representing probability evidence e. MPEcomputed way, except projection step 2c replaced maximization.complexity variable elimination linear number variables linearsize largest potential MX produced step 2b. size largest potential variessignificantly based elimination order. width elimination order simplylog2 (size(MX ))1 MX largest potential produced using elimination order. 5treewidth Bayesian network minimum width elimination orders.Pr MPE, elimination order used, complexity linear numbervariables exponential treewidth. true MAP. Variableelimination MAP similar methods, extra constraint.step 2c, X MAP variable projection replaced maximization.MAP variable projection used. extra constraint orders valid.Maximization projection commute, maximization must performed last.means elimination order valid performing MAP, X MAPvariable, potential step 2b must mention non-MAP variables. practiceensured requiring elimination order eliminate MAP variableslast. tends produce elimination orders widths much larger availablePr MPE, often placing exact MAP solutions reach.order assess magnitude increase width caused restricting elimination order, randomly generated 100 Bayesian networks, containing 100 variables,according first method Appendix B. network, computed widthusing minfill heuristic (Kjaerulff, 1990; Huang & Darwiche, 1996). Then, repeatedly added single variable set MAP variables, computed constrainedwidth, using minfill, eliminating MAP variables last. process repeated variables MAP variable set. Figure 2 contains statisticsexperiments. Xaxis corresponds number MAP variables (thus X = 0corresponds unconstrained width). axis corresponds width found.graph details minimum, maximum, mean, weighted mean 100 networks. weighted mean takes account complexity exponentialwidth, provides better representation average complexity. computedPlog2 ( n1 ni=1 2wi ). Notice unconstrained widths range 11 18,number MAP variables increases, width increases dramatically. example,even quarter variables MAP variables (X = 25) widths range22 34, (which corresponds roughly difficult doable well beyondtodays inference algorithms handle todays computers) weighted average 30. Notice also, would expect complexity analysis, problemsmany MAP variables significantly easier middlerange.consider question whether less stringent conditions validelimination orders, may allow orders smaller widths.5. -1 definition preserve compatibility previously defined notion treewidthgraph theory.108fiComplexity Results Approximation Strategies MAP Explanations80maxweighted meanmeanminConstrained Width70605040302010020406080Number MAP variables100Figure 2: Statistics constrained width different numbers MAP variables.Xaxis number MAP variables, axis width. Noticewidths required general MAP problem significantly largerPr MPE, correspond X = 0 X = 100 respectively.described earlier, given ordering, elimination algorithms work steppingordering, collecting potentials mentioning current variable, multiplying them,replacing potential formed summing (or maximizing) currentvariable product. process thought induce evaluation tree; seeFigure 3. evaluation tree elimination order described follows. leavescorrespond CPTs given Bayesian network, internal nodes correspondpotentials created elimination process. children potential representpotentials multiplied together constructing . Noteinternal node elimination tree corresponds variable order , whose elimination leads constructing node; Figure 3(b). Therefore, evaluation treeviewed inducing partial elimination order; see Figure 3(c).standard way constructing valid elimination order MAP eliminateMAP variables Q last. Two questions present themselves. First, valid orderingsvariables Q eliminated last? second, so, produce widthssmaller generated eliminating variables Q last?answer first question yes, valid elimination ordersvariables Q eliminated last. see that, suppose variable orderinduces particular evaluation tree , let partial elimination ordercorresponding . variable order 0 consistent partial order109fiPark & DarwicheCCCCD BBCEa.BCCPr(A) Pr(B|A) Pr(C|A) Pr(D|BC) Pr(E|C)b.EEBc.Figure 3: (a) Bayesian network, (b) evaluation tree elimination orderA, E, B, D, E, (c) partial elimination order induced evaluation tree.also induce tree . Hence, order valid, order 0 also valid. Figure 3shows evaluation tree induced using order A, E, B, D, C computing MAPvariables Q = C, D. order A, B, D, E, C consistent evaluation treeand, hence, also valid computing MAP variables C, D. Yet, variables C,appear last order.Unfortunately, orders variables Q eliminated last help.Theorem 5 elimination order valid computing MAP variablesQ, ordering width variables Q eliminated last.Proof: Consider evaluation tree induced valid elimination order, corresponding partial order induces. variable Q parent variable Q.prove this, suppose X parent evaluation tree, X QQ. means potential results eliminating variable includesvariable X, also means X must appeared potentialsmultiplied elimination variable . contradiction since evaluationtree underlying order valid. Since variable Q parent variableQ, variables Q eliminated first order consistent partial orderdefined evaluation tree. Then, variables Q eliminated, obeyingpartial ordering defined evaluation tree. produced orderelimination tree original order, width. 22.6 MAP PolytreesTheorem 5 significant complexity implications elimination algorithms even polytrees.Theorem 6 Elimination algorithms require exponential resources perform MAP, evenpolytrees.Proof: Consider computing MAP variables X 1 , . . . , Xn given evidence Sn =network shown Figure 4. Theorem 5, order whose width smaller110fiComplexity Results Approximation Strategies MAP Explanationsorder eliminate variables 0 , . . . , Sn first, variablesX1 , . . . , Xn last. easy show though order width n. Hence,variable elimination require exponential resources using order. 2set MAP variables makes crucial difference complexity MAP computations. example, MAP variables X 1 , . . . , Xn/2 , S0 , . . . , Sn/2 insteadX1 , . . . , Xn solved linear time.negative findings specific variable elimination algorithms. questionwhether difficulty idiosyncrasy variable elimination avoideduse method computing MAP. following result, however,shows finding good general algorithm MAP polytrees unlikely.Theorem 7 MAP N P complete restricted polytrees.Proof: Membership immediate. Given purported solution instantiation q,compute Pr(q, e) linear time test bound. show hardness, reduceMAXSAT MAP polytree.6 Similar reductions used PapadimitriouTsitsiklis (1987) Littman et al. (1998) relating partially observable Markov decisionproblems, probabilistic planning respectively. MAXSAT problem definedfollows:Given set clauses C1 , ..., Cm variables Y1 , ..., Yn integer boundk, assignment variables, k clausessatisfied.idea behind reduction model random selection clause,successively checking whether instantiation variable satisfies selected clause.clause selector variable S0 possible values 1, 2, ..., uniform prior.propositional variable Yi induces two network variables Xi Si , Xi representsvalue Yi , uniform prior, Si represents whether Y1 , ..., Yi satisfyselected clause. Si = 0 indicates selected clause satisfied one 1 , ..., Yi .Si = c > 0 indicates selected clause C c satisfied Y1 , ..., Yi . parentsSi Xi Si1 (the topology shown Figure 4). CPT , 1defined1 si = si1 = 01 si = 0 si1 = j,xi satisfies cjP r(si |xi , si1 ) =1si = si1 = j xisatisfy cj0 otherwisewords, selected clause satisfied first 1 variables (s i1 6= 0),xi satisfies it, Si becomes satisfied (si = 0). Otherwise, si = si1 . Now,particular instantiation s0 S0 , instantiation x variables X 1 , ..., Xn ,Pr(s0 , x, Sn = 0) =(1/(m2n ) x satisfies clause Cs0 ;0otherwise.6. Actually, need reduce SAT, MAXSAT result used Theorem 8.111fiPark & DarwicheS0X1X2S1S2Xn...SnFigure 4: network used reduction Theorem 7.Summing S0 yields P r(x, Sn = 0) = #C /(m2n ) #C number clausesx satisfies. Thus MAP X1 , ..., Xn evidence Sn = 0 bound k/(m2n ) solvesMAXSAT problem well. 2Since MAXSAT polynomial time approximation scheme (unless P = NP),polynomial time approximation scheme exists MAP polytrees. fact, approximatingMAP polytrees appears much harder approximating MAXSAT.Theorem 8 Approximating MAP polytrees within factor 2 n NP-hardfixed , 0 < 1, n size problem.proof appears Appendix A. So, hard approximate within constantfactor, hard approximate within polynomial factor, even subexponential factor.close section summary complexity results section:MAP NPPP complete arbitrary Bayesian networks, even evidence,every variable binary, parameters arbitrarily close 1/2.NPhard approximate MAP within factor f (n).Variable elimination MAP requires exponential time, even polytrees.MAP NPcomplete networks polytree structure.Approximating MAP polytrees within factor 2 n NPhard fixed[0, 1).3. Approximating MAP Inference EasySince exact MAP computation often intractable, approximation techniques needed.Unfortunately, spite MAPs utility, relatively little work done approximating it. fact, two previous methods approximating MAPaware of. first (Dechter & Rish, 1998) uses minibucket technique.(de Campos, Gamez, & Moral, 1999), uses genetic algorithms approximatebest k configurations MAP variables (this problem known partial abduction).Practitioners typically resort one two simple approximation methods. One commonapproximation technique compute MPE instantiation project resultMAP variables. is, want compute MAP variables given evidencee, S0 complement variables E, compute instantiation s, 0maximizes Pr(s, s0 | e) return s. method computes posterior marginals112fiComplexity Results Approximation Strategies MAP ExplanationsMAP variables, Pr(S | e), S, choose likely statevariable given e.propose general framework approximating MAP. MAP consists two problemshard generaloptimization inference. MAP approximation algorithmproduced substituting approximate versions either optimization inferencecomponent (or both). optimization problem defined MAP variables,score solution candidate instantiation MAP variables (possiblyapproximate) probability Pr(s, e) produced inference method. allows solutionstailored specific problem. networks whose treewidth manageable, containshard optimization component (e.g. polytree examples discussed previously), exactstructural inference used, coupled approximate optimization algorithm.Alternatively, optimization problem easy (e.g. MAP variables)network isnt amenable exact inference, exact optimization method couldcoupled approximate inference routine. components hard,optimization inference components need approximated.investigate section family approximation algorithms based local search.first consider case inference tractable, develop extension handlecase inference infeasible. local search algorithms work basically follows:1. Start initial guess solution.2. Iteratively try improve solution moving better neighbor 0 : Pr(s0 | e) >Pr(s | e), equivalently Pr(s0 , e) > Pr(s, e).neighbor instantiation defined instantiation results changingvalue single variable X s. new value X x, denote resultingneighbor X, x. order perform local search efficiently, need computescores neighbors sX, x efficiently. is, need compute Pr(sX, x, e)X values x s. variables binary values,| | neighbors case.Local search proposed method approximating MPE (Kask & Dechter,1999; Mengshoel, Roth, & Wilkins, 2000). MPE, MAP variables containvariables E (the evidence variables). Therefore, score neighbor,Pr(s X, x, e), computed easily since X, x, e complete instantiation. fact,given computed Pr(s, e), score Pr(sX, x, e) computed constanttime.7Unlike MPE, computing score neighbor, Pr(s X, x, e), MAP requiresglobal computation since X, x, e may complete instantiation. One mainobservations underlying approach, however, score Pr(s X, x, e)computed O(n exp(w)) time space, n number network variables7. assumes none entries CPTs 0. 0 entries CPTs, may taketime linear number network variables compute score. Pr(s, e) product singleentry CPT compatible s, e. changing state variable X x x 0 ,values product change CPTs X children. noneCPT entries 0, P r(s X, x0 , e) computed dividing Pr(s, e) old multiplyingnew entry CPTs X children. done constant time numberchildren bounded constant.113fiPark & Darwichew width arbitrary elimination order, i.e., use elimination orderpurpose, need constraints. fact, even bettercomputing scores neighbors, Pr(s X, x, e) X every value xX, O(n exp(w)) time space. Thus, elimination order width wgiven Bayesian network, perform search step O(n exp(w)) timespace. shall see later, takes small number search steps obtain goodMAP solution. Hence, overall runtime often O(n exp(w)) too. Therefore,produce good quality MAP approximations time space exponentialunconstrained width instead constrained one, typically much larger.local search method proposed section differs local search methodsused MPE unconstrained width must small enough search stepperformed relatively efficiently. pointless use method approximateMPE since time take one step, MPE could computed exactly. methodapplicable unconstrained width reasonable constrained width(see Figure 2).3.1 Computing Neighbor Scores Efficientlykey computing neighbor scores efficiently express inference problemfunction evidence indicators. state x variable X, evidenceindicator x one compatible evidence, zero otherwise.common technique typically used allow modeling wider range evidenceassertions. example, allows evidence assertions X 6= x setting x = 0,remaining indicators X one. use different purpose however.inference problem cast function f evidence indicators (f ( e ) = Pr(e),e consists evidence indicators, set compatible e),f(e ) = Pr(e X, x). add current state evidence,xpartial derivative yields Pr(s X, x, e), precisely score one neighbors.use jointree algorithm (Park & Darwiche, 2003), differential inferenceapproach (Darwiche, 2003) compute partial derivatives efficiently. differential approach, values immediate, entire approach based evaluatingdifferentiating expression f above. also computed using jointrees usingShenoyShafer propagation scheme. Specifically, evidence indicator table addedvariable, evidence variable entered setting appropriateindicator entries. partial derivatives indicators associated variableobtained multiplying tables assigned cluster, messagescluster, projecting product onto variable. either case, partial derivatives indicators, thus score neighbors, computedO(n exp(w)) time, complexity simply computing scorecurrent state.3.2 Search Methodstested two common local search methods, stochastic hill climbing taboo search.Stochastic hill climbing proceeds repeatedly either changing state variable114fiComplexity Results Approximation Strategies MAP ExplanationsGiven: Probability distribution Pr, evidence e, MAP variables S,probability taking random flip Pf , initial state s0 .Compute: instantiation (approximately) maximizes Pr(s | e).Initialize current state s0 .sbest =Repeat manytimes:probability Pf= s0 , s0 randomly selected neighbor s.OtherwiseCompute score P r(s X, x, e) neighbor X, x.neighbor higher score score= s0 , s0 randomly selected neighbor s.Else= s0 s0 neighbor highest score.P r(s, e) > P r(sbest , e)sbest =Return sbestFigure 5: Stochastic hill climbing algorithm.creates maximum probability change, changing variable random. Figure 5gives algorithm explicitly.Taboo search similar hill climbing except next state chosenbest state hasnt visited recently. number iterations relativelysmall save previous states iteration unique point chosen.Pseudocode taboo search appears Figure 6.3.3 Initializationquality solution returned local search routine depends large extentpart search space given explore. implemented several algorithmscompare solution quality different initialization schemes. Suppose nnumber network variables, w width given elimination order,number MAP variables.1. Random initialization (Rand). MAP variable, select value uniformlyset states. method takes O(m) time.2. MPE based initialization (MPE). compute MPE solution given evidence.Then, MAP variable, set value value variable takesMPE solution. method takes O(n exp(w)) time.3. Maximum likelihood initialization (ML). MAP variable X, set valueinstance x maximizes P r(x | e). method takes O(n exp(w)) time.4. Sequential initialization (Seq). method considers MAP variables X 1 , . . . , Xm ,choosing time variable Xi highest probability Pr(x | e, y) one115fiPark & DarwicheGiven: Probability distribution Pr, evidence e, MAP variables S.Compute: instantiation (approximately) maximizes Pr(s | e).Initialize current state s.sbest =Repeat many timesAdd visitedCompute score P r(s X, x, e) neighbor X, x.= s0 s0 neighbor highest score visited .neighbor exists (this rarely occurs)Repeat several times= s0 s0 randomly selected neighbor s.P r(s, e) > P r(sbest , e)sbest =Return sbestFigure 6: Taboo search. Notice action taken choose best neighborhasnt visited. leads moves decrease score peakdiscovered.values xi , instantiation MAP variables considered far.method takes O(mn exp(w)) time.3.4 Experimental ResultsTwo search methods (Hill Taboo) four initialization methods (Rand, MPE, ML,Seq) lead 8 possible algorithms. initialization methods also viewedapproximation algorithm since one simply return computed initialization.leads total 12 different algorithms. experimentally evaluated compared 11algorithms, leaving algorithm corresponding random initialization.tested algorithms various synthetically generated data sets well realworld networks. synthetic networks, generated random network structures usingtwo generation methods (see Appendix B). structure, quantified CPTsdifferent bias coefficients 0 (deterministic except roots), .5 (values chosenuniformly) could evaluate influence CPT quantification solution quality.network consisted 100 variables, root variables chosen MAPvariables. 25 root variables, randomly selected 25MAP variables. Otherwise used root variables. chose root nodesMAP variables typically subset root nodes variables interestdiagnostic applications. Evidence set instantiating leaf nodes. Care takeninsure instantiation non zero probability. algorithm allowed 150network evaluations.8 computed true MAP compared solutions foundalgorithm. Additionally, measured number network evaluations neededfind solution algorithm subsequently returned, number peaks discovered8. evaluation takes O(n exp(w)) time space, n number network variables wwidth given elimination order.116fiComplexity Results Approximation Strategies MAP ExplanationsData Set 10Rand-Hill 147Rand-Taboo 181ML 526ML-Hill 920ML-Taboo 942MPE 999MPE-Hill 999MPE-Taboo 1000Seq 930Seq-Hill 941Seq-Taboo 962Solution Quality.125 .250.375805 917946969 985993497 676766947 989993988 999999333 160127875 923952986 992990965 990999971 992999998 1000 1000.596699581799710001009739989979971000Table 1: solution quality method first data set. number associatedmethod bias number instances solved correctly 1000.best scores bias shown bold.Data Set 20Rand-Hill 20Rand-Taboo 20ML 749ML-Hill 966ML-Taboo 973MPE 858MPE-Hill 961MPE-Taboo 978Seq 988Seq-Hill 988Seq-Taboo 994Solution Quality.125 .250 .375634 713 799851 907 943453 495 519922 947 963960 986 987505 365 275853 850 874952 962 977955 964 985960 966 986977 990 994.5845965514962990206891980972976994Table 2: solution quality method second data set. number associated method bias number instances solved correctly1000. best scores bias shown bold.solution discovered. hill climbing method used data setspure hill climbing random walk restart. is, hill climbs reaches peak,randomly flips values move new location.generated 1000 random network structures two structural generation methods. random structure generated, quantification method,quantified network, computed exact MAP, applied approximation117fiPark & Darwichealgorithms. Tables 1 2 show solution quality methods reportingfraction networks solved correctly; is, approximate answervalue exact answer.One draw number observations based experiments:case, taboo search performed slightly better hill climbing randomrestarts.search methods typically able perform much better initializationalone.Even random start, search methods able find optimal solutionmajority cases.Overall, taboo search sequential initialization performed best, requirednetwork evaluations.Table 3 contains statistics number network evaluations (includingused initialization) needed achieve value method finally returned.mean number evaluations quite small methods. Surprisingly, hillclimbing methods, maximum also quite small. fact, analyzing resultsdiscovered hill climbing methods never improved first peak discovered.9 suggests one viable method quick approximation simply climbfirst peak return result. Taboo search hand able improvefirst peak cases.ran ten MAP queries real world network tested. queryrandomly selected one fourth nodes variables interest, selectedone fourth nodes evidence nodes. evidence values chosen uniformlyamong nonzero configurations. previous experiments demonstrated largenumber iterations rarely helps, reduced number iterations 30. Also,moved away hill climbing random restart stochastic hill climbing (performingrandom move probability .35) since previous experiments random restartnever helped. Also, ran minibucket approximation algorithm (the MAPapproximation algorithm aware subsumed technique) compareperformance algorithms. Since exact MAP computations networkshard current algorithms handle, compare algorithms based relativeperformance only.Table 4 shows number times (out ten) algorithm able producehighest probability configuration discovered. search based methods performedmuch better algorithms. Note outperformed minibucket approximations network. Table 5 provides specific detailsrelative performance network. block contains count numbertimes method produced solutions within range best found solution.9. appears random walk used restarting make eventually selecting better regionlikely using search steps. Often, sub optimal hill encountered, optimalhill 2 3 moves away. cases, taboo search usually able find (becausesearch guided), random walking not.118fiComplexity Results Approximation Strategies MAP ExplanationsEvaluations RequiredMethod Mean StdevRand Hill12.52.5Rand Taboo14.311.0MPE10MPE Hill2.61.3MPE Taboo4.08.3ML10ML Hill1.6.74ML Taboo1.93.3Seq250Seq Hill25.0.04Seq Taboo25.0.9Max21144181371462252645Table 3: Statistics number evaluations method required achievingvalue eventually returned. based random method 2, bias .5data set. statistics data sets similar.BarleyMildewMunin2Munin3PigsWater366909MPEH9810 1010 1010 100010 1038101058MLH101010109109101010910781010810SeqH101010108101010101081014144436MB16 18354757621669Table 4: Number times ten algorithm found instantiation yieldedhighest score. I, H, refer initialization only, hill climbing, taboosearch respectively.example, Barley group, MPE row, column labeled > .53, indicating 3 10 cases solution found .5 .9 timesbest solution found query.Qualitatively, results similar obtained random networks.search methods outperformed static initialization methods. Notedifferent networks, different initializations perform better. Notice also, searchmethods significantly outperformed minibucket approximations every network.119fiPark & DarwicheBarley network resultsBest > .9 > .5 > .01323290018011322010000901073001000010000121333125203Munin2 network resultsMethod Best > .9 > .5 > .01MPE6040Hill10000Taboo10000ML10000Hill10000Taboo10000Seq10000Hill10000Taboo10000MB 144012MB 165012MB 187001Pigs network resultsMethod Best > .9 > .5 > .01MPE0000Hill0000Taboo0023ML5131Hill9010Taboo9010Seq8020Hill8020Taboo8020MB 143034MB 161143MB 186022MethodMPEHillTabooMLHillTabooSeqHillTabooMB 14MB 16MB 18.01000300000310.01000000000322.0110105000000010Mildew network resultsBest > .9 > .5 > .01613010000100008110100001000091001000010000410140017010Munin3 network resultsMethod Best > .9 > .5 > .01MPE9001Hill10000Taboo10000ML10000Hill10000Taboo10000Seq10000Hill10000Taboo10000MB 144020MB 166010MB 182010Water network resultsMethod Best > .9 > .5 > .01MPE9010Hill10000Taboo10000ML8110Hill10000Taboo10000Seq10000Hill10000Taboo10000MB 146120MB 166121MB 189001MethodMPEHillTabooMLHillTabooSeqHillTabooMB 14MB 16MB 18.01000000000452.01000000000437.01000000000100Table 5: Detailed performance measures real world networks. column containsnumber times 10 algorithm able achieve givenperformance relative best solution found.120fiComplexity Results Approximation Strategies MAP Explanations4. Approximating MAP Inference Hardtechniques developed thus far depend ability perform exact inference.many networks, even inference intractable. cases, approximate inferencesubstituted order produce MAP approximations.investigate using belief propagation approximate inference scheme, localsearch optimization scheme. Iterative belief propagation useful approximateinference algorithm approximating MAP number reasons proveneffective efficient approximation method variety domains.ability approximate MPE, posterior marginals, probability evidence, allowinginitialization schemes used exact inference. Additionally,show section 4.2, single inference call, scores neighbors search spacecomputed locally, allowing us obtain linear speed obtainedusing similar approach exact inference case. Thus belief propagation allowstechniques approximating MAP inference tractable networks appliedapproximately inference tractable.4.1 Belief Propagation ReviewBelief propagation introduced exact inference method polytrees (Pearl, 1988).message passing algorithm node network sends messageneighbors. messages, along CPTs evidence used computeposterior marginals variables. networks loops, belief propagationlonger guaranteed exact, successive iterations generally produce different results,belief propagation typically run message values converge.shown provide good approximations variety networks (McEliece, Rodemich,& Cheng, 1995; Murphy, Weiss, & Jordan, 1999), recently received theoreticalexplanation (Yedidia, Freeman, & Weiss, 2000).Belief propagation works follows. node X, evidence indicator Xevidence entered. evidence sets X = x, X (x) = 1, 0 otherwise.evidence set X, X (x) = 1 x. evidence entered, node Xsends message neighbors. message node X parents U sendschild computedMXY =XX Pr(X|U)MZXZ6=YUZ ranges neighbors X normalizing constant. 10 Similarly,message X sends parent UMXU =XX Pr(X|U)MZX .Z6=UXU{U }10. use potential notation common join trees standard descriptions belief propagationbelieve many indices required standard presentations mask simplicityalgorithm.121fiPark & Darwiche10.90.80.70.60.50.40.30.20.1000.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91Figure 7: scatter plot exact versus approximate retracted values 30 variablesBarley network. x-axis true probability, y-axisapproximate probability.Message passing continues message values converge. posterior XapproximatedXPr0 (X|e) =X Pr(X|U)MZX .ZUmessages initialized 1. two main schemes orderingmessages. first scheme, messages computed simultaneously, basedprevious set messages. scheme, messages updated incrementally,two phases, consistent ordering variables. first phase, reverseorder, variable sends message neighbors precede order.second phase, order, variable sends message neighbors comeorder. implemented second scheme since empirically seems converge fasterfirst scheme (Murphy et al., 1999).4.2 Approximating Neighbors ScoresBelief propagation allows us approximate scores neighbors local search spaceefficiently, similar done case exact inference. key shallshow next able compute quantity Pr(x|e X) variable X efficiently,use quantity rank neighbors according desired score.Specifically, polytrees, incoming messages independent valuelocal CPT evidence entered. Hence, leaving evidence product yieldsPr(X|e X) =XPr(X|U)UMZX .ZTherefore, compute quantity variable single belief propagation. networks polytrees, incoming messages necessarilyindependent evidence local CPT, done BP methods,ignore hope nearly independent. Empirically, approximation seems122fiComplexity Results Approximation Strategies MAP Explanationsquite accurate. Figure 7 shows representative example, comparing correspondenceapproximate exact retracted probabilities 30 variables Barleynetwork. x axis corresponds true retracted probability, axisapproximation produced using belief propagation.Still, Pr(x|e X) quite want score neighbors local search space.quantity used compute ratio neighboring score currentscore allows comparisons. Specifically, simple algebra shows that:Pr(x, X, e)Pr(x|s X, e)=Pr(s, e)Pr(xs |s X, e)xs value X takes current instantiation s. Thus, findneighbor best score single belief propagation.4.3 Experimental Resultsfirst experiment, consider improvement possible typically done(MPE ML) using starting point hill climbing there. firstexperiment, generated 100 synthetic networks 100 variables using firstmethod described Appendix B bias parameter 0.25 width parameter 13.generated networks small enough could often compute exact MAPvalue, large enough make problem challenging. chose MAP variablesroots (typically 20 25 variables), evidence values chosen randomly10 leaves. computed true MAP ones memory constraints(512 MB RAM) allowed. computed true probability instantiations producedtwo standard methods. initialization methods also computed trueprobability instantiations returned pure hill climbing 11 (i.e. greedy stepstaken), stochastic hill climbing 100 steps, random moves takenprobability pf = .3. 100 networks, able compute exact MAP59 them. Table 6 shows number exactly solved method, well worstinstantiation produced, measured ratio probabilities found instantiationtrue MAP instantiation. hill climbing methods improved significantlyinitializations general, although 2 networks, hill climbing versionsslightly worse initial value (the worst ratio .835), slightmismatch true vs. approximate probabilities. all, stochastic hill climbingroutines outperformed methods.second experiment, generated 25 random MAP problems Barley network, 25 randomly chosen MAP variables, 10 randomly chosen evidenceassignments. use parameters previous experiment. problemshard compute exact MAP, report relative improvementsinitialization methods. Table 7 summarizes results. Again, stochastichill climbing methods able significantly improve quality instantiationscreated.11. compare pure stochastic hill climbing evaluate gained stochastic methods.initial hill climb usually requires evaluations, stochastic methods make little difference,efficiency considerations would dictate pure hill climbing used.123fiPark & DarwicheMPEMPE-HillMPE-SHillMLML-HillML-SHill# solved exactly94143313842worst.015.06.21.34.46.72Table 6: Solution quality random networks. Shows number solved exactly59 could compute true MAP value. Worst ratioprobabilities found instantiation true MAP instantiation. hillclimbing method improved significantly initializations.MPE-HillMPE-SHillML-HillML-SHillmin1.01.01.0x1047.7x103median8.48.43.6x1073.6x107mean1.3x10111.3x10113.4x10153.4x1015max3.1x10123.1x10128.4x10168.4x1016Table 7: statistics improvement initialization methodsearch method data set generated Barley network. Improvementmeasured ratio found probability probability initializationinstantiation.third experiment, performed type experiment Pigs network.None search methods able improve ML initialization. concludedproblem easy. Pigs 400 variables, seemed evidence didntforce enough dependence among variables. ran another experiment Pigs,time using 200 MAP variables 20 evidence values make difficult. Table 8summarizes results. Again, stochastic methods able improve significantlyinitialization methods.MPE-HillMPE-SHillML-HillML-SHillmin1.01.013.013.0median1.7x1052.5x1052.0x1031.2x104mean1.5x1074.5x10113.3x1058.2x105max3.3x1081.1x10134.5x1068.2x106Table 8: statistics improvement initialization method alonesearch method data set generated Pigs network. Improvementmeasured ratio found probability initialization probability.124fiComplexity Results Approximation Strategies MAP ExplanationsBarleyMildewMunin2Munin3PigsWater255809MPEH671 1009010100255809MLH7000007109110791010810SeqH990 100 1001080014144436MB16 18354757621669Table 9: Number times ten algorithm found instantiation yieldedhighest score. I, H entries stand initial, hill climbing, taboorespectively. MB stands minibuckets, 14, 16, 18 width bounds.also ran algorithms queries real world networksused Section 3.4 able compare performance methods. Table 9shows performed compares performance minibucket algorithms.Table 10 gives detailed exposition performance. coupleinteresting items data set. One surprising performance simple sequentialinitialization. all, performed best approximate algorithms. Anotherinteresting thing note hill climbing often negatively impacted performance.suggests marginal computations often accurate probability evidencecomputations. problem especially acute networks significant determinism.belief propagation believes configuration significant probability, may actually0 probability one constraints violated. experiments suggestpossible improve standard approaches used inference intractable(approximating MPE, ML using minibucket scheme) using belief propagationestimate joint, successively moving states higher approximate scores.5. ConclusionMAP computationally hard problem general amenable exactsolution even restricted classes (ex. polytrees). Even approximation difficult.Still, produce approximations much better currently usedpractitioners (MPE, ML) using approximate optimization inference methods.showed one method based belief propagation stochastic hill climbing produced significant improvements methods, extending realm MAPapproximated networks work well belief propagation.Acknowledgementwork partially supported MURI grant N00014-00-1-0617125fiPark & DarwicheBarley network resultsBest > .9 > .5 > .01231361127012231371117012711190109010121333125203Munin2 network resultsMethod Best > .9 > .5 > .01MPE5031Hill0000Taboo9000ML5031Hill0000Taboo9000Seq10000Hill0000Taboo10000MB 144012MB 165012MB 187001Pigs network resultsMethod Best > .9 > .5 > .01MPE0000Hill0000Taboo1004ML0000Hill0000Taboo1004Seq8020Hill0000Taboo8020MB 143034MB 161143MB 186022MethodMPEHillTabooMLHillTabooSEQHillTabooMB 14MB 16MB 18.01100100000310.01110111010100322.0110105101050100010Mildew network resultsBest > .9 > .5 > .01523010001000052300000100009100000010000410140017010Munin3 network resultsMethod Best > .9 > .5 > .01MPE8001Hill0000Taboo1000ML8001Hill0000Taboo1000Seq10000Hill0000Taboo1000MB 144020MB 166010MB 182010Water network resultsMethod Best > .9 > .5 > .01MPE9010Hill0000Taboo0000ML9010Hill0000Taboo0000Seq10000Hill0000Taboo0000MB 146120MB 166121MB 189001MethodMPEHillTabooMLHillTabooSeqHillTabooMB 14MB 16MB 18.0109001000100452.01110911090109437.01010100101001010100Table 10: Detailed performance measures real world networks using Belief propagation approximation methods. column contains number times10 algorithm able achieve given performance relativebest solution found.126fiComplexity Results Approximation Strategies MAP ExplanationsW1W11X1W1rWrW21X2W2rX3Figure 8: network produced using construction proof Theorem 2formula (x1 x2 ) x3 .Appendix A. Proofs TheoremsProof Theorem 2want show MAP remains NPPP -complete even restricted networksdepth 2, evidence, binary variables, parameters arbitrarily close1/2. Membership NPPP established Theorem 1. show hardness providingreduction E-MAJSAT.flow proof follows. First, construct depth 2 Bayesian networkE-MAJSAT problem. Then, show asserting evidence,overcome constraint parameters lie within [1/2 , 1/2 + ], useMAP obtain E-MAJSAT solution. Finally, show including evidencevariables MAP variables instead, evidence needed.network constructed follows. logical variable x induces network variableXi uniform prior. operand yi induces network variable Yi uniform prior.Notice connected, unlike reduction Theorem 1, CPT entriesenforce operator variable take value consistent operandsrespect logic formula. example, network assign positiveprobability node true, operand variables false.say variable Yi consistent variables Pi associated operands,logical function operator yi yields value Yi input pi . Consistency, insteadenforced rigidly, weighted introducing r weight variables W i1 ...Wir (the actualvalue r discussed subsequently) associated . parents Wijoperator variable Yi variables corresponding operands. CPT W ijdefinedPr(Wij = |Yi , Pi ) =(1212+ Yi consistent PiotherwisePi variables associated operands . Finally, children Ym(which corresponds top level operator) add r additional binary variables W 1 ...Wr ,127fiPark & DarwichePr(Wj = |Ym ) =(1212+ Ym =otherwisepurpose weighting states formula satisfied. See Figure 8example network construction.Now, consider probability complete instantiation variables,weight variables (which includes consistency weighting variables W ij ,satisfiability weighting variables W ) set true, denote W = T.Pr(x, y, W = T) =m+n121+2kr (mk)r121+2sr (1s)r12x instantiation X1 ...Xn , instantiation Y1 ...Ym , k numberoperator variables consistent operands variables s=1 = ,0 otherwise. consistent satisfying assignment xy,Pr(x, y, W = T) =m+n121+2(m+1)rinconsistent, unsatisfying assignment xy,Pr(x, y, W = T)m+n+r121( + )mr .2want choose r probability single consistent satisfying instancegreater twice sum probabilities inconsistent unsatisfying instances.number inconsistent unsatisfying instances bounded 2 n+m , want rm+n+r(m+1)rm+n111n+m+1 1( + )mr .>2+2222Solving r yieldsr>(m + n + 1)1 + log 212+linear size formula, size reduction remains polynomiallybounded.(m+1)rm+n1Let C = Pr(x, y, W = T) = 21+xy consistent satisfying2instance. Then, particular instantiation q X 1 ...Xk ,Pr(q, W = T) =XPr(q, xk+1 , ...xn , y1 , ...ym , W = T)xk+1 ,...,xn ,y1 ,...,ym= #q C +XPr(xy, W = T)xy#q number complete variable instantiations compatible q satisfies xy ranges inconsistent unsatisfying assignments compatiblex1 ...xk . Since instantiation x one compatible instantiation, # q128fiComplexity Results Approximation Strategies MAP Explanations1S02S0qS01X11X2S11S2X12X2S21S211Xn...1Sn222Xn...2SnqX1qX2Sq1S2qB1qB2...Xn...qSnBqFigure 9: network used reduction Theorem 8.also corresponds number satisfiable instantiations consistent q.choice r ensures sum unsatisfying inconsistent instantiations lessC/2, always greater 0 assuming least one operator (sinceinstantiation operator operands consistent). Thus#q C < Pr(q, W = T) < (#q + 1/2)C. 2nk possible instantiations Xk+1 ...Xn ,half less satisfied Pr(q, W = ) < (2 nk1 + 1/2)C, halfsatisfied Pr(q, W = ) > (2nk1 + 1)C. Thus D-MAP query, using MAPvariables X1 ...Xk , evidence W = T, threshold (2nk1 + 1)C trueE-MAJSAT query also true.Now, notice every table contains weight variable, value configuration takes true greater equal value takes false. ThusPr(q, W = T) Pr(q, W = w), q w. follows MAP(X 1 ...Xk , W =T)=MAP(X1 ...Xk W, ). Therefore, D-MAP query, using MAP variables X 1 ...Xk ,Wevidence, threshold (2nk1 + 1)C true E-MAJSAT queryalso true. 2Proof Theorem 8part proof theorem, use following lemma.Lemma 9 x 1, 4x +12>11 .ln(1+ 4x)11Proof: First, show f (x) = ln(1 + 4x) 4x+1 monotonically decreasing x 1.2dfdx=14+4x2 + x (4x + 21 )2129fiPark & Darwiche==16x2 + 4x (4x + 21 )2(4x + 21 )2 (4x2 + x)11 24(4x + 2 ) (4x2 + x)always negative x 1, hence f (x) monotonically decreasing.Now, since f (x) monotonically decreasing, lim f (x) = 0, f (x) must strictlypositive. Thus, x 1, ln(1 +14x )14x+ 12>ximplies 4x +12>11 .ln(1+ 4x)2basic idea proof show repeating construction Theorem 7polynomial number times, approximate MAP polytrees within relativeerror 2size [0, 1), size network parameterized numberconditional probability parameters, solve SAT polynomial time.Given SAT problem instance n variables, clauses, create Bayesiannetwork replicating construction Theorem 7 q times, connectingform polytree. Specifically, copy construction, add variable B (weuse superscripts denote variables associated particular copy construction),parents Sni , > 1, parent B i1 (see Figure 9). conditional probability1 q) > 2size .B uniform parent instantiations. choose q satisfy (1 + 4mshow q chosen network size remains polynomial sizelogical formula. resulting network q(2n + 2) variables, conditionalprobability table 2(m + 1) 2 parameters, total size reductionbounded q(m + 1)2 (4n + 4). Replacing size size bound places constraint1+14mq> 2(q(m+1)2 (4n+4))q. Since 0 < 1, solving q yields21 q> 2(q(m+1) (4n+4))4m1q ln 1 +> q (m + 1)2 (4n + 4) ln 24m(m + 1)2 (4n + 4) ln 2q 1 >1ln 1 + 4m1+q >(m + 1)2 (4n + 4) ln 2ln 1 +14m11Now, Lemma 9, 4m + 1/2 > 1/ ln(1 + 1/4m), substitution yields stronger bound,q >14m +(m + 1)2 (4n + 4) ln 2411polynomially bounded. Thus network constructed time polynomialsize formula.130fiComplexity Results Approximation Strategies MAP ExplanationsSniThen, particular instantiation x X variables X 11 ...Xnq , evidence asserting= 0 i, probabilityPr(x, s) =Pr(xi , Sni = 0)=# clauses satisfied xim2nsubnetwork independent.Thus solution MAP X 11 ...Xnqqkk maximum number clausesevidence Sn1 = .... = Snq = 0 m2nsimultaneously satisfied original SAT problem. problem satisfiablek = m, approximate solution 0 obeys>size204m4m + 1qm2nq>14!q12nq=41m2n!qhand, isnt satisfiable, k 1,2size <04m + 14mqm1m2nq=(4m + 1)(m 1)4mq1m2nq<43m2n!qupper bound 0 SAT problem unsatisfiable bounded lowerbound 0 satisfiable. network construction bound testsaccomplished polynomial time, MAP problem approximated withinfactor 2size polynomial time SAT decided polynomial time.Appendix B. Generating Random Networksgenerated several types networks perform experiments. used two methodsgenerating structure, single parametric method generating quantification.B.1 Generating Network Structurefirst method parameterized number variables N connectivity c.method tends produce structures widths close c. Darwiche (2001)provides algorithmic description.second method parameterized number variables N , probabilityp edge present. generate ordered list N variables, add edgevariables X probability p. edges added directed towardvariable appears later order.B.2 Quantifying Dependenciesquantification method parameterized bias parameter b. values CPTsroots chosen uniformly. values rest nodes basedbias, one values v chosen uniformly [0, b), 1 v.131fiPark & Darwicheexample, b = .1, non root variable given parents one value [0, .1),(.9, 1]. Special cases b = 0, b = .5 produce deterministic, uniformlyrandom quantifications respectively.ReferencesDagum, P., & Luby, M. (1997). optimal approximation algorithm Bayesian inference.Artificial Intelligence, 93, 127.Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 126 (1-2), 541.Darwiche, A. (2003). differential approach inference Bayesian networks. JournalACM, 50 (3), 280305.de Campos, L., Gamez, J., & Moral, S. (1999). Partial abductive inference Bayesianbelief networks using genetic algorithm. Pattern Recognition Letters, 20(11-13),12111217.Dechter, R., & Rish, I. (1998). Mini-buckets: general scheme approximate inference.Tech. rep. R62a, Information Computer Science Department, UC Irvine.Dechter, R. (1996). Bucket elimination: unifying framework probabilistic inference.Proceedings 12th Conference Uncertainty Artificial Intelligence (UAI),pp. 211219.Huang, C., & Darwiche, A. (1996). Inference belief networks: procedural guide. International Journal Approximate Reasoning, 15 (3), 225263.Jensen, F. V., Lauritzen, S., & Olesen, K. (1990). Bayesian updating recursive graphicalmodels local computation. Computational Statistics Quarterly, 4, 269282.Kask, K., & Dechter, R. (1999). Stochastic local search Bayesian networks. Seventh International Workshop Artificial Intelligence, Fort Lauderdale, FL. MorganKaufmaann.Kjaerulff, U. (1990). Triangulation graphsalgorithms giving small total state space.Tech. rep. R-90-09, Department Mathematics Computer Science, UniversityAalborg, Denmark.Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computations probabilitiesgraphical structures application expert systems. Journal Royal Statistics Society, Series B, 50 (2), 157224.Litmman, M., Majercik, S. M., & Pitassi, T. (2001). Stochastic boolean satisfiability. Journal Automated Reasoning, 27 (3), 251296.Littman, M. (1999). Initial experiments stochastic satisfiability. Sixteenth NationalConference Artificial Intelligence, pp. 667672.Littman, M., Goldsmith, J., & Mundhenk, M. (1998). computational complexityprobabilistic planning.. Journal Artificial Intelligence Research, 9, 136.McEliece, R. J., Rodemich, E., & Cheng, J. F. (1995). turbo decision algorithm.33rd Allerton Conference Communications, Control Computing, pp. 366379.132fiComplexity Results Approximation Strategies MAP ExplanationsMengshoel, O. J., Roth, D., & Wilkins, D. C. (2000). Stochastic greedy search: Efficientlycomputing probable explanation Bayesian networks. Tech. rep. UIUCDSR-2000-2150, U Illinois Urbana-Champaign.Murphy, K. P., Weiss, Y., & Jordan, M. I. (1999). Loopy belief propagation approximateinference: emperical study. Proceedings Uncertainty AI.Papadimitriou, C., & Tsitsiklis, J. (1987). complexity Markov decision processes.Mathematics Operations Research, 12(3), 441450.Park, J., & Darwiche, A. (2003). differential semantics jointree algorithms. NeuralInformation Processing Systems (NIPS) 15.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann Publishers, Inc., San Mateo, California.Roth, D. (1996). hardness approximate reasoning. Artificial Intelligence, 82 (1-2),273302.Shenoy, P. P., & Shafer, G. (1986). Propagating belief functions local computations.IEEE Expert, 1 (3), 4352.Shimony, S. E. (1994). Finding MAPs belief networks NPhard. Artificial Intelligence,68 (2), 399410.Toda, S. (1991). PP hard polynomial-time hierarchy. SIAM Journal Computing, 20, 865877.Yedidia, J., Freeman, W., & Weiss, Y. (2000). Generalized belief propagation. NIPS,Vol. 13.133fifffi! #"$ %'&)(!*,+-(../02135476819!*FHGJILKNM:;<=> ?).@5A.1!BDC< %&?E.(5A./OQPSRTK;KUVJWXOZY\[]R_^a`bGdcI)ceU5OfVgILVJPih$V)j2R_klceU5OfVm[n^eOfoSKRT`qprJstvu#w'xzy|{#w}ffua7a7DTaD_'DDa5SaD'a7Ta7575~v$EZ2vrJt$s#t$st)ex]wu!aTJDf,5ff8# e~vZ;T-f2$$) t2uZ2va7a7DTaD_'DDa5SaD'a7Ta7575)e>7 7DT#fEdff7L>-D-D; -Jf7!5- dD!Q\77;;75#7! ED-D;;vDffQ !E_7-DE-5)8,L!5- D!# -;;ff8>5DZ;ld>#E,ff;>lD-D;;D;dD!ldDv7f7;Dl_l5l)DEff;>;7_Dff>!2!DeD-D7;7! ;)77DD;;2>-!De;7_LDef7D27-D;;TTffd;Dd5##D-D;;vld;DZ!ffl> 7| LD-D;;T7-)D!#f; 87D5ff7; Z7;5;DDS;-D 7SD)D5f;;8#N77;D ;7fD7\;; ;Z;7L7 7DZDD)77Qfff>Dd75,>;$#;7 7DDS5 - 75,;d7f 7f;57ff8_5D ;7;T5,;;fffi"!$#%&('$)$"!$*+*,#%-).#%/!10#2)04325674898;: <=73- :8"04-32$)$5<>$ 32?@#%-).!1AB#%C-:D?E!E"#2:9F5$<ff8;*HG%# 4!E $"#%!$3J' 3%:*,K3%#2L$:H: 7-M8;:+!$3%32@89!E#2ffNff "!$4 'O$!E#2ffNP!$4CQ8"0-:C473%#%-)-RHST8; "!$#%&'$)$M!$*U8;#%K$<V!W:9.$<C:89#%#2X?E!E"#%! 3%:9Nff:!$8"0YAB#20Q!$Z!$[89#%!E :CYC-*,!$#%X$<?!$3\7-:9N!$C]!1 :9$<^8; "!$#%&C-:9_4#%).!$3%32`A:C,?E!$3%7-:/<a$V7 :9($<O0-: :b?E!E"#%! 32:RJcd0:d:9e+89#2:8;5$</!f8; "!$#\gK'$)$M!$*hC-:9'O:C4dX*,!$&5i<>!$8; $Mj#%893%7C4#%-),!])$&@CX8"0-#%8;:k<>$b0:kC-:89#\#2?E!E"#%! 32:NO!$4CQ89!E:9<>743*[C:3%3%#%-)]$<0:H8;4 "!$#%&jX0-: :?E!E"#%! 32:9R6cB0-:9:#%b$<a :Y8;@G#%C:9"! 32:+8M0-#%8;:l!$k mAB04!E60-:lC-:89#%"#2Z?E!E"#%! 3%:1!$Cn0-:#2k?!$3\7-:6"0-73%CY:9': :&9Rmo$:;p-!$*'432:$N[#%+!$+:;p@!$*q#%*:9! 3\#%-)j' 32:*WN0-:r?!EM#%! 32:8;73%C:9'4: :&/0-:r:;p@!$*,N[!$C+0-:?E!$3%7-:K:9'": :gK0-:#\*:9R6Sb32 :9"4!E#2?$:325$N^A:89!$Z7 :!ZsEtuEvO*@C-:3w#%XAB0#%8M0x0-:H?!E"#\! 32:!E:d0:d#%*:9N[!$C0-:B?!$3%7:(!E:B0:d:;p@!$*+9RwyQ:r!$32AV!5@04!?$:r!18"0-#%8;:r$<O0#%wL@#%C#%'O:9"*k7@G!E#2l'" 32:*,9Rwz{l!6'O:9"*k7-!E#2f'" 32:*WN[A:j0!`?$:.!$D*,!$&5+?E!$3%7-:V!$V?!EM#%! 32:9N-!$Cf:!$8M0?E!E"#%! 32:j!EL$:K!$m7#%|[7-:K?!$3\7-:$RyQ:k89!$}0:9:9<a$":.:!$#%325l:;p-8"0!$)$:10-:632:d$<w0-:1?!E"#\! 32:!$Cx0-:H?E!$3%7-:b#%}":9': :&#%-)f0-:7C-:9M325[#\-)' 3%:*WR.~}!$&5m!$"#2)*:&9Nff8"0-:C473%#%-)f!$C(../, %%!? v &5%2%a5?fi 8-fiffff7-#\-)i' 32:*,.!E:f'O:9"*k7-!E#2Z'4 32:*,Rfo-$H:;p-!$*'432:$N 'O$1 7-"!$*,:gk8"0-:C473%#%-)89!$ :6*[C:3%32:Ci!$r_4C#\-)!,'O:9"*k7-!E#2i$<0:1)!$*:r ]_4B#%& +0-:6#%*:13%$9N4$K!,'O:9 G*k7-!E#2i$<0:.#%*:1"32$d ,_4d0-:1)!$*,:r#%& -RVcd0-:.!$#%* $<J04#%D'4!E'O:9B#%B ]8;*'!E:178M0C# O:9:&d*@C-:3% $0W0-:9$:9#\89!$3%325l!$Cl:*'4#2M#%89!$3%325$Rcd0:,'4!E'O:91#%6 "78;7-":CZ!$1<>3%32 AB9R]z [:8;#2 [NJA:+)#2?$:]0-:+<>$"*,!$3%#\* !$CY$!E#27 :CX#%m0-:H: K$<0-:H'4!E'O:9RKz [:8;#2 [NffA:H': :& P!$-)$<>$"C b'" 32:*WNOAB0#\8"0x#%K7 :C+#%3\3%7 "!E :K0-:.C4# :9":gDAV!5@dA:189!$i*@C-:3^!H'O:9"*k7-!E#2i' 32:*iRJyQ:10-:i#%& @C78;:.!<>$"*,!$3g*,:!$7-:($<-8; "!$#%&P#2)0&-: [:8;#2 ff7 :C. r8;*'4!E":/0-:9$:9#\89!$3%325K0-:/C# :9:&*@C-:3%$<K'O:9"*k7-!E#2 '4 32:*, [:8;#2MR z{ [:8;#2 [NVA:}8;*,'4!E: -Sc&3%:!$*@C-:3% $<4'O:9"*k7-!E#2k' 32:*,9R^z{ [:8;#2 r!$C [NA:D8;*'432:*:&w0-:0-:9$:9#%89!$3@:"732AB#20k *:d!$5[*'4 $#%8V!$Ck:;p['O:9"#%*,:g!$3[!$!$325@#%9R yQ:D0-:H:;p@'432$:0-: :-:9_J "!$8M0#%-)0-:7-M#% #%89B$</0!?@#%-)l*k732#2'43%:.?[#%:9Ad'O#%gB$<(0-:H'O:9"*k7-!E#2 = :8;#2MRKz [:8;#%@NOA::;p@ :Cx7-b!$4!$325["#%B l#% :8;#2?$:H*,!E''4#\-)9Rrow#%4!$3%325$N4A:k:C}Ar#20}":3%!E :CxA$L [:8;#2!$Ci8;4893%7#24 [:8;#2MR!+9#"$ %-, /.%&651:$' (*)0$e$CEaD;+< >= @?BwSGFIHKJMLON*P{uKQRJfiNSL;u7N*QTLTU9u$F:N*QVHKJXWPYH>Z;v\[O]^SM_B#%r!l :9K$<(?E!E"#%!22143171832:9NO:!$8"0XAB#%0m!f_4#% :6C-*,!$#\m$<?E!$3%7-:9N[!$C]!k :9$<^8; "!$#\g9R(S 8;4 "!$#%&8;4#% $<^!63\#% ($<ff?E!E"#%! 3%: >0-:!$Cf!:3%!E#%iC-:9_4#\-)0-:1!$3%3%`A:Ci?E!$3%7-:d<>$B0: :.?E!E"#%! 32:RS #%!E"5f8; "!$#%&B#%d!]8; "!$#%&AB0-:f8;$'O:W#%H!x'4!$#2k$<B?E!E"#%! 3%:9RQS 3%7#2Y Q!X8; M!$#%g!E#\ <>!$8;#%n'4 32:* #%H!$!$#%)*:&D$<J?E!$3%7-:V ]?!EM#%! 32:V0!Er!E#% _:D!$3\3ff0-:.8; M!$#%gRHt u;v#%V!8; "!$#%&d!E#% <=!$8;#2W'" 32:* #%fAB0#%8M0f:!$8"0}C:89#%#2f?E!E"# G! 32:f!EL$:,!$ 7#\|&7-:]?!$3\7-:$N(!$C 0-:9:l#%k0-:W!$*:l&74* :9k$<B?E!$3%7-:H!$H?!EM#%! 32:9R r:8;:!$&5i 3%7-#2}!$#2)B!,'O:9"*k7-!E#2W$<0-:1?!$3%7:d +0:.?!EM#%! 32:9RVSb}#%*'O$!$&D<>:!E7-:1$<'O:9"*k7-!E#2W'" 32:*,D#%d0!EdA:689!$i M!$ 'O :j0-:."32:D$<w0-:.?E!E"#%! 3%:d!$CW0-:1?!$3%7:d#%:9'": :g#\-)H0-:17C:9"325@#%-)1'4 32:* ,)#%?$:.!,-:9AqsEtuEv-*@C-:3OAB04#%8"0W#%D!$3\ ,!'O:9"*k7-!E#2' 32:*WR /!$8M0l?!E"#\! 32:K#%+0:b$M#2)#%!$3,uEv:8;*:D!k?E!$3%7-:K#%f0-:jC7!$3ff JN-!$C?@#%8;:1?$:9""!@Rrcd0-:6'"#%*+!$3^!$Ci0:kC74!$3J B!E:6:|&74#2?!$3%:gb#%8;:1!$&5} 3\7-#2i f-:k89!$ :"!$"3%!E :Ci#%& ,!, 3%7#2l ,0-:.$0:9RyQ:H89!$m8M0-[ :1:#20:9r*@C-:3P!E #2 "!E"#\325+ :10-:1'M#%*,!$3ff*@C-:3 N4!$3%0-7-)0}#\W'"!$8;#%8;:k#2*,#2)0& :f:!$#2:9k X:;p['":10:]'" 32:* 8;4 "!$#%&H#%Z-:f$<d0-:l*@C-:3%6"!E0-:9k0!$ 0-:$0-:9Nw iA:+*,#%)0g. :4CZ }04#%-L}$<V0!Ek*@C-:3!$10:,'"#%*+!$3 RyQ:]!$3%}8;#%C:9 Ht[v vHt u;v#%]AB0#\8"0,0:b?E!E"#%! 32:/C#%?[#%C:B#%& k!H[7* :9$< >'O# 3256 ?$:9"3%!E''#%-):99N$:!$8M0$<AB04#%8"0k#%J!r'O:9"*k7-!E#26' 32:*WR cB0#%P3%:9J74JC4#%897P' 3%:*, 3\#2L$:|[7!$#2)$"7-'49RSbH$"C-:9 l|&7!$"#2)$7-' >$ !E#%H|&74!E: J89!$ :V*,[C-:3%:Ck!$/!b*k732#%'432:/'O:9"*k7-!E#2H'4 32:*8;&!$#%#%-) Q ?$:9"3%!E''#%-)'O:9"*k7-!E#2l' 32:*,9RSb;v#%(!.8;4 "!$#%&/!E#%<>!$8;#2+' 3%:* #%HAB0#%8M0:!$8"0fC-:89#%"#2H?!EM#%! 32:!EL$:.!$Q74#%|&7:1?E!$3%7-:$N 7-b0-:9:!E:- *$:H?!$3\7-:b0!$X?!EM#%! 32:9R?@#27325$Nff#2</0-:9:!E:.<>:9A:9d?!$3\7-:D0!$W?E!E"#%! 32:9N@0-:.'" 32:* #%V "#%?[#%!$3\325+7"!E#% _4! 3%:$R~m!$g5m32:9?$:3%r$</32@89!$3 8;"#% :8;5i0!`?$: :9:mC-:9_4-:Cm<>$b8; "!$#%&r!E#%<>!$8;#2x'" 32:*,#%&?$32?@#%-) #%4!E5k8;4 "!$#%& ><>$:9<a:9":8;::9: b: "75[-:r!$C :# :9":$NMRS ' 3%:* #%^# }#%0!$V-@G:*,' 5+C*,!$#%!$Cf!$g5+8;4#% :&#\ !$&#%!E#2]$< w?!E"#\! 32:89!$ :+8;4#% :&325X:;p[ :4C-:CZ }!$CC4#2#2!$3w?E!E"#%! 3%:9R]ST' 32:*U#%+u=SB# #2+#%G 8;4#% :&9R ' 32:* #% @uH# #2]#%G 8;#\ :g9RLOFIHIWM[`aWM[OPb] MN 7N*Q*H7JcWPdHZ \[O]'egfEhWPbQT] Sj_Sj_Sj_WM[4PI] MN 7N*QVHKJlWPdHZ \[O]mL+] N*QkW [ngop qs6opQRJ:t:[uF:N*QVHKJWPYH>Z \[O]rwRzu{}|~IdFIHKJMLIQTLbN}[4JfiNpd1K{:18BBx|s)mW 7NRdFIHKJMLIQTLbN}[4JfiN*_r:+vVy @1:070$,7z7PdF:dFIHKJMLIQTLbN}[4JfiNXV~{:18fififfv2$ff6ff )T$fiffv~ff#%mLbN*PYHKJ s-W u7NR>YF`HKJMLbQhLON}[OJfiN= SB _VSb# #2j#%.SB !$C _VbR^ST' 3%:* #\W@u7NRQRJK[4P`L:[`F H7JMLIQTLbN}[4JfiN9*_(zS6# #%k#% d1K{`$ G 8;"#% :&9R ' 32:* #%cPY[OLbN*PbQ*F8N}[s W@u7NR>YF`HKJ LIQTLbN}[OJN _VS# #%6#%HSBq!$C #2<d!x?!$3\7-:f!$"#2)-:C Q!}?E!E"#%! 32:f#\68;"#% :&kAr#20 57 6-:l?!$3\7-:]<>$!$!$C65 #%#%)i?!EM#%! 32:,0:n<>$H!$g5Z$0:9k?E!E"#%! 32:+0-:9":f#%k!x8;*'4!E# 3%:+?E!$3%7-:$RmS ' 32:* #%LIQRJ v [:NHKYJ uKPYF4YF`HKJMLbQhLON}[OJfiN @ SBSd #Z#2B0!$r-@G:*'4 5iC-*,!$#\d!$C}<a$b!$g5i#%4 !$g#\!E#2W$<(!' 32:*?E!E"#%! 32:$N@0-:1:732#\-)k"7 ' 32:*89!$ :.*,!$C-:.SBbRo-$f-@G #\!E5n8;4 "!$#%&9N0-:9:i0!$ :9: 3%:A$Ln C# O:9:g,3%:9?$:3%$<j32@89!$3D8;@G#% :8;5$R K-:H:;p-8;:9'#2Z#%K)$::9"!$3%# 9:CZ!EM8 G 8;#% :48;5$R+SAB#20 #%!E5i$6-@G #%4!E58; M!$#%gw#%uEv skuKSB# ,<>$w!$g5.?E!$3%7-:<a$w!r?!E"#\! 32:/#%k!b8; "!$#%&9N0-:9:k:;p-#% r8;*,'4!E# 32:6?!$3\7-:B<>$j!$3%3^0:6$0-:9K?E!E"#%! 3%:B#%m0-:68;4 "!$#%&9Rbo$j$"C-:9:CxC-EG*,!$#%4 =748"0l!$D#%& :9)$:9" MN-!k' 32:*q#% Et# }#2V0!$V-@G:*'4 5+C-*+!$#%!$Ci!$m!$#2)4*:gD$<#2r*,#%#\*67*T$B*+!p@#%*k7* ?E!$3%7-:j f!$g5l?E!E"#%! 3%:K#\i! #%4!E5+$b-@G#%4!E5 /8; M!$#%gD89!$ :j8;#\ :g3%5+:;p@ :C-:Cl 0-:j$0-:9D?E!E"#%! 32:D#%f0-:j8; "!$#\g9R(z{3%#%:1AB#20m0-:C-:9_44#2#2r#%g "[C748;:C 5 b: "75[-:H!$C :# :9:MN A:!`5}0!E.!l32@89!$38;#\ :8;5W'$'O:9F5#\B!$b -)f!$r!]32[89!$3 8;#% :8;5W'$'O:9F5>AB"#2 :D##%x!$g5}' 32:*h#%iAr0#%8"00-3\Cd0-:03%C9N#%b -)$:9K0!$>AdM#2 :d#1N#%1#\8;*'4!E"! 32:AB#20>Ad"#2 :K# -:#%0-:971-$-$1N !$C#%K:|[7#2?E!$32:&K>AdM#2 :b# $0h!$4C6R6z0!$ :9:l0- ABf0!E /SB V-SB/zSBr: "7-5@-::"# :9:$NMRlvj_[4J [OP Qr[ KPdF:dFIHKJMLbQhLON}[OJfiNB9ZuH jJ KLFIHKJMLIQTLbN}[4JfiN-*)!x')XVy Xd1:070$,7#$%$^*+ ! ,--/.0+56$879+ 5: !4=< _ 0! 0! _ /!'_ /! >! ) *x( !)23 !4!"+fi&!'+*( !153 !;@? ) *y1:070$,7)V!$8L[ "!$8"L[#%)l!$3%)$$"#20*,B!E:1$<> :X7 :CW f_4Ci3%7-#2d lSj_9R @78M0}!$32)$$"#%0*,D 5,:;p@ :Ci'!E#%!$3ff!$#%)*:&9N-:-<>$"89#%-),!,3%[89!$3ff8;"#% :8;5W!E<> :9d:!$8"0m:;p[ :4#2i!$C !$8L&G"!$8"L[#%)mAB0:Y0#\132@89!$38;4#% :8;5n-m32)$:9H03%C9R}o$k:;p-!$*'43%:$NJ0:/u {s!$32)$$"#%0* =o V*+!$#%g!$#\D!H: "#\8; :CW<a$M* $<JSB 0!Ed:7:V0!ED0-: #%!E5+8;4 "!$#%&:9 A:9:Q0-:,*b":8;:g325X#%!$g#%!E :CX?E!E"#%! 32:k!$4CQ!$g5X7#%4 !$g#\!E :Cm?E!E"#%! 32:K!E:SBbRT0!$ :9: )$:-:9"!$3\# 9:C Z-@G #%4!E5Z8; "!$#\g :# :9":$N~}: :9)7-:9NVo-:7C:9N!E G!@NMR/oJ 1*,!EL$::9?$:95 &G !E"5H8;4 "!$#%&/AB#%0D?E!E"#%! 3%:/#%!$g#%!E :C]SBbR&4o!E''43\#2: >:k'!$K$< rSB f:!$8M0Z8; M!$#%gK$.8; "!$#%&b'4 {:8;#%x#\g?$32?@#%-)]0-:897-:&!$Cn:;p@!$8;3%5Z-:f<>7-7:+?E!E"#%! 3%:$R}o m!E''43\#2: >-:f'4!$k$< KSB m:!$8M0 8; "!$#\gH#%@G?$32?@#%-)Q0-:m897-:&,!$C !Ef32:!$ ,-:W<=7-7-":W?!E"#\! 32:$R cd0-":9:i$0-:9+)$::9"!$3%# !E#24$<joY--G #%!E5n8; "!$#%&9NVo x [NVC-:9)$:-:9"!E :m Zo Q 0-:i#%)32:W-@G #%4!E58; M!$#%g.C:8;"# #\-)]!l'O:9"*k7-!E#2ffN^i!E:,-$.8;#\C-:9:CQ0-:9:$RHow#%!$3%3%5$Nff0-: ,uuum!$3%)$$"#20*~mSB ,*,!$#%&!$#%,SB C7-"#\-)x:!E"8"0ffNDAr0#%3% ,~ KSB *,!$#%&!$#%KSBbRU4HKPBA KP 2FOM[uFDC6QTJE@1:07070$3p*)HGIGKJ 1KPYF4YF`HKJ LIQTLbN}[OJ F=MNwl"F?Vy65IL] KQTJN KQTJfiQTJ$1OLP @ =0Q?Rcd0-: OG |&7:9:P'4 32:* #\P-:$<-0-:V#%*'32: ff:;p-!$*'432: $<4!d'O:9"*k7-!E#26' 32:*WRPS 8;*+*!$C !E7-"!$3*@C-:3/04!$6!xC-:89#%#%Y?E!E"#%! 3%:,<a$k:!$8"0 "`A.NAB#%0Y#26?!$3\7-: :#%-)}0-:f8;3\7*,#% AB0#%8M0 0:x|[7-:9: 0!El`AU3%#%:9R cB0-:mC74!$3r*@C-:3b0!$l! C-:89#%#2 ?E!E"#%! 32:m<a$l:!$8M08;3%7*+ffN&Ar#20l#2?!$3%7: :#%-)k0-:K lAB0#%8M0+0-:j|[7-:9:W#%]0!Ed8;3%7*+]3%#%:9R B A:9?$:9N0-:OG |[7-:9:K' 32:*h#\b$K8;* #%4!E $"#%!$3%325m8"0!$3%3%:-)#%-)l!$j#2 :8;*:K:!$#2:9.!$)$ AB9R6o$:;p-!$*'432:$N^~}$"#%K0!$K!E)7:CX0!EK0-:9:!E:-l32[89!$3w*,!p-#%*,!f l0- AB#%-)l|[7-:9:K!E"!$C* g m0-: !E"CY!$CQ'O:9<>$"*,#%-)W*,#\@G 8; 4#%8;j04#%3%3 G 893%#\* #\-)+AB#\3%3w!$3%* 1"7-:325m_4C!W 3%7#2ffRHyQ:,<>[897K0:9:9<a$":,Z!WC# O:9:&K' :9M*67-!E#%X' 32:* 04!E.#%j#%*'32:k3\#2L$:0-:ge/d1:0707$TSrVUfi 8-fiffffoOG |[7-:9:r'B)32:* 7-b!E''O:!E"r :k*$:H8;* #\!E $"#%!$3%3%5i8"0!$3%3%:-)#%-)-R 5m7#%-)f!]"#%*'432::;p-!$*'432:$N0:68"04!E"!$8; :9"#% #\89d$<'O:9"*k7-!E#2W'" 32:*,D!E:k0-$'O:9<>743%325+*$":1!E''4!E:&d0!$m#%*$:68;*'432:;pl' 3%:*,AB0-:9:j0-:1$0-:9B8; M!$#%gr0!?$:6!,3%!E)$:9B#\*'4!$8;9R!$-)$<a$MC k' 32:* #%#%#IK:g hyZ!$3%0ffNMRZS 8;*':0-:4#2?$:0#% $5+$<w0-:j'" 32:* #%D)#%?$: 5W~m#%3%32:9MRVcd0-:j'" 32:* #%dC-:9_-:CW!$d<>3%32 ABSj_q?S1:07070$VK373$$=<L NH NR>PY[I[cN*QT]s[OLc[u$F4j[4PY[lQTLsHKJ [fiff sKQ7Q*NSL:[9tfi[4JFI[sQRJF9v ts>[bLsNRj[ls7Q $QRNRs7Q $QRN ZI[4NAn[`[4JNRj[-PILbN NAqHKLBuKJs!HKJ ,[ s7Q $QRN ZI[4NAn[`[4J NRj[,v uKLbN NAqHKLM[OPu[+uKPu[t~LbNSNAqHmsKQ7Q*NRL+ZI[4NAn[`[4JNRj[ -PILbNN AqH ff !L ruKJ s!NAqH}sKQ7Q*NRL+ZI[4NAn[I[OJNRM[v2u6LbNNAqH ff L!"# uKJssLOHHKJ%$-QRJsluEv>vjWjH6LILIQ;Z v\[ML F4cL:[ [4JFI[OL &cd0-:1' 32:* 89!$}:!$#\325 :1)$:-:9"!$3%#9:C} ]0:lRog({ '!V'" 32:* Ar0-:9:.A:60!`?$:k!] :|[7-:8;:1$<32:-)$0po)*'QN8;g!$#%4#%-)0-:.#%& :9)$:9"j1 +' :9'O:!E :CW:;p-!$8;325Q#%*:9R(cB0-:.! `?$:1'4 32:*#%V0[7D0-:cV[Nk0$V ' 32:*WRwzFr0!$D:;p-!$8;325p& 3%7-#%=<,.-,./0,.12435624-61727/34865179-4863797/757869,./,:2,"-62;8636245/48<17-73948656149/737-796175,./,.30,"-6271756243/217-486573961;86/797-7578699785-97/78617937578-6124/327561727-,"3,./0,9785/97378-96145783/62717-275321,"/,.-0,56149-37/917578937-6148/527378627-,.2,./0,=:60!Er0-:63\!$ d0-:9:k 3%7-#%d!E:60-:1:9?$:9M :1$<0-:1_" d0:9:$Rdcd0#%B 5[*+*:9 5W89!$ :$:3%#%*+#%!E :C 5]!$C4C#%-)k8; "!$#%& @<>$V#\ !$8;:$N-#%]0-: [N ' 3%:* 0-:K :8;C k89!$$ :'43%!$8;:C]#%0-:b :8;Cf0!$32< $<ff0-:r:|&7-:48;:$N[!$C]#2<ff#2/#%/#%,0-:r8;:g "!$3'O#2#2+#%0-:b :|&7:8;:$N0-: :8;C f*k7 :6'3%!$8;:CQ#%m0-:H_" j0!$32<$<0-: :|[7-:8;:$R -78"0Q8; "!$#%&j0!?$: :9:!$CC-:C}#%lAB0!ED<>3%32 AB9Rcd0:6_M b*,[C-:3 $< !$-)$<>$"C b' 3%:* A:HAB#%3%3 8;#%C-:9`N Ar0#%8"0mA:H"0!$3%3J!E #2 "!EM#%325W89!$3%30-:d'4"#%*,!$3@*@C-:3 Ng04!$(!j?E!E"#%! 32:D<>$(:!$8M0+@89897-:48;:D$< 0:BC#2)#2RJcd0:D?!$3%7:D$< 04#%w?!EM#%! 32:#%+0-:m'O#%#2 #% 0-:x:|&7-:48;:}$<.0#\,@89897-:8;:$R o-$l:;p@!$*,'432:$ND0-: [N ,'4 32:* 0!$X?E!E"#%! 32:9N KAB#20R cB0-:W?!$3\7-:W$< j#%0-:m32[89!E#% #% 0-:m :|&7:8;:i$<K0-:jC#2?;0 @89897-:8;:]$<d0-:lC#2)#2 b*,[C QRxcB0&7N * 0!$H!$H#26?!$3%7:+0-:l32@89!E#2Y$<0-: 9 j@89897-":8;:$<0:,C#2)#2 EN ( 0!$1!$.#2K?E!$3%7-:H0-:,32@89!E#2X$<0-: 9 j@89897-:8;:$<0-:fC#2)#2 [NDR9R`R9N 9 0!$k!$k#21?E!$3%7-:+0-:f3%[89!E#2Y$<d0-: 9 6@89897-":8;:+$<d0:]C#2)#% [N * .0!$k!$6#%.?E!$3%7-:,0-:f32@89!E#2Y$<D0: ECY[89897-":8;:+$<D0:]C#2)#% EN!$CY mffRWyQ:l0!?$:W!'O:9"*k7-!E#2H8; "!$#\g0!E:47-:J0!E:!$8M0,C#2)#2J@89897-:8;:D@89897-"!E!jC# :9:&w'O#2#2#%m0-:H :|&7:8;:$Rjcd0#%r89!$ :H#%*'43%:*:g :C}:#20-:9K!$j!+)3% !$3 !$3\3 G C# O:9:&r8; M!$#%gbX!$3%30-:N$+!$'!$#2AB#%:l${G:|&7!$3\,8; "!$#\g :!$8"0 ' "# 32:f'4!$#2$<b?E!E"#%! 32:R yQ:x89!$3%30-:]<a$"*,:910-: {'"#%*,!$3!$3%3 G C# :9:& i*@C-:3(!$4CY0-:f3%!E :910: {'"#\*,!$3/-${G:|[7!$3% }*@C-:3 Row#%!$3%3%5$NPA:+0!?$:l8; M!$#%g60!E60-:+C#2)#%j[89897-":8;:6[89897-k#%Q$"C:96C- ABZ0-:f :|[7-:8;:!$Cn8; "!$#%&6Z0-:f :9'4!E"!E#2n$<D0-:fC# O:9:g1[89897-":8;:.$<d!mC#2)#2 60!EH#%.A:]0!`?$:!$C/<>$QR( 6N(cJ! 32: V)#2?$:(0-:D'"#%*+!$3g:9'4: :&!E#2$< 0-:d :|[7-:8;:N$!j 3%7-#2H j0: [N' 32:*WRwo-$893\!E"#2 5$N&A:b!$3% 1#\C#%89!E :D0-:b8;$: 'OC4#%-)jC#2)#2@89897-":8;:B7#\-)j0-:r$!E#2m<a$60-: [0n[89897:8;:]$<0:]C#2)#% Rio-$k:;p-!$*'432:$N ( #%10-: E4CZ@89897-:8;:]$<D0-:C#2)#%]!$C@R* #%V0: 9 d@89897-:48;:j$<J0:.C#2)#2?>+V k0$.n$,z'I1p1A@BzDCFE1K{`$,#GJ'H@BV k0$K@1cp1s191GXs1PJND@BWS U J@BWS0U[Y%z SzA\R'27948<2,.90,#8_]-cauN :P#b#c0 @@BQR@BTS0UVQR@BWS U <@BWS0U JX@BZY z`Nu:Pz1 L@M@D@BN(]4^.Pp0P=<Vfififfv2$ff6ff )T$fiffv~ff'Rzz 4C-:;p!$3%7-:j$<w'"#\*,!$3 ?!E"#\! 32:/|[7#2?E!$32:&VC4#2)#2V[89897:8;:+ @B}f+1 <Bcd0:r'M#%*,!$3:9': :&!E#2l$<cJ! 32:11"**0-:j :|&7:8;:32:*iR1"*,1(*2497862,.9,"8&&(,..((/V[N V'N[!H 3%7-#2]$<P0:Gc 0:KC47!$3ff*@C-:3 $< !$-)$<a$MC V' 3%:*T04!$d!H?!EM#%! 32:b<a$B:!$8"0}3%[89!E#2W#\]0:. :|&7:8;:$Rcd0-:r?!$3\7-:r$<P0#%(?E!E"#%! 32:B:9'4: :&0:bC#%)#2(@89897-:8;:K!EV0#%32[89!E#%ffR(o-$V:;p@!$*,'432:$N@0-:[N .' 32:* 04!$ f?E!E"#%! 3%:9N +AB#20RWcd0-:?E!$3%7-: r$< ,#\.!$Y#%& :9)$:96#\Q0-:#%& :9?E!$3N:9'::g#%)B0-:V<=!$8;J04!EJ0: ^C#%?;06@89897-":8;:V$<0-:VC#2)#2 ^*[C@89897-"D!ED32@89!E#2 -R/cB0&7N 1k:9'::gV0-:K<=!$8;D0!EV0-: 9 V@89897-:8;:K$<J0-:jC#2)#2@89897-"b!Er0-: "Cx32[89!E#%ffN !$4C]:9': :&r0-:6<=!$8;b0!Er0: ECm[89897:8;:6$<(0-:/C#2)#% .[89897-Md!Ed0-: $0}32[89!E#%ffN!$Ci +ffRz{0-:dC74!$3*@C-:3 N$A:B!E)!$#%+04!?$:B!j'O:9"*k7-!E#2+8; "!$#%&0!E(:!$8M0]3%[89!E#2+8;g!$#%4/!C# O:9:&^C#2)#2O@89897-:48;:$RJcd04#%ff89!$.!E)!$#% :(#\*'432:*:& :CK?@#%!D!D)32 !$3g!$3%3 G C# O:9:&ff8; "!$#%&m0-: .$ 5W'!$#2AB#%:K${G:|&7!$3\r8; "!$#%&B}:!$8"0x'4!$#2r$<(C7!$3ff?E!E"#%! 3%:9RVyQ:k89!$3%3P0-:<>$"*:9i0-:C47!$3.!$3%3 G C4# :9":g*@C-:3.!$C 0:Y3%!E :9i0-:C7!$3j-${G:|[7!$3%*[C:3 R cd0-::9'4!EM!E#2 8; M!$#%g!E":i-$,!$,#%*,'432:f 'O:89#2<>5Y#% 0-:WC7!$3D*,[C-:3R o-$+:;p@!$*'32:$N/<>$QNDA:x89!$ !$CC 8; "!$#\g]$<j0-:}<>$"*H#!$CH#*QR.cJ! 32: ])#2?$:K0-:HC7!$3J:9'": :g!E#%m$<0-:H :|&7:8;:NO!(;+ *703%7#2l ,0-: [N D' 3%:*WRV k0$0E1K{Yo)7' G1]z\ ']BWS]$,]] 143+|9!| C E1K{`$,#Gnz 'I/1+1z'NPzI ) 'N/lV|~fP<X] z ] S0BTS Y(z*I 'z C:;p!$3%7-:.$<wC7!$3O?E!E"#%! 3%:/|[7#2?E!$32:gDC#%)#2D[89897:8;:cJ! 32:+z ]/] 61***E<nxb74!$3ff:9'::g!E#2i$<J0:. :|&7:8;: 27948<2,.90,#8&(11&*] OY(z2497862,.9,"8,,"1(.(.(sV[N D'N-! 3%7-#2l$<w0-:3%:*WRz 1#%j'O"# 32:k }8;* #%-:'M#%*,!$3!$CZC47!$3*@C-:3% 5X3\#%-L@#%-)+0:, :91$<V?!E"#\! 32:9N7#\-)[u;v>v{uZ*,!$#\g!$#% 8;"#% :8;5 :9FA:9: 0-:l AQ?[#2:9AB' #\g9Rncd0#%!E''"!$8"0 #%}89!$3%32:C {:C74C!$&l*,[C-:3\3%#%-)MRhS #%*+#%3%!EW#%C-:! AV!$5 V0-:) :9}!$3 R':9?@#27"325r7)$)$: :CffN$ 'O:89#2_489!$3%3%5B<>$P'O:9"*k7-!E#2.'4 32:*,N 5 K:9:32:MR(z{!$-)$<a$MC' 32:*WN[0-:j8"0!$:3%3%#%-)k8; "!$#\gd!E":,#N!$Cl8; "!$#\gV$<J0-:j!$*:K<>$"*89!$ :r7:Cf#\ 74#%3%C#%)K!H8;* #%-:Cf'M#%*,!$3C74!$3*@C-:34$<J!$g5+' :9M*67-!E#%]'4 32:*WR~m!$g58; M!$#%gk [32L[#%k"7-''O$68M0!$-:3\3%#%-)i$<B0#%6L[#%4CYAB#%0Y:9e+89#%:gk)32 !$3V8; M!$#%gRXo$:;p-!$*'432:$N-z@32?$:9B0!$D!8; "!$#%&9N fffi 4NgAB04#%8"0W89!$ :j7 :Cl ":9'43%!$8;:.! :9d$<#%C4#2?[#\C7!$3@8; "!$#\g$<P0:r<>$"*H#"N@!$4C+0-: @#%897/_#2 :rC*,!$#%f8; "!$#%&3%# "!E5+0!$B!$ ff,'4:C#%89!E :jAB04#%8"0W89!$ :.74 :CW#%*,#\3%!E"325$Rcd0:B8;* #%-:C,*@C-:3#\(8932:!E"325k:C74C!$&w!$A:B89!$+C-:3%:9 :B0-:B8;4 "!$#%&($<O:#20-:9#%C# G?@#%C7!$3^*,[C-:3PAB#20-7r#%8;":!$#%-),0:6 :9b$</ 3%7#29Rdo-$j#%4 !$8;:$N#\ P!$)$<a$"C B'" 32:*WNF4 KJ J@[ wQRJ2F`HKJMLON*P 7QTJfiNRLN;Pd1:07070$2@B Y| ] %Y zv2@ B | ] Y%zld1:0707$ 99X#b!fi 8-fiffffA:+-:9:CQ3%5}:;p@':j0:, :9'4!E"!E#2Y8; "!$#%&.#%X :9M*,j$<V:#20-:9j0-:+'"#%*,!$3J$.0:,C7!$3?E!E"#%! 32:R~i$":67-"'"#%#\-)325$N[0-:.'O:9"*k7-!E#2i8; "!$#%&d $0W0:1'"#\*,!$3ff!$CW0-:6C7!$3?E!E"#%! 32:r!E:6!$3\ +:C47C!$&9Rdcd0-:6:;p@#% :8;:1$<(0:6C7!$3P?!EM#%! 32:d!$4C}0-:k8"0!$4-:3%3%#%),8;@G"!$#\g(3\#%-L@#%-)B0:* .0-:D'M#%*,!$3[?!EM#%! 32:w!E:B"7-e+89#2:&w .:7-":04!E(0-:D?E!$3%7-:(!$"#2)-:CH0-:K'"#%*+!$3?E!E"#%! 32:!E:j!6' :9M*67-!E#% =!$4C]0:9:9<a$":b0:b!$*,:K*k7 :r M7-:r$< 0-:KC7!$3?E!E"#%! 32: MR'bfn/?$:6#2<-8; "!$#%& !E:32$)#%89!$3%3%5b":C7C!$& >0!Ew#%9N0-:95.89!$ :/C-:32:9 :C6AB#20-7P8"0!$)#%-)0-:d :9/$< 3\7-#2 MNg0-:95k*+!5 #%3%3 :d7 :9<=73@C7-M#%-)K :!E"8"0^R ^$)#%89!$3%3%51:C74C!$&w8;4 "!$#%&!E:$<a :k89!$3%32:C #%*'3%#2:Cj8; M!$#%g @N!$C674 :9<>743#%*'43\#2:Cj8; "!$#\gw!E:<a:|[7-:&325.!$CC-:C1!B*,[C-:3 r#%8;":!$ :(0-:/!$*,7g $<-8; "!$#%&P'4$'4!E)!E#2 @*,#20ffN [ :9)#27^NyZ!$3%"0ffNMRz{H0-:D-:;p[(:8;#2ffNA:D': :&(!j*:!$7:V$< 8; "!$#%&J#%)0g-:"0!E(!$3%32 AB7w .C-:9 :9M*,#%-:AB0-:x!$x#%*'43%#%:Ci8; "!$#\gK!$CC-:Cm f!f*@C-:3 AB#%3%3^#\*' ?$:68; M!$#%gb'$'!E)!E#2ffRbz m0-:<>3%32`Ar#%-)l :8;#2ffN^A:!E''4325}0#%j*:!$7:H$<8; M!$#%gj#%)0g-:"K W0-:,C# O:9:&K*,[C-:3\r$<'O:9"*k7-!E#26' 32:*+ #%& @C78;:C6#%104#% :8;#2ffRwyQ:D!E:D! 32: K"0-`A.NE<>$w:;p@!$*'32:$N$0!Ew0-:8"04!$-:3%3%#\-)i8; "!$#\gk-$14325Q*,!EL$:]0: #%4!E5x${G:|&7!$3\18; M!$#%g6:C74C!$& 60-:95!E:.#%)0g :9B!$4Ci89!$i)#2?$:6*$:.C-*,!$#\W'"74#%-)-RN-Pc?7KK37373>Y=<]}D# R2ffff(vK7-6C-:9_4#2#%X$<d8; "!$#%&.#2)0&-:6!$7*,:.0!Ek8; "!$#%&k!E:+C-:9_4:CQ`?$:9H0-:f!$*:?E!E"#%! 32:!$4CH?!$3%7:w$N&!$(#\H0-:d89!$ :d$< '"#%*+!$3[!$4CC7!$3-*@C-:3%9N$?E!E"#%! 32:w!$C?E!$3%7-:wAB0#\8"0!E: # {:8;#%?$:325}:3\!E :CffR,z{X0#%KAV!5$NwA:,89!$n!$32AV!5@18;*'4!E":+3%#2L$:HAB#20Q3\#2L$:$R K7-.C-:9_#2#2$<r8;4 "!$#%&#2)0&-:#%H )325Y#% 7-:8;:C 5n0-:WA!`5 3%[89!$3V8;#\ :8;5Y'4$'O:9#2:H!E:8;*'4!E":C 5 b: M7-5[:i!$C :# :9:MR z 4C-:9:CffN0-:}C-:9_44#2#2 #%'!E"!$*:9 :9"# 9:C 5!W32@89!$3w8;#% :48;5}'$'O:9F5}#%48;:k0:H!$*,7gj$<'"74#%-)+'4`?@#%C-:C 5x!W :9j$<8;4 "!$#%&C-:9'O:Cw7-'O60-:d32:9?$:3@$<O32@89!$3[8;4#% :8;5 :#%-)b:-<>$"8;:CffRJzF<A:D:-<>$"8;:d!j0#%)0k3%:9?$:3@$< 32@89!$38;#\ :8;5$NA:1*,!`5f)$:9r!$B*k78"0i8;4 "!$#%&d'$'!E)!E#2WAB#20W!+32&:.8; "!$#%&B!$d!+*678M032 A:9,32:9?$:3V$<b32@89!$38;4#% :8;5 !E''43%#%:CY Z!x#2)0&8; "!$#%&9R j7-*:!$7-:l$<b8; "!$#%&#2)0&-:rA743%Ci!$3% :17 :9<=73^#%m!+[7* :9r$<w$0-:9j!E''43%#%89!E#% >:$R )-Rd:!$#%-)+! 7-r0-:#%*'!$8;D$<wC4# :9":gD32@89!$3^8;#\ :8;5f :8"0#\|&7-:d}!#%)32:K_-p[:C}*[C:3 MR5x)lvTSVy d1:070$,7v4#%C-:9l! :9l$<68; M!$#%gC-:9_4-:C `?$:9m! :9f$<6?!EM#%! 32:/Nr!$C !$$0-:9W :9$<68;4 "!$#%&C-:9_4:C ?$:9}! :9l$<.?!EM#%! 32:DNBAB0-:9":m0-:9":X#\]! # :8;#2 :9 A:9:!$#%)*:&}!$C=#%H0-:d:($<O0-:d'4!E'O:9N$0#\ # :8;#2#%:#%0-:9(0-:B#\C-:g#% 5H*,!E'^N$K0!EKC-:9_4-:C 5i0:k8M0!$-:3%3\#%-),8; M!$#%g MRbyQ:H!`5}0!Eb0-:H :9r$<8; "!$#%&#%kuv u duDu D0-:6:9AB#20W": 'O:8;d G 8;4#% :8;5 >AdM#2 :V# wN)#2?$:x!$g5C-*,!$#\<>$0-:#%/?E!E"#%! 32:Ng#2<#% G 8;4#% :&/0-:]0-:d:|[7#2?E!$32:&C-*+!$#%$<!$898;$"C4#%-)k0-: # {:8;#2]!E:K!$3% G 8;#% :&9R 5,8;#%C:9"#%-).!$3%34'O# 3%:dC-*,!$#%/<a$0-:r?!E"#\! 32:9N0#%.$"C-:9"#%)l*,:!$7-:j0:,'O$ :g#%!$3/<a$6C-*,!$#%K :'"7-:CQC47-"#%-)l :!E"8M0Y!$1?!E"#\! 32:!E:k#% !$&#%!E :C}!$4CiC-*,!$#%4D'"7:C >' "# 325 5l$0-:9K8; M!$#%gr#%W0-:1' 32:* MR$ :0!EHA:lC4#%89710-:l:|&74#2?!$3%:gkC-*,!$#%k X0!EHA:l89!$ 8;"#%C-:9H'"#%*+!$3/!$C C7!$3*@C-:3%#%nAB0#%8M0Y0:]?E!E"#%! 3%:1!$C ?E!$3%7-:k!E:WC# O:9:g 7-H!E:l#%Y:f x-:l:3%!E#%YAB#%0Y:!$8M0$0-:9 MRyQ:r!50!E!. :9($<ff8; M!$#%g#\0!$+!. :9Ad G 8;#% :48;5 >AB"#2 :1#(N#%@u u ;v HAB G 8;4#% :8;576-$>Ad"#% :b# -:#%0-:9-$(NP!$4C T#% EuEv w G8;#\ :8;5 >Ad"#2 :w# $0Q!$C(RwyQ:b89!$,:!$#%3%56)$:-:9M!$3%# 9:fififffiff#\[ 6LON 6L N*Q 7N 6LK5b55! ff-)@c =b7NN*Q 7N}[OP! ffq ! ffff ! lQTJFIHK]W KP >Z \[ 56.ff-!ff ff ! # +[ jQ [4JfiN 6p#7ff- # !ffff- !#b"!fififfv2$ff6ff )T$fiffv~fffi#0-: :kC-:9_4#%#2D f8;*'4!E: G 8;#% :8;5WAr#20 rG 8;"#% :8;5W fRVcB0#%dC-:9_#2#2$<^8; M!$#%g#2)0&-:0!$ *:K#%8;:B*$ #%89#2F5+!$C,_p[:C@G'O#%&('"$' :9"#2:(AB04#%8"0+A:bAB#%3%37 :j:;p@ :#%?$:325]07-)0-7-D04#%V'4!E'O:9RQs s; 5x { {#w}w! tv{fffiff4 ff !" $ ! ffff ! ff QR] W vwQ[bLK ff 7w'{@#\*,#%3%!Ed*$ #%89#2F5}!$Ci_p[:C@G'O#%&d'"$'_)_@_9: "#2:d0-3%C}<a$ dbN bN (zbN -SBbNSB VbN!$C KSBbRDyQ:Z!$3\ n:;p@ :C 0-::XC:9_4#2#24+ 8;*'4!E":X8;4 "!$#%&l#2)0g:lAdl :!E"8M0!$32)$$"#%0*,f3%#2L$:x~mSB !$C h0!El*,!$#%&!$#% *:Q32@89!$3r8;"#% :8;5 C47-"#%-)n :!E"8"0^R o$:;p-!$*'432:$N A:H!`5i0!E#%Hu Vv "u ruduAdb!$32)$$"#204*>Ad"#% :D# wN)#2?$:]!$g5_-p@:C?E!E"#%! 3%:d!$C,?E!$3%7-:D$"C:9"#%-).!$C]!$g5C-*+!$#%<>$/0-:B?!E"#\! 32:$< 6N?[#\#2-X*$:l-@C-:k x_CY!X 3%7#2Z$<$H'`?$:W#2H7"!E#% _4! 3%:,0!$?[#\#26 UAB#200-:H:|[7#2?E!$32:gKC-*,!$#\9Nff!$Cx0-::|[7#2?E!$32:gb?E!E"#%! 32:H!$Cx?E!$3%7-:k$"C-:9"#%)-R /|[7#2?E!$32:8;:H0-:9:#%D!E)!$#%iAB#20l: 'O:8;D 0-: # {:8;#% :9FA:9:}0-:.!$#%)*:&V ,0-:j?E!E"#%! 32:V$<!$ClfRwyQ:r!`5,0!E#%(04!$Ad(!$3%)$$"#20*>AB"#2 :(#7--$R @#%*,#%3\!E/*-$ #\89#2 5+!$4C,_-p@:C@G'O#%&/'$'O:9#%:/89!$ :r)#2?$:f<a$Vo bN-~mSB!$CX~ jSBbR ow#%4!$3%325$N4A:kAd"#2 :#2<!$Cm0-:9:k#%r!+'4!E"!$*:9 :9M# 9:CX :9r$<' 32:*,J$< "# 9: i!$C!j_-p@:CH?E!E"#%! 32:D!$CH?E!$3%7-:D$MC-:9"#%-)bAB#20HAr0#%8"0 h?[#\#2w:;p@' :g#%!$3\325<>:9A:9b-[C:d#% QAB0:i!E''43%#%:Cl0!$iAB0:i!E''43%#%:Cl fR K7-B:732D89!$ :j:;p[ :4C-:C+!$32)$$M#20*,V0!ED_CW!$3%3ff 3%7#29Rwz{W!$CC#2#2^N[0:95f89!$i!$3\ :j:;p[ :C:CW +!H: M#%8; :C893%!$d$<wC5[!$*+#%8b?!E"#\! 32:j!$CW?E!$3%7-:j$"C:9"#%-) V!$898"0[79NffV0-:ffN-?E!$ :9:9L NyZ!$3%"0ffNMR7N [ 6LbN 6LmN*QKN KL$ ff !" q#N*Q 7N}[OP5% & ff ' -! ff9fFv(9*))wR R ? =0QTg7! 5!" ffq # 5 !" ff! ffqp)?fiK373$$yQ:K-`A 0!?$:b0-:d0-:9$":9#%89!$3*+!$8"0#%:95-:9:C-:C, k8;*'4!E:r0-:BC# :9:&(AV!5@A:B89!$f*@C-:3!k'O:9"*67!E#2+'4 32:*T78M0f!$ !$-)$<>$"C ' 3%:*WRJcB0-:,uEv[${G:|&7!$3\*,[C-:3$< !k'O:9 G*k7-!E#2+04!$(-${G:|[7!$3%/8;4 "!$#%& :9 A:9:+0:D?!E"#\! 32:(#%H:!$8M0+'O:9"*k7-!E#2^RJcd0:,uEv!$3%3 G C4# :9":gw*@C-:3@0!$!$!$3%3 G C# :9:&w8; "!$#%& :9 A:9:0-:V?E!E"#%! 32:w#%k:!$8M0,'O:9"*k7-!E#2ffRz{Q!ZsEtuEvff*[C:3 NffA:]#%g :9M8"0!$-)$:?E!E"#%! 32:K<a$j?E!$3%7-:RHST8;* #%-:Cu$vu sxsEtuEvff*@C-:30!$ $00:d'"#%*+!$3[!$4C0-:BC74!$3@?!E"#\! 32:9N!$C [u;v>v{u/3%#\-L[#\-)B0-:*iNg$<O0-:<>$"*f#AB0-:9:#%d!'M#%*,!$3O?E!E"#%! 32:.!$4C 6#%d!+C7!$3ff?E!E"#%! 3%:$RS 8;* #%-:C*@C-:3J89!$Q!$3% l0!?$:,-${G:|[7!$3%K!$C $K!$3\3 G C# O:9:&b8; "!$#%&rx0-:H'M#%*,!$3P!$4C $jC7!$3?E!E"#%! 32:R cB0-:9:WAB#%3\3 N($<K8;7-" :$NV 5['4#%89!$3\325 :W$0-:9+8;4 "!$#%&, $0 :9,$<b?!E"#\! 32:AB0#\8"0fC-:9'O:C,f0-:K!E7-:b$<^0-:K'O:9"*k7-!E#2]' 3%:*WRJo$D:;p@!$*'32:$N[#\!$-)$<a$MC ' G32:* A:.!$3% ,0!`?$:.0-:. :9'!E"!E#2W8; M!$#%gR/SrD!+ :8;Cl:;p-!$*'432:$N#%l0:K!$3\3 G #%& :9?!$3ff:9"#2:' 32:* <>*P# N@0-:1?E!E"#%! 3%:d!$CW0-:kC# O:9:8;: :9FA:9:x-:#2)0 $"#%-)?E!E"#%! 3%:d!E:$0X'O:9"*67!E#29Rkz ZAB0!EK<a3\32`ABN^A:,Ci-$.8;#%C:9.C#2:8;325m0-:,8;& "# 7-#%m$<V78M0!$CC#%#2!$3O8; "!$#\gV 'M7#%-)-R r`A:9?$:9N40-:j:!$:.AB#20fAB0#\8"0fA:.89!$i:;p['":V:!$8"0m!$CC# G#2!$3P8; "!$#%&B#%l0:.'"#%*+!$3O$B0-:1C47!$3ff*@C-:3^!$CW0:.:73%#%-)H'"74#%-)k' A:9r$< 0: :8; M!$#%gB*,!5WC-:9 :9"*+#%-:j7-B8M0-#%8;:.$<J0-:.'4"#%*,!$3 N@C7!$3O$b8;* #%-:CW*@C-:3 RyQ:kAB#%3%3P7 :10:1<>3%32`Ar#%-)+7 8;"#2'4+<a$b0-:k'"#%*,!$3P-${G:|&74!$3%r8;4 "!$#%&9N,<>$8"04!$-:3%3%#\-)+8; M!$#%gNf<a$K0-:k'"#%*+!$3 -${G:|[7!$3%K!$CX8M0!$-:3%3\#%-)+8; "!$#%&9NSW PIQR]<A@B | X] zWPIQR]!WPbQR]!FO KJfiJ [ wQRJ+FIHKJMLON*P KQRJfiNRL@BKJ]lSM_qEeN+, * - P=< +N , * P#b8N.- PN+, * /- , * Pfi 8-fiffffN "P<>$.0-:'"#\*,!$3w-${G:|&74!$3%9NPC74!$3-${G:|&74!$3%.!$CQ8M0!$-:3%3\#%-)l8; "!$#%&9N l<>$.0-:'4"#%*,!$3!$3%3 G C4# :9":g,8; "!$#%&9N Q<a$]0-:i'M#%*,!$3D!$3%3 G C4# :9":g,!$C 8"0!$:3%3%#%-)Q8; "!$#%&9Nd!$CH<>$r0:.'"#%*+!$3^!$3%3 G C# :9:&9NC7!$3 !$3%3 G C# :9:&d!$Cm8"0!$:3%3%#%-)8; "!$#%&9RDcd0&74BSB#%HSBT!E''43\#2:CZ X0-:l'"#\*,!$3${G:|&7!$3\k8;4 "!$#%&9N(AB04#%3% @SB#% -SBT!E''43%#2:CY Q0-:'"#\*,!$3O-${G:|&74!$3%B!$CW8M0!$-:3\3%#%-)8; M!$#%gRN - "P`N - Ps}{#w,* -/,*{#}yQ:b_" '`?$:b0!E9N@AB#20+: 'O:8;/ HSBbN@8"0!$4-:3%3%#%).8; "!$#%&!E:r#%)0g :9V0!$]0-:B'4"#%*,!$3-${G:|[7!$3%d8;4 "!$#%&9N 7d32:V#2)0&d0!$W0:.'"#%*+!$3 !$3\3 G C# O:9:&V8;4 "!$#%&9Rsx QJ u9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]- 7 - 7 ! , * -/, * 7 , * - 7 - ! , *ufffifififf5vz{ 0#%k!$C <a3\32`AB#\-)}'"&$<=9NA: 7 k' ?$:W0-:W* #%*'O$!$&1":7329R K0-:9"Qs e<>3%32`A |&7#\8L@325$N@$<a :m7#%-)H "!$4#2#2?@#2 5$N*,-$ #%89#2F5f!$4CW0-:._-p@:C@G'O#%&V0-:9$:*,9Rc 0- KSBSB Nb8;"#%C-:9i! 'O:9"*k7-!E#2 '" 32:* AB0-:x'4"#%*,!$3K!$3%3 G C4# :9":g8; M!$#%g(#\ KSBbR @7-'4' :0:d8"0!$:3%3%#%-)b8; "!$#\g :9 A:9:!$4C VAV!$/-$(SBKRcd0-::#20-:9 (#\B :9b f!$C k0!$ :3%#%*,#%4!E :CW<a"*h#2bC-*,!$#\ffN$ 6#%b :9B D!$C(0!$:3%#%*+#%!E :CW<>* #%BC-*,!$#\ffR 7r-:#20:9B$<0-: :6 AW89!$ :r#\d'O# 32: 5l0-:k8; "748;#2$<.0-:X'M#%*,!$3B!$4C C47!$3B*@C-:3 R r:8;:X0-:X8"0!$:3%3%#%-)n8; "!$#%&W!E:X!$3%3KSBbRVcP 0-"#\8;-:9NO8;#%C:9b! G?!EM#%! 32:k'O:9"*67!E#2}'4 32:*h#%}Ar0#%8"0]!$C*(1RDcd0#%D#\DSB7$KBR/3c Y0- AhSBSBND7-''O :l0!E,0:}8"0!$4-:3%3%#%)X8;4 "!$#%&,!E:xSBbR"#%C-:9,!-${G:|[7!$3%b8; "!$#\g9ND0!Eb#%B$rSBbR A.N (!$C 1*k7 b0!?$:k0-:k!$*:#%)32:9 1C-*+!$#%ffN R(V#%C-:9 0-:V8"0!$:3%3%#%-)d8;4 "!$#%& :9 A:9:4!$4C &Rwcd0-:/4325.SB?E!$3%7-:.<>$ H#% R @#%*,#%3\!E"325$N[0-:.325lSB ?!$3%7:j<a$ H#%W0-:68"0!$:3%3%#%-)8; "!$#%& :9 A:9:!$C}#% R 7 -R B:48;:$N i0!$H-xSB ?!$3%7:9RxcB0#%1#\6!x8;g M!$C#%8;#2 !$k0-:8"04!$-:3%3%#\-)r8; "!$#%&(!E:dSBbR r:8;:d!$3%3--${G:|[7!$3%8;4 "!$#%&(!E:dSBbRc .0- "#%8;:9N8;#\C-:9b! G?E!E"#%! 32:k'O:9"*k7-!E#2m' 32:* AB#20+!$CRkcd0#%*(1#%DSB7-d#%D-$BSB Rc .0- SBSB N 5k*-$ #%89#% 5$NgSBSB Rwc .0-`A 0:":9?$:9" :$Ng8;"#%C-:9!f'O:9"*67!E#2x' 32:*hAB0#\8"0X#%KSBR1cd0:m0:9:H:;p@#%b!E.3%:!$ K-:-${G7-K-$jSB:|[7!$3%8; M!$#%gw0!E(#\J$wSBKR$y #207-32J$<4)$:-:9M!$3%#2 5$N&32:90#% :HFA6C7!$3&?!E"#\! 32:=!b 5[*+*:9 "#%8(!E")7*:& 89!$ :*,!$C-:<a$ Ab'4"#%*,!$3E?E!E"#%! 3%: MR [ $0.0-:!$ @89#%!E :C =C7!$3*k7 j0!`?$:0-:!$*:#%-)3%:9 xC*,!$#%ffNff!`?E!E"#%! 32:NO89!$3\3 0:*!$4C5 R B:48;:$N^0-:C-*,!$#\ $<K0-:i'"#\*,!$3?!E"#\! 32:x#%893\7C-: 6!$C -R #%C:90-:}8"04!$-:3%3%#\-)Q8; "!$#%&m!$CR0#%H#%k-$SBT!$H0-:l?!$3\7-:Q04!$kX"7-''O$9RQcd0#%k#%H!:9 A:9:8;& "!$C#%8;#2^Rc W0-`A jSBjSB N^8;#%C-:9K!l'O:9"*k7-!E#2x' 3%:* 0!Ej#% KSB Rko-$j:9?$:95'O# 32:b!$"#2)*:&D$<w!?!$3\7-:j +!?E!E"#%! 3%:$N[0:9:.:;p-#% d!8;#\ :gB:;p[ :"#2W ,0-:1$0-:9?E!E"#%! 32:N *! """ #%$lAB#20k<a$j!$3%3 & -R1Sbb0#%K#%b!f' :9M*67-!E#%ffNO0#%8;$:' 4C( k0-:r!$"#2)*:&/$<^74#%|[7-:D?!EM#%! 32:/ 6?E!$3%7-:9R r:8;:$N@0-:r8;$": 'OC#%).C7!$3!$3%3 G C4# :9":gD8; M!$#%gr#% KSBbR4ow#%4!$3%325$N@0-:18M0!$-:3%3\#%-)H8; "!$#%&d!E:1 "#2?@#%!$3%325+SBKR('@B@ @ r~{YM{`"-!|])"- !G,*-, * -/, * 78X]]B =@B]4^@ YV@ >1K{`cz@B|mz2|@@B_]4^@ >1K{`~{`-, * - * , * -/* !, ,EG e@0^ |- 7)@ ] 4{ @ ]=-D@^]a]@ @ @ >1K{`@]4^$e-]B@^e*@B , @ cRz |~z-]4^ zO]4^ B| ) z | e@+@B2@B YV@z |K#b%)gefififfv2$ff6ff )T$fiffv~ff#%-)b0-: :d#\C-:g#%#2:9NEA:D89!$#%*+*:C#%!E :325.C:C78;:$N<>$#% !$8;:$N$04!E(#2JC&:-$#%48;:!$ :'"74#%-), i04!?$: $0Q8"0!$4-:3%3%#%)]8; M!$#%gj!$4Cm'4"#%*,!$3 >$1C7!$3 d-${G:|[7!$3%j8;4 "!$#%&9R${G:|&7!$3\}8; M!$#%g}C- -$i#%48;:!$ :Q0-:Y!$*,7gW$<8; "!$#%&W'$'4!E)!E#2 `?$:9m0!E!$8"04#2:9?$:CXAB#20X8"04!$-:3%3%#\-)+8; M!$#%gj!$3%-:$R.SrK7b:;p@'O:9"#%*:&b0-`A 3\!E :9b^Nff0-:95}3%5!$CC `?$:9"0:!$C n0-:x8; "!$#\gl 32?$:9R zFl#%]#%#2)0& <=73D :;p[ "!$8;l<>* 0-: :x'[$<>]0-::!$ 4HAB0g5n!E"8 G 8;#% :8;5n'O:9<>$"*,kC# O:9:&k!$*,7gH$<b8; "!$#\gH'$'!E)!E#2 #% 0-:C# O:9:&d*@C-:3%9RSr"8 G 8;#% :8;5WC-:32:9 :d?E!$3%7-:D#\l0-:1C-*,!$#\V$< ?E!E"#%! 32:D!$d<>3%32 AB=R=<t2} {st2w'{ #2<0-:C-*,!$#%m$</!$&5}$<(0:k'4"#%*,!$3P?!E"#\! 32:B#\r":C78;:C!i#%-)3%:9 >:#20-:9 5x8; "!$#%&.'$'4!E)!E#%Q$ 5x!$"#2)*:&j#%Q! !$8"L& "!$8"L[#\-)!$3%)$$"#20* MN$:<a$"89#\-)bSBnH0:'4"#%*,!$3[-${G:|[7!$3%w8; "!$#\gw:*,`?$:0#\ ?E!$3%7-:<>*!$3\3ff$0-:9d'M#%*,!$3 ?!EM#%! 32:9Rsw'xt2{l}ffut2{l{''w'{]}?>!$,AB#20 '"#%*+!$3-${G:|[7!$3%+8; "!$#%& D#% !$CC#%#2ffN/#2<B0-:iC-EG*+!$#%i$<(!$&5WC7!$3^?E!E"#%! 32:.#%d":C78;:Ci f!+#\-)32:9 ffN:<a$"89#\-)+SB i0-:k8"0!$:3%3%#%-)8;4 "!$#%&b:*`?$:K04#%B?E!$3%7-:6<a"*h!$3%3P$0-:9jC7!$3 ?!EM#%! 32:9RBz m'4!E#%89743%!EN#%<w!]?E!$3%7-:@89897-"J#%j0:C-*,!$#\.$< 74 P-:$0-:9J'"#%*,!$3E?E!E"#%! 32:$N :<a$"89#\-)BSBZ.0-:8"0!$:3%3%#%-)8;4 "!$#%&d:"7-:V0!EB$0-:9B?E!$3%7-:.89!$ :.!$"#2)-:CW ,04!Ed'"#%*+!$3?E!E"#%! 32:$R{#st2w'{r5t2Ttv ,w{} {#st2w'{ :-<>$"89#%-) KSB m!'"#\*,!$3ff!$3%3 G C# :9:&d8; M!$#%grAB#%3%3'4"7-:,!$3%3(0-:+?E!$3%7-:j04!E6!E:]:*`?$:C 5x:<a$"89#\-)}SB Y0-:+'4"#%*,!$3-${G:|[7!$3%1$8M0!$-:3\3%#%-)18;4 "!$#%&9Rz{]!$CC4#2#2ffN&:-<a$M89#%-) jSB #%/ *,:9#%*:V! 32:b 6'4"7-:r$0-:9?E!$3%7-: >:$R )-R#%< A:j0!?$:KFAH'"#\*,!$3-?E!E"#%! 32:AB#20+325FAH?E!$3%7-: :9 A:9:]0:*WN&0: :?E!$3%7-:dAB#%3%3 :K:*`?$:Cm<>* !$3%3O$0-:9r'"#%*+!$3?E!E"#%! 32: MRsw'xz{ "#2:9< N-SB W0-:j'4"#%*,!$3O-${G:|[7!$3%d8;4 "!$#%&dC-:9 :8;b#%-)3%:9 l?!E"#\! 32:9N@AB0#\3% VSB0-:k8"0!$:3%3%#%-)]8; "!$#%&bC-:9 :8; $0x#%)32:9 i?E!E"#%! 3%:6u sk#\-)32:9 i?E!$3%7-:R KSB!x'"#%*+!$3(!$3%3 G C# :9:&68;4 "!$#%&9N( 0-:f$0-:90!$CffN(C-:9 :9M*,#%-:6)32 !$38;#% :8;5ZAB0#\8"0#%893\7C-:V#%-)3%:9 l?!E"#\! 32:9N#\-)32:9 l?E!$3%7-:d!$CW*+!$g5l$0-:9b#27!E#249Rt2w'{t2w'{lw'{HKJs}{wD{#}Xcd0-::d:73%(89!$ :B3%#%<a :C k!$32)$$"#204*,0!E*,!$#%&!$#% >)$:-:9"!$3\# 9:C w!E"8 G 8;4#% :8;5,C47-"#%-):!E"8M0ffRHz 4C-:9:CffNO0-:)!E'4 :9FA:9:Q0:k'4"#%*,!$3w!$3%3 G C4# :9":gK!$CX0-:8"04!$-:3%3%#\-)f8;4 "!$#%&9N!$C :9 A:9:Q0-:8"04!$-:3%3%#\-)]8; "!$#%&j!$CX0-:H'"#%*+!$3 -${G:|[7!$3%j8;4 "!$#%&j89!$ :6:;p[G'O-:&#%!$3%325,3%!E")$:$R$ :10!Ed-$d!$3%3OC# O:9:8;:D#\f8;4 "!$#%&D#2)0&-:V:7432D#%f:;p['O-:&#%!$3:C748;#2/#\+ :!E"8M0ffR(o-$D#%!$8;:$N& *,:bC# :9:48;: :9 A:9:l*[C:3%(AB04#%8"0,!E":r3%5k'O325@-EG*,#%!$34!E:B#%C-:&#2_:C#\]V0-:-)j:9!$3 RMR d:89!$3%340!E(A:BAd"#% :Y#Y!$C0-:9:H#%b!+'4 32:* mAr0#%8"0x!$32)$$"#%0*?@#%#%d:;p@' :g#%!$3\325l<a:9A:9 "!$8M0-:bAB#200!$fR$ :H0!E KSB!$CXSB !E: $0x'O325@-*,#%!$3^ f:<a$"8;:$N^l!$x:;p@'O-:g#\!$3P:C478;#2#% M!$8"0-:d "!$3%!E :d ]!$W:;p@'O-:g#\!$3O:C78;#2}#%l"7&#%*:$R=d1:07070$u&" ff #! ff=sx QJ u9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]& , * -/, * 7 , * - 7 - &fffi ffQs e-vyQ:r)#%?$:B'[$<=(<>$/0-:b* #\*'O$!$g#%C-:&#2#2:9R b0-:9:732/<a3\32`A0-:.3%!$d0-:9$:*WR<>*, *#br#%*,*:C#\!E :325fi 8-fiffff$&RoI$@ @ :o c1K{YoAI~{YoAI@BLYc 0-`A ~ KSB~}SB N@8;4#%C-:9D!G?!E"#\! 32:b' :9M*67-!E#%]'4 32:*qAB#20<>$(!$C # (#Rwcd0-:ffNE)#2?$:k!B32:;p-#%8;$)$"!E'0#%89!$31?E!E"#%! 32:K$"C:9"#%-)-N4~ KSB#%*,*,:C#%!E :325]<>!$#%3\9N@AB0#%3%V~mSB!EL$: "!$8M0-:9Rc ]0`A ~}SB~mSBN8;4#%C-:9B!G?E!E"#%! 32:1'O:9"*k7-!E#2W'4 32:* Ar#20*N !$4C"""<a$[R6cB0-:ffNO)#2?$:Q!l32:;p-#%8;$)$"!E'40#\89!$3P?E!E"#%! 3%:6$"C:9"#%-)-N~mSB!EL$:!EL$:"!$8"0:9R'"!$8M0-:D +0`A #\ 3%7 #%3%#2F5$NgAB0#\3% D~}SBz \ oAIc1>1K{"""6{Yo$- &>1K{`*@B*Y r~{ 6{Yo I%,-zt$s}fiffaw{u-Ro $,*@MRo6J 18~m!$#%g!$#\#%-) >)$:-:9M!$3%# 9:C (!E"8 G 8;#% :8;5]3\!E)$:d'O:9"*k7-!E#2,'4 32:*,/89!$ :D:;p@' :4#2?$:$RyQ:,*,!`5}0-:9:9<>$:HC-:89#%C-:k W7 :H!l8"0-:!E'O:9.32@89!$3 8;4#% :8;5m'$'O:9F5i3%#2L$:k0!Ej*,!$#%&!$#%-:C5k<>$AV!E"C,8M0-:8L@#%-)-R(o-$:;p@!$*,'432:$Ng0:bV0-@8;j_4#2 :;G C*,!$#%H [32L[#%(#%]V3%!$#%:D7 : 7 /4o!$3%3 G C# O:9:&,8; "!$#\g9R cB0-:}8"04!$-:3%3%#\-)Z8; "!$#\g]:*,!$#% #2)0& :9]0!$ 0-:}'4"#%*,!$3-${G:|[7!$3%d8;4 "!$#%&dAd"do bR5xuJ$ ff ! $3JQu9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]ff, * -/, * 7 $ , * - 7 $ - ! $ , * ! J$! J6$K:gW:9W!$3 RVK37373>f' ?$:Qo , * !J$ ff3oJR c 0- "#%8;-:f 'O:9"*k7-!E#2Qs e' 32:*, =!$]$''O: C n0-:}*$:})$:-:9"!$3B893\!$,$<.C-:8;*,' "! 32:i8; M!$#%gf 7C4#2:C 5b:&9N [ :9)#%7ffN!$C Z!$3%0ffNMN8;4#%C-:9H!G?E!E"#%! 32:]'O:9"*k7-!E#2n' 32:*UAB#%0*f!$CRHz : 'O:8;#2?$:H$</0:k?E!E"#%! 32:k!$4Cm?E!$3%7-:H$"C:9"#%-)-N(1/30- ABr0-:1'" 32:* #\r74!E#% _4! 32:.#%}!EK*"!$8M0-:9RBoJ5i8;*'4!EM#% i!EL$:!EB32:!$"!$48"0-:9Rc x"0-`AoJN8;#%C-:9k!$#2)4#%-)i0-:f?E!$3%7-: x x0-:]'"#%*,!$3(?!E"#\! 32:FRmo:* ?$: H<>*q0-:jC-*,!$#%]$<P!$3%3 $0-:9V'"#\*,!$3?E!E"#%! 3%:9Ro#% !$&#%!E :V0-:KC7!$3?!EM#%! 32:AB#%0 0-:}?E!$3%7-: NB!$C 0-: :*,`?$: k<a* 0-:mC*,!$#% $<.!$3\3D$0-:9f'"#%*,!$3d?!E"#\! 32:9Rr:8;:$N oJ'M7-:D!$3%3P0-:.?E!$3%7-:d04!EroC-&:RVc ]"0-`A "#%8;:9N48;"#%C-:9B! EG?!E"#\! 32:'O:9"*k7-!E#2W'" 32:* AB#%0,!$CR K#2?$:x!,32:;p-#%8;$)$"!E'0#%89!$3*(1/?E!E"#%! 32:!$C6[7*:9M#%89!$3?E!$3%7-:($MC-:9"#%-)-NEo0- ABP0:/' 32:* #% 7!E#\ _4! 32:(#\"!$8M0-:9R5W8;*'4!E"#\ l!EL$: 7"!$8M0-:9Rb:&1:96!$3 R.'`?$:fooJR]c }0- 0-:+:9?$:9" :$Nw8;#%C:9.!$#2)#\-)W0-:?E!$3%7-: l W0-:k'"#%*,!$3 ?E!E"#%! 32:R1o:* ?$: l<>* 0-:C-*,!$#%x$</!$3\3 '4"#%*,!$3P?!E"#\! 32::;p-8;:9'R r`A:9?$:9N^o!$3% ]:* ?$: l<a"* 0-:HC-*,!$#\}$<(!$3%3 '"#\*,!$3^?E!E"#%! 3%:d:;p-8;:9'#%48;::!$8"0Z@89897-"1#%Q! #%4!E5}-${G:|[7!$3%.8;4 "!$#%&.AB#20!$#%-:C 5m'4 {:8;#\-)W7-0-:1!$3\3 G C# O:9:&V8;4 "!$#%&9R r:8;:$N4oRc ]0`A 4ooJN8;"#%C-:9B#%4 !$g#\!E#%-)0-:6'"#%*,!$3O?E!E"#%! 32: wAB#20i0-:.?E!$3%7-:-RKo:* ?$: f<a*h0-:kC-*,!$#%m$<(!$3%3 '"#%*,!$3^?!E"#\! 32:d:;p-8;:9'N <>*h0-:kC*,!$#%$<^!$3%3C7!$3?E!E"#%! 32:(:;p-8;:9' $N[#% !$&#%!E : BAB#20+0-:B?E!$3%7-: N@!$C+0:,:* ?$: <>*q0-:C-*,!$#\,$<P!$3%3 C7!$34?!EM#%! 32:/:;p@8;:9' R(o!$3% k:*`?$: k<>*q0-:KC-*,!$#%+$<P!$3%3'4"#%*,!$3?E!E"#%! 32:K:;p-8;:9'{R,cd0-:325x'O# 32:kC4# :9":8;:#%.#2<-:,$<0-:,$0-:9kC7!$3w?E!E"#%! 32:N^!504!$b!lC-*+!$#%xAB#2'O:97-9RbzF<(04#%K0!E''O:9N /04!$K-:H?!$3%7:k#%x#2KC-*,!$#\ffN 0!Ej#%b#%x0-:C-*,!$#\Z$<B-m$0-:9k'"#%*,!$3/?!EM#%! 32:$R -<a$M89#%-) KSBq#%*+*:C#%!E :325XC-:9 :8;H0!Ed89!$4-$p@ @ @ >1K{`~{`1:.- !q|]-e-@B|H@B],*@B1:3m|zg51 !@B1!S|@"@ :M{`"D@ >1K{`nVK37373>,* - ,*|Xz-K37373>, * -/, *,*, * S|pf9|,*]]-, * @ @ Y[@ r~{Y1 7qe],*@B@ Bz-|0@B#b#b#65@B z@Bz@B,*fififfv2$ff6ff )T$fiffv~ffG 9eB|!EL$:0-:H?E!$3%7-: N^!$CX*674 b#\ :!$Cx!EL$:0-:k?!$3%7:R r:8;:o0!$K!lC-*,!$#%mAB#2'O:97-AB0-::9?$:9VoJC &:Rwc 0- "#%8;-:N@8;4#%C-:9V! `G?!E"#\! 32:b' :9M*67-!E#%+'" 32:*qAB#20f!$CR1z : 'O:8;#2?$:6$<(0-:k?!EM#%! 32:*(1/34!$Cx?E!$3%7-:k$"C-:9"#%)-N!EL$:.!Ej32:!$M!$8"0-:b W0- 0-:H' 32:*h#%K7!E#\ _4! 32:$R5l8;*'4!E"#%W!EL$:r,*$:.0!$"!$8M0-:9R:# :9":j:9d!$3 RV' ?$:jooJRcP0- "#%8;:Vf' :9M*67-!E#%l' G32:*,NP8;#%C:9.!G?!E"#\! 32:'O:9"*k7-!E#2X' 32:* AB#20i!$C*(1/RQz :' :8;#%?$:]$<d0:]?E!E"#%! 3%:+!$Cn?!$3%7:]$"C:9"#%-)-Nwo f0- ABk0-:f' 32:*#%37"!E#% _4! 3%:6#%X!Ej32:!$"!$48"0-:9Rko5m8;*'!E"#% x!EL$:1l*,$:H0!$"!$8M0-:9R* -/*@ @ @ , , @ >1K{`~{`)+,@ @ @ :M{`"~{`&~{I,&p!1, * -/, *Vyd1:07070$p"@ :M{`"@ @ @ @ >1K{`~{`&1'r{ #{}{#w5Sb-$0-:9w8;*,*k*:90-@C1 b:C78;:8; w#%^ b:-<>$"8;: 7 74C^8;"#% :8;5$Rwo-$w:;p@!$*,'432:$N7CD8;4#% :8;5l#%d74 :CW +'4"7-:j!)32 !$3^8; "!$#\gB#\g?$32?@#%-),!+7* $<w?E!E"#%! 32:D!$4C}!:9d$<#%:|&7!$3\#2#2: :9 )#%r7-:0-:9NMRdSbB!,:8;Ci:;p-!$*'432:$N *:.$<w0-:.:;p@'O:9"#%*:&Q'O:9"*k7-!E#2X'" 32:*,b'O:9<>$"*:C 5 @*+#20j7 :C 74CK8;#% :48;5mZ8;:9"!$#%$<D0-:f8; "!$#%&9Rly #20 7C4j8;#% :48;5QZ'O:9"*k7-!E#2Z'4 32:*,NPA:+ !$#%n!i?$:95#%*+#%3%!EV$"C-:9"#\-)H$< 0-:6*[C:3%D!$dAB#%0WSBbR9sxu#?fiK37373>VK37373>JQu9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]ff- ! ,*,*Qs e c i0- )d - ! )d , * NP8;#\C-:9j!l' :9M*67-!E#%X' 32:* AB04#%8"0X#%)d - 7-K-:$<0-:'M#%*,!$3J-${G:|&74!$3%.8; M!$#%gj#\j-$9)dbRffcd0-:ffN^#2KA73%CQ#%&?$32?$:H Ai?E!E"#%! 3%:9NZ@BV!$C!,* - ,* 7,* - 7ff@LE G { G7G qf/)E zu{Yz G mf0lAr#20W#%C-:&#%89!$3O#%& :9?!$3ffC*,!$#%9NR -<>$"89#%-) l0-:.8M0!$-:3%3\#%-)H8; "!$#%&$!$CA73%Cx:C748;: , l0-:C-*,!$#%R /-<>$"89#%) x0-:8"0!$:3%3%#%-):9 A:9:8; M!$#%g :9 A:9: b!$CKA743%C,0-:f89!$7 :b!6C*,!$#%,Ar#2'O:97-9R V7-0#%8;g M!$C#%8;0-:8"04!$-:3%3%#\-).8; "!$#\g :#\-) dbR r:8;:$N@!$3%30:B'"#\*,!$3--${G:|[7!$3%8; "!$#%&*67 : dbRc }0- "#%8;:9R,8;#\C-:9.!G?E!E"#%! 32:'O:9"*k7-!E#2X'4 32:* AB#%0D!$C*(Rc0\#%#7$R1c m0`ANJ8;"#%C-:91!}' :9M*67-!E#%Q' 3%:* AB0#%8M0Z#% R @7-''O :HA:!$#%)x! 7C4!E5W?!$3\7-: W i!]'4"#%*,!$3P?!E"#\! 32:$N>$j:|[7#2?E!$32:&325$Nff! 74C!E5W?E!$3%7-:!WC7!$3J?E!E"#%! 3%:$N MRHSrK0:,!$3%3 G C# O:9:&b8;4 "!$#%&j#% dbNO0#%j89!$ :H:;p[ :C:CX i!$3%3w0-:$0-:9d'4"#%*,!$3 ?E!E"#%! 32:D74#%-)H:!$8"0m$<J0-:j?!$3\7-:D8;:$R/cB0#%V)#2?$:d74d!,8;#\ :gd!$"#2)*:&<>$.!$g5m$0-:9j'"#%*+!$3P$.C74!$3 ?E!E"#%! 32:$R B:48;:$NP#2j#%R6c l"0-`A M#%8;-:9N^8;4#%C-:9j!G?E!E"#%! 32:'O:9"*k7-!E#2X'4 32:* AB#%0V!$CR]cd04#%j#%*(1/37-d$ Rc j0-`A SBN8;4#%C-:9w!B'O:9"*k7-!E#26' 32:* Ar0#%8"06#% 7 -$wSBRwcd0-:0-:9:*k7 :6:H8;4 "!$#%&9N ENffAB#20!$C 04!?@#%-)f0-:!$*:#%-)3%:9 xC*,!$#%ffNR V7-9NP#2<0#%j#%K0:,89!$ :$NP:-<>$"89#%)Z0-:,8"0!$4-:3%3%#%)]8; M!$#%g :9FA:9:!$C6!$C :9 A:9: K!$C.A73%C]'`?$:j04!ED0-:K' 32:*T#%V7"!E#% _4! 3%:$R r:8;:$N#2V#%VSBRc m0- Aq M#%8;-:9Nw8;#\C-:91!G?E!E"#%! 32:,'O:9"*k7-!E#2Z'4 32:* AB#%0$!C*(Rcd0#\D#%DSB7-D-$ R '1@ B@ E1K{`"G]4^@)]7^*! ) , , * -/, *|)]7^) eX)])"@ @ E1K{`"G-G +)4] ^@ E1K{`"G9)@BS),* ! ) -D@]7^,*Z@B @!)))@Bp@)mz)* -/*@ @ @ , E 1K, {`"G9e, * -/, *l))X@ @ E ~{`""G)-,*@Be*@ YF@ E1K{`"G ,-#bfi 8-fiffffxb:Dsw}ft$um){#w*y 9d1:070$,7m_{},_"7-5@-:V!$C :"# :9:w0!`?$:d0-`Ar10!En#%J!B'"*,#%#%)d_432 :9"#\-)B :8M0#%|[7-:V! ?$:SBbR zFr'"7:B*,!$&5W$<(0-: (z ?E!$3%7-:b!Eb3\#2 32:.:;p@ "!f8; r lSBbR @7-'4"#%#%)325$N-8"0!$:3%3%#%-)8; M!$#%gH!E:l#%8;*,'4!E"! 32:] m0:f'4"#%*,!$3(${G:|&7!$3\k8;4 "!$#%&kAdKRwV0!$4-:3%3%#%)8; M!$#%gD89!$}#%8;:!$ :K0-:j!$*74gD$< '$'4!E)!E#2 ><a$D:;p-!$*'432:$N@AB0:W!kC47!$3 ?!E"#\! 32:K0!$325m-:,?E!$3%7-:32:9<>.#%Q#2jC-*+!$#% MR r`A:9?$:9N#\j0#%C-:9":C 5x0-: #2'4!E"#2 :8; "!$#%&)$"!E'40 :9FA:9: '"#%*,!$3K!$C C47!$3K?!EM#%! 32:9R SbCC#2#24!$3r-${G:|[7!$3%}8; "!$#%&W '4"#%*,!$3!$C $rC7!$3O?E!E"#%! 32:D89!$}0-:9:9<>$:10-:3%'W'$'4!E)!E#2^R+esxu__JQu9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]! , * -/, *! , * - ! - ., * . Qs e cPX0- _ - .3 _V * Nw8;4#%C-:9k!XEG?E!E"#%! 3%:+'O:9"*k7-!E#2Z'4 32:* AB#20X@ *@ ( @ 1 >1K{`~{`m!$CO@ / >1K, {`~{`~{Y Rncd0#\6#%* _V , * 76-$ _V - RXo$k0:]:9?$:9M :C#2":8;#2ffNd8;#\C-:9f!" G?E!E"#%! 32:x'O:9"*k7-!E#2 ' 32:* AB#20 @ * @ ( @ 1 >1K{`n!$C@/ @3r~{YM{`"RDcd0#%D#\ _ - 7-d-$K _ , * Rc 0-`A_ , * - ! _ - NB8;4#%C-:9l!E)!$#% 0-:X3%!$ f:;p@!$*'32:$R cd0#%]#%H _ - 7-l-$_V , * - Rc 10- _V * -/* ! _V * - Ng8;#%C:9(!9& G?!E"#\! 32:D'O:9"*k7-!E#2H' 32:* AB#20`@ * @ (, , @ , @ @ :M{`"~{`&RVcB0#%V#% _ * - 7-d$K _V * -/* R>1K{`~{`~{YM{`"~{`&]!$C @1/3c ]0`A jSB !; _ , * - , * N48;4#%C-:9b!'O:9"*67!E#2i'" 32:, * AB04#%8"0i#\ K SB , R , @7-''O :A:+!$#2)Y!W?!$3\7-: |i m!W'"#%*+!$3 ?E!E"#%! 32:$NL@ B >$6:|&7#%?!$32:&325$NP!i?!$3%7:/zB }!iC47!$3w?!EM#%! 32:$N] 6MRmSb10:f!$3%32G C# O:9:g68; "!$#%&6#\KSBKN 0#\189!$ :,:;p[ :4C-:C x!$3%3/0-:f$0-:96'4"#%*,!$3fi?E!E"#%! 32:K7"#%-)]7'X!$3\3 0:k$0:9j?!$3%7:9R1cd0#\B)#2?$:17K!f8;4#% :&.!$#2)4*:gK<>$j!$g5x$0-:9r'"#%*,!$3 $bC7!$3O?E!E"#%! 32:R B:48;:$N40-:.'4 32:* #% /z!$CW0[7VRc +0-"#\8;-:9N^8;"#%C-:9.! `G?E!E"#%! 32:H'O:9"*k7-!E#2X'" 32:* AB#20*(1/!$CRDcd0#%D#\7$KBR34c 60`A SBVN[8;"#%C-:9(! EG?E!E"#%! 32:D'O:9"*67!E#2' 32:* AB#20*(1r!$CRcd04#%P#%V7- -$SB Rwo-$0-:V:9?$:9M :C4#2:8;#2ffN$8;"#%C-:9/! G?!E"#\! 32:b' :9M*67-!E#%+'" 32:* Ar#201!$CRVcd0#%*(1/3#%DSBR '7-D-$X,D@ @ @ :M{`"~{`&~{I,- .> _ *>1K{`~{`@ >1K{`~{`~,{Y"K_ , *-fifftu D{{wDqe_, * -/@ , * @ @ _ ,@* -/, * >1K{`~{`_ , * -/, *9_ , *@ @ @ >1K{``@ @ @@ @ r~{YM{`"{#}cd0-:,#\8;*'4!E"! #%3%#2F5W$<8M0!$-:3%3\#%-)l8; "!$#%&1!$4CX'"#%*,!$3w-${G:|[7!$3%18; "!$#%&j:*+!$#%AB0-:lA:1* ?$:67-'W0:132@89!$3^8;#% :8;5f04#2:9"!E"8M0g5f<a"*V (zbRuxff( _JQu9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]ffl_! , * - , * ! , * - ! - . , * . Qs e cP 0- AG_(z - . _(z * Nd8;#%C-:9f!EG?E!E"#%! 3%:i'O:9"*67!E#2 '" 32:* AB#%0R@ *@ ( @ 1 >1K{`~{`Y!$4C @ / , >1K{`~{`~{Y R cd0#\,#%s_/z , * 7-]$l_(z - Raf/<a$"89#\-) _(z0:}8"0!$4-:3%3%#%)X8;4 "!$#%&,:C478;:+@ / Y0-:}"#%-)32:9 C*,!$#% : R o-$f0-:i:9?$:9M :C#2":8;#2ffNd8;#\C-:9f!" G?E!E"#%! 32:x'O:9"*k7-!E#2 ' 32:* AB#20 @ * @ ( @ 1 >1K{`n!$C@/ @3r~{YM{`"RDcd0#%D#\ _(z - 7-d-$ _/z , * Rfi#b8fififfv2$ff6ff )T$fiffv~ffc x"0-`A _(z * - !_/z - N(8;"#%C-:9H! " G?E!E"#%! 3%:+'O:9"*k7-!E#2n' 32:* AB#20 @ * @ (@ 1 > 1K{`k!$CD@ , / @ 3 Yr~{YM{`"RDcd04#%V#%S_(z - 7-d$S_(z , * - Rc ]0`A _/z * - * ! _(z * - N48;#\C-:9B!s& G?E!E"#%! 3%:.'O:9"*67!E#2W' 32:* AB#20D@ * @ (, , @ , @ @ :M{`"~{`&RVcB0#%V#%S_/z * - 7d-$S_(z * -/* R>1K{`~{`~{YM{`"~{`&]!$C @1/3c }0- KSB ! _/z , * -/, * NJ8;#%C:9.!W'O:9"*67!E#2X' 3%,:* #%XAB0#%8M0X0-,:,!$, 3%3 G C4# :9":g8; M!$#%gr#% KSBKR@@7-'4' :bA:6!$"#2)i!+?!$3%7: |+ f!,'M#%*,!$3O?E!E"#%! 32:$N<@BS>$r:|[7#2?E!$32:&325$N4!?E!$3%7-:z( ,!C7!$3 ?E!E"#%! 3%:$N6] 8MR/SbD0-:j!$3%3 G C4# :9":gV8; "!$#\gd#%Kj SBbN-0#%V89!$ :K:;p[ :4C-:C}!$3%3(0-:$0-:91'4"#%*,!$3J?E!E"#%! 32:17#%-)W7-'Z!$3\3w0-:,$0-:96?!$3%7:9R,cd0#%K)#%?$:17.!}8;#% :&!$#%)*:&^<a$ !$g5jFAr$0-:9P'M#%*,!$3$ C74!$3$?!EM#%! 32:9R B:8;:$NE0-:-${G:|&74!$3%P!$C18"0!$:3%3%#%-)8; M!$#%gH!E: (zbRwcPX0- "#%8;-:"9N(8;#%C:9k! `G?!EM#%! 32:f'O:9"*67!E#2Y'4 32:* AB#20}!$CRQcd0#%j#% (z71-$*(1/34KSB Rc ,0- (zSB N8;4#%C-:9D! EG?!EM#%! 32:K'O:9"*k7-!E#2f' 3%:*qAr#20*(1!$CRVcB0#%D#% /z7V$BSB R -<a$M89#%-)SB i0:.8"0!$:3%3%#%-)/8; M!$#%g1:C78;:/ m0-:+#%)32:9 ZC-*,!$#\ RWo-$10-:]:9?$:9" :]C#2:8;#2^N 8;#\C-:91!G?E!E"#%! 32:6'O:9"*67!E#2}'4 32:* AB#20+!$CR6cd0#%*(1/3#%DSBR'7-D-$ /z_,@ @ YF@ YF@ >1K{`~{`a_ , * .>1K{`~{`D@ >1K{`~{`~{Y_@"-aw'{xb:,*_{s}){#we@ YF@ YF@ :M{`"~{`&~{I,_,*:Vy %d1:070$,7_+_!, * -/, *@ @ @- qf@ @ @ >1K{`{#}_@ @ r~{YM{`"" 7-5@-:Q!$C :# :9:l!$3% 0`A:C 0!E @SB #%l! '*+#%#%-)n_432 :9"#\-)Y :8M0#%|[7-:! `?$: $0+SBbN V !$C /z KN$'"7#\-)b*+!$g5k?!$3\7-:<>$#2#%*:$R (" :9(:9/!$3 R:9'O$ :C]'*,#\#%-)K:;p@' :9M#%*:&!$34:"732(Ar#20 @SB f|&7!$"#2)$7-']' 32:*+9Ng!k*673%#2'432:D'O:9 G*k7-!E#2m' 32:*iRDz & :9: #%)325$N !$rAB#20mSB!$C /z AB0#%8M0}3%#%: :9 A:9:7-r743%#2L$:SB !$C -SB MN 8M0!$-:3%3\#%-)i8; M!$#%gk!E:f#2)0& :960!$n0-:]'"#%*,!$3/-${G:|&74!$3%68;4 "!$#%&Ad -SBbRucsx_g_2_fiVK37373>!_JQu9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]ff,* - 7- ! ff , * . ff Qs e c }0`A @SB - ! -SB , * N 8;#%C-:9.!l'O:9"*k7-!E#2X'4 32:* 0!E.#%9@SB - !$CQ!$g5#%!$g#%!E#% <a$,!X'M#%*,!$3?!EM#%! 32: @ B R @7-''O :f0!E0-:W'4"#%*,!$3-${G:|[7!$3%,*@C-:3V$<b0-:fi!,* - ,* 7:7432#%-)]' 32:* 89!$-$ :H*,!$C-:HSBbRffcd0-:x0-:9:H*674 K:;p@#%rFAW$0-:9j'4"#%*,!$3P?!E"#\! 32:9N!`5 H!$CAr0#%8"0X0!`?$:,!E.* j-:H$0-:9j?E!$3%7-:$RH#%C:9b0:HC47!$3J?!E"#\! 32:6!$" [89#\!E :CAB#20k0#\P?E!$3%7-:$Rwcd0-:H7C:9J04#%w#% !$&#%!E#26$<0:'4"#%*,!$3g?!E"#\! 32:N:-<>$"89#%-)KSBnH0-:8"04!$-:3%3%#\-)]8; "!$#%& :9FA:9:Z0-:H'"#%*,!$3 ?E!E"#%! 32:V!$CX0-:C74!$3 ?E!E"#%! 32:$N^!$C :9 A:9:!$C0-:dC74!$3&?E!E"#%! 32:!$4Cd:"732#%H!jC-*,!$#\6Ar#2'O:97- ,0-:dC7!$3[?E!E"#%! 32:$R B:48;:d0-:' 32:* #%w-$ @SB Rcd0#%J#%!K8;& "!$C#%8;#2^RwcB0-:V'"#%*,!$3[-${G:|[7!$3%*@C-:3@89!$0-:9":9<a$: :*,!$C-:(SBQ<a3\32`AB#\-)V0-:/#%!$g#%!E#%K$<RJcd0!E^#%N`0-:('4 32:* #% -SBR c B0- AY "#%8;:9N8;#\C-:9B! G?E!E"#%! 3%:.'O:9"*67!E#2W' 32:* AB#20iC*,!$#%,!$C*(1/Rcd0#\D#% @SB7V$ @SB R3c +0- KSB@SB N48;4#%C-:9d!'O:9"*k7-!E#2i' 32:* 04!EB#% KSB RD#\C-:9d!$g5#%!$g#%!E#%Y<>$!m'4"#%*,!$3?E!E"#%! 32:$RxcB0#%k89!$ :l8;#% :g325Q:;p@ :C-:C X!$3%3?!EM#%! 32:k#%0-:i'4"#%*,!$3D*@C-:3 R 7,0#%,*,:!$,0!E]#2+89!$ :i8;"#% :&325 :;p[ :C:C Y!$3%3d?!E"#\! 32:#%}0-:1'M#%*,!$3^!$CmC7!$3 *[C:3 N4!E#%<a5@#%-)+!$&5 =8;* #%!E#%i$< d' :9M*67-!E#%i$K8"0!$:3%3%#%-)@@^n@ r~{Ys"-`@@B`@0^#)Be*D@ @ @ , @ 83j{:1K{`@ B*! , -@B-#b Ufi 8-fiffff8; M!$#%gR/Srd0-:.8"0!$4-:3%3%#%)H8;4 "!$#%&d!E:1"!E#% _4! 3%:$N[0:95f89!$ :.*+!$C-:.SBbR 4#%C-:9!$&5,#% !$&#%!E#2<>$!6C7!$3-?E!E"#%! 32:$R 5,!6#%*,#%3\!E(!E)7*:&9Ng!EL@#%-)60-:B!E''"$'"#%!E :B#\ !$@G#%!E#2X<>$j0-:,!$[89#%!E :CZ'"#%*,!$3 ?E!E"#%! 32:$Nff0:k":732#%)]'" 32:* 89!$ :k*+!$C-:,SBbR r:8;:$N)#2?$: !$&5 #\ !$&#%!E#2 <a$,!X'4"#%*,!$3$,C7!$3?E!E"#%! 32:$N0:i8"0!$:3%3%#%-)x8; "!$#\g89!$ :*,!$C-:HSBbROcd0!EK#%9N 0-:k' 32:* #% @SB NffcPl0- "#%8;-:"9N 8;4#%C-:9K! `G?E!E"#%! 32:6'O:9"*k7@G!E#2Q'4 32:* AB#20W!$CRfcd0#%*(1/34@SB7-d#%D-$ jSB R)e--, * . @, *tu {#w {}>1K{`~{`,@ @ @ @ 83j{:1K{`@ @ @ r~{YM{`"~{`&@a@c ,0- @SBSB N8;#%C-:9D!k<>7-V?!E"#\! 32:b' :9M*67-!E#%]'4 32:* #%fAB0#\8"0*10 !`?$:0-:f!$C40$!K0:C,*$!%#+Rc40%#K%#@B7K$.B6R$.0-:/:9?$:9":$N 8;#\C-:9j! EG?!E"#\! 32:H'O:9"*67!E#2X' 3%:*hAr#20l!$C*(1/RDcd0#%V#%BSBR'7-d-$ -SB83j{`~{`{-*@ @ ,Y 83j{:183j{:1K{`~{`9@ @SbCC#%-)+'"#%*,!$3P$jC7!$3P${G:|&7!$3\b8;4 "!$#%&b l8M0!$-:3\3%#%-)+8;4 "!$#%&rC&:K-$K0-:32'XSB$ @SBbR4cd0:1<>3%32`Ar#%-):7432B0- ABd04!EB0-:#2b!$CC#2#%iC-[:r-$b0-:32'}04#2)0-:9B32:9?$:3\B$<(32@89!$38;#\ :8;5l3%#2L$:. )'4!E0@G 8;#\ :8;5 =SB MR9usxJQu9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]ff,* - 7 - ! ,* . Qs e cPj0- SB _ - ! SB _V , * Ng8;#%C:9J *,:d8"0!$:3%3%#%-)r8; "!$#%&w0!E(!E:DSB _VbR= `A SB - !USB , * NE rA:57 -:9:C1 K0- A2_V - ! _ , * R(#%C:9P!B8;"#% :& '4!$#%^$<-?!$3\7-:9N!$C ' <>$j!l'4!$#2b$</'M#%*,!$3 ?!E"#\! 32:9N0@B!$C@ER6c !EL$:+!$&5}04#2"Cm'"#%*,!$3P?!EM#%! 32:$N @^&R1Sb._fi, * -/, * 7ffff] ];U@^ m_a@0^0-:i8; "!$#%& :9FA:9: N h!$4C}#\ VbNA:i89!$ _4C !X?E!$3%7-:l<a$}8;"#% :&AB#200-:b8"0!$:3%3%#%-).8;4 "!$#%&9R V7-/ 0#\/!$3% 6!E#% _:/0-:r${G:|&7!$3\8; "!$#%& :9 A:9:+'4"#%*,!$3?E!E"#%! 32:R r:8;:$N$0-:/'4 32:* #% VRwc b0`A "#%8;-:"9N8;4#%C-:9w! EG?E!E"#%! 3%:/'O:9"*k7-!E#2' 32:* AB#20RDcd0#\D#%DSB7-d-$dSB V R*(1/g)e_@ @ @ @c j0-`A SB _ * -/* 7 SB _ * , ,,A:B-:9:CB57 (0`A 04!Eq_ * - 7 _,7, *>1K{`~{`-_,*N$A:V:89!$3\3&04!EwSB_,* - 7_,* - 7-- EeSB VSBSB R r:8;:V R/#%C:9(!j'O:9"*67!E#2' 32:*WR -<>$"89#%-)0-:}8M0!$-:3\3%#%-)X8; "!$#\g]!$32-:i#%-<>:9" $0 0-:}'"#%*,!$3D!$4C 0-:}C74!$3D-${G:|&74!$3%8; M!$#%gR B:48;:$NV Rf,* - 7 _ qefi_ , * - 7 _ , * - 7 _ c .0- KSB. SB _ * -/* N&8;#%C-:9!9& G?E!E"#%! 3%:'O:9"*k7-!E#2k'" 32:* AB#20@ * @ (@ 1 @ / >1K{`~{` N!$C @ 3 , YF, @ :M{`"~{`& Rmcd04#%.#%6SB _ , * - , * 7-1-$ KSB RWo-$H0-::9?$:9":.C#2:8;#2^N@8;4#%C-:9D!+ G?E!E"#%! 32:K'O:9"*k7-!E#2f' 32:*TAB#20 @ @ @ >1K{`~{` N_4oq @ @ 9f*(1!$Cx0-:H!$CC#2#%!$3 #\!E5i8; !$#%g'"74-:B0:kC-*+!$#%*1 MR /<a$"89#\-) KSBN^!$CR -<>$"89#%-)*1( R r`A:9?$:9Nff0-: :HC-*,!$#\d!E:H-$KSBSB VT#2)0& :0-:i8; "!$#%& :9F:9:$!C><*${G:&|7$!\3Hx$*1*1ER*1@ @ >1K{`_@~{(@ 1@ r* . ,@ @ r~ {YM{`"_@@!"_ *-* f@ , , K1 {(@ Y^X@ YF@c m0`A SBSB N8;#%C:91!G?E!E"#%! %3 :,'O:9"*67!E#2Z'" 32:* AB#%0*(N&!$CRcB0#%P#\JSB7J$JBVR$0V:9:$?9:":C%#:;82#ffN1/38;#\C-:9J!E)!$#\60: EG?E!E"#%! 32:/'O:9"*k7-!E#2k' 32:* B#%0Rcd0#%*(1/#%DSB V7-D-$rSB R'@ >1K{`_,*-?c_ *_@ @ , @ @ >1K{`~{`fififfv2$ff6ff )T$fiffv~ffwsxQs , xtw {cd0-::j:732V:;p@ :C} +*67432#2'432:b'O:9"*k7-!E#2l' 32:*,D7C:9V!+#%*'43%:r: "#%8;#2W04!Ed0-:' 32:* #%u v@ :9)#27yZ!$3%"0ffNMRxcd04!E6#%9Nw!$&5Z "#2'43%:$<D?!E"#\! 32:AB0#\8"0x!E:,!$3%3 G C# :9:&r*k7 K@89897-K $)$:90-:91#%X!Ej32:!$ K-:H'O:9"*k7-!E#2ffR.o$j:;p@!$*'32:$Nff0-:0-:9:8; "!$#%&P!$3\3 G C#ff!E:/-$^ M#%!$-)32:*(/ MNE!$3%3 G C4#*13 MNE!$C.!$3%32G C#(1'::9?[#\-)d!$ * N ( !$CE!:$!%323GC#9::g7PE!V:$J%#j0V:$!*:;8"$!%#&9Rwcd0-:(<>3%32 AB#%-)10-:9$:*h8;3%32:8;D $)$:90:9b!$Ci)$:-:9"!$3%# 9:B*,!$g5l$<J0-:.'":9?[#274V:732RlN*PIQ KJ [WPY[OL4[4P rQTJ- @ ({ @ {(@M@ :@sxu, * -/, *! ,* - ,*! , * - , *! , * -/, *! , * -/, *! , * -/, *.fifi! , * -!Qs e- !ff- !fi,*,* .. .-! , * - !7 ff,* - 77 ,* - 7 -- @ {(@ {(@fig @ {(@ {(@,* - 7,* - 777 fffi1:07070$JQul]Ht[vN*QkWv [SW [OPb]ktMNu7N*QVHKJW PdH>Z9v [4]ff@?.-- .,*,* . ! ,* ! ! ,*ffg0-:V#%-)3%:/'O:9"*k7-!E#2H89!$ :$R ^@89!$3cd0:'"&$<=w3%#2<> #%k!r "!$#2)0& <a$AV!E"CH*,!$:9J<>*8;#\ :89#2:]3%#2L$:iSB VbN @SBKN (z !$C8;#%C:9, "#2'43%:H$<j?!E"#\! 32:9R zF<K0-: :x!E:3%#%L$:Cl $)$:90-:9N A:k7 :j0-:.<=!$8;B04!EB0-:.'4 32:* #%d "#%!$-)32:1': :9?@#%-)!$C}!,'O:9"*k7-!E#2#%0:9:9<a$":mC-:9_4:C `?$:9l0-:*WR z <b0: :m!E:x-$+3%#\-L$:C $)$:90-:9NBA:x89!$ C-:8;*'O :m0-:!E)7*,:g1#\g }SB Z'4!$#2MK$<?E!E"#%! 32:9Ry #20-7-j "#\!$-)32:'::9?!E#%ffN KSB NJ*,!5X3%5!$8"04#2:9?$:j!$0#%)0+!k32:9?$:34$< 8;#% :48;5,!$SBR(o$:;p-!$*'43%:$N[8;"#%C-:9!E)!$#\+0-:K-@G M#%!$-)32:'::9?[#\-),8; "!$#\gr#%W0-:13\!$ d'4!E"!E)$M!E'40ffRzF< *!$C(1/3l0-:Q0-:,'4 32:*#% jSB N 7-j#2.#\.-$VNP!$CZ0:8;:+-:#20:9 (zN @SB-$BSBR '_>1K{`~{`w__q_, * M@ @ @ >1K{`2 _ , *,*FRfi?7@ @ @ %Y9_#, * g,*Sb-$0-:9} 3\7-#2 "!E :9)$5 #%l :48;[C-:Q'O:9"*k7-!E#2 '4 32:*,W#\g @Sc !$4C 7 :Q! <=!$K!?@#%{G 7-!$*l$i32[89!$3j:!E"8"0 '@8;:C7-:$R o-$i:;p-!$*'432:$N : !E}!$C ~m!$g5!:9'O$'"*,#%#%)r:"732w<>$/'$'O#%#2!$3[:8;@C#%-)w$<O74CH #%H'" 32:*,9N$Ar0#%8"0#\893%7C-:'O:9"*k7-!E#2i8; "!$#%&9RVyQ:68;#\C-:9B0-:9: 7 C#2:8; ,:8;[C4#%-)d#%& -Sc !$d0: :60!`?$:MR(Sb1!$32 :9"4!E#2?$:/!$C1'*,#%"#%-)V:8;@C#%-):9:17:C.* J8;*,*325j#%.0-:('4!$ >yZ!$3%0ffN$<d /j#\g @Sc #\K0-: 7-''O$K:8;@C#%-) @R d:8;:&325$N b:gj0!$."0-`ABX04!E.7#2'$'!E)!E#26#\j0-:"7-''O$ff:8;@C#%-)r#%^:|[7#2?E!$32:&^ r:<a$"89#\-)B!E"8 G 8;"#% :8;5.#%10-:/$M#2)#%!$3JNJ!$CY04#%689!$ :]!$8M0#2:9?$:C #% !$ 5@*' $#%89!$3%3%5Q$'#%*,!$3/#%*:$RXcPX8;*'4!E:f0-:l7-''O$:8;@C#%-).$<D0-:+C# O:9:&1*@C-:3%j$<d!i' :9M*67-!E#%Z' 32:*WN^A:]#\*'4325x-:9:CZ0-:9":9<a$:]32[$L]!ED7-D":732W!E"8 G 8;"#% :8;5$R(y #20l0-:jC#%:8;V:8;[C4#%-)-N-7#2'$'4!E)!E#%W:-<>$"8;:!j32:9?$:3-$< 32@89!$3-8;#% :48;5k32:"w0!$,!E"8 G 8;"#% :8;5$Rz{C-:9:CffN$0:d32:9?$:3@$< 8;4#% :8;5H#%w$<> :#%C-:&#%89!$3O ,04!EB!$8"0#%:9?$:C 5f0:j<a$AV!E"Ci8M0-:8L@#%-)]!$32)$$"#20*iRx}_Sj_Sj_*x_) 5XNX5N$K37373>;P!PVK37373>,cVK373$$fi 8-fiffffSM_|aB)z{+0:rC#2":8;/:8;@C#%-)1$<^!H n#%& -ScjN&A:r04!?$:j! [32:!$f?!E"#\! 32:rAB0#%8M0+#%# i0-:r'"#%*+!$3-?!EM#%! 32: ^!EL$:0-:b?!$3%7: -Rz{]0-:B'M#%*,!$3 @Sc *@C-:3 N&0-:9:K!E: x893%!$7 :1 B893\!$7 :b l:7-:60!Ej-:7:10!EK:!$8M0Q'"#%*+!$3^?E!E"#%! 32:6!EL$:j!Ej32:!$ K-:k?!$3%7:$N1 r893%!$7:r l:"7-:k0!Ej-]FAW'4"#%*,!$3P?!E"#\! 32:'"#\*,!$3^?E!E"#%! 3%:1)$:9jFAW?E!$3%7-:9NO!$C!EL$:x0-:}!$*:}?!$3%7:$R z{g :9:#%-)325 0-:}8M0!$-:3%3\#%-) @Sc *@C-:3d04!$,0-:}"!$*:}[7* :9,$<i:9'4: :& $0X0-: 0&3%:!$Q?!EM#%! 32:K!$j0-:H'M#%*,!$3 @Sc *@C-:3 =!$jA:,89!$Z7 :?E!$3%7-:x$<10-:x'"#%*+!$3B?E!E"#%! 3%:lu sx0-: 0 ?E!$3%7-:x<>$f0:XC47!$3B?E!E"#%! 32: MNK!$C 71 d893%!$7 :B ]:47-:!$CC#%#2!$3^893%!$74 :d f:7-":.:!$8"0XC7!$3^?!E"#\! 32:.!EL$:K!,?E!$3%7-:$RdcB0-:0!E(-.C47!$3@?!EM#%! 32:)$:9(FA6?!$3%7:!E:D:|&74#2?!$3%:gw .0-:d893\!$7 :0!E:47-:D-jFA.'4"#%*,!$3?E!E"#%! 32:)$:9V0-:K!$*:b?!$3%7:$RwcB0-:r<>3%32 AB#%-)1:7320- 0!EV~mSB #%#2)0& :904!$JN-!$C#\V:|&7#%?!$32:&D ]oJ W0-::1C# O:9:&d*[C:3%9Rz{WAB0!ED<>3%32 AB9N-A:1!$7*,:.0!Ed0-:6o!$32)$$"#%0*q74 :!6<=!$#%3_M 0-:7"#% #%8V04!E#%4 !$g#\!E :?E!E"#%! 32:(AB#%0+#%)32:B?E!$3%7-:32:9<aV#%+0-:#2C-*,!$#\ :9<a$:j?E!E"#%! 32:VAB#20W!,8M0-#%8;:.$<w?E!$3%7-: b!E"!$3%#%8"L/3%3\#2$9NMR@B+Ro)-@B KJx_'zx 7fi ff!cx_6?f, * -/, * 7!ff, * - , *! $ , * -/, *fi fffi ffQs ex_1:07.K3>JXu9WM[4PI]HtMNu7N*QVHKJWPYH>Z9v [4]9|5 co] ru( B+Ro9*esRo,* 7,*, * - 77fi, *7 $ ,*- !7ff7 $, *!fi, *! $ ,*1O,* !MK37373>,*#%!6 'O:89#%!$389!$ :b$<Pcd0-:9$:*>yZ!$3%0ffNMN-AB0#%3\ /~}SB#%D!,' :89#\!$3ff89!$ :1$<wcd0-:9$:*[Rc +0-7-''O :K7#2V'"$'4!E)!E#2i :9B!3%#2 :9"!$3 R(cd0-:9":.!E:j<a7d89!$ :9R/z{0-:K_"V89!$:$N!893%!$7 :b$< 0:b<>$"*#+04!$ :9:f:C478;:Cf ,!$W7#29Rcd0!ED#%N&A:* """0!`?$:1-:.?E!$3%7-:632:9<aB<a$B!+'"#%*+!$3O?!E"#\! 32:$R/cd0:j<>!$#%3^_" d0-:7"#% #%8K#%}o '4#\8L@d0#%B3%!$ D?E!$3%7-:X#% !$&#%!E :$RWz{Z0-:f :8;CY89!$:$N(!}893%!$7:+$<D0-:]<a$"*f<a$Z04!$ :9::C748;:CH 1!$,7#%9RJcd04#%J:7-:w04!E/-j'"#\*,!$3[?!E"#\! 32:V)$:9(FA.?E!$3%7-:RwcB0-:Do !$32)$$M#20*"#2?@#%!$3%3%5.-:9?$:9 "#2:wFA1"#%*67432!$-:97J?E!$3%7-:w<>$(!K'"#%*,!$3&?E!E"#%! 32:$RJz{60:D0#2"Ck89!$ :$N&!j893\!$7 :$<w0-:.<>$"*W04!$ :9:W:C78;:C} +!$}74#29R/cd04#%V:7-":D0!EB-]C7!$3<>$?E!E"#%! 32:])$:9k Ax?E!$3%7-:RxSr)!$#%ffNw0-:lo q!$32)$$"#204* "#%?[#%!$3\325x:9?$:9k M#2:1FAQ#\*673%!$-:97# 04!$ :9:l:C78;:Cl +!$}7#29R?E!$3%7-:V<>$B!,C7!$3 ?E!E"#%! 3%:$Rz l0-:j<>7-0W89!$:$N* """cd0!E6#%9NPA:+0!`?$:]:,?!$3\7-:+32:9<>j<a$k!}C74!$3w?!E"#\! 32:$R]ST<=!$#%3_M .0-:7-"#\ #%8H#%Zo '4#%8"L[.0#%3%!$ ?E!$3%7-:l Z#%4 !$g#\!E :$R r:8;:$NV)#2?$: !Z"7#2! 32: "!$48"0#%)x0-:7"#% #%8EN/0-:io !$32)$$M#20*"!$8"L[b0-:!$32)$$M#20*WRdc ]0`A 0-:1:9?$:9M :$Nff7-''O :j<>$AV!E"Cm8M0-:8L@#%-)f:* ?$:K!,?E!$3%7-:$Rcd0-:9":b!E:KFA,89!$ :R(z f0-:K_M 89!$:$N-0-:K?!$3%7: (#%:* ?$:CW<>* !kC47!$34?E!E"#%! 32: KC7-:b*:.8M0!$-:3\3%#%-)k8; "!$#%&9R/cd04#%V*:!$V0!ED0-:9":.#%D!k'"#%*,!$3 ?E!E"#%! 3%: .AB0#\8"0l0!$ :9::9K W *:k?!$3\7-: R4#2B'"$'4!E)!E#2x:9 l<=!$3% :$Nff!$Cm0-:x:9 H<>!$3\ :b!$:|[7#2":CffR z{]0-:b:8;Cl89!$ :$N-0-:b?!$3%7: #%:*`?$:Cl<>* !C7!$3P?!EM#%! 32: $NO!E)!$#%XC7-:6 W!]8M0!$-:3%3\#%-)+8; "!$#%&9Rbcd0:1'[$<(#%b-`A C74!$3^ l0-:k_"89!$ :$Rc Z0- ~}SBN(A:i74 :l0-:l<>!$8;,04!E~mSB C-*+#%!E :o !$4C oJRc +0- "#\8;-:9N48;4#%C-:9d! G?!E"#\! 32:j'O:9"*k7-!E#2W'" 32:* AB#20}!$CC#2#24!$3 #%!E5f8;@G"!$#\gl0!E}"732:x7-W0-:Z!$*,:Q?!$3%7:x<>$}!$3%3'4"#%*,!$3r?!EM#%! 32:9R /<a$"89#\-) SB 0-:ax_ - 7-^1:"aBaBB'aB+^Sz G+^]emxB_BB ^ | GzB+^|- ! x_ -c+^+^%@0^]z- 7 x_ -9!ffififfv2$ff6ff )T$fiffv~ff8"04!$-:3%3%#\-),8; "!$#\gb89!$7 :K!fC-*,!$#%}AB#2'O:97-dm0-:6C47!$3^?E!E"#%! 3%:1!$ @89#%!E :CxAB#20}0#%?E!$3%7-:$R(Sbd0-:9:.!E":1-,7#%V893\!$7 :9NC-&:B-$B#%*,*,:C#%!E :325+32?$:.0-:.'4 32:*WRx_ - ! x_x_,*+c ]0`ANA:6$ :.0!EB0:18"0!$4-:3%3%#%) -Sc *@C-:3^8;&!$#%r*$:1893%!$74 :9Rr:8;:$N(#26C*,#%!E :10-:]'4"#%*,!$3 -Scq*@C-:3 Ric m"0-`A "#%8;:9N8;#%C:96!m<a7-6?!EM#%! 32:'O:9"*k7-!E#2m' 32:*hAB#20m0-:9:H!$CC#%#2!$3 #%!E5W8; M!$#%gK04!Eb#2< *k0-:[N(f$!CfE!:$!%33"7%3:C796R4%#C9:"$!80%#])ER%#r'"$4'E!)E!2#x1/*$0}*@C-:3%B :9* ( N(( N1( N/5( N(!* N1!* !$C/* +<=!$3% :$R Ki0:68"04!$-:3%3%#\-) @Sc*@C-:3 N@7#2'$'4!E)!E#%f!E)!$#\ 0-:j893%!$7 :* (((1(/5( 0-:f)$:-:9M!E :D!$l:*' 5893%!$7:$R 5f8;*'!E"#% ffN74#2V'$'4!E)!E#%Wi0-:j'4"#%*,!$3 @Sc *,[C-:3ffC&:r-,*$:jA$L R'e@@nq)0Q_$e = Qg-=@ 1@ 1v@cd0-:1':9?@#27D:"732D :3%3 7d-$04#%-)+! 7-d0:.:3%!E#2?$:k8; B$<(!$8M0#2:9?@#%-),0: :632@89!$3^8;"#%{G/:89#2:RmSb 5[*,' $#%8+!$4!$325["#%1!$CC46C-:9!$#\3( x0-:]":7329RWyQ:W89!$ !$8"0#%:9?$: jSB#%#%*: :9 )#%^NMRnSB #%!E"5Z8; "!$#%&H89!$ :]!$8"04#2:9?$:C #% ( .AB0-:9: l#%k0-:( 8"04!$-:3%3%#\-)Q8;@G[7* :9,$<.8;4 "!$#%&+!$C#%,0-:#%+C-*,!$#% # 9:$R Sb,0-:9:x!E:/"!$#\g9N(SB!$#%?$:325Z!EL$:.#%*:$R r`A:9?$:9N 5Z!EL@#%-)X!$C-?E!$g!E)$:}$<d0-:l<>748;#2!$31 j7#\-)W0-:]SBG }!$32)$$M#20*!E7-":,$<d8"0!$4-:3%3%#%)i8; "!$#%&9NJA:f89!$Y":C78;:,0#\./ J#%*:d!$/0-:9:B!E:(r:g :-"5[8"LN b:9?[#\3%32:$Nc :-)-NMRSB!$3% .4!$#2?$:325k!EL$:#%4!E5}-${G:|[7!$3%.8;4 "!$#%&9R B A:9?$:9`N A:,89!$Q!EL$:+!$C-?E!$&!E)$:+$<0-:, 'O:89#%!$3w!E7-:$<V!( !$V:!$8M0}-${G:|[7!$3%D8; "!$#%&V:9:CV#%4!E5,-${G:|[7!$3%d8; "!$#%& ":C78;:K0#%7r8;:$R6yQ:,0!?$:'4`?$:CX0!E jSBSBSB!$CX)$:!E :9.'4"7#%):k*+!$C-:SB/ MN1 MN( : 'O:8;#2?$:3%5 MRcd0[7'O`A:9/#%w: 4:8; :C+#%0#%)0-:9wA$"/89!$ :B8;*'32:;p@#% 5A:6 #%3\3^-:9:Cm +"7}:;p['O:9"#\*:gB ] :9:H#2<w0-:k!$CC#2#24!$3ff'"74#%-)H7- A:#2)0r0-:6' $ :&#%!$3%3%50#2)0:9d8; 9R61:0706]-*ee1:0707$/e,*?Mx=SsRoOLP Q R$g = RwsRosRo+Ro*5? =0QTg+Ro+ ]sRo- !!"*+Ro sRo +Ro ,+RoyQ:f"!$Y!WAr#%C-:?E!E"#2:9 5Q$<D:;p@'O:9"#%*:&j m:;p['43%$:,0-:f#2)#2_89!$8;:$<D0-: :]0-:9$:9#%89!$3!$C!$ 5@*' $#%8mC# O:9:8;:9R o-$l:;p@!$*'32:$ND:9?$: 0-7-)0 #%!E5 -${G:|[7!$3%f8; "!$#\glC- 32:'"74#%-) 0!$ 0:Y8"0!$4-:3%3%#%) 8;4 "!$#%&9N.0-:95 *,#%)0g}#%3%3j 'O:9:C 7-' :!E"8"0 5 '4"7#%)|[7#%8L$:9`RYyQ:}3%#\*,#210:W_" H :9$<B:;p@'O:9"#%*:&H Z!X !E#%8l?E!E"#%! 3%:f!$C ?E!$3%7-:l$"C:9"#%-)X!$A:BAB#%"0H 68;-_M* 0-:d0-:9$":9#%89!$3:"7329Ng!$4C,0-: :r!E:r3%#\*,#2 :Ck:#20-:9 6 !E#%8d$"C-:9"#%)$6!1: "#\8; :C]893%!$"($<^C-5@!$*,#%8V?E!E"#%! 32:B!$C+?!$3%7:d$"C-:9"#\-)(#%Ar0#%8"0+A:b*,!EL$: {:|&7#%?!$32:&MR"!$48"0#%)HC:89#%#2D#\l0-:.C# O:9:&d :!E"8"0} :9: V!$898M0&7r:9B!$3 R2N*)NK373$$PSbb:;p['43\!$#%-:C :9<>$:$N *+!$g5}8;4 "!$#%&B [32L@#2r"7-''O$B8M0!$-:3%3\#%-)AB#20m:9e+89#2:&b)32 !$38; M!$#%gRJo-$J:;p-!$*'432:$Nz@32?$:9J04!$^0-: fffi ffD8; "!$#%&9NE!$4C.0-: @#%89 7^_4#2 :C-*,!$#\m8; "!$#%&r3%# "!E5W0!$r0-: ffff ]'4:C#%89!E :$R $0m'O:9<>$"*h!]32:9?$:3J$<('4"7#%)AB0#\8"0 !E''O:!E"+ :i:|[7#2?E!$32:&, Y:-<>$"89#%)YSB 0-:}:;p@'43%#\89#28"0!$:3%3%#%-)Z8;4 "!$#%&9RyQ:j0-:9:9<>$:.8;*'!E:Cl0#%V#%]7-D:;p@' :9M#%*:& SB l0: #%!E5,-${G:|[7!$3%D8;4 "!$#%&!$C KSB x0-:!$3%3 G C4# :9":gb8; "!$#%&9R1Sb3%3 0-:k*,[C-:3\r!E:#%*,'432:*:& :Cm#\ @32?$:9 [R @N!$C !E:X!?E!$#%3%! 3%:m?@#%!# R yQ:X3%:;p@#%8;$)$M!E'40#%89!$3%3%5 $MC-:9l0-:X?!E"#\! 32:+!$4C !$"#2) 0-:?E!$3%7-:1#\Z&74*:9"#%89!$3$MC-:9RlyQ:]0:9:9<a$":+325 M!$8"0YZ'"#\*,!$3?E!E"#%! 32:RfSb.A:+ :9?$:?$:95 "#%*,#%3%!E":732+ !Y"!$-)$:x$<j'O:9"*67!E#2 '4 32:*,NA:}4325 0- 0-:9":}:7432,<>$P!$)$<a$"C D' 3%:*WREv2)Sj_q49" kK373fi 8-fiffff*@C-:3E< =cJ! 32:0-:7-"#%#%8!E#%8!E#%8!E#%8!E#%8!E#%8!E#%8!E#%8!E#%8!E#%8!E#%8!E#%8<=!$#%3%nV[Nk0$<>!$#\3%3@Ri373$>.7>>7"3@Ri373$"3@Ri373$3@Ri373$"3@Ri373$"3@Ri373>,3@Ri373$nV[N\143>:8ER:8EReW3@Ri373$"3@Ri3M1713@Ri3M1:3@Ri3M1713@Ri373$03@Ri373$03@Ri3M1:"3@Ri3M1713@Ri3M1:3@Ri373$0>7* :9V$< !$8L[ "!$8L@ ><=!$#%3% /!$Cf"74#%-).#\*:r _C+0:r_M 3%7-#2f H A,#%@G!$48;: $< !$-)$<>$"C ' 32:*iR B7&#%*: !E:<a$ z@32?$:9 [R bk!~$NJ:&#%7* zz"z/'@8;: $`N!$C+~$< BSK~ZR_*[C:30-:7-"#%#%8!E#%8!E#%8!E#%8!E#%8!E#%8!E#%8!E#%8!E#%8!E#%8!E#%8!E#%8T< =cJ! 32:V[Nk0$ffff<=!$#%3%77.7<=!$#%3%ffffff3@Rk7.3@Rk"703@R 13@R 13@Rk73@Rk73@Rk"73@R >3@Rk&7&3@Rk70V[N\1718:8ER<>!$#\3%1ER >"[R\1:"[Rk7&[Rk7"1 Rw,KE1 Rw,K&E[Rkj1Rk7.[R >0[Ri3$0[7" &7761:&$,K&76"7&776"7&7762effEv2#FnV[N\143>:8ER77.7&K3$&777.777.7X"j1: !)71 1:77&70K3M1:.171:77&171:77&171:77&ff:8ER, R 1&K3@Rk"7017E1 Rk0j117E1 Rk06. Rk&j1[. Rw,7,[1:[. Rkj11:[ Rk71:[ Rk0714@3 Rk"7&"7&7761:K373 !eKE" kK373nV[N\1:$ff<>!$#%3\j1:7077&06>07.$,K.j1:7077&j1:7077&ffffj1:7077&ffffff:8ERff1ERk.701:j1ERi3K,K[ Rk.7",K[ Rk"K@3 Rk"7"jE1 R 1171O- R 7,K[& Rw,7,,K[. Rk07"&7[ R >07 * :9D$< !$8"L& M!$8L@ ><=!$#%3% !$Ci"7#\-)1#%*,:j ,_4CW!$3%3ff3%7-#2N[$B'`?$:10!E0-:9":6!E:k-+ 3%7#29N ]<a7r#%!$8;:d$< !$-)$<a$MC d'" 32:*WR B7&#%*:B!E:1<>$z[32?$:9 [R ]~$N J:g#%74* zzz'@8;: $N!$C,~$< rSb~YRv2-" kK373 1:K373 !eK @_)!"j1: X)fififfv2$ff6ff )T$fiffv~ffz{ cJ! 32: [NdA:X8;*'4!E":m0-:m?!EM#27+*,[C-:3\,$<.!n' :9M*67-!E#% AB0-: _4C#\-)Q0-:x_"3%7#2Q iFAx#% !$8;:1$< P!$-)$<>$"C 1' 32:*iRz nc ! 3%: -N A:]8;*'!E:,0-:+"!$*:+*@C@G:3%jAr0-:X_4C#\-)f!$3\3( 3%7-#%K$.' ?[#\-)W0!E.0-:9":,!E:+-i 3\7-#29NP<a$.<>7-1#\ !$8;:j$<P!$)$<a$"C d' 32:*WR K325[N r!$C[N r#%}0#%D! 3%:60!`?$:k!$&5i 3%7-#%9RDcd0-:.:;p@'O:9 G#%*:&!$3:73268;-_"* 7-k0-:9$:9#%89!$3_4C4#%-)9RWow#2" 9NJ:-<>$"89#%-) KSB n!$ !$3%3 G C4# :9":g8; M!$#%gC-[:/0-:K* '"74#%-)-N$AB04#%3% :-<>$"89#%)6SB ]0: #%4!E5-${G:|&74!$3%8;4 "!$#%&C-[:.0-:]32:!$ 9Nw!$CQ:-<>$"89#%-)iSB Z0-:]8"0!$:3%3%#%-)W8; "!$#%&1#%.#\ :9 A:9:ffR r7&#%*:!E:,#\*,#%3%!E"3%5f$MC-:9:CffR @:8;CffNP!$CC4#%-)+0:'"#%*,!$3 $.C7!$3 #\!E5}-${G:|[7!$3%j8;4 "!$#%&K0-:.8M0!$-:3%3\#%-)68;4 "!$#%&DC-[:D-$ "#%)k!$&5f*,$:K'"7#\-)-N&!$CW*,:9:325]!$C4CV`?$:9"0:!$CW0-:k"7&#%*:$Rbcd0#2"C^N4!$CC#%)+:;p@ "!]8;4 "!$#%&b f0:6'M#%*,!$3^$jC74!$3P!$3%3 G C4# :9":gb8; "!$#%&!$8"04#2:9?$:j0-:,!$*,:H!$*,7gj$<'"74#%-)+!$j0:H!$3\3 G C# O:9:&b8; "!$#%&bQ#%K`ABffN^!$4CQ!E)!$#%7 d!$CCV ?$:9"0-:!$Cm ,0-:.M7g#\*:$RvnV k0$/!nV \143>55wg?R@=7? RRg $gcd0-:6:;p['O:9"#\*:g!$3P:73%B#%}0-:63%!$r :8;#2X*,#%)0gb :9:* l0!`?$:H:9 32:C}0:6*,!E :9j$<(0-+*@C-:3O'O:9"*k7-!E#2l'" 32:*,9R -<a$M89#%-) KSB }!,#%-)3%:K!$3\3 G C# O:9:&V8;4 "!$#%&d!$32AV!5@)!?$:]0-:+*+!$3%32: j :!E"8M0Q :9:6!$CX"7&#%*:9R r`A:9?$:9NJ0#%j#2)-$":.!W#2)#%_489!$gK'O$ :&#%!$3!$C-?E!$g!E)$:b$<ff8"0!$4-:3%3%#%)K#\g 1!1C7!$3*@C-:3 R b5[4!$*,#%8D?E!E"#%! 3%:V!$4C,?E!$3%7-:D$"C-:9M#%-).0-:7-M#% #%89*,!`5 :f! 3%:+ x:;p['43%#210-:f'"#\*,!$3/!$C C7!$3?@#2:9Ad'O#%&j$<B!x'O:9"*67!E#2Y X*,!EL$: :9 :9C-:89#%"#29R cd0#\,#%+$]!Z $'#%8i0!El89!$ :}:!$#%325 !$CC-": :C 0:9$:9#%89!$3%325$R B A:9?$:9Nr0-::;p@' :9M#%*:&!$3P:"732r)#2?$:X#%}04#%r:8;#2Q0- 04!Eb?E!E"#%! 32:6!$Cm?E!$3%7-:k$"C-:9"#%)]0-:7-M#% #%8989!$i'4$_V)$:!E325l<>* *k732#2'43%:r?@#2:9Ad'O#%&9R?E!E"#%! 32:j$MC-:9"#%-),0:7-"#% #\8b3\#2L$:.*,!$3%3%: BC-*,!$#\i#%d7"7!$3%325 7 #%_:Ci#%l :9"*+d$<(!<=!$#%3 G_"'"#%89#%'432: ffA:B0!?$:b .'4#\8Lk:9?$:g7!$3\325!$3%3-0-:d?E!E"#%! 32:9Ng 6#2(#%AB#\ :D 18"0& :r-:d0!E#%0!E"C] !$#2)^N&)#2?@#%-)H70-$'O:9<>743%325H*678M0W8; "!$#\g'$'4!E)!E#%f!$4Cf!k*,!$3\3 :!EM8"0l :9:$R/S?E!$3%7-:1$MC-:9"#%-)]0-:7-"#%#%8j3%#2L$:1*+!p@#%*k7* '*,#%: b:9:32:ffNr#%r77!$3%3%5 7 #2_4:Ci#%i :9M*,$< !"7898;:9:C@G_"'4"#%89#2'32: ^A:j'4#%8L]!H?!$3%7:b3%#%L$:325, 32:!$Cl ,!H 3%7-#%ffN[:C789#%)10-:K"#%L$< !$8"L& M!$8L@#%-)!$C] 5[#\-)1-:b$<^0-:K!$32 :9"4!E#2?$:K?!$3%7:9Rwz f!1'O:9"*k7-!E#2]' 32:*WNgA:j89!$"!$48"0QQ0:,'"#%*+!$3w$.0-:+C74!$3w?!E"#\! 32:K$1 $0^R,yQ:]04!$3%30- 0-:9:0!E6<>!$#\3 G_"Q-:?@#2:9Ad'O#%&K#%j8;*'4!E# 32:AB#20X7898;:9:C-G_" jQ0:,C7!$3 Rc iC-i -N^A:+8;#\C-:9j0-:<>3%32`Ar#%-)0-:7-"#\ #%899RgfEx/ee5<l<g1:0707$95fffi<d8M0-&:k0:6'M#%*,!$3ff$K0-:HC7!$3 ?!EM#%! 32:.Ar#20}0:k*+!$3%32:xtvw{C*,!$#%ffN!$C}8"0-[ :.0-:j?E!$3%7-:B#%W&74*:9"#%8b$"C-:9`Raxt2'Qsffw'xt2x<w8"0-[ :d0-:D'4"#%*,!$3&?!EM#%! 32:VAB#20H0:d*,!$3%32:wC*,!$#%ffNt2'x t2w'{!$4Ci8"0-[ :.0:.?!$3\7-:d#%W[7*:9"#\8r$MC-:9Rff<8M0-&:d0-:BC74!$3@?!E"#\! 32:DAB#20H0:B*,!$3%3%: (C-*,!$#%^Ng!$Cx t2w'{8M0-[ :10-:j?E!$3%7-:B#%W&74*:9"#%8b$"C-:9`Rt2)xtvxt2'( fi<8"0-[ :B0:r'M#%*,!$3C74!$3@?!E"#\!32:DAB#20+0-:r"*,!$3%3 G:(C-*,!$#%ffN$!$C8M0-[ :D0-:V?!$3\7-:Ar0- :DC7!$3 '4"#%*,!$3g?!E"#\! 32:V0!$w0-:D*,!$3%3%: JC*,!$#%ffR,Qsffw'xt2xt2't2w'{( <D8M0-&:60-:6'"#%*+!$3O?!E"#\!32:.AB#%0W0-:6"*,!$3%3 G:BC-*,!$#%^N!$CW8"0-[ :60-:j?!$3\7-:jAB0- :.C47!$3O?!EM#%! 32:.0!$d0-:.*,!$3%3%: dC-*,!$#%^Rlxxt2w'{:fi 8-fiffff$t2ff.<(8"0-[ :Q0-:QC7!$3r?E!E"#%! 32:xAB#20 0-:Q*+!$3%32:x t2',xt2w'{ )C*,!$#%ffN!$C}8"0-[ :.0-:j?E!$3%7-:jAr0- :.'M#%*,!$3 ?!EM#%! 32:j0!$D0:.*,!$3%32:dC-*,!$#%ffRc 0:k*+!$3%32: bC-*,!$#%x0-:7-"#\ #%8.x0-:HC7!$3J0!$ :9:X7 :Cx!$j!+?E!$3%7-:k$"C-:9"#\-)]0-:7"#% #%8#%f!6[7* :9$< :;p['O:9"#%*,:g!$3 7C#%: $7"C!$ffNV0-:)6:9D!$3 R2N@*,#%0ffNMRcd0-:<>3%32`Ar#%-)]!E")7*:&b0`ABb0!EK0-:HC-7 3%:6*,!$3\32: rC*,!$#%x0-:7-"#%#%89B!E:8;*'!E# 32:6AB#200-:b<>!$#\34_M /'M#%89#2'43%:<>$V?!EM#%! 32:r$"C-:9"#%)1!$Cf7898;:9:Cl_"/<>$?E!$3%7-:b$"C-:9"#\-)-R -7-''O :dA:!$#%)W0-:.'M#%*,!$3 ?!$3\7-: , +0-:.'"#\*,!$3 ?!E"#\! 32:=!$}!$4!$32$)$7B!E")7*:&B89!$ :K)#2?$:}#%<A: "!$48"0mm!]C7!$3ff?E!E"#%! 3%: MRr "!$#\gd'4$'4!E)!E#2mAB#%3%3O'4"7-:.0:1'"#\*,!$3ff?E!$3%7-: ]<>*0-:k$0-:9K'"#%*+!$3ff?!EM#%! 32:9N !$Cx0-:6C47!$3P?E!$3%7-: V<a*h0-:k$0-:9jC7!$3P?!E"#\! 32:9Rj "!$#\g'$'!E)!E#2 *,!`5 CY*$:W0!$ 0#%,#2<rA:m0!?$:x!$ !$3%3 G C# :9:&8; "!$#\g,$]8"0!$:3%3%#%-)8; M!$#%gR r`A:9?$:9NV Z!m_4" !E''`p@#\*,!E#2ffN0#\k#\!m":!$ ! 32:W !E"#%-)X'O#%g9Rncd0-:7898;:9:C _M ,?E!$3%7-:}$MC-:9"#%-)n0-:7-"#%#%8i8;*'47 :+0-: {'*+#% : Z$<j0:xC# :9:&,?E!$3%7-: 5*k732#2'43%5[#%)6 $)$:90-:9j0:6C-*,!$#\}# 9:r$<0-:k7#%!$g#%!E :Ci?!E"#\! 32: K:9:32:ffNMRbSbg5:9"* #%i04#%d'"[C748;B#%B748"0!$)$:Cm#2< f$ NOC-:9'O:C#%-)mAB0-:90:9B0#%r#%b!,'"#\*,!$3ff$KC7!$3?E!E"#%! 32:$N-C-[:d-$D@89897-B#\]0:.C-*,!$#%W!$C}#%V:C78;:C 5 j#2< ,$ (@89897-"9R/cB0-:K'@C78;D#%3%#2L$:3%51 :B*+!p@#%*+# 9:C 5H:"7-"#%-)KA:d:C748;:B!$(<>:9A :9"*+/!$('O"# 32:$RJcd0!E/#%N 5H:"7-"#%-)l!$C V[89897b#%x!$b<a:9A C-*+!$#%B!$b'O# 32:$RBcd04!Eb#% H!$C0!?$:H0-:H*,!$3%3%: rC-*+!$#%'O# 32:$R r:8;:C-7 3%:6*,!$3\32: rC*,!$#%xAB#%3%3 "!$8M0mx0-:H?E!E"#%! 3%:1AB#20x*,!$3\32: rC*,!$#%!$CW :4CW +!$#%)i#2D0-:j?E!$3%7-:jAB#%0W* d'"*,#% :$RyQ:K-`A 8;*'4!E":r0-::b0-:7"#% #%89/#%,!$]:;p[ :"#2?$:b:9/$<^:;p@'O:9"#%*:&9Rwcd0-:r0&5[' $0:#%A:AB#%"0m } : 1#%K0!E "!$8M0#%-)W0-:7-M#% #%89K89!$Q'"$_K<a* *673%#2'432:k?[#%:9Ad'O#%gR6yQ:+74 :0-:<>3%32`Ar#%-)8;3%32:8;#2i$< 'O:9"*k7-!E#2W'4 32:*,D#\W!$CC#2#2f!$-)$<>$"C D'" 32:*@1:0707">|@1:07070>g@Br|ze|zNP| Bz'1%]eK+' _'~K37373>E1:0707$| Sz@B<K'A'Bw {}l x Sb.$"C:9 T|&7!$"#2)$7-'1#\P! P!E#%6|&74!E:/$<# 9: QN0!E#\9Ng!$*k732#%'43%#%89!E#2k! 3%:B#%AB04#%8"0H:!$8M0]:3%:*:g/[89897-M/#%:9?$:9"5`A !$C:9?$:958;3\7*,ffRj7!$#2)$7'm:;p-#% :8;:'4 32:*,bC-:9 :9"*+#%-:k0-::;p@#\ :8;:H$.-@G:;p-#% :8;:$<|[7!$#%)$7-'4V$<!H)#2?$:m# 9:jAB#20W!$C4C#2#2!$3 '"$' :9"#2:t2w 2'! <C-:$ :B|[7!$#2)$7'4$<J$MC-:9 'Bfi !' < C-:$ :B|[7!$#2)$7'4$<J$MC-:9 '=<)ff4 )BA)fijAY 4R<>$dAB0#%8M0*)jM)B )OAY 4R<>$dAB0#%8M0`)Y}<>$.:9?$:95yQ:f!$CC#2#%!$3%325iC-:*+!$CQ0!E.0:+|&74!$#2)$7-'Q#%1#%C-:*'O$ :&9NP# R :$R:3%:*:g4R/cB0-:j' 3%:*T#\#%} P# Rx :b32*Sj_ngo"732:9r8;#% r$< n*+!EL[b!E"!$)$:Cm!$32)]!,M732:9B$<3%:-)$078"0+0!E/0:rC#%!$8;: :9 A:9:f!$g5'4!$#%($<^*,!EL@(<>$"* !1'O:9"*67!E#2ffRwcd0-:'4 32:* #%!EP# RHz{Q7-.:;p@'O:9"#%*:&jA:+ 'O:89#2<>5m0:,L[`ABQ$'4#%*,!$33%:-)$0i!$CW_CW!$3%3O$'#%*+!$3ff 3%7-#249Rx+'l_ffSj_n'w'{l x cd0-:b' 32:*q8;4#% V$<P"8"0-:C743%#%-)6)!$*: :9 A:9: :!$*+?$:9jA:9:9L@dAr0-: Z#%D:9?$: QA:9:9L@dAB0: Z#%V@CC MR /!$8M0iA:9:9Li#%DC#2?@#%C-:Cl#%&i'O:9"#2@CKAB0-:#%.:9?$:iAB0:#%1[CC MR !$8"0n)!$*:]#%68;*'O :CZ$<FA]32$9N $0-*: !$4C $!`A!`5 @N AB0-:9":r-:K :!$* '3%!5@D0-*:j!$Cf0-:K$0-:9d :!$* '3%!5@!`A!`5$R]cd0:k {:8;#2?$:]#%K }8M0-:C732:!l)!$*:+<a$.:!$8M0Z'O:9"#2@Cx$<:9?$:9"5xA:9:9LX78"0Z0!E:9?$:9"5x :!$* '43%!`5[j!E)!$#%4 .:9?$:95X$0-:9. :!$* J!W :!$* '43%!`5[K:;p-!$8;325X8;:+!lA:9:9LXAB0-:A:]04!?$:]!$n:9?$: &7* :9j$<D :!$*,9N!$CZ!Ek* 68;:]!WA:9:9LQAB0-:YA:]0!?$:l!$Z@CCsD}ffuKo6J 1<P5PP2RoPYRo5J 18 K>?b-fcf=<fififfv2$ff6ff )T$fiffv~ff?>ff[7* :9D$<JA:9:9L@ 4!$Ci!, :!$* 4' 3%!`5[B!EB* dFAB#%8;:1#\l0-:1!$*:j'O:9"#%[Cf`?$:9b0-:18;7" :$<w0-:.:!$ ffRcB0-:K' 32:* #\#\}#I RSj_q'ot2s l x SbZ$"C-:9*,!E)#%8,"|&7!E":,#%.!$*,!E " #2pX8;&!$#%#%)i0-:5( NAB#%0}0-:H7*h$<(:!$8M0X`A.Nff8;3%74*,ffNO!$CmC4#%!E)$!$3 :#\-)+:|[7!$3 Rbcd0-:7* :9" 6['4 23 :* #%#%}#I Rvw}e)t2{fi1cJ! 32:! !!I7-)8 )8)8 )8j_qQs , x'''''';-b"!#c! b! c =6U! ! 8b r!ff!!)6!b"ff!!)b8r! !! crrr!6!!! %! c =U6!!! c! 66rrU!))) )))"b8 U!I!c !8bVU! %b )! !U! "b !! %)) 8U) )! U !) 8U!)7;D: !!!b U6!!b )!!c :-!I!bff7;D!"b88 ! ! ! ! 8c)"6c 8! U 4 b!I ) )6) U!U)! !:r"!!) 8U 6 ! )"b UU c ) !I ) )6 8 !! c)"c"rb %)"b : r!!c b!!: ) ?c )6 rb! ! )#$)$ )r ff!! ! b! )"6c 8! U !!c ! )6!I U )!: ) VU#c b#c !b! ! !!I"c#rb ! ) U ff!! ! "b U U#c "U ! !!rc c U!I :! !!I"c VU r"c#cr#c#cUr# c ?b)K !!U ) VUrU U6 ))K !!Ur" !8!Ur# !b%# )$c ?!I($b# b c!) ) !r)U!b !I8% )688"E< =!!D-:!b"! b#c! c !r:%)7;!b"c c"c! !)! ) !#r" $U &b%U "%).b86ff!! ! ) )!!b8).b86ff !%! c b U"!!*!!cc ? "$br!r($r:+% ! )!!!c"b U !) U" !?b"cr! cr ff!bV U!!c"c U !8 c!ff!! !%)!"!ff! c!!b#c c#cU8 ff! !! !I ff!8! ! #c8 !)8! "! ) !!8r b%)"c c%)! 4 "!7* :9D$< !$8"L& M!$8L@ ><=!$#%3% !$Ci"7#\-)1#%*,:j ,_4CW!$3%3ff3%7-#2N[$B'`?$:10!E0-:9":+!E:+-m 3%7-#249N^ }<>7-1#\ !$8;:j$<!$-)$<a$MCZ' 32:*WR B74g#%*,:.!E:,<>$z[32?$:9 [R ]~$N J:g#%74* zzz'@8;: $N!$C,~$< rSb~YRv2" kK373 1:K373 !eK @_"N YHP>!"j1: X)Xfcd0:+:74321!E:])#2?$: #%YcJ! 32: [RiyQ:W*,!EL$:l!}[7* :96$<d :9?E!E#2R /-<>$"89#%)}SBx0-:H'"#%*+!$3P-${G:|[7!$3%j*@C-:3r)#2?$:K0-:kA$" K:73% =!$j#2KC-[:j#%x!$3%* K!$3%3J0-:7 :|&7:gd'4 32:* C-*+!$#% MRyQ:1AB#\3%3O-$B0-:9":9<a$:kC#%8974D#2d<=7-0-:9`Rcd0-: : B"7&#%*:!E:l !$#%:CnAr#20 0-:*@C-:3 N(0:7-"#% #\8>' KC MN# R :$Rw<a* :-<>$"89#%)x!x'O:9"*k7-!E#2 50-:.8M0!$-:3%3\#%-)8; "!$#\gd!$3%-:.!$Ci8M0-&"#%-)0-:j?E!E"#%! 3%:bAr#20W*,!$3%3%: DC-*,!$#%ffN-Ar0-:90-:9'"#\*,!$3g$C74!$3 R#%) 7 J0-:V'"#%*+!$3g$ 7 w0-:DC7!$3&?!EM#%! 32:w!$C-:89#%"#21?E!E"#%! 32:J :CJ#%8;":!$ :D"7&#%*:9Rwcd0-: "!$8"04#%-)j0-:7-"#\ #%8VC-[:(#%C-:9:CH'4$_<>* 0-:d*k732#2'32:V?[#2:9AB' #\g9R$ :r0!E0-:*@C-:3#\-632-)$:90-: : "!E :9)$5$N#% :9M*,/$<ff:#%0-:9<=!$#%3%7:$"7&#%*:9N!$k#21AV!$6#\YcJ! 32: -R}cB0#%.#%6C-: '4#% :,0-:+<=!$8;k0!Ek#%10!$60-:] "-)$: k'$'4!E)!E $Rmcd0#%*@C-:3ff0!$B325+:.?[#%:9Ad'O#%gD!$Ci0#%d0#\C-:9"0-: "!$8"04#%-)0-:7-"#\ #%8ER$ :k!$3% ,04!Ed0-:*,!$3\32: V :!E"8"0i :9: 7-D-$DM7g#\*: /!E:j !$#%-:ClAr#20f0-:*[C:304!Ed8;* #\-:V0-:!$3%3 G C4# :9":gK8; "!$#\gjQ0:k'4"#%*,!$3JAB#20x0-:,8M0!$-:3\3%#%-)f8; "!$#%& :9 A:9:Q0-:'4"#%*,!$3!$ClC7!$3NgAB0-:fA:b74 : $0f'"#\*,!$34!$ClC74!$3?E!E"#%! 32:!$DC-:89#%"#2+?E!E"#%! 32:9Rwcd0#%8;* #%!G#2m)#2?$:r0-: :-:9_B$<0-:H -)$: b'$'!E)!E $b!$Cx!+C47!$3^?@#2:9Ad'O#%&D<a$b0-: "!$8M0#%-)0-:7-M#% #%8ERljx/ 0I-5=5+=fi 8-fiffffeKt$w 2*@C-:30-:7-M#% #%8>'>'>' KC>'=C( >' KC( >'( =C>' KC>'=C( >' KC( >'( =Cjx/jx/jx/ 0Ijx/jx/jx 0Ijxjxjx/ 0Ijx/jx/jx 0Ijxjx<=!$#%3%.aTffaTff,3@Ri3$3@Ri3$,aTff&,3@Ri3$aTffaTff&,,3@Ri3$3@Ri3$3@Ri3$3@Ri3$3@Ri3$3@Ri3$,,,2/,72MV.$:8ER,&E< =cJ! 32:MV&$<=!$#%3%14373"70&7"70"6&7"7."6"6"70"67" .:8ER@R@R3 k73 \18,ff3@R\18,3@R\1:03@R\18,3@R\18,3@R\1:.3@R\1:.3 Rk@3@R\18,3@R\1:.3 R\1:0@ff<=!$#%3%1:.707"07"7"171718,143$70.7.7.171718,143K>.7.$,0707007"7".760707007"70MV0$:8ER[R[R[R[R<>!$#\3%. >&" w,K&" k.j1" w,63.77&7K3"[Rk.7"[Rk&7."[R >&[Ri373"[Rk.7""[Rw,K&[Ri3$""[Rk.6"[Rk.K3"7$,K&7&7.j1:07&>&7"770"7$,K.7"7.j1:07.>&$,r 1>07&$,K.>7$,K.>0$,63$7"7K3M1>6>"7:8ER@R[R[R[R[RER[R[R-R[R[R&R&K3737.7">&7$,K1:.>&j1$,K1:0,r$,K&>7.,7,ffk&j1k"$,$3k7.k07&i3$"1k06k.7i3$&k.j1i3Ke>7[Rk.707* :9D$< !$8"L& M!$8L@ ><=!$#%3% !$Ci"7#\-)1#%*,:j ,_4CW!$3%3ff3%7-#2N[$B'`?$:10!E0-:9":.!E:j- 3%7-#249N& <>7-d#%4 !$8;:V$<k' 3%:*WR r7&#%*:D!E:K<>$Dz[3%?$:9 [R +~$N J:&#%7* z"zz/'"[8;:$N!$C,~$< BSK~ZR1:K373 Xe _" kK373X"j1: !)Ev2Fcd0:.|&7!$"#2)$7-'i:;p-#% :48;:.' 3%:* 89!$ :.*@C-:3%32:CW!$r!,*67432#2'432:K'O:9"*k7-!E#2W'4 32:*AB#20#%& :9" :8;#%)m'O:9"*k7-!E#2 8; M!$#%gRYyQ:i#%& @C78;:l!m?E!E"#%! 32:]<>$:!$8"0 :g 5 #%0-:+*k732#%'43%#%89!E#2x! 3%:$<0:+|&74!$#2)$7-'PRyQ:+0-:Q'O 1' :9M*67-!E#%Q8; "!$#%&.Y0-:?E!E"#%! 32:B$<:!$8"0x !$C}:!$8"0Q8;3%74*,ffRVz mc ! 32: +!$C &N A:6)#2?$:6:73%d<>$rFAf<>!$*+#%3%#2:$<' 32:*+9RkSb :9<>$:$N^0-: *,[C-:3J)#2?$:10-:A$" j'O:9<>$"*,!$8;:$NP!$4C 5X!W8;#%C:9"! 32:*,!E)#\Z<>$60-:l3%!E)$:9H#%!$8;:9R}o-$[N(!$3%3(0-:f$0-:9H*@C-:3%1!$4C M!$8"0#\-)}0-:7-M#% #%89)#2?$: !$C3%5,#%*,#%3\!E('O:9<a$M*,!$8;:$RS C7!$34?[#%:9Ad'O#%g9N&:#20-:9 5,#2 :32<ff$D#%]8;* #\!E#2]AB#200-:]'"#%*,!$3/?[#%:9Ad'O#%g9NJC-[:6-$k O:9k!$&5Y!$C-?E!$g!E)$:$N 7-6C&:k-$60[7-k*6748"0n:#20-:9R}o$-N#\fcJ! 32: &N4!$3\30:K*,[C-:3\!$4C "!$8M0#%-)H0-:7-"#\ #%89V!E:j8;*'O:9#2#2?$:$N-:;p-8;:9'd<>$D0-:*@C-:3ff!$CW0:10-:7-"#\ #%890!E M!$8"0W4325+W0:1C7!$3O?E!E"#%! 3%:9R6oB&2Bex29K,,c m*@C-:30-: b32* "7432:9"K' 32:*U!$k!W'O:9"*67!E#2Z'" 32:*WN^A:]#\g @C78;:,!}?!EM#%! 32:<>$r:!$8M0X'4!$#2AB#\ :.C#% !$48;: :9 A:9:X*,!EL@9R @#\8;:.A:k*,!`5}0!`?$:H*,$:1?E!$3%7-:r0!$m?!E"#\! 32:9NA:,#%g "[C748;:k!$CC4#2#2!$3 ?!EM#%! 32:b W:7:604!Ej0-:9:!E:,!$j*,!$&5m?E!E"#%! 32:r!$.?!$3%7:9N^!$7-)$)$: :C 5 b:9:32:MRHyQ:+89!$X0-:X'O j!f'O:9"*k7-!E#2X8; "!$#\gKX0#%b:3\!E)$:C:9k$<d?E!E"#%! 32:R}z{ncJ! 32: [NA:f)#2?$:l:7432.<>$H_4C#%)i!$3%3/$'4#%*,!$332:-)$0n"732:9"1<a$H<>7-#%!$8;: b32**:!$V0:b'4 32:*q$<P_44C#%-)1! K32* M732:9$< =*,#%4#%*,!$3 (32:-)$0UAB#%0 Y*,!EL@9R b: '#2 :.0-:.<=!$8;B04!EB#2B0!$B0-:1 "-)$: d'"$'4!E)!E $N40:*@C-:3^#\d-$'5=<d1:0707$nxRog{('!.4fififfv2$ff6ff )T$fiffv~ff*@C-:30-:7-"#\ #%8>'>'>' KC>'=C( >' KC( >'( =C>' KC>'=C( >' KC( >'( =C<=!$#%3%jx/jx/jx/ 0Ijx/jx/jx 0Ijxjxjx/ 0Ijx/jx/jx 0IjxjxeWffeWff&3@Ri3$eWff&&3@Ri3$3@Ri3$&eWffeWff&&3@Ri3$3@Ri3$3@Ri3$eWffeWffeWff2fi,7:8ER&,< =cJ! 32:BfiV&$<=!$#%3%.7:8ER@R<=!$#%3%3 k718,7,K03@RkK33@RkK33@Rkj1077"07j11:7&7&06$3077&1:7&$,0K373e"707" 0,r7" 07" 0,K$" ,$" ,$& ,$" ,$" ,&7&BfiV.$ee3@Rk7e-Rk.$,, Rk$,&" R\1:0[" Rk70[, Ri3K&" Rk6[" Rk7[, Ri3$&17143$0K3$".70$,17143Ke3@RkK33@Rk7171:&7707."76 1:0"7"777"7"770$,.77j1:&"7"77&6"7"67>.770j1:&3@RkK33@Rkj1<>!$#\3%. k70" \1:k070k07, k"70BfiV0$:8ER[R[R-R-R&R"76 1:0,K7070$,"7K3>,7,"76>&7,K$,~1O:8ER[R[R[R[R[R.6>>07&>.70>.7",7,Kk7&k6k.70w,K\18,ff fiff ffe,K.[R >.,K&7[& Rk7>.7[& Rw,K>0jE1 Rk"6,r>[" Ri3$0>0jE1 R >">07[ Rw,63,r>[" Rk.7&7* :9D$< !$8"L& M!$8L@ ><=!$#%3% !$Ci"7#\-)1#%*,:j ,_4CW!$3%3ff3%7-#2N[$B'`?$:10!E0-:9":.!E:j- 3%7-#249N& <>7-d#%4 !$8;:V$<' 3%:*WR r7&#%*:D!E:K<>$Dz[3%?$:9 [R +~$N J:&#%7* z"zz/'"[8;:$N!$C,~$< BSK~ZRB1:K373 Xe _" kK373FX"j1: !)jx/ 0IEv28;*'O:9#2#2?$:D,0-:d3%!E)$:9/#\ !$8;:9R~}[C-:3 !$4C,0-:7-"#\ #%8>' KC P)#2?$:0: : w"7&#%*:<>$+0-:x3%!E)$:9l#% !$8;:NAr0-:9:!$+!$C4C#%-)Z0:}!$3%3 G C# :9:&+8; "!$#%& =*@C-:3N 0-:7"#% #%8>' KC )#2?$:D0-:13%:!$ :!E"8M0ffR :#%-)<>$"8;:Ci "!$8M0l 74 D0-:j'"#\*,!$3 ?!E"#\! 32:D0[7-0-: "!$8"04#%-)0-:7-"#\ #%8ERjx/ 0Ie bq)se}u'w{m53%#2L$:b0-:j':9?@#27' 32:*,9N[A:j_Cf325,0-:K_M 3%7-#2f 0-:j 'O$V8M0-:C73\#%-)k' G23 :*WRcd04#%32:!$C/ 6*k78"0])$:!E :9?!E"#\!E#2+#%,' :9"<a$"*+!$8;: :9 A:9:+0-:bC# O:9:&/*@C-:3%9RJyQ::9'O$V:7432D#%lc ! 32: [R b[[ClM7g#\*:D!E:j !$#%:ClAB#20l0-:!$C*@C-:3%N[7"#%-)H0-:C7!$3^?!E"#\! 32:d!$bC-:89#%#%W?!EM#%! 32:9N4:#20-:9B}0-:#2d ABm$r#%m8;* #\!E#2WAB#%0i0-:.'4"#%*,!$3?E!E"#%! 32:R0ew}\t$s1yQ:Q*@C-:3b0-:m$MC-:9*,!E)#\8m|[7!E:x' 32:* Ar#20 !*,!E "#2p $<.?E!E"#%! 32:]AB0#\8"05( RyQ:b0:]'O V!k'O:9"*67!E#2]8;4 "!$#%&l!$3%3 0-:b?E!E"#%! 32:#%]0-:!EL$:j?E!$3%7-:<a* b*,!E "#2p N!$C+7* 8; "!$#\g,0-:D`Ar9Ng8;3%7*+w!$CC#%!E)$!$3\9R B:732!E":D)#2?$:,#%HcJ! 32:@R(Sr)!$#%ffNg_C#%-) 7 (0:d_" 3%7-#23%:!$C( 6AB#%C-:D?E!E"#%!E#2+#%'O:9<>$"*,!$8;: :9FA:9:]0-:*@C-:3%9R"#%-)W325X0-:]C7!$3?E!E"#%! 3%:.!$1C-:89#\#2Q?E!E"#%! 3%:.#%.! !$CY8M0-#%8;:$N 7-.0-:]C7!$3?E!E"#%! 32:D!E":10-:32'<=73 #2<w7 :CW!$BC:89#%#2l?E!E"#%! 3%:D#%W8;* #%!E#2lAB#%0l0-:.'"#\*,!$3 ?!E"#\! 32:9Ro-$k0-:+3%!E")$: 6#%4 !$8;:+ 3%?$:CffN 0: : 1 "!E :9)$5n#%j0-:fC-7 32:*+!$3%32: 1C*,!$#%Z0-:7"#% #%8143n5=Ufi 8-fiffff*@C-:3jx/0j1:jx/"K373jx/ 0I &K3$&jx/.70K3jx/&77&jx &K3$.jx @077.jx&77&jx/ 0Ijx/K" 373jx/>70 "jx "K3Kjx @"6>jx>07"cJ! 32:, k7"$b32* V .[Nk6 b32* V 0[N 7 K32* 143@Nk"7"$3 \1:""7"6>706>077K3"76>770K3777"7&6>.770K3b32* &N<>!$#\3%:8ER@R0-:7-"#%#%8>'>'>' KC>'=C( >' KC( >'( =C>' KC>'=C( >' KC( >'( =C.E< =<=!$#%3%e3@R\1:3@R\1:"3@R\1:3@R\1:3@R\18,3@R\1:3@R\1:3@R\1:3@R\1:3@R\1O3@R\1O3@R\1:706>0$,K.7$,K.$,77"7.$,K06:8ERER<=!$#%3%<>!$#%3\:8ER1ERi3M11ERk7"1ERi3$1ERi3$1ERk$,1ERi3$1ER\1431ERi3$.1ER\1431 R\1E1ER\1:1ER\17118,6373$,K$,K"j1e 718,~1:"j118,63$7,&Rk"7",&Rk&7,K7"770,K7.7"7>0[Rk7">[0 Rk$,18,~18,K07,&Rk"70.[Rk70,K7&77.ffff<>0[Rk"70"6- Rk&71O>77"1O>707.[Rk7..[Rk7.&j1:&j1:&&j1:.707."6-R >&"6- Rk061O7$373.[Rk70&j1:.707"6-Rk0$,1 \1:ff:8ER7* :9+$< !$8L[ "!$8L@ ><>!$#\3% H!$C "74#%-)X#%*:i n_4C !$3\3D$'#%*,!$3B 3%7-#24<>7-1#%4 !$8;:j$<V0-: K32* "732:9MK' 32:*iN^AB0-:9:0:,$'#%*,!$3/32:-)$0Z#%.)#2?$:ffRr7g#\*:!E":r<>$Vz[32?$:9 [R H~$N J:g#%74*qz"zzw'@8;: $N@!$C~$< BSK~ZR4S C!$0}*:!$D0!Er-:73%VA:9:1:97-"-:C}!E<a :9 10-7-R!)Ev2," kK3731:K373 !e fi_"j1:1W*@C-:3 $d*,[C-:3R(cd0:b<>$"*:9V:;p@'432$:D!H3%!E")$:9d :!E"8"0i :9:$N 7-VC-[:D H?$:95f"3%#2)0&325|[7#%8L$:9B0!$i0-:.3\!E :9Rc f8;4893%7C-:$N 0-: :6:732b0- 04!EbC-5@!$*,#\8 "!$8M0#%-)]0-:7-"#%#%89d89!$ :6#2)4#2_489!$&325* $:k: O:8;#2?$:HAB0-:i0:95i32&$Li!E $0i?@#2:9Ad'O#%&d$<(!]'O:9"*k7-!E#2ffRdz{C-:9:C^N "!$48"0#%),'"#\*,!$3V$]C74!$3V?!E"#\! 32:AV!$,$<> : *$":}#%*'O$!$&, Z7-]:73%H0!$ 7"#%-)Q!Y "-)$:9'$'!E)!E $Rdo-$K:;p@!$*,'432:$N:-<>$"89#%) KSB m!$m!$3%3 G C# O:9:&B8; M!$#%g9N !$C}:!E"8"0#\-) 70-:m'4"#%*,!$3D?E!E"#%! 32:9NV$<a : )!`?$:xA$" :x'O:9<>$"*,!$8;:i04!$ :<a$"89#\-)YSB 0:m8"04!$@G-:3%3\#%-)W8; "!$#\g9NJ!$CZ0[7 :#%)i! 32: "!$8"0Z $0Z :96$<V?!E"#\! 32:9R,z{Z!$CC4#2#2ffN#%Z*:,' 32:* 893%!$ :N 0-:]C-7 32:*+!$3%32: .C-*+!$#% "!$48"0#%)l0:7-"#% #\86 O:9:CY0-: :'O:9<>$"*,!$8;:$R(SbDA:.0!?$:1!E")7-:CffN-0#%D0:7-"#% #\8r#%D8;4#% :&DAB#20l0-:j<=!$#%3O_M V'"#%89#%'432:d<>$?E!E"#%! 32:K$"C:9"#%-)!$CW0:17898;:9:CW_4" D'"#\89#2'432:r<a$B?!$3%7:j$"C-:9"#%)-R5Kz D#%A$0W$#%-)k0!ED0-:K:73%/$< 7-D:;p['O:9"#\*:g"74]8;7& :9d H0-:j774!$3:;p@'O:8;!G#2($< ?E!$3%7-:D$MC-:9"#%-)-RJyQ:B<>7C,0!E/C-7 32:D*,!$3%32:(C-*,!$#% >04!E#%9N&*,!$3%32:(C-*,!$#%H<>$$0x?!E"#\! 32:k$"C-:9"#%)f!$CX?E!$3%7-:H$"C:9"#%-) d)!?$:+C4# :9":gj[7* :9"b$< !$8L[ "!$8L@. i"*,!$3%3 G: KC-*,!$#%m?E!E"#%! 32:1$"C-:9"#%)-N:9?$:xAB0-:m_4C4#%-),!$3%3J 3%7-#%9Rdz b#\B)$:-:9M!$3%325W0-7-)0&b0!E?E!$3%7-:j$"C-:9M#%-)*,!EL$:B+C# O:9:8;:K +0:j`?$:9"!$3%3 :!E"8M0i: O$dAB0-:i_4C#%)k!$3%3ff3%7-#2N@#%<8"0-32$)#%89!$3 !$8"L& M!$8L@#%-)#%7 :CffRwz{C-:9:CffN&0-:j!E)7*:&)#%?$:]:!EM3%#2:9/<>$D7898;:9:Cf_"!$D!?E!$3%7-:j$"C-:9M#%-)H'"#%489#2'432:b#% !$ :CiW_4C#\-)64325]:. 3%7-#2 (#2<PA:18"0& :60-:j"#2)0&D?E!$3%7-:$NX<8#cfififfv2$ff6ff )T$fiffv~ff*@C-:30-:7-M#% #%8>'>'>' KC>'=C( >' KC( >'( =C>' KC>'=C( >' KC( >'( =Cjx/jx/jx/ 0Ijx/jx/jx 0Ijxjxjx/ 0Ijx/jx/jx 0IjxjxcJ! 32:8V&$['O$<=!$#%3%:8ER&76"7.70,171:"j1O&76"7.70,171:"j1O0E< =8V.$@' $"<>!$#%3\:8ER@R@R1:6>."7&7&"7&7&eW0&7&K3M1>"7&7&eWeW3@Ri3$3@Ri3>,0&7"7&7>1ERi3$03@Ri3$!3@Ri3M13@Ri3$03@Ri3>,3 k73 \1:"3@Ri3$3@Ri3$&3@R\1433@Ri3M13@Ri3$0eWeW3@Rk063@R\1:&eWeW70$, w,637"K3 k07eW1:$,K&j1O>77&17171:.7K3$&707,63$7.7"7"[Rk0703 Ri3>,@3 RkK3@1:&7[. Rk0j11 Rk"7.E1:7&j1:&7.7&77017143$.j1:7&707&&707K3$,K[R\1433 Ri3$.@3 Rk7"@1:.7[& Rk71 Rw,K&E8d1:$['O$<>!$#%3\:8ER&R@R1:.7&77$,K"1:7&j1:&7.7&3@R\1O8d143>['O$<>!$#%3\eW:8ERER-R"$,7,7,K7.77"77$,63$""7777"7$,r7,&77&7.>&j1:71:0$,~1 k071O77 7&77"77"77$,63$"&77&7>"j1:7"[Rk701 Rk07.E1O>07[" R 1R\1:.[K@3 Rk07.&j1:70[Rk"7"21:7&7.[Rk.61 Rk0E1:[. R7* :9k$< !$8"L& "!$8"L[ ><>!$#%3\ j!$Cn"74#%-)W#%*,:+ x_4CZ0:]_M 6 3\7-#2Z X<>7-%# 4 !$8;:b$</0-:H 'O$K8M0-:C73%#\-),' 32:*WR B7&#%*:b!E:H<>$bz@32?$:9 [R~$N J:&#%7* z"zz('@8;: $`N4!$C+~$< BSK~ZR1:K373 !e @_v,p"j1: !)9" kK373A:189!$}!`?$#%C !$8"L& M!$8L@#%-), f8"0-[ :1!$-$0:9B-:$R(z <JA:.AV!$&d ,_4CW!$3\3^ 3%7-#%9N@A:60!$3%30!`?$:k !$8"L& "!$8"Lm f 5}!$3\3P0-:H!$32 :9"4!E#2?$:6?E!$3%7-:b!$&5&AV!5$R @*,#%0b0- ABK0-`A ?E!$3%7-:$"C-:9M#%-)+89!$X*,!EL$:!fC# O:9:8;:1 l0-:H :!E"8M0X#%!$-)$<a$MC b' 32:*iN4:9?$:xAB0-:m_4C#\-),!$3%33%7#29Rkz "#2:9< NffAB0-:XA: !$8"L& "!$8"LZ0!?@#%-)l "#%:CQ0-:,!$#%)*:&NPA:+89!$'O B0:k8; "!$#%&Rbz{x *:H89!$ :9NO'$'4!E)!E#%x*+!5}-32:!$C} W#%*+*:C#%!E :<=!$#%3%7-:$RS )$&@Ci$MC-:9"#%-)H<>$d0-:j?E!$3%7-:d89!$}0-:9:9<>$:1!`?$:6 :!EM8"0ffRRfi 2RXVK37373>Q Qz{m*,!$&5i'" 32:*,9N?!EM#%! 32:r*,!5 :k8; "!$#%:C} l!EL$:7#%|[7-:.?E!$3%7-:N 7BA:k0!`?$:*$:?E!$3%7-:r0!$x?!E"#\! 32:9RBcB0!Eb#\9N4A:!E:k3%&$L@#%-),<>$j!$m#\ {:8;#%?$:k*,!E'4'4#%-),<>*h0-:k?!E"#\! 32:+0:1?E!$3%7-:9RDo-$b:;p@!$*,'432:$N!$m$'#%*+!$3 G#%8L b32* "7432:9B0!$B#%8L@r!Er0-:6*+!EL[ @N EN -N[N !$4CER]cd0: i#%& :9 G#%8LQC#\ !$8;:.!E:]!$3%3C# O:9:& 7-jC-}-$j<>$"*U!i' :9M*67-!E#%Z!$0-:HC#% !$48;: ]#%B! :g9RKow#%C#%),! G#%8"L b32* "732:9r$</32:-)$0H89!$ :1*,[C-:3\32:C}!$K!'O:9"*k7-!E#2.'4 32:* 5.#%& @C789#%-)d!$k!$CC#%#2!$3 ;06?!EM#%! 32:( b!EL$:V.0-:V*,#%"#%-)V?E!$3%7-:[Rz{C-:9:CffN^04#%j#%K0-:,*:90[CXA:+7 : i*,[C-:3w0:' 32:* #\X0-:,3%!$ . :8;#%ffR B A:9?$:9`N0-:9:b!E:r!6&74* :9($<^!$32 :9M!E#2?$:BAV!5@ k*@C-:3!$f#% :8;#2<>* i?!EM#%! 32:#%g?E!$3%7-:AB0#\8"0lA:.:;p['32$:.0-:9:$R0171m&p143$"#c"653 1171171&65+e_'65o-$:;p-!$*'43%:$NJ0:9:f!E":fFAQ#\*'432:'M#%*,!$3/*,[C-:3\1$<d!$ #% :8;#2ffRmz n:!$8"0 A:l0!?$:'"#\*,!$3V?!EM#%! 32:Ar0#%8"0 !EL$:x:i$<'O# 32:l?!$3%7:9R z{ 0:}'"#\*,!$3D!$3%3 G C# :9:&,*@C-:3=C-:-$ :C 5MNPA:+#\*'432:k'O j!i#%-)3%:k!$3%32G C# O:9:gj8; "!$#%&jQ0-:'"#\*,!$3J?!E"#\! 32:9RN P>'8!fi 8-fiffff*,[C-:30:7-"#% #\8>'>'>' KC>'=C( >' jC( >'( =C>' KC>'=C( >' jC( >'( =CMx @Mx @MxMx @MxMx 0IMxMxMxMx @MxMx 0IMxMx$V$"eWeWeWeWeWeWeWeW"eWeW"eWeW&"""""~m!E)#%8<>!$#\3%aWaWaWaWK31:01:.K3$ ,1431711:.1:.1:0aWaWaWaWaWaW14371 11:&3@Ri3M1aWaW$V&$~}!E)#\8<=!$#%3%:8ER@R@R@R@R-R@R1:"$,K&1:7"7">&7$,1O>"$,>07j1:"7"7">07"077.7j18,7>7&1:7"7">6>7&>7"3@Ri3M13@Ri3$c$V"$~}!E)#%8<>!$#%3\:8ER3@Ri3M1+143< =cJ! 32:$R~}!E)#%8<>!$#%3\:8ER3 \1713 \1713 k$,3 \1Ok&j13 i3$&eWeW.7&[Ri3>,3 R >.@3 R\18,@" Rk7[3 Ri3>,@0j1:0K3$"$,:8ER$,r>.7&K3$01:07&[R >"67>.j1:&76>0[Rk.6>&77.7&7"1:&6>.6$3$.1:7[Rk7"7[0 Ri3M11O>.[Rw,K143$.770707143$&[Rk"7"Tffv7* :9b$< !$8L[ "!$8"L[ ><>!$#%3\ d!$4CX"7#\-),#%*:H W_44C}0-:_4" K 3%7-#2x W<>7-#\ !$8;:l$<H*,!E)#%8Q|[7!E:Q'4 32:*WR B74g#%*,:W!E:Q<a$}z[32?$:9 [R~$N J:&#%7* zzzB'@8;: $N^!$C}~$< BSK~ZR^STC!$0Q*:!$4K0!E.-":732VA:9:.:97"-:CW!E<> :9 j0-7-`R1:K373 !e _"j1: X)1X" kK373N YHP>9z{Ev20-:W'"#%*+!$3-${G:|[7!$3%+*[C:3 =C-:-$ :C 5HA:W'O #%4!E5Y-${G:|[7!$3%,8;4 "!$#%&:9 A:9:Z:9?$:95XFAxC#% #%48;b'4"#%*,!$3w?E!E"#%! 3%:9RHyQ:]89!$Y!$3\ i7 :+C47!$3*@C-:3%9R,o-$6:;p@!$*,'432:$N#%W0:1C7!$3ff-${G:|[7!$3%r*@C-:3 N-A:60!`?$: UC7!$3^?!E"#\! 32:9N-:!$8M0}AB#%0i!,C-*,!$#\W$<' "# 32:?E!$3%7-:X$<P0: :.!E:.C7*+*15,?E!$3%7-: MN@!$4C #%!E5,${G:|&7!$3\d8; "!$#\g :9 A:9:W:!$8M0'4!$#2D$<C74!$3O?!E"#\! 32:9R_'9 ' JXo'b:AB#%3%3$8;4#%C-:9^0:9:C# :9:&P8;* #%-:C6*@C-:3%ffAB04#%8"0.8M0!$-:3 :9 A:9:1'M#%*,!$3$!$C6C7!$3Q*@C-:3%9R}z n0-:f_" k8;* #\-:Cn*,[C-:3 =C-:-$ :C 5* MNA:l0!`?$:l8M0!$-:3\3%#%-)}8;4 "!$#%&$</0:k<>$"*i#%*,'43%#2:d!$CQ-l!$CC#%#2!$3JC7*,*65i?E!$3%7-:b<a$j0:HC47!$3J?!E"#\! 32:9Rz{ 0-:x :8;C 8;* #\-:C *@C-:3 =C-:-$ :C 5:;p@ "!( MNd0-:xC7!$3B?!E"#\! 32:,0!`?$:C7*+*15 ?E!$3%7-:9Nd!$4C A:Q0!?$:Q8"04!$-:3%3%#\-)Z8; "!$#\gf$<.0-:x<>$"*#R z{0-:l0#2MCY8;* #%-:C *@C-:3 =C-:-$ :C 51 MN(0-:WC7!$3?E!E"#%! 32:k0!`?$: 7 H!X#%-)3%:+:;p@ "!C7*+*15m?!$3%7:$N^!$CXA:,04!?$:+8M0!$-:3%3\#%-)l8; "!$#%&j$<0-:<a$M*}#7-K3%5AB0-: m#\.-$.:|[7!$3 m0-:+C7*+*15x?!$3\7-:$R$ :]04!E1!$g5X$<D0-::+8"0!$4-:3%3%#%)l8;4 "!$#%&!$32-: >AB#%0-7-j!$CC#2#%!$3 8;4 "!$#%&jX0-:H'"#\*,!$3P$6C7!$3J?!EM#%! 32: B#%K:-7)0X iC-:9_4-:!$i#% {:8;#2^R@B |%] `Y zN P>' %J@B | ] OY zN P>N P>5@B*Y | J] +Y z=!|pK5X' J%o65yQ:Q89!$ !$3% n*,[C-:3r!$ #% :8;#2 5 #%& [C4789#%-)C7*,*65' "#\*,!$3D?E!E"#%! 32:]!$C:7"#%-)b0!E(0#\w:;p[ :C:C, :9($< ?!EM#%! 32:w<>$"*,(! # {:8;#%ffRJcB0#%89!$ :B\# 9N0-`A:9?$:9N@8;`?$:9:C5]7-B:!E"3%#2:9D:"732DW'O:9"*k7-!E#249R58"!fififfv2$ff6ff )T$fiffv~ff7as}{wD{#}yQ:B_4" ('"`?$:B04!E9N&AB#%0: 'O:8;( k!E"8 G 8;#% :8;5$N@0-:d_"( 5['O:d$<^8"0!$4-:3%3%#%)K8;4 "!$#%&!E:j!$#2)0&!$V0-:b'"#\*,!$34-${G:|[7!$3%V8; "!$#%&9N 7-3%:#2)0g0!$f0-:b'"#%*+!$34!$3%3 G C4# :9":g8; M!$#%g9R cB0-:ffN(A:i' ?$:i0!E0-:m :8;C F5&'O:W$<K8"0!$:3%3%#%-)X8; M!$#%g+!E:i!$,#2)0&!$l0-:X'"#\*,!$3B-${G:|[7!$3%W8;4 "!$#%&9N 7-l32:f#2)0&f04!$ 0:X8M0!$-:3\3%#%-) !$C C7!$3b-${G:|[7!$3%d8; "!$#%&9NAB0#\8"0W!E:13%:V#2)0&d0!$W0-:1'"#%*,!$3O!$3%32G C# O:9:gD8; "!$#%&9R/ow#%!$3%325$N@A:' ?$:+0!E10-:+0#%"CX 5['O:,$<D8"0!$:3%3%#%-)W8; "!$#%&1!E:+!$1#2)0&6!$.0:,'"#%*+!$3-${G:|&74!$3%8; M!$#%g 7-(32:w#%)0g(04!$0-:D'"#\*,!$3@!$3%3 G C# O:9:&8; "!$#%&9Rcd0#%w*:!$4(0!E0-:B0-:9:5['O:d$<(8M0!$-:3%3\#%-)8; "!$#\gr)#2?$:10:1!$*:6'"7#\-)HAB0-:WA:1:-<>$"8;:k!E"8 G 8;#% :8;5i!$0-:k'"#%*+!$3^-${G:|[7!$3%K8; "!$#\g9R$ :$N^0`A:9?$:9NO0!EKA:k)$:9b*,$:6'4"7#%)AB0-:iA:!$CC0-:dC74!$3[${G:|&7!$3\8; "!$#%& 7w$0-:V'"#%*,!$3@-${G:|[7!$3%8;4 "!$#%& MRcd0#%w#%wC# :9:&K'O:9"*67!E#2 AB0-:9:V-:#20-:9J0-:V!$CC#2#%1$<0:'4"#%*,!$3&-$ 0:VC47!$3&${G:|&7!$3\J8;4 "!$#%&,0-:68"0!$:3%3%#%-)H8; M!$#%gD)!`?$:k*,$:.'"74#%-)-R=sxuJXu7J QRJ:t:[uF8N*Q*HKJlWPYH>Z9v [4]!,* - 7- 7,*fiQs e-!65c ]0`A jSBSB ;N8;#\C-:9r!$x#% :8;#2W'" 32:* AB0 :1'M#%*,!$3ff!$3%3 G C4# :9":g8; M!$#%gl#% KSBbR @7-''O :}0-:x8M0!$-:3\3%#%-)Z8; "!$#%& :9 A:9:1!$4C xA!$l-$fSBbRcd0-:d#%.:91 x!$C ]0!$ B:3%#%*+#%!E :CQ<>* #%.C-*,!$#%^R V7-60#%.#%1-$1'O# 32: 50-:+8;4 "78;#2X$<V0-:+'"#%*,!$3w!$CQC47!$3*@C-:3 R r:8;:,0-:+8M0!$-:3\3%#%-)l8; "!$#\g.!E":+!$3%3SBbR cPf0-`A "#%8;:9N48;"#%C-:9B!$m#% :8;#2W' 32:* #%WAr0#%8"0,!$C*(1RDcd04#%V#%dSB7V$KBR*(1/c Q0-`A SBSBN(7-''O :+0!EH0-:f8M0!$-:3\3%#%-)}8;4 "!$#%&H!E:WSBbRV#%C-:9H!>AB0-:9:*k7 d0!`?$:.0-:-${G:|[7!$3%d8;4 "!$#%&9N04!EB#%D-$dSBbR A.N!$C!$*:H#%)32:9 mC*,!$#%ffN R1#%C:9B0-:H8"0!$4-:3%3%#%)+8; "!$#\g :9 A:9:/!$C&RKcd0-:325lSB ?E!$3%7-:j<>$ 6#% R @#%*+#%3%!E"325$N[0-:j3%5fSB ?!$3%7:j<a$ 6#\W0-:.8"04!$-:3%3%#\-)8; "!$#%&k#% -R V7--R B:48;:$NH0!$B-+SB ?E!$3%7-:R/cd0#%D#\d!,8;& "!$C#%8;#2m!$:9 A:9: 1!$C0-:18M0!$-:3\3%#%-)8; M!$#%gb!E:1SBKR r:8;:6!$3%3P-${G:|&74!$3%B8; "!$#%&B!E:kSBbR `A 7-''O :0!EK0-:-${G:|[7!$3%j8; "!$#%&b!E":HSBKR^V#%C-:9K!l8"0!$4-:3%3%#%)]8; M!$#%g :9 A:9:!$Ck04!EK#\b$KSBKROcB0-:/#%K :9K W!$C 0!$ D:3%#%*,#\!E :Ci<>* #2KC-*,!$#%^R 7r<>$:j:3%#%*+#%!E :Cf<a"* 0-:.C-*,!$#\W$< $N *,:.$0-:9B'4"#%*,!$3 ?E!E"#%! 32:$N-!`56AB0-:9:"N4#%:9 N@AB0#%8M0,:3%#%*+#%!E : k<>*q0-:bC*,!$#%,$<=#%48;:r0-:b-${G:|[7!$3%V8; "!$#\g!E:KSB MRr:8;:$N^#2K#%r$r'O# 32:. W :9 / i!$C H0!$ V:3%#%*,#%4!E :C}<>* #2KC-*,!$#%ffRKcd0[79NO!$3%38"04!$-:3%3%#\-)k8;4 "!$#%&B!E:.SBKR('@Bc|J]zc)+e65] Y[] ] ] >1K{`~{`7*<@ B , @z |G%]7^ Sz@ ]4^ q| ) Sz | e]4^@B]@ @ Y[@ >1K{`=]7^@ B@@B=e]@B|euxfin|+|M]H@B /|]Bz@Bq]]4^Bz@^9)@ BG Y%zzJXu7J QRJ:t:[uF8N*Q*HKJlWPYH>Z9v [4]!, * - , * 7 - , * ! - 7,*!- , * N@8;4#%C-:9D!$i#%K5{:8;#2f'" 32:*qAB0#%8M0f#\ jSB R@7-''O :]4BP!$C ]bAV!$$SBbR@cd0-:^N&#%]0-:b_" 89!$ :$N<];B ] Y>GSBQs e c ,0-`A jSB0-:K-${G:|[7!$3O8; "!$#%& :9FA:9:88fi 8-fiffffG Q 1@^ | ] OY"G@^ z R];B G!$CENJAB0#%8M0Q#%.#%*'O"# 32: :89!$7 :,0-:]8"0!$:3%3%#%-)W8; "!$#%&r#!$C#!E:QSBbRz{ 0-:x:8;C 89!$ :$NA73%C :x)$:!E :9W0!$ NdAB0#\8"0#%H#%*'O"# 32: 5n8;4 "78;#2 $<r0-:W'"#\*,!$3!$4C C47!$3V*[C:3 R r:8;:i!$3%3 #%!E"5Y-${G:|[7!$38; M!$#%g]0-:KC7!$3?!E"#\! 32:/!E":bSBbR@c k0`A "#%8;:9N@8;#%C-:9V!$f#% :8;#2]#\,AB0#\8"0NN !$CRrcd04#%d#%rSB*(1*(1/37-d$ KSB Rc k"0-`A SBSB N 5,*,-$ #%89#2F5$N@A:b0!?$:jSBSB Rc k"0-`A "#%8;:9N8;#\C-:9j!$Z#% :8;#2x' 3%:* #\mAr0#%8"0NJ!$4CN*(1*(!$CRcd0#\#\dSB 7-D-$ KSB R1/c Q0-`A SBSBN(7-''O :+0!EH0-:f8M0!$-:3\3%#%-)}8;4 "!$#%&H!E:WSBbRV#%C-:9H!-${G:|[7!$3%d8;4 "!$#%&9N>AB0-:9:04!EB#%D-$dSBbR A.N w!$C .*k7 d0!`?$:.0-:!$*:H#%)32:9 mC*,!$#%ffN R1#%C:9B0-:H8"0!$4-:3%3%#%)+8; "!$#\g :9 A:9:/!$C&RKcd0-:325lSB ?E!$3%7-:j<>$ 6#% R @#%*+#%3%!E"325$N[0-:j3%5fSB ?!$3%7:j<a$ 6#\W0-:.8"04!$-:3%3%#\-)8; "!$#%&k#\ -R V7- R B:8;: H0!$b-+SB ?E!$3%7-:9RDcd0#\d#%B!]8;g M!$C#%8;#2x!$:9 A:9: 1!$4C0-:8"04!$-:3%3%#\-)+8; M!$#%gj!E":HSBKR r:8;:,!$3%3J-${G:|[7!$3%j8; "!$#%&j!E:SBbRffcPW0- 0-::9?$:9":$N7-''O :r0!EV0-:j-${G:|&74!$3%D8; "!$#\gV!E:jSBbR4"#%C-:9V!k8M0!$-:3\3%#%-)k8; "!$#%&9N+#N@0!Ed#%V-$DSBbR-cd0:ffN@:#20-:9 J#%V :9V +!$C j04!$ :3\#%*,#%!E :C]<>* #2C-*,!$#\ffN$$ B#% :9!$CO0!$ 1:3%#%*,#\!E :Ck<a* #%(C-*,!$#%^R V7-9Ng<>$ ^ :V:3%#\*,#%!E :C<>* 0-:C-*,!$#%m$< $Nff *:H$0-:9j'M#%*,!$3^?E!E"#%! 32:$N "!5AB0:9:N^#%K :9K -NffAB0#\8"0AB#%3\3ff:3%#%*,#\!E : f<>* 0:kC-*+!$#%i$<=#%48;:10-:H-${G:|[7!$3%b8; "!$#%&K!E:HSB MR r:8;:H#20!$ V:3%#%*+#%!E :Ci<>* #2KC-*,!$#\ffRro-$ :H :9b#%b-$K'O# 3%:j l:9W!$CN(!$3%30-:f$0-:9H?!$3\7-:6*k7 :+:* ?$:C <a"* #2kC-*,!$#\ffN 7-10:9:f#\6-mA!`5Y x:* ?$:!$&5i$<0-:1?E!$3%7-: #2)$)$:9b0!$ Z<>* 0:kC-*+!$#%i$< N :89!$74 :6!EK* rA:H0!?$: Y'4"#%*,!$3?E!E"#%! 32:RwcB0&7N!$3%3ff8"0!$4-:3%3%#%)k8; "!$#%&B!E:1SBKR '@G#oe65@ @ @ >1K{` L] ] >1K{`~{`~{YM{`"] ] ] :M{`"- ,*- ,* ! - ,* ! 65@ @ @ >1K{`X] ] >1K{`~{`~{Y] ] :@- ,*- 7*,<@B @ +z |@= @BG@ B4] ^%]7^ Sz]7^@ ]7^ n| S) z | Se `]4^@e@BLY | ] %zA]|*]9za@B|]usxfi)nzG z@^-z]+|eBz]!o]|@B@ B +|z@BmoJXu7J QRJ:t:[uF8N*Q*HKJlWPYH>Z9v [4]! - 7,*Qs e c m0`A j SB ! SB - NJ8;#%C:96!$Y#\65{:8;#%Z#%QAB04#%8"0@ * @ ( YF@ 1 >1K{`N@ / >1K{`~{`~{YM{`"N ] * ] ( >1K{`~{`~{YM{`"N!$CX] 1 ] / ] 3 :M{`"Rkcd04#%r#\ jSB - NSB R7-d$ Kc Q0-`A SB - 7SB * N(7-''O :+0!EH0-:f8M0!$-:3\3%#%-)}8;4 "!$#%&H!E:WSBbRV#%C-:9H!,-${G:|[7!$3%d8;4 "!$#%&9N<@ B @ >AB0-:9:z |04!EB#%D-$dSBbR = A.N@ B !$C @ *k7 d0!`?$:.0-:!$*:H#%)32:9 mC*,!$#%ffNG R1#%C:9B0-:H8"0!$4-:3%3%#%)+8; "!$#\g :9 A:9:@B/!$C ]4^&RKcd0-:325lSB ?E!$3%7-:j<>$%]76^ #%SzR@#%*+#%3%!E"325$N[0-:j3%5fSB ?!$3%7:j<a$ ]76^ #\W0-:.8"04!$-:3%3%#\-)8; "!$#%&@ 1!$4C]7k^ #\n|-RS)V7-zY |RSeB:8;:`]4H^ 0!$b-+SB ?E!$3%7-:9RDcd0#\d#%B!]8;g M!$C#%8;#2x!$:9 A:9:@e0-:8"04!$-:3%3%#\-)+8; M!$#%gj!E":HSBKR r:8;:,!$3%3J-${G:|[7!$3%j8; "!$#%&j!E:SBbRffcPW0- 0-::9?$:9":$N7-''O :r0!EV0-:j-${G:|&74!$3%D8; "!$#\gV!E:jSBbR4"#%C-:9V!k8M0!$-:3\3%#%-)k8; "!$#%&9N+#N@0!Ed#%V-$DSBbR-cd0:ffN@:#20-:9 J#%V :9V +!$C j04!$ :3\#%*,#%!E :C]<>* #2C-*,!$#\ffN$$ #% :9!$C0!$ 1:3%#%*,#\!E :Ck<a* #%(C-*,!$#%^R V7-9Ng<>$ ^ :V:3%#\*,#%!E :C<>* 0-:C-*,!$#%m$< $Nff *:H$0-:9j'M#%*,!$3^?E!E"#%! 32:$N "!5AB0:9:N^#%K :9K -NffAB0#\8"0AB#%3\3ff:3%#%*,#\!E : f<>* 0:kC-*+!$#%i$<=#%48;:10-:H-${G:|[7!$3%b8; "!$#%&K!E:HSB MR r:8;:H#2#%b-$K'O# 3%:j l:9 ( W!$C H0!$ V:3%#%*+#%!E :Ci<>* #2KC-*,!$#\ffRro-$ k :H :9b@BLY | ] %zA]|*]9za@ B@B +|@B|]|@^@B])nzG z-z]Bz8%)+|efififfv2$ff6ff )T$fiffv~ffzN(!$3%30-:f$0-:9H?!$3\7-:6*k7? :C <a"* #2kC-*,!$#\ffN 7-10:9:f#\6-mA!`5Y x:* ?$::+:* $! &5i$<0-:1?E!$3%7-: #2)$)$:9b0!$ Z<>* 0:kC-*+!$#%i$< $N :89!$74 :1A:k04!?$:H!Eb* Y'4"#%*,!$3$?E!E"#%! 32:RwcB0&7N!$3%3ff8"0!$4-:3%3%#%)k8; "$! #%&B!E:1SBKR '7aDxw}x]!ot2sw {cd0-:H'":9?[#274B:"732K8;*'4!E:0:HC4# :9":gK*@C-:3%bAB#20x: 'O:8;K W0-:!$*7&K$<'4"7#%)!$8"04#2:9?$:CffRDyQ:689!$^N4<>$r:;p-!$*'432:$N-`A "732:j7B!+*@C-:3 3%#2L$:* +AB0-:W:<a$"89#\-)+SB #%8;:A:,)$:9 7 .!$6*6748"0Z'4"7#%)f!Ek32:.8; 6Z0-:]*[C:3 * R r`A:9?$:9N0-: :,":732.C}-$C#%#%-)7#%"0 :9FA:9:^N!5$N!b*@C-:3AB#%0.'"#\*,!$3-${G:|[7!$3%w8; M!$#%gN$!$&5j$<0-:V8;* #%-:C*@C-:3% * N ( $ 1 RyQ:j)$:9D0-:j!$*:j'4"7#%)1#%l!$3%3 <a7RwyQ:.89!$i!$CCl *:.C-:9!$#\3% k0: ::7432 5l8;*'4!EM#%-)0-:1!$5[*'4 $#%8 :0!?@#27-RNYS5leP$+Ro '(( MNAB0-:9":cd0:+:3%!E#%?$:f8; k$<B!$8M0#2:9?@#%-) jSB#%#\10-:f&74* :9k$<D?!E"#\! 32:1 /#%*:$R B A:9?$:9`N 5!$C#\/0-:#%C-*+!$#%]"# 9:$R(SB 9N@SB N[!$4CfSB !$#2?$:3%5,!EL$:!EL@#%-),!$C-?E!$g!E)$:6$<w0-:j<>748;#2!$3ff!E7:j$<8"0!$:3%3%#%-)H8; M!$#%gNA:.89!$}:C78;:j04#%V( b<>$r<a$ * RyQ:]' ?$:CZ#%Qcd0-:9$":*0!E jSBSB( !$C1 !$C(((SB!$C}0:#2j8; j!E:MNMNff!$Cd: 'O:8;#2?$:325$R.Sb 5@*' $#%8k!$!$325&G#%10-`Arj0!E1:<a$"89#\-)iSB0!$6!$ 5[*,' $#%89!$3%325X3%#%)0g325x*$:+8;10!$Q:<a$"89#\-)iSBRr`A:9?$:9N0!?@#%-)10-:bC47!$3-?E!E"#%! 32:8;73%C :r!$C-?!$&!E)$:97D#%+8; 748;#2,Ar#20,?E!E"#%! 3%:B!$C?E!$3%7-:+$MC-:9"#%-)x0-:7-"#\ #%899RWyQ:W!$3% x' ?$:C #%Ycd0:9$:*i0!E KSBSBSB(( MN( MN( MNP!$( B:' :8;#%?$:325$R6Sb 5@*'-GSB!$Cx0-:#2.8;.!E:C$#%8]!$!$325@#%1"0-`AB10!E10:]8"04!$-:3%3%#\-)W8; "!$#%&k!E:]*$":]8; 325Q04!$Z0-:f-${G:|&74!$3%8; M!$#%g1!$C "#%)]-i*$":'"7#\-)-Rjy 0:QA:,!$C4CQ-${G:|&74!$3%j8; "!$#\gjX0:,C7!$3?E!E"#%! 32:N[0:.`?$:9"!$3\3P!$ 5@*' $#%818; B#%d#%3%3 0-:1!$*,:.!$d0-:18M0!$-:3\3%#%-)H8; "!$#\gr!$32-:$N7-BA:H!$8"0#%:9?$:k*$":1'"74#%-)-RzFb#%d0-:9":9<a$:H!+*,[C-:3PA$"0m8;"#%C-:9"#%)-Row#%!$3\325$N4#%mcd0-:9EG(( MN( MN4!$C:*A:.'4`?$:C}0!E KSBSBSB!$Cl0:#2B8; r!E:(: 'O:8;#2?$:325$RSr)!$#%ffN&!$5[*'4 $#%8d!$!$325@#%/0-`Ar(0!E8"0!$:3%3%#%-)j8; "!$#%&/!E:b*$:8; 325]0!$f0-:j-${G:|[7!$3%D8; M!$#%gD!$4C "#%-)k-k*,$:K'"7#\-)-RJ~m!$#%&!$#%#%-)k)$:-:9"!$3\#% :C!E"8 G 8;"#% :8;5lW0-:6!$3%3 G C# O:9:&D8; "!$#%&B#%D!E)!$#\W0-:1*B8; 325$R+RoZ',*7e-'-sRoZ'!+Ro ' +RoZ'!-+Ro ' sRoZ' +RoZ'1:+Ro- 7!+RoZ'geL171+Ro,*7-651:+Ro,*- ,* !!-!,*-+Ro ' sRoZ'c Y8;893%74C-:$NV0-: :i:"732,0`A 0!E9Nd!$]*,#2)0& :i:;p['O:8; :CffNVA:}#% )$::9"!$3d)$:9f*$:'"74#%-)+#%<A:#%8;":!$ :60:H!$5[*'4 $#%868;9R6~i@C-:3%bA$0X8;#%C:9"#%-)f!E:H0-:6'4"#%*,!$3 -${G:|[7!$3%1*,[C-:3N ( kN!$CY0-:+'4"#%*,!$3(!$3\3 G C# O:9:&1*@C-:3 R /!$8M0n)#%?$:6!xC# O:9:&1!$*7&6$<'"74#%-)}!E!XC# O:9:&k!$5[*'4 $#%8l8; 9RZyQ:i*,#2)0&H!$3% Q8;#%C:9 * #% :!$C $<d0:f'4"#%*,!$3-${G:|[7!$3%*@C-:34#\8;:$NgAB04#%3% #2#%/!$ 5@*' $#\89!$3%3253%#2)0&325H*$:r:;p['O:#%?$:$N&#232:9/7 "!$48"0iC74!$3O?!E"#\! 32:9R7a sffw'x {Jfwutw}sw'{2yQ:1!E)!$#%i"!$W *:j:;p@'O:9"#%*:& ,:;p['32$:K0-:.#2)4#2_489!$8;:b$< 0: :j0-:9$:9#%89!$3P!$CW!$ 5@*'-G$#%8fC# O:9:8;:9Rmc ! 3%:])#2?$:k:7321n*:f#\ !$8;:6$<D0-: b32* "7432:9".'4 32:*7#\-)!+ !E#%8.?!E"#\! 32:j$"C-:9M#%-)-R/cd0:j:;p['O:9"#%*,:gd!E":6!E)!$#%m8;#% :&dAB#%0l0-:10-:9$":9#%89!$3:74329RDow#2" 9N:-<a$M89#%-) jSB x!$m!$3\3 G C# O:9:&B8; "!$#\gb!$8"0#2:9?$:b0-:k* r'"7#\-),!$C0!$w0-:d"*,!$3%32: w"74g#%*,:9R [:8;4CffNH0-: :D'4 32:*,w#%4 !$8;:9N:<a$"89#\-)jSBn0-: #%4!E5-${G:|[7!$3%8; "!$#%&!$8"0#2:9?$:(0-:D!$*:d!$*7&w$<4'M7#%-)b!$*,!$#%&!$#%#%)rSBnH0-:D8"04!$@G-:3%3\#%-)]8;4 "!$#%&9RHz Q!$C4C#2#2ffNff:<a$"89#\-)WSB Q0-:,8M0!$-:3%3\#%-)]8;4 "!$#%&j!EL$:13%-)$:96!171E8rfi 8-fiffffl!$8"0#2:9?$:$Rbcd0#2MCffN!$CC#%)0-:68M0!$-:3%3\#%-),8; M!$#%gr f0-:1'M#%*,!$3^!$3%32G C# O:9:gr8; "!$#%&C-[:d-$B#\8;:!$ :j'M7#%-)-N@!$Ci*,:9:325]!$C4CD`?$:9M0-:!$Ci ]0-:j"7&#%*:$R*@C-:3(0-:7-"#%#%8!E#%8!E#%8!E#%8!E#%8(<=!$#%3%aT0143K143Kcff:8ER<<3@Ri3$3@Ri3$3@Ri3$+171 < =cJ! 32:b32* V 0[N 7 b32* 143@Nk"7"$ K32* 171ENw,K$V. k6K32* [N<=!$#%3%:8ERe171714317171433@Rk7.3@Rk63@Rk7&<<ffffffff<>!$#%3\:8ERW,~1:7,~1:7efiff<>!$#%3\[R >&[Ri3$[Rw,K&:8ER$,r$3K$,r$3K7[Ri3$K@3 Rk718&, Rk0$,7 * :9b$< !$8L[ "!$8"L[ ><>!$#%3\ d!$4CX"7#\-),#%*:H W_44C}0-:_4" K 3%7-#2x W<>7-#\ !$8;:$<J0-: b32* "732:9"'" 32:*WR B74g#%*,:!E":b<>$Dz[3%?$:9 [R~$N J:&#%7* zz"z/'@8;: $`N!$C,~$< BSK~ZR1:K373 !e _7aT{t2x]w}Qt$sffwt$l {ffQt2 ff*@C-:30-:7-"#\ #%8>'>'>' KC>'=C( >' KC( >'( =C>' KC>'=C( >' KC( >'( =C(jx/jx/jx/ 0Ijx/jx/jx 0Ijxjxjx/ 0Ijx/jx/jx 0Ijxjx(((((((((((+1:E< =cJ! 32:,sww}b32* 171ENw,K$3 i3$&3 i3K7.j1437&770K3M1437.j143777K3$7>6776>.K373M1:&70"7&7K3M17107.7.K373M1:&701:7"607.6.707.6>"$,1:6,"707."7&7K3M171&K3$"<=!$#%3%3@Ri3>,eW3@Ri3M13@Ri3$&7WeeW7WeeW<=!$#%3%3 k07&3 w,K"3 k"$,1 \1:3 k&j13 k"7.1 \1:.3 k&K33 71 \1433 77&7700K3$&0j1OK3$"60K3$03@Ri3>,3@Ri3M1c:8ER@R@R@RER@R@RER@R@RER@R3@Ri3>,7"6171sw'{b32* V 0[N 7 K32* 143@Nk"7"$eW77.171" kK373V. k6b3%* [N<=!$#%3%:8ER@R@R77&77.17177&1:1:77"1:v2,X"j1: !)"K3$"77&7$,63$"77.7."K3$"77&7.777$,r"$,~1:"7.70j171eT1ER\18,eTff7$,63$"143$.$,171:7"70j1O>171:j1: 8ER[R[R@R@R@R@RER@R@R[R1:& k&$,:1 \1:3 k.7&K3 >3 k0K33 k.7&j1 k"63 k0j13 k&7&1:0 k7afiff3 Rk&70@j1ERkj13 Rw,63@<=!$#%3%&7&77.707&"707:8ER[R[R@R[R@R@RER@R7"7 k.7&7& k"73 >.1:. i3$3 k"73 >.7 1 \1:"3 k"j1:1 0[R >"3 R 7@3 R 7@>"7[& Rw,K"3 R 7@7 * :9b$< !$8L[ "!$8"L[ ><>!$#%3\ d!$4CX"7#\-),#%*:H W_44C}0-:_4" K 3%7-#2x W<>7-#\ !$8;:$<J0-: b32* "732:9"'" 32:*WR B74g#%*,:!E":b<>$Dz[3%?$:9 [R~$N J:&#%7* zz"z/'@8;: $`N!$C,~$< BSK~ZR1:K373 !e _,X"j1: !)v2,65" kK373yQ:x!$3% Z:;p@'432$:C 0:}!$C-?E!$g!E)$:x$<b*k732#%'432:l?[#%:9Ad'O#%gH$<K#% :8;#2 '4 32:*,H<>$+C-5&G!$*,#\8r?E!E"#%! 32:j!$Cl?!$3\7-:K$"C-:9"#%)k0-:7"#% #%89RJz{WcJ! 32: [NA:j)#2?$:.:7432<a$ K32* "732:9/1:8#bfififfv2$ff6ff )T$fiffv~ff' 32:*,9RryQ: :9?$:k0!EK0-:k'"#%*,!$3 !$3%3 G C# :9:&r*,[C-:3J#%b-$j8;*'O:9#2#2?$:kx0-:k3\!E)$:9' 32:*,9RKcd0-: :r"74g#%*,:r!E:H !$#\-:CxAB#20m0-:k8M0!$-:3\3%#%-)+8;4 "!$#%& =!$CQ!+'4"#%*,!$3!$3%3 G C4# :9":gd8; "!$#%& D7#\-)0-:6"*,!$3%32: dC*,!$#%i$r0-:6C7 32:.*+!$3%32: BC*,!$#%i0-:7"#% #%8$0] :9($<ff?E!E"#%! 3%:$/+0-:rC74!$3-?!EM#%! 32:9R :#%-).<>$"8;:C+ "!$48"0, 74 (0-:d'4"#%*,!$3?E!E"#%! 32:10&7-".0-: "!$8M0#%-)i0-:7"#% #%8ERfS C7!$3?@#2:9Ad'O#%&.!E''O:!E"j x O:960-: "!$8M0#%-)0-:7-M#% #%8r?$:95W#2)#%_489!$gd!$C?!$&!E)$:Bi0#%V' 32:*WR*,[C-:3((((((((((((E)0:7-"#% #\8>'>'>' KC>'=C( >' jC( >'( =C>' KC>'=C( >' jC( >'( =CMx @Mx @MxMx @MxMx 0IMxMxMxMx @MxMx 0IMxMx+1:E< =cJ! 32:/8,71OO1O1aWaWaWaWaWaWaWaWaW@' $"<>!$#%3\:8ERO1O13@Ri3M11OaWaWaWaW/K8V0$['O$<>!$#\3%1:" k7:1 & \:1>7.$,7,aTff1:.770707"60j143$7&77&7.[Rw,K1:.7[" Rk&71:7"77&7"7718,K.$,&Rkj10 Ri3$"[3 Rk&$,@18,&Rw,~11 Rk7&EaTffK3@Rk$,3 Ri3$@3 Rk6@K@3 Rkj13 Rk"7@K3M1:1O>7j1:777.:8ERaTff18,&Rkj1ff8d1718@' $"<>!$#\3%: 8ER[R[R1O$3$7.$,1:7.7&6>61O$3$7067>"77"1O>707.70171O>67.1:7.7&6>5$,r>.770&K373$&7.7&K, .[Rk"71:"j1ER\1:01:.7"6$3$.7>j1ER\1:07* :9D$< !$8L[ "!$8L@ ><>!$#%3\ !$CW"74#%-)k#%*:1 ,_4CW0:._" d3%7-#2l ]0-:9:#\ !$8;:K$<V 'O$.8M0-:C73%#\-)]'4 32:*WR B74g#%*,:j!E:<a$1z[32?$:9 [R }~$N J:g#\7* zzzO'@8;: $N!$4Cj~ Y$< BSK~ZR$S C!$"06*:!$4Jb 3\7-#2#\V<a74CW!E<a :9 .0-7-R1:K373 !e _'1:m"j1: !)m1Ev2" kK373z{ncJ! 32:[N(A:])#%?$:]:"7326<a$H0-:l 'O$6"8"0-:C743%#%-)i'4 32:* AB0-:n0-:9:l!E:f!$ @CC[7* :9H$<bA:9:9L@9R b: '4#2 :W0-:W<=!$8;,0!E,#%H04!$H0-:i -)$:,'$'4!E)!E $`N0-:l'M#%*,!$3V!$3%3 GC# O:9:&d*@C-:3^#%d$B8;*'O:9#2#2?$:j}0-:13%!E)$:9B' 32:*+9R(cd0-: : DM7g#\*:D!E:1 !$#%-:CAB#20,0-:B8"04!$-:3%3%#\-)j8; "!$#%&/!$4C "!$48"0#%)j,0-:d'4"#%*,!$3@$C7!$3-?E!E"#%! 3%:DAB#20,*+!$3%32:C-*,!$#\ffR SrAr#20 0-: b32* "7432:9H' 32:*iN :#%-)X<a$M8;:C "!$48"0 7 0-:W'4"#%*,!$3?E!E"#%! 32:10&7-".0-: "!$8M0#%-)i0-:7"#% #%8ERfS C7!$3?@#2:9Ad'O#%&.!E''O:!E"j x O:960-: "!$8M0#%-)0-:7-M#% #%8/?$:956#%)#2_489!$& !$C-?E!$&!E)$:H0#% ' 3%:*WR$ :d!$3% K0!Ewk0-:D3%!E)$: #%4 !$8;:$N0-:,*+!$3%32: j :!E"8M0Q :9:+#\K !$#%-:CXAB#%0m0:,8"0!$:3%3%#%-)l!$CX0-:,!$3\3 G C# O:9:&K8;4 "!$#%&9N"!$48"0#%)Z 0:}'"#\*,!$3D$lC7!$3d?E!E"#%! 32:iAB#20 *,!$3%3%: ]C-*+!$#%ffR c n8;893%74C-:$NdC-5@!$*,#%8"!$48"0#%)l0:7-"#% #\89b89!$n!E)!$#% :,#2)4#2_489!$&325m*$":: :8;#2?$:]AB0-:X0-:95Q32[$LX!E $0Q0-:'"#\*,!$3O!$CiC7!$3O?@#2:9Ad'O#%&9Rx#K=85fi 8-fiffffRfi? R^ ff$CMd1:07070$V0-:)b:9(!$3R( 7C#%:C6*@C-:3%3\#%-)r!$4Ck32?[#\-)r0: OG |&7-:9:4 '4 32:*WN!$4Ck!j[7-": :9 G%# -)1' 32:*q7#%).8"0!$:3%3%#%-).8;4 "!$#%&9R(cd0-:950`A 0!E8M0!$-:3\3%#%-).8; "!$#%&#%48;:!$ :0-:j!$*7&D$<w8; "!$#%&V'$'4!E)!E#%ffR(cd0-:95f8; {:8;7:j0!ED0-:K`?$:9M0-:!$CB!$" [89#\!E :ClAB#208"04!$-:3%3%#\-)+8; M!$#%grAB#%3\3ff'4!5} Yx' 32:*+dAB0#\8"0i":|&7#%:13%!E)$:H!$*7&b$</ :!E"8M0ffNO$32:!$Cl 0-M!$0#%-) :0!?@#27-`RwcB0-:95]!$3%H"0-`A 0!Ed8M0!$-:3\3%#%-)H8; "!$#\gV$'O:l0-:jC-[$D#%& :9: #%)]?E!$3%7-:H$"C-:9M#%-)l0-:7-"#%#%899Rjo-$.'O:9"*k7-!E#2x' 32:*,9NO!W#%*,#\3%!Er#%C:!]AV!$K':9?@# G73%5+'$'O :C 5 b:9:32:MR65K+d1:0707$VK373$$V0#-!$C ^:9:(<>[897:C0-:d 74C-51$<ff8;* #%-:C*@C-:3%w$< 'O:9"*k7-!E#2H' 32:*,9Rc 0-:#%J 74C-5.#%893%74C-:C1-$J325j0:'O:9"*k7-!E#2k8; M!$#%gN 7-w!$3% K!$3%3g0:$0-:9(8;4 "!$#%&$<V0-:,'" 32:*WRcd0-:#%.8;*'4!E"#%Z*:!$7-":+#%j!$Z:;p@ :#%Q$<V0-:'$'4!E)!E $H8;*'4!EM#%!E''"!$8"0k$< @8M0&73% :!$C [78"L$:95MNgAr0#%8"06*:!$7-":P0-:VC# O:9:& 8;* #%-:Ck*@C-:3%PAB#20: 'O:8;6 m0-:#%k! #\3%#2 5x x'"7-:+0-:] :!EM8"0 '4!$8;:fAB#20n8; "!$#%&1'4$'4!E)!E#2ffR B A:9?$:9`N0-:#2r*:!$7-:6#%d#%C:9' :4C-:gV$<w0-:13%:9?$:3^$<8;#% :8;5W*,!$#%&!$#%-:CWW0:68; M!$#%gr!$CC-:9'O:C 7-'O10: :9w$<48;$":8;'$'4!E)!E $M#% :!$CffRJcd0-:95.0:9$:9#%89!$3%3256C#%8; ?$:9J0:V8;M#2 :9"#%!7C:9fAr0#%8"0 *,#\#%*,!$3b8;* #%-:C *@C-:3%l0!`?$:Z0-:Z!$*:Q'4"7#%)Y'O`A:9}!$l<=73%3K8;* #%-:C*@C-:3%D!$Ci:*'4#2"#\89!$3%325,C-:*4 "!E :.0-:1:732V}C# O:9:gD'O:9"*k7-!E#2l'4 32:*,R/)fiVK373$$cVK373M18eV!$898"0[7:9/!$3 R(<a$M*,!$3%325k 7C#2:Ck0-:B: :8;#2?$::/$< A6*,[C-:3\3%#%-)r :8"0#\|&7-:(0!E" !$<a$"* !r@G #%!E"5j }#%& r!$6:|&74#2?!$3%:g #%!E5. JN`4!$*:325$NE0-:C47!$3 "!$4 <a$M*,!E#2!$CY0-:]0#\CC-:X-:$RiSbZ$"#%)#%!$3(*@C-:3$<D0:+' 32:*WN #%1C7!$3/!$CY#260#%CC:X "!$ <>$ G*,!E#24k!E":l8;*,'4!E:C AB#20n: 'O:8;H X0-:l'O:9<a$M*,!$8;:+$<b!X&7* :96$<b32@89!$38;4#% :8;5:8"04#%|[7-:,#%893\7C#%-)X!E"8 G 8;"#% :8;5$Nd!$C AB#20 : 'O:8;, n0-:}8M0--32$)#\89!$3 !$8"L& "!$8"L[#\-)!$32)$$"#%0*WNo bN!$Ci~}SBKR)SM_Sj_d1:07070$$:9K!$Cxcd!$-)bC-:9?$:32$'O:Cm!+<>"!$*:9A$Li<>$b5[ :*+!E#%86*,[C-:3J :32:8;#%ffRBcd0-:95C-:*4 "!E :C 0-:#2!E''4!$8"0 0-:W:9?E!$3%7!E#%n$<b!$CC#%)x!X8;:9!$#% 893\!$H$<r#%*,'43%#2:Cn8;@G"!$#\gk Q!$ $"#2)#%!$3*@C-:3 RQcd0-:f:9?E!$3%7!E#2 0-:7-M#% #%8,74 :C #% !$ :C !$ :;p@ :#%n$<0-:j0-:9$":9#%89!$3ff8;*'432:;p-#2F5+: #%*+!E :D'$'O :C 5!$C-:3MRcB0-:#2V:;p['O:9"#\*:g!$3 :"7320- 04!EB0-:k!E''!$8M0m#%d'*,#%"#%-)-R B A:9?$:9NOAB#%0W0#%r!E''!$8M0}-:k-:9:CB0-:6#%4 !$8;:C!E! :.!$i:;p['43\#%89#2D#%-'47 ,0:1*:90-@C9R=efi;wqd1:070K3>M?FTE65yQ:Q0!?$:x'O:9<>$"*:C !$ :;p@ :#2?$:X 7C-5 $<.C47!$3B*@C-:3%3\#%-)Q ' :9M*67-!E#% !$C #% :8;#2' 32:*,9Rc ,8;*'4!E:.*@C-:3%N[A:1C-:9_-:Cl!*:!$7-:j$<w8; M!$#%gD#%)0g-:"D'4!E"!$*:9 :9M# 9:C5 0:}32:9?$:3d$<.3%[89!$3d8;4#% :8;5 :#%-)Q:<a$"8;:C^R o-$f'O:9"*k7-!E#2 '" 32:*,,!$4C :-<>$"8 G#%-)m!E"8 G 8;#% :8;5$NwA:]'`?$:Cn0!Ek!}#%)32:'"#%*+!$3(!$3%3 G C# :9:&.8; M!$#%gk#%j#2)0& :9k0!$8"04!$-:3%3%#\-)18; M!$#%gN 7/0!ED8"04!$-:3%3%#\-)18; M!$#%g!E:K#2)0& :9V0!$+'4"#%*,!$3-${G:|&74!$3%8; M!$#%gRrcd0-:k:!$ m<>$b04#%BC# :9:48;:6#%r0!Eb0-:6'4"#%*,!$3^${G:|&7!$3\b8;4 "!$#%&KC-:9 :8;#%)32:9 l?!EM#%! 32: =# R :$R-0-:j?!E"#\! 32:VAB#20l!#\-)32:K?!$3\7-: MN-0-:j8"0!$4-:3%3%#%)k8; "!$#%&dC-:;G:8;(#%)32:9 1?E!E"#%! 32: !$4Ck"#%-)32:9 6?!$3\7-: =# R :$RE0- :?!$3%7: Ar0#%8"06@89897-#%10:VC*,!$#%6$<!#%)32:K?!E"#\! 32: MN-AB0#\3% 0-:.'"#\*,!$3O!$3%3 G C# O:9:&D8; "!$#%&dC-:9 :8;r)32 !$3^8;4#% :8;5 >AB0#\8"0#%893\7C-:k#%-)32:9 ?E!E"#%! 3%:9N(#%-)3%:9 n?E!$3%7-:!$4C *,!$g5n$0-:9,#274!E#2 MR o-$+32 A:9,32:9?&G:3%D$<32@89!$3^8;"#% :8;5 >:$R )-R04!EB*,!$#%&!$#%-:C 5f<a$AV!E"Ci8M0-:8L@#%-) MN 8"0!$4-:3%3%#%)H8;4 "!$#%&:*,!$#\ #2)0& :9+0!$ '4"#%*,!$3V-${G:|[7!$3%]8; "!$#%&9R B A:9?$:9ND<>$]8;:9"!$#% 0#2)0:9+32:9?$:3%$<rr%e88fififfv2$ff6ff )T$fiffv~ff32@89!$3ff8;#% :8;5f3\#2L$:j'4!E0W#%&?$:9" :.8;"#% :8;5$N48M0!$-:3\3%#%-)H8; "!$#\gd!E":1#%8;*'!E"! 32:b'"#\*,!$3O-${G:|&74!$3%D8; "!$#\g9Ro-$B#% :8;#2l'" 32:*,9N[A:.' ?$:CW0!E9N-AB#20l": 'O:8;D ,!E"8 G8;#\ :8;5$N!}#\-)32:'"#\*,!$3(!$3%3 G C4# :9":g.8; "!$#%&6#%1#2)0g :9k0!$n8"0!$:3%3%#%-)i8;4 "!$#%&$)$:90-:9AB#20k0-:dC7!$3@-${G:|[7!$3%8;4 "!$#%&9N 7J04!E0-:D8"0!$:3%3%#%-)K8; M!$#%g!$3%-:d!E:!$#2)0&V!$0-:b'"#\*,!$34-${G:|[7!$3%V8; "!$#%&9R(cd0-:b!$ 5[*,' $#%8K!$!$325@#%!$3%32`A:Cl7 k":C78;:<=7-0-:9d0:6[7* :9r$<w*,[C-:3\d0!Eb*,#2)0& :.A$"0m8;"#%C-:9"#%)-R (p['O:9"#%*,:g!$3ff":732Dx!AB#%C:/"!$-)$:V$<' 32:*+P7-'4' $" :C10-::0:9$:9#%89!$3@:74329R o$:;p@!$*'32:$N!$CC#%) #%4!E5j-${G:|[7!$3%.8; "!$#%&j i0-:,8M0!$-:3%3\#%-)l8; "!$#%&1C&:.$.#%8;:!$#\-)l'"7#\-)-Nff!$CZ*:9:3%5!$CC.`?$:9"0-:!$CY i0-:,M7g#\*:9R B A:9?$:9`N 0-:+:;p['O:9"#%*,:g!$3w:"732j!$3% iC:* "!E :CY0-:?$:95#2)4#2_489!$& :-:9_4w$< :#%)K! 32:D "!$48"0 $0'4"#%*,!$3@!$CC74!$3@?!E"#\! 32:9RJz +*,!$g589!$ :9NBA:m !$#%-:C 0-: : ,M7g#\*:AB#20 74 +8"04!$-:3%3%#\-)Q8; "!$#%&f!$C ! "!$8M0#%-)0-:7-M#% #%8r0!EB32[$L$:C}!E $0i'"#%*,!$3O!$CWC47!$3O?[#%:9Ad'O#%gR65f/e50!E,)$:-:9M!$3B32:89!$ :i32:!E"&<>* 0#%+ 7C-5Uow#2M 9N/0-:9":}!E:}*,!$&5 ' "# 32:*@C-:3%j$<V:9?$: !}#\*'432:H' 32:* 3%#2L$:,_C#%-)W!W'O:9"*k7-!E#2Y$6!$n#% :8;#2ffR+z{Z!$CC4#2#2ffN-]-:6*@C-:3 #% : B#\}!$3%3 #27!E#249RVyQ:60-:9":9<a$:k-:9:Cm f7''O$D0-:674 :9r#%m*@C-:3%3%#%):9?$:Q#%*,'432:j' 32:*,9R [:8;CffNff#2r$<a :x'4!`5[b W8; "748;r:C47C!$&r*@C-:3%rAB#20m*67432#2'432:?@#2:9Ad'O#%&,$<10:x!$*,:m'" 32:*WR b: '4#2 :x0-:x`?$:9"0-:!$C49Nr0:x! #\3%#2 5 M!$8"0 C7!$3?E!E"#%! 32:89!$ :W?$:95 :-:9_489#%!$3 R "!$48"0#%)Z0-:7-"#%#%89H0!E]8;4#%C-:9,*k732#2'32:l?[#2:9AB' #\g89!$ :x?$:95 : O:8;#2?$:$R cd0#%"CffND0-:Z!$CC4#2#2!$3b8; "!$#\gl'$'!E)!E#2 ' ?[#\C-:C 5 )32 !$38; M!$#%g63%#2L$:+!$3\3 G C# O:9:&.*,!5Z-$ 7#2<a5x0-:#2k8; 9RlyQ:]$<> :Y"!A :9 :9k'O:9<>$"*,!$8;:AB0-:fA:K0-:9A 7-V0-:j!$3%3 G C# :9:&8;4 "!$#%&9R(o-7-0^N-7-D*:!$7-:b$< 8;4 "!$#%&V#2)0g:89!$ :j7 :Cf +8;*'!E:.C# O:9:&V8;4 "!$#%&d*@C-:3%9R r`A:9?$:9N40#\*,:!$7-:.89!$l3%5+: :8;8;:9!$#% *@C-:3% 0-: !$"#%0!E]0-:95 !$CC ?$:9"0-:!$CffR yQ:X #%3%3D*k7 ,M7 :;p@' :9M#%*:&C-:9 :9"*+#%-:j#2<P0:.!$CC#2#24!$38; "!$#%&D'$'4!E)!E#2i'`?@#%C-:C 5+#%)0g :9b*[C:3%V#%VA$"0W0-:8; K$</04#%r8;4 "!$#%&r'4$'4!E)!E#2ffR32#%*,!E :3%5$N 0:k|[7-: #2 :#%-)f!$CC-::Cm#%K8;:& "!$3J*,!$&5]'4 32:*,D#\W!E#2_489#%!$3ff#\g :3%3\#2)$:8;: 0-:j "!$C:;G:9FA:9:m :!E"8M0}!$CW#%<a:9:48;:$R65)x5ge<5K$C !? R4-D Rfi=0R)bRre 4#%8"0X!$CQcjROyZ!$3%"0Q!E:,897-:&325}"7-''O$ :C 5 @89#2:8;:o-74C!E#2Xz :3%!$C @oJzub!$C!$nz E2v $<> AV!E:f)$"!$&9R}cjR^yZ!$3%"0YAV!$6!$3\ }7-'4' $" :C 5Z!$2fq_ !$C-?E!$8;:Cn: :!E"8M0<>:3%32`Ar0#2'^RyQ:k0!$-Ll0-:1$0-:9r*:* :9"d$<w0-:kS_nff": :!E"8"0m)$7-'=0g ' < ff AdAdA.R C89R {G!$CffR !$8ER 7L !E'O:I,<a$f0-:32'<=73DC#\897#%9N!$4C : 'O:89#%!$3%3%5Yz{!$/b:&+Ar0-Z:8;7-M!E)$:C 7+Ad"#% :j0#%V'4!E'O:9RR R$ R R2)V!$898"0[79NEoR2NV0-:ffN6R2N?E!$ ):9:9L Nr_JR2N ? yZ!$3%0ffN`c.RVK373$$MR~)V#%4!E5r?@9R = @G})V#%!E"5j4 "!$#%&9RPbN*Q nF4Q uEvIJN};[ v>vwQ [4JFI[9Nd1 G $MN-1 ~$,&R): 5 !E`N.R2N ? ~}!$&5!@NoR VK37373>MRS[32?@#%-)H0-:."7Cl #\]'4 32:* 7"#%-)H'$'O#2#%!$3O32$)#%8ERz{ PdH8FI[`"[ sKQRJ KL HYU NRfiff6u7N*QVHKJ uEv H7J:U4[4PY[4JFI[HKJ PbN*Q nF:Q=uEvIJN}[;v>vwQ[4JFI[9N(''^Rn7&7 ~7&7&[RSbSbSBz (_ ": cd0-:1~}z{ca(_ :"9R):#V:9y :$NKR2N@~}: :9)7-:9NMJ_ R2No-:74C-:9Njdf R2NT? !E!@N R@d1:07070$M RnKv ]<>$A!EMCf8M0-:8L@#%-)H<>$D-@G!E`N-R-*/f CffRwM N PdH8FI[`"[ sKQRJ KLcHYU $qQ UbNR IJfiN}[4PIJ u7N*Q*H7J uEv#%!E5}8;4 "!$#%&.!E#% <=!$8;#2ffR6z g!6O8 Ufi 8-fiffffH7J:U4[4PY[4JFI[pHKJ PbQTJF:QiW v [OL,u7Js P u$F:N*QVFI[HYU HKJMLON*P{uKQRJfiN PdH $P{u7]m]QRJ ; NP'4'^R.7.143$[R = S'18,~1:[R['"#%)$:9R)$:9 9N -R2N%? cd!$-)-NfiBf R d1:07070$M RdS 8;& :;p[b<a$b8; "!$#\gB"!E#% <=!$8;#2i'" 32:* <>$"*6743%!E#2:32:8;#2ffR H7JMLbN*P uKQRJfiNRML NN7070 ~7$&,RV0-:)-N~b) R2N4V0-# NR2NM^ :9:$N -R2NT? yZ7ffN -Rd1:07070$M Rz 48;:!$#%-)k8; "!$#\gV'$'!E)!E#2 5,":C7@GC4!$gB*,[C-:3\#%-)T<-!$W:;p@' :9M#2:8;:K:9'O$9R HKJ LbN*P uKQTJNRML N -N1:&$, 1:07[RV0-#N(KR2N ? ^:9:$N-R VK373$$M R Kv 0-:W'"74#%-) :04!?@#27-H$<b*+#%#%*,!$38;* #%-:C *@C-:3%k<>$'O:9"*k7-!E#2HSj_ 9R$z PYH:FI[I[ sKQRJ 7LSHYU ff .ff HKP C8LI~HIWXHKJ S[VUOH7PIH] t@v u$N*QTJ HKJ LbN*P uKQTJNu7N*QhLR;U u>F:N*QVHKJ PYH>;Z v\[O]mL%H A/uKP s6L M6LON}[O,] u7N*QT;L u7N*QVHKnJ uKJ tMNHK,] u7N*Q*H7J Rxb: "7-5@-:$N .R2N ?%) :#V:9y :$N&bRMd1:070$,7M R@@ *:V'"!$8;#%89! 3%:_43% :9"#%-)r :8"0#\|&7-:J<>$w0-:D8; "!$#%&"!E#% <=!$8;#2 ' 3%:*WRmz{ PYH:FI[I[ sKQRJ 7LpHYUcNRj[ ff7NR fi L MNJ''^R 1: > 18&, R/z{g :9"4!E#2!$3#%&r-<>:9:8;:.iSr#2_89#%!$3Oz & :3%3%#%)$:8;:$Rb:9:32:^N~_JRd1:0707$MRExK7!$3?@#2:9Ad'O#%&/0-:7-"#\ #%89(<>$ #%!E58; "!$#%&!E#\ <>!$8;#%+'" 32:*,9Rffz{PdH8FI[`["sKQRJK LcHYUsNRj[ NR L MNff''PRj1 ~7"[REf/7$'O:!$Y-<>:9:8;:ZSr#2_489#\!$3 z{& :3%3%# G)$:48;:$R4y#%32:95$Rb:&9Nz RVK373$$MR/SBM8j8;#% :48;5f#%p@Sc.RJz{l?E!$ er!E"*,:32:ffNoR*f/CffRwMN PdH8FI[`["sKQRJKLmHYU Lff .ff N ''^R1:j1 1:7"[ROzYv c_(:Rb:&9NEzMR2N$[ :9)#27^NR2N ? yZ!$3\0ffN c.R~VK37373>MRfixb:8;*'O! 3%:8; "!$#%&9R PON*Q nF4Q uEv IJfiN}[;v>v Q ~[OJFb[;Nff d1 G $MNg1:7 1:"7&[Rb:&9Nz R2N U? yZ!$3%0ffNcjRBd1:07070$MR V '43%# < ! :8"04*,!EL 3%# M!E5Y<>$f8;4 "!$#%&9R cP:8M0ffR:9'^R2NB_qf }3$0 `1:07070ff fi fi!#"$%ff& &R6*f w PdH:Fb[`["sKQTJEKLmHdUH$-Q UONR IJfiN}[OPbJu7N*QVHKJuEv HKJU:[OPu[OJ FI[/HKJ PIQRJF4QkW \[bL 7J P $F:N*QVFI[+HdU HKJMLON*P 7QTJfiN PYH 7P uK]]QTJ' ;( N''^R >.K3 >>.j1ER= '18,~1: @eb!E"!$3%#%8"LNE.R2N? /f 3%3%#%$9NkR d1:07.K3>M R z 8;":!$#%-)j :9:b :!E"8M0+:9e+89#%:8;5H<a$V8; "!$#\g/"!E#% <=!$8;#2'4 32:*,R PbN*Q nF4Q uEvIJN};[ v>vwQ [4JFI9[ N-N 7&7 ~j1:[Rer:g :-"5[8"LNJ_ R R2Nbx :9?@#%3%32:$N*)kR2N ?Tc :-)-NPbREd1:0707$M RHS K:-:9"#%8kSBM8V#% :48;5iSr32)$$M#20*!$4Ci#2['O:89#%!$3\#!E#29R PON*Q F4Q uEvIJfiN}9[ vav Q ~[OJFb;[ N+N 70j1 ~7jE1 R$7"C!$ffN -R d1:0707"$M R HKJ9F tjPbPY[4JfiN HKJ LbN*P uKQTJN t[vN*QkW v [ H s;[ vwLcQRJ -, uKJ ., uKEJ $tuV[OLH A/uKP sQu PYH 7P uK]]QTJ [:NR~H s$EH v H =Z H s>9[ vwQRJ R+/_ 0ffRikx Rff0-:#\9Nrx :4#%Kx #%C-:9$ # G?$:9M#2 5$N w_ !E"#% zzMR(Sr?E!$#%3%! 32:j!$KD~ G S& G 0j1 G`1:K@3 R~m#%3%32:9N -RVK373$$M R!$-)$<>$"C '" 32:*WR2RPv 3%#%: C-:8;"#%'#2 !`?!$#\3%! 32: !EKc :8"04#%89!$3w:9'O$jS[G G[R+SB?E!$#%3%! 32:6<>*0$ :9d?$:9"#%i!E''O:!E"d#% g! O!E`N -R ffC R MNv kuu{u[R ['4"#%-)$:9Rff0/1//2fi$3 ff"44%156178R_ nd1:0707$ XvPdH8FI[`[ KQRJKLcHYU+NRj[ NR=~}$"#%9N JRMR KZ0-:+C-:"#2 5m$<V3%7-#2j#\Q:|&74#%3%# M#%7*3%:*WRHz"sff6u uEvSb @89#%!E#2i<a$BSr#2_89#%!$3Oz & :3%3%#%)$:8;:$R7N*Q*H7J) nd1:070K3> H9 :;<OWM[OPON ='O#%&K<>$10-:,|[7-:9:1' GMNff''^R[RwSr*:9M#%89!$HKJ8U4[4PY[4JFI[HKJ<@>7. >>7!$C-:3 N bRMR B:9': :&!E#2n :32:8;#2Q<>$18; "!$#%&.!E#% <=!$8;#2 PST89!$ :+7C-5m74#%-)-G K7-:9:9R=N EN[R1:& ~7_(" :9N>_JR2N [ :9)#27^NR2NE? yZ!$3%0ffN[cjRVK37373>MRg@#%-)32:9 +8;#% :489#2:9ROz lxb:8"0& :9NT.R*fCffRwMN7NR IJfiN}[4PIJ u7N*Q*H7JuEv HKJ8U4[4PY[4JFI[HKJ PIQRJF:QiW v\[bjL uKJ P u$F:N*QVFI[OLHYU H7JMLbN*P uKQRJfiN PdH ${P u7]m]QRJ. R = 1:.706- R@[ '4"#%-)$:9 G :9M3%!E)-Rff N4'4'^Rfi7"7 ~7&7[U#cfififfv2$ff6ff )T$fiffv~ff:9 )#%^N R~d1:0706MR-S _432 :9"#\-)d!$32)$$"#204* <>$J8; "!$#%&P$<C# :9:48;:/#%k j_/9Rgz PYH:FI[I[sKQRJKL HdUNRM[_ ff NR ff6u7N*Q*H7JuEv HKJ:U:[OPu[OJ FI[HKJ MNg''^Rj7&7 ~7&$,&R4Sb*:9"#%89!$fSr" [89#\!E#2+<>$DSB"#2_489#%!$3z{& :3%3%#2)$:8;:$R:9 )#%^N?-VK37373>cx*f w PYH:FI[I[ KQRJ7LHdU KNR IJfiN}[OPbJ 7N*QVHKJ H7J:U4[4PY[4JFI[H7J PIQRJF:QiW \[bL 7J P $F:N*QVFI[HYU HKMJ LON*P KQRJfiN dP H $P 7]m]QRJ ff $7.6 ~707" =1:.706@8M0&73% :$NbR2N ?%[ 78L$:95$NKJ_ RVK373M18M R[y 0-:6C- 7C4ff!$C1C-*+!$#%j'$'!E)!E#2132:!$C. B0-:!$*::!E"8"0m '4!$8;:$Rz{X[C-:9)!$!EMCffN 1e R *f CffRwM N PYH:Fb[`[ s7QTJ 7LmHdU {P IJfiN}[4PIJ u7N*Q*H7J uEv H7J:U4[4PY[4JFI[H7J PIQRJF4QkW v\[b+L uKJ {P u$F8N*Q*Fb[cHdU[u9F v2uKP u7N*Q K[ PYH ${P uK]]QRJff (NP''PRE171:" 1:7[& RSBd~(/_ :9R@*,#%0ffNb) R2Nq[ :9)#27ffNR2N,? yZ!$3%"0ffNJc.RVK37373>M R #%-)x!$7@p@#\3%#%!E5x?!EM#%! 32:k!$C #%*'43%#%:CQ8;@G"!$#%&P K*@C-:3g--G #%!E5b' 32:*,9Rz{ PdH8FI[`"[ sKQRJ KL HdU NRj[ KNR ff1u7N*QVHKJ u$v H7J:U4[4PY[4JFI[H7J MN''PR@1:.7 1:.$&, RffSb*:9"#%89!$}Sr[89#%!E#%W<a$rSB#%_489#%!$3 z & :3%3%#%)$:8;:$R@*,#%0ffN b) R VK37373>M RB~i@C-:3%3\#%-)!sJ_ :9"*k7-!E#2!(_ 3%:*WR/z{ PYH:Fb[`[ s7QTJ 7LHdU L ff HKP CrLI~HIW HKJ H s>;[ v>v QTJ fuKJ EH v 8QRJ PdH;Z v\[O]mL AgQ*NR H7JMLbN*P uKQRJfiNRML RSb3% 6!`?!$#\3%! 32:b!$ B: :!E"8M0B:9'O$D<a"* 0g ' < ff8;*WR 0[7CffR !$8ER 7-L !6 8;* *, '4!E'O:9"9R 0&*,3 R[ :9)#%7ffN=R2N,?hyZ!$3%0ffNwcjR d1:07070$M RZcd0-:]C4# :9":8;:]!$3%32G C#O :9:8;:]*+!EL$:9Rmz{ PYH:FI[I[ sKQRJ KLXHdU1KNR fi L MN-''^RM 1O > 1:[0 ROz{g :9M!E#2!$3 #%grV-<a:9":8;:.iSr#2_489#\!$3Oz & :3%3%#2)$:48;:$RyZ!$3%0^NcjRVK37373>M R@Sch? SjB_ R z #rx :8M0g :9`N 1R*f CffRwM N 7NR IJfiN}[OPbJ u7N*QVHKJ u$v HKJ8U4[4PY[4JFI[ H7JPIQRJF:QiW v\[bHL uKJ {P u$F8N*Q*Fb[bLmHYU HKJMLON*{P u7QTJfiN PYH 7P uK]]QTJ ff N''PR 7 1 >>"7[& R =1:.706- R[ 'M#%-)$:9 G :9"3\!E)-RR2NB7-:0-:9`NO~YRMRkS )32 $! 3w8; "!$#\gj8;* #%#%)]!l7* 8; M!$#%gj!$CXC#%<\G<>:9:8;:K8; M!$#%gRffz b:8"0& :9N .R /CffR MNuuEvv ruu{u{u[RN&'^' R-R ['M#%-)$:9RU!fiJournal Artificial Intelligence Research 21 (2004) 499-550Submitted 08/03; published 04/04Compositional Model Repositories via Dynamic ConstraintSatisfaction Order-of-Magnitude PreferencesJeroen KeppensQiang ShenJEROEN @ INF. ED . AC . UKQIANGS @ INF. ED . AC . UKSchool Informatics, University EdinburghAppleton Tower, Crichton Street, Edinburgh EH8 9LE, UKAbstractpredominant knowledge-based approach automated model construction, compositionalmodelling, employs set models particular functional components. inference mechanismtakes scenario describing constituent interacting components system translatesuseful mathematical model. paper presents novel compositional modelling approach aimedbuilding model repositories. furthers field two respects. Firstly, expands application domain compositional modelling systems easily described termsinteracting functional components, ecological systems. Secondly, enables incorporation user preferences model selection process. features achieved castingcompositional modelling problem activity-based dynamic preference constraint satisfactionproblem, dynamic constraints describe restrictions imposed compositionpartial models preferences correspond user automated modeller.addition, preference levels represented use symbolic values differorders magnitude.1. IntroductionMathematical models form important aid understanding complex systems. also helpproblem solvers capture reason essential features dynamics systems.Constructing mathematical models easy task, however, many disciplines contributed approaches automate it. Compositional modelling (Falkenhainer & Forbus, 1991; Keppens & Shen, 2001b) important class approaches automated model construction. usespredominantly knowledge-based techniques translate high level scenario mathematicalmodel. knowledge base usually consists generic fragments models provide onepossible mathematical representation process occurs one components.inference mechanisms instantiate knowledge base, search appropriate selectionmodel fragments, compose mathematical model. Compositional modellingsuccessfully applied variety application domains ranging simple physics, variousengineering problems biological systems.present work aims compositional modelling approach building model repositoriesecological systems. ecological modelling literature, range models devisedformally characterise various phenomena occur ecological systems. example,logistic growth (Verhulst, 1838) Holling predation (Holling, 1959) models describechanges size population. former expresses changes due births deathslatter changes due one population feeding another. compositional model repository aimsc2004AI Access Foundation. rights reserved.fiK EPPENS & HENmake (partial) models generally usable providing mechanism instantiatecompose larger models complex systems involving many interacting phenomena.Thus, input compositional model repository scenario describing configurationsystem modelled. sample scenario may include number populations variouspredation competition relations them. output mathematical model, calledscenario model, representing behaviour system specified given scenario. setdifferential equations describing changes population sizes aforementioned scenariodue births, natural deaths, deaths predation, available food supply competitionwould constitute scenario model.application domain poses three important new challenges compositional modelling.Firstly, processes components ecological system representedresulting composed model depend one another ways described. population dynamics example, models describing predation competition phenomenatwo populations rely existence population growth model populationsinvolved phenomenon. inhibits conventional approach searching consistent adequate combination partial models, one component scenario.approach provides adequate solution physical systems comprised components implementing particular functionality described one multiple partialmodels. Although seminal work compositional modelling (Falkenhainer & Forbus, 1991)recognised existence complex interdependencies model construction general,provided partial solution it: conditions certain modelling choicesrelevant specified manually knowledge base.Secondly, domain ecology lacks complete theory constitutes adequate model.existing compositional modellers based predefined concept model adequacy.employ inference mechanisms guaranteed find model meets adequacy criteria.However, criteria determine adequate ecological model may vary ecologicaldomains even ecologists require model within domain. Therefore,compositional modeller requires facility define properties generated ecologicalmodels must satisfy.Thirdly, possible express criteria imposed scenario model termshard requirements. Often, ecological models describe mechanisms behaviours partially understood. cases, choice one model another becomes matter expertopinion rather pure theory. Therefore, ecological domain, modelling approaches presumptions are, extent, selected based preferences. Existing compositional modellersequipped deal user preferences paper presents first compositionalmodeller incorporates them.Generally speaking, three issues tackled paper means methodtranslate compositional modelling problem activity-based dynamic preference constraintsatisfaction problem (aDPCSP) (Keppens & Shen, 2002). aDPCSP integrates conceptactivity-based dynamic constraint satisfaction problem (aDCSP) (Miguel & Shen, 1999; Mittal &Falkenhainer, 1990) order-of-magnitude preferences (Keppens & Shen, 2002).attributes domains aDPCSP correspond model design decisions, constraints describing restrictions imposed consistency requirements properties order-of-magnitudepreferences describing users preferences modelling choices. translation method bringsadditional advantage compositional modelling problems solved means500fiC OMPOSITIONAL ODEL R EPOSITORIESefficient aDCSP techniques. such, compositional modellers benefit recent futureadvances constraint satisfaction research.remainder paper organised follows. Section 2 introduces conceptaDPCSP, preference calculus suitable express subjective user preferences modeldesign decisions integrated general framework aDPCSPs. also givessolution algorithm aDPCSPs. Next, section 3 presents compositional model repositoryshows aDPCSP employed automated model construction. theoreticalideas illustrated means large example section 4, applying compositionalmodel repository population dynamics problems. Section 5 concludes paper summaryoutline research.2. Dynamic Constraint Satisfaction Order-of-Magnitude Preferencessection, preference calculus based order-of-magnitude reasoning introduced integrated activity-based dynamic constraint satisfaction problem (aDCSP) form aDCSPorder-of-magnitude preferences (aDPCSP). Then, solution algorithm aDPCSPspresented. theory illustrated examples compositional modelling domain.2.1 Background: Activity-based dynamic preference constraint satisfactionhard constraint satisfaction problem (CSP) tuple hX, D, Ci,X = {x1 , . . . , xn } vector n attributes,= {Dx1 , . . . , Dxn } vector containing exactly one domain attribute X.domain Dx set values {di1 , . . . , dini } may assigned attributecorresponding domain.C set compatibility constraints. compatibility constraint c {xi ,...,xj } C definesrelation subset domains Dxi , ..., Dxj , hence c{xi ,...,xj } Dxi . . . Dxj .solution hard constraint satisfaction problem tuple hx 1 : dx1 , . . . , xn : dxnattribute assigned value domain: xi X, dxi Dxi ,compatibility constraints satisfied: x{xi ,...,xj } C, hdxi , . . . , dxj c{xi ,...,xj } .activity-based dynamic CSP (aDCSP), originally proposed Mittal Falkenhainer(1990), extends conventional CSPs notion activity attributes. aDCSP,attributes necessarily assigned solution, active ones. such, attributeeither active assigned value inactive:xi X, dxi Dxi , xi : dxi active(xi )activity attributes aDCSP governed activity constraints enforceassignments attributes, assignment another attribute relevant possible. informationimportant dictates attributes value must searched, alsoset compatibility constraints must satisfied. Clearly, compatibility constraints501fiK EPPENS & HENc{xi ,...,xj } C attributes xi , . . . , xj active must satisfied, hard CSPsub-type aDCSP attributes always active.summary, activity-based dynamic constraint satisfaction problem (aDCSP) tuplehX, D, C, Ai,hX, D, Ci hard CSP,set activity constraints. activity constraint restricts sets attribute-valueassignments attribute active inactive:axi ,{xj ,...,xk } Dxj . . . Dxk {active(xi ), active(xi )}xi 6 {xj , . . . , xk }.solution activity-based dynamic constraint satisfaction problem tuple hx 1 :dx1 , . . . , xl : dxlattributes part solution assigned value domain: x{x1 , . . . , xl }, dxi Dxi ,activity constraints satisfied:axi ,{xj ,...,xk } A, xj 6 {x1 , . . . , xl } . . . xk 6 {x1 , . . . , xl }xi {x1 , . . . , xl } hdxj , . . . , dxk , active(xi )i axi ,{xj ,...,xk }xi 6 {x1 , . . . , xl } hdxj , . . . , dxk , active(xi )i axi ,{xj ,...,xk }compatibility constraints satisfied:c{xi ,...,xj } C, active(xi ) . . . active(xj ) hdxi , . . . , dxj c{xi ,...,xj }2.2 Order-of-magnitude preferences (OMPs)Although aDCSP capture hard constraints decisions given problem welldynamically changing solution space (as described activity constraints), representation scheme employs take account preferences users may possiblealternative value assignments. Therefore, work extended allow preference informationattached attribute-value assignments. way achieved dependsrepresentation reasoning mechanisms underlying preference calculus. general, preference calculus defined tuple h , , 4i where:set preferences,commutative, associative operator closed ,4 forms partial order, is, reflexive, anti-symmetric transitive relation defined.4 reflexive, antisymmetric transitive, comparing preferences 4 relationyields one four possible results:502fiC OMPOSITIONAL ODEL R EPOSITORIESTwo preferences P1 , P2P2 4 P 1 .equal one another (denoted P1 = P2 ) iff P1 4 P2preference P1strictly greater preference P2P1 64 P2 P2 4 P1 .(denoted P1 P2 ) iffpreference P1strictly smaller preference P2P1 4 P2 P2 64 P1 .(denoted P1 P2 ) iffTwo preferences P1 , P2P2 64 P1 .incomparable one another (denoted P1 ?P2 ) iff P1 64 P2Thus, activity-based dynamic preference constraint satisfaction problem (aDPCSP) tuplehX, D, C, A, h , , 4i, PhX, D, C, Ai aDCSP,h , , 4i preference calculus,P mapping Dx1 . . . Dxn 7preferences.individual attribute-value assignmentspreferences attached attribute-value assignments express relative desirabilityassignments. aim aDPCSP find solution highest combined preference.is, given aDPCSP hX, D, C, A, h , , 4i, P i, solution hxi : dxi , . . . , xj : dxjaDCSP hX, D, C, Ai solution hxk : dxk , . . . , xl : dxl hX, D, C, Ai existsP (xi : dxi ) . . . P (xj : dxj ) P (xk : dxk ) . . . P (xl : dxl ) solution aDPCSP.section, preference calculus introduced extend aDCSP aDPCSP.calculus illustrated examples compositional modelling domain.2.2.1 R EPRESENTATIONOMPTechnically, OMPs combinations so-called basic preference quantities (BPQs),primitive units preference utility valuation associated possible design decisions.often difficult evaluate BPQs numerically, ordered relative one another employing similar ordering relations employed relative order-of-magnitude calculi (Dague,1993a, 1993b).Let set BPQs respect particular decision problem. BPQsordered respect one another two levels granularity, two relations <. First,partitioned orders magnitude, ordered . Then, BPQs within ordermagnitude ordered <. Formally, order-of-magnitude ordering BPQs tuplehO, i, = {O1 , . . . , Oq } partition irreflexive transitive binaryrelation O. subset BPQs said order magnitude . Similarly,within-magnitude ordering set BPQs tuple hO, <i, order magnitude< irreflexive transitive binary relation O.illustrate ideas, consider problem constructing ecological model describingscenario containing number populations. Let populations parasites othershosts parasites. Also, assume certain populations compete others scarceresources. order construct scenario model, compositional modeller must make number503fiK EPPENS & HENb15 : Lotka-Volterrapredation modelb13 : Hollingpredation model<<b11 : Rogershost-parasitoid model<b14 : Thomsonshost-parasitoid modelb12 : Nicholson-Baileyshost-parasitoid model<O1 (host-parasitoid phenomenon)b22 : exponentialgrowth model<<b21 : logisticgrowth modelb23 :growth model<b31 : competitionphenomenonO3: (competition phenomenon)O2 (population growth phenomena)Figure 1: Sample space BPQsmodel design decisions: population growth, host-parasitoid competition phenomenarelevant, types model best describe phenomena.Figure 1 shows sample space BPQs correspond selection types model.sake illustration, presumption made quality scenario model dependsinclusion types model, rather inclusion exclusion phenomena. Apartb23 b31 , BPQs correspond standard textbook ecological models 1 . BPQ b23 standsuse population growth model implicit another population growth model (the LotkaVolterra model, instance, implicitly includes concept growth). Finally, BPQ b 31preference associated competition model (say, one included knowledge base).9 BPQs sample space partitioned 3 orders magnitude. relationorders orders magnitude: O2 O1 O2 O3 . binary < relation orders individual BPQs within order magnitude. BPQ ordering within 1 , instance, Rogershost-parasitoid model (b11 ) preferred Nicholson Bailey (b12 ) Hollingpredation model (b13 ). latter two models compared one another,preferred Lotka-Volterra model. Furthermore, Thompsons host-parasitoid modelless preferred Nicholson Bailey, compared Lotka-VolterraHolling models.2.2.2 C OMBINATIONSOMPdefinition, OMPs combinations BPQs. implicit value OMP p equals combination b1 . . . bn constituent BPQs b1 , . . . , bn . property allows OMPs definedfunctions OMP P = b1 . . . bn function fP : 7 : b fP (b)1. precise, BPQs b11 , b12 , b13 , b14 , b15 , b21 b22 respectively correspond inclusion Rogershost-parasitoid model (1972), host-parasitoid model Nicholson Bailey (1935), Hollings predation model(1959), Thompsons host-parasitoid model (1929), predation model Lotka Volterra (1925, 1926), logisticpopulation growth model (Verhulst, 1838) exponential population growth model (Malthus, 1798).504fiC OMPOSITIONAL ODEL R EPOSITORIESset BPQs, set natural numbers fP (b) equals number occurrences bb1 , . . . , bn .example, let Pmodel denote OMP associated scenario model contains threelogistic population growth models (b21 ), two Holling predation model (b13 ) one competitionmodel (b31 ). Therefore,Pmodel = b21 b21 b21 b13 b13 b31hence:32fPmodel (b) =10b = b21b = b13b = b31otherwisedescribing OMPs functions, concept combinations OMPs becomes clear.two OMPs P1 P2 , combined preference P1 P2 defined as:fP1 P2 :7: b fP1 P2 (b) = fP1 (b) + fP2 (b)Note combination operator assumed commutative, associative strictly monotonic (P P P ). latter assumption made better reflect ideas underpinning conventional utility calculi (Binger & Hoffman, 1998).2.2.3 PARTIAL ORDERING OMPBased combinations OMPs, partial order 4 OMPs computed exploiting constituent BPQs OMPs considered. partial order implies comparisonpair OMPs either returns equal preference (=), smaller preference (), greater preference() incomparable preference (?). calculus developed assuming following:Prioritisation: combination BPQs never order magnitude greater constituent BPQs. is, given set BPQs belonging order magnitude{b1 , b2 , . . . , bn } O1 BPQ b O2 belonging higher order magnitude, i.e.O1 O2 ,b1 b2 . . . bn brespect ongoing example, means BPQ taken order magnitude O1 preferred combination BPQs taken O2 . words, choicemodel describe host-parasitoid phenomenon considered importantchoice population growth model (see Figure 1).Prioritisation also means distinctions higher orders magnitude consideredsignificant lower orders magnitude. Consider number BPQsb1 , . . . , bm1 , bm , . . . , bn taken one order magnitude O1 pair BPQs {b, b0 }taken order magnitude higher O1 . b < b0 , (irrespectiveordering BPQs taken O1 )b1 . . . bm1 b bm . . . bn b0505fiK EPPENS & HENStrict monotonicity: Even though distinctions higher orders magnitude significant, distinctions lower orders magnitude negligible. is, given OMPP two BPQs b1 b2 taken order magnitude b1 < b2 ,(irrespective orders magnitude BPQs constitute P )b1 P b2 Pinstance, preference ordering depicted Figure 1 shows scenario modelRogers host-parasitoid model two logistic predation models preferred oneRogers host-parasitoid model two exponential predation models:b11 b22 b22 b11 b21 b21Note departure conventional order-of-magnitude reasoning. OMPsassociated two (partial) outcomes contain equal BPQs higher order magnitude,usually desirable compare solutions terms (less important) constituentBPQs lower orders magnitude, example illustrated. However, conventional orderof-magnitude reasoning techniques handle this.Partial ordering maintenance: Conventional order-of-magnitude reasoning motivatedneed abstract descriptions real-world behaviour, whereas OMP calculus motivated incomplete knowledge decision making. opposed conventional orderof-magnitude reasoning real numbers, OMPs necessarily totally ordered.implies that, user states, example, b1 < b2 < b b3 < b4 < b,explicit absence ordering information BPQs {b 1 , b2 } {b3 , b4 }means user unable compare (e.g. entirely different things).Consequently, b1 b2 would deemed incomparable b3 b4 (i.e. b1 b2 ?b3 b4 ), ratherroughly equivalent.above, derived given two OMPs P1 P2 order magnitude O,P1 less equally preferred P2 respect order magnitude (denoted P1 4O P2 )providedbi O, fP1 (bi ) +Xbj O,bi <bjfP1 (bj ) fP2 (bi ) +Xbj O,bi <bjfP2 (bj )Thus, comparing two OMPs within order magnitude yield four possible results:P1 less preferred P2 respect (P1 P2 ) iff (P1 4O P2 ) (P2 4 P1 ),P1 preferred P2 respect (P1 P2 ) iff (P1 4O P2 ) (P2 4 P1 ),P1 equally preferred P2 respect (P1 =O P2 ) iff (P1 4O P2 ) (P2 4 P1 ),P1 incomparable P2 respect (P1 ?O P2 ) iff (P1 4O P2 ) (P2 4 P1 ).506fiC OMPOSITIONAL ODEL R EPOSITORIESongoing example Figure 1, instance, preference scenario modelRogers host-parasitoid model Holling predation model P 1 = b11 b13 preferencescenario model Rogers host-parasitoid model Lotka-Volterra predation modelP2 = b11 b15 . latter model less equally preferred former withinhost-parasitoid order magnitude (O1 ), i.e. P2 4O1 P1 ,fP2 (b11 ) = 1 1 = fP1 (b11 ),fP2 (b11 ) fP2 (b12 ) = 1 1 = fP1 (b11 ) fP1 (b12 ),fP2 (b11 ) fP2 (b13 ) = 1 2 = fP1 (b11 ) fP1 (b13 ),fP2 (b11 ) fP2 (b12 ) fP2 (b14 ) = 1 1 = fP1 (b11 ) fP1 (b12 ) fP1 (b14 ),fP2 (b11 ) fP2 (b12 ) fP2 (b13 ) fP2 (b14 ) = 2 2 = fP1 (b11 ) fP1 (b12 ) fP1 (b13 ) fP1 (b14 ).Similarly, established reverse, i.e. P1 4O1 P2 , true. Therefore, latterscenario model less preferred former within O1 , i.e. P2 O1 P1 .result generalised given two OMPs P 1 P2 , P1 lessequally preferred P2 (denoted P1 4 P2 )Oi O, (P1 4Oi P2 ) (Oj O, Oi Oj P1 Oj P2 )generally, relations , , = ? derived mannerrelation 4 , , =O ?O 4O .illustrate utility orderings, consider scenario one predator populationfeeds two prey populations two prey populations compete scarce resources.following two plausible scenario models scenario:Model 1 contains two Holling predation models three logistic population growth models,preference P1 = b13 b13 b21 b21 b21 .Model 2 contains one competition model, two Holling predation models, two logistic population growth models exponential population growth model, preferenceP2 = b13 b13 b21 b21 b22 b31 .demonstrated earlier, shown P1 =O1 P2 , P1 O2 P2 , P1 O3 P2 .relations follows P1 4 P2O1 : P1 4O1 P2 since P1 =O1 P2 ,O2 : exists order magnitude O3 O3 O2 P1 O3 P2 ,O3 : P1 4O3 P2 since P1 O3 P2 .reverse true, concluded scenario model 2 preferred scenario model1.2.3 Solving aDPCSPssection presents basic algorithm solving aDPCSPs. Although OMPs usedwork, algorithm take aDPCSP provided employs preference calculus507fiK EPPENS & HENcommutative, associative monotonic combination operator. However, use OMPs providesconvenient way specifying incomplete preference information.aDPCSP similar valued CSPs presented Schiex, Fargier Verfaillie (1995)also semiring based CSPs (Bistarelli, Montanari, & Rossi, 1997). However, extendsapproaches activity constraints involves different underlying presumptions valuationstructure. preference valuations work allowed ordered partially, opposedvalued CSPs.aDPCSP represents important type constraint satisfaction optimisation problem (Tsang,1993). order tackle optimisation preferences A* type algorithm employed (Hart,Nilsson, & Raphael, 1968; Raphael, 1990). A* algorithms known efficient termstotal number nodes explored effort find optimal solutions, given amountinformation. downside, exponential space complexity. Naturally, numberalternative approaches could explored, including conventional constraint-based solvingmethods depth first branch bound search. However, use A*-like algorithmsufficient solving aDPCSPs domain present interest. particular, algorithm 1implements A* search strategy capable handling activity constraints, involvesuse basic CSP techniques constraint propagation backtracking.A* algorithm maintains explored attribute-value assignments means priorityqueue Q nodes. node n Q corresponds set attribute-value assignments: solution(n).search proceeds number iterations. iteration, node n taken Q,replaced nodes extend solution(n) additional attribute-value assignment.specifically, node n Q, set Xu (n) remaining active unassigned attributesmaintained. iteration, possible assignments first attribute x X u (n),n node taken Q current iteration, processed. every assignment x :consistent solution(n) (i.e. solution(n) {x : d}, C 0 ), new child node n 0 ,solution(n0 ) = solution(n) {x : d} Xu (n0 ) = Xu (n) {x}, created added Q.activity constraints processed via propagation rather constraint satisfaction. Whenever node n taken Q Xu (n) empty, activity constraints fired orderobtain new set active unassigned attributes. is, X u (n) assigned{xi | solution(n), ` active(xi )} Xa (n)Xa (n) represents active, already assigned attributes node n.priority queue Q, nodes maintained means two heuristics: committed preferenceCP (n) potential preference P P (n). Here, given node n,CP (n) = x:dsolution(n) P (x : d)P P (n) = CP (n) (xXnd (n) max P (x : d))dDxXnd (n) set unassigned attributes still activated given partial assignment solution(n) (as indicated previously, actual implementation employs assumption-basedtruth maintenance system (de Kleer, 1986) efficiently determine attributes activitylonger supported). words, CP (n) preference associated partial attributevalue assignment node n P P (n) CP (n) combined highest possible preferenceassignments taken values domains attributes X nd (n). Thus, P P (n)508fiC OMPOSITIONAL ODEL R EPOSITORIESAlgorithm 1:SOLVE(X, D, C, A, P )n new node;solution(n) {};Xu (n) {xi | {}, ` active(xi )};Xa (n) {};CP (n) 0;P P (n) xX maxdD(x) P (x : d);Q createOrderedQueue();enqueue(Q, n, P P (n), CP (n)); Q 6=n dequeue(Q);Xu (n)6=x first(Xu (n));PROCESS(x, n, C, A, P, Q);Xu (n) {xi | solution(n), ` active(xi )} Xa (n);Xu (n) =nnext first(Q);CP (n) P P (nfirst)return (solution(n));elseelse P P (n) CP (n);enqueue(Q, n, P P (n), CP (n));x first(Xu (n));elsePROCESS(x, n, C, A, P, Q);procedure PROCESS(x, nparent , C, A, P, Q)D(x)solution(nparent ) {x : d}, C 0nchild new node;solution(n ) solution(nchildparent ) {x : d};X deactivated(solution(n ), X(nchildparent ));Xnd (nchild ) Xnd (nparent ) {x} Xd ;Xa (nchild ) Xa (nparent ) {x};Xu (nchild ) Xu (nparent ) {x};CP (nchild ) CP (nparent ) P (x : d);P P (nchild ) CP (nchild ) xXnd (n) maxdD(x) P (x : d);enqueue(Q, nchild , P P (nchild ), CP (nchild ));computes upper boundary preference aDPCSP solution includes partialattribute-value assignments corresponding n.following theorem shows algorithm 1 guaranteed find set attribute-valuepairs highest combined preferences, within set solutions satisfy constraints.Theorem 1 SOLVE(X, D, C, A, P ) admissibleProof: SOLVE(X, D, C, A, P ) A* algorithm guided heuristic function P P (n) = CP (n)h(n), CP (n) actual preference node n h(n) = xXnd (n) maxdDx P (x : d).follows previous discussion h(n) greater equal combined preferencevalue-assignment unassigned attributes consistent partial solution n.algorithm, nodes n maintained priority queue descending order P P (n). Letdistance function reverses preference ordering (P 1 ) (P2 ) P1 P2 .SOLVE (X, D, C, A, P ) described A* algorithm, nodes n priority509fiK EPPENS & HENqueue Q ordered ascending order (P P (n)), (P P (n)) = (CP (n)) (h(n))(h(n)) lower bound distance n optimal solution. Therefore, following work Hart, Nilsson Raphael (1968), SOLVE(X, D, C, A, P ) admissiblealgorithm, guaranteed find solution minimal (P (S)) maximal P (S).illustrate algorithm 1, consider problem finding ecological model describesbehaviour two populations, one predates other. aDPCSP constructedcompositional modelling problem following attributes domains. Note section3 demonstrates attributes, domains constraints problem constructedautomatically section 4 illustrates ideas context larger example.X = {x1 , x2 , x3 , x4 , x5 , x6 }Dx1 = {yes, no}Dx2 = {yes, no}Dx3 = {yes, no}Dx4 = {other, logistic}Dx5 = {other, logistic}Dx6 = {Holling, Lotka-Volterra}attributes x1 , x2 x3 respectvely describe relevance following phenomena:change size predator population, change size prey populationpredation prey predator. attributes x4 x5 represent choice typepopulation growth model. Two types models incorporated problem: logisticone other. Finally, attribute x6 associated choice model type predationphenomenon. Here, two types model, Holling model Lotka-Volterra model,included.Holling predation model presumes logistic models employed describepopulation growth, Lotka-Volterra Model incorporates population growthmodel, combinations assignments x4 , x5 , x6 restricted. Hence, aDPCSPcontains set C = {c{x4 ,x6 } , c{x5 ,x6 } } compatibility constraints, with:c{x4 ,x6 } = {hx4 : other, x6 : Lotka-Volterrai, hx4 : logistic, x6 : Hollingi}c{x5 ,x6 } = {hx5 : other, x6 : Lotka-Volterrai, hx5 : logistic, x6 : Hollingi}Furthermore, model type predator/prey growth must selected corresponding population growth phenomenon deemed relevant. Also, model type predation must selected population growth phenomena predation phenomenon deemed relevant (because ecological models describing predation rely submodelsdescribing population growth predator prey). Hence, aDPCSP contains set= {ax4 ,{x1 } , ax5 ,{x2 } , ax6 ,{x1 ,x2 ,x3 } } activity constraints, with:510fiC OMPOSITIONAL ODEL R EPOSITORIESax4 ,{x1 } = {hx1 : yes, active(x4 )i, hx1 : no, active(x4 )i}ax5 ,{x2 } = {hx2 : yes, active(x5 )i, hx2 : no, active(x5 )i}ax6 ,{x1 ,x2 ,x3 } = {hx1 : yes, x2 : yes, x3 : yes, active(x4 )i, hx1 : yes, x2 : yes, x3 : no, active(x4 )i,hx1 : yes, x2 : no, x3 : yes, active(x4 )i, hx1 : yes, x2 : no, x3 : no, active(x4 )i,hx1 : no, x2 : yes, x3 : yes, active(x4 )i, hx1 : no, x2 : yes, x3 : no, active(x4 )i,hx1 : no, x2 : no, x3 : yes, active(x4 )i, hx1 : no, x2 : no, x3 : no, active(x4 )i}Finally, let preference calculus consist two orders magnitude growth Opredation ,Ogrowth Opredation ,Ogrowth ={pother , plogistic } plogistic < potherOpredation ={pHolling , pLotka-Volterra } pLotka-Volterra < pHollingOMP assignments follows:P (x4 : other) = P (x5 : other) =potherP (x4 : logistic) = P (x5 : logistic) =plogisticP (x6 : Holling) =pHollingP (x6 : Lotka-Volterra) =pLotka-Volterraapplied problem, algorithm 1 initialises search creating node n 0 , where:Xu (n0 ), set currently active attributes, initialised {x1 , x2 , x3 }, activityattributes depend attribute-value assignments.Xa (n0 ) CP (n0 ) initialised empty set 0 respectively, since attributesassigned yet.Finally, P P (n0 ) equals pother pother pHolling combination highestOMPs associated domain.initial node enqueued Q. Next, algorithm proceeds number iterations.iteration, node potential (as measured P P CP ) dequeued,children generated enqueued Q. nodes created way depictedFigure 2. number subscript node ni indicates order node generation,thick arrows show order search space explored.Note three important features algorithm could clearly demonstrated within Figure 2. Firstly, node n5 , initial set unassigned attributes exhausted:Xu (n5 ) = {}. Therefore, activity constraints fired n 5 explored. n5 corresponds assignment {x1 : yes, x2 : yes, x3 : yes}, remaining attributes activatedXu (n5 ) reset {x4 , x5 , x6 }.Secondly, node n12 corresponds assignment (active) attributes consistentactivity compatibility constraints:{x1 : yes, x2 : yes, x3 : yes, x4 : other, x5 : other, x6 : Lotka-Volterra}511fix1yesn1P P = pother pother pHollingCP = 0n2P P = potherCP = 0x2yesn3n4P P = pother pother pHollingCP = 0P P = potherCP = 0x3n5yesP P = pother potherCP = 0512P P = pother pother pHollingCP = 0K EPPENS & HENn6x4n7P P = pother pother pHollingCP = pothern8logisticP P = plogistic pother pHollingCP = plogisticx5n9x5n10P P = pother pother pHollingCP = pother potherlogisticP P = pother plogistic pHollingCP = pother plogisticx6n11n12HollinginconsistentP P = plogistic pother pHollingCP = plogistic potherx6Lotka-VolterraP P = pother pother pLotka-VolterraCP = pother pother pLotka-Volterran13n16logisticP P = plogistic plogistic pHollingCP = plogistic plogisticx6n14Hollinginconsistentn15Lotka-Volterrainconsistentn17x6n18HollinginconsistentLotka-VolterrainconsistentFigure 2: Search space explored algorithm 1 solving sample aDPCSPn19HollingP P = plogistic plogistic pHollingCP = plogistic plogistic pHollingn20Lotka-VolterrainconsistentfiC OMPOSITIONAL ODEL R EPOSITORIESassignment solution aDPCSP, corresponding preference guaranteed maximal (and, assignment is, fact, optimal). creation n 12 , priority queue Q looks follows (the ordering n2 n4 may vary since P P (n2 ) = P P (n4 )CP (n2 ) = CP (n4 )):{n10 , n8 , n12 , n6 , n2 , n4 }Therefore, next node explored (after n9 subsequent creation n12 ) n10 .Thirdly, node n19 correspond optimal solution. creation, Q equals:{n19 , n12 , n6 , n2 , n4 }consequence, n19 dequeued next iteration. children n 19 created(Xu (n19 ) = activity constraints activate attributes), n 19 retained solution.user interested finding multiple alternative solutions, search may proceedQ contains nodes P P value smaller maximum preferencefirst solution. case, P P (n12 ) CP (n19 ) hence, one solutionaDPCSP.3. Compositional Model RepositoriesaDPCSPs discussed previous section provide foundation developmentcompositional model repositories. section specifies problem compositional modelrepository built solve shows translated aDPCSP, hence resolvedusing proposed aDPCSP solution algorithm.3.1 Background: assumption based truth maintenanceATMS mechanism keeps track piece inferred information dependspresumed information facts inconsistencies arise. ATMS, pieceinformation used derived problem solver stored node. Certain pieces informationknown true cannot inferred pieces information, yet plausibleinference may drawn them. nodes categorised special type referredassumptions.Inferences pieces information maintained within ATMS dependencies corresponding nodes. extended form (see de Kleer, 1988; Keppens, 2002),ATMS take inferences, called justifications form n . . . nj nk . . . nl nm ,ni , . . . , nj , nk , . . . , nl , nm nodes problem solver interested in. ATMSalso take specific type justification, called nogood, leads inconsistency,form ni . . . nj nk . . . nl (meaning least one statements{ni , . . . , nj , nk , . . . , nl } must false). ATMS, nogoods represented justifications special node, called nogood node.Based given justifications nogoods, ATMS computes label (nonassumption) node. label set environments environment set assumptions.particular, environment E depicts possible world assumptions E true.Thus, label L(n) node n describes possible worlds n true. labelcomputation algorithm ATMS guarantees label is:513fiK EPPENS & HENSound - assumptions environment within label node true sufficientcondition derive node:E L(n), [(ni E ni ) (ni E ni )] ` nConsistent - environment label node, nogood node, describesimpossible world:E L(n), [(ni E ni ) (ni E ni )] 0Minimal - label contain possible worlds less general onepossible worlds contains (i.e. environments supersets environmentslabel):E L(n)@E 0 L(n), E 0 EComplete - label node, nogood node, describes possible worldsnode inferred:E,[(ni E ni ) (ni E ni ) ` n]E 0 L(n), [(ni E 0 ni ) (ni E 0 ni ) ` n]3.2 Knowledge Representationknowledge-based approach, building compositional modeller requires formalism specification inputs, outputs knowledge base. work developedloosely based compositional modelling language (Bobrow, Falkenhainer, Farquhar, Fikes,Forbus, Gruber, Iwasaki, & Kuipers, 1996), proposed standard knowledge representation formalism compositional modellers, adapted meet challenges ecological compositionalmodelling problems identified introduction.3.2.1 P RELIMINARYCONCEPTSprimitive constructs compositional modeller participants, relations assumptions. subsection summarises concepts explains represented herein.Participants2 refer objects interest, involved scenario model.participants may real-world objects conceptual objects, variables expressfeatures real-world objects mathematical model. instance, population speciestypical example real-world object, variable expresses number individualsspecies forms example conceptual object. natural group objects sharesomething common classes. Participants herein grouped participant classes,representing set participants share certain common features. class givenname easy reference.Relations describe participants related one another. participants,relations represent real-world relationship, as:2. previous work compositional modelling refers individuals quantities, nameswould suit present application. Ecological models typically describe behaviour populations ratherindividuals often hard distinguish quantities.514fiC OMPOSITIONAL ODEL R EPOSITORIESpredation(frog, insect)(1)relations may conceptual nature, equation (2), describes importanttextbook model logistic population growth (Ford, 1999):sizechange = parameter size (1)dtcapacity(2)consistent compositional modelling approaches, paper employs LISPstyle notation relations. such, two sample relations become:(predation frog insect)(1)(d/dt change (* change-rate size (- 1 (/ size capacity))))(2)Assumptions form special type relation employed distinguish alternativemodel design decisions. Internally, assumptions stored form assumption nodesATMS (see section 3.3.1), knowledge base, assumptions appear relationsspecific syntax semantics.Two types assumptions employed article. Relevance assumptions state phenomena included excluded scenario model. Typical examples phenomenapopulation growth predation phenomena. general format relevance assumptionshown (3). phenomenon incorporated scenario model describing relevance assumption identified hnamei specific subsequent participants relations.example, relevance assumption (4) states growth participant ?populationincluded model.(relevanthnamei[{hparticipanti} | hrelationi])(relevant growth ?population)(3)(4)Model assumptions specify type model utilised describe behaviour certainparticipant relation. Typical examples model types include exponential (Malthus, 1798)logistic (Verhulst, 1838) model types population growth. formal specificationmodel assumption given (5). Often hnamei (5) corresponds name known(partial) model phenomenon process described. example (6) statespopulation ?population modelled using logistic approach.(model[hparticipanti | hrelationi]hnamei)(model ?population logistic)515(5)(6)fiK EPPENS & HENPredatorsnatalitymortalitymortalityratenatalityratepreyrequirementcapacityPreymortalitynatalitynatalityratepredationmortalityratecapacitysearchratepreyhandlingtimeFigure 3: Stock flow diagram predator prey scenario model3.2.2 CENARIOS SCENARIO MODELSformalised Keppens Shen (2001b), compositional modeller takes two inputs produces one output. first input representation (which model) describessystem interest means accessible formalism. model, normally consists(mainly) real-world participants interrelationships, called scenario. second inputtask description. formal description criteria adequacy outputevaluated. output new model describes scenario detailed formalism,usually set variables equations, model-based reasoner employ readily.model, normally contains conceptual participants interrelationships, called scenariomodel. aim compositional modeller translate scenario scenario model,means task description.work, model formally defined tuple hP, Ri, P set participantsR set relations participants P . definition applies scenarioscenario model. typical example scenario description predator population, preypopulation predation relation predator prey. scenario modelhP, Ri with:P = {predator, prey}R = {(predation predator prey)}aim compositional model repository translate scenario scenario model.Within work, systems dynamics stock-flow formalism (Forrester, 1968) ordinary differential equations (ODEs) employed modelling formalisms. example, scenariomodel corresponds scenario depicted Figure 3. Formally, scenario modelanother model hP, Ri caseP = {Npredator , Bpredator , Dpredator , Nprey , Bprey , Dprey , Pprey ,bpredator , bprey , dpredator , dprey , Cpredator , Cprey ,s(prey,predator) , t(prey,predator) , r(predator,prey) }516fiC OMPOSITIONAL ODEL R EPOSITORIESSymbolNpredator , NpreyBpredator , BpreyDpredator , DpreyPpreybpredator , bpreydpredator , dpreyCpredator , Cpreys(prey,predator)t(prey,predator)r(predator,prey)Variable namenumber predators, preynatality predators, preymortality predators, preypredation preynatality-rate predators, preymortality-rate predators, preycapacity predators, preysearch-rateprey-handling-timeprey-requirementTable 1: Variables stock flow diagram mathematical modelR={Npredator = Bpredator Dpredator ,dtNprey = Bprey Dprey Pprey ,dtBpredator = bpredator Npredator ,Bprey = bprey Nprey ,Dpredator = dpredator NpredatorNpredator,CpredatorNprey,Cpreys(prey,predator) Nprey Npredator=,1 + s(prey,predator) Nprey t(prey,predator)Dprey = dprey NpreyPpreyCpredator = r(predator,prey) Nprey ,Cprey = Nprey }relation variables mathematical model used stock-flow diagram given table 1. Generally speaking, stock-flow diagrams graphical representationssystems (ordinary qualitative) differential equations. automated modelling literaturegeneral, engineering physical systems modelling particular, sophisticated representational formalisms developed enable identification mathematical modelsbehaviour dynamic systems observations. Examples include bond graphs (Karnopp, Margolis, & Rosenberg, 1990) generalised physical networks (Easley & Bradley, 1999). However,potential benefits advanced formalisms exploited here, remaininteresting future work. Instead, stock-flow diagrams employed throughout paperfar commonly used ecological modelling (Ford, 1999).often possible construct multiple scenario models single given scenario,task specification employed guide search appropriate one(s). work,scenario models selected basis hard constraints user preferences. hard constraints stem restrictions imposed compositionality representational framework (seesection 3.2.3) properties scenario model required satisfy (see section 3.2.3).517fiK EPPENS & HENNameAdditionMultiplicationSelectionSyntax (infix notation)?var = C + (formula)?var = C (formula)?var = C (formula)?var = C (formula)?var = C if,p (antecedent, formula)?var = C else (formula)Syntax (prefix notation)(== ?var (C-add formula))(== ?var (C-sub formula))(== ?var (C-mul formula))(== ?var (C-div formula))(== ?var (C-if antecedent formula :priority p))(== ?var (C-else formula)Table 2: Composable functors composable relationsuser preferences express users subjective view modelling approachesappropriate context current scenario (see section 2.2).3.2.3KNOWLEDGE BASEconstruct scenario models given scenario, compositional modeller relies useknowledge base particular problem domain. illustrate ideas, sectionpresents constructs employed compositional modeller developed synthesisescenario models ecological domain.Composable relations knowledge base approach consists partial modelsinstantiated composed complex scenario models. composition partial modelsscenario model may involve composition partial relations (coming different partialmodels) compounded relations. sample scenario model section 3.2.2, followingrelation describes changes population size prey populationNprey = Bprey Dprey Ppreydt(7)(7), Nprey population size, Bprey number births, Dprey number natural deathsPprey number prey died due predation. Thus, relation (7) actually describes twophenomena affect population size Nprey : natural population growth (Bprey Dprey )predation related deaths (Pprey ). constructing knowledge base, desirable representtwo phenomena isolation always occur combination. example,species predators, therefore unnecessary always include predationcause death. viewpoint, relation (7) seen composed differentcomposable relations knowledge base:Nprey = C + (Bprey )dtNprey = C (Dprey )dtNprey = C (Pprey )dtuse composable relations enables knowledge base cover many combinationsphenomena may affect relation possible, representing phenomenon individually rather precompiling everything together. component parts (i.e.composable relations) relations need represented, instead possible, however complex, combinations them, knowledge base smaller effective. sectiondescribes composable relations represented knowledge base, well whethercomposed form compounded relations.518fiC OMPOSITIONAL ODEL R EPOSITORIESComposable relations containing composable functors methodcomposition exists (that describes complete set composable relations composed).composable functors employed proposed Bobrow et al. (1996) new addition:composable selection. summary composable relations presented table 2.composable relations introduced Bobrow et al. (1996) easy understand.formulae f v = C + (f ) v = C (f ) represent terms (respectively f f ) sum,formulae f v = C (f ) v = C (f ) represent factors (respectively f f1 ) product.However, ecological models use typically contain selection statements declareone certain equation must employed condition satisfied one otherwise.Formally, selection relation formc1 v = r1 else c2 . . . else v = rn(8)v participant, ci (with = 1, . . . , n1) relation describing condition statementrj (with j = 1, . . . , n) relation. selection relation consists partial relations:ci v = ri= 1, . . . , n 1else v = rnTherefore, selection relation composed two types composable relation. firstcomposable relation, form v = C if,p (a, f ), v participant, pelement taken total order, set natural numbers , denotes prioritycomposable relation sequence, f two given relations. second typecomposable relation composable else relation, form v = C else (felse ),felse given relation assigned v none antecedents composable relationstrue.illustrate notation, selection relation (8) composed following composable relations:v = C if,p1 (c1 , r1 )...v = C if,pn1 (cn1 , rn1 )v = C else (rn )p1 > . . . > pn1 .combine composable relations, number rules defined implement semanticsrepresentational formalism. theory, set rules generated enables aggregation set composable relations. practice, however, trade-off must madeflexibility (the ability combine many different types composable relation) comprehensibility (the use set rules easily understood knowledge engineer employscomposable relations). Thus, types composable relations combinedrestricted.Table 3 summarises composable relations joined form compounded relations.principle guiding construction table allow composition relationscertain types resulting compound relation intuitively obvious. example, according519fiK EPPENS & HENC + (f1 )C (f1 )C (f1 )C (f1 )C if,p1 (a1 , f1 )C if,p2 (a1 , f1 )C else (f1 )C + (f2 )yesyesC (f2 )yesyesC (f2 )yesyesC (f2 )yesyesC if,p2 (a2 , f2 )yesyesC else (f2 )yesyesTable 3: Composibility composable relationsTable 3, composable addition relation x = C + (y) combined composable subtraction relation x = C (z) combination clearly x = z. However, accordingTable 3, composable addition relation x = C + (y) combined composable multiplication relation x = C (z), arbitrary non-intuitive rule would otherwisedefined decide whether compound relation would x = + z x = z.order composable selections must considered defined priorities(or implicit case C else ). Therefore, composable selections combined oneanother provided two composable relations priority.order derive actual rules composition, sets composable relationsfunctor given model hP, Ri defined first:R(v, C + ) = {v = C + (fi ) | (v = C + (fi )) R}R(v, C ) = {v = C (fi ) | (v = C (fi )) R}R(v, C ) = {v = C (fi ) | (v = C (fi )) R}R(v, C ) = {v = C (fi ) | (v = C (fi )) R}R(v, C ) = {v = C if,pi (ai , fi ) | (v = C if,pi (ai , fi )) R}R(v, C else ) = {v = C else (fi ) | (v = C else (fi )) R}this, rules composition built given expressions (9), (10) (11).jointly state given set composable relations rewritten single compoundrelation. rules contains complete set composable relations antecedent.particular, antecedent rule (9) contains set composable addition subtractionrelations participant v left-hand side.Similarly, antecedent rule (10) contains complete set composable multiplication relations. Finally, antecedent rule (11) satisfied complete set composable elserelations left-hand participant v, provided priorities strictly ordered (i.e.two priorities equal) single composable else relation. latter twoconditions added two composable relations priority two composableelse relations compounded. consequents rules composition explaincomplete sets composable relations joined. simply matter applyingappropriate mathematical operation provided terms.520fiC OMPOSITIONAL ODEL R EPOSITORIESR(v, C + ) = {v = C + (f1+ ), . . . , v = C + (fm+ )}R(v, C ) = {v = C (f1 ), . . . , v = C (fn )}(9)v = f1+ + . . . + fm+ (f1 + . . . + fn )R(v, C ) = {v = C (f1 ), . . . , v = C (fm )}R(v, C ) = {v = C (f1 ), . . . , v = C (fn )}1 f1 . . . fmv=f1 . . . fn(10)R(v, C ) ={v = C if,p1 (a1 , f1 ), . . . , v = C if,pm (am , fm )}R(v, C else ) ={v = C else (felse )} p1 > . . . > pm(11)v =if a1 f1 , else . . . , fm , else felseProperty definitions Property definitions describe features interest application requiringscenario model. property definition tuple hP , , P = {ps1 , . . . psm } setsource-participants, predicate calculus sentence whose free variables elements P ,relation, whose free variables also elements P ,ps1 , . . . , psmtypical example feature interest requirement certain variable modelendogenous exogenous. specific, property definitions describevariable ?v endogenous exogenous respectively.(defproperty endogenous:source-participants ((?v :type variable)):structural-condition ((or (== ?v *) (d/dt ?v *))):property (endogenous ?v))(defproperty exogenous:source-participants ((?v :type variable)):structural-condition ((not (endogenous ?v))):property (exogenous ?v))?v = * true (where * matchesfirst definition states whenever either ?v = * dtconstant formula), ?v deemed endogenous. second property definition indicatesvariable said exogenous object exists endogenous.describing features formally knowledge base, property definitions enableimposed criteria selection scenario models. way, variable describingsize particular population eco-system, instance, forced endogenous.Note required properties specified two different ways: either globally goalsscenario model construction locally required purpose certain model fragment.latter use model properties illustrated later.521fiK EPPENS & HENModel fragments Model fragments building blocks scenario models constructed. model fragment tuple hP , P , , , A, P = {ps1 , . . . psm }set variables called source-participants, P = {pt1 , . . . , ptn } set variables called targetparticipants, = {s1 , . . . , sv } set relations, called structural conditions, whose free variables elements P , = {t1 , . . . , tx } set relations, called postconditions, whose freevariables elements P P , = {a1 , . . . , ay } set relations, called assumptions,= set relations, called purpose-required properties, that:ti , ps1 , . . . , psm , pt1 , . . . , ptn , s1 . . . sv (a1 . . . ay ti ), ps1 , . . . , psm , pt1 , . . . , ptn ,s1...sva1 . . . ax(12)(13)Note that, work, property definition hP , , equivalent model fragmenthP , {}, , {}, {}, {}i.example, model fragment states population ?p described twovariables ?p-size (describing size ?p) ?p-change (describing rate changepopulation size) differential equation?p-size = ?p-changedtusage partial scenario model subject two conditions: (1) growth phenomenonrelevant regard ?p, (2) variable ?p-change endogenous eventual scenariomodel. former requirement indicated relevance assumption latterpurpose-required property:(defModelFragment population-growth:source-participants ((?p :type population)):assumptions ((relevant growth ?p)):target-participants ((?p-size :type variable)(?p-change :type variable)):postconditions ((size-of ?p-size ?p)(change-of ?p-change ?p)(d/dt ?p-size ?p-change)):purpose-required ((endogenous ?p-change)))purpose-required property usually satisfied additional model fragments,one below:(defModelFragment logistic-population-growth:source-participants ((?p :type population)(?p-size :type variable)(?p-change :type variable)):structural-conditions ((size-of ?p-size ?p)(change-of ?p-births ?p)):assumptions ((model ?p-size logistic)):target-participants ((?r :type parameter)(?k :type variable)(?d :type variable)):postconditions ((capacity-of ?k ?p)(density-of ?d ?p-size)(== ?d (C-add (/ ?p-size ?k)))(== ?p-change (- (* ?r ?p-size (- 1 ?d))))))522fiC OMPOSITIONAL ODEL R EPOSITORIESModel fragments rules inference describe new knowledge derivedexisting knowledge committing emerging model certain assumptions. usedgenerate space possible models. Model fragments instantiated matching sourceparticipants existing participants scenario emerging model, matchingstructural conditions corresponding relations. possible instantiation, new instancegenerated target-participants, necessary, new instances also createdpostconditions assumptions. instances, well inferential relationshipsinstances source-participants, structural conditions assumptions one hand,target-participants postconditions other, stored ATMS, formingmodel space. explained section 3.3.1.model fragment said applied instantiated underlying assumptionshold. model fragment applied, instances target-participants postconditionscorresponding instantiation model fragment must added resulting model.respect example, model fragment implements logistic population growthmodel instantiated whenever variables exist describe size change population,applied logistic model population size also selected.Note compositional modellers, ones devised Heller Struss (1998,2001); Levy, Iwasaki Fikes (1997); Nayak Joskowicz (1996); Rickel Porter (1997),model fragments represent direct translations components physical systems influences variables. compositional modeller presented herein aims serve ecologicalmodel repository, contents model fragments employed differs conventionalcompositional modellers two important regards:Firstly, model fragments contain partial models describing certain phenomena instead influences. partial models normally correspond developed ecological modellingresearch. Typical examples include logistic population growth model (Verhulst, 1838)Holling predation model (Holling, 1959) devised population dynamics literature.Secondly, partial models contained model fragments often need composed incrementally. example, aforementioned sample model fragment logistic-populationgrowth requires emerging scenario model, may generated sample modelfragment population-growth. Thus, one model fragment, e.g. logistic-populationgrowth, expand partial model contained another, e.g. population-growth. feature, (correctly) presumed model fragment generates new relationspreconditions model fragments expands on. Violating presumption wouldmake little sense context present application would imply recursive extensionemerging scenario model set variables equations.3.2.4 PARTICIPANT CLASS DECLARATION PARTICIPANT TYPE HIERARCHIESgeneral, participant classes need defined. However, certain types participant maydescribed terms interesting participants, irrespective modelling choices.feature provides syntactic sugar describing important relations participants, makingeasier declare required properties scenario model terms participants scenario.example, behaviour population may described terms population size growthrate variables:(defEntity population:participants (size growth-rate))523fiK EPPENS & HENParticipant class declarations may also employed within model fragments providespecific definition meaning source-participants target-participants. way,participant specifications constrained feature another participant means:entity statement, following example illustrates:(defModelFragment define-population-growth-phenomenon:source-participants ((?p :type population)):target-participants((?ps :type stock :entity (size ?p))(?pg :type variable :entity (growth-rate ?p))(?pb :type flow)(?pd :type flow)):assumptions ((relevant growth ?p)):postconditions ((== ?pg (- ?pb ?pd))(flow ?pb source ?pl)(flow ?pd ?pl sink)))Furthermore, participant class declarations may define one class immediate subclassanother. example, population participant class holometabolous insects (e.g. butterflies)may defined subclass population participant class:(defEntity holometabolous-insect-population:subclass-of (population):participants(larva-number pupa-number adult-number))way, participant type hierarchy defined. subclass inherits participantssuperclasses (i.e. immediate superclass superclasses superclasses).summary, participant class declaration tuple = h , P participantclass, called immediate superclass participant class P set participants classesdescribe important features participant class.3.3 Inferencecompositional modelling method presented herein employs four step inference procedure:1. Model space construction. model space ATMS efficiently stores participants, relations model design decisions (represented form relevance modelassumptions) may part final scenario model, well conditionsparticipants relations must must part scenario model.2. aDCSP construction. model space contains number hard constraints participants relations may combined. inference step extracts restrictionstranslates aDCSP.3. Inclusion order-of-magnitude preferences. Preferences associated relevancemodel assumptions scenario space reflect relative appropriatenessassumptions, resulting aDPCSP.4. Scenario model selection. inference step solves aDPCSP. resulting solutionscorrespond scenario models consistent according domain knowledgeoptimise overall preference respect order-of-magnitude preference calculus.524fiC OMPOSITIONAL ODEL R EPOSITORIESProblem SpecificationCompositional Model Repositorypopulation(prey)population(predator)predation(predator,prey)ApplicationSTEP 1Model Space ConstructionScenarioModel SpaceRequirementsInconsistenciesGenerationActivitybased Dynamic ConstraintSatisfaction Problem ConstructionRequirementsInconsistenciesKnowledge BaseSTEP 2Scenario ModelConstructionDynamic ConstraintSatisfaction ProblemScenario ModelSTEP 3Inclusion OrderofMagnitudePreferencesPreferencesPreference OrderingDynamic Preference ConstraintSatisfaction ProblemApplicationPreydeathratebirthratecrowdingmax crowdsustainablepopulationfooddemandconsumptionSTEP 4PredatordeathratebirthrateScenario Model Selection(aDCSP solver)crowdingAssumption SetKnowledge elementsInference elementsFigure 4: Inference procedures compositional modeller525fiK EPPENS & HENfour steps correspond four squares compositional model repository Figure 4section, inference steps discussed detail illustrated meanssimple examples. next section contains detailed example shows procedureapplied non-trivial ecological modelling domain.3.3.1 CENARIO + K NOWLEDGE BASE = ODEL PACEpreviously stated, aim compositional modeller translate scenario scenariomodel. representations system interest though model system differentlevel detail. knowledge base provides foundation translation. scenario modelsconstructed given scenario, regard knowledge base, storedmodel space.model space ATMS (de Kleer, 1986) containing participants, relations assumptions instantiated given scenario. work, generalised versionATMS, introduced de Kleer (1988), employed allows use negations nodesjustifications. algorithm GENERATE ODEL PACE(hO, Ri) describes modelspace created scenario hO, Ri. first initialises model space participant instances (O) relation instances (R) scenario. Then, model fragment whose source-participants structural conditions match participants relations already, new instances target-participants, assumptions postconditions added . property definition hP , , equivalent model fragment hP , {}, , {}, {}, {}i,procedure applies property definitions well model fragments. Matching sourceparticipants structural conditions model fragment emerging model space performed function match(, , ) specified below, model fragmentmatched, substitution source-participants participant instances.true = {ps1 /o1 , . . . , psm /om }P () = {ps1 , . . . , psm }match(, , ) =o1 . . . om(),false otherwisematch, specified model fragment substitution , processed follows:assumption A(), new node, denoting assumption instance a, createdadded .Then, new node n(,) , denoting instantiation via substitution , created, addedjustified implication:(aA() a) (pP () p) (s () ) n(,)Finally, new instance target-participant p P () postcondition(), provided already exist model space , created.target-participants, involves creating new symbol new participant instancefunction gensym() extending substitution {p/gensym()}. new node n526fiC OMPOSITIONAL ODEL R EPOSITORIESAlgorithm 1:GENERATE ODEL PACE(hO, Ri)new ATMS;O, add-node(, o);r R, add-node(, r);, , match(, , )justification ;A()newnode add-node(, (a));justificationjustification {newnode};p P ()justification justification {find-node(, (p))};()justification justification {find-node(, ())};add-node(, n(,) );add-justification(, n(,) , njustificationn);p P (){p/gensym()};add-node(, (p));add-justification(, o, n(,) );()( )get-node(, ());elseadd-node(, ());add-justification(, o, n(,) );n1 , . . . , nm , inconsistent({n1 , . . . , nm )add-justification(, n , n1 . . . nm );{{{a1Instances assumptions:A() = {a1 , . . . , }...pt1xInstances sourceparticipants:P () = {ps1 , . . . , psm }Instances structuralconditions:() = {s1 , . . . , sv }ps1......ptnt1psm...s1ts...}}Instances targetparticipants:P () = {pt1 , . . . , ptn }Instances postconditions:() = {t1 , . . . , ts }svFigure 5: Model fragment applicationcreated added new participant instance p new instantiatedrelation . nodes justified implication n (,) n.527fiK EPPENS & HENglobal property must satisfiedconsistent scenario modelspurpose-required property model fragment ,applied substitution .r1 r2non-composable relationsv = r1 (. . .)(a) Inconsistency causedglobal propertyv = r2 (. . .)(b) Inconsistency causedpurpose-required property(c) Inconsistency causednon-composable relationsFigure 6: Sources inconsistencyillustrate procedure, Figure 5 shows graphical representation inferencesconstructed applying model fragment = hP , P , , , A, {}i respect substitution.possible applications model fragments exhausted, inconsistenciesmodel space identified recorded ATMS. algorithm, nogoods generatedset {n1 , . . . , nm } inconsistent nodes, denoted inconsistent({n1 , . . . , nm }).three sources inconsistencies reported ATMS different way:Global properties: Let instance global property scenario model mustsatisfy. Then, combination assumptions negations assumptions preventssatisfied inconsistent. Therefore, inconsistent({}) must reportedrequired global property . type inconsistency depicted Figure 6(a).Purpose-required properties: application model fragment without satisfyingpurpose-required properties () yields inconsistency (see (13)). Hence, noden(,) denoting instantiation via substitution , node n describingappropriate instance purpose-required property (), inconsistent({n (,) , n })reported. type inconsistency depicted Figure 6(b).Non-composable relations: mathematical formalism designed describe simulationmodels dynamic systems, certain combinations relations may over-constrain model,hence, unsuitable generating behaviour system interest. Withinsystem dynamics ODE formalisms used paper, assignments relationsvariable composable relations explicitly deemed composable.words, two relations v = ri v = rj combined one another rirj composable. Examples pairs non-composable relations includex = C + (y) x = C (z) C + C relations composable,= C + (b) = c + c + composable relation.Combinations non-composable relations must reported inconsistency well.type inconsistency depicted Figure 6(c).528fiC OMPOSITIONAL ODEL R EPOSITORIESassumption:(relevant growth frog)participant:population frog1 :2 ::1assumption:(model nfrog logistic)participant:parameter rfrogrelation:dt nfrog = cfrogparticipant:parameter kfrogparticipant:variable cfrogrelation:(capacity-of kfrog frog)participant:variable nfrogpopulation-growthmodel fragmentlogistic-population-growthmodel fragmentendogenousproperty definitioncfrog = rfrog nfrog (1nfrog)kfrogrelation:(change-of cfrog frog)relation:(size-of nfrog frog)relation:endogenous(cfrog )relation:2relation:(endogenous cfrog )Figure 7: Partial model spaceillustrate model space construction algorithm, Figure 7 presents small sample modelspace. results application population-growth logistic-population-growth model fragments endogenous property definition, describedearlier, single population frog. larger scenario involving multiple populations relations populations specified, similar partial model space would generatedindividual population.3.3.2 F ROMMODEL SPACE DCSPmodel space constructed, translated aDCSP. translationprocedure, summarised algorithm CREATEA DCSP(), consists three steps described below:Algorithm 2:CREATEA DCSP()comment: set substitutions{};comment: Generate attributes domainsA, assumption-class(A)x create-attribute();D(x){};{A/x};aAv create-value();D(x) D(x) {v};{a/x : v};comment: Generate activity constraintsA, assumption-class(A)subject(A);{a1> , . . . , ap> , a1 , . . . , aq } L(s)add(a1> . . . ap> a1 . . . aq active(A));comment: Generate compatibility constraints{a1> , . . . , ap> , a1 , . . . , aq } L(n )add(a1> . . . ap> a1 . . . aq );529fiK EPPENS & HEN1. Generate attributes domain values assumptions. aDCSP attributes correspond underlying assumption classes (i.e. groups assumptions indicating alternativechoices regards model construction decision). relevance assumptionnegation jointly form assumption class. example, 1 ={(relevant growthfrog), (relevant growth frog)} specifies assumption class. setmodel assumptions involving participants/relations, different model nameshence different descriptions, also form assumption class. instance, 2 ={(modelnfrog exponential), (model nfrog logistic), (model nfrog other)},nfrog variable denoting size population, specifies assumption class. Running step algorithm, attribute created assumption class,domain attribute consisting assumption instances assumption class.2. Create activity constraints. attributes domain values generated previous stepmeaningful situations participant and/or relation instances containedarguments corresponding assumptions exist. example, assumption (modelnfrog logistic) relevant participant instance nfrog exists. Clearly, assumptions within one assumption class participant and/or relation instancesarguments. assumption class corresponds one attribute, attributeactivated participant and/or relation instances associated related assumption class active. Therefore, step creates activity constraints activateattribute based conjunction environments contained within labelsparticipants/relations assumption class. instance, deduced Figure7, nfrog activated (relevant growth frog) committed. Thus, attributecorresponding assumption class A2 , defined step 1, activated attribute valueassignment associated (relevant growth frog) assumption.3. Create compatibility constraints. ATMS (or model space), sources inconsistencies contained label nogood node. Therefore, compatibility constraintscreated directly translating environments label L() correspondingconjunctions attribute-value assignments.3.3.3DCSP+PREFERENCES= DPCSPaDCSP produced formalises hard requirements imposed upon scenario models.Among scenario models meet requirements, may better others,underlying model design decisions may deemed appropriate user. Preferencesexpress (relative) level appropriateness attached assumptions describemodel design decisions, extension, attribute-value pairs aDCSP. discussedsection 2, extension aDCSP constitutes aDPCSP.specifically, worth recalling section 2.2 order-of-magnitude preferencecalculus presented enables representation reasoning subjective user preferencesdifferent relevance modelling assumption. Next, section 2.3 introduces solution algorithmaDPCSPs include aDCSP, ones constructed approach section 3.3.2,extended subjective user preferences alternative design decisions.530fiC OMPOSITIONAL ODEL R EPOSITORIES3.4 Outline analysis complexitycomplexity work arises four major sources: 1) model space construction, 2) labelpropagation ATMS, 3) model space aDCSP translation, 4) aDPCSP solution.GENERATE ODEL PACE (hO, Ri) essentially performs fixed sequence instructionsproduces small set nodes inferences match model fragment. Therefore,time space complexity linear respect number possible matches modelfragments. CREATEA DCSP() extracts certain information model space rewritesdifferent formalism without manipulations. Therefore, time space complexitylinear respect size model space.label propagation algorithm ATMS known exponential time complexity.However, model space built incrementally (by GENERATE ODEL PACE(hO, Ri))root nodes ATMS network (i.e. correspond facts antecedents) leaf nodes (i.e. consequents, nogood node)inconsistencies added end, complexity increases exponentiallydepth network number participants relations individual model fragments, rather size model space. fact significantly limits complexityimpact label propagation. Firstly, depth ATMS network restricted domain.many conventional compositional modellers, model fragments direct translationsscenario components scenario model equations, depth would one. Empirically, constructing model space sophisticated eco-systems, depth model space never exceeded8. Secondly, size individual model fragments change significantly sizeknowledge base.fourth final source complexity driven fact constraint satisfactionalgorithm must determine consistent combination assumptions model space. spaceattribute value assignments increases exponentially size number assumptionshence, model space. Thus, overall complexity present approach largelydominated constraint satisfaction algorithm employed.user specify preference, CSP aDCSP. Recently, number efficientmethods devised solving aDCSPs presented Minton et al. (1992); MittalFalkenhainer (1990); Verfaillie Schiex (1994). helps minimise overhead incurredcompositional modelling.preferences, CSP becomes aDPCSP. argued section 2, presents newproblem yet studied detail. work, A* algorithm proposedimplement CSP solution method. approach known efficient termsproportion search space algorithm needs explore finding optimal solution,compared search methods based heuristic (Hart et al., 1968).disadvantage incurs exponential space complexity. explained Miguel Shen(2001a, 2001b); Tsang (1993), wide range alternative solution techniques exist ordinaryCSPs many could also extended solve aDPCSPs. detailed examinationtechniques topic future research.3.5 Automated modelling scientific discoverymentioned previously, compositional model repository designed order compose modelssystems structure relevant domain knowledge. such, approach gives rise po531fiK EPPENS & HENtentially beneficial means operationalise outcomes scientific discovery. specifically,resultant compositional model repositories allow existing knowledge model constructionapplied unexperienced scenarios support investigation situations mayphysically difficult replicate create may synthesised computational representations.present work applied vegetation component MODMED n-speciesmodel (Legg, Muetzelfeldt, & Heathfield, 1995). n-species model offers system dynamicsrepresentation populations Mediterranean vegetations affected populations farm animals, climate environmental management. purpose modelinstantiated respect various Mediterranean communities, serve componentlarge scale simulation designed simulate effects various environmentalpolicies Mediterranean landscape. knowledge base containing approximately 60 modelfragments 4 property definitions constructed, basis complex partsn-species model two man-weeks. knowledge base employed reconstruct variations n-species model accommodate variety possible scenarios, wellexamine simplifications original n-species model exclude certain phenomena.compositional model repository closely related seminal work compositional modelling (Falkenhainer & Forbus, 1991). approach similar functionalitydevised specifically physical systems relies component-connection formalismrepresent scenarios.Another approach recently developed applied ecological domainHeller Struss (1998, 2001). work derives systems structure observationsbehaviour domain knowledge. Therefore, able perform diagnosis ecological systemstherapy suggestion. Another important distinction work present studypresumes process described one way instead allowing multiplealternative models.machine learning community, number approaches devised Bradley,Easley Stolle (2001); Langley et al. (2002); Todorovski Dzeroski (1997, 2001)induce sets differential equations a) observations behaviour, b) domain knowledge represented form hypothetical equations, c) description structure system.approaches aim scientific discovery generalising observed behaviour mathematicalmodels. specifications scenario domain knowledge methods similarused article. especially true work Langley et al. (2002); Todorovski Dzeroski (1997, 2001), work also applied population dynamics.However, internal mechanisms approaches different essentially relyexhaustive search procedures instead constraint satisfaction techniques.4. Population Dynamics Exampleexamples used throughout previous sections taken extensive applicationstudy present work. application aimed construct repository basic populationdynamic models, describing phenomena growth, predation competition. sectionpresents overview proposed approach employed application showability work scale larger problems.532fiC OMPOSITIONAL ODEL R EPOSITORIES4.1 Knowledge basesubsection illustrates set model fragments constructed. challengetask lies fact model fragments must encompass sufficiently general reusablecomponent part ecological models. instances models found literature ecologicalmodelling, boundaries recurring component parts hidden, thereforeknowledge engineer identify them.First, hierarchy entity types set up. system dynamics models shown earlier containthree types participant: variables, stocks flows. Here, stocks flows specialtype variable predetermined meaning. is, flow f stock corresponds= C + (f ) flow f stock denotes dt= C (f ). Hence, stocksequation dtflows defined subclasses participant class variable:(defEntity variable)(defEntity stock:subclass-of (variable))(defEntity flow:subclass-of (variable))sample properties defined section 3.2.3, describe conditionvariable endogenous exogenous, employed knowledge base:(defproperty endogenous-1:source-participants ((?v :type variable)):structural-conditions ((== ?v *)):property (endogenous ?v))(defproperty endogenous-2:source-participants ((?v :type variable)):structural-conditions ((d/dt ?v *)):property (endogenous ?v))(defproperty exogenous:source-participants ((?v :type variable)):structural-conditions ((not (endogenous ?v))):property (exogenous ?v))next three model fragments contain rules stock-flow diagrams employed systems dynamics models. respectively describe that:flow ?flow stock ?stock corresponds composable differential equation:?stock = C + (?flow)dtflow ?flow stock ?stock corresponds composable differential equation:?stock = C (?flow)dtflow ?flow one stock ?stock1 another stock ?stock2 correspondscomposable differential equations:?stock1 = C (?flow) ?stock2 = C + (?flow)dtdt533fiK EPPENS & HEN(defModelFragment inflow:source-participants((?stock :type stock)(?flow :type flow)):structural-conditions((flow ?flow source ?stock)):postconditions((d/dt ?stock (C-add ?flow))))(defModelFragment outflow:source-participants((?stock :type stock)(?flow :type flow)):structural-conditions((flow ?flow ?stock sink)):postconditions((d/dt ?stock (C-sub ?flow))))(defModelFragment inflow:source-participants((?stock1 :type stock)(?stock2 :type stock)(?flow :type flow)):structural-conditions((flow ?flow ?stock1 ?stock2)):postconditions((d/dt ?stock1 (C-sub ?flow))(d/dt ?stock2 (C-add ?flow))))declarations place, knowledge base model fragments defined. first model fragment describes population growth phenomenon. Noteaforementioned growth, predation competition models contain stock representing populationsize two flows, one flow births stock another flow deaths stock.common feature models population dynamics contained single model fragment.(defModelFragment population-growth:source-participants((?population :type population)):assumptions((relevant growth ?population)):target-participants((?size :type stock :name size)(?birth-flow :type flow :name births)(?death-flow :type flow :name deaths)):postconditions((flow ?birth-flow source ?size)(flow ?death-flow ?size sink)(size-of ?size ?population)(births-of ?birth-flow ?population)(deaths-of ?death-flow ?population)):purpose-required((endogenous ?birth-flow)(endogenous ?death-flow)))variables ?birth-flow ?death-flow become endogenous model containsequation describing birth flow death flow. equations differ population growthmodels. Two types population growth model exponential growth model (Malthus, 1798),shown Figure 8(a), logistic growth model (Verhulst, 1838), shownFigure 8(b). following two model fragments formally describe component models:534fiC OMPOSITIONAL ODEL R EPOSITORIESfffifffi!"(a) Exponential growth(b) Logistic growthFigure 8: Population growth models(defModelFragment exponential-population-growth:source-participants((?population :type population)(?size :type variable)(?birth-flow :type variable)(?death-flow :type variable)):structural-conditions((size-of ?size ?population)(births-of ?birth-flow ?population)(deaths-of ?death-flow ?population)):assumptions((model ?size exponential)):target-participants((?birth-rate :type variable :name birth-rate)(?death-rate :type variable :name death-rate)):postconditions((== ?birth-flow (* ?birth-rate ?size))(== ?death-flow (* ?death-rate ?size))))(defModelFragment logistic-population-growth:source-participants((?population :type population)(?size :type variable)(?birth-flow :type variable)(?death-flow :type variable)):structural-conditions((size-of ?size ?population)(births-of ?birth-flow ?population)(deaths-of ?death-flow ?population)):assumptions((model ?size logistic)):target-participants((?birth-rate :type variable :name birth-rate)(?death-rate :type variable :name death-rate)(?density :type variable :name total-population)(?capacity :type variable :name capacity)):postconditions((== ?birth-flow (* ?birth-rate ?size))(== ?death-flow (* ?death-rate ?size ?density))(== ?density (C-add (/ ?size ?capacity)))(density-of ?density ?population)(capacity-of ?capacity ?population)))one twist compositional modelling population growth. Sometimes, actualgrowth model implicitly contained within another type model. cases, growthphenomenon corresponding differential equations still relevant, none dedicatedgrowth models employed. example, shown later, Lotka-Volterra predationmodel comes equations describing growth.535fiK EPPENS & HENmodel fragment other-growth allows empty growth model, named other,selected. However, due purpose-required property instance ?p-change mustendogenous, empty model selected growth model implicitly includedelsewhere.(defModelFragment other-growth:source-participants((?population :type population)(?size :type variable)(?birth-flow :type variable)(?death-flow :type variable)):structural-conditions((size-of ?size ?population)(births-of ?birth-flow ?population)(deaths-of ?death-flow ?population)):assumptions((model ?population other)))addition population growth, two phenomena included knowledge base:predation competition. Predation competition relations species representedpredicates populations: e.g. (predation foxes rabbits) (competitionsheep cows). However existence phenomenon necessarily mean mustcontained within model. would make little sense model predation competition withoutmodelling size populations, models phenomena relate population sizesone another. Therefore, incorporation predation phenomenon made dependent uponexistence variables representing population size. Also, human expert modellers may preferleave phenomenon resulting model. keep choice open, following twomodel fragments construct participant representing phenomena predation competition,make dependent upon relevance assumption:(defModelFragment predation-phenomenon:source-participants((?predator :type population)(?prey :type population)(?predator-size :type variable)(?prey-size :type variable)):structural-conditions((predation ?predator ?prey)(size-of ?predator-size ?predator)(size-of ?prey-size ?prey)):assumptions((relevant predation ?predator ?prey)):target-participant((?predation-phenomenon :type phenomenon :name predation-phenomenon)):postconditions((predation-phenomenon ?predation-phenomenon ?predator ?prey)):purpose-required ((has-model ?predation-phenomenon)))(defModelFragment competition-phenomenon:source-participants((?population1 :type population)(?population2 :type population)(?size1 :type variable)(?size2 :type variable)):structural-conditions((competition ?population1 ?population2)(size-of ?size1 ?population1)(size-of ?size2 ?population2))536fiC OMPOSITIONAL ODEL R EPOSITORIESBprey = bprey NpreyBprey = bprey NpreyDprey = dprey NpreyNpreyKpreyDprey = pprey Nprey Npreddt Npreydt Nprey= Bprey Dprey P= Bprey Dprey PbpreyP =ppreybpreydpreysNprey Npred1+sNprey thKpreythDpred = dpred NpredBpred = bpred NpredDpred = dpred NpredBpred = ppred Nprey Npreddt NpredNpredKpreddt NpredNpredKpred= Bpred Dpred= Bpred DpreddpredbpreddpredppredKpred = k Nprey(b) Holling predation(a) Lotka-Volterra predationFigure 9: Predation models:assumptions((relevant competition ?population1 ?population2)):target-participant((?competition-phenomenon :type phenomenon :name competition-phenomenon)):postconditions((competition-phenomenon ?competition-phenomenon ?population1 ?population2)):purpose-required((has-model ?competition-phenomenon)))model fragments purpose-required property form (has-model ?phen).property expresses condition model must exist respect phenomenon:(defproperty has-model:source-participants ((?p :type phenomenon)):structural-conditions ((is-model-of ?p *)):property (has-model ?p))next two model fragments implement models (thereby satisfying has-modelpurpose-required property) predation phenomenon two populations. describetwo well-known predation models: Lotka-Volterra model (1925, 1926), shown Figure 9(a), Holling model (1959), shown graphically Figure 9(b).(defModelFragment Lotka-Volterra:source-participants((?predation-phenomenon :type phenomenon)(?predator :type population)(?predator-size :type stock)(?predator-birth-flow :type flow)(?predator-death-flow :type flow)(?prey :type population)(?prey-size :type stock)(?prey-birth-flow :type flow)(?prey-death-flow :type flow)):structural-conditions((predation-phenomenon ?predation-phenomenon ?predator ?prey)537fiK EPPENS & HEN(size-of ?predator-size ?predator)(births-of ?predator-birth-flow ?predator)(deaths-of ?predator-death-flow ?predator)(size-of ?prey-size ?prey)(births-of ?prey-birth-flow ?prey)(deaths-of ?prey-death-flow ?prey)):assumptions((model ?predation-phenomenon lotka-volterra)):target-participants((?prey-birth-rate :type variable :name birth-rate)(?predator-factor :type variable :name predator-factor)(?prey-factor :type variable :name prey-factor)(?predator-death-rate :type variable :name death-rate)):postconditions((== ?prey-birth-flow (* ?prey-birth-rate ?prey-size))(== ?predator-birth-flow (* ?predator-factor ?prey-size ?predator-size))(== ?prey-death-flow (* ?prey-factor ?prey-size ?predator-size))(== ?predator-death-flow (* ?predator-death-rate ?predator-size))(is-model-of lotka-volterra ?predation-phenomenon)))mentioned earlier, Lotka-Volterra model introduces growth model preypredator populations assigning specific equations variables, describe changessizes predator prey populations, ?pred-change ?prey-change respectively.Thus, satisfies purpose-required property application population-growthmodel fragment ?prey ?pred populations.(defModelFragment Holling:source-participants((?predation-phenomenon :type phenomenon)(?predator :type population)(?predator-size :type stock)(?capacity :type variable)(?prey :type population)(?prey-size :type stock)):structural-conditions((predation-phenomenon ?predation-phenomenon ?predator ?prey)(size-of ?predator-size ?predator)(size-of ?prey-size ?prey)(capacity-of ?capacity ?predator)):assumptions((model ?predation-phenomenon holling)):target-participants((?search-rate :type variable :name search-rate)(?handling-time :type variable :name handling-time)(?prey-requirement :type variable :name prey-requirement)(?predation :type flow :name predation)):postconditions((flow ?predation ?prey-size sink)(== ?predation(/ (* ?search-rate ?prey-size ?predator-size)(+ 1 (* ?search-rate ?prey-size ?handling-time))))(== ?capacity (C-add (* ?prey-requirement ?prey)))(is-model-of holling ?predation-phenomenon)))Holling model employs variable denoting capacity population. variablemay introduced logistic growth model. practice, logistic growth models Hollingpredation models often used conjunction. compositional modeller need awarecombinations models, however. needs know prerequisites individualcomponent models contained within model fragment.538fiC OMPOSITIONAL ODEL R EPOSITORIESD1 = 1 N 1B1 = b 1 N 1dt N1N1 +w12 N2K1= B1 1d1b1w12K1D2 = 2 N 2B2 = b 2 N 2dt N2w21 N1 +N2K2= B2 2d2b2w21K2Figure 10: species competition modelfinal model fragment knowledge base implements model competitiontwo species. formally describes competition model type depicted Figure 10. modelfragment contains population competition model knowledge base, containmodel assumption represent model.(defModelFragment competition:source-participants((?competition-phenomenon :type phenomenon)(?population-1 :type population)(?size-1 :type stock)(?density-1 :type variable)(?capacity-1 :type variable)(?population-2 :type population)(?size-2 :type stock)(?density-2 :type variable)(?capacity-2 :type variable)):structural-conditions((competition-phenomenon ?competition-phenomenon ?population-1 ?population-2)(density-of ?density-1 ?size-1)(capacity-of ?capacity-1 ?size-1)(density-of ?density-2 ?size-2)(capacity-of ?capacity-2 ?size-2)):assumptions((relevant competition ?population-1 ?population-2)):target-participants((?weight-12 :type variable :name weight)(?weight-21 :type variable :name weight)):postconditions((== ?density-1 (C-add (/ (* ?weight-12 ?size-2) ?capacity-1)))(== ?density-2 (C-add (/ (* ?weight-21 ?size-1) ?capacity-2)))))539fiK EPPENS & HENrelevantgrowth predatorGrowthPredatorpredationpredator,prey1relevantpredationpredator,prey1Exponentialmodel predatorexponentialLogisticmodelLogisticmodel predatorlogistic"Other"modelothergrowthmodel predatorpredationphen:predator,prey1GrowthStock +Flowspredationpredator,prey2relevantpredationpredator,prey2Exponentialmodelothergrowthmodel predatorLogisticmodelLogisticmodel predatorlogistic"Other"modelExponentialmodel predatorexponentialPredationpredationphen:predator,prey2GrowthStock +FlowsPrey2relevantgrowth prey2ExponentialmodelPredationPrey1relevantgrowth prey1Stock +FlowsExponentialmodelothergrowthmodel predatorLogisticmodelLogisticmodel predatorlogistic"Other"modelExponentialmodel predatorexponentialrelevantcompetitionprey1,prey2LotkaVolterramodelLotkaVolterramodel comp.lotkavolterraHollingmodelHollingmodel comp.hollingLotkaVolterramodelLotkaVolterramodel comp.lotkavolterraHollingmodelHollingmodel comp.hollingCompetitioncompetitionphen:prey1,prey2competitionprey1,prey2Figure 11: Model space 1 predator 2 competing prey scenario4.2 Model spacemodel space constructed knowledge base instantiated respect given scenario. Consider example following scenario, describes predator populationpreys two populations, prey1 prey2, whilst two prey populations competeone another:(defScenario pred-prey-prey-scenario:entities ((predator :type population)(prey1 :type population)(prey2 :type population)):relations ((predation predator prey1)(predation predator prey2)(competition prey1 prey2)))full specification model space unwieldy present abstract graphicalrepresentation model space scenario shown Figure 11. model space containsfollowing knowledge:three populations scenario, set three population growth models(i.e. exponential, logistic other) derived. inference dependent uponrelevance assumption population growth phenomenon, model assumptioncorresponds one three population growth models.540fiC OMPOSITIONAL ODEL R EPOSITORIESpredation relations (i.e. (predation predator prey1) (predationpredator prey2)), populations related them, set two predation models(i.e. Lotka-Volterra Holling) derived. inference dependent upon relevance assumption predation phenomenon model assumption correspondsone two predation models.competition relation (competition prey1 prey2), populations related it, competition model derived. one competition model,inference competition model dependent upon relevance assumptioncorresponds competition phenomenon.addition hypergraph Figure 11, model space also contains number constraintsconjunctions assumptions consistent. explained earlier, stem twosources: 1) non-composable relations 2) purpose-required properties. example giventype.Let predation-phen-1 predation phenomenon predator prey1,prey1-size variable representing size prey1 population. example, model fragments exponential-population-growth Lotka-Volterragenerate equation computing value variable representing changeprey1-size. equations composed, following inconsistency generated:(relevant growth prey1) (model prey1-size exponential)(relevant growth predator) (relevant predation predator prey1)(model predation-phen-1 lotka-volterra)Inconsistencies also arise purpose-required properties. example, model fragment predation-phenomenon applicable predation relation deemed relevant,purpose-required property (has-model ?pred-phen) become condition consistency. certain combinations assumptions, property may satisfied. Say,Holling predation exponential growth models selected, Holling model generated ?capacity (capacity ?capacity ?pred) true.predation model created case (because Holling model fragment instantiated), even though predation phenomenon deemed relevant set assumptions.inconsistent has-model purpose-required property predation-phenomenonmodel fragment, responsible combination assumptions therefore marked nogood.(relevant growth predator) (model predator-size exponential)(relevant growth prey1) (model prey1-size exponential)(relevant predation predator prey1) (model predation-phen-1 holling)4.3 aDPCSP solutionresultant model space translated aDCSP enable selection consistent setassumptions, using advanced CSP solution techniques. aDCSP derived modelspace depicted Figure 12.541fiK EPPENS & HENAttributex1x2x3x4x5x6x7x8x9x10x11Meaning(relevant growth prey1)(relevant growth prey2)(relevant growth predator)(relevant predation predator prey1)(relevant predation predator prey2)(relevant competition prey1 prey2)(model size-1 *)(model size-2 *)(model size-3 *)(model predation-phen-1 *)(model predation-phen-2 *)Table 4: Attribute listDomainD1D2D3D4D5D6D7D8D9D10D11Content{d1,y , d1,n }{d2,y , d2,n }{d3,y , d3,n }{d4,y , d4,n }{d5,y , d5,n }{d6,y , d6,n }{d7,l , d7,e , d7,o }{d8,l , d8,e , d8,o }{d9,l , d9,e , d9,o }{d10,h , d10,lv }{d11,h , d11,lv }Meaning{population,none}{population,none}{population,none}{(population,population),none}{(population,population),none}{(population,population),none}{logistic,exponential,other}{logistic,exponential,other}{logistic,exponential,other}{Holling,Lotka-Volterra}{Holling,Lotka-Volterra}Table 5: aDCSP 1 predator 2 competing prey scenario: domains contentsmeaningaDCSP contains 11 attributes. listed corresponding assumption classestable 4. first 6 attributes correspond notion relevance phenomenon: 3 populationgrowth phenomena, 2 predation phenomena 1 competition phenomenon precise.5 attributes correspond 5 sets model types: 3 sets population growth models 2 setspredation models.assumptions attributes generated form domains values. resulting domains aforementioned attributes summarised table 5.activity constraints aDCSP describe conditions instantiate subjectassumptions correspond attribute. Since participant relation labelmodel space, minimal set assumptions becomes part emerging modelavailable. participant relation subject assumption, label explicitlydescribes sets assumptions attribute corresponds subject542fiC OMPOSITIONAL ODEL R EPOSITORIESx1d1,yx4d1,nd4,yx7d7,ld7,ex6d4,nd6,yx2d6,nd2,yx10d7,od10,lvx5d2,nd5,yx8d10,hd8,ld8,ex3d5,nd3,yx11d8,od11,lvd3,nx9d11,hd9,ld9,ed9,oattributevaluecompatibility constraintactivity constraintFigure 12: aDCSP derived models space reflecting 1 predator 2 competing preyscenarioactivated. translating label subject sets attribute-value assignments, antecedents activity constraints constructed.example, relevance assumptions (attributes x1 , . . . , x6 ) take subjectsscenario, hence, always active. attributes related model assumptionspopulation growth active corresponding assumptions denoting relevance populationgrowth true. is,x1 : d1,y active(x7 )x2 : d2,y active(x8 )x3 : d3,y active(x9 )attributes related assumptions predation models active correspondingassumptions denoting relevance predation, assumptions describing relevance population growth, true populations involved predation relation. is,x1 : d1,y x3 : d3,y x4 : d4,y active(x10 )x2 : d2,y x3 : d3,y x5 : d5,y active(x11 )Figure 12 shows graphical representation activity constraints.compatibility constraints correspond directly inconsistencies nogood node.inconsistencies discussed previous section depicted Figure 12.aDCSP constructed, preferences may attached attribute-value assignments.Suppose preferences assigned standard population modelling choices, i.e. expo543fiK EPPENS & HENAttributex1 , . . . , x5x6x7x8x9x10x11Preference assignmentspreference assignmentsP (x6 : d6,y ) = pcompetitionP (x7 : d7,l ) = plogistic , P (x7 : d7,e ) = pexponentialP (x8 : d8,l ) = plogistic , P (x8 : d8,e ) = pexponentialP (x9 : d9,l ) = plogistic , P (x9 : d9,e ) = pexponentialP (x10 : d10,h = pholling , P (x10 : d10,lv ) = plotka-volterraP (x11 : d11,h = pholling , P (x11 : d11,lv ) = plotka-volterraTable 6: Preference assignments 1 predator 2 competing prey problemnential growth, logistic growth, lotka-volterra predation holling predation, relevancecompetition (because one type model implemented phenomenon).example, following BPQs could employed:pexponential < plogisticplotka-volterra < phollingpcompetitionlogistic Holling models preferred exponential Lotka-Volterra models former generally regarded accurate. Note preferencesordered way corresponding different phenomena related oneanother. justification ordering that, even though models structurally connected(there restrictions models combined one another), models different phenomena inherently describe behaviours compared one another. preferenceassignments attribute value assignments summarised table 6.Solving aDPCSP simple. First, attributes x1 , . . . , x6 activated.attributes assigned xi : di,y assignment maximises potential preference. Then,attributes x7 , . . . , x11 activated. Here, attributes x7 , . . . , x9 assigned xi : di,llogistic growth model highest preference. Finally, x 10 x11 assigned x10 : d10,hx11 : d11,h Holling models highest preference inconsistentlogistic model committed earlier. resulting solution satisfies following set assumptions:{(relevant growth prey1),(relevant growth prey2),(relevant growth predator),(relevant competition prey1 prey2),(relevant predation predator prey1),(relevant predation predator prey2),(model size-1 logistic),(model size-2 logistic),(model size-3 logistic),(model predation-phen-1 holling),(model predation-phen-2 holling)}544fiC OMPOSITIONAL ODEL R EPOSITORIESSYMBOLSrelevantgrowth predatorGrowthNodes entailedaDPCSP solutionPredatorpredationpredator,prey1Exponentialmodel predatorexponentialLogisticmodelLogisticmodel predatorlogistic"Other"modelothergrowthmodel predatorNodes entailedaDPCSP solutionApplied model fragmentPredationpredationphen:predator,prey1LotkaVolterramodelLotkaVolterramodel comp.lotkavolterraGrowthStock +FlowsHollingmodelHollingmodel comp.hollingPrey1predationpredator,prey2relevantpredationpredator,prey2Exponentialmodelothergrowthmodel predatorLogisticmodelLogisticmodel predatorlogistic"Other"modelExponentialmodel predatorexponentialPredationpredationphen:predator,prey2LotkaVolterramodelLotkaVolterramodel comp.lotkavolterraGrowthStock +FlowsHollingmodelHollingmodel comp.hollingPrey2relevantgrowth prey2ExponentialmodelModel fragmentappliedrelevantpredationpredator,prey1relevantgrowth prey1AssumptionsaDPCSP solutionStock +FlowsExponentialmodelothergrowthmodel predatorLogisticmodelLogisticmodel predatorlogistic"Other"modelExponentialmodel predatorexponentialrelevantcompetitionprey1,prey2Competitioncompetitionphen:prey1,prey2competitionprey1,prey2Figure 13: Deducing scenario model model space, given set assumptions4.4 Sample scenario modelFigure 13 shows scenario model deduced set assumptions exploiting model space. nodes corresponding aforementioned assumptionslogically follow assumption set indicated Figure.combining participants relations resulting scenario model, model givenFigure 14 drawn. model corresponds one ecologist would drawlogistic growth Holling predation models regarded appropriate task hand.5. Conclusion Future Workarticle presented novel approach compositional modelling enables constructionmodels ecological systems. work differs existing approaches automaticallytranslates compositional modelling problem aDCSP (order-of-magnitude) preference valuations. several benefits method.use translation algorithm converts compositional modelling problemaDCSP allows criteria formalised. importantly, also enables efficient, existingfuture, aDCSP solution techniques effectively applied solving compositional modellingproblems.545fiGrowthGrowthB1 = b1 N1D1 = 1 N 1 1dt N1B2 = b2 N2D2 = 2 N 2 2dt N2= B1 D1 P31= B2 D2 P32HollingK EPPENS & HENs31K1d11 =N1K1+P32 =b2s32w12 N2K1th,31K3Logistic3 =K2w21 N1K2+N2K2LogisticGrowthB3 = b 3 N 3b32 =th,32N3K3D3 = 3 N 3 3dt N3d2s32 N2 N31+s32 N2 th,32546P31 =b1Hollings31 N1 N31+s31 N1 th,31= B3 3d3LogisticFigure 14: Sample scenario model 1 predator 2 competing prey scenariofiC OMPOSITIONAL ODEL R EPOSITORIESextension aDCSPs (order-of-magnitude) preferences (to form aDPCSPs) alsopermits incorporation softer requirements compositional modelling problem.paper, order-of-magnitude preferences employed express appropriateness alternative model types certain phenomena. considerations may described hardconstraints physical systems domain3 , subjective less understood problemdomains, ecological modelling domain. approach presented herein provides meanscapture represent subtlety flexible model design decisions.theoretical ideas presented article applied real-world ecological modelling problems. paper, demonstrated resultant compositional modelleremployed create repository population dynamics models. approach alsoapplied automated model construction large complex ecosystems MODMEDmodel Mediterranean vegetation (Legg et al., 1995), reported Keppens (2002).practical theoretical issues need addressed, however. practical side, types ecological model design decisions, represented assumptionsassumption classes, supported inference mechanisms, extended. Ecologicalsystems tend involve interrelated populations individuals, instead functional compositionsindividual components physical systems. One particularly important type design decisionecological modelling therefore granularity. requires introduction novel representation formalisms inference mechanisms aggregation disaggregation. Initial workconsidering populations single entities dividing entities sub-populationsnecessary carried (Keppens & Shen, 2001a). Integration work presentaDPCSP framework requires investigation.theoretical side, analysis complexity present approach rather informal.Much remains done regard, especially comparing complexity existingcompositional modellers. comparison, additional work required adapt current translation procedure suit existing compositional modelling problems. compositionalmodellers exponential complexity, however. employ problem-specific solution algorithms, little known opportunities improving efficiency. work hopesfirst step toward understanding important issue.Acknowledgmentswork partly supported UK-EPSRC grant GR/S63267. first author alsosupported College Science Engineering scholarship University Edinburgh.grateful Robert Muetzelfeldt helpful discussions assistance researchreported, whilst taking full responsibility views expressed here. Thanks also goanonymous referees constructive comments useful revising earlierversion paper.ReferencesBinger, B., & Hoffman, E. (1998). Microeconomics Calculus. Longman.3. so-called operating conditions, stating range values certain variables within usecertain assumptions permitted.547fiK EPPENS & HENBistarelli, S., Montanari, U., & Rossi, F. (1997). Semiring-based constraint satisfaction optimization. Journal ACM, 44(2), 201236.Bobrow, D., Falkenhainer, B., Farquhar, A., Fikes, R., Forbus, K., Gruber, T., Iwasaki, Y., & Kuipers,B. (1996). compositional modeling language. Proceedings 10th InternationalWorkshop Qualitative Reasoning Physical Systems, pp. 1221.Bradley, E., Easley, M., & Stolle, R. (2001). Reasoning nonlinear system identification.Artificial Intelligence, 133, 139188.Dague, P. (1993a). Numeric reasoning relative orders magnitude. ProceedingsNational Conference Artificial Intelligence, pp. 541547.Dague, P. (1993b). Symbolic reasoning relative orders magnitude. Proceedings13th International Joint Conference Artificial Intelligence, pp. 15091514.de Kleer, J. (1986). assumption-based TMS. Artificial Intelligence, 28, 127162.de Kleer, J. (1988). general labeling algorithm assumption-based truth maintenance.Proceedings 7th National Conference Artificial Intelligence, pp. 188192.Easley, M., & Bradley, E. (1999). Generalized physical networks automated model building.Proceedings 16th International Joint Conference Artificial Intelligence, pp. 10471053.Falkenhainer, B., & Forbus, K. (1991). Compositional modeling: finding right modeljob. Artificial Intelligence, 51, 95143.Ford, A. (1999). Modeling Environment - Introduction System Dynamics ModelingEnvironmental Systems. Island Press.Forrester, J. (1968). Principles Systems. Wright-Allen Press, Cambridge, MA, USA.Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determinationminimal cost paths. IEEE Transactions Systems, Science Cybernetics, SSC-4(2), 100107.Heller, U., & Struss, P. (1998). Diagnosis therapy recognition ecosystems - usage modelbased diagnosis techniques. Proceedings 12th International Symposium ComputerScience Environment Protection.Heller, U., & Struss, P. (2001). Transformation qualitative dynamic models - application hydroecology. Hotz, L., Struss, P., & Guckenbienl, T. (Eds.), Intelligent Diagnosis IndustrialApplications, pp. 95106. Shaker Verlag.Holling, C. (1959). characteristics simple types predation parasitism. CanadianEntomologist, 91, 385398.Karnopp, D., Margolis, D., & Rosenberg, R. (1990). System Dynamics: United Approach (SecondEdition edition). John Wiley & Sons, Inc.Keppens, J. (2002). Compositional Ecological Modelling via Dynamic Constraint SatisfactionOrder-of-Magnitude Preferences. Ph.D. thesis, University Edinburgh.Keppens, J., & Shen, Q. (2001a). Disaggregation compositional modelling ecological systemsvia dynamic constraint satisfaction. Proceedings 15th International WorkshopQualitative Reasoning Physical Systems, pp. 2128.548fiC OMPOSITIONAL ODEL R EPOSITORIESKeppens, J., & Shen, Q. (2001b). compositional modelling. Knowledge Engineering Review,16(2), 157200.Keppens, J., & Shen, Q. (2002). supporting dynamic constraint satisfaction order magnitude preferences. Proceedings 16th International Workshop Qualitative Reasoning Physical Systems, pp. 7582.Langley, P., Sanchez, J., Todorovski, L., & Dzeroski, S. (2002). Inducing process modelscontinuous data. Proceedings 19th International Conference Machine Learning,pp. 347354.Legg, C., Muetzelfeldt, R., & Heathfield, D. (1995). Modelling vegetation dynamics mediterranean ecosystems: Issues scale. Proceedings 39th Symposium InternationalAssociation Vegetation Science.Levy, A., Iwasaki, Y., & Fikes, R. (1997). Automated model selection simulation basedrelevance reasoning. Artificial Intelligence, 96, 351394.Lotka, A. (1925). Elements physical biology. Williams & Wilkins Co., Baltimore.Malthus, T. (1798). essay principle population. Printed J. Johnson St. PaulsChurch Yard, London, England.Miguel, I., & Shen, Q. (1999). Hard, flexible dynamic constraint satisfaction. KnowledgeEngineering Review, 14(3), 199220.Miguel, I., & Shen, Q. (2001a). Solution techniques constraint satisfaction problems: Advancedapproaches. Artificial Intelligence Review, 15(4), 269293.Miguel, I., & Shen, Q. (2001b). Solution techniques constraint satisfaction problems: Foundations. Artificial Intelligence Review, 15(4), 243267.Minton, S., Johnston, M., Philips, A., & Laird, P. (1992). Minimizing conflicts: heuristic repairmethod constraint satisfaction scheduling problems. Artificial Intelligence, 58, 161205.Mittal, S., & Falkenhainer, B. (1990). Dynamic constraint satisfaction problems. Proceedings8th National Conference Artificial Intelligence, pp. 2532.Nayak, P., & Joskowicz, L. (1996). Efficient compositional modeling generating causal explanations. Artificial Intelligence, 83, 193227.Nicholson, A., & Bailey, V. (1935). balance animal populations. Proceedings Zoological Society London, 1, 551598.Raphael, B. (1990). A* algorithm. Shapiro, S.C. (Ed.), Encyclopedia Artificial Intelligence,Vol. 1, pp. 13. John Wiley & Sons.Rickel, J., & Porter, B. (1997). Automated modeling complex systems answer predictionquestions. Artificial Intelligence, 93, 201260.Rogers, D. (1972). Random search insect population models. Journal Animal Ecology, 41,369383.Schiex, T., Fargier, H., & Verfaillie, G. (1995). Valued constraint satisfaction problems: Hardeasy problems. Proceedings 14th International Joint Conference Artificial Intelligence, pp. 631637.549fiK EPPENS & HENThompson, W. (1929). relative value parasites predators biological controlinsect pests. Bull. Etnomol. Res., 19, 343350.Todorovski, L., & Dzeroski, S. (1997). Declarative bias equation discovery. Proceedings14th International Conference Machine Learning, pp. 432439.Todorovski, L., & Dzeroski, S. (2001). Using domain knowledge population dynamics modeling equation discovery. Proceedings 12th European Conference MachineLearning, pp. 478490.Tsang, E. (1993). Foundations Constraint Satisfaction. Academic Press, London San Diego.Verfaillie, G., & Schiex, T. (1994). Solution reuse dynamic constraint satisfaction problems.Proceedings 12th National Conference Artificial Intelligence, pp. 307312.Verhulst, P. (1838). Recherches mathematiques sur la loi daccroissement de la population. Nouveaux memoires de lacademie royale des sciences et belles-lettres de Bruxelles, 18, 138.Volterra, V. (1926). Fluctuations abundance species considered mathematically. Nature,118, 558560.550fiJournal Artificial Intelligence Research 21 (2004) 319356Submitted 11/02; published 03/04Representation Dependence Probabilistic InferenceJoseph Y. Halpernhalpern@cs.cornell.eduCornell University, Computer Science DepartmentIthaca, NY 14853http://www.cs.cornell.edu/home/halpernDaphne Kollerkoller@cs.stanford.eduStanford University, Computer Science DepartmentStanford, CA 94035http://www.cs.stanford.edu/ kollerAbstractNon-deductive reasoning systems often representation dependent: representingsituation two different ways may cause system return two different answers. viewed significant problem. example, principlemaximum entropy subjected much criticism due representation dependence. has, however, almost work investigating representation dependence.paper, formalize notion show problem specific maximum entropy. fact, show representation-independent probabilistic inferenceprocedure ignores irrelevant information essentially entailment, precise sense.Moreover, show representation independence incompatible even weak default assumption independence. show invariance restricted classrepresentation changes form reasonable compromise representation independence desiderata, provide construction family inference proceduresprovides restricted representation independence, using relative entropy.1. Introductionwell known way problem represented significant impactease people solve it, complexity algorithm solvingit. interested arguably even fundamental issue: extentanswers get depend input represented. too,well known work, particularly Tversky Kahneman (see, example, (Kahneman,Slovic, & Tversky, 1982)), showing answers given people vary significantly(and systematic ways) depending question framed. phenomenon oftenviewed indicating problem human information processing; implicit assumptionalthough people make mistakes sort, shouldnt. hand,competing intuition suggests representation (and ) matter;representation dependence natural consequence fact.consider one type reasoning, probabilistic inference, examine extentanswers depend representation. issue representation dependenceparticular interest context interest using probability knowledgerepresentation (e.g., (Pearl, 1988)) probabilistic inference sourcec2004AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiHalpern & Kollermany concerns expressed regarding representation. However, approachapplicable far generally.begin noting notion probabilistic inference two quite differentinterpretations. one interpretation, forms basis Bayesian paradigm,probabilitic inference consists basically conditioning: start prior distribution event space, condition whatever observations obtained.interpretation, given set probabilistic assertions, goalreach conclusions probabilities various events. paper,focus latter interpretation, although discuss relationship Bayesianapproach Section 7.2.Suppose procedure making inferences probabilistic knowledgebase. sensitive way knowledge represented? Consider followingexamples, use perhaps best-known non-deductive notion probabilistic inference,maximum entropy (Jaynes, 1978).1Example 1.1: Suppose information whatsoever regarding whetherobject colorful. probability assign proposition colorful ? Symmetryarguments might suggest 1/2. Since information, seems objectlikely colorful non-colorful. also conclusion reachedmaximum entropy provided language proposition colorful .suppose know colors red, blue, green, propositions correspondingcolors. Moreover, colorful actually mean red blue green.case, maximum entropy dictates probability red blue green 7/8. Note that,cases, conclusion follows constraints trivial one:probability query somewhere 0 1.Example 1.2: Suppose told half birds fly. two reasonable ways represent information. One propositions bird fly,use knowledge base KB fly1 =def [Pr(fly | bird ) = 1/2]. second mightbasic predicates bird flying-bird , use knowledge base KB fly2 =def [(flying-birdbird ) Pr(flying-bird | bird ) = 1/2]. Although first representation may appear natural, seems representations intuitively adequate insofar representinginformation given. use inference method maximumentropy, first representation leads us infer Pr(bird ) = 1/2, second leads usinfer Pr(bird ) = 2/3.Examples basis frequent criticisms maximum entropygrounds representation dependence. pointing examples,little work problem. fact, work Salmon (1961,1963) Paris (1994), seems work formalizing notion representation dependence. One might say consensus was: whatever representationindependence is, property enjoyed maximum entropy.1. Although much discussion motivated representation-dependence problem encounteredmaximum entropy, understanding maximum entropy works essentialunderstanding discussion.320fiRepresentation Dependenceinference procedures it? paper attempt understand notionrepresentation dependence, study extent achievable.study representation dependence, must first understand meanrepresentation. real world complex. reasoning process, must focuscertain details ignore others. semantic level, relevant distinctions capturedusing space X possible alternatives states (possible worlds). Example 1.1,first representation focused single attribute colorful . case, twostates state space, corresponding colorful true false, respectively.second representation, using red , blue, green, richer state space. Clearly,distinctions could make.also interpret representation syntactic entity. case, typically capture relevant distinctions using formal language. example, use propositionallogic basic knowledge representation language, choice primitive propositionscharacterizes distinctions chosen make. take statestruth assignments propositions. Similarly, use probabilistic representation language belief networks (Pearl, 1988) knowledge representationlanguage, must choose set relevant random variables. statespossible assignments values variables.mean shift representation (i.e., state space) X another representation ? Roughly speaking, want capture level state space shiftfrom, say, feet meters. Thus, X distances might described terms feetmight described terms meters. would expect constraintrelating feet meters. constraint would give extra information X;would relate worlds X worlds . Thus, first attempt capture representation independence somewhat indirectly, requiring adding constraints relating Xplace constraints X result different conclusions X.resulting notion, called robustness, turns surprisingly strong. showevery robust inference procedure must behave essentially like logical entailment.try define representation independence directly, using mapping fone representation another. example, f could map world individual6 feet tall corresponding world individual 1.83 meters tall.obvious constraints f necessary ensure corresponds intuitionrepresentation shift. define representation-independent inference procedureone preserves inferences every legitimate mapping f ; i.e., KB ,KB | iff f (KB ) | f ().definition turns somewhat reasonable first attempt,exist nontrivial representation-independent inference procedures. However, stillstrong notion. particular, representation-independent inference procedure mustact essentially like logical entailment knowledge base objective information(i.e., essentially non-probabilistic information). Moreover, show representationindependence incompatible even simplest default assumption independence.Even told nothing propositions p q, representation independenceallow us jump conclusion p q independent.results suggest want inference procedures capable jumpingnontrivial conclusions, must accept least degree representation de321fiHalpern & Kollerpendence. add support claim choice language carry greatdeal information, complete representation independence much expect.positive note, show use intuition choice languagecarries information get limited forms representation independence. idealanguage put constraints counts appropriate representationshift. example, suppose certain propositions represent colors others representbirds. may willing transform colorful red blue green, maywilling transform red sparrow . reason demand inference procedure behave way suddenly shift wildly inappropriate representation,symbols mean something completely different. provide general approachconstructing inference procedures invariant specific class representationshifts. construction allows us combine degree representation independencecertain non-deductive properties want inference procedure. particular, present inference method supports default assumption independence,yet invariant natural class representation shifts.rest paper organized follows. Section 2, define probabilistic inference procedures characterize them. Section 3, define robust inference proceduresshow every robust inference procedure essentially entailment. Section 4,define representation independence, show representation independencestrong requirement. particular, show representation-independent inferenceprocedure essentially acts like logical entailment objective knowledge basesrepresentation independence incompatible default assumption independence.Section 5 contains general discussion notion representation independencereasonable assume choice language affect inference.may indeed seem reasonable assume choice language affectinference, point assumption consequences might viewunfortunate. Section 6, discuss limited forms representation independenceachieved. discuss related work Section 7, conclude Section 8.2. Probabilistic Inferencebegin defining probabilistic inference procedures. discussed introduction,two quite different ways term used. one, given priordistribution probability space; knowledge typically consists eventsspace, used condition distribution obtain posterior.other, focus work, probabilistic inference procedure takes inputprobabilistic knowledge base returns probabilistic conclusion.take knowledge base conclusion assertions probabilities events measurable space (X, FX ), measurable space consistsset X algebra FX subsets X (that is, FX set subsets X closedunion complementation, containing X itself).2 Formally, assertions viewedstatements (or constraints on) probability measures (X, FX ). example,2. X infinite, may want consider countably-additive probability measures take FXclosed countable unions. issue play significant role paper. simplicity,restrict finite additivity require FX closed finite unions.322fiRepresentation DependenceFX , statement Pr(S) 2/3 holds distributions probabilityleast 2/3. Therefore, (X,FX ) set probability measures (X, FX ) (thatis, probability measures domain FX ), view knowledge base setconstraints (X,FX ) . FX clear context, often omit notation,writing X rather (X,FX ) .place restrictions language used express constraints.assume includes assertions form Pr(S) subsets FX rational[0, 1], closed conjunction negation, KB KB 0knowledge bases expressing constraints, KB KB 0 KB . (However,langauge could include many assertions besides obtained starting assertionsform Pr(S) closing conjunction negation.) Since languageputs constraints probability measures, cannot directly say FX must hold.closest approximation language assertion Pr(S) = 1. Thus, callconstraints objective. knowledge base consisting objective constraints calledobjective knowledge base. Since Pr(T1 ) = 1 Pr(T2 ) = 1 equivalent Pr(T1 T2 ) = 1,without loss generality, objective knowledge base consists single constraintform Pr(T ) = 1. Given knowledge base KB placing constraints X , write |= KBmeasure X satisfies constraints KB . use [[KB ]]X denotemeasures satisfying constraints.practice, knowledge typically represented syntactically, using logical language describe possible states. Typical languages include propositional logic, firstorder logic, language describing values set random variables. general,base logic L defines set formulas L() given vocabulary . propositionallogic, vocabulary simply set propositional symbols. probability theory,vocabulary consist set random variables. first-order logic, vocabularyset constant symbols, function symbols, predicate symbols. facilitate comparisonvocabularies, assume base logic vocabularies finitesubsets one fixed infinite vocabulary .working language, assume state state space definesinterpretation symbols hence formulas L(). casepropositional logic, thus assume associate state truth assignmentprimitive propositions . first-order logic, assume associatestate domain interpretation symbols . probabilisticsetting, assume associate state assignment valuesrandom variables. often convenient assume state space factsubset W W(), set interpretations (or assignments to) vocabulary .Note truth formula L() determined state. truestate w, write w |= .probabilistic extension Lpr () base logic L() simply set probabilityformulas L(). Formally, L(), Pr() numeric term. formulasLpr () defined Boolean combinations arithmetic expressions involvingnumeric terms. example, Pr(fly | bird ) 1/2 formula Lpr ({fly, bird }) (whereinterpret conditional probability expression Pr( | ) Pr( )/ Pr()multiply clear denominator). analogy constraints, formula formPr() = 1 called objective formula.323fiHalpern & KollerGiven set W W(), assume FW algebra consisting setsform [[]]W = {w : w |= }, L(). (In case propositional logic,consists finite set primitive propositions, FW = 2W . case first-order logic,sets necessarily definable formulas, FW may strict subset 2W .)Let probability measure (W, FW ). ascribe semantics Lpr ()probability space (W, FW , ) straightforward way. particular, interpretnumeric term Pr() ({w W : w |= }). Since formula L() describes eventspace W , formula Lpr () clearly constraint measures W . write|= measure W satisfies formula .syntactic knowledge base KB Lpr () viewed constraint Wobvious way. Formally, KB represents set probability measures [[KB ]] W ,consists measures W |= KB .say KB (whether syntactic semantic) consistent [[KB ]]X 6= , i.e.,constraints satisfiable. Finally, say KB entails (where another setconstraints X ), written KB |=X , [[KB ]]X [[]]X , i.e., every measure satisfiesKB also satisfies . write |=X satisfied every measure X . omitsubscript X |= clear context.Entailment well-known weak method drawing conclusionsknowledge base, particular respect treatment irrelevant information.Consider knowledge base consisting constraint Pr(fly | bird ) 0.9. Eventhough know nothing suggest red relevant, entailment allow usreach nontrivial conclusion Pr(fly | bird red ).One way get powerful conclusions consider, measures satisfyKB , subset them. Intuitively, given knowledge base KB , inference procedurepicks subset measures satisfying KB , infers holds subset. Clearly,conclusions hold every measure subset hold every measureentire set.Definition 2.1 : (X, FX )-inference procedure partial function : 2(X,FX ) 72(X,FX ) I(A) (X,FX ) I(A) = iff = 2(X,FX )domain (i.e., defined). write KB |I I([[KB ]]X ) [[]]X .FX clear context irrelevant, often speak X-inference procedures.remark Paris (1994) considers calls inference processes.inference procedures defined that, given set probability measures,return unique probability measure (rather arbitrary subset A). Parisgives number examples inference processes. also considers various propertiesinference process might have. closely related various propertiesrepresentation indepedence consider. discuss Pariss work Section 7.Entailment X-inference procedure defined sets determined takingidentity. Maximum entropy also inference procedure sense.Definition 2.2: Given probability measure finite space X (where setsPmeasurable), entropy H() defined xX (x) log (x). (The log taken(A) consist measuresbase 2 here.) Given set measures X , let IX324fiRepresentation Dependencehighest entropy measures whose entropy least highmeasure A; measures, inf (A) undefined.easy see inf (A) defined closed (in topological sense; i.e.,n sequence probability measures n converges , A). Thus,could take domain infX consist closed sets measures X .also open sets inf (A) defined, although definedopen sets A. example, suppose X = {x1 , x2 } let = { : (x1 ) < 1/2}. Let 00 (x1 ) = 1/2. easy check H(0 ) = 1, H() < 1 A.However, , H() > 1 . followsmeasure whose entropy higher measure A, inf (A)undefined. hand, A0 = { : (x1 ) < 2/3}, measure whoseentropy maximum open set A0 , namely measure 0 .are, course, many inference procedures besides entailment maximum entropy defined measurable space. fact, following proposition shows,binary relation | satisfying certain reasonable properties inference proceduretype.Proposition 2.3: X-inference procedure following properties holdevery KB , KB 0 , , X KB domain I.Reflexivity: KB |I KB .Left Logical Equivalence: KB logically equivalent KB 0 , i.e., |= KB KB 0 ,every KB |I iff KB 0 |I .Right Weakening: KB |I |= KB |I .And: KB |I KB |I , KB |I .Consistency: KB consistent KB |6I false.Proof: Straightforward definitions.Interestingly, properties commonly viewed part core reasonableproperties nonmonotonic inference relation (Kraus, Lehmann, & Magidor, 1990).would like also prove converse, showing relation | probabilisticconstraints space X satisfies five properties must form|IX . quite true, following example shows.Example 2.4: Fix measurable space (X, FX ). Let language consist (finite)Boolean combination statements form Pr(S) , FX . fix onenonempty strict subset S0 X, let n statement Pr(S0 ) 1/n. Defineinference procedure | follows. KB equivalent true (i.e, [[KB ]]X 6= X ),KB | iff KB |= . hand, true | iff n |= sufficiently largen. is, true | exists N n N , n |= .easy check five properties Proposition 2.3 hold |. However, | |IX-inference procedure I. suppose were. Note n |= n m,325fiHalpern & Kollertrue | m. Thus, must I(X ) [[n ]]X n. followsIX (X ) [[Pr(S0 ) = 0]]X , true |I Pr(S0 ) = 0. However, n 6|= Pr(S) = 0 n,true | Pr(S) = 0. contradicts assumption | = |I .Essentially need get converse Proposition 2.3 infinitary versionVRule, would say KB |I i, KB |I . languageclosed infinite conjunctions, rule would fact need.Since assumed language closed infinite conjunctions, usevariant rule.Infinitary And: set statements, KB |I |= ,KB |I .Proposition 2.5: Let | relation probabilistic constraints Xproperties Reflexivity, Left Logical Equivalence, Right Weakening, Infinitary And, Consistency hold KB domain | . (That is, KB domain |KB | , KB | KB , on.) | |I X-inferenceprocedure I.Proof: See Appendix A.1.typically interested inference procedure defined one space X,family related inference procedures, defined number spaces. example,entailment inference procedure defined spaces X; maximum entropydefined finite measurable spaces (X, 2X ).Definition 2.6 : X set measurable spaces, X -inference procedure set{I(X,FX ) : (X, FX ) X }, I(X,FX ) (X, FX )-inference procedure (X, FX ) X .sometimes talk X -inference procedure I, write KB |I (X, FX )X clear context. However, stressed that, formally, X -inference procedure really set inference procedures (typically related natural way).Clearly entailment X -inference procedure X , IX simply identityfunction X X . X consists finite measurable spaces sets measurable,maximum entropy X -inference procedure. typically denote inferenceprocedure | . Thus, KB | holds probability measures maximumentropy satisfying KB .Important assumptions: remainder paper, deal X inference procedures X satisfies two richness assumptions. assumptionshold standard inference procedures considered.assume X closed crossproducts, (X, FX ), (Y, FY ) X ,(X Y, FXY ) X , FXY algebra formed taking finite unionsdisjoint sets form , FX FY . easy seealgebra, since = (S ) (S 0 0 ) =(S 0 ) (T 0 ) (from also follows union sets326fiRepresentation Dependencewritten disjoint union). Note X finite sets, FX = 2X ,FY = 2Y , FX FY = 2XY . shall see, (X Y, FXY ) X(X, FX ) (Y, FY ) X allows us relate constraints X constraintsnatural way.assume X contain sets finite cardinalities; precisely, n 2,exists set (X, FX ) X |X| = n FX = 2X . assumptionactually needed results, since assumption X closedcrossproducts already implies that, finite n, exists measurablespace (X, FX ) X |X| n; already suffices prove resultspaper. However, assuming X sets cardinalities makeproofs easier.also want domain satisfy certain assumptions, defer statingassumptions introduced additional definitions notation.3. Robustnessorder define robustness representation shifts, must first define notionrepresentation shift. first attempt definition based idea usingconstraints specify relationship two vocabularies. example, Example 1.1, might X = {colorful , colorless} = {red , blue, green, colorless}.specify relationship X via constraint asserts colorful(red blue green).course, every constraint legitimate mapping representations.example, formula asserted colorful obviously legitimate representationshift. minimum, must assume constraint give additionalinformation X far logical inference goes. syntactic level, usefollowing definition. Given knowledge base KB Lpr (), say Lpr ( 0 )-conservative KB if, formulas Lpr (), KB |= iff KB |= .Thus, adding knowledge base permit additional logical inferencesvocabulary . inference procedure robust unaffected conservativeextensions; is, KB , Lpr (), KB |I iff KB |I-conservative KB . Roughly speaking, says getting new informationuninformative far logical inference goes affect default conclusions.formal definition robustness, uses semantic rather syntactic concepts,extends intuitions arbitrary constraints measures (not onesexpressed language Lpr ).Definition 3.1: X1 ...Xn , define Xi Xi taking Xi (A) = (X1Xi1 Xi+1 Xn ). constraint Xi viewed constraintX1 ...Xn taking [[]]X1 ...Xn = { X1 ...Xn : Xi |= }. frequently identifyconstraints Xi constraints X1 . . . Xn way. B X1 Xn , defineproj Xi (B) = {Xi : B}. constraint X1 Xn said Xi -conservativeconstraint KB Xi proj Xi ([[KB ]]X1 Xn ) = [[KB ]]Xi .327fiHalpern & Kollersee definition generalizes earlier language-oriented definition, noteKB constraints X constraint XY , KB |= iffproj 1 ([[KB ]]XY ) [[]]X , KB |= iff [[KB ]]X [[]]X .Definition 3.2: {IX : X X } robust X -inference procedure spaces X, X ,constraints KB X , constraints XY X-conservativeKB , KB |IX iff KB |IXY . (Note definition implicitly assumesX X X, X , assumption made explicit earlier.)first glance, robustness might seem like reasonable desideratum. all,adding constraint XY places restrictions X change conclusions might reach X? Unfortunately, turns definitiondeceptively strong, disallows interesting inference procedures. particular,one property may hope inference procedure draw nontrivial conclusionsprobabilities events, is, conclusions follow entailment.example, maximum entropy (or inference procedure based symmetry) concludePr(p) = 1/2 empty knowledge base. show inference proceduresrobust really allow much way nontrivial conclusionsprobabilities events.Definition 3.3: (X, FX )-inference procedure essentially entailment knowledge base KB X FX , KB |I < Pr(S) < KB |= Pr(S) .essentially entailment X essentially entailment knowledge bases KBdomain IX .Thus, entailment lets us conclude Pr(S) [, ], inference procedureessentially entailment lets us draw slightly stronger conclusion Pr(S) (, ).prove this, need make three assumptions domain I. (For results,need assumptions domain I.)DI1. Pr(S) domain I(X,FX ) FX , , IR.DI2. KB domain IX , also domain IXY (when KBviewed constraint XY .)DI3. KB 1 KB 2 domain IX , KB 1 KB 2 .Note sets form Pr(S) closed sets. certainly seems reasonablerequire sets domain inference procedure; correspondbasic observations. DI2 seems quite innocuous; observed earlier, wantable view constraints X constraints XY ,prevent domain I. DI3 also seems reasonable assumption,since KB 1 KB 2 correspond possible observations, want able drawconclusions combining observations. DI3 holds domain consistsclosed sets. note hold take domain consistsets measure whose entropy maximum. example, X = {x1 , x2 },= {0 } { : (x1 ) > 3/4}, B = { : (x1 ) 2/3}, 0 (x0 ) = 1/2,B measure whose entropy maximum, B measurewhose entropy maximum.328fiRepresentation DependenceTheorem 3.4: {IX : X X } robust X -inference procedure satisfies DI1, DI2,DI3, IX essentially entailment X X .Proof: See Appendix A.2.possible construct robust inference procedures almost quiteentailment, simply strengthening conclusions Pr(S) [, ] Pr(S)(, ). Clearly, however, robust inference procedure extremely limited abilityjump conclusions. next section, look definition seems closerintuitive notion representation independence, somewhat reasonableconsequences.4. Representation Independence4.1 Representation shiftsX two different representations phenomena then, intuitively,way relating states X corresponding states . wantcorrespondence respect logical structure events. Formally, requirehomomorphism respect complementation intersection.Definition 4.1: (X, FX )-(Y, FY ) embedding f function f : FX 7 FYf (S ) = f (S) f (T ) f (S) = f (S) S, FX .elsewhere, talk X-Y embeddings rather (X, FX )-(Y, FY ) embeddingsFX FY play significant role.goal consider effect transformation probabilistic formulas. Hence,interested sets states probabilities.Definition 4.2: f X-Y embedding, X , , correspondf (S) = (f (S)) events FX . define mapping f : 2X 7 2Yfollows. first define f singleton sets (except that, convenience, write f ()rather f ({}) taking f () = { : (f (S)) = (S) FX }. Thus,f () consists measures correspond f . arbitrarysubset 2X , define f (D) = f () X .constraint X expressed language, typically write f () ratherf ([[]]X ). implicitly assume language constraint f () alsoexpressible. hard see f () constraint results replacing everyset FX appears f (S).Example 4.3: Example 1.1, might X = {colorful , colorless} = {red , blue,green, colorless}. case, might f (colorful ) = {red , blue, green} f (colorless) ={colorless}. Consider measure X (colorful ) = 0.7 (colorless) =0.3. f () set measures total probability assigned setstates {red , blue, green} 0.7. Note uncountably many measures.easy check constraint X Pr(colorful ) > 3/4, f ()Pr({red , blue, green}) > 3/4.329fiHalpern & KollerEmbeddings viewed semantic analogue syntactic notion interpretation defined (Enderton, 1972, pp. 157162), also used recentliterature abstraction (Giunchiglia & Walsh, 1992; Nayak & Levy, 1995). Essentially,interpretation maps formulas vocabulary formulas different vocabularymapping primitive propositions (e.g., colorful ) formulas (e.g.,red blue green) extending complex formulas obvious way. representation shift Example 1.2 also captured terms interpretation, onetaking flying-bird fly bird .Definition 4.4: Let two vocabularies. propositional case, interpretation function associates every primitive proposition pformula i(p) L(). complex definition spirit applies first-ordervocabularies. example, R k-ary predicate, i(R) formula k freevariables.Given interpretation i, get syntactic translation formulas L() formulasL() using obvious way; example, i((p q) r) = (i(p) i(q)) i(r)(see (Enderton, 1972) details). Clearly interpretation inducesembedding f W1 W() W2 W(): map [[]]W1 [[i()]]W2 .course, embeddings count legitimate representation shifts. example,consider embedding f defined terms interpretation maps propositions p q proposition r. process changing representations usingf gives us information p q equivalent, information mightoriginally. Intuitively, f gives us new information telling us certainsituationthat p q holdsis possible.formally, embedding ffollowing undesirable property: maps set states satisfying p qempty set. means state p q holds analogue newrepresentation. want disallow embeddings.Definition 4.5: X-Y embedding f faithful if, S, FX , ifff (S) f (T ).definition desired consequence disallowing embeddings give newinformation far logical consequence goes.Lemma 4.6: X-Y embedding f faithful constraints KB ,KB |= iff f (KB ) |= f ().Proof: See Appendix A.3.clear embedding Example 4.3 faithful: f (colorful ) = {red , blue, green}f (colorless) = colorless. following proposition gives insight faithfulembeddings.Proposition 4.7: Let f faithful X-Y embedding. following statementsequivalent:(a) correspond f ;330fiRepresentation Dependence(b) formulas , |= iff |= f ().Proof: See Appendix A.3.embedding f reasonable representation shift, would like inferenceprocedure return answers shift representations using f .Definition 4.8: X, X , X -inference procedure {IX : X X } invariantX-Y embedding f constraints KB X , KB |IX ifff (KB ) |IY f (). (Note that, particular, means KB domain | IXiff f (KB ) domain | IY .)Definition 4.9: X -inference procedure {IX : X X } representation independentinvariant faithful X-Y embeddings X, X .Since embedding Example 4.3 faithful, representation-independent inference procedure would return answers Pr(colorful ) Pr(red blue green).issue somewhat subtle Example 1.2. There, would like embedding f generated interpretation i(flying-bird ) = fly bird i(bird ) = bird .faithful embedding, since flying-bird bird valid formula,i(flying-bird bird ) (fly bird ) bird valid. Looking problem semantically, see state corresponding model flying-bird bird holdsmapped . clearly source problem. According linguistic intuitions domain, legitimate state. Rather considering statesW({flying-bird , bird }), perhaps appropriate consider subset X consisting truth assignments characterized formulas {flying-bird bird , flying-birdbird , flying-bird bird }. use embed X W({fly, bird }), resulting embedding indeed faithful. So, previous example, invarianceembedding would guarantee get answers representations.4.2 Representation-independent inference proceduresAlthough definition representation independence seems natural, definitionrobustness. two definitions relate other? First, show representation independence weaker notion robustness. result, needconsider inference procedures satisfy two assumptions.DI4. f faithful X-Y embedding, KB domain IX iff f (KB )domain IY .DI5. KB domain IXY , f faithful X-Y embedding, 1 constraintX , KB (1 f (1 )) domain IXY .DI4 natural satisfied standard inference procedures. easycheck KB closed iff f (KB ) closed. DI5 may appear natural,hold domains consisting closed sets, since hard check f (1 )closed. DI5 would follow DI3 assumption f (1 ) domainIXY , actually weaker combination two assumptions.particular, holds domain consisting sets measuremaximum entropy.331fiHalpern & KollerTheorem 4.10: X -inference procedure robust satisfies DI2, DI4, DI5,representation independent.Proof: See Appendix A.3.already shown robust inference procedure must almost trivial.interesting representation-independent inference procedures? shall see,answer mixed. nontrivial representation-independent inference procedures,interesting.first result shows representation independence, like robustness, trivializesinference procedure, knowledge bases.Theorem 4.11: {IX : X X } representation-independent X -inference procedurethen, X X , IX essentially entailment objective knowledge basesdomain.3Proof: See Appendix A.3.Corollary 4.12: {IX : X X } representation-independent X -inference procedure,KB objective, KB |I < Pr(S) < 0 1, = 0= 1.result tells us objective knowledge base Pr(T ) = 1, reachthree possible conclusions set S. S, conclude Pr(S) = 1;S, conclude Pr(S) = 0; otherwise, strongest conclusionmake Pr(S) somewhere 0 1.construct representation-independent inference procedure entailment precisely behavior restrict attention countable state spaces. Suppose X countable. Given objective knowledge base KB form Pr(T ) = 1,FX , let KB + consist formulas form 0 < Pr(S) < 1nonempty strict subsets FX .4 define X-inference procedure0 follows: KB equivalent objective knowledge base, KB |IXI0KB KB + |= ; KB equivalent objective knowledge base, KB |I 00 indeed inference procedure.KB |= . follows easily Proposition 2.5 IXMoreover, equivalent standard notion entailment; example,true |I 0 0 < Pr(p) < 1, 6|=0 < Pr(p) < 1. Nevertheless, prove 0representation independent.0 : X X } representationLemma 4.13: Let X consist countable sets. {IXindependent X -inference procedure.3. earlier version paper (Halpern & Koller, 1995), claimed representationindependent inference procedure satisfied minimal irrelevance property (implied robustness,equivalent it) essentially entailment knowledge bases. Jaeger (1996) shows,inference procedure along lines 1 described constructed show resultcorrect. seem need full strength robustness.4. requirement X countable necessary here. X uncountable every singleton FX ,KB + inconsistent uncountable. impossible uncountablecollection points positive measure.332fiRepresentation DependenceProof: See Appendix A.3.objective knowledge bases may appear interesting restrict propositional languages, languages include first-order statistical informationbecome quite interesting. Indeed, shown (Bacchus, 1990; Bacchus, Grove, Halpern, &Koller, 1996), knowledge bases first-order (objective) statistical information allowus express great deal information naturally encounter. example,express fact 90% birds fly objective statement numberflying birds domain relative overall number birds. course, Theorem 4.11applies immediately knowledge bases.Theorem 4.11 also implies various inference procedures cannot representationindependent. particular, since true | Pr(p) = 1/2 primitive proposition p,follows maximum entropy essentially entailment. observation providesanother proof maximum entropy representation independent.consistent Theorem 4.11 representation-independent inferenceprocedures almost entailment probabilistic knowledge bases. example,1 defined follows. Given , existsconsider X-inference procedure IXX1 (A) = { : (S) 1/3};FX = { X : (S) 1/4}, IXX1 (A) = A. Thus, Pr(S) 1/4 | Pr(S) 1/3. Clearly, 1 essentiallyotherwise, IXXI1entailment. Yet, prove following result.Lemma 4.14: Suppose X consists measure spaces form (X, 2X ),1 : X X } representation-independent X -inference procedure.X finite. {IXProof: See Appendix A.3.Note follows Theorem 3.4 1 cannot robust. Thus, shownrepresentation independence strictly weaker notion robustness.example might lead us believe representation-independent inference procedures interesting probabilistic knowledge bases. However,show, representation-independent inference procedure cannot satisfy one key desideratum: ability conclude independence default. example, important featuremaximum-entropy approach nonmononotic reasoning (Goldszmidt, Morris, & Pearl,1993) ability ignore irrelevant information, implicitly assuming independence. course, maximum entropy satisfy representation independence.result shows approach probabilistic reasoning simultaneously assurerepresentation independence default assumption independence.try give general notion default assumption independence here,since need result. Rather, give minimal property wouldhope inference procedure might have, show property sufficient precluderepresentation independence. Syntactically, property wantdisjoint vocabularies, KB Lpr (), L(), L(), KB |I Pr( ) =Pr() Pr().333fiHalpern & KollerDefinition 4.15: X -inference procedure {IX : X X } enforces minimal default independence if, whenever X X , KB constraint X domain |IX ,FX , FY , KB |IXY Pr(S ) = Pr(S) Pr(T ).5definition clearly generalizes syntactic definition.Clearly, entailment satisfy minimal default independence. Maximum entropy,however, does. Indeed, semantic property implies minimal default independenceused Shore Johnson (1980) one axioms axiomatic characterizationmaximum-entropy.Theorem 4.16: {IX : X X } X -inference procedure enforces minimal defaultindependence satisfies DI1, IX representation independent.Proof: See Appendix A.3.result interesting far irrelevance concerned. might hopelearning irrelevant information affect conclusions. attemptdefine irrelevance here, certainly would expect KB 0 vocabulary disjointKB , then, example, KB |I Pr() = iff KB KB 0 |I Pr() = . KB 0objective, standard probabilistic approach would identify learning KB 0conditioning KB 0 . Suppose restrict inference procedures indeedcondition objective information (as case class inference proceduresconsider Section 6). KB KB 0 |I Pr() = exactly KB |I Pr( | KB 0 ) = .Thus, Theorem 4.16 tells us inference procedures condition new (objective)information cannot representation independent ignore irrelevant information.Thus, although representation independence, unlike robustness, force us useentirely trivial inference procedures, prevent us using procedurescertain highly desirable properties.5. Discussionresults suggest type representation independence hard come by.also raise concern perhaps definitions quite right. provideseems even support latter point.Example 5.1: Let Q unary predicate c1 , . . . , c100 , constant symbols. Supposetwo vocabularies = {Q, d} = {Q, c1 , . . . , c100 , d}. Considerinterpretation i(d) = i(Q(x)) = Q(x) Q(c1 ) . . . Q(c100 ).Now, consider KB = xQ(x). case, i(KB ) = x(Q(x) Q(c1 ) . . . Q(c100 ).Intuitively, since ci may refer domain element, conclusionmake certainty Q(c1 ) . . . Q(c100 ) exists least one Qdomain, gives us additional information beyond KB . convert examplegeneral argument embedding f corresponding faithful. Intuitively,5. Since working space X , KB viewed constraint XY here, Pr(S)understood Pr(S ), Pr(T ) understood Pr(X ). Recall that,assumption, X X .334fiRepresentation DependenceKB , get conclusion Q(c1 ) . . . Q(c100 ) f (KB ) Q(x) appearspositively KB ; but, case, already know least one Q,gain new information embedding. seem unreasonableinference procedure assign different degrees belief Q(d) given KB = xQ(x)one hand given i(KB ) = x(Q(x) Q(c1 ) . . . Q(c100 )) other,6 particularlydomain small. fact, many reasoning systems explicitly adopt unique namesassumption, would clearly force different conclusions two situations.example suggests that, least first-order case, even faithful embeddingsalways match intuition reasonable representation shift. One mighttherefore think perhaps problem definition even propositionalcase. Maybe totally different definition representation independence avoidsproblems. possible, believe case. techniquesused prove Theorem 4.16 3.4 seem apply reasonable notionrepresentation independence.7 give flavor type argument used provetheorems, consider Example 1.1, assume true |I Pr(colorful ) = (0, 1).8Using embedding g g(colorful ) = red , conclude true |I Pr(red ) = .Similarly, conclude Pr(blue) = Pr(green) = . order |Iinvariant original embedding, must true |I Pr(red blue green) = ,completely inconsistent previous conclusions. embeddings useargument natural ones; would want definition representationindependence disallowed them.results viewed support position representation dependencejustified; choice appropriate representation encodes significant information.particular, encodes bias knowledge-base designer world. Researchersmachine learning long realized bias inevitable component effectiveinductive reasoning (i.e., learning evidence). completely surprisedturns types leaping conclusions (as context) also dependbias.need little careful here. example, cases identifyvocabulary (and hence, representation) sensors agentdisposal. may seem unreasonable agent temperature sensormotion sensor might carve world differently agent color sensordistance sensor. consider two agents different sensors yetmade observations. Suppose talk distance tree.reasonable two agents reach different conclusions distancedifferent sensors (and thus use different vocabularies), althoughmade observations? would follow agents changeconclusions switched sensors, despite made observations.seem reasonable!Bias representation independence viewed two extremes spectrum.accept knowledge base encodes users bias, obligation6. Actually, i(Q(d)) = Q(d) Q(c1 ) . . . Q(c100 ), latter equivalent Q(d) given KB .7. certainly applied many definitions tried!8. fact, suffices assume true |I Pr(colorful ) [, ], long > 0 < 1.335fiHalpern & Kollerinvariant representation shifts all. hand, assumerepresentation used carries information, coherence requires inference proceduregive answers equivalent representations. believe right answer lies somewhere between. typically number reasonable waysrepresent information, might want inference procedure returnconclusions matter choose. thus makes sense requireinference procedure invariant embeddings take us one reasonable representation another. follow must invariantembeddings, even embeddings syntactically similar ones wishallow. may willing refine colorful red blue green define flying-birdfly bird , transform red sparrow . next section, showconstruct inference procedures representation independent limited classrepresentation shifts.6. Selective invariancediscussed above, want construct inference procedure invariantcertain embeddings. purposes section, restrict attention finitespaces, sets measurable. is, focus X -inference proceduresX consists measure spaces form (X, 2X ), X finite.first step understand conditions X -inference procedureinvariant specific X-Y embedding f . conclude KB X ?Recall inference procedure IX picks subset DX = IX (KB ), concludes iffholds every measure DX . Similarly, applied f (KB ) , IY picks subsetDY = IY (f (KB )). invariant f respect KB ,tight connection DX DY .understand connection, first consider pair measures X .Recall Proposition 4.7 correspond f iff, formulas ,|= iff |= f (). understand correspondence extends sets probabilitymeasures, consider following example:Example 6.1: Consider embedding f Example 4.3, let DX = {, 0 }above, 0 (colorful ) = 0.6. guarantee reach correspondingconclusions DX DY ? Assume, example, DY contains measurecorrespond either 0 , e.g., measure assigns probability 1/4four states. case, conclusion Pr(colorful ) 0.7 holds DX ,holds measures; corresponding conclusion Pr(red blue green) 0.7hold DY . Therefore, every probability measure DY must correspondmeasure DX . Conversely, every measure DX must correspond measure DY .suppose measure DY corresponding . get conclusionPr(blue red green) 6= 0.7 DY , corresponding conclusion Pr(colorful ) 6= 0.7follow DX . Note two conditions imply DY mustprecisely set measures corresponding measures DX . particular, mightDY containing single measure corresponding (and least one corresponding0 ), e.g., one (red ) = 0.5, (blue) = 0, (green) = 0.2, (colorless) = 0.3.336fiRepresentation DependenceBased example, use following extension definition correspondence.Definition 6.2: say DX DY correspond f DY , existscorresponding DX (so (S) = (f (S)) X), DX ,exists corresponding DY .Proposition 6.3: Suppose f faithful X-Y embedding, DX X , DY .following two conditions equivalent:(a) DX DY correspond f ;(b) , DX |= iff DY |= f ().9Proof: See Appendix A.4.produce inference procedure invariant X-Y embedding f ,must ensure every KB , IX (KB ) IY (KB ) correspond. first glance,seems rather difficult guarantee correspondence every knowledge base. turnssituation bad. remainder section, show how, startingcorrespondence knowledge base truethat is, starting correspondenceIX (X ) IY (Y )we bootstrap correspondence KB s, usingstandard probabilistic updating procedures.First consider problem updating objective information. standard wayupdate via conditioning. measure X event X, define|S measure assigns probability (w)/(S) every w S, zerostates. set measures DX X , define DX |S {|S : DX }.following result easy show.Proposition 6.4: Let X event let f faithful X-Y embedding.correspond f , |S |f (S) also correspond f .Proof: Almost immediate definitions; left reader. (In case, noteresult follows Theorem 6.4 below.)Clearly, result extends sets measures.Corollary 6.5: f faithful X-Y embedding, DX DY correspond f ,DX |S DY |f (S) also correspond f .want update constraint objective? standard extensionconditioning case via relative entropy KL-divergence (Kullback & Leibler,1951).9. (a) implies (b) arbitrary spaces, implication (b) (a) depends restrictionfinite spaces made section. suppose X natural numbers N , f identity, DXconsists probability measures N , DY consists measures measure 00 (n) = 1/2n+1 . language consists finite Boolean combinations assertions formPr(S) , N , easy see DX |= iff DY |= formulas , clearly DXDY correspond identity map.337fiHalpern & KollerDefinition 6.6: 0 measures X, relative entropy 0 ,Pdenoted KLX (0 k), defined xX 0 (x) log(0 (x)/(x)). measure Xconstraint , let | denote set measures 0 satisfying KLX (0 k)minimal.Intuitively, KL-divergence measures distance 0 . measure 0 satisfying KLX (0 k) minimal thought closest measuresatisfies . denotes objective constraint, unique measure satisfying KLX (0 k) minimal conditional measure | (Kullback & Leibler,1951). (That deliberately used notation conditioning.)Moreover, easy show KLX (0 k) = 0 iff 0 = . follows ,| = .Given set measure DX X constraint X , define DX | DX |.apply well-known result (see, e.g., (Seidenfeld, 1987)) generalize Proposition 6.4 case relative entropy.Theorem 6.7: Let arbitrary constraint X . f faithful X-Y embeddingcorrespond f , | |f () also correspond f .Proof: See Appendix A.4.Again, result clearly extends sets measures.Corollary 6.8: f faithful X-Y embedding, DX DY correspond f ,DX | DY |f () also correspond f .results give us way bootstrap invariance. construct inference procedure uses relative entropy starting set prior probability measures. Intuitively, encode users prior beliefs domain. information comes in,measures updated using cross-entropy. design priors certain invariances hold, Corollary 6.8 guarantees invariances continue hold throughoutprocess.Formally, prior function P X maps X X set P(X) probability measuresP (KB ) = P(X)|KB . NoteX . Define inference procedure P taking IXP (true) = P(X), constraints all, use P(X) basisIXinference. standard inference procedures form Pprior function P. fairly straightforward verify, example, entailment PP(X) = X . (This because, observed earlier, |KB = KB .) StandardBayesian conditioning (defined objective knowledge bases) form, takeP(X) single measure space X. interestingly, well known (Kullback& Leibler, 1951) maximum entropy Pu Pu (X) singleton set containinguniform prior X.say robustness P representation shifts? Using Proposition 6.3 Corollary 6.5, easy show want P invariantset F embeddings, must ensure prior function rightcorrespondence property.Theorem 6.9: f faithful X-Y embedding, P invariant f iff P(X)P(Y ) correspond f .338fiRepresentation DependenceProof: See Appendix A.4.Theorem 6.9 sheds light maximum entropy inference procedure.mentioned, | precisely inference procedure based prior function Pu .corollary asserts | invariant f precisely uniform priors Xcorrespond f . shows maximum entropys lack representationindependence immediate consequence identical problem uniform prior.class F embeddings maximum entropy invariant? Clearly,answer yes. easy see embedding takes elements X (disjoint)sets equal cardinality correspondence property required Theorem 6.9. followsmaximum entropy invariant embeddings. fact, requirementmaximum entropy invariant subset embeddings one axiomsShore Johnsons (1980) axiomatic characterization maximum-entropy. (We remarkParis (1994, Theorem 7.10) proves maximum entropy satisfies variantatomicity principle; invariance result essentially special case Theorem 6.9.)like behavior maximum entropy representation shifts, Theorem 6.9 provides solution: simply start different prior function.want maintain invariance representation shifts, P(X) must includenon-extreme priors (i.e., measures X (A)/ {0, 1}/ {, X}). set priors gives essential entailment inference procedure. If,however, prior knowledge embeddings encode reasonable representation shifts, often make smaller class priors, resulting inferenceprocedure prone leap conclusions. Given class reasonable embeddings F, often find prior function P closed f F, i.e.,measure P(X) X-Y embedding f F make surecorresponding measure P(Y ), vice versa. Thus, guarantee Pappropriate structure using process closing f F.course, also execute process reverse. Suppose want supportcertain reasoning pattern requires leaping conclusions. classical examplereasoning pattern is, course, default assumption independence.representation independence get without losing reasoning pattern?show, Theorem 6.9 gives us answer.begin providing one plausible formulation desired reasoning pattern.finite space X, say X1 Xn product decomposition X X =X1 Xn n largest number X written product way.(It easy see X finite, maximal product decomposition unique.)measure X product measure X X1 Xn product decompositionX exist measures Xi = 1, . . . , n = 1 n ,Qis, (U1 Un ) = ni=1 (Ui ), Ui Xi , = 1, . . . , n. Let P set productmeasures X. P prior relative entropy rule used update priorgiven knowledge base, |P satisfies form minimal default independence.fact, easy show satisfies following stronger property.339fiHalpern & KollerProposition 6.10: Suppose X1 Xn product decomposition X and,= 1, . . . , n, KB constraint Xi , Si subset Xi .n^KB |IP Pr(S1 . . . Sn ) =i=1nPr(Si ).i=1Proof: See Appendix A.4.Theorem 4.16 shows |P cannot invariant embeddings. Theorem 6.9tells us invariant precisely embeddings P invariant.embeddings characterized syntactically natural way. Suppose 1 , . . . , npartition finite set primitive propositions. Note truth assignmentprimitive propositions viewed crossproduct truth assignmentsprimitive propositions 1 , . . . , n . identification, suppose set X truthassignments decomposed X1 Xn , Xi consists truth assignments. case, p j q, r k j 6= k, true |P Pr(p q) =Pr(p) Pr(q), since since q r subset, true |P Pr(rq) = Pr(r) Pr(q). Hence, P invariant interpretation maps pr maps q itself. Intuitively, problem crossing subset boundaries;mapping primitive propositions different subsets subset.restrict interpretations thatpreserve subset boundaries, avoid problem.get semantic characterization follows. product decompositionX X1 Xn product decomposition Y1 Yn , fX-Y product embedding f X-Y embedding Xi -Yi embeddings fi ,= 1, . . . , n, f (hx1 , . . . , xn i) = f1 (x1 ) fn (xn ). Product embeddings captureintuition preserving subset boundaries; elements given subset Xi remainsubset (Yi ) embedding. However, notion product embeddingsomewhat restrictive; requires elements ith subset X map elementsith component , = 1, . . . , n. still preserve default independencecomponents product permuted. g permutation embedding existspermutation {1, . . . , n} g(hx1 , . . . , xn i) = hx(1) , . . . , x(n) i.Theorem 6.11: inference procedure IP invariant faithful product embeddingspermutation embeddings.Theorem 6.9 thus provides us basic tools easily define inference procedureenforces minimal default independence constraints involving disjoint partslanguage, time guaranteeing invariance large natural classembeddings. Given negative result Theorem 4.16, type result bestcould possibly hope for. general, Theorem 6.9 provides us principledframework controlling tradeoff strength conclusionsreached inference procedure invariance representation shifts.7. Related Workmentioned earlier, two types probabilistic inference. partitiondiscussion related work along lines.340fiRepresentation Dependence7.1 Probabilistic Inference Knowledge BaseGiven importance representation reasoning, fact one main criticisms maximum entropy sensitivity representation shifts, surprisinglittle work problem representation dependence. Indeed,best knowledge, work focused representation independencelogical sense considered prior Salmon Paris.Salmon (1961) defined criterion linguistic invariance, seems essentially equivalent notion representation independence. tried use criterion defendone particular method inductive inference but, pointed Barker commentary end (Salmon, 1961), preferred method satisfy criterion either.Salmon (1963) attempted define modified inductive inference method wouldsatisfy criterion clear attempt succeeded. case, resultsshow modified method certainly cannot representation independent sense.said earlier, Paris (1994) considers inference processes, given constraintX , choose unique measure satisfying constraint. considers various propertiesinference process might have. Several closely related propertiesconsidered here. (In describing notions, made inessentialchanges able express notation.)X -inference process language invariant X, X constraints KBX , KB |IX iff KB |IXY . Clearly language invariancespecial case robustness. Paris shows center mass inference process(that, given set X , chooses measure center mass A)language invariant; hand, well known maximum entropylanguage invariant.X -inference process satisfies principle irrelevant informationspaces X, X , constraints KB X , constraints ,KB |IX iff KB |IXY . Again, special case robustness, sinceconstraint must X-conservative. Paris shows maximum entropysatisfies principle. (He restricts domain maximum entropy processclosed convex sets, always unique probability measure maximizesentropy.)X -inference process satisfies renaming principle if, whenever Xfinite spaces, g : X isomorphism, f : 2X 2Y faithful embeddingbased g (in f (S) = {g(s) : S}), constraints KB X ,KB |IX iff f (KB ) |IY f (). Clearly, renaming principle specialcase representation independence. Paris shows number inference processes(including maximum entropy) satisfy renaming principle.X -inference process satisfies principle independence if, whenever X, ,Z X , FX , FY , U FZ , KB constraint Pr(U ) =Pr(S|U ) = b Pr(T |U ) = c, > 0, KB | Pr(S |U ) = bc. Ignoring conditional probabilities, clearly special case minimal defaultindependence. Paris Vencovska (1990) show maximum entropy unique341fiHalpern & Kollerinference process satisfying number principles, including renaming, irrelevantinformation, independence.X -inference process satisfies atomicity principle if, X, Y1 , . . . , YnX , whenever f 0 embedding {0, 1} X, f obvious extensionf 0 embedding {0, 1} Y1 . . . Yn X Y1 . . . Yn ,constraints KB {0,1}Y1 ...Yn , KB |IX iff f (KB ) |IY f ().Clearly atomicity special case representation independence. Paris showsinference process satisfies atomicity. argument similar spiritused prove Theorems 4.11 4.16, much simpler, since inferenceprocesses return unique probability measure, set them.recently, Jaeger (1996), building definitions, examined representationindependence general nonmonotonic logics. considers representation independencerespect collection transformations, proves results degreecertain nonmonotonic formalisms, rational closure (Lehmann & Magidor,1992), satisfy representation independence.Another line research relevant representation independence workabstraction (Giunchiglia & Walsh, 1992; Nayak & Levy, 1995). Although goalwork make connections two different ways representingsituation, significant differences focus. work abstraction, two waysrepresenting situation expected equivalent. Rather, one representationtypically abstracts away irrelevant details present other. hand,treatment issues terms deductive entailment, terms generalinference procedures. would interesting combine two lines work.7.2 Bayesian Probabilistic InferenceBayesian statistics takes different perspective issues discuss paper.discussed, Bayesian approach generally assumes construct prior, usestandard probabilistic conditioning update prior new information obtained.approach, representation knowledge obtained effect conclusions.Two pieces information semantically equivalent (denote event)precisely effect used condition distribution.paradigm, analysis directly related step precedesprobabilistic conditioningthe selection prior. specific beliefswant encode prior distribution (as do, example, constructingBayesian network), design prior reflect beliefs terms vocabularyused. example, particular distribution mind location object,encode one way representing space terms Cartesian coordinates,another way using polar coordinates. effect, view representationtransformation embedding f , two priors corresponding f ,sense Definition 4.2. Thus, design prior already takes representationaccount.hand, trying construct uninformed prior classproblems, issue representation independence becomes directly relevant. Indeed,342fiRepresentation Dependencestandard problems maximum entropy arise even simple casesimply Bayesian conditioning starting uniform prior space.standard approach Bayesian statistics use invariance certain transformations order define appropriate uninformed prior. example, mightwant prior images invariant rotation translation. certain cases,specify transformation want measure invariant,measure uniquely determined (Jaynes, 1968; Kass & Wasserman, 1993). case,argument goes, uniquely determined measure perforce right one. ideapicking prior using invariance properties spirit approach takeSection 6. Indeed, approach simply uses standard probabilistic conditioningobjective information (such observations), Bayesian approach uninformedprior invariant set embeddings is, sense, special case. However, approachforce us choose unique prior. Rather, allow use set priordistributions, allowing us explore wider spectrum inference procedures.approach also related work Walley (1996), observes representation independence important desideratum certain statistical applications involvingmultinomial data. Walley proposes use sets Dirichlet densities encode ignoranceprior, shows approach representation independent domainapplication.8. Conclusionspaper takes first step towards understanding issue representation dependence probabilistic reasoning, defining notions invariance representation independence, showing representation independence incompatible drawing manystandard default conclusions, defining limited notions invariance mightallow compromise desiderata able draw interesting conclusions(not already entailed evidence) representation independence. focusinference probabilistic logic, notion representation independenceimportant many contexts. definitions clearly extendednon-probabilistic logics. mentioned, Jaeger (1996) obtained results representation independence general setting, clearly muchdone. generally, would interest understand better tensionrepresentation independence strength conclusions drawninference procedure.AcknowledgmentsThanks Ed Perkins pointing us (Keisler & Tarski, 1964) and, particular,result countably additive probability measure defined subalgebra algebraF could necessarily extended countably additive probability measure F.Thanks reviewers paper perceptive comments pointing(Horn & Tarski, 1948). Much Halperns work paper doneIBM Almaden Research Center. recent work supported NSF343fiHalpern & Kollergrant IRI-96-25901 IIS-0090145 ONR grant N00014-01-1-0795.Kollers work done U.C. Berkeley. research sponsored part AirForce Office Scientific Research (AFSC), Contract F49620-91-C-0080,University California Presidents Postdoctoral Fellowship. Daphne Kollers later workpaper supported generosity Powell foundation, ONRgrant N00014-96-1-0718. preliminary version appears Proceedings IJCAI 95,pp. 18531860.Appendix A. ProofsA.1 Proofs Section 2Proposition 2.5: Let | relation probabilistic constraints Xproperties Reflexivity, Left Logical Equivalence, Right Weakening, Infinitary And, Consistency hold KB domain | . (That is, KB domain | ,KB | , KB | KB , on.) | |I X-inferenceprocedure I.Proof: Define follows. X , KB domain | , = [[KB ]]Xstatement KB , domain I(A) = {[[]]X : KB | }.Note Left Logical Equivalence, well defined, since = [[KB 0 ]]X ,{[[]]X : KB | } = {[[]]X : KB 0 | }. 6= [[KB ]]X statement KB ,domain I. remains check X-inference procedure (i.e.,I(A) I(A) = iff = domain I), | = |I .check X-inference procedure, suppose domain I. Thus,= [[KB ]]X Reflexivity, easily follows I([[KB ]]X ) [[KB ]]X . Next supposeI([[KB ]]X ) = . follows {[[]]X : KB | } = . Thus, { : KB | } |= false.Infinitary rule, must KB |I false. Consistency Rule, follows[[KB ]]X = . Thus, indeed X-inference procedure. Finally, note KB |then, definition I, I([[KB ]]X ) [[]]X , KB |I . opposite inclusion, noteKB |I , { : KB | } |= . Thus, Infinitary rule, followsKB | .A.2 Proofs Section 3prove Theorem 3.4, need following lemma.Lemma A.1: Given two spaces X0 X1 , measures 0 (X0 ,FX0 ) 1 (X1 ,FX1 ) ,subsets S0 FX0 S1 FX1 0 (S0 ) = 1 (S1 ), exists measure2 (X0 X1 ,FX0 X1 ) 2Xi = , = 1, 2, 2 (S0 S1 ) = 1.10Proof: B FX0 FX1 , define2 (A B) = (0 (A S0 )1 (B S1 )/1 (S1 )) + (0 (A S0 )1 (B S1 )/1 (S1 )),take 0 (A S0 )1 (B S1 )/1 (S1 ) = 0 1 (S1 ) = 0 take 0 (A S0 )1 (BS1 )/1 (S1 ) = 0 1 (S1 ) = 0. Extend disjoint unions sets additivity. Since10. B sets, use notation B denote set (A B) (A B).344fiRepresentation Dependencesets FX0 X1 written disjoint unions sets form B FX0 FX1 ,suffices define 2 . see 2 actually measure, note 2 (X ) =0 (S0 ) + 0 (S0 ) = 1. Additivity clearly enforced definition. Finally, see2 desired properties, suppose 1 (S1 ) 6= 0 1 (S1 ) 6= 0. (The argumenteasier case; leave details reader.)2X0 (A) = 2 (A ) = 0 (A S0 )1 (S1 )/1 (S1 ) + 0 (A S0 1 (S1 )/1 (S1 )= 0 (A S0 ) + 0 (A S0 ) = 0 (A).Since 0 (S0 ) = 1 (S1 ) assumption (and 0 (S0 ) = 1 (S1 )),2X1 (B) = 2 (X B) = 0 (S0 )1 (B S1 )/1 (S1 ) + 0 (S0 )1 (B S1 )/1 S1 )= 1 (B S1 ) + 1 (B S1 ) = 1 (B).completes proof.Theorem 3.4: {IX : X X } robust X -inference procedure satisfies DI1, DI2,DI3, IX essentially entailment X X .Proof: Suppose {IX : X X } robust IX essentially entailment X X .must constraint KB X set FX KB |I < Pr(S) <KB 6|= Pr(S) . Thus, must/ [, ] KB Pr(S) =consistent. assume without loss generality < (otherwise replaceS).first construct space Y0 X subsets U1 , . . . , Un following properties:(a) measure Y0 (Ui ) > , = 1, ..., n.(b) i, measure 0i Y0 0i (Ui ) = 1 0i (Uj ) >j 6= i.proceed follows. Choose n < (d 1)/(n 1) < d/n < .assumption, exists Y0 X |Y0 | = n!/(n d)!. Without loss generality,assume Y0 consists tuples form (a1 , . . . , ad ), aidistinct, 1 n. Let Ui consist tuples Y0 somewheresubscript; easy see d(n 1)!/(n d)! tuples. Supposeprobability measure Y0 . easy see (U1 ) + + (Un ) = d, sincetuple Y0 exactly Ui gets counted exactly times, sumprobabilities tuples 1. Thus, cannot (Ui ) > d/n (and, fortiori,cannot (Ui ) > i). takes care first requirement. Next, considerprobability distribution 0i makes tuples making Ui equally probable,gives tuples probability 0. easy see 0i (Ui ) = 1. Moreover,since straightforward check exactly d(d 1)(n 2)!/(n d)! tuplesUi Uj j 6= i, 0i (Uj ) = [d(d 1)(n 2)!/(n d)!]/[d(n 1)!/(n d)!] =(d 1)/(n 1). takes care second requirement.assumption, also measurable space X |Y | = 2. Suppose= {y, 0 }. Let Z = X n Y0 n , n n chosenconstruction Y0 . Again, assumption, Z X . = 1, . . . , n,345fiHalpern & KollerX, let Ai = X i1 X ni Y0 n Z.let KB = { Z : Xi KB };let Yi subset n ith copy replaced {y};let Vi subset Z form X n Ui Yi (where U1 , . . . , Un subsetsYi constructed above).Let following constraint Z :KB 1 . . . KB n Pr(S1 V1 ) = 1 . . . Pr(Sn Vn ) = 1.Let Xi denote ith copy X Z. is, ease exposition, view Zform X1 Xn Y0 , although Xi identical, sincehelpful able refer specific Xi . claim Xi -conservative KB ,= 1, . . . , n. Thus, must show proj Xi ([[KB ]]Z ) = [[KB ]]X . immediateproj Xi ([[KB ]]Z ) [[KB ]]X . opposite inclusion, suppose [[KB ]]X .must show exists [[KB ]]Z Xi = . proceedfollows.Let 00 measure Y0 00 (Ui ) = 1 00 (Uj ) > , j 6= i.construction Uj s, measure must exist. j {1, . . . , n}, let 0j measure0i (y) = (S) j 6= i, 0j (y) = /00 (Uj ) (and 0j (y 0 ) = 1 0j (y)).Let 0 measure Y0 n crossproduct 00 , 01 , . . . , 0n . is,0 (T0 Tn ) = 00 (T0 ) 0n (Tn ). construction, 0 (Vj ) = j 6=0 (Vi ) = (S).assumption, measure 0 X 0 |= KB Pr(S) = .proceed inductively define measure k X k Y0 n (a) Pr((S1V1 ) . . . (Sk Vk )) = 1, (b) jY = 0 jXj = j = 1, . . . , k. define 0 = 0 .inductive step, simply apply Lemma A.1. Finally, take n .construction guarantees X j = , hence |= KB j . addition, constructionguarantees |= Pr(S1 V1 ) = 1 . . . Pr(Sn Vn ) = 1. Hence |= , desired.follows DI1, DI2, DI3 domain IZ . Since KBequivalent , follows KB also domain IZ . Now, robustness,constraint Xi , KB |I iff KB |I . Since KB |I Pr(Si ) >KB equivalent , follows |I Pr(Si ) > = 1, . . . , n. rule(Proposition 2.3), follows |I Pr(S1 ) > . . . Pr(Sn ) > . Since |= Pr((S1V1 ) (Sn Vn )) = 1, easily follows |I Pr(U1 ) > . . . Pr(Un ) > .construction guarantees Pr(U1 ) > . . .Pr(Un ) > inconsistent. Thus, |I false.robustness, follows KB |I false. happen KB |= false,implies KB |= Pr(S) , contradicting original assumption.A.3 Proofs Section 4prove Lemma 4.6, useful first prove two additional results:Lemma A.2: f X-Y embedding, f (X) = f () = .346fiRepresentation DependenceProof: Suppose f X-Y embedding. first show f () = .definition embedding, follows f () = f (X ) = f (X) f (). Thus, f () f (X).definition embedding also implies f () = f (X) = f (X). Thus,f (X) f (X). happen f (X) = f () = f (X) = .Lemma A.3: f faithful X-Y embedding,(a) X , measure corresponds ;(b) , measure X corresponds .Proof: prove (a), consider algebra subsets form f (S), FX .Define function 0 algebra via 0 (f (S)) = (S). mapping well defined,f (S) = f (T ), faithfulness guarantees = . Moreover, 0 probabilitymeasure algebra. see this, note Lemma A.2 0 (Y ) = 0 (f (X)) = (X) = 1.Moreover, f (S) f (T ) = , (by definition embedding) f (S ) = so, sincef faithful, = (for otherwise f (S ) = f () Lemma A.2, 6= ).Thus,0 (f (S) f (T )) = 0 (f (S )) = (S ) = (S) + (T ) = 0 (f (S)) + 0 (f (T )).shown Horn Tarski (1948), possible extend 0 probability measureFY .11 construction, corresponds .prove (b), use similar process. Define function algebra setsX via (S) = (f (S)). easy see already probability measure X ,construction corresponds .prove Lemma 4.6.Lemma 4.6: X-Y embedding f faithful constraints KB ,KB |= iff f (KB ) |= f ().Proof: Suppose f faithful. show KB |= iff f (KB ) |= f (), mustshow [[KB ]]X [[]]X iff [[f (KB )]]Y [[f (]]Y . direction immediatedefinition f . prove direction, suppose not. must exist[[KB ]]X [[]]X f () [[f ()]]Y . Let probability measurecorresponds . Since f () f (), must 0 [[]]Xf (0 ). Since 0 6= , must FX 0 (S) 6= (S). Sincef () f (0 ), must (f (S)) = (S) (f (S)) = 0 (S).contradiction. completes proof direction.converse, suppose KB |= iff f (KB ) |= f () KB . GivenS, FX , following chain equivalences:11. critical result working finitely additive measures. maycountably additive measure extending 0 , even 0 countably additive. example, take FY0Borel sets [0, 1] take FY subsets [0, 1]. Let 0 Lebesgue measure. knownthat, continuum hypothesis, countably additive measure extending 0 definedsubsets [0, 1] (Ulam, 1930) (see (Keisler & Tarski, 1964) discussion).347fiHalpern & KollerSTiff Pr(S) = 1 |= Pr(T ) = 1iff f (Pr(S) = 1) |= f (Pr(T ) = 1) (by assumption)iff Pr(f (S)) = 1 |= Pr(f (T )) = 1 (by definition f )iff f (S) f (T ).Thus, f faithful.Proposition 4.7: Let f faithful X-Y embedding. following statementsequivalent:(a) correspond f ;(b) formulas , |= iff |= f ().Proof: first show (a) implies (b). suppose correspond f .direction (b) trivial: |= f () f (), since f faithful.direction, proceed much proof Lemma 4.6. Assume |= f ()6|= . Since f (), definition f must 0 [[]]Xf (0 ). Since 0 |= whereas 6|= , must 6= 0 . Hence, mustFX (S) 6= 0 (S). Since f () f (0 ), follows (f (S)) = (S)(f (S)) = 0 (S), gives desired contradiction.show (b) implies (a). Assume contradictioncorrespond f . must event FX (S) 6= (f (S)).Let p = (S) let constraint Pr(S) = p. |= , whereas 6|= f (),providing desired contradiction.Theorem 4.10: X -inference procedure robust satisfies DI2, DI4, DI5,representation independent.Proof: Suppose {IX : X X } robust X -inference procedure. want showrepresentation independent. suppose KB , constraints X fX-Y embedding, X, X . want show KB |IX iff f (KB ) |IY f ().Let following constraint XY :( f ()) (KB f (KB )).claim X-conservative KB -conservative f (KB ). Thus, mustshow proj X ([[KB ]]XY ) = [[KB ]]X proj ([[f (KB ) ]]XY ) = [[f (KB )]]Y .show proj X ([[KB ]]XY ) = [[KB ]]X here; argument proj ([[f (KB )]]XY ) = [[f (KB )]]Y almost identical.Clearly [[KB ]]XY X [[KB ]]X , proj X ([[KB ]]XY ) [[KB ]]X .opposite inclusion, suppose [[KB ]]X . want find measure 00 = . Let 00 measure f () let 0[[KB ]]XY ) XXY0 = .crossproduct 00 ; is, 0 (A B) = (A) 00 (B). Clearly Xsee 0 [[KB ]]XY ), clearly suffices show 0 |= . since 00correspond f , immediate Proposition 4.7 |= KB iff 00 |= f (KB )|= iff 00 |= f (). Thus, |= , desired.348fiRepresentation Dependencesuppose KB |IX . DI2 DI5, KB domain IXY .robustness, KB |IXY . Thus, I([[KB ]]XY ) [[]]XY . Since I([[KB ]]XY )[[KB ]]XY [[ f ()]]XY , follows I([[KB ]]XY ) [[f ()]]XY .Moreover, KB equivalent f (KB ) , I([[f (KB ) ]]XY ) [[f ()]]XY ,i.e., f (KB ) |IXY f (). DI4, f (KB ) domain IY . Since conservative f (KB ), robustness {IX : X X } implies f (KB ) |IY f ().opposite implication (if f (KB ) |IY f () KB |IX ) goes way. Thus,{IX : X X } representation independent.Next, turn attention Theorems 4.11 4.16. results followrelatively straightforward way one key proposition. state it, needdefinitions.Definition A.4: say constraint KB X depends S1 , . . . , Sk FX(the sets S1 , . . . , Sk necessarily disjoint) if, whenever , 0 X agree S1 , . . . , Sk ,|= KB iff 0 |= KB .example, KB form Pr(S1 ) > 1/3 Pr(S2 ) 3/4, KB dependsS1 S2 . Similarly, KB form Pr(S1 | S2 ) > 3/4, KB dependsS1 S2 .Definition A.5: Given S1 , . . . , Sk FX , atom S1 , . . . , Sk set formT1 . . . Tk , Ti either Si Si .Proposition A.6: Suppose {IX : X X } X -inference procedure and,X X , exist S, S1 , . . . , SK FX consistent constraint KB X dependsS1 , . . . , Sk , following two conditions satisfied:nonempty every nonempty atom S1 , . . . , Sk ,KB |IX < Pr(S) < , either > 0 < 1.{IX : X X } representation independent.Proof: Suppose, way contradiction, {IX : X X } representation-independentinference procedure nevertheless, X X , exists sets S, S1 , . . . , Sk FXknowledge base KB satisfies conditions above, , . Assume> 0 (a similar argument used deal case < 1).Let T1 , . . . , TM nonempty atoms S1 , . . . , Sk . Choose N 1/N < .goal find collection f1 , . . . , fN embeddings X Xembeddings effect KB , sets fj (S) disjoint.Since KB |IX Pr(fj (S)) > j = 1, . . . , N , fj (KB ) = f (KB ) j = 1, . . . , N ,follow f (KB ) |IY Pr(fj (S)) > j = 1, . . . , N , contradiction. proceedfollows.assumption, exists set Z X |Z| = N . Let = X Z.Since X closed crossproducts, X . Suppose Z = {z1 , . . . , zM N }, letZi = {zN (i1)+1 , . . . , zN }, = 1, . . . , . Thus, Zi partition Z disjoint sets,cardinality N . Let Bi = X Zi , let Bij = X {zN (i1)+j }, j = 1, . . . , N .easy see find faithful X-Y embeddings f1 , . . . , fN349fiHalpern & Koller1. fj (Ti ) = Bi , = 1, . . . , , j = 1, . . . , N ,2. fj (Ti S) = Bij , = 1, . . . , , j = 1, . . . , N .Notice need assumption Ti Ti nonempty T1 , . . . , TM(that is, nonempty atom S1 , . . . , Sk ) guarantee find faithfulembeddings. Ti = , since fj embedding, f (Ti S) = 6= Bi ;Ti = , fj (Ti S) = fj (Ti ) f (Ti S)) = , means Bi = Bij ,inconsistent construction.easy check that, since KB depends S1 , . . . , Sk , fj (KB ) dependsfj (S1 ), . . . , fj (Sk ), j = 1, . . . , N . next show fj (Si ) independent j;is, fj (Si ) = fj 0 (Si ) 1 j, j 0 N . Notice h = 1, . . . , k,fj (Sh ) = Ti Sh fj (Ti ) = {i:Ti Sh } Bi . Thus, fj (Sh ) independent j, desired. Sincefj (KB ) depends fj (S1 ), . . . , fj (Sk ), must independent j. Let KBf1 (KB ) (which, observed, identical f2 (KB ), . . . , fk (KB )).Since, assumption, {IX : X X } representation independent, KB |IX P r(S) >, KB |IY Pr(fj (S)) > , j = 1, . . . , N . Thus, KB |IY Pr(f1 (S)) >. . . Pr(fN (S)) > . note that, construction, fj (S) = {i:Ti S6=} Bij . Thus,sets fj (S) pairwise disjoint. Since > 1/N , cannot N disjoint setsprobability greater . Thus, KB |IY false. KB consistent, KB = fj (KB )must well. Thus, IY (KB ) 6= , assumption. contradicts conclusionKB |IY false. Thus, {IX : X X } cannot representation independent.use Proposition A.6 help prove Theorem 4.11.Theorem 4.11: {IX : X X } representation-independent X -inference procedurethen, X X , IX essentially entailment objective knowledge basesdomain.Proof: Suppose, way contradiction, {IX : X X } representation independentIX essentially entailment X X objective knowledge base KB .must set FX KB |IX < Pr(S) < KB 6|=Pr(S) . Without loss generality, assume KB form Pr(T ) = 1FX . Moreover, assume 6= , nonempty, measurablestrict subset. (For otherwise, choose = {y, 0 } X consider space X 0 = X .assumption, X 0 X . Let f X-Y embedding maps U FX U . Sincerepresentation independent, Pr(T ) = 1 |I < Pr(S ) < ,{y} .)nonempty, let Z nonempty, measurable strict subset (which existsassumption); otherwise let Z empty set. Let U set (T S) (T Z). Notice= U . Moreover, since, set V , Pr(T ) = 1 Pr(V ) = Pr(V ) valid,follows Reflexivity Right Weakening KB |IX Pr(V ) = Pr(V ). Thus,KB |IX Pr(S) = Pr(S ) = Pr(U ) = Pr(U ). follows KB |IX < Pr(U ) < .want apply Proposition A.6. Note KB depends . Thus,must show U U nonempty, nonempty, UU well. observed above, U = S. Thus, U = , S,contradicting assumption KB |I Pr(S) > 0. easy see U = S.Again, cannot U = , S, contradicting assumption350fiRepresentation DependenceKB |I Pr(S) < 1. construction, U = Z = Z. assumption, 6= ,Z 6= . Finally, U = Z; again, construction, nonempty set 6= .follows Proposition A.6 {IX : X X } representation independent.Corollary 4.12: {IX : X X } representation-independent X -inference procedure,X X , KB objective knowledge base putting constraints X ,KB |IX < Pr(S) < 0 1, = 0 = 1.Proof: Assume hypotheses corollary hold. Since KB objective,form Pr(T ) = 1 FX . three possibilities. Either (1) S, (2)S, (3) nonempty. (1) holds, KB |= Pr(S) = 1,(2) holds, KB |= Pr(S) = 0. Thus, (1) (2) incompatibleKB |IX < Pr(S) < . hand, (3) holds, easy see, Pr(S) = consistent KB (since probability measure assignsprobability probability 1 S). Since KB |IX < Pr(S) < ,Theorem 4.11, must KB |= Pr(S) . follows choicestrue = 0 = 1.Theorem 4.16: {IX : X X } X -inference procedure enforces minimaldefault independence satisfies DI1, IX representation independent.Proof: Suppose {IX : X X } X -inference procedure enforces minimaldefault independence satisfies DI1. Choose X = {x, x0 } X let KB 1/3Pr(x) 2/3. assumption, X X X . view KB constraint XX ;case, interpreted 1/3 Pr({x} X) 2/3. DI1, KBdomain IXX . Note KB equivalent constraint 1/3 Pr(x0 ) 2/3.minimal default independence, KB |IXX Pr((x, x)) > Pr(x X)/3KB |IXX Pr((x0 , x0 )) > Pr(x0 X)/3. Applying straightforward probabilistic reasoning,get KB |IXX Pr({(x, x), (x0 , x0 )}) > 1/3. apply Proposition A.6, taking{(x, x), (x0 , x0 )} 0 {(x, x), (x, x0 )}. Note KB depends 0 .almost immediate definition 0 0 , 0 , 0 ,0 nonempty. Thus, Proposition A.6, {IX : X X } representationindependent.0 : X X } representationLemma 4.13: Let X consist countable sets. {IXindependent X -inference procedure.Proof: said main part text, easily follows Proposition 2.50 inference procedure X X , since easily seen five propIXerties described proposition. see 0 representation independent, supposef faithful X-Y embedding, X, X . Clearly KB objectivef (KB ) objective. KB objective, easy see KB |I 0 ifff (KB ) |I 0 f (), since |I 0 reduces entailment case. suppose KB objective form Pr(T ) = 1, FX . KB |I 0 iff KB KB + |= .Lemma 4.6, holds iff f (KB )f (KB + ) |= f (). hand, f (KB ) |I 0 f ()iff f (KB ) (f (KB ))+ |= f () Thus, suffices show f (KB ) f (KB + ) |= f ()iff f (KB ) (f (KB ))+ |= f (). easy show (f (KB ))+ implies f (KB + ),f (KB ) f (KB + ) |= f () f (KB ) (f (KB ))+ |= f (). necessarily351fiHalpern & Kollercase f (KB + ) implies (f (KB ))+ . example, consider embedding describedExample 4.3. case, KB objective knowledge base Pr(colorful ) = 1,KB + empty, hence f (KB + ), (f (KB ))+ includes constraints0 < Pr(green) < 1. Nevertheless, suppose f (KB ) (f (KB ))+ |= f () and, waycontradiction, |= f (KB ) f (KB + ) f (). Choosef (). correspond, |= KB KB + . easyshow exists 0 f () 0 < 0 (S) < 1 nonempty subsetsf (T ). see this, note (x) 6= 0, suffices ensure 0 (f (x)) = (x)0 (y) 6= 0 f (x). Since countable, straightforward. Since 0correspond, must 0 |= f () f (KB ). construction, 0 |= (f (KB ))+ .contradicts assumption f (KB ) (f (KB ))+ |= f ().Lemma 4.14: Suppose X consists measure spaces form (X, 2X ),1 : X X } representation-independent X -inference procedure.X finite. {IXProof: Suppose X, X , KB constraints X , f X-Yembedding. must show KB |I 1 iff f (KB ) |I 1 f (). purposesXproof, say subset X interesting exists FX= { X : (S) 1/4}. easy see KB interesting f (KB )interesting. converse also true, given assumption X consists finitespaces sets measurable. suppose f (KB ) interesting.set f (KB ) = { : (T ) 1/4}. Let = {S 0 X : f (S 0 ) }.Since X finite, A; easily follows = A.12 Clearly (S) 1/4,f () f (KB ), [[KB ]]X . Thus, [[KB ]]X { X : (S) 1/4}.hand, KB , f () f (KB ). Thus, f (), since A, mustcase (S) = (f (S)) (T ) 1/4. Thus, [[KB ]]X { X : (S) 1/4}.follows KB equivalent Pr(S) 1/4, must interesting. (We must also= f (S), although needed result.)KB interesting, KB |I 1 iff KB |= iff f (KB ) |= f () (since entailmentXrepresentation independent) iff f (KB ) |I 1 . hand, KB interesting,KB equivalent Pr(S) 1/4 X, f (KB ) equivalentPr(f (S)) 1/4. Moreover, KB |I 1 iff Pr(S) 1/3 |= iff Pr(f (S)) 1/3 |= f () iffXf (KB ) |I 1 . Thus, get representation independence, desired.A.4 Proofs Section 6Proposition 6.3: Suppose f faithful X-Y embedding, DX X , DY .following two conditions equivalent:(a) DX DY correspond f ;(b) , DX |= iff DY |= f ().12. general true X infinite without additional requirement f (i Ai ) = f (Ai )arbitrary unions.352fiRepresentation DependenceProof: prove (a) implies (b), assume way contradiction that, ,DX |= DY 6|= f (). DY 6|= f (). Let DXmeasure corresponding . Then, Proposition 4.7, 6|= , desiredcontradiction. proof direction (a) identical.prove (b) implies (a), first consider measure DX . must find DYcorresponds . Suppose X = {x1 , . . . , xn } (recall restrictingfinite spaces Section 6) (xi ) = ai , = 1, . . . , n. Let constraintni=1 Pr({xi }) = ai . assumptions language, constraintlanguage. Clearly [[]]X = {}. Since DX , know DX 6|= . Hence, DY 6|=f (), exists DY 6 f (). Hence f () = f ({}).definition f , corresponds .consider measure DY , let measure X corresponds. Assume way contradiction 6 DX . Taking above, followsDX |= and, therefore, assumption, DY |= f (). Thus, |= f (). |= and,assumption, correspond. contradicts Proposition 4.7.Theorem 6.7: Let arbitrary constraint X . f faithful X-Y embeddingcorrespond f , | |f () also correspond f .Proof: Assume correspond f . Recall assuming sectionX finite space; let X = {x1 , . . . , xn }. Let Yi = f (xi ). Given distribution00 , define i00 = 00 |Yi let (f )1 ( 00 ) denote unique 00 X00 f (00 ).suppose 0 |. Define 0 measure0 (y) = 0 (xi ) (y),index Yi . Since = |Yi , follows (Yi ) = 1. Thus,0 (Yi ) = (xi ), 0 leaves relative probabilities elements within Yi. easy verify 0 0 correspond. Hence, Proposition 4.7, 0 |= f ().claim 0 |f (). show that, need show KLY ( 0 k) minimalamong KLY ( 00 k) 00 |= f (). follows standard properties relativeentropy (Cover & Thomas, 1991, Theorem 2.5.3) 00 ,KLY ( 00 k) = KLX ((f )1 ( 00 )k(f )1 ()) +nXKLY (i00 ki ).(1)i=1Note = i0 , KLY (i0 ki ) = 0, = 1, . . . , n. Thus, follows (1)KLY ( 0 k) = KLX (0 k).Now, let 00 00 |= f () let 00 = (f )1 (00 ). Since 00 00correspond f , follows Proposition 4.7 00 |= . Using (1) again,KLY ( 00 k) = KLX (00 k) +nXi=1KLX (00 k).353KLY (i00 ki )fiHalpern & Kollersince 0 |, know KLX (0 k) KLX (00 k). Hence concludeKLY ( 00 k) KLY ( 0 k),0 |f ().Theorem 6.9: f faithful X-Y embedding, P invariant f iff P(X)P(Y ) correspond f .Proof: Suppose f faithful X-Y embedding. definition, P invariantf iff, KB , ,KB | P iff f (KB ) | P f ().(2)definition P , (2) holds iffP(X)|KB [[]]X iff P(Y )|f (KB ) [[f ()]]Y KB , .(3)Proposition 6.3, (3) holds iff P(X)|KB P(Y )|f (KB ) correspond KB .Corollary 6.5, P(X) P(Y ) correspond, P(X)|KB P(Y )|f (KB ) correspondKB . hand, P(X)|KB P(Y )|f (KB ) correspond KB ,P(X) P(Y ) must correspond: simply take KB = true observe P(X)|KB ) =P(X) P(Y )|f (KB ) = P(Y ).Proposition 6.10: Suppose X1 Xn product decomposition X and,= 1, . . . , n, KB constraint Xi , Si subset Xi .n^i=1KB |IP Pr(S1 . . . Sn ) =nPr(Si ).i=1Proof: KB satisfiable constraint Xi , = 1, . . . , n, exist productVmeasures X satisfying constraints ni=1 KB . product measures preciselyVnmeasures P |( i=1 KB ). Since measures satisfies Pr(S1 . . . Sn ) =Qni=1 Pr(Si ) assumption, conclusion holds case. constraint KBsatisfiable, result trivially holds.Theorem 6.11: inference procedure IP invariant faithful product embeddings permutation embeddings.Proof: Suppose f faithful X-Y product embedding, X1 Xn productdecomposition X, Y1 Yn product decomposition . showP invariant f , suffices show P (X) P (Y ) correspond f .Supposethat P (Y ). = 1 n , measure Xi , = 1, . . . , n.Moreover, since f product embedding, exist f1 , . . . , fn f = f1 fn .Let fi (i ), = 1, . . . , n. easy check = 1 n f ().Conversely, suppose P (Y ). = 1 n , Yi= 1, . . . , n. Define Xi setting (S) = (fi (S)). Since fi faithful Xi -Yi354fiRepresentation Dependenceembedding, easy check Xi fi (i ). Thus, f ().completes proof P invariant faithful X-Y product embeddings.argument P invariant faithful X-X permutation embeddingssimilar (and easier). leave details reader.ReferencesBacchus, F. (1990). Representing Reasoning Probabilistic Knowledge. MIT Press,Cambridge, Mass.Bacchus, F., Grove, A. J., Halpern, J. Y., & Koller, D. (1996). statistical knowledgebases degrees belief. Artificial Intelligence, 87 (12), 75143.Cover, T. M., & Thomas, J. A. (1991). Elements Information Theory. Wiley, New York.Enderton, H. B. (1972). Mathematical Introduction Logic. Academic Press, New York.Giunchiglia, F., & Walsh, T. (1992). theory abstraction. Artificial Intelligence, 56 (23),323390.Goldszmidt, M., Morris, P., & Pearl, J. (1993). maximum entropy approach nonmonotonic reasoning. IEEE Transactions Pattern Analysis Machine Intelligence,15 (3), 220232.Halpern, J. Y., & Koller, D. (1995). Representation dependence probabilistic inference.Proc. Fourteenth International Joint Conference Artificial Intelligence (IJCAI95), pp. 18531860.Horn, A., & Tarski, A. (1948). Measures Boolean algebras. Transactions AMS,64 (1), 467497.Jaeger, M. (1996). Representation independence nonmonotonic inference relations.Principles Knowledge Representation Reasoning: Proc. Fifth InternationalConference (KR 96), pp. 461472.Jaynes, E. T. (1968). Prior probabilities. IEEE Transactions Systems ScienceCybernetics, SSC-4, 227241.Jaynes, E. T. (1978). stand maximum entropy?. Levine, R. D., & Tribus,M. (Eds.), Maximum Entropy Formalism, pp. 15118. MIT Press, Cambridge,Mass.Kahneman, D., Slovic, P., & Tversky, A. (Eds.). (1982). Judgment Uncertainty:Heuristics Biases. Cambridge University Press, Cambridge/New York.Kass, R. E., & Wasserman, L. (1993). Formal rules selecting prior distributions: reviewannotated bibliography. Tech. rep. Technical Report #583, Dept. Statistics,Carnegie Mellon University.355fiHalpern & KollerKeisler, J., & Tarski, A. (1964). accessible inaccessible cardinals. FundamentaMathematica, 53, 225308.Kraus, S., Lehmann, D., & Magidor, M. (1990). Nonmonotonic reasoning, preferentialmodels cumulative logics. Artificial Intelligence, 44, 167207.Kullback, S., & Leibler, R. A. (1951). information sufficiency. Annals Mathematical Statistics, 22, 7686.Lehmann, D., & Magidor, M. (1992). conditional knowledge base entail?.Artificial Intelligence, 55, 160.Nayak, P. P., & Levy, A. Y. (1995). semantic theory abstractions. Proc. FourteenthInternational Joint Conference Artificial Intelligence (IJCAI 95), pp. 196203.Paris, J. B. (1994). Uncertain Reasoners Companion. Cambridge University Press,Cambridge, U.K.Paris, J., & Vencovska, A. (1990). note inevitability maximum entropy. International Journal Approximate Reasoning, 4 (3), 183224.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann, SanFrancisco.Salmon, W. (1961). Vindication induction. Feigl, H., & Maxwell, G. (Eds.), CurrentIssues Philosophy Science, pp. 245264. Holt, Rinehart, Winston, NewYork.Salmon, W. (1963). vindicating induction. Kyburg, H. E., & Nagel, E. (Eds.),Induction: Current Issues, pp. 2754. Wesleyan University Press, Middletown,Conn.Seidenfeld, T. (1987). Entropy uncertainty. MacNeill, I. B., & Umphrey, G. J. (Eds.),Foundations Statistical Inferences, pp. 259287. Reidel, Dordrecht, Netherlands.Shore, J. E., & Johnson, R. W. (1980). Axiomatic derivation principle maximum entropy principle minimimum cross-entropy. IEEE TransactionsInformation Theory, IT-26 (1), 2637.Ulam, S. (1930). Zur masstheorie der allgemeinen mengenlehre. Fundamenta Mathematicae, 16, 140150.Walley, P. (1996). Inferences multinomial data: learning bag marbles.Journal Royal Statistical Society, Series B, 58 (1), 334. Discussionpaper various commentators appears pp. 3457.356fiJournal Artificial Intelligence Research 21 (2004) 193-243Submitted 09/03; published 02/04Generalizing Boolean Satisfiability I: BackgroundSurvey Existing WorkHeidi E. Dixondixon@cirl.uoregon.eduCIRLComputer Information Science1269 University OregonEugene, 97403 USAMatthew L. Ginsbergginsberg@otsys.comTime Systems, Inc.1850 Millrace, Suite 1Eugene, 97403 USAAndrew J. Parkesparkes@cirl.uoregon.eduCIRL1269 University OregonEugene, 97403 USAAbstractfirst three planned papers describing zap, satisfiability enginesubstantially generalizes existing tools retaining performance characteristicsmodern high-performance solvers. fundamental idea underlying zap manyproblems passed engines contain rich internal structure obscuredBoolean representation used; goal define representation structureapparent easily exploited improve computational performance. papersurvey work underlying zap, discusses previous attempts improveperformance Davis-Putnam-Logemann-Loveland algorithm exploiting structure problem solved. examine existing ideas including extensionsBoolean language allow cardinality constraints, pseudo-Boolean representations, symmetry, limited form quantification. paper intended survey,research results contained two subsequent articles, theoretical structurezap described second paper series, zaps implementation describedthird.1. Introductionfirst planned series three papers describing zap, satisfiability enginesubstantially generalizes existing tools retaining performance characteristicsmodern high-performance solvers zChaff (Moskewicz, Madigan, Zhao, Zhang,& Malik, 2001).1 Many Boolean satisfiability problems incorporate rich structure reflects properties domain problems arise, zap includesrepresentation language allows structure described proof engineexploits structure improve performance. first paper describes work1. second two papers published technical reports (Dixon, Ginsberg, Luks, & Parkes,2003b; Dixon, Ginsberg, Hofer, Luks, & Parkes, 2003a) yet peer reviewed.c2004AI Access Foundation. rights reserved.fiDixon, Ginsberg & Parkeszap based, intended survey existing results attemptuse problem structure improve performance satisfiability engines. resultsdiscuss include generalizations Boolean satisfiability include cardinality constraints,pseudo-Boolean representations, symmetry, limited form quantification. second paper series (Dixon et al., 2003b) describes theoretical generalizationsubsumes extends ideas, third paper (Dixon et al., 2003a) describeszap system itself.intention review work satisfiability introduction DavisPutnam-Logemann-Loveland algorithm (Davis, Logemann, & Loveland, 1962) presentday. work (thankfully), portion work thoughtattempt improve performance systematic methods exploiting generalstructure problem question.therefore include description recent work extending language Booleansatisfiability include restricted form quantification (Ginsberg & Parkes, 2000)pseudo-Boolean constraints (Barth, 1995, 1996; Chandru & Hooker, 1999; Dixon & Ginsberg, 2000; Hooker, 1988; Nemhauser & Wolsey, 1988); case, representationalextension corresponds existence structure hidden Boolean axiomatization. discuss length interplay desire speed search exploitingstructure danger slowing search using unwieldy representations. Somewhatsurprisingly, see effective representational extensions appear incur little overhead result implementation concerns. observationbetter representations better implementations led us searchgeneral sort structure zap exploits.also discuss attempts made exploit symmetrical structure satisfiability problems (Brown, Finkelstein, & Paul Walton Purdom, 1988;Crawford, 1992; Crawford, Ginsberg, Luks, & Roy, 1996; Joslin & Roy, 1997; Krishnamurthy, 1985; Puget, 1993). work appears modest impactdevelopment satisfiability engines generally, explain why: authors (Crawford, 1992; Crawford et al., 1996; Joslin & Roy, 1997; Krishnamurthy, 1985; Puget, 1993)exploit global symmetries, symmetries vanishingly rare naturally occurring problems. methods described exploiting local emergentsymmetries (Brown et al., 1988; Szeider, 2003) incur unacceptable computational overheadnode search. general arguments regarding interplay representation search suggest one identify local symmetries problemformulated, exploit symmetries throughout search.discuss heuristic search substantial literature relating it.collective eye, Boolean axiomatization obscure natural structuresearch problem, heuristics. argued elsewhere (Ginsberg & Geddis, 1991)domain-dependent search control rules never poor mans standingeneral principles based problem structure. selection survey material reflectsbias.remarked entry zap series survey paper; extentresearch contribution, overall (and, believe, novel) focustaking. basic view target new work areaspecific representational extension pseudo-Boolean first-order encoding,194fiBoolean Satisfiability Surveydirect exploitation underlying problem structure. paper originaldescribing existing work way, first time viewing first-order pseudoBoolean extensions purely structure-exploitation techniques. First-order pseudoBoolean representations effective history usefulness,mind allow identification capture two particulartypes structure inherent many classes problems. hope readerviews material presenting (and companion papers well)light: Recent progress Boolean satisfiability best thought terms structureexploitation. perspective approach paper, hopeyou, reader, come share it.let us return Davis-Putnam-Logemann-Loveland procedure (Davis et al.,1962), appears begun body work development solverssufficiently powerful used practice wide range problems. Descendantsdpll algorithm solution method choice many problems includingmicroprocessor testing verification (Biere, Clarke, Raimi, & Zhu, 1999; Copty, Fix,Giunchiglia, Kamhi, Tacchella, & Vardi, 2001; Velev & Bryant, 2001), competitivedomains planning (Kautz & Selman, 1992).return pure algorithmics dpll successors shortly, let us beginnoting spite impressive engineering successes many difficult problems,many easy problems Boolean satisfiability engines struggle. includeproblems involving parity, well known pigeonhole problem (stating cannotput n + 1 pigeons n holes pigeon needs hole), problemsdescribed naturally using first-order opposed ground representation.cases, structure problem solved structurelost ground encoding built. testament power Booleanmethods solve large difficult problems without access underlyingstructure, seems reasonable expect incorporating using structure wouldimprove performance further. survey techniques improve dpll suggestfact case, since techniques underpin performance modernBoolean satisfiability engines well understood way.turning details analysis, however, let us flesh frameworkbit more. computational procedure performance improved, seemthree ways done:1. Replace algorithm.2. Reduce time spent single iteration inner loop.3. Reduce number times inner loop must executed.first approach focus here; many potential competitorsdpll, none seems outperform dpll practice.2 Work second approach2. Systematic alternatives include original Davis-Putnam (1960) method, polynomial calculus solvers(Clegg, Edmonds, & Impagliazzo, 1996) based Buchbergers (1965, 1985) Groebner basis algorithm,methods based binary decision diagrams bdds (Bryant, 1986, 1992), direct first-order methods195fiDixon, Ginsberg & Parkeshistorically focused reducing amount time spent unit propagation,indeed appear represent inner loop dpll implementations.variety techniques available reducing number calls innerloop, divide follows:3a. Algorithmic improvements require representational changes.3b. Algorithmic improvements requiring representational changes.Attempts improve dpll without changing underlying Boolean representationfocused (i) development mechanism retaining information developed oneportion search subsequent use (caching or, commonly, learning) (ii)development search heuristics expected reduce size spaceexamined. work, however, using dpll analyze problems solutionexists equivalent building resolution proof unsatisfiability underlyingtheory, number inference steps bounded number inferencesshortest proof (Mitchell, 1998). lengths generally exponential problemsize worst case.Work involving representation change overcome difficulty reducing lengthproof dpll similar algorithm implicitly trying construct. Representations sought certain problems known require proofs exponential lengthBoolean case admit proofs polynomial length representational shift made.leads hierarchy representational choices, one representation r1 saidpolynomially simulate p-simulate another r2 proofs using representation r2converted proofs using r1 polynomial time. general, representations leadefficient proofs via efficient encodings, single axiom improvedrepresentation corresponds exponentially many original. many excellentsurveys proof complexity literature (Beame & Pitassi, 2001; Pitassi, 2002; Urquhart,1995), generally repeat material here.course, sufficient simply improve epistemological adequacy proofsystem; heuristic adequacy must maintained improved well (McCarthy, 1977).therefore assume representation introduced purposes navigatingp-simulation hierarchy also preserves basic inference mechanism dpll (resolution)maintains, ideally builds upon, improvements made propagation performance (2)learning (3a). context survey shall consider representationalchanges.representations consider following:Boolean axiomatizations. original representation used dpll, provides basic setting progress propagation, learning branchingtaken place.employed otter (McCune & Wos, 1997). Nonsystematic methods include wsatfamily (Selman, Kautz, & Cohen, 1993), received great deal attention 1990s stillappears method choice randomly generated problems specific setsinstances. general, however, systematic algorithms roots dpll tend outperformalternatives.196fiBoolean Satisfiability SurveyCardinality constraints. disjunctive Boolean axiom states least onedisjuncts true, cardinality constraint allows one state least ndisjuncts true integer n.Pseudo-Boolean constraints. Taken operations research, pseudo-Boolean constraint formXwi li kwi positive integral weights, li literals, k positive integer.Cardinality constraints special case wi = 1 i; Boolean constraintk = 1 well.Symmetric representations. problems (such pigeonhole problem)highly symmetric, possible capture symmetry directly axiomatization. variety authors developed proof systems exploitsymmetries reduce proof size (Brown et al., 1988; Crawford, 1992; Crawford et al.,1996; Joslin & Roy, 1997; Krishnamurthy, 1985; Puget, 1993).Quantified representations. many approaches quantified satisfiability, focus change underlying complexityproblem solved.3 requires domains quantificationfinite, focus propositional restriction first-order logic knownqprop (Ginsberg & Parkes, 2000).Given arguments regarding heuristic adequacy generally, goal surveycomplete following table:representationalefficiencyp-simulationhierarchyinferenceunitpropagationlearningSATcardinalitypseudo-BooleansymmetryQPROPfirst column simply names representational system question. each,describe:Representational efficiency (3b): many Boolean axioms capturedsingle axiom given representation?p-simulation hierarchy (3b): representation relative othersp-simulation hierarchy?Inference: possible lift basic dpll inference mechanism resolutionnew representation without incurring significant additional computational expense?3. Problems involving quantified Boolean formulae, qbf, pspace-complete (Cadoli, Schaerf, Giovanardi, & Giovanardi, 2002) opposed np-complete problems considered dpll directsuccessors.197fiDixon, Ginsberg & ParkesUnit propagation (2): techniques used speed unit propagation liftednew setting? new techniques available available Booleancase?Learning (3a): existing learning techniques lifted new setting? newtechniques available available Boolean case?cannot overstress fact single column table importantothers. reduction proof length little practical value associatedreduction amount computation time required find proof. Speeding timeneeded single inference hardly useful number inferences required growsexponentially.Boolean satisfiability community pioneered many techniques used reduce perinference running time understand learning declarative setting. spitefact resolution Boolean satisfiability among weakest inference systemsterms representational efficiency position p-simulation hierarchy (Pudlak,1997), almost none powerful proof systems wide computational use. Findingproofs sophisticated settings difficult; even direct attempts lift dpllfirst-order setting (Baumgartner, 2000) seem fraught complexity inabilityexploit recent ideas led substantial improvements algorithmic performance.usable proof systems often theoretically powerful.paper organized along rows table trying complete. Booleantechniques described next section; recount demonstration pigeonhole problem exponentially difficult setting (Haken, 1985). go Section 3discuss cardinality constraints pseudo-Boolean methods, showing earlierdifficulties pigeonhole problem overcome using either methodsimilar issues remain cases. Following descriptions implemented pseudoBoolean reasoning systems (Barth, 1995; Dixon & Ginsberg, 2000), show keycomputational ideas Boolean case continue applicable pseudo-Booleansetting.Axiomatizations attempt exploit symmetry directly discussed Section 4.draw distinction approaches require existence global symmetries,tend exist practice, use local ones, existdifficult find inference proceeds.Section 5, discuss axiomatizations Boolean descriptions problemsnaturally represented using quantified axioms. discuss problems arisingfact ground theories tend exponentially larger liftedcounterparts, show working first-order axiomatization directly leadlarge improvements efficiency overall system (Ginsberg & Parkes, 2000).Concluding remarks contained Section 6.2. Boolean SatisfiabilityDefinition 2.1 variable simply letter (e.g., a) either true false.literal either variable negation variable. clause disjunction literals,198fiBoolean Satisfiability SurveyBoolean satisfiability problem (in conjunctive normal form), sat problem,conjunction clauses.solution sat problem C assignment values lettersevery clause C satisfied.None new. Satisfiability sat instances well-known NPcomplete (Cook, 1971), language reasonably natural one encoding realworld problems. remarked introduction, classic algorithm solvingproblems depth-first search augmented ability set variables whose valuesforced:Procedure 2.2 (Davis-Putnam-Logemann-Loveland) Given sat problem Cpartial assignment P values variables, compute dpll(C, P ):123456789P Unit-Propagate(P )P contains contradictionreturn failureP assigns value every variablereturn successl literal assigned value Pdpll(C, P {l = true}) = successreturn successelse return dpll(C, P {l = false})Variables assigned values two ways. first, unit propagation, clause setexamined existing partial assignment new consequential assignmentsidentified. Somewhat specifically (see below), clauses found satisfiedliterals exactly one unvalued literal. clause, unvalued literal valuedsatisfy clause. process repeated contradiction encountered,solution found, clauses meet necessary conditions. unit propagationfunction terminates without reaching contradiction finding solution, variableselected assigned value, procedure recurs.practice, choice branch literal crucial performance algorithm.(Note choosing branch literal l instead l, also select ordervalues tried underlying variable.) Relatively early work dpll focusedselection branch variables produced many unit propagations possible, thusreducing size residual problems solved recursively. seeSection 2.3, however, recent ideas appear effective.Missing Procedure 2.2, however, description propagation process.is:Procedure 2.3 (Unit propagation) compute Unit-Propagate(P ):1234contradiction found c C Psatisfied literals exactly one unassigned literalv variable c unassigned PP P {v = V : V selected c satisfied}return P199fiDixon, Ginsberg & Parkes100ZCHAFF data% time spent95908580757001020304050total CPU time (sec)60708090Figure 1: Fraction CPU time spent unit propagationDpll variety well-known theoretical properties. sound completeevery candidate solution returned solution original problem,solution returned one exists (and failure eventually reported solution exists).practical point view, running time dpll obviously potentially exponential size problem, since iteration, possibly single variableassigned value routine invoked recursively. practice, course, unitpropagation reduce number branch points substantially, running timeremains exponential difficult instances (Crawford & Auton, 1996).also point difficult problem instances, running timenecessarily spent exploring portions search space solutions.all, P partial assignment extended solution, algorithmnever backtrack away P . (Indeed, cannot retain completeness, since P mayvariable assignment extends full solution point.) Givenbacktrack away satisfiable partial assignment, numberbacktracks exponential problem size, clear time spentprogram indeed evaluating unsatisfiable regions search space.2.1 Unit Propagation: Inner Loopdpll algorithm 2.2 implemented run practical problems, bulkrunning time spent unit propagation. example, Figure 1 gives amount timespent zChaff variety microprocessor test verification examples made available Velev (http://www.ece.cmu.edu/mvelev).4 problems become difficult,increasing fraction computational resources devoted unit propagation.reason, much early work improving performance dpll focused improvingspeed unit propagation.4. examples used generate graph solved zChaff within 100 seconds using IntelPentium 4M running 1.6GHz. solved within 100 second limit, average 89.4%time spent unit propagating.200fiBoolean Satisfiability SurveyWithin unit propagation procedure 2.3, bulk time spent identifyingclauses propagate; words, clauses satisfied partial assignment contain one unvalued literal:Observation 2.4 Efficient implementations dpll typically spend bulk effortsearching clauses satisfying conditions required unit propagation.go examine techniques used speed unit propagationpractice, let us remark implementations sat solvers similar properties.Nonsystematic solvers wsat (Selman et al., 1993), example, spend bulktime looking clauses containing satisfied unvalued literals (or, equivalently,maintaining data structures needed make search efficient). generalizeObservation 2.4 get:Observation 2.5 Efficient implementations sat solvers typically spend bulkeffort searching clauses satisfying specific syntactic conditions relative partialcomplete truth assignment.focus survey systematic methods, remarksimilarity techniques used dpll wsat, techniques speed inner loopone likely speed inner loop well.said, let us describe series ideas employed speedingprocess identifying clauses leading unit propagations:1. binding variable v, examine clause determine whether satisfiesconditions Procedure 2.3.2. Slightly sophisticated restrict search suitable clause clausesc C include v one disjuncts (assuming v assignedvalue true). all, v appears c, c satisfied v set true; vmentioned c change cs ability unit propagate vs valueset.3. set v true above, examine clauses containing v, walkclause determine literals satisfied (if any)still unbound. efficient keep record, clause c, numbers(c) satisfied number u(c) unbound literals.order keep counts current set variable v true, needincrement s(c) decrement u(c) clause v appears, simplydecrement u(c) clause v appears. backtrack unset v,need reverse adjustments.Compared previous approach, need examine four times many clauses(those v appears either sign, v set unset),examination takes constant time instead time proportional clause length.average clause length greater four, approach, due CrawfordAuton (1996), effective predecessor.201fiDixon, Ginsberg & Parkes4. Currently, efficient scheme searching unit propagations watchedliterals family implementations (Moskewicz et al., 2001; Zhang & Stickel, 2000).clause c, identify two watched literals l1 (c) l2 (c); basic idealong two literals either unvalued satisfied, clausecannot produce unit propagation.one watched literals set false clause mustexamined detail. another unset (and unwatched) literal, watch it.satisfied literal clause, need nothing. satisfiedliteral, clause ready unit propagate.average clause length l, set variable v (say true),probability approximately 2/l need analyze clause vappears, work involved proportional length clause.expected amount work involved twice number clauses v appears,improvement previous methods. fact, approach somewhatefficient cursory analysis suggests adjustment watchedliterals tends favor watching set deep search tree.move discuss learning Boolean satisfiability, let us remark brieflyso-called pure literal rule. understand rule itself, suppose theorypartial variable assignment P . Suppose also literal q appearsclauses yet satisfied P , negation q appearclauses. q may consequence partial assignment P ,clearly set q true without removing solutions portion searchspace remains.pure literal rule generally included implementations either dpllunit propagation procedure relatively expensive work with. Countsnumber unsatisfied clauses containing variables negations must maintainedtimes, checked see literal become pure. addition, seeSection 2.3 many branching heuristics obviate need pure literal ruleemployed.2.2 Learning RelevanceLet us turn final column table, considering progress madeavoiding rework Boolean satisfiability engines. basic idea avoid situationsconventional implementation thrash, solving subproblem manytimes different contexts.understand source difficulty, consider example involving sat problemC variables v1 , v2 , . . . , v100 , suppose also subset C involvingvariables v50 , . . . , v100 fact implies v50 must true.imagine constructed partial solution values variablesv1 , . . . , v49 , initially set v50 false. amount backtracking,realize v50 must true. Unfortunately, subsequently change valueone vi < 50, forget v50 needs true dangersetting false again, followed repetition search showed v50202fiBoolean Satisfiability Surveyconsequence C. Indeed, danger solving subproblem twice,search node examine space vi < 50.indicated, solution problem cache fact v50 needstrue; information generally saved form new clause called nogood .case, record unary clause v50 . Modifying original problem Cway allow us immediately prune subproblem set v50 false.technique introduced Stallman Sussman (1977) dependency directedbacktracking.Learning new constraints way prevent thrashing. contradictionencountered, set assignments caused contradiction identified;call set conflict set. new constraint constructed excludesassignments conflict set. Adding constraint problem ensurefaulty set assignments avoided future.description learning fairly syntactic; also give semantic description. Suppose partial assignment contains {a, b, d, e} problemcontains clausesb c e(1)c d.(2)first clause unit propagates allow us conclude c; second allows us concludec. contradiction causes us backtrack, learning nogoodb e.(3)semantic point view, derived nogood (3) simply result resolvingreason (1) c reason (2) c.general phenomenon. point, suppose v variableset partial assignment P . vs value result branch choice,associated reason. vs current value result unit propagation, however,associate v reason clause produced propagation. vs valueresult backtrack, value must result contradiction identifiedsubsequent variable v 0 set reason v result resolvingreasons v 0 v 0 . point, variable whose value forcedassociated reason, accumulated reasons avoid need reexamineparticular portion search space.Modifying dpll exploit derived information requires include derived clauses overall problem C, thus enabling new unit propagations restrictingsubsequent search. implicit change well.earlier example, suppose set variables v1 , . . . , v49 order,learned nogoodv7 v9(4)(presumably situation v7 false v9 true). long v7 remains falsev9 remains true, unit propagation fail immediately (4) unsatisfied.203fiDixon, Ginsberg & Parkesallow us backtrack directly v9 example. semantic justificationtechnique known backjumping (Gaschnig, 1979) search jumps backvariable relevant problem hand.5attractive theory, however, technique difficult apply practice.reason new nogood learned every backtrack; since number backtracksproportional running time program, exponential number nogoodslearned. expected overtax memory available systemalgorithm running, increase time node expansion. numberclauses containing particular variable v grows without bound, unit propagationprocedure 2.3 grind halt.Addressing problem primary focus work systematic sat solvers1990s. Since impractical retain nogoods learned,method needed found would allow polynomial number nogoods retainedothers discarded. hope relatively small number nogoods couldstill used prune search space effectively.6Length-bounded Learning first method used bound size set learnedclauses retain clauses whose length less given bound k (Dechter, 1990;Frost & Dechter, 1994). addition providing polynomial bound numbernogoods retained, approach felt likely retain important learned clauses,since clause length l general prune 21l possible assignments valuesvariables. Length-bounded learning draws observation conclusion shortclauses retained preference long ones.Relevance-bounded Learning Unfortunately, length may best indicatorvalue particular learned clause. restrict attention particularsubproblem search, short clauses may applicable all,longer clauses may lead significant pruning. example, consider node definedpartial assignment P = {a, b, c} together two learned clauses:abe(5)b c e(6)long assignments P retained, clause (5) cannot used pruningsatisfied P itself. fact, clause useful backtrackchange values b. clause (6) likely usefulcurrent subproblem, since lead unit propagation either e set false.subproblem node given P large, (6) may used many times. Withincontext subproblem, longer clause actually useful.5. particular example, also possible backtrack v8 well, although reason recorded.branch point v8 removed search, v9 set false unit propagation.advantage flexibility either choice value v8 choicebranch variable itself. idea related Bakers (1994) work difficulties associatedbackjumping schemes, employed zChaff (Moskewicz et al., 2001).6. Indeed, final column table might better named forgetting learning. Learning(everything) easy; forgetting coherent way thats hard.204fiBoolean Satisfiability Surveylevel, appearance a, b, c (6) shouldnt contributeeffective length learned clause literals currently false.Length-bounded learning cannot make distinction.see example useful retain clauses arbitrary length providedrelevant current search context. context subsequently changes,remove clauses make room new ones suitable newcontext. relevance-bounded learning does.relevance-bounded learning, effective length clause defined termscurrent partial assignment. irrelevance clause defined one lessnumber unvalued satisfied literals contains. example, clause (6)partial assignment P irrelevance 1. idea relevance-bounded learningoriginated dynamic backtracking (Ginsberg, 1993), clauses deletedirrelevance exceeded 1. idea generalized Bayardo Miranker (1996),defined general relevance bound deleted clauses whose irrelevance exceededbound. generalization implemented relsat satisfiability engine (Bayardo& Schrag, 1997).Like length-bounded learning, relevance-bounded learning retains clauses lengthless irrelevance bound k, since irrelevance clause never exceedlength. technique relevance-bounded learning also allows retention longerclauses applicable current portion search space. clausesremoved longer relevant.Returning example, backtrack original partial assignment{a, b, c} find exploring {a, b, c}, short nogood (5) 0-irrelevant(since unit propagate conclude e) long one (6) 4-irrelevant. Usingrelevance bound 3 less, nogood would discarded.condition nogoods retained irrelevance exceeds bound ksufficient ensure polynomial number nogoods retained point.7Experimental results (Bayardo & Miranker, 1996) show relevance-bounded learningeffective length-bounded counterpart and, even relevance bound 1,results comparable learning without restriction.8Hybrid Approaches Finally, note solvers employ hybrid approach.chaff algorithm (Moskewicz et al., 2001) uses relevance bound (larger) lengthbound. Clauses must meet relevance bound length bound retained.Yet another approach taken berkmin algorithm (Goldberg & Novikov, 2002).Here, set nogoods partitioned two separate groups based recently15nogoods acquired; 16nogoods kept recent group remaining116 old group. relatively large length bound used cull recently acquirednogoods smaller length bound used aggressively cull smaller group older7. Although accepted wisdom, know proof literature result; Bayardo Mirankers(1996) proof appears assume order branch variables chosen fixed. presentgeneral proof next paper series (Dixon et al., 2003b).8. Results necessarily become somewhat suspect algorithmic methods mature; unfortunate consequence extremely rapid progress satisfiability engines recent years lackcareful experimental work evaluating host new ideas developed.205fiDixon, Ginsberg & Parkesnogoods. aware studies comparing hybrid approaches purelength-bounded relevance-bounded methods.2.3 Branching HeuristicsLet us turn examination progress made developmenteffective heuristics selection branch variables dpll. discussedintroduction, focus specific heuristics work selected domains,general ideas attempt exploit structure axiomatization directly.Prior development successful learning techniques, branching heuristicsprimary method used reduce size search space. seems likely, therefore,role branching heuristics likely change significantly algorithms prunesearch space using learning. heuristics used zChaff appear firststep direction, little known new role.Initially, however, primary criterion selection branch variablepick one would enable cascade unit propagations; result cascadesmaller tractable subproblem.9first rule based idea called moms rule, branches variableMaximum Occurrences clauses Minimum Size (Crawford & Auton, 1996;Dubois, Andre, Boufkhad, & Carlier, 1993; Hooker & Vinay, 1995; Jeroslow & Wang, 1990;Pretolani, 1993). Moms provides rough easily computed approximation numberunit propagations particular variable assignment might cause.Alternatively, one use unit propagation rule (Crawford & Auton, 1996; Freeman,1995), compute exact number propagations would caused branchingchoice. Given branching candidate vi , variable separately fixed true falseunit propagation procedure executed choice. precise number unitpropagations caused used evaluate possible branching choices. Unlike momsheuristic, rule obviously exact attempt judge number unit propagationscaused potential variable assignment. Unfortunately, also considerablyexpensive compute expense unit propagation itself. ledadoption composite approaches (Li & Anbulagan, 1997) moms used identifysmall number branching candidates, evaluated exactly usingexpensive unit propagation heuristic. randomly generated problems, compositetechnique outperforms either heuristic isolation.Another strategy branch variables likely backbone variables (Dubois& Dequen, 2001). backbone literal (also often referred unary prime implicatetheory) one must true solutions given problem. Given problem Cpartial assignment P , backbone heuristic attempts branch variablesbackbones subset clauses C satisfied P ; likelihoodparticular variable backbone literal approximated counting appearances9. idea tends obviate need use pure literal rule, well. p pure literal,particular reason hurry set p true; key thing avoid setting false. p pure,p cannot generate unit propagations, p tend selected branch variable. Pureliterals obviously never set false unit propagation, heuristics based unit propagationcounts tend achieve advantages pure literal rule without incurring associatedcomputational costs.206fiBoolean Satisfiability Surveyliteral satisfied clauses C. heuristic outperforms discussedprevious paragraphs.heuristics described thus far developed communitys research emphasis focused solution randomly generated satisfiability problems.development bounded learning methods enabled solvers address issue thrashing,caused natural shift focus toward structured, realistic problems.formal studies comparing previously discussed heuristics structured problems,value studies exist reduced implementationsdpll isolation without learning techniques since proved important.Branching techniques learning deeply related, addition learning dpllimplementation significant effect effectiveness branchingstrategies. new clauses learned, number unit propagations assignmentcause expected vary; reverse also true choice branch variableaffect clauses algorithm learns. formal comparison branching techniquesperformance structured problems presence learning would extremelyuseful.Branching heuristics designed function well context learning algorithm generally try branch variables things learned recently.tends allow implementation keep making progress single sectionsearch space opposed moving one area another; additional benefitexisting nogoods tend remain relevant, avoiding inefficiencies associated losinginformation present nogoods become irrelevant deleted. zChaff,example, count maintained number times literal occurs theorysolved. new clause added, count associated literalclause incremented. branch heuristic selects variable appears manyclauses possible. periodically dividing counts constant factor, biasintroduced toward branching variables appear recently learned clauses. Likemoms rule, rule inexpensive calculate.heuristic used berkmin (Goldberg & Novikov, 2002) builds idearesponds dynamically recently learned clauses. berkmin heuristic prefersbranch variables unvalued recently learned clause yetsatisfied, zChaff-like heuristic used break ties.told, many competing branching heuristics satisfiability solvers,still much done evaluating relative effectiveness. interestingexperiments done using implementations learn, realistic, structuredproblems opposed randomly generated ones.2.4 Proof Complexityalready commented briefly fact proof systems evaluated basedprovable bounds proofs certain classes formulae, developmentpolynomial transformations proofs one system proofs another.207fiDixon, Ginsberg & Parkesregard first metric, least three classes problems knownexponentially difficult conventional resolution-based provers (including dpllimplementation):101. Pigeonhole problems (Haken, 1985)2. Parity problems (Tseitin, 1970)3. Clique coloring problems (Bonet, Pitassi, & Raz, 1997; Krajicek, 1997; Pudlak, 1997)turning discussion problems specifically, however, let us pointmany proof systems known powerfulones discuss paper. perspective, interesting extendedresolution involves introduction new variables correspond arbitrary logicalexpressions built original variables theory.Since logical expressions always built term-by-term, suffices allowintroduction new variables corresponding pairwise combinations existing ones;since disjunction replaced conjunction using de Morgans laws, sufficesintroduce new variables formw xy(7)literals x y. Writing (7) disjunctive normal form, get:Definition 2.6 (Tseitin, 1970) extended resolution proof theory onefirst augmented collection groups axioms, group formx wx w(8)wx literals (possibly already extended) theory w new variable.Following this, derivation proceeds using conventional resolution augmented theory.proof system known stronger extended resolution; fact,class problems known polynomially sized proofsextended resolution.stressed, however, fact proof system strong meanworks well practice. know implementation extended resolutionsimple reason virtually nothing known select new variablesshorten proof length.Understanding introduction new variables reduce proof lengthconsiderably simpler. example, suppose resolution proof,managed derive nogood ax, also derived ay. order completeproof, need perform lengthy identical analyses nogoods,eventually deriving simply x first second (and resolvingx y, example).10. hard problems well, Hakens (1995) broken mosquito screen problem.three examples quoted sufficient purposes.208fiBoolean Satisfiability Surveycould replace pair nogoods x single nogood wusing (8), two proofs x could collapsed single proof,potentially halving size proof entirety. Introducing still variablesrepeat effect, resulting exponential reductions proof size.Another way look improvement expressivity. simplyway write (a x) (a y) equivalent (x y) single Boolean axiomdisjunctive normal form. power extended resolution rests fact subexpression substitution makes possible capture expressions (x y) singleaxiom.None proof systems considered survey powerful extendedresolution, therefore evaluate based performance threeproblems mentioned beginning section. Let us therefore describeproblems detail.2.4.1 Pigeonhole problemspigeonhole problem involves showing impossible put n + 1 pigeons nholes pigeon must go distinct hole. write pij fact pigeonhole j, straightforward axiomatization says every pigeon must leastone hole:pi1 pi2 pin = 1, . . . , n + 1(9)two pigeons hole:pik pjk 1 < j n + 1 k = 1, . . . , n(10)Note (n3 ) axioms form (10).well known polynomial-sized proof unsatisfiabilityaxioms (9)(10) (Haken, 1985). proof technical, essential reasonpigeonhole problem counting. point, proving cant putn + 1 pigeons n holes requires saying cant put n pigeons last n 1holes, thus n 1 pigeons last n 2 holes, on. Saying languagesat awkward, possible show proof pigeonhole problemcompleted without, point, working extremely long individual clauses.again, see connection expressive efficiency; readers interested additionaldetails, Pitassis (2002) explanation reasonably accessible.2.4.2 Parity problemsparity problem, mean collection axioms specifying parity setsinputs. write, example,x1 xn = 1(11)indicates odd number xi true; right hand side zero would indicateeven number true. indicates exclusive or.209fiDixon, Ginsberg & ParkesReduction (11) collection Boolean axioms best described example.parity constraint x z = 1 equivalentxyzx zx zx zgeneral, number Boolean axioms needed exponential length parityclause (11), clauses fixed length, number axioms obviously fixedwell.proof complexity result interest, suppose G graph, nodeG correspond clause edge literal. label edges distinctliterals, label node graph zero one. n nodegraph labeled value vn edges e1n , . . . , ei(n),n incident n labeledliterals l1n , . . . , li(n),n , add theory Boolean version clausel1n li(n),n = vn(12)Since every edge connects two nodes, every literal theory appears exactly twiceaxioms form (12). Adding constraints therefore produces valuePPequivalent zero mod 2 must equal n vn well. n vn odd, theoryunsatisfiable. Tseitins (1970) principal result show unsatisfiability cannotgeneral proven number resolution steps polynomial size Booleanencoding.2.4.3 Clique coloring problemslast examples consider known clique coloring problems.derivatives pigeonhole problems exact nature pigeonhole problemobscured. Somewhat specifically, problems indicate graph includes cliquen + 1 nodes (where every node clique connected every other),graph must colored n colors. graph known clique, problemequivalent pigeonhole problem. know clique embeddedgraph, problem difficult.axiomatization, use eij describe edges graph, cij describecoloring graph, qij describe embedding cliQue graph.graph nodes, clique size n + 1, n colors available.axiomatization is:eij cil cjl1 < j m, l = 1, . . . , n(13)ci1 cin= 1, . . . ,(14)qi1 qim= 1, . . . , n + 1(15)1 < k n + 1, j = 1, . . . ,(16)1 < j m, 1 k 6= l n + 1(17)qij qkjeij qki qlj210fiBoolean Satisfiability Surveyeij means edge graph nodes j, cij means graphnode colored jth color, qij means ith element cliquemapped graph node j. Thus first axiom (13) says two nodesgraph cannot color (of n colors available) connected edge.(14) says every graph node color. (15) says every element cliqueappears graph, (16) says two elements clique mapnode graph. Finally, (17) says clique indeed clique two cliqueelements map disconnected nodes graph.Since polynomially sized resolution proof pigeonhole problemBoolean satisfiability, obviously polynomially sized proof clique coloring problems, either. shall see, clique coloring problems casesused distinguish among elements proof complexity hierarchy.2.5 Boolean Summarysummarize results section completing first row table follows:SATrep. eff.1p-simulationhierarchyEEEinferenceresolutionunitpropagationwatched literalslearningrelevancecardinalityPBsymmetryQPROPentries really informal shorthand:Representational efficiency: Boolean satisfiability benchmarklanguages measured; give relative savingschanging representation.p-simulation hierarchy: give proof complexity three problem classesdiscussed Section 2.4. Boolean satisfiability, problems require proofsexponential length.Inference: basic inference mechanism used dpll resolution.Propagation: Watched literals lead efficient implementation.Learning: Relevance-based learning appears effective polysized methods.3. Pseudo-Boolean Cardinality Constraintsentries previous table summarize fact Boolean satisfiability weakmethod admits efficient implementations. representation relatively inefficient,none canonical problems solved polynomial time.difficulties, least, overcome via representational shift.211fiDixon, Ginsberg & Parkesunderstand shift, note write axiomx zx+y+z 1(18)thinking x, z variables value either 0 (false) 1 (true)written z 1 z or, equivalently, z. v variable, continue referv negation v.familiar logical operations obvious analogs notation. If,example, want resolveb cbget c d, simply adda+b+c1b+d1simplify using identity b + b = 1 geta+c+d1required.Whats nice notation extends easily general descriptions.Pgeneral form disjunction li literalsli 1 (18), droprequirement right-hand side 1:Definition 3.1 cardinality constraint extended clause constraint formXli kk integer li required value 0 1.cardinality constraint simply says least k literals question true.Proposition 3.2 (Cook, Coullard Turan (1987)) unsatisfiability proofpolynomial length pigeonhole problem using cardinality constraints.Proof. Cook et al. (1987) give derivation o(n3 ) steps; presented o(n2 )derivation elsewhere (Dixon & Ginsberg, 2000).course, fact extension sat language allows us find polynomiallength derivations pigeonhole problem necessarily show changecomputational value; need examine columns table well.remainder section, show go further, describing newcomputational techniques applied broader settingconsidering. Experimental results also presented. let us begin examiningfirst column table:212fiBoolean Satisfiability SurveyProposition 3.3 (Benhamou, Sais, & Siegel, 1994) cardinality constraintx1 + + xm klogically equivalent setk1(19)axiomsxi1 + + ximk+1 1(20)every set k + 1 distinct variables {xi1 , . . . , ximk+1 }. Furthermore,compact Boolean encoding (19).Proof. first show (19) implies (20). see this, suppose setk + 1 variables. Suppose also set xi true, sizeleast k. Since variables, 6= least one xi musttrue.see (20) implies (19), suppose (20) true appropriate sets xi s.(19) false, set false xi would size least k + 1,instance (20) would unsatisfied.see efficient encoding, first note (19) implies Booleanaxiomx1 xk xk+1 xmmust also implyx1 xksince always change xi false true without reducing satisfiability(19).Next, note axiom length less k + 1 consequence (19), sinceaxiom falsified satisfying (19) setting every unmentioned variabletrue rest false.Finally, suppose leave single instance (20) include otherswell every clause length greater k + 1. setting variables falseevery variable true, given clauses satisfied (19)be. follows Boolean equivalent (19) must include least k1instances(20).follows Proposition 3.3 provided new variables introduced, cardinality constraints exponentially efficient Boolean counterparts.discussing columns table, let us consider extendingrepresentation include known pseudo-Boolean constraints:Definition 3.4 pseudo-Boolean constraint axiom formXwi li k(21)wi k positive integer li required value 0 1.213fiDixon, Ginsberg & ParkesPseudo-Boolean representations typically allow linear inequalities linear equalities Boolean variables. Linear equalities easily translated pair inequalities form definition; prefer inequality-based description (Barth, 1996;Chandru & Hooker, 1999, also known pseudo-Boolean normal form) betteranalogy Boolean satisfiability unit propagation becomes unmanageableequality constraints considered. Indeed, simply determining equality clausesatisfiable subsumes subset-sum therefore (weakly) NP-complete (Garey & Johnson,1979).Compare (21) Definition 3.1; wi weights attached various literals.pseudo-Boolean language somewhat flexible still, allowing us say (for example)2a + b + c 2indicating either true b c (equivalent crucial representationalefficiency obtained extended resolution). see shortly, natural makeextension result resolving two cardinality constraintsnaturally written form.3.1 Unit PropagationLet us begin discussing propagation techniques cardinality pseudo-Boolean setting.11pseudo-Boolean version unit propagation first presented Barth (1996)described number papers (Aloul, Ramani, Markov, & Sakallah, 2002; Dixon &Ginsberg, 2000). Boolean case, describe clause unit containssatisfied literals one unvalued one. generalize pseudo-Booleansetting, make following definition, view partial assignment P simplyset literals values true:Definition 3.5 Let wi li k pseudo-Boolean clause, denote c.suppose P partial assignment values variables. saycurrent value c P givenPXcurr(c, P ) =wi k{i|li P }ambiguity possible, write simply curr(c) instead curr(c, P ).words, curr(c) sum weights literals already satisfied P , reducedrequired total weight k.similar way, say possible value c P givenXposs(c, P ) =wi k{i|li 6P }11. remarked, table designed reflect issues involved lifting dpllexpressive representation. Extending nonsystematic search technique wsat pseudoBoolean setting discussed Walser (1997) Prestwich (2002).214fiBoolean Satisfiability Surveyambiguity possible, write simply poss(c) instead poss(c, P ).words, poss(c) sum weights literals either already satisfiedvalued P , reduced required total weight k.12Definition 3.6 Let c clause, P partial assignment. say c unitvariable v appearing P either P {v} P {v} cannotextended assignment satisfies c.situation, variable v forced take value help satisfy clause.creates new consequential assignment. Note c already unsatisfiable,meet conditions definition choosing v variable assigned valueP . Note also pseudo-Boolean case, clause may actually containone variable forced specific value. clear Boolean case,definition duplicates conditions original unit propagation procedure 2.3.Lemma 3.7 partial assignment P extended way satisfies clause cposs(c, P ) 0.Proof. Assume first poss(c, P ) 0, suppose value every remainingvariable way helps satisfy c. done so, every literal ccurrently made false P true, resulting value cXwi li =Xwi = poss(c, P ) + k k{i|li 6P }c becomes satisfied.Conversely, suppose poss(c, P ) < 0. best still valueunvalued literals favorably, value c becomesXwi li =Xwi = poss(c, P ) + k < k{i|li 6P }c unsatisfiable.Proposition 3.8 clause c containing least one unvalued literal unit ccontains unvalued literal li weight wi > poss(c).Proof. literal weight wi > poss(c), setting literal false reduceposs(c) wi , making negative thus making c unsatisfiable. Conversely,literal, poss(c) remain positive single unvalued literal set,c remains satisfiable therefore unit.Given result, little impact time needed find unit clauses.need simply keep literals clause sorted weight maintain,clause, value poss weight largest unvalued literal. value literaldifferent weight, apply test Proposition 3.8 directly. value literalgiven weight, short walk along clause allow us identify new unvaluedliteral maximum weight, proposition continues apply.12. Chai Kuehlmann (2003) refer poss slack.215fiDixon, Ginsberg & ParkesWatched literals Generalizing idea watched literals difficult. makefollowing definition:Definition 3.9 Let c clause. watching set c set variablesproperty c cannot unit long variables either unvaluedsatisfied.Proposition 3.10 Given clause c formwatching set cXPwi lik, let set variables.wi max wi k(22)sum maximum taken literals involving variables S.Proof. Suppose variables unvalued satisfied. let ljPunvalued literal c. lj 6 S, poss(c) wj + wi k thus poss(c) wj sincePPwiwi maxi wi k. If, hand, lj S,poss(c)Xwi kXwi wjXwi max wi kCombining these, getposs(c) wjEither way, cannot poss(c) < wj Proposition 3.8 therefore implies ccannot unit. follows watching set.Pconverse simpler. wi maxi wi < k, value every literal outsidePmake c false. poss(c) = wi k, lj literal greatest weight,associated weight wj satisfies wj > poss(c) c unit. Thus cannot watchingset.generalizes definition Boolean case, fact made even obviousby:Corollary 3.11 Given cardinality constraint c requiring least k associated literalstrue, watching set c includes least k + 1 literals c.Proof. expression (22) becomesX1 max 1 k|S| 1 k.216fiBoolean Satisfiability Survey3.2 Inference Resolutionunit propagation, resolution also lifts fairly easily pseudo-Boolean setting.general computation follows:Proposition 3.12 Suppose two clauses c c0 , c givenXwi li + wl k(23)wi0 li0 + w0 l k 0(24)c0 givenXlegitimate concludeXw0 wi li +Xwwi0 li0 w0 k + wk 0 ww0(25)Proof. immediate. Multiply (23) w0 , multiply (24) w, add simplify usingl + l = 1.weights, k k 0 1, generalizes conventional resolution providedsets nonresolving literals c c0 disjoint. deal caseoverlap set li set li0 , need:Lemma 3.13 Suppose c clause wi li k. c equivalentwi0 given by:wi , wi < k;wi0 (j) =k, otherwise.P0wi liPk,Proof. lj literal wj k, c rewrite true lj satisfied.lj = 0, c rewrite equivalent.words, reduce coefficient greater requiredsatisfy clause entirety, example rewriting3x + + z 22x + + z 2either equivalent x (y z).Proposition 3.14 (Cook et al., 1987; Hooker, 1988) construction Proposition 3.12 generalizes conventional resolution.217fiDixon, Ginsberg & ParkesProof. already discussed case sets li li0 disjoint.literal li c negation literal c0 , li + li (25),simplify 1 make resolved constraint trivial; resolution producesresult. literal li c also appears c0 , coefficient literalresolvent (25) 2 reduced 1 virtue lemma.Cardinality constraints bit interesting. Suppose resolvingtwo clauses+ b + c2+c + 1add get2a + b + 2(26)words, either true b are. problemcardinality constraint, cannot rewritten one.One possibility rewrite (26) pair cardinality constraintsa+b 1(27)a+d 1(28)If, however, want result resolving pair constraints single axiom,must either select one axioms extend language further.3.3 Learning Relevance Boundsidea relevance also natural generalization pseudo-Boolean setting. Recallbasic definition Section 2.2:Definition 3.15 Let c clause P partial assignment. c i-irrelevantnumber literals c either unvalued true P least + 1.Proposition 3.16 Given partial assignment P Boolean clause c, c i-irrelevantposs(c, P ) i.Proof. Boolean case, number literals c either unvalued trueposs(c, P ) + 1 since right hand side constraint always 1. irrelevanceconditionposs(c, P ) + 1 + 1result follows.pseudo-Boolean case, additional learning techniques also possible.present ideas detail, however, let us point sort inferentialextension needed overcome shortcomings dpll revealedpigeonhole problems. all, recall Proposition 3.14: pseudo-Boolean inferencegeneralizes Boolean resolution. begin Boolean axiomatization (aspigeonhole problem), derivation using techniques reproducible usingconventional resolution-based methods, therefore exponential length. (Amajority inference steps various proofs Proposition 3.2 resolutionsteps literal cancellations occur.)218fiBoolean Satisfiability SurveyStrengthening specific method discuss operations researchused preprocess mixed integer programming problems (Guignard & Spielberg, 1981;Savelsbergh, 1994).Suppose setting l0 true applying form propagationPconstraint set, discover assumption constraint c givenwi li rbecomes oversatisfied amount sum left hand side greater(by s) amount required right hand side inequality; termsDefinition 3.5, curr(c) = s. oversatisfied constraint c replacedfollowing:Xwi li r +(29)sl0 +l0 true, know wi li r + s, (29) holds. l0 false, sl0 =Pstill must satisfy original constraint wi li r, (29) still holds. new constraintimplies original one, information lost replacement.remarked, community uses technique preprocessing.literal fixed, propagation applied, oversatisfied constraint strengthened.Consider following set clauses:Pa+b1a+c1b+c1set false, must value b c true order satisfy first twoconstraints. third constraint oversatisfied thus replaceda+b+c2power method allows us build complex axioms setsimple ones. strengthened constraint often subsume constraints involved generating it. case, new constraint subsumes threegenerating constraints.Proposition 3.17 Let c constraint P partial assignment. conclude curr(c) solution overall problem extends P , replacecXXli +wi li r +(30)Pfirst summation literals li P .Proof. truth assignment extends P , (30) follows fact curr(c)s. truth assignment P 0 extend P , lj P falseP 0 ,XliPCombining original constraint c produces (30).219fiDixon, Ginsberg & ParkesInstance2pipe2pipe-1-ooo2pipe-2-ooo3pipe3pipe-1-ooo3pipe-2-ooo3pipe-3-ooo4pipezChaffsec nodes08994110725066907484336335709412511146504244 411107pbchaffsec nodes089481953406706125721893658916450031957370263 382750Table 1: Run time (seconds) nodes expandedLearning inference present experimental results relatedeffectiveness pseudo-Boolean inference, point one additional problemarise setting. possible branch variable v, result resolvingreasons v v new nogood falsified partial assignmentv search space.example (Dixon & Ginsberg, 2002), suppose partial assignment{a, b, c, d} constraints2e + + c 2(31)2e + b + 2(32)unit propagate conclude e virtue (31) e virtue (32); isnthard conclude conflict set b either b must true (31)(32) simultaneously satisfiable. simply add (31) (32) simplify,geta+b+c+d2still allows b false. difficulty addressed derivingcardinality constraint guaranteed falsified current partial solutioninvestigated (Dixon & Ginsberg, 2002); Chai Kuehlmann (2003) developed stillstronger method.Experimental results Many ideas described implementedpbchaff satisfiability solver. earlier paper (Dixon & Ginsberg, 2002), compared results obtained using prs, pseudo-Boolean version relsat, obtainedusing relsat (Bayardo & Miranker, 1996). Pbchaff updated version prsmodeled closely zChaff (Moskewicz et al., 2001). implements watched literalscardinality constraints applies strengthening idea. compare pbchaffsperformance Boolean counterpart zChaff.Results (unsatisfiable) problem instances Velev suite discussedbeginning Section 2.1 shown Table 1. seen, performance comparable; pbchaff pays small (although noticeable) cost extended expressivity.220fiBoolean Satisfiability SurveyInstancehole8.cnfhole9.cnfhole10.cnfhole11.cnfhole12.cnfhole20.cnfhole30.cnfhole40.cnfhole50.cnfzChaffsecnodes03544181441727399339 126962Preprocesssec00000042595pbchaffsec nodes011012017015020034052075095Table 2: Run time (seconds) nodes expandedexperiments run 1.5 GHz AMD Athlon processor, solvers usedvalues various tuning parameters available (relevance length bounds, etc.).Results pigeonhole problem appear Table 2. case, pbchaffpermitted preprocess problem using strengthening described earlier section.ZChaff unable solve problem twelve pigeons 1000-secondtimeout using 1.5 GHz Athlon processor. surprisingly, pbchaff preprocessingdramatically outperformed zChaff instances.133.4 Proof Complexityalready shown Proposition 3.2 pseudo-Boolean cardinality-based axiomatizations produce polynomially sized proofs pigeonhole problem. alsoknown methods lead polynomially sized proofs clique coloringproblem (Bonet et al., 1997; Krajicek, 1997; Pudlak, 1997). situation regardparity constraints bit interesting.Let us first point possible capture parity constraints, modularityconstraints generally pseudo-Boolean setting:Definition 3.18 modularity constraint constraint c formXwi li n(mod m)(33)positive integers wi , n m.remainder section, show modularity constraints easilyencoded using pseudo-Boolean axioms, also constraint sets consisting entirelymod 2 constraints easily solved either directly using encoding, althoughclear recover pseudo-Boolean encodings Boolean versions.13. Without preprocessing, two systems perform comparably class problems.stressed, representational extensions little use without matching modifications inference methods.221fiDixon, Ginsberg & ParkesModularity constraints pseudo-Boolean encodings encode modularityconstraint way, first note easily capture equality axiomformXwi li = k(34)pseudo-Boolean setting, simply rewriting (34) pair constraintsXwi li kXwi liXwi kfollows, therefore feel free write axioms form (34).denote bxc floor x, say smallest integer greaterx, have:Proposition 3.19 Suppose modularity constraint form (33). setPww = wi introduce new variables si = 1, . . . , bc. (33) equivalentXwi li +Xjwkmsi =+n(35)Proof. Reducing sides (35) mod shows (35) clearly implies (33).Pconverse, note (33) satisfied, integer wi li = sm + n.PPwFurther, since wi li wi = w, follows sm + n w, wnwthus bc. therefore satisfy (35) valuing exactly many sitrue.Understand introduction new variables part intendedinference procedure; simply fashion modularity constraintscaptured within pseudo-Boolean setting.case constraints parity constraints, have:Proposition 3.20 set mod 2 constraints solved polynomial time.Proof. individual constraint (recall corresponds exclusive or, additionmod 2)l li = nviewed simply definingl = n lidefinition inserted remove l remaining constraints. Continuingway, either define variables (and return solution) derive1 = 0 return failure.result, thought little application Gaussianelimination, also instance far general result Schaefers (1978).Proposition 3.21 set mod 2 constraints solved polynomial time usingpseudo-Boolean axiomatization given (35).222fiBoolean Satisfiability SurveyProof. technique unchanged. combinel+Xli + 2Xsi = nl+li0 + 2XXs0i = n0getXli +Xli0 + 2(Xsi +Xs0i + l) = n + n0treat l one auxiliary variables. Eventually, get2Xsi = nlarge (but polynomially sized) set auxiliary variables n eithereven odd. n even, value variables return solution; n oddk auxiliary variables,Xsi =Xsin2n+12(36)since si integral. also2Xsi 2k nXsi kn12(37)Adding (36) (37) produces k k + 1, contradiction.Let us point out, however, mod 2 constraint encoded normal Booleanway, x z = 1 becomesxyz(38)x zx zx z(39)obvious pseudo-Boolean analog reconstructed. problemmentioned beginning section: enough simply extendrepresentation; need extend inference methods well. fact, even questionwhether families mod 2 constraints solved polynomial time pseudo-Booleanmethods without introduction auxiliary variables (35) open. authorsalso considered problem reasoning constraints directly (Li, 2000).223fiDixon, Ginsberg & ParkesPseudo-Boolean constraints extended resolution Finally, let us clarify pointmade earlier. Given encoding a(bc) single pseudo-Booleanclause, pseudo-Boolean inference properly extended resolutionp-simulation hierarchy?answer follows. fact (b c) logically equivalent2a + b + c 2 allows us remove one variables introduced extended resolution,cannot combine encoding others remove subsequent variables. specificexample, suppose learn(b c)(b c)wish conclude(a d) (b c)(40)single pseudo-Boolean axiom equivalent (40).3.5 SummarySATcardinalityPBsymmetryrep. eff.1expexpp-simulationhierarchyEEEP?EP?Einferenceresolutionuniqueuniquely definedunitpropagationwatched literalswatched literalswatched literalslearningrelevancerelevance+ strengtheningQPROPbefore, notes order:cardinality pseudo-Boolean representations exponentiallyefficient Boolean counterpart, clear often compressionsmagnitude occur practice.entries p-simulation column indicate pigeonhole problem easy,clique coloring remains hard, complexity parity problems unknownnew variables introduced.cardinality entry inference intended reflect fact naturalresolvent two cardinality constraints need one.Pseudo-Boolean systems use existing learning techniques, augmentedstrengthening idea.224fiBoolean Satisfiability Survey4. SymmetryGiven pigeonhole problem clique-coloring problems involve great dealsymmetry arguments, variety authors suggested extending Boolean representation inference way allows symmetry exploited directly.discuss variety approaches proposed separating basedwhether modification basic resolution inference rule suggested.event, make following definition:Definition 4.1 Let collection axioms. symmetry meanpermutation variables leaves unchanged.example, consists single axiom x y, clearly symmetricexchange x y. contains two axiomsaxaysymmetric exchange x y.Exploiting symmetry without changing inference One way exploit symmetrymodify set axioms way captures power symmetry.pigeonhole problem, example, argue since symmetryexchange pigeons holes, assume without loss generality pigeon 1hole 1, virtue residual symmetry pigeon 2 hole 2, on.basic idea add so-called symmetry-breaking axioms original theory,axioms break existing symmetry without affecting overall satisfiabilitytheory itself. idea introduced Crawford et al. (1996).attractive theory, least two fundamental difficultiessymmetry-breaking approach:1. Luks Roy (2002) shown breaking symmetries particular problem may require introduction set symmetry-breaking axiomsexponential size. problem sidestepped breakingsymmetries, although little known set broken symmetriesselected.2. Far serious, technique applied symmetry questionglobal. basic argument satisfiability unaffectedintroduction new axioms requires additional axioms consider.theoretical problems, global symmetries exist. practice, even additionasymmetric axioms constrain problem (e.g., cant put pigeon 4hole 7) break required global symmetry render method inapplicable.problematic still possibility symmetries obscured replacingsingle axiomp11 p21(41)225fiDixon, Ginsberg & Parkesequivalent pairp11 p21p11 p21(41) obviously recovered using resolution. again, symmetryoriginal problem vanished method cannot applied.arguments could perhaps anticipated consideration usualtable; since inference mechanism modified (and possible break globalsymmetries), none entries changed. Let us turn, then, techniquesmodify inference itself.Exploiting symmetry changing inference Rather modifying set clausesproblem, also possible modify notion inference, particularnogood derived, symmetric equivalents derived single step. basicidea due Krishnamurthy (1985) follows:Lemma 4.2 Suppose |= q theory nogood q. symmetry, |= (q).hard see technique allows pigeonhole problem solvedpolynomial time, since symmetric versions specific conclusions (e.g., pigeon 1 hole1) derived without repeating analysis led original. dependenceglobal symmetries remains, addressed following modification:Proposition 4.3 Let theory, suppose 0 |= q 0 nogoodq. symmetry 0 , |= (q).Instead needing find symmetry theory entirety, suffices findlocal symmetry subset actually used proof q.idea, generalized somewhat Szeider (2003), allows us avoidfact introduction additional axioms break global symmetry. problemsymmetries obscured (41) remains, however, accompaniednew one, need identify local symmetries inference step (Brown et al., 1988).straightforward identify support new nogood q termssubtheory 0 original theory , finding symmetries particular theoryequivalent graph isomorphism problem (Crawford, 1992). precise complexitygraph isomorphism unknown, felt likely properly P N P(Babai, 1995). basic table becomes:SATcardinalityPBsymmetryrep. eff.1expexp1p-simulationhierarchyEEEP?EP?EEEEinferenceresolutionuniqueuniquePQPROP226unitpropagationwatched literalswatched literalswatched literalssatlearningrelevancerelevance+ strengtheningsatfiBoolean Satisfiability Surveyclear representational efficiency system described, sincesingle concluded nogood serve standin image symmetriesproof produced it.specific instances pigeonhole problem clique coloring problemsaddressed using symmetries, even trivial modifications problems rendertechniques inapplicable. Hence appearance asterisk table: Textbook problem instances may admit polynomially sized proofs,instances require proofs exponential length. Parity problems seemamenable techniques all.remarked, inference using Krishnamurthys related ideas appearsrequire multiple solutions graph isomorphism problem, therefore unlikelyremain P .know implemented system based ideas discussed section.5. Quantification QPROPconclude survey examination ideas used tryingextend Boolean work cope theories naturally thought usingquantification sort. Indeed, Boolean satisfiability engines applied everlarger problems, many theories question produced large part constructingset ground instances quantified axiomsxyz.[a(x, y) b(y, z) c(x, z)](42)size domain x, z taken, single axiom d3ground instances. Researchers dealt difficulty buying machinesmemory finding clever axiomatizations ground theories remain manageablysized (Kautz & Selman, 1998). general, however, memory cleverness scarceresources natural solution needs found.call clause (42) quantified , assume throughout sectionquantification universal opposed existential, domains quantificationfinite.14remarked beginning section, quantified clauses commonencodings realistic problems, problems general solved convertingquantified clauses standard propositional formulae. quantifiers expanded first(possible domains quantification finite), resulting set predicateslinearized relabeling atoms that, example, a(2, 3) might becomev24 . number ground clauses produced exponential number variablesquantified clause.14. appears effective alternative treat existentially quantified clauses simple disjunctions, (9).227fiDixon, Ginsberg & Parkes5.1 Unit Propagationprimary goal work quantified formulation directly, opposedmuch larger ground translation. Unfortunately, significant constant-factor costsincurred so, since inference step need deal issues involvingbindings variables question. Simply finding value assigned a(2, 3) mightwell take several times longer finding value assigned equivalent v24 . Findingoccurrences given literal achieved ground case simple indexingschemes, whereas quantified case likely require unification step.unification performed time linear length terms unified,obviously efficient simple equality check. routine essential operationsexpected significantly slow cost every inference undertaken system.fundamental point costs associated using quantifiedaxioms, significant savings well. savings consequence factbasic unit propagation procedure uses amount time scales roughly linearlysize theory; use quantified axioms reduce size theorysubstantially constant-factor costs overcome.make argument two phases. Section 5.1.1, generalize specificcomputational subtask shared unit propagation satisfiability procedureswsat. show generalization NP-complete formal sense,call subsearch reason. specific procedures unit propagationneeded wsat encounter NP-complete subproblem inference step,show subsearch generally problem randomly generated theories,subsearch cost expected dominate running time realistic instances.Section 5.1.2, discuss consequences fact subsearch NP-complete.Search techniques used speed solution NP-complete problems, subsearchexception. show quantified axiomatizations support application simplesearch techniques subsearch problem, argue realistic examples likelylead subsearch problems polynomial difficulty although existing unit propagationimplementations solve exponentially.5.1.1 Subsearchiteration dpll (or wsat) involves search original theory clausessatisfy numeric property. specific examples already seenfollowing:1. Procedure 2.2 (dpll) (and similarly wsat), need determine Psolution problem hand. involves searching unsatisfied clause.2. Procedure 2.3 (unit propagation), need find unsatisfied clauses containone unvalued literal.addition, wsat needs compute number clauses become unsatisfiedparticular variable flipped.tasks rewritten using following:228fiBoolean Satisfiability SurveyDefinition 5.1 Suppose C set quantified clauses, P partial assignmentvalues atoms clauses. denote Sus (C, P ) set ground instancesC u literals unvalued P literals satisfied assignments P .15say checking problem determining whether Sus (C, P ) 6= .subsearch problem, mean instance checking problem, problemeither enumerating Sus (C, P ) determining size.Proposition 5.2 fixed u s, checking problem NP-complete.Proof. Checking NP, since witness Sus (C, P ) 6= need simply give suitablebindings variables clause C.see NP-hardness, assume u = = 0; cases significantly different.reduce binary constraint satisfaction problem (csp), producing single clauseC set bindings P S00 (C, P ) 6= original binary cspsatisfiable. basic idea variable constraint problem becomequantified variable C.Suppose binary csp variables v1 , . . . , vn binaryconstraints form (vi1 , vi2 ) ci , (vi1 , vi2 ) pair variables constrainedci . constraint, introduce corresponding binary relation ri (vi1 , vi2 ),take C single quantified clause v1 , . . . , vn . ri (vi1 , vi2 ). assignment P ,set ri (vi1 , vi2 ) false (vi1 , vi2 ) ci , true otherwise.note since P values every instance every ri , S00 (C, P ) nonemptyset values vi every literal ri (vi1 , vi2 ) false. Sinceliteral ri (vi1 , vi2 ) false case original constraint ci satisfied, followsS00 (C, P ) 6= original csp satisfiable.moving on, let us place result context. First, important, notefact checking problem NP-complete imply qpropunwieldy representation; subsearch problem indeed appear exponentialsize qprop axioms, exponentially fewer groundcase. So, similar results elsewhere (Galperin & Wigderson, 1983; Papadimitriou,1994), net effect complexity.Second, result embodied Proposition 5.2 appears general phenomenonpropagation difficult compact representations. earlier discussioncardinality pseudo-Boolean axioms, complexity unit propagationunchanged Boolean case, appears much exceptionrule. already remarked, extend pseudo-Boolean representationslightly, addition axioms formXwi li k(43)Definition 3.4 allow axiomsXwi li = k15. interpreting expression Sus (C, P ), set C clauses partial assignment P generallyclear context. superscript refers number satisfied literals satisfied literalssuper good subscript refers unvalued literals unvalued literals arentgood.229fiDixon, Ginsberg & Parkes(replacing inequality (43) equality), determining whether single axiomsatisfiable becomes weakly NP-complete. Symmetry, example examined,involves effective change representational power single axiom.recasting unit propagation terms Definition 5.1:Procedure 5.3 (Unit propagation) compute Unit-Propagate(P ):12345S00 (C, P ) = S10 (C, P ) 6=select c S10 (C, P )v variable c unassigned PP P {v = V : V selected c satisfied}return Pimportant recognize recasting changing procedure significant way; simply making explicit subsearch tasks previously describedimplicitly. procedure unchanged, procedural details variable value choice heuristics irrelevant general point unit propagationdepends solving subsearch instance every step. Wsat similar.5.1.2 Subsearch quantificationdiscussed Section 2.1, efficient implementations sat solvers go great lengthsminimize amount time spent solving subsearch problems. watched literalidea efficient mechanism known here, discuss problem termssimpler scheme maintains updates poss curr counts clause.discussed earlier, scheme half fast watched literal approach,general arguments make expected lead constant-factorimprovements.16notational convenience follows, suppose C quantified theoryl ground literal. Cl mean subset clauses C includeterms l instance. C contains quantified clauses, Cl well;clauses Cl found matching literal l clauses C.discussed Section 2.1, possible compute Sus (C, P ) initialization phase, update incrementally. terms Definition 5.1, update rulemight oneS00 (C, P 0 ) = S00 (C, P ) S10 (Cl , P )literal l changed unvalued true. P 0 partial assignmentupdate; P assignment before. compute number fully assigned unsatisfiedclauses update, start number before, add newly unsatisfied clauses(unsatisfied clauses previously containing single unvalued literal l).argued previously, reorganizing computation way leads substantialspeedups subsearch problem solved longer NP-complete size16. know effective way lift watched literal idea qprop setting. seediscuss zap implementation (Dixon et al., 2003a), still broader generalization allows watchedliterals return elegant far general way.230fiBoolean Satisfiability SurveyC, size Cl Cl . incremental techniques essential performance modern search implementations runtime implementationsdominated time spent propagation (i.e., subsearch).Given subsearch computation time potentially exponential sizesubtheory Cl literal l valued unvalued, let us consider questionsmuch concern practice, (if anything) done it.all, one primary lessons recent satisfiability research problemsNP-hard theory tend strongly exponentially difficult practice.Let us begin noting subsearch likely much issuerandomly generated satisfiability problems focus research 1990sdrove development algorithms wsat. reason nnumber clauses theory C v number variables C, random problemsdifficult fairly narrow ranges values ratio n/v (Coarfa, Demopoulos,San Miguel Aguirre, Subramanian, & Vardi, 2000). 3-SAT (where every clause Ccontains exactly three literals), difficult random problems appear n/v 4.2 (Kirkpatrick& Selman, 1994). problem, number clauses particular literall appears small (on average 3 4.2/2 = 6.3 random 3-SAT). Thus sizerelevant subtheory Cl Cl also small, subsearch cost still tendsdominate running time algorithms question, little gainedapplying sophisticated techniques reduce time needed examine relative handfulclauses.realistic problems, situation dramatically different. axiomlogistics domain encoded satplan style (Kautz & Selman, 1992):at(o, l, t) duration(l, l0 , dt)between(t, t0 , + dt) at(o, l0 , t0 )(44)axiom says object location l time takes time dt flyl l0 , t0 + dt, cannot l0 t0 .given ground atom form at(o, l, t) appear |t|2 |l| clauses form,|t| number time points increments |l| number locations. Even100 each, 106 axioms created seem likely make computing Sus (Cl , P )impractical.Let us examine computation bit detail. Suppose indeedvariable = at(O, L, ) fixed O, L , interested countingnumber unit propagations possible set true. words,want know many instances (44) unsatisfied single unvaluedliteral so.Existing implementations, faced problem (or analogous one wsatanother approach used), consider axioms form (44) o, l boundl0 , t0 dt allowed vary. examine every axiom form simplycount number possible unit propagations.watched literal idea isolation cannot help problem. If, example,watch duration predicates (44), reduce half probability231fiDixon, Ginsberg & Parkesneed solve subsearch problem particular variable valued,cases problem encountered, fierce ever.existing approach solving subsearch problems taken existing systemsuse quantified clauses (44), set ground instances clauses.Computing Sus (C, P ) ground C involves simply checking axiom individually; indeed,axiom replaced set ground instances, approach seemspossible.Set context quantified axiom, however, seems inappropriate. Computing Sus (C, P ) quantified C reducing C set ground clausesexamining equivalent solving original NP-complete problem generatetest one thing state confidence NP-complete problems, generate test general effective way solve them.Returning example at(O, L, ) true, looking variable bindings l0 , dt t0 that, amongst duration(L, l0 , dt), between(T, t0 , + dt)at(O, l0 , t0 ), precisely two literals false third unvalued. Proposition 5.2 suggests subsearch exponentially hard (with respect numberquantifiers) worst case, likely like practice?practice, things going much better. Suppose possible destination l0 , know duration(L, l0 , dt) false dt except specific value D.immediately ignore bindings dt except dt = D, reducing sizesubsearch space factor |t|. depended previous choices search (aircraftloads, etc.), would impossible perform analysis advance thereby removeunnecessary bindings ground theory.Pushing example somewhat further, suppose small +time point immediately . words, between(T, t0 , + D) alwaysfalse, between(T, t0 , + D) always true unit propagationpossible value t0 all. backtrack away unfortunate choicedestination l0 (sub)search variable bindings unit propagation possible.backtracking supported generate-and-test subsearch philosophy usedexisting implementations.sort computational savings likely possible general. naturallyoccurring theories, variables involved likely either unvalued (becauseyet managed determine truth values) false (by virtue closedworld assumption, Reiter, 1978, nothing else). Domain constraints typicallyform a1 ak l, premises ai variables conclusion l literalunknown sign. Unit propagation (or likely instances subsearch problem)thus involve finding situation one ai unvalued, resttrue. use efficient data structures identify instances relational expressionstrue, unreasonable expect instances subsearch problemsoluble time polynomial length clauses involved, opposedexponential length.232fiBoolean Satisfiability Survey5.2 Inference LearningSection 3, working modified representation allows certain inference techniquesapplicable Boolean case.example, suppose resolvinga(A, B) b(B, C) c(C)c(C) d(C, D)concludea(A, B) b(B, C) d(C, D)(45)capital letters indicate ground elements domain resolventsactually ground instancesa(x, y) b(y, C) c(C)(46)c(z) d(z, w)(47)obviously possible resolve (46) (47) directly obtaina(x, y) b(y, C) d(C, w)(48)general (45). procedure learns new nogoods usesprune resulting search, impact learning general (48) substantialeasily outweigh cost unification step required conclude c(C)c(z) resolve z = C. also discussed elsewhere (Parkes, 1999).two new difficulties arise implement ideas. firstconsequence fact resolution ambiguously defined two quantifiedclauses. Consider resolvinga(A, x) a(y, B)(49)a(A, B) b(A, B)(50)unify first term (50) first term (49), obtain a(y, B) b(A, B)resolvent; unify second term (49), obtain a(A, x) b(A, B).practice, however, need problem:Proposition 5.4 Let c1 c2 two lifted clauses, g1 g2 ground instancesresolve produce g. unique natural resolvent c1 c2 gground instance.Proof. one pair resolving literals g1 g2 resultresolution vacuous, assume single literal l g1 lg2 . l ith literal g1 l jth literal g2 , follows resolveoriginal c1 c2 unifying ith literal c1 jth literal c2 . clearresolution generalization g.233fiDixon, Ginsberg & Parkessuggests reasons associated literal valueslifted nogoods retained clauses, ground instances thereof initially used prune search space subsequently used break ambiguitieslearning.second difficulty far substantial. Suppose axioma(x, y) a(y, z) a(x, z)or, familiar form, usual transitivity axioma(x, y) a(y, z) a(x, z)might used reasoning logistics problem, example, gave conditionstwo cities connected roads.suppose trying prove a(A, B) B far apartgiven skeleton relation already know. possible use resolutionderivea(A, x) a(x, B) a(A, B)search proof involving single intermediate location,a(A, x) a(x, y) a(y, B) a(A, B)search proof involving two locations, on, eventually derivingwonderfully concisea(A, x1 ) a(xn , B) a(A, B)(51)suitably large n.problem size domain, (51) dn ground instancesdanger overwhelming unit propagation algorithm even presencereasonably sophisticated subsearch techniques. technique needs adoptedensure difficulty sidestepped practice. One way learnfully general (51), partially bound instance fewer ground instances.Procedure 5.5 construct learn(c, g), nogood learned clause cproduced response backtrack, g ground reason associated c:1234c ground instance i-irrelevantv variable cbind v value greturn cmay still learn nogood exponential number ground instances,least reason believe instances useful pruningsubsequent search. Note subsearch component Procedure 5.5, sinceneed find ground instances c irrelevant. cost incurredclause learned, however, every unit propagation use.234fiBoolean Satisfiability Surveymight seem natural learn general (51), modify subsearch algorithm used unit propagation subset candidate clauses considered.above, natural approach would likely restrict subsearch clausesparticular irrelevance better. Unfortunately, wont help, since irrelevant clausescannot unit. Restricting subsearch relevant clauses useful practicerequiring search algorithm expand successful nodes.moving on, let us note similar phenomenon occurs pseudo-Booleancase. Suppose partial assignment {b, c, d, e} constraintsa+d+e 1(52)a+b+c 2(53)Unit propagation causes variable simultaneously true (by virtue (52))false (because (53)). Resolving reasons together Proposition 3.12 givesus(54)b+c+d+e2conflict set easily seen {b, d, e}, indeed prohibitedderived constraint (54). (54) eliminates additional bad assignments well,{c, d, e}. lifted case, learned something portionsearch space yet examined.5.3 SummarySATcardinalityPBsymmetryQPROPrep. eff.1expexp1expp-simulationhierarchyEEEP?EP?EEEE???inferenceresolutionuniqueuniquePP using reasonsunitpropagationwatched literalswatched literalswatched literalssatexp improvementlearningrelevancerelevance+ strengtheningsat+ first-orderusual, points made.important difference practice exponential savings representation provided qprop savings provided pseudo-Boolean cardinality encodings. exponential savings previous cases mathematicalpossibilities uncertain use practice, savings provided qpropexpected achieved axiomatization constructed groundingrelative handful universally quantified physical laws.clear whether qprop leads polynomially sized solutions pigeonhole clique coloring problems. appears first blush should, sincequantification pigeons holes qprop analog identificationcorresponding symmetry previous section. know detailed proofliterature, however, attempts construct one unsuccessful.Similar remarks apply parity problems.235fiDixon, Ginsberg & ParkesInference qprop requires introduction (linear complexity) unification step,uniquely defined reasons maintained choices madesearch.exponential savings claimed unit propagation obviously average caseresult, opposed worst case one. consequence factpossible use subsearch part unit propagation, opposed generatetest mechanism used ground methods.addition usual idea relevance-based learning, quantified methodsextend power individual nogoods resolving quantified clauses insteadground instances.Finally, remark representation similar qprop also usedAnswer Set Programming (asp) (Marek & Truszczynski, 1999; Niemela, 1999)name propositional schemata (East & Truszczynski, 2001, 2002). approach usedasp resembles existing satisfiability work, however, clauses always grounded out.potential advantages intelligent subsearch thus exploited, although expectmany motivations results given would also apply asp. fact, aspmany features common sat:commonly used semantics, (non-disjunctive) stable model logicprogramming (Gelfond & Lifschitz, 1988), representational power preciselyNP (or N P N P disjunctive programming).Cardinality constraints allowed (East & Truszczynski, 2002; Simons, 2000).Solution methods (Leone, Pfeifer, & et al., 2002; Niemela, 1999; Simons, 2000) usedpll form propagation.significant difference conventional satisfiability work aspstable model semantics relevant logic classical logic(Pearce, 1997). logic there, law excluded middlehold, weaker p p. sufficient dpll applied,imply classical resolution longer valid. result, seems prooftheory resulting system, learning within framework yet understood.Backjumping used, mechanism seem learn new rules failedsubtrees search. analogous way, cardinality constraints used cuttingplane proof systems not. Despite many parallels sat asp, includingapproach survey seems somewhat premature.6. ConclusionSatisfiability algorithms often developed framework providedeither random instances or, worse still, instances designed solely showtechnique proposed computational merit. algorithmsthus tended ignore problem features dominate computational requirementsapplied real problems.236fiBoolean Satisfiability Surveyrealistic problems, possible improve speed algorithmsinner loops (via qprop subsearch) reduce number times innerloops need executed (via learning move p-simulation hierarchy).classes improvements arise problems question structure.structure learned nogoods, used re-represent problem using pseudoBoolean quantified expressions.true table previous subsection viewed survey recent work satisfiability, also true table viewed rationalreconstruction goals researchers investigated various representationalextensions. mind, table accurately viewed report extentlinguistic semantic modifications successfully capture problem structure.Every column table structure. Improved representational efficiencypossible problem structure Boolean axiomatization typicallyobscures. structure allows progress made terms proof complexity.structure must preserved basic inference mechanism system questionremain useful, qprops ability speed inner loop unit propagationdirect consequence structure present subsearch problem. Finally, learningthought search reasonably concise descriptions large sectionssearch space contain solutions words, learning discoverystructure search space itself.setting next two papers series set. muchprogress satisfiability techniques thought structure exploitation,surely natural attempt understand exploit structure directly.see, techniques discussed work exploiting structure,exploit different instances single structure. zap work attemptunderstand, generalize streamline previous results setting uniformsetting.Acknowledgmentswould like thank members cirl, technical staff Time Systems,Eugene Luks David Hofer CIS department University Oregonassistance ideas series papers. would also like thankanonymous reviewers comments suggestions, found extremelyvaluable.work sponsored part grants Air Force Office Scientific Research(afosr) number F49620-92-J-0384, Air Force Research Laboratory (afrl) numberF30602-97-0294, Small Business Technology Transfer Research, Advanced Technology Institute (sttr-ati) number 20000766, Office Naval Research (onr) number N00014-00-C0233, Defense Advanced Research Projects Agency (darpa) Air Force Research Laboratory, Rome, NY, agreements numbered F30602-95-1-0023, F30602-971-0294, F30602-98-2-0181, F30602-00-2-0534, F33615-02-C-4032. views expressedauthors.237fiDixon, Ginsberg & ParkesReferencesAloul, F., Ramani, A., Markov, I., & Sakallah, K. (2002). PBS: backtrack search pseudoBoolean solver. Symposium Theory Applications Satisfiability Testing.Babai, L. (1995). Automorphism groups, isomorphism, reconstruction. Lovasz, L., Graham, R., & Grotschel, M. (Eds.), Handbook Combinatorics, chap. 27, pp. 14471540. North-Holland-Elsevier.Baker, A. B. (1994). hazards fancy backtracking. Proceedings TwelfthNational Conference Artificial Intelligence.Barth, P. (1995). Davis-Putnam based enumeration algorithm linear pseudoboolean optimization. Tech. rep. MPI-I-95-2-003, Max Planck Institut fur Informatik,Saarbrucken, Germany.Barth, P. (1996). Logic-Based 0-1 Constraint Programming, Vol. 5 Operations Research/Computer Science Interfaces Series. Kluwer.Baumgartner, P. (2000). FDPLL First-Order Davis-Putnam-Logeman-Loveland Procedure. McAllester, D. (Ed.), CADE-17 17th International ConferenceAutomated Deduction, Vol. 1831, pp. 200219. Springer.Bayardo, R. J., & Miranker, D. P. (1996). complexity analysis space-bounded learningalgorithms constraint satisfaction problem. Proceedings ThirteenthNational Conference Artificial Intelligence, pp. 298304.Bayardo, R. J., & Schrag, R. C. (1997). Using CSP look-back techniques solve real-worldSAT instances. Proceedings Fourteenth National Conference ArtificialIntelligence, pp. 203208.Beame, P., & Pitassi, T. (2001). Propositional proof complexity: Past, present future.Paun, G., Rozenberg, G., & Salomaa, A. (Eds.), Current Trends TheoreticalComputer Science, Entering 21th Century, pp. 4270. World Scientific.Benhamou, B., Sais, L., & Siegel, P. (1994). Two proof procedures cardinality basedlanguage propositional calculus. Proceedings STACS94, volume 775 de LectureNotes Computer Science.Biere, A., Clarke, E., Raimi, R., & Zhu, Y. (1999). Verifying safety properties PowerPC microprocessor using symbolic model checking without BDDs. Lecture NotesComputer Science, 1633.Bonet, M. L., Pitassi, T., & Raz, R. (1997). Lower bounds cutting planes proofssmall coefficients. Journal Symbolic Logic, 62 (3), 708728.Brown, C. A., Finkelstein, L., & Paul Walton Purdom, J. (1988). Backtrack searchingpresence symmetry. Mora, T. (Ed.), Applied Algebra, Algebraic AlgorithmsError-Correcting Codes, 6th Intl. Conf., pp. 99110. Springer-Verlag.Bryant, R. E. (1986). Graph-based algorithms Boolean function manipulation. IEEETransactions Computers, C-35 (8), 677691.Bryant, R. E. (1992). Symbolic Boolean manipulation ordered binary-decision diagrams. ACM Computing Surveys, 24 (3), 293318.238fiBoolean Satisfiability SurveyBuchberger, B. (1965). Ein Algorithmus zum Auffinden der Basiselemente des Restklassenringes nach einum nulldimensionalen Polynomideal. Ph.D. thesis, University Innsbruck, Innsbruck.Buchberger, B. (1985). Grobner bases: algorithmic method polynomial ideal theory.Bose, N. (Ed.), Multidimensional Systems Theory. D. Reidel, Dordrecht, Holland.Cadoli, M., Schaerf, M., Giovanardi, A., & Giovanardi, M. (2002). algorithm evaluatequantified boolean formulae experimental evaluation. Journal AutomatedReasoning, 28 (2), 101142.Chai, D., & Kuehlmann, A. (2003). fast pseudo-Boolean constraint solver. Proceedings40th Design Automation Conference, pp. 830835.Chandru, V., & Hooker, J. N. (1999). Optimization Mehtods Logical Inference. WileyInterscience.Clegg, M., Edmonds, J., & Impagliazzo, R. (1996). Using Groebner basis algorithmfind proofs unsatisfiability. Proceedings Twenty-Eighth Annual ACMSymp. Theory Computing, pp. 174183.Coarfa, C., Demopoulos, D. D., San Miguel Aguirre, A., Subramanian, D., & Vardi, M.(2000). Random 3-SAT: plot thickens. Proceedings International Conference Constraint Programming.Cook, S. A. (1971). complexity theorem-proving procedures. Proceedings3rd Annual ACM Symposium Theory Computing, pp. 151158.Cook, W., Coullard, C., & Turan, G. (1987). complexity cutting-plane proofs.Discrete Applied Mathematics, 18, 2538.Copty, F., Fix, L., Giunchiglia, E., Kamhi, G., Tacchella, A., & Vardi, M. (2001). Benefitsbounded model checking industrial setting. 13th Conference ComputerAided Verification, CAV01, Paris, France.Crawford, J. M. (1992). theoretical analysis reasoning symmetry first-order logic(extended abstract). AAAI Workshop Tractable Reasoning.Crawford, J. M., & Auton, L. D. (1996). Experimental results crossover pointrandom 3SAT. Artificial Intelligence, 81, 3157.Crawford, J. M., Ginsberg, M. L., Luks, E., & Roy, A. (1996). Symmetry breaking predicatessearch problems. Proceedings Fifth International Conference PrinciplesKnowledge Representation Reasoning, Boston, MA.Davis, M., & Putnam, H. (1960). computing procedure quantification theory. J.Assoc. Comput. Mach., 7, 201215.Davis, M., Logemann, G., & Loveland, D. (1962). machine program theorem-proving.Communications ACM, 5 (7), 394397.Dechter, R. (1990). Enhancement schemes constraint processing: Backjumping, learning,cutset decomposition. Artificial Intelligence, 41, 273312.Dixon, H. E., & Ginsberg, M. L. (2000). Combining satisfiability techniques AIOR. Knowledge Engrg. Rev., 15, 3145.239fiDixon, Ginsberg & ParkesDixon, H. E., & Ginsberg, M. L. (2002). Inference methods pseudo-Boolean satisfiability solver. Proceedings Eighteenth National Conference ArtificialIntelligence.Dixon, H. E., Ginsberg, M. L., Hofer, D., Luks, E. M., & Parkes, A. J. (2003a). GeneralizingBoolean satisfiability III: Implementation. Tech. rep., Computational IntelligenceResearch Laboratory, Eugene, Oregon.Dixon, H. E., Ginsberg, M. L., Luks, E. M., & Parkes, A. J. (2003b). Generalizing Booleansatisfiability II: Theory. Tech. rep., Computational Intelligence Research Laboratory,Eugene, Oregon.Dubois, O., Andre, P., Boufkhad, Y., & Carlier, J. (1993). SAT versus UNSAT. SecondDIMACS Challenge: Cliques, Colorings Satisfiability, Rutgers University, NJ.Dubois, O., & Dequen, G. (2001). backbone-search heuristic efficient solving hard3-SAT formulae. Proceedings Seventeenth International Joint ConferenceArtificial Intelligence, pp. 248253.East, D., & Truszczynski, M. (2001). Propositional satisfiability answer-set programming.Lecture Notes Computer Science, 2174.East, D., & Truszczynski, M. (2002). Propositional satisfiability declarative programming. Extended version papers appeared Proceedings AAAI-2000Proceedings KI-2001. http://xxx.lanl.gov/abs/cs.LO/0211033.Freeman, J. W. (1995). Improvements propositional satisfiability search algorithms. Ph.D.thesis, University Pennsylvania, PA.Frost, D., & Dechter, R. (1994). Dead-end driven learning. Proceedings TwelfthNational Conference Artificial Intelligence, pp. 294300.Galperin, H., & Wigderson, A. (1983). Succinct representation graphs. InformationControl, 56, 183198.Garey, M., & Johnson, D. (1979). Computers Intractability. W.H. Freeman Co.,New York.Gaschnig, J. (1979). Performance measurement analysis certain search algorithms.Tech. rep. CMU-CS-79-124, Carnegie-Mellon University.Gelfond, M., & Lifschitz, V. (1988). stable semantics logic programs. Proceedings5th International Conference Logic Programming, pp. 10701080. MIT Press.Ginsberg, M. L. (1993). Dynamic backtracking. Journal Artificial Intelligence Research,1, 2546.Ginsberg, M. L., & Geddis, D. F. (1991). need domain-dependent controlinformation?. Proceedings Ninth National Conference Artificial Intelligence.Ginsberg, M. L., & Parkes, A. J. (2000). Search, subsearch QPROP. ProceedingsSeventh International Conference Principles Knowledge RepresentationReasoning, Breckenridge, Colorado.240fiBoolean Satisfiability SurveyGoldberg, E., & Novikov, Y. (2002). Berkmin: fast robust SAT solver. DesignAutomation Test Europe (DATE), pp. 142149.Guignard, M., & Spielberg, K. (1981). Logical reduction methods zero-one programming.Operations Research, 29.Haken, A. (1985). intractability resolution. Theoretical Computer Science, 39, 297308.Haken, A. (1995). Counting bottlenecks show monotone P 6= N P . Proceedings 36thAnnual IEEE Symp. Foundations Computer Science (FOCS-95), pp. 3640,Milwaukee, MN. IEEE.Hooker, J. N. (1988). Generalized resolution cutting planes. Annals OperationsResearch, 12, 217239.Hooker, J. N., & Vinay, V. (1995). Branching rules satisfiability. J. Automated Reasoning,15, 359383.Jeroslow, R., & Wang, J. (1990). Solving propositional satisfiability problem. AnnalsMathematics Artificial Intelligence, 1, 167187.Joslin, D., & Roy, A. (1997). Exploiting symmetry lifted CSPs. ProceedingsFourteenth National Conference Artificial Intelligence, pp. 197202.Kautz, H., & Selman, B. (1998). BLACKBOX: new approach applicationtheorem proving problem solving. Artificial Intelligence Planning Systems: Proceedings Fourth International Conference. AAAI Press.Kautz, H. A., & Selman, B. (1992). Planning satisfiability. Proceedings TenthEuropean Conference Artificial Intelligence (ECAI92), pp. 359363.Kirkpatrick, S., & Selman, B. (1994). Critical behavior satisfiability randomBoolean expressions. Science, 264, 12971301.Krajicek, J. (1997). Interpolation theorems, lower bounds proof systems, independence results bounded arithmetic. J. Symb. Logic, 62 (2), 457486.Krishnamurthy, B. (1985). Short proofs tricky formulas. Acta Informatica, 22 (3), 253275.Leone, N., Pfeifer, G., & et al. (2002). DLV system knowledge representationreasoning. Tech. rep. 1843-02-14, Technical University Vienna.Li, C. M. (2000). Integrating equivalency reasoning Davis-Putnam procedure.Proceedings Seventeenth National Conference Artificial Intelligence, pp. 291296.Li, C. M., & Anbulagan (1997). Heuristics based unit propagation satisfiabilityproblems. Proceedings Fifteenth International Joint Conference ArtificialIntelligence, pp. 366371.Luks, E., & Roy, A. (2002). Symmetry breaking constraint satisfaction. Intl. Conf.Artificial Intelligence Mathematics, Ft. Lauderdale, Florida.Marek, V. W., & Truszczynski, M. (1999). Stable models alternative logic programming paradigm..241fiDixon, Ginsberg & ParkesMcCarthy, J. (1977). Epistemological problems artificial intelligence. ProceedingsFifth International Joint Conference Artificial Intelligence, pp. 10381044,Cambridge, MA.McCune, W., & Wos, L. (1997). Otter - CADE-13 competition incarnations. JournalAutomated Reasoning, 18 (2), 211220.Mitchell, D. G. (1998). Hard problems CSP algorithms. Proceedings FifteenthNational Conference Artificial Intelligence, pp. 398405.Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineeringefficient SAT solver. 39th Design Automation Conference.Nemhauser, G., & Wolsey, L. (1988). Integer Combinatorial Optimization. Wiley, NewYork.Niemela, I. (1999). Logic programs stable model semantics constraint programmingparadigm. Annals Mathematics Artificial Intelligence, 25, 241273.Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley.Parkes, A. J. (1999). Lifted Search Engines Satisfiability. Ph.D. thesis, UniversityOregon. Available http://www.cirl.uoregon.edu/parkes.Pearce, D. (1997). new logical characterization stable models answer sets.Dix, J., Pereira, L., & Przymusinski, T. (Eds.), Non-monotonic Extensions LogicProgramming, Vol. 1216 Lecture Notes Artificial Intelligence, pp. 5770.Pitassi,T.(2002).Propositionalproofcomplexitylecturenotes.www.cs.toronto.edu/toni/Courses/Proofcomplexity/Lectures/Lecture1/lecture1.ps(other lectures titled similarly).Prestwich, S. (2002). Randomised backtracking linear pseudo-Boolean constraint problems. Proceedings 4th International Workshop Integration AITechniques Constraint Programming Combinatorial Optimisation Problems(CPAIOR-02), pp. 720.Pretolani, D. (1993). Satisfiability hypergraphs. Ph.D. thesis, Universita di Pisa.Pudlak, P. (1997). Lower bounds resolution cutting planes proofs monotonecomputations. J. Symbolic Logic, 62 (3), 981998.Puget, J.-F. (1993). satisfiability symmetrical constrained satisfaction problems.J. Komorowski Z.W. Ras, editors, Proceedings ISMIS93, pages 350361.Springer-Verlag, 1993. Lecture Notes Artificial Intelligence 689.Reiter, R. (1978). closed world data bases. Gallaire, H., & Minker, J. (Eds.), LogicData Bases, pp. 119140. Plenum, New York.Savelsbergh, M. W. P. (1994). Preprocessing probing mixed integer programmingproblems. ORSA Journal Computing, 6, 445454.Schaefer, T. J. (1978). complexity satisfiability problems. ProceedingsTenth Annual ACM Symposium Theory Computing, pp. 216226.242fiBoolean Satisfiability SurveySelman, B., Kautz, H. A., & Cohen, B. (1993). Local search strategies satisfiability testing. Proceedings 1993 DIMACS Workshop Maximum Clique, Graph Coloring,Satisfiability.Simons, P. (2000). Extending implementing stable model semantics.. ResearchReport 58, Helsinki University Technology, Helsinki, Finland.Stallman, R. M., & Sussman, G. J. (1977). Forward reasoning dependency-directedbacktracking system computer-aided circuit analysis. Artificial Intelligence,9, 135196.Szeider, S. (2003). complexity resolution generalized symmetry rules. Alt,H., & Habib, M. (Eds.), Proceedings STACS03, volume 2607 Springer LectureNotes Computer Science, pp. 475486.Tseitin, G. (1970). complexity derivation propositional calculus. Slisenko,A. (Ed.), Studies Constructive Mathematics Mathematical Logic, Part 2, pp.466483. Consultants Bureau.Urquhart, A. (1995). complexity propositional proofs. Bull. Symbolic Logic, 1 (4),425467.Velev, M. N., & Bryant, R. E. (2001). Effective use boolean satisfiability proceduresformal verification superscalar VLIW. Proceedings 38th ConferenceDesign Automation Conference 2001, pp. 226231, New York, NY, USA. ACMPress.Walser, J. P. (1997). Solving linear pseudo-Boolean constraint problems local search.Proceedings Fourteenth National Conference Artificial Intelligence, pp.269274.Zhang, H., & Stickel, M. E. (2000). Implementing Davis-Putnam method. JournalAutomated Reasoning, 24 (1/2), 277296.243fiJournal Artificial Intelligence Research 21 (2004) 429-470Submitted 07/03; published 04/04Grounded Semantic Composition Visual ScenesPeter GorniakDeb Roypgorniak@media.mit.edudkroy@media.mit.eduMIT Media Laboratory20 Ames St.,Cambridge, 02139 USAAbstractpresent visually-grounded language understanding model based studypeople verbally describe objects scenes. emphasis model combinationindividual word meanings produce meanings complex referring expressions.model implemented, able understand broad range spatial referringexpressions. describe implementation word level visually-grounded semanticsembedding compositional parsing framework. implemented system selectscorrect referents response natural language expressions large percentagetest cases. analysis systems successes failures reveal visual contextinfluences semantics utterances propose future extensions model takecontext account.1. Introductionintroduce visually-grounded language understanding model based studypeople describe objects visual scenes kind shown Figure 1. designed studyelicit descriptions would naturally occur joint reference setting easyproduce understand human listener. typical referring expression Figure 1might be, far back purple cone thats behind row green ones. Speakers constructexpressions guide listeners attention intended objects. referring expressionssucceed communication speakers listeners find similar features visualscene salient, share understanding language grounded termsfeatures. work step towards longer term goals develop conversationalrobot (Hsiao, Mavridis, & Roy, 2003) fluidly connect language perceptionaction.study use descriptive spatial language task similar one robotsperform, collected several hundred referring expressions based scenes similar Figure1. analysed descriptions cataloguing visual features referredwithin scene, range linguistic devices (words grammatical patterns)used refer features. combination visual feature correspondinglinguistic device referred descriptive strategy. example sentence containsseveral descriptive strategies make use colour, spatial relations, spatial grouping.descriptive strategies used composition speaker make referenceunique object.propose set computational mechanisms correspond commonlyused descriptive strategies study. resulting model implementedset visual feature extraction algorithms, lexicon grounded terms visualc2004AI Access Foundation. rights reserved.fiGorniak & RoyFigure 1: sample scene used elicit visually-grounded referring expressions (if figurereproduced black white, light cones green colour,dark cones purple)features, robust parser capture syntax spoken utterances, compositionalengine driven parser combines visual groundings lexical units. evaluatesystem, collected set spoken utterances three speakers. verbatim transcriptions speech, complete speech repairs various ungrammaticalitiescommon spoken language, fed model. model able correctlyunderstand visual referents 59% expressions (chance performanceassumingP30random object selected sessions 30 trials 1/30 i=1 1/i = 13%).system able resolve range linguistic phenomena made use relativelycomplex compositions spatial semantics. provide detailed analysis sourcesfailure evaluation, based propose number improvementsrequired achieve human level performance. designing framework buildprior work human reference resolution integration semantics parsing.main contribution work lies using visual features based study human visuallinguistic reference grounded semantic core natural language understandingsystem.previous work visually-grounded language centred machine learningapproaches (Roy, Gorniak, Mukherjee, & Juster, 2002; Roy & Pentland, 2002; Roy, 2002),chose apply machine learning problem compositional grounded semanticsinvestigation. Rather, endeavoured provide framework processtypes descriptive strategies compositionality found study humanparticipants. future work, investigate machine learning methods usedacquire parts framework experience, leading robust accurateperformance.430fiGrounded Semantic Composition Visual Scenes1.1 Grounded Semantic Compositionuse term grounded semantic composition highlight semanticsindividual words word composition process visually-grounded.model, lexical entrys meaning grounded association visual model.example, green associated probability distribution function definedcolour space. propose processes combine visual models words, governedrules syntax.Given goal understanding modelling grounded semantic composition, severalquestions arise:visual features people use describe objects scenesFigure 1?features connect language?features descriptions combine produce whole utterancesmeanings?word meanings independent visual scene describe?meaning whole utterance based meanings parts?composition meanings purely incremental process?assumed easy answers questions place start modellingeffort. current implementation assumes meaning whole utterancefully derived meanings parts, performs composition incrementally (withoutbacktracking), let visual context influence interpretation wordmeanings. Despite assumptions, system handles relatively sophisticated semanticcomposition. evaluated test data, system correctly understood choseappropriate referents expressions as, purple one behind two green onesleft green cone front back purple one.analysing systems performance human participants utterances, foundthat:Word meanings strongly dependent visual scene describe.instance, found four distinct visual interpretations word middlelinguistically indistinguishable, instead depend different visual contextsunderstood.meaning utterance may sometimes depend meaningsparts. meaning may also depend visual context utteranceoccurs, modify parts compose. example, objectsreferred frontmost left would referred neither left frontmostisolation, result multiplicative joined estimation two.Composition meanings task purely incremental process.cases found necessary backtrack reinterpret parts utterance431fiGorniak & Roygood referents found later processing stage, ambiguities cannotresolved current interpretation. Due strictly feed forward modellanguage understanding, current implementation fails cases.results similar reported prior studies (Brown-Schmidt, Campana,& Tanenhaus, 2002; Griffin & Bock, 2000; Pechmann, 1989). Although modelcurrently address issues context-dependent interpretation backtracking,believe framework approach grounded compositional semantics provideuseful steps towards understanding spatial language. system performs well understanding spatial descriptions, applied various tasks natural languagespeech based human-machine interfaces.paper begins highlighting several strands related previous work. section 2,introduce visual description task serves basis study model.Section 3 presents framework grounded compositional semantics. Section 4 describesresulting computational model. example whole system work givenSection 5. discuss results applying system human data spatialdescription task section 6, together analysis systems successes failures.leads suggestions future work Section 7, followed summary section 8.1.2 Related WorkWinograds SHRDLU well known system could understand generate naturallanguage referring objects actions simple blocks world (Winograd, 1970). Likesystem performs semantic interpretation parsing attaching short procedureslexical units (see also Miller & Johnson-Laird, 1976). However, SHRDLU accessclean symbolic representation scene, whereas system discussed workssynthetic vision system reasons geometric visual measures. Furthermore,intend system robustly understand many ways human participantsverbally describe objects complex visual scenes other, whereas SHRDLUrestricted sentences could parse completely translate correctly formalism.SHRDLU based formal approach semantics problem meaningaddressed logical set theoretic formalisms. Partee provides overviewapproach problems context based meanings meaning compositionalityperspective (Partee, 1995). work reflects many ideas work,viewing adjectives functions. Pustejovskys theory Generative Lexicon(GL) particular takes seriously noun phrase semantics semantic compositionality(Pustejovsky, 1995). approach lexical semantic composition originally inspiredPustejovskys qualia structures. However, formal approaches operate symbolicdomain leave details non-linguistic influences meaning unspecified, whereastake computational modelling influences primary concern.Research concerning human production referring expressions lead studies related one described here, without computational counterparts. Brown-Schmidte.a., example, engage participants free-form dialogue (as opposed one-sideddescriptions task) producing referential descriptions solve spatial arrangementproblem (Brown-Schmidt et al., 2002). Due use complicated scenescomplete dialogues, find participants often engage agreement behaviours432fiGrounded Semantic Composition Visual Scenesuse discourse visual context disambiguate underspecified referring expressionsoften study. Similar tasks used studies dialoguereferring expressions (Pechmann, 1989; Griffin & Bock, 2000). intentionally eliminated dialogue used simpler visual scene task elicit spatial descriptions (asopposed description object attributes), able computationally modelstrategies participants employ. Formal theories vagueness support findingsexpressions produced participants (Kyburg & Morreau, 2000; Barker, 2002).Word meanings approached several researchers problem associatingvisual representations, often complex internal structure, word forms. Modelssuggested visual representations underlying colour (Lammens, 1994) spatialrelations (Regier, 1996; Regier & Carlson, 2001). Models verbs include groundingsemantics perception actions (Siskind, 2001), grounding terms motorcontrol programs (Bailey, 1997; Narayanan, 1997). Object shape clearly importantconnecting language world, remains challenging problem computationalmodels language grounding. previous work, used histograms local geometricfeatures found sufficient grounding names basic objects (dogs, shoes, cars,etc.) (Roy & Pentland, 2002). representation captures characteristics overalloutline form object invariant in-plane rotations changes scale. LandauJackendoff provide detailed analysis additional visual shape features playrole language (Landau & Jackendoff, 1993). example, suggest importanceextracting geometric axes objects order ground words end, endstick. Shi Malik propose approach performing visual grouping images(Shi & Malik, 2000). work draws findings Gestalt psychology providemany insights visual grouping behaviour (Wertheimer, 1999; Desolneux, Moisan, &Morel, 2003). Engbers e.a. give overview formalization grouping problemgeneral various approaches solution (Engbers & Smeulders, 2003). parallelwork presented paper, also studying visual grouping foldresults systen described (Dhande, 2003).model incremental semantic interpretation parsing follows traditionemploying constraint satisfaction algorithms incorporate semantic information startingSHRDLU continued systems (Haddock, 1989). prior systems usedeclaratively stated set semantic facts disconnected perception. Closelyrelated work area Schulers (2003), integrates determination referentsparsing process augmenting grammar logical expressions, much likeaugment grammar grounded composition rules (see Section 3.4). emphasis,however, system actively ground word utterance meaningssensory system. Even though system described senses synthetic scene,makes continuous measurements parsing process integratingactive vision system (Hsiao et al., 2003). Schulers system requires human-specifiedclean logical encoding world state, ignores noisy, complex difficultto-maintain process linking language sensed world. consider process,call grounding process, one important aspects situated human-likelanguage understanding.SAM (Brown, Buntschuh, & Wilpon, 1992) Ubiquitous Talker (Nagao & Rekimoto,1995) language understanding systems map language objects visual scenes.433fiGorniak & RoySimilar SHDRLU, underlying representation visual scenes symbolic losesmuch subtle visual information work, work cited above, focuson. SAM Ubiquitous Talker incorporate vision system, phrase parserunderstanding system. systems translate visually perceived objects symbolicknowledge base map utterances plans operate knowledge base.contrast, primarily concerned understanding language referring objectsrelations appear visually.previously proposed methods visually-grounded language learning (Roy &Pentland, 2002), understanding (Roy et al., 2002), generation (Roy, 2002). However,treatment semantic composition efforts relatively primitive. phrase,visual models word phrase individually evaluated multiplied.method works phrases conjunctive modifiers, even cases,discuss later, ordering modifiers sometimes needs taken account (i.e., leftmostfront always refer front leftmost does). simple approachworked constrained domains addressed past, scalepresent task. example, Describer system (Roy, 2002) encodes spatial locationsabsolute terms within frame reference visual scene. result, Describermakes mistakes humans would make. grounding word highest,example, defined probability distribution centred specific height scene,object closest height best example highest, accountingfact may objects greater height (depending relative sizesshapes objects). addition, Describers interpretation phrase like highestgreen rectangle find object close center probabilitydistributions highest green, accounting fact humanlistener highest green rectangle need high screen (but highergreen rectangles). word highest requires visual bindingincludes notion rank ordering. move, however, requires rethinkingachieve semantic composition, addressed approach here.2. Spatial Description Taskdesigned task requires people describe objects computer generated scenescontaining 30 objects random positions virtual surface. objectsidentical shapes sizes, either green purple colour. object50% chance green, otherwise purple. refer task Bishoptask, resulting language understanding model implemented system simplyBishop.2.1 Motivation Task Designprevious work, investigated speakers describe objects distinctiveattributes like colour, shape size constraint speaking task scenesconstant number objects (Roy, 2002). Speakers task rarely compelleduse spatial relations never use groups objects, cases objectsdistinguished listing properties. designing Bishop task, goalnaturally lead speakers make reference spatial aspects scene. Therefore,434fiGrounded Semantic Composition Visual Scenesdrastically increased number objects scene decreased numberdistinctive object attributes. also let number objects vary throughout trialscover scenes cluttered objects scenes objects analysis.variation task, ran experiments system chose objectsrandom speaker describer, rather allowing describer make choice.found made task difficult highly unnatural speakeroften visually salient arrangements randomly chosen objects took partin. result, listeners make many errors resolving reference variationtask (3.5% error speaker chose object versus 13% systemchose). limits accuracy pure linguistic reference appearedreaching random selection version task. Speakers seemed muchharder time finding visually salient landmarks, leading long less natural descriptions,example centre bunch green cones, four them, um, actuallyfour, but, ah, theres one thats centre pretty much piletop, ahm, say this... seventh cone rightside (followed listener counting cones pointing screen). avoid collectingunnatural data, decided use random selection version task.Another possible variant task would let system choose objectsnon-random manner based systems analysis objects would natural describe. However, approach would clearly bias data towards objectsmatched preexisting models system.Since interested people described objects spatially well visualfeatures found salient, decided let listener pick objects feltconcisely yet trivially describable. acknowledge task design easesdifficulty understanding task; speakers could find interesting objecteasy describe ways, resorted simpler choices like leftmostone. Yet, utterances elicited task relatively complex (see Appendixcomplete listing) provided serious challenges automatic languageunderstanding perspective.Scenes rendered 3D instead using equivalent 2D scene anticipationtransition understanding system camera driven vision system. use3D rendering introduces occlusion, shadows, sources ambiguity musteventually addressed transition real vision system. However, notescene include interesting 3D spatial relations features,claim description task thus system presented would generalizedirectly true 3D setting. Furthermore, use 3D informationvisual scene, system interprets spatial relations 2D. errs 2D sideambiguity inherent word like leftmost reference one scenes (theinterpretation differ due perspective effects: leftmost object interpretingscene 2D might leftmost interpreting 3D. believetask types visually grounded descriptions produced challengingcomputational system understand, hope show remainder paper.Finally, note goal design task study collaboration,dialogue agreement, goal experiments analyses (Carletta &Mellish, 1996; Eugenio, Jordan, Thomason, & Moore, 2000). use speaker/listener dyad435fiGorniak & Royensure descriptions produced understandable human listener,purposefully allow listeners speak. feedback channel speakerssuccessful unsuccessful selection described object. introduceminimal form dialogue, low error rate listeners leads us believe negativereinforcement negligible speakers task viewedexercise collaboration. cannot rule listeners adopted strategies usedpartners turn speak. However, relative similarity strategiespairs shows phenomenon make data unrepresentative,even produces types shortenings vagueness would expect seeextended description task speaking machine.2.2 Data CollectionParticipants study ranged age 22 30 years, included nativenon-native English speakers. Pairs participants seated backsother, looking computer screen displayed identical scenesFigure 1. pair, one participant served describer, listener.describer wore microphone used record speech. describer usedmouse select object scene, verbally described selected objectlistener. listener allowed communicate verbally otherwise all,except object selections. listeners task select objectcomputer display based verbal description. selected objects matched,disappeared scene describer would select describe another object.match, describer would re-attempt description understoodlistener. Using describer-listener dyad ensured speech data resembled naturalcommunicative dialogue. Participants told free select objectscene describe way thought would clear. also toldmake task trivial by, example, always selecting leftmost object describingleftmost. scene contained 30 objects beginning session,session ended objects remained, point describer listener switchedroles completed second session (some participants fulfilled role multiple times).found listeners study made extremely mistakes interpreting descriptions,seemed generally find task easy perform.Initially, collected 268 spoken object descriptions 6 participants. rawaudio segmented using speech segmentation algorithm based pause structure(Yoshida, 2002). Along utterances, corresponding scene layout targetobject identity recorded together times objects selected.268 utterance corpus referred development data set. manually transcribedspoken utterance verbatim, retaining speech errors (false starts variousungrammaticalities). Rather working grammatically controlled language,interest model language occurs conversational settings since longer termgoal transplant results work conversational robots languagespoken form. Off-topic speech events (laughter, questions task,remarks, filled pauses) marked (they appear resultsreport).436fiGrounded Semantic Composition Visual Scenesdeveloped simple algorithm pair utterances selections based timestamps. algorithm works backwards time point correctobject removed scene. collects on-topic utterances occurredremoval event previous removal event 4seconds apart. fuses single utterance, sends scene description,complete utterance identity removed object understanding system.utterance fusing necessary participants often paused descriptions.time, pauses beyond certain length usually indicated utterancespause contained errors rephrase occurred. pairing algorithmobviously heuristic nature, mark instances makes mistakes (wronglyleaving utterances attributing utterances wrong selection event) analysisdata below. report numbers utterances data sets paper,correspond many utterance-selection pairs pairing algorithm produces.means due errors algorithm numbers utterances reportdivisible 30, actual number objects selected session.development corpus analysed catalogue range common referring strategies (see Section 2.3). analysis served basis developing visually-groundedlanguage understanding system designed replace human listener task describedabove. implementation yielded acceptable results development corpus,collected another 179 spoken descriptions three additional participants evaluate generalization coverage approach. used exactly equipment, instructionscollection protocol collecting development data collect test data.average length utterances development test set 89 words. discussion analysis following sections focuses developmentset. Section 6 discuss performance test set.2.3 Descriptive Strategies Achieving Joint Referencenoted earlier, call combination visual feature measured current scene(or, case anaphora, previous scene) together linguistic realizationdescriptive strategy. section, catalogue common strategies describers used communicate listeners. analysis based strictly developmentdata set. discuss implemented system handles categories Section 4.distinguish three subsets development data:set containing utterance/selection pairs contain errors. errordue repair mistake human speakers part, segmentation mistakespeech segmenter, error utterance/selection pairing algorithm.set contains utterance/selection pairs employ descriptive strategiescover computational understanding system (we coverSections 2.3.1 2.3.5).set utterance/selection pairs development data membereither subset described above. refer subset clean set.437fiGorniak & RoyNote first two subsets mutually exclusive. following sections,report two percentages descriptive strategy. first percentageutterance/selection pairs employ specific descriptive strategy relative utterance/selection pairs development data set. second percentage utterance/selection pairs relative clean set utterance/selection pairs, described above.examples given paper actual utterances scenes developmenttest sets.2.3.1 ColourAlmost every utterance employs colour pick objects. designing task,intentionally trivialized problem colour reference. Objects come two distinctcolours, green purple. Unsurprisingly, participants used terms greenpurple refer colours. previous work addressed problemslearning visually-grounded models colour words (Roy & Pentland, 2002; Roy, 2002).Here, focus semantic compositionality terms, chose simplify colournaming problem. Figure 2 shows one instances colour used pickreferent. examples subsequent sections colour composeddescriptive strategies.Syntactically, colours expressed adjectives (as mentioned: green purple) always directly precede nouns modify. is, nobody ever saidgreen left one data, rather adjectives would commonly occur orderleft green one.data, green purple also sometimes take roles nouns, leastleft dangling noun phrase ellipse like leftmost purple. Althoughform dangling modifier might seem unlikely, occur spoken utterancestask. objects Bishop cones, participants trouble understandingellipsis, occur 7% data.Participants used colour identify one objects 96% data, 95%clean data.purple coneFigure 2: Example utterance using colour438fiGrounded Semantic Composition Visual Scenes2.3.2 Spatial Regions Extremasecond common descriptive strategy refer spatial extremes within groupsobjects spatial regions scene. left example Figure 3 uses two spatialterms pick referent: closest front, leverage spatialextrema direct listeners attention. example, selection spatial extremumappears operate relative green objects, i.e. speaker seems first attendset green cones, choose amongst them. Alternatively, closestfront could pick several objects colour, colour specification could filterspatial extrema determine final referent. case two interpretations yieldreferent, cases corpus second alternative (spatialselection followed colour selection) yields referents all.green one thats closest usfrontpurple one left sideFigure 3: Example utterances specifying objects referring spatial extremaright example Figure 3 shows phrases explicitly indicating spatial extrema still sometimes intended interpreted referring extrema. listenerinterpret left side referring left side scene, phrase wouldambiguous since four purple cones left side scene.hand, phrase unambiguous interpreted picking extremum. Figure 4 showsinstance right hand side actually refers region board.first example figure shows phrase right hand side combinedextremum term, lowest. Note referent right extremum. secondexample Figure 4, referent bottommost green object, and, (arguably,taking scene existing 3D), neither leftmost. Regions boardseem play role cases. Often local context region may play strongerrole global one, referent second example Figure 4 foundattending front left area scene, selecting left bottom exampleamongst candidates area. Along lines, words like middle largelyused describe region board, position relative cones.rather ubiquitous data, spatial extrema spatial regions often usedcombination descriptive strategies like grouping, frequentlycombined extrema region specifications. opposed combinedcolour adjectives, multiple spatial specifications tend interpreted left right order,is, selecting group objects matching first term, amongst choosingobjects match second term. examples Figure 4 could understood439fiGorniak & Roylowest purple right hand sidegreen cone left bottomFigure 4: Example utterances specifying regionspurple one front left cornerFigure 5: Extrema sequencesimply ignoring order spatial specifications instead finding conjoined best fit, i.e.best example bottommost leftmost. However, Figure 5 demonstratesgenerally case. scene contains two objects best fitsunordered interpretation front left, yet human participant confidently picksfront object. Possible conclusions extrema need interpreted sequence,participants demonstrating bias preferring front-back features left-rightones. implementation, choose sequence spatial extrema order occurinput.Participants used single spatial extrema identify one objects 72%data, 78% clean data. used spatial region specifications 20%data (also 20% clean data), combined multiple extrema regions 28% (30%clean data).2.3.3 Groupingprovide landmarks spatial relations specify sets objects select from, participants used language describe groups objects. Figure 6 shows two examplesgrouping constructs, first using unnumbered group cones (the green cones),second using count specify group (three). function group differenttwo examples: left scene participant specifies group landmarkserve spatial relation (see Section 2.3.4), whereas right scene participantfirst specifies group containing target object, utters another description selectwithin group. Note grouping alone never yields individual reference, participants compose grouping constructs referential tactics (predominantly extremaspatial relations) cases.440fiGrounded Semantic Composition Visual Scenespurple cone middle leftgreen conestheres three left side; onefurthest backFigure 6: Example utterances using groupingParticipants used grouping identify objects 12% data 10% cleandata. selected objects within described groups 7.5% data (8% cleandata) specified groups number objects (two, three, ...) 8.5% data(also 8.5% clean data).2.3.4 Spatial Relationsalready mentioned Section 2.3.3, participants sometimes used spatial relationsobjects groups objects. Examples relations expressed prepositionslike behind well phrases like left front of. alreadysaw example spatial relation involving group objects Figure 6, Figure 7shows two examples involve spatial relations individual objects.first example one examples pure spatial relations two individualobjects referenced colour. second example typical onespatial relation combined another strategy, extremum (as well two speecherrors describer).green cone green conetheres purple cone thatsway left hand sideanother purpleFigure 7: Example utterances specifying spatial relationsParticipants used spatial relations 6% data (7% clean data).441fiGorniak & Roy2.3.5 Anaphoranumber cases participants used anaphoric references previous object removeddescription task. Figure 8 shows sequence two scenes correspondingutterances second utterance refers back object selected first.closest purple one far leftsidegreen one right behind oneFigure 8: Example sequence anaphoric utteranceParticipants employed spatial relations 4% data (3% clean data).2.3.6addition phenomena listed preceding sections, participants used smallnumber description strategies. occurredyet addressed computational model selection distance (lexicalisedclose next to), selection neighbourhood (the green one surrounded purpleones), selection symmetry (the one opposite one), selection somethingakin local connectivity (the lone one). also additional types groupings,example grouping linearity (the row green ones, three purple diagonal)picking objects within group number (the second one left rowfive purple ones) cover here. strategies occurs less oftendata anaphora (it occurs 4% utterances, see previous section).annotated 13% data containing descriptive strategies onescovered preceding sections. However, devices often combinedphenomena covered here. marked 15% data containing errors. Errorscome form repairs speaker, faulty utterance segmentation speechsegmenter, misaligning utterances scenes system.also instances participants composing semantic phenomena wayshandle. two instances combining spatial relations (the oneright) instances specifying groups spatial extremaregions (the group purple ones left). countevaluation, rather counted errors; reported success rate correspondingly lower.442fiGrounded Semantic Composition Visual Scenes2.4 Summarypreceding sections catalogue strategies participants employed describing objects.computational system understands utterances using strategies must fulfillfollowing requirements:system must access visual scene able compute visualfeatures like used human speakers: natural groupings, inter-object distances,orderings spatial relationsmust robust language parsing mechanism discovers grammatical patterns associated descriptive strategiesFeeding parsing mechanism must visually grounded lexicon; entrylexicon must carry information descriptive strategies takes partin, descriptive strategies combine otherssemantic interpretation composition machinery must embeddedparsing processsystem must able interpret results parsing utterance makebest guess object whole utterance describesgo describe systems understanding framework, consisting visualsystem, grounded lexical entries parser Section 3. Section 4 discussmodules implemented understand human descriptive strategies.3. Understanding Frameworksection describe components Bishop understanding system detail,emphasis fit together work visually grounded understandingsystem. cover turn Bishops vision system, parser lexicon give shortoverview implementation descriptive strategies fits framework.3.1 Synthetic VisionInstead relying information use render scenes Bishop, includes3D object locations viewing angle, implemented simple synthetic vision algorithm. algorithm produces map attributing pixel rendered image oneobjects background. addition, use full colour informationpixel drawn rendered scene. goal loosely simulate view camerapointed scene real world objects, situation robots find in.past successfully migrated models synthetic vision (Roy, 2002) computervision (Roy et al., 2002) plan similar route deploy Bishop. Obviously, manyhard problems object detection well lighting noise robustness needsolved synthetic case, hope transfer back robots cameramade easier working 2D image. chose work virtual worldproject could freely change colour, number, size, shape arrangement443fiGorniak & Royobjects elicit interesting verbal behaviours participants, without runninglimitations object detection algorithms field view problems.Given input image regions corresponding objects segmented,features produced vision system are:average RGB colour average red, green blue components pixelsattributed objectcentre mass average x pixel positions objectdistance euclidean distance pairs objects centres massgroups groups objects scene determined finding sets objectscontain one object, object less thresholddistance another object group (distances measured centresmass)pairs groups, filtered produce groups two objectstriplets groups, filtered produce groups three objectsconvex hull set pixels forming smallest convex region enclosing set objectsattentional vector sum (AVS) AVS spatial relation measure objects. extreme parameter settings measures one two angles, formedcentres, formed closest points two objects. useparameter setting ( = 0.7) two extremes, produces intermediate angle depending objects shape. resulting direction measured relative set reference angles, system four Cartesian vectors(0, 1), (0, 1), (1, 0), (1, 0) (Regier & Carlson, 2001).3.2 Knowledge RepresentationObjects represented integer IDs system. ID set IDs visionsystem compute visual features described Section 3.1 based correspondingset pixels image. distinguishing ID together visual features representssystems total knowledge objects present scene. systeminstantiate new objects vision system convex hull groups objects.system also remembers ID object removed last, ask visionsystem perform feature computation visual scene objectremoved.Groups objects integer IDs treated objects(all visual features available them). IDs stored together listconstituent objects IDs, groups broken apart necessary.Finally, visible lexicon file Appendix B, lexical item stored setassociated parameters. parameters specify grammatical type, compositional arityreference behaviour (what word sense taken referring own:single object, group objects objects.) Furthermore, lexical item associated444fiGrounded Semantic Composition Visual Scenessemantic composer (see Sections 3.3 4) store sets parameters,specifying Gaussian together applicable dimensions caseprobabilistic composers.3.3 Lexical Entries ConceptsConceptually, treat lexical entries like classes object oriented programming language. instantiated, maintain internal state simple tagidentifying dimension along perform ordering, complex multidimensional probability distributions. entry also function interface specifiesperforms semantic composition. Currently, interface definition consists numberarrangement arguments entry willing accept, whereas type mismatcheshandled composition rather enforced interface. Finally,entry contain semantic composer encapsulates actual function combineentry constituents parse. composers described in-depthSection 4. lexicon used Bishop contains many lexical entries attaching differentsemantic composers word. example, left either spatial relationextremum. grammatical structure detected parser (see next Section)determines compositions attempted given utterance.composition, structures representing objects constituent referencespassed lexical entries. refer structures concepts. entryaccepts zero concepts, produces zero concepts resultcomposition operation. concept lists entities world possible referentsconstituent associated with, together real numbers representing rankingdue last composition operation. composer also mark concept referringprevious visual scene, allow anaphoric reference (see Section 4.5). also containsflags specifying whether referent group objects single object (conesvs. cone), whether uniquely pick single object ambiguousnature (the vs. a). flags used post-processing stage determinepossible ambiguities conflicts.lexicon, based development corpus, contains 93 words: 33 ADJ (adjectives), 2 CADJ (colour adjectives: green, purple), 38 N (nouns), 2 REL (relativepronouns: that, which), 1 VPRES (present tense verbs: is), 2 RELVPRES (relative pronoun/present tense verb combinations: thats, its), 1 ART (the), 3 SPEC(adjective specifiers: right (as right above), just), 7 P (prepositions), 4 specificprepositions (POF, PAT, two versions PIN). complete lexicon specificationreproduced Appendix B.3.4 Parsingprevious work used Markov models parse generate utterances(Roy, 2002), employ context free grammars. grammars naturally let usspecify local compositional constraints iterative structures. Specifically, allow usnaturally perform grounded semantic composition whenever grammar rule syntacticallycomplete, producing partial understanding fragments every node parse tree.parse structure utterance thus dictates compositions attempted. use445fiGorniak & Roybottom-up chart parser guide interpretation phrases (Allen, 1995). parseradvantage employs dynamic programming strategy efficiently reusealready computed subtrees parse. Furthermore, produces sub-componentsparse thus produces useable result without need parse specific symbol.using dynamic programming approach assuming meanings partsassembled meanings wholes. strictly committed assumptionfuture consider backtracking strategies necessary. Also note duefact framework often produces functions applied later stagesinterpretation (see section 4) avoid possible overcommitting decisions (excludingcorrect referent early stage understanding).Bishop performs partial parse, parse required cover wholeutterance, simply takes longest referring parsed segments best guess.Unknown words stop parse process. Rather, constituents would otherwiseend unknown word taken include unknown word, essence makingunknown words invisible parser understanding process. way recoveressentially grammatical chunks relations important understandingrestricted task. overview related partial parsing techniques, see work Abney(1997).grammar used partial chart parser shown Figure 1. Togethergrammar rules table shows argument structures associated rule.given grammar one argument structure per rule,number argument structures. design grammar constrainedcompositions must occur rule applied. especially seenprepositional phrases, must occur rule noun phrase modify.chart parser incrementally builds rule fragments left right fashion parse.rule syntactically complete, checks whether composers constituentstail rule accept number arguments specified rule (as shownlast column Table 1). so, calls semantic composer associatedconstituent concepts yielded arguments produce concept headrule. compose operation fails reason (the constituent cannot acceptarguments compose operation yield new concept) rulesucceed produce new constituent. several argument structures(not case final grammar shown here) compose operation yields severalalternative concepts, several instances head constituent created,concept.provide example chart produced grammar Figure 10 Section 5,part example whole understanding process. composition actions associatedlexical item, thus rule completion using grammar, listedAppendix B.3.5 Post-Parse Filteringparse performed, post-parse filtering algorithm picks best interpretation utterance. First, algorithm extracts longest constituentschart marked referring objects, assuming parsing utter446fiGrounded Semantic Composition Visual ScenesADJNPNPNPNPNPNPNPNPNPNPNPNPNPNPNPPPPT0ADJADJCADJNARTNPNPNPNPNPNPNPNPNPNPNPSPECPPOFT1ADJNPNT2NPPPRELVPRESPRELRELRELRELVPRESRELRELVPRESRELPPNPARTPNVPRESPVPRESPVPRESADJCADJT3T4T5T6NARTPOFNPNPPNPADJPOFNNPNPPOFNPNPArg StructureT1 (T0 )T0 (T1 )T0 (T1 )T0 ()T0 (T1 )T1 (T0 , T2 )T3 (T0 , T5 )T3 (T0 , T5 )T2 (T0 , T4 )T1 (T0 , T3 )T2 (T0 , T3 )T3 (T0 , T4 )T2 (T0 , T3 )T3 (T0 )T2 (T0 )T2 (T0 )T0 (T1 )T1 ()T0 ()Table 1: Grammar used Bishopance implies better understanding. filtering process checks candidatesconsistency along following criteria:candidates must either refer group single objectcandidates marked referring unambiguously specified single object,must unambiguously pick referentreferent specified single object case must across candidatescandidates marked selecting group, must select groupconsistency checks fail, filtering algorithm provide exact information type inconsistency occurred (within-group ambiguity, contradictingconstituents, object fulfilling description), constituents involvedinconsistency objects (if any) referenced candidate constituent.future, plan use information resolve inconsistencies active dialogue.Currently, enforce best single object choice post processing stage.filtering yields single object, nothing needs done. filtering yields groupobjects, choose best matching object (note case ignore factwhether resulting concept marked referring group single object).several inconsistent groups referents remain filtering, randomly pick one objectgroups.447fiGorniak & Roy4. Semantic Compositionsection revisit list descriptive strategies Section 2.3 explaincomputationally capture strategy composition partsutterance. composers presented follow composition schema: takeone concepts arguments yield another concept references possiblydifferent set objects. Concepts reference objects real numbered values indicatingstrength reference. Composers may introduce new objects, even onesexist scene such, may introduce new types objects (e.g. groupsobjects referenced one object). perform compositions, conceptprovides functionality produce single referent, group referents. singleobject produced simply one maximum reference strength, whereas groupproduced using reference strength threshold objects consideredpossible referents concept. threshold relative minimum maximumreference strength concept. composers first convert incoming conceptobjects references, subsequently perform computations objects.Furthermore, composers mark concepts referring, referring singleobject referring group objects. independent actual numberobjects yielded concept, used identify misinterpretations ambiguities. currently use flags delay composition arguments referobjects. example, constraint prevents left green cause composition green considered adjective. cases, new chaining semanticcomposers created delay application whole chain compositionsreferring word encountered. chaining composers internally maintain queuecomposers. argument composition operation refer object,composer producing argument composer accepting pushed ontoqueue. first referring argument encountered, whole queue composersexecuted starting new argument proceeding backwards ordercomposers encountered.plan exploit features co-operative setting onedescribed here, system engage clarifying dialogue user. explainSection 3.5 converged single object reference task discussionhere, alternatives would be.4.1 Colour - Probabilistic Attribute Composersmentioned Section 3.1, chose exploit information used renderscene, therefore must recover colour information final rendered image.hard problem Bishop presents virtual objects two colours.renderer produce colour variations objects due different angles distanceslight sources camera. colour average 2D projection object alsovaries due occlusion objects. interest making framework easilytransferable noisier vision system, worked within probabilistic framework.separately collected set labelled instances green purple cones, estimatedthree dimensional Gaussian distribution average red, green blue valuespixel belonging example cones.448fiGrounded Semantic Composition Visual Scenesasked compose given concept, type probabilistic attribute composer assigns object referenced source concept probability density functionevaluated measured average colour object.4.2 Spatial Extrema Spatial Regions - Ordering Composersdetermine spatial regions extrema, ordering composer orders objects alongspecified feature dimension (e.g. x coordinate relative group) picks referentsextreme end ordering. so, assigns exponential weight function objectsaccordingi(1+v)picking minimal objects, objects position sequence, v valuealong feature dimension specified, normalized range 0 1 objectsconsideration. maximal case weighted similarly, using reverse orderingsubtracting fraction exponent 2,(imax i)(2v)imax number objects considered. reported results = 0.38.formula lets referent weights fall exponentially position ordering distance extreme object. way extreme objects isolatedexcept cases many referents cluster around extremum, making pickingsingle referent difficult. attach type composer words like leftmosttop.ordering composer also order objects according absolute position, corresponding closely spatial regions rather spatial extrema relative group.reference strength formula version(1+ dmax )euclidean distance reference point, dmax maximumdistance amongst objects consideration.version composer attached words like middle. effectreference weights relative absolute position screen. object close centre board achieves greater reference weight word middle, independentlyposition objects kind. Ordering composers work across numberdimensions simply ordering objects Euclidean distance, using exponential falloff function cases. ordering composer middle,example, computes distance board centre centres mass objects,thus prefers centred screen.4.3 Grouping Composersnon-numbered grouping (e.g., describer says group cones), groupingcomposer searches scene groups objects within maximum distancethreshold another group member. threshold currently set hand basedsmall number random scenes designers identified isolated groups449fiGorniak & Royadjusted threshold correctly find others. considers objectsreferenced concept passed argument. numbered groups(two, three), composer applies additional constraint groupscontain correct number objects. Reference strengths concept determinedaverage distance objects within group. acknowledge approachgrouping simplistic currently investigating powerful visual groupingalgorithms take topological features consideration. spite simple approach,demonstrate instances successfully understanding references groupsBishop.output grouping composer may thought group groups. understand motivation this, consider utterance, one left grouppurple ones. expression, phrase group purple ones activate groupingcomposer find clusters purple cones. cluster, composer computesconvex hull (the minimal elastic band encompasses objects) createsnew composite object convex hull shape. compositiontakes place understand entire utterance, composite group serves potentiallandmark relative left.However, concepts marked behaviour changes split apart conceptsrefering groups. example, composer attached sets flag conceptspassing it. Note involved composition grammar rulestype NP NP P NP, performing spatial compositions phrases likeleft of. Therefore, phrase frontmost one three green ones pickfront object within best group three green objects.4.4 Spatial Relations - Spatial Composersspatial semantic composer employs version Attentional Vector Sum (AVS)suggested Regier Carlson (2001). AVS measure spatial relation meantapproximate human judgements corresponding words like left2D scenes objects. computes interpolation anglecentres masses objects angle two closest points objects,addition value depending height relative top landmark object.Despite participants talking 2D projections 3D scenes found AVSdistinguishes spatial relations used data rather well simply applied2D projections. participants often used spatial descriptors below, suggestingsometimes conceptualized scenes 2D. 3D setting would expectsee consistent use semantic patterns like front instead below.Given two concepts arguments, spatial semantic composer converts setsobjects, treating one set providing possible landmarks, providing possibletargets. composer calculates AVS possible combination landmarkstargets. reference vector used AVS specified lexical entry containingcomposer, e.g. (0, 1) behind. Finally, spatial composer divides resultEuclidean distance objects centres mass, account factparticipants exclusively used nearby objects select spatial relations.450fiGrounded Semantic Composition Visual Scenes4.5 Anaphoric ComposersTriggered words like (as left one) previous, anaphoriccomposer produces concept refers single object, namely last object removedscene session. object specially marks concept referringcurrent, previous visual scene, calculations conceptperformed visual context.example, parser calls upon anaphoric composer attached lexical entry provide interpretation one, composer marksproduced concept referring back previous visual scene, sets previously selected object possible referent. consider another composer, say spatialcomposer attached left one left one. asks spatialrelation features referents one one, spatial relationfeatures (see Section 4.4) computed previous visual scene objectremoved due previous utterance possible landmark spatialrelation.5. Example: Understanding Descriptionexample scenepurple oneone leftpurple one leftFigure 9: Example: purple one leftillustrate operation overall system, section stepexamples Bishop works detail. Consider scene top left Figure 9,output chart parser utterance, purple one left Figure 10.Starting top left parse output, parser finds lexicon ART(article) selecting composer takes one argument. finds two lexical entriespurple, one marked CADJ (colour adjective), one N (noun).composer, probabilistic attribute composer marked P(),adjective expects one argument whereas noun expects none. Given nounexpects arguments grammar contains rule form NP N, NP(noun phrase) instantiated probabilistic composer applied default setobjects yielded N, consists objects visible. composer call marked451fiGorniak & RoyP(N) chart. composition, NP contains subset purple objects(Figure 9, top right). point parser applies NP ART NP, producesNP spanning first two words contains purple objects, markedunambiguously referring object. S(NP) marks application selectingcomposer called S.ART:thepurpleoneleftCADJ:purpleN:purpleNP:P(N)NP:S(NP)N:oneNP:oneNP:P(N)NP:S(NP)P:onART:theN:leftADJ:leftN:leftNP:leftNP:leftNP:S(NP)NP:S(NP)NP:O.x.min(NP)NP:O.x.min(NP)NP:O.x.min(NP)Figure 10: Sample parse referring noun phraseparser goes produce similar NP covering first three words combiningpurple CADJ one result the. P (preposition) leftdangling moment needs constituent follows it. contains modifyingsemantic composer simply bridges P, applying first argument second.another the, left several lexical entries: ADJ one N formscontains ordering semantic composer takes single argument, whereas second Nform contains spatial semantic composer takes two arguments determine targetlandmark object. point parser combine left twopossible NPs, one containing ordering spatial composer. firstNPs turn fulfills need P second argument according NPNP P NP, performing ordering compose first one (for one left), selectingobjects left (Figure 9, bottom left). application ordering composer452fiGrounded Semantic Composition Visual Scenesdenoted O.x.min(NP) chart, indicating ordering composer orderingalong x axis selecting minimum along axis. combining purpleone, composer selects purple objects left (Figure 9, bottom right).Finally purple one, produces set objects purple one, marksconcept unambiguously picking single object. Note parser attemptsuse second interpretation left (the one containing spatial composer) failscomposer expects two arguments provided grammaticalstructure sentence.6. Results Discussionsection first discuss systems overall performance collected data, followed detailed discussion performance implemented descriptive strategies.6.1 Overall Performanceevaluation purposes, hand-annotated data, marking descriptive strategiesoccurred example. examples use several reference strategies. Table 2present overall accuracy results, indicating percentage different groupsexamples system picked referent person describing object. firstline table shows performance relative total set utterances collected.second one shows percentage utterances system understood correctly excludingmarked using descriptive strategy listed Section 4, thusexpected understood Bishop. examples given Section 2.3.6.final line Table 2 shows percentage utterances system pickedcorrect referent relative clean development testing sets, leaving utterancesmarked well marked containing kind error. definedearlier, could speech error still understood human listener, dueerror algorithm pairs utterances selection events. Additionally, relyingautomatic speech segmentation sometimes merged utterances oneseparate utterances. mistakenly attributes combination two descriptionsone object selection leaves another object selection without corresponding utterance.Note, however, due loose parsing strategy frequent redundanciesspeakers utterances system able handle good number utterances markedeither error.Utterance Setexceptexcept Errors (clean)Accuracy - Development76.5%83.2%86.7%Accuracy - Testing58.7%68.8%72.5%Table 2: Overall ResultsUsing unconstrained speech primarily made writing covering yet precise grammar difficult. difficulty together loose parsing strategy made system occasionallyattempt compositions supported grammatical structure utterance.453fiGorniak & Royovereager parsing strategy also produces number correct guesses wouldfound tighter grammar, found development tradeoff oftenfavoured looser parsing terms number correct responses produced. Constructinggrammar obvious area addressed machine learning approachfuture. Using speech segmenter together utterance reassembler producederrors used successful selection event strong guideline decidingspeech segments part description. Errors type occur less 1%data.Bishop Performance100random guess meanrandom guess mean +/ std dev9080Average Accuracy7060504030201001234developmentdevelopment cleantesttest cleanFigure 11: Results development test corporaFigure 11 graphs results corpus simulation uniform randomselection strategy. bar shows mean performance data set, error bars delimiting one standard deviation. figure shows results left right completedevelopment corpus, clean development corpus, complete test corpus cleantest corpus. system understands vast majority targeted utterances performssignificantly better random baseline. Given unconstrained nature inputcomplexity descriptive strategies described Section 2.3 considerimportant achievement.Table 3 provides detail various descriptive strategies lists percentage correctly identified referents utterances employing spatial extrema regions,454fiGrounded Semantic Composition Visual Scenescombinations one spatial extremum, grouping constructs, spatial relationsanaphora. Note categories mutually exclusive. listseparate results utterances employing colour terms colour termssource errors (due synthetic nature vision system).Utterance SetSpatial ExtremaCombined Spatial ExtremaGroupingSpatial RelationsAnaphoraAccuracy - Development86.8% (132/152)87.5% (49/56)34.8% (8/23)64.3% (9/14)100% (6/6)Accuracy - Test77.4% (72/93)75.0% (27/36)38.5% (5/13)40.0% (8/20)75.0% (3/4)Table 3: Detailed Resultssurprisingly, Bishop makes mistakes errors present strategiesimplemented occur. However, Bishop achieves good coverage evencases. often result overspecification part describer.tendency towards redundancy shows even simple cases, example usepurple even though purple cones left scene. translates furthermorespecifications relative groups objects simple leftmost wouldsuffice. Overspecification human referring expressions well-known phenomenon oftenattributed incremental nature speech production. Speakers may listing visuallysalient characteristics colour determining whether colour distinguishingfeature intended referent (Pechmann, 1989).worst performance, grouping composers, attributedfact visual grouping strategy simplistic task hand,phenomenon often combined rather complex ways strategies.combinations also account number mistakes amongst composerperform much better combined strategies grouping. covershortcomings grouping composers detail Section 6.2.3.Mistakes amongst descriptive strategies cover several causes:Overcommitment/undercommitment errors due fact interpretation implemented filtering process without backtracking. semanticcomposer must produce set objects attached reference strengths,next composer works set objects strictly feedforward manner.composition strategy fails target object left one stage (e.g.leftmost one front, leftmost selects leftmost objects, includingobvious example front good example leftmost). alsofails many target objects included (e.g. poor example leftmostincluded set turns ideal example front). Estimatinggroup membership thresholds data certainly decrease occurrenceerrors, real solution lies backtracking strategy combined composers sensitive visual scenery beyond immediate function.sensitive composers might take account facts isolated nature certain455fiGorniak & Roycandidates well global distribution cones across board. discussspecific cases global local visual context influence interpretationswords Section 6.2.Insufficient grammar cases contain many prepositional phrases (e.g.leftmost one group purple ones right bottom) grammarspecific enough produce unambiguous answers. grammar might attachright object rather group objects, taking accountbiases parsing human listeners showed.Flawed composers composers implemented sufficientunderstand facets corresponding human descriptive strategies.mention problems following section.6.2 Performance Composersgo reconsider descriptive strategy discuss successes failurescomposers designed deal each.6.2.1 ColourDue simple nature colour naming Bishop task, probabilistic composersresponsible selecting objects based colour made errors.6.2.2 Spatial Regions Extremaordering composers correctly identify 100% cases participant usescolour single spatial extremum description. conclude participantsfollow process yields result ordering objects along spatial dimensionpicking extreme candidate. Participants also favour descriptive strategy, usingcolour alone 38% clean data. Figure 3 provides examples typesystem handles without problems.Description spatial region occurs alone 5% clean data, togetherstrategies 15% clean data. Almost examples strategyoccurring alone use words like middle centre. left image Figure 12 exemplifiesuse middle ordering semantic composer models. object referredone closest centre board. model fact human speakersuse version descriptive strategy obvious single candidate object.right image Figure 12 shows different interpretation middle: objectmiddle group objects. Note group objects linguistically mentioned.Also note within group two candidate centre objects, onefront preferred. composer picks correct object use middletarget object also happens one closest centre board.Figure 13 shows another use word middle. strategy seems relatedlast one (picking object middle group), however scene happensdivided two groups objects single object them. Even thoughobject back closest one centre board, due visual456fiGrounded Semantic Composition Visual Scenesgreen one middlepurple cone middleFigure 12: Types middles 1context participants understand object middle. composer failscase.purple one middleFigure 13: Types middles 2Figure 14 shows sequence two scenes followed data collectionsession. first scene utterance clear example extremum combinedregion specification, ordering composers easily pick correct object.next scene, listener identified leftmost object one right middle,despite scenes similarity right image Figure 12, middle objectmiddle group. suspect use middle scenebiases understanding middle relative board case, providingexample visual, also historical context influence meaningswords. (Note right utterance right middle interpretedSPEC grammatical type Bishop, spatial role. See grammarTable 1.)green one middle frontpurple one right middleFigure 14: Types middles 3457fiGorniak & Roysummary, catalogue number different meanings word middledata linguistically indistinguishable, depend visual historicalcontext correctly understood. generally, impossible distinguish regionbased uses various extrema-based uses words based utterance alonedata. made decision treat middle signify regions left, top, etc.signify extrema, examples middle show selection meaningwords use depends far subtler criteria global local visual context,existence unambiguous candidate past use descriptive strategies.Participants composed one spatial region extrema references 30%clean data. ordering composers correctly interpret 85% cases, exampleFigure 4 Section 2.3.2. mistakes composers make usually dueovercommitment faulty ordering. Figure 15 shows example could interpretedeither problem (we indicate correct example object system selects).note example comes non-native English speaker oftenused native speakers would use in. system selects purple objectclosest back board instead indicated correct solution. couldinterpreted overcommitment, composer back includetarget object, leaving composer left wrong set objects choose from.better explanation perhaps ordering composers reversedcase, composer back take objects selected left input.However, ordering violates far common left-to-right ordering regionextrema strategies data, selected implement system. questionthus becomes causes difference ordering cases like one Figure 15.again, suspect visual context plays role. Perhaps clear listenerdouble spatial specification would overspecification object systemselects (it simply purple one back). response, listener may seekobject needs full utterance, true target. However, analysishard combine common trend towards overspecification partspeaker, leaving us need run focused study phenomena pinfactors play role interpretation.purple cone back left sideFigure 15: Misinterpreted utterance using composed extrema458fiGrounded Semantic Composition Visual Scenes6.2.3 Groupingcomposers implementing grouping strategies used participants simplistic composers implemented, compared depth actual phenomenonvisual grouping. left scene Figure 16 shows example grouping composerhandles without problem. group two cones isolated conesexample, thus easily found distance thresholding algorithm. contrast,right scene depicts example would require much greater sophistication findcorrect group. target group three cones visually isolated scene,requiring criteria like colinearity even make candidate. Furthermore,second colinear group three cones would easily best example rowthree purple cones absence target group. target groupsalignment vertical axis let stand row makelikely interpretation. algorithm currently fails include grouping hints,thus fails pick correct answer scene. Note hints always linguistically marked (through row), often colinearity silentlyassumed holding groups, making simple grouping operator fail. rich sourcemodels possible human grouping strategies like co-linearity comes researchGestalt Grouping (Wertheimer, 1999).cone right pairconespurple cone frontrow three purple conesFigure 16: Easy hard visual grouping6.2.4 Spatial Relationsgreen cone behind purple coneFigure 17: Successful spatial relation understandingAVS measure divided distance objects corresponds well humanspatial relation judgements task. errors occur utterances contain459fiGorniak & Royspatial relations due possible landmarks targets correctly identified(grouping region composers might fail provide correct referents). spatialrelation composer picks correct referent cases landmarks targetscorrect ones, example Figure 17. Also see next section correctexample spatial relations. Obviously, types spatial relations relationsbased purely distance combined relations (to left behind) decidedcover implementation, occur data coveredfuture efforts.6.2.5 Anaphoracone right frontcone behind leftFigure 18: Successful Anaphora Understandingsolution use anaphora Bishop task performs perfectly replicatingreference back single object clean data. reference usually combinedspatial relation data, Figure 18. Due equally good performancespatial relation composer, cover cases anaphora development data. However,complex variants anaphora currently cover, examplereference back groups objects sequence Figure 19, followsright example Figure 16.next cone rowlast cone rowFigure 19: Group Based Anaphora7. Future DirectionsGiven analysis Bishops performance, several areas future improvements may explored. descriptive strategies classifiedunderstood computational system:460fiGrounded Semantic Composition Visual ScenesDistance simple implementation understand strategy grammatical behaviour spatial relation composers, uses inverted distance measurescore target objects.Symmetry Selection symmetry occurred symmetry across horizontal centreboard data. thus propose mirror landmark object acrosshorizontal centre, scoring possible targets inverted distance mirrorimage.Numbered grouping limited groups two three objects,algorithm work higher numbers.In-group numbering descriptive strategy like second row understood slight modification ordering composer put peakexponential distribution ends middle sequences,rather arbitrary points.Connectivity simple way understand lonely cone could measure distanceclosest objects within group possible referents. better solution mightconstruct local connectivity graph look topologically isolated objects.Furthermore, already mentioned several areas possible improvementexisting system due faulty assumptions:Individual composers Every one semantic composers attempts solve separatehard problem, (e.g. grouping spatial relations) seen longlines work dedicated sophisticated solutions ours. individualproblems emphasis paper. believe improvementsimplementation improve system whole, much followingpossible techniques.Backtracking lack backtracking Bishop addressed. parseproduce single referent, backtracking would provide opportunity reviseloosen decisions made various stages interpretation referentproduced.Visual context semantics Backtracking solves problems system knowseither failed obtain answer, knows answer producedunlikely one. However, numerous examples data oneinterpretation utterance produces perfectly likely answer accordingmeasurements, example middle finds object exact centre screen.many scenes interpretation produces correct answer, measurementrelative objects would produce wrong one. However, observeparticipants interpret middle way obvious structurerest scene. chance scene divided group objects leftgroup objects right, middle reliably refer isolated objectgroups, even another object closer actual centrescreen. future system take account local global visual contextcomposition account human selection strategies.461fiGorniak & RoyLexical entries made assumption lexical entries word-like entitiescontain encapsulated semantic information. Even relatively constrainedtask, somewhat faulty assumption. example, ambiguity word likeresolved careful design grammar (see section 4.3), mayuseful treat entire phrases left single lexical entries, perhapsgrammar replace left spatial markers (Jackendoffproposed absorbing rules syntax lexicon, see Jackendoff, 2002).Dialogue constructing parse charts obtain rich set partial full syntacticsemantic fragments offering explanations parts utterance. present,largely ignore information-rich resource selecting best referent.successful approach might entail backtracking revision described above,also engage clarification dialogue human speaker. system coulduse fragments knows check validity interpretation (isgroup green ones mean?) could simply disambiguate directly (Whichtwo mean?) followed explanation confusion termssemantic fragments formed.Manual construction visually-grounded lexicon presented limitedaccuracy due various structural parametric decisions manually approximated. Machine learning algorithms may used learn many parameter settingsset hand work, including on-line learning adapt parametersverbal interaction. Although thresholds probability distribution functions mayestimated data using relatively straightforward methods, learning problemsfar challenging. example, learning new types composers appropriatecorresponding grammatical constructs poses difficult challenge future. Minimally,plan automate creation new versions old composers (e.g. applied differentdimensions attributes). Moving beyond this, clear how, example, sethandling functionality used determine groups referents expand automaticallyuseful ways. also interesting study people learn understand noveldescriptive strategies.also continuing work applying results grounded language systemsmultimodal interface design (Gorniak & Roy, 2003). recently demonstrated application Bishop system described paper problem referent resolutiongraphical user interface 3D modelling application, Blender (Blender Foundation ,2003). Using Bishop/Blender hybrid, users select sets objects correct wrongmouse selections voice commands select door behind one showwindows.8. Summarypresented model visually-grounded language understanding. heartmodel set lexical items, grounded terms visual features groupingproperties applied objects scene. robust parsing algorithm finds chunkssyntactically coherent words input utterance. determine semanticsphrases, parser activates semantic composers combine words determine462fiGrounded Semantic Composition Visual Scenesjoint reference. robust parser able process grammatically ill-formed transcriptsnatural spoken utterances. evaluations, system selected correct objects responseutterances 76.5% development set data, 58.7% test set data.clean data sets various speech processing errors held out, performance higheryet. suggested several avenues improving performance system including bettermethods spatial grouping, semantically guided backtracking sentence processing,use machine learning replace hand construction models, use interactivedialogue resolve ambiguities. near future, plan transplant Bishopinteractive conversational robot (Hsiao et al., 2003), vastly improving robots abilitycomprehend spatial language situated spoken dialogue.AcknowledgmentsThanks Ripley, Newt Jones.Appendix A. Utterances Test Data Setfollowing 179 utterances collected test data set. presentedcorrect order seen understanding system. means includeerrors due faulty speech segmentation well due algorithm stitches oversegmented utterances back together.green cone middlepurple cone behindpurple cone way leftpurple cone corner rightgreen cone frontgreen cone back next purple conepurple cone middle frontpurple cone middlefrontmost purple conegreen cone cornerobstructed green conepurple cone hidden backpurple cone right reargreen cone frontsolitary green conepurple cone front row three purple conesnext cone rowlast cone rowcone right pair conesconecone closest middle frontcone right set cones furthest leftcone right front463fiGorniak & Roycone behind leftfrontmost left conebackmost left conesolitary conecone middle rightfront cone conefrontmost green conegreen cone front right purple conegreen cone back row fourcone green cone behind purple conepurple cone behind row three green conesfrontmost green cone rightgreen cone corner backgreen cone backpurple cone back rightgreen cone front leftpurple cone behindpurple cone behind onegreen cone behind purple conegreen cone two purple conepurple cone frontpurple cone touching green conegreen cone frontpurple cone leftgreen cone back leftpurple cone middle front two purple conespurple cone left four green conespurple cone left leftmostgreen conefrontmost green conerear conerightmost conerearmost coneleft green conepurple cone green conegreen conefurthestmost green cone exact middlefrontmost green conerightmost green cone clump four green cones rightgreen cone front two purple cones near leftgreen cone two purple cones back middlefrontmost purple coneleftmost two purple cones right mean left,sorry leftmost two purple cones left sidegreen cone left halfway back464fiGrounded Semantic Composition Visual Scenesfrontmost green cone front purple conemiddle purple conegreen cone left leftgreen cone middle front green conegreen cone leftfurthestmost purple cone leftfurthest green coneleftmost green coneleftmost purple conemiddle green conegreen cone two green conesfrontmost purple conebackmost purple conegreen cone two purple cones nearest frontleftmost purple conegreen cone frontgreen conefrontmost two back purple conesrightmost purple coneleftmost purple conepurple cone frontlast purple conefrontmost purple cone clump five purple conesrightbackmost green conebackmost purple conegreen cone directly front purple conepurple cone behind green cone leftgreen cone behind purple cone leftleftmost two left back corner green conesrightmost purple conemiddle cone behind frontmost purple conegreen cone left front cornerpurple cone right back cornerthird green cone line green cones near middlegreen cone two purple cones near backgreen cone back leftgreen cone backgreen cone behind frontmost green conefrontmost green conegreen conelast line four purple conescentre purple cone three cones leftpurple cone two purple conesmiddle purple cone465fiGorniak & Royleftmost purple conemiddle purple conefront left purple conefront right purple conesecond four purple conesmiddle purple conepurple cone leftlast purple conegreen one middle way backpurple one way middle littleleft way backgreen one middle front thats front anothergreen onepurple one middle thats behind green oneright purple one front linepurple onesleft green one two purple ones lineleft purple one middle row mean middleline three purple onesgreen one left thats hidden purple oneleft purple one thats way corner separatemiddle towards right theres line purple onestheres kink line one thats right lines turnspurple one way right frontpurple one front middlegreen one middlepurple front way rightrightmost green oneleftmost green onelast green one last green onefrontmost purple one rightpurple one back towards left thats next twopurple onespurple one back towards right thats part pairpurple one front group rightpurple one middle thats front group one rightpurple one left way backpurple one left thats behind another purple onepurple one left thats front purple oneleft thatspurple one right thats hidden two purple onespurple one way back corner rightpurple one thats front right last onelast onepurple front right466fiGrounded Semantic Composition Visual Scenespurple one way rightgreen one right thats middle bunchgreen one way left thats almost totally obscuredlast purple one left crooked line purple onesfirst purple one left thats crooked linepurple one way one way back towardsleft thats behind green purple one waybackpurple one towards back thats pretty much backthats front green purple onepurple one middle backpurple one left thats furthest backgreen one middle thats furthest frontpurple one towards middle towards leftthats closestmiddle purple one stands thats closestpurple ones middle towards right onecornerpurple one thats closest middleway right green one middle line threegreen onesway right closest green oneway right close green onegreen one way right corner backpurple one thats towards back left cornerpurple one front left cornerpurple one near middle thats another purple onepurple one thats front another purple onepurple one rightpurple one leftgreen one middle thats behind another green oneclosest green one middle green one thats closestmiddlegreen one way back towards rightclose green one one leftone leftAppendix Bfollowing specifies complete lexicon used Bishop XML format. initialcomment explains attributes lexical entries.see online appendix file lexicon.xml.467fiGorniak & RoyReferencesAbney, S. (1997). Part-of-speech tagging partial parsing. Corpus-Based MethodsLanguage Speech, chap. 4, pp. 118136. Kluwer Academic Press, Dordrecht.Allen, J. (1995). Natural Language Understanding, chap. 3. Benjamin/CummingsPublishing Company, Inc, Redwood City, CA, USA.Bailey, D. (1997). push comes shove: computational model role motorcontrol acquisition action verbs. Ph.D. thesis, Computer science division,EECS Department, University California Berkeley.Barker, C. (2002). dynamics vagueness. Linguistics Philosophy, 25, 136.Blender Foundation (2003). Blender 3D graphics creation suite. http://www.blender3d.org.Brown, M., Buntschuh, B., & Wilpon, J. (1992). SAM: perceptive spoken languageunderstanding robot. IEEE Transactions Systems, Man Cybernetics, 6 (22),13901402.Brown-Schmidt, S., Campana, E., & Tanenhaus, M. K. (2002). Reference resolutionwild. Proceedings Cognitive Science Society.Carletta, J., & Mellish, C. (1996). Risk-taking recovery task-oriented dialogue.Journal Pragmatics, 26, 71107.Desolneux, A., Moisan, L., & Morel, J. (2003). grouping principle four applications.IEEE Transactions Pattern Analysis Machine Intelligence, 255 (4), 508513.Dhande, S. (2003). computational model connect gestalt perception naturallanguage. Masters thesis, Massachusetts Institure Technology.Engbers, E., & Smeulders, A. (2003). Design considerations generic grouping vision.IEEE Transactions Pattern Analysis Machine Intelligence, 255 (4), 445457.Eugenio, B. D., Jordan, P. W., Thomason, R. H., & Moore, J. D. (2000). agreementprocess: empirical investigation human-human computer-mediated collaborativedialogues. International Journal Human-Computer Studies, 53 (6), 10171076.Gorniak, P., & Roy, D. (2003). Augmenting user interfaces adaptive speech commands.Proceedings International Conference Multimodal Interfaces.Griffin, Z., & Bock, K. (2000). eyes say speaking. Psychological Science,11, 274279.Haddock, N. (1989). Computational models incremental semantic interpretation. Language Cognitive Processes, 4, 337368.Hsiao, K., Mavridis, N., & Roy, D. (2003). Coupling perception simulation: Stepstowards conversational robotics. Proceedings IEEE/RSJ International Conference Intelligent Robots Systems (IROS).Jackendoff, R. (2002). Whats lexicon?. Noteboom, S., Weerman, F., & Wijnen(Eds.), Storage Computation Language Faculty, chap. 2. Kluwer AcademicPress.468fiGrounded Semantic Composition Visual ScenesKyburg, A., & Morreau, M. (2000). Fitting words: vague words context. LinguisticsPhilosophy, 23, 577597.Lammens, J. M. (1994). computational model color perception color naming. Ph.D.thesis, State University New York.Landau, B., & Jackendoff, R. (1993). spatial language spatialcognition. Behavioural Brain Sciences, 2 (16), 217238.Miller, G., & Johnson-Laird, P. (1976). Language Perception. Harvard University Press.Nagao, K., & Rekimoto, J. (1995). Ubiquitous talker: Spoken language interactionreal world objects. Proceeding International Joint Conference ArtificialIntelligence.Narayanan, S. (1997). KARMA: Knowledge-based Action Representations MetaphorAspect. Ph.D. thesis, Univesity California, Berkeley.Partee, B. H. (1995). Lexical semantics compositionality. Gleitman, L. R., & Liberman, M. (Eds.), Invitation Cognitive Science: Language, Vol. 1, chap. 11, pp.311360. MIT Press, Cambridge, MA.Pechmann, T. (1989). Incremental speech production referential overspecification. Linguistics, 27, 89110.Pustejovsky, J. (1995). Generative Lexicon. MIT Press, Cambridge, MA, USA.Regier, T. (1996). Human Semantic Potential. MIT Press.Regier, T., & Carlson, L. (2001). Grounding spatial language perception: empirical computational investigation. Journal Experimental Psychology: General,130 (2), 273298.Roy, D. (2002). Learning visually-grounded words syntax scene description task.Computer Speech Language, 16 (3).Roy, D., Gorniak, P. J., Mukherjee, N., & Juster, J. (2002). trainable spoken languageunderstanding system. Proceedings International Conference Spoken Language Processing.Roy, D., & Pentland, A. (2002). Learning words sights sounds: computationalmodel. Cognitive Science, 26 (1), 113146.Schuler, W. (2003). Using model-theoretic semantic interpretation guide statistical parsing word recognition spoken language interface. Proceedings Association Computational Linguistics.Shi, J., & Malik, J. (2000). Normalized cuts image segmentation. IEEE TransactionsPattern Analysis Machine Intelligence, 8 (22), 888905.Siskind, J. M. (2001). Grounding lexical semantics verbs visual perception usingforce dynamics event logic. Journal Artificial Intelligence Research, 15, 3190.Wertheimer, M. (1999). Laws organization perceptual forms. source bookGestalt psychology, pp. 7188. Routledge, New York.Winograd, T. (1970). Procedures representation data computer programunderstanding natural language. Ph.D. thesis, Massachusetts Institute Technology.469fiGorniak & RoyYoshida, N. (2002). Utterance segmenation spontaneous speech recognition. Mastersthesis, Massachusetts Institute Technology.470fiJournal Artificial Intelligence Research 21 (2004) 1-17Submitted 7/03; published 1/04Effective Dimensions Hierarchical Latent Class ModelsNevin L. Zhanglzhang@cs.ust.hkDepartment Computer ScienceHong Kong University Science Technology, ChinaTomas Kockakocka@lisp.vse.czLaboratory Intelligent Systems PraguePrague University Economics, Czech RepublicAbstractHierarchical latent class (HLC) models tree-structured Bayesian networksleaf nodes observed internal nodes latent. theoretically welljustified model selection criteria HLC models particular Bayesian networkslatent nodes general. Nonetheless, empirical studies suggest BIC scorereasonable criterion use practice learning HLC models. Empirical studies alsosuggest sometimes model selection improved standard model dimensionreplaced effective model dimension penalty term BIC score.Effective dimensions difficult compute. paper, prove theoremrelates effective dimension HLC model effective dimensions numberlatent class models. theorem makes computationally feasible computeeffective dimensions large HLC models. theorem also used computeeffective dimensions general tree models.1. IntroductionHierarchical latent class (HLC) models (Zhang, 2002) tree-structured Bayesian networks(BNs) leaf nodes observed internal nodes latent. generalize latentclass models (Lazarsfeld Henry, 1968) first identified potentially usefulclass Bayesian networks Pearl (1988). concerned learning HLC modelsdata. fundamental question select among competing models.BIC score (Schwarz, 1978) popular metric researchers use select amongBayesian network models. consists loglikelihood term measures fitnessdata penalty term depends linearly upon standard model dimension, i.e.number linearly independent standard model parameters. variablesobserved, BIC score asymptotic approximation (the logarithm) marginallikelihood (Schwarz, 1978). also consistent sense that, given sufficient data,BIC score generative model model data sampled largermodels equivalent generative model.latent variables present, BIC score longer asymptotic approximation marginal likelihood (Geiger et al., 1996). remedied,extent, using concept effective model dimension. fact replace standard modeldimension effective model dimension BIC score, resulting scoring function,called BICe score, asymptotic approximation marginal likelihood almosteverywhere except singular points (Rusakov Geiger, 2002).c2004AI Access Foundation. rights reserved.fiZhang & KockaNeither BIC BICe proved consistent latent variable models.matter fact, even defined means model selection criterionconsistent latent variable models. Empirical studies suggest BIC scorewell-behaved practice task learning HLC models. three related searchbased algorithms learning HLC models, namely double hill-climbing (DHC) (Zhang,2002), single hill-climbing (SHC) (Zhang et al., 2003), heuristic SHC (HSHC) (Zhang,2003). absence theoretically well justified model selection criterion, Zhang (2002)tested DHC four existing scoring functions, namely AIC score (Akaike, 1974),BIC score, Cheeseman-Stutz (CS) score (Cheeseman Stutz, 1995), holdoutlogarithmic score (HLS)(Cowell et al., 1999). real-world synthetic data used.real-world data, BIC CS enabled DHC find models regardedbest domain experts. synthetic data, BIC CS enabled DHC findmodels either identical resemble closely true generative models.coupled AIC HLS, hand, DHC performed significantly worse. SHCHSHC tested synthetic data sampled fairly large HLC models (as much28 nodes). BIC used tests. cases, BIC enabled SHCHSHC find models either identical resemble closely true generativemodels. empirical results indicate algorithms perform well,also suggest BIC reasonable scoring function use learning HLC models.experiments also reveal model selection sometimes improved BICescore used instead BIC score. explain detail Section 3order use BICe score practice, need way compute effective dimensions. trivial task. effective dimension HLC model rankJacobian matrix mapping parameters model parametersjoint distribution observed variables. number rows Jacobianmatrix increases exponentially number observed variables. constructionJacobian matrix calculation rank computationally demanding.Moreover done algebraically high numerical precision avoiddegenerate cases. necessary precision grows size matrix.Settimi Smith (1998, 1999) studied effective dimensions two classes models:trees binary variables latent class (LC) models two observed variables.obtained complete characterization two classes. Geiger et al. (1996) computed effective dimensions number models. conjectured rareeffective standard dimensions LC model differ. matter fact,found one model. Kocka Zhang (2002) found quite number LC modelswhose effective standard dimensions differ. also proposed easily computableformula estimating effective dimensions LC models. estimation formulaempirically shown accurate.paper, prove theorem relates effective dimension HLC modeleffective dimensions two HLC models contain fewer latent variables.Repeated application theorem allows one reduce task computing effectivedimension HLC model subtasks computing effective dimensions LC models.makes computationally feasible compute effective dimensions large HLCmodels.2fiEffective Dimensions HLC Modelsstart Section 2 formal definition effective dimensions Bayesian networks latent variables. Section 3, provide empirical evidence suggest useBICe instead BIC sometimes improves model selection. Section 4 presents maintheorem Section 5 devoted proof theorem. Section 6, prove theorem effective dimensions general tree models explain maintheorem allows one compute effective dimension arbitrary tree models. Finally,concluding remarks provided Section 7.2. Effective Dimensions Bayesian Networkspaper, use capital letters X denote variables lower caseletters x denote states variables. domain cardinalityvariable X denoted X |X| respectively. Bold face capital lettersdenote sets variables. denotes Cartesian product domains variablesset Y. Elements denoted bold lower case letterssometimes referred states Y. consider variables finitenumber states.Consider Bayesian network model possibly contains latent variables.standard dimension ds(M ) number linearly independent parametersstandard parameterization . parameters denote, variable parentconfiguration variable, probability variable state (except one)given parent configuration. Suppose consist k variables x1 , x2 , . . . , xk . Let riqi respectively number states xi number possible combinationsstates parents. xi parent, let qi 1. ds(M ) givends(M ) =kXqi (ri 1).i=1~notational simplicity, denote standard dimension n. Let =(1 , 2 , . . . , n )vector n linearly independent model parameters . let setobserved variables. Suppose m+1 possible states. enumerate first statesy1 , y1 , . . . , ym .~ mapping(1im), P (yi ) function parameters .nn dimensional parameter space (a subspace R ) R , namely : (1 , 2 , . . . , n )(P (y1 ), P (y2 ), . . . , P (ym )). Jacobian matrix mapping following mnmatrix:~ = [Jij ] = [ P (yi ) ]JM ()j], understandingconvenience, often write matrix JM = [ P(Y)jelements j-th column obtained allowing run possible statesexcept one.~ commonly used parameterizationsi, P (yi ) function .~ Hence make followingBayesian networks, actually polynomial function .assumption:3fiZhang & KockaAssumption 1 Bayesian network parameterized parametersjoint distribution observed variables polynomial functions parametersM.obvious consequence assumption elements JM also polynomial~functions .~ JM matrix real numbers. Due Assumption 1, rankgiven value ,matrix constant almost everywhere parameter space (Geiger et al.,1996. Also see Section 5.1.). specific, rank everywhere except setmeasure zero smaller d. constant called regular rank JM .regular rank JM also called effective dimension Bayesian networkmodel . Hence denote de(M ). understand term effective dimension,consider subspace Rm spanned joint probability P (Y) observed variables,equivalently range mapping . term reflects fact that, almost every~ small enough open ball around ()~ resembles Euclidean space dimensionvalue ,(Geiger et al., 1996).multiple ways parameterize given Bayesian network model. However,choice parameterization affect space spanned joint probability P (Y).Together interpretation previous paragraph, implies definitioneffective dimension depend particular parameterization one uses.3. Selecting among HLC Modelshierarchical latent class (HLC) model Bayesian network (1) network structure rooted tree (2) variables leaf nodes observedvariables not. observed variables sometimes referred manifest variablesvariables latent variables. Figure 1 shows structures two HLCmodels. latent class (LC) model HLC model one latent variable.theme paper computation effective dimensions HLC models.mentioned introduction, interesting effective dimension, usedBIC score, gives us better approximation marginal likelihood. section,give example illustrate use effective dimension sometimes also leadsbetter model selection. also motivate introduce concept regularityused subsequent sections.3.1 Example Model SelectionConsider two HLC models shown Figure 1. one experiment, instantiatedparameters M1 random fashion sampled set D1 10,000 data recordsobserved variables. ran SHC HSHC data set D1 guidanceBIC score. algorithms produced model M2 . following, explain why,based D1 , one would prefer M2 M1 BIC used model selection M1would preferred BICe used instead. argue M1 preferred basedD1 hence BICe better scoring metric case.4fiEffective Dimensions HLC ModelsX1X2X2Y1Y2X3Y3Y4Y1Y5Y2Y6M1Y3X3Y4Y5Y6M2Figure 1: Two HLC models. shaded variables latent, variablesobserved. cardinality X1 2, cardinalities variables3.BIC BICe scores model given data set defined follows:ds(M )BIC(M |D) = logP (D|M, ~ )logN,2de(M )BICe(M |D) = logP (D|M, ~ )logN2~ maximum likelihood estimate parameters based Nsample size.example, notice M2 includes M1 sense M2 representprobability distributions observed variables M1 can. fact, makeconditional probability distributions observed variables M2 M1set PM2 (X2 ) PM2 (X3 |X2 )PM2 (X2 )PM2 (X3 |X2 ) =XPM1 (X1 )PM1 (X2 |X1 )PM1 (X3 |X1 ),X1probability distribution observed variables two models identical.M2 includes M1 , logP (D1 |M1 , ~1 ) logP (D1 |M2 , ~2 ). Togetherfact D1 sampled M1 , implies logP (D1 |M1 , ~1 ) logP (D1 |M2 , ~2 )sufficiently large enough sample size. standard dimension M1 45,M2 44. HenceBIC(M1 |D1 ) < BIC(M2 |D1 ).hand, effective dimensions M1 M2 43 44 respectively. HenceBICe(M1 |D1 ) > BICe(M2 |D1 ).Model M2 includes M1 . opposite clearly true effective dimensionM1 smaller M2 . So, M2 reality complex model M1 .model fit data D1 equally well. Hence simpler one, i.e. M1 , preferredother. agrees choice BICe score, disagrees choiceBIC score. Hence, BICe appropriate BIC case.5fiZhang & Kocka3.2 Regularityconsider another model M1 M1 except cardinality X1increased 2 3. easy show M2 includes M1 vice versa. So, twomodels equivalent terms capabilities representing probability distributionsobserved variables. hence said marginally equivalent. However, M1standard parameters M2 hence would always prefer M2 M1 .formalize consideration, introduce concept regularity.latent variable Z HLC model, enumerate neighbors (parent children)X1 , X2 , . . . , Xk . HLC model regular latent variable Z,|Z|Qki=1 |Xi |,maxki=1 |Xi |(1)strict inequality holds Z two neighbors least onelatent node. Models M1 M2 regular, model M1 not.irregular model always exists regular model marginally equivalent fewer standard parameters (Zhang, 2003b). regular modelobtained follows: latent node two neighborscardinality smaller one neighbors, remove latent nodeconnect two neighbors. latent node two neighborsviolates (1), reduce cardinality quantity right hand side. Repeatsteps changes made.also interesting note collection regular HLC models given setobserved variables finite (Zhang, 2002). provides finite search space tasklearning regular HLC models.1 rest paper, consider regularHLC models.ending subsection, point nice property effective model dimensionrelation model inclusion. HLC model includes another model, effectivedimension less latter. consequence, two marginally equivalentmodels effective dimensions hence BICe score.true standard model dimension BIC score.3.3 CS CSe Scoresargued empirical grounds BIC score reasonable scoring functionuse learning HLC models BICe score sometimes improve modelselection. two scores free problems. One problem derivationLaplace approximations marginal likelihood valid boundaryparameter space. CS score way alleviates problem. involves BIC scorebased completed data BIC score based original data. words,involves two Laplace approximations marginal likelihood. lets errors twoapproximation cancel other.Chickering Heckerman (1997) empirically found CS score quite accurateapproximation marginal likelihood robust boundary parameter1. definition regularity given paper slightly different one given Zhang (2002).Nonetheless, two conclusions mentioned paragraph remain true.6fiEffective Dimensions HLC ModelsXZXZZXM2M1Figure 2: Problem reduction.space. realized need effective model dimension CS score, althoughactually use it. would made differences experimentsbecause, models used, standard effective dimensions agree.use CSe refer scoring function one obtains replacing standard modeldimension CS score effective model dimensions. BICe betterBIC approximations marginal likelihood (Geiger et al., 1996), CSe betterCS. compute CSe, also need calculate effective dimensions.4. Effective Dimensions HLC Modelsseen, effective model dimension interesting number reasons.main result paper theorem effective dimension de(M ) regularHLC model contains one latent variable. Let X root ,latent node. least two latent nodes, must exist another latentnode Z child X. following, use terms X-branch Z-branchrespectively refer sets nodes separated Z X X Z.Let set observed variables Z-branch let setobserved variables. Note X-branch doesnt contain node X. relationshipamong X, Z, Y, depicted left-most picture Figure 2.standard parameterization includes parameters P (X) parametersP (Z|X). convenience, replace parameters parameters P (X, Z).mentioned end Section 2, reparameterization affect effectivedimension de(M ). reflect reparameterization, edge X Zdirected Figure 2.(0)(0)(0)Suppose P (X, Z) k0 parameters 1 , 2 , . . . , k0 . Suppose conditional distri(1)(1)(1)butions variables X-branch consists k1 parameters 1 , 2 , . . . , k1(2)(2)conditional distributions variables Z-branch consists k2 parameters 1 , 2 ,(2). . . , k2 . convenience sometimes refer three groups parameters usingthree vectors ~(0) , ~(1) ~(2) respectively.following, define two HLC models M1 M2 startingestablish relationship effective dimensions effective dimension. context, , M1 , M2 regarded purely Mathematical objects.semantics variables concern. particular, variable H latent7fiZhang & KockaX1X1 (6)X2X2 (3)Y3 (3)X4 (5)X3 (3)Y3X3X2X4X5 (5)X3X1X1X4Y1 (3)Y2 (2)Y4 (2)X5X5Y5 (6)Y1Y2X2X3Y4Y5Figure 3: picture left shows HLC model five observed five latentvariables, variable annotated name cardinality. pictureright shows components decompose HLC modelapplying Theorem 1. Latent variables shaded, observed variablesnot.might designated observed M1 M2 part definitionMathematical objects.obtain Bayesian network model B1 deleting Z-branch. Strictlyspeaking B1 Bayesian network due parameterization inherits : insteadprobability tables P (X) P (Z|X), table P (X, Z). P (X) P (Z|X)readily obtained P (X, Z). mind, view B1 Bayesian network.network obviously tree-structured. leaf variables include setvariable Z. define M1 HLC model share structure B1variable Z variables observed. parameters M1~(0) ~(1) .Similarly let B2 Bayesian network model obtained deleting Xbranch. tree-structure leaf variables include variable X.define M2 HLC model share structure B2variable X variables observed. parameters M2 ~(0) ~(2) .Theorem 1 Suppose regular HLC model contains two latent nodes.two HLC models M1 M2 defined text also regular. Moreover,de(M ) = de(M1 )+de(M2 )[ds(M1 )+ds(M2 )ds(M )].(2)words, effective dimension equals sum effective dimensions M1M2 minus number common parameters M1 M2 share.appreciate significance theorem, consider task computing effective dimension regular HLC model contains two latent nodes.8fiEffective Dimensions HLC Modelsrepeatedly applying theorem, reduce task subtasks calculating effective dimensions LC models. example, consider HLC model depictedpicture left Figure 3. Theorem 1 allows us to, purpose computingeffective dimension, decompose HLC model five LC models, shownright Figure 3.might one compute effective dimension LC model? One way usealgorithm suggested Geiger et al. (1996). algorithm first symbolically computesJacobian matrix, possible due Assumption 1. randomly assignsvalues parameters, resulting numerical matrix. rank numerical matrixcomputed diagonalization. rank Jacobian matrix equals effectivedimension LC model almost everywhere, get regular rank probabilityone. algorithm recently implemented Rusakov Geiger (2003). KockaZhang (2002) suggest alternative algorithm computes upper bound.algorithm fast empirically shown produce extremely tight bounds.Going back example, effective dimension LC models X1 , X2 , X3 , X4X5 26, 23, 23, 34 17 respectively. Thus effective dimension HLC modelFigure 3 26+23+34+23+17(531)(361)(631)(351) = 61. contrast,standard dimension model 5+62+62+62+34+55+5+34+52+5 = 110.5. Proof Main Resultsection devoted proof Theorem 1. begin propertiesJacobian matrices Bayesian network models.5.1 Properties Jacobian MatricesConsider Jacobian matrix JM Bayesian network model . matrix parameterized parameters ~ . Let v1 , v2 , . . . , vm column vectors JM .Lemma 1 number column vectors v1 , v2 , . . . , vm Jacobian matrix JMeither linearly dependent everywhere linearly independent almost everywhere.linearly dependent everywhere exists least one column vector vjexpressed linear combination column vectors everywhere.Proof: Consider diagonalizing following transposed matrix:[v1 , v2 , . . . , vm ]T .~ Hence wouldAccording Assumption 1, elements matrix polynomials (of ).multiply rows polynomials fraction polynomials. course, need also addone row another row. end process, get diagonal matrix whose nonzeroelements polynomials fractions polynomials. Suppose k nonzero rowssuppose correspond v1 , v2 , . . . , vk .elements diagonalized matrix polynomials fractions polynomials,~well-defined 2 nonzero almost everywhere (i.e. almost values ).k=m, vectors linearly independent almost everywhere.2. fraction well defined denominator zero.9fiZhang & Kockak<m, exist, j (k<jm), polynomials fractions polynomials ci(1ik)vj =kXci vi .(3)i=1coefficients ci determined tracing diagonalization process. vjexpressed linear combination {vi |i = 1, . . . , k} everywhere 3 . 2Although might sound trivial, lemma actually quite interesting.JM parameterized matrix. first part, example, implies existtwo subspaces parameter space nonzero measuresvectors linearly independent one subspace linearly dependent other.total number column vectors JM , get following lemma:Lemma 2 Jacobian matrix JM , exists collection column vectors formbasis column space almost everywhere. number vectors collectionequals regular rank matrix. Moreover, collection chosen includegiven set column vectors linearly independent almost everywhere.Proof: first part already proved. second part follows definitionregular rank. last part true could start diagonalization processtranspose vectors set top matrix. 25.2 Proof Theorem 1set prove Theorem 1. straightforward verify HLC modelsM1 M2 regular. suffices prove equation (2). restsection.set observed variables Y, set observed variables M1{Z} set observed variables M2 {X}. Hence Jacobian matricesmodels , M1 , M2 respectively written follows:JM= [JM1= [JM2= [P (O, Y),...,P (O, Y) P (O, Y)P (O, Y) P (O, Y)P (O, Y);;],...,,...,(0)(1)(1)(2)(2)k01k11k2,...,P (O, Z) P (O, Z)P (O, Z);],...,(0)(1)(1)k01k1(0)1P (O, Z)(0)1P (X, Y)(0)1,...,P (X, Y) P (X, Y)P (X, Y);],...,(0)(2)(2)k01k2~ ci might undefined3. subtle point here. fractions polynomials ,~ equation (3) alone, cannot conclude vj linearly depends {vi |i = 1, . . . , k}values .everywhere.conclusion nonetheless true two reasons. First set ~ values ciundefined measure zero. Second, vj linearly depend {vi |i = 1, . . . , k} one value~ would true sufficiently small nonetheless measure-positive ball around,value.10fiEffective Dimensions HLC Modelsclear one-to-one correspondence first k0 +k1 column vectorsJM column vectors JM1 one-to-one correspondencefirst k0 last k2 column vectors JM column vectors JM2 .first showClaim 1: first k0 vectors JM (JM1 JM2 ) linearly independentalmost everywhere.Together Lemma 2, Claim 1 implies collection column vectorsJM1 includes first k0 vectors basis column space JM1 almosteverywhere. particular, implies de(M1 )k0 . Suppose de(M1 )=k0 +r. Withoutloss generality, suppose basis vectorsP (O, Z)(0)1,...,P (O, Z) P (O, Z)P (O, Z);.,...,(0)(1)(1)k01r(4)symmetry, assume de(M2 )=k0 +s s0 following columnvectors form basis JM2 almost everywhere:P (X, Y)(0)1,...,P (X, Y) P (X, Y)P (X, Y).;,...,(0)(2)(2)k01(5)consider following list vectors JM :P (O, Y)(0)1,...,P (O, Y) P (O, Y)P (O, Y) P (O, Y)P (O, Y);;.,...,,...,(0)(1)(1)(2)(2)k01r1(6)showClaim 2: column vectors JM linearly depend vectors listed (6)everywhere.Claim 3: vectors listed (6) linearly independent almost everywhere.two claims imply vectors listed (6) form basis column spaceJM almost everywhere. Thereforede(M ) = k0 +r+s = de(M1 )+de(M2 )k0 .clear k0 =ds(M1 )+ds(M2 )ds(M ). Therefore Theorem 1 proved. 25.3 Proof Claim 1Lemma 3 Let Z latent node HLC model set observednodes subtree rooted Z. regular, set conditional distributionsnodes subtree way encode injective mapping Zsense P (Y=(z)|Z=z) = 1 z Z .11fiZhang & KockaProof: prove lemma induction number latent nodes subtreerooted Z. First consider case one latent node, namely Z.case, Z parent nodes Y. Enumerate nodes Y1 , Y2 , . . . , Yk .Qregular, |Z| ki=1 |Yi |. Hence define injective mappingQZ = ki=1 Yi . state z Z, (z) written = (y1 , y2 , . . . , yk ),yi state Yi . setP (Yi =yi |Z=z) = 1,P (Y=(z)|Z=z) = 1.consider case least two hidden nodes subtree rootedZ. Let W one latent node latent node descendants. Let (1)set observed nodes subtree rooted W Y(2) =Y\Y(1) . inductionhypothesis, parameterize subtree rooted W way encodesinjective mapping W Y(1) . Moreover, nodes W removed ,remains regular HLC model. model, parameterize subtree rootedZ way encodes injective mapping Z (W,Y(2) ) = W Y(2) .Together, two facts prove lemma. 2Corollary 1 Let Z latent node HLC model . Suppose Z latent neighborX. Let set observed nodes separated X Z. regular,set probability distributions nodes separated X Z wayencode injective mapping Z sense P (Y=(z)|Z=z) = 1z Z .Proof: corollary follows readily Lemma 3 property root-walkingoperation (Zhang, 2002). 2Proof Claim 1: Consider following matrix[(0)(0)P (X, Z)(0)1...,P (X, Z)(0)k0](7)(0)1 , 2 , . . . , k0 parameters joint distribution P (X, Z), matrixidentity matrix rows properly arranged. column vectors linearlyindependent almost everywhere.(0)(0)consider first k0 column vectors JM : P (O, Y)/1 , . . . , P (O, Y)/k0 .must linearly independent almost everywhere. not, one vectors, say(0)P (O, Y)/k0 , would linearly depend rest everywhere according Lemma 1.Observe (1ik0 ),P (O, Y)(0)=XP (O|X)P (Y|Z)X,ZP (X, Z)(0).(0)Choose P (O|X) P (Y|Z) Corollary 1. vector P (O, Y)/i might contain zero elements. remove zero elements, remains vector iden(0)(0)tical P (X, Z)/i . conclude P (X, Z)/k0 linearly depends12fiEffective Dimensions HLC Models(0)(0)P (X, Z)/1 . . . , P (X, Z)/k0 1 everywhere, contradicts conclusionprevious paragraph. Hence first k0 vectors JM must linearly independent almosteverywhere.evident that, using similar arguments, also show first k0 vectorsJM1 (JM2 ) linearly independent almost everywhere. Claim 1 therefore proved. 25.4 Proof Claim 2Every column vector JM1 linearly depends vectors listed (4) everywhere. ObserveP (O, Y)(0)XP (Y|Z)ZP (O, Y)(1)==XP (Y|Z)ZP (O, Z)(0), = 1, . . . , k0(1), = 1, . . . , k1 .P (O, Z)Therefore every column vector JM corresponds vectors JM1 linearly dependsfirst k0 +r vectors listed (6) everywhere.symmetry, every column vector JM corresponds vectors JM2 linearlydepends first k0 last vectors listed (6) everywhere. claim proved.25.5 Proof Claim 3prove claim contradiction. Assume vectors listed (6) linearlyindependent almost everywhere. According Lemma 1, one them, say v, must linearlydepend rest everywhere. Claim 1 Lemma 2, assume v(2)among last r+s vectors. Without loss generality, assume v P (O, Y)/s .~ exist real numbers ci (1ik0 ), c(1) (1ir), c(2)value ,(1is1)P (O, Y)(2)=k0Xi=1ciP (O, Y)(0)+rX(1) P (O, Y)(1)i=1ci+s1X(2) P (O, Y).(2)i=1ciNote last term right hand side, runs 1 s1.parameter vector ~ consists three subvectors ~(0) , ~(1) ~(2) . Set parameters(1)~ (for X-branch) Lemma 3. exists injective mapping XP (O=(x)|X=x) = 1 x X .(8)vectors (6), consider subvector consisting elementsstates images states X mapping . subvectors(0)(1)(2)denoted P (OX , Y)/i , P (OX , Y)/i , P (OX , Y)/i .values ~(0) ~(2) , still13fiZhang & KockaP (OX , Y)=(2)k0XciP (OX , Y)i=1(0)+rX(1) P (OX , Y)(1)i=1ci+s1X(2) P (OX , Y).(2)i=1ci(9)Consider first two terms right hand side:k0XcirX(1) P (OX , Y)(1)i=1k0rXXP (OX , Z)P (OX , Z) X(1) Xcci+P (Y|Z)P (Y|Z)(0)(1)i=1i=1ZZk0rXXP (OX , Z) X(1) P (OX , Z)ci+}P (Y|Z){ ci(0)(1)i=1i=1ZP (OX , Y)(0)i=1==+ciP(8) fact P (O, Z) =X P (X, Z)P (O|X), column vector(0)(0)P (OX , Z)/i identical vector P (X, Z)/i . argued proving(0)Claim 1, vectors {P (X, Z)/i |i=1, . . . , k0 } constitute basis k0 -dimensional(1)Euclidian space. implies that, vectors P (OX , Z)/i represented(0)linear combination vectors {P (OX , Z)/i |i = 1, . . . , k0 }. Consequently,exist ci (1ik0 )k0XciP (OX , Z)i=1(0)k0XP (OX , Y)+rX(1) P (OX , Z)(1)i=1ci=k0XciP (OX , Z)ciP (OX , Y)i=1(0)Hencei=1ci+(0)rX(1) P (OX , Y)(1)i=1ci=k0Xi=1(0)Combining equation equation (9), getP (OX , Y)(2)=k0XciP (OX , Y)i=1(0)+s1X(2) P (OX , Y).(2)i=1ciP(8) fact fact P (O, Y) = X P (X, Y)P (O|X), column(1)(1)vector P (OX , Y)/i identical vector P (X, Y)/i column vector(2)(2)P (OX , Y)/i identical vector P (X, Y)/i . HenceP (X, Y)(2)=k0Xi=1ciP (X, Y)(0)+s1X(2) P (X, Y).(2)i=1cicontradicts fact vectors equation form basis column spaceJM2 almost everywhere (see (5) Section 5.2) Therefore, Claim 3 must true. 214fiEffective Dimensions HLC Models6. Effective Dimensions TreesLet us use term tree model refer Markov random fields undirected treesfinite number random variables. root tree model nodes, gettree-structured Bayesian network model. tree model, define leaf nodesone neighbor. HLC model tree model leaf nodes observedothers latent.turns Theorem 1 enables us compute effective dimension treemodel. Consider arbitrary tree model. leaf nodes latent, removenodes without affecting effective dimension.removing latent leaf nodes, leaf nodes observed. non-leaf nodesalso observed, decompose model submodels observed non-leafnode. following theorem tells us model submodels related termseffective dimensions.Theorem 2 Suppose observed non-leaf node tree model . decomposesk submodels M1 , . . . , Mk ,de(M ) =kXde(Mi ) (k 1)(|Y | 1).i=1possible decompositions, final submodels either contain latent nodesHLC models. Effective dimensions submodels latent variables simplystandard dimensions. HLC submodel irregular, make regular applyingtransformation mentioned end Section 3.2. transformation affecteffective dimensions submodels. Finally, effective dimensions regular HLCsubmodels computed using Theorem 1.Proof Theorem 2: possible prove theorem starting Jacobianmatrix. take less formal revealing approach.suffices consider case k 2. two submodels M1 M2 share onenode, namely . Let O1 O2 respectively sets observed nodes twosubmodels excluding . Root .P (Y, O1 , O2 )P (Y ) = P (O1 , )P (O2 , ).Let ~0 set parameters distribution P (Y ), ~1 ~2 respectively setsparameters conditional probability distributions nodes M1 M2 . Considerfixing ~0 letting ~1 ~2 vary. case, space spanned P (Y ) consistsone vector, namely ~0 itself. Moreover, one-to-one correspondence vectorsspace spanned P (Y, O1 , O2 ) vectors Cartesian product spacesspanned P (O1 , ) P (O2 , ). let ~0 vary. adds |Y |1 dimensionsfour spaces spanned P (Y, O1 , O2 ), P (Y ), P (O1 , ), P (O2 , ). Consequently,de(M ) = de(M1 ) + de(M2 ) (|Y | 1).theorem proved. 215fiZhang & Kocka7. Concluding Remarkspaper study effective dimensions HLC models. work motivatedempirical evidence BIC behaves quite well used several hill-climbingalgorithms learning HLC models BICe score sometimes leads bettermodel selection BIC score. proved theorem relates effectivedimension HLC model effective dimensions two HLC modelscontain fewer latent variables. Repeated application theorem allows one reducetask computing effective dimension HLC model subtasks computingeffective dimensions LC models. makes computationally feasible computeeffective dimensions large HLC models. addition, proved theoremeffective dimensions general tree models. main theorem allows onecompute effective dimension arbitrary tree models.Acknowledgementswork initiated authors visiting Department Computer Science,Aalborg University, Denmark. thank Poul S. Eriksen, Finn V. Jensen, Jiri Vomlel,Marta Vomlelova, Thomas D. Nielsen, Olav Bangso, Jose Pena, Kristian G. Olesen.also grateful annonymous reviewers whose comments helped us greatlyimproving paper. Research paper partially supported GA CR grant201/02/1269 Hong Kong Research Grant Council grant HKUST6088/01E.ReferencesAkaike, H. (1974). new look statistical model identification. IEEE Trans. Autom.Contr., 19, 716-723.Bartholomew, D. J. Knott, M. (1999). Latent variable models factor analysis, 2ndedition. Kendalls Library Statistics 7. London: Arnold.Cheeseman, P. Stutz, J. (1995). Bayesian classification (AutoClass): Theoryresults. Fayyad, U., Piatesky-Shaoiro, G., Smyth, P., Uthurusamy, R. (eds.),Advancesin Knowledge Discovery Data Mining, AAAI Press, Menlo Park, CA.Chickering D. M. Heckerman D. (1997). Efficient Approximations MarginalLikelihood Bayesian Networks Hidden variables. Machine Learning, 29, 181212.Cowell, R. G., Dawid, A. P., Lauritzen, S. L., Spiegelhalter, D. J. (1999). Probabilisticnetworks expert systems, Springer.Kocka, T. Zhang, N. L. (2002). Dimension correction hierarchical latent classmodels. Proc. 18th Conference Uncertainty Artificial Intelligence(UAI-02).Geiger D., Heckerman D. Meek C. (1996). Asymptotic model selection directednetworks hidden variables. Proc. 12th Conference UncertaintyArtificial Intelligence, 283-290.16fiEffective Dimensions HLC ModelsGoodman, L. A. (1974). Exploratory latent structure analysis using identifiableunidentifiable models. Biometrika, 61, 215-231.Lazarsfeld, P. F., Henry, N.W. (1968).Mifflin.Rusakov, D. Geiger, D. (2002).networks. UAI-02.Latent structure analysis. Boston: HoughtonAsymptotic model selection Naive BayesianRusakov, D. Geiger, D. (2003). Automated analytic asymptotic evaluation marginallikelihood latent models. UAI-03.Schwarz G. (1978). Estimating dimension model. Annals Statistics, 6, 461-464.Settimi, R. Smith, J.Q. (1998). geometry Bayesian graphical modelshidden variables. Proceedings Fourteenth Conference UncertaintyArtificial Intelligence, Morgan Kaufmann Publishers, S. Francisco, CA, 472-479.Settimi, R. Smith, J.Q. (1999). Geometry, moments Bayesian networks hiddenvariables. Proceedings Seventh International Workshop Artificial Intelligence Statistics, Fort Lauderdale, Florida (3-6 January 1999), Morgan KaufmannPublishers, S. Francisco, CA.Zhang N. L. (2002). Hierarchical latent class models cluster analysis. AAAI-02, 230-237.Zhang, N. L., Kocka, T., Karciauskas, G., Jensen, F. V. (2003). Learning hierarchicallatent class models. Technical Report HKUST-CS03-01, Department ComputerScience, Hong Kong University Science Technology.Zhang, N. L. (2003).Structural EM Hierarchical Latent Class Models. TechnicalReport HKUST-CS03-06, Department Computer Science, Hong Kong UniversityScience Technology.Zhang, N. L. (2003b). Hierarchical latent class models cluster analysis. JournalMachine Learning Research, appear.17fiJournal Arti cial Intelligence Research 21 (2004) 631-670Submitted 9/03 published 6/04PHA*: Finding Shortest Path A* UnknownPhysical EnvironmentAriel FelnerDepartment Information Systems Engineering,Ben-Gurion University Negev, Beer-Sheva, 85104, IsraelRoni SternAsaph Ben-YairSarit KrausNathan NetanyahuDepartment Computer Science, Bar-Ilan UniversityRamat-Gan, Israel, 52900felner@bgumail.bgu.ac.ilsternr2@cs.biu.ac.ilbenyaya@cs.biu.ac.ilsarit@cs.biu.ac.ilnathan@cs.biu.ac.ilAbstractaddress problem nding shortest path two points unknownreal physical environment, traveling agent must move around environmentexplore unknown territory. introduce Physical-A* algorithm (PHA*) solvingproblem. PHA* expands mandatory nodes A* would expand returnsshortest path two points. However, due physical natureproblem, complexity algorithm measured traveling eort movingagent number generated nodes, standard A*. PHA* presentedtwo-level algorithm, high level, A*, chooses next node expandedlow level directs agent node order explore it. presentnumber variations high-level low-level procedures evaluateperformance theoretically experimentally. show travel cost bestvariation fairly close optimal travel cost, assuming mandatory nodesA* known advance. generalize algorithm multi-agent case,number cooperative agents designed solve problem. Speci cally, provideexperimental implementation system. noted problemaddressed navigation problem, rather problem nding shortestpath two points future usage.1. Introductionpaper address problem nding shortest path two pointsunknown real physical environment, mobile agent must travel aroundenvironment explore unknown territories. Search spaces path- nding problemscommonly represented graphs, states associated search spacerepresented graph nodes, transition states captured graph edges.Graphs represent dierent environments, road maps, games, communicationnetworks. Moving one node graph another done applying logicaloperators manipulate current state actual agent move onenode another. sliding-tile puzzle Rubik's Cube (Korf, 1999) examplesc 2004 AI Access Foundation. rights reserved.fiFelner, Stern, Ben-Yair, Kraus, & Netanyahurst type, road map example second type. Graphs search problemsdivided following three classes:Fully known graphs: nodes edges graph stored com-puter, graph fully known. input problems usuallycomplete graph represented adjacency matrix adjacency list.relevant problem case would nd, example, shortest pathroad map nodes edges known advance.large graphs: Graphs due storage time limitations completely known cannot fully stored storage device. Many graphssearch problems exponential number nodes. example, 24-tile puzzle problem 1025 states cannot completely stored current machines.input problems usually speci ed general structure statesearch space, dierent operators, initial state, set goal states.small portions graphs visited search algorithmsstored memory.Small, partially known graphs: third class contains graphs representpartially known physical environment. example, mobile agent unknownarea without map full knowledge environment. Given enoughtime, however, agent fully explore environment since large.Due partial knowledge, small portion graph given input.class fully-known graphs, classical algorithms, Dijkstra's single-sourceshortest-path algorithm (Dijkstra, 1959) Bellman-Ford algorithm (Bellman, 1958),used nd optimal path two nodes. algorithms assumenode graph accessed algorithm constant time. assumptionvalid since nodes edges graph known advance storedcomputer's memory. Thus time complexity algorithms measurednumber nodes edges process course search.second class graphs algorithms usually ecient, sincenumber nodes graph large (usually exponential). Also, smallportion graph stored memory given time. A* algorithm (Hart, Nilsson, & Raphael, 1968) linear space versions, e.g., IDA* (Korf, 1985) RBFS (Korf,1993), common methods nding shortest paths large graphs. A* keepsopen list nodes generated yet expanded, choosespromising node (the best node) expansion. node expanded movedopen list closed list, neighbors generated put openlist. search terminates goal node chosen expansion openlist empty. cost function A* f (n) = g(n) + h(n) g(n) distancetraveled initial state n, h(n) heuristic estimate cost noden goal. h(n) never overestimates actual cost node n goal, sayh(n) admissible. using admissible heuristic h(n), A* provedadmissible, complete, optimally eective (Dechter & Pearl, 1985). words,heuristic, A* guaranteed always return shortest path. Furthermore,632fiPHA*: Finding Shortest Path A* Unknown Physical Environmentalgorithm claiming return optimal path must expand least nodesexpanded A* given heuristic.A* expansion cycle carried constant time. takes constantamount time retrieve node open list, generate neighbors.latter involves applying domain-speci c operators expanded node. Thus timecomplexity A* also measured terms number generated nodes.1paper deal nding shortest path graphs third class, i.e.,small, partially known graphs correspond real physical environment. Unlikegraphs two classes, constant number computer operations donenode expansion, cannot assume, type graphs, visiting node takesconstant time. Many nodes edges graph known advance.Therefore, expand node known advance, mobile agent must rst travelnode order explore learn neighbors. cost searchcase cost moving agent physical environment, i.e., proportionaldistance traveled agent. ecient algorithm would therefore minimizedistance traveled agent optimal path found. Note since small graphsconsidered here, omit actual computation time focus traveltime agent. paper introduce Physical-A* algorithm (PHA*) solvingproblem. PHA* expands mandatory nodes A* would expand returnsshortest path two points. However, complexity algorithmmeasured traveling eort moving agent. order minimize travelingshown, PHA* designed minimize traveling eort agentintelligently choosing next assignment traveling agent. described below, manytimes agent chooses rst move nearby nodes even though immediatelycontribute proceeding A*.Unlike ordinary navigation tasks (Cucka, Netanyahu, & Rosenfeld, 1996 Korf, 1990Stentz, 1994 Shmoulian & Rimon, 1998), purpose agent reach goalnode soon possible, rather explore graph manner shortestpath retrieved future usage. hand, problem ordinaryexploration problem (Bender, Fernandez, Ron, Sahai, & Vadhan, 1998), entiregraph explored order mapped out. Following two motivatingexamples real world applications problem:Example 1: division troops ordered reach speci c location. coordinates location known. Navigating entire division unknownhostile territory reaching destination unreasonable inecient. common case team scouts search best path divisionpass through. scouts explore terrain report best path divisionmove along order reach destination ecient manner.1. fact, A*, open list stored priority queue, would take logarithmic timeretrieve best node. However, many problems, sliding tile puzzles Rubik's Cube,simple rst-in rst-out queue suces (Korf, 1993ff Taylor & Korf, 1993ff Korf, 1997). Likewise,linear space versions, IDA* RBFS (which based depth-rst search), assumptiontakes constant time per node valid. Also assume number neighborsbounded.633fiFelner, Stern, Ben-Yair, Kraus, & NetanyahuExample 2: Computer systems connected networks on- o-linedierent times, throughput seriously degraded due busy communication channels. Therefore, many networks cannot represented xed, fullyknown graphs. Transferring large amounts data (e.g., multimedia les)two computers network often time consuming, since data mayrouted many communication channels computer systems reachingdestination. Finding optimal path computer systems couldimprove transfer time large les. Since network may fully known,nding optimal path two nodes requires exploration network.ecient elegant solution might send small packets (operating scouts)explore network return optimal path, given network stableleast short period time. Assuming computer system networkrecognized neighboring systems, faced problem ndingoptimal path real physical environment.2general, would worthwhile search optimal path followingconditions hold:Preliminary search (with usage scouts) possible cheap.optimal path required future usage.Often one might settle suboptimal path. However, path neededconsiderable trac volume, e.g., path traveled large number timespath traveled simultaneously large number agents, ndingoptimal path essential. paper focus solving problem.paper organized follows. Section 2 provides speci c formulationproblem question. Section 3 discusses related work, Section 4 presentsPHA* algorithm single mobile agent. Several (enhanced) variations introduceddiscussed domain, followed extensive empirical results demonstratesuperiority enhanced variants pursued. Section 5 provide analysisPHA* overall evaluation performance. Section 6, provide numbergeneralizations multi-agent case, number traveling agents availablesolving problem. Experimental results schemes presented discussed.Section 7 contains concluding remarks discusses future research. preliminary versionpaper appeared earlier (Felner, Stern, & Kraus, 2002).2. research concerned high-level, abstract graphs intend provide newapplicable routing algorithm. Current routing technologies maintain large databases store bestpaths node node, broadcast changes network, update paths necessary, thusmaking essentially network graph fully known. Also, network domains one createdestroy packages thus necessarily given number agents. algorithmmay relevant future network architectures routing technologies, routers usedatabases. far-fetched view, example, rapid growth Internet.thus conceivable future storing paths would become infeasible.634fiPHA*: Finding Shortest Path A* Unknown Physical Environment2. Problem Specicationmentioned general terms, problem nd shortest path twonodes unknown undirected graph. speci cally, assume weighted graph,node represented 2-dimensional coordinate (i.e., location realworld), weight edge Euclidean distance two nodes.input problem consists coordinates initial goal nodes.nodes assumed known advance. agent assumed locatedstart node. task nd shortest path (unknown) graphinitial node goal node future usage. order accomplish that, agentrequired traverse graph explore relevant parts leading desired solution.agent allowed visit nodes travel one node another via existing edges.assume node v visited search agent, neighboringnodes discovered, well edges connecting v. assumptionunreasonable, considering, e.g., (trac) signs road intersection often indicateneighboring destinations lengths corresponding road segments connectlocations. Even without road signs, scouts reach new location,look around, observe neighboring locations, assess distances currentlocation. general, assumption neighboring nodes discovered instantlyfairly common search problems algorithms.3Since goal search nd best path goal, clear { givenadmissible heuristic { agent must expand nodes expanded A*, A*optimally eective (Dechter & Pearl, 1985). Let C length shortest pathinitial node goal node. A* expand nodes, that, f (n) =g(n) + h(n) < C nodes f (n) = C . refer nodes(set mandatory) A* nodes. stated above, agent must visit A* nodesorder nd shortest path. However, may need visit additional nodes.make following fundamental observations respect problem question:First, even set A* nodes known advance, agent may need visitadditional nodes traversing related portions graph.shortest path two A* nodes may include graph nodesbelong A* nodes, i.e., f value greater C . Given A* nodes,nding shortest path visits { confusedshortest path origin node goal node { could consideredsolving traveling salesman problem (TSP) respect set A* nodes.Note TSP solution may include nodes belong A* nodes.Second, agent know A* nodes advance. nodes addedopen list expanded search progresses. Thus agent cannotuse solution TSP, since TSP assumes nodes visited providedinput.3. are, however, domains assumption may hold. domains, node becomesfully known agent reaches physically. work restrictassumption. domains addressed part future work.635fiFelner, Stern, Ben-Yair, Kraus, & Netanyahucases, order A* nodes expanded dierentorder visited according TSP solution. Thus minimalpath traversing A* nodes cannot used.Third, node added open list agent cannot know whetherbelongs A* nodes, since C known search concluded. Considernode n open list, head open list. Supposeagent physically located near node. decide whetherslightly extend path visit node n skip n continue nodehead open list. n turned belong A* nodes, visitingmay prove bene cial. (This n might reach head openlist agent physically located far away it, visiting npoint incur signi cant travel cost.) However, turns nbelong A* nodes, (small) detour visiting proven useless.Intuitively, however, decision never visit would result bad strategy.Thus agent may visit nodes belong A* nodes futureexpected bene ts. actual decision whether visit n dependdistance agent's location n (at time decision)agent's estimate whether n belongs set A* nodes.following sections present PHA* algorithm ecient explorationgraph, order nd shortest path two given nodes single travelingagent, well multiple agents. study dierent heuristics direct agentmake intelligent decision, attempt achieve small overall travel cost.3. Related WorkMuch research devoted guiding mobile agent exploring new unknownenvironments order study map out. work dierent,sense explores merely necessary regions graph order retrieveshortest path two nodes entire graph. literaturearea deals physical mobile robot moves real environment. publishedresearch focuses usually issue assisting robot recognize physical objectsenvironment. refer reader (Bender et al., 1998), contains extensivesurvey various related approaches state art techniques.Another class algorithms navigation algorithms. navigation problem concernednavigating mobile agent goal fast possible, necessarily via shortest(optimal) path. navigator always proceed towards goal, ignoring whethertrail traversed thus far lies shortest path. Deviations optimal pathneglected since navigation problem reconsidered every move respectnew source node, i.e., current position agent. navigation algorithm haltsmobile agent reaches goal. path passed usually lacks importance usuallyoptimal. problem, hand, nd optimal path goal nodefuture usage. Even agent nds path goal node, search continues636fiPHA*: Finding Shortest Path A* Unknown Physical Environmentshortest path goal found. Next, describe briey work donenavigation partially known graphs.(Cucka et al., 1996) introduced navigation algorithms sensory-based environments automated robots moving room. used depth rst search(DFS)-based navigation algorithms, use heuristic function choosing next nodeagent go to.Real-Time-A* (RTA*) (Korf, 1990) sophisticated version, Learning RealTime-A* (LRTA*), also algorithms nding paths two nodes graph.However, deal large graphs assume constraint timecomputation move retrieve given constant time. Thus limitedsearch performed, node best cost search frontier picked.problem solver moves one step along path node. search continuesnew state problem solver. merit node n (in RTA* LRTA*)f (n) = g(n) + h(n), similarly A*. Unlike A*, though, g(n) actual distance noden current state problem solver, rather original initial state.dierence RTA* LRTA* search terminated, LRTA*also stores heuristic estimation value node visited problem solver. Alsomethod successor nodes chosen dierent two variations. Korf (Korf,1990) proves large number runs, run start node selectedrandom, stored value node visited LRTA* problem solver convergesoptimal distance goal. RTA* LRTA* signi cantly dierentapproach, assume node expanded computer's memorywithout agent physically visit node. (Also, algorithms designedlarge graphs.) Furthermore, RTA* nd optimal path goal. trivialversion LRTA* could used solve problem, e.g., limiting search depthone level, every node visited agent could physically expanded. However,variant competitive approach, perform like simplehill-climbing procedure. addition, order attain optimal path, LRTA*select many start nodes random. relevant case, givenone initial node.MARTA* (Knight, 1993) multi-agent version RTA*. MARTA* every agent runsRTA* independently. Kitamura et al. (Kitamura, Teranishi, & Tatsumi, 1996) modi ed MARTA* using coordination strategies based attraction repulsion.strategies employed tie-breaking situations. using repulsion strategy,idea spread agents, agent intends maximize distanceothers. Again, path provided algorithm optimal also, agentsneed physically visit node order expand it. work inspired algorithmspresented paper, far handling multi-agent mutual decision concerned.Life-long planing A* (LPA*) (Koenig & Likhachev, 2002b) remarkable algorithmgeneralizes A* handle dynamically changing graph. LPA* activated every timegraph changed order nd current shortest path given startgoal nodes. utilizes fact much old data explored previous runsLPA* still valid current run. A* special case LPA* entire graphexplored yet.637fiFelner, Stern, Ben-Yair, Kraus, & NetanyahuD*-lite (Koenig & Likhachev, 2002a) applies LPA* case mobile robot needsnd shortest path unknown environment environment changesdynamically (i.e., edges added deleted times). LPA*, startnode identical runs. D*-lite, however, robot moves along pathcalculates new shortest path current location. D*-lite modi es LPA*old data previous runs eciently used case start node alsochanged according new location robot. D*-Lite actually simpli ed versionprevious algorithm D* Stenz (Stentz, 1994).main dierence algorithms approach they, too, expandnode computer's memory without requiring mobile agent physically visitnode. Indeed, following every move robot D* Lite, changes graphprovided immediately robot need physically visit nodes order gatherrsthand information. task agent, context D*, repeatedlydetermine shortest path current location robot goal locationedge costs graph changes robot moves. D* lite nd pathreturns it. simply navigation algorithm guides agent goal nodebased previous new information terrain.agent operating real world must often choose maximizing expectedutility (according current knowledge \world") learningenvironment, attempt improve future gains. problem known tradeo exploitation exploration reinforcement learning (Kaelbling & Moore,1996). Argamon et al. (Argamon-Engelson, Kraus, & Sina, 1998, 1999) address tradeo exploration exploitation agent moves repeatedly twolocations. propose utility-based on-line exploration algorithm takesaccount cost attempting improve currently best route knownestimate potential bene ts future task repetitions. expected utilityexploration positive, agent takes actions improve route otherwise,continues using known path. authors compare utility-based on-line explorationheuristic backtracking search algorithm exhaustively searches graphstarting perform task, randomized interleaved exploration algorithm.assume agent knows path two nodes, makeassumption.Argamon et al. also suggest larger number times task repeated,merit interleaved exploration diminishes. agent required moveback forth two nodes large number times, need decideon-line whether exploit explore instead, shortest path found soonpossible. Thus good search algorithm may prove useful. respect workcomplements Argamon et al., provides ecient search algorithms situationsoptimal path needed advance. contrast, applying techniques Argamon etal. situations yields poor results demonstrated experiments.Roadmap-A* (Shmoulian & Rimon, 1998) sophisticated single agent navigationalgorithm. chooses navigate node assumed close goal node.algorithm supervised high-level procedure called A" (Pearl & Kim, 1982). Insteadalways selecting best node open list, A" allows search agent chooseset \good nodes". set called focal set. focal set nodes638fiPHA*: Finding Shortest Path A* Unknown Physical Environmentopen list whose f value greater value best node ".focal nodes determined, local search performed navigate agentone nodes, believed close goal node. role high-levelphase prevent navigating agent going wrong direction consideringalso path traveled thus far.Roadmap-A*, " pre-speci ed constant, determines trade-olocal search A*. example, A0 A* A1 local search, choosingiteration node believed close goal node. algorithmhalts goal node reached, thus " > 0 optimal path mightknown. paradigm Roadmap-A* similar ours, sense node knownagent explores it. fact, trivial case " = 0, Roadmap-A*similar approach simple heuristic \shortest-known path" (presentedSubsection 4.1 below). comments basic dierence RoadmapA*PHA* provided Section 5.summary, listed algorithms navigation algorithms, i.e.,necessarily require agent physically visit node order expand it,necessarily return optimal path goal node. Thus inherently solve dierentproblem one pursued paper.4. PHA* Single Agentturn description PHA* algorithm, focusing rst casesingle mobile agent available.Nodes environment divided explored unexplored nodes. Exploringnode means physically visiting node agent, learning locationlocation neighbors. new algorithm PHA* activates essentially A*environment. However, order expand node A*, node must rst exploredagent order obtain relevant data associated (i.e., neighboring nodesincident edges). Throughout discussion paper treat PHA* twolevel algorithm. Although principle PHA* could also viewed one-level algorithm(see discussion Subsection 4.2), nd two-level presentationwell-structured better understood conceptually. two-level framework consistshigh-level low-level routine. high level (which invokes low level variousstages PHA*), acts essentially like regular A* search algorithm. choosescycle node open list expansion. heuristic function h(n) usedEuclidean distance n goal node. (This heuristic admissible course,de nition.) node chosen high level explored agent,low level, navigation algorithm, activated navigate agent nodeexplore it. node explored low level expandable highlevel. chosen node already explored, neighbors already known,readily expandable high level without need send agent visitnode. pseudo-code high level given below.639fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu.......ghigh-level(open-list) f(open-list empty) ftarget = best node open-listtarget unexploredfexplore(target) low levelggexpand(target)4.1 Low-Level Algorithmshigh-level algorithm, A*, chooses expand node smallest f valueopen list, regardless whether agent already visited node. chosennode visited agent, low level instructs agent visit node.call node target node low level. order reach target node,must use navigation algorithm. implemented number navigation variantslow level. rst describe simple algorithms use known informationgraph. present ecient algorithms, also explore graphnavigation provide new information high level. assumeagent current node needs navigate target node.4.1.1 Simple Navigation AlgorithmsTree path: Like every best- rst search, A* spans nodes generatestree called search tree. Every known node node search tree.trivial way move one node search tree.tree-path algorithm instructs agent move current node targetnode shortest path search tree. words,agent walk tree current node reaches ancestortarget node, walk node target node. trivialalgorithm, presented mainly comparison purposes.Shortest known path: nodes search tree alreadyexplored agent, incident edges known. search treenodes plus additional edges explored nodes viewed subgraphfully known. nodes subgraph connectedpart search tree. Using subgraph, calculate shortest pathtarget node via known nodes edges. mentioned above, nding shortestpath known graph done easily, agent simply computes shortestpath target node travels along path.4Aerial path: Assuming agent able move freely environmentrestricted edges graph, simply move agent4. navigation algorithm similar local A* search Roadmap-A* trivial case" = 0. Roadmap-A*, shortest path target node determined known graphagent moves along path.640fiPHA*: Finding Shortest Path A* Unknown Physical Environmentcurrent node target node via straight line connecting nodes.method may relevant search agents highly mobile, exploreenvironment agents restricted travel along edges. Notelength due \aerial path" never greater length due \shortestknown path".4.1.2 DFS-Based Navigation Algorithmssimple navigation algorithms described above, exploration new nodes donehigh-level algorithm. Thus low level add new knowledgegraph, sense inecient. propose intelligent navigationapproaches nding path target pass also trough unexplored nodes.approaches provide following advantages: paths currently knownagent may much longer paths explored yet. mayprove ecient navigate unknown parts graph seem leadbetter path target. important advantage navigatingunknown parts graph, agent might visit new nodes exploredexplore y. may save need travel back nodeslater time, selected expansion high-level algorithm.advantages suggest use DFS-based navigation low level.DFS-based navigation algorithm, search agent moves neighboring node,visited, typical DFS manner. algorithm backtracks upon reaching deadend search continues reaches target. one neighbor,use heuristic evaluate neighbor likely lead faster target,visit node rst. experimented following DFS-based navigationalgorithms proposed (Cucka et al., 1996):Positional DFS (P-DFS): DFS-based navigation algorithm sorts neighborsaccording Euclidean distance target node, choosing nodeminimum distance target node rst.Directional DFS (D-DFS): DFS-based navigation algorithm sorts neigh-bors according direction edges current node v.rst chooses node u dierence angle line segments(v u) (v t) smallest, denotes target node. words,nodes prioritized directional dierence target node,giving priority nodes dier least.A*DFS: A*DFS improved version P-DFS. step agent choosesneighbor w minimizes sum distances current node vw w target node t. call A*DFS since uses cost functionsimilar A*, i.e., f (n) = g(n) + h(n).5 Note, however, costfunction used locally nd path current node target node.5. generalized version navigating cost function similar A* called \robotic A*" (RA*),also proposed (Cucka et al., 1996)ff node w either neighbor (of v) already visitednode.641fiFelner, Stern, Ben-Yair, Kraus, & Netanyahudierent high-level A* uses cost function nd pathinput initial state input goal state.R12C0000000000T11111111110000000000111111111111111111111000000000000000000000111111111100000000000111111111110000000000D1111111111000000000001111111111100000000000111111111110000000000011111111111PFigure 1: Illustration various low-level navigation algorithms.Figure 1 illustrates navigation algorithms listed above. Let R denote sourcenode, suppose search agent currently node C , high-levelprocedure chooses expand node . squared nodes already visitedagent, i.e., already explored. nodes edges connectingcomprise tree spanned high-level A* search. Since yet explored,low-level procedure navigate target node . tree path navigatealong path C ; 1 ; R ; 2 ; , whereas shortest known path navigate alongpath C ; 1 ; 2 ; . Note since node yet explored, path C viaknown point. aerial path go directly C . Using oneDFS-based navigations, agent move via P , D, depending, respectively,whether P-DFS, D-DFS, A*DFS used. bene DFS-based algorithmsexplore new nodes navigation (nodes P , D,example), revisit nodes, high-level procedure expandlater stage.4.2 Enhanced PHA*4.2.1 PHA* One-Level Procedurementioned previous subsection, PHA* presented principle one-levelalgorithm. done follows. Whenever best node open list known(i.e., explored), expansion cycle A* takes place background,new best node determined. Upon arriving node, agent makes navigationdecision follows:best node open list one current node's neighbors, agentmoves node.Otherwise, agent moves neighboring node minimizes relevantheuristic function (among variants proposed previous subsection).642fiPHA*: Finding Shortest Path A* Unknown Physical Environmentheuristics would valid heuristic function one-levelalgorithm. (The latter confused heuristic function associatedA* expansion cycle.) example, agent node v, using A*DFSvisit neighbor w minimizes sum distances current node vw w best current node open list.compact one-level presentation notwithstanding, prefer { reasons clarity{ use two-level formulation PHA*. believe clear distinctionhigh-level A* low-level navigation procedure provides overall frameworkwell-structured conceptually clearly understood. addition, twolevel framework lends naturally two enhancements presented followingsubsections.enhancements draw basic principle navigation might proceednecessarily best node, dierent node fairly close currentlocation agent. (The idea long run would prove bene cial.)principle realized two main scenarios: (1) navigating best node,agent might choose rst visit nearby neighbor, (2) procedure might chooseignore best node open list select instead dierent node open listclose agent's location. context two-level framework,rst scenario corresponds low-level enhancement (see I-A*DFS below), secondscenario corresponds high-level enhancement (see WinA*, subsection 4.2.3).reasons, choose stick proposed two-level approachPHA*.4.2.2 Improved Low Level: I-A*DFSDFS-based navigation algorithms explore new nodes traverse graph, therebyavoiding future navigations nodes selected later expansion highlevel. bene cial, seen experimental results nextsubsection, take approach much further.Suppose agent navigating target node. Along way, may pass nearnodes small f value without visiting them, pathtarget node according navigation algorithm. counter-productive, since nodessmall f values likely chosen expansion high level near future.Visiting nodes agent nearby, may save lot traveling eort future.order motivate agent visit nodes, want identify arti ciallydecrease cost value (without changing value nodes).incorporate notion, introduce Improved A*DFS (I-A*DFS) variant.basic concept navigating target, low level select next nodevisit considering approximate distance target also node's fvalue. way target, I-A*DFS tend visit, one hand, nodessmall f value, avoid visiting, hand, nodes completely track.Let n denote, respectively, target node neighboring nodecurrently evaluated. Also, let f (:) denote f value node provided high-levelA*, let c1 , c2 denote constants speci ed. used following heuristic functionselecting next node I-A*DFS:643fiFelner, Stern, Ben-Yair, Kraus, & Netanyahuc2(DFS(n)1 ; c1 ff((Tn))n 2 OPENh(n) =(1)DFS(n)otherwise:neighbor n open list, h(n) value due A*DFS remains intact. If,however, neighboring node open list, I-A*DFS considers also goodnessf value. node's h(n) adjusted according product term decreasesnode's f value (i.e., node small f value assigned smaller heuristic)6 .Speci cally, goodness f measured ratio f (T )=f (n). target nodesmallest f value among nodes open list (for otherwise wouldselected expansion high level) therefore 0 < f (T )=f (n) < 1. f (T )=f (n)close 1, f (n) close f (T ). case, highly probable node nvisited A* next steps. Thus want assign higher priority nodevisited agent, decreasing heuristic value. If, however, f (n) >> f (T )(i.e., f (T )=f (n) ! 0), highly unlikely node n selected anytime soonhigh level A*. interest raise node's priority, case,A*DFS heuristic retained, like nodes open list.expression provided (1) meets requirements. f (n) f (T ),term 1 ; f (T )=f (n) becomes small, overall h value node decreases.provides agent option visit nodes open listsmall f values, even though A*DFS heuristic best. If,hand, f (n) >> f (T ), term 1 ; f (T )=f (n) approach 1, negligible eecth(n). main reason multiplying A*DFS heuristic 1 ; f (T )=f (n) (andf (n)=f (T ), example) leave intact cost value node relatively largef value, continue compete (in local heuristic sense) nodesopen list. free parameters, c1 c2 , aect qualitativelyperformance I-A*DFS, merely add module's overall exibility.experimented various constants c1 c2 , attempt determineoptimal performance. extensive empirical studies shown c1 = 0:25c2 = 2:5 produced best performance. experiments also demonstratedusing I-A*DFS yielded better results obtained navigation algorithmslisted Subsection 4.1.2.Figure 2 illustrates dierence A*DFS I-A*DFS. numeric valuesnodes indicate order expanded A*. Suppose agentcurrently located node C node 1 target. A*DFS navigatetarget via node 5, since node best f (= g + h) value scenario described.node 1, agent travel back side graph, node 2selected (by high level) expanded next. agent go back node 3eventually reach goal via node 4. I-A*DFS, hand, navigateC node 1 via node 2 although node 2 assumed shortest pathnode 1, smaller f value node 5. Thus I-A*DFS chooses visit node 2 rst.Incorporating principle saves considerable amount travel cost. agentlocated node 1 next node expanded node 2, high level6. Since A*DFS(.) f (:) measure distances graph, represent, essentially, scale.Thus combined directly.644fiPHA*: Finding Shortest Path A* Unknown Physical EnvironmentR51C324A*DFSI-A*DFSGFigure 2: example A*DFS versus I-A*DFS navigation.expand immediately, explored agent, thusreadily available. Thus agent travel directly node 1 node 3 avoidnavigating back forth opposite sides graph.4.2.3 Improved High-Level: WinA*A* expands nodes open list best- rst order according f value.order optimal complexity expanding node O(1). However, realphysical environment, node expansion requires agent perform costly tasks,always ecient expand current best node. Consider, example, nearby nodebest node open list, whose f value suciently small,high probability would selected expansion A* next iterations.intelligent agent choose explore node rst, even though currentlybest node open list.R485679GFigure 3: example illustrating disadvantage A*.principle illustrated, example, subgraph Figure 3contains two node clusters. numeric label node associated f value.agent visiting nodes best- rst order (i.e., order A* expands them),travel back forth one cluster other. much better approach645fiFelner, Stern, Ben-Yair, Kraus, & Netanyahuwould explore nodes one cluster move cluster, therebytraveling one cluster other.order incorporate capability algorithm, generalized A*call Window A* (WinA*). A* chooses expand node lowest f value,WinA* creates set (i.e., window) k nodes smallest f values choosesone node set expansion7 . window uses principle A" (Pearl& Kim, 1982) mentioned before. constructing window selectnode expansion. objective minimize traveling eort agent,reduce, necessarily, number expanded nodes. Thus rather selectingnodes small f value, choose also nodes suciently closelocation agent. experimented large number combinations,concluded best way capturing two aspects simply takingproduct. Thus order nodes window cost functionc(n) = f (n) dist(curr n)n node evaluated, f (n) f value, dist(curr n) distance ncurrent location agent. choose expand node smallest cost c.(It sensible combine f (n) dist(curr n) manner, expresseddistance units.) Note node small f value chosenexpansion, f value relative nodes open list tend decreasetime. f value newly generated nodes monotonically increasing,heuristic used consistent admissible. property reduces chancestarvation. (At least encountered phenomenon experiments.)intention demonstrate combining two factors, mannerfavors nearby nodes small f value, indeed yields enhanced performance.tried many functions combine two factors (e.g. weighted sum) choosepaper discuss product, c(n) = f (n) dist(curr n), since provided bestresults.Combining modi ed high-level variant low-level navigation createstechnical diculties, due fact longer expand nodes open listbest- rst order. Recall standard A* expands node generating neighborsputting node closed list. node v closed list, shortestpath source node v known. Hence, goal expanded foundshortest path it, search terminate. However, WinA* node mayexpanded although exists another node smaller f valueexpanded yet. words, node v expanded, necessarily implybest path v found. Expanding node smaller f value mightdiscover better path. Thus search cannot simply terminate goal nodechosen expansion.problem solved splitting standard node expansion stage two phases:7. related algorithm derived, k-best rst search (KBFS) (Felner, Kraus, & Korf, 2003),window size k determined open list, window nodes expandedstage. neighbors nodes generated added open list,new iteration begins.646fiPHA*: Finding Shortest Path A* Unknown Physical Environment1. Node expansion. Expanding node means visiting node, generatingneighbors, adding open list. stage takes place immediatelynode chosen high level.2. Node closing. Closing node means removing open list puttingclosed list. takes place nodes smaller f valueexplored. ensures, essentially, node placed closedlist best path source node found (See Section 5comments). Thus search continue, even goal nodeexpanded, placed closed list. goal node placedclosed list, search terminate.Following pseudo-code WinA*. Note standard expansion dividedaccording two phases. end cycle, algorithm attemptsclose many nodes possible.WinA*() f.(goal closed-list) f.target = node window minimizes (node) dist(current node).target unexplored.explore(target) low level.expand(target).(best node (with minimal f value) open-list expanded).close(best node).gfg4.3 Experimental ResultsFigure 4: 20-node Delaunay graph.experimented Delaunay graphs (Okabe, Boots, & Sugihara, 1992),derived Delaunay triangulations. latter computed set planar pointpatterns, generated Poisson point process (Okabe et al., 1992). Points distributed647fiFelner, Stern, Ben-Yair, Kraus, & Netanyahurandom unit square, using uniform probability density function. Delaunaytriangulation planar point pattern constructed creating line segmentpair points (u v), exists circle passing u v enclosespoint. triangulation characterized, sense, onepoint joined line segment nearest neighbors points.(We refer type Delaunay graphs regular Delaunay graphs.) usedQhull software package (Barber, Dobkin, & Huhdanpaa, 1993) construct Delaunaytriangulations (i.e., Delaunay graphs) sets points generated randomunit square. Figure 4 illustrates 20-node Delaunay graph.principle, characteristic whereby node connected neighbors seemssuitable representing real road maps, main object research.practice, however, additional characteristics accommodated capture adequately real road map. Thus also pursued sparse dense Delaunay graphsobtained regular Delaunay graphs random deletion additionedges, respectively. (See Appendix detailed discussion.)4.3.1 Low Level Experimental Results90Tree pathShrtest known pathAerial pathP-DFSD-DFSA*DFSI-A*DFS8070Search cost605040302010050010001500200025003000Number nodes graph35004000Figure 5: Search cost versus number nodes regular Delaunay graphs variouslow-level algorithms.Figure 5 displays traveling distance (or search cost) agent functionnumber nodes Delaunay graph (i.e., 500, 1000, 2000, 4000 nodes).graphs depicted correspond various low-level algorithms PHA* tested on.Every data point (here experiments) corresponds average 250dierent pairs initial goal nodes, picked random. average optimalpath observed 0.55.8 gure clearly demonstrates higher eciencyinvolved algorithms. particular, I-A*DFS consistently superioralgorithms graph sizes. graph size 4000, example, outperformed8. Note closeness average optimal path observed (i.e., 0.55) expected arc lengthrandom graph dened set points (i.e., 0.521) (Ghosh, 1951).648fiPHA*: Finding Shortest Path A* Unknown Physical Environmentsimple algorithm factor 10, outperformed basic A*DFSfactor 2. Note search cost increases number nodes grows,i.e., domain becomes denser connected. attributed factnumber nodes grows, number nodes closed list I-A*DFSprocedure visit.relative performance various algorithms considered remainedsparse dense Delaunay graphs (see Appendix A).4.3.2 Experimental Results WinA*6500 nodes1000 nodes2000 nodes5.55Search cost4.543.532.521.501020304050Window size607080Figure 6: Search cost WinA* versus window size various sizes regular Delaunaygraphs.experiments show using WinA* high-level procedure PHA* leadssigni cant improvement eciency algorithm. Figure 6 presents averagedistance traveled search agent optimal path found, functionwindow size. I-A*DFS employed low-level algorithm. results shownFigure 6 indicate using window size larger 1 (which corresponds standardA*) signi cantly improves algorithm's performance various graph sizesexperimented with. Also, found optimal size window tendsvary size graph. Based empirical observations, setting optimalwindow size (1=50) times number nodes graph seemed providegood approximation. (For example, best window sizes observed 500- 2000-nodegraphs 10 40, respectively.) Note window size becomes larger (i.e.,number candidate nodes increases), algorithm tends select nodes largef value, results performance degradation. Additional results sparse denseDelaunay graphs presented Appendix A.rst glance, improvement WinA* standard A* (for high level) seemssomewhat modest, exceed 30%. due fact I-A*DFS exploresmany nearby nodes, already powerful begin with. WinA* I-A*DFSdesigned assign high priority nearby nodes. dierent stagesPHA* algorithm, sense \compete" type improvement.649fiFelner, Stern, Ben-Yair, Kraus, & NetanyahuIndeed, using navigating algorithms, improvement WinA* relativestandard A* much signi cant. However, dealing real physical agents |let alone humans | even 30%-time reduction WinA* (relative I-A*DFS)viewed signi cant. Similar results obtained sparse dense Delaunay graphs(see Appendix A).5. Analysis PHA*Analyzing performance PHA*, distinguish following three parameters:(1) Cost returned path, (2) shortest possible path agent travel, (3) costactual path traveled agent. Subsection 5.1 argue path reportedPHA* (for future use) optimal. addition, present Subsection 5.2 extensiveempirical study compares (2) (3). Finally, provide Subsection 5.3brief discussion PHA*'s underlying methodology overall performance.5.1 Optimality SolutionRecall A* expands nodes best- rst order according f value. heuristicfunction, h(n), admissible, f (n) = g(n)+ h(n) lower bound path goalvia node n. well-known, paradigm, goal node selectedexpansion, A* found optimal path (Hart et al., 1968 Karp & Pearl, 1983 Dechter& Pearl, 1985). Put dierently, upon goal expansion f (goal) = c, nodesestimated paths f (n) < c already expanded length optimalpath goal c (Karp & Pearl, 1983 Dechter & Pearl, 1985).PHA* supervised high level, activates admissible A*. (Recallh(n) Euclidean distance n goal, i.e., admissible.) designalgorithm, high level terminates goal node selected expansion. Thusproperties admissible A*, nodes smaller f value must alreadyexpanded, f value goal optimal. Note also holds enhancedPHA* WinA* (see Subsection 4.2.3). Although WinA* necessarily expandnodes according best f value, designed remove node open listsmallest f value among nodes list. algorithm haltsgoal node expanded removed open list, implying f valuesmallest list. Thus enhanced PHA* variant also compatibleadmissible A* paradigm, path returns optimal. basic theoretical resultpaper follows.Theorem: PHA* enhanced versions return optimal path startnode goal.5.2 Performance Evaluation PHA*demonstrated above, complex algorithmic schemes provided dramatic improvement search time. interest assess, least extent, performancebest navigation variant, i.e., WinA* (for high level) conjunction I-A*DFS(for low level).650fiPHA*: Finding Shortest Path A* Unknown Physical Environmentgraphsize305075100150200250300closednodes11.3215.4517.9320.3224.1228.4331.5735.78jTSP j00.620.740.770.850.910.991.021.05PHA* ratio0.800.940.971.101.271.421.481.511.291.271.221.291.391.431.451.44Table 1: Comparison shortest paths nodes closed list actual pathsobtained PHA*.agent's task visit essentially nodes expanded A*.nodes comprise set nodes closed list algorithm terminates.general, invoking A* subgraph induced nodes, sourcegoal states heuristic function, exhibit behavior yieldopen closed lists. Thus given static graph, set nodes A* visitxed. Ideally, would like agent visit set closed nodes along shortestpossible path. course infeasible, since nodes known advance,rather determined y. However, order evaluate algorithm's performance,compare output shortest possible path travelsnodes. computation latter carried o-line, i.e., set (closed)nodes known.Speci cally, computed shortest possible path case respectcomplete graph corresponding set closed nodes. weight w(ni nj ) associatededge (ni nj ) (in complete graph) set length shortest pathni nj (in original Delaunay graph instance). Finding shortest pathtravels via given set nodes known traveling salesman problem (TSP),notorious exponential running time. conventional TSP path travelsnodes returns start node. However, interested pathtravels nodes without returning start node. denote pathTSP distinguish conventional TSP tour. TSP tour actually TSP tourwithout last edge. view exponential nature problem, usedsimple branch-and-bound tree search compute desired paths. However, solvingproblem optimally feasible relatively small graph sizes.Table 1 provides comparison PHA* shortest path travelsclosed nodes various small sized graphs. table indicates PHA*algorithm quite ecient small graphs. Speci cally, average travel cost (duePHA*) greater shortest possible path (passing closed00651fiFelner, Stern, Ben-Yair, Kraus, & Netanyahunodes) 45%. graphs 200 nodes less, number closednodes observed smaller 30. average cost cases computed 50random instances. graphs sizes greater 200, average cost computed5 instances only.order evaluate, however, performance PHA* graphs larger size (whereoptimal path could computed reasonable amount time), employedlower-bound approximation cost TSP . Speci cally, computed minimumspanning tree (MST) complete graph (de ned set closed nodes). Let jTSP jjMSTj denote, respectively, costs associated desired path minimumspanning tree.00Claim:0:5 jTSP j < jMSTj jTSP j:00Proof: claim follows basic graph theory (Cormen, Leiserson, Rivest, & Stein,2001). Speci cally, inequality right hand side stems fact TSPspanning tree complete graph. Thus cost minimum spanning tree mustsmaller (or equal to) jTSP j.prove inequality left hand side, note triangular inequalityholds respect de ned complete graph. (That is, three nodes, ni ,nj , nk , w(nj nk ) w(ni nj ) + w(nj nk ).) easily shown, based facttriangular inequality holds respect original Delaunay graphsde nition edge weight complete graph. Thus construct tour goestwice around MST use triangular inequality shortcut edges.Hence2 jMSTj jTSPj > jTSP j000inequality left hand side follows. 2Given infeasible computation jTSP j, claim suggests jMSTj, instead,reasonably good approximation. Speci cally, inequality right hand side impliestravel cost agent performing PHA* is, say, c jMSTj, travel costPHA* greater c jTSP j. Given merely lower bound, PHA*expected perform better practice.Table 2 provides comparison PHA* MST lower bound shortestpath described above. average cost entered graph size computed250 randomly generated instances. table indicates that, average, costPHA* 2.74 times best possible path graph sizes 8000 nodescorresponding sets closed nodes 460 nodes.005.3 Discussionrepeatedly noted, algorithm returns optimal solution must expandleast nodes expanded A*. Drawing basic premise, PHA*algorithm designed visit set \mandatory" nodes eciently possible.rationale visiting also nearby nodes (whose f value necessarily smallest)nodes likely expanded next iterations. contrast,652fiPHA*: Finding Shortest Path A* Unknown Physical Environmentgraphsize4005001000200040008000closednodes40.2743.0062.72131.56233.26460.66jMSTjapprox.1.051.151.422.012.523.45PHA* ratio1.911.973.034.896.769.441.821.872.132.432.692.74Table 2: Comparison lower bounds shortest paths nodes closed listactual paths obtained PHA*.bene enhanced variation context navigation algorithmpresume return optimal solution.Reconsider Roadmap-A*, example. A" activated prevent local navigation phase going wrong direction. However, since algorithm designedreturn optimal solution, deviate stage promising routevisit nearby node may expanded later on. Put dierently, notionset mandatory nodes agent visit. Furthermore, soon agentreaches goal, search halts. conclusion, although PHA* Roadmap-A*two-level navigation schemes, objectives dierent solve essentiallydierent problems.Based properties admissible A* design algorithm,argued (enhanced) PHA* returns path (for future use) optimal. addition,absence theoretically known bound actual cost PHA*, runextensive empirical study, comparing observed costs best possible costscomputed o-line. Given agent lacks priori information set mandatorynodes, highly unlikely exists on-line PHA*-like algorithm performseciently o-line version. extensive empirical study demonstrates, nevertheless,actual cost associated PHA* order magnitude optimalcost computed o-line.6. MAPHA*: Multi-Agent PHA*section generalize techniques discussed previous sections multiagent case, number agents cooperate order nd shortest path. callresulting algorithm Multi-Agent Physical A* (MAPHA*).would like divide traveling eort agents ecient waypossible. measure eciency multi-agent case using two dierent criteria.rst overall global time needed solve problem. second totalamount fuel consumed agents search. requirementminimize cost moving agents time important, considering fuel653fiFelner, Stern, Ben-Yair, Kraus, & Netanyahucost mobilizing agents cost function choice. case, may wisemove agents agents remain idle. However, task nd bestpath goal, soon possible, idle agents seem wasteful, better utilizetime exploration graph. case, available agentsmoving times. introduce two algorithms two perspectives, namelyfuel-ecient algorithm time-ecient algorithm. Note single agent casetwo criteria coincide.assume agent communicate freely agents sharedata time. Thus information gathered one agent available knownagents. framework obtained using model centralizedsupervisor moves agents according complete knowledge gatheredthem. reasonable assumption since many cases dispatchercentralized controller gathers information agents instructsaccordingly. Another possible model complete knowledge-sharing agentbroadcasts new data graph agents. Future research mayaddress restrictive communication model, limiting communication rangeinducing communication errors.also assume search terminates, soon goal node expandedmoved closed list. objective minimize travel eort point,care moving agents pre-speci ed location (e.g.,goal vertex start node), desired shortest path identi ed. conventionaccordance many algorithms neglect report time spent \reset"system (e.g., garbage collection), desired solution arrived at.main idea MAPHA* algorithm similar PHA* singleagent. use two-level framework. high level chooses nodes expand,low level navigates agents nodes. studied multi-agentcase enhanced techniques only, i.e., WinA* high level I-A*DFSlow level. problem deal assign dierent agentsexplore eciently dierent nodes.6.1 MAPHA*: Fuel-E cient Algorithmsimplicity, assume amount fuel consumed agent equaltraveling distance search. Since purpose algorithm caseminimize amount fuel consumed agents, regardless overall search time,bene moving one agent time. movingone agent, agent might gain new knowledge graph would allowagents make informed intelligent moves.beginning, agents situated source node. Then, casesingle agent, high level de nes window unexplored nodes open listpotential candidates expansion. pair (a n), agent nnode window, compute allocation cost functionc(a n) = f (n) dist(a n)f (n) f value node n dist(a n) denotes distance locationagent node n. select agent target node minimize allocation654fiPHA*: Finding Shortest Path A* Unknown Physical Environmentfunction. case tie-breaking (e.g., beginning search agentslocated initial state), pick randomly one agent relevant candidates.stage, low-level algorithm navigates selected agent target node selectedwindow order explore node. single-agent case, additionalknowledge graph obtained navigation many unexplorednodes visited traveling agent. selected agent reaches targetnew cycle activated high- low-level procedures.9 Following pseudo-codefuel ecient algorithm.fuel-efficient algorithm() f.(goal closed-list) f.agent.select node window minimizes.best = agent minimizes ( i) dist( ).best unexplored.explore( best ) low level using best.expand( best ).(best node open-list expanded).close(best node).gnnnnfnnf (n) dist(ai n)g6.2 MAPHA*: Time-E cient Algorithmtime-ecient algorithm similar described fuel-ecient algorithmone basic modi cation. Instead moving one agent high-level cycle,move available agents since care time spentagents fuel consumption. idle agent save time.Every moving agent help gather knowledge environmentadditional cost, clock ticks away regardless time measured globally.cannot use allocation function used fuel-ecientalgorithm, agents located initially node, fuel-ecient allocationfunction choose node agents. main idea time-ecientstrategy agents move simultaneously. Thus ensure ecient performanceneed distribute much possible. Suppose p available agents knodes window. would like distribute p agents k nodes ecientlypossible. brute-force approach randomly distribute agents nodes.However, provide eective distribution, incorporate following three criteriadistribution formula time-ecient procedure:1. Since f values neighboring nodes somewhat correlated other,nodes small f value likely generate new nodes small f9. also implemented complex variant, whenever new unexplored node reached,new high-level cycle activated. Results obtained signicantly dierent, omitdetails variant simplicity. See (Stern, 2001) comprehensive description.655fiFelner, Stern, Ben-Yair, Kraus, & Netanyahuvalues nodes large f value. Therefore, distribution favorassigning agent node small f value.2. Another attribute taken consideration distance targetnode agent. would like assign agent one nodesmanner, expected travel distance agent (for assignment)minimized. words, agent assigned, preferably, relatively closeby node.3. order expand entire window prevent \starvation", would also likedistribution function raise priority nodes assigned small numberagents. Thus keep track number agents assignednode give preference nodes small number assignments.Note rst third criteria may contradict, i.e, rst criterion prefernodes small f value third criterion favor nodes large f value,small number agents assigned them.found taking product values associated three criteriagives good distribution function suitable load balancing criteria.Speci cally, agent allocation procedure iterates agents picks,agent, node minimizes following allocation function:alloc(agent node) = f (node) dist(agent node) (count(node) + 1)dist(node agent) Euclidean distance node agent, f (node)node's f value, count(node) counter keeps track number agentsalready assigned explore node. count(node) initially set 0incremented every time agent assigned node. Thus load balancingthree factors kept throughout distribution process. beginningsearch agents located start node, initial allocationdierent nodes determined, essentially, count factor. (Without factor,product f (n) dist(agent n) would returned node n agents.)search progresses, agents move dierent locations get assigned stepnodes closer location small f value. Thus productthree factors creates good distribution (of agents) dierent partsgraph.Consider, example, case illustrated Figure 7. Suppose 100 agentslocated node x, window consists three nodes a, b, c locatedequal distance x. Suppose also f (a) = 2, f (b) = 4 and, f (c) = 8.numbers agents assigned nodes, using allocation procedure,57, 29, 17, respectively. good balance various requirements.tried many variations distribution procedure foundperformed well long three requirements met. See (Stern, 2001)discussion agent distribution.before, agent navigates assigned target using enhanced low-level algorithm, I-A*DFS. Another high-level iteration begins soon rst agent reaches656fiPHA*: Finding Shortest Path A* Unknown Physical Environmentxbf(a)=2cf(c)=8f(b)=4Figure 7: example agent distribution according proposed allocation procedure.target node.10 Note computation time window agentdistribution/allocation neglected, since care travel timeagents. Following pseudo code time-ecient algorithm.time-efficient algorithm() f.(goal closed-list) f.free agent.select window node minimizes dist(.move agents agent reaches node.expand nodes currently visited agent.(best node open-list expanded).close(best node).gnn) f (n) (count(n +1))g6.3 Experimental Resultsexperiments performed multi-agent case also conducted Delaunaygraphs 500, 1000, 2000, 4000, 8000 nodes. Additional results sparsedense Delaunay graphs provided Appendix A.6.3.1 MAPHA*: Results Fuel-Efficient Algorithmprovide results fuel-ecient algorithm Subsection 6.1. fuel consumption reported total fuel consumed agents. (As before, graphsgenerated unit square, average optimal path observed 0.55.)Figure 8 presents costs fuel-ecient algorithm function numberagents various sizes regular Delaunay graphs. (Results sparse graphs, wellgraphs edges added random, presented Appendix A.) gure clearly10. observed new iteration begins, almost every agent assigned nodeassigned previous iteration. Typically agent's location becomescloser \its" target node, criteria change. Thus practice, agentsgo complete (original) tasks, agent reached target assigned newgoal node. See (Stern, 2001) detailed discussion.657fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu6.5500 nodes1000 nodes2000 nodes4000 nodes65.5Fuel consumption54.543.532.521.5123456Number agents789Figure 8: Fuel consumption function number agents various sizes regularDelaunay graphs.demonstrates agents added, overall fuel consumption decreasespoint adding agents tends increase overall consumption. Thusoptimal number agents exists graphs. phenomenon due factA* usually characterized small number search regions. Therefore, smallnumber agents suces cover regions. number agents increases, fuelconsumption goes up. phenomenon explained follows. large number agentsincreases likelihood nearby agent assigned speci c node,case relatively little exploration graph takes place. Assigning, hand,distant agent node would result larger degree graph exploration,essential, long run, ecient navigation (especially I-A*DFS employed). Thuslarge number agents navigating small graph (which search regions), wouldresult excessive fuel consumption. See (Stern, 2001) detailed explanationphenomenon.optimal number agents increases number nodes graph increases.optimal number agents graph 500 nodes 2, number increases7 graph size 4000. stems fact larger graphs searchregions thus agents needed explore them.described before, one agent allowed move experiment, pointtime. measured total amount fuel consumedagents. interest nd whether work uniformly distributed amongagents, whether large portion work carried small number agents.Table 3 presents distribution work among agents 14 agentsactive Delaunay graphs size 8000. graph instance, sorted agentsdecreasing order fuel consumption. table shows relative fuel consumptionagents 3, 7, 14 activated agents.general, remark overall work uniformly distributed, quitebalanced. example, 14 agents activated, 40% work done 4658fiPHA*: Finding Shortest Path A* Unknown Physical EnvironmentAgent No. 3 agents %] 7 agents %] 14 agents %]142.0328.3416.01233.5419.3213.21324.4316.2311.41412.7610.3159.026.6467.865.4876.485.0884.5494.10103.59113.20123.02132.81142.70Table 3: Work distribution among multiple agents running fuel-ecient algorithmDelaunay graphs size 8000.agents. similar tendency observed graphs sizes, well sparsedense Delaunay graphs (see Appendix A).order improve eciency fuel-ecient algorithm make overallwork distribution balanced, several improvements might suggested. example,currently agents positioned initially source node. might considerrst spread agents number directions invoke algorithm.Notwithstanding additional overhead may incurred spreading agents,technique result balanced work distribution reduced overall fuelconsumption.6.3.2 MAPHA*: Results Time-Efficient Algorithmsubsection report results time-ecient algorithm Subsection 6.2.explained, main objective conclude task fast possible,fuel consumption concern, agents always moving, i.e., noneidle point time. overall search time casemaximal distance either agent travels shortest path goal node found.Figure 9 shows search time obtained time-ecient algorithm functionnumber agents, various regular Delaunay graphs. Note search timenever smaller time takes travel along shortest path goal.results indicate, adding agents always ecient since measure overalltime elapsed goal found. makes algorithm interestingecient fact add agents, search time converges asymptotically659fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu10500 nodes1000 nodes2000 nodes4000 nodes8000 nodes98Search time7654321002468Number agents101214Figure 9: Time consumption function number agents, various regularDelaunay graphs.length shortest path. Recall average length observed shortest pathapproximately 0.55. Indeed, large number agents tend nd optimal pathwithin time frame approaches limit. overall time 2.3single agent, reduced 0.7 14 agents graphs 500 nodes example.Using proposed agent allocation procedure, note asymptotically pathsinitial state traveled breadth- rst search manner. saysuciently large team agents likely produce single agent travel alongactual shortest path little deviation it. Similar results time- ecientalgorithm also obtained types graphs (see Appendix A).6.4 Combined Requirements Fuel Timedistinction time-ecient algorithm fuel-ecient algorithmreasonable, may suitable many practical situations. Practical considerationstime fuel resources may suggest combined approach, one described below.Consider, example, commander operating constraint fuel consumptionrestriction number troops assigned certain task.order complete task fast possible, commander may want use maximalpossible number agents without exceeding fuel consumption limit.essence, seek generalize MAPHA*, agents minimize costfunction combination time fuel consumption. suggest general costfunction takes account requirements measures. objectiveactivate MAPHA*, minimize cost function. Speci cally, suggestfollowing linear combination:Ctotal = wt time + wf fuelwt wf (normalized) weights attached, respectively, time fuelconsumption (i.e., 0:0 wt wf 1:0 wt + wf = 1:0). Ctotal calculated globally, i.e.,660fiPHA*: Finding Shortest Path A* Unknown Physical Environmentmeasure amount time beginning task optimal pathfound, total amount fuel consumed agents. multiplyquantities corresponding weights report combined cost.wt wf prespeci ed user. wt = 0, time costfuel-ecient algorithm appropriate one use. wf = 0, fuel cost,use time-ecient algorithm. Otherwise, neither wt wf 0,use dierent algorithm minimize Ctotal.suggest two algorithms general case.Simple combined algorithm.algorithm actually identical time-ecient algorithm. numberparticipating agents parameter provided user. iterationhigh level participating agents move according allocation functiontime-ecient algorithm. Given formulation total cost, Ctotal, wouldlike determine optimal number agents, wt wf . Notetrivial case wf = 0, adding agents always valuable, sinceconsume resources, reduce time cost. However, wf increases,large number agents may increase total cost.Improved combined algorithm.main limitation simple combined algorithm even though costincurred fuel consumption, agents always moving. improved combined algorithm addresses problem suggests moving agentssimultaneously. Using formalization, rst determine p, i.e., numberagents participate task. Given p, determine m, i.e., number agents actually distributed nodes selected window (byhigh level). remaining p ; agents stay idle. Note simplecombined algorithm p coincide. use mechanism time-ecientallocation function, except algorithm chooses (out p) agentsminimize allocation function. time-ecient algorithm, rstdetermine size window, i.e., number nodes open listexpanded. Then, invoke allocation function. Whereastime-ecient case allocation terminates agents assigned nodes,allocation stops agents selected. selected agents bestagents expansion cycle since minimize allocation function.6.5 Results Combined Algorithmprovide experimental results combined algorithm introducedprevious subsection. results Tables 4 5 obtained Delaunay graphssize 2000 table entry represents average 250 problem instances.column, bold face number smallest total cost corresponding wt =wf ratio.minimal costs determine optimal number agents given wt =wf ratio.Table 4 provides total costs simple combined algorithm functionnumber agents various wt =wf ratios. leftmost column corresponds case661fiFelner, Stern, Ben-Yair, Kraus, & Netanyahuwtwf1.00.0# agents14.8222.5831.7441.4451.2461.1171.0380.9790.93100.89110.85120.84130.84140.820.90.10.80.24.822.842.091.871.731.674.823.102.442.302.231.651.681.701.701.771.841.882.332.422.502.562.702.842.942.221.64 2.260.70.30.60.40.50.50.40.64.82 4.82 4.82 4.823.36 3.61 3.87 4.132.79 3.14 3.49 3.842.73 3.16 3.60 4.032.72 3.22 3.71 4.212.78 3.33 3.89 4.442.88 3.49 4.11 4.733.01 3.69 4.37 5.053.17 3.91 4.66 5.403.31 4.11 4.92 5.723.41 4.26 5.11 5.963.63 4.56 5.49 6.423.84 4.84 5.85 6.854.01 5.07 6.13 7.190.30.70.20.80.10.90.01.04.82 4.82 4.824.39 4.65 4.914.18 4.53 4.884.46 4.89 5.324.70 5.20 5.695.00 5.55 6.115.34 5.96 6.585.73 6.41 7.096.15 6.89 7.646.53 7.33 8.146.81 7.67 8.527.35 8.27 9.207.85 8.85 9.868.26 9.32 10.384.825.165.235.756.196.667.197.778.388.959.3710.1310.8611.44Table 4: Ctotal simple combined algorithm function number agents,various ratio wt =wf ratios.662fiPHA*: Finding Shortest Path A* Unknown Physical Environmenttime matters. Thus entries identical values obtained timeecient algorithm. fuel consumption becomes signi cant, longer bene cialincrease number agents thus optimal number agents decreases.wt = wf = 0:5, Ctotal = 0:5 time + 0:5 fuel, optimal number agents obtainedthree, total cost 3.49. critical fuel consumption becomes,bene cial use smaller number agents. rightmost column correspondsextreme case, wf = 1:0, i.e., fuel consumption matters. Noteentries column dier counterpart costs obtained fuel-ecientalgorithm. dierence stems fact that, context simple combinedalgorithm, picking p agents means moving simultaneously, whereascase fuel-ecient algorithm employed one agent (out p) allowedmove times. Note fuel-ecient algorithm essentially special caseimproved combined algorithm = 1.Table 5 provides total costs improved combined algorithm functionnumber agents various wt =wf ratios. number participating agents p = 14(i.e., 14 available agents could move simultaneously). row correspondsdierent m, i.e., actual number moving agents. (Clearly, 1 p = 14.)before, column bold face number smallest total cost correspondingwt =wf ratio. minimal costs determine optimal number moving agentsgiven wt =wf ratio.top entry rightmost column identical cost obtained fuelecient algorithm, 14 agents. case wf = 1, one agent allowedmove point time. bottom entry leftmost column identical costobtained time-ecient algorithm, 14 agents. case wt = 1,14 participating agents moving times.signi cant fuel consumption becomes, less bene cial move manyagents. Thus optimal number moving agents decreases. example, wt = wf =0:5, optimal number moving agents obtained three, total cost 3.23.fuel consumption becomes crucial, would bene cial move smaller numberparticipating agents.Comparing results simple combined algorithm improvedcombined algorithm reveals wt =wf ratio numbermoving agents (which equal number participating agents simplecombined case) improved combined algorithm usually performs better.pick moving agents larger sample. Also, appears optimal numbermoving agents smaller improved combined algorithm. algorithm,moving agents picked clever manner cycle thus better utilized.Additional experiments conducted graph sizes, well sparsedense Delaunay graphs. results obtained cases rather consistent. Futurework attempt predict advance best number agents.7. Conclusions Future Workaddressed problem nding shortest path goal node unknowngraphs represent physical environments. presented two-level algorithm,663fiFelner, Stern, Ben-Yair, Kraus, & Netanyahuwtwf1.00.0# agents14.0222.2631.6141.3151.1361.0070.9280.8590.82100.79110.77120.76130.75140.750.90.10.80.20.70.34.022.491.941.701.581.491.474.022.722.262.092.034.022.942.582.492.481.99 2.492.02 2.571.45 2.05 2.651.471.501.541.591.641.722.122.212.312.422.542.692.772.923.073.253.443.670.60.40.50.50.40.60.30.74.02 4.02 4.02 4.023.17 3.40 3.62 3.852.91 3.23 3.55 3.882.88 3.27 3.67 4.062.93 3.38 3.83 4.282.99 3.49 3.98 4.483.13 3.68 4.23 4.783.25 3.85 4.44 5.043.43 4.08 4.73 5.393.63 4.34 5.05 5.763.84 4.61 5.38 6.154.09 4.92 5.75 6.584.33 5.23 6.13 7.024.64 5.61 6.59 7.560.20.80.10.90.01.04.02 4.02 4.024.084.204.454.734.985.335.646.046.476.927.417.928.534.30 4.534.52 4.844.84 5.245.18 5.635.48 5.985.88 6.436.24 6.846.69 7.347.18 7.897.69 8.468.25 9.088.82 9.719.51 10.48Table 5: Total costs improved combined algorithm function numbermoving agents (out 14 participating agents), various wt =wf values.664fiPHA*: Finding Shortest Path A* Unknown Physical EnvironmentPHA*, environments single search agent, MAPHA* algorithmmultiple-agents. experimented several variations Delaunay graphs, containing 8000 nodes. enhanced single agent algorithm yielded signi cantly betterresults ones obtained simpler variants. results fuel-ecientalgorithm show using agents bene cial extent.agents initially located source node consume fuelmove make. reason, bene using optimal number agentsopposed one agent modest. results time-ecient algorithmencouraging, since search time converges quickly optimum numbersearch agents increases. also introduced cost function combines timeconsumption fuel consumption, presented two algorithms paradigm.results show combination exists optimal number agentstends increase weight time cost increases.Future work pursued along following directions:assumed upon reaching node, agent learn locationsneighbors. many domains model may valid, locationnode known agent actually visits it. model alsosuggested (Shmoulian & Rimon, 1998). research done orderimplement algorithms, context model.used traveling agents solve shortest path problem. similar mechanism might used solving known graph problems, minimumspanning tree, traveling salesman problem, problem requiresconsideration node visited next.proposed two algorithms combining time consumption fuel consumption. algorithms assume number agents determined priori. Futurework try theoretically determine optimal number agents given constraints. Also, future work done see whether changing numberwould increase eciency algorithm. Also, assumed agentsconsume fuel move, measured total performancetime task. Thus idle agents consume resources. However,think model idle agents consume resources (e.g., time energy).assumed centralized model, agents share knowledgetimes. Future work assume communication paradigms. particular,interested model communicationagents. model known ant-robotics model (Wagner & Bruckstein, 2000Yanovski, Wagner, & Bruckstein, 2001). model, information spreadagents pheromones, i.e., data written agent node. agentsread pheromones reaching nodes. currently workingtowards applying MAPHA* algorithm model. believeincrease size data allowed written node,agent able write complete knowledge node environment.challenge applying A* model lies fact since A* maintainsglobal open list, data opposite sides graph inuence behavior665fiFelner, Stern, Ben-Yair, Kraus, & Netanyahualgorithm. Thus need knowledge sharing system largepossible. purpose, believe new type communication agentsintroduced. Agents type try increase search frontierrather move around environment spread recent data available.Acknowledgmentspreliminary version paper appeared Proceedings First International JointConference Autonomous Agents Multi-Agent Systems, 2002 (Felner et al., 2002).work carried rst author Bar-Ilan University. materialbased upon work supported part NSF grant #0222914 ISF grant#8008.Appendix A. Additional Experimental Resultsmentioned Subsection 4.3, node regular Delaunay graph connectedneighbors. property may always apply real road map. example,nearby geographic locations may always connected road segment, dueexistence obstacles like mountain river. addition, distant locations oftenconnected highways. capture additional characteristics, also considered so-called sparse dense Delaunay graphs. Instances variants easilyobtained regular Delaunay graphs random deletion addition edges, respectively. Speci cally, generated sparse Delaunay graph instances deleting roughly60% edges random. Likewise, dense instances generated introducing 400edges random. (A new edge created selecting random pair nodes.)run algorithms presented main body paper alsoDelaunay graph variants. results obtained presented here.expected, sparse graph, often agent runs deadends. Indeed, algorithms required additional travel eort nd optimal pathedges removed. However, ratio travel cost two algorithmsseems remain (for various Delaunay graph types), I-A*DFS exhibitssuperior performance graph instances. See Figures 10(a), (b). behavior provedconsistent experiments, single agent multi-agent environment.Also, Figures 11(a), (b) exhibit similar behavior search cost WinA* versus windowsize sparse Delaunay graphs dense Delaunay graphs, respectively, observedregular Delaunay graphs (see Figure 6).Figures 12(a), (b) present costs fuel-ecient algorithm functionnumber agents various sizes sparse dense Delaunay graphs, respectively.overall fuel consumption recorded sparse Delaunay graphs larger fuelconsumption recorded counterpart regular graphs (see Figure 8) factor1.5. graphs simulating highways (i.e., dense graphs) fuel consumptiondecreases relative sparse regular Delaunay graphs.Note optimal number agents navigating sparse graph also increases, sinceagents need backtrack often case. Thus agents assist666fiPHA*: Finding Shortest Path A* Unknown Physical Environment10045Tree pathShrtest known pathAerial pathP-DFSD-DFSA*DFSI-A*DFS9080353060Search costSearch cost70Tree pathShrtest known pathAerial pathP-DFSD-DFSA*DFSI-A*DFS405040252015301020510050010001500200025003000Number nodes graph35000500400010001500200025003000Number nodes graph35004000(a)(b)Figure 10: Search cost versus number nodes of: (a) Sparse Delaunay graphs, (b)dense Delaunay graphs various low-level algorithms.75500 nodes1000 nodes2000 nodes6.5500 nodes1000 nodes2000 nodes4.5645Search costSearch cost5.54.543.53.532.5322.521.501020304050Window size60708001020304050Window size60(a)(b)Figure 11: Search cost WinA* versus window size various sizes of: (a) Sparse Delaunaygraphs, (b) dense Delaunay graphs.6677080fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu7.53500 nodes1000 nodes2000 nodes4000 nodes76.52.62.4Fuel consumption6Fuel consumption500 nodes1000 nodes2000 nodes4000 nodes2.85.554.52.221.841.63.51.431.22.51123456Number agents789123456Number agents789(a)(b)Figure 12: Fuel consumption function number agents various sizes of: (a)Sparse Delaunay graphs, (b) dense Delaunay graphs.search. hand, adding random edges graphs causes opposite eect,i.e., less fuel consumed optimal number agents reduced. explainedfact new edges add connections nodes, i.e., many "shortcuts"created search carried faster smaller number agents.114.5500 nodes1000 nodes2000 nodes4000 nodes8000 nodes1093.5873Search timeSearch time500 nodes1000 nodes2000 nodes4000 nodes8000 nodes4652.52431.521100.5123456Number agents789123456Number agents7(a)(b)Figure 13: Time consumption function number agents various sizes of: (a)Sparse Delaunay graphs, (b) dense Delaunay graphs.Figures 13(a), (b) present costs time-ecient algorithm functionnumber agents various sizes sparse dense Delaunay graphs, respectively.results con rm tendency observed regular Delaunay graphs (see66889fiPHA*: Finding Shortest Path A* Unknown Physical EnvironmentFigure 9), namely number agents grows, overall cost convergeslength optimal path.ReferencesArgamon-Engelson, S., Kraus, S., & Sina, S. (1998). Utility-based on-line explorationrepeated navigation embedded graph. Articial Intelligence, 101(1-2), 967{984.Argamon-Engelson, S., Kraus, S., & Sina, S. (1999). Interleaved vs. priori explorationrepeated navigation partially-known graph. International Journal PatternRecognition Articial Intelligence, 13(7), 963{968.Barber, C. B., Dobkin, D. P., & Huhdanpaa, H. (1993). Quickhull algorithm convexhull. Tech. rep., Geometry Center Technical Report GCG53, University Minnesota.Bellman, R. (1958). routing problem. Quarterly Applied Mathematics, 16 (1), 87{90.Bender, M. A., Fernandez, A., Ron, D., Sahai, A., & Vadhan, S. P. (1998). powerpebble: Exploring mapping directed graphs. Proceedings ThirtiethAnnual ACM Symposium Theory Computing, pp. 269{278, Dallas, Texas.Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction Algorithms. MIT Press, Cambridge, Massachusetts. 2nd edition.Cucka, P., Netanyahu, N. S., & Rosenfeld, A. (1996). Learning navigation: Goal ndinggraphs. International Journal Pattern Recognition Articial Intelligence,10(5), 429{446.Dechter, R., & Pearl, J. (1985). Generalized best- rst search strategies optimalityA*. Journal Association Computing Machinery, 32(3), 505{536.Dijkstra, E. W. (1959). note two problems connexion graphs. NumerischeMathematik, 1, 269{271.Felner, A., Kraus, S., & Korf, R. E. (2003). KBFS: K-best rst search. Annals Mathematics Articial Intelligence, press.Felner, A., Stern, R., & Kraus, S. (2002). PHA*: Performing A* unknown physical environments. Proceedings First International Joint Conference AutonomousAgents Multi-Agent Systems, pp. 240{247, Bologna, Italy.Ghosh, B. (1951). Random distances within rectangle two rectangles. BulletinCulcutta Mathematical Society, 43, 17{24.Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determination minimum cost paths. IEEE Transactions Systems Science Cybernetics,SCC-4(2), 100{107.Kaelbling, L. P., & Moore, A. W. (1996). Reinforcement learning: survey. JournalArticial Intelligence Research, 4, 237{285.Karp, R., & Pearl, J. (1983). Searching optimal path tree random costs.Articial Intelligence, 21(1-2), 99{116.669fiFelner, Stern, Ben-Yair, Kraus, & NetanyahuKitamura, Y., Teranishi, K., & Tatsumi, S. (1996). Organizational strategies multiagent real-time search. Proceedings Second International ConferenceMulti-Agent Systems, 409{416.Knight, K. (1993). many reactive agents better deliberative ones?.Proceedings Thirteenth International Joint Conference Articial Intelligence,pp. 432{437, Chamb$ery, France.Koenig, S., & Likhachev, M. (2002a). D* lite. Proceedings Eighteenth NationalConference Articial Intelligence (AAAI), pp. 476{483, Edmonton, Canada.Koenig, S., & Likhachev, M. (2002b). Incremental A*. Advances Neural InformationProcessing Systems 14 (NIPS). MIT Press, Cambridge, MA.Korf, R. E. (1985). Depth- rst iterative-deepening: optimal admissible tree search.Articial Intelligence, 27(1), 97{109.Korf, R. E. (1990). Real-time heuristic search. Articial Intelligence, 42(3), 189{211.Korf, R. E. (1993). Linear-space best- rst search. Articial Intelligence, 62(1), 41{78.Korf, R. E. (1997). Finding optimal solutions Rubik's Cube using pattern databases.Proceedings Fourteenth National Conference Articial Intelligence, pp.700{705, Providence, Rhode Island.Korf, R. E. (1999). Sliding-tile puzzles Rubik's Cube AI research. IEEE IntelligentSystems, 14, 8{12.Okabe, A., Boots, B., & Sugihara, K. (1992). Spatial Tessellations, Concepts, Applications Voronoi Diagrams. Wiley, Chichester, UK.Pearl, J., & Kim, J. H. (1982). Studies semi-admissible heursitics. IEEE TransactionsPattern Analysis Machine Intelligence, 4, 392{400.Shmoulian, L., & Rimon, E. (1998). Roadmap-A*: algorithm minimizing travel eortsensor based mobile robot navigation. Proceedings IEEE InternationalConference Robotics Automation, pp. 356{362, Leuven, Belgium.Stentz, A. (1994). Optimal ecient path planning partially-known environments.Proceedings IEEE International Conference Robotics Automation,pp. 3310{3317, San Diego, CA.Stern, R. (2001). Optimal Path Search Unknown Physical Enviroments. M.Sc.Thesis, Department Computer Science, Bar-Ilan University, Israel availablehttp://www.cs.biu.ac.il/felner.Taylor, L., & Korf, R. (1993). Pruning duplicate nodes depth- rst search. ProceedingsEleventh National Conference Articial Intelligence, pp. 756{761, Washington, D.C.Wagner, A., & Bruckstein, A. M. (2000). ANTS: Agents, networks, trees, subgraphs.Future Generation Computer Systems Journal, 16(8), 915{926.Yanovski, V., Wagner, I. A., & Bruckstein, A. M. (2001). Vertex-ant-walk: robust methodecient exploration faulty graphs. Annals Mathematics Articial Intelligence, 31(1-4), 99{112.670fiJournal Artificial Intelligence Research 21 (2004) 551-577Submitted 09/02; published 04/04Polynomial Sized MDP Succinct PoliciesPaolo Liberatorepaolo@liberatore.orgDipartimento di Informatica e SistemisticaUniversita di Roma La SapienzaVia Salaria 113, 00198, Roma, ItalyAbstractPolicies Markov Decision Processes (MDPs) determine next action executecurrent state and, possibly, history (the past states). numberstates large, succinct representations often used compactly representMDPs policies reduced amount space. paper, problems relatedsize succinctly represented policies analyzed. Namely, shownMDPs policies represented space super-polynomial sizeMDP, unless polynomial hierarchy collapses. fact motivates studyproblem deciding whether given MDP policy given size reward. Sincealgorithms MDPs work finding succinct representation value function,problem deciding existence succinct representation value functiongiven size reward also considered.1. IntroductionMarkov Decision Processes (MDPs) (Bellman, 1957) used AI planningeffects actions probabilistically known (Puterman, 1994). partiallyobservable extension (POMDP) formalizes scenarios observations givecomplete description state.best plan domains may simple sequence actions. Indeed, bestaction take may depend current state, known (partially, POMDPs)previous actions executed. conditional plans namedpolicies. Finding policies MDPs problem deeply investigated;algorithms developed, e.g., value iteration, policy iteration, methods basedlinear programming (Littman, Dean, & Kaebling, 1995). POMDPs, variantsvalue iteration algorithm developed (Cassandra, Littman, & Zhang, 1997; Zhang& Zhang, 2001).Formally, MDP composed set states, set actions, specification(probabilistic) effects actions, measure good state considered (rewardfunction). Initially, MDPs defined explicit form: states elementsgiven set {s1 , . . . , sn }; true false state si specified. effectsactions reward function represented explicit form, e.g., nstates, vector n elements represents reward function (each element vectorreward state.)explicit representation simple, practical use many cases.Indeed, many real-world scenarios described set variables (state variables);example, set Boolean variables specify true false currentc2004AI Access Foundation. rights reserved.fiLiberatorestate. explicit representation domain always size exponentialnumber state variables, contains enumeration states. succinctrepresentations used instead: states assumed possible evaluationsset Boolean variables; effects actions rewards statesrepresented succinct form. succinct representation considered paper usescircuits (Mundhenk, Goldsmith, Lusena, & Allender, 2000), others exist: decision trees,stochastic STRIPS operators, two-stage Bayes networks, variants BDDs (Boutilier, Dean,& Hanks, 1999; Littman, 1997; Dean & Kanazawa, 1989; Dearden & Boutilier, 1997; Hansen& Feng, 2000). Littman (1997) shown representations polynomiallyreduced other: choice circuits motivated ease use.Typically, scenarios expressed MDPs informally representedamount space scale number variables. example,domain ten coins ten actions tossing representedsuccinctly: necessary specify action ai tosses coin (e.g., resultai state side coin head tail probability 0.5state variables unchanged.) hand, explicit representationdomain contains set 210 states representation transition function,turns requires specification probability action change statestate s0 pair 210 states ten actions. result,transition function contains set 210 210 10 = 10 220 probabilities.Succinct representations, hand, follow intuition formal representation much larger informal representation domain.explicit representations always exponential number state variables,informal representation may short: case, often also caseinformal description converted formal succinct onelarge.Explicit succinct representations lead different computational properties:example, problems (e.g., checking existence policy given expected reward)PSPACE-hard succinct representations (Mundhenk et al., 2000) polynomialexplicit one (Papadimitriou & Tsitsiklis, 1987). apparent simplificationdue fact complexity measured relative size input problem,explicit representation introduces artificial blow-up size.paper, MDPs assumed succinct representation. particular,states possible evaluations set Boolean variables (state variables); effectsactions reward function described using circuits. Since numberstates exponential number state variables, policy indicates actionexecute state, explicit representation policies always exponentialnumber state variables. hand, succinct representation MDP maytake polynomial space number state variables. case, policyexponentially larger MDP. However, MDPs succinctly representedsmall amount space, policies expressed succinct form well (Koller &Parr, 1999). paper, policies represented circuits take input state(possibly) history (the past states), output next action execute.first result proved paper (Section 4) optimal policies, even succinctrepresentation, may require amount space exponential size MDP.552fiOn Polynomial Sized MDP Succinct Policiesresult new MDPs succinct form; particular, hardness findingoptimal policy imply anything policy size. Indeed, even casesfinding optimal policy undecidable (Madani, Hanks, & Condon, 1999), policymay short. Many hard problems, even undecidable ones, knownshort solutions: example, solution halting problem single bit,finding undecidable. Therefore, impossibility representing solutionsproblem polynomial space follow complexity problem.Given optimal policies cannot always represented space polynomialsize MDP, reasonable request best possible policy representedwithin given space bound (Section 5). show bounding size policies simplifies policy existence problem. Bounding size succinct representationvalue function (the function giving expected reward states) simplifiesproblem (Section 6). second bound intended shed light complexityalgorithms work estimating expected reward state, value iteration algorithm (Littman et al., 1995; Cassandra et al., 1997; Zhang & Zhang, 2001).complete analysis (Section 7) considering problem finding policies givensize reward, size exponentially larger MDP. Implicationsdiscussions results paper given last section (Section 8).2. Markov Decision ProcessesMarkov Decision Processes formalize problems planning probabilistic domains.components are: set states, set probabilistic actions, function evaluatesstates according notion goodness (reward function).Formally, MDP 5-tuple = hS, s0 , A, t, ri, where: set states, s0distinguished state (the initial state), set actions, function representingeffects actions, r function giving reward states.effects actions known sure, according probabilitydistribution. Therefore, effects actions cannot represented using functionmaps state another state, given specific action. function instead functionactions pairs states numbers interval [0, 1]. function representsprobability transitions: t(s1 , s2 , a) = p means result executing actionstate s1 state s2 probability p. reward function measuremuch state matches goals. Formally, function states integer numbers.MDPs assumed represented succinct form. Namely, states assumedrepresented tuples Boolean variables (state variables), i.e., set statesset propositional interpretations set variables. functions rrepresented Boolean circuits. representations commonly usedpractice (e.g., probabilistic STRIPS operators, two-stage Bayes network, etc.) Booleancircuits advantage able encode exactly polynomial-time computablefunctions. fact makes suitable computational analysis: indeed,transition reward function polynomial, encoded polynomial-sizecircuit without show details encoding (Mundhenk et al., 2000; Boutilieret al., 1999). order encode polynomial-time functions, hand,553fiLiberatorerepresentations require introduction new variables dummy time points(Littman, 1997).Definition 1 succinct MDP 5-tuple = hV, s0 , A, t, ri, V set Booleanvariables, s0 propositional interpretation V, set actions, circuitwhose input pair interpretations V element A, r circuitwhose input interpretation V.succinct MDP, set states factored, i.e., Cartesian productpossible values variables. succinct MDPs also called factoredMDPs. term succinct, however, appropriate, r expressedproduct something. also important note set actions explicitlyrepresented: affects complexity checking consistency value function, i.e.,Theorem 8.succinct MDP = hV, s0 , A, t, ri represents (explicit) MDP M0 = hS, s0 , A, t0 , r0 i,set propositional interpretations variables V; transition functiont0 function represented circuit t; reward function r0 function represented circuit r. words, value t0 (s, s0 , a) output circuitinputs s, s0 , Boolean representation a. applies r r0 .Planning deterministic domains consists finding sequence actions reachgoal state. Nondeterminism introduces two complications: first, extentgoal reached probabilistically determined; second, state timepoint cannot uniquely determined initial state actions executed far.Planning nondeterministic domains consists finding actions leadbest possible states (according reward function). Since effects actionsknown sure, expected value reward determined. example,result applying state s0 s1 probability 1/3 s2 probability2/3, expected reward executing given r(s0 ) + 1/3 r(s1 ) + 2/3 r(s2 ),since r(s0 ), r(s1 ), r(s2 ) rewards s0 , s1 , s2 , respectively. Formally,expected undiscounted reward considered. sum reward stateweighted probability reaching it.second effect nondeterminism best action execute dependscurrent state, unambiguously determined initial state actionsexecuted far, effects actions probabilistically known. example,executing may lead state s1 s2 . executed, actual result known.point, may best action execute s1 a0 , a00 s2 : optimalchoice depends current state, cannot unambiguously determinedinitial state previous actions. simplest case, policy function givesbest action execute state. policies called stationary. policy mayalso depend past states; policies called history dependent.reward associated policy expected average reward obtained executing, state, associated action. horizon assumed finite, i.e.,happens given number steps considered. complexity problem changes according whether horizon unary binary notation (Mundhenket al., 2000). Informally, assumption horizon unary notation meansnumber steps consider polynomial size instance problem.554fiOn Polynomial Sized MDP Succinct Policiespaper, assumed unary notation. assumption called shorthorizon polynomial horizon papers.size explicit representation every policy succinct MDP exponential number state variables. However, policies take less space succinctrepresentations. paper, succinct representation employing circuits considered:input current state (possibly) history; output next actionexecute. example policy small succinct representation alwaysexecuting action; policy exponential explicit representationnecessary specify action state number states exponentialnumber state variables.first question considered paper whether always possible representoptimal policy succinct MDP circuit size polynomial sizeMDP. Namely, succinct policy defined circuit outputs, given current state,action.Definition 2 succinct stationary policy P circuit takes state inputoutputs action execute state.succinct policy circuit represents function states actions. Since (nonsuccinct) policies functions states actions, succinct non-succinct policiescorrespondence. expected reward succinct policy optimality thereforedefined terms analogous concepts non-succinct policies.Succinct history-dependent policies defined similar way: circuitssequences states actions (such representation possible finite horizon.)expected reward optimality succinct history-dependent policy definedcorresponding concepts non-succinct policies.first result paper that, MDPs optimal succinct policies polynomial size, polynomial hierarchy coincides complexity class p2 (consideredunlikely). proof based compilability classes reduction, summarizednext section.3. Complexity, Compilability, Circuitsreader assumed familiar complexity classes P, NP, classespolynomial hierarchy (Stockmeyer, 1976; Garey & Johnson, 1979). countingclasses also used paper: PP class problems polynomiallyreduced problem checking whether formula satisfied least halfpossible truth assignments set variables. alternative definitionproblem PP polynomially reduced checking whetherPV (M ) k, k integer, V function propositional interpretationsintegers calculated polynomial time, sum ranges possiblepropositional interpretations (Johnson, 1990). class NPPP defined terms oracles:class problems solved polynomial time non-deterministicTuring machine access PP-oracle, device solve problemPP one time unit.555fiLiberatoreWithout loss generality, instances problems assumed strings. lengthstring x denoted ||x||. cardinality set instead denoted |S|.function g called poly-time exists polynomial p algorithmthat, x, time taken compute g(x) less equal p(||x||).function f called poly-size exists polynomial p that, strings x,holds ||f (x)|| p(||x||); whenever argument function f number,assumed unary notation. definitions extend functions oneargument usual.Circuits defined standard way. Whenever C circuit possibleassignment input gates, output denoted C(s) output. formal definitioncircuits important paper, take advantage well-known resultcircuit complexity relates poly-time functions poly-size circuits. Informally, givencircuit value inputs, determine output polynomial time.similar result holds way around, is, poly-time functions representedmeans circuits (Boppana & Sipser, 1990).Formally, however, poly-time function strings strings string arbitrarylength argument, string arbitrary length (in general) output. Even consideringfunctions binary output (i.e., single bit), input may arbitrarilylong. hand, circuit specified number input gates.correspondence poly-time functions poly-size circuits one-to-one.However, following correspondence holds: poly-time function stringsstrings, exists uniform family poly-size circuits {C0 , C1 , C2 , . . .}, Cicircuit input gates calculates result f strings length i; uniformmeans exists function calculates Ci runs timebounded polynomial value i.result, class P also defined set problems solveduniform family circuits. replacing assumption family uniformCi size polynomial value i, definition gives class P/poly.paper, problem whether succinct MDPs optimal succinct policypolynomial size considered. Note that, given MDP, succinct policiescircuits take input state (or, sequence states). result, policysingle circuit, family.question given (negative) answer using two different proofs, based differenttechniques: first one based compilability classes (Cadoli, Donini, Liberatore, &Schaerf, 2002), second one employs standard complexity classes. compilabilityclasses introduced characterize complexity intractable problemspreprocessing part data allowed. problems characterized wayinstances divided two parts: one part fixed (knownadvance) one part varying (known solution needed.) problemdetermining action execute specific state form: MDP (withhorizon) part known advance, describes domain; contrary,state determined previous actions executed. Compilabilityclasses reductions formalize complexity problems first partpreprocessed.556fiOn Polynomial Sized MDP Succinct Policiescommon complexity theory, decision problems considered, i.e., problemswhose solution single bit. problems usually identified languages (setsstrings): language L represents decision problem whose output 1 x L0 x 6 L.problems whose instances composed two parts formalized languagespairs (of strings): language pairs subset . order characterizeproblems, non-uniform compilability classes introduced (Cadoli et al.,2002). classes denoted k;C read nu-comp-C, C arbitraryuniform complexity class, usually based time bounds, P, NP, etc.Definition 3 (k;C classes) k;C composed languages pairsexists poly-size function f pairs strings strings languagepairs 0 C that, hx, yi , holds:hx, yi iff hf (x, ||y||), yi 0problem k;C reduces one C suitable polynomial-size preprocessing (compiling) step. problem C also k;C (i.e., f (x, n) = x0 = S). Preprocessing useful problem C k;C0 C0 C: case,preprocessing decreases complexity problem. reduction complexitypossible problems (Cadoli et al., 2002).tool used proving decrease complexity possibleconcept hardness respect compilability classes, turn baseddefinition reductions. Since general concepts really needed paper,condition based monotonic polynomial reductions presented. conditionsufficient prove problem k;NP k;P unless NP P/poly,currently considered unlikely.Let us assume hr, hi polynomial reduction 3sat problem pairs S,is, r h poly-time functions satisfiable hr(), h()i S.pair hr, hi monotonic polynomial reduction if, pair sets clauses 12 literals, 1 2 , holds:hr(1 ), h(1 )i iff hr(2 ), h(1 )iNote second instance combines part 2 part 1 :intentional. Roughly speaking, reduction implies hardness problemcomes second part instances only, first part r(1 ) instanceresult reduction replaced another one r(2 ) without changingmembership S. complexity problem due part instance only,preprocessing part reduce complexity problem. formalproof fact existence reduction implies k;P (unlessNP P/poly) found elsewhere (Liberatore, 2001).4. Super-polynomially Sized PoliciesSuppose always possible find optimal succinct policy P polynomial size.Since succinct policies circuits definition, deciding action execute state557fiLiberatorepolynomial-time problem: given P , compute P (s), output circuit Pgiven input. whole two-step process finding policy usingfind next action specific state seen algorithm finding nextaction execute state. first step (finding policy) likely hard,second one polynomial-time computable. means problem decidingnext action compilable k;P. done present paper proveproblem instead k;P. implies succinct policies cannot alwaysrepresented polynomial space. aim, formal definition problemdeciding next action execute given.Definition 4 next-action problem problem deciding, given MDP, horizonunary notation, state, action, whether action one executestate according optimal policy.problem proved NP-hard intermediate step. Actually, result easyderive known theorems: interesting reduction used proof. Namely,given set clauses , clause composed three literals set variablesX = {x1 , . . . , xn }, instance next-action problem polynomial time builtpolynomial time. reduction denoted f ; formally, function setsclauses quadruples hM, T, s, ai, first element succinct MDP, secondone number unary (the horizon), third one state, fourth oneaction. Let L set literals X, let E = L {sat, unsat}. MDPdefined as:= hV, , A, t, ricomponents defined follows.States: set states correspondence set sequences (2n)3 +n + 1 elements E; obtained using following set variables:V = {qi | 1 log((2n)3 + n + 1)} {vij | 1 log(|E|), 1 j (2n)3 + n + 1}.idea variables qi represent length sequence binary,jvariables v1j , . . . , vlog(|E|)represent j-th element sequence;Initial state: initial state interpretation representing empty sequence ;Actions: contains three actions A, S, U , one action ai xi ;Transition function: action change current state either sat unsatbelong sequence represented current state; otherwise, effectrandomly select (with equal probability) literal L add sequencerepresenting current state; effect U add sat unsatsequence, respectively (these deterministic actions); actions ai change stateeither sat unsat, both, belong sequence; case, aiadds either xi xi sequence, probability;558fiOn Polynomial Sized MDP Succinct PoliciesReward function: involved part MDP. Given sequence 3mliterals L, following 3cnf formula considered:C(l11 , l21 , l31 , . . . , l1m , l2m , l3m ) ={l11 l21 l31 , . . . , l1m l2m l3m }.Given number possible distinct clauses L less (2n)3 , setclauses represented sequence 3m literals, = (2n)3 .function C encodes sets clauses L sequences literals.sequences reward different zero composed sequence3m literals E, followed either sat unsat, followed sequence s0 =l1 , . . . , ln , li either xi xi . Namely, sequence (s, unsat, s0 )reward 2; sequence (s, sat, s0 ) reward 1 set clauses C(s) satisfiedmodel s0 , 2n+1 otherwise;Note states reward 0. expected reward calculatedreached states, r defined way that, state nonzero reward,previous succeeding states reward zero. reward function r definedway sake making proof simpler; however, expected reward calculatedstates, including intermediate ones.MDP single optimal policy: execute 3m times, execute either US, execute a1 , . . . , . choice U gives greatestexpected reward depends result execution A. Namely, possible resultexecution first 3m actions corresponds set clauses. next actionoptimal policy U set unsatisfiable satisfiable.definition MDP M. instance next-action problemcomposed MDP, horizon unary, state, action, problemcheck whether action optimal state. horizon consider = (2n)3 +n+1,state one corresponding sequence literals C(s) = ,action S. possible prove function f defined f () = hM, T, s, Sireduction satisfiability next-action problem.Theorem 1 f () = hM, T, s, Si, unique (stationary history-dependent)optimal policy w.r.t. horizon , satisfiable action execute optimal policy horizon .Proof. MDPs defined policies positive reward: policy executingA3m U a1 , . . . , expected reward equal 2, since leaves reward 2internal nodes reward 0. sequence actions executed stationarypolicy history inferred current state.sequences actions end state positive reward similarother. Indeed, begin 3m times action A. Then, either Uexecuted, followed sequence a1 , . . . , . difference sequenceschoice U S.optimal policies therefore execute first 3m time points, regardlessgenerated state. execute either U S, execute a1 , . . . , . choice559fiLiberatoreU made differently different states: policy execute Udepending state results 3m executions A.sequence 1:. . A} U a1 . . .| .{zsequence 2:. . A} a1 . . .| .{z3m3mFigure 1: Sequences leading state reward > 0. fragments extensionsgive reward 0.Let us consider state 3m executions A. Since executiongenerates random literal, point state sequence 3m literals.sequence represents set clauses later used reward function. Intuitively,point optimal policy execute U set clauses unsatisfiable,satisfiable.Let state results execution 3m times, let C(s)corresponding formula. expected reward policy executes sequenceU, a1 , . . . , 2 sequence leads states reward 2.hand, reward policy executes A, a1 , . . . , depends satisfiabilityformula represented state. Namely, sequence leads statepossible truth interpretation X; reward state 1 C(s) satisfiedmodel, 2n+1 otherwise. means reward policy 1formula unsatisfiable, least (2n 1)/2n + 2n+1 /2n = (2n 1)/2n + 2 formulaleast one model.optimal choice therefore U formula C(s) unsatisfiable (reward 2)otherwise (reward greater equal (2n 1)/2n + 2n+1 /2n > 2). Since historyinferred state, optimal choice depend whether stationaryhistory-dependent policies considered.Incidentally, theorem implies choosing next action optimally NP-hard.important, however, function f used prove problemchoosing next action optimally cannot simplified P thanks preprocessingstep working MDP horizon only. This, turn, implies nonexistencepolynomially sized circuit representing optimal policy.polynomial reduction 3sat problem necessarily proveproblem cannot efficiently preprocessed. Consider, however, case problem instances divided two parts. reduction decomposed twoseparate functions, one generating part instance preprocessedone generating rest instance. case, form partpreprocessed, state action rest instance. result,f () = hM, T, s, ai, two functions defined by:r() = hM,560fiOn Polynomial Sized MDP Succinct PoliciesJJJJJJexecution Jlevel, state3mtimesJrepresents set clausesJJJU U Usatisfiable formulaunsatisfiableformulas1 1 2n+12n+1modelsmodelsFigure 2: optimal policy MDP proof.h() = hs, ai.Provided polynomial hierarchy collapse, problem k;Pexists monotonic Polynomial reduction 3sat problem (Liberatore, 2001).polynomial reduction monotonic two functions made satisfy followingcondition: every pair sets clauses 1 2 three literals setvariables, 1 2 hr(1 ), h(1 )i yes instance hr(2 ), h(1 )iyes instance. Note second instance hr(2 ), h(1 )i, is, combinespart derived 2 part 1 .specialization condition case next-action problem follows.Let hM1 , = r(1 ) hM2 , = r(2 ): two horizons 12 alphabet. Let hs, ai = h(1 ). Monotonicity holds if, two setsclauses 1 2 set literals 1 2 , optimal actionexecute state M1 M2 . reduction fdefined satisfies condition.Theorem 2 function f sets clauses quadruples hM, T, s, ai monotonicpolynomial reduction.Proof. Let 1 2 two sets clauses set variables, clausecomposed three literals, let M1 M2 corresponding MDPs. SinceMDP corresponding set clauses dependsby constructionon set variablesonly, M1 M2 exactly MDP. Since horizons r(1 ) r(2 )same, r(1 ) = r(2 ). result, state action a, latteroptimal action execute M1 horizon M2 ,definition monotonicity case MDPs.561fiLiberatoretheorem implies next-action problem hard compilability classk;NP. turn, result implies MDPs optimal succinct policysize polynomial MDP horizon. specifically, waystoring optimal actions states way required space polynomialtime needed determine action execute state polynomial well.Theorem 3 exists data structure, size polynomial MDPhorizon, allows computing best action execute (either current statehistory) given MDP horizon unary notation polynomial time,NP P/poly.Proof. data structure exists, next-action problem k;P: given fixedpart problem (the MDP horizon), possible determine datastructure preprocessing step; result step makes determining next actionpolynomial task. result, next-action problem k;P. hand,existence monotonic polynomial reduction propositional satisfiabilitynext-action problem implies problem k;P, k;NP=k;P (Liberatore,2001, Theorem 3). turn, result implies NP P/poly (Cadoli et al., 2002,Theorem 2.12).Note NP P/poly implies p2 = p2 = PH, i.e., polynomial hierarchycollapses second level (Karp & Lipton, 1980). result, theorem impliesone always represent optimal policies space polynomial MDPhorizon polynomial hierarchy collapses. Since succinct representationpolicies based circuits subcase data structures allowing determinationnext action polynomial time, following corollary holds.Corollary 1 exists polynomial p every succinct MDP horizonsuccinct optimal policies (either stationary history dependent) size boundedp(||M|| + ), NP P/poly p2 = p2 = PH.5. Finding Evaluating Succinct Policiesproblem considered section checking existence succinct policiesgiven size reward.subproblem interest evaluating policy, is, calculating expectedreward. Mundhenk et al. (2000) found complexity problem variouscases, left open case full observability succinct representation,one considered paper. problem instead analyzed Littman,Goldsmith, Mundhenk (1998) considering succinct representation plans basedST plan representation (which also equivalent succinct representation.)proof presented current paper could follow along similar lines.evaluation problem make sense, given MDPs optimalsuccinct policies polynomial size? theoretical point view, super-polynomialityforbid complexity analysis. Indeed, complexity measured relative totalsize problem instances; instances policy evaluation problem include562fiOn Polynomial Sized MDP Succinct Policiespolicy MDP. policy exponentially larger MDP, meansMDP logarithmic part instance.Formally, problem policy evaluation is: given MDP, policy, number k,decide whether expected reward policy greater k. counting problem, amounts summing evaluations result computing polynomialfunction set propositional models. surprisingly, PP.Theorem 4 Given succinct MDP = hV, s0 , A, t, ri, horizon unary notation,succinct policy P (either stationary history dependent), number k, deciding whetherpolicy P expected reward greater k PP.Proof. expected reward policy weighted sum rewards states. Let usconsider sequence states s0 , s1 , . . . , sd . probability sequenceactual history computed follows: pair consecutive states si , si+1factor given probability t(si , si+1 , a), unique action chosenpolicy state si (or, history s0 , . . . , si .) Multiplying factors,result probability whole sequence s0 , s1 , . . . , sd actual historypoint d. Since P (si ) denotes output circuit P si input,P (si ) represents action executed state si . result, probabilitys0 , s1 , . . . , sd actual sequence states follows:H(s0 , s1 , . . . , sd ) =t(si , si+1 , P (si ))i=0,...,d1probability history-dependent policies determined way,P (si ) replaced P (s0 , . . . , si ). Given specific sequence s0 , . . . , sd , possibledetermine H(s0 , . . . , sd ) time polynomial size sequence plusMDP.expected reward policy calculated sum expected rewardstate sd multiplied probability sequence ending sd actualhistory. sum expressed follows:R(P ) = r(s0 ) +d=1,...,TXH(s0 , . . . , sd ) r(sd ).s1 ,...,sdnumber ranges 1 take account sequences length. intermediate states sequences dealt with: sequences0 , . . . , si , . . . , sd , sum contains term subsequence s0 , . . . , si well.Roughly speaking, membership PP due fact expected rewardpolicy result sum exponential number terms, termsdetermined polynomial time uniform manner. Formally, provedproblem expressed sum terms V (M ), V polynomialfunction ranges propositional interpretations given alphabet.complete proof, therefore, needed encode possible sequence s0 , . . . , sdpropositional model, constraint H(s0 , . . . , sd ) r(sd ) determinedmodel polynomial time.563fiLiberatoreemployed encoding following one: alphabet X1 XT ,set Xi set variables one-to-one correspondence variables MDP,set variables size log(T ). model represents sequence whose lengthgiven values whose i-th state given values Xi .left show expected probability sequencedetermined polynomial time given model represents sequence. truebecause, given M, possible rebuild sequence model time linearsize model, evaluate H. Since sequences represented propositionalinterpretations, function giving weighted reward interpretation polytime, problem PP.theorem shows problem policy evaluation succinct MDPs horizonunary notation PP. problem also proved hard class.Theorem 5 Given succinct MDP M, horizon unary notation, succinct policyP , number k, deciding whether expected reward P greater k PP-hard.Proof. theorem proved reduction problem checking whether formulasatisfied least half possible truth assignments variables. Let Qformula alphabet X = {x1 , . . . , xn }. define MDP = hV, s0 , A, t, riway states correspondence sequences literals X.actions {a1 , . . . , }, ai modifies state adding xi xisequence represented state, probability. reward function assigns1 sequences represent models Q, 0 sequences. Namely,sequence contains variable twice, contain variable, corresponding statereward 0.Let P policy executing ai state composed 1 literals. statesresult application policy time consistent sets literalsalphabet x1 , . . . , xi1 . < n 1, state reward 0. Therefore,states time = n 1 relevant calculation expected reward P .states sets literals represent models alphabet X; probabilitystate time = n 1 1/2n . Given reward state 1corresponding model satisfies Q 0 otherwise, expected reward P m/2n ,number models Q.proof used prove problem finding expectedreward policy #P-hard, decision problems considered paper.two theorems allow concluding problem PP-complete.Corollary 2 Given succinct MDP, horizon unary notation, succinct policy,number k, deciding whether expected reward policy larger k PP-complete.Let us turn problem checking existence policy given sizeexpected reward. problem without size constraint PSPACE-hard(Mundhenk et al., 2000). corollary indicates size bound allowsguess-and-check algorithm slightly lower complexity.564fiOn Polynomial Sized MDP Succinct PoliciesTheorem 6 Given succinct MDP, horizon unary notation, size bound z unarynotation, reward bound k, checking existence succinct policy size boundedz expected reward greater equal k NPPP -complete.Proof. Membership easy prove: guess circuit size z representing policy checkwhether expected reward greater k. Note z unary essential.Hardness proved reduction e-majsat, problem defined Littman et al.(1998) follows: given formula Q variables X , decide whether existstruth assignment X least half interpretations extending satisfyQ. problem NPPP -complete (Littman et al., 1998).Given instance e-majsat, corresponding MDP defined follows: statesrepresent sequences literals X (we assume, w.l.o.g., |X| = |Y | = n.)action ai variable . ai adds literal yi yi sequencerepresenting current state probability. variable xi associatedactions bi ci , add xi xi state, respectively. reward state1 represents sequence form []x1 , []x2 , . . . , []yn literalssatisfy Q, is, sequence complete interpretation satisfies Q.policy expected reward greater 0 made sequence whosei-th element (1 n) either bi ci , + n-th element (1 n) ai .expected reward policy horizon = 2n 1 least halfcompletions model defined actions bi cj satisfy Q. Therefore, MDPpolicy reward 1 Q e-majsat.result used alternative proof claim MDPs admitpolynomially sized optimal succinct policies. proof interesting employsdifferent technique, conditioned differently previous one.Theorem 7 MDPs optimal succinct policies (either stationary history dependent) size polynomial sum size MDP length horizonPSPACE NPPP .Proof. Checking existence policy given expected reward, regardlesssize, PSPACE-hard (Mundhenk et al., 2000). hand, policiesrepresented polynomial space, problem coincides checking existencepolicy given bound size, latter problem NPPP .Theorem 7 proved using fact problems evaluating policy given sizedeciding existence policy different complexity characterizations.technique used Papadimitriou Tsitsiklis (1987) case POMDPsexplicit representation.Precisely, proof composed following sequence statements:1. Evaluating expected reward succinct policy C1 -complete, C1complexity class;2. Deciding existence policy (with size bound) given expected rewardC2 -complete, C2 complexity class;565fiLiberatore3. policy could succinctly represented space polynomial sizeMDP value horizon, second problem could solved guessingpolicy evaluating it; since policy guess size polynomial sizeinstance B, C2 NPC1 .makes proof worthy (probable) falsity conclusion C2 NPC1 .words, proof applied prove given data structurealways polynomially large if:1. problem C1 -complete;2. problem B C2 -complete;3. problem B expressed as: exists data structure satisfying problem A.proof schema simply generalization one above: data structuresuccinct policy; first problem evaluating succinct policy; second problemdeciding existence policy giving given expected reward. definition,instance satisfying B implies existence data structure satisfying A. possiblereplace existence data structure existence polynomial-size datastructure sentence, C2 NPC1 , B solved guessing datastructure checking whether satisfied not. conclusion C2 NPC1false, instances satisfying B related data structuresatisfying size polynomial size instance B.6. Bounding Value FunctionBounding policy size motivated fact interested policiesactually stored: policy used size less equalavailable storage space. section, similar constraint considered, motivatedalgorithms MDPs work. Namely, programs based popular value iterationalgorithm (Littman et al., 1995; Cassandra et al., 1997; Koller & Parr, 1999; Zhang &Zhang, 2001) work finding value function, function giving expectedreward obtained state executing actions according givenpolicy.Definition 5 value function E MDP = hS, s0 , A, t, ri horizonpolicy P function gives expected reward state stepshorizon:(E(s, i) =r(s)= 0Pr(s) + s0 t(s, s0 , P (s)) E(s0 , 1) otherwise.similar definition given history-dependent policies including historyarguments value function policy:(E(s0 , . . . , sj , i) =r(sj )= 0;Pr(sj ) + s0 t(sj , s0 , P (s0 , . . . , sj )) E(s0 , 1) otherwise.566fiOn Polynomial Sized MDP Succinct Policiesvalue function succinct MDPs defined simply replacing s0 s0propositional interpretation V.MDP policy succinctly represented, value function cannot necessarily represented explicitly polynomial space. Rather, form succinct representation employed, usually decomposition state space (for example, groupingstates expected reward.) already done policies, valuefunctions succinctly represented circuits.Definition 6 succinct value function circuit E whose inputs stateinteger whose output expected reward points horizon.Given policy P , always exists associated value function, givesexpected reward obtained state executing actions specifiedpolicy. converse, however, always possible. value functions, indeed,correspond policy. value function used derive policyselecting actions maximize expected reward resulting states (thisvalue functions often used), value functions completely unrelatedactual expected reward states. example, reward s0 0, actionchanges states (i.e., t(s, s, a) = 1 actions states s), value functionE E(s0 , ) = 1000 correspond policy. words, valuefunctions assign expected rewards states way consistent MDP,i.e., way obtain reward executing whichever actions. Therefore,value function may may consistent, according following definition.Definition 7 value function E consistent MDP horizonexists policy P E value function M, , P .interesting property value functions that, cases, actually representpolicies. Indeed, policy determined value function polynomial timedegree non-determinism bounded, list possible states may resultexecuting action calculated polynomial time. particular, set statesresulting applying assumed result circuit na .Definition 8 bounded-action MDP 6-tuple = hV, s0 , A, N , t, ri, M0 =hV, s0 , A, t, ri succinct MDP N = {na } set circuits, one na NA, na (s) list states s0 t(s, s0 , a) > 0.Beside N , definition bounded-action MDPs succinct MDPs.difference explained two ways: intuitively, assumed possibleoutcomes actions determined time polynomial size MDP; technically, time needed determine possible states result applying actionincluded size input.proof NPPP -hardness problem policy existence uses bounded-actionMDPs, therefore still holds case. seems contradict intuitionlarge degree non-determinism one sources complexity problems MDPs.next results explain contradiction.567fiLiberatoreGiven bounded-action MDP M, horizon, succinct value function E,identify polynomial time succinct policy corresponds E, is, policyleads expected reward states specified E. case stationary policies,P determined follows: given state s, consider action time point i,check whether result executing consistent value function, assumingtime point i. done determining sum t(s, s0 , a) E(s0 , 1).equal E(s, i) r(s), action action execute, i.e., P (s) = a.whole process polynomial, is, P (s) determined polynomial time.E consistent MDP, policies expected rewardstate E(s). result, consistent value function E represents grouppolicies, expected reward. therefore makes sense considerproblem finding E rather finding P . precisely, since decision problemsconsidered, analyzed problem checking whether exists Eexpected reward corresponding policies greater equal given number.Given E, expected reward simply given E(s0 , ). order check whetherreward actually obtained MDP, however, also check whethervalue function E consistent MDP. point assumptionset actions succinct MDP succinct representation used.Theorem 8 Checking whether succinct value function E consistent boundedaction MDP horizon unary coNP-complete, stationary historydependent policies.Proof. problem check whether exists policy P gives expectedreward specified E. Namely, i, check whether equationDefinition 5 holds policy P . turn, existence policy means that,s, exists associated action satisfies equation P (s) replaceda.Formally, let = hV, s0 , A, t, ri MDP, horizon, setinterpretations alphabet V. condition formally expressed follows.every state s, holds E(s, 0) = r(s), and:{1, . . . , } . E(s, i) = r(s) +Xt(s, s0 , a) E(s, 1).s0condition contains three alternating quantifiers; however, second one rangesA, third one ranges = {1, . . . , }. cases, numberpossibilities polynomial size MDP horizon.._^aAi=1,...,TE(s, i) = r(s) +Xt(s, s0 , a) E(s0 , 1) .s0number terms s0 sum not, general, polynomial. boundedaction assumption, however, implies states relevant s0belongs na (s), i.e., s0 one elements list produced circuit nagiven input.568fiOn Polynomial Sized MDP Succinct Policies._^aAi=1,...,TXE(s, i) = r(s) +t(s, s0 , P (s)) E(s0 , 1) .s0 na (s)Considering given only, condition checked polynomial time. Sincecondition checked possible S, problem coNP. casehistory-dependent policies dealt replacing s0 , . . . , sj j.Hardness proved follows: given formula Q variables X = {x1 , . . . , xn },build succinct MDP V = X single action a. action takesnumber {1, . . . , n} equal probability changes value xi . rewardfunction 1 state satisfies Q, 0 otherwise. value function E(s, i) = 0= 1, . . . , consistent MDP formula Qunsatisfiable. Indeed, since MDP contains one action, possible stationaryhistory-dependent policy always executing action a. policy leadsrandom interpretation. expected reward policy 1interpretation satisfies Q.Checking existence consistent succinct value function size bounded zexpected reward bounded k therefore p2 , done guessing succinctvalue function E, checking consistency MDP horizon, determiningexpected reward initial state E(s0 , ). Since consistency coNP boundedaction MDPs, problem p2 . following theorem also shows problemcomplete class.Theorem 9 Given bounded-action MDP M, horizon unary notation, size boundz unary notation, reward bound k, checking existence succinct valuefunction E consistent , size bounded z expected rewardbounded k p2 -complete stationary history-dependent policies.Proof. Membership: guess succinct value function E (i.e., guess circuit statehistory integer input) size z; check whether consistentMDP horizon, whether E(s0 , ) k. Since checking consistency coNPstationary history-dependent policies, problem p2 cases.Hardness proved Theorem 6. use problem checking whetherexists truth evaluation X whose extensions X models Q, Qformula X |X| = |Y | = n.MDP corresponds Q sequences literals X states.action ai variable two actions bi ci variable X.effect ai add either yi yi state, probability. actions bici add xi xi , respectively, therefore deterministic actions. rewardstate 1 state sequence comprised either x1 x1 followedx2 x2 , etc., literals form model Q. states reward 0.policy nonzero reward executes either bi ci time i, executeactions a1 , . . . , sequence. first n actions, state exactly model X.reward policy number models extend satisfy Q. Therefore,569fiLiberatoreexists policy reward 1 exists model X whose extensionssatisfy Q.theorem shows checking existence succinct value function givensize reward complexity class second level polynomial hierarchybounded-action MDPs. corresponding problem policies (instead value functions)remains NPPP -hard bounded-action MDPs. Assuming p2 6= NPPP , checkingexistence bounded-size succinct policies harder checking existence valuefunctions, sense problems polynomially reduced latter problemformer. Let us consider result.1. problem easier additional constraint size valuefunction. Assuming p2 6= NPPP , implies succinct policies sizepolynomial MDP whose value function cannot expressed circuitsize polynomial MDP, bounded-actions MDPs. Translating succinctvalue functions succinct policies instead always feasible polynomial spacebounded-actions MDPs. Therefore, succinct policy translatedsuccinct value function polynomial size (in size policy, MDP,horizon), inverse translation always polynomial size.2. Given amount physical memory always bounded, problemsolved algorithms finding value function (such value iteration) actuallychecking existence succinct value function bounded size.problem easier similar problem bound, sense firstproblem polynomially reduced former vice versa, providedp2 6= NPPP .Roughly speaking, finding bounded-size succinct value function simpler findingbounded-size succinct policy, solving former problem may produce solutionsworse, term overall reward, solutions latter.7. Exponential Boundsprevious sections, size bounds assumed unary notation.assumption made because, feasible store unary representationsize, feasible store data structure size well. unary notationformalizes informal statement policy take amount spacepolynomial size instance.Let us consider problem checking existence policy bounded sizereward, bound reward binary notation: searchingpolicy exponentially larger MDP, still size bounded givennumber. problem interest whenever exponential succinct policy acceptable,still limit size.important observation circuits necessary consider circuitsarbitrary size. Indeed, circuit n inputs outputs equivalent circuitsize m2n . circuit equivalent circuits,n inputs one output. circuit represents Boolean function,570fiOn Polynomial Sized MDP Succinct Policiestherefore expressed DNF term contains variables. termrepresents model: therefore, 2n different terms alphabetmade n variables; result, function expressed circuit size 2n .result, policy expressed circuit size |A|2n , n number statevariables.fact two consequences: first, necessary consider circuits arbitrarysize, policy represented circuit size |A|2n ; second, problemfinding succinct policy size bounded z = |A|2n bounded rewardfinding arbitrary policy bound reward. result, problemchecking whether MDP succinct policy size bounded z reward boundedk least hard problems bound size policy.latter problem PSPACE-hard; therefore, former PSPACE-hard well.result holds history-dependent policies: instead n input gates, nT inputgates. observation, following result follows simple corollary resultMundhenk et al. (2000).Corollary 3 Checking whether MDP horizon unary notation succinctpolicy expected reward greater equal k, size bounded z,PSPACE-hard, z binary notation.Membership problem PSPACE appears simple prove.surprising case stationary policies, question still openproblem bound size. However, problem history-dependentpolicies bound size known PSPACE (incidentally, problemstationary policies infinite horizon EXPTIME-complete, proved Littman, 1997.)proof result cannot however modified cover addition boundsize. Intuitively, looking policy given reward, alsogiven size. constraint reward somehow easier check, done locally:expected reward state obtained summing reward possible nextstates. hand, size circuit representing policy cannot checkedwhole circuit determined, circuit exponentially large.proved section EXPTIME-hardness problemhistory-dependent policies related open conjecture computational complexity(P=PSPACE). Namely, proved P = PSPACE implies problem P.result rephrased as: problem P, P 6= PSPACE. Since EXPTIMEhard problems P, problem EXPTIME-hard, P 6= PSPACE.conclusion really unlikely: contrary, believed true.hand, proving problem EXPTIME-hard least hard solving conjectureopen twenty years. intermediate step, provedP = PSPACE implies existence polynomially-sized history-dependent policiesMDPs.Theorem 10 P = PSPACE, every succinct MDP horizon unary notationoptimal succinct history-dependent policy size polynomial size MDPhorizon.571fiLiberatoreProof. Let us consider state results sequence actions. Since consideredpolicies history dependent, possible find optimal choices pointtaking current state new initial state MDP, reducing horizon,determining optimal policy modified MDP.Since problem checking whether MDP policy given rewardPSPACE (Mundhenk et al., 2000), P assumption. binary search, possible determine expected optimal reward MDP polynomial time. Sinceexpected optimal reward state computed finding expected optimalreward MDP, problem polynomial well. function determinesoptimal expected reward state therefore represented polynomial circuit.best action state one leads best possible next states.determined polynomial space checking, action, possible next states,determining expected reward. Therefore, best action execute PSPACE,therefore P assumption. result, optimal policy representedpolynomial circuit.following corollary immediate consequence.Corollary 4 P = PSPACE, problem existence succinct history-dependentpolicy given expected reward size bound binary notation P.Proof. P = PSPACE, MDP optimal history-dependent policy polynomialsize. problem therefore solved iterating possible circuits whose sizebounded polynomial. problem therefore solved polynomialamount memory, therefore PSPACE. assumption, P well.first look, corollary seems prove problem PSPACE. However,prove result. shown following related example: problemp2 -complete P P = NP; however, generally believed p2 -completeproblems NP.easy consequence result EXPTIME-hardness problem wouldimply P 6= PSPACE. Indeed, problem EXPTIME-hard P,two classes known different thanks theorem Hartmanis Stearns(1965). result, cannot P=PSPACE, proved implyproblem P. words, problem history-dependent policy existencesize bound binary notation EXPTIME-hard P 6= PSPACE.8. Conclusionsalways possible represent optimal policies succinct MDP using circuitssize polynomial MDP horizon. result affects choicepolicies generated executed. Indeed, planning nondeterministic scenariodone two ways:1. Determine actions execute possible states (i.e., determinewhole policy); state, corresponding action executed;572fiOn Polynomial Sized MDP Succinct Policies2. Determine best action execute initial state only; execute observeresulting state; find best action new state, etc.Many algorithms MDPs find representation whole policy,solve next-action problem directly (Kearns, Mansour, & Ng, 2002). result formallyproves optimal policy cannot always represented polynomial space, unlesspolynomial hierarchy collapses. result holds existing algorithms(such value iteration policy iteration), also algorithm findswhole policy once.second solution theoretically optimal, involves finding best actiontime step, problem hard, proved Theorem 1. advantagefirst solution hard step find optimal policy; finding actionexecute state polynomial.impossibility always representing optimal policies polynomial space raisesnew problem: since size physical memory bounded, feasible searchbest among policies, among possible store, is,bounded size available memory size. problem checking whether succinctMDP succinct policy given size reward proved NPPP -complete,therefore slightly easier problem without bound, PSPACEhard (Mundhenk et al., 2000). similar result proved slightly differentformalization non-deterministic planning Littman et al. (1998).complexity result holds policies represented particular succinctform, circuits giving next action current state (possibly)history. Nevertheless, different representations policies lead different results. Namely,algorithms actually find value function (a function determining expected rewardstate), considered representation policy statesresult execution action listed polynomial time. particular,algorithms based value iteration, applied form decomposition, findsuccinct representation value function. Finding succinct representationfunction, given bound size, proved easier finding succinctpolicy, states result execution action listed polynomialtime: p2 -complete.result two consequences: first, problem algorithms solvesmaller complexity class problem bound size; second, policiesrepresented polynomial space, associated value functions cannot:exists trade-off complexity ability finding good solutions. resultalso interesting characterizes complexity family algorithms (thosedetermining value function succinct form) rather complexityproblem. result therefore kind efficiency analysis singlealgorithm (e.g., big-O running time) inherent complexity problem (e.g.,NP-completeness). such, similar results complexity proof procedures(Beame & Pitassi, 1998; Egly & Tompits, 2001). analysis however limitedcase exact value functions. Approximation interesting open problem. effectsvarious restrictions done MDP interesting open problemswell.573fiLiberatoreanalysis bound size initially done assuming boundpolynomial size instance. justified fact resultingpolicy much larger MDP. However, moderate increase maytolerated. Therefore, considered problem removing assumptionpolynomiality bound. new problem, PSPACE-hardproblem bound size is. However, EXPTIME-hard unless P 6= PSPACE.result shows proving hardness result least hard proving conjectureopen twenty years.Let us discuss results presented paper relate similar onesliterature. already remarked Introduction, complexity problems relatedfinding policy necessarily imply policies cannot compactly represented.Namely, even result undecidability forbid compactness policies. Therefore,result implied previous complexity results literature.hand, result non-polynomiality size policies POMDPs already appearedpaper Papadimitriou Tsitsiklis (1987). Namely, proved that, unlessPSPACE = p2 , algorithm mapping POMDPs explicit formstrings polynomial length used compute optimal action.result basically proves non-polynomiality policies. However, cannot imply ours,holds POMDPs explicit representation non-positive rewards;problem PSPACE-hard formalization polynomial (Mundhenk et al.,2000). precisely, two results cannot derived other. relatedresults literature non-representability (as opposite compactrepresentability studied paper): Littman (1997) shown (infinite horizon)plan existence EXPTIME-complete, Littman et al. (1998) shownproblem restricted looping plans PSPACE-complete: result, infinite-horizonpolicies cannot represented looping plans unless EXPTIME=PSPACE.review period paper, different proof impossibility representing optimal policies MDPs polynomial space published (Allender,Arora, Kearns, Moore, & Russell, 2002). new proof improves ones presentedpaper conference version (Liberatore, 2002) two ways: first, condition new result holds PSPACE 6 P/poly instead NP 6 P/polyPSPACE 6 NPPP ; second, proof holds even approximately optimal policies.differences (the new proof infinite horizon discount factor, rewardfunction linear, two-levels Bayes nets used instead circuits) inessential, i.e.,proofs modified way affected differences.current paper also contains results problem bound policy valuefunction.Let us consider complexity results MDPs literature.problem deciding whether succinct MDP horizon unary notation policy given reward PSPACE-hard, PSPACE-complete history-dependentpolicies. problem bound unary notation size policyNPPP -complete. class contains class PPP , turn contains whole polynomial hierarchy. Therefore, problem NPPP -complete hard class574fiOn Polynomial Sized MDP Succinct Policiespolynomial hierarchy. means bound size unary notation decreasecomplexity problem much. hand, bounding size valuefunction representation decreases complexity more, problem p2 -complete.conclude, observe negative results (impossibility polynomial policieshardness results) hold POMDPs, since MDPs special cases POMDPseverything observable. results, however, apply casePOMDP policy succinct form. case explicit representationstudied Papadimitriou Tsitsiklis (1987) (their results discussed above),Mundhenk (1999), considered problem deciding whether POMDP explicitform c-small policy, given c, c-small-ness includes bound size dependingc.AcknowledgmentsMany thanks Michael Littman anonymous referees suggestions. Partwork appeared proceedings Eighteenth National Conference ArtificialIntelligence (AAAI-2002).ReferencesAllender, E., Arora, S., Kearns, M., Moore, C., & Russell, A. (2002). note representational incompatability function approximation factored dynamics.Proceedings Sixteenth Annual Conference Neural Information ProcessingSystems (NIPS 2002).Beame, P., & Pitassi, T. (1998). Propositional proof complexity: Past, presentfuture. Tech. rep. 067, Electronic Colloquium Computational Complexity,http://www.eccc.uni-trier.de/eccc/.Bellman, R. (1957). Dynamic Programming. Princeton University Press.Boppana, R., & Sipser, M. (1990). complexity finite functions. van Leeuwen,J. (Ed.), Handbook Theoretical Computer Science, Vol. A, chap. 14, pp. 757804.Elsevier Science Publishers (North-Holland), Amsterdam.Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,11, 194.Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (2002). Preprocessing intractableproblems. Information Computation, 176 (2), 89120.Cassandra, A., Littman, M., & Zhang, N. (1997). Incremental pruning: simple, fast, exactmethod partially observable Markov decision processes. ProceedingsThirteenth Conference Uncertainty Artificial Intelligence (UAI97), pp. 5461.Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.Computational Intelligence, 5 (3), 142150.575fiLiberatoreDearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning. Artificial Intelligence, 89 (1), 219283.Egly, U., & Tompits, H. (2001). Proof-complexity results nonmonotonic reasoning. ACMTransactions Computational Logic, 2 (3), 34038.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryNP-Completeness. W.H. Freeman Company, San Francisco, Ca.Hansen, E., & Feng, Z. (2000). Dynamic programming POMDPs using factored staterepresentation. Proceedings Fifth International Conference ArtificialIntelligence Planning Systems (AIPS 2000), pp. 130139.Hartmanis, J., & Stearns, R. E. (1965). computational complexity algorithms.Trans. Amer. Math. Soc. (AMS), 117, 285306.Johnson, D. S. (1990). catalog complexity classes. van Leeuwen, J. (Ed.), HandbookTheoretical Computer Science, Vol. A, chap. 2, pp. 67161. Elsevier Science Publishers(North-Holland), Amsterdam.Karp, R. M., & Lipton, R. J. (1980). connections non-uniform uniformcomplexity classes. Proceedings Twelfth ACM Symposium TheoryComputing (STOC80), pp. 302309.Kearns, M., Mansour, Y., & Ng, A. (2002). sparse sampling algorithm near-optimalplanning large markov decision processes. Machine Learning, 49 (23), 193208.Koller, D., & Parr, D. (1999). Computing factored value functions policies structuredMDPs. Proceedings Sixteenth International Joint Conference ArtificialIntelligence (IJCAI99), pp. 13321339.Liberatore, P. (2001). Monotonic reductions, representative equivalence, compilationintractable problems. Journal ACM, 48 (6), 10911125.Liberatore, P. (2002). size MDP factored policies. Proceedings EighteenthNational Conference Artificial Intelligence (AAAI 2002), pp. 267272.Littman, M. (1997). Probabilistic propositional planning: representations complexity. Proceedings Fourteenth National Conference Artificial Intelligence(AAAI97), pp. 748754.Littman, M., Dean, T., & Kaebling, L. (1995). complexity solving Markovdecision processes. Proceedings Eleventh Annual Conference UncertaintyArtificial Intelligence (UAI95), pp. 394402.Littman, M., Goldsmith, J., & Mundhenk, M. (1998). computational complexityprobabilistic planning. Journal Artificial Intelligence Research, 9 (1), 136.Madani, O., Hanks, S., & Condon, A. (1999). undecidability probabilistic planninginfinite-horizon partially observable Markov decision problems. ProceedingsSixteenth National Conference Artificial Intelligence (AAAI99), pp. 541548.Mundhenk, M. (1999). complexity optimal small policies. Tech. rep. 9922, UniversityTrier.576fiOn Polynomial Sized MDP Succinct PoliciesMundhenk, M., Goldsmith, J., Lusena, C., & Allender, E. (2000). Complexity finitehorizon Markov decision processes problems. Journal ACM, 47 (4), 681720.Papadimitriou, C., & Tsitsiklis, J. (1987). complexity Markov decision processes.Mathematics Operations Research, 12 (3), 441450.Puterman, M. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons.Stockmeyer, L. J. (1976). polynomial-time hierarchy. Theoretical Computer Science,3, 122.Zhang, N., & Zhang, W. (2001). Speeding convergence value iteration partiallyobservable Markov decision processes. Journal Artificial Intelligence Research, 14,2951.577fiJournal Artificial Intelligence Research 21 (2004) 595-629Submitted 7/03; published 5/04Concurrent Auctions Across Supply Chainmosheb@cs.huji.ac.ilnoam@cs.huji.ac.ilMoshe BabaioNoam NisanSchool Computer Science Engineering,Hebrew University Jerusalem, Jerusalem 91904, IsraelAbstractrecent technological feasibility electronic commerce Internet,much attention given design electronic markets various typeselectronically-tradable goods. markets, however, normally need functionrelationship markets related goods, usually downstream upstream supply chain. Thus, example, electronic market rubber tirestrucks likely need strongly inuenced rubber market welltruck market.paper design protocols exchange information sequence marketsalong single supply chain. protocols allow markets function separately, information exchanged ensures ecient global behavior across supplychain. market forms link supply chain operates double auction,bids one side double auction come bidders correspondingsegment industry, bids side synthetically generatedprotocol express combined information links chain. doubleauctions markets several types, study several variantsincentive compatible double auctions, comparing terms eciencymarket revenue.1. Introductionrecent rush towards electronic commerce Internet raises many challenges,technological conceptual. paper deals conceptual challenge coordination electronic markets. Let us look years technological futureelectronic commerce. seems likely following two key challengesadequately solved industry:Supply Chain Integration: enterprise information systems businessesable securely eciently share information inter operate information systems suppliers, customers, partners.Electronic Markets: Ecient, sophisticated, robust liquid electronic marketsavailable trade goods segments industry. marketsinteractively respond changes supply demand, dynamically changingtrade quantities prices.interested conceptual question markets related goodsshare information. Consider, example, ctional market rubber tires trucks,two related markets rubber trucks. One imagine followingc2004AI Access Foundation. rights reserved.fiBabaioff & Nisansimplied supply chain forming: rubber manufacturers placing sell bids rubberrubber market; tire manufacturers placing buy orders rubber market sell bidstire market; truck manufacturers placing buy bids tire market sellingtrucks truck market; nally customers bidding trucks. One would expectcombination markets together information systems manufacturersable automatically respond markets changes economically ecient way.Thus, example, surge demand certain type trucks, raise pricetruck market causing manufacturers type truck automatically decideincrease production, consequently automatically raising electronic bids tires.This, turn, may increase tire prices tire market, etc., etc., leading, eventuallyindirectly, still completely automatically, increased rubber production rubbermanufacturers.Let us emphasize: process described occurs, slowly, normal human tradecombined eects large number self-interested decisions many peopleinvolved supply chain. desire combination participatinginformation systems electronic markets automatically (without human control)rapidly (within time-frames electronic commerce), reach similar resultseven economically ecient ones humans usually achieve. resultsachieved despite fact information systems manufacturers stillself-interested optimizing companys prot global economic eciency.Seeing invisible hand function normal human economic activity, one wouldcertainly expect electronic markets reach types results. However, key conceptual design challenge emerges bidders must concurrently activeone market. Tire manufacturers must concurrently participate buyers rubber market sellers tire market. Clearly, quantity rubber wish buygiven price determined amount tires sell given price. Thus,price bid buying rubber must intimately related price bid sellingtires increase one lead corresponding increase other.theoretically impossible dene suggested bid one without other. Thus,two markets operate independently, tires manufacturers able reasonablyparticipate them. operate sequentially, say rst rubber boughtafterwards tires sold, serious exposure problem emerges: tire manufacturers mustconservative bids rubber, know advance priceget tire market.One approach handling inter-dependence markets run completesupply chain single complex huge market. Conceptually spiritrecently popular vertical markets vertical portals try vertically integrateinformation trade complete vertical segment industry. integrationmarkets single complex market results complex optimization problem,research done address problems pure optimization problems.centralized solution obvious advantages, also problematic due necessityconcentrating information, communications, decision making single point.centralization information problematic sense distributed computingsystems economic sense.596fiConcurrent Auctions Across Supply Chainpaper suggest alternative approach: supply chain organizedsequence separate markets (which act strategically) communicate amongusing xed protocol create distributed mechanism (for general overviewdistributed algorithmic mechanism design, refer reader Feigenbaum & Shenker,2002). similar approach suggested Walsh Wellman (2003) Walsh,Wellman, Ygge (2000), formulate general problem NP-complete obtainsolutions provably ecient, either computational sense economicsense. much simpler problem linear supply chain considered Bosch-DomenechSunder (1999), provably ecient protocol suggested. also considerlinear supply chain problem, obtain computationally ecient protocols achieveprovably high economic eciency budget balance (or full eciency budget decit).protocols, intermediate markets along chain transform one good another.Thus, example, tire manufacturers place bids operation transforming unitrubber tire. protocol dierent markets assures dierentmarkets reach compatible decisions, i.e. amount rubber units tiresallocated equal amount rubber manufactured amount tiresneeded. Furthermore, amounts achieve global economic eciency across supplychain. Finally, result assume manufactures information systemsglobal knowledge behavior, beyond knowledge cost structureself-interested (rational) behavior.paper focuses case simple linear supply chain discrete units goods,manufacturer able transform single unit one good single unitanother good, incurring cost transformation. consumer obtains valueacquiring single unit nal good. assume agent quasi-linearutility function, goal maximize utility.markets takes form double auction (see Friedman & Rust, 1991,study double auction markets), consider several variants double auctions.variants address four issues: Incentive Compatibility (IC): double auction rulesmotivate self-interested agents (the manufacturers consumers) reveal costs /values truthfully (we use standard notions dominant strategies IC MechanismDesign literature; Mas-Collel, Whinston, & Green, 1995; Osborne & Rubinstein, 1994).Individual Rationality (IR): Every agent strategy ensures non-negative utility,agents participate voluntarily (Ex-Post individual rationality). Economic Eciency:desired outcome optimize sum valuations participants. Budget Balance(BB): payment buyer necessarily equal payment receivedseller, wish ensure market mechanism subsidizetrade. Thus, total payment buyers least total amount givensellers. Myerson Satterthwaite (1983) impossibility result, four conditionscannot apply simultaneously. paper Parkes, Kalagnanam, Eso (2001) presentsmechanisms combinatorial exchange BB IR fairly ecient fairlyincentive compatible. take dierent approach consider variants alwaysIR IC trade-o last two conditions eciency budget balance.deterministic rule previously suggested McAfee (1992), but, surprisingly, ruleturned compatible supply-chain protocols. Therefore suggest twonew randomized double auction rules used supply-chain protocols,597fiBabaioff & Nisanobtain budget balance surplus slight loss eciency. also providesimulation results comparing eciency budget surplus dierent variantsdouble auctions.main contribution paper description two alternative protocolsallow supply chain mechanism formed double auctions operate eciently.protocols computationally ecient terms communication computation time.prove protocols applied specic double auction rule (theTrade Reduction rule), resulting system exhibits following three key properties:1. Ex-post Individual Rationality Incentive Compatibility (in sense dominantstrategies).2. High global economic eciency (not optimal, good underlying doubleauction guarantees high fraction ecient allocation value).3. Budget Balance.new double auctions, even higher global economic eciency achievedexpectation, expected (ex-ante) budget balance rather worst case (ex-post).protocols use VCG (Vickrey, 1961; Clarke, 1971; Groves, 1973) doubleauction rule ecient, maintain IR IC achieve full economic eciencyentire supply chain, budget decit.Clearly stylized model general supply chain model farsimple realistic model. purpose analyzing model present rst steptowards mechanism design approach supply chain problems. far knowrst design mechanism IC, IR, BB highly ecient supply chainmodel.rest paper structured follows. Section 2 give complete selfcontained example organization simple supply chain demonstratetype calculations information transfer one protocols. Section 3present model conditions incentive compatibility. Section 4 summarizeproperties several variants incentive-compatible double auctions present twonew randomized double auction rules. Section 5 present two alternative protocolssupply chain coordination markets, prove properties achievedprotocols, Section 6 conclude directions future work.2. Lemonade-Stand IndustryCharlie Brown decided draw vast experience lemonade stand industrytransform whole industry bringing online Internet. Charlie Brownalready struck partnerships strategic players three core segmentsindustry:Lemon Pickers: Alice, Ann, Abe pick lemon neighborhood lemontree (one lemon maximum per day).Lemonade Squeezers: Bob, Barb, Boris know squeeze single lemonmake glass lemonade (one glass maximum per day).598fiConcurrent Auctions Across Supply ChainLemonade Consumers: Chris, Carol, Cindy want buy one glass lemonade(per day).Charlie Brown obtained preliminary version paper built Internetsystems accordingly, using Symmetric Protocol suggested here. Charlie Browncreated three communicating electronic markets: lemon market Alice,Anna, Abe sell lemons. squeezing market Bob, Barb, Borisoer squeezing services, juice market Chris , Carol, Cindy buylemonade.rst day operations participants logged marketentered bid: Alice asking $3 order pick lemon, Ann wanted $6,Abe (who living farthest tree) wanted $7. Bob, Barb, Boris askingfor, respectively, $1, $3, $6 order squeeze lemon, Chris, Carol, Cindy,willing pay, respectively, $12, $11, $7 glass lemonade. Figure 1presents three markets, supply curves lemon market (S L ) squeezingmarket (S LJ ), demand curve juice market (DJ ). Knowing auctionworks optimal bidding strategy report true value cost, agentreports truthfully. Let us follow operation system see manages reachsocially ecient allocation decide many lemons picked squeezedlemonade.Lemon MarketSupply Demand(S L )(DL )367Squeezing MarketSupply Demand(S LJ )(DLJ )136Juice MarketSupply Demand(S J )(DJ )12117Figure 1: Supply Chain bidsrst stage, markets send information two phases.rst phase, lemon market aggregates supply curve lemons, L , sendsinformation squeezing market. squeezing market aggregates supply curvesqueezing services, LJ , adds vector, point-wise, L , sends sumjuice market. juice market, sum represents supply curve juice, J ,aggregated complete supply chain. seen Figure 2, cost rstglass lemonade $4, since rst lemon cost $3 squeezing operation cost$1.second phase, juice market sends demand curve juice, DJsqueezing market, subtracts it, point wise, supply curve squeezing services, sending dierence vector lemon market, interpreteddemand curve lemons, DL , aggregated complete supply chain. seenFigure 3, demand rst lemon $11, since rst glass lemonadedemand $12 squeezing operation cost $1.net demand curve squeezing services, DLJ calculatedsqueezing market DJ L . seen Figure 4, demand rst599fiBabaioff & NisanLemon MarketSupply Demand(S L )(DL )367Squeezing MarketSupply Demand(S LJ )(DLJ )136SLJuice MarketSupply Demand(S J )(DJ )412911137L +S LJFigure 2: Supply Chain supply graphs propagationLemon MarketSupply Demand(S L )(DL )3116871Squeezing MarketSupply Demand(S LJ )(DLJ )136J LJJuice MarketSupply Demand(S J )(DJ )412911137DJFigure 3: Supply Chain demand graphs propogationsqueezing operation $9 since rst glass lemonade demand $12 rstlemon cost $3. means lowest cost squeezing operation less$9, least one glass lemonade manufactured.Lemon MarketSupply Demand(S L )(DL )3116871Squeezing MarketSupply Demand(S LJ )(DLJ )193560Juice MarketSupply Demand(S J )(DJ )412911137Figure 4: Supply Chain constructing supply demand graphspoint three markets supply curve demand curve,market conduct double auction.Internet startup, Charlie Brown decided subsidize trademarkets, ignoring sections paper aim eliminate budget decitmarkets. market thus uses VCG (Vickrey, 1961; Clarke, 1971; Groves, 1973)double auction rule1 , derived Vickrey, Clarke Groves general auction scheme.VCG double auction picks highest value allocation market, winnersagents socially ecient allocation. ecient allocation marketincludes two highest value (lowest cost) bidders, since two tradespositive gain, third trade negative gain. VCG double auction charges1. formal denition VCG double auction rule appears Section 4600fiConcurrent Auctions Across Supply Chainconsumer (demand side) minimal value agent must bid orderecient allocation. Similarly, supplier (supply side) receives payment equalsmaximal cost may bid still ecient allocation. payment takesaccount competition agents class competitivenessbid respect bids agents classes. payment scheme,best strategy agent bid truthfully. lemon market, two lemons sold(by Alice Ann) $7 = min($7, $8) (in order win, must bid cost lowercost Abe, rst non-winning lemon supplier, $7. must also bidcost lower $8, otherwise cost lemon high match lowestdemand lemon). squeezing market, two squeezing contracts awarded (toBob Barb) $5 = min($6, $5) (since one bids cost higher $5,demand squeezing operation, squeezing operation cost Boris$6, higher $5). juice market, VCG rule awards two glasseslemonade (to Chris Carol) price $9 = max($7, $9) (since must bidleast $9 match supply juice, do, also defeat Cindy).Charlie Brown thrilled: dierent markets reached allocationamount, 2, which, veried indeed social optimum: Societys net gainstrade system (12 + 11) (1 + 3) (3 + 6) = $10, cant beaten. CharlieBrowns investors somewhat worried fact system subsidized every glasslemonade $3 (= 7 + 5 9), Charlie Brown assures changing doubleauction rules one double auction rules suggested paper leadbudget balance even surplus, maintaining high social gain. nine tradingpartners evaluated carefully operation chain markets assuredbest served always bidding true cost structure.3. IC Mechanisms Single-Minded AgentsSection 3.1 presents abstract model agents partition set outcomes two,case win case loss. agent valuation casewins, valuation represented single parameter. double auction modelsupply chain model consider special cases model. main theoremsection presents necessary sucient conditions incentive compatibilityabstract model. Mualem Nisan (2002) presented theorem contextcombinatorial auction agents desire one specic bundle publicly known(the Known Single Minded Model). theorem derived general resultagents private bundles (the Single Minded Model), proved Lehmann, OCallaghan,Shoham (2002). Similar result dierent general model one-parameter agentsArcher Tardos (2001) appears Computer Science literature. completeness,present results relevant single-minded agents model.section 3.2 farther restrict model single-minded agents casesets agents replace other, characterize non-discriminating mechanismscase.601fiBabaioff & Nisan3.1 General ModelSingle-Minded agents model, nite set agents N set outcomesO. agent N , partitioned two disjoint sets OiW OiL , outcomesWins outcomes Loses, respectively (this partition publicknowledge). ordered set values space Vi case wins. Agentprivate value vi Vi OiW (he wins) value 0 OiL(he loses). private values private information, informationpublicly known participants mechanism (and known restinformation public). technical reasons assume vi1 , vi2 Vi ,vi1 > vi2 , exist vi3 Vi vi1 > vi3 > vi2 (For example Vi setReal numbers Rational numbers). agent wins pays pi quasi-linearutility ui = vi pi , normalized mechanisms pays 0 0 utility loses.assume agents self-interested try maximize utility. model,outcome one-to-one mapping set winners called allocationA, = {i N |o OiW }. say thatthe allocation ecient iA vi maximized.vieciency allocation iA v , ecient allocation. Let ViAset possible values agents, V = Vi . work mechanismsagents required report values, mechanism decides allocationpayments agents deterministic way. reported value bi Vi agentcalled bid agent might dierent private value vi . Let b Vbids agents. allocation rule R decides allocation according reportedvalues b V , R function R : V O. payment rule P decides payment pi agenti, P function P : V RN . mechanism denes allocation payment rule,= (R, P ). Mechanism Budget Balanced (BB) pi 0 bids b V .Incentive-Compatible (IC) dominant strategies agent i, bidding vi maximizesutility possible bids agents. normalized losing agentspayment (and utility) 0. (ex-post) Individually Rational (IR) agentvalue vi , bid bi bids bi , ui 0 possible bidsagents. Note normalized incentive-compatible mechanism individuallyrational (since truthful bidding ensures non negative utility).present necessary sucient conditions mechanism incentivecompatible dominant strategies single-minded agents model. conditionslater used prove properties double auctions supply chain mechanismscreated protocols.bids b V denote b = (bi , bi ) bi bids agents i.Denition 1. Allocation rule R Bid Monotonic bids b V , agenttwo possible bids i, bi > bi , allocation R(bi , bi ) alsoallocation R(bi , bi ).bid monotonic allocation rule ensures winning agent becomes loserimproving bid. following observation direct result denition.Observation 3.1. Let R bid monotonic allocation rule, let b V set bidsagents let agent bid bi .602fiConcurrent Auctions Across Supply Chainexists bid bi allocation R(bi , bi ), existscritical value Ci wins bids bi > Ci loses bids bi < Ci (Ciindependent bid bi )2 .Note bid equal critical value, observation sayagent wins not.Lemma 3.2. Let normalized IC mechanism allocation rule R. RBid Monotonic.Proof. Assume contradiction auction normalized IC (thus also IR)allocation bid monotonic. exists agent two values bH > bL ,wins auction pays PL bids bL , loses auction pays zerobids bH . mechanism IR therefore bL PL 0. true value bHgain misreporting private value: reports true value loses auctionutility zero, bids bL wins auction pays PL . case utilitybH PL > bL PL 0 contradiction assumption auction IC.Theorem 3.3. normalized mechanism allocation rule R IC RBid Monotonic trading agent pays critical value Ci (pi = Ci ).Proof. Case if: Assume normalized, R bid monotonic agent value vipays Ci wins. rst show auction IR, show receives non-negativeutility bidding truthfully. loses, pays zero zero utility. winsauction bidding truthfully, Observation 3.1, vi Ci = pi , hence utilityvi pi = vi Ci 0.prove IC prove cannot improve utility misreporting value. Consider case agent wins auction bidding true value vi . bidsuntruthfully loses, gets zero utility, IR cannot betterutility truthful bid. bids untruthfully wins auction, since stillpays critical value Ci utility remains same.consider case loses auction bidding truthfully. utilityzero vi Ci Observation 3.1. bids untruthfully loses, utility remainszero. bids untruthfully wins, utility vi Ci 0. cases, shownagent cannot improve utility bidding untruthfully, thus provingIC.Case if: Assume auction normalized IC (thus also IR). Lemma 3.2allocation rule bid monotonic, need prove agent value vimust pay critical value Ci . Assume pi = Ci agent i. pi > Ci ,pi > vi > Ci bids truthfully, Observation 3.1 wins utilityvi pi < 0 contradicts individual rationality. pi < Ci , pi < vi < CiObservation 3.1 loses zero utility bids truthfully. misreports valuebidding bi > Ci , would win auction utility vi pi > 0 contradictsincentive compatibility.normalized IC mechanisms, allocation rule bid monotonicuniquely denes critical values agents thus payments.2. Ci = always wins603fiBabaioff & NisanObservation 3.4. Let 1 2 two normalized IC mechanismsallocation rule. 1 2 must payment rule, means 12 mechanisms.3.2 Non-Discriminating Mechanismsinterested subclass mechanisms single-minded agentsdiscriminate agents roll outcome. agentsalways replace other.Denition 2. Agent class agent j, every allocationj/ A, allocation = \ {i} {j} also O, every allocation/ j A, allocation = \ {j} {i} also O.Note class agent equivalence class.Denition 3. Let mechanism, class agents i, j two agentsclass . non-discriminating identity bids b V , wins bidsbi bj > bi j also wins.non-discriminating pricing bids b V every classvalue pT , agent class bid bi bi > pT wins pays pT .non-discriminating (ND) non-discriminating identity nondiscriminating pricing.Non-discriminating mechanisms also called Envy-Free mechanisms GoldbergHartline (2003), since loser envies winner fact auctionprice paid. paper consider mechanisms non-discriminatingidentity, means agents class picked bid order,high low. Also note IR non-discriminating pricing agentclass bids pT must lose auction (otherwise negative utilitycontradicting IR).following observation direct result Theorem 3.3.Observation 3.5. Let normalized mechanism IC ND. Let pT paymentwinners class . critical value winning agents class pT .normalized, IC ND mechanisms, prove payment winnersclass independent bids.Lemma 3.6. Let normalized IC ND mechanism. every agent class ,agent j class wins pays pT bids bj > pT winning agent classbids bi pT , j also wins pays pT bids bi > bi .Proof. Since normalized IC, Theorem 3.3 bid monotonicObservation 3.5 pT critical value j. Observation 3.1 pT independentbid bi , must pay pT bids bi > bi . Since non-discriminating, j mustalso win (since bj > pT ) pay pT .604fiConcurrent Auctions Across Supply Chain4. Incentive Compatible Double Auctionsmarkets along supply chain performs double auction (for studydouble auctions see Friedman & Rust, 1991). protocols supply chains workwide variety double auction rules, section present several doubleauction rules suggested literature, two new randomized double auctionrules later use create supply chain mechanisms. double auction rule decidestrading agents payments agents.rst give description Double Auction (DA) model. single-mindedagents model following specications possible outcomes, value spaces classesagents. homogeneous good g traded discrete quantities,agents one two classes agents. agent either seller buyer oneunit good g. Seller single unit g non-negative cost si (vi = si )sells unit. Buyer non-negative value bi (vi = bi ) receives one unitgood g. set possible allocations set materially balanced allocations,allocations number sellers buyers.DA begins agents reporting values one unit good. sellerreports cost Si might dierent real cost si . buyer reports valueBi might dierent real value bi . DA rules non-discriminatingidentity, rst construct supply demand curves sorting supply bidss.t. S1 S2 . . . demand bids s.t. B1 B2 . . .. agents report truthfullyorder maximize eciency, trade quantity q set, trading agents mustrst q sellers rst q buyers. optimal trade quantity l denedmaximal Bl Sl . Trading l units maximizes eciency agents bidsagents true values. double auctions consider non-trading agents pay zero.setting trade quantity optimal trade quantity, real markets proceedchoosing market clearing price anywhere [Sl , Bl ]. example setting price(Bl + Sl )/2 1/2-Double Auction special case k-DoubleAuction (Wilson, 1985; Chatterjee & Samuelson, 1983; Satterthwaite & Williams, 1989).general, k-Double Auction double auction auction begins,parameter k chosen k [0, 1]. k used calculate clearing price P =k Sl + (1 k) Bl . l units good traded uniform price P sellersbuyers. pricing scheme incentive compatible (in dominant strategies),additional strong assumptions agents values (all known distribution),shown perform reasonably well many cases strategic behaviorparticipants (Rustichini, Satterthwaite, & Williams, 1994; Satterthwaite & Williams,1991). Nevertheless, even assuming uniform distribution agents values,non zero eciency loss Bayesian Nash equilibrium achieved (and equilibriumconcept much weaker dominant strategies equilibrium discuss paper).One may alternatively use VCG Double Auction rule, also sets tradequantity optimal trade quantity l. rule non-discriminating, l winning buyerspay pB = max(Sl , Bl+1 ) l winning sellers receive pS = min(Sl+1 , Bl ). VCG mechanisms(Vickrey, 1961; Clarke, 1971; Groves, 1973) general, VCG DA mechanismspecial case, IR IC. Incentive compatibility leads maximal eciency sincetrade optimal size l, also leads budget decit since pB pS .605fiBabaioff & NisanObservation 3.4 normalized, IC ecient mechanism must paymentsVCG DA thus mechanism budget-balanced.Myerson Satterthwaite (1983) shown long individual rationalityparticipation constraints met (e.g. non-traders pay 0), incentive compatiblemechanism always achieves ecient outcome lead budget decit (evengeneral model weaker solution concept consider). turn lookseveral normalized IC double auction rules achieve budget balance (sometimeseven surplus) price achieving slightly sub-optimal eciency (by reducing tradequantity).simplest auction property Trade Reduction (TR) DA.non-discriminating auction, l 1 units good traded. trading buyer payspB = Bl , trading seller receives pS = Sl . Since Bl Sl auction BB.rule extension Trade Reduction DA rule, previously suggested+BMcAfee (1992). McAfees DA rule, suggested clearing price p = l+1 2 l+1accepted l buyer seller (p [Sl , Bl ]) l units good tradedprice p, otherwise TR rule used. Trade Reduction DA rule McAfeesrule normalized, bid monotonic payments critical values, thusIC (and IR) applying Theorem 3.3. rules eciency least (l 1)/lagents values, since unit lowest trade value might traded.McAfees rule turned compatible protocols expandappendix A.suggest two new normalized randomized double auctions, capturetradeo auction eciency budget balance one parameter . UnlikeMcAfees rule, rules used supply-chain protocols, achievehigher eciency Trade Reduction DA expectation. carefully chosenparameter also achieve exact budget balance (zero balance) expectation (Ex anteBudget-Balanced).Reduction DA, xed 0 1, bids submittedprobability Trade Reduction DA rule used, probability 1VCG DA rule used. randomized double auction universally incentive compatible(dened Nisan & Ronen, 1999), means agents bid truthfully evenknow randomization result. seen fact bidding truthfullydominant strategy Trade Reduction DA rule VCG DA rule. alsonormalized ND since Trade Reduction DA VCG DA normalized ND.Payment DA another randomized double auction distribution allocation former auction (and therefore expected eciency),payment agent expected payment Reduction DA (soexpected budget auctions same). auction, parameter chosenReduction DA. bids submitted allocation paymentsdecided. l 1 units traded buyers, paying Bl + (1 ) max(Bl+1 , Sl ),sellers, receiving Sl + (1 ) min(Sl+1 , Bl ). probability anotherunit good traded l buyer pays max(Bl+1 , Sl ) l sellerreceives min(Sl+1 , Bl ). auction discriminating since l buyer pays dierent price rest winning buyers wins auction (and holdssellers). Payment DA much lower variance agents payments606fiConcurrent Auctions Across Supply ChainReduction DA, hides randomized character almostagents. allocation payment agents l buyer seller independentcoin toss, thus zero variance. auction individually rational incentivecompatible; universally.Theorem 4.1. Payment DA incentive compatible randomized DA,universally incentive compatible randomized DA.Proof. Since Reduction DA universally incentive compatible, truth telling maximizes expected utility agent Reduction DA. Payment DAprobability distributions allocation payments Reduction DA,therefore truth telling maximizes expected utility agent Payment DAwell.Payment DA universally incentive compatible randomized DA. Assumeseller one rst l 1 sellers knows randomization resultVCG allocation, facing deterministic auction. Observation 3.4unique payment rule ensures dominant strategy IC normalized mechanismecient allocation (the VCG allocation), VCG payment rule.Payment DA normalized gives dierent payment agent, therefore cannotincentive compatible dominant strategies.auctions eciency least (l 1)/l, since least l 1 l unitstraded. grows zero one, expected revenue increases negativepositive expected eciency decreases (linearly). distribution agentsvalues known prior beginning auction, parameter = (D)chosen expected revenue zero (dierent distributions dierent values(D), paper solve problem nding (D) analytically). denotevalue (D). uniform distribution agents values [0, 1] denotevalue .Table 1 present summary properties double auction rules.DA rulek-DAVCGTrade Reduction(D) Reduction(D) P aymentMcAfeeIncentive compatibleyesyesyes, universallyyes, universallyyesRevenue0decitsurplus0 (expected)0 (expected)surpluseciency loss00LFT(D) LF (expected)(D) LF (expected)LFTTable 1: Double Auction Rules comparison tableNotes : a) k-DA assume agents bid truthfully, not, non zero eciencyloss equilibrium. b) LFT means Least Favorable Trade Bl Sl .run simulations comparing dierent DA rules respect marketrevenue ( pi ) total social eciency ( iA vi ). Figures 5 6 showresults average 100 random executions auction given number607fiBabaioff & Nisanbuyers sellers, values drawn uniformly independently random [0, 1].calculated Reduction DA eciency revenue ratios {0.25, 0.5, 0.75}.Note expectation, results would obtained PaymentDA, since expected eciency revenue Reduction DA.calculated = (U ) (where U uniform distribution agents values [0, 1])simulations using binary search. Note gures, values TradeReduction DA, dierent Reduction DA, VCG DA well ReductionDA, lay linear curve. point representing McAfees DA lays linesimulations lower eciency Reduction DA. ReductionDA extracts eciency McAfees DA, still budget balanced.simulations also show symmetric case number buyers sellerssame, auctions extract high fraction eciency (more 99%).hand asymmetric case number buyers much largernumber sellers, Trade Reduction DA well McAfees DA extract signicantlylower eciency Reduction DA (about 5% less).0.70.60.50.4Revenue0.30.20.1VCGTrade Reduction00.25 Reduction0.5 Reduction0.10.75 ReductionMcAfee0.2a*0.36.066.076.086.096.1Social Efficiency6.116.126.136.14Figure 5: 25 buyers 25 sellersRevenue/Eciency tradeo simulations results5. Supply Chain Protocolsbegin presenting linear supply chain model, unit initial goodconverted unit nal good sequence unit unit conversions.supply chain model single-minded agents model following specicationspossible outcomes, value spaces classes agents. ordered set G (|G| = t)homogeneous goods traded discrete quantities. agent onefollowing + 1 classes. initial supplier supply one unit rst good608fiConcurrent Auctions Across Supply Chain0.70.60.5Revenue0.40.30.2VCGTrade Reduction0.10.25 Reduction0.5 Reduction0.75 Reduction0McAfeea*0.12.12.152.2Social Efficiency2.252.32.35Figure 6: 50 buyers 5 sellersRevenue/Eciency tradeo simulations resultsnon-negative cost si (vi = si ) trades. converter one unit good r {1, . . . , t1}one unit good r + 1 non-negative cost si (vi = si ) converts unit.consumer non-negative value bi (vi = bi ) receives one unit nal good.market set agents class. allocation materially balancedgood, number units produced number units consumed.means number winners class agents same. Clearly numberwinners market cannot exceed minimal number bidders market n,assume markets n bidders (n found single messagemarket next market. message includes minimal size seenfar. consumer market sends n backwards chain markets).suggest two protocols used conduct auction chain marketsdistributed manner: Symmetric Protocol Pivot Protocol.protocols run servers connected network linear chain topology. serverrepresents one market receives bids one class agents (suppliers, consumersconverters specied good next good chain). serversact strategically, follow specied protocols (they owned entity).denote supply market 1 conversion market good rfollowing good r + 1 rr+1 . consumer (demand) market marked .conversion markets connected bi-directional communication channelsmarket supplies input good market demands output good.denote supply demand curves good r r Dr respectively,supply demand curves conversion good r good r + 1 rr+1rr+1 respectively.609fiBabaioff & Nisanagents must bid xed deadline. input protocols supplybids rst good, conversion bids conversion markets demandbids nal good. protocols decide allocation payments tradingagents (the protocols normalized, losing agents pay zero). protocols genericoperate various normalized DA rules.Symmetric Protocol, markets conducts double auction, constructing demand supply curves. agent views supply chain auctiondouble auction since market conducts double auction central market makes allocation payments decisions. order protocol creatematerially balanced allocation, restriction imposed double auction ruleused Symmetric Protocol, later describe. protocol use either discriminating non-discriminating DA rule. present protocol using abstractdiscriminating DA rule takes supply demand curves inputs, returnstrade quantity q two price vectors, PS sellers PB buyers.Pivot Protocol, one market (the demand market) constructs demandsupply curves, market applies double auction allocation payments rule,sends results auction predecessor. market uses informationreceives bids market send predecessor information usedcalculate allocation payments. Unlike Symmetric Protocol, Pivot Protocolmust use non-discriminating DA rule, creates materially balanced allocationDA rule. many DA rules, protocol improved muchlower communication burden Symmetric Protocol, show Section 5.3.present protocol using abstract non-discriminating double auction rule takessupply demand curves inputs, returns trade quantity q single pricePS trading sellers another price PB trading buyers.5.1 Symmetric Protocolseen example Section 2, bids submitted SymmetricProtocol begins supply curve propagation along supply chain, supplymarket consumer market. protocol continues demand curve propagationalong supply chain way (demand curves propagation may done concurrently supply curves propagation). process markets buildssupply demand curves information receives. pointmarkets supply demand curves, double auction conducted. rulerandomized, assume random coins public, means marketsaccess random coins (a public coin created one market tossingcoin propagating result markets along supply chain). formal protocol supply market 1 , conversion markets rr+1 demandmarket described Figures 7, 8 9, respectively.DA rule decides trade size market according supply demandcurves created protocol. allocation materially balanced casetrade size decided markets.Denition 4. Let S(M ) D(M ) supply demand curves created Symmetric Protocol market . Let q(M ) trade size obtained applying DA rule610fiConcurrent Auctions Across Supply ChainSymmetric Protocol Supply Market 1 .1. 1 sort list supply bids non-decreasing order.2. Send 1 demand market 12 .3. Receive D1 demand market 12 .4. Apply DA Rule (S 1 , D1 ) obtain (q, PS , PB ).5. Output: q lowest bidders sell unit pay vector PS .Figure 7: Symmetric Protocol Supply MarketSymmetric Protocol Conversion Market rr+11. rr+1 sort list supply bids non-decreasing order.2. receiving r r1r ,send r+1 = r + rr+1 r+1r+2 .3. receiving Dr+1 r+1r+2 ,send Dr = Dr+1 rr+1 r1r .4. Construct market demand curve Drr+1 = Dr+1 r .5. Apply DA Rule (S rr+1 , Drr+1 ) obtain (q, PS , PB ).6. Output: q lowest bidders convert unit pay vector PS .Figure 8: Symmetric Protocol Conversion MarketSymmetric Protocol Demand Market1. Dt sort list demand bids non-increasing order.2. Send Dt supply market t1t .3. Receive supply market t1t .4. Apply DA Rule (S , Dt ) obtain (q, PS , PB ).5. Output: q highest bidders buy unit pay vector PB .Figure 9: Symmetric Protocol Demand Market611fiBabaioff & NisanR (S(M ), D(M )). DA rule R called consistent q(M 1) = q(M 2) twomarkets 1 2.following Lemma shows optimal trade size markets same,rule set trade size function optimal trade size consistent.Lemma 5.1. Let l(M ) optimal trade size market . two markets 12, l(M 1) = l(M 2). denote optimal trade size l.Proof. denition optimal trade size, l(M ) maximal indexBl(M ) (M ) Sl(M ) (M ). enough show two markets 1 2,B(M 1) S(M 1) = B(M 2) S(M 2) (as vectors).r:mm+1 ) r rr+1 =rr+1 rr+1 = (Dr+1 r ) rr+1 = (Dt t1m=r+1mm+1 ) =(S r + rr+1 + t1m=r+1Similar argument shows D1 1 = Dtconclude trade size qM decided DA rule R marketfunction optimal trade size lM = l, R consistent. Clearly consistent DArule always creates materially balanced allocation.Since trade size decided VCG DA rule l, rule consistent.Similarly Trade Reduction DA rule consistent since trade size l 1.double auction rules consistent, McAfees DA example inconsistent rule (aspecic example inconsistency presented appendix A). intuitive reasonconsistent trade size dependent comparisonfunction l + 1 bids l bids, might dierent markets.hand two new randomized DA, Reduction DA Payment DAconsistent. markets share public coin, either trade size l 1markets, l markets.Following main theorem regarding Symmetric Protocol.Theorem 5.2. normalized DA rule R consistent used SymmetricProtocol create supply chain mechanism normalized materially balanced.R also IC mechanism created Symmetric Protocol IC (thus also IR)eciency eciency R. R also non-discriminating, mechanismalso non-discriminating.Proof. Since R consistent allocation materially balanced denition. payments normalized denition, show R IC mechanism also IC.Theorem 3.3 DA rule R bid monotonic payments critical values.agent, supply demand curves (disregarding bid) independentbid way built protocol. mechanism bid monotonicpayments critical values (from agent point view, submits bidDA), therefore direction Theorem 3.3 supply chain mechanism IC(and IR). IC way costs aggregated protocol, eciencymechanism eciency R. R non-discriminating,payments winning agents market same, mechanismnon-discriminating.612fiConcurrent Auctions Across Supply ChainNote normalized, IC consistent DA rule must usedmarkets mechanism materially balanced, normalized IC. DA rulefunction supply demand curves market. two supplierconversion markets get supply bids, also get demand curvesconsistency must allocation. Since normalized ICObservation 3.4 must also payment rule. demand market mustDA rule supply market, since case converterszero cost, markets supply demand curves. Applyingargument presented markets, proves must DA rule.5.2 Pivot ProtocolPivot Protocol supply chain protocol creates normalized materially balancedsupply chain mechanism non-discriminating normalized double auction rule.DA rule also incentive compatible protocol creates incentive compatible supplychain mechanism. Unlike Symmetric Protocol, Pivot Protocol restrictedusing consistent double auction rules, sense less restricted SymmetricProtocol. hand, unlike Symmetric Protocol restricted use nondiscriminating double auction rules. use randomized double auctions alsonatural Pivot Protocol, since one market (the Pivot market) uses randomcoins need public coins created distribution random coinsone market markets.dening Pivot Protocol formally, explain execution Lemonadestand industry example Section 2. Symmetric Protocol, protocol beginssupply curve propagation along supply chain juice market (see Figure 2).Figure 10 presents information propagation Pivot Protocol supply curvepropagation stage, explained below.Lemon MarketSupply Demand(S L )(DL )3687Squeezing MarketSupply Demand(S LJ )(DLJ )1356(V,q)=(8,2)Juice MarketSupply Demand(S J )(DJ )412911137(V,q)=(11,2)Figure 10: Pivot Protocol Lemonade-stand industryUnlike Symmetric Protocol, demand curve propagated backwards,rather, juice market, supply demand curves stage,conducts non-discriminating double auction. example, running VCG doubleauction results ecient trade size 2 juice market. trade size propagatedmarkets set trade size market along supply chain,ensuring materially balanced allocation (the trade size denoted q Figure 10).VCG DA rule charges two winning consumers juice market $9 = max($9, $7)613fiBabaioff & Nisan(since must bid least $9 match supply juice, do, alsodefeat Cindy, losing consumer). single agent juice supplier rolejuice market, rather, juice supplier aggregation lemon picker juicesqueezer. use price double auction would charged supplierjuice market nd prices lemon pickers juice squeezers. market sendspredecessor chain highest price winning agents willing pay one unitinput good. supplier juice market paid $11 = min($13, $11),maximal cost winning supplier charge juice. So, $11 highest costjuice, juice market informs squeezing market trade size 2price juice exceed $11 (V gure marks propagated value).squeezing market informs lemon market trade size 2 pricelemon market $8 = $11 $3, since highest cost winnersqueezing market $3. price squeezing market cannot exceed $5 = $11 $6,since price juice cannot exceed $11 cost lemon winning lemon pickermight high $6. Also, price squeezing market cannot exceed $6, costloser market, price set $5 = min($6, $5). price lemonmarket set $7 = min($7, $8), since price cannot exceed $7, cost losermarket, lower propagated maximal cost $8.Note use VCG double auction rule, Lemonade-stand industryallocation payments Pivot Protocol exactly allocationpayments Symmetric Protocol presented Section 2. later show (Theorem 5.5)general phenomena happens normalized DA incentive compatiblerule used protocols, is, DA rule non-discriminatingconsistent (if DA rule inconsistent, like McAfees DA rule, allocation createdSymmetric Protocol materially balanced. hand rulediscriminating, cannot used Pivot Protocol since dierent maximalunit costs dierent winning agents market).turn general denition Pivot Protocol. Pivot Protocol, onemarkets chosen pivot double auction held there. marketmay chosen pivot; describe case pivot consumer market.bids submitted, protocol begins supply curves propagation alongsupply chain Symmetric Protocol. point consumer marketsupply demand curves, non-discriminating double auction conductedmarket. DA sets trade size, price mechanism charges consumers,highest cost unit good (suppliers price), sent predecessormarket consumer market. that, starting consumer marketviewed demand market predecessor, demand markets sendssupply market size trade highest price demand market willingpay one unit input good, without reducing trade quantity. paymentmarket set maximal cost market results trade size,calculated minimum two terms. rst term dierencepropagated maximal cost market output maximal cost market input.second term highest cost loser market. showpayment scheme creates normalized, incentive compatible non-discriminating supply614fiConcurrent Auctions Across Supply ChainPivot Protocol Pivot Market1. Dt sort list demand bids non-increasing order.2. Receive supply market t1t .3. apply DA Rule (S , Dt ) obtain (q, PS , PB ).// Send results markets:4. Send (PS , q) market t1t .5. Output: q highest bidders buy price PB .Figure 11: Pivot Protocol Pivot MarketPivot Protocol Conversion Market rr+11. rr+1 sort list supply bids non-decreasing order.2. receiving r r1r ,send r+1 = r + rr+1 r+1r+2 .3. receiving pair (V, q) r+1r+2 ,send (V Sqrr+1 , q) r1r .rr+14. Output: q lowest bidders sell price min(V Sqr , Sq+1).Figure 12: Pivot Protocol Conversion Marketchain mechanism, given normalized, incentive compatible non-discriminating doubleauction rule.formal protocol pivot market, conversion markets supply marketpresented Figures 11, 12 13, respectively.following theorem shows properties mechanism created PivotProtocol derived properties DA rule used pivot market.Theorem 5.3. normalized DA rule R non-discriminating usedPivot Protocol create normalized supply chain mechanism materially balancednon-discriminating. R also IC mechanism created Pivot ProtocolIC (thus also IR) eciency eciency R.Pivot Protocol Supply Market 11.2.3.4.1 sort list supply bids non-decreasing order.Send 1 demand market 12 .Receive pair (V, q) demand market 12 .1).Output: q lowest bidders sell price min(V, Sq+1Figure 13: Pivot Protocol Supply Market615fiBabaioff & NisanProof. protocol denition mechanism materially balanced, normalizednon-discriminating. Assume DA rule normalized IC. Then, sinceprotocol denition supply curve pivot market independent bidsconsumers, mechanism normalized IC consumers. proveIC initial suppliers, proof converters similar (so omitted),conclude mechanism normalized IC (thus also IR). showmechanism bid monotonic payments critical values, thus Theorem 3.3mechanism IC.Assume supplier bids Si1 wins. Assume trade size q (q since wins)PS price sellers pivot market. allocation bid monotonic sincebids lower cost still one q lowest bidding suppliers bid changereduce cost q lowest bidding sellers pivot market. applyingLemma 3.6 DA rule, PS remains same, corresponding seller bid stilllower PS remains winner. Let Ci1 critical value supplier i.Pivot Protocol payment winning supplier supply marketp1i= min(PSt11Sqrr+1 , Sq+1)r=1need show Ci1 = p1i prove payments critical values. Assumechanges bid X j bid market new orderbids. show X < p1i wins X > p1i loses (noteconsider costs, values). means p1i critical value supplier Si1 winauction.1supplier still one q highest biddersX < p1i X < Sq+1market (j q). way protocol builds supply curvepivot market, bid change change q highest sellers bids pivotmarket. q highest sellers bids pivot market still PS , sincerr+1 < PS , even q bidder assumptionoriginally Sq1 + t1t1 rr+1 r=1 q< PS . Lemma 3.6 applied DA rule, PS remainsX + r=1 Sqsince corresponding seller bid PS , supplier wins auction.proves Ci1 p1i1 , j q + 1. win bidding X mustX > Sq+1rr+1rr+111, Ci1 > Sq+1+ t1ND meansCi > X + t1r=1 Sjr=1 Sq+1original q + 1 supplier (which q supplier) must win bids X.Lemma 3.6 original q + 1 supplier must also win bids Si1 contradictsassumption trade size q bids Si1 .rr+1 X 1X > PS t1q+1 j q (without loss generalityr=1 Sqrr+1 > Pordered q + 1 bid tie). Since X + t1r=1 Sqt1 rr+111(the original q supplier wins bids Si ) concludePS Sq + r=1 SqX > Sq1 means q bidder (j = q). correspondingrr+1 > P , high enough winseller bid DA X + t1r=1 SqDA therefore agent loses.616fiConcurrent Auctions Across Supply Chainconclude agent wins auction pays critical value, thereforeauction IR IC. IC way costs aggregated protocol,eciency mechanism eciency R.Theorem 5.3 presented relationship incentive compatibility DArule resulting supply chain auction created Pivot Protocol. bringsquestion whether budget balance DA rule ensures budget balancesupply chain mechanism. turn so! appendix present exampleDA rule (McAfees rule) revenue surplus, Pivot Protocol usingrule creates mechanism revenue decit.next theorem gives condition DA rule sucient ensuresupply chain mechanism created Pivot Protocol budget-balanced. also showsDA rule budget decit creates mechanism decit.Theorem 5.4. Let R normalized DA rule IC non-discriminating. Letsupply chain mechanism created Pivot Protocol using rule R .supply demand curves, let q trade size decided R, let PS PBsellers buyers prices respectively.PB Sq+1 holds supply demand curves, budget-balanced.PB < PS supply demand curves (R budget decit) budgetdecit.Proof. rst show PB Sq+1 holds supply demand curves,budget-balanced. divide supply chain allocation q disjoint procurementsets. set single winner market. show total paymentprocurement set non-negative, therefore auction BB. Theorem 5.3mechanism non-discriminating, procurement sets payments.Let P total payments agents procurement set, except consumer.P PS . PB Sq+1PB P ,Lemma B.1 (see appendix B) shows Sq+1conclude PB P 0 BB.Similarly supply demand curves R budget decit, every procurement set decit. PB < PS supply demand curves, PB < Pconclude total payment procurement set PB P < 0,means budget decit.Note double auction rule R BB, sucient PB PS .prove supply chain mechanism budget balanced, theorem relies strongercondition PB Sq+1 (it always true Sq+1 PS Lemma B.1 shows).McAfees DA satises PB PS , stronger condition PB Sq+1 always hold,indeed example Appendix shows supply chain mechanismbudget decit cases. hand, Trade Reduction DA rule satisesconstraints since q = l 1 (recall l optimal trade size) PB = Bq+1 =Bl Sl = Sq+1 = PS .Clearly DA rule budget decit might create supply chain mechanismbudget decit used Symmetric Protocol. since double auction specialcase supply chain. able present parallel sucient conditionSymmetric Protocol ensure budget-balanced supply chain mechanism. case617fiBabaioff & Nisannormalized DA rule incentive compatible, consistent non-discriminating, nexttheorem proves two protocols create mechanism, impliessucient condition budget balance holds also Symmetric ProtocolDA rules.turn look relationship two protocols. casenormalized DA rule consistent non-discriminating used protocols.following theorem shows rule also IC, Symmetric ProtocolPivot Protocol create mechanism:Theorem 5.5. Let R normalized DA rule non-discriminating consistent.R IC mechanism created Pivot Protocol using Rmechanism created Symmetric Protocol using R (for set bids,allocation payments). Thus, eciency budget mechanismsequal.Proof. Since supply demand curves demand market builtway, DA rule used demand market protocols, markettrade sizes decided rule mechanisms. Symmetric Protocolconsistent, therefore trade sizes markets same. Since mechanismnon-discriminating, allocations markets protocols.Observation 3.4 mechanisms must payments, sinceallocation.5.3 Communication Complexitynaive implementation supply chain mechanism centralized mechanism wouldrequired sending (tn) bids one centralized point (t number goods nminimal number agents market). protocols describedmuch lower communication O(n) prices received sent market (notebid single agent requires k bits, prices communicated growmuch k + bits). many interesting DA rules reducecommunication market O(log(n)) prices improved Pivot Protocolpresent below. Thus, DA rules Pivot Protocol implementedexponentially larger markets still using bandwidth. example,use improved protocol size trade q set DA rule calculatedOptimal Trade Quantity l, payments dependent l l + 1 bidssupply demand curves (this case DA rules presented).Theorem 5.6. Let R normalized DA rule IC non-discriminating. Assumetrade size decided rule function Optimal Trade Size l only,payments decided using O(1) prices knowing l. Pivot Protocolimplemented O(log(n)) messages sent received market,message contains single price.Proof. Pivot Protocol improved using binary search nd l thus sendingvalues needed supply curves, instead passing entire supply curvesalong chain. search l preformed binary search sending618fiConcurrent Auctions Across Supply ChainO(log(n)) messages two consecutive markets. improved protocol, rsttn passed pivot market, n minimal number bids market2(known markets protocol explained beginning section).pivot market checks value smaller greater Dtn asks 3n42n4 element supply curves respectively. pivot market receives requestedvalue continues similar way search, l found. O(1) pricesneeded supply curve nd payments (which function lassumption) propagated pivot market request.Similar results presented Symmetric Protocol restrictionsDA rule used protocol. Symmetric Protocol protocol choiceDA rule discriminating, case likely entire supplydemand curves needed order nd prices winners.5.4 Global Properties Dierent DA Typessection examine properties mechanisms created supply chainprotocols using dierent double auction rules.Chain VCG Double Auctions created running Symmetric Protocolusing VCG DA rule. creates local VCG double auction markets.Since VCG DA rule IR, IC, consistent non-discriminating, Theorem 5.5 PivotProtocol VCG DA rule creates mechanism.following proposition summarizes properties supply chain mechanismusing VCG double auction rule.Proposition 5.7. Chain VCG Double Auctions IR, IC, non-discriminating, SociallyEcient revenue decit. (RChain V CG 0)Proof. Since VCG DA rule normalized, IC non-discriminating, Theorem 5.3mechanism created IR, IC non-discriminating. outcome maximizes sumvaluations (Socially Ecient), since agents bid truthfully, trade size OptimalTrade Size highest value bidders markets chosen winners.fact VCG DA budget decit Theorem 5.4 concludeChain VCG DAs budget decit.Note Observation 3.4 mechanism created using VCG DA rule VCGglobal sense presented Vickrey, Clarke Groves (A mechanism IR, ICecient).Chain Trade Reduction Double Auctions created running SymmetricProtocol using Trade Reduction DA Rule. creates local Trade Reduction doubleauction markets, double auctions chained. Since TradeReduction DA rule normalized, IC, consistent non-discriminating, Theorem 5.5Pivot Protocol Trade Reduction DA rule creates mechanism.Proposition 5.8. Chain Trade Reduction Double Auctions IR, IC , non-discriminating,revenue surplusRChainrade Reduction= (l 1) (Dlt Slt ) 0619fiBabaioff & Nisanreduction Social Eciency Dlt Slt eciency least (l 1)/lmaximal eciency.Proof. Since Trade Reduction DA rule normalized, IC ND, Theorem 5.3mechanism created IR, IC non-discriminating.revenue sum payments(l 1) (Dltt1Slrr+1 Sl1 ) = (l 1) (Dlt Slt )r=1non-negative since Dlt Slt denition Optimal Trade Quantity l.reduction social eciency Dlt Slt , since l unit traded,Trade Reduction DA l 1 highest trades optimal l trades conducted,giving eciency least (l 1)/l.Chain Reduction Double Auctions created running Pivot Protocolusing Reduction Rule, creates randomized mechanism. mechanismprobability distribution two mechanisms - Chain Trade Reduction DAChain VCG DA. mechanisms used according random variableone probability zero otherwise. rst mechanism chosen one(with probability ), second mechanism chosen zero (which happensprobability 1 ).auction also preformed using Symmetric Protocol public coinassumption (which created distributing random coin, tossed onemarkets). case random variable shared markets, meaningeither Trade Reduction rule used markets, non markets.ensures trade consistency creates mechanism.Chain Reduction Double Auctions achieves allocation paymentsChain Trade Reduction DA probability , probability 1 achievesallocation payments Chain VCG DA.Proposition 5.9. Chain Reduction Double Auctions individually rationaluniversally incentive compatible randomized mechanism. expected revenueRChainReduction= RChainrade Reduction+ (1 ) RChainV CGeciency least (l 1)/l maximal eciency. expected reductionSocial Eciency (Dlt Slt ) expectation random choice .Proof. mechanism individually rational universally incentive compatible randomized mechanism, since Chain Trade Reduction DAs Chain VCGDAs normalized incentive compatible Proposition 5.8 Proposition 5.7.easy verify expected revenue reduction social eciencyclaimed.Chain Payment Double Auctions created running Symmetric Protocol, market running Payment Double Auction, creates randomized620fiConcurrent Auctions Across Supply Chainmechanism (again public coin assumption). mechanism expected revenue social eciency Chain Reduction DAs,agents probability winning auction expected utilityChain Reduction DAs. Note since Payment DA rule discriminating,Pivot Protocol dened cannot used create supply chain mechanism.Proposition 5.10. Chain Payment Double Auctions individually rationalincentive compatible randomized mechanism. expected revenue expected socialeciency Chain Reduction Double Auctions.Proof. Chain Payment Double Auctions distribution allocationpayments Chain Reduction Double Auctions, therefore agent facingmechanism expectation bids truthfully. Since expected paymentagent mechanisms, expected revenue same. Sinceallocation mechanisms, expected social eciency same.Unfortunately, chaining McAfees Double Auction preserve nice propertiesMcAfees DA rule has. run Symmetric Protocol using McAfees DA Rule,might get inconsistent trade. hand, run Pivot Protocol usingMcAfees DA rule, revenue decit! (See examples appendix A). Howeverweaker claim made - mechanism revenue least high chainVCG DA, eciency least high chain Trade Reduction DA.Chain McAfees Double Auction created running Pivot Protocol usingMcAfees Rule.Proposition 5.11. Chain McAfees Double Auction IR, IC, ND, revenue neversmaller revenue Chain VCG Double Auction, reductionSocial Eciency never greater reduction Chain Trade Reduction DoubleAuction.Proof. Chain McAfees Double Auction IR, IC ND Theorem 5.3 factMcAfees DA rule normalized, IC ND.mechanism revenue never smaller revenue Chain VCG DoubleAuctions since trade reduction, revenue surplus Proposition 5.8,know Chain VCG Double Auctions revenue decit Proposition 5.7.case trade reduction, revenue never smaller revenue ChainSt+DVCG Double Auctions, since winning buyer pays p = PB = PS = l+1 2 l+1> Dl+1least much pays VCG mechanism. Sl+1Sl+1 > p > Dl+1 Sl p therefore max(Dl+1 , Sl ) p. winning seller1 , 1 ) receives VCGsupply good, never receives min(Sl+1lmechanism. since p min(Sl+1 , Dl ) receives1,pmin(Sl+1t11Slrr+1 ) min(Sl+1, min(Dlt , Sl+1)r=11, Dltmin(Sl+1t1Slrr+1 ) =r=1t1Slrr+1 , Sl+1r=1t1r=16211Slrr+1 ) = min(Sl+1, Dl1 )fiBabaioff & Nisanrr+1rr+1last equality holds since Dl1 = Dlt t1Sl+1t1r=1 Slr=1 Slt1rr+11= Sl+1 . similar argument shows winning convert neverSl+1 r=1 Sl+1receives VCG mechanism.reduction social eciency never greater reduction ChainTrade Reduction Double Auction, since trade size never smaller trade sizeauction, l 1.Table 2 summarizes properties supply chain mechanisms created twoprotocols using dierent double auction rules. normalized mechanisms, one caresincentive compatibility eciency budget balance, VCGmechanism used. budget balance must ensured every executionincentive compatible mechanism, Trade Reduction auction mechanismchoice. one wants mechanism incentive compatible every execution,settle expected budget balance instead ensured one, higher expected eciencyachieved (D) Reduction, assuming right choice (D). Finally,variance agents payments important, (D) P ayment usedorder lower variance agents. cost, since mechanismincentive compatible every execution (universally), rather, incentivecompatible agents care expected utility. table also presentsMcAfee supply chain mechanism, point fact budget supply chainmechanism might dierent budget underlying double auction.DA ruleVCGTrade Red.(D) Reduction(D) P aymentMcAfeeIncentive Comp.yesyesyes, universallyyes, universallyyesRevenuedecitsurplus0 (expected)0 (expected)surplus decitEciency loss0LFT(D) LF (expected)(D) LF (expected)LFTTable 2: Supply Chain AuctionsNotes: a) McAfees rule used Symmetric Protocol, table presents propertiesPivot Protocol. b) LFT (least favorable trade) context supply chain, means net total utilityleast favorable item. c) distribution agents supply chain. d) (D) P aymentlower variance payments (D) Reduction.comparing properties chain mechanisms Table 2 propertiesoriginal double auction rules Figure 1, one see following. incentivecompatibility eciency properties preserved protocols, achievingrevenue surplus requires stronger condition DA rule, shown Theorem 5.4. consistency property, enables chaining markets SymmetricProtocol, applies rst two deterministic rules, two randomized rulesassumption common coin toss. exist McAfees rule since rule setstrade quantity function bids submitted, way trade quantitydierent two dierent markets. k-DA presented table since622fiConcurrent Auctions Across Supply Chainincentive compatible therefore cannot used protocols (the two protocolscreate dierent mechanisms rule).6. Discussion Future Workpaper presented two distributed protocols create supply chain mechanisms using double auction rules. protocols generic use dierent doubleauction rules create dierent mechanisms. characterized propertiessupply chain mechanisms derive properties underlining double auctionrule. protocols use VCG DA achieve IR, IC full eciency budgetdecit. also use Trade Reduction DA ensure budget balance surplushigh eciency. even higher eciency maintaining BB expectationachieved using two new randomized double auction rules.work concentrated simple case indivisible homogeneous goods,agents desires one good desires convert one unit good one unitanother good, supply chain linear form. Pivot Protocol easily extendedcase trees good created one several goods (butgood used create two dierent goods). example, might lemon marketFlorida another lemon market California, also separate squeezing marketsstates, one lemonade market. tree auctions decides many lemonadeglasses produced, many glasses produced Florida lemonsCalifornia lemons. Since main ideas extension similar onespresented paper, omit technical details refer interested readerBabaio (2001) details. Babaio (2001) also discusses extension modelcase agent allowed bid multiple units market, biddingmaximal quantity willing trade price per unit.paper Babaio Walsh (2004) extends model case itemproduced combination several items several dierent goods, goodstill produced exactly one market. presented specic auction (unlikework presents generic protocols supply chain formation) individuallyrational, incentive compatible, budget-balanced, yet highly ecient.Similar results (IR, IC, BB high eciency) presented Babaio, Nisan,Pavlov (2004) related model Spatially Distributed Market. model singlegood traded set independent markets, shipment markets possibleincurs publicly known cost (but model strategic producersmodel).One interesting challenge stay unit-to-unit conversion model, extendmodel allow general directed a-cyclic supply chain graph topology. Yet anotherimportant challenge extend model case agents single-mindedagents, submit exclusive non exclusive bids several markets.623fiBabaioff & NisanAcknowledgmentsresearch supported grants Israeli Ministry Science, IsraeliAcademy Sciences USA-Israel Bi-national Science Foundation. rst author(Babaio) also supported Yeshaya Horowitz Association.thank William Walsh, Daniel Lehmann, Hila Babaio anonymous reviewershelpful comments.Appendix A. Problems Chaining McAfees Double AuctionConsistency Problem: following example supply chain auction using McAfeesrule shows double auction rules consistent. Figure 14 shows supplydemand curves three markets supply demand curves propagation.Lemon MarketSupply Demand(S L )(DL )10202010Squeezing MarketSupply Demand(S LJ )(DLJ )5157-3Juice MarketSupply Demand(S J )(DJ )15252717Figure 14: Chaining McAfees DA exampleoptimal trade quantity example 1, seen Figure 14. McAfees= 22 [15, 25].rule Lemonade Market set trade size 1, since 27+172hand, use McAfees rule Squeezing Market, trade reduction= 2/ [5, 15] trade quantity zero, contradictionmade since 7+(3)2previous decision. conclude exists non-consistent DA rule cannotused Symmetric Protocol.Revenue Problem: example Pivot Protocol presentedFigure 15 shows fact DA rule revenue surplus, ensuresupply chain mechanism created Pivot Protocol using DA rule revenuesurplus well.Lemon MarketSupply Demand(S L )(DL )1020Squeezing MarketSupply Demand(S LJ )(DLJ )57(V,q)=(17,1)Juice MarketSupply Demand(S J )(DJ )15252717(V,q)=(22,1)Figure 15: Pivot Protocol McAfees ruleexample, McAfees rule Lemonade Market sets trade size 1, since= 22 [15, 25]. buyer market pay 22. following PivotProtocol, squeezer bid 5 paid 7 = min(7, 22 10)27+172624fiConcurrent Auctions Across Supply Chainsupplier supply market bid 10 paid 17 = min(17, 20).total revenue 22 7 17 = 2 means revenue decit.Appendix B. Supply Chain Paymentsappendix prove Lemma B.1 used proof Theorem 5.4.Lemma B.1. double auction rule used Pivot Protocol IR, IC nondiscriminating, PS sellers price double auction, total paymentP supplier converters (one market) create one unit nal.good satises PS P Sq+1Proof. induction, conversion market C C m+1 receives pair (PSt1rr+1 , q) demand market, payment winning converterr=m+1 Sqmin(PSt1mm+1Sqrr+1 Sqm , Sq+1)r=m+1Similarly, supply market 1 receives pair (PSment winning suppliermin(PSt1t1rr+1 , q),r=1 Sqpay-1Sqrr+1 , Sq+1)r=1conclude total payment converters supplier one unitP =t1m=1min(PSt1mm+1Sqrr+1 Sqm , Sq+1) + min(PSr=m+1t11Sqrr+1 , Sq+1)r=1sinceFirst notice P Sq+1Pt1mm+11Sq+1+ Sq+1= Sq+1m=1Secondly prove PS P . idea proof holdsone conversion C 1 C (as show Lemma B.2), splitting conversiontwo consecutive conversions, total payment converters grows (asshow Lemma B.3).Lemma B.2. Assume one conversion market C 1 C ,rr+1means every i, Si1t = t1r=1 Si1t1) + min(PS Sq1t , Sq+1)PS P = min(PS Sq1 , Sq+1Proof. Note double auction pivot market originalauction, since supply curve pivot market same, therefore PSsame.prove claim true checking four possible cases:625fiBabaioff & Nisan1t P 1t 1 ,1. PS Sq1 Sq+1qq+1P = (PS Sq1 ) + (PS Sq1t ) = PS + (PS (Sq1 + Sq1t )) PSsince PS Sq1 + Sq1t = Sqt , DA rule non-discriminating.1t 11t ,2. PS Sq1 Sq+1q+1 PS Sq11= PS + (Sq+1Sq1 ) PSP = (PS Sq1 ) + Sq+11since Sq+1Sq11t P 1 P 1t 1 ,3. Sq+1qqq+11t1t+ (PS Sq1t ) = PS + (Sq+1Sq1t ) PSP = Sq+11t 1t .since Sq+1q1t P 1 11t ,4. Sq+1qq+1 PS Sq1t1+ Sq+1PSP = Sq+11t + 1since Sq+1q+1 = Sq+1 PS , DA rule non-discriminating.proven P PS case one conversion market.Lemma B.3. Assume conversion market C k C split two conversionmarkets - one C k C k+1 C k+1 C , paymentconversion one unit C k C grow, meansP kt P kk+1 + P k+1tP wz cost conversion one w unit one z unit. words,always truekk+1k+1tkt) min(PS Sqk+1t Sqk , Sq+1) + min(PS Sqk+1 , Sq+1)min(PS Sqk , Sq+1Sikt =t1r=kSirr+1Proof. Note double auction pivot market originalauction, since supply curve pivot market same, therefore PSsame.prove claim true checking four possible cases:kk+1k+1tPS Sqk+1 Sq+1,1. PS Sqk+1t Sqk Sq+1kk+1k+1tktkt+ Sq+1= Sq+1min(PS Sqk , Sq+1) = P ktP kk+1 + P k+1t = Sq+1626fiConcurrent Auctions Across Supply Chainkk+1k+1t2. PS Sqk+1t Sqk Sq+1PS Sqk+1 Sq+1,k+1t=P kk+1 + P k+1t = (PS Sqk+1t Sqk ) + Sq+1k+1t(PS Sqk ) + (Sq+1Sqk+1t ) PS Sqk P ktk+1tsince Sq+1Sqk+1t .kk+1k+1t3. PS Sqk+1t Sqk Sq+1PS Sqk+1 Sq+1,kk+1+ (PS Sqk+1 ) =P kk+1 + P k+1t = Sq+1kk+1Sqkk+1 ) (PS Sqk ) P kt(PS Sqk ) + (Sq+1kk+1since Sq+1Sqkk+1 .kk+1k+1tPS Sqk+1 Sq+1,4. PS Sqk+1t Sqk Sq+1P kk+1 + P k+1t = (PS Sqk+1t Sqk ) + (PS Sqk+1 ) =(PS Sqk ) + (PS Sqk+1 Sqk+1t ) (PS Sqk ) P ktsince PS Sqk+1 + Sqk+1t = Sqt .proven payment grow splitting conversion market.conclude cost one unit C never smaller PS , sincerst lemma, true one conversion, induction secondlemma, splitting conversion market 1 conversion markets increasecost.ReferencesArcher, A., & Tardos, E. (2001). Truthful mechanisms one-parameter agents. Proceedings 42nd IEEE symposium foundations computer science, pp. 482491.Babaio, M. (2001).Concurrent auctions across supply chain. m.sc. thesis, department computer science, hebrew university jerusalem, israel.http://www.cs.huji.ac.il/mosheb.Babaio, M., Nisan, N., & Pavlov, E. (2004). Mechanisms spatially distributed market.5th ACM Conference Electronic Commerce, pp. 920.Babaio, M., & Walsh, W. E. (2004). Incentive-compatible, budget-balanced, yet highlyecient auctions supply chain formation. Decision Support Systems. press.Partial version appeared EC03.Bosch-Domenech, A., & Sunder, S. (1999). Tracking invisible hand: Convergencedouble auctions competitive equilibrium. Tech. rep., Carnegie Mellon University.627fiBabaioff & NisanChatterjee, K., & Samuelson, W. (1983). Bargaining incomplete information. Operations Research, 31, 835851.Clarke, E. H. (1971). Multipart pricing public goods. Public Choice, 1733.Feigenbaum, J., & Shenker, S. (2002). Distributed algorithmic mechanism design: Recentresults future directions. 6th International Workshop Discrete AlgorithmsMethods Mobile Computing Communications, pp. 113.Friedman, D., & Rust, J. (1991). Double Auction Market Institutions, Theories,Evidence. Addison-Wesley Publishing Company.Goldberg, A. V., & Hartline, J. D. (2003). Envy-free auctions digital goods. 4st ACMConference Electronic Commerce, pp. 2935. ACM Press.Groves, T. (1973). Incentives teams. Econometrica, 617631.Lehmann, D. J., OCallaghan, L. I., & Shoham, Y. (2002). Truth revelation approximatelyecient combinatorial auctions. Journal ACM, 49 (5), 577602.Mas-Collel, A., Whinston, W., & Green, J. (1995). Microeconomic Theory. Oxford university press.McAfee, R. (1992). dominant strategy double auction. Journal Economic Theory, 56,434450.Mualem, A., & Nisan, N. (2002). Truthful approximation mechanisms restricted combinatorial auctions.. AAAI (poster), 2002. also presented Dagstuhl workshopElectronic Market Design.Myerson, R. B., & Satterthwaite, M. A. (1983). Ecient mechanisms bilateral trading.Journal Economic Theory, 29, 265281.Nisan, N., & Ronen, A. (1999). Algorithmic mechanism design. Proceedings STOC1999, pp. 129140.Osborne, M. J., & Rubinstein, A. (1994). Course Game Theory. MIT press.Parkes, D. C., Kalagnanam, J., & Eso, M. (2001). Achieving budget-balance vickreybased payment schemes exchanges. IJCAI, pp. 11611168.Rustichini, A., Satterthwaite, M. A., & Williams, S. R. (1994). Convergence eciencysimple market incomplete information. Econometrica, 62 (5), 10411063.Satterthwaite, M., & Williams, S. (1989). Bilateral trade sealed bid k-doubleauction: Existence eciency. Journal Economic Theory, 48, 107133.Satterthwaite, M. A., & Williams, S. R. (1991). bayesian theory k-double auction.Double Auction Market Institutions, Theories, Evidence, pp. 99124.Vickrey, W. (1961). Counterspeculation, auctions competitive sealed tenders. JournalFinance, 837.628fiConcurrent Auctions Across Supply ChainWalsh, W., & Wellman, M. (2003). Decentralized supply chain formation: market protocolcompetitive equilibrium analysis. Journal Articial Intelligence Research, 19,513567.Walsh, W. E., Wellman, M. P., & Ygge, F. (2000). Combinatorial auctions supply chainformation. 2nd ACM Conference Electronic Commerce, pp. 260269.Wilson, R. (1985). Incentive eciency double auctions. Econometrica, 53, 11011115.629fiJournal Artificial Intelligence Research 21 (2004) 287-317Submitted 06/03; published 03/04IDL-Expressions:Formalism Representing ParsingFinite Languages Natural Language ProcessingMark-Jan Nederhofmarkjan@let.rug.nlFaculty Arts, University GroningenP.O. Box 716NL-9700 Groningen, NetherlandsGiorgio Sattasatta@dei.unipd.itDept. Information Engineering, University Paduavia Gradenigo, 6/AI-35131 Padova, ItalyAbstractpropose formalism representation finite languages, referred classIDL-expressions, combines concepts considered isolationexisting formalisms. suggested applications natural language processing,specifically surface natural language generation machine translation,sentence obtained first generating large set candidate sentences, representedcompact way, filtering set parser. study several formalproperties IDL-expressions compare new formalism standard ones.also present novel parsing algorithm IDL-expressions prove non-trivial upperbound time complexity.1. Introductionnatural language processing, specifically applications involve natural language generation, task surface generation consists process generating output sentence target language, basis input representation desiredmeaning output sentence. last decade, number new approachesnatural language surface generation put forward, called hybrid approaches. Hybrid approaches make use symbolic knowledge combination statistical techniquesrecently developed natural language processing. Hybrid approachestherefore share many advantages statistical methods natural language processing,high accuracy, wide coverage, robustness, portability scalability.Hybrid approaches typically based two processing phases, describedfollows (Knight & Hatzivassiloglou, 1995; Langkilde & Knight, 1998; Bangalore & Rambow,2000 report examples applications approach real world generation systems).first phase one generates large set candidate sentences relatively simpleprocess. done basis input sentence source language caseprocess embedded within machine translation system, generallybasis logical/semantic representation, called conceptual structure, denotesmeaning output sentence convey. first phase involvesc2004AI Access Foundation. rights reserved.fiNederhof & Sattaintricacies target language, set candidate sentences may contain manyungrammatical otherwise seen less desirable others.second phase one preferred sentences selected collection candidates,exploiting form syntactic processing heavily relies propertiestarget language first phase. syntactic processing may involve language modelssimple bigrams may involve powerful models based contextfree grammars, typically perform higher accuracy task (see instancework presented Charniak, 2001 references therein).hybrid approaches, generation candidate set typically involves symbolicgrammar quickly hand-written, quite small easy maintain.grammar cannot therefore account intricacies target language.instance, frequency information synonyms collocation information generalencoded grammar. Similarly, lexico-syntactic selectional constraintstarget language might fully specified, usually case small midsized grammars. Furthermore, might also underspecification stemminginput conceptual structure. usually case surface generation moduleembedded larger architecture machine translation, source languageunderspecified features definiteness, time number. Since inferring missinginformation sentence context difficult task, surface generation moduleusually deal underspecified knowledge.above-mentioned problems well-known literature natural languagesurface generation, usually referred lack knowledge systeminput. consequence problems, set candidate sentences generatedfirst phase may extremely large. real world generation systems, candidate setsreported contain many 1012 sentences (Langkilde, 2000). already explained,second processing phase hybrid approaches intended reduce huge setssubsets containing sentences. done exploiting knowledgetarget language available first phase. additional knowledgeoften obtained automatic extraction corpora, requires considerablyless effort development hand-written, purely symbolic systems.Due extremely large size set candidate sentences, feasibility hybridapproaches surface natural language generation reliescompactness representation set candidate sentences real worldsystems might large 1012 ;efficiency syntactic processing stored set.Several solutions adopted existing hybrid systems representationset candidate sentences. include bags words (Brown et al., 1990)bags complex lexical representations (Beaven, 1992; Brew, 1992; Whitelock, 1992), wordlattices (Knight & Hatzivassiloglou, 1995; Langkilde & Knight, 1998; Bangalore & Rambow,2000), non-recursive context-free grammars (Langkilde, 2000). discusseddetail Section 2, word lattices non-recursive context-free grammars allow encodingprecedence constraints choice among different words, lack primitiverepresenting strings realized combining collection words arbitrary288fiIDL-Expressions: Formalism Finite Languagesorder. hand, bags words allow encoding free word order,representation one cannot directly express precedence constraints choice amongdifferent words.paper propose new representation combines above-mentionedprimitives. representation consists IDL-expressions. term IDL-expression,stands interleave, pertains phrases may occur interleaved, allowingfreedom word order (a precise definition notion provided nextsection); stands disjunction, allows choices words phrases; L standslock, used constrain application interleave operator. studyinteresting properties representation, argue expressivityformalism makes suitable alternatives discussed use within hybridarchitectures surface natural language generation. also associate IDL-expressionsIDL-graphs, equivalent representation easily interpretedmachine, develop dynamic programming algorithm parsing IDL-graphs usingcontext-free grammar. set candidate sentences represented IDL-expressionIDL-graph, algorithm used filter ungrammatical sentencesset, rank sentences set according likelihood, case context-freegrammar assigns weights derivations. parsing traditionally defined inputconsisting single string, conceive parsing process carriedinput device denoting language, i.e., set strings.superficial similarity problem described representingfinite sets surface generation, different research topic, often referred discontinuous parsing. discontinuous parsing one seeks relax definition context-freegrammars order represent syntax languages exhibit constructions uncertainty word constituent order (see instance work reported Daniels & Meurers,2002 references therein). fact, operators use IDL-expressionsalso exploited recent work discontinuous parsing. However, parsing problemdiscontinuous grammars parsing problem IDL-expressions quite different: former, given grammar productions express uncertaintyconstituent order, need parse input string whose symbols totally ordered;latter problem given grammar total order constituents appearing production, need parse input includes uncertainty wordconstituent order.paper structured follows. Section 2 give brief overview existingrepresentations finite languages used surface generation components.discuss notational preliminaries Section 3. Section 4 introduce IDLexpressions define semantics. Section 5 associate IDL-expressionsequivalent procedural representation, called IDL-graphs. also introduceimportant notion cut IDL-graph, exploited later algorithm.Section 6 briefly discuss Earley algorithm, traditional method parsingstring using context-free grammar, adapt algorithm work finite languagesencoded IDL-graphs. Section 7 prove non-trivial upper bound numbercuts IDL-graph, basis investigate computational complexityparsing algorithm. also address implementational issues. concludediscussion Section 8.289fiNederhof & Satta2. Representations Finite Languagessection analyze compare existing representations finite languagesadopted surface generation components natural language systems.Bags (or multisets) words used several approaches surface generation.basis generation component statistical machine translationmodels proposed Brown et al. (1990). Bags complex lexical signs also usedmachine translation approach described Beaven (1992) Whitelock (1992),called shake-and-bake. already mentioned, bags succinct representationfinite languages, since allow encoding exponentially many stringssize bag itself. power comes cost, however. Deciding whether stringencoded input bag parsed CFG NP-complete (Brew, 1992).difficult show result still holds case regular grammar or, equivalently,regular expression. NP-completeness result involving bags also presentedKnight (1999), related problem parsing grammar probabilistic modelbased bigrams.far expressivity concerned, bags words also strict limitations.structures lack primitive expressing choices among words. already observedintroduction, serious problem natural language generation, alternatives lexical realization must encoded presence lack detailed knowledgetarget language. addition, bags words usually come precedenceconstraints. However, natural language applications constraints common,usually derived knowledge target language or, case machine translation, parsing tree source string. order representconstraints, extra machinery must introduced. instance, Brown et al. (1990) impose,word bag, probabilistic distribution delimiting position targetstring, basis original position source word input stringtranslated. shake bake approach, bags defined functional structures,representing complex lexical information constraints derived.parsing algorithm bags interleaved constraint propagation algorithmfilter parses (e.g., done Brew, 1992). general remark, different layersrepresentation requires development involved parsing algorithms,try avoid new proposal described below.alternative representation finite languages class acyclic deterministic finite automata, also called word lattices. representation often used hybridapproaches surface generation (Knight & Hatzivassiloglou, 1995; Langkilde & Knight,1998; Bangalore & Rambow, 2000), generally natural language applicationsform uncertainty comes input, instance speech recognition (Jurafsky & Martin, 2000, Section 7.4). Word lattices inherit standard regularexpressions primitives expressing concatenation disjunction, thereby allowencoding precedence constraints word disjunction direct way. Furthermore, wordlattices efficiently parsed means CFGs, using standard techniques latticeparsing (Aust, Oerder, Seide, & Steinbiss, 1995). Lattice parsing requires cubic timenumber states input finite automaton linear time size CFG.Methods lattice parsing traced back Bar-Hillel, Perles, Shamir (1964),290fiIDL-Expressions: Formalism Finite Languagesprove class context-free languages closed intersection regularlanguages.One limitation word lattices finite automata general lack operatorfree word order. already discussed introduction, severelimitation hybrid systems, free word order sentence realization neededcase symbolic grammar used first phase fails provide ordering constraints.represent strings bag words occur every possible order, oneencode string individual path within lattice. general case,requires amount space exponential size bag.perspective, previously mentioned polynomial time result parsing avail,since input structure parser might already size exponentialsize input conceptual structure. problem free word order lattice structurespartially solved Langkilde Knight (1998) introducing external recastingmechanism preprocesses input conceptual structure. overall effectphrases normally represented two independent sublattices generatedone embedded other, therefore partially mimicking interleaving wordstwo phrases. However, enough treat free word order full generality.third representation finite languages, often found literature compressiontheory (Nevill-Manning & Witten, 1997), class non-recursive CFGs. CFGcalled non-recursive nonterminal rewritten string containingnonterminal itself. difficult see grammars generate finitelanguages. Non-recursive CFGs recently exploited hybrid systems (Langkilde,2000).1 representation inherits expressivity word lattices, thusencode precedence constraints well disjunctions. addition, non-recursive CFGsachieve much smaller encodings finite languages word lattices. doneuniquely encoding certain sets substrings occur repeatedly nonterminalreused several places. feature turns useful naturallanguage applications, shown experimental results reported Langkilde (2000).Although non-recursive CFGs compact representations word lattices,representation still lacks primitive representing free word order. fact, CFGgenerating finite language permutations n symbols must size leastexponential n.2 addition, problem deciding whether string encodednon-recursive CFG parsed general CFG PSPACE-complete (Nederhof &Satta, 2004).discussion, one draw following conclusions. consideringrange possible encodings finite languages, interested measuring (i) compactness representation, (ii) efficiency parsing obtained representationmeans CFG. one extreme naive solution enumerating stringslanguage, independently parsing individual string using traditionalstring parsing algorithm. solution obviously unfeasible, since compressionachieved overall amount time required might exponential size1. Langkilde (2000) uses term forests non-recursive CFGs, different nameconcept (Billot & Lang, 1989).2. unpublished proof fact personally communicated authors Jeffrey ShallitMing-wei Wang.291fiNederhof & Sattainput conceptual structure. Although word lattices compact representation,free word order needs encoded may still representations exponentialsize input parser, already discussed. opposite extreme, solutionslike bags words non-recursive CFGs, allow compact representations,still demanding parsing time requirements. Intuitively, explainedconsidering parsing highly compressed finite language requires additional bookkeeping respect string case. need explore trade-offsolutions, offering interesting compression factors expense parsingtime requirements provably polynomial cases interest. showsequel paper, IDL-expressions required properties thereforeinteresting solution problem.3. Notationsection briefly recall basic notions formal language theory.details refer reader standard textbooks (e.g., Harrison, 1978).set , || denotes number elements ; string x alphabet,|x| denotes length x. string x languages (sets strings) L L0 , letx L = {xy | L} L L0 = {xy | x L, L0 }. remind readerstring-valued function f alphabet extended homomorphismletting f () = f (ax) = f (a)f (x) x . also letf (L) = {f (x) | x L}.denote context-free grammar (CFG) 4-tuple G = (N , , P, S), Nfinite set nonterminals, finite set terminals, N = , N specialsymbol called start symbol, P finite set productions form ,N ( N ) . Throughout paper assume following conventions:A, B, C denote nonterminals, a, b, c denote terminals, , , , denote strings ( N )x, y, z denote strings .derives relation denoted G transitive closure +G . language generated grammar G denoted L(G). size G definedX|G| =|A| .(1)(A)P4. IDL-Expressionssection introduce class IDL-expressions define mappingexpressions sets strings. Similarly regular expressions, IDL-expressions generate setsstrings, i.e., languages. However, languages always finite. Therefore classlanguages generated IDL-expressions proper subset class regular languages.already discussed introduction, IDL-expressions combine language operatorsconsidered isolation previous representations finite languages exploitedsurface natural language generation. addition, operationsrecently used discontinuous parsing literature, syntactic description (infinite) languages weak linear precedence constraints. IDL-expressions represent choicesamong words phrases relative ordering means standard concatenation292fiIDL-Expressions: Formalism Finite Languagesoperator regular expressions, along three additional operators discussedfollows. operators take arguments one IDL-expressions,combine strings generated arguments different ways.Operator k, called interleave, interleaves strings resulting argument expressions. string z results interleaving two strings x whenever zcomposed occurrences symbols x y, symbolsappear within z relative order within x y. example, consider strings abcd efg. interleaving two strings obtain, among manyothers, strings abecfgd, eabfgcd efabcdg. formal language literature, operation also called shuffle, instance Dassow Paun(1989). discontinuous parsing literature literature head-drivenphrase-structure grammars (HPSG, Pollard & Sag, 1994) interleave operationalso called sequence union (Reape, 1989) domain union (Reape, 1994).interleave operator also occurs XML tool described van der Vlist (2003).Operator , called disjunction, allows choice strings resultingargument expressions. standard operator regular expressions,commonly written +.Operator , called lock, takes single IDL-expression argument. operatorstates additional material interleaved string resultingargument. lock operator previously used discontinuous parsingliterature, instance Daniels Meurers (2002), Gotz Penn (1997),Ramsay (1999), Suhre (1999). context, operator called isolation.interleave, disjunction lock operators also called I, L operators,respectively. see later, combination L operators within IDLexpressions provides much power existing formalisms represent free word order,maintaining computational properties quite close regular expressionsfinite automata.introductory example, discuss following IDL-expression, definedword alphabet {piano, play, must, necessarily, we}.k((necessarily, must), (play piano)).(2)IDL-expression (2) says words we, play piano must appear ordergenerated strings, specified two occurrences concatenation operator.Furthermore, use lock operator states additional words ever appearplay piano. disjunction operator expresses choice wordsnecessarily must. Finally, interleave operator states word resultingfirst arguments must inserted sequence we, play, piano,available positions. Notice interaction lock operator, which, seen,makes unavailable position play piano. Thus following sentences,among others, generated IDL-expression (2):293fiNederhof & Sattanecessarily play pianomust play pianomust play pianoplay piano necessarily.However, following sentences cannot generated IDL-expression (2):play necessarily pianonecessarily must play piano.first sentence disallowed use lock operator, second sentenceimpossible disjunction operator states exactly one arguments mustappear sentence realization. provide formal definition class IDLexpressions.Definition 1 Let finite alphabet let E symbol . IDLexpression string satisfying one following conditions:(i) = a, {E};(ii) = ( 0 ), 0 IDL-expression;(iii) = (1 , 2 , . . . , n ), n 2 IDL-expression i, 1 n;(iv) = k(1 , 2 , . . . , n ), n 2 IDL-expression i, 1 n;(v) = 1 2 , 1 2 IDL-expressions.take infix operator right associative, although definitionspaper, disambiguation associativity relevant taken arbitrarily.say IDL-expression 0 subexpression 0 appears argumentoperator .develop precise semantics IDL-expressions. technical difficultyarises proper treatment lock operator.3 Let x string. basic idea use new symbol , already . occurrencetwo terminals indicates additional string inserted position.example, x = x0 x00 x000 x0 , x00 x000 strings , needinterleave x string y, may get result string x0 yx00 x000 stringx0 x00 yx000 . lock operator corresponds removal every occurrencestring.precisely, strings ( {}) used represent sequences strings; symbol used separate strings sequence. Furthermore, introducestring homomorphism lock ( {}) letting lock(a) = lock() = .application lock input sequence seen operation concatenatingtogether strings sequence.3. add Kleene star, infinite languages specified, interleave lockconveniently defined using derivatives (Brzozowski, 1964), noted van der Vlist (2003).294fiIDL-Expressions: Formalism Finite Languagesdefine basic operation comb, plays important role sequel.operation composes two sequences x strings, represented explained above,set new sequences strings. done interleaving two input sequencesevery possible way. Operation comb makes use auxiliary operation comb0 ,also constructs interleaved sequences input sequences x y, always startingfirst string first argument x. sequence comb(x, y) must startstring x string y, comb(x, y) union comb0 (x, y) comb0 (y, x).definition comb0 , distinguish case x consists single stringcase x consists least two strings. latter case, tail outputsequence obtained applying comb recursively tail sequence xcomplete sequence y. x, ( {}) , have:comb(x, y) = comb0 (x, y) comb0 (y, x){x y}, x ;{x0 } comb(x00 , y),comb0 (x, y) =x0 x00x = x0 x00 .example, let = {a, b, c, d, e} consider two sequences bb c e.comb(a bb c, e) ={a bb c e, bb c e, bb e c,bb c e, bb e c, e bb c,bb c e, bb e c, e bb c,e bb c}.languages L1 , L2 define comb(L1 , L2 ) = xL1 ,yL2 comb(x, y). generally,languages L1 , L2 , . . . , Ld , 2, define combdi=1 Li = comb(L1 , L2 ) = 2,combdi=1 Li = comb(combd1i=1 Li , Ld ) > 2.Definition 2 Let finite alphabet. Let function mapping IDL-expressionssubsets ( {}) , specified following conditions:(i) (a) = {a} , (E) = {};(ii) (()) = lock(());(iii) ((1 , 2 , . . . , n )) = ni=1 (i );(iv) (k(1 , 2 , . . . , n )) = combni=1 (i );(v) ( 0 ) = () ( 0 ).set strings satisfy IDL-expression , written L(), given L() =lock(()).295fiNederhof & Sattaexample definition, show interleave operatorused IDL-expression denote set strings realizing permutations givenbag symbols. Let = {a, b, c}. Consider bag ha, a, b, c, ci IDL-expressionk(a, a, b, c, c).(3)applying Definition 2 IDL-expression (3), obtain first steps(a) = {a},(b) = {b},(c) = {c},(k(a, a)) = comb({a}, {a}) = {a a},(k(a, a, b)) = comb({a a}, {b}) = {b a, b a, b}.next step obtain 3 4 sequences length 4, using symbolsbag ha, a, b, ci. One application comb operator, set set {c}, provides possible sequences singleton strings expressing permutations symbols bagha, a, b, c, ci. removing symbol throughout, conceptually turns sequencesstrings undivided strings, obtain desired language L(k(a, a, b, c, c)) permutations bag ha, a, b, c, ci.conclude section, compare expressivity IDL-expressionsformalisms discussed Section 2. means simple example.follows, use alphabet {NP, PP, V}. symbols denote units standardlyused syntactic analysis natural language, stand for, respectively, noun phrase,prepositional phrase verb. Symbols NP, PP V rewritten actual wordslanguage, use terminal symbols simplify presentation. Considerlanguage subject-verb-object (SVO) order sentence structure[S NP1 V NP2 ],NP1 realizes subject position NP2 realizes object position. Let PP1PP2 phrases must inserted sentence modifiers. Assumeknow language hand allow modifiers appear verbalobject positions. left 3 available positions realizationfirst modifier, 4 positions string. first modifier insertedwithin string, 5 positions, 4 available realization secondmodifier, assumption. results total 3 4 = 12 possible sentencerealizations.bag words sentences unable capture constraintpositioning modifiers. time, word lattice sentences would contain12 distinct paths, corresponding different realizations modifiers basicsentence. Using IDL formalism, easily capture desired realizations meansIDL-expression:k(PP1 , PP2 , NP1 (V NP2 )).296fiIDL-Expressions: Formalism Finite LanguagesAgain, note presence lock operator, implements restrictionmodifiers appearing verbal object position, similarlydone IDL-expression (2).Consider sentence subordinate clause, structure[S NP1 V1 NP2 [S0 NP3 V2 NP4 ]],assume modifiers PP1 PP2 apply main clause, modifiers PP3PP4 apply subordinate clause. before, 3 4 possible realizationssubordinate sentence. allow main clause modifiers appear positionssubordinate clause well subordinate clause, 45 possible realizationsmain sentence. Overall, gives total 3 42 5 = 240 possible sentencerealizations.Again, bag representation sentences unable capture restrictions word order, would therefore badly overgenerate. Since main sentencemodifiers could placed subordinate clause, need recordtwo modifiers main clause whether already seen, processing 12possible realizations subordinate clause. increases size representationfactor 2 2 = 4. hand, desired realizations easily capturedmeans IDL-expression:k(PP1 , PP2 , NP1 (V1 NP2 ) (k(PP3 , PP4 , NP3 (V2 NP4 )))).Note use embedded lock operators (the two rightmost occurrences). rightmostleftmost occurrences lock operator implement restriction modifiers appearing verbal object position. occurrence lockoperator middle IDL-expression prevents modifiers PP1 PP2modifying elements appearing within subordinate clause. Observegeneralize examples embedding n subordinate clauses, corresponding wordlattice grow exponentially n, IDL-expression linear size n.5. IDL-GraphsAlthough IDL-expressions may easily composed linguists, allow directalgorithmic interpretation efficient recognition strings. therefore define equivalent lower-level representation IDL-expressions, call IDL-graphs.purpose, exploit specific kind edge-labelled acyclic graphs ranked nodes.first introduce notation, define encoding function IDL-expressionsIDL-graphs.graphs use denoted tuples (V, E, vs , , , r), where:V E finite sets vertices edges, respectively;vs special vertices V called start end vertices, respectively;edge-labelling function, mapping E alphabet {, `, a};r vertex-ranking function, mapping V N, set non-negative integernumbers.297fiNederhof & SattaLabel indicates edge consume input symbols. Edge labels `meaning, additionally encode start end, respectively, corresponds operator. precisely, let IDL-expressionheaded occurrence operator let () associated IDL-graph.use edges labelled ` connect start vertex () start verticessubgraphs encoding arguments I. Similarly, use edges labelled connectend vertices subgraphs encoding arguments end vertex(). Edge labels ` needed next section distinguish occurrencesoperator occurrences L operators. Finally, function r ranksvertex according deeply embedded (the encoding of) expressions headedoccurrence L operator. see later, information necessaryprocessing locked vertices correct priority.map IDL-expression corresponding IDL-graph.Definition 3 Let finite alphabet, let j non-negative integer number.IDL-expression associated graph j () = (V, E, vs , , , r)specified follows:(i) = a, {E}, let vs , new nodes;(a) V = {vs , },(b) E = {(vs , )},(c) ((vs , )) = ((vs , )) = = E,(d) r(vs ) = r(ve ) = j;(ii) = ( 0 ) j+1 ( 0 ) = (V 0 , E 0 , vs0 , ve0 , 0 , r0 ), let vs , new nodes;(a) V = V 0 {vs , },(b) E = E 0 {(vs , vs0 ), (ve0 , )},(c) (e) = 0 (e) e E 0 , ((vs , vs0 )) = ((ve0 , )) = ,(d) r(v) = r0 (v) v V 0 , r(vs ) = r(ve ) = j;(iii) = (1 , 2 , . . . , n ) j (i ) = (Vi , Ei , vi,s , vi,e , , ri ), 1 n, let vs ,new nodes;(a) V = ni=1 Vi {vs , },(b) E = ni=1 Ei {(vs , vi,s ) | 1 n} {(vi,e , ) | 1 n},(c) (e) = (e) e Ei , ((vs , vi,s )) = ((vi,e , )) = 1 n,(d) r(v) = ri (v) v Vi , r(vs ) = r(ve ) = j;(iv) = k(1 , 2 , . . . , n ) j (i ) = (Vi , Ei , vi,s , vi,e , , ri ), 1 n, let vs ,new nodes;(a) V = ni=1 Vi {vs , },(b) E = ni=1 Ei {(vs , vi,s ) | 1 n} {(vi,e , ) | 1 n},298fiIDL-Expressions: Formalism Finite Languagesv1v2necessarily0v00v50v3vsv4must00000v60v70v80v9 playv101v11 pianov1211Figure 1:IDL-graphassociatedk((necessarily, must), (play piano)).v1310IDL-expression(c) (e) = (e) e Ei , ((vs , vi,s )) = ` ((vi,e , )) = 1 n,(d) r(v) = ri (v) v Vi , r(vs ) = r(ve ) = j;(v) = 1 2 j (i ) = (Vi , Ei , vi,s , vi,e , , ri ), {1, 2}, let vs = v1,s = v2,e ;(a) V = V1 V2 ,(b) E = E1 E2 {(v1,e , v2,s )},(c) (e) = (e) e Ei {1, 2}, ((v1,e , v2,s )) = ,(d) r(v) = ri (v) v Vi , {1, 2}.let () = 0 (). IDL-graph graph form () IDLexpression .Figure 1 presents IDL-graph (), IDL-expression (2).introduce important notion cut IDL-graph. notion neededdefine language described IDL-graph, talk equivalenceIDL-expressions IDL-graphs. time, notion play crucialrole specification parsing algorithm IDL-graphs next section. Letus fix IDL-expression let () = (V, E, vs , , , r) associated IDLgraph. Intuitively speaking, cut () set vertices might reachsimultaneously traversing () start vertex end vertex, followingdifferent branches prescribed encoded I, L operators, attemptproduce string L().follows view V finite alphabet, define set V containstrings V symbol occurs once. Therefore V finite setstring c V |c| |V |. assume outgoing edges vertexIDL-graph linearly ordered, represent cuts canonical way meansstrings V defined below.299fiNederhof & SattaLet r ranking function associated (). write c[v1 vm ] denotestring c V satisfying following conditions:c form xv1 vm x, V vi V 1 m;vertex v within c i, 1 m, r(v) r(vi ).words, c[v1 vm ] indicates vertices v1 , . . . , vm occur adjacent cmaximal rank among vertices within string c. Let c[v1 vm ] = xv1 vm string0 V second string symbol v 0 , 1 m0 ,defined let v10 vm00 ] denote string xv 0 v 0 V .appears x y. write c[v1 vm := v10 vm01m0reason distinguish vertices maximal rank lower rankformer correspond subexpressions nested deeper within subexpressions headed L operator. substring originating within scope occurrencelock operator cannot interleaved symbols originating outside scope,terminate processing vertices higher rank resuming processinglower rank.define relation plays crucial role definition notion cut,well specification parsing algorithm.Definition 4 Let finite alphabet, let IDL-expression , let() = (V, E, vs , , , r) associated IDL-graph. relation () V ( {}) Vsmallest satisfying following conditions:(i) c[v] V (v, v 0 ) E ((v, v 0 )) = X {},(c[v], X, c[v := v 0 ]) () ;(4)(ii) c[v] V outgoing edges v exactly (v, v1 ), . . . , (v, vn ) E,order, ((v, vi )) = `, 1 n,(c[v], , c[v := v1 vn ]) () ;(5)(iii) c[v1 vn ] V incoming edges v V exactly(v1 , v), . . . , (vn , v) E, order, ((vi , v)) = a, 1 n,(c[v1 vn ], , c[v1 vn := v]) () .(6)Henceforth, abuse notation writing place () . Intuitively speaking,relation used simulate one-step move IDL-graph (). Condition (4)refers moves follow single edge graph, labelled symbol alphabetempty string. move exploited, e.g., upon visiting vertex startsubgraph encodes IDL-expression headed occurrence operator.case, outgoing edge represents possible next move, one edgechosen. Condition (5) refers moves simultaneously follow edges emanatingvertex hand. used processing vertex start subgraphencodes IDL-expression headed occurrence operator. fact, accordance300fiIDL-Expressions: Formalism Finite Languagesgiven semantics, possible argument expressions must evaluated parallelsingle computation. Finally, Condition (6) refers move readcomplement previous type move.Examples elements case Figure 1 (vs , , v0 v6 ) followingCondition (5) (v5 v13 , , ) following Condition (6), start endevaluation occurrence operator. elements (v0 v6 , , v1 v6 ),(v1 v9 , play, v1 v10 ) (v1 v13 , necessarily, v2 v13 ) following Condition (4). Note that, e.g.,(v1 v10 , necessarily, v2 v10 ) element , v9 higher rank v1 .ready define notion cut.Definition 5 Let finite alphabet, let IDL-expression , let() = (V, E, vs , , , r) associated IDL-graph. set cuts (), writtencut(()), smallest subset V satisfying following conditions:(i) string vs belongs cut(());(ii) c cut(()) (c, X, c0 ) , string c0 belongs cut(()).Henceforth, abuse notation writing cut() cut(()). already remarked,interpret cut v1 v2 vk cut(), vi V 1 k, follows.attempt generate string L(), traverse several paths IDL-graph ().corresponds parallel evaluation subexpressions , viv1 v2 vk refers one subexpression. Thus, k provides number evaluationscarrying parallel point computation represented cut. Notehowever that, drawing straight line across planar representation IDL-graph,separating start vertex end vertex, set vertices identifynecessarily cut.4 fact, already explained discussing relation ,one path followed start subgraph encodes IDL-expression headedoccurrence operator. Furthermore, even several arcs followedstart subgraph encodes IDL-expression headed occurrenceoperator, combinations vertices satisfy definition cutL operators within argument expressions. observations preciselyaddressed Section 7, provide mathematical analysis complexityalgorithm.Examples cuts case Figure 1 vs , , v0 v6 , v1 v6 , v3 v6 , v0 v7 , etc. Stringsv1 v3 cuts, v1 v3 belong two disjoint subgraphs sets vertices{v1 , v2 } {v3 , v4 }, respectively, corresponds different argumentoccurrence disjunction operator.Given notion cut, associate finite language IDL-graphtalk equivalence IDL-expressions. Let IDL-expression , let() = (V, E, vs , , , r) associated IDL-graph. Let also c, c0 cut() w .write w L(c, c0 ) exists q |w|, Xi {}, 1 q, ci cut(),0 q, X1 Xq = w, c0 = c, cq = c0 (ci1 , Xi , ci ) 1 q.4. pictorial representation mentioned comes close different definition cut standardliterature graph theory operating research. reader aware standardgraph-theoretic notion cut different one introduced paper.301fiNederhof & Sattaalso assume L(c, c) = {}. show L(vs , ) = L(), i.e.,language generated IDL-expression language obtaintraversal IDL-graph (), described above, starting cut vs endingcut . proof property rather long add much alreadyprovided intuition underlying definitions section; therefore omit it.close section informal discussion relation associated notioncut. Observe Definition 4 Definition 5 implicitly define nondeterministic finiteautomaton. Again, refer reader Harrison (1978) definition finite automata.states automaton cuts cut() transitions givenelements . initial state automaton cut vs , final statecut . difficult see every state automaton one always reachfinal state. Furthermore, language recognized automaton preciselylanguage L(vs , ) defined above. However, remark automatonnever constructed parsing algorithm, emphasized next section.6. CFG Parsing IDL-Graphsstart section brief overview Earley algorithm (Earley, 1970), wellknown tabular method parsing input strings according given CFG. reformulate Earley algorithm order parse IDL-graphs. already mentionedintroduction, parsing traditionally defined input consisting single string,conceive parsing process carried input device representinglanguage, i.e., set strings.Let G = (N , , P, S) CFG, let w = a1 input stringparsed. Standard implementations Earley algorithm (Graham & Harrison, 1976) usecalled parsing items record partial results parsing process w. parsing itemform [A , i, j], production G j indicesidentifying substring ai+1 aj w. parsing item constructed algorithmexist string (N ) two derivations G formG a1 aiG a1 ai ;G ai+1 aj .algorithm accepts w construct item form [S , 0, n],production G. Figure 2 provides abstract specification algorithmexpressed deduction system, following Shieber, Schabes, Pereira (1995). Inferencerules specify types steps algorithm apply constructing new items.Rule (7) Figure 2 serves initialization step, constructing itemsstart analyses productions start symbol right-hand side. Rule (8)similar purpose: constructs items start analyses productionsnonterminal B left-hand side, provided B next nonterminalexisting item analysis found. Rule (9) matches terminal iteminput symbol, new item signifies larger part right-hand sidematched larger part input. Finally, Rule (10) combines two partial302fiIDL-Expressions: Formalism Finite Languages[S , 0, 0](7)[A B, i, j]B[B , j, j](8)[A a, i, j]= aj+1[A , i, j + 1](9)[A B, i, j][B , j, k][A B , i, k](10)Figure 2: Abstract specification parsing algorithm Earley input stringa1 . algorithm accepts w construct itemform [S , 0, n], production G.analyses, second represents analysis symbol B, analysisrepresented first item extended.move algorithm IDL-graph parsing using CFG. algorithmmakes use relation Definition 4, mean relationfully computed invoking algorithm. instead compute elements onthe-fly first visit cut, cache elements possible later use.advantage that, parsing input IDL-graph, algorithm processesportions graph represent prefixes strings generated CFGhand. practical cases, input IDL-graph never completely unfolded,compactness proposed representation preserved large extent.alternative way viewing algorithm this. already informally discussed Section 5 relation implicitly defines nondeterministic finite automatonwhose states elements cut() whose transitions elements .also mentioned automaton precisely recognizes finite language L().perspective, algorithm seen standard lattice parsing algorithm,discussed Section 2. must emphasized precomputefinite automaton prior parsing. approach consists lazy evaluationtransitions automaton, basis demand part parsing process.contrast approach, full expansion finite automaton parsingseveral disadvantages. Firstly, although finite automaton generating finite language303fiNederhof & Sattamight considerably smaller representation language consistinglist elements, easy see cases finite automatonmight size exponentially larger corresponding IDL-expression (see alsodiscussion Section 2). cases, full expansion destroys compactness IDLexpressions, main motivation use formalism hybrid surfacegeneration systems, discussed introduction. Furthermore, full expansionautomaton also computationally unattractive, since may lead unfolding partsinput IDL-graph never processed parsing algorithm.Let G = (N , , P, S) CFG let input IDL-expression. algorithmuses parsing items form [A , c1 , c2 ], production Pc1 , c2 cut(). items meaning used original Earleyalgorithm, refer strings languages L(vs , c1 ) L(c1 , c2 ), vsstart vertex IDL-graph (). (Recall Section 5 L(c, c0 ), c, c0 cut(),set strings whose symbols consumed traversal () startingcut c ending cut c0 .) also use items forms [c1 , c2 ] [a, c1 , c2 ], ,c1 , c2 cut(). done order by-pass traversals () involving sequence zerotriples form (c1 , , c2 ) , followed triple form (c1 , a, c2 ) .Figure 3 presents abstract specification algorithm, using set inferencerules. issues control flow implementation deferred next section.follows, let vs start end vertices IDL-graph (), respectively. Rules (11), (12) (15) Figure 3 closely resemble Rules (7), (8) (10)original Earley algorithm, reported Figure 2. Rules (13), (16) (17) introduced purpose efficiently computing traversals () involving sequence zerotriples form (c1 , , c2 ) , followed triple form (c1 , a, c2 ) ,already mentioned. one traversal computed, fact recordeditem form [a, c1 , c2 ], avoiding later recomputation. Rule (14) closelyresembles Rule (9) original Earley algorithm. Finally, computing traversals() involving triples form (c1 , , c2 ) only, Rule (18) may derive itemsform [S , vs , ]; algorithm accepts input IDL-graphitem derived inference rules.turn discussion correctness algorithm Figure 3.algorithm derives parsing item [A , c1 , c2 ] exist string(N ) , integers i, j 0 j, a1 a2 aj followingconditions satisfied:a1 ai L(vs , c1 );ai+1 aj L(c1 , c2 );exist two derivations G formG a1 aiG a1 aiG ai+1 aj .statement closely resembles existential condition previously discussedoriginal Earley algorithm, proved using arguments similar presented304fiIDL-Expressions: Formalism Finite Languages[S , vs , vs ](11)[A B, c1 , c2 ]B[B , c2 , c2 ](12)[A a, c1 , c2 ][c2 , c2 ](13)[A a, c1 , c2 ][a, c2 , c3 ][A , c1 , c3 ][A B, c1 , c2 ][B , c2 , c3 ][A B , c1 , c3 ][c1 , c2 ](c2 , , c3 )[c1 , c3 ][c1 , c2 ][a, c1 , c3 ](c2 , a, c3 ) ,[S , c0 , c1 ](c1 , , c2 )[S , c0 , c2 ](14)(15)(16)(17)(18)Figure 3: abstract specification parsing algorithm IDL-graphs. algorithmaccepts IDL-graph () item form [S, vs , ] derived inference rules, production Gvs start end vertices (), respectively.305fiNederhof & Sattainstance Aho Ullman (1972) Graham Harrison (1976); thereforeomit complete proof here. Note correctness algorithm Figure 3 directlyfollows statement, taking item [A , c1 , c2 ] form[S , vs , ] production G.7. Complexity Implementationsection provide computational analysis parsing algorithm IDL-graphs.analysis based development tight upper bound number possiblecuts admitted IDL-graph. also discuss two possible implementationsparsing algorithm.need introduce notation. Let IDL-expression let () =(V, E, vs , , , r) associated IDL-graph. vertex v V called L-free () if,every subexpression 0 j ( 0 ) = (V 0 , E 0 , vs0 , ve0 , 0 , r0 ) j, V 0 V ,E 0 E, v V 0 , 0 form ( 00 ). words, vertexL-free () belong subgraph () encodes IDL-expressionheaded L operator. () understood context, write L-freeplace L-free (). write 0-cut() denote set cuts cut()contain vertices L-free (). introduce two functions usedlater complexity analysis algorithm. cut c cut() write |c| denotelength c, i.e., number vertices cut.Definition 6 Let IDL-expression. Functions width 0-width specifiedfollows:width() =max |c| ,ccut()0-width() =maxc0-cut()|c| .Function width provides maximum length cut (). quantity givesmaximum number subexpressions need evaluated parallelgenerating string L(). Similarly, function 0-width provides maximum lengthcut () includes L-free nodes.Despite fact cut() always finite set, computation functions width0-width direct computation cut() 0-cut() practical, sincesets may exponential size number vertices (). next characterizationprovides efficient way compute functions, used proofLemma 2 below.Lemma 1 Let IDL-expression. quantities width() 0-width() satisfyfollowing equations:(i) = a, {E},width() = 1,0-width() = 1;306fiIDL-Expressions: Formalism Finite Languages(ii) = ( 0 )width() = width( 0 ),0-width() = 1;(iii) = (1 , 2 , . . . , n )nwidth() = max width(i ),i=1n0-width() = max 0-width(i );i=1(iv) = k(1 , 2 , . . . , n )nwidth() = max (width(j ) +j=10-width() =nXX0-width(i )),i:1ini6=j0-width(j );j=1(v) = 1 2width() = max {width(1 ), width(2 )},0-width() = max {0-width(1 ), 0-width(2 )}.Proof. equations statement lemma straightforwardly followdefinitions cut() (Definitions 4 5, respectively). developlength two cases leave remainder proof reader. followsassume () = (V, E, vs , , , r).case = (1 , 2 , . . . , n ), let (i ) = (Vi , Ei , vi,s , vi,e , , ri ), 1 n.Definition 4 (vs , , vi,s ) (vi,e , , ) , every i, 1 n. Thuscut() = ni=1 cut(i ) {vs , } and, since vs L-free (),0-cut() = ni=1 0-cut(i ) {vs , }. provides relations (iii).case = k(1 , 2 , . . . , n ), let (i ) = (Vi , Ei , vi,s , vi,e , , ri ), 1 n.Definition 4 (vs , , v1,s vn,s ) (v1,e vn,e , , ) . Thus everyc cut() must belong {vs , } must form c = c1 cn ci cut(i )1 n. Since vs L-free (), immediately derive0-cut() = {vs , } 0-cut(1 ) 0-cut(n ),Phence 0-width() = nj=1 0-width(j ). observe that, c = c1 cn specifiednever indices j, 1 i, j n 6= j, vertices v1 v2occurring ci cj , respectively, neither v1 v2 L-free ().thereby derivecut() = {vs , }cut(1 )0-cut(2 ) 0-cut(n )0-cut(1 )cut(2 ) 0-cut(n )...0-cut(1 )0-cut(2 ) cut(n ).307fiNederhof & SattaPHence write width() = maxnj=1 (width(j ) + i:1ini6=j 0-width(i )).consider quantity |cut()|, i.e., number different cuts IDL-graph ().quantity obviously bounded |V |width() . derive tighterupper bound quantity.Lemma 2 Let finite alphabet, let IDL-expression , let () =(V, E, vs , , , r) associated IDL-graph. Let also k = width().|cut()||V |kk.Proof. use following inequality. integer h 2 real values xi > 0,1 h,hxiPhi=1i=1hxi!h.(19)words, (19) states geometric mean never larger arithmetic mean.prove (19) following equivalentform. real values c > 0 yi ,P1 h h 2, yi > c hi=1 yi = 0,h(c + yi ) ch .(20)i=1start observing yi equal zero, done. Otherwisemust j 1 i, j h yi yj < 0. Without loss generality, assume= 1 j = 2. Since yi yj < 0,(c + y1 )(c + y2 ) = c(c + y1 + y2 ) + y1 y2 < c(c + y1 + y2 ).SinceQhi=3 (c(21)+ yi ) > 0,(c + y1 )(c + y2 )h(c + yi ) < c(c + y1 + y2 )i=3h(c + yi ).(22)i=3observe right-hand side (22) form left-hand side(20), fewer yi non-zero. therefore iterate procedure,yi become zero valued. concludes proof (19).Let us turn proof statement lemma. Recall cut c cut()string V vertex V one occurrence c, ccanonically represented, i.e., permutation vertices c possible cut.later prove following claim.Claim. Let , V k statement lemma. partition Vsubsets V [, j], 1 j k, following property. every V [, j], 1 j k,every pair distinct vertices v1 , v2 V [, j], v1 v2 occur together cutc cut().308fiIDL-Expressions: Formalism Finite Languageswrite|cut()|Qkj=1Pk=|V [, j]|j=1|V |kk|V [,j]|kk(by claim canonicalrepresentation cuts)(by (19)).complete proof lemma need prove claim above. provefollowing statement, slightly stronger version claim. partitionset V subsets V [, j], 1 j k = width(), following two properties:every V [, j], 1 j k, every pair distinct vertices v1 , v2 V [, j], v1v2 occur together cut c cut();vertices V L-free () included V [, j], 1 j0-width(). (In words, sets V [, j], 0-width() < j width(),contain vertices L-free ().)follows use induction #op (), number operator occurrences (I, D, Lconcatenation) appearing within .Base: #op () = 0. = a, {E}, V = {vs , vf }. Since width() = 1,set V [, 1] = V . satisfies claim, since cut() = {vs , vf }, vertices VL-free () 0-width() = 1.Induction: #op () > 0. distinguish among three possible cases.Case 1: = (1 , 2 , . . . , n ). Let (i ) = (Vi , Ei , vi,s , vi,e , , ri ), 1 n. Lemma 1width() = maxni=1 width(i ). i, 1 n, let us define V [i , j] =every j width(i ) < j width(). setV [, 1] = (ni=1 V [i , 1]) {vs , };V [, j] = ni=1 V [i , j], 2 j width().sets V [, j] define partition V , since V = (ni=1 Vi ) {vs , } and, i,sets V [i , j] define partition Vi inductive hypothesis. showpartition satisfies two conditions statement.Let v1 v2 two distinct vertices V [, j]. already establishedproof Lemma 1 cut() = (ni=1 cut(i )) {vs , }. either v1 v2 belongsset {vs , }, v1 v2 cannot occur cut cut(), sincecuts cut() vertices set {vs , } vs . Let us consider casev1 , v2 ni=1 Vi . distinguish two subcases. first subcase, existsv1 , v2 V [i , j]. inductive hypothesis states v1 v2 cannot occurcut cut(i ), hence cannot occur cut cut(). secondsubcase, v1 V [i , j] v2 V [i0 , j] distinct i0 . v1 v2 must belongdifferent graphs (i ) (i0 ), hence cannot occur cut cut().Furthermore, every vertex ni=1 Vi L-free (i ) belongsV [i , j] 1 j 0-width(i ), inductive hypothesis. Since 0-width() =309fiNederhof & Sattamaxni=1 0-width(i ) (Lemma 1) state vertices V L-free ()belong V [, j], 1 j 0-width().Case 2: = ( 0 ) = 1 2 . proof almost identical Case 1,n = 1 n = 2, respectively.Case 3: = k(1 , 2 , . . . , n ). Let (i ) = (Vi , Ei , vi,s , vi,e , , ri ), 1 n. Lemma 10-width() =nX0-width(j ),j=1nwidth() = max (width(j ) +j=1X0-width(i )).i:1ini6=jlatter equation rewrittenwidth() =nXn0-width(j ) + max (width(j ) 0-width(j )).j=1j=1(23)1 n, let us define V [i , j] = every j width(i ) < j width().setV [, 1] = V [1 , 1] {vs , };V [, j] = V [1 , j], 2 j 0-width(1 );V [, 0-width(1 ) + j] = V [2 , j], 1 j 0-width(2 );...Pn1V [, Pi=1 0-width(i ) + j] = V [n , j], 1 j 0-width(n );V [, ni=1 0-width(i ) + j] = ni=1 V [i , 0-width(i ) + j],1 j maxnj=1 (width(j ) 0-width(j )).sets V [, j] define partition V , since V = (ni=1 Vi ) {vs , } and, i,sets V [i , j] define partition Vi inductive hypothesis. showpartition satisfies conditions statement.Let v1 v2 distinct vertices V [, j], 1 j n. already establishedproof Lemma 1 cut c cut() either belongs {vs , } else mustform c = c1 cn ci cut(i ) 1 n. Case 1, either v1 v2 belongsset {vs , }, v1 v2 cannot occur cut cut(), sincecuts cut() vertices set {vs , } vs . Consider casev1 , v2 ni=1 Vi . distinguish two subcases.first subcase, exists v1 , v2 V [i , j]. exists cutc cut() v1 v2 occur within c, v1 v2 must occur withinc0 cut(i ). contradicts inductive hypothesis .second subcase, v1 V [i0 , j 0 ] v2 V [i00 , j 00 ], distinct i0 i00 . Notehappen 0-width() < j width(), 0-width(i0 ) < j 0 width(i0 )0-width(i00 ) < j 00 width(i00 ), definition partition V (23).inductive hypothesis i0 i00 , v1 L-free (i0 ) v2 L-free (i00 ),means v1 v2 occur within scope occurrence lock310fiIDL-Expressions: Formalism Finite Languagesoperator. Note however v1 v2 cannot occur within scope occurrencelock operator, since belong different subgraphs (i0 ) (i00 ). Assumeexists cut c cut() v1 v2 occur within c. wouldinconsistent definitions cut (Definitions 4 5, respectively) sincetwo vertices L-free within scope occurrencelock operator cannot belong cut.Finally, directly follows definition partition V inductive hypothesis vertices V L-free () belongV [, j] 1 j 0-width(). concludes proof statement.upper bound reported Lemma 2 tight. example, 1 k 2,let i,k = {a1 , . . . , aik }. Consider class IDL-expressionsi,k = k(a1 a2 ai , ai+1 ai+2 a2i , . . . , ai(k1)+1 ai(k1)+2 aik ).Let also Vi,k vertex set IDL-graph (i,k ). difficult see|Vi,k | = 2 k + 2, width(i,k ) = k2|cut(i,k )| = (2 i)k + 2 (2 + )k ,kinequality results upper bound. coarser upper bound presentedLemma 2 would give instead |cut(i,k )| < (2 k + 2)k .turn discussion worst case running time algorithmFigure 3. simplify presentation, let us ignore moment term solelydepends input grammar G.store retrieve items [A , c1 , c2 ], [a, c1 , c2 ] [c1 , c2 ] exploitdata structure access using cut c1 cut c2 indices. follows makeassumption access operation carried amount timeO(d(k)), k = width() function depends implementationdata structure itself, discussed later. access pair c1 , c2 ,array returned length proportional |G|. Thus, array inquireconstant time whether given item already constructed.worst case time complexity dominated rules Figure 3 involvemaximum number cuts, namely rules like (15) three cuts each. maximumnumber different calls rules proportional |cut()|3 . Consideringassumptions , total amount time charged executionrules O(d(k) |cut()|3 ). case standard Earley algorithm,working grammar G taken account must include factor |G|2 ,reduced |G| using techniques discussed Graham, Harrison, Ruzzo (1980).also need consider amount time required construction relation, happens on-the-fly, already discussed. takes place Rules (16), (17)(18). Recall elements relation form (c1 , X, c2 ) c1 , c2 cut()X {}. follows, view directed graph whose vertices cuts,thus refer elements relation (labelled) arcs. arc emanatingcut c1 label X visited first time, compute arcreached cut, cache possible later use. However, case reached cut c2already exists previously visited arc (c01 , X 0 , c2 ), cache311fiNederhof & Sattanew arc. arc , easily carried time O(k),k = width(). total time required on-the-fly construction relationO(k | |). later use, express bound terms quantity |cut()|.definition easily see one arc twocuts, therefore | | |cut()|2 . obviously k |V |. Also, difficultprove |V | |cut()|, using induction number operator occurrences appearingwithin . thus conclude that, worst case, total time required on-the-flyconstruction relation O(|cut()|3 ).observations conclude that, worst case, algorithmFigure 3 takes amount time O(|G| d(k) |cut()|3 ). Using Lemma 2,state following theorem.Theorem 1 Given context-free grammar G IDL-graph () vertex set Vk = width(), algorithm Figure 3 runs time O(|G| d(k)( |Vk | )3k ).closely consider choice data structure issueimplementation. discuss two possible solutions. first solution used|cut()| small enough store |cut()|2 pointers computers randomaccess memory. case implement square array pointers setsparsing items. cut cut() uniquely encoded non-negative integer,integers used access array. solution practice comesstandard implementation Earley algorithm parse table, presentedGraham et al. (1980). d(k) = O(1) algorithm time complexityO(|G| ( |Vk | )3k ).second solution, |cut()| quite large, implement trie (Gusfield,1997). case cut treated string set V , viewed alphabet,look string c1 #c2 (# symbol V ) order retrieve items involvingcuts c1 c2 induced far. obtain d(k) = O(k) algorithmtime complexity O(|G| k( |Vk | )3k ).first solution faster second one factor k. However, firstsolution obvious disadvantage expensive space requirements, since pairscuts might correspond grammar constituent, array sparsepractice. also observed that, natural language processing applicationsdiscussed introduction, k quite small, say three four.conclude section, compare time complexity CFG parsing traditionallydefined strings time complexity parsing IDL-graphs. reference stringparsing take Earley algorithm, already presented Section 6.minor change proposed Graham et al. (1980), Earley algorithm improvedtime complexity O(|G| n3 ), G input CFG n length inputstring. observe that, ignore factor d(k) time complexity IDL-graphparsing (Theorem 1), two upper bounds become similar, function ( |Vk | )kIDL-graph parsing replacing input sentence length n Earley algorithm.observe function ( |Vk | )k taken measure complexityinternal structure input IDL-expression. specifically, assume precedenceconstraints given words input IDL-expression. obtain IDLexpressions occurrences operator only, worst case k = |V2 | 1.312fiIDL-Expressions: Formalism Finite LanguagesO(( |Vk | )k ) written O(c|V | ) constant c > 1, resulting exponentialrunning time algorithm. comes surprise, since problem handbecomes problem recognition bag words CFG, knownNP-complete (Brew, 1992), already discussed Section 2.Conversely, operator may used IDL-expression , thus resultingrepresentation matches finite automaton word lattice. case k = 1function ( |Vk | )k becomes |V |. resulting running time cubic functioninput length, case Earley algorithm. fact (cyclic acyclic) finiteautomata parsed cubic time also well-known result (Bar-Hillel et al., 1964;van Noord, 1995).noteworthy observe applications k assumed bounded,algorithm still runs polynomial time. already discussed, practical applicationsnatural language generation, subexpressions processed simultaneously, k typically, say, three four. case algorithm behavesway much closer traditional string parsing bag parsing.conclude class IDL-expressions provides flexible representation bagswords precedence constraints, solutions range pure word bagswithout precedence constraints word lattices, depending value width().also proved fine-grained result time complexity CFG parsing problemIDL-expressions, depending values parameter width().8. Final RemarksRecent proposals view natural language surface generation multi-phase processfinite large sets candidate sentences first generated basis inputconceptual structure, filtered using statistical knowledge. architectures,crucial adopted representation set candidate sentences compact,time representation parsed polynomial time.proposed IDL-expressions solution problem. IDL-expressionscombine features considered isolation before. contrast existingformalisms, interaction features provides enough flexibility encode stringscases partial knowledge available word order, whereas parsingprocess remains polynomial practical cases.recognition algorithm presented IDL-expressions easily extendedparsing algorithm, using standard representations parse forests extractedconstructed parse table (Lang, 1994). Furthermore, productions CFGhand weighted, express preferences among derivations, easy extract parsehighest weight, adapting standard Viterbi search techniques used traditionalstring parsing (Viterbi, 1967; Teitelbaum, 1973).Although considered parsing problem CFGs, one may also parseIDL-expressions language models based finite automata, including n-gram models. Since finite automata represented right-linear context-free grammars,algorithm Figure 3 still applicable.Apart natural language generation, IDL-expressions useful wherever uncertainty word constituent order represented level syntax313fiNederhof & Sattalinearized purpose parsing. already discussed introduction,active research topic generative linguistics natural language parsing,given rise several paradigms, importantly immediate dominance linear precedence parsing (Gazdar, Klein, Pullum, & Sag, 1985), discontinuous parsing DanielsMeurers (2002), Ramsay (1999), Suhre (1999) grammar linearization (Gotz & Penn,1997; Gotz & Meurers, 1995; Manandhar, 1995). Nederhof, Satta, Shieber (2003)use IDL-expressions define new rewriting formalism, based context-free grammarsIDL-expressions right-hand sides productions. means formalism,fine-grained results proven immediate dominance linear precedence parsing.5IDL-expressions similar spirit formalisms developed programming language literature representation semantics concurrent programs.specifically, called series-parallel partially ordered multisets, series-parallel pomsets,proposed Gischer (1988) represent choice parallelism among processes.However, basic idea lock operator absent series-parallel pomsets.Acknowledgmentspreliminary version paper appeared Proceedings 7th ConferenceFormal Grammars (FG2002), Trento, Italy. notions IDL-graph cut, centralpresent study, found earlier paper. wish thank Michael Daniels,Irene Langkilde, Owen Rambow Stuart Shieber helpful discussions relatedtopics paper. also grateful anonymous reviewers helpful comments pointers relevant literature. first author supported PIONIERProject Algorithms Linguistic Processing, funded NWO (Dutch OrganizationScientific Research). second author supported MIUR project PRIN No.2003091149 005.ReferencesAho, A., & Ullman, J. (1972). Parsing, Vol. 1 Theory Parsing, TranslationCompiling. Prentice-Hall.Aust, H., Oerder, M., Seide, F., & Steinbiss, V. (1995). Philips automatic traintimetable information system. Speech Communication, 17, 249262.Bangalore, S., & Rambow, O. (2000). Exploiting probabilistic hierarchical model generation. 18th International Conference Computational Linguistics, Vol. 1,pp. 4248, Saarbrucken, Germany.Bar-Hillel, Y., Perles, M., & Shamir, E. (1964). formal properties simple phrasestructure grammars. Bar-Hillel, Y. (Ed.), Language Information: SelectedEssays Theory Application, chap. 9, pp. 116150. Addison-Wesley.Beaven, J. (1992). Shake-and-bake machine translation. Proc. fifteenth International Conference Computational Linguistics, Vol. 2, pp. 602609, Nantes.5. cited work, lock operator ignored, affect weak generative capacitycompactness grammars.314fiIDL-Expressions: Formalism Finite LanguagesBillot, S., & Lang, B. (1989). structure shared forests ambiguous parsing. 27thAnnual Meeting Association Computational Linguistics, ProceedingsConference, pp. 143151, Vancouver, British Columbia, Canada.Brew, C. (1992). Letting cat bag: generation Shake-and-Bake MT.Proc. fifteenth International Conference Computational Linguistics, Vol. 2,pp. 610616, Nantes.Brown, P., et al. (1990). statistical approach machine translation. ComputationalLinguistics, 16 (2), 7985.Brzozowski, J. (1964). Derivatives regular expressions. Journal ACM, 11 (4),481494.Charniak, E. (2001). Immediate-head parsing language models. 39th Annual Meeting10th Conference European Chapter Association ComputationalLinguistics, Proceedings Conference, pp. 116123, Toulouse, France.Daniels, M., & Meurers, W. (2002). Improving efficiency parsing discontinuousconstituents. Wintner, S. (Ed.), Proceedings NLULP02: 7th InternationalWorkshop Natural Language Understanding Logic Programming, Vol. 92Datalogiske Skrifter, pp. 4968, Copenhagen. Roskilde Universitetscenter.Dassow, J., & Paun, G. (1989). Regulated Rewriting Formal Language Theory. SpringerVerlag.Earley, J. (1970). efficient context-free parsing algorithm. Communications ACM,13 (2), 94102.Gazdar, G., Klein, E., Pullum, G., & Sag, I. (1985). Generalized Phrase Structure Grammar.Harvard University Press, Cambridge, MA.Gischer, J. (1988). equational theory pomsets. Theoretical Computer Science, 61,199224.Gotz, T., & Meurers, W. (1995). Compiling HPSG type constraints definite clauseprograms. 33rd Annual Meeting Association Computational Linguistics,Proceedings Conference, pp. 8591, Cambridge, Massachusetts, USA.Gotz, T., & Penn, G. (1997). proposed linear specification language. Volume 134Arbeitspapiere des SFB 340, Universitat Tubingen.Graham, S., & Harrison, M. (1976). Parsing general context free languages. AdvancesComputers, Vol. 14, pp. 77185. Academic Press, New York, NY.Graham, S., Harrison, M., & Ruzzo, W. (1980). improved context-free recognizer. ACMTransactions Programming Languages Systems, 2 (3), 415462.Gusfield, D. (1997). Algorithms Strings, Trees Sequences. Cambridge UniversityPress, Cambridge, UK.Harrison, M. (1978). Introduction Formal Language Theory. Addison-Wesley.Jurafsky, D., & Martin, J. (2000). Speech Language Processing. Prentice-Hall.Knight, K. (1999). Decoding complexity word-replacement translation models. Computational Linguistics, 25 (4), 607615.315fiNederhof & SattaKnight, K., & Hatzivassiloglou, V. (1995). Two-level, many-paths generation. 33rdAnnual Meeting Association Computational Linguistics, ProceedingsConference, pp. 252260, Cambridge, Massachusetts, USA.Lang, B. (1994). Recognition harder parsing. Computational Intelligence,10 (4), 486494.Langkilde, I. (2000). Forest-based statistical sentence generation. 6th Applied NaturalLanguage Processing Conference 1st Meeting North American ChapterAssociation Computational Linguistics, pp. 170177 (Section 2), Seattle,Washington, USA.Langkilde, I., & Knight, K. (1998). Generation exploits corpus-based statistical knowledge. 36th Annual Meeting Association Computational Linguistics17th International Conference Computational Linguistics, Vol. 1, pp. 704710,Montreal, Quebec, Canada.Manandhar, S. (1995). Deterministic consistency checking LP constraints. SeventhConference European Chapter Association Computational Linguistics,Proceedings Conference, pp. 165172, Belfield, Dublin, Ireland.Nederhof, M.-J., & Satta, G. (2004). language intersection problem non-recursivecontext-free grammars. Information Computation. Accepted publication.Nederhof, M.-J., Satta, G., & Shieber, S. (2003). Partially ordered multiset context-freegrammars free-word-order parsing. 8th International Workshop ParsingTechnologies, pp. 171182, LORIA, Nancy, France.Nevill-Manning, C., & Witten, I. (1997). Compression explanation using hierarchicalgrammars. Computer Journal, 40 (2/3), 103116.Pollard, C., & Sag, I. (1994). Head-Driven Phrase Structure Grammar. UniversityChicago Press.Ramsay, A. (1999). Direct parsing discontinuous phrases. Natural Language Engineering, 5 (3), 271300.Reape, M. (1989). logical treatment semi-free word order bounded discontinuousconstituency. Fourth Conference European Chapter AssociationComputational Linguistics, Proceedings Conference, pp. 103110, Manchester,England.Reape, M. (1994). Domain union word order variation german. Nerbonne, J.,Netter, K., & Pollard, C. (Eds.), German Head-Driven Phrase Structure Grammar,pp. 151197. CSLI Publications.Shieber, S., Schabes, Y., & Pereira, F. (1995). Principles implementation deductiveparsing. Journal Logic Programming, 24, 336.Suhre, O. (1999). Computational aspects grammar formalism languages freerword order. Diplomarbeit, Department Computer Science, University Tubingen.Published 2000 Volume 154 Arbeitspapiere des SFB 340.316fiIDL-Expressions: Formalism Finite LanguagesTeitelbaum, R. (1973). Context-free error analysis evaluation algebraic power series.Conference Record Fifth Annual ACM Symposium Theory Computing,pp. 196199.van der Vlist, E. (2003). RELAX NG. OReilly.van Noord, G. (1995). intersection finite state automata definite clause grammars. 33rd Annual Meeting Association Computational Linguistics,Proceedings Conference, pp. 159165, Cambridge, Massachusetts, USA.Viterbi, A. (1967). Error bounds convolutional codes asymptotically optimumdecoding algorithm. IEEE Transactions Information Theory, IT-13 (2), 260269.Whitelock, P. (1992). Shake-and-Bake translation. Proc. fifteenth InternationalConference Computational Linguistics, Vol. 2, pp. 784790, Nantes.317fiJournal Articial Intelligence Research 21 (2004) 37-62Submitted 03/03; published 01/04K-ImplementationDov MondererMoshe Tennenholtzdov@ie.technion.ac.ilmoshet@ie.technion.ac.ilFaculty Industrial Engineering ManagementTechnion Israel Institute TechnologyHaifa 32000, IsraelAbstractpaper discusses interested party wishes inuence behavior agentsgame (multi-agent interaction), control. interested partycannot design new game, cannot enforce agents behavior, cannot enforce paymentsagents, cannot prohibit strategies available agents. However, inuenceoutcome game committing non-negative monetary transfers dierentstrategy proles may selected agents. interested party assumesagents rational commonly agreed sense use dominated strategies.Hence, certain subset outcomes implemented given game adding nonnegative payments, rational players necessarily produce outcome subset.Obviously, making suciently big payments one implement desirable outcome.question cost implementation? paper introduce notionk-implementation desired set strategy proles, k stands amountpayment need actually made order implement desirable outcomes. majorpoint k-implementation monetary oers need necessarily materializefollowing desired behaviors. dene study k-implementation contexts gamescomplete incomplete information. latter case mainly focus VCGgames. setting later extended deal mixed strategies using correlation devices.Together, paper introduces studies implementation desirable outcomesreliable party cannot modify game rules (i.e. provide protocols), complementingprevious work mechanism design, making applicable many realistic CSsettings.1. Introductiondesign analysis interactions self-interested parties central theoryapplication multi-agent systems. particular, theory economic mechanism designor, generally, implementation theory (Maskin, 1999; Maskin & Sjostrom, 2002)become standard tool researchers areas multi-agent systems e-commerce(Rosenschein & Zlotkin, 1994; Nisan & Ronen, 1999; Shoham & Tennenholtz, 2001; Feigenbuam & S, 2002; Tennenholtz, 1999; Papadimitriou, 2001). classical mechanism design1center denes interaction self-motivated parties allow obtaindesired goal (such maximizing revenue social welfare) taking agents incentives1. See e.g., Fudenberg Tirole (1991), Chapter 7, Mas-Colell, Whinston, Green (1995), Chapter23.c2004AI Access Foundation. rights reserved.fiMonderer & Tennenholtzaccount. perspective largely motivated view centergovernment seller dene control rules interaction. However,many distributed systems multi-agent interactions, interested parties cannot controlrules interactions. network manager example cannot simply change communication protocols given distributed systems order lead desired behaviors,broker cannot change rules goods sold agency auctioneerpublic. focus paper reliable interested party, cannot changerules interaction, cannot enforce behavior, obtain desired goals (in servicecommunity benets). reliable party one source power:reliability. commit payments dierent agents, certain observableoutcomes reached, agents sure paid appropriately.work introduce study implementation desired behaviors interestedparty above.2 two major issues make task non-trivial challenging:1. interested party may wish assume little possible agents rationality.Ideally, assumed agent adopt strategy dominatedanother strategy.2. interested party may wish minimize expenses.Consider following simple congestion setting.3 Assume two agents,1 2, select among two service providers (e.g., machines, communicationlines, etc.) One service providers, f , fast one, other, s, slower one.capture agent obtaining payo 6 oneuses f , payo 4 one uses s. agents selectservice provider speed operation decreases factor 2, leading halfpayo. is, agents use f one obtains payo 3,agents use one obtains 2. matrix form, game describedfollowing bimatrix:f36f34M=42622. another interesting use interested party see Naor, Pinkas, Sumner (1999).3. Congestion context self-motivated parties central topic recent CS literature (Koutsoupias & Papadimitriou, 1999; Roughgarden, 2001; Roughgarden & Tardos, 2002), wellgame theory literature (Rosenthal, 1973; Monderer & Shapley, 1996). example used purposesillustration only; however, technique used example extended arbitrary complexgames, later show.38fiK-ImplementationAssume reliable interested party may wish prevent agents usingservice provider (leading low payos both). follows:promise pay agent 1 value 10 agents use f , promise pay agent 2value 10 agents use s. promises transform following game:f136f3=442612Notice , strategy f dominant agent 1, strategy dominantagent 2. result rational strategy prole one agent 1 chooses fagent 2 chooses s. Hence, interested party implements one desired outcomes.Moreover, given strategy prole (f, s) selected interested partypay nothing. implemented (in dominant strategies) desired behavior (obtainedone Nash equilibria) zero cost, relying creditability, withoutmodifying rules interactions enforcing type behavior.Similar simple examples found contexts (see e.g., Segal (1999), footnote30, Dybvig Spatt (1983), Spiegler (2000)). work advocates following linethought. Instead reasoning agents behave given protocol,may wish cause agents follow particular behaviors making desirable, usingmonetary oers. important point monetary oers need necessarilyfully materialized agents follow desired behavior.formally, paper introduce notion study k-implementationdesired set strategy proles, k stands amount payment needactually made order implement desirable outcomes.4 Section 3 providescharacterization k-implementation single pure strategy prole nite gamesinnite regular games complete information. provides eective algorithmdetermining optimal monetary oers made order implement desiredoutcome, minimizing expenses. Section 4 address problem nding kimplementation set strategy proles. show general problem regard4. Notice perspective spirit work Articial Social Systems AI (see e.g., ShohamTennenholtz (1995)), search form modication system,given modied system, assuming agents tend work individually, desirable outcomeobtained.39fiMonderer & TennenholtzNP-hard, consider modication k-implementation, titled exact implementation,problem becomes tractable.5Games incomplete information introduce challenges. particular, Section 5 consider VCG mechanisms combinatorial auctions 6 . settinginteresting characteristics since interested party cannot general see agents typesneeds decide appropriate payment based observed behaviors. showgeneral 0-implementation (i.e. implementation zero cost) settings incomplete information impossible, ex-post equilibrium frugal VCG mechanism0-implementable.Section 6 study important case mixed strategies. context, unlessassume algorithmic observability, interested party observe actions selected probabilistic process leading selection, therefore earlierresults apply. example, consider simple routing problem above, one maywish consider implementation fair outcome, one obtainedmixed strategy Nash equilibrium game . order address issue,introduce concept implementation devices, show mixed strategy equilibrium 0-implementable implementation device. also show correlatedequilibrium property.2. k-implementationpre-game strategic form pair G = (N, X), N = {1, 2, , n} setplayers, X = X1 X2 Xn , every i, Xi set strategies availableplayer i. Let player, set strategy proles players denoted Xi ,generic element Xi denoted xi .payo function vector n-tuple U = (U1 , U2 , , Un ), Ui : Xpayo function player i. assume payos players representedcommon monetary unit, payo functions bounded7 .pre-game G payo function vector U denes game strategic form denotedG(U ). game G(U ) nite strategy sets nite.Let xi , yi strategies player game G(U ).xi dominates yi Ui (xi , xi ) Ui (yi , xi ) every xi Xi , exists xiXi strict inequality holds. yi dominated strategy dominatedstrategy i. xi dominant strategy dominates every strategy i.prole strategies x (Nash) equilibrium every player i, xi best-response5. Complexity implementation organizer controls structure game discussedConitzer Sandholm (2002).6. VCG mechanisms (Vickrey, 1961; Clarke, 1971; Groves, 1973) widely discussedcontext combinatorial auctions, topic received much attention recent multi-agentsystems e-commerce literature, e.g., (Nisan, 2000; Sandholm, Suri, Gilpin, & Levine, 2001; Parkes,1999)7. game nite payo functions automatically bounded.40fiK-Implementationxi . is,Ui (xi , xi ) Ui (yi , xi )every N yi Xi .is, every player believes players act according x, betterplaying according x. Modern economic theory made (some times implicit)assumption economic interactions equilibrium. However, rationaleassumption debate many cases, particulary exist multipleequilibrium proles. contrast, using non-dominated strategy rational behaviorreasonable denition rationality. Moreover, refraining use dominatedstrategies taken basic idea agreed upon technique decision theory.Let G = (N, X) pre-game. every vector payo functions V , let Xi (V )set non-dominated strategies game G(V ), let X(V ) = X1 (V )X2 (V ), , Xn (V ). G(V ) game (N, X, V ), where, innocent abuse notationsV denotes vector payo functions restricted X. vector V payo functionsnon-negative (V 0) Vi (x) 0 every player every x X.Consider set desired strategy proles X game G(U ). non-negativevector payo functions V implements G(U )X(U + V ) O.V called k-implementation G(U ), additionni=1 Vi (x)k every x X(U + V ).Obviously, paying every player sucient amount money playing strategyassociated particular strategy prole O, one implement O.is, interested party commits certain non-negative payos V ,way rational players choose strategy proles O,worst case interested party pay k.Note implicitly made two important assumptions :Output observability: interested party observe actions chosenplayers.Commitment power: interested party reliable sense playersbelieve indeed pay additional payo dened V .However, requirement V 0 means interested party cannot force playersmake payments based actions. addition, interested party cannot modifyset available strategies, enforce behavior way. reliably promisepositive monetary transfers conditioned observed outcome.Let k(O) price implementing O. is, k(O) greatest lower bound(GLB) non-negative numbers q exists q- implementation. is,41fiMonderer & Tennenholtzk(O) = k implies every > 0 (k + )- implementation vector V ,k - implementation k < k. V optimal implementationV implementsnmaxVi (x) = k(O).xX(U +V )i=1V optimal implementation V implementsmaxxX(U +V )nVi (x) k(O) + .i=13. k-Implementation singletonssingleton, = {z}, sometimes abuse notations sayz (instead {z}) k-implementation G(U ), refer k(z) priceimplementing z.3.1 Finite gamessection focus nite games, characterization optimal k implementation singletons.Theorem 1 Let G(U ) nite game least two strategies every player. Everystrategy prole z optimal implementation V , moreover:k(z) =ni=1max (Ui (xi , zi ) Ui (zi , zi )) .xi Xi3.1.1Proof : Let z X let V implements z. Let N . xi = zi ,xi , Vi (xi , xi ) > 0, one modify Vi changing term 0, get cheaperimplementation z. Hence, assume without loss generality dealpayo function vectors V which, every i, Vi (xi , ) = 0 every xi = zi .zi dominant strategy G(U + V ),Vi (zi , xi )+Ui (zi , xi ) Vi (xi , xi )+Ui (xi , xi )every xi Xi , every xi Xi .Since xi = zi , Vi (xi , ) = 0, necessary condition implementationVi (zi , xi ) + Ui (zi , xi ) Ui (xi , xi )every xi Xi .is,Vi (zi , xi ) max (Ui (xi , xi ) Ui (zi , xi )).xi Xi42fiK-ImplementationOne use xi = zi order get costless strict inequality required denitiondomination ( use assumption every player least two strategies).Hence, optimal implementation vector z, V dened every by: Vi (xi , ) = 0xi = zi , Vi (zi , xi ) = maxxi Xi (Ui (xi , zi ) Ui (zi , zi )) + (xi ), : Xinonnegative function satises (zi ) = 0, xi = zi , (xi ) > 0.Therefore (3.1.1) satised.Note z equilibrium every player i, maxxi Xi (Ui (xi , zi )Ui (zi , zi )) = 0. Hence following characterization equilibrium corollary Theorem 1:Corollary 1 Let G(U ) nite game least two strategies every player, letz X. z equilibrium z zero- implementation.3.2 Innite gamesgame G(U ) innite, one get phenomena contradicts intuition.example, possible Xi = {zi } zi dominant strategy. E.g., considertwo-person game player 1 choose strategy z1 , number 0 < x1 < 1,player 2 choose z2 x2 . U1 (z1 , z2 ) = 0.5, U1 (z1 , x2 ) = 10, U1 (x1 , ) = x1 . U2arbitrary function. easily seen every x1 dominated bigger numberopen interval (0,1), z1 dominated, hence X1 = {z1 }. However, z1dominate x1 x1 > 0.5. Moreover, max operator used proofs Theorem 12 may well-dened innite games. game G(U ) = (N, X, U ) called regularevery Xi compact metric space, payo functions continuous X endowedproduct metric.Theorem 2 Theorems 1 holds regular games.Proof : proof requires standard techniques, therefore omitted.immediately get:Corollary 2 Corollary 1 holds regular games.3.3 Mixed strategiesevery nite set B denote (B) setprobability distributions B. is,(B) consists functions q : B [0, 1] bB q(b) = 1. Let G(U ) = (N, X, U )nite game. mixed extension G(U ) innite game Gm (U ) = (N, X , U ),X = (X1 ) (X2 ) (Xn ), every player i, Uim (p1 , p2 , , pn ) =xX p1 (x1 )p2 (x2 ) pn (xn )Ui (x). is, Ui (p) expected payo playerevery player j (including i) choosing strategy ( independently players)randomizing device chooses strategy xj probability pj (xj ).prole mixed strategies p X called mixed-strategy equilibrium G(U )p equilibrium game Gm (U ). Nash (1950) every nite game possesses43fiMonderer & Tennenholtzmixed strategy equilibrium. Note every strategy xi Xi identiedmixed strategy chooses xi probability 1. sense, Xi subsetXim . deal environment mixed strategies considered, referevery strategy xi Xi pure strategy i.Note possibility using mixed strategies destroy previous results.is, xi dominant (dominated) strategy G(U ), continues dominant(dominated) strategy Gm (U ).Gm (U ) regular game apply Theorem 2 Corollary 2 deduce:Theorem 3 Let G(U ) nite game strategic form least two strategies everyplayer. Let p prole mixed strategies G(U ). p mixed strategy equilibriumG(U ) p 0-implementation Gm (U ).Hence, technically, case mixed strategies follows theorems regarding purestrategies innite games. However, reader notice case outputobservability assumption strong implication. Implementing mixed strategy proleGm (U ) actually means algorithm observability G(U ). is, interested partyobserve mixed strategies used players. realistic assumption thinkinterested party systems administrator deploys algorithms submittedusers. designer allowed alter users algorithms, verify exactcontent algorithms. Hence, example, setting, users algorithmips coin order decide course action, exact randomized algorithms,including particular coin ipping, viewed interested party. interestingcase interested party cannot observe mixed strategies discussedSection 6.4. k-implementation setsprevious sections dealt properties k- implementation. particularemphasized interesting cases k-implementations singletons. However,computational perspective, given game G(U ), set desired strategy prole O,may interest nd smallest integer k 0 k- implementation exists.show:Theorem 4 Given game G(U ), set desired strategy proles O, integer k 0,deciding whether exists k implementation G(U ) NP-hard.Proof : order prove theorem, use reduction SAT problem.Given set primitive propositions {x1 , x2 , . . . , xn }, consider CNF formula. CNFformula conjunction clauses C1 C2 . . . Cm , Ci = l1i li2 . . . lisi (si 2)ljq = xi ljq = xi (for every 1 j 1 q sj ). SATproblem following decision problem: given CNF formula, truth assignmentprimitive propositions satises it? problem known NP-complete.44fiK-Implementationshow polynomial reduction SAT problem deciding whether2- implementation exists, games 2-person games. suce proveresult. Without loss generality restrict attention CNF formulasxi xi appear formula, clause refers xi xi (1 n).agents strategy ci associated clause Ci , every 1 m.addition, agents strategies yi , zi , associated literals xi xi ,respectively (1 n).payo agent 1, p1 , dened follows. strategy prole form(ci , yj ) payo 3 xj appears clause 0 otherwise. strategy proleform (ci , zj ) payo 3 xj appears clause 0 otherwise.strategy prole form (ci , cj ) payo 50 = j 0 otherwise.strategy prole form (yi , yj ) payo 2 = j 3 otherwise.strategy prole form (zi , zj ) payo 2 = j 3 otherwise.strategy prole form (yi , zj ) form (zi , yj ) payo 1 = j 3otherwise. strategy prole form (yi , cj ) payo 51 xi xiappear clause Cj 0 otherwise. strategy prole form (zi , cj ) payo51 xi xi appears clause Cj 0 otherwise.payo agent 2, p2 , follows. payo strategy proleform (yi , yi ) (zi , zi ) 101; payo strategy prole form (yi , zi )(zi , yi ) 100; payo strategy prole form (yi , zj ) (zi , yj )= j 0. payo strategy prole form (ci , cj ) 50 = j,0 otherwise. strategy prole form (yi , cj ), (zi , cj ) payo 50= j 0 otherwise. payo strategy prole form (ci , yj ), (ci , zj )0.set desired strategy proles include strategy proles excludingfollowing: strategy proles form (ci , s),(yi , zi ),(zi , yi ) (where strategy)prohibited.formula satisable 2- implementation: add 1 payo obtainedagent 1 strategy prole form (yi , s) xi true satisfying assignment,add 1 payo obtained agent 1 strategy prole form (zi , s) xifalse. agent 2, increase payo 1 (zi , yi ) xi true, increase payo1 (yi , zi ) xi false.Notice given construction strategies form ci agent 1become dominated removed. addition, xi assigned true (resp.false) strategy zi (resp. yi ) become dominated. corresponding strategiesagent 2 (i.e. zi xi true yi xi false) become dominated too, yielddesired behavior.Similarly, notice since must remain least one yi zi agent 1,must case least one yi zi removed agent 2, cannotobtained payment 1 (increasing payo 100 101). Hence, 2implementation payment agent 2 1. However, notice must add 1payo agent 1 least one elements form (yi , yi ) (zi , zi )45fiMonderer & Tennenholtzcorrespond xi xi appear clause j (i.e. least one literalsclause j, need add 1 strategy prole form (yi , yi ) (zi , zi ) associatedliteral). Notice based fact strategy ci agent cannotremoved adding payment 1 outcome strategy form yj , zj , cjagent 1, xj xj appear Ci , since payo agent 1 (yj , ci )(zj , ci ) 0, payo agent 1 (ci , ci ) 50. Moreover, add 1payo agent 1 obtains (yi , yi ) (zi , zi ) yi zidominated agent 1, result possibility playing strategy proledesired(given impossible remove yi zi agent 2). Hence,implementation corresponds sound truth assignment CNF formula, xiassigned true payo agent 1 (yi , yi ) augmented 1.Notice previous result applies already case constantnumber agents. previous result suggests one may wish consider relaxationsoptimal implementation problem tractable.8 One interesting relaxation9following one:non-negative vector payo functions V called k- exact implementationG(U ), following two conditions satised:X(U + V ) = O.ni=1 Vi (x) k every x O.Hence, V implements means set non-dominated strategies G(U + V )subset O, V exact implementation set equals O. dealingsingletons concepts implementation exact implementation coincide.Notice concept exact implementation makes sense = O1O2 X = X1 X2 Xn since otherwise impossible (exactly)implement O. also assume Oi strictly contained Xi every agent i,Oi contain two strategies one dominates other. show:Theorem 5 Computing optimal k exact implementation exists polynomial.algorithm leading result illustrated case two agents.construct game matrix G , payo function agent denoted pi ;pi describes payment agent dierent strategy proles (if/when selected).matrix G matrix perturbations (non-negative monetary promises),G denote perturbed matrix generated. Let = K + 1 K maximalelement original game matrix.optimal perturbation [OP] algorithm:8. Another approach may search good approximation techniques.9. See discussion last section.46fiK-Implementation1. Let (e1 , . . . , ek ) list possible dierences agents payos originalgame (i.e. possible results one obtains subtracting two possible payosagent given game) , sorted small large.2. Let p1 (a, b) := every O1 b X2 \ O2 , let p1 (a, b) = 0 wheneverX1 \ O1 b O2 .3. Let p2 (a, b) := every b O2 X1 \ O1 , let p2 (a, b) := 0 wheneverb X2 \ O2 O1 ,4. Let i:=15. Let e := ei6. Let p1 (a, b) := e every strategy prole form (a, b) O1 b O27. Let G := G + G8. non-dominated strategies agent 1 G coincide O1 leti:=i+1 return 59. Let i:=110. Let e := ei11. Let p2 (a, b) := e every O1 b O212. Let G = G + G13. non-dominated strategies agent 2 G coincide O2 leti:=i+1 return 105. Incomplete informationprevious sections dealt games complete information. However, manyreal life situations players ( interested party) incomplete informationcertain parameters game. economic literature phenomenonmainly modelled Bayesian setting. setting every player receivesprivate signal, correlated unknown parameters, joint distributionsignals commonly known players (and interested party). followingsubsection deal Bayesian games without probabilistic information. gamescalled games informational form.5.1 Games informational formprecise denition games informational form given paper,focus particular type games combinatorial auctions. However,typical example shown Figure 1.47fiMonderer & TennenholtzFigure 1: game informational formL1s1U1D1t1U2D2s2a1111R1L2a1112b1111a1121a1211a1212b1212b1122b1121a1222b1222b2112a2211b2211a2212b2212b2122a2221b2221a2222b2222a1122a2111a1221a2112b2111a2121a2122b2121R2b1211b1112b1121t2game, Player 1 receive one signals s1 t1 , Player 2receive one signals10 , s2 t2 . true game played determinedpair (c1 , d2 ), c, {s, t}. However, neither player knows exact game. Givens1 (s2 ) player 1 (player 2) choose action {U1 , D1 } ({L1 , R1 }), given t1 (t2 )player 1 (player 2) choose action {U2 , D2 } ({L2 , R2 }). payos showngure. Bayesian game obtained game informational form addingprobability distribution pairs signals described Figure 2. probability1 receives signal c1 , 2 receives d2 equals pij , = 1 c = s, = 2 c = t,j = 1 = s, j = 2 = t.s2a1111s1U1D1L1Figure 2: Bayesian Gamet2R1b1111a1121a1211b1112a1212b1211a1122b1121R2L2a1112a1221b1122b1212a1222b1121b1222p11t1U2D2a2111a2112b2111a2121p12a2211b2112a2122b2121a2212b2211a2221b2122a2222b2221p2110. signals times called types.48b2212b2222p22fiK-Implementationstrategy player function dened set signals, assigns everysignal action11 games consistent signal. example, gameFigure 1, strategy player 1 function b1 : {s1 , t1 } {U1 , D1 , U2 , D2 },property b1 (s1 ) {U1 , D1 } b1 (t1 ) {U2 , D2 }. strategy player 2 analogouslydened function b2 : {s2 , t2 } {L1 , R1 , L2 , R2 }. concepts dominationequilibrium ( traditionally referred ex post equilibrium) naturally dened.example, Figure 3 strategy player 1 chooses U1 receivessignal s1 , chooses D2 given t1 dominates four strategies player1.Figure 3: DominationR15, 2t2L2 R25, 0 1, 1D1 0, 54, 44, 40, 5U2 0, 54, 44, 40, 5D2 1, 15, 05, 01, 1L1U1 1, 1s1t1s2is, given s1 , independently players signal action, choosing U1least good choosing D1 , least one signal action Player 2, choosingU1 strictly better choosing D1 .Figure 4 demonstrate ex post equilibrium.11. environment complex strategies exist, refer choices player gamestrategic form actions.49fiMonderer & TennenholtzFigure 4: Ex Post EquilibriumL2, 8R5, 1L0, 5R3, 61, 56, 47, 21, 4U 0, 25, 25, 02, 41, 16, 04, 23, 3UNote even output observability assumption playerreveal strategy. signal player private knowledge, revealsaction chooses. interested party observe signals; therefore,action sets games ( case Figure 4) interested partyreceive information true game played. Hence, thinguse vector V games. Therefore:Claim every k 0 impossible k-implement ex post equilibrium describedFigure 4.Proof interested party wishes make U dominant strategy game (s1 , t2 ),wishes make dominant strategy (t1 , t2 ). Assume V1 (U, L) = x V1 (D, L) =y, following two contradictory inequalities satised: 0 + x 7 + y,5 + x 4 + y.stated next subsection, game informational form particularstructure, results complete information case generalized.5.2 VCG combinatorial auctionsCombinatorial auctions constitute special class games informational form.notations denitions taken Holzman Monderer (2002).combinatorial auction seller, denoted 0, wishes sell setgoods = {a1 , . . . , }, 1, owns. denote 2A family bundlesgoods (i.e., subsets A). set n buyers N = {1, . . . , n}, n 1. allocationgoods ordered partition = (0 , 1 , . . . , n ) A.12 denote setallocations.12. Note goods allocated among buyers seller. assume, however, sellerderives utility keeping goods, set strategic reserve prices.50fiK-Implementationbuyers valuation function function v : 2A , satisfying v() = 0B C, B, C 2A v(B) v(C).buyer valuation function vi receives set goods B, pays monetarytransfer ci utility vi (B) ci . Every buyer knows valuation function.denote V set possible valuation functions. set V N , n-foldproduct set V , set proles valuations v = (v1 , . . . , vn ), onebuyer.allocation = (0 , 1 , . . . , n ) prole valuations v = (v1 , . . . , vn )VN denote S(v, ) total social surplus buyers, is,S(v, ) =vi (i ).also denoteSmax (v) = max S(v, ),refer allocation achieves maximum optimal allocation v.Vickrey-Clarke (VC) auction mechanism described follows. Every buyer = (quired report valuation function vi . Based reported valuations vv1 , . . . , vn )Nv) = (d0 (v), . . . , dn (v)) , optimalV mechanism selects allocation d(. ties possible, allocation may unique, thereforevv, d(v)) = Smax (v)one VC mechanism. Every function : V N satisfying S(NV determines uniquely VC mechanism, refer VC mechafor vnism d. mechanism assigns buyer bundle di (v) makes pay cdi (v)seller,cdi (v) = maxvj (j )vj (dj (v)).j=ij=irepresents loss agents total surplus caused agent presence.Vickrey-Clarke-Groves (VCG) auction mechanism parameterized VC mechanism d, n-tuple h = (h1 , . . . , hn ) functions hi : V N \{i} . mechanismselects allocation according allocation function d, transfer functionbuyercd,hv) = cdi (v) + hi (vi ).(Hence, VC auction mechanism special type VCG auction mechanism, hifunction identically equal zero every i.Let = (d, h) VCG mechanism. utility valuation vi depends), denoted ui (vi , vi , v).= (vi , vvector reported valuations vis,) = vi (di (ui (vi , vi , vv)) cd,hv).(behavior buyer mechanism described strategy bi : V V .51fiMonderer & Tennenholtzstrategy bi dominant strategy following two conditions hold:13V N \{i}every vi V , every v) ui (vi , vi , v)ui (vi , bi (vi ), vevery vi V .V N \{i}every vi V , exists v) > ui (vi , vi , v).ui (vi , bi (vi ), vstrategy prole (b1 , . . . , bn ) forms ex post equilibrium every prole valuations v = (v1 , . . . , vn ) VN , every buyer i,ui (vi , bi (vi ), bi (vi )) ui (vi , vi , bi (vi ))every vi V ,bi (vi ) = (bj (vj ))j=i . prole (b1 , . . . , bn ) symmetric bi = bj every twobuyers i, j N .well-known every VCG auction mechanism truth-telling senseevery buyer i, strategy bi (vi ) = vi revealing true valuation dominantstrategy.14Special type strategies considered Holzman et al. (2003), Holzman Monderer (2002). bundling strategy buyer parameterized subfamily 2A, denoted f . maps every v V v V denedv (B) =maxCB,Civ(C)every B 2A .eect pretending agent cares bundles (forannounces true valuation), derives valuation bundles maximizingbundles contain.valuation v satises equalities said based ( or, simply-based). set -based valuation function denoted V .subfamily 2A quasi eld satises following twoconditions:B \ B ,B, C B C = B C .work Holzman Monderer (2002) proven every ex post equilibrium VCG mechanisms bundling equilibrium following sense: every13. classical mechanism design second condition required. use sakeconsistency rest paper.14. one reasons fact concept ex post equilibrium VCG auctionmechanisms private values largely ignored economics literature. However, truthtelling strategy induces high communication complexity; requires player communicate 2mnumbers. Hence, computer science perspective, ex post equilibrium less communicationcomplexity desirable. tradeo communication complexity economic eciencydiscussed Holzman, Kr-Dahav, Monderer, Tennenholtz (2003).52fiK-Implementationn 3, every prole (b1 , b2 , , bn ) strategies, satises subprole (bi )iN ,N N , ex post equilibrium VCG mechanisms symmetric prole bundlingstrategies. is, exists 2A bi = f every N , moreover,proved Holzman et al. (2003) must quasi eld.5.3 0-Implementation ex post equilibrium frugal VCG auctionmechanismsbegin formal denition frugal VCG combinatorial auction:Denition Frugal VCG mechanisms VCG mechanism (d, h) called frugal= (allocate unnecessary goods buyers. is, every vv1 , . . . , vn ) V Nmechanism selects allocation d(v) = (d0 (v), . . . , dn (v)) , optimal vsatises addition:every player i, every Bi ,vi (Bi ) < vi (i ),= di (v).Intuitively, frugal VCG mechanism center never allocates unnecessary goods.agent bid bundle B1 bid superset it, B2 , centernever allocate B2 agent (the bid B2 strictly higher bidB1 order allocating B2 agent possibility.)Consider interested party wishes 0-implement ex post equilibrium b =(b1 , b2 , , bn ) VCG auction mechanism (d, h). Given result HolzmanMonderer (2002) stated end previous subsection assume almost withoutloss generality bi = f every N . interested party wishes promisepositive payment every buyer whenever follows recommendation play accordingbi , least one players, say j, play according bj . However,interested party know valuation functions. Hence, could know whetherplayer follows recommendation? Indeed cannot. However, since knowninterested party partially monitor players strategies, because, independentlyplayers valuation function reported valuation function must -based. Hence,best interested party oer every player positive payment reportedvaluation -based, least one players reported valuation function,-based. payments made arbitrarily high reporting-based valuation function yield higher payo other, non -basedvaluation function least one players report -based valuationfunction. However, player cheat within set -based valuations withoutcaught! turns VCG mechanism frugal, every player bettercheating.Lemma 1 Let (d, h) frugal VCG mechanism, let quasi eld, let N .every prole reported valuations players, v) ui (vi , wi , v)ui (vi , vi , v53every wi V .fiMonderer & TennenholtzProof : Without loss generality assume hj constantly 0 every j.Hence, VCG mechanism actually VC mechanism. vj -based every j, j = i,inequality follows fact f induces ex post equilibrium. However,) allocation chosen auctioneerproof use fact. Let = d(vi , vreports vi , let = d(wi , vi ). Denotevj (j ).= maxj=iVCG mechanism frugal, . Therefore) =vi (i ) = vi (i ), vi (i ) = vi (i ). denition ui (vi , vi , v, ) t. since vi (i ) = vi (i ) ui (vi , vi , v) = S(vi , v, ) t.S(vi , v, ) S(vi , v, ) t. However, sinceoptimality get S(vi , vfrugality also implies vi (i ) = vi (i ), get following equation desiredinequality:, ) = S(vi , v, ) = ui (vi , wi , v).S(vi , v.Hence, proof Lemma 1 used fact frugal VCG mechanism mustallocate subset goods every player reports -based valuation function.next example shows Lemma 1 hold arbitrary VCG mechanism.Exampletwo buyers four goods a,b,c,d.= {, ab, cd, abcd}.valuation function 1 v1 , reported valuation 2 v2 . Consider VCauction mechanism allocate ab 1 cd 2 1 reports v1 , allocates abc1, 2, 1 reports -based valuation function w1 . cases 1 pays0. Hence, reporting v1 yields utility 1, cheating yields 1.1. Note VCmechanism frugal 1 declares w1 receives abc, w1 (ab) = w1 (abc).v1v2v1w100000000b0000cab ac ad bc bdcd abc abd acd bcd abcd001 000001.11001.10 0.75 0 0 0.75 0 0.75 0.75 0 0.75 0.75 0.75 0.75001 0000011001.1001 00000.1110.1 0.1 1.1need following terminology. Let (d, h) VCG combinatorial auction, let> 0. denote (d, h, ) direct combinatorial auction rules induced(d, h) which, set feasible valuation functions ( set bids) V (M ),set valuation functions v satisfying v(A) < . assumption upperbound natural common literature mechanism design. veriedLemma 1 holds VCG combinatorial auctions bounded valuation functions.54fiK-ImplementationTheorem 6 Let (d, h, ) frugal VCG auction mechanism least two buyers.Let quasi elds bundles. symmetric ex post equilibrium induced0-implementable.Proof : every player i, interested party promises high payo ( e.g., 2M + 1reports valuation function V , least one playersreport -based valuation function. Player promised payment players reportvaluation functions V . Lemma 1, f dominant strategy every player.6. Implementation devicesmentioned Section 3, proof result (Theorem 3) every mixed strategyequilibrium 0-implementable relies assumption interested party observesmixed strategies used players. section prove result withoutassumption. is, interested party observe actions generatedmixed strategies, algorithms generate them. order dealissue dene new type implementation implementation device.Let G(U ) = (N, X, U ) nite game strategic form. implementation deviceG(U ) tuple = (S, h, V ), = S1 S2 Sn , h (S) probabilitydistribution S, V : X n+ . Si nite15 set signals senti. interested party uses implementation device follows: makes devicepublic, secretly runs randomizing scheme chooses every probabilityh(s). = (s1 , s2 , , sn ) chosen, sends player signal si . strategy prolex selected agent paid Vi (s, x). implementation device generates new gameG(U, I). actually Bayesian game. strategy game functionbi : Si Xi . every si xi , vector bi players let Wi (xi |si , bi )expected payo game G(U, I) chooses xi given receives signalsi players use bi . is,Wi (xi |si , bi ) = Esi (Ui (xi , bi (si )) + Vi (si , si , xi , bi (si )|si ) ,si = (sj )j=i , bi (si ) = (bj (sj ))j=i . strategybi dominant strategyevery signal si positive probability ( tS,ti =si h(t) > 0), everybiWi (bi (si )|si , bi ) Wi (xi |si , bi ) every xi Xi ,exists prole bi players strict inequality holds.Every prole b = (bi )iN determines probability distribution probb X denedfollows:probb (x) = h(b1 = x1 , b2 = x2 , , bn = xn ).15. innite set, must specify additional parameters required probability theory. associateSi -algebra events , endow product -algebra, , dene h.55fiMonderer & TennenholtzLet desired probability distribution X. say implements G(U ),G(U, I) every player dominant strategy bi , probb = . sayk-implementation G(U ) implements , every h(s) > 0,nVi (si , bi (si )) k.i=16.1 Mixed Strategies: Removing algorithm observability assumptionLet p = (p1 , p2 , , pn ) mixed strategy prole nite game G(U ). p generatesprobability distribution p follows:p (x) = p1 (x1 )p2 (x2 ) pn (xn )every x X.say implementation device implements p implements p .order implement mixed strategy equilibrium , interested party employsimplementation device I, set signals Si set actions i, Xi . hproduct probability p. is, h(x) = p1 (x1 )p2 (x2 ) pn (xn ), functionVi : Si Xi + designated way strategy bi (si ) = si , si Xi ,dominant strategy every player i.Hence, interested party ips coin player according probabilitypi prole wishes implement, sends outcome coin ippingi. Thus, signals sent players recommendations play. payofunctions Vi , N designed way obeying recommendationdominant strategy every player.Theorem 7 Let G(U ) nite game least two actions every player. Everymixed strategy equilibrium prole p 0-implementable G(U ) appropriate implementation device = (S, h, V ) = X, h = p product probability Xdened p.Proof : Denote bi strategy obeys every recommendation.bi (si ) = si every si Xi . function Vi satisfy every vector diplayers strategies,Wi (si |si , di ) Wi (xi |si , di ) xi Xi .assume without loss generality every player i, Vi (si , si , xi , yi ) = 0every xi = si . Therefore inequality written follows:Esi Vi (si , si , si , di (si )) Esi Ui (xi , di (si )) Ui (si , di (si )) .di = bi , dj (sj ) = sj every j = every sj Xj , right-hand-sideinequalities non-positive p mixed strategy equilibrium. Hence,may dene Vi (s, s) = 0 every X. make sure inequalities holdcases (i.e., di ), dene Vi (s, si , xi ) = 2M + 1 every xi = si ,> 0 upper bound absolute value players payo functions.choice 2M + 1 (rather 2M ) ensures existence strict inequality requireddenition domination.56fiK-Implementation6.2 0-implementations correlated equilibriumAumann (1974) introduced concept correlated equilibrium. provide onemany equivalent denitions:Denition Let G(U ) = (N, X, U ) nite game strategic form. correlated equilibrium G(U ) probability distribution X ( (X)) strategiesbi (si ) = si , si Xi , N , form equilibrium game G(U, I), = (S, h, V )following implementation device:= X,h = ,Vi (s, x) = Ui (x) every N every s, x X.Hence, forms correlated equilibrium mediator makes changesplayers payo run randomization device according , picks prole pure actionss, sends every player recommendation play si , every player betterobeying recommendation believes players obey recommendations.well-known ( implicitly used proof Theorem 7) p mixedstrategy equilibrium, p correlated equilibrium. Moreover, going proofTheorem 7 reveals property mixed-strategy equilibrium p usefact p correlated equilibrium. Hence get:Theorem 8 Let G(U ) = (N, X, U ) nite game least two actions everyplayer. Every correlated equilibrium prole 0-implementable G(U ) appropriate implementation device = (S, h, V ) which, = X h = .Note eventually, interested party implements mixed strategy equilibriumcorrelated equilibrium implementation device I, players using purestrategies game G(U, I). expected value operator linear,easily seen obeying recommendation remains dominant strategy every playereven player believes players use mixed strategies G(U, I),mixed strategy G(U, I) probability distribution Qi set purestrategies. mixed strategy natural description behavior G(U, I).natural, less computational demanding concept one behavior strategy:behavior strategy game G(U, I) function ci : Si (Xi ). Hence, playerusing behavioral strategy chooses mixed strategy game strategic formfunction signal, player using mixed strategy G(U, I) pickingpure strategy G(U, I) randomization device receives signal. setsmixed behavioral strategies technically related other. However,Kuhn (1953) ( see also Hart (1992) details), every player i, every strategy bievery prole Qi = (Qj )j=i mixed strategies players, existsprole ci = (cj )j=i behavioral strategies players every signal57fiMonderer & Tennenholtzsi expected utility using bi given players using Qi equalsexpected utility players using ci , vice versa.16Hence, theorems 7 8 remain valid environment allows utilizationeither mixed behavioral strategies G(U, I).7. Conclusions, discussion researchOne may distinguish two main lines research multi-agent systems. One lineresearch design mechanisms protocols. contextfact deal design games, agents assumed behaveparticular way (e.g., law-abiding, playing equilibrium, reinforcement learners, etc.)desired behavior (e.g., revenue maximization, maximization social surplus,etc.) obtained. line research deals study behavioragents given game. economic theory leading paradigm last decadesagents use equilibrium strategies. However, paradigm implicitly assumesagents rational moreover, agents believe agents rational.17paper introduced intermediate approach. game/interaction given,agents provided newly designed protocol. inuence agents behaviorcredible promises positive monetary transfers conditioned actionsselected agents game. also assume:1. Minimal rationality: would like assume little possible agent rationality.Indeed assume agent use strategy dominatedanother strategy.2. Minimal expenses: assume interested party wishes minimize expensesleading agents desired behavior.notion k-implementation captures basic ideas. paperprovided several basic results k-implementation. provided full characterization k-implementation case implementation singletons gamescomplete information. particular shown formula computing optimal k,yield desired agent behavior, fully specied procedure implementation. result also applicable general class games innitely many strategies.shown characterization likely impossible implementationsets strategies. particular, shown problem whether desiredbehavior implemented cost k NP-hard. led us considering exactk-implementation shown tractable. Exact k-implementation requiresevery desirable strategy prole rational result promises monetary16. Actually, theorem presented Kuhn (1953) stronger one quote here.17. computer science issue agent choose action , unless dominant strategy,central one general satisfactory solution known. Researchers appeal caseconcept competitive analysis (Borodin & El-Yaniv, 1998). context games, promisingresults direction presented Tennenholtz (2002).58fiK-Implementationtransfers (in addition requirement undesirable strategy prole remainrational). sense, exact implementation requires number strategiessystem remove minimal. consistent basic ideas normative systems order obtain desired behavior would like minimal lawsleave maximal freedom agents long enable obtain desired(social) behavior. discussion issue see (Fitoussi & Tennenholtz, 2000).extension results context mixed strategies interpretedstrong evidence importance Nash equilibrium normative perspective,descriptive approach attempts explain/predict agent behavioreconomic contexts. fact mixed strategy equilibrium made dominantone zero payments given credible interested party, cannot forcebehaviors punish agents, tells us many practical situations Nash equilibriumspecial merit also normative perspective.games incomplete information discussed section games informational form, rather Bayesian games. spirit work computer sciencetries minimize probabilistic assumptions economic environment,particular use part solution concept. context VCG mechanisms probably central widely studied mechanisms turnattention study k-implementation context mechanisms. Indeed,show, unlike case nite games complete information alwayslarge k (if paid) lead desired behavior, longer true gamesincomplete information. VCG mechanisms turn complexequilibrium analysis. recent work shown (Holzman et al., 2003) exponentially many equilibria VCG mechanisms dier truth telling.special interest since equilibria exhibit lower communication complexitystandard truth revealing equilibrium. Notice equilibria obtained withoutrestriction possible bids agents (this called imposition property). interested party access VCG protocol, inuenceindirectly. proved 0-implementation ex-post equilibrium frugal VCGmechanism, frugality requirement necessary.many things left done. particular, interestingdevelop study k-implementation better understanding case k > 0.example, may interesting study eects cost implementation economiceciency. study tractable cases also interest. interested alsoextending study k-implementation games informational form beyond VCGmechanisms. issue collusion may interesting ramications contextk-implementation. Collusive agreements may make benet promises madeinterested party. Similarly, failures system possible, interested partymight nd paying oers hoping ignore given rational behavioragents. also assumed one interested party. caseseveral interested parties may wish lead dierent desired behaviorsnew strategic situation emerges. believe issues signicant importance.hope address future work, others join us study59fiMonderer & Tennenholtzk-implementation exploring spectrum system agent perspectivesmulti-agent systems.Acknowledgementsthank Assaf Cohen helpful comments. rst author thanks Israeli ScienceFoundation partial support research BIKURA grant, secondauthor thanks Israeli Science Foundations partial support research ISFindividual grants. preliminary version paper appears proceedings 4thACM Conference Electronic Commerce (EC03).ReferencesAumann, R. (1974). Subjectivity correlation randomized strategies. JournalMathematical Economics, 1, 6796.Borodin, A., & El-Yaniv, R. (1998). On-Line Computation Competitive Analysis. Cambridge University Press.Clarke, E. (1971). Multipart pricing public goods. Public Choice, 18, 1933.Conitzer, V., & Sandholm, T. (2002). Complexity mechanism design. Proceedings18th conference uncertainity Articial Intelligence (UAI-02), pp. 103110.Dybvig, P., & Spatt, C. (1983). Adoption externalities public goods. Journal PublicEconomics, 20, 231247.Feigenbuam, J., & S, S. (2002). Distributed Algorithmic Mechanism Design: Recent ResultsFutute Directions. Proceedings 6th International Workshop DiscreteAlgorithms Methods Mobile Computing Communications, pp. 113.Fitoussi, D., & Tennenholtz, M. (2000). Choosing Social Laws Multi-Agent Systems:Minimality Simplicity. Articial Intelligence, 119 (12), 61101.Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.Groves, T. (1973). Incentives teams. Econometrica, 41, 617631.Hart, S. (1992). games extensive strategic forms. Aumann, R., & Hart, S. (Eds.),Handbook Game Theory, chap. 2, pp. 1940. North Holland, Amsterdam.Holzman, R., Kr-Dahav, N., Monderer, D., & Tennenholtz, M. (2003). Bundling Equilibrium Combinatorial Auctions. Games Economic Behavior, appear. Workingpaper Technion http://ie.technion.ac.il/dov.phtml.Holzman, R., & Monderer, D. (2002). Characterization ex post equilibrium VCGcombinatorial auctions. Game Economic Bahavior, appear. Working Paper,Technion, http://ie.technion.ac.il/dov.phtml.60fiK-ImplementationKoutsoupias, E., & Papadimitriou, C. (1999). Worst-Case Equilibria. STACS.Kuhn, H. (1953). Extensive games problem information. Annals MathematicsStudies, 48.Mas-Colell, A., Whinston, M., & Green, J. (1995). Microeconomic Theory. Oxford University Press.Maskin, E. (1999). Nash equilibrium welfare optimality. Review Economic Studies,66, 2338.Maskin, E., & Sjostrom, T. (2002). Implementation theory. Arrow, K. J., Sen, A. K.,& Suzumura, K. (Eds.), Handbook Social Choice Theory Welfare Volume 1.North-Holland, Amsterdam.Monderer, D., & Shapley, L. (1996). Potential games. Games Economic Behavior, 14,124143.Naor, M., Pinkas, B., & Sumner, R. (1999). Privacy Preserving Auctions MechanismDesign. Proceedings EC99, pp. 129139.Nash, J. (1950). Equilibrium points n-person games. Proceedings National AcademySciences United States America, 36, 4849.Nisan, N. (2000). Bidding allocation combinatorial auctions. ACM ConferenceElectronic Commerce, pp. 112.Nisan, N., & Ronen, A. (1999). Algorithmic mechanism design. Proceedings STOC-99.Papadimitriou, C. H. (2001). Algorithms,Games,and Internet. STOC 2001.Parkes, D. C. (1999). ibundle: ecient ascending price bundle auction. ACMConference Electronic Commerce, pp. 148157.Rosenschein, J. S., & Zlotkin, G. (1994). Rules Encounter. MIT Press.Rosenthal, R. (1973). class games possessing pure-strategy nash equilibria. International Journal Game Theory, 2, 6567.Roughgarden, T. (2001). price anarchy independent network topology.Proceedings 34th Annual ACM Symposium Theory Computing, pp.428437.Roughgarden, T., & Tardos, E. (2002). bad selsh routing?. Journal ACM,49 (2), 236259.Sandholm, T., Suri, S., Gilpin, A., & Levine, D. (2001). Cabob: fast optimal algorithm combinatorial auctions. 17th International Joint Conference ArticialIntelligence, pp. 11021108.Segal, I. (1999). Contracting externalities. Quarterly Journal Economics,CXIV (2), 337388.61fiMonderer & TennenholtzShoham, Y., & Tennenholtz, M. (1995). Social Laws Articial Agent Societies: O-lineDesign. Articial Intelligence, 73.Shoham, Y., & Tennenholtz, M. (2001). rational computability communicationcomplexity. Games Economic Behavior, 35, 197211.Spiegler, R. (2000). Extracting intercation-created surplus. Games Economic Behavior,30, 142162.Tennenholtz, M. (1999). Electronic commerce: game-theoretic economic modelsworking protocols. IJCAI-99.Tennenholtz, M. (2002). Competive Safety Analyis: robust decision-making multi-agentsystems. Journal Articial Intelligence Research, 17, 363378.Vickrey, W. (1961). Counterspeculations, auctions, competitive sealed tenders. JournalFinance, 16, 1527.62fiJournal Artificial Intelligence Research 21 (2004) 135191Submitted 03/03; published 02/04CP-nets: Tool Representing ReasoningConditional Ceteris Paribus Preference StatementsCraig Boutiliercebly@cs.toronto.eduDepartment Computer ScienceUniversity TorontoToronto, ON, M5S 3H8, CanadaRonen I. Brafmanbrafman@cs.bgu.ac.ilDepartment Computer ScienceBen-Gurion UniversityBeer Sheva, Israel 84105Carmel Domshlakdcarmel@cs.cornell.eduDepartment Computer ScienceCornell UniversityIthaca, NY 14853, USAHolger H. Hooshoos@cs.ubc.caDepartment Computer ScienceUniversity British ColumbiaVancouver, BC, V6T 1Z4, CanadaDavid Poolepoole@cs.ubc.caDepartment Computer ScienceUniversity British ColumbiaVancouver, BC, V6T 1Z4, CanadaAbstractInformation user preferences plays key role automated decision making.many domains desirable assess preferences qualitative rather quantitative way. paper, propose qualitative graphical representation preferencesreflects conditional dependence independence preference statementsceteris paribus (all else equal) interpretation. representation often compactarguably quite natural many circumstances. provide formal semanticsmodel, describe structure network exploited several inferencetasks, determining whether one outcome dominates (is preferred to) another, ordering set outcomes according preference relation, constructing best outcomesubject available evidence.1. IntroductionExtracting preference information users generally arduous process, humandecision analysts developed sophisticated techniques help elicit information(Howard & Matheson, 1984). key goal study computer-based decision supportconstruction tools allow preference elicitation process automated, eitherpartially fully. Methods extracting, representing reasoning preferencesnaive users particularly important AI applications, ranging collaborativec2004AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiBoutilier, Brafman, Domshlak, Hoos & Poolefiltering (Lashkari, Metral, & Maes, 1994) recommender systems (Nguyen & Haddawy,1998) product configuration (DAmbrosio & Birmingham, 1995) medical decisionmaking (Chajewska, Getoor, Norman, & Shahar, 1998). many applicationsusers cannot expected patience (or sometimes ability) provide detailedpreference relations utility functions. Typical users may able provide muchqualitative rankings fairly circumscribed outcomes.paper describe novel graphical representation, CP-nets, usedspecifying preference relations relatively compact, intuitive, structured mannerusing conditional ceteris paribus (all else equal) preference statements. CP-netsused specify different types preference relations, preference orderingpotential decision outcomes likelihood ordering possible states world,example, Shohams (1987) preference semantics. However, mainly firsttypepreferences outcomes decisionsthat motivates development CPnets. inference techniques CP-nets described paper focus two important,related questions: perform preferential comparison outcomes,find optimal outcome given partial assignment problem attributes.Ideally, preference representation capture statements natural usersassess, reasonably compact, support effective inference. conditional ceterisparibus semantics requires user specify, specific attribute interest,attributes impact preferences values A. instantiationrelevant attributesthe parents Athe user must specify preference orderingvalues conditional parents assuming instantiated values; instance,a1 may preferred a2 b1 c2 hold. preference given ceteris paribusinterpretation: a1 preferred a2 given b1 c2 else equal. words,fixed instantiation remaining attributes, outcome a1 holds preferredone a2 holds (assuming b1 c2 ). statements arguably quite naturalappear several places (e.g., e-commerce applications). instance, productselection service offered Active Buyers Guide1 asks (unconditional) ceteris paribusstatements assessing users preference various products. tools also askcertain semi-quantitative information preferences. Conditional expressions offereven greater flexibility.Preference elicitation complex task key focus work decision analysis(Keeney & Raiffa, 1976; Howard & Matheson, 1984; French, 1986), especially elicitationinvolving non-expert users. Automating process preference extractiondifficult. considerable work exploiting structure preferencesutility functions way allows appropriately decomposed (Keeney &Raiffa, 1976; Bacchus & Grove, 1995, 1996; La Mura & Shoham, 1999). instance,certain attributes preferentially independent others (Keeney & Raiffa, 1976), oneassign degrees preference attribute values without worrying attributevalues. Furthermore, one assumes stringent conditions, often one constructadditive value function attribute contributes overall preferencespecificdegree (the weight attribute) (Keeney & Raiffa, 1976). instance,common engineering design problems make assumptions simply1. See www.activebuyersguide.com.136fiCP-Netsrequire users assess weights (DAmbrosio & Birmingham, 1995). allowsdirect tradeoffs values different attributes assessed concisely. Case-basedapproaches also recently considered (Ha & Haddawy, 1998).Models make preference elicitation process easier imposing specificrequirements form utility preference function. consider CP-net representation offer appropriate tradeoff allowing flexible preference expressionimposing particular preference structure. Specifically, unlike much work citedabove, CP-nets capture conditional preference statements.remainder paper organized follows. Section 2 provides backgroundpreference orderings, important notions preferential independence conditional ceteris paribus preference statements. define CP-nets, discussingsemantics expressive power depth, models properties. Section 3present algorithm outcome optimization CP-nets provide exampleapplication CP-nets illustrates optimization process. Section 4 introduces twokinds queries preferential comparison, namely, ordering dominance queries,investigates computational properties. Section 5 discusses several general techniquesanswering dominance queries exploit structure CP-net. Section 6discuss applicability complexity results algorithms slight generalizationCP-nets allow incompletely specified local preferences and/or statements preferential indifference. Finally, Section 7 examine related work applicationsCP-nets, discuss number interesting directions future theoretical researchapplications.2. Model DefinitionPhilosophical treatment many intuitive qualitative preferential statements began 1957pioneering work Hallden (1957), continued Castaneda (1958), von Wright(1963, 1972), Kron Milovanovic (1975), Trapp (1985), Hansson (1996). reasonintensive analysis statements expressed concisely openingHanssons (1996) paper:discussing wife table buy living room, said:round table better square one. meanirrespectively properties, round table bettersquare-shaped table. Rather, meant round table better (forliving room) square table differ significantlycharacteristics, height, sort wood, finishing, price, etc.preference ceteris paribus everything else equal.preferences express act upon seem type. [Emphasisadded.]important property ceteris paribus preferential statements intuitive natureusers types. Independently work philosophers area, reasoningceteris paribus statements drawn attention AI researchers. example, Doyleet al. (1991) introduced logic relative desire treat preference statementsceteris paribus assumption. logic bears similarity von Wrights (1963) logic137fiBoutilier, Brafman, Domshlak, Hoos & Poolepreferences, supports complicated inferences.2 However, bestknowledge, serious attempt made exploit preferential independencecompact efficient representation ceteris paribus statements. paper,take steps toward structured modeling qualitative ceteris paribus preferential statements.start defining notion (qualitative) preference relation numberbasic preference independence concepts, followed introduction CP-netssemantics.2.1 Preference Relationsfocus attention single-stage decision problems complete information, ignoring paper issues arise multi-stage, sequential decision analysisconsiderations risk arise context uncertainty.3 begin outlinerelevant notions decision theory. assume world one numberstates state number actions performed.action, performed state, specific outcome (we concernuncertainty action effects knowledge state). set outcomes denotedO. preference ranking total preorder set outcomes: o1 o2 meansoutcome o1 equally preferred decision maker o2 . use o1 o2denote fact outcome o1 strictly preferred decision maker o2(i.e., o1 o2 o2 6 o1 ), o1 o2 denotes decision maker indifferent o1o2 (i.e., o1 o2 o2 o1 ). use terms preference ordering relationinterchangeably ranking.aim decision making certainty is, given knowledge specific state,choose action preferred outcome. note orderingvary across decision makers. instance, two customers might radically differentpreferences computer system configurations sales program helping construct.Often, state s, certain outcomes cannot result action :outcomes obtained called feasible outcomes (given s). many instances,mapping states actions outcomes quite complex. decisionscenarios, actions outcomes may equated: user allowed directly selectfeasible outcome (e.g., select product desirable combination attributes). Oftenstates play role (i.e., single state).One thing makes decision problems difficult fact outcomes actionspreferences usually represented directly. example, actions may representedset constraints set decision variables. focus preferences.assume set variables (or features attributes) V = {X1 , . . . , Xn }decision maker preferences. variable Xi associated domain Dom(Xi ) ={xi1 , . . . , xini } values take. assignment x values set X V variables(also called instantiation X) function maps variable X elementdomain; X = V, x complete assignment, otherwise x called partial assignment.2. detailed discussion issue, refer reader Doyle Wellman (1994).3. issues include assigning preferences sequences outcome states, assessing uncertainty beliefssystem dynamics, assessing users attitude towards risk.138fiCP-Netsdenote set assignments X V Asst(X). x assignmentsdisjoint sets X Y, respectively (X = ), denote combination xxy. X = V, call xy completion assignment x. denote Comp(x) setcompletions x. Complete assignments correspond directly outcomesuser possesses preferences. outcome o, denote o[X] value x Dom(X)assigned variable X outcome.Given problem defined n variables domains Dom(X1 ), . . . , Dom(Xn ),|Dom(X1 )| |Dom(Xn )| assignments. Thus direct assessment preferencefunction usually infeasible due exponential number outcomes. Fortunately,preference function specified (or partially specified) concisely exhibits sufficientstructure. describe certain standard types structure here, referring KeeneyRaiffa (1976) detailed description (and other) structural forms discussionimplications. set variables X preferentially independent complement= V X iff, x1 , x2 Asst(X) y1 , y2 Asst(Y),x1 y1 x2 y1 iff x1 y2 x2 y2 .words, structure preference relation assignments X,variables held fixed, matter values variables take.relation holds, say x1 preferred x2 ceteris paribus. Thus, oneassess relative preferences assignments X once, knowing preferenceschange attributes vary.define conditional preferential independence analogously. Let X, Y, Znonempty sets partition V. X conditionally preferentially independent givenassignment z Z iff, x1 , x2 Asst(X) y1 , y2 Asst(Y),x1 y1 z x2 y1 z iff x1 y2 z x2 y2 z.words, X preferentially independent Z assigned z. X conditionally preferentially independent z Asst(Z), X conditionally preferentiallyindependent given set variables Z.Note ceteris paribus component definitions ensures statementsone makes relatively weak. particular, imply stance specific valuetradeoffs. Consider two variables B preferentially independent,preferences values B assessed separately; instance, supposea1 a2 b1 b2 . Clearly, a1 b1 preferred outcome a2 b2 least;feasibility constraints make a1 b1 impossible, must satisfied one a1 b2a2 b1 . cannot tell preferred using separate assessments. However,stronger conditions (e.g., additive preferential independence) one constructadditive value function weights assigned different attributes (or attributegroups). decomposition preference function allows one identifypreferred outcomes rather readily, this, well special forms preferencestructure, especially appropriate attributes take numerical values.extensive discussion various special forms preference functions refer KeeneyRaiffa (1976), well Bacchus Grove (1995, 1996) Shoham (1997).139fiBoutilier, Brafman, Domshlak, Hoos & Poole2.2 CP-Networksrepresentation preferences graphical nature, exploits conditional preferential independence structuring preferences user. model similar Bayesiannetwork (Pearl, 1988) surface; however, nature relation nodeswithin network generally quite weak (e.g., compared probabilistic relationsBayes nets). Others defined graphical representations preference relations;instance Bacchus Grove (1995, 1996) shown strong results pertainingundirected graphical representations additive independence. representation semantics rather distinct, main aim using graph capture statementsqualitative conditional preferential independence. note reasoning ceterisparibus statements explored AI (Doyle et al., 1991; Wellman & Doyle, 1991;Doyle & Wellman, 1994), though context network representations exploitingpreferential independence computational fashion.variable Xi , ask user identify set parent variables Pa(Xi )affect preference various values Xi . is, given particular value assignmentPa(Xi ), user able determine preference order values Xi ,things equal. Formally, given Pa(Xi ) Xi conditionallypreferentially independent V (Pa(Xi ) {Xi }). Given information, ask userexplicitly specify preferences values Xi instantiations variableset Pa(Xi ). use information create annotated directed graphnodes stand problem variables, every node Xi Pa(Xi ) immediateancestors. node Xi annotated conditional preference table (CPT) describingusers preferences values variable Xi given every combination parentvalues. words, letting Pa(Xi ) = U, assignment u Asst(U), assumetotal preorder iu provided domain Xi : two values x x, eitherx ju x0 , x0 ju x, x ju x0 . simplicity presentation, ignore indifferencealgorithms. Though treatment indifference straightforward semantically, consistencyarbitrary networks indifference cannot assumed, discuss Section 2.5.Likewise, assume CPTs variables fully specified, though discusspartially specified CPTs Section 6.call structures conditional preference networks CP-networks (CP-nets,short).Definition 1 CP-net variables V = {X1 , . . . , Xn } directed graph GX1 , . . . , Xn whose nodes annotated conditional preference tables CPT(Xi )Xi V. conditional preference table CPT(Xi ) associates total order iuinstantiation u Xi parents P a(Xi ) = U.illustrate CP-net semantics consequences several smallexamples. ease presentation, variables examples boolean, thoughsemantics defined features arbitrary finite domains.Example 1 (My Dinner I) Consider simple CP-net Figure 1(a) expressespreference dinner configurations. network consist two variables W ,standing soup wine, respectively. Now, strictly prefer fish soup (Sf ) vegetable soup (Sv ), preference red (Wr ) white (Ww ) wine conditioned140fiCP-Nets89:;?>=<?>=<89:;WSv WwSf SvWrvSfSvWw WrWr WwWrfzWwf(a)(b)Figure 1: (a) CP-net Dinner I: Soup Wine; (b) induced preference graph.soup served: prefer red wine served vegetable soup, white wineserved fish soup.Figure 1(b) shows preference graph outcomes induced CP-net. arcgraph directed outcome oi oj indicates preference oj oidetermined directly one CPTs CP-net. example, factSv Wr preferred Sv Ww (as indicated direct arc them) directconsequence semantics CPT(W ). top element (Sv Ww ) worst outcomebottom element (Sf Ww ) best.Example 2 (My Dinner II) Figure 2(a) extends chain CP-net Example 1 addingmain course another variable. example, preference optionsmain course clear: strictly prefer meat course Mmc fish course Mf c .addition, prefer two fish courses one dinner; thus preferencevegetable fish soup conditioned main course: prefer open fish soupmain course meat, vegetable soup main course fish. Example 1,Figure 2(b) shows corresponding induced preference graph outcomes.Example 3 (Evening Dress) Figure 3(a) illustrates another CP-net expresses preferences evening dress. consists three variables J, P , S, standing jacket,pants, shirt, respectively. unconditionally prefer black white colorjacket pants, preference red white shirts conditionedcombination jacket pants: color, white shirtmake outfit colorless, thus prefer red shirt. Otherwise, jacketpants different colors, red shirt probably make outfit flashy, thusprefer white shirt. Figure 3(b) shows corresponding preference graph.141fiBoutilier, Brafman, Domshlak, Hoos & Poole89:;?>=<Mmc Mf c?>=<89:;MmcMf c89:;?>=<WSfSv(a)Sf SvSv SfWw WrWr WwMf c SfWrMf c SfWwMfmc Sv Wwmmmmmmff vmmmWvrf cSv WwMmcllllllllllllvllMmc Sv WrqMmc Sf QWQrQQQQQQQQQQ~Q(mc Sf Ww(b)Figure 2: (a) CP-net Dinner II; (b) induced preference graph.142fiCP-NetsJw Pw SwJb JwJw Pw SrPb Pw89:;?>=<?>=<89:;J3P33ffff33ff33 ffffffff89:;?>=<Jb PbJw PbJb PwJw PwSr SwSw SrSw SrSr SwJw Pb SriJb Pw Sriiiiiiiiiiiiiiii"Jb Pw LSwJw Pb SwLLLLLLLLL&Jb Pb Sw(a)|Jb Pb Sr(b)Figure 3: (a) CP-Net Evening Dress: Jacket, Pants Shirt; (b) induced preference graph.2.3 Semanticssemantics CP-net straightforward. defined terms set preferencerankings consistent set preference constraints imposed CPTs.Definition 2 Let N CP-net variables V, Xi V variable, U Vparents Xi N . Let = V (U {Xi }). Let iu ordering Dom(Xi )dictated CPT(Xi ) instantiation u Asst(U) Xi parents. Finally letpreference ranking Asst(V).preference ranking satisfies iu iff havefor Asst(Y) x, x0Dom(Xi ) yxu yx0 u whenever x iu x0 . preference ranking satisfies CPTCPT(Xi ) iff satisfies iu u Asst(U). preference ranking satisfiesCP-net N iff satisfies CPT(Xi ) variable Xi .CP-net N satisfiable iff preference ranking satisfies it.Thus network N satisfied iff satisfies conditional preferences expressedCPTs N ceteris paribus interpretation.Theorem 1 Every acyclic CP-net satisfiable.143fiBoutilier, Brafman, Domshlak, Hoos & PooleProof: prove constructively building satisfying preference ordering. proofinduction number variables. theorem trivially holds one variable,total ordering specified directly CP-net.Suppose theorem holds CP-nets fewer n variables. Let Nnetwork n variables. N acyclic, least one variable parents;let X variable. Let x1 x2 . . . xk ordering Dom(X) dictatedCP (X). xi , construct CP-net, Ni , n 1 variables V {X}removing X initial CP-net, variable child X, revisingCPT restricting row X = xi . inductive hypothesis, constructpreference ordering reduced CP-nets Ni .construct preference ordering original network N follows.rank every outcome X = xi preferred outcome X = xj xi xjCPT(X). outcomes identical values xi X, rank accordingordering associated Ni (ignoring value X). easy seepreference ordering satisfies N .example, consider CP-net Example 1 (Figure 1). Somewhat surprisingly,information captured network sufficient totally order outcomes:f Ww f Wr v Wr v Wwsince ranking satisfies CP-net. However, need casegeneral, satisfiable CP-net satisfied one ranking. instance,consider CP-net Figure 4.4 two rankings satisfy network:abc abc abc abc abc abc abc abcabc abc abc abc abc abc abc abcPreferential entailment CP-net defined standard way.Definition 3 Let N CP-net variables V, o, o0 Asst(V) two outcomes. N entails o0 (i.e., outcome preferred o0 ), written N |= o0 , iffo0 holds every preference ordering satisfies N .Lemma 2 Preferential entailment transitive. is, N |= o0 N |= o0 o00N |= o00 .Proof: N |= o0 N |= o0 o00 o0 o0 o00 preference rankingssatisfying N . rankings transitive, must o00 satisfyingrankings.example, consider CP-net N Figure 4(a) following three outcomes:= abc, o0 = abc, o00 = abc. outcomes o0 assign valuesvariables except B. addition, given value Pa(B) = {A} o0 , valueB (B = b) preferred value B (B = b) o0 , else equal. Therefore,N |= o0 . case o0 o00 , argument respect4. network Example 2 (My Dinner II) variables renamed.144fiCP-Netsabc89:;?>=<abcabcttzttt?>=<89:;Bb bb babcbb?>=<89:;Cc cc cabcttzttabcabc KKKKKK%(a)abc(b)Figure 4: simple chain-structured CP-network.variable C show N |= o0 o00 well. Observe o00 cannot deriveddirectly CPTs N . However, Lemma 2, follows relationinferred taking transitive closure direct relations o0 o0 o00 .Notice that, given CP-net, assess outcome terms conditionalpreferences violates. example, CP-net Example 1: outcome Sf Wwviolates none preference constraints; Sf Wr violates conditional preferenceW ; Sv Wr violates preference S; Sv Ww violates both. Somewhat surprisingly,ceteris paribus semantics implicitly ensures violating preference worseviolating W , since Sf Wr Sv Wr . is, parent preferenceshigher priority child preferences. property important implicationsinference see below.2.4 Cyclic Networksmentioned, nothing semantics CP-net model forces acyclic. However,according Theorem 1, acyclicity network automatically confers importantproperty model: network satisfiable (i.e., exists preference orderingsatisfies ceteris paribus preference assertions imposed CPTs).cyclic CP-nets, situation much complicated. example, considerbinary-valued cyclic CP-net structure Figure 5(e). CPTs networkspecified Figure 5(a), induced preference graph (see Figure 5(b))extended complete preference ordering consistently. However, CPTs specifiedFigure 5(c), network unsatisfiable (the induced preference graph, shown145fiBoutilier, Brafman, Domshlak, Hoos & PooleFigure 5(d) cannot completed consistently). example shows consistencycyclic CP-nets guaranteed, depends actual nature CPTs.a1 : b1 b2a2 : b2 b1b1 : a1 a2b2 : a2 a1a2 b1Fa1 b1(a)a1 : b1 b2a1 b2FF xxFxFxxx FFFx|x"a2 b2(b)a1O b1/ a1 b2a2 b1a2 b2a2 : b2 b1b1 : a2 a1b2 : a1 a2(c)?>=<89:;AU?>=<89:;B(d)(e)Figure 5: Examples satisfiable unsatisfiable cyclic CP-net binary variables.Recently, initial results consistency testing cyclic CP-nets presentedDomshlak Brafman (2002a). particular, wide class cyclic, binary-valued CPnets identified efficiently testable consistency. However, results coverpart spectrum, research cyclic CP-nets needed.Beyond open computational questions cyclic CP-nets raise, usefulnessrequires analysis. One argue possible cluster variablespreserve acyclicity. Although approach technically feasible probably usefulmany domains, cannot provide general solution. First, clustering affectspace requirements problem description thus, generally degrade efficiencyreasoning preferences. Second, certain domains, may naturalexpress cyclic preferences even acyclic representation could used. example,seems case work preference-based presentation web page content(Domshlak, Brafman, & Shimony, 2001), argued preferred presentationcertain component web page may depend presentation neighborspage, whose preferred presentation depend presentation, on.One could argue preferences naturally exhibit cyclic structure acyclic netstheoretical interest only. experience indicates opposite. Acyclic CP-netsshown effective natural above-mentioned work web page presentation(Domshlak et al., 2001), well related project deals presentationmulti-media content medical domain (Gudes, Domshlak, & Orlov, 2002)aextensive example latter domain presented next section. Moreover,domains, found difficult generate intuitively reasonable cyclic networks.due fact cycle implies variables equally important.146fiCP-NetsTypically, case. Thus, apparent utility acyclic networks,fact use composite variables made clustering primitive variables,additional complexity involved cyclic networks, consider acyclic CP-netsremainder paper. However, investigation cyclic CP-nets, wellcharacterization different classes utility functions represented cyclicacyclic networks, remains interest.2.5 Indifferencefar assumed preference constraints CPT CP-net totallyorder outcomes variable question. Specifically, variable Xi parentsU, u Asst(U), assume iu total order Dom(Xi ). generaldefinition CP-net allow arbitrary total preorder iu Dom(Xi ), thus allowinguser express indifference two values variable X, say x x0 , given u.denote x x0 CPT(X).turns ceteris paribus semantics quite strong say twovariable values equally preferred.Example 4 Consider following two CPTs network variables B,parent B:a;: b b; : b b;assert user indifferent a, hold, prefers b,hold, prefers b. derive following preferences outcomes:ab ab ab ab abstatements consistent preference ranking, hence networksatisfiable. One way interpret someone really preferences:: b b; : b b;cannot indifferent a, ceteris paribus.points potential difficulty use indifference CP-nets. One mustcareful express indifference two values variable (A case), yetexpress (strict) conditional preference child variable (B) dependsvalues user indifferent. Intuitively, case, seems userexpressing fact would like value B match (with respectsign), intends preference ab ab (or vice versa). case,making parent B expresses preference B subsidiary A,intent. case, either cyclic network (indeed satisfiable networkdiscussed Section 2.4) clustering variables B seems appropriate.Despite this, indifference used safely follows. Let Xi variablenetwork N parents U, let Xj child Xi . Let denote remaining147fiBoutilier, Brafman, Domshlak, Hoos & Pooleparents Xj (those excluding Xi ). Suppose u Asst(U), x, x0Dom(Xi ), x x0 iu . long local orderings CPT(Xj ) fixedinstantiation identical whether x x0 holds, network N satisfiable.precisely, jxy =jx0 Asst(Y), network N satisfiable. Thus,indifferent x x0 , preferences values Xi children,exhibit indifference whether context includes x x0 .5simplicity presentation, remainder paper continue assumepreference constraints CPT CP-net totally order outcomes variablequestion. However, Section 6, discuss applicability results satisfiableCP-nets capture statements preferential indifference.3. Outcome OptimizationOne principal properties CP-nets that, given CP-net N , easily determinebest outcome among preference rankings satisfy N . call queryoutcome optimization query. turns simple task CP-nets.3.1 Algorithm Outcome OptimizationIntuitively, generate optimal outcome simply need sweep networktop bottom (i.e., ancestors descendents) setting variablepreferred value given instantiation parents. Indeed, networkgenerally determine unique ranking, determine unique best outcome (assumingindifference). generally, suppose given evidence constraining possible outcomesform instantiation z subset Z V network variables. Determiningbest completion z (that is, best outcome consistent z) achievedsimilar fashion, outline.Outcome optimization queries answered using following forward sweep procedure, taking time linear number variables. Assume partial instantiationz Asst(Z), goal determining (unique) Comp(z) N |= o0o0 Comp(z) {o}. effected straightforward sweepnetwork. Let X1 , . . . , Xn topological ordering network variables. setZ = z, instantiate Xi 6 Z turn maximal value given instantiationparents. procedure exploits considerable power ceteris paribus semantics graphical modeling preferential statements easily find optimaloutcome given certain observed evidence (or imposed conjunctive constraints).Lemma 3 forward sweep procedure constructs preferred outcome Comp(z).Proof: Let vz outcome set completions z. Define sequence outcomesvi , 0 n, follows: (a) v0 = vz ; (b) Xi 6 Z, vi constructed settingvalue Xi preferred value given instantiation parents vi1 ,variables retaining values vi1 ; (c) Xi Z, vi = vi1 .construction, vi vi1 . last outcome vn precisely constructed forward5. restriction relaxed somewhat take account fact Xj parents couldlie set U, case rankings need agree every indifference pair x x0 .148fiCP-Netssweep algorithm. Notice arrive outcome irrespective startingpoint vz (by assumption, ties). Since vn vz outcome vz consistentevidence, forward sweep procedure yields optimal outcome.3.2 Example Applicationturn illustration use CP-nets context CP-net based systemadaptive multimedia document presentation. Applications based systempresentation web-based content multi-media medical data recently presentedDomshlak et al. (2001) Gudes et al. (2002). example demonstratesimplicity preference specification using CP-nets, utility acyclic networks,use optimization algorithm described above.system consists two toolsthe authoring tool, viewing tool. centralpart authoring tool module specification CP-net correspondingcreated and/or edited multimedia document. Using CP-net, content providerexpress preferences regarding presentation document content. example,content provider may prefer material presentedmaterial presented. result preference-based multimedia document designmeta-document specifying present present it.description content providers preferences, captured acyclic CP-net,becomes static part document, sets parameters initial presentation.Given document, viewing tool responsible reasoning preferences;specifically, must determine optimal reconfiguration document contextinteraction viewer document. process, users k recentcontent choices viewed constraints form items must appear specified.Subject constraints, optimal document presentation respect contentproviders CP-net must generated. Thus, particular session, actualpresentation changes dynamically based users choices. precisely, whenevernew user input obtained, optimization algorithm constructs best presentationdocument components respect content providers preferences amongpresentations conform users recent viewing choices. process usesforward sweep procedure described above.Example 5 (Multimedia Document) Consider medical record consists six components: two components correspond set medical tests conducted 2001an X-rayimage textual notes physicianand four components correspond set medicaltests 2002a CT (computerized tomography) image, X-ray image, graph illustrating results electromyography, textual notes physician. purposesillustration, assume preferences content provider (e.g., latter physician)presentation options components captured using CP-net shownFigure 7. specific details preferencesthe nature preferential dependenciesprecise details CPTs summarized follows:CT-image [CT image, 2002] consist four CT images different parts body,shown Figure 6(a). six presentation options CT-image:either completely presented (ctplain ), completely hidden (cthide ), presented149fiBoutilier, Brafman, Domshlak, Hoos & Poole(a)(b)(c)(d)Figure 6: Document components Multimedia Document example.CTimageGGGGGGGGGGGG#Xraywwwwwwwwwww{wGraphXrayoldNotesNotesoldFigure 7: Multimedia Document CP-net.zoom-in one four parts (ctlt , ctrt , ctlb , ctrb , standing left-top, righttop, left-bottom, right-bottom parts, respectively). physicians preferencepresentation options CT-image unconditional:cthide ctlt ctrt ctlb ctrb ctplainX-ray [X-ray, 2002] either hidden (xrayhide ), presented (xrayplain ),presented segmentation (xraysegm ); image segmentation depictedFigures 6(b) 6(c), respectively. preference presentation optionsX-ray depends presentation CT-image:ctplain(ctplain cthide )cthidexrayhide xrayplain xraysegmxrayplain xraysegm xrayhidexraysegm xrayplain xrayhide150fiCP-NetsGraph [Electromyography, 2002] shown Figure 6(d), either presented(graphplain ), hidden (graphhide ). preference presentation optionsGraph depends presentation CT-image X-ray:(ctlt ctrt ctlb ctrb ) xraysegmotherwisegraphplain graphhidegraphhide graphplainNotes [Textual notes, 2002] either fully presented (notesplain ), summarized(notessumm ), omitted together (noteshide ). preference presentationoptions Notes depends presentation CT-image Graph:cthide(cthide ) graphplainotherwisenoteshide notessumm notesplainnotessumm notesplain noteshidenoteshide notessumm graphplainX-ray-old [X-ray, 2001] either hidden (xrayoldhide ), presented (xrayoldplain );image depicted below. preference presentation options X-ray-olddepends presentation X-ray:xrayhide(xrayhide )xrayoldhide xrayoldplainxrayoldplain xrayoldhideNotes-old [Textual notes, 2001] either presented (notesoldplain ), omittedtogether (notesoldhide ). preference presentation options Notes-olddepends presentation X-ray-old:xrayoldhidexrayoldplainnotesoldplain notesoldhidenotesoldhide notesoldplainbeginning viewing session, initial presentation document, depictedFigure 8(a), determined using forward sweep procedure Z = : component set preferred presentation given presentation immediate parentsCP-net. example, CT-image hidden, since preferred optioncomponent. Subsequently, X-ray image presented segmented, since CT-imagepresented, and, turn, electromyography Graph presented decision presentation options CT-image X-ray. Suppose viewer chooses151fiBoutilier, Brafman, Domshlak, Hoos & Poolecthide MMMMMMMMM&xraysegmqqqqqqqqqxgraphplain xrayoldplain"notesplain notesold hide(a)NNNNNNNNN&xrayplainqqqqqqqqxqgraphplain xrayoldplainctrt#notessumm notesold hide(b)OOOOOOOOOO'xrayhideppppppxpppgraphplain xrayoldhidectrt#notessumm notesold plain(c)ctplainMMMMMMMMM&xrayhideqqqqqqqxqqgraphhide xrayoldhide#noteshide notesold plain(d)Figure 8: Document presentations various user choices.152fiCP-Netslook right-top part CT-image.6 terms forward sweep procedure,choice sets Z = {CTimage}, z = {ctrt }. result forward sweep procedure appears Figure 8(b); follows, shaded nodes standevidence-constrained variables Z. Now, X-ray image presented without segmentationzoom-in right-top part CT-image, Notes summarized sinceelectromyography Graph presented, CT-image hidden.Suppose viewer consequently chooses hide X-ray image. numberrecent viewer choices taken constrain presentation greater one,choice set Z = {CTimage, Xray}, z = {ctrt , xrayhide }. result forwardsweep procedure appears Figure 8(c). consequently viewer chooses see wholeCT-image, z = {ctplain , xrayhide }, updated presentation shown Figure 8(d).4. Comparing OutcomesOutcome optimization task supported preference representation model. Another basic query respect model preferential comparison outcomes. Two outcomes o0 stand one three possible relationsrespect CP-net N : either N |= o0 ; N |= o0 o; N 6|= o0N 6|= o0 o.7 third case, specifically, means network N containenough information prove either outcome preferred (i.e., existpreference orderings satisfying N o0 o0 o). twodistinct ways compare two outcomes using CP-net:1. Dominance queries Given CP-net N pair outcomes o0 , ask whetherN |= o0 . relation holds, preferred o0 , say dominates o0respect N .2. Ordering queries Given CP-net N pair outcomes o0 , ask N 6|=o0 o. relation holds, exists preference ordering consistent No0 . words, consistent knowledge expressed Norder o0 (i.e., assert preferred o0 ). case sayconsistently orderable o0 respect N .Ordering queries clearly weaker dominance queries. Indeed, N |= o0 ,N 6|= o0 o. may case N 6|= o0 even though N 6|= o0 .dominance queries typically useful, ordering queries sufficient manyapplications one may satisfied knowing outcome consistentlyordered o0 . example, consider set products human automated sellerwould like present customer non-increasing order customer preference.seems reason use strong dominance relation sort products.applications, dominance queries cannot replaced ordering queries. instance, dominance queries shown integral part constraint-based preferentialoptimization CP-nets (Boutilier, Brafman, Geib, & Poole, 1997).6. document explorer (which part viewing tool) illustrated order makesnapshots smaller.7. Recall that, time being, consider CP-nets indifference CPTs; hence two outcomescannot proven equally preferred.153fiBoutilier, Brafman, Domshlak, Hoos & Poolebegin showing ordering queries respect acyclic CP-netsanswered time linear number variables. addition, show setoutcomes sorted consistent non-increasing order respect acyclicCP-net using ordering queries only. provide complexity analysis dominancequeries. First, introduce study particular form reasoning, namely searchflipping sequences, used answer dominance queries. Using technique,focusing binary-valued CP-nets, show connections structureCP-net graph worst-case complexity dominance queries. discuss dominancequeries detail Section 5, present general search techniquesflipping sequences.4.1 Ordering Queries Easyshow ordering queries respect acyclic (not necessarily binary-valued)CP-nets answered time linear number variables. correspondingalgorithm exploits graphical structure model. Likewise, show acyclicCP-nets, construct non-increasing ordering outcomes, consistent CPnet, using ordering queries.Corollary 4 Let N acyclic CP-net, o, o0 pair outcomes. existsvariable X N , that:1. o0 assign values ancestors X N ,2. given assignment provided (and o0 ) Pa(X), assigns preferredvalue X assigned o0N 6|= o0 o.Proof: construction Theorem 1 provides preference ordering satisfying No0 . Thus o0 true models N , consequence N .Corollary 4 presents condition sufficient necessary truthordering query N 6|= o0 o. instance, consider Example 2, let = Mmc Sv Wwo0 = Mf c Sv Wr . two outcomes incomparable according CP-network(i.e., neither proven preferred other), 6 o0 cannot deducedusing conditions Corollary 4, root variable chain CP-net,assigns preferred value assigned o0 .Despite fact condition provided Corollary 4 N 6|= o0necessary consistent orderability, show sufficient provide consistentordering pair outcomes.Theorem 5 Given acyclic CP-net N , two outcomes o0 variablesN , complexity determining truth least one ordering queries, N 6|= o0N 6|= o0 , O(n).Proof: variable Xi , let Pa(Xi ) = U N u u denote assignmentU made outcomes o0 , respectively. variables Xi , o0 assigndifferent values Xi values ancestors Xi N , identified154fiCP-NetsO(n) time top-down traversal N . (Note u = u0 Xi ).Xi o[Xi ] iu o0 [Xi ], using Corollary 4 deduce N 6|= o0 o.Otherwise, exist two variables type, Xi Xj , o[Xi ] iu o0 [Xi ]o[Xj ] iu o0 [Xj ]; case, Corollary 4 implies N 6|= o0 N 6|= o0Corollary 4 provides effective algorithm answering ordering queries; however,computational efficiency comes price: soundif algorithm saysconsistently orderable o0 , indeed, N 6|= o0 o; incompleteif providesnegative response query N 6|= o0 o, still may case N 6|= o0 o. Theorem 5provides effective algorithm sound, partially complete sensereturn positive answer least one N 6|= o0 N 6|= o0 . words,allow us determine least one outcome consistently orderedother.Though incompleteness algorithm single ordering queries problematic,partial completeness algorithm paired queries sufficient allow onefind consistent ordering outcomes polynomial time, least caseacyclic CP-net. first introduce notation. write N `oq o0 representalgorithm paired ordering queries tells us N 6|= o0 holds (i.e.,consistently orderable o0 ) N 6|= o0 (i.e., o0 orderable o).N `oq o0 , assured indeed orderable o0 ; dueincompleteness algorithm, cannot sure o0 orderable o. writeN `oq ' o0 denote algorithm returns positive response orderingqueries (i.e., tells us outcomes consistently orderable other).soundness algorithm ensures outcomes indeed consistently preferredcase. Note partial completeness ensures either N `oq o0 , N `oq o0 o,N `oq ' o0 . sufficient allow us produce consistent orderingset outcomes.Theorem 6 Given acyclic CP-net N variable set V, set outcomeso1 , . . . , om V, ordering outcomes consistently N done using orderingqueries only.Proof: Define two binary relations outcomes: o0 iff N `oq o0 ' o0 iffN `oq ' o0 . first show transitive closure relation asymmetric.Assume contrary exists set outcomes o1 , . . . , ok that:o1 o2 ok o1(1)1 k, let V (oi ) set variables X value assigned X oiimproved given assignment provided oi Pa(X). Let Ni subgraph Nconsisting variables V (oi ) descendants N . Observe Corollary 4implies Ni Ni+1 1 < k, Nk N1 . see this, notice if, i,Ni 6 Ni+1 , exists variable X that: (i) ancestors X assignedpreferred values oi oi+1 ; (ii) given oi [Pa(X)] = oi+1 [Pa(X)],X assigned preferred value oi+1 one less preferred values oi .155fiBoutilier, Brafman, Domshlak, Hoos & PooleHowever, case, ordering query determine N 6|= oi oi+1 , contradictsassumption oi oi+1 .one graph containment relations Ni Ni+1 strict, initial assumption (1)trivially contradicted. Therefore, left case of:N1 = N2 = = Nk = N 0Recalling N 0 acyclic, consider variable Xj N 0 ancestors N 0 . LetU = Pa(Xj ) parents Xj original network N (note U N 0 = ).construction Ni have:o1 [U] = o2 [U] = = ok [U] = umust case since ancestors Xj assigned unique optimalassignment (of u part) since none variables improvable. entailso1 [Xj ] ju o2 [Xj ] ju ju ok [Xj ] ju o1 [Xj ],inconsistent definition CP-net.exploit asymmetric nature relation follows. N |= o0 ,must o0 . Therefore, relation N representing induced preference graphN subset . Thus total ordering o1 , . . . , om consistentconsistent N .immediate consequence Theorems 5 6 that, given set outcomesCP-net N , complexity ordering outcomes consistently preferencegraph induced N O(nm2 ) (i.e., cost comparing every pair outcomesordering accordingly).4.2 Dominance Queries Flipping Sequencesceteris paribus semantics CP-nets allows one directly use informationCPT variable X alter flip value X within outcome directly obtainimproved (preferred) worsened (dispreferred) outcome. sequence improving flipsone outcome another provides proof one outcome preferred, dispreferred,another rankings satisfying network. defining notion precisely,illustrate intuitions example.Example 6 Consider CP-net Figure 4. following tworankings satisfy network:z }| {abc abc abc abc abc abc abc abcabc abc abc |abc {zabc} abc abc abcThus, two outcomes totally ordered abc abc. Notice removeeither abc abc chains outcomes, move one outcomenext chain flipping value exactly one variable according preference156fiCP-Netsinformation CPT given instantiation parents. example, movefirst outcome sequences (abc) second (abc), use fact c cgiven b prove second outcome dispreferred first; flip Cless preferred value given instantiation b parent B. Conversely, movebackwards sequence flipping c second outcome c, thereby obtainingpreferred first outcome.Recall Corollary 4 demonstrates violating preference constraintsparent variable less preferred violating preference constraintschildren. greater importance parent variables implicit ceteris paribussemantics. consider two outcomes abc abc, unordered CPnet Figure 4. outcome abc violates preference values A,outcome abc violates preferences values B C; ancestorB C. semantics CP-nets specify outcomespreferredintuitively, though preference higher priority B C, twoviolations lower priority preferences may preferred violationsingle higher priority preference.two outcomes o0 , every improving flipping sequence o0 uniquelycorresponds directed path node o0 node preference graphinduced CP-net. instance, consider CP-net Figure 9(a), exactlynetwork Evening Dress example (Example 3), simpler variable names.four alternative flipping sequences outcome abc outcome abc,corresponding four paths outcomes induced preference graph,depicted Figure 9(b):abc abc abcabc abc abc abc abcabc abc abcabc abc abc abc abcTherefore, abc abc consequence CP-net. contrast, flippingsequence (directed path) abc abc, hence two outcomes incomparable.examples suggest construction flipping sequence usedprove dominance.Definition 4 Let N CP-net variables V, Xi V, U parents Xi ,= V (U {Xi }). Let uxy Asst(V) outcome, x Dom(Xi ),u Asst(U), Asst(Y). improving flip outcome uxy respect variableXi outcome ux0 x0 iu x. (Note improving flip w.r.t. Xiexist x preferred value Xi given u.)improving flipping sequence respect N sequence outcomes o1 , . . . , okthat, < k, oi+1 improving flip oi respect variable.improving flipping sequence outcome outcome o0 improving sequenceo1 , . . . , ok o1 = ok = o0 .157fiBoutilier, Brafman, Domshlak, Hoos & Pooleb babc89:;?>=<?>=<89:;A3B33ffff33ff33 ffffffff89:;?>=<Cabababababcabcabcjjjjjjjjj jjjjtjjabcabc JJJJJJ%c cc cc cc cabc}abc(b)(a)Figure 9: Evening Dress CP-net Example 2 simpler names.One define worsening flips worsening flipping sequences entirely analogousway. Obviously, worsening flipping sequence reverse improving flippingsequence, vice versa.two important things notice examples above. First, improving (or worsening) flipping sequence used show one outcome betteranother. Second, preference violations worse (i.e., larger negative impactpreference outcome) higher network, although cannot compare always two (or more) lower level violations violation single ancestor constraint.observations underlie inference algorithms below.Theorem 7 (soundness) improving flipping sequence CP-net Noutcome o0 , N |= o0 o.Proof: improving flip outcome o1 another outcome o2 N |=o2 o1 definition |=. theorem follows transitivity preferentialentailment (Lemma 2).Theorem 8 (completeness) N acyclic CP-net improving flippingsequence N outcome o0 , N 6|= o0 o.Proof: Let G graph whose nodes outcomes (i.e., complete assignmentsvariable set V), directed edge o1 o2 iff improving flip o1 o2sanctioned network N . Clearly, directed paths G equivalent improving flippingsequences respect N .Next, show total preference ordering respects paths G (thatis, path o1 o2 G, o2 o1 ) satisfies network N :satisfy N , must exist variable X parents U, instantiation u Asst(U),158fiCP-Netsvalues x, x0 Dom(X), instantiation remaining variables = V (U {X}),that:(a) uxy ux0 y;(b) CPT(X) dictates x0 x given u.direct consequence definition satisfaction. However, N requiresx0 x given u, direct flip xuy x0 uy, contradicting fact extendsgraph G.Based observation, prove theorem: improvingflipping sequence o0 , directed path G o0 . Therefore,exists preference ordering respecting paths G o0 .preference ordering also satisfies N , implies N 6|= o0 o.4.3 Flipping Sequences PlansSearching flipping sequences seen type planning problem: given CPnet N , variable X parents Pa(X) N , row CP (X) conditionalpreference statement formu : x1 x2 xdu Asst(Pa(X)), = |Dom(X)|. statement converted setplanning operators improving value X. particular, conditional preferencestatement converted set d1 planning operators form (for 1 < d):Preconditions: u xiPostconditions:Delete list: xiAdd list: xi1corresponds action improving xi xi1 context u. (An inverseset operators would created worsening sequences).Given query N |= x y, treat start state x goal stateplanning problem. readily apparent query consequence CP-netplan associated planning problem, since plan correspondsflipping sequence.planning problem multi-valued variables discrete, finite domains knownpspace-complete (Backstrom & Nebel, 1995), remains pspace-completeassumption variables binary (Bylander, 1994) (i.e., planning problemsstrips formalism negative effects). However, upper bound informativerespect dominance queries, since planning problems generatedgenerally look quite different form standard AI planning problems,many actions, action directed toward achieving particular propositionrequires specific preconditions. Thus dominance queries respect binaryvalued CP-nets correspond specific class strips planning problems, complexity159fiBoutilier, Brafman, Domshlak, Hoos & Poolerecently analyzed Domshlak Brafman (2002b, 2003). explainrelationship.First, divide preconditions every operator planning problem two types:prevailing conditions, variable values required prior executionoperator affected operator, preconditions, affectedoperator. Second, introduce notion causal graph (Knoblock, 1994),directed graph whose nodes stand problem variables. edge (X, ) appearscausal graph operator changes value prevailingcondition involving X.complexity analysis Brafman Domshlak (2003) addresses planning problemsbinary variables, unary operators (i.e., operators affect single variable),acyclic causal graphs. planning problem generated dominance queryrespect binary-valued CP-net, have:1. operators unary, flip improves value single variable;2. causal graph problem exactly graph CP-net, since valuesPa(X) required value flip X exactly prevailing conditionscorresponding planning operator.Therefore, computational analysis dominance queries binary-valued acyclicCP-nets use results techniques Brafman Domshlak (2003).4.4 Complexity Dominance Queries Binary-valued, Acyclic CP-netssection analyze complexity dominance testing respect binaryvalued CP-nets, showing connection structure CP-net graphworst-case complexity dominance queries. particular, show that:binary-valued CP-net forms directed tree, complexity dominancetesting quadratic number variables.binary-valued CP-net forms polytree (i.e., induced undirected graphacyclic), dominance testing polynomial size CP-net description.binary-valued CP-net directed-path singly connected (i.e.,one directed path pair nodes), dominance testing np-complete.problem remains hard even node in-degree network bounded lowconstant.Dominance testing binary-valued CP-nets remains np-complete numberalternative paths pair nodes CP-net polynomially bounded.exact complexity dominance testing multiply connected, binary-valued, acyclicCP-nets remains open problemat stage clear whether problemnp harder.follows, make assumption number parents variable(i.e., node in-degree CP-net) bounded constant. assumption160fiCP-Netsjustified CPTs part problem description, size CP (X)exponential |Pa(X)|.4.4.1 General Propertiesstart notation two useful lemmas. First, given CP-net N pairoutcomes o, o0 respect N , improving flipping sequence F o0called irreducible subsequence F 0 F obtained deletion entries exceptendpoints o, o0 F improving flipping sequence.8Given CP-net N , let F set irreducible improving flipping sequencesamong outcomes. denote MaxFlip(Xi ) maximal number times variableXi changes value flipping sequence F. Formally, let Flip(F, Xi ) numbervalue flips Xi flipping sequence F . Then,MaxFlip(Xi ) = max {Flip(F, Xi )}F FLemma 9 formalizes first observation irreducible flipping sequencesrespect binary-valued CP-nets.Lemma 9 every variable Xi binary-valued CP-net N , have:MaxFlip(Xi )X1 +MaxFlip(Xj )Xj :Xi Pa(Xj )Proof: Let F irreducible flipping sequence respect N (from outcomeo0 outcome o), MaxFlip(Xi ) = Flip(F, Xi ). Consider subsequenceF 0 = f1 , f2 , . . . , fk F consists value flips children Xi N . Observethat: (i) every fl F 0 requires Xi take one two possible values; (ii) valueflip F F 0 depends value Xi .Now, 1 l < k, fl fl+1 require value Xi , valueflips Xi F fl fl+1 : flips, simply redundant,contradicts assumption F irreducible. (Recall fl fl+1 adjacentF 0 , may separated several flips original sequence F .) Alternatively,fl fl+1 require different values Xi , due irreducibility F , exactlyone value flip Xi F fl fl+1 . Similarly showone value flip Xi F f1 , one value flip Xi Ffk . latter flip necessary fk requires Xi take value o[Xi ], thus,supporting immediate successors, Xi still flip value once, order obtainvalue required o.implies that:MaxFlip(Xi ) = Flip(F, Xi ) 1 +XFlip(F, Xj )Xj :Xi Pa(Xj )8. Note removing proper initial final subsequence F results valid flipping sequence.refer deletion arbitrary elements sequence, excluding endpoints.161fiBoutilier, Brafman, Domshlak, Hoos & Poolethus, definition MaxFlip, have:XMaxFlip(Xi ) 1 +MaxFlip(Xj )Xj :Xi Pa(Xj )Adopting terminology Domshlak Shimony (2003) Shimony Domshlak(2002), directed acyclic graph G directed-path singly connected if, every pair nodess, G, one directed path t. Using Lemma 9, provebinary-valued CP-net forms directed-path singly connected DAG then, everyvariable Xi , MaxFlip(Xi ) bounded n (the number variables).Lemma 10 binary-valued CP-net N forms directed-path singly connected DAG, then,every variable Xi N , have:MaxFlip(Xi ) nn number variables N .Proof: follows, denote MaxFlip(Xi ) N MaxFlipN (Xi ). proof induction n. n = 1 obvious MaxFlip(X1 ) 1. Assume binary-valued,directed-path singly connected CP-net N consists n 1 variables, then, every Xi N ,MaxFlipN (Xi ) n 1Let N 0 binary-valued, directed-path singly connected CP-net n variables.Without loss generality, let variables {X1 , . . . Xn } N 0 topologically orderedbased (acyclic) graph N 0 . Clearly, Xn leaf node, denote NCP-net obtained removing Xn N 0 . Lemma 9, have:MaxFlipN 0 (Xn ) 1Observe directed paths predecessors Xn N 0 , since N 0assumed directed-path singly connected. Therefore, Lemma 9, parent XiXn N 0 , have:MaxFlipN 0 (Xi ) MaxFlipN (Xi ) + MaxFlipN 0 (Xn )thus:MaxFlipN 0 (Xi ) MaxFlipN (Xi ) + 1Generally, since N directed-path singly connected, variable Xi N 0 ,MaxFlipN (Xi ) + 1, path Xi XnMaxFlipN 0 (Xi )MaxFlipN (Xi ),otherwisethus, Xi N 0 , have:MaxFlipN 0 (Xi ) n162fiCP-NetsTreeDT (JN |= o0 K)Initialize variables V outcome o0 .Loop:Iteratively remove leaf variables V assignedvalues o.V = , return yes.Find variable X s.t. value improved, valuedescendants N improved, given current assignment V.variable found, return no.Otherwise, change value X.Figure 10: Flipping sequence search algorithm binary-valued, tree-structured CP-nets4.4.2 Tree-structured CP-netsstart presenting flipping sequence search algorithm class binary-valued,tree-structured CP-nets, prove correctness. show time complexityalgorithm O(n2 ), show actually lower bound flipping-sequencesearch binary-valued, tree-structured CP-nets.Figure 10 presents TreeDT algorithm binary-valued, tree-structured CP-nets.Informally, TreeDT starts initializing variables V values (purported) less preferred outcome o0 , continues incremental, bottom-up conversioninitial assignment assignment induced (purported) preferred outcome o. step starts iteratively removing N leaf variables (i.e., maximalfringe canopy) already consistent o. iteration step removesvariables N , variables assigned values o, thus requiredimproving flipping sequence o0 generated.Otherwise, let N 0 stand updated N (with nodes removed). node X N 0candidate variable if: (i) value X flipped; (ii) descendant X N 0value flipped, given current assignment variables N 0 . flipvalue arbitrary candidate variable (if one exists) repeat node removal;report improving flipping sequence o0 candidatevariables.TreeDT algorithm deterministic backtrack-free. show TreeDTcomplete binary-valued, tree-structured CP-nets, generates irreducible flipping sequencesthus time complexity TreeDT O(n2 ). fact generatesirreducible flipping sequences ensures soundness (since generating valid flippingsequences provide correct positive answers dominance queries).Theorem 11 algorithm TreeDT sound complete binary-valued, tree-structuredCP-nets.Proof: Consider execution TreeDT dominance query N |= o0 respectbinary-valued, tree-structured CP-net N .163fiBoutilier, Brafman, Domshlak, Hoos & PooleFirst, suppose iteratively remove tree leaf variables valuesrequired target outcome o. easy see affectcompleteness algorithm: N acyclic, variables fringeancestors variables. Hence, value variables fringeinfluence ability flip values variables (hence removeimproving flipping sequence consideration).Second, consider variable X that, iteratively removing variables above,value improved, yet none descendents N improved, givencurrent assignment v V. X leaf node, changing value influenceability flip values variables. addition, current value v[X] Xdifferent o[X], otherwise X would part removed fringe. Therefore,(improving) change Xs value point necessary improving flippingsequence. Alternatively, suppose X leaf node. Since leaf nodessubtree N rooted X part removed fringe, (at least) valuesremain changed. N tree, X completely separates descendents Nvariables; improving flip subtree X possiblechange value X. Hence value X must changed flipping sequencepoint value descendent X.remains shown several candidate variablesflipped, matter one flip first. one candidatevariable, one flipped pointeach flip necessaryflipping children corresponding variable. However, changes made onecandidates affect candidates descendants.Thus, evaluation order candidate flips irrelevant, cannot prevent usfinding flipping sequence one exists.Thus algorithm complete. soundness algorithm clearproof well. flip generated step algorithm valid improving flipgiven current outcome v.Theorem 12 time complexity flipping-sequence search binary-valued, treestructured CP-nets O(n2 ), n number variables CP-net.Proof: Since algorithm TreeDT backtrack-free, thing remainsshown addition Lemma 10 Theorem 11 (on binary-valued, tree-structuredCP-nets) TreeDT generates irreducible flipping sequences. However, proofsubclaim straightforward since already shown that:1. TreeDT flips value variable X either achieve value X(purported) preferred outcome query, allow required value flipsdescendants X,2. role latter flips X cannot fulfilled value flips variables.Therefore, given improving flipping sequence F generated TreeDT, removingsubset value flips F makes F either illegal, ends (o0 , o), respectively.Hence, improving flipping sequence generated TreeDT algorithm irreducible.164fiCP-NetsTheorem 13 shows even limiting chain binary-valued CPnets, dominance queries whose minimal flipping sequences quadratic length.Thus TreeDT asymptotically optimal.Theorem 13 (n2 ) lower bound flipping-sequence search binary-valuedtree-structured CP-nets.Proof: proof see Appendix A. proof example, providing dominancequery binary-valued, tree CP-net size minimal flipping sequence(n2 ).4.4.3 Polytree CP-netsDAGs cycles underlying undirected graphs, also known polytrees, offerminimal extension directed trees. Unfortunately, TreeDT procedure completepolytree CP-nets unless extended form backtracking (even restrictedbinary variables). due fact several parents given node mayallowed values flipped, one choices may lead targetoutcome others lead dead end. instance, consider CP-net NFigure 9 query N |= abc abc. Starting outcome abc, first iterationTreeDT, choice flipping either B. B chosen, assignmentchanged abc. However, cannot lead target, since way flip Bback b. Thus, dead end reached. hand, chosen TreeDTsuccessfully generate improving flipping sequence abc abc abc. Thus, incorrectchoice improving variable may require backtracking.However, dominance testing binary-valued, polytree CP-nets remains polynomialtime, although algorithm solution complicated TreeDT. algorithmadapted corresponding algorithm planning problems binary variables,unary operators, polytree causal graphs described Domshlak Brafman (2003).Theorem 14 Dominance testing binary-valued, polytree CP-nets p.Proof: According reduction CP-net dominance queries classicalplanning problems (see Section 4.3), every dominance query respect binary-valued,polytree CP-net compiled propositional planning problem unary operatorspolytree causal graph. algorithm latter problem presented BrafmanDomshlak (2003), time complexity algorithm O(22 n2+3 ),maximal in-degree causal graph.Recall assumption in-degree CP-net bounded constant (incase, ). assumption justified CPTs part problem description,size CP (X) exponential |Pa(X)|. Therefore, complexityalgorithm Brafman Domshlak (2003) CP-nets polynomial size CPnet.165fiBoutilier, Brafman, Domshlak, Hoos & Poole4.4.4 Intractable dominance queriesdominance testing binary-valued, polytree CP-nets polynomial, showbinary-valued, directed-path singly connected CP-nets, problem np-complete.9proved using CP-net-oriented extension proof correspondingclaim respect planning problems (Brafman & Domshlak, 2003). results alsoentail dominance testing binary-valued CP-nets remains np numberdistinct paths pair nodes CP-net polynomially bounded.Theorem 15 Dominance testing binary-valued, directed-path singly connected CP-netsnp-complete.Proof: proof see Appendix A.immediate extension directed-path singly connected DAGs max--connectedDAGs: directed graph max--connected number different directed paths two nodes graph bounded .Theorem 16 Dominance testing binary-valued, max--connected CP-nets,polynomially bounded size CP-net, np-complete.Proof: theorem immediately entailed corresponding result planningBrafman Domshlak (2003); proof terms CP-nets, see Appendix A.Theorem 15 implies dominance testing binary-valued, acyclic CP-nets hard.However, exact complexity problem still open questionit clearwhether problem np harder. preliminary analysis problem (Domshlak & Brafman, 2002a) shows connection complexity flipping sequence search binary-valued, acyclic CP-nets diameters specificgraphs. complementary result respect graphs (Domshlak, 2002b), namely recursively directed hypercubes, shows dominance queries respect binary-valued,acyclic CP-nets unbounded node in-degree may require flipping sequences size exponential n, n number variables CP-net.shown general class planning problems binary variables,unary operators acyclic causal graphs harder np (Brafman & Domshlak, 2003).However, result imply answering dominance queries harder npwell, know reduction class planning problems CP-nets.5. Search Techniques Dominance Queriesprevious section showed dominance testing generally hard even binaryvalued CP-nets, tractable algorithms exist specific problem classes. However, CP-nets impose rich structure preferences exploited various searchstrategies heuristics often significantly reduce search effort, allow effective solution many problem instances. section discuss search flipping9. every polytree directed-path singly connected, converse true.166fiCP-Netssequences several rules allow significant pruning search tree without impacting soundness completeness search procedure. rules describedcontext improving flipping sequences, applied worsening searchmutatis mutandis.Given CP-net N , outcome o, define improving search tree o, (o),follows: (o) rooted o, children every node o0 (o) outcomesreached single improving flip o0 . easy see every rooted path(o) corresponds improving flipping sequence outcome (with respectN ), vice versa.10 example, consider preference graph shown Figure 11(a),induced CP-net Figure 4. Figure 11(b) depicts improving searchtree (abc) respect preference graph. Clearly, treat every dominancequery N |= o0 searching (o0 ) node associated outcome o, startingroot node o0 . instance, example above, given dominance queryN |= abc abc, dotted paths shown Figure 11(c) bring us outcome abc.However, taking different direction tree traversal would lead dead end,requiring backtracking order find suitable flipping sequence. generic searchalgorithm used traverse improving search tree find improving sequencesupports dominance query.5.1 Suffix FixingSuffix fixing rule allows certain moves (o) pruned searchtree without impacting completeness search. Let N CP-net variablesV = {X1 , . . . , Xn }, numbered according arbitrary topological ordering consistentnetwork N . define rth suffix outcome Asst(V), r 1,subset outcome values o[Xr ]o[Xr+1 ] o[Xn ]. Notice rth suffixoutcome depends topological ordering variables used. Finally, say rthsuffixes outcomes o0 match iff o[Xj ] = o0 [Xj ] r j n.Let node improving search tree (o0 ), let target search;words, attempting find flipping sequence proves N |= o0 .suffix fixing rule requires rth suffix matches, value r,neighbors explored whose rth suffix also match o. equivalentruling exploration flipping sequences destroy suffix outcomematches target outcome o. following lemma ensures pruning branchessearch tree (o0 ) searching path affect completeness.Lemma 17 Let N CP-net, outcomes whose rth suffix matches (fortopologically consistent ordering X1 , , Xn variables N ). path(o ) root o, path root every outcomepath assigns values Xr , Xn (i.e., suffix match).Proof: proof straightforward: N acyclic, suffix variable ancestornon-suffix variables. Hence, value suffix variables influence (forbetter worse) ability flip values remaining variables.10. corresponding worsening search tree defined similarly using worsening flips.167fiBoutilier, Brafman, Domshlak, Hoos & Pooleabcabcabcttztttabcabc EEEEEyyE"y|abcabcabcabcabcabcabcDDDD"zzzz| zabcttttzabcDDDD"abcabcabcabcabcabcabcabcabc KKKKKK%abcabcabcabc(a)(b)abcabcyyyy|abcabcabc"abcabcabc"abcabcabcabcabcabc"abcyyyyy|abcabcabcabczzz|zzabcabcabcabc(c)(d)Figure 11: (a) preference graph induced CP-net; (b) improving search tree(abc); (c) paths outcome abc; (d) pruning (abc) suffix fixing.168fiCP-NetsSuppose, given query N |= o0 , path (o0 ) outcome typementioned lemma above. Since subtree (o0 ) rooted (o ),assured path o0 passes , path o0passes suffix preserved subpath o.conclude:Proposition 18 complete search algorithm improving search tree remains complete pruning using suffix rule used.suffix fixing rule effectively prunes search tree node describedcontain paths retain suffix values target o. Though backtrackingchoices lead may required, may choices preservesuffixes, consideration full search tree required (ifnontrivial suffix match). example, improving search tree (abc) queryN |= abc abc discussed above, pruned using suffix fixing rule variable orderingA, B, C, appears Figure 11(d). see, pruning dramatically reducesize effective search tree.5.2 Least-Variable Flippingextension suffix fixing rule least-variable flipping rule, defined follows.Suppose CP-net N , query N |= o0 . Let outcome,variable Xj , let u denote instantiation U = Pa(Xj ) . say Xjleast-improvable variable value x Dom(Xj ) x ju [Xj ],descendent Xj N property. Intuitively, least-improvable variableone lowest N flipped produce outcome preferred. Naturally, may several variables. say Xj least improvablerespect target iff Xj least improvable variable among part suffixmatch o. words, suffix matches, apply definitionleast-improvable variable, restricting attention variables partmatching suffix. least-variable flipping rule requires neighbors nodeexpanded search improving sequence targetleast improvable variable respect improved.following lemma ensures binary-valued, directed-path singly connectedCP-nets, pruning improving search tree using least-variable rule affectcompleteness search procedure.Lemma 19 Let N binary-valued, directed-path singly connected CP-net,outcomes whose rth suffix matches (for topologically consistent ordering X1 , , Xnvariables N ). Let {o1 , . . . , om } (m n) set outcomes reachablevia least-variable flip affect matched suffix. path(o ) root o, path os (s m) o.Proof: Without loss generality based earlier observations, assumesuffix match (if so, restricting attention set non-suffix variablesaffect argument).169fiBoutilier, Brafman, Domshlak, Hoos & Pooleassume, contradicting statement theorem, none least-variableflips os path o, exist path o. impliesnone least-variable flips involve leaf variables network N . Otherwise,could flip value leaf variable without effect ability flipvariables, thus able construct path passesos flip leaves.Now, consider leaf variable Xi N . Since dealing binary variables,irreducible flipping sequence value Xi flipped exactlyonce. However, current assignment [Pa(Xi )] allow us perform flip(see observation above). Thus, must achieve another assignment Pa(Xi )flip value Xi , making part suffix match.Let NXi subnetwork N induced Xi ancestors Xi N .N directed-path singly connected, NXi forms tree, directed toward root Xi .reduce NXi removing subtrees NXj NXi , variable NXjflipped . Note step cause loss generality, since (due acyclicityN ) assignment variables NXj cannot flipped first improvingflipping sequence beginning . Likewise, let Xi1 , . . . , Xim0 , m0 m, variablescorresponding least-variable flips NXi , and, 1 k m0 , let ik(single) directed pathXikXi .Sm0Let = j=1 NXij \ {Xij } . NXi tree, 1 j m0 , variable Xijseparates variables NXij \ {Xij } paths i1 , . . . , im0 . Therefore:(i) set flips variables enable us flip values variablesi1 , . . . , im0 ; particular flip Pa(Xi ) cannot enabled. Thus, enable flipXi , eventually flip value least one variable Xi1 , . . . , Xim0 .(ii) value flip Xi1 , . . . , Xim0 affect (neither positively negatively) abilityflip values variables . Thus, improving flipping sequenceo, least one sequence starts value flip variableXi1 , . . . , Xim0 .last observation contradicts assumption flipping sequencepass least one os .least-variable flipping rule distinguish flips different candidate leastimprovable variables; simply restricts flips variables. general, leastvariable flips suitablesome may lead dead-ends, requiring backtracking (a pointillustrated Section 4.4.3). However, backtrack, need considerleast-variable flips, flips, thus significantly reducing size search treeexpected amount backtracking. observe TreeDT algorithm binary-valued,tree-structured CP-nets essentially implements least-variable flipping rule, is,therefore, complete backtrack-free search procedure binary-valued, tree-structurednetworks.Examples 7 8 show Lemma 19 presents probably widest class CPnets least-variable flipping rule complete: Example 7 shows leastvariable flipping rule preserve completeness binary-valued, max-2-connected170fiCP-NetsCP-nets, Example 8 shows multi-valued, directed-path singly connectedCP-nets. Note CP-net Example 8 forms chain. Therefore, binary-valued, treestructured CP-nets probably widest class CP-nets least-variableflipping rule complete backtrack-free.Example 7 Consider binary-valued CP-net Figure 12, Figure 12(a) depictsgraph CP-net, Figure 12(b) shows corresponding CPTs. Given querywork improving tree rooted abcde.N |= abcde abcde,leastimprovable variable flipped root outcome B (while Eflipped, least improvable). Unfortunately, flipping B value b leadstarget abcde unreachable:outcome abcde,(i) order reach target value variable D, first achieveassignment b c variables B C;(ii) Achieving assignment b c flip question b b (given e)possible restoring value b B, requires e;(iii) Flipping value B b b given e lead us situationlonger flip B, preventing us achieving target value b.Thus least-variable flipping rule allow discovery improving flippingsequence.abcde proveshand, flipping sequence abcdequery:abcdeabcdeabcde abcde abcdeabcdesequence requires flipped root, despite factleast-improvable variable.Example 8 Consider chain CP-net three variables A, B, C,parent B, B parent C. Suppose domain {a, a}, B domain{b1 , b2 , b3 }, C domain {c, c}, following conditional preferences:a;: b 3 b2 b1 ;: b3 b1 b2 ;b1 : c c b2 : c c; b3 : c cConsider query N |= ab3 c ab1 c. value c cannot improved context b1 ,b1 improved b3 context a; fact least-improvablevariable flip outcome ab1 c. Unfortunately, flip leads outcome ab3 cpath target ab3 c exists. contrast, flipping non-least-improving variablefirst allows discovery successful improving path: flipping a, changeb1 b2 , c c, finally b2 b3 .171fiBoutilier, Brafman, Domshlak, Hoos & Poole89:;?>=<?>=<89:;A>E>>>>>>>>>>>89:;?>=<89:;?>=<C/B////////89:;?>=<(a)VariableCPTBe : b be : b bC: c c: c cc b :c b :Ee e(b)Figure 12: multiply connected CP-net least-variable flipping rule causesincompleteness.Though multiply-connected networks, networks multi-valued variables,least-variable flipping rule complete, believe provide useful heuristicguidance cases. least-variable-first heuristic heuristic ordering childrenimproving search treeit requires expanding node (with respecttarget o), children corresponding least-improving variables explored first.typically reduce number nodes expanded tree, leastvariable-first heuristic viewed embodying form least commitment. Flippingvalues least-improving variable seen leaving maximum flexibility flippingvalues variables. upstream variable limits possible flipping sequencesdrastically downstream variable since altering variable limitability flip values non-descendants.multivalued networks, least commitment strategy used extendleast-variable-first heuristic using least improving rule: alternative improving flipsleast-improvable variable considered least improving flipimproving flip (i.e., first flip leads least preferred improving value adopted).allows greater flexibility movement downstream variables. onealways improve value variable question less preferred valuepreferred value (provided parent values maintained), skipping values mayprevent us setting descendants desired values. fact, illustratedExample 8, where, crucial flip variable A, using least improving ruleheuristic leads directly target outcome.172fiCP-Nets5.3 Forward Pruningsearch procedure improving flipping sequence, matter whether procedureadopts heuristics not, one use general forward pruning technique.technique number desirable properties:(a) often quickly shows flipping sequence possible;(b) prunes domains variables reduce flipping search space;(c) doesnt compromise soundness completeness;(d) relatively cheap: time complexity O(nrd2 ), n number variables,r maximum number conditional preference rules variable,size largest variable domain.general idea sweep forward network, pruning values variablecannot appear improving flipping sequence given query. Intuitively,consider set flips possible, ignoring interdependence parents numbertimes parents change values.consider variables order consistent network topology (soparents node considered node). variable Xj parentsU, build domain transition graph nodes corresponding possible valuesxi Dom(Xj ). conditional preference relation ju Dom(Xj ) form:xu1 xu2 xudu contains unpruned values parents U Xj , include directedarcs successive values ordering ju (i.e., arc xi xi1 ,1 < d).answering query N |= o0 , prune value Xjdirected path o0 [Xj ] o[Xj ] domain transition graph Xj .implemented running well-known Dijkstras algorithm (Cormen, Lierson, & Rivest,1990) twice: find nodes reachable o0 [Xj ] find nodesreach o[Xj ]. sets nodes intersected find possible values Xjalong path o0 [Xj ] o[Xj ] (o0 ) (i.e., along improving sequence o0 [Xj ]o[Xj ] respect N ). intersection empty, dominance query fails:legal flipping sequence o0 [Xj ] o[Xj ]. often results quick failurestraightforward queries, carry search non-obvious cases.Example 9 Consider CP-net N B binary root variables,preferences values B b b. Given queryN |= ab . . . ab . . ., first consider A. domain transition graph consistsarc a, thus values pruned. example changed slightlythird value a, a, third value could pruned A, thussimplifying tables children A.consider B, whose domain transition graph consists single arc b b. Sincevalue B (purported) preferred outcome query (b) reachabledomain transition graph B value B (purported) less preferredoutcome query (b), query fails quickly without looking variables.173fiBoutilier, Brafman, Domshlak, Hoos & Poole6. Incompletely Specified Preferences Indifferencemany practical applications, one expects see reluctance behalf user providecomplete CPTs totally order values variable every possible context.Thus, natural ask results techniques applied cases.turns results presented paper, except linear time procedure ordering queries, easily extended work CP-nets partiallyspecified CPTs, satisfiable CP-nets capture statements preferential indifference. instance, results presented Section 4.4 respect dominance queriesremain applicable is, almost results shown BrafmanDomshlak (2003) valid general setting classical planning. caseanalyzed Domshlak Brafman case tree CP-nets. However,correctness TreeDT procedure extended CP-nets easily verified,complexity remains quadratic. stems fact Lemma 10 validgeneral planning setting (Brafman & Domshlak, 2003). point requiresclarification difference partial specification indifference respectflipping sequences: Given variable X parents U, two values x1 , x2 Dom(X),x1 x2 equally preferred given u Asst(U), given u flip valueX x1 x2 , vice versa. Alternatively, x1 x2 incomparable givenu Asst(U), given u cannot flip value X x1 x2 , x2 x1 .complexity dominance testing context indifference incompletelyspecified CPTs remains open problem. However, Theorem 20 showsflipping sequence search multi-valued CP-nets partially specified preferencesnp, even CP-net forms chain variables three-valued only.Theorem 20 Flipping sequence search multi-valued CP-nets partially specifiedpreferences np.Proof: proof see Appendix A.Now, consider outcome optimization queries. CPTs allowed partiallyspecified, statements indifference allowed, CP-net may induceone nondominated outcome. instance, consider Example 1 (My Dinner I).preference type wine given fish soup specified, decision maker considersred white wine go equally well fish soup, CP-net induces twonondominated outcomes: fish soup white wine, fish soup red wine.forward sweep procedure outcome optimization queries presented Section 3easily extended cases partial specification and/or indifference addingbranching variable X (already generated) assignment Pa(X)induces one nondominated value Dom(X). complexity resultingalgorithm O(n), n number variables CP-net, numbernondominated outcomes induced CP-net. course, exponentialsize CP-net.11 However, adapted forward sweep procedure anytimepropertythe solutions generated iteratively, time add new nondominated11. tight upper bound = 2n shown CP-net simply leaving CPTs unspecified.174fiCP-Netsoutcome current set generated solutions O(n). Therefore, complexitygenerating k nondominated solutions linear k.Finally, important query answered efficiently standard CP-netsoutcome ordering query (see Section 4.1). Although basic Corollary 4 remains validcase CP-nets partial specifications statements indifference,case Theorem 5. Therefore, computational complexity ordering queriesextended CP-nets remains open question, conjecture problem hard.7. Concluding Remarkspaper introduced CP-nets, new graphical model representing qualitativepreference orderings reflects conditional dependence independence preferencestatements ceteris paribus semantics. formal framework offers compactarguably natural representation preference information, allows us efficientlyanswer principal forms preference queries.described several types queries algorithms answering respectspecific CP-net. particular, outcome optimization outcome ordering queriesshown solvable time linear number variables network.dominance queries, however, situation complicated. First, demonstratedequivalence answering dominance queries task determining existenceimproving (or worsening) sequence variable value flips respect given CP-net.Then, reduced latter task special subclass classical planning problems.insights allowed us show that, general, answering dominance queries np-hard,polynomial algorithms exist tree polytree-structured, binary-valued CP-nets.addition, presented several techniques one use generic search procedure improving flipping sequence. techniques shownimpact soundness completeness search CP-net, techniquesproperty binary-valued CP-nets. However, argued latter techniques modified general purpose heuristics likely reduce significantlysize expanded search tree.Finally, analyzed applicability results CP-nets allow partiallyspecified preferences and/or capture statements indifference.7.1 Applicationsgoal developing CP-nets formalism facilitate development applications.One applicationpreference-driven, adaptive multimedia document presentationwasrecently developed Ben-Gurion University (Domshlak et al., 2001; Gudes et al., 2002).described central components system Section 3.2. Another applicationconceptual computational properties CP-nets seem useful distributedmeeting scheduler. basic prototype system implemented BenGurion University (Brafman & Domshlak, 2001). currently extending system,working related theoretical issue multi-agent preference-based optimization.Another potential application qualitative preferences general, thus CPnets particular, sorting product database according user-specified preferences.problem highly relevant context electronic commerce. Several rather175fiBoutilier, Brafman, Domshlak, Hoos & Pooleconceptually simplistic, though quite interesting, commercial applications relyunconditional preference statements available World Wide Web; examples include Active Sales AssistantTM (see www.activebuyersguide.com) PersonalogicTM (seewww.personalogic.com). general idea assist user selecting specific productproduct database according preferences. Here, important usecompact natural representations preference information. CP-nets extend currentmodels (which typically dont allow conditional preference statements), yet offer efficiencyordering given set alternatives. Another important aspect problemgiven database precisely defines products (represented vectors attribute values) available, preference information required extent narrowschoice product sufficiently small selection products database.graphical properties CP-nets underlying efficiency ordering queries,various dominance testing strategies, exploited context find subsetproducts dominated product database, given (conditional) preference information extracted user. Here, interactive dynamicapproach appears promising, user prompted additional preference statements ordering products database sufficiently constrainedpreference information offer reasonably small selection products.Another growing application area CP-nets automated constraint-based productconfiguration (Sabin & Weigel, 1998). task assemble number componentscompose product given compatibility constraints satisfied. simpleexample assembly components computer system where, instance,type system bus constrains choice video sound cards.wide growing body research modeling configuration problems efficientproblem solving methods, still need work modeling learninguser preferences, using achieve configurations feasible,also satisfactory user point view. issues emphasized many papersconfiguration (Freuder & OSullivan, 2001; Haag, 1998; Junker, 1997, 2001; Soininen& Niemela, 1998), especially high-level configurators specific, real-life domainsdiscussed (Haag, 1998). importance incorporating user preferencesconfiguration problem stems fact many problems weakly constrainednumerous feasible solutions (DAmbrosio & Birmingham, 1995). valuesolutions, subjective point view particular user, may vary significantly.CP-nets used represent user preferences used togethercompatibility constraints search preferred, feasible configurations. contrastdatabase sorting application above, set possible vectors feature values(i.e., configurations) explicitly given, implicitly specified compatibilityconstraints. CP-net based search algorithm Boutilier et al. (1997) specificallydesigned address problem. description algorithm, well analysiscomputational properties refer reader Boutilier et al. (1997), BrafmanDomshlak (2001).176fiCP-Nets7.2 Related Worknumber lines research related CP-nets. addition conceptual workphilosophy philosophical logic described Section 2, AI, Doyle Wellman (1991,1994) explored ceteris paribus assertions logical properties. However, workexploit notions preferential independence, particular consideredgraphical representations preference statements. best knowledge,computational results known formalism. Therefore, clear whether usefulqueries answered efficiently framework.surface, CP-nets reminiscent Bayesian networks (Pearl, 1988),also graphical structures capturing conditional independence assertions. Indeed, Bayesiannetworks utilization probabilistic independence provide important motivationwork, two structures differ considerably properties typeinformation present.Motivated considerations driving work, Bacchus Grove (1995)La Mura Shoham (1999) study different notions independence associatedgraphical representations. representations allow quantitative assessments, unlikeCP-nets (at least current form) differ CP-nets precise natureindependence concept studied. particular, Bacchus Grove concentrate notionconditional additive independence. Additive independence strong property,requiring utility outcome sum utilities different variablevalues outcome. Conditional additive independence weaker requirement,thus promising practice. Bacchus Grove show conditional additiveindependence properties domain captured undirected graph setnodes A, B, C, independent B given C C separates nodesB. La Mura Shoham (1999) define concept u-independence using ceteris paribuscomparison operator utilities. operator measures intensity preferencespecific values certain variables given fixed value variablesrespect fixed reference point. also define undirected graphical structure,expected utility networks, u-independence represented using notion nodeseparation.Finally, recent work Benferhat, Dubois Prade (2001) provides preliminary investigation potential possibilistic logic qualitative decision analysis, specifically qualitative preference representation. possibilistic approach takes utilitiesaccount, well probabilities, provides qualitative approach decision problemsreplacing numeric preferences probabilities linear orderings. discussionapproach utilities preference representation, see Benferhat et al. (2001), Dubois,Godo, Prade, Zapico (1998b), Dubois, Berre, Prade, Sabbadin (1998a), Dubois,Prade, Sabbadin (1997), related qualitative models, see Boutilier (1994), TanPearl (1994).7.3 Future Worksee number potential extensions work described paper. particular,work leaves number interesting open theoretical questions. First, worst casecomplexity dominance testing respect acyclic, binary-valued CP-nets needs177fiBoutilier, Brafman, Domshlak, Hoos & Pooleestablishedit still open question whether problem np. Second,complexity dominance queries respect multi-valued CP-nets remains openproblem. Third, clear hard ordering queries CP-nets capturepartial preference specification and/or statements preferential indifference. Finally,need understand expressive power CP-nets better, specifically:sort partial orderings representable CP-nets; among orderingsrepresentable CP-nets, ones represented cyclic network.Cyclic networks present another important challenge. networks arise applications natural notion neighborhood defined set variablespreferences one variable value depend value neighboring variables.cases, user may find natural provide cyclic preference structuremust able determine whether specified network satisfiable. addition,inference methods networks need developed or, alternatively, methodsreducing cyclic networks acyclic networks without significant blow-up size. numberrecent papers started deal question, interesting insightsresults (Domshlak, 2002a; Domshlak & Brafman, 2002a; Brafman & Dimopoulos, 2003;Domshlak, Rossi, Venable, & Walsh, 2003).working various extensions framework presented here. extensions include use conditional preference statements contain small amountquantitative information. Existing applications (such online interactive consumer guides)suggest limited amount quantitative preference information might relatively easy extract user natural way, useful inducing strongerpreference orderings. Preliminary work topic undertaken (Boutilier, Bacchus, & Brafman, 2001).Another interesting extension CP-nets TCP-nets (Brafman & Domshlak, 2002).TCP-nets add importance relations conditional relative importance statementsconditional ceteris paribus statements supported CP-nets. Conditional importancestatements form = optimizing B importantoptimizing C. example, flying night, better seatimportant getting preferred carrier.Finally, preference-based optimization presents interesting tradeoff amountuser interaction required extracting preference information amount computation needed determine preferred feature vectors. asking specificquestions particular, potentially complex preferences, finding preferred featurevectors become much easier. hand, asking many questions, especiallyreally necessary establishing relevant preferences, annoy user makesystem less usable. Thus, finding good tradeoffs amount user-interactioncomputation time answering queriessuch finding preferred itemsdatabase optimal configurationsseems promising direction future research.related motivation underlying goal programming (Dyer, 1972; Ignizio, 1982).structure CP-nets exploited determining preference elicitation strategies.explored context CP-nets quantitative information (Boutilieret al., 2001); remains seen use techniques purely qualitative setting.178fiCP-NetsAcknowledgmentsparts paper appeared C. Boutilier, R. Brafman, H. Hoos, D. Poole,Reasoning Conditional Ceteris Paribus Preference Statements, ProceedingsFifteenth Conference Uncertainty Artificial Intelligence, pp.7180, Stockholm, 1999;C. Domshlak R. Brafman, CP-netsReasoning Consistency Testing, Proceedings Eighth International Conference Principles Knowledge RepresentationReasoning, pp.121132, Toulouse, 2002. Thanks Chris Geib contributionsearlier, related models CP-nets, anonymous referees many usefulsuggestions. Boutilier, Hoos Poole supported Natural Sciences Engineering Research Council, Institute Robotics Intelligent Systems. Brafmanacknowledges support Paul Ivanier Center Robotics Research ProductionManagement.Appendix A.Theorem 13 (n2 ) lower bound flipping-sequence search binary-valued,tree-structured CP-nets.Proof: proof example dominance testing query binary-valued, treeCP-net size minimal flipping sequence (n2 ).Consider following CP-net N defined binary variables {X1 , . . . , Xn },n = 2k + 1 k N. domain variable Xi D(Xi ) = {xi , xi }.1 n, Pa(Xi ) = {Xi1 }, thus N forms directed chain. CPTs capturingpreferences values {X1 , . . . , Xn } shown Figure 13(a).consider dominance testing query JN |= o0 K, where, 1 n:o[Xi ] = xixi , = 2m,0[Xi ] =xi , = 2m + 1mNlength minimal (and actually only) flipping sequence o0 k2 +2k+1,proving required lower bound. jth flip sequence changes valuevariable Xk+1+ , where:= bj/(k + 1)c= (j 1) mod (k + 1)Informally, first k + 1 flips change values Xk+1 , . . . , Xn (in order), next k + 1flips change values Xk , . . . , Xn1 , Xk1 , . . . , Xn2 , etc. Finally, last k + 1flips change values X1 , . . . , Xk+1 . k + 1 sets, k + 1 flips each,length flipping sequence thus k2 + 2k + 1.Figure 13(b-c) illustrates o0 , corresponding flipping sequence k = 2(i.e., n = 5). Figure 13(c), variable Xi annotated value flips alongflipping sequence: Arrows values stand value flips, sequentialnumbers flips along flipping sequence appear arrow labels.179fiBoutilier, Brafman, Domshlak, Hoos & Poole89:;?>=<X1x17/ x1?>=<89:;X2x24/ x28/ x2?>=<89:;X3x31/ x35/ x3x42/ x46(a)?>=<89:;X4/ x4= x1 x2 x3 x4 x5o0 = x1 x2 x3 x4 x5?>=<89:;X5x53/ x5VariableCPTi=1xi1<ik+1k+1<inx0ixi1:xi xixi1:xi xixi1:xi xixi1:xi xi(b)9/ x3(c)Figure 13: Illustration proof Theorem 13: (a) CPTs n variables; (b) o0k = 2; (c) flipping sequence k = 2.prove size minimal flipping sequence example (withn = 2k + 1) k2 + 2k + 1. divide proof two steps, show minimalflipping sequence o0 o:1. 1 k + 1, every variable Xk+i (last k + 1 variables) must change valueleast k + 2 times,2. 1 j k, every variable Xj (first k variables) must change value least jtimes.(1) proof first claim induction i. = k + 1, variable X2k+1 (i.e.,Xn ) must change value least since o[X2k+1 ] 6= o0 [X2k+1 ].Now, assume induction hypothesis that, l k + 1, every variable Xk+imust change value least k + 2 times, prove claim Xk+l1 . RecallXk+l1 parent Xk+l , thus every pair successive flips Xk+l must requiredifferent values Xk+l1 .Assume k + l even. Then, first flip Xk+l xk+l xk+l , requiringXk+l1 take value xk+l1 o0 [Xk+l1 ] = xk+l1 . Therefore, first flip Xk+lmay performed first flip Xk+l1 , therefore, flip numberk + 2 l Xk+l , variable Xk+l1 change value least k + 2 l times.Now, since k + l even, k + 2 l. Thus, k + 2 l flips, variable Xk+l1assigned value o0 [Xk+l1 ] = xk+l1 . However, o[Xk+l1 ] = xk+l1 , thus Xk+l1change value least one time.proof case k + l odd similar: first flip Xk+l xk+l xk+l ,requiring Xk+l1 take value xk+l1 o0 [Xk+l1 ] = xk+l1 . Therefore,180fiCP-Netsflip number k+2l Xk+l , variable Xk+l1 change value least k+2ltimes. Now, since k + l odd, k + 2 l. Thus, k + 2 l flips, variableXk+l1 assigned value xk+l1 . However, o[Xk+l1 ] = o0 [Xk+l1 ] = xk+l1 ,thus Xk+l1 change value least one time. Hence, proved that,= l 1, Xk+i change value least k + 2 (= k + 3 l) times.(2) proof second claim induction j. First, show Xk must changevalue least k times. first claim Xk+1 changes value leastk + 1 times. Therefore, since every pair successive flips Xk+1 requires different valuesXk , variable Xk must change value least k times.Now, assume induction hypothesis that, l j k, every variable Xj mustchange value least j times, prove claim Xl1 . Again, since every pairsuccessive flips Xl requires different values Xl1 , variable Xl1 must changevalue least l 1 times.Theorem 15 Dominance testing binary-valued, directed-path singly connected CP-netsnp-complete.Proof: First show membership np. Given dominance query = JN |= o0 K,let MinFS() denote size minimal (= shortest) improving flipping sequence o0o. Using MaxFlip property variables, following upper bound MinFS()straightforward Lemma 10:XMinFS()MaxFlip(Xi ) n2(2)Xi VThus, guess minimal improving flipping sequence given solvable problem,verify low order polynomial time.proof hardness reduction 3-sat, i.e., problem finding satisfyingassignment propositional formula conjunctive normal form conjunct(clause) three literals.Let F = c1 . . . cn 3-sat propositional formula, x1 , . . . , xm variablesused F. equivalent problem constructed follows:V = {V1 , V1 , . . . , Vm , Vm } {C1 , . . . , Cn }, 1 2m + n, domainD(Xi ) = {f, t} (f stand false true, respectively).P a(Vi ) = P a(Vi ) =P a(Ci ) = {Vi1 , Vi1 , Vi2 , Vi2 , Vi3 , Vi3 }, xi1 , xi2 , xi3 variablesparticipate ith clause F.Outcome o0 assigns f variables V.Outcome assigns variables V.variable Vi Vi , 1 m, value (unconditionally) preferredvalue f . turn, 1 n, value preferred value f variable Ciexist j, 1 j 3, that:181fiBoutilier, Brafman, Domshlak, Hoos & Poole1. Vij 6= Vij2. literal xij (and xij ) belongs clause ci Vij = t, otherwise Vij =assignments Pa(Ci ), value f preferred value variableCi .constructed inference problem directed-path singly connected, binary-valuedCP-net 1 n, |Pa(Xi )| 6. Let us show improving flippingsequence o0 exists satisfying assignment F found.Given satisfying assignment , improving flipping sequence follows: First,1 j m, xj = , flip value variable Vj f t. Otherwise,xj = f , flip value variable Vj f t. actual orderingflips irrelevant since variables mutually independent. Then, 1 n, flipvalue Ci f t. also, ordering flips significant. Finally,flip values variables Vj Vj still value f .Let improving flipping sequence o0 o. value variable Ciflipped variables Vj , Vj Pa(Ci ) values t, f , respectively, variablexj set true . Otherwise, Ci flipped variables Vj , Vj Pa(Ci )values f, t, respectively, variable xj set false . Existence jensured construction CP (Ci ).turn, 1 j m, neither Vj Vj achieve value f achievingvalue t. Hence, outcome o1 {Vj = t, Vj = f } o1 ,outcome o2 {Vj = f, Vj = t} o2 , vice versa. showsvalue variable Ci flipped f context consistent. Therefore, close ci , literal lij ci , 1 j 3, lij .Theorem 16 Dominance testing binary-valued, max--connected CP-nets,polynomially bounded size CP-net, np-complete.Proof: prove theorem showing that, acyclic, binary-valued CP-net N ,variable Xi N , have:MaxFlip(Xi ) 1 +nX(Xi , Xj )(3)j=i+1(Xi , Xj ) denotes total number different, necessary disjoint, paths XiXj . simplicity presentation, assume variables {X1 , . . . , Xn } Nnumbered according topological order induced graph N , rewrite Eq. 3into:nXMaxFlip(Xi ) 1 +(Xi , Xj )(4)j=i+1proof induction i. = n obvious MaxFlip(Xn ) 1, since Xnleaf node N . assume lemma holds > k, prove= k. Without loss generality, assume exist least one variable XjXk Pa(Xj ). Otherwise, simply MaxFlip(Xk ) 1.182fiCP-NetsDenote succ(Xk ) immediate successors Xk N , i.e. Xik succ(Xk ) iff XkPa(Xik ). proof straightforward:MaxFlip(Xk )Lemma 9X1 +MaxFlip(Xik )Xik succ(Xk )I.H.X1 + |succ(Xk )| +Xik succ(Xk )=1+nXnX(Xik , Xj ) =j=ik +1(Xk , Xj )j=k+1Now, binary-valued CP-net max--connected, variables given problemconsidered topological ordering induced N , Eq. 4 follows that,variable Xi N , have:MaxFlip(Xi ) n + 1(5)Let MinFS() denote size minimal (= shortest) improving flipping sequence. Eq. 5 follows MinFS() n2 + n. Hence, polynomially boundedsize , guess verify minimal improving flipping sequencepolynomial time, thus class dominance testing queries np.Theorem 20 Flipping sequence search multi-valued CP-nets partially specifiedpreferences harder np.Proof: proof example dominance query multi-valued, chain CP-netpartially specified preferences, minimal flipping sequence exponentiallylong.Consider following CP-net N defined three-valued variables {X1 , . . . , Xn },}.n = 2k + 1 k N. domain variable Xi D(Xi ) = {xi , xi , x1 n, Pa(Xi ) = {Xi1 }, thus N forms directed chain. CPTs capturingpreferences values {X1 , . . . , Xn } shown Figure 14.consider dominance testing query JN |= o0 K, where, 1 n,. Let sequence {ai } defined as:o[Xi ] = xi o0 [Xi ] = x(2,i=1ai =(6)2ai1 + 2, 2length minimal flipping sequence o0 greater than:kXnai > 2 2i=1Figure 15 illustrates corresponding flipping sequence k = 2 (i.e., n = 5),variable Xi annotated value flips along flipping sequence: Arrows183fiBoutilier, Brafman, Domshlak, Hoos & PooleVariablei=1CPTxi xi x1<ik+1xi1 : xi xi xxi xixi1 : xi1 : xi xi xxxi1 :k+1<ini1 :xxi xixi xxi xixixFigure 14:89:;?>=<X11x10/ x118/ x1?>=<89:;X22x3/ x27/ x211/ x215/ x219/ x222/ x2?>=<89:;X33x1/ x34/ x38/ x312/ x316/ x320/ x3?>=<89:;X44x2/ x45/ x49/ x413/ x417/ x421/ x4?>=<89:;X55x6/ x514/ x523/ x3Figure 15:values stand value flips, sequential numbers flips along flippingsequence appear arrow labels.proveP size minimal flipping sequence dominance querygreater ki=1 ai . divide proof four steps, first stepPshows necessity ki=1 ai flips flipping sequence o0 above,subsequent three steps prove existence flipping sequence o0 o. Recall that,since N forms chain, 1 < n, Xi1 parent Xi , thus value flipsXi depend value Xi1 .steps proof follows:184fiCP-Nets(1) flipping sequence o0 o, 2 k + 1, every variable Xk+i (last kvariables) must change value least ak+2i times, f1k+i, . . . , fak+ik+2icorresponding sequence flips Xi , then, 1 j ak+2i , have:xi xi , j = 4k + 1x x , j = 4k + 2fjk+i =kN(7)xx,j=4k+3x x, j = 4k(2) 1 k, every variable Xi (first k variables) change value ai times,f1i , . . . , fai corresponding sequence flips Xi , then, 1 j ai ,every flip fji consistent Eq. 7.(3) Given sequence flips Xk (2), variable Xk+1 change valueak + 1 times, f1k+1 , . . . , fak+1corresponding sequence flips Xk+1 ,k +1then, 1 j ak + 1, have:(k+1 xk+1 , j = 2k + 1xfjk+1 =kN(8)k+1 , j = 2kxk+1 x(4) sequence flips Xk+2 (1) feasible given sequence flips Xk+1(3).Step (1): proof induction i. = k + 1, variable X2k+1 (i.e., Xn ) must2k+1 x2k+1 .change value least twice, since value X2k induces x2k+1 x2k+1 flip X2k+1 first x2k+1way change value X2k+1 xx2k+1 , x2k+1 x2k+1 . Note flips consistent Eq. 7ak+2(k+1) = a1 = 2, thus established induction base.Now, assume induction hypothesis that, 2 < l k + 1, every variableXi must change value least ak+2i times according Eq. 7, prove claimXk+l1 .Consider sequence flips Xk+l assumed necessary, i.e., f1k+l , . . . , fak+l.k+2lAccording CP (Xk+l ) that:(i) every pair successive flips Xk+l require different values Xk+l1 ,k+l1 , xk+l1 },(ii) required values Xk+l1 {xk+l xk+l ) requires Xk+l1 = xk+l1 , o0 [Xk+l1 ] =(iii) first flip Xk+1 (from xk+l1 .xk+l1 xk+l1 back,Therefore, Xk+l1 must perform ak+2l value changes xorder support required value changes Xk+l . addition, supporting valuek+l1changes Xk+l , variable Xk+l1 must perform another value change x0k+l1 o[Xk+l1 ] = xk+l1 .xk+l1 , since ak+2l even (see Eq. 6), [Xk+l1 ] = x185fiBoutilier, Brafman, Domshlak, Hoos & Poolek+l1 xk+l1Finally, according CP (Xk+l1 ), every value change Xk+l1 xk+l1 consists two flips: initial value xk+l1 ,xk+l1 xxk+l1 target value. Thus proved Xk+l1 must perform least2(ak+2l + 1) = ak+3l value changes, value changes consistentEq. 7.Step (2): proof induction i. X1 proof straightforward, since o0 [X1 ] =1 , o[X1 ] = x1 , CP (X1 ) allows us flip value X1 x1 x1 ,xx1 x1 .assume induction hypothesis that, 1 < k, every variable Xichange value ai times according Eq. 7, prove claim Xi+1 . proofapparent CP (Xi+1 ), outcomes o0 query, induction hypothesis.every value achieved Xi along sequence ai flips (including initial value), Xi+1 flip value twice: Given Xi = xXi = xi , flip valueo0 [Xi ] = xi+1 xi+1 , xi+1 xi+1 . Alternatively, given Xi = xi ,Xi+1 first xi+1 . Therefore,flip value Xi+1 xi+1 xi+1 , xi+1 xXi+1 flip value 2(ai + 1) = ai+1 times, easy see value flipsXi+1 consistent Eq. 7.Step (3): Given sequence ak flips f1k , . . . , fakk Xk Eq. 7, let val(fjk ) Dom(Xk )value Xk achieved flip fjk , 1 j ak . CP (Xk+1 ) entails that,k , f k , 1 j 2,triple values Xk achieved triple successive flips fjk , fj+1kj+2either:val(fjk ):k+1xk+1 xkval(fj+1):k+1 xk+1xkval(fj+2):k+1xk+1 xval(fjk ):k+1 xk+1xkval(fj+1):k+1xk+1 xkval(fj+2):k+1 xk+1xaddition, know that:k : xk+1 xk+1o0 [Xk ] = xk+1 xk+1val(f1k ) = xk : xentails Xk+1 change value ak + 1 times according Eq. 8.Step (4): proof subclaim straightforward CP (Xk+2 ). Given sequence Xk+2 value flips f1k+2 , . . . , fak+2Eq. 7, observe that, 1 j ak ,k186fiCP-Netsj = 2k + 1 k N, fjk+2 supported value xk+1 Xk+1 , otherk+1 Xk+1 . Now, sincewise, j = 2k, k N, fjk+2 supported value x1 l ak + 1,(xk+1 , l = 2m + 1val(flk+1 ) =mNk+1 , l = 2mxapparent sequence ak value flips Xk+2 Eq. 7 feasible givensequence ak + 1 value flips12 Xk+1 Eq. 8.0entailssize minimal flipping sequencePk N |= ,n0N i=1 ai , greater 2 2 . fact, show value flipsX1 , . . . , Xk+1 , constructed steps (2)-(3), part minimal flipping sequenceo0 o, thus length sequence greater than:2kXai + (ak + 1)i=1However, result achieved step (1) already proves claim exist dominancequeries multi-valued CP-nets partially specified preferences,exponentially sized minimal flipping sequences.ReferencesBacchus, F., & Grove, A. (1995). Graphical models preference utility. Proceedings Eleventh Conference Uncertainty Artificial Intelligence, pp. 310,Montreal.Bacchus, F., & Grove, A. (1996). Utility independence qualitative decision theory.Proceedings Sixth International Conference Principles Knowledge Representation Reasoning, pp. 542552, Cambridge.Backstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. ComputationalIntelligence, 11 (4), 625655.Benferhat, S., Dubois, D., & Prade, H. (2001). Towards possibilistic logic handlingpreferences. Applied Intelligence, 303317.Boutilier, C. (1994). Toward logic qualitative decision theory. ProceedingsFifth International Conference Principles Knowledge RepresentationReasoning, pp. 7586, Bonn.Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-Networks: directed graphicalrepresentation conditional utilities. Proceedings Seventeenth ConferenceUncertainty Artificial Intelligence, pp. 5664, Seattle.Boutilier, C., Brafman, R., Geib, C., & Poole, D. (1997). constraint-based approachpreference elicitation decision making. AAAI Spring Symposium QualitativeDecision Theory, Stanford.12. fact, ak value flips Xk+1 used order perform ak value flips Xk+2 required.last flip Xk+1 used order achieve value o[Xk+1 ] Xk+1 supporting Xk+2 .187fiBoutilier, Brafman, Domshlak, Hoos & PooleBrafman, R., & Domshlak, C. (2001). CP-networks preference-based CSP. ProceedingsWorkshop Modelling Solving Problems Soft Constraints (in CP-01),pp. 3142, Paphos, Cyprus.Brafman, R., & Domshlak, C. (2002). Introducing variable importance tradeoffs CPnets. Proceedings Eighteenth Annual Conference Uncertainty ArtificialIntelligence, pp. 6976, Edmonton, Canada.Brafman, R., & Domshlak, C. (2003). Structure complexity planning unaryoperators. Journal Artificial Intelligence Research. appear.Brafman, R. I., & Dimopoulos, Y. (2003). new look semantics optimizationmethods CP-networks. Proceedings Eighteenth International JointConference Artificial Intelligence, Acapulco, Mexico. appear.Bylander, T. (1994). computational complexity propositional STRIPS planning.Artificial Intelligence, 69 (1-2), 165204.Castaneda, H. N. (1958). Review Hallden Logic Better. PhilosophyPhenomenological Research, 19, 266.Chajewska, U., Getoor, L., Norman, J., & Shahar, Y. (1998). Utility elicitation classification problem. Proceedings Fourteenth Conference UncertaintyArtificial Intelligence, pp. 7988, Madison, WI.Cormen, T. H., Lierson, C. E., & Rivest, R. L. (1990). Introduction Algorithms. MITPress, Cambridge, MA.DAmbrosio, J. G., & Birmingham, W. P. (1995). Preference-directed design. JournalArtificial Intelligence Engineering Design, Analysis Manufacturing, 9, 219230.Domshlak, C. (2002a). Modeling Reasoning Preferences CP-nets. Ph.D.thesis, Ben-Gurion University. (forthcoming).Domshlak, C. (2002b). recursively directed hypercubes. Electronic JournalCombinatorics, 9 (1).Domshlak, C., & Brafman, R. (2002a). CP-nets - reasoning consistency testing.Proceedings Eighth International Conference Principles Knowledge Representation Reasoning, pp. 121132, Toulouse, France.Domshlak, C., & Brafman, R. (2002b). Structure complexity planning unary operators. Proceedings Sixth International Conference Artificial IntelligencePlanning Scheduling, pp. 6069, Toulouse, France.Domshlak, C., Brafman, R., & Shimony, S. E. (2001). Preference-based configurationweb page content. Proceedings Seventeenth International Joint ConferenceArtificial Intelligence, pp. 14511456, Seattle.Domshlak, C., Rossi, F., Venable, C., & Walsh, T. (2003). Reasoning soft constraintsconditional preferences: Complexity results approximation techniques.Proceedings Eighteenth International Joint Conference Artificial Intelligence, Acapulco, Mexico. appear.188fiCP-NetsDomshlak, C., & Shimony, S. E. (2003). Efficient probabilistic reasoning bayes netsmutual exclusion context specific independence. appear ProceedingsSixteenth International FLAIRS Conference, Special Track Uncertain Reasoning.Doyle, J., Shoham, Y., & Wellman, M. (1991). logic relative desire (preliminary report).Proceedings Sixth International Symposium Methodologies IntelligentSystems (ISMIS91), Lecture Notes Computer Science, pp. 1631. Springer-Verlag.Doyle, J., & Wellman, M. (1994). Representing preferences ceteris paribus comparatives.Proceedings AAAI Spring Symposium Decision-Theoretic Planning, pp.6975, Stanford.Dubois, D., Berre, D. L., Prade, H., & Sabbadin, R. (1998a). Logical representationcomputation optimal decisions qualitative setting. Proceedings Fifteenth National Conference Artificial Intelligence, pp. 588593, Madison, WI.Dubois, D., Godo, L., Prade, H., & Zapico, A. (1998b). Making decision qualitativesetting: decision uncertainty case-based decision. ProceedingsInternational Conference Principles Knowledge Representation Reasoning,pp. 594605. Morgan-Kauffman.Dubois, D., Prade, H., & Sabbadin, R. (1997). possibilistic logic machinery qualitativedecision. AAAI Spring Symposium Qualitative Preferences DeliberationPractical Reasoning, pp. 4754, Stanford.Dyer, J. S. (1972). Interactive goal programming. Management Science, 19, 6270.French, S. (1986). Decision Theory. Halsted Press, New York.Freuder, E., & OSullivan, B. (2001). Modeling generating tradeoffs constraintbased configuration. Proceedings 4th Workshop Configuration (IJCAI-01),pp. 3844, Seattle.Gudes, E., Domshlak, C., & Orlov, N. (2002). Remote conferencing multimedia objects.Proceedings Second International Workshop Multimedia Data DocumentEngineering (MDDE02).Ha, V., & Haddawy, P. (1998). Toward case-based preference elicitation: Similarity measurespreference structures. Proceedings Fourteenth Conference UncertaintyArtificial Intelligence, pp. 193201, Madison, WI.Haag, A. (1998). Sales configuration business processes. IEEE Intelligent SystemsApplications, 13 (4), 7885.Hallden, S. (1957). Logic Better. Lund.Hansson, S. O. (1996). ceteris paribus preference. Journal Philosophical Logic,25 (3), 307332.Howard, R. A., & Matheson, J. E. (Eds.). (1984). Readings Principles Applications Decision Analysis. Strategic Decision Group, Menlo Park, CA.Ignizio, J. P. (1982). Linear Programming Single Multiple Objective Systems.Prentice-Hall, Englewood Cliffs.189fiBoutilier, Brafman, Domshlak, Hoos & PooleJunker, U. (1997). cumulative-model semantics dynamic preferences assumptions.Proceedings Fifteen International Joint Conference Artificial Intelligence,pp. 162167, Nagoya, Japan.Junker, U. (2001). Preference programming configuration. Proceedings 4th Workshop Configuration (IJCAI-01), pp. 5056, Seattle.Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: PreferencesValue Trade-offs. Wiley, New York.Knoblock, C. A. (1994). Automatically generating abstractions planning. ArtificialIntelligence, 68, 243302.Kron, A., & Milovanovic, V. (1975). Preference choice. Theory Decision, 6, 185196.La Mura, P., & Shoham, Y. (1999). Expected utility networks. ProceedingsFifteenth Conference Uncertainty Artificial Intelligence, pp. 366373, Stockholm.Lashkari, Y., Metral, M., & Maes, P. (1994). Collaborative interface agents. ProceedingsTwelfth National Conference Artificial Intelligence, pp. 444449, Seattle.Nguyen, H., & Haddawy, P. (1998). decision-theoretic video advisor. AAAI-98Workshop Recommender Systems, pp. 7780, Madison, WI.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann, San Mateo.Sabin, D., & Weigel, R. (1998). Product conguration frameworks - survey. IEEE IntelligentSystems Applications, 13 (4), 4249.Shimony, S. E., & Domshlak, C. (2002). Complexity probabilistic reasoning (directedpath) singly connected (not polytree!) Bayes networks. submitted publication.Shoham, Y. (1987). semantical approach nonmonotonic logics. Proceedings TenthInternational Joint Conference Artificial Intelligence, pp. 388392.Shoham, Y. (1997). Conditional utility, utility independence, utility networks. Proceedings Thirteenth Annual Conference Uncertainty Artificial Intelligence,pp. 429436, San Francisco, CA. Morgan Kaufmann Publishers.Soininen, T., & Niemela, I. (1998). Formalizing configuration knowledge using ruleschoices. Seventh International Workshop Nonmonotonic Reasoning, Trento.Tan, S.-W., & Pearl, J. (1994). Specification evaluation preferences planninguncertainty. Proceedings Fifth International Conference PrinciplesKnowledge Representation Reasoning, pp. 530539, Bonn.Trapp, R. (1985). Utility theory preference logic. Erkenntnis, 22, 331339.von Wright, G. H. (1963). Logic Preference: Essay. Edinburgh University Press.von Wright, G. H. (1972). logic preference reconsidered. Theory Decisions, 3,140167. Reprinted (von Wright, 1984).von Wright, G. H. (1984). Philosophical logic: Philosophical Papers, Volume II. CornellUniversity Press, Ithaca, NY.190fiCP-NetsWellman, M., & Doyle, J. (1991). Preferential semantics goals. ProceedingsNinth National Conference Artificial Intelligence (AAAI-91), pp. 698703, Anaheim.191fiJournal Artificial Intelligence Research 21 (2004) 393-428Submitted 07/03; published 03/04Personalized System Conversational RecommendationsCynthia A. Thompsoncindi@cs.utah.eduSchool ComputingUniversity Utah50 Central Campus Drive, Rm. 3190Salt Lake City, UT 84112 USAMehmet H. Gokermgoker@kaidara.comKaidara Software Inc.330 Distel Circle, Suite 150Los Altos, CA 94022 USAPat Langleylangley@isle.orgInstitute Study Learning Expertise2164 Staunton CourtPalo Alto, CA 94306 USAAbstractSearching making decisions information becoming increasingly difficult amount information number choices increases. Recommendationsystems help users find items interest particular type, movies restaurants, still somewhat awkward use. solution take advantagecomplementary strengths personalized recommendation systems dialogue systems,creating personalized aides. present system Adaptive Place Advisortreats item selection interactive, conversational process, program inquiring item attributes user responding. Individual, long-term user preferencesunobtrusively obtained course normal recommendation dialogues useddirect future conversations user. present novel user model influences item search questions asked conversation. demonstrateeffectiveness system significantly reducing time number interactionsrequired find satisfactory item, compared control group users interactingnon-adaptive version system.1. Introduction MotivationRecommendation systems help users find select items (e.g., books, movies, restaurants)huge number available web electronic information sources (Burke,1999; Resnick & Varian, 1997; Burke, Hammond, & Young, 1996). Given large setitems description users needs, present user small set itemswell suited description. Recent work recommendation systems includesintelligent aides filtering choosing web sites (Eliassi-Rad & Shavlik, 2001), newsstories (Ardissono, Goy, Console, & Torre, 2001), TV listings (Cotter & Smyth, 2000),information.users systems often diverse, conflicting needs. Differences personalpreferences, social educational backgrounds, private professional interestspervasive. result, seems desirable personalized intelligent systemsc2004AI Access Foundation. rights reserved.fiThompson, Goker, & Langleyprocess, filter, display available information manner suits individualusing them. need personalization led development systems adaptchanging behavior based inferred characteristics userinteracting (Ardissono & Goy, 2000; Ferrario, Waters, & Smyth, 2000; Fiechter& Rogers, 2000; Langley, 1999; Rich, 1979).ability computers converse users natural language would arguablyincrease usefulness flexibility even further. Research practical dialogue systems,still infancy, matured tremendously recent years (Allen, Byron, Dzikovska,Ferguson, Galescu, & Stent, 2001; Dybkjr, Hasida, & Traum, 2000; Maier, Mast, &Luperfoy, 1996). Todays dialogue systems typically focus helping users completespecific task, planning, information search, event management, diagnosis.paper, describe personalized conversational recommendation system designed help users choose item large set basic type. goalsupport conversations become efficient individual users time.system, Adaptive Place Advisor, aims help users select destination (incase, restaurants) meets preferences.Adaptive Place Advisor makes three novel contributions. knowledge,first personalized spoken dialogue system recommendation, oneconversational natural language interfaces includes personalized, long-term usermodel. Second, introduces novel model acquiring, utilizing, representing usermodels. Third, used demonstrate reduction number system-user interactions conversation time needed find satisfactory item.combination dialogue systems personalized recommendation addresses weaknesses approaches. dialogue systems react similarly user interactingthem, store information gained one conversation use future.Thus, interactions tend tedious repetitive. adding personalized, long-termuser model, quality interactions improve drastically. time,collecting user preferences recommendation systems often requires form fillingexplicit statements preferences users part, difficult time consuming. Collecting preferences course dialogue lets user begin taskitem search immediately.interaction conversation personalized recommendation also affected choices acquisition, utilization, representation user models.Adaptive Place Advisor learns information users unobtrusively, coursenormal conversation whose purpose find satisfactory item. system storesinformation use future conversations individual. acquisitionutilization occur items presented chosen user,also search items. Finally, systems representation models goesbeyond item preferences include preferences item characteristics particular values characteristics. believe ideas extend typespreferences types conversations.paper, describe work Adaptive Place Advisor. beginintroducing personalized conversational recommendation systems, presentingdesign decisions along way. Section 3 describe system detail,394fiPersonalized Conversational RecommendationSection 4 present experimental evaluation. Sections 5 6 discuss relatedfuture work, respectively, Section 7 conclude summarize paper.2. Personalized Conversational Recommendation Systemsresearch goals two-fold. First, want improve interaction qualityrecommendation systems utility results returned making user adaptiveconversational. Second, want improve dialogue system performance meanspersonalization. such, goals user modeling differ commonly assumedrecommendation systems, improving accuracy related measures like precisionrecall. goals also differ previous work user modeling dialoguesystems (Haller & McRoy, 1998; Kobsa & Wahlster, 1989; Carberry, 1990; Kass, 1991),emphasizes ability track users goals dialogue progresses,typically maintain models across multiple conversations.hypothesis improvements efficiency effectiveness achievedusing unobtrusively obtained user model help direct systems conversational searchitems recommend. approach assumes large database itemschoose, reasonably large number attributes needed describeitems. Simpler techniques might suffice situations database smallitems easy describe.2.1 PersonalizationPersonalized user adaptive systems obtain preferences interactions users,keep summaries preferences user model, utilize model generatecustomized information behavior. goal customization increasequality appropriateness interaction result(s) generateduser.user models stored personalized systems represent stereotypical users (Chin,1989; Rich, 1979) individuals, hand-crafted learned (e.g., questionnaires, ratings, usage traces), contain information behaviorpreviously selected items, preferences regarding item characteristics (such locationprice), properties users (such age occupation) (Kobsa & Wahlster,1989; Rich, 1979). Also, systems store user models duration one interaction user (Carberry, Chu-Carroll, & Elzer, 1999; Smith & Hipp, 1994), whereasothers store long term (Rogers, Fiechter, & Langley, 1999; Billsus & Pazzani,1998).approach learn probabilistic, long-term, individual user models containinformation preferences items item characteristics. chose learned modelsdue difficulty devising stereotypes reasonable initial models new domainencountered. chose probabilistic models flexibility: single userexhibit variable behavior preferences relative rather absolute. Long-termmodels important allow influence across multiple conversations. Also, alreadynoticed, different users different preferences, chose individual models. Finally,preferences items item characteristics needed influence conversationsretrieval.395fiThompson, Goker, & Langleydecision made learn models, another design decision relates methodsystem collects preferences subsequent input learning algorithm(s).distinguish two approaches. direct feedback approach placesburden user soliciting preference information directly. example, systemmight ask user complete form asks classify weight interests usingvariety categories item characteristics. recent study (McNee, Lam, Konstan,& Riedl, 2003) showed forcing user provide ratings items (movies,case) choose, rather system chooses, actually leadbetter accuracy rates better user loyalty. However, users irritated needcomplete long questionnaires even begin enjoy given service,study context dialogue system involved simpler interaction.Another, slightly less obtrusive, form direct feedback encourages user providefeedback continues use particular service.second approach acquiring user models, one taken AdaptivePlace Advisor, infer user preferences unobtrusively, examining normal online behavior (Fiechter & Rogers, 2000; Rafter, Bradley, & Smyth, 2000). feel unobtrusivecollection preferences advantageous, requires less effort user. Also, usersoften cannot articulate preferences clearly learn domain.possible disadvantage unobtrusive approaches users may trust understandsystems actions change one interaction next. couldaddressed also letting user view modify user model (Kay & Thomas, 2000).Systems typically take one two approaches preference determination. Contentbased methods recommend items similar ones user liked past (Segal &Kephart, 1999; Pazzani, Muramatsu, & Billsus, 1996; Lang, 1995). contrast, collaborativemethods select recommend items users similar current user likedprevious interactions (Cotter & Smyth, 2000; Billsus & Pazzani, 1998; Konstan, Miller,Maltz, Herlocker, Gordon, & Riedl, 1997; Shardanand & Maes, 1995). collaborativefiltering bases recommendations previous selections users, suitablenew one-off items users uncommon preferences. content-based approach,hand, uses item description recommendation, thereforeprone problems. However, content-based techniques tend prefer attributevalues users preferred past, though allow new combinationsvalues. feel benefits content-based approach outweigh disadvantages;discuss methods overcoming disadvantages combining two techniquesSection 6.3.Ultimately, personalization one utilize learned user profile searchfor, identify, present relevant information right user right way righttime. User models utilized recommendation systems content processingselection (information filtering), navigation support web browsers (Pazzani et al., 1996),choice modality style presentation interaction (Brusilovsky & Maybury,2002). Adaptive Place Advisor adapts information filtering interactionbehavior, since relevant application since majorityinteraction natural language.396fiPersonalized Conversational Recommendation2.2 Conversational Recommendationgoal recommendation systems help users find items interest, thusmust eliminate choices consideration keeping others. achieve this,must either know determine interests current user, present itemssatisfy interests.One widely used approaches information acquisition recommendationsystems use simple form (usually single query box) user fills in. Coupledthis, common presentation format ranked list approach. scheme,system presents usually long list results, ordered predicted relevanceuser. combined technique information acquisition form presentationlist originated information retrieval, used help find documentsreports library research. basic method, refer ranked listapproach, general, however, underlies simpler search enginesWorld Wide Web, used regularly many different selection tasks.contrast, Adaptive Place Advisor supports conversations modeinteraction. approach, one conversational participant, call inquirer,asks second participant, call advisor, assistance selecting item.example, inquirer deciding restaurant dinner, conversationmay proceed follows:1.2.3.4.5.6.7.8.9.10.11.12.13.Inquirer: think eat tonight?Advisor: type food would like?Inquirer: types there?Advisor: say things like Chinese, Indian, Mediterranean.Inquirer: Oh, maybe cheap Indian place.Advisor: kind parking would like?Inquirer: dont care, long Palo Alto.Advisor: Im sorry, dont know restaurants like that, would like searchprice?Inquirer: No, think Id like Chinese instead.Advisor: Mandarin Gourmet 420 Ramona sound?1Inquirer: No, else have?Advisor: Jing-Jing Szechwan Hunan Gourmet 443 Emerson Street?Inquirer: Sure, sounds fine.refer systems mimic approach recommendation conversationalrecommendation systems.interaction supported conversational recommendation systems quite differentfound ranked list approach. important distinctioninteraction takes form sequence questions, designed eliminate itemsconsideration. Answering questions plays similar role giving keywordsform, aim remove alternatives rather rank results list.1. system support exact sample dialogue. example, point conversationdisplays information restaurant screen asks one sound?similar question.397fiThompson, Goker, & Langleyconversational process also help inquirer better understand desires, sincethinking possible questions answers may clarify goals ways keyword entryencourage. addition, conversation support user variability lettinguser control item attribute choices conversation. Finally, inquirer neverhears complete item choices remain. avoids presentinglong list items lets user narrow choices iterative, manageablefashion.dialogues seem better recommendations must delivered speech rathervisually, example engaged inquirer driving. also seemideal, independent modality, tasks like destination selection help-desk support(Goker & Roth-Berghofer, 1999; Aha & Breslow, 1997), user needs convergeitems. hand, keyword entry ranked list methods seemappropriate situations user prefers provide requirements once,situations information presented visually, situations usermay want examine many options.eliminating options, conversational recommendation systems ultimately directusers suitable solution. However, conversation become tiring qualityfirst result returned may acceptable user. interactionsfriend knows concerns directed produce better resultsstranger, dialogues conversational advisor become efficienteffective time. goals user modeling include improvement subjectivequality effectiveness results (found items) conversation leadsresults. example, several conversations inquirer above, newinteraction may proceed follows, question parking eliminateditem presentation order changed:1.2.3.4.5.6.7.Inquirer:Advisor:Inquirer:Advisor:Inquirer:Advisor:Inquirer:think eat tonight?type food would like?Oh, maybe Chinese place.2city prefer?something Palo Alto?Jing-Jing Szechuan Gourmet 443 Emerson sound?Sure, sounds fine.turn next design choices concerning management conversations.2.3 Conversation via Dialogue ManagementDialogue systems carry conversations users natural language, whether spokentyped. main tasks performed dialogue systems language interpretation, languagegeneration, dialogue management. Natural language interpretation generationtopics onto discuss here; two introductory texts,see Allen (1995) Jurafsky Martin (2000). enable focus user modeling,system allows moderately complex user utterances pre-coded set systemutterances, discussed Section 3.3.2. response shows Inquirer learned use system efficiently well.398fiPersonalized Conversational Recommendationsimplest dialogue managers based finite-state automata statescorrespond questions arcs correspond actions depend user-providedresponse (Stent, Dowding, Gawron, Bratt, & Moore, 1999; Winograd & Flores, 1986).systems support called fixed- system-initiative conversations,one participants controls actions, whether system helping useruser asking questions system. Next complexity frame- template-basedsystems questions asked answered order (Bobrow et al., 1977).Next, true mixed-initiative systems allow either dialogue participant contributeinteraction knowledge permits (Allen, 1999; Haller & McRoy, 1998; Pieraccini,Levin, & Eckert, 1997). Thus, conversational focus change time dueusers (or systems) initiative change. Finally, different approaches supportsophisticated dialogues include plan-based systems (Allen et al., 1995; Cohen & Perrault,1979) systems using models rational interaction (Sadek, Bretier, & Panaget, 1997).allow reasonably complex conversations keeping system design straightforward, chose frame-based approach dialogue management. Thus, Adaptive Place Advisor allows conversational flexibility fully system-initiativeparadigm would allow. Users fill attributes addition suggested system. However, cannot force system transition new subtasks,system negotiate users determine participant takeinitiative.2.4 Interactive Constraint-Satisfaction SearchConstraint-satisfaction problems provide general framework defining problems interest many areas artificial intelligence, scheduling satisfiability (Kumar,1992). general form, constraint-satisfaction problems involve set variableswhose domains finite discrete, along set constraints definedsubset variables limit value combinations variables take.goal find assignment values variables satisfies given constraints.Cucchiara, Lamma, Mello, Milano (1997) define class interactive constraintsatisfaction problems involve three extensions standard formulation. First,include constraint acquisition stage user incrementally add newconstraints problem solved. Second, variables domain includedefined undefined portion, user add new values defined portionconstraint acquisition. Third, allow incremental update partial solutionbased domain constraint updates.framework encompass item search portion conversations managedAdaptive Place Advisor; include item presentation portion.setting, constraints simply attribute-value specifications, cuisine = Chinese.Place Advisors search fully general framework,incorporate notion undefined portions domains. However, acquireconstraints via users specifications conversation incrementally updatessolutions response.399fiThompson, Goker, & LangleySystem Output(Voice)PromptsUser Input(Voice)Speech GeneratorSpeech RecognizerSystem OperatorsValuesUser OperatorsValuesDomainModelDialogue ManagerConversation HistoryUserModelsInitial QueryRecognitionGrammarsResults,Attribute InformationUser ModelingSystemUpdated QueryRetrieval EngineItemDatabaseFigure 1: Components Adaptive Place Advisor interactions.3. Adaptive Place Advisorsection, first present overview Adaptive Place Advisors functioning,follow details components.3 system carries number taskssupport personalized interaction user; particular, it:utilizes user model initialize probabilistic item description expanded query,generates context-appropriate utterances,understands users responses,refines expanded query explicit requirements (constraints) obtaineduser conversation,retrieves items matching explicitly specified part query database,calculates similarity retrieved items query,selects next attribute constrained relaxed conversationnumber highly similar items acceptable,presents suitable items number items acceptable,acquires updates user model based interactions.responsibilities tasks distributed among various modules system,shown Figure 1. Dialogue Manager generates, interprets, processes conversations; also updates expanded query user interaction. Retrieval Enginecase-based reasoning system (Aamodt & Plaza, 1994) uses expanded queryretrieve items database measure similarity users preferences.User Modeling System generates initial (probabilistic) query updates longterm user model based conversation history. Speech Recognizer SpeechGenerator handle users input control systems output, respectively.3. discussed Section 5.2, approach destination advice draws earlier analysistask Elio Haddadi (1998, 1999).400fiPersonalized Conversational Recommendationfind items recommend user, Place Advisor carries augmentedinteractive constraint-satisfaction search. goal entire conversation presentitem acceptable user. constraint-satisfaction portion,system carries conversation find small set items. searchphase, two situations determine systems search operators thus questions. First,under-constrained specification means many items match constraints,system must obtain information user. Second, matching items,system must relax constraint, thus allowing items contain domain valuerelaxed attribute.4 system ends search phase small number itemsmatch constraints highly similar (based similarity threshold) userspreferences. Item presentation (in similarity order) begins point, similaritycomputation used rank items satisfy constraints.search item presentation process also influenced User Modeling Systemthus personalized. main mechanism personalization expandedquery, probabilistic representation users preferences, long-term (over manyconversations) short-term (within conversation). often referquery, always refers constraints explicitly implicitly specifieduser. Thus, query expanded beyond explicit (short-term) constraintsusing (long-term) constraints implicit user model. sense, initial queryrepresents constraints system thinks user probably want. systemincrementally refines query course conversation user, settingexplicit, firm constraints user verifies disconfirms assumptions. longterm, User Modeling System updates user model based users responsesattributes items offered conversation.Retrieval Engine searches database items match explicit constraintsquery. computes similarity retrieved items users preferencesreflected expanded part query. Depending number highly similar results, Retrieval Engine also determines attribute constrainedrelaxed.sum, system directs conversation manner similar frame-based system,retrieves ranks items using case-based reasoning paradigm, adapts weightssimilarity calculation based past conversations user, thereby personalizingfuture retrievals conversations. section, present details AdaptivePlace Advisors architecture. describing user model, elaborateRetrieval Engine Dialogue Manager. Finally, discuss system updatesuser model user interacts it.3.1 User Modelfocus personalized conversation suggests fine-grained model user preferences,emphasizing questions user prefers answer responses tends give,addition preferences entire items. describe model detail.later sections, describe influences item ranking question ordering,4. constraints later modified, system lets user later specify value, evenone caused over-constrained situation.401fiThompson, Goker, & LangleyTable 1: Example user model.User NameHomerAttributeswiCuisine0.4Values probabilitiesItalianFrenchTurkishChineseGermanEnglish0.350.20.250.10.10.00.2onetwothreefourfive0.30.30.10.1......0.2...Parking0.1Price RangeItem Nbr.Accept/PresentValetStreetLot0.50.40.1081553727638...639923 / 2510 / 1933 / 36...12 / 23turn determine quickly system stop asking questions start presentingitems. general, user may tend to:answer questions attributes often others,provide attribute values often others,choose items often others,provide certain combinations values often independent distributionwould predict,accept either large small amounts value item diversity.tendencies influenced users preferences, turn captureduser model. Attribute preferences represent relative importance user placesattributes (e.g., cuisine vs. price) selecting item. Preferred values show usersbias towards certain item characteristics (e.g., Italian restaurants vs. French restaurants).Item preferences reflected users bias certain item, independentcharacteristics. Combination preferences represent constraints combined occurrenceitem characteristics (e.g., accepts restaurants San Francisco valetparking). Diversity preferences model time needs pass itemcharacteristic suggested users tolerance unseen values items. Itempreferences related single items, whereas attribute, value, combination preferencesapplicable search items general. Diversity preferences relateitems search.Currently, Adaptive Place Advisor models preferences user mayattributes, values, items, combination diversity preferences.former easily captured either probability distributions counts, illustratedTable 1. Place Advisor maintains probability distribution represent attributepreferences independent probability distributions represent preferences attributes set values. attribute preferences, system uses domain knowledgeinitialize weights; example, price usually considered important park-402fiPersonalized Conversational Recommendationing. absence information, case value preferences, systembegins uniform distribution.system represents item preferences ratio number times itemaccepted number times presented; initialized assumingitems presented accepted large percentage (nine ten, 90%)time. may cause updates (see below) small effect undesirableitems suggested once, effect quickly discounting alternativesearly learning process. turn encourages user explore alternatives,allowing system learn additional items. sum, item preferences representprobability user accepting particular item presented, ratherrepresenting probability distribution items.3.2 Retrieval Engineplace, user model affects behavior system RetrievalEngine, interacts database retrieve items, any, satisfy currently agreed upon constraints. module also interacts query determinesimilar items users preferences determine best orderingattributes constraining relaxing, appropriate. types interactionsuser model support goal quickly narrowing search satisfactory item.Similar way human advisor bases assumptions regarding inquirerprevious interactions, system uses cumulative experience, reflected user model,basis computation. Retrieval Engine represents users preferencesrequirements expanded query, partial, probabilistic item specification determinedconversation user model. query initialized user model,thus contains preference-based probabilities attributes values useryet explicitly specified along users item preferences. courseconversation, system updates query reflect values user specifies.attribute, sets probability value agreed upon conversation 1.0,probabilities zero. example, user says Chinese Italian,system sets value probabilities Chinese Italian 1.0, cuisineprobabilities zero. equivalent disjunction probabilistic queries, onevalue combination specified user.5first main aspect Retrieval Engine personalized item rankingtechnique. Unlike typical case-based similarity computation, retrieves items beyondmatch query exactly, computation used system restricts retrieveditems desirable user. system filters items includedcurrent case base according characteristics explicitly specified user sortsremaining items according similarity items user liked past.Thus, Engine first queries database retrieve items match currentconstraints exactly.6 Then, Place Advisor uses probability distributionsquery compute likely user choose item. system calculates5. user study described Section 4, users specified disjunctive query.6. attributes user selected one value, assume supplied value wouldacceptable.403fiThompson, Goker, & Langleysimilarity current query, Q, item, usingSim(Q, I) = RInXwj P (Vj ) ,j=1RI users item preference ratio item I, n number attributes, wjweight attribute j Q, Vj value attribute j I, P (Vj )value preference (probability Q) value. Similarity formula baseduser model search state. Thus, unconstrained attribute, estimatesprobability user accept items value attribute.system calculates similarity item, compares items similarity constantsimilarity threshold, retains items exceed threshold.second main personalized aspect Retrieval Engine ranking attributesunder- over-constrained situations. helps assure user likelyrespond informatively systems questions under-constrained situations,allow suggested attribute relaxed over-constrained situations.situations, one option order attributes randomly, technique usedsimple dialogue systems. Another option use conditional entropy measure selectattributes (see Section 6.1). third rank attributes order desirabilityuser, reflected user model, take option.addition using long term user model rank attributes, system also usesattribute weights reflected query. see see later querysattribute weights, initialized user model, also influenced conversation. over-constrained situation, attribute ranking order highest lowest,under-constrained situation, reverse.7 Using attribute weights ratherconditional entropy avoids pitfalls arise continuously changing valuedistribution data (new restaurants, restaurants going business, etc.).affect attribute rankings, users may confused resulting variability.even worse, would reflect users preferences. Further, every questionhigh information gain high relevance user selecting destination (e.g., parkingoptions may high score asked user decidedcuisine location.)summary, user model influences item retrieval, item ranking, attribute ranking, turn influence systems utterances conversation.3.3 Conversing Userconverses user, Dialogue Manager uses results Retrieval Enginesfunctions. system uses frame containing simple list constraints supportinteractive constraint-satisfaction search (see Jurafsky et al. 1994 Dowding et al. 1993similar formulation). usual type system, user respondsystem request fill constraint ignoring attribute specifying value7. CBR systems necessarily use weighting factors similarity computationquestion ordering. However, application area, correct make assumptionattributes importance impact similarity computation.404fiPersonalized Conversational RecommendationTable 2: Speech acts supported Adaptive Place Advisor.System Speech ActsAttempt-ConstrainSuggest-RelaxRecommend-ItemQuit-Start-ModProvide-ValuesClarifyAsks question obtain value attribute.Asks question remove values attribute.Recommends item satisfies constraints.States matching items remain asks whethermodify search, start over, quit.Lists small set values attribute.Asks clarifying question.User Speech ActsProvide-ConstrainAcceptRejectProvide-RelaxStart-OverQuitQuery-ValuesProvides value attribute.Accepts relaxation suggestion item generated system.Rejects systems proposed attribute, relaxation attempt, item.Provides attribute value removal.Indicates desire reinitialize constraints begin again.Indicates desire stop conversation.Asks information possible values attribute.Table 3: Dialogue State.VariableDescriptionConstrainedRejectedFixedConstrainRelaxQueryNumber-of-ItemsAttributes whose values specified.Attributes whose value user declined provide.Constrained attributes user indicated relaxed.next attribute constrain, any.next attribute relax, any.Probability model desired item constraints.Number database items matching query exceedingsimilarity threshold.matching items ranked similarity order.Items user rejected.users recently uttered speech act.systems recently uttered speech act.Ranked-ItemsRejected-ItemsUser-MoveSystem-Actdifferent one(s) instead (Goddeau et al., 1996; Ward & Issar, 1996). speech actssupported listed Table 2.two main phases dialogue, interactive constraint-satisfaction portionitem presentation portion. constraint-satisfaction portion dividedover- under-constrained situations. dialogue state (Table 3) determinessystems utterance range responses expected point. system updatesdialogue states variables appropriate throughout conversation.405fiThompson, Goker, & LangleyTable 4: Sample Conversation.1.2.3.4.5.6.7.8.9.10.11.12.13.Inquirer:Advisor:Inquirer:Advisor:Inquirer:Advisor:Inquirer:Advisor:price?Inquirer:Advisor:Inquirer:Advisor:Inquirer:think eat tonight?type food would like?types there?say things like Chinese, Indian, Mediterranean.Oh, maybe cheap Indian place.kind parking would like?dont care, long Palo Alto.Im sorry, dont know restaurants like that, would like searchNo, think Id like Chinese instead.Mandarin Gourmet 420 Ramona sound?No, else have?Jing-Jing Szechwan Hunan Gourmet 443 Emerson Street?Sure, sounds fine.detail, systems speech act (or move) interactive constraint-satisfaction determined Number-of-Items dialogue state variable. Further, speechact determines speech recognition grammar employ interpret users nextutterance. common situation many items (more small threshold, three) match current constraints.8 situation, system makesAttempt-Constrain move, asks user fill value attribute.move, responded appropriately user, would reduce number itemsconsidered satisfactory user. attribute Constrain one rankedhighest Retrieval Engine already Constrained Rejected.first sample conversation, repeated Table 4, utterances 2 6 illustrate AttemptConstrains system.One user response Attempt-Constrain Provide-Constrain,provides value specified attribute additional attributes, utterances 57. second possible response Reject, user indicates disinterestdislike attribute, first part utterance 7. illustratedexamples, user combine one move single utterance.second situation, over-constrained query, occurs itemssatisfy agreed upon constraints similar enough users preferences,thus Retrieval Engine returns empty set (Number-of-Items = 0). case,system performs Suggest-Relax move informs user situation askswould like relax given constraint. attribute Relax chosen RetrievalEngines highest ranked attribute9 already Fixed. illustratedutterance 8 conversation Table 4. utterance 9 conversation, userrespond rejecting (Reject) systems suggestion accept (Accept).former case, attribute Fixed system try relax again.8. discuss number items matching constraints, refer items remainsimilarity filtering discussed Section 3.2.9. Recall Section 3.2 actually lowest ranking attribute user model.406fiPersonalized Conversational Recommendationcombination either speech acts, user specify attributesrelax addition to, instead of, system-suggested attribute (Provide-Relax).items satisfy constraints, system ends interactive searchbegins suggest items user (Recommend-Item) order similarity,utterances 10 12 above. user either accept reject item. useraccepts item (Accept), system ends conversation, reached goalstate. user rejects item (Reject), system presents alternative,remain. Note three meanings Reject speech acts user,two meanings Accept speech acts, since user accept AttemptConstrain providing explicit value attribute constrained.three special situations covered above. first queryover-constrained, user Fixed attributes could relaxed. seconduser rejected items match constraints. two situations,system informs user situation, asks whether would like quit, startover, modify search (Quit-Start-Mod), reacts accordingly. third specialsituation Number-of-Items exceeds presentation threshold, attributesConstrained Rejected. case, Place Advisor begins presentitems user.support spoken natural language input output, use speech recognition package Nuance Communications, Inc. package lets us write differentrecognition grammar situations described use human-recordedprompts (rather text-to-speech). string words recognized systemparsed using recognition grammars wrote, used users withoutadaptation. Future work could include personalized recognition grammars well personalized information preferences. grammars use semantic tags fill slot:besides slots attribute, define slots rejection acceptance systemssuggestions. complex domains, sophisticated parsing methods may required, simple scheme gives user reasonably diverse set utterance options.Nuance modules also generate response user requests help (Query-Values)Provide-Values speech act, enter clarification dialogues confidencerecognized utterance given threshold (Clarify). currently simpleinteractions system provides examples answers recently utteredprompt, asks user repeat themselves.Finally, item presentation portion dialogue only, system displaysrestaurant information (name, address, phone number) screen, outputsspoken prompt one? chose presentation modality duereluctance use text-to-speech generation large number prompts wouldrecord produce spoken language restaurant. However, noteuser still responds spoken reply, feel presentation modesubstantially influenced user-modeling behavior Place Advisor.system-user interaction affects subsequent rounds database retrieval similarity calculation via updates expanded query. Table 5 shows effects relevantspeech acts query, turn used similarity calculation describedSection 3.2. table, shortened names system movessake brevity.407fiThompson, Goker, & LangleyTable 5: effects speech acts query. Key: Constrain = AttemptConstrain, Relax = Suggest-Relax, Recommend = Recommend-Item.System-MoveUser-MoveEffect QueryConstrainProvideConstrainConstrainRelaxRecommendRelaxRecommendRejectRejectRejectAcceptAcceptProvide-RelaxStart-OverSet probabilities provided values one.Set probability values constrained attributeszero. attribute rejected previously, resetattribute probability user model.Drop attribute setting probability zero.effect; Dialogue Manager selects next attribute.Update item preference counts (see Section 3.4).Reset value probabilities attribute user model.Update item preference counts (see Section 3.4).Reset value probabilities attribute user model.Initialize user model.3.4 Updating User Modelmain contribution addition personalization conversational recommendation model. user model (Section 3.1) represents personalization,Adaptive Place Advisor must update appropriately. adaptive recommendation systems (Smyth & Cotter, 1999; Linden, Hanks, & Lesh, 1997; Pazzani et al.,1996; Lang, 1995) require user provide direct feedback generate user model,basic approach unobtrusively derive user preferences. Thus, systemintroduce unnecessary interactions, learns interactions needed supportitem recommendation. describe system gathers item, attribute, valuepreferences. described fully below, system modifies feature value weights(Fiechter & Rogers, 2000; Zhang & Yang, 1998; Bonzano, Cunningham, & Smyth, 1997;Wettschereck & Aha, 1995) latter two, increases counts ratioaccepted presented items.determining points dialogue update user model,considered several factors. wanted enable system acquire information quickly,discourage making erroneous assumptions. thought users mightexplore search space constraining attributes, decidedsystem update value preferences user-specified constraint. However,instead chose allow model updates item suggestion, learning processmight slow. choices described are, feel, good tradeoffextremes.three circumstances chose user model update (1) usersAccept speech acts Suggest-Relax situation, (2) users Accept speechacts Recommend-Item situation, (3) users Reject speech actRecommend-Item speech act system. First, assume user acceptsitem, indicating: (1) preference item itself, (2) preferences attributes408fiPersonalized Conversational Recommendationconstrained find item, (3) preferences values providedattributes. Thus, user accepts item presented system, probabilitiesappropriate item, attributes, values increased. item preference,system simply adds one presentation acceptance counts. attributevalue preferences, system increases probability appropriate weight smallamount proportional current weight, renormalizes weights. Thus attributevalue preferences biased measures avoid zero counts values usernever chooses, typical type probabilistic representation.Second, user rejects item presented system, assumedislike particular item. assume anything characteristicsitem, since user specified characteristics. Therefore, rejecteditems system simply adds one presentation count.third situation system updates user model when, querybecome over-constrained, presents attribute relaxation user acceptsrelaxation. situation, assume that, matching item,user would satisfied it, since characteristics specified conversationfar satisfactory. Therefore, relaxation occurs, system increasesattribute preferences constrained attributes increases value preferencesuser-specified values, manner similar Accept situation RecommendItem. enables Adaptive Place Advisor quickly make inferencesusers preferences.4. System Evaluationstated earlier, believe user modeling increases effectiveness efficiencyconversations system time. test hypothesis, carriedexperiment version Adaptive Place Advisor recommends restaurantsSan Francisco Bay Area. system describes items using seven attributes: cuisine,rating, price, location, reservations, parking options, payment options. attributesvalues, cuisine location dozens. approximately 1900 itemsdatabase.asked several users, Bay Area, interact system helpdecide go eat. users given external guidance instructionstypes restaurants select, look choosemight actually patronize. experimenter present interactions,filmed, help needed except rare occasions subject repeatedlytried words included speech recognition grammar.4.1 Experimental Variablestest hypothesis benefits personalization Adaptive Place Advisor, controlled two independent variables: presence user modelingnumber times user interacted system. First, anticipatedusers might improve interactions Place Advisor time, dividedsubjects experimental modeling group control group. 13 subjectsmodeling group interacted version system updated user model409fiThompson, Goker, & Langleydescribed Section 3.4. 11 subjects control group interacted versionupdate model, selected attributes items defaultdistribution described Section 3.1. Naturally, users unaware assignedgroup. Second, since predicted systems interactions would improve time,gained experience user, observed behavior successive points alonglearning curve. particular, subject interacted system around 15successive sessions. tried separate subjects sessions several hours,always possible. However, general subjects use system actuallyhelp decide eat either day near future; provideconstraints telling system knew restaurants BayArea.determine versions efficiency recommending items, measured severalconversational variables. One average number interactions needed findrestaurant accepted user. defined interaction cycle startedsystem providing prompt ended systems recognition users utteranceresponse, even response answer question posed prompt.also measured time taken conversation. began start transactionbutton pushed ended system printed Done (after user accepteditem quit).also collected two statistics depended whether user modelingeffect. First number system rejections, is, number timessystem either obtain recognition result confidence low.either case system asked user repeat himself. Since measurerecognition quality effects personalization, omitted countinteractions. second, serious problem speech misrecognition errorsystem assigned utterance different meaning user intended.Effectiveness, thus subjective quality results, somewhat difficultquantify. wanted know users degree satisfaction systems behavior.One indication rejection rate: proportion attributessystem asked subject care (Rejects Attempt-Constrain situations).second measure hit rate: percentage conversations first itempresented acceptable user. Finally, also administered questionnaire usersstudy get subjective evaluations.4.2 Experimental Resultsresults experiment generally supported hypothesis respect efficiency.provide figures show average values users particular group, errorbars showing 95% confidence intervals. x axis always shows progression usersinteractions system time: point nth conversation completedeither finding acceptable restaurant quitting.Figure 2 shows that, modeling group, average number interactions requiredfind acceptable restaurant decreased 8.7 5.5, whereas control groupquantity actually increased 7.6 10.3. used linear regression characterizetrend group compared resulting lines. slope modeling line410fi468Number interactions10121416Personalized Conversational Recommendation2Modeling0Control01234567891011 12 13 14 15Conversation numberFigure 2: Average number interactions per conversation.differed significantly (p = 0.017) control line, former smallerlatter, expected.difference interaction times (Figure 3) even dramatic. modelinggroup, quantity started 181 seconds ended 96 seconds, whereas controlgroup, started 132 seconds ended 152 seconds. used linear regressioncharacterize trends group time found significant difference(p = 0.011) two curves, slope modeling subjects smallercontrol subjects. also note measures includetime system initialization (which could 10% total dialogue time).instead used start time first system utterance dialogue, differencetwo conditions would even clearer.speech recognizer rejected 28 percent interactions study. Rejectionsslow conversation introduce errors. misrecognition rate muchlower occurred seven percent interactions experiment. feelrates acceptable, expanding number supported utterancescould reduce first number further, potentially increasing second.common recognition error, Adaptive Place Advisor inserted extra constraintsuser intend.results effectiveness ambiguous. Figure 4 plots rejection ratefunction number sessions. decrease rejection rate time would mean that,system gains experience user, asks fewer features irrelevantuser. However, dependent variable found significant difference (p = 0.515)regression slopes two conditions and, indeed, rejection rateneither group appears decrease experience. negative results may duerarity rejection speech acts experiment. Six people never rejected constraint411fi50100Time per conversation150200250Thompson, Goker, & LangleyModeling0Control01234567891011 12 13 14 15Conversation numberFigure 3: Average time per conversation.average person used 0.53 Reject speech acts Attempt-Constrainper conversation (standard deviation = 0.61).Figure 5 shows results hit rate, indicate suggestion accuracy stayedstable time modeling group decreased control group. One explanation latter, expect, control users became less satisfiedPlace Advisors suggestions time thus carried explorationitem presentation time. However, concerned differencetwo groups. Unfortunately, slopes two regression lines significantlydifferent (p = 0.1354) case.also analyzed questionnaire presented subjects experiment. firstsix questions (see Appendix A) check boxes assigned numerical values, nonerevealed significant difference two groups. second partquestionnaire contained open-ended questions users experienceAdaptive Place Advisor. general, subjects groups liked systemsaid would use fairly often given opportunity.4.3 Discussionsummary, experiment showed Adaptive Place Advisor improvedefficiency conversations subjects gained experience time,improvement due systems update user models rather subjectslearning interact system. conclusion due significan differencesuser modeling control groups, number interactions timeper conversation. significance holds even face large error bars smallsample size. turn implies differences large system could makesubstantial difference users.412fiNumber rejections1.522.5Personalized Conversational RecommendationModeling00.51Control01234567891011 12 13 14 15Conversation numberFigure 4: Rejection rate modeling control groups.results effectiveness ambiguous, trends right directionsignificant differences modeling control groups. Subjectsconditions generally liked system, found significant differences alongdimension. larger study may needed determine whether differences occur.user studies warranted investigate source differencestwo groups. One plausible explanation items presented sooner, average,user modeling group control group. measured value (i.e.,average number interactions first item presentation) current studyfound decrease user modeling group (from 4.7 3.9) increasedcontrol group (from 4.5 5.8). reasonably large difference differenceslope two regression lines statistically significant (p=0.165). larger studymay needed obtain significant difference. general, however, interactionuser model order questions asked, turn influencesnumber items matching point conversation. turn determinessoon items presented conversation. Therefore, items presented oftenuser modeling group, largest influence user model due item acceptsrejects.5. Related ResearchPrevious research related topics roughly broken three areas, firstfocusing personalized recommendation systems, second conversational interfaces,third adaptive dialogue systems. restrict discussionstrongly related work.413fiHit rate1.5Thompson, Goker, & LangleyModeling0.00.51Control01234567891011 12 13 14 15Conversation numberFigure 5: Hit rate modeling control groups.5.1 Personalized Recommendation SystemsAlthough research personalized recommendation systems become widespreadrecent years, basic idea traced back Rich (1979), discussedwork conversational interfaces. Langley (1999) gives thorough review recentresearch topic adaptive interfaces personalization.Several adaptive interfaces attempt collect user information unobtrusively.interesting example Casper project (Rafter et al., 2000), online recruitmentservice. project investigates methods translate click read-time dataaccurate relevancy information, given raw data inherently noisy. Similarly,Goecks Shavlik (2000) describe technique learning web preferences observingusers browsing behavior. Another example Adaptive Route Advisor (Rogerset al., 1999), recommends driving routes specified destination. systemcollects preferences attributes number turns driving time basisusers selections modifications systems proposed routes.Adaptive Place Advisor uses constraint-based interaction searchitems, interaction approach item search. alternative takencandidate/critique, tweaking, approach. Tweaking systems, Findsuite (Burke, 1999), typically require user begin interaction filling valuespredetermined attributes. present item, point useropportunity change search parameters try find desirable item. Eaton,Freuder, Wallace (1997) take similar approach Matchmaking system.addition, exploit constraint satisfaction manage search. Neither Findsuite Matchmaking system, however, learns user models. related systeminclude learning component Shearin Lieberman (2001), learnsattribute preferences unobtrusively. tweaking valid method, appropriate,414fiPersonalized Conversational Recommendationfeel, environment speech interaction mode, since presentinguser options would somewhat cumbersome. Even though current systemalso presents options search constrained, limits number items presented.Even full speech version seem onerous.5.2 Conversational Interfacesconsiderable ongoing work area conversational systems, evidencedgeneral surveys Dybkjr et al. (2000) Maier et al. (1996). ZukermanLitman (2001) give thorough overview user modeling dialogue systems. Rich(1979) reported one earliest (typewritten) conversational interfaces, focusedbook recommendation. beginning interaction, system asked severalquestions place user stereotype group, thereby initializing user model.conversation progressed, model adjusted, system using ratingsrepresent uncertainty. However, language understanding capabilities systemlimited, mostly allowing yes/no user answers. recently, dialogue systems utilizemodels users beliefs intentions aid dialogue management understanding,though typically systems maintain models course single conversation(Kobsa & Wahlster, 1989).noted Section 2.3, important distinction whether one conversationalparticipant keeps initiative, whether initiative switch participants.Two ambitious mixed-initiative systems planning tasks Trains (Allen et al., 1995)recent Trips (Allen et al., 2001). Like Place Advisor, programs interact user progressively construct solution, though knowledge structurespartial plans rather constraints, search involves operators plan modification rather database contraction expansion. Trains Trips lackmechanism user modeling, underlying systems considerably matureevaluated extensively.Smith Hipp (1994) describe another related mixed-initiative system limiteduser modeling, case conversational interface circuit diagnosis. systemaims construct plan set constraints, rather proof tree. centralspeech act, requests knowledge user would aid proof process,invoked program detects missing axiom needs reasoning.heuristic plays role system Place Advisors heuristicselecting attributes constrain item selection. interface infers user knowledgecourse single conversation, long term approach.respect dialogue management, several previous systems used methodsimilar frame-based search. particular, Seneff et al. (1998) Dowding et al.(1993) developed conversational interfaces give advice air travel. Like PlaceAdvisor, systems ask user questions reduce number candidates, treatingflight selection interactive construction database queries. However, questionsequence typically fixed advance, despite clear differences among individualsdomain. Also, systems usually require constraints specified itempresentation begins.415fiThompson, Goker, & Langleyalternative technique selecting questions ask information elicitation presented Raskutti Zukerman (1997). overall system necessitatessystem recognize plans user attempting carry out. system mustdecide best complete plans. insufficient information available planformation, system enters information seeking subdialogue similar constraintsatisfaction portion dialogues. system decide question ask baseddomain knowledge based potential informativeness question.Another approach dialogue management conversational case-based reasoning(Aha, Breslow & Munoz-Avila, 2001), relies interactions user retrievecases (items) recommend actions correct problem. speech actsbasic flow control much common Adaptive Place Advisor,process answering questions increasingly constrains available answers. One significantdifference approach generates several questions items, respectively, time,user selects question answer item closest needs,respectively.Finally, approach draws alternative analysis item recommendation, described Elio Haddadi (1998, 1999). main distinctions workapproach include personalization, distinguish searchtask space discourse space, combine two,place greater emphasis user intentions. Keeping distinction taskdiscourse space personalized system would unnecessarily complicate decisionsperform user model updates utilize model.5.3 Adaptive Dialogue SystemsFinally, another body recent work describes use machine learning formsadaptation improve dialogue systems.10 Researchers area develop systemslearn user preferences, improve task completion, adapt dialogue strategiesindividual conversation.closest work also pursues goal learning user preferences. Carberry et al.(1999) report one example consultation dialogues, take different approach.system acquires value preferences analyzing users explicit statementspreferences acceptance rejection systems proposals. uses discretepreference values instead fine-grained probability model. Also, systemuse preferences item search item presentation time help evaluatewhether better alternatives exist. Finally, evaluation based subjects judgementsquality systems hypotheses recommendations, characteristicsactual user interactions. could, however, incorporate item search ideas,allowing near misses user-specified constraints actual items.Another system focuses user preferences interactive travel assistant (Lindenet al., 1997) carries conversations via graphical interface. system asksquestions goal narrowing available candidates, using speech acts similarours, also aims satisfy user interactions possible. approach10. work adaptation speech recognition grammars (e.g., Stolcke et al., 2000), related, addresses different problem uses different learning techniques, discuss here.416fiPersonalized Conversational Recommendationminimizing number interactions use candidate/critique approach.users responses, system infers model represented weights attributes pricetravel time. Unlike Adaptive Place Advisor, carry profilesfuture conversations, one envision version so.Several authors use reinforcement learning techniques improve probabilityprocess task completion conversation. example, Singh et al. (2002) useapproach determine systems level initiative amount confirmationuser utterances. goal optimize, users, percentage dialoguesgiven task successfully completed. system leverages learned informationinteracting users, rather personalizing information. Also, Levin,Pieraccini, Eckert (2000) use reinforcement learning determine questionask point information seeking search, demonstrate utilityapproach real users.Finally, number systems adapt dialogue management strategy courseconversation based user responses dialogue characteristics. example,Litman Pan (2002) use set learned rules decide whether user difficultyachieving task, modify level system initiative confirmation accordingly.Maloor Chai (2000) present help-desk application first classifies usernovice, moderate, expert based responses prompts. adjusts complexitysystem utterances, jargon, complexity path taken achieve goals.Horvitz Paek (2001) apply user modeling dialogue system uses evidencecurrent context conversation update Bayesian network. networkinfluences spoken language recognition hypothesis causes appropriate adjustmentssystems level initiative. Chu-Carroll (2000) describes system adaptslanguage generation initiative strategies individual user within single dialogue.Also, Jameson et al. (1994) use Bayesian networks system take roleeither buyer seller transaction, changes inquiry sales strategybased beliefs inferred participants utterances.6. Directions Future Workresults date Adaptive Place Advisor promising much remainsdone. section, discuss ways make search model flexible,expand conversational model, enrich user model learning technique.also consider extensive evaluations system.6.1 Search Modelrespect search mechanism, first plan investigate alternative techniquesusing item similarity values determine return, example cuttingitems point similarity drops steeply, instead current usethreshold. also note work Cohen, Schapire, Singer (1999)learning rank instances could apply nicely work, augmenting currentitem ranking scheme. Additionally, plan develop version system generatesalternative items values over-constrained situation (Qu & Beale, 1999). One waywould use preferences estimate strength stated constraint,417fiThompson, Goker, & Langleymerge preference-based similarity metric traditional domain-specificsimilarity metric (Pieraccini et al., 1997). also plan evaluate effect makingeven stronger assumptions user preferences. example, system certainenough value preference, may ask question associatedattribute.final improvement search mechanism concerns techniques ranking attributes constraining relaxing. attribute constraint ranking, implemented yet evaluated conditional entropy measure (Goker & Thompson, 2000).system selects attribute constrain determining attribute highestconditional entropy among unconstrained attributes. scheme would usefulranking attributes relax. Therefore, system simply determines size casebase would result attribute relaxed, ranks case bases smallestlargest, orders attributes accordingly, excluding attributes that, relaxed,would still result empty case base. also plan investigate combinationuser model information gain, well alternative attribute ranking techniques one used Abella, Brown, Buntschuh (1996). Another optionadd personalization otherwise adapt variable selection techniques usedconstraint-satisfaction solvers.6.2 Conversational Modelplan progress towards complex dialogues complex constraints. First,plan increase number speech acts available user. example, addconfirmation dialogues improve current clarification dialogues, thus allowingtypes adaptation strategies, Singh et al. (2002). longer term investigation,plan extend adaptation techniques handle complex travel planning dialogues(Seneff, Lau, & Polifroni, 1999; Walker & Hirschman, 2000). may require additionsuser model, preferences regarding language dialogue style, includinginitiative, system verbosity, vocabulary. turn need appropriatelyacquired utilized system. general, insights already gainedutilizing acquiring user preferences different junctures dialogue searchprocess prove useful supporting personalization tasks.6.3 User Modelimprove user model, first plan add types preferences. discussedSection 3.1, combination diversity preferences capture complex user behaviorcurrent model, plan incorporate next versionsystem. Combination preferences help better predict either values acceptableattributes, based previously provided constraints. Place Advisor modelvalue combination preferences learning association rules (Agrawal, Imielinski, & Swami,1993) extending Bayesian network, either would influence query,turn influencing similarity calculation case base. preferencesacceptable attribute combinations, system learn conditional probabilities basedpast interactions use influence attribute ranking.418fiPersonalized Conversational Recommendationdrifting preferences likely cause problems item selection applications might ones like news updates, model could extended handlewithin-user diversity. One way capture users desired time interval suggestion particular item value. calculate determiningmean time interval users explicit selection rejection value (value diversitypreferences) item (item diversity preferences). incorporate diversitypreferences similarity calculation Section 3.2 extending RI P (Vj )equation incorporate time effects. define RD (I) PD (Vj ) as:1RD (I) = RI1+PD (Vj ) = P (Vj )ekI (ttI tID )1,1+current time, tI tV time item value last selected,tID tV time differences user wants itemvalue suggested again. RD PD form sigmoid function kI kVdetermine curves slope. One empirical question whether users also attributediversity preferences. hypothesize diversity preferences differ valueattribute, implicitly overrides attribute diversity. example, usermay different preferences frequency expensive restaurants versuscheap ones suggested, may care often questions price asked.plan investigate hypothesis.improvements might add user modeling technique. example, system may learn quickly updates user model dialogue situationscurrent three. Also, using collaborative user models initialize individualmodels could speed learning process. explicit combination collaborativeindividual user models (Melville, Mooney, & Nagarajan, 2002; Jameson & Wittig, 2001)also viable direction explore.ekV (ttV tV )6.4 EvaluationFinally, planning carry larger user study. must verifydifferences study due task difficulty differences since controldifficulty finding particular item. particular, different valuesconstraints may result number matching items. even though twousers answered number questions, number matching items oneuser may small enough system begin presenting them, usermay need answer additional questions first. support expanded evaluation,implemented version system recommends movies, let us drawbroader user base. help us measure user satisfaction easily, Walkeret al. (1998) noted efficiency important consideration,users might tend prefer predictable interfaces.419fiThompson, Goker, & Langley7. Conclusionspaper, described intelligent adaptive conversational assistant designed helppeople select item. Overall, made significant inroads methods unobtrusivelyacquiring individual, long term user model recommendation conversations.expanded previous work adaptive recommendation systems conversational, dialogue systems user adaptive. long-term goaldevelop even powerful methods, capable adapting users needs, goals,preferences multiple conversations. leveraged feedback conversation recommendation, feedback likely present tasksplanning scheduling.two key problems addressed research design adaptive recommendation systems conversations interaction mode, addition personalization dialogue systems, starting dialogues recommendation. Thus, unlikemany recommendation systems accept keywords produce ranked list, onecarries conversation user progressively narrow options. solvingproblems, introduced novel approach acquisition, use, representationuser models. Unlike many adaptive interfaces, system constructs utilizesuser models include information beyond complete item preferences. keysupport personalization conversations. used relatively simple model dialoguefocus issues involved personalization. also described experimental resultsshowing promise technique, demonstrating reduction numberinteractions conversation time users interacting adaptive systemcompared control group.course, still several open questions opportunities improvement.user model, conversational model, search models functional plan improvefurther. also extending conversational approach itemsdestinations, books movies, plan link system assistantslike Adaptive Route Advisor (Rogers et al., 1999). goal additionsprovide new functionality make Adaptive Place Advisor attractiveusers, also test generality approach adaptive recommendation.turn, bring us closer truly flexible computational aides carry naturaldialogues humans.Acknowledgmentsresearch carried first author Center StudyLanguage Information, Stanford University, authors DaimlerChrysler Research Technology Center Palo Alto, California. thank Renee Elio,Afsaneh Haddadi, Jeff Shrager initial conception design AdaptivePlace Advisor, Cynthia Kuo Zhao-Ping Tang help implementation effort,Stanley Peters enlightening discussions design conversational interfaces.Robert Mertens Dana Dahlstrom crucial carrying user studies.420fiPersonalized Conversational RecommendationAppendix A. Questionnaire1. think interaction system,01talkmuch2345rightamounttalking678enoughtalking2. easy find restaurant liked?01easy24easy3. system deliver restaurants liked?0yes1244. Please rate interaction system scale standard humancomputer-interaction person person conversation via telephone.012humancomputerinteraction3456phoneconversation5. think APA useful system?0yes12346. think conversation significantly distracting similar conversation real person?01234yes421fiThompson, Goker, & LangleyReferencesAamodt, A., & Plaza, E. (1994). Case-based reasoning: Foundational issues, methodologicalvariations, system approaches. Artificial Intelligence Communications, 7, 3959.Abella, A., Brown, M. K., & Buntschuh, B. (1996). Development principles dialog-basedinterfaces. Proceedings ECAI-96 Spoken Dialog Processing Workshop, pp.17. Budapest, Hungary.Agrawal, R., Imielinski, T., & Swami, A. (1993). Mining association rules setsitems large databases. Buneman, P., & Jajodia, S. (Eds.), Proceedings1993 ACM SIGMOD International Conference Management Data, pp. 207216.Washington, D.C. ACM Press.Aha, D., & Breslow, L. (1997). Refining conversational case libraries. ProceedingsSecond International Conference Case-Based Reasoning, pp. 267278. Providence,RI. Springer Verlag.Aha, D., Breslow, L., & Munoz Avila, H. M. (2001). Conversational case-based reasoning.Applied Intelligence, 14, 932.Allen, J. (1999). Mixed-initiative interaction. IEEE Intelligent Systems, September/October,1416.Allen, J., Byron, D., Dzikovska, M., Ferguson, G., Galescu, L., & Stent, A. (2001). Towardsconversational human-computer interaction. AI Magazine, 22, 2737.Allen, J., Schubert, L., Ferguson, G., Heeman, P., Hwang, C. H., Kato, T., Light, M.,Martin, N., Miller, B., Poesio, M., & Traum, D. (1995). TRAINS project:case study building conversational planning agent. Journal ExperimentalTheoretical AI, 7, 748.Allen, J. F. (1995). Natural language understanding (second edition). Benjamin/Cummings,Menlo Park, CA.Ardissono, L., & Goy, A. (2000). Tailoring interaction users web stores. UserModeling User-Adapted Interaction, 10, 251303.Ardissono, L., Goy, A., Console, L., & Torre, I. (2001). adaptive system personalized access news. AI Communications, 14, 129147.Billsus, D., & Pazzani, M. (1998). Learning collaborative information filters. ProceedingsFifteenth International Conference Machine Learning, pp. 4654. Madison,WI. Morgan Kaufmann.Bobrow, D., Kaplan, R., Kay, M., Norman, D., Thompson, H., & Winograd, T. (1977).Gus, frame driven dialog system. Artificial Intelligence, 8, 155173.422fiPersonalized Conversational RecommendationBonzano, A., Cunningham, P., & Smyth, B. (1997). Using introspective learning improveretrieval CBR: case study air traffic control. Proceedings Second International Conference Case-Based Reasoning, pp. 413424. Providence, RI. SpringerVerlag.Brusilovsky, P., & Maybury, M. (2002). Introduction special section adaptive web.Communications ACM, 45, 3033.Burke, R. (1999). Wasabi personal shopper: case-based recommender system.Proceedings Sixteenth National Conference Artificial Intelligence, pp. 844849. Orlando, FL. AAAI Press.Burke, R., Hammond, K., & Young, B. (1996). Knowledge-based navigation complexinformation spaces. Proceedings Thirteenth National Conference ArtificialIntelligence, pp. 462468. Portland, OR. AAAI Press.Carberry, S. (1990). Plan recognition natural language dialogue. MIT Press, Cambridge,MA.Carberry, S., Chu-Carroll, J., & Elzer, S. (1999). Constructing utilizing modeluser preferences collaborative consultation dialogues. Computational IntelligenceJournal, 15, 185217.Chin, D. (1989). KNOME: Modeling user knows UC. Kobsa, A., & Wahlster,W. (Eds.), User models dialog systems, pp. 74107. Springer Verlag, Berlin.Chu-Carroll, J. (2000). MIMIC: adaptive mixed initiative spoken dialogue systeminformation queries. Proceedings Sixth Conference Applied Natural Language Processing, pp. 97104. Seattle, WA. AAAI Press.Cohen, P. R., & Perrault, C. (1979). Elements plan-based theory speech acts.Cognitive Science, 3, 177212.Cohen, W., Schapire, R., & Singer, Y. (1999). Learning order things. Journal ArtificialIntelligence Research, 10, 243270.Cotter, P., & Smyth, B. (2000). PTV: Intelligent personalized TV guides. ProceedingsTwelfth Innovative Applications Artificial Intelligence Conference, pp. 957964.Austin, TX. AAAI Press.Cucchiara, R., Lamma, E., Mello, P., & Milano, M. (1997). Interactive constraint satisfaction. Tech. rep. DEIS-LIA-97-00, University Bologna.Dowding, J., Gawron, J., Appelt, D., Bear, J., Cherny, L., Moore, R., & Moran, D. (1993).Gemini: natural language system spoken-language understanding. Proceedings Thirty-first Annual Meeting Association Computational Linguistics, pp. 5461. Columbus, OH. Association Computational Linguistics.Dybkjr, L., Hasida, K., & Traum, D. (Eds.). (2000). Proceedings 1st SIGdialWorkshop Discourse Dialogue, Hong Kong. Association ComputationalLinguistics.423fiThompson, Goker, & LangleyEaton, P., Freuder, E., & Wallace, R. (1997). Constraint-based agents: Assistance, cooperation, compromise. Proceedings CP97 Workshop Constraint ReasoningInternet. Schloss Hagenberg, Austria.Eliassi-Rad, T., & Shavlik, J. (2001). system building intelligent agents learnretrieve extract information. User Modeling User-Adapted Interaction, 13,3588.Elio, R., & Haddadi, A. (1998). Dialog management adaptive database assistant.Tech. rep. 98-3, Daimler-Benz research Technology Center, Palo Alto, CA.Elio, R., & Haddadi, A. (1999). abstract task models conversation policies.Proceedings Agents99 Workshop Specifying Implementing ConversationPolicies. Seattle, WA.Ferrario, M., Waters, K., & Smyth, B. (2000). Collaborative maintenance ULYSSES.Proceedings International Conference Adaptive Hypermedia AdaptiveWeb-based Systems, pp. 301304. Trento, Italy.Fiechter, C., & Rogers, S. (2000). Learning subjective functions large margins.Proceedings Seventeenth International Conference Machine Learning, pp.287294. Stanford University, CA. Morgan Kaufmann.Goddeau, D., Meng, H., Polifroni, J., Seneff, S., & Busayapongchai, S. (1996). form-baseddialogue manager spoken language applications. Proceedings Fourth International Conference Spoken Language Processing, Vol. 2, pp. 701704. Philadelphia, PA.Goecks, J., & Shavlik, J. (2000). Learning users interests unobtrusively observingnormal behavior. Proceedings 2000 International Conference IntelligentUser Interfaces, pp. 129132. New Orleans, LA. ACM Press.Goker, M., & Roth-Berghofer, T. (1999). development utilization case-basedhelp-desk support system HOMER. Engineering Applications Artificial Intelligence,12, 665680.Goker, M., & Thompson, C. (2000). Personalized, conversational case-based recommendation. Proceedings Fifth European Workshop Case-Based Reasoning, pp.99111. Trento Italy. Springer Verlag.Haller, S., & McRoy, S. (1998). Preface special issue computational models mixedinitiative interaction. User Modeling User-Adapted Interaction, 8, 167170.Horvitz, E., & Paek, T. (2001). Harnessing models users goals mediate clarification dialog spoken language systems. Proceedings Eighth InternationalConference User Modeling, pp. 201210. Sonthofen, Germany. Springer.Jameson, A., Kipper, B., Ndiaye, A., Schafer, R., Simons, J., Weis, T., & Zimmermann,D. (1994). Cooperating noncooperative: dialog system PRACMA.424fiPersonalized Conversational RecommendationProceedings KI-94: Advances Artificial Intelligence, pp. 106117. Seattle, WA.Morgan Kaufmann.Jameson, A., & Wittig, F. (2001). Leveraging data users general learning individual user models. Proceedings Seventeenth International JointConference Artificial Intelligence, pp. 11851192. Seattle, WA. Morgan Kaufmann.Jurafsky, D., & Martin, J. (2000). Speech language processing. Prentice Hall.Jurafsky, D., Wooters, C., Tajchman, G., Segal, J., Stolcke, A., Fosler, E., & Morgan, N.(1994). Berkeley restaurant project. Proceedings International Conference Spoken Language Processing, pp. 21392142. Yokohama, Japan.Kass, R. (1991). Building user model implicitly cooperative advisory dialog. UserModeling User-Adapted Interaction, 3, 203258.Kay, J., & Thomas, R. C. (2000). Personal usability based upon scrutable, dynamic,individual user model. Proceedings Australasian Computer Human InterfacesConference, pp. 292298.Kobsa, A., & Wahlster, W. (Eds.). (1989). User models dialog systems. Springer, NewYork.Konstan, J., Miller, B., Maltz, D., Herlocker, J., Gordon, L., & Riedl, J. (1997). Grouplens:Applying collaborative filtering usenet news. Communications ACM, 40,7787.Kumar, V. (1992). Algorithms constraint-satisfaction problems: survey. AIMagazine, 13, 3244.Lang, K. (1995). NewsWeeder: Learning filter netnews. Proceedings TwelfthInternational Conference Machine Learning, pp. 331339. San Francisco, CA. Morgan Kaufmann.Langley, P. (1999). User modeling adaptive interfaces. Proceedings SeventhInternational Conference User Modeling, pp. 357370. Banff, Alberta. Springer.Levin, E., Pieraccini, R., & Eckert, W. (2000). stochastic model human-machineinteraction learning dialog strategies. IEEE Transactions Speech AudioProcessing, 8, 1123.Linden, G., Hanks, S., & Lesh, N. (1997). Interactive assessment user preference models:automated travel assistant. Proceedings Sixth International ConferenceUser Modeling, pp. 6778. Chia Laguna, Sardinia. Springer.Litman, D., & Pan, S. (2002). Designing evaluating adaptive spoken dialogue system.User Modeling User-Adapted Interaction, 12, 111137.Maier, E., Mast, M., & Luperfoy, S. (Eds.). (1996). Proceedings ECAI96 workshopDialogue processing spoken language systems, Budapest, Hungary. Springer Verlag.425fiThompson, Goker, & LangleyMaloor, P., & Chai, J. (2000). Dynamic user level utility measurement adaptivedialog help-desk system. Proceedings 1st SIGdial Workshop DiscourseDialogue, pp. 94101 Hong Kong. Association Computational Linguistics.McNee, S., Lam, S., Konstan, J., & Riedl, J. (2003). Interfaces eliciting new user preferences recommender systems. Proceedings Ninth International ConferenceUser Modeling, pp. 178188. Johnstown, PA. Springer.Melville, P., Mooney, R., & Nagarajan, R. (2002). Content-boosted collaborative filteringimproved recommendations. Proceedings Eighteenth National ConferenceArtificial Intelligence, pp. 187192. Edmonton, Canada. AAAI Press.Pazzani, M., Muramatsu, J., & Billsus, D. (1996). Syskill & Webert: Identifying interesting web sites. Proceedings Thirteenth National Conference ArtificialIntelligence, pp. 5461. Portland, OR. AAAI Press.Pieraccini, R., Levin, E., & Eckert, W. (1997). AMICA: AT&T mixed initiativeconversational architecture. Proceedings European Conference SpeechCommunication Technology, pp. 18751878. Rhodes, Greece.Qu, Y., & Beale, S. (1999). constraint-based model cooperative response generationinformation dialogues. Proceedings Sixteenth National ConferenceArtificial Intelligence, pp. 148155. Orlando, FL. AAAI Press.Rafter, R., Bradley, K., & Smyth, B. (2000). Personalized retrieval online recruitmentservices. Proceedings Twenty-second Annual Colloquium InformationRetrieval. Cambridge, UK.Raskutti, B., & Zukerman, I. (1997). Generating queries replies informationseeking interactions. International Journal Human Computer Studies, 47, 689734.Resnick, P., & Varian, H. (1997). Recommender systems. Communications ACM,40 (3), 5658.Rich, E. (1979). User modeling via stereotypes. Cognitive Science, 3, 329354.Rogers, S., Fiechter, C., & Langley, P. (1999). adaptive interactive agent routeadvice. Proceedings Third International Conference Autonomous Agents,pp. 198205. Seattle, WA. ACM Press.Sadek, M., Bretier, P., & Panaget, F. (1997). ARTIMIS: Natural dialogue meets rationalagency. Proceedings Fifteenth International Joint Conference ArtificialIntelligence, pp. 10301035. Nagoya, Japan. Morgan Kaufmann.Segal, R., & Kephart, J. (1999). Mailcat: intelligent assistant organizing e-mail.Proceedings Third International Conference Autonomous Agents, pp. 276282. Seattle, WA. ACM Press.426fiPersonalized Conversational RecommendationSeneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., & Zue, V. (1996). Galaxy-II:reference architecture conversational system development. ProceedingsInternational Conference Spoken Language Processing, pp. 931934. Sydney, Australia.Seneff, S., Lau, R., & Polifroni, J. (1999). Organization, communication, controlGalaxy-II conversational system. Proceedings Eurospeech 1999, pp. 12711274.Budapest, Hungary.Shardanand, U., & Maes, P. (1995). Social information filtering: Algorithms automatingword mouth. Proceedings Conference Human Factors ComputingSystems, pp. 210217. Denver, CO. ACM Press.Shearin, S., & Lieberman, H. (2001). Intelligent profiling example. ProceedingsInternational Conference Intelligent User Interfaces, pp. 145152. Santa Fe, NM.ACM Press.Singh, S., Litman, D., Kearns, M., & Walker, M. (2002). Optimizing dialogue management reinforcement learning: Experiments NJFun system. JournalArtificial Intelligence Research, 16, 105133.Smith, R., & Hipp, D. (1994). Spoken natural language dialog systems: practical approach.Oxford University Press, New York, NY.Smyth, B., & Cotter, P. (1999). Surfing digital wave, generating personalized TV listingsusing collaborative, case-based recommendation. Proceedings Third International Conference Case-Based Reasoning, pp. 561571. Monastery, Germany.Springer Verlag.Stent, A., Dowding, J., Gawron, J., Bratt, E., & Moore, R. (1999). CommandTalkspoken dialogue system. Proceedings Thirty-seventh Annual MeetingAssociation Computational Linguistics, pp. 183190. College Park, MD. Association Computational Linguistics.Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin,R., Ess-Dykema, C. V., & Meteer, M. (2000). Dialog act modeling automatictagging recognition conversational speech. Computational Linguistics, 26, 339373.Walker, M., Fromer, J., Fabbrizio, G., Mestel, C., & Hindle, D. (1998). say?:Evaluating spoken language interface email. Proceedings ACM CHI 98Conference Human Factors Computing Systems, pp. 582589. Los Angeles, CA.ACM Press.Walker, M., & Hirschman, L. (2000). Evaluation DARPA communicator spoken dialoguesystems. Proceedings Second International Conference Language ResourcesEvaluation. Athens, Greece.427fiThompson, Goker, & LangleyWard, W., & Issar, S. (1996). Recent improvements CMU spoken language understanding system. Proceedings ARPA Human Language Technology Workshop,pp. 213216.Wettschereck, D., & Aha, D. (1995). Weighting features. Proceedings First International Conference Case-Based Reasoning, pp. 347358. Sesimbra, Portugal.Springer Verlag.Winograd, T., & Flores, F. (1986). Understanding computers cognition: new foundation design. Ablex Publishing, Northwood, NJ.Zhang, Z., & Yang, Q. (1998). Towards lifetime maintenance case base indexescontinual case based reasoning. Proceedings 1998 International ConferenceAI Methodologies, Systems Applications, pp. 489500. Bulgaria. Springer.Zukerman, I., & Litman, D. (2001). Natural language processing user modeling: Synergies limitations. User Modeling User-Adapted Interaction, 11, 129158.428fiJournal Artificial Intelligence Research 21 (2004) 245286Submitted 07/2003; published 03/2004Coherent Integration DatabasesAbductive Logic ProgrammingOfer Arielioarieli@mta.ac.ilDepartment Computer Science, Academic College Tel-Aviv,4 Antokolski street, Tel-Aviv 61161, Israel.Marc DeneckerBert Van NuffelenMaurice BruynoogheMarc.Denecker@cs.kuleuven.ac.beBert.VanNuffelen@cs.kuleuven.ac.beMaurice.Bruynooghe@cs.kuleuven.ac.beDepartment Computer Science, Katholieke Universiteit Leuven,Celestijnenlaan 200A, B-3001 Heverlee, Belgium.Abstractintroduce abductive method coherent integration independent datasources. idea compute list data-facts inserted amalgamated database retracted order restore consistency. methodimplemented abductive solver, called Asystem, applies SLDNFA-resolutionmeta-theory relates different, possibly contradicting, input databases. alsogive pure model-theoretic analysis possible ways recover consistent datainconsistent database terms models database exhibit minimalinconsistent information reasonably possible. allows us characterize recovered databases terms preferred (i.e., consistent) models theory.outcome abductive-based application sound complete respectcorresponding model-based, preferential semantics, best knowledgeexpressive (thus general) implementation coherent integrationdatabases.1. IntroductionComplex reasoning tasks often integrate information coming differentsources. One major challenges respect compose contradicting sourcesinformation obtained would properly reflect combination datasources one hand1 , would still coherent (in terms consistency)hand. number different issues involved process, importantfollowing:1. Unification different ontologies and/or database schemas, order get fixed(global) schema, translation integrity constraints2 databasenew ontology.2. Unification translated integrity constraints single global set integrity constraints. means, particular, elimination contradictions among translated1. property sometimes called compositionality (Verbaeten, Denecker, & De Schreye, 1997, 2000).2. I.e., rules represent intentional truths database domain.c2004AI Access Foundation. rights reserved.fiArieli, Denecker, Van Nuffelen, & Bruynoogheintegrity constraints, inclusion global integrity constraint imposedintegration process.3. Integration databases w.r.t. unified set integrity constraints, computed according previous item.one issues mentioned difficulties challenges.instance, first issue considered, e.g., Ullman (2000) Lenzerini (2001, 2002),questions express relations global database schemasource (local) schemas, influences query processing respectglobal schema (Bertossi, Chomicki, Cortes, & Gutierrez, 2002), dealt with3 .second issue concerned construction single, classically consistent, set integrity constraints, applied integrated data. database context,common assume set pre-defined, consists global integrity constraints imposed integration process (Bertossi et al., 2002; Lenzerini,2002). case need derive constraints local databases.different integrity constraints specified different local databases, requiredintegrate database instances (as specified issue 3 above), alsointegrity constraints (issue 2). reason separating two topicsintegrity constraints represent truths valid situations, databaseinstance exhibits extensional truth, i.e., actual situation. Consequently, policyresolving contradictions among integrity constraints often different oneapplied database facts, often former applied latter.Despite different nature, issues based formalisms maintain contradictions allow draw plausible conclusions inconsistent situations.Roughly, two approaches handle problem:Paraconsistent formalisms, amalgamated data may remain inconsistent,set conclusions implied explosive, i.e.: every fact followsinconsistent database, inference process become trivialpresence contradictions. Paraconsistent procedures integrating data, likeSubrahmanian (1994) de Amo, Carnielli, Marcos (2002), oftenbased paraconsistent reasoning systems, LFI (Carnielli & Marcos, 2001),annotated logics (Subrahmanian, 1990; Kifer & Lozinskii, 1992; Arenas, Bertossi, &Kifer, 2000), non-classical proof procedures (Priest, 1991; Arieli & Avron,1996; Avron, 2002; Carnielli & Marcos, 2002)4 .Coherent (consistency-based) methods, amalgamated data revisedorder restore consistency (see, e.g., Baral, Kraus, & Minker, 1991; Baral, Kraus,Minker, & Subrahmanain, 1992; Benferhat, Dubois, & Prade, 1995; Arenas, Bertossi,3. surveys schema matching related aspects, see also (Batini, Lenzerini, & Navathe, 1986)(Rahm & Bernstein, 2001).4. See also (Decker, 2003) historical perspective computational remarks kindformalisms.246fiCoherent integration databases abductive logic programming& Chomicki, 1999; Arieli & Avron, 1999; Greco & Zumpano, 2000; Liberatore &Schaerf, 2000; Bertossi & Schwind, 2002; Arieli, Denecker, Van Nuffelen, & Bruynooghe,2004). many cases underlying formalisms approaches closely related theory belief revision (Alchourron, Gardenfors, & Makinson, 1995;Gardenfors & Rott, 1995). context database systems idea considerconsistent databases close possible original database. repaired instances spoiled database correspond plausible compact waysrestoring consistency.paper follow latter approach, consider abductive approaches handle third issue above, namely: coherent methods integrating different data-sources(with ontology) w.r.t. consistent set integrity constraints5 . main difficulty process stems fact even local database consistent,collective information data-sources may remain consistent anymore.particular, facts specified particular database may violate integrity constraints defined elsewhere, data might contradict elements unifiedset integrity constraints. Moreover, noted e.g. (Lenzerini, 2001; Cali, Calvanese,De Giacomo, & Lenzerini, 2002), ability handle, plausible way, incompleteinconsistent data, inherent property system data integration integrityconstrains, matter integration phase considered. Providing proper ways gaining property major concern well.goal therefore find ways properly repair combined (unified) database,restore consistency. this, consider pure declarative representationcomposition distributed data meta-theory, relating number different inputdatabases (that may contradict other) consistent output database. underlying language theory abductive logic programming (Kakas, Kowalski,& Toni, 1992; Denecker & Kakas, 2000). reasoning theories use abductive system, called Asystem (Kakas, Van Nuffelen, & Denecker, 2001; Van Nuffelen &Kakas, 2001), abductive solver implementing SLDNFA-resolution (Denecker &De Schreye, 1992, 1998). composing system implemented abductive reasoningmeta-theory. context work, extended systemoptimizing component allows us compute preferred coherent ways restoreconsistency given database. system obtained induces operational semantics database integration. sequel also consider model-theoretic aspectsproblem, define preferential semantics (Shoham, 1988) it. Accordingsemantics, repaired databases characterized terms preferred models(i.e., most-consistent valuations) underlying theory. relate approachesshowing Asystem sound complete w.r.t. model-based semantics.also noted framework supports reasoning various types special information,timestamps source identification. implementation issues experimen5. sense, one may view work method restoring consistency single inconsistentdatabase. prefer, however, treat integration process multiple sources, since alsomediating capabilities, source identification, making priorities among different data-sources,etc. (see, e.g., Section 4.6).247fiArieli, Denecker, Van Nuffelen, & Bruynooghetal results discussed well.rest paper organized follows: next section formally definegoal, namely: coherent way integrate different data-sources. Section 3 setsemantics goal terms corresponding model theory. Then, Section 4introduce abductive-based application database integration. mainsection paper, also describe given integration problemrepresented terms meta logic programs, show reason programsabductive computational models, present experimental results, consider proper waysreasoning several types special data, show application soundcomplete respect model-based semantics, considered Section 3. Section 5contains overview related works, Section 6 concluderemarks, open issues, future work6 .2. Coherent Integration Databasesbegin formal definition goal. paper assumefirst-order language L, based fixed database schema S, fixed domain D. Everyelement unique name. database instance consists atoms languageL instances schema S. such, every instance finite active domain,subset D.Definition 1 database pair (D, IC), database instance, IC,set integrity constraints, finite classically consistent set formulae L.Given database DB = (D, IC), apply closed word assumption,facts explicitly mentioned considered true. underlying semanticscorresponds, therefore, minimal Herbrand interpretations.Definition 2 minimal Herbrand model HD database instance modelassigns true ground instances atomic formulae D, falseatoms.different views database. One view logic theory consistingatoms and, implicitly, closed world assumption (CWA) indicates atomsdatabase false. Another common view database structureconsists certain domain corresponding relations, representing stateworld. Whenever complete knowledge true atoms representeddatabase, views coincide: unique Herbrand model theory intendedstructure. However, context independent data-sources, assumptionlocal database represents state world obviously false. However, stillview local database incomplete theory, treating database theory ratherstructure appropriate case.6. combined extended version (Arieli, Van Nuffelen, Denecker, & Bruynooghe, 2001)(Arieli, Denecker, Van Nuffelen, & Bruynooghe, 2002).248fiCoherent integration databases abductive logic programmingDefinition 3 formula follows database instance (alternatively, entails ;notation: |= ) minimal Herbrand model also model .Definition 4 database DB = (D, IC) consistent every formula IC follows(notation: |= IC).goal integrate n consistent local databases, DB = (Di , IC ) (i = 1, . . . n) oneconsistent database contains much information possible local databases.idea, therefore, consider union distributed data, restoreconsistency way much information possible preserved.Notation 1 Let DB = (Di , IC ), = 1, . . . n, let I(IC 1 , . . . , IC n ) classicallyconsistent set integrity constraints. denote:UDB = (n[Di , I(IC 1 , . . . , IC n )).i=1notation above, operator combines integrity constraints eliminates contradictions (see, e.g., Alferes, Leite, Pereira, & Quaresma, 2000; Alferes, Pereira,Przymusinska, & Przymusinski, 2002). already noted, choose operator apply specific database beyond scope paper. casesunion integrity constraints classically consistent, makes sense takeunion operator. Global consistency integrity constraints indeed commonassumption database literature (Arenas et al., 1999; Greco & Zumpano, 2000; Greco,Greco, & Zumpano, 2001; Bertossi et al., 2002; Konieczny & Pino Perez, 2002; Lenzerini,2002), discussion possible take, instead union, operatorconsistency restoration.key notion database integration following:Definition 5 repair database DB = (D, IC) pair (Insert, Retract), that:1. Insert = ,2. Retract D7 ,3. (D Insert \ Retract, IC) consistent database.Intuitively, Insert set elements inserted Retract setelements removed order consistent database.noted above, repair given database key notion many formalismsdata integration. context database systems, notion first introducedArenas, Bertossi, Chomicki (1999), later considered many others (e.g., Greco& Zumpano, 2000; Liberatore & Schaerf, 2000; Franconi, Palma, Leone, Perri, & Scarcello,2001; Bertossi et al., 2002; Bertossi & Schwind, 2002; de Amo et al., 2002; Arenas, Bertossi,& Chomicki, 2003; Arieli et al., 2004). Earlier versions repairs inclusion-basedconsistency restoration may traced back Dalal (1988) Winslett (1988).7. Note conditions (1) (2) follows Insert Retract = .249fiArieli, Denecker, Van Nuffelen, & BruynoogheDefinition 6 repaired database DB = (D, IC) consistent database form(D Insert \ Retract , IC), (Insert, Retract) repair DB.may many ways repair inconsistent database8 , often convenientmake preferences among possible repairs, consider preferred ones.two common preference criteria preferring repair (Insert, Retract)repair (Insert0 , Retract0 ):Definition 7 Let (Insert, Retract) (Insert0 , Retract0 ) two repairs given database.set inclusion preference criterion : (Insert0 , Retract0 ) (Insert, Retract),Insert Insert0 Retract Retract0 .minimal cardinality preference criterion: (Insert0 , Retract0 ) c (Insert, Retract),|Insert| + |Retract| |Insert0 | + |Retract0 |.Set inclusion also considered (Arenas et al., 1999; Greco & Zumpano, 2000; Bertossiet al., 2002; Bertossi & Schwind, 2002; de Amo et al., 2002; Arenas et al., 2003; Arieli et al.,2004, others), minimal cardinality considered, e.g., (Dalal, 1988; Liberatore &Schaerf, 2000; Arenas et al., 2003; Arieli et al., 2004).follows assume preference relation fixed pre-orderrepresents preference criterion set repairs (and shall omit subscriptnotations whenever possible). shall also assume (, ) valid repair,-least (i.e., best) one. corresponds intuition databaserepaired unless inconsistent.Definition 8 -preferred repair DB repair (Insert, Retract) DB, s.t. everyrepair (Insert0 , Retract0 ) DB, (Insert, Retract) (Insert0 , Retract0 ) (Insert0 , Retract0 )(Insert, Retract). set -preferred repairs DB denoted !(DB, ).Definition 9 -repaired database DB repaired database DB, constructed-preferred repair DB. set -repaired databases denoted by:R(DB, ) = { (D Insert \ Retract , IC) | (Insert, Retract) !(DB, ) }.Note DB consistent preference relation, DB repaired database (thus, nothing repair case, expected).Note 1 usual refer -preferred databases DB consistent databasesclose possible DB (see, e.g., Arenas, Bertossi, & Chomicki, 1999;Liberatore & Schaerf, 2000; de Amo, Carnielli, & Marcos, 2002; Konieczny & Pino Perez,2002; Arenas, Bertossi, & Chomicki, 2003; Arieli, Denecker, Van Nuffelen, & Bruynooghe,2004). Indeed, letdist(D1 , D2 ) = (D1 \ D2 ) (D2 \ D1 ).8. repairs may trivial and/or useless, though. instance, one way eliminate inconsistency(D, IC) = ({p, q, r}, {p}) deleting every element D, certainly optimal wayrestoring consistency case.250fiCoherent integration databases abductive logic programmingeasy see DB 0 = (D0 , IC) -repaired database DB = (D, IC), setdist(D0 , D) minimal (w.r.t. set inclusion) among sets form dist(D00 , D),D00 |= IC. Similarly, |S| denotes size S, DB 0 = (D0 , IC) c -repaireddatabase DB = (D, IC), |dist(D0 , D)| = min{|dist(D00 , D)| | D00 |= IC}.Given n databases preference criterion , goal therefore compute setR(UDB, ) -repaired databases unified database, UDB (Notation 1).reasoner may use different strategies determine consequences set. Amongcommon approaches skeptical (conservative) one, based consensusamong elements R(UDB, ) (see Arenas et al., 1999; Greco & Zumpano, 2000),credulous approach, entailments determined element R(UDB, ),approach based majority vote (Lin & Mendelzon, 1998; Konieczny &Pino Perez, 2002), etc. cases processing time major consideration, one maywant speed-up computations considering repaired database. casessufficient find arbitrary element set R(UDB, ).examples9 integration process10 .Example 1 Consider relation teaches schema (course name, teacher name),integrity constraint, stating course cannot taught two differentteachers:IC = { XY Z (teaches(X, ) teaches(X, Z) = Z) }.Consider following two databases:DB 1 = ( {teaches(c1 , n1 ), teaches(c2 , n2 )}, IC ),DB 2 = ( {teaches(c2 , n3 )}, IC).Clearly, unified database DB 1 DB 2 inconsistent. two preferred repairs,(, {teaches(c2 , n3 )}) (, {teaches(c2 , n2 )}). corresponding repaired databasesfollowing:R1 = ( {teaches(c1 , n1 ), teaches(c2 , n2 )}, IC ),R2 = ( {teaches(c1 , n1 ), teaches(c2 , n3 )}, IC ).Thus, e.g., teaches(c1 , n1 ) true conservative approach credulousapproach database integration, conclusion teaches(c2 , n2 ) supportedcredulous reasoning.Example 2 Consider databases relations class supply, schemas (item, type)(supplier, department, item), respectively. LetDB 1 = ({supply(c1 , d1 , i1 ), class(i1 , t1 )}, IC),DB 2 = ({supply(c2 , d2 , i2 ), class(i2 , t1 )}, ),IC = { XY Z (supply(X, Y, Z)class(Z, t1 ) X = c1 ) } states supplier9. See, e.g., (Arenas et al., 1999; Greco & Zumpano, 2000; Bertossi & Schwind, 2002) discussionsexamples.10. following examples use set inclusion preference criterion, take operatorcombines integrity constraints (see Notation 1) union operator.251fiArieli, Denecker, Van Nuffelen, & Bruynooghec1 supply items type t1 . Again, DB 1 DB 2 inconsistent, two preferredrepairs: (, {supply(c2 , d2 , i2 )}) (, {class(i2 , t1 )}). follows two repairsdatabase:R1 = ( {supply(c1 , d1 , i1 ), class(i1 , t1 ), class(i2 , t1 )}, IC ),R2 = ( {supply(c1 , d1 , i1 ), supply(c2 , d2 , i2 ), class(i1 , t1 )}, IC ).Example 3 Let D1 = {p(a), p(b)}, D2 = {q(a), q(c)}, IC = {X(p(X) q(X))}. Again,(D1 , ) (D2 , IC) inconsistent. corresponding preferred repairs ({q(b)}, )(, {p(b)}). Thus, repaired databases following:R1 = ( {p(a), p(b), q(a), q(b), q(c)}, IC ),R2 = ( {p(a), q(a), q(c)}, IC ).case, then, consensus approach credulous approach allowinfer, e.g., p(a) holds, p(b) supported credulous reasoning, p(c)supported either approaches.3. Model-based Characterization Repairssection set semantics describing repairs preferred repairs termscorresponding model theory. allow us, particular, give alternativedescription preferred repairs, time terms preferential semantics databasetheory.database semantics usually defined terms two-valued (Herbrand) models(cf. Definition 2 discussion proceeds it), natural consider two-valuedsemantics first. show arbitrary repairs represented two-valued modelsintegrity constraints. database inconsistent, definition,two-valued interpretation satisfies database instance integrityconstraints. standard way cope type inconsistencies move multiplevalued semantics reasoning inconsistent incomplete information (see, e.g.,Subrahmanian, 1990, 1994; Messing, 1997; Arieli & Avron, 1999; Arenas, Bertossi, & Kifer,2000; de Amo, Carnielli, & Marcos, 2002). show below, repairscharacterized three-valued models whole database, is, database instanceintegrity constraints. Finally, concentrate preferred repairs,show certain subset three-valued models used characterizing preferred repairs.Definition 10 Given valuation truth value x. Denote:x = {p | p atomic formula, (p) = x}11 .following two propositions characterize repairs terms two-valued structures.Proposition 1 Let (D, IC) database let two-valued model IC. LetInsert = \ Retract = \ . (Insert, Retract) repair (D, IC).11. Note, particular, terms Definition 2, = HD x = t, x = D.252fiCoherent integration databases abductive logic programmingProof: definitions Insert Retract immediately imply Insert =Retract D. last condition Definition 5, note Insert \ Retract =(M \D)\(D\M ) = . follows least Herbrand model DInsert\Retractalso model IC, therefore Insert \ Retract |= IC.2Proposition 2 Let (Insert, Retract) repair database (D, IC).two-valued model IC Insert = \ Retract = \ .Proof: Consider valuation , defined every atom p follows:((p) =p Insert \ Retract,fotherwise.definition, minimal Herbrand model Insert \ Retract. Now, since(Insert, Retract) repair (D, IC), Insert \ Retract |= IC, thus(two-valued) model IC. Moreover, since (Insert, Retract) repair, necessarilyInsert = Retract D, hence following:\ = (D Insert \ Retract) \ = Insert,2\ = \ (D Insert \ Retract) = Retract.database inconsistent, models satisfy integrity constraints database instance. One common method overcome inconsistencyintroduce additional truth-values intuitively represent partial knowledge, differentamounts beliefs, etc. (see, e.g., Priest, 1989, 1991; Subrahmanian, 1990; Fitting, 1991;Arieli, 1999; Arenas et al., 2000; Avron, 2002). follow guideline, considerdatabase integration context three-valued semantics. benefit that,show below, database three-valued models, possiblepinpoint inconsistent information, accordingly construct repairs.underlying three-valued semantics considered induced algebraic structure HREE, shown double-Hasse diagram Figure 112 .k6>u@@@@@@uuf-tFigure 1: structure HREEIntuitively, elements f HREE correspond usual classical valuestrue false, third element, >, represents inconsistent information (or belief).12. structure used reasoning inconsistency several three-valued formalisms,LFI (Carnielli & Marcos, 2001, 2002) LP (Priest, 1989, 1991).253fiArieli, Denecker, Van Nuffelen, & BruynoogheViewed horizontally, HREE complete lattice. According view, f minimalelement, maximal one, > intermediate element. corresponding order relation, , intuitively represents differences amount truth elementexhibits. denote meet, join, order reversing operation , ,(respectively). Viewed vertically, HREE semi-upper lattice. view, >maximal element two classical values incomparable. partial order,denoted k , may intuitively understood representing differences amountknowledge (or information) element represents13 . denote joinoperation k 14 .Various semantic notions defined HREE natural generalizations similarclassical ones: valuation function assigns truth value HREEatomic formula. Given valuation , truth values xi {t, f, >}, atomic formulae pi ,shall sometimes write = {pi : xi } instead (pi ) = xi (i = 1, 2 . . .). valuationextended complex formulae obvious way. instance, () = (), ( ) =() (), forth15 . set designated truth values HREE (i.e.,elements HREE represent true assertions) consists >. valuationsatisfies formula iff () designated. valuation assigns designated valueevery formula theory (three-valued) model .Lemma 1 Let three-valued valuations s.t. every atom p, (p) k (p).every formula , () k ().2Proof: induction structure .shall write k , three-valued valuations, conditionLemma 1 holds.Lemma 2 k model theory , also model .Proof: every formula , () designated. Hence, Lemma 1, every formula() also designated, model .2Next characterize repairs database DB three-valued models:Proposition 3 Let (D, IC) database let two-valued model IC. Considerthree-valued valuation N , defined every atom p N (p) = HD (p) (p), letInsert = N > \ D, Retract = N > D. Then:1. N three-valued model IC,13. See (Belnap, 1977; Ginsberg, 1988; Fitting, 1991) detailed discussion ordersintuitive meaning.14. follow notations Fitting (1990, 1991).15. usual, use logical symbol denote connective appear left-handside equation, corresponding operator HREE appear right-hand sideequation.254fiCoherent integration databases abductive logic programming2. (Insert, Retract) repair (D, IC).Proposition 3 shows repairs database (D, IC) may constructed standard(uniform) way considering three-valued models k -least upper boundstwo two-valued valuations: minimal Herbrand model database instance,two-valued model integrity constraints. Proposition 4 shows repair(D, IC) form.give proof Proposition 3, lets demonstrate simple example.Example 4 Let DB = ({p, r} , {p q}). HD = {p : t, q : f, r : t}, two-valuedmodels IC = {p q} {p : t, q : t, r : t}, {p : t, q : t, r : f }, {p : f, q : t, r : t}, {p : f, q : t, r : f },{p : f, q : f, r : t}, {p : f, q : f, r : f }. Thus, (three-valued) models form HD ,two-valued model IC, {p : t, q : >, r : t}, {p : t, q : >, r : >}, {p : >, q : >, r : t},{p : >, q : >, r : >}, {p : >, q : f, r : t} {p : >, q : f, r : >}. Proposition 3, pairs({q}, {}), ({q}, {r}), ({q}, {p}), ({q}, {p, r}), ({}, {p}), ({}, {p, r}), repairs DB.Proof Proposition 3: Since definition N , N k HD , since HD modelD, Lemma 2 implies N model D. Similarly, N k , model IC,thus lemma N also model IC.second part, observe Insert = N > \ = \ Retract = N > =f= \ . Now, two-valued model IC hence, Proposition 1,(Insert, Retract) repair (D, IC).2Note specific form three-valued valuations considered Proposition 3essential here, proposition hold every three-valued model IC.see consider, e.g., = {}, IC = {p , p q}, three valued valuation Nassigns > p q. Clearly, N model IC, corresponding update,(N > \ , N > D) = ({p}, {}) repair (D, IC), since ({p}, IC) consistentdatabase.Again, noted above, possible show converse Proposition 3also true:Proposition 4 Let (Insert, Retract) repair database (D, IC).three-valued model N IC, Insert = N > \ Retract = N > D.Proof: Consider valuation N , defined every atom p follows:N (p) =>p Insert Retract,p 6 Insert Retract p D,fotherwise.definition N since (Insert, Retract) repair (D, IC), N > \D =(Insert Retract) \ = Insert N > = (Insert Retract) = Retract.remains show N (three-valued) model IC. three-valuedmodel every p D, N (p) {t, >}. Regarding IC, (Insert, Retract)255fiArieli, Denecker, Van Nuffelen, & Bruynoogherepair (D, IC), thus every formula IC true least Herbrand modelD0 = Insert \ Retract. particular, (q) = every q D0 . since everyp DInsert N (p) {t, >} D0 DInsert, necessarily q D0 N (q) {t, >}.follows every q D0 , N (q) k (q) = t, thus Lemma 1 Lemma 2, N mustalso (three-valued) model D0 . Hence N model IC.2last two propositions characterize repairs UDB terms pairsassociated certain three-valued models IC. shall denote elementspairs follows:Notation 2 Let N three-valued model let DB = (D, IC) database. Denote:InsertN = N > \ RetractN = N > D.conclude model-based analysis characterizing set -preferredrepairs, one preference criteria considered Definition 7 (i.e., set inclusionminimal cardinality). propositions show, common considerationsinconsistent databases properly recovered (e.g., keeping amount changesminimal possible, close possible original instance, etc.)captured preferential models context preferential semantics (Shoham, 1988).idea define order relation set (three-valued) modelsdatabase. relation intuitively captures criterion making preferences amongrelevant models. Then, preferred models (those minimalrespect underlying order relation) considered order determinedatabase repaired. formalize idea:Definition 11 Given database DB = (D, IC), denote:MDB = {N | N k HD classical model IC}16 .Example 5 Consider database DB = ({p, r} , {p q}) Example 4.shown, six valuations form HD , two-valued modelIC, namely:{p : , q : >, r : t},{p : , q : >, r : >},{p : >, q : >, r : t},{p : >, q : >, r : >},{p : >, q : f, r : t},{p : >, q : f, r : >}.k-minimal models among models {p : t, q : >, r : t} {p : >, q : f, r : t}, thusMDB = {N | N (p) k t, N (q) = >, N (r) k t} {N | N (p) = >, N (q) k f, N (r) k t}.Preference orders reflect normality considerations applied relevantset valuations (MDB , case); preferable , describes situationcommon (plausible) one described . Hence, natural waydefine preferences case minimizing inconsistencies. thus get followingdefinition:Definition 12 Let set three-valued valuations, N1 , N2 S.16. Note N three-valued valuation two-valued model IC.256fiCoherent integration databases abductive logic programmingN1 -more consistent N2 , N1> N2> .N1 c -more consistent N2 , |N1> | < |N2> |.N -maximally consistent (respectively, N c -maximally consistentS), N 0 -more consistent N (respectively, N 0c -more consistent N ).following propositions show close relationship consistent models MDB preferred repairs DB.Proposition 5 N -maximally consistent element MDB , (InsertN , RetractN )-preferred repair DB.Proof: Proposition 3, (InsertN , RetractN ) repair DB. -preferredrepair DB, repair (Insert, Retract) s.t. Insert InsertN , Retract RetractN ,InsertRetract InsertN RetractN . Proposition 4 proof, element0000N 0 MDB s.t. Insert = InsertN , Retract = RetractN , (N 0 )> = InsertN RetractN .follows, then, (N 0 )> N > , N maximally consistent MDB ,contradiction definition N .2Proposition 6 Suppose (Insert, Retract) -preferred repair DB.-maximally consistent element N MDB s.t. Insert = InsertN Retract = RetractN .Proof: pair (Insert, Retract) particular repair DB, thus Proposition 2classical model IC Insert = \ Retract = \ . Considerfollowing valuation:(N (p) =>p \ p \(p)otherwise.First show N = HD . since (p) = HD (p), since HDminimal Herbrand model D, necessarily p 6 \ p 6 \ , thus N (p) = (p) =(p) (p) = (p) HD (p). Otherwise, (p) 6= HD (p), either (p) =HD (p) = f , i.e., p \ D, (p) = f HD (p) = t, i.e., p \ . cases,N (p) = > = (p) HD (p)17 . Thus N = HD , N MDB . Now, Proposition 2again, definition N , InsertN = N > \ = [(M \ D) (D \ )] \ = \ =Insert, RetractN = N > = [(M \ D) (D \ )] = \ = Retract.remains show N -maximally consistent MDB . Suppose not.00N 0 MDB s.t. (N 0 )> N > = Insert Retract. Proposition 3, (InsertN , RetractN )also repair DB. Moreover,0InsertN = (N 0 )> \ N > \ = InsertN = Insert,0RetractN = (N 0 )> N > = RetractN = Retract,00InsertN RetractN = (N 0 )> N > = InsertN RetractN = Insert Retract.00Hence (InsertN , RetractN ) <i (Insert, Retract), (Insert, Retract) -preferredrepair (D, IC), contradiction.2Propositions 5 6 may formulated terms c follows:17. use fact f = >.257fiArieli, Denecker, Van Nuffelen, & BruynoogheProposition 7 N c -maximally consistent element MDB , (InsertN , RetractN )c -preferred repair DB.Proposition 8 Suppose (Insert, Retract) c -preferred repair DB.c -maximally consistent element N MDB s.t. Insert = InsertN Retract = RetractN .proofs last two propositions similar Propositions 5 6,respectively.Example 6 Consider Example 3. that:UDB = (D, IC) = ( {p(a), p(b), q(a), q(c)}, {X(p(X) q(X))} ).HDThus,= {p(a) : t, p(b) : t, p(c) : f, q(a) : t, q(b) : f, q(c) : t}, classical modelsIC either p(y) false q(y) true every {a, b, c}. Now, sinceHD neither p(b) false q(b) true, follows every element MU DB must assign> either p(b) q(b). Hence, -maximally consistent elements MU DB (whichcase also c -maximally consistent elements MU DB ) following:M1 = { p(a) : t, p(b) : >, p(c) : f, q(a) : t, q(b) : f, q(c) : },M2 = { p(a) : t, p(b) : t, p(c) : f, q(a) : t, q(b) : >, q(c) : }.Propositions 5 6, then, -preferred repairs UDB (which also c preferred repairs) (InsertM1 , RetractM1 ) = (, {p(b)}) (InsertM2 , RetractM2 ) =({q(b)}, ) (cf. Example 3).Example 7 Examples 4 5, -maximally consistent elements (and c maximally consistent elements) MDB N1 = {p : t, q : >, r : t} N2 = {p : >, q : f, r : t}.follows preferred repairs case ({q}, ) (, {p}).summarize, section considered model-based, three-valued preferential semantics database integration. shown (Propositions 5 8) commonnatural criteria making preferences among possible repairs (i.e., set inclusionminimal cardinality) expressed order relations three-valued modelsdatabase. two ways making preferences (among repairs one hand amongthree-valued models hand) thus strongly related, induce two alternative approaches database integration. next section shall consider thirdapproach problem (aimed provide operational semantics databaseintegration) relate model-based semantics, discussed above.4. Computing Repairs Abductionsection introduce abductive system consistently integrates possibly contradicting data-sources. system computes, set data-sources preferencecriterion , corresponding -repaired databases18 . framework composedabductive logic program (Denecker & Kakas, 2000) abductive solver Asystem(Kakas, Van Nuffelen, & Denecker, 2001; Van Nuffelen & Kakas, 2001) based18. important note already stage computing -repaired databases wontnecessary produce repaired databases.258fiCoherent integration databases abductive logic programmingabductive refutation procedure SLDNFA (Denecker & De Schreye, 1992, 1998). firstthree parts section describe components: Section 4.1 give generaldescription abductive reasoning, Section 4.2 show applied encodedatabase repairs, Section 4.3 describe computational platform. Then,Section 4.4 demonstrate computation process comprehensive example,Section 4.5 specify soundness completeness results approach (with respectbasic definitions Section 2 model-based semantics Section 3). Finally,Section 4.6 consider ways representing special types data system.4.1 Abductive Logic Programmingstart general description abductive reasoning context logic programming. usual logic programming, language contains constants, functions,predicate symbols. term either variable, constant, compound term f (t1 , . . . , tn ),f n-ary function symbol ti terms. atom expression formp(t1 , . . . , tm ), p m-ary predicate symbol ti (i = 1,. . ., m) terms. literalatom negated atom. denial expression form X( F ), Fconjunction literals X subset variables F . free variablesF (those X) considered place holders objects unspecifiedidentity (Skolem constants). Intuitively, body F denial X( F ) representsinvalid situation.Definition 13 (Kakas et al., 1992; Denecker & Kakas, 2000) abductive logic theorytriple = (P , , IC), where:P logic program, consisting clauses form h l1 . . . ln , hatomic formula li (i = 1, . . . , n) literals. clauses interpreteddefinitions predicates heads,set abducible predicates, i.e., predicates appear headclause P,IC set first-order formulae, called integrity constraints.main model semantics logic programming extended abductive logicprogramming. includes two-valued completion (Console, Theseider Dupre, & Torasso,1991) three-valued completion semantics (Denecker & De Schreye, 1993), extendedwell-founded semantics (Pereira, Aparicio, & Alferes, 1991), generalized stable semantics (Kakas & Mancarella, 1990b). semantics defined terms arbitraryinterpretations (Denecker & De Schreye, 1993), generally based Herbrandinterpretations. effect restriction semantics abductive theorydomain closure condition imposed: domain interpretation knownHerbrand universe. model abductive theory semanticsHerbrand interpretation H, exists collection ground abducible facts, H model logic program P (with respect correspondingsemantics logic programming) H classically satisfies element IC.259fiArieli, Denecker, Van Nuffelen, & BruynoogheSimilarly, main semantics logic programming, one definenotion abductive solution query abductive theory.Definition 14 (Kakas et al., 1992; Denecker & Kakas, 2000) (abductive) solutiontheory (P , , IC) query Q set ground abducible atoms, onepredicate symbol A, together answer substitution , followingthree conditions satisfied:a) P consistent semantics S,b) P |=S IC,c) P |=S Q.next section use abductive theory non-recursive programmodel database repairs. next proposition shows abductive theoriesHerbrand semantics coincide, models correspond abductive solutions querytrue.Proposition 9 Let = (P , , IC) abductive theory, P non-recursiveprogram. H Herbrand model three-valued completion semantics, iff HHerbrand model two-valued completion semantics, iff H generalized stable model, iff H generalized well-founded model .H model , set abducible atoms H abductive solutionquery true. Conversely, every abductive solution true, exists uniquemodel H , set true abducible atoms H.Proof: proof based well-known fact non-recursive logic programs,main semantics logic programming coincide. particular, non-recursive logicprogram P Herbrand interpretation H, unique modelsemantics (see, example, Denecker & De Schreye, 1993).Let H model = (P , , IC) four semantics mentioned above.exists collection ground abducible facts , H modellogic program P corresponding semantics logic programming. Since Pnon-recursive, P . observation, H unique model Pmentioned semantics. Hence, H modelsemantics. proves first part proposition.H Herbrand model , set abducible atoms Hmodel P . Clearly, must set true abducible atoms H. Pobviously consistent, entails integrity constraints , entails true.Hence, abductive solution true. Conversely, set abducible atoms,P unique model H set true abducible atoms H .abductive solution true, H satisfies integrity constraints, hence H model. Consequently, H unique model , set true abducible atoms . 2addition standard properties abductive solutions theory queryQ, specified Definition 14, one frequently imposes optimization conditions solutions260fiCoherent integration databases abductive logic programming, analogous found context database repairs. Two frequently used criteriagenerated abductive solution minimal respect set inclusionrespect set cardinality (cf. Definition 7). fact preferencecriteria used choosing appropriate abductive solutions selecting preferreddatabase repairs necessarily mean natural mappingcorresponding solutions. next sections show, however, meta-programmingallows us map database repair problem abductive problem (w.r.t.type preference criterion).4.2 Abductive Meta-Program Encoding Database Repairstask repairing union n given databases DB respect integration local integrity constraints IC, represented abductive theory= (P , , IC 0 ), P meta-program encoding new database obtainedupdating existing databases, set {insert, retract} abducible predicatesused describe updates, IC 0 encodes integrity constraints. P, facts pappear least one databases encoded atomic rules db(p), facts pappear updated database represented atoms fact(p). latter predicatedefined follows:fact(X) db(X) retract(X)fact(X) insert(X)assure predicates insert retract encode proper updatedatabase, following integrity constraints also specified:inserted element belong given database:insert(X) db(X)retracted element belong database:retract(X) db(X)set integrity constraints IC 0 obtained straightforward transformationIC: every occurrence database fact p integrity constraint replacedfact(p)19 .Example 8 (Example 1, revisited) Figure 2 contains meta-program encoding Example 1 (the codes Examples 2 3 similar).noted Section 4.1, main semantics abductive logic programingone one correspondence repairs composed database DBHerbrand models encoding, abductive meta theory . Consequently, abductionused compute repairs. following sections introduce abductive methodpurpose.19. Since abductive system (see Section 4.3) accept integrity constraints denial form, caseelements IC 0 form, Lloyd-Topor transformation (Lloyd & Topor, 1984)may also applied here; consider case Section 4.3.2.261fiArieli, Denecker, Van Nuffelen, & Bruynooghe% System definitions:defined(fact( ))defined(db( ))abducible(insert( ))abducible(retract( ))% composer:fact(X) db(X) retract(X)fact(X) insert(X)insert(X) db(X)retract(X) db(X)% databases:db(teaches(c1,n1))db(teaches(c2,n2))db(teaches(c2,n3))= Z fact(teaches(X,Y)) fact(teaches(X,Z))% D1% D2% ICFigure 2: meta-program Example 14.3 Abductive Computational Model (The Asystem)describe abductive system used compute database repairs.Asystem (Kakas, Van Nuffelen, & Denecker, 2001; Van Nuffelen & Kakas, 2001) toolcombining abductive logic theories constraint logic programming (CLP). synthesisrefutation procedures SLDNFA (Denecker & De Schreye, 1998) ACLP (Kakaset al., 2000), together improved control strategy. essence Asystemreduction high level specification lower level constraint store, managedconstraint solver. See http://www.cs.kuleuven.ac.be/dtai/kt/ latest versionsystem20 . review theoretical background well practicalconsiderations behind system. information, see (Denecker & De Schreye,1998) (Kakas, Van Nuffelen, & Denecker, 2001).4.3.1 Abductive Inferenceinput Asystem abductive theory = (P, A, IC), IC consistsuniversally quantified denials. process answering query Q, given conjunctionliterals, described derivation Q rewriting states. state pair(G , ST ), G, set goal formulae, set conjunctions literals denials.rewriting process elements G (the goals) reduced basic formulae20. version runs top Sicstus Prolog 3.10.1 later versions.262fiCoherent integration databases abductive logic programmingstored structure ST . structure called store, consistsfollowing elements21 :set contains abducibles a(t).set contains denials form X( a(t) Q), a(t) abducible.denial may contain free variables.set E equalities inequalities terms.consistency E maintained constraint solver uses Martelli Montanari unification algorithm (Martelli & Montanari, 1982) equalities constructivenegation inequalities.state = (G , ST ) called consistent G contain false ST consistent (since kept consistent E, latter conditionequivalent consistency E). consistent state empty set goals (G = )called solution state.derivation starts initial state (G0 , ST 0 ), every element ST 0 empty,initial goal, G0 , contains query Q integrity constraints ICtheory . sequence rewriting steps performed. step starts certain stateSi = (Gi , ST ), selects goal Gi , applies inference rule (see below) obtainnew consistent state Si+1 . consistent state reached Si derivationbacktracks. derivation terminates solution state reached, otherwise fails (seeSection 4.4 demonstration process).Next present inference rules Asystem, using following conventions:Gi = Gi {F }, F selected goal formula.SELECT denote nondeterministic choices inference rule.Q conjunction literals, possibly empty. Since empty conjunction equivalenttrue, denial Q empty Q equivalent false., , E mentioned, remain unchanged.inference rules classified four groups, named leftmost literalselected formula (shown bold). group contains rules (positive) conjunctionsliterals rules denials.1. Defined predicates:inference rules unfold bodies defined predicate. positive conjunctionscorresponds standard resolution selected clause, whereas denialclauses used every clause leads new denial.D.1 p(t) Q:Let p(si ) Bi P (i = 1, . . . , n) n clauses p head. Then:Gi+1 = Gi {t = s1 B1 Q} . . . Gi+1 = Gi {t = sn Bn Q}21. actual implementation Asystem also contains store finite domain constraint expressions.store needed application here, hence omitted.263fiArieli, Denecker, Van Nuffelen, & BruynoogheD.2 X( p(t) Q):Gi+1 = Gi {X, ( = B Q) | p(s) B P variables }2. Negations:Resolving negation corresponds switching mode reasoning positiveliteral denial vice versa. similar idea negation-as-failurelogic programming.N.1 p(t) Q:Gi+1 = Gi {Q, p(t)}N.2 X( p(t) Q) contain variables X:Gi+1 = Gi {p(t)} Gi+1 = Gi { p(t), X( Q)}3. Abducibles:first rule responsible creation new hypotheses. rules ensureelements consistent .A.1 a(t) Q:SELECT arbitrary a(s) define Gi+1 = Gi {Q} {s = t}Gi+1 = Gi {Q} {X( = R) | X( a(s) R) }{ (t = s) | a(s) } i+1 = {a(t)}A.2 X( a(t) Q):Gi+1 = Gi {X( = Q) | a(s) } i+1 = {X( a(t) Q)}4. Equalities:inference rules isolate (in)equalities, constraint solver evaluate them. first rule applies equalities goal formulae:E.1 = Q:Gi+1 = Gi {Q} Ei+1 = Ei {s = t}following three rules handle equalities denials. rule applies dependswhether contain free universally quantified variables. rules Q[X/t]denotes formula obtained Q substituting term X.E.2 X( = Q):unifiable Gi+1 = Gi ;Otherwise, let Es equation set solved form representing generalunifier (Martelli & Montanari, 1982). Gi+1 = Gi {X( Es Q)}.E.3 X, Y( X = Q) term containing X:Gi+1 = Gi {Y ( Q[X/t])}E.4 X, Y( X = Q) X free variable X set universallyquantified variables term t:Ei+1 = Ei {X(X 6= t)} Gi+1 = Gi {X = t} {Y ( Q[X/t])}22 .22. first branch inference E.4 explores condition X(X 6= t). second branch, negationcondition explored. X identical t, values assigned X.second branch, universally quantified variables X turned free variables may appearfree ( Q[X/t]).264fiCoherent integration databases abductive logic programmingusual, one check floundering negation. occurs inference ruleN.2 applied denial universally quantified variables negative literal p(t).Floundering aborts derivation.answer substitution , derived solution state S, substitutionfree variables satisfies E (i.e. (E) true) grounds . Note that, caseabductive theory without abducibles integrity constraints, computed answersdefined Lloyd (1987) general unifiers E correct answers answersubstitutions defined above.Proposition 10 (Kakas, Van Nuffelen, & Denecker, 2001) Let = (P, A, IC) abductive theory, Q query, solution state derivation Q, answersubstitution S. pair consisting ground abducible atoms ((S))answer substitution abductive solution Q.4.3.2 Constraint Transformation Denial FormSince inference rules Asystem applied integrity constraints denialform, integrity constraints IC abductive theory must translatedform. done applying variant Lloyd-Topor transformation (Lloyd &Topor, 1984) integrity constraints (see Denecker & De Schreye, 1998).procedure well-known procedure used deductive databases convert firstorder quantified query Q logically equivalent pair atomic query nonrecursive datalog procedure. transformation defined rewriting process setsformulae: initial set { F |F IC}, transformation done applying DeMorgan various distribution rules. New predicates rules may introducedtransformation order deal universal quantifications denials.illustrate transformation case integrity constraints running example.Example 9 Consider following extension integrity constraints Example 1:IC = { XY Z (teaches(X, ) teaches(X, Z) = Z) ,X (teacher(X) teaches(Y, X)) }.Note addition original integrity constraint Example 1, also demandevery teacher give least one course.Lloyd-Topor transformation first integrity constraint:(1) X Z (teaches(X, ) teaches(X, Z) = Z)(2) X Z (teaches(X, ) teaches(X, Z) 6= Z)(3) X Z ( teaches(X, ) teaches(X, Z) 6= Z)Lloyd-Topor transformation second integrity constraint:(1) X (teacher(X) teaches(Y, X))(2) X ( teacher(X) teaches(Y, X))265fiArieli, Denecker, Van Nuffelen, & Bruynooghe(3) X ( teacher(X) gives courses(X)) gives courses defined by:gives courses(X) teaches(Y, X)(4) X ( teacher(X) gives courses(X)),gives courses(X) teaches(Y, X)4.3.3 Control Strategyselection strategy applied derivation process crucial. Prolog-like selection strategy (left first, depth first) often leads trashing, blindchoices, result global overview current state computation.development Asystem main focus improvement controlstrategy. idea apply first rules result deterministic changestate, information propagated. none rules applicable, oneleft choices selected. strategy, commitment choice suspendedmoment information derived deterministic way.resembles CLP-solver, constraints propagate information soonchoice made. propagation reduce number choices made thusoften dramatically increases performance.4.3.4 Implementationsection describe structure implementation. Figure 3 shows layeredview. upper-most level consists specific abductive logic theory integrationtask, i.e., database information integrity constraints. layer togethercomposer form abductive meta-theory (see Section 4.2) processedAsystem.6DB 1DB 26DB nInputAbductiveTheory?6Composer?6OptimizerComposingSystemAbductiveSystemAsystem (based SLDNFA)Sicstus Prolog??Figure 3: schematic view system components.266fiCoherent integration databases abductive logic programmingnoted above, composer consists meta-theory integrating databasescoherent way. interpreted abductive theory, abduciblepredicates provide information restore consistency amalgamateddata.abductive system (enclosed dotted lines Figure 3) consists three maincomponents: finite domain constraint solver (part Sicstus Prolog), abductive metainterpreter (described previous sections), optimizer.optimizer component that, given preference criterion spacesolutions, computes most-preferred (abductive) solutions. Given preferencecriterion, component prunes fly branches search tree leadsolutions worse others already computed. actuallybranch bound filter solutions space, speeds-up execution makes suredesired solutions obtained23 . preference criterion pre-order,optimizer complete, is, compute optimal solutions (moreSection 4.5). Moreover, general-purpose component, may usefuldata integration, also for, e.g., solving planning problems.4.3.5 Complexitywell-known general, task repairing database tractable,may exponential number different ways repairing it. Even cases integrity constraints assumed single-headed dependencies (Greco & Zumpano, 2000),checking whether exists -repaired database certain query Q satisfied, P2 . Checking fact satisfied -repaired databases P2 (seeGreco & Zumpano, 2000). surprising light correspondencecomputations -minimal repairs computations entailment relations definedmaximally consistent models (see Propositions 58), also known second levelpolynomial hierarchy.pure upper bound Asystem still unknown, since best knowledge complexity results SLDNFA refutation procedure available.4.4 Example: Derivation Repairs AsystemConsider Example 9. corresponding meta-theory (assuming Lloyd-Toportransformation applied it) given Figure 4. case, follows,shall assume variables denials universally quantified, so, orderreduce amount notations, universal quantifiers omitted denial rules.executed code Figure 4, well examples literaturesystem. Theorem 2 Section 4.5 guarantees, output case setpreferred solutions corresponding problem. follows demonstrate23. See also third item Note 2 (at end Section 4.4).267fiArieli, Denecker, Van Nuffelen, & Bruynooghedb(teacher(n1 ))db(teacher(n2 ))db(teacher(n3 ))db(teaches(c1 ,n1 ))db(teaches(c2 ,n2 ))db(teaches(c2 ,n3 ))fact(teaches(X,Y)) fact(teaches(X,Z)) (Y 6= Z) (ic1)fact(teacher(X)) gives courses(X)(ic2)gives courses(X) fact(teaches(Y,X))fact(X) db(X) retract(X)fact(X) insert(X)insert(X) db(X)retract(X) db(X)(composer-ic1)(composer-ic2)Figure 4: meta-theory Example 9.preferred solutions meta-theory computed.follow one branch refutation tree, starting initial state (G0 , ST 0 ),initial set goals G0 = {true0 , ic1, ic2, composer-ic1, composer-ic2},initial store ST 0 = (, , ). Suppose first selected formulaF1 = ic1 = fact(teaches(X, Y)) fact(teaches(X, Z)) (Y 6= Z).Then, D.2,G1 = G0 \ F1{ db(teaches(X, Y)) retract(teaches(X, Y)) fact(teaches(X, Z)) (Y 6= Z),insert(teaches(X, Y)) fact(teaches(X, Z)) (Y 6= Z) },ST 1 = ST 0 . Now, pickF2 = db(teaches(X, Y)) retract(teaches(X, Y)) fact(teaches(X, Z)) (Y 6= Z).Select db(teaches(X,Y)), unfold corresponding atoms database, then,D.2, followed E.2 E.3,G2 = G1 \ F2{ retract(teaches(c1 , n1 )) fact(teaches(c1 , Z)) (n1 6= Z),retract(teaches(c2 , n2 )) fact(teaches(c2 , Z)) (n2 6= Z),retract(teaches(c2 , n3 )) fact(teaches(c2 , Z)) (n3 6= Z) },268fiCoherent integration databases abductive logic programmingstill ST 2 = ST 1 . Pick second denial among new goals addedG2 . Denote denial F3 . Since F3 starts negated literal, N.2 applies,derivation process splits two branches. second branch containsG3 = G2 \ F3 { retract(teaches(c2 , n2 )), fact(teaches(c2 , Z)) (n2 6= Z) },still ST 3 = ST 2 . Choose first new goal, i.e.,F4 = retract(teaches(c2 , n2 )).Now, since 3 = , option add F4 3 . Thus, A.2,G4 = G3 \ F4 ST 4 = (, {F4 }, ).Assume, now, take second new goal G3 :F5 = fact(teaches(c2 , Z)) (n2 6= Z).Following similar process unfolding data described above, using db(teaches(c2 , n3 )),end-upretract(teaches(c2 , n3 )) (n2 6= n3 ).Selecting negative literal (n2 6= n3 ), N.2 applies again. first branch quickly results failure adding (n2 = n3 ) E. second branch adds (n2 = n3 )retract(teaches(c2 , n3 )) set goals. former one added constraintstore, (n2 6= n3 ), simplifies true. Assume latter selected next. Leti-th step. i1 (the set abducible predicates producedcurrent step) empty, thus option abduce retract(teaches(c2 , n3 )). Thus,A.1, ST consists of:= {retract(teaches(c2 , n3 ))},= {F4 } = { retract(teaches(c2 , n2 ))},Ei = Ei1 ,Gi = Gi1 \ {retract(teaches(c2 , n3 ))} { teaches(c2 , n2 ) = teaches(c2 , n3 )}.last goal certainly satisfied, ic1 resolved branch.turn ic2. So:Fi+1 = ic2 = fact(teacher(X)) gives courses(X).evaluation Fi+1 either x = n1 x = n2 successful, interesting casex = n3 . case evaluation leads goal gives courses(X). Unfoldinggoal yields fact(teaches(Y, n3 )) appears goal set. order satisfy goal,resolved either one composers rules (using D.1). first rule (i.e.,fact(X) db(X) retract(X)) leads failure (since retract(teaches(c2 , n3 ))already ), second rule composer, fact(X) insert(X), mustapplied. leads abduction insert(teaches(Y, n3 )). ic1, 6= c1 6= c2derived24 . Also, composer-ic1 composer-ic2 satisfied current state,eventually solution state reached derivation path described here,24. One verify constraints indeed detected derivations process. omitdetails order keep example tractable.269fiArieli, Denecker, Van Nuffelen, & Bruynooghecontains following sets:= {retract(teaches(c2 , n3 )), insert(teaches(Y, n3 ))},E = {Y 6= c1 , 6= c2 },means retraction teaches(c2 , n3 ) insertion teaches(Y, n3 ) 6= c16= c2 . solutions obtained similar way.Note 2 remarks derivation process.1. solution contains non-ground abducible predicate. indeedexpected result, since solution resolves contradiction integrity constraint ic1 removing assumption teacher n3 teaches course c2 . result,teacher n3 teach course. Thus, order assure integrity constraint (ic2), solution indicates n3 must teach course (other c1c2 ).2. One possible (and realistic) explanation cause inconsistencydatabase Example 9 Figure 4, typographic error. might happen,instance, c2 mistakenly typed instead of, say, c3 , teaches(c2 ,n3 ).case, database repair computed pinpoints possibility (in case, then,equal c3 )25 . explanation cannot explicitly captured, unlessparticular repairs non-ground solutions constructed, indeed casehere. approaches recently introduced (e.g., Bravo &Bertossi, 2003; Cali, Lembo, & Rosati, 2003) properly capture casesExample 9, best knowledge, application database integrationability.3. system finds solution corresponds goal state Sg = (Gg , ST g )Gg = ST g = (g , g , Eg ), -optimizer may used wheneverstate = (Gs , (s , , Es )) reached, |g | < |s |, corresponding branchtree pruned26 .4.5 Soundness Completenesssection give soundness completeness results Asystem, relateresults model-based preferential semantics, considered Section 3.follows denote abductive meta-theory (constructed describedSection 4.2) composing n given databases DB 1 , . . . , DB n . Let also ProcALPsound abductive proof procedure 27 . following proposition shows ProcALPprovides coherent method integrating databases represented .Proposition 11 Every abductive solution obtained ProcALP query truetheory , repair UDB.25. variable free {Y/c3 } answer substitution grounds satisfies E.26. size increase along derivation, state cannot lead solutionbetter one induced Sg , corresponding branch tree indeed pruned.27. is, ProcALP process computing abductive solutions , sense Definition 14.270fiCoherent integration databases abductive logic programmingProof: construction easy see conditions listedDefinition 5 satisfied. Indeed, first two conditions assured integrity constraints composer. last condition also met since soundness ProcALPproduces abductive solutions query true . Thus, second propertyDefinition 14, every solution = (Inserti , Retracti ) P |= IC.Since P contains data section facts, follows |= IC, i.e. everyintegrity constraints follows Inserti \ Retracti .2SLDNFA sound abductive proof procedure (Denecker & De Schreye, 1998),taken procedure ProcALP , Proposition 11 provides soundness theoremcurrent implementation Asystem. optimizer incorporatedAsystem, following soundness result extended system:Theorem 1 (Soundness) Every output obtained query true ,Asystem executed c -optimizer [respectively, -optimizer], c preferred repair [respectively, -preferred repair] UDB.Proof: Follows Proposition 11 (since Asystem based SLDNFAsound abductive proof procedure), fact c -optimizer prunes pathslead solutions c -preferable. Similar arguments hold systems-optimizer.2Proposition 12 Suppose query true finite SLDNFA-tree w.r.t. .every c -preferred repair every -preferred repair UDB obtained runningAsystem.Outline proof: proof abductive solutions minimal cardinalityobtained system based Theorem 10.1 Denecker & De Schreye, 1998,shown SLDNFAo , extension SLDNFA, aimed computing solutionsminimal cardinality, complete; see Denecker & De Schreye, 1998, Section 10.1,details. Similarly, proof abductive solutions minimalw.r.t. set inclusion obtained system based Theorem 10.2 Denecker &De Schreye, 1998, shows SLDNFA+ , another extension SLDNFA,aimed computing minimal solutions w.r.t. set inclusion, also complete; see Denecker& De Schreye, 1998, Section 10.2, details.Now, Asystem based combination SLDNFAo SLDNFA+ . Moreover, system change refutation tree (but controls way rulesselected), Theorems 10.1 10.2 Denecker De Schreye (1998) applicablecase well. Thus, c - -minimal solutions produced.particular means every c -preferred repair well every -preferred repair UDBproduced system.2noted last proposition guarantee non-preferred repairsproduced (as true general). However, following theoremshows, use optimizer excludes possibility.271fiArieli, Denecker, Van Nuffelen, & BruynoogheTheorem 2 (Completeness) notations Proposition 12 assumptions,output execution Asystem together c -optimizer [respectively,together -optimizer] exactly !(UDB, c ) [respectively, !(UDB, )].Proof: shall show claim case c ; proof w.r.t. similar.Let (Insert, Retract) !(UDB, c ). Proposition 12, = (Insert, Retract) onesolutions produced Asystem . Now, execution systemtogether c -optimizer, path corresponds cannot prunedrefutation tree, since assumption (Insert, Retract) minimal cardinalityamong possible solutions, pruning condition satisfied. Thusproduced c -optimized system. converse, suppose (Insert, Retract)repair UDB produced c -optimized system. Suppose contradiction (Insert, Retract) 6 !(UDB, c ). proof Proposition 12,0 = (Insert0 , Retract0 ) !(UDB, c ) constructed Asystem ,(Insert0 , Retract0 ) <c (Insert, Retract). |0 | < ||, c -optimizer would prunepath solution cardinality becomes bigger |0 |. contradictsassumption (Insert, Retract) produced c -optimized system.2Note 3 SLDNFA-resolution Asystem based extension SLDNFresolution (Lloyd, 1987) coincides logic programs empty sets abducible predicates. SLDNF-resolution complete computation always terminates. SLDNFA inherits property. reason condition finiteSLDNFA-tree imposed Proposition 12 Theorem 2. Like SLDNF, terminationSLDNFA guaranteed imposing syntactic conditions program. refer(Verbaeten, 1999), conditions proposed guarantee existencefinite SLDNFA-tree.context paper, floundering would arise presence unsafe integrityconstraints (e.g., x p(x)). One way eliminate problem use unary domainpredicate dom, ranging objects database, add rangequantified variable integrity constraints, obtain formulae formx(dom(x) (x)) x(dom(x) (x)).following results immediately follow propositions Section 3 (unless explicitly said, Asystem without optimizer).Corollary 1 Suppose query true finite SLDNFA refutation tree w.r.t. inputtheory . Then:1. every output (Insert, Retract) Asystem classical model ICs.t. Insert = \ Retract = \ .2. every output (Insert, Retract) Asystem 3-valued model N ICs.t. InsertN = Insert RetractN = Retract.Corollary 2 notations Corollary 1 assumption, that:272fiCoherent integration databases abductive logic programming1. every output (Insert, Retract) obtained running Asystem together-optimizer [respectively, together c -optimizer], -maximallyconsistent element [respectively, c -maximally consistent element] N MU DB s.t.InsertN = Insert RetractN = Retract.2. every -maximally consistent element [respectively, c -maximally consistent element] N MUDB solution (Insert, Retract) obtained runningAsystem together -optimizer [respectively, together c -optimizer]s.t. Insert = InsertN Retract = RetractN .last corollaries show operational semantics, induced Asystem,also represented preferential semantics, terms preferred models theory.set R(UDB, ) represents intended meaning -recover databaseUDB, therefore obtained computationally, set{(Insert, Retract) | (Insert, Retract) output Asystem -optimizer},or, equivalently, described terms preferred models theory,following set:{(InsertN , RetractN ) | N -maximally consistent MU DB }.4.6 Handling Specialized Informationpurpose section demonstrate potential usage systemcomplex scenarios, various kinds specialized data incorporated system.particular, briefly consider time information source identification. also giveguidelines extend system capabilities handling kindsinformation.4.6.1 Timestamped InformationMany database applications contain temporal information. kind data may divided two types: time information part data itself, time informationrelated database operations (e.g., records database update time). Consider,instance, birth day(John,15/05/2001)16/05/2001 . Here, Johns date birth instanceformer type time information, subscripted data describes timefact added database, instance latter type time information.approach, timestamp information integrated adding temporal theorydescribing state database particular time point. One wayusing situation calculus. approach database described initialinformation history events performed database lifetime (see Reiter,1995). use different approach, based event calculus (Kowalski &Sergot, 1986). idea make distinction two kinds events, add dbdel db, describe database modifications, composer-driven events insert273fiArieli, Denecker, Van Nuffelen, & Bruynoogheretract used constructing database repairs. view, extendedcomposer following form:holds at(P,T) initially(P) clipped(0,P,T)holds at(P,T) add(P,E) E<T clipped(E,P,T)clipped(E,P,T) del(P,C) EC, C<Tadd(P,T)add(P,T)del(P,T)del(P,T)add db(P,T)insert(P,T)del db(P,T)retract(P,T)insert(P,T) retract(P,T)insert(P,T) add db(P,T)retract(P,T) del db(P,T)Note extended representation, integrity constraints must carefully specified. Consider, e.g. statement person born one date:holds at(birth day(P,D1),T) holds at(birth day(P,D2),T) D16=D2problem ensure consistency, constraint must checked everypoint time. may avoided simple rewriting ensures constraintverified event person occurs:ic(P,T) holds at(birth day(P,D1),T) holds at(birth day(P,D2),T) D16=D2add db(birth day(P, ),T) NT = T+1 ic(P,NT)insert(birth day(P, ),T) NT = T+1 ic(P,NT)ic(P,0)Note 4 last example used temporal integrity constraints order resolvecontradicting update events. Clearly, contradicting events necessarily yield classically inconsistent database, role integrity constraints express possibleevents terms time causation, necessary describe consequenceviolation consistency.Instead using temporal integrity constraints event calculus, one could repairdatabase time-stamps using time-based criterion making preferencesamong repairs. instance, denote db(x1 , . . . , xn )t data-fact db(x1 , . . . , xn )timestamp t, suppose (Insert, Retract) (Insert0 , Retract0 ) two repairs database (D, IC). time-based criterion preferring (Insert, Retract)(Insert0 , Retract0 ) could state, e.g., every data-fact db(x1 , . . . , xn ) timestampst1 , t2 s.t. db(x1 , . . . , xn )t1 follows Insert \ Retract db(x1 , . . . , xn )t2 followsInsert0 \ Retract0 , necessarily t1 t2 . detailed treatment issue outsidescope paper.interested reader may refer, e.g., (Sripada, 1995; Mareco & Bertossi, 1999)detailed discussion use logic programming based approaches specificationtemporal databases. specifications easily combined repairs,given above.274fiCoherent integration databases abductive logic programming4.6.2 Keeping Track Source Identitiescases important preserve identity databasespecific piece information originated. useful, instance, one wantsmake preferences among different sources, specific source filtered (e.g, corresponding database available becomes unreliable).kind information may decoded adding another argument every fact,denotes identity origin. requires minor modifications basic composer,since composer controls way data integrated. such,component keep track source information.Suppose, then, every database fact add another argument identifiessource. I.e., db(X,S) denotes X fact originated database S. composerfollowing form:fact(X,S) db(X,S) retract(X)fact(X,composer) insert(X)insert(X) db(X,S)retract(X) db(X,S)Note composer considers extra source inserts brand new datafacts. possible, e.g., trace information comes specific source, makepreferences among different sources (by specifying appropriate integrity constraints),filter data comes certain sources. last property demonstrated nextrule:validFact(X) fact(X,S) trusted source(S)trusted source enumerates reliable sources data.Note last example source identification extended ordermake preferences among different sources (and ignoring unreliable sources).introducing new predicate, trust(Source,Amount), attaches certain levelreliability source, possible, case conflicts, prefer sources higherreliability follows:fact(X,S) db(X,S0 ) 6= S0 trusted(S0 ,S)trusted(S0 ,S) trust(S0 ,A0 ) trust(S,A) A0 >method particularly useful integrity constraint acts functional dependency specific facts. following example (originally introduced Subrahmanian, 1994) demonstrates this.Example 10 Consider following simple scenario target recognition, threesensors autonomous vehicle, different degrees reliability, identifyobjects vehicles neighborhood:trust(radar,10)trust(gunchar,8)275fiArieli, Denecker, Van Nuffelen, & Bruynooghetrust(speedometer,5)db(observe(object1,t72),radar)db(observe(object1,t60),gunchar)db(observe(object1,t80),speedometer)fact(observe(O,V1 ),S) db(observe(O,V2 ),S0 ) S6=S0 trusted(S0 ,S)radar highest reliability, observation preserved. observations sensors retracted database.5. Discussion Overview Related Worksinterest systems coherent integration databases continuously growinglast years (see, e.g, Olive, 1991; Baral et al., 1991, 1992; Revesz, 1993; Subrahmanian, 1994; Bry, 1997; Gertz & Lipeck, 1997; Messing, 1997; Lin & Mendelzon, 1998;Liberatore & Schaerf, 2000; Ullman, 2000; Greco & Zumpano, 2000; Greco et al., 2001;Franconi et al., 2001; Lenzerini, 2001, 2002; Arenas et al., 1999, 2003; Bravo & Bertossi,2003; Cali et al., 2003, many others). Already early works subject became clear design systems data integration complex task, demandssolutions many questions different disciplines, belief revision, mergingupdating, reasoning inconsistent information, constraint enforcement, query processing course many aspects knowledge representation. section shalladdress issues.One important aspect data integration systems concepts independent(stand-alone) data-sources unified database mapped other.proper specification relations source schemas schemaamalgamated data exempts potential user aware dataarranged sources. One approach mapping, sometimes called global-centricglobal-as-view (Ullman, 2000), requires unified schema expressedterms local schemas. approach, every term unified schema associatedview (alternatively, query) sources. approach takensystems data integration, well ours. main advantage approachinduces simple query processing strategy based unfolding query,uses terminology databases. indeed case abductivederivation process, defined Section 4.3.1. approach, sometimes called sourcecentric local-as-view (used, e.g., Bertossi et al., 2002), considers every source viewintegrated database, meaning every source obtained conceptsglobal database. particular, global schema independent distributedones. implies, particular, addition new source system requiresprovide local definitions necessarily involves changes global schema.main advantage latter approach is, therefore, provides better settingmaintenance. detailed discussion topic, see (Ullman, 2000; Lenzerini, 2001;Cali et al., 2002; Van Nuffelen et al., 2004). references survey different approaches data integration appear papers Batini, Lenzerini, Navathe (1986),276fiCoherent integration databases abductive logic programmingRahm Bernstein (2001), Lenzerini (2002).Another major issue addressed ability data integration systemsproperly cope dynamically evolving worlds. particular, domain discoursefixed advance, information may revised regular basis. lastissue usually handled methods belief revision (Alchourron et al., 1995; Gardenfors &Rott, 1995) nonmonotonic reasoning. context belief revision commonmake distinction revisions integrity constraints changes setsdata-facts, since two types information different nature thus may requiredifferent approaches handling dynamic changes. set integrity constraintsgiven clause form, methods dynamic logic programing (Alferes et al., 2000, 2002)may useful handling revisions. noted (Alferes et al., 2002), assuminglocal database consistent (as case), dynamic logic programing (togetherproper language implementing it, like LUPS (Alferes et al., 2002)) provides wayavoiding contradictory information, may viewed method updatingdatabase sequence integrity constraints arrive different time points.types changes predictable, characterized sense, temporal integrity constraints (in context temporal databases) used orderspecify treat new information. method also useful revision criteriaknown advance (e.g., case collisions, prefer recent data, cf. Section4.6.1). See, e.g., (Sripada, 1995; Mareco & Bertossi, 1999) detailed discussion temporal integrity constraints temporal databases logic programming based formalisms.second type revisions (i.e., modifications data-facts) obtained(preferred) repairs unified database, induce corresponding modificationsdata-facts. repair usually induced method restoring (or assuring) consistencyamalgamated database minimal amount change. case, minimizationcriterion often determined aspiration remain close possible setcollective information. typical kind repair goal , standard waysformally expressing enumeration methods, following28 :Minimizing Hamming distance (propositional) models unifieddatabase repairs (Liberatore & Schaerf, 2000), minimizing distance corresponding three-valued interpretations (de Amo et al., 2002) accordingsuitable generalization Hamming distance.Minimizing symmetric distance sets consequences corresponding databases (Arenas, Bertossi, & Chomicki, 1999; Arenas, Bertossi, & Kifer,2000; Bertossi, Chomicki, Cortes, & Gutierrez, 2002) or, equivalently, minimizationsterms set inclusion (Greco & Zumpano, 2000).underlying data prioritized, corresponding quantitative informationalso considered computations distances (see, instance, workLiberatore & Schaerf, 2000).28. See also (Gertz & Lipeck, 1997, Section 5) discussion repair strategies.277fiArieli, Denecker, Van Nuffelen, & BruynoogheVarious ways computing (preferred/minimal) repairs described literature,among proof-theoretical (deductive) methods (Bertossi & Schwind, 2002; de Amoet al., 2002), abductive methods (Kakas & Mancarella, 1990a; Inoue & Sakama, 1995;Sakama & Inoue, 1999, 2000), algorithmic approaches based computationsmaximal consistent subsets (Baral et al., 1991, 1992), use techniques model-baseddiagnosis (Gertz & Lipeck, 1997). common approach view database logicprogram, adopt standard techniques giving semantics logic programs order compute database repairs. instance, stable-model semantics disjunctive logicprograms used computing repairs (Greco & Zumpano, 2000; Greco et al., 2001;Franconi et al., 2001; Arenas et al., 2003), resolution-based procedures integratingseveral annotated databases introduced Subrahmanian (1990, 1994). followsSection 4, application introduced also based extended resolutionstrategy, applied logic programs may contain negation-as-failure operators abducible predicates.repairing database means particular elimination contradictions, reasoninginconsistent information major challenge data integration systems. First,important note respect every formalism handling inconsistencyacceptable context databases, even underlying criterion handlinginconsistency one repair goals mentioned above. following exampledemonstrates case:Example 11 (Arenas, Bertossi, & Chomicki, 1999) Consider following (inconsistent)database: DB = ({p, q}, {(p q)}). approach Lin (1996), instance, p qmay inferred repaired database, following strategy minimal change. However,approach none p, q, (p q) holds repaired database. particular(since (Lin, 1996) distinction data-facts integrity constraints),integrity constraint {(p q)} cannot inferred, violates intendedmeaning integrity constraint databases.Many techniques consistency enforcement repairs constraint violationssuggested, among methods resolving contradictions quantitative considerations, majority vote (Lin & Mendelzon, 1998; Konieczny & Pino Perez, 2002)qualitative ones (e.g., defining priorities different sources information preferringcertain data another, Benferhat, Cayrol, Dubois, Lang, & Prade, 1993, Arieli,1999). Another common method handling inconsistent (and incomplete) informationturning multi-valued semantics. Three-valued formalisms one consideredSection 3 used semantical basis paraconsistent methods construct databaserepairs (de Amo, Carnielli, & Marcos, 2002) useful general pinpointing inconsistencies (Priest, 1991). approaches use lattice-based semantics decode withinlanguage meta-information, confidence factors, amount beliefspecific assertion, etc. approaches combine corresponding formalismsknowledge representation, annotated logic programs (Subrahmanian, 1990, 1994;Arenas et al., 2000) bilattice-based logics (Fitting, 1991; Arieli & Avron, 1996; Messing, 1997), together non-classical refutation procedures (Fitting, 1989; Subrahmanian,278fiCoherent integration databases abductive logic programming1990; Kifer & Lozinskii, 1992) allow detect inconsistent parts databasemaintain them.6. Summary Future Workpaper developed formal declarative foundation rendering coherentdata, provided different databases, presented application implementsapproach. Like similar applications (e.g., Subrahmanian, 1994; Bertossi, Arenas, & Ferretti,1998; Greco & Zumpano, 2000; Liberatore & Schaerf, 2000), system mediates amongsources information also reasoner underlying data.Composition several data-sources encoded meta-theories form abductivelogic programs, possible extend theories providing meta-informationdata-facts, time-stamps source identities. Moreover, since reasoningprocess system based pure generalization classical refutation procedures,syntactical embedding first-order formulae languages, extensiontwo-valued semantics, necessary.Due inherent modularity system, component independentmodified meet different needs. Thus, instance, underlying solver may replacedsolver capable dealing meta-theory, improvementoptimizer affect whole system efficiency, regardless natureinput. Also, way keeping data coherent encapsulated componentintegrates data (i.e., composer). implies, particular, inputreasoner external policy making preferences among conflicting sourcescompulsory order resolve contradictions.shown, operational semantics inconsistent databases, inducedAsystem, strongly related (multi-valued) preferential semantics. preferentialsemantics provides background many non-monotonic paraconsistent formalisms(e.g., Shoham, 1988; Priest, 1989, 1991; Kifer & Lozinskii, 1992; Arieli & Avron, 1996;Arieli, 1999, 2003), implies Asystem may useful reasoning generaluncertain theories (not necessarily form databases).important note composing system inherits functionalityunderlying solver. outcome flexibility, modularity, simple interactiondifferent sources information, ability reason set first-order formulae integrity constraints29 . best knowledge application dataintegration ability.several directions exploration. First, already noted, twophases, considered here, might needed complete processdata integration:29. Provided, of-course, constraints lead floundering.279fiArieli, Denecker, Van Nuffelen, & Bruynooghea) translation difference concepts unified ontology,b) integration integrity constraints.far, formalisms dealing first item (e.g., Lenzerini, 2001, 2002; Van Nuffelenet al., 2004) mainly focus mutual relations global schema source(local) schemas, particular concepts ontology map other.hand, formalisms handling second item concentrate nonmonotonic reasoning dynamically evolving (and mutually inconsistent) worlds. synthesis mainideas behind approaches, incorporating system, major challengefuture work.Another important issue deserves attention repair inconsistencycontext deductive databases integrity constraints definitions predicates, often called view predicates. refer (Denecker, 2000) sketch maydone. kind data may combined (possibly inconsistent) temporalinformation, (partial) transactions, (contradictory) update information.Finally, since different databases may different information predicates, reasonable use weakened version closed word assumption partintegration process (for instance, assumption something false unlessdatabase, unless database information it).Acknowledgementswould like thank anonymous reviewers many helpful comments suggestions. research supported Research Fund K.U.Leuven FWOVlaanderen.ReferencesAlchourron, C. E., Gardenfors, P., & Makinson, D. (1995). logic theory change:Partial meet contraction revision function. Journal Symbolic Logic, 50, 510530.Alferes, J. J., Leite, J. A., Pereira, L. M., & Quaresma, P. (2000). Planning abductive updating. Proc. Symposium AI Planning Intelligent Agents (AISB00),pp. 18.Alferes, J. J., Pereira, L. M., Przymusinska, H., & Przymusinski, T. C. (2002). LUPSlanguage updating logic programs. Artificial Intelligence, 138 (12), 87116.Arenas, M., Bertossi, L., & Chomicki, J. (1999). Consistent query answers inconsistentdatabases. Proc. 18th ACM Symp. Principles Database Systems (PODS99),pp. 6879.Arenas, M., Bertossi, L., & Chomicki, J. (2003). Answer sets consistent query answeringinconsistent databases. Theory Practice Logic Programming, 3 (45), 393424.280fiCoherent integration databases abductive logic programmingArenas, M., Bertossi, L., & Kifer, M. (2000). Applications annotated predicate calculusquerying inconsistent databases. Lloyd, J., et al. (Eds.), Proc. 1st Int. Conf.Computational Logic (CL2000), No. 1861 LNAI, pp. 926941. Springer.Arieli, O. (1999). Four-valued logics reasoning uncertainty prioritized data.Bouchon-Meunier, B., Yager, R. R., & Zadeh, L. (Eds.), Information, Uncertainty,Fusion, pp. 263309. Kluwer.Arieli, O. (2003). Reasoning different levels uncertainty. Journal Applied NonClassical Logics, 13 (34), 317343.Arieli, O., & Avron, A. (1996). Reasoning logical bilattices. Journal Logic, Language,Information, 5 (1), 2563.Arieli, O., & Avron, A. (1999). model theoretic approach recover consistent datainconsistent knowledge-bases. Journal Automated Reasoning, 22 (3), 263309.Arieli, O., Denecker, M., Van Nuffelen, B., & Bruynooghe, M. (2002). Repairing inconsistentdatabases: model-theoretic approach abductive reasoning. Proc. ICLP02Workshop Paraconsistent Computational Logic (PCL02), Federated Logic Conference (FLOC02), pp. 5165.Arieli, O., Denecker, M., Van Nuffelen, B., & Bruynooghe, M. (2004). Database repairsigned formulae. Seipel, D., & Turell-Torres, J. (Eds.), Proc. 3rd Int. Symp.Foundations Information Knowledge Systems (FoIKS04), No. 2942 LNCS,pp. 1430. Springer.Arieli, O., Van Nuffelen, B., Denecker, M., & Bruynooghe, M. (2001). Coherent compositiondistributed knowledge-bases abduction. Nieuwenhuis, A., & Voronkov,A. (Eds.), Proc. 8th Int. Conf. Logic Programming, Artificial Intelligence Reasoning (LPAR01), No. 2250 LNCS, pp. 620635. Springer.Avron, A. (2002). Classical gentzen-type methods propositional many-valued logics.Fitting, M., & Orlowska, E. (Eds.), Theory Applications Multiple-ValuedLogics, pp. 113151. Springer.Baral, C., Kraus, S., & Minker, J. (1991). Combining multiple knowledge bases. IEEETrans. Knowledge Data Enginnering, 3 (2), 208220.Baral, C., Kraus, S., Minker, J., & Subrahmanain, V. S. (1992). Combining multiple knowledge bases consisting first order theories. Computational Intelligence, 8, 4571.Batini, C., Lenzerini, M., & Navathe, S. B. (1986). comparative analysis methodologiesdatabase schema integration. ACM Computing Surveys, 18 (4), 323364.Belnap, N. D. (1977). computer think. Ryle, G. (Ed.), ContemporaryAspects Philosophy, pp. 3056. Oriel Press.Benferhat, S., Cayrol, C., Dubois, D., Lang, J., & Prade, H. (1993). Inconsistency management prioritized syntax-based entailment. Proc. 13th Int. Joint Conf.Artificial Intelligence (IJCAI93), pp. 640645.Benferhat, S., Dubois, D., & Prade, H. (1995). infer inconsistent beliefs withoutrevising?. Proc. 14th Int. Joint Conf. Artificial Intelligence (IJCAI95), pp.14491455.281fiArieli, Denecker, Van Nuffelen, & BruynoogheBertossi, L., Arenas, M., & Ferretti, C. (1998). SCDBR: automated reasoner specifications database updates. Intelligent Information Systems, 10 (3), 253280.Bertossi, L., Chomicki, J., Cortes, A., & Gutierrez, C. (2002). Consistent answers integrated data sources. Andreasen, A., et al. (Eds.), Proc. Flexible Query AnsweringSystems (FQAS2002), No. 2522 LNCS, pp. 7185. Springer.Bertossi, L., & Schwind, C. (2002). Analytic tableau database repairs: Foundations.Proc. 2nd Int. Symp. Foundations Information Knowledge Systems(FoIKS02), No. 2284 LNCS, pp. 3248. Springer.Bravo, L., & Bertossi, L. (2003). Logic programming consistently querying data integration systems. Gottlob, G., & Walsh, T. (Eds.), Proc. Int. Jont Conf. ArtificialIntelligence (IJCAI03), pp. 1015.Bry, F. (1997). Query answering information systems integrity constraints. Proc.Int. Conf. Integrity Control Information Systems (IICIS97), pp. 113130.Cali, A., Calvanese, D., De Giacomo, G., & Lenzerini, M. (2002). Data integrationintegrity constraints. Proc. 14th Conf. Advanced Information Systems Engineering (CAiSE 2002), pp. 262279.Cali, A., Lembo, D., & Rosati, R. (2003). Query rewriting answering constraintsdata integration systems. Gottlob, G., & Walsh, T. (Eds.), Proc. Int. Jont Conf.Artificial Intelligence (IJCAI03), pp. 1621.Carnielli, W. A., & Marcos, J. (2001). Tableau systems logics formal inconsistency.Proc. Int. Conf. Artificial Intelligence (IC-AI01), Vol. II, pp. 848852. CSREAPress.Carnielli, W. A., & Marcos, J. (2002). taxonomy C-systems. Carnielli, W. A.,Coniglio, M. E., & DOttaviano, I. M. (Eds.), Paraconsistency Logical WayInconsistent, No. 228 Lecture Notes Pure Applied Mathematics, pp.194. Marcel Dekker.Console, L., Theseider Dupre, D., & Torasso, P. (1991). relationship abduction deduction. Journal Logic Computation, 1 (5), 661690.Dalal, M. (1988). Investigations theory knowledge base revision. Proc. NationalConference Artificial Intelligence (AAAI98), pp. 475479. AAAI Press.de Amo, S., Carnielli, W. A., & Marcos, J. (2002). logical framework integrating inconsistent information multiple databases. Proc. 2nd Int. Symp. FoundationsInformation Knowledge Systems (FoIKS02), No. 2284 LNCS, pp. 6784.Springer.Decker, H. (2003). Historical computational aspects paraconsistency viewlogic foundations databases. Proc. Semantics Databases, No. 2582 LNCS,pp. 6381. Springer.Denecker, M. (2000). Extending classical logic inductive definitions. Lloyd, J., et al.(Eds.), Proc. 1st Int. Conf. Computational Logic (CL2000), No. 1861 LNAI,pp. 703717. Springer.282fiCoherent integration databases abductive logic programmingDenecker, M., & De Schreye, D. (1992). SLDNFA abductive procedure normalabductive programs. Apt, K. R. (Ed.), Proc. Int. Joint Conf. Symp. LogicProgramming, pp. 686700. MIT Press.Denecker, M., & De Schreye, D. (1993). Justification semantics: unifying frameworksemantics logic programs. Proc. Logic Programming NonmonotonicReasoning Workshop, pp. 365379. MIT Press.Denecker, M., & De Schreye, D. (1998). SLDNFA abductive procedure abductivelogic programs. Journal Logic Programming, 34 (2), 111167.Denecker, M., & Kakas, A. C. (2000). Abductive Logic Programming. special issueJournal Logic Programming, Vol.44(13).Fitting, M. (1989). Negation refutation. Proc. 4th Annual Symp. Logic ComputerScience (LICS89), pp. 6370. IEEE Press.Fitting, M. (1990). Kleenes logic, generalized. Journal Logic Computation, 1, 797810.Fitting, M. (1991). Bilattices semantics logic programming. Journal LogicProgramming, 11 (2), 91116.Franconi, E., Palma, A. L., Leone, N., Perri, D., & Scarcello, F. (2001). Census data repair:challenging application disjunctive logic programming. Nieuwenhius, A., &Voronkov, A. (Eds.), Proc. 8th Int. Conf. Logic Programming, Artificial IntelligenceReasoning (LPAR01), No. 2250 LNCS, pp. 561578. Springer.Gardenfors, P., & Rott, H. (1995). Belief revision. Gabbay, D. M., Hogger, J., & Robinson,J. A. (Eds.), Handbook Logic Artificial Intelligence Logic Programming, pp.35132. Oxford University Press.Gertz, M., & Lipeck, U. W. (1997). extensible framework repairing constraint violations. Proc. Int. Conf. Integrity Control Information Systems (IICIS97),pp. 89111.Ginsberg, M. L. (1988). Multi-valued logics: uniform approach reasoning AI. Computer Intelligence, 4, 256316.Greco, G., Greco, S., & Zumpano, E. (2001). logic programming approach integration, repairing querying inconsistent databases. Proc. 17th Int. Conf.Logic Programming (ICLP01), No. 2237 LNCS, pp. 348363. Springer.Greco, S., & Zumpano, E. (2000). Querying inconsistent databases. Proc. Int. Conf.Logic Programming Automated Reasoning (LPAR2000), No. 1955 LNAI, pp.308325. Springer.Inoue, K., & Sakama, C. (1995). Abductive framework nonmonotonic theory change.Proc. 14th Int. Joint Conf. Artificial Intelligence (IJCAI95), pp. 204210.Kakas, A., Kowalski, R. A., & Toni, F. (1992). Abductive logic programming. JournalLogic Computation, 2 (6), 719770.Kakas, A., & Mancarella, P. (1990a). Database updates abduction. Proc. 16thInt. Conf. Large Data Bases (VLDB90), pp. 650661.283fiArieli, Denecker, Van Nuffelen, & BruynoogheKakas, A., & Mancarella, P. (1990b). Generalised stable models: semantics abduction.Proc. European Conference Artificial Intelligence (ECAI90), pp. 385391.John Wiley sons.Kakas, A., Michael, A., & Mourlas, C. (2000). ACLP: Abductive constraint logic programming. Journal Logic Programming, 44 (13), 129177.Kakas, A., Van Nuffelen, B., & Denecker, M. (2001). A-System: Problem solvingabduction. Proc. 17th Int. Joint Conf. Artificial Intelligence (IJCAI01), pp.591596. Morgan Kaufmann Publishers.Kifer, M., & Lozinskii, E. L. (1992). logic reasoning inconsistency. JournalAutomated Reasoning, 9 (2), 179215.Konieczny, S., & Pino Perez, R. (2002). Merging information constraints: logicalfarmework. Journal Logic Computation, 12 (5), 773808.Kowalski, R. A., & Sergot, M. J. (1986). logic-based calculus events. New GenerationComputing, 4 (1), 6795. Reprinted Foundations Knowledge Base ManagementSystems (1989), pp. 2353. Springer-Verlag.Lenzerini, M. (2001). Data integration needs reasoning. Eiter, T., Faber, W., &Truszczynski, T. (Eds.), Proc Int. Conf. Logic Programming Non-MonotonicReasoning (LPNMR01), No. 2173 LNAI, pp. 5461. Springer.Lenzerini, M. (2002). Data integration: theoretical perspective. Proc. ACM Symp.Principles Database Systems (PODS02), pp. 233246.Liberatore, P., & Schaerf, M. (2000). BReLS: system integration knowledgebases. Proc Int. Conf. Principles Knowledge Representation Reasoning(KR2000), pp. 145152. Morgan Kaufmann Publishers.Lin, J. (1996). semantics reasoning consistently presence inconsistency.Artificial Intelligence, 86 (1), 7595.Lin, J., & Mendelzon, A. (1998). Merging databases constraints. Int. JournalCooperative Information Systems, 7 (1), 5576.Lloyd, J. W. (1987). Foundations Logic Programming. Springer-Verlag.Lloyd, J. W., & Topor, R. W. (1984). Making Prolog expressive. Journal LogicProgramming, 1 (3), 225240.Mareco, C. A., & Bertossi, L. (1999). Specification implementation temporaldatabases bitemporal event calculus. Advance Conceptual Modeling, No.1727 LNCS, pp. 7485. Springer.Martelli, A., & Montanari, U. (1982). efficient unification algorithm. ACM Trans.Programming Languages Systems, 4 (2), 258282.Messing, B. (1997). Combining knowledge many-valued logics. Data KnowledgeEngineering, 23, 297315.Olive, A. (1991). Integrity checking deductive databases. Proc. 17th Int. Conf.Large Data Bases (VLDB91), pp. 513523.284fiCoherent integration databases abductive logic programmingPereira, L. M., Aparicio, J. N., & Alferes, J. J. (1991). Hypothetical reasoning wellfounded semantics. Proc. 3th Scandinavian Conference AI, pp. 289300. IOSPress.Priest, G. (1989). Reasoning truth. Artificial Intelligence, 39, 231244.Priest, G. (1991). Minimally inconsistent LP. Studia Logica, 50, 321331.Rahm, E., & Bernstein, P. (2001). survey approaches automatic schema matching.VLDB Journal, 10 (4), 334350.Reiter, R. (1995). specifying database updates. Journal Logic Programming, 25 (1),5391.Revesz, P. Z. (1993). semantics theory change: Arbitration old newinformation. Proc. 12th ACM Symp. Principles Database Systems (PODS93),pp. 7182.Sakama, C., & Inoue, K. (1999). Updating extended logic programs abduction.Proc 5th Int. Conf. Logic Programming Non-Monotonic Reasoning (LPNMR99), pp. 147161.Sakama, C., & Inoue, K. (2000). Abductive logic programming disjunctive logic programming: relationship transferability. Journal Logic Programming, 44 (13), 75100.Shoham, Y. (1988). Reasoning change: time causation standpointartificial intelligence. MIT Press.Sripada, S. M. (1995). Efficient implementation event calculus temporal databaseapplications. Proc. Int. Conf. Logic Programming (ICLP95), pp. 99113.Subrahmanian, V. S. (1990). Mechanical proof procedures many valued lattice-basedlogic programming. Journal Non-Classical Logic, 7, 741.Subrahmanian, V. S. (1994). Amalgamating knowledge-bases. ACM Trans. DatabaseSystems, 19 (2), 291331.Ullman, J. (2000). Information integration using logical views. Theoretical Computer Science, 239 (2), 189210.Van Nuffelen, B., Cortes, A., Denecker, M., Arieli, O., & Bruynooghe, M. (2004). Dataintegration using ID-logic. Proc. 16th Int. Conf. Advanced Information SystemsEngineering (CAiSE04), LNCS. Springer. appear.Van Nuffelen, B., & Kakas, A. (2001). A-system: Declarative programming abduction. Proc. Int. Conf. Logic Programming Non-Monotonic Reasoning (LPNMR01), No. 2173 LNAI, pp. 393396. Springer.Verbaeten, S. (1999). Termination analysis abductive general logic programs. De Schreye, D. (Ed.), Proc. Int. Conf. Logic Programming (ICLP99), pp. 365379. MITPress.Verbaeten, S., Denecker, M., & De Schreye, D. (1997). Compositionality normal open logicprograms. Maluszynski, J. (Ed.), Proc. Int. Logic Programming Symp. (ILPS97),pp. 371386. MIT Press.285fiArieli, Denecker, Van Nuffelen, & BruynoogheVerbaeten, S., Denecker, M., & De Schreye, D. (2000). Compositionality normal openlogic programs. Journal Logic Programming, 41 (3), 151183.Winslett, M. (1988). Reasoning action using possible models approach. Proc.National Conference Artificial Intelligence (AAAI98), pp. 8993. AAAI press.286fiJournal Artificial Intelligence Research 21 (2004) 63-100Submitted 8/03; published 2/04Competitive Coevolution EvolutionaryComplexificationKenneth O. StanleyRisto Miikkulainenkstanley@cs.utexas.eduristo@cs.utexas.eduDepartment Computer SciencesUniversity Texas AustinAustin, TX 78712 USAAbstractTwo major goals machine learning discovery improvement solutionscomplex problems. paper, argue complexification, i.e. incremental elaboration solutions adding new structure, achieves goals. demonstrate power complexification NeuroEvolution Augmenting Topologies(NEAT) method, evolves increasingly complex neural network architectures. NEATapplied open-ended coevolutionary robot duel domain robot controllers compete head head. robot duel domain supports wide range strategies,coevolution benefits escalating arms race, serves suitable testbedstudying complexification. compared evolution networks fixed structure, complexifying evolution discovers significantly sophisticated strategies.results suggest order discover improve complex solutions, evolution,search general, allowed complexify well optimize.1. IntroductionEvolutionary Computation (EC) class algorithms applied open-endedlearning problems Artificial Intelligence. Traditionally, algorithms evolve fixedlength genomes assumption space genome sufficient encodesolution. genome containing n genes encodes single point n-dimensional searchspace. many cases, solution known exist somewhere space. example,global maximum function three arguments must exist three dimensionalspace defined arguments. Thus, genome three genes encode locationmaximum.However, many common structures defined indefinite number parameters.particular, solution types contain variable number partsrepresented number parameters minimum. example, numberparts neural networks, cellular automata, electronic circuits vary (Miller, Job,& Vassilev, 2000a; Mitchell, Crutchfield, & Das, 1996; Stanley & Miikkulainen, 2002d).fact, theoretically two neural networks different numbers connections nodesrepresent function (Cybenko, 1989). Thus, clear number genesappropriate solving particular problem. Researchers evolving fixed-length genotypesmust use heuristics estimate priori appropriate number genes encodestructures.c2004AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiStanley & Miikkulainenmajor obstacle using fixed-length encodings heuristically determiningappropriate number genes becomes impossible complex problems. example,many nodes connections necessary neural network controls pingpong playing robot? Or, many bits needed neighborhood functioncellular automaton performs information compression? answers questionshardly based empirical experience analytic methods, since little knownsolutions. One possible approach simply make genome extremely large,space encodes extremely large solution likely lie somewherewithin. Yet larger genome, higher dimensional space evolution needssearch. Even ping-pong playing robot lies somewhere 10,000 dimensional space10,000 gene genome, searching space may take prohibitively long.Even problematic open-ended problems phenotypes meant improve indefinitely known final solution. example, competitive games,estimating complexity best possible player difficult estimateimplicitly assumes better player exist, cannot always know. Moreover,many artificial life domains aimed evolving increasingly complex artificial creatureslong possible (Maley, 1999). continual evolution difficult fixedgenome two reasons: (1) good strategy found fixed-length genome,entire representational space genome used encode it. Thus, wayimprove alter strategy, thereby sacrificing functionality learnedprevious generations. (2) Fixing size genome domains arbitrarily fixesmaximum complexity evolved creatures, defeating purpose experiment.paper, argue structured phenotypes evolved effectively starting evolution population small, simple genomes systematically elaboratinggenerations adding new genes. new gene expands search space,adding new dimension previously exist. way, evolution begins searchingsmall easily-optimized space, adds new dimensions necessary. approachlikely discover highly complex phenotypes approach begins searching directly intractably large space complete solutions. fact, natural evolutionutilizes strategy, occasionally adding new genes lead increased phenotypic complexity (Martin 1999; Section 2). biology, process incremental elaboration calledcomplexification, use term describe approach well.evolutionary computation, complexification refers expanding dimensionalitysearch space preserving values majority dimensions.words, complexification elaborates existing strategy adding new structure withoutchanging existing representation. Thus strategy become different,number possible responses situations generate increases (Figure 1).EC domain neuroevolution (i.e. evolving neural networks), complexificationmeans adding nodes connections already-functioning neural networks.main idea behind NEAT (NeuroEvolution Augmenting Topologies; Stanley Miikkulainen 2002b,c,d), method described paper. NEAT begins evolving networkswithout hidden nodes. many generations, new hidden nodes connectionsadded, complexifying space potential solutions. way, complex strategieselaborate simpler strategies, focusing search solutions likely maintainexisting capabilities.64fiCompetitive Coevolution Evolutionary ComplexificationAlterationOriginal StrategyStrategy FailsAltered StrategyStrategy FailsElaborationOriginal StrategyStrategy FailsElaborated StrategySkill Remains!Figure 1: Alteration vs. elaboration example.dark robot must evolve avoidlighter robot, attempts cause collision. alteration scenario (top),dark robot first evolves strategy go around left side opponent. However,strategy fails future generation opponent begins moving left.Thus, dark robot alters strategy evolving tendency move right insteadleft. However, light robot later moves right, new, altered, strategy failsdark robot retain old ability move left. elaborationscenario (bottom), original strategy moving left also fails. However, insteadaltering strategy, elaborated adding new ability move right well. Thus,opponent later moves right, dark robot still ability avoidusing original strategy. Elaboration necessary coevolutionary arms raceemerge achieved complexification.Expanding length size genome found effective previouswork (Cliff, Harvey, & Husbands, 1993; Harvey, 1993; Koza, 1995; Lindgren & Johansson,2001). NEAT advances idea making possible search wide range increasingly complex network topologies simultaneously. process based three technicalcomponents: (1) Keeping track genes match among differently sizedgenomes throughout evolution; (2) speciating population solutions differingcomplexity exist independently; (3) starting evolution uniform populationsmall networks. components work together complexifying solutions partevolutionary process. prior work, NEAT shown solve challenging reinforcement learning problems efficiently neuroevolution methods (StanleyMiikkulainen 2002b,c,d). focus studies optimizing given fitnessfunction complexifying evolution.65fiStanley & Miikkulainenpaper, focus open-ended problems explicit fitness function;instead, fitness depends comparisons agents also evolving. goaldiscover creative solutions beyond designers ability define fitness function.difficult continually improve solutions coevolutionary domains evolutiontends oscillate idiosyncratic yet uninteresting solutions (Floreano & Nolfi, 1997).Complexification encourages continuing innovation elaborating existing solutions.order demonstrate power complexification coevolution, NEAT appliedcompetitive robot control domain Robot Duel. known optimal strategydomain substantial room come increasingly sophisticatedstrategies. main results (1) evolution complexify possible, (2)complexification led elaboration, (3) significantly sophisticated successfulstrategies evolved complexification without it. results implycomplexification allows establishing coevolutionary arms race achieves significantlyhigher level sophistication otherwise possible.begin reviewing biological support complexification, well past workcoevolution, followed description NEAT method, experimental results.2. Backgroundnatural process complexification led important biological innovations.natural application EC competitive coevolution, reviewed below.2.1 Complexification NatureMutation nature results optimizing existing structures: New genes occasionally added genome, allowing evolution perform complexifying functionoptimization. addition, complexification protected nature interspecies mating prohibited. speciation creates important dynamics differingstandard GAs. section, discuss characteristics natural evolutionbasis approach utilizing computationally genetic algorithms.Gene duplication special kind mutation one parental genescopied offsprings genome once. offspring redundant genesexpressing proteins. Gene duplication responsible key innovationsoverall body morphology course natural evolution (Amores, Force, Yan, Joly,Amemiya, Fritz, Ho, Langeland, Prince, Wang, Westerfield, Ekker, & Postlethwait, 1998;Carroll, 1995; Force, Lynch, Pickett, Amores, lin Yan, & Postlethwait, 1999; Martin, 1999).major gene duplication event occurred around time vertebrates separatedinvertebrates. evidence duplication centers around HOX genes,determine fate cells along anterior-posterior axis embryos. HOX genescrucial shaping overall pattern development embryos. fact, differencesHOX gene regulation explain great deal diversity among arthropods tetrapods(Carroll, 1995). Invertebrates single HOX cluster vertebrates four, suggesting cluster duplication significantly contributed elaborations vertebrate bodyplans (Amores et al., 1998; Holland, Garcia-Fernandez, Williams, & Sidow, 1994; Nadeau& Sankoff, 1997; Postlethwait, Yan, Gates, Horne, Amores, Brownlie, & Donovan, 1998;Sidow, 1996). additional HOX genes took new roles regulating vertebrate66fiCompetitive Coevolution Evolutionary Complexificationanterior-posterior axis develops, considerably increasing body-plan complexity. AlthoughMartin (1999) argues additional clusters explained many single gene duplications accumulating generations, opposed massive whole-genome duplications,researchers agree gene duplication form contributed significantly body-planelaboration.detailed account duplicate genes take novel roles given Force et al.(1999): Base pair mutations generations following duplication partition initiallyredundant regulatory roles genes separate classes. Thus, embryo developsway, genes determine overall body-plan confined specificroles, since them. partitioning phase completes redundantclusters genes separated enough longer produce identical proteinstime. partitioning, mutations within duplicated cluster genesaffect different steps development mutations within original cluster.words, duplication creates points mutations occur. manner,developmental processes complexify.Gene duplication possible explanation natural evolution indeed expandedsize genomes throughout evolution, provides inspiration adding new genesartificial genomes well. fact, gene duplication motivated Koza (1995) allow entirefunctions genetic programs duplicated single mutation, later differentiated mutations. evolving neural networks, process meansadding new neurons connections networks.order implement idea artificial evolutionary systems, facedtwo major challenges. First, systems evolve differently sized shaped networktopologies, difficult cross without losing information. example,depending new structure added, gene may exist different positions,conversely, different genes may exist position. Thus, artificial crossover maydisrupt evolved topologies misalignment. Second, variable-length genomes,may difficult find innovative solutions. Optimizing many genes takes longeroptimizing few, meaning complex networks may eliminatedpopulation sufficient opportunity optimized.However, biological evolution also operates variable-length genomes, problems stop complexification nature. problems avoided biologicalevolution? First, nature mechanism aligning genes counterpartscrossover, data lost obscured. alignment processclearly observed E. coli (Radding, 1982; Sigal & Alberts, 1972). special protein calledRecA takes single strand DNA aligns another strand genes expresstraits, called homologous genes. process called synapsis.Second, innovations nature protected speciation. Organisms significantly divergent genomes never mate different species. organismcould mate other, organisms initially larger, less-fit genomes would forcedcompete mates simpler, fit counterparts. result, larger,innovative genomes would fail produce offspring disappear population. contrast, speciated population, organisms larger genomes competemates among species, instead population large. way, organisms may initially lower fitness general population still chance67fiStanley & Miikkulainenreproduce, giving novel concepts chance realize potential without prematurely eliminated. speciation benefits evolution diverse populations,variety speciation methods employed EC (Goldberg & Richardson, 1987;Mahfoud, 1995; Ryan, 1994).turns complexification also possible evolutionary computation abstractionssynapsis speciation made part genetic algorithm. NEAT method(section 3) implementation idea: genome complexified adding newgenes turn encode new structure phenotype, biological evolution.Complexification especially powerful open-ended domains goalcontinually generate sophisticated strategies. Competitive coevolution particularlyimportant domain, reviewed next section.2.2 Competitive Coevolutioncompetitive coevolution, individual fitness evaluated competitionindividuals population, rather absolute fitness measure.words, fitness signifies relative strengths solutions; increased fitness onesolution leads decreased fitness another. Ideally, competing solutions continuallyoutdo one another, leading arms race increasingly better solutions (Dawkins &Krebs, 1979; Rosin, 1997; Van Valin, 1973). Competitive coevolution traditionallyused two kinds problems. First, used evolve interactive behaviorsdifficult evolve terms absolute fitness function. example, Sims (1994)evolved simulated 3D creatures attempted capture ball opponent did,resulting variety effective interactive strategies. Second, coevolution usedgain insight dynamics game-theoretic problems. example, Lindgren &Johansson (2001) coevolved iterated Prisoners Dilemma strategies order demonstratecorrespond stages natural evolution.Whatever goal competitive coevolution experiment, interesting strategiesevolve arms race continues significant number generations. practice,difficult establish arms race. Evolution tends find simplest solutionswin, meaning strategies switch back forth different idiosyncraticyet uninteresting variations (Darwen, 1996; Floreano & Nolfi, 1997; Rosin & Belew, 1997).Several methods developed encourage arms race (Angeline & Pollack, 1993;Ficici & Pollack, 2001; Noble & Watson, 2001; Rosin & Belew, 1997). example, hallfame collection past good strategies used ensure current strategiesremain competitive earlier strategies. Recently, Ficici Pollack (2001) NobleWatson (2001) introduced promising method called Pareto coevolution, findsbest learners best teachers two populations casting coevolution multiobjective optimization problem. information enables choosing best individualsreproduce, well maintaining informative diverse set opponents.Although techniques allow sustaining arms race longer, directlyencourage continual coevolution, i.e. creating new solutions maintain existing capabilities. example, matter well selection performed, well competitorschosen, search space fixed, limit eventually reached. Also, may68fiCompetitive Coevolution Evolutionary Complexificationoccasionally easier escape local optimum adding new dimension searchspace searching new path original space.reasons, complexification natural technique establishing coevolutionary arms race. Complexification elaborates strategies adding new dimensionssearch space. Thus, progress made indefinitely long: Even global optimumreached search space solutions, new dimensions added, openinghigher-dimensional space even better optima may exist.test idea experimentally, chose robot duel domain combines predator/prey interaction food foraging novel head-to-head competition (Section 4).use domain demonstrate NEAT uses complexification continually elaborate solutions. next section reviews NEAT neuroevolution method, followeddescription robot duel domain discussion results.3. NeuroEvolution Augmenting Topologies (NEAT)NEAT method evolving artificial neural networks combines usual searchappropriate network weights complexification network structure. approachhighly effective, shown e.g. comparison neuroevolution (NE) methodsdouble pole balancing benchmark task (Stanley & Miikkulainen, 2002b,c,d). NEATmethod consists solutions three fundamental challenges evolving neural networktopology: (1) kind genetic representation would allow disparate topologiescrossover meaningful way? solution use historical markings line genesorigin. (2) topological innovation needs generationsoptimize protected disappear population prematurely?solution separate innovation different species. (3) topologiesminimized throughout evolution efficient solutions discovered?solution start minimal structure add nodes connections incrementally.section, explain solutions implemented NEAT.3.1 Genetic EncodingEvolving structure requires flexible genetic encoding. order allow structures complexify, representations must dynamic expandable. genome NEATincludes list connection genes, refers two node genes connected.(Figure 2). connection gene specifies in-node, out-node, weight connection, whether connection gene expressed (an enable bit), innovationnumber, allows finding corresponding genes crossover.Mutation NEAT change connection weights network structures. Connection weights mutate NE system, connection either perturbed not.Structural mutations, form basis complexification, occur two ways (Figure3). mutation expands size genome adding gene(s). add connectionmutation, single new connection gene added connecting two previously unconnectednodes. add node mutation, existing connection split new node placedold connection used be. old connection disabled two new connections added genome. connection first node chainnew node given weight one, connection new node last69fiStanley & MiikkulainenGenome (Genotype)NodeGenesConnect.GenesNode 1 Node 2 Node 3 Node 4 Node 5Sensor Sensor Sensor Output Hidden14Weight 0.7EnabledInnov 124Weight0.5DISABLEDInnov 2Network (Phenotype)134Weight 0.5EnabledInnov 325Weight 0.2EnabledInnov 454Weight 0.4EnabledInnov 515Weight 0.6EnabledInnov 645Weight 0.6EnabledInnov 114523Figure 2: NEAT genotype phenotype mapping example. genotype depictedproduces shown phenotype. 3 input nodes, one hidden, one outputnode, seven connection definitions, one recurrent. second genedisabled, connection specifies (between nodes 2 4) expressedphenotype. order allow complexification, genome length unbounded.node chain given weight connection split. Splitting connection way introduces nonlinearity (i.e. sigmoid function) nonebefore. initialized way, nonlinearity changes function slightly,new node immediately integrated network. Old behaviors encodedpreexisting network structure destroyed remain qualitatively same,new structure provides opportunity elaborate original behaviors.mutation, genomes NEAT gradually get larger. Genomes varyingsizes result, sometimes different connections positions. Crossover mustable recombine networks differing topologies, difficult (Radcliffe,1993). next section explains NEAT addresses problem.3.2 Tracking Genes Historical Markingsturns historical origin gene used tell us exactly genesmatch individuals population. Two genes historicalorigin represent structure (although possibly different weights), sincederived ancestral gene point past. Thus,system needs keep track historical origin every gene system.Tracking historical origins requires little computation. Whenever new geneappears (through structural mutation), global innovation number incrementedassigned gene. innovation numbers thus represent chronology every genesystem. example, let us say two mutations Figure 3 occurred one70fiCompetitive Coevolution Evolutionary Complexification1234561>4 2>4 3>4 2>5 5>4 1>5DIS123Mutate Add Connection4122133456567122334891>4 2>4 3>4 2>5 5>4 1>5 3>6 6>4DIS DISMutate Add Node4452651>4 2>4 3>4 2>5 5>4 1>5DIS1545141>4 2>4 3>4 2>5 5>4 1>5 3>5DIS513263Figure 3: two types structural mutation NEAT. types, adding connectionadding node, illustrated genes phenotypes. top numbergenome innovation number gene. bottom two numbers denotetwo nodes connected gene. weight connection, also encodedgene, shown. symbol DIS means gene disabled, thereforeexpressed network. figure shows connection genes appendedgenome new connection new node added network. Assumingdepicted mutations occurred one other, genes would assigned increasinginnovation numbers figure illustrates, thereby allowing NEAT keep implicithistory origin every gene population.another system. new connection gene created first mutation assignednumber 7, two new connection genes added new node mutationassigned numbers 8 9. future, whenever genomes crossover,offspring inherit innovation numbers gene. Thus, historical originevery gene system known throughout evolution.possible problem structural innovation receive different innovationnumbers generation occurs chance once. However, keepinglist innovations occurred current generation, possible ensurestructure arises independent mutationsgeneration, identical mutation assigned innovation number.extensive experimentation, established resetting list every generation opposedkeeping growing list mutations throughout evolution sufficient preventexplosion innovation numbers.innovation numbers, system knows exactly genes match(Figure 4). Genes match either disjoint excess, dependingwhether occur within outside range parents innovation numbers.crossing over, genes innovation numbers lined71fiStanley & MiikkulainenParent11231>4 2>4 3>4DISABParent242>5855>4 1>53612451>4 2>4 3>4 2>5 5>4 5>6DISABDISAB101>665293>544176>453123disjointParent1Parent21231>4 2>4 3>4DISAB42>581>555>43612451>4 2>4 3>4 2>5 5>4 5>6DISABDISAB93>576>4excess excessdisjointdisjointOffspring3124561>4 2>4 3>4 2>5 5>4 5>6DISABDISAB76>4101>681>593>5101>6465123Figure 4: Matching genomes different network topologies using innovation numbers. Although Parent 1 Parent 2 look different, innovation numbers (showntop gene) tell us several genes match even without topological analysis. new structure combines overlapping parts two parentswell different parts created crossover. case, equal fitnessesassumed, disjoint excess genes parents inherited randomly.Otherwise would inherited fit parent. disabled genes maybecome enabled future generations: preset chance inheritedgene disabled disabled either parent.randomly chosen offspring genome. Genes match inheritedfit parent, equally fit, parents randomly. Disabled genes25% chance reenabled crossover, allowing networks make use oldergenes again.1Historical markings allow NEAT perform crossover without need expensivetopological analysis. Genomes different organizations sizes stay compatible throughout evolution, problem comparing different topologies essentially avoided.1. See Appendix specific mating parameters used experiments paper.72fiCompetitive Coevolution Evolutionary Complexificationmethodology allows NEAT complexify structure still maintaining genetic compatibility. However, turns population varying complexities cannot maintaintopological innovations own. smaller structures optimize faster largerstructures, adding nodes connections usually initially decreases fitnessnetwork, recently augmented structures little hope surviving one generation even though innovations represent might crucial towards solvingtask long run. solution protect innovation speciating population,explained next section.3.3 Protecting Innovation SpeciationNEAT speciates population individuals compete primarily withinniches instead population large. way, topological innovations protected time optimize structure competeniches population. addition, speciation prevents bloating genomes: Speciessmaller genomes survive long fitness competitive, ensuring small networksreplaced larger ones unnecessarily. Protecting innovation speciation follows philosophy new ideas must given time reach potentialeliminated.Historical markings make possible system divide population speciesbased topological similarity. measure distance two network encodings linear combination number excess (E) disjoint (D) genes, wellaverage weight differences matching genes (W ):=c1 E c2++ c3 W .NN(1)coefficients c1 , c2 , c3 adjust importance three factors, factorN , number genes larger genome, normalizes genome size (N set1 genomes small). Genomes tested one time; genomes distancerandomly chosen member species less , compatibility threshold,placed species. genome placed first species previousgeneration condition satisfied, genome one species.genome compatible existing species, new species created.problem choosing best value avoided making dynamic; is,given target number species, system raise many species,lower few.reproduction mechanism NEAT, use explicit fitness sharing (Goldberg &Richardson, 1987), organisms species must share fitness niche.Thus, species cannot afford become big even many organisms perform well.Therefore, one species unlikely take entire population, crucialspeciated evolution maintain topological diversity. adjusted fitness fi0 organismcalculated according distance every organism j population:fi.j=1 sh((i, j))fi0 = Pn73(2)fiStanley & Miikkulainensharing function sh set 0 distanceP (i, j) threshold ;otherwise, sh((i, j)) set 1 (Spears, 1995). Thus, nj=1 sh((i, j)) reduces numberorganisms species organism i. reduction natural since speciesalready clustered compatibility using threshold . Every species assignedpotentially different number offspring proportion sum adjusted fitnessesfi0 member organisms. Species reproduce first eliminating lowest performingmembers population. entire population replaced offspringremaining organisms species.net effect speciating population structural innovation protected.final goal system, then, perform search solution efficientlypossible. goal achieved complexification simple starting structure,detailed next section.3.4 Minimizing Dimensionality ComplexificationUnlike systems evolve network topologies weights (Angeline, Saunders, &Pollack, 1993; Gruau, Whitley, & Pyeatt, 1996; Yao, 1999; Zhang & Muhlenbein, 1993),NEAT begins uniform population simple networks hidden nodes, differinginitial random weights. Speciation protects new innovations, allowing topological diversity gradually introduced evolution. Thus, NEAT protectsinnovation using speciation, start manner, minimally, grow new structuregenerations.New structure introduced incrementally structural mutations occur,structures survive found useful fitness evaluations. way, NEATsearches minimal number weight dimensions, significantly reducing number generations necessary find solution, ensuring networks becomecomplex necessary. gradual production increasingly complex structuresconstitutes complexification. words, NEAT searches optimal topologyincrementally complexifying existing structure.previous work, three main components NEAT (i.e. historical markings,speciation, starting minimal structure) experimentally ablated orderdemonstrate contribute performance (Stanley & Miikkulainen, 2002b,d).ablation study demonstrated three components interdependent necessarymake NEAT work. paper, show complexification establishesarms race competitive coevolution. next section describes experimental domainresult demonstrated.4. Robot Duel Domainhypothesis complexification process outlined allows discoveringsophisticated strategies, i.e. strategies effective, flexible, general,include components variations strategies obtained searchfixed space. demonstrate effect, need domain possible developwide range increasingly sophisticated strategies, sophistication readilymeasured. coevolution domain particularly appropriate sustained arms racelead increasing sophistication.74fiCompetitive Coevolution Evolutionary Complexificationchoosing domain, difficult strike balance able evolvecomplex strategies able analyze understand them. Pursuit evasiontasks utilized purpose past (Gomez & Miikkulainen, 1997; Jim& Giles, 2000; Miller & Cliff, 1994; Reggia, Schulz, Wilkinson, & Uriagereka, 2001; Sims,1994), serve benchmark domain complexifying coevolution well.past experiments evolved either predator prey, interesting coevolution taskestablished agents instead equal engaged duel. win, agent mustdevelop strategy outwits opponent, utilizing structure environment.paper introduce duel domain, two simulated robots tryoverpower (Figure 5). two robots begin opposite sides rectangularroom facing away other. robots move, lose energy proportionamount force apply wheels. Although robots never run energy(they given enough survive entire competition), robot higher energy winscollides competitor. addition, robot sensor indicatingdifference energy robot. keep energies high,robots consume food items, arranged symmetrical pattern room.robot duel task supports broad range sophisticated strategies easyobserve interpret. competitors must become proficient foraging, prey capture,escaping predators. addition, must able quickly switch one behavioranother. task well-suited competitive coevolution naive strategiesforage-then-attack complexified sophisticated strategies luringopponent waste energy attacking.simulated robots similar Kheperas (Mondada et al. 1993; Figure 6).two wheels controlled separate motors. Five rangefinder sensors sense foodanother five sense robot. Finally, robot energy-difference sensor,single wall sensor.robots controlled neural networks evolved NEAT. networks receive robot sensors inputs, well constant bias NEAT usechange activation thresholds neurons. produce three motor outputs: Twoencode rotation either right left, third indicate forward motion power.three values translated forces applied left right wheelsrobot.state st world time defined positions robots food,energy levels robots, internal states (i.e. neural activation) robotsneural networks, including sensors, outputs, hidden nodes. subsequent state st+1determined outputs robots neural network controllers, computedinputs (i.e. sensor values) st one step propagation network. robotschange position st+1 according neural network outputs follows.change direction motion proportional difference left rightmotor outputs. robot drives forward distance proportional forward outputcontinuous board size 600 600. robot first makes half turn, movesforward, completes second half turn, turning forward motionseffectively combined. robot encounters food, receives energy boost,food disappears world. loss energy due movement computedsum turn angle forward motion, even turning place takes energy.75fiStanley & MiikkulainenFigure 5: Robot Duel Domain. robots begin opposite sides board facingaway shown arrows pointing away centers.concentric circles around robot represent separate rings opponent sensorsfood sensors available robot. ring contains five sensors. robotslose energy move around, gain energy consuming food (shownsmall sandwiches). food always placed depicted horizontally symmetricalpattern around middle board. objective attain higher levelenergy opponent, collide it. complex interactionforaging, pursuit, evasion behaviors, domain allows broad rangestrategies varying sophistication. Animated demos evolved strategiesavailable www.cs.utexas.edu/users/nn/pages/research/neatdemo.html.robots within distance 20, collision occurs robot higher energywins (see Appendix B exact parameter values used).Since observed state ot taken sensors include internal stateopponent robot, robot duel partially-observable Markov decision process(POMDP). Since next observed state ot+1 depends decision opponent,necessary robots learn predict opponent likely do, basedpast behavior, also based reasonable behavior general. example,reasonable assume opponent robot quickly approaching higherenergy, probably trying collide. important complex portionobservable, memory, hence recurrent connections, crucial success.complex robot-control domain allows competitive coevolution evolve increasinglysophisticated complex strategies, used understand complexification,described next.76fiCompetitive Coevolution Evolutionary ComplexificationFigure 6: Robot Neural Networks. Five food sensors five robot sensors detect presenceobjects around robot. single wall sensor indicates proximity walls,energy difference sensor tells robot energy level differsopponent. difference important robot lower energy losesrobots collide. three motor outputs mapped forces control leftright wheel.5. Experimentsorder demonstrate complexification enhances performance, ran thirty-three500-generation runs coevolution robot duel domain. Thirteen runsbased full NEAT method. Complexification turned remaining 20 runs(although networks still speciated based weight differences), order seecomplexification contributes evolving sophisticated strategies. competitive coevolutionsetup described first, followed overview dominance tournament methodmonitoring progress.5.1 Competitive Coevolution Setuprobot duel domain supports highly sophisticated strategies. Thus, questiondomain whether continual coevolution take place, i.e. whether increasinglysophisticated strategies appear course evolution. experimentset carefully process emerge, able identify does.competitive coevolution, every network play sufficient number gamesestablish good measure fitness. encourage interesting sophisticated strategies,networks play diverse high quality sample possible opponents. One wayaccomplish goal evolve two separate populations. generation,population evaluated intelligently chosen sample networkspopulation. population currently evaluated called host population,population opponents chosen called parasite population (Rosin &77fiStanley & MiikkulainenBelew, 1997). parasites chosen quality diversity, making host/parasiteevolution efficient reliable one based random round robin tournament.experiment, single fitness evaluation included two competitions, one eastone west starting position. way, networks needed implement generalstrategies winning, independent starting positions. Host networks receivedsingle fitness point win, points losing. competition lasted 750 timesteps winner, host received zero points.selecting parasites fitness evaluation, good use made speciationfitness sharing already occur NEAT. host evaluated fourhighest species champions. good opponents bestbest species, guaranteed diverse distance must exceedthreshold (section 3.3). Another eight opponents chosen randomly HallFame composed generation champions (Rosin & Belew, 1997). Hall Fameensures existing abilities need maintained obtain high fitness. networkevaluated 24 games (i.e. 12 opponents, 2 games each), found effectiveexperimentally. Together speciation, fitness sharing, Hall Fame comprise effectivecompetitive coevolution methodology.noted complexification depend particular coevolutionmethodology. example Pareto coevolution (Ficici & Pollack, 2001; Noble & Watson,2001) could used well, advantages complexification wouldsame. However, Pareto coevolution requires every member one population play everymember other, running time domain would prohibitivelylong.order interpret experimental results, method needed analyzing progresscompetitive coevolution. next section describes method.5.2 Monitoring Progress Competitive Coevolutioncompetitive coevolution run returns record every generation championpopulations. question is, sequence increasingly sophisticated strategiesidentified data, one exists? section describes dominance tournamentmethod monitoring progress competitive coevolution (Stanley & Miikkulainen, 2002a)allows us that.First need method performing individual comparisons, i.e. whether one strategybetter another. board configurations vary games, championnetworks compared 144 different food configurations side board,giving 288 total games comparison. food configurations included9 symmetrical food positions used training, plus additional 2 food items,placed one 12 different positions east west halves board.starting food positions give initial advantage one robot another, dependingclose robots starting positions. Thus, one wins majority288 total games demonstrated superiority many different scenarios, includingbeginning disadvantage. say network superior network bwins games b 288 total games.78fiCompetitive Coevolution Evolutionary ComplexificationGiven definition superiority, progress tracked. obvious waycompare network others throughout evolution, finding whether later strategiesbeat opponents earlier strategies. example, Floreano & Nolfi (1997) usedmeasure called master tournament, champion generation comparedgeneration champions. Unfortunately, methods impractical timeintensive domain robot duel competition. Moreover, master tournamentcounts many strategies defeated generation champion, withoutidentifying ones. Thus, fail detect cases strategies defeat fewerprevious champions actually superior direct comparison. example, strategydefeats 499 500 opponents, B defeats 498, master tournament designatesuperior B even B defeats direct comparison. order decisivelytrack strategic innovation, need identify dominant strategies, i.e. defeatprevious dominant strategies. way, make sure evolution proceedsdeveloping progression strictly powerful strategies, instead e.g. switchingalternative ones.dominance tournament method tracking progress competitive coevolutionmeets goal (Stanley & Miikkulainen, 2002a). Let generation champion winner288 game comparison host parasite champions single generation. Let dj jth dominant strategy appear evolution. Dominance definedrecursively starting first generation progressing last:first dominant strategy d1 generation champion first generation;dominant strategy dj , j > 1, generation champion < j,dj superior di (i.e. wins 288 game comparison it).strict definition dominance prohibits circularities. example, d4 must superior strategies d1 d3 , d3 superior d1 d2 , d2 superior d1 .call dn nth dominant strategy run. network c exists that, example,defeats d4 loses d3 , making superiority circular, would satisfy secondcondition would entered dominance hierarchy.entire process deriving dominance hierarchy population dominancetournament, competitors play previous dominant strategies either lose288 game comparison, win every comparison previous dominant strategies, therebybecoming new dominant strategy. Dominance tournament allows us identify sequenceincreasingly sophisticated strategies. also requires significantly fewer comparisonsmaster tournament (Stanley & Miikkulainen, 2002a).Armed appropriate coevolution methodology measure success,ask question: complexification result successful competitivecoevolution?6. Results33 evolution runs took 5 10 days 1GHz Pentium III processor,depending progress evolution sizes networks involved. NEATalgorithm used less 1% computation: time spent79fiStanley & Miikkulainen24016Connections Highest Dom.Random Fitness Min. ConnectionsRandom Fitness Max. Connections2201801216010140NodesConnections200Nodes Highest Dom.Random Fitness Min. NodesRandom Fitness Max. Nodes14120810068046024020050100150200 250 300Generation3504004500500050100150200 250 300Generation350400450500Figure 7: Complexification connections nodes generations. hashed linesdepict average number connections average number hidden nodeshighest dominant network generation. Averages taken 13 complexifyingruns. hash mark appears every generation new dominant strategy emergedleast one 13 runs. graphs show dominance increases,complexity. differences average final first dominant strategiesstatistically significant connections nodes (p < 0.001). comparisondashed lines depict sizes average smallest largest networks entirepopulation five runs fitness assigned randomly. bounds showincrease complexity inevitable; simple complex speciesexist population throughout run. dominant networks complexify,beneficial.evaluating networks robot duel task. Evolution fully-connected topologies took90% longer structure-growing NEAT larger networks take longerevaluate.order analyze results, define complexity number nodes connections network: nodes connections network,complex behavior potentially implement. results analyzed answer threequestions: (1) evolution progresses also continually complexify? (2)complexification lead sophisticated strategies? (3) complexification allow better strategies discovered evolving fixed-topology networks? questionanswered turn below.6.1 Evolution ComplexityNEAT run thirteen times, time different seed, verify resultsconsistent. highest levels dominance achieved 17, 14, 17, 16, 16, 18, 19,15, 17, 12, 12, 11, 13, averaging 15.15 (sd = 2.54).generation dominance level increased least one thirteenruns, averaged number connections number nodes current dominantstrategy across runs (Figure 7). Thus, graphs represent total 197 dominancetransitions spread 500 generations. rise complexity dramatic, averagenumber connections tripling average number hidden nodes rising 080fiCompetitive Coevolution Evolutionary Complexificationalmost six. smooth trend first 200 generations, number connectionsdominant strategy grows 50%. early period, dominance transitionsoccur frequently (fewer prior strategies need beaten achieve dominance).next 300 generations, dominance transitions become sparse, although continueoccur.200th 500th generations stepped pattern emerges: complexityfirst rises dramatically, settles, abruptly increases (This pattern evenmarked individual complexifying runs; averaging done Figure 7 smoothssomewhat). cause pattern speciation. one species addinglarge amount structure, species optimizing weights less complex networks.Initially, added complexity leads better performance, subsequent optimization takeslonger new higher-dimensional space. Meanwhile, species smaller topologieschance temporarily catch optimizing weights. Ultimately, however,complex structures eventually win, since higher complexity necessary continuedinnovation.Thus, two underlying forces progress: building new structures,continual optimization prior structures background. product twotrends gradual stepped progression towards increasing complexity.important question is: NEAT searches adding structure only,removing it, complexity always increase whether helps finding good solutionsnot? demonstrate NEAT indeed prefers simple solutions complexifiesuseful, ran five complexifying evolution runs fitness assigned randomly(i.e. winner game chosen random). expected, NEAT kept widerange networks population, simple highly complex (Figure 7). is,dominant networks become complex;beneficial. minimum complexity random-fitness population muchlower dominant strategies, maximum complexity significantlygreater. Thus, evolution complexifies sparingly, complex species holdscomparison simpler ones.6.2 Sophistication Complexificationsee complexification contributes evolution, let us observe sample dominantstrategy develops time. many complex networks evolved experiments,follow species produced winning network d17 third runprogress rather typical easy understand. Let us use Sk best networkspecies generation k, hl lth hidden node arise structural mutationcourse evolution. track strategic structural innovations ordersee correlate. Let us begin S100 (Figure 8a), mature zerohidden-node strategy:S100 main strategy follow opponent, putting position mightchance collide opponent energy up. However, S100 followedopponent even opponent energy, leaving S100 vulnerable attack.S100 clearly switch roles foraging chasing enemy, causingmiss opportunities gather food.81fiStanley & MiikkulainenFigure 8: Complexification Winning Species. best networks speciesdepicted landmark generations. Nodes depicted squares beside node numbers, line thickness represents strength connections. time, networksbecame complex gained skills. (a) champion generation 10hidden nodes. (b) addition h22 respective connections gave new abilities.(c) appearance h172 refined existing behaviors.S200 . next 100 generations, evolved resting strategy, usedsignificantly lower energy opponent. situation, robotstopped moving, robot wasted energy running around. timeopponent got close, energy often low enough attacked. restingstrategy example improvement take place without complexification:involved increasing inhibition energy difference sensor, thereby slightlymodifying intensity existing behavior.S267 (Figure 8b), new hidden node, h22 , appeared. node arrivedinterspecies mating, optimized several generations already. Nodeh22 gave robot ability change behavior all-out attack.new skill, S267 longer needed follow enemy closelytimes, allowing collect food. implementing new strategynew node, possible interfere already existing resting strategy,switched roles resting disadvantage attackinghigh energy. Thus, new structure resulted strategic elaboration.S315 (Figure 8c), another new hidden node, h172 , split link input sensorh22 . Replacing direct connection sigmoid function greatly improved S315ability attack appropriate times, leading accurate role switchingattacking foraging. S315 would try follow opponent afar focusingresting foraging, zoom attack victory certain. finalstructural addition shows new structure improve accuracy timingexisting behaviors.analysis shows cases, weight optimization alone produceimproved strategies. However, strategies need extended, adding new82fiCompetitive Coevolution Evolutionary ComplexificationFigure 9: Sophisticated Endgame.Robot S313 dashes last piece food S210still collecting second-to-last piece. Although appeared S313 would loseS210 got second-to-last piece, (gray arrow), turns S210 endsdisadvantage. chance get last piece food S313 , S313saving energy S210 wasted energy traveling long distances. way,sophisticated strategies evolve complexification, combining multiple objectives,utilizing weaknesses opponents strategy.structure allows new behaviors coexist old strategies. Also, casesnecessary add complexity make timing execution behavioraccurate. results show complexification utilized produce increasingsophistication.order illustrate level sophistication achieved process, concludesection describing competition two sophisticated strategies, S210S313 , another run evolution. beginning competition, S210 S313collected available food energy levels equal. Two piecesfood remained board locations distant S210 S313 (Figure 9).danger colliding similar energy levels, obvious strategy rushlast two pieces food. fact, S210 exactly that, consuming second-to-lastpiece, heading towards last one. contrast, S313 forfeited racesecond-to-last piece, opting sit still, even though energy temporarily droppedS210 s. However, S313 closer last piece got first. received boostenergy S210 wasted energy running long distance second-to-lastpiece. Thus, S313 strategy ensured energy finally met. RobotS313 behavior surprisingly deceptive, showing high strategic sophisticationevolved. Similar waiting behavior observed several opponents, alsoevolved several runs, suggesting robust result.analysis individual evolved behaviors shows complexification indeed elaborates existing strategies, allows highly sophisticated behaviors developbalance multiple goals utilize weaknesses opponent. last question whethercomplexification indeed necessary achieve behaviors.83fiStanley & Miikkulainen6.3 Complexification vs. Fixed-topology Evolution SimplificationComplexifying coevolution compared two alternatives: standard coevolutionfixed search space, simplifying coevolution complex starting point. Notepossible compare methods using standard crossvalidation techniquesexternal performance measure exists domain. However, evolved neuralnetworks compared directly playing duel. Thus, example, run fixedtopology coevolution compared run complexifying coevolution playinghighest dominant strategy fixed-topology run entire dominance rankingcomplexifying run. highest level strategy ranking fixed-topologystrategy defeat, normalized number dominance levels, measurequality complexifying coevolution. example, strategy defeatincluding 13th dominant strategy 15, performance run1315= 86.7%. playing every fixed-topology champion, every simplifying coevolutionchampion, every complexifying coevolution champion dominance rankingevery complexifying run averaging, measure relative performancemethods.section, first establish baseline performance playing complexifyingcoevolution runs demonstrating comparison dominancelevels meaningful measure performance. compare complexificationfixed-topology coevolution networks different architectures, including fully-connectedsmall networks, fully-connected large networks, networks optimal structuredetermined complexifying coevolution. Third, compare performancecomplexification simplifying coevolution.6.3.1 Complexifying Coevolutionhighest dominant strategy 13 complexifying runs played entiredominance ranking every run. average performance scores 87.9%,83.8%, 91.9%, 79.4%, 91.9%, 99.6%, 99.4%, 99.5%, 81.8%, 96.2%, 90.6%, 96.9%, 89.3%,overall average 91.4% (sd=12.8%). all, result shows complexifyingruns produce consistently good strategies: average, highest dominant strategiesqualify top 10% complexifying runs. best runs sixth,seventh, eighth, able defeat almost entire dominance rankingevery run. highest dominant network best run (99.6%) shownFigure 10.order understand means network one dominance levelsanother, Figure 11 shows many games dominant networkexpected win average 288-game comparisons less dominantnetwork. Even lowest difference (i.e. one dominance level), dominant network expected win 50 games average, showingdifference dominance level important. difference grows approximately linearly:network 5 dominance levels higher win 100 games, network 10 levelshigher win 150 15 levels higher win 200. results suggest dominancelevel comparisons indeed constitute meaningful way measure relative performance.84fiCompetitive Coevolution Evolutionary ComplexificationFigure 10: Best Complexifying Network.highest dominant network sixth complexifying coevolution run ablebeat 99.6% dominance hierarchy 12 runs. network 11 hiddenunits 202 connections (plotted figure 8), making significant use structure.still contains basic direct connections, strategy representelaborated adding several new nodes connections. example, lateralrecurrent connections allow taking past events account, resulting refineddecisions. structures found reliably complexification,turns difficult discover directly high dimensional spacefixed-topology evolution simplification.6.3.2 Fixed-Topology Coevolution Large Networksfixed-topology coevolution, network architecture must chosen experimenter.One sensible approach approximate complexity best complexifying network.(Figure 10). network included 11 hidden units 202 connections, recurrent connections direct connections input output. idealizationstructure, used 10-hidden-unit fully recurrent networks direct connectionsinputs outputs, total 263 connections. network type ableapproximate functionality effective complexifying strategy. Fixed-topologycoevolution runs exactly complexifying coevolution NEAT, except structuralmutations occur. particular, population still speciated based weight differences (i.e. W equation 1), using usual speciation procedure.Three runs fixed-topology coevolution performed networks,highest dominant strategies compared entire dominance ranking every complexifying run. average performances 29.1%, 34.4%, 57.8%, overallaverage 40.4%. Compared 91.4% performance complexifying coevolution,85fiStanley & Miikkulainen300Average Score250200150100500Average Score Difference (out 288)Standard Deviation024681012Dominance Level Difference141618Figure 11: Interpreting Differences Dominance Levels.graph shows manygames 288-game comparison dominant network expected win,averaged runs dominance levels complexifying coevolution. example,network one level higher wins 50 games 288. larger differencedominance levels translates larger difference performance approximately linearly,suggesting dominance levels used measure performanceabsolute measure available.clear fixed-topology coevolution produced consistently inferior solutions. matterfact, fixed-topology run could defeat highest dominant strategies13 complexifying coevolution runs.difference performance illustrated computing average generationcomplexifying coevolution performance fixed-topology coevolution.generation turns 24 (sd = 8.8). words, 500 generations fixed-topologycoevolution reach average level dominance 24 generations complexifying coevolution! effect, progress entire fixed-topology coevolution runcompressed first generations complexifying coevolution (Figure 12).6.3.3 Fixed-Topology Coevolution Small NetworksOne arguments using complexifying coevolution starting search directlyspace final solution may intractable. argument may explainattempt evolve fixed-topology solutions high level complexity failed. Thus,next experiment aimed reducing search space evolving fully-connected,fully-recurrent networks small number hidden nodes well direct connectionsinputs outputs. considerable experimentation, found five hiddennodes (144 connections) appropriate, allowing fixed-topology evolution find best86fiCompetitive Coevolution Evolutionary ComplexificationComplexifyingCoevolutionDom. Level1501Dom. Level50010 Hidden UnitFixed TopoloogyCoevolution150EquivalentPerformance1Generations500Figure 12: Comparing Typical Runs Complexifying Coevolution Fixed-TopologyCoevolution Ten Hidden Units. Dominance levels charted y-axisgenerations x-axis. line appears every generation new dominantstrategy arose run. height line represents level dominance.arrow shows highest dominant strategy found 10-hidden-unit fixed-topologyevolution performs well 8th dominant strategy complexifying run,found 19th generation. (Average = 24, sd = 8.8) words,generations complexifying coevolution effective several hundredfixed-topology evolution.solutions could. Five hidden nodes also number hidden nodeshighest dominant strategies average complexifying runs.total seven runs performed 5-hidden-node networks, averageperformances 70.7%, 85.5%, 66.1%, 87.3%, 80.8%, 88.8%, 83.1%. overall average80.3% (sd=18.4%), better still significantly 91.4% performancecomplexifying coevolution (p < 0.001).particular, two effective complexifying runs still never defeatedfixed-topology runs. Also, dominance level difficult achievelast, average fixed-topology evolution reached performance159th complexifying generation (sd=72.0). Thus, even best case, fixed-topology87fiStanley & MiikkulainenComplexifyingCoevolutionDom. Level1501EquivalentPerformanceDom. Level15015 Hidden UnitFixed TopoloogyCoevolutionGenerations500500Figure 13: Comparing Typical Runs Complexifying Coevolution Fixed-TopologyCoevolution Five Hidden Units.figure 12, dominance levelscharted y-axis generations x-axis, line appears every generationnew dominant strategy arose run, height line representslevel dominance. arrow shows highest dominant strategy found5-hidden-unit fixed-topology evolution performs well 12th dominantstrategy complexifying run, found 140th generation (Average159, sd = 72.0). Thus, even best configuration, fixed-topology evolution takestwice long achieve level performance.coevolution average finds level sophistication complexifying coevolutionfinds halfway run (Figure 13).6.3.4 Fixed-Topology Coevolution Best Complexifying NetworkOne problem evolving fully-connected architectures mayappropriate topology domain. course, difficult guess appropriatetopology priori. However, still interesting ask whether fixed-topology coevolutioncould succeed task assuming right topology known? answerquestion, evolved networks fixed-topology experiments, except timeusing topology best complexifying network (Figure 10). topology may88fiCompetitive Coevolution Evolutionary Complexificationconstrain search space way finding sophisticated solution likelyfully-connected architecture. so, possible seeding populationsuccessful topology gives advantage even complexifying coevolution,must build topology minimal starting point.Five runs performed, obtaining average performance score 86.2%, 83.3%, 88.1%,74.2%, 80.3%, overall average 82.4% (sd=15.1%). 91.4% performancecomplexifying coevolution significantly better even version fixed-topologycoevolution (p < 0.001). However, interestingly, 40.4% average performance 10hidden-unit fixed topology coevolution significantly best-topology evolution, eventhough methods search spaces similar sizes. fact, best-topology evolutionperforms level 5-hidden-unit fixed-topology evolution (80.3%), eventhough 5-hidden-unit evolution optimizes half number hidden nodes. Thus, resultsconfirm hypothesis using successful evolved topology help constrainsearch. However, comparison complexifying coevolution, advantage gainedstarting way still enough make penalty starting search directlyhigh-dimensional space. Figure 14 shows, best-topology evolution average findsstrategy performs well found 193rd generation complexifyingcoevolution.results fixed-topology coevolution experiments summarized follows:method used search directly high-dimensional space effectivesolutions, reaches 40% performance complexifying coevolution.better allowed optimize less complex networks; however, sophisticatedsolutions may exist space. Even given topology appropriate task,reach level complexifying coevolution. Thus, fixed-topology coevolution appear competitive complexifying coevolution choicetopology.conclusion complexification superior allows discoveringappropriate high-dimensional topology automatically, also makesoptimization topology efficient. point discussed Section7.6.3.5 Simplifying Coevolutionpossible remedy search high-dimensional spaces allow evolutionsearch smaller structures removing structure incrementally. simplifying coevolution opposite complexifying coevolution. idea mediocre complexsolution refined removing unnecessary dimensions search space, therebyaccelerating search.Although simplifying coevolution alternative method complexifying coevolutionfinding topologies, still requires complex starting topology specified.topology chosen two goals mind: (1) Simplifying coevolution startsufficient complexity least potentially find solutions equal complexitybest solutions complexifying coevolution, (2) rate structuralremoval equivalent rate structural addition complexifying NEAT,possible discover solutions significantly simpler best complexifying solutions.89fiStanley & MiikkulainenComplexifyingCoevolutionDom. Level1501Best SolutionFixed TopologyCoevolutionDom. Level1501EquivalentPerformanceGenerations500500Figure 14: Comparing Typical Runs Complexifying Coevolution Fixed-TopologyCoevolution Best Complexifying Network. Dominance levels chartedfigure 12. arrow shows highest dominant strategy found evolvingfixed topology best complexifying network performs well dominant strategy would found 193rd generation complexifying coevolution(Average 193, sd = 85). Thus, even appropriate topology given, fixed-topologyevolution takes almost twice long achieve level performance.Thus, chose start search 12-hidden-unit, 339 connection fully-connected fullyrecurrent network. Since 162 connections added best complexifying networkevolution, corresponding simplifying coevolution could discover solutions 177connections, 25 less best complexifying network.Thus, simplify coevolution run complexifying coevolution, exceptinitial topology contained 339 connections instead 39, structural mutations removedconnections instead adding nodes connections. connections noderemoved, node removed. Historical markings speciation workedcomplexifying NEAT, except markings assigned beginning evolution.(because structure removed never added). diversity species varyingcomplexity developed before.90fi149Dom. Level1501SimplifyingCoevolutionDom. Level1501EquivalentPerformance50039339Generations500227Connections Dom.ComplexifyingCoevolutionConnections Dom.Competitive Coevolution Evolutionary ComplexificationFigure 15: Comparing Typical Runs Complexifying Coevolution Simplifying Coevolution. Dominance levels charted figure 12. addition, line plotshows complexity dominance level terms number connectionsnetworks scale indicated y-axis right. typical simplifying run,number connections reduced 339 227 connections. arrow showshighest dominant strategy found simplifying coevolution performs well9th 10th dominant strategy complexifying coevolution, normally found56 generations (sd = 31). words, even though simplifying coevolutionfinds dominance levels, search appropriate structure less effectivecomplexifying coevolution.five runs simplifying coevolution performed 64.8%, 60.9%, 56.6%, 36.4%,67.9%, overall average 57.3% (sd=19.8%). Again, performance significantly 91.4% performance complexifying coevolution (p < 0.001). Interestingly,even though started 76 connections fixed-topology coevolution tenhidden units, simplifying coevolution still performed significantly better (p < 0.001), suggesting evolving structure reducing complexity better evolving largefixed structures.Like Figures 1214, Figure 15 compares typical runs complexifying simplifyingcoevolution. average, 500 generations simplification finds solutions equivalent 56generations complexification. Simplifying coevolution also tends find dominancelevels method tested. generated average 23.2 dominance levels perrun, even finding 30 one run, whereas e.g. complexifying coevolution averagefinds 15.2 levels. words, difference dominance levels much smaller91fiStanley & MiikkulainenCoevolution Type Ave. Highest Ave. HighestAverageDom. LevelGeneration PerformanceEquivalentGeneration(out 500)91.4%34340.4%24Complexifying15.2353.6Fixed-Topology12.017210 Hidden NodeFixed-Topology13.0291.480.3%1595 Hidden NodeFixed-Topology14.0301.882.4%193Best NetworkSimplifying23.2444.257.3%56Table 1: Summary performance comparison. second column showsmany levels dominance achieved type coevolution average. thirdspecifies average generation highest dominant strategy, indicating long innovation generally continues. fourth column gives average level complexifyingcoevolution dominance hierarchy champion could defeat, fifth column showsaverage generation. differences performance (p < 0.001) equivalent generation (p < 0.001) complexifying coevolution every method significant.main result level sophistication reached complexifying coevolutionsignificantly higher reached fixed-topology simplifying coevolution.simplifying coevolution complexifying coevolution. Unlike methods, dominantstrategies tend appear spurts time, usually complexitydecreasing several generations, also shown Figure 15. numbergenerations, evolution removes several connections smaller, easily optimizedspace discovered. Then, quick succession minute improvements creates several newlevels dominance, space refined, on. processmakes sense, inferior results simplifying coevolution suggest simplifying searchineffective way discovering useful structures compared complexification.6.3.6 Comparison SummaryTable 1 shows coevolution methods differ number dominance levels, generation highest dominance level, overall performance, equivalent generation.conclusion complexifying coevolution innovates longer finds higher levelsophistication methods.7. Discussion Future Workmakes complexification powerful search method? Whereas fixed-topologycoevolution, well simplifying coevolution, good structures must optimizedhigh-dimensional space solutions themselves, complexifying coevolutionsearches high-dimensional structures elaborations known good lower-dimensionalstructures. adding new dimension, values existing genes alreadyoptimized preceding generations. Thus, new gene added, genome92fiCompetitive Coevolution Evolutionary Complexificationalready promising part new, higher-dimensional space. Thus, searchhigher-dimensional space starting blindly would evolution began searchingspace. reason complexification find high-dimensional solutionsfixed-topology coevolution simplifying coevolution cannot.Complexification particularly well suited coevolution problems. fixedgenome used represent strategy, strategy optimized, possibleadd functionality without sacrificing knowledge already present.contrast, new genetic material added, sophisticated elaborations layeredexisting structure, establishing evolutionary arms race. process evidentrobot duel domain, successive dominant strategies often built new functionalitytop existing behavior adding new nodes connections.advantages complexification imply fixed-sized genomes cannot sometimes evolve increasingly complex phenotypic behavior. Depending mappinggenotype phenotype, may possible fixed, finite set genes representsolutions (phenotypes) varying behavioral complexity. example, behaviorsobserved Cellular Automata (CA), computational structure consistinglattice cells change state function current state statecells neighborhood. neighborhood function representedgenome size 2n+1 (assuming n neighboring cells binary state) evolved obtaindesired target behavior. example, Mitchell et al. (1996) able evolve neighborhood functions determine whether black white cells majority CAlattice. evolved CAs displayed complex global behavior patterns convergedsingle classification, depending cell type majority. courseevolution, behavioral complexity CA rose even genome remainedsize.CA example, correct neighborhood size chosen priori. choicedifficult make, crucial success. desired behavior existed withinchosen size, even behavior would become gradually complex, system wouldnever solve task. Interestingly, dead-end could avoided neighborhood(i.e. genome) could expanded evolution. possible CAs couldeffectively evolved complexifying (i.e. expanding) genomes, speciatingprotect innovation, NEAT.Moreover, chosen neighborhood small represent solution,also unnecessarily large. Searching space dimensionsnecessary impede progress, discussed above. desired function existedsmaller neighborhood could found significantly fewer evaluations. Indeed,even possible efficient neighborhood symmetric, contains cellsdirectly adjacent cell processed. Moreover, even efficientneighborhood may large space begin searching. Starting searchsmall space incrementing promising part higher-dimensional spacelikely find solution. reasons, complexification advantage, evenbehavioral complexity increase extent within fixed space.CA example raises intriguing possibility structured phenotypeevolved complexification minimal starting point, historical markings,protection innovation speciation. addition neural networks93fiStanley & MiikkulainenCA, electrical circuits (Miller et al., 2000a; Miller, Job, & Vassilev, 2000b), genetic programs (Koza, 1992), robot body morphologies (Lipson & Pollack, 2000), Bayesian networks(Mengshoel, 1999), finite automata (Brave, 1996), building vehicle architectures(OReilly, 2000) structures varying complexity benefit complexification. starting search minimal space adding new dimensions incrementally,highly complex phenotypes discovered would difficult find search beganintractable space final solution, prematurely restricted smallspace.search optimal structures common problem Artificial Intelligence (AI).example, Bayesian methods applied learning model structure (Attias, 2000;Ueda & Ghahramani, 2002). approaches, posterior probabilities differentstructures computed, allowing overly complex simplistic models eliminated.Note approaches aimed generating increasingly complex functionalstructures, rather providing model explains existing data. cases,solutions involve growing gradually larger structures, goal growth formgradually better approximations. example, methods like Incremental Grid Growing(Blackmore & Miikkulainen, 1995), Growing Neural Gas (Fritzke, 1995) add neuronsnetwork approximates topology input space reasonably well.hand, complexifying systems non-deterministic (like NEAT),need based evolutionary algorithms. example, Harvey (1993) introduceddeterministic algorithm chromosome lengths entire population increasetime order expand search space; Fahlman & Lebiere (1990) developedsupervised (non-evolutionary) neural network training method called cascade correlation,new hidden neurons added network predetermined manner ordercomplexify function computes. conclusion complexification importantgeneral principle AI.future, complexification may help general problem finding appropriate level abstraction difficult problems. Complexification startsimple, high-level description solution, composed general-purpose elements.abstraction insufficient, elaborated breaking high-levelelement lower level specific components. process continue indefinitely, leading increasingly complex substructures, increasingly low-level solutionssubproblems. Although NEAT solutions composed connectionsnodes, provide early example process could implemented.One primary elusive goals AI create systems scale up.sense, complexification process scaling up. general principle takingsimple idea elaborating broader application. Much AI concerned search,whether complex multi-dimensional landscapes, highly-branching treespossibilities. However, intelligence much deciding space searchsearching proper space already identified. Currently, humansable decide proper level abstraction solving many problems, whethersimple high-level combination general-purpose parts, extremely complex assemblylow-level components. program decide level abstractionappropriate given domain would highly compelling demonstration Artificial94fiCompetitive Coevolution Evolutionary ComplexificationIntelligence. is, believe, complexification methods largestimpact future.8. Conclusionexperiments presented paper show complexification genomes leadscontinual coevolution increasingly sophisticated strategies. Three trends foundexperiments: (1) evolution progresses, complexity solutions increases, (2) evolution uses complexification elaborate existing strategies, (3) complexifying coevolution significantly successful finding highly sophisticated strategiesnon-complexifying coevolution. results suggest complexification crucialcomponent successful search complex solutions.Acknowledgmentsresearch supported part National Science Foundation grant IIS0083776 Texas Higher Education Coordinating Board grant ARP-003658476-2001. Special thanks anonymous reviewer constructive suggestions noncomplexifying comparisons.Appendix A. NEAT System Parameterspopulation 256 NEAT networks, total 512. coefficients measuringcompatibility c1 = 1.0, c2 = 1.0, c3 = 2.0. initial compatibility distance= 3.0. However, population dynamics unpredictable hundredsgenerations, assigned target 10 species. number species grew 10,increased 0.3 reduce number species. Conversely, number speciesfell 10, decreased 0.3 increase number species. normalizationfactor N used compute compatibility fixed one. order prevent stagnation,lowest performing species 30 generations old allowed reproduce.champion species five networks copied next generationunchanged. 80% chance genome connection weights mutated,case weight 90% chance uniformly perturbed 10% chanceassigned new random value. (The system tolerant frequent mutationsprotection speciation provides.) 75% chance inherited genedisabled disabled either parent. 40% crossovers, offspring inheritedaverage connection weights matching genes parents, insteadconnection weight one parent randomly. generation, 25% offspringresulted mutation without crossover. interspecies mating rate 0.05.probability adding new node 0.01 probability new link mutation10.1. used modified sigmoidal transfer function, (x) = 1+e4.9x, nodes.parameter values found experimentally, follow logical pattern: Linksneed added significantly often nodes, average weight difference0.5 significant one disjoint excess gene. Performance robust95fiStanley & Miikkulainenmoderate variations values. NEAT software available software sectionhttp://nn.cs.utexas.edu.Appendix B. Robot Duel Domain Coefficients Motionturn angle determined = 0.24|lr|, l output left turn neuron,r output right turn neuron. robot moves forward distance 1.33f600 600 board, f forward motion output. coefficientscalibrated experimentation achieve accurate smooth motion neuraloutputs zero one.ReferencesAmores, A., Force, A., Yan, Y.-L., Joly, L., Amemiya, C., Fritz, A., Ho, R. K., Langeland,J., Prince, V., Wang, Y.-L., Westerfield, M., Ekker, M., & Postlethwait, J. H. (1998).Zebrafish HOX clusters vertebrate genome evolution. Science, 282, 17111784.Angeline, P. J., & Pollack, J. B. (1993). Competitive environments evolve better solutionscomplex tasks. Forrest, S. (Ed.), Proceedings Fifth International ConferenceGenetic Algorithms (pp. 264270). San Francisco, CA: Morgan Kaufmann.Angeline, P. J., Saunders, G. M., & Pollack, J. B. (1993). evolutionary algorithmconstructs recurrent neural networks. IEEE Transactions Neural Networks, 5,5465.Attias, H. (2000). variational bayesian framework graphical models. AdvancesNeural Information Processing Systems, 12 (pp. 209215). Cambridge, MA: MITPress.Blackmore, J., & Miikkulainen, R. (1995). Visualizing high-dimensional structureincremental grid growing neural network. Prieditis, A., & Russell, S. (Eds.), MachineLearning: Proceedings 12th Annual Conference (pp. 5563). San Francisco, CA:Morgan Kaufmann.Brave, S. (1996). Evolving deterministic finite automata using cellular encoding. Koza,J. R., Goldberg, D. E., Fogel, D. B., & Riolo, R. L. (Eds.), Genetic Programming1996: Proceedings First Annual Conference (pp. 3944). Stanford University,CA, USA: MIT Press.Carroll, S. B. (1995). Homeotic genes evolution arthropods chordates.Nature, 376, 479485.Cliff, D., Harvey, I., & Husbands, P. (1993). Explorations evolutionary robotics. AdaptiveBehavior, 2, 73110.Cybenko, G. (1989). Approximation superpositions sigmoidal function. MathematicsControl, Signals, Systems, 2 (4), 303314.96fiCompetitive Coevolution Evolutionary ComplexificationDarwen, P. J. (1996). Co-Evolutionary Learning Automatic Modularisation Speciation. Doctoral Dissertation, School Computer Science, University College, UniversityNew South Wales.Dawkins, R., & Krebs, J. R. (1979). Arms races within species. ProceedingsRoyal Society London Series B, 205, 489511.Fahlman, S. E., & Lebiere, C. (1990). cascade-correlation learning architecture.Touretzky, D. S. (Ed.), Advances Neural Information Processing Systems 2 (pp.524532). San Francisco, CA: Morgan Kaufmann.Ficici, S. G., & Pollack, J. B. (2001). Pareto optimality coevolutionary learning.Kelemen, J. (Ed.), Sixth European Conference Artificial Life. Berlin; New York:Springer-Verlag.Floreano, D., & Nolfi, S. (1997). God save red queen! Competition co-evolutionaryrobotics. Evolutionary Computation, 5.Force, A., Lynch, M., Pickett, F. B., Amores, A., lin Yan, Y., & Postlethwait, J. (1999).Preservation duplicate genes complementary, degenerative mutations. Genetics,151, 15311545.Fritzke, B. (1995). growing neural gas network learns topologies. G.Tesauro,D.S.Touretzky, & T.K.Leen (Eds.), Advances Neural Information Processing Systems 7 (pp. 625632). Cambridge, MA: MIT Press.Goldberg, D. E., & Richardson, J. (1987). Genetic algorithms sharing multimodalfunction optimization. Grefenstette, J. J. (Ed.), Proceedings Second International Conference Genetic Algorithms (pp. 148154). San Francisco, CA: MorganKaufmann.Gomez, F., & Miikkulainen, R. (1997). Incremental evolution complex general behavior.Adaptive Behavior, 5, 317342.Gruau, F., Whitley, D., & Pyeatt, L. (1996). comparison cellular encodingdirect encoding genetic neural networks. Koza, J. R., Goldberg, D. E., Fogel,D. B., & Riolo, R. L. (Eds.), Genetic Programming 1996: Proceedings FirstAnnual Conference (pp. 8189). Cambridge, MA: MIT Press.Harvey, I. (1993). Artificial Evolution Adaptive Behavior . Doctoral Dissertation,School Cognitive Computing Sciences, University Sussex, Sussex.Holland, P. W., Garcia-Fernandez, J., Williams, N. A., & Sidow, A. (1994). Gene duplications origin vertebrate development. Development Supplement, pp. 125133.Jim, K.-C., & Giles, C. L. (2000). Talking helps: Evolving communicating agentspredator-prey pursuit problem. Artificial Life, 6 (3), 237254.97fiStanley & MiikkulainenKoza, J. (1995). Gene duplication enable genetic programming concurrently evolvearchitecture work-performing steps computer program. Proceedings 14th International Joint Conference Artificial Intelligence. MorganKaufmann.Koza, J. R. (1992). Genetic Programming: Programming Computers MeansNatural Selection. Cambridge, MA: MIT Press.Lindgren, K., & Johansson, J. (2001). Coevolution strategies n-person prisonersdilemma. Crutchfield, J., & Schuster, P. (Eds.), Evolutionary Dynamics - ExploringInterplay Selection, Neutrality, Accident, Function. Reading, MA: AddisonWesley.Lipson, H., & Pollack, J. B. (2000). Automatic design manufacture robotic lifeforms.Nature, 406, 974978.Mahfoud, S. W. (1995). Niching Methods Genetic Algorithms. Doctoral Dissertation,University Illinois Urbana-Champaign, Urbana, IL.Maley, C. C. (1999). Four steps toward open-ended evolution. Proceedings Genetic Evolutionary Computation Conference (GECCO-1999) (pp. 13361343). SanFrancisco, CA: Morgan Kaufmann.Martin, A. P. (1999). Increasing genomic complexity gene duplication originvertebrates. American Naturalist, 154 (2), 111128.Mengshoel, O. J. (1999). Efficient Bayesian Network Inference: Genetic Algorithms,Stochastic Local Search, Abstraction. Doctoral Dissertation, University IllinoisUrbana-Champaign Computer Science Department, Urbana-Champaign, IL.Miller, G., & Cliff, D. (1994). Co-evolution pursuit evasion i: Biological gametheoretic foundations. Tech. Rep. CSRP311, School Cognitive Computing Sciences, University Sussex, Brighton, UK.Miller, J. F., Job, D., & Vassilev, V. K. (2000a). Principles evolutionary designdigital circuits Part I. Journal Genetic Programming Evolvable Machines,1 (1), 835.Miller, J. F., Job, D., & Vassilev, V. K. (2000b). Principles evolutionary designdigital circuits Part II. Journal Genetic Programming Evolvable Machines,3 (2), 259288.Mitchell, M., Crutchfield, J. P., & Das, R. (1996). Evolving cellular automata geneticalgorithms: review recent work. Proceedings First International Conference Evolutionary Computation Applications (EvCA96). Russian AcademySciences.Mondada, F., Franzi, E., & Ienne, P. (1993). Mobile robot miniaturization: tool investigation control algorithms. Proceedings Third International SymposiumExperimental Robotics (pp. 501513).98fiCompetitive Coevolution Evolutionary ComplexificationNadeau, J. H., & Sankoff, D. (1997). Comparable rates gene loss functional divergencegenome duplications early vertebrate evolution. Genetics, 147, 12591266.Noble, J., & Watson, R. A. (2001). Pareto coevolution: Using performance coevolved opponents game dimensions parerto selection. et al, L. S. (Ed.),Proceedings Genetic Evolutionary Computation Conference (GECCO-2001).San Francisco, CA: Morgan Kaufmann.OReilly, U.-M. (2000). Emergent design: Artificial life architecture design. 7thInternational Conference Artificial Life (ALIFE-00). Cambridge, MA: MIT Press.Postlethwait, H. H., Yan, Y. L., Gates, M. A., Horne, S., Amores, A., Brownlie, A., &Donovan, A. (1998). Vertebrate genome evolution zebrafish gene map. NatureGenetics, 18, 345349.Radcliffe, N. J. (1993). Genetic set recombination application neural networktopology optimization. Neural computing applications, 1 (1), 6790.Radding, C. M. (1982). Homologous pairing strand exchange genetic recombination.Annual Review Genetics, 16, 405437.Reggia, J. A., Schulz, R., Wilkinson, G. S., & Uriagereka, J. (2001). Conditions enablingevolution inter-agent signaling artificial world. Artificial Life, 7, 332.Rosin, C. D. (1997). Coevolutionary Search Among Adversaries. Doctoral Dissertation,University California, San Diego, San Diego, CA.Rosin, C. D., & Belew, R. K. (1997). New methods competitive evolution. EvolutionaryComputation, 5.Ryan, C. (1994). Pygmies civil servants. Kinnear, Jr., K. E. (Ed.), AdvancesGenetic Programming (Chap. 11, pp. 243263). MIT Press.Sidow, A. (1996). Gen(om)e duplications evolution early vertebrates. CurrentOpinion Genetics Development, 6, 715722.Sigal, N., & Alberts, B. (1972). Genetic recombination: nature crossed strandexchange two homologous DNA molecules. Journal Molecular Biology,71 (3), 789793.Sims, K. (1994). Evolving 3D morphology behavior competition. Brooks, R. A.,& Maes, P. (Eds.), Proceedings Fourth International Workshop SynthesisSimulation Living Systems (Artificial Life IV) (pp. 2839). Cambridge, MA:MIT Press.Spears, W. (1995). Speciation using tag bits. Handbook Evolutionary Computation.IOP Publishing Ltd. Oxford University Press.99fiStanley & MiikkulainenStanley, K. O., & Miikkulainen, R. (2002a). dominance tournament method monitoring progress coevolution. Proceedings Genetic Evolutionary Computation Conference (GECCO-2002) Workshop Program. San Francisco, CA: MorganKaufmann.Stanley, K. O., & Miikkulainen, R. (2002b). Efficient evolution neural network topologies.Proceedings 2002 Congress Evolutionary Computation (CEC02). IEEE.Stanley, K. O., & Miikkulainen, R. (2002c). Efficient reinforcement learning evolving neural network topologies. Proceedings Genetic Evolutionary Computation Conference (GECCO-2002). San Francisco, CA: Morgan Kaufmann.Stanley, K. O., & Miikkulainen, R. (2002d). Evolving neural networks augmentingtopologies. Evolutionary Computation, 10 (2), 99127.Ueda, N., & Ghahramani, Z. (2002). Bayesian model search mixture models basedoptimizing variational bounds. Neural Networks, 15, 12231241.Van Valin, L. (1973). new evolutionary law. Evolution Theory, 1, 130.Yao, X. (1999). Evolving artificial neural networks. Proceedings IEEE, 87 (9), 14231447.Zhang, B.-T., & Muhlenbein, H. (1993). Evolving optimal neural networks using geneticalgorithms Occams razor. Complex Systems, 7, 199220.100fiJournal Artificial Intelligence Research 21 (2004) 1936Submitted 7/03; published 1/04Price Prediction Trading Agent CompetitionMichael P. WellmanDaniel M. ReevesKevin M. LochnerYevgeniy VorobeychikWELLMAN @ UMICH . EDUDREEVES @ UMICH . EDUKLOCHNER @ UMICH . EDUYVOROBEY @ UMICH . EDUUniversity Michigan, Artificial Intelligence LaboratoryAnn Arbor, MI 48109-2110 USAAbstract2002 Trading Agent Competition (TAC) presented challenging market game domain travel shopping. One pivotal issues domain uncertainty hotel prices,significant influence relative cost alternative trip schedules. Thus, virtuallyparticipants employ method predicting hotel prices. survey approaches employedtournament, finding agents apply interesting diversity techniques, taking account differing sources evidence bearing prices. Based data provided entrantsagents actual predictions TAC-02 finals semifinals, analyze relative efficacyapproaches. results show taking account game-specific information flightprices major distinguishing factor. Machine learning methods effectively induce relationship flight hotel prices game data, purely analytical approach basedcompetitive equilibrium analysis achieves equal accuracy historical data. Employing newmeasure prediction quality, relate absolute accuracy bottom-line performance game.1. IntroductionMany market decision problems involve anticipation forecast future prices. Price prediction particularly important, example, committing binding offer purchase goodcomplements purchased later date. sort scenario arises wheneversequential overlapping auctions related goods. Although market forecasting techniqueswidespread use broad range applications, unaware studies exploring problemcontext reminiscent multi-auction environments.annual Trading Agent Competition (TAC) (Wellman et al., 2003) provides convenientmedium studying approaches price prediction. open-invitation tournament, attracted diverse community researchers interested many aspects trading agent strategy.Price prediction turned pivotal issue TAC market game, 1 interesting array approaches emerged agent designers efforts three years competition. SinceTAC defines controlled, regular, repeatable, transparent environment observing tradingagent behavior, also uncommonly amenable analysis.1. refer original (classic) TAC game, scenario domain travel shopping. Sadeh et al. (2003)introduced second game (TAC/SCM), domain supply chain management, expectdomains subject trading competitions coming years.c 2004 AI Access Foundation. rights reserved.fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIK2. TAC Travel-Shopping GameTAC market game presents travel-shopping task, traders assemble flights, hotels,entertainment trips set eight probabilistically generated clients. Clients describedpreferred arrival departure days (pa pd), premium (hp) willing paystay Towers (T) hotel rather Shanties (S), values three different typesentertainment events. agents objective maximize value trips clients, netexpenditures markets travel goods.Flights. feasible trip includes round-trip air, consists inflight day outflight. Flights day sold independently, prices determinedday ,stochastic process. initial price flight , follows random walkthereafter increasingly upward bias.Hotels. Feasible trips must also include room one two hotels nightclients stay. 16 rooms available hotel night, soldascending 16th-price auctions. auction closes, units allocated 16 highestoffers, bidders paying price lowest winning offer. minute, hotel auctionsissue quotes, indicating 16th- (ASK) 17th-highest (BID) prices among currently activeunit offers. minute, starting 4:00, one hotel auctions selected random close,others remaining active open bids.Entertainment. Agents receive initial random allocation entertainment tickets (indexedtype day), may allocate clients sell agents continuousdouble auctions.feasible client trip defined inflight day , outflight day , hotel type ( ,1 0 S). Trips also specify entertainment allocations, purposes papersummarize expected entertainment surplus function trip days. (The paper describingagent (Cheng et al., 2004) provides details well constructs glossedhere.) value trip client (with preferences pa, pd, hp) givenpapdhp(1)end game instance, TAC server calculates optimal allocation trips clientsagent, given final holdings flights, hotels, entertainment. agents game scoretotal client trip utility, minus net expenditures TAC auctions.3. Price PredictionTAC participants recognized early importance accurate price prediction overall performance (Stone & Greenwald, 2004). prices hotels highly variable game game,yet hotels price finalized auction closessome minutes game, dependingrandom closing order. agents tend submit serious hotel bids firstclosing imminent, useful information revealed price quotes fourth minuteplay. Complementarity among goods dictates outcomes early auctions significantly affectvalue agent places particular hotel later game, conversely, prices hotelsavailable later dictate whether agent bid wisely early game.Anticipating hotel prices key element several decisions facing TAC agent, particular:20fiTAC P RICE P REDICTION1. Selecting trip itineraries. flight prices tend increase, agents great incentivecommit traveling particular days early game. Yet quality day choicesdepends crucially hotel prices included travel days.2. Bidding policy. likelihood obtaining good given bid depends obviouslyresulting clearing price. Moreover, value particular good general functionprice others. example, value obtaining room hotel (S, ) increasingfunction projected cost alternative hotel day, (T, ), decreasingfunction projected cost complementary hotel rooms adjacent days, (S, )(S,).Given importance price prediction, surprising TAC researchers exploredvariety approaches. Stone et al. (2003) noted diversity price estimation methods amongTAC-01 agents. particular interest problem certainly connected TAC-02agent, Walverine, introduced method quite distinct reported previously. Thus,highly motivated characterize performance price prediction task.Indeed, competitions TAC successful facilitating research, necessary separately evaluate techniques developed problem subtasks (Stone, 2002). Althoughinteresting subtasks tend strictly separable complex game, price-predictioncomponent trading strategy may easier isolate most. particular, problemformulated terms, natural absolute accuracy measures. fact, see below,agent developers independently chose define price prediction distinct taskagent designs.divide price prediction two phases: initial interim. Initial refers beginninggame, hotel auctions close provide quote information. Interim refersmethod employed thereafter. Since information available initial prediction (flight prices,client preferencessee Section 5.2) strict subset available interim (which adds transaction hotel price data), agents treat initial prediction (simpler) special case.Initial prediction relevant bidding policy first hotel closing, especially salienttrip choices typically made early game. Interim prediction supports ongoing revision bids hotel auctions start close. analysis report focus initial prediction,mainly simpler two tasks, involving less potential information. Moreover,agents initially relatively comparable information sets, thus providing cleaner analysis.Interim prediction also quite important interesting, focus work.4. TAC-02 Agentsnineteen agents participated TAC-02 tournament listed Table 1. tablepresents raw average scores finals semifinals, weighted averages seedinground. overviews agents, see survey edited Greenwald (2003a).seeding rounds held period 1-12 July, agent playing 440 games.Weights increased day, later games counted earlier, lowest 10 scoresagent dropped. top 16 agents advanced semifinals, held 28 JulyEdmonton, Canada. two semifinal heats: H1 comprising agents seeded 1-4 13-16,5-12 seeds placed heat H2. top four teams heat (14 games, lowest score21fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIKAgentATTacBigRedcuhkharamikavayaHlivingagentsPackaTACPainInNECRoxyBotsicsSouthamptonTACThalistniTacTOMAhacktvadUMBCTACWalverineWhiteBearzeppAffiliationAT&T Research (et al.)McGill UChinese U Hong KongBogazici UOracle IndiaLiving Systems AGN Carolina State UNEC Research (et al.)Brown USwedish Inst Comp SciU SouthamptonU EssexPoli BucharestU TorontoTechnionU Maryland Baltimore CtyU MichiganCornell UPoli BucharestSeeding313169630552064254930912835231928552847312930002232280926183118277229662098SemifinalsH1: 3137H2: 3266H1: 3200H1: 3310H2: 3250H1: 2193H2: 3160H2: 3146H1: 3397H2: 3199H1: 3108H2: 2843H1: 2724H1: 3208H2: 3287H2: 3324Finals30693099318133853246323632103413Table 1: TAC-02 tournament participants, scores round.dropped) proceeded finals, ran 32 games day. detailsTAC-02 tournament available http://www.sics.se/tac.5. Price Prediction SurveyShortly TAC-02 event, distributed survey entrants eliciting descriptionsdata documenting agents price-prediction methods. Sixteen 19 teams respondedsurvey, including 14 16 semifinalists, eight finalists. result provides detailed pictureprediction techniques employed, enables comparison efficacy respectcommon experiencethe TAC-02 finals semifinals.Thirteen 16 respondents reported agents indeed form explicit price predictions use trading strategies. thirteen listed Table 2, alonghigh-level descriptors approach initial prediction task. 2 addition, tniTac zeppresponded price predictions part agent designs, developed sufficientlydeployed tournament. TOMAhack reported ambitious design (also actually employed) based model-free policy learning, account agents bidding behaviorwithout formulating explicit price predictions.2. Though address initial prediction report, survey also solicited descriptions interim methods.22fiTAC P RICE P REDICTIONAgentATTaccuhkharamikavayaHlivingagentsPackaTACRoxyBotsicsSouthamptonTACThalisUMBCTACWalverineWhiteBearApproachmachine learninghistoricalhistoricalmachine learninghistoricalhistoricalhistoricalhistoricalhistoricalhistorical (?)historicalcompetitivehistoricalFormprobpricelineprobpointpointprobprobpricelinepointpointpointpointpointNotesboostingmoving averageneural netclassification reference categoriessurvey incompleteequilibrium analysisTable 2: Agents reporting prediction hotel prices TAC-02.5.1 Forms PredictionOne distinction observed TAC-01 agents explicitly formulated predictions termsprobability distributions prices, rather point estimates. Predictions form enableagent properly account price uncertainty decision making. Thus, asked entrantsform predictions survey.Although agents generate point predictions, notable exceptions. ATTacs boosting algorithm (Stone et al., 2003) expressly learns probability distributions associated gamefeatures. RoxyBot tabulates game price statistics direct estimation deciles hotelauction. PackaTAC harami measure historical variance, combining historical averaging define parametric distribution hotel price. Walverine predicts point prices,hedging approach decisions amounts forming effective distribution around them.Given prediction form distribution, agents may make decisions samplingdecision-analytic techniques. distribution may also facilitate interim prediction task, enabling updates based treating observations price quotes evidence.However, first controlled experiment evaluating distribution feature, context ATTac(Stone et al., 2003), find overall advantage decision-making based distributionscompared using mean values. authors offered several possible explanations observedperformance, including (1) implementation employs insufficient samples, (2)use distributions makes unrealistic assumption subsequent decisions madefull knowledge actual price values. Greenwald (2003b) also found bidding marginal utility based means outperformed bidding expected marginal utility based distributions, timeimplemented context RoxyBot. since performed analogous trials using Walverinewhich generates applies distributions yet third wayand also found bidding basedmeans superior distribution-based bidding agent actually employed TAC-02.Although source deficiency conclusively established, speculatesecond reason adduced ATTac designers plausible.23fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIKDespite evidence, alternative ways using distributions may well prove beneficial.study Greenwald (2003b) demonstrated advantage RoxyBots 2002 strategy evaluatingcandidate bid sets respect distributions, compared 2000 strategy evaluatingrespect means.Nevertheless, agents predict probability distributions, take mean distributions subject analysis. may discount potential advantages, baseddiscussion above, suspect thatwith possible exception RoxyBotagentsactually benefit predicting distributions TAC-02.Another variation form prediction prices function quantity demanded.first TAC, entrants recognized purchasing additional units may cause price increase,introduced concept pricelines, express estimated prices unit (Boyan & Greenwald, 2001; Stone & Greenwald, 2004). Agents sics cuhk reported predicting pricelines. 3cases, agent started baseline point prediction first unit hotel,derived remainder priceline according rule. example, sics predicted priceffth unit (i.e., price given demands ff units) fi , fi baseline prediction1.15 hotels day 1 4, 1.25 hotels day 2 3.succeeding analysis, evaluate predictions terms baseline prices only. notedbelow, accuracy measures applied pricelines would reflect actual value.5.2 Information Employedset information potentially available beginning game includes data pastgames, initial vector flight prices, agents client preferences. TAC-02,agents except Walverine reported using historical information predictions. ATTac,kavayaH, Walverine employ flight prices. agents construct pricelines effectively takeaccount client preferences. Walverine construct pricelines factorclient preferences part equilibrium calculations.identities agents participating game instance known TACpreliminary (qualifying seeding) rounds, agents drawn randomly round-robin tournament. However, semifinal final rounds fixed set eight agents series games,identity agents effectively observable. ATTac agent exploitavailability information.6. Approaches Price PredictionBased survey responses, divide TAC-02 prediction techniques three categories.6.1 Historical Averagingagents took relatively straightforward approach initial price prediction, estimatinghotel clearing prices according observed historical averages. example, harami calculatesmean hotel prices preceding 200 games, uses initial prediction. respectiveagents classified adopting historical approach Table 2 differ set games3. WhiteBear also reported using pricelines interim prediction (Vetsikas & Selman, 2003), initial predictionsessentially points.24fiTAC P RICE P REDICTIONinclude average, used games seeding round. Given dataset, agents tenduse sample mean distribution estimate, 4 least baseline.majority averaging agents fixed pool prior games, update averagesfinals. exception cuhk, employed moving average previous tengames current round, previous rounds beginning new round.UMBCTAC reported employing mean prices predictions respect decisions tripstwo days, median prices (which tended lower) decisions one-daytrips. semifinals based statistics last 100 seeding games. finalsdataset comprised 14 games semifinal heat. analysis below, attributepredictions UMBCTAC based mean values samples.approach taken SouthamptonTAC (He & Jennings, 2003) unique among TACagents. SouthamptonTAC designers partitioned seeding-round games three categories, competitive, non-competitive, semi-competitive. specified referenceprice type day hotel game category. agent chooses categorygame based monitoring recent game history. actual tournament, SouthamptonTAC began semifinals predicting semi-competitive reference prices, maintaining stanceswitching non-competitive last eight games finals.6.2 Machine Learningcouple TAC-02 agents employed machine learning techniques derive relationshipsobservable parameters resulting hotel prices. premise approach game-specificfeatures provide potentially predictive information, enabling agent anticipate hotel price directions manifest price quotes themselves. surprisingly, noted Section 5.2,two learning agents employed kinds information typical TAC-02 agents.ATTac predicts prices using sophisticated boosting-based algorithm conditional densityestimation (Stone et al., 2003). Development technique expressly motivated TACprice-prediction problem, though resulting algorithm quite general. ATTac learns predictorhotel type day category (i.e., days 1 4 treated symmetrically, 2 3).predictor applied beginning game maps following features predictedprice hotel:identity agents participating game,initial flight prices,closing time hotel room.Since hotel closing times unknown game start, predictor induces distributionprice predictions, based distribution hotel closing sequences. distribution constitutesATTacs initial price prediction.kavayaH (Putchala et al., 2002) predicts initial hotel prices using neural networks trained viabackpropagation. agent separate network hotel. output network4. Several agents complement simple initial prediction relatively sophisticated approach interimprediction, using evidence price quotes gradually override initial estimate. else equal, course,straightforwardness advantage. Indeed, simplicity likely significant ingredient livingagentss successTAC-01 (Fritschi & Dorer, 2002; Wellman et al., 2003).25fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIKone discrete set prices, choice set hotel (type, day) specifiedkavayaHs designers based historical prices. inputs network based initialflight prices, specifically thresholded differences flights adjacent days. example,hotel T1 might binary input indicates whether price difference inflightsdays 1 2 greater 50. Hotel S2 might input, well another baseddifference flight prices days 2 3. kavayaHs designers selected relevant inputsbased experiments agent.6.3 Competitive AnalysisWalverines overall approach TAC markets presume well-approximatedcompetitive economy (Cheng et al., 2004). method predicting hotel prices literalapplication assumption. Specifically, calculates Walrasian competitive equilibriumTAC economy, defined set prices markets would clear, assumingagents behave price takers (Katzner, 1989). Taking account exogenously determinedflight prices, Walverine finds set hotel prices support equilibrium, returnsvalues prediction hotels final prices.Let vector hotel prices, consisting elements fi denoting price hotel typeday . Let denote agent demand hotel day prices, write vectordemands . Aggregate demand simply sum agent demands, .Prices constitute competitive equilibrium aggregate demand equals aggregate supplyhotels. Since 16 rooms available hotel day, competitiveequilibrium, .Starting initial guess , Walverine searches equilibrium prices using tatonnement protocol, iterative price adjustment mechanism originally conceived Walras (Arrow& Hahn, 1971). Given specification aggregate demand, tatonnement iteratively computesrevised price vector according following difference equation:(2)Although equilibrium prices guaranteed exist given discreteness complementarities TAC environment, found procedure typically produces approximateequilibrium well within 300 iterations Walverine devotes prediction calculation.critical step competitive analysis determining aggregate demand function. Walverine estimates sum (1) demand based eight clients knows about,(2) expected demand agents (56 clients), based specified TAC distribution client preferences. calculation expected demand others exact, modulosummarization entertainment effects, given assumption agent demands separableclient (Cheng et al., 2004). assumption true beginning game (hence initialprediction), invalidated agents accumulate holdings flights hotels. Althoughanalytic expression expected demand somewhat complicated, deriving conceptually computationally difficult.Note larger component Walverines demand estimation expectationdefined distribution client preferences. Therefore, prices derives properly viewedequilibrium expectation, rather expected equilibrium prices. latter mightactually appropriate measure price prediction. However, since expected equilibrium26fiTAC P RICE P REDICTIONmuch computationally expensive equilibrium expectation (and suspectdifference would relatively small 56 i.i.d. clients), employ simpler measure.7. Predictionspart survey, entrants provided predictions agents actually employed TAC02 finals semifinals: total 60 games. many cases, predictions constant (i.e.,every game), straightforward evaluate respect full slatefinal semifinal games. two cases initial predictions change every game (ATTacWalverine), entrants able construct agent would predictedgames, whether actually participated. one case (kavayaH), partialdata. kavayaH reported predictions 32 final games, semifinal heatparticipated (H1), except one game predictor crashed.include two versions ATTac, corresponding predictors learned 20012002 preliminary rounds. ATTac01 ATTac02, respectively, represent prediction functionsemployed TAC-01 TAC-02 finals. applying ATTac01 predictor TAC-02finals, use agent identity information disabled.price vectors supplied entrants employed analysis presented Table 3.Prices rounded nearest integer display, though analysis employed whatever precision provided. Agents condition game-specific information produce distinct vectorsinstance, tabulated here.AgentharamilivingagentsPackaTACRoxyBot5sicsWhiteBearSouthamptonTACSouthamptonTAC NUMBCTAC semifinalsUMBCTAC finalsActual MeanActual MedianBest Euc DistBest EVPPWalverine constS121272120301950202037689182828S25811811610310010210030133758548735176S3801241191031009610030124879738576776S41641382040285020452952815028T1 T2 T3 T447 108 1016473 163 164 10576 167 1649776 152 1527695 160 155 11075 144 14181100 150 150 1005080805083 192 158 110113 1419571121 124 154 10959 105985971 111956980 103 1008473 113 11373Table 3: Predicted price vectors: Shoreline Shanties, followed Tampa Towers, days 14. first ten rows represent predictions employed agents tournament. lastfive represent various benchmarks, discussed below.27fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIKfirst six rows Table 3 (harami WhiteBear) correspond constant predictionsassociated agents. noted above, SouthamptonTAC switched two predictionvectors: represents reference prices semi-competitive environment, Nnon-competitive prices. UMBCTAC well switched prediction vectors within 60 gamesincase introducing finals prediction based average semifinal (H1) prices.rows labeled Actual Mean Actual Median, respectively, present averagemedian hotel prices actually resulting 60 games interest. Although clairvoyance obviously admissible approach prediction, include benchmark. directsense, actual central tendencies represent best agents taking historical averagingapproach hope capture.price vectors labeled Best Euc Dist, Best EVPP, Walverine const discussedSection 8.2.8. Evaluating Prediction Quality8.1 Accuracy Measuresremains assess efficacy various prediction approaches, terms agentsprice predictions actual TAC-02 final semifinal games. order so, requireff given actual prices given game.measure characterizing accuracy prediction8.1.1 E UCLIDEAN ISTANCEnatural measure closeness two price vectors Euclidean distance:fffffifiindexes price hotel day fi . Clearly, lower valuespreferred, , .Calculating straightforward, done reported predictionsff form distribution, Euclidean distance mean60 games. Noteprovides lower bound average distance components distribution. Thus,least according measure, evaluation distribution predictions terms meansprovides bias favor.set gameslikewise case among constant predictions, actual meanminimizes aggregate squared distance games. is, actual price vector,game ,ff5. RoxyBots prediction based statistics seeding rounds, expressed cumulative price distributionshotel, discretized deciles. RoxyBot reportedly based decisions samples distribution,taking decile value occur probability 0.1. tends overestimate prices, however, decile valuescorrespond upper limits respective ranges. prediction vector presented Table 3 (and analyzed below)corresponds adjusted value, obtained dropping top decile averaging remaining nine.28fiTAC P RICE P REDICTIONclosed form prediction minimizing aggregate , one derive numericallygiven set games (Bose et al., 2002).8.1.2 E XPECTED VALUE P ERFECT P REDICTIONEuclidean distance appears reasonable measure accuracy absolute sense. However,purpose prediction accuracy sake, rather support decisions basedpredictions. Thus, seek measure relates principle expected TAC performance.analogy standard value-of-information measures, introduce concept valueperfect prediction (VPP).Suppose agent could anticipate perfectly eventual closing price hotels. Then, amongthings, agent would able purchase flights immediately confidenceselected optimal trips clients. 6 Since many agents apparently commit tripsbeginning game anyway, perfect prediction would translate directly improved qualitychoices.7 take primary worth predictions, measure quality predictionterms supports trip choice comparison perfect anticipation. ideaVPP particularly high agents otherwise poor estimate prices.already predicting well, value obtaining perfect prediction relatively small.corresponds use standard value-of-information concepts measuring uncertainty:agent perfect knowledge, value additional information nil.Specifically, consider client preferences pa pd hp. trips surplus clientprices , defined value minus cost,costcost simply total price flights hotel rooms included trip . Letdenote trip maximizes surplus respect prices . expressionffff , evaluated respect pricesrepresents surplus trip chosen based prices. define value perfect prediction,ffVPP ff(3)Note VPP definition (3) relative client preferences, whereas seek measureapplicable pair price vectors outside context particular client. end define6. Modulo residual uncertainty regarding availability entertainment tickets, ignore analysis.7. compiled statistics temporal profile flight purchases eight agents TAC-02 finals. Fouragents purchased 16 flights (enough round trips clients) within 45 seconds average. eight agentspurchased half flights time, average. Vetsikas Selman (2003) verified experimentallypredicting prices benefits agents commit flights early greater extent delayflight purchases.29fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIKexpected value perfect prediction, EVPP, expectation VPP respect TACsdistribution client preferences:EVPPffVPP ffff(4)Note , lower values EVPP preferred, , EVPP .(4) see computing EVPP reduces computing ff. derivelatter value follows. (pa,pd) pair, determine best trip hotel best tripff ignoring contribution hotel premium, hp.hotel T, respectively, pricesdetermine threshold value hp (if any) agent would switch T.use boundary split integration surplus (based prices ) trips,respect underlying distribution hp. Note procedure analogous Walverinesmethod calculating expected client demand (Cheng et al., 2004) competitive equilibriumcomputation.8.2 ResultsFigure 1 plots thirteen agents prediction data according two qualitymeasures. one exception, EVPP values shown represent averages 60 gamesTAC-02 finals semifinals. Since kavayaH predicted 45 games, normalized average EVPP values account relative difficulty games omitted comparedgames predicted. normalization multiplied raw average ratio prediction qualitiesgame sets another representative agent (ATTac01, favorable choicekavayaH, though normalizations would produced similar results).two dashed lines Figure 1 represent best-achievable constant predictions respecttwo accuracy measures. Best Euc Dist minimizes average Euclidean distance, indicatedvertical line. EVPP, performed hill-climbing search promising candidate vectors derive local minimum measure, represented horizontal line.reference vectors provided Table 3. Note principle, agents varied predictions across game instances (ATTac, kavayaH, cuhk, Walverine, coarser degree,SouthamptonTAC UMBCTAC) potential perform outside upper-right quadrant.assess significance accuracy rankings among agents, performed paired-samplet-tests pairs agents measures. differences WalverineATTac01 reach threshold statistical significance either measure. Walverine beatsATTac01 fiATTac01 beats Walverine EVPP fi. Walverinefi) outperforms agents measures. ATTac01 significantlysignificantly (fi) outperforms agents EVPP, statistically distinguishable(fi(fi) kavayaH, harami, cuhk. EVPP, Walverine ATTac01agents beat Best EVPP (fifi), Best EVPP turn beatsagents (all cuhk significantly). , Walverine agent significantly (fi)beat Best Euclidean Distance, turn beats every agent ATTac01 kavayaH.agent Walverine significantly better Actual Mean, ATTac01, kavayaH,harami statistically indistinguishable.large discrepancy performance ATTac01 ATTac02 unexpected, givenpredictors generated boosting-based learning algorithm (Stone et al.,30fiTAC P RICE P REDICTIONlivingagentsPackaTACSouthamptonBest Euc. Dist.Expected value perfect prediction6560RoxyBotwhitebearUMBCTACATTac0255SICS50harami4540kavayaHcuhkBest EVPPWalverine35ATTac01190200210220230Euclidean distance actual prices240Figure 1: Prediction quality eleven TAC-02 agents. Dashed lines delimit accuracy achievable constant predictions: best Euclidean distance best EVPP tworespective measures. diagonal line least-squares fit points. Observeorigin graph (190,32).2003). might explained 2002 preliminary rounds somehow less predictiveTAC-02 finals case 2001. relative success another learning agent, kavayaH,evidence this, however. likely hypothesis 2002 agent sufferedbug emerging last-minute change computing environments.directly evaluate prediction form pricelines, would need know initialdemand agent corresponding priceline. obtain information sics,found accuracy priceline prediction according measures far worsebaseline prediction. impression pricelines may well advantageousrespect decisions agents based them, improve basic accuracy. NoteEVPP inherently based interpretation prices linear, thus may provide properevaluation priceline predictions.8.3 Influence Flight PricesObserve three best price predictorsATTac01, Walverine, kavayaHare exactlyagents take flight prices account. Initial flight prices potentially affect hotel pricesinfluence agents early trip choices. theory, lower flight prices increasetendency agents travel days, else equal, thus increasing prices hotels31fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIKcorresponding days stay. Indeed, capturing effect flight prices one mainmotivations Walverines price-equilibrium approach. ATTac kavayaH attempt inducerelationship game data. kavayaHs designers, particular, explored neural network models based hypotheses flights likely affect hotel prices (Putchalaet al., 2002).isolate quantify effect flight prices, investigated contribution differentfactors employed Walverine predictions. defined three additional versions Walverines prediction model, ignores information Walverine takes account:Walv-no-cdata ignores client knowledge, effectively treating demand basedunderlying client preference distribution assumed agents.Walv-constF ignores initial flight prices, assuming set meaninitial flight distribution (i.e., 325) every game instance.Walverine const ignores client knowledge takes flight prices mean ratheractual values. result constant prediction vector, presented Table 3.Figure 2 plots prediction qualities agents. Ignoring client knowledge degraded prediction quality slightly, increasing EVPP 38.0 38.6. Neglecting initial flight prices,however, significantly hurt predictions, increasing EVPP 47.9. Ignoring both, Walverine constincurred average EVPP 49.1.50Walverine_ConstActual_MeanExpected value perfect prediction48Walv_constF46Best_EucDistActual_Median44Best_EVPP4240Walv_no_cdata38Walverine197.5200202.5205207.5210Euclidean distance actual prices212.5215Figure 2: Prediction quality Walverine variants various central tendency benchmarks.results confirm predictive value flight prices. EVPP measure, Walverinegain significant advantage considering client data, cannot beat Best EVPP32fiTAC P RICE P REDICTIONwithout considering initial flight prices. , client data make significant (fifi) difference also considering flight data. Flight data significantly (fi) affects Walverinesprediction quality metric, regardless client data.8.4 Relating Accuracy Performanceindicated scatter plot Figure 1, two accuracy measures highly correlated(). Given EVPP value-based, suggest accuracy translates somewhatproportionally performance. However, EVPP highly idealized proxy actual scores,definitively establish relation prediction accuracy overall TAC performance.relation observed prior work Stone et al. (2003), evaluated predictiveaccuracy overall performance four variations ATTac01 experimental trial. Employing different measure prediction quality above, found average scorerelated monotonically average predictive accuracy.effort directly connect accuracy measures bottom line, regressedactual TAC-02 game scores accuracy measures reported predictionsone datapoint per agent per game. controlled favorability random client preferences, employingsummary statistics used construct client preference adjustment analysisTAC-01 tournament (Wellman et al., 2003). two separate regressions, found highlysignificant coefficients (fi ) EVPP. Predictive accuracy explained score), however, might expected given unmodeledvariance quite poorly (variation across agents.reduce variation, undertook series controlled trials involving variants Walverine (Cheng et al., 2004). trial comprised set games fixed configuration agents.agents constant within trials, varied across, conducted weeks monthsapart Walverine undergoing modifications. trial, regressed actual scorefirst agent EVPP, controlling favorability random client preferences.considered one agent per game, since data points agents would dependentgiven common game instance. results linear regression summarized Table 4.Trial123200151110Mean EVPP70.432.259.5EVPP Coeff-8.89-11.59-10.260.570.260.65Table 4: Regression score EVPP three trials.EVPP coefficient highly significant cases (fi ). Note since EVPPmeasured per-client units scores, direct translation would equate reducing EVPPone increase eight score points. regressions yielded coefficients ranging-8.89 -11.59, take rough confirmation expected relationship. anything,results indicate EVPP understates value predictionwhich might expect sinceaddresses initial trip choice. Interestingly, regression model seems provide better fit(as measured ) trials involving worse price predictors (as measured mean EVPP).suggests prediction optimized, unmodeled factors may relatively greaterincremental influence score.33fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIKnoted games appear quite unrepresentative TAC tournamentgames. Since agents versions Walverine, tend make trip choicesbasis.9. Limitationsimportant emphasize several limitations study, must qualify conclusionsdrawn efficacy prediction methods evaluated here.First, focused exclusively initial price prediction, whereas many agents placedgreater emphasis interim prediction task.Second, many cases represented agents predictions abstraction actualobject produced prediction modules. particular, reduce probability distributionsmeans, consider first unit priceline prediction. generally,account different ways agents apply predictions generate. Without questioneven measure introduce, EVPP, inspired thinking terms Walverine usespredictions. Perhaps measures tailored processes agents would (justifiably) showpredictions favorable light.Third, recognized despite desirability isolating focused componentsagent analysis, complete separation possible principle. Prediction evaluation relativeagent uses prediction, also makes tradeoffs (e.g.,commits flights), ultimately entire strategy, complexity. Studiesmust strive balance benefits decomposition appreciation interconnectionssynergies among elements sophisticated agents behavior.10. Conclusionspresented comprehensive survey approaches initial price prediction TAC-02,quantitative analysis relative accuracy. analysis introduces new measure, expectedvalue perfect prediction, captures important sense instrumental value accuratepredictions, beyond nominal correspondence realized outcomes.draw several conclusions price prediction TAC exercise. First, resultsclearly demonstrate instance-specific information provide significant leverage purebackground history. Three agents use instance-specific information perform better leastone measure constant prediction. particular, initial flight prices provide substantialpredictive value. induced verified empirically, seen successmachine-learning agents. predictive value flows influence flight prices demandhotels, indicated success competitive analysis capturing relationship.believe striking purely analytical approach, without empirical tuning, couldachieve accuracy comparable best available machine-learning method. Moreover, manywould surely skeptical straight competitive analysis could prove successful, givenmanifest unreality assumptions applied TAC. analysis certainly showcompetitive equilibrium best possible model price formation TAC,demonstrate deriving shape market idealized economic theory surprisingly effective.34fiTAC P RICE P REDICTIONseveral advantages model-based reasoning, obviously ability performminimal empirical data. Even historical information available, misleading rely nonstationary environment. tournament setup like TAC naturally violatesstationarity, agent pool evolves time, selection well individual learningdevelopment. course, dealing time-variance, particularly multiagent environments,active area current research, ultimately best methods combine elementsmodel-based data-based reasoning.Finally, suggest present study represents evidence research competitionsright circumstances produce knowledge insights beyond might emerge independent research efforts. hope additional researchers inspired bring innovativeideas trading strategy next TAC, look forward investigating resultsinterplay future work.Acknowledgmentsstudy would possible without generous cooperation TAC-02 entrants.research supported part NSF grant IIS-9988715, well STIET fellowshipthird author, NSF IGERT grant.ReferencesArrow, K. J., & Hahn, F. H. (1971). General Competitive Analysis. Holden-Day, San Francisco.Bose, P., Maheshwari, A., & Morin, P. (2002). Fast approximations sums distances, clusteringFermat-Weber problem. Computational Geometry: Theory Applications, 24,135146.Boyan, J., & Greenwald, A. (2001). Bid determination simultaneous auctions: agent architecture. Third ACM Conference Electronic Commerce, pp. 210212, Tampa, FL.Cheng, S.-F., Leung, E., Lochner, K. M., OMalley, K., Reeves, D. M., & Wellman, M. P. (2004).Walverine: Walrasian trading agent. Decision Support Systems, appear.Fritschi, C., & Dorer, K. (2002). Agent-oriented software engineering successful TAC participation. First International Joint Conference Autonomous Agents Multi-Agent Systems,Bologna.Greenwald, A. (2003a). 2002 trading agent competition: overview agent strategies. AIMagazine, 24(1), 8391.Greenwald, A. (2003b). Bidding uncertainty simultaneous auctions. IJCAI-03 WorkshopTrading Agent Design Analysis, Acapulco.He, M., & Jennings, N. R. (2003). SouthamptonTAC: adaptive autonomous trading agent. ACMTransactions Internet Technology, 3, 218235.Katzner, D. W. (1989). Walrasian Vision Microeconomy. University Michigan Press.Putchala, R. P., Morris, V. N., Kazhanchi, R., Raman, L., & Shekhar, S. (2002). kavayaH: tradingagent developed TAC-02. Tech. rep., Oracle India.Sadeh, N., Arunachalam, R., Eriksson, J., Finne, N., & Janson, S. (2003). TAC-03: supply-chaintrading competition. AI Magazine, 24(1), 9294.35fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIKStone, P. (2002). Multiagent competitions research: Lessons RoboCup TAC. SixthRoboCup International Symposium, Fukuoka, Japan.Stone, P., & Greenwald, A. (2004). first international trading agent competition: Autonomousbidding agents. Electronic Commerce Research, appear.Stone, P., Schapire, R. E., Littman, M. L., Csirik, J. A., & McAllester, D. (2003). Decision-theoreticbidding based learned density models simultaneous, interacting auctions. JournalArtificial Intelligence Research, 19, 209242.Vetsikas, I. A., & Selman, B. (2003). principled study design tradeoffs autonomoustrading agents. Second International Joint Conference Autonomous Agents MultiAgent Systems, pp. 473480, Melbourne.Wellman, M. P., Greenwald, A., Stone, P., & Wurman, P. R. (2003). 2001 trading agent competition. Electronic Markets, 13, 412.36fi