Journal Artificial Intelligence Research 21 (2004) 471-497

Submitted 11/03; published 04/04

Phase Transitions Backbones
Asymmetric Traveling Salesman Problem
Weixiong Zhang

ZHANG @ CSE . WUSTL . EDU

Department Computer Science Engineering
Washington University St. Louis
St. Louis, MO 63130, U.S.A.

Abstract
recent years, much interest phase transitions combinatorial problems.
Phase transitions successfully used analyze combinatorial optimization problems,
characterize typical-case features locate hardest problem instances. paper,
study phase transitions asymmetric Traveling Salesman Problem (ATSP), NP-hard combinatorial optimization problem many real-world applications. Using random instances
1,500 cities intercity distances uniformly distributed, empirically show
many properties problem, including optimal tour cost backbone size, experience
sharp transitions precision intercity distances increases across critical value. experimental results costs ATSP tours assignment problem agree theoretical
result asymptotic cost assignment problem number cities goes infinity. addition, show average computational cost well-known branch-and-bound
subtour elimination algorithm problem also exhibits thrashing behavior, transitioning
easy difficult distance precision increases. results answer positively open question regarding existence phase transitions ATSP, provide guidance difficult
ATSP problem instances generated.

1. Introduction Overview
Phase transitions combinatorial problems thrashing behavior similar phase transitions
combinatorial algorithms drawn much attention recent years (Gomes, Hogg, Walsh, &
Zhang, 2001; Hogg, Huberman, & Williams, 1996; Martin, Monasson, & Zecchina, 2001).
extensively studied so-called spin glass theory (Mezard, Parsi, & Virasoro, 1987)
physics, phase transition refers phenomenon system properties change rapidly
dramatically control parameter undergoes slight change around critical value.
transitions appear large systems. larger system usually exhibits sharper abrupt
phase transitions, leading phenomenon crossover trajectories phase transitions
systems type different sizes.
daily-life example phase transitions water changing ice (solid phase) water
(liquid phase) steam (gas phase) temperature increases. shown many
combinatorial decision problems phase transitions, including Boolean satisfiability (Cheeseman, Kanefsky, & Taylor, 1991; Mitchell, Selman, & Levesque, 1992; Hogg, 1995; Selman &
Kirkpatrick, 1996; Monasson, Zecchina, Kirkpatrick, Selman, & Troyansky, 1999), graph coloring (Cheeseman et al., 1991), symmetric Traveling Salesman Problem (deciding existence complete tour vising given set cities cost less specified value) (Cheeseman et al., 1991; Gent & Walsh, 1996a). Phase transitions used characterize typical-case

c 2004 AI Access Foundation. rights reserved.

fiZ HANG

properties difficult combinatorial problems (Gomes et al., 2001; Martin et al., 2001). hardest
problem instances decision problems appear often points phase transitions.
Therefore, phase transitions used help generate hardest problem instances testing comparing algorithms decision problems (Achlioptas, Gomes, Kautz, & Selman, 2000;
Cheeseman et al., 1991; Mitchell et al., 1992).
Another important useful concept characterizing combinatorial problems
backbone (Kirkpatrick & Toulouse, 1985; Monasson et al., 1999). backbone variable refers
variable fixed value among optimal solutions problem; backbone
variables collectively referred backbone problem. problem backbone
variable, algorithm find solution problem backbone variable set
correct value. Therefore, fraction backbone variables, percentage variables
backbone, reflects constrainedness problem directly affects algorithm searching
solution. larger backbone, tightly constrained problem becomes.
result likely algorithm set backbone variable wrong value, may
consequently require large amount computation recover mistake.
However, research phase transitions (particularly) backbones optimization
problems limited, sharp contrast numerous studies phase transitions
backbones decision problems, represented Boolean satisfiability (e.g., Cheeseman et al.,
1991; Mitchell et al., 1992; Hogg, 1995; Selman & Kirkpatrick, 1996; Monasson et al., 1999).
early work symmetric TSP introduced concept backbones (Kirkpatrick & Toulouse,
1985). However, left question whether exists phase transition TSP,
optimization version problem specific, open since 1985. One best (rigorous)
phase-transition results obtained number partitioning (Borgs, Chayes, & Pittel, 2001),
optimization problem. However, phase transition analyzed Borgs, Chayes, & Pittel (2001),
also experimentally Gent & Walsh (1996b, 1998), existence perfect partition
given set integers, essence decision problem. addition, Gent & Walsh (1996b,
1998) also studied phase transitions size optimal 2-way partition. relationship
phase transitions satisfiability, decision problem, maximum satisfiability,
optimization problem, studied Zhang (2001). experimentally shown backbone
maximum Boolean satisfiability also exhibits phase transitions, emerging nonexistence
almost full size abruptly dramatically. addition, relationship backbones
average-case algorithmic complexity also considered (Slaney & Walsh, 2001).
paper, investigate phase transitions asymmetric Traveling Salesman Problem. Traveling Salesman Problem (TSP) (Gutin & Punnen, 2002; Lawler, Lenstra, Kan, &
Shmoys, 1985) archetypical combinatorial optimization problem one first NP-hard
problems studied (Karp, 1972). Many concepts, backbone (Kirkpatrick & Toulouse, 1985),
general algorithms, linear programming (Dantzig, Fulkerson, & Johnson, 1959), branchand-bound (Little, Murty, Sweeney, & Karel, 1963), local search (Lin & Kernighan, 1973)
simulated annealing (Kirkpatrick, Gelatt, & Vecchi, 1983) first introduced studied using
TSP. problem also often touchstone combinatorial algorithms. Furthermore,
fact many real-world problems, scheduling routing, cast TSPs
made problem practical importance. paper, consider asymmetric TSP (ATSP),
distance one city another may necessarily distance
reverse direction. ATSP general ATSPs difficult solve
symmetric counterparts (Johnson, Gutin, McGeoch, Yeo, Zhang, & Zverovitch, 2002).
472

fiP HASE RANSITIONS



BACKBONES



ATSP

Using general form problem, i.e., ATSP, provide positive answer longstanding open question posted Kirkpatrick & Toulouse (1985) regarding existence phase
transitions problem, disapprove claim made Kirkpatrick & Selman (1994)
Traveling Salesman Problem clear phase transition.
Specifically, using uniformly random problem instances 1,500 cities, empirically
reveal average optimal tour length, accuracy effective lower-bound function problem (the assignment problem, see Martello & Toth (1987)), backbone
ATSP undergo sharp phase transitions. control parameter precision intercity distances typically represented maximum number digits distances. Note
results algorithm independent properties problem. Furthermore, show
average computational cost well-known branch-and-bound subtour elimination algorithm (Balas & Toth, 1985; Bellmore & Malone, 1971; Smith, Srinivasan, & Thompson, 1977)
ATSP exhibits phase-transition thrashing behavior computational cost grows
abruptly dramatically easy difficult distance precision increases. results lead
practical guidance generate large, difficult random problem instances purpose
algorithm comparison.
worthwhile mention besides results Kirkpatrick & Toulouse (1985)
three additional pieces early empirical work related phase transitions Traveling
Salesman Problem. research Zhang & Korf (1996) investigated effects two different distance distributions average complexity subtour elimination algorithm
asymmetric TSP. main result average complexity algorithm controlled
number distinct distances random asymmetric TSP. extend result
Section 6. However, need caution results algorithm specific, may
necessarily reflect intrinsic features underlying problem. research phase transitions Cheeseman, Kanefsky, & Taylor (1991) studied decision version symmetric
TSP (Cheeseman, 1991). thorough investigation issue also carried (Gent
& Walsh, 1996a). Specifically, Gent & Walsh (1996a) analyzed probability tour whose
length less specific value exists given random symmetric euclidean TSP, showing
probability one-to-zero phase transition length desired tour increases. Note
phase-transition results Cheeseman, Kanefsky, & Taylor (1991, 1996a) address
open question Kirkpatrick & Toulouse (1985) optimization version
problem. experimental results Gent & Walsh (1996a) also showed computational
cost branch-and-bound algorithm, unfortunately specified paper, exhibits
easy-hard-easy pattern.
paper organized follows. Section 2, describe ATSP related problem
called assignment problem (AP). investigate parameter controls phase transitions
Section 3, study various phase transitions ATSP Section 4. Section 5 analyze
asymptotic ATSP tour cost, AP cost precision AP heuristic function solving
ATSP number cities grows large number. Section 6, describe well-known
subtour elimination algorithm ATSP, analyze thrashing behavior algorithm.
discuss related work Section 7 finally conclude Section 8.
473

fiZ HANG

2. Asymmetric Traveling Salesman Problem Assignment Problem
Given cities distance cost pair cities, Traveling Salesman Problem
(TSP) find minimum-cost tour visits city returns starting city (Gutin
& Punnen, 2002; Lawler et al., 1985). distance city city

distance
, problem symmetric TSP (STSP). distance city city

necessarily equal reverse distance, problem asymmetric TSP (ATSP).
ATSP difficult STSP, respect optimization approximation (Johnson
et al., 2002). TSPs important NP-hard problems (Garey & Johnson, 1979; Karp, 1972)
many practical applications. Many difficult combinatorial optimization problems,
vehicle routing, workshop scheduling computer wiring, formulated solved
TSPs (Gutin & Punnen, 2002; Lawler et al., 1985).
ATSP formulated integer linear programming problem. Let ff set
cities, fi set pairs cities, distance cost matrix specifying
distance pair cities. following integer linear programming formulation ATSP
well known:

fi !"$#&%(')

+*

,.-/021

(1)

subject

) )
+32? 73 ?

89
&:;ff&<
) - 6571
+324
8/ >:;ff <
) -=6571
.324
- A@CB BD 571E8FHGIff"1J$M
K LN<
- PORQ 1

8S T1U
:Vff

(2)
(3)
(4)
(5)

variables - take values zero one, - W5 arc + T1U
N optimal
tour,
ff . Constraints (2) (3) restrict in-degree out-degree city
one, respectively. Constraints (4) impose subtour elimination constraints complete
tours allowed.
ATSP closely related assignment problem (AP) (Martello & Toth, 1987),
assign city another city
, distance
cost assignment,
total cost assignments minimized. AP relaxation ATSP
assignments need form complete tour. words, removing subtour elimination
constraints (4) representation, integer linear programming formulation
AP. Therefore, AP cost lower bound ATSP tour cost. AP solution happens
complete tour, also solution ATSP. ATSP NP-hard, AP
solved polynomial time, X&+NY precise (Martello & Toth, 1987).

3. Control Parameter
Consider two cases ATSP, one intercity distances
every intercity distance distinct. first case, every complete tour going
474

fiP HASE RANSITIONS



BACKBONES



ATSP

cities optimal tour solution ATSP. backbone variable since removing
one edge optimal solution prevent finding another optimal solution. ATSP
case easy; finding optimal solution require search all. addition, cost
optimal solution also constant, times intercity distance. second
case distances distinct, every complete tour covering cities high probability
distinct cost. Therefore, arc optimal solution almost surely backbone variable
removing may destroy optimal solution. addition, expected difficult find
verify optimal solution among large number suboptimal solutions case.
Therefore, significant differences two extreme cases. One
important differences number distinct distances distance matrix .
precisely, many important characteristics random ATSP, including size backbone
complexity, determined fraction distinct distances among distances. denote
fraction distinct distances distance matrix Z . particularly interested determining Z affects characteristics ATSP gradually increases zero,
distances same, one, distances distinct.
practice, however, directly control number fraction distinct distances
matrix . Besides actual structures layouts cities, precision distances
affects number distinct distances. precision number usually represented
maximal number digits allowed number. even use digital
computer solve ATSP, typically 32 bits (or 4 bytes) integers 64 bits (or 8
bytes) double precision real numbers. result, number digits distances naturally
good choice control parameter.
effect given number digits fraction distinct distances distance matrix
relative problem size . Consider matrix distances uniformly chosen
integers [ Q 1571]\N1.^.^.^71T_ 5` , range _ determined number digits . fixed
number digits , fraction distinct distances larger matrix obviously smaller
smaller . Therefore, control parameter fraction Z distinct distances
must function number digits number cities , denote ZS+b1]a. .
find control parameter, consider number distinct distances given integer
range _ . problem finding number distinct distances equivalent following
bin-ball problem. given c balls _ bins, asked place balls bins.
ball independently put one bins equal probability. interested

fraction bins empty placements. Here, asymmetric TSP c
Fd balls correspond total number nondiagonal distances matrix , _ bins
represent possible integers choose from. Since ball (distance) thrown independently
uniformly one _ bins (integers), probability one bin empty
throwing c balls 5 e5 5gf_hi . expected number occupied bins (distinct distances)
simply _Mj5 e5 5gf_h ilk . Thus, expected fraction distinct distances matrix
_ j 5 e5 5gf_h k
(6)
c
Note c

5 _uswv , since case expectation
mon
number distinct distances approaches c . hand, _ fixed, Zp+b1]aUqxs Q
c goes infinity, since finite number _ bins occupied infinite
mon

pZ +b1]a.Uqr
mon
fixed, Zp+b1]aUqts

number balls case.
475

fiZ HANG

Following convention practice, use decimal values distances. Thus _yz5 Q|{ ,
number digits distances. turns plot ZS+b1]a. a.f}~2UU,+ ,
relatively scale different problem sizes . shown Figure 1(a).

means scaling function effective number digits b+h}~2U + . Function
a.f}~2 U + thus effective number digits controls fraction distinct distances
matrix , denote x+b1]a . also means effective number
digits two different problem sizes, say , range _


different. two problems, _ needs 7 , respectively, giving 9 r .


need point integer range _ also represented number
bases, binary. base use affect results quantitatively, introduces
constant factor results. fact, since alW}~2 U _h , _ range integers
chosen, t+b1]a.hya.f}~2 U +u}~2,p_h , independent base values
intercity distances.
interesting note that, controlled effective number digits a.f}~2U,+ , fraction
distinct entities Z property similar phase transition, also shown Figure 1(a). larger
problem, sharper transition, exists crossover point among transitions
problems different sizes. may examine phase-transition phenomenon closely
using finite-size scaling. Finite-size scaling (Barber, 1983; Wilson, 1979) method
successfully applied phase transitions similar systems different sizes. Based finitesize scaling, around critical parameter (temperature) . , problems different sizes tend
indistinguishable except change scale given power law characteristic length,
, problem size exponent
typically form ]
rescaling factor. Therefore, finite-size scaling characterizes phase transition precisely around
critical point control parameter problem scales infinity. However, analysis
revealed scaling factor large exponent 9 (Zhang, 2003), indicating phase
transitions Figure 1(a) exactly follow power law finite-size scaling.
find correct rescaled control parameter, reconsider (6). sv distance range
_ grows problem size , i.e., _sv ;sv , rewrite (6)



2A }|%(#*

mon

ZS+b1]a.Uq


%|#
}|A
c

_

c

_



5 j e5 5gf_h k


j 5
k 1

(7)





second equation follows }|%|# h e5 5gf_h
. Since underlying control
parameter number digits, alW}~2U2_h , take -W}~2U2_hfc . Asymptotically
Rsv , c , leads -}|~2 U _h \}~2 U ++ \,N}~2 U + . Using - ,
rewrite (7)

2A }|%(#*

mon

ZS+b1]a.UqF5 Q7 j 5

U2

k

(8)

rescaled control parameter Ms v expected number distinct distances
+ \,N}~2 U + . Therefore, critical point 2 rescaling factor }~2 U + . rescaled
phase transition shown Figure 1(b), plots ZS+b1g+ \,N}~2JU,+ .
Note number digits used intercity distances nothing measurement
precision distances. larger number digits, higher precision becomes.
476

fiP HASE RANSITIONS



BACKBONES



ATSP

average fraction distinct numbers

(a) fraction distinct numbers
1
0.8
0.6
0.4
0.2
0

average fraction distinct numbers

n = 100
n = 500
n = 1,000
n = 1,500

0.5

1
1.5
2
2.5
3
3.5
effective number digits
(b) rescaled fraction distinct numbers
1.0
n = 100
n = 500
n = 1,000
0.8
n = 1,500

0.6

0.4
0.2

0

5

4 3 2 1
0
1
2
rescaled effective number digits

3

Figure 1: (a) Average fraction distinct distances matrix , Zp+b1]a , controled effective

number digits, aS}~2U + , ;65 Q2Q 1] Q2Q 15 Q2Q2Q 5 Q2Q . (b) Average Zp+b1]a
finite-size scaling, scaling factor + .]N}|~2 U + , t\ .

agrees common practice using effective digits gain precision. Therefore,
control parameter turn determined precision intercity distances.
Finally, important note even though discussion section focused asymmetric cost matrices ATSP, arguments apply symmetric distance matrices
477

fiZ HANG

symmetric TSP well. is, c revised +d f7\ , asymptotically _ goes
infinity, }|~29U,cP\}~2U,+ , + \,N}|~29U,+ also rescaled control parameter
number distinct distances symmetric cost matrices.

4. Phase Transitions
control parameter, effective number digits t+b1]a. intercity distances, identified, position investigate possible phase transitions ATSP related
assignment problem.
set forth investigate phase transitions, generated studied uniformly random
problem instances 100-, 200-, 300- upto 1,000-cities 1,500-cities. Although
results 100-, 200-, 300-, 1,000-city well 1,500-city problems, make result
figures readable, use data 100-, 500-, 1,000- 1,500-city problems report
results. problem instances considered, intercity distances independently uniformly
chosen [ Q 1571]\N1.^.^.^71T_ 5` given range _ , controlled number digits
. varied 1.0 6.0 instances 1,000-cities 1.0 6.5 instances
1,500-cities. digits incremented 0.1, i.e., used a57 Q 157|57157\N1.^.^.^ .
4.1 Phase Transitions ATSP
particularly interested possible phase transitions ATSP cost, phase transitions
backbones phase transitions numbers ATSP tours. results backbone shed
light intrinsic tightness constraints among cities precision distance
measurement changes.
4.1.1 ATSP C OST
phase transition ATSP tour cost, fi ! , control parameter ,
effective number digits intercity distances. Figure 2(a) shows results 100-, 500-, 1,000and 1,500-city ATSP instances, averaged 10,000 instances data point. reported
tour costs obtained dividing integer ATSP tour costs I_ 5g ,
number cities _ range intercity costs. Equivalently, intercity distance virtually
n
converted real value Q 15q .
shown, ATSP tour cost increases abruptly dramatically effective number digits increases, exhibiting phase transitions. transitions become sharper problem becomes
larger, exist crossover points among curves different problem sizes. finite-size
scaling, determine critical value control parameter phase transitions occur. Following discussion Section 3, scaling factor form + ]N}~2 U + .
numerical result indicated 57 Q \A Q Q2Q . thus use y5 show result
Figure 2(b). worthwhile mention AP cost follows almost phase-transition
pattern ATSP tour cost Figure 2.
4.1.2 BACKBONE



N UMBER



PTIMAL OLUTIONS

turn backbone ATSP, percentage directed arcs appear
optimal solutions. backbone also exhibits phase transitions effective number digits
distances increases. result included Figure 3(a), data point averaged
478

fiP HASE RANSITIONS



BACKBONES



ATSP

(a) ATSP tour cost

average ATSP tour cost

1.6
1.4
1.2
1
0.8
0.6
100city
500city
1,000city
1,500city

0.4
0.2
0

0.5

1
1.5
2
effective number digits
(b) rescaled ATSP tour cost

2.5

normalized average ATSP tour cost

1
0.8
0.6

0.4
100city
500city
1,000city
1,500city

0.2

0

2
1
0
1
2
3
rescaled effective number digits

Figure 2: (a) Average optimal ATSP tour cost. (b) Scaled normalized average optimal tour cost,
rescaling factor + ,]N}~2U,+ >5 .

10,000 problem instances. rescaled result shown Figure 3(b), critical point
=5 . Interestingly, phase-transition pattern backbone follows similar trend
fraction distinct entities distance matrix, shown Figure 1. addition, phasetransition patterns tour costs backbones similar, discussed Section 4.3.
479

fiZ HANG

(a) fraction backbone

average fraction backbone

1
0.8
0.6
0.4
100city
500city
1,000city
1,500city

0.2
0

0.5

1
1.5
2
2.5
effective number digits
(b) rescaled fraction backbone

average fraction backbone

1
0.8
0.6

0.4
100city
500city
1,000city
1,500city

0.2

0

2

1
0
1
2
3
rescaled effective number digits

Figure 3: (a) Average fraction backbone variables. (b) Rescaled average backbone fraction,
rescaling factor + ]N}~2 U + t5 .

fraction backbone variables related number optimal solutions problem.
thus examined total number optimal solutions ATSP. done small ATSPs,
10 cities 150 cities, finding optimal solutions larger problems computationally
expensive. results averaged 100 trials data point. shown Figure 4,
vertical axes logarithmic scale, number optimal tours also undergoes phase
480

fiP HASE RANSITIONS

6



BACKBONES

ATSP

(a) number optimal solutions

10
average number optimal tours



20city
30city
50city
100city
150city

4

10

2

10

0

10

6

0.5

1
1.5
2
2.5
3
effective number bits
(b) rescaled number optimal tours

average number optimal tours

10

20city
30city
50city
100city
150city

4

10

2

10

0

10

1
0
1
2
rescaled effective number digits

Figure 4: (a) Average number optimal ATSP tours. (b) Rescaled average number optimal
ATSP tours, rescaling factor + ,]N}~2 U + >6572 Q Q2Q, .

transition, exponential constant, number digits increases. Note
number digits small, costly find optimal solutions, even relatively small
problems.
fraction backbone variables captures essence tightness constraints among
cities. intercity distances become distinct, number tours distinct lengths
481

fiZ HANG

increases. Consequently, number optimal solutions decreases fraction backbone
variables grows inversely. arcs part backbone, optimal solutions become
restricted. result, number optimal solutions decreases. fraction backbone
variables increases approaches one, number optimal solutions decreases becomes
one well, typically makes problem finding optimal solution difficult.
4.1.3 E XISTENCE H AMILTONIAN C IRCUIT



Z ERO - COST E DGES

precision intercity distances low, likely ATSP tour, complete tour
minimal cost among complete tours, cost zero, meaning exists Hamiltonian
circuit consisting zero-cost arcs. decision problem determine Hamiltonian circuit
exists given ATSP. examined decision problem using set 10,000 problem
instances used Figures 2 3. result shown Figure 5. Notice although follows
rescaling formula + 7]N}~2 U + , critical point transition, Q 2 ,
different critical point gu5 phase transitions backbones ATSP tour
cost, shown Figures 2 3.
4.2 Quality AP Lower-bound Function
existence Hamiltonian circuits zero-cost arcs also indicates number digits
intercity distances small, example, less 1.9 (or _ 7Q ) ;571] Q2Q ,
AP ATSP costs zero, meaning two costs well. useful know
likely AP cost equal ATSP tour cost; answers issue constitutes first
step elucidation accuracy AP lower-bound cost function.
Given random distance matrix , likely AP cost ATSP
tour cost effective number digits increases? answer question examining
probability AP cost fi ! equal corresponding ATSP cost fiP !
increases. Figure 6(a) shows results 100-, 500-, 1,000- 1,500-city ATSP instances,
averaged set 10,000 instances Figure 2 data point. shown
figure, probability fi !Pfib l also experiences abrupt dramatic phase
transitions. Figure 6(b) shows phase transitions finite-size scaling, critical point
57|5 Q Q2Q .
results Figure 6 also imply quality AP lower-bound function degrades
distance precision increases. degradation also follow phase-transition process.
verified Figure 7, using data set problem instances. Note
critical point phase transition accuracy AP Q , different
critical point >57|5 phase transition probability fi !"$fi ! .
4.3 Many Phase Transitions?
far, seen many phase transitions different features ATSP related assignment problem. Qualitatively, phase transitions follow transition pattern, meaning
captured finite-size rescaling formula + U]N}|~2 U + ,
critical point depending particular feature interest.
interesting note critical points phase transitions ATSP tour costs
fractions backbone variables x5 . close examination also indicates two
phase transitions follow almost phase transition, shown Figure 8, rescaled
482

fiP HASE RANSITIONS



BACKBONES



ATSP

(a) probability Hamiltonian circuit
probability Hamiltonian circuit

1

100city
500city
1,000city
1,500city

0.8
0.6
0.4
0.2

probability Hamiltonian circuit

0

0.5
1
1.5
effective number digits
(b) rescaled prob. Hamiltonian circuit
1
100city
500city
1,000city
0.8
1,500city

0.6

0.4
0.2

0

1 0.5
0
0.5
1
1.5
rescaled effective number digits

Figure 5: (a) Probability existence Hamiltonian circuits zero cost arcs. (b) Rescaled
probability zero-cost Hamiltonian circuits, rescaling factor + TN}~2 U +
=x Q 2 .

curves ATSP tour cost fraction backbone variables drawn 1,500-city
ATSP, averaged 10,000 problem instances.
483

fiZ HANG

normalizedaverage prob. AP(D)=ATSP(D)

average probability AP(D) = ATSP(D)

(a) probability AP(D) = ATSP(D)
1

100city
500city
1,000city
1,500city

0.8
0.6
0.4
0.2
0

0.5

1
1.5
2
2.5
effective number digits
(b) rescaled probability AP(D)=ATSP(D)

1

100city
500city
1,000city
1,500city

0.8
0.6

0.4
0.2

0
3

2
1
0
1
2
rescaled effective number digits

3

Figure 6: (a) Average probability fi !6fio l . (b) Average probability finitesize scaling, rescaling factor + ]N}~2 U + >657|5 Q Q2Q .

Except close similarity phase transitions ATSP tour cost fraction
backbone variables, phase transitions different critical points, indicating
undergo type phase transitions different ranges.
484

finormalized ave. relative error AP(D)

average relative error AP(D) ATSP(D)

P HASE RANSITIONS



BACKBONES



ATSP

(a) precision AP function
1.4

100city
500city
1,000city
1,500city

1.2
1
0.8
0.6
0.4
0.2
0

0.5

1
1.5
2
2.5
effective number digits
(b) rescaled accuracy AP function

1
0.8
0.6
0.4
100city
500city
1,000city
1,500city

0.2
0
2

1
0
1
2
3
rescaled effective number digits

Figure 7: (a) Average accuracy AP lower-bound function, measured error AP cost
relative ATSP cost. (b) normalized rescaled average accuracy, rescaling factor
+ =]N}~2=U,+ > Q .

5. Asymptotic ATSP Tour Length AP Precision
by-product phase-transition results, provide numerical values ATSP
cost, AP cost accuracy, asymptotically number cities grows. attempt
485

finormalized cost fraction backbone

Z HANG

1
0.8
0.6
0.4
0.2
0

tour cost
backbone
2

1
0
1
2
3
rescaled effective number digits

Figure 8: Simultanous examination phase transitions backbone ATSP tour cost
1,500-city problems, rescaled + 5gN}~2 U + .

extend previous theoretical results AP cost, shown asymptotically approach
f (Aldous, 2001; Mezard & Parsi, 1987), observations relative error AP
lower bounds decreases problem size increases (Balas & Toth, 1985; Smith et al., 1977).
every real number represented digital computer. Thus, infeasible directly
examine theoretical result reals using digital computer. purpose, hand,
phase-transition results indicate precision intercity distances high enough,
quantities ATSP examined, including ATSP cost, AP cost
precision lower-bound cost function, well backbone, relatively stable, sense
change significantly even precision intercity distances increases further.
Therefore, sufficient use high distance precision experimentally analyze asymptotic
properties ATSP cost related quantities.
need cautious selecting number digits intercity distances. discussed
Section 3, number digits intercity distances gives rise different effective numbers digits problems different sizes. Furthermore, phase transition results Section 4
indicate effective numbers digits must scaled properly order effect
problems different sizes investigate asymptotic feature.
Therefore, experiments, fixed scaled effective number digits intercity distances, + ]N}~2 U + , constant. Based phase-transition results, especially
control parameter Figure 1, chose take + \,N}~2 U + constant 2.1, two reasons.
First, + \,N}~2 U +"\N|5 sufficiently large almost distances distinct, regardless
problem size, quantities examine change substantially finite-size
scaling. Secondly, + \,N}~2NU,+bM\N|5 relatively small experiment problems
large sizes. save memory much possible, intercity distances integers 4 bytes
implementation subtour elimination algorithm. Thus number digits must less
486

fiP HASE RANSITIONS

n
200
400
600
800
1,000
1,200
1,400
1,600
2,000
2,200
2,400
2,600
2,800
3,000

digits
6.7021
7.3041
7.6563
7.9062
8.1000
8.2584
8.3923
8.5082
8.7021
8.7848
8.8604
8.9299
8.9943
9.0542



AP cost
1.63533 0.00254
1.63942 0.00180
1.64072 0.00146
1.64227 0.00125
1.64297 0.00114
1.64284 0.00104
1.64313 0.00096
1.64319 0.00090
1.64382 0.00082
1.64372 0.00077
1.64360 0.00074
1.64429 0.00071
1.64382 0.00068
1.64421 0.00065

BACKBONES



ATSP

ATSP cost
relative AP error (%)
1.64302 0.00254
0.46817 0.00970
1.64311 0.00180
0.22485 0.00468
1.64314 0.00145
0.14765 0.00317
1.64407 0.00125
0.10904 0.00237
1.64441 0.00114
0.08754 0.00191
1.64402 0.00105
0.07187 0.00158
1.64413 0.00096
0.06148 0.00139
1.64405 0.00090
0.05276 0.00117
1.64451 0.00082
0.04231 0.00095
1.64434 0.00077
0.03813 0.00085
1.64417 0.00073
0.03477 0.00079
1.64481 0.00071
0.03234 0.00074
1.64430 0.00068
0.02966 0.00068
1.64463 0.00065
0.02548 0.00061

Table 1: Numerical results AP cost, ATSP cost AP error relative ATSP cost,
percent. cost matrices uniformly random. data point averaged 10,000
problem instances. table, number cities, digits number digits
intercity distances, numerical error bounds represent 95 percent confidence
intervals.

9.4 without causing overflow worst case. Using + \,N}~2U2+\N|5 , go
roughly 3,000-city ATSPs.
Table 1 shows experimental results, 3,000 cities, average AP cost,
ATSP tour cost, accuracy AP cost function error AP cost relative ATSP
cost. results averaged 10,000 instances problem size. Based results,
AP cost approaches 1.6442 ATSP cost 1.6446. Note experimental AP cost
1.6442 close theoretical asymptotic AP cost dgf 57 2 (Aldous, 2001;
Mezard & Parsi, 1987). addition, accuracy AP function indeed improves problem
size increases, reduced 0.02548% 3,000-city problem instances. result supports
previous observations (Balas & Toth, 1985; Smith et al., 1977).

6. Thrashing Behavior Subtour Elimination
phase-transition results discussed previous section indicate ATSP becomes
constrained difficult distance precision becomes higher. section, study
well-known algorithm ATSP, branch-and-bound subtour elimination (Balas & Toth,
1985; Bellmore & Malone, 1971; Smith et al., 1977), behaves. separate issue
phase transition phenomena studied consider section behavior particular algorithm, may necessarily feature underlying problem.
Nevertheless, still issue interest algorithm oldest still
487

fiZ HANG

among best known methods ATSP, hope better understanding efficient
algorithm ATSP shed light typical case computational complexity problem.
6.1 Branch-and-bound Subtour Elimination
branch-and-bound (BnB) subtour algorithm elimination (Balas & Toth, 1985; Bellmore & Malone, 1971; Smith et al., 1977) solves ATSP state-space search (Pearl, 1984; Zhang, 1999)
uses assignment problem (AP) lower-bound cost function. BnB search takes
original ATSP root state space repeats following two steps. First, solves
AP current problem. AP solution complete tour, decomposes subproblems subtour elimination breaks subtour excluding arcs selected subtour.
subproblem constrained parent problem, AP cost subproblem must
much parent. means AP cost function monotonically nondecreasing. solving AP requires X&+NY computation general, AP child node
incrementally solved X +Nd time based solution AP parent.
many heuristics selecting subtour eliminate (Balas & Toth, 1985), use
Carpaneto-Toth scheme (Carpaneto & Toth, 1980), CT scheme short, algorithm.
One important feature CT scheme generates duplicate subproblem
overall search space tree. One example scheme shown Figure 9. AP solution
original ATSP contains two subtours root tree figure. subtour
\ \ chosen eliminated, since shorter subtour. two ways
break selected subtour, i.e., excluding directed arc \N1], N1]\, . Assume first exclude
\N1], N1]\, , generating two subproblems, nodes fi Figure 9. generating
second subproblem , deliberately include \N1], solution. including arc
excluded previous subproblem fi , force exclude current subproblem
solutions original problem appear fi , therefore form partition solution
space using fi . general, let excluded arc set, included arc set
problem decomposed. Assume arcs selected subtour, [ 1 1.^.^.^1 . ` ,

. CT scheme decomposes problem child subproblems, -th
one excluded arc set . included arc set 9 ,

N
F

;[ `,1
(9)
V;[ 1.^.^.^g1 `,1 &571]\N1.^.^.^1J
Since excluded arc -th subproblem, :!9 , included arc H5 -st
subproblem, ;:F. , complete tour obtained -th subproblem contain arc
, tour obtained 5 -st subproblem must arc . Thus tour -th
subproblem cannot generated &5 -st subproblem, vice versa. summary,
state space ATSP BnB using CT subtour elimination scheme represented
tree without duplicate nodes.
next step, algorithm selects current problem new subproblem set
active subproblems, ones generated yet expanded. process
continues unexpanded problem, unexpanded problems costs greater
equal cost best complete tour found far.
Thanks linear-space requirement, use depth-first branch-and-bound (DFBnB)
algorithm. DFBnB explores active subproblems depth-first order. uses upper bound
488

fiP HASE RANSITIONS

1

4

5

6



2



4

2

6

3

5



ATSP

3
E={(3,2)}
I={(2,3)}

E={(2,3)}
I={}
1

BACKBONES

B
2

3 1
6

4
5
E={(3,2),(3,6)}
I={(2,3),(6,2)}

E={(3,2),(6,2)}
I={(2,3)}

C



2

3

6

6

2

3

4

1

5

4

1

5

Figure 9: DFBnB subtour elimination ATSP.
optimal cost, whose initial value infinity cost approximate solution,
one obtained Karps patching algorithm (Karp, 1979; Karp & Steele, 1985), repeatedly
patches two smallest subtours big one complete tour forms. Starting root node,
DFBnB selects recently generated node - examine next. AP solution - complete
tour, - leaf node search tree. cost leaf node less current upper
bound , revised cost - . - AP solution complete tour cost greater
equal , - pruned, node costs monotonic descendant -
cost smaller - cost. Otherwise, - expanded, generating child nodes. find
optimal goal node quickly, children - searched increasing order
costs. words use node ordering reduce number nodes explored. speed
process reaching better, possibly optimal, solution, also apply Karps patching algorithm
best child node current node.
algorithm principle algorithm Carpaneto, DellAmico & Toth (1995),
probably best known complete algorithm ATSP. main difference
two that, due consideration space requirement, use depth-first search Carpaneto,
DellAmico & Toth (1995) used best-first search.
6.2 Thrashing Behavior
average computational complexity BnB subtour elimination algorithm determined
two factors, problem size, number cities, number digits used intercity
distances. Figure 10 illustrates average complexity, measured number calls AP
function, logarithmic scale. result averaged 10,000 problem instances
data point used phase transitions studied Section 4. Note number AP
calls increases exponentially small problems large ones generated using
number digits distances.
489

fiZ HANG

4

average number AP calls

10

3

10

2

10

1

100city
500city
1,000city
1,500city

10

0

10

1

2

3
4
5
number digits

6

Figure 10: Average computational complexity BnB subtour elimination algorithm.

characterize thrashing behavior algorithm, normalize result Figure 10
way given problem size, minimal maximal AP calls among problem
instances size mapped zero one, respectively, AP calls
proportionally adjusted ratio 0 1. allows us compare results
different problem sizes one figure. also normalize number digits distances
problem size. curves Figure 11(a) follow pattern similar phase transitions
Section 4. complexity subtour elimination algorithm increases effective number
digits, exhibits thrashing behavior similar phase transitions. Indeed, use finitesize scaling capture behavior problem size grows, illustrated Figure 11(b).
results Figure 11 phase-transition results Section 4 indicate complexity
subtour elimination algorithm goes hand-in-hand accuracy AP function
constrainedness problem, determined portion distinct entities distance
matrix, turn controlled precision distances.
Similar results reported Zhang & Korf (1996), effects two different
distance distributions average complexity subtour elimination algorithm analyzed conclude determinant average complexity number distinct distances
problem. results section extend Zhang & Korf (1996) different sizes
problems applying finite-size scaling capture thrashing behavior problem size
increases.
need contrast experimental result section theoretical result
NP-completeness TSP intercity distances 0 1. known degenerated
TSP distances 0 1 still NP-complete (Papadimitriou & Yannakakis, 1993).
hand, experimental results showed intercity distances small, relative
problem size, ATSP easy average. Based experimental result, large portion
problem instances small intercity distances solved assignment problem Karps
490

fiP HASE RANSITIONS



BACKBONES



ATSP

normalized average number AP calls

normalized average number AP calls

(a) normalized number AP calls
1
0.8
0.6
0.4
100city
500city
1,000city
1,500city

0.2
0

0.5

1
1.5
2
2.5
effective number digits
(b) rescaled normalized # AP calls

1
0.8
0.6

0.4
100city
500city
1,000city
1,500city

0.2

0
4

3
2
1
0
1
rescaled effective number digits

2

Figure 11: (a) Normalized average number AP calls DFBnB subtour elimination. (b) Scaled
average number AP calls, + 2]N}|~2 U + , 57 Q Q \2 .

patching algorithm branch-and-bound search required. discrepancy indicates
worst case problem rare likely pathological.
491

fiZ HANG

7. Related Work Discussions
Two lines previous work directly influenced inspired research. first line
related work expected complexity tree search, shed light BnB subtour
elimination algorithm described Section 6.1 solves ATSP tree search. analysis
carried abstract random tree model called incremental tree (Karp & Pearl, 1983;
McDiarmid, 1990; McDiarmid & Provan, 1991; Zhang & Korf, 1995; Zhang, 1999). internal
nodes variable number children edges assigned finite nonnegative
random values. cost node sum edge costs along path root
node. optimal goal node node minimum cost fixed depth . overall goal
find optimal goal node.
exist phase transitions cost optimal goal node complexity
problem finding optimal goal . control parameter expected number child
nodes common parent node cost parent. cost optimal goal
node almost surely undergoes phase transition linear function depth constant
expected same-cost children node increases beyond one. Meanwhile, best-first search
depth-first branch-and-bound also exhibit phase-transition behavior, i.e., expected complexity
changes dramatically exponential polynomial expected same-cost children
node reduced one. Note following result Dechter & Pearl (1985), best-first
search optimal searching random tree among algorithms using cost function,
terms number node expansions, tie breaking. Thus, results also give
expected complexity problem searching incremental tree.
second line related research characterizing assignment problem (AP)
lower-bound cost function relationship ATSP, research interest
long time (Aldous, 2001; Coppersmith & Sorkin, 1999; Frieze, Karp, & Reed, 1992; Frieze &
Sorkin, 2001; Karp, 1987; Karp & Steele, 1985; Mezard & Parsi, 1987; Walkup, 1979). first
surprising result (Walkup, 1979) expected AP cost approaches constant number
cities goes infinity entries distance matrix independent uniform reals
n
Q 15q . constant subject long history pursuit. shown rigorously,
based rigorous replica method statistical physics (Mezard et al., 1987), optimal cost
random assignment approaches asymptotically df (Aldous, 2001), approximately
1.64493. results Section 5 show AP ATSP costs approach 1.64421
1.64463, respectively, agree theoretical results AP cost.
importantly, relationship AP cost ATSP cost remarkably
different characteristics different distance distributions. one extreme, AP cost
ATSP cost high probability, extreme, differ
ATSP cost, high probability, function problem size . Let fi ! AP cost
fiP ! ATSP cost distance matrix . expected number zeros
v , fi !Vfio l probability
row approaches infinity us
tending one (Frieze et al., 1992). However, entities uniform integers
n
Q 1571.^.^.^1+ rq , fi !!fib l probability going zero, grows
n
infinity (Frieze et al., 1992). Indeed, entities uniform Q 15q ,

fio l fi ! f , positive constant (Frieze & Sorkin, 2001).
previous results indicate quality AP function varies significantly, depending
underlying distance distribution. Precisely, difference AP cost ATSP
492

fiP HASE RANSITIONS



BACKBONES



ATSP

cost two phases, controlled number zero distances distance matrix . one
phase, difference zero high probability, phase, expectation
difference function problem size . experimental results Section 4 adds
analysis existence phase transition two phases.
two-phase result accuracy AP cost function also principle consistent
phase-transition result incremental random trees. root search tree cost equal
AP cost fio l problem optimal goal node ATSP tour cost fiP ! .
subtract AP cost root every node ATSP search tree, root node
cost zero optimal goal node cost equal fib l fi ! . large
number zero distances , large number same-cost children, AP cost
child node search tree likely AP cost parent, since AP
tend use zero distances. Therefore, expected nodes search tree
one child node cost parents.
addition phase transitions combinatorial problems mentioned Section 1,
related previous results. Results scaling search cost constrainedness symmetric TSP considered Gent, MacIntyre, Prosser, & T. Walsh (1997). Phase transitions
Hamiltonian circuit studied Frank, Gent, & Walsh (1998). also shown
hard generate difficult Hamiltonian cycle problem instances (Vandegriend & Culberson, 1998).
addition, concept backbones studied many problems different names.
examples, unary prime implicate refers variable must set fixed value
instance Boolean satisfiability (Parkes, 1997); frozen development describes pair nodes
must share colors graph coloring problem (Culberson & Gent, 2001).

8. Conclusions
main contributions research twofold. First, answered positively long-standing
question Traveling Salesman Problem (TSP) phase transitions (Kirkpatrick & Toulouse,
1985) disapproved belief problem phase transition (Kirkpatrick
& Selman, 1994). studied issue general, optimization version problem,
asymmetric TSP (ATSP). empirically showed, using random problem instances distances uniform distribution, many important properties, including ATSP tour cost
fraction backbone variables, two characteristically different values, transitions rather abrupt dramatic, displaying phase-transition phenomenon.
control parameter phase transitions effective number digits representing intercity
distances precision distance measure.
Second, results provide practical guidance generate difficult random ATSP
problem instances random instances used comparing asymptotic performance ATSP algorithms. current common practice comparing algorithms using
random ensemble generate problem instances different sizes fixed distance precision.
phase transition results indicate correct way use instances different sizes
similar features fraction backbone variables. also important point locations hardest, albeit random, problem instances typically depend
distance distribution used. case uniform distribution, requires increasing precision
intercity distances problem size grows.
493

fiZ HANG

important note exact locations various phase transitions presented remain
mathematically determined, using methods probably statistical physics (Martin et al.,
2001; Mezard et al., 1987).
like conclude pointing phase transition results paper general.
argument control parameter Section 3 general applicable symmetric
TSP (STSP). unpublished data also showed phase transitions STSP. results
ATSP uniformly distributed distances hold types intercity distances.
part supported previous investigation intercity distances chosen lognormal distribution (Zhang & Korf, 1996). Finally, believe phase transitions persist
structured TSPs long intercity distances independently drawn common distribution.
TSPs include proposed studied Cirasella et al., (2001) Johnson et al., (2002),
examples, problem instances constraints triangle inequalities, instances converted
particular applications disk drive optimization, jobshop scheduling, coin collecting optimization, etc.

Acknowledgments
research supported part NSF grants IIS-0196057 ITR/EIA-0113618, part
DARPA Cooperative Agreements F30602-00-2-0531 F33615-01-C-1897. Thanks Sharlee
Climer joint work algorithm finding backbone using limit crossing (Climer &
Zhang, 2002) critical reading draft. Thanks also Scott Kirkpatrick David Johnson comments draft. Special thanks Allon Percus Sergey Knysh constructive
comments suggestions, especially finite-size scaling, significantly improved
paper. Thanks also go anonymous reviewers excellent comments. early results
presented NSF/IPAM Workshop Phase Transitions Algorithmic Complexity, June
3-5, 2002, 18-th International Joint Conference Artificial Intelligence (IJCAI-03),
Acapulco, Mexico, Aug. 9-15, 2003 (Zhang, 2003).

References
Achlioptas, D., Gomes, C., Kautz, H., & Selman, B. (2000). Generating satisfiable instances.
Proceedings 17th National Conference Artificial Intelligence (AAAI-02).
Aldous, D. J. (2001). =\, limit random assignment problem. Random Structures
Algorithms, 18, 381418.
Balas, E., & Toth, P. (1985). Branch bound methods. Traveling Salesman Problem, pp.
361401. John Wiley & Sons, Essex, England.
Barber, M. N. (1983). Finite-size scaling. Phase Transitions Critical Phenomena, Vol. 8, pp.
145266. Academic Press.
Bellmore, M., & Malone, J. C. (1971). Pathology traveling-salesman subtour-elimination algorithms. Operations Research, 19, 278307.
Borgs, C., Chayes, J. T., & Pittel, B. (2001). Phase transition finite-size scaling integer
partitioning problem. Random Structures Algorithms, 19, 247288.
494

fiP HASE RANSITIONS



BACKBONES



ATSP

Carpaneto, G., DellAmico, M., & Toth, P. (1995). Exact solution large-scale, asymmetric Traveling Salesman Problems. ACM Trans. Mathematical Software, 21, 394409.
Carpaneto, G., & Toth, P. (1980). new branching bounding criteria asymmetric
traveling salesman problem. Management Science, 26, 736743.
Cheeseman, P. (1991). Personal communications..
Cheeseman, P., Kanefsky, B., & Taylor, W. M. (1991). really hard problems are. Proc.
12th International Joint Conference Artificial Intelligence (IJCAI-91), pp. 331337.
Cirasella, J., Johnson, D., McGeoch, L. A., & Zhang, W. (2001). asymmetric traveling salesman
problem: Algorithms, instance generators, tests. Proc. 3rd Workshop Algorithm
Engineering Experiments (ALENEX-2001).
Climer, S., & Zhang, W. (2002). Searching backbones fat: limit-crossing approach
applications. Proc. 18th National Conference Artificial Intelligence (AAAI-02), pp.
707712.
Coppersmith, D., & Sorkin, G. B. (1999). Constructive bounds exact expectations
random assignment problem. Random Structures Algorithms, 15, 113144.
Culberson, J., & Gent, I. (2001). Frozen development graph coloring. Theoretical Computer
Science, 265, 227264.
Dantzig, G. B., Fulkerson, D. R., & Johnson, S. M. (1959). linear programming, combinatorial
approach traveling salesman problem. Operations Research, 7, 5866.
Dechter, R., & Pearl, J. (1985). Generalized best-first search strategies optimality .
Journal ACM, 32, 505536.
Frank, J., Gent, I., & Walsh, T. (1998). Asymptotic finite size parameters phase transitions:
Hamiltonian circuit case study. Information Processing Letters, 65(5), 241245.
Frieze, A., Karp, R. M., & Reed, B. (1992). assignment bound asymptotically tight
asymmetric traveling-salesman problem?. Proc. Integer Programming Combinatorial Optimization, pp. 453461.
Frieze, A., & Sorkin, G. B. (2001). probabilistic relationship assignment
asymmetric traveling salesman problems. Proc. SODA-01, pp. 652660.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide Theory
NP-Completeness. Freeman, New York, NY.
Gent, I., MacIntyre, E., Prosser, P., & Walsh, T. (1997). scaling search cost. Proc.
14th National Conference Artificial Intelligence (AAAI-96), pp. 315320.
Gent, I., & Walsh, T. (1996a). TSP phase transition. Artificial Intelligence, 88, 349358.
Gent, I., & Walsh, T. (1996b). Phase transitions annealed theories: Number partitioning
case study. Proceedings 12th ECAI.
Gent, I., & Walsh, T. (1998). Analysis heuristics number partitioning. Computational Intelligence, 14(3), 430451.
Gomes, C. P., Hogg, T., Walsh, T., & Zhang, W. (2001). IJCAI-2001 tutorial: Phase transitions
structure combinatorial problems. http://www.cs.wustl.edu/ zhang/links/ijcai-phasetransitions.html.
495

fiZ HANG

Gutin, G., & Punnen, A. P. (Eds.). (2002). Traveling Salesman Problem Variations.
Kluwer Academic Publishers.
Hogg, T. (1995). Exploiting problem structure search heuristic. Tech. rep., Xerox PARC.
Hogg, T., Huberman, B. A., & Williams, C. (1996). Phase transitions search problem.
Artificial Intelligence, 81, 115.
Johnson, D. S., Gutin, G., McGeoch, L. A., Yeo, A., Zhang, W., & Zverovitch, A. (2002).
Traveling Salesman Problem Variations, chap. Experimental Analysis Heuristics
ATSP, pp. 445487. Kluwer Academic Publishers, Dordrecht.
Karp, R. M. (1972). Reducibility among combinatorial problems. Miller, R. E., & Thatcher, J. W.
(Eds.), Comlexity Computer Computations, pp. 85103. Plenum Press.
Karp, R. M. (1979). patching algorithm nonsymmetric traveling-salesman problem. SIAM
Journal Computing, 8, 561573.
Karp, R. M. (1987). upper bound expected cost optimal assignment. Johnson,
D. (Ed.), Discrete Algorithms Complexity: Proc. Japan-US Joint Seminar, pp. 14,
New York. Academic Press.
Karp, R. M., & Pearl, J. (1983). Searching optimal path tree random costs. Artificial
Intelligence, 21, 99117.
Karp, R. M., & Steele, J. M. (1985). Probabilistic analysis heuristics. Traveling Salesman
Problem, pp. 181205. John Wiley & Sons, Essex, England.
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. (1983). Optimization simulated annealing. Science,
220, 671680.
Kirkpatrick, S., & Selman, B. (1994). Critical behavior satisfiability random boolean
expressions. Science, 264(5163), 12971301.
Kirkpatrick, S., & Toulouse, G. (1985). Configuration space analysis traveling salesman problems. J. Phys. (France), 46, 12771292.
Lawler, E. L., Lenstra, J. K., Kan, A. H. G. R., & Shmoys, D. B. (Eds.). (1985). Traveling
Salesman Problem. John Wiley & Sons, Essex, England.
Lin, S., & Kernighan, B. W. (1973). effective heuristic algorithm Traveling Salesman
Problem. Operations Research, 21, 498516.
Little, J. D. C., Murty, K. G., Sweeney, D. W., & Karel, C. (1963). algorithm traveling
salesman problem. Operations Research, 11, 972989.
Martello, S., & Toth, P. (1987). Linear assignment problems. Annals Discrete Mathematics, 31,
259282.
Martin, O. C., Monasson, R., & Zecchina, R. (2001). Statistical mechanics methods phase
transitions optimization problems. Theoretical Computer Science, 265, 367.
McDiarmid, C. J. H. (1990). Probabilistic analysis tree search. Gummett, G. R., & Welsh, D.
J. A. (Eds.), Disorder Physical Systems, pp. 249260. Oxford Science.
McDiarmid, C. J. H., & Provan, G. M. A. (1991). expected-cost analysis backtracking
non-backtracking algorithms. Proc. 12th International Joint Conference Artificial
Intelligence (IJCAI-91), pp. 172177, Sydney, Australia.
496

fiP HASE RANSITIONS



BACKBONES



ATSP

Mezard, M., & Parsi, G. (1987). solution random link matching problem. J. Physique,
48, 14511459.
Mezard, M., Parsi, G., & Virasoro, M. A. (Eds.). (1987). Spin Glass Theory Beyond. World
Scientific, Singapore.
Mitchell, D., Selman, B., & Levesque, H. (1992). Hard easy distributions SAT problems.
Proc. 10th National Conference Artificial Intelligence (AAAI-92), pp. 459465.
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determining
computational complexity characteristic phase transitions. Nature, 400, 133137.
Papadimitriou, C. H., & Yannakakis, M. (1993). travelling salesman problem distances
one two. Math. Oper. Res., 18, 111.
Parkes, A. (1997). Clustering phase transitions. Proc. 14th National Conference
Artificial Intelligence (AAAI-97).
Pearl, J. (1984). Heuristics: Intelligent Search Strategies Computer Problem Solving. AddisonWesley, Reading, MA.
Selman, B., & Kirkpatrick, S. (1996). Critical behavior computational cost satisfiability
testing. Artificial Intelligence, 81, 273295.
Slaney, J., & Walsh, T. (2001). Backbones optimization approximation. Proc. 17th
International Joint Conference Artificial Intelligence (IJCAI-01), pp. 254259.
Smith, T. H. C., Srinivasan, V., & Thompson, G. L. (1977). Computational performance three
subtour elimination algorithms solving asymmetric traveling salesman problems. Annals
Discrete Mathematics, 1, 495506.
Vandegriend, B., & Culberson, J. (1998). gn,m phase transition hard hamiltonian
cycle problem. J. Artificial Intelligence Research, 9, 219245.
Walkup, D. W. (1979). expected value random assignment problem. SIAM Journal
Computing, 8, 440442.
Wilson, K. G. (1979). Problems physics many scales length. Scientific American, 241,
158179.
Zhang, W. (1999). State-space Search: Algorithms, Complexity, Extensions, Applications.
Springer.
Zhang, W. (2001). Phase transitions backbones 3-SAT maximum 3-SAT. Proc. Intern.
Conf. Principles Practice Constraint Programming (CP-01), pp. 153167.
Zhang, W. (2003). Phase transitions asymmetric traveling salesman. Proc. 18th International Joint Conference Artificial Intelligence (IJCAI-03), pp. 12021207.
Zhang, W., & Korf, R. E. (1995). Performance linear-space search algorithms. Artificial Intelligence, 79, 241292.
Zhang, W., & Korf, R. E. (1996). study complexity transitions asymmetric Traveling
Salesman Problem. Artificial Intelligence, 81, 223239.

497

fiJournal Artificial Intelligence Research 21 (2004) 579-594

Submitted 08/03; published 05/04

Learn Beat Best Stock
Allan Borodin

bor@cs.toronto.edu

Department Computer Science
University Toronto
Toronto, ON, M5S 3G4 Canada

Ran El-Yaniv

rani@cs.technion.ac.il

Department Computer Science
Technion - Israel Institute Technology
Haifa 32000, Israel

Vincent Gogan

vincent@cs.toronto.edu

Department Computer Science
University Toronto
Toronto, ON, M5S 3G4 Canada

Abstract
novel algorithm actively trading stocks presented. traditional expert
advice universal algorithms (as well standard technical trading heuristics) attempt
predict winners trends, approach relies predictable statistical relations
pairs stocks market. empirical results historical markets provide strong
evidence type technical trading beat market moreover, beat
best stock market. utilize new idea smoothing critical
parameters context expert learning.

1. Introduction
portfolio selection (PS) problem challenging problem machine learning, online
algorithms and, course, computational finance. well known (e.g. see Lugosi, 2001)
sequence prediction log loss measure viewed special case portfolio selection, perhaps surprisingly, certain worst case minimax criterion,
portfolio selection essentially harder (than prediction) shown (Cover & Ordentlich, 1996) (see also Lugosi, 2001, Thm. 20 & 21). seems qualitative
difference practical utility universal sequence prediction universal
portfolio selection. Simply stated, universal sequence prediction algorithms various
probabilistic worst-case models appear work well practice whereas known
universal portfolio selection algorithms seem provide substantial benefit
naive investment strategy (see Section 5).
major pragmatic question whether computer program consistently
outperform market. closer inspection interesting ideas developed information theory online learning suggests promising approach exploit natural
volatility market particular benefit simple rather persistent statistical relations stocks rather try predict stock prices winners.

c
2004
AI Access Foundation. rights reserved.

fiBorodin, El-Yaniv, & Gogan

present non-universal portfolio selection algorithm1 , try predict
winners. motivation behind algorithm rationale behind constant rebalancing
algorithms worst case study universal trading introduced Cover (1991).
proposed algorithm substantially beat market historical markets,
also beats best stock. presenting algorithm simply
making money? are, course caveats obstacles utilizing algorithm.
large investors possibility goose laying silver (if golden) eggs
perhaps impossible.

2. Portfolio Selection Problem
Assume market stocks. Let vt = (vt (1), . . . , vt (m)) daily closing prices2
stocks tth day, vt (j) price jth stock. convenient
work relative prices xt (j) = vt (j)/vt1 (j) investment $d jth
stock tth day yields dxt (j) dollars. let xt = (xt (1), . . . , xt (m)) denote
market vector relative prices corresponding tth day. portfolio b allocation
wealth stocks, specified proportions b = (b(1),
P . . . , b(m)) current dollar
wealth invested stocks, b(j) P
0 j b(j) = 1. daily return
portfolio b w.r.t. market vector x b x = j b(j)x(j) (compound) total
return, retX (b1 , . . .Q
, bn ), sequence portfolios b1 , . . . , bn w.r.t. market sequence
X = x1 , . . . , xn nt=1 bt xt . portfolio selection algorithm deterministic
randomized rule specifying sequence portfolios let retX (A) denote total
return market sequence X.
simplest strategy buy-and-hold stocks using portfolio b. denote
strategy BAHb let U-BAH denote uniform buy-and-hold b = (1/m, . . . , 1/m).
say portfolio selection algorithm beats market outpeforms U-BAH
given market sequence although practice market represented
non-uniform BAH.3 Buy-and-hold strategies rely tendency successful markets
grow. Much modern portfolio theory focuses choose good b buyand-hold strategy. seminal ideas Markowitz (1959) yield algorithmic procedure
choosing weights portfolio b minimize variance feasible
expected return. variance minimization possible placing appropriate (larger)
weights subsets sufficiently anti-correlated stocks, idea shall also utilize.
denote optimal hindsight buy-and-hold strategy (i.e. invest best
stock) BAH .
alternative approach static buy-and-hold dynamically change portfolio
trading period. approach often called active trading. One example
active trading constant rebalancing; namely, fix portfolio b (re)invest dollars
day according b. denote constant rebalancing strategy CBALb let
CBAL denote optimal (in hindsight) CBAL. constant rebalancing strategy often
1. PS algorithm modified universal investing fixed fraction initial wealth
universal algorithm.
2. nothing special daily closing prices problem defined respect
(sub)sequence (intra-day) sequence price offers appear stock market.
3. example Dow Jones Industrial Average (DJIA) calculated non uniform average 30
DJIA stocks; see e.g. http://www.dowjones.com/

580

fiCan Learn Beat Best Stock

take advantage market fluctuations achieve return significantly greater
always least good best stock BAH real market
sequences constant rebalancing strategy take advantage market fluctuations
significantly outperform best stock (see e.g. Table 1). now, consider Cover
Glusss (1986) classic (but contrived) example
cash one stock
1 consisting

1 ofa1market
, 12 , . . .. consider CBALb
market sequence price relatives 1/2
, 2 , 1/2
b = ( 21 , 12 ). odd day daily return CBALb 21 1+ 12 12 = 34 even
day, 3/2. total return n days therefore (9/8)n/2 , illustrating constant
rebalancing strategy yield exponential returns no-growth market.
assumption daily market vectors observations identically independently
distributed (i.i.d) random variables, shown (Cover & Thomas, 1991) CBAL
performs least good (in sense expected total return) best online portfolio
selection algorithm. However, many studies (see e.g. Lo & MacKinlay, 1999) argue
stock price sequences long term memory i.i.d.
non-traditional objective (in computational finance) develop online trading
strategies sense always guaranteed perform well.4 Within line
research pioneered Cover (Cover & Gluss, 1986; Cover, 1991; Cover & Ordentlich, 1996)
one attempts design portfolio selection algorithms provably well (in terms
total return) respect online offline benchmark algorithms. Two
natural online benchmark algorithms uniform buy hold U-BAH, uniform
1
1
,...,
). natural
constant rebalancing strategy U-CBAL, CBALb b = (

offline benchmark BAH challenging offline benchmark CBAL .
portfolio selection algorithm called universal every market sequence X
n days, guarantees subexponential ratio (in n) return retX (A)
retX (CBAL ). particular, Cover Ordentlichs Universal Portfolios algorithm (Cover,
1991; Cover & Ordentlich, 1996), denoted UNIVERSAL, proven universal;
specifically every market sequence X stocks n days, guarantees
subexponential (indeed polynomial) ratio
m1
(1)
retX (CBAL )/retX (UNIVERSAL) = n 2 .
BAH . CBAL

theoretical perspective surprising performance ratio bounded
polynomial n (for fixed m) whereas CBAL capable exponential returns.
practical perspective, bound useful empirical returns observed
CBAL portfolios often exponential number trading days. However,
motivation underlies potential CBAL algorithms useful! follow motivation develop new algorithm call ANTICOR. attempting systematically
follow constant rebalancing philosophy, ANTICOR capable extraordinary performance absence transaction costs, even small transaction costs.
4. trading strategy online computes portfolio (t+1)st day using market information
first days. contrast offline algorithms U-BAH , CBAL optimal
strategy picking best stock individual day. offline algorithms compute sequence
portfolios function entire market sequence.

581

fiBorodin, El-Yaniv, & Gogan

3. Trying Learn Winners
direct approach expert learning portfolio selection (reward based)
weighted average prediction scheme, adaptively computes weighted average
experts gradually increasing (by multiplicative additive update rule) relative
weights successful experts. section briefly discuss related
portfolio selection results along lines.
example, context PS problem consider exponentiated gradient
EG() algorithm proposed (Helmbold et al., 1998). EG() algorithm computes
next portfolio
bt (j) exp {xt (j)/(bt xt )}
bt+1 (j) = Pm
,
j=1 bt (j) exp {xt (j)/(bt xt )}
learning rate parameter. EG designed greedily choose best portfolio
yesterdays market xt time paying penalty movingp
far yesterdays portfolio. universal bound EG, Helmbold et al. set = 2xmin 2(log m)/n
xmin lower bound price relative.5 easy see n increases,
decreases 0 think small order achieve universality.
= 0, algorithm EG() degenerates uniform CBAL (assuming started
uniform portfolio) universal algorithm. also case
day price relatives stocks identical, EG (as well PS algorithms)
converge uniform CBAL. Combining small learning rate reasonably
balanced market expect performance EG similar uniform
CBAL confirmed experiments (see Table 1).6
Covers universal algorithms adaptively learn days portfolio increasing
weights successful CBALs. update rule universal algorithms
R
b rett (CBALb )d(b)
bt+1 = R
,
rett (CBALb )d(b)
() prior distribution portfolios. Thus, weight possible portfolio
proportional total return rett (b) thus far times prior. particular universal algorithm consider experiments uses Dirichlet prior (with parameters
( 21 , . . . , 12 )) (Cover & Ordentlich, 1996).7 Somewhat surprisingly, noted (Cover & Ordentlich, 1996) algorithm equivalent static weighted average (given (b))
CBALs (see also Borodin & El-Yaniv, 1998, p. 291). equivalence helps demystify
universality result also shows algorithm never outperform CBAL .
5. Helmbold et al. show eliminate need know xmin n. EG made universal,
performance ratio sub-exponential (and polynomial) n.
6. Following Helmbold et al. fix = 0.01 experiments. Additional experiments, wide range
fixed settings, confirm datasets choice = 0.01 optimal near optimal
choice. course, possible adaptively set throughout trading period, beyond
scope paper.
7. papers (Cover, 1991; Cover & Ordentlich, 1996; Blum & Kalai, 1998) consider simpler version
algorithm (Dirichlet) prior uniform. algorithm also universal achieves
ratio (nm1 ). Experimentally (on datasets) negligible difference two
variants report results asymptotically optimal algorithm.

582

fiCan Learn Beat Best Stock

different type winner learning algorithm obtained sequence
prediction strategy, noted (Borodin, El-Yaniv, & Gogan, 2000). stock j,
(soft) sequence prediction algorithm provides probability p(j) next symbol
j {1, . . . , m}. view prediction stock j best relative
price next day set bt+1 (j) = pj . paper (Borodin et al., 2000) considers
predictions made using prediction component well-known Lempel-Ziv (LZ) lossless
compression algorithm (Ziv & Lempel, 1978). prediction component nicely described
(Langdon, 1983) (Feder, 1991). prediction algorithm, LZ provably powerful
various senses. First shown asymptotically optimal respect
stationary ergodic finite order Markov source (Rissanen, 1983; Ziv & Lempel, 1978).
Moreover, Feder shows LZ also universal worst case sense respect
(offline) benchmark class finite state prediction machines. summarize, common
approach devising PS algorithms attempt learn winners using simple
sophisticated winner learning schemes.

4. Anticor Algorithm
propose different approach, motivated CBAL-inspired philosophy.
interpret success uniform CBAL Cover Gluss example Section 2?
Clearly, uniform CBAL taking advantage price fluctuation constantly transferring wealth high performing stock relatively low performing stock. Even
less contrived market, CBAL capable large returns. market model favoring
use CBAL one stock growth rates stable long term occasional larger return rates followed smaller rates (and vice versa). market
phenomenon sometimes called reversal mean.
many ways one interpret implement algorithms based
philosophy reversal mean. particular, CBAL viewed static
implementation philosophy. describe motivation basic ingredients
ANTICOR algorithm adaptively (based recent empirical statistics) rather
aggressively8 implements reversal mean.
given trading day, consider recent past w trading days, w
integer parameter. growth rate stock window time measured
product relative prices window.9 Motivated assumption
portfolio stocks performing similarly terms long term growth rates,
ANTICORs first condition transferring money stock stock j growth
rate stock exceeds stock j recent window time.10 addition,
ANTICOR algorithm requires indication stock j start emulate past
growth stock near future. end, ANTICOR requires positive correlation
stock second last window stock j last window.
relative extent transfer money stock stock j depend
8. ANTICOR algorithm aggressive (say, compared CBAL) sense transfer
assets given stock. Various heuristics used moderate behavior.
9. Since would rather deal arithmetic instead geometric means use logarithms
relative prices.
10. Note umderlying model assumption reversal mean. One modify
algorithm account different means.

583

fiBorodin, El-Yaniv, & Gogan

strength correlation well strength self anti-correlations
stocks j (again two consecutive windows). ANTICOR named use
correlations anticorrelations consecutive windows indicate potential
anticorrelations growth rates stocks j near future (with hopefully
growth rate stock j becoming greater stock i).
Formally, define
LX1 = log(xt2w+1 ), . . . , log(xtw )T LX2 = log(xtw+1 ), . . . , log(xt )T ,

(2)

log(xk ) denotes (log(xk (1)), . . . , log(xk (m))). Thus, LX1 LX2 two vector
sequences (equivalently, two w matrices) constructed taking logarithm
market subsequences corresponding time windows [t 2w + 1, w] [t w + 1, t],
respectively. denote jth column LXk LXk (j). Let k = (k (1), . . . , k (m)),
vectors averages columns LXk . Similarly, let k , vector standard
deviations columns LXk . cross-correlation matrix (and normalization)
column vectors LX1 LX2 defined as11
1
(LX1 (i) 1 (i))T (LX2 (j) 2 (j));
w

1
(
Mcov (i,j)
1 (i)2 (j) 1 (i), 2 (j) 6= 0;
Mcor (i, j) =
0
otherwise.

Mcov (i, j) =

(3)

Mcor (i, j) [1, 1] measures correlation log-relative prices stock
first window stock j second window. note 1 (i) (respectively,
2 (j)) zero window growth rate stock second last
window (respectively, stock j last window) constant window.
sufficiently large windows time constant growth stock unlikely. However,
unlikely case choose move money stock i.12
pair stocks j compute claimij , extent want shift
investment stock stock j. Namely, claim iff 2 (i) > 2 (j)
Mcor (i, j) > 0 case claimij = Mcor (i, j) + A(i) + A(j) A(h) = |Mcor (h, h)|
Mcor (h, h) < 0, else 0. Following interpretation success CBAL, Mcor (i, j) > 0
used predict stocks j correlated consecutive windows (i.e.
current window next window based evidence last two windows)
Mcor (h, h) < 0 predicts stock h P
negatively auto-correlated consecutive
windows. Finally,
b
(i)
=
b
(i)
+

j6=i [transferji transferij ] transferij =
P t+1
bt (i) claimij / j claimij . pseudocode summarizing ANTICOR algorithm appears
Figure 1. pseudocode describes routine ANTICOR(w, t, Xt , bt ) receives
window size w, current trading day t, historical market sequence Xt (giving
market vectors corresponding days 1, . . . , t) current portfolio bt defined
bt = bt1xt (bt (1)xt (1), . . . , bt (m)xt (m)). routine first called empty historical
market sequence bt uniform portfolio (over stocks). routine
11. Recall correlation coefficient normalized covariance covariance divided
product standard deviations; is, Cor(X, ) = Cov(X, )/(std(X) std(Y ))
Cov(X, ) = E[(X mean(X))(Y mean(Y ))].
12. course, approaches used accommodate constant nearly constant growth rate.

584

fiCan Learn Beat Best Stock

returns new portfolio, rebalance start (t + 1)st trading
day.
Algoritm ANTICOR(w, t, Xt , bt )
Input:
1. w: Window size
2. t: Index last trading day
3. Xt = x1 , . . . , xt : Historical market sequence
4. bt : current portfolio (by end trading day t)
Output: bt+1 : Next days portfolio
1. Return current portfolio bt < 2w.
2. Compute LX1 LX2 defined Equation (2), 1 2 , (vector) averages
LX1 LX2 , respectively.
3. Compute Mcor (i, j) defined Equation (3).
4. Calculate claims: 1 i, j m, initialize claimij = 0
5. 2 (i) 2 (j) Mcor (i, j) > 0
(a) claimij = claimij + Mcor (i, j);
(b) Mcor (i, i) < 0 claimij = claimij Mcor (i, i);
(c) Mcor (j, j) < 0 claimij = claimij Mcor (j, j);
6. Calculate new portfolio: Initialize bt+1 = bt . 1 i, j
P
(a) Let transferij = bti claimij / j claimij ;
(b) bt+1
= bt+1
transferij ;


(c) bt+1
= bt+1
+ transferji ;



Figure 1: Algorithm

ANTICOR

ANTICORw algorithm one critical parameter, window size w. Figure 2
depict total return ANTICORw two historical datasets function
window size w = 2, . . . , 30 (detailed descriptions datasets appear Section 5).
might expect, performance ANTICORw depends significantly window size.
However, w, ANTICORw beats uniform market and, moreover, beats best
stock using window sizes. course, online trading cannot choose w hindsight.
Viewing ANTICORw algorithms experts, try learn best expert.
windows, like individual stocks, induce rather volatile set experts standard expert
combination algorithms (Cesa-Bianchi et al., 1997) tend fail.13
Alternatively, adaptively learn invest weighted average ANTICORw
algorithms w less maximum W . simplest case uniform investment windows; is, uniform buy-and-hold investment algorithms
ANTICORw , w [2, W ], denoted BAHW (ANTICOR). Figure 3 graphs total return
BAHW (ANTICOR) function W values 2 W 50 four datasets
consider here. Considering graphs, choice W = 30 arbitrary clearly
13. assertion based empirical studies conducted various expert advice algorithms.

585

fiBorodin, El-Yaniv, & Gogan

NYSE: Anticorw vs. window size

TSX: Anticorw vs. window size

120

BAH(Anticor(Window))
Anticor(Window)
Best Stock
Market Return

8

10

80

Anticorw

5

Total Return

Total Return (logscale)

100

10

BAH(Anticorw)
Anticorw
Best Stock
Market

Best Stock

Anticorw
Best Stock

60

40

2

10

20

1

10

0

10

2

5

10

15

20

25

0

30

5

10

Window Size (w)

(a)

30

BAH(Anticorw)
Anticorw

Best Stock
Market Return

2.5

Best Stock
Market Return

Anticorw

8

Total Return

Total Return

25

DJIA: Anticorw vs. window size

3

BAH(Anticor )
w
Anticorw

10

20

(b)

SP500: Anticorw vs. window size
12

15
Window Size (w)

Anticorw
6

2

1.5

4
2

1

Best Stock

Best Stock

1
5

10

15

20

25

30

Window Size (w)

5

10

15

20

25

30

Window Size (w)

(c)
Figure 2:

0.5

(d)

ANTICORw

total return (per $1 investment) vs. window size 2 w 30
(a) NYSE; (b) TSX; (c) SP500; (d) DJIA. dashed (red) lines represent
final return best stock dash-dotted (blue) lines, final return
(uniform) market. dotted (green) horizontal lines represent uniform
investment number ANTICORw applications later described.

optimal. course, could try optimize parameter W particular dataset
training algorithm historical data beginning trade. However, claim
almost choice W yield returns beat best stock (the exception
W = 2 DJIA dataset).
Since consider various algorithms stocks (whose prices determined
cumulative returns algorithms), back original portfolio selection
problem ANTICOR algorithm performs well stocks may also perform well
algorithms. thus consider active investment various ANTICORw algorithms using
ANTICOR. consider windows w W . course, continue compound
algorithm number times. compound twice use buy-and-hold
investment. resulting algorithm denoted BAHW (ANTICOR(ANTICOR)). One impact
compounding, depicted Figure 4, smooth anti-correlations exhibited
stocks. evident compounding twice returns become almost completely
586

fiCan Learn Beat Best Stock

NYSE: Total Return vs. Max Window

30

7

6

25

BAH (Anticor)
W

BAHW(Anticor)

5

10

Total Return

Total Return (logscale)

10

TSX: Total Return vs Max Window
BAHW(Anticor)

10

Best Stock
Market

4

10

3

10

Best Stock

20
BAHW(Anticor)
15

10

2

Best Stock
Market

Best Stock

10

1

5

10

0

10

2

10

20

30

40

2

50

10

20

(a)
1.6

W

6

Total Return

Total Return

1.4

5

Best Stock
4

Figure 3:

BAHW(Anticor)
Best Stock

1.2

BAHW(Anticor)

1

3

1
2

BAHW(Anticor)

20

30

Best Stock
Market

0.8

Best Stock
Market
10

50

DJIA: Total Return vs Max Window

BAH (Anticor)

2

40

(b)

SP500: Total Return vs Max Window
7

30

Maximal Window Size (W)

Maximal Window size (W)

40

50

2

10

20

30

Maximal Window Size (W)

Maximal Window Size (W)

(c)

(d)

40

50

BAHW (ANTICOR)s

total return (per $1 investment) function maximal
window W : NYSE (a); TSX (b); SP500 (c); DJIA (d).

correlated thus diminishing possibility additional compounding substantially
help.14 idea smoothing critical parameters may applicable learning
applications. challenge understand conditions applications
process compounding algorithms smoothing effect.

5. Empirical Comparison Algorithms
present experimental study ANTICOR algorithm three online
learning algorithms described Section 3. focus BAH30 (ANTICOR), abbreviated
ANTI1 BAH30 (ANTICOR(ANTICOR)), abbreviated ANTI2 . Four historical datasets
used. first NYSE dataset, one used (Cover, 1991; Cover & Ordentlich, 1996;
Helmbold et al., 1998) (Blum & Kalai, 1998). dataset contains 5651 daily prices
36 stocks New York Stock Exchange (NYSE) twenty two year period July
3rd , 1962 Dec 31st , 1984. second TSX dataset consists 88 stocks Toronto
Stock Exchange (TSX), five year period Jan 4th , 1994 Dec 31st , 1998. third
14. smoothing effect also allows use simple prediction algorithms expert advice
algorithms (Cesa-Bianchi et al., 1997), better predict good window size.
explored direction.

587

fiBorodin, El-Yaniv, & Gogan

DJIA: Dec 14, 2002 Jan 14, 2003
Anticor1

Stocks
1.1

2

Anticor

2.2
2.6

1

Total Return

2.8

2

0.9

2.4
1.8

0.8

2.2

1.6
0.7

2

1.4
0.6

1.8

1.2

0.5

1.6

1

0.4
5 10 15 20 25
Days

5 10 15 20 25
Days

5 10 15 20 25
Days

Figure 4: Cumulative returns last month DJIA dataset: stocks (left panel);
ANTICORw algorithms trading stocks (denoted ANTICOR1 , middle panel);
ANTICORw algorithms trading ANTICOR algorithms (right panel).

dataset consists 25 stocks SP500 (as Apr. 2003) largest market
capitalization. set spans 1276 trading days period Jan 2nd , 1998 Jan 31st ,
2003. fourth dataset consists thirty stocks composing Dow Jones Industrial
Average (DJIA) two year period (507 days) Jan 14th , 2001 Jan 14th , 2003.15
Algorithm
Market (U-BAH)
Best Stock
CBAL
U-CBAL
ANTI1
ANTI2
LZ
EG
UNIVERSAL

NYSE
14.49
54.14
250.59
27.07
17,059,811.56
238,820,058.10
79.78
27.08
26.99

TSX
1.61
6.27
6.77
1.59
26.77
39.07
1.32
1.59
1.59

SP500
1.34
3.77
4.06
1.64
5.56
5.88
1.67
1.64
1.62

DJIA
0.76
1.18
1.23
0.81
1.59
2.28
0.89
0.81
0.80

NYSE1
0.11
0.32
2.86
0.22
246.22
1383.78
5.41
0.22
0.22

TSX1
1.67
37.64
58.61
1.18
7.12
7.27
4.80
1.19
1.19

SP5001
0.87
1.65
1.91
1.09
6.61
9.69
1.20
1.09
1.07

DJIA1
1.43
2.77
2.97
1.53
3.67
4.60
1.83
1.53
1.53

Table 1: Monetary returns dollars (per $1 investment) various algorithms four
different datasets reversed versions. winner runner-up
market appear boldface. figures truncated two decimals.
four datasets quite different nature (the market returns datasets
appear first row Table 1). every stock NYSE increased value, 32
88 stocks TSX lost money, 7 25 stocks SP500 lost money
15. four datasets, including sources individual stock compositions downloaded
http://www.cs.technion.ac.il/rani/portfolios.

588

fiCan Learn Beat Best Stock

25 30 stocks negative market DJIA lost money. exception
TSX, data sets include highly liquid stocks large market capitalizations.
order maximize utility datasets yet present rather different markets,
also ran market reverse. simply done reversing order inverting
relative prices. reverse datasets denoted -1 superscript.
reverse markets particularly challenging. example, NYSE1 stocks
going down. Note forward reverse markets (i.e. U-BAH) TSX
increasing TSX1 also challenging market since many stocks (56 88)
declining.
Table 1 reports total returns various algorithms eight datasets.
see prediction algorithms LZ quite well aggressive ANTI1
ANTI2 excellent sometimes fantastic returns. Note active strategies
beat best stock even CBAL markets exception TSX1
case still significantly outperform market. reader may well distrustful
appears unbelievable returns ANTI1 ANTI2 especially applied
NYSE dataset. However, recall NYSE dataset consists n = 5651 trading
days n = total NYSE return approximately 1.0029511
ANTI1 (respectively, 1.0074539 ANTI2 ); is, average daily increase less
.3% (respectively, .75%). observe learning algorithms UNIVERSAL EG
substantial advantage U-CBAL. previous expositions algorithms
highlighted particular combinations stocks returns significantly outperformed
best stock. said U-CBAL.
DJIA: Cumulative Total Returns
2.2

Cumulative Total Return

2

Anti1
Anti2
Best Stock
Market

Anti2
1

Anti

1.8
1.6
1.4
1.2

Best Stock

1
0.8

Market

Jan01

Jan02

Jan03

Date

Figure 5: DJIA: Cumulative returns
(the market).

ANTI1 , ANTI2 ,

best stock uniform

BAH

total returns ANTI1 ANTI2 presented Table 1 impressive far
telling complete story. Consider graphs figure 6. ANTI1 ANTI2
perform well respect uniform market best stock throughout
investment period, periods cumulative return strategies
589

fiBorodin, El-Yaniv, & Gogan

decrease. (not surprising) behavior indicates certain degree risk
using investment algorithms.
finance standard risk measure standard deviation return. Table 2
provide annualized returns risks well risk-adjusted returns markets
algorithms considered here.16 example, annualized return best stock
DJIA set 8.6%, annualized risk (standard deviation) 42% annualized
risk-adjusted return (Sharpe ratio) 11%.
Algorithm
Market
(U-BAH)
Best Stock
CBAL
U-CBAL
ANTI1
ANTI2
LZ
EG
UNIVERSAL

NYSE
12 14%
58%
19 24%
63%
27 30%
78%
15 13%
88%
110 28%
367%
136 35%
370%
21 23%
76%
15 13%
88%
15 13%
87%

TSX
10 12%
46%
44 55%
73%
46 40%
106%
9 13%
44%
93 45%
196%
108 60%
172%
5 25%
6%
9 13%
44%
9 13%
44%

SP500
5 24%
8%
30 51%
50%
31 42%
65%
10 22%
28%
40 37%
95%
41 44%
86%
10 25%
25%
10 22%
28%
10 22%
27%

DJIA
12 24%
-67%
8 42%
11%
11 26%
27%
9 25%
-54%
26 35%
62%
50 39%
119%
5 28%
-33%
9 25%
-54%
9 25%
-55%

NYSE1
9 15%
-86%
4 21%
-41%
4 40%
1%
6 13%
-77%
27 27%
86%
38 33%
101%
7 21%
17
6 13%
-77%
6 13%
-77%

TSX1
10 22%
29%
106 104%
98%
125 78%
156%
3 13%
-3%
48 41%
107%
48 46%
96%
36 27%
117%
3 13%
-2%
3 13%
-2%

SP5001
2 22%
-28%
10 32%
20%
13 27%
35%
1 21%
-9%
45 32%
126%
56 36%
143%
3 26%
-0.8%
1 22%
-9%
1 22%
-11%

DJIA1
19 25%
61%
65 114%
54%
71 76%
88%
23 25%
77%
90 31%
277%
113 35%
304%
35 27%
112%
23 25%
77%
23 25%
76%

Table 2: Annualized returns respective annualized volatilities well annualized riskadjusted returns (Sharpe Ratio) various algorithms three datasets
reversed versions. winner runner-up Sharpe Ratio market
appear boldface. figures truncated two decimals.

6. Commissions, Trading Friction Caveats
handling portfolio stocks algorithm may perform transactions
per day. major concern therefore commissions incur. Within proportional commission model (see e.g. Blum & Kalai, 1998; Borodin & El-Yaniv, 1998, Section
14.5.4) exists fraction (0, 1) investor pays rate /2
buy sell. Therefore, return
sequence b1 , . . . , bn portfolios

Q
P
respect market sequence x1 , . . . , xn bt xt (1 j 2 |bt (j) bt (j)|) ,
16. annualized return estimated using geometric mean ofthe individual daily returns risk
standard deviation daily returns multiplied 252 252 assumed standard
number trading days per year. calculations standard. (annualized) Sharpe ratio
(Sharpe, 1975) ratio annualized return minus risk-free return (taken 4%) divided
(annualized) standard deviation.

590

fiCan Learn Beat Best Stock

bt = bt1xt (bt (1)xt (1), . . . , bt (m)xt (m)).17 investment algorithm simplest form
tolerate small proportional commission rates still beat best stock.
graphs Figure 6 depict total returns BAH30 (ANTICOR) proportional commission factor = 0.1%, 0.2%, . . . , 1%. strategy withstand small commission factors.
example, = 0.1% algorithm still beat best stock four markets
consider (and beats market < 0.4%). Moreover still clearly beats market
whenever < 0.4%.
NYSE

10

30

5

Best Stock

10

TSX

25

Anti1
Return

Return (logscale)

10

Market

20
15
10
5

0

10

0

6

0.2
0.4
0.6
0.8
Commission Rate ()

0
0

1

SP500

2

0.2
0.4
0.6
0.8
Commission Rate ()

1

DJIA

5
Return

Return

4
3
2

1.5

1

1
0

0.1 0.2
0.4
0.6
0.8
Commission Rate ()

Figure 6: Total returns
0.1%, 0.2%, . . . , 1%.

0.5
0

1

BAH30 (ANTICOR)

0.2
0.4
0.6
0.8
Commission Rate ()

1

proportional commissions

=

However, current online brokers charge small proportional commissions, perhaps addition small flat commission rate trades.18 means large
investor scale investment suffer small proportional transaction rate.
additional caveat assumption trades could implemented using
closing price. principle nothing special closing price (i.e.
algorithms trade time trading day) practical consideration related
dataset gathering availability dictated use prices.19 algorithms
17. note Blum Kalai (1998) showed performance guarantee UNIVERSAL still holds
(and gracefully degrades) case proportional commissions.
18. example, USA site, E*TRADE (https://us.etrade.com) offers flat fee $10 trade
5000 shares $.01/share thereafter.
19. Specifically, historical closing prices public domain allow experimental reproducibility.
Historical intraday trading quotes also gathered data usually protected
costly obtain.

591

fiBorodin, El-Yaniv, & Gogan

assume portfolio adjustments implemented using quoted prices receive
inputs. means transactions implemented simultaneously using
quoted prices. current online brokers computerized system issue transaction
orders almost instantly guarantee implemented instantly.
trading friction necessarily generate discrepancies input prices
implementation prices.
related problem one must face actually trading difference
bid ask prices. bid-ask spreads (and availability stocks buying
selling) functions stock liquidity typically small large market capitalization
stocks. consider large market cap stocks.
report abnormal returns using historical markets suspected data
snooping. particular, historical data sets conditioned fact
stocks traded every day bankrupcies stocks became
virtually worthless data sets. Furthermore, dataset excessively
mined testing many strategies substantial chance one strategies
successful simple over-fitting. Another data snooping hazard stock selection.
ANTICOR algorithms fully developed using NYSE TSX datasets.
DJIA SP500 sets obtained (from public domain sources) algorithms
fixed. Finally, algorithm one parameter (the maximal window size W ).
experiments clearly indicate algorithms performance robust respect W
(see, example, Figure 4).

7. Concluding Remarks
Traditional work financial economics tend focus understanding stock price
determination. main question is: predict stock market? Judging
extensive inconclusive work done financial forecasting, perhaps
beneficial question ask. Rather, computer program consistently outperform
market? Besides practicality, clear successful portfolio selection algorithm
mathematical model provide new intuition stock price formation.
example, case, algorithms suggest stock price fluctuations
sufficiently periodic anti-correlated.
number well-respected works report statistically robust abnormal returns
simple technical analysis heuristics, slightly beat market. example,
landmark study Brock, Lakonishok, LeBaron (1992) apply 26 simple trading
heuristics DJIA index 1897 1986 provide strong support technical
analysis heuristics. consistently beating market considered significant (if
impossible) challenge, approach portfolio selection indicates beating best
stock achievable goal. mainly focused idealized frictionless
setting, believe even frictionless setting (which seems like reasonable
starting point) results previously claimed literature.
results presented raise various interesting questions. Since simple statistical
relations correlation give rise outstanding returns plausible various
other, perhaps sophisticated machine learning techniques, give rise better

592

fiCan Learn Beat Best Stock

portfolio selection algorithms capable larger returns tolerating larger commissions
fees.
theoretical side, missing point time analytical model
better explains active trading strategies successful. regard,
investigating various statistical adversary models along lines suggested Raghavan
(1992) Chou et al. (1995). Namely, would like show algorithm performs
well (relative benchmark) market sequence satisfies certain constraints
empirical statistics.
One final caveat needs mentioned. Namely, entire theory portfolio selection
algorithms assumes one portfolio selection algorithm impact market!
like goose laying golden eggs, widespread use soon lead end
goose. case, market quickly react method consistently
substantially beat market.

Acknowledgments
thank Michael Loftus helpful comments. also thank Izzy Nelken Super
Computing Inc. help validating DJIA dataset.

References
Blum, A., & Kalai, A. (1998). Universal portfolios without transaction costs.
Machine Learning, 30 (1), 2330.
Borodin, A., & El-Yaniv, R. (1998). Online Computation Competitive Analysis. Cambridge University Press.
Borodin, A., El-Yaniv, R., & Gogan, V. (2000). competitive theory practice
portfolio selection. Proc. 4th Latin American Symposium Theoretical
Informatics (LATIN00), pp. 173196.
Brock, L., Lakonishok, J., & LeBaron, B. (1992). Simple technical trading rules
stochastic properties stock returns. Journal Finance, 47, 17311764.
Cesa-Bianchi, N., Freund, Y., Haussler, D., Helmbold, D., Schapire, R., & Warmuth, M.
(1997). use expert advice. Journal ACM, 44 (3), 427485.
Chou, A., Cooperstock, J., El-Yaniv, R., Klugerman, M., & Leighton, T. (1995).
statistical adversary allows optimal money-making trading strategies. Proceedings
6th Annual ACM-SIAM Symposium Discrete Algorithms.
Cover, T. (1991). Universal portfolios. Mathematical Finance, 1, 129.
Cover, T., & Gluss, D. (1986). Empirical bayes stock market portfolios. Advances Applied
Mathematics, 7, 170181.
Cover, T., & Ordentlich, E. (1996). Universal portfolios side information. IEEE
Transactions Information Theory, 42 (2), 348363.
Cover, T., & Thomas, J. (1991). Elements Information Theory. John Wiley & Sons, Inc.
Feder, M. (1991). Gambling using finite state machine. IEEE Transactions Information
Theory, 37, 14591465.
593

fiBorodin, El-Yaniv, & Gogan

Helmbold, D., Schapire, R., Singer, Y., & Warmuth, M. (1998). Portfolio selection using
multiplicative updates. Mathematical Finance, 8 (4), 325347.
Langdon, G. (1983). note Lempel-Ziv model compressing individual sequences.
IEEE Transactions Information Theory, 29, 284287.
Lo, A., & MacKinlay, C. (1999). Non-Random Walk Wall Street. Princeton
University Press.
Lugosi, G. (2001).
Lectures prediction
URL:http://www.econ.upf.es/lugosi/ihp.ps.



individual

sequences.

Markowitz, H. (1959). Portfolio Selection: Efficient Diversification Investments. John
Wiley Sons.
Raghavan, P. (1992). statistical adversary on-line algorithms. dimacs Series
Discrete Mathematics Theoretical Computer Science, 7, 7983.
Rissanen, J. (1983). universal data compression system. IEEE Transactions Information Theory, 29, 656664.
Sharpe, W. (1975). Adjusting risk portfolio performance measurement. Journal
Portfolio Management, 2934. Winter.
Ziv, J., & Lempel, A. (1978). Compression individual sequences via variable rate coding.
IEEE Transactions Information Theory, 24, 530536.

594

fiJournal Artificial Intelligence Research 21 (2004) 101-133

Submitted 3/03; published 2/04

Complexity Results Approximation Strategies MAP
Explanations
James D. Park
Adnan Darwiche

jd@cs.ucla.edu
darwiche@cs.ucla.edu

Computer Science Department
University California
Los Angeles, CA 90095

Abstract
MAP problem finding probable instantiation set variables given
evidence. MAP always perceived significantly harder related
problems computing probability variable instantiation (Pr), problem
computing probable explanation (MPE). paper investigates complexity
MAP Bayesian networks. Specifically, show MAP complete NPPP
provide negative complexity results algorithms based variable elimination.
also show MAP remains hard even MPE Pr become easy. example,
show MAP NP-complete networks restricted polytrees, even
effectively approximated.
Given difficulty computing MAP exactly, difficulty approximating
MAP providing useful guarantees resulting approximation, investigate
best effort approximations. introduce generic MAP approximation framework.
provide two instantiations framework; one networks amenable exact
inference (Pr), one networks even exact inference hard.
allows MAP approximation networks complex even exactly solve
easier problems, Pr MPE. Experimental results indicate using approximation
algorithms provides much better solutions standard techniques, provide accurate
MAP estimates many cases.

1. Introduction
task computing Maximum Posteriori Hypothesis (MAP) find
likely configuration set variables given partial evidence complement
set. focus paper complexity computing MAP Bayesian networks,
class best effort methods approximating MAP.
One specialization MAP received lot attention Probable
Explanation (MPE). MPE problem finding likely configuration set
variables given complete evidence complement set. primary reason
attention MPE seems much simpler problem MAP generalization.
Unfortunately, MPE always suitable task providing explanations. Consider
example problem system diagnosis, component associated
variable representing health. Given evidence system behavior, one
usually interested computing probable configuration health variables.
MAP problem since available evidence usually specify value
c
2004
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiPark & Darwiche

nonhealth variable. common approximate problem using MPE, case
one finding likely configuration every unknown variable, including health
variables variables particular interest, inputs outputs
system components. However, projection MPE solution health variables
usually likely configuration. Neither configuration obtained choosing
likely state health variable separately.
MAP turns difficult problem, even compared MPE problem,
Pr problem computing probability evidence. Specifically, provide
Section 2 complexity results indicate neither exact approximate
solutions guaranteed MAP, even restricted circumstances. Yet, MAP
remains important problem would like generate solutions. Therefore,
propose Section 3 general framework based local search besteffort approximation
MAP. also provide two specific instances proposed framework, one applicable
networks amenable exact computation Pr given Section 3,
applicable networks even amenable Pr given Section 4.
report experimental results method using realworld randomly generated
Bayesian networks, illustrate effectiveness proposed framework wide
range networks. close paper concluding remarks Section 5.

2. MAP Complexity
begin section reviewing complexity theory classes terminology pertain complexity MAP. examine complexity MAP general case,
followed examining complexity number MAP variables constrained.
consider complexity MAP algorithms based variable elimination.
conclude complexity section examining complexity MAP polytrees.
2.1 Complexity Review
assume reader familiar basic notions complexity theory like
hardness completeness languages, well complexity class NP.
addition NP, also interested class PP derivative it.
Informally, PP class contains languages exists nondeterministic Turing machine majority nondeterministic computations accept
string language. PP thought decision version
functional class #P. such, PP powerful language. fact NP PP,
inequality strict unless polynomial hierarchy collapses second level. 1
Another idea need concept oracle. Sometimes useful ask
questions could done operation free. complexity theory
modeled Turing machine oracle. oracle Turing machine Turing machine
additional capability able obtain answers certain queries single
time step. example, may want designate class languages could
1. direct result Todas theorem (Toda, 1991). Todas theorem PPP contains entire
polynomial hierarchy (PH), NP = PP, PH PPP = PNP .

102

fiComplexity Results Approximation Strategies MAP Explanations

recognized nondeterministic polynomial time PP query could answered free.
class languages would NP PP oracle, denoted NP PP .
Consider Boolean expression variables X 1 , . . . , Xn . following three
classical problems complete complexity classes:
SAT: truth assignment (world) satisfies ? problem NPcomplete.
MAJSAT: majority worlds satisfy ? problem PPcomplete.
E-MAJSAT: instantiation variables X 1 ,. . . ,Xk , 1 k n,
majority worlds satisfy ? problem NP PP complete.
Intuitively, solve NPcomplete problem search solution among
exponential number candidates, easy decide whether given candidate
constitutes solution. example, SAT, searching world satisfies
sentence (testing whether world satisfies sentence done time linear
sentence size). solve PPcomplete problem, add weights solutions,
easy decide whether particular candidate constitutes solution
also easy compute weight solution. example, MAJSAT, solution
world satisfies sentence weight solution 1. Finally, solve
NPPP complete problem, search solution among exponential number
candidates, also need solve PPcomplete problem order decide whether
particular candidate constitutes solution. example, E-MAJSAT, searching
instantiation x1 , . . . , xk , test whether instantiation satisfies condition
want, must solve MAJSAT problem.
2.2 Decision Problems
dealing decision versions Bayesian network problems paper,
define formally section.
Bayesian network pair (G, ), G directed acyclic graph (DAG)
variables X, defines conditional probability table (CPT) X|U variable
X parents U DAG G. is, value x variable X
instantiation u parents U, CPT X|U assigns number [0, 1], denoted x|u ,
represent probability x given u. 2 probability distribution Pr induced
Bayesian network (G, ) given follows. complete instantiation x network
variables X, probability x given
Pr(x)

def



=

x|u ,

xux

xu instantiation family (a variable parents) represents
compatibility relation among instantiations. is, probability assigned
complete variable instantiation x product parameters consistent
instantiation.
following decision problems assume given Bayesian network (G, )
rational parameters induces probability distribution Pr. Moreover,
evidence e, mean instantiation variables E.
2. Hence, must

P

x

x|u = 1.

103

fiPark & Darwiche

D-MPE: Given rational number p, evidence e, set network variables X,
instantiation x Pr(x, e) > p?
D-PR: Given rational number p evidence e, Pr(e) > p?
D-MAP: Given rational number p, evidence e, set variables Q,
instantiation q Pr(q, e) > p? Variables Q called MAP variables
case.
decision problems useful examining complexity finding exact solution, really interested functional problem actually computing
solution. cant solve problem exactly, would also like know close
get efficiently. consider approximation algorithms. define
notion approximation factor use discussing complexity approximation algorithms. Specifically, say approximate solution 0 within
approximation factor > 1 true solution case 0 . Moreover,
say algorithm provides f (n)factor approximation case problems
size n, approximate solutions returned algorithm within approximation
factor f (n).
2.3 MAP Complexity General Case
Computing MPE, Pr, MAP NPHard, still appears significant
differences complexity. MPE basically combinatorial optimization problem.
Computing probability complete instantiation trivial, real difficulty
determining instantiation choose. D-MPE NP-complete (Shimony, 1994). Pr
completely different type problem, characterized counting instead optimization,
need add probability network instantiations. D-PR PP-complete
(Litmman, Majercik, & Pitassi, 2001)notice complexity decision
version, functional version #P-complete (Roth, 1996). MAP combines
counting optimization paradigms. order compute probability
particular instantiation, Pr query needed. Optimization also required, order
able decide many possible instantiations. reflected complexity
MAP.
Theorem 1 D-MAP NPPP -complete.3
Proof: Membership NPPP immediate. Given instantiation q MAP variables,
verify solution querying PP oracle Pr(q, e) > k.
show hardness, reduce E-MAJSAT (Littman, Goldsmith, & Mundhenk, 1998)
D-MAP first creating Bayesian network models Boolean formula .
variable Xi formula , create analogous variable network values
{T, F } uniform prior probability. Then, logical operator, create variable
values {T, F } whose parents variables corresponding operands, whose
CPT encodes truth table operator (see Figure 1 simple example). Let V
network variable corresponding top level operand.
3. result stated without proof Littman (1999).

104

fiComplexity Results Approximation Strategies MAP Explanations

X1

X2

X3

Figure 1: Bayesian network produced using reduction Theorem 1 Boolean
formula (x1 x2 ) x3 .

complete instantiation x variables X appearing Boolean expression , evidence V = , have:
Pr(x, V = ) =

(

1
2n

0

x satisfies
otherwise

particular instantiation q MAP variables X 1 , ..., Xk , evidence V = , have:
Pr(q, V = ) =

X

Pr(q, xk+1 , ..xn )

xk+1 ,...,xn

=

#q
2n

#q number complete variable instantiations compatible q satisfies
. Since 2nk possible instantiations Xk+1 , ..., Xn , fraction fq satisfied
#q /2nk ,
fq
Pr(q, V = ) = k
2
Thus, instantiation q MAP variables compatible half
complete, satisfying instantiations Pr(q, V = ) > 1/2k+1 . MAP query
variables X1 , ..., Xk evidence V = threshold 1/2k+1 true
E-MAJSAT query also true. 2
fact, theorem strengthened.
Theorem 2 D-MAP remains complete NP PP even (1) network depth 2,
(2) evidence, (3) variables Boolean, (4) network parameters lie
interval [ 21 , 21 + ] fixed > 0.
proof appears Appendix A. Unlike computing probabilities, becomes easy
number evidence nodes bounded constant parameters
bounded away 0 (it falls RP described Dagum & Luby, 1997), MAP retains
NPPP complexity even restrictive circumstances. 4
4. altogether surprising since evaluating score possible solution, MAP variables
act evidence variables.

105

fiPark & Darwiche

NPPP powerful class, even compared NP PP. NP PP contains important
AI problems, probabilistic planning problems (Littman et al., 1998). three
classes related NP PP NPPP , equalities considered unlikely.
fact, NPPP contains entire polynomial hierarchy (Toda, 1991). Additionally,
MAP generalized Pr, MAP inherits wild nonapproximability Pr. Bayesian
network simulates SAT reduction get:
Corollary 3 Approximating MAP within approximation factor f (n) NPhard.
Proof: Using evidence V = , exact MAP solution number satisfying
instantiations divided 2n , 0 unsatisfiable, positive satisfiable.
formula unsatisfiable, approximate solution must 0 0/ = 0
0 0 = 0, = f (n). formula satisfiable, approximate solution
must positive since M/ > 0. Thus test satisfiability testing approximate
MAP solution zero not.2
2.4 Complexity Parameterized Number Maximization Variables
examine complexity restrict number maximization variables.
Let n number nonevidence variables, k number maximization
variables. extreme case k = 0, simply D-PR, PPcomplete.
extreme, k = n, becomes D-MPE, NPcomplete. constraining
number maximization variables dramatic impact complexity.
examine issue detail. Let D-MAP subset D-MAP problems
k = O(m), let D-MAPm subset D-MAP problems n k = O(m).
consider complexity parameterized classes problems. primary
results following:
Theorem 4 D-MAPlog n PPP , D-MAPlog n NP. However, > 0,

D-MAPn D-MAPn remain NPPP complete.
Proof: First, k = O(log n), number possible instantiations maximization
variables bounded polynomial. Thus, given PP oracle, possible decide
problem polynomial time asking instantiation q maximization
variables whether Pr(q, e) exceeds threshold. Similarly, n k = O(log n),
instantiation q maximization variables, test see Pr(q, e) exceeds
threshold summing polynomial number compatible instantiations.
k = O(n ) provide simple reduction solve D-MAP problem
creating polynomially larger one satisfying constraint number maximization
variables. unconstrained problem, simply create new problem adding
polynomial number irrelevant variables, parents children. Similarly,
provide reduction general D-MAP problem one constrained n k =
O(n ), adding polynomial number maximization variables parents,
children, deterministic priors. 2
106

fiComplexity Results Approximation Strategies MAP Explanations

2.5 Results Elimination Algorithms
Solution general MAP problem seems reach, easier networks? Stateoftheart exact inference algorithms (variable elimination (Dechter, 1996),
join trees (Lauritzen & Spiegelhalter, 1988; Shenoy & Shafer, 1986; Jensen, Lauritzen, &
Olesen, 1990), recursive conditioning (Darwiche, 2001)) compute P r MPE space
time complexity exponential width given elimination order.
allows many networks solved using reasonable resources even though general
problems difficult. Similarly, stateoftheart MAP algorithms also solve MAP
time space complexity exponential width used elimination order
but, MAP, orders used. section, investigate complexity
variable elimination MAP.
analyzing complexity variable elimination MAP, review variable
elimination. First, need concept potential. potential simply function
subset variables, maps instantiation variables real number.
size potential parameterized number instantiations variables,
exponential number variables. Notice CPTs potentials.
order use variable elimination Pr, MPE MAP, need three simple operations:
multiplication, summingout, maximization. Multiplication two potentials 1
2 variables XY YZ respectively (where set variables
common), defined (1 2 )(xyz) = 1 (xy)2 (yz). Notice X Z
nonempty, size 1 2 greater size either 1 2 . SumOut(,Y)
variables XY defined
SumOut(, Y)(x) =

X

(xy),



ranges instantiations Y. Maximization similar summing
maximizes unwanted variables:
Maximize(, Y)(x) = max (xy).


order handle evidence, need concept evidence indicator. evidence
indicator E associated evidence E = e potential variable E E (e) = 1,
zero values.
Given variable ordering , variable elimination used compute probability
evidence e follows:
1. Initialize P contain evidence indicators e conditional probability tables.
2. variable X, according order ,
(a) Remove P potentials mentioning X.
(b) Let MX product potentials.
(c) add SumOut(MX , X) P .
3. Return product potentials P .
107

fiPark & Darwiche

iteration, variable X eliminated leads removing mention X
P . step 3, variables removed, potentials remaining constants
resulting product single number representing probability evidence e. MPE
computed way, except projection step 2c replaced maximization.
complexity variable elimination linear number variables linear
size largest potential MX produced step 2b. size largest potential varies
significantly based elimination order. width elimination order simply
log2 (size(MX ))1 MX largest potential produced using elimination order. 5
treewidth Bayesian network minimum width elimination orders.
Pr MPE, elimination order used, complexity linear number
variables exponential treewidth. true MAP. Variable
elimination MAP similar methods, extra constraint.
step 2c, X MAP variable projection replaced maximization.
MAP variable projection used. extra constraint orders valid.
Maximization projection commute, maximization must performed last.
means elimination order valid performing MAP, X MAP
variable, potential step 2b must mention non-MAP variables. practice
ensured requiring elimination order eliminate MAP variables
last. tends produce elimination orders widths much larger available
Pr MPE, often placing exact MAP solutions reach.
order assess magnitude increase width caused restricting elimination order, randomly generated 100 Bayesian networks, containing 100 variables,
according first method Appendix B. network, computed width
using minfill heuristic (Kjaerulff, 1990; Huang & Darwiche, 1996). Then, repeatedly added single variable set MAP variables, computed constrained
width, using minfill, eliminating MAP variables last. process repeated variables MAP variable set. Figure 2 contains statistics
experiments. Xaxis corresponds number MAP variables (thus X = 0
corresponds unconstrained width). axis corresponds width found.
graph details minimum, maximum, mean, weighted mean 100 networks. weighted mean takes account complexity exponential
width, provides better representation average complexity. computed
P
log2 ( n1 ni=1 2wi ). Notice unconstrained widths range 11 18,
number MAP variables increases, width increases dramatically. example,
even quarter variables MAP variables (X = 25) widths range
22 34, (which corresponds roughly difficult doable well beyond
todays inference algorithms handle todays computers) weighted average 30. Notice also, would expect complexity analysis, problems
many MAP variables significantly easier middle
range.
consider question whether less stringent conditions valid
elimination orders, may allow orders smaller widths.
5. -1 definition preserve compatibility previously defined notion treewidth
graph theory.

108

fiComplexity Results Approximation Strategies MAP Explanations

80

max
weighted mean
mean
min

Constrained Width

70
60
50
40
30
20
10

0

20
40
60
80
Number MAP variables

100

Figure 2: Statistics constrained width different numbers MAP variables.
Xaxis number MAP variables, axis width. Notice
widths required general MAP problem significantly larger
Pr MPE, correspond X = 0 X = 100 respectively.

described earlier, given ordering, elimination algorithms work stepping
ordering, collecting potentials mentioning current variable, multiplying them,
replacing potential formed summing (or maximizing) current
variable product. process thought induce evaluation tree; see
Figure 3. evaluation tree elimination order described follows. leaves
correspond CPTs given Bayesian network, internal nodes correspond
potentials created elimination process. children potential represent
potentials multiplied together constructing . Note
internal node elimination tree corresponds variable order , whose elimination leads constructing node; Figure 3(b). Therefore, evaluation tree
viewed inducing partial elimination order; see Figure 3(c).
standard way constructing valid elimination order MAP eliminate
MAP variables Q last. Two questions present themselves. First, valid orderings
variables Q eliminated last? second, so, produce widths
smaller generated eliminating variables Q last?
answer first question yes, valid elimination orders
variables Q eliminated last. see that, suppose variable order
induces particular evaluation tree , let partial elimination order
corresponding . variable order 0 consistent partial order
109

fiPark & Darwiche

C
C


C

CD B

B

C



E
a.



BC

C



Pr(A) Pr(B|A) Pr(C|A) Pr(D|BC) Pr(E|C)
b.



E

E

B

c.

Figure 3: (a) Bayesian network, (b) evaluation tree elimination order
A, E, B, D, E, (c) partial elimination order induced evaluation tree.

also induce tree . Hence, order valid, order 0 also valid. Figure 3
shows evaluation tree induced using order A, E, B, D, C computing MAP
variables Q = C, D. order A, B, D, E, C consistent evaluation tree
and, hence, also valid computing MAP variables C, D. Yet, variables C,
appear last order.
Unfortunately, orders variables Q eliminated last help.
Theorem 5 elimination order valid computing MAP variables
Q, ordering width variables Q eliminated last.
Proof: Consider evaluation tree induced valid elimination order, corresponding partial order induces. variable Q parent variable Q.
prove this, suppose X parent evaluation tree, X Q
Q. means potential results eliminating variable includes
variable X, also means X must appeared potentials
multiplied elimination variable . contradiction since evaluation
tree underlying order valid. Since variable Q parent variable
Q, variables Q eliminated first order consistent partial order
defined evaluation tree. Then, variables Q eliminated, obeying
partial ordering defined evaluation tree. produced order
elimination tree original order, width. 2
2.6 MAP Polytrees
Theorem 5 significant complexity implications elimination algorithms even polytrees.
Theorem 6 Elimination algorithms require exponential resources perform MAP, even
polytrees.
Proof: Consider computing MAP variables X 1 , . . . , Xn given evidence Sn =
network shown Figure 4. Theorem 5, order whose width smaller
110

fiComplexity Results Approximation Strategies MAP Explanations

order eliminate variables 0 , . . . , Sn first, variables
X1 , . . . , Xn last. easy show though order width n. Hence,
variable elimination require exponential resources using order. 2
set MAP variables makes crucial difference complexity MAP computations. example, MAP variables X 1 , . . . , Xn/2 , S0 , . . . , Sn/2 instead
X1 , . . . , Xn solved linear time.
negative findings specific variable elimination algorithms. question
whether difficulty idiosyncrasy variable elimination avoided
use method computing MAP. following result, however,
shows finding good general algorithm MAP polytrees unlikely.
Theorem 7 MAP N P complete restricted polytrees.
Proof: Membership immediate. Given purported solution instantiation q,
compute Pr(q, e) linear time test bound. show hardness, reduce
MAXSAT MAP polytree.6 Similar reductions used Papadimitriou
Tsitsiklis (1987) Littman et al. (1998) relating partially observable Markov decision
problems, probabilistic planning respectively. MAXSAT problem defined
follows:
Given set clauses C1 , ..., Cm variables Y1 , ..., Yn integer bound
k, assignment variables, k clauses
satisfied.
idea behind reduction model random selection clause,
successively checking whether instantiation variable satisfies selected clause.
clause selector variable S0 possible values 1, 2, ..., uniform prior.
propositional variable Yi induces two network variables Xi Si , Xi represents
value Yi , uniform prior, Si represents whether Y1 , ..., Yi satisfy
selected clause. Si = 0 indicates selected clause satisfied one 1 , ..., Yi .
Si = c > 0 indicates selected clause C c satisfied Y1 , ..., Yi . parents
Si Xi Si1 (the topology shown Figure 4). CPT , 1
defined

1 si = si1 = 0





1 si = 0 si1 = j,



xi satisfies cj
P r(si |xi , si1 ) =

1

si = si1 = j xi





satisfy cj



0 otherwise

words, selected clause satisfied first 1 variables (s i1 6= 0),
xi satisfies it, Si becomes satisfied (si = 0). Otherwise, si = si1 . Now,
particular instantiation s0 S0 , instantiation x variables X 1 , ..., Xn ,
Pr(s0 , x, Sn = 0) =

(

1/(m2n ) x satisfies clause Cs0 ;
0
otherwise.

6. Actually, need reduce SAT, MAXSAT result used Theorem 8.

111

fiPark & Darwiche

S0

X1

X2

S1

S2

Xn
...

Sn

Figure 4: network used reduction Theorem 7.
Summing S0 yields P r(x, Sn = 0) = #C /(m2n ) #C number clauses
x satisfies. Thus MAP X1 , ..., Xn evidence Sn = 0 bound k/(m2n ) solves
MAXSAT problem well. 2
Since MAXSAT polynomial time approximation scheme (unless P = NP),
polynomial time approximation scheme exists MAP polytrees. fact, approximating
MAP polytrees appears much harder approximating MAXSAT.


Theorem 8 Approximating MAP polytrees within factor 2 n NP-hard
fixed , 0 < 1, n size problem.
proof appears Appendix A. So, hard approximate within constant
factor, hard approximate within polynomial factor, even subexponential factor.
close section summary complexity results section:
MAP NPPP complete arbitrary Bayesian networks, even evidence,
every variable binary, parameters arbitrarily close 1/2.
NPhard approximate MAP within factor f (n).
Variable elimination MAP requires exponential time, even polytrees.
MAP NPcomplete networks polytree structure.


Approximating MAP polytrees within factor 2 n NPhard fixed
[0, 1).

3. Approximating MAP Inference Easy
Since exact MAP computation often intractable, approximation techniques needed.
Unfortunately, spite MAPs utility, relatively little work done approximating it. fact, two previous methods approximating MAP
aware of. first (Dechter & Rish, 1998) uses minibucket technique.
(de Campos, Gamez, & Moral, 1999), uses genetic algorithms approximate
best k configurations MAP variables (this problem known partial abduction).
Practitioners typically resort one two simple approximation methods. One common
approximation technique compute MPE instantiation project result
MAP variables. is, want compute MAP variables given evidence
e, S0 complement variables E, compute instantiation s, 0
maximizes Pr(s, s0 | e) return s. method computes posterior marginals
112

fiComplexity Results Approximation Strategies MAP Explanations

MAP variables, Pr(S | e), S, choose likely state
variable given e.
propose general framework approximating MAP. MAP consists two problems
hard generaloptimization inference. MAP approximation algorithm
produced substituting approximate versions either optimization inference
component (or both). optimization problem defined MAP variables,
score solution candidate instantiation MAP variables (possibly
approximate) probability Pr(s, e) produced inference method. allows solutions
tailored specific problem. networks whose treewidth manageable, contains
hard optimization component (e.g. polytree examples discussed previously), exact
structural inference used, coupled approximate optimization algorithm.
Alternatively, optimization problem easy (e.g. MAP variables)
network isnt amenable exact inference, exact optimization method could
coupled approximate inference routine. components hard,
optimization inference components need approximated.
investigate section family approximation algorithms based local search.
first consider case inference tractable, develop extension handle
case inference infeasible. local search algorithms work basically follows:
1. Start initial guess solution.
2. Iteratively try improve solution moving better neighbor 0 : Pr(s0 | e) >
Pr(s | e), equivalently Pr(s0 , e) > Pr(s, e).
neighbor instantiation defined instantiation results changing
value single variable X s. new value X x, denote resulting
neighbor X, x. order perform local search efficiently, need compute
scores neighbors sX, x efficiently. is, need compute Pr(sX, x, e)
X values x s. variables binary values,
| | neighbors case.
Local search proposed method approximating MPE (Kask & Dechter,
1999; Mengshoel, Roth, & Wilkins, 2000). MPE, MAP variables contain
variables E (the evidence variables). Therefore, score neighbor,
Pr(s X, x, e), computed easily since X, x, e complete instantiation. fact,
given computed Pr(s, e), score Pr(sX, x, e) computed constant
time.7
Unlike MPE, computing score neighbor, Pr(s X, x, e), MAP requires
global computation since X, x, e may complete instantiation. One main
observations underlying approach, however, score Pr(s X, x, e)
computed O(n exp(w)) time space, n number network variables
7. assumes none entries CPTs 0. 0 entries CPTs, may take
time linear number network variables compute score. Pr(s, e) product single
entry CPT compatible s, e. changing state variable X x x 0 ,
values product change CPTs X children. none
CPT entries 0, P r(s X, x0 , e) computed dividing Pr(s, e) old multiplying
new entry CPTs X children. done constant time number
children bounded constant.

113

fiPark & Darwiche

w width arbitrary elimination order, i.e., use elimination order
purpose, need constraints. fact, even better
computing scores neighbors, Pr(s X, x, e) X every value x
X, O(n exp(w)) time space. Thus, elimination order width w
given Bayesian network, perform search step O(n exp(w)) time
space. shall see later, takes small number search steps obtain good
MAP solution. Hence, overall runtime often O(n exp(w)) too. Therefore,
produce good quality MAP approximations time space exponential
unconstrained width instead constrained one, typically much larger.
local search method proposed section differs local search methods
used MPE unconstrained width must small enough search step
performed relatively efficiently. pointless use method approximate
MPE since time take one step, MPE could computed exactly. method
applicable unconstrained width reasonable constrained width
(see Figure 2).
3.1 Computing Neighbor Scores Efficiently
key computing neighbor scores efficiently express inference problem
function evidence indicators. state x variable X, evidence
indicator x one compatible evidence, zero otherwise.
common technique typically used allow modeling wider range evidence
assertions. example, allows evidence assertions X 6= x setting x = 0,
remaining indicators X one. use different purpose however.
inference problem cast function f evidence indicators (f ( e ) = Pr(e),
e consists evidence indicators, set compatible e),
f
(e ) = Pr(e X, x). add current state evidence,

x
partial derivative yields Pr(s X, x, e), precisely score one neighbors.
use jointree algorithm (Park & Darwiche, 2003), differential inference
approach (Darwiche, 2003) compute partial derivatives efficiently. differential approach, values immediate, entire approach based evaluating
differentiating expression f above. also computed using jointrees using
ShenoyShafer propagation scheme. Specifically, evidence indicator table added
variable, evidence variable entered setting appropriate
indicator entries. partial derivatives indicators associated variable
obtained multiplying tables assigned cluster, messages
cluster, projecting product onto variable. either case, partial derivatives indicators, thus score neighbors, computed
O(n exp(w)) time, complexity simply computing score
current state.
3.2 Search Methods
tested two common local search methods, stochastic hill climbing taboo search.
Stochastic hill climbing proceeds repeatedly either changing state variable
114

fiComplexity Results Approximation Strategies MAP Explanations

Given: Probability distribution Pr, evidence e, MAP variables S,
probability taking random flip Pf , initial state s0 .
Compute: instantiation (approximately) maximizes Pr(s | e).
Initialize current state s0 .
sbest =
Repeat manytimes:
probability Pf
= s0 , s0 randomly selected neighbor s.
Otherwise
Compute score P r(s X, x, e) neighbor X, x.
neighbor higher score score
= s0 , s0 randomly selected neighbor s.
Else
= s0 s0 neighbor highest score.
P r(s, e) > P r(sbest , e)
sbest =
Return sbest

Figure 5: Stochastic hill climbing algorithm.

creates maximum probability change, changing variable random. Figure 5
gives algorithm explicitly.
Taboo search similar hill climbing except next state chosen
best state hasnt visited recently. number iterations relatively
small save previous states iteration unique point chosen.
Pseudocode taboo search appears Figure 6.
3.3 Initialization
quality solution returned local search routine depends large extent
part search space given explore. implemented several algorithms
compare solution quality different initialization schemes. Suppose n
number network variables, w width given elimination order,
number MAP variables.
1. Random initialization (Rand). MAP variable, select value uniformly
set states. method takes O(m) time.
2. MPE based initialization (MPE). compute MPE solution given evidence.
Then, MAP variable, set value value variable takes
MPE solution. method takes O(n exp(w)) time.
3. Maximum likelihood initialization (ML). MAP variable X, set value
instance x maximizes P r(x | e). method takes O(n exp(w)) time.
4. Sequential initialization (Seq). method considers MAP variables X 1 , . . . , Xm ,
choosing time variable Xi highest probability Pr(x | e, y) one
115

fiPark & Darwiche

Given: Probability distribution Pr, evidence e, MAP variables S.
Compute: instantiation (approximately) maximizes Pr(s | e).
Initialize current state s.
sbest =
Repeat many times
Add visited
Compute score P r(s X, x, e) neighbor X, x.
= s0 s0 neighbor highest score visited .
neighbor exists (this rarely occurs)
Repeat several times
= s0 s0 randomly selected neighbor s.
P r(s, e) > P r(sbest , e)
sbest =
Return sbest

Figure 6: Taboo search. Notice action taken choose best neighbor
hasnt visited. leads moves decrease score peak
discovered.

values xi , instantiation MAP variables considered far.
method takes O(mn exp(w)) time.
3.4 Experimental Results
Two search methods (Hill Taboo) four initialization methods (Rand, MPE, ML,
Seq) lead 8 possible algorithms. initialization methods also viewed
approximation algorithm since one simply return computed initialization.
leads total 12 different algorithms. experimentally evaluated compared 11
algorithms, leaving algorithm corresponding random initialization.
tested algorithms various synthetically generated data sets well real
world networks. synthetic networks, generated random network structures using
two generation methods (see Appendix B). structure, quantified CPTs
different bias coefficients 0 (deterministic except roots), .5 (values chosen
uniformly) could evaluate influence CPT quantification solution quality.
network consisted 100 variables, root variables chosen MAP
variables. 25 root variables, randomly selected 25
MAP variables. Otherwise used root variables. chose root nodes
MAP variables typically subset root nodes variables interest
diagnostic applications. Evidence set instantiating leaf nodes. Care taken
insure instantiation non zero probability. algorithm allowed 150
network evaluations.8 computed true MAP compared solutions found
algorithm. Additionally, measured number network evaluations needed
find solution algorithm subsequently returned, number peaks discovered
8. evaluation takes O(n exp(w)) time space, n number network variables w
width given elimination order.

116

fiComplexity Results Approximation Strategies MAP Explanations

Data Set 1
0
Rand-Hill 147
Rand-Taboo 181
ML 526
ML-Hill 920
ML-Taboo 942
MPE 999
MPE-Hill 999
MPE-Taboo 1000
Seq 930
Seq-Hill 941
Seq-Taboo 962

Solution Quality
.125 .250
.375
805 917
946
969 985
993
497 676
766
947 989
993
988 999
999
333 160
127
875 923
952
986 992
990
965 990
999
971 992
999
998 1000 1000

.5
966
995
817
997
1000
100
973
998
997
997
1000

Table 1: solution quality method first data set. number associated
method bias number instances solved correctly 1000.
best scores bias shown bold.
Data Set 2
0
Rand-Hill 20
Rand-Taboo 20
ML 749
ML-Hill 966
ML-Taboo 973
MPE 858
MPE-Hill 961
MPE-Taboo 978
Seq 988
Seq-Hill 988
Seq-Taboo 994

Solution Quality
.125 .250 .375
634 713 799
851 907 943
453 495 519
922 947 963
960 986 987
505 365 275
853 850 874
952 962 977
955 964 985
960 966 986
977 990 994

.5
845
965
514
962
990
206
891
980
972
976
994

Table 2: solution quality method second data set. number associated method bias number instances solved correctly
1000. best scores bias shown bold.

solution discovered. hill climbing method used data sets
pure hill climbing random walk restart. is, hill climbs reaches peak,
randomly flips values move new location.
generated 1000 random network structures two structural generation methods. random structure generated, quantification method,
quantified network, computed exact MAP, applied approximation
117

fiPark & Darwiche

algorithms. Tables 1 2 show solution quality methods reporting
fraction networks solved correctly; is, approximate answer
value exact answer.
One draw number observations based experiments:
case, taboo search performed slightly better hill climbing random
restarts.
search methods typically able perform much better initialization
alone.
Even random start, search methods able find optimal solution
majority cases.
Overall, taboo search sequential initialization performed best, required
network evaluations.
Table 3 contains statistics number network evaluations (including
used initialization) needed achieve value method finally returned.
mean number evaluations quite small methods. Surprisingly, hill
climbing methods, maximum also quite small. fact, analyzing results
discovered hill climbing methods never improved first peak discovered.9 suggests one viable method quick approximation simply climb
first peak return result. Taboo search hand able improve
first peak cases.
ran ten MAP queries real world network tested. query
randomly selected one fourth nodes variables interest, selected
one fourth nodes evidence nodes. evidence values chosen uniformly
among nonzero configurations. previous experiments demonstrated large
number iterations rarely helps, reduced number iterations 30. Also,
moved away hill climbing random restart stochastic hill climbing (performing
random move probability .35) since previous experiments random restart
never helped. Also, ran minibucket approximation algorithm (the MAP
approximation algorithm aware subsumed technique) compare
performance algorithms. Since exact MAP computations networks
hard current algorithms handle, compare algorithms based relative
performance only.
Table 4 shows number times (out ten) algorithm able produce
highest probability configuration discovered. search based methods performed
much better algorithms. Note outperformed mini
bucket approximations network. Table 5 provides specific details
relative performance network. block contains count number
times method produced solutions within range best found solution.
9. appears random walk used restarting make eventually selecting better region
likely using search steps. Often, sub optimal hill encountered, optimal
hill 2 3 moves away. cases, taboo search usually able find (because
search guided), random walking not.

118

fiComplexity Results Approximation Strategies MAP Explanations

Evaluations Required
Method Mean Stdev
Rand Hill
12.5
2.5
Rand Taboo
14.3
11.0
MPE
1
0
MPE Hill
2.6
1.3
MPE Taboo
4.0
8.3
ML
1
0
ML Hill
1.6
.74
ML Taboo
1.9
3.3
Seq
25
0
Seq Hill
25.0
.04
Seq Taboo
25.0
.9

Max
21
144
1
8
137
1
4
62
25
26
45

Table 3: Statistics number evaluations method required achieving
value eventually returned. based random method 2, bias .5
data set. statistics data sets similar.

Barley
Mildew
Munin2
Munin3
Pigs
Water


3
6
6
9
0
9

MPE
H
9
8
10 10
10 10
10 10
0
0
10 10


3
8
10
10
5
8

ML
H
10
10
10
10
9
10


9
10
10
10
9
10


7
8
10
10
8
10

Seq
H
10
10
10
10
8
10


10
10
10
10
8
10

14
1
4
4
4
3
6

MB
16 18
3
5
4
7
5
7
6
2
1
6
6
9

Table 4: Number times ten algorithm found instantiation yielded
highest score. I, H, refer initialization only, hill climbing, taboo
search respectively.

example, Barley group, MPE row, column labeled > .5
3, indicating 3 10 cases solution found .5 .9 times
best solution found query.
Qualitatively, results similar obtained random networks.
search methods outperformed static initialization methods. Note
different networks, different initializations perform better. Notice also, search
methods significantly outperformed minibucket approximations every network.
119

fiPark & Darwiche

Barley network results
Best > .9 > .5 > .01
3
2
3
2
9
0
0
1
8
0
1
1
3
2
2
0
10
0
0
0
9
0
1
0
7
3
0
0
10
0
0
0
10
0
0
0
1
2
1
3
3
3
1
2
5
2
0
3
Munin2 network results
Method Best > .9 > .5 > .01
MPE
6
0
4
0
Hill
10
0
0
0
Taboo
10
0
0
0
ML
10
0
0
0
Hill
10
0
0
0
Taboo
10
0
0
0
Seq
10
0
0
0
Hill
10
0
0
0
Taboo
10
0
0
0
MB 14
4
0
1
2
MB 16
5
0
1
2
MB 18
7
0
0
1
Pigs network results
Method Best > .9 > .5 > .01
MPE
0
0
0
0
Hill
0
0
0
0
Taboo
0
0
2
3
ML
5
1
3
1
Hill
9
0
1
0
Taboo
9
0
1
0
Seq
8
0
2
0
Hill
8
0
2
0
Taboo
8
0
2
0
MB 14
3
0
3
4
MB 16
1
1
4
3
MB 18
6
0
2
2
Method
MPE
Hill
Taboo
ML
Hill
Taboo
Seq
Hill
Taboo
MB 14
MB 16
MB 18

.01
0
0
0
3
0
0
0
0
0
3
1
0
.01
0
0
0
0
0
0
0
0
0
3
2
2
.01
10
10
5
0
0
0
0
0
0
0
1
0

Mildew network results
Best > .9 > .5 > .01
6
1
3
0
10
0
0
0
10
0
0
0
8
1
1
0
10
0
0
0
10
0
0
0
9
1
0
0
10
0
0
0
10
0
0
0
4
1
0
1
4
0
0
1
7
0
1
0
Munin3 network results
Method Best > .9 > .5 > .01
MPE
9
0
0
1
Hill
10
0
0
0
Taboo
10
0
0
0
ML
10
0
0
0
Hill
10
0
0
0
Taboo
10
0
0
0
Seq
10
0
0
0
Hill
10
0
0
0
Taboo
10
0
0
0
MB 14
4
0
2
0
MB 16
6
0
1
0
MB 18
2
0
1
0
Water network results
Method Best > .9 > .5 > .01
MPE
9
0
1
0
Hill
10
0
0
0
Taboo
10
0
0
0
ML
8
1
1
0
Hill
10
0
0
0
Taboo
10
0
0
0
Seq
10
0
0
0
Hill
10
0
0
0
Taboo
10
0
0
0
MB 14
6
1
2
0
MB 16
6
1
2
1
MB 18
9
0
0
1
Method
MPE
Hill
Taboo
ML
Hill
Taboo
Seq
Hill
Taboo
MB 14
MB 16
MB 18

.01
0
0
0
0
0
0
0
0
0
4
5
2
.01
0
0
0
0
0
0
0
0
0
4
3
7
.01
0
0
0
0
0
0
0
0
0
1
0
0

Table 5: Detailed performance measures real world networks. column contains
number times 10 algorithm able achieve given
performance relative best solution found.

120

fiComplexity Results Approximation Strategies MAP Explanations

4. Approximating MAP Inference Hard
techniques developed thus far depend ability perform exact inference.
many networks, even inference intractable. cases, approximate inference
substituted order produce MAP approximations.
investigate using belief propagation approximate inference scheme, local
search optimization scheme. Iterative belief propagation useful approximate
inference algorithm approximating MAP number reasons proven
effective efficient approximation method variety domains.
ability approximate MPE, posterior marginals, probability evidence, allowing
initialization schemes used exact inference. Additionally,
show section 4.2, single inference call, scores neighbors search space
computed locally, allowing us obtain linear speed obtained
using similar approach exact inference case. Thus belief propagation allows
techniques approximating MAP inference tractable networks applied
approximately inference tractable.
4.1 Belief Propagation Review
Belief propagation introduced exact inference method polytrees (Pearl, 1988).
message passing algorithm node network sends message
neighbors. messages, along CPTs evidence used compute
posterior marginals variables. networks loops, belief propagation
longer guaranteed exact, successive iterations generally produce different results,
belief propagation typically run message values converge.
shown provide good approximations variety networks (McEliece, Rodemich,
& Cheng, 1995; Murphy, Weiss, & Jordan, 1999), recently received theoretical
explanation (Yedidia, Freeman, & Weiss, 2000).
Belief propagation works follows. node X, evidence indicator X
evidence entered. evidence sets X = x, X (x) = 1, 0 otherwise.
evidence set X, X (x) = 1 x. evidence entered, node X
sends message neighbors. message node X parents U sends
child computed
MXY =

X

X Pr(X|U)



MZX

Z6=Y

U

Z ranges neighbors X normalizing constant. 10 Similarly,
message X sends parent U
MXU =

X

X Pr(X|U)



MZX .

Z6=U

XU{U }

10. use potential notation common join trees standard descriptions belief propagation
believe many indices required standard presentations mask simplicity
algorithm.

121

fiPark & Darwiche

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Figure 7: scatter plot exact versus approximate retracted values 30 variables
Barley network. x-axis true probability, y-axis
approximate probability.

Message passing continues message values converge. posterior X
approximated
X

Pr0 (X|e) =
X Pr(X|U)
MZX .
Z

U

messages initialized 1. two main schemes ordering
messages. first scheme, messages computed simultaneously, based
previous set messages. scheme, messages updated incrementally,
two phases, consistent ordering variables. first phase, reverse
order, variable sends message neighbors precede order.
second phase, order, variable sends message neighbors come
order. implemented second scheme since empirically seems converge faster
first scheme (Murphy et al., 1999).
4.2 Approximating Neighbors Scores
Belief propagation allows us approximate scores neighbors local search space
efficiently, similar done case exact inference. key shall
show next able compute quantity Pr(x|e X) variable X efficiently,
use quantity rank neighbors according desired score.
Specifically, polytrees, incoming messages independent value
local CPT evidence entered. Hence, leaving evidence product yields
Pr(X|e X) =

X

Pr(X|U)

U



MZX .

Z

Therefore, compute quantity variable single belief propagation. networks polytrees, incoming messages necessarily
independent evidence local CPT, done BP methods,
ignore hope nearly independent. Empirically, approximation seems
122

fiComplexity Results Approximation Strategies MAP Explanations

quite accurate. Figure 7 shows representative example, comparing correspondence
approximate exact retracted probabilities 30 variables Barley
network. x axis corresponds true retracted probability, axis
approximation produced using belief propagation.
Still, Pr(x|e X) quite want score neighbors local search space.
quantity used compute ratio neighboring score current
score allows comparisons. Specifically, simple algebra shows that:
Pr(x, X, e)
Pr(x|s X, e)
=
Pr(s, e)
Pr(xs |s X, e)
xs value X takes current instantiation s. Thus, find
neighbor best score single belief propagation.
4.3 Experimental Results
first experiment, consider improvement possible typically done
(MPE ML) using starting point hill climbing there. first
experiment, generated 100 synthetic networks 100 variables using first
method described Appendix B bias parameter 0.25 width parameter 13.
generated networks small enough could often compute exact MAP
value, large enough make problem challenging. chose MAP variables
roots (typically 20 25 variables), evidence values chosen randomly
10 leaves. computed true MAP ones memory constraints
(512 MB RAM) allowed. computed true probability instantiations produced
two standard methods. initialization methods also computed true
probability instantiations returned pure hill climbing 11 (i.e. greedy steps
taken), stochastic hill climbing 100 steps, random moves taken
probability pf = .3. 100 networks, able compute exact MAP
59 them. Table 6 shows number exactly solved method, well worst
instantiation produced, measured ratio probabilities found instantiation
true MAP instantiation. hill climbing methods improved significantly
initializations general, although 2 networks, hill climbing versions
slightly worse initial value (the worst ratio .835), slight
mismatch true vs. approximate probabilities. all, stochastic hill climbing
routines outperformed methods.
second experiment, generated 25 random MAP problems Barley network, 25 randomly chosen MAP variables, 10 randomly chosen evidence
assignments. use parameters previous experiment. problems
hard compute exact MAP, report relative improvements
initialization methods. Table 7 summarizes results. Again, stochastic
hill climbing methods able significantly improve quality instantiations
created.
11. compare pure stochastic hill climbing evaluate gained stochastic methods.
initial hill climb usually requires evaluations, stochastic methods make little difference,
efficiency considerations would dictate pure hill climbing used.

123

fiPark & Darwiche

MPE
MPE-Hill
MPE-SHill
ML
ML-Hill
ML-SHill

# solved exactly
9
41
43
31
38
42

worst
.015
.06
.21
.34
.46
.72

Table 6: Solution quality random networks. Shows number solved exactly
59 could compute true MAP value. Worst ratio
probabilities found instantiation true MAP instantiation. hill
climbing method improved significantly initializations.

MPE-Hill
MPE-SHill
ML-Hill
ML-SHill

min
1.0
1.0
1.0x104
7.7x103

median
8.4
8.4
3.6x107
3.6x107

mean
1.3x1011
1.3x1011
3.4x1015
3.4x1015

max
3.1x1012
3.1x1012
8.4x1016
8.4x1016

Table 7: statistics improvement initialization method
search method data set generated Barley network. Improvement
measured ratio found probability probability initialization
instantiation.

third experiment, performed type experiment Pigs network.
None search methods able improve ML initialization. concluded
problem easy. Pigs 400 variables, seemed evidence didnt
force enough dependence among variables. ran another experiment Pigs,
time using 200 MAP variables 20 evidence values make difficult. Table 8
summarizes results. Again, stochastic methods able improve significantly
initialization methods.

MPE-Hill
MPE-SHill
ML-Hill
ML-SHill

min
1.0
1.0
13.0
13.0

median
1.7x105
2.5x105
2.0x103
1.2x104

mean
1.5x107
4.5x1011
3.3x105
8.2x105

max
3.3x108
1.1x1013
4.5x106
8.2x106

Table 8: statistics improvement initialization method alone
search method data set generated Pigs network. Improvement
measured ratio found probability initialization probability.

124

fiComplexity Results Approximation Strategies MAP Explanations

Barley
Mildew
Munin2
Munin3
Pigs
Water


2
5
5
8
0
9

MPE
H
6
7
1 10
0
9
0
1
0
1
0
0


2
5
5
8
0
9

ML
H
7
0
0
0
0
0


7
10
9
1
1
0


7
9
10
10
8
10

Seq
H
9
9
0 10
0 10
0
1
0
8
0
0

14
1
4
4
4
3
6

MB
16 18
3
5
4
7
5
7
6
2
1
6
6
9

Table 9: Number times ten algorithm found instantiation yielded
highest score. I, H entries stand initial, hill climbing, taboo
respectively. MB stands minibuckets, 14, 16, 18 width bounds.

also ran algorithms queries real world networks
used Section 3.4 able compare performance methods. Table 9
shows performed compares performance minibucket algorithms.
Table 10 gives detailed exposition performance. couple
interesting items data set. One surprising performance simple sequential
initialization. all, performed best approximate algorithms. Another
interesting thing note hill climbing often negatively impacted performance.
suggests marginal computations often accurate probability evidence
computations. problem especially acute networks significant determinism.
belief propagation believes configuration significant probability, may actually
0 probability one constraints violated. experiments suggest
possible improve standard approaches used inference intractable
(approximating MPE, ML using minibucket scheme) using belief propagation
estimate joint, successively moving states higher approximate scores.

5. Conclusion
MAP computationally hard problem general amenable exact
solution even restricted classes (ex. polytrees). Even approximation difficult.
Still, produce approximations much better currently used
practitioners (MPE, ML) using approximate optimization inference methods.
showed one method based belief propagation stochastic hill climbing produced significant improvements methods, extending realm MAP
approximated networks work well belief propagation.

Acknowledgement
work partially supported MURI grant N00014-00-1-0617
125

fiPark & Darwiche

Barley network results
Best > .9 > .5 > .01
2
3
1
3
6
1
1
2
7
0
1
2
2
3
1
3
7
1
1
1
7
0
1
2
7
1
1
1
9
0
1
0
9
0
1
0
1
2
1
3
3
3
1
2
5
2
0
3
Munin2 network results
Method Best > .9 > .5 > .01
MPE
5
0
3
1
Hill
0
0
0
0
Taboo
9
0
0
0
ML
5
0
3
1
Hill
0
0
0
0
Taboo
9
0
0
0
Seq
10
0
0
0
Hill
0
0
0
0
Taboo
10
0
0
0
MB 14
4
0
1
2
MB 16
5
0
1
2
MB 18
7
0
0
1
Pigs network results
Method Best > .9 > .5 > .01
MPE
0
0
0
0
Hill
0
0
0
0
Taboo
1
0
0
4
ML
0
0
0
0
Hill
0
0
0
0
Taboo
1
0
0
4
Seq
8
0
2
0
Hill
0
0
0
0
Taboo
8
0
2
0
MB 14
3
0
3
4
MB 16
1
1
4
3
MB 18
6
0
2
2
Method
MPE
Hill
Taboo
ML
Hill
Taboo
SEQ
Hill
Taboo
MB 14
MB 16
MB 18

.01
1
0
0
1
0
0
0
0
0
3
1
0
.01
1
10
1
1
10
1
0
10
0
3
2
2
.01
10
10
5
10
10
5
0
10
0
0
1
0

Mildew network results
Best > .9 > .5 > .01
5
2
3
0
1
0
0
0
10
0
0
0
5
2
3
0
0
0
0
0
10
0
0
0
9
1
0
0
0
0
0
0
10
0
0
0
4
1
0
1
4
0
0
1
7
0
1
0
Munin3 network results
Method Best > .9 > .5 > .01
MPE
8
0
0
1
Hill
0
0
0
0
Taboo
1
0
0
0
ML
8
0
0
1
Hill
0
0
0
0
Taboo
1
0
0
0
Seq
10
0
0
0
Hill
0
0
0
0
Taboo
1
0
0
0
MB 14
4
0
2
0
MB 16
6
0
1
0
MB 18
2
0
1
0
Water network results
Method Best > .9 > .5 > .01
MPE
9
0
1
0
Hill
0
0
0
0
Taboo
0
0
0
0
ML
9
0
1
0
Hill
0
0
0
0
Taboo
0
0
0
0
Seq
10
0
0
0
Hill
0
0
0
0
Taboo
0
0
0
0
MB 14
6
1
2
0
MB 16
6
1
2
1
MB 18
9
0
0
1
Method
MPE
Hill
Taboo
ML
Hill
Taboo
Seq
Hill
Taboo
MB 14
MB 16
MB 18

.01
0
9
0
0
10
0
0
10
0
4
5
2
.01
1
10
9
1
10
9
0
10
9
4
3
7
.01
0
10
10
0
10
10
0
10
10
1
0
0

Table 10: Detailed performance measures real world networks using Belief propagation approximation methods. column contains number times
10 algorithm able achieve given performance relative
best solution found.

126

fiComplexity Results Approximation Strategies MAP Explanations

W1
W11

X1





W1r

Wr
W21



X2





W2r

X3

Figure 8: network produced using construction proof Theorem 2
formula (x1 x2 ) x3 .

Appendix A. Proofs Theorems
Proof Theorem 2
want show MAP remains NPPP -complete even restricted networks
depth 2, evidence, binary variables, parameters arbitrarily close
1/2. Membership NPPP established Theorem 1. show hardness providing
reduction E-MAJSAT.
flow proof follows. First, construct depth 2 Bayesian network
E-MAJSAT problem. Then, show asserting evidence,
overcome constraint parameters lie within [1/2 , 1/2 + ], use
MAP obtain E-MAJSAT solution. Finally, show including evidence
variables MAP variables instead, evidence needed.
network constructed follows. logical variable x induces network variable
Xi uniform prior. operand yi induces network variable Yi uniform prior.
Notice connected, unlike reduction Theorem 1, CPT entries
enforce operator variable take value consistent operands
respect logic formula. example, network assign positive
probability node true, operand variables false.
say variable Yi consistent variables Pi associated operands,
logical function operator yi yields value Yi input pi . Consistency, instead
enforced rigidly, weighted introducing r weight variables W i1 ...Wir (the actual
value r discussed subsequently) associated . parents Wij
operator variable Yi variables corresponding operands. CPT W ij
defined
Pr(Wij = |Yi , Pi ) =

(

1
2
1
2

+ Yi consistent Pi
otherwise

Pi variables associated operands . Finally, children Ym
(which corresponds top level operator) add r additional binary variables W 1 ...Wr ,
127

fiPark & Darwiche


Pr(Wj = |Ym ) =

(

1
2
1
2

+ Ym =
otherwise

purpose weighting states formula satisfied. See Figure 8
example network construction.
Now, consider probability complete instantiation variables,
weight variables (which includes consistency weighting variables W ij ,
satisfiability weighting variables W ) set true, denote W = T.
Pr(x, y, W = T) =

m+n

1
2

1
+
2

kr (mk)r

1
2

1
+
2

sr (1s)r

1
2

x instantiation X1 ...Xn , instantiation Y1 ...Ym , k number
operator variables consistent operands variables s=1 = ,
0 otherwise. consistent satisfying assignment xy,
Pr(x, y, W = T) =

m+n

1
2

1
+
2

(m+1)r

inconsistent, unsatisfying assignment xy,
Pr(x, y, W = T)

m+n+r

1
2

1
( + )mr .
2

want choose r probability single consistent satisfying instance
greater twice sum probabilities inconsistent unsatisfying instances.
number inconsistent unsatisfying instances bounded 2 n+m , want r

m+n+r
(m+1)r
m+n
1
1
1
n+m+1 1
( + )mr .
>2
+
2
2
2
2
Solving r yields
r>

(m + n + 1)
1 + log 2



1
2

+



linear size formula, size reduction remains polynomially
bounded.
(m+1)r
m+n
1
Let C = Pr(x, y, W = T) = 21
+

xy consistent satisfying
2
instance. Then, particular instantiation q X 1 ...Xk ,
Pr(q, W = T) =

X

Pr(q, xk+1 , ...xn , y1 , ...ym , W = T)

xk+1 ,...,xn ,y1 ,...,ym

= #q C +

X

Pr(xy, W = T)

xy

#q number complete variable instantiations compatible q satisfies xy ranges inconsistent unsatisfying assignments compatible
x1 ...xk . Since instantiation x one compatible instantiation, # q
128

fiComplexity Results Approximation Strategies MAP Explanations

1

S0

2

S0

q

S0

1

X1

1

X2

S11

S2

X1

2

X2

S21

S2

1

1

Xn
...

1

Sn

2

2

2

Xn
...

2

Sn

q

X1

q

X2

Sq1

S2

q

B1

q

B2
.
.
.

Xn
...

q

Sn

Bq

Figure 9: network used reduction Theorem 8.

also corresponds number satisfiable instantiations consistent q.
choice r ensures sum unsatisfying inconsistent instantiations less
C/2, always greater 0 assuming least one operator (since
instantiation operator operands consistent). Thus
#q C < Pr(q, W = T) < (#q + 1/2)C. 2nk possible instantiations Xk+1 ...Xn ,
half less satisfied Pr(q, W = ) < (2 nk1 + 1/2)C, half
satisfied Pr(q, W = ) > (2nk1 + 1)C. Thus D-MAP query, using MAP
variables X1 ...Xk , evidence W = T, threshold (2nk1 + 1)C true
E-MAJSAT query also true.
Now, notice every table contains weight variable, value configuration takes true greater equal value takes false. Thus
Pr(q, W = T) Pr(q, W = w), q w. follows MAP(X 1 ...Xk , W =
T)=MAP(X1 ...Xk W, ). Therefore, D-MAP query, using MAP variables X 1 ...Xk ,W
evidence, threshold (2nk1 + 1)C true E-MAJSAT query
also true. 2
Proof Theorem 8
part proof theorem, use following lemma.
Lemma 9 x 1, 4x +

1
2

>

1
1 .
ln(1+ 4x
)

1
1
Proof: First, show f (x) = ln(1 + 4x
) 4x+
1 monotonically decreasing x 1.
2

df
dx

=

1
4
+
4x2 + x (4x + 21 )2
129

fiPark & Darwiche

=
=

16x2 + 4x (4x + 21 )2
(4x + 21 )2 (4x2 + x)
1
1 2
4(4x + 2 ) (4x2 + x)

always negative x 1, hence f (x) monotonically decreasing.
Now, since f (x) monotonically decreasing, lim f (x) = 0, f (x) must strictly
positive. Thus, x 1, ln(1 +

1
4x )

1
4x+ 12

>

x

implies 4x +

1
2

>

1
1 .
ln(1+ 4x
)

2

basic idea proof show repeating construction Theorem 7
polynomial number times, approximate MAP polytrees within relative
error 2size [0, 1), size network parameterized number
conditional probability parameters, solve SAT polynomial time.
Given SAT problem instance n variables, clauses, create Bayesian
network replicating construction Theorem 7 q times, connecting
form polytree. Specifically, copy construction, add variable B (we
use superscripts denote variables associated particular copy construction),
parents Sni , > 1, parent B i1 (see Figure 9). conditional probability

1 q
) > 2size .
B uniform parent instantiations. choose q satisfy (1 + 4m
show q chosen network size remains polynomial size
logical formula. resulting network q(2n + 2) variables, conditional
probability table 2(m + 1) 2 parameters, total size reduction
bounded q(m + 1)2 (4n + 4). Replacing size size bound places constraint


1+

1
4m

q

> 2(q(m+1)

2 (4n+4)

)

q. Since 0 < 1, solving q yields

2
1 q
> 2(q(m+1) (4n+4))
4m


1
q ln 1 +
> q (m + 1)2 (4n + 4) ln 2
4m
(m + 1)2 (4n + 4) ln 2


q 1 >
1
ln 1 + 4m





1+

q >





(m + 1)2 (4n + 4) ln 2



ln 1 +

1
4m



1
1

Now, Lemma 9, 4m + 1/2 > 1/ ln(1 + 1/4m), substitution yields stronger bound,
q >



1
4m +
(m + 1)2 (4n + 4) ln 2
4




1
1

polynomially bounded. Thus network constructed time polynomial
size formula.
130

fiComplexity Results Approximation Strategies MAP Explanations

Sni

Then, particular instantiation x X variables X 11 ...Xnq , evidence asserting
= 0 i, probability
Pr(x, s) =



Pr(xi , Sni = 0)



=

# clauses satisfied xi

m2n



subnetwork independent.
Thus solution MAP X 11 ...Xnq

q
k
k maximum number clauses
evidence Sn1 = .... = Snq = 0 m2
n
simultaneously satisfied original SAT problem. problem satisfiable
k = m, approximate solution 0 obeys


>
size
2
0



4m
4m + 1

q


m2n

q

>




1
4

!q

1
2n

q

=

41
m2n

!q

hand, isnt satisfiable, k 1,

2size <

0



4m + 1
4m

q

m1
m2n

q

=



(4m + 1)(m 1)
4m

q

1
m2n

q

<

43
m2n

!q

upper bound 0 SAT problem unsatisfiable bounded lower
bound 0 satisfiable. network construction bound tests
accomplished polynomial time, MAP problem approximated within
factor 2size polynomial time SAT decided polynomial time.

Appendix B. Generating Random Networks
generated several types networks perform experiments. used two methods
generating structure, single parametric method generating quantification.
B.1 Generating Network Structure
first method parameterized number variables N connectivity c.
method tends produce structures widths close c. Darwiche (2001)
provides algorithmic description.
second method parameterized number variables N , probability
p edge present. generate ordered list N variables, add edge
variables X probability p. edges added directed toward
variable appears later order.
B.2 Quantifying Dependencies
quantification method parameterized bias parameter b. values CPTs
roots chosen uniformly. values rest nodes based
bias, one values v chosen uniformly [0, b), 1 v.
131

fiPark & Darwiche

example, b = .1, non root variable given parents one value [0, .1),
(.9, 1]. Special cases b = 0, b = .5 produce deterministic, uniformly
random quantifications respectively.

References
Dagum, P., & Luby, M. (1997). optimal approximation algorithm Bayesian inference.
Artificial Intelligence, 93, 127.
Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 126 (1-2), 541.
Darwiche, A. (2003). differential approach inference Bayesian networks. Journal
ACM, 50 (3), 280305.
de Campos, L., Gamez, J., & Moral, S. (1999). Partial abductive inference Bayesian
belief networks using genetic algorithm. Pattern Recognition Letters, 20(11-13),
12111217.
Dechter, R., & Rish, I. (1998). Mini-buckets: general scheme approximate inference.
Tech. rep. R62a, Information Computer Science Department, UC Irvine.
Dechter, R. (1996). Bucket elimination: unifying framework probabilistic inference.
Proceedings 12th Conference Uncertainty Artificial Intelligence (UAI),
pp. 211219.
Huang, C., & Darwiche, A. (1996). Inference belief networks: procedural guide. International Journal Approximate Reasoning, 15 (3), 225263.
Jensen, F. V., Lauritzen, S., & Olesen, K. (1990). Bayesian updating recursive graphical
models local computation. Computational Statistics Quarterly, 4, 269282.
Kask, K., & Dechter, R. (1999). Stochastic local search Bayesian networks. Seventh International Workshop Artificial Intelligence, Fort Lauderdale, FL. Morgan
Kaufmaann.
Kjaerulff, U. (1990). Triangulation graphsalgorithms giving small total state space.
Tech. rep. R-90-09, Department Mathematics Computer Science, University
Aalborg, Denmark.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computations probabilities
graphical structures application expert systems. Journal Royal Statistics Society, Series B, 50 (2), 157224.
Litmman, M., Majercik, S. M., & Pitassi, T. (2001). Stochastic boolean satisfiability. Journal Automated Reasoning, 27 (3), 251296.
Littman, M. (1999). Initial experiments stochastic satisfiability. Sixteenth National
Conference Artificial Intelligence, pp. 667672.
Littman, M., Goldsmith, J., & Mundhenk, M. (1998). computational complexity
probabilistic planning.. Journal Artificial Intelligence Research, 9, 136.
McEliece, R. J., Rodemich, E., & Cheng, J. F. (1995). turbo decision algorithm.
33rd Allerton Conference Communications, Control Computing, pp. 366379.
132

fiComplexity Results Approximation Strategies MAP Explanations

Mengshoel, O. J., Roth, D., & Wilkins, D. C. (2000). Stochastic greedy search: Efficiently
computing probable explanation Bayesian networks. Tech. rep. UIUCDSR-2000-2150, U Illinois Urbana-Champaign.
Murphy, K. P., Weiss, Y., & Jordan, M. I. (1999). Loopy belief propagation approximate
inference: emperical study. Proceedings Uncertainty AI.
Papadimitriou, C., & Tsitsiklis, J. (1987). complexity Markov decision processes.
Mathematics Operations Research, 12(3), 441450.
Park, J., & Darwiche, A. (2003). differential semantics jointree algorithms. Neural
Information Processing Systems (NIPS) 15.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann Publishers, Inc., San Mateo, California.
Roth, D. (1996). hardness approximate reasoning. Artificial Intelligence, 82 (1-2),
273302.
Shenoy, P. P., & Shafer, G. (1986). Propagating belief functions local computations.
IEEE Expert, 1 (3), 4352.
Shimony, S. E. (1994). Finding MAPs belief networks NPhard. Artificial Intelligence,
68 (2), 399410.
Toda, S. (1991). PP hard polynomial-time hierarchy. SIAM Journal Computing, 20, 865877.
Yedidia, J., Freeman, W., & Weiss, Y. (2000). Generalized belief propagation. NIPS,
Vol. 13.

133

fi
ff
fi
! #"$ %
'&)(!*,+-(..
/02135476819!*

FHGJILKNM

:;<=> ?).@5A
.1!BDC< %&?E.(5A
.
/

OQPSRTK;KUVJWXOZY\[]R_^a`bGdcI)ceU5OfVgILVJPih$V)j2R_klceU5OfVm[n^eOfoSKRT`qp

rJstvu#w'xzy|{#w}ffu
a7a
7DTaD_'DDa5

SaD'a7Ta7575

~v$EZ2v

rJt$s#t$st)ex]wu

!aTJDf,
5ff

8# e

~vZ;T-f2$$

) t2u

Z2v

a7a
7DTaD_'DDa5

SaD'a7Ta7575

)e
>7 7
DT#fEd
ff7L>-
D-D; -Jf7
!5- dD!Q\77;;75#7! ED-D;;vDffQ !

E_7-
DE
-5)8,L!5- D!# -;;ff8>5DZ

;ld>
#E,ff;>lD-D;;D;
dD!ld
Dv7
f7;Dl_
l5l)DEff;>;7_D
ff>!2!
DeD-D7;7! ;)77DD;;2>-!
De
;7
_LDef7D27-D;;TTffd;D
d5
##D-D;;vld;DZ!ffl> 7| L
D-D;;T7-)D!#f; 87D5ff7; Z7;5;DDS;-D 7S

D)D5f;;8#N77;D ;7fD7\;

; ;
Z;7
L7 7DZD
D)77Qf
ff>
Dd75
,>

;$#;7 7DDS5 - 75
,;d7f 7
f;57ff8_5D ;7;T5,;;

ff
fi
"!$#%&('$)$"!$*+*,#%-).#%/!10#2)04325674898;: <=73- :8"04-32$)$5<>$ 32?@#%-).!1AB#%C-:D?E!E"#2:9F5$<ff8;*HG
%# 4!E $"#%!$3J' 3%:*,K3%#2L$:H: 7-M8;:+!$3%32@89!E#2ffNff "!$4 'O$!E#2ffNP!$4CQ8"0-:C473%#%-)-RHST8; "!$#%&
'$)$M!$*U8;#%K$<V!W:9.$<C:89#%#2X?E!E"#%! 3%:9Nff:!$8"0YAB#20Q!$Z!$[89#%!E :CYC-*,!$#%X$<?!$3\7-:9N
!$C]!1 :9$<^8; "!$#%&C-:9_4#%).!$3%32`A:C,?E!$3%7-:/<a$V7 :9($<O0-: :b?E!E"#%! 32:RJcd0:d:9e+89#2:8;5
$</!f8; "!$#\gK'$)$M!$*hC-:9'O:C4dX*,!$&5i<>!$8; $Mj#%893%7C4#%-),!])$&@CX8"0-#%8;:k<>$b0:kC-:89#\#2
?E!E"#%! 32:NO!$4CQ89!E:9<>743*[C:3%3%#%-)]$<0:H8;4 "!$#%&jX0-: :?E!E"#%! 32:9R6cB0-:9:#%b$<a :Y8;@G
#%C:9"! 32:+8M0-#%8;:l!$k mAB04!E60-:lC-:89#%"#2Z?E!E"#%! 3%:1!$Cn0-:#2k?!$3\7-:6"0-73%CY:9': :&9Rmo$
:;p-!$*'432:$N[#%+!$+:;p@!$*q#%*:9! 3\#%-)j' 32:*WN0-:r?!EM#%! 32:8;73%C:9'4: :&/0-:r:;p@!$*,N[!$C+0-:
?E!$3%7-:K:9'": :gK0-:#\*:9R6Sb32 :9"4!E#2?$:325$N^A:89!$Z7 :!ZsEtuEvO*@C-:3w#%XAB0#%8M0x0-:H?!E"#\! 32:
!E:d0:d#%*:9N[!$C0-:B?!$3%7:(!E:B0:d:;p@!$*+9RwyQ:r!$32AV!5@04!?$:r!18"0-#%8;:r$<O0#%wL@#%C#%'O:9"*k7@G
!E#2l'" 32:*,9Rwz{l!6'O:9"*k7-!E#2f'" 32:*WN[A:j0!`?$:.!$D*,!$&5+?E!$3%7-:V!$V?!EM#%! 32:9N-!$Cf:!$8M0
?E!E"#%! 32:j!EL$:K!$m7#%|[7-:K?!$3\7-:$RyQ:k89!$}0:9:9<a$":.:!$#%325l:;p-8"0!$)$:10-:632:d$<w0-:1?!E"#\! 32:
!$Cx0-:H?E!$3%7-:b#%}":9': :&#%-)f0-:7C-:9M325[#\-)' 3%:*WR.~}!$&5m!$"#2)*:&9Nff8"0-:C473%#%-)f!$C
(..
/, %%!? v &5%2%a5?

fi 8-
fiffff
7-#\-)i' 32:*,.!E:f'O:9"*k7-!E#2Z'4 32:*,Rfo-$H:;p-!$*'432:$N 'O$1 7-"!$*,:gk8"0-:C473%#%-)
89!$ :6*[C:3%32:Ci!$r_4C#\-)!,'O:9"*k7-!E#2i$<0:1)!$*:r ]_4B#%& +0-:6#%*:13%$9N4$K!,'O:9 G
*k7-!E#2i$<0:.#%*:1"32$d ,_4d0-:1)!$*,:r#%& -RVcd0-:.!$#%* $<J04#%D'4!E'O:9B#%B ]8;*'!E:178M0
C# O:9:&d*@C-:3% $0W0-:9$:9#\89!$3%325l!$Cl:*'4#2M#%89!$3%325$R








cd0:,'4!E'O:91#%6 "78;7-":CZ!$1<>3%32 AB9R]z [:8;#2 [NJA:+)#2?$:]0-:+<>$"*,!$3%#\* !$CY$!E#2
7 :CX#%m0-:H: K$<0-:H'4!E'O:9RKz [:8;#2 [NffA:H': :& P!$-)$<>$"C b'" 32:*WNOAB0#\8"0x#%K7 :C
+#%3\3%7 "!E :K0-:.C4# :9":gDAV!5@dA:189!$i*@C-:3^!H'O:9"*k7-!E#2i' 32:*iRJyQ:10-:i#%& @C78;:.!
<>$"*,!$3g*,:!$7-:($<-8; "!$#%&P#2)0&-: [:8;#2 ff7 :C. r8;*'4!E":/0-:9$:9#\89!$3%325K0-:/C# :9:&
*@C-:3%$<K'O:9"*k7-!E#2 '4 32:*, [:8;#2
MR z{ [:8;#2 [NVA:}8;*,'4!E: -Sc
&3%:!$
*@C-:3% $<4'O:9"*k7-!E#2k' 32:*,9R^z{ [:8;#2 r!$C [NA:D8;*'432:*:&w0-:0-:9$:9#%89!$3@:"732
AB#20k *:d!$5[*'4 $#%8V!$Ck:;p['O:9"#%*,:g!$3[!$!$325@#%9R yQ:D0-:H:;p@'432$:0-: :-:9_J "!$8M0#%-)
0-:7-M#% #%89B$</0!?@#%-)l*k732#2'43%:.?[#%:9Ad'O#%gB$<(0-:H'O:9"*k7-!E#2 = :8;#2
MRKz [:8;#%
@NOA:
:;p@ :Cx7-b!$4!$325["#%B l#% :8;#2?$:H*,!E''4#\-)9Rrow#%4!$3%325$N4A:k:C}Ar#20}":3%!E :CxA$L [:8;#2
!$Ci8;4893%7#24 [:8;#2
MR





!
+

9






#"$ %
-, /.



%&



65
1:$



' (*)

0$

e$CEaD
;
+< >= @?B
w
SGFIHKJMLON*P{uKQRJfiNSL;u7N*QTLTU9u$F:N*QVHKJXWPYH>Z;v\[O]^SM_B#%r!l :9K$<(?E!E"#%!



2



2143
1718

32:9NO:!$8"0XAB#%0m!f_4#% :6C-*,!$#\m$<
?E!$3%7-:9N[!$C]!k :9$<^8; "!$#\g9R(S 8;4 "!$#%&8;4#% $<^!63\#% ($<ff?E!E"#%! 3%: >0-:
!$Cf!
:3%!E#%iC-:9_4#\-)0-:1!$3%3%`A:Ci?E!$3%7-:d<>$B0: :.?E!E"#%! 32:RS #%!E"5f8; "!$#%&B#%d!]8; "!$#%&
AB0-:f8;$'O:W#%H!x'4!$#2k$<B?E!E"#%! 3%:9RQS 3%7#2Y Q!X8; M!$#%g!E#\ <>!$8;#%n'4 32:* #%H!$
!$#%)*:&D$<J?E!$3%7-:V ]?!EM#%! 32:V0!Er!E#% _:D!$3\3ff0-:.8; M!$#%gR

Ht u
;v
#%V!8; "!$#%&d!E#% <=!$8;#2W'" 32:* #%fAB0#%8M0f:!$8"0}C:89#%#2f?E!E"# G
! 32:f!EL$:,!$ 7#\|&7-:]?!$3\7-:$N(!$C 0-:9:l#%k0-:W!$*:l&74* :9k$<B?E!$3%7-:H!$H?!EM#%! 32:9R r:8;:
!$&5i 3%7-#2}!$#2)B!,'O:9"*k7-!E#2W$<0-:1?!$3%7:d +0:.?!EM#%! 32:9RVSb}#%*'O$!$&D<>:!E7-:1$<
'O:9"*k7-!E#2W'" 32:*,D#%d0!EdA:689!$i M!$ 'O :j0-:."32:D$<w0-:.?E!E"#%! 3%:d!$CW0-:1?!$3%7:d#%
:9'": :g#\-)H0-:17C:9"325@#%-)1'4 32:* ,)#%?$:.!,-:9AqsEtuEv-*@C-:3OAB04#%8"0W#%D!$3\ ,!'O:9"*k7-!E#2
' 32:*WR /!$8M0l?!E"#\! 32:K#%+0:b$M#2)#%!$3
,uEv
:8;*:D!k?E!$3%7-:K#%f0-:jC7!$3ff JN-!$C
?@#%8;:1?$:9""!@Rrcd0-:6'"#%*+!$3^!$Ci0:kC74!$3J B!E:6:|&74#2?!$3%:gb#%8;:1!$&5} 3\7-#2i f-:k89!$ :
"!$"3%!E :Ci#%& ,!, 3%7#2l ,0-:.$0:9R
yQ:H89!$m8M0-[ :1:#20:9r*@C-:3P!E #2 "!E"#\325+ :10-:1'M#%*,!$3ff*@C-:3 N4!$3%0-7-)0}#\W'"!$8;#%8;:k#2
*,#2)0& :f:!$#2:9k X:;p['":10:]'" 32:* 8;4 "!$#%&H#%Z-:f$<d0-:l*@C-:3%6"!E0-:9k0!$ 0-:
$0-:9Nw iA:+*,#%)0g. :4CZ }04#%-L}$<V0!Ek*@C-:3!$10:,'"#%*+!$3 RyQ:]!$3%}8;#%C:9 Ht[v v
Ht u
;v
#%]AB0#\8"0,0:b?E!E"#%! 32:/C#%?[#%C:B#%& k!H[7* :9$< >'O# 3256 ?$:9"3%!E''#%-)
:99N$:!$8M0$<AB04#%8"0k#%J!r'O:9"*k7-!E#26' 32:*WR cB0#%P3%:9J74JC4#%897P' 3%:*, 3\#2L$:|[7!$#2)$"7-'49R
SbH$"C-:9 l|&7!$"#2)$7-' >$ !E#%H|&74!E: J89!$ :V*,[C-:3%:Ck!$/!b*k732#%'432:/'O:9"*k7-!E#2H'4 32:*
8;&!$#%#%-) Q ?$:9"3%!E''#%-)'O:9"*k7-!E#2l' 32:*,9R
Sb
;v
#%(!.8;4 "!$#%&/!E#%<>!$8;#2+' 3%:* #%HAB0#%8M0:!$8"0fC-:89#%"#2H?!EM#%! 32:
!EL$:.!$Q74#%|&7:1?E!$3%7-:$N 7-b0-:9:!E:- *$:H?!$3\7-:b0!$X?!EM#%! 32:9R
?@#27325$Nff#2</0-:9:
!E:.<>:9A:9d?!$3\7-:D0!$W?E!E"#%! 32:9N@0-:.'" 32:* #%V "#%?[#%!$3\325+7"!E#% _4! 3%:$R
~m!$g5m32:9?$:3%r$</32@89!$3 8;"#% :8;5i0!`?$: :9:mC-:9_4-:Cm<>$b8; "!$#%&r!E#%<>!$8;#2x'" 32:*,
#%&?$32?@#%-) #%4!E5k8;4 "!$#%& ><>$:9<a:9":8;::9: b: "75[-:r!$C :# :9":$N
MRS ' 3%:* #%
^# }#%0!$V-@G:*,' 5+C*,!$#%!$Cf!$g5+8;4#% :&#\ !$&#%!E#2]$< w?!E"#\! 32:
89!$ :+8;4#% :&325X:;p[ :4C-:CZ }!$CC4#2#2!$3w?E!E"#%! 3%:9R]ST' 32:*U#%+u
=SB
# #2+#%
G 8;4#% :&9R ' 32:* #% @u
H# #2]#%
G 8;#\ :g9R

LOFIHIWM[`

aWM[OPb] MN 7N*Q*H7JcWPdHZ \[O]

'e

gf

EhWPbQT] Sj_
Sj_

Sj_

WM[4PI] MN 7N*QVHKJlWPdHZ \[O]mL

+] N*QkW [


n

go
p q
s6o
pQRJ:t:[uF:N*QVHKJWPYH>Z \[O]

r

w

Rzu{}|~IdFIHKJMLIQTLbN}[4JfiN



pd1K{:18

B

Bx

|

s)

mW 7NRdFIHKJMLIQTLbN}[4JfiN*_
r:

+v

Vy @1:070$,7

z
7PdF:dFIHKJMLIQTLbN}[4JfiN
XV~{:18

fififfv2$ff6ff )T$
fi
ff
v~ff



#%mLbN*PYHK
J s-W u7NR>YF`HKJMLbQhLON}[OJfiN= SB _VSb# #2j#%.SB !$C _VbR^ST' 3%:* #\W@u7NRQRJK[4P`L:[
`F H7JMLIQTLbN}[4JfiN9*_(zS6# #%k#% d1K{`$ G 8;"#% :&9R ' 32:* #%cPY[OLbN*PbQ*F8N}[s W@u7NR>YF`HKJ LIQTLbN}[OJN _VS
# #%6#%HSBq!$C #2<d!x?!$3\7-:f!$"#2)-:C Q!}?E!E"#%! 32:f#\68;"#% :&kAr#20 57 6-:l?!$3\7-:]<>$!$
!$C65 #%#%)i?!EM#%! 32:,0:n<>$H!$g5Z$0:9k?E!E"#%! 32:+0-:9":f#%k!x8;*'4!E# 3%:+?E!$3%7-:$RmS ' 32:* #%
LIQRJ v [:NHKY
J uKPYF4YF`HKJMLbQhLON}[OJfiN @ SBSd #Z
#2B0!$r-@G:*'4 5iC-*,!$#\d!$C}<a$b!$g5i#%4 !$g#\!E#2W$<(!

' 32:*

?E!E"#%! 32:$N@0-:1:732#\-)k"7 ' 32:*

89!$ :.*,!$C-:.SBbR



o-$f-@G #\!E5n8;4 "!$#%&9N0-:9:i0!$ :9: 3%:A$Ln C# O:9:g,3%:9?$:3%$<j32@89!$3D8;@G
#% :8;5$R K-:H:;p-8;:9'#2Z#%K)$::9"!$3%# 9:CZ!EM8 G 8;#% :48;5$R+S
AB#20 #%!E5i$6-@G #%4!E5
8; M!$#%gw#%
uEv sku
KSB
# ,<>$w!$g5.?E!$3%7-:<a$w!r?!E"#\! 32:/#%k!b8; "!$#%&9N
0-:9:k:;p-#% r8;*,'4!E# 32:6?!$3\7-:B<>$j!$3%3^0:6$0-:9K?E!E"#%! 3%:B#%m0-:68;4 "!$#%&9Rbo$j$"C-:9:CxC-EG
*,!$#%4 =748"0l!$D#%& :9)$:9" MN-!k' 32:*q#% Et
# }#2V0!$V-@G:*'4 5+C-*+!$#%
!$Ci!$m!$#2)4*:gD$<#2r*,#%#\*67*T$B*+!p@#%*k7* ?E!$3%7-:j f!$g5l?E!E"#%! 3%:K#\i! #%4!E5+$b-@G
#%4!E5 /8; M!$#%gD89!$ :j8;#\ :g3%5+:;p@ :C-:Cl 0-:j$0-:9D?E!E"#%! 32:D#%f0-:j8; "!$#\g9R(z{
3%#%:1AB#20m0-:C-:9_44#2#2r#%g "[C748;:C 5 b: "75[-:H!$C :# :9:
MN A:!`5}0!E.!l32@89!$3
8;#\ :8;5W'$'O:9F5
#\B!$b -)f!$r!]32[89!$3 8;#% :8;5W'$'O:9F5
>AB"#2 :
D#
#%x!$g5}' 32:*h#%iAr0#%8"0
0-3\Cd0-:
03%C9N
#%b -)$:9K0!$
>AdM#2 :
d#
1N
#%1#\8;*'4!E"! 32:AB#20
>Ad"#2 :
K# -:#%0-:9
71-$
-$
1N !$C
#%K:|[7#2?E!$32:&K
>AdM#2 :
b# $0
h!$4C
6R6z
0!$ :9:l0- ABf0!E /SB V
-SB
/z

SB

r: "7-5@-:
:"# :9:$N
MR

lv



j_

[4J [OP Qr[ KPdF:dFIHKJMLbQhLON}[OJfiN

B



9ZuH jJ KLFIHKJMLIQTLbN}[4JfiN-*)



!x



')

X

Vy Xd1:070$,7


#
$
%
$^
*+ ! ,
-
-/.0+
5
6
$879+ 5: !4
=< _ 0! 0! _ /!'_ /! >! ) *x

( !)
23 !4

!"+
fi&!'+
*( !1
53 !;
@? ) *y

1:070$,7
)V!$8L[ "!$8"L[#%)l!$3%)$$"#20*,B!E:1$<> :X7 :CW f_4Ci3%7-#2d lSj_9R @78M0}!$32)$$"#%0*,D 5

,:;p@ :Ci'!E#%!$3ff!$#%)*:&9N-:-<>$"89#%-),!,3%[89!$3ff8;"#% :8;5W!E<> :9d:!$8"0m:;p[ :4#2i!$C !$8L&G
"!$8"L[#%)mAB0:Y0#\132@89!$38;4#% :8;5n-m32)$:9H03%C9R}o$k:;p-!$*'43%:$NJ0:
/u {s
!$32)$$"#%0* =o V*+!$#%g!$#\D!H: "#\8; :CW<a$M* $<JSB 0!Ed:7:V0!ED0-: #%!E5+8;4 "!$#%&
:9 A:9:Q0-:,*b":8;:g325X#%!$g#%!E :CX?E!E"#%! 32:k!$4CQ!$g5X7#%4 !$g#\!E :Cm?E!E"#%! 32:K!E:SBbR
T0!$ :9: )$:-:9"!$3\# 9:C Z-@G #%4!E5Z8; "!$#\g :# :9":$N~}: :9)7-:9NVo-:7C:9N
!E G
!@N
MR/oJ 1*,!EL$::9?$:95 &G !E"5H8;4 "!$#%&/AB#%0
D?E!E"#%! 3%:/#%!$g#%!E :C]SBbR&4o
!E''43\#2: >:k'!$K$< rSB f:!$8M0Z8; M!$#%gK$.8; "!$#%&b'4 {:8;#%x#\g?$32?@#%-)]0-:897-:&
!$Cn:;p@!$8;3%5Z-:f<>7-7:+?E!E"#%! 3%:$R}o m!E''43\#2: >-:f'4!$k$< KSB m:!$8M0 8; "!$#\gH#%@G
?$32?@#%-)Q0-:m897-:&,!$C !Ef32:!$ ,-:W<=7-7-":W?!E"#\! 32:$R cd0-":9:i$0-:9+)$::9"!$3%# !E#24$<jo
Y--G #%!E5n8; "!$#%&9NVo x [NVC-:9)$:-:9"!E :m Zo Q 0-:i#%)32:W-@G #%4!E5
8; M!$#%g.C:8;"# #\-)]!l'O:9"*k7-!E#2ffN^i!E:,-$.8;#\C-:9:CQ0-:9:$RHow#%!$3%3%5$Nff0-: ,u
u
u
m!$3%)$$"#20*
~mSB ,*,!$#%&!$#%,SB C7-"#\-)x:!E"8"0ffNDAr0#%3% ,~ KSB *,!$#%&!$#%
KSBbR

U4HKPBA KP 2FOM[uFDC6QTJE



@1:07070$


3



p*)



HG

IGKJ 1







KPYF4YF`HKJ LIQTLbN}[OJ F=M

Nw







l

"

F?

Vy

65
IL






] KQTJN KQTJfiQTJ

$



1

OLP @ =0Q?R





cd0-: OG |&7:9:P'4 32:* #\P-:$<-0-:V#%*'32: ff:;p-!$*'432: $<4!d'O:9"*k7-!E#26' 32:*WRPS 8;*+*
!$C !E7-"!$3*@C-:3/04!$6!xC-:89#%#%Y?E!E"#%! 3%:,<a$k:!$8"0 "`A.NAB#%0Y#26?!$3\7-: :#%-)}0-:f8;3\7*,
#% AB0#%8M0 0:x|[7-:9: 0!El`AU3%#%:9R cB0-:mC74!$3r*@C-:3b0!$l! C-:89#%#2 ?E!E"#%! 32:m<a$l:!$8M0
8;3%7*+ffN&Ar#20l#2?!$3%7: :#%-)k0-:K lAB0#%8M0+0-:j|[7-:9:W#%]0!Ed8;3%7*+]3%#%:9R B A:9?$:9N0-:
OG |[7-:9:K' 32:*h#\b$K8;* #%4!E $"#%!$3%325m8"0!$3%3%:-)#%-)l!$j#2 :8;*:K:!$#2:9.!$
)$ AB9R6o$
:;p-!$*'432:$N^~}$"#%
K0!$K!E)7:CX0!EK0-:9:!E:-l32[89!$3w*,!p-#%*,!f l0- AB#%-)l|[7-:9:K!E
"!$C* g m0-: !E"CY!$CQ'O:9<>$"*,#%-)W*,#\@G 8; 4#%8;j04#%3%3 G 893%#\* #\-)+AB#\3%3w!$3%* 1"7-:325m_4C
!W 3%7#2ffRHyQ:,<>[897K0:9:9<a$":,Z!WC# O:9:&K' :9M*67-!E#%X' 32:* 04!E.#%j#%*'32:k3\#2L$:0-:

ge


/d1:0707$

TS



rVU

fi 8-
fiffff
oOG |[7-:9:r'

B)

32:* 7-b!E''O:!E"r :k*$:H8;* #\!E $"#%!$3%3%5i8"0!$3%3%:-)#%-)-R 5m7#%-)f!]"#%*'432:
:;p-!$*'432:$N0:68"04!E"!$8; :9"#% #\89d$<'O:9"*k7-!E#2W'" 32:*,D!E:k0-$'O:9<>743%325+*$":1!E''4!E:&d0!$m#%
*$:68;*'432:;pl' 3%:*,AB0-:9:j0-:1$0-:9B8; M!$#%gr0!?$:6!,3%!E)$:9B#\*'4!$8;9R
!$-)$<a$MC k' 32:* #%
#%
#I
K:g hyZ!$3%0ffN
MRZS 8;*':0-:4#2?$:

0#% $5+$<w0-:j'" 32:* #%D)#%?$: 5W~m#%3%32:9
MRVcd0-:j'" 32:* #%dC-:9_-:CW!$d<>3%32 AB

Sj_q
?
S1:07070$
VK373$$
=<
L NH NR>PY[I[cN*QT]s[OLc[u$F4j[4PY[lQTLsHKJ [

fiff sKQ7Q*NSL:[9tfi[4JFI[sQRJF9v ts>[bLsNRj[ls7Q $QRNR
s7Q $QRN ZI[4NAn[`[4JNRj[-PILbN NAqHKLBuKJs!HKJ ,
[ s7Q $QRN ZI[4NAn[`[4J NRj[,v uKLbN NAqHKLM[OPu[+uKPu[
t~LbNSNAqHmsKQ7Q*NRL+ZI[4NAn[`[4JNRj[ -PILbNN AqH ff !L ruKJ s!NAqH}sKQ7Q*NRL+ZI[4NAn[I[OJNRM[v2u6LbNNAqH ff L!
"# uKJssLOHHKJ%$-QRJsluEv>vjWjH6LILIQ;Z v\[ML F4cL:[ [4JFI[OL &
cd0-:1' 32:* 89!$}:!$#\325 :1)$:-:9"!$3%#9:C} ]0:lRog({ '!V
'" 32:* Ar0-:9:.A:60!`?$:k!] :|[7-:8;:1$<
32:-)$0po)*'QN8;g!$#%4#%-)0-:.#%& :9)$:9"j
1 +' :9'O:!E :CW:;p-!$8;325Q
#%*:9R(cB0-:.! `?$:1'4 32:*
#%V0[7D0-:cV[
Nk0$V ' 32:*WRwzFr0!$D:;p-!$8;325p
& 3%7-#%=<








,.-,./0,.12435624-61727/34865179-4863797/757869
,./,:2,"-62;8636245/48<17-73948656149/737-796175
,./,.30,"-6271756243/217-486573961;86/797-757869
9785-97/78617937578-6124/327561727-,"3,./0,
9785/97378-96145783/62717-275321,"/,.-0,
56149-37/917578937-6148/527378627-,.2,./0,

=

:60!Er0-:63\!$ d0-:9:k 3%7-#%d!E:60-:1:9?$:9M :1$<0-:1_" d0:9:$Rdcd0#%B 5[*+*:9 5W89!$ :
$
:3%#%*+#%!E :C 5]!$C4C#%-)k8; "!$#%& @<>$V#\ !$8;:$N-#%]0-: [N ' 3%:* 0-:K :8;C k89!$$ :
'43%!$8;:C]#%0-:b :8;Cf0!$32< $<ff0-:r:|&7-:48;:$N[!$C]#2<ff#2/#%/#%,0-:r8;:g "!$3'O#2#2+#%0-:b :|&7:8;:$N
0-: :8;C f*k7 :6'3%!$8;:CQ#%m0-:H_" j0!$32<$<0-: :|[7-:8;:$R -78"0Q8; "!$#%&j0!?$: :9:
!$CC-:C}#%lAB0!ED<>3%32 AB9R
cd0:6_M b*,[C-:3 $< !$-)$<>$"C b' 3%:* A:HAB#%3%3 8;#%C-:9`N Ar0#%8"0mA:H"0!$3%3J!E #2 "!EM#%325W89!$3%3
0-:d'4"#%*,!$3@*@C-:3 Ng04!$(!j?E!E"#%! 32:D<>$(:!$8M0+@89897-:48;:D$< 0:BC#2)#2RJcd0:D?!$3%7:D$< 04#%w?!EM#%! 32:
#%+0-:m'O#%#2 #% 0-:x:|&7-:48;:}$<.0#\,@89897-:8;:$R o-$l:;p@!$*,'432:$ND0-: [N ,'4 32:* 0!$
X?E!E"#%! 32:9N KAB#20
R cB0-:W?!$3\7-:W$< j#%0-:m32[89!E#% #% 0-:m :|&7:8;:i$<K0-:
jC#2?
;0 @89897-:8;:]$<d0-:lC#2)#2 b*,[C QRxcB0&7N * 0!$H!$H#26?!$3%7:+0-:l32@89!E#2Y$<
0-: 9 j@89897-":8;:$<0:,C#2)#2 EN ( 0!$1!$.#2K?E!$3%7-:H0-:,32@89!E#2X$<0-: 9 j@89897-:8;:$<
0-:fC#2)#2 [NDR9R`R9N 9 0!$k!$k#21?E!$3%7-:+0-:f3%[89!E#2Y$<d0-: 9 6@89897-":8;:+$<d0:]C#2)#% [N * .
0!$k!$6#%.?E!$3%7-:,0-:f32@89!E#2Y$<D0: ECY[89897-":8;:+$<D0:]C#2)#% EN!$CY mffRWyQ:l0!?$:W!
'O:9"*k7-!E#2H8; "!$#\g0!E:47-:J0!E:!$8M0,C#2)#2J@89897-:8;:D@89897-"!E!jC# :9:&w'O#2#2
#%m0-:H :|&7:8;:$Rjcd0#%r89!$ :H#%*'43%:*:g :C}:#20-:9K!$j!+)3% !$3 !$3\3 G C# O:9:&r8; M!$#%gbX!$3%3
0-:
N$+!$'!$#2AB#%:l${G:|&7!$3\,8; "!$#\g :!$8"0 ' "# 32:f'4!$#2$<b?E!E"#%! 32:R yQ:x89!$3%3
0-:]<a$"*,:910-: {'"#%*,!$3!$3%3 G C# :9:& i*@C-:3(!$4CY0-:f3%!E :910: {'"#\*,!$3/-${G:|[7!$3% }*@C-:3 R
ow#%!$3%3%5$NPA:+0!?$:l8; M!$#%g60!E60-:+C#2)#%j[89897-":8;:6[89897-k#%Q$"C:96C- ABZ0-:f :|[7-:8;:
!$Cn8; "!$#%&6Z0-:f :9'4!E"!E#2n$<D0-:fC# O:9:g1[89897-":8;:.$<d!mC#2)#2 60!EH#%.A:]0!`?$:
!$C
/<>$
QR
( 6N
(
cJ! 32: V)#2?$:(0-:D'"#%*+!$3g:9'4: :&!E#2$< 0-:d :|[7-:8;:
N$!j 3%7-#2H j0: [N
' 32:*WRwo-$893\!E"#2 5$N&A:b!$3% 1#\C#%89!E :D0-:b8;$: 'OC4#%-)jC#2)#2@89897-":8;:B7#\-)j0-:r$!E#2
m<a$60-: [0n[89897:8;:]$<0:]C#2)#% Rio-$k:;p-!$*'432:$N ( #%10-: E4CZ@89897-:8;:]$<D0-:
C#2)#%
]!$C
@R
* #%V0: 9 d@89897-:48;:j$<J0:.C#2)#2

?>

+V k0$

.



n

$,

z

'I1

p1

A@B



zDCFE1K{`$,#G

J'

H@B

V k0$

K@

1

c

p1

s1





91

G

X

s1

P

JN



D@BWS U J@BWS0U[Y%z SzA\R'
27948<2,.90,#8
_]
-
c
auN :P
#b#c

0 @





@BQR@BTS0UVQR@BWS U <@BWS0U JX@BZY z
`Nu:P

z

1 L@

M@



D@B

N(]4^.P

p0

P

=<

V

fififfv2$ff6ff )T$
fi
ff
v~ff



'Rz

z 4C-:;p
!$3%7-:j$<w'"#\*,!$3 ?!E"#\! 32:
/|[7#2?E!$32:&VC4#2)#2V[89897:8;:

+ @B}

f

+1 <Bcd0:r'M#%*,!$3:9': :&!E#2l$<

cJ! 32:

1

1
"



*


*

0-:j :|&7:8;:

32:*iR




1




"





*

,

1(
*

2497862,.9,"8





&


&

(

,

.


.

(

(

/V[N V'

N[!H 3%7-#2]$<P0:

G



c 0:KC47!$3ff*@C-:3 $< !$-)$<a$MC V' 3%:*T04!$d!H?!EM#%! 32:b<a$B:!$8"0}3%[89!E#2W#\]0:. :|&7:8;:$R

cd0-:r?!$3\7-:r$<P0#%(?E!E"#%! 32:B:9'4: :&0:bC#%)#2(@89897-:8;:K!EV0#%32[89!E#%ffR(o-$V:;p@!$*,'432:$N@0-:
[N .' 32:* 04!$ f?E!E"#%! 3%:9N +AB#20
RWcd0-:?E!$3%7-: r$< ,#\.!$Y#%& :9)$:96#\Q0-:
#%& :9?E!$3
N:9'::g#%)B0-:V<=!$8;J04!EJ0: ^C#%?
;06@89897-":8;:V$<0-:VC#2)#2 ^*[C
@89897-"D!ED32@89!E#2 -R/cB0&7N 1
k:9'::gV0-:K<=!$8;D0!EV0-: 9 V@89897-:8;:K$<J0-:jC#2)#2
@89897-"b!Er0-: "Cx32[89!E#%ffN !$4C
]:9': :&r0-:6<=!$8;b0!Er0: ECm[89897:8;:6$<(0-:
/
C#2)#% .[89897-Md!Ed0-: $0}32[89!E#%ffN!$Ci +ffR
z{0-:dC74!$3*@C-:3 N$A:B!E)!$#%+04!?$:B!j'O:9"*k7-!E#2+8; "!$#%&0!E(:!$8M0]3%[89!E#2+8;g!$#%4/!
C# O:9:&^C#2)#2O@89897-:48;:$RJcd04#%ff89!$.!E)!$#% :(#\*'432:*:& :CK?@#%!D!D)32 !$3g!$3%3 G C# O:9:&ff8; "!$#%&
m0-: .$ 5W'!$#2AB#%:K${G:|&7!$3\r8; "!$#%&B}:!$8"0x'4!$#2r$<(C7!$3ff?E!E"#%! 3%:9RVyQ:k89!$3%3P0-:
<>$"*:9i0-:
C47!$3.!$3%3 G C4# :9":g
*@C-:3.!$C 0:Y3%!E :9i0-:
C7!$3j-${G:|[7!$3%
*[C:3 R cd0-:
:9'4!EM!E#2 8; M!$#%g!E":i-$,!$,#%*,'432:f 'O:89#2<>5Y#% 0-:WC7!$3D*,[C-:3R o-$+:;p@!$*'32:$N/<>$
QNDA:x89!$ !$CC 8; "!$#\g]$<j0-:}<>$"*

H#
!$C
H#
*
QR.cJ! 32: ])#2?$:K0-:HC7!$3J:9'": :g!E#%m$<0-:H :|&7:8;:
NO!
(;+ *70
3%7#2l ,0-: [N D' 3%:*WR

V k0$

0E1K{Yo)7' G


1



]

z\ '

]

BWS

]

$,

]
] 143

+|

9

!| C E1K{`$,#G
nz 'I/1

+1

z

'







N

P



zI ) '

N

/

lV

|~

f

P

<X] z ] S0BTS Y(z*I '

z C:;p
!$3%7-:.$<wC7!$3O?E!E"#%! 3%:
/|[7#2?E!$32:gDC#%)#2D[89897:8;:
cJ! 32:

+z ]

/
] 6





1


*




*








*

E<nxb74!$3ff:9'::g!E#2i$<J0:. :|&7:8;: 27948<2,.90,#8




&
(

1

1

&
*



] OY(z
2497862,.9,"8

,

,

"
1(

.

(



.
(

sV[N D'

N-! 3%7-#2l$<w0-:

3%:*WR

z 1#%j'O"# 32:k }8;* #%-:'M#%*,!$3!$CZC47!$3*@C-:3% 5X3\#%-L@#%-)+0:, :91$<V?!E"#\! 32:9N
7#\-)
[u
;v>v
{u
Z*,!$#\g!$#% 8;"#% :8;5 :9FA:9: 0-:l AQ?[#2:9AB' #\g9Rncd0#%
!E''"!$8"0 #%}89!$3%32:C {:C74C!$&l*,[C-:3\3%#%-)
MRhS #%*+#%3%!EW#%C-:! AV!$
5 V0-:) :9}!$3 R
':9?@#27"325r7)$)$: :CffN$ 'O:89#2_489!$3%3%5B<>$P'O:9"*k7-!E#2.'4 32:*,N 5 K:9:32:
MR(z{
!$-)$<a$MC
' 32:*WN[0-:j8"0!$:3%3%#%-)k8; "!$#\gd!E":
,#
N!$Cl8; "!$#\gV$<J0-:j!$*:K<>$"*
89!$ :r7:Cf#\ 74#%3%C#%)K!H8;* #%-:Cf'M#%*,!$3C74!$3*@C-:34$<J!$g5+' :9M*67-!E#%]'4 32:*WR~m!$g5
8; M!$#%gk [32L[#%k"7-''O$68M0!$-:3\3%#%-)i$<B0#%6L[#%4CYAB#%0Y:9e+89#%:gk)32 !$3V8; M!$#%gRXo$
:;p-!$*'432:$N-z
@32?$:9B0!$D!8; "!$#%&9N ff
fi 4NgAB04#%8"0W89!$ :j7 :Cl ":9'43%!$8;:.! :9d$<

#%C4#2?[#\C7!$3@8; "!$#\g$<P0:r<>$"*
H#
"N@!$4C+0-: @#%897/_#2 :rC*,!$#%f8; "!$#%&
3%# "!E5+0!$B!$ ff,'4:C#%89!E :jAB04#%8"0W89!$ :.74 :CW#%*,#\3%!E"325$R
cd0:B8;* #%-:C,*@C-:3#\(8932:!E"325k:C74C!$&w!$A:B89!$+C-:3%:9 :B0-:B8;4 "!$#%&($<O:#20-:9#%C# G
?@#%C7!$3^*,[C-:3PAB#20-7r#%8;":!$#%-),0:6 :9b$</ 3%7#29Rdo-$j#%4 !$8;:$N#\ P!$)$<a$"C B'" 32:*WN

F4 KJ J@[ wQRJ2F`HKJMLON*P 7QTJfiNRL

N

;P

d1:07070$

2

@B Y| ] %Y z

v2


@ B | ] Y%z

ld1:0707$ 9

9

X

#b!





fi 8-
fiffff
A:+-:9:CQ3%5}:;p@':j0:, :9'4!E"!E#2Y8; "!$#%&.#%X :9M*,j$<V:#20-:9j0-:+'"#%*,!$3J$.0:,C7!$3
?E!E"#%! 32:R~i$":67-"'"#%#\-)325$N[0-:.'O:9"*k7-!E#2i8; "!$#%&d $0W0:1'"#\*,!$3ff!$CW0-:6C7!$3
?E!E"#%! 32:r!E:6!$3\ +:C47C!$&9Rdcd0-:6:;p@#% :8;:1$<(0:6C7!$3P?!EM#%! 32:d!$4C}0-:k8"0!$4-:3%3%#%),8;@G
"!$#\g(3\#%-L@#%-)B0:* .0-:D'M#%*,!$3[?!EM#%! 32:w!E:B"7-e+89#2:&w .:7-":04!E(0-:D?E!$3%7-:(!$"#2)-:C
H0-:K'"#%*+!$3?E!E"#%! 32:!E:j!6' :9M*67-!E#% =!$4C]0:9:9<a$":b0:b!$*,:K*k7 :r M7-:r$< 0-:KC7!$3
?E!E"#%! 32: MR

'

b

f

n

/?$:6#2<-8; "!$#%& !E:32$)#%89!$3%3%5b":C7C!$& >0!Ew#%9N0-:95.89!$ :/C-:32:9 :C6AB#20-7P8"0!$)#%-)
0-:d :9/$< 3\7-#2 MNg0-:95k*+!5 #%3%3 :d7 :9<=73@C7-M#%-)K :!E"8"0^R ^$)#%89!$3%3%51:C74C!$&w8;4 "!$#%&
!E:$<a :k89!$3%32:C #%*'3%#2:Cj8; M!$#%g @N!$C674 :9<>743#%*'43\#2:Cj8; "!$#\gw!E:<a:|[7-:&325.!$CC-:C1
!B*,[C-:3 r#%8;":!$ :(0-:/!$*,7g $<-8; "!$#%&P'4$'4!E)!E#2 @*,#20ffN [ :9)#27^N
yZ!$3%"0ffN
MR
z{H0-:D-:;p[(:8;#2ffNA:D': :&(!j*:!$7:V$< 8; "!$#%&J#%)0g-:"0!E(!$3%32 AB7w .C-:9 :9M*,#%-:
AB0-:x!$x#%*'43%#%:Ci8; "!$#\gK!$CC-:Cm f!f*@C-:3 AB#%3%3^#\*' ?$:68; M!$#%gb'$'!E)!E#2ffRbz m0-:
<>3%32`Ar#%-)l :8;#2ffN^A:!E''4325}0#%j*:!$7:H$<8; M!$#%gj#%)0g-:"K W0-:,C# O:9:&K*,[C-:3\r$<
'O:9"*k7-!E#26' 32:*+ #%& @C78;:C6#%104#% :8;#2ffRwyQ:D!E:D! 32: K"0-`A.NE<>$w:;p@!$*'32:$N$0!Ew0-:
8"04!$-:3%3%#\-)i8; "!$#\gk-$14325Q*,!EL$:]0: #%4!E5x${G:|&7!$3\18; M!$#%g6:C74C!$& 60-:95
!E:.#%)0g :9B!$4Ci89!$i)#2?$:6*$:.C-*,!$#\W'"74#%-)-R

N



-

P

c

?

7

KK37373>Y



=<

]}D# R2ff



ff

(


vK7-6C-:9_4#2#%X$<d8; "!$#%&.#2)0&-:6!$7*,:.0!Ek8; "!$#%&k!E:+C-:9_4:CQ`?$:9H0-:f!$*:

?E!E"#%! 32:!$4CH?!$3%7:w$N&!$(#\H0-:d89!$ :d$< '"#%*+!$3[!$4CC7!$3-*@C-:3%9N$?E!E"#%! 32:w!$C?E!$3%7-:wAB0#\8"0
!E: # {:8;#%?$:325}:3\!E :CffR,z{X0#%KAV!5$NwA:,89!$n!$32AV!5@18;*'4!E":+3%#2L$:HAB#20Q3\#2L$:$R K7-.C-:9_#2#2
$<r8;4 "!$#%&#2)0&-:#%H )325Y#% 7-:8;:C 5n0-:WA!`5 3%[89!$3V8;#\ :8;5Y'4$'O:9#2:H!E:
8;*'4!E":C 5 b: M7-5[:i!$C :# :9:
MR z 4C-:9:CffN0-:}C-:9_44#2#2 #%'!E"!$*:9 :9"# 9:C 5
!W32@89!$3w8;#% :48;5}'$'O:9F5}#%48;:k0:H!$*,7gj$<'"74#%-)+'4`?@#%C-:C 5x!W :9j$<8;4 "!$#%&
C-:9'O:Cw7-'O60-:d32:9?$:3@$<O32@89!$3[8;4#% :8;5 :#%-)b:-<>$"8;:CffRJzF<A:D:-<>$"8;:d!j0#%)0k3%:9?$:3@$< 32@89!$3
8;#\ :8;5$NA:1*,!`5f)$:9r!$B*k78"0i8;4 "!$#%&d'$'!E)!E#2WAB#20W!+32&:.8; "!$#%&B!$d!+*678M0
32 A:9,32:9?$:3V$<b32@89!$38;4#% :8;5 !E''43%#%:CY Z!x#2)0&8; "!$#%&9R j7-*:!$7-:l$<b8; "!$#%&
#2)0&-:rA743%Ci!$3% :17 :9<=73^#%m!+[7* :9r$<w$0-:9j!E''43%#%89!E#% >:$R )-Rd:!$#%-)+! 7-r0-:
#%*'!$8;D$<wC4# :9":gD32@89!$3^8;#\ :8;5f :8"0#\|&7-:d}!#%)32:K_-p[:C}*[C:3 MR

5

x

)

lv

TS



Vy d1:070$,7

v












4#%C-:9l! :9l$<68; M!$#%g
C-:9_4-:C `?$:9m! :9f$<6?!EM#%! 32:
/Nr!$C !$$0-:9W :9
$<68;4 "!$#%&
C-:9_4:C ?$:9}! :9l$<.?!EM#%! 32:
DNBAB0-:9":m0-:9":X#\]! # :8;#2 :9 A:9:
!$#%)*:&
}!$C
=#%H0-:d:($<O0-:d'4!E'O:9N$0#\ # :8;#2#%:#%0-:9(0-:B#\C-:g#% 5H*,!E'^N
$K0!EKC-:9_4-:C 5i0:k8M0!$-:3%3\#%-),8; M!$#%g MRbyQ:H!`5}0!Eb0-:H :9r$<8; "!$#%&
#%ku
v u du
Du D0-:6:9
AB#20W": 'O:8;d G 8;4#% :8;5 >AdM#2 :
V# wN)#2?$:x!$g5
C-*,!$#\<>$0-:#%/?E!E"#%! 32:Ng#2<
#% G 8;4#% :&/0-:]0-:d:|[7#2?E!$32:&C-*+!$#%$<
!$898;$"C4#%-)
k0-: # {:8;#2]!E:K!$3% G 8;#% :&9R 5,8;#%C:9"#%-).!$3%34'O# 3%:dC-*,!$#%/<a$0-:r?!E"#\! 32:9N
0#%.$"C-:9"#%)l*,:!$7-:j0:,'O$ :g#%!$3/<a$6C-*,!$#%K :'"7-:CQC47-"#%-)l :!E"8M0Y!$1?!E"#\! 32:
!E:k#% !$&#%!E :C}!$4CiC-*,!$#%4D'"7:C >' "# 325 5l$0-:9K8; M!$#%gr#%W0-:1' 32:* MR
$ :
0!EHA:lC4#%89710-:l:|&74#2?!$3%:gkC-*,!$#%k X0!EHA:l89!$ 8;"#%C-:9H'"#%*+!$3/!$C C7!$3*@C-:3%
#%nAB0#%8M0Y0:]?E!E"#%! 3%:1!$C ?E!$3%7-:k!E:WC# O:9:g 7-H!E:l#%Y:f x-:l:3%!E#%YAB#%0Y:!$8M0
$0-:9 MRyQ:r!50!E!. :9($<ff8; M!$#%g
#\
0!$+!. :9
Ad G 8;#% :48;5 >AB"#2 :
1#
(N
#%
@u u ;v H
AB G 8;4#% :8;5
76-$
>Ad"#% :
b# -:#%0-:9
-$
(NP!$4C T#% EuEv w G
8;#\ :8;5 >Ad"#2 :
w# $0
Q!$C
(RwyQ:b89!$,:!$#%3%56)$:-:9M!$3%# 9:



fi

fiff

fiff#

\[ 6LON 6L N*Q 7N 6L

K

5







b

5

5

! ff



-)

@



c =



b

7N






N*Q 7N}[OP





! ffq ! ff
ff ! lQTJFIHK]W KP >Z \[ 5

6.ff-
!ff ff ! # +[ jQ [4JfiN 6
p
#7ff- # !ff
ff- !
#b"!







fififfv2$ff6ff )T$
fi
ff
v~ff







fi

#

0-: :kC-:9_4#%#2D f8;*'4!E: G 8;#% :8;5W
Ar#20 rG 8;"#% :8;5W fRVcB0#%dC-:9_#2#2
$<^8; M!$#%g#2)0&-:0!$ *:K#%8;:B*$ #%89#2F5+!$C,_p[:C@G'O#%&('"$' :9"#2:(AB04#%8"0+A:bAB#%3%3
7 :j:;p@ :#%?$:325]07-)0-7-D04#%V'4!E'O:9R

Qs s; 5x { {#w}w! tv{ff
fi
ff
4 ff !" $ ! ff
ff ! ff QR] W vwQ[bLK ff 7



w'{

@#\*,#%3%!Ed*$ #%89#2F5}!$Ci_p[:C@G'O#%&d'"$'

_

)

_

@

_

9: "#2:d0-3%C}<a$ dbN bN (zbN -SBbNSB VbN
!$C KSBbRDyQ:Z!$3\ n:;p@ :C 0-::XC:9_4#2#24+ 8;*'4!E":X8;4 "!$#%&l#2)0g:lAdl :!E"8M0
!$32)$$"#%0*,f3%#2L$:x~mSB !$C h0!El*,!$#%&!$#% *:Q32@89!$3r8;"#% :8;5 C47-"#%-)n :!E"8"0^R o$
:;p-!$*'432:$N A:H!`5i0!E
#%Hu Vv "u ru
du
Adb!$32)$$"#204*
>Ad"#% :
D# wN
)#2?$:]!$g5_-p@:C?E!E"#%! 3%:d!$C,?E!$3%7-:D$"C:9"#%-).!$C]!$g5C-*+!$#%<>$/0-:B?!E"#\! 32:$< 6N
?[#\#2
-X*$:l-@C-:k x_CY!X 3%7#2Z$<
$H'`?$:W#2H7"!E#% _4! 3%:,0!$
?[#\#26 UAB#20
0-:H:|[7#2?E!$32:gKC-*,!$#\9Nff!$Cx0-::|[7#2?E!$32:gb?E!E"#%! 32:H!$Cx?E!$3%7-:k$"C-:9"#%)-R /|[7#2?E!$32:8;:H0-:9:
#%D!E)!$#%iAB#20l: 'O:8;D 0-: # {:8;#% :9FA:9:}0-:.!$#%)*:&V ,0-:j?E!E"#%! 32:V$<
!$Cl
fRwyQ:r!`5,0!E
#%
(04!$
Ad(!$3%)$$"#20*
>AB"#2 :
(#
7-
-$
R @#%*,#%3\!E/*-$ #\89#2 5+!$4C,_-p@:C@G'O#%&/'$'O:9#%:/89!$ :r)#2?$:f<a$Vo bN-~mSB
!$CX~ jSBbR ow#%4!$3%325$N4A:kAd"#2 :
#2<
!$Cm0-:9:k#%r!+'4!E"!$*:9 :9M# 9:CX :9r$<
' 32:*,J$< "# 9: i!$C!j_-p@:CH?E!E"#%! 32:D!$CH?E!$3%7-:D$MC-:9"#%-)bAB#20HAr0#%8"0 h?[#\#2w:;p@' :g#%!$3\325
<>:9A:9b-[C:d#% QAB0:i!E''43%#%:Cl
0!$iAB0:i!E''43%#%:Cl fR K7-B:732D89!$ :j:;p[ :4C-:C
+!$32)$$M#20*,V0!ED_CW!$3%3ff 3%7#29Rwz{W!$CC#2#2^N[0:95f89!$i!$3\ :j:;p[ :C:CW +!H: M#%8; :C
893%!$d$<wC5[!$*+#%8b?!E"#\! 32:j!$CW?E!$3%7-:j$"C:9"#%-) V!$898"0[79NffV0-:ffN-?E!$ :9:9L N
yZ!$3%"0ffN
MR







7N [ 6LbN 6LmN*QKN KL









$ ff !" q
#

N*Q 7N}[OP



5


% & ff ' -! ff


9f

F


v

(

9*)

)w
R R ? =0QTg7



! 5!" ffq # 5 !" ff





! ffq


p)

?

fiK373$$





yQ:K-`A 0!?$:b0-:d0-:9$":9#%89!$3*+!$8"0#%:95-:9:C-:C, k8;*'4!E:r0-:BC# :9:&(AV!5@A:B89!$f*@C-:3
!k'O:9"*67!E#2+'4 32:*T78M0f!$ !$-)$<>$"C ' 3%:*WRJcB0-:
,uEv[${G:|&7!$3\*,[C-:3$< !k'O:9 G
*k7-!E#2+04!$(-${G:|[7!$3%/8;4 "!$#%& :9 A:9:+0:D?!E"#\! 32:(#%H:!$8M0+'O:9"*k7-!E#2^RJcd0:
,uEv
!$3%3 G C4# :9":gw*@C-:3@0!$!$!$3%3 G C# :9:&w8; "!$#%& :9 A:9:0-:V?E!E"#%! 32:w#%k:!$8M0,'O:9"*k7-!E#2ffR
z{Q!ZsEtuEvff*[C:3 NffA:]#%g :9M8"0!$-)$:?E!E"#%! 32:K<a$j?E!$3%7-:RHST8;* #%-:C
u$vu sxsEtuEvff*@C-:3
0!$ $00:d'"#%*+!$3[!$4C0-:BC74!$3@?!E"#\! 32:9N!$C [u
;v>v
{u
/3%#\-L[#\-)B0-:*iNg$<O0-:
<>$"*
f#
AB0-:9:
#%d!'M#%*,!$3O?E!E"#%! 32:.!$4C 6#%d!+C7!$3ff?E!E"#%! 3%:$RS 8;* #%-:C
*@C-:3J89!$Q!$3% l0!?$:,-${G:|[7!$3%K!$C $K!$3\3 G C# O:9:&b8; "!$#%&rx0-:H'M#%*,!$3P!$4C $jC7!$3
?E!E"#%! 32:R cB0-:9:WAB#%3\3 N($<K8;7-" :$NV 5['4#%89!$3\325 :W$0-:9+8;4 "!$#%&, $0 :9,$<b?!E"#\! 32:
AB0#\8"0fC-:9'O:C,f0-:K!E7-:b$<^0-:K'O:9"*k7-!E#2]' 3%:*WRJo$D:;p@!$*'32:$N[#\
!$-)$<a$MC ' G
32:* A:.!$3% ,0!`?$:.0-:. :9'!E"!E#2W8; M!$#%gR/SrD!+ :8;Cl:;p-!$*'432:$N#%l0:K!$3\3 G #%& :9?!$3ff:9"#2:
' 32:* <>*
P# N@0-:1?E!E"#%! 3%:d!$CW0-:kC# O:9:8;: :9FA:9:x-:#2)0 $"#%-)?E!E"#%! 3%:d!E:
$0X'O:9"*67!E#29Rkz ZAB0!EK<a3\32`ABN^A:,Ci-$.8;#%C:9.C#2:8;325m0-:,8;& "# 7-#%m$<V78M0
!$CC#%#2!$3O8; "!$#\gV 'M7#%-)-R r`A:9?$:9N40-:j:!$:.AB#20fAB0#\8"0fA:.89!$i:;p['":V:!$8"0m!$CC# G
#2!$3P8; "!$#%&B#%l0:.'"#%*+!$3O$B0-:1C47!$3ff*@C-:3^!$CW0:.:73%#%-)H'"74#%-)k' A:9r$< 0: :
8; M!$#%gB*,!5WC-:9 :9"*+#%-:j7-B8M0-#%8;:.$<J0-:.'4"#%*,!$3 N@C7!$3O$b8;* #%-:CW*@C-:3 R
yQ:kAB#%3%3P7 :10:1<>3%32`Ar#%-)+7 8;"#2'4
+<a$b0-:k'"#%*,!$3P-${G:|&74!$3%r8;4 "!$#%&9N
,<>$
8"04!$-:3%3%#\-)+8; M!$#%gN
f<a$K0-:k'"#%*+!$3 -${G:|[7!$3%K!$CX8M0!$-:3%3\#%-)+8; "!$#%&9N







SW PIQR]



<A@B | X] z

WPIQR]

!WPbQR]
!FO KJfiJ [ wQRJ+FIHKJMLON*P KQRJfiNRL

@B

KJ

]



l

SM_q





Ee

N+, * - P

=< +N , * P

#b8

N.- P
N+, * /- , * P

fi 8-
fiffff

N "P

<>$.0-:'"#\*,!$3w-${G:|&74!$3%9NPC74!$3-${G:|&74!$3%.!$CQ8M0!$-:3%3\#%-)l8; "!$#%&9N l<>$.0-:'4"#%*,!$3
!$3%3 G C4# :9":g,8; "!$#%&9N Q<a$]0-:i'M#%*,!$3D!$3%3 G C4# :9":g,!$C 8"0!$:3%3%#%-)Q8; "!$#%&9Nd!$C
H<>$r0:.'"#%*+!$3^!$3%3 G C# :9:&9NC7!$3 !$3%3 G C# :9:&d!$Cm8"0!$:3%3%#%-)8; "!$#%&9RDcd0&74BSB
#%HSBT!E''43\#2:CZ X0-:l'"#\*,!$3${G:|&7!$3\k8;4 "!$#%&9N(AB04#%3% @SB
#% -SBT!E''43%#2:CY Q0-:
'"#\*,!$3O-${G:|&74!$3%B!$CW8M0!$-:3\3%#%-)8; M!$#%gR

N - "P

`N - P









s}

{#w





,* -

/

,*



{#}

yQ:b_" '`?$:b0!E9N@AB#20+: 'O:8;/ HSBbN@8"0!$4-:3%3%#%).8; "!$#%&!E:r#%)0g :9V0!$]0-:B'4"#%*,!$3
-${G:|[7!$3%d8;4 "!$#%&9N 7d32:V#2)0&d0!$W0:.'"#%*+!$3 !$3\3 G C# O:9:&V8;4 "!$#%&9R



sx Q
J u9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]
- 7 - 7 ! , * -/, * 7 , * - 7 - ! , *
u



ff




fi

fi



fi







ff



5

v

z{ 0#%k!$C <a3\32`AB#\-)}'"&$<=9NA: 7 k' ?$:W0-:W* #%*'O$!$&1":7329R K0-:9"
Qs e
<>3%32`A |&7#\8L@325$N@$<a :m7#%-)H "!$4#2#2?@#2 5$N*,-$ #%89#2F5f!$4CW0-:._-p@:C@G'O#%&V0-:9$:*,9R
c 0- KSB
SB Nb8;"#%C-:9i! 'O:9"*k7-!E#2 '" 32:* AB0-:x'4"#%*,!$3K!$3%3 G C4# :9":g
8; M!$#%g(#\ KSBbR @7-'4' :0:d8"0!$:3%3%#%-)b8; "!$#\g :9 A:9:
!$4C VAV!$/-$(SBKRcd0-:
:#20-:9 (#\B :9b f!$C k0!$ :3%#%*,#%4!E :CW<a"*h#2bC-*,!$#\ffN$ 6#%b :9B D!$C
(0!$
:3%#%*+#%!E :CW<>* #%BC-*,!$#\ffR 7r-:#20:9B$<0-: :6 AW89!$ :r#\d'O# 32: 5l0-:k8; "748;#2
$<.0-:X'M#%*,!$3B!$4C C47!$3B*@C-:3 R r:8;:X0-:X8"0!$:3%3%#%-)n8; "!$#%&W!E:X!$3%3KSBbRVcP 0-
"#\8;-:9NO8;#%C:9b! G?!EM#%! 32:k'O:9"*67!E#2}'4 32:*h#%}Ar0#%8"0
]!$C
*
(
1

RDcd0#%D#\DSB
7



$


K
B


R

/
3
c Y0- AhSB
SB
ND7-''O :l0!E,0:}8"0!$4-:3%3%#%)X8;4 "!$#%&,!E:xSBbR"#%C-:9,!
-${G:|[7!$3%b8; "!$#\g9N
D0!Eb#%B$rSBbR A.N (!$C 1*k7 b0!?$:k0-:k!$*:
#%)32:9 1C-*+!$#%ff
N R(V#%C-:9 0-:V8"0!$:3%3%#%-)d8;4 "!$#%& :9 A:9:
4!$4C &Rwcd0-:/4325.SB
?E!$3%7-:.<>$ H#% R @#%*,#%3\!E"325$N[0-:.325lSB ?!$3%7:j<a$ H#%W0-:68"0!$:3%3%#%-)8; "!$#%& :9 A:9:
!$C
}#% R 7 -R B:48;:$N i0!$H-xSB ?!$3%7:9RxcB0#%1#\6!x8;g M!$C#%8;#2 !$k0-:
8"04!$-:3%3%#\-)r8; "!$#%&(!E:dSBbR r:8;:d!$3%3--${G:|[7!$3%8;4 "!$#%&(!E:dSBbRc .0- "#%8;:9N
8;#\C-:9b! G?E!E"#%! 32:k'O:9"*k7-!E#2m' 32:* AB#20
+!$C
Rkcd0#%
*
(
1
#%DSB
7-d#%D-$BSB R
c .0- SB
SB N 5k*-$ #%89#% 5$NgSB
SB Rwc .0-`A 0:":9?$:9" :$Ng8;"#%C-:9
!f'O:9"*67!E#2x' 32:*hAB0#\8"0X#%KSB
R1cd0:m0:9:H:;p@#%b!E.3%:!$ K-:-${G
7-K-$jSB
:|[7!$3%8; M!$#%gw0!E(#\J$wSBKR$y #207-32J$<4)$:-:9M!$3%#2 5$N&32:90#% :HFA6C7!$3&?!E"#\! 32:
=!b 5[*+*:9 "#%8(!E")7*:& 89!$ :*,!$C-:<a$ Ab'4"#%*,!$3E?E!E"#%! 3%: MR [ $0.0-:!$ @89#%!E :C =C7!$3
*k7 j0!`?$:0-:!$*:#%-)3%:9 xC*,!$#%ffNff!`
?E!E"#%! 32:NO89!$3\3 0:*
!$4C
5 R B:48;:$N^0-:
C-*,!$#\ $<K0-:i'"#\*,!$3?!E"#\! 32:
x#%893\7C-: 6!$C -R #%C:90-:}8"04!$-:3%3%#\-)Q8; "!$#%&
m!$C
R
0#%H#%k-$SBT!$H0-:l?!$3\7-:
Q04!$kX"7-''O$9RQcd0#%k#%H!
:9 A:9:
8;& "!$C#%8;#2^R
c W0-`A jSB
jSB N^8;#%C-:9K!l'O:9"*k7-!E#2x' 3%:* 0!Ej#% KSB Rko-$j:9?$:95
'O# 32:b!$"#2)*:&D$<w!?!$3\7-:j +!?E!E"#%! 3%:$N[0:9:.:;p-#% d!8;#\ :gB:;p[ :"#2W ,0-:1$0-:9
?E!E"#%! 32:N *
! """ #
%$lAB#20
k<a$j!$3%3 & -R1Sbb0#%K#%b!f' :9M*67-!E#%ffNO0#%
8;$:' 4C( k0-:r!$"#2)*:&/$<^74#%|[7-:D?!EM#%! 32:/ 6?E!$3%7-:9R r:8;:$N@0-:r8;$": 'OC#%).C7!$3
!$3%3 G C4# :9":gD8; M!$#%gr#% KSBbR4ow#%4!$3%325$N@0-:18M0!$-:3%3\#%-)H8; "!$#%&d!E:1 "#2?@#%!$3%325+SBKR('



@B



@ @ r~{YM{`"

-

!


|

]

)

"

- !

G

,*

-

, * -/, * 7

8

X]

]B =

@B

]4^

@ YV@ >1K{`

cz

@B

|

mz

2|

@
@B

_]4^

@ >1K{`~{`

-

, * - * , * -/* !
, ,

E

G e

@0^ |

- 7)

@ ] 4{ @ ]



=

-

D@^

]



a]

@ @ @ >1K{`

@

]4^

$e

-

]B

@^


e

*
@B , @ cRz |~





z

-

]4^ z
O]4^ B| ) z | e

@

+@B

2

@B YV@

z |

K

#b%)

ge





fififfv2$ff6ff )T$
fi
ff
v~ff



#%-)b0-: :d#\C-:g#%#2:9NEA:D89!$#%*+*:C#%!E :325.C:C78;:$N<>$#% !$8;:$N$04!E(#2JC&:-$#%48;:!$ :
'"74#%-), i04!?$: $0Q8"0!$4-:3%3%#%)]8; M!$#%gj!$4Cm'4"#%*,!$3 >$1C7!$3 d-${G:|[7!$3%j8;4 "!$#%&9R
${G:|&7!$3\}8; M!$#%g}C- -$i#%48;:!$ :Q0-:Y!$*,7gW$<8; "!$#%&W'$'4!E)!E#2 `?$:9m0!E
!$8"04#2:9?$:CXAB#20X8"04!$-:3%3%#\-)+8; M!$#%gj!$3%-:$R.SrK7b:;p@'O:9"#%*:&b0-`A 3\!E :9b^Nff0-:95}3%5
!$CC `?$:9"0:!$C n0-:x8; "!$#\gl 32?$:9R zFl#%]#%#2)0& <=73D :;p[ "!$8;l<>* 0-: :x'[$<>]0-:
:!$ 4HAB0g5n!E"8 G 8;#% :8;5n'O:9<>$"*,kC# O:9:&k!$*,7gH$<b8; "!$#\gH'$'!E)!E#2 #% 0-:
C# O:9:&d*@C-:3%9RSr"8 G 8;#% :8;5WC-:32:9 :d?E!$3%7-:D#\l0-:1C-*,!$#\V$< ?E!E"#%! 32:D!$d<>3%32 AB



=

R







=<

t2} {st2w'{ #2<0-:C-*,!$#%m$</!$&5}$<(0:k'4"#%*,!$3P?!E"#\! 32:B#\r":C78;:C

!i#%-)3%:9 >:#20-:9 5x8; "!$#%&.'$'4!E)!E#%Q$ 5x!$"#2)*:&j#%Q! !$8"L& "!$8"L[#\-)
!$3%)$$"#20* MN$:<a$"89#\-)bSBnH0:'4"#%*,!$3[-${G:|[7!$3%w8; "!$#\gw:*,`?$:0#\ ?E!$3%7-:<>*
!$3\3ff$0-:9d'M#%*,!$3 ?!EM#%! 32:9R

sw'x

t2{



l

}ffut2{l{

''w'{]}

?>

!$,AB#20 '"#%*+!$3-${G:|[7!$3%+8; "!$#%& D#% !$CC#%#2ffN/#2<B0-:iC-EG
*+!$#%i$<(!$&5WC7!$3^?E!E"#%! 32:.#%d":C78;:Ci f!+#\-)32:9 ffN:<a$"89#\-)+SB i0-:k8"0!$:3%3%#%-)
8;4 "!$#%&b:*`?$:K04#%B?E!$3%7-:6<a"*h!$3%3P$0-:9jC7!$3 ?!EM#%! 32:9RBz m'4!E#%89743%!EN#%<w!]?E!$3%7-:
@89897-"J#%j0:C-*,!$#\.$< 74 P-:$0-:9J'"#%*,!$3E?E!E"#%! 32:$N :<a$"89#\-)BSBZ.0-:8"0!$:3%3%#%-)
8;4 "!$#%&d:"7-:V0!EB$0-:9B?E!$3%7-:.89!$ :.!$"#2)-:CW ,04!Ed'"#%*+!$3?E!E"#%! 32:$R
{#st2w'{

r5






t2Ttv ,w



{} {#st2w'{ :-<>$"89#%-) KSB m!'"#\*,!$3ff!$3%3 G C# :9:&d8; M!$#%grAB#%3%3
'4"7-:,!$3%3(0-:+?E!$3%7-:j04!E6!E:]:*`?$:C 5x:<a$"89#\-)}SB Y0-:+'4"#%*,!$3-${G:|[7!$3%1$
8M0!$-:3\3%#%-)18;4 "!$#%&9Rz{]!$CC4#2#2ffN&:-<a$M89#%-) jSB #%/ *,:9#%*:V! 32:b 6'4"7-:r$0-:9
?E!$3%7-: >:$R )-R#%< A:j0!?$:KFAH'"#\*,!$3-?E!E"#%! 32:AB#20+325FAH?E!$3%7-: :9 A:9:]0:*WN&0: :
?E!$3%7-:dAB#%3%3 :K:*`?$:Cm<>* !$3%3O$0-:9r'"#%*+!$3?E!E"#%! 32: MR

sw'x







z{ "#2:9< N-SB W0-:j'4"#%*,!$3O-${G:|[7!$3%d8;4 "!$#%&dC-:9 :8;b#%-)3%:9 l?!E"#\! 32:9N@AB0#\3% VSB
0-:k8"0!$:3%3%#%-)]8; "!$#%&bC-:9 :8; $0x#%)32:9 i?E!E"#%! 3%:6u sk#\-)32:9 i?E!$3%7-:R KSB
!x'"#%*+!$3(!$3%3 G C# :9:&68;4 "!$#%&9N( 0-:f$0-:90!$CffN(C-:9 :9M*,#%-:6)32 !$38;#% :8;5ZAB0#\8"0
#%893\7C-:V#%-)3%:9 l?!E"#\! 32:9N#\-)32:9 l?E!$3%7-:d!$CW*+!$g5l$0-:9b#27!E#249R





t2w'{t2w'{lw'{

H



KJ



s}

{wD

{#}



X

cd0-::d:73%(89!$ :B3%#%<a :C k!$32)$$"#204*,0!E*,!$#%&!$#% >)$:-:9"!$3\# 9:C w!E"8 G 8;4#% :8;5,C47-"#%-)
:!E"8M0ffRHz 4C-:9:CffNO0-:)!E'4 :9FA:9:Q0:k'4"#%*,!$3w!$3%3 G C4# :9":gK!$CX0-:8"04!$-:3%3%#\-)f8;4 "!$#%&9N
!$C :9 A:9:Q0-:8"04!$-:3%3%#\-)]8; "!$#%&j!$CX0-:H'"#%*+!$3 -${G:|[7!$3%j8;4 "!$#%&j89!$ :6:;p[G
'O-:&#%!$3%325,3%!E")$:$R
$ :10!Ed-$d!$3%3OC# O:9:8;:D#\f8;4 "!$#%&D#2)0&-:V:7432D#%f:;p['O-:&#%!$3
:C748;#2/#\+ :!E"8M0ffR(o-$D#%!$8;:$N& *,:bC# :9:48;: :9 A:9:l*[C:3%(AB04#%8"0,!E":r3%5k'O325@-EG
*,#%!$34!E:B#%C-:&#2_:C#\]V0-:-)j:9!$3 R
MR d:89!$3%340!E(A:BAd"#% :
Y#
Y!$C
0-:9:H#%b!+'4 32:* mAr0#%8"0x!$32)$$"#%0*
?@#%#%d:;p@' :g#%!$3\325l<a:9A:9 "!$8M0-:bAB#20
0!$
fR
$ :H0!E KSB
!$CXSB !E: $0x'O325@-*,#%!$3^ f:<a$"8;:$N^l!$x:;p@'O-:g#\!$3P:C478;#2
#% M!$8"0-:d "!$3%!E :d ]!$W:;p@'O-:g#\!$3O:C78;#2}#%l"7&#%*:$R



=



d1:07070$



u




&" ff #! ff





=





sx Q
J u9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]
& , * -/, * 7 , * - 7 - &
ff




fi ff

Qs e









-v

yQ:r)#%?$:B'[$<=(<>$/0-:b* #\*'O$!$g#%C-:&#2#2:9R b0-:9:732/<a3\32`A
0-:.3%!$d0-:9$:*WR



<>*

, *

#br

#%*,*:C#\!E :325

fi 8-
fiffff

$

&

RoI$
@ @ :o c
1K{YoAI~{YoAI

@BLY

c 0-`A ~ KSB
~}SB N@8;4#%C-:9D!
G?!E"#\! 32:b' :9M*67-!E#%]'4 32:*qAB#20
<>$

(!$C # (
#

Rwcd0-:ffNE)#2?$:k!B32:;p-#%8;$)$"!E'0#%89!$3
1
?E!E"#%! 32:K$"C:9"#%-)-N4~ KSB
#%*,*,:C#%!E :325]<>!$#%3\9N@AB0#%3%V~mSB
!EL$: "!$8M0-:9R
c ]0`A ~}SB
~mSB
N8;4#%C-:9B!
G?E!E"#%! 32:1'O:9"*k7-!E#2W'4 32:* Ar#20
*

N !$4C
"""
<a$
[R6cB0-:ffNO)#2?$:Q!l32:;p-#%8;$)$"!E'40#\89!$3P?E!E"#%! 3%:6$"C:9"#%-)-N
~mSB
!EL$:
!EL$:
"!$8"0:9R'
"!$8M0-:D +0`A #\ 3%7 #%3%#2F5$NgAB0#\3% D~}SB

z \ oAIc1

>1K{"""6{Yo


$

- &

>1K{`

*
@B*Y r~{ 6{Yo I%,

-

z







t$s



}fiffaw{

u

-

Ro $





,*

@

MRo6J 18







~m!$#%g!$#\#%-) >)$:-:9M!$3%# 9:C (!E"8 G 8;#% :8;5]3\!E)$:d'O:9"*k7-!E#2,'4 32:*,/89!$ :D:;p@' :4#2?$:$R
yQ:,*,!`5}0-:9:9<>$:HC-:89#%C-:k W7 :H!l8"0-:!E'O:9.32@89!$3 8;4#% :8;5m'$'O:9F5i3%#2L$:k0!Ej*,!$#%&!$#%-:C
5k<>$AV!E"C,8M0-:8L@#%-)-R(o-$:;p@!$*,'432:$Ng0:bV0-@8;j_4#2 :;G C*,!$#%H [32L[#%(#%]V3%!$#%:D7 : 7 /4o
!$3%3 G C# O:9:&,8; "!$#\g9R cB0-:}8"04!$-:3%3%#\-)Z8; "!$#\g]:*,!$#% #2)0& :9]0!$ 0-:}'4"#%*,!$3
-${G:|[7!$3%d8;4 "!$#%&dAd"do bR

5



x
u



J$ ff ! $




3

JQu9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]ff


, * -/, * 7 $ , * - 7 $ - ! $ , * ! J$








! J6$
K:gW:9W!$3 RVK37373>f' ?$:Qo , * !



J$ ff



3

oJ
R c 0- "#%8;-:f 'O:9"*k7-!E#2
Qs e
' 32:*, =!$]$''O
: C n0-:}*$:})$:-:9"!$3B893\!$,$<.C-:8;*,' "! 32:i8; M!$#%gf 7C4#2:C 5
b:&9N [ :9)#%7ffN!$C Z
!$3%0ffN
MN8;4#%C-:9H!
G?E!E"#%! 32:]'O:9"*k7-!E#2n' 32:*UAB#%0
*

f!$C

RHz : 'O:8;#2?$:H$</0:k?E!E"#%! 32:k!$4Cm?E!$3%7-:H$"C:9"#%-)-N
(
1
/
3
0- ABr0-:1'" 32:* #\r74!E#% _4! 32:.#%}!EK*
"!$8M0-:9RBoJ
5i8;*'4!EM#% i!EL$:
!EB32:!$
"!$48"0-:9R
c x"0-`A
oJ
N8;#%C-:9k!$#2)4#%-)i0-:f?E!$3%7-: x x0-:]'"#%*,!$3(?!E"#\! 32:
FRmo
:* ?$: H<>*q0-:jC-*,!$#%]$<P!$3%3 $0-:9V'"#\*,!$3?E!E"#%! 3%:9Ro
#% !$&#%!E :V0-:KC7!$3?!EM#%! 32:
AB#%0 0-:}?E!$3%7-: NB!$C 0-: :*,`?$: k<a* 0-:mC*,!$#% $<.!$3\3D$0-:9f'"#%*,!$3d?!E"#\! 32:9R
r:8;:$N oJ
'M7-:D!$3%3P0-:.?E!$3%7-:d04!Ero
C-&:RVc ]"0-`A "#%8;:9N48;"#%C-:9B! EG?!E"#\! 32:
'O:9"*k7-!E#2W'" 32:* AB#%0

,!$C

R K#2?$:x!,32:;p-#%8;$)$"!E'0#%89!$3
*
(
1
/
?E!E"#%! 32:!$C6[7*:9M#%89!$3?E!$3%7-:($MC-:9"#%-)-NEo
0- ABP0:/' 32:* #% 7!E#\ _4! 32:(#\
"!$8M0-:9R

5W8;*'4!E"#\ l!EL$: 7
"!$8M0-:9R
b:&1:96!$3 R
.'`?$:fo
oJ
R]c }0- 0-:+:9?$:9" :$Nw8;#%C:9.!$#2)#\-)W0-:
?E!$3%7-: l W0-:k'"#%*,!$3 ?E!E"#%! 32:
R1o
:* ?$: l<>* 0-:C-*,!$#%x$</!$3\3 '4"#%*,!$3P?!E"#\! 32:
:;p-8;:9'
R r`A:9?$:9N^o
!$3% ]:* ?$: l<a"* 0-:HC-*,!$#\}$<(!$3%3 '"#\*,!$3^?E!E"#%! 3%:d:;p-8;:9'
#%48;::!$8"0Z@89897-"1#%Q! #%4!E5}-${G:|[7!$3%.8;4 "!$#%&.AB#20
!$#%-:C 5m'4 {:8;#\-)W7-
0-:1!$3\3 G C# O:9:&V8;4 "!$#%&9R r:8;:$N4o

R
c ]0`A 4o
oJ
N8;"#%C-:9B#%4 !$g#\!E#%-)0-:6'"#%*,!$3O?E!E"#%! 32: wAB#20i0-:.?E!$3%7-:
-RKo
:* ?$: f<a*h0-:kC-*,!$#%m$<(!$3%3 '"#%*,!$3^?!E"#\! 32:d:;p-8;:9'
N <>*h0-:kC*,!$#%
$<^!$3%3C7!$3?E!E"#%! 32:(:;p-8;:9' $N[#% !$&#%!E : BAB#20+0-:B?E!$3%7-: N@!$C+0:,:* ?$: <>*q0-:
C-*,!$#\,$<P!$3%3 C7!$34?!EM#%! 32:/:;p@8;:9' R(o
!$3% k:*`?$: k<>*q0-:KC-*,!$#%+$<P!$3%3'4"#%*,!$3
?E!E"#%! 32:K:;p-8;:9'
{R,cd0-:325x'O# 32:kC4# :9":8;:#%.#2<-:,$<0-:,$0-:9kC7!$3w?E!E"#%! 32:N^!5
04!$b!lC-*+!$#%xAB#2'O:97-9RbzF<(04#%K0!E''O:9N /04!$K-:H?!$3%7:k#%x#2KC-*,!$#\ffN 0!Ej#%b#%x0-:
C-*,!$#\Z$<B-m$0-:9k'"#%*,!$3/?!EM#%! 32:$R -<a$M89#%-) KSBq#%*+*:C#%!E :325XC-:9 :8;H0!E
d89!$4-$

p


@ @ @ >1K{`~{`
1:.

- !

q|

]

-

e

-

@B



|

H@B


]

,*

@B

1:

3

m|

z

g5



1 !

@B

1

!
S|

@

"

@ :M{`"

D@ >1K{`

nVK37373>

,* - ,*

|

Xz

-K37373>

, * -/, *

,*

, * S|



pf



9

|

,*

]

]

-

, * @ @ Y[@ r~{Y

1 7

qe

]

,*

@B

@ B

z
-|



0@B

#b#b

#

65

@B z

@B

z

@B

,*

fififfv2$ff6ff )T$
fi
ff
v~ff



G 9e

B|



!EL$:0-:H?E!$3%7-: N^!$CX*674 b#\ :!$Cx!EL$:0-:k?!$3%7:
R r:8;:o
0!$K!lC-*,!$#%mAB#2'O:97-
AB0-::9?$:9VoJ
C &:Rwc 0- "#%8;-:N@8;4#%C-:9V! `G?!E"#\! 32:b' :9M*67-!E#%+'" 32:*qAB#20
f!$C

R1z : 'O:8;#2?$:6$<(0-:k?!EM#%! 32:
*
(
1
/
3
4
!$Cx?E!$3%7-:k$"C-:9"#%)-N
!EL$:.!Ej32:!$
M!$8"0-:b W0- 0-:H' 32:*h#%K7!E#\ _4! 32:$R

5l8;*'4!E"#%W!EL$:r,*$:.0!$
"!$8M0-:9R
:# :9":j:9d!$3 R
V' ?$:jo
oJ
RcP0- "#%8;:Vf' :9M*67-!E#%l' G
32:*,NP8;#%C:9.!
G?!E"#\! 32:'O:9"*k7-!E#2X' 32:* AB#20

i!$C
*
(
1
/

RQz :' :8;#%?$:]$<d0:]?E!E"#%! 3%:+!$Cn?!$3%7:]$"C:9"#%-)-Nwo f0- ABk0-:f' 32:*
#%
3
7"!E#% _4! 3%:6#%X!Ej32:!$
"!$48"0-:9Rko
5m8;*'!E"#% x!EL$:1l*,$:H0!$
"!$8M0-:9R

* -/*
@ @ @ , , @ >1K{`~{`

)

+,
@ @ @ :M{`"~{`&~{I,
&
p
!
1

, * -/, *

Vy

d1:07070$
p"

@ :M{`"

@ @ @ @ >1K{`~{`

&

1





'





r






{ #

{}

{#w


5

Sb-$0-:9w8;*,*k*:90-@C1 b:C78;:8; w#%^ b:-<>$"8;: 7 74C^8;"#% :8;5$Rwo-$w:;p@!$*,'432:$N
7CD8;4#% :8;5l#%d74 :CW +'4"7-:j!)32 !$3^8; "!$#\gB#\g?$32?@#%-),!+7* $<w?E!E"#%! 32:D!$4C}!
:9d$<#%:|&7!$3\#2#2: :9 )#%
r7-:0-:9N
MRdSbB!,:8;Ci:;p-!$*'432:$N *:.$<w0-:.:;p@'O:9"#%*:&
Q'O:9"*k7-!E#2X'" 32:*,b'O:9<>$"*:C 5 @*+#20
j7 :C 74CK8;#% :48;5mZ8;:9"!$#%
$<D0-:f8; "!$#%&9Rly #20 7C4j8;#% :48;5QZ'O:9"*k7-!E#2Z'4 32:*,NPA:+ !$#%n!i?$:95
#%*+#%3%!EV$"C-:9"#\-)H$< 0-:6*[C:3%D!$dAB#%0WSBbR

9

sx

u




#?

fiK37373>


VK37373>

JQu9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]ff


- ! ,*

,*
Qs e c i0- )d - ! )d , * NP8;#\C-:9j!l' :9M*67-!E#%X' 32:* AB04#%8"0X#%)d - 7-K-:$<
0-:'M#%*,!$3J-${G:|&74!$3%.8; M!$#%gj#\j-$9)dbRffcd0-:ffN^#2KA73%CQ#%&?$32?$:H Ai?E!E"#%! 3%:9NZ@BV!$C




!





,* - ,* 7





,* - 7







ff



@

LE G { G7G qf

/)
E zu{Yz G mf

0lAr#20W#%C-:&#%89!$3O#%& :9?!$3ffC*,!$#%9N
R -<>$"89#%-) l0-:.8M0!$-:3%3\#%-)H8; "!$#%&
$
!$C
A73%Cx:C748;: , l0-:C-*,!$#%
R /-<>$"89#%) x0-:8"0!$:3%3%#%-)
:9 A:9:
8; M!$#%g :9 A:9: b!$C
KA743%C,0-:f89!$7 :b!6C*,!$#%,Ar#2'O:97-9R V7-0#%8;g M!$C#%8;0-:
8"04!$-:3%3%#\-).8; "!$#\g :#\-) dbR r:8;:$N@!$3%30:B'"#\*,!$3--${G:|[7!$3%8; "!$#%&*67 : dbR
c }0- "#%8;:9R,8;#\C-:9.!
G?E!E"#%! 32:'O:9"*k7-!E#2X'4 32:* AB#%0
D!$C
*
(


R

c

0
\
#


%
#



7



$




R

1
c m0`A

NJ8;"#%C-:91!}' :9M*67-!E#%Q' 3%:* AB0#%8M0Z#% R @7-''O :HA:
!$#%)x! 7C4!E5W?!$3\7-: W i!]'4"#%*,!$3P?!E"#\! 32:$N
>$j:|[7#2?E!$32:&325$Nff! 74C!E5W?E!$3%7-:
!WC7!$3J?E!E"#%! 3%:$N MRHSrK0:,!$3%3 G C# O:9:&b8;4 "!$#%&j#% dbNO0#%j89!$ :H:;p[ :C:CX i!$3%3w0-:
$0-:9d'4"#%*,!$3 ?E!E"#%! 32:D74#%-)H:!$8"0m$<J0-:j?!$3\7-:D8;:$R/cB0#%V)#2?$:d74d!,8;#\ :gd!$"#2)*:&
<>$.!$g5m$0-:9j'"#%*+!$3P$.C74!$3 ?E!E"#%! 32:$R B:48;:$NP#2j#%
R6c l"0-`A M#%8;-:9N^8;4#%C-:9j!
G?E!E"#%! 32:'O:9"*k7-!E#2X'4 32:* AB#%0
V!$C
R]cd04#%j#%
*
(
1
/
3

7-d$ R
c j0-`A SB
N8;4#%C-:9w!B'O:9"*k7-!E#26' 32:* Ar0#%8"06#% 7 -$wSB
Rwcd0-:
0-:9:*k7 :6:H8;4 "!$#%&9N ENffAB#20
!$C 04!?@#%-)f0-:!$*:#%-)3%:9 xC*,!$#%ffN
R V7-9NP#2<0#%j#%K0:,89!$ :$NP:-<>$"89#%)
Z0-:,8"0!$4-:3%3%#%)]8; M!$#%g :9FA:9:
!$C
6!$C :9 A:9: K!$C
.A73%C]'`?$:j04!ED0-:K' 32:*T#%V7"!E#% _4! 3%:$R r:8;:$N#2V#%VSB
R
c m0- Aq M#%8;-:9Nw8;#\C-:91!
G?E!E"#%! 32:,'O:9"*k7-!E#2Z'4 32:* AB#%0

$
!


C
*
(
Rcd0#\D#%DSB
7-D-$ R '
1

@ B

@ E1K{`"G

]4^
@
)

]7^

*

! ) , , * -/, *
|

)

]7^

) e
X
)

]

)

"

@ @ E1K{`"G

-

G +)
4] ^

@ E1K{`"G

9)

@B



S)

,* ! ) -

D@

]7^

,*

Z@B @
!
)

)

)

@B

p

@

)

mz

)

* -/*
@ @ @ , E 1K, {`"G
9e

, * -/, *

l)
)

X@ @ E ~{`""G
)

-

,*

@B

e
*
@ YF@ E1K{`"G ,

-

#b

fi 8-
fiffff





xb:

Dsw}
ft$u
m)



{#w

*y 9d1:070$,7
m_

{}

,_

"7-5@-:V!$C :"# :9:
w0!`?$:d0-`Ar10!E
n#%J!B'"*,#%#%)d_432 :9"#\-)B :8M0#%|[7-:V! ?$:
SBbR zFr'"7:B*,!$&5W$<(0-: (z ?E!$3%7-:b!Eb3\#2 32:.:;p@ "!f8; r lSBbR @7-'4"#%#%)325$N-8"0!$:3%3%#%-)
8; M!$#%gH!E:l#%8;*,'4!E"! 32:] m0:f'4"#%*,!$3(${G:|&7!$3\k8;4 "!$#%&kAd
KRwV0!$4-:3%3%#%)
8; M!$#%gD89!$}#%8;:!$ :K0-:j!$*74gD$< '$'4!E)!E#2 ><a$D:;p-!$*'432:$N@AB0:W!kC47!$3 ?!E"#\! 32:K0!$
325m-:,?E!$3%7-:32:9<>.#%Q#2jC-*+!$#% MR r`A:9?$:9N
#\j0#%C-:9":C 5x0-: #2'4!E"#2 :8; "!$#%&
)$"!E'40 :9FA:9: '"#%*,!$3K!$C C47!$3K?!EM#%! 32:9R SbCC#2#24!$3r-${G:|[7!$3%}8; "!$#%&W '4"#%*,!$3
!$C $rC7!$3O?E!E"#%! 32:D89!$}0-:9:9<>$:10-:3%'W'$'4!E)!E#2^R

+e

sx

u



_





_

JQu9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]





! , * -/, *
! , * - ! - .
, * . Qs e cPX0- _ - .3 _V * Nw8;4#%C-:9k!XEG?E!E"#%! 3%:+'O:9"*k7-!E#2Z'4 32:* AB#20X@ *
@ ( @ 1 >1K{`~{`m!$CO@ / >1K, {`~{`~{Y Rncd0#\6#%* _V , * 76-$ _V - RXo$k0:]:9?$:9M :
C#2":8;#2ffNd8;#\C-:9f!" G?E!E"#%! 32:x'O:9"*k7-!E#2 ' 32:* AB#20 @ * @ ( @ 1 >1K{`n!$C
@/ @3
r~{YM{`"RDcd0#%D#\ _ - 7-d-$K _ , * R
c 0-`A
_ , * - ! _ - NB8;4#%C-:9l!E)!$#% 0-:X3%!$ f:;p@!$*'32:$R cd0#%]#%H _ - 7-l-$
_V , * - R
c 10- _V * -/* ! _V * - Ng8;#%C:9(!9& G?!E"#\! 32:D'O:9"*k7-!E#2H' 32:* AB#20`@ * @ (
, , @ , @ @ :M{`"~{`&RVcB0#%V#% _ * - 7-d$K _V * -/* R
>1K{`~{`~{YM{`"~{`&]!$C @
1
/
3
c ]0`A j
SB !; _ , * - , * N48;4#%C-:9b!'O:9"*67!E#2i'" 32:, * AB04#%8"0i#\ K SB , R , @7-''O :
A:+!$#2)Y!W?!$3\7-: |i m!W'"#%*+!$3 ?E!E"#%! 32:$NL@ B >$6:|&7#%?!$32:&325$NP!i?!$3%7:/zB }!iC47!$3w?!EM#%! 32:$N
] 6MRmSb10:f!$3%32G C# O:9:g68; "!$#%&6#\K
SBKN 0#\189!$ :,:;p[ :4C-:C x!$3%3/0-:f$0-:96'4"#%*,!$3


fi

?E!E"#%! 32:K7"#%-)]7'X!$3\3 0:k$0:9j?!$3%7:9R1cd0#\B)#2?$:17K!f8;4#% :&.!$#2)4*:gK<>$j!$g5x
$0-:9r'"#%*,!$3 $bC7!$3O?E!E"#%! 32:R B:48;:$N40-:.'4 32:* #% /z
!$CW0[7
V
Rc +0-
"#\8;-:9N^8;"#%C-:9.! `G?E!E"#%! 32:H'O:9"*k7-!E#2X'" 32:* AB#20


*
(
1
/
!$C


RDcd0#%D#\


7



$


K
B


R

3
4
c 60`A SB
V
N[8;"#%C-:9(! EG?E!E"#%! 32:D'O:9"*67!E#2' 32:* AB#20
*
(
1

r!$C

Rcd04#%P#%
V
7- -$SB Rwo-$0-:V:9?$:9M :C4#2:8;#2ffN$8;"#%C-:9
/
! G?!E"#\! 32:b' :9M*67-!E#%+'" 32:* Ar#20
1!$C
RVcd0#%
*
(
1
/
3
#%DSB

R '
7-D-$

X,
D@ @ @ :M{`"~{`&~{I,
- .> _ *
>1K{`~{`
@ >1K{`~{`~,{Y
"
K_ , *
-

fiff

tu D{





{wD

qe

_

, * -/@ , * @ @ _ ,@* -/, * >1K{`~{`

_ , * -/, *

9
_ , *
@ @ @ >1K{`

`@ @ @

@ @ r~{YM{`"

{#}

cd0-:,#\8;*'4!E"! #%3%#2F5W$<8M0!$-:3%3\#%-)l8; "!$#%&1!$4CX'"#%*,!$3w-${G:|[7!$3%18; "!$#%&j:*+!$#%
AB0-:lA:1* ?$:67-'W0:132@89!$3^8;#% :8;5f04#2:9"!E"8M0g5f<a"*
V (zbR
u



xff



( _

JQu9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]ff


l_

! , * - , * ! , * - ! - . , * . Qs e cP 0- AG_(z - . _(z * Nd8;#%C-:9f!EG?E!E"#%! 3%:i'O:9"*67!E#2 '" 32:* AB#%0R@ *
@ ( @ 1 >1K{`~{`Y!$4C @ / , >1K{`~{`~{Y R cd0#\,#%s_/z , * 7-]$l_(z - Raf/<a$"89#\-) _(z
0:}8"0!$4-:3%3%#%)X8;4 "!$#%&,:C478;:+@ / Y0-:}"#%-)32:9 C*,!$#% : R o-$f0-:i:9?$:9M :
C#2":8;#2ffNd8;#\C-:9f!" G?E!E"#%! 32:x'O:9"*k7-!E#2 ' 32:* AB#20 @ * @ ( @ 1 >1K{`n!$C
@/ @3
r~{YM{`"RDcd0#%D#\ _(z - 7-d-$ _/z , * R


fi

#b8

fififfv2$ff6ff )T$
fi
ff
v~ff



c x"0-`A _(z * - !
_/z - N(8;"#%C-:9H! " G?E!E"#%! 3%:+'O:9"*k7-!E#2n' 32:* AB#20 @ * @ (
@ 1 > 1K{`k!$CD@ , / @ 3 Yr~{YM{`"RDcd04#%V#%S_(z - 7-d$S_(z , * - R
c ]0`A _/z * - * ! _(z * - N48;#\C-:9B!s& G?E!E"#%! 3%:.'O:9"*67!E#2W' 32:* AB#20D@ * @ (
, , @ , @ @ :M{`"~{`&RVcB0#%V#%S_/z * - 7d-$S_(z * -/* R
>1K{`~{`~{YM{`"~{`&]!$C @
1
/
3
c }0- K
SB ! _/z , * -/, * NJ8;#%C:9.!W'O:9"*67!E#2X' 3%,:* #%XAB0#%8M0X0-,:,!$, 3%3 G C4# :9":g
8; M!$#%gr#% K
SBKR@@7-'4' :bA:6!$"#2)i!+?!$3%7: |+ f!,'M#%*,!$3O?E!E"#%! 32:$N<@BS>$r:|[7#2?E!$32:&325$N4!
?E!$3%7-:z( ,!C7!$3 ?E!E"#%! 3%:$N6
] 8MR/SbD0-:j!$3%3 G C4# :9":gV8; "!$#\gd#%Kj SBbN-0#%V89!$ :K:;p[ :4C-:C

}
!$3%3(0-:$0-:91'4"#%*,!$3J?E!E"#%! 32:17#%-)W7-'Z!$3\3w0-:,$0-:96?!$3%7:9R,cd0#%K)#%?$:17.!}8;#% :&
!$#%)*:&^<a$ !$g5jFAr$0-:9P'M#%*,!$3$ C74!$3$?!EM#%! 32:9R B:8;:$NE0-:-${G:|&74!$3%P!$C18"0!$:3%3%#%-)
8; M!$#%gH!E: (zbRwcPX0- "#%8;-:"9N(8;#%C:9k! `G?!EM#%! 32:f'O:9"*67!E#2Y'4 32:* AB#20

}!$C

RQcd0#%j#% (z
71-$
*
(
1
/
3
4
KSB R
c ,0- (z
SB N8;4#%C-:9D! EG?!EM#%! 32:K'O:9"*k7-!E#2f' 3%:*qAr#20
*
(
1

!$C

RVcB0#%D#% /z
7V$BSB R -<a$M89#%-)SB i0:.8"0!$:3%3%#%-)
/
8; M!$#%g1:C78;:
/ m0-:+#%)32:9 ZC-*,!$#\ RWo-$10-:]:9?$:9" :]C#2:8;#2^N 8;#\C-:91!
G?E!E"#%! 32:6'O:9"*67!E#2}'4 32:* AB#20
+!$C
R6cd0#%
*
(
1
/
3
#%DSB
R'
7-D-$ /z

_

,

@ @ YF@ YF@ >1K{`~{`

a_ , * .
>1K{`~{`
D@ >1K{`~{`~{Y
_@
"

-

aw'{



xb:

,*

_


{



s}

)

{#w

e

@ YF@ YF@ :M{`"~{`&~{I,


_

,*

:

Vy %d1:070$,7
_
+_

!

, * -/, *

@ @ @

- qf

@ @ @ >1K{`

{#}

_

@ @ r~{YM{`"

" 7-5@-:Q!$C :# :9:
l!$3% 0`A:C 0!E @SB #%l! '*+#%#%-)n_432 :9"#\-)Y :8M0#%|[7-:
! `?$: $0+SBbN V !$C /z KN$'"7#\-)b*+!$g5k?!$3\7-:<>$#2
#%*:$R (" :9(:9/!$3 R
:9'O$ :C]'*,#\#%-)K:;p@' :9M#%*:&!$34:"732(Ar#20 @SB f|&7!$"#2)$7-']' 32:*+9Ng!k*673%#2'432:D'O:9 G
*k7-!E#2m' 32:*iRDz & :9: #%)325$N !$rAB#20mSB
!$C /z AB0#%8M0}3%#%: :9 A:9:
7-r743%#2L$:
SB !$C -SB MN 8M0!$-:3%3\#%-)i8; M!$#%gk!E:f#2)0& :960!$n0-:]'"#%*,!$3/-${G:|&74!$3%68;4 "!$#%&
Ad -SBbR



u


c



sx





_

g_

2_



fiVK37373>

!_

JQu9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]ff


,* - 7
- ! ff , * . ff Qs e c }0`A @SB - ! -SB , * N 8;#%C-:9.!l'O:9"*k7-!E#2X'4 32:* 0!E.#%9@SB - !$CQ!$g5
#%!$g#%!E#% <a$,!X'M#%*,!$3?!EM#%! 32: @ B R @7-''O :f0!E0-:W'4"#%*,!$3-${G:|[7!$3%,*@C-:3V$<b0-:


fi

!

,* - ,* 7









:7432#%-)]' 32:* 89!$-$ :H*,!$C-:HSBbRffcd0-:x0-:9:H*674 K:;p@#%rFAW$0-:9j'4"#%*,!$3P?!E"#\! 32:9N
!`5 H!$C
Ar0#%8"0X0!`?$:,!E.* j-:H$0-:9j?E!$3%7-:$RH#%C:9b0:HC47!$3J?!E"#\! 32:6!$" [89#\!E :C
AB#20k0#\P?E!$3%7-:$Rwcd0-:H7C:9J04#%w#% !$&#%!E#26$<0:'4"#%*,!$3g?!E"#\! 32:
N:-<>$"89#%-)KSBnH0-:
8"04!$-:3%3%#\-)]8; "!$#%& :9FA:9:Z0-:H'"#%*,!$3 ?E!E"#%! 32:
V!$CX0-:C74!$3 ?E!E"#%! 32:$N^!$C :9 A:9:
!$C
0-:dC74!$3&?E!E"#%! 32:!$4C
d:"732#%H!jC-*,!$#\6Ar#2'O:97- ,0-:dC7!$3[?E!E"#%! 32:$R B:48;:d0-:
' 32:* #%w-$ @SB Rcd0#%J#%!K8;& "!$C#%8;#2^RwcB0-:V'"#%*,!$3[-${G:|[7!$3%*@C-:3@89!$0-:9":9<a$: :
*,!$C-:(SBQ<a3\32`AB#\-)V0-:/#%!$g#%!E#%K$<
RJcd0!E^#%N`0-:('4 32:* #% -SB
R c B0- AY "#%8;:9N
8;#\C-:9B! G?E!E"#%! 3%:.'O:9"*67!E#2W' 32:* AB#20iC*,!$#%

,!$C
*
(
1
/

Rcd0#\D#% @SB
7V$ @SB R
3
c +0- KSB
@SB N48;4#%C-:9d!'O:9"*k7-!E#2i' 32:* 04!EB#% KSB RD#\C-:9d!$g5
#%!$g#%!E#%Y<>$!m'4"#%*,!$3?E!E"#%! 32:$RxcB0#%k89!$ :l8;#% :g325Q:;p@ :C-:C X!$3%3?!EM#%! 32:k#%
0-:i'4"#%*,!$3D*@C-:3 R 7,0#%,*,:!$,0!E]#2+89!$ :i8;"#% :&325 :;p[ :C:C Y!$3%3d?!E"#\! 32:
#%}0-:1'M#%*,!$3^!$CmC7!$3 *[C:3 N4!E#%<a5@#%-)+!$&5 =8;* #%!E#%i$< d' :9M*67-!E#%i$K8"0!$:3%3%#%-)

@

@^

n

@ r~{Y

s"



-

`@

@B

`@0^

#)

B

e

*
D@ @ @ , @ 83j{:1K{`

@ B

*
! , -

@B

-




#b U



fi 8-
fiffff
8; M!$#%gR/Srd0-:.8"0!$4-:3%3%#%)H8;4 "!$#%&d!E:1"!E#% _4! 3%:$N[0:95f89!$ :.*+!$C-:.SBbR 4#%C-:9
!$&5,#% !$&#%!E#2<>$!6C7!$3-?E!E"#%! 32:$R 5,!6#%*,#%3\!E(!E)7*:&9Ng!EL@#%-)60-:B!E''"$'"#%!E :B#\ !$@G
#%!E#2X<>$j0-:,!$[89#%!E :CZ'"#%*,!$3 ?E!E"#%! 32:$Nff0:k":732#%)]'" 32:* 89!$ :k*+!$C-:,SBbR r:8;:$N
)#2?$: !$&5 #\ !$&#%!E#2 <a$,!X'4"#%*,!$3$,C7!$3?E!E"#%! 32:$N0:i8"0!$:3%3%#%-)x8; "!$#\g89!$ :
*,!$C-:HSBbROcd0!EK#%9N 0-:k' 32:* #% @SB NffcPl0- "#%8;-:"9N 8;4#%C-:9K! `G?E!E"#%! 32:6'O:9"*k7@G
!E#2Q'4 32:* AB#20

W!$C

Rfcd0#%
*
(
1
/
3
4
@SB
7-d#%D-$ jSB R

)

e

-



-





, * . @
, *
tu {#w {}


>1K{`~{`

,

@ @ @ @ 83j{:1K{`

@ @ @ r~{YM{`"~{`&

@

a@

c ,0- @SB
SB N8;#%C-:9D!k<>7-V?!E"#\! 32:b' :9M*67-!E#%]'4 32:* #%fAB0#\8"0
*
1
0 !`?$:0-:

f!$C
4
0
$
!
K



0

:
C


,
*
$
!
%
#





+
R

c
4
0
%
#
K

%
#

@
B


7
K


$

.

B


6
R


$.0-:

/
:9?$:9":$N 8;#\C-:9j! EG?!E"#\! 32:H'O:9"*67!E#2X' 3%:*hAr#20

l!$C
*
(
1
/

RDcd0#%V#%BSB
R'
7-d-$ -SB

83j{`~{`





{

-

*
@ @ ,Y 83j{:1

83j{:1K{`~{`

9

@ @



SbCC#%-)+'"#%*,!$3P$jC7!$3P${G:|&7!$3\b8;4 "!$#%&b l8M0!$-:3\3%#%-)+8;4 "!$#%&rC&:K-$K0-:32'XSB
$ @SBbR4cd0:1<>3%32`Ar#%-):7432B0- ABd04!EB0-:#2b!$CC#2#%iC-[:r-$b0-:32'}04#2)0-:9B32:9?$:3\B$<(32@89!$3
8;#\ :8;5l3%#2L$:. )'4!E0@G 8;#\ :8;5 =SB MR

9
u





sx

JQu9WM[OPb]HtMNu7N*Q*H7JWPYH>Z;v\[O]ff





,* - 7 - ! ,* . Qs e cPj0- SB _ - ! SB _V , * Ng8;#%C:9J *,:d8"0!$:3%3%#%-)r8; "!$#%&w0!E(!E:DSB _VbR
= `A SB - !USB , * NE rA:57 -:9:C1 K0- A2_V - ! _ , * R(#%C:9P!B8;"#% :& '4!$#%^$<-?!$3\7-:9N
!$C ' <>$j!l'4!$#2b$</'M#%*,!$3 ?!E"#\! 32:9N0@B!$C@ER6c !EL$:+!$&5}04#2"Cm'"#%*,!$3P?!EM#%! 32:$N @^&R1Sb
.



_



fi



, * -/, * 7

ff









ff







] ];U

@^ m_

a@0^

0-:i8; "!$#%& :9FA:9: N h!$4C
}#\ VbNA:i89!$ _4C !X?E!$3%7-:l<a$
}8;"#% :&AB#20
0-:b8"0!$:3%3%#%-).8;4 "!$#%&9R V7-/ 0#\/!$3% 6!E#% _:/0-:r${G:|&7!$3\8; "!$#%& :9 A:9:+'4"#%*,!$3
?E!E"#%! 32:R r:8;:$N$0-:/'4 32:* #% V
Rwc b0`A "#%8;-:"9N8;4#%C-:9w! EG?E!E"#%! 3%:/'O:9"*k7-!E#2
' 32:* AB#20

RDcd0#\D#%DSB
7-d-$dSB V R
*
(
1
/

g)

e

_

@ @ @ @
c j0-`A SB _ * -/* 7 SB _ * , ,
,
A:B-:9:CB57 (0`A 04!Eq_ * - 7 _
,

7

, *>1K{`~{`

-

_

,*
N$A:V:89!$3\3&04!EwSB
_



,* - 7

_

,* - 7

-

- Ee

SB V
SB
SB R r:8;:
V R/#%C:9(!j'O:9"*67!E#2' 32:*WR -<>$"89#%-)
0-:}8M0!$-:3\3%#%-)X8; "!$#\g]!$32-:i#%-<>:9" $0 0-:}'"#%*,!$3D!$4C 0-:}C74!$3D-${G:|&74!$3%
8; M!$#%gR B:48;:$N

V R

f
,* - 7 _ qe
fi_ , * - 7 _ , * - 7 _ c .0- KSB
. SB _ * -/* N&8;#%C-:9!9& G?E!E"#%! 3%:'O:9"*k7-!E#2k'" 32:* AB#20@ * @ (
@ 1 @ / >1K{`~{` N!$C @ 3 , YF, @ :M{`"~{`& Rmcd04#%.#%6SB _ , * - , * 7-1-$ KSB RWo-$H0-:
:9?$:9":.C#2:8;#2^N@8;4#%C-:9D!+ G?E!E"#%! 32:K'O:9"*k7-!E#2f' 32:*TAB#20 @ @ @ >1K{`~{` N
_









4oq @ @ 9f



*

(

1



!$Cx0-:H!$CC#2#%!$3 #\!E5i8; !$#%g
'"74-:B0:kC-*+!$#%
*
1 MR /<a$"89#\-) KSB

N^!$C
R -<>$"89#%-)
*
1
( R r`A:9?$:9Nff0-: :HC-*,!$#\d!E:H-$KSB
SB VT#2)0& :0-:i8; "!$#%& :9F
:9:
$
!


C
>
<




*


$

{


G

:
&
|

7
$
!
\
3
H



x$
*
1
*
1
E
R
*
1

@ @ >1K{`

_
@
~{(@ 1

@ r

* . ,
@ @ r~ {YM{`"

_

@

@

!"

_ *-* f
@ , , K1 {(@ Y^

X@ YF@

c m0`A SB
SB N8;#%C:91!
G?E!E"#%! %3 :,'O:9"*67!E#2Z'" 32:* AB#%0
*
(

N&!$C

RcB0#%P#\JSB
7
J



$

J

B


V


R

$





0
V
:


9
:
$
?
9
:
"



:

C
%
#



:
;
8


2
#
ffN

1
/
3
8;#\C-:9J!E)!$#\60: EG?E!E"#%! 32:/'O:9"*k7-!E#2k' 32:* B
#%0

Rcd0#%
*
(
1
/
#%DSB V
7-D-$rSB R'

@ >1K{`
_

,*

-

?c

_ *
_@ @ , @ @ >1K{`~{`

fififfv2$ff6ff )T$
fi
ff
v~ff








w

sx




Qs , x

tw {



cd0-::j:732V:;p@ :C} +*67432#2'432:b'O:9"*k7-!E#2l' 32:*,D7C:9V!+#%*'43%:r: "#%8;#2W04!Ed0-:
' 32:* #%
u v
@ :9)#27
yZ!$3%"0ffN
MRxcd04!E6#%9Nw!$&5Z "#2'43%:$<D?!E"#\! 32:
AB0#\8"0x!E:,!$3%3 G C# :9:&r*k7 K@89897-K $)$:90-:91#%X!Ej32:!$ K-:H'O:9"*k7-!E#2ffR.o$j:;p@!$*'32:$Nff0-:
0-:9:8; "!$#%&P!$3\3 G C#
ff!E:/-$^ M#%!$-)32:
*
(
/ MNE!$3%3 G C4#
*
1
3 MNE!$C.!$3%32G C#
(
1
'::9?[#\-)d!$ * N ( !$C
E
!



:
$
!
%
3
2
3
G

C
#

9
:



:
g


7
P

E
!


V
:

$

J

%
#
j



0
V
:


$
!

*

:
;
8






"

$
!
%
#
&

9Rwcd0-:(<>3%32 AB#%-)

1
0-:9$:*h8;3%32:8;D $)$:90:9b!$Ci)$:-:9"!$3%# 9:B*,!$g5l$<J0-:.'":9?[#274V:732R

lN*PIQ KJ [WPY[OL4[4P rQTJ

- @ ({ @ {(@

M@ :@

sx
u







, * -/, *
! ,* - ,*
! , * - , *

! , * -/, *

! , * -/, *
! , * -/, *
.

fi





fi









! , * -




!





Qs e

















- !

ff


- !







fi





,*

,* .

. .
-







! , * - !

7 ff
,* - 7
7 ,* - 7 -







- @ {(@ {(@









fi

g @ {(@ {(@



,* - 7



,* - 7

7







7 ff





fi

1:07070$

JQul]Ht[vN*QkWv [SW [OPb]ktMNu7N*QVHKJW PdH>Z9v [4]ff





@

?





.
-
- .



,*




,* . ! ,* ! ! ,*






ff









g

0-:V#%-)3%:/'O:9"*k7-!E#2H89!$ :$R ^@89!$3
cd0:'"&$<=w3%#2<> #%k!r "!$#2)0& <a$AV!E"CH*,!$:9J<>*
8;#\ :89#2:]3%#2L$:iSB VbN @SBKN (z !$C
8;#%C:9, "#2'43%:H$<j?!E"#\! 32:9R zF<K0-: :x!E:
3%#%L$:Cl $)$:90-:9N A:k7 :j0-:.<=!$8;B04!EB0-:.'4 32:* #%d "#%!$-)32:1': :9?@#%-)!$C}!,'O:9"*k7-!E#2
#%0:9:9<a$":mC-:9_4:C `?$:9l0-:*WR z <b0: :m!E:x-$+3%#\-L$:C $)$:90-:9NBA:x89!$ C-:8;*'O :m0-:
!E)7*,:g1#\g }SB Z'4!$#2MK$<?E!E"#%! 32:9Ry #20-7-j "#\!$-)32:'::9?!E#%ffN KSB NJ*,!5X3%5
!$8"04#2:9?$:j!$0#%)0+!k32:9?$:34$< 8;#% :48;5,!$SB
R(o$:;p-!$*'43%:$N[8;"#%C-:9!E)!$#\+0-:K-@G M#%!$-)32:
'::9?[#\-),8; "!$#\gr#%W0-:13\!$ d'4!E"!E)$M!E'40ffRzF< *

!$C
(
1
/
3

l0-:Q0-:,'4 32:*
#% jSB N 7-j#2.#\.-$
V
NP!$CZ0:8;:+-:#20:9 (z
N @SB
-$BSB
R '

_

>1K{`~{`

w


_








_

q_

, * M@ @ @ >1K{`
2 _ , *



,*



F

Rfi?7

@ @ @ %Y
9_

#

, * g

,*

Sb-$0-:9} 3\7-#2 "!E :9)$5 #%l :48;[C-:Q'O:9"*k7-!E#2 '4 32:*,W#\g @Sc !$4C 7 :Q! <=!$
K!?@#%{G 7-!$*
l$i32[89!$3j:!E"8"0 '@8;:C7-:$R o-$i:;p-!$*'432:$N : !E}!$C ~m!$g5!
:9'O$'"*,#%#%)r:"732w<>$/'$'O#%#2!$3[:8;@C#%-)w$<O74CH #%H'" 32:*,9N$Ar0#%8"0#\893%7C-:
'O:9"*k7-!E#2i8; "!$#%&9RVyQ:68;#\C-:9B0-:9: 7 C#2:8; ,:8;[C4#%-)d#%& -Sc !$d0: :60!`?$:
MR(Sb1!$32 :9"4!E#2?$:/!$C1'*,#%"#%-)V:8;@C#%-)
:9:17:C.* J8;*,*325j#%.0-:('4!$ >yZ!$3%0ffN
$<d /j#\g @Sc #\K0-: 7-''O$K:8;@C#%-) @R d:8;:&325$N b:g
j0!$."0-`ABX04!E.7#2
'$'!E)!E#26#\j0-:"7-''O$ff:8;@C#%-)r#%^:|[7#2?E!$32:&^ r:<a$"89#\-)B!E"8 G 8;"#% :8;5.#%10-:/$M#2)#%!$3
JNJ!$CY04#%689!$ :]!$8M0#2:9?$:C #% !$ 5@*' $#%89!$3%3%5Q$'#%*,!$3/#%*:$RXcPX8;*'4!E:f0-:l7-''O$
:8;@C#%-).$<D0-:+C# O:9:&1*@C-:3%j$<d!i' :9M*67-!E#%Z' 32:*WN^A:]#\*'4325x-:9:CZ0-:9":9<a$:]
32[$L]!ED7-D":732W!E"8 G 8;"#% :8;5$R(y #20l0-:jC#%:8;V:8;[C4#%-)-N-7#2'$'4!E)!E#%W:-<>$"8;:
!j32:9?$:3-$< 32@89!$3-8;#% :48;5k32:"w0!$,!E"8 G 8;"#% :8;5$Rz{C-:9:CffN$0:d32:9?$:3@$< 8;4#% :8;5H#%w$<> :
#%C-:&#%89!$3O ,04!EB!$8"0#%:9?$:C 5f0:j<a$AV!E"Ci8M0-:8L@#%-)]!$32)$$"#20*iR

x

}_

Sj_

Sj_

*x_

) 5

XN

X



5

N

$K37373>

;P



!

P

VK37373>



,

cVK373$$

fi 8-
fiffff
SM_


|

aB

)


z{+0:rC#2":8;/:8;@C#%-)1$<^!H n#%& -ScjN&A:r04!?$:j! [32:!$f?!E"#\! 32:
rAB0#%8M0+#%
# i0-:r'"#%*+!$3-?!EM#%! 32: ^!EL$:0-:b?!$3%7: -Rz{]0-:B'M#%*,!$3 @Sc *@C-:3 N&0-:9:K!E: x893%!$7 :
1 B893\!$7 :b l:7-:60!Ej-
:7:10!EK:!$8M0Q'"#%*+!$3^?E!E"#%! 32:6!EL$:j!Ej32:!$ K-:k?!$3%7:$N
1 r893%!$7:r l:"7-:k0!Ej-]FAW'4"#%*,!$3P?!E"#\! 32:
'"#\*,!$3^?E!E"#%! 3%:1)$:9jFAW?E!$3%7-:9NO!$C
!EL$:x0-:}!$*:}?!$3%7:$R z{g :9:#%-)325 0-:}8M0!$-:3%3\#%-) @Sc *@C-:3d04!$,0-:}"!$*:}[7* :9,$<
i:9'4: :& $0X0-: 0
&3%:!$Q?!EM#%! 32:K!$j0-:H'M#%*,!$3 @Sc *@C-:3 =!$jA:,89!$Z7 :
?E!$3%7-:x$<10-:x'"#%*+!$3B?E!E"#%! 3%:
lu sx0-: 0 ?E!$3%7-:x<>$f0:XC47!$3B?E!E"#%! 32: MNK!$C 7
1 d893%!$7 :B ]:47-:
!$CC#%#2!$3^893%!$74 :d f:7-":.:!$8"0XC7!$3^?!E"#\! 32:.!EL$:K!,?E!$3%7-:$RdcB0-
:
0!E(-.C47!$3@?!EM#%! 32:)$:9(FA6?!$3%7:!E:D:|&74#2?!$3%:gw .0-:d893\!$7 :0!E:47-:D-jFA.'4"#%*,!$3
?E!E"#%! 32:)$:9V0-:K!$*:b?!$3%7:$RwcB0-:r<>3%32 AB#%-)1:7320- 0!EV~mSB #%#2)0& :904!$
JN-!$C
#\V:|&7#%?!$32:&D ]oJ W0-::1C# O:9:&d*[C:3%9Rz{WAB0!ED<>3%32 AB9N-A:1!$7*,:.0!Ed0-:6o
!$32)$$"#%0*q74 :!6<=!$#%3_M 0-:7"#% #%8V04!E#%4 !$g#\!E :?E!E"#%! 32:(AB#%0+#%)32:B?E!$3%7-:32:9<aV#%+0-:#2
C-*,!$#\ :9<a$:j?E!E"#%! 32:VAB#20W!,8M0-#%8;:.$<w?E!$3%7-: b!E"!$3%#%8"L
/3%3\#2$9N
MR

@B



+Ro

)

-

@B KJ

x_





'z

x 7


fi ff

!

cx_

6?
f

, * -/, * 7

!ff
, * - , *

! $ , * -/, *






fi ff



fi ff



Qs e

x_

1:07.K3>

JXu9WM[4PI]HtMNu7N*QVHKJWPYH>Z9v [4]





9|
5 co

] r



u



( B

+Ro

9*e



sRo



,* 7

,*

, * - 7



7fi
, *
7 $ ,*



- !

7ff

7 $



, *

!fi
, *

! $ ,*


1O

,* !

MK37373>

,*


#%!6 'O:89#%!$389!$ :b$<Pcd0-:9$:*
>yZ!$3%0ffN
MN-AB0#%3\ /~}SB


#%D!,' :89#\!$3ff89!$ :1$<wcd0-:9$:*
[R
c +0-

7-''O :K7#2V'"$'4!E)!E#2i :9B!3%#2 :9"!$3 R(cd0-:9":.!E:j<a7d89!$ :9R/z{
0-:K_"V89!$:$N!893%!$7 :b$< 0:b<>$"*
#+04!$ :9:f:C478;:Cf ,!$W7#29Rcd0!ED#%N&A:
* """
0!`?$:1-:.?E!$3%7-:632:9<aB<a$B!+'"#%*+!$3O?!E"#\! 32:$R/cd0:j<>!$#%3^_" d0-:7"#% #%8K#%}o '4#\8L@d0#%B3%!$ D?E!$3%7-:

X#% !$&#%!E :$RWz{Z0-:f :8;CY89!$:$N(!}893%!$7:+$<D0-:]<a$"*
f<a$
Z04!$ :9:
:C748;:CH 1!$,7#%9RJcd04#%J:7-:w04!E/-j'"#\*,!$3[?!E"#\! 32:V)$:9(FA.?E!$3%7-:RwcB0-:Do !$32)$$M#20*
"#2?@#%!$3%3%5.-:9?$:9 "#2:wFA1"#%*67432!$-:97J?E!$3%7-:w<>$(!K'"#%*,!$3&?E!E"#%! 32:$RJz{60:D0#2"Ck89!$ :$N&!j893\!$7 :
$<w0-:.<>$"
*

W04!$ :9:W:C78;:C} +!$}74#29R/cd04#%V:7-":D0!EB-]C7!$3
<>$
?E!E"#%! 32:])$:9k Ax?E!$3%7-:RxSr)!$#%ffNw0-:lo q!$32)$$"#204* "#%?[#%!$3\325x:9?$:9k M#2:1FAQ#\*673%!$-:97
# 04!$ :9:l:C78;:Cl +!$}7#29R
?E!$3%7-:V<>$B!,C7!$3 ?E!E"#%! 3%:$Rz l0-:j<>7-0W89!$:$N
* """
cd0!E6#%9NPA:+0!`?$:]:,?!$3\7-:+32:9<>j<a$k!}C74!$3w?!E"#\! 32:$R]ST<=!$#%3_M .0-:7-"#\ #%8H#%Zo '4#%8"L[.0#%
3%!$ ?E!$3%7-:l Z#%4 !$g#\!E :$R r:8;:$NV)#2?$: !Z"7#2! 32: "!$48"0#%)x0-:7"#% #%8EN/0-:io !$32)$$M#20*
"!$8"L[b0-:
!$32)$$M#20*WRdc ]0`A 0-:1:9?$:9M :$Nff7-''O :j<>$AV!E"Cm8M0-:8L@#%-)f:* ?$:K!,?E!$3%7-:$R
cd0-:9":b!E:KFA,89!$ :R(z f0-:K_M 89!$:$N-0-:K?!$3%7: (#%:* ?$:CW<>* !kC47!$34?E!E"#%! 32: KC7-:b
*:.8M0!$-:3\3%#%-)k8; "!$#%&9R/cd04#%V*:!$V0!ED0-:9":.#%D!k'"#%*,!$3 ?E!E"#%! 3%: .AB0#\8"0l0!$ :9:
:9K W *:k?!$3\7-: R
4#2B'"$'4!E)!E#2x

:9 l<=!$3% :$Nff!$Cm0-:x
:9 H<>!$3\ :b!$:|[7#2":CffR z{]0-:b:8;Cl89!$ :$N-0-:b?!$3%7: #%:*`?$:Cl<>* !
C7!$3P?!EM#%! 32: $NO!E)!$#%XC7-:6 W!]8M0!$-:3%3\#%-)+8; "!$#%&9Rbcd0:1'[$<(#%b-`A C74!$3^ l0-:k_"
89!$ :$R
c Z0- ~}SB
N(A:i74 :l0-:l<>!$8;,04!E~mSB C-*+#%!E :o !$4C oJ
R
c +0- "#\8;-:9N48;4#%C-:9d! G?!E"#\! 32:j'O:9"*k7-!E#2W'" 32:* AB#20}!$CC#2#24!$3 #%!E5f8;@G
"!$#\gl0!E}"732:x7-W0-:Z!$*,:Q?!$3%7:x<>$}!$3%3
'4"#%*,!$3r?!EM#%! 32:9R /<a$"89#\-) SB 0-:

ax_ - 7

-

^1:"

aB

aB

B

'aB

+^

Sz G



+^

]



e

mxB_

B

B ^ | G

z

B

+^

|

- ! x_ -

c

+^

+^

%@0^

]

z

- 7 x_ -

9

!

f

fififfv2$ff6ff )T$
fi
ff
v~ff



8"04!$-:3%3%#\-),8; "!$#\gb89!$7 :K!fC-*,!$#%}AB#2'O:97-dm0-:6C47!$3^?E!E"#%! 3%:1!$ @89#%!E :CxAB#20}0#%
?E!$3%7-:$R(Sbd0-:9:.!E":1-,7#%V893\!$7 :9N
C-&:B-$B#%*,*,:C#%!E :325+32?$:.0-:.'4 32:*WR

x_ - ! x_

x_

,*

+

c ]0`A
NA:6$ :.0!EB0:18"0!$4-:3%3%#%) -Sc *@C-:3^8;&!$#%r*$:1893%!$74 :9R
r:8;:$N(#26C*,#%!E :10-:]'4"#%*,!$3 -Scq*@C-:3 Ric m"0-`A "#%8;:9N8;#%C:96!m<a7-6?!EM#%! 32:
'O:9"*k7-!E#2m' 32:*hAB#20m0-:9:H!$CC#%#2!$3 #%!E5W8; M!$#%gK04!Eb#2< *
k0-:
[N
(
f
$
!


C
f
E
!



:
$
!
%
3
3
"


7
%
3

:

C


7
9

6
R




4



%
#
C
9
:

"

$
!



8

0
%
#

]
)



E
R


%
#
r


'
"

$

4
'
E
!

)
E
!


2
#


x





1
/
*
$0}*@C-:3%B :9
* ( N
(( N
1( N
/5( N
(!* N
1!* !$C
/* +<=!$3% :$R Ki0:68"04!$-:3%3%#\-) @Sc
*@C-:3 N@7#2'$'4!E)!E#%f!E)!$#\ 0-:j893%!$7 :
* (
((
1(
/5( 0-:f)$:-:9M!E :D!$l:*' 5
893%!$7:$R 5f8;*'!E"#% ffN74#2V'$'4!E)!E#%Wi0-:j'4"#%*,!$3 @Sc *,[C-:3ffC&:r-,*$:jA$L R'

e

@


@



n











q)

0Q_$e = Qg


-=







@ 1

@ 1



v





@




cd0-:1':9?@#27D:"732D :3%3 7d-$04#%-)+! 7-d0:.:3%!E#2?$:k8; B$<(!$8M0#2:9?@#%-),0: :632@89!$3^8;"#%{G
/
:89#2:RmSb 5[*,' $#%8+!$4!$325["#%1!$CC46C-:9!$#\3( x0-:]":7329RWyQ:W89!$ !$8"0#%:9?$: jSB
#%
#%*: :9 )#%^N
MRnSB #%!E"5Z8; "!$#%&H89!$ :]!$8"04#2:9?$:C #% ( .AB0-:9: l#%k0-:
( 8"04!$-:3%3%#\-)Q8;@G
[7* :9,$<.8;4 "!$#%&+!$C
#%,0-:#%+C-*,!$#% # 9:$R Sb,0-:9:x!E:
/
"!$#\g9N(SB
!$#%?$:325Z!EL$:
.#%*:$R r`A:9?$:9N 5Z!EL@#%-)X!$C-?E!$g!E)$:}$<d0-:l<>748;#2!$3
1 j7#\-)W0-:]SBG }!$32)$$M#20*
!E7-":,$<d8"0!$4-:3%3%#%)i8; "!$#%&9NJA:f89!$Y":C78;:,0#\.
/ J#%*:d!$/0-:9:B!E:
(
r:g :-"5[8"LN b:9?[#\3%32:$N
c :-)-N
MRSB
!$3% .4!$#2?$:325k!EL$:
#%4!E5}-${G:|[7!$3%.8;4 "!$#%&9R B A:9?$:9`N A:,89!$Q!EL$:+!$C-?E!$&!E)$:+$<0-:, 'O:89#%!$3w!E7-:$<V!
( !$V:!$8M0}-${G:|[7!$3%D8; "!$#%&V:9:CV
#%4!E5,-${G:|[7!$3%d8; "!$#%& ":C78;:K0#%
7r8;:$R6yQ:,0!?$:'4`?$:CX0!E jSB
SB
SB
!$CX)$:!E :9.'4"7#%)
:k*+!$C-:SB
/ MN
1 MN
( : 'O:8;#2?$:3%5 MRcd0[7
'O`A:9/#%w: 4:8; :C+#%0#%)0-:9wA$"/89!$ :B8;*'32:;p@#% 5
A:6 #%3\3^-:9:Cm +"7}:;p['O:9"#\*:gB ] :9:H#2<w0-:k!$CC#2#24!$3ff'"74#%-)H7- A:#2)0r0-:6' $ :&#%!$3%3%5
0#2)0:9d8; 9R



6

1:0706

]

-

*e

e

1:0707$
/e

,*

?

Mx

=S

sRo

OLP Q R$g = R

w




sRo
sRo

+Ro

*

5

? =0QTg



+Ro

+ ]
sRo

- !

!

"

*

+Ro sRo +Ro ,

+Ro





yQ:f"!$Y!WAr#%C-:?E!E"#2:9 5Q$<D:;p@'O:9"#%*:&j m:;p['43%$:,0-:f#2)#2_89!$8;:$<D0-: :]0-:9$:9#%89!$3!$C
!$ 5@*' $#%8mC# O:9:8;:9R o-$l:;p@!$*'32:$ND:9?$: 0-7-)0 #%!E5 -${G:|[7!$3%f8; "!$#\glC- 32:
'"74#%-) 0!$ 0:Y8"0!$4-:3%3%#%) 8;4 "!$#%&9N.0-:95 *,#%)0g}#%3%3j 'O:9:C 7-' :!E"8"0 5 '4"7#%)
|[7#%8L$:9`RYyQ:}3%#\*,#210:W_" H :9$<B:;p@'O:9"#%*:&H Z!X !E#%8l?E!E"#%! 3%:f!$C ?E!$3%7-:l$"C:9"#%-)X!$
A:BAB#%"0H 68;-_M* 0-:d0-:9$":9#%89!$3:"7329Ng!$4C,0-: :r!E:r3%#\*,#2 :Ck:#20-:9 6 !E#%8d$"C-:9"#%)$
6!1: "#\8; :C]893%!$"($<^C-5@!$*,#%8V?E!E"#%! 32:B!$C+?!$3%7:d$"C-:9"#\-)(#%Ar0#%8"0+A:b*,!EL$: {:|&7#%?!$32:&
MR
"!$48"0#%)HC:89#%#2D#\l0-:.C# O:9:&d :!E"8"0} :9: V!$898M0&7r:9B!$3 R2N





*)

N

K373$$

P

Sbb:;p['43\!$#%-:C :9<>$:$N *+!$g5}8;4 "!$#%&B [32L@#2r"7-''O$B8M0!$-:3%3\#%-)AB#20m:9e+89#2:&b)32 !$3
8; M!$#%gRJo-$J:;p-!$*'432:$Nz
@32?$:9J04!$^0-: ff
fi ffD8; "!$#%&9NE!$4C.0-: @#%89 7^_4#2 :
C-*,!$#\m8; "!$#%&r3%# "!E5W0!$r0-: ffff ]'4:C#%89!E :$R $0m'O:9<>$"*h!]32:9?$:3J$<('4"7#%)
AB0#\8"0 !E''O:!E"+ :i:|[7#2?E!$32:&, Y:-<>$"89#%)YSB 0-:}:;p@'43%#\89#28"0!$:3%3%#%-)Z8;4 "!$#%&9R
yQ:j0-:9:9<>$:.8;*'!E:Cl0#%V#%]7-D:;p@' :9M#%*:& SB l0: #%!E5,-${G:|[7!$3%D8;4 "!$#%&
!$C KSB x0-:!$3%3 G C4# :9":gb8; "!$#%&9R1Sb3%3 0-:k*,[C-:3\r!E:#%*,'432:*:& :Cm#\ @32?$:9 [R @N
!$C !E:X!?E!$#%3%! 3%:m?@#%!
# R yQ:X3%:;p@#%8;$)$M!E'40#%89!$3%3%5 $MC-:9l0-:X?!E"#\! 32:+!$4C !$"#2) 0-:
?E!$3%7-:1#\Z&74*:9"#%89!$3$MC-:9RlyQ:]0:9:9<a$":+325 M!$8"0YZ'"#\*,!$3?E!E"#%! 32:RfSb.A:+ :9?$:
?$:95 "#%*,#%3%!E":732+ !Y"!$-)$:x$<j'O:9"*67!E#2 '4 32:*,NA:}4325 0- 0-:9":}:7432,<>$
P!$)$<a$"C D' 3%:*WR

Ev2





)




Sj_q





4

9" kK373

fi 8-
fiffff

*@C-:3




























E< =

cJ! 32:

0-:7-"#%#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8





<=!$#%3%

nV[Nk0$

<>!$#\3%

3@Ri373$

>
.7
>
>





7"











3@Ri373$"




3@Ri373$
3@Ri373$"
3@Ri373$"
3@Ri373>,
3@Ri373$






nV[N\143>

:8ER

:8ER



eW
3@Ri373$"
3@Ri3M171
3@Ri3M1:
3@Ri3M171
3@Ri373$0
3@Ri373$0
3@Ri3M1:"
3@Ri3M171
3@Ri3M1:
3@Ri373$0



>





7* :9V$< !$8L[ "!$8L@ ><=!$#%3% /!$Cf"74#%-).#\*:r _C+0:r_M 3%7-#2f H A,#%@G
!$48;: $< !$-)$<>$"C ' 32:*iR B7&#%*: !E:<a$ z
@32?$:9 [R bk!
~
$N
J:&#%7* zz"z/'@8;: $`N!$C
+~
$< BSK~ZR



_

*[C:3





0-:7-"#%#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
!E#%8
























T< =

cJ! 32:



V[Nk0$



ff




ff

<=!$#%3%

77.7





<=!$#%3%

ff

ff

ff



3@Rk7.
3@Rk"70
3@R 1
3@R 1
3@Rk7
3@Rk7
3@Rk"7
3@R >
3@Rk&7&
3@Rk70



V[N\1718

:8ER

<>!$#\3%

1ER >"
[R\1:"
[Rk7&
[Rk7"
1 Rw,K
E
1 Rw,K&
E
[Rkj1
Rk7.
[
R >0
[
Ri3$0
[

7" &776
1:&$,K&76
"7&776
"7&776

2

e

ff

Ev2#

F

nV[N\143>

:8ER

77.7
&K3$&7
77.7
77.7



X"j1: !)

71 1:77&
70K3M1:.
171:77&
171:77&







171:77&












ff

:8ER



, R 1
&
K3@Rk"70
17E1 Rk0j1
17E1 Rk06
. Rk&j1
[
. Rw,7,
[
1:[. Rkj1
1:[ Rk7
1:[ Rk07
14@3 Rk"7&




"7&776





1:K373 !eK

E" kK373

nV[N\1:$

ff
<>!$#%3\

j1:7077&
06>07.$,K.
j1:7077&
j1:7077&

ff
ff

j1:7077&

ff
ff
ff

:8ER



ff

1ERk.70
1:j1ERi3K
,K[ Rk.7"
,K[ Rk
"K@3 Rk"7
"jE1 R 1
171O- R 7
,K[& Rw,7,
,K[. Rk07"
&7[ R >0

7 * :9D$< !$8"L& M!$8L@ ><=!$#%3% !$Ci"7#\-)1#%*,:j ,_4CW!$3%3ff3%7-#2N[$B'`?$:10!E

0-:9":6!E:k-+ 3%7#29N ]<a7r#%!$8;:d$< !$-)$<a$MC d'" 32:*WR B7&#%*:B!E:1<>$
z
[32?$:9 [R ]
~
$N J:g#%74* zzz'@8;: $N!$C
,~
$< rSb~YR

v2

-

" kK373 1:K373 !eK @_

)





!"j1: X)



fififfv2$ff6ff )T$
fi
ff
v~ff





z{ cJ! 32: [NdA:X8;*'4!E":m0-:m?!EM#27+*,[C-:3\,$<.!n' :9M*67-!E#% AB0-: _4C#\-)Q0-:x_"
3%7#2Q iFAx#% !$8;:1$< P!$-)$<>$"C 1' 32:*iRz nc ! 3%: -N A:]8;*'!E:,0-:+"!$*:+*@C@G
:3%jAr0-:X_4C#\-)f!$3\3( 3%7-#%K$.' ?[#\-)W0!E.0-:9":,!E:+-i 3\7-#29NP<a$.<>7-1#\ !$8;:j$<
P!$)$<a$"C d' 32:*WR K325
[N r!$C
[N r#%}0#%D! 3%:60!`?$:k!$&5i 3%7-#%9RDcd0-:.:;p@'O:9 G
#%*:&!$3:73268;-_"* 7-k0-:9$:9#%89!$3_4C4#%-)9RWow#2" 9NJ:-<>$"89#%-) KSB n!$ !$3%3 G C4# :9":g
8; M!$#%gC-[:/0-:K* '"74#%-)-N$AB04#%3% :-<>$"89#%)6SB ]0: #%4!E5-${G:|&74!$3%8;4 "!$#%&
C-[:.0-:]32:!$ 9Nw!$CQ:-<>$"89#%-)iSB Z0-:]8"0!$:3%3%#%-)W8; "!$#%&1#%.#\ :9 A:9:ffR r7&#%*:
!E:,#\*,#%3%!E"3%5f$MC-:9:CffR @:8;CffNP!$CC4#%-)+0:'"#%*,!$3 $.C7!$3 #\!E5}-${G:|[7!$3%j8;4 "!$#%&K
0-:.8M0!$-:3%3\#%-)68;4 "!$#%&DC-[:D-$ "#%)k!$&5f*,$:K'"7#\-)-N&!$CW*,:9:325]!$C4CV`?$:9"0:!$CW
0-:k"7&#%*:$Rbcd0#2"C^N4!$CC#%)+:;p@ "!]8;4 "!$#%&b f0:6'M#%*,!$3^$jC74!$3P!$3%3 G C4# :9":gb8; "!$#%&
!$8"04#2:9?$:j0-:,!$*,:H!$*,7gj$<'"74#%-)+!$j0:H!$3\3 G C# O:9:&b8; "!$#%&bQ#%K`ABffN^!$4CQ!E)!$#%
7 d!$CCV ?$:9"0-:!$Cm ,0-:.M7g#\*:$R







v



nV k0$

/

!nV \143>

5











5



w


g?R

@=

7



? R



R

g $g



cd0-:6:;p['O:9"#\*:g!$3P:73%B#%}0-:63%!$r :8;#2X*,#%)0gb :9:* l0!`?$:H:9 32:C}0:6*,!E :9j$<(0-
+*@C-:3O'O:9"*k7-!E#2l'" 32:*,9R -<a$M89#%-) KSB }!,#%-)3%:K!$3\3 G C# O:9:&V8;4 "!$#%&d!$32AV!5@
)!?$:]0-:+*+!$3%32: j :!E"8M0Q :9:6!$CX"7&#%*:9R r`A:9?$:9NJ0#%j#2)-$":.!W#2)#%_489!$gK'O$ :&#%!$3
!$C-?E!$g!E)$:b$<ff8"0!$4-:3%3%#%)K#\g 1!1C7!$3*@C-:3 R b5[4!$*,#%8D?E!E"#%! 3%:V!$4C,?E!$3%7-:D$"C-:9M#%-).0-:7-M#% #%89
*,!`5 :f! 3%:+ x:;p['43%#210-:f'"#\*,!$3/!$C C7!$3?@#2:9Ad'O#%&j$<B!x'O:9"*67!E#2Y X*,!EL$: :9 :9
C-:89#%"#29R cd0#\,#%+$]!Z $'#%8i0!El89!$ :}:!$#%325 !$CC-": :C 0:9$:9#%89!$3%325$R B A:9?$:9Nr0-:
:;p@' :9M#%*:&!$3P:"732r)#2?$:X#%}04#%r:8;#2Q0- 04!Eb?E!E"#%! 32:6!$Cm?E!$3%7-:k$"C-:9"#%)]0-:7-M#% #%89
89!$i'4$_V)$:!E325l<>* *k732#2'43%:r?@#2:9Ad'O#%&9R
?E!E"#%! 32:j$MC-:9"#%-),0:7-"#% #\8b3\#2L$:.*,!$3%3%: BC-*,!$#\i#%d7"7!$3%325 7 #%_:Ci#%l :9"*+d$<(!<=!$#%3 G
_"'"#%89#%'432: ffA:B0!?$:b .'4#\8Lk:9?$:g7!$3\325!$3%3-0-:d?E!E"#%! 32:9Ng 6#2(#%AB#\ :D 18"0& :r-:d0!E#%
0!E"C] !$#2)^N&)#2?@#%-)H70-$'O:9<>743%325H*678M0W8; "!$#\g'$'4!E)!E#%f!$4Cf!k*,!$3\3 :!EM8"0l :9:$R/S
?E!$3%7-:1$MC-:9"#%-)]0-:7-"#%#%8j3%#2L$:1*+!p@#%*k7* '*,#%: b:9:32:ffN
r#%r77!$3%3%5 7 #2_4:Ci#%i :9M*,
$< !"7898;:9:C@G_"'4"#%89#2'32: ^A:j'4#%8L]!H?!$3%7:b3%#%L$:325, 32:!$Cl ,!H 3%7-#%ffN[:C789#%)10-:K"#%L
$< !$8"L& M!$8L@#%-)!$C] 5[#\-)1-:b$<^0-:K!$32 :9"4!E#2?$:K?!$3%7:9Rwz f!1'O:9"*k7-!E#2]' 32:*WNgA:j89!$
"!$48"0QQ0:,'"#%*+!$3w$.0-:+C74!$3w?!E"#\! 32:K$1 $0^R,yQ:]04!$3%30- 0-:9:0!E6<>!$#\3 G_"
Q-:?@#2:9Ad'O#%&K#%j8;*'4!E# 32:AB#20X7898;:9:C-G_" jQ0:,C7!$3 Rc iC-i -N^A:+8;#\C-:9j0-:
<>3%32`Ar#%-)0-:7-"#\ #%899R



gf

Ex



/e

e

5

<

l

<




g1:0707$

95

ff
fi
<

d8M0-&:k0:6'M#%*,!$3ff$K0-:HC7!$3 ?!EM#%! 32:.Ar#20}0:k*+!$3%32:

xtvw{
C*,!$#%ffN!$C}8"0-[ :.0-:j?E!$3%7-:B#%W&74*:9"#%8b$"C-:9`R

axt2'




Qsffw'xt2x


<

w8"0-[ :d0-:D'4"#%*,!$3&?!EM#%! 32:VAB#20H0:d*,!$3%32:wC*,!$#%ffN
t2'
x t2w'{
!$4Ci8"0-[ :.0:.?!$3\7-:d#%W[7*:9"#\8r$MC-:9R












ff

<

8M0-&:d0-:BC74!$3@?!E"#\! 32:DAB#20H0:B*,!$3%3%: (C-*,!$#%^Ng!$C

x t2w'{
8M0-[ :10-:j?E!$3%7-:B#%W&74*:9"#%8b$"C-:9`R



t2)xtv






x



t2'


( fi
<8"0-[ :B0:r'M#%*,!$3C74!$3@?!E"#\!

32:DAB#20+0-:r"*,!$3%3 G
:(C-*,!$#%ffN$!$C8M0-[ :D0-:V?!$3\7-:Ar0- :DC7!$3 '4"#%*,!$3g?!E"#\! 32:V0!$w0-:D*,!$3%3%: JC*,!$#%ffR

,

Qsffw'xt2


x

t2'



t2w'{





( <D8M0-&:60-:6'"#%*+!$3O?!E"#\!

32:.AB#%0W0-:6"*,!$3%3 G
:BC-*,!$#%^N!$CW8"0-[ :60-:j?!$3\7-:jAB0- :.C47!$3O?!EM#%! 32:.0!$d0-:.*,!$3%3%: dC-*,!$#%^R


l

x

x

t2w'{

:

fi 8-
fiffff







$


t2




ff
.
<

(
8"0-[ :Q0-:QC7!$3r?E!E"#%! 32:xAB#20 0-:Q*+!$3%32:
x t2'
,
xt2w'{ )
C*,!$#%ffN!$C}8"0-[ :.0-:j?E!$3%7-:jAr0- :.'M#%*,!$3 ?!EM#%! 32:j0!$D0:.*,!$3%32:dC-*,!$#%ffR

c 0:k*+!$3%32: bC-*,!$#%x0-:7-"#\ #%8.x0-:HC7!$3J0!$ :9:X7 :Cx!$j!+?E!$3%7-:k$"C-:9"#\-)]0-:7"#% #%8

#%f!6[7* :9$< :;p['O:9"#%*,:g!$3 7C#%: $7"C!$ffN
V0-:)6:9D!$3 R2N
@*,#%0ffN
MRcd0-:
<>3%32`Ar#%-)]!E")7*:&b0`ABb0!EK0-:HC-7 3%:6*,!$3\32: rC*,!$#%x0-:7-"#%#%89B!E:8;*'!E# 32:6AB#20
0-:b<>!$#\34_M /'M#%89#2'43%:<>$V?!EM#%! 32:r$"C-:9"#%)1!$Cf7898;:9:Cl_"/<>$?E!$3%7-:b$"C-:9"#\-)-R -7-''O :dA:
!$#%)W0-:.'M#%*,!$3 ?!$3\7-: , +0-:.'"#\*,!$3 ?!E"#\! 32:
=!$}!$4!$32$)$7B!E")7*:&B89!$ :K)#2?$:}#%<
A: "!$48"0mm!]C7!$3ff?E!E"#%! 3%: MRr "!$#\gd'4$'4!E)!E#2mAB#%3%3O'4"7-:.0:1'"#\*,!$3ff?E!$3%7-: ]<>*
0-:k$0-:9K'"#%*+!$3ff?!EM#%! 32:9N !$Cx0-:6C47!$3P?E!$3%7-: V<a*h0-:k$0-:9jC7!$3P?!E"#\! 32:9Rj "!$#\g
'$'!E)!E#2 *,!`5 CY*$:W0!$ 0#%,#2<rA:m0!?$:x!$ !$3%3 G C# :9:&8; "!$#\g,$]8"0!$:3%3%#%-)
8; M!$#%gR r`A:9?$:9NV Z!m_4" !E''`p@#\*,!E#2ffN0#\k#\!m":!$ ! 32:W !E"#%-)X'O#%g9Rncd0-:
7898;:9:C _M ,?E!$3%7-:}$MC-:9"#%-)n0-:7-"#%#%8i8;*'47 :+0-: {'*+#% : Z$<j0:xC# :9:&,?E!$3%7-: 5
*k732#2'43%5[#%)6 $)$:90-:9j0:6C-*,!$#\}# 9:r$<0-:k7#%!$g#%!E :Ci?!E"#\! 32: K:9:32:ffN
MRbSbg5
:9"* #%i04#%d'"[C748;B#%B748"0!$)$:Cm#2< f$ NOC-:9'O:C#%-)mAB0-:90:9B0#%r#%b!,'"#\*,!$3ff$KC7!$3
?E!E"#%! 32:$N-C-[:d-$D@89897-B#\]0:.C-*,!$#%W!$C}#%V:C78;:C 5 j#2< ,$ (@89897-"9R/cB0-:K'@C78;D#%
3%#2L$:3%51 :B*+!p@#%*+# 9:C 5H:"7-"#%-)KA:d:C748;:B!$(<>:9A :9"*+/!$('O"# 32:$RJcd0!E/#%N 5H:"7-"#%-)
l!$C V[89897b#%x!$b<a:9A C-*+!$#%B!$b'O# 32:$RBcd04!Eb#% H!$C
0!?$:H0-:H*,!$3%3%: rC-*+!$#%
'O# 32:$R r:8;:C-7 3%:6*,!$3\32: rC*,!$#%xAB#%3%3 "!$8M0mx0-:H?E!E"#%! 3%:1AB#20x*,!$3\32: rC*,!$#%
!$CW :4CW +!$#%)i#2D0-:j?E!$3%7-:jAB#%0W* d'"*,#% :$R
yQ:K-`A 8;*'4!E":r0-::b0-:7"#% #%89/#%,!$]:;p[ :"#2?$:b:9/$<^:;p@'O:9"#%*:&9Rwcd0-:r0&5[' $0:#%A:
AB#%"0m } : 1#%K0!E "!$8M0#%-)W0-:7-M#% #%89K89!$Q'"$_K<a* *673%#2'432:k?[#%:9Ad'O#%gR6yQ:+74 :0-:
<>3%32`Ar#%-)8;3%32:8;#2i$< 'O:9"*k7-!E#2W'4 32:*,D#\W!$CC#2#2f
!$-)$<>$"C D'" 32:*

@1:0707">



|

@1:07070>

g

@B

r

|

z



e



|

z




N

P

| Bz

'1



%]

e

K
+' _'

~K37373>



E1:0707$

| Sz

@B









<



K'


A'

B


w {}
l x Sb.$"C:9 T|&7!$"#2)$7-'1#\P! P!E#%6|&74!E:/$<# 9: QN0!E
#\9Ng!$
*k732#%'43%#%89!E#2k! 3%:B#%AB04#%8"0H:!$8M0]:3%:*:g/[89897-M/#%:9?$:9"5`A !$C:9?$:95
8;3\7*,ffRj7!$#2)$7'm:;p-#% :8;:'4 32:*,bC-:9 :9"*+#%-:k0-::;p@#\ :8;:H$.-@G:;p-#% :8;:$<
|[7!$#%)$7-'4V$<!H)#2?$:m# 9:jAB#20W!$C4C#2#2!$3 '"$' :9"#2:

t2w 2











'! <C-:$ :B|[7!$#2)$7'4$<J$MC-:9 '
Bfi !
' < C-:$ :B|[7!$#2)$7'4$<J$MC-:9 '

=<

)ff
4 )B
A)fijAY 4R
<>$dAB0#%8M0
*)jM)B )
OAY 4R
<>$dAB0#%8M0

`)Y}<>$.:9?$:95

yQ:f!$CC#2#%!$3%325iC-:*+!$CQ0!E.0:+|&74!$#2)$7-'Q#%1#%C-:*'O$ :&9NP# R :$R
:3%:*:g
4R/cB0-:j' 3%:*T#\

#%} P# R





x :
b32*



Sj_n

go

"732:9r8;#% r$< n*+!EL[b!E"!$)$:Cm!$32)]!,M732:9B$<
3%:-)$0
78"0+0!E/0:rC#%!$8;: :9 A:9:f!$g5'4!$#%($<^*,!EL@(<>$"* !1'O:9"*67!E#2ffRwcd0-:
'4 32:* #%
!E
P# RHz{Q7-.:;p@'O:9"#%*:&jA:+ 'O:89#2<>5m0:,L[`ABQ$'4#%*,!$3

3%:-)$0i!$CW_CW!$3%3O$'#%*+!$3ff 3%7-#249R

x









+'

l

_



ff










Sj_n



'w'{
l x cd0-:b' 32:*q8;4#% V$<P"8"0-:C743%#%-)6)!$*: :9 A:9: :!$*+
?$:9
jA:9:9L@dAr0-: Z#%D:9?$: QA:9:9L@dAB0: Z#%V@CC MR /!$8M0iA:9:9Li#%DC#2?@#%C-:Cl#%&
i'O:9"#2@CKAB0-:
#%.:9?$:
iAB0:
#%1[CC MR !$8"0n)!$*:]#%68;*'O :CZ$<
FA]32$9N $0-*: !$4C $!`A!`5 @N AB0-:9":r-:K :!$* '3%!5@D0-*:j!$Cf0-:K$0-:9d :!$* '3%!5@
!`A!`5$R]cd0:k {:8;#2?$:]#%K }8M0-:C732:!l)!$*:+<a$.:!$8M0Z'O:9"#2@Cx$<:9?$:9"5xA:9:9LX78"0Z0!E
:9?$:9"5x :!$* '43%!`5[j!E)!$#%4 .:9?$:95X$0-:9. :!$* J!W :!$* '43%!`5[K:;p-!$8;325X8;:+!lA:9:9LXAB0-:
A:]04!?$:]!$n:9?$: &7* :9j$<D :!$*,9N!$CZ!Ek* 68;:]!WA:9:9LQAB0-:YA:]0!?$:l!$Z@CC

sD}ffu

K

o6J 1

<P

5

P





P

2Ro

P

YRo5J 18 K

>



?b



-f
cf

=<

fififfv2$ff6ff )T$
fi
ff
v~ff



?>

ff

[7* :9D$<JA:9:9L@ 4!$Ci!, :!$* 4
' 3%!`5[B!EB* dFAB#%8;:1#\l0-:1!$*:j'O:9"#%[Cf`?$:9b0-:18;7" :
$<w0-:.:!$ ffRcB0-:K' 32:* #\
#\}
#I R










Sj_q



'o

t2s l x SbZ$"C-:9
*,!E)#%8,"|&7!E":,#%.!$
*,!E " #2pX8;&!$#%#%)i0-:
5
( NAB#%0}0-:H7*h$<(:!$8M0X`A.Nff8;3%74*,ffNO!$CmC4#%!E)$!$3 :#\-)+:|[7!$3 Rbcd0-:
7* :9" 6
[
'4 23 :* #%
#%}
#I R


v
w}


e

)t2{

fi

1














cJ! 32:

! !

!I



7-


)


8 )
8

)


8 )
8




j_q


Qs , x



'
''

'
''

;



-

b"!#c! b
! c =6U
! ! 8b r



!ff!!)6
!b"ff
!!)



b8r

! !

! crrr

!
6!!! %
! c =U6
!!! c! 66rrU!
)

)

) )
)
)

"b8 U

!I



!c !
8bVU
! %b )
! !
U
! "b !
! %)
) 8
U
) )
! U !
) 8
U
!



)

7;D



: !
!!b U6

!!b )
!!c :

-



!I

!bff


7;D

!

"b88 ! ! ! ! 8c
)"6
c 8! U 4 b
!I ) )6
) U!U
)! !:r"!
!) 8U 6 ! )"b U
U c ) !I ) )6 8 !
! c
)"c"r
b %)"b : r
!!c b!
!: ) ?c )6 rb
! ! )
#$)$ )r ff!
! ! b! )"6c 8! U !!c ! )6
!I U )
!: ) VU#c b#c !b
! ! !
!I"c#r
b ! ) U ff!
! ! "b U U#c "U ! !!rc c U
!I :
! !!I"c VU r

"c#cr#c#c
Ur# c ?b
)K !!
U ) VUr
U U6 )
)K !!
Ur" !8
!Ur# !b%# )$c ?!I($b
# b c!
) ) !r
)
U!b !I
8% )68

8

"E< =

!




!D



-

:!b"! b#c
! c !r:%)

7;

!b"c c"c
! !
)
! ) !

#r" $U &b%U "

%).b86ff!
! ! ) )!!b8
).b86ff !
%
! c b U"!
!*!!cc ? "$br!r($r:+% ! )!
!!c"b U !) U
" !?b"cr
! cr ff!bV U
!!c"c U !8 c

!ff!! !%)



!"!ff! c!
!b#c c#c
U8 ff! !
! !I ff!

8! ! #c
8 !)
8! "! ) !
!8r b
%)"c c%)
! 4 "!

7* :9D$< !$8"L& M!$8L@ ><=!$#%3% !$Ci"7#\-)1#%*,:j ,_4CW!$3%3ff3%7-#2N[$B'`?$:10!E
0-:9":+!E:+-m 3%7-#249N^ }<>7-1#\ !$8;:j$<
!$-)$<a$MCZ' 32:*WR B74g#%*,:.!E:,<>$
z
[32?$:9 [R ]
~
$N J:g#%74* zzz'@8;: $N!$C
,~
$< rSb~YR

v2





" kK373 1:K373 !eK @_
"
N YHP>

!"j1: X)

Xf



cd0:+:74321!E:])#2?$: #%YcJ! 32: [RiyQ:W*,!EL$:l!}[7* :96$<d :9?E!E#2R /-<>$"89#%)}SB
x0-:H'"#%*+!$3P-${G:|[7!$3%j*@C-:3

r)#2?$:K0-:kA$" K:73% =!$j#2KC-[:j#%x!$3%* K!$3%3J0-:
7 :|&7:gd'4 32:* C-*+!$#% MRyQ:1AB#\3%3O-$B0-:9":9<a$:kC#%8974D#2d<=7-0-:9`Rcd0-: : B"7&#%*:
!E:l !$#%:CnAr#20 0-:
*@C-:3 N(0:7-"#% #\8
>' KC MN# R :$Rw<a* :-<>$"89#%)x!x'O:9"*k7-!E#2 5
0-:.8M0!$-:3%3\#%-)8; "!$#\gd!$3%-:.!$Ci8M0-&"#%-)0-:j?E!E"#%! 3%:bAr#20W*,!$3%3%: DC-*,!$#%ffN-Ar0-:90-:9
'"#\*,!$3g$C74!$3 R
#%) 7 J0-:V'"#%*+!$3g$ 7 w0-:DC7!$3&?!EM#%! 32:w!$C-:89#%"#21?E!E"#%! 32:J :CJ
#%8;":!$ :D"7&#%*:9Rwcd0-: "!$8"04#%-)j0-:7-"#\ #%8VC-[:(#%C-:9:CH'4$_<>* 0-:d*k732#2'32:V?[#2:9AB' #\g9R
$ :r0!E0-:
*@C-:3#\-632-)$:90-: : "!E :9)$5$N#% :9M*,/$<ff:#%0-:9<=!$#%3%7:$"7&#%*:9N
!$k#21AV!$6#\YcJ! 32: -R}cB0#%.#%6C-: '4#% :,0-:+<=!$8;k0!Ek#%10!$60-:] "-)$: k'$'4!E)!E $Rmcd0#%
*@C-:3ff0!$B325+:.?[#%:9Ad'O#%gD!$Ci0#%d0#\C-:9"0-: "!$8"04#%-)0-:7-"#\ #%8ER
$ :k!$3% ,04!Ed0-:
*,!$3\32: V :!E"8"0i :9: 7-D-$DM7g#\*: /!E:j !$#%-:ClAr#20f0-:
*[C:304!Ed8;* #\-:V0-:
!$3%3 G C4# :9":gK8; "!$#\gjQ0:k'4"#%*,!$3JAB#20x0-:,8M0!$-:3\3%#%-)f8; "!$#%& :9 A:9:Q0-:'4"#%*,!$3
!$ClC7!$3NgAB0-:fA:b74 : $0f'"#\*,!$34!$ClC74!$3?E!E"#%! 32:!$DC-:89#%"#2+?E!E"#%! 32:9Rwcd0#%8;* #%!G
#2m)#2?$:r0-: :-:9_B$<0-:H -)$: b'$'!E)!E $b!$Cx!+C47!$3^?@#2:9Ad'O#%&D<a$b0-: "!$8M0#%-)
0-:7-M#% #%8ER





ljx/ 0I



-5

=

5

+





=








fi 8-
fiffff
e






K

t$w 2






*@C-:3

0-:7-M#% #%8
>'
>'
>' KC
>'
=C
( >' KC
( >'
( =C
>' KC
>'
=C
( >' KC
( >'
( =C



jx/
jx/
jx/ 0I
jx/
jx/
jx 0I
jx
jx
jx/ 0I
jx/
jx/
jx 0I
jx
jx

























<=!$#%3%

.



aTff
aTff

,

3@Ri3$
3@Ri3$

,

aTff

&


,

3@Ri3$

aTff
aTff

&

,



,

3@Ri3$
3@Ri3$
3@Ri3$
3@Ri3$
3@Ri3$
3@Ri3$

,




,
,




2/,7

2MV.$



:8ER

,

&E< =

cJ! 32:

MV&$

<=!$#%3%

14373
"70
&7
"70
"6
&7
"7.
"6
"6
"70


"6
7" .




:8ER
@R
@R

3 k7
3 \18,

ff



3@R\18,
3@R\1:0
3@R\18,
3@R\18,
3@R\1:.



3@R\1:.
3 Rk
@
3@R\18,
3@R\1:.
3 R\1:0
@




ff



<=!$#%3%

1:.707"
07"7"
171718,
143$70
.7.7.
171718,
143K>
.7.$,
07070
07"7"
.76
07070
07"70



MV0$



:8ER
[R
[R
[R
[R

<>!$#\3%

. >&
" w,K&
" k.j1
" w,63

.77&7K3






"[Rk.7
"[Rk&7.
"[R >
&[Ri373
"[Rk.7"
"[Rw,K
&[Ri3$"
"[Rk.6
"[Rk.K3







"7$,K&7&
7.j1:07&
>&7"770
"7$,K.7"
7.j1:07.
>&$,r 1
>07&$,K.




>7$,K.
>0$,63$
7"7K3M1
>6>"7


:8ER
@R
[R
[R
[R
[R
ER
[R
[R
-R
[R
[R
&R

&K373
7.7"
>&7
$,K
1:.
>&j1
$,K
1:0
,r
$,K&
>7.
,7,

ff

k&j1
k"$,
$3
k7.
k07&
i3$"
1
k06
k.7
i3$&
k.j1
i3K

e

>7[Rk.70

7* :9D$< !$8"L& M!$8L@ ><=!$#%3% !$Ci"7#\-)1#%*,:j ,_4CW!$3%3ff3%7-#2N[$B'`?$:10!E
0-:9":.!E:j- 3%7-#249N& <>7-d#%4 !$8;:V$<
k' 3%:*WR r7&#%*:D!E:K<>$Dz
[3%?$:9 [R +
~
$N J:&#%7* z"zz/'"[8;:$N!$C
,~
$< BSK~ZR





1:K373 Xe _

" kK373



X"j1: !)

Ev2

F

cd0:.|&7!$"#2)$7-'i:;p-#% :48;:.' 3%:* 89!$ :.*@C-:3%32:CW!$r!,*67432#2'432:K'O:9"*k7-!E#2W'4 32:*
AB#20
#%& :9" :8;#%)m'O:9"*k7-!E#2 8; M!$#%gRYyQ:i#%& @C78;:l!m?E!E"#%! 32:]<>$:!$8"0 :g 5 #%
0-:+*k732#%'43%#%89!E#2x! 3%:$<0:+|&74!$#2)$7-'PRyQ:+0-:Q'O 1' :9M*67-!E#%Q8; "!$#%&.Y0-:
?E!E"#%! 32:B$<:!$8"0x !$C}:!$8"0Q8;3%74*,ffRVz mc ! 32: +!$C &N A:6)#2?$:6:73%d<>$rFAf<>!$*+#%3%#2:
$<' 32:*+9RkSb :9<>$:$N^0-: *,[C-:3J)#2?$:10-:A$" j'O:9<>$"*,!$8;:$NP!$4C 5X!W8;#%C:9"! 32:
*,!E)#\Z<>$60-:l3%!E)$:9H#%!$8;:9R}o-$
[N(!$3%3(0-:f$0-:9H*@C-:3%1!$4C M!$8"0#\-)}0-:7-M#% #%89
)#2?$: !$C3%5,#%*,#%3\!E('O:9<a$M*,!$8;:$RS C7!$34?[#%:9Ad'O#%g9N&:#20-:9 5,#2 :32<ff$D#%]8;* #\!E#2]AB#20
0-:]'"#%*,!$3/?[#%:9Ad'O#%g9NJC-[:6-$k O:9k!$&5Y!$C-?E!$g!E)$:$N 7-6C&:k-$60[7-k*6748"0n:#20-:9R}o$

-N#\fcJ! 32: &N4!$3\30:K*,[C-:3\!$4C "!$8M0#%-)H0-:7-"#\ #%89V!E:j8;*'O:9#2#2?$:$N-:;p-8;:9'd<>$D0-
:
*@C-:3ff!$CW0:10-:7-"#\ #%890!E M!$8"0W4325+W0:1C7!$3O?E!E"#%! 3%:9R

6o

B&



2B


e

x



29

K

,



,








c m*@C-:30-: b32* "7432:9"K' 32:*U!$k!W'O:9"*67!E#2Z'" 32:*WN^A:]#\g @C78;:,!}?!EM#%! 32:
<>$r:!$8M0X'4!$#2AB#\ :.C#% !$48;: :9 A:9:X*,!EL@9R @#\8;:.A:k*,!`5}0!`?$:H*,$:1?E!$3%7-:r0!$m?!E"#\! 32:9N
A:,#%g "[C748;:k!$CC4#2#2!$3 ?!EM#%! 32:b W:7:604!Ej0-:9:!E:,!$j*,!$&5m?E!E"#%! 32:r!$.?!$3%7:9N^!$
7-)$)$: :C 5 b:9:32:
MRHyQ:+89!$X0-:X'O j!f'O:9"*k7-!E#2X8; "!$#\gKX0#%b:3\!E)$:C
:9k$<d?E!E"#%! 32:R}z{ncJ! 32: [NA:f)#2?$:l:7432.<>$H_4C#%)i!$3%3/$'4#%*,!$332:-)$0n"732:9"1<a$H<>7-
#%!$8;: b32*
*:!$V0:b'4 32:*q$<P_44C#%-)1! K32* M732:9$< =*,#%4#%*,!$3 (32:-)$0
UAB#%0 Y*,!EL@9R b: '#2 :.0-:.<=!$8;B04!EB#2B0!$B0-:1 "-)$: d'"$'4!E)!E $N40:
*@C-:3^#\d-$



'

5



=<

d1:0707$

nx

Rog{('!

.



4





fififfv2$ff6ff )T$
fi
ff
v~ff





*@C-:3



























0-:7-"#\ #%8
>'
>'
>' KC
>'
=C
( >' KC
( >'
( =C
>' KC
>'
=C
( >' KC
( >'
( =C

<=!$#%3%

jx/
jx/
jx/ 0I
jx/
jx/
jx 0I
jx
jx
jx/ 0I
jx/
jx/
jx 0I
jx
jx

eWff
eWff

&

3@Ri3$

eWff
&
&

3@Ri3$
3@Ri3$

&

eWff
eWff
&



&

3@Ri3$
3@Ri3$
3@Ri3$




eWff
eWff
eWff





2fi,7


:8ER

&



,< =

cJ! 32:

BfiV&$

<=!$#%3%

.7





:8ER
@R

<=!$#%3%

3 k7

18,7,K0

3@RkK3
3@RkK3
3@Rkj1

077"
07j1
1:7&7&
06$3
077&
1:7&$,
0K373

e

"70
7" 0
,r
7" 0
7" 0
,K
$" ,
$" ,
$& ,
$" ,
$" ,
&7&

BfiV.$

e
e

3@Rk7

e



-Rk.$,
, Rk$,
&
" R\1:0
[
" Rk70
[
, Ri3K
&
" Rk6
[
" Rk7
[
, Ri3$
&

17143$
0K3$"
.70$,
17143K

e

3@RkK3
3@Rk7



171:&7707.
"76 1:0
"7"777
"7"770$,
.77j1:&
"7"77&6
"7"67>
.770j1:&





3@RkK3
3@Rkj1

<>!$#\3%

. k70
" \1:
k070
k07
, k"70



BfiV0$



:8ER
[R
[R
-R
-R
&R





"76 1:0
,K7070$,
"7K3>,7,
"76>&7
,K$,~1O





:8ER
[R
[R
[R
[R
[R

.6>
>07&
>.70
>.7"
,7,K

k7&
k6
k.70
w,K
\18,

ff fiff ff
e

,K.[R >.
,K&7[& Rk7
>.7[& Rw,K
>0jE1 Rk"6
,r>[" Ri3$0
>0jE1 R >"
>07[ Rw,63
,r>[" Rk.7&

7* :9D$< !$8"L& M!$8L@ ><=!$#%3% !$Ci"7#\-)1#%*,:j ,_4CW!$3%3ff3%7-#2N[$B'`?$:10!E
0-:9":.!E:j- 3%7-#249N& <>7-d#%4 !$8;:V$<
' 3%:*WR r7&#%*:D!E:K<>$Dz
[3%?$:9 [R +
~
$N J:&#%7* z"zz/'"[8;:$N!$C
,~
$< BSK~ZR



B

1:K373 Xe _

" kK373



F

X"j1: !)

jx/ 0I

Ev2

8;*'O:9#2#2?$:D,0-:d3%!E)$:9/#\ !$8;:9R~}[C-:3 !$4C,0-:7-"#\ #%8
>' KC P)#2?$:0: : w
"7&#%*:
<>$+0-:x3%!E)$:9l#% !$8;:NAr0-:9:!$+!$C4C#%-)Z0:}!$3%3 G C# :9:&+8; "!$#%& =*@C-:3
N 0-:7"#% #%8

>' KC )#2?$:D0-:13%:!$ :!E"8M0ffR :#%-)<>$"8;:Ci "!$8M0l 74 D0-:j'"#\*,!$3 ?!E"#\! 32:D0[7-
0-: "!$8"04#%-)0-:7-"#\ #%8ER





jx/ 0I



e b

q)

se}u





'w{

m5







3%#2L$:b0-:j':9?@#27' 32:*,9N[A:j_Cf325,0-:K_M 3%7-#2f 0-:j 'O$V8M0-:C73\#%-)k' G
23 :*WRcd04#%32:!$C/ 6*k78"0])$:!E :9?!E"#\!E#2+#%,' :9"<a$"*+!$8;: :9 A:9:+0-:bC# O:9:&/*@C-:3%9RJyQ:
:9'O$V:7432D#%lc ! 32: [R b[[ClM7g#\*:D!E:j !$#%:ClAB#20l0-:
!$C
*@C-:3%N[7"#%-)H0-:
C7!$3^?!E"#\! 32:d!$bC-:89#%#%W?!EM#%! 32:9N4:#20-:9B}0-:#2d ABm$r#%m8;* #\!E#2WAB#%0i0-:.'4"#%*,!$3
?E!E"#%! 32:R



0

e




w}\



t$s






1







yQ:Q*@C-:3b0-:m$MC-:9
*,!E)#\8m|[7!E:x' 32:* Ar#20 !
*,!E "#2p $<.?E!E"#%! 32:]AB0#\8"0
5
( RyQ:b0:]'O V!k'O:9"*67!E#2]8;4 "!$#%&l!$3%3 0-:b?E!E"#%! 32:#%]0-:
!EL$:j?E!$3%7-:<a* b
*,!E "#2p N!$C+7* 8; "!$#\g,0-:D`Ar9Ng8;3%7*+w!$CC#%!E)$!$3\9R B:732!E":D)#2?$:,#%HcJ! 32:
@R(Sr)!$#%ffNg_C#%-) 7 (0:d_" 3%7-#23%:!$C( 6AB#%C-:D?E!E"#%!E#2+#%'O:9<>$"*,!$8;: :9FA:9:]0-:
*@C-:3%9R
"#%-)W325X0-:]C7!$3?E!E"#%! 3%:.!$1C-:89#\#2Q?E!E"#%! 3%:.#%.! !$CY8M0-#%8;:$N 7-.0-:]C7!$3
?E!E"#%! 32:D!E":10-:32'<=73 #2<w7 :CW!$BC:89#%#2l?E!E"#%! 3%:D#%W8;* #%!E#2lAB#%0l0-:.'"#\*,!$3 ?!E"#\! 32:9R
o-$k0-:+3%!E")$: 6#%4 !$8;:+ 3%?$:CffN 0: : 1 "!E :9)$5n#%j0-:fC-7 32:*+!$3%32: 1C*,!$#%Z0-:7"#% #%8

143



n5

=U

fi 8-
fiffff

*@C-:3



jx/
0j1:
jx/
"K373
jx/ 0I &K3$&
jx/
.70K3
jx/
&77&
jx &K3$.
jx @
077.
jx
&77&
jx/ 0I
jx/
K" 373
jx/
>70 "
jx "K3K
jx @
"6>
jx
>07"

























cJ! 32:



, k7"$

b32* V .[Nk6 b32* V 0[N 7 K32* 143@Nk"7"$

3 \1:"

"7"6>
706>0
77K3
"76>
770K3
777
"7&6>.
770K3

b32* &N
<>!$#\3%
:8ER
@R

0-:7-"#%#%8
>'
>'
>' KC
>'
=C
( >' KC
( >'
( =C
>' KC
>'
=C
( >' KC
( >'
( =C

.E< =

<=!$#%3%

e
3@R\1:
3@R\1:"
3@R\1:
3@R\1:
3@R\18,
3@R\1:
3@R\1:
3@R\1:
3@R\1:
3@R\1O
3@R\1O
3@R\1:





706>0
$,K.7
$,K.$,
77"7.
$,K06

:8ER
ER

<=!$#%3%

<>!$#%3\

:8ER

1ERi3M1
1ERk7"
1ERi3$
1ERi3$
1ERk$,
1ERi3$
1ER\143
1ERi3$.
1ER\143
1 R\1
E
1ER\1:
1ER\171

18,6373$



,K$,K"j1

e 7

18,~1:"j1
18,63$7

,&Rk"7"
,&Rk&7

,K7"770
,K7.7"7

>0[Rk7"
>[0 Rk$,

18,~18,K0

7

,&Rk"70
.[Rk70

,K7&77.

ffff

<

>0[Rk"70
"6- Rk&7

1O>77"
1O>707

.[Rk7.
.[Rk7.

&j1:&j1:&
&j1:.707.

"6-R >&
"6- Rk06

1O7$373

.[Rk70

&j1:.707

"6-Rk0$,

1 \1:

ff

:8ER







7* :9+$< !$8L[ "!$8L@ ><>!$#\3% H!$C "74#%-)X#%*:i n_4C !$3\3D$'#%*,!$3B 3%7-#24
<>7-1#%4 !$8;:j$<V0-: K32* "732:9MK' 32:*iN^AB0-:9:0:,$'#%*,!$3/32:-)$0Z#%.)#2?$:ffR
r7g#\*:!E":r<>$Vz
[32?$:9 [R H
~
$N J:g#%74*qz"zzw'@8;: $N@!$C
~
$< BSK~ZR4S C!$0}*:!$D0!Er-:73%VA:9:1:97-"-:C}!E<a :9 10-7-R





!)

Ev2

,

" kK373

1:K373 !e fi_

"j1:

1

W*@C-:3 $d*,[C-:3
R(cd0:b<>$"*:9V:;p@'432$:D!H3%!E")$:9d :!E"8"0i :9:$N 7-VC-[:D H?$:95f"3%#2)0&325
|[7#%8L$:9B0!$i0-:.3\!E :9R




c f8;4893%7C-:$N 0-: :6:732b0- 04!EbC-5@!$*,#\8 "!$8M0#%-)]0-:7-"#%#%89d89!$ :6#2)4#2_489!$&325
* $:k: O:8;#2?$:HAB0-:i0:95i32&$Li!E $0i?@#2:9Ad'O#%&d$<(!]'O:9"*k7-!E#2ffRdz{C-:9:C^N "!$48"0#%),

'"#\*,!$3V$]C74!$3V?!E"#\! 32:AV!$,$<> : *$":}#%*'O$!$&, Z7-]:73%H0!$ 7"#%-)Q!Y "-)$:9
'$'!E)!E $Rdo-$K:;p@!$*,'432:$N:-<>$"89#%) KSB m!$m!$3%3 G C# O:9:&B8; M!$#%g9N !$C}:!E"8"0#\-) 7
0-:m'4"#%*,!$3D?E!E"#%! 32:9NV$<a : )!`?$:xA$" :x'O:9<>$"*,!$8;:i04!$ :<a$"89#\-)YSB 0:m8"04!$@G
-:3%3\#%-)W8; "!$#\g9NJ!$CZ0[7 :#%)i! 32: "!$8"0Z $0Z :96$<V?!E"#\! 32:9R,z{Z!$CC4#2#2ffN
#%Z*:,' 32:* 893%!$ :N 0-:]C-7 32:*+!$3%32: .C-*+!$#% "!$48"0#%)l0:7-"#% #\86 O:9:CY0-: :
'O:9<>$"*,!$8;:$R(SbDA:.0!?$:1!E")7-:CffN-0#%D0:7-"#% #\8r#%D8;4#% :&DAB#20l0-:j<=!$#%3O_M V'"#%89#%'432:d<>$
?E!E"#%! 32:K$"C:9"#%-)!$CW0:17898;:9:CW_4" D'"#\89#2'432:r<a$B?!$3%7:j$"C-:9"#%)-R







5

K

z D#%A$0W$#%-)k0!ED0-:K:73%/$< 7-D:;p['O:9"#\*:g"74]8;7& :9d H0-:j774!$3:;p@'O:8;!G
#2($< ?E!$3%7-:D$MC-:9"#%-)-RJyQ:B<>7C,0!E/C-7 32:D*,!$3%32:(C-*,!$#% >04!E#%9N&*,!$3%32:(C-*,!$#%H<>$
$0x?!E"#\! 32:k$"C-:9"#%)f!$CX?E!$3%7-:H$"C:9"#%-) d)!?$:+C4# :9":gj[7* :9"b$< !$8L[ "!$8L@. i"*,!$3%3 G
: KC-*,!$#%m?E!E"#%! 32:1$"C-:9"#%)-N:9?$:xAB0-:m_4C4#%-),!$3%3J 3%7-#%9Rdz b#\B)$:-:9M!$3%325W0-7-)0&b0!E
?E!$3%7-:j$"C-:9M#%-)*,!EL$:B+C# O:9:8;:K +0:j`?$:9"!$3%3 :!E"8M0i: O$dAB0-:i_4C#%)k!$3%3ff3%7-#2N@#%<
8"0-32$)#%89!$3 !$8"L& M!$8L@#%-)#%7 :CffRwz{C-:9:CffN&0-:j!E)7*:&)#%?$:]:!EM3%#2:9/<>$D7898;:9:Cf_"!$D!
?E!$3%7-:j$"C-:9M#%-)H'"#%489#2'432:b#% !$ :CiW_4C#\-)64325]:. 3%7-#2 (#2<PA:18"0& :60-:j"#2)0&D?E!$3%7-:$N



X







<

8#c

fififfv2$ff6ff )T$
fi
ff
v~ff



*@C-:3

0-:7-M#% #%8
>'
>'
>' KC
>'
=C
( >' KC
( >'
( =C
>' KC
>'
=C
( >' KC
( >'
( =C



jx/
jx/
jx/ 0I
jx/
jx/
jx 0I
jx
jx
jx/ 0I
jx/
jx/
jx 0I
jx
jx

























cJ! 32:



8V&$



['O$
<=!$#%3%
:8ER









&76



"7.70
,
171:
"j1O
&76







"7.70
,
171:
"j1O


0E< =

8V.$



@' $"
<>!$#%3\
:8ER
@R
@R

1:6>.
"7&7&

"7&7&

eW

0
&7&K3M1
>

"7&7&

eW
eW

3@Ri3$
3@Ri3>,

0
&7"7&7
>

1ERi3$0
3@Ri3$

!



3@Ri3M1
3@Ri3$0


3@Ri3>,

3 k7
3 \1:"


3@Ri3$
3@Ri3$&
3@R\143
3@Ri3M1
3@Ri3$0





eW
eW
3@Rk06

3@R\1:&

eW
eW



70$, w,63
7"K3 k07

eW

1:$,K&j1O>
77&
17171:
.7K3$&707
,63$7.

7"7"[Rk070
3 Ri3>,
@
3 RkK3
@
1:&7[. Rk0j1
1 Rk"7.
E

1:7&j1:&7.7&
770
17143$
.j1:7&707&
&707K3

$,K[R\143
3 Ri3$.
@
3 Rk7"
@
1:.7[& Rk7
1 Rw,K&
E

8d1:$

['O$
<>!$#%3\

:8ER
&R
@R

1:.7&77$,K"
1:7&j1:&7.7&

3@R\1O



8d143>

['O$
<>!$#%3\

eW

:8ER
ER
-R

"$,7,7,K7.7
7"77$,63$"
"777
7"7$,r7,
&77&7.
>&j1:7

1:0$,~1 k07
1O77 7

&77"7

7"77$,63$"
&77&7
>"j1:7"

[Rk70
1 Rk07.
E
1O>07[" R 1
R\1:.
[
K@3 Rk07.

&j1:70

[Rk"7"





2



1:7&7.[Rk.6
1 Rk0
E
1:[. R

7* :9k$< !$8"L& "!$8"L[ ><>!$#%3\ j!$Cn"74#%-)W#%*,:+ x_4CZ0:]_M 6 3\7-#2Z X
<>7-
%# 4 !$8;:b$</0-:H 'O$K8M0-:C73%#\-),' 32:*WR B7&#%*:b!E:H<>$bz
@32?$:9 [R

~
$N J:&#%7* z"zz('@8;: $`N4!$C
+~
$< BSK~ZR



1:K373 !e @_

v

,

p"j1: !)

9" kK373

A:189!$}!`?$#%C !$8"L& M!$8L@#%-), f8"0-[ :1!$-$0:9B-:$R(z <JA:.AV!$&d ,_4CW!$3\3^ 3%7-#%9N@A:60!$3%3
0!`?$:k !$8"L& "!$8"Lm f 5}!$3\3P0-:H!$32 :9"4!E#2?$:6?E!$3%7-:b!$&5&AV!5$R @*,#%0
b0- ABK0-`A ?E!$3%7-:
$"C-:9M#%-)+89!$X*,!EL$:!fC# O:9:8;:1 l0-:H :!E"8M0X#%
!$-)$<a$MC b' 32:*iN4:9?$:xAB0-:m_4C#\-),!$3%3
3%7#29Rkz "#2:9< NffAB0-:XA: !$8"L& "!$8"LZ0!?@#%-)l "#%:CQ0-:,!$#%)*:&

NPA:+89!$
'O B0:k8; "!$#%&
Rbz{x *:H89!$ :9NO'$'4!E)!E#%x*+!5}-
32:!$C} W#%*+*:C#%!E :
<=!$#%3%7-:$RS )$&@Ci$MC-:9"#%-)H<>$d0-:j?E!$3%7-:d89!$}0-:9:9<>$:1!`?$:6 :!EM8"0ffR





R


fi 2

R




X





VK37373>







Q Q

z{m*,!$&5i'" 32:*,9N?!EM#%! 32:r*,!5 :k8; "!$#%:C} l!EL$:7#%|[7-:.?E!$3%7-:N 7BA:k0!`?$:*$:
?E!$3%7-:r0!$x?!E"#\! 32:9RBcB0!Eb#\9N4A:!E:k3%&$L@#%-),<>$j!$m#\ {:8;#%?$:k*,!E'4'4#%-),<>*h0-:k?!E"#\! 32:
+0:1?E!$3%7-:9RDo-$b:;p@!$*,'432:$N!$m$'#%*+!$3 G#%8L b32* "7432:9B0!$B#%8L@r!Er0-:6*+!EL[ @N EN -N
[N !$4C
ER]cd0: i#%& :9 G#%8LQC#\ !$8;:.!E:]!$3%3C# O:9:& 7-jC-}-$j<>$"*U!i' :9M*67-!E#%Z!$
0-:HC#% !$48;: ]#%B! :g9RKow#%C#%),! G#%8"L b32* "732:9r$</32:-)$0
H89!$ :1*,[C-:3\32:C}!$K!
'O:9"*k7-!E#2.'4 32:* 5.#%& @C789#%-)d!$k!$CC#%#2!$3 ;06?!EM#%! 32:( b!EL$:V.0-:V*,#%"#%-)V?E!$3%7-:
[Rz{C-:9:CffN^04#%j#%K0-:,*:90[CXA:+7 : i*,[C-:3w0:' 32:* #\X0-:,3%!$ . :8;#%ffR B A:9?$:9`N
0-:9:b!E:r!6&74* :9($<^!$32 :9M!E#2?$:BAV!5@ k*@C-:3!$f#% :8;#2<>* i?!EM#%! 32:#%g
?E!$3%7-:
AB0#\8"0lA:.:;p['32$:.0-:9:$R

0

171

m&

p143

$

"

#

c"

65

3 1



171

171

&

65

+e

_'



65



o-$:;p-!$*'43%:$NJ0:9:f!E":fFAQ#\*'432:'M#%*,!$3/*,[C-:3\1$<d!$ #% :8;#2ffRmz n:!$8"0 A:l0!?$:
'"#\*,!$3V?!EM#%! 32:Ar0#%8"0 !EL$:x:i$<
'O# 32:l?!$3%7:9R z{ 0:}'"#\*,!$3D!$3%3 G C# :9:&,*@C-:3
=C-:-$ :C 5
MNPA:+#\*'432:k'O j!i#%-)3%:k!$3%32G C# O:9:gj8; "!$#%&jQ0-:'"#\*,!$3J?!E"#\! 32:9R



N P>

'



8!



fi 8-
fiffff

*,[C-:3

0:7-"#% #\8
>'
>'
>' KC
>'
=C
( >' jC
( >'
( =C
>' KC
>'
=C
( >' jC
( >'
( =C



Mx @
Mx @
Mx
Mx @
Mx
Mx 0I
Mx
Mx
Mx
Mx @
Mx
Mx 0I
Mx
Mx

























$V$

"

eW
eW
eW
eW
eW
eW
eW
eW

"

eW
eW
"

eW
eW

&


"


"
"


"



"

~m!E)#%8
<>!$#\3%

aW
aW
aW
aW

K3
1:0
1:.
K3
$ ,
143
171
1:.
1:.
1:0

aW
aW
aW
aW
aW
aW

143
71 1
1:&

3@Ri3M1

aW
aW





$V&$

~}!E)#\8
<=!$#%3%

:8ER
@R
@R
@R
@R
-R
@R

1:"$,K&
1:7"7"
>&7$,
1O>"$,
>07j1:
"7"7"
>07"
077.7j18,
7>7&
1:7"7"
>6>7&
>7"

3@Ri3M1



3@Ri3$

c

$V"$

~}!E)#%8
<>!$#%3\
:8ER

3@Ri3M1



+143< =

cJ! 32:

$R

~}!E)#%8
<>!$#%3\
:8ER

3 \171
3 \171
3 k$,
3 \1O
k&j1
3 i3$&

eW





eW



.7&[Ri3>,
3 R >.
@
3 R\18,
@
" Rk7
[
3 Ri3>,
@

0j1:0K3$"$,

:8ER

$,r>.7&K3$0

1:07&[R >"

67>.j1:&7

6>0[Rk.6

>&77.7&7"
1:&6>.6$3$.

1:7[Rk7"



7[0 Ri3M1
1O>.[Rw,K



143$.770707

143$&[Rk"7"

Tff

v

7* :9b$< !$8L[ "!$8"L[ ><>!$#%3\ d!$4CX"7#\-),#%*:H W_44C}0-:_4" K 3%7-#2x W<>7-
#\ !$8;:l$<H*,!E)#%8Q|[7!E:Q'4 32:*WR B74g#%*,:W!E:Q<a$}z
[32?$:9 [R

~
$N J:&#%7* zzzB'@8;: $N^!$C
}~
$< BSK~ZR^STC!$0Q*:!$4K0!E.-
":732VA:9:.:97"-:CW!E<> :9 j0-7-`R

1:K373 !e _



"j1: X)

1

X" kK373

N YHP>

9

z{



Ev2

0-:W'"#%*+!$3-${G:|[7!$3%+*[C:3 =C-:-$ :C 5

HA:W'O #%4!E5Y-${G:|[7!$3%,8;4 "!$#%&
:9 A:9:Z:9?$:95XFAxC#% #%48;b'4"#%*,!$3w?E!E"#%! 3%:9RHyQ:]89!$Y!$3\ i7 :+C47!$3*@C-:3%9R,o-$6:;p@!$*,'432:$N
#%W0:1C7!$3ff-${G:|[7!$3%r*@C-:3 N-A:60!`?$: UC7!$3^?!E"#\! 32:9N-:!$8M0}AB#%0i!,C-*,!$#\W$<
' "# 32:
?E!$3%7-:
X$<P0: :.!E:.C7*+*15,?E!$3%7-: MN@!$4C #%!E5,${G:|&7!$3\d8; "!$#\g :9 A:9:W:!$8M0
'4!$#2D$<C74!$3O?!E"#\! 32:9R

_'

9 ' JXo



'

b

:AB#%3%3$8;4#%C-:9^0:9:C# :9:&P8;* #%-:C6*@C-:3%ffAB04#%8"0.8M0!$-:3 :9 A:9:1'M#%*,!$3$!$C6C7!$3
Q
*@C-:3%9R}z n0-:f_" k8;* #\-:Cn*,[C-:3 =C-:-$ :C 5
* MNA:l0!`?$:l8M0!$-:3\3%#%-)}8;4 "!$#%&
$</0:k<>$"*
i#%*,'43%#2:
d!$CQ-l!$CC#%#2!$3JC7*,*65i?E!$3%7-:b<a$j0:HC47!$3J?!E"#\! 32:9R
z{ 0-:x :8;C 8;* #\-:C *@C-:3 =C-:-$ :C 5
:;p@ "!
( MNd0-:xC7!$3B?!E"#\! 32:,0!`?$:
C7*+*15 ?E!$3%7-:9Nd!$4C A:Q0!?$:Q8"04!$-:3%3%#\-)Z8; "!$#\gf$<.0-:x<>$"*
#
R z{
0-:l0#2MCY8;* #%-:C *@C-:3 =C-:-$ :C 5
1 MN(0-:WC7!$3?E!E"#%! 32:k0!`?$: 7 H!X#%-)3%:+:;p@ "!
C7*+*15m?!$3%7:$N^!$CXA:,04!?$:+8M0!$-:3%3\#%-)l8; "!$#%&j$<0-:<a$M*
}#
7-K3%5
AB0-: m#\.-$.:|[7!$3 m0-:+C7*+*15x?!$3\7-:$R
$ :]04!E1!$g5X$<D0-::+8"0!$4-:3%3%#%)l8;4 "!$#%&
!$32-: >AB#%0-7-j!$CC#2#%!$3 8;4 "!$#%&jX0-:H'"#\*,!$3P$6C7!$3J?!EM#%! 32: B#%K:-7)0X iC-:9_4-:
!$i#% {:8;#2^R

@B |

%] `Y z

N P>





' %
J
@B | ] OY z

N P>





N P>



5

@B*Y | J] +Y z



=

!|
p
K5



X' J%o

65

yQ:Q89!$ !$3% n*,[C-:3r!$ #% :8;#2 5 #%& [C4789#%-)
C7*,*65
' "#\*,!$3D?E!E"#%! 32:]!$C
:7"#%-)b0!E(0#\w:;p[ :C:C, :9($< ?!EM#%! 32:w<>$"*,(! # {:8;#%ffRJcB0#%89!$ :B\# 9N0-`A:9?$:9N@8;`?$:9:C
5]7-B:!E"3%#2:9D:"732DW'O:9"*k7-!E#249R

5

8"!

fififfv2$ff6ff )T$
fi
ff
v~ff



7a



s}

{wD

{#}

yQ:B_4" ('"`?$:B04!E9N&AB#%0: 'O:8;( k!E"8 G 8;#% :8;5$N@0-:d_"( 5['O:d$<^8"0!$4-:3%3%#%)K8;4 "!$#%&
!E:j!$#2)0&!$V0-:b'"#\*,!$34-${G:|[7!$3%V8; "!$#%&9N 7-3%:#2)0g0!$f0-:b'"#%*+!$34!$3%3 G C4# :9":g
8; M!$#%g9R cB0-:ffN(A:i' ?$:i0!E0-:m :8;C F5&'O:W$<K8"0!$:3%3%#%-)X8; M!$#%g+!E:i!$,#2)0&
!$l0-:X'"#\*,!$3B-${G:|[7!$3%W8;4 "!$#%&9N 7-l32:f#2)0&f04!$ 0:X8M0!$-:3\3%#%-) !$C C7!$3b-${G
:|[7!$3%d8; "!$#%&9NAB0#\8"0W!E:13%:V#2)0&d0!$W0-:1'"#%*,!$3O!$3%32G C# O:9:gD8; "!$#%&9R/ow#%!$3%325$N@A:
' ?$:+0!E10-:+0#%"CX 5['O:,$<D8"0!$:3%3%#%-)W8; "!$#%&1!E:+!$1#2)0&6!$.0:,'"#%*+!$3-${G:|&74!$3%
8; M!$#%g 7-(32:w#%)0g(04!$0-:D'"#\*,!$3@!$3%3 G C# O:9:&8; "!$#%&9Rcd0#%w*:!$4(0!E0-:B0-:9:
5['O:d$<(8M0!$-:3%3\#%-)8; "!$#\gr)#2?$:10:1!$*:6'"7#\-)HAB0-:WA:1:-<>$"8;:k!E"8 G 8;#% :8;5i!$
0-:k'"#%*+!$3^-${G:|[7!$3%K8; "!$#\g9R
$ :$N^0`A:9?$:9NO0!EKA:k)$:9b*,$:6'4"7#%)AB0-:iA:!$CC
0-:dC74!$3[${G:|&7!$3\8; "!$#%& 7w$0-:V'"#%*,!$3@-${G:|[7!$3%8;4 "!$#%& MRcd0#%w#%wC# :9:&
K'O:9"*67!E#2 AB0-:9:V-:#20-:9J0-:V!$CC#2#%1$<0:'4"#%*,!$3&-$ 0:VC47!$3&${G:|&7!$3\J8;4 "!$#%&
,0-:68"0!$:3%3%#%-)H8; M!$#%gD)!`?$:k*,$:.'"74#%-)-R







=



sx
u







JXu7J QRJ:t:[uF8N*Q*HKJlWPYH>Z9v [4]



!
,* - 7
- 7
,*



fi

Qs e



-

!

65



c ]0`A jSB
SB ;N8;#\C-:9r!$x#% :8;#2W'" 32:* AB0 :1'M#%*,!$3ff!$3%3 G C4# :9":g

8; M!$#%gl#% KSBbR @7-''O :}0-:x8M0!$-:3\3%#%-)Z8; "!$#%& :9 A:9:
1!$4C xA!$l-$fSBbR
cd0-:
d#%.:91 x!$C ]0!$ B:3%#%*+#%!E :CQ<>* #%.C-*,!$#%^R V7-60#%.#%1-$1'O# 32: 5
0-:+8;4 "78;#2X$<V0-:+'"#%*,!$3w!$CQC47!$3*@C-:3 R r:8;:,0-:+8M0!$-:3\3%#%-)l8; "!$#\g.!E":+!$3%3
SBbR cPf0-`A "#%8;:9N48;"#%C-:9B!$m#% :8;#2W' 32:* #%WAr0#%8"0

,!$C
*
(
1

RDcd04#%V#%dSB

7
V



$


K
B


R

*
(
1
/
c Q0-`A SB
SB
N(7-''O :+0!EH0-:f8M0!$-:3\3%#%-)}8;4 "!$#%&H!E:WSBbRV#%C-:9H!
>AB0-:9:
*k7 d0!`?$:.0-:
-${G:|[7!$3%d8;4 "!$#%&9N


04!EB#%D-$dSBbR A.N
!$C
!$*:H#%)32:9 mC*,!$#%ffN R1#%C:9B0-:H8"0!$4-:3%3%#%)+8; "!$#\g :9 A:9:
/!$C
&RKcd0-:
325lSB ?E!$3%7-:j<>$ 6#% R @#%*+#%3%!E"325$N[0-:j3%5fSB ?!$3%7:j<a$ 6#\W0-:.8"04!$-:3%3%#\-)8; "!$#%&
k#% -R V7-
-R B:48;:$N
H0!$B-+SB ?E!$3%7-:R/cd0#%D#\d!,8;& "!$C#%8;#2m!$
:9 A:9: 1!$C
0-:18M0!$-:3\3%#%-)8; M!$#%gb!E:1SBKR r:8;:6!$3%3P-${G:|&74!$3%B8; "!$#%&B!E:kSBbR `A 7-''O :
0!EK0-:-${G:|[7!$3%j8; "!$#%&b!E":HSBKR^V#%C-:9K!l8"0!$4-:3%3%#%)]8; M!$#%g :9 A:9:
!$C
k04!EK#\b$KSBKROcB0-:
/#%K :9K W!$C 0!$ D:3%#%*,#\!E :Ci<>* #2KC-*,!$#%^R 7r<>$
:j:3%#%*+#%!E :Cf<a"* 0-:.C-*,!$#\W$< $N *,:.$0-:9B'4"#%*,!$3 ?E!E"#%! 32:$N-!`5
6AB0-:9:
"N4#%
:9 N@AB0#%8M0,:3%#%*+#%!E : k<>*q0-:bC*,!$#%,$<
=#%48;:r0-:b-${G:|[7!$3%V8; "!$#\g!E:KSB MR
r:8;:$N^#2K#%r$r'O# 32:. W :9 / i!$C H0!$ V:3%#%*,#%4!E :C}<>* #2KC-*,!$#%ffRKcd0[79NO!$3%3
8"04!$-:3%3%#\-)k8;4 "!$#%&B!E:.SBKR('

@B



c|



J]

z

c)

+e

65

] Y[] ] ] >1K{`~{`
7
*
<@ B , @
z |
G
%]7^ Sz
@ ]4^ q| ) Sz | e
]4^

@B

]

@ @ Y[@ >1K{`



=

]7^

@ B

@
@B

=

e

]

@B

|

e

u




x

fi



n|



+|

M]

H@B /|

]

Bz

@Bq

]

]4^

Bz

@^

9)

@ B

G Y%z

z



JXu7J QRJ:t:[uF8N*Q*HKJlWPYH>Z9v [4]


!
, * - , * 7 - , * ! - 7
,*


!

- , * N@8;4#%C-:9D!$i#%K5{:8;#2f'" 32:*qAB0#%8M0f#\ jSB R@7-''O :
]4BP!$C ]bAV!$$SBbR@cd0-:^N&#%]0-:b_" 89!$ :$N<];B ] Y>G

SB
Qs e c ,0-`A jSB
0-:K-${G:|[7!$3O8; "!$#%& :9FA:9:

88

fi 8-
fiffff

G Q 1
@^ | ] OY"G

@^ z R];B G

!$C
ENJAB0#%8M0Q#%.#%*'O"# 32: :89!$7 :,0-:]8"0!$:3%3%#%-)W8; "!$#%&
r#
!$C
#
!E:QSBbRz{ 0-:x:8;C 89!$ :$N
A73%C :x)$:!E :9W0!$ NdAB0#\8"0
#%H#%*'O"# 32: 5n8;4 "78;#2 $<r0-:W'"#\*,!$3!$4C C47!$3V*[C:3 R r:8;:i!$3%3 #%!E"5Y-${G:|[7!$3
8; M!$#%g]0-:KC7!$3?!E"#\! 32:/!E":bSBbR@c k0`A "#%8;:9N@8;#%C-:9V!$f#% :8;#2]#\,AB0#\8"0

N
N !$C
Rrcd04#%d#%rSB
*
(
1
*
(
1
/
3

7-d$ KSB R
c k"0-`A SB
SB N 5,*,-$ #%89#2F5$N@A:b0!?$:jSB
SB Rc k"0-`A "#%8;:9N
8;#\C-:9j!$Z#% :8;#2x' 3%:* #\mAr0#%8"0

NJ!$4C

N
*
(
1
*
(
!$C
Rcd0#\#\dSB 7-D-$ KSB R
1
/
c Q0-`A SB
SB
N(7-''O :+0!EH0-:f8M0!$-:3\3%#%-)}8;4 "!$#%&H!E:WSBbRV#%C-:9H!
-${G:|[7!$3%d8;4 "!$#%&9N
>AB0-:9:

04!EB#%D-$dSBbR A.N w!$C .*k7 d0!`?$:.0-:
!$*:H#%)32:9 mC*,!$#%ffN R1#%C:9B0-:H8"0!$4-:3%3%#%)+8; "!$#\g :9 A:9:
/!$C
&RKcd0-:
325lSB ?E!$3%7-:j<>$ 6#% R @#%*+#%3%!E"325$N[0-:j3%5fSB ?!$3%7:j<a$ 6#\W0-:.8"04!$-:3%3%#\-)8; "!$#%&
k#\ -R V7- R B:8;: H0!$b-+SB ?E!$3%7-:9RDcd0#\d#%B!]8;g M!$C#%8;#2x!$
:9 A:9: 1!$4C
0-:8"04!$-:3%3%#\-)+8; M!$#%gj!E":HSBKR r:8;:,!$3%3J-${G:|[7!$3%j8; "!$#%&j!E:SBbRffcPW0- 0-:
:9?$:9":$N7-''O :r0!EV0-:j-${G:|&74!$3%D8; "!$#\gV!E:jSBbR4"#%C-:9V!k8M0!$-:3\3%#%-)k8; "!$#%&9N
+#
N@0!Ed#%V-$DSBbR-cd0:ffN@:#20-:9 J#%V :9V +!$C j04!$ :3\#%*,#%!E :C]<>* #2
C-*,!$#\ffN$$ B#% :9
!$C
O0!$ 1:3%#%*,#\!E :Ck<a* #%(C-*,!$#%^R V7-9Ng<>$ ^ :V:3%#\*,#%!E :C
<>* 0-:C-*,!$#%m$< $Nff *:H$0-:9j'M#%*,!$3^?E!E"#%! 32:$N "!5
AB0:9:
N^#%K :9K -NffAB0#\8"0
AB#%3\3ff:3%#%*,#\!E : f<>* 0:kC-*+!$#%i$<
=#%48;:10-:H-${G:|[7!$3%b8; "!$#%&K!E:HSB MR r:8;:H#2
0!$ V:3%#%*+#%!E :Ci<>* #2KC-*,!$#\ffRro-$ :H :9b
#%b-$K'O# 3%:j l:9
W!$C
N(!$3%30-:f$0-:9H?!$3\7-:6*k7 :+:* ?$:C <a"* #2kC-*,!$#\ffN 7-10:9:f#\6-mA!`5Y x:* ?$:
!$&5i$<0-:1?E!$3%7-: #2)$)$:9b0!$ Z<>* 0:kC-*+!$#%i$< N :89!$74 :6!EK* rA:H0!?$: Y'4"#%*,!$3
?E!E"#%! 32:RwcB0&7N!$3%3ff8"0!$4-:3%3%#%)k8; "!$#%&B!E:1SBKR '

@G

#o

e

65

@ @ @ >1K{` L] ] >1K{`~{`~{YM{`"
] ] ] :M{`"
- ,*

- ,* ! - ,* ! 65
@ @ @ >1K{`
X] ] >1K{`~{`~{Y
] ] :
@

- ,*
- 7
*,
<@B @ +
z |
@
= @B
G
@ B
4] ^
%]7^ Sz
]7^
@ ]7^ n| S) z | Se `]4^
@e

@BLY | ] %
z
A]
|

*]

9z

a@B

|

]

u



sx

fi



)

nz

G z

@^

-z

]

+|
e

Bz

]

!o



]

|

@B

@ B +|

z

@B

mo

JXu7J QRJ:t:[uF8N*Q*HKJlWPYH>Z9v [4]


! - 7
,*

Qs e c m0`A j SB ! SB - NJ8;#%C:96!$Y#\65{:8;#%Z#%QAB04#%8"0@ * @ ( YF@ 1 >1K{`N
@ / >1K{`~{`~{YM{`"N ] * ] ( >1K{`~{`~{YM{`"N!$CX] 1 ] / ] 3 :M{`"Rkcd04#%r#\ jSB - N
SB R
7-d$ K
c Q0-`A SB - 7
SB * N(7-''O :+0!EH0-:f8M0!$-:3\3%#%-)}8;4 "!$#%&H!E:WSBbRV#%C-:9H!
,
-${G:|[7!$3%d8;4 "!$#%&9N<@ B @ >AB0-:9:z |04!EB#%D-$dSBbR = A.N@ B !$C @ *k7 d0!`?$:.0-:
!$*:H#%)32:9 mC*,!$#%ffN
G R1#%C:9B0-:H8"0!$4-:3%3%#%)+8; "!$#\g :9 A:9:@B/!$C ]4^&RKcd0-:
325lSB ?E!$3%7-:j<>$%]76
^ #%SzR@#%*+#%3%!E"325$N[0-:j3%5fSB ?!$3%7:j<a$ ]76^ #\W0-:.8"04!$-:3%3%#\-)8; "!$#%&
@ 1!$4C]7k
^ #\n|-RS)V7-zY |RSeB:8;:`]4H
^ 0!$b-+SB ?E!$3%7-:9RDcd0#\d#%B!]8;g M!$C#%8;#2x!$
:9 A:9:
@e

0-:8"04!$-:3%3%#\-)+8; M!$#%gj!E":HSBKR r:8;:,!$3%3J-${G:|[7!$3%j8; "!$#%&j!E:SBbRffcPW0- 0-:
:9?$:9":$N7-''O :r0!EV0-:j-${G:|&74!$3%D8; "!$#\gV!E:jSBbR4"#%C-:9V!k8M0!$-:3\3%#%-)k8; "!$#%&9N
+#
N@0!Ed#%V-$DSBbR-cd0:ffN@:#20-:9 J#%V :9V +!$C j04!$ :3\#%*,#%!E :C]<>* #2
C-*,!$#\ffN$$ #% :9
!$C
0!$ 1:3%#%*,#\!E :Ck<a* #%(C-*,!$#%^R V7-9Ng<>$ ^ :V:3%#\*,#%!E :C
<>* 0-:C-*,!$#%m$< $Nff *:H$0-:9j'M#%*,!$3^?E!E"#%! 32:$N "!5
AB0:9:
N^#%K :9K -NffAB0#\8"0
AB#%3\3ff:3%#%*,#\!E : f<>* 0:kC-*+!$#%i$<
=#%48;:10-:H-${G:|[7!$3%b8; "!$#%&K!E:HSB MR r:8;:H#2
#%b-$K'O# 3%:j l:9 ( W!$C H0!$ V:3%#%*+#%!E :Ci<>* #2KC-*,!$#\ffRro-$ k :H :9b

@BLY | ] %
z
A]
|

*]

9z

a@ B

@B +|

@B

|

]

|

@^

@B

]

)

nz

G z

-z

]

Bz

8%)

+|
e

fififfv2$ff6ff )T$
fi
ff
v~ff



zN(!$3%30-:f$0-:9H?!$3\7-:6*k7

? :C <a"* #2kC-*,!$#\ffN 7-10:9:f#\6-mA!`5Y x
:* ?$:
:+:* $
! &5i$<0-:1?E!$3%7-: #2)$)$:9b0!$ Z<>* 0:kC-*+!$#%i$< $N :89!$74 :1A:k04!?$:H!Eb* Y'4"#%*,!$3
$
?E!E"#%! 32:RwcB0&7N!$3%3ff8"0!$4-:3%3%#%)k8; "$
! #%&B!E:1SBKR '

7a

Dx

w}


x

]

!o



t2sw {



cd0-:H'":9?[#274B:"732K8;*'4!E:0:HC4# :9":gK*@C-:3%bAB#20x: 'O:8;K W0-:!$*7&K$<'4"7#%)
!$8"04#2:9?$:CffRDyQ:689!$^N4<>$r:;p-!$*'432:$N-`A "732:j7B!+*@C-:3 3%#2L$:

* +AB0-:W:<a$"89#\-)+SB #%8;:
A:,)$:9 7 .!$6*6748"0Z'4"7#%)f!Ek32:.8; 6Z0-:]*[C:3 * R r`A:9?$:9N0-: :,":732.C}-$
C#%#%-)7#%"0 :9FA:9:^N!5$N!b*@C-:3AB#%0.'"#\*,!$3-${G:|[7!$3%w8; M!$#%gN$!$&5j$<0-:V8;* #%-:C
*@C-:3% * N ( $ 1 RyQ:j)$:9D0-:j!$*:j'4"7#%)1#%l!$3%3 <a7RwyQ:.89!$i!$CCl *:.C-:9!$#\3% k0: :
:7432 5l8;*'4!EM#%-)0-:1!$5[*'4 $#%8 :0!?@#27-R

NY

S5





le





P



$

+Ro '



(
( MNAB0-:9":
cd0:+:3%!E#%?$:f8; k$<B!$8M0#2:9?@#%-) jSB
#%
#\10-:f&74* :9k$<D?!E"#\! 32:
1 /#%*:$R B A:9?$:9`N 5
!$C
#\/0-:#%C-*+!$#%]"# 9:$R(SB 9N@SB N[!$4CfSB !$#2?$:3%5,!EL$:
!EL@#%-),!$C-?E!$g!E)$:6$<w0-:j<>748;#2!$3ff!E7:j$<8"0!$:3%3%#%-)H8; M!$#%gNA:.89!$}:C78;:j04#%V
( b<>$

r<a$ * RyQ:]' ?$:CZ#%Qcd0-:9$":*
0!E jSB
SB
( !$C
1 !$C
(
(
(
SB
!$C}0:#2j8; j!E:
MN
MNff!$
C
d: 'O:8;#2?$:325$R.Sb 5@*' $#%8k!$!$325&G
#%10-`Arj0!E1:<a$"89#\-)iSB
0!$6!$ 5[*,' $#%89!$3%325X3%#%)0g325x*$:+8;10!$Q:<a$"89#\-)iSB
R
r`A:9?$:9N0!?@#%-)10-:bC47!$3-?E!E"#%! 32:8;73%C :r!$C-?!$&!E)$:97D#%+8; 748;#2,Ar#20,?E!E"#%! 3%:B!$C
?E!$3%7-:+$MC-:9"#%-)x0-:7-"#\ #%899RWyQ:W!$3% x' ?$:C #%Ycd0:9$:*
i0!E KSB
SB
SB
(
( MN
( MN
( MNP!$
( B:' :8;#%?$:325$R6Sb 5@*'-G
SB
!$Cx0-:#2.8;.!E:
C
$#%8]!$!$325@#%1"0-`AB10!E10:]8"04!$-:3%3%#\-)W8; "!$#%&k!E:]*$":]8; 325Q04!$Z0-:f-${G:|&74!$3%
8; M!$#%g1!$C "#%)]-i*$":'"7#\-)-Rjy 0:QA:,!$C4CQ-${G:|&74!$3%j8; "!$#\gjX0:,C7!$3
?E!E"#%! 32:N[0:.`?$:9"!$3\3P!$ 5@*' $#%818; B#%d#%3%3 0-:1!$*,:.!$d0-:18M0!$-:3\3%#%-)H8; "!$#\gr!$32-:$N
7-BA:H!$8"0#%:9?$:k*$":1'"74#%-)-RzFb#%d0-:9":9<a$:H!+*,[C-:3PA$"0m8;"#%C-:9"#%)-Row#%!$3\325$N4#%mcd0-:9EG
(
( MN
( MN4!$C
:*
A:.'4`?$:C}0!E KSB
SB
SB
!$Cl0:#2B8; r!E
:
(

: 'O:8;#2?$:325$RSr)!$#%ffN&!$5[*'4 $#%8d!$!$325@#%/0-`Ar(0!E8"0!$:3%3%#%-)j8; "!$#%&/!E:b*$:
8; 325]0!$f0-:j-${G:|[7!$3%D8; M!$#%gD!$4C "#%-)k-k*,$:K'"7#\-)-RJ~m!$#%&!$#%#%-)k)$:-:9"!$3\#% :C
!E"8 G 8;"#% :8;5lW0-:6!$3%3 G C# O:9:&D8; "!$#%&B#%D!E)!$#\W0-:1*B8; 325$R

+RoZ'

,*

7

e

-



'



-

sRoZ'!
+Ro ' +RoZ'!



-

+Ro ' sRoZ' +RoZ'



1:
+Ro

- 7

!

+RoZ'

ge

L

171

+Ro



,*

7

-

65



1:

+Ro

,*

- ,* !

!

-

!

,*

-

+Ro ' sRoZ'



c Y8;893%74C-:$NV0-: :i:"732,0`A 0!E9Nd!$]*,#2)0& :i:;p['O:8; :CffNVA:}#% )$::9"!$3d)$:9f*$:
'"74#%-)+#%<A:#%8;":!$ :60:H!$5[*'4 $#%868;9R6~i@C-:3%bA$0X8;#%C:9"#%-)f!E:H0-:6'4"#%*,!$3 -${G
:|[7!$3%1*,[C-:3N ( k
N!$CY0-:+'4"#%*,!$3(!$3\3 G C# O:9:&1*@C-:3 R /!$8M0n)#%?$:6!xC# O:9:&1!$*7&6$<
'"74#%-)}!E!XC# O:9:&k!$5[*'4 $#%8l8; 9RZyQ:i*,#2)0&H!$3% Q8;#%C:9 * #% :!$C $<d0:f'4"#%*,!$3
-${G:|[7!$3%*@C-:34#\8;:$NgAB04#%3% #2#%/!$ 5@*' $#\89!$3%3253%#2)0&325H*$:r:;p['O:#%?$:$N&#232:9/7 "!$48"0
iC74!$3O?!E"#\! 32:9R






7a sffw'x {J



f





wu



tw}




sw'{

2

yQ:1!E)!$#%i"!$W *:j:;p@'O:9"#%*:& ,:;p['32$:K0-:.#2)4#2_489!$8;:b$< 0: :j0-:9$:9#%89!$3P!$CW!$ 5@*'-G
$#%8fC# O:9:8;:9Rmc ! 3%:
])#2?$:k:7321n*:f#\ !$8;:6$<D0-: b32* "7432:9".'4 32:*
7#\-)!+ !E#%8.?!E"#\! 32:j$"C-:9M#%-)-R/cd0:j:;p['O:9"#%*,:gd!E":6!E)!$#%m8;#% :&dAB#%0l0-:10-:9$":9#%89!$3
:74329RDow#2" 9N:-<a$M89#%-) jSB x!$m!$3\3 G C# O:9:&B8; "!$#\gb!$8"0#2:9?$:b0-:k* r'"7#\-),!$C
0!$w0-:d"*,!$3%32: w"74g#%*,:9R [:8;4CffNH0-: :D'4 32:*,w#%4 !$8;:9N:<a$"89#\-)jSBn0-: #%4!E5
-${G:|[7!$3%8; "!$#%&!$8"0#2:9?$:(0-:D!$*:d!$*7&w$<4'M7#%-)b!$*,!$#%&!$#%#%)rSBnH0-:D8"04!$@G
-:3%3\#%-)]8;4 "!$#%&9RHz Q!$C4C#2#2ffNff:<a$"89#\-)WSB Q0-:,8M0!$-:3%3\#%-)]8;4 "!$#%&j!EL$:13%-)$:9



6

!171



E



8r

fi 8-
fiffff


l
!$8"0#2:9?$:$Rbcd0#2MCffN!$CC#%)0-:68M0!$-:3%3\#%-),8; M!$#%gr f0-:1'M#%*,!$3^!$3%32G C# O:9:gr8; "!$#%&
C-[:d-$B#\8;:!$ :j'M7#%-)-N@!$Ci*,:9:325]!$C4CD`?$:9M0-:!$Ci ]0-:j"7&#%*:$R

*@C-:3



(

0-:7-"#%#%8
!E#%8
!E#%8
!E#%8
!E#%8




(





<=!$#%3%

aT0

143K
143K



c

ff

:8ER

<<

3@Ri3$
3@Ri3$
3@Ri3$

+171 < =

cJ! 32:

b32* V 0[N 7 b32* 143@Nk"7"$ K32* 171ENw,K$

V. k6

K32* [N
<=!$#%3%
:8ER

e

1717143
1717143

3@Rk7.
3@Rk6
3@Rk7&

<<





ffff



ffff

<>!$#%3\

:8ER

W

,~1:7
,~1:7

efiff

<>!$#%3\

[R >&
[Ri3$
[Rw,K&









:8ER

$,r$3K
$,r$3K

7[Ri3$
K@3 Rk7
18&, Rk0$,



7 * :9b$< !$8L[ "!$8"L[ ><>!$#%3\ d!$4CX"7#\-),#%*:H W_44C}0-:_4" K 3%7-#2x W

<>7-
#\ !$8;:$<J0-: b32* "732:9"'" 32:*WR B74g#%*,:!E":b<>$Dz
[3%?$:9 [R
~
$N J:&#%7* zz"z/'@8;: $`N!$C
,~
$< BSK~ZR



1:K373 !e _

7aT
{t2x]w}Qt$sffwt$l {ff
Qt2 ff


*@C-:3



0-:7-"#\ #%8
>'
>'
>' KC
>'
=C
( >' KC
( >'
( =C
>' KC
>'
=C
( >' KC
( >'
( =C

(

jx/
jx/
jx/ 0I
jx/
jx/
jx 0I
jx
jx
jx/ 0I
jx/
jx/
jx 0I
jx
jx



(


(


(


(


(


(


(


(


(


(




(

+1:E< =

cJ! 32:

,







sww}

b32* 171ENw,K$

3 i3$&
3 i3K

7.j143
7&770
K3M143
7.j143
777
K3$7
>67
76>

.K373M1:&70
"7&7K3M171
07.7
.K373M1:&70
1:7"6
07.6
.707.6>"$,
1:6,
"707.
"7&7K3M171
&K3$"

<=!$#%3%

3@Ri3>,

eW
3@Ri3M1
3@Ri3$&

7

W
e
eW

7

W
e
eW

<=!$#%3%

3 k07&
3 w,K"
3 k"$,
1 \1:
3 k&j1
3 k"7.
1 \1:.
3 k&K3
3 7
1 \143
3 7

7&770
0K3$&
0j1O
K3$"6
0K3$0

3@Ri3>,
3@Ri3M1
c

:8ER
@R
@R
@R
ER
@R
@R
ER
@R
@R
ER
@R



3@Ri3>,

7"6
171


sw'{

b32* V 0[N 7 K32* 143@Nk"7"$

eW

77.
171



" kK373

V. k6

b3%* [N
<=!$#%3%
:8ER
@R
@R

77&
77.
171
77&
1:
1:
77"
1:



v2


,

X"j1: !)

"K3$"77&
7$,63$"
77.7.
"K3$"77&
7.77
7$,r
"$,~1:"7.
70j171



eT
1ER\18,

eT



ff

7$,63$"
143$.$,
171:7"
70j1O>
171:j1

: 8ER
[R
[R
@R
@R
@R
@R
ER
@R
@R
[R

1:& k&$,
:1 \1:
3 k.7&
K3 >
3 k0K3
3 k.7&
j1 k"6
3 k0j1
3 k&7&
1:0 k7

afiff

3 Rk&70
@
j1ERkj1
3 Rw,63
@

<=!$#%3%



&7&77.707&
"707

:8ER
[R
[R
@R
[R
@R
@R
ER
@R

7"7 k.
7&7& k"7
3 >.
1:. i3$
3 k"7
3 >.
7 1 \1:"
3 k"j1



:1 0[R >"
3 R 7
@
3 R 7
@
>"7[& Rw,K"
3 R 7
@

7 * :9b$< !$8L[ "!$8"L[ ><>!$#%3\ d!$4CX"7#\-),#%*:H W_44C}0-:_4" K 3%7-#2x W

<>7-
#\ !$8;:$<J0-: b32* "732:9"'" 32:*WR B74g#%*,:!E":b<>$Dz
[3%?$:9 [R
~
$N J:&#%7* zz"z/'@8;: $`N!$C
,~
$< BSK~ZR

1:K373 !e _



,

X"j1: !)

v2


,

65

" kK373

yQ:x!$3% Z:;p@'432$:C 0:}!$C-?E!$g!E)$:x$<b*k732#%'432:l?[#%:9Ad'O#%gH$<K#% :8;#2 '4 32:*,H<>$+C-5&G
!$*,#\8r?E!E"#%! 32:j!$Cl?!$3\7-:K$"C-:9"#%)k0-:7"#% #%89RJz{WcJ! 32: [NA:j)#2?$:.:7432<a$ K32* "732:9

/1:

8#b



fififfv2$ff6ff )T$
fi
ff
v~ff





' 32:*,9RryQ: :9?$:k0!EK0-:k'"#%*,!$3 !$3%3 G C# :9:&r*,[C-:3J#%b-$j8;*'O:9#2#2?$:kx0-:k3\!E)$:9
' 32:*,9RKcd0-: :r"74g#%*,:r!E:H !$#\-:CxAB#20m0-:k8M0!$-:3\3%#%-)+8;4 "!$#%& =!$CQ!+'4"#%*,!$3
!$3%3 G C4# :9":gd8; "!$#%& D7#\-)0-:6"*,!$3%32: dC*,!$#%i$r0-:6C7 32:.*+!$3%32: BC*,!$#%i0-:7"#% #%8
$0] :9($<ff?E!E"#%! 3%:$/+0-:rC74!$3-?!EM#%! 32:9R :#%-).<>$"8;:C+ "!$48"0, 74 (0-:d'4"#%*,!$3
?E!E"#%! 32:10&7-".0-: "!$8M0#%-)i0-:7"#% #%8ERfS C7!$3?@#2:9Ad'O#%&.!E''O:!E"j x O:960-: "!$8M0#%-)
0-:7-M#% #%8r?$:95W#2)#%_489!$gd!$C?!$&!E)$:Bi0#%V' 32:*WR





*,[C-:3



(


(


(


(


(


(


(


(


(


(


(




(

E)

0:7-"#% #\8
>'
>'
>' KC
>'
=C
( >' jC
( >'
( =C
>' KC
>'
=C
( >' jC
( >'
( =C

Mx @
Mx @
Mx
Mx @
Mx
Mx 0I
Mx
Mx
Mx
Mx @
Mx
Mx 0I
Mx
Mx

+1:E< =

cJ! 32:

/



8,7

1O
O1

O1

aW
aW
aW
aW
aW
aW
aW
aW
aW

@' $"
<>!$#%3\
:8ER




O1


O1



3@Ri3M1


1O


aW
aW
aW
aW

/





K

8V0$

['O$
<>!$#\3%



1:" k7
:1 & \:1

>7.$,7,

aTff

1:.770707"6
0j143$7&7

7&7.[Rw,K
1:.7[" Rk&7

1:7"77&7"77

18,K.$,&Rkj1
0 Ri3$"
[

3 Rk&$,
@
18,&Rw,~1
1 Rk7&
E

aTff



K3@Rk$,
3 Ri3$
@
3 Rk6
@
K@3 Rkj1
3 Rk"7
@

K3M1:
1O>7j1:
777.

:8ER

aTff

18,&Rkj1

ff

8d1718

@' $"
<>!$#\3%

: 8ER
[R
[R

1O$3$7.$,
1:7.7&6>
6
1O$3$706
7
>"77"
1O>707.70
171O>6
7.
1:7.7&6>

5







$,r>.770
&K373$&7.7&

K, .[Rk"7
1:"j1ER\1:0

1:.7"6$3$.7

>j1ER\1:0

7* :9D$< !$8L[ "!$8L@ ><>!$#%3\ !$CW"74#%-)k#%*:1 ,_4CW0:._" d3%7-#2l ]0-:9:
#\ !$8;:K$<V 'O$.8M0-:C73%#\-)]'4 32:*WR B74g#%*,:j!E:<a$1z
[32?$:9 [R }
~
$N J:g#\7* zzzO'@8;: $N!$4C
j~ Y$< BSK~ZR$S C!$"06*:!$4Jb 3\7-#2
#\V<a74CW!E<a :9 .0-7-R

1:K373 !e _

'1:



m"j1: !)

m1



Ev2

" kK373

z{ncJ! 32:
[N(A:])#%?$:]:"7326<a$H0-:l 'O$6"8"0-:C743%#%-)i'4 32:* AB0-:n0-:9:l!E:f!$ @CC
[7* :9H$<bA:9:9L@9R b: '4#2 :W0-:W<=!$8;,0!E,#%H04!$H0-:i -)$:,'$'4!E)!E $`N0-:l'M#%*,!$3V!$3%3 G
C# O:9:&d*@C-:3^#%d$B8;*'O:9#2#2?$:j}0-:13%!E)$:9B' 32:*+9R(cd0-: : DM7g#\*:D!E:1 !$#%-:C
AB#20,0-:B8"04!$-:3%3%#\-)j8; "!$#%&/!$4C "!$48"0#%)j,0-:d'4"#%*,!$3@$C7!$3-?E!E"#%! 3%:DAB#20,*+!$3%32:
C-*,!$#\ffR SrAr#20 0-: b32* "7432:9H' 32:*iN :#%-)X<a$M8;:C "!$48"0 7 0-:W'4"#%*,!$3
?E!E"#%! 32:10&7-".0-: "!$8M0#%-)i0-:7"#% #%8ERfS C7!$3?@#2:9Ad'O#%&.!E''O:!E"j x O:960-: "!$8M0#%-)
0-:7-M#% #%8/?$:956#%)#2_489!$& !$C-?E!$&!E)$:H0#% ' 3%:*WR
$ :d!$3% K0!Ewk0-:D3%!E)$: #%4 !$8;:$N
0-:,*+!$3%32: j :!E"8M0Q :9:+#\K !$#%-:CXAB#%0m0:,8"0!$:3%3%#%-)l!$CX0-:,!$3\3 G C# O:9:&K8;4 "!$#%&9N
"!$48"0#%)Z 0:}'"#\*,!$3D$lC7!$3d?E!E"#%! 32:iAB#20 *,!$3%3%: ]C-*+!$#%ffR c n8;893%74C-:$NdC-5@!$*,#%8
"!$48"0#%)l0:7-"#% #\89b89!$n!E)!$#% :,#2)4#2_489!$&325m*$":: :8;#2?$:]AB0-:X0-:95Q32[$LX!E $0Q0-:
'"#\*,!$3O!$CiC7!$3O?@#2:9Ad'O#%&9R



x

#

K

=





8

5

fi 8-
fiffff

Rfi? R

^ ff


$C



Md1:07070$



V0-:)b:9(!$3R
( 7C#%:C6*@C-:3%3\#%-)r!$4Ck32?[#\-)r0: OG |&7-:9:4 '4 32:*WN!$4Ck!j[7-": :9 G
%# -)1' 32:*q7#%).8"0!$:3%3%#%-).8;4 "!$#%&9R(cd0-:950`A 0!E8M0!$-:3\3%#%-).8; "!$#%&#%48;:!$ :
0-:j!$*7&D$<w8; "!$#%&V'$'4!E)!E#%ffR(cd0-:95f8; {:8;7:j0!ED0-:K`?$:9M0-:!$CB!$" [89#\!E :ClAB#20
8"04!$-:3%3%#\-)+8; M!$#%grAB#%3\3ff'4!5} Yx' 32:*+dAB0#\8"0i":|&7#%:13%!E)$:H!$*7&b$</ :!E"8M0ffNO$
32:!$Cl 0-M!$0#%-) :0!?@#27-`RwcB0-:95]!$3%H"0-`A 0!Ed8M0!$-:3\3%#%-)H8; "!$#\gV$'O:l0-:jC-[$D
#%& :9: #%)]?E!$3%7-:H$"C-:9M#%-)l0-:7-"#%#%899Rjo-$.'O:9"*k7-!E#2x' 32:*,9NO!W#%*,#\3%!Er#%C:!]AV!$K':9?@# G
73%5+'$'O :C 5 b:9:32:
MR

65

K

+



d1:0707$

VK373$$

V0#-!$C ^:9:
(<>[897:C0-:d 74C-51$<ff8;* #%-:C*@C-:3%w$< 'O:9"*k7-!E#2H' 32:*,9R
c 0-:#%J 74C-5.#%893%74C-:C1-$J325j0:'O:9"*k7-!E#2k8; M!$#%gN 7-w!$3% K!$3%3g0:$0-:9(8;4 "!$#%&

$<V0-:,'" 32:*WRcd0-:#%.8;*'4!E"#%Z*:!$7-":+#%j!$Z:;p@ :#%Q$<V0-:'$'4!E)!E $H8;*'4!EM#%
!E''"!$8"0k$< @8M0&73% :!$C [78"L$:95
MNgAr0#%8"06*:!$7-":P0-:VC# O:9:& 8;* #%-:Ck*@C-:3%PAB#20
: 'O:8;6 m0-:#%k! #\3%#2 5x x'"7-:+0-:] :!EM8"0 '4!$8;:fAB#20n8; "!$#%&1'4$'4!E)!E#2ffR B A:9?$:9`N
0-:#2r*:!$7-:6#%d#%C:9' :4C-:gV$<w0-:13%:9?$:3^$<8;#% :8;5W*,!$#%&!$#%-:CWW0:68; M!$#%gr!$C
C-:9'O:C 7-'O10: :9w$<48;$":8;'$'4!E)!E $M#% :!$CffRJcd0-:95.0:9$:9#%89!$3%3256C#%8; ?$:9J0:V8;M#2 :9"#%!
7C:9fAr0#%8"0 *,#\#%*,!$3b8;* #%-:C *@C-:3%l0!`?$:Z0-:Z!$*:Q'4"7#%)Y'O`A:9}!$l<=73%3K8;* #%-:C
*@C-:3%D!$Ci:*'4#2"#\89!$3%325,C-:*4 "!E :.0-:1:732V}C# O:9:gD'O:9"*k7-!E#2l'4 32:*,R



/

)

fiVK373$$

cVK373M18



e





V!$898"0[7:9/!$3 R
(<a$M*,!$3%325k 7C#2:Ck0-:B: :8;#2?$::/$< A6*,[C-:3\3%#%-)r :8"0#\|&7-:(0!E
" !$<a$"* !r@G #%!E"5j }#%& r!$6:|&74#2?!$3%:g #%!E5. JN`4!$*:325$NE0-:C47!$3 "!$4 <a$M*,!E#2
!$CY0-:]0#\CC-:X-:$RiSbZ$"#%)#%!$3(*@C-:3$<D0:+' 32:*WN #%1C7!$3/!$CY#260#%CC:X "!$ <>$ G
*,!E#24k!E":l8;*,'4!E:C AB#20n: 'O:8;H X0-:l'O:9<a$M*,!$8;:+$<b!X&7* :96$<b32@89!$38;4#% :8;5
:8"04#%|[7-:,#%893\7C#%-)X!E"8 G 8;"#% :8;5$Nd!$C AB#20 : 'O:8;, n0-:}8M0--32$)#\89!$3 !$8"L& "!$8"L[#\-)
!$32)$$"#%0*WNo bN!$Ci~}SBKR

)

SM_

Sj_

d1:07070$

$:9K!$Cxcd!$-)
bC-:9?$:32$'O:Cm!+<>"!$*:9A$Li<>$b5[ :*+!E#%86*,[C-:3J :32:8;#%ffRBcd0-:95
C-:*4 "!E :C 0-:#2!E''4!$8"0 0-:W:9?E!$3%7!E#%n$<b!$CC#%)x!X8;:9!$#% 893\!$H$<r#%*,'43%#2:Cn8;@G
"!$#\gk Q!$ $"#2)#%!$3*@C-:3 RQcd0-:f:9?E!$3%7!E#2 0-:7-M#% #%8,74 :C #% !$ :C !$ :;p@ :#%n$<
0-:j0-:9$":9#%89!$3ff8;*'432:;p-#2F5+: #%*+!E :D'$'O :C 5
!$C-:3
MRcB0-:#2V:;p['O:9"#\*:g!$3 :"732
0- 04!EB0-:k!E''!$8M0m#%d'*,#%"#%-)-R B A:9?$:9NOAB#%0W0#%r!E''!$8M0}-:k-:9:CB0-:6#%4 !$8;:
C!E! :.!$i:;p['43\#%89#2D#%-'47 ,0:1*:90-@C9R

=

e

fi;
w


qd1:070K3>

M?

FTE

65

yQ:Q0!?$:x'O:9<>$"*:C !$ :;p@ :#2?$:X 7C-5 $<.C47!$3B*@C-:3%3\#%-)Q ' :9M*67-!E#% !$C #% :8;#2
' 32:*,9Rc ,8;*'4!E:.*@C-:3%N[A:1C-:9_-:Cl!*:!$7-:j$<w8; M!$#%gD#%)0g-:"D'4!E"!$*:9 :9M# 9:C
5 0:}32:9?$:3d$<.3%[89!$3d8;4#% :8;5 :#%-)Q:<a$"8;:C^R o-$f'O:9"*k7-!E#2 '" 32:*,,!$4C :-<>$"8 G
#%-)m!E"8 G 8;#% :8;5$NwA:]'`?$:Cn0!Ek!}#%)32:'"#%*+!$3(!$3%3 G C# :9:&.8; M!$#%gk#%j#2)0& :9k0!$
8"04!$-:3%3%#\-)18; M!$#%gN 7/0!ED8"04!$-:3%3%#\-)18; M!$#%g!E:K#2)0& :9V0!$+'4"#%*,!$3-${G:|&74!$3%
8; M!$#%gRrcd0-:k:!$ m<>$b04#%BC# :9:48;:6#%r0!Eb0-:6'4"#%*,!$3^${G:|&7!$3\b8;4 "!$#%&KC-:9 :8;
#%)32:9 l?!EM#%! 32: =# R :$R-0-:j?!E"#\! 32:VAB#20l!#\-)32:K?!$3\7-: MN-0-:j8"0!$4-:3%3%#%)k8; "!$#%&dC-:;G
:8;(#%)32:9 1?E!E"#%! 32: !$4Ck"#%-)32:9 6?!$3\7-: =# R :$RE0- :?!$3%7: Ar0#%8"06@89897-#%10:VC*,!$#%6$<!
#%)32:K?!E"#\! 32: MN-AB0#\3% 0-:.'"#\*,!$3O!$3%3 G C# O:9:&D8; "!$#%&dC-:9 :8;r)32 !$3^8;4#% :8;5 >AB0#\8"0
#%893\7C-:k#%-)32:9 ?E!E"#%! 3%:9N(#%-)3%:9 n?E!$3%7-:!$4C *,!$g5n$0-:9,#274!E#2 MR o-$+32 A:9,32:9?&G
:3%D$<32@89!$3^8;"#% :8;5 >:$R )-R04!EB*,!$#%&!$#%-:C 5f<a$AV!E"Ci8M0-:8L@#%-) MN 8"0!$4-:3%3%#%)H8;4 "!$#%&
:*,!$#\ #2)0& :9+0!$ '4"#%*,!$3V-${G:|[7!$3%]8; "!$#%&9R B A:9?$:9ND<>$]8;:9"!$#% 0#2)0:9+32:9?$:3%$<









r



r





%e

88







fififfv2$ff6ff )T$
fi
ff
v~ff



32@89!$3ff8;#% :8;5f3\#2L$:j'4!E0W#%&?$:9" :.8;"#% :8;5$N48M0!$-:3\3%#%-)H8; "!$#\gd!E":1#%8;*'!E"! 32:b
'"#\*,!$3O-${G:|&74!$3%D8; "!$#\g9Ro-$B#% :8;#2l'" 32:*,9N[A:.' ?$:CW0!E9N-AB#20l": 'O:8;D ,!E"8 G
8;#\ :8;5$N!}#\-)32:'"#\*,!$3(!$3%3 G C4# :9":g.8; "!$#%&6#%1#2)0g :9k0!$n8"0!$:3%3%#%-)i8;4 "!$#%&
$)$:90-:9AB#20k0-:dC7!$3@-${G:|[7!$3%8;4 "!$#%&9N 7J04!E0-:D8"0!$:3%3%#%-)K8; M!$#%g!$3%-:d!E:
!$#2)0&V!$0-:b'"#\*,!$34-${G:|[7!$3%V8; "!$#%&9R(cd0-:b!$ 5[*,' $#%8K!$!$325@#%!$3%32`A:Cl7 k":C78;:
<=7-0-:9d0:6[7* :9r$<w*,[C-:3\d0!Eb*,#2)0& :.A$"0m8;"#%C-:9"#%)-R (p['O:9"#%*,:g!$3ff":732Dx!
AB#%C:/"!$-)$:V$<' 32:*+P7-'4' $" :C10-::0:9$:9#%89!$3@:74329R o$:;p@!$*'32:$N!$CC#%) #%4!E5j-${G
:|[7!$3%.8; "!$#%&j i0-:,8M0!$-:3%3\#%-)l8; "!$#%&1C&:.$.#%8;:!$#\-)l'"7#\-)-Nff!$CZ*:9:3%5
!$CC.`?$:9"0-:!$CY i0-:,M7g#\*:9R B A:9?$:9`N 0-:+:;p['O:9"#%*,:g!$3w:"732j!$3% iC:* "!E :CY0-:
?$:95#2)4#2_489!$& :-:9_4w$< :#%)K! 32:D "!$48"0 $0'4"#%*,!$3@!$CC74!$3@?!E"#\! 32:9RJz +*,!$g5
89!$ :9NBA:m !$#%-:C 0-: : ,M7g#\*:AB#20 74 +8"04!$-:3%3%#\-)Q8; "!$#%&f!$C ! "!$8M0#%-)
0-:7-M#% #%8r0!EB32[$L$:C}!E $0i'"#%*,!$3O!$CWC47!$3O?[#%:9Ad'O#%gR



65

f

/e

5

0!E,)$:-:9M!$3B32:89!$ :i32:!E"&<>* 0#%+ 7C-5Uow#2M 9N/0-:9":}!E:}*,!$&5 ' "# 32:
*@C-:3%j$<V:9?$: !}#\*'432:H' 32:* 3%#2L$:,_C#%-)W!W'O:9"*k7-!E#2Y$6!$n#% :8;#2ffR+z{Z!$CC4#2#2ffN
-]-:6*@C-:3 #% : B#\}!$3%3 #27!E#249RVyQ:60-:9":9<a$:k-:9:Cm f7''O$D0-:674 :9r#%m*@C-:3%3%#%)
:9?$:Q#%*,'432:j' 32:*,9R [:8;CffNff#2r$<a :x'4!`5[b W8; "748;r:C47C!$&r*@C-:3%rAB#20m*67432#2'432:
?@#2:9Ad'O#%&,$<10:x!$*,:m'" 32:*WR b: '4#2 :x0-:x`?$:9"0-:!$C49Nr0:x! #\3%#2 5 M!$8"0 C7!$3
?E!E"#%! 32:89!$ :W?$:95 :-:9_489#%!$3 R "!$48"0#%)Z0-:7-"#%#%89H0!E]8;4#%C-:9,*k732#2'32:l?[#2:9AB' #\g
89!$ :x?$:95 : O:8;#2?$:$R cd0#%"CffND0-:Z!$CC4#2#2!$3b8; "!$#\gl'$'!E)!E#2 ' ?[#\C-:C 5 )32 !$3
8; M!$#%g63%#2L$:+!$3\3 G C# O:9:&.*,!5Z-$ 7#2<a5x0-:#2k8; 9RlyQ:]$<> :Y"!A :9 :9k'O:9<>$"*,!$8;:
AB0-:fA:K0-:9A 7-V0-:j!$3%3 G C# :9:&8;4 "!$#%&9R(o-7-0^N-7-D*:!$7-:b$< 8;4 "!$#%&V#2)0g:
89!$ :j7 :Cf +8;*'!E:.C# O:9:&V8;4 "!$#%&d*@C-:3%9R r`A:9?$:9N40#\*,:!$7-:.89!$l3%5+: :8;
8;:9!$#% *@C-:3% 0-: !$"#%0!E]0-:95 !$CC ?$:9"0-:!$CffR yQ:X #%3%3D*k7 ,M7 :;p@' :9M#%*:&
C-:9 :9"*+#%-:j#2<P0:.!$CC#2#24!$38; "!$#%&D'$'4!E)!E#2i'`?@#%C-:C 5+#%)0g :9b*[C:3%V#%VA$"0W0-:
8; K$</04#%r8;4 "!$#%&r'4$'4!E)!E#2ffR
32#%*,!E :3%5$N 0:k|[7-: #2 :#%-)f!$CC-::Cm#%K8;:& "!$3J
*,!$&5]'4 32:*,D#\W!E#2_489#%!$3ff#\g :3%3\#2)$:8;: 0-:j "!$C:;G
:9FA:9:m :!E"8M0}!$CW#%<a:9:48;:$R

65





)







x

5

ge

<

5

K

$C !? R4-D Rfi=0R
)bRre 4#%8"0X!$CQcjROyZ!$3%"0Q!E:,897-:&325}"7-''O$ :C 5 @89#2:8;:o-74C!E#2Xz :3%!$C @oJzub!$C
!$nz E2
v $<> AV!E:f)$"!$&9R}cjR^yZ!$3%"0YAV!$6!$3\ }7-'4' $" :C 5Z!$2fq_ !$C-?E!$8;:Cn: :!E"8M0
<>:3%32`Ar0#2'^RyQ:k0!$-Ll0-:1$0-:9r*:* :9"d$<w0-:kS_nff": :!E"8"0m)$7-'=0g ' < ff AdAdA.R C89R {G
!$CffR !$8ER 7L !E'O:I,
<a$f0-:32'<=73DC#\897#%9N!$4C : 'O:89#%!$3%3%5Yz{!$/b:&+Ar0-Z:8;7-M!E)$:C 7+


Ad"#% :j0#%V'4!E'O:9R

R R$ R R2

)V!$898"0[79NEoR2NV0-:ffN6R2N?E!$ ):9:9L Nr_JR2N ? yZ!$3%0ffN`c.RVK373$$MR~)V#%4!E5r?@9R = @G})V#%!E"5j4 "!$#%&9R
PbN*Q nF4Q uEvIJN};[ v>vwQ [4JFI[9N
d1 G $MN-1 ~$,&R
): 5 !E`N.R2N ? ~}!$&5!@NoR VK37373>MRS[32?@#%-)H0-:."7Cl #\]'4 32:* 7"#%-)H'$'O#2#%!$3O32$)#%8ER
z{ PdH8FI[`"[ sKQR
J KL HY
U NRfi
ff6u7N*QVHKJ uEv H7J:U4[4PY[4JFI[HKJ PbN*Q nF:Q=uEvIJN}[;v>vwQ[4JFI[9N(''^Rn7&7 ~7&7&[R
SbSbSBz (
_ ": cd0-:1~}z{ca(_ :"9R
):#V:9y :$NKR2N@~}: :9)7-:9NMJ_ R2No-:74C-:9Njdf R2NT? !E!@N R@d1:07070$M RnK
v ]<>$A!EMCf8M0-:8L@#%-)H<>$D-@G
!E`N-R-*/f CffRwM N PdH8FI[`"[ sKQRJ KLcHYU $qQ UbNR IJfiN}[4PIJ u7N*Q*H7J uEv
#%!E5}8;4 "!$#%&.!E#% <=!$8;#2ffR6z g!6O
8 U

fi 8-
fiffff
H7J:U4[4PY[4JFI[pHKJ PbQTJF:QiW v [OL,u7Js P u$F:N*QVFI[HYU HKJMLON*P{uKQRJfiN PdH $P{u7]m]QRJ ; NP'4'^R.7.
143$[R = S'18,~1:[R['"#%)$:9R
)$:9 9N -R2N%? cd!$-)-NfiBf R d1:07070$M RdS 8;& :;p[b<a$b8; "!$#\gB"!E#% <=!$8;#2i'" 32:* <>$"*6743%!E#2
:32:8;#2ffR H7JMLbN*P uKQRJfiNRML NN7070 ~7$&
,R
V0-:)-N~b
) R2N4V0-# NR2NM^ :9:$N -R2NT? yZ7ffN -Rd1:07070$M Rz 48;:!$#%-)k8; "!$#\gV'$'!E)!E#2 5,":C7@G
C4!$gB*,[C-:3\#%-)T<-!$W:;p@' :9M#2:8;:K:9'O$9R HKJ LbN*P uKQTJNRML N -N1:&$, 1:07[
R
V0-#N(KR2N ? ^
:9:$N-R VK373$$M R K
v 0-:W'"74#%-) :04!?@#27-H$<b*+#%#%*,!$38;* #%-:C *@C-:3%k<>$
'O:9"*k7-!E#2HSj
_ 9R$z PYH:FI[I[ sKQRJ 7LSHYU ff .ff HKP C8LI~HIWXHK
J S[VUOH7PIH
] t@v u$N*QTJ HKJ LbN*P uKQTJN
u7N*QhLR;
U u>F:N*QVHKJ PYH>;Z v\[O]mL
%H A/uKP s6L M6LON}[O,
] u7N*QT;L u7N*QVHKn
J uKJ tMNHK,
] u7N*Q*H7J R
xb: "7-5@-:$N .R2N ?%) :#V:9y :$N&bRMd1:070$,7M R@@ *:V'"!$8;#%89! 3%:_43% :9"#%-)r :8"0#\|&7-:J<>$w0-:D8; "!$#%&
"!E#% <=!$8;#2 ' 3%:*WRmz{ PYH:FI[I[ sKQR
J 7LpHYUcNRj
[ ff
7NR fi L MNJ''^R 1: > 18&, R/z{g :9"4!E#2!$3

#%&r-<>:9:8;:.iSr#2_89#%!$3Oz & :3%3%#%)$:8;:$R

b:9:32:^N~_JRd1:0707$MRExK7!$3?@#2:9Ad'O#%&/0-:7-"#\ #%89(<>$ #%!E58; "!$#%&!E#\ <>!$8;#%+'" 32:*,9Rffz{
PdH8FI[`["sKQRJK LcHYUsNRj[ NR L MNff''PRj1 ~7"[REf/7$'O:!$Y-<>:9:8;:ZSr#2_489#\!$3 z{& :3%3%# G
)$:48;:$R4y

#%32:95$R

b:&9Nz RVK373$$MR/SBM8j8;#% :48;5f#%p@Sc.RJz{l?E!$ er!E"*,:32:ffNoR*f/CffRwMN PdH8FI[`["sKQRJKLmHYU L
ff .ff N ''^R1:j1 1:7"[ROzYv c_(:R
b:&9NEzMR2N$[ :9)#27^NR2N ? yZ!$3\0ffN c.R~VK37373>MRfixb:8;*'O! 3%:8; "!$#%&9R PON*Q nF4Q uEv IJfiN}[;v>v Q ~[OJFb[;N
ff d1 G $MNg1:7 1:"7&[R
b:&9Nz R2N U
? yZ!$3%0ffNcjRBd1:07070$MR V '43%# < ! :8"04*,!EL 3%# M!E5Y<>$f8;4 "!$#%&9R cP:8M0ffR:9'^R2N

B_qf }3$0 `1:07070
ff fi fi!#"$%ff& &R
6
*f w PdH:Fb[`["sKQTJEKLmHdUH$-Q UONR IJfiN}[OPbJu7N*QVHKJuEv HKJ
U:[OPu[OJ FI[/HKJ PIQRJF4QkW \[bL 7J P $F:N*QVFI[+HdU HKJMLON*P 7QTJfiN PYH 7P uK]]QTJ' ;( N''^R >.K3 >>.j1ER
= '18,~1: @
eb!E"!$3%#%8"LNE.R2N? /f 3%3%#%$9NkR d1:07.K3>M R z 8;":!$#%-)j :9:b :!E"8M0+:9e+89#%:8;5H<a$V8; "!$#\g/"!E#% <=!$8;#2
'4 32:*,R PbN*Q nF4Q uEvIJN};[ v>vwQ [4JFI9[ N
-N 7&7 ~j1:[
R
er:g :-"5[8"LNJ_ R R2Nb
x :9?@#%3%32:$N*)kR2N ?Tc :-)-NPbREd1:0707$M RHS K:-:9"#%8kSBM8V#% :48;5iSr32)$$M#20*
!$4Ci#2[
'O:89#%!$3\#!E#29R PON*Q F4Q uEvIJfiN}9[ vav Q ~[OJFb;[ N+
N 70j1 ~7jE1 R
$7"C!$ffN -R d1:0707"$M R HKJ9F tjPbPY[4JfiN HKJ LbN*P uKQTJN t[vN*Qk
W v [ H s;[ vwLcQRJ -, uKJ ., uKEJ $tuV[OL

H A/uKP sQu PYH 7P uK]]QTJ [:NR~H s$EH v H =Z H s>9[ vwQRJ R+/_ 0ffRik
x Rff0-:#\9Nr
x :4#%K
x #%C-:9$ # G
?$:9M#2 5$N w
_ !E"#% zzMR(Sr?E!$#%3%! 32:j!$KD~ G S& G 0j1 G`1:K@3 R
~m#%3%32:9N -R
VK373$$M R
!$-)$<>$"C '" 32:*WR2R
P
v 3%#%: C-:8;"#%'#2 !`?!$#\3%! 32: !E
K
c :8"04#%89!$3w:9'O$jS
[G G
[R+SB?E!$#%3%! 32:6<>*
0$ :9d?$:9"#%i!E''O:!E"d#% g! O!E`N -R ff
C R MN
v ku
u
{u


[R ['4"#%-)$:9R

ff0/1//2
fi$3 ff"44%



156178
R






_ nd1:0707$ Xv
PdH8FI[`[ KQRJKLcHYU+NRj[ NR

=

~}$"#%9N JR
MR KZ0-:+C-:"#2 5m$<V3%7-#2j#\Q:|&74#%3%# M#%7*
3%:*WRHz
"s
ff6u uEv
Sb @89#%!E#2i<a$BSr#2_89#%!$3Oz & :3%3%#%)$:8;:$R

7N*Q*H7J

) nd1:070K3> H

9 :;<OWM[OPON =




'O#%&K<>$10-:,|[7-:9:1' G
MNff''^R
[RwSr*:9M#%89!$

HKJ8U4[4PY[4JFI[HKJ

<

@>7. >>7

!$C-:3 N bR
MR B:9': :&!E#2n :32:8;#2Q<>$18; "!$#%&.!E#% <=!$8;#2 PST89!$ :+7C-5m74#%-)
-G K7-:9:9R
=N EN
[R

1:& ~7
_(" :9N>_JR2N [ :9)#27^NR2NE? yZ!$3%0ffN[cjRVK37373>MRg@#%-)32:9 +8;#% :489#2:9ROz lxb:8"0& :9NT.R*fCffRwMN
7NR IJfiN}[4PIJ u7N*Q*H7JuEv HKJ8U4[4PY[4JFI[HKJ PIQRJF:QiW v\[bjL uKJ P u$F:N*QVFI[OLHYU H7JMLbN*P uKQRJfiN PdH ${P u7]m]QRJ
. R = 1:.706- R@[ '4"#%-)$:9 G :9M3%!E)-R
ff N4'4'^Rfi7"7 ~7&7[
U#c

fififfv2$ff6ff )T$
fi
ff
v~ff



:9 )#%^N R~d1:0706MR-S _432 :9"#\-)d!$32)$$"#204* <>$J8; "!$#%&P$<C# :9:48;:/#%k j_/9Rgz PYH:FI[I[sKQRJKL HdU
NRM[_ ff NR ff6u7N*Q*H7JuEv HKJ:U:[OPu[OJ FI[HKJ MNg''^Rj7&7 ~7&$,&R4Sb*:9"#%89!$fSr" [89#\!E#2+<>$DSB"#2_489#%!$3
z{& :3%3%#2)$:8;:$R

:9 )#%^N

?

-VK37373>
cx
*f w PYH:FI[I[ KQRJ7LHdU KNR IJfiN}[OPbJ 7N*QVHKJ H7J:U4[4PY[4JFI[
H7J PIQRJF:QiW \[bL 7J P $F:N*QVFI[HYU HKMJ LON*P KQRJfiN dP H $P 7]m]QRJ ff $7.6 ~707" =
1:.706
@8M0&73% :$NbR2N ?%[ 78L$:95$NKJ_ RVK373M18M R[y 0-:6C- 7C4ff!$C1C-*+!$#%j'$'!E)!E#2132:!$C. B0-:!$*:
:!E"8"0m '4!$8;:$Rz{X[
C-:9)!$!EMCffN 1
e R *f CffRwM N PYH:Fb[`[ s7QTJ 7LmHdU {P IJfiN}[4PIJ u7N*Q*H7J uEv H7J:U4[4PY[4JFI[
H7J PIQRJF4QkW v\[b+
L uKJ {P u$F8N*Q*Fb[cHdU
[u9F v2uKP u7N*Q K[ PYH ${P uK]]QRJ
ff (NP''PRE171:" 1:7[& R
SBd~(/
_ :9R
@*,#%0ffNb
) R2Nq[ :9)#27ffNR2N,? yZ!$3%"0ffNJc.RVK37373>M R #%-)x!$7@p@#\3%#%!E5x?!EM#%! 32:k!$C #%*'43%#%:CQ8;@G
"!$#%&P K*@C-:3g--G #%!E5b' 32:*,9Rz{ PdH8FI[`"[ sKQR
J KL HdU NRj[ KNR ff1u7N*QVHKJ u$v H7J:U4[4PY[4JFI[
H7J MN''PR@1:.7 1:.$&, RffSb*:9"#%89!$}Sr[89#%!E#%W<a$rSB#%_489#%!$3 z & :3%3%#%)$:8;:$R
@*,#%0ffN b
) R VK37373>M RB~i@C-:3%3\#%-)!sJ_ :9"*k7-!E#2!(_ 3%:*WR/z{ PYH:Fb[`[ s7QTJ 7LHdU L ff HKP Cr
LI~HIW HKJ H s>;[ v>v QTJ fuKJ EH v 8QRJ PdH;Z v\[O]mL AgQ*NR H7JMLbN*P uKQRJfiNRML RSb3% 6!`?!$#\3%! 32:b!$ B: :!E"8M0
B:9'O$D<a"* 0g ' < ff8;*WR 0[7CffR !$8ER 7-L !6 8;* *, '4!E'O:9"9R 0&*,3 R
[ :9)#%7ffN=R2N,?hyZ!$3%0ffNwcjR d1:07070$M RZcd0-:]C4# :9":8;:]!$3%32G C#O :9:8;:]*+!EL$:9Rmz{ PYH:FI[I[ sKQRJ KLXHdU
1KNR fi L MN-''^RM 1O > 1:[0 ROz{g :9M!E#2!$3 #%grV-<a:9":8;:.iSr#2_489#\!$3Oz & :3%3%#2)$:48;:$R
yZ!$3%0^NcjRVK37373>M R
@
Sch? SjB_ R z #r
x :8M0g :9`N 1R*f CffRwM N 7NR IJfiN}[OPbJ u7N*QVHKJ u$v HKJ8U4[4PY[4JFI[ H7J
PIQRJF:QiW v\[bHL uKJ {P u$F8N*Q*Fb[bLmHYU HKJMLON*{P u7QTJfiN PYH 7P uK]]QTJ ff N''PR 7 1 >>"7[& R =
1:.706- R[ 'M#%-)$:9 G :9"3\!E)-R
R2N
B7-:0-:9`NO~YR
MRkS )32 $
! 3w8; "!$#\gj8;* #%#%)]!l7* 8; M!$#%gj!$CXC#%<\G
<>:9:8;:K8; M!$#%gRffz b:8"0& :9N .R /CffR MN

u
uEv
v ru
u
{u
{u
[R



N&'^' R
-R ['M#%-)$:9R



U!

fiJournal Artificial Intelligence Research 21 (2004) 499-550

Submitted 08/03; published 04/04

Compositional Model Repositories via Dynamic Constraint
Satisfaction Order-of-Magnitude Preferences
Jeroen Keppens
Qiang Shen

JEROEN @ INF. ED . AC . UK
QIANGS @ INF. ED . AC . UK

School Informatics, University Edinburgh
Appleton Tower, Crichton Street, Edinburgh EH8 9LE, UK

Abstract
predominant knowledge-based approach automated model construction, compositional
modelling, employs set models particular functional components. inference mechanism
takes scenario describing constituent interacting components system translates
useful mathematical model. paper presents novel compositional modelling approach aimed
building model repositories. furthers field two respects. Firstly, expands application domain compositional modelling systems easily described terms
interacting functional components, ecological systems. Secondly, enables incorporation user preferences model selection process. features achieved casting
compositional modelling problem activity-based dynamic preference constraint satisfaction
problem, dynamic constraints describe restrictions imposed composition
partial models preferences correspond user automated modeller.
addition, preference levels represented use symbolic values differ
orders magnitude.

1. Introduction
Mathematical models form important aid understanding complex systems. also help
problem solvers capture reason essential features dynamics systems.
Constructing mathematical models easy task, however, many disciplines contributed approaches automate it. Compositional modelling (Falkenhainer & Forbus, 1991; Keppens & Shen, 2001b) important class approaches automated model construction. uses
predominantly knowledge-based techniques translate high level scenario mathematical
model. knowledge base usually consists generic fragments models provide one
possible mathematical representation process occurs one components.
inference mechanisms instantiate knowledge base, search appropriate selection
model fragments, compose mathematical model. Compositional modelling
successfully applied variety application domains ranging simple physics, various
engineering problems biological systems.
present work aims compositional modelling approach building model repositories
ecological systems. ecological modelling literature, range models devised
formally characterise various phenomena occur ecological systems. example,
logistic growth (Verhulst, 1838) Holling predation (Holling, 1959) models describe
changes size population. former expresses changes due births deaths
latter changes due one population feeding another. compositional model repository aims
c
2004
AI Access Foundation. rights reserved.

fiK EPPENS & HEN

make (partial) models generally usable providing mechanism instantiate
compose larger models complex systems involving many interacting phenomena.
Thus, input compositional model repository scenario describing configuration
system modelled. sample scenario may include number populations various
predation competition relations them. output mathematical model, called
scenario model, representing behaviour system specified given scenario. set
differential equations describing changes population sizes aforementioned scenario
due births, natural deaths, deaths predation, available food supply competition
would constitute scenario model.
application domain poses three important new challenges compositional modelling.
Firstly, processes components ecological system represented
resulting composed model depend one another ways described. population dynamics example, models describing predation competition phenomena
two populations rely existence population growth model populations
involved phenomenon. inhibits conventional approach searching consistent adequate combination partial models, one component scenario.
approach provides adequate solution physical systems comprised components implementing particular functionality described one multiple partial
models. Although seminal work compositional modelling (Falkenhainer & Forbus, 1991)
recognised existence complex interdependencies model construction general,
provided partial solution it: conditions certain modelling choices
relevant specified manually knowledge base.
Secondly, domain ecology lacks complete theory constitutes adequate model.
existing compositional modellers based predefined concept model adequacy.
employ inference mechanisms guaranteed find model meets adequacy criteria.
However, criteria determine adequate ecological model may vary ecological
domains even ecologists require model within domain. Therefore,
compositional modeller requires facility define properties generated ecological
models must satisfy.
Thirdly, possible express criteria imposed scenario model terms
hard requirements. Often, ecological models describe mechanisms behaviours partially understood. cases, choice one model another becomes matter expert
opinion rather pure theory. Therefore, ecological domain, modelling approaches presumptions are, extent, selected based preferences. Existing compositional modellers
equipped deal user preferences paper presents first compositional
modeller incorporates them.
Generally speaking, three issues tackled paper means method
translate compositional modelling problem activity-based dynamic preference constraint
satisfaction problem (aDPCSP) (Keppens & Shen, 2002). aDPCSP integrates concept
activity-based dynamic constraint satisfaction problem (aDCSP) (Miguel & Shen, 1999; Mittal &
Falkenhainer, 1990) order-of-magnitude preferences (Keppens & Shen, 2002).
attributes domains aDPCSP correspond model design decisions, constraints describing restrictions imposed consistency requirements properties order-of-magnitude
preferences describing users preferences modelling choices. translation method brings
additional advantage compositional modelling problems solved means
500

fiC OMPOSITIONAL ODEL R EPOSITORIES

efficient aDCSP techniques. such, compositional modellers benefit recent future
advances constraint satisfaction research.
remainder paper organised follows. Section 2 introduces concept
aDPCSP, preference calculus suitable express subjective user preferences model
design decisions integrated general framework aDPCSPs. also gives
solution algorithm aDPCSPs. Next, section 3 presents compositional model repository
shows aDPCSP employed automated model construction. theoretical
ideas illustrated means large example section 4, applying compositional
model repository population dynamics problems. Section 5 concludes paper summary
outline research.

2. Dynamic Constraint Satisfaction Order-of-Magnitude Preferences
section, preference calculus based order-of-magnitude reasoning introduced integrated activity-based dynamic constraint satisfaction problem (aDCSP) form aDCSP
order-of-magnitude preferences (aDPCSP). Then, solution algorithm aDPCSPs
presented. theory illustrated examples compositional modelling domain.
2.1 Background: Activity-based dynamic preference constraint satisfaction
hard constraint satisfaction problem (CSP) tuple hX, D, Ci,
X = {x1 , . . . , xn } vector n attributes,
= {Dx1 , . . . , Dxn } vector containing exactly one domain attribute X.
domain Dx set values {di1 , . . . , dini } may assigned attribute
corresponding domain.
C set compatibility constraints. compatibility constraint c {xi ,...,xj } C defines
relation subset domains Dxi , ..., Dxj , hence c{xi ,...,xj } Dxi . . . Dxj .
solution hard constraint satisfaction problem tuple hx 1 : dx1 , . . . , xn : dxn

attribute assigned value domain: xi X, dxi Dxi ,
compatibility constraints satisfied: x{xi ,...,xj } C, hdxi , . . . , dxj c{xi ,...,xj } .
activity-based dynamic CSP (aDCSP), originally proposed Mittal Falkenhainer
(1990), extends conventional CSPs notion activity attributes. aDCSP,
attributes necessarily assigned solution, active ones. such, attribute
either active assigned value inactive:

xi X, dxi Dxi , xi : dxi active(xi )

activity attributes aDCSP governed activity constraints enforce
assignments attributes, assignment another attribute relevant possible. information
important dictates attributes value must searched, also
set compatibility constraints must satisfied. Clearly, compatibility constraints
501

fiK EPPENS & HEN

c{xi ,...,xj } C attributes xi , . . . , xj active must satisfied, hard CSP
sub-type aDCSP attributes always active.
summary, activity-based dynamic constraint satisfaction problem (aDCSP) tuple
hX, D, C, Ai,
hX, D, Ci hard CSP,
set activity constraints. activity constraint restricts sets attribute-value
assignments attribute active inactive:
axi ,{xj ,...,xk } Dxj . . . Dxk {active(xi ), active(xi )}
xi 6 {xj , . . . , xk }.
solution activity-based dynamic constraint satisfaction problem tuple hx 1 :
dx1 , . . . , xl : dxl
attributes part solution assigned value domain: x
{x1 , . . . , xl }, dxi Dxi ,
activity constraints satisfied:





axi ,{xj ,...,xk } A, xj 6 {x1 , . . . , xl } . . . xk 6 {x1 , . . . , xl }

xi {x1 , . . . , xl } hdxj , . . . , dxk , active(xi )i axi ,{xj ,...,xk }

xi 6 {x1 , . . . , xl } hdxj , . . . , dxk , active(xi )i axi ,{xj ,...,xk }

compatibility constraints satisfied:
c{xi ,...,xj } C, active(xi ) . . . active(xj ) hdxi , . . . , dxj c{xi ,...,xj }
2.2 Order-of-magnitude preferences (OMPs)
Although aDCSP capture hard constraints decisions given problem well
dynamically changing solution space (as described activity constraints), representation scheme employs take account preferences users may possible
alternative value assignments. Therefore, work extended allow preference information
attached attribute-value assignments. way achieved depends
representation reasoning mechanisms underlying preference calculus. general, preference calculus defined tuple h , , 4i where:


set preferences,

commutative, associative operator closed ,
4 forms partial order, is, reflexive, anti-symmetric transitive relation defined
.
4 reflexive, antisymmetric transitive, comparing preferences 4 relation
yields one four possible results:
502

fiC OMPOSITIONAL ODEL R EPOSITORIES

Two preferences P1 , P2
P2 4 P 1 .

equal one another (denoted P1 = P2 ) iff P1 4 P2

preference P1
strictly greater preference P2
P1 64 P2 P2 4 P1 .

(denoted P1 P2 ) iff

preference P1
strictly smaller preference P2
P1 4 P2 P2 64 P1 .

(denoted P1 P2 ) iff

Two preferences P1 , P2
P2 64 P1 .

incomparable one another (denoted P1 ?P2 ) iff P1 64 P2

Thus, activity-based dynamic preference constraint satisfaction problem (aDPCSP) tuple
hX, D, C, A, h , , 4i, P
hX, D, C, Ai aDCSP,
h , , 4i preference calculus,
P mapping Dx1 . . . Dxn 7
preferences.

individual attribute-value assignments

preferences attached attribute-value assignments express relative desirability
assignments. aim aDPCSP find solution highest combined preference.
is, given aDPCSP hX, D, C, A, h , , 4i, P i, solution hxi : dxi , . . . , xj : dxj
aDCSP hX, D, C, Ai solution hxk : dxk , . . . , xl : dxl hX, D, C, Ai exists
P (xi : dxi ) . . . P (xj : dxj ) P (xk : dxk ) . . . P (xl : dxl ) solution aDPCSP.
section, preference calculus introduced extend aDCSP aDPCSP.
calculus illustrated examples compositional modelling domain.
2.2.1 R EPRESENTATION



OMP

Technically, OMPs combinations so-called basic preference quantities (BPQs),
primitive units preference utility valuation associated possible design decisions.
often difficult evaluate BPQs numerically, ordered relative one another employing similar ordering relations employed relative order-of-magnitude calculi (Dague,
1993a, 1993b).
Let set BPQs respect particular decision problem. BPQs
ordered respect one another two levels granularity, two relations <. First,
partitioned orders magnitude, ordered . Then, BPQs within order
magnitude ordered <. Formally, order-of-magnitude ordering BPQs tuple
hO, i, = {O1 , . . . , Oq } partition irreflexive transitive binary
relation O. subset BPQs said order magnitude . Similarly,
within-magnitude ordering set BPQs tuple hO, <i, order magnitude
< irreflexive transitive binary relation O.
illustrate ideas, consider problem constructing ecological model describing
scenario containing number populations. Let populations parasites others
hosts parasites. Also, assume certain populations compete others scarce
resources. order construct scenario model, compositional modeller must make number
503

fiK EPPENS & HEN

b15 : Lotka-Volterra
predation model

b13 : Holling
predation model

<

<
b11 : Rogers
host-parasitoid model

<
b14 : Thomsons
host-parasitoid model

b12 : Nicholson-Baileys
host-parasitoid model

<

O1 (host-parasitoid phenomenon)



b22 : exponential
growth model

<

<
b21 : logistic
growth model


b23 :
growth model

<

b31 : competition
phenomenon

O3: (competition phenomenon)

O2 (population growth phenomena)

Figure 1: Sample space BPQs
model design decisions: population growth, host-parasitoid competition phenomena
relevant, types model best describe phenomena.
Figure 1 shows sample space BPQs correspond selection types model.
sake illustration, presumption made quality scenario model depends
inclusion types model, rather inclusion exclusion phenomena. Apart
b23 b31 , BPQs correspond standard textbook ecological models 1 . BPQ b23 stands
use population growth model implicit another population growth model (the LotkaVolterra model, instance, implicitly includes concept growth). Finally, BPQ b 31
preference associated competition model (say, one included knowledge base).
9 BPQs sample space partitioned 3 orders magnitude. relation
orders orders magnitude: O2 O1 O2 O3 . binary < relation orders individual BPQs within order magnitude. BPQ ordering within 1 , instance, Rogers
host-parasitoid model (b11 ) preferred Nicholson Bailey (b12 ) Holling
predation model (b13 ). latter two models compared one another,
preferred Lotka-Volterra model. Furthermore, Thompsons host-parasitoid model
less preferred Nicholson Bailey, compared Lotka-Volterra
Holling models.
2.2.2 C OMBINATIONS



OMP

definition, OMPs combinations BPQs. implicit value OMP p equals combination b1 . . . bn constituent BPQs b1 , . . . , bn . property allows OMPs defined
functions OMP P = b1 . . . bn function fP : 7 : b fP (b)
1. precise, BPQs b11 , b12 , b13 , b14 , b15 , b21 b22 respectively correspond inclusion Rogers
host-parasitoid model (1972), host-parasitoid model Nicholson Bailey (1935), Hollings predation model
(1959), Thompsons host-parasitoid model (1929), predation model Lotka Volterra (1925, 1926), logistic
population growth model (Verhulst, 1838) exponential population growth model (Malthus, 1798).

504

fiC OMPOSITIONAL ODEL R EPOSITORIES

set BPQs, set natural numbers fP (b) equals number occurrences b
b1 , . . . , bn .
example, let Pmodel denote OMP associated scenario model contains three
logistic population growth models (b21 ), two Holling predation model (b13 ) one competition
model (b31 ). Therefore,
Pmodel = b21 b21 b21 b13 b13 b31
hence:



3



2
fPmodel (b) =

1



0

b = b21
b = b13
b = b31
otherwise

describing OMPs functions, concept combinations OMPs becomes clear.
two OMPs P1 P2 , combined preference P1 P2 defined as:
fP1 P2 :

7

: b fP1 P2 (b) = fP1 (b) + fP2 (b)

Note combination operator assumed commutative, associative strictly monotonic (P P P ). latter assumption made better reflect ideas underpinning conventional utility calculi (Binger & Hoffman, 1998).
2.2.3 PARTIAL ORDERING OMP
Based combinations OMPs, partial order 4 OMPs computed exploiting constituent BPQs OMPs considered. partial order implies comparison
pair OMPs either returns equal preference (=), smaller preference (), greater preference
() incomparable preference (?). calculus developed assuming following:
Prioritisation: combination BPQs never order magnitude greater constituent BPQs. is, given set BPQs belonging order magnitude
{b1 , b2 , . . . , bn } O1 BPQ b O2 belonging higher order magnitude, i.e.
O1 O2 ,
b1 b2 . . . bn b
respect ongoing example, means BPQ taken order magnitude O1 preferred combination BPQs taken O2 . words, choice
model describe host-parasitoid phenomenon considered important
choice population growth model (see Figure 1).
Prioritisation also means distinctions higher orders magnitude considered
significant lower orders magnitude. Consider number BPQs
b1 , . . . , bm1 , bm , . . . , bn taken one order magnitude O1 pair BPQs {b, b0 }
taken order magnitude higher O1 . b < b0 , (irrespective
ordering BPQs taken O1 )
b1 . . . bm1 b bm . . . bn b0
505

fiK EPPENS & HEN

Strict monotonicity: Even though distinctions higher orders magnitude significant, distinctions lower orders magnitude negligible. is, given OMP
P two BPQs b1 b2 taken order magnitude b1 < b2 ,
(irrespective orders magnitude BPQs constitute P )
b1 P b2 P
instance, preference ordering depicted Figure 1 shows scenario model
Rogers host-parasitoid model two logistic predation models preferred one
Rogers host-parasitoid model two exponential predation models:
b11 b22 b22 b11 b21 b21
Note departure conventional order-of-magnitude reasoning. OMPs
associated two (partial) outcomes contain equal BPQs higher order magnitude,
usually desirable compare solutions terms (less important) constituent
BPQs lower orders magnitude, example illustrated. However, conventional orderof-magnitude reasoning techniques handle this.
Partial ordering maintenance: Conventional order-of-magnitude reasoning motivated
need abstract descriptions real-world behaviour, whereas OMP calculus motivated incomplete knowledge decision making. opposed conventional orderof-magnitude reasoning real numbers, OMPs necessarily totally ordered.
implies that, user states, example, b1 < b2 < b b3 < b4 < b,
explicit absence ordering information BPQs {b 1 , b2 } {b3 , b4 }
means user unable compare (e.g. entirely different things).
Consequently, b1 b2 would deemed incomparable b3 b4 (i.e. b1 b2 ?b3 b4 ), rather
roughly equivalent.
above, derived given two OMPs P1 P2 order magnitude O,
P1 less equally preferred P2 respect order magnitude (denoted P1 4O P2 )
provided
bi O, fP1 (bi ) +

X

bj O,bi <bj


fP1 (bj ) fP2 (bi ) +

X

bj O,bi <bj

fP2 (bj )



Thus, comparing two OMPs within order magnitude yield four possible results:
P1 less preferred P2 respect (P1 P2 ) iff (P1 4O P2 ) (P2 4 P1 ),
P1 preferred P2 respect (P1 P2 ) iff (P1 4O P2 ) (P2 4 P1 ),
P1 equally preferred P2 respect (P1 =O P2 ) iff (P1 4O P2 ) (P2 4 P1 ),

P1 incomparable P2 respect (P1 ?O P2 ) iff (P1 4O P2 ) (P2 4 P1 ).
506

fiC OMPOSITIONAL ODEL R EPOSITORIES

ongoing example Figure 1, instance, preference scenario model
Rogers host-parasitoid model Holling predation model P 1 = b11 b13 preference
scenario model Rogers host-parasitoid model Lotka-Volterra predation model
P2 = b11 b15 . latter model less equally preferred former within
host-parasitoid order magnitude (O1 ), i.e. P2 4O1 P1 ,
fP2 (b11 ) = 1 1 = fP1 (b11 ),
fP2 (b11 ) fP2 (b12 ) = 1 1 = fP1 (b11 ) fP1 (b12 ),
fP2 (b11 ) fP2 (b13 ) = 1 2 = fP1 (b11 ) fP1 (b13 ),
fP2 (b11 ) fP2 (b12 ) fP2 (b14 ) = 1 1 = fP1 (b11 ) fP1 (b12 ) fP1 (b14 ),
fP2 (b11 ) fP2 (b12 ) fP2 (b13 ) fP2 (b14 ) = 2 2 = fP1 (b11 ) fP1 (b12 ) fP1 (b13 ) fP1 (b14 ).
Similarly, established reverse, i.e. P1 4O1 P2 , true. Therefore, latter
scenario model less preferred former within O1 , i.e. P2 O1 P1 .
result generalised given two OMPs P 1 P2 , P1 less
equally preferred P2 (denoted P1 4 P2 )
Oi O, (P1 4Oi P2 ) (Oj O, Oi Oj P1 Oj P2 )
generally, relations , , = ? derived manner
relation 4 , , =O ?O 4O .
illustrate utility orderings, consider scenario one predator population
feeds two prey populations two prey populations compete scarce resources.
following two plausible scenario models scenario:
Model 1 contains two Holling predation models three logistic population growth models,
preference P1 = b13 b13 b21 b21 b21 .
Model 2 contains one competition model, two Holling predation models, two logistic population growth models exponential population growth model, preference
P2 = b13 b13 b21 b21 b22 b31 .
demonstrated earlier, shown P1 =O1 P2 , P1 O2 P2 , P1 O3 P2 .
relations follows P1 4 P2
O1 : P1 4O1 P2 since P1 =O1 P2 ,
O2 : exists order magnitude O3 O3 O2 P1 O3 P2 ,
O3 : P1 4O3 P2 since P1 O3 P2 .
reverse true, concluded scenario model 2 preferred scenario model
1.
2.3 Solving aDPCSPs
section presents basic algorithm solving aDPCSPs. Although OMPs used
work, algorithm take aDPCSP provided employs preference calculus
507

fiK EPPENS & HEN

commutative, associative monotonic combination operator. However, use OMPs provides
convenient way specifying incomplete preference information.
aDPCSP similar valued CSPs presented Schiex, Fargier Verfaillie (1995)
also semiring based CSPs (Bistarelli, Montanari, & Rossi, 1997). However, extends
approaches activity constraints involves different underlying presumptions valuation
structure. preference valuations work allowed ordered partially, opposed
valued CSPs.
aDPCSP represents important type constraint satisfaction optimisation problem (Tsang,
1993). order tackle optimisation preferences A* type algorithm employed (Hart,
Nilsson, & Raphael, 1968; Raphael, 1990). A* algorithms known efficient terms
total number nodes explored effort find optimal solutions, given amount
information. downside, exponential space complexity. Naturally, number
alternative approaches could explored, including conventional constraint-based solving
methods depth first branch bound search. However, use A*-like algorithm
sufficient solving aDPCSPs domain present interest. particular, algorithm 1
implements A* search strategy capable handling activity constraints, involves
use basic CSP techniques constraint propagation backtracking.
A* algorithm maintains explored attribute-value assignments means priority
queue Q nodes. node n Q corresponds set attribute-value assignments: solution(n).
search proceeds number iterations. iteration, node n taken Q,
replaced nodes extend solution(n) additional attribute-value assignment.
specifically, node n Q, set Xu (n) remaining active unassigned attributes
maintained. iteration, possible assignments first attribute x X u (n),
n node taken Q current iteration, processed. every assignment x :
consistent solution(n) (i.e. solution(n) {x : d}, C 0 ), new child node n 0 ,
solution(n0 ) = solution(n) {x : d} Xu (n0 ) = Xu (n) {x}, created added Q.
activity constraints processed via propagation rather constraint satisfaction. Whenever node n taken Q Xu (n) empty, activity constraints fired order
obtain new set active unassigned attributes. is, X u (n) assigned
{xi | solution(n), ` active(xi )} Xa (n)
Xa (n) represents active, already assigned attributes node n.
priority queue Q, nodes maintained means two heuristics: committed preference
CP (n) potential preference P P (n). Here, given node n,
CP (n) = x:dsolution(n) P (x : d)
P P (n) = CP (n) (xXnd (n) max P (x : d))
dDx

Xnd (n) set unassigned attributes still activated given partial assignment solution(n) (as indicated previously, actual implementation employs assumption-based
truth maintenance system (de Kleer, 1986) efficiently determine attributes activity
longer supported). words, CP (n) preference associated partial attributevalue assignment node n P P (n) CP (n) combined highest possible preference
assignments taken values domains attributes X nd (n). Thus, P P (n)
508

fiC OMPOSITIONAL ODEL R EPOSITORIES

Algorithm 1:

SOLVE(X, D, C, A, P )

n new node;
solution(n) {};
Xu (n) {xi | {}, ` active(xi )};
Xa (n) {};
CP (n) 0;
P P (n) xX maxdD(x) P (x : d);
Q createOrderedQueue();
enqueue(Q, n, P P (n), CP (n)); Q 6=

n dequeue(Q);





Xu (n)

6=




x first(Xu (n));




PROCESS(x, n, C, A, P, Q);


Xu (n) {xi | solution(n), ` active(xi )} Xa (n);







Xu (n) =






nnext first(Q);












CP (n) P P (nfirst)





return (solution(n));
else













else P P (n) CP (n);







enqueue(Q, n, P P (n), CP (n));










x first(Xu (n));



else
PROCESS(x, n, C, A, P, Q);
procedure PROCESS(x, nparent , C, A, P, Q)

D(x)
solution(n


parent ) {x : d}, C 0


nchild new node;





solution(n ) solution(n



child
parent ) {x : d};




X deactivated(solution(n ), X(n




child
parent ));






Xnd (nchild ) Xnd (nparent ) {x} Xd ;

Xa (nchild ) Xa (nparent ) {x};






Xu (nchild ) Xu (nparent ) {x};









CP (nchild ) CP (nparent ) P (x : d);








P P (nchild ) CP (nchild ) xXnd (n) maxdD(x) P (x : d);




enqueue(Q, nchild , P P (nchild ), CP (nchild ));

computes upper boundary preference aDPCSP solution includes partial
attribute-value assignments corresponding n.
following theorem shows algorithm 1 guaranteed find set attribute-value
pairs highest combined preferences, within set solutions satisfy constraints.
Theorem 1 SOLVE(X, D, C, A, P ) admissible
Proof: SOLVE(X, D, C, A, P ) A* algorithm guided heuristic function P P (n) = CP (n)
h(n), CP (n) actual preference node n h(n) = xXnd (n) maxdDx P (x : d).
follows previous discussion h(n) greater equal combined preference
value-assignment unassigned attributes consistent partial solution n.
algorithm, nodes n maintained priority queue descending order P P (n). Let
distance function reverses preference ordering (P 1 ) (P2 ) P1 P2 .
SOLVE (X, D, C, A, P ) described A* algorithm, nodes n priority
509

fiK EPPENS & HEN

queue Q ordered ascending order (P P (n)), (P P (n)) = (CP (n)) (h(n))
(h(n)) lower bound distance n optimal solution. Therefore, following work Hart, Nilsson Raphael (1968), SOLVE(X, D, C, A, P ) admissible
algorithm, guaranteed find solution minimal (P (S)) maximal P (S).
illustrate algorithm 1, consider problem finding ecological model describes
behaviour two populations, one predates other. aDPCSP constructed
compositional modelling problem following attributes domains. Note section
3 demonstrates attributes, domains constraints problem constructed
automatically section 4 illustrates ideas context larger example.

X = {x1 , x2 , x3 , x4 , x5 , x6 }
Dx1 = {yes, no}
Dx2 = {yes, no}
Dx3 = {yes, no}
Dx4 = {other, logistic}
Dx5 = {other, logistic}
Dx6 = {Holling, Lotka-Volterra}
attributes x1 , x2 x3 respectvely describe relevance following phenomena:
change size predator population, change size prey population
predation prey predator. attributes x4 x5 represent choice type
population growth model. Two types models incorporated problem: logistic
one other. Finally, attribute x6 associated choice model type predation
phenomenon. Here, two types model, Holling model Lotka-Volterra model,
included.
Holling predation model presumes logistic models employed describe
population growth, Lotka-Volterra Model incorporates population growth
model, combinations assignments x4 , x5 , x6 restricted. Hence, aDPCSP
contains set C = {c{x4 ,x6 } , c{x5 ,x6 } } compatibility constraints, with:
c{x4 ,x6 } = {hx4 : other, x6 : Lotka-Volterrai, hx4 : logistic, x6 : Hollingi}
c{x5 ,x6 } = {hx5 : other, x6 : Lotka-Volterrai, hx5 : logistic, x6 : Hollingi}
Furthermore, model type predator/prey growth must selected corresponding population growth phenomenon deemed relevant. Also, model type predation must selected population growth phenomena predation phenomenon deemed relevant (because ecological models describing predation rely submodels
describing population growth predator prey). Hence, aDPCSP contains set
= {ax4 ,{x1 } , ax5 ,{x2 } , ax6 ,{x1 ,x2 ,x3 } } activity constraints, with:
510

fiC OMPOSITIONAL ODEL R EPOSITORIES

ax4 ,{x1 } = {hx1 : yes, active(x4 )i, hx1 : no, active(x4 )i}
ax5 ,{x2 } = {hx2 : yes, active(x5 )i, hx2 : no, active(x5 )i}
ax6 ,{x1 ,x2 ,x3 } = {hx1 : yes, x2 : yes, x3 : yes, active(x4 )i, hx1 : yes, x2 : yes, x3 : no, active(x4 )i,
hx1 : yes, x2 : no, x3 : yes, active(x4 )i, hx1 : yes, x2 : no, x3 : no, active(x4 )i,
hx1 : no, x2 : yes, x3 : yes, active(x4 )i, hx1 : no, x2 : yes, x3 : no, active(x4 )i,
hx1 : no, x2 : no, x3 : yes, active(x4 )i, hx1 : no, x2 : no, x3 : no, active(x4 )i}
Finally, let preference calculus consist two orders magnitude growth Opredation ,
Ogrowth Opredation ,
Ogrowth ={pother , plogistic } plogistic < pother
Opredation ={pHolling , pLotka-Volterra } pLotka-Volterra < pHolling
OMP assignments follows:
P (x4 : other) = P (x5 : other) =pother
P (x4 : logistic) = P (x5 : logistic) =plogistic
P (x6 : Holling) =pHolling
P (x6 : Lotka-Volterra) =pLotka-Volterra
applied problem, algorithm 1 initialises search creating node n 0 , where:
Xu (n0 ), set currently active attributes, initialised {x1 , x2 , x3 }, activity
attributes depend attribute-value assignments.
Xa (n0 ) CP (n0 ) initialised empty set 0 respectively, since attributes
assigned yet.
Finally, P P (n0 ) equals pother pother pHolling combination highest
OMPs associated domain.
initial node enqueued Q. Next, algorithm proceeds number iterations.
iteration, node potential (as measured P P CP ) dequeued,
children generated enqueued Q. nodes created way depicted
Figure 2. number subscript node ni indicates order node generation,
thick arrows show order search space explored.
Note three important features algorithm could clearly demonstrated within Figure 2. Firstly, node n5 , initial set unassigned attributes exhausted:
Xu (n5 ) = {}. Therefore, activity constraints fired n 5 explored. n5 corresponds assignment {x1 : yes, x2 : yes, x3 : yes}, remaining attributes activated
Xu (n5 ) reset {x4 , x5 , x6 }.
Secondly, node n12 corresponds assignment (active) attributes consistent
activity compatibility constraints:
{x1 : yes, x2 : yes, x3 : yes, x4 : other, x5 : other, x6 : Lotka-Volterra}
511

fix1
yes

n1

P P = pother pother pHolling
CP = 0



n2
P P = pother
CP = 0

x2
yes

n3

n4

P P = pother pother pHolling
CP = 0



P P = pother
CP = 0

x3
n5

yes

P P = pother pother
CP = 0

512

P P = pother pother pHolling
CP = 0

K EPPENS & HEN



n6

x4


n7

P P = pother pother pHolling
CP = pother

n8

logistic

P P = plogistic pother pHolling
CP = plogistic

x5
n9



x5

n10

P P = pother pother pHolling
CP = pother pother

logistic

P P = pother plogistic pHolling
CP = pother plogistic

x6
n11

n12

Holling
inconsistent

P P = plogistic pother pHolling
CP = plogistic pother

x6
Lotka-Volterra

P P = pother pother pLotka-Volterra
CP = pother pother pLotka-Volterra

n13

n16

logistic

P P = plogistic plogistic pHolling
CP = plogistic plogistic

x6

n14

Holling
inconsistent



n15

Lotka-Volterra
inconsistent

n17

x6

n18

Holling
inconsistent

Lotka-Volterra
inconsistent

Figure 2: Search space explored algorithm 1 solving sample aDPCSP

n19

Holling

P P = plogistic plogistic pHolling
CP = plogistic plogistic pHolling

n20

Lotka-Volterra
inconsistent

fiC OMPOSITIONAL ODEL R EPOSITORIES

assignment solution aDPCSP, corresponding preference guaranteed maximal (and, assignment is, fact, optimal). creation n 12 , priority queue Q looks follows (the ordering n2 n4 may vary since P P (n2 ) = P P (n4 )
CP (n2 ) = CP (n4 )):
{n10 , n8 , n12 , n6 , n2 , n4 }
Therefore, next node explored (after n9 subsequent creation n12 ) n10 .
Thirdly, node n19 correspond optimal solution. creation, Q equals:
{n19 , n12 , n6 , n2 , n4 }
consequence, n19 dequeued next iteration. children n 19 created
(Xu (n19 ) = activity constraints activate attributes), n 19 retained solution.
user interested finding multiple alternative solutions, search may proceed
Q contains nodes P P value smaller maximum preference
first solution. case, P P (n12 ) CP (n19 ) hence, one solution
aDPCSP.

3. Compositional Model Repositories
aDPCSPs discussed previous section provide foundation development
compositional model repositories. section specifies problem compositional model
repository built solve shows translated aDPCSP, hence resolved
using proposed aDPCSP solution algorithm.
3.1 Background: assumption based truth maintenance
ATMS mechanism keeps track piece inferred information depends
presumed information facts inconsistencies arise. ATMS, piece
information used derived problem solver stored node. Certain pieces information
known true cannot inferred pieces information, yet plausible
inference may drawn them. nodes categorised special type referred
assumptions.
Inferences pieces information maintained within ATMS dependencies corresponding nodes. extended form (see de Kleer, 1988; Keppens, 2002),
ATMS take inferences, called justifications form n . . . nj nk . . . nl nm ,
ni , . . . , nj , nk , . . . , nl , nm nodes problem solver interested in. ATMS
also take specific type justification, called nogood, leads inconsistency,
form ni . . . nj nk . . . nl (meaning least one statements
{ni , . . . , nj , nk , . . . , nl } must false). ATMS, nogoods represented justifications special node, called nogood node.
Based given justifications nogoods, ATMS computes label (nonassumption) node. label set environments environment set assumptions.
particular, environment E depicts possible world assumptions E true.
Thus, label L(n) node n describes possible worlds n true. label
computation algorithm ATMS guarantees label is:
513

fiK EPPENS & HEN

Sound - assumptions environment within label node true sufficient
condition derive node:
E L(n), [(ni E ni ) (ni E ni )] ` n
Consistent - environment label node, nogood node, describes
impossible world:
E L(n), [(ni E ni ) (ni E ni )] 0
Minimal - label contain possible worlds less general one
possible worlds contains (i.e. environments supersets environments
label):
E L(n)@E 0 L(n), E 0 E
Complete - label node, nogood node, describes possible worlds
node inferred:
E,[(ni E ni ) (ni E ni ) ` n]
E 0 L(n), [(ni E 0 ni ) (ni E 0 ni ) ` n]
3.2 Knowledge Representation
knowledge-based approach, building compositional modeller requires formalism specification inputs, outputs knowledge base. work developed
loosely based compositional modelling language (Bobrow, Falkenhainer, Farquhar, Fikes,
Forbus, Gruber, Iwasaki, & Kuipers, 1996), proposed standard knowledge representation formalism compositional modellers, adapted meet challenges ecological compositional
modelling problems identified introduction.
3.2.1 P RELIMINARY

CONCEPTS

primitive constructs compositional modeller participants, relations assumptions. subsection summarises concepts explains represented herein.
Participants2 refer objects interest, involved scenario model.
participants may real-world objects conceptual objects, variables express
features real-world objects mathematical model. instance, population species
typical example real-world object, variable expresses number individuals
species forms example conceptual object. natural group objects share
something common classes. Participants herein grouped participant classes,
representing set participants share certain common features. class given
name easy reference.
Relations describe participants related one another. participants,
relations represent real-world relationship, as:
2. previous work compositional modelling refers individuals quantities, names
would suit present application. Ecological models typically describe behaviour populations rather
individuals often hard distinguish quantities.

514

fiC OMPOSITIONAL ODEL R EPOSITORIES

predation(frog, insect)

(1)

relations may conceptual nature, equation (2), describes important
textbook model logistic population growth (Ford, 1999):

size
change = parameter size (1
)
dt
capacity

(2)

consistent compositional modelling approaches, paper employs LISPstyle notation relations. such, two sample relations become:

(predation frog insect)

(1)

(d/dt change (* change-rate size (- 1 (/ size capacity))))

(2)

Assumptions form special type relation employed distinguish alternative
model design decisions. Internally, assumptions stored form assumption nodes
ATMS (see section 3.3.1), knowledge base, assumptions appear relations
specific syntax semantics.
Two types assumptions employed article. Relevance assumptions state phenomena included excluded scenario model. Typical examples phenomena
population growth predation phenomena. general format relevance assumption
shown (3). phenomenon incorporated scenario model describing relevance assumption identified hnamei specific subsequent participants relations.
example, relevance assumption (4) states growth participant ?population
included model.

(relevant

hnamei

[{hparticipanti} | hrelationi])

(relevant growth ?population)

(3)
(4)

Model assumptions specify type model utilised describe behaviour certain
participant relation. Typical examples model types include exponential (Malthus, 1798)
logistic (Verhulst, 1838) model types population growth. formal specification
model assumption given (5). Often hnamei (5) corresponds name known
(partial) model phenomenon process described. example (6) states
population ?population modelled using logistic approach.
(model

[hparticipanti | hrelationi]

hnamei)

(model ?population logistic)

515

(5)
(6)

fiK EPPENS & HEN

Predators
natality

mortality
mortalityrate

natalityrate
preyrequirement

capacity

Prey
mortality

natality

natalityrate

predation

mortalityrate
capacity
searchrate
preyhandlingtime

Figure 3: Stock flow diagram predator prey scenario model
3.2.2 CENARIOS SCENARIO MODELS
formalised Keppens Shen (2001b), compositional modeller takes two inputs produces one output. first input representation (which model) describes
system interest means accessible formalism. model, normally consists
(mainly) real-world participants interrelationships, called scenario. second input
task description. formal description criteria adequacy output
evaluated. output new model describes scenario detailed formalism,
usually set variables equations, model-based reasoner employ readily.
model, normally contains conceptual participants interrelationships, called scenario
model. aim compositional modeller translate scenario scenario model,
means task description.
work, model formally defined tuple hP, Ri, P set participants
R set relations participants P . definition applies scenario
scenario model. typical example scenario description predator population, prey
population predation relation predator prey. scenario model
hP, Ri with:
P = {predator, prey}
R = {(predation predator prey)}
aim compositional model repository translate scenario scenario model.
Within work, systems dynamics stock-flow formalism (Forrester, 1968) ordinary differential equations (ODEs) employed modelling formalisms. example, scenario
model corresponds scenario depicted Figure 3. Formally, scenario model
another model hP, Ri case
P = {Npredator , Bpredator , Dpredator , Nprey , Bprey , Dprey , Pprey ,
bpredator , bprey , dpredator , dprey , Cpredator , Cprey ,
s(prey,predator) , t(prey,predator) , r(predator,prey) }
516

fiC OMPOSITIONAL ODEL R EPOSITORIES

Symbol
Npredator , Nprey
Bpredator , Bprey
Dpredator , Dprey
Pprey
bpredator , bprey
dpredator , dprey
Cpredator , Cprey
s(prey,predator)
t(prey,predator)
r(predator,prey)

Variable name
number predators, prey
natality predators, prey
mortality predators, prey
predation prey
natality-rate predators, prey
mortality-rate predators, prey
capacity predators, prey
search-rate
prey-handling-time
prey-requirement

Table 1: Variables stock flow diagram mathematical model
R={


Npredator = Bpredator Dpredator ,
dt

Nprey = Bprey Dprey Pprey ,
dt
Bpredator = bpredator Npredator ,
Bprey = bprey Nprey ,
Dpredator = dpredator Npredator

Npredator
,
Cpredator

Nprey
,
Cprey
s(prey,predator) Nprey Npredator
=
,
1 + s(prey,predator) Nprey t(prey,predator)

Dprey = dprey Nprey
Pprey

Cpredator = r(predator,prey) Nprey ,
Cprey = Nprey }
relation variables mathematical model used stock-flow diagram given table 1. Generally speaking, stock-flow diagrams graphical representations
systems (ordinary qualitative) differential equations. automated modelling literature
general, engineering physical systems modelling particular, sophisticated representational formalisms developed enable identification mathematical models
behaviour dynamic systems observations. Examples include bond graphs (Karnopp, Margolis, & Rosenberg, 1990) generalised physical networks (Easley & Bradley, 1999). However,
potential benefits advanced formalisms exploited here, remain
interesting future work. Instead, stock-flow diagrams employed throughout paper
far commonly used ecological modelling (Ford, 1999).
often possible construct multiple scenario models single given scenario,
task specification employed guide search appropriate one(s). work,
scenario models selected basis hard constraints user preferences. hard constraints stem restrictions imposed compositionality representational framework (see
section 3.2.3) properties scenario model required satisfy (see section 3.2.3).
517

fiK EPPENS & HEN

Name
Addition
Multiplication
Selection

Syntax (infix notation)
?var = C + (formula)
?var = C (formula)
?var = C (formula)
?var = C (formula)
?var = C if,p (antecedent, formula)
?var = C else (formula)

Syntax (prefix notation)
(== ?var (C-add formula))
(== ?var (C-sub formula))
(== ?var (C-mul formula))
(== ?var (C-div formula))
(== ?var (C-if antecedent formula :priority p))
(== ?var (C-else formula)

Table 2: Composable functors composable relations
user preferences express users subjective view modelling approaches
appropriate context current scenario (see section 2.2).
3.2.3

KNOWLEDGE BASE

construct scenario models given scenario, compositional modeller relies use
knowledge base particular problem domain. illustrate ideas, section
presents constructs employed compositional modeller developed synthesise
scenario models ecological domain.
Composable relations knowledge base approach consists partial models
instantiated composed complex scenario models. composition partial models
scenario model may involve composition partial relations (coming different partial
models) compounded relations. sample scenario model section 3.2.2, following
relation describes changes population size prey population

Nprey = Bprey Dprey Pprey
dt

(7)

(7), Nprey population size, Bprey number births, Dprey number natural deaths
Pprey number prey died due predation. Thus, relation (7) actually describes two
phenomena affect population size Nprey : natural population growth (Bprey Dprey )
predation related deaths (Pprey ). constructing knowledge base, desirable represent
two phenomena isolation always occur combination. example,
species predators, therefore unnecessary always include predation
cause death. viewpoint, relation (7) seen composed different
composable relations knowledge base:

Nprey = C + (Bprey )
dt


Nprey = C (Dprey )
dt


Nprey = C (Pprey )
dt

use composable relations enables knowledge base cover many combinations
phenomena may affect relation possible, representing phenomenon individually rather precompiling everything together. component parts (i.e.
composable relations) relations need represented, instead possible, however complex, combinations them, knowledge base smaller effective. section
describes composable relations represented knowledge base, well whether
composed form compounded relations.
518

fiC OMPOSITIONAL ODEL R EPOSITORIES

Composable relations containing composable functors method
composition exists (that describes complete set composable relations composed).
composable functors employed proposed Bobrow et al. (1996) new addition:
composable selection. summary composable relations presented table 2.
composable relations introduced Bobrow et al. (1996) easy understand.
formulae f v = C + (f ) v = C (f ) represent terms (respectively f f ) sum,
formulae f v = C (f ) v = C (f ) represent factors (respectively f f1 ) product.
However, ecological models use typically contain selection statements declare
one certain equation must employed condition satisfied one otherwise.
Formally, selection relation form
c1 v = r1 else c2 . . . else v = rn

(8)

v participant, ci (with = 1, . . . , n1) relation describing condition statement
rj (with j = 1, . . . , n) relation. selection relation consists partial relations:
ci v = ri

= 1, . . . , n 1

else v = rn
Therefore, selection relation composed two types composable relation. first
composable relation, form v = C if,p (a, f ), v participant, p
element taken total order, set natural numbers , denotes priority
composable relation sequence, f two given relations. second type
composable relation composable else relation, form v = C else (felse ),
felse given relation assigned v none antecedents composable relations
true.
illustrate notation, selection relation (8) composed following composable relations:
v = C if,p1 (c1 , r1 )
..
.
v = C if,pn1 (cn1 , rn1 )
v = C else (rn )
p1 > . . . > pn1 .
combine composable relations, number rules defined implement semantics
representational formalism. theory, set rules generated enables aggregation set composable relations. practice, however, trade-off must made
flexibility (the ability combine many different types composable relation) comprehensibility (the use set rules easily understood knowledge engineer employs
composable relations). Thus, types composable relations combined
restricted.
Table 3 summarises composable relations joined form compounded relations.
principle guiding construction table allow composition relations
certain types resulting compound relation intuitively obvious. example, according
519

fiK EPPENS & HEN

C + (f1 )
C (f1 )
C (f1 )
C (f1 )
C if,p1 (a1 , f1 )
C if,p2 (a1 , f1 )
C else (f1 )

C + (f2 )
yes
yes






C (f2 )
yes
yes






C (f2 )


yes
yes




C (f2 )


yes
yes




C if,p2 (a2 , f2 )




yes

yes

C else (f2 )




yes
yes


Table 3: Composibility composable relations
Table 3, composable addition relation x = C + (y) combined composable subtraction relation x = C (z) combination clearly x = z. However, according
Table 3, composable addition relation x = C + (y) combined composable multiplication relation x = C (z), arbitrary non-intuitive rule would otherwise
defined decide whether compound relation would x = + z x = z.
order composable selections must considered defined priorities
(or implicit case C else ). Therefore, composable selections combined one
another provided two composable relations priority.
order derive actual rules composition, sets composable relations
functor given model hP, Ri defined first:

R(v, C + ) = {v = C + (fi ) | (v = C + (fi )) R}
R(v, C ) = {v = C (fi ) | (v = C (fi )) R}
R(v, C ) = {v = C (fi ) | (v = C (fi )) R}
R(v, C ) = {v = C (fi ) | (v = C (fi )) R}
R(v, C ) = {v = C if,pi (ai , fi ) | (v = C if,pi (ai , fi )) R}
R(v, C else ) = {v = C else (fi ) | (v = C else (fi )) R}
this, rules composition built given expressions (9), (10) (11).
jointly state given set composable relations rewritten single compound
relation. rules contains complete set composable relations antecedent.
particular, antecedent rule (9) contains set composable addition subtraction
relations participant v left-hand side.
Similarly, antecedent rule (10) contains complete set composable multiplication relations. Finally, antecedent rule (11) satisfied complete set composable else
relations left-hand participant v, provided priorities strictly ordered (i.e.
two priorities equal) single composable else relation. latter two
conditions added two composable relations priority two composable
else relations compounded. consequents rules composition explain
complete sets composable relations joined. simply matter applying
appropriate mathematical operation provided terms.
520

fiC OMPOSITIONAL ODEL R EPOSITORIES

R(v, C + ) = {v = C + (f1+ ), . . . , v = C + (fm+ )}
R(v, C ) = {v = C (f1 ), . . . , v = C (fn )}

(9)

v = f1+ + . . . + fm+ (f1 + . . . + fn )
R(v, C ) = {v = C (f1 ), . . . , v = C (fm )}
R(v, C ) = {v = C (f1 ), . . . , v = C (fn )}
1 f1 . . . fm
v=
f1 . . . fn

(10)

R(v, C ) ={v = C if,p1 (a1 , f1 ), . . . , v = C if,pm (am , fm )}
R(v, C else ) ={v = C else (felse )} p1 > . . . > pm

(11)

v =if a1 f1 , else . . . , fm , else felse
Property definitions Property definitions describe features interest application requiring
scenario model. property definition tuple hP , , P = {ps1 , . . . psm } set
source-participants, predicate calculus sentence whose free variables elements P ,
relation, whose free variables also elements P ,
ps1 , . . . , psm

typical example feature interest requirement certain variable model
endogenous exogenous. specific, property definitions describe
variable ?v endogenous exogenous respectively.
(defproperty endogenous
:source-participants ((?v :type variable))
:structural-condition ((or (== ?v *) (d/dt ?v *)))
:property (endogenous ?v))
(defproperty exogenous
:source-participants ((?v :type variable))
:structural-condition ((not (endogenous ?v)))
:property (exogenous ?v))

?v = * true (where * matches
first definition states whenever either ?v = * dt
constant formula), ?v deemed endogenous. second property definition indicates
variable said exogenous object exists endogenous.
describing features formally knowledge base, property definitions enable
imposed criteria selection scenario models. way, variable describing
size particular population eco-system, instance, forced endogenous.
Note required properties specified two different ways: either globally goals
scenario model construction locally required purpose certain model fragment.
latter use model properties illustrated later.

521

fiK EPPENS & HEN

Model fragments Model fragments building blocks scenario models constructed. model fragment tuple hP , P , , , A, P = {ps1 , . . . psm }
set variables called source-participants, P = {pt1 , . . . , ptn } set variables called targetparticipants, = {s1 , . . . , sv } set relations, called structural conditions, whose free variables elements P , = {t1 , . . . , tx } set relations, called postconditions, whose free
variables elements P P , = {a1 , . . . , ay } set relations, called assumptions,
= set relations, called purpose-required properties, that:
ti , ps1 , . . . , psm , pt1 , . . . , ptn , s1 . . . sv (a1 . . . ay ti )


, ps1 , . . . , psm , pt1 , . . . , ptn ,

s1

...

sv

a1 . . . ax

(12)
(13)

Note that, work, property definition hP , , equivalent model fragment
hP , {}, , {}, {}, {}i.
example, model fragment states population ?p described two
variables ?p-size (describing size ?p) ?p-change (describing rate change
population size) differential equation

?p-size = ?p-change
dt
usage partial scenario model subject two conditions: (1) growth phenomenon
relevant regard ?p, (2) variable ?p-change endogenous eventual scenario
model. former requirement indicated relevance assumption latter
purpose-required property:
(defModelFragment population-growth
:source-participants ((?p :type population))
:assumptions ((relevant growth ?p))
:target-participants ((?p-size :type variable)
(?p-change :type variable))
:postconditions ((size-of ?p-size ?p)
(change-of ?p-change ?p)
(d/dt ?p-size ?p-change))
:purpose-required ((endogenous ?p-change)))

purpose-required property usually satisfied additional model fragments,
one below:
(defModelFragment logistic-population-growth
:source-participants ((?p :type population)
(?p-size :type variable)
(?p-change :type variable))
:structural-conditions ((size-of ?p-size ?p)
(change-of ?p-births ?p))
:assumptions ((model ?p-size logistic))
:target-participants ((?r :type parameter)
(?k :type variable)
(?d :type variable))
:postconditions ((capacity-of ?k ?p)
(density-of ?d ?p-size)
(== ?d (C-add (/ ?p-size ?k)))
(== ?p-change (- (* ?r ?p-size (- 1 ?d))))))

522

fiC OMPOSITIONAL ODEL R EPOSITORIES

Model fragments rules inference describe new knowledge derived
existing knowledge committing emerging model certain assumptions. used
generate space possible models. Model fragments instantiated matching sourceparticipants existing participants scenario emerging model, matching
structural conditions corresponding relations. possible instantiation, new instance
generated target-participants, necessary, new instances also created
postconditions assumptions. instances, well inferential relationships
instances source-participants, structural conditions assumptions one hand,
target-participants postconditions other, stored ATMS, forming
model space. explained section 3.3.1.
model fragment said applied instantiated underlying assumptions
hold. model fragment applied, instances target-participants postconditions
corresponding instantiation model fragment must added resulting model.
respect example, model fragment implements logistic population growth
model instantiated whenever variables exist describe size change population,
applied logistic model population size also selected.
Note compositional modellers, ones devised Heller Struss (1998,
2001); Levy, Iwasaki Fikes (1997); Nayak Joskowicz (1996); Rickel Porter (1997),
model fragments represent direct translations components physical systems influences variables. compositional modeller presented herein aims serve ecological
model repository, contents model fragments employed differs conventional
compositional modellers two important regards:
Firstly, model fragments contain partial models describing certain phenomena instead influences. partial models normally correspond developed ecological modelling
research. Typical examples include logistic population growth model (Verhulst, 1838)
Holling predation model (Holling, 1959) devised population dynamics literature.
Secondly, partial models contained model fragments often need composed incrementally. example, aforementioned sample model fragment logistic-populationgrowth requires emerging scenario model, may generated sample model
fragment population-growth. Thus, one model fragment, e.g. logistic-populationgrowth, expand partial model contained another, e.g. population-growth. feature, (correctly) presumed model fragment generates new relations
preconditions model fragments expands on. Violating presumption would
make little sense context present application would imply recursive extension
emerging scenario model set variables equations.
3.2.4 PARTICIPANT CLASS DECLARATION PARTICIPANT TYPE HIERARCHIES
general, participant classes need defined. However, certain types participant may
described terms interesting participants, irrespective modelling choices.
feature provides syntactic sugar describing important relations participants, making
easier declare required properties scenario model terms participants scenario.
example, behaviour population may described terms population size growth
rate variables:
(defEntity population
:participants (size growth-rate))

523

fiK EPPENS & HEN

Participant class declarations may also employed within model fragments provide
specific definition meaning source-participants target-participants. way,
participant specifications constrained feature another participant means
:entity statement, following example illustrates:
(defModelFragment define-population-growth-phenomenon
:source-participants ((?p :type population))
:target-participants
((?ps :type stock :entity (size ?p))
(?pg :type variable :entity (growth-rate ?p))
(?pb :type flow)
(?pd :type flow))
:assumptions ((relevant growth ?p))
:postconditions ((== ?pg (- ?pb ?pd))
(flow ?pb source ?pl)
(flow ?pd ?pl sink)))

Furthermore, participant class declarations may define one class immediate subclass
another. example, population participant class holometabolous insects (e.g. butterflies)
may defined subclass population participant class:
(defEntity holometabolous-insect-population
:subclass-of (population)
:participants
(larva-number pupa-number adult-number))

way, participant type hierarchy defined. subclass inherits participants
superclasses (i.e. immediate superclass superclasses superclasses).
summary, participant class declaration tuple = h , P participant
class, called immediate superclass participant class P set participants classes
describe important features participant class.
3.3 Inference
compositional modelling method presented herein employs four step inference procedure:
1. Model space construction. model space ATMS efficiently stores participants, relations model design decisions (represented form relevance model
assumptions) may part final scenario model, well conditions
participants relations must must part scenario model.
2. aDCSP construction. model space contains number hard constraints participants relations may combined. inference step extracts restrictions
translates aDCSP.
3. Inclusion order-of-magnitude preferences. Preferences associated relevance
model assumptions scenario space reflect relative appropriateness
assumptions, resulting aDPCSP.
4. Scenario model selection. inference step solves aDPCSP. resulting solutions
correspond scenario models consistent according domain knowledge
optimise overall preference respect order-of-magnitude preference calculus.
524

fiC OMPOSITIONAL ODEL R EPOSITORIES

Problem Specification

Compositional Model Repository

population(prey)
population(predator)
predation(predator,prey)

Application

STEP 1
Model Space Construction

Scenario
Model Space

Requirements
Inconsistencies
Generation

Activitybased Dynamic Constraint
Satisfaction Problem Construction

Requirements
Inconsistencies

Knowledge Base
STEP 2

Scenario Model
Construction
Dynamic Constraint
Satisfaction Problem

Scenario Model
STEP 3
Inclusion OrderofMagnitude
Preferences

Preferences
Preference Ordering

Dynamic Preference Constraint
Satisfaction Problem

Application

Prey
death
rate

birth
rate

crowding
max crowd
sustainable
population

fooddemand

consumption

STEP 4

Predator
death
rate

birth
rate

Scenario Model Selection
(aDCSP solver)

crowding

Assumption Set

Knowledge elements

Inference elements

Figure 4: Inference procedures compositional modeller

525

fiK EPPENS & HEN

four steps correspond four squares compositional model repository Figure 4
section, inference steps discussed detail illustrated means
simple examples. next section contains detailed example shows procedure
applied non-trivial ecological modelling domain.
3.3.1 CENARIO + K NOWLEDGE BASE = ODEL PACE
previously stated, aim compositional modeller translate scenario scenario
model. representations system interest though model system different
level detail. knowledge base provides foundation translation. scenario models
constructed given scenario, regard knowledge base, stored
model space.
model space ATMS (de Kleer, 1986) containing participants, relations assumptions instantiated given scenario. work, generalised version
ATMS, introduced de Kleer (1988), employed allows use negations nodes
justifications. algorithm GENERATE ODEL PACE(hO, Ri) describes model
space created scenario hO, Ri. first initialises model space participant instances (O) relation instances (R) scenario. Then, model fragment whose source-participants structural conditions match participants relations already
, new instances target-participants, assumptions postconditions added . property definition hP , , equivalent model fragment hP , {}, , {}, {}, {}i,
procedure applies property definitions well model fragments. Matching sourceparticipants structural conditions model fragment emerging model space performed function match(, , ) specified below, model fragment
matched, substitution source-participants participant instances.


true = {ps1 /o1 , . . . , psm /om }





P () = {ps1 , . . . , psm }

match(, , ) =
o1 . . . om



(),




false otherwise
match, specified model fragment substitution , processed follows:

assumption A(), new node, denoting assumption instance a, created
added .
Then, new node n(,) , denoting instantiation via substitution , created, added
justified implication:
(aA() a) (pP () p) (s () ) n(,)
Finally, new instance target-participant p P () postcondition
(), provided already exist model space , created.
target-participants, involves creating new symbol new participant instance
function gensym() extending substitution {p/gensym()}. new node n
526

fiC OMPOSITIONAL ODEL R EPOSITORIES

Algorithm 1:

GENERATE ODEL PACE(hO, Ri)

new ATMS;
O, add-node(, o);
r R, add-node(, r);
, , match(, , )

justification ;






A()



newnode add-node(, (a));





justification
justification {newnode};


p P ()





justification justification {find-node(, (p))};



()




justification justification {find-node(, ())};




add-node(, n(,) );
add-justification(, n(,) , njustificationn);





p P ()



{p/gensym()};




add-node(, (p));





add-justification(, o, n(,) );







()



( )







get-node(, ());




else
add-node(, ());





add-justification(, o, n(,) );
n1 , . . . , nm , inconsistent({n1 , . . . , nm )
add-justification(, n , n1 . . . nm );

{
{
{

a1

Instances assumptions:
A() = {a1 , . . . , }

..
.
pt1x



Instances sourceparticipants:
P () = {ps1 , . . . , psm }

Instances structural
conditions:
() = {s1 , . . . , sv }

ps1

..
.

..
.







ptn


t1

psm

..
.

s1

ts

..
.

}
}

Instances target
participants:
P () = {pt1 , . . . , ptn }

Instances postconditions:
() = {t1 , . . . , ts }

sv

Figure 5: Model fragment application

created added new participant instance p new instantiated
relation . nodes justified implication n (,) n.
527

fiK EPPENS & HEN

global property must satisfied
consistent scenario models

purpose-required property model fragment ,
applied substitution .

r1 r2
non-composable relations

v = r1 (. . .)

















(a) Inconsistency caused
global property



v = r2 (. . .)




(b) Inconsistency caused
purpose-required property

(c) Inconsistency caused
non-composable relations

Figure 6: Sources inconsistency
illustrate procedure, Figure 5 shows graphical representation inferences
constructed applying model fragment = hP , P , , , A, {}i respect substitution
.
possible applications model fragments exhausted, inconsistencies
model space identified recorded ATMS. algorithm, nogoods generated
set {n1 , . . . , nm } inconsistent nodes, denoted inconsistent({n1 , . . . , nm }).
three sources inconsistencies reported ATMS different way:
Global properties: Let instance global property scenario model must
satisfy. Then, combination assumptions negations assumptions prevents
satisfied inconsistent. Therefore, inconsistent({}) must reported
required global property . type inconsistency depicted Figure 6(a).
Purpose-required properties: application model fragment without satisfying
purpose-required properties () yields inconsistency (see (13)). Hence, node
n(,) denoting instantiation via substitution , node n describing
appropriate instance purpose-required property (), inconsistent({n (,) , n })
reported. type inconsistency depicted Figure 6(b).
Non-composable relations: mathematical formalism designed describe simulation
models dynamic systems, certain combinations relations may over-constrain model,
hence, unsuitable generating behaviour system interest. Within
system dynamics ODE formalisms used paper, assignments relations
variable composable relations explicitly deemed composable.
words, two relations v = ri v = rj combined one another ri
rj composable. Examples pairs non-composable relations include
x = C + (y) x = C (z) C + C relations composable,
= C + (b) = c + c + composable relation.
Combinations non-composable relations must reported inconsistency well.
type inconsistency depicted Figure 6(c).
528

fiC OMPOSITIONAL ODEL R EPOSITORIES

assumption:
(relevant growth frog)
participant:
population frog
1 :
2 :
:

1

assumption:
(model nfrog logistic)

participant:
parameter rfrog

relation:

dt nfrog = cfrog

participant:
parameter kfrog

participant:
variable cfrog

relation:
(capacity-of kfrog frog)

participant:
variable nfrog

population-growth
model fragment
logistic-population-growth
model fragment
endogenous
property definition

cfrog = rfrog nfrog (1

nfrog
)
kfrog

relation:
(change-of cfrog frog)
relation:
(size-of nfrog frog)

relation:
endogenous(cfrog )

relation:

2





relation:
(endogenous cfrog )



Figure 7: Partial model space
illustrate model space construction algorithm, Figure 7 presents small sample model
space. results application population-growth logistic-population-growth model fragments endogenous property definition, described
earlier, single population frog. larger scenario involving multiple populations relations populations specified, similar partial model space would generated
individual population.
3.3.2 F ROM

MODEL SPACE DCSP

model space constructed, translated aDCSP. translation
procedure, summarised algorithm CREATEA DCSP(), consists three steps described below:
Algorithm 2:

CREATEA DCSP()

comment: set substitutions
{};
comment: Generate attributes domains

A, assumption-class(A)
x create-attribute();




D(x)
{};




{A/x};

aA



v create-value();



D(x) D(x) {v};



{a/x : v};
comment: Generate activity constraints

A, assumption-class(A)
subject(A);
{a1> , . . . , ap> , a1 , . . . , aq } L(s)

add(a1> . . . ap> a1 . . . aq active(A));
comment: Generate compatibility constraints
{a1> , . . . , ap> , a1 , . . . , aq } L(n )
add(a1> . . . ap> a1 . . . aq );

529

fiK EPPENS & HEN

1. Generate attributes domain values assumptions. aDCSP attributes correspond underlying assumption classes (i.e. groups assumptions indicating alternative
choices regards model construction decision). relevance assumption
negation jointly form assumption class. example, 1 ={(relevant growth
frog), (relevant growth frog)} specifies assumption class. set
model assumptions involving participants/relations, different model names
hence different descriptions, also form assumption class. instance, 2 ={(model
nfrog exponential), (model nfrog logistic), (model nfrog other)},
nfrog variable denoting size population, specifies assumption class. Running step algorithm, attribute created assumption class,
domain attribute consisting assumption instances assumption class.
2. Create activity constraints. attributes domain values generated previous step
meaningful situations participant and/or relation instances contained
arguments corresponding assumptions exist. example, assumption (model
nfrog logistic) relevant participant instance nfrog exists. Clearly, assumptions within one assumption class participant and/or relation instances
arguments. assumption class corresponds one attribute, attribute
activated participant and/or relation instances associated related assumption class active. Therefore, step creates activity constraints activate
attribute based conjunction environments contained within labels
participants/relations assumption class. instance, deduced Figure
7, nfrog activated (relevant growth frog) committed. Thus, attribute
corresponding assumption class A2 , defined step 1, activated attribute value
assignment associated (relevant growth frog) assumption.
3. Create compatibility constraints. ATMS (or model space), sources inconsistencies contained label nogood node. Therefore, compatibility constraints
created directly translating environments label L() corresponding
conjunctions attribute-value assignments.
3.3.3

DCSP

+

PREFERENCES

= DPCSP

aDCSP produced formalises hard requirements imposed upon scenario models.
Among scenario models meet requirements, may better others,
underlying model design decisions may deemed appropriate user. Preferences
express (relative) level appropriateness attached assumptions describe
model design decisions, extension, attribute-value pairs aDCSP. discussed
section 2, extension aDCSP constitutes aDPCSP.
specifically, worth recalling section 2.2 order-of-magnitude preference
calculus presented enables representation reasoning subjective user preferences
different relevance modelling assumption. Next, section 2.3 introduces solution algorithm
aDPCSPs include aDCSP, ones constructed approach section 3.3.2,
extended subjective user preferences alternative design decisions.
530

fiC OMPOSITIONAL ODEL R EPOSITORIES

3.4 Outline analysis complexity
complexity work arises four major sources: 1) model space construction, 2) label
propagation ATMS, 3) model space aDCSP translation, 4) aDPCSP solution.
GENERATE ODEL PACE (hO, Ri) essentially performs fixed sequence instructions
produces small set nodes inferences match model fragment. Therefore,
time space complexity linear respect number possible matches model
fragments. CREATEA DCSP() extracts certain information model space rewrites
different formalism without manipulations. Therefore, time space complexity
linear respect size model space.
label propagation algorithm ATMS known exponential time complexity.
However, model space built incrementally (by GENERATE ODEL PACE(hO, Ri))
root nodes ATMS network (i.e. correspond facts antecedents) leaf nodes (i.e. consequents, nogood node)
inconsistencies added end, complexity increases exponentially
depth network number participants relations individual model fragments, rather size model space. fact significantly limits complexity
impact label propagation. Firstly, depth ATMS network restricted domain.
many conventional compositional modellers, model fragments direct translations
scenario components scenario model equations, depth would one. Empirically, constructing model space sophisticated eco-systems, depth model space never exceeded
8. Secondly, size individual model fragments change significantly size
knowledge base.
fourth final source complexity driven fact constraint satisfaction
algorithm must determine consistent combination assumptions model space. space
attribute value assignments increases exponentially size number assumptions
hence, model space. Thus, overall complexity present approach largely
dominated constraint satisfaction algorithm employed.
user specify preference, CSP aDCSP. Recently, number efficient
methods devised solving aDCSPs presented Minton et al. (1992); Mittal
Falkenhainer (1990); Verfaillie Schiex (1994). helps minimise overhead incurred
compositional modelling.
preferences, CSP becomes aDPCSP. argued section 2, presents new
problem yet studied detail. work, A* algorithm proposed
implement CSP solution method. approach known efficient terms
proportion search space algorithm needs explore finding optimal solution,
compared search methods based heuristic (Hart et al., 1968).
disadvantage incurs exponential space complexity. explained Miguel Shen
(2001a, 2001b); Tsang (1993), wide range alternative solution techniques exist ordinary
CSPs many could also extended solve aDPCSPs. detailed examination
techniques topic future research.
3.5 Automated modelling scientific discovery
mentioned previously, compositional model repository designed order compose models
systems structure relevant domain knowledge. such, approach gives rise po531

fiK EPPENS & HEN

tentially beneficial means operationalise outcomes scientific discovery. specifically,
resultant compositional model repositories allow existing knowledge model construction
applied unexperienced scenarios support investigation situations may
physically difficult replicate create may synthesised computational representations.
present work applied vegetation component MODMED n-species
model (Legg, Muetzelfeldt, & Heathfield, 1995). n-species model offers system dynamics
representation populations Mediterranean vegetations affected populations farm animals, climate environmental management. purpose model
instantiated respect various Mediterranean communities, serve component
large scale simulation designed simulate effects various environmental
policies Mediterranean landscape. knowledge base containing approximately 60 model
fragments 4 property definitions constructed, basis complex parts
n-species model two man-weeks. knowledge base employed reconstruct variations n-species model accommodate variety possible scenarios, well
examine simplifications original n-species model exclude certain phenomena.
compositional model repository closely related seminal work compositional modelling (Falkenhainer & Forbus, 1991). approach similar functionality
devised specifically physical systems relies component-connection formalism
represent scenarios.
Another approach recently developed applied ecological domain
Heller Struss (1998, 2001). work derives systems structure observations
behaviour domain knowledge. Therefore, able perform diagnosis ecological systems
therapy suggestion. Another important distinction work present study
presumes process described one way instead allowing multiple
alternative models.
machine learning community, number approaches devised Bradley,
Easley Stolle (2001); Langley et al. (2002); Todorovski Dzeroski (1997, 2001)
induce sets differential equations a) observations behaviour, b) domain knowledge represented form hypothetical equations, c) description structure system.
approaches aim scientific discovery generalising observed behaviour mathematical
models. specifications scenario domain knowledge methods similar
used article. especially true work Langley et al. (2002); Todorovski Dzeroski (1997, 2001), work also applied population dynamics.
However, internal mechanisms approaches different essentially rely
exhaustive search procedures instead constraint satisfaction techniques.

4. Population Dynamics Example
examples used throughout previous sections taken extensive application
study present work. application aimed construct repository basic population
dynamic models, describing phenomena growth, predation competition. section
presents overview proposed approach employed application show
ability work scale larger problems.
532

fiC OMPOSITIONAL ODEL R EPOSITORIES

4.1 Knowledge base
subsection illustrates set model fragments constructed. challenge
task lies fact model fragments must encompass sufficiently general reusable
component part ecological models. instances models found literature ecological
modelling, boundaries recurring component parts hidden, therefore
knowledge engineer identify them.
First, hierarchy entity types set up. system dynamics models shown earlier contain
three types participant: variables, stocks flows. Here, stocks flows special
type variable predetermined meaning. is, flow f stock corresponds


= C + (f ) flow f stock denotes dt
= C (f ). Hence, stocks
equation dt
flows defined subclasses participant class variable:
(defEntity variable)
(defEntity stock
:subclass-of (variable))
(defEntity flow
:subclass-of (variable))

sample properties defined section 3.2.3, describe condition
variable endogenous exogenous, employed knowledge base:
(defproperty endogenous-1
:source-participants ((?v :type variable))
:structural-conditions ((== ?v *))
:property (endogenous ?v))
(defproperty endogenous-2
:source-participants ((?v :type variable))
:structural-conditions ((d/dt ?v *))
:property (endogenous ?v))
(defproperty exogenous
:source-participants ((?v :type variable))
:structural-conditions ((not (endogenous ?v)))
:property (exogenous ?v))

next three model fragments contain rules stock-flow diagrams employed systems dynamics models. respectively describe that:
flow ?flow stock ?stock corresponds composable differential equation:

?stock = C + (?flow)
dt
flow ?flow stock ?stock corresponds composable differential equation:

?stock = C (?flow)
dt
flow ?flow one stock ?stock1 another stock ?stock2 corresponds
composable differential equations:


?stock1 = C (?flow) ?stock2 = C + (?flow)
dt
dt
533

fiK EPPENS & HEN

(defModelFragment inflow
:source-participants
((?stock :type stock)
(?flow :type flow))
:structural-conditions
((flow ?flow source ?stock))
:postconditions
((d/dt ?stock (C-add ?flow))))
(defModelFragment outflow
:source-participants
((?stock :type stock)
(?flow :type flow))
:structural-conditions
((flow ?flow ?stock sink))
:postconditions
((d/dt ?stock (C-sub ?flow))))
(defModelFragment inflow
:source-participants
((?stock1 :type stock)
(?stock2 :type stock)
(?flow :type flow))
:structural-conditions
((flow ?flow ?stock1 ?stock2))
:postconditions
((d/dt ?stock1 (C-sub ?flow))
(d/dt ?stock2 (C-add ?flow))))

declarations place, knowledge base model fragments defined. first model fragment describes population growth phenomenon. Note
aforementioned growth, predation competition models contain stock representing population
size two flows, one flow births stock another flow deaths stock.
common feature models population dynamics contained single model fragment.
(defModelFragment population-growth
:source-participants
((?population :type population))
:assumptions
((relevant growth ?population))
:target-participants
((?size :type stock :name size)
(?birth-flow :type flow :name births)
(?death-flow :type flow :name deaths))
:postconditions
((flow ?birth-flow source ?size)
(flow ?death-flow ?size sink)
(size-of ?size ?population)
(births-of ?birth-flow ?population)
(deaths-of ?death-flow ?population))
:purpose-required
((endogenous ?birth-flow)
(endogenous ?death-flow)))

variables ?birth-flow ?death-flow become endogenous model contains
equation describing birth flow death flow. equations differ population growth
models. Two types population growth model exponential growth model (Malthus, 1798),
shown Figure 8(a), logistic growth model (Verhulst, 1838), shown
Figure 8(b). following two model fragments formally describe component models:
534

fiC OMPOSITIONAL ODEL R EPOSITORIES




fffi








fffi!
"



(a) Exponential growth




(b) Logistic growth

Figure 8: Population growth models
(defModelFragment exponential-population-growth
:source-participants
((?population :type population)
(?size :type variable)
(?birth-flow :type variable)
(?death-flow :type variable))
:structural-conditions
((size-of ?size ?population)
(births-of ?birth-flow ?population)
(deaths-of ?death-flow ?population))
:assumptions
((model ?size exponential))
:target-participants
((?birth-rate :type variable :name birth-rate)
(?death-rate :type variable :name death-rate))
:postconditions
((== ?birth-flow (* ?birth-rate ?size))
(== ?death-flow (* ?death-rate ?size))))
(defModelFragment logistic-population-growth
:source-participants
((?population :type population)
(?size :type variable)
(?birth-flow :type variable)
(?death-flow :type variable))
:structural-conditions
((size-of ?size ?population)
(births-of ?birth-flow ?population)
(deaths-of ?death-flow ?population))
:assumptions
((model ?size logistic))
:target-participants
((?birth-rate :type variable :name birth-rate)
(?death-rate :type variable :name death-rate)
(?density :type variable :name total-population)
(?capacity :type variable :name capacity))
:postconditions
((== ?birth-flow (* ?birth-rate ?size))
(== ?death-flow (* ?death-rate ?size ?density))
(== ?density (C-add (/ ?size ?capacity)))
(density-of ?density ?population)
(capacity-of ?capacity ?population)))

one twist compositional modelling population growth. Sometimes, actual
growth model implicitly contained within another type model. cases, growth
phenomenon corresponding differential equations still relevant, none dedicated
growth models employed. example, shown later, Lotka-Volterra predation
model comes equations describing growth.
535

fiK EPPENS & HEN

model fragment other-growth allows empty growth model, named other,
selected. However, due purpose-required property instance ?p-change must
endogenous, empty model selected growth model implicitly included
elsewhere.
(defModelFragment other-growth
:source-participants
((?population :type population)
(?size :type variable)
(?birth-flow :type variable)
(?death-flow :type variable))
:structural-conditions
((size-of ?size ?population)
(births-of ?birth-flow ?population)
(deaths-of ?death-flow ?population))
:assumptions
((model ?population other)))

addition population growth, two phenomena included knowledge base:
predation competition. Predation competition relations species represented
predicates populations: e.g. (predation foxes rabbits) (competition
sheep cows). However existence phenomenon necessarily mean must
contained within model. would make little sense model predation competition without
modelling size populations, models phenomena relate population sizes
one another. Therefore, incorporation predation phenomenon made dependent upon
existence variables representing population size. Also, human expert modellers may prefer
leave phenomenon resulting model. keep choice open, following two
model fragments construct participant representing phenomena predation competition,
make dependent upon relevance assumption:
(defModelFragment predation-phenomenon
:source-participants
((?predator :type population)
(?prey :type population)
(?predator-size :type variable)
(?prey-size :type variable))
:structural-conditions
((predation ?predator ?prey)
(size-of ?predator-size ?predator)
(size-of ?prey-size ?prey))
:assumptions
((relevant predation ?predator ?prey))
:target-participant
((?predation-phenomenon :type phenomenon :name predation-phenomenon))
:postconditions
((predation-phenomenon ?predation-phenomenon ?predator ?prey))
:purpose-required ((has-model ?predation-phenomenon)))
(defModelFragment competition-phenomenon
:source-participants
((?population1 :type population)
(?population2 :type population)
(?size1 :type variable)
(?size2 :type variable))
:structural-conditions
((competition ?population1 ?population2)
(size-of ?size1 ?population1)
(size-of ?size2 ?population2))

536

fiC OMPOSITIONAL ODEL R EPOSITORIES

Bprey = bprey Nprey

Bprey = bprey Nprey

Dprey = dprey Nprey

Nprey
Kprey

Dprey = pprey Nprey Npred

dt Nprey


dt Nprey

= Bprey Dprey P

= Bprey Dprey P

bprey

P =

pprey

bprey

dprey

sNprey Npred
1+sNprey th

Kprey


th

Dpred = dpred Npred

Bpred = bpred Npred

Dpred = dpred Npred

Bpred = ppred Nprey Npred

dt Npred

Npred
Kpred


dt Npred

Npred
Kpred

= Bpred Dpred

= Bpred Dpred
dpred

bpred

dpred

ppred

Kpred = k Nprey

(b) Holling predation

(a) Lotka-Volterra predation

Figure 9: Predation models
:assumptions
((relevant competition ?population1 ?population2))
:target-participant
((?competition-phenomenon :type phenomenon :name competition-phenomenon))
:postconditions
((competition-phenomenon ?competition-phenomenon ?population1 ?population2))
:purpose-required
((has-model ?competition-phenomenon)))

model fragments purpose-required property form (has-model ?phen).
property expresses condition model must exist respect phenomenon:
(defproperty has-model
:source-participants ((?p :type phenomenon))
:structural-conditions ((is-model-of ?p *))
:property (has-model ?p))

next two model fragments implement models (thereby satisfying has-model
purpose-required property) predation phenomenon two populations. describe
two well-known predation models: Lotka-Volterra model (1925, 1926), shown Figure 9(a), Holling model (1959), shown graphically Figure 9(b).
(defModelFragment Lotka-Volterra
:source-participants
((?predation-phenomenon :type phenomenon)
(?predator :type population)
(?predator-size :type stock)
(?predator-birth-flow :type flow)
(?predator-death-flow :type flow)
(?prey :type population)
(?prey-size :type stock)
(?prey-birth-flow :type flow)
(?prey-death-flow :type flow))
:structural-conditions
((predation-phenomenon ?predation-phenomenon ?predator ?prey)

537

fiK EPPENS & HEN

(size-of ?predator-size ?predator)
(births-of ?predator-birth-flow ?predator)
(deaths-of ?predator-death-flow ?predator)
(size-of ?prey-size ?prey)
(births-of ?prey-birth-flow ?prey)
(deaths-of ?prey-death-flow ?prey))
:assumptions
((model ?predation-phenomenon lotka-volterra))
:target-participants
((?prey-birth-rate :type variable :name birth-rate)
(?predator-factor :type variable :name predator-factor)
(?prey-factor :type variable :name prey-factor)
(?predator-death-rate :type variable :name death-rate))
:postconditions
((== ?prey-birth-flow (* ?prey-birth-rate ?prey-size))
(== ?predator-birth-flow (* ?predator-factor ?prey-size ?predator-size))
(== ?prey-death-flow (* ?prey-factor ?prey-size ?predator-size))
(== ?predator-death-flow (* ?predator-death-rate ?predator-size))
(is-model-of lotka-volterra ?predation-phenomenon)))

mentioned earlier, Lotka-Volterra model introduces growth model prey
predator populations assigning specific equations variables, describe changes
sizes predator prey populations, ?pred-change ?prey-change respectively.
Thus, satisfies purpose-required property application population-growth
model fragment ?prey ?pred populations.
(defModelFragment Holling
:source-participants
((?predation-phenomenon :type phenomenon)
(?predator :type population)
(?predator-size :type stock)
(?capacity :type variable)
(?prey :type population)
(?prey-size :type stock))
:structural-conditions
((predation-phenomenon ?predation-phenomenon ?predator ?prey)
(size-of ?predator-size ?predator)
(size-of ?prey-size ?prey)
(capacity-of ?capacity ?predator))
:assumptions
((model ?predation-phenomenon holling))
:target-participants
((?search-rate :type variable :name search-rate)
(?handling-time :type variable :name handling-time)
(?prey-requirement :type variable :name prey-requirement)
(?predation :type flow :name predation))
:postconditions
((flow ?predation ?prey-size sink)
(== ?predation
(/ (* ?search-rate ?prey-size ?predator-size)
(+ 1 (* ?search-rate ?prey-size ?handling-time))))
(== ?capacity (C-add (* ?prey-requirement ?prey)))
(is-model-of holling ?predation-phenomenon)))

Holling model employs variable denoting capacity population. variable
may introduced logistic growth model. practice, logistic growth models Holling
predation models often used conjunction. compositional modeller need aware
combinations models, however. needs know prerequisites individual
component models contained within model fragment.
538

fiC OMPOSITIONAL ODEL R EPOSITORIES

D1 = 1 N 1

B1 = b 1 N 1

dt N1

N1 +w12 N2
K1

= B1 1

d1

b1

w12
K1

D2 = 2 N 2

B2 = b 2 N 2

dt N2

w21 N1 +N2
K2

= B2 2

d2

b2

w21
K2

Figure 10: species competition model

final model fragment knowledge base implements model competition
two species. formally describes competition model type depicted Figure 10. model
fragment contains population competition model knowledge base, contain
model assumption represent model.

(defModelFragment competition
:source-participants
((?competition-phenomenon :type phenomenon)
(?population-1 :type population)
(?size-1 :type stock)
(?density-1 :type variable)
(?capacity-1 :type variable)
(?population-2 :type population)
(?size-2 :type stock)
(?density-2 :type variable)
(?capacity-2 :type variable))
:structural-conditions
((competition-phenomenon ?competition-phenomenon ?population-1 ?population-2)
(density-of ?density-1 ?size-1)
(capacity-of ?capacity-1 ?size-1)
(density-of ?density-2 ?size-2)
(capacity-of ?capacity-2 ?size-2))
:assumptions
((relevant competition ?population-1 ?population-2))
:target-participants
((?weight-12 :type variable :name weight)
(?weight-21 :type variable :name weight))
:postconditions
((== ?density-1 (C-add (/ (* ?weight-12 ?size-2) ?capacity-1)))
(== ?density-2 (C-add (/ (* ?weight-21 ?size-1) ?capacity-2)))))

539

fiK EPPENS & HEN

relevant
growth predator

Growth

Predator

predation
predator,prey1
relevant
predation
predator,prey1

Exponential

model predator
exponential

Logistic
model

Logistic

model predator
logistic

"Other"
model

othergrowth

model predator


predationphen:
predator,prey1

Growth

Stock +
Flows

predation
predator,prey2
relevant
predation
predator,prey2

Exponential
model

othergrowth

model predator


Logistic
model

Logistic

model predator
logistic

"Other"
model

Exponential

model predator
exponential

Predation

predationphen:
predator,prey2

Growth

Stock +
Flows

Prey2
relevant
growth prey2

Exponential
model

Predation
Prey1
relevant
growth prey1

Stock +
Flows

Exponential
model

othergrowth

model predator


Logistic
model

Logistic

model predator
logistic

"Other"
model

Exponential

model predator
exponential

relevant
competition
prey1,prey2

LotkaVolterra
model

LotkaVolterra

model comp.
lotkavolterra

Holling
model

Holling

model comp.
holling

LotkaVolterra
model

LotkaVolterra

model comp.
lotkavolterra

Holling
model

Holling

model comp.
holling

Competition

competitionphen:
prey1,prey2

competition
prey1,prey2

Figure 11: Model space 1 predator 2 competing prey scenario
4.2 Model space
model space constructed knowledge base instantiated respect given scenario. Consider example following scenario, describes predator population
preys two populations, prey1 prey2, whilst two prey populations compete
one another:
(defScenario pred-prey-prey-scenario
:entities ((predator :type population)
(prey1 :type population)
(prey2 :type population))
:relations ((predation predator prey1)
(predation predator prey2)
(competition prey1 prey2)))

full specification model space unwieldy present abstract graphical
representation model space scenario shown Figure 11. model space contains
following knowledge:
three populations scenario, set three population growth models
(i.e. exponential, logistic other) derived. inference dependent upon
relevance assumption population growth phenomenon, model assumption
corresponds one three population growth models.
540

fiC OMPOSITIONAL ODEL R EPOSITORIES

predation relations (i.e. (predation predator prey1) (predation
predator prey2)), populations related them, set two predation models
(i.e. Lotka-Volterra Holling) derived. inference dependent upon relevance assumption predation phenomenon model assumption corresponds
one two predation models.
competition relation (competition prey1 prey2), populations related it, competition model derived. one competition model,
inference competition model dependent upon relevance assumption
corresponds competition phenomenon.
addition hypergraph Figure 11, model space also contains number constraints
conjunctions assumptions consistent. explained earlier, stem two
sources: 1) non-composable relations 2) purpose-required properties. example given
type.
Let predation-phen-1 predation phenomenon predator prey1,
prey1-size variable representing size prey1 population. example, model fragments exponential-population-growth Lotka-Volterra
generate equation computing value variable representing change
prey1-size. equations composed, following inconsistency generated:
(relevant growth prey1) (model prey1-size exponential)
(relevant growth predator) (relevant predation predator prey1)
(model predation-phen-1 lotka-volterra)

Inconsistencies also arise purpose-required properties. example, model fragment predation-phenomenon applicable predation relation deemed relevant,
purpose-required property (has-model ?pred-phen) become condition consistency. certain combinations assumptions, property may satisfied. Say,
Holling predation exponential growth models selected, Holling model generated ?capacity (capacity ?capacity ?pred) true.
predation model created case (because Holling model fragment instantiated), even though predation phenomenon deemed relevant set assumptions.
inconsistent has-model purpose-required property predation-phenomenon
model fragment, responsible combination assumptions therefore marked nogood.
(relevant growth predator) (model predator-size exponential)
(relevant growth prey1) (model prey1-size exponential)
(relevant predation predator prey1) (model predation-phen-1 holling)

4.3 aDPCSP solution
resultant model space translated aDCSP enable selection consistent set
assumptions, using advanced CSP solution techniques. aDCSP derived model
space depicted Figure 12.
541

fiK EPPENS & HEN

Attribute
x1
x2
x3
x4
x5
x6
x7
x8
x9
x10
x11

Meaning
(relevant growth prey1)
(relevant growth prey2)
(relevant growth predator)
(relevant predation predator prey1)
(relevant predation predator prey2)
(relevant competition prey1 prey2)
(model size-1 *)
(model size-2 *)
(model size-3 *)
(model predation-phen-1 *)
(model predation-phen-2 *)
Table 4: Attribute list

Domain
D1
D2
D3
D4
D5
D6
D7
D8
D9
D10
D11

Content
{d1,y , d1,n }
{d2,y , d2,n }
{d3,y , d3,n }
{d4,y , d4,n }
{d5,y , d5,n }
{d6,y , d6,n }
{d7,l , d7,e , d7,o }
{d8,l , d8,e , d8,o }
{d9,l , d9,e , d9,o }
{d10,h , d10,lv }
{d11,h , d11,lv }

Meaning
{population,none}
{population,none}
{population,none}
{(population,population),none}
{(population,population),none}
{(population,population),none}
{logistic,exponential,other}
{logistic,exponential,other}
{logistic,exponential,other}
{Holling,Lotka-Volterra}
{Holling,Lotka-Volterra}

Table 5: aDCSP 1 predator 2 competing prey scenario: domains contents
meaning

aDCSP contains 11 attributes. listed corresponding assumption classes
table 4. first 6 attributes correspond notion relevance phenomenon: 3 population
growth phenomena, 2 predation phenomena 1 competition phenomenon precise.
5 attributes correspond 5 sets model types: 3 sets population growth models 2 sets
predation models.
assumptions attributes generated form domains values. resulting domains aforementioned attributes summarised table 5.
activity constraints aDCSP describe conditions instantiate subject
assumptions correspond attribute. Since participant relation label
model space, minimal set assumptions becomes part emerging model
available. participant relation subject assumption, label explicitly
describes sets assumptions attribute corresponds subject
542

fiC OMPOSITIONAL ODEL R EPOSITORIES

x1
d1,y

x4
d1,n

d4,y

x7
d7,l

d7,e

x6
d4,n

d6,y

x2
d6,n

d2,y

x10
d7,o

d10,lv

x5
d2,n

d5,y

x8
d10,h

d8,l

d8,e

x3
d5,n

d3,y

x11
d8,o

d11,lv

d3,n

x9
d11,h

d9,l

d9,e

d9,o

attribute
value
compatibility constraint
activity constraint

Figure 12: aDCSP derived models space reflecting 1 predator 2 competing prey
scenario

activated. translating label subject sets attribute-value assignments, antecedents activity constraints constructed.
example, relevance assumptions (attributes x1 , . . . , x6 ) take subjects
scenario, hence, always active. attributes related model assumptions
population growth active corresponding assumptions denoting relevance population
growth true. is,
x1 : d1,y active(x7 )
x2 : d2,y active(x8 )
x3 : d3,y active(x9 )
attributes related assumptions predation models active corresponding
assumptions denoting relevance predation, assumptions describing relevance population growth, true populations involved predation relation. is,
x1 : d1,y x3 : d3,y x4 : d4,y active(x10 )
x2 : d2,y x3 : d3,y x5 : d5,y active(x11 )
Figure 12 shows graphical representation activity constraints.
compatibility constraints correspond directly inconsistencies nogood node.
inconsistencies discussed previous section depicted Figure 12.
aDCSP constructed, preferences may attached attribute-value assignments.
Suppose preferences assigned standard population modelling choices, i.e. expo543

fiK EPPENS & HEN

Attribute
x1 , . . . , x5
x6
x7
x8
x9
x10
x11

Preference assignments
preference assignments
P (x6 : d6,y ) = pcompetition
P (x7 : d7,l ) = plogistic , P (x7 : d7,e ) = pexponential
P (x8 : d8,l ) = plogistic , P (x8 : d8,e ) = pexponential
P (x9 : d9,l ) = plogistic , P (x9 : d9,e ) = pexponential
P (x10 : d10,h = pholling , P (x10 : d10,lv ) = plotka-volterra
P (x11 : d11,h = pholling , P (x11 : d11,lv ) = plotka-volterra

Table 6: Preference assignments 1 predator 2 competing prey problem
nential growth, logistic growth, lotka-volterra predation holling predation, relevance
competition (because one type model implemented phenomenon).
example, following BPQs could employed:
pexponential < plogistic
plotka-volterra < pholling
pcompetition
logistic Holling models preferred exponential Lotka-Volterra models former generally regarded accurate. Note preferences
ordered way corresponding different phenomena related one
another. justification ordering that, even though models structurally connected
(there restrictions models combined one another), models different phenomena inherently describe behaviours compared one another. preference
assignments attribute value assignments summarised table 6.
Solving aDPCSP simple. First, attributes x1 , . . . , x6 activated.
attributes assigned xi : di,y assignment maximises potential preference. Then,
attributes x7 , . . . , x11 activated. Here, attributes x7 , . . . , x9 assigned xi : di,l
logistic growth model highest preference. Finally, x 10 x11 assigned x10 : d10,h
x11 : d11,h Holling models highest preference inconsistent
logistic model committed earlier. resulting solution satisfies following set assumptions:
{(relevant growth prey1),
(relevant growth prey2),
(relevant growth predator),
(relevant competition prey1 prey2),
(relevant predation predator prey1),
(relevant predation predator prey2),
(model size-1 logistic),
(model size-2 logistic),
(model size-3 logistic),
(model predation-phen-1 holling),
(model predation-phen-2 holling)}

544

fiC OMPOSITIONAL ODEL R EPOSITORIES

SYMBOLS
relevant
growth predator

Growth

Nodes entailed
aDPCSP solution

Predator

predation
predator,prey1

Exponential

model predator
exponential

Logistic
model

Logistic

model predator
logistic

"Other"
model

othergrowth

model predator


Nodes entailed
aDPCSP solution
Applied model fragment

Predation

predationphen:
predator,prey1

LotkaVolterra
model

LotkaVolterra

model comp.
lotkavolterra

Growth

Stock +
Flows

Holling
model

Holling

model comp.
holling

Prey1

predation
predator,prey2
relevant
predation
predator,prey2

Exponential
model

othergrowth

model predator


Logistic
model

Logistic

model predator
logistic

"Other"
model

Exponential

model predator
exponential

Predation

predationphen:
predator,prey2

LotkaVolterra
model

LotkaVolterra

model comp.
lotkavolterra

Growth

Stock +
Flows

Holling
model

Holling

model comp.
holling

Prey2
relevant
growth prey2

Exponential
model

Model fragment
applied

relevant
predation
predator,prey1

relevant
growth prey1

Assumptions
aDPCSP solution

Stock +
Flows

Exponential
model

othergrowth

model predator


Logistic
model

Logistic

model predator
logistic

"Other"
model

Exponential

model predator
exponential

relevant
competition
prey1,prey2

Competition

competitionphen:
prey1,prey2

competition
prey1,prey2

Figure 13: Deducing scenario model model space, given set assumptions

4.4 Sample scenario model
Figure 13 shows scenario model deduced set assumptions exploiting model space. nodes corresponding aforementioned assumptions
logically follow assumption set indicated Figure.
combining participants relations resulting scenario model, model given
Figure 14 drawn. model corresponds one ecologist would draw
logistic growth Holling predation models regarded appropriate task hand.

5. Conclusion Future Work
article presented novel approach compositional modelling enables construction
models ecological systems. work differs existing approaches automatically
translates compositional modelling problem aDCSP (order-of-magnitude) preference valuations. several benefits method.
use translation algorithm converts compositional modelling problem
aDCSP allows criteria formalised. importantly, also enables efficient, existing
future, aDCSP solution techniques effectively applied solving compositional modelling
problems.
545

fiGrowth

Growth

B1 = b1 N1

D1 = 1 N 1 1

dt N1

B2 = b2 N2

D2 = 2 N 2 2

dt N2

= B1 D1 P31

= B2 D2 P32

Holling

K EPPENS & HEN

s31

K1

d1

1 =

N1
K1

+

P32 =

b2
s32

w12 N2
K1

th,31

K3

Logistic

3 =

K2

w21 N1
K2

+

N2
K2

Logistic

Growth

B3 = b 3 N 3

b3

2 =

th,32

N3
K3

D3 = 3 N 3 3

dt N3

d2

s32 N2 N3
1+s32 N2 th,32

546

P31 =

b1

Holling

s31 N1 N3
1+s31 N1 th,31

= B3 3

d3
Logistic

Figure 14: Sample scenario model 1 predator 2 competing prey scenario

fiC OMPOSITIONAL ODEL R EPOSITORIES

extension aDCSPs (order-of-magnitude) preferences (to form aDPCSPs) also
permits incorporation softer requirements compositional modelling problem.
paper, order-of-magnitude preferences employed express appropriateness alternative model types certain phenomena. considerations may described hard
constraints physical systems domain3 , subjective less understood problem
domains, ecological modelling domain. approach presented herein provides means
capture represent subtlety flexible model design decisions.
theoretical ideas presented article applied real-world ecological modelling problems. paper, demonstrated resultant compositional modeller
employed create repository population dynamics models. approach also
applied automated model construction large complex ecosystems MODMED
model Mediterranean vegetation (Legg et al., 1995), reported Keppens (2002).
practical theoretical issues need addressed, however. practical side, types ecological model design decisions, represented assumptions
assumption classes, supported inference mechanisms, extended. Ecological
systems tend involve interrelated populations individuals, instead functional compositions
individual components physical systems. One particularly important type design decision
ecological modelling therefore granularity. requires introduction novel representation formalisms inference mechanisms aggregation disaggregation. Initial work
considering populations single entities dividing entities sub-populations
necessary carried (Keppens & Shen, 2001a). Integration work present
aDPCSP framework requires investigation.
theoretical side, analysis complexity present approach rather informal.
Much remains done regard, especially comparing complexity existing
compositional modellers. comparison, additional work required adapt current translation procedure suit existing compositional modelling problems. compositional
modellers exponential complexity, however. employ problem-specific solution algorithms, little known opportunities improving efficiency. work hopes
first step toward understanding important issue.

Acknowledgments
work partly supported UK-EPSRC grant GR/S63267. first author also
supported College Science Engineering scholarship University Edinburgh.
grateful Robert Muetzelfeldt helpful discussions assistance research
reported, whilst taking full responsibility views expressed here. Thanks also go
anonymous referees constructive comments useful revising earlier
version paper.

References
Binger, B., & Hoffman, E. (1998). Microeconomics Calculus. Longman.
3. so-called operating conditions, stating range values certain variables within use
certain assumptions permitted.

547

fiK EPPENS & HEN

Bistarelli, S., Montanari, U., & Rossi, F. (1997). Semiring-based constraint satisfaction optimization. Journal ACM, 44(2), 201236.
Bobrow, D., Falkenhainer, B., Farquhar, A., Fikes, R., Forbus, K., Gruber, T., Iwasaki, Y., & Kuipers,
B. (1996). compositional modeling language. Proceedings 10th International
Workshop Qualitative Reasoning Physical Systems, pp. 1221.
Bradley, E., Easley, M., & Stolle, R. (2001). Reasoning nonlinear system identification.
Artificial Intelligence, 133, 139188.
Dague, P. (1993a). Numeric reasoning relative orders magnitude. Proceedings
National Conference Artificial Intelligence, pp. 541547.
Dague, P. (1993b). Symbolic reasoning relative orders magnitude. Proceedings
13th International Joint Conference Artificial Intelligence, pp. 15091514.
de Kleer, J. (1986). assumption-based TMS. Artificial Intelligence, 28, 127162.
de Kleer, J. (1988). general labeling algorithm assumption-based truth maintenance.
Proceedings 7th National Conference Artificial Intelligence, pp. 188192.
Easley, M., & Bradley, E. (1999). Generalized physical networks automated model building.
Proceedings 16th International Joint Conference Artificial Intelligence, pp. 1047
1053.
Falkenhainer, B., & Forbus, K. (1991). Compositional modeling: finding right model
job. Artificial Intelligence, 51, 95143.
Ford, A. (1999). Modeling Environment - Introduction System Dynamics Modeling
Environmental Systems. Island Press.
Forrester, J. (1968). Principles Systems. Wright-Allen Press, Cambridge, MA, USA.
Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determination
minimal cost paths. IEEE Transactions Systems, Science Cybernetics, SSC-4(2), 100
107.
Heller, U., & Struss, P. (1998). Diagnosis therapy recognition ecosystems - usage modelbased diagnosis techniques. Proceedings 12th International Symposium Computer
Science Environment Protection.
Heller, U., & Struss, P. (2001). Transformation qualitative dynamic models - application hydroecology. Hotz, L., Struss, P., & Guckenbienl, T. (Eds.), Intelligent Diagnosis Industrial
Applications, pp. 95106. Shaker Verlag.
Holling, C. (1959). characteristics simple types predation parasitism. Canadian
Entomologist, 91, 385398.
Karnopp, D., Margolis, D., & Rosenberg, R. (1990). System Dynamics: United Approach (Second
Edition edition). John Wiley & Sons, Inc.
Keppens, J. (2002). Compositional Ecological Modelling via Dynamic Constraint Satisfaction
Order-of-Magnitude Preferences. Ph.D. thesis, University Edinburgh.
Keppens, J., & Shen, Q. (2001a). Disaggregation compositional modelling ecological systems
via dynamic constraint satisfaction. Proceedings 15th International Workshop
Qualitative Reasoning Physical Systems, pp. 2128.
548

fiC OMPOSITIONAL ODEL R EPOSITORIES

Keppens, J., & Shen, Q. (2001b). compositional modelling. Knowledge Engineering Review,
16(2), 157200.
Keppens, J., & Shen, Q. (2002). supporting dynamic constraint satisfaction order magnitude preferences. Proceedings 16th International Workshop Qualitative Reasoning Physical Systems, pp. 7582.
Langley, P., Sanchez, J., Todorovski, L., & Dzeroski, S. (2002). Inducing process models
continuous data. Proceedings 19th International Conference Machine Learning,
pp. 347354.
Legg, C., Muetzelfeldt, R., & Heathfield, D. (1995). Modelling vegetation dynamics mediterranean ecosystems: Issues scale. Proceedings 39th Symposium International
Association Vegetation Science.
Levy, A., Iwasaki, Y., & Fikes, R. (1997). Automated model selection simulation based
relevance reasoning. Artificial Intelligence, 96, 351394.
Lotka, A. (1925). Elements physical biology. Williams & Wilkins Co., Baltimore.
Malthus, T. (1798). essay principle population. Printed J. Johnson St. Pauls
Church Yard, London, England.
Miguel, I., & Shen, Q. (1999). Hard, flexible dynamic constraint satisfaction. Knowledge
Engineering Review, 14(3), 199220.
Miguel, I., & Shen, Q. (2001a). Solution techniques constraint satisfaction problems: Advanced
approaches. Artificial Intelligence Review, 15(4), 269293.
Miguel, I., & Shen, Q. (2001b). Solution techniques constraint satisfaction problems: Foundations. Artificial Intelligence Review, 15(4), 243267.
Minton, S., Johnston, M., Philips, A., & Laird, P. (1992). Minimizing conflicts: heuristic repair
method constraint satisfaction scheduling problems. Artificial Intelligence, 58, 161
205.
Mittal, S., & Falkenhainer, B. (1990). Dynamic constraint satisfaction problems. Proceedings
8th National Conference Artificial Intelligence, pp. 2532.
Nayak, P., & Joskowicz, L. (1996). Efficient compositional modeling generating causal explanations. Artificial Intelligence, 83, 193227.
Nicholson, A., & Bailey, V. (1935). balance animal populations. Proceedings Zoological Society London, 1, 551598.
Raphael, B. (1990). A* algorithm. Shapiro, S.C. (Ed.), Encyclopedia Artificial Intelligence,
Vol. 1, pp. 13. John Wiley & Sons.
Rickel, J., & Porter, B. (1997). Automated modeling complex systems answer prediction
questions. Artificial Intelligence, 93, 201260.
Rogers, D. (1972). Random search insect population models. Journal Animal Ecology, 41,
369383.
Schiex, T., Fargier, H., & Verfaillie, G. (1995). Valued constraint satisfaction problems: Hard
easy problems. Proceedings 14th International Joint Conference Artificial Intelligence, pp. 631637.
549

fiK EPPENS & HEN

Thompson, W. (1929). relative value parasites predators biological control
insect pests. Bull. Etnomol. Res., 19, 343350.
Todorovski, L., & Dzeroski, S. (1997). Declarative bias equation discovery. Proceedings
14th International Conference Machine Learning, pp. 432439.
Todorovski, L., & Dzeroski, S. (2001). Using domain knowledge population dynamics modeling equation discovery. Proceedings 12th European Conference Machine
Learning, pp. 478490.
Tsang, E. (1993). Foundations Constraint Satisfaction. Academic Press, London San Diego.
Verfaillie, G., & Schiex, T. (1994). Solution reuse dynamic constraint satisfaction problems.
Proceedings 12th National Conference Artificial Intelligence, pp. 307312.
Verhulst, P. (1838). Recherches mathematiques sur la loi daccroissement de la population. Nouveaux memoires de lacademie royale des sciences et belles-lettres de Bruxelles, 18, 138.
Volterra, V. (1926). Fluctuations abundance species considered mathematically. Nature,
118, 558560.

550

fiJournal Artificial Intelligence Research 21 (2004) 319356

Submitted 11/02; published 03/04

Representation Dependence Probabilistic Inference
Joseph Y. Halpern

halpern@cs.cornell.edu

Cornell University, Computer Science Department
Ithaca, NY 14853
http://www.cs.cornell.edu/home/halpern

Daphne Koller

koller@cs.stanford.edu

Stanford University, Computer Science Department
Stanford, CA 94035
http://www.cs.stanford.edu/ koller

Abstract
Non-deductive reasoning systems often representation dependent: representing
situation two different ways may cause system return two different answers. viewed significant problem. example, principle
maximum entropy subjected much criticism due representation dependence. has, however, almost work investigating representation dependence.
paper, formalize notion show problem specific maximum entropy. fact, show representation-independent probabilistic inference
procedure ignores irrelevant information essentially entailment, precise sense.
Moreover, show representation independence incompatible even weak default assumption independence. show invariance restricted class
representation changes form reasonable compromise representation independence desiderata, provide construction family inference procedures
provides restricted representation independence, using relative entropy.

1. Introduction
well known way problem represented significant impact
ease people solve it, complexity algorithm solving
it. interested arguably even fundamental issue: extent
answers get depend input represented. too,
well known work, particularly Tversky Kahneman (see, example, (Kahneman,
Slovic, & Tversky, 1982)), showing answers given people vary significantly
(and systematic ways) depending question framed. phenomenon often
viewed indicating problem human information processing; implicit assumption
although people make mistakes sort, shouldnt. hand,
competing intuition suggests representation (and ) matter;
representation dependence natural consequence fact.
consider one type reasoning, probabilistic inference, examine extent
answers depend representation. issue representation dependence
particular interest context interest using probability knowledge
representation (e.g., (Pearl, 1988)) probabilistic inference source
c
2004
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiHalpern & Koller

many concerns expressed regarding representation. However, approach
applicable far generally.
begin noting notion probabilistic inference two quite different
interpretations. one interpretation, forms basis Bayesian paradigm,
probabilitic inference consists basically conditioning: start prior distribution event space, condition whatever observations obtained.
interpretation, given set probabilistic assertions, goal
reach conclusions probabilities various events. paper,
focus latter interpretation, although discuss relationship Bayesian
approach Section 7.2.
Suppose procedure making inferences probabilistic knowledge
base. sensitive way knowledge represented? Consider following
examples, use perhaps best-known non-deductive notion probabilistic inference,
maximum entropy (Jaynes, 1978).1
Example 1.1: Suppose information whatsoever regarding whether
object colorful. probability assign proposition colorful ? Symmetry
arguments might suggest 1/2. Since information, seems object
likely colorful non-colorful. also conclusion reached
maximum entropy provided language proposition colorful .
suppose know colors red, blue, green, propositions corresponding
colors. Moreover, colorful actually mean red blue green.
case, maximum entropy dictates probability red blue green 7/8. Note that,
cases, conclusion follows constraints trivial one:
probability query somewhere 0 1.
Example 1.2: Suppose told half birds fly. two reasonable ways represent information. One propositions bird fly,
use knowledge base KB fly
1 =def [Pr(fly | bird ) = 1/2]. second might
basic predicates bird flying-bird , use knowledge base KB fly
2 =def [(flying-bird
bird ) Pr(flying-bird | bird ) = 1/2]. Although first representation may appear natural, seems representations intuitively adequate insofar representing
information given. use inference method maximum
entropy, first representation leads us infer Pr(bird ) = 1/2, second leads us
infer Pr(bird ) = 2/3.
Examples basis frequent criticisms maximum entropy
grounds representation dependence. pointing examples,
little work problem. fact, work Salmon (1961,
1963) Paris (1994), seems work formalizing notion representation dependence. One might say consensus was: whatever representation
independence is, property enjoyed maximum entropy.
1. Although much discussion motivated representation-dependence problem encountered
maximum entropy, understanding maximum entropy works essential
understanding discussion.

320

fiRepresentation Dependence

inference procedures it? paper attempt understand notion
representation dependence, study extent achievable.
study representation dependence, must first understand mean
representation. real world complex. reasoning process, must focus
certain details ignore others. semantic level, relevant distinctions captured
using space X possible alternatives states (possible worlds). Example 1.1,
first representation focused single attribute colorful . case, two
states state space, corresponding colorful true false, respectively.
second representation, using red , blue, green, richer state space. Clearly,
distinctions could make.
also interpret representation syntactic entity. case, typically capture relevant distinctions using formal language. example, use propositional
logic basic knowledge representation language, choice primitive propositions
characterizes distinctions chosen make. take states
truth assignments propositions. Similarly, use probabilistic representation language belief networks (Pearl, 1988) knowledge representation
language, must choose set relevant random variables. states
possible assignments values variables.
mean shift representation (i.e., state space) X another representation ? Roughly speaking, want capture level state space shift
from, say, feet meters. Thus, X distances might described terms feet
might described terms meters. would expect constraint
relating feet meters. constraint would give extra information X;
would relate worlds X worlds . Thus, first attempt capture representation independence somewhat indirectly, requiring adding constraints relating X
place constraints X result different conclusions X.
resulting notion, called robustness, turns surprisingly strong. show
every robust inference procedure must behave essentially like logical entailment.
try define representation independence directly, using mapping f
one representation another. example, f could map world individual
6 feet tall corresponding world individual 1.83 meters tall.
obvious constraints f necessary ensure corresponds intuition
representation shift. define representation-independent inference procedure
one preserves inferences every legitimate mapping f ; i.e., KB ,
KB | iff f (KB ) | f ().
definition turns somewhat reasonable first attempt,
exist nontrivial representation-independent inference procedures. However, still
strong notion. particular, representation-independent inference procedure must
act essentially like logical entailment knowledge base objective information
(i.e., essentially non-probabilistic information). Moreover, show representation
independence incompatible even simplest default assumption independence.
Even told nothing propositions p q, representation independence
allow us jump conclusion p q independent.
results suggest want inference procedures capable jumping
nontrivial conclusions, must accept least degree representation de321

fiHalpern & Koller

pendence. add support claim choice language carry great
deal information, complete representation independence much expect.
positive note, show use intuition choice language
carries information get limited forms representation independence. idea
language put constraints counts appropriate representation
shift. example, suppose certain propositions represent colors others represent
birds. may willing transform colorful red blue green, may
willing transform red sparrow . reason demand inference procedure behave way suddenly shift wildly inappropriate representation,
symbols mean something completely different. provide general approach
constructing inference procedures invariant specific class representation
shifts. construction allows us combine degree representation independence
certain non-deductive properties want inference procedure. particular, present inference method supports default assumption independence,
yet invariant natural class representation shifts.
rest paper organized follows. Section 2, define probabilistic inference procedures characterize them. Section 3, define robust inference procedures
show every robust inference procedure essentially entailment. Section 4,
define representation independence, show representation independence
strong requirement. particular, show representation-independent inference
procedure essentially acts like logical entailment objective knowledge bases
representation independence incompatible default assumption independence.
Section 5 contains general discussion notion representation independence
reasonable assume choice language affect inference.
may indeed seem reasonable assume choice language affect
inference, point assumption consequences might view
unfortunate. Section 6, discuss limited forms representation independence
achieved. discuss related work Section 7, conclude Section 8.

2. Probabilistic Inference
begin defining probabilistic inference procedures. discussed introduction,
two quite different ways term used. one, given prior
distribution probability space; knowledge typically consists events
space, used condition distribution obtain posterior.
other, focus work, probabilistic inference procedure takes input
probabilistic knowledge base returns probabilistic conclusion.
take knowledge base conclusion assertions probabilities events measurable space (X, FX ), measurable space consists
set X algebra FX subsets X (that is, FX set subsets X closed
union complementation, containing X itself).2 Formally, assertions viewed
statements (or constraints on) probability measures (X, FX ). example,
2. X infinite, may want consider countably-additive probability measures take FX
closed countable unions. issue play significant role paper. simplicity,
restrict finite additivity require FX closed finite unions.

322

fiRepresentation Dependence

FX , statement Pr(S) 2/3 holds distributions probability
least 2/3. Therefore, (X,FX ) set probability measures (X, FX ) (that
is, probability measures domain FX ), view knowledge base set
constraints (X,FX ) . FX clear context, often omit notation,
writing X rather (X,FX ) .
place restrictions language used express constraints.
assume includes assertions form Pr(S) subsets FX rational
[0, 1], closed conjunction negation, KB KB 0
knowledge bases expressing constraints, KB KB 0 KB . (However,
langauge could include many assertions besides obtained starting assertions
form Pr(S) closing conjunction negation.) Since language
puts constraints probability measures, cannot directly say FX must hold.
closest approximation language assertion Pr(S) = 1. Thus, call
constraints objective. knowledge base consisting objective constraints called
objective knowledge base. Since Pr(T1 ) = 1 Pr(T2 ) = 1 equivalent Pr(T1 T2 ) = 1,
without loss generality, objective knowledge base consists single constraint
form Pr(T ) = 1. Given knowledge base KB placing constraints X , write |= KB
measure X satisfies constraints KB . use [[KB ]]X denote
measures satisfying constraints.
practice, knowledge typically represented syntactically, using logical language describe possible states. Typical languages include propositional logic, firstorder logic, language describing values set random variables. general,
base logic L defines set formulas L() given vocabulary . propositional
logic, vocabulary simply set propositional symbols. probability theory,
vocabulary consist set random variables. first-order logic, vocabulary
set constant symbols, function symbols, predicate symbols. facilitate comparison
vocabularies, assume base logic vocabularies finite
subsets one fixed infinite vocabulary .
working language, assume state state space defines
interpretation symbols hence formulas L(). case
propositional logic, thus assume associate state truth assignment
primitive propositions . first-order logic, assume associate
state domain interpretation symbols . probabilistic
setting, assume associate state assignment values
random variables. often convenient assume state space fact
subset W W(), set interpretations (or assignments to) vocabulary .
Note truth formula L() determined state. true
state w, write w |= .
probabilistic extension Lpr () base logic L() simply set probability
formulas L(). Formally, L(), Pr() numeric term. formulas
Lpr () defined Boolean combinations arithmetic expressions involving
numeric terms. example, Pr(fly | bird ) 1/2 formula Lpr ({fly, bird }) (where
interpret conditional probability expression Pr( | ) Pr( )/ Pr()
multiply clear denominator). analogy constraints, formula form
Pr() = 1 called objective formula.
323

fiHalpern & Koller

Given set W W(), assume FW algebra consisting sets
form [[]]W = {w : w |= }, L(). (In case propositional logic,
consists finite set primitive propositions, FW = 2W . case first-order logic,
sets necessarily definable formulas, FW may strict subset 2W .)
Let probability measure (W, FW ). ascribe semantics Lpr ()
probability space (W, FW , ) straightforward way. particular, interpret
numeric term Pr() ({w W : w |= }). Since formula L() describes event
space W , formula Lpr () clearly constraint measures W . write
|= measure W satisfies formula .
syntactic knowledge base KB Lpr () viewed constraint W
obvious way. Formally, KB represents set probability measures [[KB ]] W ,
consists measures W |= KB .
say KB (whether syntactic semantic) consistent [[KB ]]X 6= , i.e.,
constraints satisfiable. Finally, say KB entails (where another set
constraints X ), written KB |=X , [[KB ]]X [[]]X , i.e., every measure satisfies
KB also satisfies . write |=X satisfied every measure X . omit
subscript X |= clear context.
Entailment well-known weak method drawing conclusions
knowledge base, particular respect treatment irrelevant information.
Consider knowledge base consisting constraint Pr(fly | bird ) 0.9. Even
though know nothing suggest red relevant, entailment allow us
reach nontrivial conclusion Pr(fly | bird red ).
One way get powerful conclusions consider, measures satisfy
KB , subset them. Intuitively, given knowledge base KB , inference procedure
picks subset measures satisfying KB , infers holds subset. Clearly,
conclusions hold every measure subset hold every measure
entire set.
Definition 2.1 : (X, FX )-inference procedure partial function : 2(X,FX ) 7
2(X,FX ) I(A) (X,FX ) I(A) = iff = 2(X,FX )
domain (i.e., defined). write KB |I I([[KB ]]X ) [[]]X .
FX clear context irrelevant, often speak X-inference procedures.
remark Paris (1994) considers calls inference processes.
inference procedures defined that, given set probability measures,
return unique probability measure (rather arbitrary subset A). Paris
gives number examples inference processes. also considers various properties
inference process might have. closely related various properties
representation indepedence consider. discuss Pariss work Section 7.
Entailment X-inference procedure defined sets determined taking
identity. Maximum entropy also inference procedure sense.
Definition 2.2: Given probability measure finite space X (where sets
P
measurable), entropy H() defined xX (x) log (x). (The log taken
(A) consist measures
base 2 here.) Given set measures X , let IX
324

fiRepresentation Dependence

highest entropy measures whose entropy least high
measure A; measures, inf (A) undefined.
easy see inf (A) defined closed (in topological sense; i.e.,
n sequence probability measures n converges , A). Thus,
could take domain inf
X consist closed sets measures X .
also open sets inf (A) defined, although defined
open sets A. example, suppose X = {x1 , x2 } let = { : (x1 ) < 1/2}. Let 0
0 (x1 ) = 1/2. easy check H(0 ) = 1, H() < 1 A.
However, , H() > 1 . follows
measure whose entropy higher measure A, inf (A)
undefined. hand, A0 = { : (x1 ) < 2/3}, measure whose
entropy maximum open set A0 , namely measure 0 .
are, course, many inference procedures besides entailment maximum entropy defined measurable space. fact, following proposition shows,
binary relation | satisfying certain reasonable properties inference procedure
type.
Proposition 2.3: X-inference procedure following properties hold
every KB , KB 0 , , X KB domain I.
Reflexivity: KB |I KB .
Left Logical Equivalence: KB logically equivalent KB 0 , i.e., |= KB KB 0 ,
every KB |I iff KB 0 |I .
Right Weakening: KB |I |= KB |I .
And: KB |I KB |I , KB |I .
Consistency: KB consistent KB |6I false.
Proof: Straightforward definitions.
Interestingly, properties commonly viewed part core reasonable
properties nonmonotonic inference relation (Kraus, Lehmann, & Magidor, 1990).
would like also prove converse, showing relation | probabilistic
constraints space X satisfies five properties must form
|IX . quite true, following example shows.
Example 2.4: Fix measurable space (X, FX ). Let language consist (finite)
Boolean combination statements form Pr(S) , FX . fix one
nonempty strict subset S0 X, let n statement Pr(S0 ) 1/n. Define
inference procedure | follows. KB equivalent true (i.e, [[KB ]]X 6= X ),
KB | iff KB |= . hand, true | iff n |= sufficiently large
n. is, true | exists N n N , n |= .
easy check five properties Proposition 2.3 hold |. However, | |I
X-inference procedure I. suppose were. Note n |= n m,
325

fiHalpern & Koller

true | m. Thus, must I(X ) [[n ]]X n. follows
IX (X ) [[Pr(S0 ) = 0]]X , true |I Pr(S0 ) = 0. However, n 6|= Pr(S) = 0 n,
true | Pr(S) = 0. contradicts assumption | = |I .
Essentially need get converse Proposition 2.3 infinitary version
V
Rule, would say KB |I i, KB |I . language
closed infinite conjunctions, rule would fact need.
Since assumed language closed infinite conjunctions, use
variant rule.
Infinitary And: set statements, KB |I |= ,
KB |I .
Proposition 2.5: Let | relation probabilistic constraints X
properties Reflexivity, Left Logical Equivalence, Right Weakening, Infinitary And, Consistency hold KB domain | . (That is, KB domain |
KB | , KB | KB , on.) | |I X-inference
procedure I.
Proof: See Appendix A.1.
typically interested inference procedure defined one space X,
family related inference procedures, defined number spaces. example,
entailment inference procedure defined spaces X; maximum entropy
defined finite measurable spaces (X, 2X ).
Definition 2.6 : X set measurable spaces, X -inference procedure set
{I(X,FX ) : (X, FX ) X }, I(X,FX ) (X, FX )-inference procedure (X, FX ) X .
sometimes talk X -inference procedure I, write KB |I (X, FX )
X clear context. However, stressed that, formally, X -inference procedure really set inference procedures (typically related natural way).
Clearly entailment X -inference procedure X , IX simply identity
function X X . X consists finite measurable spaces sets measurable,
maximum entropy X -inference procedure. typically denote inference
procedure | . Thus, KB | holds probability measures maximum
entropy satisfying KB .
Important assumptions: remainder paper, deal X inference procedures X satisfies two richness assumptions. assumptions
hold standard inference procedures considered.
assume X closed crossproducts, (X, FX ), (Y, FY ) X ,
(X Y, FXY ) X , FXY algebra formed taking finite unions
disjoint sets form , FX FY . easy see
algebra, since = (S ) (S 0 0 ) =
(S 0 ) (T 0 ) (from also follows union sets
326

fiRepresentation Dependence

written disjoint union). Note X finite sets, FX = 2X ,
FY = 2Y , FX FY = 2XY . shall see, (X Y, FXY ) X
(X, FX ) (Y, FY ) X allows us relate constraints X constraints
natural way.
assume X contain sets finite cardinalities; precisely, n 2,
exists set (X, FX ) X |X| = n FX = 2X . assumption
actually needed results, since assumption X closed
crossproducts already implies that, finite n, exists measurable
space (X, FX ) X |X| n; already suffices prove results
paper. However, assuming X sets cardinalities make
proofs easier.
also want domain satisfy certain assumptions, defer stating
assumptions introduced additional definitions notation.

3. Robustness
order define robustness representation shifts, must first define notion
representation shift. first attempt definition based idea using
constraints specify relationship two vocabularies. example, Example 1.1, might X = {colorful , colorless} = {red , blue, green, colorless}.
specify relationship X via constraint asserts colorful
(red blue green).
course, every constraint legitimate mapping representations.
example, formula asserted colorful obviously legitimate representation
shift. minimum, must assume constraint give additional
information X far logical inference goes. syntactic level, use
following definition. Given knowledge base KB Lpr (), say Lpr ( 0 )
-conservative KB if, formulas Lpr (), KB |= iff KB |= .
Thus, adding knowledge base permit additional logical inferences
vocabulary . inference procedure robust unaffected conservative
extensions; is, KB , Lpr (), KB |I iff KB |I
-conservative KB . Roughly speaking, says getting new information
uninformative far logical inference goes affect default conclusions.
formal definition robustness, uses semantic rather syntactic concepts,
extends intuitions arbitrary constraints measures (not ones
expressed language Lpr ).
Definition 3.1: X1 ...Xn , define Xi Xi taking Xi (A) = (X1
Xi1 Xi+1 Xn ). constraint Xi viewed constraint
X1 ...Xn taking [[]]X1 ...Xn = { X1 ...Xn : Xi |= }. frequently identify
constraints Xi constraints X1 . . . Xn way. B X1 Xn , define
proj Xi (B) = {Xi : B}. constraint X1 Xn said Xi -conservative
constraint KB Xi proj Xi ([[KB ]]X1 Xn ) = [[KB ]]Xi .
327

fiHalpern & Koller

see definition generalizes earlier language-oriented definition, note
KB constraints X constraint XY , KB |= iff
proj 1 ([[KB ]]XY ) [[]]X , KB |= iff [[KB ]]X [[]]X .
Definition 3.2: {IX : X X } robust X -inference procedure spaces X, X ,
constraints KB X , constraints XY X-conservative
KB , KB |IX iff KB |IXY . (Note definition implicitly assumes
X X X, X , assumption made explicit earlier.)
first glance, robustness might seem like reasonable desideratum. all,
adding constraint XY places restrictions X change conclusions might reach X? Unfortunately, turns definition
deceptively strong, disallows interesting inference procedures. particular,
one property may hope inference procedure draw nontrivial conclusions
probabilities events, is, conclusions follow entailment.
example, maximum entropy (or inference procedure based symmetry) conclude
Pr(p) = 1/2 empty knowledge base. show inference procedures
robust really allow much way nontrivial conclusions
probabilities events.
Definition 3.3: (X, FX )-inference procedure essentially entailment knowledge base KB X FX , KB |I < Pr(S) < KB |= Pr(S) .
essentially entailment X essentially entailment knowledge bases KB
domain IX .
Thus, entailment lets us conclude Pr(S) [, ], inference procedure
essentially entailment lets us draw slightly stronger conclusion Pr(S) (, ).
prove this, need make three assumptions domain I. (For results,
need assumptions domain I.)
DI1. Pr(S) domain I(X,FX ) FX , , IR.
DI2. KB domain IX , also domain IXY (when KB
viewed constraint XY .)
DI3. KB 1 KB 2 domain IX , KB 1 KB 2 .
Note sets form Pr(S) closed sets. certainly seems reasonable
require sets domain inference procedure; correspond
basic observations. DI2 seems quite innocuous; observed earlier, want
able view constraints X constraints XY ,
prevent domain I. DI3 also seems reasonable assumption,
since KB 1 KB 2 correspond possible observations, want able draw
conclusions combining observations. DI3 holds domain consists
closed sets. note hold take domain consist
sets measure whose entropy maximum. example, X = {x1 , x2 },
= {0 } { : (x1 ) > 3/4}, B = { : (x1 ) 2/3}, 0 (x0 ) = 1/2,
B measure whose entropy maximum, B measure
whose entropy maximum.
328

fiRepresentation Dependence

Theorem 3.4: {IX : X X } robust X -inference procedure satisfies DI1, DI2,
DI3, IX essentially entailment X X .
Proof: See Appendix A.2.
possible construct robust inference procedures almost quite
entailment, simply strengthening conclusions Pr(S) [, ] Pr(S)
(, ). Clearly, however, robust inference procedure extremely limited ability
jump conclusions. next section, look definition seems closer
intuitive notion representation independence, somewhat reasonable
consequences.

4. Representation Independence
4.1 Representation shifts
X two different representations phenomena then, intuitively,
way relating states X corresponding states . want
correspondence respect logical structure events. Formally, require
homomorphism respect complementation intersection.
Definition 4.1: (X, FX )-(Y, FY ) embedding f function f : FX 7 FY
f (S ) = f (S) f (T ) f (S) = f (S) S, FX .
elsewhere, talk X-Y embeddings rather (X, FX )-(Y, FY ) embeddings
FX FY play significant role.
goal consider effect transformation probabilistic formulas. Hence,
interested sets states probabilities.
Definition 4.2: f X-Y embedding, X , , correspond
f (S) = (f (S)) events FX . define mapping f : 2X 7 2Y
follows. first define f singleton sets (except that, convenience, write f ()
rather f ({}) taking f () = { : (f (S)) = (S) FX }. Thus,
f () consists measures correspond f . arbitrary
subset 2X , define f (D) = f () X .
constraint X expressed language, typically write f () rather
f ([[]]X ). implicitly assume language constraint f () also
expressible. hard see f () constraint results replacing every
set FX appears f (S).
Example 4.3: Example 1.1, might X = {colorful , colorless} = {red , blue,
green, colorless}. case, might f (colorful ) = {red , blue, green} f (colorless) =
{colorless}. Consider measure X (colorful ) = 0.7 (colorless) =
0.3. f () set measures total probability assigned set
states {red , blue, green} 0.7. Note uncountably many measures.
easy check constraint X Pr(colorful ) > 3/4, f ()
Pr({red , blue, green}) > 3/4.
329

fiHalpern & Koller

Embeddings viewed semantic analogue syntactic notion interpretation defined (Enderton, 1972, pp. 157162), also used recent
literature abstraction (Giunchiglia & Walsh, 1992; Nayak & Levy, 1995). Essentially,
interpretation maps formulas vocabulary formulas different vocabulary
mapping primitive propositions (e.g., colorful ) formulas (e.g.,
red blue green) extending complex formulas obvious way. representation shift Example 1.2 also captured terms interpretation, one
taking flying-bird fly bird .
Definition 4.4: Let two vocabularies. propositional case, interpretation function associates every primitive proposition p
formula i(p) L(). complex definition spirit applies first-order
vocabularies. example, R k-ary predicate, i(R) formula k free
variables.
Given interpretation i, get syntactic translation formulas L() formulas
L() using obvious way; example, i((p q) r) = (i(p) i(q)) i(r)
(see (Enderton, 1972) details). Clearly interpretation induces
embedding f W1 W() W2 W(): map [[]]W1 [[i()]]W2 .
course, embeddings count legitimate representation shifts. example,
consider embedding f defined terms interpretation maps propositions p q proposition r. process changing representations using
f gives us information p q equivalent, information might
originally. Intuitively, f gives us new information telling us certain
situationthat p q holdsis possible.
formally, embedding f
following undesirable property: maps set states satisfying p q
empty set. means state p q holds analogue new
representation. want disallow embeddings.
Definition 4.5: X-Y embedding f faithful if, S, FX , iff
f (S) f (T ).
definition desired consequence disallowing embeddings give new
information far logical consequence goes.
Lemma 4.6: X-Y embedding f faithful constraints KB ,
KB |= iff f (KB ) |= f ().
Proof: See Appendix A.3.
clear embedding Example 4.3 faithful: f (colorful ) = {red , blue, green}
f (colorless) = colorless. following proposition gives insight faithful
embeddings.
Proposition 4.7: Let f faithful X-Y embedding. following statements
equivalent:
(a) correspond f ;
330

fiRepresentation Dependence

(b) formulas , |= iff |= f ().
Proof: See Appendix A.3.
embedding f reasonable representation shift, would like inference
procedure return answers shift representations using f .
Definition 4.8: X, X , X -inference procedure {IX : X X } invariant
X-Y embedding f constraints KB X , KB |IX iff
f (KB ) |IY f (). (Note that, particular, means KB domain | IX
iff f (KB ) domain | IY .)
Definition 4.9: X -inference procedure {IX : X X } representation independent
invariant faithful X-Y embeddings X, X .
Since embedding Example 4.3 faithful, representation-independent inference procedure would return answers Pr(colorful ) Pr(red blue green).
issue somewhat subtle Example 1.2. There, would like embedding f generated interpretation i(flying-bird ) = fly bird i(bird ) = bird .
faithful embedding, since flying-bird bird valid formula,
i(flying-bird bird ) (fly bird ) bird valid. Looking problem semantically, see state corresponding model flying-bird bird holds
mapped . clearly source problem. According linguistic intuitions domain, legitimate state. Rather considering states
W({flying-bird , bird }), perhaps appropriate consider subset X consisting truth assignments characterized formulas {flying-bird bird , flying-bird
bird , flying-bird bird }. use embed X W({fly, bird }), resulting embedding indeed faithful. So, previous example, invariance
embedding would guarantee get answers representations.
4.2 Representation-independent inference procedures
Although definition representation independence seems natural, definition
robustness. two definitions relate other? First, show representation independence weaker notion robustness. result, need
consider inference procedures satisfy two assumptions.
DI4. f faithful X-Y embedding, KB domain IX iff f (KB )
domain IY .
DI5. KB domain IXY , f faithful X-Y embedding, 1 constraint
X , KB (1 f (1 )) domain IXY .
DI4 natural satisfied standard inference procedures. easy
check KB closed iff f (KB ) closed. DI5 may appear natural,
hold domains consisting closed sets, since hard check f (1 )
closed. DI5 would follow DI3 assumption f (1 ) domain
IXY , actually weaker combination two assumptions.
particular, holds domain consisting sets measure
maximum entropy.
331

fiHalpern & Koller

Theorem 4.10: X -inference procedure robust satisfies DI2, DI4, DI5,
representation independent.
Proof: See Appendix A.3.
already shown robust inference procedure must almost trivial.
interesting representation-independent inference procedures? shall see,
answer mixed. nontrivial representation-independent inference procedures,
interesting.
first result shows representation independence, like robustness, trivializes
inference procedure, knowledge bases.
Theorem 4.11: {IX : X X } representation-independent X -inference procedure
then, X X , IX essentially entailment objective knowledge bases
domain.3
Proof: See Appendix A.3.
Corollary 4.12: {IX : X X } representation-independent X -inference procedure,
KB objective, KB |I < Pr(S) < 0 1, = 0
= 1.
result tells us objective knowledge base Pr(T ) = 1, reach
three possible conclusions set S. S, conclude Pr(S) = 1;
S, conclude Pr(S) = 0; otherwise, strongest conclusion
make Pr(S) somewhere 0 1.
construct representation-independent inference procedure entailment precisely behavior restrict attention countable state spaces. Suppose X countable. Given objective knowledge base KB form Pr(T ) = 1,
FX , let KB + consist formulas form 0 < Pr(S) < 1
nonempty strict subsets FX .4 define X-inference procedure
0 follows: KB equivalent objective knowledge base, KB |
IX
I0
KB KB + |= ; KB equivalent objective knowledge base, KB |I 0
0 indeed inference procedure.
KB |= . follows easily Proposition 2.5 IX
Moreover, equivalent standard notion entailment; example,
true |I 0 0 < Pr(p) < 1, 6|=0 < Pr(p) < 1. Nevertheless, prove 0
representation independent.
0 : X X } representationLemma 4.13: Let X consist countable sets. {IX
independent X -inference procedure.

3. earlier version paper (Halpern & Koller, 1995), claimed representationindependent inference procedure satisfied minimal irrelevance property (implied robustness,
equivalent it) essentially entailment knowledge bases. Jaeger (1996) shows,
inference procedure along lines 1 described constructed show result
correct. seem need full strength robustness.
4. requirement X countable necessary here. X uncountable every singleton FX ,
KB + inconsistent uncountable. impossible uncountable
collection points positive measure.

332

fiRepresentation Dependence

Proof: See Appendix A.3.
objective knowledge bases may appear interesting restrict propositional languages, languages include first-order statistical information
become quite interesting. Indeed, shown (Bacchus, 1990; Bacchus, Grove, Halpern, &
Koller, 1996), knowledge bases first-order (objective) statistical information allow
us express great deal information naturally encounter. example,
express fact 90% birds fly objective statement number
flying birds domain relative overall number birds. course, Theorem 4.11
applies immediately knowledge bases.
Theorem 4.11 also implies various inference procedures cannot representation
independent. particular, since true | Pr(p) = 1/2 primitive proposition p,
follows maximum entropy essentially entailment. observation provides
another proof maximum entropy representation independent.
consistent Theorem 4.11 representation-independent inference
procedures almost entailment probabilistic knowledge bases. example,
1 defined follows. Given , exists
consider X-inference procedure IX
X
1 (A) = { : (S) 1/3};
FX = { X : (S) 1/4}, IX
X
1 (A) = A. Thus, Pr(S) 1/4 | Pr(S) 1/3. Clearly, 1 essentially
otherwise, IX
X
I1
entailment. Yet, prove following result.

Lemma 4.14: Suppose X consists measure spaces form (X, 2X ),
1 : X X } representation-independent X -inference procedure.
X finite. {IX

Proof: See Appendix A.3.
Note follows Theorem 3.4 1 cannot robust. Thus, shown
representation independence strictly weaker notion robustness.
example might lead us believe representation-independent inference procedures interesting probabilistic knowledge bases. However,
show, representation-independent inference procedure cannot satisfy one key desideratum: ability conclude independence default. example, important feature
maximum-entropy approach nonmononotic reasoning (Goldszmidt, Morris, & Pearl,
1993) ability ignore irrelevant information, implicitly assuming independence. course, maximum entropy satisfy representation independence.
result shows approach probabilistic reasoning simultaneously assure
representation independence default assumption independence.
try give general notion default assumption independence here,
since need result. Rather, give minimal property would
hope inference procedure might have, show property sufficient preclude
representation independence. Syntactically, property want
disjoint vocabularies, KB Lpr (), L(), L(), KB |I Pr( ) =
Pr() Pr().
333

fiHalpern & Koller

Definition 4.15: X -inference procedure {IX : X X } enforces minimal default independence if, whenever X X , KB constraint X domain |IX ,
FX , FY , KB |IXY Pr(S ) = Pr(S) Pr(T ).5
definition clearly generalizes syntactic definition.
Clearly, entailment satisfy minimal default independence. Maximum entropy,
however, does. Indeed, semantic property implies minimal default independence
used Shore Johnson (1980) one axioms axiomatic characterization
maximum-entropy.
Theorem 4.16: {IX : X X } X -inference procedure enforces minimal default
independence satisfies DI1, IX representation independent.
Proof: See Appendix A.3.
result interesting far irrelevance concerned. might hope
learning irrelevant information affect conclusions. attempt
define irrelevance here, certainly would expect KB 0 vocabulary disjoint
KB , then, example, KB |I Pr() = iff KB KB 0 |I Pr() = . KB 0
objective, standard probabilistic approach would identify learning KB 0
conditioning KB 0 . Suppose restrict inference procedures indeed
condition objective information (as case class inference procedures
consider Section 6). KB KB 0 |I Pr() = exactly KB |I Pr( | KB 0 ) = .
Thus, Theorem 4.16 tells us inference procedures condition new (objective)
information cannot representation independent ignore irrelevant information.
Thus, although representation independence, unlike robustness, force us use
entirely trivial inference procedures, prevent us using procedures
certain highly desirable properties.

5. Discussion
results suggest type representation independence hard come by.
also raise concern perhaps definitions quite right. provide
seems even support latter point.
Example 5.1: Let Q unary predicate c1 , . . . , c100 , constant symbols. Suppose
two vocabularies = {Q, d} = {Q, c1 , . . . , c100 , d}. Consider
interpretation i(d) = i(Q(x)) = Q(x) Q(c1 ) . . . Q(c100 ).
Now, consider KB = xQ(x). case, i(KB ) = x(Q(x) Q(c1 ) . . . Q(c100 ).
Intuitively, since ci may refer domain element, conclusion
make certainty Q(c1 ) . . . Q(c100 ) exists least one Q
domain, gives us additional information beyond KB . convert example
general argument embedding f corresponding faithful. Intuitively,
5. Since working space X , KB viewed constraint XY here, Pr(S)
understood Pr(S ), Pr(T ) understood Pr(X ). Recall that,
assumption, X X .

334

fiRepresentation Dependence

KB , get conclusion Q(c1 ) . . . Q(c100 ) f (KB ) Q(x) appears
positively KB ; but, case, already know least one Q,
gain new information embedding. seem unreasonable
inference procedure assign different degrees belief Q(d) given KB = xQ(x)
one hand given i(KB ) = x(Q(x) Q(c1 ) . . . Q(c100 )) other,6 particularly
domain small. fact, many reasoning systems explicitly adopt unique names
assumption, would clearly force different conclusions two situations.
example suggests that, least first-order case, even faithful embeddings
always match intuition reasonable representation shift. One might
therefore think perhaps problem definition even propositional
case. Maybe totally different definition representation independence avoids
problems. possible, believe case. techniques
used prove Theorem 4.16 3.4 seem apply reasonable notion
representation independence.7 give flavor type argument used prove
theorems, consider Example 1.1, assume true |I Pr(colorful ) = (0, 1).8
Using embedding g g(colorful ) = red , conclude true |I Pr(red ) = .
Similarly, conclude Pr(blue) = Pr(green) = . order |I
invariant original embedding, must true |I Pr(red blue green) = ,
completely inconsistent previous conclusions. embeddings use
argument natural ones; would want definition representation
independence disallowed them.
results viewed support position representation dependence
justified; choice appropriate representation encodes significant information.
particular, encodes bias knowledge-base designer world. Researchers
machine learning long realized bias inevitable component effective
inductive reasoning (i.e., learning evidence). completely surprised
turns types leaping conclusions (as context) also depend
bias.
need little careful here. example, cases identify
vocabulary (and hence, representation) sensors agent
disposal. may seem unreasonable agent temperature sensor
motion sensor might carve world differently agent color sensor
distance sensor. consider two agents different sensors yet
made observations. Suppose talk distance tree.
reasonable two agents reach different conclusions distance
different sensors (and thus use different vocabularies), although
made observations? would follow agents change
conclusions switched sensors, despite made observations.
seem reasonable!
Bias representation independence viewed two extremes spectrum.
accept knowledge base encodes users bias, obligation
6. Actually, i(Q(d)) = Q(d) Q(c1 ) . . . Q(c100 ), latter equivalent Q(d) given KB .
7. certainly applied many definitions tried!
8. fact, suffices assume true |I Pr(colorful ) [, ], long > 0 < 1.

335

fiHalpern & Koller

invariant representation shifts all. hand, assume
representation used carries information, coherence requires inference procedure
give answers equivalent representations. believe right answer lies somewhere between. typically number reasonable ways
represent information, might want inference procedure return
conclusions matter choose. thus makes sense require
inference procedure invariant embeddings take us one reasonable representation another. follow must invariant
embeddings, even embeddings syntactically similar ones wish
allow. may willing refine colorful red blue green define flying-bird
fly bird , transform red sparrow . next section, show
construct inference procedures representation independent limited class
representation shifts.

6. Selective invariance
discussed above, want construct inference procedure invariant
certain embeddings. purposes section, restrict attention finite
spaces, sets measurable. is, focus X -inference procedures
X consists measure spaces form (X, 2X ), X finite.
first step understand conditions X -inference procedure
invariant specific X-Y embedding f . conclude KB X ?
Recall inference procedure IX picks subset DX = IX (KB ), concludes iff
holds every measure DX . Similarly, applied f (KB ) , IY picks subset
DY = IY (f (KB )). invariant f respect KB ,
tight connection DX DY .
understand connection, first consider pair measures X .
Recall Proposition 4.7 correspond f iff, formulas ,
|= iff |= f (). understand correspondence extends sets probability
measures, consider following example:
Example 6.1: Consider embedding f Example 4.3, let DX = {, 0 }
above, 0 (colorful ) = 0.6. guarantee reach corresponding
conclusions DX DY ? Assume, example, DY contains measure
correspond either 0 , e.g., measure assigns probability 1/4
four states. case, conclusion Pr(colorful ) 0.7 holds DX ,
holds measures; corresponding conclusion Pr(red blue green) 0.7
hold DY . Therefore, every probability measure DY must correspond
measure DX . Conversely, every measure DX must correspond measure DY .
suppose measure DY corresponding . get conclusion
Pr(blue red green) 6= 0.7 DY , corresponding conclusion Pr(colorful ) 6= 0.7
follow DX . Note two conditions imply DY must
precisely set measures corresponding measures DX . particular, might
DY containing single measure corresponding (and least one corresponding
0 ), e.g., one (red ) = 0.5, (blue) = 0, (green) = 0.2, (colorless) = 0.3.
336

fiRepresentation Dependence

Based example, use following extension definition correspondence.
Definition 6.2: say DX DY correspond f DY , exists
corresponding DX (so (S) = (f (S)) X), DX ,
exists corresponding DY .
Proposition 6.3: Suppose f faithful X-Y embedding, DX X , DY .
following two conditions equivalent:
(a) DX DY correspond f ;
(b) , DX |= iff DY |= f ().9
Proof: See Appendix A.4.
produce inference procedure invariant X-Y embedding f ,
must ensure every KB , IX (KB ) IY (KB ) correspond. first glance,
seems rather difficult guarantee correspondence every knowledge base. turns
situation bad. remainder section, show how, starting
correspondence knowledge base truethat is, starting correspondence
IX (X ) IY (Y )we bootstrap correspondence KB s, using
standard probabilistic updating procedures.
First consider problem updating objective information. standard way
update via conditioning. measure X event X, define
|S measure assigns probability (w)/(S) every w S, zero
states. set measures DX X , define DX |S {|S : DX }.
following result easy show.
Proposition 6.4: Let X event let f faithful X-Y embedding.
correspond f , |S |f (S) also correspond f .
Proof: Almost immediate definitions; left reader. (In case, note
result follows Theorem 6.4 below.)
Clearly, result extends sets measures.
Corollary 6.5: f faithful X-Y embedding, DX DY correspond f ,
DX |S DY |f (S) also correspond f .
want update constraint objective? standard extension
conditioning case via relative entropy KL-divergence (Kullback & Leibler,
1951).
9. (a) implies (b) arbitrary spaces, implication (b) (a) depends restriction
finite spaces made section. suppose X natural numbers N , f identity, DX
consists probability measures N , DY consists measures measure 0
0 (n) = 1/2n+1 . language consists finite Boolean combinations assertions form
Pr(S) , N , easy see DX |= iff DY |= formulas , clearly DX
DY correspond identity map.

337

fiHalpern & Koller

Definition 6.6: 0 measures X, relative entropy 0 ,
P
denoted KLX (0 k), defined xX 0 (x) log(0 (x)/(x)). measure X
constraint , let | denote set measures 0 satisfying KLX (0 k)
minimal.
Intuitively, KL-divergence measures distance 0 . measure 0 satisfying KLX (0 k) minimal thought closest measure
satisfies . denotes objective constraint, unique measure satisfying KLX (0 k) minimal conditional measure | (Kullback & Leibler,
1951). (That deliberately used notation conditioning.)
Moreover, easy show KLX (0 k) = 0 iff 0 = . follows ,
| = .
Given set measure DX X constraint X , define DX | DX |.
apply well-known result (see, e.g., (Seidenfeld, 1987)) generalize Proposition 6.4 case relative entropy.
Theorem 6.7: Let arbitrary constraint X . f faithful X-Y embedding
correspond f , | |f () also correspond f .
Proof: See Appendix A.4.
Again, result clearly extends sets measures.
Corollary 6.8: f faithful X-Y embedding, DX DY correspond f ,
DX | DY |f () also correspond f .
results give us way bootstrap invariance. construct inference procedure uses relative entropy starting set prior probability measures. Intuitively, encode users prior beliefs domain. information comes in,
measures updated using cross-entropy. design priors certain invariances hold, Corollary 6.8 guarantees invariances continue hold throughout
process.
Formally, prior function P X maps X X set P(X) probability measures
P (KB ) = P(X)|KB . Note
X . Define inference procedure P taking IX
P (true) = P(X), constraints all, use P(X) basis
IX
inference. standard inference procedures form P
prior function P. fairly straightforward verify, example, entailment P
P(X) = X . (This because, observed earlier, |KB = KB .) Standard
Bayesian conditioning (defined objective knowledge bases) form, take
P(X) single measure space X. interestingly, well known (Kullback
& Leibler, 1951) maximum entropy Pu Pu (X) singleton set containing
uniform prior X.
say robustness P representation shifts? Using Proposition 6.3 Corollary 6.5, easy show want P invariant
set F embeddings, must ensure prior function right
correspondence property.
Theorem 6.9: f faithful X-Y embedding, P invariant f iff P(X)
P(Y ) correspond f .
338

fiRepresentation Dependence

Proof: See Appendix A.4.
Theorem 6.9 sheds light maximum entropy inference procedure.
mentioned, | precisely inference procedure based prior function Pu .
corollary asserts | invariant f precisely uniform priors X
correspond f . shows maximum entropys lack representation
independence immediate consequence identical problem uniform prior.
class F embeddings maximum entropy invariant? Clearly,
answer yes. easy see embedding takes elements X (disjoint)
sets equal cardinality correspondence property required Theorem 6.9. follows
maximum entropy invariant embeddings. fact, requirement
maximum entropy invariant subset embeddings one axioms
Shore Johnsons (1980) axiomatic characterization maximum-entropy. (We remark
Paris (1994, Theorem 7.10) proves maximum entropy satisfies variant
atomicity principle; invariance result essentially special case Theorem 6.9.)
like behavior maximum entropy representation shifts, Theorem 6.9 provides solution: simply start different prior function.
want maintain invariance representation shifts, P(X) must include
non-extreme priors (i.e., measures X (A)
/ {0, 1}

/ {, X}). set priors gives essential entailment inference procedure. If,
however, prior knowledge embeddings encode reasonable representation shifts, often make smaller class priors, resulting inference
procedure prone leap conclusions. Given class reasonable embeddings F, often find prior function P closed f F, i.e.,
measure P(X) X-Y embedding f F make sure
corresponding measure P(Y ), vice versa. Thus, guarantee P
appropriate structure using process closing f F.
course, also execute process reverse. Suppose want support
certain reasoning pattern requires leaping conclusions. classical example
reasoning pattern is, course, default assumption independence.
representation independence get without losing reasoning pattern?
show, Theorem 6.9 gives us answer.
begin providing one plausible formulation desired reasoning pattern.
finite space X, say X1 Xn product decomposition X X =
X1 Xn n largest number X written product way.
(It easy see X finite, maximal product decomposition unique.)
measure X product measure X X1 Xn product decomposition
X exist measures Xi = 1, . . . , n = 1 n ,
Q
is, (U1 Un ) = ni=1 (Ui ), Ui Xi , = 1, . . . , n. Let P set product
measures X. P prior relative entropy rule used update prior
given knowledge base, |P satisfies form minimal default independence.
fact, easy show satisfies following stronger property.
339

fiHalpern & Koller

Proposition 6.10: Suppose X1 Xn product decomposition X and,
= 1, . . . , n, KB constraint Xi , Si subset Xi .
n
^

KB |IP Pr(S1 . . . Sn ) =

i=1



n


Pr(Si ).

i=1

Proof: See Appendix A.4.
Theorem 4.16 shows |P cannot invariant embeddings. Theorem 6.9
tells us invariant precisely embeddings P invariant.
embeddings characterized syntactically natural way. Suppose 1 , . . . , n
partition finite set primitive propositions. Note truth assignment
primitive propositions viewed crossproduct truth assignments
primitive propositions 1 , . . . , n . identification, suppose set X truth
assignments decomposed X1 Xn , Xi consists truth assignments
. case, p j q, r k j 6= k, true |P Pr(p q) =
Pr(p) Pr(q), since since q r subset, true |P Pr(r
q) = Pr(r) Pr(q). Hence, P invariant interpretation maps p
r maps q itself. Intuitively, problem crossing subset boundaries;
mapping primitive propositions different subsets subset.
restrict interpretations thatpreserve subset boundaries, avoid problem.
get semantic characterization follows. product decomposition
X X1 Xn product decomposition Y1 Yn , f
X-Y product embedding f X-Y embedding Xi -Yi embeddings fi ,
= 1, . . . , n, f (hx1 , . . . , xn i) = f1 (x1 ) fn (xn ). Product embeddings capture
intuition preserving subset boundaries; elements given subset Xi remain
subset (Yi ) embedding. However, notion product embedding
somewhat restrictive; requires elements ith subset X map elements
ith component , = 1, . . . , n. still preserve default independence
components product permuted. g permutation embedding exists
permutation {1, . . . , n} g(hx1 , . . . , xn i) = hx(1) , . . . , x(n) i.
Theorem 6.11: inference procedure IP invariant faithful product embeddings
permutation embeddings.
Theorem 6.9 thus provides us basic tools easily define inference procedure
enforces minimal default independence constraints involving disjoint parts
language, time guaranteeing invariance large natural class
embeddings. Given negative result Theorem 4.16, type result best
could possibly hope for. general, Theorem 6.9 provides us principled
framework controlling tradeoff strength conclusions
reached inference procedure invariance representation shifts.

7. Related Work
mentioned earlier, two types probabilistic inference. partition
discussion related work along lines.
340

fiRepresentation Dependence

7.1 Probabilistic Inference Knowledge Base
Given importance representation reasoning, fact one main criticisms maximum entropy sensitivity representation shifts, surprising
little work problem representation dependence. Indeed,
best knowledge, work focused representation independence
logical sense considered prior Salmon Paris.
Salmon (1961) defined criterion linguistic invariance, seems essentially equivalent notion representation independence. tried use criterion defend
one particular method inductive inference but, pointed Barker commentary end (Salmon, 1961), preferred method satisfy criterion either.
Salmon (1963) attempted define modified inductive inference method would
satisfy criterion clear attempt succeeded. case, results
show modified method certainly cannot representation independent sense.
said earlier, Paris (1994) considers inference processes, given constraint
X , choose unique measure satisfying constraint. considers various properties
inference process might have. Several closely related properties
considered here. (In describing notions, made inessential
changes able express notation.)
X -inference process language invariant X, X constraints KB
X , KB |IX iff KB |IXY . Clearly language invariance
special case robustness. Paris shows center mass inference process
(that, given set X , chooses measure center mass A)
language invariant; hand, well known maximum entropy
language invariant.
X -inference process satisfies principle irrelevant information
spaces X, X , constraints KB X , constraints ,
KB |IX iff KB |IXY . Again, special case robustness, since
constraint must X-conservative. Paris shows maximum entropy
satisfies principle. (He restricts domain maximum entropy process
closed convex sets, always unique probability measure maximizes
entropy.)
X -inference process satisfies renaming principle if, whenever X
finite spaces, g : X isomorphism, f : 2X 2Y faithful embedding
based g (in f (S) = {g(s) : S}), constraints KB X ,
KB |IX iff f (KB ) |IY f (). Clearly, renaming principle special
case representation independence. Paris shows number inference processes
(including maximum entropy) satisfy renaming principle.
X -inference process satisfies principle independence if, whenever X, ,
Z X , FX , FY , U FZ , KB constraint Pr(U ) =
Pr(S|U ) = b Pr(T |U ) = c, > 0, KB | Pr(S |U ) = bc. Ignoring conditional probabilities, clearly special case minimal default
independence. Paris Vencovska (1990) show maximum entropy unique
341

fiHalpern & Koller

inference process satisfying number principles, including renaming, irrelevant
information, independence.
X -inference process satisfies atomicity principle if, X, Y1 , . . . , Yn
X , whenever f 0 embedding {0, 1} X, f obvious extension
f 0 embedding {0, 1} Y1 . . . Yn X Y1 . . . Yn ,
constraints KB {0,1}Y1 ...Yn , KB |IX iff f (KB ) |IY f ().
Clearly atomicity special case representation independence. Paris shows
inference process satisfies atomicity. argument similar spirit
used prove Theorems 4.11 4.16, much simpler, since inference
processes return unique probability measure, set them.
recently, Jaeger (1996), building definitions, examined representation
independence general nonmonotonic logics. considers representation independence
respect collection transformations, proves results degree
certain nonmonotonic formalisms, rational closure (Lehmann & Magidor,
1992), satisfy representation independence.
Another line research relevant representation independence work
abstraction (Giunchiglia & Walsh, 1992; Nayak & Levy, 1995). Although goal
work make connections two different ways representing
situation, significant differences focus. work abstraction, two ways
representing situation expected equivalent. Rather, one representation
typically abstracts away irrelevant details present other. hand,
treatment issues terms deductive entailment, terms general
inference procedures. would interesting combine two lines work.
7.2 Bayesian Probabilistic Inference
Bayesian statistics takes different perspective issues discuss paper.
discussed, Bayesian approach generally assumes construct prior, use
standard probabilistic conditioning update prior new information obtained.
approach, representation knowledge obtained effect conclusions.
Two pieces information semantically equivalent (denote event)
precisely effect used condition distribution.
paradigm, analysis directly related step precedes
probabilistic conditioningthe selection prior. specific beliefs
want encode prior distribution (as do, example, constructing
Bayesian network), design prior reflect beliefs terms vocabulary
used. example, particular distribution mind location object,
encode one way representing space terms Cartesian coordinates,
another way using polar coordinates. effect, view representation
transformation embedding f , two priors corresponding f ,
sense Definition 4.2. Thus, design prior already takes representation
account.
hand, trying construct uninformed prior class
problems, issue representation independence becomes directly relevant. Indeed,
342

fiRepresentation Dependence

standard problems maximum entropy arise even simple case
simply Bayesian conditioning starting uniform prior space.
standard approach Bayesian statistics use invariance certain transformations order define appropriate uninformed prior. example, might
want prior images invariant rotation translation. certain cases,
specify transformation want measure invariant,
measure uniquely determined (Jaynes, 1968; Kass & Wasserman, 1993). case,
argument goes, uniquely determined measure perforce right one. idea
picking prior using invariance properties spirit approach take
Section 6. Indeed, approach simply uses standard probabilistic conditioning
objective information (such observations), Bayesian approach uninformed
prior invariant set embeddings is, sense, special case. However, approach
force us choose unique prior. Rather, allow use set prior
distributions, allowing us explore wider spectrum inference procedures.
approach also related work Walley (1996), observes representation independence important desideratum certain statistical applications involving
multinomial data. Walley proposes use sets Dirichlet densities encode ignorance
prior, shows approach representation independent domain
application.

8. Conclusions
paper takes first step towards understanding issue representation dependence probabilistic reasoning, defining notions invariance representation independence, showing representation independence incompatible drawing many
standard default conclusions, defining limited notions invariance might
allow compromise desiderata able draw interesting conclusions
(not already entailed evidence) representation independence. focus
inference probabilistic logic, notion representation independence
important many contexts. definitions clearly extended
non-probabilistic logics. mentioned, Jaeger (1996) obtained results representation independence general setting, clearly much
done. generally, would interest understand better tension
representation independence strength conclusions drawn
inference procedure.

Acknowledgments
Thanks Ed Perkins pointing us (Keisler & Tarski, 1964) and, particular,
result countably additive probability measure defined subalgebra algebra
F could necessarily extended countably additive probability measure F.
Thanks reviewers paper perceptive comments pointing
(Horn & Tarski, 1948). Much Halperns work paper done
IBM Almaden Research Center. recent work supported NSF
343

fiHalpern & Koller

grant IRI-96-25901 IIS-0090145 ONR grant N00014-01-1-0795.
Kollers work done U.C. Berkeley. research sponsored part Air
Force Office Scientific Research (AFSC), Contract F49620-91-C-0080,
University California Presidents Postdoctoral Fellowship. Daphne Kollers later work
paper supported generosity Powell foundation, ONR
grant N00014-96-1-0718. preliminary version appears Proceedings IJCAI 95,
pp. 18531860.

Appendix A. Proofs
A.1 Proofs Section 2
Proposition 2.5: Let | relation probabilistic constraints X
properties Reflexivity, Left Logical Equivalence, Right Weakening, Infinitary And, Consistency hold KB domain | . (That is, KB domain | ,
KB | , KB | KB , on.) | |I X-inference
procedure I.
Proof: Define follows. X , KB domain | , = [[KB ]]X
statement KB , domain I(A) = {[[]]X : KB | }.
Note Left Logical Equivalence, well defined, since = [[KB 0 ]]X ,
{[[]]X : KB | } = {[[]]X : KB 0 | }. 6= [[KB ]]X statement KB ,
domain I. remains check X-inference procedure (i.e.,
I(A) I(A) = iff = domain I), | = |I .
check X-inference procedure, suppose domain I. Thus,
= [[KB ]]X Reflexivity, easily follows I([[KB ]]X ) [[KB ]]X . Next suppose
I([[KB ]]X ) = . follows {[[]]X : KB | } = . Thus, { : KB | } |= false.
Infinitary rule, must KB |I false. Consistency Rule, follows
[[KB ]]X = . Thus, indeed X-inference procedure. Finally, note KB |
then, definition I, I([[KB ]]X ) [[]]X , KB |I . opposite inclusion, note
KB |I , { : KB | } |= . Thus, Infinitary rule, follows
KB | .
A.2 Proofs Section 3
prove Theorem 3.4, need following lemma.
Lemma A.1: Given two spaces X0 X1 , measures 0 (X0 ,FX0 ) 1 (X1 ,FX1 ) ,
subsets S0 FX0 S1 FX1 0 (S0 ) = 1 (S1 ), exists measure
2 (X0 X1 ,FX0 X1 ) 2Xi = , = 1, 2, 2 (S0 S1 ) = 1.10
Proof: B FX0 FX1 , define
2 (A B) = (0 (A S0 )1 (B S1 )/1 (S1 )) + (0 (A S0 )1 (B S1 )/1 (S1 )),
take 0 (A S0 )1 (B S1 )/1 (S1 ) = 0 1 (S1 ) = 0 take 0 (A S0 )1 (B
S1 )/1 (S1 ) = 0 1 (S1 ) = 0. Extend disjoint unions sets additivity. Since
10. B sets, use notation B denote set (A B) (A B).

344

fiRepresentation Dependence

sets FX0 X1 written disjoint unions sets form B FX0 FX1 ,
suffices define 2 . see 2 actually measure, note 2 (X ) =
0 (S0 ) + 0 (S0 ) = 1. Additivity clearly enforced definition. Finally, see
2 desired properties, suppose 1 (S1 ) 6= 0 1 (S1 ) 6= 0. (The argument
easier case; leave details reader.)
2X0 (A) = 2 (A ) = 0 (A S0 )1 (S1 )/1 (S1 ) + 0 (A S0 1 (S1 )/1 (S1 )
= 0 (A S0 ) + 0 (A S0 ) = 0 (A).
Since 0 (S0 ) = 1 (S1 ) assumption (and 0 (S0 ) = 1 (S1 )),
2X1 (B) = 2 (X B) = 0 (S0 )1 (B S1 )/1 (S1 ) + 0 (S0 )1 (B S1 )/1 S1 )
= 1 (B S1 ) + 1 (B S1 ) = 1 (B).
completes proof.
Theorem 3.4: {IX : X X } robust X -inference procedure satisfies DI1, DI2,
DI3, IX essentially entailment X X .
Proof: Suppose {IX : X X } robust IX essentially entailment X X .
must constraint KB X set FX KB |I < Pr(S) <
KB 6|= Pr(S) . Thus, must
/ [, ] KB Pr(S) =
consistent. assume without loss generality < (otherwise replace
S).
first construct space Y0 X subsets U1 , . . . , Un following properties:
(a) measure Y0 (Ui ) > , = 1, ..., n.
(b) i, measure 0i Y0 0i (Ui ) = 1 0i (Uj ) >
j 6= i.
proceed follows. Choose n < (d 1)/(n 1) < d/n < .
assumption, exists Y0 X |Y0 | = n!/(n d)!. Without loss generality,
assume Y0 consists tuples form (a1 , . . . , ad ), ai
distinct, 1 n. Let Ui consist tuples Y0 somewhere
subscript; easy see d(n 1)!/(n d)! tuples. Suppose
probability measure Y0 . easy see (U1 ) + + (Un ) = d, since
tuple Y0 exactly Ui gets counted exactly times, sum
probabilities tuples 1. Thus, cannot (Ui ) > d/n (and, fortiori,
cannot (Ui ) > i). takes care first requirement. Next, consider
probability distribution 0i makes tuples making Ui equally probable,
gives tuples probability 0. easy see 0i (Ui ) = 1. Moreover,
since straightforward check exactly d(d 1)(n 2)!/(n d)! tuples
Ui Uj j 6= i, 0i (Uj ) = [d(d 1)(n 2)!/(n d)!]/[d(n 1)!/(n d)!] =
(d 1)/(n 1). takes care second requirement.
assumption, also measurable space X |Y | = 2. Suppose
= {y, 0 }. Let Z = X n Y0 n , n n chosen
construction Y0 . Again, assumption, Z X . = 1, . . . , n,
345

fiHalpern & Koller

X, let Ai = X i1 X ni Y0 n Z.
let KB = { Z : Xi KB };
let Yi subset n ith copy replaced {y};
let Vi subset Z form X n Ui Yi (where U1 , . . . , Un subsets
Yi constructed above).
Let following constraint Z :
KB 1 . . . KB n Pr(S1 V1 ) = 1 . . . Pr(Sn Vn ) = 1.
Let Xi denote ith copy X Z. is, ease exposition, view Z
form X1 Xn Y0 , although Xi identical, since
helpful able refer specific Xi . claim Xi -conservative KB ,
= 1, . . . , n. Thus, must show proj Xi ([[KB ]]Z ) = [[KB ]]X . immediate
proj Xi ([[KB ]]Z ) [[KB ]]X . opposite inclusion, suppose [[KB ]]X .
must show exists [[KB ]]Z Xi = . proceed
follows.
Let 00 measure Y0 00 (Ui ) = 1 00 (Uj ) > , j 6= i.
construction Uj s, measure must exist. j {1, . . . , n}, let 0j measure
0i (y) = (S) j 6= i, 0j (y) = /00 (Uj ) (and 0j (y 0 ) = 1 0j (y)).
Let 0 measure Y0 n crossproduct 00 , 01 , . . . , 0n . is,
0 (T0 Tn ) = 00 (T0 ) 0n (Tn ). construction, 0 (Vj ) = j 6=
0 (Vi ) = (S).
assumption, measure 0 X 0 |= KB Pr(S) = .
proceed inductively define measure k X k Y0 n (a) Pr((S1
V1 ) . . . (Sk Vk )) = 1, (b) jY = 0 jXj = j = 1, . . . , k. define 0 = 0 .
inductive step, simply apply Lemma A.1. Finally, take n .
construction guarantees X j = , hence |= KB j . addition, construction
guarantees |= Pr(S1 V1 ) = 1 . . . Pr(Sn Vn ) = 1. Hence |= , desired.
follows DI1, DI2, DI3 domain IZ . Since KB
equivalent , follows KB also domain IZ . Now, robustness,
constraint Xi , KB |I iff KB |I . Since KB |I Pr(Si ) >
KB equivalent , follows |I Pr(Si ) > = 1, . . . , n. rule
(Proposition 2.3), follows |I Pr(S1 ) > . . . Pr(Sn ) > . Since |= Pr((S1
V1 ) (Sn Vn )) = 1, easily follows |I Pr(U1 ) > . . . Pr(Un ) > .
construction guarantees Pr(U1 ) > . . .Pr(Un ) > inconsistent. Thus, |I false.
robustness, follows KB |I false. happen KB |= false,
implies KB |= Pr(S) , contradicting original assumption.
A.3 Proofs Section 4
prove Lemma 4.6, useful first prove two additional results:
Lemma A.2: f X-Y embedding, f (X) = f () = .
346

fiRepresentation Dependence

Proof: Suppose f X-Y embedding. first show f () = .
definition embedding, follows f () = f (X ) = f (X) f (). Thus, f () f (X).
definition embedding also implies f () = f (X) = f (X). Thus,
f (X) f (X). happen f (X) = f () = f (X) = .

Lemma A.3: f faithful X-Y embedding,
(a) X , measure corresponds ;
(b) , measure X corresponds .
Proof: prove (a), consider algebra subsets form f (S), FX .
Define function 0 algebra via 0 (f (S)) = (S). mapping well defined,
f (S) = f (T ), faithfulness guarantees = . Moreover, 0 probability
measure algebra. see this, note Lemma A.2 0 (Y ) = 0 (f (X)) = (X) = 1.
Moreover, f (S) f (T ) = , (by definition embedding) f (S ) = so, since
f faithful, = (for otherwise f (S ) = f () Lemma A.2, 6= ).
Thus,
0 (f (S) f (T )) = 0 (f (S )) = (S ) = (S) + (T ) = 0 (f (S)) + 0 (f (T )).
shown Horn Tarski (1948), possible extend 0 probability measure
FY .11 construction, corresponds .
prove (b), use similar process. Define function algebra sets
X via (S) = (f (S)). easy see already probability measure X ,
construction corresponds .
prove Lemma 4.6.
Lemma 4.6: X-Y embedding f faithful constraints KB ,
KB |= iff f (KB ) |= f ().
Proof: Suppose f faithful. show KB |= iff f (KB ) |= f (), must
show [[KB ]]X [[]]X iff [[f (KB )]]Y [[f (]]Y . direction immediate
definition f . prove direction, suppose not. must exist
[[KB ]]X [[]]X f () [[f ()]]Y . Let probability measure
corresponds . Since f () f (), must 0 [[]]X
f (0 ). Since 0 6= , must FX 0 (S) 6= (S). Since
f () f (0 ), must (f (S)) = (S) (f (S)) = 0 (S).
contradiction. completes proof direction.
converse, suppose KB |= iff f (KB ) |= f () KB . Given
S, FX , following chain equivalences:
11. critical result working finitely additive measures. may
countably additive measure extending 0 , even 0 countably additive. example, take FY0
Borel sets [0, 1] take FY subsets [0, 1]. Let 0 Lebesgue measure. known
that, continuum hypothesis, countably additive measure extending 0 defined
subsets [0, 1] (Ulam, 1930) (see (Keisler & Tarski, 1964) discussion).

347

fiHalpern & Koller

ST
iff Pr(S) = 1 |= Pr(T ) = 1
iff f (Pr(S) = 1) |= f (Pr(T ) = 1) (by assumption)
iff Pr(f (S)) = 1 |= Pr(f (T )) = 1 (by definition f )
iff f (S) f (T ).
Thus, f faithful.
Proposition 4.7: Let f faithful X-Y embedding. following statements
equivalent:
(a) correspond f ;
(b) formulas , |= iff |= f ().
Proof: first show (a) implies (b). suppose correspond f .
direction (b) trivial: |= f () f (), since f faithful.
direction, proceed much proof Lemma 4.6. Assume |= f ()
6|= . Since f (), definition f must 0 [[]]X
f (0 ). Since 0 |= whereas 6|= , must 6= 0 . Hence, must
FX (S) 6= 0 (S). Since f () f (0 ), follows (f (S)) = (S)
(f (S)) = 0 (S), gives desired contradiction.
show (b) implies (a). Assume contradiction
correspond f . must event FX (S) 6= (f (S)).
Let p = (S) let constraint Pr(S) = p. |= , whereas 6|= f (),
providing desired contradiction.
Theorem 4.10: X -inference procedure robust satisfies DI2, DI4, DI5,
representation independent.
Proof: Suppose {IX : X X } robust X -inference procedure. want show
representation independent. suppose KB , constraints X f
X-Y embedding, X, X . want show KB |IX iff f (KB ) |IY f ().
Let following constraint XY :
( f ()) (KB f (KB )).
claim X-conservative KB -conservative f (KB ). Thus, must
show proj X ([[KB ]]XY ) = [[KB ]]X proj ([[f (KB ) ]]XY ) = [[f (KB )]]Y .
show proj X ([[KB ]]XY ) = [[KB ]]X here; argument proj ([[f (KB )
]]XY ) = [[f (KB )]]Y almost identical.
Clearly [[KB ]]XY X [[KB ]]X , proj X ([[KB ]]XY ) [[KB ]]X .
opposite inclusion, suppose [[KB ]]X . want find measure 0
0 = . Let 00 measure f () let 0
[[KB ]]XY ) X
XY
0 = .
crossproduct 00 ; is, 0 (A B) = (A) 00 (B). Clearly X
see 0 [[KB ]]XY ), clearly suffices show 0 |= . since 00
correspond f , immediate Proposition 4.7 |= KB iff 00 |= f (KB )
|= iff 00 |= f (). Thus, |= , desired.
348

fiRepresentation Dependence

suppose KB |IX . DI2 DI5, KB domain IXY .
robustness, KB |IXY . Thus, I([[KB ]]XY ) [[]]XY . Since I([[KB ]]XY )
[[KB ]]XY [[ f ()]]XY , follows I([[KB ]]XY ) [[f ()]]XY .
Moreover, KB equivalent f (KB ) , I([[f (KB ) ]]XY ) [[f ()]]XY ,
i.e., f (KB ) |IXY f (). DI4, f (KB ) domain IY . Since conservative f (KB ), robustness {IX : X X } implies f (KB ) |IY f ().
opposite implication (if f (KB ) |IY f () KB |IX ) goes way. Thus,
{IX : X X } representation independent.
Next, turn attention Theorems 4.11 4.16. results follow
relatively straightforward way one key proposition. state it, need
definitions.
Definition A.4: say constraint KB X depends S1 , . . . , Sk FX
(the sets S1 , . . . , Sk necessarily disjoint) if, whenever , 0 X agree S1 , . . . , Sk ,
|= KB iff 0 |= KB .
example, KB form Pr(S1 ) > 1/3 Pr(S2 ) 3/4, KB depends
S1 S2 . Similarly, KB form Pr(S1 | S2 ) > 3/4, KB depends
S1 S2 .
Definition A.5: Given S1 , . . . , Sk FX , atom S1 , . . . , Sk set form
T1 . . . Tk , Ti either Si Si .
Proposition A.6: Suppose {IX : X X } X -inference procedure and,
X X , exist S, S1 , . . . , SK FX consistent constraint KB X depends
S1 , . . . , Sk , following two conditions satisfied:
nonempty every nonempty atom S1 , . . . , Sk ,
KB |IX < Pr(S) < , either > 0 < 1.
{IX : X X } representation independent.
Proof: Suppose, way contradiction, {IX : X X } representation-independent
inference procedure nevertheless, X X , exists sets S, S1 , . . . , Sk FX
knowledge base KB satisfies conditions above, , . Assume
> 0 (a similar argument used deal case < 1).
Let T1 , . . . , TM nonempty atoms S1 , . . . , Sk . Choose N 1/N < .
goal find collection f1 , . . . , fN embeddings X X
embeddings effect KB , sets fj (S) disjoint.
Since KB |IX Pr(fj (S)) > j = 1, . . . , N , fj (KB ) = f (KB ) j = 1, . . . , N ,
follow f (KB ) |IY Pr(fj (S)) > j = 1, . . . , N , contradiction. proceed
follows.
assumption, exists set Z X |Z| = N . Let = X Z.
Since X closed crossproducts, X . Suppose Z = {z1 , . . . , zM N }, let
Zi = {zN (i1)+1 , . . . , zN }, = 1, . . . , . Thus, Zi partition Z disjoint sets,
cardinality N . Let Bi = X Zi , let Bij = X {zN (i1)+j }, j = 1, . . . , N .
easy see find faithful X-Y embeddings f1 , . . . , fN
349

fiHalpern & Koller

1. fj (Ti ) = Bi , = 1, . . . , , j = 1, . . . , N ,
2. fj (Ti S) = Bij , = 1, . . . , , j = 1, . . . , N .
Notice need assumption Ti Ti nonempty T1 , . . . , TM
(that is, nonempty atom S1 , . . . , Sk ) guarantee find faithful
embeddings. Ti = , since fj embedding, f (Ti S) = 6= Bi ;
Ti = , fj (Ti S) = fj (Ti ) f (Ti S)) = , means Bi = Bij ,
inconsistent construction.
easy check that, since KB depends S1 , . . . , Sk , fj (KB ) depends
fj (S1 ), . . . , fj (Sk ), j = 1, . . . , N . next show fj (Si ) independent j;
is, fj (Si ) = fj 0 (Si ) 1 j, j 0 N . Notice h = 1, . . . , k,
fj (Sh ) = Ti Sh fj (Ti ) = {i:Ti Sh } Bi . Thus, fj (Sh ) independent j, desired. Since
fj (KB ) depends fj (S1 ), . . . , fj (Sk ), must independent j. Let KB
f1 (KB ) (which, observed, identical f2 (KB ), . . . , fk (KB )).
Since, assumption, {IX : X X } representation independent, KB |IX P r(S) >
, KB |IY Pr(fj (S)) > , j = 1, . . . , N . Thus, KB |IY Pr(f1 (S)) >
. . . Pr(fN (S)) > . note that, construction, fj (S) = {i:Ti S6=} Bij . Thus,
sets fj (S) pairwise disjoint. Since > 1/N , cannot N disjoint sets
probability greater . Thus, KB |IY false. KB consistent, KB = fj (KB )
must well. Thus, IY (KB ) 6= , assumption. contradicts conclusion
KB |IY false. Thus, {IX : X X } cannot representation independent.
use Proposition A.6 help prove Theorem 4.11.
Theorem 4.11: {IX : X X } representation-independent X -inference procedure
then, X X , IX essentially entailment objective knowledge bases
domain.
Proof: Suppose, way contradiction, {IX : X X } representation independent
IX essentially entailment X X objective knowledge base KB .
must set FX KB |IX < Pr(S) < KB 6|=
Pr(S) . Without loss generality, assume KB form Pr(T ) = 1
FX . Moreover, assume 6= , nonempty, measurable
strict subset. (For otherwise, choose = {y, 0 } X consider space X 0 = X .
assumption, X 0 X . Let f X-Y embedding maps U FX U . Since
representation independent, Pr(T ) = 1 |I < Pr(S ) < ,
{y} .)
nonempty, let Z nonempty, measurable strict subset (which exists
assumption); otherwise let Z empty set. Let U set (T S) (T Z). Notice
= U . Moreover, since, set V , Pr(T ) = 1 Pr(V ) = Pr(V ) valid,
follows Reflexivity Right Weakening KB |IX Pr(V ) = Pr(V ). Thus,
KB |IX Pr(S) = Pr(S ) = Pr(U ) = Pr(U ). follows KB |IX < Pr(U ) < .
want apply Proposition A.6. Note KB depends . Thus,
must show U U nonempty, nonempty, U
U well. observed above, U = S. Thus, U = , S,
contradicting assumption KB |I Pr(S) > 0. easy see U = S.
Again, cannot U = , S, contradicting assumption
350

fiRepresentation Dependence

KB |I Pr(S) < 1. construction, U = Z = Z. assumption, 6= ,
Z 6= . Finally, U = Z; again, construction, nonempty set 6= .
follows Proposition A.6 {IX : X X } representation independent.
Corollary 4.12: {IX : X X } representation-independent X -inference procedure,
X X , KB objective knowledge base putting constraints X ,
KB |IX < Pr(S) < 0 1, = 0 = 1.
Proof: Assume hypotheses corollary hold. Since KB objective,
form Pr(T ) = 1 FX . three possibilities. Either (1) S, (2)
S, (3) nonempty. (1) holds, KB |= Pr(S) = 1,
(2) holds, KB |= Pr(S) = 0. Thus, (1) (2) incompatible
KB |IX < Pr(S) < . hand, (3) holds, easy see
, Pr(S) = consistent KB (since probability measure assigns
probability probability 1 S). Since KB |IX < Pr(S) < ,
Theorem 4.11, must KB |= Pr(S) . follows choices
true = 0 = 1.
Theorem 4.16: {IX : X X } X -inference procedure enforces minimal
default independence satisfies DI1, IX representation independent.
Proof: Suppose {IX : X X } X -inference procedure enforces minimal
default independence satisfies DI1. Choose X = {x, x0 } X let KB 1/3
Pr(x) 2/3. assumption, X X X . view KB constraint XX ;
case, interpreted 1/3 Pr({x} X) 2/3. DI1, KB
domain IXX . Note KB equivalent constraint 1/3 Pr(x0 ) 2/3.
minimal default independence, KB |IXX Pr((x, x)) > Pr(x X)/3
KB |IXX Pr((x0 , x0 )) > Pr(x0 X)/3. Applying straightforward probabilistic reasoning,
get KB |IXX Pr({(x, x), (x0 , x0 )}) > 1/3. apply Proposition A.6, taking
{(x, x), (x0 , x0 )} 0 {(x, x), (x, x0 )}. Note KB depends 0 .
almost immediate definition 0 0 , 0 , 0 ,
0 nonempty. Thus, Proposition A.6, {IX : X X } representation
independent.
0 : X X } representationLemma 4.13: Let X consist countable sets. {IX
independent X -inference procedure.

Proof: said main part text, easily follows Proposition 2.5
0 inference procedure X X , since easily seen five propIX
erties described proposition. see 0 representation independent, suppose
f faithful X-Y embedding, X, X . Clearly KB objective
f (KB ) objective. KB objective, easy see KB |I 0 iff
f (KB ) |I 0 f (), since |I 0 reduces entailment case. suppose KB objective form Pr(T ) = 1, FX . KB |I 0 iff KB KB + |= .
Lemma 4.6, holds iff f (KB )f (KB + ) |= f (). hand, f (KB ) |I 0 f ()
iff f (KB ) (f (KB ))+ |= f () Thus, suffices show f (KB ) f (KB + ) |= f ()
iff f (KB ) (f (KB ))+ |= f (). easy show (f (KB ))+ implies f (KB + ),
f (KB ) f (KB + ) |= f () f (KB ) (f (KB ))+ |= f (). necessarily
351

fiHalpern & Koller

case f (KB + ) implies (f (KB ))+ . example, consider embedding described
Example 4.3. case, KB objective knowledge base Pr(colorful ) = 1,
KB + empty, hence f (KB + ), (f (KB ))+ includes constraints
0 < Pr(green) < 1. Nevertheless, suppose f (KB ) (f (KB ))+ |= f () and, way
contradiction, |= f (KB ) f (KB + ) f (). Choose
f (). correspond, |= KB KB + . easy
show exists 0 f () 0 < 0 (S) < 1 nonempty subsets
f (T ). see this, note (x) 6= 0, suffices ensure 0 (f (x)) = (x)
0 (y) 6= 0 f (x). Since countable, straightforward. Since 0
correspond, must 0 |= f () f (KB ). construction, 0 |= (f (KB ))+ .
contradicts assumption f (KB ) (f (KB ))+ |= f ().
Lemma 4.14: Suppose X consists measure spaces form (X, 2X ),
1 : X X } representation-independent X -inference procedure.
X finite. {IX
Proof: Suppose X, X , KB constraints X , f X-Y
embedding. must show KB |I 1 iff f (KB ) |I 1 f (). purposes
X

proof, say subset X interesting exists FX
= { X : (S) 1/4}. easy see KB interesting f (KB )
interesting. converse also true, given assumption X consists finite
spaces sets measurable. suppose f (KB ) interesting.
set f (KB ) = { : (T ) 1/4}. Let = {S 0 X : f (S 0 ) }.
Since X finite, A; easily follows = A.12 Clearly (S) 1/4,
f () f (KB ), [[KB ]]X . Thus, [[KB ]]X { X : (S) 1/4}.
hand, KB , f () f (KB ). Thus, f (), since A, must
case (S) = (f (S)) (T ) 1/4. Thus, [[KB ]]X { X : (S) 1/4}.
follows KB equivalent Pr(S) 1/4, must interesting. (We must also
= f (S), although needed result.)
KB interesting, KB |I 1 iff KB |= iff f (KB ) |= f () (since entailment
X
representation independent) iff f (KB ) |I 1 . hand, KB interesting,

KB equivalent Pr(S) 1/4 X, f (KB ) equivalent
Pr(f (S)) 1/4. Moreover, KB |I 1 iff Pr(S) 1/3 |= iff Pr(f (S)) 1/3 |= f () iff
X
f (KB ) |I 1 . Thus, get representation independence, desired.


A.4 Proofs Section 6
Proposition 6.3: Suppose f faithful X-Y embedding, DX X , DY .
following two conditions equivalent:
(a) DX DY correspond f ;
(b) , DX |= iff DY |= f ().
12. general true X infinite without additional requirement f (i Ai ) = f (Ai )
arbitrary unions.

352

fiRepresentation Dependence

Proof: prove (a) implies (b), assume way contradiction that, ,
DX |= DY 6|= f (). DY 6|= f (). Let DX
measure corresponding . Then, Proposition 4.7, 6|= , desired
contradiction. proof direction (a) identical.
prove (b) implies (a), first consider measure DX . must find DY
corresponds . Suppose X = {x1 , . . . , xn } (recall restricting
finite spaces Section 6) (xi ) = ai , = 1, . . . , n. Let constraint
ni=1 Pr({xi }) = ai . assumptions language, constraint
language. Clearly [[]]X = {}. Since DX , know DX 6|= . Hence, DY 6|=
f (), exists DY 6 f (). Hence f () = f ({}).
definition f , corresponds .
consider measure DY , let measure X corresponds
. Assume way contradiction 6 DX . Taking above, follows
DX |= and, therefore, assumption, DY |= f (). Thus, |= f (). |= and,
assumption, correspond. contradicts Proposition 4.7.
Theorem 6.7: Let arbitrary constraint X . f faithful X-Y embedding
correspond f , | |f () also correspond f .
Proof: Assume correspond f . Recall assuming section
X finite space; let X = {x1 , . . . , xn }. Let Yi = f (xi ). Given distribution
00 , define i00 = 00 |Yi let (f )1 ( 00 ) denote unique 00 X
00 f (00 ).
suppose 0 |. Define 0 measure
0 (y) = 0 (xi ) (y),
index Yi . Since = |Yi , follows (Yi ) = 1. Thus,
0 (Yi ) = (xi ), 0 leaves relative probabilities elements within Yi
. easy verify 0 0 correspond. Hence, Proposition 4.7, 0 |= f ().
claim 0 |f (). show that, need show KLY ( 0 k) minimal
among KLY ( 00 k) 00 |= f (). follows standard properties relative
entropy (Cover & Thomas, 1991, Theorem 2.5.3) 00 ,
KLY ( 00 k) = KLX ((f )1 ( 00 )k(f )1 ()) +

n
X

KLY (i00 ki ).

(1)

i=1

Note = i0 , KLY (i0 ki ) = 0, = 1, . . . , n. Thus, follows (1)
KLY ( 0 k) = KLX (0 k).
Now, let 00 00 |= f () let 00 = (f )1 (00 ). Since 00 00
correspond f , follows Proposition 4.7 00 |= . Using (1) again,

KLY ( 00 k) = KLX (00 k) +

n
X
i=1

KLX (00 k).
353

KLY (i00 ki )

fiHalpern & Koller

since 0 |, know KLX (0 k) KLX (00 k). Hence conclude
KLY ( 00 k) KLY ( 0 k),
0 |f ().
Theorem 6.9: f faithful X-Y embedding, P invariant f iff P(X)
P(Y ) correspond f .
Proof: Suppose f faithful X-Y embedding. definition, P invariant
f iff, KB , ,
KB | P iff f (KB ) | P f ().

(2)

definition P , (2) holds iff
P(X)|KB [[]]X iff P(Y )|f (KB ) [[f ()]]Y KB , .

(3)

Proposition 6.3, (3) holds iff P(X)|KB P(Y )|f (KB ) correspond KB .
Corollary 6.5, P(X) P(Y ) correspond, P(X)|KB P(Y )|f (KB ) correspond
KB . hand, P(X)|KB P(Y )|f (KB ) correspond KB ,
P(X) P(Y ) must correspond: simply take KB = true observe P(X)|KB ) =
P(X) P(Y )|f (KB ) = P(Y ).
Proposition 6.10: Suppose X1 Xn product decomposition X and,
= 1, . . . , n, KB constraint Xi , Si subset Xi .
n
^
i=1

KB |IP Pr(S1 . . . Sn ) =


n


Pr(Si ).

i=1

Proof: KB satisfiable constraint Xi , = 1, . . . , n, exist product
V
measures X satisfying constraints ni=1 KB . product measures precisely
Vn
measures P |( i=1 KB ). Since measures satisfies Pr(S1 . . . Sn ) =
Qn
i=1 Pr(Si ) assumption, conclusion holds case. constraint KB
satisfiable, result trivially holds.
Theorem 6.11: inference procedure IP invariant faithful product embeddings permutation embeddings.
Proof: Suppose f faithful X-Y product embedding, X1 Xn product
decomposition X, Y1 Yn product decomposition . show
P invariant f , suffices show P (X) P (Y ) correspond f .
Supposethat P (Y ). = 1 n , measure Xi , = 1, . . . , n.
Moreover, since f product embedding, exist f1 , . . . , fn f = f1 fn .
Let fi (i ), = 1, . . . , n. easy check = 1 n f ().
Conversely, suppose P (Y ). = 1 n , Yi
= 1, . . . , n. Define Xi setting (S) = (fi (S)). Since fi faithful Xi -Yi
354

fiRepresentation Dependence

embedding, easy check Xi fi (i ). Thus, f ().
completes proof P invariant faithful X-Y product embeddings.
argument P invariant faithful X-X permutation embeddings
similar (and easier). leave details reader.

References
Bacchus, F. (1990). Representing Reasoning Probabilistic Knowledge. MIT Press,
Cambridge, Mass.
Bacchus, F., Grove, A. J., Halpern, J. Y., & Koller, D. (1996). statistical knowledge
bases degrees belief. Artificial Intelligence, 87 (12), 75143.
Cover, T. M., & Thomas, J. A. (1991). Elements Information Theory. Wiley, New York.
Enderton, H. B. (1972). Mathematical Introduction Logic. Academic Press, New York.
Giunchiglia, F., & Walsh, T. (1992). theory abstraction. Artificial Intelligence, 56 (23),
323390.
Goldszmidt, M., Morris, P., & Pearl, J. (1993). maximum entropy approach nonmonotonic reasoning. IEEE Transactions Pattern Analysis Machine Intelligence,
15 (3), 220232.
Halpern, J. Y., & Koller, D. (1995). Representation dependence probabilistic inference.
Proc. Fourteenth International Joint Conference Artificial Intelligence (IJCAI
95), pp. 18531860.
Horn, A., & Tarski, A. (1948). Measures Boolean algebras. Transactions AMS,
64 (1), 467497.
Jaeger, M. (1996). Representation independence nonmonotonic inference relations.
Principles Knowledge Representation Reasoning: Proc. Fifth International
Conference (KR 96), pp. 461472.
Jaynes, E. T. (1968). Prior probabilities. IEEE Transactions Systems Science
Cybernetics, SSC-4, 227241.
Jaynes, E. T. (1978). stand maximum entropy?. Levine, R. D., & Tribus,
M. (Eds.), Maximum Entropy Formalism, pp. 15118. MIT Press, Cambridge,
Mass.
Kahneman, D., Slovic, P., & Tversky, A. (Eds.). (1982). Judgment Uncertainty:
Heuristics Biases. Cambridge University Press, Cambridge/New York.
Kass, R. E., & Wasserman, L. (1993). Formal rules selecting prior distributions: review
annotated bibliography. Tech. rep. Technical Report #583, Dept. Statistics,
Carnegie Mellon University.
355

fiHalpern & Koller

Keisler, J., & Tarski, A. (1964). accessible inaccessible cardinals. Fundamenta
Mathematica, 53, 225308.
Kraus, S., Lehmann, D., & Magidor, M. (1990). Nonmonotonic reasoning, preferential
models cumulative logics. Artificial Intelligence, 44, 167207.
Kullback, S., & Leibler, R. A. (1951). information sufficiency. Annals Mathematical Statistics, 22, 7686.
Lehmann, D., & Magidor, M. (1992). conditional knowledge base entail?.
Artificial Intelligence, 55, 160.
Nayak, P. P., & Levy, A. Y. (1995). semantic theory abstractions. Proc. Fourteenth
International Joint Conference Artificial Intelligence (IJCAI 95), pp. 196203.
Paris, J. B. (1994). Uncertain Reasoners Companion. Cambridge University Press,
Cambridge, U.K.
Paris, J., & Vencovska, A. (1990). note inevitability maximum entropy. International Journal Approximate Reasoning, 4 (3), 183224.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann, San
Francisco.
Salmon, W. (1961). Vindication induction. Feigl, H., & Maxwell, G. (Eds.), Current
Issues Philosophy Science, pp. 245264. Holt, Rinehart, Winston, New
York.
Salmon, W. (1963). vindicating induction. Kyburg, H. E., & Nagel, E. (Eds.),
Induction: Current Issues, pp. 2754. Wesleyan University Press, Middletown,
Conn.
Seidenfeld, T. (1987). Entropy uncertainty. MacNeill, I. B., & Umphrey, G. J. (Eds.),
Foundations Statistical Inferences, pp. 259287. Reidel, Dordrecht, Netherlands.
Shore, J. E., & Johnson, R. W. (1980). Axiomatic derivation principle maximum entropy principle minimimum cross-entropy. IEEE Transactions
Information Theory, IT-26 (1), 2637.
Ulam, S. (1930). Zur masstheorie der allgemeinen mengenlehre. Fundamenta Mathematicae, 16, 140150.
Walley, P. (1996). Inferences multinomial data: learning bag marbles.
Journal Royal Statistical Society, Series B, 58 (1), 334. Discussion
paper various commentators appears pp. 3457.

356

fiJournal Artificial Intelligence Research 21 (2004) 193-243

Submitted 09/03; published 02/04

Generalizing Boolean Satisfiability I: Background
Survey Existing Work
Heidi E. Dixon

dixon@cirl.uoregon.edu

CIRL
Computer Information Science
1269 University Oregon
Eugene, 97403 USA

Matthew L. Ginsberg

ginsberg@otsys.com

Time Systems, Inc.
1850 Millrace, Suite 1
Eugene, 97403 USA

Andrew J. Parkes

parkes@cirl.uoregon.edu

CIRL
1269 University Oregon
Eugene, 97403 USA

Abstract
first three planned papers describing zap, satisfiability engine
substantially generalizes existing tools retaining performance characteristics
modern high-performance solvers. fundamental idea underlying zap many
problems passed engines contain rich internal structure obscured
Boolean representation used; goal define representation structure
apparent easily exploited improve computational performance. paper
survey work underlying zap, discusses previous attempts improve
performance Davis-Putnam-Logemann-Loveland algorithm exploiting structure problem solved. examine existing ideas including extensions
Boolean language allow cardinality constraints, pseudo-Boolean representations, symmetry, limited form quantification. paper intended survey,
research results contained two subsequent articles, theoretical structure
zap described second paper series, zaps implementation described
third.

1. Introduction
first planned series three papers describing zap, satisfiability engine
substantially generalizes existing tools retaining performance characteristics
modern high-performance solvers zChaff (Moskewicz, Madigan, Zhao, Zhang,
& Malik, 2001).1 Many Boolean satisfiability problems incorporate rich structure reflects properties domain problems arise, zap includes
representation language allows structure described proof engine
exploits structure improve performance. first paper describes work
1. second two papers published technical reports (Dixon, Ginsberg, Luks, & Parkes,
2003b; Dixon, Ginsberg, Hofer, Luks, & Parkes, 2003a) yet peer reviewed.

c
2004
AI Access Foundation. rights reserved.

fiDixon, Ginsberg & Parkes

zap based, intended survey existing results attempt
use problem structure improve performance satisfiability engines. results
discuss include generalizations Boolean satisfiability include cardinality constraints,
pseudo-Boolean representations, symmetry, limited form quantification. second paper series (Dixon et al., 2003b) describes theoretical generalization
subsumes extends ideas, third paper (Dixon et al., 2003a) describes
zap system itself.
intention review work satisfiability introduction DavisPutnam-Logemann-Loveland algorithm (Davis, Logemann, & Loveland, 1962) present
day. work (thankfully), portion work thought
attempt improve performance systematic methods exploiting general
structure problem question.
therefore include description recent work extending language Boolean
satisfiability include restricted form quantification (Ginsberg & Parkes, 2000)
pseudo-Boolean constraints (Barth, 1995, 1996; Chandru & Hooker, 1999; Dixon & Ginsberg, 2000; Hooker, 1988; Nemhauser & Wolsey, 1988); case, representational
extension corresponds existence structure hidden Boolean axiomatization. discuss length interplay desire speed search exploiting
structure danger slowing search using unwieldy representations. Somewhat
surprisingly, see effective representational extensions appear incur little overhead result implementation concerns. observation
better representations better implementations led us search
general sort structure zap exploits.
also discuss attempts made exploit symmetrical structure satisfiability problems (Brown, Finkelstein, & Paul Walton Purdom, 1988;
Crawford, 1992; Crawford, Ginsberg, Luks, & Roy, 1996; Joslin & Roy, 1997; Krishnamurthy, 1985; Puget, 1993). work appears modest impact
development satisfiability engines generally, explain why: authors (Crawford, 1992; Crawford et al., 1996; Joslin & Roy, 1997; Krishnamurthy, 1985; Puget, 1993)
exploit global symmetries, symmetries vanishingly rare naturally occurring problems. methods described exploiting local emergent
symmetries (Brown et al., 1988; Szeider, 2003) incur unacceptable computational overhead
node search. general arguments regarding interplay representation search suggest one identify local symmetries problem
formulated, exploit symmetries throughout search.
discuss heuristic search substantial literature relating it.
collective eye, Boolean axiomatization obscure natural structure
search problem, heuristics. argued elsewhere (Ginsberg & Geddis, 1991)
domain-dependent search control rules never poor mans standin
general principles based problem structure. selection survey material reflects
bias.
remarked entry zap series survey paper; extent
research contribution, overall (and, believe, novel) focus
taking. basic view target new work area
specific representational extension pseudo-Boolean first-order encoding,
194

fiBoolean Satisfiability Survey

direct exploitation underlying problem structure. paper original
describing existing work way, first time viewing first-order pseudoBoolean extensions purely structure-exploitation techniques. First-order pseudoBoolean representations effective history usefulness,
mind allow identification capture two particular
types structure inherent many classes problems. hope reader
views material presenting (and companion papers well)
light: Recent progress Boolean satisfiability best thought terms structure
exploitation. perspective approach paper, hope
you, reader, come share it.
let us return Davis-Putnam-Logemann-Loveland procedure (Davis et al.,
1962), appears begun body work development solvers
sufficiently powerful used practice wide range problems. Descendants
dpll algorithm solution method choice many problems including
microprocessor testing verification (Biere, Clarke, Raimi, & Zhu, 1999; Copty, Fix,
Giunchiglia, Kamhi, Tacchella, & Vardi, 2001; Velev & Bryant, 2001), competitive
domains planning (Kautz & Selman, 1992).
return pure algorithmics dpll successors shortly, let us begin
noting spite impressive engineering successes many difficult problems,
many easy problems Boolean satisfiability engines struggle. include
problems involving parity, well known pigeonhole problem (stating cannot
put n + 1 pigeons n holes pigeon needs hole), problems
described naturally using first-order opposed ground representation.
cases, structure problem solved structure
lost ground encoding built. testament power Boolean
methods solve large difficult problems without access underlying
structure, seems reasonable expect incorporating using structure would
improve performance further. survey techniques improve dpll suggest
fact case, since techniques underpin performance modern
Boolean satisfiability engines well understood way.
turning details analysis, however, let us flesh framework
bit more. computational procedure performance improved, seem
three ways done:
1. Replace algorithm.
2. Reduce time spent single iteration inner loop.
3. Reduce number times inner loop must executed.
first approach focus here; many potential competitors
dpll, none seems outperform dpll practice.2 Work second approach
2. Systematic alternatives include original Davis-Putnam (1960) method, polynomial calculus solvers
(Clegg, Edmonds, & Impagliazzo, 1996) based Buchbergers (1965, 1985) Groebner basis algorithm,
methods based binary decision diagrams bdds (Bryant, 1986, 1992), direct first-order methods

195

fiDixon, Ginsberg & Parkes

historically focused reducing amount time spent unit propagation,
indeed appear represent inner loop dpll implementations.
variety techniques available reducing number calls inner
loop, divide follows:
3a. Algorithmic improvements require representational changes.
3b. Algorithmic improvements requiring representational changes.
Attempts improve dpll without changing underlying Boolean representation
focused (i) development mechanism retaining information developed one
portion search subsequent use (caching or, commonly, learning) (ii)
development search heuristics expected reduce size space
examined. work, however, using dpll analyze problems solution
exists equivalent building resolution proof unsatisfiability underlying
theory, number inference steps bounded number inferences
shortest proof (Mitchell, 1998). lengths generally exponential problem
size worst case.
Work involving representation change overcome difficulty reducing length
proof dpll similar algorithm implicitly trying construct. Representations sought certain problems known require proofs exponential length
Boolean case admit proofs polynomial length representational shift made.
leads hierarchy representational choices, one representation r1 said
polynomially simulate p-simulate another r2 proofs using representation r2
converted proofs using r1 polynomial time. general, representations lead
efficient proofs via efficient encodings, single axiom improved
representation corresponds exponentially many original. many excellent
surveys proof complexity literature (Beame & Pitassi, 2001; Pitassi, 2002; Urquhart,
1995), generally repeat material here.
course, sufficient simply improve epistemological adequacy proof
system; heuristic adequacy must maintained improved well (McCarthy, 1977).
therefore assume representation introduced purposes navigating
p-simulation hierarchy also preserves basic inference mechanism dpll (resolution)
maintains, ideally builds upon, improvements made propagation performance (2)
learning (3a). context survey shall consider representational
changes.
representations consider following:
Boolean axiomatizations. original representation used dpll, provides basic setting progress propagation, learning branching
taken place.
employed otter (McCune & Wos, 1997). Nonsystematic methods include wsat
family (Selman, Kautz, & Cohen, 1993), received great deal attention 1990s still
appears method choice randomly generated problems specific sets
instances. general, however, systematic algorithms roots dpll tend outperform
alternatives.

196

fiBoolean Satisfiability Survey

Cardinality constraints. disjunctive Boolean axiom states least one
disjuncts true, cardinality constraint allows one state least n
disjuncts true integer n.
Pseudo-Boolean constraints. Taken operations research, pseudo-Boolean constraint form
X
wi li k


wi positive integral weights, li literals, k positive integer.
Cardinality constraints special case wi = 1 i; Boolean constraint
k = 1 well.
Symmetric representations. problems (such pigeonhole problem)
highly symmetric, possible capture symmetry directly axiomatization. variety authors developed proof systems exploit
symmetries reduce proof size (Brown et al., 1988; Crawford, 1992; Crawford et al.,
1996; Joslin & Roy, 1997; Krishnamurthy, 1985; Puget, 1993).
Quantified representations. many approaches quantified satisfiability, focus change underlying complexity
problem solved.3 requires domains quantification
finite, focus propositional restriction first-order logic known
qprop (Ginsberg & Parkes, 2000).
Given arguments regarding heuristic adequacy generally, goal survey
complete following table:
representational
efficiency

p-simulation
hierarchy

inference

unit
propagation

learning

SAT

cardinality
pseudo-Boolean
symmetry
QPROP

first column simply names representational system question. each,
describe:
Representational efficiency (3b): many Boolean axioms captured
single axiom given representation?
p-simulation hierarchy (3b): representation relative others
p-simulation hierarchy?
Inference: possible lift basic dpll inference mechanism resolution
new representation without incurring significant additional computational expense?
3. Problems involving quantified Boolean formulae, qbf, pspace-complete (Cadoli, Schaerf, Giovanardi, & Giovanardi, 2002) opposed np-complete problems considered dpll direct
successors.

197

fiDixon, Ginsberg & Parkes

Unit propagation (2): techniques used speed unit propagation lifted
new setting? new techniques available available Boolean
case?
Learning (3a): existing learning techniques lifted new setting? new
techniques available available Boolean case?
cannot overstress fact single column table important
others. reduction proof length little practical value associated
reduction amount computation time required find proof. Speeding time
needed single inference hardly useful number inferences required grows
exponentially.
Boolean satisfiability community pioneered many techniques used reduce perinference running time understand learning declarative setting. spite
fact resolution Boolean satisfiability among weakest inference systems
terms representational efficiency position p-simulation hierarchy (Pudlak,
1997), almost none powerful proof systems wide computational use. Finding
proofs sophisticated settings difficult; even direct attempts lift dpll
first-order setting (Baumgartner, 2000) seem fraught complexity inability
exploit recent ideas led substantial improvements algorithmic performance.
usable proof systems often theoretically powerful.
paper organized along rows table trying complete. Boolean
techniques described next section; recount demonstration pigeonhole problem exponentially difficult setting (Haken, 1985). go Section 3
discuss cardinality constraints pseudo-Boolean methods, showing earlier
difficulties pigeonhole problem overcome using either method
similar issues remain cases. Following descriptions implemented pseudoBoolean reasoning systems (Barth, 1995; Dixon & Ginsberg, 2000), show key
computational ideas Boolean case continue applicable pseudo-Boolean
setting.
Axiomatizations attempt exploit symmetry directly discussed Section 4.
draw distinction approaches require existence global symmetries,
tend exist practice, use local ones, exist
difficult find inference proceeds.
Section 5, discuss axiomatizations Boolean descriptions problems
naturally represented using quantified axioms. discuss problems arising
fact ground theories tend exponentially larger lifted
counterparts, show working first-order axiomatization directly lead
large improvements efficiency overall system (Ginsberg & Parkes, 2000).
Concluding remarks contained Section 6.

2. Boolean Satisfiability
Definition 2.1 variable simply letter (e.g., a) either true false.
literal either variable negation variable. clause disjunction literals,

198

fiBoolean Satisfiability Survey

Boolean satisfiability problem (in conjunctive normal form), sat problem,
conjunction clauses.
solution sat problem C assignment values letters
every clause C satisfied.
None new. Satisfiability sat instances well-known NPcomplete (Cook, 1971), language reasonably natural one encoding realworld problems. remarked introduction, classic algorithm solving
problems depth-first search augmented ability set variables whose values
forced:
Procedure 2.2 (Davis-Putnam-Logemann-Loveland) Given sat problem C
partial assignment P values variables, compute dpll(C, P ):
1
2
3
4
5
6
7
8
9

P Unit-Propagate(P )
P contains contradiction
return failure
P assigns value every variable
return success
l literal assigned value P
dpll(C, P {l = true}) = success
return success
else return dpll(C, P {l = false})

Variables assigned values two ways. first, unit propagation, clause set
examined existing partial assignment new consequential assignments
identified. Somewhat specifically (see below), clauses found satisfied
literals exactly one unvalued literal. clause, unvalued literal valued
satisfy clause. process repeated contradiction encountered,
solution found, clauses meet necessary conditions. unit propagation
function terminates without reaching contradiction finding solution, variable
selected assigned value, procedure recurs.
practice, choice branch literal crucial performance algorithm.
(Note choosing branch literal l instead l, also select order
values tried underlying variable.) Relatively early work dpll focused
selection branch variables produced many unit propagations possible, thus
reducing size residual problems solved recursively. see
Section 2.3, however, recent ideas appear effective.
Missing Procedure 2.2, however, description propagation process.
is:
Procedure 2.3 (Unit propagation) compute Unit-Propagate(P ):
1
2
3
4

contradiction found c C P
satisfied literals exactly one unassigned literal
v variable c unassigned P
P P {v = V : V selected c satisfied}
return P
199

fiDixon, Ginsberg & Parkes

100

ZCHAFF data

% time spent

95

90

85

80

75

70

0

10

20

30

40
50
total CPU time (sec)

60

70

80

90

Figure 1: Fraction CPU time spent unit propagation
Dpll variety well-known theoretical properties. sound complete
every candidate solution returned solution original problem,
solution returned one exists (and failure eventually reported solution exists).
practical point view, running time dpll obviously potentially exponential size problem, since iteration, possibly single variable
assigned value routine invoked recursively. practice, course, unit
propagation reduce number branch points substantially, running time
remains exponential difficult instances (Crawford & Auton, 1996).
also point difficult problem instances, running time
necessarily spent exploring portions search space solutions.
all, P partial assignment extended solution, algorithm
never backtrack away P . (Indeed, cannot retain completeness, since P may
variable assignment extends full solution point.) Given
backtrack away satisfiable partial assignment, number
backtracks exponential problem size, clear time spent
program indeed evaluating unsatisfiable regions search space.
2.1 Unit Propagation: Inner Loop
dpll algorithm 2.2 implemented run practical problems, bulk
running time spent unit propagation. example, Figure 1 gives amount time
spent zChaff variety microprocessor test verification examples made available Velev (http://www.ece.cmu.edu/mvelev).4 problems become difficult,
increasing fraction computational resources devoted unit propagation.
reason, much early work improving performance dpll focused improving
speed unit propagation.
4. examples used generate graph solved zChaff within 100 seconds using Intel
Pentium 4M running 1.6GHz. solved within 100 second limit, average 89.4%
time spent unit propagating.

200

fiBoolean Satisfiability Survey

Within unit propagation procedure 2.3, bulk time spent identifying
clauses propagate; words, clauses satisfied partial assignment contain one unvalued literal:
Observation 2.4 Efficient implementations dpll typically spend bulk effort
searching clauses satisfying conditions required unit propagation.
go examine techniques used speed unit propagation
practice, let us remark implementations sat solvers similar properties.
Nonsystematic solvers wsat (Selman et al., 1993), example, spend bulk
time looking clauses containing satisfied unvalued literals (or, equivalently,
maintaining data structures needed make search efficient). generalize
Observation 2.4 get:
Observation 2.5 Efficient implementations sat solvers typically spend bulk
effort searching clauses satisfying specific syntactic conditions relative partial
complete truth assignment.
focus survey systematic methods, remark
similarity techniques used dpll wsat, techniques speed inner loop
one likely speed inner loop well.
said, let us describe series ideas employed speeding
process identifying clauses leading unit propagations:
1. binding variable v, examine clause determine whether satisfies
conditions Procedure 2.3.
2. Slightly sophisticated restrict search suitable clause clauses
c C include v one disjuncts (assuming v assigned
value true). all, v appears c, c satisfied v set true; v
mentioned c change cs ability unit propagate vs value
set.
3. set v true above, examine clauses containing v, walk
clause determine literals satisfied (if any)
still unbound. efficient keep record, clause c, number
s(c) satisfied number u(c) unbound literals.
order keep counts current set variable v true, need
increment s(c) decrement u(c) clause v appears, simply
decrement u(c) clause v appears. backtrack unset v,
need reverse adjustments.
Compared previous approach, need examine four times many clauses
(those v appears either sign, v set unset),
examination takes constant time instead time proportional clause length.
average clause length greater four, approach, due Crawford
Auton (1996), effective predecessor.
201

fiDixon, Ginsberg & Parkes

4. Currently, efficient scheme searching unit propagations watched
literals family implementations (Moskewicz et al., 2001; Zhang & Stickel, 2000).
clause c, identify two watched literals l1 (c) l2 (c); basic idea
long two literals either unvalued satisfied, clause
cannot produce unit propagation.
one watched literals set false clause must
examined detail. another unset (and unwatched) literal, watch it.
satisfied literal clause, need nothing. satisfied
literal, clause ready unit propagate.
average clause length l, set variable v (say true),
probability approximately 2/l need analyze clause v
appears, work involved proportional length clause.
expected amount work involved twice number clauses v appears,
improvement previous methods. fact, approach somewhat
efficient cursory analysis suggests adjustment watched
literals tends favor watching set deep search tree.
move discuss learning Boolean satisfiability, let us remark briefly
so-called pure literal rule. understand rule itself, suppose theory
partial variable assignment P . Suppose also literal q appears
clauses yet satisfied P , negation q appear
clauses. q may consequence partial assignment P ,
clearly set q true without removing solutions portion search
space remains.
pure literal rule generally included implementations either dpll
unit propagation procedure relatively expensive work with. Counts
number unsatisfied clauses containing variables negations must maintained
times, checked see literal become pure. addition, see
Section 2.3 many branching heuristics obviate need pure literal rule
employed.
2.2 Learning Relevance
Let us turn final column table, considering progress made
avoiding rework Boolean satisfiability engines. basic idea avoid situations
conventional implementation thrash, solving subproblem many
times different contexts.
understand source difficulty, consider example involving sat problem
C variables v1 , v2 , . . . , v100 , suppose also subset C involving
variables v50 , . . . , v100 fact implies v50 must true.
imagine constructed partial solution values variables
v1 , . . . , v49 , initially set v50 false. amount backtracking,
realize v50 must true. Unfortunately, subsequently change value
one vi < 50, forget v50 needs true danger
setting false again, followed repetition search showed v50
202

fiBoolean Satisfiability Survey

consequence C. Indeed, danger solving subproblem twice,
search node examine space vi < 50.
indicated, solution problem cache fact v50 needs
true; information generally saved form new clause called nogood .
case, record unary clause v50 . Modifying original problem C
way allow us immediately prune subproblem set v50 false.
technique introduced Stallman Sussman (1977) dependency directed
backtracking.
Learning new constraints way prevent thrashing. contradiction
encountered, set assignments caused contradiction identified;
call set conflict set. new constraint constructed excludes
assignments conflict set. Adding constraint problem ensure
faulty set assignments avoided future.
description learning fairly syntactic; also give semantic description. Suppose partial assignment contains {a, b, d, e} problem
contains clauses
b c e
(1)

c d.

(2)

first clause unit propagates allow us conclude c; second allows us conclude
c. contradiction causes us backtrack, learning nogood
b e.

(3)

semantic point view, derived nogood (3) simply result resolving
reason (1) c reason (2) c.
general phenomenon. point, suppose v variable
set partial assignment P . vs value result branch choice,
associated reason. vs current value result unit propagation, however,
associate v reason clause produced propagation. vs value
result backtrack, value must result contradiction identified
subsequent variable v 0 set reason v result resolving
reasons v 0 v 0 . point, variable whose value forced
associated reason, accumulated reasons avoid need reexamine
particular portion search space.
Modifying dpll exploit derived information requires include derived clauses overall problem C, thus enabling new unit propagations restricting
subsequent search. implicit change well.
earlier example, suppose set variables v1 , . . . , v49 order,
learned nogood
v7 v9

(4)

(presumably situation v7 false v9 true). long v7 remains false
v9 remains true, unit propagation fail immediately (4) unsatisfied.
203

fiDixon, Ginsberg & Parkes

allow us backtrack directly v9 example. semantic justification
technique known backjumping (Gaschnig, 1979) search jumps back
variable relevant problem hand.5
attractive theory, however, technique difficult apply practice.
reason new nogood learned every backtrack; since number backtracks
proportional running time program, exponential number nogoods
learned. expected overtax memory available system
algorithm running, increase time node expansion. number
clauses containing particular variable v grows without bound, unit propagation
procedure 2.3 grind halt.
Addressing problem primary focus work systematic sat solvers
1990s. Since impractical retain nogoods learned,
method needed found would allow polynomial number nogoods retained
others discarded. hope relatively small number nogoods could
still used prune search space effectively.6
Length-bounded Learning first method used bound size set learned
clauses retain clauses whose length less given bound k (Dechter, 1990;
Frost & Dechter, 1994). addition providing polynomial bound number
nogoods retained, approach felt likely retain important learned clauses,
since clause length l general prune 21l possible assignments values
variables. Length-bounded learning draws observation conclusion short
clauses retained preference long ones.
Relevance-bounded Learning Unfortunately, length may best indicator
value particular learned clause. restrict attention particular
subproblem search, short clauses may applicable all,
longer clauses may lead significant pruning. example, consider node defined
partial assignment P = {a, b, c} together two learned clauses:
abe

(5)

b c e

(6)

long assignments P retained, clause (5) cannot used pruning
satisfied P itself. fact, clause useful backtrack
change values b. clause (6) likely useful
current subproblem, since lead unit propagation either e set false.
subproblem node given P large, (6) may used many times. Within
context subproblem, longer clause actually useful.
5. particular example, also possible backtrack v8 well, although reason recorded.
branch point v8 removed search, v9 set false unit propagation.
advantage flexibility either choice value v8 choice
branch variable itself. idea related Bakers (1994) work difficulties associated
backjumping schemes, employed zChaff (Moskewicz et al., 2001).
6. Indeed, final column table might better named forgetting learning. Learning
(everything) easy; forgetting coherent way thats hard.

204

fiBoolean Satisfiability Survey

level, appearance a, b, c (6) shouldnt contribute
effective length learned clause literals currently false.
Length-bounded learning cannot make distinction.
see example useful retain clauses arbitrary length provided
relevant current search context. context subsequently changes,
remove clauses make room new ones suitable new
context. relevance-bounded learning does.
relevance-bounded learning, effective length clause defined terms
current partial assignment. irrelevance clause defined one less
number unvalued satisfied literals contains. example, clause (6)
partial assignment P irrelevance 1. idea relevance-bounded learning
originated dynamic backtracking (Ginsberg, 1993), clauses deleted
irrelevance exceeded 1. idea generalized Bayardo Miranker (1996),
defined general relevance bound deleted clauses whose irrelevance exceeded
bound. generalization implemented relsat satisfiability engine (Bayardo
& Schrag, 1997).
Like length-bounded learning, relevance-bounded learning retains clauses length
less irrelevance bound k, since irrelevance clause never exceed
length. technique relevance-bounded learning also allows retention longer
clauses applicable current portion search space. clauses
removed longer relevant.
Returning example, backtrack original partial assignment
{a, b, c} find exploring {a, b, c}, short nogood (5) 0-irrelevant
(since unit propagate conclude e) long one (6) 4-irrelevant. Using
relevance bound 3 less, nogood would discarded.
condition nogoods retained irrelevance exceeds bound k
sufficient ensure polynomial number nogoods retained point.7
Experimental results (Bayardo & Miranker, 1996) show relevance-bounded learning
effective length-bounded counterpart and, even relevance bound 1,
results comparable learning without restriction.8
Hybrid Approaches Finally, note solvers employ hybrid approach.
chaff algorithm (Moskewicz et al., 2001) uses relevance bound (larger) length
bound. Clauses must meet relevance bound length bound retained.
Yet another approach taken berkmin algorithm (Goldberg & Novikov, 2002).
Here, set nogoods partitioned two separate groups based recently
15
nogoods acquired; 16
nogoods kept recent group remaining
1
16 old group. relatively large length bound used cull recently acquired
nogoods smaller length bound used aggressively cull smaller group older
7. Although accepted wisdom, know proof literature result; Bayardo Mirankers
(1996) proof appears assume order branch variables chosen fixed. present
general proof next paper series (Dixon et al., 2003b).
8. Results necessarily become somewhat suspect algorithmic methods mature; unfortunate consequence extremely rapid progress satisfiability engines recent years lack
careful experimental work evaluating host new ideas developed.

205

fiDixon, Ginsberg & Parkes

nogoods. aware studies comparing hybrid approaches pure
length-bounded relevance-bounded methods.
2.3 Branching Heuristics
Let us turn examination progress made development
effective heuristics selection branch variables dpll. discussed
introduction, focus specific heuristics work selected domains,
general ideas attempt exploit structure axiomatization directly.
Prior development successful learning techniques, branching heuristics
primary method used reduce size search space. seems likely, therefore,
role branching heuristics likely change significantly algorithms prune
search space using learning. heuristics used zChaff appear first
step direction, little known new role.
Initially, however, primary criterion selection branch variable
pick one would enable cascade unit propagations; result cascade
smaller tractable subproblem.9
first rule based idea called moms rule, branches variable
Maximum Occurrences clauses Minimum Size (Crawford & Auton, 1996;
Dubois, Andre, Boufkhad, & Carlier, 1993; Hooker & Vinay, 1995; Jeroslow & Wang, 1990;
Pretolani, 1993). Moms provides rough easily computed approximation number
unit propagations particular variable assignment might cause.
Alternatively, one use unit propagation rule (Crawford & Auton, 1996; Freeman,
1995), compute exact number propagations would caused branching
choice. Given branching candidate vi , variable separately fixed true false
unit propagation procedure executed choice. precise number unit
propagations caused used evaluate possible branching choices. Unlike moms
heuristic, rule obviously exact attempt judge number unit propagations
caused potential variable assignment. Unfortunately, also considerably
expensive compute expense unit propagation itself. led
adoption composite approaches (Li & Anbulagan, 1997) moms used identify
small number branching candidates, evaluated exactly using
expensive unit propagation heuristic. randomly generated problems, composite
technique outperforms either heuristic isolation.
Another strategy branch variables likely backbone variables (Dubois
& Dequen, 2001). backbone literal (also often referred unary prime implicate
theory) one must true solutions given problem. Given problem C
partial assignment P , backbone heuristic attempts branch variables
backbones subset clauses C satisfied P ; likelihood
particular variable backbone literal approximated counting appearances
9. idea tends obviate need use pure literal rule, well. p pure literal,
particular reason hurry set p true; key thing avoid setting false. p pure,
p cannot generate unit propagations, p tend selected branch variable. Pure
literals obviously never set false unit propagation, heuristics based unit propagation
counts tend achieve advantages pure literal rule without incurring associated
computational costs.

206

fiBoolean Satisfiability Survey

literal satisfied clauses C. heuristic outperforms discussed
previous paragraphs.
heuristics described thus far developed communitys research emphasis focused solution randomly generated satisfiability problems.
development bounded learning methods enabled solvers address issue thrashing,
caused natural shift focus toward structured, realistic problems.
formal studies comparing previously discussed heuristics structured problems,
value studies exist reduced implementations
dpll isolation without learning techniques since proved important.
Branching techniques learning deeply related, addition learning dpll
implementation significant effect effectiveness branching
strategies. new clauses learned, number unit propagations assignment
cause expected vary; reverse also true choice branch variable
affect clauses algorithm learns. formal comparison branching techniques
performance structured problems presence learning would extremely
useful.
Branching heuristics designed function well context learning algorithm generally try branch variables things learned recently.
tends allow implementation keep making progress single section
search space opposed moving one area another; additional benefit
existing nogoods tend remain relevant, avoiding inefficiencies associated losing
information present nogoods become irrelevant deleted. zChaff,
example, count maintained number times literal occurs theory
solved. new clause added, count associated literal
clause incremented. branch heuristic selects variable appears many
clauses possible. periodically dividing counts constant factor, bias
introduced toward branching variables appear recently learned clauses. Like
moms rule, rule inexpensive calculate.
heuristic used berkmin (Goldberg & Novikov, 2002) builds idea
responds dynamically recently learned clauses. berkmin heuristic prefers
branch variables unvalued recently learned clause yet
satisfied, zChaff-like heuristic used break ties.
told, many competing branching heuristics satisfiability solvers,
still much done evaluating relative effectiveness. interesting
experiments done using implementations learn, realistic, structured
problems opposed randomly generated ones.
2.4 Proof Complexity
already commented briefly fact proof systems evaluated based
provable bounds proofs certain classes formulae, development
polynomial transformations proofs one system proofs another.

207

fiDixon, Ginsberg & Parkes

regard first metric, least three classes problems known
exponentially difficult conventional resolution-based provers (including dpll
implementation):10
1. Pigeonhole problems (Haken, 1985)
2. Parity problems (Tseitin, 1970)
3. Clique coloring problems (Bonet, Pitassi, & Raz, 1997; Krajicek, 1997; Pudlak, 1997)
turning discussion problems specifically, however, let us point
many proof systems known powerful
ones discuss paper. perspective, interesting extended
resolution involves introduction new variables correspond arbitrary logical
expressions built original variables theory.
Since logical expressions always built term-by-term, suffices allow
introduction new variables corresponding pairwise combinations existing ones;
since disjunction replaced conjunction using de Morgans laws, suffices
introduce new variables form
w xy
(7)
literals x y. Writing (7) disjunctive normal form, get:
Definition 2.6 (Tseitin, 1970) extended resolution proof theory one
first augmented collection groups axioms, group form
x w
x w

(8)

w
x literals (possibly already extended) theory w new variable.
Following this, derivation proceeds using conventional resolution augmented theory.
proof system known stronger extended resolution; fact,
class problems known polynomially sized proofs
extended resolution.
stressed, however, fact proof system strong mean
works well practice. know implementation extended resolution
simple reason virtually nothing known select new variables
shorten proof length.
Understanding introduction new variables reduce proof length
considerably simpler. example, suppose resolution proof,
managed derive nogood ax, also derived ay. order complete
proof, need perform lengthy identical analyses nogoods,
eventually deriving simply x first second (and resolving
x y, example).
10. hard problems well, Hakens (1995) broken mosquito screen problem.
three examples quoted sufficient purposes.

208

fiBoolean Satisfiability Survey

could replace pair nogoods x single nogood w
using (8), two proofs x could collapsed single proof,
potentially halving size proof entirety. Introducing still variables
repeat effect, resulting exponential reductions proof size.
Another way look improvement expressivity. simply
way write (a x) (a y) equivalent (x y) single Boolean axiom
disjunctive normal form. power extended resolution rests fact subexpression substitution makes possible capture expressions (x y) single
axiom.
None proof systems considered survey powerful extended
resolution, therefore evaluate based performance three
problems mentioned beginning section. Let us therefore describe
problems detail.
2.4.1 Pigeonhole problems
pigeonhole problem involves showing impossible put n + 1 pigeons n
holes pigeon must go distinct hole. write pij fact pigeon
hole j, straightforward axiomatization says every pigeon must least
one hole:
pi1 pi2 pin = 1, . . . , n + 1
(9)
two pigeons hole:
pik pjk 1 < j n + 1 k = 1, . . . , n

(10)

Note (n3 ) axioms form (10).
well known polynomial-sized proof unsatisfiability
axioms (9)(10) (Haken, 1985). proof technical, essential reason
pigeonhole problem counting. point, proving cant put
n + 1 pigeons n holes requires saying cant put n pigeons last n 1
holes, thus n 1 pigeons last n 2 holes, on. Saying language
sat awkward, possible show proof pigeonhole problem
completed without, point, working extremely long individual clauses.
again, see connection expressive efficiency; readers interested additional
details, Pitassis (2002) explanation reasonably accessible.
2.4.2 Parity problems
parity problem, mean collection axioms specifying parity sets
inputs. write, example,
x1 xn = 1

(11)

indicates odd number xi true; right hand side zero would indicate
even number true. indicates exclusive or.

209

fiDixon, Ginsberg & Parkes

Reduction (11) collection Boolean axioms best described example.
parity constraint x z = 1 equivalent
xyz
x z
x z
x z
general, number Boolean axioms needed exponential length parity
clause (11), clauses fixed length, number axioms obviously fixed
well.
proof complexity result interest, suppose G graph, node
G correspond clause edge literal. label edges distinct
literals, label node graph zero one. n node
graph labeled value vn edges e1n , . . . , ei(n),n incident n labeled
literals l1n , . . . , li(n),n , add theory Boolean version clause
l1n li(n),n = vn

(12)

Since every edge connects two nodes, every literal theory appears exactly twice
axioms form (12). Adding constraints therefore produces value
P
P
equivalent zero mod 2 must equal n vn well. n vn odd, theory
unsatisfiable. Tseitins (1970) principal result show unsatisfiability cannot
general proven number resolution steps polynomial size Boolean
encoding.
2.4.3 Clique coloring problems
last examples consider known clique coloring problems.
derivatives pigeonhole problems exact nature pigeonhole problem
obscured. Somewhat specifically, problems indicate graph includes clique
n + 1 nodes (where every node clique connected every other),
graph must colored n colors. graph known clique, problem
equivalent pigeonhole problem. know clique embedded
graph, problem difficult.
axiomatization, use eij describe edges graph, cij describe
coloring graph, qij describe embedding cliQue graph.
graph nodes, clique size n + 1, n colors available.
axiomatization is:
eij cil cjl

1 < j m, l = 1, . . . , n

(13)

ci1 cin

= 1, . . . ,

(14)

qi1 qim

= 1, . . . , n + 1

(15)

1 < k n + 1, j = 1, . . . ,

(16)

1 < j m, 1 k 6= l n + 1

(17)

qij qkj
eij qki qlj

210

fiBoolean Satisfiability Survey

eij means edge graph nodes j, cij means graph
node colored jth color, qij means ith element clique
mapped graph node j. Thus first axiom (13) says two nodes
graph cannot color (of n colors available) connected edge.
(14) says every graph node color. (15) says every element clique
appears graph, (16) says two elements clique map
node graph. Finally, (17) says clique indeed clique two clique
elements map disconnected nodes graph.
Since polynomially sized resolution proof pigeonhole problem
Boolean satisfiability, obviously polynomially sized proof clique coloring problems, either. shall see, clique coloring problems cases
used distinguish among elements proof complexity hierarchy.
2.5 Boolean Summary
summarize results section completing first row table follows:

SAT

rep. eff.
1

p-simulation
hierarchy
EEE

inference
resolution

unit
propagation
watched literals

learning
relevance

cardinality
PB
symmetry
QPROP

entries really informal shorthand:
Representational efficiency: Boolean satisfiability benchmark
languages measured; give relative savings
changing representation.
p-simulation hierarchy: give proof complexity three problem classes
discussed Section 2.4. Boolean satisfiability, problems require proofs
exponential length.
Inference: basic inference mechanism used dpll resolution.
Propagation: Watched literals lead efficient implementation.
Learning: Relevance-based learning appears effective polysized methods.

3. Pseudo-Boolean Cardinality Constraints
entries previous table summarize fact Boolean satisfiability weak
method admits efficient implementations. representation relatively inefficient,
none canonical problems solved polynomial time.
difficulties, least, overcome via representational shift.

211

fiDixon, Ginsberg & Parkes

understand shift, note write axiom
x z

x+y+z 1

(18)

thinking x, z variables value either 0 (false) 1 (true)
written z 1 z or, equivalently, z. v variable, continue refer
v negation v.
familiar logical operations obvious analogs notation. If,
example, want resolve
b c

b
get c d, simply add
a+b+c1

b+d1
simplify using identity b + b = 1 get
a+c+d1
required.
Whats nice notation extends easily general descriptions.
P
general form disjunction li literals
li 1 (18), drop
requirement right-hand side 1:
Definition 3.1 cardinality constraint extended clause constraint form
X

li k



k integer li required value 0 1.
cardinality constraint simply says least k literals question true.
Proposition 3.2 (Cook, Coullard Turan (1987)) unsatisfiability proof
polynomial length pigeonhole problem using cardinality constraints.
Proof. Cook et al. (1987) give derivation o(n3 ) steps; presented o(n2 )
derivation elsewhere (Dixon & Ginsberg, 2000).
course, fact extension sat language allows us find polynomiallength derivations pigeonhole problem necessarily show change
computational value; need examine columns table well.
remainder section, show go further, describing new
computational techniques applied broader setting
considering. Experimental results also presented. let us begin examining
first column table:
212

fiBoolean Satisfiability Survey

Proposition 3.3 (Benhamou, Sais, & Siegel, 1994) cardinality constraint
x1 + + xm k
logically equivalent set


k1

(19)

axioms

xi1 + + ximk+1 1

(20)

every set k + 1 distinct variables {xi1 , . . . , ximk+1 }. Furthermore,
compact Boolean encoding (19).
Proof. first show (19) implies (20). see this, suppose set
k + 1 variables. Suppose also set xi true, size
least k. Since variables, 6= least one xi must
true.
see (20) implies (19), suppose (20) true appropriate sets xi s.
(19) false, set false xi would size least k + 1,
instance (20) would unsatisfied.
see efficient encoding, first note (19) implies Boolean
axiom
x1 xk xk+1 xm
must also imply
x1 xk
since always change xi false true without reducing satisfiability
(19).
Next, note axiom length less k + 1 consequence (19), since
axiom falsified satisfying (19) setting every unmentioned variable
true rest false.
Finally, suppose leave single instance (20) include others
well every clause length greater k + 1. setting variables false
every variable true, given clauses satisfied (19)


be. follows Boolean equivalent (19) must include least k1
instances
(20).
follows Proposition 3.3 provided new variables introduced, cardinality constraints exponentially efficient Boolean counterparts.
discussing columns table, let us consider extending
representation include known pseudo-Boolean constraints:
Definition 3.4 pseudo-Boolean constraint axiom form
X

wi li k

(21)



wi k positive integer li required value 0 1.

213

fiDixon, Ginsberg & Parkes

Pseudo-Boolean representations typically allow linear inequalities linear equalities Boolean variables. Linear equalities easily translated pair inequalities form definition; prefer inequality-based description (Barth, 1996;
Chandru & Hooker, 1999, also known pseudo-Boolean normal form) better
analogy Boolean satisfiability unit propagation becomes unmanageable
equality constraints considered. Indeed, simply determining equality clause
satisfiable subsumes subset-sum therefore (weakly) NP-complete (Garey & Johnson,
1979).
Compare (21) Definition 3.1; wi weights attached various literals.
pseudo-Boolean language somewhat flexible still, allowing us say (for example)
2a + b + c 2
indicating either true b c (equivalent crucial representational
efficiency obtained extended resolution). see shortly, natural make
extension result resolving two cardinality constraints
naturally written form.
3.1 Unit Propagation
Let us begin discussing propagation techniques cardinality pseudo-Boolean setting.11
pseudo-Boolean version unit propagation first presented Barth (1996)
described number papers (Aloul, Ramani, Markov, & Sakallah, 2002; Dixon &
Ginsberg, 2000). Boolean case, describe clause unit contains
satisfied literals one unvalued one. generalize pseudo-Boolean
setting, make following definition, view partial assignment P simply
set literals values true:
Definition 3.5 Let wi li k pseudo-Boolean clause, denote c.
suppose P partial assignment values variables. say
current value c P given
P

X

curr(c, P ) =

wi k

{i|li P }

ambiguity possible, write simply curr(c) instead curr(c, P ).
words, curr(c) sum weights literals already satisfied P , reduced
required total weight k.
similar way, say possible value c P given
X

poss(c, P ) =

wi k

{i|li 6P }

11. remarked, table designed reflect issues involved lifting dpll
expressive representation. Extending nonsystematic search technique wsat pseudoBoolean setting discussed Walser (1997) Prestwich (2002).

214

fiBoolean Satisfiability Survey

ambiguity possible, write simply poss(c) instead poss(c, P ).
words, poss(c) sum weights literals either already satisfied
valued P , reduced required total weight k.12
Definition 3.6 Let c clause, P partial assignment. say c unit
variable v appearing P either P {v} P {v} cannot
extended assignment satisfies c.
situation, variable v forced take value help satisfy clause.
creates new consequential assignment. Note c already unsatisfiable,
meet conditions definition choosing v variable assigned value
P . Note also pseudo-Boolean case, clause may actually contain
one variable forced specific value. clear Boolean case,
definition duplicates conditions original unit propagation procedure 2.3.
Lemma 3.7 partial assignment P extended way satisfies clause c
poss(c, P ) 0.
Proof. Assume first poss(c, P ) 0, suppose value every remaining
variable way helps satisfy c. done so, every literal c
currently made false P true, resulting value c
X

wi li =



X

wi = poss(c, P ) + k k

{i|li 6P }

c becomes satisfied.
Conversely, suppose poss(c, P ) < 0. best still value
unvalued literals favorably, value c becomes
X


wi li =

X

wi = poss(c, P ) + k < k

{i|li 6P }

c unsatisfiable.
Proposition 3.8 clause c containing least one unvalued literal unit c
contains unvalued literal li weight wi > poss(c).
Proof. literal weight wi > poss(c), setting literal false reduce
poss(c) wi , making negative thus making c unsatisfiable. Conversely,
literal, poss(c) remain positive single unvalued literal set,
c remains satisfiable therefore unit.
Given result, little impact time needed find unit clauses.
need simply keep literals clause sorted weight maintain,
clause, value poss weight largest unvalued literal. value literal
different weight, apply test Proposition 3.8 directly. value literal
given weight, short walk along clause allow us identify new unvalued
literal maximum weight, proposition continues apply.
12. Chai Kuehlmann (2003) refer poss slack.

215

fiDixon, Ginsberg & Parkes

Watched literals Generalizing idea watched literals difficult. make
following definition:
Definition 3.9 Let c clause. watching set c set variables
property c cannot unit long variables either unvalued
satisfied.
Proposition 3.10 Given clause c form
watching set c
X

P

wi li

k, let set variables.

wi max wi k




(22)

sum maximum taken literals involving variables S.
Proof. Suppose variables unvalued satisfied. let lj
P
unvalued literal c. lj 6 S, poss(c) wj + wi k thus poss(c) wj since
P
P
wi
wi maxi wi k. If, hand, lj S,
poss(c)

X

wi k




X

wi wj

X

wi max wi k






Combining these, get
poss(c) wj
Either way, cannot poss(c) < wj Proposition 3.8 therefore implies c
cannot unit. follows watching set.
P
converse simpler. wi maxi wi < k, value every literal outside
P
make c false. poss(c) = wi k, lj literal greatest weight,
associated weight wj satisfies wj > poss(c) c unit. Thus cannot watching
set.
generalizes definition Boolean case, fact made even obvious
by:
Corollary 3.11 Given cardinality constraint c requiring least k associated literals
true, watching set c includes least k + 1 literals c.
Proof. expression (22) becomes
X


1 max 1 k



|S| 1 k.

216

fiBoolean Satisfiability Survey

3.2 Inference Resolution
unit propagation, resolution also lifts fairly easily pseudo-Boolean setting.
general computation follows:
Proposition 3.12 Suppose two clauses c c0 , c given
X

wi li + wl k

(23)

wi0 li0 + w0 l k 0

(24)



c0 given
X


legitimate conclude
X


w0 wi li +

X

wwi0 li0 w0 k + wk 0 ww0

(25)



Proof. immediate. Multiply (23) w0 , multiply (24) w, add simplify using
l + l = 1.
weights, k k 0 1, generalizes conventional resolution provided
sets nonresolving literals c c0 disjoint. deal case
overlap set li set li0 , need:
Lemma 3.13 Suppose c clause wi li k. c equivalent
wi0 given by:

wi , wi < k;
wi0 (j) =
k, otherwise.
P

0
wi li

P

k,

Proof. lj literal wj k, c rewrite true lj satisfied.
lj = 0, c rewrite equivalent.
words, reduce coefficient greater required
satisfy clause entirety, example rewriting
3x + + z 2

2x + + z 2
either equivalent x (y z).
Proposition 3.14 (Cook et al., 1987; Hooker, 1988) construction Proposition 3.12 generalizes conventional resolution.

217

fiDixon, Ginsberg & Parkes

Proof. already discussed case sets li li0 disjoint.
literal li c negation literal c0 , li + li (25),
simplify 1 make resolved constraint trivial; resolution produces
result. literal li c also appears c0 , coefficient literal
resolvent (25) 2 reduced 1 virtue lemma.
Cardinality constraints bit interesting. Suppose resolving
two clauses
+ b + c
2
+
c + 1
add get
2a + b + 2

(26)

words, either true b are. problem
cardinality constraint, cannot rewritten one.
One possibility rewrite (26) pair cardinality constraints
a+b 1

(27)

a+d 1

(28)

If, however, want result resolving pair constraints single axiom,
must either select one axioms extend language further.
3.3 Learning Relevance Bounds
idea relevance also natural generalization pseudo-Boolean setting. Recall
basic definition Section 2.2:
Definition 3.15 Let c clause P partial assignment. c i-irrelevant
number literals c either unvalued true P least + 1.
Proposition 3.16 Given partial assignment P Boolean clause c, c i-irrelevant
poss(c, P ) i.
Proof. Boolean case, number literals c either unvalued true
poss(c, P ) + 1 since right hand side constraint always 1. irrelevance
condition
poss(c, P ) + 1 + 1
result follows.
pseudo-Boolean case, additional learning techniques also possible.
present ideas detail, however, let us point sort inferential
extension needed overcome shortcomings dpll revealed
pigeonhole problems. all, recall Proposition 3.14: pseudo-Boolean inference
generalizes Boolean resolution. begin Boolean axiomatization (as
pigeonhole problem), derivation using techniques reproducible using
conventional resolution-based methods, therefore exponential length. (A
majority inference steps various proofs Proposition 3.2 resolution
steps literal cancellations occur.)
218

fiBoolean Satisfiability Survey

Strengthening specific method discuss operations research
used preprocess mixed integer programming problems (Guignard & Spielberg, 1981;
Savelsbergh, 1994).
Suppose setting l0 true applying form propagation
P
constraint set, discover assumption constraint c given
wi li r
becomes oversatisfied amount sum left hand side greater
(by s) amount required right hand side inequality; terms
Definition 3.5, curr(c) = s. oversatisfied constraint c replaced
following:
X
wi li r +
(29)
sl0 +
l0 true, know wi li r + s, (29) holds. l0 false, sl0 =
P
still must satisfy original constraint wi li r, (29) still holds. new constraint
implies original one, information lost replacement.
remarked, community uses technique preprocessing.
literal fixed, propagation applied, oversatisfied constraint strengthened.
Consider following set clauses:
P

a+b1
a+c1
b+c1
set false, must value b c true order satisfy first two
constraints. third constraint oversatisfied thus replaced
a+b+c2
power method allows us build complex axioms set
simple ones. strengthened constraint often subsume constraints involved generating it. case, new constraint subsumes three
generating constraints.
Proposition 3.17 Let c constraint P partial assignment. conclude curr(c) solution overall problem extends P , replace
c
X
X

li +
wi li r +
(30)
P

first summation literals li P .
Proof. truth assignment extends P , (30) follows fact curr(c)
s. truth assignment P 0 extend P , lj P false
P 0 ,
X
li

P

Combining original constraint c produces (30).

219

fiDixon, Ginsberg & Parkes

Instance
2pipe
2pipe-1-ooo
2pipe-2-ooo
3pipe
3pipe-1-ooo
3pipe-2-ooo
3pipe-3-ooo
4pipe

zChaff
sec nodes
0
8994
1
10725
0
6690
7
48433
6
33570
9
41251
11
46504
244 411107

pbchaff
sec nodes
0
8948
1
9534
0
6706
12
57218
9
36589
16
45003
19
57370
263 382750

Table 1: Run time (seconds) nodes expanded

Learning inference present experimental results related
effectiveness pseudo-Boolean inference, point one additional problem
arise setting. possible branch variable v, result resolving
reasons v v new nogood falsified partial assignment
v search space.
example (Dixon & Ginsberg, 2002), suppose partial assignment
{a, b, c, d} constraints
2e + + c 2

(31)

2e + b + 2

(32)

unit propagate conclude e virtue (31) e virtue (32); isnt
hard conclude conflict set b either b must true (31)
(32) simultaneously satisfiable. simply add (31) (32) simplify,
get
a+b+c+d2
still allows b false. difficulty addressed deriving
cardinality constraint guaranteed falsified current partial solution
investigated (Dixon & Ginsberg, 2002); Chai Kuehlmann (2003) developed still
stronger method.
Experimental results Many ideas described implemented
pbchaff satisfiability solver. earlier paper (Dixon & Ginsberg, 2002), compared results obtained using prs, pseudo-Boolean version relsat, obtained
using relsat (Bayardo & Miranker, 1996). Pbchaff updated version prs
modeled closely zChaff (Moskewicz et al., 2001). implements watched literals
cardinality constraints applies strengthening idea. compare pbchaffs
performance Boolean counterpart zChaff.
Results (unsatisfiable) problem instances Velev suite discussed
beginning Section 2.1 shown Table 1. seen, performance comparable; pbchaff pays small (although noticeable) cost extended expressivity.
220

fiBoolean Satisfiability Survey

Instance
hole8.cnf
hole9.cnf
hole10.cnf
hole11.cnf
hole12.cnf
hole20.cnf
hole30.cnf
hole40.cnf
hole50.cnf

zChaff
sec
nodes
0
3544
1
8144
17
27399
339 126962

Preprocess
sec
0
0
0
0
0
0
4
25
95

pbchaff
sec nodes
0
11
0
12
0
17
0
15
0
20
0
34
0
52
0
75
0
95

Table 2: Run time (seconds) nodes expanded

experiments run 1.5 GHz AMD Athlon processor, solvers used
values various tuning parameters available (relevance length bounds, etc.).
Results pigeonhole problem appear Table 2. case, pbchaff
permitted preprocess problem using strengthening described earlier section.
ZChaff unable solve problem twelve pigeons 1000-second
timeout using 1.5 GHz Athlon processor. surprisingly, pbchaff preprocessing
dramatically outperformed zChaff instances.13
3.4 Proof Complexity
already shown Proposition 3.2 pseudo-Boolean cardinality-based axiomatizations produce polynomially sized proofs pigeonhole problem. also
known methods lead polynomially sized proofs clique coloring
problem (Bonet et al., 1997; Krajicek, 1997; Pudlak, 1997). situation regard
parity constraints bit interesting.
Let us first point possible capture parity constraints, modularity
constraints generally pseudo-Boolean setting:
Definition 3.18 modularity constraint constraint c form
X

wi li n(mod m)

(33)



positive integers wi , n m.
remainder section, show modularity constraints easily
encoded using pseudo-Boolean axioms, also constraint sets consisting entirely
mod 2 constraints easily solved either directly using encoding, although
clear recover pseudo-Boolean encodings Boolean versions.
13. Without preprocessing, two systems perform comparably class problems.
stressed, representational extensions little use without matching modifications inference methods.

221

fiDixon, Ginsberg & Parkes

Modularity constraints pseudo-Boolean encodings encode modularity
constraint way, first note easily capture equality axiom
form
X
wi li = k
(34)


pseudo-Boolean setting, simply rewriting (34) pair constraints
X

wi li k



X

wi li

X

wi k





follows, therefore feel free write axioms form (34).
denote bxc floor x, say smallest integer greater
x, have:
Proposition 3.19 Suppose modularity constraint form (33). set
P
w
w = wi introduce new variables si = 1, . . . , b
c. (33) equivalent
X


wi li +

X

jwk

msi =





+n

(35)

Proof. Reducing sides (35) mod shows (35) clearly implies (33).
P
converse, note (33) satisfied, integer wi li = sm + n.
P
P
w
Further, since wi li wi = w, follows sm + n w, wn

w
thus b
c. therefore satisfy (35) valuing exactly many si
true.
Understand introduction new variables part intended
inference procedure; simply fashion modularity constraints
captured within pseudo-Boolean setting.
case constraints parity constraints, have:
Proposition 3.20 set mod 2 constraints solved polynomial time.
Proof. individual constraint (recall corresponds exclusive or, addition
mod 2)
l li = n
viewed simply defining
l = n li
definition inserted remove l remaining constraints. Continuing
way, either define variables (and return solution) derive
1 = 0 return failure.
result, thought little application Gaussian
elimination, also instance far general result Schaefers (1978).
Proposition 3.21 set mod 2 constraints solved polynomial time using
pseudo-Boolean axiomatization given (35).
222

fiBoolean Satisfiability Survey

Proof. technique unchanged. combine
l+

X

li + 2

X



si = n




l+

li0 + 2

X

X



s0i = n0



get
X


li +

X

li0 + 2(

X



si +

X



s0i + l) = n + n0



treat l one auxiliary variables. Eventually, get
2

X

si = n



large (but polynomially sized) set auxiliary variables n either
even odd. n even, value variables return solution; n odd
k auxiliary variables,
X

si =




X

si



n
2

n+1
2

(36)

since si integral. also
2

X

si 2k n




X

si k



n1
2

(37)

Adding (36) (37) produces k k + 1, contradiction.
Let us point out, however, mod 2 constraint encoded normal Boolean
way, x z = 1 becomes
xyz

(38)

x z
x z
x z

(39)

obvious pseudo-Boolean analog reconstructed. problem
mentioned beginning section: enough simply extend
representation; need extend inference methods well. fact, even question
whether families mod 2 constraints solved polynomial time pseudo-Boolean
methods without introduction auxiliary variables (35) open. authors
also considered problem reasoning constraints directly (Li, 2000).
223

fiDixon, Ginsberg & Parkes

Pseudo-Boolean constraints extended resolution Finally, let us clarify point
made earlier. Given encoding a(bc) single pseudo-Boolean
clause, pseudo-Boolean inference properly extended resolution
p-simulation hierarchy?
answer follows. fact (b c) logically equivalent
2a + b + c 2 allows us remove one variables introduced extended resolution,
cannot combine encoding others remove subsequent variables. specific
example, suppose learn
(b c)

(b c)
wish conclude
(a d) (b c)

(40)

single pseudo-Boolean axiom equivalent (40).
3.5 Summary

SAT

cardinality
PB
symmetry

rep. eff.
1
exp
exp

p-simulation
hierarchy
EEE
P?E
P?E

inference
resolution
unique
uniquely defined

unit
propagation
watched literals
watched literals
watched literals

learning
relevance
relevance
+ strengthening

QPROP

before, notes order:
cardinality pseudo-Boolean representations exponentially
efficient Boolean counterpart, clear often compressions
magnitude occur practice.
entries p-simulation column indicate pigeonhole problem easy,
clique coloring remains hard, complexity parity problems unknown
new variables introduced.
cardinality entry inference intended reflect fact natural
resolvent two cardinality constraints need one.
Pseudo-Boolean systems use existing learning techniques, augmented
strengthening idea.

224

fiBoolean Satisfiability Survey

4. Symmetry
Given pigeonhole problem clique-coloring problems involve great deal
symmetry arguments, variety authors suggested extending Boolean representation inference way allows symmetry exploited directly.
discuss variety approaches proposed separating based
whether modification basic resolution inference rule suggested.
event, make following definition:
Definition 4.1 Let collection axioms. symmetry mean
permutation variables leaves unchanged.
example, consists single axiom x y, clearly symmetric
exchange x y. contains two axioms
ax

ay
symmetric exchange x y.
Exploiting symmetry without changing inference One way exploit symmetry
modify set axioms way captures power symmetry.
pigeonhole problem, example, argue since symmetry
exchange pigeons holes, assume without loss generality pigeon 1
hole 1, virtue residual symmetry pigeon 2 hole 2, on.
basic idea add so-called symmetry-breaking axioms original theory,
axioms break existing symmetry without affecting overall satisfiability
theory itself. idea introduced Crawford et al. (1996).
attractive theory, least two fundamental difficulties
symmetry-breaking approach:
1. Luks Roy (2002) shown breaking symmetries particular problem may require introduction set symmetry-breaking axioms
exponential size. problem sidestepped breaking
symmetries, although little known set broken symmetries
selected.
2. Far serious, technique applied symmetry question
global. basic argument satisfiability unaffected
introduction new axioms requires additional axioms consider.
theoretical problems, global symmetries exist. practice, even addition
asymmetric axioms constrain problem (e.g., cant put pigeon 4
hole 7) break required global symmetry render method inapplicable.
problematic still possibility symmetries obscured replacing
single axiom
p11 p21
(41)
225

fiDixon, Ginsberg & Parkes

equivalent pair
p11 p21

p11 p21
(41) obviously recovered using resolution. again, symmetry
original problem vanished method cannot applied.
arguments could perhaps anticipated consideration usual
table; since inference mechanism modified (and possible break global
symmetries), none entries changed. Let us turn, then, techniques
modify inference itself.
Exploiting symmetry changing inference Rather modifying set clauses
problem, also possible modify notion inference, particular
nogood derived, symmetric equivalents derived single step. basic
idea due Krishnamurthy (1985) follows:
Lemma 4.2 Suppose |= q theory nogood q. symmetry
, |= (q).
hard see technique allows pigeonhole problem solved
polynomial time, since symmetric versions specific conclusions (e.g., pigeon 1 hole
1) derived without repeating analysis led original. dependence
global symmetries remains, addressed following modification:
Proposition 4.3 Let theory, suppose 0 |= q 0 nogood
q. symmetry 0 , |= (q).
Instead needing find symmetry theory entirety, suffices find
local symmetry subset actually used proof q.
idea, generalized somewhat Szeider (2003), allows us avoid
fact introduction additional axioms break global symmetry. problem
symmetries obscured (41) remains, however, accompanied
new one, need identify local symmetries inference step (Brown et al., 1988).
straightforward identify support new nogood q terms
subtheory 0 original theory , finding symmetries particular theory
equivalent graph isomorphism problem (Crawford, 1992). precise complexity
graph isomorphism unknown, felt likely properly P N P
(Babai, 1995). basic table becomes:

SAT

cardinality
PB
symmetry

rep. eff.
1
exp
exp
1

p-simulation
hierarchy
EEE
P?E
P?E
EEE

inference
resolution
unique
unique
P

QPROP

226

unit
propagation
watched literals
watched literals
watched literals
sat

learning
relevance
relevance
+ strengthening
sat

fiBoolean Satisfiability Survey

clear representational efficiency system described, since
single concluded nogood serve standin image symmetries
proof produced it.
specific instances pigeonhole problem clique coloring problems
addressed using symmetries, even trivial modifications problems render
techniques inapplicable. Hence appearance asterisk table: Textbook problem instances may admit polynomially sized proofs,
instances require proofs exponential length. Parity problems seem
amenable techniques all.
remarked, inference using Krishnamurthys related ideas appears
require multiple solutions graph isomorphism problem, therefore unlikely
remain P .
know implemented system based ideas discussed section.

5. Quantification QPROP
conclude survey examination ideas used trying
extend Boolean work cope theories naturally thought using
quantification sort. Indeed, Boolean satisfiability engines applied ever
larger problems, many theories question produced large part constructing
set ground instances quantified axioms
xyz.[a(x, y) b(y, z) c(x, z)]

(42)

size domain x, z taken, single axiom d3
ground instances. Researchers dealt difficulty buying machines
memory finding clever axiomatizations ground theories remain manageably
sized (Kautz & Selman, 1998). general, however, memory cleverness scarce
resources natural solution needs found.
call clause (42) quantified , assume throughout section
quantification universal opposed existential, domains quantification
finite.14
remarked beginning section, quantified clauses common
encodings realistic problems, problems general solved converting
quantified clauses standard propositional formulae. quantifiers expanded first
(possible domains quantification finite), resulting set predicates
linearized relabeling atoms that, example, a(2, 3) might become
v24 . number ground clauses produced exponential number variables
quantified clause.
14. appears effective alternative treat existentially quantified clauses simple disjunctions, (9).

227

fiDixon, Ginsberg & Parkes

5.1 Unit Propagation
primary goal work quantified formulation directly, opposed
much larger ground translation. Unfortunately, significant constant-factor costs
incurred so, since inference step need deal issues involving
bindings variables question. Simply finding value assigned a(2, 3) might
well take several times longer finding value assigned equivalent v24 . Finding
occurrences given literal achieved ground case simple indexing
schemes, whereas quantified case likely require unification step.
unification performed time linear length terms unified,
obviously efficient simple equality check. routine essential operations
expected significantly slow cost every inference undertaken system.
fundamental point costs associated using quantified
axioms, significant savings well. savings consequence fact
basic unit propagation procedure uses amount time scales roughly linearly
size theory; use quantified axioms reduce size theory
substantially constant-factor costs overcome.
make argument two phases. Section 5.1.1, generalize specific
computational subtask shared unit propagation satisfiability procedures
wsat. show generalization NP-complete formal sense,
call subsearch reason. specific procedures unit propagation
needed wsat encounter NP-complete subproblem inference step,
show subsearch generally problem randomly generated theories,
subsearch cost expected dominate running time realistic instances.
Section 5.1.2, discuss consequences fact subsearch NP-complete.
Search techniques used speed solution NP-complete problems, subsearch
exception. show quantified axiomatizations support application simple
search techniques subsearch problem, argue realistic examples likely
lead subsearch problems polynomial difficulty although existing unit propagation
implementations solve exponentially.
5.1.1 Subsearch
iteration dpll (or wsat) involves search original theory clauses
satisfy numeric property. specific examples already seen
following:
1. Procedure 2.2 (dpll) (and similarly wsat), need determine P
solution problem hand. involves searching unsatisfied clause.
2. Procedure 2.3 (unit propagation), need find unsatisfied clauses contain
one unvalued literal.
addition, wsat needs compute number clauses become unsatisfied
particular variable flipped.
tasks rewritten using following:

228

fiBoolean Satisfiability Survey

Definition 5.1 Suppose C set quantified clauses, P partial assignment
values atoms clauses. denote Sus (C, P ) set ground instances
C u literals unvalued P literals satisfied assignments P .15
say checking problem determining whether Sus (C, P ) 6= .
subsearch problem, mean instance checking problem, problem
either enumerating Sus (C, P ) determining size.
Proposition 5.2 fixed u s, checking problem NP-complete.
Proof. Checking NP, since witness Sus (C, P ) 6= need simply give suitable
bindings variables clause C.
see NP-hardness, assume u = = 0; cases significantly different.
reduce binary constraint satisfaction problem (csp), producing single clause
C set bindings P S00 (C, P ) 6= original binary csp
satisfiable. basic idea variable constraint problem become
quantified variable C.
Suppose binary csp variables v1 , . . . , vn binary
constraints form (vi1 , vi2 ) ci , (vi1 , vi2 ) pair variables constrained
ci . constraint, introduce corresponding binary relation ri (vi1 , vi2 ),
take C single quantified clause v1 , . . . , vn . ri (vi1 , vi2 ). assignment P ,
set ri (vi1 , vi2 ) false (vi1 , vi2 ) ci , true otherwise.
note since P values every instance every ri , S00 (C, P ) nonempty
set values vi every literal ri (vi1 , vi2 ) false. Since
literal ri (vi1 , vi2 ) false case original constraint ci satisfied, follows
S00 (C, P ) 6= original csp satisfiable.
moving on, let us place result context. First, important, note
fact checking problem NP-complete imply qprop
unwieldy representation; subsearch problem indeed appear exponential
size qprop axioms, exponentially fewer ground
case. So, similar results elsewhere (Galperin & Wigderson, 1983; Papadimitriou,
1994), net effect complexity.
Second, result embodied Proposition 5.2 appears general phenomenon
propagation difficult compact representations. earlier discussion
cardinality pseudo-Boolean axioms, complexity unit propagation
unchanged Boolean case, appears much exception
rule. already remarked, extend pseudo-Boolean representation
slightly, addition axioms form
X

wi li k

(43)



Definition 3.4 allow axioms
X

wi li = k



15. interpreting expression Sus (C, P ), set C clauses partial assignment P generally
clear context. superscript refers number satisfied literals satisfied literals
super good subscript refers unvalued literals unvalued literals arent
good.

229

fiDixon, Ginsberg & Parkes

(replacing inequality (43) equality), determining whether single axiom
satisfiable becomes weakly NP-complete. Symmetry, example examined,
involves effective change representational power single axiom.
recasting unit propagation terms Definition 5.1:
Procedure 5.3 (Unit propagation) compute Unit-Propagate(P ):
1
2
3
4
5

S00 (C, P ) = S10 (C, P ) 6=
select c S10 (C, P )
v variable c unassigned P
P P {v = V : V selected c satisfied}
return P

important recognize recasting changing procedure significant way; simply making explicit subsearch tasks previously described
implicitly. procedure unchanged, procedural details variable value choice heuristics irrelevant general point unit propagation
depends solving subsearch instance every step. Wsat similar.
5.1.2 Subsearch quantification
discussed Section 2.1, efficient implementations sat solvers go great lengths
minimize amount time spent solving subsearch problems. watched literal
idea efficient mechanism known here, discuss problem terms
simpler scheme maintains updates poss curr counts clause.
discussed earlier, scheme half fast watched literal approach,
general arguments make expected lead constant-factor
improvements.16
notational convenience follows, suppose C quantified theory
l ground literal. Cl mean subset clauses C include
terms l instance. C contains quantified clauses, Cl well;
clauses Cl found matching literal l clauses C.
discussed Section 2.1, possible compute Sus (C, P ) initialization phase, update incrementally. terms Definition 5.1, update rule
might one
S00 (C, P 0 ) = S00 (C, P ) S10 (Cl , P )
literal l changed unvalued true. P 0 partial assignment
update; P assignment before. compute number fully assigned unsatisfied
clauses update, start number before, add newly unsatisfied clauses
(unsatisfied clauses previously containing single unvalued literal l).
argued previously, reorganizing computation way leads substantial
speedups subsearch problem solved longer NP-complete size
16. know effective way lift watched literal idea qprop setting. see
discuss zap implementation (Dixon et al., 2003a), still broader generalization allows watched
literals return elegant far general way.

230

fiBoolean Satisfiability Survey

C, size Cl Cl . incremental techniques essential performance modern search implementations runtime implementations
dominated time spent propagation (i.e., subsearch).
Given subsearch computation time potentially exponential size
subtheory Cl literal l valued unvalued, let us consider questions
much concern practice, (if anything) done it.
all, one primary lessons recent satisfiability research problems
NP-hard theory tend strongly exponentially difficult practice.
Let us begin noting subsearch likely much issue
randomly generated satisfiability problems focus research 1990s
drove development algorithms wsat. reason n
number clauses theory C v number variables C, random problems
difficult fairly narrow ranges values ratio n/v (Coarfa, Demopoulos,
San Miguel Aguirre, Subramanian, & Vardi, 2000). 3-SAT (where every clause C
contains exactly three literals), difficult random problems appear n/v 4.2 (Kirkpatrick
& Selman, 1994). problem, number clauses particular literal
l appears small (on average 3 4.2/2 = 6.3 random 3-SAT). Thus size
relevant subtheory Cl Cl also small, subsearch cost still tends
dominate running time algorithms question, little gained
applying sophisticated techniques reduce time needed examine relative handful
clauses.
realistic problems, situation dramatically different. axiom
logistics domain encoded satplan style (Kautz & Selman, 1992):
at(o, l, t) duration(l, l0 , dt)
between(t, t0 , + dt) at(o, l0 , t0 )

(44)

axiom says object location l time takes time dt fly
l l0 , t0 + dt, cannot l0 t0 .
given ground atom form at(o, l, t) appear |t|2 |l| clauses form,
|t| number time points increments |l| number locations. Even
100 each, 106 axioms created seem likely make computing Sus (Cl , P )
impractical.
Let us examine computation bit detail. Suppose indeed
variable = at(O, L, ) fixed O, L , interested counting
number unit propagations possible set true. words,
want know many instances (44) unsatisfied single unvalued
literal so.
Existing implementations, faced problem (or analogous one wsat
another approach used), consider axioms form (44) o, l bound
l0 , t0 dt allowed vary. examine every axiom form simply
count number possible unit propagations.
watched literal idea isolation cannot help problem. If, example,
watch duration predicates (44), reduce half probability

231

fiDixon, Ginsberg & Parkes

need solve subsearch problem particular variable valued,
cases problem encountered, fierce ever.
existing approach solving subsearch problems taken existing systems
use quantified clauses (44), set ground instances clauses.
Computing Sus (C, P ) ground C involves simply checking axiom individually; indeed,
axiom replaced set ground instances, approach seems
possible.
Set context quantified axiom, however, seems inappropriate. Computing Sus (C, P ) quantified C reducing C set ground clauses
examining equivalent solving original NP-complete problem generate
test one thing state confidence NP-complete problems, generate test general effective way solve them.
Returning example at(O, L, ) true, looking variable bindings l0 , dt t0 that, amongst duration(L, l0 , dt), between(T, t0 , + dt)
at(O, l0 , t0 ), precisely two literals false third unvalued. Proposition 5.2 suggests subsearch exponentially hard (with respect number
quantifiers) worst case, likely like practice?
practice, things going much better. Suppose possible destination l0 , know duration(L, l0 , dt) false dt except specific value D.
immediately ignore bindings dt except dt = D, reducing size
subsearch space factor |t|. depended previous choices search (aircraft
loads, etc.), would impossible perform analysis advance thereby remove
unnecessary bindings ground theory.
Pushing example somewhat further, suppose small +
time point immediately . words, between(T, t0 , + D) always
false, between(T, t0 , + D) always true unit propagation
possible value t0 all. backtrack away unfortunate choice
destination l0 (sub)search variable bindings unit propagation possible.
backtracking supported generate-and-test subsearch philosophy used
existing implementations.
sort computational savings likely possible general. naturally
occurring theories, variables involved likely either unvalued (because
yet managed determine truth values) false (by virtue closedworld assumption, Reiter, 1978, nothing else). Domain constraints typically
form a1 ak l, premises ai variables conclusion l literal
unknown sign. Unit propagation (or likely instances subsearch problem)
thus involve finding situation one ai unvalued, rest
true. use efficient data structures identify instances relational expressions
true, unreasonable expect instances subsearch problem
soluble time polynomial length clauses involved, opposed
exponential length.

232

fiBoolean Satisfiability Survey

5.2 Inference Learning
Section 3, working modified representation allows certain inference techniques
applicable Boolean case.
example, suppose resolving
a(A, B) b(B, C) c(C)

c(C) d(C, D)
conclude
a(A, B) b(B, C) d(C, D)

(45)

capital letters indicate ground elements domain resolvents
actually ground instances
a(x, y) b(y, C) c(C)

(46)

c(z) d(z, w)

(47)


obviously possible resolve (46) (47) directly obtain
a(x, y) b(y, C) d(C, w)

(48)

general (45). procedure learns new nogoods uses
prune resulting search, impact learning general (48) substantial
easily outweigh cost unification step required conclude c(C)
c(z) resolve z = C. also discussed elsewhere (Parkes, 1999).
two new difficulties arise implement ideas. first
consequence fact resolution ambiguously defined two quantified
clauses. Consider resolving
a(A, x) a(y, B)
(49)

a(A, B) b(A, B)

(50)

unify first term (50) first term (49), obtain a(y, B) b(A, B)
resolvent; unify second term (49), obtain a(A, x) b(A, B).
practice, however, need problem:
Proposition 5.4 Let c1 c2 two lifted clauses, g1 g2 ground instances
resolve produce g. unique natural resolvent c1 c2 g
ground instance.
Proof. one pair resolving literals g1 g2 result
resolution vacuous, assume single literal l g1 l
g2 . l ith literal g1 l jth literal g2 , follows resolve
original c1 c2 unifying ith literal c1 jth literal c2 . clear
resolution generalization g.
233

fiDixon, Ginsberg & Parkes

suggests reasons associated literal values
lifted nogoods retained clauses, ground instances thereof initially used prune search space subsequently used break ambiguities
learning.
second difficulty far substantial. Suppose axiom
a(x, y) a(y, z) a(x, z)
or, familiar form, usual transitivity axiom
a(x, y) a(y, z) a(x, z)
might used reasoning logistics problem, example, gave conditions
two cities connected roads.
suppose trying prove a(A, B) B far apart
given skeleton relation already know. possible use resolution
derive
a(A, x) a(x, B) a(A, B)
search proof involving single intermediate location,
a(A, x) a(x, y) a(y, B) a(A, B)
search proof involving two locations, on, eventually deriving
wonderfully concise
a(A, x1 ) a(xn , B) a(A, B)
(51)
suitably large n.
problem size domain, (51) dn ground instances
danger overwhelming unit propagation algorithm even presence
reasonably sophisticated subsearch techniques. technique needs adopted
ensure difficulty sidestepped practice. One way learn
fully general (51), partially bound instance fewer ground instances.
Procedure 5.5 construct learn(c, g), nogood learned clause c
produced response backtrack, g ground reason associated c:
1
2
3
4

c ground instance i-irrelevant
v variable c
bind v value g
return c

may still learn nogood exponential number ground instances,
least reason believe instances useful pruning
subsequent search. Note subsearch component Procedure 5.5, since
need find ground instances c irrelevant. cost incurred
clause learned, however, every unit propagation use.
234

fiBoolean Satisfiability Survey

might seem natural learn general (51), modify subsearch algorithm used unit propagation subset candidate clauses considered.
above, natural approach would likely restrict subsearch clauses
particular irrelevance better. Unfortunately, wont help, since irrelevant clauses
cannot unit. Restricting subsearch relevant clauses useful practice
requiring search algorithm expand successful nodes.
moving on, let us note similar phenomenon occurs pseudo-Boolean
case. Suppose partial assignment {b, c, d, e} constraints
a+d+e 1

(52)

a+b+c 2

(53)

Unit propagation causes variable simultaneously true (by virtue (52))
false (because (53)). Resolving reasons together Proposition 3.12 gives
us
(54)
b+c+d+e2
conflict set easily seen {b, d, e}, indeed prohibited
derived constraint (54). (54) eliminates additional bad assignments well,
{c, d, e}. lifted case, learned something portion
search space yet examined.
5.3 Summary

SAT

cardinality
PB
symmetry
QPROP

rep. eff.
1
exp
exp
1
exp

p-simulation
hierarchy
EEE
P?E
P?E
EEE
???

inference
resolution
unique
unique
P
P using reasons

unit
propagation
watched literals
watched literals
watched literals
sat
exp improvement

learning
relevance
relevance
+ strengthening
sat
+ first-order

usual, points made.
important difference practice exponential savings representation provided qprop savings provided pseudo-Boolean cardinality encodings. exponential savings previous cases mathematical
possibilities uncertain use practice, savings provided qprop
expected achieved axiomatization constructed grounding
relative handful universally quantified physical laws.
clear whether qprop leads polynomially sized solutions pigeonhole clique coloring problems. appears first blush should, since
quantification pigeons holes qprop analog identification
corresponding symmetry previous section. know detailed proof
literature, however, attempts construct one unsuccessful.
Similar remarks apply parity problems.

235

fiDixon, Ginsberg & Parkes

Inference qprop requires introduction (linear complexity) unification step,
uniquely defined reasons maintained choices made
search.
exponential savings claimed unit propagation obviously average case
result, opposed worst case one. consequence fact
possible use subsearch part unit propagation, opposed generate
test mechanism used ground methods.
addition usual idea relevance-based learning, quantified methods
extend power individual nogoods resolving quantified clauses instead
ground instances.
Finally, remark representation similar qprop also used
Answer Set Programming (asp) (Marek & Truszczynski, 1999; Niemela, 1999)
name propositional schemata (East & Truszczynski, 2001, 2002). approach used
asp resembles existing satisfiability work, however, clauses always grounded out.
potential advantages intelligent subsearch thus exploited, although expect
many motivations results given would also apply asp. fact, asp
many features common sat:
commonly used semantics, (non-disjunctive) stable model logic
programming (Gelfond & Lifschitz, 1988), representational power precisely
NP (or N P N P disjunctive programming).
Cardinality constraints allowed (East & Truszczynski, 2002; Simons, 2000).
Solution methods (Leone, Pfeifer, & et al., 2002; Niemela, 1999; Simons, 2000) use
dpll form propagation.
significant difference conventional satisfiability work asp
stable model semantics relevant logic classical logic
(Pearce, 1997). logic there, law excluded middle
hold, weaker p p. sufficient dpll applied,
imply classical resolution longer valid. result, seems proof
theory resulting system, learning within framework yet understood.
Backjumping used, mechanism seem learn new rules failed
subtrees search. analogous way, cardinality constraints used cutting
plane proof systems not. Despite many parallels sat asp, including
approach survey seems somewhat premature.

6. Conclusion
Satisfiability algorithms often developed framework provided
either random instances or, worse still, instances designed solely show
technique proposed computational merit. algorithms
thus tended ignore problem features dominate computational requirements
applied real problems.
236

fiBoolean Satisfiability Survey

realistic problems, possible improve speed algorithms
inner loops (via qprop subsearch) reduce number times inner
loops need executed (via learning move p-simulation hierarchy).
classes improvements arise problems question structure.
structure learned nogoods, used re-represent problem using pseudoBoolean quantified expressions.
true table previous subsection viewed survey recent work satisfiability, also true table viewed rational
reconstruction goals researchers investigated various representational
extensions. mind, table accurately viewed report extent
linguistic semantic modifications successfully capture problem structure.
Every column table structure. Improved representational efficiency
possible problem structure Boolean axiomatization typically
obscures. structure allows progress made terms proof complexity.
structure must preserved basic inference mechanism system question
remain useful, qprops ability speed inner loop unit propagation
direct consequence structure present subsearch problem. Finally, learning
thought search reasonably concise descriptions large sections
search space contain solutions words, learning discovery
structure search space itself.
setting next two papers series set. much
progress satisfiability techniques thought structure exploitation,
surely natural attempt understand exploit structure directly.
see, techniques discussed work exploiting structure,
exploit different instances single structure. zap work attempt
understand, generalize streamline previous results setting uniform
setting.

Acknowledgments
would like thank members cirl, technical staff Time Systems,
Eugene Luks David Hofer CIS department University Oregon
assistance ideas series papers. would also like thank
anonymous reviewers comments suggestions, found extremely
valuable.
work sponsored part grants Air Force Office Scientific Research
(afosr) number F49620-92-J-0384, Air Force Research Laboratory (afrl) number
F30602-97-0294, Small Business Technology Transfer Research, Advanced Technology Institute (sttr-ati) number 20000766, Office Naval Research (onr) number N00014-00-C0233, Defense Advanced Research Projects Agency (darpa) Air Force Research Laboratory, Rome, NY, agreements numbered F30602-95-1-0023, F30602-971-0294, F30602-98-2-0181, F30602-00-2-0534, F33615-02-C-4032. views expressed
authors.

237

fiDixon, Ginsberg & Parkes

References
Aloul, F., Ramani, A., Markov, I., & Sakallah, K. (2002). PBS: backtrack search pseudoBoolean solver. Symposium Theory Applications Satisfiability Testing.
Babai, L. (1995). Automorphism groups, isomorphism, reconstruction. Lovasz, L., Graham, R., & Grotschel, M. (Eds.), Handbook Combinatorics, chap. 27, pp. 1447
1540. North-Holland-Elsevier.
Baker, A. B. (1994). hazards fancy backtracking. Proceedings Twelfth
National Conference Artificial Intelligence.
Barth, P. (1995). Davis-Putnam based enumeration algorithm linear pseudoboolean optimization. Tech. rep. MPI-I-95-2-003, Max Planck Institut fur Informatik,
Saarbrucken, Germany.
Barth, P. (1996). Logic-Based 0-1 Constraint Programming, Vol. 5 Operations Research/Computer Science Interfaces Series. Kluwer.
Baumgartner, P. (2000). FDPLL First-Order Davis-Putnam-Logeman-Loveland Procedure. McAllester, D. (Ed.), CADE-17 17th International Conference
Automated Deduction, Vol. 1831, pp. 200219. Springer.
Bayardo, R. J., & Miranker, D. P. (1996). complexity analysis space-bounded learning
algorithms constraint satisfaction problem. Proceedings Thirteenth
National Conference Artificial Intelligence, pp. 298304.
Bayardo, R. J., & Schrag, R. C. (1997). Using CSP look-back techniques solve real-world
SAT instances. Proceedings Fourteenth National Conference Artificial
Intelligence, pp. 203208.
Beame, P., & Pitassi, T. (2001). Propositional proof complexity: Past, present future.
Paun, G., Rozenberg, G., & Salomaa, A. (Eds.), Current Trends Theoretical
Computer Science, Entering 21th Century, pp. 4270. World Scientific.
Benhamou, B., Sais, L., & Siegel, P. (1994). Two proof procedures cardinality based
language propositional calculus. Proceedings STACS94, volume 775 de Lecture
Notes Computer Science.
Biere, A., Clarke, E., Raimi, R., & Zhu, Y. (1999). Verifying safety properties PowerPC microprocessor using symbolic model checking without BDDs. Lecture Notes
Computer Science, 1633.
Bonet, M. L., Pitassi, T., & Raz, R. (1997). Lower bounds cutting planes proofs
small coefficients. Journal Symbolic Logic, 62 (3), 708728.
Brown, C. A., Finkelstein, L., & Paul Walton Purdom, J. (1988). Backtrack searching
presence symmetry. Mora, T. (Ed.), Applied Algebra, Algebraic Algorithms
Error-Correcting Codes, 6th Intl. Conf., pp. 99110. Springer-Verlag.
Bryant, R. E. (1986). Graph-based algorithms Boolean function manipulation. IEEE
Transactions Computers, C-35 (8), 677691.
Bryant, R. E. (1992). Symbolic Boolean manipulation ordered binary-decision diagrams. ACM Computing Surveys, 24 (3), 293318.
238

fiBoolean Satisfiability Survey

Buchberger, B. (1965). Ein Algorithmus zum Auffinden der Basiselemente des Restklassenringes nach einum nulldimensionalen Polynomideal. Ph.D. thesis, University Innsbruck, Innsbruck.
Buchberger, B. (1985). Grobner bases: algorithmic method polynomial ideal theory.
Bose, N. (Ed.), Multidimensional Systems Theory. D. Reidel, Dordrecht, Holland.
Cadoli, M., Schaerf, M., Giovanardi, A., & Giovanardi, M. (2002). algorithm evaluate
quantified boolean formulae experimental evaluation. Journal Automated
Reasoning, 28 (2), 101142.
Chai, D., & Kuehlmann, A. (2003). fast pseudo-Boolean constraint solver. Proceedings
40th Design Automation Conference, pp. 830835.
Chandru, V., & Hooker, J. N. (1999). Optimization Mehtods Logical Inference. WileyInterscience.
Clegg, M., Edmonds, J., & Impagliazzo, R. (1996). Using Groebner basis algorithm
find proofs unsatisfiability. Proceedings Twenty-Eighth Annual ACM
Symp. Theory Computing, pp. 174183.
Coarfa, C., Demopoulos, D. D., San Miguel Aguirre, A., Subramanian, D., & Vardi, M.
(2000). Random 3-SAT: plot thickens. Proceedings International Conference Constraint Programming.
Cook, S. A. (1971). complexity theorem-proving procedures. Proceedings
3rd Annual ACM Symposium Theory Computing, pp. 151158.
Cook, W., Coullard, C., & Turan, G. (1987). complexity cutting-plane proofs.
Discrete Applied Mathematics, 18, 2538.
Copty, F., Fix, L., Giunchiglia, E., Kamhi, G., Tacchella, A., & Vardi, M. (2001). Benefits
bounded model checking industrial setting. 13th Conference Computer
Aided Verification, CAV01, Paris, France.
Crawford, J. M. (1992). theoretical analysis reasoning symmetry first-order logic
(extended abstract). AAAI Workshop Tractable Reasoning.
Crawford, J. M., & Auton, L. D. (1996). Experimental results crossover point
random 3SAT. Artificial Intelligence, 81, 3157.
Crawford, J. M., Ginsberg, M. L., Luks, E., & Roy, A. (1996). Symmetry breaking predicates
search problems. Proceedings Fifth International Conference Principles
Knowledge Representation Reasoning, Boston, MA.
Davis, M., & Putnam, H. (1960). computing procedure quantification theory. J.
Assoc. Comput. Mach., 7, 201215.
Davis, M., Logemann, G., & Loveland, D. (1962). machine program theorem-proving.
Communications ACM, 5 (7), 394397.
Dechter, R. (1990). Enhancement schemes constraint processing: Backjumping, learning,
cutset decomposition. Artificial Intelligence, 41, 273312.
Dixon, H. E., & Ginsberg, M. L. (2000). Combining satisfiability techniques AI
OR. Knowledge Engrg. Rev., 15, 3145.
239

fiDixon, Ginsberg & Parkes

Dixon, H. E., & Ginsberg, M. L. (2002). Inference methods pseudo-Boolean satisfiability solver. Proceedings Eighteenth National Conference Artificial
Intelligence.
Dixon, H. E., Ginsberg, M. L., Hofer, D., Luks, E. M., & Parkes, A. J. (2003a). Generalizing
Boolean satisfiability III: Implementation. Tech. rep., Computational Intelligence
Research Laboratory, Eugene, Oregon.
Dixon, H. E., Ginsberg, M. L., Luks, E. M., & Parkes, A. J. (2003b). Generalizing Boolean
satisfiability II: Theory. Tech. rep., Computational Intelligence Research Laboratory,
Eugene, Oregon.
Dubois, O., Andre, P., Boufkhad, Y., & Carlier, J. (1993). SAT versus UNSAT. Second
DIMACS Challenge: Cliques, Colorings Satisfiability, Rutgers University, NJ.
Dubois, O., & Dequen, G. (2001). backbone-search heuristic efficient solving hard
3-SAT formulae. Proceedings Seventeenth International Joint Conference
Artificial Intelligence, pp. 248253.
East, D., & Truszczynski, M. (2001). Propositional satisfiability answer-set programming.
Lecture Notes Computer Science, 2174.
East, D., & Truszczynski, M. (2002). Propositional satisfiability declarative programming. Extended version papers appeared Proceedings AAAI-2000
Proceedings KI-2001. http://xxx.lanl.gov/abs/cs.LO/0211033.
Freeman, J. W. (1995). Improvements propositional satisfiability search algorithms. Ph.D.
thesis, University Pennsylvania, PA.
Frost, D., & Dechter, R. (1994). Dead-end driven learning. Proceedings Twelfth
National Conference Artificial Intelligence, pp. 294300.
Galperin, H., & Wigderson, A. (1983). Succinct representation graphs. Information
Control, 56, 183198.
Garey, M., & Johnson, D. (1979). Computers Intractability. W.H. Freeman Co.,
New York.
Gaschnig, J. (1979). Performance measurement analysis certain search algorithms.
Tech. rep. CMU-CS-79-124, Carnegie-Mellon University.
Gelfond, M., & Lifschitz, V. (1988). stable semantics logic programs. Proceedings
5th International Conference Logic Programming, pp. 10701080. MIT Press.
Ginsberg, M. L. (1993). Dynamic backtracking. Journal Artificial Intelligence Research,
1, 2546.
Ginsberg, M. L., & Geddis, D. F. (1991). need domain-dependent control
information?. Proceedings Ninth National Conference Artificial Intelligence.
Ginsberg, M. L., & Parkes, A. J. (2000). Search, subsearch QPROP. Proceedings
Seventh International Conference Principles Knowledge Representation
Reasoning, Breckenridge, Colorado.

240

fiBoolean Satisfiability Survey

Goldberg, E., & Novikov, Y. (2002). Berkmin: fast robust SAT solver. Design
Automation Test Europe (DATE), pp. 142149.
Guignard, M., & Spielberg, K. (1981). Logical reduction methods zero-one programming.
Operations Research, 29.
Haken, A. (1985). intractability resolution. Theoretical Computer Science, 39, 297
308.
Haken, A. (1995). Counting bottlenecks show monotone P 6= N P . Proceedings 36th
Annual IEEE Symp. Foundations Computer Science (FOCS-95), pp. 3640,
Milwaukee, MN. IEEE.
Hooker, J. N. (1988). Generalized resolution cutting planes. Annals Operations
Research, 12, 217239.
Hooker, J. N., & Vinay, V. (1995). Branching rules satisfiability. J. Automated Reasoning,
15, 359383.
Jeroslow, R., & Wang, J. (1990). Solving propositional satisfiability problem. Annals
Mathematics Artificial Intelligence, 1, 167187.
Joslin, D., & Roy, A. (1997). Exploiting symmetry lifted CSPs. Proceedings
Fourteenth National Conference Artificial Intelligence, pp. 197202.
Kautz, H., & Selman, B. (1998). BLACKBOX: new approach application
theorem proving problem solving. Artificial Intelligence Planning Systems: Proceedings Fourth International Conference. AAAI Press.
Kautz, H. A., & Selman, B. (1992). Planning satisfiability. Proceedings Tenth
European Conference Artificial Intelligence (ECAI92), pp. 359363.
Kirkpatrick, S., & Selman, B. (1994). Critical behavior satisfiability random
Boolean expressions. Science, 264, 12971301.
Krajicek, J. (1997). Interpolation theorems, lower bounds proof systems, independence results bounded arithmetic. J. Symb. Logic, 62 (2), 457486.
Krishnamurthy, B. (1985). Short proofs tricky formulas. Acta Informatica, 22 (3), 253
275.
Leone, N., Pfeifer, G., & et al. (2002). DLV system knowledge representation
reasoning. Tech. rep. 1843-02-14, Technical University Vienna.
Li, C. M. (2000). Integrating equivalency reasoning Davis-Putnam procedure.
Proceedings Seventeenth National Conference Artificial Intelligence, pp. 291
296.
Li, C. M., & Anbulagan (1997). Heuristics based unit propagation satisfiability
problems. Proceedings Fifteenth International Joint Conference Artificial
Intelligence, pp. 366371.
Luks, E., & Roy, A. (2002). Symmetry breaking constraint satisfaction. Intl. Conf.
Artificial Intelligence Mathematics, Ft. Lauderdale, Florida.
Marek, V. W., & Truszczynski, M. (1999). Stable models alternative logic programming paradigm..
241

fiDixon, Ginsberg & Parkes

McCarthy, J. (1977). Epistemological problems artificial intelligence. Proceedings
Fifth International Joint Conference Artificial Intelligence, pp. 10381044,
Cambridge, MA.
McCune, W., & Wos, L. (1997). Otter - CADE-13 competition incarnations. Journal
Automated Reasoning, 18 (2), 211220.
Mitchell, D. G. (1998). Hard problems CSP algorithms. Proceedings Fifteenth
National Conference Artificial Intelligence, pp. 398405.
Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
efficient SAT solver. 39th Design Automation Conference.
Nemhauser, G., & Wolsey, L. (1988). Integer Combinatorial Optimization. Wiley, New
York.
Niemela, I. (1999). Logic programs stable model semantics constraint programming
paradigm. Annals Mathematics Artificial Intelligence, 25, 241273.
Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley.
Parkes, A. J. (1999). Lifted Search Engines Satisfiability. Ph.D. thesis, University
Oregon. Available http://www.cirl.uoregon.edu/parkes.
Pearce, D. (1997). new logical characterization stable models answer sets.
Dix, J., Pereira, L., & Przymusinski, T. (Eds.), Non-monotonic Extensions Logic
Programming, Vol. 1216 Lecture Notes Artificial Intelligence, pp. 5770.
Pitassi,
T.
(2002).
Propositional
proof
complexity
lecture
notes.
www.cs.toronto.edu/toni/Courses/Proofcomplexity/Lectures/Lecture1/lecture1.ps
(other lectures titled similarly).
Prestwich, S. (2002). Randomised backtracking linear pseudo-Boolean constraint problems. Proceedings 4th International Workshop Integration AI
Techniques Constraint Programming Combinatorial Optimisation Problems
(CPAIOR-02), pp. 720.
Pretolani, D. (1993). Satisfiability hypergraphs. Ph.D. thesis, Universita di Pisa.
Pudlak, P. (1997). Lower bounds resolution cutting planes proofs monotone
computations. J. Symbolic Logic, 62 (3), 981998.
Puget, J.-F. (1993). satisfiability symmetrical constrained satisfaction problems.
J. Komorowski Z.W. Ras, editors, Proceedings ISMIS93, pages 350361.
Springer-Verlag, 1993. Lecture Notes Artificial Intelligence 689.
Reiter, R. (1978). closed world data bases. Gallaire, H., & Minker, J. (Eds.), Logic
Data Bases, pp. 119140. Plenum, New York.
Savelsbergh, M. W. P. (1994). Preprocessing probing mixed integer programming
problems. ORSA Journal Computing, 6, 445454.
Schaefer, T. J. (1978). complexity satisfiability problems. Proceedings
Tenth Annual ACM Symposium Theory Computing, pp. 216226.

242

fiBoolean Satisfiability Survey

Selman, B., Kautz, H. A., & Cohen, B. (1993). Local search strategies satisfiability testing. Proceedings 1993 DIMACS Workshop Maximum Clique, Graph Coloring,
Satisfiability.
Simons, P. (2000). Extending implementing stable model semantics.. Research
Report 58, Helsinki University Technology, Helsinki, Finland.
Stallman, R. M., & Sussman, G. J. (1977). Forward reasoning dependency-directed
backtracking system computer-aided circuit analysis. Artificial Intelligence,
9, 135196.
Szeider, S. (2003). complexity resolution generalized symmetry rules. Alt,
H., & Habib, M. (Eds.), Proceedings STACS03, volume 2607 Springer Lecture
Notes Computer Science, pp. 475486.
Tseitin, G. (1970). complexity derivation propositional calculus. Slisenko,
A. (Ed.), Studies Constructive Mathematics Mathematical Logic, Part 2, pp.
466483. Consultants Bureau.
Urquhart, A. (1995). complexity propositional proofs. Bull. Symbolic Logic, 1 (4),
425467.
Velev, M. N., & Bryant, R. E. (2001). Effective use boolean satisfiability procedures
formal verification superscalar VLIW. Proceedings 38th Conference
Design Automation Conference 2001, pp. 226231, New York, NY, USA. ACM
Press.
Walser, J. P. (1997). Solving linear pseudo-Boolean constraint problems local search.
Proceedings Fourteenth National Conference Artificial Intelligence, pp.
269274.
Zhang, H., & Stickel, M. E. (2000). Implementing Davis-Putnam method. Journal
Automated Reasoning, 24 (1/2), 277296.

243

fiJournal Artificial Intelligence Research 21 (2004) 429-470

Submitted 07/03; published 04/04

Grounded Semantic Composition Visual Scenes
Peter Gorniak
Deb Roy

pgorniak@media.mit.edu
dkroy@media.mit.edu

MIT Media Laboratory
20 Ames St.,Cambridge, 02139 USA

Abstract
present visually-grounded language understanding model based study
people verbally describe objects scenes. emphasis model combination
individual word meanings produce meanings complex referring expressions.
model implemented, able understand broad range spatial referring
expressions. describe implementation word level visually-grounded semantics
embedding compositional parsing framework. implemented system selects
correct referents response natural language expressions large percentage
test cases. analysis systems successes failures reveal visual context
influences semantics utterances propose future extensions model take
context account.

1. Introduction
introduce visually-grounded language understanding model based study
people describe objects visual scenes kind shown Figure 1. designed study
elicit descriptions would naturally occur joint reference setting easy
produce understand human listener. typical referring expression Figure 1
might be, far back purple cone thats behind row green ones. Speakers construct
expressions guide listeners attention intended objects. referring expressions
succeed communication speakers listeners find similar features visual
scene salient, share understanding language grounded terms
features. work step towards longer term goals develop conversational
robot (Hsiao, Mavridis, & Roy, 2003) fluidly connect language perception
action.
study use descriptive spatial language task similar one robots
perform, collected several hundred referring expressions based scenes similar Figure
1. analysed descriptions cataloguing visual features referred
within scene, range linguistic devices (words grammatical patterns)
used refer features. combination visual feature corresponding
linguistic device referred descriptive strategy. example sentence contains
several descriptive strategies make use colour, spatial relations, spatial grouping.
descriptive strategies used composition speaker make reference
unique object.
propose set computational mechanisms correspond commonly
used descriptive strategies study. resulting model implemented
set visual feature extraction algorithms, lexicon grounded terms visual
c
2004
AI Access Foundation. rights reserved.

fiGorniak & Roy

Figure 1: sample scene used elicit visually-grounded referring expressions (if figure
reproduced black white, light cones green colour,
dark cones purple)

features, robust parser capture syntax spoken utterances, compositional
engine driven parser combines visual groundings lexical units. evaluate
system, collected set spoken utterances three speakers. verbatim transcriptions speech, complete speech repairs various ungrammaticalities
common spoken language, fed model. model able correctly
understand visual referents 59% expressions (chance performance
assuming
P30
random object selected sessions 30 trials 1/30 i=1 1/i = 13%).
system able resolve range linguistic phenomena made use relatively
complex compositions spatial semantics. provide detailed analysis sources
failure evaluation, based propose number improvements
required achieve human level performance. designing framework build
prior work human reference resolution integration semantics parsing.
main contribution work lies using visual features based study human visual
linguistic reference grounded semantic core natural language understanding
system.
previous work visually-grounded language centred machine learning
approaches (Roy, Gorniak, Mukherjee, & Juster, 2002; Roy & Pentland, 2002; Roy, 2002),
chose apply machine learning problem compositional grounded semantics
investigation. Rather, endeavoured provide framework process
types descriptive strategies compositionality found study human
participants. future work, investigate machine learning methods used
acquire parts framework experience, leading robust accurate
performance.
430

fiGrounded Semantic Composition Visual Scenes

1.1 Grounded Semantic Composition
use term grounded semantic composition highlight semantics
individual words word composition process visually-grounded.
model, lexical entrys meaning grounded association visual model.
example, green associated probability distribution function defined
colour space. propose processes combine visual models words, governed
rules syntax.
Given goal understanding modelling grounded semantic composition, several
questions arise:
visual features people use describe objects scenes
Figure 1?
features connect language?
features descriptions combine produce whole utterances
meanings?
word meanings independent visual scene describe?
meaning whole utterance based meanings parts?
composition meanings purely incremental process?
assumed easy answers questions place start modelling
effort. current implementation assumes meaning whole utterance
fully derived meanings parts, performs composition incrementally (without
backtracking), let visual context influence interpretation word
meanings. Despite assumptions, system handles relatively sophisticated semantic
composition. evaluated test data, system correctly understood chose
appropriate referents expressions as, purple one behind two green ones
left green cone front back purple one.
analysing systems performance human participants utterances, found
that:
Word meanings strongly dependent visual scene describe.
instance, found four distinct visual interpretations word middle
linguistically indistinguishable, instead depend different visual contexts
understood.
meaning utterance may sometimes depend meanings
parts. meaning may also depend visual context utterance
occurs, modify parts compose. example, objects
referred frontmost left would referred neither left frontmost
isolation, result multiplicative joined estimation two.
Composition meanings task purely incremental process.
cases found necessary backtrack reinterpret parts utterance
431

fiGorniak & Roy

good referents found later processing stage, ambiguities cannot
resolved current interpretation. Due strictly feed forward model
language understanding, current implementation fails cases.
results similar reported prior studies (Brown-Schmidt, Campana,
& Tanenhaus, 2002; Griffin & Bock, 2000; Pechmann, 1989). Although model
currently address issues context-dependent interpretation backtracking,
believe framework approach grounded compositional semantics provide
useful steps towards understanding spatial language. system performs well understanding spatial descriptions, applied various tasks natural language
speech based human-machine interfaces.
paper begins highlighting several strands related previous work. section 2,
introduce visual description task serves basis study model.
Section 3 presents framework grounded compositional semantics. Section 4 describes
resulting computational model. example whole system work given
Section 5. discuss results applying system human data spatial
description task section 6, together analysis systems successes failures.
leads suggestions future work Section 7, followed summary section 8.
1.2 Related Work
Winograds SHRDLU well known system could understand generate natural
language referring objects actions simple blocks world (Winograd, 1970). Like
system performs semantic interpretation parsing attaching short procedures
lexical units (see also Miller & Johnson-Laird, 1976). However, SHRDLU access
clean symbolic representation scene, whereas system discussed works
synthetic vision system reasons geometric visual measures. Furthermore,
intend system robustly understand many ways human participants
verbally describe objects complex visual scenes other, whereas SHRDLU
restricted sentences could parse completely translate correctly formalism.
SHRDLU based formal approach semantics problem meaning
addressed logical set theoretic formalisms. Partee provides overview
approach problems context based meanings meaning compositionality
perspective (Partee, 1995). work reflects many ideas work,
viewing adjectives functions. Pustejovskys theory Generative Lexicon
(GL) particular takes seriously noun phrase semantics semantic compositionality
(Pustejovsky, 1995). approach lexical semantic composition originally inspired
Pustejovskys qualia structures. However, formal approaches operate symbolic
domain leave details non-linguistic influences meaning unspecified, whereas
take computational modelling influences primary concern.
Research concerning human production referring expressions lead studies related one described here, without computational counterparts. Brown-Schmidt
e.a., example, engage participants free-form dialogue (as opposed one-sided
descriptions task) producing referential descriptions solve spatial arrangement
problem (Brown-Schmidt et al., 2002). Due use complicated scenes
complete dialogues, find participants often engage agreement behaviours
432

fiGrounded Semantic Composition Visual Scenes

use discourse visual context disambiguate underspecified referring expressions
often study. Similar tasks used studies dialogue
referring expressions (Pechmann, 1989; Griffin & Bock, 2000). intentionally eliminated dialogue used simpler visual scene task elicit spatial descriptions (as
opposed description object attributes), able computationally model
strategies participants employ. Formal theories vagueness support findings
expressions produced participants (Kyburg & Morreau, 2000; Barker, 2002).
Word meanings approached several researchers problem associating
visual representations, often complex internal structure, word forms. Models
suggested visual representations underlying colour (Lammens, 1994) spatial
relations (Regier, 1996; Regier & Carlson, 2001). Models verbs include grounding
semantics perception actions (Siskind, 2001), grounding terms motor
control programs (Bailey, 1997; Narayanan, 1997). Object shape clearly important
connecting language world, remains challenging problem computational
models language grounding. previous work, used histograms local geometric
features found sufficient grounding names basic objects (dogs, shoes, cars,
etc.) (Roy & Pentland, 2002). representation captures characteristics overall
outline form object invariant in-plane rotations changes scale. Landau
Jackendoff provide detailed analysis additional visual shape features play
role language (Landau & Jackendoff, 1993). example, suggest importance
extracting geometric axes objects order ground words end, end
stick. Shi Malik propose approach performing visual grouping images
(Shi & Malik, 2000). work draws findings Gestalt psychology provide
many insights visual grouping behaviour (Wertheimer, 1999; Desolneux, Moisan, &
Morel, 2003). Engbers e.a. give overview formalization grouping problem
general various approaches solution (Engbers & Smeulders, 2003). parallel
work presented paper, also studying visual grouping fold
results systen described (Dhande, 2003).
model incremental semantic interpretation parsing follows tradition
employing constraint satisfaction algorithms incorporate semantic information starting
SHRDLU continued systems (Haddock, 1989). prior systems use
declaratively stated set semantic facts disconnected perception. Closely
related work area Schulers (2003), integrates determination referents
parsing process augmenting grammar logical expressions, much like
augment grammar grounded composition rules (see Section 3.4). emphasis,
however, system actively ground word utterance meanings
sensory system. Even though system described senses synthetic scene,
makes continuous measurements parsing process integrating
active vision system (Hsiao et al., 2003). Schulers system requires human-specified
clean logical encoding world state, ignores noisy, complex difficultto-maintain process linking language sensed world. consider process,
call grounding process, one important aspects situated human-like
language understanding.
SAM (Brown, Buntschuh, & Wilpon, 1992) Ubiquitous Talker (Nagao & Rekimoto,
1995) language understanding systems map language objects visual scenes.
433

fiGorniak & Roy

Similar SHDRLU, underlying representation visual scenes symbolic loses
much subtle visual information work, work cited above, focus
on. SAM Ubiquitous Talker incorporate vision system, phrase parser
understanding system. systems translate visually perceived objects symbolic
knowledge base map utterances plans operate knowledge base.
contrast, primarily concerned understanding language referring objects
relations appear visually.
previously proposed methods visually-grounded language learning (Roy &
Pentland, 2002), understanding (Roy et al., 2002), generation (Roy, 2002). However,
treatment semantic composition efforts relatively primitive. phrase,
visual models word phrase individually evaluated multiplied.
method works phrases conjunctive modifiers, even cases,
discuss later, ordering modifiers sometimes needs taken account (i.e., leftmost
front always refer front leftmost does). simple approach
worked constrained domains addressed past, scale
present task. example, Describer system (Roy, 2002) encodes spatial locations
absolute terms within frame reference visual scene. result, Describer
makes mistakes humans would make. grounding word highest,
example, defined probability distribution centred specific height scene,
object closest height best example highest, accounting
fact may objects greater height (depending relative sizes
shapes objects). addition, Describers interpretation phrase like highest
green rectangle find object close center probability
distributions highest green, accounting fact human
listener highest green rectangle need high screen (but higher
green rectangles). word highest requires visual binding
includes notion rank ordering. move, however, requires rethinking
achieve semantic composition, addressed approach here.

2. Spatial Description Task
designed task requires people describe objects computer generated scenes
containing 30 objects random positions virtual surface. objects
identical shapes sizes, either green purple colour. object
50% chance green, otherwise purple. refer task Bishop
task, resulting language understanding model implemented system simply
Bishop.
2.1 Motivation Task Design
previous work, investigated speakers describe objects distinctive
attributes like colour, shape size constraint speaking task scenes
constant number objects (Roy, 2002). Speakers task rarely compelled
use spatial relations never use groups objects, cases objects
distinguished listing properties. designing Bishop task, goal
naturally lead speakers make reference spatial aspects scene. Therefore,
434

fiGrounded Semantic Composition Visual Scenes

drastically increased number objects scene decreased number
distinctive object attributes. also let number objects vary throughout trials
cover scenes cluttered objects scenes objects analysis.
variation task, ran experiments system chose objects
random speaker describer, rather allowing describer make choice.
found made task difficult highly unnatural speaker
often visually salient arrangements randomly chosen objects took part
in. result, listeners make many errors resolving reference variation
task (3.5% error speaker chose object versus 13% system
chose). limits accuracy pure linguistic reference appeared
reaching random selection version task. Speakers seemed much
harder time finding visually salient landmarks, leading long less natural descriptions,
example centre bunch green cones, four them, um, actually
four, but, ah, theres one thats centre pretty much pile
top, ahm, say this... seventh cone right
side (followed listener counting cones pointing screen). avoid collecting
unnatural data, decided use random selection version task.
Another possible variant task would let system choose objects
non-random manner based systems analysis objects would natural describe. However, approach would clearly bias data towards objects
matched preexisting models system.
Since interested people described objects spatially well visual
features found salient, decided let listener pick objects felt
concisely yet trivially describable. acknowledge task design eases
difficulty understanding task; speakers could find interesting object
easy describe ways, resorted simpler choices like leftmost
one. Yet, utterances elicited task relatively complex (see Appendix
complete listing) provided serious challenges automatic language
understanding perspective.
Scenes rendered 3D instead using equivalent 2D scene anticipation
transition understanding system camera driven vision system. use
3D rendering introduces occlusion, shadows, sources ambiguity must
eventually addressed transition real vision system. However, note
scene include interesting 3D spatial relations features,
claim description task thus system presented would generalize
directly true 3D setting. Furthermore, use 3D information
visual scene, system interprets spatial relations 2D. errs 2D side
ambiguity inherent word like leftmost reference one scenes (the
interpretation differ due perspective effects: leftmost object interpreting
scene 2D might leftmost interpreting 3D. believe
task types visually grounded descriptions produced challenging
computational system understand, hope show remainder paper.
Finally, note goal design task study collaboration,
dialogue agreement, goal experiments analyses (Carletta &
Mellish, 1996; Eugenio, Jordan, Thomason, & Moore, 2000). use speaker/listener dyad
435

fiGorniak & Roy

ensure descriptions produced understandable human listener,
purposefully allow listeners speak. feedback channel speakers
successful unsuccessful selection described object. introduce
minimal form dialogue, low error rate listeners leads us believe negative
reinforcement negligible speakers task viewed
exercise collaboration. cannot rule listeners adopted strategies used
partners turn speak. However, relative similarity strategies
pairs shows phenomenon make data unrepresentative,
even produces types shortenings vagueness would expect see
extended description task speaking machine.
2.2 Data Collection
Participants study ranged age 22 30 years, included native
non-native English speakers. Pairs participants seated backs
other, looking computer screen displayed identical scenes
Figure 1. pair, one participant served describer, listener.
describer wore microphone used record speech. describer used
mouse select object scene, verbally described selected object
listener. listener allowed communicate verbally otherwise all,
except object selections. listeners task select object
computer display based verbal description. selected objects matched,
disappeared scene describer would select describe another object.
match, describer would re-attempt description understood
listener. Using describer-listener dyad ensured speech data resembled natural
communicative dialogue. Participants told free select object
scene describe way thought would clear. also told
make task trivial by, example, always selecting leftmost object describing
leftmost. scene contained 30 objects beginning session,
session ended objects remained, point describer listener switched
roles completed second session (some participants fulfilled role multiple times).
found listeners study made extremely mistakes interpreting descriptions,
seemed generally find task easy perform.
Initially, collected 268 spoken object descriptions 6 participants. raw
audio segmented using speech segmentation algorithm based pause structure
(Yoshida, 2002). Along utterances, corresponding scene layout target
object identity recorded together times objects selected.
268 utterance corpus referred development data set. manually transcribed
spoken utterance verbatim, retaining speech errors (false starts various
ungrammaticalities). Rather working grammatically controlled language,
interest model language occurs conversational settings since longer term
goal transplant results work conversational robots language
spoken form. Off-topic speech events (laughter, questions task,
remarks, filled pauses) marked (they appear results
report).
436

fiGrounded Semantic Composition Visual Scenes

developed simple algorithm pair utterances selections based time
stamps. algorithm works backwards time point correct
object removed scene. collects on-topic utterances occurred
removal event previous removal event 4
seconds apart. fuses single utterance, sends scene description,
complete utterance identity removed object understanding system.
utterance fusing necessary participants often paused descriptions.
time, pauses beyond certain length usually indicated utterances
pause contained errors rephrase occurred. pairing algorithm
obviously heuristic nature, mark instances makes mistakes (wrongly
leaving utterances attributing utterances wrong selection event) analysis
data below. report numbers utterances data sets paper,
correspond many utterance-selection pairs pairing algorithm produces.
means due errors algorithm numbers utterances report
divisible 30, actual number objects selected session.
development corpus analysed catalogue range common referring strategies (see Section 2.3). analysis served basis developing visually-grounded
language understanding system designed replace human listener task described
above. implementation yielded acceptable results development corpus,
collected another 179 spoken descriptions three additional participants evaluate generalization coverage approach. used exactly equipment, instructions
collection protocol collecting development data collect test data.
average length utterances development test set 8
9 words. discussion analysis following sections focuses development
set. Section 6 discuss performance test set.
2.3 Descriptive Strategies Achieving Joint Reference
noted earlier, call combination visual feature measured current scene
(or, case anaphora, previous scene) together linguistic realization
descriptive strategy. section, catalogue common strategies describers used communicate listeners. analysis based strictly development
data set. discuss implemented system handles categories Section 4.
distinguish three subsets development data:
set containing utterance/selection pairs contain errors. error
due repair mistake human speakers part, segmentation mistake
speech segmenter, error utterance/selection pairing algorithm.
set contains utterance/selection pairs employ descriptive strategies
cover computational understanding system (we cover
Sections 2.3.1 2.3.5).
set utterance/selection pairs development data member
either subset described above. refer subset clean set.
437

fiGorniak & Roy

Note first two subsets mutually exclusive. following sections,
report two percentages descriptive strategy. first percentage
utterance/selection pairs employ specific descriptive strategy relative utterance/selection pairs development data set. second percentage utterance/selection pairs relative clean set utterance/selection pairs, described above.
examples given paper actual utterances scenes development
test sets.
2.3.1 Colour
Almost every utterance employs colour pick objects. designing task,
intentionally trivialized problem colour reference. Objects come two distinct
colours, green purple. Unsurprisingly, participants used terms green
purple refer colours. previous work addressed problems
learning visually-grounded models colour words (Roy & Pentland, 2002; Roy, 2002).
Here, focus semantic compositionality terms, chose simplify colour
naming problem. Figure 2 shows one instances colour used pick
referent. examples subsequent sections colour composed
descriptive strategies.
Syntactically, colours expressed adjectives (as mentioned: green purple) always directly precede nouns modify. is, nobody ever said
green left one data, rather adjectives would commonly occur order
left green one.
data, green purple also sometimes take roles nouns, least
left dangling noun phrase ellipse like leftmost purple. Although
form dangling modifier might seem unlikely, occur spoken utterances
task. objects Bishop cones, participants trouble understanding
ellipsis, occur 7% data.
Participants used colour identify one objects 96% data, 95%
clean data.

purple cone
Figure 2: Example utterance using colour

438

fiGrounded Semantic Composition Visual Scenes

2.3.2 Spatial Regions Extrema
second common descriptive strategy refer spatial extremes within groups
objects spatial regions scene. left example Figure 3 uses two spatial
terms pick referent: closest front, leverage spatial
extrema direct listeners attention. example, selection spatial extremum
appears operate relative green objects, i.e. speaker seems first attend
set green cones, choose amongst them. Alternatively, closest
front could pick several objects colour, colour specification could filter
spatial extrema determine final referent. case two interpretations yield
referent, cases corpus second alternative (spatial
selection followed colour selection) yields referents all.

green one thats closest us
front

purple one left side

Figure 3: Example utterances specifying objects referring spatial extrema
right example Figure 3 shows phrases explicitly indicating spatial extrema still sometimes intended interpreted referring extrema. listener
interpret left side referring left side scene, phrase would
ambiguous since four purple cones left side scene.
hand, phrase unambiguous interpreted picking extremum. Figure 4 shows
instance right hand side actually refers region board.
first example figure shows phrase right hand side combined
extremum term, lowest. Note referent right extremum. second
example Figure 4, referent bottommost green object, and, (arguably,
taking scene existing 3D), neither leftmost. Regions board
seem play role cases. Often local context region may play stronger
role global one, referent second example Figure 4 found
attending front left area scene, selecting left bottom example
amongst candidates area. Along lines, words like middle largely
used describe region board, position relative cones.
rather ubiquitous data, spatial extrema spatial regions often used
combination descriptive strategies like grouping, frequently
combined extrema region specifications. opposed combined
colour adjectives, multiple spatial specifications tend interpreted left right order,
is, selecting group objects matching first term, amongst choosing
objects match second term. examples Figure 4 could understood
439

fiGorniak & Roy

lowest purple right hand side

green cone left bottom

Figure 4: Example utterances specifying regions

purple one front left corner
Figure 5: Extrema sequence
simply ignoring order spatial specifications instead finding conjoined best fit, i.e.
best example bottommost leftmost. However, Figure 5 demonstrates
generally case. scene contains two objects best fits
unordered interpretation front left, yet human participant confidently picks
front object. Possible conclusions extrema need interpreted sequence,
participants demonstrating bias preferring front-back features left-right
ones. implementation, choose sequence spatial extrema order occur
input.
Participants used single spatial extrema identify one objects 72%
data, 78% clean data. used spatial region specifications 20%
data (also 20% clean data), combined multiple extrema regions 28% (30%
clean data).
2.3.3 Grouping
provide landmarks spatial relations specify sets objects select from, participants used language describe groups objects. Figure 6 shows two examples
grouping constructs, first using unnumbered group cones (the green cones),
second using count specify group (three). function group different
two examples: left scene participant specifies group landmark
serve spatial relation (see Section 2.3.4), whereas right scene participant
first specifies group containing target object, utters another description select
within group. Note grouping alone never yields individual reference, participants compose grouping constructs referential tactics (predominantly extrema
spatial relations) cases.
440

fiGrounded Semantic Composition Visual Scenes

purple cone middle left
green cones

theres three left side; one
furthest back

Figure 6: Example utterances using grouping

Participants used grouping identify objects 12% data 10% clean
data. selected objects within described groups 7.5% data (8% clean
data) specified groups number objects (two, three, ...) 8.5% data
(also 8.5% clean data).
2.3.4 Spatial Relations
already mentioned Section 2.3.3, participants sometimes used spatial relations
objects groups objects. Examples relations expressed prepositions
like behind well phrases like left front of. already
saw example spatial relation involving group objects Figure 6, Figure 7
shows two examples involve spatial relations individual objects.
first example one examples pure spatial relations two individual
objects referenced colour. second example typical one
spatial relation combined another strategy, extremum (as well two speech
errors describer).

green cone green cone

theres purple cone thats
way left hand side
another purple

Figure 7: Example utterances specifying spatial relations

Participants used spatial relations 6% data (7% clean data).
441

fiGorniak & Roy

2.3.5 Anaphora
number cases participants used anaphoric references previous object removed
description task. Figure 8 shows sequence two scenes corresponding
utterances second utterance refers back object selected first.

closest purple one far left
side

green one right behind one

Figure 8: Example sequence anaphoric utterance

Participants employed spatial relations 4% data (3% clean data).
2.3.6
addition phenomena listed preceding sections, participants used small
number description strategies. occurred
yet addressed computational model selection distance (lexicalised
close next to), selection neighbourhood (the green one surrounded purple
ones), selection symmetry (the one opposite one), selection something
akin local connectivity (the lone one). also additional types groupings,
example grouping linearity (the row green ones, three purple diagonal)
picking objects within group number (the second one left row
five purple ones) cover here. strategies occurs less often
data anaphora (it occurs 4% utterances, see previous section).
annotated 13% data containing descriptive strategies ones
covered preceding sections. However, devices often combined
phenomena covered here. marked 15% data containing errors. Errors
come form repairs speaker, faulty utterance segmentation speech
segmenter, misaligning utterances scenes system.
also instances participants composing semantic phenomena ways
handle. two instances combining spatial relations (the one
right) instances specifying groups spatial extrema
regions (the group purple ones left). count
evaluation, rather counted errors; reported success rate correspondingly lower.
442

fiGrounded Semantic Composition Visual Scenes

2.4 Summary
preceding sections catalogue strategies participants employed describing objects.
computational system understands utterances using strategies must fulfill
following requirements:
system must access visual scene able compute visual
features like used human speakers: natural groupings, inter-object distances,
orderings spatial relations
must robust language parsing mechanism discovers grammatical patterns associated descriptive strategies
Feeding parsing mechanism must visually grounded lexicon; entry
lexicon must carry information descriptive strategies takes part
in, descriptive strategies combine others
semantic interpretation composition machinery must embedded
parsing process
system must able interpret results parsing utterance make
best guess object whole utterance describes
go describe systems understanding framework, consisting visual
system, grounded lexical entries parser Section 3. Section 4 discuss
modules implemented understand human descriptive strategies.

3. Understanding Framework
section describe components Bishop understanding system detail,
emphasis fit together work visually grounded understanding
system. cover turn Bishops vision system, parser lexicon give short
overview implementation descriptive strategies fits framework.
3.1 Synthetic Vision
Instead relying information use render scenes Bishop, includes
3D object locations viewing angle, implemented simple synthetic vision algorithm. algorithm produces map attributing pixel rendered image one
objects background. addition, use full colour information
pixel drawn rendered scene. goal loosely simulate view camera
pointed scene real world objects, situation robots find in.
past successfully migrated models synthetic vision (Roy, 2002) computer
vision (Roy et al., 2002) plan similar route deploy Bishop. Obviously, many
hard problems object detection well lighting noise robustness need
solved synthetic case, hope transfer back robots camera
made easier working 2D image. chose work virtual world
project could freely change colour, number, size, shape arrangement
443

fiGorniak & Roy

objects elicit interesting verbal behaviours participants, without running
limitations object detection algorithms field view problems.
Given input image regions corresponding objects segmented,
features produced vision system are:
average RGB colour average red, green blue components pixels
attributed object
centre mass average x pixel positions object
distance euclidean distance pairs objects centres mass
groups groups objects scene determined finding sets objects
contain one object, object less threshold
distance another object group (distances measured centres
mass)
pairs groups, filtered produce groups two objects
triplets groups, filtered produce groups three objects
convex hull set pixels forming smallest convex region enclosing set objects
attentional vector sum (AVS) AVS spatial relation measure objects. extreme parameter settings measures one two angles, formed
centres, formed closest points two objects. use
parameter setting ( = 0.7) two extremes, produces intermediate angle depending objects shape. resulting direction measured relative set reference angles, system four Cartesian vectors
(0, 1), (0, 1), (1, 0), (1, 0) (Regier & Carlson, 2001).
3.2 Knowledge Representation
Objects represented integer IDs system. ID set IDs vision
system compute visual features described Section 3.1 based corresponding
set pixels image. distinguishing ID together visual features represents
systems total knowledge objects present scene. system
instantiate new objects vision system convex hull groups objects.
system also remembers ID object removed last, ask vision
system perform feature computation visual scene object
removed.
Groups objects integer IDs treated objects
(all visual features available them). IDs stored together list
constituent objects IDs, groups broken apart necessary.
Finally, visible lexicon file Appendix B, lexical item stored set
associated parameters. parameters specify grammatical type, compositional arity
reference behaviour (what word sense taken referring own:
single object, group objects objects.) Furthermore, lexical item associated
444

fiGrounded Semantic Composition Visual Scenes

semantic composer (see Sections 3.3 4) store sets parameters,
specifying Gaussian together applicable dimensions case
probabilistic composers.
3.3 Lexical Entries Concepts
Conceptually, treat lexical entries like classes object oriented programming language. instantiated, maintain internal state simple tag
identifying dimension along perform ordering, complex multidimensional probability distributions. entry also function interface specifies
performs semantic composition. Currently, interface definition consists number
arrangement arguments entry willing accept, whereas type mismatches
handled composition rather enforced interface. Finally,
entry contain semantic composer encapsulates actual function combine
entry constituents parse. composers described in-depth
Section 4. lexicon used Bishop contains many lexical entries attaching different
semantic composers word. example, left either spatial relation
extremum. grammatical structure detected parser (see next Section)
determines compositions attempted given utterance.
composition, structures representing objects constituent references
passed lexical entries. refer structures concepts. entry
accepts zero concepts, produces zero concepts result
composition operation. concept lists entities world possible referents
constituent associated with, together real numbers representing ranking
due last composition operation. composer also mark concept referring
previous visual scene, allow anaphoric reference (see Section 4.5). also contains
flags specifying whether referent group objects single object (cones
vs. cone), whether uniquely pick single object ambiguous
nature (the vs. a). flags used post-processing stage determine
possible ambiguities conflicts.
lexicon, based development corpus, contains 93 words: 33 ADJ (adjectives), 2 CADJ (colour adjectives: green, purple), 38 N (nouns), 2 REL (relative
pronouns: that, which), 1 VPRES (present tense verbs: is), 2 RELVPRES (relative pronoun/present tense verb combinations: thats, its), 1 ART (the), 3 SPEC
(adjective specifiers: right (as right above), just), 7 P (prepositions), 4 specific
prepositions (POF, PAT, two versions PIN). complete lexicon specification
reproduced Appendix B.
3.4 Parsing
previous work used Markov models parse generate utterances
(Roy, 2002), employ context free grammars. grammars naturally let us
specify local compositional constraints iterative structures. Specifically, allow us
naturally perform grounded semantic composition whenever grammar rule syntactically
complete, producing partial understanding fragments every node parse tree.
parse structure utterance thus dictates compositions attempted. use
445

fiGorniak & Roy

bottom-up chart parser guide interpretation phrases (Allen, 1995). parser
advantage employs dynamic programming strategy efficiently reuse
already computed subtrees parse. Furthermore, produces sub-components
parse thus produces useable result without need parse specific symbol.
using dynamic programming approach assuming meanings parts
assembled meanings wholes. strictly committed assumption
future consider backtracking strategies necessary. Also note due
fact framework often produces functions applied later stages
interpretation (see section 4) avoid possible overcommitting decisions (excluding
correct referent early stage understanding).
Bishop performs partial parse, parse required cover whole
utterance, simply takes longest referring parsed segments best guess.
Unknown words stop parse process. Rather, constituents would otherwise
end unknown word taken include unknown word, essence making
unknown words invisible parser understanding process. way recover
essentially grammatical chunks relations important understanding
restricted task. overview related partial parsing techniques, see work Abney
(1997).
grammar used partial chart parser shown Figure 1. Together
grammar rules table shows argument structures associated rule.
given grammar one argument structure per rule,
number argument structures. design grammar constrained
compositions must occur rule applied. especially seen
prepositional phrases, must occur rule noun phrase modify.
chart parser incrementally builds rule fragments left right fashion parse.
rule syntactically complete, checks whether composers constituents
tail rule accept number arguments specified rule (as shown
last column Table 1). so, calls semantic composer associated
constituent concepts yielded arguments produce concept head
rule. compose operation fails reason (the constituent cannot accept
arguments compose operation yield new concept) rule
succeed produce new constituent. several argument structures
(not case final grammar shown here) compose operation yields several
alternative concepts, several instances head constituent created,
concept.
provide example chart produced grammar Figure 10 Section 5,
part example whole understanding process. composition actions associated
lexical item, thus rule completion using grammar, listed
Appendix B.
3.5 Post-Parse Filtering
parse performed, post-parse filtering algorithm picks best interpretation utterance. First, algorithm extracts longest constituents
chart marked referring objects, assuming parsing utter446

fiGrounded Semantic Composition Visual Scenes

ADJ
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
P
P
P





















T0
ADJ
ADJ
CADJ
N
ART
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
NP
SPEC
P
POF

T1
ADJ
NP
N

T2

NP
P
P
RELVPRES
P
REL
REL
REL
RELVPRES
REL
RELVPRES
REL
P
P

NP
ART
P
N
VPRES
P
VPRES
P
VPRES
ADJ
CADJ

T3

T4

T5

T6

N
ART
POF
NP
NP
P
NP
ADJ

POF
N
NP

NP
POF

NP

NP

Arg Structure
T1 (T0 )
T0 (T1 )
T0 (T1 )
T0 ()
T0 (T1 )
T1 (T0 , T2 )
T3 (T0 , T5 )
T3 (T0 , T5 )
T2 (T0 , T4 )
T1 (T0 , T3 )
T2 (T0 , T3 )
T3 (T0 , T4 )
T2 (T0 , T3 )
T3 (T0 )
T2 (T0 )
T2 (T0 )
T0 (T1 )
T1 ()
T0 ()

Table 1: Grammar used Bishop
ance implies better understanding. filtering process checks candidates
consistency along following criteria:
candidates must either refer group single object
candidates marked referring unambiguously specified single object,
must unambiguously pick referent
referent specified single object case must across candidates
candidates marked selecting group, must select group
consistency checks fail, filtering algorithm provide exact information type inconsistency occurred (within-group ambiguity, contradicting
constituents, object fulfilling description), constituents involved
inconsistency objects (if any) referenced candidate constituent.
future, plan use information resolve inconsistencies active dialogue.
Currently, enforce best single object choice post processing stage.
filtering yields single object, nothing needs done. filtering yields group
objects, choose best matching object (note case ignore fact
whether resulting concept marked referring group single object).
several inconsistent groups referents remain filtering, randomly pick one object
groups.
447

fiGorniak & Roy

4. Semantic Composition
section revisit list descriptive strategies Section 2.3 explain
computationally capture strategy composition parts
utterance. composers presented follow composition schema: take
one concepts arguments yield another concept references possibly
different set objects. Concepts reference objects real numbered values indicating
strength reference. Composers may introduce new objects, even ones
exist scene such, may introduce new types objects (e.g. groups
objects referenced one object). perform compositions, concept
provides functionality produce single referent, group referents. single
object produced simply one maximum reference strength, whereas group
produced using reference strength threshold objects considered
possible referents concept. threshold relative minimum maximum
reference strength concept. composers first convert incoming concept
objects references, subsequently perform computations objects.
Furthermore, composers mark concepts referring, referring single
object referring group objects. independent actual number
objects yielded concept, used identify misinterpretations ambiguities. currently use flags delay composition arguments refer
objects. example, constraint prevents left green cause composition green considered adjective. cases, new chaining semantic
composers created delay application whole chain compositions
referring word encountered. chaining composers internally maintain queue
composers. argument composition operation refer object,
composer producing argument composer accepting pushed onto
queue. first referring argument encountered, whole queue composers
executed starting new argument proceeding backwards order
composers encountered.
plan exploit features co-operative setting one
described here, system engage clarifying dialogue user. explain
Section 3.5 converged single object reference task discussion
here, alternatives would be.
4.1 Colour - Probabilistic Attribute Composers
mentioned Section 3.1, chose exploit information used render
scene, therefore must recover colour information final rendered image.
hard problem Bishop presents virtual objects two colours.
renderer produce colour variations objects due different angles distances
light sources camera. colour average 2D projection object also
varies due occlusion objects. interest making framework easily
transferable noisier vision system, worked within probabilistic framework.
separately collected set labelled instances green purple cones, estimated
three dimensional Gaussian distribution average red, green blue values
pixel belonging example cones.
448

fiGrounded Semantic Composition Visual Scenes

asked compose given concept, type probabilistic attribute composer assigns object referenced source concept probability density function
evaluated measured average colour object.
4.2 Spatial Extrema Spatial Regions - Ordering Composers
determine spatial regions extrema, ordering composer orders objects along
specified feature dimension (e.g. x coordinate relative group) picks referents
extreme end ordering. so, assigns exponential weight function objects
according
i(1+v)
picking minimal objects, objects position sequence, v value
along feature dimension specified, normalized range 0 1 objects
consideration. maximal case weighted similarly, using reverse ordering
subtracting fraction exponent 2,
(imax i)(2v)
imax number objects considered. reported results = 0.38.
formula lets referent weights fall exponentially position ordering distance extreme object. way extreme objects isolated
except cases many referents cluster around extremum, making picking
single referent difficult. attach type composer words like leftmost
top.
ordering composer also order objects according absolute position, corresponding closely spatial regions rather spatial extrema relative group.
reference strength formula version


(1+ dmax )
euclidean distance reference point, dmax maximum
distance amongst objects consideration.
version composer attached words like middle. effect
reference weights relative absolute position screen. object close centre board achieves greater reference weight word middle, independently
position objects kind. Ordering composers work across number
dimensions simply ordering objects Euclidean distance, using exponential falloff function cases. ordering composer middle,
example, computes distance board centre centres mass objects,
thus prefers centred screen.
4.3 Grouping Composers
non-numbered grouping (e.g., describer says group cones), grouping
composer searches scene groups objects within maximum distance
threshold another group member. threshold currently set hand based
small number random scenes designers identified isolated groups
449

fiGorniak & Roy

adjusted threshold correctly find others. considers objects
referenced concept passed argument. numbered groups
(two, three), composer applies additional constraint groups
contain correct number objects. Reference strengths concept determined
average distance objects within group. acknowledge approach
grouping simplistic currently investigating powerful visual grouping
algorithms take topological features consideration. spite simple approach,
demonstrate instances successfully understanding references groups
Bishop.
output grouping composer may thought group groups. understand motivation this, consider utterance, one left group
purple ones. expression, phrase group purple ones activate grouping
composer find clusters purple cones. cluster, composer computes
convex hull (the minimal elastic band encompasses objects) creates
new composite object convex hull shape. composition
takes place understand entire utterance, composite group serves potential
landmark relative left.
However, concepts marked behaviour changes split apart concepts
refering groups. example, composer attached sets flag concepts
passing it. Note involved composition grammar rules
type NP NP P NP, performing spatial compositions phrases like
left of. Therefore, phrase frontmost one three green ones pick
front object within best group three green objects.
4.4 Spatial Relations - Spatial Composers
spatial semantic composer employs version Attentional Vector Sum (AVS)
suggested Regier Carlson (2001). AVS measure spatial relation meant
approximate human judgements corresponding words like left
2D scenes objects. computes interpolation angle
centres masses objects angle two closest points objects,
addition value depending height relative top landmark object.
Despite participants talking 2D projections 3D scenes found AVS
distinguishes spatial relations used data rather well simply applied
2D projections. participants often used spatial descriptors below, suggesting
sometimes conceptualized scenes 2D. 3D setting would expect
see consistent use semantic patterns like front instead below.
Given two concepts arguments, spatial semantic composer converts sets
objects, treating one set providing possible landmarks, providing possible
targets. composer calculates AVS possible combination landmarks
targets. reference vector used AVS specified lexical entry containing
composer, e.g. (0, 1) behind. Finally, spatial composer divides result
Euclidean distance objects centres mass, account fact
participants exclusively used nearby objects select spatial relations.
450

fiGrounded Semantic Composition Visual Scenes

4.5 Anaphoric Composers
Triggered words like (as left one) previous, anaphoric
composer produces concept refers single object, namely last object removed
scene session. object specially marks concept referring
current, previous visual scene, calculations concept
performed visual context.
example, parser calls upon anaphoric composer attached lexical entry provide interpretation one, composer marks
produced concept referring back previous visual scene, sets previously selected object possible referent. consider another composer, say spatial
composer attached left one left one. asks spatial
relation features referents one one, spatial relation
features (see Section 4.4) computed previous visual scene object
removed due previous utterance possible landmark spatial
relation.

5. Example: Understanding Description

example scene

purple one

one left

purple one left

Figure 9: Example: purple one left
illustrate operation overall system, section step
examples Bishop works detail. Consider scene top left Figure 9,
output chart parser utterance, purple one left Figure 10.
Starting top left parse output, parser finds lexicon ART
(article) selecting composer takes one argument. finds two lexical entries
purple, one marked CADJ (colour adjective), one N (noun).
composer, probabilistic attribute composer marked P(),
adjective expects one argument whereas noun expects none. Given noun
expects arguments grammar contains rule form NP N, NP
(noun phrase) instantiated probabilistic composer applied default set
objects yielded N, consists objects visible. composer call marked
451

fiGorniak & Roy

P(N) chart. composition, NP contains subset purple objects
(Figure 9, top right). point parser applies NP ART NP, produces
NP spanning first two words contains purple objects, marked
unambiguously referring object. S(NP) marks application selecting
composer called S.

ART:the

purple

one





left

CADJ:purple
N:purple
NP:P(N)
NP:S(NP)
N:one
NP:one
NP:P(N)
NP:S(NP)
P:on
ART:the
N:left
ADJ:left
N:left
NP:left
NP:left
NP:S(NP)
NP:S(NP)
NP:O.x.min(NP)
NP:O.x.min(NP)
NP:O.x.min(NP)

Figure 10: Sample parse referring noun phrase

parser goes produce similar NP covering first three words combining
purple CADJ one result the. P (preposition) left
dangling moment needs constituent follows it. contains modifying
semantic composer simply bridges P, applying first argument second.
another the, left several lexical entries: ADJ one N forms
contains ordering semantic composer takes single argument, whereas second N
form contains spatial semantic composer takes two arguments determine target
landmark object. point parser combine left two
possible NPs, one containing ordering spatial composer. first
NPs turn fulfills need P second argument according NP
NP P NP, performing ordering compose first one (for one left), selecting
objects left (Figure 9, bottom left). application ordering composer
452

fiGrounded Semantic Composition Visual Scenes

denoted O.x.min(NP) chart, indicating ordering composer ordering
along x axis selecting minimum along axis. combining purple
one, composer selects purple objects left (Figure 9, bottom right).
Finally purple one, produces set objects purple one, marks
concept unambiguously picking single object. Note parser attempts
use second interpretation left (the one containing spatial composer) fails
composer expects two arguments provided grammatical
structure sentence.

6. Results Discussion
section first discuss systems overall performance collected data, followed detailed discussion performance implemented descriptive strategies.
6.1 Overall Performance
evaluation purposes, hand-annotated data, marking descriptive strategies
occurred example. examples use several reference strategies. Table 2
present overall accuracy results, indicating percentage different groups
examples system picked referent person describing object. first
line table shows performance relative total set utterances collected.
second one shows percentage utterances system understood correctly excluding
marked using descriptive strategy listed Section 4, thus
expected understood Bishop. examples given Section 2.3.6.
final line Table 2 shows percentage utterances system picked
correct referent relative clean development testing sets, leaving utterances
marked well marked containing kind error. defined
earlier, could speech error still understood human listener, due
error algorithm pairs utterances selection events. Additionally, relying
automatic speech segmentation sometimes merged utterances one
separate utterances. mistakenly attributes combination two descriptions
one object selection leaves another object selection without corresponding utterance.
Note, however, due loose parsing strategy frequent redundancies
speakers utterances system able handle good number utterances marked
either error.
Utterance Set

except
except Errors (clean)

Accuracy - Development
76.5%
83.2%
86.7%

Accuracy - Testing
58.7%
68.8%
72.5%

Table 2: Overall Results

Using unconstrained speech primarily made writing covering yet precise grammar difficult. difficulty together loose parsing strategy made system occasionally
attempt compositions supported grammatical structure utterance.
453

fiGorniak & Roy

overeager parsing strategy also produces number correct guesses would
found tighter grammar, found development tradeoff often
favoured looser parsing terms number correct responses produced. Constructing
grammar obvious area addressed machine learning approach
future. Using speech segmenter together utterance reassembler produced
errors used successful selection event strong guideline deciding
speech segments part description. Errors type occur less 1%
data.
Bishop Performance

100

random guess mean
random guess mean +/ std dev

90
80

Average Accuracy

70
60
50
40
30
20
10
0

1

2

3

4

development

development clean

test

test clean

Figure 11: Results development test corpora

Figure 11 graphs results corpus simulation uniform random
selection strategy. bar shows mean performance data set, error bars delimiting one standard deviation. figure shows results left right complete
development corpus, clean development corpus, complete test corpus clean
test corpus. system understands vast majority targeted utterances performs
significantly better random baseline. Given unconstrained nature input
complexity descriptive strategies described Section 2.3 consider
important achievement.
Table 3 provides detail various descriptive strategies lists percentage correctly identified referents utterances employing spatial extrema regions,
454

fiGrounded Semantic Composition Visual Scenes

combinations one spatial extremum, grouping constructs, spatial relations
anaphora. Note categories mutually exclusive. list
separate results utterances employing colour terms colour terms
source errors (due synthetic nature vision system).
Utterance Set
Spatial Extrema
Combined Spatial Extrema
Grouping
Spatial Relations
Anaphora

Accuracy - Development
86.8% (132/152)
87.5% (49/56)
34.8% (8/23)
64.3% (9/14)
100% (6/6)

Accuracy - Test
77.4% (72/93)
75.0% (27/36)
38.5% (5/13)
40.0% (8/20)
75.0% (3/4)

Table 3: Detailed Results
surprisingly, Bishop makes mistakes errors present strategies
implemented occur. However, Bishop achieves good coverage even
cases. often result overspecification part describer.
tendency towards redundancy shows even simple cases, example use
purple even though purple cones left scene. translates furthermore
specifications relative groups objects simple leftmost would
suffice. Overspecification human referring expressions well-known phenomenon often
attributed incremental nature speech production. Speakers may listing visually
salient characteristics colour determining whether colour distinguishing
feature intended referent (Pechmann, 1989).
worst performance, grouping composers, attributed
fact visual grouping strategy simplistic task hand,
phenomenon often combined rather complex ways strategies.
combinations also account number mistakes amongst composer
perform much better combined strategies grouping. cover
shortcomings grouping composers detail Section 6.2.3.
Mistakes amongst descriptive strategies cover several causes:
Overcommitment/undercommitment errors due fact interpretation implemented filtering process without backtracking. semantic
composer must produce set objects attached reference strengths,
next composer works set objects strictly feedforward manner.
composition strategy fails target object left one stage (e.g.
leftmost one front, leftmost selects leftmost objects, including
obvious example front good example leftmost). also
fails many target objects included (e.g. poor example leftmost
included set turns ideal example front). Estimating
group membership thresholds data certainly decrease occurrence
errors, real solution lies backtracking strategy combined composers sensitive visual scenery beyond immediate function.
sensitive composers might take account facts isolated nature certain
455

fiGorniak & Roy

candidates well global distribution cones across board. discuss
specific cases global local visual context influence interpretations
words Section 6.2.
Insufficient grammar cases contain many prepositional phrases (e.g.
leftmost one group purple ones right bottom) grammar
specific enough produce unambiguous answers. grammar might attach
right object rather group objects, taking account
biases parsing human listeners showed.
Flawed composers composers implemented sufficient
understand facets corresponding human descriptive strategies.
mention problems following section.
6.2 Performance Composers
go reconsider descriptive strategy discuss successes failures
composers designed deal each.
6.2.1 Colour
Due simple nature colour naming Bishop task, probabilistic composers
responsible selecting objects based colour made errors.
6.2.2 Spatial Regions Extrema
ordering composers correctly identify 100% cases participant uses
colour single spatial extremum description. conclude participants
follow process yields result ordering objects along spatial dimension
picking extreme candidate. Participants also favour descriptive strategy, using
colour alone 38% clean data. Figure 3 provides examples type
system handles without problems.
Description spatial region occurs alone 5% clean data, together
strategies 15% clean data. Almost examples strategy
occurring alone use words like middle centre. left image Figure 12 exemplifies
use middle ordering semantic composer models. object referred
one closest centre board. model fact human speakers
use version descriptive strategy obvious single candidate object.
right image Figure 12 shows different interpretation middle: object
middle group objects. Note group objects linguistically mentioned.
Also note within group two candidate centre objects, one
front preferred. composer picks correct object use middle
target object also happens one closest centre board.
Figure 13 shows another use word middle. strategy seems related
last one (picking object middle group), however scene happens
divided two groups objects single object them. Even though
object back closest one centre board, due visual
456

fiGrounded Semantic Composition Visual Scenes

green one middle

purple cone middle

Figure 12: Types middles 1
context participants understand object middle. composer fails
case.

purple one middle
Figure 13: Types middles 2
Figure 14 shows sequence two scenes followed data collection
session. first scene utterance clear example extremum combined
region specification, ordering composers easily pick correct object.
next scene, listener identified leftmost object one right middle,
despite scenes similarity right image Figure 12, middle object
middle group. suspect use middle scene
biases understanding middle relative board case, providing
example visual, also historical context influence meanings
words. (Note right utterance right middle interpreted
SPEC grammatical type Bishop, spatial role. See grammar
Table 1.)

green one middle front

purple one right middle

Figure 14: Types middles 3
457

fiGorniak & Roy

summary, catalogue number different meanings word middle
data linguistically indistinguishable, depend visual historical
context correctly understood. generally, impossible distinguish regionbased uses various extrema-based uses words based utterance alone
data. made decision treat middle signify regions left, top, etc.
signify extrema, examples middle show selection meaning
words use depends far subtler criteria global local visual context,
existence unambiguous candidate past use descriptive strategies.
Participants composed one spatial region extrema references 30%
clean data. ordering composers correctly interpret 85% cases, example
Figure 4 Section 2.3.2. mistakes composers make usually due
overcommitment faulty ordering. Figure 15 shows example could interpreted
either problem (we indicate correct example object system selects).
note example comes non-native English speaker often
used native speakers would use in. system selects purple object
closest back board instead indicated correct solution. could
interpreted overcommitment, composer back include
target object, leaving composer left wrong set objects choose from.
better explanation perhaps ordering composers reversed
case, composer back take objects selected left input.
However, ordering violates far common left-to-right ordering region
extrema strategies data, selected implement system. question
thus becomes causes difference ordering cases like one Figure 15.
again, suspect visual context plays role. Perhaps clear listener
double spatial specification would overspecification object system
selects (it simply purple one back). response, listener may seek
object needs full utterance, true target. However, analysis
hard combine common trend towards overspecification part
speaker, leaving us need run focused study phenomena pin
factors play role interpretation.

purple cone back left side
Figure 15: Misinterpreted utterance using composed extrema

458

fiGrounded Semantic Composition Visual Scenes

6.2.3 Grouping
composers implementing grouping strategies used participants simplistic composers implemented, compared depth actual phenomenon
visual grouping. left scene Figure 16 shows example grouping composer
handles without problem. group two cones isolated cones
example, thus easily found distance thresholding algorithm. contrast,
right scene depicts example would require much greater sophistication find
correct group. target group three cones visually isolated scene,
requiring criteria like colinearity even make candidate. Furthermore,
second colinear group three cones would easily best example row
three purple cones absence target group. target groups
alignment vertical axis let stand row make
likely interpretation. algorithm currently fails include grouping hints,
thus fails pick correct answer scene. Note hints always linguistically marked (through row), often colinearity silently
assumed holding groups, making simple grouping operator fail. rich source
models possible human grouping strategies like co-linearity comes research
Gestalt Grouping (Wertheimer, 1999).

cone right pair
cones

purple cone front
row three purple cones

Figure 16: Easy hard visual grouping

6.2.4 Spatial Relations

green cone behind purple cone
Figure 17: Successful spatial relation understanding
AVS measure divided distance objects corresponds well human
spatial relation judgements task. errors occur utterances contain
459

fiGorniak & Roy

spatial relations due possible landmarks targets correctly identified
(grouping region composers might fail provide correct referents). spatial
relation composer picks correct referent cases landmarks targets
correct ones, example Figure 17. Also see next section correct
example spatial relations. Obviously, types spatial relations relations
based purely distance combined relations (to left behind) decided
cover implementation, occur data covered
future efforts.
6.2.5 Anaphora

cone right front

cone behind left

Figure 18: Successful Anaphora Understanding
solution use anaphora Bishop task performs perfectly replicating
reference back single object clean data. reference usually combined
spatial relation data, Figure 18. Due equally good performance
spatial relation composer, cover cases anaphora development data. However,
complex variants anaphora currently cover, example
reference back groups objects sequence Figure 19, follows
right example Figure 16.

next cone row

last cone row

Figure 19: Group Based Anaphora

7. Future Directions
Given analysis Bishops performance, several areas future improvements may explored. descriptive strategies classified
understood computational system:
460

fiGrounded Semantic Composition Visual Scenes

Distance simple implementation understand strategy grammatical behaviour spatial relation composers, uses inverted distance measure
score target objects.
Symmetry Selection symmetry occurred symmetry across horizontal centre
board data. thus propose mirror landmark object across
horizontal centre, scoring possible targets inverted distance mirror
image.
Numbered grouping limited groups two three objects,
algorithm work higher numbers.
In-group numbering descriptive strategy like second row understood slight modification ordering composer put peak
exponential distribution ends middle sequences,
rather arbitrary points.
Connectivity simple way understand lonely cone could measure distance
closest objects within group possible referents. better solution might
construct local connectivity graph look topologically isolated objects.
Furthermore, already mentioned several areas possible improvement
existing system due faulty assumptions:
Individual composers Every one semantic composers attempts solve separate
hard problem, (e.g. grouping spatial relations) seen long
lines work dedicated sophisticated solutions ours. individual
problems emphasis paper. believe improvements
implementation improve system whole, much following
possible techniques.
Backtracking lack backtracking Bishop addressed. parse
produce single referent, backtracking would provide opportunity revise
loosen decisions made various stages interpretation referent
produced.
Visual context semantics Backtracking solves problems system knows
either failed obtain answer, knows answer produced
unlikely one. However, numerous examples data one
interpretation utterance produces perfectly likely answer according
measurements, example middle finds object exact centre screen.
many scenes interpretation produces correct answer, measurement
relative objects would produce wrong one. However, observe
participants interpret middle way obvious structure
rest scene. chance scene divided group objects left
group objects right, middle reliably refer isolated object
groups, even another object closer actual centre
screen. future system take account local global visual context
composition account human selection strategies.
461

fiGorniak & Roy

Lexical entries made assumption lexical entries word-like entities
contain encapsulated semantic information. Even relatively constrained
task, somewhat faulty assumption. example, ambiguity word like
resolved careful design grammar (see section 4.3), may
useful treat entire phrases left single lexical entries, perhaps
grammar replace left spatial markers (Jackendoff
proposed absorbing rules syntax lexicon, see Jackendoff, 2002).
Dialogue constructing parse charts obtain rich set partial full syntactic
semantic fragments offering explanations parts utterance. present,
largely ignore information-rich resource selecting best referent.
successful approach might entail backtracking revision described above,
also engage clarification dialogue human speaker. system could
use fragments knows check validity interpretation (is
group green ones mean?) could simply disambiguate directly (Which
two mean?) followed explanation confusion terms
semantic fragments formed.
Manual construction visually-grounded lexicon presented limited
accuracy due various structural parametric decisions manually approximated. Machine learning algorithms may used learn many parameter settings
set hand work, including on-line learning adapt parameters
verbal interaction. Although thresholds probability distribution functions may
estimated data using relatively straightforward methods, learning problems
far challenging. example, learning new types composers appropriate
corresponding grammatical constructs poses difficult challenge future. Minimally,
plan automate creation new versions old composers (e.g. applied different
dimensions attributes). Moving beyond this, clear how, example, set
handling functionality used determine groups referents expand automatically
useful ways. also interesting study people learn understand novel
descriptive strategies.
also continuing work applying results grounded language systems
multimodal interface design (Gorniak & Roy, 2003). recently demonstrated application Bishop system described paper problem referent resolution
graphical user interface 3D modelling application, Blender (Blender Foundation ,
2003). Using Bishop/Blender hybrid, users select sets objects correct wrong
mouse selections voice commands select door behind one show
windows.

8. Summary
presented model visually-grounded language understanding. heart
model set lexical items, grounded terms visual features grouping
properties applied objects scene. robust parsing algorithm finds chunks
syntactically coherent words input utterance. determine semantics
phrases, parser activates semantic composers combine words determine
462

fiGrounded Semantic Composition Visual Scenes

joint reference. robust parser able process grammatically ill-formed transcripts
natural spoken utterances. evaluations, system selected correct objects response
utterances 76.5% development set data, 58.7% test set data.
clean data sets various speech processing errors held out, performance higher
yet. suggested several avenues improving performance system including better
methods spatial grouping, semantically guided backtracking sentence processing,
use machine learning replace hand construction models, use interactive
dialogue resolve ambiguities. near future, plan transplant Bishop
interactive conversational robot (Hsiao et al., 2003), vastly improving robots ability
comprehend spatial language situated spoken dialogue.

Acknowledgments
Thanks Ripley, Newt Jones.

Appendix A. Utterances Test Data Set
following 179 utterances collected test data set. presented
correct order seen understanding system. means include
errors due faulty speech segmentation well due algorithm stitches oversegmented utterances back together.
























green cone middle
purple cone behind
purple cone way left
purple cone corner right
green cone front
green cone back next purple cone
purple cone middle front
purple cone middle
frontmost purple cone
green cone corner
obstructed green cone
purple cone hidden back
purple cone right rear
green cone front
solitary green cone
purple cone front row three purple cones
next cone row
last cone row
cone right pair cones
cone
cone closest middle front
cone right set cones furthest left
cone right front
463

fiGorniak & Roy

cone behind left
frontmost left cone
backmost left cone
solitary cone
cone middle right
front cone cone
frontmost green cone
green cone front right purple cone
green cone back row four
cone green cone behind purple cone
purple cone behind row three green cones
frontmost green cone right
green cone corner back
green cone back
purple cone back right
green cone front left
purple cone behind
purple cone behind one
green cone behind purple cone
green cone two purple cone
purple cone front
purple cone touching green cone
green cone front
purple cone left
green cone back left
purple cone middle front two purple cones
purple cone left four green cones
purple cone left leftmost
green cone
frontmost green cone
rear cone
rightmost cone
rearmost cone
left green cone
purple cone green cone
green cone
furthestmost green cone exact middle
frontmost green cone
rightmost green cone clump four green cones right
green cone front two purple cones near left
green cone two purple cones back middle
frontmost purple cone
leftmost two purple cones right mean left,
sorry leftmost two purple cones left side
green cone left halfway back
464

fiGrounded Semantic Composition Visual Scenes

























frontmost green cone front purple cone
middle purple cone
green cone left left
green cone middle front green cone
green cone left
furthestmost purple cone left
furthest green cone
leftmost green cone
leftmost purple cone
middle green cone
green cone two green cones
frontmost purple cone
backmost purple cone
green cone two purple cones nearest front
leftmost purple cone
green cone front
green cone
frontmost two back purple cones
rightmost purple cone
leftmost purple cone
purple cone front
last purple cone
frontmost purple cone clump five purple cones
right
backmost green cone
backmost purple cone
green cone directly front purple cone
purple cone behind green cone left
green cone behind purple cone left
leftmost two left back corner green cones
rightmost purple cone
middle cone behind frontmost purple cone
green cone left front corner
purple cone right back corner
third green cone line green cones near middle
green cone two purple cones near back
green cone back left
green cone back
green cone behind frontmost green cone
frontmost green cone
green cone
last line four purple cones
centre purple cone three cones left
purple cone two purple cones
middle purple cone
465

fiGorniak & Roy












leftmost purple cone
middle purple cone
front left purple cone
front right purple cone
second four purple cones
middle purple cone
purple cone left
last purple cone
green one middle way back
purple one way middle little
left way back
green one middle front thats front another
green one
purple one middle thats behind green one
right purple one front line
purple ones
left green one two purple ones line
left purple one middle row mean middle
line three purple ones
green one left thats hidden purple one
left purple one thats way corner separate
middle towards right theres line purple ones
theres kink line one thats right lines turns
purple one way right front
purple one front middle
green one middle
purple front way right
rightmost green one
leftmost green one
last green one last green one
frontmost purple one right
purple one back towards left thats next two
purple ones
purple one back towards right thats part pair
purple one front group right
purple one middle thats front group one right
purple one left way back
purple one left thats behind another purple one
purple one left thats front purple one
left thats
purple one right thats hidden two purple ones
purple one way back corner right
purple one thats front right last one
last one
purple front right
466

fiGrounded Semantic Composition Visual Scenes








purple one way right
green one right thats middle bunch
green one way left thats almost totally obscured
last purple one left crooked line purple ones
first purple one left thats crooked line
purple one way one way back towards
left thats behind green purple one way
back
purple one towards back thats pretty much back
thats front green purple one
purple one middle back
purple one left thats furthest back
green one middle thats furthest front
purple one towards middle towards left
thats closest
middle purple one stands thats closest
purple ones middle towards right one
corner
purple one thats closest middle
way right green one middle line three
green ones
way right closest green one
way right close green one
green one way right corner back
purple one thats towards back left corner
purple one front left corner
purple one near middle thats another purple one
purple one thats front another purple one
purple one right
purple one left
green one middle thats behind another green one
closest green one middle green one thats closest
middle
green one way back towards right
close green one one left
one left

Appendix B
following specifies complete lexicon used Bishop XML format. initial
comment explains attributes lexical entries.
see online appendix file lexicon.xml.

467

fiGorniak & Roy

References
Abney, S. (1997). Part-of-speech tagging partial parsing. Corpus-Based Methods
Language Speech, chap. 4, pp. 118136. Kluwer Academic Press, Dordrecht.
Allen, J. (1995). Natural Language Understanding, chap. 3. Benjamin/Cummings
Publishing Company, Inc, Redwood City, CA, USA.
Bailey, D. (1997). push comes shove: computational model role motor
control acquisition action verbs. Ph.D. thesis, Computer science division,
EECS Department, University California Berkeley.
Barker, C. (2002). dynamics vagueness. Linguistics Philosophy, 25, 136.
Blender Foundation (2003). Blender 3D graphics creation suite. http://www.blender3d.org.
Brown, M., Buntschuh, B., & Wilpon, J. (1992). SAM: perceptive spoken languageunderstanding robot. IEEE Transactions Systems, Man Cybernetics, 6 (22),
13901402.
Brown-Schmidt, S., Campana, E., & Tanenhaus, M. K. (2002). Reference resolution
wild. Proceedings Cognitive Science Society.
Carletta, J., & Mellish, C. (1996). Risk-taking recovery task-oriented dialogue.
Journal Pragmatics, 26, 71107.
Desolneux, A., Moisan, L., & Morel, J. (2003). grouping principle four applications.
IEEE Transactions Pattern Analysis Machine Intelligence, 255 (4), 508513.
Dhande, S. (2003). computational model connect gestalt perception natural
language. Masters thesis, Massachusetts Institure Technology.
Engbers, E., & Smeulders, A. (2003). Design considerations generic grouping vision.
IEEE Transactions Pattern Analysis Machine Intelligence, 255 (4), 445457.
Eugenio, B. D., Jordan, P. W., Thomason, R. H., & Moore, J. D. (2000). agreement
process: empirical investigation human-human computer-mediated collaborative
dialogues. International Journal Human-Computer Studies, 53 (6), 10171076.
Gorniak, P., & Roy, D. (2003). Augmenting user interfaces adaptive speech commands.
Proceedings International Conference Multimodal Interfaces.
Griffin, Z., & Bock, K. (2000). eyes say speaking. Psychological Science,
11, 274279.
Haddock, N. (1989). Computational models incremental semantic interpretation. Language Cognitive Processes, 4, 337368.
Hsiao, K., Mavridis, N., & Roy, D. (2003). Coupling perception simulation: Steps
towards conversational robotics. Proceedings IEEE/RSJ International Conference Intelligent Robots Systems (IROS).
Jackendoff, R. (2002). Whats lexicon?. Noteboom, S., Weerman, F., & Wijnen
(Eds.), Storage Computation Language Faculty, chap. 2. Kluwer Academic
Press.
468

fiGrounded Semantic Composition Visual Scenes

Kyburg, A., & Morreau, M. (2000). Fitting words: vague words context. Linguistics
Philosophy, 23, 577597.
Lammens, J. M. (1994). computational model color perception color naming. Ph.D.
thesis, State University New York.
Landau, B., & Jackendoff, R. (1993). spatial language spatial
cognition. Behavioural Brain Sciences, 2 (16), 217238.
Miller, G., & Johnson-Laird, P. (1976). Language Perception. Harvard University Press.
Nagao, K., & Rekimoto, J. (1995). Ubiquitous talker: Spoken language interaction
real world objects. Proceeding International Joint Conference Artificial
Intelligence.
Narayanan, S. (1997). KARMA: Knowledge-based Action Representations Metaphor
Aspect. Ph.D. thesis, Univesity California, Berkeley.
Partee, B. H. (1995). Lexical semantics compositionality. Gleitman, L. R., & Liberman, M. (Eds.), Invitation Cognitive Science: Language, Vol. 1, chap. 11, pp.
311360. MIT Press, Cambridge, MA.
Pechmann, T. (1989). Incremental speech production referential overspecification. Linguistics, 27, 89110.
Pustejovsky, J. (1995). Generative Lexicon. MIT Press, Cambridge, MA, USA.
Regier, T. (1996). Human Semantic Potential. MIT Press.
Regier, T., & Carlson, L. (2001). Grounding spatial language perception: empirical computational investigation. Journal Experimental Psychology: General,
130 (2), 273298.
Roy, D. (2002). Learning visually-grounded words syntax scene description task.
Computer Speech Language, 16 (3).
Roy, D., Gorniak, P. J., Mukherjee, N., & Juster, J. (2002). trainable spoken language
understanding system. Proceedings International Conference Spoken Language Processing.
Roy, D., & Pentland, A. (2002). Learning words sights sounds: computational
model. Cognitive Science, 26 (1), 113146.
Schuler, W. (2003). Using model-theoretic semantic interpretation guide statistical parsing word recognition spoken language interface. Proceedings Association Computational Linguistics.
Shi, J., & Malik, J. (2000). Normalized cuts image segmentation. IEEE Transactions
Pattern Analysis Machine Intelligence, 8 (22), 888905.
Siskind, J. M. (2001). Grounding lexical semantics verbs visual perception using
force dynamics event logic. Journal Artificial Intelligence Research, 15, 3190.
Wertheimer, M. (1999). Laws organization perceptual forms. source book
Gestalt psychology, pp. 7188. Routledge, New York.
Winograd, T. (1970). Procedures representation data computer program
understanding natural language. Ph.D. thesis, Massachusetts Institute Technology.
469

fiGorniak & Roy

Yoshida, N. (2002). Utterance segmenation spontaneous speech recognition. Masters
thesis, Massachusetts Institute Technology.

470

fiJournal Artificial Intelligence Research 21 (2004) 1-17

Submitted 7/03; published 1/04

Effective Dimensions Hierarchical Latent Class Models
Nevin L. Zhang

lzhang@cs.ust.hk

Department Computer Science
Hong Kong University Science Technology, China

Tomas Kocka

kocka@lisp.vse.cz

Laboratory Intelligent Systems Prague
Prague University Economics, Czech Republic

Abstract
Hierarchical latent class (HLC) models tree-structured Bayesian networks
leaf nodes observed internal nodes latent. theoretically well
justified model selection criteria HLC models particular Bayesian networks
latent nodes general. Nonetheless, empirical studies suggest BIC score
reasonable criterion use practice learning HLC models. Empirical studies also
suggest sometimes model selection improved standard model dimension
replaced effective model dimension penalty term BIC score.
Effective dimensions difficult compute. paper, prove theorem
relates effective dimension HLC model effective dimensions number
latent class models. theorem makes computationally feasible compute
effective dimensions large HLC models. theorem also used compute
effective dimensions general tree models.

1. Introduction
Hierarchical latent class (HLC) models (Zhang, 2002) tree-structured Bayesian networks
(BNs) leaf nodes observed internal nodes latent. generalize latent
class models (Lazarsfeld Henry, 1968) first identified potentially useful
class Bayesian networks Pearl (1988). concerned learning HLC models
data. fundamental question select among competing models.
BIC score (Schwarz, 1978) popular metric researchers use select among
Bayesian network models. consists loglikelihood term measures fitness
data penalty term depends linearly upon standard model dimension, i.e.
number linearly independent standard model parameters. variables
observed, BIC score asymptotic approximation (the logarithm) marginal
likelihood (Schwarz, 1978). also consistent sense that, given sufficient data,
BIC score generative model model data sampled larger
models equivalent generative model.
latent variables present, BIC score longer asymptotic approximation marginal likelihood (Geiger et al., 1996). remedied,
extent, using concept effective model dimension. fact replace standard model
dimension effective model dimension BIC score, resulting scoring function,
called BICe score, asymptotic approximation marginal likelihood almost
everywhere except singular points (Rusakov Geiger, 2002).
c
2004
AI Access Foundation. rights reserved.

fiZhang & Kocka

Neither BIC BICe proved consistent latent variable models.
matter fact, even defined means model selection criterion
consistent latent variable models. Empirical studies suggest BIC score
well-behaved practice task learning HLC models. three related searchbased algorithms learning HLC models, namely double hill-climbing (DHC) (Zhang,
2002), single hill-climbing (SHC) (Zhang et al., 2003), heuristic SHC (HSHC) (Zhang,
2003). absence theoretically well justified model selection criterion, Zhang (2002)
tested DHC four existing scoring functions, namely AIC score (Akaike, 1974),
BIC score, Cheeseman-Stutz (CS) score (Cheeseman Stutz, 1995), holdout
logarithmic score (HLS)(Cowell et al., 1999). real-world synthetic data used.
real-world data, BIC CS enabled DHC find models regarded
best domain experts. synthetic data, BIC CS enabled DHC find
models either identical resemble closely true generative models.
coupled AIC HLS, hand, DHC performed significantly worse. SHC
HSHC tested synthetic data sampled fairly large HLC models (as much
28 nodes). BIC used tests. cases, BIC enabled SHC
HSHC find models either identical resemble closely true generative
models. empirical results indicate algorithms perform well,
also suggest BIC reasonable scoring function use learning HLC models.
experiments also reveal model selection sometimes improved BICe
score used instead BIC score. explain detail Section 3
order use BICe score practice, need way compute effective dimensions. trivial task. effective dimension HLC model rank
Jacobian matrix mapping parameters model parameters
joint distribution observed variables. number rows Jacobian
matrix increases exponentially number observed variables. construction
Jacobian matrix calculation rank computationally demanding.
Moreover done algebraically high numerical precision avoid
degenerate cases. necessary precision grows size matrix.
Settimi Smith (1998, 1999) studied effective dimensions two classes models:
trees binary variables latent class (LC) models two observed variables.
obtained complete characterization two classes. Geiger et al. (1996) computed effective dimensions number models. conjectured rare
effective standard dimensions LC model differ. matter fact,
found one model. Kocka Zhang (2002) found quite number LC models
whose effective standard dimensions differ. also proposed easily computable
formula estimating effective dimensions LC models. estimation formula
empirically shown accurate.
paper, prove theorem relates effective dimension HLC model
effective dimensions two HLC models contain fewer latent variables.
Repeated application theorem allows one reduce task computing effective
dimension HLC model subtasks computing effective dimensions LC models.
makes computationally feasible compute effective dimensions large HLC
models.
2

fiEffective Dimensions HLC Models

start Section 2 formal definition effective dimensions Bayesian networks latent variables. Section 3, provide empirical evidence suggest use
BICe instead BIC sometimes improves model selection. Section 4 presents main
theorem Section 5 devoted proof theorem. Section 6, prove theorem effective dimensions general tree models explain main
theorem allows one compute effective dimension arbitrary tree models. Finally,
concluding remarks provided Section 7.

2. Effective Dimensions Bayesian Networks
paper, use capital letters X denote variables lower case
letters x denote states variables. domain cardinality
variable X denoted X |X| respectively. Bold face capital letters
denote sets variables. denotes Cartesian product domains variables
set Y. Elements denoted bold lower case letters
sometimes referred states Y. consider variables finite
number states.
Consider Bayesian network model possibly contains latent variables.
standard dimension ds(M ) number linearly independent parameters
standard parameterization . parameters denote, variable parent
configuration variable, probability variable state (except one)
given parent configuration. Suppose consist k variables x1 , x2 , . . . , xk . Let ri
qi respectively number states xi number possible combinations
states parents. xi parent, let qi 1. ds(M ) given
ds(M ) =

k
X

qi (ri 1).

i=1

~
notational simplicity, denote standard dimension n. Let =(
1 , 2 , . . . , n )
vector n linearly independent model parameters . let set
observed variables. Suppose m+1 possible states. enumerate first states
y1 , y1 , . . . , ym .
~ mapping
(1im), P (yi ) function parameters .
n

n dimensional parameter space (a subspace R ) R , namely : (1 , 2 , . . . , n )
(P (y1 ), P (y2 ), . . . , P (ym )). Jacobian matrix mapping following mn
matrix:
~ = [Jij ] = [ P (yi ) ]
JM ()
j
], understanding
convenience, often write matrix JM = [ P(Y)
j
elements j-th column obtained allowing run possible states
except one.
~ commonly used parameterizations
i, P (yi ) function .
~ Hence make following
Bayesian networks, actually polynomial function .
assumption:
3

fiZhang & Kocka

Assumption 1 Bayesian network parameterized parameters
joint distribution observed variables polynomial functions parameters
M.
obvious consequence assumption elements JM also polynomial
~
functions .
~ JM matrix real numbers. Due Assumption 1, rank
given value ,
matrix constant almost everywhere parameter space (Geiger et al.,
1996. Also see Section 5.1.). specific, rank everywhere except set
measure zero smaller d. constant called regular rank JM .
regular rank JM also called effective dimension Bayesian network
model . Hence denote de(M ). understand term effective dimension,
consider subspace Rm spanned joint probability P (Y) observed variables,
equivalently range mapping . term reflects fact that, almost every
~ small enough open ball around ()
~ resembles Euclidean space dimension
value ,
(Geiger et al., 1996).
multiple ways parameterize given Bayesian network model. However,
choice parameterization affect space spanned joint probability P (Y).
Together interpretation previous paragraph, implies definition
effective dimension depend particular parameterization one uses.

3. Selecting among HLC Models
hierarchical latent class (HLC) model Bayesian network (1) network structure rooted tree (2) variables leaf nodes observed
variables not. observed variables sometimes referred manifest variables
variables latent variables. Figure 1 shows structures two HLC
models. latent class (LC) model HLC model one latent variable.
theme paper computation effective dimensions HLC models.
mentioned introduction, interesting effective dimension, used
BIC score, gives us better approximation marginal likelihood. section,
give example illustrate use effective dimension sometimes also leads
better model selection. also motivate introduce concept regularity
used subsequent sections.
3.1 Example Model Selection
Consider two HLC models shown Figure 1. one experiment, instantiated
parameters M1 random fashion sampled set D1 10,000 data records
observed variables. ran SHC HSHC data set D1 guidance
BIC score. algorithms produced model M2 . following, explain why,
based D1 , one would prefer M2 M1 BIC used model selection M1
would preferred BICe used instead. argue M1 preferred based
D1 hence BICe better scoring metric case.
4

fiEffective Dimensions HLC Models

X1

X2

X2
Y1

Y2

X3
Y3

Y4

Y1

Y5

Y2

Y6

M1

Y3

X3

Y4

Y5

Y6

M2

Figure 1: Two HLC models. shaded variables latent, variables
observed. cardinality X1 2, cardinalities variables
3.

BIC BICe scores model given data set defined follows:
ds(M )
BIC(M |D) = logP (D|M, ~ )
logN,
2
de(M )
BICe(M |D) = logP (D|M, ~ )
logN
2
~ maximum likelihood estimate parameters based N
sample size.
example, notice M2 includes M1 sense M2 represent
probability distributions observed variables M1 can. fact, make
conditional probability distributions observed variables M2 M1
set PM2 (X2 ) PM2 (X3 |X2 )
PM2 (X2 )PM2 (X3 |X2 ) =

X

PM1 (X1 )PM1 (X2 |X1 )PM1 (X3 |X1 ),

X1

probability distribution observed variables two models identical.
M2 includes M1 , logP (D1 |M1 , ~1 ) logP (D1 |M2 , ~2 ). Together
fact D1 sampled M1 , implies logP (D1 |M1 , ~1 ) logP (D1 |M2 , ~2 )
sufficiently large enough sample size. standard dimension M1 45,
M2 44. Hence
BIC(M1 |D1 ) < BIC(M2 |D1 ).
hand, effective dimensions M1 M2 43 44 respectively. Hence
BICe(M1 |D1 ) > BICe(M2 |D1 ).
Model M2 includes M1 . opposite clearly true effective dimension
M1 smaller M2 . So, M2 reality complex model M1 .
model fit data D1 equally well. Hence simpler one, i.e. M1 , preferred
other. agrees choice BICe score, disagrees choice
BIC score. Hence, BICe appropriate BIC case.
5

fiZhang & Kocka

3.2 Regularity
consider another model M1 M1 except cardinality X1
increased 2 3. easy show M2 includes M1 vice versa. So, two
models equivalent terms capabilities representing probability distributions
observed variables. hence said marginally equivalent. However, M1
standard parameters M2 hence would always prefer M2 M1 .
formalize consideration, introduce concept regularity.
latent variable Z HLC model, enumerate neighbors (parent children)
X1 , X2 , . . . , Xk . HLC model regular latent variable Z,
|Z|

Qk

i=1 |Xi |
,
maxki=1 |Xi |

(1)

strict inequality holds Z two neighbors least one
latent node. Models M1 M2 regular, model M1 not.
irregular model always exists regular model marginally equivalent fewer standard parameters (Zhang, 2003b). regular model
obtained follows: latent node two neighbors
cardinality smaller one neighbors, remove latent node
connect two neighbors. latent node two neighbors
violates (1), reduce cardinality quantity right hand side. Repeat
steps changes made.
also interesting note collection regular HLC models given set
observed variables finite (Zhang, 2002). provides finite search space task
learning regular HLC models.1 rest paper, consider regular
HLC models.
ending subsection, point nice property effective model dimension
relation model inclusion. HLC model includes another model, effective
dimension less latter. consequence, two marginally equivalent
models effective dimensions hence BICe score.
true standard model dimension BIC score.
3.3 CS CSe Scores
argued empirical grounds BIC score reasonable scoring function
use learning HLC models BICe score sometimes improve model
selection. two scores free problems. One problem derivation
Laplace approximations marginal likelihood valid boundary
parameter space. CS score way alleviates problem. involves BIC score
based completed data BIC score based original data. words,
involves two Laplace approximations marginal likelihood. lets errors two
approximation cancel other.
Chickering Heckerman (1997) empirically found CS score quite accurate
approximation marginal likelihood robust boundary parameter
1. definition regularity given paper slightly different one given Zhang (2002).
Nonetheless, two conclusions mentioned paragraph remain true.

6

fiEffective Dimensions HLC Models

X

Z

X









Z

Z

X


M2

M1

Figure 2: Problem reduction.
space. realized need effective model dimension CS score, although
actually use it. would made differences experiments
because, models used, standard effective dimensions agree.
use CSe refer scoring function one obtains replacing standard model
dimension CS score effective model dimensions. BICe better
BIC approximations marginal likelihood (Geiger et al., 1996), CSe better
CS. compute CSe, also need calculate effective dimensions.

4. Effective Dimensions HLC Models
seen, effective model dimension interesting number reasons.
main result paper theorem effective dimension de(M ) regular
HLC model contains one latent variable. Let X root ,
latent node. least two latent nodes, must exist another latent
node Z child X. following, use terms X-branch Z-branch
respectively refer sets nodes separated Z X X Z.
Let set observed variables Z-branch let set
observed variables. Note X-branch doesnt contain node X. relationship
among X, Z, Y, depicted left-most picture Figure 2.
standard parameterization includes parameters P (X) parameters
P (Z|X). convenience, replace parameters parameters P (X, Z).
mentioned end Section 2, reparameterization affect effective
dimension de(M ). reflect reparameterization, edge X Z
directed Figure 2.
(0)

(0)

(0)

Suppose P (X, Z) k0 parameters 1 , 2 , . . . , k0 . Suppose conditional distri(1)

(1)

(1)

butions variables X-branch consists k1 parameters 1 , 2 , . . . , k1
(2)

(2)

conditional distributions variables Z-branch consists k2 parameters 1 , 2 ,
(2)
. . . , k2 . convenience sometimes refer three groups parameters using
three vectors ~(0) , ~(1) ~(2) respectively.
following, define two HLC models M1 M2 starting
establish relationship effective dimensions effective dimension
. context, , M1 , M2 regarded purely Mathematical objects.
semantics variables concern. particular, variable H latent
7

fiZhang & Kocka

X1
X1 (6)
X2
X2 (3)

Y3 (3)

X4 (5)

X3 (3)

Y3

X3

X2
X4

X5 (5)

X3
X1

X1

X4
Y1 (3)

Y2 (2)

Y4 (2)

X5
X5

Y5 (6)
Y1

Y2

X2

X3

Y4

Y5

Figure 3: picture left shows HLC model five observed five latent
variables, variable annotated name cardinality. picture
right shows components decompose HLC model
applying Theorem 1. Latent variables shaded, observed variables
not.

might designated observed M1 M2 part definition
Mathematical objects.
obtain Bayesian network model B1 deleting Z-branch. Strictly
speaking B1 Bayesian network due parameterization inherits : instead
probability tables P (X) P (Z|X), table P (X, Z). P (X) P (Z|X)
readily obtained P (X, Z). mind, view B1 Bayesian network.
network obviously tree-structured. leaf variables include set
variable Z. define M1 HLC model share structure B1
variable Z variables observed. parameters M1
~(0) ~(1) .
Similarly let B2 Bayesian network model obtained deleting Xbranch. tree-structure leaf variables include variable X.
define M2 HLC model share structure B2
variable X variables observed. parameters M2 ~(0) ~(2) .
Theorem 1 Suppose regular HLC model contains two latent nodes.
two HLC models M1 M2 defined text also regular. Moreover,
de(M ) = de(M1 )+de(M2 )[ds(M1 )+ds(M2 )ds(M )].

(2)

words, effective dimension equals sum effective dimensions M1
M2 minus number common parameters M1 M2 share.
appreciate significance theorem, consider task computing effective dimension regular HLC model contains two latent nodes.
8

fiEffective Dimensions HLC Models

repeatedly applying theorem, reduce task subtasks calculating effective dimensions LC models. example, consider HLC model depicted
picture left Figure 3. Theorem 1 allows us to, purpose computing
effective dimension, decompose HLC model five LC models, shown
right Figure 3.
might one compute effective dimension LC model? One way use
algorithm suggested Geiger et al. (1996). algorithm first symbolically computes
Jacobian matrix, possible due Assumption 1. randomly assigns
values parameters, resulting numerical matrix. rank numerical matrix
computed diagonalization. rank Jacobian matrix equals effective
dimension LC model almost everywhere, get regular rank probability
one. algorithm recently implemented Rusakov Geiger (2003). Kocka
Zhang (2002) suggest alternative algorithm computes upper bound.
algorithm fast empirically shown produce extremely tight bounds.
Going back example, effective dimension LC models X1 , X2 , X3 , X4
X5 26, 23, 23, 34 17 respectively. Thus effective dimension HLC model
Figure 3 26+23+34+23+17(531)(361)(631)(351) = 61. contrast,
standard dimension model 5+62+62+62+34+55+5+34+52+5 = 110.

5. Proof Main Result
section devoted proof Theorem 1. begin properties
Jacobian matrices Bayesian network models.
5.1 Properties Jacobian Matrices
Consider Jacobian matrix JM Bayesian network model . matrix parameterized parameters ~ . Let v1 , v2 , . . . , vm column vectors JM .
Lemma 1 number column vectors v1 , v2 , . . . , vm Jacobian matrix JM
either linearly dependent everywhere linearly independent almost everywhere.
linearly dependent everywhere exists least one column vector vj
expressed linear combination column vectors everywhere.
Proof: Consider diagonalizing following transposed matrix:
[v1 , v2 , . . . , vm ]T .
~ Hence would
According Assumption 1, elements matrix polynomials (of ).
multiply rows polynomials fraction polynomials. course, need also add
one row another row. end process, get diagonal matrix whose nonzero
elements polynomials fractions polynomials. Suppose k nonzero rows
suppose correspond v1 , v2 , . . . , vk .
elements diagonalized matrix polynomials fractions polynomials,
~
well-defined 2 nonzero almost everywhere (i.e. almost values ).
k=m, vectors linearly independent almost everywhere.
2. fraction well defined denominator zero.

9

fiZhang & Kocka

k<m, exist, j (k<jm), polynomials fractions polynomials ci
(1ik)
vj =

k
X

ci vi .

(3)

i=1

coefficients ci determined tracing diagonalization process. vj
expressed linear combination {vi |i = 1, . . . , k} everywhere 3 . 2
Although might sound trivial, lemma actually quite interesting.
JM parameterized matrix. first part, example, implies exist
two subspaces parameter space nonzero measures
vectors linearly independent one subspace linearly dependent other.
total number column vectors JM , get following lemma:
Lemma 2 Jacobian matrix JM , exists collection column vectors form
basis column space almost everywhere. number vectors collection
equals regular rank matrix. Moreover, collection chosen include
given set column vectors linearly independent almost everywhere.
Proof: first part already proved. second part follows definition
regular rank. last part true could start diagonalization process
transpose vectors set top matrix. 2
5.2 Proof Theorem 1
set prove Theorem 1. straightforward verify HLC models
M1 M2 regular. suffices prove equation (2). rest
section.
set observed variables Y, set observed variables M1
{Z} set observed variables M2 {X}. Hence Jacobian matrices
models , M1 , M2 respectively written follows:
JM

= [

JM1

= [

JM2

= [

P (O, Y)

,...,

P (O, Y) P (O, Y)
P (O, Y) P (O, Y)
P (O, Y)
;
;
]
,...,
,...,
(0)
(1)
(1)
(2)
(2)
k0
1
k1
1
k2

,...,

P (O, Z) P (O, Z)
P (O, Z)
;
]
,...,
(0)
(1)
(1)
k0
1
k1

(0)
1

P (O, Z)
(0)
1

P (X, Y)
(0)
1

,...,

P (X, Y) P (X, Y)
P (X, Y)
;
]
,...,
(0)
(2)
(2)
k0
1
k2

~ ci might undefined
3. subtle point here. fractions polynomials ,
~ equation (3) alone, cannot conclude vj linearly depends {vi |i = 1, . . . , k}
values .
everywhere.
conclusion nonetheless true two reasons. First set ~ values ci
undefined measure zero. Second, vj linearly depend {vi |i = 1, . . . , k} one value
~ would true sufficiently small nonetheless measure-positive ball around
,
value.

10

fiEffective Dimensions HLC Models

clear one-to-one correspondence first k0 +k1 column vectors
JM column vectors JM1 one-to-one correspondence
first k0 last k2 column vectors JM column vectors JM2 .
first show
Claim 1: first k0 vectors JM (JM1 JM2 ) linearly independent
almost everywhere.
Together Lemma 2, Claim 1 implies collection column vectors
JM1 includes first k0 vectors basis column space JM1 almost
everywhere. particular, implies de(M1 )k0 . Suppose de(M1 )=k0 +r. Without
loss generality, suppose basis vectors
P (O, Z)
(0)
1

,...,

P (O, Z) P (O, Z)
P (O, Z)
;
.
,...,
(0)
(1)
(1)
k0
1
r

(4)

symmetry, assume de(M2 )=k0 +s s0 following column
vectors form basis JM2 almost everywhere:
P (X, Y)
(0)
1

,...,

P (X, Y) P (X, Y)
P (X, Y)
.
;
,...,
(0)
(2)
(2)
k0
1


(5)

consider following list vectors JM :
P (O, Y)
(0)
1

,...,

P (O, Y) P (O, Y)
P (O, Y) P (O, Y)
P (O, Y)
;
;
.
,...,
,...,
(0)
(1)
(1)
(2)
(2)
k0
1
r
1


(6)

show
Claim 2: column vectors JM linearly depend vectors listed (6)
everywhere.
Claim 3: vectors listed (6) linearly independent almost everywhere.
two claims imply vectors listed (6) form basis column space
JM almost everywhere. Therefore
de(M ) = k0 +r+s = de(M1 )+de(M2 )k0 .
clear k0 =ds(M1 )+ds(M2 )ds(M ). Therefore Theorem 1 proved. 2
5.3 Proof Claim 1
Lemma 3 Let Z latent node HLC model set observed
nodes subtree rooted Z. regular, set conditional distributions
nodes subtree way encode injective mapping Z
sense P (Y=(z)|Z=z) = 1 z Z .
11

fiZhang & Kocka

Proof: prove lemma induction number latent nodes subtree
rooted Z. First consider case one latent node, namely Z.
case, Z parent nodes Y. Enumerate nodes Y1 , Y2 , . . . , Yk .
Q
regular, |Z| ki=1 |Yi |. Hence define injective mapping
Q
Z = ki=1 Yi . state z Z, (z) written = (y1 , y2 , . . . , yk ),
yi state Yi . set
P (Yi =yi |Z=z) = 1,
P (Y=(z)|Z=z) = 1.
consider case least two hidden nodes subtree rooted
Z. Let W one latent node latent node descendants. Let (1)
set observed nodes subtree rooted W Y(2) =Y\Y(1) . induction
hypothesis, parameterize subtree rooted W way encodes
injective mapping W Y(1) . Moreover, nodes W removed ,
remains regular HLC model. model, parameterize subtree rooted
Z way encodes injective mapping Z (W,Y(2) ) = W Y(2) .
Together, two facts prove lemma. 2
Corollary 1 Let Z latent node HLC model . Suppose Z latent neighbor
X. Let set observed nodes separated X Z. regular,
set probability distributions nodes separated X Z way
encode injective mapping Z sense P (Y=(z)|Z=z) = 1
z Z .
Proof: corollary follows readily Lemma 3 property root-walking
operation (Zhang, 2002). 2
Proof Claim 1: Consider following matrix
[
(0)

(0)

P (X, Z)
(0)
1

...,

P (X, Z)
(0)

k0

]

(7)

(0)

1 , 2 , . . . , k0 parameters joint distribution P (X, Z), matrix
identity matrix rows properly arranged. column vectors linearly
independent almost everywhere.
(0)
(0)
consider first k0 column vectors JM : P (O, Y)/1 , . . . , P (O, Y)/k0 .
must linearly independent almost everywhere. not, one vectors, say
(0)
P (O, Y)/k0 , would linearly depend rest everywhere according Lemma 1.
Observe (1ik0 ),
P (O, Y)
(0)


=

X

P (O|X)P (Y|Z)

X,Z

P (X, Z)
(0)



.
(0)

Choose P (O|X) P (Y|Z) Corollary 1. vector P (O, Y)/i might contain zero elements. remove zero elements, remains vector iden(0)
(0)
tical P (X, Z)/i . conclude P (X, Z)/k0 linearly depends
12

fiEffective Dimensions HLC Models

(0)

(0)

P (X, Z)/1 . . . , P (X, Z)/k0 1 everywhere, contradicts conclusion
previous paragraph. Hence first k0 vectors JM must linearly independent almost
everywhere.
evident that, using similar arguments, also show first k0 vectors
JM1 (JM2 ) linearly independent almost everywhere. Claim 1 therefore proved. 2
5.4 Proof Claim 2
Every column vector JM1 linearly depends vectors listed (4) everywhere. Observe

P (O, Y)
(0)


X

P (Y|Z)

Z

P (O, Y)
(1)


=
=

X

P (Y|Z)

Z

P (O, Z)
(0)

, = 1, . . . , k0

(1)

, = 1, . . . , k1 .


P (O, Z)


Therefore every column vector JM corresponds vectors JM1 linearly depends
first k0 +r vectors listed (6) everywhere.
symmetry, every column vector JM corresponds vectors JM2 linearly
depends first k0 last vectors listed (6) everywhere. claim proved.
2
5.5 Proof Claim 3
prove claim contradiction. Assume vectors listed (6) linearly
independent almost everywhere. According Lemma 1, one them, say v, must linearly
depend rest everywhere. Claim 1 Lemma 2, assume v
(2)
among last r+s vectors. Without loss generality, assume v P (O, Y)/s .
~ exist real numbers ci (1ik0 ), c(1) (1ir), c(2)
value ,


(1is1)
P (O, Y)
(2)


=

k0
X
i=1

ci

P (O, Y)
(0)


+

r
X

(1) P (O, Y)
(1)

i=1

ci

+

s1
X

(2) P (O, Y)
.
(2)

i=1

ci

Note last term right hand side, runs 1 s1.
parameter vector ~ consists three subvectors ~(0) , ~(1) ~(2) . Set parameters
(1)
~ (for X-branch) Lemma 3. exists injective mapping X

P (O=(x)|X=x) = 1 x X .

(8)

vectors (6), consider subvector consisting elements
states images states X mapping . subvectors
(0)
(1)
(2)
denoted P (OX , Y)/i , P (OX , Y)/i , P (OX , Y)/i .
values ~(0) ~(2) , still
13

fiZhang & Kocka

P (OX , Y)

=

(2)


k0
X

ci

P (OX , Y)

i=1

(0)


+

r
X

(1) P (OX , Y)
(1)

i=1

ci

+

s1
X

(2) P (OX , Y)
.
(2)

i=1

ci

(9)

Consider first two terms right hand side:
k0
X

ci

r
X

(1) P (OX , Y)
(1)

i=1
k0
r
X
X
P (OX , Z)
P (OX , Z) X
(1) X
c
ci
+
P (Y|Z)
P (Y|Z)

(0)
(1)


i=1
i=1
Z
Z
k0
r
X
X
P (OX , Z) X
(1) P (OX , Z)
ci
+
}
P (Y|Z){ ci
(0)
(1)


i=1
i=1
Z

P (OX , Y)
(0)


i=1

=
=

+

ci

P

(8) fact P (O, Z) =
X P (X, Z)P (O|X), column vector
(0)
(0)
P (OX , Z)/i identical vector P (X, Z)/i . argued proving
(0)
Claim 1, vectors {P (X, Z)/i |i=1, . . . , k0 } constitute basis k0 -dimensional
(1)
Euclidian space. implies that, vectors P (OX , Z)/i represented
(0)
linear combination vectors {P (OX , Z)/i |i = 1, . . . , k0 }. Consequently,
exist ci (1ik0 )
k0
X

ci

P (OX , Z)

i=1

(0)


k0
X

P (OX , Y)

+

r
X

(1) P (OX , Z)
(1)

i=1

ci

=

k0
X

ci

P (OX , Z)

ci

P (OX , Y)

i=1

(0)



Hence

i=1

ci

+

(0)


r
X

(1) P (OX , Y)
(1)

i=1

ci

=

k0
X
i=1

(0)



Combining equation equation (9), get
P (OX , Y)
(2)


=

k0
X

ci

P (OX , Y)

i=1

(0)


+

s1
X

(2) P (OX , Y)
.
(2)

i=1


ci

P

(8) fact fact P (O, Y) = X P (X, Y)P (O|X), column
(1)
(1)
vector P (OX , Y)/i identical vector P (X, Y)/i column vector
(2)
(2)
P (OX , Y)/i identical vector P (X, Y)/i . Hence
P (X, Y)
(2)


=

k0
X
i=1

ci

P (X, Y)
(0)


+

s1
X

(2) P (X, Y)
.
(2)

i=1

ci

contradicts fact vectors equation form basis column space
JM2 almost everywhere (see (5) Section 5.2) Therefore, Claim 3 must true. 2
14

fiEffective Dimensions HLC Models

6. Effective Dimensions Trees
Let us use term tree model refer Markov random fields undirected trees
finite number random variables. root tree model nodes, get
tree-structured Bayesian network model. tree model, define leaf nodes
one neighbor. HLC model tree model leaf nodes observed
others latent.
turns Theorem 1 enables us compute effective dimension tree
model. Consider arbitrary tree model. leaf nodes latent, remove
nodes without affecting effective dimension.
removing latent leaf nodes, leaf nodes observed. non-leaf nodes
also observed, decompose model submodels observed non-leaf
node. following theorem tells us model submodels related terms
effective dimensions.
Theorem 2 Suppose observed non-leaf node tree model . decomposes
k submodels M1 , . . . , Mk ,
de(M ) =

k
X

de(Mi ) (k 1)(|Y | 1).

i=1

possible decompositions, final submodels either contain latent nodes
HLC models. Effective dimensions submodels latent variables simply
standard dimensions. HLC submodel irregular, make regular applying
transformation mentioned end Section 3.2. transformation affect
effective dimensions submodels. Finally, effective dimensions regular HLC
submodels computed using Theorem 1.
Proof Theorem 2: possible prove theorem starting Jacobian
matrix. take less formal revealing approach.
suffices consider case k 2. two submodels M1 M2 share one
node, namely . Let O1 O2 respectively sets observed nodes two
submodels excluding . Root .
P (Y, O1 , O2 )P (Y ) = P (O1 , )P (O2 , ).
Let ~0 set parameters distribution P (Y ), ~1 ~2 respectively sets
parameters conditional probability distributions nodes M1 M2 . Consider
fixing ~0 letting ~1 ~2 vary. case, space spanned P (Y ) consists
one vector, namely ~0 itself. Moreover, one-to-one correspondence vectors
space spanned P (Y, O1 , O2 ) vectors Cartesian product spaces
spanned P (O1 , ) P (O2 , ). let ~0 vary. adds |Y |1 dimensions
four spaces spanned P (Y, O1 , O2 ), P (Y ), P (O1 , ), P (O2 , ). Consequently,

de(M ) = de(M1 ) + de(M2 ) (|Y | 1).
theorem proved. 2
15

fiZhang & Kocka

7. Concluding Remarks
paper study effective dimensions HLC models. work motivated
empirical evidence BIC behaves quite well used several hill-climbing
algorithms learning HLC models BICe score sometimes leads better
model selection BIC score. proved theorem relates effective
dimension HLC model effective dimensions two HLC models
contain fewer latent variables. Repeated application theorem allows one reduce
task computing effective dimension HLC model subtasks computing
effective dimensions LC models. makes computationally feasible compute
effective dimensions large HLC models. addition, proved theorem
effective dimensions general tree models. main theorem allows one
compute effective dimension arbitrary tree models.
Acknowledgements
work initiated authors visiting Department Computer Science,
Aalborg University, Denmark. thank Poul S. Eriksen, Finn V. Jensen, Jiri Vomlel,
Marta Vomlelova, Thomas D. Nielsen, Olav Bangso, Jose Pena, Kristian G. Olesen.
also grateful annonymous reviewers whose comments helped us greatly
improving paper. Research paper partially supported GA CR grant
201/02/1269 Hong Kong Research Grant Council grant HKUST6088/01E.

References
Akaike, H. (1974). new look statistical model identification. IEEE Trans. Autom.
Contr., 19, 716-723.
Bartholomew, D. J. Knott, M. (1999). Latent variable models factor analysis, 2nd
edition. Kendalls Library Statistics 7. London: Arnold.
Cheeseman, P. Stutz, J. (1995). Bayesian classification (AutoClass): Theory
results. Fayyad, U., Piatesky-Shaoiro, G., Smyth, P., Uthurusamy, R. (eds.),
Advancesin Knowledge Discovery Data Mining, AAAI Press, Menlo Park, CA.
Chickering D. M. Heckerman D. (1997). Efficient Approximations Marginal
Likelihood Bayesian Networks Hidden variables. Machine Learning, 29, 181212.
Cowell, R. G., Dawid, A. P., Lauritzen, S. L., Spiegelhalter, D. J. (1999). Probabilistic
networks expert systems, Springer.
Kocka, T. Zhang, N. L. (2002). Dimension correction hierarchical latent class
models. Proc. 18th Conference Uncertainty Artificial Intelligence
(UAI-02).
Geiger D., Heckerman D. Meek C. (1996). Asymptotic model selection directed
networks hidden variables. Proc. 12th Conference Uncertainty
Artificial Intelligence, 283-290.
16

fiEffective Dimensions HLC Models

Goodman, L. A. (1974). Exploratory latent structure analysis using identifiable
unidentifiable models. Biometrika, 61, 215-231.
Lazarsfeld, P. F., Henry, N.W. (1968).
Mifflin.
Rusakov, D. Geiger, D. (2002).
networks. UAI-02.

Latent structure analysis. Boston: Houghton

Asymptotic model selection Naive Bayesian

Rusakov, D. Geiger, D. (2003). Automated analytic asymptotic evaluation marginal
likelihood latent models. UAI-03.
Schwarz G. (1978). Estimating dimension model. Annals Statistics, 6, 461-464.
Settimi, R. Smith, J.Q. (1998). geometry Bayesian graphical models
hidden variables. Proceedings Fourteenth Conference Uncertainty
Artificial Intelligence, Morgan Kaufmann Publishers, S. Francisco, CA, 472-479.
Settimi, R. Smith, J.Q. (1999). Geometry, moments Bayesian networks hidden
variables. Proceedings Seventh International Workshop Artificial Intelligence Statistics, Fort Lauderdale, Florida (3-6 January 1999), Morgan Kaufmann
Publishers, S. Francisco, CA.
Zhang N. L. (2002). Hierarchical latent class models cluster analysis. AAAI-02, 230-237.
Zhang, N. L., Kocka, T., Karciauskas, G., Jensen, F. V. (2003). Learning hierarchical
latent class models. Technical Report HKUST-CS03-01, Department Computer
Science, Hong Kong University Science Technology.
Zhang, N. L. (2003).
Structural EM Hierarchical Latent Class Models. Technical
Report HKUST-CS03-06, Department Computer Science, Hong Kong University
Science Technology.
Zhang, N. L. (2003b). Hierarchical latent class models cluster analysis. Journal
Machine Learning Research, appear.

17

fiJournal Arti cial Intelligence Research 21 (2004) 631-670

Submitted 9/03 published 6/04

PHA*: Finding Shortest Path A* Unknown
Physical Environment
Ariel Felner

Department Information Systems Engineering,
Ben-Gurion University Negev, Beer-Sheva, 85104, Israel

Roni Stern
Asaph Ben-Yair
Sarit Kraus
Nathan Netanyahu

Department Computer Science, Bar-Ilan University
Ramat-Gan, Israel, 52900

felner@bgumail.bgu.ac.il
sternr2@cs.biu.ac.il
benyaya@cs.biu.ac.il
sarit@cs.biu.ac.il
nathan@cs.biu.ac.il

Abstract
address problem nding shortest path two points unknown
real physical environment, traveling agent must move around environment
explore unknown territory. introduce Physical-A* algorithm (PHA*) solving
problem. PHA* expands mandatory nodes A* would expand returns
shortest path two points. However, due physical nature
problem, complexity algorithm measured traveling eort moving
agent number generated nodes, standard A*. PHA* presented
two-level algorithm, high level, A*, chooses next node expanded
low level directs agent node order explore it. present
number variations high-level low-level procedures evaluate
performance theoretically experimentally. show travel cost best
variation fairly close optimal travel cost, assuming mandatory nodes
A* known advance. generalize algorithm multi-agent case,
number cooperative agents designed solve problem. Speci cally, provide
experimental implementation system. noted problem
addressed navigation problem, rather problem nding shortest
path two points future usage.

1. Introduction
paper address problem nding shortest path two points
unknown real physical environment, mobile agent must travel around
environment explore unknown territories. Search spaces path- nding problems
commonly represented graphs, states associated search space
represented graph nodes, transition states captured graph edges.
Graphs represent dierent environments, road maps, games, communication
networks. Moving one node graph another done applying logical
operators manipulate current state actual agent move one
node another. sliding-tile puzzle Rubik's Cube (Korf, 1999) examples
c 2004 AI Access Foundation. rights reserved.

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

rst type, road map example second type. Graphs search problems
divided following three classes:

Fully known graphs: nodes edges graph stored com-

puter, graph fully known. input problems usually
complete graph represented adjacency matrix adjacency list.
relevant problem case would nd, example, shortest path
road map nodes edges known advance.

large graphs: Graphs due storage time limitations completely known cannot fully stored storage device. Many graphs
search problems exponential number nodes. example, 24-tile puzzle problem 1025 states cannot completely stored current machines.
input problems usually speci ed general structure state
search space, dierent operators, initial state, set goal states.
small portions graphs visited search algorithms
stored memory.

Small, partially known graphs: third class contains graphs represent
partially known physical environment. example, mobile agent unknown
area without map full knowledge environment. Given enough
time, however, agent fully explore environment since large.
Due partial knowledge, small portion graph given input.

class fully-known graphs, classical algorithms, Dijkstra's single-source
shortest-path algorithm (Dijkstra, 1959) Bellman-Ford algorithm (Bellman, 1958),
used nd optimal path two nodes. algorithms assume
node graph accessed algorithm constant time. assumption
valid since nodes edges graph known advance stored
computer's memory. Thus time complexity algorithms measured
number nodes edges process course search.
second class graphs algorithms usually ecient, since
number nodes graph large (usually exponential). Also, small
portion graph stored memory given time. A* algorithm (Hart, Nilsson, & Raphael, 1968) linear space versions, e.g., IDA* (Korf, 1985) RBFS (Korf,
1993), common methods nding shortest paths large graphs. A* keeps
open list nodes generated yet expanded, chooses
promising node (the best node) expansion. node expanded moved
open list closed list, neighbors generated put open
list. search terminates goal node chosen expansion open
list empty. cost function A* f (n) = g(n) + h(n) g(n) distance
traveled initial state n, h(n) heuristic estimate cost node
n goal. h(n) never overestimates actual cost node n goal, say
h(n) admissible. using admissible heuristic h(n), A* proved
admissible, complete, optimally eective (Dechter & Pearl, 1985). words,
heuristic, A* guaranteed always return shortest path. Furthermore,
632

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

algorithm claiming return optimal path must expand least nodes
expanded A* given heuristic.
A* expansion cycle carried constant time. takes constant
amount time retrieve node open list, generate neighbors.
latter involves applying domain-speci c operators expanded node. Thus time
complexity A* also measured terms number generated nodes.1
paper deal nding shortest path graphs third class, i.e.,
small, partially known graphs correspond real physical environment. Unlike
graphs two classes, constant number computer operations done
node expansion, cannot assume, type graphs, visiting node takes
constant time. Many nodes edges graph known advance.
Therefore, expand node known advance, mobile agent must rst travel
node order explore learn neighbors. cost search
case cost moving agent physical environment, i.e., proportional
distance traveled agent. ecient algorithm would therefore minimize
distance traveled agent optimal path found. Note since small graphs
considered here, omit actual computation time focus travel
time agent. paper introduce Physical-A* algorithm (PHA*) solving
problem. PHA* expands mandatory nodes A* would expand returns
shortest path two points. However, complexity algorithm
measured traveling eort moving agent. order minimize traveling
shown, PHA* designed minimize traveling eort agent
intelligently choosing next assignment traveling agent. described below, many
times agent chooses rst move nearby nodes even though immediately
contribute proceeding A*.
Unlike ordinary navigation tasks (Cucka, Netanyahu, & Rosenfeld, 1996 Korf, 1990
Stentz, 1994 Shmoulian & Rimon, 1998), purpose agent reach goal
node soon possible, rather explore graph manner shortest
path retrieved future usage. hand, problem ordinary
exploration problem (Bender, Fernandez, Ron, Sahai, & Vadhan, 1998), entire
graph explored order mapped out. Following two motivating
examples real world applications problem:

Example 1: division troops ordered reach speci c location. coordinates location known. Navigating entire division unknown
hostile territory reaching destination unreasonable inecient. common case team scouts search best path division
pass through. scouts explore terrain report best path division
move along order reach destination ecient manner.

1. fact, A*, open list stored priority queue, would take logarithmic time
retrieve best node. However, many problems, sliding tile puzzles Rubik's Cube,
simple rst-in rst-out queue suces (Korf, 1993ff Taylor & Korf, 1993ff Korf, 1997). Likewise,
linear space versions, IDA* RBFS (which based depth-rst search), assumption
takes constant time per node valid. Also assume number neighbors
bounded.

633

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

Example 2: Computer systems connected networks on- o-line
dierent times, throughput seriously degraded due busy communication channels. Therefore, many networks cannot represented xed, fully
known graphs. Transferring large amounts data (e.g., multimedia les)
two computers network often time consuming, since data may
routed many communication channels computer systems reaching
destination. Finding optimal path computer systems could
improve transfer time large les. Since network may fully known,
nding optimal path two nodes requires exploration network.
ecient elegant solution might send small packets (operating scouts)
explore network return optimal path, given network stable
least short period time. Assuming computer system network
recognized neighboring systems, faced problem nding
optimal path real physical environment.2

general, would worthwhile search optimal path following
conditions hold:
Preliminary search (with usage scouts) possible cheap.
optimal path required future usage.
Often one might settle suboptimal path. However, path needed
considerable trac volume, e.g., path traveled large number times
path traveled simultaneously large number agents, nding
optimal path essential. paper focus solving problem.
paper organized follows. Section 2 provides speci c formulation
problem question. Section 3 discusses related work, Section 4 presents
PHA* algorithm single mobile agent. Several (enhanced) variations introduced
discussed domain, followed extensive empirical results demonstrate
superiority enhanced variants pursued. Section 5 provide analysis
PHA* overall evaluation performance. Section 6, provide number
generalizations multi-agent case, number traveling agents available
solving problem. Experimental results schemes presented discussed.
Section 7 contains concluding remarks discusses future research. preliminary version
paper appeared earlier (Felner, Stern, & Kraus, 2002).
2. research concerned high-level, abstract graphs intend provide new
applicable routing algorithm. Current routing technologies maintain large databases store best
paths node node, broadcast changes network, update paths necessary, thus
making essentially network graph fully known. Also, network domains one create
destroy packages thus necessarily given number agents. algorithm
may relevant future network architectures routing technologies, routers use
databases. far-fetched view, example, rapid growth Internet.
thus conceivable future storing paths would become infeasible.

634

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

2. Problem Specication
mentioned general terms, problem nd shortest path two
nodes unknown undirected graph. speci cally, assume weighted graph,
node represented 2-dimensional coordinate (i.e., location real
world), weight edge Euclidean distance two nodes.
input problem consists coordinates initial goal nodes.
nodes assumed known advance. agent assumed located
start node. task nd shortest path (unknown) graph
initial node goal node future usage. order accomplish that, agent
required traverse graph explore relevant parts leading desired solution.
agent allowed visit nodes travel one node another via existing edges.
assume node v visited search agent, neighboring
nodes discovered, well edges connecting v. assumption
unreasonable, considering, e.g., (trac) signs road intersection often indicate
neighboring destinations lengths corresponding road segments connect
locations. Even without road signs, scouts reach new location,
look around, observe neighboring locations, assess distances current
location. general, assumption neighboring nodes discovered instantly
fairly common search problems algorithms.3
Since goal search nd best path goal, clear { given
admissible heuristic { agent must expand nodes expanded A*, A*
optimally eective (Dechter & Pearl, 1985). Let C length shortest path
initial node goal node. A* expand nodes, that, f (n) =
g(n) + h(n) < C nodes f (n) = C . refer nodes
(set mandatory) A* nodes. stated above, agent must visit A* nodes
order nd shortest path. However, may need visit additional nodes.
make following fundamental observations respect problem question:
First, even set A* nodes known advance, agent may need visit
additional nodes traversing related portions graph.
shortest path two A* nodes may include graph nodes
belong A* nodes, i.e., f value greater C . Given A* nodes,
nding shortest path visits { confused
shortest path origin node goal node { could considered
solving traveling salesman problem (TSP) respect set A* nodes.
Note TSP solution may include nodes belong A* nodes.
Second, agent know A* nodes advance. nodes added
open list expanded search progresses. Thus agent cannot
use solution TSP, since TSP assumes nodes visited provided
input.
3. are, however, domains assumption may hold. domains, node becomes
fully known agent reaches physically. work restrict
assumption. domains addressed part future work.

635

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

cases, order A* nodes expanded dierent
order visited according TSP solution. Thus minimal
path traversing A* nodes cannot used.
Third, node added open list agent cannot know whether
belongs A* nodes, since C known search concluded. Consider
node n open list, head open list. Suppose
agent physically located near node. decide whether
slightly extend path visit node n skip n continue node
head open list. n turned belong A* nodes, visiting
may prove bene cial. (This n might reach head open
list agent physically located far away it, visiting n
point incur signi cant travel cost.) However, turns n
belong A* nodes, (small) detour visiting proven useless.
Intuitively, however, decision never visit would result bad strategy.
Thus agent may visit nodes belong A* nodes future
expected bene ts. actual decision whether visit n depend
distance agent's location n (at time decision)
agent's estimate whether n belongs set A* nodes.
following sections present PHA* algorithm ecient exploration
graph, order nd shortest path two given nodes single traveling
agent, well multiple agents. study dierent heuristics direct agent
make intelligent decision, attempt achieve small overall travel cost.

3. Related Work
Much research devoted guiding mobile agent exploring new unknown
environments order study map out. work dierent,
sense explores merely necessary regions graph order retrieve
shortest path two nodes entire graph. literature
area deals physical mobile robot moves real environment. published
research focuses usually issue assisting robot recognize physical objects
environment. refer reader (Bender et al., 1998), contains extensive
survey various related approaches state art techniques.
Another class algorithms navigation algorithms. navigation problem concerned
navigating mobile agent goal fast possible, necessarily via shortest
(optimal) path. navigator always proceed towards goal, ignoring whether
trail traversed thus far lies shortest path. Deviations optimal path
neglected since navigation problem reconsidered every move respect
new source node, i.e., current position agent. navigation algorithm halts
mobile agent reaches goal. path passed usually lacks importance usually
optimal. problem, hand, nd optimal path goal node
future usage. Even agent nds path goal node, search continues
636

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

shortest path goal found. Next, describe briey work done
navigation partially known graphs.
(Cucka et al., 1996) introduced navigation algorithms sensory-based environments automated robots moving room. used depth rst search
(DFS)-based navigation algorithms, use heuristic function choosing next node
agent go to.
Real-Time-A* (RTA*) (Korf, 1990) sophisticated version, Learning RealTime-A* (LRTA*), also algorithms nding paths two nodes graph.
However, deal large graphs assume constraint time
computation move retrieve given constant time. Thus limited
search performed, node best cost search frontier picked.
problem solver moves one step along path node. search continues
new state problem solver. merit node n (in RTA* LRTA*)
f (n) = g(n) + h(n), similarly A*. Unlike A*, though, g(n) actual distance node
n current state problem solver, rather original initial state.
dierence RTA* LRTA* search terminated, LRTA*
also stores heuristic estimation value node visited problem solver. Also
method successor nodes chosen dierent two variations. Korf (Korf,
1990) proves large number runs, run start node selected
random, stored value node visited LRTA* problem solver converges
optimal distance goal. RTA* LRTA* signi cantly dierent
approach, assume node expanded computer's memory
without agent physically visit node. (Also, algorithms designed
large graphs.) Furthermore, RTA* nd optimal path goal. trivial
version LRTA* could used solve problem, e.g., limiting search depth
one level, every node visited agent could physically expanded. However,
variant competitive approach, perform like simple
hill-climbing procedure. addition, order attain optimal path, LRTA*
select many start nodes random. relevant case, given
one initial node.
MARTA* (Knight, 1993) multi-agent version RTA*. MARTA* every agent runs
RTA* independently. Kitamura et al. (Kitamura, Teranishi, & Tatsumi, 1996) modi ed MARTA* using coordination strategies based attraction repulsion.
strategies employed tie-breaking situations. using repulsion strategy,
idea spread agents, agent intends maximize distance
others. Again, path provided algorithm optimal also, agents
need physically visit node order expand it. work inspired algorithms
presented paper, far handling multi-agent mutual decision concerned.
Life-long planing A* (LPA*) (Koenig & Likhachev, 2002b) remarkable algorithm
generalizes A* handle dynamically changing graph. LPA* activated every time
graph changed order nd current shortest path given start
goal nodes. utilizes fact much old data explored previous runs
LPA* still valid current run. A* special case LPA* entire graph
explored yet.
637

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

D*-lite (Koenig & Likhachev, 2002a) applies LPA* case mobile robot needs
nd shortest path unknown environment environment changes
dynamically (i.e., edges added deleted times). LPA*, start
node identical runs. D*-lite, however, robot moves along path
calculates new shortest path current location. D*-lite modi es LPA*
old data previous runs eciently used case start node also
changed according new location robot. D*-Lite actually simpli ed version
previous algorithm D* Stenz (Stentz, 1994).
main dierence algorithms approach they, too, expand
node computer's memory without requiring mobile agent physically visit
node. Indeed, following every move robot D* Lite, changes graph
provided immediately robot need physically visit nodes order gather
rsthand information. task agent, context D*, repeatedly
determine shortest path current location robot goal location
edge costs graph changes robot moves. D* lite nd path
returns it. simply navigation algorithm guides agent goal node
based previous new information terrain.
agent operating real world must often choose maximizing expected
utility (according current knowledge \world") learning
environment, attempt improve future gains. problem known tradeo exploitation exploration reinforcement learning (Kaelbling & Moore,
1996). Argamon et al. (Argamon-Engelson, Kraus, & Sina, 1998, 1999) address tradeo exploration exploitation agent moves repeatedly two
locations. propose utility-based on-line exploration algorithm takes
account cost attempting improve currently best route known
estimate potential bene ts future task repetitions. expected utility
exploration positive, agent takes actions improve route otherwise,
continues using known path. authors compare utility-based on-line exploration
heuristic backtracking search algorithm exhaustively searches graph
starting perform task, randomized interleaved exploration algorithm.
assume agent knows path two nodes, make
assumption.
Argamon et al. also suggest larger number times task repeated,
merit interleaved exploration diminishes. agent required move
back forth two nodes large number times, need decide
on-line whether exploit explore instead, shortest path found soon
possible. Thus good search algorithm may prove useful. respect work
complements Argamon et al., provides ecient search algorithms situations
optimal path needed advance. contrast, applying techniques Argamon et
al. situations yields poor results demonstrated experiments.
Roadmap-A* (Shmoulian & Rimon, 1998) sophisticated single agent navigation
algorithm. chooses navigate node assumed close goal node.
algorithm supervised high-level procedure called A" (Pearl & Kim, 1982). Instead
always selecting best node open list, A" allows search agent choose
set \good nodes". set called focal set. focal set nodes
638

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

open list whose f value greater value best node ".
focal nodes determined, local search performed navigate agent
one nodes, believed close goal node. role high-level
phase prevent navigating agent going wrong direction considering
also path traveled thus far.
Roadmap-A*, " pre-speci ed constant, determines trade-o
local search A*. example, A0 A* A1 local search, choosing
iteration node believed close goal node. algorithm
halts goal node reached, thus " > 0 optimal path might
known. paradigm Roadmap-A* similar ours, sense node known
agent explores it. fact, trivial case " = 0, Roadmap-A*
similar approach simple heuristic \shortest-known path" (presented
Subsection 4.1 below). comments basic dierence RoadmapA*
PHA* provided Section 5.
summary, listed algorithms navigation algorithms, i.e.,
necessarily require agent physically visit node order expand it,
necessarily return optimal path goal node. Thus inherently solve dierent
problem one pursued paper.

4. PHA* Single Agent

turn description PHA* algorithm, focusing rst case
single mobile agent available.
Nodes environment divided explored unexplored nodes. Exploring
node means physically visiting node agent, learning location
location neighbors. new algorithm PHA* activates essentially A*
environment. However, order expand node A*, node must rst explored
agent order obtain relevant data associated (i.e., neighboring nodes
incident edges). Throughout discussion paper treat PHA* twolevel algorithm. Although principle PHA* could also viewed one-level algorithm
(see discussion Subsection 4.2), nd two-level presentation
well-structured better understood conceptually. two-level framework consists
high-level low-level routine. high level (which invokes low level various
stages PHA*), acts essentially like regular A* search algorithm. chooses
cycle node open list expansion. heuristic function h(n) used
Euclidean distance n goal node. (This heuristic admissible course,
de nition.) node chosen high level explored agent,
low level, navigation algorithm, activated navigate agent node
explore it. node explored low level expandable high
level. chosen node already explored, neighbors already known,
readily expandable high level without need send agent visit
node. pseudo-code high level given below.

639

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

.
.
.
.
.
.
.

g

high-level(open-list) f
(open-list empty) f
target = best node open-list
target unexplored
f
explore(target) low level







g

g

expand(target)

4.1 Low-Level Algorithms

high-level algorithm, A*, chooses expand node smallest f value
open list, regardless whether agent already visited node. chosen
node visited agent, low level instructs agent visit node.
call node target node low level. order reach target node,
must use navigation algorithm. implemented number navigation variants
low level. rst describe simple algorithms use known information
graph. present ecient algorithms, also explore graph
navigation provide new information high level. assume
agent current node needs navigate target node.
4.1.1 Simple Navigation Algorithms

Tree path: Like every best- rst search, A* spans nodes generates

tree called search tree. Every known node node search tree.
trivial way move one node search tree.
tree-path algorithm instructs agent move current node target
node shortest path search tree. words,
agent walk tree current node reaches ancestor
target node, walk node target node. trivial
algorithm, presented mainly comparison purposes.

Shortest known path: nodes search tree already

explored agent, incident edges known. search tree
nodes plus additional edges explored nodes viewed subgraph
fully known. nodes subgraph connected
part search tree. Using subgraph, calculate shortest path
target node via known nodes edges. mentioned above, nding shortest
path known graph done easily, agent simply computes shortest
path target node travels along path.4

Aerial path: Assuming agent able move freely environment

restricted edges graph, simply move agent

4. navigation algorithm similar local A* search Roadmap-A* trivial case
" = 0. Roadmap-A*, shortest path target node determined known graph
agent moves along path.

640

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

current node target node via straight line connecting nodes.
method may relevant search agents highly mobile, explore
environment agents restricted travel along edges. Note
length due \aerial path" never greater length due \shortest
known path".
4.1.2 DFS-Based Navigation Algorithms

simple navigation algorithms described above, exploration new nodes done
high-level algorithm. Thus low level add new knowledge
graph, sense inecient. propose intelligent navigation
approaches nding path target pass also trough unexplored nodes.
approaches provide following advantages: paths currently known
agent may much longer paths explored yet. may
prove ecient navigate unknown parts graph seem lead
better path target. important advantage navigating
unknown parts graph, agent might visit new nodes explored
explore y. may save need travel back nodes
later time, selected expansion high-level algorithm.
advantages suggest use DFS-based navigation low level.
DFS-based navigation algorithm, search agent moves neighboring node,
visited, typical DFS manner. algorithm backtracks upon reaching deadend search continues reaches target. one neighbor,
use heuristic evaluate neighbor likely lead faster target,
visit node rst. experimented following DFS-based navigation
algorithms proposed (Cucka et al., 1996):

Positional DFS (P-DFS): DFS-based navigation algorithm sorts neighbors

according Euclidean distance target node, choosing node
minimum distance target node rst.

Directional DFS (D-DFS): DFS-based navigation algorithm sorts neigh-

bors according direction edges current node v.
rst chooses node u dierence angle line segments
(v u) (v t) smallest, denotes target node. words,
nodes prioritized directional dierence target node,
giving priority nodes dier least.

A*DFS: A*DFS improved version P-DFS. step agent chooses

neighbor w minimizes sum distances current node v
w w target node t. call A*DFS since uses cost function
similar A*, i.e., f (n) = g(n) + h(n).5 Note, however, cost
function used locally nd path current node target node.

5. generalized version navigating cost function similar A* called \robotic A*" (RA*),
also proposed (Cucka et al., 1996)ff node w either neighbor (of v) already visited
node.

641

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

dierent high-level A* uses cost function nd path
input initial state input goal state.
R
1

2


C

0000000000
T1111111111
0000000000
1111111111
11111111111
00000000000
0000000000
1111111111
00000000000
11111111111
0000000000D
1111111111
00000000000
11111111111
00000000000
11111111111
00000000000
11111111111

P

Figure 1: Illustration various low-level navigation algorithms.
Figure 1 illustrates navigation algorithms listed above. Let R denote source
node, suppose search agent currently node C , high-level
procedure chooses expand node . squared nodes already visited
agent, i.e., already explored. nodes edges connecting
comprise tree spanned high-level A* search. Since yet explored,
low-level procedure navigate target node . tree path navigate
along path C ; 1 ; R ; 2 ; , whereas shortest known path navigate along
path C ; 1 ; 2 ; . Note since node yet explored, path C via
known point. aerial path go directly C . Using one
DFS-based navigations, agent move via P , D, depending, respectively,
whether P-DFS, D-DFS, A*DFS used. bene DFS-based algorithms
explore new nodes navigation (nodes P , D,
example), revisit nodes, high-level procedure expand
later stage.

4.2 Enhanced PHA*
4.2.1 PHA* One-Level Procedure

mentioned previous subsection, PHA* presented principle one-level
algorithm. done follows. Whenever best node open list known
(i.e., explored), expansion cycle A* takes place background,
new best node determined. Upon arriving node, agent makes navigation
decision follows:
best node open list one current node's neighbors, agent
moves node.
Otherwise, agent moves neighboring node minimizes relevant
heuristic function (among variants proposed previous subsection).
642

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

heuristics would valid heuristic function one-level
algorithm. (The latter confused heuristic function associated
A* expansion cycle.) example, agent node v, using A*DFS
visit neighbor w minimizes sum distances current node v
w w best current node open list.
compact one-level presentation notwithstanding, prefer { reasons clarity
{ use two-level formulation PHA*. believe clear distinction
high-level A* low-level navigation procedure provides overall framework
well-structured conceptually clearly understood. addition, twolevel framework lends naturally two enhancements presented following
subsections.
enhancements draw basic principle navigation might proceed
necessarily best node, dierent node fairly close current
location agent. (The idea long run would prove bene cial.)
principle realized two main scenarios: (1) navigating best node,
agent might choose rst visit nearby neighbor, (2) procedure might choose
ignore best node open list select instead dierent node open list
close agent's location. context two-level framework,
rst scenario corresponds low-level enhancement (see I-A*DFS below), second
scenario corresponds high-level enhancement (see WinA*, subsection 4.2.3).
reasons, choose stick proposed two-level approach
PHA*.
4.2.2 Improved Low Level: I-A*DFS

DFS-based navigation algorithms explore new nodes traverse graph, thereby
avoiding future navigations nodes selected later expansion high
level. bene cial, seen experimental results next
subsection, take approach much further.
Suppose agent navigating target node. Along way, may pass near
nodes small f value without visiting them, path
target node according navigation algorithm. counter-productive, since nodes
small f values likely chosen expansion high level near future.
Visiting nodes agent nearby, may save lot traveling eort future.
order motivate agent visit nodes, want identify arti cially
decrease cost value (without changing value nodes).
incorporate notion, introduce Improved A*DFS (I-A*DFS) variant.
basic concept navigating target, low level select next node
visit considering approximate distance target also node's f
value. way target, I-A*DFS tend visit, one hand, nodes
small f value, avoid visiting, hand, nodes completely track.
Let n denote, respectively, target node neighboring node
currently evaluated. Also, let f (:) denote f value node provided high-level
A*, let c1 , c2 denote constants speci ed. used following heuristic function
selecting next node I-A*DFS:
643

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu


c2
(

DFS(
n
)
1 ; c1 ff((Tn))
n 2 OPEN
h(n) =
(1)

DFS(n)
otherwise:
neighbor n open list, h(n) value due A*DFS remains intact. If,

however, neighboring node open list, I-A*DFS considers also goodness
f value. node's h(n) adjusted according product term decreases
node's f value (i.e., node small f value assigned smaller heuristic)6 .
Speci cally, goodness f measured ratio f (T )=f (n). target node
smallest f value among nodes open list (for otherwise would
selected expansion high level) therefore 0 < f (T )=f (n) < 1. f (T )=f (n)
close 1, f (n) close f (T ). case, highly probable node n
visited A* next steps. Thus want assign higher priority node
visited agent, decreasing heuristic value. If, however, f (n) >> f (T )
(i.e., f (T )=f (n) ! 0), highly unlikely node n selected anytime soon
high level A*. interest raise node's priority, case,
A*DFS heuristic retained, like nodes open list.
expression provided (1) meets requirements. f (n) f (T ),
term 1 ; f (T )=f (n) becomes small, overall h value node decreases.
provides agent option visit nodes open list
small f values, even though A*DFS heuristic best. If,
hand, f (n) >> f (T ), term 1 ; f (T )=f (n) approach 1, negligible eect
h(n). main reason multiplying A*DFS heuristic 1 ; f (T )=f (n) (and
f (n)=f (T ), example) leave intact cost value node relatively large
f value, continue compete (in local heuristic sense) nodes
open list. free parameters, c1 c2 , aect qualitatively
performance I-A*DFS, merely add module's overall exibility.
experimented various constants c1 c2 , attempt determine
optimal performance. extensive empirical studies shown c1 = 0:25
c2 = 2:5 produced best performance. experiments also demonstrated
using I-A*DFS yielded better results obtained navigation algorithms
listed Subsection 4.1.2.
Figure 2 illustrates dierence A*DFS I-A*DFS. numeric values
nodes indicate order expanded A*. Suppose agent
currently located node C node 1 target. A*DFS navigate
target via node 5, since node best f (= g + h) value scenario described.
node 1, agent travel back side graph, node 2
selected (by high level) expanded next. agent go back node 3
eventually reach goal via node 4. I-A*DFS, hand, navigate
C node 1 via node 2 although node 2 assumed shortest path
node 1, smaller f value node 5. Thus I-A*DFS chooses visit node 2 rst.
Incorporating principle saves considerable amount travel cost. agent
located node 1 next node expanded node 2, high level
6. Since A*DFS(.) f (:) measure distances graph, represent, essentially, scale.
Thus combined directly.

644

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

R

5
1

C

3

2
4

A*DFS
I-A*DFS

G

Figure 2: example A*DFS versus I-A*DFS navigation.
expand immediately, explored agent, thus
readily available. Thus agent travel directly node 1 node 3 avoid
navigating back forth opposite sides graph.
4.2.3 Improved High-Level: WinA*

A* expands nodes open list best- rst order according f value.
order optimal complexity expanding node O(1). However, real
physical environment, node expansion requires agent perform costly tasks,
always ecient expand current best node. Consider, example, nearby node
best node open list, whose f value suciently small,
high probability would selected expansion A* next iterations.
intelligent agent choose explore node rst, even though currently
best node open list.

R
4
8

5
6

7

9

G
Figure 3: example illustrating disadvantage A*.
principle illustrated, example, subgraph Figure 3
contains two node clusters. numeric label node associated f value.
agent visiting nodes best- rst order (i.e., order A* expands them),
travel back forth one cluster other. much better approach
645

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

would explore nodes one cluster move cluster, thereby
traveling one cluster other.
order incorporate capability algorithm, generalized A*
call Window A* (WinA*). A* chooses expand node lowest f value,
WinA* creates set (i.e., window) k nodes smallest f values chooses
one node set expansion7 . window uses principle A" (Pearl
& Kim, 1982) mentioned before. constructing window select
node expansion. objective minimize traveling eort agent,
reduce, necessarily, number expanded nodes. Thus rather selecting
nodes small f value, choose also nodes suciently close
location agent. experimented large number combinations,
concluded best way capturing two aspects simply taking
product. Thus order nodes window cost function

c(n) = f (n) dist(curr n)
n node evaluated, f (n) f value, dist(curr n) distance n
current location agent. choose expand node smallest cost c.
(It sensible combine f (n) dist(curr n) manner, expressed
distance units.) Note node small f value chosen
expansion, f value relative nodes open list tend decrease
time. f value newly generated nodes monotonically increasing,
heuristic used consistent admissible. property reduces chance
starvation. (At least encountered phenomenon experiments.)
intention demonstrate combining two factors, manner
favors nearby nodes small f value, indeed yields enhanced performance.
tried many functions combine two factors (e.g. weighted sum) choose
paper discuss product, c(n) = f (n) dist(curr n), since provided best
results.
Combining modi ed high-level variant low-level navigation creates
technical diculties, due fact longer expand nodes open list
best- rst order. Recall standard A* expands node generating neighbors
putting node closed list. node v closed list, shortest
path source node v known. Hence, goal expanded found
shortest path it, search terminate. However, WinA* node may
expanded although exists another node smaller f value
expanded yet. words, node v expanded, necessarily imply
best path v found. Expanding node smaller f value might
discover better path. Thus search cannot simply terminate goal node
chosen expansion.
problem solved splitting standard node expansion stage two phases:
7. related algorithm derived, k-best rst search (KBFS) (Felner, Kraus, & Korf, 2003),
window size k determined open list, window nodes expanded
stage. neighbors nodes generated added open list,
new iteration begins.

646

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

1. Node expansion. Expanding node means visiting node, generating
neighbors, adding open list. stage takes place immediately
node chosen high level.
2. Node closing. Closing node means removing open list putting
closed list. takes place nodes smaller f value
explored. ensures, essentially, node placed closed
list best path source node found (See Section 5
comments). Thus search continue, even goal node
expanded, placed closed list. goal node placed
closed list, search terminate.
Following pseudo-code WinA*. Note standard expansion divided
according two phases. end cycle, algorithm attempts
close many nodes possible.
WinA*() f
.
(goal closed-list) f
.
target = node window minimizes (node) dist(current node)
.
target unexplored
.
explore(target) low level
.
expand(target)
.
(best node (with minimal f value) open-list expanded)
.
close(best node)
.
g





f





g

4.3 Experimental Results

Figure 4: 20-node Delaunay graph.
experimented Delaunay graphs (Okabe, Boots, & Sugihara, 1992),
derived Delaunay triangulations. latter computed set planar point
patterns, generated Poisson point process (Okabe et al., 1992). Points distributed
647

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

random unit square, using uniform probability density function. Delaunay
triangulation planar point pattern constructed creating line segment
pair points (u v), exists circle passing u v encloses
point. triangulation characterized, sense, one
point joined line segment nearest neighbors points.
(We refer type Delaunay graphs regular Delaunay graphs.) used
Qhull software package (Barber, Dobkin, & Huhdanpaa, 1993) construct Delaunay
triangulations (i.e., Delaunay graphs) sets points generated random
unit square. Figure 4 illustrates 20-node Delaunay graph.
principle, characteristic whereby node connected neighbors seems
suitable representing real road maps, main object research.
practice, however, additional characteristics accommodated capture adequately real road map. Thus also pursued sparse dense Delaunay graphs
obtained regular Delaunay graphs random deletion addition
edges, respectively. (See Appendix detailed discussion.)
4.3.1 Low Level Experimental Results
90
Tree path
Shrtest known path
Aerial path
P-DFS
D-DFS
A*DFS
I-A*DFS

80
70

Search cost

60
50
40
30
20
10
0
500

1000

1500
2000
2500
3000
Number nodes graph

3500

4000

Figure 5: Search cost versus number nodes regular Delaunay graphs various
low-level algorithms.
Figure 5 displays traveling distance (or search cost) agent function
number nodes Delaunay graph (i.e., 500, 1000, 2000, 4000 nodes).
graphs depicted correspond various low-level algorithms PHA* tested on.
Every data point (here experiments) corresponds average 250
dierent pairs initial goal nodes, picked random. average optimal
path observed 0.55.8 gure clearly demonstrates higher eciency
involved algorithms. particular, I-A*DFS consistently superior
algorithms graph sizes. graph size 4000, example, outperformed
8. Note closeness average optimal path observed (i.e., 0.55) expected arc length
random graph dened set points (i.e., 0.521) (Ghosh, 1951).

648

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

simple algorithm factor 10, outperformed basic A*DFS
factor 2. Note search cost increases number nodes grows,
i.e., domain becomes denser connected. attributed fact
number nodes grows, number nodes closed list I-A*DFS
procedure visit.
relative performance various algorithms considered remained
sparse dense Delaunay graphs (see Appendix A).
4.3.2 Experimental Results WinA*
6
500 nodes
1000 nodes
2000 nodes

5.5
5

Search cost

4.5
4
3.5
3
2.5
2
1.5
0

10

20

30

40
50
Window size

60

70

80

Figure 6: Search cost WinA* versus window size various sizes regular Delaunay
graphs.
experiments show using WinA* high-level procedure PHA* leads
signi cant improvement eciency algorithm. Figure 6 presents average
distance traveled search agent optimal path found, function
window size. I-A*DFS employed low-level algorithm. results shown
Figure 6 indicate using window size larger 1 (which corresponds standard
A*) signi cantly improves algorithm's performance various graph sizes
experimented with. Also, found optimal size window tends
vary size graph. Based empirical observations, setting optimal
window size (1=50) times number nodes graph seemed provide
good approximation. (For example, best window sizes observed 500- 2000-node
graphs 10 40, respectively.) Note window size becomes larger (i.e.,
number candidate nodes increases), algorithm tends select nodes large
f value, results performance degradation. Additional results sparse dense
Delaunay graphs presented Appendix A.
rst glance, improvement WinA* standard A* (for high level) seems
somewhat modest, exceed 30%. due fact I-A*DFS explores
many nearby nodes, already powerful begin with. WinA* I-A*DFS
designed assign high priority nearby nodes. dierent stages
PHA* algorithm, sense \compete" type improvement.
649

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

Indeed, using navigating algorithms, improvement WinA* relative
standard A* much signi cant. However, dealing real physical agents |
let alone humans | even 30%-time reduction WinA* (relative I-A*DFS)
viewed signi cant. Similar results obtained sparse dense Delaunay graphs
(see Appendix A).

5. Analysis PHA*
Analyzing performance PHA*, distinguish following three parameters:
(1) Cost returned path, (2) shortest possible path agent travel, (3) cost
actual path traveled agent. Subsection 5.1 argue path reported
PHA* (for future use) optimal. addition, present Subsection 5.2 extensive
empirical study compares (2) (3). Finally, provide Subsection 5.3
brief discussion PHA*'s underlying methodology overall performance.

5.1 Optimality Solution

Recall A* expands nodes best- rst order according f value. heuristic
function, h(n), admissible, f (n) = g(n)+ h(n) lower bound path goal
via node n. well-known, paradigm, goal node selected
expansion, A* found optimal path (Hart et al., 1968 Karp & Pearl, 1983 Dechter
& Pearl, 1985). Put dierently, upon goal expansion f (goal) = c, nodes
estimated paths f (n) < c already expanded length optimal
path goal c (Karp & Pearl, 1983 Dechter & Pearl, 1985).
PHA* supervised high level, activates admissible A*. (Recall
h(n) Euclidean distance n goal, i.e., admissible.) design
algorithm, high level terminates goal node selected expansion. Thus
properties admissible A*, nodes smaller f value must already
expanded, f value goal optimal. Note also holds enhanced
PHA* WinA* (see Subsection 4.2.3). Although WinA* necessarily expand
nodes according best f value, designed remove node open list
smallest f value among nodes list. algorithm halts
goal node expanded removed open list, implying f value
smallest list. Thus enhanced PHA* variant also compatible
admissible A* paradigm, path returns optimal. basic theoretical result
paper follows.
Theorem: PHA* enhanced versions return optimal path start
node goal.

5.2 Performance Evaluation PHA*
demonstrated above, complex algorithmic schemes provided dramatic improvement search time. interest assess, least extent, performance
best navigation variant, i.e., WinA* (for high level) conjunction I-A*DFS
(for low level).
650

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

graph
size
30
50
75
100
150
200
250
300

closed
nodes
11.32
15.45
17.93
20.32
24.12
28.43
31.57
35.78

jTSP j
0

0.62
0.74
0.77
0.85
0.91
0.99
1.02
1.05

PHA* ratio
0.80
0.94
0.97
1.10
1.27
1.42
1.48
1.51

1.29
1.27
1.22
1.29
1.39
1.43
1.45
1.44

Table 1: Comparison shortest paths nodes closed list actual paths
obtained PHA*.
agent's task visit essentially nodes expanded A*.
nodes comprise set nodes closed list algorithm terminates.
general, invoking A* subgraph induced nodes, source
goal states heuristic function, exhibit behavior yield
open closed lists. Thus given static graph, set nodes A* visit
xed. Ideally, would like agent visit set closed nodes along shortest
possible path. course infeasible, since nodes known advance,
rather determined y. However, order evaluate algorithm's performance,
compare output shortest possible path travels
nodes. computation latter carried o-line, i.e., set (closed)
nodes known.
Speci cally, computed shortest possible path case respect
complete graph corresponding set closed nodes. weight w(ni nj ) associated
edge (ni nj ) (in complete graph) set length shortest path
ni nj (in original Delaunay graph instance). Finding shortest path
travels via given set nodes known traveling salesman problem (TSP),
notorious exponential running time. conventional TSP path travels
nodes returns start node. However, interested path
travels nodes without returning start node. denote path
TSP distinguish conventional TSP tour. TSP tour actually TSP tour
without last edge. view exponential nature problem, used
simple branch-and-bound tree search compute desired paths. However, solving
problem optimally feasible relatively small graph sizes.
Table 1 provides comparison PHA* shortest path travels
closed nodes various small sized graphs. table indicates PHA*
algorithm quite ecient small graphs. Speci cally, average travel cost (due
PHA*) greater shortest possible path (passing closed
0

0

651

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

nodes) 45%. graphs 200 nodes less, number closed
nodes observed smaller 30. average cost cases computed 50
random instances. graphs sizes greater 200, average cost computed
5 instances only.
order evaluate, however, performance PHA* graphs larger size (where
optimal path could computed reasonable amount time), employed
lower-bound approximation cost TSP . Speci cally, computed minimum
spanning tree (MST) complete graph (de ned set closed nodes). Let jTSP j
jMSTj denote, respectively, costs associated desired path minimum
spanning tree.
0

0

Claim:

0:5 jTSP j < jMSTj jTSP j:
0

0

Proof: claim follows basic graph theory (Cormen, Leiserson, Rivest, & Stein,

2001). Speci cally, inequality right hand side stems fact TSP
spanning tree complete graph. Thus cost minimum spanning tree must
smaller (or equal to) jTSP j.
prove inequality left hand side, note triangular inequality
holds respect de ned complete graph. (That is, three nodes, ni ,
nj , nk , w(nj nk ) w(ni nj ) + w(nj nk ).) easily shown, based fact
triangular inequality holds respect original Delaunay graphs
de nition edge weight complete graph. Thus construct tour goes
twice around MST use triangular inequality shortcut edges.
Hence
2 jMSTj jTSPj > jTSP j
0

0

0

inequality left hand side follows. 2
Given infeasible computation jTSP j, claim suggests jMSTj, instead,
reasonably good approximation. Speci cally, inequality right hand side implies
travel cost agent performing PHA* is, say, c jMSTj, travel cost
PHA* greater c jTSP j. Given merely lower bound, PHA*
expected perform better practice.
Table 2 provides comparison PHA* MST lower bound shortest
path described above. average cost entered graph size computed
250 randomly generated instances. table indicates that, average, cost
PHA* 2.74 times best possible path graph sizes 8000 nodes
corresponding sets closed nodes 460 nodes.
0

0

5.3 Discussion

repeatedly noted, algorithm returns optimal solution must expand
least nodes expanded A*. Drawing basic premise, PHA*
algorithm designed visit set \mandatory" nodes eciently possible.
rationale visiting also nearby nodes (whose f value necessarily smallest)
nodes likely expanded next iterations. contrast,
652

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

graph
size
400
500
1000
2000
4000
8000

closed
nodes
40.27
43.00
62.72
131.56
233.26
460.66

jMSTj

approx.
1.05
1.15
1.42
2.01
2.52
3.45

PHA* ratio
1.91
1.97
3.03
4.89
6.76
9.44

1.82
1.87
2.13
2.43
2.69
2.74

Table 2: Comparison lower bounds shortest paths nodes closed list
actual paths obtained PHA*.
bene enhanced variation context navigation algorithm
presume return optimal solution.
Reconsider Roadmap-A*, example. A" activated prevent local navigation phase going wrong direction. However, since algorithm designed
return optimal solution, deviate stage promising route
visit nearby node may expanded later on. Put dierently, notion
set mandatory nodes agent visit. Furthermore, soon agent
reaches goal, search halts. conclusion, although PHA* Roadmap-A*
two-level navigation schemes, objectives dierent solve essentially
dierent problems.
Based properties admissible A* design algorithm,
argued (enhanced) PHA* returns path (for future use) optimal. addition,
absence theoretically known bound actual cost PHA*, run
extensive empirical study, comparing observed costs best possible costs
computed o-line. Given agent lacks priori information set mandatory
nodes, highly unlikely exists on-line PHA*-like algorithm performs
eciently o-line version. extensive empirical study demonstrates, nevertheless,
actual cost associated PHA* order magnitude optimal
cost computed o-line.

6. MAPHA*: Multi-Agent PHA*
section generalize techniques discussed previous sections multiagent case, number agents cooperate order nd shortest path. call
resulting algorithm Multi-Agent Physical A* (MAPHA*).
would like divide traveling eort agents ecient way
possible. measure eciency multi-agent case using two dierent criteria.
rst overall global time needed solve problem. second total
amount fuel consumed agents search. requirement
minimize cost moving agents time important, considering fuel
653

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

cost mobilizing agents cost function choice. case, may wise
move agents agents remain idle. However, task nd best
path goal, soon possible, idle agents seem wasteful, better utilize
time exploration graph. case, available agents
moving times. introduce two algorithms two perspectives, namely
fuel-ecient algorithm time-ecient algorithm. Note single agent case
two criteria coincide.
assume agent communicate freely agents share
data time. Thus information gathered one agent available known
agents. framework obtained using model centralized
supervisor moves agents according complete knowledge gathered
them. reasonable assumption since many cases dispatcher
centralized controller gathers information agents instructs
accordingly. Another possible model complete knowledge-sharing agent
broadcasts new data graph agents. Future research may
address restrictive communication model, limiting communication range
inducing communication errors.
also assume search terminates, soon goal node expanded
moved closed list. objective minimize travel eort point,
care moving agents pre-speci ed location (e.g.,
goal vertex start node), desired shortest path identi ed. convention
accordance many algorithms neglect report time spent \reset"
system (e.g., garbage collection), desired solution arrived at.
main idea MAPHA* algorithm similar PHA* single
agent. use two-level framework. high level chooses nodes expand,
low level navigates agents nodes. studied multi-agent
case enhanced techniques only, i.e., WinA* high level I-A*DFS
low level. problem deal assign dierent agents
explore eciently dierent nodes.

6.1 MAPHA*: Fuel-E cient Algorithm

simplicity, assume amount fuel consumed agent equal
traveling distance search. Since purpose algorithm case
minimize amount fuel consumed agents, regardless overall search time,
bene moving one agent time. moving
one agent, agent might gain new knowledge graph would allow
agents make informed intelligent moves.
beginning, agents situated source node. Then, case
single agent, high level de nes window unexplored nodes open list
potential candidates expansion. pair (a n), agent n
node window, compute allocation cost function
c(a n) = f (n) dist(a n)
f (n) f value node n dist(a n) denotes distance location
agent node n. select agent target node minimize allocation
654

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

function. case tie-breaking (e.g., beginning search agents
located initial state), pick randomly one agent relevant candidates.
stage, low-level algorithm navigates selected agent target node selected
window order explore node. single-agent case, additional
knowledge graph obtained navigation many unexplored
nodes visited traveling agent. selected agent reaches target
new cycle activated high- low-level procedures.9 Following pseudo-code
fuel ecient algorithm.
fuel-efficient algorithm() f
.
(goal closed-list) f
.
agent
.
select node window minimizes
.
best = agent minimizes ( i) dist( )
.
best unexplored
.
explore( best ) low level using best
.
expand( best )
.
(best node open-list expanded)
.
close(best node)
.
g








n



n

n

n



fn

n

f (n) dist(ai n)



g

6.2 MAPHA*: Time-E cient Algorithm

time-ecient algorithm similar described fuel-ecient algorithm
one basic modi cation. Instead moving one agent high-level cycle,
move available agents since care time spent
agents fuel consumption. idle agent save time.
Every moving agent help gather knowledge environment
additional cost, clock ticks away regardless time measured globally.
cannot use allocation function used fuel-ecient
algorithm, agents located initially node, fuel-ecient allocation
function choose node agents. main idea time-ecient
strategy agents move simultaneously. Thus ensure ecient performance
need distribute much possible. Suppose p available agents k
nodes window. would like distribute p agents k nodes eciently
possible. brute-force approach randomly distribute agents nodes.
However, provide eective distribution, incorporate following three criteria
distribution formula time-ecient procedure:
1. Since f values neighboring nodes somewhat correlated other,
nodes small f value likely generate new nodes small f
9. also implemented complex variant, whenever new unexplored node reached,
new high-level cycle activated. Results obtained signicantly dierent, omit
details variant simplicity. See (Stern, 2001) comprehensive description.

655

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

values nodes large f value. Therefore, distribution favor
assigning agent node small f value.
2. Another attribute taken consideration distance target
node agent. would like assign agent one nodes
manner, expected travel distance agent (for assignment)
minimized. words, agent assigned, preferably, relatively closeby node.
3. order expand entire window prevent \starvation", would also like
distribution function raise priority nodes assigned small number
agents. Thus keep track number agents assigned
node give preference nodes small number assignments.
Note rst third criteria may contradict, i.e, rst criterion prefer
nodes small f value third criterion favor nodes large f value,
small number agents assigned them.
found taking product values associated three criteria
gives good distribution function suitable load balancing criteria.
Speci cally, agent allocation procedure iterates agents picks,
agent, node minimizes following allocation function:
alloc(agent node) = f (node) dist(agent node) (count(node) + 1)
dist(node agent) Euclidean distance node agent, f (node)
node's f value, count(node) counter keeps track number agents
already assigned explore node. count(node) initially set 0
incremented every time agent assigned node. Thus load balancing
three factors kept throughout distribution process. beginning
search agents located start node, initial allocation
dierent nodes determined, essentially, count factor. (Without factor,
product f (n) dist(agent n) would returned node n agents.)
search progresses, agents move dierent locations get assigned step
nodes closer location small f value. Thus product
three factors creates good distribution (of agents) dierent parts
graph.
Consider, example, case illustrated Figure 7. Suppose 100 agents
located node x, window consists three nodes a, b, c located
equal distance x. Suppose also f (a) = 2, f (b) = 4 and, f (c) = 8.
numbers agents assigned nodes, using allocation procedure,
57, 29, 17, respectively. good balance various requirements.
tried many variations distribution procedure found
performed well long three requirements met. See (Stern, 2001)
discussion agent distribution.
before, agent navigates assigned target using enhanced low-level algorithm, I-A*DFS. Another high-level iteration begins soon rst agent reaches
656

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

x


b

f(a)=2

c
f(c)=8

f(b)=4

Figure 7: example agent distribution according proposed allocation procedure.
target node.10 Note computation time window agent
distribution/allocation neglected, since care travel time
agents. Following pseudo code time-ecient algorithm.
time-efficient algorithm() f
.
(goal closed-list) f
.
free agent
.
select window node minimizes dist(
.
move agents agent reaches node
.
expand nodes currently visited agent
.
(best node open-list expanded)
.
close(best node)
.
g






n

n) f (n) (count(n +1))



g

6.3 Experimental Results

experiments performed multi-agent case also conducted Delaunay
graphs 500, 1000, 2000, 4000, 8000 nodes. Additional results sparse
dense Delaunay graphs provided Appendix A.
6.3.1 MAPHA*: Results Fuel-Efficient Algorithm

provide results fuel-ecient algorithm Subsection 6.1. fuel consumption reported total fuel consumed agents. (As before, graphs
generated unit square, average optimal path observed 0.55.)
Figure 8 presents costs fuel-ecient algorithm function number
agents various sizes regular Delaunay graphs. (Results sparse graphs, well
graphs edges added random, presented Appendix A.) gure clearly
10. observed new iteration begins, almost every agent assigned node
assigned previous iteration. Typically agent's location becomes
closer \its" target node, criteria change. Thus practice, agents
go complete (original) tasks, agent reached target assigned new
goal node. See (Stern, 2001) detailed discussion.

657

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

6.5
500 nodes
1000 nodes
2000 nodes
4000 nodes

6
5.5

Fuel consumption

5
4.5
4
3.5
3
2.5
2
1.5
1

2

3

4

5
6
Number agents

7

8

9

Figure 8: Fuel consumption function number agents various sizes regular
Delaunay graphs.
demonstrates agents added, overall fuel consumption decreases
point adding agents tends increase overall consumption. Thus
optimal number agents exists graphs. phenomenon due fact
A* usually characterized small number search regions. Therefore, small
number agents suces cover regions. number agents increases, fuel
consumption goes up. phenomenon explained follows. large number agents
increases likelihood nearby agent assigned speci c node,
case relatively little exploration graph takes place. Assigning, hand,
distant agent node would result larger degree graph exploration,
essential, long run, ecient navigation (especially I-A*DFS employed). Thus
large number agents navigating small graph (which search regions), would
result excessive fuel consumption. See (Stern, 2001) detailed explanation
phenomenon.
optimal number agents increases number nodes graph increases.
optimal number agents graph 500 nodes 2, number increases
7 graph size 4000. stems fact larger graphs search
regions thus agents needed explore them.
described before, one agent allowed move experiment, point
time. measured total amount fuel consumed
agents. interest nd whether work uniformly distributed among
agents, whether large portion work carried small number agents.
Table 3 presents distribution work among agents 14 agents
active Delaunay graphs size 8000. graph instance, sorted agents
decreasing order fuel consumption. table shows relative fuel consumption
agents 3, 7, 14 activated agents.
general, remark overall work uniformly distributed, quite
balanced. example, 14 agents activated, 40% work done 4
658

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

Agent No. 3 agents %] 7 agents %] 14 agents %]
1
42.03
28.34
16.01
2
33.54
19.32
13.21
3
24.43
16.23
11.41
4
12.76
10.31
5
9.02
6.64
6
7.86
5.48
7
6.48
5.08
8
4.54
9
4.10
10
3.59
11
3.20
12
3.02
13
2.81
14
2.70
Table 3: Work distribution among multiple agents running fuel-ecient algorithm
Delaunay graphs size 8000.
agents. similar tendency observed graphs sizes, well sparse
dense Delaunay graphs (see Appendix A).
order improve eciency fuel-ecient algorithm make overall
work distribution balanced, several improvements might suggested. example,
currently agents positioned initially source node. might consider
rst spread agents number directions invoke algorithm.
Notwithstanding additional overhead may incurred spreading agents,
technique result balanced work distribution reduced overall fuel
consumption.
6.3.2 MAPHA*: Results Time-Efficient Algorithm

subsection report results time-ecient algorithm Subsection 6.2.
explained, main objective conclude task fast possible,
fuel consumption concern, agents always moving, i.e., none
idle point time. overall search time case
maximal distance either agent travels shortest path goal node found.
Figure 9 shows search time obtained time-ecient algorithm function
number agents, various regular Delaunay graphs. Note search time
never smaller time takes travel along shortest path goal.
results indicate, adding agents always ecient since measure overall
time elapsed goal found. makes algorithm interesting
ecient fact add agents, search time converges asymptotically
659

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

10
500 nodes
1000 nodes
2000 nodes
4000 nodes
8000 nodes

9
8

Search time

7
6
5
4
3
2
1
0
0

2

4

6
8
Number agents

10

12

14

Figure 9: Time consumption function number agents, various regular
Delaunay graphs.
length shortest path. Recall average length observed shortest path
approximately 0.55. Indeed, large number agents tend nd optimal path
within time frame approaches limit. overall time 2.3
single agent, reduced 0.7 14 agents graphs 500 nodes example.
Using proposed agent allocation procedure, note asymptotically paths
initial state traveled breadth- rst search manner. say
suciently large team agents likely produce single agent travel along
actual shortest path little deviation it. Similar results time- ecient
algorithm also obtained types graphs (see Appendix A).

6.4 Combined Requirements Fuel Time

distinction time-ecient algorithm fuel-ecient algorithm
reasonable, may suitable many practical situations. Practical considerations
time fuel resources may suggest combined approach, one described below.
Consider, example, commander operating constraint fuel consumption
restriction number troops assigned certain task.
order complete task fast possible, commander may want use maximal
possible number agents without exceeding fuel consumption limit.
essence, seek generalize MAPHA*, agents minimize cost
function combination time fuel consumption. suggest general cost
function takes account requirements measures. objective
activate MAPHA*, minimize cost function. Speci cally, suggest
following linear combination:

Ctotal = wt time + wf fuel
wt wf (normalized) weights attached, respectively, time fuel
consumption (i.e., 0:0 wt wf 1:0 wt + wf = 1:0). Ctotal calculated globally, i.e.,
660

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

measure amount time beginning task optimal path
found, total amount fuel consumed agents. multiply
quantities corresponding weights report combined cost.
wt wf prespeci ed user. wt = 0, time cost
fuel-ecient algorithm appropriate one use. wf = 0, fuel cost,
use time-ecient algorithm. Otherwise, neither wt wf 0,
use dierent algorithm minimize Ctotal.
suggest two algorithms general case.
Simple combined algorithm.
algorithm actually identical time-ecient algorithm. number
participating agents parameter provided user. iteration
high level participating agents move according allocation function
time-ecient algorithm. Given formulation total cost, Ctotal, would
like determine optimal number agents, wt wf . Note
trivial case wf = 0, adding agents always valuable, since
consume resources, reduce time cost. However, wf increases,
large number agents may increase total cost.
Improved combined algorithm.
main limitation simple combined algorithm even though cost
incurred fuel consumption, agents always moving. improved combined algorithm addresses problem suggests moving agents
simultaneously. Using formalization, rst determine p, i.e., number
agents participate task. Given p, determine m, i.e., number agents actually distributed nodes selected window (by
high level). remaining p ; agents stay idle. Note simple
combined algorithm p coincide. use mechanism time-ecient
allocation function, except algorithm chooses (out p) agents
minimize allocation function. time-ecient algorithm, rst
determine size window, i.e., number nodes open list
expanded. Then, invoke allocation function. Whereas
time-ecient case allocation terminates agents assigned nodes,
allocation stops agents selected. selected agents best
agents expansion cycle since minimize allocation function.

6.5 Results Combined Algorithm

provide experimental results combined algorithm introduced
previous subsection. results Tables 4 5 obtained Delaunay graphs
size 2000 table entry represents average 250 problem instances.
column, bold face number smallest total cost corresponding wt =wf ratio.
minimal costs determine optimal number agents given wt =wf ratio.
Table 4 provides total costs simple combined algorithm function
number agents various wt =wf ratios. leftmost column corresponds case
661

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

wt
wf

1.0
0.0

# agents
1
4.82
2
2.58
3
1.74
4
1.44
5
1.24
6
1.11
7
1.03
8
0.97
9
0.93
10
0.89
11
0.85
12
0.84
13
0.84
14
0.82

0.9
0.1

0.8
0.2

4.82
2.84
2.09
1.87
1.73
1.67

4.82
3.10
2.44
2.30
2.23

1.65
1.68
1.70
1.70
1.77
1.84
1.88

2.33
2.42
2.50
2.56
2.70
2.84
2.94

2.22
1.64 2.26

0.7
0.3

0.6
0.4

0.5
0.5

0.4
0.6

4.82 4.82 4.82 4.82
3.36 3.61 3.87 4.13
2.79 3.14 3.49 3.84
2.73 3.16 3.60 4.03
2.72 3.22 3.71 4.21
2.78 3.33 3.89 4.44
2.88 3.49 4.11 4.73
3.01 3.69 4.37 5.05
3.17 3.91 4.66 5.40
3.31 4.11 4.92 5.72
3.41 4.26 5.11 5.96
3.63 4.56 5.49 6.42
3.84 4.84 5.85 6.85
4.01 5.07 6.13 7.19

0.3
0.7

0.2
0.8

0.1
0.9

0.0
1.0

4.82 4.82 4.82
4.39 4.65 4.91
4.18 4.53 4.88
4.46 4.89 5.32
4.70 5.20 5.69
5.00 5.55 6.11
5.34 5.96 6.58
5.73 6.41 7.09
6.15 6.89 7.64
6.53 7.33 8.14
6.81 7.67 8.52
7.35 8.27 9.20
7.85 8.85 9.86
8.26 9.32 10.38

4.82

5.16
5.23
5.75
6.19
6.66
7.19
7.77
8.38
8.95
9.37
10.13
10.86
11.44

Table 4: Ctotal simple combined algorithm function number agents,
various ratio wt =wf ratios.

662

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

time matters. Thus entries identical values obtained timeecient algorithm. fuel consumption becomes signi cant, longer bene cial
increase number agents thus optimal number agents decreases.
wt = wf = 0:5, Ctotal = 0:5 time + 0:5 fuel, optimal number agents obtained
three, total cost 3.49. critical fuel consumption becomes,
bene cial use smaller number agents. rightmost column corresponds
extreme case, wf = 1:0, i.e., fuel consumption matters. Note
entries column dier counterpart costs obtained fuel-ecient
algorithm. dierence stems fact that, context simple combined
algorithm, picking p agents means moving simultaneously, whereas
case fuel-ecient algorithm employed one agent (out p) allowed
move times. Note fuel-ecient algorithm essentially special case
improved combined algorithm = 1.
Table 5 provides total costs improved combined algorithm function
number agents various wt =wf ratios. number participating agents p = 14
(i.e., 14 available agents could move simultaneously). row corresponds
dierent m, i.e., actual number moving agents. (Clearly, 1 p = 14.)
before, column bold face number smallest total cost corresponding
wt =wf ratio. minimal costs determine optimal number moving agents
given wt =wf ratio.
top entry rightmost column identical cost obtained fuelecient algorithm, 14 agents. case wf = 1, one agent allowed
move point time. bottom entry leftmost column identical cost
obtained time-ecient algorithm, 14 agents. case wt = 1,
14 participating agents moving times.
signi cant fuel consumption becomes, less bene cial move many
agents. Thus optimal number moving agents decreases. example, wt = wf =
0:5, optimal number moving agents obtained three, total cost 3.23.
fuel consumption becomes crucial, would bene cial move smaller number
participating agents.
Comparing results simple combined algorithm improved
combined algorithm reveals wt =wf ratio number
moving agents (which equal number participating agents simple
combined case) improved combined algorithm usually performs better.
pick moving agents larger sample. Also, appears optimal number
moving agents smaller improved combined algorithm. algorithm,
moving agents picked clever manner cycle thus better utilized.
Additional experiments conducted graph sizes, well sparse
dense Delaunay graphs. results obtained cases rather consistent. Future
work attempt predict advance best number agents.

7. Conclusions Future Work
addressed problem nding shortest path goal node unknown
graphs represent physical environments. presented two-level algorithm,
663

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

wt
wf

1.0
0.0

# agents
1
4.02
2
2.26
3
1.61
4
1.31
5
1.13
6
1.00
7
0.92
8
0.85
9
0.82
10
0.79
11
0.77
12
0.76
13
0.75
14
0.75

0.9
0.1

0.8
0.2

0.7
0.3

4.02
2.49
1.94
1.70
1.58
1.49
1.47

4.02
2.72
2.26
2.09
2.03

4.02
2.94
2.58
2.49

2.48

1.99 2.49
2.02 2.57

1.45 2.05 2.65
1.47
1.50
1.54
1.59
1.64
1.72

2.12
2.21
2.31
2.42
2.54
2.69

2.77
2.92
3.07
3.25
3.44
3.67

0.6
0.4

0.5
0.5

0.4
0.6

0.3
0.7

4.02 4.02 4.02 4.02
3.17 3.40 3.62 3.85
2.91 3.23 3.55 3.88
2.88 3.27 3.67 4.06
2.93 3.38 3.83 4.28
2.99 3.49 3.98 4.48
3.13 3.68 4.23 4.78
3.25 3.85 4.44 5.04
3.43 4.08 4.73 5.39
3.63 4.34 5.05 5.76
3.84 4.61 5.38 6.15
4.09 4.92 5.75 6.58
4.33 5.23 6.13 7.02
4.64 5.61 6.59 7.56

0.2
0.8

0.1
0.9

0.0
1.0

4.02 4.02 4.02
4.08
4.20
4.45
4.73
4.98
5.33
5.64
6.04
6.47
6.92
7.41
7.92
8.53

4.30 4.53
4.52 4.84
4.84 5.24
5.18 5.63
5.48 5.98
5.88 6.43
6.24 6.84
6.69 7.34
7.18 7.89
7.69 8.46
8.25 9.08
8.82 9.71
9.51 10.48

Table 5: Total costs improved combined algorithm function number
moving agents (out 14 participating agents), various wt =wf values.

664

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

PHA*, environments single search agent, MAPHA* algorithm
multiple-agents. experimented several variations Delaunay graphs, containing 8000 nodes. enhanced single agent algorithm yielded signi cantly better
results ones obtained simpler variants. results fuel-ecient
algorithm show using agents bene cial extent.
agents initially located source node consume fuel
move make. reason, bene using optimal number agents
opposed one agent modest. results time-ecient algorithm
encouraging, since search time converges quickly optimum number
search agents increases. also introduced cost function combines time
consumption fuel consumption, presented two algorithms paradigm.
results show combination exists optimal number agents
tends increase weight time cost increases.
Future work pursued along following directions:
assumed upon reaching node, agent learn locations
neighbors. many domains model may valid, location
node known agent actually visits it. model also
suggested (Shmoulian & Rimon, 1998). research done order
implement algorithms, context model.
used traveling agents solve shortest path problem. similar mechanism might used solving known graph problems, minimum
spanning tree, traveling salesman problem, problem requires
consideration node visited next.
proposed two algorithms combining time consumption fuel consumption. algorithms assume number agents determined priori. Future
work try theoretically determine optimal number agents given constraints. Also, future work done see whether changing number
would increase eciency algorithm. Also, assumed agents
consume fuel move, measured total performance
time task. Thus idle agents consume resources. However,
think model idle agents consume resources (e.g., time energy).
assumed centralized model, agents share knowledge
times. Future work assume communication paradigms. particular,
interested model communication
agents. model known ant-robotics model (Wagner & Bruckstein, 2000
Yanovski, Wagner, & Bruckstein, 2001). model, information spread
agents pheromones, i.e., data written agent node. agents
read pheromones reaching nodes. currently working
towards applying MAPHA* algorithm model. believe
increase size data allowed written node,
agent able write complete knowledge node environment.
challenge applying A* model lies fact since A* maintains
global open list, data opposite sides graph inuence behavior
665

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

algorithm. Thus need knowledge sharing system large
possible. purpose, believe new type communication agents
introduced. Agents type try increase search frontier
rather move around environment spread recent data available.

Acknowledgments
preliminary version paper appeared Proceedings First International Joint
Conference Autonomous Agents Multi-Agent Systems, 2002 (Felner et al., 2002).
work carried rst author Bar-Ilan University. material
based upon work supported part NSF grant #0222914 ISF grant
#8008.

Appendix A. Additional Experimental Results
mentioned Subsection 4.3, node regular Delaunay graph connected
neighbors. property may always apply real road map. example,
nearby geographic locations may always connected road segment, due
existence obstacles like mountain river. addition, distant locations often
connected highways. capture additional characteristics, also considered so-called sparse dense Delaunay graphs. Instances variants easily
obtained regular Delaunay graphs random deletion addition edges, respectively. Speci cally, generated sparse Delaunay graph instances deleting roughly
60% edges random. Likewise, dense instances generated introducing 400
edges random. (A new edge created selecting random pair nodes.)
run algorithms presented main body paper also
Delaunay graph variants. results obtained presented here.
expected, sparse graph, often agent runs deadends. Indeed, algorithms required additional travel eort nd optimal path
edges removed. However, ratio travel cost two algorithms
seems remain (for various Delaunay graph types), I-A*DFS exhibits
superior performance graph instances. See Figures 10(a), (b). behavior proved
consistent experiments, single agent multi-agent environment.
Also, Figures 11(a), (b) exhibit similar behavior search cost WinA* versus window
size sparse Delaunay graphs dense Delaunay graphs, respectively, observed
regular Delaunay graphs (see Figure 6).
Figures 12(a), (b) present costs fuel-ecient algorithm function
number agents various sizes sparse dense Delaunay graphs, respectively.
overall fuel consumption recorded sparse Delaunay graphs larger fuel
consumption recorded counterpart regular graphs (see Figure 8) factor
1.5. graphs simulating highways (i.e., dense graphs) fuel consumption
decreases relative sparse regular Delaunay graphs.
Note optimal number agents navigating sparse graph also increases, since
agents need backtrack often case. Thus agents assist
666

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

100

45
Tree path
Shrtest known path
Aerial path
P-DFS
D-DFS
A*DFS
I-A*DFS

90
80

35
30

60

Search cost

Search cost

70

Tree path
Shrtest known path
Aerial path
P-DFS
D-DFS
A*DFS
I-A*DFS

40

50
40

25
20
15

30

10

20

5

10
0
500

1000

1500
2000
2500
3000
Number nodes graph

3500

0
500

4000

1000

1500
2000
2500
3000
Number nodes graph

3500

4000

(a)
(b)
Figure 10: Search cost versus number nodes of: (a) Sparse Delaunay graphs, (b)
dense Delaunay graphs various low-level algorithms.

7

5
500 nodes
1000 nodes
2000 nodes

6.5

500 nodes
1000 nodes
2000 nodes

4.5

6
4

5

Search cost

Search cost

5.5

4.5
4
3.5

3.5

3

2.5

3
2
2.5
2

1.5
0

10

20

30

40
50
Window size

60

70

80

0

10

20

30

40
50
Window size

60

(a)
(b)
Figure 11: Search cost WinA* versus window size various sizes of: (a) Sparse Delaunay
graphs, (b) dense Delaunay graphs.

667

70

80

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

7.5

3
500 nodes
1000 nodes
2000 nodes
4000 nodes

7
6.5

2.6
2.4
Fuel consumption

6
Fuel consumption

500 nodes
1000 nodes
2000 nodes
4000 nodes

2.8

5.5
5
4.5

2.2
2
1.8

4

1.6

3.5

1.4

3

1.2

2.5

1
1

2

3

4

5
6
Number agents

7

8

9

1

2

3

4

5
6
Number agents

7

8

9

(a)
(b)
Figure 12: Fuel consumption function number agents various sizes of: (a)
Sparse Delaunay graphs, (b) dense Delaunay graphs.
search. hand, adding random edges graphs causes opposite eect,
i.e., less fuel consumed optimal number agents reduced. explained
fact new edges add connections nodes, i.e., many "shortcuts"
created search carried faster smaller number agents.
11

4.5
500 nodes
1000 nodes
2000 nodes
4000 nodes
8000 nodes

10
9

3.5

8
7

3
Search time

Search time

500 nodes
1000 nodes
2000 nodes
4000 nodes
8000 nodes

4

6
5

2.5
2

4
3

1.5

2
1

1
0

0.5
1

2

3

4

5
6
Number agents

7

8

9

1

2

3

4

5
6
Number agents

7

(a)
(b)
Figure 13: Time consumption function number agents various sizes of: (a)
Sparse Delaunay graphs, (b) dense Delaunay graphs.
Figures 13(a), (b) present costs time-ecient algorithm function
number agents various sizes sparse dense Delaunay graphs, respectively.
results con rm tendency observed regular Delaunay graphs (see
668

8

9

fiPHA*: Finding Shortest Path A* Unknown Physical Environment

Figure 9), namely number agents grows, overall cost converges
length optimal path.

References
Argamon-Engelson, S., Kraus, S., & Sina, S. (1998). Utility-based on-line exploration
repeated navigation embedded graph. Articial Intelligence, 101(1-2), 967{984.
Argamon-Engelson, S., Kraus, S., & Sina, S. (1999). Interleaved vs. priori exploration
repeated navigation partially-known graph. International Journal Pattern
Recognition Articial Intelligence, 13(7), 963{968.
Barber, C. B., Dobkin, D. P., & Huhdanpaa, H. (1993). Quickhull algorithm convex
hull. Tech. rep., Geometry Center Technical Report GCG53, University Minnesota.
Bellman, R. (1958). routing problem. Quarterly Applied Mathematics, 16 (1), 87{90.
Bender, M. A., Fernandez, A., Ron, D., Sahai, A., & Vadhan, S. P. (1998). power
pebble: Exploring mapping directed graphs. Proceedings Thirtieth
Annual ACM Symposium Theory Computing, pp. 269{278, Dallas, Texas.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction Algorithms. MIT Press, Cambridge, Massachusetts. 2nd edition.
Cucka, P., Netanyahu, N. S., & Rosenfeld, A. (1996). Learning navigation: Goal nding
graphs. International Journal Pattern Recognition Articial Intelligence,
10(5), 429{446.
Dechter, R., & Pearl, J. (1985). Generalized best- rst search strategies optimality
A*. Journal Association Computing Machinery, 32(3), 505{536.
Dijkstra, E. W. (1959). note two problems connexion graphs. Numerische
Mathematik, 1, 269{271.
Felner, A., Kraus, S., & Korf, R. E. (2003). KBFS: K-best rst search. Annals Mathematics Articial Intelligence, press.
Felner, A., Stern, R., & Kraus, S. (2002). PHA*: Performing A* unknown physical environments. Proceedings First International Joint Conference Autonomous
Agents Multi-Agent Systems, pp. 240{247, Bologna, Italy.
Ghosh, B. (1951). Random distances within rectangle two rectangles. Bulletin
Culcutta Mathematical Society, 43, 17{24.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determination minimum cost paths. IEEE Transactions Systems Science Cybernetics,
SCC-4(2), 100{107.
Kaelbling, L. P., & Moore, A. W. (1996). Reinforcement learning: survey. Journal
Articial Intelligence Research, 4, 237{285.
Karp, R., & Pearl, J. (1983). Searching optimal path tree random costs.
Articial Intelligence, 21(1-2), 99{116.
669

fiFelner, Stern, Ben-Yair, Kraus, & Netanyahu

Kitamura, Y., Teranishi, K., & Tatsumi, S. (1996). Organizational strategies multiagent real-time search. Proceedings Second International Conference
Multi-Agent Systems, 409{416.
Knight, K. (1993). many reactive agents better deliberative ones?.
Proceedings Thirteenth International Joint Conference Articial Intelligence,
pp. 432{437, Chamb$ery, France.
Koenig, S., & Likhachev, M. (2002a). D* lite. Proceedings Eighteenth National
Conference Articial Intelligence (AAAI), pp. 476{483, Edmonton, Canada.
Koenig, S., & Likhachev, M. (2002b). Incremental A*. Advances Neural Information
Processing Systems 14 (NIPS). MIT Press, Cambridge, MA.
Korf, R. E. (1985). Depth- rst iterative-deepening: optimal admissible tree search.
Articial Intelligence, 27(1), 97{109.
Korf, R. E. (1990). Real-time heuristic search. Articial Intelligence, 42(3), 189{211.
Korf, R. E. (1993). Linear-space best- rst search. Articial Intelligence, 62(1), 41{78.
Korf, R. E. (1997). Finding optimal solutions Rubik's Cube using pattern databases.
Proceedings Fourteenth National Conference Articial Intelligence, pp.
700{705, Providence, Rhode Island.
Korf, R. E. (1999). Sliding-tile puzzles Rubik's Cube AI research. IEEE Intelligent
Systems, 14, 8{12.
Okabe, A., Boots, B., & Sugihara, K. (1992). Spatial Tessellations, Concepts, Applications Voronoi Diagrams. Wiley, Chichester, UK.
Pearl, J., & Kim, J. H. (1982). Studies semi-admissible heursitics. IEEE Transactions
Pattern Analysis Machine Intelligence, 4, 392{400.
Shmoulian, L., & Rimon, E. (1998). Roadmap-A*: algorithm minimizing travel eort
sensor based mobile robot navigation. Proceedings IEEE International
Conference Robotics Automation, pp. 356{362, Leuven, Belgium.
Stentz, A. (1994). Optimal ecient path planning partially-known environments.
Proceedings IEEE International Conference Robotics Automation,
pp. 3310{3317, San Diego, CA.
Stern, R. (2001). Optimal Path Search Unknown Physical Enviroments. M.Sc.
Thesis, Department Computer Science, Bar-Ilan University, Israel available
http://www.cs.biu.ac.il/felner.
Taylor, L., & Korf, R. (1993). Pruning duplicate nodes depth- rst search. Proceedings
Eleventh National Conference Articial Intelligence, pp. 756{761, Washington, D.C.
Wagner, A., & Bruckstein, A. M. (2000). ANTS: Agents, networks, trees, subgraphs.
Future Generation Computer Systems Journal, 16(8), 915{926.
Yanovski, V., Wagner, I. A., & Bruckstein, A. M. (2001). Vertex-ant-walk: robust method
ecient exploration faulty graphs. Annals Mathematics Articial Intelligence, 31(1-4), 99{112.
670

fiJournal Artificial Intelligence Research 21 (2004) 551-577

Submitted 09/02; published 04/04

Polynomial Sized MDP Succinct Policies
Paolo Liberatore

paolo@liberatore.org

Dipartimento di Informatica e Sistemistica
Universita di Roma La Sapienza
Via Salaria 113, 00198, Roma, Italy

Abstract
Policies Markov Decision Processes (MDPs) determine next action execute
current state and, possibly, history (the past states). number
states large, succinct representations often used compactly represent
MDPs policies reduced amount space. paper, problems related
size succinctly represented policies analyzed. Namely, shown
MDPs policies represented space super-polynomial size
MDP, unless polynomial hierarchy collapses. fact motivates study
problem deciding whether given MDP policy given size reward. Since
algorithms MDPs work finding succinct representation value function,
problem deciding existence succinct representation value function
given size reward also considered.

1. Introduction
Markov Decision Processes (MDPs) (Bellman, 1957) used AI planning
effects actions probabilistically known (Puterman, 1994). partially
observable extension (POMDP) formalizes scenarios observations give
complete description state.
best plan domains may simple sequence actions. Indeed, best
action take may depend current state, known (partially, POMDPs)
previous actions executed. conditional plans named
policies. Finding policies MDPs problem deeply investigated;
algorithms developed, e.g., value iteration, policy iteration, methods based
linear programming (Littman, Dean, & Kaebling, 1995). POMDPs, variants
value iteration algorithm developed (Cassandra, Littman, & Zhang, 1997; Zhang
& Zhang, 2001).
Formally, MDP composed set states, set actions, specification
(probabilistic) effects actions, measure good state considered (reward
function). Initially, MDPs defined explicit form: states elements
given set {s1 , . . . , sn }; true false state si specified. effects
actions reward function represented explicit form, e.g., n
states, vector n elements represents reward function (each element vector
reward state.)
explicit representation simple, practical use many cases.
Indeed, many real-world scenarios described set variables (state variables);
example, set Boolean variables specify true false current

c
2004
AI Access Foundation. rights reserved.

fiLiberatore

state. explicit representation domain always size exponential
number state variables, contains enumeration states. succinct
representations used instead: states assumed possible evaluations
set Boolean variables; effects actions rewards states
represented succinct form. succinct representation considered paper uses
circuits (Mundhenk, Goldsmith, Lusena, & Allender, 2000), others exist: decision trees,
stochastic STRIPS operators, two-stage Bayes networks, variants BDDs (Boutilier, Dean,
& Hanks, 1999; Littman, 1997; Dean & Kanazawa, 1989; Dearden & Boutilier, 1997; Hansen
& Feng, 2000). Littman (1997) shown representations polynomially
reduced other: choice circuits motivated ease use.
Typically, scenarios expressed MDPs informally represented
amount space scale number variables. example,
domain ten coins ten actions tossing represented
succinctly: necessary specify action ai tosses coin (e.g., result
ai state side coin head tail probability 0.5
state variables unchanged.) hand, explicit representation
domain contains set 210 states representation transition function,
turns requires specification probability action change state
state s0 pair 210 states ten actions. result,
transition function contains set 210 210 10 = 10 220 probabilities.
Succinct representations, hand, follow intuition formal representation much larger informal representation domain.
explicit representations always exponential number state variables,
informal representation may short: case, often also case
informal description converted formal succinct one
large.
Explicit succinct representations lead different computational properties:
example, problems (e.g., checking existence policy given expected reward)
PSPACE-hard succinct representations (Mundhenk et al., 2000) polynomial
explicit one (Papadimitriou & Tsitsiklis, 1987). apparent simplification
due fact complexity measured relative size input problem,
explicit representation introduces artificial blow-up size.
paper, MDPs assumed succinct representation. particular,
states possible evaluations set Boolean variables (state variables); effects
actions reward function described using circuits. Since number
states exponential number state variables, policy indicates action
execute state, explicit representation policies always exponential
number state variables. hand, succinct representation MDP may
take polynomial space number state variables. case, policy
exponentially larger MDP. However, MDPs succinctly represented
small amount space, policies expressed succinct form well (Koller &
Parr, 1999). paper, policies represented circuits take input state
(possibly) history (the past states), output next action execute.
first result proved paper (Section 4) optimal policies, even succinct
representation, may require amount space exponential size MDP.
552

fiOn Polynomial Sized MDP Succinct Policies

result new MDPs succinct form; particular, hardness finding
optimal policy imply anything policy size. Indeed, even cases
finding optimal policy undecidable (Madani, Hanks, & Condon, 1999), policy
may short. Many hard problems, even undecidable ones, known
short solutions: example, solution halting problem single bit,
finding undecidable. Therefore, impossibility representing solutions
problem polynomial space follow complexity problem.
Given optimal policies cannot always represented space polynomial
size MDP, reasonable request best possible policy represented
within given space bound (Section 5). show bounding size policies simplifies policy existence problem. Bounding size succinct representation
value function (the function giving expected reward states) simplifies
problem (Section 6). second bound intended shed light complexity
algorithms work estimating expected reward state, value iteration algorithm (Littman et al., 1995; Cassandra et al., 1997; Zhang & Zhang, 2001).
complete analysis (Section 7) considering problem finding policies given
size reward, size exponentially larger MDP. Implications
discussions results paper given last section (Section 8).

2. Markov Decision Processes
Markov Decision Processes formalize problems planning probabilistic domains.
components are: set states, set probabilistic actions, function evaluates
states according notion goodness (reward function).
Formally, MDP 5-tuple = hS, s0 , A, t, ri, where: set states, s0
distinguished state (the initial state), set actions, function representing
effects actions, r function giving reward states.
effects actions known sure, according probability
distribution. Therefore, effects actions cannot represented using function
maps state another state, given specific action. function instead function
actions pairs states numbers interval [0, 1]. function represents
probability transitions: t(s1 , s2 , a) = p means result executing action
state s1 state s2 probability p. reward function measure
much state matches goals. Formally, function states integer numbers.
MDPs assumed represented succinct form. Namely, states assumed
represented tuples Boolean variables (state variables), i.e., set states
set propositional interpretations set variables. functions r
represented Boolean circuits. representations commonly used
practice (e.g., probabilistic STRIPS operators, two-stage Bayes network, etc.) Boolean
circuits advantage able encode exactly polynomial-time computable
functions. fact makes suitable computational analysis: indeed,
transition reward function polynomial, encoded polynomial-size
circuit without show details encoding (Mundhenk et al., 2000; Boutilier
et al., 1999). order encode polynomial-time functions, hand,

553

fiLiberatore

representations require introduction new variables dummy time points
(Littman, 1997).
Definition 1 succinct MDP 5-tuple = hV, s0 , A, t, ri, V set Boolean
variables, s0 propositional interpretation V, set actions, circuit
whose input pair interpretations V element A, r circuit
whose input interpretation V.
succinct MDP, set states factored, i.e., Cartesian product
possible values variables. succinct MDPs also called factored
MDPs. term succinct, however, appropriate, r expressed
product something. also important note set actions explicitly
represented: affects complexity checking consistency value function, i.e.,
Theorem 8.
succinct MDP = hV, s0 , A, t, ri represents (explicit) MDP M0 = hS, s0 , A, t0 , r0 i,
set propositional interpretations variables V; transition function
t0 function represented circuit t; reward function r0 function represented circuit r. words, value t0 (s, s0 , a) output circuit
inputs s, s0 , Boolean representation a. applies r r0 .
Planning deterministic domains consists finding sequence actions reach
goal state. Nondeterminism introduces two complications: first, extent
goal reached probabilistically determined; second, state time
point cannot uniquely determined initial state actions executed far.
Planning nondeterministic domains consists finding actions lead
best possible states (according reward function). Since effects actions
known sure, expected value reward determined. example,
result applying state s0 s1 probability 1/3 s2 probability
2/3, expected reward executing given r(s0 ) + 1/3 r(s1 ) + 2/3 r(s2 ),
since r(s0 ), r(s1 ), r(s2 ) rewards s0 , s1 , s2 , respectively. Formally,
expected undiscounted reward considered. sum reward state
weighted probability reaching it.
second effect nondeterminism best action execute depends
current state, unambiguously determined initial state actions
executed far, effects actions probabilistically known. example,
executing may lead state s1 s2 . executed, actual result known.
point, may best action execute s1 a0 , a00 s2 : optimal
choice depends current state, cannot unambiguously determined
initial state previous actions. simplest case, policy function gives
best action execute state. policies called stationary. policy may
also depend past states; policies called history dependent.
reward associated policy expected average reward obtained executing, state, associated action. horizon assumed finite, i.e.,
happens given number steps considered. complexity problem changes according whether horizon unary binary notation (Mundhenk
et al., 2000). Informally, assumption horizon unary notation means
number steps consider polynomial size instance problem.
554

fiOn Polynomial Sized MDP Succinct Policies

paper, assumed unary notation. assumption called short
horizon polynomial horizon papers.
size explicit representation every policy succinct MDP exponential number state variables. However, policies take less space succinct
representations. paper, succinct representation employing circuits considered:
input current state (possibly) history; output next action
execute. example policy small succinct representation always
executing action; policy exponential explicit representation
necessary specify action state number states exponential
number state variables.
first question considered paper whether always possible represent
optimal policy succinct MDP circuit size polynomial size
MDP. Namely, succinct policy defined circuit outputs, given current state,
action.
Definition 2 succinct stationary policy P circuit takes state input
outputs action execute state.
succinct policy circuit represents function states actions. Since (nonsuccinct) policies functions states actions, succinct non-succinct policies
correspondence. expected reward succinct policy optimality therefore
defined terms analogous concepts non-succinct policies.
Succinct history-dependent policies defined similar way: circuits
sequences states actions (such representation possible finite horizon.)
expected reward optimality succinct history-dependent policy defined
corresponding concepts non-succinct policies.
first result paper that, MDPs optimal succinct policies polynomial size, polynomial hierarchy coincides complexity class p2 (considered
unlikely). proof based compilability classes reduction, summarized
next section.

3. Complexity, Compilability, Circuits
reader assumed familiar complexity classes P, NP, classes
polynomial hierarchy (Stockmeyer, 1976; Garey & Johnson, 1979). counting
classes also used paper: PP class problems polynomially
reduced problem checking whether formula satisfied least half
possible truth assignments set variables. alternative definition
problem PP polynomially reduced checking whether
P
V (M ) k, k integer, V function propositional interpretations
integers calculated polynomial time, sum ranges possible
propositional interpretations (Johnson, 1990). class NPPP defined terms oracles:
class problems solved polynomial time non-deterministic
Turing machine access PP-oracle, device solve problem
PP one time unit.

555

fiLiberatore

Without loss generality, instances problems assumed strings. length
string x denoted ||x||. cardinality set instead denoted |S|.
function g called poly-time exists polynomial p algorithm
that, x, time taken compute g(x) less equal p(||x||).
function f called poly-size exists polynomial p that, strings x,
holds ||f (x)|| p(||x||); whenever argument function f number,
assumed unary notation. definitions extend functions one
argument usual.
Circuits defined standard way. Whenever C circuit possible
assignment input gates, output denoted C(s) output. formal definition
circuits important paper, take advantage well-known result
circuit complexity relates poly-time functions poly-size circuits. Informally, given
circuit value inputs, determine output polynomial time.
similar result holds way around, is, poly-time functions represented
means circuits (Boppana & Sipser, 1990).
Formally, however, poly-time function strings strings string arbitrary
length argument, string arbitrary length (in general) output. Even considering
functions binary output (i.e., single bit), input may arbitrarily
long. hand, circuit specified number input gates.
correspondence poly-time functions poly-size circuits one-to-one.
However, following correspondence holds: poly-time function strings
strings, exists uniform family poly-size circuits {C0 , C1 , C2 , . . .}, Ci
circuit input gates calculates result f strings length i; uniform
means exists function calculates Ci runs time
bounded polynomial value i.
result, class P also defined set problems solved
uniform family circuits. replacing assumption family uniform
Ci size polynomial value i, definition gives class P/poly.
paper, problem whether succinct MDPs optimal succinct policy
polynomial size considered. Note that, given MDP, succinct policies
circuits take input state (or, sequence states). result, policy
single circuit, family.
question given (negative) answer using two different proofs, based different
techniques: first one based compilability classes (Cadoli, Donini, Liberatore, &
Schaerf, 2002), second one employs standard complexity classes. compilability
classes introduced characterize complexity intractable problems
preprocessing part data allowed. problems characterized way
instances divided two parts: one part fixed (known
advance) one part varying (known solution needed.) problem
determining action execute specific state form: MDP (with
horizon) part known advance, describes domain; contrary,
state determined previous actions executed. Compilability
classes reductions formalize complexity problems first part
preprocessed.

556

fiOn Polynomial Sized MDP Succinct Policies

common complexity theory, decision problems considered, i.e., problems
whose solution single bit. problems usually identified languages (sets
strings): language L represents decision problem whose output 1 x L
0 x 6 L.
problems whose instances composed two parts formalized languages
pairs (of strings): language pairs subset . order characterize
problems, non-uniform compilability classes introduced (Cadoli et al.,
2002). classes denoted k;C read nu-comp-C, C arbitrary
uniform complexity class, usually based time bounds, P, NP, etc.
Definition 3 (k;C classes) k;C composed languages pairs
exists poly-size function f pairs strings strings language
pairs 0 C that, hx, yi , holds:
hx, yi iff hf (x, ||y||), yi 0
problem k;C reduces one C suitable polynomial-size preprocessing (compiling) step. problem C also k;C (i.e., f (x, n) = x
0 = S). Preprocessing useful problem C k;C0 C0 C: case,
preprocessing decreases complexity problem. reduction complexity
possible problems (Cadoli et al., 2002).
tool used proving decrease complexity possible
concept hardness respect compilability classes, turn based
definition reductions. Since general concepts really needed paper,
condition based monotonic polynomial reductions presented. condition
sufficient prove problem k;NP k;P unless NP P/poly,
currently considered unlikely.
Let us assume hr, hi polynomial reduction 3sat problem pairs S,
is, r h poly-time functions satisfiable hr(), h()i S.
pair hr, hi monotonic polynomial reduction if, pair sets clauses 1
2 literals, 1 2 , holds:
hr(1 ), h(1 )i iff hr(2 ), h(1 )i
Note second instance combines part 2 part 1 :
intentional. Roughly speaking, reduction implies hardness problem
comes second part instances only, first part r(1 ) instance
result reduction replaced another one r(2 ) without changing
membership S. complexity problem due part instance only,
preprocessing part reduce complexity problem. formal
proof fact existence reduction implies k;P (unless
NP P/poly) found elsewhere (Liberatore, 2001).

4. Super-polynomially Sized Policies
Suppose always possible find optimal succinct policy P polynomial size.
Since succinct policies circuits definition, deciding action execute state
557

fiLiberatore

polynomial-time problem: given P , compute P (s), output circuit P
given input. whole two-step process finding policy using
find next action specific state seen algorithm finding next
action execute state. first step (finding policy) likely hard,
second one polynomial-time computable. means problem deciding
next action compilable k;P. done present paper prove
problem instead k;P. implies succinct policies cannot always
represented polynomial space. aim, formal definition problem
deciding next action execute given.
Definition 4 next-action problem problem deciding, given MDP, horizon
unary notation, state, action, whether action one execute
state according optimal policy.
problem proved NP-hard intermediate step. Actually, result easy
derive known theorems: interesting reduction used proof. Namely,
given set clauses , clause composed three literals set variables
X = {x1 , . . . , xn }, instance next-action problem polynomial time built
polynomial time. reduction denoted f ; formally, function sets
clauses quadruples hM, T, s, ai, first element succinct MDP, second
one number unary (the horizon), third one state, fourth one
action. Let L set literals X, let E = L {sat, unsat}. MDP
defined as:
= hV, , A, t, ri
components defined follows.
States: set states correspondence set sequences (2n)3 +
n + 1 elements E; obtained using following set variables:
V = {qi | 1 log((2n)3 + n + 1)} {vij | 1 log(|E|), 1 j (2n)3 + n + 1}.
idea variables qi represent length sequence binary,
j
variables v1j , . . . , vlog(|E|)
represent j-th element sequence;
Initial state: initial state interpretation representing empty sequence ;
Actions: contains three actions A, S, U , one action ai xi ;
Transition function: action change current state either sat unsat
belong sequence represented current state; otherwise, effect
randomly select (with equal probability) literal L add sequence
representing current state; effect U add sat unsat
sequence, respectively (these deterministic actions); actions ai change state
either sat unsat, both, belong sequence; case, ai
adds either xi xi sequence, probability;

558

fiOn Polynomial Sized MDP Succinct Policies

Reward function: involved part MDP. Given sequence 3m
literals L, following 3cnf formula considered:
C(l11 , l21 , l31 , . . . , l1m , l2m , l3m ) =
{l11 l21 l31 , . . . , l1m l2m l3m }.
Given number possible distinct clauses L less (2n)3 , set
clauses represented sequence 3m literals, = (2n)3 .
function C encodes sets clauses L sequences literals.
sequences reward different zero composed sequence
3m literals E, followed either sat unsat, followed sequence s0 =
l1 , . . . , ln , li either xi xi . Namely, sequence (s, unsat, s0 )
reward 2; sequence (s, sat, s0 ) reward 1 set clauses C(s) satisfied
model s0 , 2n+1 otherwise;
Note states reward 0. expected reward calculated
reached states, r defined way that, state nonzero reward,
previous succeeding states reward zero. reward function r defined
way sake making proof simpler; however, expected reward calculated
states, including intermediate ones.
MDP single optimal policy: execute 3m times, execute either U
S, execute a1 , . . . , . choice U gives greatest
expected reward depends result execution A. Namely, possible result
execution first 3m actions corresponds set clauses. next action
optimal policy U set unsatisfiable satisfiable.
definition MDP M. instance next-action problem
composed MDP, horizon unary, state, action, problem
check whether action optimal state. horizon consider = (2n)3 +n+1,
state one corresponding sequence literals C(s) = ,
action S. possible prove function f defined f () = hM, T, s, Si
reduction satisfiability next-action problem.
Theorem 1 f () = hM, T, s, Si, unique (stationary history-dependent)
optimal policy w.r.t. horizon , satisfiable action execute optimal policy horizon .
Proof. MDPs defined policies positive reward: policy executing
A3m U a1 , . . . , expected reward equal 2, since leaves reward 2
internal nodes reward 0. sequence actions executed stationary
policy history inferred current state.
sequences actions end state positive reward similar
other. Indeed, begin 3m times action A. Then, either U
executed, followed sequence a1 , . . . , . difference sequences
choice U S.
optimal policies therefore execute first 3m time points, regardless
generated state. execute either U S, execute a1 , . . . , . choice
559

fiLiberatore

U made differently different states: policy execute U
depending state results 3m executions A.
sequence 1:


. . A} U a1 . . .
| .{z

sequence 2:


. . A} a1 . . .
| .{z

3m
3m

Figure 1: Sequences leading state reward > 0. fragments extensions
give reward 0.
Let us consider state 3m executions A. Since execution
generates random literal, point state sequence 3m literals.
sequence represents set clauses later used reward function. Intuitively,
point optimal policy execute U set clauses unsatisfiable,
satisfiable.
Let state results execution 3m times, let C(s)
corresponding formula. expected reward policy executes sequence
U, a1 , . . . , 2 sequence leads states reward 2.
hand, reward policy executes A, a1 , . . . , depends satisfiability
formula represented state. Namely, sequence leads state
possible truth interpretation X; reward state 1 C(s) satisfied
model, 2n+1 otherwise. means reward policy 1
formula unsatisfiable, least (2n 1)/2n + 2n+1 /2n = (2n 1)/2n + 2 formula
least one model.
optimal choice therefore U formula C(s) unsatisfiable (reward 2)
otherwise (reward greater equal (2n 1)/2n + 2n+1 /2n > 2). Since history
inferred state, optimal choice depend whether stationary
history-dependent policies considered.
Incidentally, theorem implies choosing next action optimally NP-hard.
important, however, function f used prove problem
choosing next action optimally cannot simplified P thanks preprocessing
step working MDP horizon only. This, turn, implies nonexistence
polynomially sized circuit representing optimal policy.
polynomial reduction 3sat problem necessarily prove
problem cannot efficiently preprocessed. Consider, however, case problem instances divided two parts. reduction decomposed two
separate functions, one generating part instance preprocessed
one generating rest instance. case, form part
preprocessed, state action rest instance. result,
f () = hM, T, s, ai, two functions defined by:
r() = hM,
560

fiOn Polynomial Sized MDP Succinct Policies

J
J
J

J

J

J
execution J
level, state

3m
times

J
represents set clauses

J

J


J

U U U


satisfiable formula
unsatisfiable


formulas





1 1 2n+12n+1
models
models

Figure 2: optimal policy MDP proof.

h() = hs, ai.
Provided polynomial hierarchy collapse, problem k;P
exists monotonic Polynomial reduction 3sat problem (Liberatore, 2001).
polynomial reduction monotonic two functions made satisfy following
condition: every pair sets clauses 1 2 three literals set
variables, 1 2 hr(1 ), h(1 )i yes instance hr(2 ), h(1 )i
yes instance. Note second instance hr(2 ), h(1 )i, is, combines
part derived 2 part 1 .
specialization condition case next-action problem follows.
Let hM1 , = r(1 ) hM2 , = r(2 ): two horizons 1
2 alphabet. Let hs, ai = h(1 ). Monotonicity holds if, two sets
clauses 1 2 set literals 1 2 , optimal action
execute state M1 M2 . reduction f
defined satisfies condition.
Theorem 2 function f sets clauses quadruples hM, T, s, ai monotonic
polynomial reduction.
Proof. Let 1 2 two sets clauses set variables, clause
composed three literals, let M1 M2 corresponding MDPs. Since
MDP corresponding set clauses dependsby constructionon set variables
only, M1 M2 exactly MDP. Since horizons r(1 ) r(2 )
same, r(1 ) = r(2 ). result, state action a, latter
optimal action execute M1 horizon M2 ,
definition monotonicity case MDPs.

561

fiLiberatore

theorem implies next-action problem hard compilability class
k;NP. turn, result implies MDPs optimal succinct policy
size polynomial MDP horizon. specifically, way
storing optimal actions states way required space polynomial
time needed determine action execute state polynomial well.
Theorem 3 exists data structure, size polynomial MDP
horizon, allows computing best action execute (either current state
history) given MDP horizon unary notation polynomial time,
NP P/poly.
Proof. data structure exists, next-action problem k;P: given fixed
part problem (the MDP horizon), possible determine data
structure preprocessing step; result step makes determining next action
polynomial task. result, next-action problem k;P. hand,
existence monotonic polynomial reduction propositional satisfiability
next-action problem implies problem k;P, k;NP=k;P (Liberatore,
2001, Theorem 3). turn, result implies NP P/poly (Cadoli et al., 2002,
Theorem 2.12).
Note NP P/poly implies p2 = p2 = PH, i.e., polynomial hierarchy
collapses second level (Karp & Lipton, 1980). result, theorem implies
one always represent optimal policies space polynomial MDP
horizon polynomial hierarchy collapses. Since succinct representation
policies based circuits subcase data structures allowing determination
next action polynomial time, following corollary holds.
Corollary 1 exists polynomial p every succinct MDP horizon
succinct optimal policies (either stationary history dependent) size bounded
p(||M|| + ), NP P/poly p2 = p2 = PH.

5. Finding Evaluating Succinct Policies
problem considered section checking existence succinct policies
given size reward.
subproblem interest evaluating policy, is, calculating expected
reward. Mundhenk et al. (2000) found complexity problem various
cases, left open case full observability succinct representation,
one considered paper. problem instead analyzed Littman,
Goldsmith, Mundhenk (1998) considering succinct representation plans based
ST plan representation (which also equivalent succinct representation.)
proof presented current paper could follow along similar lines.
evaluation problem make sense, given MDPs optimal
succinct policies polynomial size? theoretical point view, super-polynomiality
forbid complexity analysis. Indeed, complexity measured relative total
size problem instances; instances policy evaluation problem include
562

fiOn Polynomial Sized MDP Succinct Policies

policy MDP. policy exponentially larger MDP, means
MDP logarithmic part instance.
Formally, problem policy evaluation is: given MDP, policy, number k,
decide whether expected reward policy greater k. counting problem, amounts summing evaluations result computing polynomial
function set propositional models. surprisingly, PP.
Theorem 4 Given succinct MDP = hV, s0 , A, t, ri, horizon unary notation,
succinct policy P (either stationary history dependent), number k, deciding whether
policy P expected reward greater k PP.
Proof. expected reward policy weighted sum rewards states. Let us
consider sequence states s0 , s1 , . . . , sd . probability sequence
actual history computed follows: pair consecutive states si , si+1
factor given probability t(si , si+1 , a), unique action chosen
policy state si (or, history s0 , . . . , si .) Multiplying factors,
result probability whole sequence s0 , s1 , . . . , sd actual history
point d. Since P (si ) denotes output circuit P si input,
P (si ) represents action executed state si . result, probability
s0 , s1 , . . . , sd actual sequence states follows:
H(s0 , s1 , . . . , sd ) =



t(si , si+1 , P (si ))

i=0,...,d1

probability history-dependent policies determined way,
P (si ) replaced P (s0 , . . . , si ). Given specific sequence s0 , . . . , sd , possible
determine H(s0 , . . . , sd ) time polynomial size sequence plus
MDP.
expected reward policy calculated sum expected reward
state sd multiplied probability sequence ending sd actual
history. sum expressed follows:
R(P ) = r(s0 ) +

d=1,...,T
X

H(s0 , . . . , sd ) r(sd ).

s1 ,...,sd

number ranges 1 take account sequences length
. intermediate states sequences dealt with: sequence
s0 , . . . , si , . . . , sd , sum contains term subsequence s0 , . . . , si well.
Roughly speaking, membership PP due fact expected reward
policy result sum exponential number terms, terms
determined polynomial time uniform manner. Formally, proved
problem expressed sum terms V (M ), V polynomial
function ranges propositional interpretations given alphabet.
complete proof, therefore, needed encode possible sequence s0 , . . . , sd
propositional model, constraint H(s0 , . . . , sd ) r(sd ) determined
model polynomial time.
563

fiLiberatore

employed encoding following one: alphabet X1 XT ,
set Xi set variables one-to-one correspondence variables MDP,
set variables size log(T ). model represents sequence whose length
given values whose i-th state given values Xi .
left show expected probability sequence
determined polynomial time given model represents sequence. true
because, given M, possible rebuild sequence model time linear
size model, evaluate H. Since sequences represented propositional
interpretations, function giving weighted reward interpretation polytime, problem PP.
theorem shows problem policy evaluation succinct MDPs horizon
unary notation PP. problem also proved hard class.
Theorem 5 Given succinct MDP M, horizon unary notation, succinct policy
P , number k, deciding whether expected reward P greater k PP-hard.
Proof. theorem proved reduction problem checking whether formula
satisfied least half possible truth assignments variables. Let Q
formula alphabet X = {x1 , . . . , xn }. define MDP = hV, s0 , A, t, ri
way states correspondence sequences literals X.
actions {a1 , . . . , }, ai modifies state adding xi xi
sequence represented state, probability. reward function assigns
1 sequences represent models Q, 0 sequences. Namely,
sequence contains variable twice, contain variable, corresponding state
reward 0.
Let P policy executing ai state composed 1 literals. states
result application policy time consistent sets literals
alphabet x1 , . . . , xi1 . < n 1, state reward 0. Therefore,
states time = n 1 relevant calculation expected reward P .
states sets literals represent models alphabet X; probability
state time = n 1 1/2n . Given reward state 1
corresponding model satisfies Q 0 otherwise, expected reward P m/2n ,
number models Q.
proof used prove problem finding expected
reward policy #P-hard, decision problems considered paper.
two theorems allow concluding problem PP-complete.
Corollary 2 Given succinct MDP, horizon unary notation, succinct policy,
number k, deciding whether expected reward policy larger k PP-complete.
Let us turn problem checking existence policy given size
expected reward. problem without size constraint PSPACE-hard
(Mundhenk et al., 2000). corollary indicates size bound allows
guess-and-check algorithm slightly lower complexity.

564

fiOn Polynomial Sized MDP Succinct Policies

Theorem 6 Given succinct MDP, horizon unary notation, size bound z unary
notation, reward bound k, checking existence succinct policy size bounded
z expected reward greater equal k NPPP -complete.
Proof. Membership easy prove: guess circuit size z representing policy check
whether expected reward greater k. Note z unary essential.
Hardness proved reduction e-majsat, problem defined Littman et al.
(1998) follows: given formula Q variables X , decide whether exists
truth assignment X least half interpretations extending satisfy
Q. problem NPPP -complete (Littman et al., 1998).
Given instance e-majsat, corresponding MDP defined follows: states
represent sequences literals X (we assume, w.l.o.g., |X| = |Y | = n.)
action ai variable . ai adds literal yi yi sequence
representing current state probability. variable xi associated
actions bi ci , add xi xi state, respectively. reward state
1 represents sequence form []x1 , []x2 , . . . , []yn literals
satisfy Q, is, sequence complete interpretation satisfies Q.
policy expected reward greater 0 made sequence whose
i-th element (1 n) either bi ci , + n-th element (1 n) ai .
expected reward policy horizon = 2n 1 least half
completions model defined actions bi cj satisfy Q. Therefore, MDP
policy reward 1 Q e-majsat.
result used alternative proof claim MDPs admit
polynomially sized optimal succinct policies. proof interesting employs
different technique, conditioned differently previous one.
Theorem 7 MDPs optimal succinct policies (either stationary history dependent) size polynomial sum size MDP length horizon
PSPACE NPPP .
Proof. Checking existence policy given expected reward, regardless
size, PSPACE-hard (Mundhenk et al., 2000). hand, policies
represented polynomial space, problem coincides checking existence
policy given bound size, latter problem NPPP .
Theorem 7 proved using fact problems evaluating policy given size
deciding existence policy different complexity characterizations.
technique used Papadimitriou Tsitsiklis (1987) case POMDPs
explicit representation.
Precisely, proof composed following sequence statements:
1. Evaluating expected reward succinct policy C1 -complete, C1
complexity class;
2. Deciding existence policy (with size bound) given expected reward
C2 -complete, C2 complexity class;
565

fiLiberatore

3. policy could succinctly represented space polynomial size
MDP value horizon, second problem could solved guessing
policy evaluating it; since policy guess size polynomial size
instance B, C2 NPC1 .
makes proof worthy (probable) falsity conclusion C2 NPC1 .
words, proof applied prove given data structure
always polynomially large if:
1. problem C1 -complete;
2. problem B C2 -complete;
3. problem B expressed as: exists data structure satisfying problem A.
proof schema simply generalization one above: data structure
succinct policy; first problem evaluating succinct policy; second problem
deciding existence policy giving given expected reward. definition,
instance satisfying B implies existence data structure satisfying A. possible
replace existence data structure existence polynomial-size data
structure sentence, C2 NPC1 , B solved guessing data
structure checking whether satisfied not. conclusion C2 NPC1
false, instances satisfying B related data structure
satisfying size polynomial size instance B.

6. Bounding Value Function
Bounding policy size motivated fact interested policies
actually stored: policy used size less equal
available storage space. section, similar constraint considered, motivated
algorithms MDPs work. Namely, programs based popular value iteration
algorithm (Littman et al., 1995; Cassandra et al., 1997; Koller & Parr, 1999; Zhang &
Zhang, 2001) work finding value function, function giving expected
reward obtained state executing actions according given
policy.
Definition 5 value function E MDP = hS, s0 , A, t, ri horizon
policy P function gives expected reward state steps
horizon:
(

E(s, i) =

r(s)
= 0
P
r(s) + s0 t(s, s0 , P (s)) E(s0 , 1) otherwise.

similar definition given history-dependent policies including history
arguments value function policy:
(

E(s0 , . . . , sj , i) =

r(sj )
= 0;
P
r(sj ) + s0 t(sj , s0 , P (s0 , . . . , sj )) E(s0 , 1) otherwise.
566

fiOn Polynomial Sized MDP Succinct Policies

value function succinct MDPs defined simply replacing s0 s0
propositional interpretation V.
MDP policy succinctly represented, value function cannot necessarily represented explicitly polynomial space. Rather, form succinct representation employed, usually decomposition state space (for example, grouping
states expected reward.) already done policies, value
functions succinctly represented circuits.
Definition 6 succinct value function circuit E whose inputs state
integer whose output expected reward points horizon.
Given policy P , always exists associated value function, gives
expected reward obtained state executing actions specified
policy. converse, however, always possible. value functions, indeed,
correspond policy. value function used derive policy
selecting actions maximize expected reward resulting states (this
value functions often used), value functions completely unrelated
actual expected reward states. example, reward s0 0, action
changes states (i.e., t(s, s, a) = 1 actions states s), value function
E E(s0 , ) = 1000 correspond policy. words, value
functions assign expected rewards states way consistent MDP,
i.e., way obtain reward executing whichever actions. Therefore,
value function may may consistent, according following definition.
Definition 7 value function E consistent MDP horizon
exists policy P E value function M, , P .
interesting property value functions that, cases, actually represent
policies. Indeed, policy determined value function polynomial time
degree non-determinism bounded, list possible states may result
executing action calculated polynomial time. particular, set states
resulting applying assumed result circuit na .
Definition 8 bounded-action MDP 6-tuple = hV, s0 , A, N , t, ri, M0 =
hV, s0 , A, t, ri succinct MDP N = {na } set circuits, one na N
A, na (s) list states s0 t(s, s0 , a) > 0.
Beside N , definition bounded-action MDPs succinct MDPs.
difference explained two ways: intuitively, assumed possible
outcomes actions determined time polynomial size MDP; technically, time needed determine possible states result applying action
included size input.
proof NPPP -hardness problem policy existence uses bounded-action
MDPs, therefore still holds case. seems contradict intuition
large degree non-determinism one sources complexity problems MDPs.
next results explain contradiction.
567

fiLiberatore

Given bounded-action MDP M, horizon, succinct value function E,
identify polynomial time succinct policy corresponds E, is, policy
leads expected reward states specified E. case stationary policies,
P determined follows: given state s, consider action time point i,
check whether result executing consistent value function, assuming
time point i. done determining sum t(s, s0 , a) E(s0 , 1).
equal E(s, i) r(s), action action execute, i.e., P (s) = a.
whole process polynomial, is, P (s) determined polynomial time.
E consistent MDP, policies expected reward
state E(s). result, consistent value function E represents group
policies, expected reward. therefore makes sense consider
problem finding E rather finding P . precisely, since decision problems
considered, analyzed problem checking whether exists E
expected reward corresponding policies greater equal given number.
Given E, expected reward simply given E(s0 , ). order check whether
reward actually obtained MDP, however, also check whether
value function E consistent MDP. point assumption
set actions succinct MDP succinct representation used.
Theorem 8 Checking whether succinct value function E consistent boundedaction MDP horizon unary coNP-complete, stationary historydependent policies.
Proof. problem check whether exists policy P gives expected
reward specified E. Namely, i, check whether equation
Definition 5 holds policy P . turn, existence policy means that,
s, exists associated action satisfies equation P (s) replaced
a.
Formally, let = hV, s0 , A, t, ri MDP, horizon, set
interpretations alphabet V. condition formally expressed follows.
every state s, holds E(s, 0) = r(s), and:
{1, . . . , } . E(s, i) = r(s) +

X

t(s, s0 , a) E(s, 1).

s0

condition contains three alternating quantifiers; however, second one ranges
A, third one ranges = {1, . . . , }. cases, number
possibilities polynomial size MDP horizon.


.

_

^

aA

i=1,...,T



E(s, i) = r(s) +

X

t(s, s0 , a) E(s0 , 1) .

s0

number terms s0 sum not, general, polynomial. bounded
action assumption, however, implies states relevant s0
belongs na (s), i.e., s0 one elements list produced circuit na
given input.
568

fiOn Polynomial Sized MDP Succinct Policies



.

_

^

aA

i=1,...,T


X

E(s, i) = r(s) +

t(s, s0 , P (s)) E(s0 , 1) .

s0 na (s)

Considering given only, condition checked polynomial time. Since
condition checked possible S, problem coNP. case
history-dependent policies dealt replacing s0 , . . . , sj j.
Hardness proved follows: given formula Q variables X = {x1 , . . . , xn },
build succinct MDP V = X single action a. action takes
number {1, . . . , n} equal probability changes value xi . reward
function 1 state satisfies Q, 0 otherwise. value function E(s, i) = 0
= 1, . . . , consistent MDP formula Q
unsatisfiable. Indeed, since MDP contains one action, possible stationary
history-dependent policy always executing action a. policy leads
random interpretation. expected reward policy 1
interpretation satisfies Q.
Checking existence consistent succinct value function size bounded z
expected reward bounded k therefore p2 , done guessing succinct
value function E, checking consistency MDP horizon, determining
expected reward initial state E(s0 , ). Since consistency coNP boundedaction MDPs, problem p2 . following theorem also shows problem
complete class.
Theorem 9 Given bounded-action MDP M, horizon unary notation, size bound
z unary notation, reward bound k, checking existence succinct value
function E consistent , size bounded z expected reward
bounded k p2 -complete stationary history-dependent policies.
Proof. Membership: guess succinct value function E (i.e., guess circuit state
history integer input) size z; check whether consistent
MDP horizon, whether E(s0 , ) k. Since checking consistency coNP
stationary history-dependent policies, problem p2 cases.
Hardness proved Theorem 6. use problem checking whether
exists truth evaluation X whose extensions X models Q, Q
formula X |X| = |Y | = n.
MDP corresponds Q sequences literals X states.
action ai variable two actions bi ci variable X.
effect ai add either yi yi state, probability. actions bi
ci add xi xi , respectively, therefore deterministic actions. reward
state 1 state sequence comprised either x1 x1 followed
x2 x2 , etc., literals form model Q. states reward 0.
policy nonzero reward executes either bi ci time i, execute
actions a1 , . . . , sequence. first n actions, state exactly model X.
reward policy number models extend satisfy Q. Therefore,
569

fiLiberatore

exists policy reward 1 exists model X whose extensions
satisfy Q.
theorem shows checking existence succinct value function given
size reward complexity class second level polynomial hierarchy
bounded-action MDPs. corresponding problem policies (instead value functions)
remains NPPP -hard bounded-action MDPs. Assuming p2 6= NPPP , checking
existence bounded-size succinct policies harder checking existence value
functions, sense problems polynomially reduced latter problem
former. Let us consider result.
1. problem easier additional constraint size value
function. Assuming p2 6= NPPP , implies succinct policies size
polynomial MDP whose value function cannot expressed circuit
size polynomial MDP, bounded-actions MDPs. Translating succinct
value functions succinct policies instead always feasible polynomial space
bounded-actions MDPs. Therefore, succinct policy translated
succinct value function polynomial size (in size policy, MDP,
horizon), inverse translation always polynomial size.
2. Given amount physical memory always bounded, problem
solved algorithms finding value function (such value iteration) actually
checking existence succinct value function bounded size.
problem easier similar problem bound, sense first
problem polynomially reduced former vice versa, provided
p2 6= NPPP .
Roughly speaking, finding bounded-size succinct value function simpler finding
bounded-size succinct policy, solving former problem may produce solutions
worse, term overall reward, solutions latter.

7. Exponential Bounds
previous sections, size bounds assumed unary notation.
assumption made because, feasible store unary representation
size, feasible store data structure size well. unary notation
formalizes informal statement policy take amount space
polynomial size instance.
Let us consider problem checking existence policy bounded size
reward, bound reward binary notation: searching
policy exponentially larger MDP, still size bounded given
number. problem interest whenever exponential succinct policy acceptable,
still limit size.
important observation circuits necessary consider circuits
arbitrary size. Indeed, circuit n inputs outputs equivalent circuit
size m2n . circuit equivalent circuits,
n inputs one output. circuit represents Boolean function,
570

fiOn Polynomial Sized MDP Succinct Policies

therefore expressed DNF term contains variables. term
represents model: therefore, 2n different terms alphabet
made n variables; result, function expressed circuit size 2n .
result, policy expressed circuit size |A|2n , n number state
variables.
fact two consequences: first, necessary consider circuits arbitrary
size, policy represented circuit size |A|2n ; second, problem
finding succinct policy size bounded z = |A|2n bounded reward
finding arbitrary policy bound reward. result, problem
checking whether MDP succinct policy size bounded z reward bounded
k least hard problems bound size policy.
latter problem PSPACE-hard; therefore, former PSPACE-hard well.
result holds history-dependent policies: instead n input gates, nT input
gates. observation, following result follows simple corollary result
Mundhenk et al. (2000).
Corollary 3 Checking whether MDP horizon unary notation succinct
policy expected reward greater equal k, size bounded z,
PSPACE-hard, z binary notation.
Membership problem PSPACE appears simple prove.
surprising case stationary policies, question still open
problem bound size. However, problem history-dependent
policies bound size known PSPACE (incidentally, problem
stationary policies infinite horizon EXPTIME-complete, proved Littman, 1997.)
proof result cannot however modified cover addition bound
size. Intuitively, looking policy given reward, also
given size. constraint reward somehow easier check, done locally:
expected reward state obtained summing reward possible next
states. hand, size circuit representing policy cannot checked
whole circuit determined, circuit exponentially large.
proved section EXPTIME-hardness problem
history-dependent policies related open conjecture computational complexity
(P=PSPACE). Namely, proved P = PSPACE implies problem P.
result rephrased as: problem P, P 6= PSPACE. Since EXPTIMEhard problems P, problem EXPTIME-hard, P 6= PSPACE.
conclusion really unlikely: contrary, believed true.
hand, proving problem EXPTIME-hard least hard solving conjecture
open twenty years. intermediate step, proved
P = PSPACE implies existence polynomially-sized history-dependent policies
MDPs.
Theorem 10 P = PSPACE, every succinct MDP horizon unary notation
optimal succinct history-dependent policy size polynomial size MDP
horizon.

571

fiLiberatore

Proof. Let us consider state results sequence actions. Since considered
policies history dependent, possible find optimal choices point
taking current state new initial state MDP, reducing horizon,
determining optimal policy modified MDP.
Since problem checking whether MDP policy given reward
PSPACE (Mundhenk et al., 2000), P assumption. binary search, possible determine expected optimal reward MDP polynomial time. Since
expected optimal reward state computed finding expected optimal
reward MDP, problem polynomial well. function determines
optimal expected reward state therefore represented polynomial circuit.
best action state one leads best possible next states.
determined polynomial space checking, action, possible next states,
determining expected reward. Therefore, best action execute PSPACE,
therefore P assumption. result, optimal policy represented
polynomial circuit.
following corollary immediate consequence.
Corollary 4 P = PSPACE, problem existence succinct history-dependent
policy given expected reward size bound binary notation P.
Proof. P = PSPACE, MDP optimal history-dependent policy polynomial
size. problem therefore solved iterating possible circuits whose size
bounded polynomial. problem therefore solved polynomial
amount memory, therefore PSPACE. assumption, P well.
first look, corollary seems prove problem PSPACE. However,
prove result. shown following related example: problem
p2 -complete P P = NP; however, generally believed p2 -complete
problems NP.
easy consequence result EXPTIME-hardness problem would
imply P 6= PSPACE. Indeed, problem EXPTIME-hard P,
two classes known different thanks theorem Hartmanis Stearns
(1965). result, cannot P=PSPACE, proved imply
problem P. words, problem history-dependent policy existence
size bound binary notation EXPTIME-hard P 6= PSPACE.

8. Conclusions
always possible represent optimal policies succinct MDP using circuits
size polynomial MDP horizon. result affects choice
policies generated executed. Indeed, planning nondeterministic scenario
done two ways:
1. Determine actions execute possible states (i.e., determine
whole policy); state, corresponding action executed;

572

fiOn Polynomial Sized MDP Succinct Policies

2. Determine best action execute initial state only; execute observe
resulting state; find best action new state, etc.
Many algorithms MDPs find representation whole policy,
solve next-action problem directly (Kearns, Mansour, & Ng, 2002). result formally
proves optimal policy cannot always represented polynomial space, unless
polynomial hierarchy collapses. result holds existing algorithms
(such value iteration policy iteration), also algorithm finds
whole policy once.
second solution theoretically optimal, involves finding best action
time step, problem hard, proved Theorem 1. advantage
first solution hard step find optimal policy; finding action
execute state polynomial.
impossibility always representing optimal policies polynomial space raises
new problem: since size physical memory bounded, feasible search
best among policies, among possible store, is,
bounded size available memory size. problem checking whether succinct
MDP succinct policy given size reward proved NPPP -complete,
therefore slightly easier problem without bound, PSPACEhard (Mundhenk et al., 2000). similar result proved slightly different
formalization non-deterministic planning Littman et al. (1998).
complexity result holds policies represented particular succinct
form, circuits giving next action current state (possibly)
history. Nevertheless, different representations policies lead different results. Namely,
algorithms actually find value function (a function determining expected reward
state), considered representation policy states
result execution action listed polynomial time. particular,
algorithms based value iteration, applied form decomposition, find
succinct representation value function. Finding succinct representation
function, given bound size, proved easier finding succinct
policy, states result execution action listed polynomial
time: p2 -complete.
result two consequences: first, problem algorithms solve
smaller complexity class problem bound size; second, policies
represented polynomial space, associated value functions cannot:
exists trade-off complexity ability finding good solutions. result
also interesting characterizes complexity family algorithms (those
determining value function succinct form) rather complexity
problem. result therefore kind efficiency analysis single
algorithm (e.g., big-O running time) inherent complexity problem (e.g.,
NP-completeness). such, similar results complexity proof procedures
(Beame & Pitassi, 1998; Egly & Tompits, 2001). analysis however limited
case exact value functions. Approximation interesting open problem. effects
various restrictions done MDP interesting open problems
well.
573

fiLiberatore

analysis bound size initially done assuming bound
polynomial size instance. justified fact resulting
policy much larger MDP. However, moderate increase may
tolerated. Therefore, considered problem removing assumption
polynomiality bound. new problem, PSPACE-hard
problem bound size is. However, EXPTIME-hard unless P 6= PSPACE.
result shows proving hardness result least hard proving conjecture
open twenty years.
Let us discuss results presented paper relate similar ones
literature. already remarked Introduction, complexity problems related
finding policy necessarily imply policies cannot compactly represented.
Namely, even result undecidability forbid compactness policies. Therefore,
result implied previous complexity results literature.
hand, result non-polynomiality size policies POMDPs already appeared
paper Papadimitriou Tsitsiklis (1987). Namely, proved that, unless
PSPACE = p2 , algorithm mapping POMDPs explicit form
strings polynomial length used compute optimal action.
result basically proves non-polynomiality policies. However, cannot imply ours,
holds POMDPs explicit representation non-positive rewards;
problem PSPACE-hard formalization polynomial (Mundhenk et al.,
2000). precisely, two results cannot derived other. related
results literature non-representability (as opposite compactrepresentability studied paper): Littman (1997) shown (infinite horizon)
plan existence EXPTIME-complete, Littman et al. (1998) shown
problem restricted looping plans PSPACE-complete: result, infinite-horizon
policies cannot represented looping plans unless EXPTIME=PSPACE.
review period paper, different proof impossibility representing optimal policies MDPs polynomial space published (Allender,
Arora, Kearns, Moore, & Russell, 2002). new proof improves ones presented
paper conference version (Liberatore, 2002) two ways: first, condition new result holds PSPACE 6 P/poly instead NP 6 P/poly
PSPACE 6 NPPP ; second, proof holds even approximately optimal policies.
differences (the new proof infinite horizon discount factor, reward
function linear, two-levels Bayes nets used instead circuits) inessential, i.e.,
proofs modified way affected differences.
current paper also contains results problem bound policy value
function.
Let us consider complexity results MDPs literature.
problem deciding whether succinct MDP horizon unary notation policy given reward PSPACE-hard, PSPACE-complete history-dependent
policies. problem bound unary notation size policy
NPPP -complete. class contains class PPP , turn contains whole polynomial hierarchy. Therefore, problem NPPP -complete hard class
574

fiOn Polynomial Sized MDP Succinct Policies

polynomial hierarchy. means bound size unary notation decrease
complexity problem much. hand, bounding size value
function representation decreases complexity more, problem p2 -complete.
conclude, observe negative results (impossibility polynomial policies
hardness results) hold POMDPs, since MDPs special cases POMDPs
everything observable. results, however, apply case
POMDP policy succinct form. case explicit representation
studied Papadimitriou Tsitsiklis (1987) (their results discussed above),
Mundhenk (1999), considered problem deciding whether POMDP explicit
form c-small policy, given c, c-small-ness includes bound size depending
c.

Acknowledgments
Many thanks Michael Littman anonymous referees suggestions. Part
work appeared proceedings Eighteenth National Conference Artificial
Intelligence (AAAI-2002).

References
Allender, E., Arora, S., Kearns, M., Moore, C., & Russell, A. (2002). note representational incompatability function approximation factored dynamics.
Proceedings Sixteenth Annual Conference Neural Information Processing
Systems (NIPS 2002).
Beame, P., & Pitassi, T. (1998). Propositional proof complexity: Past, present
future. Tech. rep. 067, Electronic Colloquium Computational Complexity,
http://www.eccc.uni-trier.de/eccc/.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Boppana, R., & Sipser, M. (1990). complexity finite functions. van Leeuwen,
J. (Ed.), Handbook Theoretical Computer Science, Vol. A, chap. 14, pp. 757804.
Elsevier Science Publishers (North-Holland), Amsterdam.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 194.
Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (2002). Preprocessing intractable
problems. Information Computation, 176 (2), 89120.
Cassandra, A., Littman, M., & Zhang, N. (1997). Incremental pruning: simple, fast, exact
method partially observable Markov decision processes. Proceedings
Thirteenth Conference Uncertainty Artificial Intelligence (UAI97), pp. 5461.
Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.
Computational Intelligence, 5 (3), 142150.

575

fiLiberatore

Dearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning. Artificial Intelligence, 89 (1), 219283.
Egly, U., & Tompits, H. (2001). Proof-complexity results nonmonotonic reasoning. ACM
Transactions Computational Logic, 2 (3), 34038.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide Theory
NP-Completeness. W.H. Freeman Company, San Francisco, Ca.
Hansen, E., & Feng, Z. (2000). Dynamic programming POMDPs using factored state
representation. Proceedings Fifth International Conference Artificial
Intelligence Planning Systems (AIPS 2000), pp. 130139.
Hartmanis, J., & Stearns, R. E. (1965). computational complexity algorithms.
Trans. Amer. Math. Soc. (AMS), 117, 285306.
Johnson, D. S. (1990). catalog complexity classes. van Leeuwen, J. (Ed.), Handbook
Theoretical Computer Science, Vol. A, chap. 2, pp. 67161. Elsevier Science Publishers
(North-Holland), Amsterdam.
Karp, R. M., & Lipton, R. J. (1980). connections non-uniform uniform
complexity classes. Proceedings Twelfth ACM Symposium Theory
Computing (STOC80), pp. 302309.
Kearns, M., Mansour, Y., & Ng, A. (2002). sparse sampling algorithm near-optimal
planning large markov decision processes. Machine Learning, 49 (23), 193208.
Koller, D., & Parr, D. (1999). Computing factored value functions policies structured
MDPs. Proceedings Sixteenth International Joint Conference Artificial
Intelligence (IJCAI99), pp. 13321339.
Liberatore, P. (2001). Monotonic reductions, representative equivalence, compilation
intractable problems. Journal ACM, 48 (6), 10911125.
Liberatore, P. (2002). size MDP factored policies. Proceedings Eighteenth
National Conference Artificial Intelligence (AAAI 2002), pp. 267272.
Littman, M. (1997). Probabilistic propositional planning: representations complexity. Proceedings Fourteenth National Conference Artificial Intelligence
(AAAI97), pp. 748754.
Littman, M., Dean, T., & Kaebling, L. (1995). complexity solving Markov
decision processes. Proceedings Eleventh Annual Conference Uncertainty
Artificial Intelligence (UAI95), pp. 394402.
Littman, M., Goldsmith, J., & Mundhenk, M. (1998). computational complexity
probabilistic planning. Journal Artificial Intelligence Research, 9 (1), 136.
Madani, O., Hanks, S., & Condon, A. (1999). undecidability probabilistic planning
infinite-horizon partially observable Markov decision problems. Proceedings
Sixteenth National Conference Artificial Intelligence (AAAI99), pp. 541548.
Mundhenk, M. (1999). complexity optimal small policies. Tech. rep. 9922, University
Trier.

576

fiOn Polynomial Sized MDP Succinct Policies

Mundhenk, M., Goldsmith, J., Lusena, C., & Allender, E. (2000). Complexity finitehorizon Markov decision processes problems. Journal ACM, 47 (4), 681720.
Papadimitriou, C., & Tsitsiklis, J. (1987). complexity Markov decision processes.
Mathematics Operations Research, 12 (3), 441450.
Puterman, M. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons.
Stockmeyer, L. J. (1976). polynomial-time hierarchy. Theoretical Computer Science,
3, 122.
Zhang, N., & Zhang, W. (2001). Speeding convergence value iteration partially
observable Markov decision processes. Journal Artificial Intelligence Research, 14,
2951.

577

fiJournal Artificial Intelligence Research 21 (2004) 595-629

Submitted 7/03; published 5/04

Concurrent Auctions Across Supply Chain
mosheb@cs.huji.ac.il
noam@cs.huji.ac.il

Moshe Babaio
Noam Nisan
School Computer Science Engineering,
Hebrew University Jerusalem, Jerusalem 91904, Israel

Abstract
recent technological feasibility electronic commerce Internet,
much attention given design electronic markets various types
electronically-tradable goods. markets, however, normally need function
relationship markets related goods, usually downstream upstream supply chain. Thus, example, electronic market rubber tires
trucks likely need strongly inuenced rubber market well
truck market.
paper design protocols exchange information sequence markets
along single supply chain. protocols allow markets function separately, information exchanged ensures ecient global behavior across supply
chain. market forms link supply chain operates double auction,
bids one side double auction come bidders corresponding
segment industry, bids side synthetically generated
protocol express combined information links chain. double
auctions markets several types, study several variants
incentive compatible double auctions, comparing terms eciency
market revenue.

1. Introduction
recent rush towards electronic commerce Internet raises many challenges,
technological conceptual. paper deals conceptual challenge coordination electronic markets. Let us look years technological future
electronic commerce. seems likely following two key challenges
adequately solved industry:
Supply Chain Integration: enterprise information systems businesses
able securely eciently share information inter operate information systems suppliers, customers, partners.
Electronic Markets: Ecient, sophisticated, robust liquid electronic markets
available trade goods segments industry. markets
interactively respond changes supply demand, dynamically changing
trade quantities prices.
interested conceptual question markets related goods
share information. Consider, example, ctional market rubber tires trucks,
two related markets rubber trucks. One imagine following
c
2004
AI Access Foundation. rights reserved.

fiBabaioff & Nisan

simplied supply chain forming: rubber manufacturers placing sell bids rubber
rubber market; tire manufacturers placing buy orders rubber market sell bids
tire market; truck manufacturers placing buy bids tire market selling
trucks truck market; nally customers bidding trucks. One would expect
combination markets together information systems manufacturers
able automatically respond markets changes economically ecient way.
Thus, example, surge demand certain type trucks, raise price
truck market causing manufacturers type truck automatically decide
increase production, consequently automatically raising electronic bids tires.
This, turn, may increase tire prices tire market, etc., etc., leading, eventually
indirectly, still completely automatically, increased rubber production rubber
manufacturers.
Let us emphasize: process described occurs, slowly, normal human trade
combined eects large number self-interested decisions many people
involved supply chain. desire combination participating
information systems electronic markets automatically (without human control)
rapidly (within time-frames electronic commerce), reach similar results
even economically ecient ones humans usually achieve. results
achieved despite fact information systems manufacturers still
self-interested optimizing companys prot global economic eciency.
Seeing invisible hand function normal human economic activity, one would
certainly expect electronic markets reach types results. However, key conceptual design challenge emerges bidders must concurrently active
one market. Tire manufacturers must concurrently participate buyers rubber market sellers tire market. Clearly, quantity rubber wish buy
given price determined amount tires sell given price. Thus,
price bid buying rubber must intimately related price bid selling
tires increase one lead corresponding increase other.
theoretically impossible dene suggested bid one without other. Thus,
two markets operate independently, tires manufacturers able reasonably
participate them. operate sequentially, say rst rubber bought
afterwards tires sold, serious exposure problem emerges: tire manufacturers must
conservative bids rubber, know advance price
get tire market.
One approach handling inter-dependence markets run complete
supply chain single complex huge market. Conceptually spirit
recently popular vertical markets vertical portals try vertically integrate
information trade complete vertical segment industry. integration
markets single complex market results complex optimization problem,
research done address problems pure optimization problems.
centralized solution obvious advantages, also problematic due necessity
concentrating information, communications, decision making single point.
centralization information problematic sense distributed computing
systems economic sense.
596

fiConcurrent Auctions Across Supply Chain

paper suggest alternative approach: supply chain organized
sequence separate markets (which act strategically) communicate among
using xed protocol create distributed mechanism (for general overview
distributed algorithmic mechanism design, refer reader Feigenbaum & Shenker,
2002). similar approach suggested Walsh Wellman (2003) Walsh,
Wellman, Ygge (2000), formulate general problem NP-complete obtain
solutions provably ecient, either computational sense economic
sense. much simpler problem linear supply chain considered Bosch-Domenech
Sunder (1999), provably ecient protocol suggested. also consider
linear supply chain problem, obtain computationally ecient protocols achieve
provably high economic eciency budget balance (or full eciency budget decit).
protocols, intermediate markets along chain transform one good another.
Thus, example, tire manufacturers place bids operation transforming unit
rubber tire. protocol dierent markets assures dierent
markets reach compatible decisions, i.e. amount rubber units tires
allocated equal amount rubber manufactured amount tires
needed. Furthermore, amounts achieve global economic eciency across supply
chain. Finally, result assume manufactures information systems
global knowledge behavior, beyond knowledge cost structure
self-interested (rational) behavior.
paper focuses case simple linear supply chain discrete units goods,
manufacturer able transform single unit one good single unit
another good, incurring cost transformation. consumer obtains value
acquiring single unit nal good. assume agent quasi-linear
utility function, goal maximize utility.
markets takes form double auction (see Friedman & Rust, 1991,
study double auction markets), consider several variants double auctions.
variants address four issues: Incentive Compatibility (IC): double auction rules
motivate self-interested agents (the manufacturers consumers) reveal costs /
values truthfully (we use standard notions dominant strategies IC Mechanism
Design literature; Mas-Collel, Whinston, & Green, 1995; Osborne & Rubinstein, 1994).
Individual Rationality (IR): Every agent strategy ensures non-negative utility,
agents participate voluntarily (Ex-Post individual rationality). Economic Eciency:
desired outcome optimize sum valuations participants. Budget Balance
(BB): payment buyer necessarily equal payment received
seller, wish ensure market mechanism subsidize
trade. Thus, total payment buyers least total amount given
sellers. Myerson Satterthwaite (1983) impossibility result, four conditions
cannot apply simultaneously. paper Parkes, Kalagnanam, Eso (2001) presents
mechanisms combinatorial exchange BB IR fairly ecient fairly
incentive compatible. take dierent approach consider variants always
IR IC trade-o last two conditions eciency budget balance.
deterministic rule previously suggested McAfee (1992), but, surprisingly, rule
turned compatible supply-chain protocols. Therefore suggest two
new randomized double auction rules used supply-chain protocols,
597

fiBabaioff & Nisan

obtain budget balance surplus slight loss eciency. also provide
simulation results comparing eciency budget surplus dierent variants
double auctions.
main contribution paper description two alternative protocols
allow supply chain mechanism formed double auctions operate eciently.
protocols computationally ecient terms communication computation time.
prove protocols applied specic double auction rule (the
Trade Reduction rule), resulting system exhibits following three key properties:
1. Ex-post Individual Rationality Incentive Compatibility (in sense dominant
strategies).
2. High global economic eciency (not optimal, good underlying double
auction guarantees high fraction ecient allocation value).
3. Budget Balance.
new double auctions, even higher global economic eciency achieved
expectation, expected (ex-ante) budget balance rather worst case (ex-post).
protocols use VCG (Vickrey, 1961; Clarke, 1971; Groves, 1973) double
auction rule ecient, maintain IR IC achieve full economic eciency
entire supply chain, budget decit.
Clearly stylized model general supply chain model far
simple realistic model. purpose analyzing model present rst step
towards mechanism design approach supply chain problems. far know
rst design mechanism IC, IR, BB highly ecient supply chain
model.
rest paper structured follows. Section 2 give complete self
contained example organization simple supply chain demonstrate
type calculations information transfer one protocols. Section 3
present model conditions incentive compatibility. Section 4 summarize
properties several variants incentive-compatible double auctions present two
new randomized double auction rules. Section 5 present two alternative protocols
supply chain coordination markets, prove properties achieved
protocols, Section 6 conclude directions future work.

2. Lemonade-Stand Industry
Charlie Brown decided draw vast experience lemonade stand industry
transform whole industry bringing online Internet. Charlie Brown
already struck partnerships strategic players three core segments
industry:
Lemon Pickers: Alice, Ann, Abe pick lemon neighborhood lemon
tree (one lemon maximum per day).
Lemonade Squeezers: Bob, Barb, Boris know squeeze single lemon
make glass lemonade (one glass maximum per day).
598

fiConcurrent Auctions Across Supply Chain

Lemonade Consumers: Chris, Carol, Cindy want buy one glass lemonade
(per day).
Charlie Brown obtained preliminary version paper built Internet
systems accordingly, using Symmetric Protocol suggested here. Charlie Brown
created three communicating electronic markets: lemon market Alice,
Anna, Abe sell lemons. squeezing market Bob, Barb, Boris
oer squeezing services, juice market Chris , Carol, Cindy buy
lemonade.
rst day operations participants logged market
entered bid: Alice asking $3 order pick lemon, Ann wanted $6,
Abe (who living farthest tree) wanted $7. Bob, Barb, Boris asking
for, respectively, $1, $3, $6 order squeeze lemon, Chris, Carol, Cindy,
willing pay, respectively, $12, $11, $7 glass lemonade. Figure 1
presents three markets, supply curves lemon market (S L ) squeezing
market (S LJ ), demand curve juice market (DJ ). Knowing auction
works optimal bidding strategy report true value cost, agent
reports truthfully. Let us follow operation system see manages reach
socially ecient allocation decide many lemons picked squeezed
lemonade.
Lemon Market
Supply Demand
(S L )
(DL )
3
6
7

Squeezing Market
Supply Demand
(S LJ )
(DLJ )
1
3
6

Juice Market
Supply Demand
(S J )
(DJ )
12
11
7

Figure 1: Supply Chain bids
rst stage, markets send information two phases.
rst phase, lemon market aggregates supply curve lemons, L , sends
information squeezing market. squeezing market aggregates supply curve
squeezing services, LJ , adds vector, point-wise, L , sends sum
juice market. juice market, sum represents supply curve juice, J ,
aggregated complete supply chain. seen Figure 2, cost rst
glass lemonade $4, since rst lemon cost $3 squeezing operation cost
$1.
second phase, juice market sends demand curve juice, DJ
squeezing market, subtracts it, point wise, supply curve squeezing services, sending dierence vector lemon market, interpreted
demand curve lemons, DL , aggregated complete supply chain. seen
Figure 3, demand rst lemon $11, since rst glass lemonade
demand $12 squeezing operation cost $1.
net demand curve squeezing services, DLJ calculated
squeezing market DJ L . seen Figure 4, demand rst
599

fiBabaioff & Nisan

Lemon Market
Supply Demand
(S L )
(DL )
3
6
7

Squeezing Market
Supply Demand
(S LJ )
(DLJ )
1
3
6
SL

Juice Market
Supply Demand
(S J )
(DJ )
4
12
9
11
13
7

L +S LJ





Figure 2: Supply Chain supply graphs propagation
Lemon Market
Supply Demand
(S L )
(DL )
3
11
6
8
7
1

Squeezing Market
Supply Demand
(S LJ )
(DLJ )
1
3
6

J LJ

Juice Market
Supply Demand
(S J )
(DJ )
4
12
9
11
13
7
DJ





Figure 3: Supply Chain demand graphs propogation
squeezing operation $9 since rst glass lemonade demand $12 rst
lemon cost $3. means lowest cost squeezing operation less
$9, least one glass lemonade manufactured.
Lemon Market
Supply Demand
(S L )
(DL )
3
11
6
8
7
1

Squeezing Market
Supply Demand
(S LJ )
(DLJ )
1
9
3
5
6
0

Juice Market
Supply Demand
(S J )
(DJ )
4
12
9
11
13
7

Figure 4: Supply Chain constructing supply demand graphs
point three markets supply curve demand curve,
market conduct double auction.
Internet startup, Charlie Brown decided subsidize trade
markets, ignoring sections paper aim eliminate budget decit
markets. market thus uses VCG (Vickrey, 1961; Clarke, 1971; Groves, 1973)
double auction rule1 , derived Vickrey, Clarke Groves general auction scheme.
VCG double auction picks highest value allocation market, winners
agents socially ecient allocation. ecient allocation market
includes two highest value (lowest cost) bidders, since two trades
positive gain, third trade negative gain. VCG double auction charges
1. formal denition VCG double auction rule appears Section 4

600

fiConcurrent Auctions Across Supply Chain

consumer (demand side) minimal value agent must bid order
ecient allocation. Similarly, supplier (supply side) receives payment equals
maximal cost may bid still ecient allocation. payment takes
account competition agents class competitiveness
bid respect bids agents classes. payment scheme,
best strategy agent bid truthfully. lemon market, two lemons sold
(by Alice Ann) $7 = min($7, $8) (in order win, must bid cost lower
cost Abe, rst non-winning lemon supplier, $7. must also bid
cost lower $8, otherwise cost lemon high match lowest
demand lemon). squeezing market, two squeezing contracts awarded (to
Bob Barb) $5 = min($6, $5) (since one bids cost higher $5,
demand squeezing operation, squeezing operation cost Boris
$6, higher $5). juice market, VCG rule awards two glasses
lemonade (to Chris Carol) price $9 = max($7, $9) (since must bid
least $9 match supply juice, do, also defeat Cindy).
Charlie Brown thrilled: dierent markets reached allocation
amount, 2, which, veried indeed social optimum: Societys net gains
trade system (12 + 11) (1 + 3) (3 + 6) = $10, cant beaten. Charlie
Browns investors somewhat worried fact system subsidized every glass
lemonade $3 (= 7 + 5 9), Charlie Brown assures changing doubleauction rules one double auction rules suggested paper lead
budget balance even surplus, maintaining high social gain. nine trading
partners evaluated carefully operation chain markets assured
best served always bidding true cost structure.

3. IC Mechanisms Single-Minded Agents
Section 3.1 presents abstract model agents partition set outcomes two,
case win case loss. agent valuation case
wins, valuation represented single parameter. double auction model
supply chain model consider special cases model. main theorem
section presents necessary sucient conditions incentive compatibility
abstract model. Mualem Nisan (2002) presented theorem context
combinatorial auction agents desire one specic bundle publicly known
(the Known Single Minded Model). theorem derived general result
agents private bundles (the Single Minded Model), proved Lehmann, OCallaghan,
Shoham (2002). Similar result dierent general model one-parameter agents
Archer Tardos (2001) appears Computer Science literature. completeness,
present results relevant single-minded agents model.
section 3.2 farther restrict model single-minded agents case
sets agents replace other, characterize non-discriminating mechanisms
case.
601

fiBabaioff & Nisan

3.1 General Model
Single-Minded agents model, nite set agents N set outcomes
O. agent N , partitioned two disjoint sets OiW OiL , outcomes
Wins outcomes Loses, respectively (this partition public
knowledge). ordered set values space Vi case wins. Agent
private value vi Vi OiW (he wins) value 0 OiL
(he loses). private values private information, information
publicly known participants mechanism (and known rest
information public). technical reasons assume vi1 , vi2 Vi ,
vi1 > vi2 , exist vi3 Vi vi1 > vi3 > vi2 (For example Vi set
Real numbers Rational numbers). agent wins pays pi quasi-linear
utility ui = vi pi , normalized mechanisms pays 0 0 utility loses.
assume agents self-interested try maximize utility. model,
outcome one-to-one mapping set winners called allocation

A, = {i N |o OiW }. say thatthe allocation ecient iA vi maximized.
vi
eciency allocation iA v , ecient allocation. Let V
iA



set possible values agents, V = Vi . work mechanisms
agents required report values, mechanism decides allocation
payments agents deterministic way. reported value bi Vi agent
called bid agent might dierent private value vi . Let b V
bids agents. allocation rule R decides allocation according reported
values b V , R function R : V O. payment rule P decides payment pi agent
i, P function P : V RN . mechanism denes allocation payment rule,

= (R, P ). Mechanism Budget Balanced (BB) pi 0 bids b V .
Incentive-Compatible (IC) dominant strategies agent i, bidding vi maximizes
utility possible bids agents. normalized losing agents
payment (and utility) 0. (ex-post) Individually Rational (IR) agent
value vi , bid bi bids bi , ui 0 possible bids
agents. Note normalized incentive-compatible mechanism individually
rational (since truthful bidding ensures non negative utility).
present necessary sucient conditions mechanism incentive
compatible dominant strategies single-minded agents model. conditions
later used prove properties double auctions supply chain mechanisms
created protocols.
bids b V denote b = (bi , bi ) bi bids agents i.
Denition 1. Allocation rule R Bid Monotonic bids b V , agent
two possible bids i, bi > bi , allocation R(bi , bi ) also
allocation R(bi , bi ).
bid monotonic allocation rule ensures winning agent becomes loser
improving bid. following observation direct result denition.
Observation 3.1. Let R bid monotonic allocation rule, let b V set bids
agents let agent bid bi .
602

fiConcurrent Auctions Across Supply Chain

exists bid bi allocation R(bi , bi ), exists
critical value Ci wins bids bi > Ci loses bids bi < Ci (Ci
independent bid bi )2 .
Note bid equal critical value, observation say
agent wins not.
Lemma 3.2. Let normalized IC mechanism allocation rule R. R
Bid Monotonic.
Proof. Assume contradiction auction normalized IC (thus also IR)
allocation bid monotonic. exists agent two values bH > bL ,
wins auction pays PL bids bL , loses auction pays zero
bids bH . mechanism IR therefore bL PL 0. true value bH
gain misreporting private value: reports true value loses auction
utility zero, bids bL wins auction pays PL . case utility
bH PL > bL PL 0 contradiction assumption auction IC.
Theorem 3.3. normalized mechanism allocation rule R IC R
Bid Monotonic trading agent pays critical value Ci (pi = Ci ).
Proof. Case if: Assume normalized, R bid monotonic agent value vi
pays Ci wins. rst show auction IR, show receives non-negative
utility bidding truthfully. loses, pays zero zero utility. wins
auction bidding truthfully, Observation 3.1, vi Ci = pi , hence utility
vi pi = vi Ci 0.
prove IC prove cannot improve utility misreporting value. Consider case agent wins auction bidding true value vi . bids
untruthfully loses, gets zero utility, IR cannot better
utility truthful bid. bids untruthfully wins auction, since still
pays critical value Ci utility remains same.
consider case loses auction bidding truthfully. utility
zero vi Ci Observation 3.1. bids untruthfully loses, utility remains
zero. bids untruthfully wins, utility vi Ci 0. cases, shown
agent cannot improve utility bidding untruthfully, thus proving
IC.
Case if: Assume auction normalized IC (thus also IR). Lemma 3.2
allocation rule bid monotonic, need prove agent value vi
must pay critical value Ci . Assume pi = Ci agent i. pi > Ci ,
pi > vi > Ci bids truthfully, Observation 3.1 wins utility
vi pi < 0 contradicts individual rationality. pi < Ci , pi < vi < Ci
Observation 3.1 loses zero utility bids truthfully. misreports value
bidding bi > Ci , would win auction utility vi pi > 0 contradicts
incentive compatibility.
normalized IC mechanisms, allocation rule bid monotonic
uniquely denes critical values agents thus payments.
2. Ci = always wins

603

fiBabaioff & Nisan

Observation 3.4. Let 1 2 two normalized IC mechanisms
allocation rule. 1 2 must payment rule, means 1
2 mechanisms.
3.2 Non-Discriminating Mechanisms
interested subclass mechanisms single-minded agents
discriminate agents roll outcome. agents
always replace other.
Denition 2. Agent class agent j, every allocation
j
/ A, allocation = \ {i} {j} also O, every allocation

/ j A, allocation = \ {j} {i} also O.
Note class agent equivalence class.
Denition 3. Let mechanism, class agents i, j two agents
class . non-discriminating identity bids b V , wins bids
bi bj > bi j also wins.
non-discriminating pricing bids b V every class
value pT , agent class bid bi bi > pT wins pays pT .
non-discriminating (ND) non-discriminating identity nondiscriminating pricing.
Non-discriminating mechanisms also called Envy-Free mechanisms Goldberg
Hartline (2003), since loser envies winner fact auction
price paid. paper consider mechanisms non-discriminating
identity, means agents class picked bid order,
high low. Also note IR non-discriminating pricing agent
class bids pT must lose auction (otherwise negative utility
contradicting IR).
following observation direct result Theorem 3.3.
Observation 3.5. Let normalized mechanism IC ND. Let pT payment
winners class . critical value winning agents class pT .
normalized, IC ND mechanisms, prove payment winners
class independent bids.
Lemma 3.6. Let normalized IC ND mechanism. every agent class ,
agent j class wins pays pT bids bj > pT winning agent class
bids bi pT , j also wins pays pT bids bi > bi .
Proof. Since normalized IC, Theorem 3.3 bid monotonic
Observation 3.5 pT critical value j. Observation 3.1 pT independent
bid bi , must pay pT bids bi > bi . Since non-discriminating, j must
also win (since bj > pT ) pay pT .
604

fiConcurrent Auctions Across Supply Chain

4. Incentive Compatible Double Auctions
markets along supply chain performs double auction (for study
double auctions see Friedman & Rust, 1991). protocols supply chains work
wide variety double auction rules, section present several double
auction rules suggested literature, two new randomized double auction
rules later use create supply chain mechanisms. double auction rule decides
trading agents payments agents.
rst give description Double Auction (DA) model. single-minded
agents model following specications possible outcomes, value spaces classes
agents. homogeneous good g traded discrete quantities,
agents one two classes agents. agent either seller buyer one
unit good g. Seller single unit g non-negative cost si (vi = si )
sells unit. Buyer non-negative value bi (vi = bi ) receives one unit
good g. set possible allocations set materially balanced allocations,
allocations number sellers buyers.
DA begins agents reporting values one unit good. seller
reports cost Si might dierent real cost si . buyer reports value
Bi might dierent real value bi . DA rules non-discriminating
identity, rst construct supply demand curves sorting supply bids
s.t. S1 S2 . . . demand bids s.t. B1 B2 . . .. agents report truthfully
order maximize eciency, trade quantity q set, trading agents must
rst q sellers rst q buyers. optimal trade quantity l dened
maximal Bl Sl . Trading l units maximizes eciency agents bids
agents true values. double auctions consider non-trading agents pay zero.
setting trade quantity optimal trade quantity, real markets proceed
choosing market clearing price anywhere [Sl , Bl ]. example setting price
(Bl + Sl )/2 1/2-Double Auction special case k-Double
Auction (Wilson, 1985; Chatterjee & Samuelson, 1983; Satterthwaite & Williams, 1989).
general, k-Double Auction double auction auction begins,
parameter k chosen k [0, 1]. k used calculate clearing price P =
k Sl + (1 k) Bl . l units good traded uniform price P sellers
buyers. pricing scheme incentive compatible (in dominant strategies),
additional strong assumptions agents values (all known distribution),
shown perform reasonably well many cases strategic behavior
participants (Rustichini, Satterthwaite, & Williams, 1994; Satterthwaite & Williams,
1991). Nevertheless, even assuming uniform distribution agents values,
non zero eciency loss Bayesian Nash equilibrium achieved (and equilibrium
concept much weaker dominant strategies equilibrium discuss paper).
One may alternatively use VCG Double Auction rule, also sets trade
quantity optimal trade quantity l. rule non-discriminating, l winning buyers
pay pB = max(Sl , Bl+1 ) l winning sellers receive pS = min(Sl+1 , Bl ). VCG mechanisms
(Vickrey, 1961; Clarke, 1971; Groves, 1973) general, VCG DA mechanism
special case, IR IC. Incentive compatibility leads maximal eciency since
trade optimal size l, also leads budget decit since pB pS .
605

fiBabaioff & Nisan

Observation 3.4 normalized, IC ecient mechanism must payments
VCG DA thus mechanism budget-balanced.
Myerson Satterthwaite (1983) shown long individual rationality
participation constraints met (e.g. non-traders pay 0), incentive compatible
mechanism always achieves ecient outcome lead budget decit (even
general model weaker solution concept consider). turn look
several normalized IC double auction rules achieve budget balance (sometimes
even surplus) price achieving slightly sub-optimal eciency (by reducing trade
quantity).
simplest auction property Trade Reduction (TR) DA.
non-discriminating auction, l 1 units good traded. trading buyer pays
pB = Bl , trading seller receives pS = Sl . Since Bl Sl auction BB.
rule extension Trade Reduction DA rule, previously suggested

+B
McAfee (1992). McAfees DA rule, suggested clearing price p = l+1 2 l+1
accepted l buyer seller (p [Sl , Bl ]) l units good traded
price p, otherwise TR rule used. Trade Reduction DA rule McAfees
rule normalized, bid monotonic payments critical values, thus
IC (and IR) applying Theorem 3.3. rules eciency least (l 1)/l
agents values, since unit lowest trade value might traded.
McAfees rule turned compatible protocols expand
appendix A.
suggest two new normalized randomized double auctions, capture
tradeo auction eciency budget balance one parameter . Unlike
McAfees rule, rules used supply-chain protocols, achieve
higher eciency Trade Reduction DA expectation. carefully chosen
parameter also achieve exact budget balance (zero balance) expectation (Ex ante
Budget-Balanced).
Reduction DA, xed 0 1, bids submitted
probability Trade Reduction DA rule used, probability 1
VCG DA rule used. randomized double auction universally incentive compatible
(dened Nisan & Ronen, 1999), means agents bid truthfully even
know randomization result. seen fact bidding truthfully
dominant strategy Trade Reduction DA rule VCG DA rule. also
normalized ND since Trade Reduction DA VCG DA normalized ND.
Payment DA another randomized double auction distribution allocation former auction (and therefore expected eciency),
payment agent expected payment Reduction DA (so
expected budget auctions same). auction, parameter chosen
Reduction DA. bids submitted allocation payments
decided. l 1 units traded buyers, paying Bl + (1 ) max(Bl+1 , Sl ),
sellers, receiving Sl + (1 ) min(Sl+1 , Bl ). probability another
unit good traded l buyer pays max(Bl+1 , Sl ) l seller
receives min(Sl+1 , Bl ). auction discriminating since l buyer pays dierent price rest winning buyers wins auction (and holds
sellers). Payment DA much lower variance agents payments
606

fiConcurrent Auctions Across Supply Chain

Reduction DA, hides randomized character almost
agents. allocation payment agents l buyer seller independent
coin toss, thus zero variance. auction individually rational incentive
compatible; universally.
Theorem 4.1. Payment DA incentive compatible randomized DA,
universally incentive compatible randomized DA.
Proof. Since Reduction DA universally incentive compatible, truth telling maximizes expected utility agent Reduction DA. Payment DA
probability distributions allocation payments Reduction DA,
therefore truth telling maximizes expected utility agent Payment DA
well.
Payment DA universally incentive compatible randomized DA. Assume
seller one rst l 1 sellers knows randomization result
VCG allocation, facing deterministic auction. Observation 3.4
unique payment rule ensures dominant strategy IC normalized mechanism
ecient allocation (the VCG allocation), VCG payment rule.
Payment DA normalized gives dierent payment agent, therefore cannot
incentive compatible dominant strategies.
auctions eciency least (l 1)/l, since least l 1 l units
traded. grows zero one, expected revenue increases negative
positive expected eciency decreases (linearly). distribution agents
values known prior beginning auction, parameter = (D)
chosen expected revenue zero (dierent distributions dierent values
(D), paper solve problem nding (D) analytically). denote
value (D). uniform distribution agents values [0, 1] denote
value .
Table 1 present summary properties double auction rules.
DA rule
k-DA
VCG
Trade Reduction
(D) Reduction
(D) P ayment
McAfee

Incentive compatible

yes
yes
yes, universally
yes, universally
yes

Revenue
0
decit
surplus
0 (expected)
0 (expected)
surplus

eciency loss
0
0
LFT
(D) LF (expected)
(D) LF (expected)
LFT

Table 1: Double Auction Rules comparison table
Notes : a) k-DA assume agents bid truthfully, not, non zero eciency
loss equilibrium. b) LFT means Least Favorable Trade Bl Sl .

run simulations comparing dierent DA rules respect market


revenue ( pi ) total social eciency ( iA vi ). Figures 5 6 show
results average 100 random executions auction given number
607

fiBabaioff & Nisan

buyers sellers, values drawn uniformly independently random [0, 1].
calculated Reduction DA eciency revenue ratios {0.25, 0.5, 0.75}.
Note expectation, results would obtained Payment
DA, since expected eciency revenue Reduction DA.
calculated = (U ) (where U uniform distribution agents values [0, 1])
simulations using binary search. Note gures, values Trade
Reduction DA, dierent Reduction DA, VCG DA well Reduction
DA, lay linear curve. point representing McAfees DA lays line
simulations lower eciency Reduction DA. Reduction
DA extracts eciency McAfees DA, still budget balanced.
simulations also show symmetric case number buyers sellers
same, auctions extract high fraction eciency (more 99%).
hand asymmetric case number buyers much larger
number sellers, Trade Reduction DA well McAfees DA extract signicantly
lower eciency Reduction DA (about 5% less).
0.7

0.6

0.5

0.4

Revenue

0.3

0.2

0.1
VCG
Trade Reduction

0

0.25 Reduction
0.5 Reduction

0.1

0.75 Reduction
McAfee

0.2

a*
0.3
6.06

6.07

6.08

6.09
6.1
Social Efficiency

6.11

6.12

6.13

6.14

Figure 5: 25 buyers 25 sellers
Revenue/Eciency tradeo simulations results

5. Supply Chain Protocols
begin presenting linear supply chain model, unit initial good
converted unit nal good sequence unit unit conversions.
supply chain model single-minded agents model following specications
possible outcomes, value spaces classes agents. ordered set G (|G| = t)
homogeneous goods traded discrete quantities. agent one
following + 1 classes. initial supplier supply one unit rst good
608

fiConcurrent Auctions Across Supply Chain

0.7

0.6

0.5

Revenue

0.4

0.3

0.2

VCG
Trade Reduction

0.1

0.25 Reduction
0.5 Reduction
0.75 Reduction

0

McAfee
a*

0.1
2.1

2.15

2.2
Social Efficiency

2.25

2.3

2.35

Figure 6: 50 buyers 5 sellers
Revenue/Eciency tradeo simulations results

non-negative cost si (vi = si ) trades. converter one unit good r {1, . . . , t1}
one unit good r + 1 non-negative cost si (vi = si ) converts unit.
consumer non-negative value bi (vi = bi ) receives one unit nal good.
market set agents class. allocation materially balanced
good, number units produced number units consumed.
means number winners class agents same. Clearly number
winners market cannot exceed minimal number bidders market n,
assume markets n bidders (n found single message
market next market. message includes minimal size seen
far. consumer market sends n backwards chain markets).
suggest two protocols used conduct auction chain markets
distributed manner: Symmetric Protocol Pivot Protocol.
protocols run servers connected network linear chain topology. server
represents one market receives bids one class agents (suppliers, consumers
converters specied good next good chain). servers
act strategically, follow specied protocols (they owned entity).
denote supply market 1 conversion market good r
following good r + 1 rr+1 . consumer (demand) market marked .
conversion markets connected bi-directional communication channels
market supplies input good market demands output good.
denote supply demand curves good r r Dr respectively,
supply demand curves conversion good r good r + 1 rr+1
rr+1 respectively.
609

fiBabaioff & Nisan

agents must bid xed deadline. input protocols supply
bids rst good, conversion bids conversion markets demand
bids nal good. protocols decide allocation payments trading
agents (the protocols normalized, losing agents pay zero). protocols generic
operate various normalized DA rules.
Symmetric Protocol, markets conducts double auction, constructing demand supply curves. agent views supply chain auction
double auction since market conducts double auction central market makes allocation payments decisions. order protocol create
materially balanced allocation, restriction imposed double auction rule
used Symmetric Protocol, later describe. protocol use either discriminating non-discriminating DA rule. present protocol using abstract
discriminating DA rule takes supply demand curves inputs, returns
trade quantity q two price vectors, PS sellers PB buyers.
Pivot Protocol, one market (the demand market) constructs demand
supply curves, market applies double auction allocation payments rule,
sends results auction predecessor. market uses information
receives bids market send predecessor information used
calculate allocation payments. Unlike Symmetric Protocol, Pivot Protocol
must use non-discriminating DA rule, creates materially balanced allocation
DA rule. many DA rules, protocol improved much
lower communication burden Symmetric Protocol, show Section 5.3.
present protocol using abstract non-discriminating double auction rule takes
supply demand curves inputs, returns trade quantity q single price
PS trading sellers another price PB trading buyers.
5.1 Symmetric Protocol
seen example Section 2, bids submitted Symmetric
Protocol begins supply curve propagation along supply chain, supply
market consumer market. protocol continues demand curve propagation
along supply chain way (demand curves propagation may done concurrently supply curves propagation). process markets builds
supply demand curves information receives. point
markets supply demand curves, double auction conducted. rule
randomized, assume random coins public, means markets
access random coins (a public coin created one market tossing
coin propagating result markets along supply chain). formal protocol supply market 1 , conversion markets rr+1 demand
market described Figures 7, 8 9, respectively.
DA rule decides trade size market according supply demand
curves created protocol. allocation materially balanced case
trade size decided markets.
Denition 4. Let S(M ) D(M ) supply demand curves created Symmetric Protocol market . Let q(M ) trade size obtained applying DA rule
610

fiConcurrent Auctions Across Supply Chain

Symmetric Protocol Supply Market 1 .
1. 1 sort list supply bids non-decreasing order.
2. Send 1 demand market 12 .
3. Receive D1 demand market 12 .
4. Apply DA Rule (S 1 , D1 ) obtain (q, PS , PB ).
5. Output: q lowest bidders sell unit pay vector PS .

Figure 7: Symmetric Protocol Supply Market

Symmetric Protocol Conversion Market rr+1
1. rr+1 sort list supply bids non-decreasing order.
2. receiving r r1r ,
send r+1 = r + rr+1 r+1r+2 .
3. receiving Dr+1 r+1r+2 ,
send Dr = Dr+1 rr+1 r1r .
4. Construct market demand curve Drr+1 = Dr+1 r .
5. Apply DA Rule (S rr+1 , Drr+1 ) obtain (q, PS , PB ).
6. Output: q lowest bidders convert unit pay vector PS .

Figure 8: Symmetric Protocol Conversion Market

Symmetric Protocol Demand Market
1. Dt sort list demand bids non-increasing order.
2. Send Dt supply market t1t .
3. Receive supply market t1t .
4. Apply DA Rule (S , Dt ) obtain (q, PS , PB ).
5. Output: q highest bidders buy unit pay vector PB .

Figure 9: Symmetric Protocol Demand Market

611

fiBabaioff & Nisan

R (S(M ), D(M )). DA rule R called consistent q(M 1) = q(M 2) two
markets 1 2.
following Lemma shows optimal trade size markets same,
rule set trade size function optimal trade size consistent.
Lemma 5.1. Let l(M ) optimal trade size market . two markets 1
2, l(M 1) = l(M 2). denote optimal trade size l.
Proof. denition optimal trade size, l(M ) maximal index
Bl(M ) (M ) Sl(M ) (M ). enough show two markets 1 2,
B(M 1) S(M 1) = B(M 2) S(M 2) (as vectors).
r:

mm+1 ) r rr+1 =
rr+1 rr+1 = (Dr+1 r ) rr+1 = (Dt t1
m=r+1

mm+1 ) =
(S r + rr+1 + t1
m=r+1
Similar argument shows D1 1 = Dt
conclude trade size qM decided DA rule R market
function optimal trade size lM = l, R consistent. Clearly consistent DA
rule always creates materially balanced allocation.
Since trade size decided VCG DA rule l, rule consistent.
Similarly Trade Reduction DA rule consistent since trade size l 1.
double auction rules consistent, McAfees DA example inconsistent rule (a
specic example inconsistency presented appendix A). intuitive reason
consistent trade size dependent comparison
function l + 1 bids l bids, might dierent markets.
hand two new randomized DA, Reduction DA Payment DA
consistent. markets share public coin, either trade size l 1
markets, l markets.
Following main theorem regarding Symmetric Protocol.
Theorem 5.2. normalized DA rule R consistent used Symmetric
Protocol create supply chain mechanism normalized materially balanced.
R also IC mechanism created Symmetric Protocol IC (thus also IR)
eciency eciency R. R also non-discriminating, mechanism
also non-discriminating.
Proof. Since R consistent allocation materially balanced denition. payments normalized denition, show R IC mechanism also IC.
Theorem 3.3 DA rule R bid monotonic payments critical values.
agent, supply demand curves (disregarding bid) independent
bid way built protocol. mechanism bid monotonic
payments critical values (from agent point view, submits bid
DA), therefore direction Theorem 3.3 supply chain mechanism IC
(and IR). IC way costs aggregated protocol, eciency
mechanism eciency R. R non-discriminating,
payments winning agents market same, mechanism
non-discriminating.
612

fiConcurrent Auctions Across Supply Chain

Note normalized, IC consistent DA rule must used
markets mechanism materially balanced, normalized IC. DA rule
function supply demand curves market. two supplier
conversion markets get supply bids, also get demand curves
consistency must allocation. Since normalized IC
Observation 3.4 must also payment rule. demand market must
DA rule supply market, since case converters
zero cost, markets supply demand curves. Applying
argument presented markets, proves must DA rule.
5.2 Pivot Protocol
Pivot Protocol supply chain protocol creates normalized materially balanced
supply chain mechanism non-discriminating normalized double auction rule.
DA rule also incentive compatible protocol creates incentive compatible supply
chain mechanism. Unlike Symmetric Protocol, Pivot Protocol restricted
using consistent double auction rules, sense less restricted Symmetric
Protocol. hand, unlike Symmetric Protocol restricted use nondiscriminating double auction rules. use randomized double auctions also
natural Pivot Protocol, since one market (the Pivot market) uses random
coins need public coins created distribution random coins
one market markets.
dening Pivot Protocol formally, explain execution Lemonadestand industry example Section 2. Symmetric Protocol, protocol begins
supply curve propagation along supply chain juice market (see Figure 2).
Figure 10 presents information propagation Pivot Protocol supply curve
propagation stage, explained below.
Lemon Market
Supply Demand
(S L )
(DL )
3
6
8
7

Squeezing Market
Supply Demand
(S LJ )
(DLJ )
1
3
5
6

(V,q)=(8,2)

Juice Market
Supply Demand
(S J )
(DJ )
4
12
9
11
13
7

(V,q)=(11,2)





Figure 10: Pivot Protocol Lemonade-stand industry
Unlike Symmetric Protocol, demand curve propagated backwards,
rather, juice market, supply demand curves stage,
conducts non-discriminating double auction. example, running VCG double
auction results ecient trade size 2 juice market. trade size propagated
markets set trade size market along supply chain,
ensuring materially balanced allocation (the trade size denoted q Figure 10).
VCG DA rule charges two winning consumers juice market $9 = max($9, $7)
613

fiBabaioff & Nisan

(since must bid least $9 match supply juice, do, also
defeat Cindy, losing consumer). single agent juice supplier role
juice market, rather, juice supplier aggregation lemon picker juice
squeezer. use price double auction would charged supplier
juice market nd prices lemon pickers juice squeezers. market sends
predecessor chain highest price winning agents willing pay one unit
input good. supplier juice market paid $11 = min($13, $11),
maximal cost winning supplier charge juice. So, $11 highest cost
juice, juice market informs squeezing market trade size 2
price juice exceed $11 (V gure marks propagated value).
squeezing market informs lemon market trade size 2 price
lemon market $8 = $11 $3, since highest cost winner
squeezing market $3. price squeezing market cannot exceed $5 = $11 $6,
since price juice cannot exceed $11 cost lemon winning lemon picker
might high $6. Also, price squeezing market cannot exceed $6, cost
loser market, price set $5 = min($6, $5). price lemon
market set $7 = min($7, $8), since price cannot exceed $7, cost loser
market, lower propagated maximal cost $8.
Note use VCG double auction rule, Lemonade-stand industry
allocation payments Pivot Protocol exactly allocation
payments Symmetric Protocol presented Section 2. later show (Theorem 5.5)
general phenomena happens normalized DA incentive compatible
rule used protocols, is, DA rule non-discriminating
consistent (if DA rule inconsistent, like McAfees DA rule, allocation created
Symmetric Protocol materially balanced. hand rule
discriminating, cannot used Pivot Protocol since dierent maximal
unit costs dierent winning agents market).
turn general denition Pivot Protocol. Pivot Protocol, one
markets chosen pivot double auction held there. market
may chosen pivot; describe case pivot consumer market.
bids submitted, protocol begins supply curves propagation along
supply chain Symmetric Protocol. point consumer market
supply demand curves, non-discriminating double auction conducted
market. DA sets trade size, price mechanism charges consumers,
highest cost unit good (suppliers price), sent predecessor
market consumer market. that, starting consumer market
viewed demand market predecessor, demand markets sends
supply market size trade highest price demand market willing
pay one unit input good, without reducing trade quantity. payment
market set maximal cost market results trade size,
calculated minimum two terms. rst term dierence
propagated maximal cost market output maximal cost market input.
second term highest cost loser market. show
payment scheme creates normalized, incentive compatible non-discriminating supply
614

fiConcurrent Auctions Across Supply Chain

Pivot Protocol Pivot Market
1. Dt sort list demand bids non-increasing order.
2. Receive supply market t1t .
3. apply DA Rule (S , Dt ) obtain (q, PS , PB ).
// Send results markets:
4. Send (PS , q) market t1t .
5. Output: q highest bidders buy price PB .

Figure 11: Pivot Protocol Pivot Market
Pivot Protocol Conversion Market rr+1
1. rr+1 sort list supply bids non-decreasing order.
2. receiving r r1r ,
send r+1 = r + rr+1 r+1r+2 .
3. receiving pair (V, q) r+1r+2 ,
send (V Sqrr+1 , q) r1r .

rr+1
4. Output: q lowest bidders sell price min(V Sqr , Sq+1
).

Figure 12: Pivot Protocol Conversion Market
chain mechanism, given normalized, incentive compatible non-discriminating double
auction rule.
formal protocol pivot market, conversion markets supply market
presented Figures 11, 12 13, respectively.
following theorem shows properties mechanism created Pivot
Protocol derived properties DA rule used pivot market.
Theorem 5.3. normalized DA rule R non-discriminating used
Pivot Protocol create normalized supply chain mechanism materially balanced
non-discriminating. R also IC mechanism created Pivot Protocol
IC (thus also IR) eciency eciency R.
Pivot Protocol Supply Market 1
1.
2.
3.
4.

1 sort list supply bids non-decreasing order.
Send 1 demand market 12 .
Receive pair (V, q) demand market 12 .
1
).
Output: q lowest bidders sell price min(V, Sq+1

Figure 13: Pivot Protocol Supply Market
615

fiBabaioff & Nisan

Proof. protocol denition mechanism materially balanced, normalized
non-discriminating. Assume DA rule normalized IC. Then, since
protocol denition supply curve pivot market independent bids
consumers, mechanism normalized IC consumers. prove
IC initial suppliers, proof converters similar (so omitted),
conclude mechanism normalized IC (thus also IR). show
mechanism bid monotonic payments critical values, thus Theorem 3.3
mechanism IC.
Assume supplier bids Si1 wins. Assume trade size q (q since wins)
PS price sellers pivot market. allocation bid monotonic since
bids lower cost still one q lowest bidding suppliers bid change
reduce cost q lowest bidding sellers pivot market. applying
Lemma 3.6 DA rule, PS remains same, corresponding seller bid still
lower PS remains winner. Let Ci1 critical value supplier i.
Pivot Protocol payment winning supplier supply market
p1i

= min(PS

t1


1
Sqrr+1 , Sq+1
)

r=1

need show Ci1 = p1i prove payments critical values. Assume
changes bid X j bid market new order
bids. show X < p1i wins X > p1i loses (note
consider costs, values). means p1i critical value supplier Si1 win
auction.
1
supplier still one q highest bidders
X < p1i X < Sq+1
market (j q). way protocol builds supply curve
pivot market, bid change change q highest sellers bids pivot
market. q highest sellers bids pivot market still PS , since

rr+1 < PS , even q bidder assumption
originally Sq1 + t1
t1 rr+1 r=1 q
< PS . Lemma 3.6 applied DA rule, PS remains
X + r=1 Sq
since corresponding seller bid PS , supplier wins auction.
proves Ci1 p1i
1 , j q + 1. win bidding X must
X > Sq+1


rr+1
rr+1
1
1
, Ci1 > Sq+1
+ t1
ND means
Ci > X + t1
r=1 Sj
r=1 Sq+1
original q + 1 supplier (which q supplier) must win bids X.
Lemma 3.6 original q + 1 supplier must also win bids Si1 contradicts
assumption trade size q bids Si1 .



rr+1 X 1
X > PS t1
q+1 j q (without loss generality
r=1 Sq

rr+1 > P
ordered q + 1 bid tie). Since X + t1

r=1 Sq

t1 rr+1
1
1
(the original q supplier wins bids Si ) conclude
PS Sq + r=1 Sq
X > Sq1 means q bidder (j = q). corresponding

rr+1 > P , high enough win
seller bid DA X + t1

r=1 Sq
DA therefore agent loses.

616

fiConcurrent Auctions Across Supply Chain

conclude agent wins auction pays critical value, therefore
auction IR IC. IC way costs aggregated protocol,
eciency mechanism eciency R.
Theorem 5.3 presented relationship incentive compatibility DA
rule resulting supply chain auction created Pivot Protocol. brings
question whether budget balance DA rule ensures budget balance
supply chain mechanism. turn so! appendix present example
DA rule (McAfees rule) revenue surplus, Pivot Protocol using
rule creates mechanism revenue decit.
next theorem gives condition DA rule sucient ensure
supply chain mechanism created Pivot Protocol budget-balanced. also shows
DA rule budget decit creates mechanism decit.
Theorem 5.4. Let R normalized DA rule IC non-discriminating. Let
supply chain mechanism created Pivot Protocol using rule R .
supply demand curves, let q trade size decided R, let PS PB
sellers buyers prices respectively.
PB Sq+1 holds supply demand curves, budget-balanced.
PB < PS supply demand curves (R budget decit) budget
decit.
Proof. rst show PB Sq+1 holds supply demand curves,
budget-balanced. divide supply chain allocation q disjoint procurement
sets. set single winner market. show total payment
procurement set non-negative, therefore auction BB. Theorem 5.3
mechanism non-discriminating, procurement sets payments.
Let P total payments agents procurement set, except consumer.


P PS . PB Sq+1
PB P ,
Lemma B.1 (see appendix B) shows Sq+1
conclude PB P 0 BB.
Similarly supply demand curves R budget decit, every procurement set decit. PB < PS supply demand curves, PB < P
conclude total payment procurement set PB P < 0,
means budget decit.
Note double auction rule R BB, sucient PB PS .
prove supply chain mechanism budget balanced, theorem relies stronger
condition PB Sq+1 (it always true Sq+1 PS Lemma B.1 shows).
McAfees DA satises PB PS , stronger condition PB Sq+1 always hold,
indeed example Appendix shows supply chain mechanism
budget decit cases. hand, Trade Reduction DA rule satises
constraints since q = l 1 (recall l optimal trade size) PB = Bq+1 =
Bl Sl = Sq+1 = PS .
Clearly DA rule budget decit might create supply chain mechanism
budget decit used Symmetric Protocol. since double auction special
case supply chain. able present parallel sucient condition
Symmetric Protocol ensure budget-balanced supply chain mechanism. case
617

fiBabaioff & Nisan

normalized DA rule incentive compatible, consistent non-discriminating, next
theorem proves two protocols create mechanism, implies
sucient condition budget balance holds also Symmetric Protocol
DA rules.
turn look relationship two protocols. case
normalized DA rule consistent non-discriminating used protocols.
following theorem shows rule also IC, Symmetric Protocol
Pivot Protocol create mechanism:
Theorem 5.5. Let R normalized DA rule non-discriminating consistent.
R IC mechanism created Pivot Protocol using R
mechanism created Symmetric Protocol using R (for set bids,
allocation payments). Thus, eciency budget mechanisms
equal.
Proof. Since supply demand curves demand market built
way, DA rule used demand market protocols, market
trade sizes decided rule mechanisms. Symmetric Protocol
consistent, therefore trade sizes markets same. Since mechanism
non-discriminating, allocations markets protocols.
Observation 3.4 mechanisms must payments, since
allocation.
5.3 Communication Complexity
naive implementation supply chain mechanism centralized mechanism would
required sending (tn) bids one centralized point (t number goods n
minimal number agents market). protocols described
much lower communication O(n) prices received sent market (note
bid single agent requires k bits, prices communicated grow
much k + bits). many interesting DA rules reduce
communication market O(log(n)) prices improved Pivot Protocol
present below. Thus, DA rules Pivot Protocol implemented
exponentially larger markets still using bandwidth. example,
use improved protocol size trade q set DA rule calculated
Optimal Trade Quantity l, payments dependent l l + 1 bids
supply demand curves (this case DA rules presented).
Theorem 5.6. Let R normalized DA rule IC non-discriminating. Assume
trade size decided rule function Optimal Trade Size l only,
payments decided using O(1) prices knowing l. Pivot Protocol
implemented O(log(n)) messages sent received market,
message contains single price.
Proof. Pivot Protocol improved using binary search nd l thus sending
values needed supply curves, instead passing entire supply curves
along chain. search l preformed binary search sending
618

fiConcurrent Auctions Across Supply Chain

O(log(n)) messages two consecutive markets. improved protocol, rst
tn passed pivot market, n minimal number bids market
2
(known markets protocol explained beginning section).
pivot market checks value smaller greater Dtn asks 3n
4
2
n4 element supply curves respectively. pivot market receives requested
value continues similar way search, l found. O(1) prices
needed supply curve nd payments (which function l
assumption) propagated pivot market request.
Similar results presented Symmetric Protocol restrictions
DA rule used protocol. Symmetric Protocol protocol choice
DA rule discriminating, case likely entire supply
demand curves needed order nd prices winners.
5.4 Global Properties Dierent DA Types
section examine properties mechanisms created supply chain
protocols using dierent double auction rules.
Chain VCG Double Auctions created running Symmetric Protocol
using VCG DA rule. creates local VCG double auction markets.
Since VCG DA rule IR, IC, consistent non-discriminating, Theorem 5.5 Pivot
Protocol VCG DA rule creates mechanism.
following proposition summarizes properties supply chain mechanism
using VCG double auction rule.
Proposition 5.7. Chain VCG Double Auctions IR, IC, non-discriminating, Socially
Ecient revenue decit. (RChain V CG 0)
Proof. Since VCG DA rule normalized, IC non-discriminating, Theorem 5.3
mechanism created IR, IC non-discriminating. outcome maximizes sum
valuations (Socially Ecient), since agents bid truthfully, trade size Optimal
Trade Size highest value bidders markets chosen winners.
fact VCG DA budget decit Theorem 5.4 conclude
Chain VCG DAs budget decit.
Note Observation 3.4 mechanism created using VCG DA rule VCG
global sense presented Vickrey, Clarke Groves (A mechanism IR, IC
ecient).
Chain Trade Reduction Double Auctions created running Symmetric
Protocol using Trade Reduction DA Rule. creates local Trade Reduction double
auction markets, double auctions chained. Since Trade
Reduction DA rule normalized, IC, consistent non-discriminating, Theorem 5.5
Pivot Protocol Trade Reduction DA rule creates mechanism.
Proposition 5.8. Chain Trade Reduction Double Auctions IR, IC , non-discriminating,
revenue surplus
RChain

rade Reduction

= (l 1) (Dlt Slt ) 0
619

fiBabaioff & Nisan

reduction Social Eciency Dlt Slt eciency least (l 1)/l
maximal eciency.
Proof. Since Trade Reduction DA rule normalized, IC ND, Theorem 5.3
mechanism created IR, IC non-discriminating.
revenue sum payments
(l 1) (Dlt

t1


Slrr+1 Sl1 ) = (l 1) (Dlt Slt )

r=1

non-negative since Dlt Slt denition Optimal Trade Quantity l.
reduction social eciency Dlt Slt , since l unit traded,
Trade Reduction DA l 1 highest trades optimal l trades conducted,
giving eciency least (l 1)/l.
Chain Reduction Double Auctions created running Pivot Protocol
using Reduction Rule, creates randomized mechanism. mechanism
probability distribution two mechanisms - Chain Trade Reduction DA
Chain VCG DA. mechanisms used according random variable
one probability zero otherwise. rst mechanism chosen one
(with probability ), second mechanism chosen zero (which happens
probability 1 ).
auction also preformed using Symmetric Protocol public coin
assumption (which created distributing random coin, tossed one
markets). case random variable shared markets, meaning
either Trade Reduction rule used markets, non markets.
ensures trade consistency creates mechanism.
Chain Reduction Double Auctions achieves allocation payments
Chain Trade Reduction DA probability , probability 1 achieves
allocation payments Chain VCG DA.
Proposition 5.9. Chain Reduction Double Auctions individually rational
universally incentive compatible randomized mechanism. expected revenue
RChain

Reduction

= RChain

rade Reduction

+ (1 ) RChain

V CG

eciency least (l 1)/l maximal eciency. expected reduction
Social Eciency (Dlt Slt ) expectation random choice .
Proof. mechanism individually rational universally incentive compatible randomized mechanism, since Chain Trade Reduction DAs Chain VCG
DAs normalized incentive compatible Proposition 5.8 Proposition 5.7.
easy verify expected revenue reduction social eciency
claimed.
Chain Payment Double Auctions created running Symmetric Protocol, market running Payment Double Auction, creates randomized
620

fiConcurrent Auctions Across Supply Chain

mechanism (again public coin assumption). mechanism expected revenue social eciency Chain Reduction DAs,
agents probability winning auction expected utility
Chain Reduction DAs. Note since Payment DA rule discriminating,
Pivot Protocol dened cannot used create supply chain mechanism.
Proposition 5.10. Chain Payment Double Auctions individually rational
incentive compatible randomized mechanism. expected revenue expected social
eciency Chain Reduction Double Auctions.
Proof. Chain Payment Double Auctions distribution allocation
payments Chain Reduction Double Auctions, therefore agent facing
mechanism expectation bids truthfully. Since expected payment
agent mechanisms, expected revenue same. Since
allocation mechanisms, expected social eciency same.
Unfortunately, chaining McAfees Double Auction preserve nice properties
McAfees DA rule has. run Symmetric Protocol using McAfees DA Rule,
might get inconsistent trade. hand, run Pivot Protocol using
McAfees DA rule, revenue decit! (See examples appendix A). However
weaker claim made - mechanism revenue least high chain
VCG DA, eciency least high chain Trade Reduction DA.
Chain McAfees Double Auction created running Pivot Protocol using
McAfees Rule.
Proposition 5.11. Chain McAfees Double Auction IR, IC, ND, revenue never
smaller revenue Chain VCG Double Auction, reduction
Social Eciency never greater reduction Chain Trade Reduction Double
Auction.
Proof. Chain McAfees Double Auction IR, IC ND Theorem 5.3 fact
McAfees DA rule normalized, IC ND.
mechanism revenue never smaller revenue Chain VCG Double
Auctions since trade reduction, revenue surplus Proposition 5.8,
know Chain VCG Double Auctions revenue decit Proposition 5.7.
case trade reduction, revenue never smaller revenue Chain
St

+D

VCG Double Auctions, since winning buyer pays p = PB = PS = l+1 2 l+1


> Dl+1

least much pays VCG mechanism. Sl+1





Sl+1 > p > Dl+1 Sl p therefore max(Dl+1 , Sl ) p. winning seller
1 , 1 ) receives VCG
supply good, never receives min(Sl+1
l


mechanism. since p min(Sl+1 , Dl ) receives
1
,p
min(Sl+1

t1


1

Slrr+1 ) min(Sl+1
, min(Dlt , Sl+1
)

r=1
1
, Dlt
min(Sl+1

t1


Slrr+1 ) =

r=1
t1



Slrr+1 , Sl+1


r=1

t1

r=1

621

1
Slrr+1 ) = min(Sl+1
, Dl1 )

fiBabaioff & Nisan





rr+1
rr+1

last equality holds since Dl1 = Dlt t1
Sl+1
t1

r=1 Sl
r=1 Sl

t1
rr+1

1
= Sl+1 . similar argument shows winning convert never
Sl+1 r=1 Sl+1
receives VCG mechanism.
reduction social eciency never greater reduction Chain
Trade Reduction Double Auction, since trade size never smaller trade size
auction, l 1.

Table 2 summarizes properties supply chain mechanisms created two
protocols using dierent double auction rules. normalized mechanisms, one cares
incentive compatibility eciency budget balance, VCG
mechanism used. budget balance must ensured every execution
incentive compatible mechanism, Trade Reduction auction mechanism
choice. one wants mechanism incentive compatible every execution,
settle expected budget balance instead ensured one, higher expected eciency
achieved (D) Reduction, assuming right choice (D). Finally,
variance agents payments important, (D) P ayment used
order lower variance agents. cost, since mechanism
incentive compatible every execution (universally), rather, incentive
compatible agents care expected utility. table also presents
McAfee supply chain mechanism, point fact budget supply chain
mechanism might dierent budget underlying double auction.
DA rule
VCG
Trade Red.
(D) Reduction
(D) P ayment
McAfee

Incentive Comp.
yes
yes
yes, universally
yes, universally
yes

Revenue
decit
surplus
0 (expected)
0 (expected)
surplus decit

Eciency loss
0
LFT
(D) LF (expected)
(D) LF (expected)
LFT

Table 2: Supply Chain Auctions
Notes: a) McAfees rule used Symmetric Protocol, table presents properties
Pivot Protocol. b) LFT (least favorable trade) context supply chain, means net total utility
least favorable item. c) distribution agents supply chain. d) (D) P ayment
lower variance payments (D) Reduction.

comparing properties chain mechanisms Table 2 properties
original double auction rules Figure 1, one see following. incentive
compatibility eciency properties preserved protocols, achieving
revenue surplus requires stronger condition DA rule, shown Theorem 5.4. consistency property, enables chaining markets Symmetric
Protocol, applies rst two deterministic rules, two randomized rules
assumption common coin toss. exist McAfees rule since rule sets
trade quantity function bids submitted, way trade quantity
dierent two dierent markets. k-DA presented table since
622

fiConcurrent Auctions Across Supply Chain

incentive compatible therefore cannot used protocols (the two protocols
create dierent mechanisms rule).

6. Discussion Future Work
paper presented two distributed protocols create supply chain mechanisms using double auction rules. protocols generic use dierent double
auction rules create dierent mechanisms. characterized properties
supply chain mechanisms derive properties underlining double auction
rule. protocols use VCG DA achieve IR, IC full eciency budget
decit. also use Trade Reduction DA ensure budget balance surplus
high eciency. even higher eciency maintaining BB expectation
achieved using two new randomized double auction rules.
work concentrated simple case indivisible homogeneous goods,
agents desires one good desires convert one unit good one unit
another good, supply chain linear form. Pivot Protocol easily extended
case trees good created one several goods (but
good used create two dierent goods). example, might lemon market
Florida another lemon market California, also separate squeezing markets
states, one lemonade market. tree auctions decides many lemonade
glasses produced, many glasses produced Florida lemons
California lemons. Since main ideas extension similar ones
presented paper, omit technical details refer interested reader
Babaio (2001) details. Babaio (2001) also discusses extension model
case agent allowed bid multiple units market, bidding
maximal quantity willing trade price per unit.
paper Babaio Walsh (2004) extends model case item
produced combination several items several dierent goods, good
still produced exactly one market. presented specic auction (unlike
work presents generic protocols supply chain formation) individually
rational, incentive compatible, budget-balanced, yet highly ecient.
Similar results (IR, IC, BB high eciency) presented Babaio, Nisan,
Pavlov (2004) related model Spatially Distributed Market. model single
good traded set independent markets, shipment markets possible
incurs publicly known cost (but model strategic producers
model).
One interesting challenge stay unit-to-unit conversion model, extend
model allow general directed a-cyclic supply chain graph topology. Yet another
important challenge extend model case agents single-minded
agents, submit exclusive non exclusive bids several markets.
623

fiBabaioff & Nisan

Acknowledgments
research supported grants Israeli Ministry Science, Israeli
Academy Sciences USA-Israel Bi-national Science Foundation. rst author
(Babaio) also supported Yeshaya Horowitz Association.
thank William Walsh, Daniel Lehmann, Hila Babaio anonymous reviewers
helpful comments.

Appendix A. Problems Chaining McAfees Double Auction
Consistency Problem: following example supply chain auction using McAfees
rule shows double auction rules consistent. Figure 14 shows supply
demand curves three markets supply demand curves propagation.
Lemon Market
Supply Demand
(S L )
(DL )
10
20
20
10

Squeezing Market
Supply Demand
(S LJ )
(DLJ )
5
15
7
-3

Juice Market
Supply Demand
(S J )
(DJ )
15
25
27
17

Figure 14: Chaining McAfees DA example
optimal trade quantity example 1, seen Figure 14. McAfees
= 22 [15, 25].
rule Lemonade Market set trade size 1, since 27+17
2
hand, use McAfees rule Squeezing Market, trade reduction
= 2
/ [5, 15] trade quantity zero, contradiction
made since 7+(3)
2
previous decision. conclude exists non-consistent DA rule cannot
used Symmetric Protocol.
Revenue Problem: example Pivot Protocol presented
Figure 15 shows fact DA rule revenue surplus, ensure
supply chain mechanism created Pivot Protocol using DA rule revenue
surplus well.
Lemon Market
Supply Demand
(S L )
(DL )
10
20

Squeezing Market
Supply Demand
(S LJ )
(DLJ )
5
7

(V,q)=(17,1)

Juice Market
Supply Demand
(S J )
(DJ )
15
25
27
17

(V,q)=(22,1)





Figure 15: Pivot Protocol McAfees rule
example, McAfees rule Lemonade Market sets trade size 1, since
= 22 [15, 25]. buyer market pay 22. following Pivot
Protocol, squeezer bid 5 paid 7 = min(7, 22 10)
27+17
2

624

fiConcurrent Auctions Across Supply Chain

supplier supply market bid 10 paid 17 = min(17, 20).
total revenue 22 7 17 = 2 means revenue decit.

Appendix B. Supply Chain Payments
appendix prove Lemma B.1 used proof Theorem 5.4.
Lemma B.1. double auction rule used Pivot Protocol IR, IC nondiscriminating, PS sellers price double auction, total payment
P supplier converters (one market) create one unit nal
.
good satises PS P Sq+1
Proof. induction, conversion market C C m+1 receives pair (PS
t1
rr+1 , q) demand market, payment winning converter
r=m+1 Sq
min(PS

t1


mm+1
Sqrr+1 Sqm , Sq+1
)

r=m+1

Similarly, supply market 1 receives pair (PS
ment winning supplier
min(PS

t1


t1

rr+1 , q),
r=1 Sq

pay-

1
Sqrr+1 , Sq+1
)

r=1

conclude total payment converters supplier one unit
P =

t1

m=1

min(PS

t1


mm+1
Sqrr+1 Sqm , Sq+1
) + min(PS

r=m+1

t1


1
Sqrr+1 , Sq+1
)

r=1


since
First notice P Sq+1

P

t1


mm+1
1

Sq+1
+ Sq+1
= Sq+1

m=1

Secondly prove PS P . idea proof holds
one conversion C 1 C (as show Lemma B.2), splitting conversion
two consecutive conversions, total payment converters grows (as
show Lemma B.3).
Lemma B.2. Assume one conversion market C 1 C ,

rr+1

means every i, Si1t = t1
r=1 Si
1t
1
) + min(PS Sq1t , Sq+1
)
PS P = min(PS Sq1 , Sq+1

Proof. Note double auction pivot market original
auction, since supply curve pivot market same, therefore PS
same.
prove claim true checking four possible cases:
625

fiBabaioff & Nisan

1t P 1t 1 ,
1. PS Sq1 Sq+1

q
q+1

P = (PS Sq1 ) + (PS Sq1t ) = PS + (PS (Sq1 + Sq1t )) PS
since PS Sq1 + Sq1t = Sqt , DA rule non-discriminating.
1t 1
1t ,
2. PS Sq1 Sq+1
q+1 PS Sq
1
1
= PS + (Sq+1
Sq1 ) PS
P = (PS Sq1 ) + Sq+1
1
since Sq+1
Sq1
1t P 1 P 1t 1 ,
3. Sq+1


q
q
q+1
1t
1t
+ (PS Sq1t ) = PS + (Sq+1
Sq1t ) PS
P = Sq+1
1t 1t .
since Sq+1
q
1t P 1 1
1t ,
4. Sq+1

q
q+1 PS Sq
1t
1
+ Sq+1
PS
P = Sq+1
1t + 1

since Sq+1
q+1 = Sq+1 PS , DA rule non-discriminating.

proven P PS case one conversion market.
Lemma B.3. Assume conversion market C k C split two conversion
markets - one C k C k+1 C k+1 C , payment
conversion one unit C k C grow, means
P kt P kk+1 + P k+1t
P wz cost conversion one w unit one z unit. words,
always true
kk+1
k+1t
kt
) min(PS Sqk+1t Sqk , Sq+1
) + min(PS Sqk+1 , Sq+1
)
min(PS Sqk , Sq+1

Sikt =

t1

r=k

Sirr+1

Proof. Note double auction pivot market original
auction, since supply curve pivot market same, therefore PS
same.
prove claim true checking four possible cases:
kk+1
k+1t
PS Sqk+1 Sq+1
,
1. PS Sqk+1t Sqk Sq+1
kk+1
k+1t
kt
kt
+ Sq+1
= Sq+1
min(PS Sqk , Sq+1
) = P kt
P kk+1 + P k+1t = Sq+1

626

fiConcurrent Auctions Across Supply Chain

kk+1
k+1t
2. PS Sqk+1t Sqk Sq+1
PS Sqk+1 Sq+1
,
k+1t
=
P kk+1 + P k+1t = (PS Sqk+1t Sqk ) + Sq+1
k+1t
(PS Sqk ) + (Sq+1
Sqk+1t ) PS Sqk P kt
k+1t
since Sq+1
Sqk+1t .
kk+1
k+1t
3. PS Sqk+1t Sqk Sq+1
PS Sqk+1 Sq+1
,
kk+1
+ (PS Sqk+1 ) =
P kk+1 + P k+1t = Sq+1
kk+1
Sqkk+1 ) (PS Sqk ) P kt
(PS Sqk ) + (Sq+1
kk+1
since Sq+1
Sqkk+1 .
kk+1
k+1t
PS Sqk+1 Sq+1
,
4. PS Sqk+1t Sqk Sq+1

P kk+1 + P k+1t = (PS Sqk+1t Sqk ) + (PS Sqk+1 ) =
(PS Sqk ) + (PS Sqk+1 Sqk+1t ) (PS Sqk ) P kt
since PS Sqk+1 + Sqk+1t = Sqt .
proven payment grow splitting conversion market.
conclude cost one unit C never smaller PS , since
rst lemma, true one conversion, induction second
lemma, splitting conversion market 1 conversion markets increase
cost.

References
Archer, A., & Tardos, E. (2001). Truthful mechanisms one-parameter agents. Proceedings 42nd IEEE symposium foundations computer science, pp. 482491.
Babaio, M. (2001).
Concurrent auctions across supply chain. m.sc. thesis, department computer science, hebrew university jerusalem, israel.
http://www.cs.huji.ac.il/mosheb.
Babaio, M., Nisan, N., & Pavlov, E. (2004). Mechanisms spatially distributed market.
5th ACM Conference Electronic Commerce, pp. 920.
Babaio, M., & Walsh, W. E. (2004). Incentive-compatible, budget-balanced, yet highly
ecient auctions supply chain formation. Decision Support Systems. press.
Partial version appeared EC03.
Bosch-Domenech, A., & Sunder, S. (1999). Tracking invisible hand: Convergence
double auctions competitive equilibrium. Tech. rep., Carnegie Mellon University.
627

fiBabaioff & Nisan

Chatterjee, K., & Samuelson, W. (1983). Bargaining incomplete information. Operations Research, 31, 835851.
Clarke, E. H. (1971). Multipart pricing public goods. Public Choice, 1733.
Feigenbaum, J., & Shenker, S. (2002). Distributed algorithmic mechanism design: Recent
results future directions. 6th International Workshop Discrete Algorithms
Methods Mobile Computing Communications, pp. 113.
Friedman, D., & Rust, J. (1991). Double Auction Market Institutions, Theories,
Evidence. Addison-Wesley Publishing Company.
Goldberg, A. V., & Hartline, J. D. (2003). Envy-free auctions digital goods. 4st ACM
Conference Electronic Commerce, pp. 2935. ACM Press.
Groves, T. (1973). Incentives teams. Econometrica, 617631.
Lehmann, D. J., OCallaghan, L. I., & Shoham, Y. (2002). Truth revelation approximately
ecient combinatorial auctions. Journal ACM, 49 (5), 577602.
Mas-Collel, A., Whinston, W., & Green, J. (1995). Microeconomic Theory. Oxford university press.
McAfee, R. (1992). dominant strategy double auction. Journal Economic Theory, 56,
434450.
Mualem, A., & Nisan, N. (2002). Truthful approximation mechanisms restricted combinatorial auctions.. AAAI (poster), 2002. also presented Dagstuhl workshop
Electronic Market Design.
Myerson, R. B., & Satterthwaite, M. A. (1983). Ecient mechanisms bilateral trading.
Journal Economic Theory, 29, 265281.
Nisan, N., & Ronen, A. (1999). Algorithmic mechanism design. Proceedings STOC
1999, pp. 129140.
Osborne, M. J., & Rubinstein, A. (1994). Course Game Theory. MIT press.
Parkes, D. C., Kalagnanam, J., & Eso, M. (2001). Achieving budget-balance vickreybased payment schemes exchanges. IJCAI, pp. 11611168.
Rustichini, A., Satterthwaite, M. A., & Williams, S. R. (1994). Convergence eciency
simple market incomplete information. Econometrica, 62 (5), 10411063.
Satterthwaite, M., & Williams, S. (1989). Bilateral trade sealed bid k-double
auction: Existence eciency. Journal Economic Theory, 48, 107133.
Satterthwaite, M. A., & Williams, S. R. (1991). bayesian theory k-double auction.
Double Auction Market Institutions, Theories, Evidence, pp. 99124.
Vickrey, W. (1961). Counterspeculation, auctions competitive sealed tenders. Journal
Finance, 837.
628

fiConcurrent Auctions Across Supply Chain

Walsh, W., & Wellman, M. (2003). Decentralized supply chain formation: market protocol
competitive equilibrium analysis. Journal Articial Intelligence Research, 19,
513567.
Walsh, W. E., Wellman, M. P., & Ygge, F. (2000). Combinatorial auctions supply chain
formation. 2nd ACM Conference Electronic Commerce, pp. 260269.
Wilson, R. (1985). Incentive eciency double auctions. Econometrica, 53, 11011115.

629

fiJournal Artificial Intelligence Research 21 (2004) 287-317

Submitted 06/03; published 03/04

IDL-Expressions:
Formalism Representing Parsing
Finite Languages Natural Language Processing
Mark-Jan Nederhof

markjan@let.rug.nl

Faculty Arts, University Groningen
P.O. Box 716
NL-9700 Groningen, Netherlands

Giorgio Satta

satta@dei.unipd.it

Dept. Information Engineering, University Padua
via Gradenigo, 6/A
I-35131 Padova, Italy

Abstract
propose formalism representation finite languages, referred class
IDL-expressions, combines concepts considered isolation
existing formalisms. suggested applications natural language processing,
specifically surface natural language generation machine translation,
sentence obtained first generating large set candidate sentences, represented
compact way, filtering set parser. study several formal
properties IDL-expressions compare new formalism standard ones.
also present novel parsing algorithm IDL-expressions prove non-trivial upper
bound time complexity.

1. Introduction
natural language processing, specifically applications involve natural language generation, task surface generation consists process generating output sentence target language, basis input representation desired
meaning output sentence. last decade, number new approaches
natural language surface generation put forward, called hybrid approaches. Hybrid approaches make use symbolic knowledge combination statistical techniques
recently developed natural language processing. Hybrid approaches
therefore share many advantages statistical methods natural language processing,
high accuracy, wide coverage, robustness, portability scalability.
Hybrid approaches typically based two processing phases, described
follows (Knight & Hatzivassiloglou, 1995; Langkilde & Knight, 1998; Bangalore & Rambow,
2000 report examples applications approach real world generation systems).
first phase one generates large set candidate sentences relatively simple
process. done basis input sentence source language case
process embedded within machine translation system, generally
basis logical/semantic representation, called conceptual structure, denotes
meaning output sentence convey. first phase involves

c
2004
AI Access Foundation. rights reserved.

fiNederhof & Satta

intricacies target language, set candidate sentences may contain many
ungrammatical otherwise seen less desirable others.
second phase one preferred sentences selected collection candidates,
exploiting form syntactic processing heavily relies properties
target language first phase. syntactic processing may involve language models
simple bigrams may involve powerful models based contextfree grammars, typically perform higher accuracy task (see instance
work presented Charniak, 2001 references therein).
hybrid approaches, generation candidate set typically involves symbolic
grammar quickly hand-written, quite small easy maintain.
grammar cannot therefore account intricacies target language.
instance, frequency information synonyms collocation information general
encoded grammar. Similarly, lexico-syntactic selectional constraints
target language might fully specified, usually case small midsized grammars. Furthermore, might also underspecification stemming
input conceptual structure. usually case surface generation module
embedded larger architecture machine translation, source language
underspecified features definiteness, time number. Since inferring missing
information sentence context difficult task, surface generation module
usually deal underspecified knowledge.
above-mentioned problems well-known literature natural language
surface generation, usually referred lack knowledge system
input. consequence problems, set candidate sentences generated
first phase may extremely large. real world generation systems, candidate sets
reported contain many 1012 sentences (Langkilde, 2000). already explained,
second processing phase hybrid approaches intended reduce huge sets
subsets containing sentences. done exploiting knowledge
target language available first phase. additional knowledge
often obtained automatic extraction corpora, requires considerably
less effort development hand-written, purely symbolic systems.
Due extremely large size set candidate sentences, feasibility hybrid
approaches surface natural language generation relies
compactness representation set candidate sentences real world
systems might large 1012 ;
efficiency syntactic processing stored set.
Several solutions adopted existing hybrid systems representation
set candidate sentences. include bags words (Brown et al., 1990)
bags complex lexical representations (Beaven, 1992; Brew, 1992; Whitelock, 1992), word
lattices (Knight & Hatzivassiloglou, 1995; Langkilde & Knight, 1998; Bangalore & Rambow,
2000), non-recursive context-free grammars (Langkilde, 2000). discussed
detail Section 2, word lattices non-recursive context-free grammars allow encoding
precedence constraints choice among different words, lack primitive
representing strings realized combining collection words arbitrary
288

fiIDL-Expressions: Formalism Finite Languages

order. hand, bags words allow encoding free word order,
representation one cannot directly express precedence constraints choice among
different words.
paper propose new representation combines above-mentioned
primitives. representation consists IDL-expressions. term IDL-expression,
stands interleave, pertains phrases may occur interleaved, allowing
freedom word order (a precise definition notion provided next
section); stands disjunction, allows choices words phrases; L stands
lock, used constrain application interleave operator. study
interesting properties representation, argue expressivity
formalism makes suitable alternatives discussed use within hybrid
architectures surface natural language generation. also associate IDL-expressions
IDL-graphs, equivalent representation easily interpreted
machine, develop dynamic programming algorithm parsing IDL-graphs using
context-free grammar. set candidate sentences represented IDL-expression
IDL-graph, algorithm used filter ungrammatical sentences
set, rank sentences set according likelihood, case context-free
grammar assigns weights derivations. parsing traditionally defined input
consisting single string, conceive parsing process carried
input device denoting language, i.e., set strings.
superficial similarity problem described representing
finite sets surface generation, different research topic, often referred discontinuous parsing. discontinuous parsing one seeks relax definition context-free
grammars order represent syntax languages exhibit constructions uncertainty word constituent order (see instance work reported Daniels & Meurers,
2002 references therein). fact, operators use IDL-expressions
also exploited recent work discontinuous parsing. However, parsing problem
discontinuous grammars parsing problem IDL-expressions quite different: former, given grammar productions express uncertainty
constituent order, need parse input string whose symbols totally ordered;
latter problem given grammar total order constituents appearing production, need parse input includes uncertainty word
constituent order.
paper structured follows. Section 2 give brief overview existing
representations finite languages used surface generation components.
discuss notational preliminaries Section 3. Section 4 introduce IDLexpressions define semantics. Section 5 associate IDL-expressions
equivalent procedural representation, called IDL-graphs. also introduce
important notion cut IDL-graph, exploited later algorithm.
Section 6 briefly discuss Earley algorithm, traditional method parsing
string using context-free grammar, adapt algorithm work finite languages
encoded IDL-graphs. Section 7 prove non-trivial upper bound number
cuts IDL-graph, basis investigate computational complexity
parsing algorithm. also address implementational issues. conclude
discussion Section 8.
289

fiNederhof & Satta

2. Representations Finite Languages
section analyze compare existing representations finite languages
adopted surface generation components natural language systems.
Bags (or multisets) words used several approaches surface generation.
basis generation component statistical machine translation
models proposed Brown et al. (1990). Bags complex lexical signs also used
machine translation approach described Beaven (1992) Whitelock (1992),
called shake-and-bake. already mentioned, bags succinct representation
finite languages, since allow encoding exponentially many strings
size bag itself. power comes cost, however. Deciding whether string
encoded input bag parsed CFG NP-complete (Brew, 1992).
difficult show result still holds case regular grammar or, equivalently,
regular expression. NP-completeness result involving bags also presented
Knight (1999), related problem parsing grammar probabilistic model
based bigrams.
far expressivity concerned, bags words also strict limitations.
structures lack primitive expressing choices among words. already observed
introduction, serious problem natural language generation, alternatives lexical realization must encoded presence lack detailed knowledge
target language. addition, bags words usually come precedence
constraints. However, natural language applications constraints common,
usually derived knowledge target language or, case machine translation, parsing tree source string. order represent
constraints, extra machinery must introduced. instance, Brown et al. (1990) impose,
word bag, probabilistic distribution delimiting position target
string, basis original position source word input string
translated. shake bake approach, bags defined functional structures,
representing complex lexical information constraints derived.
parsing algorithm bags interleaved constraint propagation algorithm
filter parses (e.g., done Brew, 1992). general remark, different layers
representation requires development involved parsing algorithms,
try avoid new proposal described below.
alternative representation finite languages class acyclic deterministic finite automata, also called word lattices. representation often used hybrid
approaches surface generation (Knight & Hatzivassiloglou, 1995; Langkilde & Knight,
1998; Bangalore & Rambow, 2000), generally natural language applications
form uncertainty comes input, instance speech recognition (Jurafsky & Martin, 2000, Section 7.4). Word lattices inherit standard regular
expressions primitives expressing concatenation disjunction, thereby allow
encoding precedence constraints word disjunction direct way. Furthermore, word
lattices efficiently parsed means CFGs, using standard techniques lattice
parsing (Aust, Oerder, Seide, & Steinbiss, 1995). Lattice parsing requires cubic time
number states input finite automaton linear time size CFG.
Methods lattice parsing traced back Bar-Hillel, Perles, Shamir (1964),

290

fiIDL-Expressions: Formalism Finite Languages

prove class context-free languages closed intersection regular
languages.
One limitation word lattices finite automata general lack operator
free word order. already discussed introduction, severe
limitation hybrid systems, free word order sentence realization needed
case symbolic grammar used first phase fails provide ordering constraints.
represent strings bag words occur every possible order, one
encode string individual path within lattice. general case,
requires amount space exponential size bag.
perspective, previously mentioned polynomial time result parsing avail,
since input structure parser might already size exponential
size input conceptual structure. problem free word order lattice structures
partially solved Langkilde Knight (1998) introducing external recasting
mechanism preprocesses input conceptual structure. overall effect
phrases normally represented two independent sublattices generated
one embedded other, therefore partially mimicking interleaving words
two phrases. However, enough treat free word order full generality.
third representation finite languages, often found literature compression
theory (Nevill-Manning & Witten, 1997), class non-recursive CFGs. CFG
called non-recursive nonterminal rewritten string containing
nonterminal itself. difficult see grammars generate finite
languages. Non-recursive CFGs recently exploited hybrid systems (Langkilde,
2000).1 representation inherits expressivity word lattices, thus
encode precedence constraints well disjunctions. addition, non-recursive CFGs
achieve much smaller encodings finite languages word lattices. done
uniquely encoding certain sets substrings occur repeatedly nonterminal
reused several places. feature turns useful natural
language applications, shown experimental results reported Langkilde (2000).
Although non-recursive CFGs compact representations word lattices,
representation still lacks primitive representing free word order. fact, CFG
generating finite language permutations n symbols must size least
exponential n.2 addition, problem deciding whether string encoded
non-recursive CFG parsed general CFG PSPACE-complete (Nederhof &
Satta, 2004).
discussion, one draw following conclusions. considering
range possible encodings finite languages, interested measuring (i) compactness representation, (ii) efficiency parsing obtained representation
means CFG. one extreme naive solution enumerating strings
language, independently parsing individual string using traditional
string parsing algorithm. solution obviously unfeasible, since compression
achieved overall amount time required might exponential size
1. Langkilde (2000) uses term forests non-recursive CFGs, different name
concept (Billot & Lang, 1989).
2. unpublished proof fact personally communicated authors Jeffrey Shallit
Ming-wei Wang.

291

fiNederhof & Satta

input conceptual structure. Although word lattices compact representation,
free word order needs encoded may still representations exponential
size input parser, already discussed. opposite extreme, solutions
like bags words non-recursive CFGs, allow compact representations,
still demanding parsing time requirements. Intuitively, explained
considering parsing highly compressed finite language requires additional bookkeeping respect string case. need explore trade-off
solutions, offering interesting compression factors expense parsing
time requirements provably polynomial cases interest. show
sequel paper, IDL-expressions required properties therefore
interesting solution problem.

3. Notation
section briefly recall basic notions formal language theory.
details refer reader standard textbooks (e.g., Harrison, 1978).
set , || denotes number elements ; string x alphabet,
|x| denotes length x. string x languages (sets strings) L L0 , let
x L = {xy | L} L L0 = {xy | x L, L0 }. remind reader
string-valued function f alphabet extended homomorphism
letting f () = f (ax) = f (a)f (x) x . also let
f (L) = {f (x) | x L}.
denote context-free grammar (CFG) 4-tuple G = (N , , P, S), N
finite set nonterminals, finite set terminals, N = , N special
symbol called start symbol, P finite set productions form ,
N ( N ) . Throughout paper assume following conventions:
A, B, C denote nonterminals, a, b, c denote terminals, , , , denote strings ( N )
x, y, z denote strings .
derives relation denoted G transitive closure +
G . language generated grammar G denoted L(G). size G defined
X
|G| =
|A| .
(1)
(A)P

4. IDL-Expressions
section introduce class IDL-expressions define mapping
expressions sets strings. Similarly regular expressions, IDL-expressions generate sets
strings, i.e., languages. However, languages always finite. Therefore class
languages generated IDL-expressions proper subset class regular languages.
already discussed introduction, IDL-expressions combine language operators
considered isolation previous representations finite languages exploited
surface natural language generation. addition, operations
recently used discontinuous parsing literature, syntactic description (infinite) languages weak linear precedence constraints. IDL-expressions represent choices
among words phrases relative ordering means standard concatenation
292

fiIDL-Expressions: Formalism Finite Languages

operator regular expressions, along three additional operators discussed
follows. operators take arguments one IDL-expressions,
combine strings generated arguments different ways.
Operator k, called interleave, interleaves strings resulting argument expressions. string z results interleaving two strings x whenever z
composed occurrences symbols x y, symbols
appear within z relative order within x y. example, consider strings abcd efg. interleaving two strings obtain, among many
others, strings abecfgd, eabfgcd efabcdg. formal language literature, operation also called shuffle, instance Dassow Paun
(1989). discontinuous parsing literature literature head-driven
phrase-structure grammars (HPSG, Pollard & Sag, 1994) interleave operation
also called sequence union (Reape, 1989) domain union (Reape, 1994).
interleave operator also occurs XML tool described van der Vlist (2003).
Operator , called disjunction, allows choice strings resulting
argument expressions. standard operator regular expressions,
commonly written +.
Operator , called lock, takes single IDL-expression argument. operator
states additional material interleaved string resulting
argument. lock operator previously used discontinuous parsing
literature, instance Daniels Meurers (2002), Gotz Penn (1997),
Ramsay (1999), Suhre (1999). context, operator called isolation.
interleave, disjunction lock operators also called I, L operators,
respectively. see later, combination L operators within IDLexpressions provides much power existing formalisms represent free word order,
maintaining computational properties quite close regular expressions
finite automata.
introductory example, discuss following IDL-expression, defined
word alphabet {piano, play, must, necessarily, we}.
k((necessarily, must), (play piano)).

(2)

IDL-expression (2) says words we, play piano must appear order
generated strings, specified two occurrences concatenation operator.
Furthermore, use lock operator states additional words ever appear
play piano. disjunction operator expresses choice words
necessarily must. Finally, interleave operator states word resulting
first arguments must inserted sequence we, play, piano,
available positions. Notice interaction lock operator, which, seen,
makes unavailable position play piano. Thus following sentences,
among others, generated IDL-expression (2):

293

fiNederhof & Satta

necessarily play piano
must play piano
must play piano
play piano necessarily.
However, following sentences cannot generated IDL-expression (2):
play necessarily piano
necessarily must play piano.
first sentence disallowed use lock operator, second sentence
impossible disjunction operator states exactly one arguments must
appear sentence realization. provide formal definition class IDLexpressions.
Definition 1 Let finite alphabet let E symbol . IDLexpression string satisfying one following conditions:
(i) = a, {E};
(ii) = ( 0 ), 0 IDL-expression;
(iii) = (1 , 2 , . . . , n ), n 2 IDL-expression i, 1 n;
(iv) = k(1 , 2 , . . . , n ), n 2 IDL-expression i, 1 n;
(v) = 1 2 , 1 2 IDL-expressions.
take infix operator right associative, although definitions
paper, disambiguation associativity relevant taken arbitrarily.
say IDL-expression 0 subexpression 0 appears argument
operator .
develop precise semantics IDL-expressions. technical difficulty
arises proper treatment lock operator.3 Let x string
. basic idea use new symbol , already . occurrence
two terminals indicates additional string inserted position.
example, x = x0 x00 x000 x0 , x00 x000 strings , need
interleave x string y, may get result string x0 yx00 x000 string
x0 x00 yx000 . lock operator corresponds removal every occurrence
string.
precisely, strings ( {}) used represent sequences strings
; symbol used separate strings sequence. Furthermore, introduce
string homomorphism lock ( {}) letting lock(a) = lock() = .
application lock input sequence seen operation concatenating
together strings sequence.
3. add Kleene star, infinite languages specified, interleave lock
conveniently defined using derivatives (Brzozowski, 1964), noted van der Vlist (2003).

294

fiIDL-Expressions: Formalism Finite Languages

define basic operation comb, plays important role sequel.
operation composes two sequences x strings, represented explained above,
set new sequences strings. done interleaving two input sequences
every possible way. Operation comb makes use auxiliary operation comb0 ,
also constructs interleaved sequences input sequences x y, always starting
first string first argument x. sequence comb(x, y) must start
string x string y, comb(x, y) union comb0 (x, y) comb0 (y, x).
definition comb0 , distinguish case x consists single string
case x consists least two strings. latter case, tail output
sequence obtained applying comb recursively tail sequence x
complete sequence y. x, ( {}) , have:
comb(x, y) = comb0 (x, y) comb0 (y, x)

{x y}, x ;



{x0 } comb(x00 , y),
comb0 (x, y) =
x0 x00



x = x0 x00 .
example, let = {a, b, c, d, e} consider two sequences bb c e.

comb(a bb c, e) =
{a bb c e, bb c e, bb e c,
bb c e, bb e c, e bb c,
bb c e, bb e c, e bb c,
e bb c}.
languages L1 , L2 define comb(L1 , L2 ) = xL1 ,yL2 comb(x, y). generally,
languages L1 , L2 , . . . , Ld , 2, define combdi=1 Li = comb(L1 , L2 ) = 2,
combdi=1 Li = comb(combd1
i=1 Li , Ld ) > 2.
Definition 2 Let finite alphabet. Let function mapping IDL-expressions
subsets ( {}) , specified following conditions:
(i) (a) = {a} , (E) = {};
(ii) (()) = lock(());
(iii) ((1 , 2 , . . . , n )) = ni=1 (i );
(iv) (k(1 , 2 , . . . , n )) = combni=1 (i );
(v) ( 0 ) = () ( 0 ).
set strings satisfy IDL-expression , written L(), given L() =
lock(()).

295

fiNederhof & Satta

example definition, show interleave operator
used IDL-expression denote set strings realizing permutations given
bag symbols. Let = {a, b, c}. Consider bag ha, a, b, c, ci IDL-expression
k(a, a, b, c, c).

(3)

applying Definition 2 IDL-expression (3), obtain first steps
(a) = {a},
(b) = {b},
(c) = {c},
(k(a, a)) = comb({a}, {a}) = {a a},
(k(a, a, b)) = comb({a a}, {b}) = {b a, b a, b}.
next step obtain 3 4 sequences length 4, using symbols
bag ha, a, b, ci. One application comb operator, set set {c}, provides possible sequences singleton strings expressing permutations symbols bag
ha, a, b, c, ci. removing symbol throughout, conceptually turns sequences
strings undivided strings, obtain desired language L(k(a, a, b, c, c)) permutations bag ha, a, b, c, ci.
conclude section, compare expressivity IDL-expressions
formalisms discussed Section 2. means simple example.
follows, use alphabet {NP, PP, V}. symbols denote units standardly
used syntactic analysis natural language, stand for, respectively, noun phrase,
prepositional phrase verb. Symbols NP, PP V rewritten actual words
language, use terminal symbols simplify presentation. Consider
language subject-verb-object (SVO) order sentence structure
[S NP1 V NP2 ],
NP1 realizes subject position NP2 realizes object position. Let PP1
PP2 phrases must inserted sentence modifiers. Assume
know language hand allow modifiers appear verbal
object positions. left 3 available positions realization
first modifier, 4 positions string. first modifier inserted
within string, 5 positions, 4 available realization second
modifier, assumption. results total 3 4 = 12 possible sentence
realizations.
bag words sentences unable capture constraint
positioning modifiers. time, word lattice sentences would contain
12 distinct paths, corresponding different realizations modifiers basic
sentence. Using IDL formalism, easily capture desired realizations means
IDL-expression:
k(PP1 , PP2 , NP1 (V NP2 )).

296

fiIDL-Expressions: Formalism Finite Languages

Again, note presence lock operator, implements restriction
modifiers appearing verbal object position, similarly
done IDL-expression (2).
Consider sentence subordinate clause, structure
[S NP1 V1 NP2 [S0 NP3 V2 NP4 ]],
assume modifiers PP1 PP2 apply main clause, modifiers PP3
PP4 apply subordinate clause. before, 3 4 possible realizations
subordinate sentence. allow main clause modifiers appear positions
subordinate clause well subordinate clause, 45 possible realizations
main sentence. Overall, gives total 3 42 5 = 240 possible sentence
realizations.
Again, bag representation sentences unable capture restrictions word order, would therefore badly overgenerate. Since main sentence
modifiers could placed subordinate clause, need record
two modifiers main clause whether already seen, processing 12
possible realizations subordinate clause. increases size representation
factor 2 2 = 4. hand, desired realizations easily captured
means IDL-expression:
k(PP1 , PP2 , NP1 (V1 NP2 ) (k(PP3 , PP4 , NP3 (V2 NP4 )))).
Note use embedded lock operators (the two rightmost occurrences). rightmost
leftmost occurrences lock operator implement restriction modifiers appearing verbal object position. occurrence lock
operator middle IDL-expression prevents modifiers PP1 PP2
modifying elements appearing within subordinate clause. Observe
generalize examples embedding n subordinate clauses, corresponding word
lattice grow exponentially n, IDL-expression linear size n.

5. IDL-Graphs
Although IDL-expressions may easily composed linguists, allow direct
algorithmic interpretation efficient recognition strings. therefore define equivalent lower-level representation IDL-expressions, call IDL-graphs.
purpose, exploit specific kind edge-labelled acyclic graphs ranked nodes.
first introduce notation, define encoding function IDL-expressions
IDL-graphs.
graphs use denoted tuples (V, E, vs , , , r), where:
V E finite sets vertices edges, respectively;
vs special vertices V called start end vertices, respectively;
edge-labelling function, mapping E alphabet {, `, a};
r vertex-ranking function, mapping V N, set non-negative integer
numbers.
297

fiNederhof & Satta

Label indicates edge consume input symbols. Edge labels `
meaning, additionally encode start end, respectively, corresponds operator. precisely, let IDL-expression
headed occurrence operator let () associated IDL-graph.
use edges labelled ` connect start vertex () start vertices
subgraphs encoding arguments I. Similarly, use edges labelled connect
end vertices subgraphs encoding arguments end vertex
(). Edge labels ` needed next section distinguish occurrences
operator occurrences L operators. Finally, function r ranks
vertex according deeply embedded (the encoding of) expressions headed
occurrence L operator. see later, information necessary
processing locked vertices correct priority.
map IDL-expression corresponding IDL-graph.
Definition 3 Let finite alphabet, let j non-negative integer number.
IDL-expression associated graph j () = (V, E, vs , , , r)
specified follows:
(i) = a, {E}, let vs , new nodes;
(a) V = {vs , },
(b) E = {(vs , )},
(c) ((vs , )) = ((vs , )) = = E,
(d) r(vs ) = r(ve ) = j;
(ii) = ( 0 ) j+1 ( 0 ) = (V 0 , E 0 , vs0 , ve0 , 0 , r0 ), let vs , new nodes;
(a) V = V 0 {vs , },
(b) E = E 0 {(vs , vs0 ), (ve0 , )},
(c) (e) = 0 (e) e E 0 , ((vs , vs0 )) = ((ve0 , )) = ,
(d) r(v) = r0 (v) v V 0 , r(vs ) = r(ve ) = j;
(iii) = (1 , 2 , . . . , n ) j (i ) = (Vi , Ei , vi,s , vi,e , , ri ), 1 n, let vs ,
new nodes;
(a) V = ni=1 Vi {vs , },
(b) E = ni=1 Ei {(vs , vi,s ) | 1 n} {(vi,e , ) | 1 n},
(c) (e) = (e) e Ei , ((vs , vi,s )) = ((vi,e , )) = 1 n,
(d) r(v) = ri (v) v Vi , r(vs ) = r(ve ) = j;
(iv) = k(1 , 2 , . . . , n ) j (i ) = (Vi , Ei , vi,s , vi,e , , ri ), 1 n, let vs ,
new nodes;
(a) V = ni=1 Vi {vs , },
(b) E = ni=1 Ei {(vs , vi,s ) | 1 n} {(vi,e , ) | 1 n},
298

fiIDL-Expressions: Formalism Finite Languages

v1



v2

necessarily

0

v0



0

v5



0


v3

vs

v4

must

0

0


0

0

0
v6



0

v7

0



v8



0

v9 playv10

1

v11 pianov12

1

1

Figure 1:
IDL-graph
associated


k((necessarily, must), (play piano)).

v13

1

0

IDL-expression

(c) (e) = (e) e Ei , ((vs , vi,s )) = ` ((vi,e , )) = 1 n,
(d) r(v) = ri (v) v Vi , r(vs ) = r(ve ) = j;
(v) = 1 2 j (i ) = (Vi , Ei , vi,s , vi,e , , ri ), {1, 2}, let vs = v1,s = v2,e ;

(a) V = V1 V2 ,
(b) E = E1 E2 {(v1,e , v2,s )},
(c) (e) = (e) e Ei {1, 2}, ((v1,e , v2,s )) = ,
(d) r(v) = ri (v) v Vi , {1, 2}.
let () = 0 (). IDL-graph graph form () IDLexpression .
Figure 1 presents IDL-graph (), IDL-expression (2).
introduce important notion cut IDL-graph. notion needed
define language described IDL-graph, talk equivalence
IDL-expressions IDL-graphs. time, notion play crucial
role specification parsing algorithm IDL-graphs next section. Let
us fix IDL-expression let () = (V, E, vs , , , r) associated IDLgraph. Intuitively speaking, cut () set vertices might reach
simultaneously traversing () start vertex end vertex, following
different branches prescribed encoded I, L operators, attempt
produce string L().
follows view V finite alphabet, define set V contain
strings V symbol occurs once. Therefore V finite set
string c V |c| |V |. assume outgoing edges vertex
IDL-graph linearly ordered, represent cuts canonical way means
strings V defined below.
299

fiNederhof & Satta

Let r ranking function associated (). write c[v1 vm ] denote
string c V satisfying following conditions:
c form xv1 vm x, V vi V 1 m;
vertex v within c i, 1 m, r(v) r(vi ).
words, c[v1 vm ] indicates vertices v1 , . . . , vm occur adjacent c
maximal rank among vertices within string c. Let c[v1 vm ] = xv1 vm string
0 V second string symbol v 0 , 1 m0 ,
defined let v10 vm
0

0 ] denote string xv 0 v 0 V .
appears x y. write c[v1 vm := v10 vm
0
1
m0
reason distinguish vertices maximal rank lower rank
former correspond subexpressions nested deeper within subexpressions headed L operator. substring originating within scope occurrence
lock operator cannot interleaved symbols originating outside scope,
terminate processing vertices higher rank resuming processing
lower rank.
define relation plays crucial role definition notion cut,
well specification parsing algorithm.
Definition 4 Let finite alphabet, let IDL-expression , let
() = (V, E, vs , , , r) associated IDL-graph. relation () V ( {}) V
smallest satisfying following conditions:
(i) c[v] V (v, v 0 ) E ((v, v 0 )) = X {},
(c[v], X, c[v := v 0 ]) () ;

(4)

(ii) c[v] V outgoing edges v exactly (v, v1 ), . . . , (v, vn ) E,
order, ((v, vi )) = `, 1 n,
(c[v], , c[v := v1 vn ]) () ;

(5)

(iii) c[v1 vn ] V incoming edges v V exactly
(v1 , v), . . . , (vn , v) E, order, ((vi , v)) = a, 1 n,
(c[v1 vn ], , c[v1 vn := v]) () .

(6)

Henceforth, abuse notation writing place () . Intuitively speaking,
relation used simulate one-step move IDL-graph (). Condition (4)
refers moves follow single edge graph, labelled symbol alphabet
empty string. move exploited, e.g., upon visiting vertex start
subgraph encodes IDL-expression headed occurrence operator.
case, outgoing edge represents possible next move, one edge
chosen. Condition (5) refers moves simultaneously follow edges emanating
vertex hand. used processing vertex start subgraph
encodes IDL-expression headed occurrence operator. fact, accordance
300

fiIDL-Expressions: Formalism Finite Languages

given semantics, possible argument expressions must evaluated parallel
single computation. Finally, Condition (6) refers move read
complement previous type move.
Examples elements case Figure 1 (vs , , v0 v6 ) following
Condition (5) (v5 v13 , , ) following Condition (6), start end
evaluation occurrence operator. elements (v0 v6 , , v1 v6 ),
(v1 v9 , play, v1 v10 ) (v1 v13 , necessarily, v2 v13 ) following Condition (4). Note that, e.g.,
(v1 v10 , necessarily, v2 v10 ) element , v9 higher rank v1 .
ready define notion cut.
Definition 5 Let finite alphabet, let IDL-expression , let
() = (V, E, vs , , , r) associated IDL-graph. set cuts (), written
cut(()), smallest subset V satisfying following conditions:
(i) string vs belongs cut(());
(ii) c cut(()) (c, X, c0 ) , string c0 belongs cut(()).
Henceforth, abuse notation writing cut() cut(()). already remarked,
interpret cut v1 v2 vk cut(), vi V 1 k, follows.
attempt generate string L(), traverse several paths IDL-graph ().
corresponds parallel evaluation subexpressions , vi
v1 v2 vk refers one subexpression. Thus, k provides number evaluations
carrying parallel point computation represented cut. Note
however that, drawing straight line across planar representation IDL-graph,
separating start vertex end vertex, set vertices identify
necessarily cut.4 fact, already explained discussing relation ,
one path followed start subgraph encodes IDL-expression headed
occurrence operator. Furthermore, even several arcs followed
start subgraph encodes IDL-expression headed occurrence
operator, combinations vertices satisfy definition cut
L operators within argument expressions. observations precisely
addressed Section 7, provide mathematical analysis complexity
algorithm.
Examples cuts case Figure 1 vs , , v0 v6 , v1 v6 , v3 v6 , v0 v7 , etc. Strings
v1 v3 cuts, v1 v3 belong two disjoint subgraphs sets vertices
{v1 , v2 } {v3 , v4 }, respectively, corresponds different argument
occurrence disjunction operator.
Given notion cut, associate finite language IDL-graph
talk equivalence IDL-expressions. Let IDL-expression , let
() = (V, E, vs , , , r) associated IDL-graph. Let also c, c0 cut() w .
write w L(c, c0 ) exists q |w|, Xi {}, 1 q, ci cut(),
0 q, X1 Xq = w, c0 = c, cq = c0 (ci1 , Xi , ci ) 1 q.
4. pictorial representation mentioned comes close different definition cut standard
literature graph theory operating research. reader aware standard
graph-theoretic notion cut different one introduced paper.

301

fiNederhof & Satta

also assume L(c, c) = {}. show L(vs , ) = L(), i.e.,
language generated IDL-expression language obtain
traversal IDL-graph (), described above, starting cut vs ending
cut . proof property rather long add much already
provided intuition underlying definitions section; therefore omit it.
close section informal discussion relation associated notion
cut. Observe Definition 4 Definition 5 implicitly define nondeterministic finite
automaton. Again, refer reader Harrison (1978) definition finite automata.
states automaton cuts cut() transitions given
elements . initial state automaton cut vs , final state
cut . difficult see every state automaton one always reach
final state. Furthermore, language recognized automaton precisely
language L(vs , ) defined above. However, remark automaton
never constructed parsing algorithm, emphasized next section.

6. CFG Parsing IDL-Graphs
start section brief overview Earley algorithm (Earley, 1970), wellknown tabular method parsing input strings according given CFG. reformulate Earley algorithm order parse IDL-graphs. already mentioned
introduction, parsing traditionally defined input consisting single string,
conceive parsing process carried input device representing
language, i.e., set strings.
Let G = (N , , P, S) CFG, let w = a1 input string
parsed. Standard implementations Earley algorithm (Graham & Harrison, 1976) use
called parsing items record partial results parsing process w. parsing item
form [A , i, j], production G j indices
identifying substring ai+1 aj w. parsing item constructed algorithm
exist string (N ) two derivations G form
G a1 ai
G a1 ai ;
G ai+1 aj .
algorithm accepts w construct item form [S , 0, n],
production G. Figure 2 provides abstract specification algorithm
expressed deduction system, following Shieber, Schabes, Pereira (1995). Inference
rules specify types steps algorithm apply constructing new items.
Rule (7) Figure 2 serves initialization step, constructing items
start analyses productions start symbol right-hand side. Rule (8)
similar purpose: constructs items start analyses productions
nonterminal B left-hand side, provided B next nonterminal
existing item analysis found. Rule (9) matches terminal item
input symbol, new item signifies larger part right-hand side
matched larger part input. Finally, Rule (10) combines two partial

302

fiIDL-Expressions: Formalism Finite Languages

[S , 0, 0]





(7)

[A B, i, j]
B
[B , j, j]

(8)

[A a, i, j]
= aj+1
[A , i, j + 1]

(9)

[A B, i, j]
[B , j, k]
[A B , i, k]

(10)

Figure 2: Abstract specification parsing algorithm Earley input string
a1 . algorithm accepts w construct item
form [S , 0, n], production G.

analyses, second represents analysis symbol B, analysis
represented first item extended.
move algorithm IDL-graph parsing using CFG. algorithm
makes use relation Definition 4, mean relation
fully computed invoking algorithm. instead compute elements onthe-fly first visit cut, cache elements possible later use.
advantage that, parsing input IDL-graph, algorithm processes
portions graph represent prefixes strings generated CFG
hand. practical cases, input IDL-graph never completely unfolded,
compactness proposed representation preserved large extent.
alternative way viewing algorithm this. already informally discussed Section 5 relation implicitly defines nondeterministic finite automaton
whose states elements cut() whose transitions elements .
also mentioned automaton precisely recognizes finite language L().
perspective, algorithm seen standard lattice parsing algorithm,
discussed Section 2. must emphasized precompute
finite automaton prior parsing. approach consists lazy evaluation
transitions automaton, basis demand part parsing process.
contrast approach, full expansion finite automaton parsing
several disadvantages. Firstly, although finite automaton generating finite language

303

fiNederhof & Satta

might considerably smaller representation language consisting
list elements, easy see cases finite automaton
might size exponentially larger corresponding IDL-expression (see also
discussion Section 2). cases, full expansion destroys compactness IDLexpressions, main motivation use formalism hybrid surface
generation systems, discussed introduction. Furthermore, full expansion
automaton also computationally unattractive, since may lead unfolding parts
input IDL-graph never processed parsing algorithm.
Let G = (N , , P, S) CFG let input IDL-expression. algorithm
uses parsing items form [A , c1 , c2 ], production P
c1 , c2 cut(). items meaning used original Earley
algorithm, refer strings languages L(vs , c1 ) L(c1 , c2 ), vs
start vertex IDL-graph (). (Recall Section 5 L(c, c0 ), c, c0 cut(),
set strings whose symbols consumed traversal () starting
cut c ending cut c0 .) also use items forms [c1 , c2 ] [a, c1 , c2 ], ,
c1 , c2 cut(). done order by-pass traversals () involving sequence zero
triples form (c1 , , c2 ) , followed triple form (c1 , a, c2 ) .
Figure 3 presents abstract specification algorithm, using set inference
rules. issues control flow implementation deferred next section.
follows, let vs start end vertices IDL-graph (), respectively. Rules (11), (12) (15) Figure 3 closely resemble Rules (7), (8) (10)
original Earley algorithm, reported Figure 2. Rules (13), (16) (17) introduced purpose efficiently computing traversals () involving sequence zero
triples form (c1 , , c2 ) , followed triple form (c1 , a, c2 ) ,
already mentioned. one traversal computed, fact recorded
item form [a, c1 , c2 ], avoiding later recomputation. Rule (14) closely
resembles Rule (9) original Earley algorithm. Finally, computing traversals
() involving triples form (c1 , , c2 ) only, Rule (18) may derive items
form [S , vs , ]; algorithm accepts input IDL-graph
item derived inference rules.
turn discussion correctness algorithm Figure 3.
algorithm derives parsing item [A , c1 , c2 ] exist string
(N ) , integers i, j 0 j, a1 a2 aj following
conditions satisfied:
a1 ai L(vs , c1 );
ai+1 aj L(c1 , c2 );
exist two derivations G form
G a1 ai
G a1 ai
G ai+1 aj .
statement closely resembles existential condition previously discussed
original Earley algorithm, proved using arguments similar presented
304

fiIDL-Expressions: Formalism Finite Languages

[S , vs , vs ]





(11)

[A B, c1 , c2 ]
B
[B , c2 , c2 ]

(12)

[A a, c1 , c2 ]
[c2 , c2 ]

(13)

[A a, c1 , c2 ]
[a, c2 , c3 ]
[A , c1 , c3 ]
[A B, c1 , c2 ]
[B , c2 , c3 ]
[A B , c1 , c3 ]
[c1 , c2 ]
(c2 , , c3 )
[c1 , c3 ]

[c1 , c2 ]
[a, c1 , c3 ]



(c2 , a, c3 ) ,


[S , c0 , c1 ]
(c1 , , c2 )
[S , c0 , c2 ]

(14)

(15)

(16)

(17)

(18)

Figure 3: abstract specification parsing algorithm IDL-graphs. algorithm
accepts IDL-graph () item form [S
, vs , ] derived inference rules, production G
vs start end vertices (), respectively.

305

fiNederhof & Satta

instance Aho Ullman (1972) Graham Harrison (1976); therefore
omit complete proof here. Note correctness algorithm Figure 3 directly
follows statement, taking item [A , c1 , c2 ] form
[S , vs , ] production G.

7. Complexity Implementation
section provide computational analysis parsing algorithm IDL-graphs.
analysis based development tight upper bound number possible
cuts admitted IDL-graph. also discuss two possible implementations
parsing algorithm.
need introduce notation. Let IDL-expression let () =
(V, E, vs , , , r) associated IDL-graph. vertex v V called L-free () if,
every subexpression 0 j ( 0 ) = (V 0 , E 0 , vs0 , ve0 , 0 , r0 ) j, V 0 V ,
E 0 E, v V 0 , 0 form ( 00 ). words, vertex
L-free () belong subgraph () encodes IDL-expression
headed L operator. () understood context, write L-free
place L-free (). write 0-cut() denote set cuts cut()
contain vertices L-free (). introduce two functions used
later complexity analysis algorithm. cut c cut() write |c| denote
length c, i.e., number vertices cut.
Definition 6 Let IDL-expression. Functions width 0-width specified
follows:
width() =

max |c| ,

ccut()

0-width() =

max
c0-cut()

|c| .

Function width provides maximum length cut (). quantity gives
maximum number subexpressions need evaluated parallel
generating string L(). Similarly, function 0-width provides maximum length
cut () includes L-free nodes.
Despite fact cut() always finite set, computation functions width
0-width direct computation cut() 0-cut() practical, since
sets may exponential size number vertices (). next characterization
provides efficient way compute functions, used proof
Lemma 2 below.
Lemma 1 Let IDL-expression. quantities width() 0-width() satisfy
following equations:
(i) = a, {E},
width() = 1,
0-width() = 1;
306

fiIDL-Expressions: Formalism Finite Languages

(ii) = ( 0 )
width() = width( 0 ),
0-width() = 1;
(iii) = (1 , 2 , . . . , n )
n

width() = max width(i ),
i=1
n

0-width() = max 0-width(i );
i=1

(iv) = k(1 , 2 , . . . , n )
n

width() = max (width(j ) +
j=1

0-width() =

n
X

X

0-width(i )),

i:1ini6=j

0-width(j );

j=1

(v) = 1 2
width() = max {width(1 ), width(2 )},
0-width() = max {0-width(1 ), 0-width(2 )}.
Proof. equations statement lemma straightforwardly follow
definitions cut() (Definitions 4 5, respectively). develop
length two cases leave remainder proof reader. follows
assume () = (V, E, vs , , , r).
case = (1 , 2 , . . . , n ), let (i ) = (Vi , Ei , vi,s , vi,e , , ri ), 1 n.
Definition 4 (vs , , vi,s ) (vi,e , , ) , every i, 1 n. Thus
cut() = ni=1 cut(i ) {vs , } and, since vs L-free (),
0-cut() = ni=1 0-cut(i ) {vs , }. provides relations (iii).
case = k(1 , 2 , . . . , n ), let (i ) = (Vi , Ei , vi,s , vi,e , , ri ), 1 n.
Definition 4 (vs , , v1,s vn,s ) (v1,e vn,e , , ) . Thus every
c cut() must belong {vs , } must form c = c1 cn ci cut(i )
1 n. Since vs L-free (), immediately derive
0-cut() = {vs , } 0-cut(1 ) 0-cut(n ),
P
hence 0-width() = nj=1 0-width(j ). observe that, c = c1 cn specified
never indices j, 1 i, j n 6= j, vertices v1 v2
occurring ci cj , respectively, neither v1 v2 L-free ().
thereby derive
cut() = {vs , }
cut(1 )0-cut(2 ) 0-cut(n )
0-cut(1 )cut(2 ) 0-cut(n )
..
.
0-cut(1 )0-cut(2 ) cut(n ).
307

fiNederhof & Satta

P
Hence write width() = maxnj=1 (width(j ) + i:1ini6=j 0-width(i )).
consider quantity |cut()|, i.e., number different cuts IDL-graph ().
quantity obviously bounded |V |width() . derive tighter
upper bound quantity.
Lemma 2 Let finite alphabet, let IDL-expression , let () =
(V, E, vs , , , r) associated IDL-graph. Let also k = width().


|cut()|

|V |
k

k

.

Proof. use following inequality. integer h 2 real values xi > 0,
1 h,
h


xi

Ph

i=1

i=1

h

xi

!h

.

(19)

words, (19) states geometric mean never larger arithmetic mean.
prove (19) following equivalent
form. real values c > 0 yi ,
P
1 h h 2, yi > c hi=1 yi = 0,
h


(c + yi ) ch .

(20)

i=1

start observing yi equal zero, done. Otherwise
must j 1 i, j h yi yj < 0. Without loss generality, assume
= 1 j = 2. Since yi yj < 0,
(c + y1 )(c + y2 ) = c(c + y1 + y2 ) + y1 y2 < c(c + y1 + y2 ).
Since

Qh

i=3 (c

(21)

+ yi ) > 0,
(c + y1 )(c + y2 )

h


(c + yi ) < c(c + y1 + y2 )

i=3

h


(c + yi ).

(22)

i=3

observe right-hand side (22) form left-hand side
(20), fewer yi non-zero. therefore iterate procedure,
yi become zero valued. concludes proof (19).
Let us turn proof statement lemma. Recall cut c cut()
string V vertex V one occurrence c, c
canonically represented, i.e., permutation vertices c possible cut.
later prove following claim.
Claim. Let , V k statement lemma. partition V
subsets V [, j], 1 j k, following property. every V [, j], 1 j k,
every pair distinct vertices v1 , v2 V [, j], v1 v2 occur together cut
c cut().
308

fiIDL-Expressions: Formalism Finite Languages

write
|cut()|

Qk

j=1



Pk

=



|V [, j]|

j=1

|V |
k

k

|V [,j]|
k

k

(by claim canonical
representation cuts)
(by (19))

.

complete proof lemma need prove claim above. prove
following statement, slightly stronger version claim. partition
set V subsets V [, j], 1 j k = width(), following two properties:
every V [, j], 1 j k, every pair distinct vertices v1 , v2 V [, j], v1
v2 occur together cut c cut();
vertices V L-free () included V [, j], 1 j
0-width(). (In words, sets V [, j], 0-width() < j width(),
contain vertices L-free ().)
follows use induction #op (), number operator occurrences (I, D, L
concatenation) appearing within .
Base: #op () = 0. = a, {E}, V = {vs , vf }. Since width() = 1,
set V [, 1] = V . satisfies claim, since cut() = {vs , vf }, vertices V
L-free () 0-width() = 1.
Induction: #op () > 0. distinguish among three possible cases.
Case 1: = (1 , 2 , . . . , n ). Let (i ) = (Vi , Ei , vi,s , vi,e , , ri ), 1 n. Lemma 1
width() = maxni=1 width(i ). i, 1 n, let us define V [i , j] =
every j width(i ) < j width(). set
V [, 1] = (ni=1 V [i , 1]) {vs , };
V [, j] = ni=1 V [i , j], 2 j width().
sets V [, j] define partition V , since V = (ni=1 Vi ) {vs , } and, i,
sets V [i , j] define partition Vi inductive hypothesis. show
partition satisfies two conditions statement.
Let v1 v2 two distinct vertices V [, j]. already established
proof Lemma 1 cut() = (ni=1 cut(i )) {vs , }. either v1 v2 belongs
set {vs , }, v1 v2 cannot occur cut cut(), since
cuts cut() vertices set {vs , } vs . Let us consider case
v1 , v2 ni=1 Vi . distinguish two subcases. first subcase, exists
v1 , v2 V [i , j]. inductive hypothesis states v1 v2 cannot occur
cut cut(i ), hence cannot occur cut cut(). second
subcase, v1 V [i , j] v2 V [i0 , j] distinct i0 . v1 v2 must belong
different graphs (i ) (i0 ), hence cannot occur cut cut().
Furthermore, every vertex ni=1 Vi L-free (i ) belongs
V [i , j] 1 j 0-width(i ), inductive hypothesis. Since 0-width() =

309

fiNederhof & Satta

maxni=1 0-width(i ) (Lemma 1) state vertices V L-free ()
belong V [, j], 1 j 0-width().
Case 2: = ( 0 ) = 1 2 . proof almost identical Case 1,
n = 1 n = 2, respectively.
Case 3: = k(1 , 2 , . . . , n ). Let (i ) = (Vi , Ei , vi,s , vi,e , , ri ), 1 n. Lemma 1

0-width() =

n
X

0-width(j ),

j=1
n

width() = max (width(j ) +
j=1

X

0-width(i )).

i:1ini6=j

latter equation rewritten
width() =

n
X

n

0-width(j ) + max (width(j ) 0-width(j )).
j=1

j=1

(23)

1 n, let us define V [i , j] = every j width(i ) < j width().
set
V [, 1] = V [1 , 1] {vs , };
V [, j] = V [1 , j], 2 j 0-width(1 );
V [, 0-width(1 ) + j] = V [2 , j], 1 j 0-width(2 );
..
.
Pn1
V [, Pi=1 0-width(i ) + j] = V [n , j], 1 j 0-width(n );
V [, ni=1 0-width(i ) + j] = ni=1 V [i , 0-width(i ) + j],
1 j maxnj=1 (width(j ) 0-width(j )).
sets V [, j] define partition V , since V = (ni=1 Vi ) {vs , } and, i,
sets V [i , j] define partition Vi inductive hypothesis. show
partition satisfies conditions statement.
Let v1 v2 distinct vertices V [, j], 1 j n. already established
proof Lemma 1 cut c cut() either belongs {vs , } else must
form c = c1 cn ci cut(i ) 1 n. Case 1, either v1 v2 belongs
set {vs , }, v1 v2 cannot occur cut cut(), since
cuts cut() vertices set {vs , } vs . Consider case
v1 , v2 ni=1 Vi . distinguish two subcases.
first subcase, exists v1 , v2 V [i , j]. exists cut
c cut() v1 v2 occur within c, v1 v2 must occur within
c0 cut(i ). contradicts inductive hypothesis .
second subcase, v1 V [i0 , j 0 ] v2 V [i00 , j 00 ], distinct i0 i00 . Note
happen 0-width() < j width(), 0-width(i0 ) < j 0 width(i0 )
0-width(i00 ) < j 00 width(i00 ), definition partition V (23).
inductive hypothesis i0 i00 , v1 L-free (i0 ) v2 L-free (i00 ),
means v1 v2 occur within scope occurrence lock
310

fiIDL-Expressions: Formalism Finite Languages

operator. Note however v1 v2 cannot occur within scope occurrence
lock operator, since belong different subgraphs (i0 ) (i00 ). Assume
exists cut c cut() v1 v2 occur within c. would
inconsistent definitions cut (Definitions 4 5, respectively) since
two vertices L-free within scope occurrence
lock operator cannot belong cut.
Finally, directly follows definition partition V inductive hypothesis vertices V L-free () belong
V [, j] 1 j 0-width(). concludes proof statement.
upper bound reported Lemma 2 tight. example, 1 k 2,
let i,k = {a1 , . . . , aik }. Consider class IDL-expressions
i,k = k(a1 a2 ai , ai+1 ai+2 a2i , . . . , ai(k1)+1 ai(k1)+2 aik ).
Let also Vi,k vertex set IDL-graph (i,k ). difficult see
|Vi,k | = 2 k + 2, width(i,k ) = k
2
|cut(i,k )| = (2 i)k + 2 (2 + )k ,
k
inequality results upper bound. coarser upper bound presented
Lemma 2 would give instead |cut(i,k )| < (2 k + 2)k .
turn discussion worst case running time algorithm
Figure 3. simplify presentation, let us ignore moment term solely
depends input grammar G.
store retrieve items [A , c1 , c2 ], [a, c1 , c2 ] [c1 , c2 ] exploit
data structure access using cut c1 cut c2 indices. follows make
assumption access operation carried amount time
O(d(k)), k = width() function depends implementation
data structure itself, discussed later. access pair c1 , c2 ,
array returned length proportional |G|. Thus, array inquire
constant time whether given item already constructed.
worst case time complexity dominated rules Figure 3 involve
maximum number cuts, namely rules like (15) three cuts each. maximum
number different calls rules proportional |cut()|3 . Considering
assumptions , total amount time charged execution
rules O(d(k) |cut()|3 ). case standard Earley algorithm,
working grammar G taken account must include factor |G|2 ,
reduced |G| using techniques discussed Graham, Harrison, Ruzzo (1980).
also need consider amount time required construction relation
, happens on-the-fly, already discussed. takes place Rules (16), (17)
(18). Recall elements relation form (c1 , X, c2 ) c1 , c2 cut()
X {}. follows, view directed graph whose vertices cuts,
thus refer elements relation (labelled) arcs. arc emanating
cut c1 label X visited first time, compute arc
reached cut, cache possible later use. However, case reached cut c2
already exists previously visited arc (c01 , X 0 , c2 ), cache
311

fiNederhof & Satta

new arc. arc , easily carried time O(k),
k = width(). total time required on-the-fly construction relation
O(k | |). later use, express bound terms quantity |cut()|.
definition easily see one arc two
cuts, therefore | | |cut()|2 . obviously k |V |. Also, difficult
prove |V | |cut()|, using induction number operator occurrences appearing
within . thus conclude that, worst case, total time required on-the-fly
construction relation O(|cut()|3 ).
observations conclude that, worst case, algorithm
Figure 3 takes amount time O(|G| d(k) |cut()|3 ). Using Lemma 2,
state following theorem.
Theorem 1 Given context-free grammar G IDL-graph () vertex set V
k = width(), algorithm Figure 3 runs time O(|G| d(k)( |Vk | )3k ).
closely consider choice data structure issue
implementation. discuss two possible solutions. first solution used
|cut()| small enough store |cut()|2 pointers computers randomaccess memory. case implement square array pointers sets
parsing items. cut cut() uniquely encoded non-negative integer,
integers used access array. solution practice comes
standard implementation Earley algorithm parse table, presented
Graham et al. (1980). d(k) = O(1) algorithm time complexity
O(|G| ( |Vk | )3k ).
second solution, |cut()| quite large, implement trie (Gusfield,
1997). case cut treated string set V , viewed alphabet,
look string c1 #c2 (# symbol V ) order retrieve items involving
cuts c1 c2 induced far. obtain d(k) = O(k) algorithm
time complexity O(|G| k( |Vk | )3k ).
first solution faster second one factor k. However, first
solution obvious disadvantage expensive space requirements, since pairs
cuts might correspond grammar constituent, array sparse
practice. also observed that, natural language processing applications
discussed introduction, k quite small, say three four.
conclude section, compare time complexity CFG parsing traditionally
defined strings time complexity parsing IDL-graphs. reference string
parsing take Earley algorithm, already presented Section 6.
minor change proposed Graham et al. (1980), Earley algorithm improved
time complexity O(|G| n3 ), G input CFG n length input
string. observe that, ignore factor d(k) time complexity IDL-graph
parsing (Theorem 1), two upper bounds become similar, function ( |Vk | )k
IDL-graph parsing replacing input sentence length n Earley algorithm.
observe function ( |Vk | )k taken measure complexity
internal structure input IDL-expression. specifically, assume precedence
constraints given words input IDL-expression. obtain IDLexpressions occurrences operator only, worst case k = |V2 | 1.
312

fiIDL-Expressions: Formalism Finite Languages

O(( |Vk | )k ) written O(c|V | ) constant c > 1, resulting exponential
running time algorithm. comes surprise, since problem hand
becomes problem recognition bag words CFG, known
NP-complete (Brew, 1992), already discussed Section 2.
Conversely, operator may used IDL-expression , thus resulting
representation matches finite automaton word lattice. case k = 1
function ( |Vk | )k becomes |V |. resulting running time cubic function
input length, case Earley algorithm. fact (cyclic acyclic) finite
automata parsed cubic time also well-known result (Bar-Hillel et al., 1964;
van Noord, 1995).
noteworthy observe applications k assumed bounded,
algorithm still runs polynomial time. already discussed, practical applications
natural language generation, subexpressions processed simultaneously, k typically, say, three four. case algorithm behaves
way much closer traditional string parsing bag parsing.
conclude class IDL-expressions provides flexible representation bags
words precedence constraints, solutions range pure word bags
without precedence constraints word lattices, depending value width().
also proved fine-grained result time complexity CFG parsing problem
IDL-expressions, depending values parameter width().

8. Final Remarks
Recent proposals view natural language surface generation multi-phase process
finite large sets candidate sentences first generated basis input
conceptual structure, filtered using statistical knowledge. architectures,
crucial adopted representation set candidate sentences compact,
time representation parsed polynomial time.
proposed IDL-expressions solution problem. IDL-expressions
combine features considered isolation before. contrast existing
formalisms, interaction features provides enough flexibility encode strings
cases partial knowledge available word order, whereas parsing
process remains polynomial practical cases.
recognition algorithm presented IDL-expressions easily extended
parsing algorithm, using standard representations parse forests extracted
constructed parse table (Lang, 1994). Furthermore, productions CFG
hand weighted, express preferences among derivations, easy extract parse
highest weight, adapting standard Viterbi search techniques used traditional
string parsing (Viterbi, 1967; Teitelbaum, 1973).
Although considered parsing problem CFGs, one may also parse
IDL-expressions language models based finite automata, including n-gram models. Since finite automata represented right-linear context-free grammars,
algorithm Figure 3 still applicable.
Apart natural language generation, IDL-expressions useful wherever uncertainty word constituent order represented level syntax

313

fiNederhof & Satta

linearized purpose parsing. already discussed introduction,
active research topic generative linguistics natural language parsing,
given rise several paradigms, importantly immediate dominance linear precedence parsing (Gazdar, Klein, Pullum, & Sag, 1985), discontinuous parsing Daniels
Meurers (2002), Ramsay (1999), Suhre (1999) grammar linearization (Gotz & Penn,
1997; Gotz & Meurers, 1995; Manandhar, 1995). Nederhof, Satta, Shieber (2003)
use IDL-expressions define new rewriting formalism, based context-free grammars
IDL-expressions right-hand sides productions. means formalism,
fine-grained results proven immediate dominance linear precedence parsing.5
IDL-expressions similar spirit formalisms developed programming language literature representation semantics concurrent programs.
specifically, called series-parallel partially ordered multisets, series-parallel pomsets,
proposed Gischer (1988) represent choice parallelism among processes.
However, basic idea lock operator absent series-parallel pomsets.

Acknowledgments
preliminary version paper appeared Proceedings 7th Conference
Formal Grammars (FG2002), Trento, Italy. notions IDL-graph cut, central
present study, found earlier paper. wish thank Michael Daniels,
Irene Langkilde, Owen Rambow Stuart Shieber helpful discussions related
topics paper. also grateful anonymous reviewers helpful comments pointers relevant literature. first author supported PIONIER
Project Algorithms Linguistic Processing, funded NWO (Dutch Organization
Scientific Research). second author supported MIUR project PRIN No.
2003091149 005.

References
Aho, A., & Ullman, J. (1972). Parsing, Vol. 1 Theory Parsing, Translation
Compiling. Prentice-Hall.
Aust, H., Oerder, M., Seide, F., & Steinbiss, V. (1995). Philips automatic train
timetable information system. Speech Communication, 17, 249262.
Bangalore, S., & Rambow, O. (2000). Exploiting probabilistic hierarchical model generation. 18th International Conference Computational Linguistics, Vol. 1,
pp. 4248, Saarbrucken, Germany.
Bar-Hillel, Y., Perles, M., & Shamir, E. (1964). formal properties simple phrase
structure grammars. Bar-Hillel, Y. (Ed.), Language Information: Selected
Essays Theory Application, chap. 9, pp. 116150. Addison-Wesley.
Beaven, J. (1992). Shake-and-bake machine translation. Proc. fifteenth International Conference Computational Linguistics, Vol. 2, pp. 602609, Nantes.
5. cited work, lock operator ignored, affect weak generative capacity
compactness grammars.

314

fiIDL-Expressions: Formalism Finite Languages

Billot, S., & Lang, B. (1989). structure shared forests ambiguous parsing. 27th
Annual Meeting Association Computational Linguistics, Proceedings
Conference, pp. 143151, Vancouver, British Columbia, Canada.
Brew, C. (1992). Letting cat bag: generation Shake-and-Bake MT.
Proc. fifteenth International Conference Computational Linguistics, Vol. 2,
pp. 610616, Nantes.
Brown, P., et al. (1990). statistical approach machine translation. Computational
Linguistics, 16 (2), 7985.
Brzozowski, J. (1964). Derivatives regular expressions. Journal ACM, 11 (4),
481494.
Charniak, E. (2001). Immediate-head parsing language models. 39th Annual Meeting
10th Conference European Chapter Association Computational
Linguistics, Proceedings Conference, pp. 116123, Toulouse, France.
Daniels, M., & Meurers, W. (2002). Improving efficiency parsing discontinuous
constituents. Wintner, S. (Ed.), Proceedings NLULP02: 7th International
Workshop Natural Language Understanding Logic Programming, Vol. 92
Datalogiske Skrifter, pp. 4968, Copenhagen. Roskilde Universitetscenter.
Dassow, J., & Paun, G. (1989). Regulated Rewriting Formal Language Theory. SpringerVerlag.
Earley, J. (1970). efficient context-free parsing algorithm. Communications ACM,
13 (2), 94102.
Gazdar, G., Klein, E., Pullum, G., & Sag, I. (1985). Generalized Phrase Structure Grammar.
Harvard University Press, Cambridge, MA.
Gischer, J. (1988). equational theory pomsets. Theoretical Computer Science, 61,
199224.
Gotz, T., & Meurers, W. (1995). Compiling HPSG type constraints definite clause
programs. 33rd Annual Meeting Association Computational Linguistics,
Proceedings Conference, pp. 8591, Cambridge, Massachusetts, USA.
Gotz, T., & Penn, G. (1997). proposed linear specification language. Volume 134
Arbeitspapiere des SFB 340, Universitat Tubingen.
Graham, S., & Harrison, M. (1976). Parsing general context free languages. Advances
Computers, Vol. 14, pp. 77185. Academic Press, New York, NY.
Graham, S., Harrison, M., & Ruzzo, W. (1980). improved context-free recognizer. ACM
Transactions Programming Languages Systems, 2 (3), 415462.
Gusfield, D. (1997). Algorithms Strings, Trees Sequences. Cambridge University
Press, Cambridge, UK.
Harrison, M. (1978). Introduction Formal Language Theory. Addison-Wesley.
Jurafsky, D., & Martin, J. (2000). Speech Language Processing. Prentice-Hall.
Knight, K. (1999). Decoding complexity word-replacement translation models. Computational Linguistics, 25 (4), 607615.
315

fiNederhof & Satta

Knight, K., & Hatzivassiloglou, V. (1995). Two-level, many-paths generation. 33rd
Annual Meeting Association Computational Linguistics, Proceedings
Conference, pp. 252260, Cambridge, Massachusetts, USA.
Lang, B. (1994). Recognition harder parsing. Computational Intelligence,
10 (4), 486494.
Langkilde, I. (2000). Forest-based statistical sentence generation. 6th Applied Natural
Language Processing Conference 1st Meeting North American Chapter
Association Computational Linguistics, pp. 170177 (Section 2), Seattle,
Washington, USA.
Langkilde, I., & Knight, K. (1998). Generation exploits corpus-based statistical knowledge. 36th Annual Meeting Association Computational Linguistics
17th International Conference Computational Linguistics, Vol. 1, pp. 704710,
Montreal, Quebec, Canada.
Manandhar, S. (1995). Deterministic consistency checking LP constraints. Seventh
Conference European Chapter Association Computational Linguistics,
Proceedings Conference, pp. 165172, Belfield, Dublin, Ireland.
Nederhof, M.-J., & Satta, G. (2004). language intersection problem non-recursive
context-free grammars. Information Computation. Accepted publication.
Nederhof, M.-J., Satta, G., & Shieber, S. (2003). Partially ordered multiset context-free
grammars free-word-order parsing. 8th International Workshop Parsing
Technologies, pp. 171182, LORIA, Nancy, France.
Nevill-Manning, C., & Witten, I. (1997). Compression explanation using hierarchical
grammars. Computer Journal, 40 (2/3), 103116.
Pollard, C., & Sag, I. (1994). Head-Driven Phrase Structure Grammar. University
Chicago Press.
Ramsay, A. (1999). Direct parsing discontinuous phrases. Natural Language Engineering, 5 (3), 271300.
Reape, M. (1989). logical treatment semi-free word order bounded discontinuous
constituency. Fourth Conference European Chapter Association
Computational Linguistics, Proceedings Conference, pp. 103110, Manchester,
England.
Reape, M. (1994). Domain union word order variation german. Nerbonne, J.,
Netter, K., & Pollard, C. (Eds.), German Head-Driven Phrase Structure Grammar,
pp. 151197. CSLI Publications.
Shieber, S., Schabes, Y., & Pereira, F. (1995). Principles implementation deductive
parsing. Journal Logic Programming, 24, 336.
Suhre, O. (1999). Computational aspects grammar formalism languages freer
word order. Diplomarbeit, Department Computer Science, University Tubingen.
Published 2000 Volume 154 Arbeitspapiere des SFB 340.

316

fiIDL-Expressions: Formalism Finite Languages

Teitelbaum, R. (1973). Context-free error analysis evaluation algebraic power series.
Conference Record Fifth Annual ACM Symposium Theory Computing,
pp. 196199.
van der Vlist, E. (2003). RELAX NG. OReilly.
van Noord, G. (1995). intersection finite state automata definite clause grammars. 33rd Annual Meeting Association Computational Linguistics,
Proceedings Conference, pp. 159165, Cambridge, Massachusetts, USA.
Viterbi, A. (1967). Error bounds convolutional codes asymptotically optimum
decoding algorithm. IEEE Transactions Information Theory, IT-13 (2), 260269.
Whitelock, P. (1992). Shake-and-Bake translation. Proc. fifteenth International
Conference Computational Linguistics, Vol. 2, pp. 784790, Nantes.

317

fiJournal Articial Intelligence Research 21 (2004) 37-62

Submitted 03/03; published 01/04

K-Implementation
Dov Monderer
Moshe Tennenholtz

dov@ie.technion.ac.il
moshet@ie.technion.ac.il

Faculty Industrial Engineering Management
Technion Israel Institute Technology
Haifa 32000, Israel

Abstract
paper discusses interested party wishes inuence behavior agents
game (multi-agent interaction), control. interested party
cannot design new game, cannot enforce agents behavior, cannot enforce payments
agents, cannot prohibit strategies available agents. However, inuence
outcome game committing non-negative monetary transfers dierent
strategy proles may selected agents. interested party assumes
agents rational commonly agreed sense use dominated strategies.
Hence, certain subset outcomes implemented given game adding nonnegative payments, rational players necessarily produce outcome subset.
Obviously, making suciently big payments one implement desirable outcome.
question cost implementation? paper introduce notion
k-implementation desired set strategy proles, k stands amount
payment need actually made order implement desirable outcomes. major
point k-implementation monetary oers need necessarily materialize
following desired behaviors. dene study k-implementation contexts games
complete incomplete information. latter case mainly focus VCG
games. setting later extended deal mixed strategies using correlation devices.
Together, paper introduces studies implementation desirable outcomes
reliable party cannot modify game rules (i.e. provide protocols), complementing
previous work mechanism design, making applicable many realistic CS
settings.

1. Introduction
design analysis interactions self-interested parties central theory
application multi-agent systems. particular, theory economic mechanism design
or, generally, implementation theory (Maskin, 1999; Maskin & Sjostrom, 2002)
become standard tool researchers areas multi-agent systems e-commerce
(Rosenschein & Zlotkin, 1994; Nisan & Ronen, 1999; Shoham & Tennenholtz, 2001; Feigenbuam & S, 2002; Tennenholtz, 1999; Papadimitriou, 2001). classical mechanism design1
center denes interaction self-motivated parties allow obtain
desired goal (such maximizing revenue social welfare) taking agents incentives
1. See e.g., Fudenberg Tirole (1991), Chapter 7, Mas-Colell, Whinston, Green (1995), Chapter
23.
c
2004
AI Access Foundation. rights reserved.

fiMonderer & Tennenholtz

account. perspective largely motivated view center
government seller dene control rules interaction. However,
many distributed systems multi-agent interactions, interested parties cannot control
rules interactions. network manager example cannot simply change communication protocols given distributed systems order lead desired behaviors,
broker cannot change rules goods sold agency auctioneer
public. focus paper reliable interested party, cannot change
rules interaction, cannot enforce behavior, obtain desired goals (in service
community benets). reliable party one source power:
reliability. commit payments dierent agents, certain observable
outcomes reached, agents sure paid appropriately.
work introduce study implementation desired behaviors interested
party above.2 two major issues make task non-trivial challenging:
1. interested party may wish assume little possible agents rationality.
Ideally, assumed agent adopt strategy dominated
another strategy.
2. interested party may wish minimize expenses.
Consider following simple congestion setting.3 Assume two agents,
1 2, select among two service providers (e.g., machines, communication
lines, etc.) One service providers, f , fast one, other, s, slower one.
capture agent obtaining payo 6 one
uses f , payo 4 one uses s. agents select
service provider speed operation decreases factor 2, leading half
payo. is, agents use f one obtains payo 3,
agents use one obtains 2. matrix form, game described
following bimatrix:

f



3

6

f
3

4

M=
4

2


6

2

2. another interesting use interested party see Naor, Pinkas, Sumner (1999).
3. Congestion context self-motivated parties central topic recent CS literature (Koutsoupias & Papadimitriou, 1999; Roughgarden, 2001; Roughgarden & Tardos, 2002), well
game theory literature (Rosenthal, 1973; Monderer & Shapley, 1996). example used purposes
illustration only; however, technique used example extended arbitrary complex
games, later show.

38

fiK-Implementation

Assume reliable interested party may wish prevent agents using
service provider (leading low payos both). follows:
promise pay agent 1 value 10 agents use f , promise pay agent 2
value 10 agents use s. promises transform following game:

f



13

6

f
3

=
4

4
2


6

12

Notice , strategy f dominant agent 1, strategy dominant
agent 2. result rational strategy prole one agent 1 chooses f
agent 2 chooses s. Hence, interested party implements one desired outcomes.
Moreover, given strategy prole (f, s) selected interested party
pay nothing. implemented (in dominant strategies) desired behavior (obtained
one Nash equilibria) zero cost, relying creditability, without
modifying rules interactions enforcing type behavior.
Similar simple examples found contexts (see e.g., Segal (1999), footnote
30, Dybvig Spatt (1983), Spiegler (2000)). work advocates following line
thought. Instead reasoning agents behave given protocol,
may wish cause agents follow particular behaviors making desirable, using
monetary oers. important point monetary oers need necessarily
fully materialized agents follow desired behavior.
formally, paper introduce notion study k-implementation
desired set strategy proles, k stands amount payment need
actually made order implement desirable outcomes.4 Section 3 provides
characterization k-implementation single pure strategy prole nite games
innite regular games complete information. provides eective algorithm
determining optimal monetary oers made order implement desired
outcome, minimizing expenses. Section 4 address problem nding kimplementation set strategy proles. show general problem regard
4. Notice perspective spirit work Articial Social Systems AI (see e.g., Shoham
Tennenholtz (1995)), search form modication system,
given modied system, assuming agents tend work individually, desirable outcome
obtained.

39

fiMonderer & Tennenholtz

NP-hard, consider modication k-implementation, titled exact implementation,
problem becomes tractable.5
Games incomplete information introduce challenges. particular, Section 5 consider VCG mechanisms combinatorial auctions 6 . setting
interesting characteristics since interested party cannot general see agents types
needs decide appropriate payment based observed behaviors. show
general 0-implementation (i.e. implementation zero cost) settings incomplete information impossible, ex-post equilibrium frugal VCG mechanism
0-implementable.
Section 6 study important case mixed strategies. context, unless
assume algorithmic observability, interested party observe actions selected probabilistic process leading selection, therefore earlier
results apply. example, consider simple routing problem above, one may
wish consider implementation fair outcome, one obtained
mixed strategy Nash equilibrium game . order address issue,
introduce concept implementation devices, show mixed strategy equilibrium 0-implementable implementation device. also show correlated
equilibrium property.

2. k-implementation
pre-game strategic form pair G = (N, X), N = {1, 2, , n} set
players, X = X1 X2 Xn , every i, Xi set strategies available
player i. Let player, set strategy proles players denoted Xi ,
generic element Xi denoted xi .
payo function vector n-tuple U = (U1 , U2 , , Un ), Ui : X
payo function player i. assume payos players represented
common monetary unit, payo functions bounded7 .
pre-game G payo function vector U denes game strategic form denoted
G(U ). game G(U ) nite strategy sets nite.
Let xi , yi strategies player game G(U ).
xi dominates yi Ui (xi , xi ) Ui (yi , xi ) every xi Xi , exists xi
Xi strict inequality holds. yi dominated strategy dominated
strategy i. xi dominant strategy dominates every strategy i.
prole strategies x (Nash) equilibrium every player i, xi best-response
5. Complexity implementation organizer controls structure game discussed
Conitzer Sandholm (2002).
6. VCG mechanisms (Vickrey, 1961; Clarke, 1971; Groves, 1973) widely discussed
context combinatorial auctions, topic received much attention recent multi-agent
systems e-commerce literature, e.g., (Nisan, 2000; Sandholm, Suri, Gilpin, & Levine, 2001; Parkes,
1999)
7. game nite payo functions automatically bounded.

40

fiK-Implementation

xi . is,
Ui (xi , xi ) Ui (yi , xi )

every N yi Xi .

is, every player believes players act according x, better
playing according x. Modern economic theory made (some times implicit)
assumption economic interactions equilibrium. However, rationale
assumption debate many cases, particulary exist multiple
equilibrium proles. contrast, using non-dominated strategy rational behavior
reasonable denition rationality. Moreover, refraining use dominated
strategies taken basic idea agreed upon technique decision theory.
Let G = (N, X) pre-game. every vector payo functions V , let Xi (V )
set non-dominated strategies game G(V ), let X(V ) = X1 (V )
X2 (V ), , Xn (V ). G(V ) game (N, X, V ), where, innocent abuse notations
V denotes vector payo functions restricted X. vector V payo functions
non-negative (V 0) Vi (x) 0 every player every x X.
Consider set desired strategy proles X game G(U ). non-negative
vector payo functions V implements G(U )
X(U + V ) O.
V called k-implementation G(U ), addition


n

i=1 Vi (x)

k every x X(U + V ).

Obviously, paying every player sucient amount money playing strategy
associated particular strategy prole O, one implement O.
is, interested party commits certain non-negative payos V ,
way rational players choose strategy proles O,
worst case interested party pay k.
Note implicitly made two important assumptions :
Output observability: interested party observe actions chosen
players.
Commitment power: interested party reliable sense players
believe indeed pay additional payo dened V .
However, requirement V 0 means interested party cannot force players
make payments based actions. addition, interested party cannot modify
set available strategies, enforce behavior way. reliably promise
positive monetary transfers conditioned observed outcome.
Let k(O) price implementing O. is, k(O) greatest lower bound
(GLB) non-negative numbers q exists q- implementation. is,
41

fiMonderer & Tennenholtz

k(O) = k implies every > 0 (k + )- implementation vector V ,
k - implementation k < k. V optimal implementation
V implements
n

max
Vi (x) = k(O).
xX(U +V )

i=1

V optimal implementation V implements
max
xX(U +V )

n


Vi (x) k(O) + .

i=1

3. k-Implementation singletons
singleton, = {z}, sometimes abuse notations say
z (instead {z}) k-implementation G(U ), refer k(z) price
implementing z.
3.1 Finite games
section focus nite games, characterization optimal k implementation singletons.
Theorem 1 Let G(U ) nite game least two strategies every player. Every
strategy prole z optimal implementation V , moreover:
k(z) =

n

i=1

max (Ui (xi , zi ) Ui (zi , zi )) .

xi Xi

3.1.1

Proof : Let z X let V implements z. Let N . xi = zi ,
xi , Vi (xi , xi ) > 0, one modify Vi changing term 0, get cheaper
implementation z. Hence, assume without loss generality deal
payo function vectors V which, every i, Vi (xi , ) = 0 every xi = zi .
zi dominant strategy G(U + V ),
Vi (zi , xi )+Ui (zi , xi ) Vi (xi , xi )+Ui (xi , xi )

every xi Xi , every xi Xi .

Since xi = zi , Vi (xi , ) = 0, necessary condition implementation
Vi (zi , xi ) + Ui (zi , xi ) Ui (xi , xi )

every xi Xi .

is,
Vi (zi , xi ) max (Ui (xi , xi ) Ui (zi , xi )).
xi Xi

42

fiK-Implementation

One use xi = zi order get costless strict inequality required denition
domination ( use assumption every player least two strategies).
Hence, optimal implementation vector z, V dened every by: Vi (xi , ) = 0
xi = zi , Vi (zi , xi ) = maxxi Xi (Ui (xi , zi ) Ui (zi , zi )) + (xi ), : Xi
nonnegative function satises (zi ) = 0, xi = zi , (xi ) > 0.
Therefore (3.1.1) satised.
Note z equilibrium every player i, maxxi Xi (Ui (xi , zi )
Ui (zi , zi )) = 0. Hence following characterization equilibrium corollary Theorem 1:
Corollary 1 Let G(U ) nite game least two strategies every player, let
z X. z equilibrium z zero- implementation.
3.2 Innite games
game G(U ) innite, one get phenomena contradicts intuition.
example, possible Xi = {zi } zi dominant strategy. E.g., consider
two-person game player 1 choose strategy z1 , number 0 < x1 < 1,
player 2 choose z2 x2 . U1 (z1 , z2 ) = 0.5, U1 (z1 , x2 ) = 10, U1 (x1 , ) = x1 . U2
arbitrary function. easily seen every x1 dominated bigger number
open interval (0,1), z1 dominated, hence X1 = {z1 }. However, z1
dominate x1 x1 > 0.5. Moreover, max operator used proofs Theorem 1
2 may well-dened innite games. game G(U ) = (N, X, U ) called regular
every Xi compact metric space, payo functions continuous X endowed
product metric.
Theorem 2 Theorems 1 holds regular games.
Proof : proof requires standard techniques, therefore omitted.
immediately get:
Corollary 2 Corollary 1 holds regular games.
3.3 Mixed strategies
every nite set B denote (B) set
probability distributions B. is,
(B) consists functions q : B [0, 1] bB q(b) = 1. Let G(U ) = (N, X, U )
nite game. mixed extension G(U ) innite game Gm (U ) = (N, X , U ),
X = (X1 ) (X2 ) (Xn ), every player i, Uim (p1 , p2 , , pn ) =


xX p1 (x1 )p2 (x2 ) pn (xn )Ui (x). is, Ui (p) expected payo player
every player j (including i) choosing strategy ( independently players)
randomizing device chooses strategy xj probability pj (xj ).
prole mixed strategies p X called mixed-strategy equilibrium G(U )
p equilibrium game Gm (U ). Nash (1950) every nite game possesses
43

fiMonderer & Tennenholtz

mixed strategy equilibrium. Note every strategy xi Xi identied
mixed strategy chooses xi probability 1. sense, Xi subset
Xim . deal environment mixed strategies considered, refer
every strategy xi Xi pure strategy i.
Note possibility using mixed strategies destroy previous results.
is, xi dominant (dominated) strategy G(U ), continues dominant
(dominated) strategy Gm (U ).
Gm (U ) regular game apply Theorem 2 Corollary 2 deduce:
Theorem 3 Let G(U ) nite game strategic form least two strategies every
player. Let p prole mixed strategies G(U ). p mixed strategy equilibrium
G(U ) p 0-implementation Gm (U ).
Hence, technically, case mixed strategies follows theorems regarding pure
strategies innite games. However, reader notice case output
observability assumption strong implication. Implementing mixed strategy prole
Gm (U ) actually means algorithm observability G(U ). is, interested party
observe mixed strategies used players. realistic assumption think
interested party systems administrator deploys algorithms submitted
users. designer allowed alter users algorithms, verify exact
content algorithms. Hence, example, setting, users algorithm
ips coin order decide course action, exact randomized algorithms,
including particular coin ipping, viewed interested party. interesting
case interested party cannot observe mixed strategies discussed
Section 6.

4. k-implementation sets
previous sections dealt properties k- implementation. particular
emphasized interesting cases k-implementations singletons. However,
computational perspective, given game G(U ), set desired strategy prole O,
may interest nd smallest integer k 0 k- implementation exists.
show:
Theorem 4 Given game G(U ), set desired strategy proles O, integer k 0,
deciding whether exists k implementation G(U ) NP-hard.
Proof : order prove theorem, use reduction SAT problem.
Given set primitive propositions {x1 , x2 , . . . , xn }, consider CNF formula. CNF
formula conjunction clauses C1 C2 . . . Cm , Ci = l1i li2 . . . lisi (si 2)
ljq = xi ljq = xi (for every 1 j 1 q sj ). SAT
problem following decision problem: given CNF formula, truth assignment
primitive propositions satises it? problem known NP-complete.
44

fiK-Implementation

show polynomial reduction SAT problem deciding whether
2- implementation exists, games 2-person games. suce prove
result. Without loss generality restrict attention CNF formulas
xi xi appear formula, clause refers xi xi (1 n).
agents strategy ci associated clause Ci , every 1 m.
addition, agents strategies yi , zi , associated literals xi xi ,
respectively (1 n).
payo agent 1, p1 , dened follows. strategy prole form
(ci , yj ) payo 3 xj appears clause 0 otherwise. strategy prole
form (ci , zj ) payo 3 xj appears clause 0 otherwise.
strategy prole form (ci , cj ) payo 50 = j 0 otherwise.
strategy prole form (yi , yj ) payo 2 = j 3 otherwise.
strategy prole form (zi , zj ) payo 2 = j 3 otherwise.
strategy prole form (yi , zj ) form (zi , yj ) payo 1 = j 3
otherwise. strategy prole form (yi , cj ) payo 51 xi xi
appear clause Cj 0 otherwise. strategy prole form (zi , cj ) payo
51 xi xi appears clause Cj 0 otherwise.
payo agent 2, p2 , follows. payo strategy prole
form (yi , yi ) (zi , zi ) 101; payo strategy prole form (yi , zi )
(zi , yi ) 100; payo strategy prole form (yi , zj ) (zi , yj )
= j 0. payo strategy prole form (ci , cj ) 50 = j,
0 otherwise. strategy prole form (yi , cj ), (zi , cj ) payo 50
= j 0 otherwise. payo strategy prole form (ci , yj ), (ci , zj )
0.
set desired strategy proles include strategy proles excluding
following: strategy proles form (ci , s),(yi , zi ),(zi , yi ) (where strategy)
prohibited.
formula satisable 2- implementation: add 1 payo obtained
agent 1 strategy prole form (yi , s) xi true satisfying assignment,
add 1 payo obtained agent 1 strategy prole form (zi , s) xi
false. agent 2, increase payo 1 (zi , yi ) xi true, increase payo
1 (yi , zi ) xi false.
Notice given construction strategies form ci agent 1
become dominated removed. addition, xi assigned true (resp.
false) strategy zi (resp. yi ) become dominated. corresponding strategies
agent 2 (i.e. zi xi true yi xi false) become dominated too, yield
desired behavior.
Similarly, notice since must remain least one yi zi agent 1,
must case least one yi zi removed agent 2, cannot
obtained payment 1 (increasing payo 100 101). Hence, 2implementation payment agent 2 1. However, notice must add 1
payo agent 1 least one elements form (yi , yi ) (zi , zi )
45

fiMonderer & Tennenholtz

correspond xi xi appear clause j (i.e. least one literals
clause j, need add 1 strategy prole form (yi , yi ) (zi , zi ) associated
literal). Notice based fact strategy ci agent cannot
removed adding payment 1 outcome strategy form yj , zj , cj
agent 1, xj xj appear Ci , since payo agent 1 (yj , ci )
(zj , ci ) 0, payo agent 1 (ci , ci ) 50. Moreover, add 1
payo agent 1 obtains (yi , yi ) (zi , zi ) yi zi
dominated agent 1, result possibility playing strategy prole
desired(given impossible remove yi zi agent 2). Hence,
implementation corresponds sound truth assignment CNF formula, xi
assigned true payo agent 1 (yi , yi ) augmented 1.

Notice previous result applies already case constant
number agents. previous result suggests one may wish consider relaxations
optimal implementation problem tractable.8 One interesting relaxation9
following one:
non-negative vector payo functions V called k- exact implementation
G(U ), following two conditions satised:
X(U + V ) = O.
n

i=1 Vi (x) k every x O.
Hence, V implements means set non-dominated strategies G(U + V )
subset O, V exact implementation set equals O. dealing
singletons concepts implementation exact implementation coincide.
Notice concept exact implementation makes sense = O1
O2 X = X1 X2 Xn since otherwise impossible (exactly)
implement O. also assume Oi strictly contained Xi every agent i,
Oi contain two strategies one dominates other. show:
Theorem 5 Computing optimal k exact implementation exists polynomial.
algorithm leading result illustrated case two agents.
construct game matrix G , payo function agent denoted pi ;
pi describes payment agent dierent strategy proles (if/when selected).
matrix G matrix perturbations (non-negative monetary promises),
G denote perturbed matrix generated. Let = K + 1 K maximal
element original game matrix.
optimal perturbation [OP] algorithm:
8. Another approach may search good approximation techniques.
9. See discussion last section.

46

fiK-Implementation

1. Let (e1 , . . . , ek ) list possible dierences agents payos original
game (i.e. possible results one obtains subtracting two possible payos
agent given game) , sorted small large.
2. Let p1 (a, b) := every O1 b X2 \ O2 , let p1 (a, b) = 0 whenever
X1 \ O1 b O2 .
3. Let p2 (a, b) := every b O2 X1 \ O1 , let p2 (a, b) := 0 whenever
b X2 \ O2 O1 ,
4. Let i:=1
5. Let e := ei
6. Let p1 (a, b) := e every strategy prole form (a, b) O1 b O2
7. Let G := G + G
8. non-dominated strategies agent 1 G coincide O1 let
i:=i+1 return 5
9. Let i:=1
10. Let e := ei
11. Let p2 (a, b) := e every O1 b O2
12. Let G = G + G
13. non-dominated strategies agent 2 G coincide O2 let
i:=i+1 return 10

5. Incomplete information
previous sections dealt games complete information. However, many
real life situations players ( interested party) incomplete information
certain parameters game. economic literature phenomenon
mainly modelled Bayesian setting. setting every player receives
private signal, correlated unknown parameters, joint distribution
signals commonly known players (and interested party). following
subsection deal Bayesian games without probabilistic information. games
called games informational form.
5.1 Games informational form
precise denition games informational form given paper,
focus particular type games combinatorial auctions. However,
typical example shown Figure 1.
47

fiMonderer & Tennenholtz

Figure 1: game informational form

L1
s1

U1
D1

t1

U2
D2

s2

a1111

R1

L2

a1112
b1111

a1121

a1211

a1212
b1212

b1122

b1121

a1222
b1222

b2112

a2211
b2211

a2212
b2212

b2122

a2221
b2221

a2222
b2222

a1122

a2111

a1221

a2112
b2111

a2121

a2122
b2121

R2

b1211

b1112

b1121

t2

game, Player 1 receive one signals s1 t1 , Player 2
receive one signals10 , s2 t2 . true game played determined
pair (c1 , d2 ), c, {s, t}. However, neither player knows exact game. Given
s1 (s2 ) player 1 (player 2) choose action {U1 , D1 } ({L1 , R1 }), given t1 (t2 )
player 1 (player 2) choose action {U2 , D2 } ({L2 , R2 }). payos shown
gure. Bayesian game obtained game informational form adding
probability distribution pairs signals described Figure 2. probability
1 receives signal c1 , 2 receives d2 equals pij , = 1 c = s, = 2 c = t,
j = 1 = s, j = 2 = t.

s2
a1111

s1

U1
D1

L1

Figure 2: Bayesian Game

t2

R1

b1111
a1121

a1211

b1112

a1212
b1211

a1122
b1121

R2

L2

a1112

a1221

b1122

b1212
a1222

b1121

b1222

p11

t1

U2
D2

a2111

a2112
b2111

a2121

p12

a2211
b2112

a2122
b2121

a2212
b2211

a2221
b2122

a2222
b2221

p21

10. signals times called types.

48

b2212
b2222
p22

fiK-Implementation

strategy player function dened set signals, assigns every
signal action11 games consistent signal. example, game
Figure 1, strategy player 1 function b1 : {s1 , t1 } {U1 , D1 , U2 , D2 },
property b1 (s1 ) {U1 , D1 } b1 (t1 ) {U2 , D2 }. strategy player 2 analogously
dened function b2 : {s2 , t2 } {L1 , R1 , L2 , R2 }. concepts domination
equilibrium ( traditionally referred ex post equilibrium) naturally dened.
example, Figure 3 strategy player 1 chooses U1 receives
signal s1 , chooses D2 given t1 dominates four strategies player
1.

Figure 3: Domination
R1
5, 2

t2
L2 R2
5, 0 1, 1

D1 0, 5

4, 4

4, 4

0, 5

U2 0, 5

4, 4

4, 4

0, 5

D2 1, 1

5, 0

5, 0

1, 1

L1
U1 1, 1
s1

t1

s2

is, given s1 , independently players signal action, choosing U1
least good choosing D1 , least one signal action Player 2, choosing
U1 strictly better choosing D1 .
Figure 4 demonstrate ex post equilibrium.

11. environment complex strategies exist, refer choices player game
strategic form actions.

49

fiMonderer & Tennenholtz

Figure 4: Ex Post Equilibrium

L
2, 8

R
5, 1

L
0, 5

R
3, 6

1, 5

6, 4

7, 2

1, 4

U 0, 2

5, 2

5, 0

2, 4

1, 1

6, 0

4, 2

3, 3

U



Note even output observability assumption player
reveal strategy. signal player private knowledge, reveals
action chooses. interested party observe signals; therefore,
action sets games ( case Figure 4) interested party
receive information true game played. Hence, thing
use vector V games. Therefore:
Claim every k 0 impossible k-implement ex post equilibrium described
Figure 4.
Proof interested party wishes make U dominant strategy game (s1 , t2 ),
wishes make dominant strategy (t1 , t2 ). Assume V1 (U, L) = x V1 (D, L) =
y, following two contradictory inequalities satised: 0 + x 7 + y,
5 + x 4 + y.
stated next subsection, game informational form particular
structure, results complete information case generalized.
5.2 VCG combinatorial auctions
Combinatorial auctions constitute special class games informational form.
notations denitions taken Holzman Monderer (2002).
combinatorial auction seller, denoted 0, wishes sell set
goods = {a1 , . . . , }, 1, owns. denote 2A family bundles
goods (i.e., subsets A). set n buyers N = {1, . . . , n}, n 1. allocation
goods ordered partition = (0 , 1 , . . . , n ) A.12 denote set
allocations.
12. Note goods allocated among buyers seller. assume, however, seller
derives utility keeping goods, set strategic reserve prices.

50

fiK-Implementation

buyers valuation function function v : 2A , satisfying v() = 0
B C, B, C 2A v(B) v(C).
buyer valuation function vi receives set goods B, pays monetary
transfer ci utility vi (B) ci . Every buyer knows valuation function.
denote V set possible valuation functions. set V N , n-fold
product set V , set proles valuations v = (v1 , . . . , vn ), one
buyer.
allocation = (0 , 1 , . . . , n ) prole valuations v = (v1 , . . . , vn )
VN denote S(v, ) total social surplus buyers, is,

S(v, ) =
vi (i ).


also denote
Smax (v) = max S(v, ),


refer allocation achieves maximum optimal allocation v.
Vickrey-Clarke (VC) auction mechanism described follows. Every buyer = (
quired report valuation function vi . Based reported valuations v
v1 , . . . , vn )
N
v) = (d0 (
v), . . . , dn (
v)) , optimal
V mechanism selects allocation d(
. ties possible, allocation may unique, therefore
v
v, d(
v)) = Smax (
v)
one VC mechanism. Every function : V N satisfying S(
N
V determines uniquely VC mechanism, refer VC mechafor v
nism d. mechanism assigns buyer bundle di (
v) makes pay cdi (
v)
seller,


cdi (
v) = max
vj (j )
vj (dj (
v)).


j=i

j=i

represents loss agents total surplus caused agent presence.
Vickrey-Clarke-Groves (VCG) auction mechanism parameterized VC mechanism d, n-tuple h = (h1 , . . . , hn ) functions hi : V N \{i} . mechanism
selects allocation according allocation function d, transfer function
buyer
cd,h
v) = cdi (
v) + hi (
vi ).
(
Hence, VC auction mechanism special type VCG auction mechanism, hi
function identically equal zero every i.
Let = (d, h) VCG mechanism. utility valuation vi depends
), denoted ui (vi , vi , v
).
= (vi , v
vector reported valuations v
is,
) = vi (di (
ui (vi , vi , v
v)) cd,h
v).
(
behavior buyer mechanism described strategy bi : V V .
51

fiMonderer & Tennenholtz

strategy bi dominant strategy following two conditions hold:13
V N \{i}
every vi V , every v
) ui (vi , vi , v
)
ui (vi , bi (vi ), v

every vi V .

V N \{i}
every vi V , exists v
) > ui (vi , vi , v
).
ui (vi , bi (vi ), v
strategy prole (b1 , . . . , bn ) forms ex post equilibrium every prole valuations v = (v1 , . . . , vn ) VN , every buyer i,
ui (vi , bi (vi ), bi (vi )) ui (vi , vi , bi (vi ))

every vi V ,

bi (vi ) = (bj (vj ))j=i . prole (b1 , . . . , bn ) symmetric bi = bj every two
buyers i, j N .
well-known every VCG auction mechanism truth-telling sense
every buyer i, strategy bi (vi ) = vi revealing true valuation dominant
strategy.14
Special type strategies considered Holzman et al. (2003), Holzman Monderer (2002). bundling strategy buyer parameterized subfamily 2A
, denoted f . maps every v V v V dened
v (B) =

max

CB,Ci

v(C)

every B 2A .

eect pretending agent cares bundles (for
announces true valuation), derives valuation bundles maximizing
bundles contain.
valuation v satises equalities said based ( or, simply
-based). set -based valuation function denoted V .
subfamily 2A quasi eld satises following two
conditions:
B \ B ,
B, C B C = B C .
work Holzman Monderer (2002) proven every ex post equilibrium VCG mechanisms bundling equilibrium following sense: every
13. classical mechanism design second condition required. use sake
consistency rest paper.
14. one reasons fact concept ex post equilibrium VCG auction
mechanisms private values largely ignored economics literature. However, truth
telling strategy induces high communication complexity; requires player communicate 2m
numbers. Hence, computer science perspective, ex post equilibrium less communication
complexity desirable. tradeo communication complexity economic eciency
discussed Holzman, Kr-Dahav, Monderer, Tennenholtz (2003).

52

fiK-Implementation

n 3, every prole (b1 , b2 , , bn ) strategies, satises subprole (bi )iN ,
N N , ex post equilibrium VCG mechanisms symmetric prole bundling
strategies. is, exists 2A bi = f every N , moreover,
proved Holzman et al. (2003) must quasi eld.
5.3 0-Implementation ex post equilibrium frugal VCG auction
mechanisms
begin formal denition frugal VCG combinatorial auction:
Denition Frugal VCG mechanisms VCG mechanism (d, h) called frugal
= (
allocate unnecessary goods buyers. is, every v
v1 , . . . , vn ) V N

mechanism selects allocation d(
v) = (d0 (
v), . . . , dn (
v)) , optimal v
satises addition:
every player i, every Bi ,
vi (Bi ) < vi (i ),
= di (
v).
Intuitively, frugal VCG mechanism center never allocates unnecessary goods.
agent bid bundle B1 bid superset it, B2 , center
never allocate B2 agent (the bid B2 strictly higher bid
B1 order allocating B2 agent possibility.)
Consider interested party wishes 0-implement ex post equilibrium b =
(b1 , b2 , , bn ) VCG auction mechanism (d, h). Given result Holzman
Monderer (2002) stated end previous subsection assume almost without
loss generality bi = f every N . interested party wishes promise
positive payment every buyer whenever follows recommendation play according
bi , least one players, say j, play according bj . However,
interested party know valuation functions. Hence, could know whether
player follows recommendation? Indeed cannot. However, since known
interested party partially monitor players strategies, because, independently
players valuation function reported valuation function must -based. Hence,
best interested party oer every player positive payment reported
valuation -based, least one players reported valuation function,
-based. payments made arbitrarily high reporting
-based valuation function yield higher payo other, non -based
valuation function least one players report -based valuation
function. However, player cheat within set -based valuations without
caught! turns VCG mechanism frugal, every player better
cheating.
Lemma 1 Let (d, h) frugal VCG mechanism, let quasi eld, let N .

every prole reported valuations players, v
) ui (vi , wi , v
)
ui (vi , vi , v
53

every wi V .

fiMonderer & Tennenholtz

Proof : Without loss generality assume hj constantly 0 every j.
Hence, VCG mechanism actually VC mechanism. vj -based every j, j = i,
inequality follows fact f induces ex post equilibrium. However,
) allocation chosen auctioneer
proof use fact. Let = d(vi , v


reports vi , let = d(wi , vi ). Denote

vj (j ).
= max


j=i

VCG mechanism frugal, . Therefore
) =
vi (i ) = vi (i ), vi (i ) = vi (i ). denition ui (vi , vi , v



, ) t. since vi (i ) = vi (i ) ui (vi , vi , v
) = S(vi , v
, ) t.
S(vi , v
, ) S(vi , v
, ) t. However, since
optimality get S(vi , v
frugality also implies vi (i ) = vi (i ), get following equation desired
inequality:
, ) = S(vi , v
, ) = ui (vi , wi , v
).
S(vi , v
.
Hence, proof Lemma 1 used fact frugal VCG mechanism must
allocate subset goods every player reports -based valuation function.
next example shows Lemma 1 hold arbitrary VCG mechanism.
Example
two buyers four goods a,b,c,d.
= {, ab, cd, abcd}.
valuation function 1 v1 , reported valuation 2 v2 . Consider VC
auction mechanism allocate ab 1 cd 2 1 reports v1 , allocates abc
1, 2, 1 reports -based valuation function w1 . cases 1 pays
0. Hence, reporting v1 yields utility 1, cheating yields 1.1. Note VC
mechanism frugal 1 declares w1 receives abc, w1 (ab) = w1 (abc).

v1
v2
v1
w1


0
0
0
0


0
0
0
0

b
0
0
0
0

c

ab ac ad bc bd
cd abc abd acd bcd abcd
0
0
1 0
0
0
0
0
1.1
1
0
0
1.1
0 0.75 0 0 0.75 0 0.75 0.75 0 0.75 0.75 0.75 0.75
0
0
1 0
0
0
0
0
1
1
0
0
1.1
0
0
1 0
0
0
0
0.1
1
1
0.1 0.1 1.1

need following terminology. Let (d, h) VCG combinatorial auction, let
> 0. denote (d, h, ) direct combinatorial auction rules induced
(d, h) which, set feasible valuation functions ( set bids) V (M ),
set valuation functions v satisfying v(A) < . assumption upper
bound natural common literature mechanism design. veried
Lemma 1 holds VCG combinatorial auctions bounded valuation functions.
54

fiK-Implementation

Theorem 6 Let (d, h, ) frugal VCG auction mechanism least two buyers.
Let quasi elds bundles. symmetric ex post equilibrium induced
0-implementable.
Proof : every player i, interested party promises high payo ( e.g., 2M + 1
reports valuation function V , least one players
report -based valuation function. Player promised payment players report
valuation functions V . Lemma 1, f dominant strategy every player.

6. Implementation devices
mentioned Section 3, proof result (Theorem 3) every mixed strategy
equilibrium 0-implementable relies assumption interested party observes
mixed strategies used players. section prove result without
assumption. is, interested party observe actions generated
mixed strategies, algorithms generate them. order deal
issue dene new type implementation implementation device.
Let G(U ) = (N, X, U ) nite game strategic form. implementation device
G(U ) tuple = (S, h, V ), = S1 S2 Sn , h (S) probability
distribution S, V : X n+ . Si nite15 set signals sent
i. interested party uses implementation device follows: makes device
public, secretly runs randomizing scheme chooses every probability
h(s). = (s1 , s2 , , sn ) chosen, sends player signal si . strategy prole
x selected agent paid Vi (s, x). implementation device generates new game
G(U, I). actually Bayesian game. strategy game function
bi : Si Xi . every si xi , vector bi players let Wi (xi |si , bi )
expected payo game G(U, I) chooses xi given receives signal
si players use bi . is,
Wi (xi |si , bi ) = Esi (Ui (xi , bi (si )) + Vi (si , si , xi , bi (si )|si ) ,
si = (sj )j=i , bi (si ) = (bj (sj ))j=i . strategy
bi dominant strategy
every signal si positive probability ( tS,ti =si h(t) > 0), every
bi
Wi (bi (si )|si , bi ) Wi (xi |si , bi ) every xi Xi ,
exists prole bi players strict inequality holds.
Every prole b = (bi )iN determines probability distribution probb X dened
follows:
probb (x) = h(b1 = x1 , b2 = x2 , , bn = xn ).
15. innite set, must specify additional parameters required probability theory. associate
Si -algebra events , endow product -algebra, , dene h
.

55

fiMonderer & Tennenholtz

Let desired probability distribution X. say implements G(U ),
G(U, I) every player dominant strategy bi , probb = . say
k-implementation G(U ) implements , every h(s) > 0,
n


Vi (si , bi (si )) k.

i=1

6.1 Mixed Strategies: Removing algorithm observability assumption
Let p = (p1 , p2 , , pn ) mixed strategy prole nite game G(U ). p generates
probability distribution p follows:
p (x) = p1 (x1 )p2 (x2 ) pn (xn )

every x X.

say implementation device implements p implements p .
order implement mixed strategy equilibrium , interested party employs
implementation device I, set signals Si set actions i, Xi . h
product probability p. is, h(x) = p1 (x1 )p2 (x2 ) pn (xn ), function
Vi : Si Xi + designated way strategy bi (si ) = si , si Xi ,
dominant strategy every player i.
Hence, interested party ips coin player according probability
pi prole wishes implement, sends outcome coin ipping
i. Thus, signals sent players recommendations play. payo
functions Vi , N designed way obeying recommendation
dominant strategy every player.
Theorem 7 Let G(U ) nite game least two actions every player. Every
mixed strategy equilibrium prole p 0-implementable G(U ) appropriate implementation device = (S, h, V ) = X, h = p product probability X
dened p.
Proof : Denote bi strategy obeys every recommendation.
bi (si ) = si every si Xi . function Vi satisfy every vector di
players strategies,
Wi (si |si , di ) Wi (xi |si , di ) xi Xi .
assume without loss generality every player i, Vi (si , si , xi , yi ) = 0
every xi = si . Therefore inequality written follows:




Esi Vi (si , si , si , di (si )) Esi Ui (xi , di (si )) Ui (si , di (si )) .
di = bi , dj (sj ) = sj every j = every sj Xj , right-hand-side
inequalities non-positive p mixed strategy equilibrium. Hence,
may dene Vi (s, s) = 0 every X. make sure inequalities hold
cases (i.e., di ), dene Vi (s, si , xi ) = 2M + 1 every xi = si ,
> 0 upper bound absolute value players payo functions.
choice 2M + 1 (rather 2M ) ensures existence strict inequality required
denition domination.
56

fiK-Implementation

6.2 0-implementations correlated equilibrium
Aumann (1974) introduced concept correlated equilibrium. provide one
many equivalent denitions:
Denition Let G(U ) = (N, X, U ) nite game strategic form. correlated equilibrium G(U ) probability distribution X ( (X)) strategies
bi (si ) = si , si Xi , N , form equilibrium game G(U, I), = (S, h, V )
following implementation device:
= X,
h = ,
Vi (s, x) = Ui (x) every N every s, x X.
Hence, forms correlated equilibrium mediator makes changes
players payo run randomization device according , picks prole pure actions
s, sends every player recommendation play si , every player better
obeying recommendation believes players obey recommendations.
well-known ( implicitly used proof Theorem 7) p mixed
strategy equilibrium, p correlated equilibrium. Moreover, going proof
Theorem 7 reveals property mixed-strategy equilibrium p use
fact p correlated equilibrium. Hence get:
Theorem 8 Let G(U ) = (N, X, U ) nite game least two actions every
player. Every correlated equilibrium prole 0-implementable G(U ) appropriate implementation device = (S, h, V ) which, = X h = .
Note eventually, interested party implements mixed strategy equilibrium
correlated equilibrium implementation device I, players using pure
strategies game G(U, I). expected value operator linear,
easily seen obeying recommendation remains dominant strategy every player
even player believes players use mixed strategies G(U, I),
mixed strategy G(U, I) probability distribution Qi set pure
strategies. mixed strategy natural description behavior G(U, I).
natural, less computational demanding concept one behavior strategy:
behavior strategy game G(U, I) function ci : Si (Xi ). Hence, player
using behavioral strategy chooses mixed strategy game strategic form
function signal, player using mixed strategy G(U, I) picking
pure strategy G(U, I) randomization device receives signal. sets
mixed behavioral strategies technically related other. However,
Kuhn (1953) ( see also Hart (1992) details), every player i, every strategy bi
every prole Qi = (Qj )j=i mixed strategies players, exists
prole ci = (cj )j=i behavioral strategies players every signal
57

fiMonderer & Tennenholtz

si expected utility using bi given players using Qi equals
expected utility players using ci , vice versa.16
Hence, theorems 7 8 remain valid environment allows utilization
either mixed behavioral strategies G(U, I).

7. Conclusions, discussion research
One may distinguish two main lines research multi-agent systems. One line
research design mechanisms protocols. context
fact deal design games, agents assumed behave
particular way (e.g., law-abiding, playing equilibrium, reinforcement learners, etc.)
desired behavior (e.g., revenue maximization, maximization social surplus,
etc.) obtained. line research deals study behavior
agents given game. economic theory leading paradigm last decades
agents use equilibrium strategies. However, paradigm implicitly assumes
agents rational moreover, agents believe agents rational.17
paper introduced intermediate approach. game/interaction given,
agents provided newly designed protocol. inuence agents behavior
credible promises positive monetary transfers conditioned actions
selected agents game. also assume:
1. Minimal rationality: would like assume little possible agent rationality.
Indeed assume agent use strategy dominated
another strategy.
2. Minimal expenses: assume interested party wishes minimize expenses
leading agents desired behavior.
notion k-implementation captures basic ideas. paper
provided several basic results k-implementation. provided full characterization k-implementation case implementation singletons games
complete information. particular shown formula computing optimal k,
yield desired agent behavior, fully specied procedure implementation. result also applicable general class games innitely many strategies.
shown characterization likely impossible implementation
sets strategies. particular, shown problem whether desired
behavior implemented cost k NP-hard. led us considering exact
k-implementation shown tractable. Exact k-implementation requires
every desirable strategy prole rational result promises monetary
16. Actually, theorem presented Kuhn (1953) stronger one quote here.
17. computer science issue agent choose action , unless dominant strategy,
central one general satisfactory solution known. Researchers appeal case
concept competitive analysis (Borodin & El-Yaniv, 1998). context games, promising
results direction presented Tennenholtz (2002).

58

fiK-Implementation

transfers (in addition requirement undesirable strategy prole remain
rational). sense, exact implementation requires number strategies
system remove minimal. consistent basic ideas normative systems order obtain desired behavior would like minimal laws
leave maximal freedom agents long enable obtain desired
(social) behavior. discussion issue see (Fitoussi & Tennenholtz, 2000).
extension results context mixed strategies interpreted
strong evidence importance Nash equilibrium normative perspective,
descriptive approach attempts explain/predict agent behavior
economic contexts. fact mixed strategy equilibrium made dominant
one zero payments given credible interested party, cannot force
behaviors punish agents, tells us many practical situations Nash equilibrium
special merit also normative perspective.
games incomplete information discussed section games informational form, rather Bayesian games. spirit work computer science
tries minimize probabilistic assumptions economic environment,
particular use part solution concept. context VCG mechanisms probably central widely studied mechanisms turn
attention study k-implementation context mechanisms. Indeed,
show, unlike case nite games complete information always
large k (if paid) lead desired behavior, longer true games
incomplete information. VCG mechanisms turn complex
equilibrium analysis. recent work shown (Holzman et al., 2003) exponentially many equilibria VCG mechanisms dier truth telling.
special interest since equilibria exhibit lower communication complexity
standard truth revealing equilibrium. Notice equilibria obtained without
restriction possible bids agents (this called imposition property). interested party access VCG protocol, inuence
indirectly. proved 0-implementation ex-post equilibrium frugal VCG
mechanism, frugality requirement necessary.
many things left done. particular, interesting
develop study k-implementation better understanding case k > 0.
example, may interesting study eects cost implementation economic
eciency. study tractable cases also interest. interested also
extending study k-implementation games informational form beyond VCG
mechanisms. issue collusion may interesting ramications context
k-implementation. Collusive agreements may make benet promises made
interested party. Similarly, failures system possible, interested party
might nd paying oers hoping ignore given rational behavior
agents. also assumed one interested party. case
several interested parties may wish lead dierent desired behaviors
new strategic situation emerges. believe issues signicant importance.
hope address future work, others join us study
59

fiMonderer & Tennenholtz

k-implementation exploring spectrum system agent perspectives
multi-agent systems.

Acknowledgements
thank Assaf Cohen helpful comments. rst author thanks Israeli Science
Foundation partial support research BIKURA grant, second
author thanks Israeli Science Foundations partial support research ISF
individual grants. preliminary version paper appears proceedings 4th
ACM Conference Electronic Commerce (EC03).

References
Aumann, R. (1974). Subjectivity correlation randomized strategies. Journal
Mathematical Economics, 1, 6796.
Borodin, A., & El-Yaniv, R. (1998). On-Line Computation Competitive Analysis. Cambridge University Press.
Clarke, E. (1971). Multipart pricing public goods. Public Choice, 18, 1933.
Conitzer, V., & Sandholm, T. (2002). Complexity mechanism design. Proceedings
18th conference uncertainity Articial Intelligence (UAI-02), pp. 103110.
Dybvig, P., & Spatt, C. (1983). Adoption externalities public goods. Journal Public
Economics, 20, 231247.
Feigenbuam, J., & S, S. (2002). Distributed Algorithmic Mechanism Design: Recent Results
Futute Directions. Proceedings 6th International Workshop Discrete
Algorithms Methods Mobile Computing Communications, pp. 113.
Fitoussi, D., & Tennenholtz, M. (2000). Choosing Social Laws Multi-Agent Systems:
Minimality Simplicity. Articial Intelligence, 119 (12), 61101.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Groves, T. (1973). Incentives teams. Econometrica, 41, 617631.
Hart, S. (1992). games extensive strategic forms. Aumann, R., & Hart, S. (Eds.),
Handbook Game Theory, chap. 2, pp. 1940. North Holland, Amsterdam.
Holzman, R., Kr-Dahav, N., Monderer, D., & Tennenholtz, M. (2003). Bundling Equilibrium Combinatorial Auctions. Games Economic Behavior, appear. Working
paper Technion http://ie.technion.ac.il/dov.phtml.
Holzman, R., & Monderer, D. (2002). Characterization ex post equilibrium VCG
combinatorial auctions. Game Economic Bahavior, appear. Working Paper,
Technion, http://ie.technion.ac.il/dov.phtml.
60

fiK-Implementation

Koutsoupias, E., & Papadimitriou, C. (1999). Worst-Case Equilibria. STACS.
Kuhn, H. (1953). Extensive games problem information. Annals Mathematics
Studies, 48.
Mas-Colell, A., Whinston, M., & Green, J. (1995). Microeconomic Theory. Oxford University Press.
Maskin, E. (1999). Nash equilibrium welfare optimality. Review Economic Studies,
66, 2338.
Maskin, E., & Sjostrom, T. (2002). Implementation theory. Arrow, K. J., Sen, A. K.,
& Suzumura, K. (Eds.), Handbook Social Choice Theory Welfare Volume 1.
North-Holland, Amsterdam.
Monderer, D., & Shapley, L. (1996). Potential games. Games Economic Behavior, 14,
124143.
Naor, M., Pinkas, B., & Sumner, R. (1999). Privacy Preserving Auctions Mechanism
Design. Proceedings EC99, pp. 129139.
Nash, J. (1950). Equilibrium points n-person games. Proceedings National Academy
Sciences United States America, 36, 4849.
Nisan, N. (2000). Bidding allocation combinatorial auctions. ACM Conference
Electronic Commerce, pp. 112.
Nisan, N., & Ronen, A. (1999). Algorithmic mechanism design. Proceedings STOC-99.
Papadimitriou, C. H. (2001). Algorithms,Games,and Internet. STOC 2001.
Parkes, D. C. (1999). ibundle: ecient ascending price bundle auction. ACM
Conference Electronic Commerce, pp. 148157.
Rosenschein, J. S., & Zlotkin, G. (1994). Rules Encounter. MIT Press.
Rosenthal, R. (1973). class games possessing pure-strategy nash equilibria. International Journal Game Theory, 2, 6567.
Roughgarden, T. (2001). price anarchy independent network topology.
Proceedings 34th Annual ACM Symposium Theory Computing, pp.
428437.
Roughgarden, T., & Tardos, E. (2002). bad selsh routing?. Journal ACM,
49 (2), 236259.
Sandholm, T., Suri, S., Gilpin, A., & Levine, D. (2001). Cabob: fast optimal algorithm combinatorial auctions. 17th International Joint Conference Articial
Intelligence, pp. 11021108.
Segal, I. (1999). Contracting externalities. Quarterly Journal Economics,
CXIV (2), 337388.
61

fiMonderer & Tennenholtz

Shoham, Y., & Tennenholtz, M. (1995). Social Laws Articial Agent Societies: O-line
Design. Articial Intelligence, 73.
Shoham, Y., & Tennenholtz, M. (2001). rational computability communication
complexity. Games Economic Behavior, 35, 197211.
Spiegler, R. (2000). Extracting intercation-created surplus. Games Economic Behavior,
30, 142162.
Tennenholtz, M. (1999). Electronic commerce: game-theoretic economic models
working protocols. IJCAI-99.
Tennenholtz, M. (2002). Competive Safety Analyis: robust decision-making multi-agent
systems. Journal Articial Intelligence Research, 17, 363378.
Vickrey, W. (1961). Counterspeculations, auctions, competitive sealed tenders. Journal
Finance, 16, 1527.

62

fiJournal Artificial Intelligence Research 21 (2004) 135191

Submitted 03/03; published 02/04

CP-nets: Tool Representing Reasoning
Conditional Ceteris Paribus Preference Statements
Craig Boutilier

cebly@cs.toronto.edu

Department Computer Science
University Toronto
Toronto, ON, M5S 3H8, Canada

Ronen I. Brafman

brafman@cs.bgu.ac.il

Department Computer Science
Ben-Gurion University
Beer Sheva, Israel 84105

Carmel Domshlak

dcarmel@cs.cornell.edu

Department Computer Science
Cornell University
Ithaca, NY 14853, USA

Holger H. Hoos

hoos@cs.ubc.ca

Department Computer Science
University British Columbia
Vancouver, BC, V6T 1Z4, Canada

David Poole

poole@cs.ubc.ca

Department Computer Science
University British Columbia
Vancouver, BC, V6T 1Z4, Canada

Abstract
Information user preferences plays key role automated decision making.
many domains desirable assess preferences qualitative rather quantitative way. paper, propose qualitative graphical representation preferences
reflects conditional dependence independence preference statements
ceteris paribus (all else equal) interpretation. representation often compact
arguably quite natural many circumstances. provide formal semantics
model, describe structure network exploited several inference
tasks, determining whether one outcome dominates (is preferred to) another, ordering set outcomes according preference relation, constructing best outcome
subject available evidence.

1. Introduction
Extracting preference information users generally arduous process, human
decision analysts developed sophisticated techniques help elicit information
(Howard & Matheson, 1984). key goal study computer-based decision support
construction tools allow preference elicitation process automated, either
partially fully. Methods extracting, representing reasoning preferences
naive users particularly important AI applications, ranging collaborative
c
2004
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBoutilier, Brafman, Domshlak, Hoos & Poole

filtering (Lashkari, Metral, & Maes, 1994) recommender systems (Nguyen & Haddawy,
1998) product configuration (DAmbrosio & Birmingham, 1995) medical decision
making (Chajewska, Getoor, Norman, & Shahar, 1998). many applications
users cannot expected patience (or sometimes ability) provide detailed
preference relations utility functions. Typical users may able provide much
qualitative rankings fairly circumscribed outcomes.
paper describe novel graphical representation, CP-nets, used
specifying preference relations relatively compact, intuitive, structured manner
using conditional ceteris paribus (all else equal) preference statements. CP-nets
used specify different types preference relations, preference ordering
potential decision outcomes likelihood ordering possible states world,
example, Shohams (1987) preference semantics. However, mainly first
typepreferences outcomes decisionsthat motivates development CPnets. inference techniques CP-nets described paper focus two important,
related questions: perform preferential comparison outcomes,
find optimal outcome given partial assignment problem attributes.
Ideally, preference representation capture statements natural users
assess, reasonably compact, support effective inference. conditional ceteris
paribus semantics requires user specify, specific attribute interest,
attributes impact preferences values A. instantiation
relevant attributesthe parents Athe user must specify preference ordering
values conditional parents assuming instantiated values; instance,
a1 may preferred a2 b1 c2 hold. preference given ceteris paribus
interpretation: a1 preferred a2 given b1 c2 else equal. words,
fixed instantiation remaining attributes, outcome a1 holds preferred
one a2 holds (assuming b1 c2 ). statements arguably quite natural
appear several places (e.g., e-commerce applications). instance, product
selection service offered Active Buyers Guide1 asks (unconditional) ceteris paribus
statements assessing users preference various products. tools also ask
certain semi-quantitative information preferences. Conditional expressions offer
even greater flexibility.
Preference elicitation complex task key focus work decision analysis
(Keeney & Raiffa, 1976; Howard & Matheson, 1984; French, 1986), especially elicitation
involving non-expert users. Automating process preference extraction
difficult. considerable work exploiting structure preferences
utility functions way allows appropriately decomposed (Keeney &
Raiffa, 1976; Bacchus & Grove, 1995, 1996; La Mura & Shoham, 1999). instance,
certain attributes preferentially independent others (Keeney & Raiffa, 1976), one
assign degrees preference attribute values without worrying attribute
values. Furthermore, one assumes stringent conditions, often one construct
additive value function attribute contributes overall preference
specificdegree (the weight attribute) (Keeney & Raiffa, 1976). instance,
common engineering design problems make assumptions simply
1. See www.activebuyersguide.com.

136

fiCP-Nets

require users assess weights (DAmbrosio & Birmingham, 1995). allows
direct tradeoffs values different attributes assessed concisely. Case-based
approaches also recently considered (Ha & Haddawy, 1998).
Models make preference elicitation process easier imposing specific
requirements form utility preference function. consider CP-net representation offer appropriate tradeoff allowing flexible preference expression
imposing particular preference structure. Specifically, unlike much work cited
above, CP-nets capture conditional preference statements.
remainder paper organized follows. Section 2 provides background
preference orderings, important notions preferential independence conditional ceteris paribus preference statements. define CP-nets, discussing
semantics expressive power depth, models properties. Section 3
present algorithm outcome optimization CP-nets provide example
application CP-nets illustrates optimization process. Section 4 introduces two
kinds queries preferential comparison, namely, ordering dominance queries,
investigates computational properties. Section 5 discusses several general techniques
answering dominance queries exploit structure CP-net. Section 6
discuss applicability complexity results algorithms slight generalization
CP-nets allow incompletely specified local preferences and/or statements preferential indifference. Finally, Section 7 examine related work applications
CP-nets, discuss number interesting directions future theoretical research
applications.

2. Model Definition
Philosophical treatment many intuitive qualitative preferential statements began 1957
pioneering work Hallden (1957), continued Castaneda (1958), von Wright
(1963, 1972), Kron Milovanovic (1975), Trapp (1985), Hansson (1996). reason
intensive analysis statements expressed concisely opening
Hanssons (1996) paper:
discussing wife table buy living room, said:
round table better square one. mean
irrespectively properties, round table better
square-shaped table. Rather, meant round table better (for
living room) square table differ significantly
characteristics, height, sort wood, finishing, price, etc.
preference ceteris paribus everything else equal.
preferences express act upon seem type. [Emphasis
added.]
important property ceteris paribus preferential statements intuitive nature
users types. Independently work philosophers area, reasoning
ceteris paribus statements drawn attention AI researchers. example, Doyle
et al. (1991) introduced logic relative desire treat preference statements
ceteris paribus assumption. logic bears similarity von Wrights (1963) logic
137

fiBoutilier, Brafman, Domshlak, Hoos & Poole

preferences, supports complicated inferences.2 However, best
knowledge, serious attempt made exploit preferential independence
compact efficient representation ceteris paribus statements. paper,
take steps toward structured modeling qualitative ceteris paribus preferential statements.
start defining notion (qualitative) preference relation number
basic preference independence concepts, followed introduction CP-nets
semantics.
2.1 Preference Relations
focus attention single-stage decision problems complete information, ignoring paper issues arise multi-stage, sequential decision analysis
considerations risk arise context uncertainty.3 begin outline
relevant notions decision theory. assume world one number
states state number actions performed.
action, performed state, specific outcome (we concern
uncertainty action effects knowledge state). set outcomes denoted
O. preference ranking total preorder set outcomes: o1 o2 means
outcome o1 equally preferred decision maker o2 . use o1 o2
denote fact outcome o1 strictly preferred decision maker o2
(i.e., o1 o2 o2 6 o1 ), o1 o2 denotes decision maker indifferent o1
o2 (i.e., o1 o2 o2 o1 ). use terms preference ordering relation
interchangeably ranking.
aim decision making certainty is, given knowledge specific state,
choose action preferred outcome. note ordering
vary across decision makers. instance, two customers might radically different
preferences computer system configurations sales program helping construct.
Often, state s, certain outcomes cannot result action :
outcomes obtained called feasible outcomes (given s). many instances,
mapping states actions outcomes quite complex. decision
scenarios, actions outcomes may equated: user allowed directly select
feasible outcome (e.g., select product desirable combination attributes). Often
states play role (i.e., single state).
One thing makes decision problems difficult fact outcomes actions
preferences usually represented directly. example, actions may represented
set constraints set decision variables. focus preferences.
assume set variables (or features attributes) V = {X1 , . . . , Xn }
decision maker preferences. variable Xi associated domain Dom(Xi ) =
{xi1 , . . . , xini } values take. assignment x values set X V variables
(also called instantiation X) function maps variable X element
domain; X = V, x complete assignment, otherwise x called partial assignment.
2. detailed discussion issue, refer reader Doyle Wellman (1994).
3. issues include assigning preferences sequences outcome states, assessing uncertainty beliefs
system dynamics, assessing users attitude towards risk.

138

fiCP-Nets

denote set assignments X V Asst(X). x assignments
disjoint sets X Y, respectively (X = ), denote combination x
xy. X = V, call xy completion assignment x. denote Comp(x) set
completions x. Complete assignments correspond directly outcomes
user possesses preferences. outcome o, denote o[X] value x Dom(X)
assigned variable X outcome.
Given problem defined n variables domains Dom(X1 ), . . . , Dom(Xn ),
|Dom(X1 )| |Dom(Xn )| assignments. Thus direct assessment preference
function usually infeasible due exponential number outcomes. Fortunately,
preference function specified (or partially specified) concisely exhibits sufficient
structure. describe certain standard types structure here, referring Keeney
Raiffa (1976) detailed description (and other) structural forms discussion
implications. set variables X preferentially independent complement
= V X iff, x1 , x2 Asst(X) y1 , y2 Asst(Y),
x1 y1 x2 y1 iff x1 y2 x2 y2 .
words, structure preference relation assignments X,
variables held fixed, matter values variables take.
relation holds, say x1 preferred x2 ceteris paribus. Thus, one
assess relative preferences assignments X once, knowing preferences
change attributes vary.
define conditional preferential independence analogously. Let X, Y, Z
nonempty sets partition V. X conditionally preferentially independent given
assignment z Z iff, x1 , x2 Asst(X) y1 , y2 Asst(Y),
x1 y1 z x2 y1 z iff x1 y2 z x2 y2 z.
words, X preferentially independent Z assigned z. X conditionally preferentially independent z Asst(Z), X conditionally preferentially
independent given set variables Z.
Note ceteris paribus component definitions ensures statements
one makes relatively weak. particular, imply stance specific value
tradeoffs. Consider two variables B preferentially independent,
preferences values B assessed separately; instance, suppose
a1 a2 b1 b2 . Clearly, a1 b1 preferred outcome a2 b2 least;
feasibility constraints make a1 b1 impossible, must satisfied one a1 b2
a2 b1 . cannot tell preferred using separate assessments. However,
stronger conditions (e.g., additive preferential independence) one construct
additive value function weights assigned different attributes (or attribute
groups). decomposition preference function allows one identify
preferred outcomes rather readily, this, well special forms preference
structure, especially appropriate attributes take numerical values.
extensive discussion various special forms preference functions refer Keeney
Raiffa (1976), well Bacchus Grove (1995, 1996) Shoham (1997).
139

fiBoutilier, Brafman, Domshlak, Hoos & Poole

2.2 CP-Networks
representation preferences graphical nature, exploits conditional preferential independence structuring preferences user. model similar Bayesian
network (Pearl, 1988) surface; however, nature relation nodes
within network generally quite weak (e.g., compared probabilistic relations
Bayes nets). Others defined graphical representations preference relations;
instance Bacchus Grove (1995, 1996) shown strong results pertaining
undirected graphical representations additive independence. representation semantics rather distinct, main aim using graph capture statements
qualitative conditional preferential independence. note reasoning ceteris
paribus statements explored AI (Doyle et al., 1991; Wellman & Doyle, 1991;
Doyle & Wellman, 1994), though context network representations exploiting
preferential independence computational fashion.
variable Xi , ask user identify set parent variables Pa(Xi )
affect preference various values Xi . is, given particular value assignment
Pa(Xi ), user able determine preference order values Xi ,
things equal. Formally, given Pa(Xi ) Xi conditionally
preferentially independent V (Pa(Xi ) {Xi }). Given information, ask user
explicitly specify preferences values Xi instantiations variable
set Pa(Xi ). use information create annotated directed graph
nodes stand problem variables, every node Xi Pa(Xi ) immediate
ancestors. node Xi annotated conditional preference table (CPT) describing
users preferences values variable Xi given every combination parent
values. words, letting Pa(Xi ) = U, assignment u Asst(U), assume
total preorder iu provided domain Xi : two values x x, either
x ju x0 , x0 ju x, x ju x0 . simplicity presentation, ignore indifference
algorithms. Though treatment indifference straightforward semantically, consistency
arbitrary networks indifference cannot assumed, discuss Section 2.5.
Likewise, assume CPTs variables fully specified, though discuss
partially specified CPTs Section 6.
call structures conditional preference networks CP-networks (CP-nets,
short).
Definition 1 CP-net variables V = {X1 , . . . , Xn } directed graph G
X1 , . . . , Xn whose nodes annotated conditional preference tables CPT(Xi )
Xi V. conditional preference table CPT(Xi ) associates total order iu
instantiation u Xi parents P a(Xi ) = U.
illustrate CP-net semantics consequences several small
examples. ease presentation, variables examples boolean, though
semantics defined features arbitrary finite domains.
Example 1 (My Dinner I) Consider simple CP-net Figure 1(a) expresses
preference dinner configurations. network consist two variables W ,
standing soup wine, respectively. Now, strictly prefer fish soup (Sf ) vegetable soup (Sv ), preference red (Wr ) white (Ww ) wine conditioned
140

fiCP-Nets

89:;
?>=<



?>=<
89:;
W



Sv Ww

Sf Sv






Wr
v

Sf
Sv

Ww Wr
Wr Ww






Wr
f

z



Ww
f


(a)
(b)

Figure 1: (a) CP-net Dinner I: Soup Wine; (b) induced preference graph.

soup served: prefer red wine served vegetable soup, white wine
served fish soup.
Figure 1(b) shows preference graph outcomes induced CP-net. arc
graph directed outcome oi oj indicates preference oj oi
determined directly one CPTs CP-net. example, fact
Sv Wr preferred Sv Ww (as indicated direct arc them) direct
consequence semantics CPT(W ). top element (Sv Ww ) worst outcome
bottom element (Sf Ww ) best.

Example 2 (My Dinner II) Figure 2(a) extends chain CP-net Example 1 adding
main course another variable. example, preference options
main course clear: strictly prefer meat course Mmc fish course Mf c .
addition, prefer two fish courses one dinner; thus preference
vegetable fish soup conditioned main course: prefer open fish soup
main course meat, vegetable soup main course fish. Example 1,
Figure 2(b) shows corresponding induced preference graph outcomes.

Example 3 (Evening Dress) Figure 3(a) illustrates another CP-net expresses preferences evening dress. consists three variables J, P , S, standing jacket,
pants, shirt, respectively. unconditionally prefer black white color
jacket pants, preference red white shirts conditioned
combination jacket pants: color, white shirt
make outfit colorless, thus prefer red shirt. Otherwise, jacket
pants different colors, red shirt probably make outfit flashy, thus
prefer white shirt. Figure 3(b) shows corresponding preference graph.
141

fiBoutilier, Brafman, Domshlak, Hoos & Poole

89:;
?>=<


Mmc Mf c


?>=<
89:;


Mmc
Mf c


89:;
?>=<
W

Sf
Sv

(a)

Sf Sv
Sv Sf

Ww Wr
Wr Ww


Mf c Sf

Wr





Mf c Sf

Ww






Mfmc Sv Ww
mmm
mmm



ff vmmm





W
v
r
f c



Sv Ww
Mmc
l
l
l
l
lll
lll
l
l

vll

Mmc Sv Wr


q
Mmc Sf QWQr
QQQ
QQQ
QQQ
Q
~

Q(

mc Sf Ww
(b)

Figure 2: (a) CP-net Dinner II; (b) induced preference graph.

142

fiCP-Nets



Jw Pw Sw

Jb Jw




Jw Pw Sr

Pb Pw

89:;
?>=<
?>=<
89:;
J3
P
33
ff
ff
33
ff
33 ffffff
ff
89:;
?>=<

Jb Pb
Jw Pb
Jb Pw
Jw Pw

Sr Sw
Sw Sr
Sw Sr
Sr Sw






Jw Pb Sr
iJb Pw Sr



iiii
iiii



iiii
iiii



"





Jb Pw LSw
Jw Pb Sw
LLL

LLL
LLL

&

Jb Pb Sw

(a)

|

Jb Pb Sr
(b)

Figure 3: (a) CP-Net Evening Dress: Jacket, Pants Shirt; (b) induced preference graph.

2.3 Semantics
semantics CP-net straightforward. defined terms set preference
rankings consistent set preference constraints imposed CPTs.
Definition 2 Let N CP-net variables V, Xi V variable, U V
parents Xi N . Let = V (U {Xi }). Let iu ordering Dom(Xi )
dictated CPT(Xi ) instantiation u Asst(U) Xi parents. Finally let
preference ranking Asst(V).
preference ranking satisfies iu iff havefor Asst(Y) x, x0
Dom(Xi ) yxu yx0 u whenever x iu x0 . preference ranking satisfies CPT
CPT(Xi ) iff satisfies iu u Asst(U). preference ranking satisfies
CP-net N iff satisfies CPT(Xi ) variable Xi .
CP-net N satisfiable iff preference ranking satisfies it.
Thus network N satisfied iff satisfies conditional preferences expressed
CPTs N ceteris paribus interpretation.
Theorem 1 Every acyclic CP-net satisfiable.
143

fiBoutilier, Brafman, Domshlak, Hoos & Poole

Proof: prove constructively building satisfying preference ordering. proof
induction number variables. theorem trivially holds one variable,
total ordering specified directly CP-net.
Suppose theorem holds CP-nets fewer n variables. Let N
network n variables. N acyclic, least one variable parents;
let X variable. Let x1 x2 . . . xk ordering Dom(X) dictated
CP (X). xi , construct CP-net, Ni , n 1 variables V {X}
removing X initial CP-net, variable child X, revising
CPT restricting row X = xi . inductive hypothesis, construct
preference ordering reduced CP-nets Ni .
construct preference ordering original network N follows.
rank every outcome X = xi preferred outcome X = xj xi xj
CPT(X). outcomes identical values xi X, rank according
ordering associated Ni (ignoring value X). easy see
preference ordering satisfies N .
example, consider CP-net Example 1 (Figure 1). Somewhat surprisingly,
information captured network sufficient totally order outcomes:
f Ww f Wr v Wr v Ww
since ranking satisfies CP-net. However, need case
general, satisfiable CP-net satisfied one ranking. instance,
consider CP-net Figure 4.4 two rankings satisfy network:
abc abc abc abc abc abc abc abc
abc abc abc abc abc abc abc abc
Preferential entailment CP-net defined standard way.
Definition 3 Let N CP-net variables V, o, o0 Asst(V) two outcomes. N entails o0 (i.e., outcome preferred o0 ), written N |= o0 , iff
o0 holds every preference ordering satisfies N .
Lemma 2 Preferential entailment transitive. is, N |= o0 N |= o0 o00
N |= o00 .
Proof: N |= o0 N |= o0 o00 o0 o0 o00 preference rankings
satisfying N . rankings transitive, must o00 satisfying
rankings.
example, consider CP-net N Figure 4(a) following three outcomes:
= abc, o0 = abc, o00 = abc. outcomes o0 assign values
variables except B. addition, given value Pa(B) = {A} o0 , value
B (B = b) preferred value B (B = b) o0 , else equal. Therefore,
N |= o0 . case o0 o00 , argument respect
4. network Example 2 (My Dinner II) variables renamed.

144

fiCP-Nets

abc

89:;
?>=<




abc



abc
tt
zttt







?>=<
89:;
B

b b
b b

abc


b
b


?>=<
89:;
C

c c
c c



abc



tt
ztt

abc


abc KK

KK
KK
%

(a)

abc

(b)
Figure 4: simple chain-structured CP-network.
variable C show N |= o0 o00 well. Observe o00 cannot derived
directly CPTs N . However, Lemma 2, follows relation
inferred taking transitive closure direct relations o0 o0 o00 .
Notice that, given CP-net, assess outcome terms conditional
preferences violates. example, CP-net Example 1: outcome Sf Ww
violates none preference constraints; Sf Wr violates conditional preference
W ; Sv Wr violates preference S; Sv Ww violates both. Somewhat surprisingly,
ceteris paribus semantics implicitly ensures violating preference worse
violating W , since Sf Wr Sv Wr . is, parent preferences
higher priority child preferences. property important implications
inference see below.
2.4 Cyclic Networks
mentioned, nothing semantics CP-net model forces acyclic. However,
according Theorem 1, acyclicity network automatically confers important
property model: network satisfiable (i.e., exists preference ordering
satisfies ceteris paribus preference assertions imposed CPTs).
cyclic CP-nets, situation much complicated. example, consider
binary-valued cyclic CP-net structure Figure 5(e). CPTs network
specified Figure 5(a), induced preference graph (see Figure 5(b))
extended complete preference ordering consistently. However, CPTs specified
Figure 5(c), network unsatisfiable (the induced preference graph, shown
145

fiBoutilier, Brafman, Domshlak, Hoos & Poole

Figure 5(d) cannot completed consistently). example shows consistency
cyclic CP-nets guaranteed, depends actual nature CPTs.

a1 : b1 b2
a2 : b2 b1
b1 : a1 a2
b2 : a2 a1

a2 b1F
a1 b1

(a)
a1 : b1 b2

a1 b2

FF xx
FxFx
xx FFF
x
|x
"

a2 b2

(b)
a1O b1

/ a1 b2

a2 b1

a2 b2

a2 : b2 b1
b1 : a2 a1
b2 : a1 a2
(c)

?>=<
89:;
AU


?>=<
89:;
B



(d)

(e)

Figure 5: Examples satisfiable unsatisfiable cyclic CP-net binary variables.

Recently, initial results consistency testing cyclic CP-nets presented
Domshlak Brafman (2002a). particular, wide class cyclic, binary-valued CPnets identified efficiently testable consistency. However, results cover
part spectrum, research cyclic CP-nets needed.
Beyond open computational questions cyclic CP-nets raise, usefulness
requires analysis. One argue possible cluster variables
preserve acyclicity. Although approach technically feasible probably useful
many domains, cannot provide general solution. First, clustering affect
space requirements problem description thus, generally degrade efficiency
reasoning preferences. Second, certain domains, may natural
express cyclic preferences even acyclic representation could used. example,
seems case work preference-based presentation web page content
(Domshlak, Brafman, & Shimony, 2001), argued preferred presentation
certain component web page may depend presentation neighbors
page, whose preferred presentation depend presentation, on.
One could argue preferences naturally exhibit cyclic structure acyclic nets
theoretical interest only. experience indicates opposite. Acyclic CP-nets
shown effective natural above-mentioned work web page presentation
(Domshlak et al., 2001), well related project deals presentation
multi-media content medical domain (Gudes, Domshlak, & Orlov, 2002)a
extensive example latter domain presented next section. Moreover,
domains, found difficult generate intuitively reasonable cyclic networks.
due fact cycle implies variables equally important.
146

fiCP-Nets

Typically, case. Thus, apparent utility acyclic networks,
fact use composite variables made clustering primitive variables,
additional complexity involved cyclic networks, consider acyclic CP-nets
remainder paper. However, investigation cyclic CP-nets, well
characterization different classes utility functions represented cyclic
acyclic networks, remains interest.
2.5 Indifference
far assumed preference constraints CPT CP-net totally
order outcomes variable question. Specifically, variable Xi parents
U, u Asst(U), assume iu total order Dom(Xi ). general
definition CP-net allow arbitrary total preorder iu Dom(Xi ), thus allowing
user express indifference two values variable X, say x x0 , given u.
denote x x0 CPT(X).
turns ceteris paribus semantics quite strong say two
variable values equally preferred.
Example 4 Consider following two CPTs network variables B,
parent B:
a;
: b b; : b b;
assert user indifferent a, hold, prefers b,
hold, prefers b. derive following preferences outcomes:
ab ab ab ab ab
statements consistent preference ranking, hence network
satisfiable. One way interpret someone really preferences:
: b b; : b b;
cannot indifferent a, ceteris paribus.
points potential difficulty use indifference CP-nets. One must
careful express indifference two values variable (A case), yet
express (strict) conditional preference child variable (B) depends
values user indifferent. Intuitively, case, seems user
expressing fact would like value B match (with respect
sign), intends preference ab ab (or vice versa). case,
making parent B expresses preference B subsidiary A,
intent. case, either cyclic network (indeed satisfiable network
discussed Section 2.4) clustering variables B seems appropriate.
Despite this, indifference used safely follows. Let Xi variable
network N parents U, let Xj child Xi . Let denote remaining
147

fiBoutilier, Brafman, Domshlak, Hoos & Poole

parents Xj (those excluding Xi ). Suppose u Asst(U), x, x0
Dom(Xi ), x x0 iu . long local orderings CPT(Xj ) fixed
instantiation identical whether x x0 holds, network N satisfiable.
precisely, jxy =jx0 Asst(Y), network N satisfiable. Thus,
indifferent x x0 , preferences values Xi children,
exhibit indifference whether context includes x x0 .5
simplicity presentation, remainder paper continue assume
preference constraints CPT CP-net totally order outcomes variable
question. However, Section 6, discuss applicability results satisfiable
CP-nets capture statements preferential indifference.

3. Outcome Optimization
One principal properties CP-nets that, given CP-net N , easily determine
best outcome among preference rankings satisfy N . call query
outcome optimization query. turns simple task CP-nets.
3.1 Algorithm Outcome Optimization
Intuitively, generate optimal outcome simply need sweep network
top bottom (i.e., ancestors descendents) setting variable
preferred value given instantiation parents. Indeed, network
generally determine unique ranking, determine unique best outcome (assuming
indifference). generally, suppose given evidence constraining possible outcomes
form instantiation z subset Z V network variables. Determining
best completion z (that is, best outcome consistent z) achieved
similar fashion, outline.
Outcome optimization queries answered using following forward sweep procedure, taking time linear number variables. Assume partial instantiation
z Asst(Z), goal determining (unique) Comp(z) N |= o0
o0 Comp(z) {o}. effected straightforward sweep
network. Let X1 , . . . , Xn topological ordering network variables. set
Z = z, instantiate Xi 6 Z turn maximal value given instantiation
parents. procedure exploits considerable power ceteris paribus semantics graphical modeling preferential statements easily find optimal
outcome given certain observed evidence (or imposed conjunctive constraints).
Lemma 3 forward sweep procedure constructs preferred outcome Comp(z).
Proof: Let vz outcome set completions z. Define sequence outcomes
vi , 0 n, follows: (a) v0 = vz ; (b) Xi 6 Z, vi constructed setting
value Xi preferred value given instantiation parents vi1 ,
variables retaining values vi1 ; (c) Xi Z, vi = vi1 .
construction, vi vi1 . last outcome vn precisely constructed forward
5. restriction relaxed somewhat take account fact Xj parents could
lie set U, case rankings need agree every indifference pair x x0 .

148

fiCP-Nets

sweep algorithm. Notice arrive outcome irrespective starting
point vz (by assumption, ties). Since vn vz outcome vz consistent
evidence, forward sweep procedure yields optimal outcome.
3.2 Example Application
turn illustration use CP-nets context CP-net based system
adaptive multimedia document presentation. Applications based system
presentation web-based content multi-media medical data recently presented
Domshlak et al. (2001) Gudes et al. (2002). example demonstrate
simplicity preference specification using CP-nets, utility acyclic networks,
use optimization algorithm described above.
system consists two toolsthe authoring tool, viewing tool. central
part authoring tool module specification CP-net corresponding
created and/or edited multimedia document. Using CP-net, content provider
express preferences regarding presentation document content. example,
content provider may prefer material presented
material presented. result preference-based multimedia document design
meta-document specifying present present it.
description content providers preferences, captured acyclic CP-net,
becomes static part document, sets parameters initial presentation.
Given document, viewing tool responsible reasoning preferences;
specifically, must determine optimal reconfiguration document context
interaction viewer document. process, users k recent
content choices viewed constraints form items must appear specified.
Subject constraints, optimal document presentation respect content
providers CP-net must generated. Thus, particular session, actual
presentation changes dynamically based users choices. precisely, whenever
new user input obtained, optimization algorithm constructs best presentation
document components respect content providers preferences among
presentations conform users recent viewing choices. process uses
forward sweep procedure described above.
Example 5 (Multimedia Document) Consider medical record consists six components: two components correspond set medical tests conducted 2001an X-ray
image textual notes physicianand four components correspond set medical
tests 2002a CT (computerized tomography) image, X-ray image, graph illustrating results electromyography, textual notes physician. purposes
illustration, assume preferences content provider (e.g., latter physician)
presentation options components captured using CP-net shown
Figure 7. specific details preferencesthe nature preferential dependencies
precise details CPTs summarized follows:
CT-image [CT image, 2002] consist four CT images different parts body,
shown Figure 6(a). six presentation options CT-image:
either completely presented (ctplain ), completely hidden (cthide ), presented
149

fiBoutilier, Brafman, Domshlak, Hoos & Poole

(a)

(b)

(c)

(d)

Figure 6: Document components Multimedia Document example.


CTimage
GG
GG
GG
GG
GG
GG
#

Xray
ww
ww
w
w
ww
ww
w

{w


Graph
Xrayold


Notes




Notesold

Figure 7: Multimedia Document CP-net.
zoom-in one four parts (ctlt , ctrt , ctlb , ctrb , standing left-top, righttop, left-bottom, right-bottom parts, respectively). physicians preference
presentation options CT-image unconditional:
cthide ctlt ctrt ctlb ctrb ctplain
X-ray [X-ray, 2002] either hidden (xrayhide ), presented (xrayplain ),
presented segmentation (xraysegm ); image segmentation depicted
Figures 6(b) 6(c), respectively. preference presentation options
X-ray depends presentation CT-image:
ctplain
(ctplain cthide )
cthide

xrayhide xrayplain xraysegm
xrayplain xraysegm xrayhide
xraysegm xrayplain xrayhide
150

fiCP-Nets

Graph [Electromyography, 2002] shown Figure 6(d), either presented
(graphplain ), hidden (graphhide ). preference presentation options
Graph depends presentation CT-image X-ray:
(ctlt ctrt ctlb ctrb ) xraysegm
otherwise

graphplain graphhide
graphhide graphplain

Notes [Textual notes, 2002] either fully presented (notesplain ), summarized
(notessumm ), omitted together (noteshide ). preference presentation
options Notes depends presentation CT-image Graph:
cthide
(cthide ) graphplain
otherwise

noteshide notessumm notesplain
notessumm notesplain noteshide
noteshide notessumm graphplain

X-ray-old [X-ray, 2001] either hidden (xrayoldhide ), presented (xrayoldplain );
image depicted below. preference presentation options X-ray-old
depends presentation X-ray:

xrayhide
(xrayhide )

xrayoldhide xrayoldplain
xrayoldplain xrayoldhide

Notes-old [Textual notes, 2001] either presented (notesoldplain ), omitted
together (notesoldhide ). preference presentation options Notes-old
depends presentation X-ray-old:
xrayoldhide
xrayoldplain

notesoldplain notesoldhide
notesoldhide notesoldplain


beginning viewing session, initial presentation document, depicted
Figure 8(a), determined using forward sweep procedure Z = : component set preferred presentation given presentation immediate parents
CP-net. example, CT-image hidden, since preferred option
component. Subsequently, X-ray image presented segmented, since CT-image
presented, and, turn, electromyography Graph presented decision presentation options CT-image X-ray. Suppose viewer chooses
151

fiBoutilier, Brafman, Domshlak, Hoos & Poole

ct
hide MM
MMM
MMM
M&
xray

segm
q
q
qqq
qqq

q
x




graphplain xrayoldplain

"
notes


plain notesold hide


(a)
NNN
NNN
NNN
&
xray

plain
q
qq
qqq
q
q
xq



graphplain xrayoldplain
ctrt


#

notes

summ notesold hide


(b)
OOO
OOO
OOO
O'
xrayhide
pp
p
p
pp
xppp



graphplain xrayoldhide
ctrt


#

notes

summ notesold plain


(c)
ctplain

MMM
MMM
MMM
&
xrayhide
qq
qqq
q
q
xqq



graphhide xrayoldhide

#

notes

hide notesold plain


(d)
Figure 8: Document presentations various user choices.

152

fiCP-Nets

look right-top part CT-image.6 terms forward sweep procedure,
choice sets Z = {CTimage}, z = {ctrt }. result forward sweep procedure appears Figure 8(b); follows, shaded nodes stand
evidence-constrained variables Z. Now, X-ray image presented without segmentation
zoom-in right-top part CT-image, Notes summarized since
electromyography Graph presented, CT-image hidden.
Suppose viewer consequently chooses hide X-ray image. number
recent viewer choices taken constrain presentation greater one,
choice set Z = {CTimage, Xray}, z = {ctrt , xrayhide }. result forward
sweep procedure appears Figure 8(c). consequently viewer chooses see whole
CT-image, z = {ctplain , xrayhide }, updated presentation shown Figure 8(d).

4. Comparing Outcomes
Outcome optimization task supported preference representation model. Another basic query respect model preferential comparison outcomes. Two outcomes o0 stand one three possible relations
respect CP-net N : either N |= o0 ; N |= o0 o; N 6|= o0
N 6|= o0 o.7 third case, specifically, means network N contain
enough information prove either outcome preferred (i.e., exist
preference orderings satisfying N o0 o0 o). two
distinct ways compare two outcomes using CP-net:
1. Dominance queries Given CP-net N pair outcomes o0 , ask whether
N |= o0 . relation holds, preferred o0 , say dominates o0
respect N .
2. Ordering queries Given CP-net N pair outcomes o0 , ask N 6|=
o0 o. relation holds, exists preference ordering consistent N
o0 . words, consistent knowledge expressed N
order o0 (i.e., assert preferred o0 ). case say
consistently orderable o0 respect N .
Ordering queries clearly weaker dominance queries. Indeed, N |= o0 ,
N 6|= o0 o. may case N 6|= o0 even though N 6|= o0 .
dominance queries typically useful, ordering queries sufficient many
applications one may satisfied knowing outcome consistently
ordered o0 . example, consider set products human automated seller
would like present customer non-increasing order customer preference.
seems reason use strong dominance relation sort products.
applications, dominance queries cannot replaced ordering queries. instance, dominance queries shown integral part constraint-based preferential
optimization CP-nets (Boutilier, Brafman, Geib, & Poole, 1997).
6. document explorer (which part viewing tool) illustrated order make
snapshots smaller.
7. Recall that, time being, consider CP-nets indifference CPTs; hence two outcomes
cannot proven equally preferred.

153

fiBoutilier, Brafman, Domshlak, Hoos & Poole

begin showing ordering queries respect acyclic CP-nets
answered time linear number variables. addition, show set
outcomes sorted consistent non-increasing order respect acyclic
CP-net using ordering queries only. provide complexity analysis dominance
queries. First, introduce study particular form reasoning, namely search
flipping sequences, used answer dominance queries. Using technique,
focusing binary-valued CP-nets, show connections structure
CP-net graph worst-case complexity dominance queries. discuss dominance
queries detail Section 5, present general search techniques
flipping sequences.
4.1 Ordering Queries Easy
show ordering queries respect acyclic (not necessarily binary-valued)
CP-nets answered time linear number variables. corresponding
algorithm exploits graphical structure model. Likewise, show acyclic
CP-nets, construct non-increasing ordering outcomes, consistent CPnet, using ordering queries.
Corollary 4 Let N acyclic CP-net, o, o0 pair outcomes. exists
variable X N , that:
1. o0 assign values ancestors X N ,
2. given assignment provided (and o0 ) Pa(X), assigns preferred
value X assigned o0
N 6|= o0 o.
Proof: construction Theorem 1 provides preference ordering satisfying N
o0 . Thus o0 true models N , consequence N .
Corollary 4 presents condition sufficient necessary truth
ordering query N 6|= o0 o. instance, consider Example 2, let = Mmc Sv Ww
o0 = Mf c Sv Wr . two outcomes incomparable according CP-network
(i.e., neither proven preferred other), 6 o0 cannot deduced
using conditions Corollary 4, root variable chain CP-net,
assigns preferred value assigned o0 .
Despite fact condition provided Corollary 4 N 6|= o0
necessary consistent orderability, show sufficient provide consistent
ordering pair outcomes.
Theorem 5 Given acyclic CP-net N , two outcomes o0 variables
N , complexity determining truth least one ordering queries, N 6|= o0
N 6|= o0 , O(n).
Proof: variable Xi , let Pa(Xi ) = U N u u denote assignment
U made outcomes o0 , respectively. variables Xi , o0 assign
different values Xi values ancestors Xi N , identified
154

fiCP-Nets

O(n) time top-down traversal N . (Note u = u0 Xi ).
Xi o[Xi ] iu o0 [Xi ], using Corollary 4 deduce N 6|= o0 o.
Otherwise, exist two variables type, Xi Xj , o[Xi ] iu o0 [Xi ]
o[Xj ] iu o0 [Xj ]; case, Corollary 4 implies N 6|= o0 N 6|= o0

Corollary 4 provides effective algorithm answering ordering queries; however,
computational efficiency comes price: soundif algorithm says
consistently orderable o0 , indeed, N 6|= o0 o; incompleteif provides
negative response query N 6|= o0 o, still may case N 6|= o0 o. Theorem 5
provides effective algorithm sound, partially complete sense
return positive answer least one N 6|= o0 N 6|= o0 . words,
allow us determine least one outcome consistently ordered
other.
Though incompleteness algorithm single ordering queries problematic,
partial completeness algorithm paired queries sufficient allow one
find consistent ordering outcomes polynomial time, least case
acyclic CP-net. first introduce notation. write N `oq o0 represent
algorithm paired ordering queries tells us N 6|= o0 holds (i.e.,
consistently orderable o0 ) N 6|= o0 (i.e., o0 orderable o).
N `oq o0 , assured indeed orderable o0 ; due
incompleteness algorithm, cannot sure o0 orderable o. write
N `oq ' o0 denote algorithm returns positive response ordering
queries (i.e., tells us outcomes consistently orderable other).
soundness algorithm ensures outcomes indeed consistently preferred
case. Note partial completeness ensures either N `oq o0 , N `oq o0 o,
N `oq ' o0 . sufficient allow us produce consistent ordering
set outcomes.
Theorem 6 Given acyclic CP-net N variable set V, set outcomes
o1 , . . . , om V, ordering outcomes consistently N done using ordering
queries only.
Proof: Define two binary relations outcomes: o0 iff N `oq o0 ' o0 iff
N `oq ' o0 . first show transitive closure relation asymmetric.
Assume contrary exists set outcomes o1 , . . . , ok that:
o1 o2 ok o1

(1)

1 k, let V (oi ) set variables X value assigned X oi
improved given assignment provided oi Pa(X). Let Ni subgraph N
consisting variables V (oi ) descendants N . Observe Corollary 4
implies Ni Ni+1 1 < k, Nk N1 . see this, notice if, i,
Ni 6 Ni+1 , exists variable X that: (i) ancestors X assigned
preferred values oi oi+1 ; (ii) given oi [Pa(X)] = oi+1 [Pa(X)],
X assigned preferred value oi+1 one less preferred values oi .
155

fiBoutilier, Brafman, Domshlak, Hoos & Poole

However, case, ordering query determine N 6|= oi oi+1 , contradicts
assumption oi oi+1 .
one graph containment relations Ni Ni+1 strict, initial assumption (1)
trivially contradicted. Therefore, left case of:
N1 = N2 = = Nk = N 0
Recalling N 0 acyclic, consider variable Xj N 0 ancestors N 0 . Let
U = Pa(Xj ) parents Xj original network N (note U N 0 = ).
construction Ni have:
o1 [U] = o2 [U] = = ok [U] = u
must case since ancestors Xj assigned unique optimal
assignment (of u part) since none variables improvable. entails
o1 [Xj ] ju o2 [Xj ] ju ju ok [Xj ] ju o1 [Xj ],
inconsistent definition CP-net.
exploit asymmetric nature relation follows. N |= o0 ,
must o0 . Therefore, relation N representing induced preference graph
N subset . Thus total ordering o1 , . . . , om consistent
consistent N .
immediate consequence Theorems 5 6 that, given set outcomes
CP-net N , complexity ordering outcomes consistently preference
graph induced N O(nm2 ) (i.e., cost comparing every pair outcomes
ordering accordingly).
4.2 Dominance Queries Flipping Sequences
ceteris paribus semantics CP-nets allows one directly use information
CPT variable X alter flip value X within outcome directly obtain
improved (preferred) worsened (dispreferred) outcome. sequence improving flips
one outcome another provides proof one outcome preferred, dispreferred,
another rankings satisfying network. defining notion precisely,
illustrate intuitions example.
Example 6 Consider CP-net Figure 4. following two
rankings satisfy network:
z }| {
abc abc abc abc abc abc abc abc
abc abc abc |abc {z
abc} abc abc abc
Thus, two outcomes totally ordered abc abc. Notice remove
either abc abc chains outcomes, move one outcome
next chain flipping value exactly one variable according preference
156

fiCP-Nets

information CPT given instantiation parents. example, move
first outcome sequences (abc) second (abc), use fact c c
given b prove second outcome dispreferred first; flip C
less preferred value given instantiation b parent B. Conversely, move
backwards sequence flipping c second outcome c, thereby obtaining
preferred first outcome.
Recall Corollary 4 demonstrates violating preference constraints
parent variable less preferred violating preference constraints
children. greater importance parent variables implicit ceteris paribus
semantics. consider two outcomes abc abc, unordered CPnet Figure 4. outcome abc violates preference values A,
outcome abc violates preferences values B C; ancestor
B C. semantics CP-nets specify outcomes
preferredintuitively, though preference higher priority B C, two
violations lower priority preferences may preferred violation
single higher priority preference.
two outcomes o0 , every improving flipping sequence o0 uniquely
corresponds directed path node o0 node preference graph
induced CP-net. instance, consider CP-net Figure 9(a), exactly
network Evening Dress example (Example 3), simpler variable names.
four alternative flipping sequences outcome abc outcome abc,
corresponding four paths outcomes induced preference graph,
depicted Figure 9(b):
abc abc abc
abc abc abc abc abc
abc abc abc
abc abc abc abc abc
Therefore, abc abc consequence CP-net. contrast, flipping
sequence (directed path) abc abc, hence two outcomes incomparable.
examples suggest construction flipping sequence used
prove dominance.
Definition 4 Let N CP-net variables V, Xi V, U parents Xi ,
= V (U {Xi }). Let uxy Asst(V) outcome, x Dom(Xi ),
u Asst(U), Asst(Y). improving flip outcome uxy respect variable
Xi outcome ux0 x0 iu x. (Note improving flip w.r.t. Xi
exist x preferred value Xi given u.)
improving flipping sequence respect N sequence outcomes o1 , . . . , ok
that, < k, oi+1 improving flip oi respect variable.
improving flipping sequence outcome outcome o0 improving sequence
o1 , . . . , ok o1 = ok = o0 .
157

fiBoutilier, Brafman, Domshlak, Hoos & Poole



b b

abc


89:;
?>=<
?>=<
89:;
A3
B
33
ff
ff
33
ff
33 ffffff
ff
89:;
?>=<
C
ab
ab
ab
ab

abc




abc

abc
jjjj

jj
jjj j
j
j
j
tjj
abc
abc JJ
JJ
JJ
%


c c
c c
c c
c c

abc

}

abc
(b)

(a)

Figure 9: Evening Dress CP-net Example 2 simpler names.
One define worsening flips worsening flipping sequences entirely analogous
way. Obviously, worsening flipping sequence reverse improving flipping
sequence, vice versa.
two important things notice examples above. First, improving (or worsening) flipping sequence used show one outcome better
another. Second, preference violations worse (i.e., larger negative impact
preference outcome) higher network, although cannot compare always two (or more) lower level violations violation single ancestor constraint.
observations underlie inference algorithms below.
Theorem 7 (soundness) improving flipping sequence CP-net N
outcome o0 , N |= o0 o.
Proof: improving flip outcome o1 another outcome o2 N |=
o2 o1 definition |=. theorem follows transitivity preferential
entailment (Lemma 2).
Theorem 8 (completeness) N acyclic CP-net improving flipping
sequence N outcome o0 , N 6|= o0 o.
Proof: Let G graph whose nodes outcomes (i.e., complete assignments
variable set V), directed edge o1 o2 iff improving flip o1 o2
sanctioned network N . Clearly, directed paths G equivalent improving flipping
sequences respect N .
Next, show total preference ordering respects paths G (that
is, path o1 o2 G, o2 o1 ) satisfies network N :
satisfy N , must exist variable X parents U, instantiation u Asst(U),
158

fiCP-Nets

values x, x0 Dom(X), instantiation remaining variables = V (U {X}),
that:
(a) uxy ux0 y;
(b) CPT(X) dictates x0 x given u.
direct consequence definition satisfaction. However, N requires
x0 x given u, direct flip xuy x0 uy, contradicting fact extends
graph G.
Based observation, prove theorem: improving
flipping sequence o0 , directed path G o0 . Therefore,
exists preference ordering respecting paths G o0 .
preference ordering also satisfies N , implies N 6|= o0 o.
4.3 Flipping Sequences Plans
Searching flipping sequences seen type planning problem: given CPnet N , variable X parents Pa(X) N , row CP (X) conditional
preference statement form
u : x1 x2 xd
u Asst(Pa(X)), = |Dom(X)|. statement converted set
planning operators improving value X. particular, conditional preference
statement converted set d1 planning operators form (for 1 < d):
Preconditions: u xi
Postconditions:
Delete list: xi
Add list: xi1
corresponds action improving xi xi1 context u. (An inverse
set operators would created worsening sequences).
Given query N |= x y, treat start state x goal state
planning problem. readily apparent query consequence CP-net
plan associated planning problem, since plan corresponds
flipping sequence.
planning problem multi-valued variables discrete, finite domains known
pspace-complete (Backstrom & Nebel, 1995), remains pspace-complete
assumption variables binary (Bylander, 1994) (i.e., planning problems
strips formalism negative effects). However, upper bound informative
respect dominance queries, since planning problems generated
generally look quite different form standard AI planning problems,
many actions, action directed toward achieving particular proposition
requires specific preconditions. Thus dominance queries respect binaryvalued CP-nets correspond specific class strips planning problems, complexity
159

fiBoutilier, Brafman, Domshlak, Hoos & Poole

recently analyzed Domshlak Brafman (2002b, 2003). explain
relationship.
First, divide preconditions every operator planning problem two types:
prevailing conditions, variable values required prior execution
operator affected operator, preconditions, affected
operator. Second, introduce notion causal graph (Knoblock, 1994),
directed graph whose nodes stand problem variables. edge (X, ) appears
causal graph operator changes value prevailing
condition involving X.
complexity analysis Brafman Domshlak (2003) addresses planning problems
binary variables, unary operators (i.e., operators affect single variable),
acyclic causal graphs. planning problem generated dominance query
respect binary-valued CP-net, have:
1. operators unary, flip improves value single variable;

2. causal graph problem exactly graph CP-net, since values
Pa(X) required value flip X exactly prevailing conditions
corresponding planning operator.
Therefore, computational analysis dominance queries binary-valued acyclic
CP-nets use results techniques Brafman Domshlak (2003).
4.4 Complexity Dominance Queries Binary-valued, Acyclic CP-nets
section analyze complexity dominance testing respect binaryvalued CP-nets, showing connection structure CP-net graph
worst-case complexity dominance queries. particular, show that:
binary-valued CP-net forms directed tree, complexity dominance
testing quadratic number variables.
binary-valued CP-net forms polytree (i.e., induced undirected graph
acyclic), dominance testing polynomial size CP-net description.
binary-valued CP-net directed-path singly connected (i.e.,
one directed path pair nodes), dominance testing np-complete.
problem remains hard even node in-degree network bounded low
constant.
Dominance testing binary-valued CP-nets remains np-complete number
alternative paths pair nodes CP-net polynomially bounded.
exact complexity dominance testing multiply connected, binary-valued, acyclic
CP-nets remains open problemat stage clear whether problem
np harder.
follows, make assumption number parents variable
(i.e., node in-degree CP-net) bounded constant. assumption
160

fiCP-Nets

justified CPTs part problem description, size CP (X)
exponential |Pa(X)|.
4.4.1 General Properties
start notation two useful lemmas. First, given CP-net N pair
outcomes o, o0 respect N , improving flipping sequence F o0
called irreducible subsequence F 0 F obtained deletion entries except
endpoints o, o0 F improving flipping sequence.8
Given CP-net N , let F set irreducible improving flipping sequences
among outcomes. denote MaxFlip(Xi ) maximal number times variable
Xi changes value flipping sequence F. Formally, let Flip(F, Xi ) number
value flips Xi flipping sequence F . Then,
MaxFlip(Xi ) = max {Flip(F, Xi )}
F F

Lemma 9 formalizes first observation irreducible flipping sequences
respect binary-valued CP-nets.
Lemma 9 every variable Xi binary-valued CP-net N , have:
MaxFlip(Xi )

X

1 +

MaxFlip(Xj )

Xj :Xi Pa(Xj )

Proof: Let F irreducible flipping sequence respect N (from outcome
o0 outcome o), MaxFlip(Xi ) = Flip(F, Xi ). Consider subsequence
F 0 = f1 , f2 , . . . , fk F consists value flips children Xi N . Observe
that: (i) every fl F 0 requires Xi take one two possible values; (ii) value
flip F F 0 depends value Xi .
Now, 1 l < k, fl fl+1 require value Xi , value
flips Xi F fl fl+1 : flips, simply redundant,
contradicts assumption F irreducible. (Recall fl fl+1 adjacent
F 0 , may separated several flips original sequence F .) Alternatively,
fl fl+1 require different values Xi , due irreducibility F , exactly
one value flip Xi F fl fl+1 . Similarly show
one value flip Xi F f1 , one value flip Xi F
fk . latter flip necessary fk requires Xi take value o[Xi ], thus,
supporting immediate successors, Xi still flip value once, order obtain
value required o.
implies that:
MaxFlip(Xi ) = Flip(F, Xi ) 1 +

X

Flip(F, Xj )

Xj :Xi Pa(Xj )

8. Note removing proper initial final subsequence F results valid flipping sequence.
refer deletion arbitrary elements sequence, excluding endpoints.

161

fiBoutilier, Brafman, Domshlak, Hoos & Poole

thus, definition MaxFlip, have:
X

MaxFlip(Xi ) 1 +

MaxFlip(Xj )

Xj :Xi Pa(Xj )


Adopting terminology Domshlak Shimony (2003) Shimony Domshlak
(2002), directed acyclic graph G directed-path singly connected if, every pair nodes
s, G, one directed path t. Using Lemma 9, prove
binary-valued CP-net forms directed-path singly connected DAG then, every
variable Xi , MaxFlip(Xi ) bounded n (the number variables).
Lemma 10 binary-valued CP-net N forms directed-path singly connected DAG, then,
every variable Xi N , have:
MaxFlip(Xi ) n
n number variables N .
Proof: follows, denote MaxFlip(Xi ) N MaxFlipN (Xi ). proof induction n. n = 1 obvious MaxFlip(X1 ) 1. Assume binary-valued,
directed-path singly connected CP-net N consists n 1 variables, then, every Xi N ,
MaxFlipN (Xi ) n 1
Let N 0 binary-valued, directed-path singly connected CP-net n variables.
Without loss generality, let variables {X1 , . . . Xn } N 0 topologically ordered
based (acyclic) graph N 0 . Clearly, Xn leaf node, denote N
CP-net obtained removing Xn N 0 . Lemma 9, have:
MaxFlipN 0 (Xn ) 1
Observe directed paths predecessors Xn N 0 , since N 0
assumed directed-path singly connected. Therefore, Lemma 9, parent Xi
Xn N 0 , have:
MaxFlipN 0 (Xi ) MaxFlipN (Xi ) + MaxFlipN 0 (Xn )
thus:
MaxFlipN 0 (Xi ) MaxFlipN (Xi ) + 1
Generally, since N directed-path singly connected, variable Xi N 0 ,

MaxFlipN (Xi ) + 1, path Xi Xn
MaxFlipN 0 (Xi )
MaxFlipN (Xi ),
otherwise
thus, Xi N 0 , have:
MaxFlipN 0 (Xi ) n

162

fiCP-Nets

TreeDT (JN |= o0 K)
Initialize variables V outcome o0 .
Loop:
Iteratively remove leaf variables V assigned
values o.
V = , return yes.
Find variable X s.t. value improved, value
descendants N improved, given current assignment V.
variable found, return no.
Otherwise, change value X.
Figure 10: Flipping sequence search algorithm binary-valued, tree-structured CP-nets

4.4.2 Tree-structured CP-nets
start presenting flipping sequence search algorithm class binary-valued,
tree-structured CP-nets, prove correctness. show time complexity
algorithm O(n2 ), show actually lower bound flipping-sequence
search binary-valued, tree-structured CP-nets.
Figure 10 presents TreeDT algorithm binary-valued, tree-structured CP-nets.
Informally, TreeDT starts initializing variables V values (purported) less preferred outcome o0 , continues incremental, bottom-up conversion
initial assignment assignment induced (purported) preferred outcome o. step starts iteratively removing N leaf variables (i.e., maximal
fringe canopy) already consistent o. iteration step removes
variables N , variables assigned values o, thus required
improving flipping sequence o0 generated.
Otherwise, let N 0 stand updated N (with nodes removed). node X N 0
candidate variable if: (i) value X flipped; (ii) descendant X N 0
value flipped, given current assignment variables N 0 . flip
value arbitrary candidate variable (if one exists) repeat node removal;
report improving flipping sequence o0 candidate
variables.
TreeDT algorithm deterministic backtrack-free. show TreeDT
complete binary-valued, tree-structured CP-nets, generates irreducible flipping sequencesthus time complexity TreeDT O(n2 ). fact generates
irreducible flipping sequences ensures soundness (since generating valid flipping
sequences provide correct positive answers dominance queries).
Theorem 11 algorithm TreeDT sound complete binary-valued, tree-structured
CP-nets.
Proof: Consider execution TreeDT dominance query N |= o0 respect
binary-valued, tree-structured CP-net N .
163

fiBoutilier, Brafman, Domshlak, Hoos & Poole

First, suppose iteratively remove tree leaf variables values
required target outcome o. easy see affect
completeness algorithm: N acyclic, variables fringe
ancestors variables. Hence, value variables fringe
influence ability flip values variables (hence remove
improving flipping sequence consideration).
Second, consider variable X that, iteratively removing variables above,
value improved, yet none descendents N improved, given
current assignment v V. X leaf node, changing value influence
ability flip values variables. addition, current value v[X] X
different o[X], otherwise X would part removed fringe. Therefore,
(improving) change Xs value point necessary improving flipping
sequence. Alternatively, suppose X leaf node. Since leaf nodes
subtree N rooted X part removed fringe, (at least) values
remain changed. N tree, X completely separates descendents N
variables; improving flip subtree X possible
change value X. Hence value X must changed flipping sequence
point value descendent X.
remains shown several candidate variables
flipped, matter one flip first. one candidate
variable, one flipped pointeach flip necessary
flipping children corresponding variable. However, changes made one
candidates affect candidates descendants.
Thus, evaluation order candidate flips irrelevant, cannot prevent us
finding flipping sequence one exists.
Thus algorithm complete. soundness algorithm clear
proof well. flip generated step algorithm valid improving flip
given current outcome v.
Theorem 12 time complexity flipping-sequence search binary-valued, treestructured CP-nets O(n2 ), n number variables CP-net.
Proof: Since algorithm TreeDT backtrack-free, thing remains
shown addition Lemma 10 Theorem 11 (on binary-valued, tree-structured
CP-nets) TreeDT generates irreducible flipping sequences. However, proof
subclaim straightforward since already shown that:
1. TreeDT flips value variable X either achieve value X
(purported) preferred outcome query, allow required value flips
descendants X,
2. role latter flips X cannot fulfilled value flips variables.
Therefore, given improving flipping sequence F generated TreeDT, removing
subset value flips F makes F either illegal, ends (o0 , o), respectively.
Hence, improving flipping sequence generated TreeDT algorithm irreducible.

164

fiCP-Nets

Theorem 13 shows even limiting chain binary-valued CPnets, dominance queries whose minimal flipping sequences quadratic length.
Thus TreeDT asymptotically optimal.
Theorem 13 (n2 ) lower bound flipping-sequence search binary-valued
tree-structured CP-nets.
Proof: proof see Appendix A. proof example, providing dominance
query binary-valued, tree CP-net size minimal flipping sequence
(n2 ).

4.4.3 Polytree CP-nets
DAGs cycles underlying undirected graphs, also known polytrees, offer
minimal extension directed trees. Unfortunately, TreeDT procedure complete
polytree CP-nets unless extended form backtracking (even restricted
binary variables). due fact several parents given node may
allowed values flipped, one choices may lead target
outcome others lead dead end. instance, consider CP-net N
Figure 9 query N |= abc abc. Starting outcome abc, first iteration
TreeDT, choice flipping either B. B chosen, assignment
changed abc. However, cannot lead target, since way flip B
back b. Thus, dead end reached. hand, chosen TreeDT
successfully generate improving flipping sequence abc abc abc. Thus, incorrect
choice improving variable may require backtracking.
However, dominance testing binary-valued, polytree CP-nets remains polynomial
time, although algorithm solution complicated TreeDT. algorithm
adapted corresponding algorithm planning problems binary variables,
unary operators, polytree causal graphs described Domshlak Brafman (2003).
Theorem 14 Dominance testing binary-valued, polytree CP-nets p.
Proof: According reduction CP-net dominance queries classical
planning problems (see Section 4.3), every dominance query respect binary-valued,
polytree CP-net compiled propositional planning problem unary operators
polytree causal graph. algorithm latter problem presented Brafman
Domshlak (2003), time complexity algorithm O(22 n2+3 ),
maximal in-degree causal graph.
Recall assumption in-degree CP-net bounded constant (in
case, ). assumption justified CPTs part problem description,
size CP (X) exponential |Pa(X)|. Therefore, complexity
algorithm Brafman Domshlak (2003) CP-nets polynomial size CPnet.
165

fiBoutilier, Brafman, Domshlak, Hoos & Poole

4.4.4 Intractable dominance queries
dominance testing binary-valued, polytree CP-nets polynomial, show
binary-valued, directed-path singly connected CP-nets, problem np-complete.9
proved using CP-net-oriented extension proof corresponding
claim respect planning problems (Brafman & Domshlak, 2003). results also
entail dominance testing binary-valued CP-nets remains np number
distinct paths pair nodes CP-net polynomially bounded.
Theorem 15 Dominance testing binary-valued, directed-path singly connected CP-nets
np-complete.
Proof: proof see Appendix A.
immediate extension directed-path singly connected DAGs max--connected
DAGs: directed graph max--connected number different directed paths two nodes graph bounded .
Theorem 16 Dominance testing binary-valued, max--connected CP-nets,
polynomially bounded size CP-net, np-complete.
Proof: theorem immediately entailed corresponding result planning
Brafman Domshlak (2003); proof terms CP-nets, see Appendix A.

Theorem 15 implies dominance testing binary-valued, acyclic CP-nets hard.
However, exact complexity problem still open questionit clear
whether problem np harder. preliminary analysis problem (Domshlak & Brafman, 2002a) shows connection complexity flipping sequence search binary-valued, acyclic CP-nets diameters specific
graphs. complementary result respect graphs (Domshlak, 2002b), namely recursively directed hypercubes, shows dominance queries respect binary-valued,
acyclic CP-nets unbounded node in-degree may require flipping sequences size exponential n, n number variables CP-net.
shown general class planning problems binary variables,
unary operators acyclic causal graphs harder np (Brafman & Domshlak, 2003).
However, result imply answering dominance queries harder np
well, know reduction class planning problems CP-nets.

5. Search Techniques Dominance Queries
previous section showed dominance testing generally hard even binaryvalued CP-nets, tractable algorithms exist specific problem classes. However, CP-nets impose rich structure preferences exploited various search
strategies heuristics often significantly reduce search effort, allow effective solution many problem instances. section discuss search flipping
9. every polytree directed-path singly connected, converse true.

166

fiCP-Nets

sequences several rules allow significant pruning search tree without impacting soundness completeness search procedure. rules described
context improving flipping sequences, applied worsening search
mutatis mutandis.
Given CP-net N , outcome o, define improving search tree o, (o),
follows: (o) rooted o, children every node o0 (o) outcomes
reached single improving flip o0 . easy see every rooted path
(o) corresponds improving flipping sequence outcome (with respect
N ), vice versa.10 example, consider preference graph shown Figure 11(a),
induced CP-net Figure 4. Figure 11(b) depicts improving search
tree (abc) respect preference graph. Clearly, treat every dominance
query N |= o0 searching (o0 ) node associated outcome o, starting
root node o0 . instance, example above, given dominance query
N |= abc abc, dotted paths shown Figure 11(c) bring us outcome abc.
However, taking different direction tree traversal would lead dead end,
requiring backtracking order find suitable flipping sequence. generic search
algorithm used traverse improving search tree find improving sequence
supports dominance query.
5.1 Suffix Fixing
Suffix fixing rule allows certain moves (o) pruned search
tree without impacting completeness search. Let N CP-net variables
V = {X1 , . . . , Xn }, numbered according arbitrary topological ordering consistent
network N . define rth suffix outcome Asst(V), r 1,
subset outcome values o[Xr ]o[Xr+1 ] o[Xn ]. Notice rth suffix
outcome depends topological ordering variables used. Finally, say rth
suffixes outcomes o0 match iff o[Xj ] = o0 [Xj ] r j n.
Let node improving search tree (o0 ), let target search;
words, attempting find flipping sequence proves N |= o0 .
suffix fixing rule requires rth suffix matches, value r,
neighbors explored whose rth suffix also match o. equivalent
ruling exploration flipping sequences destroy suffix outcome
matches target outcome o. following lemma ensures pruning branches
search tree (o0 ) searching path affect completeness.
Lemma 17 Let N CP-net, outcomes whose rth suffix matches (for
topologically consistent ordering X1 , , Xn variables N ). path
(o ) root o, path root every outcome
path assigns values Xr , Xn (i.e., suffix match).
Proof: proof straightforward: N acyclic, suffix variable ancestor
non-suffix variables. Hence, value suffix variables influence (for
better worse) ability flip values remaining variables.
10. corresponding worsening search tree defined similarly using worsening flips.

167

fiBoutilier, Brafman, Domshlak, Hoos & Poole

abc



abc



abc
tt
zttt

abc

abc E
EE

EE
yy

E"


y|
abc

abc






abc

abc

abc
abc



abc

DD
DD
"






z
zz

z| z

abc


tt
tt

z


abc

DD
DD
"

abc

abc

abc

abc

abc

abc

abc


abc



abc KK



KKK
K%

abc




abc


abc

abc
(a)

(b)

abc

abc

yy
yy

|


abc

abc






abc

"

abc

abc

abc
"

abc




abc

abc

abc


abc

abc
"



abc

yy
yy
y|

abc

abc

abc

abc

z
zz

|z
z


abc


abc




abc


abc

(c)

(d)

Figure 11: (a) preference graph induced CP-net; (b) improving search tree
(abc); (c) paths outcome abc; (d) pruning (abc) suffix fixing.

168

fiCP-Nets

Suppose, given query N |= o0 , path (o0 ) outcome type
mentioned lemma above. Since subtree (o0 ) rooted (o ),
assured path o0 passes , path o0
passes suffix preserved subpath o.
conclude:
Proposition 18 complete search algorithm improving search tree remains complete pruning using suffix rule used.
suffix fixing rule effectively prunes search tree node described
contain paths retain suffix values target o. Though backtracking
choices lead may required, may choices preserve
suffixes, consideration full search tree required (if
nontrivial suffix match). example, improving search tree (abc) query
N |= abc abc discussed above, pruned using suffix fixing rule variable ordering
A, B, C, appears Figure 11(d). see, pruning dramatically reduce
size effective search tree.
5.2 Least-Variable Flipping
extension suffix fixing rule least-variable flipping rule, defined follows.
Suppose CP-net N , query N |= o0 . Let outcome,
variable Xj , let u denote instantiation U = Pa(Xj ) . say Xj
least-improvable variable value x Dom(Xj ) x ju [Xj ],
descendent Xj N property. Intuitively, least-improvable variable
one lowest N flipped produce outcome preferred
. Naturally, may several variables. say Xj least improvable
respect target iff Xj least improvable variable among part suffix
match o. words, suffix matches, apply definition
least-improvable variable, restricting attention variables part
matching suffix. least-variable flipping rule requires neighbors node
expanded search improving sequence target
least improvable variable respect improved.
following lemma ensures binary-valued, directed-path singly connected
CP-nets, pruning improving search tree using least-variable rule affect
completeness search procedure.
Lemma 19 Let N binary-valued, directed-path singly connected CP-net,
outcomes whose rth suffix matches (for topologically consistent ordering X1 , , Xn
variables N ). Let {o1 , . . . , om } (m n) set outcomes reachable
via least-variable flip affect matched suffix. path
(o ) root o, path os (s m) o.
Proof: Without loss generality based earlier observations, assume
suffix match (if so, restricting attention set non-suffix variables
affect argument).
169

fiBoutilier, Brafman, Domshlak, Hoos & Poole

assume, contradicting statement theorem, none least-variable
flips os path o, exist path o. implies
none least-variable flips involve leaf variables network N . Otherwise,
could flip value leaf variable without effect ability flip
variables, thus able construct path passes
os flip leaves.
Now, consider leaf variable Xi N . Since dealing binary variables,
irreducible flipping sequence value Xi flipped exactly
once. However, current assignment [Pa(Xi )] allow us perform flip
(see observation above). Thus, must achieve another assignment Pa(Xi )
flip value Xi , making part suffix match.
Let NXi subnetwork N induced Xi ancestors Xi N .
N directed-path singly connected, NXi forms tree, directed toward root Xi .
reduce NXi removing subtrees NXj NXi , variable NXj
flipped . Note step cause loss generality, since (due acyclicity
N ) assignment variables NXj cannot flipped first improving
flipping sequence beginning . Likewise, let Xi1 , . . . , Xim0 , m0 m, variables
corresponding least-variable flips NXi , and, 1 k m0 , let ik
(single) directed path
Xik
Xi .
Sm0
Let = j=1 NXij \ {Xij } . NXi tree, 1 j m0 , variable Xij
separates variables NXij \ {Xij } paths i1 , . . . , im0 . Therefore:
(i) set flips variables enable us flip values variables
i1 , . . . , im0 ; particular flip Pa(Xi ) cannot enabled. Thus, enable flip
Xi , eventually flip value least one variable Xi1 , . . . , Xim0 .
(ii) value flip Xi1 , . . . , Xim0 affect (neither positively negatively) ability
flip values variables . Thus, improving flipping sequence
o, least one sequence starts value flip variable
Xi1 , . . . , Xim0 .
last observation contradicts assumption flipping sequence
pass least one os .
least-variable flipping rule distinguish flips different candidate least
improvable variables; simply restricts flips variables. general, leastvariable flips suitablesome may lead dead-ends, requiring backtracking (a point
illustrated Section 4.4.3). However, backtrack, need consider
least-variable flips, flips, thus significantly reducing size search tree
expected amount backtracking. observe TreeDT algorithm binary-valued,
tree-structured CP-nets essentially implements least-variable flipping rule, is,
therefore, complete backtrack-free search procedure binary-valued, tree-structured
networks.
Examples 7 8 show Lemma 19 presents probably widest class CPnets least-variable flipping rule complete: Example 7 shows leastvariable flipping rule preserve completeness binary-valued, max-2-connected
170

fiCP-Nets

CP-nets, Example 8 shows multi-valued, directed-path singly connected
CP-nets. Note CP-net Example 8 forms chain. Therefore, binary-valued, treestructured CP-nets probably widest class CP-nets least-variable
flipping rule complete backtrack-free.
Example 7 Consider binary-valued CP-net Figure 12, Figure 12(a) depicts
graph CP-net, Figure 12(b) shows corresponding CPTs. Given query
work improving tree rooted abcde.

N |= abcde abcde,
least
improvable variable flipped root outcome B (while E
flipped, least improvable). Unfortunately, flipping B value b leads
target abcde unreachable:
outcome abcde,
(i) order reach target value variable D, first achieve
assignment b c variables B C;
(ii) Achieving assignment b c flip question b b (given e)
possible restoring value b B, requires e;
(iii) Flipping value B b b given e lead us situation
longer flip B, preventing us achieving target value b.
Thus least-variable flipping rule allow discovery improving flipping
sequence.
abcde proves
hand, flipping sequence abcde
query:
abcde
abcde
abcde abcde abcde
abcde
sequence requires flipped root, despite fact
least-improvable variable.
Example 8 Consider chain CP-net three variables A, B, C,
parent B, B parent C. Suppose domain {a, a}, B domain
{b1 , b2 , b3 }, C domain {c, c}, following conditional preferences:
a;
: b 3 b2 b1 ;
: b3 b1 b2 ;
b1 : c c b2 : c c; b3 : c c
Consider query N |= ab3 c ab1 c. value c cannot improved context b1 ,
b1 improved b3 context a; fact least-improvable
variable flip outcome ab1 c. Unfortunately, flip leads outcome ab3 c
path target ab3 c exists. contrast, flipping non-least-improving variable
first allows discovery successful improving path: flipping a, change
b1 b2 , c c, finally b2 b3 .
171

fiBoutilier, Brafman, Domshlak, Hoos & Poole

89:;
?>=<
?>=<
89:;
A>
E
>>
>>
>>
>>
>>
>

89:;
?>=<
89:;
?>=<
C/
B
//


//

//


//

89:;
?>=<


(a)

Variable


CPT


B

e : b b
e : b b

C

: c c
: c c



c b :
c b :

E

e e
(b)

Figure 12: multiply connected CP-net least-variable flipping rule causes
incompleteness.

Though multiply-connected networks, networks multi-valued variables,
least-variable flipping rule complete, believe provide useful heuristic
guidance cases. least-variable-first heuristic heuristic ordering children
improving search treeit requires expanding node (with respect
target o), children corresponding least-improving variables explored first.
typically reduce number nodes expanded tree, leastvariable-first heuristic viewed embodying form least commitment. Flipping
values least-improving variable seen leaving maximum flexibility flipping
values variables. upstream variable limits possible flipping sequences
drastically downstream variable since altering variable limit
ability flip values non-descendants.
multivalued networks, least commitment strategy used extend
least-variable-first heuristic using least improving rule: alternative improving flips
least-improvable variable considered least improving flip
improving flip (i.e., first flip leads least preferred improving value adopted).
allows greater flexibility movement downstream variables. one
always improve value variable question less preferred value
preferred value (provided parent values maintained), skipping values may
prevent us setting descendants desired values. fact, illustrated
Example 8, where, crucial flip variable A, using least improving rule
heuristic leads directly target outcome.
172

fiCP-Nets

5.3 Forward Pruning
search procedure improving flipping sequence, matter whether procedure
adopts heuristics not, one use general forward pruning technique.
technique number desirable properties:
(a) often quickly shows flipping sequence possible;
(b) prunes domains variables reduce flipping search space;
(c) doesnt compromise soundness completeness;
(d) relatively cheap: time complexity O(nrd2 ), n number variables,
r maximum number conditional preference rules variable,
size largest variable domain.
general idea sweep forward network, pruning values variable
cannot appear improving flipping sequence given query. Intuitively,
consider set flips possible, ignoring interdependence parents number
times parents change values.
consider variables order consistent network topology (so
parents node considered node). variable Xj parents
U, build domain transition graph nodes corresponding possible values
xi Dom(Xj ). conditional preference relation ju Dom(Xj ) form:
xu1 xu2 xud
u contains unpruned values parents U Xj , include directed
arcs successive values ordering ju (i.e., arc xi xi1 ,
1 < d).
answering query N |= o0 , prune value Xj
directed path o0 [Xj ] o[Xj ] domain transition graph Xj .
implemented running well-known Dijkstras algorithm (Cormen, Lierson, & Rivest,
1990) twice: find nodes reachable o0 [Xj ] find nodes
reach o[Xj ]. sets nodes intersected find possible values Xj
along path o0 [Xj ] o[Xj ] (o0 ) (i.e., along improving sequence o0 [Xj ]
o[Xj ] respect N ). intersection empty, dominance query fails:
legal flipping sequence o0 [Xj ] o[Xj ]. often results quick failure
straightforward queries, carry search non-obvious cases.
Example 9 Consider CP-net N B binary root variables,
preferences values B b b. Given query
N |= ab . . . ab . . ., first consider A. domain transition graph consists
arc a, thus values pruned. example changed slightly
third value a, a, third value could pruned A, thus
simplifying tables children A.
consider B, whose domain transition graph consists single arc b b. Since
value B (purported) preferred outcome query (b) reachable
domain transition graph B value B (purported) less preferred
outcome query (b), query fails quickly without looking variables.
173

fiBoutilier, Brafman, Domshlak, Hoos & Poole

6. Incompletely Specified Preferences Indifference
many practical applications, one expects see reluctance behalf user provide
complete CPTs totally order values variable every possible context.
Thus, natural ask results techniques applied cases.
turns results presented paper, except linear time procedure ordering queries, easily extended work CP-nets partially
specified CPTs, satisfiable CP-nets capture statements preferential indifference. instance, results presented Section 4.4 respect dominance queries
remain applicable is, almost results shown Brafman
Domshlak (2003) valid general setting classical planning. case
analyzed Domshlak Brafman case tree CP-nets. However,
correctness TreeDT procedure extended CP-nets easily verified,
complexity remains quadratic. stems fact Lemma 10 valid
general planning setting (Brafman & Domshlak, 2003). point requires
clarification difference partial specification indifference respect
flipping sequences: Given variable X parents U, two values x1 , x2 Dom(X),
x1 x2 equally preferred given u Asst(U), given u flip value
X x1 x2 , vice versa. Alternatively, x1 x2 incomparable given
u Asst(U), given u cannot flip value X x1 x2 , x2 x1 .
complexity dominance testing context indifference incompletely
specified CPTs remains open problem. However, Theorem 20 shows
flipping sequence search multi-valued CP-nets partially specified preferences
np, even CP-net forms chain variables three-valued only.
Theorem 20 Flipping sequence search multi-valued CP-nets partially specified
preferences np.
Proof: proof see Appendix A.
Now, consider outcome optimization queries. CPTs allowed partially
specified, statements indifference allowed, CP-net may induce
one nondominated outcome. instance, consider Example 1 (My Dinner I).
preference type wine given fish soup specified, decision maker considers
red white wine go equally well fish soup, CP-net induces two
nondominated outcomes: fish soup white wine, fish soup red wine.
forward sweep procedure outcome optimization queries presented Section 3
easily extended cases partial specification and/or indifference adding
branching variable X (already generated) assignment Pa(X)
induces one nondominated value Dom(X). complexity resulting
algorithm O(n), n number variables CP-net, number
nondominated outcomes induced CP-net. course, exponential
size CP-net.11 However, adapted forward sweep procedure anytime
propertythe solutions generated iteratively, time add new nondominated
11. tight upper bound = 2n shown CP-net simply leaving CPTs unspecified.

174

fiCP-Nets

outcome current set generated solutions O(n). Therefore, complexity
generating k nondominated solutions linear k.
Finally, important query answered efficiently standard CP-nets
outcome ordering query (see Section 4.1). Although basic Corollary 4 remains valid
case CP-nets partial specifications statements indifference,
case Theorem 5. Therefore, computational complexity ordering queries
extended CP-nets remains open question, conjecture problem hard.

7. Concluding Remarks
paper introduced CP-nets, new graphical model representing qualitative
preference orderings reflects conditional dependence independence preference
statements ceteris paribus semantics. formal framework offers compact
arguably natural representation preference information, allows us efficiently
answer principal forms preference queries.
described several types queries algorithms answering respect
specific CP-net. particular, outcome optimization outcome ordering queries
shown solvable time linear number variables network.
dominance queries, however, situation complicated. First, demonstrated
equivalence answering dominance queries task determining existence
improving (or worsening) sequence variable value flips respect given CP-net.
Then, reduced latter task special subclass classical planning problems.
insights allowed us show that, general, answering dominance queries np-hard,
polynomial algorithms exist tree polytree-structured, binary-valued CP-nets.
addition, presented several techniques one use generic search procedure improving flipping sequence. techniques shown
impact soundness completeness search CP-net, techniques
property binary-valued CP-nets. However, argued latter techniques modified general purpose heuristics likely reduce significantly
size expanded search tree.
Finally, analyzed applicability results CP-nets allow partially
specified preferences and/or capture statements indifference.
7.1 Applications
goal developing CP-nets formalism facilitate development applications.
One applicationpreference-driven, adaptive multimedia document presentationwas
recently developed Ben-Gurion University (Domshlak et al., 2001; Gudes et al., 2002).
described central components system Section 3.2. Another application
conceptual computational properties CP-nets seem useful distributed
meeting scheduler. basic prototype system implemented BenGurion University (Brafman & Domshlak, 2001). currently extending system,
working related theoretical issue multi-agent preference-based optimization.
Another potential application qualitative preferences general, thus CPnets particular, sorting product database according user-specified preferences.
problem highly relevant context electronic commerce. Several rather
175

fiBoutilier, Brafman, Domshlak, Hoos & Poole

conceptually simplistic, though quite interesting, commercial applications rely
unconditional preference statements available World Wide Web; examples include Active Sales AssistantTM (see www.activebuyersguide.com) PersonalogicTM (see
www.personalogic.com). general idea assist user selecting specific product
product database according preferences. Here, important use
compact natural representations preference information. CP-nets extend current
models (which typically dont allow conditional preference statements), yet offer efficiency
ordering given set alternatives. Another important aspect problem
given database precisely defines products (represented vectors attribute values) available, preference information required extent narrows
choice product sufficiently small selection products database.
graphical properties CP-nets underlying efficiency ordering queries,
various dominance testing strategies, exploited context find subset
products dominated product database, given (conditional) preference information extracted user. Here, interactive dynamic
approach appears promising, user prompted additional preference statements ordering products database sufficiently constrained
preference information offer reasonably small selection products.

Another growing application area CP-nets automated constraint-based product
configuration (Sabin & Weigel, 1998). task assemble number components
compose product given compatibility constraints satisfied. simple
example assembly components computer system where, instance,
type system bus constrains choice video sound cards.
wide growing body research modeling configuration problems efficient
problem solving methods, still need work modeling learning
user preferences, using achieve configurations feasible,
also satisfactory user point view. issues emphasized many papers
configuration (Freuder & OSullivan, 2001; Haag, 1998; Junker, 1997, 2001; Soininen
& Niemela, 1998), especially high-level configurators specific, real-life domains
discussed (Haag, 1998). importance incorporating user preferences
configuration problem stems fact many problems weakly constrained
numerous feasible solutions (DAmbrosio & Birmingham, 1995). value
solutions, subjective point view particular user, may vary significantly.

CP-nets used represent user preferences used together
compatibility constraints search preferred, feasible configurations. contrast
database sorting application above, set possible vectors feature values
(i.e., configurations) explicitly given, implicitly specified compatibility
constraints. CP-net based search algorithm Boutilier et al. (1997) specifically
designed address problem. description algorithm, well analysis
computational properties refer reader Boutilier et al. (1997), Brafman
Domshlak (2001).
176

fiCP-Nets

7.2 Related Work
number lines research related CP-nets. addition conceptual work
philosophy philosophical logic described Section 2, AI, Doyle Wellman (1991,
1994) explored ceteris paribus assertions logical properties. However, work
exploit notions preferential independence, particular considered
graphical representations preference statements. best knowledge,
computational results known formalism. Therefore, clear whether useful
queries answered efficiently framework.
surface, CP-nets reminiscent Bayesian networks (Pearl, 1988),
also graphical structures capturing conditional independence assertions. Indeed, Bayesian
networks utilization probabilistic independence provide important motivation
work, two structures differ considerably properties type
information present.
Motivated considerations driving work, Bacchus Grove (1995)
La Mura Shoham (1999) study different notions independence associated
graphical representations. representations allow quantitative assessments, unlike
CP-nets (at least current form) differ CP-nets precise nature
independence concept studied. particular, Bacchus Grove concentrate notion
conditional additive independence. Additive independence strong property,
requiring utility outcome sum utilities different variable
values outcome. Conditional additive independence weaker requirement,
thus promising practice. Bacchus Grove show conditional additive
independence properties domain captured undirected graph set
nodes A, B, C, independent B given C C separates nodes
B. La Mura Shoham (1999) define concept u-independence using ceteris paribus
comparison operator utilities. operator measures intensity preference
specific values certain variables given fixed value variables
respect fixed reference point. also define undirected graphical structure,
expected utility networks, u-independence represented using notion node
separation.
Finally, recent work Benferhat, Dubois Prade (2001) provides preliminary investigation potential possibilistic logic qualitative decision analysis, specifically qualitative preference representation. possibilistic approach takes utilities
account, well probabilities, provides qualitative approach decision problems
replacing numeric preferences probabilities linear orderings. discussion
approach utilities preference representation, see Benferhat et al. (2001), Dubois,
Godo, Prade, Zapico (1998b), Dubois, Berre, Prade, Sabbadin (1998a), Dubois,
Prade, Sabbadin (1997), related qualitative models, see Boutilier (1994), Tan
Pearl (1994).
7.3 Future Work
see number potential extensions work described paper. particular,
work leaves number interesting open theoretical questions. First, worst case
complexity dominance testing respect acyclic, binary-valued CP-nets needs
177

fiBoutilier, Brafman, Domshlak, Hoos & Poole

establishedit still open question whether problem np. Second,
complexity dominance queries respect multi-valued CP-nets remains open
problem. Third, clear hard ordering queries CP-nets capture
partial preference specification and/or statements preferential indifference. Finally,
need understand expressive power CP-nets better, specifically:
sort partial orderings representable CP-nets; among orderings
representable CP-nets, ones represented cyclic network.
Cyclic networks present another important challenge. networks arise applications natural notion neighborhood defined set variables
preferences one variable value depend value neighboring variables.
cases, user may find natural provide cyclic preference structure
must able determine whether specified network satisfiable. addition,
inference methods networks need developed or, alternatively, methods
reducing cyclic networks acyclic networks without significant blow-up size. number
recent papers started deal question, interesting insights
results (Domshlak, 2002a; Domshlak & Brafman, 2002a; Brafman & Dimopoulos, 2003;
Domshlak, Rossi, Venable, & Walsh, 2003).
working various extensions framework presented here. extensions include use conditional preference statements contain small amount
quantitative information. Existing applications (such online interactive consumer guides)
suggest limited amount quantitative preference information might relatively easy extract user natural way, useful inducing stronger
preference orderings. Preliminary work topic undertaken (Boutilier, Bacchus, & Brafman, 2001).
Another interesting extension CP-nets TCP-nets (Brafman & Domshlak, 2002).
TCP-nets add importance relations conditional relative importance statements
conditional ceteris paribus statements supported CP-nets. Conditional importance
statements form = optimizing B important
optimizing C. example, flying night, better seat
important getting preferred carrier.
Finally, preference-based optimization presents interesting tradeoff amount
user interaction required extracting preference information amount computation needed determine preferred feature vectors. asking specific
questions particular, potentially complex preferences, finding preferred feature
vectors become much easier. hand, asking many questions, especially
really necessary establishing relevant preferences, annoy user make
system less usable. Thus, finding good tradeoffs amount user-interaction
computation time answering queriessuch finding preferred items
database optimal configurationsseems promising direction future research.
related motivation underlying goal programming (Dyer, 1972; Ignizio, 1982).
structure CP-nets exploited determining preference elicitation strategies.
explored context CP-nets quantitative information (Boutilier
et al., 2001); remains seen use techniques purely qualitative setting.
178

fiCP-Nets

Acknowledgments
parts paper appeared C. Boutilier, R. Brafman, H. Hoos, D. Poole,
Reasoning Conditional Ceteris Paribus Preference Statements, Proceedings
Fifteenth Conference Uncertainty Artificial Intelligence, pp.7180, Stockholm, 1999;
C. Domshlak R. Brafman, CP-netsReasoning Consistency Testing, Proceedings Eighth International Conference Principles Knowledge Representation
Reasoning, pp.121132, Toulouse, 2002. Thanks Chris Geib contributions
earlier, related models CP-nets, anonymous referees many useful
suggestions. Boutilier, Hoos Poole supported Natural Sciences Engineering Research Council, Institute Robotics Intelligent Systems. Brafman
acknowledges support Paul Ivanier Center Robotics Research Production
Management.

Appendix A.
Theorem 13 (n2 ) lower bound flipping-sequence search binary-valued,
tree-structured CP-nets.
Proof: proof example dominance testing query binary-valued, tree
CP-net size minimal flipping sequence (n2 ).
Consider following CP-net N defined binary variables {X1 , . . . , Xn },
n = 2k + 1 k N. domain variable Xi D(Xi ) = {xi , xi }.
1 n, Pa(Xi ) = {Xi1 }, thus N forms directed chain. CPTs capturing
preferences values {X1 , . . . , Xn } shown Figure 13(a).
consider dominance testing query JN |= o0 K, where, 1 n:
o[Xi ] = xi

xi , = 2m,
0
[Xi ] =
xi , = 2m + 1

mN

length minimal (and actually only) flipping sequence o0 k2 +2k+1,
proving required lower bound. jth flip sequence changes value
variable Xk+1+ , where:
= bj/(k + 1)c
= (j 1) mod (k + 1)
Informally, first k + 1 flips change values Xk+1 , . . . , Xn (in order), next k + 1
flips change values Xk , . . . , Xn1 , Xk1 , . . . , Xn2 , etc. Finally, last k + 1
flips change values X1 , . . . , Xk+1 . k + 1 sets, k + 1 flips each,
length flipping sequence thus k2 + 2k + 1.
Figure 13(b-c) illustrates o0 , corresponding flipping sequence k = 2
(i.e., n = 5). Figure 13(c), variable Xi annotated value flips along
flipping sequence: Arrows values stand value flips, sequential
numbers flips along flipping sequence appear arrow labels.
179

fiBoutilier, Brafman, Domshlak, Hoos & Poole

89:;
?>=<
X1

x1

7

/ x1


?>=<
89:;
X2

x2

4

/ x2

8

/ x2


?>=<
89:;
X3

x3

1

/ x3

5

/ x3

x4

2

/ x4

6

(a)


?>=<
89:;
X4

/ x4

= x1 x2 x3 x4 x5
o0 = x1 x2 x3 x4 x5


?>=<
89:;
X5

x5

3

/ x5

Variable

CPT

i=1

xi

1<ik+1

k+1<in

x0i

xi1

:

xi xi

xi1

:

xi xi

xi1

:

xi xi

xi1

:

xi xi

(b)

9

/ x3

(c)

Figure 13: Illustration proof Theorem 13: (a) CPTs n variables; (b) o0
k = 2; (c) flipping sequence k = 2.

prove size minimal flipping sequence example (with
n = 2k + 1) k2 + 2k + 1. divide proof two steps, show minimal
flipping sequence o0 o:
1. 1 k + 1, every variable Xk+i (last k + 1 variables) must change value
least k + 2 times,
2. 1 j k, every variable Xj (first k variables) must change value least j
times.
(1) proof first claim induction i. = k + 1, variable X2k+1 (i.e.,
Xn ) must change value least since o[X2k+1 ] 6= o0 [X2k+1 ].
Now, assume induction hypothesis that, l k + 1, every variable Xk+i
must change value least k + 2 times, prove claim Xk+l1 . Recall
Xk+l1 parent Xk+l , thus every pair successive flips Xk+l must require
different values Xk+l1 .
Assume k + l even. Then, first flip Xk+l xk+l xk+l , requiring
Xk+l1 take value xk+l1 o0 [Xk+l1 ] = xk+l1 . Therefore, first flip Xk+l
may performed first flip Xk+l1 , therefore, flip number
k + 2 l Xk+l , variable Xk+l1 change value least k + 2 l times.
Now, since k + l even, k + 2 l. Thus, k + 2 l flips, variable Xk+l1
assigned value o0 [Xk+l1 ] = xk+l1 . However, o[Xk+l1 ] = xk+l1 , thus Xk+l1
change value least one time.
proof case k + l odd similar: first flip Xk+l xk+l xk+l ,
requiring Xk+l1 take value xk+l1 o0 [Xk+l1 ] = xk+l1 . Therefore,
180

fiCP-Nets

flip number k+2l Xk+l , variable Xk+l1 change value least k+2l
times. Now, since k + l odd, k + 2 l. Thus, k + 2 l flips, variable
Xk+l1 assigned value xk+l1 . However, o[Xk+l1 ] = o0 [Xk+l1 ] = xk+l1 ,
thus Xk+l1 change value least one time. Hence, proved that,
= l 1, Xk+i change value least k + 2 (= k + 3 l) times.
(2) proof second claim induction j. First, show Xk must change
value least k times. first claim Xk+1 changes value least
k + 1 times. Therefore, since every pair successive flips Xk+1 requires different values
Xk , variable Xk must change value least k times.
Now, assume induction hypothesis that, l j k, every variable Xj must
change value least j times, prove claim Xl1 . Again, since every pair
successive flips Xl requires different values Xl1 , variable Xl1 must change
value least l 1 times.
Theorem 15 Dominance testing binary-valued, directed-path singly connected CP-nets
np-complete.
Proof: First show membership np. Given dominance query = JN |= o0 K,
let MinFS() denote size minimal (= shortest) improving flipping sequence o0
o. Using MaxFlip property variables, following upper bound MinFS()
straightforward Lemma 10:
X
MinFS()
MaxFlip(Xi ) n2
(2)
Xi V

Thus, guess minimal improving flipping sequence given solvable problem,
verify low order polynomial time.
proof hardness reduction 3-sat, i.e., problem finding satisfying
assignment propositional formula conjunctive normal form conjunct
(clause) three literals.
Let F = c1 . . . cn 3-sat propositional formula, x1 , . . . , xm variables
used F. equivalent problem constructed follows:
V = {V1 , V1 , . . . , Vm , Vm } {C1 , . . . , Cn }, 1 2m + n, domain
D(Xi ) = {f, t} (f stand false true, respectively).
P a(Vi ) = P a(Vi ) =
P a(Ci ) = {Vi1 , Vi1 , Vi2 , Vi2 , Vi3 , Vi3 }, xi1 , xi2 , xi3 variables
participate ith clause F.
Outcome o0 assigns f variables V.
Outcome assigns variables V.
variable Vi Vi , 1 m, value (unconditionally) preferred
value f . turn, 1 n, value preferred value f variable Ci
exist j, 1 j 3, that:
181

fiBoutilier, Brafman, Domshlak, Hoos & Poole

1. Vij 6= Vij
2. literal xij (and xij ) belongs clause ci Vij = t, otherwise Vij =
assignments Pa(Ci ), value f preferred value variable
Ci .
constructed inference problem directed-path singly connected, binary-valued
CP-net 1 n, |Pa(Xi )| 6. Let us show improving flipping
sequence o0 exists satisfying assignment F found.
Given satisfying assignment , improving flipping sequence follows: First,
1 j m, xj = , flip value variable Vj f t. Otherwise,
xj = f , flip value variable Vj f t. actual ordering
flips irrelevant since variables mutually independent. Then, 1 n, flip
value Ci f t. also, ordering flips significant. Finally,
flip values variables Vj Vj still value f .
Let improving flipping sequence o0 o. value variable Ci
flipped variables Vj , Vj Pa(Ci ) values t, f , respectively, variable
xj set true . Otherwise, Ci flipped variables Vj , Vj Pa(Ci )
values f, t, respectively, variable xj set false . Existence j
ensured construction CP (Ci ).
turn, 1 j m, neither Vj Vj achieve value f achieving
value t. Hence, outcome o1 {Vj = t, Vj = f } o1 ,
outcome o2 {Vj = f, Vj = t} o2 , vice versa. shows
value variable Ci flipped f context consistent
. Therefore, close ci , literal lij ci , 1 j 3, lij .
Theorem 16 Dominance testing binary-valued, max--connected CP-nets,
polynomially bounded size CP-net, np-complete.
Proof: prove theorem showing that, acyclic, binary-valued CP-net N ,
variable Xi N , have:
MaxFlip(Xi ) 1 +

n
X

(Xi , Xj )

(3)

j=i+1

(Xi , Xj ) denotes total number different, necessary disjoint, paths Xi
Xj . simplicity presentation, assume variables {X1 , . . . , Xn } N
numbered according topological order induced graph N , rewrite Eq. 3
into:
n
X
MaxFlip(Xi ) 1 +
(Xi , Xj )
(4)
j=i+1

proof induction i. = n obvious MaxFlip(Xn ) 1, since Xn
leaf node N . assume lemma holds > k, prove
= k. Without loss generality, assume exist least one variable Xj
Xk Pa(Xj ). Otherwise, simply MaxFlip(Xk ) 1.
182

fiCP-Nets

Denote succ(Xk ) immediate successors Xk N , i.e. Xik succ(Xk ) iff Xk
Pa(Xik ). proof straightforward:
MaxFlip(Xk )

Lemma 9



X

1 +

MaxFlip(Xik )

Xik succ(Xk )
I.H.



X

1 + |succ(Xk )| +

Xik succ(Xk )

=

1+

n
X

n
X

(Xik , Xj ) =

j=ik +1

(Xk , Xj )

j=k+1

Now, binary-valued CP-net max--connected, variables given problem
considered topological ordering induced N , Eq. 4 follows that,
variable Xi N , have:
MaxFlip(Xi ) n + 1

(5)

Let MinFS() denote size minimal (= shortest) improving flipping sequence
. Eq. 5 follows MinFS() n2 + n. Hence, polynomially bounded
size , guess verify minimal improving flipping sequence
polynomial time, thus class dominance testing queries np.
Theorem 20 Flipping sequence search multi-valued CP-nets partially specified
preferences harder np.
Proof: proof example dominance query multi-valued, chain CP-net
partially specified preferences, minimal flipping sequence exponentially
long.
Consider following CP-net N defined three-valued variables {X1 , . . . , Xn },
}.
n = 2k + 1 k N. domain variable Xi D(Xi ) = {xi , xi , x
1 n, Pa(Xi ) = {Xi1 }, thus N forms directed chain. CPTs capturing
preferences values {X1 , . . . , Xn } shown Figure 14.
consider dominance testing query JN |= o0 K, where, 1 n,
. Let sequence {ai } defined as:
o[Xi ] = xi o0 [Xi ] = x
(
2,
i=1
ai =
(6)
2ai1 + 2, 2
length minimal flipping sequence o0 greater than:
k
X

n

ai > 2 2

i=1

Figure 15 illustrates corresponding flipping sequence k = 2 (i.e., n = 5),
variable Xi annotated value flips along flipping sequence: Arrows
183

fiBoutilier, Brafman, Domshlak, Hoos & Poole

Variable
i=1

CPT

xi xi x

1<ik+1


xi1 : xi xi x
xi xi
xi1 : x
i1 : xi xi x

x

xi1 :

k+1<in


i1 :
x

xi xi

xi x
xi xi
xi
x

Figure 14:
89:;
?>=<
X1

1
x

10

/ x1

18

/ x1


?>=<
89:;
X2

2
x

3

/ x2

7

/ x2

11

/ x2

15

/ x2

19

/ x2

22

/ x2


?>=<
89:;
X3

3
x

1

/ x3

4

/ x3

8

/ x3

12

/ x3

16

/ x3

20

/ x3


?>=<
89:;
X4

4
x

2

/ x4

5

/ x4

9

/ x4

13

/ x4

17

/ x4

21

/ x4


?>=<
89:;
X5

5
x

6

/ x5

14

/ x5

23

/ x3

Figure 15:

values stand value flips, sequential numbers flips along flipping
sequence appear arrow labels.
prove
P size minimal flipping sequence dominance query
greater ki=1 ai . divide proof four steps, first step
P
shows necessity ki=1 ai flips flipping sequence o0 above,
subsequent three steps prove existence flipping sequence o0 o. Recall that,
since N forms chain, 1 < n, Xi1 parent Xi , thus value flips
Xi depend value Xi1 .
steps proof follows:

184

fiCP-Nets

(1) flipping sequence o0 o, 2 k + 1, every variable Xk+i (last k
variables) must change value least ak+2i times, f1k+i, . . . , fak+i

k+2i
corresponding sequence flips Xi , then, 1 j ak+2i , have:




xi xi , j = 4k + 1

x x , j = 4k + 2


fjk+i =
kN
(7)

x

x
,
j
=
4k
+
3





x x
, j = 4k

(2) 1 k, every variable Xi (first k variables) change value ai times,
f1i , . . . , fai corresponding sequence flips Xi , then, 1 j ai ,
every flip fji consistent Eq. 7.
(3) Given sequence flips Xk (2), variable Xk+1 change value
ak + 1 times, f1k+1 , . . . , fak+1
corresponding sequence flips Xk+1 ,
k +1
then, 1 j ak + 1, have:
(
k+1 xk+1 , j = 2k + 1
x
fjk+1 =
kN
(8)
k+1 , j = 2k
xk+1 x
(4) sequence flips Xk+2 (1) feasible given sequence flips Xk+1
(3).
Step (1): proof induction i. = k + 1, variable X2k+1 (i.e., Xn ) must
2k+1 x2k+1 .
change value least twice, since value X2k induces x
2k+1 x2k+1 flip X2k+1 first x
2k+1
way change value X2k+1 x
x2k+1 , x2k+1 x2k+1 . Note flips consistent Eq. 7
ak+2(k+1) = a1 = 2, thus established induction base.
Now, assume induction hypothesis that, 2 < l k + 1, every variable
Xi must change value least ak+2i times according Eq. 7, prove claim
Xk+l1 .
Consider sequence flips Xk+l assumed necessary, i.e., f1k+l , . . . , fak+l
.
k+2l
According CP (Xk+l ) that:
(i) every pair successive flips Xk+l require different values Xk+l1 ,
k+l1 , xk+l1 },
(ii) required values Xk+l1 {x
k+l xk+l ) requires Xk+l1 = xk+l1 , o0 [Xk+l1 ] =
(iii) first flip Xk+1 (from x
k+l1 .
x
k+l1 xk+l1 back,
Therefore, Xk+l1 must perform ak+2l value changes x
order support required value changes Xk+l . addition, supporting value
k+l1
changes Xk+l , variable Xk+l1 must perform another value change x
0
k+l1 o[Xk+l1 ] = xk+l1 .
xk+l1 , since ak+2l even (see Eq. 6), [Xk+l1 ] = x
185

fiBoutilier, Brafman, Domshlak, Hoos & Poole

k+l1 xk+l1
Finally, according CP (Xk+l1 ), every value change Xk+l1 x
k+l1 consists two flips: initial value xk+l1 ,
xk+l1 x
xk+l1 target value. Thus proved Xk+l1 must perform least
2(ak+2l + 1) = ak+3l value changes, value changes consistent
Eq. 7.
Step (2): proof induction i. X1 proof straightforward, since o0 [X1 ] =
1 , o[X1 ] = x1 , CP (X1 ) allows us flip value X1 x
1 x1 ,
x
x1 x1 .
assume induction hypothesis that, 1 < k, every variable Xi
change value ai times according Eq. 7, prove claim Xi+1 . proof
apparent CP (Xi+1 ), outcomes o0 query, induction hypothesis.
every value achieved Xi along sequence ai flips (including initial value
), Xi+1 flip value twice: Given Xi = x
Xi = xi , flip value
o0 [Xi ] = x
i+1 xi+1 , xi+1 xi+1 . Alternatively, given Xi = xi ,
Xi+1 first x
i+1 . Therefore,
flip value Xi+1 xi+1 xi+1 , xi+1 x
Xi+1 flip value 2(ai + 1) = ai+1 times, easy see value flips
Xi+1 consistent Eq. 7.
Step (3): Given sequence ak flips f1k , . . . , fakk Xk Eq. 7, let val(fjk ) Dom(Xk )
value Xk achieved flip fjk , 1 j ak . CP (Xk+1 ) entails that,
k , f k , 1 j 2,
triple values Xk achieved triple successive flips fjk , fj+1
k
j+2
either:
val(fjk )

:

k+1
xk+1 x

k
val(fj+1
)

:

k+1 xk+1
x

k
val(fj+2
)

:

k+1
xk+1 x

val(fjk )

:

k+1 xk+1
x

k
val(fj+1
)

:

k+1
xk+1 x

k
val(fj+2
)

:

k+1 xk+1
x



addition, know that:
k : xk+1 x
k+1
o0 [Xk ] = x

k+1 xk+1
val(f1k ) = xk : x
entails Xk+1 change value ak + 1 times according Eq. 8.
Step (4): proof subclaim straightforward CP (Xk+2 ). Given sequence Xk+2 value flips f1k+2 , . . . , fak+2
Eq. 7, observe that, 1 j ak ,
k
186

fiCP-Nets

j = 2k + 1 k N, fjk+2 supported value xk+1 Xk+1 , otherk+1 Xk+1 . Now, since
wise, j = 2k, k N, fjk+2 supported value x
1 l ak + 1,
(
xk+1 , l = 2m + 1
val(flk+1 ) =
mN
k+1 , l = 2m
x
apparent sequence ak value flips Xk+2 Eq. 7 feasible given
sequence ak + 1 value flips12 Xk+1 Eq. 8.
0
entails
size minimal flipping sequence
Pk N |= ,
n
0
N i=1 ai , greater 2 2 . fact, show value flips
X1 , . . . , Xk+1 , constructed steps (2)-(3), part minimal flipping sequence
o0 o, thus length sequence greater than:

2

k
X

ai + (ak + 1)

i=1

However, result achieved step (1) already proves claim exist dominance
queries multi-valued CP-nets partially specified preferences,
exponentially sized minimal flipping sequences.

References
Bacchus, F., & Grove, A. (1995). Graphical models preference utility. Proceedings Eleventh Conference Uncertainty Artificial Intelligence, pp. 310,
Montreal.
Bacchus, F., & Grove, A. (1996). Utility independence qualitative decision theory.
Proceedings Sixth International Conference Principles Knowledge Representation Reasoning, pp. 542552, Cambridge.
Backstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. Computational
Intelligence, 11 (4), 625655.
Benferhat, S., Dubois, D., & Prade, H. (2001). Towards possibilistic logic handling
preferences. Applied Intelligence, 303317.
Boutilier, C. (1994). Toward logic qualitative decision theory. Proceedings
Fifth International Conference Principles Knowledge Representation
Reasoning, pp. 7586, Bonn.
Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-Networks: directed graphical
representation conditional utilities. Proceedings Seventeenth Conference
Uncertainty Artificial Intelligence, pp. 5664, Seattle.
Boutilier, C., Brafman, R., Geib, C., & Poole, D. (1997). constraint-based approach
preference elicitation decision making. AAAI Spring Symposium Qualitative
Decision Theory, Stanford.
12. fact, ak value flips Xk+1 used order perform ak value flips Xk+2 required.
last flip Xk+1 used order achieve value o[Xk+1 ] Xk+1 supporting Xk+2 .

187

fiBoutilier, Brafman, Domshlak, Hoos & Poole

Brafman, R., & Domshlak, C. (2001). CP-networks preference-based CSP. Proceedings
Workshop Modelling Solving Problems Soft Constraints (in CP-01),
pp. 3142, Paphos, Cyprus.
Brafman, R., & Domshlak, C. (2002). Introducing variable importance tradeoffs CPnets. Proceedings Eighteenth Annual Conference Uncertainty Artificial
Intelligence, pp. 6976, Edmonton, Canada.
Brafman, R., & Domshlak, C. (2003). Structure complexity planning unary
operators. Journal Artificial Intelligence Research. appear.
Brafman, R. I., & Dimopoulos, Y. (2003). new look semantics optimization
methods CP-networks. Proceedings Eighteenth International Joint
Conference Artificial Intelligence, Acapulco, Mexico. appear.
Bylander, T. (1994). computational complexity propositional STRIPS planning.
Artificial Intelligence, 69 (1-2), 165204.
Castaneda, H. N. (1958). Review Hallden Logic Better. Philosophy
Phenomenological Research, 19, 266.
Chajewska, U., Getoor, L., Norman, J., & Shahar, Y. (1998). Utility elicitation classification problem. Proceedings Fourteenth Conference Uncertainty
Artificial Intelligence, pp. 7988, Madison, WI.
Cormen, T. H., Lierson, C. E., & Rivest, R. L. (1990). Introduction Algorithms. MIT
Press, Cambridge, MA.
DAmbrosio, J. G., & Birmingham, W. P. (1995). Preference-directed design. Journal
Artificial Intelligence Engineering Design, Analysis Manufacturing, 9, 219230.
Domshlak, C. (2002a). Modeling Reasoning Preferences CP-nets. Ph.D.
thesis, Ben-Gurion University. (forthcoming).
Domshlak, C. (2002b). recursively directed hypercubes. Electronic Journal
Combinatorics, 9 (1).
Domshlak, C., & Brafman, R. (2002a). CP-nets - reasoning consistency testing.
Proceedings Eighth International Conference Principles Knowledge Representation Reasoning, pp. 121132, Toulouse, France.
Domshlak, C., & Brafman, R. (2002b). Structure complexity planning unary operators. Proceedings Sixth International Conference Artificial Intelligence
Planning Scheduling, pp. 6069, Toulouse, France.
Domshlak, C., Brafman, R., & Shimony, S. E. (2001). Preference-based configuration
web page content. Proceedings Seventeenth International Joint Conference
Artificial Intelligence, pp. 14511456, Seattle.
Domshlak, C., Rossi, F., Venable, C., & Walsh, T. (2003). Reasoning soft constraints
conditional preferences: Complexity results approximation techniques.
Proceedings Eighteenth International Joint Conference Artificial Intelligence, Acapulco, Mexico. appear.
188

fiCP-Nets

Domshlak, C., & Shimony, S. E. (2003). Efficient probabilistic reasoning bayes nets
mutual exclusion context specific independence. appear Proceedings
Sixteenth International FLAIRS Conference, Special Track Uncertain Reasoning.
Doyle, J., Shoham, Y., & Wellman, M. (1991). logic relative desire (preliminary report).
Proceedings Sixth International Symposium Methodologies Intelligent
Systems (ISMIS91), Lecture Notes Computer Science, pp. 1631. Springer-Verlag.
Doyle, J., & Wellman, M. (1994). Representing preferences ceteris paribus comparatives.
Proceedings AAAI Spring Symposium Decision-Theoretic Planning, pp.
6975, Stanford.
Dubois, D., Berre, D. L., Prade, H., & Sabbadin, R. (1998a). Logical representation
computation optimal decisions qualitative setting. Proceedings Fifteenth National Conference Artificial Intelligence, pp. 588593, Madison, WI.
Dubois, D., Godo, L., Prade, H., & Zapico, A. (1998b). Making decision qualitative
setting: decision uncertainty case-based decision. Proceedings
International Conference Principles Knowledge Representation Reasoning,
pp. 594605. Morgan-Kauffman.
Dubois, D., Prade, H., & Sabbadin, R. (1997). possibilistic logic machinery qualitative
decision. AAAI Spring Symposium Qualitative Preferences Deliberation
Practical Reasoning, pp. 4754, Stanford.
Dyer, J. S. (1972). Interactive goal programming. Management Science, 19, 6270.
French, S. (1986). Decision Theory. Halsted Press, New York.
Freuder, E., & OSullivan, B. (2001). Modeling generating tradeoffs constraintbased configuration. Proceedings 4th Workshop Configuration (IJCAI-01),
pp. 3844, Seattle.
Gudes, E., Domshlak, C., & Orlov, N. (2002). Remote conferencing multimedia objects.
Proceedings Second International Workshop Multimedia Data Document
Engineering (MDDE02).
Ha, V., & Haddawy, P. (1998). Toward case-based preference elicitation: Similarity measures
preference structures. Proceedings Fourteenth Conference Uncertainty
Artificial Intelligence, pp. 193201, Madison, WI.
Haag, A. (1998). Sales configuration business processes. IEEE Intelligent Systems
Applications, 13 (4), 7885.
Hallden, S. (1957). Logic Better. Lund.
Hansson, S. O. (1996). ceteris paribus preference. Journal Philosophical Logic,
25 (3), 307332.
Howard, R. A., & Matheson, J. E. (Eds.). (1984). Readings Principles Applications Decision Analysis. Strategic Decision Group, Menlo Park, CA.
Ignizio, J. P. (1982). Linear Programming Single Multiple Objective Systems.
Prentice-Hall, Englewood Cliffs.
189

fiBoutilier, Brafman, Domshlak, Hoos & Poole

Junker, U. (1997). cumulative-model semantics dynamic preferences assumptions.
Proceedings Fifteen International Joint Conference Artificial Intelligence,
pp. 162167, Nagoya, Japan.
Junker, U. (2001). Preference programming configuration. Proceedings 4th Workshop Configuration (IJCAI-01), pp. 5056, Seattle.
Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences
Value Trade-offs. Wiley, New York.
Knoblock, C. A. (1994). Automatically generating abstractions planning. Artificial
Intelligence, 68, 243302.
Kron, A., & Milovanovic, V. (1975). Preference choice. Theory Decision, 6, 185196.
La Mura, P., & Shoham, Y. (1999). Expected utility networks. Proceedings
Fifteenth Conference Uncertainty Artificial Intelligence, pp. 366373, Stockholm.
Lashkari, Y., Metral, M., & Maes, P. (1994). Collaborative interface agents. Proceedings
Twelfth National Conference Artificial Intelligence, pp. 444449, Seattle.
Nguyen, H., & Haddawy, P. (1998). decision-theoretic video advisor. AAAI-98
Workshop Recommender Systems, pp. 7780, Madison, WI.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann, San Mateo.
Sabin, D., & Weigel, R. (1998). Product conguration frameworks - survey. IEEE Intelligent
Systems Applications, 13 (4), 4249.
Shimony, S. E., & Domshlak, C. (2002). Complexity probabilistic reasoning (directedpath) singly connected (not polytree!) Bayes networks. submitted publication.
Shoham, Y. (1987). semantical approach nonmonotonic logics. Proceedings Tenth
International Joint Conference Artificial Intelligence, pp. 388392.
Shoham, Y. (1997). Conditional utility, utility independence, utility networks. Proceedings Thirteenth Annual Conference Uncertainty Artificial Intelligence,
pp. 429436, San Francisco, CA. Morgan Kaufmann Publishers.
Soininen, T., & Niemela, I. (1998). Formalizing configuration knowledge using rules
choices. Seventh International Workshop Nonmonotonic Reasoning, Trento.
Tan, S.-W., & Pearl, J. (1994). Specification evaluation preferences planning
uncertainty. Proceedings Fifth International Conference Principles
Knowledge Representation Reasoning, pp. 530539, Bonn.
Trapp, R. (1985). Utility theory preference logic. Erkenntnis, 22, 331339.
von Wright, G. H. (1963). Logic Preference: Essay. Edinburgh University Press.
von Wright, G. H. (1972). logic preference reconsidered. Theory Decisions, 3,
140167. Reprinted (von Wright, 1984).
von Wright, G. H. (1984). Philosophical logic: Philosophical Papers, Volume II. Cornell
University Press, Ithaca, NY.
190

fiCP-Nets

Wellman, M., & Doyle, J. (1991). Preferential semantics goals. Proceedings
Ninth National Conference Artificial Intelligence (AAAI-91), pp. 698703, Anaheim.

191

fiJournal Artificial Intelligence Research 21 (2004) 393-428

Submitted 07/03; published 03/04

Personalized System Conversational Recommendations
Cynthia A. Thompson

cindi@cs.utah.edu

School Computing
University Utah
50 Central Campus Drive, Rm. 3190
Salt Lake City, UT 84112 USA

Mehmet H. Goker

mgoker@kaidara.com

Kaidara Software Inc.
330 Distel Circle, Suite 150
Los Altos, CA 94022 USA

Pat Langley

langley@isle.org

Institute Study Learning Expertise
2164 Staunton Court
Palo Alto, CA 94306 USA

Abstract
Searching making decisions information becoming increasingly difficult amount information number choices increases. Recommendation
systems help users find items interest particular type, movies restaurants, still somewhat awkward use. solution take advantage
complementary strengths personalized recommendation systems dialogue systems,
creating personalized aides. present system Adaptive Place Advisor
treats item selection interactive, conversational process, program inquiring item attributes user responding. Individual, long-term user preferences
unobtrusively obtained course normal recommendation dialogues used
direct future conversations user. present novel user model influences item search questions asked conversation. demonstrate
effectiveness system significantly reducing time number interactions
required find satisfactory item, compared control group users interacting
non-adaptive version system.

1. Introduction Motivation
Recommendation systems help users find select items (e.g., books, movies, restaurants)
huge number available web electronic information sources (Burke,
1999; Resnick & Varian, 1997; Burke, Hammond, & Young, 1996). Given large set
items description users needs, present user small set items
well suited description. Recent work recommendation systems includes
intelligent aides filtering choosing web sites (Eliassi-Rad & Shavlik, 2001), news
stories (Ardissono, Goy, Console, & Torre, 2001), TV listings (Cotter & Smyth, 2000),
information.
users systems often diverse, conflicting needs. Differences personal
preferences, social educational backgrounds, private professional interests
pervasive. result, seems desirable personalized intelligent systems
c
2004
AI Access Foundation. rights reserved.

fiThompson, Goker, & Langley

process, filter, display available information manner suits individual
using them. need personalization led development systems adapt
changing behavior based inferred characteristics user
interacting (Ardissono & Goy, 2000; Ferrario, Waters, & Smyth, 2000; Fiechter
& Rogers, 2000; Langley, 1999; Rich, 1979).
ability computers converse users natural language would arguably
increase usefulness flexibility even further. Research practical dialogue systems,
still infancy, matured tremendously recent years (Allen, Byron, Dzikovska,
Ferguson, Galescu, & Stent, 2001; Dybkjr, Hasida, & Traum, 2000; Maier, Mast, &
Luperfoy, 1996). Todays dialogue systems typically focus helping users complete
specific task, planning, information search, event management, diagnosis.
paper, describe personalized conversational recommendation system designed help users choose item large set basic type. goal
support conversations become efficient individual users time.
system, Adaptive Place Advisor, aims help users select destination (in
case, restaurants) meets preferences.
Adaptive Place Advisor makes three novel contributions. knowledge,
first personalized spoken dialogue system recommendation, one
conversational natural language interfaces includes personalized, long-term user
model. Second, introduces novel model acquiring, utilizing, representing user
models. Third, used demonstrate reduction number system-user interactions conversation time needed find satisfactory item.
combination dialogue systems personalized recommendation addresses weaknesses approaches. dialogue systems react similarly user interacting
them, store information gained one conversation use future.
Thus, interactions tend tedious repetitive. adding personalized, long-term
user model, quality interactions improve drastically. time,
collecting user preferences recommendation systems often requires form filling
explicit statements preferences users part, difficult time consuming. Collecting preferences course dialogue lets user begin task
item search immediately.
interaction conversation personalized recommendation also affected choices acquisition, utilization, representation user models.
Adaptive Place Advisor learns information users unobtrusively, course
normal conversation whose purpose find satisfactory item. system stores
information use future conversations individual. acquisition
utilization occur items presented chosen user,
also search items. Finally, systems representation models goes
beyond item preferences include preferences item characteristics particular values characteristics. believe ideas extend types
preferences types conversations.
paper, describe work Adaptive Place Advisor. begin
introducing personalized conversational recommendation systems, presenting
design decisions along way. Section 3 describe system detail,

394

fiPersonalized Conversational Recommendation

Section 4 present experimental evaluation. Sections 5 6 discuss related
future work, respectively, Section 7 conclude summarize paper.

2. Personalized Conversational Recommendation Systems
research goals two-fold. First, want improve interaction quality
recommendation systems utility results returned making user adaptive
conversational. Second, want improve dialogue system performance means
personalization. such, goals user modeling differ commonly assumed
recommendation systems, improving accuracy related measures like precision
recall. goals also differ previous work user modeling dialogue
systems (Haller & McRoy, 1998; Kobsa & Wahlster, 1989; Carberry, 1990; Kass, 1991),
emphasizes ability track users goals dialogue progresses,
typically maintain models across multiple conversations.
hypothesis improvements efficiency effectiveness achieved
using unobtrusively obtained user model help direct systems conversational search
items recommend. approach assumes large database items
choose, reasonably large number attributes needed describe
items. Simpler techniques might suffice situations database small
items easy describe.
2.1 Personalization
Personalized user adaptive systems obtain preferences interactions users,
keep summaries preferences user model, utilize model generate
customized information behavior. goal customization increase
quality appropriateness interaction result(s) generated
user.
user models stored personalized systems represent stereotypical users (Chin,
1989; Rich, 1979) individuals, hand-crafted learned (e.g., questionnaires, ratings, usage traces), contain information behavior
previously selected items, preferences regarding item characteristics (such location
price), properties users (such age occupation) (Kobsa & Wahlster,
1989; Rich, 1979). Also, systems store user models duration one interaction user (Carberry, Chu-Carroll, & Elzer, 1999; Smith & Hipp, 1994), whereas
others store long term (Rogers, Fiechter, & Langley, 1999; Billsus & Pazzani,
1998).
approach learn probabilistic, long-term, individual user models contain
information preferences items item characteristics. chose learned models
due difficulty devising stereotypes reasonable initial models new domain
encountered. chose probabilistic models flexibility: single user
exhibit variable behavior preferences relative rather absolute. Long-term
models important allow influence across multiple conversations. Also, already
noticed, different users different preferences, chose individual models. Finally,
preferences items item characteristics needed influence conversations
retrieval.
395

fiThompson, Goker, & Langley

decision made learn models, another design decision relates method
system collects preferences subsequent input learning algorithm(s).
distinguish two approaches. direct feedback approach places
burden user soliciting preference information directly. example, system
might ask user complete form asks classify weight interests using
variety categories item characteristics. recent study (McNee, Lam, Konstan,
& Riedl, 2003) showed forcing user provide ratings items (movies,
case) choose, rather system chooses, actually lead
better accuracy rates better user loyalty. However, users irritated need
complete long questionnaires even begin enjoy given service,
study context dialogue system involved simpler interaction.
Another, slightly less obtrusive, form direct feedback encourages user provide
feedback continues use particular service.
second approach acquiring user models, one taken Adaptive
Place Advisor, infer user preferences unobtrusively, examining normal online behavior (Fiechter & Rogers, 2000; Rafter, Bradley, & Smyth, 2000). feel unobtrusive
collection preferences advantageous, requires less effort user. Also, users
often cannot articulate preferences clearly learn domain.
possible disadvantage unobtrusive approaches users may trust understand
systems actions change one interaction next. could
addressed also letting user view modify user model (Kay & Thomas, 2000).
Systems typically take one two approaches preference determination. Contentbased methods recommend items similar ones user liked past (Segal &
Kephart, 1999; Pazzani, Muramatsu, & Billsus, 1996; Lang, 1995). contrast, collaborative
methods select recommend items users similar current user liked
previous interactions (Cotter & Smyth, 2000; Billsus & Pazzani, 1998; Konstan, Miller,
Maltz, Herlocker, Gordon, & Riedl, 1997; Shardanand & Maes, 1995). collaborative
filtering bases recommendations previous selections users, suitable
new one-off items users uncommon preferences. content-based approach,
hand, uses item description recommendation, therefore
prone problems. However, content-based techniques tend prefer attribute
values users preferred past, though allow new combinations
values. feel benefits content-based approach outweigh disadvantages;
discuss methods overcoming disadvantages combining two techniques
Section 6.3.
Ultimately, personalization one utilize learned user profile search
for, identify, present relevant information right user right way right
time. User models utilized recommendation systems content processing
selection (information filtering), navigation support web browsers (Pazzani et al., 1996),
choice modality style presentation interaction (Brusilovsky & Maybury,
2002). Adaptive Place Advisor adapts information filtering interaction
behavior, since relevant application since majority
interaction natural language.

396

fiPersonalized Conversational Recommendation

2.2 Conversational Recommendation
goal recommendation systems help users find items interest, thus
must eliminate choices consideration keeping others. achieve this,
must either know determine interests current user, present items
satisfy interests.
One widely used approaches information acquisition recommendation
systems use simple form (usually single query box) user fills in. Coupled
this, common presentation format ranked list approach. scheme,
system presents usually long list results, ordered predicted relevance
user. combined technique information acquisition form presentation
list originated information retrieval, used help find documents
reports library research. basic method, refer ranked list
approach, general, however, underlies simpler search engines
World Wide Web, used regularly many different selection tasks.
contrast, Adaptive Place Advisor supports conversations mode
interaction. approach, one conversational participant, call inquirer,
asks second participant, call advisor, assistance selecting item.
example, inquirer deciding restaurant dinner, conversation
may proceed follows:
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.

Inquirer: think eat tonight?
Advisor: type food would like?
Inquirer: types there?
Advisor: say things like Chinese, Indian, Mediterranean.
Inquirer: Oh, maybe cheap Indian place.
Advisor: kind parking would like?
Inquirer: dont care, long Palo Alto.
Advisor: Im sorry, dont know restaurants like that, would like search
price?
Inquirer: No, think Id like Chinese instead.
Advisor: Mandarin Gourmet 420 Ramona sound?1
Inquirer: No, else have?
Advisor: Jing-Jing Szechwan Hunan Gourmet 443 Emerson Street?
Inquirer: Sure, sounds fine.

refer systems mimic approach recommendation conversational
recommendation systems.
interaction supported conversational recommendation systems quite different
found ranked list approach. important distinction
interaction takes form sequence questions, designed eliminate items
consideration. Answering questions plays similar role giving keywords
form, aim remove alternatives rather rank results list.
1. system support exact sample dialogue. example, point conversation
displays information restaurant screen asks one sound?
similar question.

397

fiThompson, Goker, & Langley

conversational process also help inquirer better understand desires, since
thinking possible questions answers may clarify goals ways keyword entry
encourage. addition, conversation support user variability letting
user control item attribute choices conversation. Finally, inquirer never
hears complete item choices remain. avoids presenting
long list items lets user narrow choices iterative, manageable
fashion.
dialogues seem better recommendations must delivered speech rather
visually, example engaged inquirer driving. also seem
ideal, independent modality, tasks like destination selection help-desk support
(Goker & Roth-Berghofer, 1999; Aha & Breslow, 1997), user needs converge
items. hand, keyword entry ranked list methods seem
appropriate situations user prefers provide requirements once,
situations information presented visually, situations user
may want examine many options.
eliminating options, conversational recommendation systems ultimately direct
users suitable solution. However, conversation become tiring quality
first result returned may acceptable user. interactions
friend knows concerns directed produce better results
stranger, dialogues conversational advisor become efficient
effective time. goals user modeling include improvement subjective
quality effectiveness results (found items) conversation leads
results. example, several conversations inquirer above, new
interaction may proceed follows, question parking eliminated
item presentation order changed:
1.
2.
3.
4.
5.
6.
7.

Inquirer:
Advisor:
Inquirer:
Advisor:
Inquirer:
Advisor:
Inquirer:

think eat tonight?
type food would like?
Oh, maybe Chinese place.2
city prefer?
something Palo Alto?
Jing-Jing Szechuan Gourmet 443 Emerson sound?
Sure, sounds fine.

turn next design choices concerning management conversations.
2.3 Conversation via Dialogue Management
Dialogue systems carry conversations users natural language, whether spoken
typed. main tasks performed dialogue systems language interpretation, language
generation, dialogue management. Natural language interpretation generation
topics onto discuss here; two introductory texts,
see Allen (1995) Jurafsky Martin (2000). enable focus user modeling,
system allows moderately complex user utterances pre-coded set system
utterances, discussed Section 3.3.
2. response shows Inquirer learned use system efficiently well.

398

fiPersonalized Conversational Recommendation

simplest dialogue managers based finite-state automata states
correspond questions arcs correspond actions depend user-provided
response (Stent, Dowding, Gawron, Bratt, & Moore, 1999; Winograd & Flores, 1986).
systems support called fixed- system-initiative conversations,
one participants controls actions, whether system helping user
user asking questions system. Next complexity frame- template-based
systems questions asked answered order (Bobrow et al., 1977).
Next, true mixed-initiative systems allow either dialogue participant contribute
interaction knowledge permits (Allen, 1999; Haller & McRoy, 1998; Pieraccini,
Levin, & Eckert, 1997). Thus, conversational focus change time due
users (or systems) initiative change. Finally, different approaches support
sophisticated dialogues include plan-based systems (Allen et al., 1995; Cohen & Perrault,
1979) systems using models rational interaction (Sadek, Bretier, & Panaget, 1997).
allow reasonably complex conversations keeping system design straightforward, chose frame-based approach dialogue management. Thus, Adaptive Place Advisor allows conversational flexibility fully system-initiative
paradigm would allow. Users fill attributes addition suggested system. However, cannot force system transition new subtasks,
system negotiate users determine participant take
initiative.
2.4 Interactive Constraint-Satisfaction Search
Constraint-satisfaction problems provide general framework defining problems interest many areas artificial intelligence, scheduling satisfiability (Kumar,
1992). general form, constraint-satisfaction problems involve set variables
whose domains finite discrete, along set constraints defined
subset variables limit value combinations variables take.
goal find assignment values variables satisfies given constraints.
Cucchiara, Lamma, Mello, Milano (1997) define class interactive constraintsatisfaction problems involve three extensions standard formulation. First,
include constraint acquisition stage user incrementally add new
constraints problem solved. Second, variables domain include
defined undefined portion, user add new values defined portion
constraint acquisition. Third, allow incremental update partial solution
based domain constraint updates.
framework encompass item search portion conversations managed
Adaptive Place Advisor; include item presentation portion.
setting, constraints simply attribute-value specifications, cuisine = Chinese.
Place Advisors search fully general framework,
incorporate notion undefined portions domains. However, acquire
constraints via users specifications conversation incrementally updates
solutions response.

399

fiThompson, Goker, & Langley

System Output
(Voice)

Prompts

User Input
(Voice)

Speech Generator

Speech Recognizer

System Operators
Values

User Operators
Values
Domain
Model

Dialogue Manager

Conversation History

User
Models

Initial Query

Recognition
Grammars

Results,
Attribute Information

User Modeling
System

Updated Query

Retrieval Engine

Item
Database

Figure 1: Components Adaptive Place Advisor interactions.

3. Adaptive Place Advisor
section, first present overview Adaptive Place Advisors functioning,
follow details components.3 system carries number tasks
support personalized interaction user; particular, it:










utilizes user model initialize probabilistic item description expanded query,
generates context-appropriate utterances,
understands users responses,
refines expanded query explicit requirements (constraints) obtained
user conversation,
retrieves items matching explicitly specified part query database,
calculates similarity retrieved items query,
selects next attribute constrained relaxed conversation
number highly similar items acceptable,
presents suitable items number items acceptable,
acquires updates user model based interactions.

responsibilities tasks distributed among various modules system,
shown Figure 1. Dialogue Manager generates, interprets, processes conversations; also updates expanded query user interaction. Retrieval Engine
case-based reasoning system (Aamodt & Plaza, 1994) uses expanded query
retrieve items database measure similarity users preferences.
User Modeling System generates initial (probabilistic) query updates longterm user model based conversation history. Speech Recognizer Speech
Generator handle users input control systems output, respectively.
3. discussed Section 5.2, approach destination advice draws earlier analysis
task Elio Haddadi (1998, 1999).

400

fiPersonalized Conversational Recommendation

find items recommend user, Place Advisor carries augmented
interactive constraint-satisfaction search. goal entire conversation present
item acceptable user. constraint-satisfaction portion,
system carries conversation find small set items. search
phase, two situations determine systems search operators thus questions. First,
under-constrained specification means many items match constraints,
system must obtain information user. Second, matching items,
system must relax constraint, thus allowing items contain domain value
relaxed attribute.4 system ends search phase small number items
match constraints highly similar (based similarity threshold) users
preferences. Item presentation (in similarity order) begins point, similarity
computation used rank items satisfy constraints.
search item presentation process also influenced User Modeling System
thus personalized. main mechanism personalization expanded
query, probabilistic representation users preferences, long-term (over many
conversations) short-term (within conversation). often refer
query, always refers constraints explicitly implicitly specified
user. Thus, query expanded beyond explicit (short-term) constraints
using (long-term) constraints implicit user model. sense, initial query
represents constraints system thinks user probably want. system
incrementally refines query course conversation user, setting
explicit, firm constraints user verifies disconfirms assumptions. long
term, User Modeling System updates user model based users responses
attributes items offered conversation.
Retrieval Engine searches database items match explicit constraints
query. computes similarity retrieved items users preferences
reflected expanded part query. Depending number highly similar results, Retrieval Engine also determines attribute constrained
relaxed.
sum, system directs conversation manner similar frame-based system,
retrieves ranks items using case-based reasoning paradigm, adapts weights
similarity calculation based past conversations user, thereby personalizing
future retrievals conversations. section, present details Adaptive
Place Advisors architecture. describing user model, elaborate
Retrieval Engine Dialogue Manager. Finally, discuss system updates
user model user interacts it.
3.1 User Model
focus personalized conversation suggests fine-grained model user preferences,
emphasizing questions user prefers answer responses tends give,
addition preferences entire items. describe model detail.
later sections, describe influences item ranking question ordering,
4. constraints later modified, system lets user later specify value, even
one caused over-constrained situation.

401

fiThompson, Goker, & Langley

Table 1: Example user model.
User Name

Homer

Attributes

wi

Cuisine

0.4

Values probabilities
Italian

French

Turkish

Chinese

German

English

0.35

0.2

0.25

0.1

0.1

0.0

0.2

one

two

three

four

five

0.3

0.3

0.1

0.1

...

...

0.2
...

Parking

0.1

Price Range

Item Nbr.
Accept/Present

Valet

Street

Lot

0.5

0.4

0.1

0815

5372

7638

...

6399

23 / 25

10 / 19

33 / 36

...

12 / 23

turn determine quickly system stop asking questions start presenting
items. general, user may tend to:





answer questions attributes often others,
provide attribute values often others,
choose items often others,
provide certain combinations values often independent distribution
would predict,
accept either large small amounts value item diversity.

tendencies influenced users preferences, turn captured
user model. Attribute preferences represent relative importance user places
attributes (e.g., cuisine vs. price) selecting item. Preferred values show users
bias towards certain item characteristics (e.g., Italian restaurants vs. French restaurants).
Item preferences reflected users bias certain item, independent
characteristics. Combination preferences represent constraints combined occurrence
item characteristics (e.g., accepts restaurants San Francisco valet
parking). Diversity preferences model time needs pass item
characteristic suggested users tolerance unseen values items. Item
preferences related single items, whereas attribute, value, combination preferences
applicable search items general. Diversity preferences relate
items search.
Currently, Adaptive Place Advisor models preferences user may
attributes, values, items, combination diversity preferences.
former easily captured either probability distributions counts, illustrated
Table 1. Place Advisor maintains probability distribution represent attribute
preferences independent probability distributions represent preferences attributes set values. attribute preferences, system uses domain knowledge
initialize weights; example, price usually considered important park-

402

fiPersonalized Conversational Recommendation

ing. absence information, case value preferences, system
begins uniform distribution.
system represents item preferences ratio number times item
accepted number times presented; initialized assuming
items presented accepted large percentage (nine ten, 90%)
time. may cause updates (see below) small effect undesirable
items suggested once, effect quickly discounting alternatives
early learning process. turn encourages user explore alternatives,
allowing system learn additional items. sum, item preferences represent
probability user accepting particular item presented, rather
representing probability distribution items.
3.2 Retrieval Engine
place, user model affects behavior system Retrieval
Engine, interacts database retrieve items, any, satisfy currently agreed upon constraints. module also interacts query determine
similar items users preferences determine best ordering
attributes constraining relaxing, appropriate. types interactions
user model support goal quickly narrowing search satisfactory item.
Similar way human advisor bases assumptions regarding inquirer
previous interactions, system uses cumulative experience, reflected user model,
basis computation. Retrieval Engine represents users preferences
requirements expanded query, partial, probabilistic item specification determined
conversation user model. query initialized user model,
thus contains preference-based probabilities attributes values user
yet explicitly specified along users item preferences. course
conversation, system updates query reflect values user specifies.
attribute, sets probability value agreed upon conversation 1.0,
probabilities zero. example, user says Chinese Italian,
system sets value probabilities Chinese Italian 1.0, cuisine
probabilities zero. equivalent disjunction probabilistic queries, one
value combination specified user.5
first main aspect Retrieval Engine personalized item ranking
technique. Unlike typical case-based similarity computation, retrieves items beyond
match query exactly, computation used system restricts retrieved
items desirable user. system filters items included
current case base according characteristics explicitly specified user sorts
remaining items according similarity items user liked past.
Thus, Engine first queries database retrieve items match current
constraints exactly.6 Then, Place Advisor uses probability distributions
query compute likely user choose item. system calculates
5. user study described Section 4, users specified disjunctive query.
6. attributes user selected one value, assume supplied value would
acceptable.

403

fiThompson, Goker, & Langley

similarity current query, Q, item, using
Sim(Q, I) = RI

n
X

wj P (Vj ) ,

j=1

RI users item preference ratio item I, n number attributes, wj
weight attribute j Q, Vj value attribute j I, P (Vj )
value preference (probability Q) value. Similarity formula based
user model search state. Thus, unconstrained attribute, estimates
probability user accept items value attribute.
system calculates similarity item, compares items similarity constant
similarity threshold, retains items exceed threshold.
second main personalized aspect Retrieval Engine ranking attributes
under- over-constrained situations. helps assure user likely
respond informatively systems questions under-constrained situations,
allow suggested attribute relaxed over-constrained situations.
situations, one option order attributes randomly, technique used
simple dialogue systems. Another option use conditional entropy measure select
attributes (see Section 6.1). third rank attributes order desirability
user, reflected user model, take option.
addition using long term user model rank attributes, system also uses
attribute weights reflected query. see see later querys
attribute weights, initialized user model, also influenced conversation. over-constrained situation, attribute ranking order highest lowest,
under-constrained situation, reverse.7 Using attribute weights rather
conditional entropy avoids pitfalls arise continuously changing value
distribution data (new restaurants, restaurants going business, etc.).
affect attribute rankings, users may confused resulting variability.
even worse, would reflect users preferences. Further, every question
high information gain high relevance user selecting destination (e.g., parking
options may high score asked user decided
cuisine location.)
summary, user model influences item retrieval, item ranking, attribute ranking, turn influence systems utterances conversation.
3.3 Conversing User
converses user, Dialogue Manager uses results Retrieval Engines
functions. system uses frame containing simple list constraints support
interactive constraint-satisfaction search (see Jurafsky et al. 1994 Dowding et al. 1993
similar formulation). usual type system, user respond
system request fill constraint ignoring attribute specifying value
7. CBR systems necessarily use weighting factors similarity computation
question ordering. However, application area, correct make assumption
attributes importance impact similarity computation.

404

fiPersonalized Conversational Recommendation

Table 2: Speech acts supported Adaptive Place Advisor.
System Speech Acts
Attempt-Constrain
Suggest-Relax
Recommend-Item
Quit-Start-Mod
Provide-Values
Clarify

Asks question obtain value attribute.
Asks question remove values attribute.
Recommends item satisfies constraints.
States matching items remain asks whether
modify search, start over, quit.
Lists small set values attribute.
Asks clarifying question.

User Speech Acts
Provide-Constrain
Accept
Reject
Provide-Relax
Start-Over
Quit
Query-Values

Provides value attribute.
Accepts relaxation suggestion item generated system.
Rejects systems proposed attribute, relaxation attempt, item.
Provides attribute value removal.
Indicates desire reinitialize constraints begin again.
Indicates desire stop conversation.
Asks information possible values attribute.

Table 3: Dialogue State.
Variable

Description

Constrained
Rejected
Fixed
Constrain
Relax
Query
Number-of-Items

Attributes whose values specified.
Attributes whose value user declined provide.
Constrained attributes user indicated relaxed.
next attribute constrain, any.
next attribute relax, any.
Probability model desired item constraints.
Number database items matching query exceeding
similarity threshold.
matching items ranked similarity order.
Items user rejected.
users recently uttered speech act.
systems recently uttered speech act.

Ranked-Items
Rejected-Items
User-Move
System-Act

different one(s) instead (Goddeau et al., 1996; Ward & Issar, 1996). speech acts
supported listed Table 2.
two main phases dialogue, interactive constraint-satisfaction portion
item presentation portion. constraint-satisfaction portion divided
over- under-constrained situations. dialogue state (Table 3) determines
systems utterance range responses expected point. system updates
dialogue states variables appropriate throughout conversation.

405

fiThompson, Goker, & Langley

Table 4: Sample Conversation.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.

Inquirer:
Advisor:
Inquirer:
Advisor:
Inquirer:
Advisor:
Inquirer:
Advisor:
price?
Inquirer:
Advisor:
Inquirer:
Advisor:
Inquirer:

think eat tonight?
type food would like?
types there?
say things like Chinese, Indian, Mediterranean.
Oh, maybe cheap Indian place.
kind parking would like?
dont care, long Palo Alto.
Im sorry, dont know restaurants like that, would like search
No, think Id like Chinese instead.
Mandarin Gourmet 420 Ramona sound?
No, else have?
Jing-Jing Szechwan Hunan Gourmet 443 Emerson Street?
Sure, sounds fine.

detail, systems speech act (or move) interactive constraint-satisfaction determined Number-of-Items dialogue state variable. Further, speech
act determines speech recognition grammar employ interpret users next
utterance. common situation many items (more small threshold, three) match current constraints.8 situation, system makes
Attempt-Constrain move, asks user fill value attribute.
move, responded appropriately user, would reduce number items
considered satisfactory user. attribute Constrain one ranked
highest Retrieval Engine already Constrained Rejected.
first sample conversation, repeated Table 4, utterances 2 6 illustrate AttemptConstrains system.
One user response Attempt-Constrain Provide-Constrain,
provides value specified attribute additional attributes, utterances 5
7. second possible response Reject, user indicates disinterest
dislike attribute, first part utterance 7. illustrated
examples, user combine one move single utterance.
second situation, over-constrained query, occurs items
satisfy agreed upon constraints similar enough users preferences,
thus Retrieval Engine returns empty set (Number-of-Items = 0). case,
system performs Suggest-Relax move informs user situation asks
would like relax given constraint. attribute Relax chosen Retrieval
Engines highest ranked attribute9 already Fixed. illustrated
utterance 8 conversation Table 4. utterance 9 conversation, user
respond rejecting (Reject) systems suggestion accept (Accept).
former case, attribute Fixed system try relax again.
8. discuss number items matching constraints, refer items remain
similarity filtering discussed Section 3.2.
9. Recall Section 3.2 actually lowest ranking attribute user model.

406

fiPersonalized Conversational Recommendation

combination either speech acts, user specify attributes
relax addition to, instead of, system-suggested attribute (Provide-Relax).
items satisfy constraints, system ends interactive search
begins suggest items user (Recommend-Item) order similarity,
utterances 10 12 above. user either accept reject item. user
accepts item (Accept), system ends conversation, reached goal
state. user rejects item (Reject), system presents alternative,
remain. Note three meanings Reject speech acts user,
two meanings Accept speech acts, since user accept AttemptConstrain providing explicit value attribute constrained.
three special situations covered above. first query
over-constrained, user Fixed attributes could relaxed. second
user rejected items match constraints. two situations,
system informs user situation, asks whether would like quit, start
over, modify search (Quit-Start-Mod), reacts accordingly. third special
situation Number-of-Items exceeds presentation threshold, attributes
Constrained Rejected. case, Place Advisor begins present
items user.
support spoken natural language input output, use speech recognition package Nuance Communications, Inc. package lets us write different
recognition grammar situations described use human-recorded
prompts (rather text-to-speech). string words recognized system
parsed using recognition grammars wrote, used users without
adaptation. Future work could include personalized recognition grammars well personalized information preferences. grammars use semantic tags fill slot:
besides slots attribute, define slots rejection acceptance systems
suggestions. complex domains, sophisticated parsing methods may required, simple scheme gives user reasonably diverse set utterance options.
Nuance modules also generate response user requests help (Query-Values)
Provide-Values speech act, enter clarification dialogues confidence
recognized utterance given threshold (Clarify). currently simple
interactions system provides examples answers recently uttered
prompt, asks user repeat themselves.
Finally, item presentation portion dialogue only, system displays
restaurant information (name, address, phone number) screen, outputs
spoken prompt one? chose presentation modality due
reluctance use text-to-speech generation large number prompts would
record produce spoken language restaurant. However, note
user still responds spoken reply, feel presentation mode
substantially influenced user-modeling behavior Place Advisor.
system-user interaction affects subsequent rounds database retrieval similarity calculation via updates expanded query. Table 5 shows effects relevant
speech acts query, turn used similarity calculation described
Section 3.2. table, shortened names system moves
sake brevity.
407

fiThompson, Goker, & Langley

Table 5: effects speech acts query. Key: Constrain = AttemptConstrain, Relax = Suggest-Relax, Recommend = Recommend-Item.
System-Move

User-Move

Effect Query

Constrain

ProvideConstrain

Constrain
Relax
Recommend
Relax
Recommend



Reject
Reject
Reject
Accept
Accept
Provide-Relax
Start-Over

Set probabilities provided values one.
Set probability values constrained attributes
zero. attribute rejected previously, reset
attribute probability user model.
Drop attribute setting probability zero.
effect; Dialogue Manager selects next attribute.
Update item preference counts (see Section 3.4).
Reset value probabilities attribute user model.
Update item preference counts (see Section 3.4).
Reset value probabilities attribute user model.
Initialize user model.

3.4 Updating User Model
main contribution addition personalization conversational recommendation model. user model (Section 3.1) represents personalization,
Adaptive Place Advisor must update appropriately. adaptive recommendation systems (Smyth & Cotter, 1999; Linden, Hanks, & Lesh, 1997; Pazzani et al.,
1996; Lang, 1995) require user provide direct feedback generate user model,
basic approach unobtrusively derive user preferences. Thus, system
introduce unnecessary interactions, learns interactions needed support
item recommendation. describe system gathers item, attribute, value
preferences. described fully below, system modifies feature value weights
(Fiechter & Rogers, 2000; Zhang & Yang, 1998; Bonzano, Cunningham, & Smyth, 1997;
Wettschereck & Aha, 1995) latter two, increases counts ratio
accepted presented items.
determining points dialogue update user model,
considered several factors. wanted enable system acquire information quickly,
discourage making erroneous assumptions. thought users might
explore search space constraining attributes, decided
system update value preferences user-specified constraint. However,
instead chose allow model updates item suggestion, learning process
might slow. choices described are, feel, good tradeoff
extremes.
three circumstances chose user model update (1) users
Accept speech acts Suggest-Relax situation, (2) users Accept speech
acts Recommend-Item situation, (3) users Reject speech act
Recommend-Item speech act system. First, assume user accepts
item, indicating: (1) preference item itself, (2) preferences attributes

408

fiPersonalized Conversational Recommendation

constrained find item, (3) preferences values provided
attributes. Thus, user accepts item presented system, probabilities
appropriate item, attributes, values increased. item preference,
system simply adds one presentation acceptance counts. attribute
value preferences, system increases probability appropriate weight small
amount proportional current weight, renormalizes weights. Thus attribute
value preferences biased measures avoid zero counts values user
never chooses, typical type probabilistic representation.
Second, user rejects item presented system, assume
dislike particular item. assume anything characteristics
item, since user specified characteristics. Therefore, rejected
items system simply adds one presentation count.
third situation system updates user model when, query
become over-constrained, presents attribute relaxation user accepts
relaxation. situation, assume that, matching item,
user would satisfied it, since characteristics specified conversation
far satisfactory. Therefore, relaxation occurs, system increases
attribute preferences constrained attributes increases value preferences
user-specified values, manner similar Accept situation RecommendItem. enables Adaptive Place Advisor quickly make inferences
users preferences.

4. System Evaluation
stated earlier, believe user modeling increases effectiveness efficiency
conversations system time. test hypothesis, carried
experiment version Adaptive Place Advisor recommends restaurants
San Francisco Bay Area. system describes items using seven attributes: cuisine,
rating, price, location, reservations, parking options, payment options. attributes
values, cuisine location dozens. approximately 1900 items
database.
asked several users, Bay Area, interact system help
decide go eat. users given external guidance instructions
types restaurants select, look choose
might actually patronize. experimenter present interactions,
filmed, help needed except rare occasions subject repeatedly
tried words included speech recognition grammar.
4.1 Experimental Variables
test hypothesis benefits personalization Adaptive Place Advisor, controlled two independent variables: presence user modeling
number times user interacted system. First, anticipated
users might improve interactions Place Advisor time, divided
subjects experimental modeling group control group. 13 subjects
modeling group interacted version system updated user model
409

fiThompson, Goker, & Langley

described Section 3.4. 11 subjects control group interacted version
update model, selected attributes items default
distribution described Section 3.1. Naturally, users unaware assigned
group. Second, since predicted systems interactions would improve time,
gained experience user, observed behavior successive points along
learning curve. particular, subject interacted system around 15
successive sessions. tried separate subjects sessions several hours,
always possible. However, general subjects use system actually
help decide eat either day near future; provide
constraints telling system knew restaurants Bay
Area.
determine versions efficiency recommending items, measured several
conversational variables. One average number interactions needed find
restaurant accepted user. defined interaction cycle started
system providing prompt ended systems recognition users utterance
response, even response answer question posed prompt.
also measured time taken conversation. began start transaction
button pushed ended system printed Done (after user accepted
item quit).
also collected two statistics depended whether user modeling
effect. First number system rejections, is, number times
system either obtain recognition result confidence low.
either case system asked user repeat himself. Since measure
recognition quality effects personalization, omitted count
interactions. second, serious problem speech misrecognition error
system assigned utterance different meaning user intended.
Effectiveness, thus subjective quality results, somewhat difficult
quantify. wanted know users degree satisfaction systems behavior.
One indication rejection rate: proportion attributes
system asked subject care (Rejects Attempt-Constrain situations).
second measure hit rate: percentage conversations first item
presented acceptable user. Finally, also administered questionnaire users
study get subjective evaluations.
4.2 Experimental Results
results experiment generally supported hypothesis respect efficiency.
provide figures show average values users particular group, error
bars showing 95% confidence intervals. x axis always shows progression users
interactions system time: point nth conversation completed
either finding acceptable restaurant quitting.
Figure 2 shows that, modeling group, average number interactions required
find acceptable restaurant decreased 8.7 5.5, whereas control group
quantity actually increased 7.6 10.3. used linear regression characterize
trend group compared resulting lines. slope modeling line

410

fi4

6

8

Number interactions
10
12
14
16

Personalized Conversational Recommendation

2

Modeling

0

Control

0

1

2

3

4

5

6

7

8

9

10

11 12 13 14 15
Conversation number

Figure 2: Average number interactions per conversation.
differed significantly (p = 0.017) control line, former smaller
latter, expected.
difference interaction times (Figure 3) even dramatic. modeling
group, quantity started 181 seconds ended 96 seconds, whereas control
group, started 132 seconds ended 152 seconds. used linear regression
characterize trends group time found significant difference
(p = 0.011) two curves, slope modeling subjects smaller
control subjects. also note measures include
time system initialization (which could 10% total dialogue time).
instead used start time first system utterance dialogue, difference
two conditions would even clearer.
speech recognizer rejected 28 percent interactions study. Rejections
slow conversation introduce errors. misrecognition rate much
lower occurred seven percent interactions experiment. feel
rates acceptable, expanding number supported utterances
could reduce first number further, potentially increasing second.
common recognition error, Adaptive Place Advisor inserted extra constraints
user intend.
results effectiveness ambiguous. Figure 4 plots rejection rate
function number sessions. decrease rejection rate time would mean that,
system gains experience user, asks fewer features irrelevant
user. However, dependent variable found significant difference (p = 0.515)
regression slopes two conditions and, indeed, rejection rate
neither group appears decrease experience. negative results may due
rarity rejection speech acts experiment. Six people never rejected constraint

411

fi50

100

Time per conversation
150
200
250

Thompson, Goker, & Langley

Modeling

0

Control

0

1

2

3

4

5

6

7

8

9

10

11 12 13 14 15
Conversation number

Figure 3: Average time per conversation.
average person used 0.53 Reject speech acts Attempt-Constrain
per conversation (standard deviation = 0.61).
Figure 5 shows results hit rate, indicate suggestion accuracy stayed
stable time modeling group decreased control group. One explanation latter, expect, control users became less satisfied
Place Advisors suggestions time thus carried exploration
item presentation time. However, concerned difference
two groups. Unfortunately, slopes two regression lines significantly
different (p = 0.1354) case.
also analyzed questionnaire presented subjects experiment. first
six questions (see Appendix A) check boxes assigned numerical values, none
revealed significant difference two groups. second part
questionnaire contained open-ended questions users experience
Adaptive Place Advisor. general, subjects groups liked system
said would use fairly often given opportunity.
4.3 Discussion
summary, experiment showed Adaptive Place Advisor improved
efficiency conversations subjects gained experience time,
improvement due systems update user models rather subjects
learning interact system. conclusion due significan differences
user modeling control groups, number interactions time
per conversation. significance holds even face large error bars small
sample size. turn implies differences large system could make
substantial difference users.
412

fiNumber rejections
1.5
2
2.5

Personalized Conversational Recommendation

Modeling

0

0.5

1

Control

0

1

2

3

4

5

6

7

8

9

10

11 12 13 14 15
Conversation number

Figure 4: Rejection rate modeling control groups.
results effectiveness ambiguous, trends right direction
significant differences modeling control groups. Subjects
conditions generally liked system, found significant differences along
dimension. larger study may needed determine whether differences occur.
user studies warranted investigate source differences
two groups. One plausible explanation items presented sooner, average,
user modeling group control group. measured value (i.e.,
average number interactions first item presentation) current study
found decrease user modeling group (from 4.7 3.9) increased
control group (from 4.5 5.8). reasonably large difference difference
slope two regression lines statistically significant (p=0.165). larger study
may needed obtain significant difference. general, however, interaction
user model order questions asked, turn influences
number items matching point conversation. turn determines
soon items presented conversation. Therefore, items presented often
user modeling group, largest influence user model due item accepts
rejects.

5. Related Research
Previous research related topics roughly broken three areas, first
focusing personalized recommendation systems, second conversational interfaces,
third adaptive dialogue systems. restrict discussion
strongly related work.

413

fiHit rate
1.5

Thompson, Goker, & Langley

Modeling

0.0

0.5

1

Control

0

1

2

3

4

5

6

7

8

9

10

11 12 13 14 15
Conversation number

Figure 5: Hit rate modeling control groups.
5.1 Personalized Recommendation Systems
Although research personalized recommendation systems become widespread
recent years, basic idea traced back Rich (1979), discussed
work conversational interfaces. Langley (1999) gives thorough review recent
research topic adaptive interfaces personalization.
Several adaptive interfaces attempt collect user information unobtrusively.
interesting example Casper project (Rafter et al., 2000), online recruitment
service. project investigates methods translate click read-time data
accurate relevancy information, given raw data inherently noisy. Similarly,
Goecks Shavlik (2000) describe technique learning web preferences observing
users browsing behavior. Another example Adaptive Route Advisor (Rogers
et al., 1999), recommends driving routes specified destination. system
collects preferences attributes number turns driving time basis
users selections modifications systems proposed routes.
Adaptive Place Advisor uses constraint-based interaction search
items, interaction approach item search. alternative taken
candidate/critique, tweaking, approach. Tweaking systems, Find
suite (Burke, 1999), typically require user begin interaction filling values
predetermined attributes. present item, point user
opportunity change search parameters try find desirable item. Eaton,
Freuder, Wallace (1997) take similar approach Matchmaking system.
addition, exploit constraint satisfaction manage search. Neither Find
suite Matchmaking system, however, learns user models. related system
include learning component Shearin Lieberman (2001), learns
attribute preferences unobtrusively. tweaking valid method, appropriate,
414

fiPersonalized Conversational Recommendation

feel, environment speech interaction mode, since presenting
user options would somewhat cumbersome. Even though current system
also presents options search constrained, limits number items presented.
Even full speech version seem onerous.
5.2 Conversational Interfaces
considerable ongoing work area conversational systems, evidenced
general surveys Dybkjr et al. (2000) Maier et al. (1996). Zukerman
Litman (2001) give thorough overview user modeling dialogue systems. Rich
(1979) reported one earliest (typewritten) conversational interfaces, focused
book recommendation. beginning interaction, system asked several
questions place user stereotype group, thereby initializing user model.
conversation progressed, model adjusted, system using ratings
represent uncertainty. However, language understanding capabilities system
limited, mostly allowing yes/no user answers. recently, dialogue systems utilize
models users beliefs intentions aid dialogue management understanding,
though typically systems maintain models course single conversation
(Kobsa & Wahlster, 1989).
noted Section 2.3, important distinction whether one conversational
participant keeps initiative, whether initiative switch participants.
Two ambitious mixed-initiative systems planning tasks Trains (Allen et al., 1995)
recent Trips (Allen et al., 2001). Like Place Advisor, programs interact user progressively construct solution, though knowledge structures
partial plans rather constraints, search involves operators plan modification rather database contraction expansion. Trains Trips lack
mechanism user modeling, underlying systems considerably mature
evaluated extensively.
Smith Hipp (1994) describe another related mixed-initiative system limited
user modeling, case conversational interface circuit diagnosis. system
aims construct plan set constraints, rather proof tree. central
speech act, requests knowledge user would aid proof process,
invoked program detects missing axiom needs reasoning.
heuristic plays role system Place Advisors heuristic
selecting attributes constrain item selection. interface infers user knowledge
course single conversation, long term approach.
respect dialogue management, several previous systems used method
similar frame-based search. particular, Seneff et al. (1998) Dowding et al.
(1993) developed conversational interfaces give advice air travel. Like Place
Advisor, systems ask user questions reduce number candidates, treating
flight selection interactive construction database queries. However, question
sequence typically fixed advance, despite clear differences among individuals
domain. Also, systems usually require constraints specified item
presentation begins.

415

fiThompson, Goker, & Langley

alternative technique selecting questions ask information elicitation presented Raskutti Zukerman (1997). overall system necessitates
system recognize plans user attempting carry out. system must
decide best complete plans. insufficient information available plan
formation, system enters information seeking subdialogue similar constraintsatisfaction portion dialogues. system decide question ask based
domain knowledge based potential informativeness question.
Another approach dialogue management conversational case-based reasoning
(Aha, Breslow & Munoz-Avila, 2001), relies interactions user retrieve
cases (items) recommend actions correct problem. speech acts
basic flow control much common Adaptive Place Advisor,
process answering questions increasingly constrains available answers. One significant
difference approach generates several questions items, respectively, time,
user selects question answer item closest needs,
respectively.
Finally, approach draws alternative analysis item recommendation, described Elio Haddadi (1998, 1999). main distinctions work
approach include personalization, distinguish search
task space discourse space, combine two,
place greater emphasis user intentions. Keeping distinction task
discourse space personalized system would unnecessarily complicate decisions
perform user model updates utilize model.
5.3 Adaptive Dialogue Systems
Finally, another body recent work describes use machine learning forms
adaptation improve dialogue systems.10 Researchers area develop systems
learn user preferences, improve task completion, adapt dialogue strategies
individual conversation.
closest work also pursues goal learning user preferences. Carberry et al.
(1999) report one example consultation dialogues, take different approach.
system acquires value preferences analyzing users explicit statements
preferences acceptance rejection systems proposals. uses discrete
preference values instead fine-grained probability model. Also, system
use preferences item search item presentation time help evaluate
whether better alternatives exist. Finally, evaluation based subjects judgements
quality systems hypotheses recommendations, characteristics
actual user interactions. could, however, incorporate item search ideas,
allowing near misses user-specified constraints actual items.
Another system focuses user preferences interactive travel assistant (Linden
et al., 1997) carries conversations via graphical interface. system asks
questions goal narrowing available candidates, using speech acts similar
ours, also aims satisfy user interactions possible. approach
10. work adaptation speech recognition grammars (e.g., Stolcke et al., 2000), related, addresses different problem uses different learning techniques, discuss here.

416

fiPersonalized Conversational Recommendation

minimizing number interactions use candidate/critique approach.
users responses, system infers model represented weights attributes price
travel time. Unlike Adaptive Place Advisor, carry profiles
future conversations, one envision version so.
Several authors use reinforcement learning techniques improve probability
process task completion conversation. example, Singh et al. (2002) use
approach determine systems level initiative amount confirmation
user utterances. goal optimize, users, percentage dialogues
given task successfully completed. system leverages learned information
interacting users, rather personalizing information. Also, Levin,
Pieraccini, Eckert (2000) use reinforcement learning determine question
ask point information seeking search, demonstrate utility
approach real users.
Finally, number systems adapt dialogue management strategy course
conversation based user responses dialogue characteristics. example,
Litman Pan (2002) use set learned rules decide whether user difficulty
achieving task, modify level system initiative confirmation accordingly.
Maloor Chai (2000) present help-desk application first classifies user
novice, moderate, expert based responses prompts. adjusts complexity
system utterances, jargon, complexity path taken achieve goals.
Horvitz Paek (2001) apply user modeling dialogue system uses evidence
current context conversation update Bayesian network. network
influences spoken language recognition hypothesis causes appropriate adjustments
systems level initiative. Chu-Carroll (2000) describes system adapts
language generation initiative strategies individual user within single dialogue.
Also, Jameson et al. (1994) use Bayesian networks system take role
either buyer seller transaction, changes inquiry sales strategy
based beliefs inferred participants utterances.

6. Directions Future Work
results date Adaptive Place Advisor promising much remains
done. section, discuss ways make search model flexible,
expand conversational model, enrich user model learning technique.
also consider extensive evaluations system.
6.1 Search Model
respect search mechanism, first plan investigate alternative techniques
using item similarity values determine return, example cutting
items point similarity drops steeply, instead current use
threshold. also note work Cohen, Schapire, Singer (1999)
learning rank instances could apply nicely work, augmenting current
item ranking scheme. Additionally, plan develop version system generates
alternative items values over-constrained situation (Qu & Beale, 1999). One way
would use preferences estimate strength stated constraint,
417

fiThompson, Goker, & Langley

merge preference-based similarity metric traditional domain-specific
similarity metric (Pieraccini et al., 1997). also plan evaluate effect making
even stronger assumptions user preferences. example, system certain
enough value preference, may ask question associated
attribute.
final improvement search mechanism concerns techniques ranking attributes constraining relaxing. attribute constraint ranking, implemented yet evaluated conditional entropy measure (Goker & Thompson, 2000).
system selects attribute constrain determining attribute highest
conditional entropy among unconstrained attributes. scheme would useful
ranking attributes relax. Therefore, system simply determines size case
base would result attribute relaxed, ranks case bases smallest
largest, orders attributes accordingly, excluding attributes that, relaxed,
would still result empty case base. also plan investigate combination
user model information gain, well alternative attribute ranking techniques one used Abella, Brown, Buntschuh (1996). Another option
add personalization otherwise adapt variable selection techniques used
constraint-satisfaction solvers.
6.2 Conversational Model
plan progress towards complex dialogues complex constraints. First,
plan increase number speech acts available user. example, add
confirmation dialogues improve current clarification dialogues, thus allowing
types adaptation strategies, Singh et al. (2002). longer term investigation,
plan extend adaptation techniques handle complex travel planning dialogues
(Seneff, Lau, & Polifroni, 1999; Walker & Hirschman, 2000). may require additions
user model, preferences regarding language dialogue style, including
initiative, system verbosity, vocabulary. turn need appropriately
acquired utilized system. general, insights already gained
utilizing acquiring user preferences different junctures dialogue search
process prove useful supporting personalization tasks.
6.3 User Model
improve user model, first plan add types preferences. discussed
Section 3.1, combination diversity preferences capture complex user behavior
current model, plan incorporate next version
system. Combination preferences help better predict either values acceptable
attributes, based previously provided constraints. Place Advisor model
value combination preferences learning association rules (Agrawal, Imielinski, & Swami,
1993) extending Bayesian network, either would influence query,
turn influencing similarity calculation case base. preferences
acceptable attribute combinations, system learn conditional probabilities based
past interactions use influence attribute ranking.

418

fiPersonalized Conversational Recommendation

drifting preferences likely cause problems item selection applications might ones like news updates, model could extended handle
within-user diversity. One way capture users desired time interval suggestion particular item value. calculate determining
mean time interval users explicit selection rejection value (value diversity
preferences) item (item diversity preferences). incorporate diversity
preferences similarity calculation Section 3.2 extending RI P (Vj )
equation incorporate time effects. define RD (I) PD (Vj ) as:
1

RD (I) = RI

1+

PD (Vj ) = P (Vj )

ekI (ttI tID )
1

,
1+
current time, tI tV time item value last selected,
tID tV time differences user wants item
value suggested again. RD PD form sigmoid function kI kV
determine curves slope. One empirical question whether users also attribute
diversity preferences. hypothesize diversity preferences differ value
attribute, implicitly overrides attribute diversity. example, user
may different preferences frequency expensive restaurants versus
cheap ones suggested, may care often questions price asked.
plan investigate hypothesis.
improvements might add user modeling technique. example, system may learn quickly updates user model dialogue situations
current three. Also, using collaborative user models initialize individual
models could speed learning process. explicit combination collaborative
individual user models (Melville, Mooney, & Nagarajan, 2002; Jameson & Wittig, 2001)
also viable direction explore.
ekV (ttV tV )

6.4 Evaluation
Finally, planning carry larger user study. must verify
differences study due task difficulty differences since control
difficulty finding particular item. particular, different values
constraints may result number matching items. even though two
users answered number questions, number matching items one
user may small enough system begin presenting them, user
may need answer additional questions first. support expanded evaluation,
implemented version system recommends movies, let us draw
broader user base. help us measure user satisfaction easily, Walker
et al. (1998) noted efficiency important consideration,
users might tend prefer predictable interfaces.

419

fiThompson, Goker, & Langley

7. Conclusions
paper, described intelligent adaptive conversational assistant designed help
people select item. Overall, made significant inroads methods unobtrusively
acquiring individual, long term user model recommendation conversations.
expanded previous work adaptive recommendation systems conversational, dialogue systems user adaptive. long-term goal
develop even powerful methods, capable adapting users needs, goals,
preferences multiple conversations. leveraged feedback conversation recommendation, feedback likely present tasks
planning scheduling.
two key problems addressed research design adaptive recommendation systems conversations interaction mode, addition personalization dialogue systems, starting dialogues recommendation. Thus, unlike
many recommendation systems accept keywords produce ranked list, one
carries conversation user progressively narrow options. solving
problems, introduced novel approach acquisition, use, representation
user models. Unlike many adaptive interfaces, system constructs utilizes
user models include information beyond complete item preferences. key
support personalization conversations. used relatively simple model dialogue
focus issues involved personalization. also described experimental results
showing promise technique, demonstrating reduction number
interactions conversation time users interacting adaptive system
compared control group.
course, still several open questions opportunities improvement.
user model, conversational model, search models functional plan improve
further. also extending conversational approach items
destinations, books movies, plan link system assistants
like Adaptive Route Advisor (Rogers et al., 1999). goal additions
provide new functionality make Adaptive Place Advisor attractive
users, also test generality approach adaptive recommendation.
turn, bring us closer truly flexible computational aides carry natural
dialogues humans.

Acknowledgments
research carried first author Center Study
Language Information, Stanford University, authors DaimlerChrysler Research Technology Center Palo Alto, California. thank Renee Elio,
Afsaneh Haddadi, Jeff Shrager initial conception design Adaptive
Place Advisor, Cynthia Kuo Zhao-Ping Tang help implementation effort,
Stanley Peters enlightening discussions design conversational interfaces.
Robert Mertens Dana Dahlstrom crucial carrying user studies.

420

fiPersonalized Conversational Recommendation

Appendix A. Questionnaire
1. think interaction system,
0
1
talk

much

2

3

4
5
right
amount
talking

6

7

8

enough
talking

2. easy find restaurant liked?
0
1

easy

2

4

easy

3. system deliver restaurants liked?
0
yes

1

2

4


4. Please rate interaction system scale standard humancomputer-interaction person person conversation via telephone.
0
1
2
humancomputer
interaction

3

4

5

6
phone
conversation

5. think APA useful system?
0
yes

1

2

3

4


6. think conversation significantly distracting similar conversation real person?
0


1

2

3

4
yes

421

fiThompson, Goker, & Langley

References
Aamodt, A., & Plaza, E. (1994). Case-based reasoning: Foundational issues, methodological
variations, system approaches. Artificial Intelligence Communications, 7, 3959.
Abella, A., Brown, M. K., & Buntschuh, B. (1996). Development principles dialog-based
interfaces. Proceedings ECAI-96 Spoken Dialog Processing Workshop, pp.
17. Budapest, Hungary.
Agrawal, R., Imielinski, T., & Swami, A. (1993). Mining association rules sets
items large databases. Buneman, P., & Jajodia, S. (Eds.), Proceedings
1993 ACM SIGMOD International Conference Management Data, pp. 207216.
Washington, D.C. ACM Press.
Aha, D., & Breslow, L. (1997). Refining conversational case libraries. Proceedings
Second International Conference Case-Based Reasoning, pp. 267278. Providence,
RI. Springer Verlag.
Aha, D., Breslow, L., & Munoz Avila, H. M. (2001). Conversational case-based reasoning.
Applied Intelligence, 14, 932.
Allen, J. (1999). Mixed-initiative interaction. IEEE Intelligent Systems, September/October,
1416.
Allen, J., Byron, D., Dzikovska, M., Ferguson, G., Galescu, L., & Stent, A. (2001). Towards
conversational human-computer interaction. AI Magazine, 22, 2737.
Allen, J., Schubert, L., Ferguson, G., Heeman, P., Hwang, C. H., Kato, T., Light, M.,
Martin, N., Miller, B., Poesio, M., & Traum, D. (1995). TRAINS project:
case study building conversational planning agent. Journal Experimental
Theoretical AI, 7, 748.
Allen, J. F. (1995). Natural language understanding (second edition). Benjamin/Cummings,
Menlo Park, CA.
Ardissono, L., & Goy, A. (2000). Tailoring interaction users web stores. User
Modeling User-Adapted Interaction, 10, 251303.
Ardissono, L., Goy, A., Console, L., & Torre, I. (2001). adaptive system personalized access news. AI Communications, 14, 129147.
Billsus, D., & Pazzani, M. (1998). Learning collaborative information filters. Proceedings
Fifteenth International Conference Machine Learning, pp. 4654. Madison,
WI. Morgan Kaufmann.
Bobrow, D., Kaplan, R., Kay, M., Norman, D., Thompson, H., & Winograd, T. (1977).
Gus, frame driven dialog system. Artificial Intelligence, 8, 155173.

422

fiPersonalized Conversational Recommendation

Bonzano, A., Cunningham, P., & Smyth, B. (1997). Using introspective learning improve
retrieval CBR: case study air traffic control. Proceedings Second International Conference Case-Based Reasoning, pp. 413424. Providence, RI. Springer
Verlag.
Brusilovsky, P., & Maybury, M. (2002). Introduction special section adaptive web.
Communications ACM, 45, 3033.
Burke, R. (1999). Wasabi personal shopper: case-based recommender system.
Proceedings Sixteenth National Conference Artificial Intelligence, pp. 844
849. Orlando, FL. AAAI Press.
Burke, R., Hammond, K., & Young, B. (1996). Knowledge-based navigation complex
information spaces. Proceedings Thirteenth National Conference Artificial
Intelligence, pp. 462468. Portland, OR. AAAI Press.
Carberry, S. (1990). Plan recognition natural language dialogue. MIT Press, Cambridge,
MA.
Carberry, S., Chu-Carroll, J., & Elzer, S. (1999). Constructing utilizing model
user preferences collaborative consultation dialogues. Computational Intelligence
Journal, 15, 185217.
Chin, D. (1989). KNOME: Modeling user knows UC. Kobsa, A., & Wahlster,
W. (Eds.), User models dialog systems, pp. 74107. Springer Verlag, Berlin.
Chu-Carroll, J. (2000). MIMIC: adaptive mixed initiative spoken dialogue system
information queries. Proceedings Sixth Conference Applied Natural Language Processing, pp. 97104. Seattle, WA. AAAI Press.
Cohen, P. R., & Perrault, C. (1979). Elements plan-based theory speech acts.
Cognitive Science, 3, 177212.
Cohen, W., Schapire, R., & Singer, Y. (1999). Learning order things. Journal Artificial
Intelligence Research, 10, 243270.
Cotter, P., & Smyth, B. (2000). PTV: Intelligent personalized TV guides. Proceedings
Twelfth Innovative Applications Artificial Intelligence Conference, pp. 957964.
Austin, TX. AAAI Press.
Cucchiara, R., Lamma, E., Mello, P., & Milano, M. (1997). Interactive constraint satisfaction. Tech. rep. DEIS-LIA-97-00, University Bologna.
Dowding, J., Gawron, J., Appelt, D., Bear, J., Cherny, L., Moore, R., & Moran, D. (1993).
Gemini: natural language system spoken-language understanding. Proceedings Thirty-first Annual Meeting Association Computational Linguistics, pp. 5461. Columbus, OH. Association Computational Linguistics.
Dybkjr, L., Hasida, K., & Traum, D. (Eds.). (2000). Proceedings 1st SIGdial
Workshop Discourse Dialogue, Hong Kong. Association Computational
Linguistics.
423

fiThompson, Goker, & Langley

Eaton, P., Freuder, E., & Wallace, R. (1997). Constraint-based agents: Assistance, cooperation, compromise. Proceedings CP97 Workshop Constraint Reasoning
Internet. Schloss Hagenberg, Austria.
Eliassi-Rad, T., & Shavlik, J. (2001). system building intelligent agents learn
retrieve extract information. User Modeling User-Adapted Interaction, 13,
3588.
Elio, R., & Haddadi, A. (1998). Dialog management adaptive database assistant.
Tech. rep. 98-3, Daimler-Benz research Technology Center, Palo Alto, CA.
Elio, R., & Haddadi, A. (1999). abstract task models conversation policies.
Proceedings Agents99 Workshop Specifying Implementing Conversation
Policies. Seattle, WA.
Ferrario, M., Waters, K., & Smyth, B. (2000). Collaborative maintenance ULYSSES.
Proceedings International Conference Adaptive Hypermedia Adaptive
Web-based Systems, pp. 301304. Trento, Italy.
Fiechter, C., & Rogers, S. (2000). Learning subjective functions large margins.
Proceedings Seventeenth International Conference Machine Learning, pp.
287294. Stanford University, CA. Morgan Kaufmann.
Goddeau, D., Meng, H., Polifroni, J., Seneff, S., & Busayapongchai, S. (1996). form-based
dialogue manager spoken language applications. Proceedings Fourth International Conference Spoken Language Processing, Vol. 2, pp. 701704. Philadelphia, PA.
Goecks, J., & Shavlik, J. (2000). Learning users interests unobtrusively observing
normal behavior. Proceedings 2000 International Conference Intelligent
User Interfaces, pp. 129132. New Orleans, LA. ACM Press.
Goker, M., & Roth-Berghofer, T. (1999). development utilization case-based
help-desk support system HOMER. Engineering Applications Artificial Intelligence,
12, 665680.
Goker, M., & Thompson, C. (2000). Personalized, conversational case-based recommendation. Proceedings Fifth European Workshop Case-Based Reasoning, pp.
99111. Trento Italy. Springer Verlag.
Haller, S., & McRoy, S. (1998). Preface special issue computational models mixedinitiative interaction. User Modeling User-Adapted Interaction, 8, 167170.
Horvitz, E., & Paek, T. (2001). Harnessing models users goals mediate clarification dialog spoken language systems. Proceedings Eighth International
Conference User Modeling, pp. 201210. Sonthofen, Germany. Springer.
Jameson, A., Kipper, B., Ndiaye, A., Schafer, R., Simons, J., Weis, T., & Zimmermann,
D. (1994). Cooperating noncooperative: dialog system PRACMA.
424

fiPersonalized Conversational Recommendation

Proceedings KI-94: Advances Artificial Intelligence, pp. 106117. Seattle, WA.
Morgan Kaufmann.
Jameson, A., & Wittig, F. (2001). Leveraging data users general learning individual user models. Proceedings Seventeenth International Joint
Conference Artificial Intelligence, pp. 11851192. Seattle, WA. Morgan Kaufmann.
Jurafsky, D., & Martin, J. (2000). Speech language processing. Prentice Hall.
Jurafsky, D., Wooters, C., Tajchman, G., Segal, J., Stolcke, A., Fosler, E., & Morgan, N.
(1994). Berkeley restaurant project. Proceedings International Conference Spoken Language Processing, pp. 21392142. Yokohama, Japan.
Kass, R. (1991). Building user model implicitly cooperative advisory dialog. User
Modeling User-Adapted Interaction, 3, 203258.
Kay, J., & Thomas, R. C. (2000). Personal usability based upon scrutable, dynamic,
individual user model. Proceedings Australasian Computer Human Interfaces
Conference, pp. 292298.
Kobsa, A., & Wahlster, W. (Eds.). (1989). User models dialog systems. Springer, New
York.
Konstan, J., Miller, B., Maltz, D., Herlocker, J., Gordon, L., & Riedl, J. (1997). Grouplens:
Applying collaborative filtering usenet news. Communications ACM, 40,
7787.
Kumar, V. (1992). Algorithms constraint-satisfaction problems: survey. AI
Magazine, 13, 3244.
Lang, K. (1995). NewsWeeder: Learning filter netnews. Proceedings Twelfth
International Conference Machine Learning, pp. 331339. San Francisco, CA. Morgan Kaufmann.
Langley, P. (1999). User modeling adaptive interfaces. Proceedings Seventh
International Conference User Modeling, pp. 357370. Banff, Alberta. Springer.
Levin, E., Pieraccini, R., & Eckert, W. (2000). stochastic model human-machine
interaction learning dialog strategies. IEEE Transactions Speech Audio
Processing, 8, 1123.
Linden, G., Hanks, S., & Lesh, N. (1997). Interactive assessment user preference models:
automated travel assistant. Proceedings Sixth International Conference
User Modeling, pp. 6778. Chia Laguna, Sardinia. Springer.
Litman, D., & Pan, S. (2002). Designing evaluating adaptive spoken dialogue system.
User Modeling User-Adapted Interaction, 12, 111137.
Maier, E., Mast, M., & Luperfoy, S. (Eds.). (1996). Proceedings ECAI96 workshop
Dialogue processing spoken language systems, Budapest, Hungary. Springer Verlag.
425

fiThompson, Goker, & Langley

Maloor, P., & Chai, J. (2000). Dynamic user level utility measurement adaptive
dialog help-desk system. Proceedings 1st SIGdial Workshop Discourse
Dialogue, pp. 94101 Hong Kong. Association Computational Linguistics.
McNee, S., Lam, S., Konstan, J., & Riedl, J. (2003). Interfaces eliciting new user preferences recommender systems. Proceedings Ninth International Conference
User Modeling, pp. 178188. Johnstown, PA. Springer.
Melville, P., Mooney, R., & Nagarajan, R. (2002). Content-boosted collaborative filtering
improved recommendations. Proceedings Eighteenth National Conference
Artificial Intelligence, pp. 187192. Edmonton, Canada. AAAI Press.
Pazzani, M., Muramatsu, J., & Billsus, D. (1996). Syskill & Webert: Identifying interesting web sites. Proceedings Thirteenth National Conference Artificial
Intelligence, pp. 5461. Portland, OR. AAAI Press.
Pieraccini, R., Levin, E., & Eckert, W. (1997). AMICA: AT&T mixed initiative
conversational architecture. Proceedings European Conference Speech
Communication Technology, pp. 18751878. Rhodes, Greece.
Qu, Y., & Beale, S. (1999). constraint-based model cooperative response generation
information dialogues. Proceedings Sixteenth National Conference
Artificial Intelligence, pp. 148155. Orlando, FL. AAAI Press.
Rafter, R., Bradley, K., & Smyth, B. (2000). Personalized retrieval online recruitment
services. Proceedings Twenty-second Annual Colloquium Information
Retrieval. Cambridge, UK.
Raskutti, B., & Zukerman, I. (1997). Generating queries replies informationseeking interactions. International Journal Human Computer Studies, 47, 689734.
Resnick, P., & Varian, H. (1997). Recommender systems. Communications ACM,
40 (3), 5658.
Rich, E. (1979). User modeling via stereotypes. Cognitive Science, 3, 329354.
Rogers, S., Fiechter, C., & Langley, P. (1999). adaptive interactive agent route
advice. Proceedings Third International Conference Autonomous Agents,
pp. 198205. Seattle, WA. ACM Press.
Sadek, M., Bretier, P., & Panaget, F. (1997). ARTIMIS: Natural dialogue meets rational
agency. Proceedings Fifteenth International Joint Conference Artificial
Intelligence, pp. 10301035. Nagoya, Japan. Morgan Kaufmann.
Segal, R., & Kephart, J. (1999). Mailcat: intelligent assistant organizing e-mail.
Proceedings Third International Conference Autonomous Agents, pp. 276
282. Seattle, WA. ACM Press.

426

fiPersonalized Conversational Recommendation

Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., & Zue, V. (1996). Galaxy-II:
reference architecture conversational system development. Proceedings
International Conference Spoken Language Processing, pp. 931934. Sydney, Australia.
Seneff, S., Lau, R., & Polifroni, J. (1999). Organization, communication, control
Galaxy-II conversational system. Proceedings Eurospeech 1999, pp. 12711274.
Budapest, Hungary.
Shardanand, U., & Maes, P. (1995). Social information filtering: Algorithms automating
word mouth. Proceedings Conference Human Factors Computing
Systems, pp. 210217. Denver, CO. ACM Press.
Shearin, S., & Lieberman, H. (2001). Intelligent profiling example. Proceedings
International Conference Intelligent User Interfaces, pp. 145152. Santa Fe, NM.
ACM Press.
Singh, S., Litman, D., Kearns, M., & Walker, M. (2002). Optimizing dialogue management reinforcement learning: Experiments NJFun system. Journal
Artificial Intelligence Research, 16, 105133.
Smith, R., & Hipp, D. (1994). Spoken natural language dialog systems: practical approach.
Oxford University Press, New York, NY.
Smyth, B., & Cotter, P. (1999). Surfing digital wave, generating personalized TV listings
using collaborative, case-based recommendation. Proceedings Third International Conference Case-Based Reasoning, pp. 561571. Monastery, Germany.
Springer Verlag.
Stent, A., Dowding, J., Gawron, J., Bratt, E., & Moore, R. (1999). CommandTalk
spoken dialogue system. Proceedings Thirty-seventh Annual Meeting
Association Computational Linguistics, pp. 183190. College Park, MD. Association Computational Linguistics.
Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin,
R., Ess-Dykema, C. V., & Meteer, M. (2000). Dialog act modeling automatic
tagging recognition conversational speech. Computational Linguistics, 26, 339
373.
Walker, M., Fromer, J., Fabbrizio, G., Mestel, C., & Hindle, D. (1998). say?:
Evaluating spoken language interface email. Proceedings ACM CHI 98
Conference Human Factors Computing Systems, pp. 582589. Los Angeles, CA.
ACM Press.
Walker, M., & Hirschman, L. (2000). Evaluation DARPA communicator spoken dialogue
systems. Proceedings Second International Conference Language Resources
Evaluation. Athens, Greece.

427

fiThompson, Goker, & Langley

Ward, W., & Issar, S. (1996). Recent improvements CMU spoken language understanding system. Proceedings ARPA Human Language Technology Workshop,
pp. 213216.
Wettschereck, D., & Aha, D. (1995). Weighting features. Proceedings First International Conference Case-Based Reasoning, pp. 347358. Sesimbra, Portugal.
Springer Verlag.
Winograd, T., & Flores, F. (1986). Understanding computers cognition: new foundation design. Ablex Publishing, Northwood, NJ.
Zhang, Z., & Yang, Q. (1998). Towards lifetime maintenance case base indexes
continual case based reasoning. Proceedings 1998 International Conference
AI Methodologies, Systems Applications, pp. 489500. Bulgaria. Springer.
Zukerman, I., & Litman, D. (2001). Natural language processing user modeling: Synergies limitations. User Modeling User-Adapted Interaction, 11, 129158.

428

fiJournal Artificial Intelligence Research 21 (2004) 245286

Submitted 07/2003; published 03/2004

Coherent Integration Databases
Abductive Logic Programming
Ofer Arieli

oarieli@mta.ac.il

Department Computer Science, Academic College Tel-Aviv,
4 Antokolski street, Tel-Aviv 61161, Israel.

Marc Denecker
Bert Van Nuffelen
Maurice Bruynooghe

Marc.Denecker@cs.kuleuven.ac.be
Bert.VanNuffelen@cs.kuleuven.ac.be
Maurice.Bruynooghe@cs.kuleuven.ac.be
Department Computer Science, Katholieke Universiteit Leuven,
Celestijnenlaan 200A, B-3001 Heverlee, Belgium.

Abstract
introduce abductive method coherent integration independent datasources. idea compute list data-facts inserted amalgamated database retracted order restore consistency. method
implemented abductive solver, called Asystem, applies SLDNFA-resolution
meta-theory relates different, possibly contradicting, input databases. also
give pure model-theoretic analysis possible ways recover consistent data
inconsistent database terms models database exhibit minimal
inconsistent information reasonably possible. allows us characterize recovered databases terms preferred (i.e., consistent) models theory.
outcome abductive-based application sound complete respect
corresponding model-based, preferential semantics, best knowledge
expressive (thus general) implementation coherent integration
databases.

1. Introduction
Complex reasoning tasks often integrate information coming different
sources. One major challenges respect compose contradicting sources
information obtained would properly reflect combination datasources one hand1 , would still coherent (in terms consistency)
hand. number different issues involved process, important
following:
1. Unification different ontologies and/or database schemas, order get fixed
(global) schema, translation integrity constraints2 database
new ontology.
2. Unification translated integrity constraints single global set integrity constraints. means, particular, elimination contradictions among translated
1. property sometimes called compositionality (Verbaeten, Denecker, & De Schreye, 1997, 2000).
2. I.e., rules represent intentional truths database domain.
c
2004
AI Access Foundation. rights reserved.

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

integrity constraints, inclusion global integrity constraint imposed
integration process.
3. Integration databases w.r.t. unified set integrity constraints, computed according previous item.
one issues mentioned difficulties challenges.
instance, first issue considered, e.g., Ullman (2000) Lenzerini (2001, 2002),
questions express relations global database schema
source (local) schemas, influences query processing respect
global schema (Bertossi, Chomicki, Cortes, & Gutierrez, 2002), dealt with3 .
second issue concerned construction single, classically consistent, set integrity constraints, applied integrated data. database context,
common assume set pre-defined, consists global integrity constraints imposed integration process (Bertossi et al., 2002; Lenzerini,
2002). case need derive constraints local databases.
different integrity constraints specified different local databases, required
integrate database instances (as specified issue 3 above), also
integrity constraints (issue 2). reason separating two topics
integrity constraints represent truths valid situations, database
instance exhibits extensional truth, i.e., actual situation. Consequently, policy
resolving contradictions among integrity constraints often different one
applied database facts, often former applied latter.
Despite different nature, issues based formalisms maintain contradictions allow draw plausible conclusions inconsistent situations.
Roughly, two approaches handle problem:
Paraconsistent formalisms, amalgamated data may remain inconsistent,
set conclusions implied explosive, i.e.: every fact follows
inconsistent database, inference process become trivial
presence contradictions. Paraconsistent procedures integrating data, like
Subrahmanian (1994) de Amo, Carnielli, Marcos (2002), often
based paraconsistent reasoning systems, LFI (Carnielli & Marcos, 2001),
annotated logics (Subrahmanian, 1990; Kifer & Lozinskii, 1992; Arenas, Bertossi, &
Kifer, 2000), non-classical proof procedures (Priest, 1991; Arieli & Avron,
1996; Avron, 2002; Carnielli & Marcos, 2002)4 .
Coherent (consistency-based) methods, amalgamated data revised
order restore consistency (see, e.g., Baral, Kraus, & Minker, 1991; Baral, Kraus,
Minker, & Subrahmanain, 1992; Benferhat, Dubois, & Prade, 1995; Arenas, Bertossi,
3. surveys schema matching related aspects, see also (Batini, Lenzerini, & Navathe, 1986)
(Rahm & Bernstein, 2001).
4. See also (Decker, 2003) historical perspective computational remarks kind
formalisms.

246

fiCoherent integration databases abductive logic programming

& Chomicki, 1999; Arieli & Avron, 1999; Greco & Zumpano, 2000; Liberatore &
Schaerf, 2000; Bertossi & Schwind, 2002; Arieli, Denecker, Van Nuffelen, & Bruynooghe,
2004). many cases underlying formalisms approaches closely related theory belief revision (Alchourron, Gardenfors, & Makinson, 1995;
Gardenfors & Rott, 1995). context database systems idea consider
consistent databases close possible original database. repaired instances spoiled database correspond plausible compact ways
restoring consistency.
paper follow latter approach, consider abductive approaches handle third issue above, namely: coherent methods integrating different data-sources
(with ontology) w.r.t. consistent set integrity constraints5 . main difficulty process stems fact even local database consistent,
collective information data-sources may remain consistent anymore.
particular, facts specified particular database may violate integrity constraints defined elsewhere, data might contradict elements unified
set integrity constraints. Moreover, noted e.g. (Lenzerini, 2001; Cali, Calvanese,
De Giacomo, & Lenzerini, 2002), ability handle, plausible way, incomplete
inconsistent data, inherent property system data integration integrity
constrains, matter integration phase considered. Providing proper ways gaining property major concern well.
goal therefore find ways properly repair combined (unified) database,
restore consistency. this, consider pure declarative representation
composition distributed data meta-theory, relating number different input
databases (that may contradict other) consistent output database. underlying language theory abductive logic programming (Kakas, Kowalski,
& Toni, 1992; Denecker & Kakas, 2000). reasoning theories use abductive system, called Asystem (Kakas, Van Nuffelen, & Denecker, 2001; Van Nuffelen &
Kakas, 2001), abductive solver implementing SLDNFA-resolution (Denecker &
De Schreye, 1992, 1998). composing system implemented abductive reasoning
meta-theory. context work, extended system
optimizing component allows us compute preferred coherent ways restore
consistency given database. system obtained induces operational semantics database integration. sequel also consider model-theoretic aspects
problem, define preferential semantics (Shoham, 1988) it. According
semantics, repaired databases characterized terms preferred models
(i.e., most-consistent valuations) underlying theory. relate approaches
showing Asystem sound complete w.r.t. model-based semantics.
also noted framework supports reasoning various types special information,
timestamps source identification. implementation issues experimen5. sense, one may view work method restoring consistency single inconsistent
database. prefer, however, treat integration process multiple sources, since also
mediating capabilities, source identification, making priorities among different data-sources,
etc. (see, e.g., Section 4.6).

247

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

tal results discussed well.
rest paper organized follows: next section formally define
goal, namely: coherent way integrate different data-sources. Section 3 set
semantics goal terms corresponding model theory. Then, Section 4
introduce abductive-based application database integration. main
section paper, also describe given integration problem
represented terms meta logic programs, show reason programs
abductive computational models, present experimental results, consider proper ways
reasoning several types special data, show application sound
complete respect model-based semantics, considered Section 3. Section 5
contains overview related works, Section 6 conclude
remarks, open issues, future work6 .

2. Coherent Integration Databases
begin formal definition goal. paper assume
first-order language L, based fixed database schema S, fixed domain D. Every
element unique name. database instance consists atoms language
L instances schema S. such, every instance finite active domain,
subset D.
Definition 1 database pair (D, IC), database instance, IC,
set integrity constraints, finite classically consistent set formulae L.
Given database DB = (D, IC), apply closed word assumption,
facts explicitly mentioned considered true. underlying semantics
corresponds, therefore, minimal Herbrand interpretations.
Definition 2 minimal Herbrand model HD database instance model
assigns true ground instances atomic formulae D, false
atoms.
different views database. One view logic theory consisting
atoms and, implicitly, closed world assumption (CWA) indicates atoms
database false. Another common view database structure
consists certain domain corresponding relations, representing state
world. Whenever complete knowledge true atoms represented
database, views coincide: unique Herbrand model theory intended
structure. However, context independent data-sources, assumption
local database represents state world obviously false. However, still
view local database incomplete theory, treating database theory rather
structure appropriate case.
6. combined extended version (Arieli, Van Nuffelen, Denecker, & Bruynooghe, 2001)
(Arieli, Denecker, Van Nuffelen, & Bruynooghe, 2002).

248

fiCoherent integration databases abductive logic programming

Definition 3 formula follows database instance (alternatively, entails ;
notation: |= ) minimal Herbrand model also model .
Definition 4 database DB = (D, IC) consistent every formula IC follows
(notation: |= IC).
goal integrate n consistent local databases, DB = (Di , IC ) (i = 1, . . . n) one
consistent database contains much information possible local databases.
idea, therefore, consider union distributed data, restore
consistency way much information possible preserved.
Notation 1 Let DB = (Di , IC ), = 1, . . . n, let I(IC 1 , . . . , IC n ) classically
consistent set integrity constraints. denote:
UDB = (

n
[

Di , I(IC 1 , . . . , IC n )).

i=1

notation above, operator combines integrity constraints eliminates contradictions (see, e.g., Alferes, Leite, Pereira, & Quaresma, 2000; Alferes, Pereira,
Przymusinska, & Przymusinski, 2002). already noted, choose operator apply specific database beyond scope paper. cases
union integrity constraints classically consistent, makes sense take
union operator. Global consistency integrity constraints indeed common
assumption database literature (Arenas et al., 1999; Greco & Zumpano, 2000; Greco,
Greco, & Zumpano, 2001; Bertossi et al., 2002; Konieczny & Pino Perez, 2002; Lenzerini,
2002), discussion possible take, instead union, operator
consistency restoration.
key notion database integration following:
Definition 5 repair database DB = (D, IC) pair (Insert, Retract), that:
1. Insert = ,
2. Retract D7 ,
3. (D Insert \ Retract, IC) consistent database.
Intuitively, Insert set elements inserted Retract set
elements removed order consistent database.
noted above, repair given database key notion many formalisms
data integration. context database systems, notion first introduced
Arenas, Bertossi, Chomicki (1999), later considered many others (e.g., Greco
& Zumpano, 2000; Liberatore & Schaerf, 2000; Franconi, Palma, Leone, Perri, & Scarcello,
2001; Bertossi et al., 2002; Bertossi & Schwind, 2002; de Amo et al., 2002; Arenas, Bertossi,
& Chomicki, 2003; Arieli et al., 2004). Earlier versions repairs inclusion-based
consistency restoration may traced back Dalal (1988) Winslett (1988).
7. Note conditions (1) (2) follows Insert Retract = .

249

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

Definition 6 repaired database DB = (D, IC) consistent database form
(D Insert \ Retract , IC), (Insert, Retract) repair DB.
may many ways repair inconsistent database8 , often convenient
make preferences among possible repairs, consider preferred ones.
two common preference criteria preferring repair (Insert, Retract)
repair (Insert0 , Retract0 ):
Definition 7 Let (Insert, Retract) (Insert0 , Retract0 ) two repairs given database.
set inclusion preference criterion : (Insert0 , Retract0 ) (Insert, Retract),
Insert Insert0 Retract Retract0 .
minimal cardinality preference criterion: (Insert0 , Retract0 ) c (Insert, Retract),
|Insert| + |Retract| |Insert0 | + |Retract0 |.
Set inclusion also considered (Arenas et al., 1999; Greco & Zumpano, 2000; Bertossi
et al., 2002; Bertossi & Schwind, 2002; de Amo et al., 2002; Arenas et al., 2003; Arieli et al.,
2004, others), minimal cardinality considered, e.g., (Dalal, 1988; Liberatore &
Schaerf, 2000; Arenas et al., 2003; Arieli et al., 2004).
follows assume preference relation fixed pre-order
represents preference criterion set repairs (and shall omit subscript
notations whenever possible). shall also assume (, ) valid repair,
-least (i.e., best) one. corresponds intuition database
repaired unless inconsistent.
Definition 8 -preferred repair DB repair (Insert, Retract) DB, s.t. every
repair (Insert0 , Retract0 ) DB, (Insert, Retract) (Insert0 , Retract0 ) (Insert0 , Retract0 )
(Insert, Retract). set -preferred repairs DB denoted !(DB, ).
Definition 9 -repaired database DB repaired database DB, constructed
-preferred repair DB. set -repaired databases denoted by:
R(DB, ) = { (D Insert \ Retract , IC) | (Insert, Retract) !(DB, ) }.
Note DB consistent preference relation, DB repaired database (thus, nothing repair case, expected).
Note 1 usual refer -preferred databases DB consistent databases
close possible DB (see, e.g., Arenas, Bertossi, & Chomicki, 1999;
Liberatore & Schaerf, 2000; de Amo, Carnielli, & Marcos, 2002; Konieczny & Pino Perez,
2002; Arenas, Bertossi, & Chomicki, 2003; Arieli, Denecker, Van Nuffelen, & Bruynooghe,
2004). Indeed, let
dist(D1 , D2 ) = (D1 \ D2 ) (D2 \ D1 ).
8. repairs may trivial and/or useless, though. instance, one way eliminate inconsistency
(D, IC) = ({p, q, r}, {p}) deleting every element D, certainly optimal way
restoring consistency case.

250

fiCoherent integration databases abductive logic programming

easy see DB 0 = (D0 , IC) -repaired database DB = (D, IC), set
dist(D0 , D) minimal (w.r.t. set inclusion) among sets form dist(D00 , D),
D00 |= IC. Similarly, |S| denotes size S, DB 0 = (D0 , IC) c -repaired
database DB = (D, IC), |dist(D0 , D)| = min{|dist(D00 , D)| | D00 |= IC}.
Given n databases preference criterion , goal therefore compute set
R(UDB, ) -repaired databases unified database, UDB (Notation 1).
reasoner may use different strategies determine consequences set. Among
common approaches skeptical (conservative) one, based consensus
among elements R(UDB, ) (see Arenas et al., 1999; Greco & Zumpano, 2000),
credulous approach, entailments determined element R(UDB, ),
approach based majority vote (Lin & Mendelzon, 1998; Konieczny &
Pino Perez, 2002), etc. cases processing time major consideration, one may
want speed-up computations considering repaired database. cases
sufficient find arbitrary element set R(UDB, ).
examples9 integration process10 .
Example 1 Consider relation teaches schema (course name, teacher name),
integrity constraint, stating course cannot taught two different
teachers:
IC = { XY Z (teaches(X, ) teaches(X, Z) = Z) }.
Consider following two databases:
DB 1 = ( {teaches(c1 , n1 ), teaches(c2 , n2 )}, IC ),
DB 2 = ( {teaches(c2 , n3 )}, IC).
Clearly, unified database DB 1 DB 2 inconsistent. two preferred repairs,
(, {teaches(c2 , n3 )}) (, {teaches(c2 , n2 )}). corresponding repaired databases
following:
R1 = ( {teaches(c1 , n1 ), teaches(c2 , n2 )}, IC ),
R2 = ( {teaches(c1 , n1 ), teaches(c2 , n3 )}, IC ).
Thus, e.g., teaches(c1 , n1 ) true conservative approach credulous
approach database integration, conclusion teaches(c2 , n2 ) supported
credulous reasoning.
Example 2 Consider databases relations class supply, schemas (item, type)
(supplier, department, item), respectively. Let
DB 1 = ({supply(c1 , d1 , i1 ), class(i1 , t1 )}, IC),
DB 2 = ({supply(c2 , d2 , i2 ), class(i2 , t1 )}, ),
IC = { XY Z (supply(X, Y, Z)class(Z, t1 ) X = c1 ) } states supplier
9. See, e.g., (Arenas et al., 1999; Greco & Zumpano, 2000; Bertossi & Schwind, 2002) discussions
examples.
10. following examples use set inclusion preference criterion, take operator
combines integrity constraints (see Notation 1) union operator.

251

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

c1 supply items type t1 . Again, DB 1 DB 2 inconsistent, two preferred
repairs: (, {supply(c2 , d2 , i2 )}) (, {class(i2 , t1 )}). follows two repairs
database:
R1 = ( {supply(c1 , d1 , i1 ), class(i1 , t1 ), class(i2 , t1 )}, IC ),
R2 = ( {supply(c1 , d1 , i1 ), supply(c2 , d2 , i2 ), class(i1 , t1 )}, IC ).
Example 3 Let D1 = {p(a), p(b)}, D2 = {q(a), q(c)}, IC = {X(p(X) q(X))}. Again,
(D1 , ) (D2 , IC) inconsistent. corresponding preferred repairs ({q(b)}, )
(, {p(b)}). Thus, repaired databases following:
R1 = ( {p(a), p(b), q(a), q(b), q(c)}, IC ),
R2 = ( {p(a), q(a), q(c)}, IC ).
case, then, consensus approach credulous approach allow
infer, e.g., p(a) holds, p(b) supported credulous reasoning, p(c)
supported either approaches.

3. Model-based Characterization Repairs
section set semantics describing repairs preferred repairs terms
corresponding model theory. allow us, particular, give alternative
description preferred repairs, time terms preferential semantics database
theory.
database semantics usually defined terms two-valued (Herbrand) models
(cf. Definition 2 discussion proceeds it), natural consider two-valued
semantics first. show arbitrary repairs represented two-valued models
integrity constraints. database inconsistent, definition,
two-valued interpretation satisfies database instance integrity
constraints. standard way cope type inconsistencies move multiplevalued semantics reasoning inconsistent incomplete information (see, e.g.,
Subrahmanian, 1990, 1994; Messing, 1997; Arieli & Avron, 1999; Arenas, Bertossi, & Kifer,
2000; de Amo, Carnielli, & Marcos, 2002). show below, repairs
characterized three-valued models whole database, is, database instance
integrity constraints. Finally, concentrate preferred repairs,
show certain subset three-valued models used characterizing preferred repairs.
Definition 10 Given valuation truth value x. Denote:
x = {p | p atomic formula, (p) = x}11 .
following two propositions characterize repairs terms two-valued structures.
Proposition 1 Let (D, IC) database let two-valued model IC. Let
Insert = \ Retract = \ . (Insert, Retract) repair (D, IC).
11. Note, particular, terms Definition 2, = HD x = t, x = D.

252

fiCoherent integration databases abductive logic programming

Proof: definitions Insert Retract immediately imply Insert =
Retract D. last condition Definition 5, note Insert \ Retract =
(M \D)\(D\M ) = . follows least Herbrand model DInsert\Retract
also model IC, therefore Insert \ Retract |= IC.
2
Proposition 2 Let (Insert, Retract) repair database (D, IC).
two-valued model IC Insert = \ Retract = \ .
Proof: Consider valuation , defined every atom p follows:
(

(p) =



p Insert \ Retract,

f

otherwise.

definition, minimal Herbrand model Insert \ Retract. Now, since
(Insert, Retract) repair (D, IC), Insert \ Retract |= IC, thus
(two-valued) model IC. Moreover, since (Insert, Retract) repair, necessarily
Insert = Retract D, hence following:
\ = (D Insert \ Retract) \ = Insert,
2

\ = \ (D Insert \ Retract) = Retract.

database inconsistent, models satisfy integrity constraints database instance. One common method overcome inconsistency
introduce additional truth-values intuitively represent partial knowledge, different
amounts beliefs, etc. (see, e.g., Priest, 1989, 1991; Subrahmanian, 1990; Fitting, 1991;
Arieli, 1999; Arenas et al., 2000; Avron, 2002). follow guideline, consider
database integration context three-valued semantics. benefit that,
show below, database three-valued models, possible
pinpoint inconsistent information, accordingly construct repairs.
underlying three-valued semantics considered induced algebraic structure HREE, shown double-Hasse diagram Figure 112 .
k
6

>

u
@
@
@
@
@
@u

u

f





-t

Figure 1: structure HREE
Intuitively, elements f HREE correspond usual classical values
true false, third element, >, represents inconsistent information (or belief).
12. structure used reasoning inconsistency several three-valued formalisms,
LFI (Carnielli & Marcos, 2001, 2002) LP (Priest, 1989, 1991).

253

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

Viewed horizontally, HREE complete lattice. According view, f minimal
element, maximal one, > intermediate element. corresponding order relation, , intuitively represents differences amount truth element
exhibits. denote meet, join, order reversing operation , ,
(respectively). Viewed vertically, HREE semi-upper lattice. view, >
maximal element two classical values incomparable. partial order,
denoted k , may intuitively understood representing differences amount
knowledge (or information) element represents13 . denote join
operation k 14 .
Various semantic notions defined HREE natural generalizations similar
classical ones: valuation function assigns truth value HREE
atomic formula. Given valuation , truth values xi {t, f, >}, atomic formulae pi ,
shall sometimes write = {pi : xi } instead (pi ) = xi (i = 1, 2 . . .). valuation
extended complex formulae obvious way. instance, () = (), ( ) =
() (), forth15 . set designated truth values HREE (i.e.,
elements HREE represent true assertions) consists >. valuation
satisfies formula iff () designated. valuation assigns designated value
every formula theory (three-valued) model .
Lemma 1 Let three-valued valuations s.t. every atom p, (p) k (p).
every formula , () k ().
2

Proof: induction structure .

shall write k , three-valued valuations, condition
Lemma 1 holds.
Lemma 2 k model theory , also model .
Proof: every formula , () designated. Hence, Lemma 1, every formula
() also designated, model .
2
Next characterize repairs database DB three-valued models:
Proposition 3 Let (D, IC) database let two-valued model IC. Consider
three-valued valuation N , defined every atom p N (p) = HD (p) (p), let
Insert = N > \ D, Retract = N > D. Then:
1. N three-valued model IC,
13. See (Belnap, 1977; Ginsberg, 1988; Fitting, 1991) detailed discussion orders
intuitive meaning.
14. follow notations Fitting (1990, 1991).
15. usual, use logical symbol denote connective appear left-hand
side equation, corresponding operator HREE appear right-hand side
equation.

254

fiCoherent integration databases abductive logic programming

2. (Insert, Retract) repair (D, IC).
Proposition 3 shows repairs database (D, IC) may constructed standard
(uniform) way considering three-valued models k -least upper bounds
two two-valued valuations: minimal Herbrand model database instance,
two-valued model integrity constraints. Proposition 4 shows repair
(D, IC) form.
give proof Proposition 3, lets demonstrate simple example.
Example 4 Let DB = ({p, r} , {p q}). HD = {p : t, q : f, r : t}, two-valued
models IC = {p q} {p : t, q : t, r : t}, {p : t, q : t, r : f }, {p : f, q : t, r : t}, {p : f, q : t, r : f },
{p : f, q : f, r : t}, {p : f, q : f, r : f }. Thus, (three-valued) models form HD ,
two-valued model IC, {p : t, q : >, r : t}, {p : t, q : >, r : >}, {p : >, q : >, r : t},
{p : >, q : >, r : >}, {p : >, q : f, r : t} {p : >, q : f, r : >}. Proposition 3, pairs
({q}, {}), ({q}, {r}), ({q}, {p}), ({q}, {p, r}), ({}, {p}), ({}, {p, r}), repairs DB.
Proof Proposition 3: Since definition N , N k HD , since HD model
D, Lemma 2 implies N model D. Similarly, N k , model IC,
thus lemma N also model IC.
second part, observe Insert = N > \ = \ Retract = N > =
f
= \ . Now, two-valued model IC hence, Proposition 1,
(Insert, Retract) repair (D, IC).
2
Note specific form three-valued valuations considered Proposition 3
essential here, proposition hold every three-valued model IC.
see consider, e.g., = {}, IC = {p , p q}, three valued valuation N
assigns > p q. Clearly, N model IC, corresponding update,
(N > \ , N > D) = ({p}, {}) repair (D, IC), since ({p}, IC) consistent
database.
Again, noted above, possible show converse Proposition 3
also true:
Proposition 4 Let (Insert, Retract) repair database (D, IC).
three-valued model N IC, Insert = N > \ Retract = N > D.
Proof: Consider valuation N , defined every atom p follows:
N (p) =



>

p Insert Retract,



p 6 Insert Retract p D,




f

otherwise.

definition N since (Insert, Retract) repair (D, IC), N > \D =
(Insert Retract) \ = Insert N > = (Insert Retract) = Retract.
remains show N (three-valued) model IC. three-valued
model every p D, N (p) {t, >}. Regarding IC, (Insert, Retract)
255

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

repair (D, IC), thus every formula IC true least Herbrand model
D0 = Insert \ Retract. particular, (q) = every q D0 . since every
p DInsert N (p) {t, >} D0 DInsert, necessarily q D0 N (q) {t, >}.
follows every q D0 , N (q) k (q) = t, thus Lemma 1 Lemma 2, N must
also (three-valued) model D0 . Hence N model IC.
2
last two propositions characterize repairs UDB terms pairs
associated certain three-valued models IC. shall denote elements
pairs follows:
Notation 2 Let N three-valued model let DB = (D, IC) database. Denote:
InsertN = N > \ RetractN = N > D.
conclude model-based analysis characterizing set -preferred
repairs, one preference criteria considered Definition 7 (i.e., set inclusion
minimal cardinality). propositions show, common considerations
inconsistent databases properly recovered (e.g., keeping amount changes
minimal possible, close possible original instance, etc.)
captured preferential models context preferential semantics (Shoham, 1988).
idea define order relation set (three-valued) models
database. relation intuitively captures criterion making preferences among
relevant models. Then, preferred models (those minimal
respect underlying order relation) considered order determine
database repaired. formalize idea:
Definition 11 Given database DB = (D, IC), denote:
MDB = {N | N k HD classical model IC}16 .
Example 5 Consider database DB = ({p, r} , {p q}) Example 4.
shown, six valuations form HD , two-valued model
IC, namely:
{p : , q : >, r : t},

{p : , q : >, r : >},

{p : >, q : >, r : t},

{p : >, q : >, r : >},

{p : >, q : f, r : t},

{p : >, q : f, r : >}.

k-minimal models among models {p : t, q : >, r : t} {p : >, q : f, r : t}, thus
MDB = {N | N (p) k t, N (q) = >, N (r) k t} {N | N (p) = >, N (q) k f, N (r) k t}.
Preference orders reflect normality considerations applied relevant
set valuations (MDB , case); preferable , describes situation
common (plausible) one described . Hence, natural way
define preferences case minimizing inconsistencies. thus get following
definition:
Definition 12 Let set three-valued valuations, N1 , N2 S.
16. Note N three-valued valuation two-valued model IC.

256

fiCoherent integration databases abductive logic programming

N1 -more consistent N2 , N1> N2> .
N1 c -more consistent N2 , |N1> | < |N2> |.
N -maximally consistent (respectively, N c -maximally consistent
S), N 0 -more consistent N (respectively, N 0
c -more consistent N ).
following propositions show close relationship consistent models MDB preferred repairs DB.
Proposition 5 N -maximally consistent element MDB , (InsertN , RetractN )
-preferred repair DB.
Proof: Proposition 3, (InsertN , RetractN ) repair DB. -preferred
repair DB, repair (Insert, Retract) s.t. Insert InsertN , Retract RetractN ,
InsertRetract InsertN RetractN . Proposition 4 proof, element
0
0
0
0
N 0 MDB s.t. Insert = InsertN , Retract = RetractN , (N 0 )> = InsertN RetractN .
follows, then, (N 0 )> N > , N maximally consistent MDB ,
contradiction definition N .
2
Proposition 6 Suppose (Insert, Retract) -preferred repair DB.
-maximally consistent element N MDB s.t. Insert = InsertN Retract = RetractN .
Proof: pair (Insert, Retract) particular repair DB, thus Proposition 2
classical model IC Insert = \ Retract = \ . Consider
following valuation:
(

N (p) =

>

p \ p \

(p)

otherwise.

First show N = HD . since (p) = HD (p), since HD
minimal Herbrand model D, necessarily p 6 \ p 6 \ , thus N (p) = (p) =
(p) (p) = (p) HD (p). Otherwise, (p) 6= HD (p), either (p) =
HD (p) = f , i.e., p \ D, (p) = f HD (p) = t, i.e., p \ . cases,
N (p) = > = (p) HD (p)17 . Thus N = HD , N MDB . Now, Proposition 2
again, definition N , InsertN = N > \ = [(M \ D) (D \ )] \ = \ =
Insert, RetractN = N > = [(M \ D) (D \ )] = \ = Retract.
remains show N -maximally consistent MDB . Suppose not.
0
0
N 0 MDB s.t. (N 0 )> N > = Insert Retract. Proposition 3, (InsertN , RetractN )
also repair DB. Moreover,
0
InsertN = (N 0 )> \ N > \ = InsertN = Insert,
0
RetractN = (N 0 )> N > = RetractN = Retract,
0
0
InsertN RetractN = (N 0 )> N > = InsertN RetractN = Insert Retract.
0
0
Hence (InsertN , RetractN ) <i (Insert, Retract), (Insert, Retract) -preferred
repair (D, IC), contradiction.
2
Propositions 5 6 may formulated terms c follows:
17. use fact f = >.

257

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

Proposition 7 N c -maximally consistent element MDB , (InsertN , RetractN )
c -preferred repair DB.
Proposition 8 Suppose (Insert, Retract) c -preferred repair DB.
c -maximally consistent element N MDB s.t. Insert = InsertN Retract = RetractN .
proofs last two propositions similar Propositions 5 6,
respectively.
Example 6 Consider Example 3. that:
UDB = (D, IC) = ( {p(a), p(b), q(a), q(c)}, {X(p(X) q(X))} ).
HD

Thus,
= {p(a) : t, p(b) : t, p(c) : f, q(a) : t, q(b) : f, q(c) : t}, classical models
IC either p(y) false q(y) true every {a, b, c}. Now, since
HD neither p(b) false q(b) true, follows every element MU DB must assign
> either p(b) q(b). Hence, -maximally consistent elements MU DB (which
case also c -maximally consistent elements MU DB ) following:
M1 = { p(a) : t, p(b) : >, p(c) : f, q(a) : t, q(b) : f, q(c) : },
M2 = { p(a) : t, p(b) : t, p(c) : f, q(a) : t, q(b) : >, q(c) : }.
Propositions 5 6, then, -preferred repairs UDB (which also c preferred repairs) (InsertM1 , RetractM1 ) = (, {p(b)}) (InsertM2 , RetractM2 ) =
({q(b)}, ) (cf. Example 3).
Example 7 Examples 4 5, -maximally consistent elements (and c maximally consistent elements) MDB N1 = {p : t, q : >, r : t} N2 = {p : >, q : f, r : t}.
follows preferred repairs case ({q}, ) (, {p}).
summarize, section considered model-based, three-valued preferential semantics database integration. shown (Propositions 5 8) common
natural criteria making preferences among possible repairs (i.e., set inclusion
minimal cardinality) expressed order relations three-valued models
database. two ways making preferences (among repairs one hand among
three-valued models hand) thus strongly related, induce two alternative approaches database integration. next section shall consider third
approach problem (aimed provide operational semantics database
integration) relate model-based semantics, discussed above.

4. Computing Repairs Abduction
section introduce abductive system consistently integrates possibly contradicting data-sources. system computes, set data-sources preference
criterion , corresponding -repaired databases18 . framework composed
abductive logic program (Denecker & Kakas, 2000) abductive solver Asystem
(Kakas, Van Nuffelen, & Denecker, 2001; Van Nuffelen & Kakas, 2001) based
18. important note already stage computing -repaired databases wont
necessary produce repaired databases.

258

fiCoherent integration databases abductive logic programming

abductive refutation procedure SLDNFA (Denecker & De Schreye, 1992, 1998). first
three parts section describe components: Section 4.1 give general
description abductive reasoning, Section 4.2 show applied encode
database repairs, Section 4.3 describe computational platform. Then,
Section 4.4 demonstrate computation process comprehensive example,
Section 4.5 specify soundness completeness results approach (with respect
basic definitions Section 2 model-based semantics Section 3). Finally,
Section 4.6 consider ways representing special types data system.
4.1 Abductive Logic Programming
start general description abductive reasoning context logic programming. usual logic programming, language contains constants, functions,
predicate symbols. term either variable, constant, compound term f (t1 , . . . , tn ),
f n-ary function symbol ti terms. atom expression form
p(t1 , . . . , tm ), p m-ary predicate symbol ti (i = 1,. . ., m) terms. literal
atom negated atom. denial expression form X( F ), F
conjunction literals X subset variables F . free variables
F (those X) considered place holders objects unspecified
identity (Skolem constants). Intuitively, body F denial X( F ) represents
invalid situation.
Definition 13 (Kakas et al., 1992; Denecker & Kakas, 2000) abductive logic theory
triple = (P , , IC), where:
P logic program, consisting clauses form h l1 . . . ln , h
atomic formula li (i = 1, . . . , n) literals. clauses interpreted
definitions predicates heads,
set abducible predicates, i.e., predicates appear head
clause P,
IC set first-order formulae, called integrity constraints.
main model semantics logic programming extended abductive logic
programming. includes two-valued completion (Console, Theseider Dupre, & Torasso,
1991) three-valued completion semantics (Denecker & De Schreye, 1993), extended
well-founded semantics (Pereira, Aparicio, & Alferes, 1991), generalized stable semantics (Kakas & Mancarella, 1990b). semantics defined terms arbitrary
interpretations (Denecker & De Schreye, 1993), generally based Herbrand
interpretations. effect restriction semantics abductive theory
domain closure condition imposed: domain interpretation known
Herbrand universe. model abductive theory semantics
Herbrand interpretation H, exists collection ground abducible facts
, H model logic program P (with respect corresponding
semantics logic programming) H classically satisfies element IC.

259

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

Similarly, main semantics logic programming, one define
notion abductive solution query abductive theory.
Definition 14 (Kakas et al., 1992; Denecker & Kakas, 2000) (abductive) solution
theory (P , , IC) query Q set ground abducible atoms, one
predicate symbol A, together answer substitution , following
three conditions satisfied:
a) P consistent semantics S,
b) P |=S IC,
c) P |=S Q.
next section use abductive theory non-recursive program
model database repairs. next proposition shows abductive theories
Herbrand semantics coincide, models correspond abductive solutions query
true.
Proposition 9 Let = (P , , IC) abductive theory, P non-recursive
program. H Herbrand model three-valued completion semantics, iff H
Herbrand model two-valued completion semantics, iff H generalized stable model
, iff H generalized well-founded model .
H model , set abducible atoms H abductive solution
query true. Conversely, every abductive solution true, exists unique
model H , set true abducible atoms H.
Proof: proof based well-known fact non-recursive logic programs,
main semantics logic programming coincide. particular, non-recursive logic
program P Herbrand interpretation H, unique model
semantics (see, example, Denecker & De Schreye, 1993).
Let H model = (P , , IC) four semantics mentioned above.
exists collection ground abducible facts , H model
logic program P corresponding semantics logic programming. Since P
non-recursive, P . observation, H unique model P
mentioned semantics. Hence, H model
semantics. proves first part proposition.
H Herbrand model , set abducible atoms H
model P . Clearly, must set true abducible atoms H. P
obviously consistent, entails integrity constraints , entails true.
Hence, abductive solution true. Conversely, set abducible atoms,
P unique model H set true abducible atoms H .
abductive solution true, H satisfies integrity constraints, hence H model
. Consequently, H unique model , set true abducible atoms . 2
addition standard properties abductive solutions theory query
Q, specified Definition 14, one frequently imposes optimization conditions solutions
260

fiCoherent integration databases abductive logic programming

, analogous found context database repairs. Two frequently used criteria
generated abductive solution minimal respect set inclusion
respect set cardinality (cf. Definition 7). fact preference
criteria used choosing appropriate abductive solutions selecting preferred
database repairs necessarily mean natural mapping
corresponding solutions. next sections show, however, meta-programming
allows us map database repair problem abductive problem (w.r.t.
type preference criterion).
4.2 Abductive Meta-Program Encoding Database Repairs
task repairing union n given databases DB respect integration local integrity constraints IC, represented abductive theory
= (P , , IC 0 ), P meta-program encoding new database obtained
updating existing databases, set {insert, retract} abducible predicates
used describe updates, IC 0 encodes integrity constraints. P, facts p
appear least one databases encoded atomic rules db(p), facts p
appear updated database represented atoms fact(p). latter predicate
defined follows:
fact(X) db(X) retract(X)
fact(X) insert(X)
assure predicates insert retract encode proper update
database, following integrity constraints also specified:
inserted element belong given database:
insert(X) db(X)
retracted element belong database:
retract(X) db(X)
set integrity constraints IC 0 obtained straightforward transformation
IC: every occurrence database fact p integrity constraint replaced
fact(p)19 .
Example 8 (Example 1, revisited) Figure 2 contains meta-program encoding Example 1 (the codes Examples 2 3 similar).
noted Section 4.1, main semantics abductive logic programing
one one correspondence repairs composed database DB
Herbrand models encoding, abductive meta theory . Consequently, abduction
used compute repairs. following sections introduce abductive method
purpose.
19. Since abductive system (see Section 4.3) accept integrity constraints denial form, case
elements IC 0 form, Lloyd-Topor transformation (Lloyd & Topor, 1984)
may also applied here; consider case Section 4.3.2.

261

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

% System definitions:
defined(fact( ))
defined(db( ))
abducible(insert( ))
abducible(retract( ))
% composer:
fact(X) db(X) retract(X)
fact(X) insert(X)
insert(X) db(X)
retract(X) db(X)
% databases:
db(teaches(c1,n1))
db(teaches(c2,n2))
db(teaches(c2,n3))
= Z fact(teaches(X,Y)) fact(teaches(X,Z))

% D1
% D2
% IC

Figure 2: meta-program Example 1

4.3 Abductive Computational Model (The Asystem)
describe abductive system used compute database repairs.
Asystem (Kakas, Van Nuffelen, & Denecker, 2001; Van Nuffelen & Kakas, 2001) tool
combining abductive logic theories constraint logic programming (CLP). synthesis
refutation procedures SLDNFA (Denecker & De Schreye, 1998) ACLP (Kakas
et al., 2000), together improved control strategy. essence Asystem
reduction high level specification lower level constraint store, managed
constraint solver. See http://www.cs.kuleuven.ac.be/dtai/kt/ latest version
system20 . review theoretical background well practical
considerations behind system. information, see (Denecker & De Schreye,
1998) (Kakas, Van Nuffelen, & Denecker, 2001).
4.3.1 Abductive Inference
input Asystem abductive theory = (P, A, IC), IC consists
universally quantified denials. process answering query Q, given conjunction
literals, described derivation Q rewriting states. state pair
(G , ST ), G, set goal formulae, set conjunctions literals denials.
rewriting process elements G (the goals) reduced basic formulae
20. version runs top Sicstus Prolog 3.10.1 later versions.

262

fiCoherent integration databases abductive logic programming

stored structure ST . structure called store, consists
following elements21 :
set contains abducibles a(t).
set contains denials form X( a(t) Q), a(t) abducible.
denial may contain free variables.
set E equalities inequalities terms.
consistency E maintained constraint solver uses Martelli Montanari unification algorithm (Martelli & Montanari, 1982) equalities constructive
negation inequalities.
state = (G , ST ) called consistent G contain false ST consistent (since kept consistent E, latter condition
equivalent consistency E). consistent state empty set goals (G = )
called solution state.
derivation starts initial state (G0 , ST 0 ), every element ST 0 empty,
initial goal, G0 , contains query Q integrity constraints IC
theory . sequence rewriting steps performed. step starts certain state
Si = (Gi , ST ), selects goal Gi , applies inference rule (see below) obtain
new consistent state Si+1 . consistent state reached Si derivation
backtracks. derivation terminates solution state reached, otherwise fails (see
Section 4.4 demonstration process).
Next present inference rules Asystem, using following conventions:
Gi = Gi {F }, F selected goal formula.
SELECT denote nondeterministic choices inference rule.
Q conjunction literals, possibly empty. Since empty conjunction equivalent
true, denial Q empty Q equivalent false.
, , E mentioned, remain unchanged.
inference rules classified four groups, named leftmost literal
selected formula (shown bold). group contains rules (positive) conjunctions
literals rules denials.
1. Defined predicates:
inference rules unfold bodies defined predicate. positive conjunctions
corresponds standard resolution selected clause, whereas denial
clauses used every clause leads new denial.
D.1 p(t) Q:
Let p(si ) Bi P (i = 1, . . . , n) n clauses p head. Then:
Gi+1 = Gi {t = s1 B1 Q} . . . Gi+1 = Gi {t = sn Bn Q}
21. actual implementation Asystem also contains store finite domain constraint expressions.
store needed application here, hence omitted.

263

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

D.2 X( p(t) Q):
Gi+1 = Gi {X, ( = B Q) | p(s) B P variables }
2. Negations:
Resolving negation corresponds switching mode reasoning positive
literal denial vice versa. similar idea negation-as-failure
logic programming.
N.1 p(t) Q:
Gi+1 = Gi {Q, p(t)}
N.2 X( p(t) Q) contain variables X:
Gi+1 = Gi {p(t)} Gi+1 = Gi { p(t), X( Q)}
3. Abducibles:
first rule responsible creation new hypotheses. rules ensure
elements consistent .
A.1 a(t) Q:
SELECT arbitrary a(s) define Gi+1 = Gi {Q} {s = t}
Gi+1 = Gi {Q} {X( = R) | X( a(s) R) }
{ (t = s) | a(s) } i+1 = {a(t)}
A.2 X( a(t) Q):
Gi+1 = Gi {X( = Q) | a(s) } i+1 = {X( a(t) Q)}
4. Equalities:
inference rules isolate (in)equalities, constraint solver evaluate them. first rule applies equalities goal formulae:
E.1 = Q:
Gi+1 = Gi {Q} Ei+1 = Ei {s = t}
following three rules handle equalities denials. rule applies depends
whether contain free universally quantified variables. rules Q[X/t]
denotes formula obtained Q substituting term X.
E.2 X( = Q):
unifiable Gi+1 = Gi ;
Otherwise, let Es equation set solved form representing general
unifier (Martelli & Montanari, 1982). Gi+1 = Gi {X( Es Q)}.
E.3 X, Y( X = Q) term containing X:
Gi+1 = Gi {Y ( Q[X/t])}
E.4 X, Y( X = Q) X free variable X set universally
quantified variables term t:
Ei+1 = Ei {X(X 6= t)} Gi+1 = Gi {X = t} {Y ( Q[X/t])}22 .
22. first branch inference E.4 explores condition X(X 6= t). second branch, negation
condition explored. X identical t, values assigned X.
second branch, universally quantified variables X turned free variables may appear
free ( Q[X/t]).

264

fiCoherent integration databases abductive logic programming

usual, one check floundering negation. occurs inference rule
N.2 applied denial universally quantified variables negative literal p(t).
Floundering aborts derivation.
answer substitution , derived solution state S, substitution
free variables satisfies E (i.e. (E) true) grounds . Note that, case
abductive theory without abducibles integrity constraints, computed answers
defined Lloyd (1987) general unifiers E correct answers answer
substitutions defined above.
Proposition 10 (Kakas, Van Nuffelen, & Denecker, 2001) Let = (P, A, IC) abductive theory, Q query, solution state derivation Q, answer
substitution S. pair consisting ground abducible atoms ((S))
answer substitution abductive solution Q.
4.3.2 Constraint Transformation Denial Form
Since inference rules Asystem applied integrity constraints denial
form, integrity constraints IC abductive theory must translated
form. done applying variant Lloyd-Topor transformation (Lloyd &
Topor, 1984) integrity constraints (see Denecker & De Schreye, 1998).
procedure well-known procedure used deductive databases convert first
order quantified query Q logically equivalent pair atomic query nonrecursive datalog procedure. transformation defined rewriting process sets
formulae: initial set { F |F IC}, transformation done applying De
Morgan various distribution rules. New predicates rules may introduced
transformation order deal universal quantifications denials.
illustrate transformation case integrity constraints running example.
Example 9 Consider following extension integrity constraints Example 1:
IC = { XY Z (teaches(X, ) teaches(X, Z) = Z) ,
X (teacher(X) teaches(Y, X)) }.
Note addition original integrity constraint Example 1, also demand
every teacher give least one course.
Lloyd-Topor transformation first integrity constraint:
(1) X Z (teaches(X, ) teaches(X, Z) = Z)
(2) X Z (teaches(X, ) teaches(X, Z) 6= Z)
(3) X Z ( teaches(X, ) teaches(X, Z) 6= Z)
Lloyd-Topor transformation second integrity constraint:
(1) X (teacher(X) teaches(Y, X))
(2) X ( teacher(X) teaches(Y, X))
265

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

(3) X ( teacher(X) gives courses(X)) gives courses defined by:
gives courses(X) teaches(Y, X)
(4) X ( teacher(X) gives courses(X)),
gives courses(X) teaches(Y, X)
4.3.3 Control Strategy
selection strategy applied derivation process crucial. Prolog-like selection strategy (left first, depth first) often leads trashing, blind
choices, result global overview current state computation.
development Asystem main focus improvement control
strategy. idea apply first rules result deterministic change
state, information propagated. none rules applicable, one
left choices selected. strategy, commitment choice suspended
moment information derived deterministic way.
resembles CLP-solver, constraints propagate information soon
choice made. propagation reduce number choices made thus
often dramatically increases performance.
4.3.4 Implementation
section describe structure implementation. Figure 3 shows layered
view. upper-most level consists specific abductive logic theory integration
task, i.e., database information integrity constraints. layer together
composer form abductive meta-theory (see Section 4.2) processed
Asystem.

6

DB 1

DB 2



6

DB n

Input

Abductive
Theory

?
6

Composer
?
6

Optimizer

Composing
System

Abductive
System
Asystem (based SLDNFA)

Sicstus Prolog
?

?

Figure 3: schematic view system components.
266

fiCoherent integration databases abductive logic programming

noted above, composer consists meta-theory integrating databases
coherent way. interpreted abductive theory, abducible
predicates provide information restore consistency amalgamated
data.
abductive system (enclosed dotted lines Figure 3) consists three main
components: finite domain constraint solver (part Sicstus Prolog), abductive metainterpreter (described previous sections), optimizer.
optimizer component that, given preference criterion space
solutions, computes most-preferred (abductive) solutions. Given preference
criterion, component prunes fly branches search tree lead
solutions worse others already computed. actually
branch bound filter solutions space, speeds-up execution makes sure
desired solutions obtained23 . preference criterion pre-order,
optimizer complete, is, compute optimal solutions (more
Section 4.5). Moreover, general-purpose component, may useful
data integration, also for, e.g., solving planning problems.
4.3.5 Complexity
well-known general, task repairing database tractable,
may exponential number different ways repairing it. Even cases integrity constraints assumed single-headed dependencies (Greco & Zumpano, 2000),
checking whether exists -repaired database certain query Q satisfied, P2 . Checking fact satisfied -repaired databases P2 (see
Greco & Zumpano, 2000). surprising light correspondence
computations -minimal repairs computations entailment relations defined
maximally consistent models (see Propositions 58), also known second level
polynomial hierarchy.
pure upper bound Asystem still unknown, since best knowledge complexity results SLDNFA refutation procedure available.
4.4 Example: Derivation Repairs Asystem
Consider Example 9. corresponding meta-theory (assuming Lloyd-Topor
transformation applied it) given Figure 4. case, follows,
shall assume variables denials universally quantified, so, order
reduce amount notations, universal quantifiers omitted denial rules.
executed code Figure 4, well examples literature
system. Theorem 2 Section 4.5 guarantees, output case set
preferred solutions corresponding problem. follows demonstrate
23. See also third item Note 2 (at end Section 4.4).

267

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

db(teacher(n1 ))
db(teacher(n2 ))
db(teacher(n3 ))
db(teaches(c1 ,n1 ))
db(teaches(c2 ,n2 ))
db(teaches(c2 ,n3 ))
fact(teaches(X,Y)) fact(teaches(X,Z)) (Y 6= Z) (ic1)
fact(teacher(X)) gives courses(X)
(ic2)
gives courses(X) fact(teaches(Y,X))
fact(X) db(X) retract(X)
fact(X) insert(X)
insert(X) db(X)
retract(X) db(X)

(composer-ic1)
(composer-ic2)

Figure 4: meta-theory Example 9.
preferred solutions meta-theory computed.
follow one branch refutation tree, starting initial state (G0 , ST 0 ),
initial set goals G0 = {true0 , ic1, ic2, composer-ic1, composer-ic2},
initial store ST 0 = (, , ). Suppose first selected formula
F1 = ic1 = fact(teaches(X, Y)) fact(teaches(X, Z)) (Y 6= Z).
Then, D.2,
G1 = G0 \ F1
{ db(teaches(X, Y)) retract(teaches(X, Y)) fact(teaches(X, Z)) (Y 6= Z),
insert(teaches(X, Y)) fact(teaches(X, Z)) (Y 6= Z) },
ST 1 = ST 0 . Now, pick
F2 = db(teaches(X, Y)) retract(teaches(X, Y)) fact(teaches(X, Z)) (Y 6= Z).
Select db(teaches(X,Y)), unfold corresponding atoms database, then,
D.2, followed E.2 E.3,
G2 = G1 \ F2
{ retract(teaches(c1 , n1 )) fact(teaches(c1 , Z)) (n1 6= Z),
retract(teaches(c2 , n2 )) fact(teaches(c2 , Z)) (n2 6= Z),
retract(teaches(c2 , n3 )) fact(teaches(c2 , Z)) (n3 6= Z) },
268

fiCoherent integration databases abductive logic programming

still ST 2 = ST 1 . Pick second denial among new goals added
G2 . Denote denial F3 . Since F3 starts negated literal, N.2 applies,
derivation process splits two branches. second branch contains
G3 = G2 \ F3 { retract(teaches(c2 , n2 )), fact(teaches(c2 , Z)) (n2 6= Z) },
still ST 3 = ST 2 . Choose first new goal, i.e.,
F4 = retract(teaches(c2 , n2 )).
Now, since 3 = , option add F4 3 . Thus, A.2,
G4 = G3 \ F4 ST 4 = (, {F4 }, ).
Assume, now, take second new goal G3 :
F5 = fact(teaches(c2 , Z)) (n2 6= Z).
Following similar process unfolding data described above, using db(teaches(c2 , n3 )),
end-up
retract(teaches(c2 , n3 )) (n2 6= n3 ).
Selecting negative literal (n2 6= n3 ), N.2 applies again. first branch quickly results failure adding (n2 = n3 ) E. second branch adds (n2 = n3 )
retract(teaches(c2 , n3 )) set goals. former one added constraint
store, (n2 6= n3 ), simplifies true. Assume latter selected next. Let
i-th step. i1 (the set abducible predicates produced
current step) empty, thus option abduce retract(teaches(c2 , n3 )). Thus,
A.1, ST consists of:
= {retract(teaches(c2 , n3 ))},

= {F4 } = { retract(teaches(c2 , n2 ))},

Ei = Ei1 ,
Gi = Gi1 \ {retract(teaches(c2 , n3 ))} { teaches(c2 , n2 ) = teaches(c2 , n3 )}.
last goal certainly satisfied, ic1 resolved branch.
turn ic2. So:
Fi+1 = ic2 = fact(teacher(X)) gives courses(X).
evaluation Fi+1 either x = n1 x = n2 successful, interesting case
x = n3 . case evaluation leads goal gives courses(X). Unfolding
goal yields fact(teaches(Y, n3 )) appears goal set. order satisfy goal,
resolved either one composers rules (using D.1). first rule (i.e.,
fact(X) db(X) retract(X)) leads failure (since retract(teaches(c2 , n3 ))
already ), second rule composer, fact(X) insert(X), must
applied. leads abduction insert(teaches(Y, n3 )). ic1, 6= c1 6= c2
derived24 . Also, composer-ic1 composer-ic2 satisfied current state,
eventually solution state reached derivation path described here,
24. One verify constraints indeed detected derivations process. omit
details order keep example tractable.

269

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

contains following sets:
= {retract(teaches(c2 , n3 )), insert(teaches(Y, n3 ))},
E = {Y 6= c1 , 6= c2 },
means retraction teaches(c2 , n3 ) insertion teaches(Y, n3 ) 6= c1
6= c2 . solutions obtained similar way.
Note 2 remarks derivation process.
1. solution contains non-ground abducible predicate. indeed
expected result, since solution resolves contradiction integrity constraint ic1 removing assumption teacher n3 teaches course c2 . result,
teacher n3 teach course. Thus, order assure integrity constraint (ic2), solution indicates n3 must teach course (other c1
c2 ).
2. One possible (and realistic) explanation cause inconsistency
database Example 9 Figure 4, typographic error. might happen,
instance, c2 mistakenly typed instead of, say, c3 , teaches(c2 ,n3 ).
case, database repair computed pinpoints possibility (in case, then,
equal c3 )25 . explanation cannot explicitly captured, unless
particular repairs non-ground solutions constructed, indeed case
here. approaches recently introduced (e.g., Bravo &
Bertossi, 2003; Cali, Lembo, & Rosati, 2003) properly capture cases
Example 9, best knowledge, application database integration
ability.
3. system finds solution corresponds goal state Sg = (Gg , ST g )
Gg = ST g = (g , g , Eg ), -optimizer may used whenever
state = (Gs , (s , , Es )) reached, |g | < |s |, corresponding branch
tree pruned26 .
4.5 Soundness Completeness
section give soundness completeness results Asystem, relate
results model-based preferential semantics, considered Section 3.
follows denote abductive meta-theory (constructed described
Section 4.2) composing n given databases DB 1 , . . . , DB n . Let also ProcALP
sound abductive proof procedure 27 . following proposition shows ProcALP
provides coherent method integrating databases represented .
Proposition 11 Every abductive solution obtained ProcALP query true
theory , repair UDB.
25. variable free {Y/c3 } answer substitution grounds satisfies E.
26. size increase along derivation, state cannot lead solution
better one induced Sg , corresponding branch tree indeed pruned.
27. is, ProcALP process computing abductive solutions , sense Definition 14.

270

fiCoherent integration databases abductive logic programming

Proof: construction easy see conditions listed
Definition 5 satisfied. Indeed, first two conditions assured integrity constraints composer. last condition also met since soundness ProcALP
produces abductive solutions query true . Thus, second property
Definition 14, every solution = (Inserti , Retracti ) P |= IC.
Since P contains data section facts, follows |= IC, i.e. every
integrity constraints follows Inserti \ Retracti .
2
SLDNFA sound abductive proof procedure (Denecker & De Schreye, 1998),
taken procedure ProcALP , Proposition 11 provides soundness theorem
current implementation Asystem. optimizer incorporated
Asystem, following soundness result extended system:
Theorem 1 (Soundness) Every output obtained query true ,
Asystem executed c -optimizer [respectively, -optimizer], c preferred repair [respectively, -preferred repair] UDB.
Proof: Follows Proposition 11 (since Asystem based SLDNFA
sound abductive proof procedure), fact c -optimizer prunes paths
lead solutions c -preferable. Similar arguments hold systems
-optimizer.
2
Proposition 12 Suppose query true finite SLDNFA-tree w.r.t. .
every c -preferred repair every -preferred repair UDB obtained running
Asystem.
Outline proof: proof abductive solutions minimal cardinality
obtained system based Theorem 10.1 Denecker & De Schreye, 1998,
shown SLDNFAo , extension SLDNFA, aimed computing solutions
minimal cardinality, complete; see Denecker & De Schreye, 1998, Section 10.1,
details. Similarly, proof abductive solutions minimal
w.r.t. set inclusion obtained system based Theorem 10.2 Denecker &
De Schreye, 1998, shows SLDNFA+ , another extension SLDNFA,
aimed computing minimal solutions w.r.t. set inclusion, also complete; see Denecker
& De Schreye, 1998, Section 10.2, details.
Now, Asystem based combination SLDNFAo SLDNFA+ . Moreover, system change refutation tree (but controls way rules
selected), Theorems 10.1 10.2 Denecker De Schreye (1998) applicable
case well. Thus, c - -minimal solutions produced.
particular means every c -preferred repair well every -preferred repair UDB
produced system.
2
noted last proposition guarantee non-preferred repairs
produced (as true general). However, following theorem
shows, use optimizer excludes possibility.
271

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

Theorem 2 (Completeness) notations Proposition 12 assumptions,
output execution Asystem together c -optimizer [respectively,
together -optimizer] exactly !(UDB, c ) [respectively, !(UDB, )].
Proof: shall show claim case c ; proof w.r.t. similar.
Let (Insert, Retract) !(UDB, c ). Proposition 12, = (Insert, Retract) one
solutions produced Asystem . Now, execution system
together c -optimizer, path corresponds cannot pruned
refutation tree, since assumption (Insert, Retract) minimal cardinality
among possible solutions, pruning condition satisfied. Thus
produced c -optimized system. converse, suppose (Insert, Retract)
repair UDB produced c -optimized system. Suppose contradiction (Insert, Retract) 6 !(UDB, c ). proof Proposition 12,
0 = (Insert0 , Retract0 ) !(UDB, c ) constructed Asystem ,
(Insert0 , Retract0 ) <c (Insert, Retract). |0 | < ||, c -optimizer would prune
path solution cardinality becomes bigger |0 |. contradicts
assumption (Insert, Retract) produced c -optimized system.
2
Note 3 SLDNFA-resolution Asystem based extension SLDNFresolution (Lloyd, 1987) coincides logic programs empty sets abducible predicates. SLDNF-resolution complete computation always terminates. SLDNFA inherits property. reason condition finite
SLDNFA-tree imposed Proposition 12 Theorem 2. Like SLDNF, termination
SLDNFA guaranteed imposing syntactic conditions program. refer
(Verbaeten, 1999), conditions proposed guarantee existence
finite SLDNFA-tree.
context paper, floundering would arise presence unsafe integrity
constraints (e.g., x p(x)). One way eliminate problem use unary domain
predicate dom, ranging objects database, add range
quantified variable integrity constraints, obtain formulae form
x(dom(x) (x)) x(dom(x) (x)).
following results immediately follow propositions Section 3 (unless explicitly said, Asystem without optimizer).
Corollary 1 Suppose query true finite SLDNFA refutation tree w.r.t. input
theory . Then:
1. every output (Insert, Retract) Asystem classical model IC
s.t. Insert = \ Retract = \ .
2. every output (Insert, Retract) Asystem 3-valued model N IC
s.t. InsertN = Insert RetractN = Retract.
Corollary 2 notations Corollary 1 assumption, that:
272

fiCoherent integration databases abductive logic programming

1. every output (Insert, Retract) obtained running Asystem together
-optimizer [respectively, together c -optimizer], -maximally
consistent element [respectively, c -maximally consistent element] N MU DB s.t.
InsertN = Insert RetractN = Retract.
2. every -maximally consistent element [respectively, c -maximally consistent element] N MUDB solution (Insert, Retract) obtained running
Asystem together -optimizer [respectively, together c -optimizer]
s.t. Insert = InsertN Retract = RetractN .
last corollaries show operational semantics, induced Asystem,
also represented preferential semantics, terms preferred models theory.
set R(UDB, ) represents intended meaning -recover database
UDB, therefore obtained computationally, set
{(Insert, Retract) | (Insert, Retract) output Asystem -optimizer},
or, equivalently, described terms preferred models theory,
following set:
{(InsertN , RetractN ) | N -maximally consistent MU DB }.
4.6 Handling Specialized Information
purpose section demonstrate potential usage system
complex scenarios, various kinds specialized data incorporated system.
particular, briefly consider time information source identification. also give
guidelines extend system capabilities handling kinds
information.
4.6.1 Timestamped Information
Many database applications contain temporal information. kind data may divided two types: time information part data itself, time information
related database operations (e.g., records database update time). Consider,
instance, birth day(John,15/05/2001)16/05/2001 . Here, Johns date birth instance
former type time information, subscripted data describes time
fact added database, instance latter type time information.
approach, timestamp information integrated adding temporal theory
describing state database particular time point. One way
using situation calculus. approach database described initial
information history events performed database lifetime (see Reiter,
1995). use different approach, based event calculus (Kowalski &
Sergot, 1986). idea make distinction two kinds events, add db
del db, describe database modifications, composer-driven events insert
273

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

retract used constructing database repairs. view, extended
composer following form:
holds at(P,T) initially(P) clipped(0,P,T)
holds at(P,T) add(P,E) E<T clipped(E,P,T)
clipped(E,P,T) del(P,C) EC, C<T
add(P,T)
add(P,T)
del(P,T)
del(P,T)






add db(P,T)
insert(P,T)
del db(P,T)
retract(P,T)

insert(P,T) retract(P,T)
insert(P,T) add db(P,T)
retract(P,T) del db(P,T)
Note extended representation, integrity constraints must carefully specified. Consider, e.g. statement person born one date:
holds at(birth day(P,D1),T) holds at(birth day(P,D2),T) D16=D2
problem ensure consistency, constraint must checked every
point time. may avoided simple rewriting ensures constraint
verified event person occurs:
ic(P,T) holds at(birth day(P,D1),T) holds at(birth day(P,D2),T) D16=D2
add db(birth day(P, ),T) NT = T+1 ic(P,NT)
insert(birth day(P, ),T) NT = T+1 ic(P,NT)
ic(P,0)
Note 4 last example used temporal integrity constraints order resolve
contradicting update events. Clearly, contradicting events necessarily yield classically inconsistent database, role integrity constraints express possible
events terms time causation, necessary describe consequence
violation consistency.
Instead using temporal integrity constraints event calculus, one could repair
database time-stamps using time-based criterion making preferences
among repairs. instance, denote db(x1 , . . . , xn )t data-fact db(x1 , . . . , xn )
timestamp t, suppose (Insert, Retract) (Insert0 , Retract0 ) two repairs database (D, IC). time-based criterion preferring (Insert, Retract)
(Insert0 , Retract0 ) could state, e.g., every data-fact db(x1 , . . . , xn ) timestamps
t1 , t2 s.t. db(x1 , . . . , xn )t1 follows Insert \ Retract db(x1 , . . . , xn )t2 follows
Insert0 \ Retract0 , necessarily t1 t2 . detailed treatment issue outside
scope paper.
interested reader may refer, e.g., (Sripada, 1995; Mareco & Bertossi, 1999)
detailed discussion use logic programming based approaches specification
temporal databases. specifications easily combined repairs,
given above.
274

fiCoherent integration databases abductive logic programming

4.6.2 Keeping Track Source Identities
cases important preserve identity database
specific piece information originated. useful, instance, one wants
make preferences among different sources, specific source filtered (e.g, corresponding database available becomes unreliable).
kind information may decoded adding another argument every fact,
denotes identity origin. requires minor modifications basic composer,
since composer controls way data integrated. such,
component keep track source information.
Suppose, then, every database fact add another argument identifies
source. I.e., db(X,S) denotes X fact originated database S. composer
following form:
fact(X,S) db(X,S) retract(X)
fact(X,composer) insert(X)
insert(X) db(X,S)
retract(X) db(X,S)
Note composer considers extra source inserts brand new data
facts. possible, e.g., trace information comes specific source, make
preferences among different sources (by specifying appropriate integrity constraints),
filter data comes certain sources. last property demonstrated next
rule:
validFact(X) fact(X,S) trusted source(S)
trusted source enumerates reliable sources data.
Note last example source identification extended order
make preferences among different sources (and ignoring unreliable sources).
introducing new predicate, trust(Source,Amount), attaches certain level
reliability source, possible, case conflicts, prefer sources higher
reliability follows:
fact(X,S) db(X,S0 ) 6= S0 trusted(S0 ,S)
trusted(S0 ,S) trust(S0 ,A0 ) trust(S,A) A0 >
method particularly useful integrity constraint acts functional dependency specific facts. following example (originally introduced Subrahmanian, 1994) demonstrates this.
Example 10 Consider following simple scenario target recognition, three
sensors autonomous vehicle, different degrees reliability, identify
objects vehicles neighborhood:
trust(radar,10)
trust(gunchar,8)
275

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

trust(speedometer,5)
db(observe(object1,t72),radar)
db(observe(object1,t60),gunchar)
db(observe(object1,t80),speedometer)
fact(observe(O,V1 ),S) db(observe(O,V2 ),S0 ) S6=S0 trusted(S0 ,S)
radar highest reliability, observation preserved. observations sensors retracted database.

5. Discussion Overview Related Works
interest systems coherent integration databases continuously growing
last years (see, e.g, Olive, 1991; Baral et al., 1991, 1992; Revesz, 1993; Subrahmanian, 1994; Bry, 1997; Gertz & Lipeck, 1997; Messing, 1997; Lin & Mendelzon, 1998;
Liberatore & Schaerf, 2000; Ullman, 2000; Greco & Zumpano, 2000; Greco et al., 2001;
Franconi et al., 2001; Lenzerini, 2001, 2002; Arenas et al., 1999, 2003; Bravo & Bertossi,
2003; Cali et al., 2003, many others). Already early works subject became clear design systems data integration complex task, demands
solutions many questions different disciplines, belief revision, merging
updating, reasoning inconsistent information, constraint enforcement, query processing course many aspects knowledge representation. section shall
address issues.
One important aspect data integration systems concepts independent
(stand-alone) data-sources unified database mapped other.
proper specification relations source schemas schema
amalgamated data exempts potential user aware data
arranged sources. One approach mapping, sometimes called global-centric
global-as-view (Ullman, 2000), requires unified schema expressed
terms local schemas. approach, every term unified schema associated
view (alternatively, query) sources. approach taken
systems data integration, well ours. main advantage approach
induces simple query processing strategy based unfolding query,
uses terminology databases. indeed case abductive
derivation process, defined Section 4.3.1. approach, sometimes called sourcecentric local-as-view (used, e.g., Bertossi et al., 2002), considers every source view
integrated database, meaning every source obtained concepts
global database. particular, global schema independent distributed
ones. implies, particular, addition new source system requires
provide local definitions necessarily involves changes global schema.
main advantage latter approach is, therefore, provides better setting
maintenance. detailed discussion topic, see (Ullman, 2000; Lenzerini, 2001;
Cali et al., 2002; Van Nuffelen et al., 2004). references survey different approaches data integration appear papers Batini, Lenzerini, Navathe (1986),
276

fiCoherent integration databases abductive logic programming

Rahm Bernstein (2001), Lenzerini (2002).
Another major issue addressed ability data integration systems
properly cope dynamically evolving worlds. particular, domain discourse
fixed advance, information may revised regular basis. last
issue usually handled methods belief revision (Alchourron et al., 1995; Gardenfors &
Rott, 1995) nonmonotonic reasoning. context belief revision common
make distinction revisions integrity constraints changes sets
data-facts, since two types information different nature thus may require
different approaches handling dynamic changes. set integrity constraints
given clause form, methods dynamic logic programing (Alferes et al., 2000, 2002)
may useful handling revisions. noted (Alferes et al., 2002), assuming
local database consistent (as case), dynamic logic programing (together
proper language implementing it, like LUPS (Alferes et al., 2002)) provides way
avoiding contradictory information, may viewed method updating
database sequence integrity constraints arrive different time points.
types changes predictable, characterized sense, temporal integrity constraints (in context temporal databases) used order
specify treat new information. method also useful revision criteria
known advance (e.g., case collisions, prefer recent data, cf. Section
4.6.1). See, e.g., (Sripada, 1995; Mareco & Bertossi, 1999) detailed discussion temporal integrity constraints temporal databases logic programming based formalisms.
second type revisions (i.e., modifications data-facts) obtained
(preferred) repairs unified database, induce corresponding modifications
data-facts. repair usually induced method restoring (or assuring) consistency
amalgamated database minimal amount change. case, minimization
criterion often determined aspiration remain close possible set
collective information. typical kind repair goal , standard ways
formally expressing enumeration methods, following28 :
Minimizing Hamming distance (propositional) models unified
database repairs (Liberatore & Schaerf, 2000), minimizing distance corresponding three-valued interpretations (de Amo et al., 2002) according
suitable generalization Hamming distance.
Minimizing symmetric distance sets consequences corresponding databases (Arenas, Bertossi, & Chomicki, 1999; Arenas, Bertossi, & Kifer,
2000; Bertossi, Chomicki, Cortes, & Gutierrez, 2002) or, equivalently, minimizations
terms set inclusion (Greco & Zumpano, 2000).
underlying data prioritized, corresponding quantitative information
also considered computations distances (see, instance, work
Liberatore & Schaerf, 2000).
28. See also (Gertz & Lipeck, 1997, Section 5) discussion repair strategies.

277

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

Various ways computing (preferred/minimal) repairs described literature,
among proof-theoretical (deductive) methods (Bertossi & Schwind, 2002; de Amo
et al., 2002), abductive methods (Kakas & Mancarella, 1990a; Inoue & Sakama, 1995;
Sakama & Inoue, 1999, 2000), algorithmic approaches based computations
maximal consistent subsets (Baral et al., 1991, 1992), use techniques model-based
diagnosis (Gertz & Lipeck, 1997). common approach view database logic
program, adopt standard techniques giving semantics logic programs order compute database repairs. instance, stable-model semantics disjunctive logic
programs used computing repairs (Greco & Zumpano, 2000; Greco et al., 2001;
Franconi et al., 2001; Arenas et al., 2003), resolution-based procedures integrating
several annotated databases introduced Subrahmanian (1990, 1994). follows
Section 4, application introduced also based extended resolution
strategy, applied logic programs may contain negation-as-failure operators abducible predicates.
repairing database means particular elimination contradictions, reasoning
inconsistent information major challenge data integration systems. First,
important note respect every formalism handling inconsistency
acceptable context databases, even underlying criterion handling
inconsistency one repair goals mentioned above. following example
demonstrates case:
Example 11 (Arenas, Bertossi, & Chomicki, 1999) Consider following (inconsistent)
database: DB = ({p, q}, {(p q)}). approach Lin (1996), instance, p q
may inferred repaired database, following strategy minimal change. However,
approach none p, q, (p q) holds repaired database. particular
(since (Lin, 1996) distinction data-facts integrity constraints),
integrity constraint {(p q)} cannot inferred, violates intended
meaning integrity constraint databases.
Many techniques consistency enforcement repairs constraint violations
suggested, among methods resolving contradictions quantitative considerations, majority vote (Lin & Mendelzon, 1998; Konieczny & Pino Perez, 2002)
qualitative ones (e.g., defining priorities different sources information preferring
certain data another, Benferhat, Cayrol, Dubois, Lang, & Prade, 1993, Arieli,
1999). Another common method handling inconsistent (and incomplete) information
turning multi-valued semantics. Three-valued formalisms one considered
Section 3 used semantical basis paraconsistent methods construct database
repairs (de Amo, Carnielli, & Marcos, 2002) useful general pinpointing inconsistencies (Priest, 1991). approaches use lattice-based semantics decode within
language meta-information, confidence factors, amount belief
specific assertion, etc. approaches combine corresponding formalisms
knowledge representation, annotated logic programs (Subrahmanian, 1990, 1994;
Arenas et al., 2000) bilattice-based logics (Fitting, 1991; Arieli & Avron, 1996; Messing, 1997), together non-classical refutation procedures (Fitting, 1989; Subrahmanian,
278

fiCoherent integration databases abductive logic programming

1990; Kifer & Lozinskii, 1992) allow detect inconsistent parts database
maintain them.

6. Summary Future Work
paper developed formal declarative foundation rendering coherent
data, provided different databases, presented application implements
approach. Like similar applications (e.g., Subrahmanian, 1994; Bertossi, Arenas, & Ferretti,
1998; Greco & Zumpano, 2000; Liberatore & Schaerf, 2000), system mediates among
sources information also reasoner underlying data.
Composition several data-sources encoded meta-theories form abductive
logic programs, possible extend theories providing meta-information
data-facts, time-stamps source identities. Moreover, since reasoning
process system based pure generalization classical refutation procedures,
syntactical embedding first-order formulae languages, extension
two-valued semantics, necessary.
Due inherent modularity system, component independent
modified meet different needs. Thus, instance, underlying solver may replaced
solver capable dealing meta-theory, improvement
optimizer affect whole system efficiency, regardless nature
input. Also, way keeping data coherent encapsulated component
integrates data (i.e., composer). implies, particular, input
reasoner external policy making preferences among conflicting sources
compulsory order resolve contradictions.
shown, operational semantics inconsistent databases, induced
Asystem, strongly related (multi-valued) preferential semantics. preferential
semantics provides background many non-monotonic paraconsistent formalisms
(e.g., Shoham, 1988; Priest, 1989, 1991; Kifer & Lozinskii, 1992; Arieli & Avron, 1996;
Arieli, 1999, 2003), implies Asystem may useful reasoning general
uncertain theories (not necessarily form databases).
important note composing system inherits functionality
underlying solver. outcome flexibility, modularity, simple interaction
different sources information, ability reason set first-order formulae integrity constraints29 . best knowledge application data
integration ability.
several directions exploration. First, already noted, two
phases, considered here, might needed complete process
data integration:
29. Provided, of-course, constraints lead floundering.

279

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

a) translation difference concepts unified ontology,
b) integration integrity constraints.
far, formalisms dealing first item (e.g., Lenzerini, 2001, 2002; Van Nuffelen
et al., 2004) mainly focus mutual relations global schema source
(local) schemas, particular concepts ontology map other.
hand, formalisms handling second item concentrate nonmonotonic reasoning dynamically evolving (and mutually inconsistent) worlds. synthesis main
ideas behind approaches, incorporating system, major challenge
future work.
Another important issue deserves attention repair inconsistency
context deductive databases integrity constraints definitions predicates, often called view predicates. refer (Denecker, 2000) sketch may
done. kind data may combined (possibly inconsistent) temporal
information, (partial) transactions, (contradictory) update information.
Finally, since different databases may different information predicates, reasonable use weakened version closed word assumption part
integration process (for instance, assumption something false unless
database, unless database information it).

Acknowledgements
would like thank anonymous reviewers many helpful comments suggestions. research supported Research Fund K.U.Leuven FWO
Vlaanderen.

References
Alchourron, C. E., Gardenfors, P., & Makinson, D. (1995). logic theory change:
Partial meet contraction revision function. Journal Symbolic Logic, 50, 510
530.
Alferes, J. J., Leite, J. A., Pereira, L. M., & Quaresma, P. (2000). Planning abductive updating. Proc. Symposium AI Planning Intelligent Agents (AISB00),
pp. 18.
Alferes, J. J., Pereira, L. M., Przymusinska, H., & Przymusinski, T. C. (2002). LUPS
language updating logic programs. Artificial Intelligence, 138 (12), 87116.
Arenas, M., Bertossi, L., & Chomicki, J. (1999). Consistent query answers inconsistent
databases. Proc. 18th ACM Symp. Principles Database Systems (PODS99),
pp. 6879.
Arenas, M., Bertossi, L., & Chomicki, J. (2003). Answer sets consistent query answering
inconsistent databases. Theory Practice Logic Programming, 3 (45), 393
424.
280

fiCoherent integration databases abductive logic programming

Arenas, M., Bertossi, L., & Kifer, M. (2000). Applications annotated predicate calculus
querying inconsistent databases. Lloyd, J., et al. (Eds.), Proc. 1st Int. Conf.
Computational Logic (CL2000), No. 1861 LNAI, pp. 926941. Springer.
Arieli, O. (1999). Four-valued logics reasoning uncertainty prioritized data.
Bouchon-Meunier, B., Yager, R. R., & Zadeh, L. (Eds.), Information, Uncertainty,
Fusion, pp. 263309. Kluwer.
Arieli, O. (2003). Reasoning different levels uncertainty. Journal Applied NonClassical Logics, 13 (34), 317343.
Arieli, O., & Avron, A. (1996). Reasoning logical bilattices. Journal Logic, Language,
Information, 5 (1), 2563.
Arieli, O., & Avron, A. (1999). model theoretic approach recover consistent data
inconsistent knowledge-bases. Journal Automated Reasoning, 22 (3), 263309.
Arieli, O., Denecker, M., Van Nuffelen, B., & Bruynooghe, M. (2002). Repairing inconsistent
databases: model-theoretic approach abductive reasoning. Proc. ICLP02
Workshop Paraconsistent Computational Logic (PCL02), Federated Logic Conference (FLOC02), pp. 5165.
Arieli, O., Denecker, M., Van Nuffelen, B., & Bruynooghe, M. (2004). Database repair
signed formulae. Seipel, D., & Turell-Torres, J. (Eds.), Proc. 3rd Int. Symp.
Foundations Information Knowledge Systems (FoIKS04), No. 2942 LNCS,
pp. 1430. Springer.
Arieli, O., Van Nuffelen, B., Denecker, M., & Bruynooghe, M. (2001). Coherent composition
distributed knowledge-bases abduction. Nieuwenhuis, A., & Voronkov,
A. (Eds.), Proc. 8th Int. Conf. Logic Programming, Artificial Intelligence Reasoning (LPAR01), No. 2250 LNCS, pp. 620635. Springer.
Avron, A. (2002). Classical gentzen-type methods propositional many-valued logics.
Fitting, M., & Orlowska, E. (Eds.), Theory Applications Multiple-Valued
Logics, pp. 113151. Springer.
Baral, C., Kraus, S., & Minker, J. (1991). Combining multiple knowledge bases. IEEE
Trans. Knowledge Data Enginnering, 3 (2), 208220.
Baral, C., Kraus, S., Minker, J., & Subrahmanain, V. S. (1992). Combining multiple knowledge bases consisting first order theories. Computational Intelligence, 8, 4571.
Batini, C., Lenzerini, M., & Navathe, S. B. (1986). comparative analysis methodologies
database schema integration. ACM Computing Surveys, 18 (4), 323364.
Belnap, N. D. (1977). computer think. Ryle, G. (Ed.), Contemporary
Aspects Philosophy, pp. 3056. Oriel Press.
Benferhat, S., Cayrol, C., Dubois, D., Lang, J., & Prade, H. (1993). Inconsistency management prioritized syntax-based entailment. Proc. 13th Int. Joint Conf.
Artificial Intelligence (IJCAI93), pp. 640645.
Benferhat, S., Dubois, D., & Prade, H. (1995). infer inconsistent beliefs without
revising?. Proc. 14th Int. Joint Conf. Artificial Intelligence (IJCAI95), pp.
14491455.
281

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

Bertossi, L., Arenas, M., & Ferretti, C. (1998). SCDBR: automated reasoner specifications database updates. Intelligent Information Systems, 10 (3), 253280.
Bertossi, L., Chomicki, J., Cortes, A., & Gutierrez, C. (2002). Consistent answers integrated data sources. Andreasen, A., et al. (Eds.), Proc. Flexible Query Answering
Systems (FQAS2002), No. 2522 LNCS, pp. 7185. Springer.
Bertossi, L., & Schwind, C. (2002). Analytic tableau database repairs: Foundations.
Proc. 2nd Int. Symp. Foundations Information Knowledge Systems
(FoIKS02), No. 2284 LNCS, pp. 3248. Springer.
Bravo, L., & Bertossi, L. (2003). Logic programming consistently querying data integration systems. Gottlob, G., & Walsh, T. (Eds.), Proc. Int. Jont Conf. Artificial
Intelligence (IJCAI03), pp. 1015.
Bry, F. (1997). Query answering information systems integrity constraints. Proc.
Int. Conf. Integrity Control Information Systems (IICIS97), pp. 113130.
Cali, A., Calvanese, D., De Giacomo, G., & Lenzerini, M. (2002). Data integration
integrity constraints. Proc. 14th Conf. Advanced Information Systems Engineering (CAiSE 2002), pp. 262279.
Cali, A., Lembo, D., & Rosati, R. (2003). Query rewriting answering constraints
data integration systems. Gottlob, G., & Walsh, T. (Eds.), Proc. Int. Jont Conf.
Artificial Intelligence (IJCAI03), pp. 1621.
Carnielli, W. A., & Marcos, J. (2001). Tableau systems logics formal inconsistency.
Proc. Int. Conf. Artificial Intelligence (IC-AI01), Vol. II, pp. 848852. CSREA
Press.
Carnielli, W. A., & Marcos, J. (2002). taxonomy C-systems. Carnielli, W. A.,
Coniglio, M. E., & DOttaviano, I. M. (Eds.), Paraconsistency Logical Way
Inconsistent, No. 228 Lecture Notes Pure Applied Mathematics, pp.
194. Marcel Dekker.
Console, L., Theseider Dupre, D., & Torasso, P. (1991). relationship abduction deduction. Journal Logic Computation, 1 (5), 661690.
Dalal, M. (1988). Investigations theory knowledge base revision. Proc. National
Conference Artificial Intelligence (AAAI98), pp. 475479. AAAI Press.
de Amo, S., Carnielli, W. A., & Marcos, J. (2002). logical framework integrating inconsistent information multiple databases. Proc. 2nd Int. Symp. Foundations
Information Knowledge Systems (FoIKS02), No. 2284 LNCS, pp. 6784.
Springer.
Decker, H. (2003). Historical computational aspects paraconsistency view
logic foundations databases. Proc. Semantics Databases, No. 2582 LNCS,
pp. 6381. Springer.
Denecker, M. (2000). Extending classical logic inductive definitions. Lloyd, J., et al.
(Eds.), Proc. 1st Int. Conf. Computational Logic (CL2000), No. 1861 LNAI,
pp. 703717. Springer.
282

fiCoherent integration databases abductive logic programming

Denecker, M., & De Schreye, D. (1992). SLDNFA abductive procedure normal
abductive programs. Apt, K. R. (Ed.), Proc. Int. Joint Conf. Symp. Logic
Programming, pp. 686700. MIT Press.
Denecker, M., & De Schreye, D. (1993). Justification semantics: unifying framework
semantics logic programs. Proc. Logic Programming Nonmonotonic
Reasoning Workshop, pp. 365379. MIT Press.
Denecker, M., & De Schreye, D. (1998). SLDNFA abductive procedure abductive
logic programs. Journal Logic Programming, 34 (2), 111167.
Denecker, M., & Kakas, A. C. (2000). Abductive Logic Programming. special issue
Journal Logic Programming, Vol.44(13).
Fitting, M. (1989). Negation refutation. Proc. 4th Annual Symp. Logic Computer
Science (LICS89), pp. 6370. IEEE Press.
Fitting, M. (1990). Kleenes logic, generalized. Journal Logic Computation, 1, 797
810.
Fitting, M. (1991). Bilattices semantics logic programming. Journal Logic
Programming, 11 (2), 91116.
Franconi, E., Palma, A. L., Leone, N., Perri, D., & Scarcello, F. (2001). Census data repair:
challenging application disjunctive logic programming. Nieuwenhius, A., &
Voronkov, A. (Eds.), Proc. 8th Int. Conf. Logic Programming, Artificial Intelligence
Reasoning (LPAR01), No. 2250 LNCS, pp. 561578. Springer.
Gardenfors, P., & Rott, H. (1995). Belief revision. Gabbay, D. M., Hogger, J., & Robinson,
J. A. (Eds.), Handbook Logic Artificial Intelligence Logic Programming, pp.
35132. Oxford University Press.
Gertz, M., & Lipeck, U. W. (1997). extensible framework repairing constraint violations. Proc. Int. Conf. Integrity Control Information Systems (IICIS97),
pp. 89111.
Ginsberg, M. L. (1988). Multi-valued logics: uniform approach reasoning AI. Computer Intelligence, 4, 256316.
Greco, G., Greco, S., & Zumpano, E. (2001). logic programming approach integration, repairing querying inconsistent databases. Proc. 17th Int. Conf.
Logic Programming (ICLP01), No. 2237 LNCS, pp. 348363. Springer.
Greco, S., & Zumpano, E. (2000). Querying inconsistent databases. Proc. Int. Conf.
Logic Programming Automated Reasoning (LPAR2000), No. 1955 LNAI, pp.
308325. Springer.
Inoue, K., & Sakama, C. (1995). Abductive framework nonmonotonic theory change.
Proc. 14th Int. Joint Conf. Artificial Intelligence (IJCAI95), pp. 204210.
Kakas, A., Kowalski, R. A., & Toni, F. (1992). Abductive logic programming. Journal
Logic Computation, 2 (6), 719770.
Kakas, A., & Mancarella, P. (1990a). Database updates abduction. Proc. 16th
Int. Conf. Large Data Bases (VLDB90), pp. 650661.
283

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

Kakas, A., & Mancarella, P. (1990b). Generalised stable models: semantics abduction.
Proc. European Conference Artificial Intelligence (ECAI90), pp. 385391.
John Wiley sons.
Kakas, A., Michael, A., & Mourlas, C. (2000). ACLP: Abductive constraint logic programming. Journal Logic Programming, 44 (13), 129177.
Kakas, A., Van Nuffelen, B., & Denecker, M. (2001). A-System: Problem solving
abduction. Proc. 17th Int. Joint Conf. Artificial Intelligence (IJCAI01), pp.
591596. Morgan Kaufmann Publishers.
Kifer, M., & Lozinskii, E. L. (1992). logic reasoning inconsistency. Journal
Automated Reasoning, 9 (2), 179215.
Konieczny, S., & Pino Perez, R. (2002). Merging information constraints: logical
farmework. Journal Logic Computation, 12 (5), 773808.
Kowalski, R. A., & Sergot, M. J. (1986). logic-based calculus events. New Generation
Computing, 4 (1), 6795. Reprinted Foundations Knowledge Base Management
Systems (1989), pp. 2353. Springer-Verlag.
Lenzerini, M. (2001). Data integration needs reasoning. Eiter, T., Faber, W., &
Truszczynski, T. (Eds.), Proc Int. Conf. Logic Programming Non-Monotonic
Reasoning (LPNMR01), No. 2173 LNAI, pp. 5461. Springer.
Lenzerini, M. (2002). Data integration: theoretical perspective. Proc. ACM Symp.
Principles Database Systems (PODS02), pp. 233246.
Liberatore, P., & Schaerf, M. (2000). BReLS: system integration knowledge
bases. Proc Int. Conf. Principles Knowledge Representation Reasoning
(KR2000), pp. 145152. Morgan Kaufmann Publishers.
Lin, J. (1996). semantics reasoning consistently presence inconsistency.
Artificial Intelligence, 86 (1), 7595.
Lin, J., & Mendelzon, A. (1998). Merging databases constraints. Int. Journal
Cooperative Information Systems, 7 (1), 5576.
Lloyd, J. W. (1987). Foundations Logic Programming. Springer-Verlag.
Lloyd, J. W., & Topor, R. W. (1984). Making Prolog expressive. Journal Logic
Programming, 1 (3), 225240.
Mareco, C. A., & Bertossi, L. (1999). Specification implementation temporal
databases bitemporal event calculus. Advance Conceptual Modeling, No.
1727 LNCS, pp. 7485. Springer.
Martelli, A., & Montanari, U. (1982). efficient unification algorithm. ACM Trans.
Programming Languages Systems, 4 (2), 258282.
Messing, B. (1997). Combining knowledge many-valued logics. Data Knowledge
Engineering, 23, 297315.
Olive, A. (1991). Integrity checking deductive databases. Proc. 17th Int. Conf.
Large Data Bases (VLDB91), pp. 513523.
284

fiCoherent integration databases abductive logic programming

Pereira, L. M., Aparicio, J. N., & Alferes, J. J. (1991). Hypothetical reasoning well
founded semantics. Proc. 3th Scandinavian Conference AI, pp. 289300. IOS
Press.
Priest, G. (1989). Reasoning truth. Artificial Intelligence, 39, 231244.
Priest, G. (1991). Minimally inconsistent LP. Studia Logica, 50, 321331.
Rahm, E., & Bernstein, P. (2001). survey approaches automatic schema matching.
VLDB Journal, 10 (4), 334350.
Reiter, R. (1995). specifying database updates. Journal Logic Programming, 25 (1),
5391.
Revesz, P. Z. (1993). semantics theory change: Arbitration old new
information. Proc. 12th ACM Symp. Principles Database Systems (PODS93),
pp. 7182.
Sakama, C., & Inoue, K. (1999). Updating extended logic programs abduction.
Proc 5th Int. Conf. Logic Programming Non-Monotonic Reasoning (LPNMR99), pp. 147161.
Sakama, C., & Inoue, K. (2000). Abductive logic programming disjunctive logic programming: relationship transferability. Journal Logic Programming, 44 (1
3), 75100.
Shoham, Y. (1988). Reasoning change: time causation standpoint
artificial intelligence. MIT Press.
Sripada, S. M. (1995). Efficient implementation event calculus temporal database
applications. Proc. Int. Conf. Logic Programming (ICLP95), pp. 99113.
Subrahmanian, V. S. (1990). Mechanical proof procedures many valued lattice-based
logic programming. Journal Non-Classical Logic, 7, 741.
Subrahmanian, V. S. (1994). Amalgamating knowledge-bases. ACM Trans. Database
Systems, 19 (2), 291331.
Ullman, J. (2000). Information integration using logical views. Theoretical Computer Science, 239 (2), 189210.
Van Nuffelen, B., Cortes, A., Denecker, M., Arieli, O., & Bruynooghe, M. (2004). Data
integration using ID-logic. Proc. 16th Int. Conf. Advanced Information Systems
Engineering (CAiSE04), LNCS. Springer. appear.
Van Nuffelen, B., & Kakas, A. (2001). A-system: Declarative programming abduction. Proc. Int. Conf. Logic Programming Non-Monotonic Reasoning (LPNMR01), No. 2173 LNAI, pp. 393396. Springer.
Verbaeten, S. (1999). Termination analysis abductive general logic programs. De Schreye, D. (Ed.), Proc. Int. Conf. Logic Programming (ICLP99), pp. 365379. MIT
Press.
Verbaeten, S., Denecker, M., & De Schreye, D. (1997). Compositionality normal open logic
programs. Maluszynski, J. (Ed.), Proc. Int. Logic Programming Symp. (ILPS97),
pp. 371386. MIT Press.
285

fiArieli, Denecker, Van Nuffelen, & Bruynooghe

Verbaeten, S., Denecker, M., & De Schreye, D. (2000). Compositionality normal open
logic programs. Journal Logic Programming, 41 (3), 151183.
Winslett, M. (1988). Reasoning action using possible models approach. Proc.
National Conference Artificial Intelligence (AAAI98), pp. 8993. AAAI press.

286

fiJournal Artificial Intelligence Research 21 (2004) 63-100

Submitted 8/03; published 2/04

Competitive Coevolution Evolutionary
Complexification
Kenneth O. Stanley
Risto Miikkulainen

kstanley@cs.utexas.edu
risto@cs.utexas.edu

Department Computer Sciences
University Texas Austin
Austin, TX 78712 USA

Abstract
Two major goals machine learning discovery improvement solutions
complex problems. paper, argue complexification, i.e. incremental elaboration solutions adding new structure, achieves goals. demonstrate power complexification NeuroEvolution Augmenting Topologies
(NEAT) method, evolves increasingly complex neural network architectures. NEAT
applied open-ended coevolutionary robot duel domain robot controllers compete head head. robot duel domain supports wide range strategies,
coevolution benefits escalating arms race, serves suitable testbed
studying complexification. compared evolution networks fixed structure, complexifying evolution discovers significantly sophisticated strategies.
results suggest order discover improve complex solutions, evolution,
search general, allowed complexify well optimize.

1. Introduction
Evolutionary Computation (EC) class algorithms applied open-ended
learning problems Artificial Intelligence. Traditionally, algorithms evolve fixedlength genomes assumption space genome sufficient encode
solution. genome containing n genes encodes single point n-dimensional search
space. many cases, solution known exist somewhere space. example,
global maximum function three arguments must exist three dimensional
space defined arguments. Thus, genome three genes encode location
maximum.
However, many common structures defined indefinite number parameters.
particular, solution types contain variable number parts
represented number parameters minimum. example, number
parts neural networks, cellular automata, electronic circuits vary (Miller, Job,
& Vassilev, 2000a; Mitchell, Crutchfield, & Das, 1996; Stanley & Miikkulainen, 2002d).
fact, theoretically two neural networks different numbers connections nodes
represent function (Cybenko, 1989). Thus, clear number genes
appropriate solving particular problem. Researchers evolving fixed-length genotypes
must use heuristics estimate priori appropriate number genes encode
structures.

c
2004
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiStanley & Miikkulainen

major obstacle using fixed-length encodings heuristically determining
appropriate number genes becomes impossible complex problems. example,
many nodes connections necessary neural network controls pingpong playing robot? Or, many bits needed neighborhood function
cellular automaton performs information compression? answers questions
hardly based empirical experience analytic methods, since little known
solutions. One possible approach simply make genome extremely large,
space encodes extremely large solution likely lie somewhere
within. Yet larger genome, higher dimensional space evolution needs
search. Even ping-pong playing robot lies somewhere 10,000 dimensional space
10,000 gene genome, searching space may take prohibitively long.
Even problematic open-ended problems phenotypes meant improve indefinitely known final solution. example, competitive games,
estimating complexity best possible player difficult estimate
implicitly assumes better player exist, cannot always know. Moreover,
many artificial life domains aimed evolving increasingly complex artificial creatures
long possible (Maley, 1999). continual evolution difficult fixed
genome two reasons: (1) good strategy found fixed-length genome,
entire representational space genome used encode it. Thus, way
improve alter strategy, thereby sacrificing functionality learned
previous generations. (2) Fixing size genome domains arbitrarily fixes
maximum complexity evolved creatures, defeating purpose experiment.
paper, argue structured phenotypes evolved effectively starting evolution population small, simple genomes systematically elaborating
generations adding new genes. new gene expands search space,
adding new dimension previously exist. way, evolution begins searching
small easily-optimized space, adds new dimensions necessary. approach
likely discover highly complex phenotypes approach begins searching directly intractably large space complete solutions. fact, natural evolution
utilizes strategy, occasionally adding new genes lead increased phenotypic complexity (Martin 1999; Section 2). biology, process incremental elaboration called
complexification, use term describe approach well.
evolutionary computation, complexification refers expanding dimensionality
search space preserving values majority dimensions.
words, complexification elaborates existing strategy adding new structure without
changing existing representation. Thus strategy become different,
number possible responses situations generate increases (Figure 1).
EC domain neuroevolution (i.e. evolving neural networks), complexification
means adding nodes connections already-functioning neural networks.
main idea behind NEAT (NeuroEvolution Augmenting Topologies; Stanley Miikkulainen 2002b,c,d), method described paper. NEAT begins evolving networks
without hidden nodes. many generations, new hidden nodes connections
added, complexifying space potential solutions. way, complex strategies
elaborate simpler strategies, focusing search solutions likely maintain
existing capabilities.
64

fiCompetitive Coevolution Evolutionary Complexification

Alteration
Original Strategy

Strategy Fails

Altered Strategy

Strategy Fails

Elaboration
Original Strategy

Strategy Fails

Elaborated Strategy

Skill Remains!

Figure 1: Alteration vs. elaboration example.

dark robot must evolve avoid
lighter robot, attempts cause collision. alteration scenario (top),
dark robot first evolves strategy go around left side opponent. However,
strategy fails future generation opponent begins moving left.
Thus, dark robot alters strategy evolving tendency move right instead
left. However, light robot later moves right, new, altered, strategy fails
dark robot retain old ability move left. elaboration
scenario (bottom), original strategy moving left also fails. However, instead
altering strategy, elaborated adding new ability move right well. Thus,
opponent later moves right, dark robot still ability avoid
using original strategy. Elaboration necessary coevolutionary arms race
emerge achieved complexification.

Expanding length size genome found effective previous
work (Cliff, Harvey, & Husbands, 1993; Harvey, 1993; Koza, 1995; Lindgren & Johansson,
2001). NEAT advances idea making possible search wide range increasingly complex network topologies simultaneously. process based three technical
components: (1) Keeping track genes match among differently sized
genomes throughout evolution; (2) speciating population solutions differing
complexity exist independently; (3) starting evolution uniform population
small networks. components work together complexifying solutions part
evolutionary process. prior work, NEAT shown solve challenging reinforcement learning problems efficiently neuroevolution methods (Stanley
Miikkulainen 2002b,c,d). focus studies optimizing given fitness
function complexifying evolution.

65

fiStanley & Miikkulainen

paper, focus open-ended problems explicit fitness function;
instead, fitness depends comparisons agents also evolving. goal
discover creative solutions beyond designers ability define fitness function.
difficult continually improve solutions coevolutionary domains evolution
tends oscillate idiosyncratic yet uninteresting solutions (Floreano & Nolfi, 1997).
Complexification encourages continuing innovation elaborating existing solutions.
order demonstrate power complexification coevolution, NEAT applied
competitive robot control domain Robot Duel. known optimal strategy
domain substantial room come increasingly sophisticated
strategies. main results (1) evolution complexify possible, (2)
complexification led elaboration, (3) significantly sophisticated successful
strategies evolved complexification without it. results imply
complexification allows establishing coevolutionary arms race achieves significantly
higher level sophistication otherwise possible.
begin reviewing biological support complexification, well past work
coevolution, followed description NEAT method, experimental results.

2. Background
natural process complexification led important biological innovations.
natural application EC competitive coevolution, reviewed below.
2.1 Complexification Nature
Mutation nature results optimizing existing structures: New genes occasionally added genome, allowing evolution perform complexifying function
optimization. addition, complexification protected nature interspecies mating prohibited. speciation creates important dynamics differing
standard GAs. section, discuss characteristics natural evolution
basis approach utilizing computationally genetic algorithms.
Gene duplication special kind mutation one parental genes
copied offsprings genome once. offspring redundant genes
expressing proteins. Gene duplication responsible key innovations
overall body morphology course natural evolution (Amores, Force, Yan, Joly,
Amemiya, Fritz, Ho, Langeland, Prince, Wang, Westerfield, Ekker, & Postlethwait, 1998;
Carroll, 1995; Force, Lynch, Pickett, Amores, lin Yan, & Postlethwait, 1999; Martin, 1999).
major gene duplication event occurred around time vertebrates separated
invertebrates. evidence duplication centers around HOX genes,
determine fate cells along anterior-posterior axis embryos. HOX genes
crucial shaping overall pattern development embryos. fact, differences
HOX gene regulation explain great deal diversity among arthropods tetrapods
(Carroll, 1995). Invertebrates single HOX cluster vertebrates four, suggesting cluster duplication significantly contributed elaborations vertebrate bodyplans (Amores et al., 1998; Holland, Garcia-Fernandez, Williams, & Sidow, 1994; Nadeau
& Sankoff, 1997; Postlethwait, Yan, Gates, Horne, Amores, Brownlie, & Donovan, 1998;
Sidow, 1996). additional HOX genes took new roles regulating vertebrate
66

fiCompetitive Coevolution Evolutionary Complexification

anterior-posterior axis develops, considerably increasing body-plan complexity. Although
Martin (1999) argues additional clusters explained many single gene duplications accumulating generations, opposed massive whole-genome duplications,
researchers agree gene duplication form contributed significantly body-plan
elaboration.
detailed account duplicate genes take novel roles given Force et al.
(1999): Base pair mutations generations following duplication partition initially
redundant regulatory roles genes separate classes. Thus, embryo develops
way, genes determine overall body-plan confined specific
roles, since them. partitioning phase completes redundant
clusters genes separated enough longer produce identical proteins
time. partitioning, mutations within duplicated cluster genes
affect different steps development mutations within original cluster.
words, duplication creates points mutations occur. manner,
developmental processes complexify.
Gene duplication possible explanation natural evolution indeed expanded
size genomes throughout evolution, provides inspiration adding new genes
artificial genomes well. fact, gene duplication motivated Koza (1995) allow entire
functions genetic programs duplicated single mutation, later differentiated mutations. evolving neural networks, process means
adding new neurons connections networks.
order implement idea artificial evolutionary systems, faced
two major challenges. First, systems evolve differently sized shaped network
topologies, difficult cross without losing information. example,
depending new structure added, gene may exist different positions,
conversely, different genes may exist position. Thus, artificial crossover may
disrupt evolved topologies misalignment. Second, variable-length genomes,
may difficult find innovative solutions. Optimizing many genes takes longer
optimizing few, meaning complex networks may eliminated
population sufficient opportunity optimized.
However, biological evolution also operates variable-length genomes, problems stop complexification nature. problems avoided biological
evolution? First, nature mechanism aligning genes counterparts
crossover, data lost obscured. alignment process
clearly observed E. coli (Radding, 1982; Sigal & Alberts, 1972). special protein called
RecA takes single strand DNA aligns another strand genes express
traits, called homologous genes. process called synapsis.
Second, innovations nature protected speciation. Organisms significantly divergent genomes never mate different species. organism
could mate other, organisms initially larger, less-fit genomes would forced
compete mates simpler, fit counterparts. result, larger,
innovative genomes would fail produce offspring disappear population. contrast, speciated population, organisms larger genomes compete
mates among species, instead population large. way, organisms may initially lower fitness general population still chance
67

fiStanley & Miikkulainen

reproduce, giving novel concepts chance realize potential without prematurely eliminated. speciation benefits evolution diverse populations,
variety speciation methods employed EC (Goldberg & Richardson, 1987;
Mahfoud, 1995; Ryan, 1994).
turns complexification also possible evolutionary computation abstractions
synapsis speciation made part genetic algorithm. NEAT method
(section 3) implementation idea: genome complexified adding new
genes turn encode new structure phenotype, biological evolution.
Complexification especially powerful open-ended domains goal
continually generate sophisticated strategies. Competitive coevolution particularly
important domain, reviewed next section.
2.2 Competitive Coevolution
competitive coevolution, individual fitness evaluated competition
individuals population, rather absolute fitness measure.
words, fitness signifies relative strengths solutions; increased fitness one
solution leads decreased fitness another. Ideally, competing solutions continually
outdo one another, leading arms race increasingly better solutions (Dawkins &
Krebs, 1979; Rosin, 1997; Van Valin, 1973). Competitive coevolution traditionally
used two kinds problems. First, used evolve interactive behaviors
difficult evolve terms absolute fitness function. example, Sims (1994)
evolved simulated 3D creatures attempted capture ball opponent did,
resulting variety effective interactive strategies. Second, coevolution used
gain insight dynamics game-theoretic problems. example, Lindgren &
Johansson (2001) coevolved iterated Prisoners Dilemma strategies order demonstrate
correspond stages natural evolution.
Whatever goal competitive coevolution experiment, interesting strategies
evolve arms race continues significant number generations. practice,
difficult establish arms race. Evolution tends find simplest solutions
win, meaning strategies switch back forth different idiosyncratic
yet uninteresting variations (Darwen, 1996; Floreano & Nolfi, 1997; Rosin & Belew, 1997).
Several methods developed encourage arms race (Angeline & Pollack, 1993;
Ficici & Pollack, 2001; Noble & Watson, 2001; Rosin & Belew, 1997). example, hall
fame collection past good strategies used ensure current strategies
remain competitive earlier strategies. Recently, Ficici Pollack (2001) Noble
Watson (2001) introduced promising method called Pareto coevolution, finds
best learners best teachers two populations casting coevolution multiobjective optimization problem. information enables choosing best individuals
reproduce, well maintaining informative diverse set opponents.
Although techniques allow sustaining arms race longer, directly
encourage continual coevolution, i.e. creating new solutions maintain existing capabilities. example, matter well selection performed, well competitors
chosen, search space fixed, limit eventually reached. Also, may

68

fiCompetitive Coevolution Evolutionary Complexification

occasionally easier escape local optimum adding new dimension search
space searching new path original space.
reasons, complexification natural technique establishing coevolutionary arms race. Complexification elaborates strategies adding new dimensions
search space. Thus, progress made indefinitely long: Even global optimum
reached search space solutions, new dimensions added, opening
higher-dimensional space even better optima may exist.
test idea experimentally, chose robot duel domain combines predator/prey interaction food foraging novel head-to-head competition (Section 4).
use domain demonstrate NEAT uses complexification continually elaborate solutions. next section reviews NEAT neuroevolution method, followed
description robot duel domain discussion results.

3. NeuroEvolution Augmenting Topologies (NEAT)
NEAT method evolving artificial neural networks combines usual search
appropriate network weights complexification network structure. approach
highly effective, shown e.g. comparison neuroevolution (NE) methods
double pole balancing benchmark task (Stanley & Miikkulainen, 2002b,c,d). NEAT
method consists solutions three fundamental challenges evolving neural network
topology: (1) kind genetic representation would allow disparate topologies
crossover meaningful way? solution use historical markings line genes
origin. (2) topological innovation needs generations
optimize protected disappear population prematurely?
solution separate innovation different species. (3) topologies
minimized throughout evolution efficient solutions discovered?
solution start minimal structure add nodes connections incrementally.
section, explain solutions implemented NEAT.
3.1 Genetic Encoding
Evolving structure requires flexible genetic encoding. order allow structures complexify, representations must dynamic expandable. genome NEAT
includes list connection genes, refers two node genes connected.
(Figure 2). connection gene specifies in-node, out-node, weight connection, whether connection gene expressed (an enable bit), innovation
number, allows finding corresponding genes crossover.
Mutation NEAT change connection weights network structures. Connection weights mutate NE system, connection either perturbed not.
Structural mutations, form basis complexification, occur two ways (Figure
3). mutation expands size genome adding gene(s). add connection
mutation, single new connection gene added connecting two previously unconnected
nodes. add node mutation, existing connection split new node placed
old connection used be. old connection disabled two new connections added genome. connection first node chain
new node given weight one, connection new node last
69

fiStanley & Miikkulainen

Genome (Genotype)
Node
Genes
Connect.

Genes

Node 1 Node 2 Node 3 Node 4 Node 5
Sensor Sensor Sensor Output Hidden
1
4
Weight 0.7
Enabled
Innov 1

2
4
Weight0.5
DISABLED
Innov 2

Network (Phenotype)
1

3
4
Weight 0.5
Enabled
Innov 3

2
5
Weight 0.2
Enabled
Innov 4

5
4
Weight 0.4
Enabled
Innov 5

1
5
Weight 0.6
Enabled
Innov 6

4
5
Weight 0.6
Enabled
Innov 11

4
5
2

3

Figure 2: NEAT genotype phenotype mapping example. genotype depicted
produces shown phenotype. 3 input nodes, one hidden, one output
node, seven connection definitions, one recurrent. second gene
disabled, connection specifies (between nodes 2 4) expressed
phenotype. order allow complexification, genome length unbounded.

node chain given weight connection split. Splitting connection way introduces nonlinearity (i.e. sigmoid function) none
before. initialized way, nonlinearity changes function slightly,
new node immediately integrated network. Old behaviors encoded
preexisting network structure destroyed remain qualitatively same,
new structure provides opportunity elaborate original behaviors.
mutation, genomes NEAT gradually get larger. Genomes varying
sizes result, sometimes different connections positions. Crossover must
able recombine networks differing topologies, difficult (Radcliffe,
1993). next section explains NEAT addresses problem.
3.2 Tracking Genes Historical Markings
turns historical origin gene used tell us exactly genes
match individuals population. Two genes historical
origin represent structure (although possibly different weights), since
derived ancestral gene point past. Thus,
system needs keep track historical origin every gene system.
Tracking historical origins requires little computation. Whenever new gene
appears (through structural mutation), global innovation number incremented
assigned gene. innovation numbers thus represent chronology every gene
system. example, let us say two mutations Figure 3 occurred one
70

fiCompetitive Coevolution Evolutionary Complexification

1

2

3

4

5

6

1>4 2>4 3>4 2>5 5>4 1>5
DIS

1

2

3

Mutate Add Connection

4

1

2

2

1

3

3

4

5

6

5

6

7

1

2

2

3

3

4

8

9

1>4 2>4 3>4 2>5 5>4 1>5 3>6 6>4
DIS DIS

Mutate Add Node

4

4

5
2

6

5

1>4 2>4 3>4 2>5 5>4 1>5
DIS

1

5

4

5
1

4

1>4 2>4 3>4 2>5 5>4 1>5 3>5
DIS

5
1

3

2

6

3

Figure 3: two types structural mutation NEAT. types, adding connection
adding node, illustrated genes phenotypes. top number
genome innovation number gene. bottom two numbers denote
two nodes connected gene. weight connection, also encoded
gene, shown. symbol DIS means gene disabled, therefore
expressed network. figure shows connection genes appended
genome new connection new node added network. Assuming
depicted mutations occurred one other, genes would assigned increasing
innovation numbers figure illustrates, thereby allowing NEAT keep implicit
history origin every gene population.

another system. new connection gene created first mutation assigned
number 7, two new connection genes added new node mutation
assigned numbers 8 9. future, whenever genomes crossover,
offspring inherit innovation numbers gene. Thus, historical origin
every gene system known throughout evolution.
possible problem structural innovation receive different innovation
numbers generation occurs chance once. However, keeping
list innovations occurred current generation, possible ensure
structure arises independent mutations
generation, identical mutation assigned innovation number.
extensive experimentation, established resetting list every generation opposed
keeping growing list mutations throughout evolution sufficient prevent
explosion innovation numbers.
innovation numbers, system knows exactly genes match
(Figure 4). Genes match either disjoint excess, depending
whether occur within outside range parents innovation numbers.
crossing over, genes innovation numbers lined

71

fiStanley & Miikkulainen

Parent1
1
2
3
1>4 2>4 3>4
DISAB

Parent2

4
2>5

8
5
5>4 1>5

3
6
1
2
4
5
1>4 2>4 3>4 2>5 5>4 5>6
DISAB
DISAB

10
1>6

6

5
2

9
3>5

4

4

1

7
6>4

5

3

1

2

3

disjoint

Parent1
Parent2

1
2
3
1>4 2>4 3>4
DISAB

4
2>5

8
1>5

5
5>4

3
6
1
2
4
5
1>4 2>4 3>4 2>5 5>4 5>6
DISAB
DISAB

9
3>5

7
6>4

excess excess

disjointdisjoint

Offspring

3
1
2
4
5
6
1>4 2>4 3>4 2>5 5>4 5>6
DISAB
DISAB

7
6>4

10
1>6

8
1>5

9
3>5

10
1>6

4
6
5
1

2

3

Figure 4: Matching genomes different network topologies using innovation numbers. Although Parent 1 Parent 2 look different, innovation numbers (shown
top gene) tell us several genes match even without topological analysis. new structure combines overlapping parts two parents
well different parts created crossover. case, equal fitnesses
assumed, disjoint excess genes parents inherited randomly.
Otherwise would inherited fit parent. disabled genes may
become enabled future generations: preset chance inherited
gene disabled disabled either parent.

randomly chosen offspring genome. Genes match inherited
fit parent, equally fit, parents randomly. Disabled genes
25% chance reenabled crossover, allowing networks make use older
genes again.1
Historical markings allow NEAT perform crossover without need expensive
topological analysis. Genomes different organizations sizes stay compatible throughout evolution, problem comparing different topologies essentially avoided.
1. See Appendix specific mating parameters used experiments paper.

72

fiCompetitive Coevolution Evolutionary Complexification

methodology allows NEAT complexify structure still maintaining genetic compatibility. However, turns population varying complexities cannot maintain
topological innovations own. smaller structures optimize faster larger
structures, adding nodes connections usually initially decreases fitness
network, recently augmented structures little hope surviving one generation even though innovations represent might crucial towards solving
task long run. solution protect innovation speciating population,
explained next section.
3.3 Protecting Innovation Speciation
NEAT speciates population individuals compete primarily within
niches instead population large. way, topological innovations protected time optimize structure compete
niches population. addition, speciation prevents bloating genomes: Species
smaller genomes survive long fitness competitive, ensuring small networks
replaced larger ones unnecessarily. Protecting innovation speciation follows philosophy new ideas must given time reach potential
eliminated.
Historical markings make possible system divide population species
based topological similarity. measure distance two network encodings linear combination number excess (E) disjoint (D) genes, well
average weight differences matching genes (W ):
=

c1 E c2
+
+ c3 W .
N
N

(1)

coefficients c1 , c2 , c3 adjust importance three factors, factor
N , number genes larger genome, normalizes genome size (N set
1 genomes small). Genomes tested one time; genomes distance
randomly chosen member species less , compatibility threshold,
placed species. genome placed first species previous
generation condition satisfied, genome one species.
genome compatible existing species, new species created.
problem choosing best value avoided making dynamic; is,
given target number species, system raise many species,
lower few.
reproduction mechanism NEAT, use explicit fitness sharing (Goldberg &
Richardson, 1987), organisms species must share fitness niche.
Thus, species cannot afford become big even many organisms perform well.
Therefore, one species unlikely take entire population, crucial
speciated evolution maintain topological diversity. adjusted fitness fi0 organism
calculated according distance every organism j population:
fi
.
j=1 sh((i, j))

fi0 = Pn

73

(2)

fiStanley & Miikkulainen

sharing function sh set 0 distance
P (i, j) threshold ;
otherwise, sh((i, j)) set 1 (Spears, 1995). Thus, nj=1 sh((i, j)) reduces number
organisms species organism i. reduction natural since species
already clustered compatibility using threshold . Every species assigned
potentially different number offspring proportion sum adjusted fitnesses
fi0 member organisms. Species reproduce first eliminating lowest performing
members population. entire population replaced offspring
remaining organisms species.
net effect speciating population structural innovation protected.
final goal system, then, perform search solution efficiently
possible. goal achieved complexification simple starting structure,
detailed next section.
3.4 Minimizing Dimensionality Complexification
Unlike systems evolve network topologies weights (Angeline, Saunders, &
Pollack, 1993; Gruau, Whitley, & Pyeatt, 1996; Yao, 1999; Zhang & Muhlenbein, 1993),
NEAT begins uniform population simple networks hidden nodes, differing
initial random weights. Speciation protects new innovations, allowing topological diversity gradually introduced evolution. Thus, NEAT protects
innovation using speciation, start manner, minimally, grow new structure
generations.
New structure introduced incrementally structural mutations occur,
structures survive found useful fitness evaluations. way, NEAT
searches minimal number weight dimensions, significantly reducing number generations necessary find solution, ensuring networks become
complex necessary. gradual production increasingly complex structures
constitutes complexification. words, NEAT searches optimal topology
incrementally complexifying existing structure.
previous work, three main components NEAT (i.e. historical markings,
speciation, starting minimal structure) experimentally ablated order
demonstrate contribute performance (Stanley & Miikkulainen, 2002b,d).
ablation study demonstrated three components interdependent necessary
make NEAT work. paper, show complexification establishes
arms race competitive coevolution. next section describes experimental domain
result demonstrated.

4. Robot Duel Domain
hypothesis complexification process outlined allows discovering
sophisticated strategies, i.e. strategies effective, flexible, general,
include components variations strategies obtained search
fixed space. demonstrate effect, need domain possible develop
wide range increasingly sophisticated strategies, sophistication readily
measured. coevolution domain particularly appropriate sustained arms race
lead increasing sophistication.
74

fiCompetitive Coevolution Evolutionary Complexification

choosing domain, difficult strike balance able evolve
complex strategies able analyze understand them. Pursuit evasion
tasks utilized purpose past (Gomez & Miikkulainen, 1997; Jim
& Giles, 2000; Miller & Cliff, 1994; Reggia, Schulz, Wilkinson, & Uriagereka, 2001; Sims,
1994), serve benchmark domain complexifying coevolution well.
past experiments evolved either predator prey, interesting coevolution task
established agents instead equal engaged duel. win, agent must
develop strategy outwits opponent, utilizing structure environment.
paper introduce duel domain, two simulated robots try
overpower (Figure 5). two robots begin opposite sides rectangular
room facing away other. robots move, lose energy proportion
amount force apply wheels. Although robots never run energy
(they given enough survive entire competition), robot higher energy wins
collides competitor. addition, robot sensor indicating
difference energy robot. keep energies high,
robots consume food items, arranged symmetrical pattern room.
robot duel task supports broad range sophisticated strategies easy
observe interpret. competitors must become proficient foraging, prey capture,
escaping predators. addition, must able quickly switch one behavior
another. task well-suited competitive coevolution naive strategies
forage-then-attack complexified sophisticated strategies luring
opponent waste energy attacking.
simulated robots similar Kheperas (Mondada et al. 1993; Figure 6).
two wheels controlled separate motors. Five rangefinder sensors sense food
another five sense robot. Finally, robot energy-difference sensor,
single wall sensor.
robots controlled neural networks evolved NEAT. networks receive robot sensors inputs, well constant bias NEAT use
change activation thresholds neurons. produce three motor outputs: Two
encode rotation either right left, third indicate forward motion power.
three values translated forces applied left right wheels
robot.
state st world time defined positions robots food,
energy levels robots, internal states (i.e. neural activation) robots
neural networks, including sensors, outputs, hidden nodes. subsequent state st+1
determined outputs robots neural network controllers, computed
inputs (i.e. sensor values) st one step propagation network. robots
change position st+1 according neural network outputs follows.
change direction motion proportional difference left right
motor outputs. robot drives forward distance proportional forward output
continuous board size 600 600. robot first makes half turn, moves
forward, completes second half turn, turning forward motions
effectively combined. robot encounters food, receives energy boost,
food disappears world. loss energy due movement computed
sum turn angle forward motion, even turning place takes energy.
75

fiStanley & Miikkulainen

Figure 5: Robot Duel Domain. robots begin opposite sides board facing
away shown arrows pointing away centers.
concentric circles around robot represent separate rings opponent sensors
food sensors available robot. ring contains five sensors. robots
lose energy move around, gain energy consuming food (shown
small sandwiches). food always placed depicted horizontally symmetrical
pattern around middle board. objective attain higher level
energy opponent, collide it. complex interaction
foraging, pursuit, evasion behaviors, domain allows broad range
strategies varying sophistication. Animated demos evolved strategies
available www.cs.utexas.edu/users/nn/pages/research/neatdemo.html.

robots within distance 20, collision occurs robot higher energy
wins (see Appendix B exact parameter values used).
Since observed state ot taken sensors include internal state
opponent robot, robot duel partially-observable Markov decision process
(POMDP). Since next observed state ot+1 depends decision opponent,
necessary robots learn predict opponent likely do, based
past behavior, also based reasonable behavior general. example,
reasonable assume opponent robot quickly approaching higher
energy, probably trying collide. important complex portion
observable, memory, hence recurrent connections, crucial success.
complex robot-control domain allows competitive coevolution evolve increasingly
sophisticated complex strategies, used understand complexification,
described next.

76

fiCompetitive Coevolution Evolutionary Complexification

Figure 6: Robot Neural Networks. Five food sensors five robot sensors detect presence
objects around robot. single wall sensor indicates proximity walls,
energy difference sensor tells robot energy level differs
opponent. difference important robot lower energy loses
robots collide. three motor outputs mapped forces control left
right wheel.

5. Experiments
order demonstrate complexification enhances performance, ran thirty-three
500-generation runs coevolution robot duel domain. Thirteen runs
based full NEAT method. Complexification turned remaining 20 runs
(although networks still speciated based weight differences), order see
complexification contributes evolving sophisticated strategies. competitive coevolution
setup described first, followed overview dominance tournament method
monitoring progress.
5.1 Competitive Coevolution Setup
robot duel domain supports highly sophisticated strategies. Thus, question
domain whether continual coevolution take place, i.e. whether increasingly
sophisticated strategies appear course evolution. experiment
set carefully process emerge, able identify does.
competitive coevolution, every network play sufficient number games
establish good measure fitness. encourage interesting sophisticated strategies,
networks play diverse high quality sample possible opponents. One way
accomplish goal evolve two separate populations. generation,
population evaluated intelligently chosen sample networks
population. population currently evaluated called host population,
population opponents chosen called parasite population (Rosin &

77

fiStanley & Miikkulainen

Belew, 1997). parasites chosen quality diversity, making host/parasite
evolution efficient reliable one based random round robin tournament.
experiment, single fitness evaluation included two competitions, one east
one west starting position. way, networks needed implement general
strategies winning, independent starting positions. Host networks received
single fitness point win, points losing. competition lasted 750 time
steps winner, host received zero points.
selecting parasites fitness evaluation, good use made speciation
fitness sharing already occur NEAT. host evaluated four
highest species champions. good opponents best
best species, guaranteed diverse distance must exceed
threshold (section 3.3). Another eight opponents chosen randomly Hall
Fame composed generation champions (Rosin & Belew, 1997). Hall Fame
ensures existing abilities need maintained obtain high fitness. network
evaluated 24 games (i.e. 12 opponents, 2 games each), found effective
experimentally. Together speciation, fitness sharing, Hall Fame comprise effective
competitive coevolution methodology.
noted complexification depend particular coevolution
methodology. example Pareto coevolution (Ficici & Pollack, 2001; Noble & Watson,
2001) could used well, advantages complexification would
same. However, Pareto coevolution requires every member one population play every
member other, running time domain would prohibitively
long.
order interpret experimental results, method needed analyzing progress
competitive coevolution. next section describes method.
5.2 Monitoring Progress Competitive Coevolution
competitive coevolution run returns record every generation champion
populations. question is, sequence increasingly sophisticated strategies
identified data, one exists? section describes dominance tournament
method monitoring progress competitive coevolution (Stanley & Miikkulainen, 2002a)
allows us that.
First need method performing individual comparisons, i.e. whether one strategy
better another. board configurations vary games, champion
networks compared 144 different food configurations side board,
giving 288 total games comparison. food configurations included
9 symmetrical food positions used training, plus additional 2 food items,
placed one 12 different positions east west halves board.
starting food positions give initial advantage one robot another, depending
close robots starting positions. Thus, one wins majority
288 total games demonstrated superiority many different scenarios, including
beginning disadvantage. say network superior network b
wins games b 288 total games.

78

fiCompetitive Coevolution Evolutionary Complexification

Given definition superiority, progress tracked. obvious way
compare network others throughout evolution, finding whether later strategies
beat opponents earlier strategies. example, Floreano & Nolfi (1997) used
measure called master tournament, champion generation compared
generation champions. Unfortunately, methods impractical timeintensive domain robot duel competition. Moreover, master tournament
counts many strategies defeated generation champion, without
identifying ones. Thus, fail detect cases strategies defeat fewer
previous champions actually superior direct comparison. example, strategy
defeats 499 500 opponents, B defeats 498, master tournament designate
superior B even B defeats direct comparison. order decisively
track strategic innovation, need identify dominant strategies, i.e. defeat
previous dominant strategies. way, make sure evolution proceeds
developing progression strictly powerful strategies, instead e.g. switching
alternative ones.
dominance tournament method tracking progress competitive coevolution
meets goal (Stanley & Miikkulainen, 2002a). Let generation champion winner
288 game comparison host parasite champions single generation. Let dj jth dominant strategy appear evolution. Dominance defined
recursively starting first generation progressing last:
first dominant strategy d1 generation champion first generation;
dominant strategy dj , j > 1, generation champion < j,
dj superior di (i.e. wins 288 game comparison it).
strict definition dominance prohibits circularities. example, d4 must superior strategies d1 d3 , d3 superior d1 d2 , d2 superior d1 .
call dn nth dominant strategy run. network c exists that, example,
defeats d4 loses d3 , making superiority circular, would satisfy second
condition would entered dominance hierarchy.
entire process deriving dominance hierarchy population dominance
tournament, competitors play previous dominant strategies either lose
288 game comparison, win every comparison previous dominant strategies, thereby
becoming new dominant strategy. Dominance tournament allows us identify sequence
increasingly sophisticated strategies. also requires significantly fewer comparisons
master tournament (Stanley & Miikkulainen, 2002a).
Armed appropriate coevolution methodology measure success,
ask question: complexification result successful competitive
coevolution?

6. Results
33 evolution runs took 5 10 days 1GHz Pentium III processor,
depending progress evolution sizes networks involved. NEAT
algorithm used less 1% computation: time spent
79

fiStanley & Miikkulainen

240

16
Connections Highest Dom.
Random Fitness Min. Connections
Random Fitness Max. Connections

220

180

12

160

10

140

Nodes

Connections

200

Nodes Highest Dom.
Random Fitness Min. Nodes
Random Fitness Max. Nodes

14

120

8

100

6

80

4

60
2

40
20

0

50

100

150

200 250 300
Generation

350

400

450

0

500

0

50

100

150

200 250 300
Generation

350

400

450

500

Figure 7: Complexification connections nodes generations. hashed lines
depict average number connections average number hidden nodes
highest dominant network generation. Averages taken 13 complexifying
runs. hash mark appears every generation new dominant strategy emerged
least one 13 runs. graphs show dominance increases,
complexity. differences average final first dominant strategies
statistically significant connections nodes (p < 0.001). comparison
dashed lines depict sizes average smallest largest networks entire
population five runs fitness assigned randomly. bounds show
increase complexity inevitable; simple complex species
exist population throughout run. dominant networks complexify,
beneficial.

evaluating networks robot duel task. Evolution fully-connected topologies took
90% longer structure-growing NEAT larger networks take longer
evaluate.
order analyze results, define complexity number nodes connections network: nodes connections network,
complex behavior potentially implement. results analyzed answer three
questions: (1) evolution progresses also continually complexify? (2)
complexification lead sophisticated strategies? (3) complexification allow better strategies discovered evolving fixed-topology networks? question
answered turn below.
6.1 Evolution Complexity
NEAT run thirteen times, time different seed, verify results
consistent. highest levels dominance achieved 17, 14, 17, 16, 16, 18, 19,
15, 17, 12, 12, 11, 13, averaging 15.15 (sd = 2.54).
generation dominance level increased least one thirteen
runs, averaged number connections number nodes current dominant
strategy across runs (Figure 7). Thus, graphs represent total 197 dominance
transitions spread 500 generations. rise complexity dramatic, average
number connections tripling average number hidden nodes rising 0
80

fiCompetitive Coevolution Evolutionary Complexification

almost six. smooth trend first 200 generations, number connections
dominant strategy grows 50%. early period, dominance transitions
occur frequently (fewer prior strategies need beaten achieve dominance).
next 300 generations, dominance transitions become sparse, although continue
occur.
200th 500th generations stepped pattern emerges: complexity
first rises dramatically, settles, abruptly increases (This pattern even
marked individual complexifying runs; averaging done Figure 7 smooths
somewhat). cause pattern speciation. one species adding
large amount structure, species optimizing weights less complex networks.
Initially, added complexity leads better performance, subsequent optimization takes
longer new higher-dimensional space. Meanwhile, species smaller topologies
chance temporarily catch optimizing weights. Ultimately, however,
complex structures eventually win, since higher complexity necessary continued
innovation.
Thus, two underlying forces progress: building new structures,
continual optimization prior structures background. product two
trends gradual stepped progression towards increasing complexity.
important question is: NEAT searches adding structure only,
removing it, complexity always increase whether helps finding good solutions
not? demonstrate NEAT indeed prefers simple solutions complexifies
useful, ran five complexifying evolution runs fitness assigned randomly
(i.e. winner game chosen random). expected, NEAT kept wide
range networks population, simple highly complex (Figure 7). is,
dominant networks become complex;
beneficial. minimum complexity random-fitness population much
lower dominant strategies, maximum complexity significantly
greater. Thus, evolution complexifies sparingly, complex species holds
comparison simpler ones.
6.2 Sophistication Complexification
see complexification contributes evolution, let us observe sample dominant
strategy develops time. many complex networks evolved experiments,
follow species produced winning network d17 third run
progress rather typical easy understand. Let us use Sk best network
species generation k, hl lth hidden node arise structural mutation
course evolution. track strategic structural innovations order
see correlate. Let us begin S100 (Figure 8a), mature zerohidden-node strategy:
S100 main strategy follow opponent, putting position might
chance collide opponent energy up. However, S100 followed
opponent even opponent energy, leaving S100 vulnerable attack.
S100 clearly switch roles foraging chasing enemy, causing
miss opportunities gather food.
81

fiStanley & Miikkulainen

Figure 8: Complexification Winning Species. best networks species
depicted landmark generations. Nodes depicted squares beside node numbers, line thickness represents strength connections. time, networks
became complex gained skills. (a) champion generation 10
hidden nodes. (b) addition h22 respective connections gave new abilities.
(c) appearance h172 refined existing behaviors.

S200 . next 100 generations, evolved resting strategy, used
significantly lower energy opponent. situation, robot
stopped moving, robot wasted energy running around. time
opponent got close, energy often low enough attacked. resting
strategy example improvement take place without complexification:
involved increasing inhibition energy difference sensor, thereby slightly
modifying intensity existing behavior.
S267 (Figure 8b), new hidden node, h22 , appeared. node arrived
interspecies mating, optimized several generations already. Node
h22 gave robot ability change behavior all-out attack.
new skill, S267 longer needed follow enemy closely
times, allowing collect food. implementing new strategy
new node, possible interfere already existing resting strategy,
switched roles resting disadvantage attacking
high energy. Thus, new structure resulted strategic elaboration.
S315 (Figure 8c), another new hidden node, h172 , split link input sensor
h22 . Replacing direct connection sigmoid function greatly improved S315
ability attack appropriate times, leading accurate role switching
attacking foraging. S315 would try follow opponent afar focusing
resting foraging, zoom attack victory certain. final
structural addition shows new structure improve accuracy timing
existing behaviors.
analysis shows cases, weight optimization alone produce
improved strategies. However, strategies need extended, adding new
82

fiCompetitive Coevolution Evolutionary Complexification

Figure 9: Sophisticated Endgame.

Robot S313 dashes last piece food S210
still collecting second-to-last piece. Although appeared S313 would lose
S210 got second-to-last piece, (gray arrow), turns S210 ends
disadvantage. chance get last piece food S313 , S313
saving energy S210 wasted energy traveling long distances. way,
sophisticated strategies evolve complexification, combining multiple objectives,
utilizing weaknesses opponents strategy.

structure allows new behaviors coexist old strategies. Also, cases
necessary add complexity make timing execution behavior
accurate. results show complexification utilized produce increasing
sophistication.
order illustrate level sophistication achieved process, conclude
section describing competition two sophisticated strategies, S210
S313 , another run evolution. beginning competition, S210 S313
collected available food energy levels equal. Two pieces
food remained board locations distant S210 S313 (Figure 9).
danger colliding similar energy levels, obvious strategy rush
last two pieces food. fact, S210 exactly that, consuming second-to-last
piece, heading towards last one. contrast, S313 forfeited race
second-to-last piece, opting sit still, even though energy temporarily dropped
S210 s. However, S313 closer last piece got first. received boost
energy S210 wasted energy running long distance second-to-last
piece. Thus, S313 strategy ensured energy finally met. Robot
S313 behavior surprisingly deceptive, showing high strategic sophistication
evolved. Similar waiting behavior observed several opponents, also
evolved several runs, suggesting robust result.
analysis individual evolved behaviors shows complexification indeed elaborates existing strategies, allows highly sophisticated behaviors develop
balance multiple goals utilize weaknesses opponent. last question whether
complexification indeed necessary achieve behaviors.

83

fiStanley & Miikkulainen

6.3 Complexification vs. Fixed-topology Evolution Simplification
Complexifying coevolution compared two alternatives: standard coevolution
fixed search space, simplifying coevolution complex starting point. Note
possible compare methods using standard crossvalidation techniques
external performance measure exists domain. However, evolved neural
networks compared directly playing duel. Thus, example, run fixedtopology coevolution compared run complexifying coevolution playing
highest dominant strategy fixed-topology run entire dominance ranking
complexifying run. highest level strategy ranking fixed-topology
strategy defeat, normalized number dominance levels, measure
quality complexifying coevolution. example, strategy defeat
including 13th dominant strategy 15, performance run
13
15
= 86.7%. playing every fixed-topology champion, every simplifying coevolution
champion, every complexifying coevolution champion dominance ranking
every complexifying run averaging, measure relative performance
methods.
section, first establish baseline performance playing complexifying
coevolution runs demonstrating comparison dominance
levels meaningful measure performance. compare complexification
fixed-topology coevolution networks different architectures, including fully-connected
small networks, fully-connected large networks, networks optimal structure
determined complexifying coevolution. Third, compare performance
complexification simplifying coevolution.
6.3.1 Complexifying Coevolution
highest dominant strategy 13 complexifying runs played entire
dominance ranking every run. average performance scores 87.9%,
83.8%, 91.9%, 79.4%, 91.9%, 99.6%, 99.4%, 99.5%, 81.8%, 96.2%, 90.6%, 96.9%, 89.3%,
overall average 91.4% (sd=12.8%). all, result shows complexifying
runs produce consistently good strategies: average, highest dominant strategies
qualify top 10% complexifying runs. best runs sixth,
seventh, eighth, able defeat almost entire dominance ranking
every run. highest dominant network best run (99.6%) shown
Figure 10.
order understand means network one dominance levels
another, Figure 11 shows many games dominant network
expected win average 288-game comparisons less dominant
network. Even lowest difference (i.e. one dominance level), dominant network expected win 50 games average, showing
difference dominance level important. difference grows approximately linearly:
network 5 dominance levels higher win 100 games, network 10 levels
higher win 150 15 levels higher win 200. results suggest dominance
level comparisons indeed constitute meaningful way measure relative performance.

84

fiCompetitive Coevolution Evolutionary Complexification

Figure 10: Best Complexifying Network.
highest dominant network sixth complexifying coevolution run able
beat 99.6% dominance hierarchy 12 runs. network 11 hidden
units 202 connections (plotted figure 8), making significant use structure.
still contains basic direct connections, strategy represent
elaborated adding several new nodes connections. example, lateral
recurrent connections allow taking past events account, resulting refined
decisions. structures found reliably complexification,
turns difficult discover directly high dimensional space
fixed-topology evolution simplification.

6.3.2 Fixed-Topology Coevolution Large Networks
fixed-topology coevolution, network architecture must chosen experimenter.
One sensible approach approximate complexity best complexifying network.
(Figure 10). network included 11 hidden units 202 connections, recurrent connections direct connections input output. idealization
structure, used 10-hidden-unit fully recurrent networks direct connections
inputs outputs, total 263 connections. network type able
approximate functionality effective complexifying strategy. Fixed-topology
coevolution runs exactly complexifying coevolution NEAT, except structural
mutations occur. particular, population still speciated based weight differences (i.e. W equation 1), using usual speciation procedure.
Three runs fixed-topology coevolution performed networks,
highest dominant strategies compared entire dominance ranking every complexifying run. average performances 29.1%, 34.4%, 57.8%, overall
average 40.4%. Compared 91.4% performance complexifying coevolution,

85

fiStanley & Miikkulainen

300

Average Score

250
200
150
100
50
0

Average Score Difference (out 288)
Standard Deviation
0

2

4

6
8
10
12
Dominance Level Difference

14

16

18

Figure 11: Interpreting Differences Dominance Levels.

graph shows many
games 288-game comparison dominant network expected win,
averaged runs dominance levels complexifying coevolution. example,
network one level higher wins 50 games 288. larger difference
dominance levels translates larger difference performance approximately linearly,
suggesting dominance levels used measure performance
absolute measure available.

clear fixed-topology coevolution produced consistently inferior solutions. matter
fact, fixed-topology run could defeat highest dominant strategies
13 complexifying coevolution runs.
difference performance illustrated computing average generation
complexifying coevolution performance fixed-topology coevolution.
generation turns 24 (sd = 8.8). words, 500 generations fixed-topology
coevolution reach average level dominance 24 generations complexifying coevolution! effect, progress entire fixed-topology coevolution run
compressed first generations complexifying coevolution (Figure 12).
6.3.3 Fixed-Topology Coevolution Small Networks
One arguments using complexifying coevolution starting search directly
space final solution may intractable. argument may explain
attempt evolve fixed-topology solutions high level complexity failed. Thus,
next experiment aimed reducing search space evolving fully-connected,
fully-recurrent networks small number hidden nodes well direct connections
inputs outputs. considerable experimentation, found five hidden
nodes (144 connections) appropriate, allowing fixed-topology evolution find best

86

fiCompetitive Coevolution Evolutionary Complexification

Complexifying
Coevolution

Dom. Level

15

0

1

Dom. Level

500

10 Hidden Unit
Fixed Topoloogy
Coevolution

15

0

Equivalent
Performance

1

Generations

500

Figure 12: Comparing Typical Runs Complexifying Coevolution Fixed-Topology
Coevolution Ten Hidden Units. Dominance levels charted y-axis
generations x-axis. line appears every generation new dominant
strategy arose run. height line represents level dominance.
arrow shows highest dominant strategy found 10-hidden-unit fixed-topology
evolution performs well 8th dominant strategy complexifying run,
found 19th generation. (Average = 24, sd = 8.8) words,
generations complexifying coevolution effective several hundred
fixed-topology evolution.

solutions could. Five hidden nodes also number hidden nodes
highest dominant strategies average complexifying runs.
total seven runs performed 5-hidden-node networks, average
performances 70.7%, 85.5%, 66.1%, 87.3%, 80.8%, 88.8%, 83.1%. overall average
80.3% (sd=18.4%), better still significantly 91.4% performance
complexifying coevolution (p < 0.001).
particular, two effective complexifying runs still never defeated
fixed-topology runs. Also, dominance level difficult achieve
last, average fixed-topology evolution reached performance
159th complexifying generation (sd=72.0). Thus, even best case, fixed-topology

87

fiStanley & Miikkulainen

Complexifying
Coevolution

Dom. Level

15

0

1
Equivalent
Performance

Dom. Level

15

0

1

5 Hidden Unit
Fixed Topoloogy
Coevolution

Generations

500

500

Figure 13: Comparing Typical Runs Complexifying Coevolution Fixed-Topology
Coevolution Five Hidden Units.
figure 12, dominance levels
charted y-axis generations x-axis, line appears every generation
new dominant strategy arose run, height line represents
level dominance. arrow shows highest dominant strategy found
5-hidden-unit fixed-topology evolution performs well 12th dominant
strategy complexifying run, found 140th generation (Average
159, sd = 72.0). Thus, even best configuration, fixed-topology evolution takes
twice long achieve level performance.

coevolution average finds level sophistication complexifying coevolution
finds halfway run (Figure 13).
6.3.4 Fixed-Topology Coevolution Best Complexifying Network
One problem evolving fully-connected architectures may
appropriate topology domain. course, difficult guess appropriate
topology priori. However, still interesting ask whether fixed-topology coevolution
could succeed task assuming right topology known? answer
question, evolved networks fixed-topology experiments, except time
using topology best complexifying network (Figure 10). topology may
88

fiCompetitive Coevolution Evolutionary Complexification

constrain search space way finding sophisticated solution likely
fully-connected architecture. so, possible seeding population
successful topology gives advantage even complexifying coevolution,
must build topology minimal starting point.
Five runs performed, obtaining average performance score 86.2%, 83.3%, 88.1%,
74.2%, 80.3%, overall average 82.4% (sd=15.1%). 91.4% performance
complexifying coevolution significantly better even version fixed-topology
coevolution (p < 0.001). However, interestingly, 40.4% average performance 10hidden-unit fixed topology coevolution significantly best-topology evolution, even
though methods search spaces similar sizes. fact, best-topology evolution
performs level 5-hidden-unit fixed-topology evolution (80.3%), even
though 5-hidden-unit evolution optimizes half number hidden nodes. Thus, results
confirm hypothesis using successful evolved topology help constrain
search. However, comparison complexifying coevolution, advantage gained
starting way still enough make penalty starting search directly
high-dimensional space. Figure 14 shows, best-topology evolution average finds
strategy performs well found 193rd generation complexifying
coevolution.
results fixed-topology coevolution experiments summarized follows:
method used search directly high-dimensional space effective
solutions, reaches 40% performance complexifying coevolution.
better allowed optimize less complex networks; however, sophisticated
solutions may exist space. Even given topology appropriate task,
reach level complexifying coevolution. Thus, fixed-topology coevolution appear competitive complexifying coevolution choice
topology.
conclusion complexification superior allows discovering
appropriate high-dimensional topology automatically, also makes
optimization topology efficient. point discussed Section
7.
6.3.5 Simplifying Coevolution
possible remedy search high-dimensional spaces allow evolution
search smaller structures removing structure incrementally. simplifying coevolution opposite complexifying coevolution. idea mediocre complex
solution refined removing unnecessary dimensions search space, thereby
accelerating search.
Although simplifying coevolution alternative method complexifying coevolution
finding topologies, still requires complex starting topology specified.
topology chosen two goals mind: (1) Simplifying coevolution start
sufficient complexity least potentially find solutions equal complexity
best solutions complexifying coevolution, (2) rate structural
removal equivalent rate structural addition complexifying NEAT,
possible discover solutions significantly simpler best complexifying solutions.

89

fiStanley & Miikkulainen

Complexifying
Coevolution

Dom. Level

15

0

1
Best Solution
Fixed Topology
Coevolution

Dom. Level

15

0

1

Equivalent
Performance

Generations

500

500

Figure 14: Comparing Typical Runs Complexifying Coevolution Fixed-Topology
Coevolution Best Complexifying Network. Dominance levels charted
figure 12. arrow shows highest dominant strategy found evolving
fixed topology best complexifying network performs well dominant strategy would found 193rd generation complexifying coevolution
(Average 193, sd = 85). Thus, even appropriate topology given, fixed-topology
evolution takes almost twice long achieve level performance.

Thus, chose start search 12-hidden-unit, 339 connection fully-connected fullyrecurrent network. Since 162 connections added best complexifying network
evolution, corresponding simplifying coevolution could discover solutions 177
connections, 25 less best complexifying network.
Thus, simplify coevolution run complexifying coevolution, except
initial topology contained 339 connections instead 39, structural mutations removed
connections instead adding nodes connections. connections node
removed, node removed. Historical markings speciation worked
complexifying NEAT, except markings assigned beginning evolution.
(because structure removed never added). diversity species varying
complexity developed before.

90

fi149

Dom. Level

15

0

1
Simplifying
Coevolution

Dom. Level

15

0

1

Equivalent
Performance

500

39

339

Generations

500

227

Connections Dom.

Complexifying
Coevolution

Connections Dom.

Competitive Coevolution Evolutionary Complexification

Figure 15: Comparing Typical Runs Complexifying Coevolution Simplifying Coevolution. Dominance levels charted figure 12. addition, line plot
shows complexity dominance level terms number connections
networks scale indicated y-axis right. typical simplifying run,
number connections reduced 339 227 connections. arrow shows
highest dominant strategy found simplifying coevolution performs well
9th 10th dominant strategy complexifying coevolution, normally found
56 generations (sd = 31). words, even though simplifying coevolution
finds dominance levels, search appropriate structure less effective
complexifying coevolution.

five runs simplifying coevolution performed 64.8%, 60.9%, 56.6%, 36.4%,
67.9%, overall average 57.3% (sd=19.8%). Again, performance significantly 91.4% performance complexifying coevolution (p < 0.001). Interestingly,
even though started 76 connections fixed-topology coevolution ten
hidden units, simplifying coevolution still performed significantly better (p < 0.001), suggesting evolving structure reducing complexity better evolving large
fixed structures.
Like Figures 1214, Figure 15 compares typical runs complexifying simplifying
coevolution. average, 500 generations simplification finds solutions equivalent 56
generations complexification. Simplifying coevolution also tends find dominance
levels method tested. generated average 23.2 dominance levels per
run, even finding 30 one run, whereas e.g. complexifying coevolution average
finds 15.2 levels. words, difference dominance levels much smaller
91

fiStanley & Miikkulainen

Coevolution Type Ave. Highest Ave. Highest
Average
Dom. Level
Generation Performance

Equivalent
Generation
(out 500)
91.4%
343
40.4%
24

Complexifying
15.2
353.6
Fixed-Topology
12.0
172
10 Hidden Node
Fixed-Topology
13.0
291.4
80.3%
159
5 Hidden Node
Fixed-Topology
14.0
301.8
82.4%
193
Best Network
Simplifying
23.2
444.2
57.3%
56
Table 1: Summary performance comparison. second column shows
many levels dominance achieved type coevolution average. third
specifies average generation highest dominant strategy, indicating long innovation generally continues. fourth column gives average level complexifying
coevolution dominance hierarchy champion could defeat, fifth column shows
average generation. differences performance (p < 0.001) equivalent generation (p < 0.001) complexifying coevolution every method significant.
main result level sophistication reached complexifying coevolution
significantly higher reached fixed-topology simplifying coevolution.
simplifying coevolution complexifying coevolution. Unlike methods, dominant
strategies tend appear spurts time, usually complexity
decreasing several generations, also shown Figure 15. number
generations, evolution removes several connections smaller, easily optimized
space discovered. Then, quick succession minute improvements creates several new
levels dominance, space refined, on. process
makes sense, inferior results simplifying coevolution suggest simplifying search
ineffective way discovering useful structures compared complexification.
6.3.6 Comparison Summary
Table 1 shows coevolution methods differ number dominance levels, generation highest dominance level, overall performance, equivalent generation.
conclusion complexifying coevolution innovates longer finds higher level
sophistication methods.

7. Discussion Future Work
makes complexification powerful search method? Whereas fixed-topology
coevolution, well simplifying coevolution, good structures must optimized
high-dimensional space solutions themselves, complexifying coevolution
searches high-dimensional structures elaborations known good lower-dimensional
structures. adding new dimension, values existing genes already
optimized preceding generations. Thus, new gene added, genome

92

fiCompetitive Coevolution Evolutionary Complexification

already promising part new, higher-dimensional space. Thus, search
higher-dimensional space starting blindly would evolution began searching
space. reason complexification find high-dimensional solutions
fixed-topology coevolution simplifying coevolution cannot.
Complexification particularly well suited coevolution problems. fixed
genome used represent strategy, strategy optimized, possible
add functionality without sacrificing knowledge already present.
contrast, new genetic material added, sophisticated elaborations layered
existing structure, establishing evolutionary arms race. process evident
robot duel domain, successive dominant strategies often built new functionality
top existing behavior adding new nodes connections.
advantages complexification imply fixed-sized genomes cannot sometimes evolve increasingly complex phenotypic behavior. Depending mapping
genotype phenotype, may possible fixed, finite set genes represent
solutions (phenotypes) varying behavioral complexity. example, behaviors
observed Cellular Automata (CA), computational structure consisting
lattice cells change state function current state state
cells neighborhood. neighborhood function represented
genome size 2n+1 (assuming n neighboring cells binary state) evolved obtain
desired target behavior. example, Mitchell et al. (1996) able evolve neighborhood functions determine whether black white cells majority CA
lattice. evolved CAs displayed complex global behavior patterns converged
single classification, depending cell type majority. course
evolution, behavioral complexity CA rose even genome remained
size.
CA example, correct neighborhood size chosen priori. choice
difficult make, crucial success. desired behavior existed within
chosen size, even behavior would become gradually complex, system would
never solve task. Interestingly, dead-end could avoided neighborhood
(i.e. genome) could expanded evolution. possible CAs could
effectively evolved complexifying (i.e. expanding) genomes, speciating
protect innovation, NEAT.
Moreover, chosen neighborhood small represent solution,
also unnecessarily large. Searching space dimensions
necessary impede progress, discussed above. desired function existed
smaller neighborhood could found significantly fewer evaluations. Indeed,
even possible efficient neighborhood symmetric, contains cells
directly adjacent cell processed. Moreover, even efficient
neighborhood may large space begin searching. Starting search
small space incrementing promising part higher-dimensional space
likely find solution. reasons, complexification advantage, even
behavioral complexity increase extent within fixed space.
CA example raises intriguing possibility structured phenotype
evolved complexification minimal starting point, historical markings,
protection innovation speciation. addition neural networks
93

fiStanley & Miikkulainen

CA, electrical circuits (Miller et al., 2000a; Miller, Job, & Vassilev, 2000b), genetic programs (Koza, 1992), robot body morphologies (Lipson & Pollack, 2000), Bayesian networks
(Mengshoel, 1999), finite automata (Brave, 1996), building vehicle architectures
(OReilly, 2000) structures varying complexity benefit complexification. starting search minimal space adding new dimensions incrementally,
highly complex phenotypes discovered would difficult find search began
intractable space final solution, prematurely restricted small
space.
search optimal structures common problem Artificial Intelligence (AI).
example, Bayesian methods applied learning model structure (Attias, 2000;
Ueda & Ghahramani, 2002). approaches, posterior probabilities different
structures computed, allowing overly complex simplistic models eliminated.
Note approaches aimed generating increasingly complex functional
structures, rather providing model explains existing data. cases,
solutions involve growing gradually larger structures, goal growth form
gradually better approximations. example, methods like Incremental Grid Growing
(Blackmore & Miikkulainen, 1995), Growing Neural Gas (Fritzke, 1995) add neurons
network approximates topology input space reasonably well.
hand, complexifying systems non-deterministic (like NEAT),
need based evolutionary algorithms. example, Harvey (1993) introduced
deterministic algorithm chromosome lengths entire population increase
time order expand search space; Fahlman & Lebiere (1990) developed
supervised (non-evolutionary) neural network training method called cascade correlation,
new hidden neurons added network predetermined manner order
complexify function computes. conclusion complexification important
general principle AI.
future, complexification may help general problem finding appropriate level abstraction difficult problems. Complexification start
simple, high-level description solution, composed general-purpose elements.
abstraction insufficient, elaborated breaking high-level
element lower level specific components. process continue indefinitely, leading increasingly complex substructures, increasingly low-level solutions
subproblems. Although NEAT solutions composed connections
nodes, provide early example process could implemented.
One primary elusive goals AI create systems scale up.
sense, complexification process scaling up. general principle taking
simple idea elaborating broader application. Much AI concerned search,
whether complex multi-dimensional landscapes, highly-branching trees
possibilities. However, intelligence much deciding space search
searching proper space already identified. Currently, humans
able decide proper level abstraction solving many problems, whether
simple high-level combination general-purpose parts, extremely complex assembly
low-level components. program decide level abstraction
appropriate given domain would highly compelling demonstration Artificial

94

fiCompetitive Coevolution Evolutionary Complexification

Intelligence. is, believe, complexification methods largest
impact future.

8. Conclusion
experiments presented paper show complexification genomes leads
continual coevolution increasingly sophisticated strategies. Three trends found
experiments: (1) evolution progresses, complexity solutions increases, (2) evolution uses complexification elaborate existing strategies, (3) complexifying coevolution significantly successful finding highly sophisticated strategies
non-complexifying coevolution. results suggest complexification crucial
component successful search complex solutions.

Acknowledgments
research supported part National Science Foundation grant IIS0083776 Texas Higher Education Coordinating Board grant ARP-003658476-2001. Special thanks anonymous reviewer constructive suggestions noncomplexifying comparisons.

Appendix A. NEAT System Parameters
population 256 NEAT networks, total 512. coefficients measuring
compatibility c1 = 1.0, c2 = 1.0, c3 = 2.0. initial compatibility distance
= 3.0. However, population dynamics unpredictable hundreds
generations, assigned target 10 species. number species grew 10,
increased 0.3 reduce number species. Conversely, number species
fell 10, decreased 0.3 increase number species. normalization
factor N used compute compatibility fixed one. order prevent stagnation,
lowest performing species 30 generations old allowed reproduce.
champion species five networks copied next generation
unchanged. 80% chance genome connection weights mutated,
case weight 90% chance uniformly perturbed 10% chance
assigned new random value. (The system tolerant frequent mutations
protection speciation provides.) 75% chance inherited gene
disabled disabled either parent. 40% crossovers, offspring inherited
average connection weights matching genes parents, instead
connection weight one parent randomly. generation, 25% offspring
resulted mutation without crossover. interspecies mating rate 0.05.
probability adding new node 0.01 probability new link mutation
1
0.1. used modified sigmoidal transfer function, (x) = 1+e4.9x
, nodes.
parameter values found experimentally, follow logical pattern: Links
need added significantly often nodes, average weight difference
0.5 significant one disjoint excess gene. Performance robust

95

fiStanley & Miikkulainen

moderate variations values. NEAT software available software section
http://nn.cs.utexas.edu.

Appendix B. Robot Duel Domain Coefficients Motion
turn angle determined = 0.24|lr|, l output left turn neuron,
r output right turn neuron. robot moves forward distance 1.33f
600 600 board, f forward motion output. coefficients
calibrated experimentation achieve accurate smooth motion neural
outputs zero one.

References
Amores, A., Force, A., Yan, Y.-L., Joly, L., Amemiya, C., Fritz, A., Ho, R. K., Langeland,
J., Prince, V., Wang, Y.-L., Westerfield, M., Ekker, M., & Postlethwait, J. H. (1998).
Zebrafish HOX clusters vertebrate genome evolution. Science, 282, 17111784.
Angeline, P. J., & Pollack, J. B. (1993). Competitive environments evolve better solutions
complex tasks. Forrest, S. (Ed.), Proceedings Fifth International Conference
Genetic Algorithms (pp. 264270). San Francisco, CA: Morgan Kaufmann.
Angeline, P. J., Saunders, G. M., & Pollack, J. B. (1993). evolutionary algorithm
constructs recurrent neural networks. IEEE Transactions Neural Networks, 5,
5465.
Attias, H. (2000). variational bayesian framework graphical models. Advances
Neural Information Processing Systems, 12 (pp. 209215). Cambridge, MA: MIT
Press.
Blackmore, J., & Miikkulainen, R. (1995). Visualizing high-dimensional structure
incremental grid growing neural network. Prieditis, A., & Russell, S. (Eds.), Machine
Learning: Proceedings 12th Annual Conference (pp. 5563). San Francisco, CA:
Morgan Kaufmann.
Brave, S. (1996). Evolving deterministic finite automata using cellular encoding. Koza,
J. R., Goldberg, D. E., Fogel, D. B., & Riolo, R. L. (Eds.), Genetic Programming
1996: Proceedings First Annual Conference (pp. 3944). Stanford University,
CA, USA: MIT Press.
Carroll, S. B. (1995). Homeotic genes evolution arthropods chordates.
Nature, 376, 479485.
Cliff, D., Harvey, I., & Husbands, P. (1993). Explorations evolutionary robotics. Adaptive
Behavior, 2, 73110.
Cybenko, G. (1989). Approximation superpositions sigmoidal function. Mathematics
Control, Signals, Systems, 2 (4), 303314.

96

fiCompetitive Coevolution Evolutionary Complexification

Darwen, P. J. (1996). Co-Evolutionary Learning Automatic Modularisation Speciation. Doctoral Dissertation, School Computer Science, University College, University
New South Wales.
Dawkins, R., & Krebs, J. R. (1979). Arms races within species. Proceedings
Royal Society London Series B, 205, 489511.
Fahlman, S. E., & Lebiere, C. (1990). cascade-correlation learning architecture.
Touretzky, D. S. (Ed.), Advances Neural Information Processing Systems 2 (pp.
524532). San Francisco, CA: Morgan Kaufmann.
Ficici, S. G., & Pollack, J. B. (2001). Pareto optimality coevolutionary learning.
Kelemen, J. (Ed.), Sixth European Conference Artificial Life. Berlin; New York:
Springer-Verlag.
Floreano, D., & Nolfi, S. (1997). God save red queen! Competition co-evolutionary
robotics. Evolutionary Computation, 5.
Force, A., Lynch, M., Pickett, F. B., Amores, A., lin Yan, Y., & Postlethwait, J. (1999).
Preservation duplicate genes complementary, degenerative mutations. Genetics,
151, 15311545.
Fritzke, B. (1995). growing neural gas network learns topologies. G.Tesauro,
D.S.Touretzky, & T.K.Leen (Eds.), Advances Neural Information Processing Systems 7 (pp. 625632). Cambridge, MA: MIT Press.
Goldberg, D. E., & Richardson, J. (1987). Genetic algorithms sharing multimodal
function optimization. Grefenstette, J. J. (Ed.), Proceedings Second International Conference Genetic Algorithms (pp. 148154). San Francisco, CA: Morgan
Kaufmann.
Gomez, F., & Miikkulainen, R. (1997). Incremental evolution complex general behavior.
Adaptive Behavior, 5, 317342.
Gruau, F., Whitley, D., & Pyeatt, L. (1996). comparison cellular encoding
direct encoding genetic neural networks. Koza, J. R., Goldberg, D. E., Fogel,
D. B., & Riolo, R. L. (Eds.), Genetic Programming 1996: Proceedings First
Annual Conference (pp. 8189). Cambridge, MA: MIT Press.
Harvey, I. (1993). Artificial Evolution Adaptive Behavior . Doctoral Dissertation,
School Cognitive Computing Sciences, University Sussex, Sussex.
Holland, P. W., Garcia-Fernandez, J., Williams, N. A., & Sidow, A. (1994). Gene duplications origin vertebrate development. Development Supplement, pp. 125133.
Jim, K.-C., & Giles, C. L. (2000). Talking helps: Evolving communicating agents
predator-prey pursuit problem. Artificial Life, 6 (3), 237254.

97

fiStanley & Miikkulainen

Koza, J. (1995). Gene duplication enable genetic programming concurrently evolve
architecture work-performing steps computer program. Proceedings 14th International Joint Conference Artificial Intelligence. Morgan
Kaufmann.
Koza, J. R. (1992). Genetic Programming: Programming Computers Means
Natural Selection. Cambridge, MA: MIT Press.
Lindgren, K., & Johansson, J. (2001). Coevolution strategies n-person prisoners
dilemma. Crutchfield, J., & Schuster, P. (Eds.), Evolutionary Dynamics - Exploring
Interplay Selection, Neutrality, Accident, Function. Reading, MA: AddisonWesley.
Lipson, H., & Pollack, J. B. (2000). Automatic design manufacture robotic lifeforms.
Nature, 406, 974978.
Mahfoud, S. W. (1995). Niching Methods Genetic Algorithms. Doctoral Dissertation,
University Illinois Urbana-Champaign, Urbana, IL.
Maley, C. C. (1999). Four steps toward open-ended evolution. Proceedings Genetic Evolutionary Computation Conference (GECCO-1999) (pp. 13361343). San
Francisco, CA: Morgan Kaufmann.
Martin, A. P. (1999). Increasing genomic complexity gene duplication origin
vertebrates. American Naturalist, 154 (2), 111128.
Mengshoel, O. J. (1999). Efficient Bayesian Network Inference: Genetic Algorithms,
Stochastic Local Search, Abstraction. Doctoral Dissertation, University Illinois
Urbana-Champaign Computer Science Department, Urbana-Champaign, IL.
Miller, G., & Cliff, D. (1994). Co-evolution pursuit evasion i: Biological gametheoretic foundations. Tech. Rep. CSRP311, School Cognitive Computing Sciences, University Sussex, Brighton, UK.
Miller, J. F., Job, D., & Vassilev, V. K. (2000a). Principles evolutionary design
digital circuits Part I. Journal Genetic Programming Evolvable Machines,
1 (1), 835.
Miller, J. F., Job, D., & Vassilev, V. K. (2000b). Principles evolutionary design
digital circuits Part II. Journal Genetic Programming Evolvable Machines,
3 (2), 259288.
Mitchell, M., Crutchfield, J. P., & Das, R. (1996). Evolving cellular automata genetic
algorithms: review recent work. Proceedings First International Conference Evolutionary Computation Applications (EvCA96). Russian Academy
Sciences.
Mondada, F., Franzi, E., & Ienne, P. (1993). Mobile robot miniaturization: tool investigation control algorithms. Proceedings Third International Symposium
Experimental Robotics (pp. 501513).
98

fiCompetitive Coevolution Evolutionary Complexification

Nadeau, J. H., & Sankoff, D. (1997). Comparable rates gene loss functional divergence
genome duplications early vertebrate evolution. Genetics, 147, 12591266.
Noble, J., & Watson, R. A. (2001). Pareto coevolution: Using performance coevolved opponents game dimensions parerto selection. et al, L. S. (Ed.),
Proceedings Genetic Evolutionary Computation Conference (GECCO-2001).
San Francisco, CA: Morgan Kaufmann.
OReilly, U.-M. (2000). Emergent design: Artificial life architecture design. 7th
International Conference Artificial Life (ALIFE-00). Cambridge, MA: MIT Press.
Postlethwait, H. H., Yan, Y. L., Gates, M. A., Horne, S., Amores, A., Brownlie, A., &
Donovan, A. (1998). Vertebrate genome evolution zebrafish gene map. Nature
Genetics, 18, 345349.
Radcliffe, N. J. (1993). Genetic set recombination application neural network
topology optimization. Neural computing applications, 1 (1), 6790.
Radding, C. M. (1982). Homologous pairing strand exchange genetic recombination.
Annual Review Genetics, 16, 405437.
Reggia, J. A., Schulz, R., Wilkinson, G. S., & Uriagereka, J. (2001). Conditions enabling
evolution inter-agent signaling artificial world. Artificial Life, 7, 332.
Rosin, C. D. (1997). Coevolutionary Search Among Adversaries. Doctoral Dissertation,
University California, San Diego, San Diego, CA.
Rosin, C. D., & Belew, R. K. (1997). New methods competitive evolution. Evolutionary
Computation, 5.
Ryan, C. (1994). Pygmies civil servants. Kinnear, Jr., K. E. (Ed.), Advances
Genetic Programming (Chap. 11, pp. 243263). MIT Press.
Sidow, A. (1996). Gen(om)e duplications evolution early vertebrates. Current
Opinion Genetics Development, 6, 715722.
Sigal, N., & Alberts, B. (1972). Genetic recombination: nature crossed strandexchange two homologous DNA molecules. Journal Molecular Biology,
71 (3), 789793.
Sims, K. (1994). Evolving 3D morphology behavior competition. Brooks, R. A.,
& Maes, P. (Eds.), Proceedings Fourth International Workshop Synthesis
Simulation Living Systems (Artificial Life IV) (pp. 2839). Cambridge, MA:
MIT Press.
Spears, W. (1995). Speciation using tag bits. Handbook Evolutionary Computation.
IOP Publishing Ltd. Oxford University Press.

99

fiStanley & Miikkulainen

Stanley, K. O., & Miikkulainen, R. (2002a). dominance tournament method monitoring progress coevolution. Proceedings Genetic Evolutionary Computation Conference (GECCO-2002) Workshop Program. San Francisco, CA: Morgan
Kaufmann.
Stanley, K. O., & Miikkulainen, R. (2002b). Efficient evolution neural network topologies.
Proceedings 2002 Congress Evolutionary Computation (CEC02). IEEE.
Stanley, K. O., & Miikkulainen, R. (2002c). Efficient reinforcement learning evolving neural network topologies. Proceedings Genetic Evolutionary Computation Conference (GECCO-2002). San Francisco, CA: Morgan Kaufmann.
Stanley, K. O., & Miikkulainen, R. (2002d). Evolving neural networks augmenting
topologies. Evolutionary Computation, 10 (2), 99127.
Ueda, N., & Ghahramani, Z. (2002). Bayesian model search mixture models based
optimizing variational bounds. Neural Networks, 15, 12231241.
Van Valin, L. (1973). new evolutionary law. Evolution Theory, 1, 130.
Yao, X. (1999). Evolving artificial neural networks. Proceedings IEEE, 87 (9), 1423
1447.
Zhang, B.-T., & Muhlenbein, H. (1993). Evolving optimal neural networks using genetic
algorithms Occams razor. Complex Systems, 7, 199220.

100

fiJournal Artificial Intelligence Research 21 (2004) 1936

Submitted 7/03; published 1/04

Price Prediction Trading Agent Competition
Michael P. Wellman
Daniel M. Reeves
Kevin M. Lochner
Yevgeniy Vorobeychik

WELLMAN @ UMICH . EDU
DREEVES @ UMICH . EDU
KLOCHNER @ UMICH . EDU
YVOROBEY @ UMICH . EDU

University Michigan, Artificial Intelligence Laboratory
Ann Arbor, MI 48109-2110 USA

Abstract
2002 Trading Agent Competition (TAC) presented challenging market game domain travel shopping. One pivotal issues domain uncertainty hotel prices,
significant influence relative cost alternative trip schedules. Thus, virtually
participants employ method predicting hotel prices. survey approaches employed
tournament, finding agents apply interesting diversity techniques, taking account differing sources evidence bearing prices. Based data provided entrants
agents actual predictions TAC-02 finals semifinals, analyze relative efficacy
approaches. results show taking account game-specific information flight
prices major distinguishing factor. Machine learning methods effectively induce relationship flight hotel prices game data, purely analytical approach based
competitive equilibrium analysis achieves equal accuracy historical data. Employing new
measure prediction quality, relate absolute accuracy bottom-line performance game.

1. Introduction
Many market decision problems involve anticipation forecast future prices. Price prediction particularly important, example, committing binding offer purchase good
complements purchased later date. sort scenario arises whenever
sequential overlapping auctions related goods. Although market forecasting techniques
widespread use broad range applications, unaware studies exploring problem
context reminiscent multi-auction environments.
annual Trading Agent Competition (TAC) (Wellman et al., 2003) provides convenient
medium studying approaches price prediction. open-invitation tournament, attracted diverse community researchers interested many aspects trading agent strategy.
Price prediction turned pivotal issue TAC market game, 1 interesting array approaches emerged agent designers efforts three years competition. Since
TAC defines controlled, regular, repeatable, transparent environment observing trading
agent behavior, also uncommonly amenable analysis.

1. refer original (classic) TAC game, scenario domain travel shopping. Sadeh et al. (2003)
introduced second game (TAC/SCM), domain supply chain management, expect
domains subject trading competitions coming years.

c 2004 AI Access Foundation. rights reserved.

fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIK

2. TAC Travel-Shopping Game
TAC market game presents travel-shopping task, traders assemble flights, hotels,
entertainment trips set eight probabilistically generated clients. Clients described
preferred arrival departure days (pa pd), premium (hp) willing pay
stay Towers (T) hotel rather Shanties (S), values three different types
entertainment events. agents objective maximize value trips clients, net
expenditures markets travel goods.
Flights. feasible trip includes round-trip air, consists inflight day outflight

. Flights day sold independently, prices determined
day ,
stochastic process. initial price flight , follows random walk
thereafter increasingly upward bias.
Hotels. Feasible trips must also include room one two hotels night
clients stay. 16 rooms available hotel night, sold
ascending 16th-price auctions. auction closes, units allocated 16 highest
offers, bidders paying price lowest winning offer. minute, hotel auctions
issue quotes, indicating 16th- (ASK) 17th-highest (BID) prices among currently active
unit offers. minute, starting 4:00, one hotel auctions selected random close,
others remaining active open bids.
Entertainment. Agents receive initial random allocation entertainment tickets (indexed
type day), may allocate clients sell agents continuous
double auctions.
feasible client trip defined inflight day , outflight day , hotel type ( ,
1 0 S). Trips also specify entertainment allocations, purposes paper
summarize expected entertainment surplus function trip days. (The paper describing
agent (Cheng et al., 2004) provides details well constructs glossed
here.) value trip client (with preferences pa, pd, hp) given


pa
pd
hp



(1)

end game instance, TAC server calculates optimal allocation trips clients
agent, given final holdings flights, hotels, entertainment. agents game score
total client trip utility, minus net expenditures TAC auctions.

3. Price Prediction
TAC participants recognized early importance accurate price prediction overall performance (Stone & Greenwald, 2004). prices hotels highly variable game game,
yet hotels price finalized auction closessome minutes game, depending
random closing order. agents tend submit serious hotel bids first
closing imminent, useful information revealed price quotes fourth minute
play. Complementarity among goods dictates outcomes early auctions significantly affect
value agent places particular hotel later game, conversely, prices hotels
available later dictate whether agent bid wisely early game.
Anticipating hotel prices key element several decisions facing TAC agent, particular:
20

fiTAC P RICE P REDICTION

1. Selecting trip itineraries. flight prices tend increase, agents great incentive
commit traveling particular days early game. Yet quality day choices
depends crucially hotel prices included travel days.
2. Bidding policy. likelihood obtaining good given bid depends obviously
resulting clearing price. Moreover, value particular good general function
price others. example, value obtaining room hotel (S, ) increasing
function projected cost alternative hotel day, (T, ), decreasing
function projected cost complementary hotel rooms adjacent days, (S, )
(S,
).
Given importance price prediction, surprising TAC researchers explored
variety approaches. Stone et al. (2003) noted diversity price estimation methods among
TAC-01 agents. particular interest problem certainly connected TAC-02
agent, Walverine, introduced method quite distinct reported previously. Thus,
highly motivated characterize performance price prediction task.
Indeed, competitions TAC successful facilitating research, necessary separately evaluate techniques developed problem subtasks (Stone, 2002). Although
interesting subtasks tend strictly separable complex game, price-prediction
component trading strategy may easier isolate most. particular, problem
formulated terms, natural absolute accuracy measures. fact, see below,
agent developers independently chose define price prediction distinct task
agent designs.
divide price prediction two phases: initial interim. Initial refers beginning
game, hotel auctions close provide quote information. Interim refers
method employed thereafter. Since information available initial prediction (flight prices,
client preferencessee Section 5.2) strict subset available interim (which adds transaction hotel price data), agents treat initial prediction (simpler) special case.
Initial prediction relevant bidding policy first hotel closing, especially salient
trip choices typically made early game. Interim prediction supports ongoing revision bids hotel auctions start close. analysis report focus initial prediction,
mainly simpler two tasks, involving less potential information. Moreover,
agents initially relatively comparable information sets, thus providing cleaner analysis.
Interim prediction also quite important interesting, focus work.

4. TAC-02 Agents
nineteen agents participated TAC-02 tournament listed Table 1. table
presents raw average scores finals semifinals, weighted averages seeding
round. overviews agents, see survey edited Greenwald (2003a).
seeding rounds held period 1-12 July, agent playing 440 games.
Weights increased day, later games counted earlier, lowest 10 scores
agent dropped. top 16 agents advanced semifinals, held 28 July
Edmonton, Canada. two semifinal heats: H1 comprising agents seeded 1-4 13-16,
5-12 seeds placed heat H2. top four teams heat (14 games, lowest score
21

fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIK

Agent
ATTac
BigRed
cuhk
harami
kavayaH
livingagents
PackaTAC
PainInNEC
RoxyBot
sics
SouthamptonTAC
Thalis
tniTac
TOMAhack
tvad
UMBCTAC
Walverine
WhiteBear
zepp

Affiliation
AT&T Research (et al.)
McGill U
Chinese U Hong Kong
Bogazici U
Oracle India
Living Systems AG
N Carolina State U
NEC Research (et al.)
Brown U
Swedish Inst Comp Sci
U Southampton
U Essex
Poli Bucharest
U Toronto
Technion
U Maryland Baltimore Cty
U Michigan
Cornell U
Poli Bucharest

Seeding
3131
696
3055
2064
2549
3091
2835
2319
2855
2847
3129
3000
2232
2809
2618
3118
2772
2966
2098

Semifinals
H1: 3137

H2: 3266

H1: 3200
H1: 3310
H2: 3250
H1: 2193
H2: 3160
H2: 3146
H1: 3397
H2: 3199
H1: 3108
H2: 2843
H1: 2724
H1: 3208
H2: 3287
H2: 3324


Finals


3069

3099
3181




3385
3246



3236
3210
3413


Table 1: TAC-02 tournament participants, scores round.

dropped) proceeded finals, ran 32 games day. details
TAC-02 tournament available http://www.sics.se/tac.

5. Price Prediction Survey
Shortly TAC-02 event, distributed survey entrants eliciting descriptions
data documenting agents price-prediction methods. Sixteen 19 teams responded
survey, including 14 16 semifinalists, eight finalists. result provides detailed picture
prediction techniques employed, enables comparison efficacy respect
common experiencethe TAC-02 finals semifinals.
Thirteen 16 respondents reported agents indeed form explicit price predictions use trading strategies. thirteen listed Table 2, along
high-level descriptors approach initial prediction task. 2 addition, tniTac zepp
responded price predictions part agent designs, developed sufficiently
deployed tournament. TOMAhack reported ambitious design (also actually employed) based model-free policy learning, account agents bidding behavior
without formulating explicit price predictions.
2. Though address initial prediction report, survey also solicited descriptions interim methods.

22

fiTAC P RICE P REDICTION

Agent
ATTac
cuhk
harami
kavayaH
livingagents
PackaTAC
RoxyBot
sics
SouthamptonTAC
Thalis
UMBCTAC
Walverine
WhiteBear

Approach
machine learning
historical
historical
machine learning
historical
historical
historical
historical
historical
historical (?)
historical
competitive
historical

Form
prob
priceline
prob
point
point
prob
prob
priceline
point
point
point
point
point

Notes
boosting
moving average
neural net

classification reference categories
survey incomplete
equilibrium analysis

Table 2: Agents reporting prediction hotel prices TAC-02.

5.1 Forms Prediction
One distinction observed TAC-01 agents explicitly formulated predictions terms
probability distributions prices, rather point estimates. Predictions form enable
agent properly account price uncertainty decision making. Thus, asked entrants
form predictions survey.
Although agents generate point predictions, notable exceptions. ATTacs boosting algorithm (Stone et al., 2003) expressly learns probability distributions associated game
features. RoxyBot tabulates game price statistics direct estimation deciles hotel
auction. PackaTAC harami measure historical variance, combining historical averaging define parametric distribution hotel price. Walverine predicts point prices,
hedging approach decisions amounts forming effective distribution around them.
Given prediction form distribution, agents may make decisions sampling
decision-analytic techniques. distribution may also facilitate interim prediction task, enabling updates based treating observations price quotes evidence.
However, first controlled experiment evaluating distribution feature, context ATTac
(Stone et al., 2003), find overall advantage decision-making based distributions
compared using mean values. authors offered several possible explanations observed
performance, including (1) implementation employs insufficient samples, (2)
use distributions makes unrealistic assumption subsequent decisions made
full knowledge actual price values. Greenwald (2003b) also found bidding marginal utility based means outperformed bidding expected marginal utility based distributions, time
implemented context RoxyBot. since performed analogous trials using Walverinewhich generates applies distributions yet third wayand also found bidding based
means superior distribution-based bidding agent actually employed TAC-02.
Although source deficiency conclusively established, speculate
second reason adduced ATTac designers plausible.
23

fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIK

Despite evidence, alternative ways using distributions may well prove beneficial.
study Greenwald (2003b) demonstrated advantage RoxyBots 2002 strategy evaluating
candidate bid sets respect distributions, compared 2000 strategy evaluating
respect means.
Nevertheless, agents predict probability distributions, take mean distributions subject analysis. may discount potential advantages, based
discussion above, suspect thatwith possible exception RoxyBotagents
actually benefit predicting distributions TAC-02.
Another variation form prediction prices function quantity demanded.
first TAC, entrants recognized purchasing additional units may cause price increase,
introduced concept pricelines, express estimated prices unit (Boyan & Greenwald, 2001; Stone & Greenwald, 2004). Agents sics cuhk reported predicting pricelines. 3
cases, agent started baseline point prediction first unit hotel,
derived remainder priceline according rule. example, sics predicted price
ffth unit (i.e., price given demands ff units) fi , fi baseline prediction
1.15 hotels day 1 4, 1.25 hotels day 2 3.
succeeding analysis, evaluate predictions terms baseline prices only. noted
below, accuracy measures applied pricelines would reflect actual value.
5.2 Information Employed
set information potentially available beginning game includes data past
games, initial vector flight prices, agents client preferences. TAC-02,
agents except Walverine reported using historical information predictions. ATTac,
kavayaH, Walverine employ flight prices. agents construct pricelines effectively take
account client preferences. Walverine construct pricelines factor
client preferences part equilibrium calculations.
identities agents participating game instance known TAC
preliminary (qualifying seeding) rounds, agents drawn randomly round-robin tournament. However, semifinal final rounds fixed set eight agents series games,
identity agents effectively observable. ATTac agent exploit
availability information.

6. Approaches Price Prediction
Based survey responses, divide TAC-02 prediction techniques three categories.
6.1 Historical Averaging
agents took relatively straightforward approach initial price prediction, estimating
hotel clearing prices according observed historical averages. example, harami calculates
mean hotel prices preceding 200 games, uses initial prediction. respective
agents classified adopting historical approach Table 2 differ set games
3. WhiteBear also reported using pricelines interim prediction (Vetsikas & Selman, 2003), initial predictions
essentially points.

24

fiTAC P RICE P REDICTION

include average, used games seeding round. Given dataset, agents tend
use sample mean distribution estimate, 4 least baseline.
majority averaging agents fixed pool prior games, update averages
finals. exception cuhk, employed moving average previous ten
games current round, previous rounds beginning new round.
UMBCTAC reported employing mean prices predictions respect decisions trips
two days, median prices (which tended lower) decisions one-day
trips. semifinals based statistics last 100 seeding games. finals
dataset comprised 14 games semifinal heat. analysis below, attribute
predictions UMBCTAC based mean values samples.
approach taken SouthamptonTAC (He & Jennings, 2003) unique among TAC
agents. SouthamptonTAC designers partitioned seeding-round games three categories, competitive, non-competitive, semi-competitive. specified reference
price type day hotel game category. agent chooses category
game based monitoring recent game history. actual tournament, SouthamptonTAC began semifinals predicting semi-competitive reference prices, maintaining stance
switching non-competitive last eight games finals.
6.2 Machine Learning
couple TAC-02 agents employed machine learning techniques derive relationships
observable parameters resulting hotel prices. premise approach game-specific
features provide potentially predictive information, enabling agent anticipate hotel price directions manifest price quotes themselves. surprisingly, noted Section 5.2,
two learning agents employed kinds information typical TAC-02 agents.
ATTac predicts prices using sophisticated boosting-based algorithm conditional density
estimation (Stone et al., 2003). Development technique expressly motivated TAC
price-prediction problem, though resulting algorithm quite general. ATTac learns predictor
hotel type day category (i.e., days 1 4 treated symmetrically, 2 3).
predictor applied beginning game maps following features predicted
price hotel:





identity agents participating game,
initial flight prices,
closing time hotel room.

Since hotel closing times unknown game start, predictor induces distribution
price predictions, based distribution hotel closing sequences. distribution constitutes
ATTacs initial price prediction.
kavayaH (Putchala et al., 2002) predicts initial hotel prices using neural networks trained via
backpropagation. agent separate network hotel. output network
4. Several agents complement simple initial prediction relatively sophisticated approach interim
prediction, using evidence price quotes gradually override initial estimate. else equal, course,
straightforwardness advantage. Indeed, simplicity likely significant ingredient livingagentss success
TAC-01 (Fritschi & Dorer, 2002; Wellman et al., 2003).

25

fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIK

one discrete set prices, choice set hotel (type, day) specified
kavayaHs designers based historical prices. inputs network based initial
flight prices, specifically thresholded differences flights adjacent days. example,
hotel T1 might binary input indicates whether price difference inflights
days 1 2 greater 50. Hotel S2 might input, well another based
difference flight prices days 2 3. kavayaHs designers selected relevant inputs
based experiments agent.
6.3 Competitive Analysis
Walverines overall approach TAC markets presume well-approximated
competitive economy (Cheng et al., 2004). method predicting hotel prices literal
application assumption. Specifically, calculates Walrasian competitive equilibrium
TAC economy, defined set prices markets would clear, assuming
agents behave price takers (Katzner, 1989). Taking account exogenously determined
flight prices, Walverine finds set hotel prices support equilibrium, returns
values prediction hotels final prices.
Let vector hotel prices, consisting elements fi denoting price hotel type
day . Let denote agent demand hotel day prices, write vector
demands . Aggregate demand simply sum agent demands, .
Prices constitute competitive equilibrium aggregate demand equals aggregate supply
hotels. Since 16 rooms available hotel day, competitive
equilibrium, .
Starting initial guess , Walverine searches equilibrium prices using tatonnement protocol, iterative price adjustment mechanism originally conceived Walras (Arrow
& Hahn, 1971). Given specification aggregate demand, tatonnement iteratively computes
revised price vector according following difference equation:




(2)

Although equilibrium prices guaranteed exist given discreteness complementarities TAC environment, found procedure typically produces approximate
equilibrium well within 300 iterations Walverine devotes prediction calculation.
critical step competitive analysis determining aggregate demand function. Walverine estimates sum (1) demand based eight clients knows about,
(2) expected demand agents (56 clients), based specified TAC distribution client preferences. calculation expected demand others exact, modulo
summarization entertainment effects, given assumption agent demands separable
client (Cheng et al., 2004). assumption true beginning game (hence initial
prediction), invalidated agents accumulate holdings flights hotels. Although
analytic expression expected demand somewhat complicated, deriving conceptually computationally difficult.
Note larger component Walverines demand estimation expectation
defined distribution client preferences. Therefore, prices derives properly viewed
equilibrium expectation, rather expected equilibrium prices. latter might
actually appropriate measure price prediction. However, since expected equilibrium
26

fiTAC P RICE P REDICTION

much computationally expensive equilibrium expectation (and suspect
difference would relatively small 56 i.i.d. clients), employ simpler measure.

7. Predictions
part survey, entrants provided predictions agents actually employed TAC02 finals semifinals: total 60 games. many cases, predictions constant (i.e.,
every game), straightforward evaluate respect full slate
final semifinal games. two cases initial predictions change every game (ATTac
Walverine), entrants able construct agent would predicted
games, whether actually participated. one case (kavayaH), partial
data. kavayaH reported predictions 32 final games, semifinal heat
participated (H1), except one game predictor crashed.
include two versions ATTac, corresponding predictors learned 2001
2002 preliminary rounds. ATTac01 ATTac02, respectively, represent prediction functions
employed TAC-01 TAC-02 finals. applying ATTac01 predictor TAC-02
finals, use agent identity information disabled.
price vectors supplied entrants employed analysis presented Table 3.
Prices rounded nearest integer display, though analysis employed whatever precision provided. Agents condition game-specific information produce distinct vectors
instance, tabulated here.
Agent
harami
livingagents
PackaTAC
RoxyBot5
sics
WhiteBear
SouthamptonTAC
SouthamptonTAC N
UMBCTAC semifinals
UMBCTAC finals
Actual Mean
Actual Median
Best Euc Dist
Best EVPP
Walverine const

S1
21
27
21
20
30
19
50
20
20
37
68
9
18
28
28

S2
58
118
116
103
100
102
100
30
133
75
85
48
73
51
76

S3
80
124
119
103
100
96
100
30
124
87
97
38
57
67
76

S4
16
41
38
20
40
28
50
20
45
29
52
8
15
0
28

T1 T2 T3 T4
47 108 101
64
73 163 164 105
76 167 164
97
76 152 152
76
95 160 155 110
75 144 141
81
100 150 150 100
50
80
80
50
83 192 158 110
113 141
95
71
121 124 154 109
59 105
98
59
71 111
95
69
80 103 100
84
73 113 113
73

Table 3: Predicted price vectors: Shoreline Shanties, followed Tampa Towers, days 1
4. first ten rows represent predictions employed agents tournament. last
five represent various benchmarks, discussed below.

27

fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIK

first six rows Table 3 (harami WhiteBear) correspond constant predictions
associated agents. noted above, SouthamptonTAC switched two prediction
vectors: represents reference prices semi-competitive environment, N
non-competitive prices. UMBCTAC well switched prediction vectors within 60 gamesin
case introducing finals prediction based average semifinal (H1) prices.
rows labeled Actual Mean Actual Median, respectively, present average
median hotel prices actually resulting 60 games interest. Although clairvoyance obviously admissible approach prediction, include benchmark. direct
sense, actual central tendencies represent best agents taking historical averaging
approach hope capture.
price vectors labeled Best Euc Dist, Best EVPP, Walverine const discussed
Section 8.2.

8. Evaluating Prediction Quality
8.1 Accuracy Measures
remains assess efficacy various prediction approaches, terms agents
price predictions actual TAC-02 final semifinal games. order so, require
ff given actual prices given game.
measure characterizing accuracy prediction
8.1.1 E UCLIDEAN ISTANCE
natural measure closeness two price vectors Euclidean distance:


ff
ff





fi



fi







indexes price hotel day fi . Clearly, lower values
preferred, , .
Calculating straightforward, done reported predictions
ff form distribution, Euclidean distance mean
60 games. Note
provides lower bound average distance components distribution. Thus,
least according measure, evaluation distribution predictions terms means
provides bias favor.
set games
likewise case among constant predictions, actual mean
minimizes aggregate squared distance games. is, actual price vector

,
game ,
















ff













5. RoxyBots prediction based statistics seeding rounds, expressed cumulative price distributions
hotel, discretized deciles. RoxyBot reportedly based decisions samples distribution,
taking decile value occur probability 0.1. tends overestimate prices, however, decile values
correspond upper limits respective ranges. prediction vector presented Table 3 (and analyzed below)
corresponds adjusted value, obtained dropping top decile averaging remaining nine.

28

fiTAC P RICE P REDICTION

closed form prediction minimizing aggregate , one derive numerically
given set games (Bose et al., 2002).
8.1.2 E XPECTED VALUE P ERFECT P REDICTION
Euclidean distance appears reasonable measure accuracy absolute sense. However,
purpose prediction accuracy sake, rather support decisions based
predictions. Thus, seek measure relates principle expected TAC performance.
analogy standard value-of-information measures, introduce concept value
perfect prediction (VPP).
Suppose agent could anticipate perfectly eventual closing price hotels. Then, among
things, agent would able purchase flights immediately confidence
selected optimal trips clients. 6 Since many agents apparently commit trips
beginning game anyway, perfect prediction would translate directly improved quality
choices.7 take primary worth predictions, measure quality prediction
terms supports trip choice comparison perfect anticipation. idea
VPP particularly high agents otherwise poor estimate prices.
already predicting well, value obtaining perfect prediction relatively small.
corresponds use standard value-of-information concepts measuring uncertainty:
agent perfect knowledge, value additional information nil.
Specifically, consider client preferences pa pd hp. trips surplus client
prices , defined value minus cost,


cost

cost simply total price flights hotel rooms included trip . Let





denote trip maximizes surplus respect prices . expression



ff



ff , evaluated respect prices
represents surplus trip chosen based prices
. define value perfect prediction,

ff

VPP ff

(3)

Note VPP definition (3) relative client preferences, whereas seek measure
applicable pair price vectors outside context particular client. end define
6. Modulo residual uncertainty regarding availability entertainment tickets, ignore analysis.
7. compiled statistics temporal profile flight purchases eight agents TAC-02 finals. Four
agents purchased 16 flights (enough round trips clients) within 45 seconds average. eight agents
purchased half flights time, average. Vetsikas Selman (2003) verified experimentally
predicting prices benefits agents commit flights early greater extent delay
flight purchases.

29

fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIK

expected value perfect prediction, EVPP, expectation VPP respect TACs
distribution client preferences:
EVPPff







VPP ff




ff

(4)

Note , lower values EVPP preferred, , EVPP .
(4) see computing EVPP reduces computing ff
. derive
latter value follows. (pa,pd) pair, determine best trip hotel best trip
ff ignoring contribution hotel premium, hp.
hotel T, respectively, prices
determine threshold value hp (if any) agent would switch T.
use boundary split integration surplus (based prices ) trips,
respect underlying distribution hp. Note procedure analogous Walverines
method calculating expected client demand (Cheng et al., 2004) competitive equilibrium
computation.
8.2 Results
Figure 1 plots thirteen agents prediction data according two quality
measures. one exception, EVPP values shown represent averages 60 games
TAC-02 finals semifinals. Since kavayaH predicted 45 games, normalized average EVPP values account relative difficulty games omitted compared
games predicted. normalization multiplied raw average ratio prediction qualities
game sets another representative agent (ATTac01, favorable choice
kavayaH, though normalizations would produced similar results).
two dashed lines Figure 1 represent best-achievable constant predictions respect
two accuracy measures. Best Euc Dist minimizes average Euclidean distance, indicated
vertical line. EVPP, performed hill-climbing search promising candidate vectors derive local minimum measure, represented horizontal line.
reference vectors provided Table 3. Note principle, agents varied predictions across game instances (ATTac, kavayaH, cuhk, Walverine, coarser degree,
SouthamptonTAC UMBCTAC) potential perform outside upper-right quadrant.
assess significance accuracy rankings among agents, performed paired-sample
t-tests pairs agents measures. differences Walverine
ATTac01 reach threshold statistical significance either measure. Walverine beats
ATTac01 fi
ATTac01 beats Walverine EVPP fi
. Walverine

fi) outperforms agents measures. ATTac01 significantly
significantly (fi

) outperforms agents EVPP, statistically distinguishable
(fi
(fi

) kavayaH, harami, cuhk. EVPP, Walverine ATTac01
agents beat Best EVPP (fi
fi
), Best EVPP turn beats
agents (all cuhk significantly). , Walverine agent significantly (fi
)
beat Best Euclidean Distance, turn beats every agent ATTac01 kavayaH.
agent Walverine significantly better Actual Mean, ATTac01, kavayaH,
harami statistically indistinguishable.
large discrepancy performance ATTac01 ATTac02 unexpected, given
predictors generated boosting-based learning algorithm (Stone et al.,
30

fiTAC P RICE P REDICTION

livingagents
PackaTAC
Southampton

Best Euc. Dist.

Expected value perfect prediction

65
60

RoxyBot

whitebear

UMBCTAC

ATTac02

55

SICS

50
harami
45
40

kavayaH

cuhk
Best EVPP

Walverine

35

ATTac01
190

200

210
220
230
Euclidean distance actual prices

240

Figure 1: Prediction quality eleven TAC-02 agents. Dashed lines delimit accuracy achievable constant predictions: best Euclidean distance best EVPP two
respective measures. diagonal line least-squares fit points. Observe
origin graph (190,32).

2003). might explained 2002 preliminary rounds somehow less predictive
TAC-02 finals case 2001. relative success another learning agent, kavayaH,
evidence this, however. likely hypothesis 2002 agent suffered
bug emerging last-minute change computing environments.
directly evaluate prediction form pricelines, would need know initial
demand agent corresponding priceline. obtain information sics,
found accuracy priceline prediction according measures far worse
baseline prediction. impression pricelines may well advantageous
respect decisions agents based them, improve basic accuracy. Note
EVPP inherently based interpretation prices linear, thus may provide proper
evaluation priceline predictions.
8.3 Influence Flight Prices
Observe three best price predictorsATTac01, Walverine, kavayaHare exactly
agents take flight prices account. Initial flight prices potentially affect hotel prices
influence agents early trip choices. theory, lower flight prices increase
tendency agents travel days, else equal, thus increasing prices hotels
31

fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIK

corresponding days stay. Indeed, capturing effect flight prices one main
motivations Walverines price-equilibrium approach. ATTac kavayaH attempt induce
relationship game data. kavayaHs designers, particular, explored neural network models based hypotheses flights likely affect hotel prices (Putchala
et al., 2002).
isolate quantify effect flight prices, investigated contribution different
factors employed Walverine predictions. defined three additional versions Walverines prediction model, ignores information Walverine takes account:





Walv-no-cdata ignores client knowledge, effectively treating demand based
underlying client preference distribution assumed agents.
Walv-constF ignores initial flight prices, assuming set mean
initial flight distribution (i.e., 325) every game instance.
Walverine const ignores client knowledge takes flight prices mean rather
actual values. result constant prediction vector, presented Table 3.

Figure 2 plots prediction qualities agents. Ignoring client knowledge degraded prediction quality slightly, increasing EVPP 38.0 38.6. Neglecting initial flight prices,
however, significantly hurt predictions, increasing EVPP 47.9. Ignoring both, Walverine const
incurred average EVPP 49.1.
50

Walverine_Const
Actual_Mean

Expected value perfect prediction

48

Walv_constF

46
Best_EucDist
Actual_Median

44

Best_EVPP

42

40
Walv_no_cdata
38

Walverine
197.5

200

202.5
205
207.5
210
Euclidean distance actual prices

212.5

215

Figure 2: Prediction quality Walverine variants various central tendency benchmarks.
results confirm predictive value flight prices. EVPP measure, Walverine
gain significant advantage considering client data, cannot beat Best EVPP
32

fiTAC P RICE P REDICTION

without considering initial flight prices. , client data make significant (fi
fi) difference also considering flight data. Flight data significantly (fi
) affects Walverines
prediction quality metric, regardless client data.
8.4 Relating Accuracy Performance
indicated scatter plot Figure 1, two accuracy measures highly correlated
(
). Given EVPP value-based, suggest accuracy translates somewhat
proportionally performance. However, EVPP highly idealized proxy actual scores,
definitively establish relation prediction accuracy overall TAC performance.
relation observed prior work Stone et al. (2003), evaluated predictive
accuracy overall performance four variations ATTac01 experimental trial. Employing different measure prediction quality above, found average score
related monotonically average predictive accuracy.
effort directly connect accuracy measures bottom line, regressed
actual TAC-02 game scores accuracy measures reported predictionsone data
point per agent per game. controlled favorability random client preferences, employing
summary statistics used construct client preference adjustment analysis
TAC-01 tournament (Wellman et al., 2003). two separate regressions, found highly
significant coefficients (fi ) EVPP. Predictive accuracy explained score

), however, might expected given unmodeled
variance quite poorly (
variation across agents.
reduce variation, undertook series controlled trials involving variants Walverine (Cheng et al., 2004). trial comprised set games fixed configuration agents.
agents constant within trials, varied across, conducted weeks months
apart Walverine undergoing modifications. trial, regressed actual score
first agent EVPP, controlling favorability random client preferences.
considered one agent per game, since data points agents would dependent
given common game instance. results linear regression summarized Table 4.
Trial
1
2
3



200
151
110

Mean EVPP
70.4
32.2
59.5

EVPP Coeff
-8.89
-11.59
-10.26





0.57
0.26
0.65

Table 4: Regression score EVPP three trials.
EVPP coefficient highly significant cases (fi ). Note since EVPP
measured per-client units scores, direct translation would equate reducing EVPP
one increase eight score points. regressions yielded coefficients ranging
-8.89 -11.59, take rough confirmation expected relationship. anything,
results indicate EVPP understates value predictionwhich might expect since
addresses initial trip choice. Interestingly, regression model seems provide better fit
(as measured ) trials involving worse price predictors (as measured mean EVPP).
suggests prediction optimized, unmodeled factors may relatively greater
incremental influence score.
33

fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIK

noted games appear quite unrepresentative TAC tournament
games. Since agents versions Walverine, tend make trip choices
basis.

9. Limitations
important emphasize several limitations study, must qualify conclusions
drawn efficacy prediction methods evaluated here.
First, focused exclusively initial price prediction, whereas many agents placed
greater emphasis interim prediction task.
Second, many cases represented agents predictions abstraction actual
object produced prediction modules. particular, reduce probability distributions
means, consider first unit priceline prediction. generally,
account different ways agents apply predictions generate. Without question
even measure introduce, EVPP, inspired thinking terms Walverine uses
predictions. Perhaps measures tailored processes agents would (justifiably) show
predictions favorable light.
Third, recognized despite desirability isolating focused components
agent analysis, complete separation possible principle. Prediction evaluation relative
agent uses prediction, also makes tradeoffs (e.g.,
commits flights), ultimately entire strategy, complexity. Studies
must strive balance benefits decomposition appreciation interconnections
synergies among elements sophisticated agents behavior.

10. Conclusions
presented comprehensive survey approaches initial price prediction TAC-02,
quantitative analysis relative accuracy. analysis introduces new measure, expected
value perfect prediction, captures important sense instrumental value accurate
predictions, beyond nominal correspondence realized outcomes.
draw several conclusions price prediction TAC exercise. First, results
clearly demonstrate instance-specific information provide significant leverage pure
background history. Three agents use instance-specific information perform better least
one measure constant prediction. particular, initial flight prices provide substantial
predictive value. induced verified empirically, seen success
machine-learning agents. predictive value flows influence flight prices demand
hotels, indicated success competitive analysis capturing relationship.
believe striking purely analytical approach, without empirical tuning, could
achieve accuracy comparable best available machine-learning method. Moreover, many
would surely skeptical straight competitive analysis could prove successful, given
manifest unreality assumptions applied TAC. analysis certainly show
competitive equilibrium best possible model price formation TAC,
demonstrate deriving shape market idealized economic theory surprisingly effective.
34

fiTAC P RICE P REDICTION

several advantages model-based reasoning, obviously ability perform
minimal empirical data. Even historical information available, misleading rely nonstationary environment. tournament setup like TAC naturally violates
stationarity, agent pool evolves time, selection well individual learning
development. course, dealing time-variance, particularly multiagent environments,
active area current research, ultimately best methods combine elements
model-based data-based reasoning.
Finally, suggest present study represents evidence research competitions
right circumstances produce knowledge insights beyond might emerge independent research efforts. hope additional researchers inspired bring innovative
ideas trading strategy next TAC, look forward investigating results
interplay future work.

Acknowledgments
study would possible without generous cooperation TAC-02 entrants.
research supported part NSF grant IIS-9988715, well STIET fellowship
third author, NSF IGERT grant.

References
Arrow, K. J., & Hahn, F. H. (1971). General Competitive Analysis. Holden-Day, San Francisco.
Bose, P., Maheshwari, A., & Morin, P. (2002). Fast approximations sums distances, clustering
Fermat-Weber problem. Computational Geometry: Theory Applications, 24,
135146.
Boyan, J., & Greenwald, A. (2001). Bid determination simultaneous auctions: agent architecture. Third ACM Conference Electronic Commerce, pp. 210212, Tampa, FL.
Cheng, S.-F., Leung, E., Lochner, K. M., OMalley, K., Reeves, D. M., & Wellman, M. P. (2004).
Walverine: Walrasian trading agent. Decision Support Systems, appear.
Fritschi, C., & Dorer, K. (2002). Agent-oriented software engineering successful TAC participation. First International Joint Conference Autonomous Agents Multi-Agent Systems,
Bologna.
Greenwald, A. (2003a). 2002 trading agent competition: overview agent strategies. AI
Magazine, 24(1), 8391.
Greenwald, A. (2003b). Bidding uncertainty simultaneous auctions. IJCAI-03 Workshop
Trading Agent Design Analysis, Acapulco.
He, M., & Jennings, N. R. (2003). SouthamptonTAC: adaptive autonomous trading agent. ACM
Transactions Internet Technology, 3, 218235.
Katzner, D. W. (1989). Walrasian Vision Microeconomy. University Michigan Press.
Putchala, R. P., Morris, V. N., Kazhanchi, R., Raman, L., & Shekhar, S. (2002). kavayaH: trading
agent developed TAC-02. Tech. rep., Oracle India.
Sadeh, N., Arunachalam, R., Eriksson, J., Finne, N., & Janson, S. (2003). TAC-03: supply-chain
trading competition. AI Magazine, 24(1), 9294.
35

fiW ELLMAN , R EEVES , L OCHNER , & VOROBEYCHIK

Stone, P. (2002). Multiagent competitions research: Lessons RoboCup TAC. Sixth
RoboCup International Symposium, Fukuoka, Japan.
Stone, P., & Greenwald, A. (2004). first international trading agent competition: Autonomous
bidding agents. Electronic Commerce Research, appear.
Stone, P., Schapire, R. E., Littman, M. L., Csirik, J. A., & McAllester, D. (2003). Decision-theoretic
bidding based learned density models simultaneous, interacting auctions. Journal
Artificial Intelligence Research, 19, 209242.
Vetsikas, I. A., & Selman, B. (2003). principled study design tradeoffs autonomous
trading agents. Second International Joint Conference Autonomous Agents MultiAgent Systems, pp. 473480, Melbourne.
Wellman, M. P., Greenwald, A., Stone, P., & Wurman, P. R. (2003). 2001 trading agent competition. Electronic Markets, 13, 412.

36

fi
